From hb at stat.berkeley.edu  Thu Mar  1 00:01:08 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Wed, 28 Feb 2007 15:01:08 -0800
Subject: [R] Removing directory?
Message-ID: <59d7961d0702281501o7523bfc7j2e9f1a9ea439ab3f@mail.gmail.com>

Hi,

I'm trying to remove/delete a directory usingR.  I've tried the
following with no success:

% Rterm --vanilla
> getwd()
[1] "C:/Documents and Settings/hb/braju.com.R/aroma.affymetrix/test"

> dir.create("foo")
> file.info("foo")
    size isdir mode               mtime               ctime               atime
foo    0  TRUE  777 2007-02-28 14:52:10 2007-02-28 14:52:10 2007-02-28 14:52:10

# Using file.remove()
> res <- sapply(c("foo", "foo/", "foo\\", "./foo", "./foo/"), file.remove)
> res
   foo   foo/  foo\\  ./foo ./foo/
 FALSE  FALSE  FALSE  FALSE  FALSE

# Using unlink()
> res <- sapply(c("foo", "foo/", "foo\\", "./foo", "./foo/"), unlink)
> res
   foo   foo/  foo\\  ./foo ./foo/
     1      0      0      1      0

# Directory is still there
> file.info("foo")
    size isdir mode               mtime               ctime               atime
foo    0  TRUE  777 2007-02-28 14:52:10 2007-02-28 14:52:10 2007-02-28 14:52:10

I've tried the above from a different directory too, i.e.
setwd("C:/"), with no success.  Using absolute pathnames the same.

This is on WinXP R v2.4.1:

> sessionInfo()
R version 2.4.1 Patched (2007-01-13 r40470)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United States.1252;LC_MON
ETARY=English_United States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252


attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
[7] "base"

Thanks

Henrik


From topkatz at msn.com  Thu Mar  1 00:07:21 2007
From: topkatz at msn.com (Talbot Katz)
Date: Wed, 28 Feb 2007 18:07:21 -0500
Subject: [R] Efficient way to repeat rows (or columns) of a matrix?
Message-ID: <BAY132-F10B746E67AC7046CC9E7B1AA810@phx.gbl>

Hi.

If I have a vector, v_1, and another vector of positive integers, i_1, the 
same length as v_1, then rep(v_1,i_1) will repeat v_i[j] exactly i_1[j] 
times, like so:

>rep(c(1,2,3),c(3,2,1))
[1] 1 1 1 2 2 3
>

I'd like to do the same sort of thing where I replace v_1 with a matrix, and 
the jth row of the matrix is repeated i_1 times.

Obviously, I could do this with for loops, like the following:

>(ma1=matrix(1:6,nrow=2))
     [,1] [,2] [,3]
[1,]    1    3    5
[2,]    2    4    6
>vr1=c(2,3)
>rma1=NULL
>for(i in 1:length(vr1)){for(j in 1:vr1[i]){rma1=rbind(rma1,ma1[i,])}}
>rma1
     [,1] [,2] [,3]
[1,]    1    3    5
[2,]    1    3    5
[3,]    2    4    6
[4,]    2    4    6
[5,]    2    4    6
>

I just thought some of you clever programmers could show me a more efficient 
way.  (I apologize if this question has come up before and you're tired of 
answering it ;-)

Thanks!

--  TMK  --
212-460-5430	home
917-656-5351	cell


From jmacdon at med.umich.edu  Thu Mar  1 00:09:40 2007
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Wed, 28 Feb 2007 18:09:40 -0500
Subject: [R] Removing directory?
In-Reply-To: <59d7961d0702281501o7523bfc7j2e9f1a9ea439ab3f@mail.gmail.com>
References: <59d7961d0702281501o7523bfc7j2e9f1a9ea439ab3f@mail.gmail.com>
Message-ID: <45E60BB4.8020907@med.umich.edu>

I think you want to add a recursive = TRUE to your call to unlink().

Best,

Jim

Henrik Bengtsson wrote:
> Hi,
> 
> I'm trying to remove/delete a directory usingR.  I've tried the
> following with no success:
> 
> % Rterm --vanilla
> 
>>getwd()
> 
> [1] "C:/Documents and Settings/hb/braju.com.R/aroma.affymetrix/test"
> 
> 
>>dir.create("foo")
>>file.info("foo")
> 
>     size isdir mode               mtime               ctime               atime
> foo    0  TRUE  777 2007-02-28 14:52:10 2007-02-28 14:52:10 2007-02-28 14:52:10
> 
> # Using file.remove()
> 
>>res <- sapply(c("foo", "foo/", "foo\\", "./foo", "./foo/"), file.remove)
>>res
> 
>    foo   foo/  foo\\  ./foo ./foo/
>  FALSE  FALSE  FALSE  FALSE  FALSE
> 
> # Using unlink()
> 
>>res <- sapply(c("foo", "foo/", "foo\\", "./foo", "./foo/"), unlink)
>>res
> 
>    foo   foo/  foo\\  ./foo ./foo/
>      1      0      0      1      0
> 
> # Directory is still there
> 
>>file.info("foo")
> 
>     size isdir mode               mtime               ctime               atime
> foo    0  TRUE  777 2007-02-28 14:52:10 2007-02-28 14:52:10 2007-02-28 14:52:10
> 
> I've tried the above from a different directory too, i.e.
> setwd("C:/"), with no success.  Using absolute pathnames the same.
> 
> This is on WinXP R v2.4.1:
> 
> 
>>sessionInfo()
> 
> R version 2.4.1 Patched (2007-01-13 r40470)
> i386-pc-mingw32
> 
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United States.1252;LC_MON
> ETARY=English_United States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
> 
> 
> attached base packages:
> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
> [7] "base"
> 
> Thanks
> 
> Henrik
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
James W. MacDonald
University of Michigan
Affymetrix and cDNA Microarray Core
1500 E Medical Center Drive
Ann Arbor MI 48109
734-647-5623



**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.


From jmacdon at med.umich.edu  Thu Mar  1 00:20:32 2007
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Wed, 28 Feb 2007 18:20:32 -0500
Subject: [R] Efficient way to repeat rows (or columns) of a matrix?
In-Reply-To: <BAY132-F10B746E67AC7046CC9E7B1AA810@phx.gbl>
References: <BAY132-F10B746E67AC7046CC9E7B1AA810@phx.gbl>
Message-ID: <45E60E40.5040903@med.umich.edu>

Hi Talbot,

ma <- matrix(1:6, nrow  = 2)
rma <- ma[rep(1:2, c(2,3)),]

Best,

Jim



Talbot Katz wrote:
> Hi.
> 
> If I have a vector, v_1, and another vector of positive integers, i_1, the 
> same length as v_1, then rep(v_1,i_1) will repeat v_i[j] exactly i_1[j] 
> times, like so:
> 
> 
>>rep(c(1,2,3),c(3,2,1))
> 
> [1] 1 1 1 2 2 3
> 
> 
> I'd like to do the same sort of thing where I replace v_1 with a matrix, and 
> the jth row of the matrix is repeated i_1 times.
> 
> Obviously, I could do this with for loops, like the following:
> 
> 
>>(ma1=matrix(1:6,nrow=2))
> 
>      [,1] [,2] [,3]
> [1,]    1    3    5
> [2,]    2    4    6
> 
>>vr1=c(2,3)
>>rma1=NULL
>>for(i in 1:length(vr1)){for(j in 1:vr1[i]){rma1=rbind(rma1,ma1[i,])}}
>>rma1
> 
>      [,1] [,2] [,3]
> [1,]    1    3    5
> [2,]    1    3    5
> [3,]    2    4    6
> [4,]    2    4    6
> [5,]    2    4    6
> 
> 
> I just thought some of you clever programmers could show me a more efficient 
> way.  (I apologize if this question has come up before and you're tired of 
> answering it ;-)
> 
> Thanks!
> 
> --  TMK  --
> 212-460-5430	home
> 917-656-5351	cell
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
James W. MacDonald
University of Michigan
Affymetrix and cDNA Microarray Core
1500 E Medical Center Drive
Ann Arbor MI 48109
734-647-5623



**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.


From dmca at ucla.edu  Thu Mar  1 00:13:35 2007
From: dmca at ucla.edu (D L McArthur)
Date: Wed, 28 Feb 2007 23:13:35 +0000 (UTC)
Subject: [R]
	=?utf-8?q?Packages_in_R_for_least_median_squares_regression_a?=
	=?utf-8?q?nd_computing=09outliers_=28thompson_tau_technique_etc=2E?=
	=?utf-8?q?=29?=
References: <916363.92662.qm@web43135.mail.sp1.yahoo.com>
Message-ID: <loom.20070301T001232-961@post.gmane.org>

lalitha viswanath <lalithaviswanath <at> yahoo.com> writes:
> 
> Hi
> I am looking for suitable packages in R that do
> regression analyses using least median squares method
> (or better). Additionally, I am also looking for
> packages that implement algorithms/methods for
> detecting outliers that can be discarded before doing
> the regression analyses....
> 
Function lmsreg in package MASS provides least median of 
squares regression estimator.  See RR Wilcox, Introduction 
to Robust Estimation and Hypothesis Testing, Elsevier 2005 
and a wealth of corresponding R functions for robust data
analyses available at 
http://psychology.usc.edu/faculty_homepage.php?id=43


From topkatz at msn.com  Thu Mar  1 00:35:00 2007
From: topkatz at msn.com (Talbot Katz)
Date: Wed, 28 Feb 2007 18:35:00 -0500
Subject: [R] Efficient way to repeat rows (or columns) of a matrix?
In-Reply-To: <45E60E40.5040903@med.umich.edu>
Message-ID: <BAY132-F21B1A6FC00CE122E8031D8AA810@phx.gbl>

Brilliant, thanks!

--  TMK  --
212-460-5430	home
917-656-5351	cell



>From: "James W. MacDonald" <jmacdon at med.umich.edu>
>To: Talbot Katz <topkatz at msn.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] Efficient way to repeat rows (or columns) of a matrix?
>Date: Wed, 28 Feb 2007 18:20:32 -0500
>
>Hi Talbot,
>
>ma <- matrix(1:6, nrow  = 2)
>rma <- ma[rep(1:2, c(2,3)),]
>
>Best,
>
>Jim
>
>
>
>Talbot Katz wrote:
>>Hi.
>>
>>If I have a vector, v_1, and another vector of positive integers, i_1, the 
>>same length as v_1, then rep(v_1,i_1) will repeat v_i[j] exactly i_1[j] 
>>times, like so:
>>
>>
>>>rep(c(1,2,3),c(3,2,1))
>>
>>[1] 1 1 1 2 2 3
>>
>>
>>I'd like to do the same sort of thing where I replace v_1 with a matrix, 
>>and the jth row of the matrix is repeated i_1 times.
>>
>>Obviously, I could do this with for loops, like the following:
>>
>>
>>>(ma1=matrix(1:6,nrow=2))
>>
>>      [,1] [,2] [,3]
>>[1,]    1    3    5
>>[2,]    2    4    6
>>
>>>vr1=c(2,3)
>>>rma1=NULL
>>>for(i in 1:length(vr1)){for(j in 1:vr1[i]){rma1=rbind(rma1,ma1[i,])}}
>>>rma1
>>
>>      [,1] [,2] [,3]
>>[1,]    1    3    5
>>[2,]    1    3    5
>>[3,]    2    4    6
>>[4,]    2    4    6
>>[5,]    2    4    6
>>
>>
>>I just thought some of you clever programmers could show me a more 
>>efficient way.  (I apologize if this question has come up before and 
>>you're tired of answering it ;-)
>>
>>Thanks!
>>
>>--  TMK  --
>>212-460-5430	home
>>917-656-5351	cell
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>
>--
>James W. MacDonald
>University of Michigan
>Affymetrix and cDNA Microarray Core
>1500 E Medical Center Drive
>Ann Arbor MI 48109
>734-647-5623
>
>
>
>**********************************************************
>Electronic Mail is not secure, may not be read every day, and should not be 
>used for urgent or sensitive issues.


From valderama at gmail.com  Thu Mar  1 00:49:38 2007
From: valderama at gmail.com (Laurent Valdes)
Date: Thu, 1 Mar 2007 00:49:38 +0100
Subject: [R] PROC TABULATE with R
In-Reply-To: <f8e6ff050702280738x2b357ed6jf19c59fff88a146c@mail.gmail.com>
References: <3ef00e160702280609h5f0c0e4o1d60199706b30d4e@mail.gmail.com>
	<f8e6ff050702280738x2b357ed6jf19c59fff88a146c@mail.gmail.com>
Message-ID: <3ef00e160702281549h3136b780ha55bf998bc49a42e@mail.gmail.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070301/baeb8abd/attachment.pl 

From hb at stat.berkeley.edu  Thu Mar  1 00:52:12 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Wed, 28 Feb 2007 15:52:12 -0800
Subject: [R] Removing directory?
In-Reply-To: <45E60BB4.8020907@med.umich.edu>
References: <59d7961d0702281501o7523bfc7j2e9f1a9ea439ab3f@mail.gmail.com>
	<45E60BB4.8020907@med.umich.edu>
Message-ID: <59d7961d0702281552t78fd1da3pae3af0f731706546@mail.gmail.com>

Thanks!  ...as the help indeed says:

     If 'recursive = FALSE' directories are not deleted, not even empty
     ones.

It works;

> dir.create("foo")
> file.info("foo")
    size isdir mode               mtime               ctime               atime
foo    0  TRUE  777 2007-02-28 15:50:33 2007-02-28 15:50:33 2007-02-28 15:50:33
> print(file.remove("foo", recursive=TRUE))
[1] FALSE
> file.info("foo")
    size isdir mode               mtime               ctime               atime
foo    0  TRUE  777 2007-02-28 15:50:33 2007-02-28 15:50:33 2007-02-28 15:50:33
> print(unlink("foo", recursive=TRUE))
[1] 0
> file.info("foo")
    size isdir mode mtime ctime atime
foo   NA    NA <NA>  <NA>  <NA>  <NA>
> file.exists("foo")
[1] FALSE

/Henrik

On 2/28/07, James W. MacDonald <jmacdon at med.umich.edu> wrote:
> I think you want to add a recursive = TRUE to your call to unlink().
>
> Best,
>
> Jim
>
> Henrik Bengtsson wrote:
> > Hi,
> >
> > I'm trying to remove/delete a directory usingR.  I've tried the
> > following with no success:
> >
> > % Rterm --vanilla
> >
> >>getwd()
> >
> > [1] "C:/Documents and Settings/hb/braju.com.R/aroma.affymetrix/test"
> >
> >
> >>dir.create("foo")
> >>file.info("foo")
> >
> >     size isdir mode               mtime               ctime               atime
> > foo    0  TRUE  777 2007-02-28 14:52:10 2007-02-28 14:52:10 2007-02-28 14:52:10
> >
> > # Using file.remove()
> >
> >>res <- sapply(c("foo", "foo/", "foo\\", "./foo", "./foo/"), file.remove)
> >>res
> >
> >    foo   foo/  foo\\  ./foo ./foo/
> >  FALSE  FALSE  FALSE  FALSE  FALSE
> >
> > # Using unlink()
> >
> >>res <- sapply(c("foo", "foo/", "foo\\", "./foo", "./foo/"), unlink)
> >>res
> >
> >    foo   foo/  foo\\  ./foo ./foo/
> >      1      0      0      1      0
> >
> > # Directory is still there
> >
> >>file.info("foo")
> >
> >     size isdir mode               mtime               ctime               atime
> > foo    0  TRUE  777 2007-02-28 14:52:10 2007-02-28 14:52:10 2007-02-28 14:52:10
> >
> > I've tried the above from a different directory too, i.e.
> > setwd("C:/"), with no success.  Using absolute pathnames the same.
> >
> > This is on WinXP R v2.4.1:
> >
> >
> >>sessionInfo()
> >
> > R version 2.4.1 Patched (2007-01-13 r40470)
> > i386-pc-mingw32
> >
> > locale:
> > LC_COLLATE=English_United States.1252;LC_CTYPE=English_United States.1252;LC_MON
> > ETARY=English_United States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
> >
> >
> > attached base packages:
> > [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
> > [7] "base"
> >
> > Thanks
> >
> > Henrik
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
> --
> James W. MacDonald
> University of Michigan
> Affymetrix and cDNA Microarray Core
> 1500 E Medical Center Drive
> Ann Arbor MI 48109
> 734-647-5623
>
>
>
> **********************************************************
> Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.
>


From ggrothendieck at gmail.com  Thu Mar  1 01:06:51 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 28 Feb 2007 19:06:51 -0500
Subject: [R] What is a expression good for?
In-Reply-To: <20070228205814.M50062@centroin.com.br>
References: <20070228205814.M50062@centroin.com.br>
Message-ID: <971536df0702281606u4a4b3f19h34fc1f8fdcdbd991@mail.gmail.com>

You can evaluate it, differentiate it, pick apart its components,
use it as a title or legend in a plot, use it as a function body
and probably other things too:

e <- expression(x+y)

eval(e, list(x = 1, y = 2)) # 3

D(e, "x")

e[[1]][[1]] # +
e[[1]][[2]] # x
e[[1]][[3]] # y

plot(1:10, main = e)
legend("topleft", e, pch = 1)


f <- function(x,y) {}
body(f) <- e
f(1,2) # 3

On 2/28/07, Alberto Monteiro <albmont at centroin.com.br> wrote:
> I mean, I can generate a expression, for example, with:
>
> z <- expression(x+y)
>
> But then how can I _use_ it? Is it possible to retrieve
> information from it, for example, that z is a sum, its
> first argument is x (or expression(x)) and its second
> argument is y?
>
> Alberto Monteiro
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dylan.beaudette at gmail.com  Thu Mar  1 04:41:33 2007
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Wed, 28 Feb 2007 19:41:33 -0800
Subject: [R] prop.test or chisq.test ..?
In-Reply-To: <17893.17994.538502.77917@stat.math.ethz.ch>
References: <200702261736.31001.dylan.beaudette@gmail.com>
	<17893.17994.538502.77917@stat.math.ethz.ch>
Message-ID: <200702281941.33368.dylan.beaudette@gmail.com>

On Wednesday 28 February 2007 01:07, Christoph Buser wrote:
> Hi
>
> Some comments are inside.
>
> Dylan Beaudette writes:
>  > Hi everyone,
>  >
>  > Suppose I have a count the occurrences of positive results, and the
>  > total number of occurrences:
>  >
>  >
>  > pos <- 14
>  > total <- 15
>  >
>  > testing that the proportion of positive occurrences is greater than 0.5
>  > gives a p-value and confidence interval:
>  >
>  > prop.test( pos, total, p=0.5, alternative='greater')
>  >
>  >         1-sample proportions test with continuity correction
>  >
>  > data:  14 out of 15, null probability 0.5
>  > X-squared = 9.6, df = 1, p-value = 0.0009729
>  > alternative hypothesis: true p is greater than 0.5
>  > 95 percent confidence interval:
>  >  0.706632 1.000000
>  > sample estimates:
>  >         p
>  > 0.9333333
>
> First of all by default there is a continuity correction in
> prop.test(). If you use
>
> prop.test(pos, total, p=0.5, alternative="greater", correct = FALSE)
>
> 	1-sample proportions test without continuity correction
>
> data:  pos out of total, null probability 0.5
> X-squared = 11.2667, df = 1, p-value = 0.0003946
> alternative hypothesis: true p is greater than 0.5
> 95 percent confidence interval:
>  0.7492494 1.0000000
> sample estimates:
>         p
> 0.9333333
>
> Remark that know the X-squared is identical to  your result from
> chisq.test(), but the p-value is 0.0007891/2
>
> The reason is that you are testing here the against the
> alternative "greater"
>
> If you use a two sided test
>
> prop.test(pos, total, p=0.5, alternative="two.sided", correct = FALSE)
>
> then you reporduce the results form chisq.test().
>
>  > My question is how does the use of chisq.test() differ from the above
>  > operation. For example:
>  >
>  > chisq.test(table( c(rep('pos', 14), rep('neg', 1)) ))
>  >
>  >         Chi-squared test for given probabilities
>  >
>  > data:  table(c(rep("pos", 14), rep("neg", 1)))
>  > X-squared = 11.2667, df = 1, p-value = 0.0007891
>  >
>  > ... gives slightly different results. I am corrent in interpreting that
>  > the chisq.test() function in this case is giving me a p-value associated
>  > with the test that the probabilities of pos are *different* than the
>  > probabilities of neg -- and thus a larger p-value than the prop.test(...
>  > , p=0.5, alternative='greater') ?
>
> Yes. In your example chisq.test() tests the null hypothesis if
> all population probabilities are equal
>
> ?chisq.test says:
> "In this case, the hypothesis tested is whether the population
> probabilities equal those in 'p', or are all equal if 'p' is not
> given."
>
> which means p1 = p2 = 0.5 in your two population case against
> the alternative p1 != p2.
>
> This is similar to the test in prop.test() p=0.5 against p!=0.5
> and therefore you get identical results if you choose
> alternative="two.sided" in prop.test().
>
>  > I realize that this is a rather elementary question, and references to a
>  > text would be just as helpful. Ideally, I would like a measure of how
>  > much I can 'trust' that a larger proportion is also statistically
>  > meaningful. Thus far the results from prop.test() match my intuition,
>  > but
>  > affirmation would be
>
> Your intuition was correct. Nevertheless in your original
> results (with the continuity correction), the p-value of
> prop.test()  (0.0009729) was larger than the p-value of
> chisq.test() (0.0007891) and therefore against your intuition.
>
>  > great.
>  >
>  > Cheers,
>  >
>  >
>  > --
>  > Dylan Beaudette
>  > Soils and Biogeochemistry Graduate Group
>  > University of California at Davis
>  > 530.754.7341
>  >
>  > ______________________________________________
>  > R-help at stat.math.ethz.ch mailing list
>  > https://stat.ethz.ch/mailman/listinfo/r-help
>  > PLEASE do read the posting guide
>  > http://www.R-project.org/posting-guide.html and provide commented,
>  > minimal, self-contained, reproducible code.
>
> Hope this helps
>
> Christoph Buser
>

Thanks for the tips Christoph, this was the help that I was looking for.

cheers,

-- 
Dylan Beaudette
Soils and Biogeochemistry Graduate Group
University of California at Davis
530.754.7341


From statba at nus.edu.sg  Thu Mar  1 05:15:28 2007
From: statba at nus.edu.sg (Berwin A Turlach)
Date: Thu, 1 Mar 2007 12:15:28 +0800
Subject: [R] problem with help.start and ?somefunction
In-Reply-To: <20070228164118.ALN76279@expms1.cites.uiuc.edu>
References: <20070228164118.ALN76279@expms1.cites.uiuc.edu>
Message-ID: <20070301121528.7a2779f6@berwin5>

G'day Jenny,

On Wed, 28 Feb 2007 16:41:18 -0600 (CST)
<drnevich at uiuc.edu> wrote:

> I am going to be teaching a workshop next week using R and
> Bioconductor in one of our university's computer labs. They have
> recently installed R 2.4.1 for me, and I'm checking all my scripts. I
> just noticed that using the ?somefunction call to access the
> documentation for that function is not working. On my own PC, the ?
> call output changed between R 2.3 and 2.4; before it would open some
> sort of plain text file and now it opens a nice browser with all the
> functions of that package listed in a side frame and the function
> documentation listed in the main window. 

Yes, as far as I know, one of the changes on MS Windows from R 2.3.x to
R 2.4.x was that the compiled html help system became the default.  

> At the computer lab, calling ?somefunction opens the browser with
> the functions in the side frame, but the main window says "The page
> cannot be displayed"

On MS Windows (which is not my preferred platform, so I might use some
inaccurate terminology in the sequel) I always preferred the compiled
html help system.  So I usually asked our IT guys to make that the
default via the etc/Rprofile.site configuration file, and ran into the
same problem you have about a year ago.

In a nutshell, yet another security issue was discovered if compiled
html help files are run over a network.  Hence MS released a patch that
switched this possibility off.  If I remember correctly, the discussion
at:
	http://forums.techarena.in/showthread.php?t=322640&goto=nextnewest

proved to be very helpful to us and enabled us to allow again the use
of compiled help files over the network.  But I never really understood
what the security implications are and what issues really existed.  I
trusted our IT guys to understand that and to decide whether it was
o.k. to make the necessary registry changes to allow compiled html help
again. :-)

> If I try help.start(), I get the warnings below, but it does open the
> html search page. I'm guessing the two are related, 

No, they are not related. :-)

As the warnings message says, if you say help.start() R tries to update
a file to which it does not have write permissions.  

As I understand it, help.start() runs by default with "update=TRUE" (you
may want to try "update=FALSE") on MS Windows.  The "update=TRUE"
options means that the functions make.packages.html() and
make.search.html() are called to update some information (details are
on the help pages of these two functions).  The updated information is
written to a file, in your case
P:\xpapps\R\R-2.4.1/doc/html/search/index.txt, but you have no write
access to this file.  Hence the warning.  This is a problem with how
the permissions are set up on your lab/network.  Not sure how to solve
this one, our lab had a setup where users had write access to the
location of that file, so we never experienced the problem (except
once, when due to a server upgrade the permissions on the lab machines
got messed up).

Hope this helps.

Cheers,

	Berwin


From statba at nus.edu.sg  Thu Mar  1 05:15:28 2007
From: statba at nus.edu.sg (Berwin A Turlach)
Date: Thu, 1 Mar 2007 12:15:28 +0800
Subject: [R] problem with help.start and ?somefunction
In-Reply-To: <20070228164118.ALN76279@expms1.cites.uiuc.edu>
References: <20070228164118.ALN76279@expms1.cites.uiuc.edu>
Message-ID: <20070301121528.7a2779f6@berwin5>

G'day Jenny,

On Wed, 28 Feb 2007 16:41:18 -0600 (CST)
<drnevich at uiuc.edu> wrote:

> I am going to be teaching a workshop next week using R and
> Bioconductor in one of our university's computer labs. They have
> recently installed R 2.4.1 for me, and I'm checking all my scripts. I
> just noticed that using the ?somefunction call to access the
> documentation for that function is not working. On my own PC, the ?
> call output changed between R 2.3 and 2.4; before it would open some
> sort of plain text file and now it opens a nice browser with all the
> functions of that package listed in a side frame and the function
> documentation listed in the main window. 

Yes, as far as I know, one of the changes on MS Windows from R 2.3.x to
R 2.4.x was that the compiled html help system became the default.  

> At the computer lab, calling ?somefunction opens the browser with
> the functions in the side frame, but the main window says "The page
> cannot be displayed"

On MS Windows (which is not my preferred platform, so I might use some
inaccurate terminology in the sequel) I always preferred the compiled
html help system.  So I usually asked our IT guys to make that the
default via the etc/Rprofile.site configuration file, and ran into the
same problem you have about a year ago.

In a nutshell, yet another security issue was discovered if compiled
html help files are run over a network.  Hence MS released a patch that
switched this possibility off.  If I remember correctly, the discussion
at:
	http://forums.techarena.in/showthread.php?t=322640&goto=nextnewest

proved to be very helpful to us and enabled us to allow again the use
of compiled help files over the network.  But I never really understood
what the security implications are and what issues really existed.  I
trusted our IT guys to understand that and to decide whether it was
o.k. to make the necessary registry changes to allow compiled html help
again. :-)

> If I try help.start(), I get the warnings below, but it does open the
> html search page. I'm guessing the two are related, 

No, they are not related. :-)

As the warnings message says, if you say help.start() R tries to update
a file to which it does not have write permissions.  

As I understand it, help.start() runs by default with "update=TRUE" (you
may want to try "update=FALSE") on MS Windows.  The "update=TRUE"
options means that the functions make.packages.html() and
make.search.html() are called to update some information (details are
on the help pages of these two functions).  The updated information is
written to a file, in your case
P:\xpapps\R\R-2.4.1/doc/html/search/index.txt, but you have no write
access to this file.  Hence the warning.  This is a problem with how
the permissions are set up on your lab/network.  Not sure how to solve
this one, our lab had a setup where users had write access to the
location of that file, so we never experienced the problem (except
once, when due to a server upgrade the permissions on the lab machines
got messed up).

Hope this helps.

Cheers,

	Berwin


From ahimsa at camposarceiz.com  Thu Mar  1 08:54:53 2007
From: ahimsa at camposarceiz.com (ahimsa campos-arceiz)
Date: Thu, 1 Mar 2007 16:54:53 +0900
Subject: [R] how to apply the function cut( ) to many columns in a
	data.frame?
Message-ID: <45e920ef0702282354q1d195ea4j5d98e2098a206aa3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070301/faea802b/attachment.pl 

From dacha.atienza at gmail.com  Thu Mar  1 09:38:57 2007
From: dacha.atienza at gmail.com (Dacha Atienza)
Date: Thu, 1 Mar 2007 09:38:57 +0100
Subject: [R] Help on GAM
In-Reply-To: <358012680702280721g27db0186l4919afb57784df9a@mail.gmail.com>
References: <358012680702280721g27db0186l4919afb57784df9a@mail.gmail.com>
Message-ID: <358012680703010038n1796084bs4abc32bf15722064@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070301/bd6b66de/attachment.pl 

From ted.harding at nessie.mcc.ac.uk  Thu Mar  1 10:01:34 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 01 Mar 2007 09:01:34 -0000 (GMT)
Subject: [R] random uniform sample of points on an ellipsoid (e.g. WG
In-Reply-To: <XFMail.070222141036.ted.harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.070228233314.efh@nessie.mcc.ac.uk>

Following up to Russell Senior's original query:

On 21 Feb 2007, Russell Senior wrote:
> I am interested in making a random sample from a uniform distribution
> of points over the surface of the earth, using the WGS84 ellipsoid as
> a model for the earth.  I know how to do this for a sphere, but would
> like to do better.  I can supply random numbers, want latitude
> longitude pairs out.
> 
> Can anyone point me at a solution?  Thanks very much.

As I wrote before:

 "For the application you have in hand, uniform distribution
  over a sphere is a fairly close approximation to uniform
  distriobution over the ellipspoid -- but not quite."

Well, it's "a bit closer than not quite".

I've looked up the WGS84 details, and the ellipsoid has

  major axis a = 6378137.000 metres
  minor axis b = 6356752.314 metres

so b/a = 0.9966471893

As I wrote before:

  "But a rejection method, applied to points uniform on the
   sphere, can give you points uniform on the ellipsoid and,
   because of the close approximation of the sphere to the
   ellipsoid, you would not be rejecting many points."


  "Consider a point X0 on the sphere, at radial distance r0
   from the centre of the sphere (same as the centre of the
   ellipsoid). Let the radius through that point meet the
   ellipsoid at a point X1, at radial distance R1.

   Let dS0 be an element of area at X0 on the sphere, which
   projects radially onto an element of area dS1 on the
   ellipsoid. You want all elements dS1 of equal size to be
   equally likely to receive a random point.

   Let the angle between the tangent plane to the ellipsoid
   at X1, and the tangent plane to the sphere at X0, be phi.

   The the ratio of areas dS1/dS0 is R(X0), say, where

    R(X0) = dS1/dS0 = r1^2/(r0^2 * cos(phi))

  and the smaller this ratio, the less likely you want a point
  u.d. on the sphere to give rise to a point on the ellipsoid.

  Now define an acceptance probability P(X0) by

    P(X0) = R(X0)/sup[R(X)]

  taking the supremum over X on the sphere. Then sample points
  X0 unformly on the sphere, rejecting each on with probability
  P(X0), and continue sampling until you have the number of
  points that you need."

Without loss of generality, take a = 1 and b as the value of
b'a above, namely

  b = 0.9966471893

Then the minimum value of r1^2/r0^2 is the square of this,
namely 0.99330562 -- this is only 0.67% less than 1.0!

Suppose, therefore, that the ellipsoid has a horizontal
major axis of length a=1, and vertical minor axis of length
b = 0.9966471893, with a circumscribing sphere with the same
centre and radius 1. Consider a planar section containing
the vertical axis. We then have an ellipsoid with horizontal
major axis of length 1 and vertical minor axis of length b.

Let (x,y) be a point of this ellisoid, where

  x = r0*cos(theta), y = r0*sin(theta)

  x^2 + y^2/b^2 = 1

The tangent to the ellipsoid at (x,y) has slope -b^2*cot(theta).

Project (x,y) radially out to the circle. The tangent to the
circle at this point has slope cot(theta).

Hence the angle phi (see above) between the tangent to the
ellipsoid and the tangent to the circle is the difference
between theta and atan(tan(theta)/b^2).

Finally, from the two equations above in (x,y),

  r0 <- 1/sqrt(cos(theta)^2 + sin(theta)^2/b^2)

Therefore we can find out the minimum value of the acceptance
probability above:

  R(X0) = dS1/dS0 = r1^2/(r0^2 * cos(phi))
  P(X0) = R(X0)/max(R(X))

So we can implement this is some simple R code:

  b <- 0.9966471893
  theta <- (pi/2)*0.001*(0:1000)
  r0 <- 1/sqrt(cos(theta)^2 + sin(theta)^2/b^2)

  phi <- theta - atan(tan(theta)/b^2)
  P <- 1/(r0^2 * cos(phi)) ; P<- P/max(P)

from which min(P) = 0.9933056 -- identical to (b/a)^2 above,
which is not surprising since it occurs where theta = pi/2.

Now, this acceptance probability can be applied to the ellipsoid
as well as to the ellipse, since it involves only the radii and
the angle between the tangent planes; and the the latter is the
same as the angle between the tangents to ellipse and circle.

The question therefore turns on how many points Russell wants
to sample on the ellipsoid of the Earth. If it is a hundred
or two, then there will little perceptible difference between
simply sampling on the sphere and projecting radially onto the
ellipsoid -- maybe 1 or 2 or 3 points should have been rejected,
and quite likely none should have been rejected.

The following code (I hope!) implements the above on the ellipsoid,
testing it for a sample of N=10000, and also prints out the numbers
of points not accepted in each pass.

## Uniform sampling on ellipsoid
b <- 0.9966471893
N <- 10000 ; 
N <- 400
N0 <- N; Accepted <- 0 ;
Lat0 <- numeric(0); Long0 <- numeric(0)
while(Accepted<N0){
  ## Uniform sample on surface of sphere
  x1   <- rnorm(N); x2<-rnorm(N); x3<-rnorm(N)
  requ <- sqrt(x1^2 + x2^2)
  Lat  <- atan2(x3,requ)
  Long <- atan2(x2,x1)
  ## Accept for uniform on ellipsoid
    R0  <- 1/sqrt(cos(Lat)^2 + (sin(Lat)^2)/b^2)
    Phi <- Lat - atan(tan(Lat)/b^2)
    P   <- 1/(R0^2 * abs(cos(Phi))) ; P<- P/max(P)
    Accept   <- (runif(N) <= P)
    Lat0     <- c(Lat0,Lat[Accept])
    Long0    <-c(Long0, Long[Accept])
    Accepted <- Accepted + sum(Accept)
  N <- N0 - Accepted
  print(N)  ## Number not accepted
  ## Another pass if sample size not complete
}

## For instance, on a first pass I got 42 rejected out of 10000,
## then 2 out of the 42, then none out of the final 2.

## Check-plots:
## Polar view:
plot(cos(Long0)*cos(Lat0),sin(Long0)*cos(Lat0),pch="+")
## Equatorial view:
plot(sin(Long0)*cos(Lat0),sin(Lat0),pch="+")
## (These should be similar)

The output of the above code is a set of (Lat0,Long0) points
on the sphere which [should] project radially onto uniformly
distributed points on the surface of the ellipsoid.

I have not gone into the conversion of these (Lat0,Long0)
coordinates on the sphere into latitude and Longitude on
the [WGS80] ellipsoid. This is because there are several
different possible definitions of Latitude (some more
esoteric than others ... ) which can differ by up to 12
minutes of arc. See for instance

  http://en.wikipedia.org/wiki/Latitude

the most "intuitive" ones being "Geographic Latitude" (angle
between equatorial plane and the normal to the reference
ellipsoid) and "Geocentric latitude" (angle between equatorial
plane and radius to point on surface of Earth).

Hoping that helps ...
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <efh at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 28-Feb-07                                       Time: 23:33:11
------------------------------ XFMail ------------------------------


From dacha.atienza at gmail.com  Thu Mar  1 10:01:51 2007
From: dacha.atienza at gmail.com (Dacha Atienza)
Date: Thu, 1 Mar 2007 10:01:51 +0100
Subject: [R] HELP GAM
Message-ID: <358012680703010101ja994c6dq2d914790ed53d6b5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070301/9cbe2270/attachment.pl 

From bcutayar at lfdj.com  Thu Mar  1 10:05:40 2007
From: bcutayar at lfdj.com (bcutayar at lfdj.com)
Date: Thu, 01 Mar 2007 10:05:40 +0100
Subject: [R] absent
Message-ID: <OF394ED73D.9B29C6E6-ONC1257291.0031F56A-C1257291.0031F56A@lfdj.com>




Je serai absent(e) ? partir du  01/03/2007 de retour le 05/03/2007.



>>>>>>>>>>>>>    Bonne ann?e 2007  <<<<<<<<<<


Si vous n'etes pas destinataires de ce message, merci d'aver...{{dropped}}


From anja.eggert at uni-rostock.de  Thu Mar  1 10:07:49 2007
From: anja.eggert at uni-rostock.de (Anja Eggert)
Date: Thu, 1 Mar 2007 10:07:49 +0100
Subject: [R] stl cycle sub-series plot
Message-ID: <45E697E5.3020105@biologie.uni-rostock.de>

Dear R-people,

I tried to build a seasonal cycle sub-series plot of my time series using:

monthplot(nitrat.stl, choice="seasonal")

However, I only get the horizontal lines of the mean values and not the 
vertical lines for each year. Can anybody help me?

Sincerely,
Anja

-- 
*************************************************************
Dr. Anja Eggert						
Universit?t Rostock
Institut f?r Biowissenschaften
AG Angewandte ?kologie

Albert-Einstein-Str. 3
18059 Rostock

T: ++49 381 498 6094
F: ++49 381 498 6072

e-mail: anja.eggert at uni-rostock.de


From anja.eggert at uni-rostock.de  Thu Mar  1 10:10:37 2007
From: anja.eggert at uni-rostock.de (Anja Eggert)
Date: Thu, 1 Mar 2007 10:10:37 +0100
Subject: [R] pheno package
Message-ID: <45E6988D.9070206@biologie.uni-rostock.de>

Dear R-people,

I tried to use a command seqMK(x) of the pheno package. However, the 
installation of the package fails with following error message:

Error in library(pkg, character.only = TRUE) :
        'pheno' is not a valid package -- installed < 2.0.0?

What is the matter? Can anybody help me? I installed the latest version 
of R.

Cheers,
Anja

-- 
*************************************************************
Dr. Anja Eggert						
Universit?t Rostock
Institut f?r Biowissenschaften
AG Angewandte ?kologie

Albert-Einstein-Str. 3
18059 Rostock

T: ++49 381 498 6094
F: ++49 381 498 6072

e-mail: anja.eggert at uni-rostock.de


From luwis.diya at med.kuleuven.be  Thu Mar  1 10:11:58 2007
From: luwis.diya at med.kuleuven.be (Luwis Diya)
Date: Thu, 1 Mar 2007 10:11:58 +0100
Subject: [R] Matrix Library failing to load
Message-ID: <00df01c75be1$aa364100$1140210a@www.domain>

Dear R users

I am trying to load the package lmer4 but it seems the Matrix library is 
failing to load and giving me the following error message :

>library(Matrix)
Error in importIntoEnv(impenv, impnames, ns, impvars) :
        object 'Logic' is not exported by 'namespace:methods'
Error: package/namespace load failed for 'Matrix'

I have even reinstalled the Matrix package but it seems the problem is not 
going away.

Regards,


Luwis Diya 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From bartjoosen at hotmail.com  Thu Mar  1 10:14:28 2007
From: bartjoosen at hotmail.com (Bart Joosen)
Date: Thu, 01 Mar 2007 09:14:28 +0000
Subject: [R] How to read in this data format?
Message-ID: <BAY134-F394F6FDE418B617D16A196D8800@phx.gbl>

Hi,

I recieved an ascii file, containing following information:

$$ Experiment Number:
$$ Associated Data:

FUNCTION 1

Scan		1
Retention Time	0.017

399.8112	184
399.8742	0
399.9372	152
....

Scan		2
Retention Time	0.021

399.8112	181
399.8742	1
399.9372	153
.....


I would like to import this data in R into a dataframe, where there is a 
column time, the first numbers as column names, and the second numbers as 
data in the dataframe:

Time	399.8112	399.8742	399.9372
0.017	184	0	152
0.021	181	1	153

I did take a look at the read.table, read.delim, scan, ... But I 've no idea 
about how to solve this problem.

Anyone?


Thanks

Bart


From ccleland at optonline.net  Thu Mar  1 10:40:29 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 01 Mar 2007 04:40:29 -0500
Subject: [R] how to apply the function cut( ) to many columns in
	a	data.frame?
In-Reply-To: <45e920ef0702282354q1d195ea4j5d98e2098a206aa3@mail.gmail.com>
References: <45e920ef0702282354q1d195ea4j5d98e2098a206aa3@mail.gmail.com>
Message-ID: <45E69F8D.2090602@optonline.net>

ahimsa campos-arceiz wrote:
> Dear useRs,
> 
> In a data.frame (df) I have several columns (x1, x2, x3....xn) containing
> data as a continuous numerical response:
> 
> df
>  var     x1    x2     x3
>   1    143   147   137
>   2      93    93   117
>   3    164    39   101
>   4    123   118    97
>   5     63   125     97
>   6    129    83   124
>   7    123    93   136
>   8    123    80     79
>   9     89   107   150
> 10     78    95    121
> 
> I want to classify the values in the columns x1, x2, etc, into bins of fix
> margins (0-5, 5-10, ....). For one vector I can do it easily with the
> function cut:
> 
>> df$x1 <- cut(df$x1, br=5*(0:40), labels=5*(1:40))
>> df$x1
>  [1] 145 95  165 125 65  130 125 125 90  80
> 40 Levels: 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 ...
> 200
> 
> However if I try to use a subset of my data.frame:
> 
> df[,3:4] <- cut(df[,3:4], br=5*(0:40), labels=5*(1:40))
> 
> Error in cut.default(df[, 3:4], br = 5 * (0:40), labels = 5 * (1:40)) :
>         'x' must be numeric
> 
> 
> How can I make this work with data frames in which I want to apply the
> function cut( ) to many columns in a data.frame?

  You have an answer within your question - use one of the various
"apply" functions.  For example:

lapply(df[,3:4], function(x){cut(x, br=5*(0:40), labels=5*(1:40))})

?lapply
?sapply
?apply

> I guess that I might have to use something like for ( ) (which I'm not
> familiar with), but maybe you know a straight forward method to use with
> data.frames.
> 
> 
> Thanks a lot!
> 
> Ahimsa
> 
> *********************************************
> 
> # data
> var <- 1:10
> x1 <- rnorm(10, mean=100, sd=25)
> x2 <- rnorm(10, mean=100, sd=25)
> x3 <- rnorm(10, mean=100, sd=25)
> df <- data.frame(var,x1,x2,x3)
> df
> 
> # classifying the values of the vector df$x1 into bins of width 5
> df$x1 <- cut(df$x1, br=5*(0:40), labels=5*(1:40))
> df$x1
> 
> # trying it a subset of the data.frame
> df[,3:4] <- cut(df[,3:4], br=5*(0:40), labels=5*(1:40))
> df[,3:4] 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From dray at biomserv.univ-lyon1.fr  Thu Mar  1 10:55:21 2007
From: dray at biomserv.univ-lyon1.fr (=?ISO-8859-1?Q?St=E9phane_Dray?=)
Date: Thu, 01 Mar 2007 10:55:21 +0100
Subject: [R] RDA and trend surface regression
In-Reply-To: <d4bc47d41f3a6171ff60fed5290e68ed@sun3.oulu.fi>
References: <71257D09F114DA4A8E134DEAC70F25D307A25406@groamrexm03.amer.pfizer.com>	<1172602508.19931.71.camel@gsimpson.geog.ucl.ac.uk>
	<d4bc47d41f3a6171ff60fed5290e68ed@sun3.oulu.fi>
Message-ID: <45E6A309.7010204@biomserv.univ-lyon1.fr>

Dear All,

PCNM and more general MEM can be found in the package spacemakeR
available here (http://biomserv.univ-lyon1.fr/~dray/). As said by Jari,
I do not want to put the package on CRAN. I have no no problems to
include it in another package (vegan or spdep). It has a vignette which
can be interesting for spdep.  However, I have some problems with this
vector approach for selection of variables in multivariate analysis.
spdep has some function to select these eigenvectors in univariate
regression, we have to find some rules for selecting in the multivariate
case. I use AIC for the paper but it is quite sure that it is not
correct (see comments of Jari in the vegan help page).
multispati is a spatially constrained multivariate analysis. I think
that this approach is more clean in a 'statistical sense' but quite
different: we do not have variance but autocorrelation and so negative
eigenvalues in multivariate analysis !

I have see that Roger has send an announce for a spatial-R meeting in
R-Geo List. I am interested to participate to the Bergen session (I love
rain ;-) ). It could be nice to speak about this subject in Bergen.


Cheers,




Jari Oksanen wrote:
> On 27 Feb 2007, at 20:55, Gavin Simpson wrote:
>
>   
>> On Tue, 2007-02-27 at 13:13 -0500, Kuhn, Max wrote:
>>     
>>> Helene,
>>>
>>> My point was only that RDA may fit a quadratic model for the terms
>>> specified in your model. The terms that you had specified were already
>>> higher order polynomials (some cubic). So a QDA classifier with the
>>> model terms that you specified my be a fifth order polynomial in the
>>> original data. I don't know the reference you cite or even the
>>> subject-matter specifics. I'm just a simple cave man (for you SNL 
>>> fans).
>>> But I do know that there are more reliable ways to get nonlinear
>>> classification boundaries than using x^5.
>>>       
>> I doubt that Helene is trying to do a classification - unless you
>> consider classification to mean that all rows/samples are in different
>> groups (i.e. n samples therefore n groups) - which is how RDA
>> (Redundancy Analysis) is used in ecology.
>>
>> You could take a look at multispati in package ade4 for a different way
>> to handle spatial constraints. There is also the principle coordinates
>> analysis of neighbour matrices (PCNM) method - not sure this is coded
>> anywhere in R yet though. Here are two references that may be useful:
>>
>>     
> St?phane Dray has R code for finding PCNM matrices. Google for his 
> name: it's not that common. I also have a copy of his function and can 
> send it if really needed, but it may be better to check Dray's page 
> first. St?phane Dray says think that not all functions need be in CRAN. 
> May be true, but I think it might help many people.
>
> There are at least three reasons why not use polynomial constraints in 
> RDA. Max Kuhn mentioned one: polynomials typically flip wildly at 
> margins (or they are unstable in more neutral speech). Second reason is 
> that they are almost impossible to interpret in ordination display. The 
> third reason is that RDA (or CCA) avoid some ordination artefacts 
> (curvature, horseshoe, arc etc.) just because the constraints are 
> linear: allowing them to be curved allows curved solutions. These 
> arguments are not necessarily valid if you only want to have variance 
> partitioning, or if you use polynomial conditions ("partial out" 
> polynomial effects in Canoco language). In that case it may make sense 
> to use quadratic (or polynomial) constraints or conditions.
>
> cheers, Jari Oksanen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>   


-- 
St?phane DRAY (dray at biomserv.univ-lyon1.fr )
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - Lyon I
43, Bd du 11 Novembre 1918, 69622 Villeurbanne Cedex, France
Tel: 33 4 72 43 27 57       Fax: 33 4 72 43 13 88
http://biomserv.univ-lyon1.fr/~dray/


From grahamleask at btopenworld.com  Thu Mar  1 11:09:00 2007
From: grahamleask at btopenworld.com (GRAHAM LEASK)
Date: Thu, 1 Mar 2007 10:09:00 +0000 (GMT)
Subject: [R] Reshape data
Message-ID: <887525.44067.qm@web86203.mail.ird.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070301/f4adf473/attachment.pl 

From samay.sar at gmail.com  Thu Mar  1 11:53:58 2007
From: samay.sar at gmail.com (d. sarthi maheshwari)
Date: Thu, 1 Mar 2007 16:23:58 +0530
Subject: [R] R (Input from Keyboard): How do I remove error?
Message-ID: <d4327f7e0703010253q580af7e0p7426215ff1895acc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070301/98445779/attachment.pl 

From Claudio.isella at ircc.it  Thu Mar  1 12:35:32 2007
From: Claudio.isella at ircc.it (Claudio Isella)
Date: Thu, 01 Mar 2007 12:35:32 +0100
Subject: [R] function with Multiple Output
Message-ID: <1172748932.29120.15.camel@OGC-FU-WorkStation>

Dear R user,


I have a simple question for you: I have created a global function that
evoke other subsidiary functions. when I run the global function I want
to store the outcomes of the of each subsidiary function into a
variables. an examples

my.fun=function(vector, index){
    a=fun.a(vector, index)
    b=fun.b(vector, index)
    }


if i use return() I can only retrive one single results. what can I do
to store the outcome of the two function in differnt enviroment
variables?


--
Claudio


From klaster at karlin.mff.cuni.cz  Thu Mar  1 12:29:35 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Thu, 01 Mar 2007 12:29:35 +0100
Subject: [R] How to read in this data format?
In-Reply-To: <BAY134-F394F6FDE418B617D16A196D8800@phx.gbl>
References: <BAY134-F394F6FDE418B617D16A196D8800@phx.gbl>
Message-ID: <45E6B91F.60306@karlin.mff.cuni.cz>

Well, not extremely elegant, but should work:
1) open your file in some ascii text editor, delete the rubbish at the 
beginning up to line Scan 1, and replace all spaces in names - e.g. make 
a mass replace of 'Retention Time' by let say 'RetentionTime'.

2) Use read.table(), matrix() and data.frame():
d <- read.table('yourfile')
dd <- matrix(as.numeric(t(d)[2,]),byrow=TRUE,nrow=HowManyScansYouHave)
dd <- data.frame(dd)
names(dd) <- d[[1]][1:HowManyObservationsYouHavePerScan]

Petr

Bart Joosen napsal(a):
> Hi,
> 
> I recieved an ascii file, containing following information:
> 
> $$ Experiment Number:
> $$ Associated Data:
> 
> FUNCTION 1
> 
> Scan		1
> Retention Time	0.017
> 
> 399.8112	184
> 399.8742	0
> 399.9372	152
> ....
> 
> Scan		2
> Retention Time	0.021
> 
> 399.8112	181
> 399.8742	1
> 399.9372	153
> .....
> 
> 
> I would like to import this data in R into a dataframe, where there is a 
> column time, the first numbers as column names, and the second numbers as 
> data in the dataframe:
> 
> Time	399.8112	399.8742	399.9372
> 0.017	184	0	152
> 0.021	181	1	153
> 
> I did take a look at the read.table, read.delim, scan, ... But I 've no idea 
> about how to solve this problem.
> 
> Anyone?
> 
> 
> Thanks
> 
> Bart
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From klaster at karlin.mff.cuni.cz  Thu Mar  1 12:34:17 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Thu, 01 Mar 2007 12:34:17 +0100
Subject: [R] R (Input from Keyboard): How do I remove error?
In-Reply-To: <d4327f7e0703010253q580af7e0p7426215ff1895acc@mail.gmail.com>
References: <d4327f7e0703010253q580af7e0p7426215ff1895acc@mail.gmail.com>
Message-ID: <45E6BA39.4020909@karlin.mff.cuni.cz>

?readline
Note that this gives you a character vector, so you might want to use 
as.numeric() after that.

Petr

d. sarthi maheshwari napsal(a):
> Hi,
> 
> I am trying to read some value from keyboard in one of my R program. I want
> to do manipulation on data based on this value. But I am facing a problem
> which is described below with the help of an example:
> 
> My code is similar to:
> 
> :
> :
> cat("\n","Enter value:: ","\n")
> y<-scan(n=1)
> a <- b*y
> cat("\n","Enter new value::","\n")
> y<-scan(n=1)
> :
> :
> 
> Now if I run the whole script then something like this happens:
> 
> :
> :
> cat("\n","Enter value::","\n")
> 
>  Enter value::
>> y<-scan(n=1)
> 1: a <- b*y
> Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,
> :
>         scan() expected 'a real', got 'a'
>> cat("\n","Enter new value::","\n")
> 
>  Enter new value::
>> y<-scan(n=1)
> 1: 2
> Read 1 item
> :
> :
> 
> Is there any solution to this problem or shall I execute my script in two
> parts:
> First part:
> 
> :
> :
> cat("\n","Enter value:: ","\n")
> y<-scan(n=1)
> 
> and second part:
> 
> a <- b*y
> cat("\n","Enter new value::","\n")
> y<-scan(n=1)
> :
> :
> 
> Any help will be greatly appreciated.

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From blatny at biomed.cas.cz  Thu Mar  1 13:47:29 2007
From: blatny at biomed.cas.cz (Radek Blatny)
Date: Thu, 1 Mar 2007 13:47:29 +0100
Subject: [R] object is not subsettable
Message-ID: <40CB4023-3489-40F3-883B-6567241DB690@biomed.cas.cz>

Dear colleagues,
I've just come across a problem with the following command which is a  
part of the "metaOverview.R" code file provided as an monography- 
accompanying file at
http://www.bioconductor.org/docs/mogr/metadata:

##################################
R>  hasChr <- eapply(GOTERM, function(x)
+              x[grep("chromosome", Term(x))])

Error in x[grep("chromosome", Term(x))] : object is not subsettable
##################################

I have run the command in the (PPC) Mac OS X R 2.4.1 and (AMD Ubuntu)  
Linux R 2.4.0 with the same result so it shouldn't be any  
distribution-dependent problem. Obviously the "metaOverview.R" is not  
up-to-date since I had few problems before as well (e.g. that a  
function is in another package in BioC 1.9 etc.) but I was able to  
"repair" everything myself. However, this one I don't understand.  
Anyone can help? Some classes have changed?!

Best regards, Radek Blatny

Radek Blatny, MSc.
Institute of Molecular Genetics
Department of Mouse Molecular Genetics (Jiri Forejt unit)
Czech Academy of Sciences
Videnska 1083
142 20, Prague
Czech Republic
Tel. (+420) 241 062 260
Fax (+420) 241 062 154
http://www.img.cas.cz/mmg
email: blatny at biomed.cas.cz
Skype name: blatny


From s.wood at bath.ac.uk  Thu Mar  1 13:51:16 2007
From: s.wood at bath.ac.uk (Simon Wood)
Date: Thu, 1 Mar 2007 12:51:16 +0000
Subject: [R] interactions and GAM
In-Reply-To: <182D8C7FAD4DCE499BF5AD749B3AA06404C233@hermes.bordeaux.cemagref.fr>
References: <182D8C7FAD4DCE499BF5AD749B3AA06404C233@hermes.bordeaux.cemagref.fr>
Message-ID: <200703011251.16336.s.wood@bath.ac.uk>

Don't know if this helps, but... 
gam in package mgcv will let you set up smooths that interact with factors 
using the `by' variable mechanism. See ?gam.models, particularly the last 
example. Prediction is supported.

Simon

-- 
> Simon Wood, Mathematical Sciences, University of Bath, Bath, BA2 7AY UK
> +44 1225 386603  www.maths.bath.ac.uk/~sw283 

On Tuesday 27 February 2007 20:10, Beaulaton Laurent wrote:
> Dear R-users,
>
> I have 1 remark and 1 question on the inclusion of interactions in the gam
> function from the gam package.
>
> I need to fit quantitative predictors in interactions with factors. You can
> see an example of what I need in fig 9.13 p265  from Hastie and Tibshirani
> book (1990). It's clearly stated that in ?gam  "Interactions with
> nonparametric smooth terms are not fully supported". I have found a trick
> in a former http://www.math.yorku.ca/Who/Faculty/Monette/S-news/2284.html,
> using NAs and na.gam.replace argument, but some points are still unclear
> for me.
>
> First the prediction of new data (using predict function) is not so easy
> (see script below), and need a close reading from section 7.3.2 of the
> Chambers and Hastie (1992).
>
> Second I need to have the same intercept for all levels of factor and this
> not achievable with this trick. My question is : why not replacing NA by 0
> (or any other particular value) ?
>
> Here is a quite long (sorry for that) script with a generated dataset to
> better undestand my question. in this script the model to fit is (in a
> GLM-like writing) : y~s(x2):x1 the generated dataset follows this model and
> y(x2=0)=10 whatever x1.
>
> ########################
> #start of script
> ########################
>
> #data construction  (with deliberately very small noise)
> data1=data.frame(x1=rep(NA,27),x2=NA,y=NA)
>
> data1$x1=factor(c(rep(1,11),rep(2,11),rep(3,5)))
> data1$x2=c(rep(0:10,2),0:4)
>
> data1[data1$x1==1,"y"]=data1[data1$x1==1,"x2"]^4*5+rnorm(11)+10000
> data1[data1$x1==2,"y"]=data1[data1$x1==2,"x2"]^4*(-3)+rnorm(11)+10000
> data1[data1$x1==3,"y"]=10000*data1[data1$x1==3,"x2"]+rnorm(5)+10000
>
> library(lattice)
> xyplot(data1$y~data1$x2,groups=data1$x1)
>
> #creation of dummy variables for interactions
> data1$x2_1=ifelse(data1$x1=="1",data1$x2,NA)
> data1$x2_2=ifelse(data1$x1=="2",data1$x2,NA)
> data1$x2_3=ifelse(data1$x1=="3",data1$x2,NA)
>
> #model fitting
> library(gam)
> model=gam(y~s(x2_1)+s(x2_2)+s(x2_3)+x1,data=data1,na=na.gam.replace)
>
> #prediction fit well data :
> summary(model)
> plot(data1$x2,data1$y)
> points(data1$x2,model$fitted.value,col="red",pch="+")
>
> #trying to see prediction:
> predict(model) #does work
> predict(model,newdata=data1) #produce NA
>
> #trying to replace NA in data1 by mean, to mimic na.gam.replace:
> Ndata=data1
> Ndata$x2_1=ifelse(data1$x1=="1",data1$x2,mean(data1$x2_1,na.rm=TRUE))
> Ndata$x2_2=ifelse(data1$x1=="2",data1$x2,mean(data1$x2_2,na.rm=TRUE))
> Ndata$x2_3=ifelse(data1$x1=="3",data1$x2,mean(data1$x2_3,na.rm=TRUE))
>
> predict(model,Ndata)-predict(model) #as you can see there is a systematic
> biais
>
> #correct way to predict (=returned 0 for terms with NA value):
> p=predict(model,data1,type="term")
> rowSums(cbind(p,attr(p,"constant")),na.rm=TRUE)-predict(model)
>
> #alternative solution, 0 instead of NA
> data1$v1=ifelse(data1$x1=="1",data1$x2,0)
> data1$v2=ifelse(data1$x1=="2",data1$x2,0)
> data1$v3=ifelse(data1$x1=="3",data1$x2,0)
>
> model1=gam(y~s(v1)+s(v2)+s(v3),data=data1)
> summary(model1)
> points(data1$x2,predict(model1,data1),col="green",pch="X")
> #no particular problem with predict function
>
> #what's happened in x2=0 ?
> predict(model)[data1$x2==0]
> predict(model1)[data1$x2==0]
>
> ########################
> #end of script
> ########################
>
> thanks in advance
> best regards
> Laurent Beaulaton
>
> ---------------------------------------------
> Laurent Beaulaton
> ###############################
> # NEW !!!!                                       #
> #  http://www.laurent-beaulaton.fr/    #
> # Tel + 33 (0)5 57 89 27 17             #
> ###############################
> ---------------------------------------------
> Cemagref (French Institute of Agricultural and Environmental Engineering
> Research ) Unit? "Ecosyst?mes estuariens et poissons migrateurs
> amphihalins"
> (anciennement Unit? "Ressources aquatiques continentales")
> 50 avenue de Verdun
> F 33612 Cestas Cedex
>
> Tel + 33 (0)5 57 89 27 17
> Fax + 33 (0)5 57 89 08 01
> mailto:laurent.beaulaton at bordeaux.cemagref.fr
>
> http://www.laurent-beaulaton.fr/
> http://www.bordeaux.cemagref.fr/rabx/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.


From marc at rohling-online.de  Thu Mar  1 14:56:25 2007
From: marc at rohling-online.de (Marc A. Rohling)
Date: Thu, 01 Mar 2007 14:56:25 +0100
Subject: [R] barplot2, gap.barplot
Message-ID: <1172757385.5102.42.camel@stryder.skrupellos.priv>

Hello,

I try to handle a simple bar-plot, but it turns out to be not as simple
as I thought.

1) I have created a .dat-File, e.g. test.dat:

DATA	DATA-SEM
2.2	0.32 
6.2	1.30
12.7	1.61
48.6	3.08
4.1	0.86
4.5	0.32
1.5	1.13
1.2	1.08

The first row is the data represented by bars. The second row deals with
the Standard Error of Mean. The lines correspond to time-intervals of
experiments. 

2) I now wrote this R-Script:
#=====BEGIN=====
# example.R

library(gdata)
library(gtools)
library(gmodels)
library(gplots)
library(plotrix)

data <- matrix(scan("./plot/example.dat", skip=1), ncol=2, nrow=8,
byrow=TRUE, 
dimnames=list(c("1", "2", "3", "4", "5", "6", "7", "8"),c("DATA",
"DATA-SEM")))

conf_l <- data[, 1]	   		
conf_u <- data[, 1] + data[, 2]	

op <- par(no.readonly = TRUE)		

par(lab=c(8,10,7))

barplot2(
height=data[, 1],		
width=1,			
space=1,			
col='black',
border='black',
angle=0,			
density=NULL,		
ylim=c(-2,55),			
xpd=FALSE,			
axes=TRUE,			
las=1,
ci.u=conf_u,			
ci.l=conf_l,
plot.ci=TRUE,
ci.color="black",
ci.lty="solid",
ci.lwd=1,
horiz=FALSE,
main="Header",
ylab="",
xlab="\nduration of treatment",
plot.grid=TRUE)

par(op)

#====END=====

As you can see, because of the 4th bar (value > 45), the other bars look
a little bit tiny: there is too much white-space. What I need to handle
this problem is a function to insert a gap.

I tried to use gap.barplot, but unfortunately, it cannot handle any of
the parameters I need from the barplot2-function.

After days of missing effort, I am sick of this problem. Is there a
solution?

Thanks for your help,

Marc


From s.wood at bath.ac.uk  Thu Mar  1 13:56:35 2007
From: s.wood at bath.ac.uk (Simon Wood)
Date: Thu, 1 Mar 2007 12:56:35 +0000
Subject: [R] Help on GAM
In-Reply-To: <358012680703010038n1796084bs4abc32bf15722064@mail.gmail.com>
References: <358012680702280721g27db0186l4919afb57784df9a@mail.gmail.com>
	<358012680703010038n1796084bs4abc32bf15722064@mail.gmail.com>
Message-ID: <200703011256.35264.s.wood@bath.ac.uk>

On Thursday 01 March 2007 08:38, Dacha Atienza wrote:
> I want to do this model using mgcv
>
> caco1.gam<-gam(caco1$Pa~s(caco1$T10)+s(caco1$S10)+s(caco1$C10), data=caco1)
> But I want to indicate the degrees of freedom of each term, how I have to
> do it?
e.g. 
s(caco1$T10,k=6,fx=TRUE)
will give you a 5 DoF term. (Alternatively you can supply smoothing parameters 
directly to gam, which ultimately control the termwise DoFs.) 

>
> Then, I saw that always the results refered to positive interactions, there
> are not negative ones?
- I don't understand this question. If you specifiy an interaction with 
something like te(x,z) then the effect of the response could be positive or 
negativedepending on the values of x and z.

Simon

-- 
> Simon Wood, Mathematical Sciences, University of Bath, Bath, BA2 7AY UK
> +44 1225 386603  www.maths.bath.ac.uk/~sw283


From jrkrideau at yahoo.ca  Thu Mar  1 14:34:37 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 1 Mar 2007 08:34:37 -0500 (EST)
Subject: [R] legend question
In-Reply-To: <200702281652.l1SGq5tv003033@msslhb.mssl.ucl.ac.uk>
Message-ID: <20070301133437.53311.qmail@web32801.mail.mud.yahoo.com>

? par 
it is the xpd you're looking for.

x <- seq(-pi, pi, len = 65)
par(xpd=TRUE)
plot(x, sin(x), type="l", col = 2,xpd=NA)
legend(x = 0, y = -1.5, "legend text", pch = 1, xjust
= 0.5)



--- Jenny Barnes <jmb at mssl.ucl.ac.uk> wrote:

> Hi folks,
> 
> Do you mind if I ask a related question that I have
> been having trouble with - 
> how do you put the legend outside of the plot area
> (to the bottom of the area - 
> below the x-axis title)? Could anybody show me using
> the example given below:
> 
> x <- seq(-pi, pi, len = 65)
> plot(x, sin(x), type="l", col = 2)
> legend(x = -3, y = .9, "legend text", pch = 1, xjust
> = 0.5)
> 
> Thank you, I've not been able to do this simple bit
> of programming and it is 
> very frustrating not to be able to add a simple key.
> 
> Best Wishes,
> 
> Jenny
> 
> Hi Emili,
> 
> Even though you are calling your horizontal
> coordinate y, and vertical
> coordinate z, the first and second arguments to
> legend(), namely x and y,
> should be the horizontal and vertical coordinates,
> respectively; and they are
> given in user coordinates (e.g., legend()'s x should
> be between 1960 and 1975
> and legend()'s y should be between 1 and 4).
> 
> If you want to use normalized coordinates (i.e. 0 to
> 1), you can scale as in
> this example:
> 
> legend(x = par("usr")[1] +
> diff(par("usr")[1:2])*normalizedCoordX,
>        y = par("usr")[3] +
> diff(par("usr")[3:4])*normalizedCoordY,
>        ...)
> 
> where normalizedCoordX and Y go from 0 to 1 (see
> ?par, par("usr") returns
> vector of c(xmin,xmax,ymin,ymax) of user coordinates
> on a plot)
> 
> You can alternatively use legend(x = "topleft",...)
> or "bottomright", and so
> on to place your legend.
> 
> If you want to add your legend outside of the plot,
> you should consider
> increasing the margins using the 'mar' argument in
> par(), and also setting
> par(xpd=TRUE) (so stuff can show up outside of the
> plotting region).
> 
> Best regards,
> ST
> 
> 
> > y<-c(1960, 1965, 1970, 1975)
> > z<-c(1, 2, 3, 4)
> within the data limits of your x and y)
> 
> 
> 
> 
> 
> --- Emili Tortosa-Ausina <Emili.Tortosa at eco.uji.es>
> wrote:
> 
> > Hi to all,
> > 
> > I'm sorry for posting this question, I am sure I
> am missing something 
> > important but after reading the documentation I
> cannot find where the 
> > problem is.
> > 
> > I want to add a legend to a figure. If I use a
> simple example drawn 
> > from the R Reference Manual such as, for instance:
> > 
> > x <- seq(-pi, pi, len = 65)
> > plot(x, sin(x), type="l", col = 2)
> > legend(x = -3, y = .9, "legend text", pch = 1,
> xjust = 0.5)
> > 
> > then everything works just fine.
> > 
> > However, if I use other data such as, for
> instance:
> > 
> > y<-c(1960, 1965, 1970, 1975)
> > z<-c(1, 2, 3, 4)
> > plot(y, z, type="l", col = 2)
> > legend(x = -3, y = .9, "legend text", pch = 1,
> xjust = 0.5)
> > 
> > then the legend is not shown.
> > 
> > Any hints?
> > 
> > Thanks in advance,
> > 
> > Emili
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> > 
> 
> 
> 
>  
>
________________________________________________________________________________
> ____
> Food fight? Enjoy some healthy debate
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
> 
> ------------- End Forwarded Message -------------
> 
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Jennifer Barnes
> PhD student: long range drought prediction 
> Climate Extremes Group
> Department of Space and Climate Physics
> University College London
> Holmbury St Mary 
> Dorking, Surrey, RH5 6NT
> Tel: 01483 204149
> Mob: 07916 139187
> Web: http://climate.mssl.ucl.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From bunny at lautloscrew.com  Thu Mar  1 14:46:21 2007
From: bunny at lautloscrew.com (bunny , lautloscrew.com)
Date: Thu, 1 Mar 2007 14:46:21 +0100
Subject: [R] need indexing help again
Message-ID: <772B6EAB-B9D3-4DEC-98F1-F50916360C64@lautloscrew.com>

Dear all,


i have a problem which deals with indexing again, i guess.
I am a beginner and though i read the S Poetry stuff it is not that  
easy for me
to use the examples for my problems, maybe all this needs some  
practice...

Here?s the question. As i want to perform a factor analysis i need  
the same number of rows for every column to get a matrix that i use  
as an argument of factanal.
I?d like to find out which of 5 question i asked explain the variance  
in my dataset.
People were able to choose "dont know".
Of course someone who didn?t know, could know the answers for the  
other question.

To put in a nutshell,  only those results are valid for my factanal  
in which every question is answered (which is ok, as it is that way  
most of the time).
i have matrices  question1, question2 etc which contain the answers  
to each question.
Every person has a result id.

There fore my idea is to put all the matrices into one big matrices  
and then just take those of out the matrices, where the resultid  
occurs 5 times - as i have 5 questions.


And here are my two questions :

  a) is there some syntax that  helps me with:
if number of any resultid appearance in column 1 is is smaller than  
5, dont put it into my new matrix ?

b) is the idea in general wrong ? is there an easier way to get the  
matrices into one.. prepared for a factanal ?


thx in advance!

matthias


From heberto.ghezzo at mcgill.ca  Thu Mar  1 14:43:39 2007
From: heberto.ghezzo at mcgill.ca (R Heberto Ghezzo, Dr)
Date: Thu, 1 Mar 2007 08:43:39 -0500
Subject: [R] random uniform sample of points on an ellipsoid (e.g. WG
References: <XFMail.070228233314.efh@nessie.mcc.ac.uk>
Message-ID: <05BE78B0CF1BBC4BBA4AA255568D8611029A99AD@EXCHANGE2VS1.campus.mcgill.ca>


Hello,
I do not know if I am completely out of it but . . .
if x,y,z is a point in a sphere and [u,v,w]'A[u,v,w] = 1 is the equation of an ellipsoid and A = T'T (cholesky) then
T.[x,y,z] should be a point in the ellipsoid ? isn't it?
Heberto Ghezzo
McGill University
Montreal Canada


From marc_schwartz at comcast.net  Thu Mar  1 14:58:34 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 01 Mar 2007 07:58:34 -0600
Subject: [R] barplot2, gap.barplot
In-Reply-To: <1172757385.5102.42.camel@stryder.skrupellos.priv>
References: <1172757385.5102.42.camel@stryder.skrupellos.priv>
Message-ID: <1172757514.4897.42.camel@localhost.localdomain>

On Thu, 2007-03-01 at 14:56 +0100, Marc A. Rohling wrote:
> Hello,
> 
> I try to handle a simple bar-plot, but it turns out to be not as simple
> as I thought.
> 
> 1) I have created a .dat-File, e.g. test.dat:
> 
> DATA	DATA-SEM
> 2.2	0.32 
> 6.2	1.30
> 12.7	1.61
> 48.6	3.08
> 4.1	0.86
> 4.5	0.32
> 1.5	1.13
> 1.2	1.08
> 
> The first row is the data represented by bars. The second row deals with
> the Standard Error of Mean. The lines correspond to time-intervals of
> experiments. 
> 
> 2) I now wrote this R-Script:
> #=====BEGIN=====
> # example.R
> 
> library(gdata)
> library(gtools)
> library(gmodels)
> library(gplots)
> library(plotrix)
> 
> data <- matrix(scan("./plot/example.dat", skip=1), ncol=2, nrow=8,
> byrow=TRUE, 
> dimnames=list(c("1", "2", "3", "4", "5", "6", "7", "8"),c("DATA",
> "DATA-SEM")))
> 
> conf_l <- data[, 1]	   		
> conf_u <- data[, 1] + data[, 2]	
> 
> op <- par(no.readonly = TRUE)		
> 
> par(lab=c(8,10,7))
> 
> barplot2(
> height=data[, 1],		
> width=1,			
> space=1,			
> col='black',
> border='black',
> angle=0,			
> density=NULL,		
> ylim=c(-2,55),			
> xpd=FALSE,			
> axes=TRUE,			
> las=1,
> ci.u=conf_u,			
> ci.l=conf_l,
> plot.ci=TRUE,
> ci.color="black",
> ci.lty="solid",
> ci.lwd=1,
> horiz=FALSE,
> main="Header",
> ylab="",
> xlab="\nduration of treatment",
> plot.grid=TRUE)
> 
> par(op)
> 
> #====END=====
> 
> As you can see, because of the 4th bar (value > 45), the other bars look
> a little bit tiny: there is too much white-space. What I need to handle
> this problem is a function to insert a gap.
> 
> I tried to use gap.barplot, but unfortunately, it cannot handle any of
> the parameters I need from the barplot2-function.
> 
> After days of missing effort, I am sick of this problem. Is there a
> solution?
> 
> Thanks for your help,
> 
> Marc

Marc,

A few comments:

1. I am not a big fan of axis gaps, as they tend to alter the perception
of the differences in the values. You will find similar comments in
books by Cleveland et al on statistical graphs.

2. For continuous data, I would similarly argue against using barplots
and consider a regular point plot with error bars. This can be done
easily with the combination of plot() and arrows() in base R graphics or
plotCI() in 'gplots'.

3. Depending on the nature of your data, if the extreme value is
representative of an important marked difference relative to the other
values, then I don't particularly find the 'look' of the plot to be
overly problematic. It does appropriately emphasize the large
difference.

On the other hand, you might want to consider using a log scale on the y
axis as an alternative to an axis gap. This would be a reasonable
approach to plotting values that have a notable difference in range.  If
you do this, note that you would need to ensure that all y values are >0
(ie. y axis range minimum, lower bounds of CI's, etc.) since:

> log10(0)
[1] -Inf


I don't have the plotrix package installed, but the docs for it indicate
that the gap.barplot() function does not return any values, such as the
bar midpoints, which is the case for barplot() and barplot2(). This
would suggest that Jim did not anticipate the need to add additional
plot components to the bars.

Lacking this, you have have to review the function source code to
ascertain how Jim is drawing the bars (presumably using rect() ) and
figure out the x axis values for the bar midpoints so that you could
then add the CI's. Of course, you would have to consider the axis gaps
here as well, making it a bit more cumbersome. As I note above, however,
I would advise against this approach.

HTH,

Marc Schwartz


From albmont at centroin.com.br  Thu Mar  1 15:17:07 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Thu, 1 Mar 2007 12:17:07 -0200
Subject: [R] random uniform sample of points on an ellipsoid (e.g. WG
In-Reply-To: <05BE78B0CF1BBC4BBA4AA255568D8611029A99AD@EXCHANGE2VS1.campus.mcgill.ca>
References: <XFMail.070228233314.efh@nessie.mcc.ac.uk>
	<05BE78B0CF1BBC4BBA4AA255568D8611029A99AD@EXCHANGE2VS1.campus.mcgill.ca>
Message-ID: <20070301140243.M44898@centroin.com.br>

R Heberto Ghezzo, Dr wrote:
>
> I do not know if I am completely out of it but . . .
> if x,y,z is a point in a sphere and [u,v,w]'A[u,v,w] = 1 is the 
> equation of an ellipsoid and A = T'T (cholesky) then
> T.[x,y,z] should be a point in the ellipsoid ? isn't it? 
>
Yes, it's a point _on_ the ellipsoid, but it's not "uniformly"
distributed over the ellipsoid. The easy part is generating 
points on the ellipsoid, the hard part is generating them
uniformly.

For example, imagine a very flat ellipsoid, so flat that it's
almost a disk. Then A is something like the diagonal matrix
diag(c(1, 1, 0.0001)), T (let's call it Chol) is diag(c(1,1,0.01)), 
and a plot of Chol.[x,y,z] will look like this:

v <- cbind(rnorm(1000), rnorm(1000), rnorm(1000))
# there may be a better way to write the expression below:
v <- v / sqrt(v[,1]^2 + v[,2]^2 + v[,3]^2)
# now v is uniformly distributed over the sphere
Chol <- diag(c(1, 1, 0.01))
ep <- v %*% t(Chol)
plot(ep[,1], ep[,2])

with a clear trend to generate points closer to the equator
then in the polar regions, against the assumption that they
should be uniformly distributed over the surface of the
ellipsoid.

Alberto Monteiro


From ted.harding at nessie.mcc.ac.uk  Thu Mar  1 15:17:02 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 01 Mar 2007 14:17:02 -0000 (GMT)
Subject: [R] random uniform sample of points on an ellipsoid (e.g. WG
In-Reply-To: <05BE78B0CF1BBC4BBA4AA255568D8611029A99AD@EXCHANGE2VS1.campus.mcgill.ca>
Message-ID: <XFMail.070301141702.ted.harding@nessie.mcc.ac.uk>

On 01-Mar-07 R Heberto Ghezzo, Dr wrote:
> 
> Hello,
> I do not know if I am completely out of it but . . .
> if x,y,z is a point in a sphere and [u,v,w]'A[u,v,w] = 1 is the
> equation of an ellipsoid and A = T'T (cholesky) then
> T.[x,y,z] should be a point in the ellipsoid ? isn't it?
> Heberto Ghezzo
> McGill University
> Montreal Canada

Yes, but the issue in this thread is to generate a sample of
points uniformly distributed over the surface of the ellipsoid.

It is easy to get points uniformly distributed over the surface
of a sphere; but if your transformation (or equivalent) is
applied to these points, the resulting points on the surface
of the ellipsoid are not uniformly distributed!

For example, if the transformation squashes the sphere vertically
(so that its vertical axis is shorter than its horizontal axis),
then the resulting points near the "equator" of the ellipsoid will
have a higher density per unit area than the points near the "poles".

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 01-Mar-07                                       Time: 14:16:59
------------------------------ XFMail ------------------------------


From bunny at lautloscrew.com  Thu Mar  1 15:20:19 2007
From: bunny at lautloscrew.com (bunny , lautloscrew.com)
Date: Thu, 1 Mar 2007 15:20:19 +0100
Subject: [R] count the # of appearances...
Message-ID: <1FE90BC0-6C42-44CE-A890-DC4FDF951919@lautloscrew.com>

Hi there,

is there a possibility to count the number of appearances of an  
element in a vector ?
i mean of any given element.. deliver all elements which are exactly  
xtimes in this vector ?

thx in advance !!


From Chaofeng.Kou at postgrad.manchester.ac.uk  Thu Mar  1 15:19:02 2007
From: Chaofeng.Kou at postgrad.manchester.ac.uk (Chaofeng Kou)
Date: Thu,  1 Mar 2007 14:19:02 +0000
Subject: [R] question about returning Random Effects' covariance
	matrix	estimate using lme fitting
Message-ID: <20070301141902.r4fdz35x9cgwowkc@webmail.manchester.ac.uk>

Dear all

I am fitting and analyzing linear mixed-effects models using the
R command 'lme'. The following is the results:

dental.fit <- lme(fixed = distance~age, random = ~age + cluster
= ~subject, data = dental)

> summary(dental.fit)

Variance/Covariance Components Estimates:
                 Standard Deviation(s) of Random Effect(s)
                            (Intercept)        age
                            2.134464     0.1541247
                 Correlation of Random Effects
                                 (Intercept)
                        age    -0.6024329
                 Cluster Residual Variance: 1.716232

Fixed Effects Estimates:
                           Value       Approx. Std.Error    z ratio(C)
          (Intercept)     16.3406250     0.98005731       16.6731321
               age         0.7843750      0.08275189    9.4786353
               sex         1.0321023      1.53545472     0.6721802
              age:sex   -0.3048295      0.12964730      -2.3512218
    Conditional Correlations of Fixed Effects Estimates
               (Intercept)    age          sex
     age       -0.8801554
     sex       -0.6382847   0.5617897
     age:sex 0.5617897   -0.6382847    -0.8801554
I have known that using command 'dental.fit$varFix' I can obtain
the conditional covariance matrix of the fixed effects.
My question is how I can return the covariance matrix estimate of
the random effects. I tried many commands such as 'dental.fit$varRan',
'dental.fit$var.Ran', but they didn't work.

Thanks very much!

Chaofeng


From maitra at iastate.edu  Thu Mar  1 15:27:56 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Thu, 1 Mar 2007 08:27:56 -0600
Subject: [R] count the # of appearances...
In-Reply-To: <1FE90BC0-6C42-44CE-A890-DC4FDF951919@lautloscrew.com>
References: <1FE90BC0-6C42-44CE-A890-DC4FDF951919@lautloscrew.com>
Message-ID: <20070301082756.5a1ddad8@triveni.stat.iastate.edu>

Hello Bunny/Lautioscrew,

sum(your vector == your chosen element) should do what you want...

HTH,
Ranjan

On Thu, 1 Mar 2007 15:20:19 +0100 "bunny , lautloscrew.com" <bunny at lautloscrew.com> wrote:

> Hi there,
> 
> is there a possibility to count the number of appearances of an  
> element in a vector ?
> i mean of any given element.. deliver all elements which are exactly  
> xtimes in this vector ?
> 
> thx in advance !!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dimitris.rizopoulos at med.kuleuven.be  Thu Mar  1 15:28:46 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 1 Mar 2007 15:28:46 +0100
Subject: [R] count the # of appearances...
References: <1FE90BC0-6C42-44CE-A890-DC4FDF951919@lautloscrew.com>
Message-ID: <007201c75c0d$ec275780$0540210a@www.domain>

?table

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "bunny , lautloscrew.com" <bunny at lautloscrew.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, March 01, 2007 3:20 PM
Subject: [R] count the # of appearances...


> Hi there,
>
> is there a possibility to count the number of appearances of an
> element in a vector ?
> i mean of any given element.. deliver all elements which are exactly
> xtimes in this vector ?
>
> thx in advance !!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From huber at ebi.ac.uk  Thu Mar  1 15:29:12 2007
From: huber at ebi.ac.uk (Wolfgang Huber)
Date: Thu, 01 Mar 2007 15:29:12 +0100
Subject: [R] count the # of appearances...
In-Reply-To: <1FE90BC0-6C42-44CE-A890-DC4FDF951919@lautloscrew.com>
References: <1FE90BC0-6C42-44CE-A890-DC4FDF951919@lautloscrew.com>
Message-ID: <45E6E338.7060400@ebi.ac.uk>

Dear Bunny,

? table

might be what you wish.

  Best wishes
  Wolfgang Huber
  EBI

bunny , lautloscrew.com wrote:
> Hi there,
> 
> is there a possibility to count the number of appearances of an  
> element in a vector ?
> i mean of any given element.. deliver all elements which are exactly  
> xtimes in this vector ?
> 
> thx in advance !!


From matthew_wiener at merck.com  Thu Mar  1 15:30:16 2007
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Thu, 1 Mar 2007 09:30:16 -0500
Subject: [R] count the # of appearances...  [Broadcast]
In-Reply-To: <1FE90BC0-6C42-44CE-A890-DC4FDF951919@lautloscrew.com>
References: <1FE90BC0-6C42-44CE-A890-DC4FDF951919@lautloscrew.com>
Message-ID: <4E9A692D8755DF478B56A2892388EE1F018E65FC@usctmx1118.merck.com>

Take a look at "table".
Hope this helps,
Matt 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bunny ,
lautloscrew.com
Sent: Thursday, March 01, 2007 9:20 AM
To: r-help at stat.math.ethz.ch
Subject: [R] count the # of appearances... [Broadcast]

Hi there,

is there a possibility to count the number of appearances of an  
element in a vector ?
i mean of any given element.. deliver all elements which are exactly  
xtimes in this vector ?

thx in advance !!

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.




------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From fisk at bowdoin.edu  Thu Mar  1 15:31:35 2007
From: fisk at bowdoin.edu (steve)
Date: Thu, 01 Mar 2007 09:31:35 -0500
Subject: [R] question about xtable and Hmisc
Message-ID: <es6o3t$bhu$1@sea.gmane.org>

I would like to get rid of the row numbers using xtable and latex.
The commands

  d = cbind(1:10,rep(1:2,5))
  ans = xtable(d)
  latex(ans)

gives output containing

\begin{tabular}{rrr}
   \hline
  & 1 & 2 \\
   \hline
1 & 1.00 & 1.00 \\
   2 & 2.00 & 2.00 \\
   3 & 3.00 & 1.00 \\
   4 & 4.00 & 2.00 \\
   5 & 5.00 & 1.00 \\
   6 & 6.00 & 2.00 \\
   7 & 7.00 & 1.00 \\
   8 & 8.00 & 2.00 \\
   9 & 9.00 & 1.00 \\
   10 & 10.00 & 2.00 \\
    \hline
\end{tabular}

but I don't want the row numbers. Is it possible to get rid of them?


Also, if x is a data frame, latex(x) contains the row numbers.
Can I get rid of them here as well?

Steve


From marc_schwartz at comcast.net  Thu Mar  1 15:32:54 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 01 Mar 2007 08:32:54 -0600
Subject: [R] count the # of appearances...
In-Reply-To: <1FE90BC0-6C42-44CE-A890-DC4FDF951919@lautloscrew.com>
References: <1FE90BC0-6C42-44CE-A890-DC4FDF951919@lautloscrew.com>
Message-ID: <1172759575.4897.49.camel@localhost.localdomain>

On Thu, 2007-03-01 at 15:20 +0100, bunny , lautloscrew.com wrote:
> Hi there,
> 
> is there a possibility to count the number of appearances of an  
> element in a vector ?
> i mean of any given element.. deliver all elements which are exactly  
> xtimes in this vector ?
> 
> thx in advance !!

Vec <- sample(20, replace = TRUE)

> Vec
 [1] 16  4 16  1 13 13  7 14 20 18 18 18  6  1  8  5 12  8  5  2

# See ?table
> table(Vec)
Vec
 1  2  4  5  6  7  8 12 13 14 16 18 20 
 2  1  1  2  1  1  2  1  2  1  2  3  1


# Get the elements that appear 3 times

Vec.tab <- table(Vec)

> Vec.tab[Vec.tab == 3]
18 
 3 


HTH,

Marc Schwartz


From ccleland at optonline.net  Thu Mar  1 15:43:09 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 01 Mar 2007 09:43:09 -0500
Subject: [R] count the # of appearances...
In-Reply-To: <1FE90BC0-6C42-44CE-A890-DC4FDF951919@lautloscrew.com>
References: <1FE90BC0-6C42-44CE-A890-DC4FDF951919@lautloscrew.com>
Message-ID: <45E6E67D.3020109@optonline.net>

bunny , lautloscrew.com wrote:
> Hi there,
> 
> is there a possibility to count the number of appearances of an  
> element in a vector ?
> i mean of any given element.. deliver all elements which are exactly  
> xtimes in this vector ?
> 
> thx in advance !!

> myvec <- c(0, rep(1:6, each = 2), 7, 7, 7)

> table(myvec)
myvec
0 1 2 3 4 5 6 7
1 2 2 2 2 2 2 3

# Extract elements appearing twice

> myvec[myvec %in% names(which(table(myvec) == 2))]
 [1] 1 1 2 2 3 3 4 4 5 5 6 6

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From robertas.kavaliunas at gmail.com  Thu Mar  1 15:57:14 2007
From: robertas.kavaliunas at gmail.com (=?ISO-8859-13?Q?Robertas_Kavali=FBnas?=)
Date: Thu, 1 Mar 2007 16:57:14 +0200
Subject: [R] time series data ploting
Message-ID: <83d0f4740703010657o1e4c603fjcb6e33434ab890ef@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070301/d8a9f61f/attachment.pl 

From kzembowe at jhuccp.org  Thu Mar  1 16:07:12 2007
From: kzembowe at jhuccp.org (Zembower, Kevin)
Date: Thu, 1 Mar 2007 10:07:12 -0500
Subject: [R] Another newbie book recommandation question
Message-ID: <2E8AE992B157C0409B18D0225D0B476304C578F5@XCH-VN01.sph.ad.jhsph.edu>

I hope this question is sufficiently different from the other requests
for book recommendations that it's not repetitious. If not, I apologize
in advance.

I'm curious what standard reference books working statisticians, or
biostatisticians, have within easy reach of their desk. I'm a computer
systems administrator, and have a two-foot bookshelf directory under my
monitor that contains 13 paperback manuals that I refer to frequently,
some once or twice a day. Are there standard reference works for
statisticians that are used the same way? From reading this list, I'm
guessing that one might be W. N. Venables and B. D. Ripley (2002),
"Modern Applied Statistics with S. Fourth Edition", Springer, ISBN
0-387-95457-0. However, I'm not limiting this to books pertaining to R.

On the other hand, maybe Google and other on-line sources, as well as
interactive programs like R that can spit out numbers previously looked
up in tables, have completely replaced the need for reference books. Is
this the case today?

I'm particularly interested in reference books that may be helpful in my
organization's work. We typically deal with datasets from international
Demographic and Health Surveys (DHS) similar to those available at
http://www.measuredhs.com/aboutsurveys/search/search_survey_main.cfm?Srv
yTp=type&listtypes=1. These typically contain 10,000+ respondents and
can have up to 800 fields. We currently analyze these datasets using
Stata.

Thanks for taking the time to think about and respond to this question.
I'll summarize the answers in a later post for the archive.

-Kevin

Kevin Zembower
Internet Services Group manager
Center for Communication Programs
Bloomberg School of Public Health
Johns Hopkins University
111 Market Place, Suite 310
Baltimore, Maryland  21202
410-659-6139


From ted.harding at nessie.mcc.ac.uk  Thu Mar  1 16:11:23 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 01 Mar 2007 15:11:23 -0000 (GMT)
Subject: [R] count the # of appearances...
In-Reply-To: <1FE90BC0-6C42-44CE-A890-DC4FDF951919@lautloscrew.com>
Message-ID: <XFMail.070301151123.ted.harding@nessie.mcc.ac.uk>

On 01-Mar-07 bunny , lautloscrew.com wrote:
> Hi there,
> 
> is there a possibility to count the number of appearances of an  
> element in a vector ?
> i mean of any given element.. deliver all elements which are exactly  
> xtimes in this vector ?
> 
> thx in advance !!

If it is a specific element value which you designate beforehand,
then Ranjan Maitra's method

  sum(your vector == your chosen element)

will of course give you the number of times this occurs.

However, your query suggests this may not be what you want.

First, if you do not designate a specific element, then table()
can give you counts of repetitions of all the distinct elements.
For example:

## Generate the vector y with repetitions of elements of x:
> x<-round(runif(10),digits=3)
> y<-sample(x,25,replace=TRUE)

## Count the numbers of repetitions
> x<-round(runif(10),digits=3)
> y<-sample(x,25,replace=TRUE)
> table(y)
## y
## 0.122 0.372 0.431 0.486 0.523 0.858 0.886 0.948 
##    5     4     5     3     3     2     2     1 

Second: You ask for a method to "deliver all elements which are
exactly xtimes in this vector". So suppose "xtimes" is a given
number of times, and you want to know all elements which each
occur xtimes times in the vector y.

You could base a method for this on the ouput of table()
as above (look at "?table" for the background).

  counts <- as.data.frame(table(y))
  counts
##       y Freq
## 1 0.122    5
## 2 0.372    4
## 3 0.431    5
## 4 0.486    3
## 5 0.523    3
## 6 0.858    2
## 7 0.886    2
## 8 0.948    1

so, if your "xtimes" is say 3, then

  counts$y[counts$Freq==3]
## [1] 0.486 0.523
## Levels: 0.122 0.372 0.431 0.486 0.523 0.858 0.886 0.948

showing that elements "0.486" and "0.523" occurred 3 times each
in the vector y.

Hoping this helps,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 01-Mar-07                                       Time: 15:11:09
------------------------------ XFMail ------------------------------


From andy_liaw at merck.com  Thu Mar  1 16:30:29 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 1 Mar 2007 10:30:29 -0500
Subject: [R] how to apply the function cut( ) to many columns in a
 data.frame?
In-Reply-To: <45E69F8D.2090602@optonline.net>
References: <45e920ef0702282354q1d195ea4j5d98e2098a206aa3@mail.gmail.com>
	<45E69F8D.2090602@optonline.net>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03C9537A@usctmx1106.merck.com>

From: Chuck Cleland
> 
> ahimsa campos-arceiz wrote:
> > Dear useRs,
> > 
> > In a data.frame (df) I have several columns (x1, x2, x3....xn) 
> > containing data as a continuous numerical response:
> > 
> > df
> >  var     x1    x2     x3
> >   1    143   147   137
> >   2      93    93   117
> >   3    164    39   101
> >   4    123   118    97
> >   5     63   125     97
> >   6    129    83   124
> >   7    123    93   136
> >   8    123    80     79
> >   9     89   107   150
> > 10     78    95    121
> > 
> > I want to classify the values in the columns x1, x2, etc, 
> into bins of 
> > fix margins (0-5, 5-10, ....). For one vector I can do it 
> easily with 
> > the function cut:
> > 
> >> df$x1 <- cut(df$x1, br=5*(0:40), labels=5*(1:40))
> >> df$x1
> >  [1] 145 95  165 125 65  130 125 125 90  80 40 Levels: 5 10 
> 15 20 25 
> > 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 ...
> > 200
> > 
> > However if I try to use a subset of my data.frame:
> > 
> > df[,3:4] <- cut(df[,3:4], br=5*(0:40), labels=5*(1:40))
> > 
> > Error in cut.default(df[, 3:4], br = 5 * (0:40), labels = 5 
> * (1:40)) :
> >         'x' must be numeric
> > 
> > 
> > How can I make this work with data frames in which I want 
> to apply the 
> > function cut( ) to many columns in a data.frame?
> 
>   You have an answer within your question - use one of the 
> various "apply" functions.  For example:
> 
> lapply(df[,3:4], function(x){cut(x, br=5*(0:40), labels=5*(1:40))})

Or perhaps a bit more simply:

lapply(df[, 3:4], cut, br=5*(0:40), labels=5*(1:40)))

and if a data frame is desired as output, wrap the above in
as.data.frame().

(Just keep in mind that a data frame is like a list.)

Andy

 
> ?lapply
> ?sapply
> ?apply
> 
> > I guess that I might have to use something like for ( ) 
> (which I'm not 
> > familiar with), but maybe you know a straight forward method to use 
> > with data.frames.
> > 
> > 
> > Thanks a lot!
> > 
> > Ahimsa
> > 
> > *********************************************
> > 
> > # data
> > var <- 1:10
> > x1 <- rnorm(10, mean=100, sd=25)
> > x2 <- rnorm(10, mean=100, sd=25)
> > x3 <- rnorm(10, mean=100, sd=25)
> > df <- data.frame(var,x1,x2,x3)
> > df
> > 
> > # classifying the values of the vector df$x1 into bins of width 5
> > df$x1 <- cut(df$x1, br=5*(0:40), labels=5*(1:40))
> > df$x1
> > 
> > # trying it a subset of the data.frame df[,3:4] <- cut(df[,3:4], 
> > br=5*(0:40), labels=5*(1:40)) df[,3:4]
> 
> --
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 512-0171 (M, W, F)
> fax: (917) 438-0894
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From Charles.Annis at StatisticalEngineering.com  Thu Mar  1 16:52:14 2007
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Thu, 1 Mar 2007 10:52:14 -0500
Subject: [R] Another newbie book recommandation question
In-Reply-To: <2E8AE992B157C0409B18D0225D0B476304C578F5@XCH-VN01.sph.ad.jhsph.edu>
References: <2E8AE992B157C0409B18D0225D0B476304C578F5@XCH-VN01.sph.ad.jhsph.edu>
Message-ID: <08ce01c75c19$94fc77e0$6400a8c0@DD4XFW31>

Oh, Boy.  This might result in a data dump since each of us has a personal
library.  Here are the top dozen or so from mine:


   1. Agresti, Alan, Categorical Data Analysis, 2nd ed., Wiley, 2002
       
   2. Box, George E. P., William G. Hunter, and J. Stewart Hunter,
Statistics for Experimenters, Wiley, 1978
       
   3. Casella, George and Roger L. Berger, Statistical Inference, Duxbury
Press, 2001
       
   4. Chatfield, C., The Analysis of Time Series, 4th ed., Chapman & Hall,
1989
       
   5. Cressie, Noel A. C., Statistics for Spatial Data, Wiley, 1993
       
   6. Fisher, Ronald A., Statistical Methods for Research Workers.  (First
published in 1925; 14th edition was ready for publication in 1962, when
Fisher died, and was published in 1990, by the Oxford University Press,
along with Experimental Design and Scientific Inference, with corrections to
the 1991 edition, in 1993.)
       
   7. Efron, Bradley and Robert J. Tibshirani, An Introduction to the
Bootstrap, Chapman and Hall, 1993
       
   8. Gelman, Andrew, John B. Carlin, Hal S. Stern, Donald B. Rubin,
Bayesian Data Analysis, 2nd ed., Chapman & Hall/CRC, 2003
       
   9. Johnson, Richard A. and Dean W. Wichern, Applied Multivariate
Statistical Analysis, 5th ed., Prentice Hall, 20021988
       
  10. Kutner, Michael, and Christopher J. Nachtsheim, John Neter, William
Li, Applied Linear Statistical Models, 5th ed., McGraw-Hill/Irwin, 2005
       
  11. Lawless, Jerald F., Statistical Models and Methods for Lifetime Data,
Wiley, 1982
       
  12. McCullagh, P. and J.A. Nelder, Generalized Linear Models, Chapman &
Hall, 2nd ed., 1989
       
  13. Meeker and Escobar, Statistical Methods for Reliability Data, Wiley,
1998
       
  14. Robert, Christian P. and George Casella, Monte Carlo Statistical
Methods, Springer, 1999
       
  15. Venables and Ripley, Modern Applied Statistics with S, 4th ed.,
Springer, 2002



Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Zembower, Kevin
Sent: Thursday, March 01, 2007 10:07 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Another newbie book recommandation question

I hope this question is sufficiently different from the other requests
for book recommendations that it's not repetitious. If not, I apologize
in advance.

I'm curious what standard reference books working statisticians, or
biostatisticians, have within easy reach of their desk. I'm a computer
systems administrator, and have a two-foot bookshelf directory under my
monitor that contains 13 paperback manuals that I refer to frequently,
some once or twice a day. Are there standard reference works for
statisticians that are used the same way? From reading this list, I'm
guessing that one might be W. N. Venables and B. D. Ripley (2002),
"Modern Applied Statistics with S. Fourth Edition", Springer, ISBN
0-387-95457-0. However, I'm not limiting this to books pertaining to R.

On the other hand, maybe Google and other on-line sources, as well as
interactive programs like R that can spit out numbers previously looked
up in tables, have completely replaced the need for reference books. Is
this the case today?

I'm particularly interested in reference books that may be helpful in my
organization's work. We typically deal with datasets from international
Demographic and Health Surveys (DHS) similar to those available at
http://www.measuredhs.com/aboutsurveys/search/search_survey_main.cfm?Srv
yTp=type&listtypes=1. These typically contain 10,000+ respondents and
can have up to 800 fields. We currently analyze these datasets using
Stata.

Thanks for taking the time to think about and respond to this question.
I'll summarize the answers in a later post for the archive.

-Kevin

Kevin Zembower
Internet Services Group manager
Center for Communication Programs
Bloomberg School of Public Health
Johns Hopkins University
111 Market Place, Suite 310
Baltimore, Maryland  21202
410-659-6139

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From greg at nosnhoj.org  Thu Mar  1 16:58:50 2007
From: greg at nosnhoj.org (Greg Johnson)
Date: Thu, 1 Mar 2007 15:58:50 +0000 (UTC)
Subject: [R] question about xtable and Hmisc
References: <es6o3t$bhu$1@sea.gmane.org>
Message-ID: <loom.20070301T165510-843@post.gmane.org>

steve <fisk <at> bowdoin.edu> writes:

> 
> I would like to get rid of the row numbers using xtable and latex.
> 
> but I don't want the row numbers. Is it possible to get rid of them?
> 
> Also, if x is a data frame, latex(x) contains the row numbers.
> Can I get rid of them here as well?
> 
> Steve


Steve,

?print.xtable

look at the include.rownames option.

Greg


From liuwensui at gmail.com  Thu Mar  1 17:16:19 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Thu, 1 Mar 2007 11:16:19 -0500
Subject: [R] Another newbie book recommandation question
In-Reply-To: <2E8AE992B157C0409B18D0225D0B476304C578F5@XCH-VN01.sph.ad.jhsph.edu>
References: <2E8AE992B157C0409B18D0225D0B476304C578F5@XCH-VN01.sph.ad.jhsph.edu>
Message-ID: <1115a2b00703010816wd78389cwf6d289913cc8ed43@mail.gmail.com>

for the size of your data file, I think R can handle it. of course, it
also depends on your hardware. however, it might not be a good idea to
do heavy data manipulation work in R.

stata has very good routine for survey analysis. i am not sure if R is
as good as stata in terms of survey analysis.

S programming by the same authors as MASS might be a good reference
good you would like it on your shelf.

On 3/1/07, Zembower, Kevin <kzembowe at jhuccp.org> wrote:
> I hope this question is sufficiently different from the other requests
> for book recommendations that it's not repetitious. If not, I apologize
> in advance.
>
> I'm curious what standard reference books working statisticians, or
> biostatisticians, have within easy reach of their desk. I'm a computer
> systems administrator, and have a two-foot bookshelf directory under my
> monitor that contains 13 paperback manuals that I refer to frequently,
> some once or twice a day. Are there standard reference works for
> statisticians that are used the same way? From reading this list, I'm
> guessing that one might be W. N. Venables and B. D. Ripley (2002),
> "Modern Applied Statistics with S. Fourth Edition", Springer, ISBN
> 0-387-95457-0. However, I'm not limiting this to books pertaining to R.
>
> On the other hand, maybe Google and other on-line sources, as well as
> interactive programs like R that can spit out numbers previously looked
> up in tables, have completely replaced the need for reference books. Is
> this the case today?
>
> I'm particularly interested in reference books that may be helpful in my
> organization's work. We typically deal with datasets from international
> Demographic and Health Surveys (DHS) similar to those available at
> http://www.measuredhs.com/aboutsurveys/search/search_survey_main.cfm?Srv
> yTp=type&listtypes=1. These typically contain 10,000+ respondents and
> can have up to 800 fields. We currently analyze these datasets using
> Stata.
>
> Thanks for taking the time to think about and respond to this question.
> I'll summarize the answers in a later post for the archive.
>
> -Kevin
>
> Kevin Zembower
> Internet Services Group manager
> Center for Communication Programs
> Bloomberg School of Public Health
> Johns Hopkins University
> 111 Market Place, Suite 310
> Baltimore, Maryland  21202
> 410-659-6139
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From d.s.robinson at dur.ac.uk  Thu Mar  1 17:16:58 2007
From: d.s.robinson at dur.ac.uk (d.s.robinson at dur.ac.uk)
Date: Thu, 01 Mar 2007 16:16:58 +0000
Subject: [R] Creating a vector of variable bin widths
Message-ID: <45E6FC7A.8090600@dur.ac.uk>

Dear R users,
              I am having a little trouble with grouping data.

-----------Detailed explanation (summary below)------------

A small sample of my data is below (which has already been rounded and 
grouped a little from the raw data for clarity).

I am sampling data from an unknown game which, according to my null 
hypothesis, follows a binomial distribution. The game can be supposedly 
be played with a range of probabilities (the independent variable) of 
success, 0.0-0.3 are shown below, although my full data set goes all the 
way up 0.99. The number of observations for each probability of success, 
and the actual proportion of wins in the sample (the dependant variable) 
are also shown.

By CLT, the sample winning proportions (the dependant variable) should 
be a unbiased estimator of the population proportion (the independent 
variable). I want to perform a significance test at each probability 
level to see if the null hypothesis can be rejected.

But, the problem is in defining those probability levels. At the moment, 
some probabilities of success have a very low number of observations, 
whilst others have very many. Leaving the data as it is results in 
statistically meaningless results at the low and high levels of success. 
Further grouping the data using fixed group widths results very few data 
points at high and low probabilities, and a few data points in the 
middle with a very high number of observations.

The way around this (I think) is to use variable bin widths. The width 
of each bin should be wide enough so that (again, I think this is a 
reasonable idea) the variance of the sample estimate (using the normal 
approximation to the binomial), [p(1-p)]/n, is less than a certain 
value, say 2% squared. I presume I also need to make sure that for each 
group np<5 and n(1-p)<5, or can this simply replace the variance test?

IndependantVar	Observations	DependantVar	
--------------------------------------------
0.01		1		0.000	
0.03		5		0.000	
0.04		11		0.000	
0.05		9		0.000	
0.06		19		0.000	
0.07		12		0.000	
0.08		18		0.056	
0.09		10		0.200	
0.10		13		0.077	
0.11		17		0.118	
0.12		17		0.059	
0.13		18		0.056	
0.14		21		0.000	
0.15		25		0.160	
0.16		23		0.000	
0.17		35		0.314	
0.18		26		0.231	
0.19		31		0.226	
0.20		27		0.148	
0.21		26		0.462	
0.22		21		0.286	
0.23		29		0.207	
0.24		38		0.289	
0.25		38		0.132	
0.26		27		0.259	
0.27		52		0.308	
0.28		62		0.194	
0.29		82		0.232	
0.30		97		0.278	

------------------Summary---------------------------

So, I how can I write a function that creates a vector of variable break 
values for, say, cut(). It should iteratively make bin widths wider 
until an condition based on the value to be binned (the probability of 
success), and a second value, the number of observations, is met 
(assuming you agree with my method of restricting the variance, the 
rational of which is outlined above).

I would appreciate any comments on either the reasoning (I am fairly new 
to this sort of statistics) or how I can write the R code to achieve the 
proposed goal. I hope I have explained this clearly enough to merit a 
response.

Regards,
	DR


From andy_liaw at merck.com  Thu Mar  1 17:27:33 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 1 Mar 2007 11:27:33 -0500
Subject: [R] How to read in this data format?
In-Reply-To: <BAY134-F394F6FDE418B617D16A196D8800@phx.gbl>
References: <BAY134-F394F6FDE418B617D16A196D8800@phx.gbl>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03C95412@usctmx1106.merck.com>

You can't expect general-purpose tools like read.table in R to be able
to deal with highly specialized file format.  Here's what I'd start.  It
doesn't put data in the format you specified exactly, but I doubt you'll
need that.  This might be sufficient for your purpose:

dat <- readLines(file("yourdata.dat"))
## Get rid of blank lines.
dat <- dat[dat != ""]
scan.lines <- grep("Scan", dat)
## Chop off the header rows.
dat <- dat[scan.lines[1]:length(dat)]
scan.lines <- scan.lines - scan.lines[1] + 1
lines.per.scan <- c(scan.lines[-1], length(dat) + 1) - scan.lines
## Split the data into a list, with each scan taking up one component.
dat <- split(dat, rep(seq(along=lines.per.scan), each=lines.per.scan))
## Process the data one scan at a time.
result <- lapply(dat, function(x) {
    x <- strsplit(x, "\t")
    rtime <- x[[2]][2]  # second field of second line
    t(matrix(as.numeric(do.call(rbind, c(rtime, x[-(1:2)]))), ncol=2))
})

This is what I get from the data you've shown:

R> result
$`1`
      [,1]     [,2]     [,3]     [,4]
[1,] 0.017 399.8112 399.8742 399.9372
[2,] 0.017 184.0000   0.0000 152.0000

$`2`
      [,1]     [,2]     [,3]     [,4]
[1,] 0.021 399.8112 399.8742 399.9372
[2,] 0.021 181.0000   1.0000 153.0000

Note that you probably should avoid using numbers as column names in a
data frame, even if it's possible.

Andy


From: Bart Joosen
> 
> Hi,
> 
> I recieved an ascii file, containing following information:
> 
> $$ Experiment Number:
> $$ Associated Data:
> 
> FUNCTION 1
> 
> Scan		1
> Retention Time	0.017
> 
> 399.8112	184
> 399.8742	0
> 399.9372	152
> ....
> 
> Scan		2
> Retention Time	0.021
> 
> 399.8112	181
> 399.8742	1
> 399.9372	153
> .....
> 
> 
> I would like to import this data in R into a dataframe, where 
> there is a column time, the first numbers as column names, 
> and the second numbers as data in the dataframe:
> 
> Time	399.8112	399.8742	399.9372
> 0.017	184	0	152
> 0.021	181	1	153
> 
> I did take a look at the read.table, read.delim, scan, ... 
> But I 've no idea about how to solve this problem.
> 
> Anyone?
> 
> 
> Thanks
> 
> Bart
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From david.meyer at wu-wien.ac.at  Thu Mar  1 17:34:53 2007
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Thu, 01 Mar 2007 17:34:53 +0100
Subject: [R] PROC TABULATE with R
Message-ID: <45E700AD.3040604@wu-wien.ac.at>

You can also have a look at structable() in vcd, especially the indexing 
functions. The problem with OLAP in R is, that you will have to create 
sth. like a hierarchical factor to handle rollup/drilldown correctly.

And you will have to decide whether it's purely memory-based (fast 
calculations, but memory limit), or you do it using data bases / SQL (slow).

And finally, you will need some simple GUI to provide interactive use 
(doing OLAP using command-line functions is not really OLAP).

Best,
David

-----------------

 > Hi !
 > > >
 > > > with apply or tapply-like functions, is it possible to create
 > > > multidimensional cubes with R ? Like with SAS and its function PROC
 > TABULATE
 > > > or OLAP ?
 > > > Is there some functions or modules to access OLAP databases with R ?
 > > >
 > > > My idea is to create a package for that, since XMLA and JOLAP
 > specifications
 > > > should able us to do so !
 > > >
 > > >

-- 
Dr. David Meyer
Department of Information Systems and Operations

Vienna University of Economics and Business Administration
Augasse 2-6, A-1090 Wien, Austria, Europe
Tel: +43-1-313 36 4393
Fax: +43-1-313 36 90 4393
HP:  http://wi.wu-wien.ac.at/~meyer/


From lalithaviswanath at yahoo.com  Thu Mar  1 17:34:19 2007
From: lalithaviswanath at yahoo.com (lalitha viswanath)
Date: Thu, 1 Mar 2007 08:34:19 -0800 (PST)
Subject: [R] Query about data manipulation
Message-ID: <434261.69370.qm@web43135.mail.sp1.yahoo.com>

Hi
Thanks much for the prompt response to my earlier
enquiry on packages for regression analyses.
Along the same topic(?), I have another question about
which I could use some input.

I am retreiving data from a MySQL database using
RODBC. 
The table has many BLOB columns and each BLOB column
has data in the format
"id1 \t id2 \t measure \n id3 \t id4 \t measure...."
(i.e. multiple rows compressed as one long string)

I am retreiving them as follows.

dataFromDB <- sqlQuery(channel, "select
uncompress(columnName) from tableName");


I am looking for ways to convert this long "string"
into a table/dataframe in R, making it easier for
further post processing etc without reading/writing it
to a file first.

Although by doing write.table and reading it in again,
I got the result in a data frame, with the \t and \n
interpreted correctly, I wish to sidestep this as I
need to carry out this analyses for over 4 million
such entries.
I tried 
write.table(dataFromDB, file="FileName");
dataFromFile <- read.table(FileName, sep="\t") 
dataFromFile is of the form

92_8_nmenA      993_7_mpul      1.042444
92_8_nmenA      3_5_cpneuA      0.900939
190_1_rpxx      34_4_ctraM      0.822532
190_1_rpxx      781_6_pmul      0.870016

Your input on the above is greatly appreciated.
Thanks
Lalitha



 
____________________________________________________________________________________
Never miss an email again!


From pierreclauss at yahoo.fr  Thu Mar  1 18:01:28 2007
From: pierreclauss at yahoo.fr (pierre clauss)
Date: Thu, 1 Mar 2007 17:01:28 +0000 (GMT)
Subject: [R] Fit Student Copula
Message-ID: <20070301170128.51897.qmail@web26313.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070301/b6ab7b03/attachment.pl 

From dacha.atienza at gmail.com  Thu Mar  1 09:59:54 2007
From: dacha.atienza at gmail.com (Dacha Atienza)
Date: Thu, 1 Mar 2007 09:59:54 +0100
Subject: [R] HELP on GAM MODELS
Message-ID: <358012680703010059x635468cdke21a190915f43ba1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070301/3d442c10/attachment.pl 

From marc_schwartz at comcast.net  Thu Mar  1 18:18:36 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 01 Mar 2007 11:18:36 -0600
Subject: [R] Query about data manipulation
In-Reply-To: <434261.69370.qm@web43135.mail.sp1.yahoo.com>
References: <434261.69370.qm@web43135.mail.sp1.yahoo.com>
Message-ID: <1172769516.4897.64.camel@localhost.localdomain>

On Thu, 2007-03-01 at 08:34 -0800, lalitha viswanath wrote:
> Hi
> Thanks much for the prompt response to my earlier
> enquiry on packages for regression analyses.
> Along the same topic(?), I have another question about
> which I could use some input.
> 
> I am retreiving data from a MySQL database using
> RODBC. 
> The table has many BLOB columns and each BLOB column
> has data in the format
> "id1 \t id2 \t measure \n id3 \t id4 \t measure...."
> (i.e. multiple rows compressed as one long string)
> 
> I am retreiving them as follows.
> 
> dataFromDB <- sqlQuery(channel, "select
> uncompress(columnName) from tableName");
> 
> 
> I am looking for ways to convert this long "string"
> into a table/dataframe in R, making it easier for
> further post processing etc without reading/writing it
> to a file first.
> 
> Although by doing write.table and reading it in again,
> I got the result in a data frame, with the \t and \n
> interpreted correctly, I wish to sidestep this as I
> need to carry out this analyses for over 4 million
> such entries.
> I tried 
> write.table(dataFromDB, file="FileName");
> dataFromFile <- read.table(FileName, sep="\t") 
> dataFromFile is of the form
> 
> 92_8_nmenA      993_7_mpul      1.042444
> 92_8_nmenA      3_5_cpneuA      0.900939
> 190_1_rpxx      34_4_ctraM      0.822532
> 190_1_rpxx      781_6_pmul      0.870016
> 
> Your input on the above is greatly appreciated.
> Thanks
> Lalitha

The easiest way might be to use a textConnection().

Let's say that you have read in your data as above and you have a column
called 'blob':

> dataFromDB
                                            blob
1 id1 \t id2 \t measure \n id3 \t id4 \t measure


#Open textConnection.  Note coercion to character
BLOB <- textConnection(as.character(dataFromDB$blob))

# Read in the column
DF <- read.table(BLOB, sep = "\t")

# Close the connection
close(BLOB)


> DF
     V1    V2        V3
1  id1   id2   measure 
2  id3   id4   measure


See ?textConnection

HTH,

Marc Schwartz


From mailinglists at versanet.de  Thu Mar  1 14:40:40 2007
From: mailinglists at versanet.de (Reinhard Sy)
Date: Thu, 01 Mar 2007 14:40:40 +0100
Subject: [R] Where to find ROracle for R 2.2.1
Message-ID: <1172756440.7970.3.camel@localhost.localdomain>

	Hello

I am looking for ROracle - Module but found only a version that can
installed with R Version > 2.30. 

I am running a production system where I can't change the currently
installed R. That's why I need a ROracle - Module that is compatible
with  R version 2.2.1  

Thanks
Reinhard


From mark-j-allen at hotmail.com  Thu Mar  1 14:48:33 2007
From: mark-j-allen at hotmail.com (Mark Allen)
Date: Thu, 01 Mar 2007 23:48:33 +1000
Subject: [R] setting font in plots
Message-ID: <BAY116-F27D7F5F2DE9924F5A4DE6EDA800@phx.gbl>

Dear Reader

I am trying to change the font in a plot and after several trials finally 
came up with the following code.

plot(var_a, var_b, pch = 16, font.lab = 10, font = 10)
points(var_a, var_c, pch = 3, font = 10)
legend(0.1, 0.8, legend = c(?var_b?, ?var_c?), pch = c(16,3))

It does change the font in the plot (Courier), but not in the legend that 
does not accept the "font = ..." argument. I could also not find a list with 
font names and corresponding numbers.

Surely there must be a better way to do this that I cannot seem to find. 
Looking at the R website I came across e.g. gpar() and par(), but 
unfortunately could not make it work... Is there perhaps anyone that could 
help me out here?

Thanks very much
Mark Allen

_________________________________________________________________
Advertisement: It's simple! Sell your car for just $20 at carsales.com.au


From bruno.c at inwind.it  Thu Mar  1 18:20:57 2007
From: bruno.c at inwind.it (Bruno C.)
Date: Thu,  1 Mar 2007 18:20:57 +0100
Subject: [R] matrices dimensions limitation
Message-ID: <JE8IUX$7EB8E4D804CF95E3B44D09B8899239BD@libero.it>

Hello,
I have several questions around matrices size limitation:
     can i create a matrix with 700000 rows?
     does it depends on the number of columns?
     if so can Is there a way to ask to R, given the number of columns, the max number of rows I can have?

I need in fact to do regression prediction on a huge matrix (700000x1000)ca. ... 
I will probably have to decompose it on several matrices, and I would do that in an intelligent way so that if my observation matrix size change I do not need to rewrite my code...

Thanx in advance



------------------------------------------------------
Passa a Infostrada. ADSL e Telefono senza limiti e senza canone Telecom
http://click.libero.it/infostrada1marz07


From ramziabb at yahoo.com  Thu Mar  1 18:22:22 2007
From: ramziabb at yahoo.com (ramzi abboud)
Date: Thu, 1 Mar 2007 09:22:22 -0800 (PST)
Subject: [R] R File IO Slow?
Message-ID: <515147.48704.qm@web55609.mail.re4.yahoo.com>

Is R file IO slow in general or am I missing
something?  It takes me 5 minutes to do a load(MYFILE)
where MYFILE is a 27 MB Rdata file.  Is there any way
to speed this up?  

The one idea I have is having R call a C or Perl
routine, reading the file in that language, converting
the data in to R objects, then sending them back into
R.  This is more work that I want to do, however, in
loading Rdata files.

Any ideas would be appreciated.
Ramzi Aboud
University of Rochester





 
____________________________________________________________________________________
Need Mail bonding?


From jasoncbarnhart at msn.com  Thu Mar  1 18:31:08 2007
From: jasoncbarnhart at msn.com (Jason Barnhart)
Date: Thu, 1 Mar 2007 10:31:08 -0700
Subject: [R] Double-banger function names: preferences and suggestions
References: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>
Message-ID: <BAY116-DAV4AC499B924DC4285E9C03CF800@phx.gbl>

Definitely not #2.   Prefer #1 but #3 is ok as well.

Thanks for contributing and inquiring.


----- Original Message ----- 
From: "hadley wickham" <h.wickham at gmail.com>
To: <R-help at r-project.org>
Sent: Sunday, February 25, 2007 7:44 AM
Subject: [R] Double-banger function names: preferences and suggestions


> What do you prefer/recommend for double-banger function names:
>
> 1 scale.colour
> 2 scale_colour
> 3 scaleColour
>
> 1 is more R-like, but conflicts with S3.  2 is a modern version of
> number 1, but not many packages use it.  Number 3 is more java-like.
> (I like number 2 best)
>
> Any suggestions?
>
> Thanks,
>
> Hadley
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Thu Mar  1 18:35:43 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 1 Mar 2007 12:35:43 -0500
Subject: [R] How to read in this data format?
In-Reply-To: <BAY134-F394F6FDE418B617D16A196D8800@phx.gbl>
References: <BAY134-F394F6FDE418B617D16A196D8800@phx.gbl>
Message-ID: <971536df0703010935r6abcd474v839a3c765c796699@mail.gmail.com>

Read in the data using readLines, extract out
all desired lines (namely those containing only
numbers, dots and spaces or those with the
word Time) and remove Retention from all
lines so that all remaining lines have two
fields.  Now that we have desired lines
and all lines have two fields read them in
using read.table.

Finally, split them into groups and restructure
them using "by" and in the last line we
convert the "by" output to a data frame.

At the end we display an alternate function f
for use with by should we wish to generate long
rather than wide output (using the terminology
of the reshape command).


Lines <- "$$ Experiment Number:
$$ Associated Data:

FUNCTION 1

Scan            1
Retention Time  0.017

399.8112        184
399.8742        0
399.9372        152
....

Scan            2
Retention Time  0.021

399.8112        181
399.8742        1
399.9372        153
"

# replace next line with: Lines. <- readLines("myfile.dat")
Lines. <- readLines(textConnection(Lines))
Lines. <- grep("^[1-9][0-9. ]*$|Time", Lines., value = TRUE)
Lines. <- gsub("Retention", "", Lines.)

DF <- read.table(textConnection(Lines.), as.is = TRUE)
closeAllConnections()

f <- function(x) c(id = x[1,2], structure(x[-1,2], .Names = x[-1,1]))
out.by <- by(DF, cumsum(DF[,1] == "Time"), f)
as.data.frame(do.call("rbind", out.by))


We could alternately consider producing long
format by replacing the function f with:

f <- function(x) data.frame(x[-1,], id = x[1,2])


On 3/1/07, Bart Joosen <bartjoosen at hotmail.com> wrote:
> Hi,
>
> I recieved an ascii file, containing following information:
>
> $$ Experiment Number:
> $$ Associated Data:
>
> FUNCTION 1
>
> Scan            1
> Retention Time  0.017
>
> 399.8112        184
> 399.8742        0
> 399.9372        152
> ....
>
> Scan            2
> Retention Time  0.021
>
> 399.8112        181
> 399.8742        1
> 399.9372        153
> .....
>
>
> I would like to import this data in R into a dataframe, where there is a
> column time, the first numbers as column names, and the second numbers as
> data in the dataframe:
>
> Time    399.8112        399.8742        399.9372
> 0.017   184     0       152
> 0.021   181     1       153
>
> I did take a look at the read.table, read.delim, scan, ... But I 've no idea
> about how to solve this problem.
>
> Anyone?
>
>
> Thanks
>
> Bart
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jimenez.daniel77 at gmail.com  Thu Mar  1 18:44:42 2007
From: jimenez.daniel77 at gmail.com (Daniel Jimenez)
Date: Thu, 1 Mar 2007 18:44:42 +0100
Subject: [R] GLM problems.
Message-ID: <e5e455f20703010944v6d21dd8em5fb0f173310accbc@mail.gmail.com>

Dear R users I'm new in R management and maybe It's a silly question. I'm
working with GLM to obtain predictive models. I have some problesm with the
prediction instruction:



> DatosTotal <- read.csv("Var_perdizcsv.csv", sep =";")
> edvariable <- edit(DatosTotal)
> pre <- predict(rlfinal, DatosTotal, type = 'probs')
Erro en match.arg(type) : 'arg' should be one of link, response, terms
>


I check the database and the name variables are the same... I do not know
what happen. Please help me.

I attach the complet proccess.

Thank you.

-- 
Daniel Jim?nez Garc?a
------------ pr?xima parte ------------

R : Copyright 2006, The R Foundation for Statistical Computing
Version 2.3.1 (2006-06-01)
ISBN 3-900051-07-0

R es un software libre y viene sin GARANTIA ALGUNA.
Usted puede redistribuirlo bajo ciertas circunstancias.
Escriba 'license()' o 'licence()' para detalles de distribucion.

R es un proyecto colaborativo con muchos contribuyentes.
Escriba 'contributors()' para obtener mas informacion y
'citation()' para saber como citar R o paquetes de R en publicaciones.

Escriba 'demo()' para demostraciones, 'help()' para el sistema on-line de ayuda,
o 'help.start()' para abrir el sistema de ayuda HTML con su navegador.
Escriba 'q()' para salir de R.

[Previously saved workspace restored]

> Datos <- read.csv("variables_perdiz_01_03.csv", sep =";")
> attach(Datos)
> library(MASS)
> 
> rl0 <- glm(PRESAUS ~ 1, family = binomial)
> 
> rl1 <- stepAIC(rl0, direction = c("both"), scope = PRESAUS ~ 1 + elev + vias + orient + pend + 
+ freg + frsec + labsec + matarb + matcl + matden + ripar + visec + pinar + forest + matorrales + 
+ abandonos + ombro + termic, keep = extractAIC)
Start:  AIC= 99.04 
 PRESAUS ~ 1 

             Df Deviance     AIC
+ freg        1   91.415  95.415
+ matorrales  1   93.457  97.457
+ ripar       1   93.766  97.766
+ matarb      1   94.975  98.975
<none>            97.041  99.041
+ forest      1   95.043  99.043
+ abandonos   1   95.093  99.093
+ matden      1   95.133  99.133
+ ombro       1   95.655  99.655
+ pend        1   95.986  99.986
+ termic      1   96.156 100.156
+ vias        1   96.641 100.641
+ elev        1   96.680 100.680
+ pinar       1   96.710 100.710
+ orient      1   96.775 100.775
+ labsec      1   96.987 100.987
+ visec       1   97.028 101.028
+ matcl       1   97.040 101.040
+ frsec       1   97.041 101.041

Step:  AIC= 95.42 
 PRESAUS ~ freg 

             Df Deviance    AIC
+ termic      1   87.885 93.885
+ abandonos   1   88.134 94.134
+ forest      1   88.550 94.550
+ elev        1   88.668 94.668
+ ripar       1   88.802 94.802
<none>            91.415 95.415
+ matorrales  1   89.648 95.648
+ matarb      1   90.023 96.023
+ matden      1   90.443 96.443
+ pinar       1   90.796 96.796
+ vias        1   91.154 97.154
+ ombro       1   91.156 97.156
+ pend        1   91.233 97.233
+ frsec       1   91.340 97.340
+ matcl       1   91.345 97.345
+ visec       1   91.347 97.347
+ labsec      1   91.378 97.378
+ orient      1   91.410 97.410
- freg        1   97.041 99.041

Step:  AIC= 93.89 
 PRESAUS ~ freg + termic 

             Df Deviance     AIC
+ abandonos   1   83.370  91.370
+ matorrales  1   83.491  91.491
+ matden      1   83.771  91.771
<none>            87.885  93.885
+ ripar       1   86.312  94.312
+ elev        1   86.501  94.501
+ forest      1   86.719  94.719
+ pend        1   86.743  94.743
+ visec       1   87.118  95.118
+ matarb      1   87.242  95.242
- termic      1   91.415  95.415
+ orient      1   87.416  95.416
+ vias        1   87.498  95.498
+ labsec      1   87.544  95.544
+ ombro       1   87.718  95.718
+ pinar       1   87.720  95.720
+ frsec       1   87.856  95.856
+ matcl       1   87.878  95.878
- freg        1   96.156 100.156

Step:  AIC= 91.37 
 PRESAUS ~ freg + termic + abandonos 

             Df Deviance     AIC
+ matden      1   80.343  90.343
+ matorrales  1   80.823  90.823
<none>            83.370  91.370
+ visec       1   81.465  91.465
+ ripar       1   81.791  91.791
+ forest      1   82.089  92.089
+ labsec      1   82.335  92.335
+ pend        1   82.699  92.699
+ vias        1   83.057  93.057
+ elev        1   83.091  93.091
+ matarb      1   83.166  93.166
+ pinar       1   83.193  93.193
+ orient      1   83.242  93.242
+ ombro       1   83.282  93.282
+ matcl       1   83.357  93.357
+ frsec       1   83.369  93.369
- abandonos   1   87.885  93.885
- termic      1   88.134  94.134
- freg        1   94.026 100.026

Step:  AIC= 90.34 
 PRESAUS ~ freg + termic + abandonos + matden 

             Df Deviance    AIC
<none>            80.343 90.343
+ ripar       1   78.766 90.766
+ visec       1   78.862 90.862
- matden      1   83.370 91.370
- abandonos   1   83.771 91.771
+ pend        1   79.825 91.825
+ labsec      1   79.837 91.837
+ matorrales  1   79.861 91.861
+ forest      1   79.965 91.965
+ matarb      1   80.009 92.009
+ frsec       1   80.087 92.087
+ vias        1   80.171 92.171
+ matcl       1   80.217 92.217
+ elev        1   80.240 92.240
+ ombro       1   80.322 92.322
+ pinar       1   80.329 92.329
+ orient      1   80.342 92.342
- termic      1   87.726 95.726
- freg        1   90.534 98.534
> tabla <- data.frame(rl1$keep)
> tabla
       X1       X2       X3      X4       X5
1  1.0000  2.00000  3.00000  4.0000  5.00000
2 99.0406 95.41544 93.88513 91.3704 90.34343
> rlfinal <- glm(PRESAUS ~ freg + termic + abandonos + matden , family = binomial)
> summary (rlfinal)

Call:
glm(formula = PRESAUS ~ freg + termic + abandonos + matden, family = binomial)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.08022  -0.82070   0.04674   0.90439   1.95034  

Coefficients:
             Estimate Std. Error z value Pr(>|z|)   
(Intercept) -5.888148   2.533235  -2.324  0.02011 * 
freg        -0.004076   0.001440  -2.830  0.00465 **
termic       0.017090   0.006828   2.503  0.01231 * 
abandonos   -0.002735   0.001556  -1.758  0.07871 . 
matden       0.002493   0.001534   1.625  0.10414   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 97.041  on 69  degrees of freedom
Residual deviance: 80.343  on 65  degrees of freedom
AIC: 90.343

Number of Fisher Scoring iterations: 4

> DatosTotal <- read.csv("Var_perdizcsv.csv", sep =";")
> edvariable <- edit(DatosTotal)
> pre <- predict(rlfinal, DatosTotal, type = 'probs')
Erro en match.arg(type) : 'arg' should be one of link, response, terms
> 

From sfalcon at fhcrc.org  Thu Mar  1 18:47:37 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Thu, 01 Mar 2007 09:47:37 -0800
Subject: [R] object is not subsettable
In-Reply-To: <40CB4023-3489-40F3-883B-6567241DB690@biomed.cas.cz> (Radek
	Blatny's message of "Thu, 1 Mar 2007 13:47:29 +0100")
References: <40CB4023-3489-40F3-883B-6567241DB690@biomed.cas.cz>
Message-ID: <m2mz2wdfeu.fsf@ziti.local>

Hi Radek,

Radek Blatny <blatny at biomed.cas.cz> writes:

> Dear colleagues,
> I've just come across a problem with the following command which is a  
> part of the "metaOverview.R" code file provided as an monography- 
> accompanying file at
> http://www.bioconductor.org/docs/mogr/metadata:

The appropriate place to ask about Bioconductor packages (and the BCBS
monograph) is on the bioconductor list.  I've cc'd the bioconductor
list, please send further replies or questions there.

> ##################################
> R>  hasChr <- eapply(GOTERM, function(x)
> +              x[grep("chromosome", Term(x))])
>
> Error in x[grep("chromosome", Term(x))] : object is not subsettable
> ##################################
>
> I have run the command in the (PPC) Mac OS X R 2.4.1 and (AMD Ubuntu)  
> Linux R 2.4.0 with the same result so it shouldn't be any  
> distribution-dependent problem. Obviously the "metaOverview.R" is not  
> up-to-date since I had few problems before as well (e.g. that a  
> function is in another package in BioC 1.9 etc.) but I was able to  
> "repair" everything myself. However, this one I don't understand.  
> Anyone can help? Some classes have changed?!

You are correct that this code is out of date.  The reason is actually
due to changes in R.  Since R 2.4.0, S4 classes now have their own
internal type and do not act like lists.  This is a very good thing,
but it means that some code that relied on it will break.

The elements of the GOTERM environment are instances of the "GOTerms"
class defined in the annotate package.  So taking a look at the old
code:

    hasChr <- eapply(GOTERM, function(x)
                 x[grep("chromosome", Term(x))])

This is looping over all GOTerms instances in the GOTERM environment
and calling grep on the term summary:

     grep("chromosome", Term(x))

Since Term(x) returns a character vector of length one, if a match is
found the return value of grep will be 1.  If no match is found it
will be integer(0) (a zero-length integer vector).

If x (the GOTerms instance) was a list, then we would have either x[1]
or x[integer(0)].  But since x is an S4 class with no "[" method
defined, you now get an error.

The code example is just trying to count the number of GO Terms that
have "chromosome" in their description.  You can achieve this as
follows:

    hasChr <- eapply(GOTERM, function(x)
                 length(grep("chromosome", Term(x))))
    sum(unlist(hasChr))



+ seth

--
Seth Falcon | Computational Biology | Fred Hutchinson Cancer Research Center
http://bioconductor.org


From nlobo at uchicago.edu  Thu Mar  1 18:51:29 2007
From: nlobo at uchicago.edu (Nameeta Lobo)
Date: Thu,  1 Mar 2007 11:51:29 -0600 (CST)
Subject: [R] Row-wise two sample T-test on subsets of a matrix
Message-ID: <20070301115129.AKS97495@m4500-02.uchicago.edu>

Hello all,

I am trying to run a two sample t-test on a matrix which is a
196002*22 matrix. I want to run the t-test, row-wise, with the
first 11 columns being a part of the first group and columns
12-22 being a part of the second group. 

I tried running something like (temp.matrix being my 196002*22
matrix)

t.test(temp.matrix[,1:11],temp.matrix[,12:22],paired=TRUE)

or somthing like

as.numeric(t.test(temp.matrix[,1:11],temp.matrix[,12:22],paired=TRUE)[[1]])
so as to only capture the t-value alone and 

and I get a result for the whole matrix instead of a row-wise
result. 

I want to avoid using a "for" loop to increment the number of
rows as it would take a huge amount of time.


Any suggestions would be really appreciated.

thanks
nameeta


From marc_schwartz at comcast.net  Thu Mar  1 18:55:26 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 01 Mar 2007 11:55:26 -0600
Subject: [R] setting font in plots
In-Reply-To: <BAY116-F27D7F5F2DE9924F5A4DE6EDA800@phx.gbl>
References: <BAY116-F27D7F5F2DE9924F5A4DE6EDA800@phx.gbl>
Message-ID: <1172771726.4897.71.camel@localhost.localdomain>

On Thu, 2007-03-01 at 23:48 +1000, Mark Allen wrote:
> Dear Reader
> 
> I am trying to change the font in a plot and after several trials finally 
> came up with the following code.
> 
> plot(var_a, var_b, pch = 16, font.lab = 10, font = 10)
> points(var_a, var_c, pch = 3, font = 10)
> legend(0.1, 0.8, legend = c(var_b, var_c), pch = c(16,3))
> 
> It does change the font in the plot (Courier), but not in the legend that 
> does not accept the "font = ..." argument. I could also not find a list with 
> font names and corresponding numbers.
> 
> Surely there must be a better way to do this that I cannot seem to find. 
> Looking at the R website I came across e.g. gpar() and par(), but 
> unfortunately could not make it work... Is there perhaps anyone that could 
> help me out here?
> 
> Thanks very much
> Mark Allen


Mark,

In ?par, note 'family', which can take on the values:

"serif", "sans", "mono", and "symbol", as well as supporting the Hershey
fonts.

So, if you want a mono font in the legend, use:

  par(family = "mono")

before your legend() call:

  plot(1:10)
  par(family = "mono")
  legend("topleft", legend = "This is a test")


HTH,

Marc Schwartz


From clint at ecy.wa.gov  Thu Mar  1 18:56:00 2007
From: clint at ecy.wa.gov (Clint Bowman)
Date: Thu, 1 Mar 2007 09:56:00 -0800 (PST)
Subject: [R] Double-banger function names: preferences and suggestions
In-Reply-To: <BAY116-DAV4AC499B924DC4285E9C03CF800@phx.gbl>
References: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>
	<BAY116-DAV4AC499B924DC4285E9C03CF800@phx.gbl>
Message-ID: <Pine.LNX.4.62.0703010953520.6451@aeolus.ecy.wa.gov>

I agree with Jason:  !2, prefer 1, can accept 3.

Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Dispersion Modeler		INTERNET:	clint at math.utah.edu
Air Quality Program		VOICE:		(360) 407-6815
Department of Ecology		FAX:		(360) 407-7534

	USPS:  		PO Box 47600, Olympia, WA 98504-7600
	Parcels:	300 Desmond Drive, Lacey, WA 98503-1274

On Thu, 1 Mar 2007, Jason Barnhart wrote:

> Definitely not #2.   Prefer #1 but #3 is ok as well.
>
> Thanks for contributing and inquiring.
>
>
> ----- Original Message -----
> From: "hadley wickham" <h.wickham at gmail.com>
> To: <R-help at r-project.org>
> Sent: Sunday, February 25, 2007 7:44 AM
> Subject: [R] Double-banger function names: preferences and suggestions
>
>
> > What do you prefer/recommend for double-banger function names:
> >
> > 1 scale.colour
> > 2 scale_colour
> > 3 scaleColour
> >
> > 1 is more R-like, but conflicts with S3.  2 is a modern version of
> > number 1, but not many packages use it.  Number 3 is more java-like.
> > (I like number 2 best)
> >
> > Any suggestions?
> >
> > Thanks,
> >
> > Hadley
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From hb at stat.berkeley.edu  Thu Mar  1 18:59:29 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Thu, 1 Mar 2007 09:59:29 -0800
Subject: [R] R File IO Slow?
In-Reply-To: <515147.48704.qm@web55609.mail.re4.yahoo.com>
References: <515147.48704.qm@web55609.mail.re4.yahoo.com>
Message-ID: <59d7961d0703010959l6033dfafy914492dce097a8c5@mail.gmail.com>

Just an idea: Two things that can slow down save()/load() is if you
save() in ASCII format or a compressed binary format.  If this is your
case for MYFILE, try to resave in a non-compressed binary format.  See
?save for details.

/HB

On 3/1/07, ramzi abboud <ramziabb at yahoo.com> wrote:
> Is R file IO slow in general or am I missing
> something?  It takes me 5 minutes to do a load(MYFILE)
> where MYFILE is a 27 MB Rdata file.  Is there any way
> to speed this up?
>
> The one idea I have is having R call a C or Perl
> routine, reading the file in that language, converting
> the data in to R objects, then sending them back into
> R.  This is more work that I want to do, however, in
> loading Rdata files.
>
> Any ideas would be appreciated.
> Ramzi Aboud
> University of Rochester
>
>
>
>
>
>
> ____________________________________________________________________________________
> Need Mail bonding?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rdpeng at gmail.com  Thu Mar  1 19:04:54 2007
From: rdpeng at gmail.com (Roger D. Peng)
Date: Thu, 01 Mar 2007 13:04:54 -0500
Subject: [R] R File IO Slow?
In-Reply-To: <515147.48704.qm@web55609.mail.re4.yahoo.com>
References: <515147.48704.qm@web55609.mail.re4.yahoo.com>
Message-ID: <45E715C6.3070702@gmail.com>

A 27MB .RData file is relatively big, in may experience.  What do you think is 
slow?  Maybe it's your computer that is slow?

-roger

ramzi abboud wrote:
> Is R file IO slow in general or am I missing
> something?  It takes me 5 minutes to do a load(MYFILE)
> where MYFILE is a 27 MB Rdata file.  Is there any way
> to speed this up?  
> 
> The one idea I have is having R call a C or Perl
> routine, reading the file in that language, converting
> the data in to R objects, then sending them back into
> R.  This is more work that I want to do, however, in
> loading Rdata files.
> 
> Any ideas would be appreciated.
> Ramzi Aboud
> University of Rochester
> 
> 
> 
> 
> 
>  
> ____________________________________________________________________________________
> Need Mail bonding?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/


From marc_schwartz at comcast.net  Thu Mar  1 19:11:39 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 01 Mar 2007 12:11:39 -0600
Subject: [R] matrices dimensions limitation
In-Reply-To: <JE8IUX$7EB8E4D804CF95E3B44D09B8899239BD@libero.it>
References: <JE8IUX$7EB8E4D804CF95E3B44D09B8899239BD@libero.it>
Message-ID: <1172772699.4897.79.camel@localhost.localdomain>

On Thu, 2007-03-01 at 18:20 +0100, Bruno C. wrote:
> Hello,
> I have several questions around matrices size limitation:
>      can i create a matrix with 700000 rows?
>      does it depends on the number of columns?
>      if so can Is there a way to ask to R, given the number of
> columns, the max number of rows I can have?
> 
> I need in fact to do regression prediction on a huge matrix
> (700000x1000)ca. ... 
> I will probably have to decompose it on several matrices, and I would
> do that in an intelligent way so that if my observation matrix size
> change I do not need to rewrite my code...
> 
> Thanx in advance

See ?"Memory-limits":

"There are also limits on individual objects. On all versions of R, the
maximum length (number of elements) of a vector is 2^31 - 1 ~ 2*10^9, as
lengths are stored as signed integers. In addition, the storage space
cannot exceed the address limit, and if you try to exceed that limit,
the error message begins cannot allocate vector of length. The number of
characters in a character string is in theory only limited by the
address space."


Keep in mind that a matrix is a vector with a 'dim' attribute.

I am running Linux on a system with 2Gb of RAM:

> MAT <- matrix(rnorm(700000 * 1000), ncol = 1000)
Error in rnorm(7e+05 * 1000) : cannot allocate vector of length 70000000

So hopefully, you have more RAM that I do...  :-)

HTH,

Marc Schwartz


From aiminy at iastate.edu  Thu Mar  1 19:17:09 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Thu, 01 Mar 2007 12:17:09 -0600
Subject: [R] number of levels for a factor
Message-ID: <6.2.3.4.2.20070301121038.037fea30@aiminy.mail.iastate.edu>

I have temp list which have 19 data.frame
I want to get number of levels for pr in the first dat.frame
I do this like this:
temp[[1]]$pr just has "1A24"
after I do nlevels(temp[[1]]$pr)
I expect to get 1, but I get 19

anyone know why?

 > tail(temp[[1]]$pr)
[1] 1A24 1A24 1A24 1A24 1A24 1A24
19 Levels: 1A24 1A57 1A5J 1A6X 1AB7 1AF8 1AFI 1AGG 1AH9 1AHL 1AJ3 1AJW ... 1AZK
 > nlevels(temp[[1]]$pr)
[1] 19

Aimin


From jimholtman at yahoo.com  Thu Mar  1 19:25:51 2007
From: jimholtman at yahoo.com (jim holtman)
Date: Thu, 1 Mar 2007 10:25:51 -0800 (PST)
Subject: [R] R File IO Slow?
Message-ID: <246842.21095.qm@web54007.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070301/7ee9f5be/attachment.pl 

From jimholtman at yahoo.com  Thu Mar  1 19:39:49 2007
From: jimholtman at yahoo.com (jim holtman)
Date: Thu, 1 Mar 2007 10:39:49 -0800 (PST)
Subject: [R] How to read in this data format?
Message-ID: <248216.24526.qm@web54009.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070301/3691c6ef/attachment.pl 

From marc_schwartz at comcast.net  Thu Mar  1 19:42:35 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 01 Mar 2007 12:42:35 -0600
Subject: [R] R File IO Slow?
In-Reply-To: <515147.48704.qm@web55609.mail.re4.yahoo.com>
References: <515147.48704.qm@web55609.mail.re4.yahoo.com>
Message-ID: <1172774555.4897.89.camel@localhost.localdomain>

On Thu, 2007-03-01 at 09:22 -0800, ramzi abboud wrote:
> Is R file IO slow in general or am I missing
> something?  It takes me 5 minutes to do a load(MYFILE)
> where MYFILE is a 27 MB Rdata file.  Is there any way
> to speed this up?  
> 
> The one idea I have is having R call a C or Perl
> routine, reading the file in that language, converting
> the data in to R objects, then sending them back into
> R.  This is more work that I want to do, however, in
> loading Rdata files.
> 
> Any ideas would be appreciated.
> Ramzi Aboud
> University of Rochester

Here are some timings on my system, which runs Linux on a 3.2 Ghz P4
with 2 Gb of RAM and a 7200 rpm HD. I typically get around 28 Mb/sec
throughput on this drive, which is about 15% lower than normal, as it is
an encrypted partition using 256 bit AES.


> Vec <- 1:15000000

> system.time(save(Vec, file = "Vec.RData"))
[1] 33.297  0.565 38.889  0.000  0.000

# File is ~29 Mb
> file.info("Vec.RData")$size
[1] 30112009

> system.time(load("Vec.RData"))
[1] 5.607 0.167 6.575 0.000 0.000


Not terribly burdensome...

You might want to be sure that you are not low on RAM, resulting in a
lot of swapping to disk, or perhaps just a slow drive.

HTH,

Marc Schwartz


From p.murrell at auckland.ac.nz  Thu Mar  1 19:46:01 2007
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 02 Mar 2007 07:46:01 +1300
Subject: [R] setting font in plots
In-Reply-To: <BAY116-F27D7F5F2DE9924F5A4DE6EDA800@phx.gbl>
References: <BAY116-F27D7F5F2DE9924F5A4DE6EDA800@phx.gbl>
Message-ID: <45E71F69.2050808@stat.auckland.ac.nz>

Hi


Mark Allen wrote:
> Dear Reader
> 
> I am trying to change the font in a plot and after several trials finally 
> came up with the following code.
> 
> plot(var_a, var_b, pch = 16, font.lab = 10, font = 10)
> points(var_a, var_c, pch = 3, font = 10)
> legend(0.1, 0.8, legend = c(?var_b?, ?var_c?), pch = c(16,3))
> 
> It does change the font in the plot (Courier), but not in the legend that 
> does not accept the "font = ..." argument. I could also not find a list with 
> font names and corresponding numbers.
> 
> Surely there must be a better way to do this that I cannot seem to find. 
> Looking at the R website I came across e.g. gpar() and par(), but 
> unfortunately could not make it work... Is there perhaps anyone that could 
> help me out here?


Try ...

par(family="mono")
var_a <- 1:10
var_b <- 1:10
var_c <- 10:1
plot(var_a, var_b, pch = 16)
points(var_a, var_c, pch = 3)
legend(1, 6, legend = c("var_b", "var_c"), pch = c(16,3))

... and take a look at "Fonts, Lines, and Transparency in R Graphics" in
http://cran.stat.auckland.ac.nz/doc/Rnews/Rnews_2004-2.pdf

Paul


> Thanks very much
> Mark Allen
> 
> _________________________________________________________________
> Advertisement: It's simple! Sell your car for just $20 at carsales.com.au
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From maitra at iastate.edu  Thu Mar  1 19:46:06 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Thu, 1 Mar 2007 12:46:06 -0600
Subject: [R] R File IO Slow?
In-Reply-To: <45E715C6.3070702@gmail.com>
References: <515147.48704.qm@web55609.mail.re4.yahoo.com>
	<45E715C6.3070702@gmail.com>
Message-ID: <20070301124606.470db1d2@triveni.stat.iastate.edu>

I decided to run an experiment: just reading in a file which is 78MB in binary format (of ints). It takes less than 30s using a laptop with 512 MB RAM, 2.3 GHz Intel-4 single processor. At that point, I did not notice that Ramzi was talking about a .RData file.

For huge files, I usually do not save my files. I run the R code whenever I need it: the entire exercise usually takes a few minutes, at the most. If something takes very long, I usually save the output into a file and read from there. I have found that this is more efficient (besides helping in reproducing my results).

HTH!
Ranjan


On Thu, 01 Mar 2007 13:04:54 -0500 "Roger D. Peng" <rdpeng at gmail.com> wrote:

> A 27MB .RData file is relatively big, in may experience.  What do you think is 
> slow?  Maybe it's your computer that is slow?
> 
> -roger
> 
> ramzi abboud wrote:
> > Is R file IO slow in general or am I missing
> > something?  It takes me 5 minutes to do a load(MYFILE)
> > where MYFILE is a 27 MB Rdata file.  Is there any way
> > to speed this up?  
> > 
> > The one idea I have is having R call a C or Perl
> > routine, reading the file in that language, converting
> > the data in to R objects, then sending them back into
> > R.  This is more work that I want to do, however, in
> > loading Rdata files.
> > 
> > Any ideas would be appreciated.
> > Ramzi Aboud
> > University of Rochester
> > 
> > 
> > 
> > 
> > 
> >  
> > ____________________________________________________________________________________
> > Need Mail bonding?
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> 
> -- 
> Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From maitra at iastate.edu  Thu Mar  1 19:48:29 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Thu, 1 Mar 2007 12:48:29 -0600
Subject: [R] Row-wise two sample T-test on subsets of a matrix
In-Reply-To: <20070301115129.AKS97495@m4500-02.uchicago.edu>
References: <20070301115129.AKS97495@m4500-02.uchicago.edu>
Message-ID: <20070301124829.3ab26ad9@triveni.stat.iastate.edu>

Here's one suggestion: convert the matrix into a three-dimensional array and use apply on it. 

Ranjan

On Thu,  1 Mar 2007 11:51:29 -0600 (CST) Nameeta Lobo <nlobo at uchicago.edu> wrote:

> Hello all,
> 
> I am trying to run a two sample t-test on a matrix which is a
> 196002*22 matrix. I want to run the t-test, row-wise, with the
> first 11 columns being a part of the first group and columns
> 12-22 being a part of the second group. 
> 
> I tried running something like (temp.matrix being my 196002*22
> matrix)
> 
> t.test(temp.matrix[,1:11],temp.matrix[,12:22],paired=TRUE)
> 
> or somthing like
> 
> as.numeric(t.test(temp.matrix[,1:11],temp.matrix[,12:22],paired=TRUE)[[1]])
> so as to only capture the t-value alone and 
> 
> and I get a result for the whole matrix instead of a row-wise
> result. 
> 
> I want to avoid using a "for" loop to increment the number of
> rows as it would take a huge amount of time.
> 
> 
> Any suggestions would be really appreciated.
> 
> thanks
> nameeta
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From p.dalgaard at biostat.ku.dk  Thu Mar  1 20:04:05 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 01 Mar 2007 20:04:05 +0100
Subject: [R] GLM problems.
In-Reply-To: <e5e455f20703010944v6d21dd8em5fb0f173310accbc@mail.gmail.com>
References: <e5e455f20703010944v6d21dd8em5fb0f173310accbc@mail.gmail.com>
Message-ID: <45E723A5.7030607@biostat.ku.dk>

Daniel Jimenez wrote:
> Dear R users I'm new in R management and maybe It's a silly question. I'm
> working with GLM to obtain predictive models. I have some problesm 
> with the
> prediction instruction:
>
>
>
>> DatosTotal <- read.csv("Var_perdizcsv.csv", sep =";")
>> edvariable <- edit(DatosTotal)
>> pre <- predict(rlfinal, DatosTotal, type = 'probs')
> Erro en match.arg(type) : 'arg' should be one of link, response, terms
What is unclear about that? From help(predict.glm):

    type: the type of prediction required.  The default is on the scale
          of the linear predictors; the alternative '"response"' is on
          the scale of the response variable.  Thus for a default
          binomial model the default predictions are of log-odds
          (probabilities on logit scale) and 'type = "response"' gives
          the predicted probabilities.  The '"terms"' option returns a
          matrix giving the fitted values of each term in the model
          formula on the linear predictor scale.


From bunny at lautloscrew.com  Thu Mar  1 20:08:37 2007
From: bunny at lautloscrew.com (bunny , lautloscrew.com)
Date: Thu, 1 Mar 2007 20:08:37 +0100
Subject: [R] problem with throwing lines out of matrix
Message-ID: <14958B1E-DAFA-447C-8AA0-441E72FEC80F@lautloscrew.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070301/73dbf896/attachment.pl 

From stef at biostatistics.it  Thu Mar  1 20:21:16 2007
From: stef at biostatistics.it (Stefano Calza)
Date: Thu, 1 Mar 2007 20:21:16 +0100
Subject: [R] Row-wise two sample T-test on subsets of a matrix
In-Reply-To: <20070301124829.3ab26ad9@triveni.stat.iastate.edu>
References: <20070301115129.AKS97495@m4500-02.uchicago.edu> 
	<20070301124829.3ab26ad9@triveni.stat.iastate.edu>
Message-ID: <20070301192116.GE27296@med.unibs.it>

I'd suggest to use the function mt.teststat in the package multtest or rowttests in the package genefilter. Both can be found athe the bioconductor webpage (www.bioconductor.org)

Stef	


On Thu, Mar 01, 2007 at 12:48:29PM -0600, Ranjan Maitra wrote:
<Ranjan>Here's one suggestion: convert the matrix into a three-dimensional array and use apply on it. 
<Ranjan>
<Ranjan>Ranjan
<Ranjan>
<Ranjan>On Thu,  1 Mar 2007 11:51:29 -0600 (CST) Nameeta Lobo <nlobo a uchicago.edu> wrote:
<Ranjan>
<Ranjan>> Hello all,
<Ranjan>> 
<Ranjan>> I am trying to run a two sample t-test on a matrix which is a
<Ranjan>> 196002*22 matrix. I want to run the t-test, row-wise, with the
<Ranjan>> first 11 columns being a part of the first group and columns
<Ranjan>> 12-22 being a part of the second group. 
<Ranjan>> 
<Ranjan>> I tried running something like (temp.matrix being my 196002*22
<Ranjan>> matrix)
<Ranjan>> 
<Ranjan>> t.test(temp.matrix[,1:11],temp.matrix[,12:22],paired=TRUE)
<Ranjan>> 
<Ranjan>> or somthing like
<Ranjan>> 
<Ranjan>> as.numeric(t.test(temp.matrix[,1:11],temp.matrix[,12:22],paired=TRUE)[[1]])
<Ranjan>> so as to only capture the t-value alone and 
<Ranjan>> 
<Ranjan>> and I get a result for the whole matrix instead of a row-wise
<Ranjan>> result. 
<Ranjan>> 
<Ranjan>> I want to avoid using a "for" loop to increment the number of
<Ranjan>> rows as it would take a huge amount of time.
<Ranjan>> 
<Ranjan>> 
<Ranjan>> Any suggestions would be really appreciated.
<Ranjan>> 
<Ranjan>> thanks
<Ranjan>> nameeta
<Ranjan>> 
<Ranjan>> ______________________________________________
<Ranjan>> R-help a stat.math.ethz.ch mailing list
<Ranjan>> https://stat.ethz.ch/mailman/listinfo/r-help
<Ranjan>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
<Ranjan>> and provide commented, minimal, self-contained, reproducible code.
<Ranjan>>
<Ranjan>
<Ranjan>______________________________________________
<Ranjan>R-help a stat.math.ethz.ch mailing list
<Ranjan>https://stat.ethz.ch/mailman/listinfo/r-help
<Ranjan>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
<Ranjan>and provide commented, minimal, self-contained, reproducible code.


From thomas.friedrichsmeier at ruhr-uni-bochum.de  Thu Mar  1 20:40:41 2007
From: thomas.friedrichsmeier at ruhr-uni-bochum.de (Thomas Friedrichsmeier)
Date: Thu, 1 Mar 2007 20:40:41 +0100
Subject: [R] Default par() options
Message-ID: <200703012040.46329.thomas.friedrichsmeier@ruhr-uni-bochum.de>

The following question/idea came up on the RKWard development mailing list, 
but might be of general interest:

Is there a nice way to customize the default look of all graphs on all 
devices? I.e. a way to - for instance - set the following options before each 
plot:

par(bg="light gray", las=2, pch=19)

As far as I have found, there would currently be two ways to do this:
1) Adding the above statement manually after opening the device, and before 
starting the plot. It could of course be wrapped inside a custom function to 
save some typing, but you'd still need to make sure to always add the 
command.

2) Overriding all device functions with something like:
X11 <- function (...) {
	grDevices::X11 (...)
	par ([custom options])
}
This would be feasible, but feels rather dirty. Also, something substantially 
more elaborate would be needed to honor e.g. fonts and bg arguments, if 
explicitely specified in the call to X11. Would have to be done for each 
device separately.

Does a third, more elegant solution exist?

If not, would the following idea have any chances of being added to R?

Create a new options("par.default"), similar to the already existing 
options("par.ask.default"). This would take a list of par() options to set a 
default value for, like e.g.:

options(par.default=list(bg="light gray", las=2, pch=19))

Only those options would need to be specified in the list, for which you 
actually want to set a default different from the built-in. Options 
explicitely specified in X11(), plot(), additional calls to par(), etc. would 
take precedence over options("par.default").

Regards
Thomas Friedrichsmeier
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070301/13fb8bea/attachment.bin 

From matthew_wiener at merck.com  Thu Mar  1 20:46:43 2007
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Thu, 1 Mar 2007 14:46:43 -0500
Subject: [R] problem with throwing lines out of matrix  [Broadcast]
In-Reply-To: <14958B1E-DAFA-447C-8AA0-441E72FEC80F@lautloscrew.com>
References: <14958B1E-DAFA-447C-8AA0-441E72FEC80F@lautloscrew.com>
Message-ID: <4E9A692D8755DF478B56A2892388EE1F018E67AD@usctmx1118.merck.com>

You didn't' actually tell us what forfact is.  You could try something like (untested)

My.matrix <- my.matrix[!(my.matrix[, 1] %in% remove.values), , drop = FALSE]

Also, reading some of the introductory R manuals you can find on CRAN in the documentation section will really give you a leg up on some of these things.

Hope this helps.

Matt Wiener


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bunny , lautloscrew.com
Sent: Thursday, March 01, 2007 2:09 PM
To: r-help at stat.math.ethz.ch
Subject: [R] problem with throwing lines out of matrix [Broadcast]

Dear all,

again i have a problem with special case of dropping lines out of a  
matrix.

i tried the following, where throwout is a vector of length 18 with  
simple id values that should be compared to any

for (k in 1:length(throwout))
{
mymatrix=matrix(mymatrix[-(forfact[k]),],ncol=4)	
	}


this didnt work and i tried the following to find the error:

mymatrix[,1][mymatrix[,1]==throwout[7]]
this returned me a nice value - everything fine...
but other same other values for throwout didnt work - because the  
throwout value simply is not in this matrix.
but i need the same throwout vector for different matrices - so i  
need to use it.

does anybody have an idea how i can throw out all lines in every of  
my 5 matrices that contain values of the throwout in their first  
column ?
and how to deal with the problem that not every matrix contains all  
throwout values in its first column...

maybe this might help:
in the end all matrices should have the same number of lines.



thx in advance, i?ll keep on trying ;)



	[[alternative HTML version deleted]]



------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From fisk at bowdoin.edu  Thu Mar  1 20:52:40 2007
From: fisk at bowdoin.edu (steve)
Date: Thu, 01 Mar 2007 14:52:40 -0500
Subject: [R] question about xtable and Hmisc
In-Reply-To: <loom.20070301T165510-843@post.gmane.org>
References: <es6o3t$bhu$1@sea.gmane.org>
	<loom.20070301T165510-843@post.gmane.org>
Message-ID: <es7atr$cp3$1@sea.gmane.org>

Unfortunately, this applies to print.xtable, and not to latex. I want to 
know how to eliminate them using latex()


Greg Johnson wrote:
> steve <fisk <at> bowdoin.edu> writes:
> 
>> I would like to get rid of the row numbers using xtable and latex.
>>
>> but I don't want the row numbers. Is it possible to get rid of them?
>>
>> Also, if x is a data frame, latex(x) contains the row numbers.
>> Can I get rid of them here as well?
>>
>> Steve
> 
> 
> Steve,
> 
> ?print.xtable
> 
> look at the include.rownames option.
> 
> Greg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From brown_emu at yahoo.com  Thu Mar  1 20:55:08 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Thu, 1 Mar 2007 11:55:08 -0800 (PST)
Subject: [R] count the # of appearances...
In-Reply-To: <XFMail.070301151123.ted.harding@nessie.mcc.ac.uk>
Message-ID: <293116.75970.qm@web39704.mail.mud.yahoo.com>

You can split() the original vector according to its elements into a list,
then use lapply() to count the lengths of the list:

> x <- round(runif(100,1,10))
> unlist(lapply(split(x,f=x),length))
 1  2  3  4  5  6  7  8  9 10 
 5 14  9 13 11  8  8 14 12  6 

This can be useful if you want to use some criteria for counting, and then
you can replace length() with your own function.

Best regards,

ST


--- ted.harding at nessie.mcc.ac.uk wrote:

> On 01-Mar-07 bunny , lautloscrew.com wrote:
> > Hi there,
> > 
> > is there a possibility to count the number of appearances of an  
> > element in a vector ?
> > i mean of any given element.. deliver all elements which are exactly  
> > xtimes in this vector ?
> > 
> > thx in advance !!
> 
> If it is a specific element value which you designate beforehand,
> then Ranjan Maitra's method
> 
>   sum(your vector == your chosen element)
> 
> will of course give you the number of times this occurs.
> 
> However, your query suggests this may not be what you want.
> 
> First, if you do not designate a specific element, then table()
> can give you counts of repetitions of all the distinct elements.
> For example:
> 
> ## Generate the vector y with repetitions of elements of x:
> > x<-round(runif(10),digits=3)
> > y<-sample(x,25,replace=TRUE)
> 
> ## Count the numbers of repetitions
> > x<-round(runif(10),digits=3)
> > y<-sample(x,25,replace=TRUE)
> > table(y)
> ## y
> ## 0.122 0.372 0.431 0.486 0.523 0.858 0.886 0.948 
> ##    5     4     5     3     3     2     2     1 
> 
> Second: You ask for a method to "deliver all elements which are
> exactly xtimes in this vector". So suppose "xtimes" is a given
> number of times, and you want to know all elements which each
> occur xtimes times in the vector y.
> 
> You could base a method for this on the ouput of table()
> as above (look at "?table" for the background).
> 
>   counts <- as.data.frame(table(y))
>   counts
> ##       y Freq
> ## 1 0.122    5
> ## 2 0.372    4
> ## 3 0.431    5
> ## 4 0.486    3
> ## 5 0.523    3
> ## 6 0.858    2
> ## 7 0.886    2
> ## 8 0.948    1
> 
> so, if your "xtimes" is say 3, then
> 
>   counts$y[counts$Freq==3]
> ## [1] 0.486 0.523
> ## Levels: 0.122 0.372 0.431 0.486 0.523 0.858 0.886 0.948
> 
> showing that elements "0.486" and "0.523" occurred 3 times each
> in the vector y.
> 
> Hoping this helps,
> Ted.
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 01-Mar-07                                       Time: 15:11:09
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Mike.Prager at noaa.gov  Thu Mar  1 20:01:12 2007
From: Mike.Prager at noaa.gov (Michael H. Prager)
Date: Thu, 01 Mar 2007 14:01:12 -0500
Subject: [R] [R-pkgs] Update of X2R sent to CRAN
Message-ID: <45E722F8.5060206@noaa.gov>

A new version of X2R has just been uploaded to CRAN.  It should be 
available at mirrors within a few days.

This contains revisions to the For2R component to fix a bug in which 
data frames were not written correctly if the user did not pass row labels.

The new version is supplied as files X2R.zip and X2R.tar.gz, which are 
equivalent.  The version can be identified from the contents of file 
"VersionInfo.txt" in the root of each archive.  The new version is dated 
March 1, 2007.

-----

 From the original announcement:

X2R is composed of three related software libraries: C2R,  ADMB2R, and 
For2R (together, X2R). Each contains output routines to simplify 
transfer of complicated data structures from models written in a 
compiled language to R.  The user's data can be written as a structured 
ASCII file which, when read by R (note 1) with a single dget() function 
call, will become an R data object of type list. The list, may contain 
components such as data frames, matrices, and other lists.

These are NOT R packages; rather they are subroutine libraries to be 
used with programmers' own modeling codes.  Limited testing indicates 
that they are compatible with S-PLUS, as well (note 1, note 2).

Languages supported are Fortran 95 (with For2R), C and C++ (with C2R) 
and AD Model Builder (with ADMB2R) (note 1, note 3).  Source code and 
users' manuals are supplied.

This work has been tested and used by the authors. However, any software 
may contain bugs, and these works are classified by NOAA as  
"Experimental Products."  Although the software is supplied with no 
warranty whatsoever, bug reports, suggestions, and extensions are 
solicited (send to Prager or Martin).  The authors will attempt to fix 
all bugs promptly and to add requested features.

Software is now available at CRAN,  http://cran.r-project.org/ .  Look 
under "Software / Other" for the current X2R distribution.


Michael H. Prager - mike.prager at noaa.gov
Andi Stephens
Southeast Fisheries Science Center
National Marine Fisheries Service, NOAA
101 Pivers Island Road
Beaufort, North Carolina 28516 USA

Jennifer L. Martin - jennifer.martin at noaa.gov
Northeast Fisheries Science Center
National Marine Fisheries Service, NOAA
166 Water Street
Woods Hole, Massachusetts 02543 USA


* Note 1.  Use of product names (commercial or otherwise) does not imply 
endorsement or recommendation by any U.S. government agency, nor by the 
authors in their government capacities.
* Note 2.  S-PLUS is a commercial product, released by Insightful 
Corporation.
* Note 3.  AD Model Builder is a commercial product, released by Otter 
Research.

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From jyan at stat.uiowa.edu  Thu Mar  1 21:17:10 2007
From: jyan at stat.uiowa.edu (Jun Yan)
Date: Thu, 1 Mar 2007 14:17:10 -0600
Subject: [R] Fit Student Copula
In-Reply-To: <20070301170128.51897.qmail@web26313.mail.ukl.yahoo.com>
References: <20070301170128.51897.qmail@web26313.mail.ukl.yahoo.com>
Message-ID: <591fc1b70703011217l4ec027d0k5c4aab0087386e54@mail.gmail.com>

> set.seed(1)
> student.cop <- ellipCopula("t", param = c(0.5, 0.6, 0.7), dim = 3, dispstr = "un",df=5)
> x<-rcopula(student.cop,1000)
> fit <- fitCopula(x, student.cop, c(0.5,0.5,0.5,5))
Warning messages:
1: NaNs produced in: qt(p, df, lower.tail, log.p)
2: NaNs produced in: sqrt((df + d)/(Q + df))
3: NaNs produced in: sqrt((df + d)/(Q + df))
4: NaNs produced in: sqrt((df + d)/(Q + df))
5: NaNs produced in: sqrt((df + d)/(Q + df))
> fit
The ML estimation is based on  1000  observations.
       Estimate Std. Error   z value     Pr(>|z|)
rho.1 0.4824018 0.02460306 19.607386 0.000000e+00
rho.2 0.6076102 0.01993649 30.477289 0.000000e+00
rho.3 0.6938202 0.01596493 43.459018 0.000000e+00
df    5.2299217 0.64155266  8.151976 4.440892e-16
The maximized loglikelihood is  665.585
The convergence code is  0


On 3/1/07, pierre clauss <pierreclauss at yahoo.fr> wrote:
> Hello everybody,
> I have a big problem that I do not manage to solve !
> I will be very grateful if you can solve this !
>
> I want to fit a t Copula with the copula package :
>
> > student.cop <- ellipCopula("t", param = c(0.5, 0.6, 0.7), dim = 3, dispstr = "un",df=5)
> > x<-rcopula(student.cop,1000)
> > fit <- fitCopula(x, student.cop, c(0.5,0.5,0.5,5))
>
> And there is an error for the optimization.
>
> Thanks a lot if you respond to me !
> Pierre.
>
>
>
>
>
>
> ___________________________________________________________________________
> D?couvrez une nouvelle fa?on d'obtenir des r?ponses ? toutes vos questions !
> Profitez des connaissances, des opinions et des exp?riences des internautes sur Yahoo! Questions/R?ponses
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
Jun Yan
Assistant Professor
Department of Statistics and
 Actuarial Science                             Voice: 319-335-0824
University of Iowa                               Fax: 319-335-3017
219 Schaeffer Hall                          Email: j-yan at uiowa.edu
Iowa City, IA 52242           Web: http://www.stat.uiowa.edu/~jyan


From jimholtman at yahoo.com  Thu Mar  1 21:21:19 2007
From: jimholtman at yahoo.com (jim holtman)
Date: Thu, 1 Mar 2007 12:21:19 -0800 (PST)
Subject: [R] Query about data manipulation
Message-ID: <956454.42210.qm@web54011.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070301/8a3e4e46/attachment.pl 

From Bartjoosen at hotmail.com  Thu Mar  1 21:28:49 2007
From: Bartjoosen at hotmail.com (Bart Joosen)
Date: Thu, 1 Mar 2007 21:28:49 +0100
Subject: [R] How to read in this data format?
References: <BAY134-F394F6FDE418B617D16A196D8800@phx.gbl>
	<971536df0703010935r6abcd474v839a3c765c796699@mail.gmail.com>
Message-ID: <BAY134-DAV122D0BF3F65D8B567DFF32D8800@phx.gbl>

Dear All,

thanks for the replies, Jim Holtman has given a solution which fits my 
needs, but Gabor Grothendieck did the same thing,
but it looks like the coding will allow faster processing (should check this 
out tomorrow on a big datafile).

@gabor: I don't understand the use of the grep command:
        grep("^[1-9][0-9. ]*$|Time", Lines., value = TRUE)
What is this expression  ("^[1-9][0-9. ]*$|Time") actually doing?
I looked in the help page, but couldn't find a suitable answer.


Thanks to All


Bart

----- Original Message ----- 
From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
To: "Bart Joosen" <bartjoosen at hotmail.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Thursday, March 01, 2007 6:35 PM
Subject: Re: [R] How to read in this data format?


> Read in the data using readLines, extract out
> all desired lines (namely those containing only
> numbers, dots and spaces or those with the
> word Time) and remove Retention from all
> lines so that all remaining lines have two
> fields.  Now that we have desired lines
> and all lines have two fields read them in
> using read.table.
>
> Finally, split them into groups and restructure
> them using "by" and in the last line we
> convert the "by" output to a data frame.
>
> At the end we display an alternate function f
> for use with by should we wish to generate long
> rather than wide output (using the terminology
> of the reshape command).
>
>
> Lines <- "$$ Experiment Number:
> $$ Associated Data:
>
> FUNCTION 1
>
> Scan            1
> Retention Time  0.017
>
> 399.8112        184
> 399.8742        0
> 399.9372        152
> ....
>
> Scan            2
> Retention Time  0.021
>
> 399.8112        181
> 399.8742        1
> 399.9372        153
> "
>
> # replace next line with: Lines. <- readLines("myfile.dat")
> Lines. <- readLines(textConnection(Lines))
> Lines. <- grep("^[1-9][0-9. ]*$|Time", Lines., value = TRUE)
> Lines. <- gsub("Retention", "", Lines.)
>
> DF <- read.table(textConnection(Lines.), as.is = TRUE)
> closeAllConnections()
>
> f <- function(x) c(id = x[1,2], structure(x[-1,2], .Names = x[-1,1]))
> out.by <- by(DF, cumsum(DF[,1] == "Time"), f)
> as.data.frame(do.call("rbind", out.by))
>
>
> We could alternately consider producing long
> format by replacing the function f with:
>
> f <- function(x) data.frame(x[-1,], id = x[1,2])
>
>
> On 3/1/07, Bart Joosen <bartjoosen at hotmail.com> wrote:
>> Hi,
>>
>> I recieved an ascii file, containing following information:
>>
>> $$ Experiment Number:
>> $$ Associated Data:
>>
>> FUNCTION 1
>>
>> Scan            1
>> Retention Time  0.017
>>
>> 399.8112        184
>> 399.8742        0
>> 399.9372        152
>> ....
>>
>> Scan            2
>> Retention Time  0.021
>>
>> 399.8112        181
>> 399.8742        1
>> 399.9372        153
>> .....
>>
>>
>> I would like to import this data in R into a dataframe, where there is a
>> column time, the first numbers as column names, and the second numbers as
>> data in the dataframe:
>>
>> Time    399.8112        399.8742        399.9372
>> 0.017   184     0       152
>> 0.021   181     1       153
>>
>> I did take a look at the read.table, read.delim, scan, ... But I 've no 
>> idea
>> about how to solve this problem.
>>
>> Anyone?
>>
>>
>> Thanks
>>
>> Bart
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From skiadas at hanover.edu  Thu Mar  1 21:30:27 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Thu, 1 Mar 2007 15:30:27 -0500
Subject: [R] question about xtable and Hmisc
In-Reply-To: <3FCE6E6A-C3F1-41EB-A896-E617765793DF@hanover.edu>
References: <es6o3t$bhu$1@sea.gmane.org>
	<loom.20070301T165510-843@post.gmane.org>
	<es7atr$cp3$1@sea.gmane.org>
	<3FCE6E6A-C3F1-41EB-A896-E617765793DF@hanover.edu>
Message-ID: <A92BE2BD-2DE7-4A87-968D-E208439C513E@hanover.edu>

Sorry, meant for this to go to the whole list.

On Mar 1, 2007, at 3:29 PM, Charilaos Skiadas wrote:

> On Mar 1, 2007, at 2:52 PM, steve wrote:
>
>> Unfortunately, this applies to print.xtable, and not to latex. I  
>> want to
>> know how to eliminate them using latex()
>
> 1) Why do you need to use latex() instead of print.xtable?
> 2) If you want to use latex(), then why are you using xtable at  
> all, instead of latex(d) directly?

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From klaster at karlin.mff.cuni.cz  Thu Mar  1 21:33:13 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Thu, 01 Mar 2007 21:33:13 +0100
Subject: [R] Row-wise two sample T-test on subsets of a matrix
In-Reply-To: <20070301124829.3ab26ad9@triveni.stat.iastate.edu>
References: <20070301115129.AKS97495@m4500-02.uchicago.edu>
	<20070301124829.3ab26ad9@triveni.stat.iastate.edu>
Message-ID: <45E73889.1080103@karlin.mff.cuni.cz>

Ranjan Maitra napsal(a):
> Here's one suggestion: convert the matrix into a three-dimensional array and use apply on it. 

Converting to 3 dims should not be neccessary:
m <- matrix(rnorm(110),ncol=22)
t.list <- apply(m,1,function(x){t.test(x[1:11],x[12:22],paired=TRUE)})

However, I have no idea how much time it takes with almost 200.000 rows...
Petr

> 
> Ranjan
> 
> On Thu,  1 Mar 2007 11:51:29 -0600 (CST) Nameeta Lobo <nlobo at uchicago.edu> wrote:
> 
>> Hello all,
>>
>> I am trying to run a two sample t-test on a matrix which is a
>> 196002*22 matrix. I want to run the t-test, row-wise, with the
>> first 11 columns being a part of the first group and columns
>> 12-22 being a part of the second group. 
>>
>> I tried running something like (temp.matrix being my 196002*22
>> matrix)
>>
>> t.test(temp.matrix[,1:11],temp.matrix[,12:22],paired=TRUE)
>>
>> or somthing like
>>
>> as.numeric(t.test(temp.matrix[,1:11],temp.matrix[,12:22],paired=TRUE)[[1]])
>> so as to only capture the t-value alone and 
>>
>> and I get a result for the whole matrix instead of a row-wise
>> result. 
>>
>> I want to avoid using a "for" loop to increment the number of
>> rows as it would take a huge amount of time.
>>
>>
>> Any suggestions would be really appreciated.
>>
>> thanks
>> nameeta
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From lalithaviswanath at yahoo.com  Thu Mar  1 21:35:46 2007
From: lalithaviswanath at yahoo.com (lalitha viswanath)
Date: Thu, 1 Mar 2007 12:35:46 -0800 (PST)
Subject: [R] Query about data manipulation
In-Reply-To: <1172769516.4897.64.camel@localhost.localdomain>
Message-ID: <752252.69071.qm@web43136.mail.sp1.yahoo.com>

Hi
Thanks much for that input.It was extremely helpful.

I am seeking some input about another stumbling block
using RODBC; SQLQuery et al with large BLOB values.

Although the following query 
dataFromDB <- sqlQuery(channel, "select
uncompress(columnName) from tableName where Id=id ");
returns just one row , dataFromDB[1,1] actually
contains 4000+ rows of the form 
field1 \t field2 \t value\n.... described earlier.
(4000+ rows compressed as one long string)

On printing dataFromDB[1,1], it does not print beyond
3600 such rows or so (printing in fact "field1 \t
field2 \t value \n.....field3600 \t field3601"),
abruptly missing the rest of the result. 

Hence it throws an error when I try to use read.table
(after using textConnection as suggested) that row xyz
does not contain 3 values,etc.

It seems to be missing 1/4th of the actual result that
should contain 4000+ such pairs.

The set of 4000+ rows occupy just 100KB if written out
to a file directly from MySQL.
Is there anyway to increase the capacity of the return
result in R so that it does not get thrown off as
above and retrieves the ENTIRE result?

I tried increasing buffsize, but as I understand,
since SqlQuery itself returns just one row in this
case, it  is possibly not very relevant here?

Note that the above mentioned problem does not arise
when the data returned from SQL query contains less
than 3500 such concatenated entries.

Your input is greatly appreciated.
Thanks
Lalitha
--- Marc Schwartz <marc_schwartz at comcast.net> wrote:

> On Thu, 2007-03-01 at 08:34 -0800, lalitha viswanath
> wrote:
> > Hi
> > Thanks much for the prompt response to my earlier
> > enquiry on packages for regression analyses.
> > Along the same topic(?), I have another question
> about
> > which I could use some input.
> > 
> > I am retreiving data from a MySQL database using
> > RODBC. 
> > The table has many BLOB columns and each BLOB
> column
> > has data in the format
> > "id1 \t id2 \t measure \n id3 \t id4 \t
> measure...."
> > (i.e. multiple rows compressed as one long string)
> > 
> > I am retreiving them as follows.
> > 
> > dataFromDB <- sqlQuery(channel, "select
> > uncompress(columnName) from tableName");
> > 
> > 
> > I am looking for ways to convert this long
> "string"
> > into a table/dataframe in R, making it easier for
> > further post processing etc without
> reading/writing it
> > to a file first.
> > 
> > Although by doing write.table and reading it in
> again,
> > I got the result in a data frame, with the \t and
> \n
> > interpreted correctly, I wish to sidestep this as
> I
> > need to carry out this analyses for over 4 million
> > such entries.
> > I tried 
> > write.table(dataFromDB, file="FileName");
> > dataFromFile <- read.table(FileName, sep="\t") 
> > dataFromFile is of the form
> > 
> > 92_8_nmenA      993_7_mpul      1.042444
> > 92_8_nmenA      3_5_cpneuA      0.900939
> > 190_1_rpxx      34_4_ctraM      0.822532
> > 190_1_rpxx      781_6_pmul      0.870016
> > 
> > Your input on the above is greatly appreciated.
> > Thanks
> > Lalitha
> 
> The easiest way might be to use a textConnection().
> 
> Let's say that you have read in your data as above
> and you have a column
> called 'blob':
> 
> > dataFromDB
>                                             blob
> 1 id1 \t id2 \t measure \n id3 \t id4 \t measure
> 
> 
> #Open textConnection.  Note coercion to character
> BLOB <-
> textConnection(as.character(dataFromDB$blob))
> 
> # Read in the column
> DF <- read.table(BLOB, sep = "\t")
> 
> # Close the connection
> close(BLOB)
> 
> 
> > DF
>      V1    V2        V3
> 1  id1   id2   measure 
> 2  id3   id4   measure
> 
> 
> See ?textConnection
> 
> HTH,
> 
> Marc Schwartz
> 
> 
> 



 
____________________________________________________________________________________
Any questions? Get answers on any topic at www.Answers.yahoo.com.  Try it now.


From klaster at karlin.mff.cuni.cz  Thu Mar  1 21:50:56 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Thu, 01 Mar 2007 21:50:56 +0100
Subject: [R] Default par() options
In-Reply-To: <200703012040.46329.thomas.friedrichsmeier@ruhr-uni-bochum.de>
References: <200703012040.46329.thomas.friedrichsmeier@ruhr-uni-bochum.de>
Message-ID: <45E73CB0.3020501@karlin.mff.cuni.cz>

I am no expert on these topics but currently I am solving a similar 
issue using the .Rprofile file and the .First function. So maybe it's 
enough to put
.First <- function(){
par(whatever you want)
further instructions if neccessary
}

Petr

Thomas Friedrichsmeier napsal(a):
> The following question/idea came up on the RKWard development mailing list, 
> but might be of general interest:
> 
> Is there a nice way to customize the default look of all graphs on all 
> devices? I.e. a way to - for instance - set the following options before each 
> plot:
> 
> par(bg="light gray", las=2, pch=19)
> 
> As far as I have found, there would currently be two ways to do this:
> 1) Adding the above statement manually after opening the device, and before 
> starting the plot. It could of course be wrapped inside a custom function to 
> save some typing, but you'd still need to make sure to always add the 
> command.
> 
> 2) Overriding all device functions with something like:
> X11 <- function (...) {
> 	grDevices::X11 (...)
> 	par ([custom options])
> }
> This would be feasible, but feels rather dirty. Also, something substantially 
> more elaborate would be needed to honor e.g. fonts and bg arguments, if 
> explicitely specified in the call to X11. Would have to be done for each 
> device separately.
> 
> Does a third, more elegant solution exist?
> 
> If not, would the following idea have any chances of being added to R?
> 
> Create a new options("par.default"), similar to the already existing 
> options("par.ask.default"). This would take a list of par() options to set a 
> default value for, like e.g.:
> 
> options(par.default=list(bg="light gray", las=2, pch=19))
> 
> Only those options would need to be specified in the list, for which you 
> actually want to set a default different from the built-in. Options 
> explicitely specified in X11(), plot(), additional calls to par(), etc. would 
> take precedence over options("par.default").
> 
> Regards
> Thomas Friedrichsmeier
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From skiadas at hanover.edu  Thu Mar  1 21:55:52 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Thu, 1 Mar 2007 15:55:52 -0500
Subject: [R] R code for "Statistical Models in S" ?
Message-ID: <FE68A329-25B8-4014-A090-BB9436D33489@hanover.edu>

I just acquired a copy of "Statistical Models in S", I guess most  
commonly known as the "white book", and realized to my dismay that  
most of the code is not directly executable in R, and I was wondering  
if there was a source discussing the things that are different and  
what the new ways of calling things are.

For instance, the first obstacle was the solder.balance data set. I  
found a "solder" data set in rpart, which is very close to it except  
for the fact that the Panel variable is not a factor, but that's  
easily fixed.
The first problem is the next two calls, on pages 2 and 3. One is  
"plot(solder.balance)", which is supposed to produce a very different  
plot than it does in R (I actually don't know the name of the plot,  
which is part of the problem I guess). Then one is supposed to call  
"plot.factor(skips ~ Opening + Mask)", which I took to mean:
"plot(skips ~ Opening + Mask, data=solder)", and that worked, though  
I still haven't been able to make a direct call to plot.factor work  
(I keep getting a "could not find function plot.factor" error).

Anyway, just wondered whether there is some page somewhere that  
discusses these little differences here and there, as I am sure there  
will be a number of other problems such as these along the way.

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From topkatz at msn.com  Thu Mar  1 21:57:51 2007
From: topkatz at msn.com (Talbot Katz)
Date: Thu, 01 Mar 2007 15:57:51 -0500
Subject: [R] Preserving order in an intersection
Message-ID: <BAY132-F1610CBB2B7618D5AD4F54FAA800@phx.gbl>

Hi.

Here's an odd request that actually arose out of my own bad planning.

Suppose I do the following:

which(v1 %in% v2)

I will get a set of indices for v1, and they will be ordered in the same 
order that v1 is in.  I want the indices of the intersection for v1 ordered 
according to v2.

I do this as follows, and it works in my particular case, although it looks 
like it might not work in general (i.e., if some of the v1 entries are not 
legal as names):

v1io1 = which(v1 %in% v2)
names(v1io1) = v1[v1io1]
v1io2 = v1io1[intersect(v2,v1)]

Is there an easier (or at least easy), reliable way to do this?  Thanks!

--  TMK  --
212-460-5430	home
917-656-5351	cell


From gunter.berton at gene.com  Thu Mar  1 22:23:47 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Thu, 1 Mar 2007 13:23:47 -0800
Subject: [R] R code for "Statistical Models in S" ?
In-Reply-To: <FE68A329-25B8-4014-A090-BB9436D33489@hanover.edu>
Message-ID: <002c01c75c47$e613f710$4d908980@gne.windows.gene.com>

The White Book provides the original S Language Specification. This was what
existed at Bell labs way back then. Subsequent implementations, both S-Plus
and R, will differ on details.

Also, a lot of development effort has flowed over the dam since publication,
so both implementations contain lots of stuff not even mentioned there.See
also the Green book.

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Charilaos Skiadas
Sent: Thursday, March 01, 2007 12:56 PM
To: R-Mailingliste
Subject: [R] R code for "Statistical Models in S" ?

I just acquired a copy of "Statistical Models in S", I guess most  
commonly known as the "white book", and realized to my dismay that  
most of the code is not directly executable in R, and I was wondering  
if there was a source discussing the things that are different and  
what the new ways of calling things are.

For instance, the first obstacle was the solder.balance data set. I  
found a "solder" data set in rpart, which is very close to it except  
for the fact that the Panel variable is not a factor, but that's  
easily fixed.
The first problem is the next two calls, on pages 2 and 3. One is  
"plot(solder.balance)", which is supposed to produce a very different  
plot than it does in R (I actually don't know the name of the plot,  
which is part of the problem I guess). Then one is supposed to call  
"plot.factor(skips ~ Opening + Mask)", which I took to mean:
"plot(skips ~ Opening + Mask, data=solder)", and that worked, though  
I still haven't been able to make a direct call to plot.factor work  
(I keep getting a "could not find function plot.factor" error).

Anyway, just wondered whether there is some page somewhere that  
discusses these little differences here and there, as I am sure there  
will be a number of other problems such as these along the way.

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From magno_yu at ml.com  Thu Mar  1 22:28:02 2007
From: magno_yu at ml.com (yoooooo)
Date: Thu, 1 Mar 2007 13:28:02 -0800 (PST)
Subject: [R] Simplest question ever...
Message-ID: <9258932.post@talk.nabble.com>


Let's say i have

a = c(1, 4, 5)
b = c(2, 6, 7)

and i have matrix m, what's an efficient way of access
m[1, 2], m[4, 6], m[5, 7]
like of course m[a, b] = is not going to do, but what's an expression that
will allow me to have that list? 

Thanks!
-- 
View this message in context: http://www.nabble.com/Simplest-question-ever...-tf3329894.html#a9258932
Sent from the R help mailing list archive at Nabble.com.


From rduval at gmail.com  Thu Mar  1 22:36:29 2007
From: rduval at gmail.com (Robert Duval)
Date: Thu, 1 Mar 2007 16:36:29 -0500
Subject: [R] R code for "Statistical Models in S" ?
In-Reply-To: <FE68A329-25B8-4014-A090-BB9436D33489@hanover.edu>
References: <FE68A329-25B8-4014-A090-BB9436D33489@hanover.edu>
Message-ID: <2b6e342f0703011336h1e7baa32ge552dd4db34f8f1c@mail.gmail.com>

You might want to start looking at the FAQ's

http://cran.r-project.org/faqs.html

in particular

http://cran.r-project.org/doc/FAQ/R-FAQ.html#R-and-S

robert

On 3/1/07, Charilaos Skiadas <skiadas at hanover.edu> wrote:
> I just acquired a copy of "Statistical Models in S", I guess most
> commonly known as the "white book", and realized to my dismay that
> most of the code is not directly executable in R, and I was wondering
> if there was a source discussing the things that are different and
> what the new ways of calling things are.
>
> For instance, the first obstacle was the solder.balance data set. I
> found a "solder" data set in rpart, which is very close to it except
> for the fact that the Panel variable is not a factor, but that's
> easily fixed.
> The first problem is the next two calls, on pages 2 and 3. One is
> "plot(solder.balance)", which is supposed to produce a very different
> plot than it does in R (I actually don't know the name of the plot,
> which is part of the problem I guess). Then one is supposed to call
> "plot.factor(skips ~ Opening + Mask)", which I took to mean:
> "plot(skips ~ Opening + Mask, data=solder)", and that worked, though
> I still haven't been able to make a direct call to plot.factor work
> (I keep getting a "could not find function plot.factor" error).
>
> Anyway, just wondered whether there is some page somewhere that
> discusses these little differences here and there, as I am sure there
> will be a number of other problems such as these along the way.
>
> Haris Skiadas
> Department of Mathematics and Computer Science
> Hanover College
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From maitra at iastate.edu  Thu Mar  1 22:40:26 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Thu, 1 Mar 2007 15:40:26 -0600
Subject: [R] permutation tests on autocorrelations
Message-ID: <20070301154026.6ea45bee@triveni.stat.iastate.edu>

Dear list,

I have this huge array of numbers, of dimensions  67 x 33 x 51 x 6, the 6 being the replications. I wanted to test for evidence of autocorrelation between the 6 replications, marginally. I can calculate the first-order autocorrelation very easily using an appropriately defined function and apply on the first three dimensions. But how can I perform a permutation test to test for significance?** I have looked around with  RSiteSearch and searched for permutation tests and autocorrelations, but did not come up with something that I could use. Anything I am missing?

** I understand that with such a large number of tests, there will be the issue of getting some false positives and will look into that issue separately, but right now, I need to be able to get a p-value for the marginal tests even before I get there.

Many thanks and best wishes,
Ranjan


From ggrothendieck at gmail.com  Thu Mar  1 22:46:21 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 1 Mar 2007 16:46:21 -0500
Subject: [R] How to read in this data format?
In-Reply-To: <BAY134-DAV122D0BF3F65D8B567DFF32D8800@phx.gbl>
References: <BAY134-F394F6FDE418B617D16A196D8800@phx.gbl>
	<971536df0703010935r6abcd474v839a3c765c796699@mail.gmail.com>
	<BAY134-DAV122D0BF3F65D8B567DFF32D8800@phx.gbl>
Message-ID: <971536df0703011346u6d65f1faid814798e98a3e10d@mail.gmail.com>

On 3/1/07, Bart Joosen <Bartjoosen at hotmail.com> wrote:
> Dear All,
>
> thanks for the replies, Jim Holtman has given a solution which fits my
> needs, but Gabor Grothendieck did the same thing,
> but it looks like the coding will allow faster processing (should check this
> out tomorrow on a big datafile).
>
> @gabor: I don't understand the use of the grep command:
>        grep("^[1-9][0-9. ]*$|Time", Lines., value = TRUE)
> What is this expression  ("^[1-9][0-9. ]*$|Time") actually doing?
> I looked in the help page, but couldn't find a suitable answer.

I briefly discussed it in the first paragraph of my response.  It
matches and returns only those lines that start (^ matches start of line)
with a digit, i.e. [1-9], and contains only digits, dots and spaces,
i.e. [0-9. ]*, to end of line, i.e. $ matches end of line, or (| means
or) contains the word Time.
If you don't have lines like ... (which you did in your example) then
the regexp
could be simplified to "^[0-9. ]+$|Time".  You may need to match tabs too
if your input contains those.

>
>
> Thanks to All
>
>
> Bart
>
> ----- Original Message -----
> From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
> To: "Bart Joosen" <bartjoosen at hotmail.com>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Thursday, March 01, 2007 6:35 PM
> Subject: Re: [R] How to read in this data format?
>
>
> > Read in the data using readLines, extract out
> > all desired lines (namely those containing only
> > numbers, dots and spaces or those with the
> > word Time) and remove Retention from all
> > lines so that all remaining lines have two
> > fields.  Now that we have desired lines
> > and all lines have two fields read them in
> > using read.table.
> >
> > Finally, split them into groups and restructure
> > them using "by" and in the last line we
> > convert the "by" output to a data frame.
> >
> > At the end we display an alternate function f
> > for use with by should we wish to generate long
> > rather than wide output (using the terminology
> > of the reshape command).
> >
> >
> > Lines <- "$$ Experiment Number:
> > $$ Associated Data:
> >
> > FUNCTION 1
> >
> > Scan            1
> > Retention Time  0.017
> >
> > 399.8112        184
> > 399.8742        0
> > 399.9372        152
> > ....
> >
> > Scan            2
> > Retention Time  0.021
> >
> > 399.8112        181
> > 399.8742        1
> > 399.9372        153
> > "
> >
> > # replace next line with: Lines. <- readLines("myfile.dat")
> > Lines. <- readLines(textConnection(Lines))
> > Lines. <- grep("^[1-9][0-9. ]*$|Time", Lines., value = TRUE)
> > Lines. <- gsub("Retention", "", Lines.)
> >
> > DF <- read.table(textConnection(Lines.), as.is = TRUE)
> > closeAllConnections()
> >
> > f <- function(x) c(id = x[1,2], structure(x[-1,2], .Names = x[-1,1]))
> > out.by <- by(DF, cumsum(DF[,1] == "Time"), f)
> > as.data.frame(do.call("rbind", out.by))
> >
> >
> > We could alternately consider producing long
> > format by replacing the function f with:
> >
> > f <- function(x) data.frame(x[-1,], id = x[1,2])
> >
> >
> > On 3/1/07, Bart Joosen <bartjoosen at hotmail.com> wrote:
> >> Hi,
> >>
> >> I recieved an ascii file, containing following information:
> >>
> >> $$ Experiment Number:
> >> $$ Associated Data:
> >>
> >> FUNCTION 1
> >>
> >> Scan            1
> >> Retention Time  0.017
> >>
> >> 399.8112        184
> >> 399.8742        0
> >> 399.9372        152
> >> ....
> >>
> >> Scan            2
> >> Retention Time  0.021
> >>
> >> 399.8112        181
> >> 399.8742        1
> >> 399.9372        153
> >> .....
> >>
> >>
> >> I would like to import this data in R into a dataframe, where there is a
> >> column time, the first numbers as column names, and the second numbers as
> >> data in the dataframe:
> >>
> >> Time    399.8112        399.8742        399.9372
> >> 0.017   184     0       152
> >> 0.021   181     1       153
> >>
> >> I did take a look at the read.table, read.delim, scan, ... But I 've no
> >> idea
> >> about how to solve this problem.
> >>
> >> Anyone?
> >>
> >>
> >> Thanks
> >>
> >> Bart
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
>
>


From rmh at temple.edu  Thu Mar  1 22:55:13 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Thu,  1 Mar 2007 16:55:13 -0500 (EST)
Subject: [R] question about xtable and Hmisc
Message-ID: <20070301165513.BVR96835@po-d.temple.edu>

>> Also, if x is a data frame, latex(x) contains the row numbers.
>> Can I get rid of them here as well?


I think you are asking for the rowname=NULL argument.
   latex(x, rowname=NULL)
See ?latex to confirm if that is what you are looking for.

Rich


From Mark.Leeds at morganstanley.com  Thu Mar  1 23:01:46 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Thu, 1 Mar 2007 17:01:46 -0500
Subject: [R] covariance question which has nothing to do with R
Message-ID: <D3AEEDA31E57474B840BEBC25A8A834401635110@NYWEXMB23.msad.ms.com>

This is a covariance calculation question so nothing to do with R but
maybe someone could help me anyway.

Suppose, I have two random variables X and Y whose means are both known
to be zero and I want to get an estimate of their covariance.

I have n sample pairs 

(X1,Y1)
(X2,Y2)
.
.
.
.
.
(Xn,Yn)

, so that the covariance estimate is clearly 1/n *(sum from i = 1 to n
of ( X_i*Y_i) ) 

But, suppose that it is know that the X_i are positively correlated with
each other and that the Y_i are independent
of each other.

Then, does this change the formula for the covariance estimate at all ?
Intuitively, I would think that, if the X_i's are positively
correlated , then something should change because there is less info
there than if they were independent but i'm not sure what should change
and I couldn't find it in a book.  

I can assume that the correlation between the X_i's is rho if this makes
things easier ? Thanks.

References are appreciated also.
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}


From thomas.friedrichsmeier at ruhr-uni-bochum.de  Thu Mar  1 23:25:40 2007
From: thomas.friedrichsmeier at ruhr-uni-bochum.de (Thomas Friedrichsmeier)
Date: Thu, 1 Mar 2007 23:25:40 +0100
Subject: [R] Default par() options
In-Reply-To: <45E73CB0.3020501@karlin.mff.cuni.cz>
References: <200703012040.46329.thomas.friedrichsmeier@ruhr-uni-bochum.de>
	<45E73CB0.3020501@karlin.mff.cuni.cz>
Message-ID: <200703012325.43541.thomas.friedrichsmeier@ruhr-uni-bochum.de>

Hi Petr,

On Thursday 01 March 2007 21:50, Petr Klasterecky wrote:
> I am no expert on these topics but currently I am solving a similar
> issue using the .Rprofile file and the .First function. So maybe it's
> enough to put
> .First <- function(){
> par(whatever you want)
> further instructions if neccessary
> }

thanks for your suggestion. The problem in this particular case is that the 
par() options are kept separate for each created device. Hence the defaults 
would need to be set while the device is created (or directly afterwards), 
and simply setting them once on startup won't work.

E.g.:
par(bg="blue") 	# creates a new x11 (or system default) device
par("bg")			# "blue"
dev.off()
par("bg")			# creates a new device; "transparent"
dev.off()

X11()			# or windows()/quartz()/...
par(bg="blue")
par("bg")			# "blue"
dev.off()
X11()			# or windows()/quartz()/...
par("bg")			# "transparent"
dev.off()

Regards
Thomas Friedrichsmeier
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070301/f533c6aa/attachment.bin 

From epurdom at stat.Berkeley.EDU  Thu Mar  1 23:35:40 2007
From: epurdom at stat.Berkeley.EDU (Elizabeth Purdom)
Date: Thu, 01 Mar 2007 14:35:40 -0800
Subject: [R] passing arguments to multiple functions
Message-ID: <45E7553C.4020707@stat.berkeley.edu>

Hi,
I am writing a function and I want to pass arguments to a function I 
will call internally without having to specify all the possible 
arguments to pass. Usually I would use '...' but I want to do this for 
two functions that I will call, and the two functions do not take the 
same arguments. So I would like a call to my function to look like this:

 > myfunc(x, fun1Args, fun2Args)

where fun1Args would be a list of arguments to pass to the first 
function and fun2Args would be list of arguments to pass to the second 
function. The problem is that I don't know how to write 'myfunc' so that 
I can input these elements into the calls I will make to the two 
functions. I certainly don't want to have to specify all the possible 
values and input them manually (for example if one of the functions is a 
plotting function!) I know I've run across functions that require you to 
pass arguments in a list like this, but the only one I can remember is 
'mapply' which calls a C function so it doesn't help me figure out how 
to parse/send the arguments to the functions. Is this possible to do 
within R? Or is there a way to parse the '...' so that only the right 
arguments go to the right function?

Thanks,
Elizabeth


From Cody_Hamilton at Edwards.com  Thu Mar  1 23:43:08 2007
From: Cody_Hamilton at Edwards.com (Cody_Hamilton at Edwards.com)
Date: Thu, 1 Mar 2007 14:43:08 -0800
Subject: [R] Using R for devices trial
Message-ID: <OFC2E9EDF7.C0709DCC-ON88257291.007CBF3D-88257291.007C91B1@irvine.edwards.com>


I would like to use R for submissions to FDA/CDRH (the medical device
company I work for currently uses only SAS).  Previous postings to the list
regarding R and 21 CFR 11 compliance have been very helpful.  However,
reluctance to using open source software for statistical analyses and
reporting remains high here at my company.  Has anyone used R for an
official submission to FDA/CDRH?  It would be most helpful if I could tell
our group that others have been able to use R for this purpose.

Regards,
Cody Hamilton
Staff Biostatistician
Edwards Lifesciences


From plynchnlm at gmail.com  Thu Mar  1 23:45:55 2007
From: plynchnlm at gmail.com (Paul Lynch)
Date: Thu, 1 Mar 2007 17:45:55 -0500
Subject: [R] Simplest question ever...
In-Reply-To: <9258932.post@talk.nabble.com>
References: <9258932.post@talk.nabble.com>
Message-ID: <50d6c72a0703011445v5c064d74geb4aa268612f28ac@mail.gmail.com>

I'm not sure this is the most efficient, but how about:
   diag(m[a,b])
?

On 3/1/07, yoooooo <magno_yu at ml.com> wrote:
>
> Let's say i have
>
> a = c(1, 4, 5)
> b = c(2, 6, 7)
>
> and i have matrix m, what's an efficient way of access
> m[1, 2], m[4, 6], m[5, 7]
> like of course m[a, b] = is not going to do, but what's an expression that
> will allow me to have that list?
>
> Thanks!
> --
> View this message in context: http://www.nabble.com/Simplest-question-ever...-tf3329894.html#a9258932
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From roger.bos at us.rothschild.com  Thu Mar  1 23:53:26 2007
From: roger.bos at us.rothschild.com (Bos, Roger)
Date: Thu, 1 Mar 2007 17:53:26 -0500
Subject: [R] FTP download from ftp.sec.gov
Message-ID: <D8C95B444AD6EE4AAD638D818A9CFD343A1E48@RINNYCSE000.rth.ad.rothschild.com>

All,

I have managed to download files from web sites and ftp sites using R,
so just for fun I tried to do so from the SEC's ftp site using the
following code:

ftp <- "ftp://anonymous:test at ftp.sec.gov/edgar/full-index/company.idx"
download.file(url=ftp, destfile="test.txt")

And it does not work.  R says it cannot open the URL. If I paste the ftp
part into IE it works fine.  Its not critical, but I would be interested
if anything figures out how to make it work.  (I know R is probably not
the best language for FTP download and text parsing, but that's another
topic).

Thanks, Roger


********************************************************************** * 
This message is for the named person's use only. It may 
contain confidential, proprietary or legally privileged 
information. No right to confidential or privileged treatment 
of this message is waived or lost by any error in 
transmission. If you have received this message in error, 
please immediately notify the sender by e-mail, 
delete the message and all copies from your system and destroy 
any hard copies. You must not, directly or indirectly, use, 
disclose, distribute, print or copy any part of this message 
if you are not the intended recipient.


From gunter.berton at gene.com  Thu Mar  1 23:59:17 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Thu, 1 Mar 2007 14:59:17 -0800
Subject: [R] Default par() options
In-Reply-To: <45E73CB0.3020501@karlin.mff.cuni.cz>
Message-ID: <000601c75c55$3d654110$b17510ac@gne.windows.gene.com>


Thomas:
I am not sure exactly what you are asking for below, but I wonder if your
query could be satisfied by the judicious use of the ... argument in a
wrapper function to par(), like

myPar=function(bg="lightgray", pch=19,...)par(bg=bg,pch=pch,...)

or perhaps 

myX11 <- function(width=10, bg="lightgray", pch=19,...)
{
X11(width=width)
par(bg=bg,pch = pch,...)
}

This would use the existing user-chosen defaults for the respective devices
if no other values were provided, and would allow the user to explicitly
specify any different values for them or additional arguments to par if
needed. I agree that it ain't elegant, though, so I'd welcome better
alternatives, too.

Of course, one can explicitly use formals() and the construction:

dots <- as.list(substitute(list(...)))[-1]   ## V&R: S PROGRAMMING p. 46

to obtain all the arguments and their names and appropriately stuff them
into either par() or X11() using do.call() or something similar; but that
seems like more than you need here.

Anyway, HTH.


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Petr 
> Klasterecky
> Sent: Thursday, March 01, 2007 12:51 PM
> To: Thomas Friedrichsmeier
> Cc: r-help at r-project.org
> Subject: Re: [R] Default par() options
> 
> I am no expert on these topics but currently I am solving a similar 
> issue using the .Rprofile file and the .First function. So maybe it's 
> enough to put
> .First <- function(){
> par(whatever you want)
> further instructions if neccessary
> }
> 
> Petr
> 
> Thomas Friedrichsmeier napsal(a):
> > The following question/idea came up on the RKWard 
> development mailing list, 
> > but might be of general interest:
> > 
> > Is there a nice way to customize the default look of all 
> graphs on all 
> > devices? I.e. a way to - for instance - set the following 
> options before each 
> > plot:
> > 
> > par(bg="light gray", las=2, pch=19)
> > 
> > As far as I have found, there would currently be two ways 
> to do this:
> > 1) Adding the above statement manually after opening the 
> device, and before 
> > starting the plot. It could of course be wrapped inside a 
> custom function to 
> > save some typing, but you'd still need to make sure to 
> always add the 
> > command.
> > 
> > 2) Overriding all device functions with something like:
> > X11 <- function (...) {
> > 	grDevices::X11 (...)
> > 	par ([custom options])
> > }
> > This would be feasible, but feels rather dirty. Also, 
> something substantially 
> > more elaborate would be needed to honor e.g. fonts and bg 
> arguments, if 
> > explicitely specified in the call to X11. Would have to be 
> done for each 
> > device separately.
> > 
> > Does a third, more elegant solution exist?
> > 
> > If not, would the following idea have any chances of being 
> added to R?
> > 
> > Create a new options("par.default"), similar to the already 
> existing 
> > options("par.ask.default"). This would take a list of par() 
> options to set a 
> > default value for, like e.g.:
> > 
> > options(par.default=list(bg="light gray", las=2, pch=19))
> > 
> > Only those options would need to be specified in the list, 
> for which you 
> > actually want to set a default different from the built-in. Options 
> > explicitely specified in X11(), plot(), additional calls to 
> par(), etc. would 
> > take precedence over options("par.default").
> > 
> > Regards
> > Thomas Friedrichsmeier
> > 
> > 
> > 
> --------------------------------------------------------------
> ----------
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> Petr Klasterecky
> Dept. of Probability and Statistics
> Charles University in Prague
> Czech Republic
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From marc_schwartz at comcast.net  Fri Mar  2 00:03:42 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 01 Mar 2007 17:03:42 -0600
Subject: [R] Query about data manipulation
In-Reply-To: <752252.69071.qm@web43136.mail.sp1.yahoo.com>
References: <752252.69071.qm@web43136.mail.sp1.yahoo.com>
Message-ID: <1172790222.4897.109.camel@localhost.localdomain>

On Thu, 2007-03-01 at 12:35 -0800, lalitha viswanath wrote:
> Hi
> Thanks much for that input.It was extremely helpful.
> 
> I am seeking some input about another stumbling block
> using RODBC; SQLQuery et al with large BLOB values.
> 
> Although the following query 
> dataFromDB <- sqlQuery(channel, "select
> uncompress(columnName) from tableName where Id=id ");
> returns just one row , dataFromDB[1,1] actually
> contains 4000+ rows of the form 
> field1 \t field2 \t value\n.... described earlier.
> (4000+ rows compressed as one long string)
> 
> On printing dataFromDB[1,1], it does not print beyond
> 3600 such rows or so (printing in fact "field1 \t
> field2 \t value \n.....field3600 \t field3601"),
> abruptly missing the rest of the result. 
> 
> Hence it throws an error when I try to use read.table
> (after using textConnection as suggested) that row xyz
> does not contain 3 values,etc.
> 
> It seems to be missing 1/4th of the actual result that
> should contain 4000+ such pairs.
> 
> The set of 4000+ rows occupy just 100KB if written out
> to a file directly from MySQL.
> Is there anyway to increase the capacity of the return
> result in R so that it does not get thrown off as
> above and retrieves the ENTIRE result?
> 
> I tried increasing buffsize, but as I understand,
> since SqlQuery itself returns just one row in this
> case, it  is possibly not very relevant here?
> 
> Note that the above mentioned problem does not arise
> when the data returned from SQL query contains less
> than 3500 such concatenated entries.
> 
> Your input is greatly appreciated.
> Thanks
> Lalitha


Lalitha,

Post the results of the following, when reading the blob where there is
4,000+ rows:

  str(dataFromDB)

  nchar(dataFromDB[1,1])


That might help us ascertain what is causing the problem relative to the
size of the objects.

RODBC has a field length limit of 64k, so if the blob is larger than
that, which it seems may be the case, this would be the root problem and
would result in your data frame column being truncated at that point
during the SQL query.

I don't know if the RMySQL package has larger limits, but if so, you may
have to go that route for a solution.

HTH,

Marc Schwartz


From jimholtman at yahoo.com  Fri Mar  2 00:03:48 2007
From: jimholtman at yahoo.com (jim holtman)
Date: Thu, 1 Mar 2007 15:03:48 -0800 (PST)
Subject: [R] Simplest question ever...
Message-ID: <902340.96197.qm@web54013.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070301/fd32c4bf/attachment.pl 

From Ryan.G.Huckstorf at wellsfargo.com  Fri Mar  2 00:28:14 2007
From: Ryan.G.Huckstorf at wellsfargo.com (Ryan.G.Huckstorf at wellsfargo.com)
Date: Thu, 1 Mar 2007 17:28:14 -0600
Subject: [R] Generating R plots through Perl
Message-ID: <CCA676360D67EA47A0D5CC6ACC22523E011D3AEF@msgawbmnmsp09.wellsfargo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070301/9a707814/attachment.pl 

From ggrothendieck at gmail.com  Fri Mar  2 00:36:39 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 1 Mar 2007 18:36:39 -0500
Subject: [R] FTP download from ftp.sec.gov
In-Reply-To: <D8C95B444AD6EE4AAD638D818A9CFD343A1E48@RINNYCSE000.rth.ad.rothschild.com>
References: <D8C95B444AD6EE4AAD638D818A9CFD343A1E48@RINNYCSE000.rth.ad.rothschild.com>
Message-ID: <971536df0703011536g6ae0e1e3v165f1f7150b9759b@mail.gmail.com>

On Windows XP it worked for me on both 2.4.1 and 2.5.0.  I did notice
that on 2.4.1 it says "using Synchronous WinInet calls" but does not
say this on 2.5.0.  See below for the two transcripts.

> ftp <- "ftp://anonymous:test at ftp.sec.gov/edgar/full-index/company.idx"
> download.file(url=ftp, destfile="test.txt")
trying URL 'ftp://anonymous:test at ftp.sec.gov/edgar/full-index/company.idx'
using Synchronous WinInet calls
opened URL
downloaded 33930Kb

> R.version.string # XP
[1] "R version 2.4.1 Patched (2006-12-30 r40331)"

---

> ftp <- "ftp://anonymous:test at ftp.sec.gov/edgar/full-index/company.idx"
> download.file(url=ftp, destfile="test.txt")
trying URL 'ftp://anonymous:test at ftp.sec.gov/edgar/full-index/company.idx'
ftp data connection made, file length 34744813 bytes
opened URL
downloaded 33930Kb

> R.version.string # XP
[1] "R version 2.5.0 Under development (unstable) (2007-02-25 r40804)"




On 3/1/07, Bos, Roger <roger.bos at us.rothschild.com> wrote:
> All,
>
> I have managed to download files from web sites and ftp sites using R,
> so just for fun I tried to do so from the SEC's ftp site using the
> following code:
>
> ftp <- "ftp://anonymous:test at ftp.sec.gov/edgar/full-index/company.idx"
> download.file(url=ftp, destfile="test.txt")
>
> And it does not work.  R says it cannot open the URL. If I paste the ftp
> part into IE it works fine.  Its not critical, but I would be interested
> if anything figures out how to make it work.  (I know R is probably not
> the best language for FTP download and text parsing, but that's another
> topic).
>
> Thanks, Roger
>
>


From ted.harding at nessie.mcc.ac.uk  Fri Mar  2 00:44:27 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 01 Mar 2007 23:44:27 -0000 (GMT)
Subject: [R] Simplest question ever...
In-Reply-To: <50d6c72a0703011445v5c064d74geb4aa268612f28ac@mail.gmail.com>
Message-ID: <XFMail.070301234427.ted.harding@nessie.mcc.ac.uk>


On 01-Mar-07 Paul Lynch wrote:
> I'm not sure this is the most efficient, but how about:
>    diag(m[a,b])
> ?

m[cbind(a,b)] will also do it:

m
     [,1] [,2] [,3] [,4] [,5] [,6] [,7]
[1,]  1.1  1.2  1.3  1.4  1.5  1.6  1.7
[2,]  2.1  2.2  2.3  2.4  2.5  2.6  2.7
[3,]  3.1  3.2  3.3  3.4  3.5  3.6  3.7
[4,]  4.1  4.2  4.3  4.4  4.5  4.6  4.7
[5,]  5.1  5.2  5.3  5.4  5.5  5.6  5.7

a <- c(1, 4, 5)
b <- c(2, 6, 7)

diag(m[a,b])
[1] 1.2 4.6 5.7

m[cbind(a,b)]
[1] 1.2 4.6 5.7

Ted.

> On 3/1/07, yoooooo <magno_yu at ml.com> wrote:
>>
>> Let's say i have
>>
>> a = c(1, 4, 5)
>> b = c(2, 6, 7)
>>
>> and i have matrix m, what's an efficient way of access
>> m[1, 2], m[4, 6], m[5, 7]
>> like of course m[a, b] = is not going to do, but what's an expression
>> that
>> will allow me to have that list?
>>
>> Thanks!
>> --
>> View this message in context:
>> http://www.nabble.com/Simplest-question-ever...-tf3329894.html#a9258932
>> Sent from the R help mailing list archive at Nabble.com.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 01-Mar-07                                       Time: 23:44:22
------------------------------ XFMail ------------------------------


From rsb at wsu.edu  Fri Mar  2 00:33:31 2007
From: rsb at wsu.edu (Bricklemyer, Ross S)
Date: Thu, 1 Mar 2007 15:33:31 -0800
Subject: [R] repost-configure error on Mandriva 2007
Message-ID: <2FC987BC0B90B24786CAF43DD3F5719C814754@CRU105.cahe.ad.wsu.edu>

All,

Apparently the attachment did not make it through the list serve.

I am having difficulty installing R-2.4.1 on Mandriva Linux 2007
Discovery.  Thanks to the help of Doug Bates I got further along the
path (Thanks Doug!!).  I now cannot figure out which line of code needs
modified in order for configure to complete.  Please see below for the
last several lines of output.


checking whether the gfortran linker (/usr/bin/ld -m elf_x86_64)
supports shared libraries... yes
checking dynamic linker characteristics... GNU/Linux ld.so
checking how to hardcode library paths into programs... immediate
checking whether stripping libraries is possible... yes
checking whether makeinfo version is at least 4.7... yes
checking for cos in -lm... yes
checking for sin in -lm... yes
checking for dlopen in -ldl... yes
checking readline/history.h usability... yes
checking readline/history.h presence... yes
checking for readline/history.h... yes
checking readline/readline.h usability... yes
checking readline/readline.h presence... yes
checking for readline/readline.h... yes
checking for rl_callback_read_char in -lreadline... no
checking for main in -lncurses... no
checking for main in -ltermcap... no
checking for main in -ltermlib... no
checking for rl_callback_read_char in -lreadline... no
checking for history_truncate_file... no
configure: error: --with-readline=yes (default) and headers/libs are not
available



*******************************************************************
Ross Bricklemyer
Dept. of Crop and Soil Sciences
Washington State University
201 Johnson Hall
Pullman, WA 99164-6420
Work: 509.335.3661
Cell/Home: 406.570.8576
Fax: 509.335.8674
Email: rsb at wsu.edu


From marc_schwartz at comcast.net  Fri Mar  2 01:04:00 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 01 Mar 2007 18:04:00 -0600
Subject: [R] repost-configure error on Mandriva 2007
In-Reply-To: <2FC987BC0B90B24786CAF43DD3F5719C814754@CRU105.cahe.ad.wsu.edu>
References: <2FC987BC0B90B24786CAF43DD3F5719C814754@CRU105.cahe.ad.wsu.edu>
Message-ID: <1172793840.4897.148.camel@localhost.localdomain>

On Thu, 2007-03-01 at 15:33 -0800, Bricklemyer, Ross S wrote:
> All,
> 
> Apparently the attachment did not make it through the list serve.
> 
> I am having difficulty installing R-2.4.1 on Mandriva Linux 2007
> Discovery.  Thanks to the help of Doug Bates I got further along the
> path (Thanks Doug!!).  I now cannot figure out which line of code needs
> modified in order for configure to complete.  Please see below for the
> last several lines of output.
> 
> 
> checking whether the gfortran linker (/usr/bin/ld -m elf_x86_64)
> supports shared libraries... yes
> checking dynamic linker characteristics... GNU/Linux ld.so
> checking how to hardcode library paths into programs... immediate
> checking whether stripping libraries is possible... yes
> checking whether makeinfo version is at least 4.7... yes
> checking for cos in -lm... yes
> checking for sin in -lm... yes
> checking for dlopen in -ldl... yes
> checking readline/history.h usability... yes
> checking readline/history.h presence... yes
> checking for readline/history.h... yes
> checking readline/readline.h usability... yes
> checking readline/readline.h presence... yes
> checking for readline/readline.h... yes
> checking for rl_callback_read_char in -lreadline... no
> checking for main in -lncurses... no
> checking for main in -ltermcap... no
> checking for main in -ltermlib... no
> checking for rl_callback_read_char in -lreadline... no
> checking for history_truncate_file... no
> configure: error: --with-readline=yes (default) and headers/libs are not
> available

You need to have the readline development related files installed. On
RPM based systems it is typically called 'readline-devel' or similar.

Install that RPM and you should be good to go.

HTH,

Marc Schwartz


From skiadas at hanover.edu  Fri Mar  2 01:04:39 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Thu, 1 Mar 2007 19:04:39 -0500
Subject: [R] R code for "Statistical Models in S" ?
In-Reply-To: <2b6e342f0703011336h1e7baa32ge552dd4db34f8f1c@mail.gmail.com>
References: <FE68A329-25B8-4014-A090-BB9436D33489@hanover.edu>
	<2b6e342f0703011336h1e7baa32ge552dd4db34f8f1c@mail.gmail.com>
Message-ID: <1E1A906A-FA70-414D-9E8A-2CD5E9252DFA@hanover.edu>

On Mar 1, 2007, at 4:36 PM, Robert Duval wrote:

> You might want to start looking at the FAQ's
>
> http://cran.r-project.org/faqs.html
>
> in particular
>
> http://cran.r-project.org/doc/FAQ/R-FAQ.html#R-and-S
>
Thanks I must admit that I had not looked at the FAQ's, but I have  
now and though it might become relevant a bit down the line, it  
doesn't seem to answer my questions, unless I've missed something in  
there. The only relevant bit I found was this phrase:

"Apart from lexical scoping and its implications, R follows the S  
language definition in the Blue and White Books as much as possible,  
and hence really is an ?implementation? of S."

The question (one part of it at least) had to do with the data sets  
and functions used, and the fact that some of these data sets are not  
there in R. In the book they refer to a "data" package for instance,  
which seems to contain things different than R's "datasets" package.  
So the question was if the necessary data sets are available somewhere.

The second part was in particular about a call to "plot", namely "plot 
(solder.balance)", which in S according to the white book is supposed  
to produce the graph in the top of page 3, for those having the book,  
the caption of the plot being: "A plot of the mean of \textbf{skips}  
at each of the levels of the factors in the solder experiment."
I have now found out, thanks to "A handbook of statistical analyses  
using R", that the corresponding call in R would be: "plot.design 
(solder)".

I understand of course that not every difference between  
implementations in S and R should be documented, but I was hoping  
that other people who have already gone through this book would have  
documented these differences. I guess not, and I will be doing so now.

> robert

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From thomas.friedrichsmeier at ruhr-uni-bochum.de  Fri Mar  2 01:13:08 2007
From: thomas.friedrichsmeier at ruhr-uni-bochum.de (Thomas Friedrichsmeier)
Date: Fri, 2 Mar 2007 01:13:08 +0100
Subject: [R] Default par() options
In-Reply-To: <000601c75c55$3d654110$b17510ac@gne.windows.gene.com>
References: <000601c75c55$3d654110$b17510ac@gne.windows.gene.com>
Message-ID: <200703020113.12264.thomas.friedrichsmeier@ruhr-uni-bochum.de>

Hi Bert,

On Thursday 01 March 2007 23:59, Bert Gunter wrote:
> I am not sure exactly what you are asking for below,

I guess I'm really asking for some advanced comfort / elegance. See below.

> but I wonder if your 
> query could be satisfied by the judicious use of the ... argument in a
> wrapper function to par(), like
>
> myPar=function(bg="lightgray", pch=19,...)par(bg=bg,pch=pch,...)

True. Such a function solves most of "the issue". Also, it should work 
universally for all types of devices. However, a manual call to myPar() would 
need to be added after each creation of a new device.

> or perhaps
>
> myX11 <- function(width=10, bg="lightgray", pch=19,...)
> {
> X11(width=width)
> par(bg=bg,pch = pch,...)
> }

This one would automate the call to par(), but a similar function would be 
needed for each type of device (X11(), postscript(), pdf(), etc.) in order to 
achieve a fully automatic consistent behavior.
Also in this case options("device") would need to be adjusted, which is no big 
deal, but simply doesn't feel nice.

> This would use the existing user-chosen defaults for the respective devices
> if no other values were provided, and would allow the user to explicitly
> specify any different values for them or additional arguments to par if
> needed. I agree that it ain't elegant, though, so I'd welcome better
> alternatives, too.

Part of the background is that I'm writing a GUI frontend to R. Both of your 
suggested solutions certainly work well, and are easy enough to use, 
regularly. For writing the frontend, however, we'd like to support all sorts 
of devices (the standard ones, and ones in uncommon packages / future 
devices) in a consistent way, and with as little code as possible (add to 
that: without using obscure tricks, so the user will easily be able to 
understand what's going on, and achieve the same results on the R command 
line). So that's what makes me look for (excessive?) elegance in this case.

> Of course, one can explicitly use formals() and the construction:
>
> dots <- as.list(substitute(list(...)))[-1]   ## V&R: S PROGRAMMING p. 46
>
> to obtain all the arguments and their names and appropriately stuff them
> into either par() or X11() using do.call() or something similar; but that
> seems like more than you need here.

Not what I'm looking for, here, but that trick may come in handy in a 
different situation. Thanks!

Regards
Thomas Friedrichsmeier
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070302/8708dc7b/attachment.bin 

From skiadas at hanover.edu  Fri Mar  2 01:14:31 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Thu, 1 Mar 2007 19:14:31 -0500
Subject: [R] Generating R plots through Perl
In-Reply-To: <CCA676360D67EA47A0D5CC6ACC22523E011D3AEF@msgawbmnmsp09.wellsfargo.com>
References: <CCA676360D67EA47A0D5CC6ACC22523E011D3AEF@msgawbmnmsp09.wellsfargo.com>
Message-ID: <28B7F2B9-5FA6-4AB2-9602-00B219EE1337@hanover.edu>

On Mar 1, 2007, at 6:28 PM, <Ryan.G.Huckstorf at wellsfargo.com> wrote:

First off, if you are working in perl you might want to be aware of  
ruby and the "r for ruby" project:
http://rubyforge.org/projects/r4ruby/

> Hello,
>
> $R->send(qq (xVal <- c(1,2,3,4,5,6)));
> $R->send(qq (yVal <- c(3,5,2,6,1,5)));
> $R->send(qq (pdf("C:/Test Environment/R/perlPlotTest.pdf")));
> $R->send(qq (plot(xVal, yVal)));
> $R->send(qq (graphics.off()));

I don't really know how to write this in perl, but could you perhaps  
put the last three lines all in one call to "$R->send", using dev.off 
() then? Don't know if it would make a difference, but that's the  
only thing I could think of. I'm guessing something like this:

$R->send(qq (pdf("C:/Test Environment/R/perlPlotTest.pdf"); plot 
(xVal, yVal); dev.off()));

> As the code indicates, I am using R's pdf function to create a pdf  
> file
> containing the plot of xVal and yVal.  I am using the graphics.off()
> function rather than the dev.off() function as I get an error  
> message of
> "<simpleError in dev.off(): cannot shut down device 1 (the null
> device)>" when dev.off() is used.  Is there another way to generate  
> and
> save a plot using the bridge connection that I described?  If not,  
> what
> would be an efficient way of generating and saving plots from  
> within my
> Perl program?  Any help would be greatly appreciated.
>
> Thank you,
> Ryan

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From aiminy at iastate.edu  Fri Mar  2 05:17:26 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Thu, 01 Mar 2007 22:17:26 -0600
Subject: [R] label histogram question
Message-ID: <6.1.2.0.2.20070301220726.01d9a378@aiminy.mail.iastate.edu>

Dear R list:

I have a data like this
 > head(data.19.pr.2)
     pr    Ave    Sd M#    Re  Aa
1 1A24  57.66 33.50 20 ALA_1 ALA
2 1A24  72.16 19.75 20 GLN_2 GLN
3 1A24 103.52  8.64 20 TYR_3 TYR
4 1A24  38.67 15.51 20 GLU_4 GLU
5 1A24  54.56 16.43 20 ASP_5 ASP
6 1A24 999.00  0.00 20 GLY_6 GLY
 > levels(data.19.pr.2$Aa)
  [1] "ALA" "ARG" "ASN" "ASP" "CYS" "GLN" "GLU" "GLY" "HIS" "ILE" "LEU" 
"LYS" "MET" "PHE" "PRO" "SER" "THR" "TRP" "TYR" "VAL"

I want to do histogram for Sd grouped by 20 levels of Aa, and put 20 
histograms in one page.
I use this code to do job

par(mfrow=c(4,5))
tapply(data.19.pr.2$Sd,data.19.pr.2$Aa,hist)

I get all 20 histogram in one page,but main label of each histogram is 
labeled by "Histogram of X[[1]]" and xlab is labeled by "X[[1]]".
I want to change these labels using 20 levels of Aa . it look like this: 
"Histogram of ALA"

Does anyone know how to code these?

Aimin


From bartjoosen at hotmail.com  Fri Mar  2 08:57:30 2007
From: bartjoosen at hotmail.com (Bart Joosen)
Date: Fri, 02 Mar 2007 07:57:30 +0000
Subject: [R] How to read in this data format?
In-Reply-To: <971536df0703011346u6d65f1faid814798e98a3e10d@mail.gmail.com>
Message-ID: <BAY134-F21AAD1190F16D0D67A4817D8870@phx.gbl>

Gabor,

thanks for the clarification, now I understand the expression.

Thanks to everyone


Bart


>From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
>To: "Bart Joosen" <Bartjoosen at hotmail.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] How to read in this data format?
>Date: Thu, 1 Mar 2007 16:46:21 -0500
>
>On 3/1/07, Bart Joosen <Bartjoosen at hotmail.com> wrote:
>>Dear All,
>>
>>thanks for the replies, Jim Holtman has given a solution which fits my
>>needs, but Gabor Grothendieck did the same thing,
>>but it looks like the coding will allow faster processing (should check 
>>this
>>out tomorrow on a big datafile).
>>
>>@gabor: I don't understand the use of the grep command:
>>        grep("^[1-9][0-9. ]*$|Time", Lines., value = TRUE)
>>What is this expression  ("^[1-9][0-9. ]*$|Time") actually doing?
>>I looked in the help page, but couldn't find a suitable answer.
>
>I briefly discussed it in the first paragraph of my response.  It
>matches and returns only those lines that start (^ matches start of line)
>with a digit, i.e. [1-9], and contains only digits, dots and spaces,
>i.e. [0-9. ]*, to end of line, i.e. $ matches end of line, or (| means
>or) contains the word Time.
>If you don't have lines like ... (which you did in your example) then
>the regexp
>could be simplified to "^[0-9. ]+$|Time".  You may need to match tabs too
>if your input contains those.
>
>>
>>
>>Thanks to All
>>
>>
>>Bart
>>
>>----- Original Message -----
>>From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
>>To: "Bart Joosen" <bartjoosen at hotmail.com>
>>Cc: <r-help at stat.math.ethz.ch>
>>Sent: Thursday, March 01, 2007 6:35 PM
>>Subject: Re: [R] How to read in this data format?
>>
>>
>> > Read in the data using readLines, extract out
>> > all desired lines (namely those containing only
>> > numbers, dots and spaces or those with the
>> > word Time) and remove Retention from all
>> > lines so that all remaining lines have two
>> > fields.  Now that we have desired lines
>> > and all lines have two fields read them in
>> > using read.table.
>> >
>> > Finally, split them into groups and restructure
>> > them using "by" and in the last line we
>> > convert the "by" output to a data frame.
>> >
>> > At the end we display an alternate function f
>> > for use with by should we wish to generate long
>> > rather than wide output (using the terminology
>> > of the reshape command).
>> >
>> >
>> > Lines <- "$$ Experiment Number:
>> > $$ Associated Data:
>> >
>> > FUNCTION 1
>> >
>> > Scan            1
>> > Retention Time  0.017
>> >
>> > 399.8112        184
>> > 399.8742        0
>> > 399.9372        152
>> > ....
>> >
>> > Scan            2
>> > Retention Time  0.021
>> >
>> > 399.8112        181
>> > 399.8742        1
>> > 399.9372        153
>> > "
>> >
>> > # replace next line with: Lines. <- readLines("myfile.dat")
>> > Lines. <- readLines(textConnection(Lines))
>> > Lines. <- grep("^[1-9][0-9. ]*$|Time", Lines., value = TRUE)
>> > Lines. <- gsub("Retention", "", Lines.)
>> >
>> > DF <- read.table(textConnection(Lines.), as.is = TRUE)
>> > closeAllConnections()
>> >
>> > f <- function(x) c(id = x[1,2], structure(x[-1,2], .Names = x[-1,1]))
>> > out.by <- by(DF, cumsum(DF[,1] == "Time"), f)
>> > as.data.frame(do.call("rbind", out.by))
>> >
>> >
>> > We could alternately consider producing long
>> > format by replacing the function f with:
>> >
>> > f <- function(x) data.frame(x[-1,], id = x[1,2])
>> >
>> >
>> > On 3/1/07, Bart Joosen <bartjoosen at hotmail.com> wrote:
>> >> Hi,
>> >>
>> >> I recieved an ascii file, containing following information:
>> >>
>> >> $$ Experiment Number:
>> >> $$ Associated Data:
>> >>
>> >> FUNCTION 1
>> >>
>> >> Scan            1
>> >> Retention Time  0.017
>> >>
>> >> 399.8112        184
>> >> 399.8742        0
>> >> 399.9372        152
>> >> ....
>> >>
>> >> Scan            2
>> >> Retention Time  0.021
>> >>
>> >> 399.8112        181
>> >> 399.8742        1
>> >> 399.9372        153
>> >> .....
>> >>
>> >>
>> >> I would like to import this data in R into a dataframe, where there is 
>>a
>> >> column time, the first numbers as column names, and the second numbers 
>>as
>> >> data in the dataframe:
>> >>
>> >> Time    399.8112        399.8742        399.9372
>> >> 0.017   184     0       152
>> >> 0.021   181     1       153
>> >>
>> >> I did take a look at the read.table, read.delim, scan, ... But I 've 
>>no
>> >> idea
>> >> about how to solve this problem.
>> >>
>> >> Anyone?
>> >>
>> >>
>> >> Thanks
>> >>
>> >> Bart
>> >>
>> >> ______________________________________________
>> >> R-help at stat.math.ethz.ch mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >
>>
>>


From P.Dalgaard at biostat.ku.dk  Fri Mar  2 09:16:49 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 02 Mar 2007 09:16:49 +0100
Subject: [R] FTP download from ftp.sec.gov
In-Reply-To: <971536df0703011536g6ae0e1e3v165f1f7150b9759b@mail.gmail.com>
References: <D8C95B444AD6EE4AAD638D818A9CFD343A1E48@RINNYCSE000.rth.ad.rothschild.com>
	<971536df0703011536g6ae0e1e3v165f1f7150b9759b@mail.gmail.com>
Message-ID: <45E7DD71.7050107@biostat.ku.dk>

Gabor Grothendieck wrote:
> On Windows XP it worked for me on both 2.4.1 and 2.5.0.  I did notice
> that on 2.4.1 it says "using Synchronous WinInet calls" but does not
> say this on 2.5.0.  See below for the two transcripts.
>
>   
>> ftp <- "ftp://anonymous:test at ftp.sec.gov/edgar/full-index/company.idx"
>> download.file(url=ftp, destfile="test.txt")
>>     
> trying URL 'ftp://anonymous:test at ftp.sec.gov/edgar/full-index/company.idx'
> using Synchronous WinInet calls
> opened URL
> downloaded 33930Kb
>   
This appears to be highly system dependent. Works for me on my home
machine using Fedora 6, but not on the office machine running SUSE 10. I
wouldn't be surprised if firewall configuration plays a part.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From petr.pikal at precheza.cz  Fri Mar  2 09:22:34 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 02 Mar 2007 09:22:34 +0100
Subject: [R] number of levels for a factor
In-Reply-To: <6.2.3.4.2.20070301121038.037fea30@aiminy.mail.iastate.edu>
Message-ID: <45E7ECDA.27401.5829F2@localhost>



On 1 Mar 2007 at 12:17, Aimin Yan wrote:

Date sent:      	Thu, 01 Mar 2007 12:17:09 -0600
To:             	r-help at stat.math.ethz.ch
From:           	Aimin Yan <aiminy at iastate.edu>
Subject:        	[R] number of levels for a factor

> I have temp list which have 19 data.frame
> I want to get number of levels for pr in the first dat.frame
> I do this like this:
> temp[[1]]$pr just has "1A24"
> after I do nlevels(temp[[1]]$pr)
> I expect to get 1, but I get 19
> 
> anyone know why?

Hi

help page for [.factor tells you that using subsetting results in

A factor with the same set of levels as x unless drop=TRUE.

I presume your list resulted from some splitting operation and that 
original set of levels was preserved.

HTH
Petr


> 
>  > tail(temp[[1]]$pr)
> [1] 1A24 1A24 1A24 1A24 1A24 1A24
> 19 Levels: 1A24 1A57 1A5J 1A6X 1AB7 1AF8 1AFI 1AGG 1AH9 1AHL 1AJ3 1AJW
> ... 1AZK
>  > nlevels(temp[[1]]$pr)
> [1] 19
> 
> Aimin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From akuma at swing.be  Fri Mar  2 09:25:40 2007
From: akuma at swing.be (Nicolas Mazziotta)
Date: Fri, 2 Mar 2007 09:25:40 +0100
Subject: [R] mixing symbols and rectangles in legend()
Message-ID: <200703020925.40530.akuma@swing.be>

Hello,

I try to mix symbols and coloured rectangles in a legend:

> plot(10,10)
> legend("top", c("text","text2"), pch=c(21,22), fill=c("red","green"), 
+ pt.bg="black") 

On the resulting graph, the symbol is not centered upon the coloured 
rectangle. Is there a way to adjust their relative position, so that they are 
centered? Looking through ?legend has not helped me (but I might have missed 
the line where it is explained)...
 
[R version 2.4.0 (2006-10-03) on linux]

Thanks for any help.

Best regards,

-- 
Nicolas Mazziotta

The contents of this e-mail, including any attachments, are ...{{dropped}}


From petr.pikal at precheza.cz  Fri Mar  2 09:29:41 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 02 Mar 2007 09:29:41 +0100
Subject: [R] label histogram question
In-Reply-To: <6.1.2.0.2.20070301220726.01d9a378@aiminy.mail.iastate.edu>
Message-ID: <45E7EE85.14024.5EAB88@localhost>

Hi

in your case I would use for loop (although common practice i to 
distract from their use :-), maybe together with main and axes 
options.

Or probably lattice histogram could be used too.

HTH
Petr





On 1 Mar 2007 at 22:17, Aimin Yan wrote:

Date sent:      	Thu, 01 Mar 2007 22:17:26 -0600
To:             	r-help at stat.math.ethz.ch
From:           	Aimin Yan <aiminy at iastate.edu>
Subject:        	[R] label histogram question

> Dear R list:
> 
> I have a data like this
>  > head(data.19.pr.2)
>      pr    Ave    Sd M#    Re  Aa
> 1 1A24  57.66 33.50 20 ALA_1 ALA
> 2 1A24  72.16 19.75 20 GLN_2 GLN
> 3 1A24 103.52  8.64 20 TYR_3 TYR
> 4 1A24  38.67 15.51 20 GLU_4 GLU
> 5 1A24  54.56 16.43 20 ASP_5 ASP
> 6 1A24 999.00  0.00 20 GLY_6 GLY
>  > levels(data.19.pr.2$Aa)
>   [1] "ALA" "ARG" "ASN" "ASP" "CYS" "GLN" "GLU" "GLY" "HIS" "ILE"
>   "LEU" 
> "LYS" "MET" "PHE" "PRO" "SER" "THR" "TRP" "TYR" "VAL"
> 
> I want to do histogram for Sd grouped by 20 levels of Aa, and put 20
> histograms in one page. I use this code to do job
> 
> par(mfrow=c(4,5))
> tapply(data.19.pr.2$Sd,data.19.pr.2$Aa,hist)
> 
> I get all 20 histogram in one page,but main label of each histogram is
> labeled by "Histogram of X[[1]]" and xlab is labeled by "X[[1]]". I
> want to change these labels using 20 levels of Aa . it look like this:
> "Histogram of ALA"
> 
> Does anyone know how to code these?
> 
> Aimin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From bruno.c at inwind.it  Fri Mar  2 10:15:33 2007
From: bruno.c at inwind.it (Bruno C.)
Date: Fri,  2 Mar 2007 10:15:33 +0100
Subject: [R] Reformulated  matrices dimensions limitation problem
Message-ID: <JE9R1X$A34FE2FE383B5764E18B4760DF51C54D@libero.it>

First I wanted to thank both Marc Schwartz Greg Snow and for their reply.

Then I needed to add a level of complexity to the problem.
I would be able to create the biggest possible matrix.

In other way does it exist a method to ask smthing like the following :

max number of rows for a matrix if column=x?

Thank you





------------------------------------------------------
Passa a Infostrada. ADSL e Telefono senza limiti e senza canone Telecom
http://click.libero.it/infostrada2marz07


From jhfowler at ucsd.edu  Fri Mar  2 10:33:53 2007
From: jhfowler at ucsd.edu (James Fowler)
Date: Fri, 2 Mar 2007 01:33:53 -0800 (PST)
Subject: [R] Help with faster optimization for large parameter problem
Message-ID: <Pine.LNX.4.64.0703020110540.29534@jhfowler.ucsd.edu>

Hello all,

I have a large parameter problem with the following very simple likelihood 
function:

fn<-function(param) {
  x1<-param[1:n]
  g1<-param[(n+1):(2*n)]
  beta<-param[(2*n+1):(2*n+k)]
  sigma2<-param[2*n+k+1]^2
  meang1sp<-mean(g1[sp])
  mu<-beta%*%matrix(x1,1,n)-(g1[sp]-meang1sp)%*%matrix(g1,1,n)
  return(sum((ydc-mu)^2)/(2*sigma2) + n*k*log(sqrt(sigma2)) +
   mean(x1)^2 + mean(g1)^2 + 1000*(x1[1]>x1[n]))
}

There's nothing special here -- it's just plain old OLS, except all the 
variables on the right hand side are parameters (only the variable ydc is 
observed).  I have no problems getting this to recover parameter estimates 
from data I myself have generated for smaller problems (e.g. where ydc is 
a k=500 by n=50 matrix and there are 601 parameters).  But the target 
problem with real data will be k=6000 by n=400 with 6801 parameters.

I am currently using optim(method="BFGS") but it is slow.  I can get good 
starting values for x1 and g1 with some svd techniques, and these help me 
generate the starting values for the betas via lm().

I then use optim() on a modified likelihood function to find g1,x1,sigma2 
while holding beta fixed and then use optim() again to find beta while 
holding the other variables fixed.  But eventually, I have to run optim on 
the unmodified likelihood function above and it is very slow, taking 
several days for large problems.

I have also tried metrop() in mcmc, but I find this needs to be very 
close to the mode of the likelihood to be efficient (in fact, MCMCpack's 
metropolis function calls optim first and even requires it to invert the 
hessian before even starting the metropolis algorithm, unless we can 
provide our own covariance matrix).  I will probably use metrop() to 
generate standard errors once I find a mode....

In the mean time, I can't help thinking that there is some easy way to 
make this much faster than I am currently doing, especially since the 
likelihood is normal.  I am sure I have missed something obvious so I'd 
very much appreciate any advice you could give on packages in R or code 
that might help.

Thanks!
james

----------------------------------------------------------------------
James H. Fowler, Associate Professor   web:   http://jhfowler.ucsd.edu
Department of Political Science        email: jhfowler at ucsd.edu
University of California, San Diego    phone: (858) 534-6807
Social Sciences Building 383, #0521    fax:   (858) 534-7130
9500 Gilman Drive, La Jolla, CA 92093


From petr.pikal at precheza.cz  Fri Mar  2 10:54:57 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 02 Mar 2007 10:54:57 +0100
Subject: [R] Reformulated  matrices dimensions limitation problem
In-Reply-To: <JE9R1X$A34FE2FE383B5764E18B4760DF51C54D@libero.it>
Message-ID: <45E80281.8479.ACCA02@localhost>

Hi

creating a biggest possible matrix does not automaticaly mean you can 
do some computation with it. 

So the size will depend partly on what you want to do with it. I 
presume that you do not want only to create a matrix just for 
pleasure to be able to.

Cheers
Petr
 

On 2 Mar 2007 at 10:15, Bruno C. wrote:

Date sent:      	Fri, 2 Mar 2007 10:15:33 +0100
From:           	"Bruno C." <bruno.c at inwind.it>
To:             	"r-help" <r-help at r-project.org>
Subject:        	[R] Reformulated  matrices dimensions limitation problem

> First I wanted to thank both Marc Schwartz Greg Snow and for their
> reply.
> 
> Then I needed to add a level of complexity to the problem.
> I would be able to create the biggest possible matrix.
> 
> In other way does it exist a method to ask smthing like the following
> :
> 
> max number of rows for a matrix if column=x?
> 
> Thank you
> 
> 
> 
> 
> 
> ------------------------------------------------------
> Passa a Infostrada. ADSL e Telefono senza limiti e senza canone
> Telecom http://click.libero.it/infostrada2marz07
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From taojurongbe at yahoo.com  Fri Mar  2 11:11:23 2007
From: taojurongbe at yahoo.com (Taiwo Ojurongbe)
Date: Fri, 2 Mar 2007 02:11:23 -0800 (PST)
Subject: [R] Dice dissimilarity output and 'phylo' function in R
Message-ID: <374204.58820.qm@web60412.mail.yahoo.com>

Dear All,

I get some problems using the 'phylo' and
dissimilarity functions in R. I converted an output
from 'hclust' into an order of phylo so as to be able
to use the 'consensus' function on it. Each time I
submit the consensus codes, my computer hangs. When I
tried to see what the contents of the object converted
into order phylo is, I get the message 

"Phylogenetic tree with 0 tips and 7 internal nodes.
Tip labels:
NULL

Rooted; includes branch lengths".

So I guess this explains why the consensus function
does not work. 

Another thing I noticed in the output from the
'dissimilarity' function is that when I compared the
distances computed in R with that from NTSYS or SAS,
for example dice and jaccard coefficients I realised
that the dice distances are very different while the
jaccard distances are the same with those from these
other softwares. 


The codes I used for a small example are shown below:

samptest4<- scan (file = "samp-test4.txt")
samptest4<- matrix(data = samptest4,nrow=8, ncol=4,
byrow=T)

library(MASS) 
library(arules)
library(ape)
#CFI<- numeric (M)

CFI <- function(ctree)
{
    (ctree$Nnode -1)/(length(ctree$tip.label) - 2)
}
#calculation of dissimilarities & construction of
trees#

#Calc Jacc disimi#

disjacc <- dissimilarity(samptest4,
y=NULL,method="jaccard")

#Calc Dice disimi#

disdice <- dissimilarity(samptest4, 
y=NULL,method="dice")

#Construct Jacc dendro#

tjacc<- hclust(disjacc, method = "average")
tjaccphylo<- as.phylo(tjacc)

#Construct Dice dendro#

tdice<- hclust(disdice, method = "average")
tdicephylo<- as.phylo(tdice)

pdf (file = paste("TJD4", ".pdf", sep = ""))
par(mfrow = c(1,2))
plot (tjacc, hang = -1)
plot (tdice, hang = -1)
dev.off()

#Construct consensus tree#
ctree<-consensus(tdicephylo,tjaccphylo)
plot(ctree)
CFI(ctree) 


I will appreciate any help in solving these problems.

Thank you and best regards,

Taiwo 





 
____________________________________________________________________________________
Don't pick lemons.


From hb at stat.berkeley.edu  Fri Mar  2 11:33:10 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Fri, 2 Mar 2007 02:33:10 -0800
Subject: [R] hpush not allowed
Message-ID: <59d7961d0703020233m4114d2bv1c7b40a3407467f5@mail.gmail.com>

Hi,

I've get the following on both strider and frodo:

frodo{hb}: hpush -v
Sorry, user hb is not allowed to execute '/usr/local/bin/hpush -v' as
upush on frodo.Berkeley.EDU.

Strange, because it worked when we set it up Thursday.

Thxs

Henrik


From hb at stat.berkeley.edu  Fri Mar  2 11:34:00 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Fri, 2 Mar 2007 02:34:00 -0800
Subject: [R] hpush not allowed
In-Reply-To: <59d7961d0703020233m4114d2bv1c7b40a3407467f5@mail.gmail.com>
References: <59d7961d0703020233m4114d2bv1c7b40a3407467f5@mail.gmail.com>
Message-ID: <59d7961d0703020234j1b3e5e79uf0616c160b4e8be@mail.gmail.com>

[that one went to the wrong help at ... sorry about that. /Henrik]

On 3/2/07, Henrik Bengtsson <hb at stat.berkeley.edu> wrote:
> Hi,
>
> I've get the following on both strider and frodo:
>
> frodo{hb}: hpush -v
> Sorry, user hb is not allowed to execute '/usr/local/bin/hpush -v' as
> upush on frodo.Berkeley.EDU.
>
> Strange, because it worked when we set it up Thursday.
>
> Thxs
>
> Henrik
>


From ibanez at bioef.org  Fri Mar  2 11:54:57 2007
From: ibanez at bioef.org (Berta)
Date: Fri, 2 Mar 2007 11:54:57 +0100
Subject: [R] plot of 2 time series with very different values
Message-ID: <009001c75cb9$39416280$6601a8c0@BIOEF.ORG>


Hi R-Users,

I am trying to plot two time series in the same plot, but they measure 
different things and hence one
 has values around 1-9 (Use=c(7,8, 6, 2, 3)), and the other one around 
20-100 (Resitance=c(80, 100, 95, 35, 28)). I have thought of plotting both 
in the same graph but with two axes, one from 1 to 9 and the other from 20 
to 100. To do so, I needed to do a regression for corrsepondence (1 goes to 
20 and 9 goes to 100); the code to produce the graph would be:

plot(1996:2000, xlim=c(1996, 2000),ylab="Resistence", ylim=c(20,100), 
type="n", xlab="Date")
lines(1996:2000, c(80, 100, 95, 35, 28), col=1)
axis(side=4, at=c(20,30,40,50,60,70,80,90,100), labels=c(1:9))
lines(1996:2000, lsfit(c(1,9),c(20,100))$coef[1]+ 
lsfit(c(1,9),c(20,100))$coef[2]*c(7,8,6, 2, 3), col=2)
legend(1998.5, 90, legend=c("Resistence", "Use"), fill=c(1,2))

However, I suspect there are better ways to do so, and I would like to know 
one because I have to do that many times.

Thanks a lot in advance,

Berta


From i.visser at uva.nl  Fri Mar  2 12:08:20 2007
From: i.visser at uva.nl (Ingmar Visser)
Date: Fri, 2 Mar 2007 12:08:20 +0100
Subject: [R] vectorized dmvnorm
Message-ID: <A8A46040-D211-4EAE-9C48-798EBC5EE4B2@uva.nl>

Dear List,
Is there an R-function that computes vectorized densities of the  
multivariate normal distribution, ie vectorized in x, mean and sigma?

That is, a function that takes eg:

x <- matrix(0,3,2)
y <- matrix(1:6,3)-3
sig <- array(c(1,0,0,1,1,0.1,0.1,1,1,0.3,0.3,1),c(2,2,3))

 > x
      [,1] [,2]
[1,]    0    0
[2,]    0    0
[3,]    0    0
 > y
      [,1] [,2]
[1,]   -2    1
[2,]   -1    2
[3,]    0    3
 > sig
, , 1

      [,1] [,2]
[1,]    1    0
[2,]    0    1

, , 2

      [,1] [,2]
[1,]  1.0  0.1
[2,]  0.1  1.0

, , 3

      [,1] [,2]
[1,]  1.0  0.3
[2,]  0.3  1.0

and then returns a vector with elements:

dmvnorm(x[i,],y[i,],sig[,,i])

And/or is there another efficient of computing these without using  
loops:
for(i in 1:3)  print(dmvnorm(x[i,],y[i,],sig[,,i]))

best, Ingmar Visser


From jim at bitwrit.com.au  Fri Mar  2 12:39:35 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Fri, 02 Mar 2007 22:39:35 +1100
Subject: [R] barplot2, gap.barplot
In-Reply-To: <1172757385.5102.42.camel@stryder.skrupellos.priv>
References: <1172757385.5102.42.camel@stryder.skrupellos.priv>
Message-ID: <45E80CF7.9090404@bitwrit.com.au>

Marc A. Rohling wrote:
> Hello,
> 
> I try to handle a simple bar-plot, but it turns out to be not as simple
> as I thought.
...
> As you can see, because of the 4th bar (value > 45), the other bars look
> a little bit tiny: there is too much white-space. What I need to handle
> this problem is a function to insert a gap.
> 
> I tried to use gap.barplot, but unfortunately, it cannot handle any of
> the parameters I need from the barplot2-function.
> 
> After days of missing effort, I am sick of this problem. Is there a
> solution?

Hi Marc1,
As Marc2 said, the use of discontinuous axes in plots is a contentious 
one. No, I did not try to make gap.barplot compatible with barplot2 as 
the two seem to have different aims. gap.barplot is one solution to the 
troublesome issue of the outlier. What I would suggest as a one-off 
solution (to the horror of some) is to subtract, say, 25 from the 
outlier value, do the barplot, then:

par(xpd=TRUE)
axis.break(2,21,style="gap")
text(barpos[4],24,48.6)

I realize that your plot is more complex (I don't have gplots, etc. 
installed so I can't reproduce it at the moment), but that might give 
you something with which to work.

Jim


From petr.pikal at precheza.cz  Fri Mar  2 12:36:31 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 02 Mar 2007 12:36:31 +0100
Subject: [R] plot of 2 time series with very different values
In-Reply-To: <009001c75cb9$39416280$6601a8c0@BIOEF.ORG>
Message-ID: <45E81A4F.25966.109C635@localhost>

Hi

I use a function plot.yy which i designed for convenieant plotting on 
2 y axes for myself (see code below). You can modify its internals to 
suit your needs easily but this will give you something quite close.

plot.yy(1996:2000, c(80, 100, 95, 35, 28), c(7,8,6, 2, 3), 
xlim=c(1996, 2000),
yylab=c("Resistence","Use"), xlab="Date", pch=c(NA,NA), linky=T)

HTH
Petr

On 2 Mar 2007 at 11:54, Berta wrote:

From:           	"Berta" <ibanez at bioef.org>
To:             	<r-help at stat.math.ethz.ch>
Date sent:      	Fri, 2 Mar 2007 11:54:57 +0100
Organization:   	bioef
Subject:        	[R] plot of 2 time series with very different values

> 
> Hi R-Users,
> 
> I am trying to plot two time series in the same plot, but they measure
> different things and hence one
>  has values around 1-9 (Use=c(7,8, 6, 2, 3)), and the other one around
>  
> 20-100 (Resitance=c(80, 100, 95, 35, 28)). I have thought of plotting
> both in the same graph but with two axes, one from 1 to 9 and the
> other from 20 to 100. To do so, I needed to do a regression for
> corrsepondence (1 goes to 20 and 9 goes to 100); the code to produce
> the graph would be:
> 
> plot(1996:2000, xlim=c(1996, 2000),ylab="Resistence", ylim=c(20,100),
> type="n", xlab="Date") lines(1996:2000, c(80, 100, 95, 35, 28), col=1)
> axis(side=4, at=c(20,30,40,50,60,70,80,90,100), labels=c(1:9))
> lines(1996:2000, lsfit(c(1,9),c(20,100))$coef[1]+
> lsfit(c(1,9),c(20,100))$coef[2]*c(7,8,6, 2, 3), col=2) legend(1998.5,
> 90, legend=c("Resistence", "Use"), fill=c(1,2))
> 
> However, I suspect there are better ways to do so, and I would like to
> know one because I have to do that many times.
> 
> Thanks a lot in advance,
> 
> Berta
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Here is the code, all parameters are easily understood except of 
rect, which will was designed for a plotting a rectangle and you can 
ignore it completely.

plot.yy<-function(x,yright,yleft, yleftlim=NULL, yrightlim = NULL, 
xlab = NULL ,yylab=c("",""),
pch=c(1,2),col=c(1,2), linky=F, smooth=0, lwds=1, length=10, 
format="%d/%m", rect=NULL, type="p",...)

{

par(mar=c(5,4,4,2),oma=c(0,0,0,3))

plot(x, yright, ylim=yrightlim, axes=F,ylab="", xlab=xlab, 
pch=pch[1],col=col[1], type=type, ...)
if (!is.null(rect)) rect(x[rect[1]],rect[2],cas.osa[rect[3]],rect[4], 
col="grey")
points(x, yright, ylim=yrightlim, ylab="", xlab=xlab, 
pch=pch[1],col=col[1], ...)
axis(4,pretty(range(yright,na.rm=T),10),col=col[1])

if (linky) lines(x,yright,col=col[1], ...)
if (smooth!=0) lines(supsmu(x,yright,span=smooth),col=col[1], 
lwd=lwds, ...)

if(yylab[1]=="")
mtext(deparse(substitute(yright)),side=4,outer=T,line=1, col=col[1], 
...)
else
mtext(yylab[1],side=4,outer=T,line=1, col=col[1], ...)

par(new=T)
plot(x,yleft, ylim=yleftlim, ylab="", axes=F ,xlab=xlab, 
pch=pch[2],col=col[2], ...)
box()
axis(2,pretty(range(yleft,na.rm=T),10),col=col[2], col.axis=col[2])

if (!inherits(x,c("Date","POSIXt"))) 
axis(1,pretty(range(x,na.rm=T),10)) else
{
l<-length(x)
axis(1,at=x[seq(1,l,length=length)],labels=format(as.POSIXct(x[seq(1,l
,length=length)]),format=format))
}

if(yylab[2]=="")
mtext(deparse(substitute(yleft)),side=2,line=2, col=col[2], ...)
else
mtext(yylab[2],side=2,line=2, col=col[2], ...)

if (linky) lines(x,yleft,col=col[2], lty=2, ...)
if (smooth!=0) lines(supsmu(x,yleft,span=smooth),col=col[2], lty=2, 
lwd=lwds, ...)

}
Petr Pikal
petr.pikal at precheza.cz


From ibanez at bioef.org  Fri Mar  2 12:53:13 2007
From: ibanez at bioef.org (Berta)
Date: Fri, 2 Mar 2007 12:53:13 +0100
Subject: [R] plot of 2 time series with very different values
References: <45E81A4F.25966.109C635@localhost>
Message-ID: <00fa01c75cc1$5d316430$6601a8c0@BIOEF.ORG>

Thank you so much Petr, it is exaclty what I was looking for!!

Berta.


> Hi
> 
> I use a function plot.yy which i designed for convenieant plotting on 
> 2 y axes for myself (see code below). You can modify its internals to 
> suit your needs easily but this will give you something quite close.
> 
> plot.yy(1996:2000, c(80, 100, 95, 35, 28), c(7,8,6, 2, 3), 
> xlim=c(1996, 2000),
> yylab=c("Resistence","Use"), xlab="Date", pch=c(NA,NA), linky=T)
> 
> HTH
> Petr
> 
> On 2 Mar 2007 at 11:54, Berta wrote:
> 
> From:           "Berta" <ibanez at bioef.org>
> To:             <r-help at stat.math.ethz.ch>
> Date sent:      Fri, 2 Mar 2007 11:54:57 +0100
> Organization:   bioef
> Subject:        [R] plot of 2 time series with very different values
> 
>> 
>> Hi R-Users,
>> 
>> I am trying to plot two time series in the same plot, but they measure
>> different things and hence one
>>  has values around 1-9 (Use=c(7,8, 6, 2, 3)), and the other one around
>>  
>> 20-100 (Resitance=c(80, 100, 95, 35, 28)). I have thought of plotting
>> both in the same graph but with two axes, one from 1 to 9 and the
>> other from 20 to 100. To do so, I needed to do a regression for
>> corrsepondence (1 goes to 20 and 9 goes to 100); the code to produce
>> the graph would be:
>> 
>> plot(1996:2000, xlim=c(1996, 2000),ylab="Resistence", ylim=c(20,100),
>> type="n", xlab="Date") lines(1996:2000, c(80, 100, 95, 35, 28), col=1)
>> axis(side=4, at=c(20,30,40,50,60,70,80,90,100), labels=c(1:9))
>> lines(1996:2000, lsfit(c(1,9),c(20,100))$coef[1]+
>> lsfit(c(1,9),c(20,100))$coef[2]*c(7,8,6, 2, 3), col=2) legend(1998.5,
>> 90, legend=c("Resistence", "Use"), fill=c(1,2))
>> 
>> However, I suspect there are better ways to do so, and I would like to
>> know one because I have to do that many times.
>> 
>> Thanks a lot in advance,
>> 
>> Berta
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html and provide commented,
>> minimal, self-contained, reproducible code.
> 
> Here is the code, all parameters are easily understood except of 
> rect, which will was designed for a plotting a rectangle and you can 
> ignore it completely.
> 
> plot.yy<-function(x,yright,yleft, yleftlim=NULL, yrightlim = NULL, 
> xlab = NULL ,yylab=c("",""),
> pch=c(1,2),col=c(1,2), linky=F, smooth=0, lwds=1, length=10, 
> format="%d/%m", rect=NULL, type="p",...)
> 
> {
> 
> par(mar=c(5,4,4,2),oma=c(0,0,0,3))
> 
> plot(x, yright, ylim=yrightlim, axes=F,ylab="", xlab=xlab, 
> pch=pch[1],col=col[1], type=type, ...)
> if (!is.null(rect)) rect(x[rect[1]],rect[2],cas.osa[rect[3]],rect[4], 
> col="grey")
> points(x, yright, ylim=yrightlim, ylab="", xlab=xlab, 
> pch=pch[1],col=col[1], ...)
> axis(4,pretty(range(yright,na.rm=T),10),col=col[1])
> 
> if (linky) lines(x,yright,col=col[1], ...)
> if (smooth!=0) lines(supsmu(x,yright,span=smooth),col=col[1], 
> lwd=lwds, ...)
> 
> if(yylab[1]=="")
> mtext(deparse(substitute(yright)),side=4,outer=T,line=1, col=col[1], 
> ...)
> else
> mtext(yylab[1],side=4,outer=T,line=1, col=col[1], ...)
> 
> par(new=T)
> plot(x,yleft, ylim=yleftlim, ylab="", axes=F ,xlab=xlab, 
> pch=pch[2],col=col[2], ...)
> box()
> axis(2,pretty(range(yleft,na.rm=T),10),col=col[2], col.axis=col[2])
> 
> if (!inherits(x,c("Date","POSIXt"))) 
> axis(1,pretty(range(x,na.rm=T),10)) else
> {
> l<-length(x)
> axis(1,at=x[seq(1,l,length=length)],labels=format(as.POSIXct(x[seq(1,l
> ,length=length)]),format=format))
> }
> 
> if(yylab[2]=="")
> mtext(deparse(substitute(yleft)),side=2,line=2, col=col[2], ...)
> else
> mtext(yylab[2],side=2,line=2, col=col[2], ...)
> 
> if (linky) lines(x,yleft,col=col[2], lty=2, ...)
> if (smooth!=0) lines(supsmu(x,yleft,span=smooth),col=col[2], lty=2, 
> lwd=lwds, ...)
> 
> }
> Petr Pikal
> petr.pikal at precheza.cz
> 
> 
> 
>


From azzalini at stat.unipd.it  Fri Mar  2 13:02:35 2007
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Fri, 2 Mar 2007 13:02:35 +0100
Subject: [R] vectorized dmvnorm
In-Reply-To: <A8A46040-D211-4EAE-9C48-798EBC5EE4B2@uva.nl>
References: <A8A46040-D211-4EAE-9C48-798EBC5EE4B2@uva.nl>
Message-ID: <20070302130235.11e5b54c.azzalini@stat.unipd.it>

On Fri, 2 Mar 2007 12:08:20 +0100, Ingmar Visser wrote:

IV> Dear List,
IV> Is there an R-function that computes vectorized densities of the  
IV> multivariate normal distribution, ie vectorized in x, mean and sigma?
IV> 
IV> That is, a function that takes eg:
IV> 
IV> x <- matrix(0,3,2)
IV> y <- matrix(1:6,3)-3
IV> sig <- array(c(1,0,0,1,1,0.1,0.1,1,1,0.3,0.3,1),c(2,2,3))
IV> 
IV>  > x
IV>       [,1] [,2]
IV> [1,]    0    0
IV> [2,]    0    0
IV> [3,]    0    0
IV>  > y
IV>       [,1] [,2]
IV> [1,]   -2    1
IV> [2,]   -1    2
IV> [3,]    0    3
IV>  > sig
IV> , , 1
IV> 
IV>       [,1] [,2]
IV> [1,]    1    0
IV> [2,]    0    1
IV> 
IV> , , 2
IV> 
IV>       [,1] [,2]
IV> [1,]  1.0  0.1
IV> [2,]  0.1  1.0
IV> 
IV> , , 3
IV> 
IV>       [,1] [,2]
IV> [1,]  1.0  0.3
IV> [2,]  0.3  1.0
IV> 
IV> and then returns a vector with elements:
IV> 
IV> dmvnorm(x[i,],y[i,],sig[,,i])
IV> 
IV> And/or is there another efficient of computing these without using  
IV> loops:
IV> for(i in 1:3)  print(dmvnorm(x[i,],y[i,],sig[,,i]))
IV> 

a partial answer:
dmnorm() in package mnormt is vectorized for x, not for mean and covariance;
dmvnorm() in package mvtnorm works similarly

-- 
Adelchi Azzalini  <azzalini a stat.unipd.it>
Dipart.Scienze Statistiche, Universit? di Padova, Italia
tel. +39 049 8274147,  http://azzalini.stat.unipd.it/


From bruno.c at inwind.it  Fri Mar  2 13:12:07 2007
From: bruno.c at inwind.it (Bruno C.)
Date: Fri,  2 Mar 2007 13:12:07 +0100
Subject: [R] Reformulated  matrices dimensions limitation problem
Message-ID: <JE9Z87$712E68A81DB843FC1A2B0BB24DAB053B@libero.it>

You are right. and I am aware of that.

This is what I need to do:
load a regression model
load the bigget posible matrix
do prediction on this matrix

the first and 3rd step will not need to much memory... 

and I would really appreciate this degree of introspection from R: it would be great if there would be a package that could simulate the amount of memory needed from a process
But I don't pretend that much, this is why I am asking only about  a function able to build a matrix matrix, with size based on memory available...




> Hi
> 
> creating a biggest possible matrix does not automaticaly mean you can 
> do some computation with it. 
> 
> So the size will depend partly on what you want to do with it. I 
> presume that you do not want only to create a matrix just for 
> pleasure to be able to.
> 
> Cheers
> Petr
>  
> 
> On 2 Mar 2007 at 10:15, Bruno C. wrote:
> 
> Date sent:      	Fri, 2 Mar 2007 10:15:33 +0100
> From:           	"Bruno C." <bruno.c at inwind.it>
> To:             	"r-help" <r-help at r-project.org>
> Subject:        	[R] Reformulated  matrices dimensions limitation problem
> 
> > First I wanted to thank both Marc Schwartz Greg Snow and for their
> > reply.
> > 
> > Then I needed to add a level of complexity to the problem.
> > I would be able to create the biggest possible matrix.
> > 
> > In other way does it exist a method to ask smthing like the following
> > :
> > 
> > max number of rows for a matrix if column=x?
> > 
> > Thank you
> > 
> > 
> > 
> > 
> > 
> > ------------------------------------------------------
> > Passa a Infostrada. ADSL e Telefono senza limiti e senza canone
> > Telecom http://click.libero.it/infostrada2marz07
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> 


------------------------------------------------------
Passa a Infostrada. ADSL e Telefono senza limiti e senza canone Telecom
http://click.libero.it/infostrada2marz07


From snunes at gmail.com  Fri Mar  2 13:33:46 2007
From: snunes at gmail.com (=?ISO-8859-1?Q?S=E9rgio_Nunes?=)
Date: Fri, 2 Mar 2007 12:33:46 +0000
Subject: [R] Error in length of vector ?
Message-ID: <4c817d530703020433y60fbad9ah70f2deb38d8aebe5@mail.gmail.com>

Hi,

I'm having a weird result with the length() function:

>a
[... omited ...]
[9994] NA                    "2003-12-03 16:37:00" "2002-06-26 18:43:00"
[9997] "2005-07-04 04:00:00" "2007-02-16 22:09:00" "2007-02-24 15:49:00"
[10000] NA

> length(LastModified)
[1] 9

> length(c(LastModified))
[1] 9

I was expecting to get "10000" as an answer.
I'm trying to bind two vector, and I keep getting the error - "number
of rows of result is not a multiple of vector length". Thus I tested
length and got this value.

Any hint?

Thanks in advance,
S?rgio Nunes


From roger.bos at us.rothschild.com  Fri Mar  2 13:36:42 2007
From: roger.bos at us.rothschild.com (Bos, Roger)
Date: Fri, 2 Mar 2007 07:36:42 -0500
Subject: [R] FTP download from ftp.sec.gov
Message-ID: <D8C95B444AD6EE4AAD638D818A9CFD343A1E4B@RINNYCSE000.rth.ad.rothschild.com>

Thanks to everyone for trying it out.  I guess the part that surprises me is that I currently download a file from a different FTP site every night with no problem.  I have also downloaded from web sites with no problem.  Buy now that I know the R code is okay I can look into other fixes for the problems.  Thanks again, Roger


-----Original Message-----
From: Peter Dalgaard [mailto:P.Dalgaard at biostat.ku.dk] 
Sent: Friday, March 02, 2007 3:17 AM
To: Gabor Grothendieck
Cc: Bos, Roger; r-help at stat.math.ethz.ch
Subject: Re: [R] FTP download from ftp.sec.gov

Gabor Grothendieck wrote:
> On Windows XP it worked for me on both 2.4.1 and 2.5.0.  I did notice 
> that on 2.4.1 it says "using Synchronous WinInet calls" but does not 
> say this on 2.5.0.  See below for the two transcripts.
>
>   
>> ftp <- "ftp://anonymous:test at ftp.sec.gov/edgar/full-index/company.idx"
>> download.file(url=ftp, destfile="test.txt")
>>     
> trying URL 'ftp://anonymous:test at ftp.sec.gov/edgar/full-index/company.idx'
> using Synchronous WinInet calls
> opened URL
> downloaded 33930Kb
>   
This appears to be highly system dependent. Works for me on my home machine using Fedora 6, but not on the office machine running SUSE 10. I wouldn't be surprised if firewall configuration plays a part.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



********************************************************************** * 
This message is for the named person's use only. It may 
contain confidential, proprietary or legally privileged 
information. No right to confidential or privileged treatment 
of this message is waived or lost by any error in 
transmission. If you have received this message in error, 
please immediately notify the sender by e-mail, 
delete the message and all copies from your system and destroy 
any hard copies. You must not, directly or indirectly, use, 
disclose, distribute, print or copy any part of this message 
if you are not the intended recipient.


From petr.pikal at precheza.cz  Fri Mar  2 14:08:56 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 02 Mar 2007 14:08:56 +0100
Subject: [R] Error in length of vector ?
In-Reply-To: <4c817d530703020433y60fbad9ah70f2deb38d8aebe5@mail.gmail.com>
Message-ID: <45E82FF8.935.15E6163@localhost>

Hi

you stepped on a difference between POSIXct and POSIXlt

Details
There are two basic classes of date/times. Class "POSIXct" represents 
the (signed) number of seconds since the beginning of 1970 as a 
***numeric vector***. Class "POSIXlt" is a ***named list*** of 
vectors representing 

so you need to change your POSIXlt - named list to POSIXct by

as.POSIXct(your vector)

HTH
Petr

Maybe it could be useful to give some kind of warning into the help 
page of strptime e.g.

Be aware of length of an objects created by strptime as POSIXlt 
class.  It is always 9. See Details section of  DateTimeClasses.

 
On 2 Mar 2007 at 12:33, S?rgio Nunes wrote:

Date sent:      	Fri, 2 Mar 2007 12:33:46 +0000
From:           	"S?rgio Nunes" <snunes at gmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Error in length of vector ?

> Hi,
> 
> I'm having a weird result with the length() function:
> 
> >a
> [... omited ...]
> [9994] NA                    "2003-12-03 16:37:00" "2002-06-26
> 18:43:00" [9997] "2005-07-04 04:00:00" "2007-02-16 22:09:00"
> "2007-02-24 15:49:00" [10000] NA
> 
> > length(LastModified)
> [1] 9
> 
> > length(c(LastModified))
> [1] 9
> 
> I was expecting to get "10000" as an answer.
> I'm trying to bind two vector, and I keep getting the error - "number
> of rows of result is not a multiple of vector length". Thus I tested
> length and got this value.
> 
> Any hint?
> 
> Thanks in advance,
> S?rgio Nunes
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Fri Mar  2 14:13:00 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 02 Mar 2007 14:13:00 +0100
Subject: [R] Reformulated  matrices dimensions limitation problem
In-Reply-To: <JE9Z87$712E68A81DB843FC1A2B0BB24DAB053B@libero.it>
Message-ID: <45E830EC.1360.1621AB2@localhost>

Did you consider biglm  package. I did not use it myself but from its 
description it can be used for linear models on objects that do not 
fit into memory.

HTH
Petr


On 2 Mar 2007 at 13:12, Bruno C. wrote:

Date sent:      	Fri, 2 Mar 2007 13:12:07 +0100
From:           	"Bruno C." <bruno.c at inwind.it>
To:             	"petr.pikal" <petr.pikal at precheza.cz>
Copies to:      	r-help <r-help at r-project.org>
Subject:        	Re: [R] Reformulated  matrices dimensions limitation problem

> You are right. and I am aware of that.
> 
> This is what I need to do:
> load a regression model
> load the bigget posible matrix
> do prediction on this matrix
> 
> the first and 3rd step will not need to much memory... 
> 
> and I would really appreciate this degree of introspection from R: it
> would be great if there would be a package that could simulate the
> amount of memory needed from a process But I don't pretend that much,
> this is why I am asking only about  a function able to build a matrix
> matrix, with size based on memory available...
> 
> 
> 
> 
> > Hi
> > 
> > creating a biggest possible matrix does not automaticaly mean you
> > can do some computation with it. 
> > 
> > So the size will depend partly on what you want to do with it. I
> > presume that you do not want only to create a matrix just for
> > pleasure to be able to.
> > 
> > Cheers
> > Petr
> >  
> > 
> > On 2 Mar 2007 at 10:15, Bruno C. wrote:
> > 
> > Date sent:      	Fri, 2 Mar 2007 10:15:33 +0100
> > From:           	"Bruno C." <bruno.c at inwind.it>
> > To:             	"r-help" <r-help at r-project.org>
> > Subject:        	[R] Reformulated  matrices dimensions limitation
> > problem
> > 
> > > First I wanted to thank both Marc Schwartz Greg Snow and for their
> > > reply.
> > > 
> > > Then I needed to add a level of complexity to the problem.
> > > I would be able to create the biggest possible matrix.
> > > 
> > > In other way does it exist a method to ask smthing like the
> > > following :
> > > 
> > > max number of rows for a matrix if column=x?
> > > 
> > > Thank you
> > > 
> > > 
> > > 
> > > 
> > > 
> > > ------------------------------------------------------
> > > Passa a Infostrada. ADSL e Telefono senza limiti e senza canone
> > > Telecom http://click.libero.it/infostrada2marz07
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html and provide commented,
> > > minimal, self-contained, reproducible code.
> > 
> > Petr Pikal
> > petr.pikal at precheza.cz
> > 
> > 
> 
> 
> ------------------------------------------------------
> Passa a Infostrada. ADSL e Telefono senza limiti e senza canone
> Telecom http://click.libero.it/infostrada2marz07
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From snunes at gmail.com  Fri Mar  2 14:22:43 2007
From: snunes at gmail.com (=?ISO-8859-1?Q?S=E9rgio_Nunes?=)
Date: Fri, 2 Mar 2007 13:22:43 +0000
Subject: [R] Error in length of vector ?
In-Reply-To: <45E82FF8.935.15E6163@localhost>
References: <4c817d530703020433y60fbad9ah70f2deb38d8aebe5@mail.gmail.com>
	<45E82FF8.935.15E6163@localhost>
Message-ID: <4c817d530703020522w6ec9a5a0j93f81aebec14d1b2@mail.gmail.com>

Thanks. Using as.POSIXct() worked fine, length = 10000.
Basically I cannot have a column of POSIXlt values in a matrix?

S?rgio Nunes

On 3/2/07, Petr Pikal <petr.pikal at precheza.cz> wrote:
> Hi
>
> you stepped on a difference between POSIXct and POSIXlt
>
> Details
> There are two basic classes of date/times. Class "POSIXct" represents
> the (signed) number of seconds since the beginning of 1970 as a
> ***numeric vector***. Class "POSIXlt" is a ***named list*** of
> vectors representing
>
> so you need to change your POSIXlt - named list to POSIXct by
>
> as.POSIXct(your vector)
>
> HTH
> Petr
>
> Maybe it could be useful to give some kind of warning into the help
> page of strptime e.g.
>
> Be aware of length of an objects created by strptime as POSIXlt
> class.  It is always 9. See Details section of  DateTimeClasses.
>
>
> On 2 Mar 2007 at 12:33, S?rgio Nunes wrote:
>
> Date sent:              Fri, 2 Mar 2007 12:33:46 +0000
> From:                   "S?rgio Nunes" <snunes at gmail.com>
> To:                     r-help at stat.math.ethz.ch
> Subject:                [R] Error in length of vector ?
>
> > Hi,
> >
> > I'm having a weird result with the length() function:
> >
> > >a
> > [... omited ...]
> > [9994] NA                    "2003-12-03 16:37:00" "2002-06-26
> > 18:43:00" [9997] "2005-07-04 04:00:00" "2007-02-16 22:09:00"
> > "2007-02-24 15:49:00" [10000] NA
> >
> > > length(LastModified)
> > [1] 9
> >
> > > length(c(LastModified))
> > [1] 9
> >
> > I was expecting to get "10000" as an answer.
> > I'm trying to bind two vector, and I keep getting the error - "number
> > of rows of result is not a multiple of vector length". Thus I tested
> > length and got this value.
> >
> > Any hint?
> >
> > Thanks in advance,
> > S?rgio Nunes
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
>
> Petr Pikal
> petr.pikal at precheza.cz
>
>


From Mat.Soukup at fda.hhs.gov  Fri Mar  2 14:28:45 2007
From: Mat.Soukup at fda.hhs.gov (Soukup, Mat)
Date: Fri, 2 Mar 2007 08:28:45 -0500
Subject: [R] Using R for devices trial
In-Reply-To: <OFC2E9EDF7.C0709DCC-ON88257291.007CBF3D-88257291.007C91B1@irvine.edwards.com>
References: <OFC2E9EDF7.C0709DCC-ON88257291.007CBF3D-88257291.007C91B1@irvine.edwards.com>
Message-ID: <27CA3827C6B33E40874682C469E774DD04DB910A@FMD3CT001.fda.gov>

Hi Cody,

I would point you to the presentation by Sue Bell at least year's ASA
meeting available here:
http://www.fda.gov/Cder/Offices/Biostatistics/Bell.pdf. I can't speak
for CDRH, but at CDER we are making some progress towards the level of
comfort with the use of R as a "valid" software tool. Specifically, 
1. R was granted approval by our IT folks for use on our government PC's
(2 years in the making to get this).
2. An R course is in development for FDA reviewers to use R for review
of clinical trial data.
3. This Monday at the 1st FDA/DIA Spring Meeting I will be offering a
tutorial on Statistical Graphics with R for Clinical Trial Data.
4. An increasing effort to pigeon-tail R to ongoing projects to show
proof by example that R can be trusted when used properly.

So from the regulatory side, progress is being made, but there still do
exist those who have some discomfort with the use of an open-source tool
for data analysis. Someday hopefully that too will be changed.

HTH,

-Mat

Disclaimer: The views expressed are those of the author and must not be
taken to represent policy or guidance on behalf of the Food and Drug
Administration.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
Cody_Hamilton at edwards.com
Sent: Thursday, March 01, 2007 5:43 PM
To: r-help at r-project.org
Subject: [R] Using R for devices trial


I would like to use R for submissions to FDA/CDRH (the medical device
company I work for currently uses only SAS).  Previous postings to the
list
regarding R and 21 CFR 11 compliance have been very helpful.  However,
reluctance to using open source software for statistical analyses and
reporting remains high here at my company.  Has anyone used R for an
official submission to FDA/CDRH?  It would be most helpful if I could
tell
our group that others have been able to use R for this purpose.

Regards,
Cody Hamilton
Staff Biostatistician
Edwards Lifesciences

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From valderama at gmail.com  Fri Mar  2 14:32:29 2007
From: valderama at gmail.com (Laurent Valdes)
Date: Fri, 2 Mar 2007 14:32:29 +0100
Subject: [R] PROC TABULATE with R
In-Reply-To: <45E700AD.3040604@wu-wien.ac.at>
References: <45E700AD.3040604@wu-wien.ac.at>
Message-ID: <3ef00e160703020532i43435275h7dbfb15651419b02@mail.gmail.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070302/ba2a5da6/attachment.pl 

From bruno.c at inwind.it  Fri Mar  2 14:40:18 2007
From: bruno.c at inwind.it (Bruno C.)
Date: Fri,  2 Mar 2007 14:40:18 +0100
Subject: [R] Reformulated  matrices dimensions limitation problem
Message-ID: <JEA3B6$D049EC29F5E065770DE226C0B47F6466@libero.it>

bigml is for generalized regression model. I need to use lars ans svm.
Anway I am perfectly able to train my model: my training matrix is not so big.
The big matrix is the test one then I can split it by rows  into several matrices without affecting the results.
The point is that I wanted to  split it in an elegant way, mainimizing the needed submatrices wrt memory.
But I have the impression I am asking R too much :D

 
> Did you consider biglm  package. I did not use it myself but from its 
> description it can be used for linear models on objects that do not 
> fit into memory.
> 
> HTH
> Petr
> 
> 
> On 2 Mar 2007 at 13:12, Bruno C. wrote:
> 
> Date sent:      	Fri, 2 Mar 2007 13:12:07 +0100
> From:           	"Bruno C." <bruno.c at inwind.it>
> To:             	"petr.pikal" <petr.pikal at precheza.cz>
> Copies to:      	r-help <r-help at r-project.org>
> Subject:        	Re: [R] Reformulated  matrices dimensions limitation problem
> 
> > You are right. and I am aware of that.
> > 
> > This is what I need to do:
> > load a regression model
> > load the bigget posible matrix
> > do prediction on this matrix
> > 
> > the first and 3rd step will not need to much memory... 
> > 
> > and I would really appreciate this degree of introspection from R: it
> > would be great if there would be a package that could simulate the
> > amount of memory needed from a process But I don't pretend that much,
> > this is why I am asking only about  a function able to build a matrix
> > matrix, with size based on memory available...
> > 
> > 
> > 
> > 
> > > Hi
> > > 
> > > creating a biggest possible matrix does not automaticaly mean you
> > > can do some computation with it. 
> > > 
> > > So the size will depend partly on what you want to do with it. I
> > > presume that you do not want only to create a matrix just for
> > > pleasure to be able to.
> > > 
> > > Cheers
> > > Petr
> > >  
> > > 
> > > On 2 Mar 2007 at 10:15, Bruno C. wrote:
> > > 
> > > Date sent:      	Fri, 2 Mar 2007 10:15:33 +0100
> > > From:           	"Bruno C." <bruno.c at inwind.it>
> > > To:             	"r-help" <r-help at r-project.org>
> > > Subject:        	[R] Reformulated  matrices dimensions limitation
> > > problem
> > > 
> > > > First I wanted to thank both Marc Schwartz Greg Snow and for their
> > > > reply.
> > > > 
> > > > Then I needed to add a level of complexity to the problem.
> > > > I would be able to create the biggest possible matrix.
> > > > 
> > > > In other way does it exist a method to ask smthing like the
> > > > following :
> > > > 
> > > > max number of rows for a matrix if column=x?
> > > > 
> > > > Thank you
> > > > 
> > > > 
> > > > 
> > > > 
> > > > 
> > > > ------------------------------------------------------
> > > > Passa a Infostrada. ADSL e Telefono senza limiti e senza canone
> > > > Telecom http://click.libero.it/infostrada2marz07
> > > > 
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html and provide commented,
> > > > minimal, self-contained, reproducible code.
> > > 
> > > Petr Pikal
> > > petr.pikal at precheza.cz
> > > 
> > > 
> > 
> > 
> > ------------------------------------------------------
> > Passa a Infostrada. ADSL e Telefono senza limiti e senza canone
> > Telecom http://click.libero.it/infostrada2marz07
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> 


------------------------------------------------------
Passa a Infostrada. ADSL e Telefono senza limiti e senza canone Telecom
http://click.libero.it/infostrada2marz07


From jholtman at gmail.com  Fri Mar  2 14:46:02 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 2 Mar 2007 08:46:02 -0500
Subject: [R] function with Multiple Output
In-Reply-To: <1172748932.29120.15.camel@OGC-FU-WorkStation>
References: <1172748932.29120.15.camel@OGC-FU-WorkStation>
Message-ID: <644e1f320703020546r2fa8c190m749d67bafecd0443@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070302/30b12776/attachment.pl 

From petr.pikal at precheza.cz  Fri Mar  2 15:18:03 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 02 Mar 2007 15:18:03 +0100
Subject: [R] Error in length of vector ?
In-Reply-To: <4c817d530703020522w6ec9a5a0j93f81aebec14d1b2@mail.gmail.com>
References: <45E82FF8.935.15E6163@localhost>
Message-ID: <45E8402B.29991.19DA733@localhost>



On 2 Mar 2007 at 13:22, S?rgio Nunes wrote:

Date sent:      	Fri, 2 Mar 2007 13:22:43 +0000
From:           	"S?rgio Nunes" <snunes at gmail.com>
To:             	"Petr Pikal" <petr.pikal at precheza.cz>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] Error in length of vector ?

> Thanks. Using as.POSIXct() worked fine, length = 10000.
> Basically I cannot have a column of POSIXlt values in a matrix?

No, it has different length as it is a ***named list*** see e.g. 
Last.Modified[[1]] if it is POSIXlt class.

Petr

> 
> S?rgio Nunes
> 
> On 3/2/07, Petr Pikal <petr.pikal at precheza.cz> wrote:
> > Hi
> >
> > you stepped on a difference between POSIXct and POSIXlt
> >
> > Details
> > There are two basic classes of date/times. Class "POSIXct"
> > represents the (signed) number of seconds since the beginning of
> > 1970 as a ***numeric vector***. Class "POSIXlt" is a ***named
> > list*** of vectors representing
> >
> > so you need to change your POSIXlt - named list to POSIXct by
> >
> > as.POSIXct(your vector)
> >
> > HTH
> > Petr
> >
> > Maybe it could be useful to give some kind of warning into the help
> > page of strptime e.g.
> >
> > Be aware of length of an objects created by strptime as POSIXlt
> > class.  It is always 9. See Details section of  DateTimeClasses.
> >
> >
> > On 2 Mar 2007 at 12:33, S?rgio Nunes wrote:
> >
> > Date sent:              Fri, 2 Mar 2007 12:33:46 +0000
> > From:                   "S?rgio Nunes" <snunes at gmail.com>
> > To:                     r-help at stat.math.ethz.ch
> > Subject:                [R] Error in length of vector ?
> >
> > > Hi,
> > >
> > > I'm having a weird result with the length() function:
> > >
> > > >a
> > > [... omited ...]
> > > [9994] NA                    "2003-12-03 16:37:00" "2002-06-26
> > > 18:43:00" [9997] "2005-07-04 04:00:00" "2007-02-16 22:09:00"
> > > "2007-02-24 15:49:00" [10000] NA
> > >
> > > > length(LastModified)
> > > [1] 9
> > >
> > > > length(c(LastModified))
> > > [1] 9
> > >
> > > I was expecting to get "10000" as an answer.
> > > I'm trying to bind two vector, and I keep getting the error -
> > > "number of rows of result is not a multiple of vector length".
> > > Thus I tested length and got this value.
> > >
> > > Any hint?
> > >
> > > Thanks in advance,
> > > S?rgio Nunes
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html and provide commented,
> > > minimal, self-contained, reproducible code.
> >
> > Petr Pikal
> > petr.pikal at precheza.cz
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Fri Mar  2 15:22:06 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 02 Mar 2007 15:22:06 +0100
Subject: [R] Reformulated  matrices dimensions limitation problem
In-Reply-To: <JEA3B6$D049EC29F5E065770DE226C0B47F6466@libero.it>
Message-ID: <45E8411E.30416.1A15F2B@localhost>

I believe that due to memory management the size of  biggest possible 
matrix can change. But I probably am not the persou who can explain 
it correctly.

Petr


On 2 Mar 2007 at 14:40, Bruno C. wrote:

Date sent:      	Fri, 2 Mar 2007 14:40:18 +0100
From:           	"Bruno C." <bruno.c at inwind.it>
To:             	"petr.pikal" <petr.pikal at precheza.cz>
Copies to:      	r-help <r-help at r-project.org>
Subject:        	Re: [R] Reformulated  matrices dimensions limitation problem

> bigml is for generalized regression model. I need to use lars ans svm.
> Anway I am perfectly able to train my model: my training matrix is not
> so big. The big matrix is the test one then I can split it by rows 
> into several matrices without affecting the results. The point is that
> I wanted to  split it in an elegant way, mainimizing the needed
> submatrices wrt memory. But I have the impression I am asking R too
> much :D
> 
> 
> > Did you consider biglm  package. I did not use it myself but from
> > its description it can be used for linear models on objects that do
> > not fit into memory.
> > 
> > HTH
> > Petr
> > 
> > 
> > On 2 Mar 2007 at 13:12, Bruno C. wrote:
> > 
> > Date sent:      	Fri, 2 Mar 2007 13:12:07 +0100
> > From:           	"Bruno C." <bruno.c at inwind.it>
> > To:             	"petr.pikal" <petr.pikal at precheza.cz>
> > Copies to:      	r-help <r-help at r-project.org>
> > Subject:        	Re: [R] Reformulated  matrices dimensions
> > limitation problem
> > 
> > > You are right. and I am aware of that.
> > > 
> > > This is what I need to do:
> > > load a regression model
> > > load the bigget posible matrix
> > > do prediction on this matrix
> > > 
> > > the first and 3rd step will not need to much memory... 
> > > 
> > > and I would really appreciate this degree of introspection from R:
> > > it would be great if there would be a package that could simulate
> > > the amount of memory needed from a process But I don't pretend
> > > that much, this is why I am asking only about  a function able to
> > > build a matrix matrix, with size based on memory available...
> > > 
> > > 
> > > 
> > > 
> > > > Hi
> > > > 
> > > > creating a biggest possible matrix does not automaticaly mean
> > > > you can do some computation with it. 
> > > > 
> > > > So the size will depend partly on what you want to do with it. I
> > > > presume that you do not want only to create a matrix just for
> > > > pleasure to be able to.
> > > > 
> > > > Cheers
> > > > Petr
> > > >  
> > > > 
> > > > On 2 Mar 2007 at 10:15, Bruno C. wrote:
> > > > 
> > > > Date sent:      	Fri, 2 Mar 2007 10:15:33 +0100
> > > > From:           	"Bruno C." <bruno.c at inwind.it>
> > > > To:             	"r-help" <r-help at r-project.org>
> > > > Subject:        	[R] Reformulated  matrices dimensions
> > > > limitation problem
> > > > 
> > > > > First I wanted to thank both Marc Schwartz Greg Snow and for
> > > > > their reply.
> > > > > 
> > > > > Then I needed to add a level of complexity to the problem. I
> > > > > would be able to create the biggest possible matrix.
> > > > > 
> > > > > In other way does it exist a method to ask smthing like the
> > > > > following :
> > > > > 
> > > > > max number of rows for a matrix if column=x?
> > > > > 
> > > > > Thank you
> > > > > 
> > > > > 
> > > > > 
> > > > > 
> > > > > 
> > > > > ------------------------------------------------------
> > > > > Passa a Infostrada. ADSL e Telefono senza limiti e senza
> > > > > canone Telecom http://click.libero.it/infostrada2marz07
> > > > > 
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide
> > > > > http://www.R-project.org/posting-guide.html and provide
> > > > > commented, minimal, self-contained, reproducible code.
> > > > 
> > > > Petr Pikal
> > > > petr.pikal at precheza.cz
> > > > 
> > > > 
> > > 
> > > 
> > > ------------------------------------------------------
> > > Passa a Infostrada. ADSL e Telefono senza limiti e senza canone
> > > Telecom http://click.libero.it/infostrada2marz07
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html and provide commented,
> > > minimal, self-contained, reproducible code.
> > 
> > Petr Pikal
> > petr.pikal at precheza.cz
> > 
> > 
> 
> 
> ------------------------------------------------------
> Passa a Infostrada. ADSL e Telefono senza limiti e senza canone
> Telecom http://click.libero.it/infostrada2marz07
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From fisk at bowdoin.edu  Fri Mar  2 15:25:23 2007
From: fisk at bowdoin.edu (steve)
Date: Fri, 02 Mar 2007 09:25:23 -0500
Subject: [R] question about xtable and Hmisc
In-Reply-To: <20070301165513.BVR96835@po-d.temple.edu>
References: <20070301165513.BVR96835@po-d.temple.edu>
Message-ID: <es9c4j$dc5$1@sea.gmane.org>

Richard M. Heiberger wrote:
>>> Also, if x is a data frame, latex(x) contains the row numbers.
>>> Can I get rid of them here as well?
> 
> 
> I think you are asking for the rowname=NULL argument.
>    latex(x, rowname=NULL)
> See ?latex to confirm if that is what you are looking for.
> 
> Rich
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

Yes - that is exactly what I wanted.

thanks,

Steve


From danbebber at forestecology.co.uk  Fri Mar  2 15:34:01 2007
From: danbebber at forestecology.co.uk (Dan Bebber)
Date: Fri, 2 Mar 2007 14:34:01 -0000
Subject: [R] lattice: clipping data, not plot margins
Message-ID: <000801c75cd7$d208bae0$d22401a3@plants.ox.ac.uk>

I am plotting subsets of my data, using ylim.
This works fine, but the outer margin line widths of the plot are thin, due 
to clipping.
If I include
> trellis.par.set(clip=list(panel = "off"))
then the outer margin line widths are fine, but the outlying data is 
visible.

Is there any way of achieving both correct margin line widths and clipping 
of outlying data?

Thanks,
Dan Bebber

info: Windows XP, R 2.4.1., lattice 0.14-16


From cincinattikid at bigpond.com  Fri Mar  2 02:54:23 2007
From: cincinattikid at bigpond.com (Alfonso Sammassimo)
Date: Fri, 2 Mar 2007 12:54:23 +1100
Subject: [R] extracting data from zoo series
Message-ID: <000501c75c6d$b35f33e0$0300a8c0@Vaio>

Dear List,

Sorry if I'm overlooking something simple here but I have gotten a bit 
tangled.

I am trying to print the next five values(with their dates), which occur 
after a certain condition is met.

I have a series of data in zoo format, call it "A". From this series I have 
created a subset (also in zoo format) based on a certain condition, call 
this series "B".

I want to use the dates from "B"  as an index, so as each of these dates 
occur in "A", the succeeding five rows  from "A" after this date are printed 
out.

Any help would be most appreciated.

Regards,
Alfonso Sammassimo
Melbourne, Victoria


From Roger.Vallejo at ARS.USDA.GOV  Fri Mar  2 15:39:09 2007
From: Roger.Vallejo at ARS.USDA.GOV (Vallejo, Roger)
Date: Fri, 2 Mar 2007 09:39:09 -0500
Subject: [R] LIMMA contrast.matrix
Message-ID: <F649B8DBD3DEAD4BB163AC4D9AC18B5F41BDCA@MD-MAIL-01.ARSNET.ARS.USDA.GOV>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070302/1087c9bf/attachment.pl 

From liuwensui at gmail.com  Fri Mar  2 15:43:15 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Fri, 2 Mar 2007 09:43:15 -0500
Subject: [R] [friday topic]: what exactly is statistical computing
Message-ID: <1115a2b00703020643r5e0ee8d5n7f7399dc0bb3c22c@mail.gmail.com>

Dear List,
on www.r-project.org, the title says 'The R Project for Statistical Computing'.

but what exactly is the definition of statistical computing?


From tamir at imp.univie.ac.at  Fri Mar  2 15:42:46 2007
From: tamir at imp.univie.ac.at (Ido M. Tamir)
Date: Fri, 2 Mar 2007 15:42:46 +0100
Subject: [R] from function to its name?
Message-ID: <200703021542.46118.tamir@imp.univie.ac.at>

Hi,

I can get from a string to a function with this name:

>f1 <- function(x){ mean(x) }

>do.call("f1",list{1:4})
>get("f1")
etc...

But how do I get from a function to its name?

>funcVec <- c(f1,median)

>funcVec
[[1]]
function(x){ mean(x) }
> str(funcVec)
List of 2
 $ :function (x)
  ..- attr(*, "source")= chr "function(x){ mean(x) }"
 $ :function (x, ...)
> deparse(funcVec[1])
[1] "list(function (x) " "{"                  "    mean(x)"
[4] "})"


thank you very much
ido


From rvaradhan at jhmi.edu  Fri Mar  2 15:50:40 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Fri, 2 Mar 2007 09:50:40 -0500
Subject: [R] Help with faster optimization for large parameter problem
In-Reply-To: <Pine.LNX.4.64.0703020110540.29534@jhfowler.ucsd.edu>
References: <Pine.LNX.4.64.0703020110540.29534@jhfowler.ucsd.edu>
Message-ID: <000301c75cda$26061ff0$7c94100a@win.ad.jhu.edu>

Hi James,

There are a few things that immediately come to my mind that you could try:

1.  Profile your code using Rprof to detect which steps are the most
time-consuming.
2.  In your likelihood function you can make the code a bit more faster by
using "outer" instead of %*%. 
3.  Using conjugate-gradient type methods in "optim" might be a better
option since you are dealing with thousands of parameters and "BFGS" could
use up a lot of memory working with a large Hessian matrix.  CG methods use
much less storage and memory than quasi-Newton methods.  The main caveat
here is that the CG methods generally have slower convergence than QN type
methods, unless you can precondition the problem.

Hope this is helpful,
Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of James Fowler
Sent: Friday, March 02, 2007 4:34 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Help with faster optimization for large parameter problem

Hello all,

I have a large parameter problem with the following very simple likelihood 
function:

fn<-function(param) {
  x1<-param[1:n]
  g1<-param[(n+1):(2*n)]
  beta<-param[(2*n+1):(2*n+k)]
  sigma2<-param[2*n+k+1]^2
  meang1sp<-mean(g1[sp])
  mu<-beta%*%matrix(x1,1,n)-(g1[sp]-meang1sp)%*%matrix(g1,1,n)
  return(sum((ydc-mu)^2)/(2*sigma2) + n*k*log(sqrt(sigma2)) +
   mean(x1)^2 + mean(g1)^2 + 1000*(x1[1]>x1[n]))
}

There's nothing special here -- it's just plain old OLS, except all the 
variables on the right hand side are parameters (only the variable ydc is 
observed).  I have no problems getting this to recover parameter estimates 
from data I myself have generated for smaller problems (e.g. where ydc is 
a k=500 by n=50 matrix and there are 601 parameters).  But the target 
problem with real data will be k=6000 by n=400 with 6801 parameters.

I am currently using optim(method="BFGS") but it is slow.  I can get good 
starting values for x1 and g1 with some svd techniques, and these help me 
generate the starting values for the betas via lm().

I then use optim() on a modified likelihood function to find g1,x1,sigma2 
while holding beta fixed and then use optim() again to find beta while 
holding the other variables fixed.  But eventually, I have to run optim on 
the unmodified likelihood function above and it is very slow, taking 
several days for large problems.

I have also tried metrop() in mcmc, but I find this needs to be very 
close to the mode of the likelihood to be efficient (in fact, MCMCpack's 
metropolis function calls optim first and even requires it to invert the 
hessian before even starting the metropolis algorithm, unless we can 
provide our own covariance matrix).  I will probably use metrop() to 
generate standard errors once I find a mode....

In the mean time, I can't help thinking that there is some easy way to 
make this much faster than I am currently doing, especially since the 
likelihood is normal.  I am sure I have missed something obvious so I'd 
very much appreciate any advice you could give on packages in R or code 
that might help.

Thanks!
james

----------------------------------------------------------------------
James H. Fowler, Associate Professor   web:   http://jhfowler.ucsd.edu
Department of Political Science        email: jhfowler at ucsd.edu
University of California, San Diego    phone: (858) 534-6807
Social Sciences Building 383, #0521    fax:   (858) 534-7130
9500 Gilman Drive, La Jolla, CA 92093

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From h.wickham at gmail.com  Fri Mar  2 15:53:36 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 2 Mar 2007 08:53:36 -0600
Subject: [R] barplot2, gap.barplot
In-Reply-To: <1172757514.4897.42.camel@localhost.localdomain>
References: <1172757385.5102.42.camel@stryder.skrupellos.priv>
	<1172757514.4897.42.camel@localhost.localdomain>
Message-ID: <f8e6ff050703020653o1b2f9485v82c78eca40df0890@mail.gmail.com>

> 3. Depending on the nature of your data, if the extreme value is
> representative of an important marked difference relative to the other
> values, then I don't particularly find the 'look' of the plot to be
> overly problematic. It does appropriately emphasize the large
> difference.
>
> On the other hand, you might want to consider using a log scale on the y
> axis as an alternative to an axis gap. This would be a reasonable
> approach to plotting values that have a notable difference in range.  If
> you do this, note that you would need to ensure that all y values are >0
> (ie. y axis range minimum, lower bounds of CI's, etc.) since:
>
> > log10(0)
> [1] -Inf
>
>

Of course, you can't do this with a bar plot, because bars should be
anchored at 0.

Hadley


From h.wickham at gmail.com  Fri Mar  2 15:56:57 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 2 Mar 2007 08:56:57 -0600
Subject: [R] lattice: clipping data, not plot margins
In-Reply-To: <000801c75cd7$d208bae0$d22401a3@plants.ox.ac.uk>
References: <000801c75cd7$d208bae0$d22401a3@plants.ox.ac.uk>
Message-ID: <f8e6ff050703020656p309f7afh46226d5b9b4b14ae@mail.gmail.com>

On 3/2/07, Dan Bebber <danbebber at forestecology.co.uk> wrote:
> I am plotting subsets of my data, using ylim.
> This works fine, but the outer margin line widths of the plot are thin, due
> to clipping.
> If I include
> > trellis.par.set(clip=list(panel = "off"))
> then the outer margin line widths are fine, but the outlying data is
> visible.
>
> Is there any way of achieving both correct margin line widths and clipping
> of outlying data?

Why not subset your data?  (Of course if you are overlaying
statistical transformations, eg. a smoother, this will affect those
displays as well)

Hadley


From h.wickham at gmail.com  Fri Mar  2 15:59:35 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 2 Mar 2007 08:59:35 -0600
Subject: [R] Reformulated matrices dimensions limitation problem
In-Reply-To: <JEA3B6$D049EC29F5E065770DE226C0B47F6466@libero.it>
References: <JEA3B6$D049EC29F5E065770DE226C0B47F6466@libero.it>
Message-ID: <f8e6ff050703020659k3f55b48dv415d0ed847ce91d1@mail.gmail.com>

On 3/2/07, Bruno C. <bruno.c at inwind.it> wrote:
> bigml is for generalized regression model. I need to use lars ans svm.
> Anway I am perfectly able to train my model: my training matrix is not so big.
> The big matrix is the test one then I can split it by rows  into several matrices without affecting the results.
> The point is that I wanted to  split it in an elegant way, mainimizing the needed submatrices wrt memory.
> But I have the impression I am asking R too much :D
>

Design an experiment.  Start with a small matrix and keep increasing
until it gets too big.  You will want to control for other factors
like what other objects are in memory.

I don't think this is particularly more elegant than splitting the
matrix into pieces that you know will fit into memory.  And will
certainly take much more time.

Hadley


From j.van_den_hoff at fzd.de  Fri Mar  2 16:01:19 2007
From: j.van_den_hoff at fzd.de (Joerg van den Hoff)
Date: Fri, 2 Mar 2007 16:01:19 +0100
Subject: [R] Double-banger function names: preferences and suggestions
In-Reply-To: <BAY116-DAV4AC499B924DC4285E9C03CF800@phx.gbl>
References: <f8e6ff050702250644q63cc1455t5a4c20768a1be73c@mail.gmail.com>
	<BAY116-DAV4AC499B924DC4285E9C03CF800@phx.gbl>
Message-ID: <20070302150119.GB4620@marco.fz-rossendorf.de>

On Thu, Mar 01, 2007 at 10:31:08AM -0700, Jason Barnhart wrote:
> Definitely not #2.   Prefer #1 but #3 is ok as well.

  Definitely not #1.   Prefer #2 but #3 is ok as well. 
  
  there is nothing like unanimous agreement :-)

  but in earnest: I don't think #1 is good on the already mentioned grounds
  that it masks the difference between a method call and a simple function name.
  I don't think a function name such as

  plot.mydata

  would be reasonable: is this a method call for plotting objects of class
  `mdyata' or is it some other function call (probably plotting my data)?

  in browsing through source code this is at least an unnecessary nuisance
  having to check this point.

> 
> Thanks for contributing and inquiring.
> 
> 
> ----- Original Message ----- 
> From: "hadley wickham" <h.wickham at gmail.com>
> To: <R-help at r-project.org>
> Sent: Sunday, February 25, 2007 7:44 AM
> Subject: [R] Double-banger function names: preferences and suggestions
> 
> 
> > What do you prefer/recommend for double-banger function names:
> >
> > 1 scale.colour
> > 2 scale_colour
> > 3 scaleColour
> >
> > 1 is more R-like, but conflicts with S3.  2 is a modern version of
> > number 1, but not many packages use it.  Number 3 is more java-like.
> > (I like number 2 best)
> >
> > Any suggestions?
> >
> > Thanks,
> >
> > Hadley
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Manager at hasslefree.net  Fri Mar  2 16:04:27 2007
From: Manager at hasslefree.net (Quote)
Date: Fri, 02 Mar 2007 07:04:27 -0800
Subject: [R] Account Completed
Message-ID: <5D6A.A7EB.82.605@dg.net>



Thank you for inquiring about your mortgage rate.

Feel free to visit our Hassle-FreeQuote website
so we can review more specific information in
order to reduce your current rate. The process
takes less than a minute to complete. We're happy
to have this chance to save you even more money.

*Current Rates: 3.81 - 4.92
*Average savings: $3,000 - $12,000 per year

Hassle-FreeQuotes?

Secure Website---siloids.com


From ggrothendieck at gmail.com  Fri Mar  2 16:30:58 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Mar 2007 10:30:58 -0500
Subject: [R] plot of 2 time series with very different values
In-Reply-To: <009001c75cb9$39416280$6601a8c0@BIOEF.ORG>
References: <009001c75cb9$39416280$6601a8c0@BIOEF.ORG>
Message-ID: <971536df0703020730p6b2501b8ye852d8d9d364f411@mail.gmail.com>

There is an example in the example section of plotting two time series
on the same plot with different left hand and right hand scales here:

library(zoo)
?plot.zoo

On 3/2/07, Berta <ibanez at bioef.org> wrote:
>
> Hi R-Users,
>
> I am trying to plot two time series in the same plot, but they measure
> different things and hence one
>  has values around 1-9 (Use=c(7,8, 6, 2, 3)), and the other one around
> 20-100 (Resitance=c(80, 100, 95, 35, 28)). I have thought of plotting both
> in the same graph but with two axes, one from 1 to 9 and the other from 20
> to 100. To do so, I needed to do a regression for corrsepondence (1 goes to
> 20 and 9 goes to 100); the code to produce the graph would be:
>
> plot(1996:2000, xlim=c(1996, 2000),ylab="Resistence", ylim=c(20,100),
> type="n", xlab="Date")
> lines(1996:2000, c(80, 100, 95, 35, 28), col=1)
> axis(side=4, at=c(20,30,40,50,60,70,80,90,100), labels=c(1:9))
> lines(1996:2000, lsfit(c(1,9),c(20,100))$coef[1]+
> lsfit(c(1,9),c(20,100))$coef[2]*c(7,8,6, 2, 3), col=2)
> legend(1998.5, 90, legend=c("Resistence", "Use"), fill=c(1,2))
>
> However, I suspect there are better ways to do so, and I would like to know
> one because I have to do that many times.
>
> Thanks a lot in advance,
>
> Berta
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From skiadas at hanover.edu  Fri Mar  2 16:33:44 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Fri, 2 Mar 2007 10:33:44 -0500
Subject: [R] from function to its name?
In-Reply-To: <200703021542.46118.tamir@imp.univie.ac.at>
References: <200703021542.46118.tamir@imp.univie.ac.at>
Message-ID: <458DDB09-0F23-49E3-A691-DD2F8C7C04F3@hanover.edu>

On Mar 2, 2007, at 9:42 AM, Ido M. Tamir wrote:

> Hi,
>
> I can get from a string to a function with this name:
>
>> f1 <- function(x){ mean(x) }
>
>> do.call("f1",list{1:4})
>> get("f1")
> etc...
>
> But how do I get from a function to its name?
>
>> funcVec <- c(f1,median)
>
>> funcVec
> [[1]]
> function(x){ mean(x) }

I suppose you could do:

"funcVec"

but that's probably not what you want ;).

Can you do this with any object in R?
In what situation will you be wanting this name? I mean, how would  
you be given this object, but not know its name in advance? If it is  
passed as an argument in a function or something, then what would you  
consider to be its name?
I.e. I don't really see where you would reasonably want to do  
something like this, without there being another way around it.

Btw, perhaps this does what you want:

as.character(quote(f))

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From ggrothendieck at gmail.com  Fri Mar  2 16:35:47 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Mar 2007 10:35:47 -0500
Subject: [R] Error in length of vector ?
In-Reply-To: <4c817d530703020433y60fbad9ah70f2deb38d8aebe5@mail.gmail.com>
References: <4c817d530703020433y60fbad9ah70f2deb38d8aebe5@mail.gmail.com>
Message-ID: <971536df0703020735l110c7f84r427b3765bcde66f5@mail.gmail.com>

Suggest you review the help desk article on dates in
R News 4/1.   It mentions that POSIXlt objects are
lists with 9 components and other facts about date
objects in R.

On 3/2/07, S?rgio Nunes <snunes at gmail.com> wrote:
> Hi,
>
> I'm having a weird result with the length() function:
>
> >a
> [... omited ...]
> [9994] NA                    "2003-12-03 16:37:00" "2002-06-26 18:43:00"
> [9997] "2005-07-04 04:00:00" "2007-02-16 22:09:00" "2007-02-24 15:49:00"
> [10000] NA
>
> > length(LastModified)
> [1] 9
>
> > length(c(LastModified))
> [1] 9
>
> I was expecting to get "10000" as an answer.
> I'm trying to bind two vector, and I keep getting the error - "number
> of rows of result is not a multiple of vector length". Thus I tested
> length and got this value.
>
> Any hint?
>
> Thanks in advance,
> S?rgio Nunes
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From marc_schwartz at comcast.net  Fri Mar  2 16:37:50 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 02 Mar 2007 09:37:50 -0600
Subject: [R] barplot2, gap.barplot
In-Reply-To: <f8e6ff050703020653o1b2f9485v82c78eca40df0890@mail.gmail.com>
References: <1172757385.5102.42.camel@stryder.skrupellos.priv>
	<1172757514.4897.42.camel@localhost.localdomain>
	<f8e6ff050703020653o1b2f9485v82c78eca40df0890@mail.gmail.com>
Message-ID: <1172849870.4897.180.camel@localhost.localdomain>

On Fri, 2007-03-02 at 08:53 -0600, hadley wickham wrote:
> > 3. Depending on the nature of your data, if the extreme value is
> > representative of an important marked difference relative to the other
> > values, then I don't particularly find the 'look' of the plot to be
> > overly problematic. It does appropriately emphasize the large
> > difference.
> >
> > On the other hand, you might want to consider using a log scale on the y
> > axis as an alternative to an axis gap. This would be a reasonable
> > approach to plotting values that have a notable difference in range.  If
> > you do this, note that you would need to ensure that all y values are >0
> > (ie. y axis range minimum, lower bounds of CI's, etc.) since:
> >
> > > log10(0)
> > [1] -Inf
> >
> >
> 
> Of course, you can't do this with a bar plot, because bars should be
> anchored at 0.

Both barplot() and barplot2() support log scaling for both x and y axes.

In both functions, the default axis minimum for the 'height' axis (y by
default, x if 'horizontal = TRUE') will be 0.9 * min(height) to avert
log10(0) related issues. Errors will be issued otherwise if any values
of 'height' are <= 0 or 'ylim'/'xlim' args are similarly set.

Using a function that Martin had posted some time ago, to nicely format
the axis labels:

axTexpr <- function(side, 
                    at = axTicks(side, axp=axp, usr=usr, log=log), 
                    axp = NULL, usr = NULL, log = NULL) 
{ 
    ## Purpose: Do "a 10^k" labeling instead of "a e<k>" 
    ## this auxiliary should return 'at' and 'label' (expression) 
    ## ---------------------------------------------------------------------- 
    ## Arguments: as for axTicks() 
    ## ---------------------------------------------------------------------- 
    ## Author: Martin Maechler, Date: 7 May 2004, 18:01 
    eT <- floor(log10(abs(at)))# at == 0 case is dealt with below 
    mT <- at / 10^eT 
    ss <- lapply(seq(along = at), 
                 function(i) if(at[i] == 0) quote(0) else 
                 substitute(A %*% 10^E, list(A=mT[i], E=eT[i]))) 
    do.call("expression", ss) 
}



  x <- 10 ^ (0:10) 
  barplot(x, log = "y", yaxt = "n")
  axis(2, at = x, labels = axTexpr(2), las = 2)
  box()
  

HTH,

Marc


From roger.bos at us.rothschild.com  Fri Mar  2 16:40:27 2007
From: roger.bos at us.rothschild.com (Bos, Roger)
Date: Fri, 2 Mar 2007 10:40:27 -0500
Subject: [R] [friday topic]: what exactly is statistical computing
Message-ID: <D8C95B444AD6EE4AAD638D818A9CFD343A1E4C@RINNYCSE000.rth.ad.rothschild.com>

This means it comes with substantial statistical routines built-in.  You
could just as well use VBA or Java for your programming language, but
with those you would have to write pretty much any stat routine you
need.  With R, since it is a 'statistical computing' language, you know
that most of what you need has already been programmed, tested(?), and
is ready to use. 

I have seen you on this list for a while.  You already know all this.  I
am not sure why you are asking this question.

Roger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Wensui Liu
Sent: Friday, March 02, 2007 9:43 AM
To: r-help at stat.math.ethz.ch
Subject: [R] [friday topic]: what exactly is statistical computing

Dear List,
on www.r-project.org, the title says 'The R Project for Statistical
Computing'.

but what exactly is the definition of statistical computing?

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

********************************************************************** * 
This message is for the named person's use only. It may 
contain confidential, proprietary or legally privileged 
information. No right to confidential or privileged treatment 
of this message is waived or lost by any error in 
transmission. If you have received this message in error, 
please immediately notify the sender by e-mail, 
delete the message and all copies from your system and destroy 
any hard copies. You must not, directly or indirectly, use, 
disclose, distribute, print or copy any part of this message 
if you are not the intended recipient.


From milferst at uiuc.edu  Fri Mar  2 16:48:51 2007
From: milferst at uiuc.edu (Kim Milferstedt)
Date: Fri, 02 Mar 2007 09:48:51 -0600
Subject: [R] barplot with different color combination for each bar
Message-ID: <6.2.5.6.2.20070302093533.01f88e78@uiuc.edu>

Hi,

I'd like to construct a somewhat unusual barplot. In "barplot" I use 
beside=F as I'd like to have stacked bars. The height of each bar is 
always the same. Information in my plot is coded in the color of the 
bar. I therefore need to be able so assign a different combination 
(or order) of colors to each individual stacked bar.

In the example below, the combination of colors for my plot is 
generated by X, Q, color and cols. These colors are supposed to fill 
the stacked bars with the height of H. However, only the first column 
of cols is used for all columns of H as "barplot" only allows me to 
assign one vector for the color scheme of the entire barplot.

Does anybody know a way how I can assign each bar a potentially 
unique color combination?

Thanks for your help!

Kim

X <- seq(1:6)
Q    <- matrix(sample(X, 60, replace = T), nrow=6, byrow = T)
H   <-  matrix(rep(1,60), nrow=6, byrow=T)

color   <-  c("blue", "orange", "gold", "indianred", "skyblue4", "lightblue")
cols <-     ifelse(
                 (Q ==1) , color[1],
                     ifelse(
                         (Q ==2), color[2],
                             ifelse(
                                 (Q ==3) , color[3],
                                     ifelse(
                                         (Q ==4), color[4],
                                             ifelse(
                                                 (Q ==5) , color[5], color[6]
                                                     )
                                             )
                                     )
                             )
                     )

barplot(
         H,
         col=cols,
         width = c(0.1),
         xlim = c(0,3),
         beside=F
         )

__________________________________________

Kim Milferstedt
University of Illinois at Urbana-Champaign
Department of Civil and Environmental Engineering
4125 Newmark Civil Engineering Laboratory
205 North Mathews Avenue MC-250
Urbana, IL 61801
USA
phone: (001) 217 333-9663
fax: (001) 217 333-6968
email: milferst at uiuc.edu
http://cee.uiuc.edu/research/morgenroth


From ggrothendieck at gmail.com  Fri Mar  2 16:48:52 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Mar 2007 10:48:52 -0500
Subject: [R] extracting data from zoo series
In-Reply-To: <000501c75c6d$b35f33e0$0300a8c0@Vaio>
References: <000501c75c6d$b35f33e0$0300a8c0@Vaio>
Message-ID: <971536df0703020748j1584f992t83adb3fb4f43a7@mail.gmail.com>

Please read the last line of every message to r-help and follow it when
posting.  You ought to have provided small examples for A and B and
the result.

The following lists every row in z that is at a date in zs or is up to
4 rows later:

library(zoo)
z <- zoo(matrix(101:148, 24), 2001:2024)
zs <- z[c(1, 3, 10)]
z[unique(c(sapply(match(time(zs), time(z)), seq, length = 5)))]


On 3/1/07, Alfonso Sammassimo <cincinattikid at bigpond.com> wrote:
> Dear List,
>
> Sorry if I'm overlooking something simple here but I have gotten a bit
> tangled.
>
> I am trying to print the next five values(with their dates), which occur
> after a certain condition is met.
>
> I have a series of data in zoo format, call it "A". From this series I have
> created a subset (also in zoo format) based on a certain condition, call
> this series "B".
>
> I want to use the dates from "B"  as an index, so as each of these dates
> occur in "A", the succeeding five rows  from "A" after this date are printed
> out.
>


From j.van_den_hoff at fzd.de  Fri Mar  2 16:50:48 2007
From: j.van_den_hoff at fzd.de (Joerg van den Hoff)
Date: Fri, 2 Mar 2007 16:50:48 +0100
Subject: [R] from function to its name?
In-Reply-To: <200703021542.46118.tamir@imp.univie.ac.at>
References: <200703021542.46118.tamir@imp.univie.ac.at>
Message-ID: <20070302155047.GC4620@marco.fz-rossendorf.de>

On Fri, Mar 02, 2007 at 03:42:46PM +0100, Ido M. Tamir wrote:
> Hi,
> 
> I can get from a string to a function with this name:
> 
> >f1 <- function(x){ mean(x) }
> 
> >do.call("f1",list{1:4})
> >get("f1")
> etc...
> 
> But how do I get from a function to its name?
> 
> >funcVec <- c(f1,median)
> 
> >funcVec
> [[1]]
> function(x){ mean(x) }
> > str(funcVec)
> List of 2
>  $ :function (x)
>   ..- attr(*, "source")= chr "function(x){ mean(x) }"
>  $ :function (x, ...)
> > deparse(funcVec[1])
> [1] "list(function (x) " "{"                  "    mean(x)"
> [4] "})"
> 


for any symbol/name

deparse(substitute(f1))

yields the string representation, but this won't give you "f1" for

deparse(substitute(funcVec[1])). 

but rather the string "funcVec[1]".

if you actually want to access funcVec components via strings denoting the
components, maybe you simply could use

funcVec <- list(f1 = f1, median = median)

and access these via

funcVec[["f1"]]

which would enable using a string variable holding the name:

dum = "f1"

funcVec[[dum]]

hth
joerg


From h.wickham at gmail.com  Fri Mar  2 17:07:28 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 2 Mar 2007 10:07:28 -0600
Subject: [R] barplot2, gap.barplot
In-Reply-To: <1172849870.4897.180.camel@localhost.localdomain>
References: <1172757385.5102.42.camel@stryder.skrupellos.priv>
	<1172757514.4897.42.camel@localhost.localdomain>
	<f8e6ff050703020653o1b2f9485v82c78eca40df0890@mail.gmail.com>
	<1172849870.4897.180.camel@localhost.localdomain>
Message-ID: <f8e6ff050703020807q387d34e1gfa94167e78f25618@mail.gmail.com>

On 3/2/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> On Fri, 2007-03-02 at 08:53 -0600, hadley wickham wrote:
> > > 3. Depending on the nature of your data, if the extreme value is
> > > representative of an important marked difference relative to the other
> > > values, then I don't particularly find the 'look' of the plot to be
> > > overly problematic. It does appropriately emphasize the large
> > > difference.
> > >
> > > On the other hand, you might want to consider using a log scale on the y
> > > axis as an alternative to an axis gap. This would be a reasonable
> > > approach to plotting values that have a notable difference in range.  If
> > > you do this, note that you would need to ensure that all y values are >0
> > > (ie. y axis range minimum, lower bounds of CI's, etc.) since:
> > >
> > > > log10(0)
> > > [1] -Inf
> > >
> > >
> >
> > Of course, you can't do this with a bar plot, because bars should be
> > anchored at 0.
>
> Both barplot() and barplot2() support log scaling for both x and y axes.
>
> In both functions, the default axis minimum for the 'height' axis (y by
> default, x if 'horizontal = TRUE') will be 0.9 * min(height) to avert
> log10(0) related issues. Errors will be issued otherwise if any values
> of 'height' are <= 0 or 'ylim'/'xlim' args are similarly set.

I think that's a pretty bad idea - in a bar plot you are comparing the
ratio of heights of the bars, not the absolute heights.  It's the same
reason it's a bad idea to have a bar graph with a non-0 y-axis - it's
misleading.

Hadley


From tamir at imp.univie.ac.at  Fri Mar  2 17:06:57 2007
From: tamir at imp.univie.ac.at (Ido M. Tamir)
Date: Fri, 2 Mar 2007 17:06:57 +0100
Subject: [R] from function to its name?
Message-ID: <200703021706.57491.tamir@imp.univie.ac.at>

Charilaos Skiadas wrote:

> On Mar 2, 2007, at 9:42 AM, Ido M. Tamir wrote:
>>
>> But how do I get from a function to its name?
> 
> Can you do this with any object in R?
> In what situation will you be wanting this name? I mean, how would
> you be given this object, but not know its name in advance? If it is
> passed as an argument in a function or something, then what would you
> consider to be its name?
> I.e. I don't really see where you would reasonably want to do
> something like this, without there being another way around it.
> 

I wanted to pass a vector of functions as an argument to a function to do some 
calculations and put the results in a list where each list entry has 
the "name" of the function.
I thought I could either pass a vector of function names as character, then
retrieve the functions etc...
Or do the opposite, pass the functions and then retrieve the names, but
this seems not to be possible it occurred to me, hence my question.


thanks
ido


From nlobo at uchicago.edu  Fri Mar  2 17:09:12 2007
From: nlobo at uchicago.edu (Nameeta Lobo)
Date: Fri,  2 Mar 2007 10:09:12 -0600 (CST)
Subject: [R] RE-Row-wise two sample T-test on subsets of a matrix
Message-ID: <20070302100912.AKT95925@m4500-02.uchicago.edu>

hello all

thanks a lot for the info, I just actually needed to remove
that comma in my command line

what i had typed in was
t.test(temp.matrix[,1:11],temp.matrix[,12:22],paired=TRUE)
what i needed to do was 
t.test(temp.matrix[1:11],temp.matrix[12:22],paired=TRUE)

thanks Petr, saw your comment later.

nameeta


From Cameron.Guenther at MyFWC.com  Fri Mar  2 17:38:59 2007
From: Cameron.Guenther at MyFWC.com (Guenther, Cameron)
Date: Fri, 2 Mar 2007 11:38:59 -0500
Subject: [R] Matrix looping
Message-ID: <BA6FF017E924044A9BF748AFAEEA6F30015E9B14@FWC-TLEX3.fwc.state.fl.us>

Hi all, 
I am having a problem getting my fucntion to work correctly.

Here is my problem.

I have three ages: Nage<-c(1,2,3)
I have an weight matrix: Wt<-c( 0.04952867, 0.23808432, 0.34263880)
I have an age schedule of maturity: Mat<-c(0,1,1) where 0 is not mature,
and 1 is mature
I have a vulnerability schedule: Vul<-c(0,1,1)
I have an survivorship schedule: Survship<-c(1,0.4,0.16)
I also have leading parameters R0<-130.66; recK<-3.068; a<-5.48;
b<-0.0282; S<-0.4
I have annual catches for 100 years, ct<-runif(100,5,20)

Now I want a matrix of 100 years x 3 ages
yr<-c(1:100)
Nt<-matrix(0,nrow=length(yr)+1),ncol=length(Nage))

Now the first row of my matrix needs to be the product of R0 and
Survship, no problem Nt[1,]<-Ro*Survship
I also need to create a new vector of egg production so
Eggs<-vector();Eggs[1]<-sum(Mat*Nt[1,])
I also calculate the vulnerable biomass for each year so
VulBio<-vector();VulBio[1]<-sum(Wt*Vul*Nt[1,])
Now I calculate the exploitation Ut<-min(0.99,ct[1]/VulBio[1])

For (i in 1:length(Nt[,1])){
Now I need to calculate the first column of values where;
Nt[i+1,1]<-a*Eggs[i]/(1+b*Eggs[i])
Eggs[i+1]<-sum(Mat*Nt[i+1,]
VulBio[i+1]<-sum(Wt*Vul*Nt[i+1,])
Ut[i+1]<-min(0.99,ct[i+1]/VulBio[i+1])
Now here is the rub, I need to calculate the rest of the matrix based on
the previous years values for the previous age, but I don't want to
overwrite the first column or first row.
Nt[i+1,i+1]<-Nt[i,i]*S*(1-Ut[i]*Vul[i] gives the diagonal.  How do I
fill the matrix without overwriting colum and row 1?  



Cameron Guenther, Ph.D.
100 8th Ave. SE
St. Petersburg, Fl 33701
727-896-8626 ext. 4305
cameron.guenther at myfwc.com 

From nlobo at uchicago.edu  Fri Mar  2 17:45:28 2007
From: nlobo at uchicago.edu (Nameeta Lobo)
Date: Fri,  2 Mar 2007 10:45:28 -0600 (CST)
Subject: [R] RE-Row-wise two sample T-test on subsets of a matrix
Message-ID: <20070302104528.AKT99597@m4500-02.uchicago.edu>

whoops. forgot to mention the apply command.

so this is what I exactly typed in
fun5<-function(m){as.numeric(t.test(m[1:11],m[12:22])[[1]])}
abc<-apply(temp.matrix,1,fun5)


temp.matrix or course being my 196002*22 matrix

and then I got the t-test rowwise for my two groups.


It's just that previously I had the extra commas in my
function. Post doc in lab helped me. Syntax always worries me.

nameeta



---- Original message ----
>Date: Fri, 2 Mar 2007 11:28:26 -0500
>From: "Leeds, Mark (IED)" <Mark.Leeds at morganstanley.com>  
>Subject: RE: [R] RE-Row-wise two sample T-test on subsets of
a matrix  
>To: "Nameeta Lobo" <nlobo at uchicago.edu>
>
>wow, so it didn't even need a call to apply. t.test must be
vectorized.
>interesting.
>
> 
>
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
Nameeta Lobo
>Sent: Friday, March 02, 2007 11:09 AM
>To: r-help at stat.math.ethz.ch
>Subject: [R] RE-Row-wise two sample T-test on subsets of a matrix
>
>hello all
>
>thanks a lot for the info, I just actually needed to remove
that comma
>in my command line
>
>what i had typed in was
>t.test(temp.matrix[,1:11],temp.matrix[,12:22],paired=TRUE)
>what i needed to do was
>t.test(temp.matrix[1:11],temp.matrix[12:22],paired=TRUE)
>
>thanks Petr, saw your comment later.
>
>nameeta
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible
code.
>--------------------------------------------------------
>
>This is not an offer (or solicitation of an offer) to
buy/sell the securities/instruments mentioned or an official
confirmation.  Morgan Stanley may deal as principal in or own
or act as market maker for securities/instruments mentioned or
may advise the issuers.  This is not research and is not from
MS Research but it may refer to a research analyst/research
report.  Unless indicated, these views are the author's and
may differ from those of Morgan Stanley research or others in
the Firm.  We do not represent this is accurate or complete
and we may not update this.  Past performance is not
indicative of future returns.  For additional information,
research reports and important disclosures, contact me or see
https://secure.ms.com/servlet/cls.  You should not use e-mail
to request, authorize or effect the purchase or sale of any
security or instrument, to send transfer instructions, or to
effect any other transactions.  We cannot guarantee that any
such requests received via e-mail will be processed in a
timely manner.  This communication is solely for the
addressee(s) and may contain confidential information.  We do
not waive confidentiality by mistransmission.  Contact me if
you do not wish to receive these communications.  In the UK,
this communication is directed in the UK to those persons who
are market counterparties or intermediate customers (as
defined in the UK Financial Services Authority's rules).


From deepayan.sarkar at gmail.com  Fri Mar  2 17:45:37 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 2 Mar 2007 08:45:37 -0800
Subject: [R] lattice: clipping data, not plot margins
In-Reply-To: <000801c75cd7$d208bae0$d22401a3@plants.ox.ac.uk>
References: <000801c75cd7$d208bae0$d22401a3@plants.ox.ac.uk>
Message-ID: <eb555e660703020845pea92baeg2d5b5a764df121e7@mail.gmail.com>

On 3/2/07, Dan Bebber <danbebber at forestecology.co.uk> wrote:
> I am plotting subsets of my data, using ylim.
> This works fine, but the outer margin line widths of the plot are thin, due
> to clipping.
> If I include
> > trellis.par.set(clip=list(panel = "off"))
> then the outer margin line widths are fine, but the outlying data is
> visible.
>
> Is there any way of achieving both correct margin line widths and clipping
> of outlying data?

I believe this behaviour has already been changed in the svn sources
of lattice (i.e. the boundaries are not clipped). There hasn't been a
release since then, but I'll try to make one within a week or two.

Deepayan


From heberto.ghezzo at mcgill.ca  Fri Mar  2 18:02:55 2007
From: heberto.ghezzo at mcgill.ca (R Heberto Ghezzo, Dr)
Date: Fri, 2 Mar 2007 12:02:55 -0500
Subject: [R] random uniform sample of points on an ellipsoid (e.g. WG
References: <XFMail.070228233314.efh@nessie.mcc.ac.uk>
Message-ID: <05BE78B0CF1BBC4BBA4AA255568D8611029A99AE@EXCHANGE2VS1.campus.mcgill.ca>

OK, I stand corrected, my previous post generated a point on an ellipsoid as a proyection of an uniform random  point in a sphere.
now, to generate a random point in a sphere I generate an uniform on one axis and an uniform angle in the second, since the sphere has the property that the perifery of all slices of equal thickness parallel to an equator have equal surface.
So, if I devide an oblate ellipsoid, like a geoid, in slices parallel to the equator, I can easily compute the surface area of the slices  
  lower <- 0
  upper <- pi/2
  a <- 1
  b <- 1.001
  e <- sqrt(1-a^2/b^2)
#
#   area revolution
#
  fun3 <- function(t,az,bz){
    2 * pi * bz * sin(t) * sqrt(az^2 * sin(t)^2 + bz^2 * cos(t)^2)
  }
  are3 <- integrate(fun3,lower,upper,az=a, bz=b)
  are3
  s <- 2 * pi *(a*a + b*b*atanh(e)/e)
  print(s/2)
so the integration works ok on the semi-ellipsoid, Now I devide it in 100 slices
#
  v <- rep(0,100)
  for(i in 1:100){
    l <- (i-1)*pi/200
    u <- i*pi/200
    v[i] <- integrate(fun3,lower=l, upper=u, az=a, bz = b)$value
  }
  print(sum(v))
#[1] 6.291565
  s/2
#[1] 6.29157
  w <- cumsum(v)
  w <- w/w[100]
#
now I choose a slice at random proportional to its area and a point in the latitude axis
the longitude is random 0-2pi.
#
 x <- runif(1)
 xx <- max(which(w < x))
 lat <- (xx+runif(1)) * pi / 200
 lon <- 2 * runif(1) * pi
#
 cat(lat*180/pi,lon*180/pi)
#
This point should be 'approximately' at random on the surface of the ellipsoid (revolution oblate). The error in the distribution of the point versus perfectly random is 'only' in the determination of latitude where I interpolate within a slice with a, straight line ' xx + runif(1) ' instead of following the curve. Taking 1000 slices instead of 100 obviously reduces this error to almost nothing, but not zero.
Am I correct this time?
Heberto Ghezzo
McGill University
Montreal - Canada


From sfalcon at fhcrc.org  Fri Mar  2 18:18:13 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Fri, 02 Mar 2007 09:18:13 -0800
Subject: [R] from function to its name?
In-Reply-To: <200703021706.57491.tamir@imp.univie.ac.at> (Ido M. Tamir's
	message of "Fri, 2 Mar 2007 17:06:57 +0100")
References: <200703021706.57491.tamir@imp.univie.ac.at>
Message-ID: <m24pp3a7je.fsf@ziti.local>

"Ido M. Tamir" <tamir at imp.univie.ac.at> writes:
> I wanted to pass a vector of functions as an argument to a function to do some 
> calculations and put the results in a list where each list entry has 
> the "name" of the function.
> I thought I could either pass a vector of function names as character, then
> retrieve the functions etc...
> Or do the opposite, pass the functions and then retrieve the names, but
> this seems not to be possible it occurred to me, hence my question.

Functions don't have to have names, by which I mean that the
definition doesn't have to be bound to a symbol.  If your function
takes a list of functions then:

  yourFunc(theFuncs=list(function(x) x + 1))

You could force the list to have names and use them.  Or you could
force function names to be passed in (your other idea).

+ seth


From paolo.radaelli at unimib.it  Fri Mar  2 18:26:23 2007
From: paolo.radaelli at unimib.it (Paolo Radaelli)
Date: Fri, 2 Mar 2007 18:26:23 +0100
Subject: [R] Problem with user defined split function in Rpart
Message-ID: <014e01c75cef$e6a969f0$90788495@radaelli>

Dear all,
I'm trying to manage with user defined split function in rpart. I (hope) 
correctly defined three functions (eval, split, init) I need.
I tested these function by executing them step by step and they work 
correctly.
When I try to build the tree by the rpart command (by specifying method= the 
list of the my three functions) I get no errors but the tree I obtain 
contains only the root !
I tried in different ways to understand the reason why the tree does not 
grow up but I had not success ...
The only strange thing I observed is that the CP parameter in summary.rpart 
is negative

         CP nsplit rel error
1 -1.011038      0         1

Where does the problem lie ?
Thank you for any helpful suggestion.

Paolo Radaelli

Paolo Radaelli
Dipartimento di Metodi Quantitativi per le Scienze Economiche ed Aziendali
Facolt? di Economia
Universit? degli Studi di Milano-Bicocca
P.zza dell'Ateneo Nuovo, 1
20126 Milano
Italy
e-mail paolo.radaelli a unimib.it


From gunter.berton at gene.com  Fri Mar  2 18:43:24 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Fri, 2 Mar 2007 09:43:24 -0800
Subject: [R] from function to its name?
In-Reply-To: <m24pp3a7je.fsf@ziti.local>
Message-ID: <004d01c75cf2$47066170$4d908980@gne.windows.gene.com>

 Seth is, of course, correct, but perhaps the following may help:

## function that takes a function as an argument

> foo <- function(f,x)list(deparse(substitute(f)),f(x))

## Value is a list of length 2; first component is a character string giving
the "name" of the funtion; second component is the result of applying the
function to the "x" argument.

##pass in the name (UNquoted) of the function as the first argument
## This works because the evaluator looks up the function that the symbol is
bound to in the usual way

> foo(mean, 1:5)
[[1]]
[1] "mean"

[[2]]
[1] 3

## pass in an unnamed function as the first argument
> foo(function(y)sum(y)/length(y), 1:5)
[[1]]
[1] "function(y) sum(y)/length(y)"

[[2]]
[1] 3

## the following gives an error since the first argument is a character
string, not a name/symbol:

> foo(f="mean", 1:5)
Error in foo(f = "mean", 1:5) : could not find function "f"


Cheers,

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Seth Falcon
Sent: Friday, March 02, 2007 9:18 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] from function to its name?

"Ido M. Tamir" <tamir at imp.univie.ac.at> writes:
> I wanted to pass a vector of functions as an argument to a function to do
some 
> calculations and put the results in a list where each list entry has 
> the "name" of the function.
> I thought I could either pass a vector of function names as character,
then
> retrieve the functions etc...
> Or do the opposite, pass the functions and then retrieve the names, but
> this seems not to be possible it occurred to me, hence my question.

Functions don't have to have names, by which I mean that the
definition doesn't have to be bound to a symbol.  If your function
takes a list of functions then:

  yourFunc(theFuncs=list(function(x) x + 1))

You could force the list to have names and use them.  Or you could
force function names to be passed in (your other idea).

+ seth

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From finbref.2006 at gmail.com  Fri Mar  2 18:48:32 2007
From: finbref.2006 at gmail.com (Thomas Steiner)
Date: Fri, 2 Mar 2007 18:48:32 +0100
Subject: [R] plot with fixed axis proportion
Message-ID: <d0f55a670703020948w327859c5t6bedc211a05923c1@mail.gmail.com>

I want to plot something (eg a circle) with a fixed ratio of the x and
y axis, or (even better) with a fixed size when I print it. Output
should then be a circle (actually it'll be someting more complicated)
with radius 5cm and not an ellipse.

I'm _sure_ this is not new, but after looking 45min for a solution, I
post here...

Thanks for help
Thomas


From seniorr at aracnet.com  Fri Mar  2 18:59:09 2007
From: seniorr at aracnet.com (Russell Senior)
Date: 02 Mar 2007 09:59:09 -0800
Subject: [R] random uniform sample of points on an ellipsoid (e.g. WG
In-Reply-To: <200702252348.15167.albmont@centroin.com.br>
References: <XFMail.070225220953.ted.harding@nessie.mcc.ac.uk>
	<200702252348.15167.albmont@centroin.com.br>
Message-ID: <86649jzfv6.fsf@coulee.tdb.com>

>>>>> "Alberto" == Alberto Vieira Ferreira Monteiro <albmont at centroin.com.br> writes:

Alberto> I guess this sample is required for some practical
Alberto> application, say a simulation for something done over the
Alberto> Earth. Then, I also guess that the sample does not have to be
Alberto> _absolutely_ exact, but a reasonable approximation can do
Alberto> it. And the ellipsoid is a rotation ellipsoid.

Alberto> This is my suggestion:

Alberto> (1) Divide the Ellipsoid by latitudes in _n_ horizontal
Alberto> slices in such a way that each slice can be considered
Alberto> "almost" spherical. Of course here lies the problem:
Alberto> depending on the purpose of the simulation, _n_ would be so
Alberto> big as to make it impractical

Alberto> (2) Compute the area of each slice (there are formulas for
Alberto> that, whose error is not very big - again, we rely on the
Alberto> purpose of the simulation)

Alberto> (3) Chose a random slice based on weight = area

Alberto> (4) Chose the random latitude by a uniform from the minimum
Alberto> to the maximum latitude (a much better approximation would
Alberto> give higher weight to the latitude closer to the equator)

Alberto> (5) lon = 2 pi runif(1) # :-)

Alberto> Now the question is: do you know the formulas to compute the
Alberto> area in (2)?  I know these formulas exist, I learned them in
Alberto> the last century, but I can't remember them and I don't know
Alberto> how to find them.

This is essentially the approach I (and my local helpers) have taken.
With garden-variety calculus, we have derived a function t(z) that
maps equal area over the oblate ellipsoid.  We select t from a uniform
distribution and then find the corresponding z dimension.  The
expression is quite hairy, but apparently analytically correct (I
haven't found any errors yet), if possibly unstable numerically.

The derivative with respect to z of area on the ellipsoid at a plane
of latitude intersecting at a height z above the equator, where the
ellipsoid has been scaled such that a is the polar radius and the
equatorial radius is 1, is: 

  dA/dz = 2 * pi * f

where

  f = sqrt((z^2 * (1 - a^2))/a^4 + 1)

You find the function t(z) such that dA/dt is constant.

Then you select from a uniform distribution and then find the value of
z that corresponds.


-- 
Russell Senior         ``I have nine fingers; you have ten.''
seniorr at aracnet.com


From ggrothendieck at gmail.com  Fri Mar  2 19:03:02 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Mar 2007 13:03:02 -0500
Subject: [R] from function to its name?
In-Reply-To: <200703021706.57491.tamir@imp.univie.ac.at>
References: <200703021706.57491.tamir@imp.univie.ac.at>
Message-ID: <971536df0703021003t3c35359av3a8ced9d40da7f3b@mail.gmail.com>

Check out:

https://stat.ethz.ch/pipermail/r-help/2006-October/114431.html

On 3/2/07, Ido M. Tamir <tamir at imp.univie.ac.at> wrote:
> Charilaos Skiadas wrote:
>
> > On Mar 2, 2007, at 9:42 AM, Ido M. Tamir wrote:
> >>
> >> But how do I get from a function to its name?
> >
> > Can you do this with any object in R?
> > In what situation will you be wanting this name? I mean, how would
> > you be given this object, but not know its name in advance? If it is
> > passed as an argument in a function or something, then what would you
> > consider to be its name?
> > I.e. I don't really see where you would reasonably want to do
> > something like this, without there being another way around it.
> >
>
> I wanted to pass a vector of functions as an argument to a function to do some
> calculations and put the results in a list where each list entry has
> the "name" of the function.
> I thought I could either pass a vector of function names as character, then
> retrieve the functions etc...
> Or do the opposite, pass the functions and then retrieve the names, but
> this seems not to be possible it occurred to me, hence my question.
>
>
> thanks
> ido
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From albmont at centroin.com.br  Fri Mar  2 19:19:50 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 2 Mar 2007 16:19:50 -0200
Subject: [R] random uniform sample of points on an ellipsoid (e.g. WG
In-Reply-To: <86649jzfv6.fsf@coulee.tdb.com>
References: <XFMail.070225220953.ted.harding@nessie.mcc.ac.uk>
	<200702252348.15167.albmont@centroin.com.br>
	<86649jzfv6.fsf@coulee.tdb.com>
Message-ID: <20070302180649.M37412@centroin.com.br>

Russell Senior wrote:
> 
> This is essentially the approach I (and my local helpers) have taken.
> With garden-variety calculus, we have derived a function t(z) that
> maps equal area over the oblate ellipsoid.  We select t from a 
> uniform distribution and then find the corresponding z dimension.  
> The expression is quite hairy, but apparently analytically correct 
> (I haven't found any errors yet), if possibly unstable numerically.
> 
Hmmm... I have just thought about something. The ellipsoid is
"almost" a sphere, so every formula for the sphere should "map"
into a formula for the ellipsoid that can be expressed as a power
series in the eccentricity or the oblateness. Since these parameters
are small, most of these series will converge very fast, and it
might be possible to do most transformations with them.

So, you could have a fast and accurate series that would generate
the random latitudes.

> The derivative with respect to z of area on the ellipsoid at a plane
> of latitude intersecting at a height z above the equator, where the
> ellipsoid has been scaled such that a is the polar radius and the
> equatorial radius is 1, is:
> 
>   dA/dz = 2 * pi * f
> 
> where
> 
>   f = sqrt((z^2 * (1 - a^2))/a^4 + 1)
> 
> You find the function t(z) such that dA/dt is constant.
> 
> Then you select from a uniform distribution and then find the value 
> of z that corresponds.
> 
Yikes! I can't read a formula in ascii notation :-P

Let's parametrize the ellipsoid by the corresponding sphere,
using coordinates lats (latitude of the corresponding sphere)
and lon. So:

x = a cos(lats) cos(lon)
y = a cos(lats) sin(lon)
z = b sin(lats)

But then the surface are between lats and lats + dl will require
a little bit of calculus...

> Russell Senior         ``I have nine fingers; you have ten.''
>
So do Frodo, Sauron and brazilian president Lula :-)

Alberto Monteiro


From liuwensui at gmail.com  Fri Mar  2 19:20:33 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Fri, 2 Mar 2007 13:20:33 -0500
Subject: [R] [friday topic]: what exactly is statistical computing
In-Reply-To: <D8C95B444AD6EE4AAD638D818A9CFD343A1E4C@RINNYCSE000.rth.ad.rothschild.com>
References: <D8C95B444AD6EE4AAD638D818A9CFD343A1E4C@RINNYCSE000.rth.ad.rothschild.com>
Message-ID: <1115a2b00703021020nc53c830w3c9682123d2124a2@mail.gmail.com>

Thanks for your insight, Roger.

Actually, my question is not related to R only.

statistical computing is a popular topic recently. However, when I
check its meaning on wikipedia/google, I couldn't find it.

another reason why I asked is related to myself. I am very interested
in this area and maintaining a blog in this topic. however, when asked
what 'statistical computing' is, I am not able to give a
well-verbalized answer.


On 3/2/07, Bos, Roger <roger.bos at us.rothschild.com> wrote:
> This means it comes with substantial statistical routines built-in.  You
> could just as well use VBA or Java for your programming language, but
> with those you would have to write pretty much any stat routine you
> need.  With R, since it is a 'statistical computing' language, you know
> that most of what you need has already been programmed, tested(?), and
> is ready to use.
>
> I have seen you on this list for a while.  You already know all this.  I
> am not sure why you are asking this question.
>
> Roger
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Wensui Liu
> Sent: Friday, March 02, 2007 9:43 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] [friday topic]: what exactly is statistical computing
>
> Dear List,
> on www.r-project.org, the title says 'The R Project for Statistical
> Computing'.
>
> but what exactly is the definition of statistical computing?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ********************************************************************** *
> This message is for the named person's use only. It may
> contain confidential, proprietary or legally privileged
> information. No right to confidential or privileged treatment
> of this message is waived or lost by any error in
> transmission. If you have received this message in error,
> please immediately notify the sender by e-mail,
> delete the message and all copies from your system and destroy
> any hard copies. You must not, directly or indirectly, use,
> disclose, distribute, print or copy any part of this message
> if you are not the intended recipient.
> **********************************************************************
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From ggrothendieck at gmail.com  Fri Mar  2 19:21:37 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Mar 2007 13:21:37 -0500
Subject: [R] plot with fixed axis proportion
In-Reply-To: <d0f55a670703020948w327859c5t6bedc211a05923c1@mail.gmail.com>
References: <d0f55a670703020948w327859c5t6bedc211a05923c1@mail.gmail.com>
Message-ID: <971536df0703021021h12e7329eo63247414a27b9800@mail.gmail.com>

See the aspect argument, asp, in ?plot.default .  Also eqscplot in MASS.

On 3/2/07, Thomas Steiner <finbref.2006 at gmail.com> wrote:
> I want to plot something (eg a circle) with a fixed ratio of the x and
> y axis, or (even better) with a fixed size when I print it. Output
> should then be a circle (actually it'll be someting more complicated)
> with radius 5cm and not an ellipse.
>
> I'm _sure_ this is not new, but after looking 45min for a solution, I
> post here...
>
> Thanks for help
> Thomas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mervyn at iastate.edu  Fri Mar  2 19:32:19 2007
From: mervyn at iastate.edu (Mervyn G Marasinghe)
Date: Fri, 02 Mar 2007 12:32:19 -0600
Subject: [R] lme problem : extra parameters
Message-ID: <200703021832.l22IWKRI014754@mailhub-4.iastate.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070302/3c9d940b/attachment.pl 

From marc_schwartz at comcast.net  Fri Mar  2 19:32:57 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 02 Mar 2007 12:32:57 -0600
Subject: [R] barplot2, gap.barplot
In-Reply-To: <f8e6ff050703020807q387d34e1gfa94167e78f25618@mail.gmail.com>
References: <1172757385.5102.42.camel@stryder.skrupellos.priv>
	<1172757514.4897.42.camel@localhost.localdomain>
	<f8e6ff050703020653o1b2f9485v82c78eca40df0890@mail.gmail.com>
	<1172849870.4897.180.camel@localhost.localdomain>
	<f8e6ff050703020807q387d34e1gfa94167e78f25618@mail.gmail.com>
Message-ID: <1172860377.4897.219.camel@localhost.localdomain>

On Fri, 2007-03-02 at 10:07 -0600, hadley wickham wrote:
> On 3/2/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> > On Fri, 2007-03-02 at 08:53 -0600, hadley wickham wrote:
> > > > 3. Depending on the nature of your data, if the extreme value is
> > > > representative of an important marked difference relative to the other
> > > > values, then I don't particularly find the 'look' of the plot to be
> > > > overly problematic. It does appropriately emphasize the large
> > > > difference.
> > > >
> > > > On the other hand, you might want to consider using a log scale on the y
> > > > axis as an alternative to an axis gap. This would be a reasonable
> > > > approach to plotting values that have a notable difference in range.  If
> > > > you do this, note that you would need to ensure that all y values are >0
> > > > (ie. y axis range minimum, lower bounds of CI's, etc.) since:
> > > >
> > > > > log10(0)
> > > > [1] -Inf
> > > >
> > > >
> > >
> > > Of course, you can't do this with a bar plot, because bars should be
> > > anchored at 0.
> >
> > Both barplot() and barplot2() support log scaling for both x and y axes.
> >
> > In both functions, the default axis minimum for the 'height' axis (y by
> > default, x if 'horizontal = TRUE') will be 0.9 * min(height) to avert
> > log10(0) related issues. Errors will be issued otherwise if any values
> > of 'height' are <= 0 or 'ylim'/'xlim' args are similarly set.
> 
> I think that's a pretty bad idea - in a bar plot you are comparing the
> ratio of heights of the bars, not the absolute heights.  It's the same
> reason it's a bad idea to have a bar graph with a non-0 y-axis - it's
> misleading.

Hadley,

I might note that even lattice will do this, arguably easier than
barplot[2]():

library(lattice)
x <- 10 ^ (0:10)
barchart(x ~ 0:10, horizontal = FALSE, 
         scales = list(y = list(log = 10)))


Is it the right thing to do?  I'll leave that for others to debate. I
have stronger feelings on the 'gapped axis' issue.

I don't tend to use bar plots too much myself any longer and it all
depends upon the audience.

There have been requests for log scales on barplots on the R lists going
back several years, which is one of the reasons that I wrote barplot2()
some years ago.  It was also one of my first exercises in gaining a
lower level understanding of R's graphics models.

The 'log' and 'add' arguments and related code from barplot2() were then
included in R's barplot() in version 2.2.0, I believe by Paul.

HTH,

Marc


From mherzog at prbo.org  Fri Mar  2 19:34:19 2007
From: mherzog at prbo.org (=?UTF-8?B?TWFyayBIZXJ6b2c=?=)
Date: Fri, 2 Mar 2007 18:34:19 +0000
Subject: [R] Reformulated  matrices dimensions limitation problem
In-Reply-To: <45E8411E.30416.1A15F2B@localhost>
References: <JEA3B6$D049EC29F5E065770DE226C0B47F6466@libero.it>
	<45E8411E.30416.1A15F2B@localhost>
Message-ID: <1221313244-1172860468-cardhu_blackberry.rim.net-1509046322-@bwe014-cell00.bisx.prod.on.blackberry>

I don't have anything in front of me but just to make sure everyone knows thee are 2 rfp's out there right now from sea grant. 1 is a speceduc climate change and the other the normal RFP. I think the proopal would fit in either

Perhaps that is the discrepency in amounts?
-----------------------------------------------------------
Mark Herzog PhD
Program Director
San Francisco Bay Research, Wetlands Division
PRBO Conservation Science
(415) 893-7677 x308 (office)
mherzog at prbo.org  

-----Original Message-----
From: "Petr Pikal" <petr.pikal at precheza.cz>
Date: Fri, 02 Mar 2007 15:22:06 
To:"Bruno C." <bruno.c at inwind.it>, r-help <r-help at r-project.org>
Subject: Re: [R] Reformulated  matrices dimensions limitation problem

I believe that due to memory management the size of  biggest possible 
matrix can change. But I probably am not the persou who can explain 
it correctly.

Petr


On 2 Mar 2007 at 14:40, Bruno C. wrote:

Date sent:      	Fri, 2 Mar 2007 14:40:18 +0100
From:           	"Bruno C." <bruno.c at inwind.it>
To:             	"petr.pikal" <petr.pikal at precheza.cz>
Copies to:      	r-help <r-help at r-project.org>
Subject:        	Re: [R] Reformulated  matrices dimensions limitation problem

> bigml is for generalized regression model. I need to use lars ans svm.
> Anway I am perfectly able to train my model: my training matrix is not
> so big. The big matrix is the test one then I can split it by rows 
> into several matrices without affecting the results. The point is that
> I wanted to  split it in an elegant way, mainimizing the needed
> submatrices wrt memory. But I have the impression I am asking R too
> much :D
> 
> 
> > Did you consider biglm  package. I did not use it myself but from
> > its description it can be used for linear models on objects that do
> > not fit into memory.
> > 
> > HTH
> > Petr
> > 
> > 
> > On 2 Mar 2007 at 13:12, Bruno C. wrote:
> > 
> > Date sent:      	Fri, 2 Mar 2007 13:12:07 +0100
> > From:           	"Bruno C." <bruno.c at inwind.it>
> > To:             	"petr.pikal" <petr.pikal at precheza.cz>
> > Copies to:      	r-help <r-help at r-project.org>
> > Subject:        	Re: [R] Reformulated  matrices dimensions
> > limitation problem
> > 
> > > You are right. and I am aware of that.
> > > 
> > > This is what I need to do:
> > > load a regression model
> > > load the bigget posible matrix
> > > do prediction on this matrix
> > > 
> > > the first and 3rd step will not need to much memory... 
> > > 
> > > and I would really appreciate this degree of introspection from R:
> > > it would be great if there would be a package that could simulate
> > > the amount of memory needed from a process But I don't pretend
> > > that much, this is why I am asking only about  a function able to
> > > build a matrix matrix, with size based on memory available...
> > > 
> > > 
> > > 
> > > 
> > > > Hi
> > > > 
> > > > creating a biggest possible matrix does not automaticaly mean
> > > > you can do some computation with it. 
> > > > 
> > > > So the size will depend partly on what you want to do with it. I
> > > > presume that you do not want only to create a matrix just for
> > > > pleasure to be able to.
> > > > 
> > > > Cheers
> > > > Petr
> > > >  
> > > > 
> > > > On 2 Mar 2007 at 10:15, Bruno C. wrote:
> > > > 
> > > > Date sent:      	Fri, 2 Mar 2007 10:15:33 +0100
> > > > From:           	"Bruno C." <bruno.c at inwind.it>
> > > > To:             	"r-help" <r-help at r-project.org>
> > > > Subject:        	[R] Reformulated  matrices dimensions
> > > > limitation problem
> > > > 
> > > > > First I wanted to thank both Marc Schwartz Greg Snow and for
> > > > > their reply.
> > > > > 
> > > > > Then I needed to add a level of complexity to the problem. I
> > > > > would be able to create the biggest possible matrix.
> > > > > 
> > > > > In other way does it exist a method to ask smthing like the
> > > > > following :
> > > > > 
> > > > > max number of rows for a matrix if column=x?
> > > > > 
> > > > > Thank you
> > > > > 
> > > > > 
> > > > > 
> > > > > 
> > > > > 
> > > > > ------------------------------------------------------
> > > > > Passa a Infostrada. ADSL e Telefono senza limiti e senza
> > > > > canone Telecom http://click.libero.it/infostrada2marz07
> > > > > 
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide
> > > > > http://www.R-project.org/posting-guide.html and provide
> > > > > commented, minimal, self-contained, reproducible code.
> > > > 
> > > > Petr Pikal
> > > > petr.pikal at precheza.cz
> > > > 
> > > > 
> > > 
> > > 
> > > ------------------------------------------------------
> > > Passa a Infostrada. ADSL e Telefono senza limiti e senza canone
> > > Telecom http://click.libero.it/infostrada2marz07
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html and provide commented,
> > > minimal, self-contained, reproducible code.
> > 
> > Petr Pikal
> > petr.pikal at precheza.cz
> > 
> > 
> 
> 
> ------------------------------------------------------
> Passa a Infostrada. ADSL e Telefono senza limiti e senza canone
> Telecom http://click.libero.it/infostrada2marz07
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Greg.Snow at intermountainmail.org  Fri Mar  2 19:37:12 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 2 Mar 2007 11:37:12 -0700
Subject: [R] plot with fixed axis proportion
In-Reply-To: <d0f55a670703020948w327859c5t6bedc211a05923c1@mail.gmail.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB87985D@LP-EXCHVS07.CO.IHC.COM>

Your question can be interpreted a couple of ways, Gabor gave you the
answer to one of those interpretations.  Another interpretation of your
question makes it the same as one that was asked earlier in the week
with the subject: "PLotting R graphics/symbols without user x-y
scaling", looking at that question and the replies to it should help if
that is the correct interpretation of your question.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas Steiner
> Sent: Friday, March 02, 2007 10:49 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] plot with fixed axis proportion
> 
> I want to plot something (eg a circle) with a fixed ratio of 
> the x and y axis, or (even better) with a fixed size when I 
> print it. Output should then be a circle (actually it'll be 
> someting more complicated) with radius 5cm and not an ellipse.
> 
> I'm _sure_ this is not new, but after looking 45min for a 
> solution, I post here...
> 
> Thanks for help
> Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From aiminy at iastate.edu  Fri Mar  2 19:49:30 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Fri, 02 Mar 2007 12:49:30 -0600
Subject: [R] add mean, sd,
 number of observation in each panel for lattice  histogram
Message-ID: <6.2.3.4.2.20070302124612.0285a9a0@aiminy.mail.iastate.edu>


From Cody_Hamilton at Edwards.com  Fri Mar  2 19:55:43 2007
From: Cody_Hamilton at Edwards.com (Cody_Hamilton at Edwards.com)
Date: Fri, 2 Mar 2007 10:55:43 -0800
Subject: [R] Using R for devices trial
In-Reply-To: <27CA3827C6B33E40874682C469E774DD04DB910A@FMD3CT001.fda.gov>
Message-ID: <OF230BF484.CBCBAF41-ON88257292.0067C5F0-88257292.0067BFE4@irvine.edwards.com>


Mat,

Thank you for the update and for the link.  I look forward to (hopefully)
using R for future FDA submissions.

Regards,
   -Cody



                                                                           
             "Soukup, Mat"                                                 
             <Mat.Soukup at fda.h                                             
             hs.gov>                                                    To 
                                       Cody_Hamilton at edwards.com,          
             03/02/2007 05:28          r-help at r-project.org                
             AM                                                         cc 
                                                                           
                                                                   Subject 
                                       RE: [R] Using R for devices trial   
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Hi Cody,

I would point you to the presentation by Sue Bell at least year's ASA
meeting available here:
http://www.fda.gov/Cder/Offices/Biostatistics/Bell.pdf. I can't speak
for CDRH, but at CDER we are making some progress towards the level of
comfort with the use of R as a "valid" software tool. Specifically,
1. R was granted approval by our IT folks for use on our government PC's
(2 years in the making to get this).
2. An R course is in development for FDA reviewers to use R for review
of clinical trial data.
3. This Monday at the 1st FDA/DIA Spring Meeting I will be offering a
tutorial on Statistical Graphics with R for Clinical Trial Data.
4. An increasing effort to pigeon-tail R to ongoing projects to show
proof by example that R can be trusted when used properly.

So from the regulatory side, progress is being made, but there still do
exist those who have some discomfort with the use of an open-source tool
for data analysis. Someday hopefully that too will be changed.

HTH,

-Mat

Disclaimer: The views expressed are those of the author and must not be
taken to represent policy or guidance on behalf of the Food and Drug
Administration.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
Cody_Hamilton at edwards.com
Sent: Thursday, March 01, 2007 5:43 PM
To: r-help at r-project.org
Subject: [R] Using R for devices trial


I would like to use R for submissions to FDA/CDRH (the medical device
company I work for currently uses only SAS).  Previous postings to the
list
regarding R and 21 CFR 11 compliance have been very helpful.  However,
reluctance to using open source software for statistical analyses and
reporting remains high here at my company.  Has anyone used R for an
official submission to FDA/CDRH?  It would be most helpful if I could
tell
our group that others have been able to use R for this purpose.

Regards,
Cody Hamilton
Staff Biostatistician
Edwards Lifesciences

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From rdpeng at gmail.com  Fri Mar  2 20:07:19 2007
From: rdpeng at gmail.com (Roger D. Peng)
Date: Fri, 02 Mar 2007 14:07:19 -0500
Subject: [R] [friday topic]: what exactly is statistical computing
In-Reply-To: <1115a2b00703021020nc53c830w3c9682123d2124a2@mail.gmail.com>
References: <D8C95B444AD6EE4AAD638D818A9CFD343A1E4C@RINNYCSE000.rth.ad.rothschild.com>
	<1115a2b00703021020nc53c830w3c9682123d2124a2@mail.gmail.com>
Message-ID: <45E875E7.4070400@gmail.com>

This is indeed a "Friday" topic.  I think I understand the difficulty that 
Wensui has.  I teach two classes, each with the phrase "statistical computing" 
in the title, and yet they cover completely different topics.

-roger

Wensui Liu wrote:
> Thanks for your insight, Roger.
> 
> Actually, my question is not related to R only.
> 
> statistical computing is a popular topic recently. However, when I
> check its meaning on wikipedia/google, I couldn't find it.
> 
> another reason why I asked is related to myself. I am very interested
> in this area and maintaining a blog in this topic. however, when asked
> what 'statistical computing' is, I am not able to give a
> well-verbalized answer.
> 
> 
> On 3/2/07, Bos, Roger <roger.bos at us.rothschild.com> wrote:
>> This means it comes with substantial statistical routines built-in.  You
>> could just as well use VBA or Java for your programming language, but
>> with those you would have to write pretty much any stat routine you
>> need.  With R, since it is a 'statistical computing' language, you know
>> that most of what you need has already been programmed, tested(?), and
>> is ready to use.
>>
>> I have seen you on this list for a while.  You already know all this.  I
>> am not sure why you are asking this question.
>>
>> Roger
>>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Wensui Liu
>> Sent: Friday, March 02, 2007 9:43 AM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] [friday topic]: what exactly is statistical computing
>>
>> Dear List,
>> on www.r-project.org, the title says 'The R Project for Statistical
>> Computing'.
>>
>> but what exactly is the definition of statistical computing?
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ********************************************************************** *
>> This message is for the named person's use only. It may
>> contain confidential, proprietary or legally privileged
>> information. No right to confidential or privileged treatment
>> of this message is waived or lost by any error in
>> transmission. If you have received this message in error,
>> please immediately notify the sender by e-mail,
>> delete the message and all copies from your system and destroy
>> any hard copies. You must not, directly or indirectly, use,
>> disclose, distribute, print or copy any part of this message
>> if you are not the intended recipient.
>> **********************************************************************
>>
> 
> 

-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/


From Cody_Hamilton at Edwards.com  Fri Mar  2 20:16:49 2007
From: Cody_Hamilton at Edwards.com (Cody_Hamilton at Edwards.com)
Date: Fri, 2 Mar 2007 11:16:49 -0800
Subject: [R]  Using R for devices trial
Message-ID: <OFF202FC34.9E67CA31-ON88257292.0069DF72-88257292.0069AEBA@irvine.edwards.com>



Mat,

Thank you for the update and for the link.  I look forward to (hopefully)
using R for future FDA submissions.

Regards,
   -Cody



                                                                           
             "Soukup, Mat"                                                 
             <Mat.Soukup at fda.h                                             
             hs.gov>                                                    To 
                                       Cody_Hamilton at edwards.com,          
             03/02/2007 05:28          r-help at r-project.org                
             AM                                                         cc 
                                                                           
                                                                   Subject 
                                       RE: [R] Using R for devices trial   
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Hi Cody,

I would point you to the presentation by Sue Bell at least year's ASA
meeting available here:
http://www.fda.gov/Cder/Offices/Biostatistics/Bell.pdf. I can't speak
for CDRH, but at CDER we are making some progress towards the level of
comfort with the use of R as a "valid" software tool. Specifically,
1. R was granted approval by our IT folks for use on our government PC's
(2 years in the making to get this).
2. An R course is in development for FDA reviewers to use R for review
of clinical trial data.
3. This Monday at the 1st FDA/DIA Spring Meeting I will be offering a
tutorial on Statistical Graphics with R for Clinical Trial Data.
4. An increasing effort to pigeon-tail R to ongoing projects to show
proof by example that R can be trusted when used properly.

So from the regulatory side, progress is being made, but there still do
exist those who have some discomfort with the use of an open-source tool
for data analysis. Someday hopefully that too will be changed.

HTH,

-Mat

Disclaimer: The views expressed are those of the author and must not be
taken to represent policy or guidance on behalf of the Food and Drug
Administration.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
Cody_Hamilton at edwards.com
Sent: Thursday, March 01, 2007 5:43 PM
To: r-help at r-project.org
Subject: [R] Using R for devices trial


I would like to use R for submissions to FDA/CDRH (the medical device
company I work for currently uses only SAS).  Previous postings to the
list
regarding R and 21 CFR 11 compliance have been very helpful.  However,
reluctance to using open source software for statistical analyses and
reporting remains high here at my company.  Has anyone used R for an
official submission to FDA/CDRH?  It would be most helpful if I could
tell
our group that others have been able to use R for this purpose.

Regards,
Cody Hamilton
Staff Biostatistician
Edwards Lifesciences

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From mail at malte-ruesing.de  Fri Mar  2 20:23:45 2007
From: mail at malte-ruesing.de (=?iso-8859-1?Q?Malte_R=FCsing?=)
Date: Fri, 2 Mar 2007 20:23:45 +0100
Subject: [R] R: ARIMA forecasting
Message-ID: <003001c75d00$4ce0c2d0$e6a24870$@de>

Dear all,


I just have a short question regarding the forecasting of ARIMA models with
external regressors. 

I tried to program a ARX(1) model
	arx.mod <- arima(reihe.lern, order = c(1, 0, 0), seasonal =
list(order = c(0, 0, 0), period = 52), xreg = lern.design, include.mean =
TRUE)
for which I need to estimate the next (105th) value. Xreg=lern.design is -
at this time - 104 rows long. I tried to use the following function:
	predict(arx.mod, n.ahead = 1, newxreg=lern.design[105,])

Here, lern.design is 105 rows long, same number of columns. Now the problem
is the following error:
	Error in predict.Arima(arx.mod, n.ahead = 1, newxreg =
lern.design[105,  : 
        'xreg' and 'newxreg' have different numbers of columns

If I use the total "new" lern.design matrix with the external regressor
information (105 rows), R estimates every observation and the 105th is no
longer correct (I recalculated is manually).

What can I do? I just need the forecasted value for the 105th observation so
that I can add this to my data and create a new one-step forecast.

Thanks for your help!


Kind regards
Malte R?sing


From Joseph.F.Lucke at uth.tmc.edu  Fri Mar  2 20:25:59 2007
From: Joseph.F.Lucke at uth.tmc.edu (Lucke, Joseph F)
Date: Fri, 2 Mar 2007 13:25:59 -0600
Subject: [R] [friday topic]: what exactly is statistical computing
In-Reply-To: <45E875E7.4070400@gmail.com>
References: <D8C95B444AD6EE4AAD638D818A9CFD343A1E4C@RINNYCSE000.rth.ad.rothschild.com>
	<1115a2b00703021020nc53c830w3c9682123d2124a2@mail.gmail.com>
	<45E875E7.4070400@gmail.com>
Message-ID: <4677FCB5A35A0441A0E0C99D56B23D910777FDDF@UTHEVS2.mail.uthouston.edu>

Statistical computing perhaps is not so much a single topic as a family
of related topics (a la Wittgenstein) that share a lot in common but
perhaps very little is common to all. For example,
1. Statistical computing in contrast to statistical theory.
6. Statistical computing as a supplement to statistical theory.
2. Statistical computing as gaining insight through data visualization
3. Statistical computing for asymptotic analyses
4. Statistical computing for approximations to unknown distributions
5. Statistical computing for teaching demonstrations
7. Statistical computing for assessing behavior of statistics.

and so on.

Joe
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Roger D. Peng
Sent: Friday, March 02, 2007 1:07 PM
To: Wensui Liu
Cc: r-help at stat.math.ethz.ch; Bos, Roger
Subject: Re: [R] [friday topic]: what exactly is statistical computing

This is indeed a "Friday" topic.  I think I understand the difficulty
that Wensui has.  I teach two classes, each with the phrase "statistical
computing" 
in the title, and yet they cover completely different topics.

-roger

Wensui Liu wrote:
> Thanks for your insight, Roger.
> 
> Actually, my question is not related to R only.
> 
> statistical computing is a popular topic recently. However, when I 
> check its meaning on wikipedia/google, I couldn't find it.
> 
> another reason why I asked is related to myself. I am very interested 
> in this area and maintaining a blog in this topic. however, when asked

> what 'statistical computing' is, I am not able to give a 
> well-verbalized answer.
> 
> 
> On 3/2/07, Bos, Roger <roger.bos at us.rothschild.com> wrote:
>> This means it comes with substantial statistical routines built-in.  
>> You could just as well use VBA or Java for your programming language,

>> but with those you would have to write pretty much any stat routine 
>> you need.  With R, since it is a 'statistical computing' language, 
>> you know that most of what you need has already been programmed, 
>> tested(?), and is ready to use.
>>
>> I have seen you on this list for a while.  You already know all this.

>> I am not sure why you are asking this question.
>>
>> Roger
>>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Wensui Liu
>> Sent: Friday, March 02, 2007 9:43 AM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] [friday topic]: what exactly is statistical computing
>>
>> Dear List,
>> on www.r-project.org, the title says 'The R Project for Statistical 
>> Computing'.
>>
>> but what exactly is the definition of statistical computing?
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> *********************************************************************
>> * * This message is for the named person's use only. It may contain 
>> confidential, proprietary or legally privileged information. No right

>> to confidential or privileged treatment of this message is waived or 
>> lost by any error in transmission. If you have received this message 
>> in error, please immediately notify the sender by e-mail, delete the 
>> message and all copies from your system and destroy any hard copies. 
>> You must not, directly or indirectly, use, disclose, distribute, 
>> print or copy any part of this message if you are not the intended 
>> recipient.
>> *********************************************************************
>> *
>>
> 
> 

--
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jadamson at PARTNERS.ORG  Fri Mar  2 20:30:33 2007
From: jadamson at PARTNERS.ORG (Joel J. Adamson)
Date: Fri, 2 Mar 2007 14:30:33 -0500
Subject: [R]  [friday topic]: what exactly is statistical computing
In-Reply-To: <1115a2b00703020643r5e0ee8d5n7f7399dc0bb3c22c@mail.gmail.com>
References: <1115a2b00703020643r5e0ee8d5n7f7399dc0bb3c22c@mail.gmail.com>
Message-ID: <17896.31577.853792.498096@W0053328.mgh.harvard.edu>



Wensui Liu writes:
 > but what exactly is the definition of statistical computing?

In my world, statistical computing is using computers for statistical
calculation and data management.  By "using computers" I mean "using
computers for their greatest strengths," and by that I mean
programming them to do exactly what I need.

Joel

-- 
Joel J. Adamson
Biostatistician
Pediatric Psychopharmacology Research Unit
Massachusetts General Hospital
Boston, MA  02114
(617) 643-1432
(303) 880-3109





The information transmitted in this electronic communication is intended only for the person or entity to whom it is addressed and may contain confidential and/or privileged material. Any review, retransmission, dissemination or other use of or taking of any action in reliance upon this information by persons or entities other than the intended recipient is prohibited. If you received this information in error, please contact the Compliance HelpLine at 800-856-1983 and properly dispose of this information.


From jadamson at PARTNERS.ORG  Fri Mar  2 20:33:40 2007
From: jadamson at PARTNERS.ORG (Joel J. Adamson)
Date: Fri, 2 Mar 2007 14:33:40 -0500
Subject: [R] [friday topic]: what exactly is statistical computing
In-Reply-To: <D8C95B444AD6EE4AAD638D818A9CFD343A1E4C@RINNYCSE000.rth.ad.rothschild.com>
References: <D8C95B444AD6EE4AAD638D818A9CFD343A1E4C@RINNYCSE000.rth.ad.rothschild.com>
Message-ID: <17896.31764.552380.18879@W0053328.mgh.harvard.edu>

Bos, Roger writes:
 > I have seen you on this list for a while.  You already know all this.  I
 > am not sure why you are asking this question.
 > 
 > Roger

I think "[friday topic]" means "fun discussion."

I agree with your main comment Roger; I often think about the power of
"straight-up" languages like C, Java, etc., and wonder "Why do we need
so many statistical packages, much less totally awesome ones like S?"
  It would be a pain to have to program everything from scratch, and
having something like R eases the communication routes.

Joel

-- 
Joel J. Adamson
Biostatistician
Pediatric Psychopharmacology Research Unit
Massachusetts General Hospital
Boston, MA  02114
(617) 643-1432
(303) 880-3109





The information transmitted in this electronic communication is intended only for the person or entity to whom it is addressed and may contain confidential and/or privileged material. Any review, retransmission, dissemination or other use of or taking of any action in reliance upon this information by persons or entities other than the intended recipient is prohibited. If you received this information in error, please contact the Compliance HelpLine at 800-856-1983 and properly dispose of this information.


From zia.wadud at imperial.ac.uk  Fri Mar  2 21:11:35 2007
From: zia.wadud at imperial.ac.uk (Wadud, Zia)
Date: Fri, 2 Mar 2007 20:11:35 -0000
Subject: [R] sampling random groups with all observations in the group
Message-ID: <735C1873E656C24699818814048F8FB004C7CC37@icex1.ic.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070302/8cb4f900/attachment.pl 

From mervyn at iastate.edu  Fri Mar  2 21:26:31 2007
From: mervyn at iastate.edu (Mervyn G Marasinghe)
Date: Fri, 02 Mar 2007 14:26:31 -0600
Subject: [R]  nlm() problem : extra parameters
Message-ID: <200703022026.l22KQVuC003385@mailhub-4.iastate.edu>

Hello:

Below is a toy logistic regression problem. When I wrote my own code,
Newton-Raphson converged in three iterations using both the gradient
and the Hessian  and the starting values  given below. But I can't
get nlm() to work! I would much appreciate any help.

  > x
[1] 10.2  7.7  5.1  3.8  2.6
  > y
[1] 9 8 3 2 1
  > n
[1] 10  9  6  8 10


derfs4=function(b,x,y,n)
{
          b0 = b[1]
          b1 = b[2]
          c=b0+b1*x
          d=exp(c)
          p=d/(1+d)
          e=d/(1+d)^2
          f  = -sum(log(choose(n,y))-n*log(1+d)+y*c)
          attr(f,"gradient")=c(-sum(y-n*p),-sum(x*(y-n*p)))
          attr(f,"hessian")=matrix(c(sum(n*e),sum(n*x*e),sum(n*x*e),sum(n*x^2*e)),2,2)
          return(f)
}


  > nlm(derfs4,c(-3.9,.64),hessian=T,print.level=2,x=x,y=y,n=n)
Error in choose(n, y) : argument "n" is missing, with no default
  >
I tried a variety of other ways  too.  When I got it to work it did not
converge in 100 iterations ;rather like the fgh example given on the lme
help page.

Mervyn


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From h.wickham at gmail.com  Fri Mar  2 21:57:59 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 2 Mar 2007 14:57:59 -0600
Subject: [R] barplot2, gap.barplot
In-Reply-To: <1172860377.4897.219.camel@localhost.localdomain>
References: <1172757385.5102.42.camel@stryder.skrupellos.priv>
	<1172757514.4897.42.camel@localhost.localdomain>
	<f8e6ff050703020653o1b2f9485v82c78eca40df0890@mail.gmail.com>
	<1172849870.4897.180.camel@localhost.localdomain>
	<f8e6ff050703020807q387d34e1gfa94167e78f25618@mail.gmail.com>
	<1172860377.4897.219.camel@localhost.localdomain>
Message-ID: <f8e6ff050703021257s21fce751x6ed231543bc9df6f@mail.gmail.com>

On 3/2/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> On Fri, 2007-03-02 at 10:07 -0600, hadley wickham wrote:
> > On 3/2/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> > > On Fri, 2007-03-02 at 08:53 -0600, hadley wickham wrote:
> > > > > 3. Depending on the nature of your data, if the extreme value is
> > > > > representative of an important marked difference relative to the other
> > > > > values, then I don't particularly find the 'look' of the plot to be
> > > > > overly problematic. It does appropriately emphasize the large
> > > > > difference.
> > > > >
> > > > > On the other hand, you might want to consider using a log scale on the y
> > > > > axis as an alternative to an axis gap. This would be a reasonable
> > > > > approach to plotting values that have a notable difference in range.  If
> > > > > you do this, note that you would need to ensure that all y values are >0
> > > > > (ie. y axis range minimum, lower bounds of CI's, etc.) since:
> > > > >
> > > > > > log10(0)
> > > > > [1] -Inf
> > > > >
> > > > >
> > > >
> > > > Of course, you can't do this with a bar plot, because bars should be
> > > > anchored at 0.
> > >
> > > Both barplot() and barplot2() support log scaling for both x and y axes.
> > >
> > > In both functions, the default axis minimum for the 'height' axis (y by
> > > default, x if 'horizontal = TRUE') will be 0.9 * min(height) to avert
> > > log10(0) related issues. Errors will be issued otherwise if any values
> > > of 'height' are <= 0 or 'ylim'/'xlim' args are similarly set.
> >
> > I think that's a pretty bad idea - in a bar plot you are comparing the
> > ratio of heights of the bars, not the absolute heights.  It's the same
> > reason it's a bad idea to have a bar graph with a non-0 y-axis - it's
> > misleading.
>
> Hadley,
>
> I might note that even lattice will do this, arguably easier than
> barplot[2]():
>
> library(lattice)
> x <- 10 ^ (0:10)
> barchart(x ~ 0:10, horizontal = FALSE,
>          scales = list(y = list(log = 10)))
>
>
> Is it the right thing to do?  I'll leave that for others to debate. I
> have stronger feelings on the 'gapped axis' issue.

I think this is up there with double and gapped axes.  Although it's
much easier to resolve - just use a dot plot instead (which is
generally a pretty good rule whenever you want to use a bar plot)

> There have been requests for log scales on barplots on the R lists going
> back several years, which is one of the reasons that I wrote barplot2()
> some years ago.  It was also one of my first exercises in gaining a
> lower level understanding of R's graphics models.

People are always asking for things they don't really want! ;)

I (obviously) have pretty strong feelings about graphics - I don't
think you should be able to create meaningless (in some sense)
graphics.

Hadley


From rmh at temple.edu  Fri Mar  2 22:14:52 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri,  2 Mar 2007 16:14:52 -0500 (EST)
Subject: [R] Fwd: Re: [friday topic]: what exactly is statistical computing
Message-ID: <20070302161452.BVU00087@po-d.temple.edu>

This is a very fascinating discussion topic.  I find I run into
some fundamental differences in interpretation of the phrase "statistical
computing".  I think of it as writing programs or functions, such as R or packages
in R, and of understanding the numerical analysis behind these functions.

I exclude USING computer programs, such as R, for data analysis from my
definition of statistical computing.  I see that as doing statistics.
I have had students, some sent by other faculty members, in my class
on statistical computing thinking they were going to learn how to do statistical
analysis using the computer.  There was a clash of expectations between what
they thought they were taking and what I had in the syllabus.

Rich

---- Original message ----
>Date: Fri, 2 Mar 2007 13:25:59 -0600
>From: "Lucke, Joseph F" <Joseph.F.Lucke at uth.tmc.edu>  
>Subject: Re: [R] [friday topic]: what exactly is statistical computing  
>To: "Roger D. Peng" <rdpeng at gmail.com>, "Wensui Liu" <liuwensui at gmail.com>
>Cc: r-help at stat.math.ethz.ch, "Bos, Roger" <roger.bos at us.rothschild.com>
>
>Statistical computing perhaps is not so much a single topic as a family
>of related topics (a la Wittgenstein) that share a lot in common but
>perhaps very little is common to all. For example,
>1. Statistical computing in contrast to statistical theory.
>6. Statistical computing as a supplement to statistical theory.
>2. Statistical computing as gaining insight through data visualization
>3. Statistical computing for asymptotic analyses
>4. Statistical computing for approximations to unknown distributions
>5. Statistical computing for teaching demonstrations
>7. Statistical computing for assessing behavior of statistics.
>
>and so on.
>
>Joe


From ggrothendieck at gmail.com  Fri Mar  2 22:25:22 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Mar 2007 16:25:22 -0500
Subject: [R] nlm() problem : extra parameters
In-Reply-To: <200703022026.l22KQVuC003385@mailhub-4.iastate.edu>
References: <200703022026.l22KQVuC003385@mailhub-4.iastate.edu>
Message-ID: <971536df0703021325h22dbaf9bjbd3366952085c178@mail.gmail.com>

Change the name of n to something else.  Also please using spacing
in your code for readability.

> x <- c(10.2, 7.7, 5.1, 3.8, 2.6)
> y <- c(9, 8, 3, 2, 1)
> n. <- c(10, 9, 6, 8, 10)
>
>
> derfs4 <- function(b, x, y, n.)
+ {
+          b0 <- b[1]
+          b1 <- b[2]
+          c=b0 +b1 * x
+          d <- exp(c)
+          p= d/(1+d)
+          e <- d/(1+d)^2
+          f  <- - sum(log(choose(n., y)) - n. * log(1 + d) + y * c)
+          attr(f, "gradient") <- c(- sum(y - n. * p), -sum(x * (y - n. * p)))
+          attr(f, "hessian") <- matrix(c(sum(n. * e), sum(n. * x * e),
+               sum(n. * x * e), sum(n. * x^2 * e)), 2, 2)
+          return(f)
+ }
>
>
> nlm(derfs4, c(-3.9, 0.64), hessian = TRUE, x = x, y = y, n. = n.)
$minimum
[1] 5.746945

$estimate
[1] -3.5380638  0.6505037

$gradient
[1] -0.031425476 -0.006637742

$hessian
          [,1]      [,2]
[1,]  5.963954  31.15266
[2,] 31.152658 192.53408

$code
[1] 4

$iterations
[1] 100



On 3/2/07, Mervyn G Marasinghe <mervyn at iastate.edu> wrote:
> Hello:
>
> Below is a toy logistic regression problem. When I wrote my own code,
> Newton-Raphson converged in three iterations using both the gradient
> and the Hessian  and the starting values  given below. But I can't
> get nlm() to work! I would much appreciate any help.
>
>  > x
> [1] 10.2  7.7  5.1  3.8  2.6
>  > y
> [1] 9 8 3 2 1
>  > n
> [1] 10  9  6  8 10
>
>
> derfs4=function(b,x,y,n)
> {
>          b0 = b[1]
>          b1 = b[2]
>          c=b0+b1*x
>          d=exp(c)
>          p=d/(1+d)
>          e=d/(1+d)^2
>          f  = -sum(log(choose(n,y))-n*log(1+d)+y*c)
>          attr(f,"gradient")=c(-sum(y-n*p),-sum(x*(y-n*p)))
>          attr(f,"hessian")=matrix(c(sum(n*e),sum(n*x*e),sum(n*x*e),sum(n*x^2*e)),2,2)
>          return(f)
> }
>
>
>  > nlm(derfs4,c(-3.9,.64),hessian=T,print.level=2,x=x,y=y,n=n)
> Error in choose(n, y) : argument "n" is missing, with no default
>  >
> I tried a variety of other ways  too.  When I got it to work it did not
> converge in 100 iterations ;rather like the fgh example given on the lme
> help page.
>
> Mervyn
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ccleland at optonline.net  Fri Mar  2 22:26:11 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 02 Mar 2007 16:26:11 -0500
Subject: [R] sampling random groups with all observations in the group
In-Reply-To: <735C1873E656C24699818814048F8FB004C7CC37@icex1.ic.ac.uk>
References: <735C1873E656C24699818814048F8FB004C7CC37@icex1.ic.ac.uk>
Message-ID: <45E89673.2000503@optonline.net>

Wadud, Zia wrote:
> Hi
> I have a panel dataset with large number of groups and differing number
> of observations for each group. I want to randomly select say, 20% of
> the groups or 200 groups, but along with all observations from the
> selcted groups (with the corresponding data). 
> I guess it is possible to generate a random sample from the groups ids
> and then match that with the entire dataset to have the intended
> dataset, but it sounds cumbersome and possibly there is an easier way to
> do this? checked the package 'sampling' or command 'sample', but they
> cant do exactly the same thing.
> I was wondering if someone on this list will be able to share his/her
> knowldege?

  How about something like this?

df <- data.frame(GROUP = rep(1:5, c(2,3,4,2,2)), Y = runif(13))

# Sample Two of the Five Groups

subset(df, GROUP %in% with(df, sample(unique(GROUP), 2)))

> Thanks in advance,
> Zia
> **********************************************************
> Zia Wadud
> PhD Student
> Centre for Transport Studies
> Department of Civil and Environmental Engineering
> Imperial College London
> London SW7 2AZ
> Tel +44 (0) 207 594 6055
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From rolf at math.unb.ca  Fri Mar  2 22:31:10 2007
From: rolf at math.unb.ca (rolf at math.unb.ca)
Date: Fri, 2 Mar 2007 17:31:10 -0400 (AST)
Subject: [R] nlm() problem : extra parameters
Message-ID: <200703022131.l22LVAWv013242@weisner.math.unb.ca>

Here's a partial answer to your question.

The argument ``n'' is getting lost because it ``matches'' one of the
formal arguments to nlm, namely ``ndigit''.  If you change the
argument list of ``derfs4'' to ``b,x,y,size'' (and replace references
to ``n'' in the body of derfs4 by references to ``size'') then nlm()
will work (?) --- but it seems to take 337 iterations, r.t. 3 (!!!)
to converge.

No idea what the problem is.

			cheers,

				Rolf Turner

===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===
Original message:

Mervyn G Marasinghe wrote:

> Below is a toy logistic regression problem. When I wrote my own code,
> Newton-Raphson converged in three iterations using both the gradient
> and the Hessian  and the starting values  given below. But I can't
> get nlm() to work! I would much appreciate any help.
> 
>   > x
> [1] 10.2  7.7  5.1  3.8  2.6
>   > y
> [1] 9 8 3 2 1
>   > n
> [1] 10  9  6  8 10
> 
> 
> derfs4=function(b,x,y,n)
> {
>           b0 = b[1]
>           b1 = b[2]
>           c=b0+b1*x
>           d=exp(c)
>           p=d/(1+d)
>           e=d/(1+d)^2
>           f  = -sum(log(choose(n,y))-n*log(1+d)+y*c)
>           attr(f,"gradient")=c(-sum(y-n*p),-sum(x*(y-n*p)))
>           attr(f,"hessian")=matrix(c(sum(n*e),sum(n*x*e),sum(n*x*e),sum(n*x^2*e)),2,2)
>           return(f)
> }
> 
> 
>   > nlm(derfs4,c(-3.9,.64),hessian=T,print.level=2,x=x,y=y,n=n)
> Error in choose(n, y) : argument "n" is missing, with no default
>   >
> I tried a variety of other ways  too.  When I got it to work it did not
> converge in 100 iterations ;rather like the fgh example given on the lme
> help page.


From marc_schwartz at comcast.net  Fri Mar  2 22:38:16 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 02 Mar 2007 15:38:16 -0600
Subject: [R] barplot2, gap.barplot
In-Reply-To: <f8e6ff050703021257s21fce751x6ed231543bc9df6f@mail.gmail.com>
References: <1172757385.5102.42.camel@stryder.skrupellos.priv>
	<1172757514.4897.42.camel@localhost.localdomain>
	<f8e6ff050703020653o1b2f9485v82c78eca40df0890@mail.gmail.com>
	<1172849870.4897.180.camel@localhost.localdomain>
	<f8e6ff050703020807q387d34e1gfa94167e78f25618@mail.gmail.com>
	<1172860377.4897.219.camel@localhost.localdomain>
	<f8e6ff050703021257s21fce751x6ed231543bc9df6f@mail.gmail.com>
Message-ID: <1172871497.4897.328.camel@localhost.localdomain>

On Fri, 2007-03-02 at 14:57 -0600, hadley wickham wrote:
> On 3/2/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> > On Fri, 2007-03-02 at 10:07 -0600, hadley wickham wrote:
> > > On 3/2/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> > > > On Fri, 2007-03-02 at 08:53 -0600, hadley wickham wrote:
> > > > > > 3. Depending on the nature of your data, if the extreme value is
> > > > > > representative of an important marked difference relative to the other
> > > > > > values, then I don't particularly find the 'look' of the plot to be
> > > > > > overly problematic. It does appropriately emphasize the large
> > > > > > difference.
> > > > > >
> > > > > > On the other hand, you might want to consider using a log scale on the y
> > > > > > axis as an alternative to an axis gap. This would be a reasonable
> > > > > > approach to plotting values that have a notable difference in range.  If
> > > > > > you do this, note that you would need to ensure that all y values are >0
> > > > > > (ie. y axis range minimum, lower bounds of CI's, etc.) since:
> > > > > >
> > > > > > > log10(0)
> > > > > > [1] -Inf
> > > > > >
> > > > > >
> > > > >
> > > > > Of course, you can't do this with a bar plot, because bars should be
> > > > > anchored at 0.
> > > >
> > > > Both barplot() and barplot2() support log scaling for both x and y axes.
> > > >
> > > > In both functions, the default axis minimum for the 'height' axis (y by
> > > > default, x if 'horizontal = TRUE') will be 0.9 * min(height) to avert
> > > > log10(0) related issues. Errors will be issued otherwise if any values
> > > > of 'height' are <= 0 or 'ylim'/'xlim' args are similarly set.
> > >
> > > I think that's a pretty bad idea - in a bar plot you are comparing the
> > > ratio of heights of the bars, not the absolute heights.  It's the same
> > > reason it's a bad idea to have a bar graph with a non-0 y-axis - it's
> > > misleading.
> >
> > Hadley,
> >
> > I might note that even lattice will do this, arguably easier than
> > barplot[2]():
> >
> > library(lattice)
> > x <- 10 ^ (0:10)
> > barchart(x ~ 0:10, horizontal = FALSE,
> >          scales = list(y = list(log = 10)))
> >
> >
> > Is it the right thing to do?  I'll leave that for others to debate. I
> > have stronger feelings on the 'gapped axis' issue.
> 
> I think this is up there with double and gapped axes.  Although it's
> much easier to resolve - just use a dot plot instead (which is
> generally a pretty good rule whenever you want to use a bar plot)

Indeed. Beyond Tufte and Cleveland, a while back I had been doing some
searches on related matters and happened to come across this article by
Naomi Robbins:

  http://www.b-eye-network.com/newsletters/ben/2468

You may find the graphics somewhat familiar looking... 

> > There have been requests for log scales on barplots on the R lists going
> > back several years, which is one of the reasons that I wrote barplot2()
> > some years ago.  It was also one of my first exercises in gaining a
> > lower level understanding of R's graphics models.
> 
> People are always asking for things they don't really want! ;)

Quite...  :-)

> I (obviously) have pretty strong feelings about graphics - I don't
> think you should be able to create meaningless (in some sense)
> graphics.

But, how do you really feel?  ;-)

Regards,

Marc


From Greg.Snow at intermountainmail.org  Fri Mar  2 22:42:27 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 2 Mar 2007 14:42:27 -0700
Subject: [R] sampling random groups with all observations in the group
In-Reply-To: <735C1873E656C24699818814048F8FB004C7CC37@icex1.ic.ac.uk>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB8798C9@LP-EXCHVS07.CO.IHC.COM>

One possibility is to use split to create a list with each of your
groups as an element, sample from the list, then combine back into a
data frame.  For example:

> mydata <- data.frame(group=sample(LETTERS[1:5], 100, replace=TRUE),
+ x= 1:100, y= rnorm(100) )
> head(mydata)
  group x          y
1     B 1 -1.1709539
2     A 2  0.2438249
3     C 3 -1.9079472
4     E 4  0.6155387
5     E 5 -1.0671110
6     C 6  0.8109344
> mydata2 <- split(mydata, mydata$group)
> mysamp <- sample(5,2)
> mydata3 <- do.call('rbind',mydata2[mysamp])
> summary(mydata3)
 group        x               y          
 A: 0   Min.   : 3.00   Min.   :-1.9079  
 B: 0   1st Qu.:18.75   1st Qu.:-0.9798  
 C:17   Median :46.50   Median :-0.4309  
 D:19   Mean   :45.19   Mean   :-0.2333  
 E: 0   3rd Qu.:68.25   3rd Qu.: 0.4351  
        Max.   :97.00   Max.   : 3.0469  
> 

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Wadud, Zia
> Sent: Friday, March 02, 2007 1:12 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] sampling random groups with all observations in the group
> 
> Hi
> I have a panel dataset with large number of groups and 
> differing number of observations for each group. I want to 
> randomly select say, 20% of the groups or 200 groups, but 
> along with all observations from the selcted groups (with the 
> corresponding data). 
> I guess it is possible to generate a random sample from the 
> groups ids and then match that with the entire dataset to 
> have the intended dataset, but it sounds cumbersome and 
> possibly there is an easier way to do this? checked the 
> package 'sampling' or command 'sample', but they cant do 
> exactly the same thing.
> I was wondering if someone on this list will be able to share 
> his/her knowldege?
> Thanks in advance,
> Zia
> **********************************************************
> Zia Wadud
> PhD Student
> Centre for Transport Studies
> Department of Civil and Environmental Engineering Imperial 
> College London London SW7 2AZ Tel +44 (0) 207 594 6055
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ccleland at optonline.net  Fri Mar  2 22:46:35 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 02 Mar 2007 16:46:35 -0500
Subject: [R] sampling random groups with all observations in the group
In-Reply-To: <45E89673.2000503@optonline.net>
References: <735C1873E656C24699818814048F8FB004C7CC37@icex1.ic.ac.uk>
	<45E89673.2000503@optonline.net>
Message-ID: <45E89B3B.1040905@optonline.net>

Chuck Cleland wrote:
> Wadud, Zia wrote:
>> Hi
>> I have a panel dataset with large number of groups and differing number
>> of observations for each group. I want to randomly select say, 20% of
>> the groups or 200 groups, but along with all observations from the
>> selcted groups (with the corresponding data). 
>> I guess it is possible to generate a random sample from the groups ids
>> and then match that with the entire dataset to have the intended
>> dataset, but it sounds cumbersome and possibly there is an easier way to
>> do this? checked the package 'sampling' or command 'sample', but they
>> cant do exactly the same thing.
>> I was wondering if someone on this list will be able to share his/her
>> knowldege?
> 
>   How about something like this?
> 
> df <- data.frame(GROUP = rep(1:5, c(2,3,4,2,2)), Y = runif(13))
> 
> # Sample Two of the Five Groups
> 
> subset(df, GROUP %in% with(df, sample(unique(GROUP), 2)))

  The with() part can be dropped too.

subset(df, GROUP %in% sample(unique(GROUP), 2))

>> Thanks in advance,
>> Zia
>> **********************************************************
>> Zia Wadud
>> PhD Student
>> Centre for Transport Studies
>> Department of Civil and Environmental Engineering
>> Imperial College London
>> London SW7 2AZ
>> Tel +44 (0) 207 594 6055
>>  
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From rvaradhan at jhmi.edu  Fri Mar  2 22:52:27 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Fri, 2 Mar 2007 16:52:27 -0500
Subject: [R] Questions regarding biplot
Message-ID: <000001c75d15$11d9a0c0$7c94100a@win.ad.jhu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070302/e0d09870/attachment.pl 

From Frederic.Jean at univ-brest.fr  Fri Mar  2 22:52:31 2007
From: Frederic.Jean at univ-brest.fr (Frederic Jean)
Date: Fri, 02 Mar 2007 22:52:31 +0100
Subject: [R] significant anova but no distinct groups ?
Message-ID: <20070302225231.qkybsr1b4koc08ck@cassis-gw.univ-brest.fr>

Dear all,

I am studying a dataset using the aov() function.

The independant variable 'cds' is a factor() with 8 levels and here is  
the result in studying the dependant variable 'rta' with aov() :

> summary(aov(rta ~ cds))
             Df  Sum Sq Mean Sq F value  Pr(>F)
cds          7 0.34713 0.04959  2.3807 0.02777
Residuals   92 1.91635 0.02083

The dependant variable 'rta' is normally distributed and variances are  
homogeneous.
But when studying the result with TukeyHSD, no differences in 'rta'  
are seen among groups of 'cds' :

> TukeyHSD(aov(rta ~ cds), which="cds")
   Tukey multiple comparisons of means
     95% family-wise confidence level

Fit: aov(formula = rta ~ cds)

$cds
              diff        lwr        upr     p adj
1-0 -0.1046092796 -0.4331100 0.22389141 0.9751178
2-0  0.0359991860 -0.1371359 0.20913425 0.9980970
3-0  0.0261665235 -0.1348524 0.18718540 0.9996165
4-0  0.0004502442 -0.1805448 0.18144531 1.0000000
5-0 -0.1438949939 -0.3104752 0.02268526 0.1422670
[...]
7-5  0.0621598639 -0.1027595 0.22707926 0.9386170
7-6  0.0256519274 -0.1757408 0.22704465 0.9999248

I tried a pairwise.t.test (holm correction) which also was not able to  
detect differences in 'rta' among groups of 'cds'
I've never been confronted to such a situation before : is it just a  
problem of power of the /a posteriori/ tests used ? Do I miss  
something important in basic stats or in R ?
How to highlight differences among 'cds' groups seen with aov() ?

Any help appreciated
Thanks in advance,

Fred J.


From hpbenton at scripps.edu  Fri Mar  2 22:53:29 2007
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Fri, 02 Mar 2007 13:53:29 -0800
Subject: [R] reading text file not table
In-Reply-To: <45D6D677.8090405@pburns.seanet.com>
References: <45D64B9E.2070708@scripps.edu> <45D6D677.8090405@pburns.seanet.com>
Message-ID: <45E89CD9.40605@scripps.edu>

Sorry just one more question,

Patrick Burns wrote:
>
> as part of your alternative plan.  But I don't think you would
> need to write a file and read it back in again.
Yea So I tried to use the connection but that doesn't work. How can I
read the object back in with read.csv, or at very least get it into a
matrix seperated by comma?

Thanks

PB


From rmh at temple.edu  Fri Mar  2 23:13:55 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri,  2 Mar 2007 17:13:55 -0500 (EST)
Subject: [R] significant anova but no distinct groups ?
Message-ID: <20070302171355.BVU09919@po-d.temple.edu>

The pairwise tests compare only pairs of means.  Plot the means
themselves.  Chances are you will see some clustering of groups,
or a visible contrast of several groups.

Rich


From Cody_Hamilton at Edwards.com  Fri Mar  2 23:22:32 2007
From: Cody_Hamilton at Edwards.com (Cody_Hamilton at Edwards.com)
Date: Fri, 2 Mar 2007 14:22:32 -0800
Subject: [R] significant anova but no distinct groups ?
In-Reply-To: <20070302225231.qkybsr1b4koc08ck@cassis-gw.univ-brest.fr>
Message-ID: <OF265846C4.6EA82E2A-ON88257292.007A126A-88257292.007AAF7F@irvine.edwards.com>


Frederic,

You're performing 8*7/2 = 28 multiple comparisons controlling the FWE at
the .05 level.  Using a Bonferroni's adjustment (admittedly more
conservative than the Holm's or Tukey's approach), that's testing each
comparison at the .05/28 = 0.0018 level.  With only 100 observations spread
over 8 levels, you won't have much power to detect a difference.

Regards,
   -Cody



                                                                           
             Frederic Jean                                                 
             <Frederic.Jean at un                                             
             iv-brest.fr>                                               To 
             Sent by:                  r-help at r-project.org                
             r-help-bounces at st                                          cc 
             at.math.ethz.ch                                               
                                                                   Subject 
                                       [R] significant anova but no        
             03/02/2007 01:52          distinct groups ?                   
             PM                                                            
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Dear all,

I am studying a dataset using the aov() function.

The independant variable 'cds' is a factor() with 8 levels and here is
the result in studying the dependant variable 'rta' with aov() :

> summary(aov(rta ~ cds))
             Df  Sum Sq Mean Sq F value  Pr(>F)
cds          7 0.34713 0.04959  2.3807 0.02777
Residuals   92 1.91635 0.02083

The dependant variable 'rta' is normally distributed and variances are
homogeneous.
But when studying the result with TukeyHSD, no differences in 'rta'
are seen among groups of 'cds' :

> TukeyHSD(aov(rta ~ cds), which="cds")
   Tukey multiple comparisons of means
     95% family-wise confidence level

Fit: aov(formula = rta ~ cds)

$cds
              diff        lwr        upr     p adj
1-0 -0.1046092796 -0.4331100 0.22389141 0.9751178
2-0  0.0359991860 -0.1371359 0.20913425 0.9980970
3-0  0.0261665235 -0.1348524 0.18718540 0.9996165
4-0  0.0004502442 -0.1805448 0.18144531 1.0000000
5-0 -0.1438949939 -0.3104752 0.02268526 0.1422670
[...]
7-5  0.0621598639 -0.1027595 0.22707926 0.9386170
7-6  0.0256519274 -0.1757408 0.22704465 0.9999248

I tried a pairwise.t.test (holm correction) which also was not able to
detect differences in 'rta' among groups of 'cds'
I've never been confronted to such a situation before : is it just a
problem of power of the /a posteriori/ tests used ? Do I miss
something important in basic stats or in R ?
How to highlight differences among 'cds' groups seen with aov() ?

Any help appreciated
Thanks in advance,

Fred J.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From gingerlygen at gmail.com  Fri Mar  2 23:25:04 2007
From: gingerlygen at gmail.com (Gen)
Date: Fri, 2 Mar 2007 14:25:04 -0800 (PST)
Subject: [R] Horvitz Thompson Variance
Message-ID: <9278653.post@talk.nabble.com>


I am performing linear regression R and would like to incorporate sampling
weights.  
Does any one know how to obtain Horvitz-Thompson variance estimates when
performing linear regression?
Thank you for your help.
-- 
View this message in context: http://www.nabble.com/Horvitz-Thompson-Variance-tf3336348.html#a9278653
Sent from the R help mailing list archive at Nabble.com.


From beth.gifford at duke.edu  Fri Mar  2 23:43:45 2007
From: beth.gifford at duke.edu (Beth Gifford)
Date: Fri, 2 Mar 2007 17:43:45 -0500
Subject: [R] Mitools and lmer
In-Reply-To: <cbbcbd790703021440g4c0e287o713ee14f1e472285@mail.gmail.com>
References: <cbbcbd790703021440g4c0e287o713ee14f1e472285@mail.gmail.com>
Message-ID: <cbbcbd790703021443y22d32cf0j49c5e623035e2b2d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070302/456e19eb/attachment.pl 

From ted.harding at nessie.mcc.ac.uk  Fri Mar  2 23:53:45 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 02 Mar 2007 22:53:45 -0000 (GMT)
Subject: [R] Fwd: Re: [friday topic]: what exactly is statistical com
In-Reply-To: <20070302161452.BVU00087@po-d.temple.edu>
Message-ID: <XFMail.070302225345.ted.harding@nessie.mcc.ac.uk>

On 02-Mar-07 Richard M. Heiberger wrote:
> This is a very fascinating discussion topic.  I find I run into
> some fundamental differences in interpretation of the phrase
> "statistical computing".  I think of it as writing programs or
> functions, such as R or packages in R, and of understanding the
> numerical analysis behind these functions.
> 
> I exclude USING computer programs, such as R, for data analysis
> from my definition of statistical computing.  I see that as doing
> statistics. I have had students, some sent by other faculty members,
> in my class on statistical computing thinking they were going to
> learn how to do statistical analysis using the computer.  There
> was a clash of expectations between what they thought they were
> taking and what I had in the syllabus.

It is indeed a fascinating topic, and I agree with the implications
of Richard's views above.

Though computing machinery (from Brunsvigas with manual crank-handles
upwards) has been used for doing the computations of statistics
since the year dot, statistical computing (in my view of it)
did not begin to develop until much later.

I think the first developments which could be recognised as
"statistical cmputing" (as opposed to using computers to do
statistics) were the pioneering GENSTAT and GLIM (1973-4,
though developed over some years previously). Possibly
what characterised them for this was the fact that their
programming language was recognisably statistical in flavour,
and the commands triggered computational procedures in which
statistical algorithms were implemented.

Then, as the science of computer programming developed, and
became more generalised, with "structures", "methods" and
all the rest, so these concepts were implemented for statistics.
The result of a computation was a data-structure, which could
be recognised by any method that was capable of dealing with

It's perhaps hard to say when S was actually born: perhaps
passage to the outside world began with its port to UNIX
in 1979, though it was conceived around 1975. But it must
be acknowledged that in its adaptation of advanced (for the
time) programming methods to statistics was a breakthrough
in statistical computing.

A quite early simple instance of this kind of programming
was SPIDA (Statistical Program for Interactive Data Analysis)
which was developed prior to 1988 -- since Dan Lunn & Don McNeil
issued a SPIDA User's Manual in 1988 (and perhaps grew out of
NcNeil's approaches described in his 1977 book "Interactive
Data Analysis: A Practical Primer"), followed by the book

  Computer-Interactive Data Aanalysis
  A.D. Lunn and D.R. McNeil (Wiley 1991)
  (with a couple of 5.25" DOS floppies with the SPIDA software)

which I remember using with pleasure!

So I see this confluence of the evolution of computational
concepts and techniques through the 1970's and 80's, with
the development of statistical modelling techniques and
their implementation in software, as the core of "statistical
computing".

Best wishes to all,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 02-Mar-07                                       Time: 22:53:33
------------------------------ XFMail ------------------------------


From jholtman at gmail.com  Sat Mar  3 00:37:28 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 2 Mar 2007 18:37:28 -0500
Subject: [R] reading text file not table
In-Reply-To: <45E89CD9.40605@scripps.edu>
References: <45D64B9E.2070708@scripps.edu> <45D6D677.8090405@pburns.seanet.com>
	<45E89CD9.40605@scripps.edu>
Message-ID: <644e1f320703021537m2e8098eat1930b3bf7c45b922@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070302/7beed051/attachment.pl 

From mervyn at iastate.edu  Sat Mar  3 00:46:04 2007
From: mervyn at iastate.edu (Mervyn G Marasinghe)
Date: Fri, 02 Mar 2007 17:46:04 -0600
Subject: [R] Fwd: Re: [friday topic]: what exactly is statistical  com
In-Reply-To: <XFMail.070302225345.ted.harding@nessie.mcc.ac.uk>
References: <20070302161452.BVU00087@po-d.temple.edu>
	<XFMail.070302225345.ted.harding@nessie.mcc.ac.uk>
Message-ID: <200703022346.l22Nk5VC008814@mailhub-3.iastate.edu>

Ted:

I agree with what you and Richard have to say on this topic but 
disagree with your time frame. I would like to believe that rudiments 
of what we call statistical computing today started long before the 
mid-seventies. Most probably it goes back to the mid-fifties.  There 
is a book that is long out of print "Statistical Computation" edited 
by Milton and Nelder  (a copy of which I am privileged to 
possess)  of the proceedings of a meeting  held at the University of 
Wisconsin in April 1969. The list of contributors read like a who's 
who of statistical computing. To name a few: Box, Chambers, Dixon, 
Golub, Hartley, Hemmerle, Kruskal, Massey, Nelder, Wilkinson etc.. I 
know for a fact  they have been doing research on statistical 
computing for a while before 1969! I am also pretty sure that the 
work on  BMD programs at UCLA by Dixon and Jennrich preceded 
Genstat  and Glim by many years because they were working on it in 
the mid-sixties. And they weren't using software; they were writing 
them. Just my bit. Take care.

Mervyn


  At 04:53 PM 3/2/2007, you wrote:
>On 02-Mar-07 Richard M. Heiberger wrote:
> > This is a very fascinating discussion topic.  I find I run into
> > some fundamental differences in interpretation of the phrase
> > "statistical computing".  I think of it as writing programs or
> > functions, such as R or packages in R, and of understanding the
> > numerical analysis behind these functions.
> >
> > I exclude USING computer programs, such as R, for data analysis
> > from my definition of statistical computing.  I see that as doing
> > statistics. I have had students, some sent by other faculty members,
> > in my class on statistical computing thinking they were going to
> > learn how to do statistical analysis using the computer.  There
> > was a clash of expectations between what they thought they were
> > taking and what I had in the syllabus.
>
>It is indeed a fascinating topic, and I agree with the implications
>of Richard's views above.
>
>Though computing machinery (from Brunsvigas with manual crank-handles
>upwards) has been used for doing the computations of statistics
>since the year dot, statistical computing (in my view of it)
>did not begin to develop until much later.
>
>I think the first developments which could be recognised as
>"statistical cmputing" (as opposed to using computers to do
>statistics) were the pioneering GENSTAT and GLIM (1973-4,
>though developed over some years previously). Possibly
>what characterised them for this was the fact that their
>programming language was recognisably statistical in flavour,
>and the commands triggered computational procedures in which
>statistical algorithms were implemented.
>
>Then, as the science of computer programming developed, and
>became more generalised, with "structures", "methods" and
>all the rest, so these concepts were implemented for statistics.
>The result of a computation was a data-structure, which could
>be recognised by any method that was capable of dealing with
>
>It's perhaps hard to say when S was actually born: perhaps
>passage to the outside world began with its port to UNIX
>in 1979, though it was conceived around 1975. But it must
>be acknowledged that in its adaptation of advanced (for the
>time) programming methods to statistics was a breakthrough
>in statistical computing.
>
>A quite early simple instance of this kind of programming
>was SPIDA (Statistical Program for Interactive Data Analysis)
>which was developed prior to 1988 -- since Dan Lunn & Don McNeil
>issued a SPIDA User's Manual in 1988 (and perhaps grew out of
>NcNeil's approaches described in his 1977 book "Interactive
>Data Analysis: A Practical Primer"), followed by the book
>
>   Computer-Interactive Data Aanalysis
>   A.D. Lunn and D.R. McNeil (Wiley 1991)
>   (with a couple of 5.25" DOS floppies with the SPIDA software)
>
>which I remember using with pleasure!
>
>So I see this confluence of the evolution of computational
>concepts and techniques through the 1970's and 80's, with
>the development of statistical modelling techniques and
>their implementation in software, as the core of "statistical
>computing".
>
>Best wishes to all,
>Ted.
>
>--------------------------------------------------------------------
>E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
>Fax-to-email: +44 (0)870 094 0861
>Date: 02-Mar-07                                       Time: 22:53:33
>------------------------------ XFMail ------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From F.MENDIBURU at CGIAR.ORG  Sat Mar  3 00:51:36 2007
From: F.MENDIBURU at CGIAR.ORG (Mendiburu, Felipe (CIP))
Date: Fri, 2 Mar 2007 18:51:36 -0500
Subject: [R] significant anova but no distinct groups ?
References: <20070302225231.qkybsr1b4koc08ck@cassis-gw.univ-brest.fr>
Message-ID: <B7B34444ECA41A41AC47DABA057CE7A206FCC2@webmail.cip.cgiar.org>

You can use the LSD.test or waller.test of the package agricolae that less conservatives than tukey. 

________________________________

From: r-help-bounces at stat.math.ethz.ch on behalf of Frederic Jean
Sent: Fri 3/2/2007 4:52 PM
To: r-help at r-project.org
Subject: [R] significant anova but no distinct groups ?



Dear all,

I am studying a dataset using the aov() function.

The independant variable 'cds' is a factor() with 8 levels and here is 
the result in studying the dependant variable 'rta' with aov() :

> summary(aov(rta ~ cds))
             Df  Sum Sq Mean Sq F value  Pr(>F)
cds          7 0.34713 0.04959  2.3807 0.02777
Residuals   92 1.91635 0.02083

The dependant variable 'rta' is normally distributed and variances are 
homogeneous.
But when studying the result with TukeyHSD, no differences in 'rta' 
are seen among groups of 'cds' :

> TukeyHSD(aov(rta ~ cds), which="cds")
   Tukey multiple comparisons of means
     95% family-wise confidence level

Fit: aov(formula = rta ~ cds)

$cds
              diff        lwr        upr     p adj
1-0 -0.1046092796 -0.4331100 0.22389141 0.9751178
2-0  0.0359991860 -0.1371359 0.20913425 0.9980970
3-0  0.0261665235 -0.1348524 0.18718540 0.9996165
4-0  0.0004502442 -0.1805448 0.18144531 1.0000000
5-0 -0.1438949939 -0.3104752 0.02268526 0.1422670
[...]
7-5  0.0621598639 -0.1027595 0.22707926 0.9386170
7-6  0.0256519274 -0.1757408 0.22704465 0.9999248

I tried a pairwise.t.test (holm correction) which also was not able to 
detect differences in 'rta' among groups of 'cds'
I've never been confronted to such a situation before : is it just a 
problem of power of the /a posteriori/ tests used ? Do I miss 
something important in basic stats or in R ?
How to highlight differences among 'cds' groups seen with aov() ?

Any help appreciated
Thanks in advance,

Fred J.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From hpbenton at scripps.edu  Sat Mar  3 01:08:56 2007
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Fri, 02 Mar 2007 16:08:56 -0800
Subject: [R] reading text file not table
In-Reply-To: <644e1f320703021537m2e8098eat1930b3bf7c45b922@mail.gmail.com>
References: <45D64B9E.2070708@scripps.edu>
	<45D6D677.8090405@pburns.seanet.com>	 <45E89CD9.40605@scripps.edu>
	<644e1f320703021537m2e8098eat1930b3bf7c45b922@mail.gmail.com>
Message-ID: <45E8BC98.2080306@scripps.edu>


From ggrothendieck at gmail.com  Sat Mar  3 01:17:52 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 2 Mar 2007 19:17:52 -0500
Subject: [R] Fwd: Re: [friday topic]: what exactly is statistical com
In-Reply-To: <XFMail.070302225345.ted.harding@nessie.mcc.ac.uk>
References: <20070302161452.BVU00087@po-d.temple.edu>
	<XFMail.070302225345.ted.harding@nessie.mcc.ac.uk>
Message-ID: <971536df0703021617j7be99487obd7eb7a8d9e724dd@mail.gmail.com>

On 3/2/07, Ted Harding <ted.harding at nessie.mcc.ac.uk> wrote:

> I think the first developments which could be recognised as
> "statistical cmputing" (as opposed to using computers to do
> statistics) were the pioneering GENSTAT and GLIM (1973-4,
> though developed over some years previously). Possibly
> what characterised them for this was the fact that their
> programming language was recognisably statistical in flavour,
> and the commands triggered computational procedures in which
> statistical algorithms were implemented.

There were also many people using APL for statistical computing
in that time frame.

>
> Then, as the science of computer programming developed, and
> became more generalised, with "structures", "methods" and
> all the rest, so these concepts were implemented for statistics.
> The result of a computation was a data-structure, which could
> be recognised by any method that was capable of dealing with
>
> It's perhaps hard to say when S was actually born: perhaps
> passage to the outside world began with its port to UNIX
> in 1979, though it was conceived around 1975. But it must
> be acknowledged that in its adaptation of advanced (for the
> time) programming methods to statistics was a breakthrough
> in statistical computing.
>
> A quite early simple instance of this kind of programming
> was SPIDA (Statistical Program for Interactive Data Analysis)
> which was developed prior to 1988 -- since Dan Lunn & Don McNeil
> issued a SPIDA User's Manual in 1988 (and perhaps grew out of
> NcNeil's approaches described in his 1977 book "Interactive
> Data Analysis: A Practical Primer"), followed by the book
>
>  Computer-Interactive Data Aanalysis
>  A.D. Lunn and D.R. McNeil (Wiley 1991)
>  (with a couple of 5.25" DOS floppies with the SPIDA software)

McNeil's earlier book used APL.

Around the same time frame, i.e. late seventies, David Donoho at
Princeton

  http://www.ims.nus.edu.sg/imprints/interviews/DavidDonoho.pdf

developed ISP possibly along with Peter Bloomfield.  I think this
pre-dated S but was somewhat S-like in that it:
- was written in C using yacc
- ran on UNIX
- was interactive and had builtin regression and other stat routines
- eventually made its way to hundreds of universities (though it was
ultimately displaced by S)

>
> which I remember using with pleasure!
>
> So I see this confluence of the evolution of computational
> concepts and techniques through the 1970's and 80's, with
> the development of statistical modelling techniques and
> their implementation in software, as the core of "statistical
> computing".

A few other points.  There was an ASA section on statistical
computing in the 70's:

http://www.statcomputing.org/officers/1970s.html

and getting back to the subject heading of this thread, one of the Chairs
wrote a book called Statistical Computing published in 1980 (which I
own):

http://www.amazon.ca/Statistical-Computing-William-J-Kennedy/dp/0824768981


From jungpinwu at gmail.com  Sat Mar  3 02:25:29 2007
From: jungpinwu at gmail.com (Jungpin Wu)
Date: Sat, 3 Mar 2007 09:25:29 +0800
Subject: [R] package installation for windows vista
Message-ID: <3379b4a00703021725v251454d2t706a7608413c7891@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070303/5af469aa/attachment.pl 

From jh56h7h4yerg6w53643g at yahoo.co.jp  Sat Mar  3 07:20:08 2007
From: jh56h7h4yerg6w53643g at yahoo.co.jp (=?iso-2022-jp?B?GyRCOzNFRCEhOS1KczJdGyhC?=)
Date: Sat, 3 Mar 2007 07:20:08 +0100
Subject: [R] =?iso-2022-jp?b?GyRCN25LdiReJEckSxsoQg==?=
Message-ID: <200703030620.l236K8oB019209@hypatia.math.ethz.ch>


From chiya31 at gmail.com  Sat Mar  3 08:38:51 2007
From: chiya31 at gmail.com (chiya sharma)
Date: Sat, 3 Mar 2007 13:08:51 +0530
Subject: [R] Simplest question ever...
In-Reply-To: <9258932.post@talk.nabble.com>
References: <9258932.post@talk.nabble.com>
Message-ID: <92cc2b7b0703022338s38c217c2t92fc1ab14a86fca3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070303/0009cb9a/attachment.pl 

From pilar.grau at urjc.es  Sat Mar  3 10:41:12 2007
From: pilar.grau at urjc.es (pilar.grau at urjc.es)
Date: Sat, 03 Mar 2007 10:41:12 +0100
Subject: [R] GarchOxFit Interface
Message-ID: <20070303104112.5lhpqon8eq2o8wcg@www.webmail.urjc.es>



Hello,

I am having problems with the GarchOxFit. I have my Ox Console instaled in
c:\Program Files\ox, and when I execute the GarchOxFit the result is 
C:\Ox\bin\oxl.exe not found. I there any posiblility to execute the command
without installing again Ox in c:\?

My OS is windows XP.

Thankyou for your help.

Pilar Grau


From patrick at pdrechsler.de  Sat Mar  3 11:14:06 2007
From: patrick at pdrechsler.de (Patrick Drechsler)
Date: Sat, 03 Mar 2007 11:14:06 +0100
Subject: [R] format of summary.lm for 2-way ANOVA
Message-ID: <877ityeis1.fsf@pdrechsler.de>

 Hi,

I am performing a two-way ANOVA (2 factors with 4 and 5 levels,
respectively). If I'm interpreting the output of summary correctly,
then the interaction between both factors is significant:

,----
| ## Two-way ANOVA with possible interaction:
| > model1 <- aov(log(y) ~ xForce*xVel, data=mydataset)
|
| > summary(model1)
|              Df  Sum Sq Mean Sq F value    Pr(>F)    
| xForce        3  16.640   5.547 19.0191 1.708e-11 ***
| xVel          4  96.391  24.098 82.6312 < 2.2e-16 ***
| xForce:xVel  12  10.037   0.836  2.8681 0.0008528 ***
| Residuals   371 108.194   0.292                      
| ---
| Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 
| 3 observations deleted due to missingness
`----

To see the interactions in detail I call summary.lm:

,----
| > summary.lm(model1)
| 
| Call:
| aov(formula = log(y) ~ xForce * xVel, data = mydataset)
| 
| Residuals:
|      Min       1Q   Median       3Q      Max 
| -2.04830 -0.32420 -0.04653  0.34928  1.46755 
| 
| Coefficients:
|                  Estimate Std. Error t value Pr(>|t|)    
| (Intercept)     -1.663406   0.027335 -60.853  < 2e-16 ***
| xForce.L         0.408977   0.054726   7.473 5.68e-13 ***
| xForce.Q         0.101240   0.054670   1.852   0.0648 .  
| xForce.C        -0.068068   0.054613  -1.246   0.2134    
| xVel.L           1.079042   0.061859  17.444  < 2e-16 ***
| xVel.Q           0.339802   0.061439   5.531 6.03e-08 ***
| xVel.C           0.015422   0.060751   0.254   0.7997    
| xVel^4          -0.044399   0.060430  -0.735   0.4630    
| xForce.L:xVel.L  0.622060   0.123966   5.018 8.12e-07 ***
| xForce.Q:xVel.L  0.034298   0.123718   0.277   0.7818    
| xForce.C:xVel.L -0.114776   0.123470  -0.930   0.3532    
| xForce.L:xVel.Q  0.309293   0.123057   2.513   0.0124 *  
| xForce.Q:xVel.Q  0.054798   0.122879   0.446   0.6559    
| xForce.C:xVel.Q -0.144219   0.122700  -1.175   0.2406    
| xForce.L:xVel.C  0.110588   0.121565   0.910   0.3636    
| xForce.Q:xVel.C -0.001929   0.121502  -0.016   0.9873    
| xForce.C:xVel.C -0.039477   0.121438  -0.325   0.7453    
| xForce.L:xVel^4  0.090491   0.120870   0.749   0.4545    
| xForce.Q:xVel^4 -0.002762   0.120861  -0.023   0.9818    
| xForce.C:xVel^4 -0.028836   0.120852  -0.239   0.8115    
| ---
| Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 
| 
| Residual standard error: 0.54 on 371 degrees of freedom
|   (3 observations deleted due to missingness)
| Multiple R-Squared: 0.5322,	Adjusted R-squared: 0.5082 
| F-statistic: 22.21 on 19 and 371 DF,  p-value: < 2.2e-16 
`----

I am wondering what the logic is behind the formatting of
rownames. What do the strings "L", "Q", "C" and "^4" mean?

Apologies in case I missed something obvious in the relevant
documentations/archives.

Thanks for any pointers,

Patrick

PS: Here are some details about the dataset:

,----
| > summary(mydataset)
|        y            xForce     xVel   
|  Min.   :0.03662   0.01:97   10  :79  
|  1st Qu.:0.10376   0.1 :98   50  :80  
|  Median :0.16314   1   :98   100 :80  
|  Mean   :0.26592   2   :98   500 :80  
|  3rd Qu.:0.28077   NA's: 3   5000:72  
|  Max.   :2.39490             NA's: 3  
|  NA's   :3.00000
| 
| > str(mydataset)
| 'data.frame':	394 obs. of  3 variables:
|  $ y     : num  0.167 0.158 0.152 0.158 0.131 ...
|  $ xForce: Ord.factor w/ 4 levels "0.01"<"0.1"<"1"<..: 1 2 3 4 1 2 3 4 1 2 ...
|  $ xVel  : Ord.factor w/ 5 levels "10"<"50"<"100"<..: 3 3 3 3 1 1 1 1 2 2 ...
`----

I am using

platform       i486-pc-linux-gnu           
arch           i486                        
os             linux-gnu                   
system         i486, linux-gnu             
status                                     
major          2                           
minor          4.1                         
year           2006                        
month          12                          
day            18                          
svn rev        40228                       
language       R                           
version.string R version 2.4.1 (2006-12-18)


From martin.becker at mx.uni-saarland.de  Sat Mar  3 11:48:08 2007
From: martin.becker at mx.uni-saarland.de (Martin Becker)
Date: Sat, 03 Mar 2007 11:48:08 +0100
Subject: [R] GarchOxFit Interface
In-Reply-To: <20070303104112.5lhpqon8eq2o8wcg@www.webmail.urjc.es>
References: <20070303104112.5lhpqon8eq2o8wcg@www.webmail.urjc.es>
Message-ID: <45E95268.7030501@mx.uni-saarland.de>

pilar.grau at urjc.es wrote:
> Hello,
>
> I am having problems with the GarchOxFit. I have my Ox Console instaled in
> c:\Program Files\ox, and when I execute the GarchOxFit the result is 
> C:\Ox\bin\oxl.exe not found. I there any posiblility to execute the command
> without installing again Ox in c:\?
>
>   
I suppose you are speaking of GarchOxFit in package fSeries, but you 
didn't mention which version of fSeries you are using. I think, in 
current versions (240.10068), you may set OXPATH in the global 
environment to your installation of OX, maybe it is even necessary to do so.
Apart from that I think there is still a buglet in the Ox-Interface 
functions, which I mentioned some time ago on R-sig-finance, see  
https://stat.ethz.ch/pipermail/r-sig-finance/2005q4/000498.html. I think 
you still find this buglet in the current version source (line 217 of 
4D-GarchOxInterface.R), but since I don't use the Ox interface any more, 
I am not sure. I think the quick workaround is to always store the time 
series to be fitted in the variable "x" in the global environment.

hth,
  Martin
> My OS is windows XP.
>
> Thankyou for your help.
>
> Pilar Grau
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tobias.verbeke at gmail.com  Sat Mar  3 11:51:33 2007
From: tobias.verbeke at gmail.com (Tobias Verbeke)
Date: Sat, 03 Mar 2007 11:51:33 +0100
Subject: [R] Horvitz Thompson Variance
In-Reply-To: <9278653.post@talk.nabble.com>
References: <9278653.post@talk.nabble.com>
Message-ID: <45E95335.4020902@businessdecision.com>

Gen wrote:
> I am performing linear regression R and would like to incorporate sampling
> weights.  
>   

There is a survey package for the analysis of data from complex sample 
designs
of which svyglm might fit your purposes.

For the home page of the package (with lots of materials), see
http://faculty.washington.edu/tlumley/survey/

HTH,
Tobias

> Does any one know how to obtain Horvitz-Thompson variance estimates when
> performing linear regression?
> Thank you for your help.
>   

-- 

Tobias Verbeke - Consultant
Business & Decision Benelux
Rue de la r?volution 8
1000 Brussels - BELGIUM

+32 499 36 33 15
tobias.verbeke at businessdecision.com


From ccleland at optonline.net  Sat Mar  3 11:53:56 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Sat, 03 Mar 2007 05:53:56 -0500
Subject: [R] format of summary.lm for 2-way ANOVA
In-Reply-To: <877ityeis1.fsf@pdrechsler.de>
References: <877ityeis1.fsf@pdrechsler.de>
Message-ID: <45E953C4.6000508@optonline.net>

Patrick Drechsler wrote:
>  Hi,
> 
> I am performing a two-way ANOVA (2 factors with 4 and 5 levels,
> respectively). If I'm interpreting the output of summary correctly,
> then the interaction between both factors is significant:
> 
> ,----
> | ## Two-way ANOVA with possible interaction:
> | > model1 <- aov(log(y) ~ xForce*xVel, data=mydataset)
> |
> | > summary(model1)
> |              Df  Sum Sq Mean Sq F value    Pr(>F)    
> | xForce        3  16.640   5.547 19.0191 1.708e-11 ***
> | xVel          4  96.391  24.098 82.6312 < 2.2e-16 ***
> | xForce:xVel  12  10.037   0.836  2.8681 0.0008528 ***
> | Residuals   371 108.194   0.292                      
> | ---
> | Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 
> | 3 observations deleted due to missingness
> `----
> 
> To see the interactions in detail I call summary.lm:
> 
> ,----
> | > summary.lm(model1)
> | 
> | Call:
> | aov(formula = log(y) ~ xForce * xVel, data = mydataset)
> | 
> | Residuals:
> |      Min       1Q   Median       3Q      Max 
> | -2.04830 -0.32420 -0.04653  0.34928  1.46755 
> | 
> | Coefficients:
> |                  Estimate Std. Error t value Pr(>|t|)    
> | (Intercept)     -1.663406   0.027335 -60.853  < 2e-16 ***
> | xForce.L         0.408977   0.054726   7.473 5.68e-13 ***
> | xForce.Q         0.101240   0.054670   1.852   0.0648 .  
> | xForce.C        -0.068068   0.054613  -1.246   0.2134    
> | xVel.L           1.079042   0.061859  17.444  < 2e-16 ***
> | xVel.Q           0.339802   0.061439   5.531 6.03e-08 ***
> | xVel.C           0.015422   0.060751   0.254   0.7997    
> | xVel^4          -0.044399   0.060430  -0.735   0.4630    
> | xForce.L:xVel.L  0.622060   0.123966   5.018 8.12e-07 ***
> | xForce.Q:xVel.L  0.034298   0.123718   0.277   0.7818    
> | xForce.C:xVel.L -0.114776   0.123470  -0.930   0.3532    
> | xForce.L:xVel.Q  0.309293   0.123057   2.513   0.0124 *  
> | xForce.Q:xVel.Q  0.054798   0.122879   0.446   0.6559    
> | xForce.C:xVel.Q -0.144219   0.122700  -1.175   0.2406    
> | xForce.L:xVel.C  0.110588   0.121565   0.910   0.3636    
> | xForce.Q:xVel.C -0.001929   0.121502  -0.016   0.9873    
> | xForce.C:xVel.C -0.039477   0.121438  -0.325   0.7453    
> | xForce.L:xVel^4  0.090491   0.120870   0.749   0.4545    
> | xForce.Q:xVel^4 -0.002762   0.120861  -0.023   0.9818    
> | xForce.C:xVel^4 -0.028836   0.120852  -0.239   0.8115    
> | ---
> | Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 
> | 
> | Residual standard error: 0.54 on 371 degrees of freedom
> |   (3 observations deleted due to missingness)
> | Multiple R-Squared: 0.5322,	Adjusted R-squared: 0.5082 
> | F-statistic: 22.21 on 19 and 371 DF,  p-value: < 2.2e-16 
> `----
> 
> I am wondering what the logic is behind the formatting of
> rownames. What do the strings "L", "Q", "C" and "^4" mean?

  The default contrast for ordered factors is contr.poly(), and "L",
"Q", "C" and "^4" refer to linear, quadratic, cubic, and quartic,
respectively.
  You might look at some plots if you have not already.  For example:

with(mydataset, interaction.plot(xForce, xVel, log(y)))

library(lattice)
bwplot(log(y) ~ xForce | xVel, data = mydataset)

> Apologies in case I missed something obvious in the relevant
> documentations/archives.
> 
> Thanks for any pointers,
> 
> Patrick
> 
> PS: Here are some details about the dataset:
> 
> ,----
> | > summary(mydataset)
> |        y            xForce     xVel   
> |  Min.   :0.03662   0.01:97   10  :79  
> |  1st Qu.:0.10376   0.1 :98   50  :80  
> |  Median :0.16314   1   :98   100 :80  
> |  Mean   :0.26592   2   :98   500 :80  
> |  3rd Qu.:0.28077   NA's: 3   5000:72  
> |  Max.   :2.39490             NA's: 3  
> |  NA's   :3.00000
> | 
> | > str(mydataset)
> | 'data.frame':	394 obs. of  3 variables:
> |  $ y     : num  0.167 0.158 0.152 0.158 0.131 ...
> |  $ xForce: Ord.factor w/ 4 levels "0.01"<"0.1"<"1"<..: 1 2 3 4 1 2 3 4 1 2 ...
> |  $ xVel  : Ord.factor w/ 5 levels "10"<"50"<"100"<..: 3 3 3 3 1 1 1 1 2 2 ...
> `----
> 
> I am using
> 
> platform       i486-pc-linux-gnu           
> arch           i486                        
> os             linux-gnu                   
> system         i486, linux-gnu             
> status                                     
> major          2                           
> minor          4.1                         
> year           2006                        
> month          12                          
> day            18                          
> svn rev        40228                       
> language       R                           
> version.string R version 2.4.1 (2006-12-18)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From patrick at pdrechsler.de  Sat Mar  3 12:22:03 2007
From: patrick at pdrechsler.de (Patrick Drechsler)
Date: Sat, 03 Mar 2007 12:22:03 +0100
Subject: [R] format of summary.lm for 2-way ANOVA
References: <877ityeis1.fsf@pdrechsler.de> <45E953C4.6000508@optonline.net>
Message-ID: <873b4mefms.fsf@pdrechsler.de>

Chuck Cleland <ccleland at optonline.net> writes:

> Patrick Drechsler wrote:

[...]
>> To see the interactions in detail I call summary.lm:
>> 
>> ,----
>> | > summary.lm(model1)
[...]
>> | xVel.L           1.079042   0.061859  17.444  < 2e-16 ***
>> | xVel.Q           0.339802   0.061439   5.531 6.03e-08 ***
>> | xVel.C           0.015422   0.060751   0.254   0.7997    
>> | xVel^4          -0.044399   0.060430  -0.735   0.4630    
[...]

>> I am wondering what the logic is behind the formatting of
>> rownames. What do the strings "L", "Q", "C" and "^4" mean?
>
>   The default contrast for ordered factors is contr.poly(), and "L",
> "Q", "C" and "^4" refer to linear, quadratic, cubic, and quartic,
> respectively.

Thank you very much this explanation! Could you point me in the right
direction where this is documented? I have looked at ?summary,
?summary.lm and ?contr.poly etc.

>   You might look at some plots if you have not already.  For example:
>
> with(mydataset, interaction.plot(xForce, xVel, log(y)))
>
> library(lattice)
> bwplot(log(y) ~ xForce | xVel, data = mydataset)

Nice, thanks also for this pointer.

Cheers

Patrick
-- 
By working faithfully eight hours a day,
you may eventually get to be boss and work twelve.
		-- Robert Frost


From rolf at math.unb.ca  Sat Mar  3 15:18:17 2007
From: rolf at math.unb.ca (rolf at math.unb.ca)
Date: Sat, 3 Mar 2007 10:18:17 -0400 (AST)
Subject: [R] significant anova but no distinct groups ?
Message-ID: <200703031418.l23EIHv8020938@weisner.math.unb.ca>

Frederic Jean wrote:

> I am studying a dataset using the aov() function.
> 
> The independant variable 'cds' is a factor() with 8 levels and here is  
> the result in studying the dependant variable 'rta' with aov() :
> 
> > summary(aov(rta ~ cds))
>              Df  Sum Sq Mean Sq F value  Pr(>F)
> cds          7 0.34713 0.04959  2.3807 0.02777
> Residuals   92 1.91635 0.02083
> 
> The dependant variable 'rta' is normally distributed and variances are  
> homogeneous.
> But when studying the result with TukeyHSD, no differences in 'rta'  
> are seen among groups of 'cds' :
> 
> > TukeyHSD(aov(rta ~ cds), which="cds")
>    Tukey multiple comparisons of means
>      95% family-wise confidence level
> 
> Fit: aov(formula = rta ~ cds)
> 
> $cds
>               diff        lwr        upr     p adj
> 1-0 -0.1046092796 -0.4331100 0.22389141 0.9751178
> 2-0  0.0359991860 -0.1371359 0.20913425 0.9980970
> 3-0  0.0261665235 -0.1348524 0.18718540 0.9996165
> 4-0  0.0004502442 -0.1805448 0.18144531 1.0000000
> 5-0 -0.1438949939 -0.3104752 0.02268526 0.1422670
> [...]
> 7-5  0.0621598639 -0.1027595 0.22707926 0.9386170
> 7-6  0.0256519274 -0.1757408 0.22704465 0.9999248
> 
> I tried a pairwise.t.test (holm correction) which also was not able to  
> detect differences in 'rta' among groups of 'cds'
> I've never been confronted to such a situation before : is it just a  
> problem of power of the /a posteriori/ tests used ? Do I miss  
> something important in basic stats or in R ?
> How to highlight differences among 'cds' groups seen with aov() ?

        The apparent paradox is only apparent.  This sort of thing
        can and does happen.

	One way of thinking about this situation is to envisage a
	circle (Anova) and a square (multiple comparisons),
	superimposed, with the corners of the square sticking outside
	of the circle, and the extremities of the circle protruding
	beyond the edges of the square.

	You get a ``significant'' result from the Anova if a point
	lands outside the circle; you get a ``significant'' result
	from the multiple comparisons if a point lands outside the
	square.  So if a point lands in the corners of the square
	that stick out beyond the circle, you have a ``significant''
	Anova result, but find no ``significant'' differences in the
	multiple comparisons.  Conversely a point could land in the
	extremities of the circle that protrude beyond the edges of
	the square, in which case you would find ``significant''
	differences in the multiple comparisons but your Anova test
	would not be ``significant''.

	These are rare but not unheard of phenomena.  The essence of
	the situation is that the data are giving you an ambiguous
	message.  There is no real way to resolve the ambiguity
	except by collecting more data.

        Note that if there is a significant Anova result there
        will be at least one ``contrast'' amongst the means that
        is significantly different from zero on an a posteriori
        basis.  This contrast need not however be a pairwise
        difference between means.

                                        cheers,

                                                Rolf Turner
                                                rolf at math.unb.ca


From shiazy at gmail.com  Sat Mar  3 15:32:37 2007
From: shiazy at gmail.com (Shiazy)
Date: Sat, 03 Mar 2007 15:32:37 +0100
Subject: [R] How to convert List object to function arguments?
Message-ID: <45E98705.7060705@gmail.com>

Dear R gurus,

I have a function "goftests" that receives the following arguments:
* a vector "x" of data values;
* a distribution name "dist";
* the dots list ("...") containing a list a parameters to pass to CDF 
function;
and calls several goodness-of-fit tests on the given data values against 
the given distribution.
That is:

##### BEGIN CODE SNIP #####

# Covert a distribution name to the related CDF function name
distname2cdfname <- function( dist )
{
   if ( dist == "beta" ) { return( "pbeta" ); }
   if ( dist == "cauchy" ) { return( "pcauchy" ); }

   # many other and personalized (e.g. pareto, ...) distributions ...

   return( "" ); # fall-back case
}

# Performs some GoF tests (KS, Chi-Square, Anderson-Darling,...)
# (the "..." list contains parameters for CDF function)
goftests <- function( x, dist, ... )
{
   cdfname <- distname2cdfname( dist );
   res$ks <- ks.test( x, cdfname, ... );
   # res$chisq <- ...
   # res$anddarl <- ...

   return( res )
};

##### END CODE SNIP #####

So the problem is passing "..." to CDF function (for instance, see 
ks.test above) in the "right form". The "..." is passed (to "goftests") 
as a list object and it should be converted to an "argument list".
For instance:

##### BEGIN CODE SNIP #####

parms <- list( shape1 = 2, shape2 = 5 );
goftests( x, "beta", parms[["shape1"]], parms[["shape2"]] ); # Works!
goftests( x, "beta", parms ) # Don't Works!

parms <- list( aNum = 5, aVector = c(1,2,3), aMatrix = 
matrix(c(4,5,6,7,8,9),nrow=2,byrow=T) );
goftests( x, "my-special-distr", parms[["aVector"]], parms[["aMatrix"]], 
parms[["aNumber"]] ); # Works!
goftests( x, "my-special-distr", parms ) # Don't Works!

##### END CODE SNIP #####

Obviously I have to use the "don't work" form.

How can I do this in R (if possible)?

Thank you very much in advance!

Best regards,

-- Marco


From h.wickham at gmail.com  Sat Mar  3 16:05:38 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sat, 3 Mar 2007 09:05:38 -0600
Subject: [R] How to convert List object to function arguments?
In-Reply-To: <45E98705.7060705@gmail.com>
References: <45E98705.7060705@gmail.com>
Message-ID: <f8e6ff050703030705p364c37d7y59fb7c8951c463cd@mail.gmail.com>

Have a look at do.call

Hadley

On 3/3/07, Shiazy <shiazy at gmail.com> wrote:
> Dear R gurus,
>
> I have a function "goftests" that receives the following arguments:
> * a vector "x" of data values;
> * a distribution name "dist";
> * the dots list ("...") containing a list a parameters to pass to CDF
> function;
> and calls several goodness-of-fit tests on the given data values against
> the given distribution.
> That is:
>
> ##### BEGIN CODE SNIP #####
>
> # Covert a distribution name to the related CDF function name
> distname2cdfname <- function( dist )
> {
>    if ( dist == "beta" ) { return( "pbeta" ); }
>    if ( dist == "cauchy" ) { return( "pcauchy" ); }
>
>    # many other and personalized (e.g. pareto, ...) distributions ...
>
>    return( "" ); # fall-back case
> }
>
> # Performs some GoF tests (KS, Chi-Square, Anderson-Darling,...)
> # (the "..." list contains parameters for CDF function)
> goftests <- function( x, dist, ... )
> {
>    cdfname <- distname2cdfname( dist );
>    res$ks <- ks.test( x, cdfname, ... );
>    # res$chisq <- ...
>    # res$anddarl <- ...
>
>    return( res )
> };
>
> ##### END CODE SNIP #####
>
> So the problem is passing "..." to CDF function (for instance, see
> ks.test above) in the "right form". The "..." is passed (to "goftests")
> as a list object and it should be converted to an "argument list".
> For instance:
>
> ##### BEGIN CODE SNIP #####
>
> parms <- list( shape1 = 2, shape2 = 5 );
> goftests( x, "beta", parms[["shape1"]], parms[["shape2"]] ); # Works!
> goftests( x, "beta", parms ) # Don't Works!
>
> parms <- list( aNum = 5, aVector = c(1,2,3), aMatrix =
> matrix(c(4,5,6,7,8,9),nrow=2,byrow=T) );
> goftests( x, "my-special-distr", parms[["aVector"]], parms[["aMatrix"]],
> parms[["aNumber"]] ); # Works!
> goftests( x, "my-special-distr", parms ) # Don't Works!
>
> ##### END CODE SNIP #####
>
> Obviously I have to use the "don't work" form.
>
> How can I do this in R (if possible)?
>
> Thank you very much in advance!
>
> Best regards,
>
> -- Marco
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From shiazy at gmail.com  Sat Mar  3 16:34:06 2007
From: shiazy at gmail.com (Shiazy)
Date: Sat, 03 Mar 2007 16:34:06 +0100
Subject: [R] How to convert List object to function arguments?
In-Reply-To: <f8e6ff050703030705p364c37d7y59fb7c8951c463cd@mail.gmail.com>
References: <45E98705.7060705@gmail.com>
	<f8e6ff050703030705p364c37d7y59fb7c8951c463cd@mail.gmail.com>
Message-ID: <45E9956E.2040202@gmail.com>

"do.call" createa a function call and evaluates it. I think this isn't 
for me. Same thing for "call"; it wants the function name as first argument.

However, I have to pass to "ks.test" the CDF function name and, 
separately, the related arguments.

ks.test( x, cdfname, ... );

The problem is "..." comes as a list object and I didn't find a way to 
"coerce" this object to a arguments list, preserving names a values 
(note each list item can be a complex object such as a vector, matrix, 
... so I cannot use unlist o as.array)

-- Marco

hadley wickham wrote:
> Have a look at do.call
> 
> Hadley
> 
> On 3/3/07, Shiazy <shiazy at gmail.com> wrote:
>> Dear R gurus,
>>
>> I have a function "goftests" that receives the following arguments:
>> * a vector "x" of data values;
>> * a distribution name "dist";
>> * the dots list ("...") containing a list a parameters to pass to CDF
>> function;
>> and calls several goodness-of-fit tests on the given data values against
>> the given distribution.
>> That is:
>>
>> ##### BEGIN CODE SNIP #####
>>
>> # Covert a distribution name to the related CDF function name
>> distname2cdfname <- function( dist )
>> {
>>    if ( dist == "beta" ) { return( "pbeta" ); }
>>    if ( dist == "cauchy" ) { return( "pcauchy" ); }
>>
>>    # many other and personalized (e.g. pareto, ...) distributions ...
>>
>>    return( "" ); # fall-back case
>> }
>>
>> # Performs some GoF tests (KS, Chi-Square, Anderson-Darling,...)
>> # (the "..." list contains parameters for CDF function)
>> goftests <- function( x, dist, ... )
>> {
>>    cdfname <- distname2cdfname( dist );
>>    res$ks <- ks.test( x, cdfname, ... );
>>    # res$chisq <- ...
>>    # res$anddarl <- ...
>>
>>    return( res )
>> };
>>
>> ##### END CODE SNIP #####
>>
>> So the problem is passing "..." to CDF function (for instance, see
>> ks.test above) in the "right form". The "..." is passed (to "goftests")
>> as a list object and it should be converted to an "argument list".
>> For instance:
>>
>> ##### BEGIN CODE SNIP #####
>>
>> parms <- list( shape1 = 2, shape2 = 5 );
>> goftests( x, "beta", parms[["shape1"]], parms[["shape2"]] ); # Works!
>> goftests( x, "beta", parms ) # Don't Works!
>>
>> parms <- list( aNum = 5, aVector = c(1,2,3), aMatrix =
>> matrix(c(4,5,6,7,8,9),nrow=2,byrow=T) );
>> goftests( x, "my-special-distr", parms[["aVector"]], parms[["aMatrix"]],
>> parms[["aNumber"]] ); # Works!
>> goftests( x, "my-special-distr", parms ) # Don't Works!
>>
>> ##### END CODE SNIP #####
>>
>> Obviously I have to use the "don't work" form.
>>
>> How can I do this in R (if possible)?
>>
>> Thank you very much in advance!
>>
>> Best regards,
>>
>> -- Marco
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From shiazy at gmail.com  Sat Mar  3 17:11:06 2007
From: shiazy at gmail.com (Shiazy)
Date: Sat, 03 Mar 2007 17:11:06 +0100
Subject: [R] How to convert List object to function arguments?
In-Reply-To: <f8e6ff050703030737n524407ady200158465c8d0112@mail.gmail.com>
References: <45E98705.7060705@gmail.com>	
	<f8e6ff050703030705p364c37d7y59fb7c8951c463cd@mail.gmail.com>	
	<45E9956E.2040202@gmail.com>
	<f8e6ff050703030737n524407ady200158465c8d0112@mail.gmail.com>
Message-ID: <45E99E1A.8010808@gmail.com>

With a little change it's seem to work:

x <- c(0, 1855, 72, 4830, 493, 424, 193, 2489, 156262, 4557, 958)
params <- list(shape2=5, shape1=2)
do.call( "ks.test",  c(list(x), cdfname( "beta" ), params ))

In fact if I do

call("ks.test", list(x=x, cdfname=cdfname, ...))

I get:

ks.test(list(x = c(0, 1855, 72, 4830, 493, 424, 193, 2489, 156262, 4557, 
958), y = "pbeta", list(shape2 = 5, shape1 = 2)))

and the error:

Error in y(sort(x), ...) : argument "shape2" is missing, with no default

Instead with the new form:

call( "ks.test",  c(list(x), mg_dists_cdfName( "beta" ), parms ))

I get:

ks.test(list(c(0, 1855, 72, 4830, 493, 424, 193, 2489, 156262, 4557, 
958), "pbeta", shape2 = 5, shape1 = 2))

and no error.

Thank you so much!!!

-- Marco

hadley wickham wrote:
> On 3/3/07, Shiazy <shiazy at gmail.com> wrote:
>> "do.call" createa a function call and evaluates it. I think this isn't
>> for me. Same thing for "call"; it wants the function name as first 
>> argument.
>>
>> However, I have to pass to "ks.test" the CDF function name and,
>> separately, the related arguments.
>>
>> ks.test( x, cdfname, ... );
>>
>> The problem is "..." comes as a list object and I didn't find a way to
>> "coerce" this object to a arguments list, preserving names a values
>> (note each list item can be a complex object such as a vector, matrix,
>> ... so I cannot use unlist o as.array)
> 
> I think
> do.call("ks.test", list(x=x, cdfname=cdfname, ...))
> will do what you want.
> 
> Hadley
> 
> PS.  You don't need ; at the end of every line.
>


From bates at stat.wisc.edu  Sat Mar  3 17:13:31 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 3 Mar 2007 10:13:31 -0600
Subject: [R] Mitools and lmer
In-Reply-To: <cbbcbd790703021443y22d32cf0j49c5e623035e2b2d@mail.gmail.com>
References: <cbbcbd790703021440g4c0e287o713ee14f1e472285@mail.gmail.com>
	<cbbcbd790703021443y22d32cf0j49c5e623035e2b2d@mail.gmail.com>
Message-ID: <40e66e0b0703030813u3bf4daebm4743fd233ff23ed1@mail.gmail.com>

On 3/2/07, Beth Gifford <beth.gifford at duke.edu> wrote:
> Hey there
> I am estimating a multilevel model using lmer.  I have 5 imputed datasets so
> I am using mitools to pool the estimates from the 5
> datasets.  Everything seems to work until I try to use
> MIcombine to produced pooled estimates.  Does anyone have any suggestions?  The betas and the standard errors were extracted with no problem so everything seems to work smoothly up until that point.

I'm not familiar with the mltools package and I didn't see it listed
in the CRAN packages.  Can you provide a reference or a link to the
package?

> > Program
> > #Read data
> > data.dir<-system.file("dta",package="mitools")
> > files.imp<-imputationList(lapply(list.files(data.dir,
> > pattern="imp.\\.dta", full=TRUE), read.dta))
> >
> > #estimate model over each imputed dataset
> > model0<-with(files.imp,lmer( erq2tnc ~1+trt2+nash+wash+male+coh2+coh3+(1 |
> > sitebeth)))
> > #extract betas and standard errors
> > betas<-MIextract(model0,fun=coef)
> > vars<-MIextract(model0,fun=vcov)
> > #Combine the results
> > summary(MIcombine(betas,vars))

> > Error in cbar + results[[i]] : non-numeric argument to binary operator
> > Error in summary(MIcombine(betas, vars)) :
> > error in evaluating the argument 'object' in selecting a method for
> > function 'summary'

First use traceback() to discover where the (first) error occurred.
My guess is that Mlcombine expects a particular type of object for the
vars argument and it is not getting that type (and not checking for
the correct type).

>
>
>
> Thanks
> Beth
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From shiazy at gmail.com  Sat Mar  3 17:45:31 2007
From: shiazy at gmail.com (Shiazy)
Date: Sat, 03 Mar 2007 17:45:31 +0100
Subject: [R] How to convert List object to function arguments?
In-Reply-To: <45E99999.6090202@pburns.seanet.com>
References: <45E98705.7060705@gmail.com> <45E99999.6090202@pburns.seanet.com>
Message-ID: <45E9A62B.5010102@gmail.com>

Patrick Burns wrote:
> You might have found 'do.call' in S Poetry.
> 

I've taken a look just now.

Thank you so much!

-- Marco

> Patrick Burns
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Shiazy wrote:
> 
>> Dear R gurus,
>>
>> I have a function "goftests" that receives the following arguments:
>> * a vector "x" of data values;
>> * a distribution name "dist";
>> * the dots list ("...") containing a list a parameters to pass to CDF 
>> function;
>> and calls several goodness-of-fit tests on the given data values 
>> against the given distribution.
>> That is:
>>
>> ##### BEGIN CODE SNIP #####
>>
>> # Covert a distribution name to the related CDF function name
>> distname2cdfname <- function( dist )
>> {
>>   if ( dist == "beta" ) { return( "pbeta" ); }
>>   if ( dist == "cauchy" ) { return( "pcauchy" ); }
>>
>>   # many other and personalized (e.g. pareto, ...) distributions ...
>>
>>   return( "" ); # fall-back case
>> }
>>
>> # Performs some GoF tests (KS, Chi-Square, Anderson-Darling,...)
>> # (the "..." list contains parameters for CDF function)
>> goftests <- function( x, dist, ... )
>> {
>>   cdfname <- distname2cdfname( dist );
>>   res$ks <- ks.test( x, cdfname, ... );
>>   # res$chisq <- ...
>>   # res$anddarl <- ...
>>
>>   return( res )
>> };
>>
>> ##### END CODE SNIP #####
>>
>> So the problem is passing "..." to CDF function (for instance, see 
>> ks.test above) in the "right form". The "..." is passed (to 
>> "goftests") as a list object and it should be converted to an 
>> "argument list".
>> For instance:
>>
>> ##### BEGIN CODE SNIP #####
>>
>> parms <- list( shape1 = 2, shape2 = 5 );
>> goftests( x, "beta", parms[["shape1"]], parms[["shape2"]] ); # Works!
>> goftests( x, "beta", parms ) # Don't Works!
>>
>> parms <- list( aNum = 5, aVector = c(1,2,3), aMatrix = 
>> matrix(c(4,5,6,7,8,9),nrow=2,byrow=T) );
>> goftests( x, "my-special-distr", parms[["aVector"]], 
>> parms[["aMatrix"]], parms[["aNumber"]] ); # Works!
>> goftests( x, "my-special-distr", parms ) # Don't Works!
>>
>> ##### END CODE SNIP #####
>>
>> Obviously I have to use the "don't work" form.
>>
>> How can I do this in R (if possible)?
>>
>> Thank you very much in advance!
>>
>> Best regards,
>>
>> -- Marco
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>  
>>
>


From kubovy at virginia.edu  Sat Mar  3 20:29:35 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sat, 3 Mar 2007 14:29:35 -0500
Subject: [R] Help with paste()
Message-ID: <C0E8EEA6-F36B-4818-9B7E-D924B859389E@virginia.edu>

Dear r-helpers,

Could you please tell me what's missing:
rbind(paste('txt.est',1:24, sep = ''))
txt.est1, ... txt.est24 are vectors that I wish to rbind.
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From christos at nuverabio.com  Sat Mar  3 20:41:24 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Sat, 3 Mar 2007 14:41:24 -0500
Subject: [R] Help with paste()
In-Reply-To: <C0E8EEA6-F36B-4818-9B7E-D924B859389E@virginia.edu>
References: <C0E8EEA6-F36B-4818-9B7E-D924B859389E@virginia.edu>
Message-ID: <000d01c75dcb$ee84e6e0$0202a8c0@headquarters.silicoinsights>

What do you want to do with rbind?
paste produces a single vector of 24 character-valued elements.
If you want a column vector, you could do that by

x <- paste('txt.est',1:24, sep = '')
matrix(x, ncol=1)

-Christos

Christos Hatzis, Ph.D.
Nuvera Biosciences, Inc.
400 West Cummings Park
Suite 5350
Woburn, MA 01801
Tel: 781-938-3830
www.nuverabio.com
 
  

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Michael Kubovy
> Sent: Saturday, March 03, 2007 2:30 PM
> To: r-help at stat.math.ethz.ch list
> Subject: [R] Help with paste()
> 
> Dear r-helpers,
> 
> Could you please tell me what's missing:
> rbind(paste('txt.est',1:24, sep = ''))
> txt.est1, ... txt.est24 are vectors that I wish to rbind.
> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
> Parcels:    Room 102        Gilmer Hall
>          McCormick Road    Charlottesville, VA 22903
> Office:    B011    +1-434-982-4729
> Lab:        B019    +1-434-982-4751
> Fax:        +1-434-982-4766
> WWW:    http://www.people.virginia.edu/~mk9y/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From skiadas at hanover.edu  Sat Mar  3 20:44:49 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Sat, 3 Mar 2007 14:44:49 -0500
Subject: [R] Help with paste()
In-Reply-To: <C0E8EEA6-F36B-4818-9B7E-D924B859389E@virginia.edu>
References: <C0E8EEA6-F36B-4818-9B7E-D924B859389E@virginia.edu>
Message-ID: <61098A2A-CA85-4DB8-B1B3-746994A505DB@hanover.edu>


On Mar 3, 2007, at 2:29 PM, Michael Kubovy wrote:

> Dear r-helpers,
>
> Could you please tell me what's missing:
> rbind(paste('txt.est',1:24, sep = ''))
> txt.est1, ... txt.est24 are vectors that I wish to rbind.

the paste call just returns a vector of the strings "txt.est1" and so  
on. Then you tell it to rbind this vector with nothing else.

You might want to try something like this, though I hope someone else  
comes with a better solution:

cmd <- paste("rbind(",paste('txt.est',1:24, sep = '',collapse=", "),  
")", sep="")
eval(parse(text=cmd))


Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From ggrothendieck at gmail.com  Sat Mar  3 20:53:14 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 3 Mar 2007 14:53:14 -0500
Subject: [R] R software to place points on Yahoo maps
Message-ID: <971536df0703031153g448e3f03n15c13b5ba20d824a@mail.gmail.com>

Is there any R software that create an image from Yahoo maps together
with points of known UTM coordinates (or lat/long marked?  Note that
my region of interest is not covered in sufficient detail by Google maps.
It actually does not have to be Yahoo maps as long as it has sufficient
coverage of my region but that's the one I have found with coverage of
my region.  The scale I am interested in is a city block.

Thanks.


From kubovy at virginia.edu  Sat Mar  3 20:53:44 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sat, 3 Mar 2007 14:53:44 -0500
Subject: [R] How to override ordering of panels in xyplot()
Message-ID: <57C54374-D6B3-4DD7-88B9-4D2971A454DA@virginia.edu>

Dear r-helpers,

I'm conditioning an xyplot on a variable whose levels are'low',  
'med', 'high'. How do I override the alphabetical ordering for the  
panels of the plot?
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From marc_schwartz at comcast.net  Sat Mar  3 20:58:52 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 03 Mar 2007 13:58:52 -0600
Subject: [R] Help with paste()
In-Reply-To: <C0E8EEA6-F36B-4818-9B7E-D924B859389E@virginia.edu>
References: <C0E8EEA6-F36B-4818-9B7E-D924B859389E@virginia.edu>
Message-ID: <1172951932.10294.12.camel@localhost.localdomain>

On Sat, 2007-03-03 at 14:29 -0500, Michael Kubovy wrote:
> Dear r-helpers,
> 
> Could you please tell me what's missing:
> rbind(paste('txt.est',1:24, sep = ''))
> txt.est1, ... txt.est24 are vectors that I wish to rbind.


Micheal,

Try this, presuming that each vector is the same length:

# Create three vectors 

txt.est1 <- letters
txt.est2 <- letters
txt.est3 <- letters


MAT <- t(sapply(paste('txt.est',1:3, sep = ''), get))

> MAT
         [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
txt.est1 "a"  "b"  "c"  "d"  "e"  "f"  "g"  "h"  "i"  "j"   "k"   "l"  
txt.est2 "a"  "b"  "c"  "d"  "e"  "f"  "g"  "h"  "i"  "j"   "k"   "l"  
txt.est3 "a"  "b"  "c"  "d"  "e"  "f"  "g"  "h"  "i"  "j"   "k"   "l"  
         [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22]
txt.est1 "m"   "n"   "o"   "p"   "q"   "r"   "s"   "t"   "u"   "v"  
txt.est2 "m"   "n"   "o"   "p"   "q"   "r"   "s"   "t"   "u"   "v"  
txt.est3 "m"   "n"   "o"   "p"   "q"   "r"   "s"   "t"   "u"   "v"  
         [,23] [,24] [,25] [,26]
txt.est1 "w"   "x"   "y"   "z"  
txt.est2 "w"   "x"   "y"   "z"  
txt.est3 "w"   "x"   "y"   "z" 


So in your case:

MAT <- t(sapply(paste('txt.est',1:24, sep = ''), get))

See ?get

You need to use get() in order to 'get' the actual vectors using the
vector names created in paste()

HTH,

Marc Schwartz


From deepayan.sarkar at gmail.com  Sat Mar  3 21:19:20 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sat, 3 Mar 2007 12:19:20 -0800
Subject: [R] How to override ordering of panels in xyplot()
In-Reply-To: <57C54374-D6B3-4DD7-88B9-4D2971A454DA@virginia.edu>
References: <57C54374-D6B3-4DD7-88B9-4D2971A454DA@virginia.edu>
Message-ID: <eb555e660703031219j66794bafnaec7ed88640ad082@mail.gmail.com>

On 3/3/07, Michael Kubovy <kubovy at virginia.edu> wrote:
> Dear r-helpers,
>
> I'm conditioning an xyplot on a variable whose levels are'low',
> 'med', 'high'. How do I override the alphabetical ordering for the
> panels of the plot?

This has less to do with xyplot and more to do with the default of the
'levels' argument in the factor() function. Just make sure the levels
are in the right order in your data when xyplot is called.

Deepayan


From matthias at mediabar.info  Sat Mar  3 22:18:18 2007
From: matthias at mediabar.info (Matthias Bannert)
Date: Sat, 3 Mar 2007 22:18:18 +0100
Subject: [R] function doesnt return/create object
Message-ID: <B6BBAE7F-3259-4575-8301-FAAC1C2C4D16@mediabar.info>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070303/002c3f76/attachment.pl 

From bunny at lautloscrew.com  Sat Mar  3 22:28:39 2007
From: bunny at lautloscrew.com (bunny , lautloscrew.com)
Date: Sat, 3 Mar 2007 22:28:39 +0100
Subject: [R] apply ? function doesnt create object
Message-ID: <7D888384-C66D-4F8E-A730-2E186511B986@lautloscrew.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070303/f894c16a/attachment.pl 

From cressonim at nhlbi.nih.gov  Sat Mar  3 22:30:17 2007
From: cressonim at nhlbi.nih.gov (Cressoni, Massimo (NIH/NHLBI) [F])
Date: Sat, 3 Mar 2007 16:30:17 -0500
Subject: [R] Sigmoidal fitting
Message-ID: <B0F504209244B14EA9A4C1DFB599B9224FFE1D@NIHCESMLBX6.nih.gov>


I am trying to write a function that fits a sigmoid given a X and Y vector guessing the start parameters.
I use nls. What I did (enclosed) seems to work well with many data points but if I want to fit small
vectors like :

pressure <- c(5,15,9,35,45)
gas <- c(1000,2000,3000,4000,5000)

it do not work. The help page says that it do no not work on zero residual data.

Massimo Cressoni


From cressonim at nhlbi.nih.gov  Sat Mar  3 22:46:56 2007
From: cressonim at nhlbi.nih.gov (Cressoni, Massimo (NIH/NHLBI) [F])
Date: Sat, 3 Mar 2007 16:46:56 -0500
Subject: [R] Fitting a sigmoid
Message-ID: <B0F504209244B14EA9A4C1DFB599B9224FFE1E@NIHCESMLBX6.nih.gov>


I am trying to write a function that fits a sigmoid given a X and Y vector guessing the start parameters.
I use nls. What I did (enclosed) seems to work well with many data points but if I want to fit small
vectors like :

pressure <- c(5,15,9,35,45)
gas <- c(1000,2000,3000,4000,5000)

it do not work. The help page says that it do no not work on zero residual data.

Massimo Cressoni

PS 
The functions I wrote are the following :


x25 = function(y_range)
{
 x25r = min(y_range) + (max(y_range) - min(y_range))/4
 x25r
}

x75 = function(y_range)
{
 x75r = min(y_range) + 3*(max(y_range) - min(y_range))/4
 x75r
}Get

GetStartSigmoidValues = function(pointsY)
{
 start_a = max(pointsY) - min(pointsY)
 start_x0 = min(pointsY) + (max(pointsY) - min(pointsY))/2
 start_b = (x75(pointsY) - x25(pointsY))/4
 start_y0 = min(pointsY)

 start_list = list(a=start_a,x0=start_x0,b=start_b,y0=start_y0)
 start_list
}

FitSigmoid = function(pointsX,pointsY)
{
 start_list = GetStartSigmoidValues(pointsY)
 fitting = nls(pointsY ~ y0+a/(1+exp(-(pointsX - x0)/b)),start=start_list)
 fitting
}


From skiadas at hanover.edu  Sat Mar  3 22:53:57 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Sat, 3 Mar 2007 16:53:57 -0500
Subject: [R] apply ? function doesnt create object
In-Reply-To: <7D888384-C66D-4F8E-A730-2E186511B986@lautloscrew.com>
References: <7D888384-C66D-4F8E-A730-2E186511B986@lautloscrew.com>
Message-ID: <0E226223-FE67-43D3-B1CA-DC15057FFB61@hanover.edu>

On Mar 3, 2007, at 4:28 PM, bunny , lautloscrew.com wrote:

Please use <- for assignments instead of = :

> getans = function(x=qids,bnr=1,type="block")
> 	{
> #generate name of matrix
> matnam=paste("ans",type,as.character(bnr),sep="")
>
> #display result matrix
> show(assign(matnam,matrix(as.numeric(as.matrix(allans[(allans[, 3] %in
> % x), , drop = FALSE])),ncol=dim(allans)[2])))

You are assigning things twice here.

> #create result matrix
> assign(matnam,matrix(as.numeric(as.matrix(allans[(allans[, 3] %in%
> x), , drop = FALSE])),ncol=dim(allans)[2]))

The documentation for assign makes it pretty clear that the  
assignment happens by default in the current environment, so it will  
be local to the function unless you alter the call. The description  
there, along with the examples, and a study of environments, should  
provide you with the answer.

> #print info
> cat("the matrix",matnam,"contains answers to",type,as.character 
> (bnr))		
> 	
> }

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From alex at mofo.ca  Sat Mar  3 23:22:27 2007
From: alex at mofo.ca (Alex Couture-Beil)
Date: Sat, 03 Mar 2007 14:22:27 -0800
Subject: [R] tcl/tk printing to console is far too slow
Message-ID: <45E9F523.9080805@mofo.ca>

Hello - I am finding that printing to the console from a function that 
is invoked from tcl/tk (i.e. pushing a button) is really slow.
Consider the following example.

doStuff <- function() {
    source(file.path(system.file(package = "base"), "demo", 
"is.things.R"), echo=TRUE)
}

tt <- tktoplevel()
button.widget <- tkbutton(tt,text="Do Stuff", command=doStuff)
tkpack(button.widget)


I find that this is significantly slower than if I ran source(...) 
directly from the console prompt. In the back of my mind, I am thinking 
this might be some effect related to multi-threading and the 
locking/unlocking of a mutex - am I right?

Any help to speed this up would be appreciated.

Thanks,
Alex Couture-Beil

PS: I am running this test with R 2.4.1 on WinXP


From jholtman at gmail.com  Sun Mar  4 01:45:36 2007
From: jholtman at gmail.com (jim holtman)
Date: Sat, 3 Mar 2007 19:45:36 -0500
Subject: [R] function doesnt return/create object
In-Reply-To: <B6BBAE7F-3259-4575-8301-FAAC1C2C4D16@mediabar.info>
References: <B6BBAE7F-3259-4575-8301-FAAC1C2C4D16@mediabar.info>
Message-ID: <644e1f320703031645k5d6dbdbbkbc3bc1a14483976d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070303/da1a45b3/attachment.pl 

From aiminy at iastate.edu  Sun Mar  4 08:22:06 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Sun, 04 Mar 2007 01:22:06 -0600
Subject: [R] lattice histogram
Message-ID: <6.1.2.0.2.20070304012030.01dc5d58@aiminy.mail.iastate.edu>

How to add mean,sd, number of observation in each panel for lattice histogram?

Aimin


From renaud.lancelot at gmail.com  Sun Mar  4 09:54:54 2007
From: renaud.lancelot at gmail.com (Renaud Lancelot)
Date: Sun, 4 Mar 2007 09:54:54 +0100
Subject: [R] lattice histogram
In-Reply-To: <6.1.2.0.2.20070304012030.01dc5d58@aiminy.mail.iastate.edu>
References: <6.1.2.0.2.20070304012030.01dc5d58@aiminy.mail.iastate.edu>
Message-ID: <c2ee56800703040054i141786edo49a4320071d0d9b2@mail.gmail.com>

Here is an example using the grid package to annotate the graphs:

library(lattice)
library(grid)
resp  <- rnorm(200)
group <- sample(c("G1", "G2", "G3"), replace = TRUE, size = 100)
histogram(~ resp | group,
  panel = function(x, ...){
    std <- round(sd(x), 2)
    n <- length(x)
    m <- round(mean(x), 2)
    panel.histogram(x, ...)
    x1 <- unit(1, "npc") - unit(2, "mm")
    y1 <- unit(1, "npc") - unit(2, "mm")
    grid.text(label = bquote(n == .(n)), x = x1, y = y1, just = "right")
    grid.text(label = bquote(hat(m) == .(m)), x = x1, y = y1 - unit(1,
"lines"), just = "right")
    grid.text(label = bquote(hat(s) == .(std)), x = x1, y = y1 -
unit(2, "lines"), just = "right")
    })
Best,

Renaud

2007/3/4, Aimin Yan <aiminy at iastate.edu>:
> How to add mean,sd, number of observation in each panel for lattice histogram?
>
> Aimin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Renaud LANCELOT
D?partement Syst?mes Biologiques du CIRAD
CIRAD, Biological Systems Department

Campus International de Baillarguet
TA 30 / B
F34398 Montpellier
Tel   +33 (0)4 67 59 37 17
Secr. +33 (0)4 67 59 37 37
Fax   +33 (0)4 67 59 37 95


From mdsumner at utas.edu.au  Sun Mar  4 11:47:47 2007
From: mdsumner at utas.edu.au (Michael Sumner)
Date: Sun, 04 Mar 2007 21:47:47 +1100
Subject: [R] R software to place points on Yahoo maps
Message-ID: <45EAA3D3.3090802@utas.edu.au>

> Is there any R software that create an image from Yahoo maps together 
> with points of known UTM coordinates (or lat/long marked? Note that my 
> region of interest is not covered in sufficient detail by Google maps. 
> It actually does not have to be Yahoo maps as long as it has 
> sufficient coverage of my region but that's the one I have found with 
> coverage of my region. The scale I am interested in is a city block. 
> Thanks.

rgdal in combination with sp. You'll need to georegister the image, as 
the yahoo maps interface is unlikely to provide a format supporting this 
metadata, but you can do that easily with two reference coordinates.

The workflow might go like this (untested).  You could continue the 
query on R-Sig-Geo. Hope this helps.

There are freely available tools for reading image data directly from 
Yahoo, Google and Virtual Earth in georeferenced
versions, but I've only used them via commercial GIS. If you can 
describe the map you want I'd be interested in
exploring that option.

Cheers, Mike.

library(rgdal)
im <- readGDAL("yahooMap.png")  ## a file saved from Yahoo Maps

## create index for RGB colours
col <- SGDF2PCT(im)  ## im is a spatialGridDataFrame with 3 bands

im$ind <- col$idx  ## add the colour index to the data frame

image(im, "ind", col = col$ct)

## BUT you won't be able to plot UTM on this yet

## you'll need to create a new GridTopology with appropriate 
cellcentre.offset and cellsize

ogt <- getGridTopology(im)
. . . find values for offsets and pixel size

gt <- GridTopology(cc.offset, csize,  gt at cells.dim)

## recreate the object
im <- SpatialGridDataFrame(gt, im at data)

image(im, "idx", col = col$ct)
points(utm.x, utm.y)  ## etc.


From tura at centroin.com.br  Sun Mar  4 14:55:09 2007
From: tura at centroin.com.br (Bernardo Rangel Tura)
Date: Sun, 04 Mar 2007 10:55:09 -0300
Subject: [R] Sigmoidal fitting
In-Reply-To: <B0F504209244B14EA9A4C1DFB599B9224FFE1D@NIHCESMLBX6.nih.gov>
References: <B0F504209244B14EA9A4C1DFB599B9224FFE1D@NIHCESMLBX6.nih.gov>
Message-ID: <1173016509.15254.14.camel@Thux>

On Sat, 2007-03-03 at 16:30 -0500, Cressoni, Massimo (NIH/NHLBI) [F]
wrote:
> I am trying to write a function that fits a sigmoid given a X and Y vector guessing the start parameters.
> I use nls. What I did (enclosed) seems to work well with many data points but if I want to fit small
> vectors like :
> 
> pressure <- c(5,15,9,35,45)
> gas <- c(1000,2000,3000,4000,5000)
> 
> it do not work. The help page says that it do no not work on zero residual data.
> 
> Massimo Cressoni

Hi Massimo

I think which this script solve your question
------------------------
pressure <- c(5,9,15,35,45)
gas <- c(1000,2000,3000,4000,5000)
plot(gas,pressure)
model.1 <- nls(pressure ~ SSlogis(gas, ASym, xmid, scal)))
coef.sig<-coef(summary(model.1))[,1]
est.p<-coef.sig[1]/(1+exp((coef.sig[2]-gas)/coef.sig[3]))
points(gas,est.p,col=2)
------------------------

If you need more help just need a mail


-- 
Bernardo Rangel Tura,M.D.,Ph.D
National Institute of Cardiology
Rio de Janeiro - Brazil


From fisk at bowdoin.edu  Sun Mar  4 14:57:53 2007
From: fisk at bowdoin.edu (steve)
Date: Sun, 04 Mar 2007 08:57:53 -0500
Subject: [R] lattice histogram
In-Reply-To: <c2ee56800703040054i141786edo49a4320071d0d9b2@mail.gmail.com>
References: <6.1.2.0.2.20070304012030.01dc5d58@aiminy.mail.iastate.edu>
	<c2ee56800703040054i141786edo49a4320071d0d9b2@mail.gmail.com>
Message-ID: <esej92$na9$1@sea.gmane.org>

Hi,

When I tried this the groups had sizes 36 30 34 as expected, but
the annotations "n = " were 68 72 60 - twice as large. I don't 
understand why.

Steve

Renaud Lancelot wrote:
> Here is an example using the grid package to annotate the graphs:
> 
> library(lattice)
> library(grid)
> resp  <- rnorm(200)
> group <- sample(c("G1", "G2", "G3"), replace = TRUE, size = 100)
> histogram(~ resp | group,
>   panel = function(x, ...){
>     std <- round(sd(x), 2)
>     n <- length(x)
>     m <- round(mean(x), 2)
>     panel.histogram(x, ...)
>     x1 <- unit(1, "npc") - unit(2, "mm")
>     y1 <- unit(1, "npc") - unit(2, "mm")
>     grid.text(label = bquote(n == .(n)), x = x1, y = y1, just = "right")
>     grid.text(label = bquote(hat(m) == .(m)), x = x1, y = y1 - unit(1,
> "lines"), just = "right")
>     grid.text(label = bquote(hat(s) == .(std)), x = x1, y = y1 -
> unit(2, "lines"), just = "right")
>     })


From F.MENDIBURU at CGIAR.ORG  Sun Mar  4 15:14:43 2007
From: F.MENDIBURU at CGIAR.ORG (Mendiburu, Felipe (CIP))
Date: Sun, 4 Mar 2007 09:14:43 -0500
Subject: [R] lattice histogram
Message-ID: <B7B34444ECA41A41AC47DABA057CE7A2014069D6@webmail.cip.cgiar.org>

Steve,

it says resp  <- rnorm(200) must be resp  <- rnorm(100) 

regards

Felipe

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of steve
Sent: Sunday, March 04, 2007 8:58 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] lattice histogram


Hi,

When I tried this the groups had sizes 36 30 34 as expected, but
the annotations "n = " were 68 72 60 - twice as large. I don't 
understand why.

Steve

Renaud Lancelot wrote:
> Here is an example using the grid package to annotate the graphs:
> 
> library(lattice)
> library(grid)
> resp  <- rnorm(200)
> group <- sample(c("G1", "G2", "G3"), replace = TRUE, size = 100)
> histogram(~ resp | group,
>   panel = function(x, ...){
>     std <- round(sd(x), 2)
>     n <- length(x)
>     m <- round(mean(x), 2)
>     panel.histogram(x, ...)
>     x1 <- unit(1, "npc") - unit(2, "mm")
>     y1 <- unit(1, "npc") - unit(2, "mm")
>     grid.text(label = bquote(n == .(n)), x = x1, y = y1, just = "right")
>     grid.text(label = bquote(hat(m) == .(m)), x = x1, y = y1 - unit(1,
> "lines"), just = "right")
>     grid.text(label = bquote(hat(s) == .(std)), x = x1, y = y1 -
> unit(2, "lines"), just = "right")
>     })

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From gomes at eva.mpg.de  Sun Mar  4 17:32:35 2007
From: gomes at eva.mpg.de (Cristina Gomes)
Date: Sun, 04 Mar 2007 17:32:35 +0100
Subject: [R] residuals in lme4 package
Message-ID: <45EAF4A3.9080600@eva.mpg.de>

Hi,
I have not been able to calculate residuals in the lme4 package. I've 
been trying the resid() function after I ran a GLMM with the lmer() 
function, but I get an error message that says "residuals are not 
inserted yet". I looked it up in the "help" history and I realized that 
several people have had this problem in the past, related to some bug in 
this function and fixed it usually just by upgrading the lme4 package or 
Matrix package. I did, but it didn't work.
Does anybody have any idea whats going on? Or could you suggest me 
another way of getting and plotting the residuals?
Thanks a lot,
Cristina.
-- 
Cristina Gomes Parisca
Max Planck Institute for Evolutionary Anthropology
Primatology
Deutscher Platz 6
D-04103 Leipzig, Germany
Phone: +49 341 3550 220
Fax: +49 341 3550 299
E-mail: gomes at eva.mpg.de


From ligges at statistik.uni-dortmund.de  Sun Mar  4 17:35:35 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 04 Mar 2007 17:35:35 +0100
Subject: [R] package installation for windows vista
In-Reply-To: <3379b4a00703021725v251454d2t706a7608413c7891@mail.gmail.com>
References: <3379b4a00703021725v251454d2t706a7608413c7891@mail.gmail.com>
Message-ID: <45EAF557.8040205@statistik.uni-dortmund.de>

See, e.g.,

RSiteSearch("Vista Package")


Uwe Ligges



Jungpin Wu wrote:
> Hi guys,
> 
> How to install selected packages for R 2.4.1 under windows vista?
> 
> Thanks a lot.
> 
> JP
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tkobayas at indiana.edu  Sun Mar  4 18:42:14 2007
From: tkobayas at indiana.edu (Takatsugu Kobayashi)
Date: Sun, 04 Mar 2007 12:42:14 -0500
Subject: [R] Newbie questions about "Wireframe"
Message-ID: <45EB04F6.8020102@indiana.edu>

Rusers:

I have been looking into lattice manual to see how I can delete a plot 
frame and a box frame. I just want to show x label, y label, and the 
actual surface only.
\Is this something I should define in axis.default? And how can I change 
the view perspective like persp(phi=, theta=)? I also added "colorkey" 
but the colors were not the same as the ones used in the actual 
surface.... How can I synchronize?

What I did was

wireframe(grid$z, shade = 
TRUE,aspect=c(1,0.3),light.source=c(20,0,15),xlab="X",zoom=1,contour=T,par.box=F,axs=F)

Thanks

Taka


From ThadenJohnJ at uams.edu  Sun Mar  4 19:31:08 2007
From: ThadenJohnJ at uams.edu (Thaden, John J)
Date: Sun, 4 Mar 2007 12:31:08 -0600
Subject: [R] Scoping issue?
References: <mailman.7.1172919606.16782.r-help@stat.math.ethz.ch>
Message-ID: <B1614B0C915A654A9C29BB71DA80E0DD01773D1B@MAIL2.ad.uams.edu>

Hello, The code below is supposed to be a wrapper for matplot to 
do columnwise visible comparison of several matrices, but I'm 
doing something wrong because I can't access an argument called
'colnum'.  I'd be most grateful for some insight.

Thanks,
John Thaden
Little Rock, AR
################################
# mmatplot is a matplot wrapper to compare the same column of 
# several matrices. Arg y is either a list of matrices with 
# equal number of rows, or an array. The scalar n gives the 
# column of each matrix or array slab to plot. par values and
# matplot args are accepted, e.g., ylog.  mmatplot is intended
# to be mapply-compatible to test multiple columns.

mmatplot <- function(colnum, x, y, ...){
  switch(class(y),
    array = y <- y[, colnum, ], 
    list = y <- sapply(X = y, FUN = subset, select = colnum))
  stopifnot(is.matrix(y))
  matplot(x, y, ...)
}

#This is just a tester function
mmatplotTest <- function(){
  oldmf <- par("mfrow")
  par(mfrow = c(2,3))
  A <- array(data = rnorm(90), dim = c(10, 3, 3))
  L <- list(A[, , 1], A[, , 2], A[, , 3])

  # The 'main' argument below throws the error, but if
  # commented out, another error crops up due to 'colnum'.
  # Test with class(y) == "array"
  mapply(X = 1:ncol(A), FUN = mmatplot, x = 1:nrow(A), y = A,
                 main = paste("Array input, column", colnum))
  # Test with class(y) == "list"
  mapply(1:ncol(L[[1]]), mmatplot, x = 1:nrow(L[[1]]), y = L,
                 main = paste("List input, column", colnum))
  par(mfrow = oldmf) 
}

#Run the test
mmatplotTest()


From renaud.lancelot at gmail.com  Sun Mar  4 20:58:38 2007
From: renaud.lancelot at gmail.com (Renaud Lancelot)
Date: Sun, 4 Mar 2007 20:58:38 +0100
Subject: [R] residuals in lme4 package
In-Reply-To: <45EAF4A3.9080600@eva.mpg.de>
References: <45EAF4A3.9080600@eva.mpg.de>
Message-ID: <c2ee56800703041158s4d99b5dat272b60b9ddb2204f@mail.gmail.com>

resid is not yet implemented for GLMMs.

Renaud

2007/3/4, Cristina Gomes <gomes at eva.mpg.de>:
> Hi,
> I have not been able to calculate residuals in the lme4 package. I've
> been trying the resid() function after I ran a GLMM with the lmer()
> function, but I get an error message that says "residuals are not
> inserted yet". I looked it up in the "help" history and I realized that
> several people have had this problem in the past, related to some bug in
> this function and fixed it usually just by upgrading the lme4 package or
> Matrix package. I did, but it didn't work.
> Does anybody have any idea whats going on? Or could you suggest me
> another way of getting and plotting the residuals?
> Thanks a lot,
> Cristina.
> --
> Cristina Gomes Parisca
> Max Planck Institute for Evolutionary Anthropology
> Primatology
> Deutscher Platz 6
> D-04103 Leipzig, Germany
> Phone: +49 341 3550 220
> Fax: +49 341 3550 299
> E-mail: gomes at eva.mpg.de
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Renaud LANCELOT
D?partement Syst?mes Biologiques du CIRAD
CIRAD, Biological Systems Department

Campus International de Baillarguet
TA 30 / B
F34398 Montpellier
Tel   +33 (0)4 67 59 37 17
Secr. +33 (0)4 67 59 37 37
Fax   +33 (0)4 67 59 37 95


From slacey at umich.edu  Sun Mar  4 23:26:21 2007
From: slacey at umich.edu (Steven Lacey)
Date: Sun, 4 Mar 2007 17:26:21 -0500
Subject: [R] factor analysis and pattern matrix
Message-ID: <000301c75eac$235fb850$6400a8c0@zephyr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070304/1f433e73/attachment.pl 

From qyang at bu.edu  Sun Mar  4 23:49:04 2007
From: qyang at bu.edu (Qiong Yang)
Date: Sun, 04 Mar 2007 17:49:04 -0500
Subject: [R] plot groupedData in nlme
Message-ID: <45EB4CE0.6090800@bu.edu>

Hi,

Does anyone know how to make the color of the lines all black when 
plotting groupedData with an outer factor:

For example,
library(nlme)
plot(Dialyzer, outer=~QB, key=F)

This generated colored curves in R.2.4.1. How to make all the curves black ?
(or how to alter the color (type) of lines for the nlme groupedData 
plotting function in general?)

Thanks
Qiong


From jholtman at gmail.com  Mon Mar  5 00:25:22 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 4 Mar 2007 18:25:22 -0500
Subject: [R] Scoping issue?
In-Reply-To: <B1614B0C915A654A9C29BB71DA80E0DD01773D1B@MAIL2.ad.uams.edu>
References: <mailman.7.1172919606.16782.r-help@stat.math.ethz.ch>
	<B1614B0C915A654A9C29BB71DA80E0DD01773D1B@MAIL2.ad.uams.edu>
Message-ID: <644e1f320703041525w5af344a5mea578b13f61f9014@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070304/06e22ca1/attachment.pl 

From herrdittmann at yahoo.co.uk  Mon Mar  5 00:26:13 2007
From: herrdittmann at yahoo.co.uk (Bernd Dittmann)
Date: Sun, 04 Mar 2007 23:26:13 +0000
Subject: [R]  - Nonparametric variance test
Message-ID: <45EB5595.400@yahoo.co.uk>

Hi useRs,

can a variance test for 2 non-normal samples be tested in R? Also, thus 
far I have not been able to find the Friedman two way analysis of variance.

For normal r.v., the var.test is available, but are there any tests 
available for non-normal samples?

Thanks!

Bernd


From g_smits at verizon.net  Mon Mar  5 00:47:41 2007
From: g_smits at verizon.net (Gerard Smits)
Date: Sun, 04 Mar 2007 15:47:41 -0800
Subject: [R] Trouble figuring messages from rcspline.plot
Message-ID: <7.0.1.0.2.20070304154601.00bffec0@verizon.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070304/4f0c9224/attachment.pl 

From renaud.lancelot at gmail.com  Mon Mar  5 01:02:51 2007
From: renaud.lancelot at gmail.com (Renaud Lancelot)
Date: Mon, 5 Mar 2007 01:02:51 +0100
Subject: [R] plot groupedData in nlme
In-Reply-To: <45EB4CE0.6090800@bu.edu>
References: <45EB4CE0.6090800@bu.edu>
Message-ID: <c2ee56800703041602vb39127am3808501afd865480@mail.gmail.com>

library(nlme)
trellis.par.set(list(superpose.line = list(col = "black")))
plot(Dialyzer, outer = ~QB, key = FALSE, col = "black")

I prefer to use directly xyplot (package lattice) for a finer control.

Best,

Renaud


2007/3/4, Qiong Yang <qyang at bu.edu>:
> Hi,
>
> Does anyone know how to make the color of the lines all black when
> plotting groupedData with an outer factor:
>
> For example,
> library(nlme)
> plot(Dialyzer, outer=~QB, key=F)
>
> This generated colored curves in R.2.4.1. How to make all the curves black ?
> (or how to alter the color (type) of lines for the nlme groupedData
> plotting function in general?)
>
> Thanks
> Qiong
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Renaud LANCELOT
D?partement Syst?mes Biologiques du CIRAD
CIRAD, Biological Systems Department

Campus International de Baillarguet
TA 30 / B
F34398 Montpellier
Tel   +33 (0)4 67 59 37 17
Secr. +33 (0)4 67 59 37 37
Fax   +33 (0)4 67 59 37 95


From ted.harding at nessie.mcc.ac.uk  Mon Mar  5 01:49:59 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 05 Mar 2007 00:49:59 -0000 (GMT)
Subject: [R] - Nonparametric variance test
In-Reply-To: <45EB5595.400@yahoo.co.uk>
Message-ID: <XFMail.070305004959.ted.harding@nessie.mcc.ac.uk>

On 04-Mar-07 Bernd Dittmann wrote:
> Hi useRs,
> 
> can a variance test for 2 non-normal samples be tested in R? Also, thus
> far I have not been able to find the Friedman two way analysis of
> variance.
> 
> For normal r.v., the var.test is available, but are there any tests 
> available for non-normal samples?
> 
> Thanks!
> 
> Bernd

If you are seeking to test whether two samples have equal "variances",
you might like to consider Tukey's "hijack" of the Mann-Whitney test.

First, relocate one sample so that both have the same median (NOTE
that there may be an implicit assumption here).

Next, pool the samples (retaining identity), just as in Mann-Whitney.
Suppose the result for two samples X and Y (in increasing order) looks
like

  X X Y X Y X X Y Y Y X X Y X Y Y Y Y X Y X X Y X X X  [**]

  A B C D E F G H I J K L M N O P Q R S T U V W X Y Z

where the second row of letters is used as labels.

Then read off the identities of the elements alternately from the
two ends of [**], from the outside in:

  A Z B Y C X D W E V F U G T H S I R J Q K P L O M N

leading to

  X X X X Y X X Y Y X X X X Y Y X Y Y Y Y X Y X Y Y X

and then do a Wilcoxon test (equivalent to Mann-Whitney) on the
result.

Here you may as well allocate the final sequence of X and Y terms
the values 1,2,3,...,26 from left to right:

  X
  [1]  1  2  3  4  6  7 10 11 12 13 16 21 23 26
  Y
  [1]  5  8  9 14 15 17 18 19 20 22 24 25


and then wilcox.test() gives the result

  wilcox.test(x=X,Y)
        Wilcoxon rank sum test
  data:  X and Y

  W = 50, p-value = 0.08493
  alternative hypothesis: true mu is not equal to 0 


What this is really testing is whether the X values lie closer
to, or further from, their median than the Y values do relative
to their median. Using the default two-sided alternative, the
P-value is then 0.08493; if you already expected that any difference
would be in the direction of Y closer to median than X, for instance,
then

  wilcox.test(X,Y,alternative="less")
        Wilcoxon rank sum test
  data:  X and Y 

  W = 50, p-value = 0.04246
  alternative hypothesis: true mu is less than 0 

since "less" means that the X's tend to occur earlier in the
sequence [**] than the Y's (i.e. tend to occur nearer the ends).

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-Mar-07                                       Time: 00:49:43
------------------------------ XFMail ------------------------------


From bli1 at bcm.tmc.edu  Mon Mar  5 02:15:52 2007
From: bli1 at bcm.tmc.edu (Bingshan Li)
Date: Sun, 4 Mar 2007 19:15:52 -0600
Subject: [R] logistic regression on contingency table
Message-ID: <90076531-9050-4090-95A7-0CED6786FDDB@bcm.tmc.edu>

Hi folks,

I am wondering if there is a way in R to fit logistic regression on  
contingency table. If I have original data, I can transform the data  
into a design matrix and then call glm to fit the regression. But now  
I have a 2x3 contingency table with first row for response 0 and  
second row for response 1, and the columns are 3 levels of predictor  
variable. The 3 levels are not ordinal though and indicator variables  
would be more appreciate.

Thanks a lot!

Bingshan


From don_newport at mail2world.com  Mon Mar  5 04:48:40 2007
From: don_newport at mail2world.com (don_newport at mail2world.com)
Date: Mon, 05 Mar 2007 06:48:40 +0300
Subject: [R] Sehr Geehrter
Message-ID: <97E7B0AD.3E76C56@mail2world.com>

Sehr Geehrter,

 
Trauriges morgens empfing ich nicht Ihre Antwort. Auch wenn dieser Brief

Sie sicherlich ?berraschen wird, nehmen Sie sich bitte einen Moment Zeit um
ihn zu lesen. Es ist

sehr wichtig. Ich bin Steve Morgan und ich arbeite bei einer Finanzhaus in
den Niederlanden. 
Ich habe Ihre Adresse durch den International Web Directory Online
gefunden. W?hrend unseres letzten
Treffens und ?berpr?fung der Bankkontos hat meine Abteilung ein unt?tiges
Konto mit einer riesigen
Geldsumme, US$6,500,000.00(Sechs Million f?nfhundert tausend US Dollar)
gefunden, das
einem unseren gestorbenen 

Kunden geh?rt: Herr Williams aus England. Er ist gestorben und hat keine
Beg?nstigten hinterlassen. So dass die Fonds auf seinem Konto unt?tig
geblieben sind, ohne jeden
Anspruch oder Aktivit?t f?r einige Zeit schon.


Wegen unseren Finanzhaus vorschriften kann nur ein Ausl?nder als
n?chster Verwandten stehen und deshalb habe ich mich entschlossen Sie zu
kontaktieren, um mit Ihnen
zusammen zu arbeiten um diese unt?tigen Fonds zu reaktivieren. Und so jede
negative Entwicklung oder
sogar den endg?ltigen Verlust der Fonds abzuwenden. In Namen meiner
Kollegen suchen ich Ihre Erlaubnis
als n?chster Verwandte unseres verstorbenen Kunden zu stehen, so dass die
Fonds freigestellt
und auf ihr Konto ?berwiesen werden k?nnen. 

Sie w?rden zum n?chsten Verwandten des Beg?nstigten werden und die Fonds
werden 
in Ihre Verantwortung freigestellt werden. Wir d?rfen mit ausl?ndischen
Kontos nicht arbeiten, das k?nnte in der Zeit der ?berweisung auffallen.
Ich arbeite noch bei dieser
Finanzhaus, das ist der eigentliche Grund, dass ich eine zweite Partei oder
Person ben?tige, um mit mir  zu
arbeiten und Anforderungen als n?chster Verwandte zu schicken und auch um
ein Bankkonto bereit zu
stellen, oder eines bei einer neuen Bank zu er?ffnen, um die unt?tige Fonds
zu erhalten.

Am Ende der Transaktion werden Ihnen 40% Prozent zustehen, zur Seite
gelegt und 60% werden f?r meine Kollegen und mich sein.
Was ich von Ihnen verlange ist als n?chster Verwandte des Verstorbenen
zu stehen. Ich besitze alle notwendigen Dokumente um die Transaktion
erfolgreich zu verwirklichen.
Weitere Informationen werden Sie so bald ich Ihre positive Antwort bekomme
erhalten.

Ich schlage Ihnen vor so bald wie m?glich mir zu antworten.

Wir haben nicht viel Zeit diese ungl?ckliche Situation zu ?ndern und ich
bef?rchte, dass 
ohne Ihre Hilfe alles verloren gehen wird. Wegen der Vertraulichkeit bitte
ich Sie mir auf meine privaten 
Email Adresse mit folgenden Angaben zu antworten: Vollst?ndiger Name,
Adresse, Telefon- und Fax:nummer.

In Erwartung Ihrer Antwort, verbleibe ich,mit freundlichen Gruessen,

Steve Morgan.
morganstv154 at aim.com


From ThadenJohnJ at uams.edu  Mon Mar  5 04:36:13 2007
From: ThadenJohnJ at uams.edu (Thaden, John J)
Date: Sun, 4 Mar 2007 21:36:13 -0600
Subject: [R] Scoping issue?
References: <mailman.7.1172919606.16782.r-help@stat.math.ethz.ch>
	<B1614B0C915A654A9C29BB71DA80E0DD01773D1B@MAIL2.ad.uams.edu>
	<644e1f320703041525w5af344a5mea578b13f61f9014@mail.gmail.com>
Message-ID: <B1614B0C915A654A9C29BB71DA80E0DD01773D1C@MAIL2.ad.uams.edu>

Apparently you're right that colnum doesn't exist when it needs to
be evaluated, but why?  Why is 'paste' being evaluated so early? It is,
after all, the value of an argument ('main') of my mmatplot function
with colnum being another argument.  I thought arguments were lazy-loaded.
Does using mapply change the rules?

Is there a way (like mapply) to loop at some lower level rather than
Explicitly, in the R script, as in your suggestion?  For speed's sake?

Thanks.  -John


On Sunday Mar 4 2007, jim holtman <jholtman at gmail.com> replied 

> First of all, 'colnum' does not exist when the 'paste' is called.? 
> This probably does what you want:
?
> for (colnum in 1:ncol(A)){
> ??? mmatplot(colnum, 1:nrow(A), A, main=paste("Array input, column",
colnum))
> }

?
On 3/4/07, John Thaden <jthaden at uams.edu> wrote: 
Hello, the code below is supposed to be a wrapper for matplot to
do columnwise visible comparison of several matrices, but I'm 
doing something wrong because I can't access an argument called
'colnum'.??I'd be most grateful for some insight.

Thanks,
John
Little Rock, AR
################################ 
# mmatplot is a matplot wrapper to compare the same column of
# several matrices. Arg y is either a list of matrices with
# equal number of rows, or an array. The scalar n gives the
# column of each matrix or array slab to plot. par values and 
# matplot args are accepted, e.g., ylog.??mmatplot is intended
# to be mapply-compatible to test multiple columns.

mmatplot <- function(colnum, x, y, ...){
switch(class(y),
?? array = y <- y[, colnum, ], 
?? list = y <- sapply(X = y, FUN = subset, select = colnum))
stopifnot(is.matrix(y))
matplot(x, y, ...)
}

#This is just a tester function
mmatplotTest <- function(){
oldmf <- par("mfrow") 
par(mfrow = c(2,3))
A <- array(data = rnorm(90), dim = c(10, 3, 3))
L <- list(A[, , 1], A[, , 2], A[, , 3])

# The 'main' argument below throws the error, but if
# commented out, another error crops up due to 'colnum'. 
# Test with class(y) == "array"
mapply(X = 1:ncol(A), FUN = mmatplot, x = 1:nrow(A), y = A,
????????????????main = paste("Array input, column", colnum))
# Test with class(y) == "list" 
mapply(1:ncol(L[[1]]), mmatplot, x = 1:nrow(L[[1]]), y = L,
????????????????main = paste("List input, column", colnum))
par(mfrow = oldmf)
}

#Run the test
mmatplotTest()

______________________________________________ 
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve? 

Confidentiality Notice: This e-mail message, including any a...{{dropped}}


From aiminy at iastate.edu  Mon Mar  5 05:46:26 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Sun, 04 Mar 2007 22:46:26 -0600
Subject: [R] lattice histogram
In-Reply-To: <c2ee56800703040054i141786edo49a4320071d0d9b2@mail.gmail.co
 m>
References: <6.1.2.0.2.20070304012030.01dc5d58@aiminy.mail.iastate.edu>
	<c2ee56800703040054i141786edo49a4320071d0d9b2@mail.gmail.com>
Message-ID: <6.1.2.0.2.20070304223616.01cff338@aiminy.mail.iastate.edu>

thank you very much.  Your code almost solve my problem, but I have a 
further question.
In my data, there is no observation in some group, I want to label that 
panel by
n=0
hat(m)=NA
hat(s)=NA.

I try to modify your panel function, but it doesn't work out. Do you know 
how to add something to your panel
  function so that it can deal with some group that has 0 observation.

Aimin

At 02:54 AM 3/4/2007, Renaud Lancelot wrote:
>Here is an example using the grid package to annotate the graphs:
>
>library(lattice)
>library(grid)
>resp  <- rnorm(200)
>group <- sample(c("G1", "G2", "G3"), replace = TRUE, size = 100)
>histogram(~ resp | group,
>  panel = function(x, ...){
>    std <- round(sd(x), 2)
>    n <- length(x)
>    m <- round(mean(x), 2)
>    panel.histogram(x, ...)
>    x1 <- unit(1, "npc") - unit(2, "mm")
>    y1 <- unit(1, "npc") - unit(2, "mm")
>    grid.text(label = bquote(n == .(n)), x = x1, y = y1, just = "right")
>    grid.text(label = bquote(hat(m) == .(m)), x = x1, y = y1 - unit(1,
>"lines"), just = "right")
>    grid.text(label = bquote(hat(s) == .(std)), x = x1, y = y1 -
>unit(2, "lines"), just = "right")
>    })
>Best,
>
>Renaud
>
>2007/3/4, Aimin Yan <aiminy at iastate.edu>:
>>How to add mean,sd, number of observation in each panel for lattice 
>>histogram?
>>
>>Aimin
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>
>--
>Renaud LANCELOT
>D?partement Syst?mes Biologiques du CIRAD
>CIRAD, Biological Systems Department
>
>Campus International de Baillarguet
>TA 30 / B
>F34398 Montpellier
>Tel   +33 (0)4 67 59 37 17
>Secr. +33 (0)4 67 59 37 37
>Fax   +33 (0)4 67 59 37 95


From ggrothendieck at gmail.com  Mon Mar  5 08:48:47 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 5 Mar 2007 02:48:47 -0500
Subject: [R] R software to place points on Yahoo maps
In-Reply-To: <45EAA3D3.3090802@utas.edu.au>
References: <45EAA3D3.3090802@utas.edu.au>
Message-ID: <971536df0703042348u621ac520mdcbdf4878df9d3cd@mail.gmail.com>

Hi, Your response to my post was EXTREMELY useful.

For georegistering I used the site:

   http://www.runningmap.com

which lets one click on a a Yahoo! map and get lat/long in lower right.
As my data is in UTM I converted the lat/long to UTM using:

   http://home.hiwaay.net/~taylorc/toolbox/geography/geoutm.html

Then using locator() in R I picked out the corresponding points on the
R image that I had created using your code.  That allowed me to
calculate the inputs for GridTopology (are there any convenience functions
to assist in this? -- I found it a bit error prone)

(I actually noticed that there is a lat/long in Yahoo! map URLs so it might
be that if the maps are a known width and height that one could georegister
them based only on that.)

At any rate I seem to be having some success with this.  Thanks !!!


On 3/4/07, Michael Sumner <mdsumner at utas.edu.au> wrote:
> > Is there any R software that create an image from Yahoo maps together
> > with points of known UTM coordinates (or lat/long marked? Note that my
> > region of interest is not covered in sufficient detail by Google maps.
> > It actually does not have to be Yahoo maps as long as it has
> > sufficient coverage of my region but that's the one I have found with
> > coverage of my region. The scale I am interested in is a city block.
> > Thanks.
>
> rgdal in combination with sp. You'll need to georegister the image, as
> the yahoo maps interface is unlikely to provide a format supporting this
> metadata, but you can do that easily with two reference coordinates.
>
> The workflow might go like this (untested).  You could continue the
> query on R-Sig-Geo. Hope this helps.
>
> There are freely available tools for reading image data directly from
> Yahoo, Google and Virtual Earth in georeferenced
> versions, but I've only used them via commercial GIS. If you can
> describe the map you want I'd be interested in
> exploring that option.
>
> Cheers, Mike.
>
> library(rgdal)
> im <- readGDAL("yahooMap.png")  ## a file saved from Yahoo Maps
>
> ## create index for RGB colours
> col <- SGDF2PCT(im)  ## im is a spatialGridDataFrame with 3 bands
>
> im$ind <- col$idx  ## add the colour index to the data frame
>
> image(im, "ind", col = col$ct)
>
> ## BUT you won't be able to plot UTM on this yet
>
> ## you'll need to create a new GridTopology with appropriate
> cellcentre.offset and cellsize
>
> ogt <- getGridTopology(im)
> . . . find values for offsets and pixel size
>
> gt <- GridTopology(cc.offset, csize,  gt at cells.dim)
>
> ## recreate the object
> im <- SpatialGridDataFrame(gt, im at data)
>
> image(im, "idx", col = col$ct)
> points(utm.x, utm.y)  ## etc.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From samay.sar at gmail.com  Mon Mar  5 08:56:28 2007
From: samay.sar at gmail.com (d. sarthi maheshwari)
Date: Mon, 5 Mar 2007 13:26:28 +0530
Subject: [R] plot(): I want to display dates on X-axis.
Message-ID: <d4327f7e0703042356g7cce6b8bi67a517525c5e30b1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070305/c0d72473/attachment.pl 

From Sophie.Richier at obs-vlfr.fr  Mon Mar  5 09:14:17 2007
From: Sophie.Richier at obs-vlfr.fr (Sophie Richier)
Date: Mon, 05 Mar 2007 09:14:17 +0100
Subject: [R] Error in La.svd(X) : error code 1 from Lapack routine 'dgesdd'
Message-ID: <45EBD159.1000302@obs-vlfr.fr>

Dear R helpers,
I am working with R 2.4.1 GUI 1.18 (4038) for MacOSX. I have a matrix of 
10 000 genes  and try to run the following commands:
 > model.mix<-makeModel (data=data, formula=~Dye+Array+Sample+Time, 
random=~Array+Sample)
 > anova.mix<-fitmaanova (data, model.mix)
 > test.mix<-matest (data, model=model.mix, term="Time", n.perm=100, 
test.method=c(1,0,1,1))

I get the following error message:
Doing F-test on observed data ...
Doing permutation. This may take a long time ...
Error in La.svd(X) : error code 1 from Lapack routine 'dgesdd'

What does this mean? is my matrix too big? What can I do?
Thanks a lot in adavance

Sophie


From EMA_CHO at promos.com.tw  Mon Mar  5 10:30:01 2007
From: EMA_CHO at promos.com.tw (EMA_CHO at promos.com.tw)
Date: Mon, 5 Mar 2007 17:30:01 +0800
Subject: [R] about find the solution
Message-ID: <OFBDC5267A.10918A62-ON48257295.0033ECFA@promos.com.tw>


If I want to find out the soltion of X1,X2
that min(3X1+2X2+X1X2) subject to
    20<=X1+3X2<=50
    10<=X1
which function or package can I use?
Thanks.


From petr.pikal at precheza.cz  Mon Mar  5 10:30:23 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 05 Mar 2007 10:30:23 +0100
Subject: [R] plot(): I want to display dates on X-axis.
In-Reply-To: <d4327f7e0703042356g7cce6b8bi67a517525c5e30b1@mail.gmail.com>
Message-ID: <45EBF13F.723.95824A@localhost>

Hi

you probably know that the second column are dates but your poor PC 
does not, so you should to tell him.

You have several options:

Change the column to suitable date format - see chron package or help 
pages related to date functions e.g. strptime, as.Date, ... and 
perform your plot.

Change your dat column to character vector and using it as a labels 
to x axis - see help pages to plot, axes, titles....

On 5 Mar 2007 at 13:26, d. sarthi maheshwari wrote:

Date sent:      	Mon, 5 Mar 2007 13:26:28 +0530
From:           	"d. sarthi maheshwari" <samay.sar at gmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] plot(): I want to display dates on X-axis.

> Hi,
> 
> I want to display dates on my x-axis of the plot. I was trying to use
> plot() command for the same and passing the values in following
> manner:
> 
> The variable "dat" is a data frame. The first column has numeric
> values and second column has date.
> 
> e.g. dat
> 
>                  [,1]                        dat[,2]
> 
> [1,]            300                       20060101
> [2,]            257                       20060102
> [3,]            320                       20060103
> [4,]            311                       20060104
> [5,]            297                       20060105
> [6,]            454                       20060106
> [7,]            360                       20060107
> [8,]            307                       20060108
> ....
> ....
> 

However what did you suppose this command will do?
Did you even try to read plot help page?

> the command I am performing is::
> 
> plot(x=dat[1], y=as.character(dat[2]))
> 

If you want to plot date on x axis and values on y axis why you did 
it in opposite way?

plot(dat[,2], dat[,1])

after transformation to date format shall do what you want.

Regards
Petr

> 
> Kindly suggest some method by which I can perform my task of
> displaying the first column values on y-axis against dates on x-axis.
> 
> -- 
> Thanks & Regards
> Sarthi M.
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From Patrik.Ohagen at mpa.se  Mon Mar  5 10:30:56 2007
From: Patrik.Ohagen at mpa.se (=?iso-8859-1?Q?=D6hagen_Patrik?=)
Date: Mon, 5 Mar 2007 10:30:56 +0100
Subject: [R] Non : Confidence intervals for p**2 ??
In-Reply-To: <OFBDC5267A.10918A62-ON48257295.0033ECFA@promos.com.tw>
Message-ID: <2BAF2D3C41D1274E9228E63287F19B7E27DB0A@mailsrv2.loginmpa.mpa.se>



Dear List,

I was asked to calculate a confidence interval for p*p. Is there any standard techniques for calculating such an interval? Delta Method?

Thank you in advance!


Cheers, Patrik


From renaud.lancelot at gmail.com  Mon Mar  5 10:37:23 2007
From: renaud.lancelot at gmail.com (Renaud Lancelot)
Date: Mon, 5 Mar 2007 10:37:23 +0100
Subject: [R] lattice histogram
In-Reply-To: <6.1.2.0.2.20070304223616.01cff338@aiminy.mail.iastate.edu>
References: <6.1.2.0.2.20070304012030.01dc5d58@aiminy.mail.iastate.edu>
	<c2ee56800703040054i141786edo49a4320071d0d9b2@mail.gmail.com>
	<6.1.2.0.2.20070304223616.01cff338@aiminy.mail.iastate.edu>
Message-ID: <c2ee56800703050137v2a35ac11u4f3f3d591300f3de@mail.gmail.com>

See argument drop.unused.levels in xyplot. You will also need to
manage the case n = 0 for dispalying the mean and stdv.

Best,

Renaud

histogram(~ resp | group, drop.unused.levels = FALSE,
  panel = function(x, ...){
    std <- if(length(x) > 0) format(round(sd(x), 2), nsmall = 2) else "NA"
    n <- length(x)
    m <- if(length(x) > 0) format(round(mean(x), 2), nsmall = 2) else "NA"
    panel.histogram(x, ...)
    x1 <- unit(1, "npc") - unit(2, "mm")
    y1 <- unit(1, "npc") - unit(2, "mm")
    grid.text(label = bquote(n == .(n)), x = x1, y = y1, just = "right")
    grid.text(label = bquote(hat(m) == .(m)), x = x1, y = y1 - unit(1,
"lines"), just = "right")
    grid.text(label = bquote(hat(s) == .(std)), x = x1, y = y1 -
unit(2, "lines"), just = "right")
    })


2007/3/5, Aimin Yan <aiminy at iastate.edu>:
> thank you very much.  Your code almost solve my problem, but I have a
> further question.
> In my data, there is no observation in some group, I want to label that
> panel by
> n=0
> hat(m)=NA
> hat(s)=NA.
>
> I try to modify your panel function, but it doesn't work out. Do you know
> how to add something to your panel
>   function so that it can deal with some group that has 0 observation.
>
> Aimin
>
> At 02:54 AM 3/4/2007, Renaud Lancelot wrote:
> >Here is an example using the grid package to annotate the graphs:
> >
> >library(lattice)
> >library(grid)
> >resp  <- rnorm(200)
> >group <- sample(c("G1", "G2", "G3"), replace = TRUE, size = 100)
> >histogram(~ resp | group,
> >  panel = function(x, ...){
> >    std <- round(sd(x), 2)
> >    n <- length(x)
> >    m <- round(mean(x), 2)
> >    panel.histogram(x, ...)
> >    x1 <- unit(1, "npc") - unit(2, "mm")
> >    y1 <- unit(1, "npc") - unit(2, "mm")
> >    grid.text(label = bquote(n == .(n)), x = x1, y = y1, just = "right")
> >    grid.text(label = bquote(hat(m) == .(m)), x = x1, y = y1 - unit(1,
> >"lines"), just = "right")
> >    grid.text(label = bquote(hat(s) == .(std)), x = x1, y = y1 -
> >unit(2, "lines"), just = "right")
> >    })
> >Best,
> >
> >Renaud
> >
> >2007/3/4, Aimin Yan <aiminy at iastate.edu>:
> >>How to add mean,sd, number of observation in each panel for lattice
> >>histogram?
> >>
> >>Aimin
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >--
> >Renaud LANCELOT
> >D?partement Syst?mes Biologiques du CIRAD
> >CIRAD, Biological Systems Department
> >
> >Campus International de Baillarguet
> >TA 30 / B
> >F34398 Montpellier
> >Tel   +33 (0)4 67 59 37 17
> >Secr. +33 (0)4 67 59 37 37
> >Fax   +33 (0)4 67 59 37 95
>
>
>


-- 
Renaud LANCELOT
D?partement Syst?mes Biologiques du CIRAD
CIRAD, Biological Systems Department

Campus International de Baillarguet
TA 30 / B
F34398 Montpellier
Tel   +33 (0)4 67 59 37 17
Secr. +33 (0)4 67 59 37 37
Fax   +33 (0)4 67 59 37 95


From jholtman at gmail.com  Mon Mar  5 10:58:36 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 5 Mar 2007 04:58:36 -0500
Subject: [R] Scoping issue?
In-Reply-To: <B1614B0C915A654A9C29BB71DA80E0DD01773D1C@MAIL2.ad.uams.edu>
References: <mailman.7.1172919606.16782.r-help@stat.math.ethz.ch>
	<B1614B0C915A654A9C29BB71DA80E0DD01773D1B@MAIL2.ad.uams.edu>
	<644e1f320703041525w5af344a5mea578b13f61f9014@mail.gmail.com>
	<B1614B0C915A654A9C29BB71DA80E0DD01773D1C@MAIL2.ad.uams.edu>
Message-ID: <644e1f320703050158t6d8ea799wa1879f56a13bb7f7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070305/16786d2f/attachment.pl 

From james at ma.hw.ac.uk  Mon Mar  5 10:59:20 2007
From: james at ma.hw.ac.uk (james at ma.hw.ac.uk)
Date: Mon, 5 Mar 2007 09:59:20 -0000 (GMT)
Subject: [R] Heteroskedastic Time Series
Message-ID: <17372.85.118.9.118.1173088760.squirrel@85.118.9.118>

Hi R-helpers,

I'm new to time series modelling, but my requirement seems to fall just
outside the capabilities of the arima function in R.  I'd like to fit an
ARMA model where the variance of the disturbances is a function of some
exogenous variable.  So something like:

Y_t = a_0 + a_1 * Y_(t-1) +...+ a_p * Y_(t-p) + b_1 * e_(t-1) +...+ b_q *
e_(t-q) + e_t,

where

e_t ~ N(0, sigma^2_t),

and with the variance specified by something like

sigma^2_t = exp(beta_t * X_t),

where X_t is my exogenous variable.  I would be very grateful if somebody
could point me in the direction of a library that could fit this (or a
similar) model.

Thanks,

James Kirkby
Actuarial Maths and Stats
Heriot Watt University


From ted.harding at nessie.mcc.ac.uk  Mon Mar  5 11:06:05 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 05 Mar 2007 10:06:05 -0000 (GMT)
Subject: [R] Non : Confidence intervals for p**2 ??
In-Reply-To: <2BAF2D3C41D1274E9228E63287F19B7E27DB0A@mailsrv2.loginmpa.mpa.se>
Message-ID: <XFMail.070305100605.ted.harding@nessie.mcc.ac.uk>

On 05-Mar-07 ?hagen Patrik wrote:
> 
> Dear List,
> 
> I was asked to calculate a confidence interval for p*p. Is there any
> standard techniques for calculating such an interval? Delta Method?
> 
> Thank you in advance!
> 
> Cheers, Patrik

If "p" is meant to denote a probability between 0 and 1, then

  pL^2 < p^2 < pU^2

is exactly equivalent to

  pL < p < pU

where pL and pU are the upper and lower limits for p. Indeed, this
will be so if p is any quantity which is necessarily non-negative.

Hence, if this is your situation, simply square the confidence
limits for p.

If, however, this is not your situation, then please explain
what "p" represents.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-Mar-07                                       Time: 10:05:50
------------------------------ XFMail ------------------------------


From bartjoosen at hotmail.com  Mon Mar  5 11:31:08 2007
From: bartjoosen at hotmail.com (Bart Joosen)
Date: Mon, 05 Mar 2007 10:31:08 +0000
Subject: [R] How to read in this data format?
In-Reply-To: <971536df0703011346u6d65f1faid814798e98a3e10d@mail.gmail.com>
Message-ID: <BAY134-F1E3BE5B6DFA3094C8A04BD8840@phx.gbl>

Hi,

Although the solution worked, I'v got some troubles with some data files.
These datafiles are very large (600-700 MB), so my computer starts swapping.

If I use the code, written below, I get:
Error in .Call("R_lazyLoadDBfetch", key, file, compressed, hook, PACKAGE = 
"base") :
        recursive default argument reference
After about 15 minutes of loading the data with the  Lines. <- 
readLines("myfile.dat") command.

When I look in the help for readLines, I saw that there is a n to setup a 
maximum number, but is there a way to set a starting row number? If I can 
split up my datafiles in 4-8 small datasets, it's ok for me. But I couldn't 
figure it out.


Thanks

Bart




>From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
>To: "Bart Joosen" <Bartjoosen at hotmail.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] How to read in this data format?
>Date: Thu, 1 Mar 2007 16:46:21 -0500
>
>On 3/1/07, Bart Joosen <Bartjoosen at hotmail.com> wrote:
>>Dear All,
>>
>>thanks for the replies, Jim Holtman has given a solution which fits my
>>needs, but Gabor Grothendieck did the same thing,
>>but it looks like the coding will allow faster processing (should check 
>>this
>>out tomorrow on a big datafile).
>>
>>@gabor: I don't understand the use of the grep command:
>>        grep("^[1-9][0-9. ]*$|Time", Lines., value = TRUE)
>>What is this expression  ("^[1-9][0-9. ]*$|Time") actually doing?
>>I looked in the help page, but couldn't find a suitable answer.
>
>I briefly discussed it in the first paragraph of my response.  It
>matches and returns only those lines that start (^ matches start of line)
>with a digit, i.e. [1-9], and contains only digits, dots and spaces,
>i.e. [0-9. ]*, to end of line, i.e. $ matches end of line, or (| means
>or) contains the word Time.
>If you don't have lines like ... (which you did in your example) then
>the regexp
>could be simplified to "^[0-9. ]+$|Time".  You may need to match tabs too
>if your input contains those.
>
>>
>>
>>Thanks to All
>>
>>
>>Bart
>>
>>----- Original Message -----
>>From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
>>To: "Bart Joosen" <bartjoosen at hotmail.com>
>>Cc: <r-help at stat.math.ethz.ch>
>>Sent: Thursday, March 01, 2007 6:35 PM
>>Subject: Re: [R] How to read in this data format?
>>
>>
>> > Read in the data using readLines, extract out
>> > all desired lines (namely those containing only
>> > numbers, dots and spaces or those with the
>> > word Time) and remove Retention from all
>> > lines so that all remaining lines have two
>> > fields.  Now that we have desired lines
>> > and all lines have two fields read them in
>> > using read.table.
>> >
>> > Finally, split them into groups and restructure
>> > them using "by" and in the last line we
>> > convert the "by" output to a data frame.
>> >
>> > At the end we display an alternate function f
>> > for use with by should we wish to generate long
>> > rather than wide output (using the terminology
>> > of the reshape command).
>> >
>> >
>> > Lines <- "$$ Experiment Number:
>> > $$ Associated Data:
>> >
>> > FUNCTION 1
>> >
>> > Scan            1
>> > Retention Time  0.017
>> >
>> > 399.8112        184
>> > 399.8742        0
>> > 399.9372        152
>> > ....
>> >
>> > Scan            2
>> > Retention Time  0.021
>> >
>> > 399.8112        181
>> > 399.8742        1
>> > 399.9372        153
>> > "
>> >
>> > # replace next line with: Lines. <- readLines("myfile.dat")
>> > Lines. <- readLines(textConnection(Lines))
>> > Lines. <- grep("^[1-9][0-9. ]*$|Time", Lines., value = TRUE)
>> > Lines. <- gsub("Retention", "", Lines.)
>> >
>> > DF <- read.table(textConnection(Lines.), as.is = TRUE)
>> > closeAllConnections()
>> >
>> > f <- function(x) c(id = x[1,2], structure(x[-1,2], .Names = x[-1,1]))
>> > out.by <- by(DF, cumsum(DF[,1] == "Time"), f)
>> > as.data.frame(do.call("rbind", out.by))
>> >
>> >
>> > We could alternately consider producing long
>> > format by replacing the function f with:
>> >
>> > f <- function(x) data.frame(x[-1,], id = x[1,2])
>> >
>> >
>> > On 3/1/07, Bart Joosen <bartjoosen at hotmail.com> wrote:
>> >> Hi,
>> >>
>> >> I recieved an ascii file, containing following information:
>> >>
>> >> $$ Experiment Number:
>> >> $$ Associated Data:
>> >>
>> >> FUNCTION 1
>> >>
>> >> Scan            1
>> >> Retention Time  0.017
>> >>
>> >> 399.8112        184
>> >> 399.8742        0
>> >> 399.9372        152
>> >> ....
>> >>
>> >> Scan            2
>> >> Retention Time  0.021
>> >>
>> >> 399.8112        181
>> >> 399.8742        1
>> >> 399.9372        153
>> >> .....
>> >>
>> >>
>> >> I would like to import this data in R into a dataframe, where there is 
>>a
>> >> column time, the first numbers as column names, and the second numbers 
>>as
>> >> data in the dataframe:
>> >>
>> >> Time    399.8112        399.8742        399.9372
>> >> 0.017   184     0       152
>> >> 0.021   181     1       153
>> >>
>> >> I did take a look at the read.table, read.delim, scan, ... But I 've 
>>no
>> >> idea
>> >> about how to solve this problem.
>> >>
>> >> Anyone?
>> >>
>> >>
>> >> Thanks
>> >>
>> >> Bart
>> >>
>> >> ______________________________________________
>> >> R-help at stat.math.ethz.ch mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >
>>
>>


From debmidya at yahoo.com  Mon Mar  5 11:37:46 2007
From: debmidya at yahoo.com (Deb Midya)
Date: Mon, 5 Mar 2007 02:37:46 -0800 (PST)
Subject: [R] Difference between two time series
Message-ID: <415785.54452.qm@web50409.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070305/aefc4e3e/attachment.pl 

From j.zutt at tudelft.nl  Mon Mar  5 11:43:32 2007
From: j.zutt at tudelft.nl (Jonne Zutt)
Date: Mon, 05 Mar 2007 11:43:32 +0100
Subject: [R] barplot with different color combination for each bar
In-Reply-To: <6.2.5.6.2.20070302093533.01f88e78@uiuc.edu>
References: <6.2.5.6.2.20070302093533.01f88e78@uiuc.edu>
Message-ID: <1173091412.9287.13.camel@dutiih.st.ewi.tudelft.nl>

Hi, I'd suggest you use ?rect for this.
Here's an example (I did not check whether it's correct...)
I also improved (but not checked :) your definition of cols.

Jonne.

X <- seq(1:6)
Q <- matrix(sample(X, 60, replace = T), nrow=6, byrow = T)
H <- matrix(rep(1,60), nrow=6, byrow=T)

color <-  c("blue", "orange", "gold", "indianred", "skyblue4",
"lightblue")
cols <- matrix(data=color[Q], ncol=10)

# Old:
barplot(H, col=cols, width = c(0.1), xlim = c(0,3), beside=F)

# New:
x11()
plot(0, 0, type="n", ylim=c(0,nrow(Q)), xlim=c(0,ncol(Q)), 
     xlab="xlabel", ylab="")
xleft <- rep(1:ncol(Q), each=nrow(Q))
ybottom <- rep(1:nrow(Q), times=ncol(Q))
rect(xleft-1, ybottom-1, xleft, ybottom, col=cols)

On Fri, 2007-03-02 at 09:48 -0600, Kim Milferstedt wrote:
> Hi,
> 
> I'd like to construct a somewhat unusual barplot. In "barplot" I use 
> beside=F as I'd like to have stacked bars. The height of each bar is 
> always the same. Information in my plot is coded in the color of the 
> bar. I therefore need to be able so assign a different combination 
> (or order) of colors to each individual stacked bar.
> 
> In the example below, the combination of colors for my plot is 
> generated by X, Q, color and cols. These colors are supposed to fill 
> the stacked bars with the height of H. However, only the first column 
> of cols is used for all columns of H as "barplot" only allows me to 
> assign one vector for the color scheme of the entire barplot.
> 
> Does anybody know a way how I can assign each bar a potentially 
> unique color combination?
> 
> Thanks for your help!
> 
> Kim
> 
> X <- seq(1:6)
> Q    <- matrix(sample(X, 60, replace = T), nrow=6, byrow = T)
> H   <-  matrix(rep(1,60), nrow=6, byrow=T)
> 
> color   <-  c("blue", "orange", "gold", "indianred", "skyblue4", "lightblue")
> cols <-     ifelse(
>                  (Q ==1) , color[1],
>                      ifelse(
>                          (Q ==2), color[2],
>                              ifelse(
>                                  (Q ==3) , color[3],
>                                      ifelse(
>                                          (Q ==4), color[4],
>                                              ifelse(
>                                                  (Q ==5) , color[5], color[6]
>                                                      )
>                                              )
>                                      )
>                              )
>                      )
> 
> barplot(
>          H,
>          col=cols,
>          width = c(0.1),
>          xlim = c(0,3),
>          beside=F
>          )
> 
> __________________________________________
> 
> Kim Milferstedt
> University of Illinois at Urbana-Champaign
> Department of Civil and Environmental Engineering
> 4125 Newmark Civil Engineering Laboratory
> 205 North Mathews Avenue MC-250
> Urbana, IL 61801
> USA
> phone: (001) 217 333-9663
> fax: (001) 217 333-6968
> email: milferst at uiuc.edu
> http://cee.uiuc.edu/research/morgenroth
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From cgb at datanalytics.com  Mon Mar  5 12:20:15 2007
From: cgb at datanalytics.com (Carlos J. Gil Bellosta )
Date: Mon, 5 Mar 2007 11:20:15 +0000
Subject: [R] Error loading a dependency in a package: missing namespace?
Message-ID: <b028350f0703050320l3ef0fd50uea75d63022adf8b3@mail.gmail.com>

Dear r-helpers,

I am building a package that depends on some others. I recently added
a new dependency: package "outliers". But does not work any more.

Let me show some information below:

carlos at kropotkin:pcrAnalysis$ cat DESCRIPTION
Package: pcrAnalysis
Type: Package
Title: pcrAnalysis
Version: 0.7.2
Date: 2007-02-27
Depends: Biobase, methods, outliers
Author: Carlos J. Gil Bellosta <gilbellosta at gmail.com>
Maintainer: Carlos J. Gil Bellosta <gilbellosta at gmail.com>
Description: Package for the analysis of Taqman experiments
License: TBA

carlos at kropotkin:pcrAnalysis$ cat NAMESPACE
import(methods, Biobase, outliers)
exportPattern("^tqmn")
exportClasses("pcrExprSet")
exportMethods(task, "task<-", phenoData.sort)

But now, the load of the packages fails. If I try to run

carlos at kropotkin:tmp$ R CMD check pcrAnalysis

I get the following log:

* checking for working latex ... OK
* using log directory '/tmp/pcrAnalysis.Rcheck'
* using R version 2.4.1 (2006-12-18)
* checking for file 'pcrAnalysis/DESCRIPTION' ... OK
* checking extension type ... Package
* this is package 'pcrAnalysis' version '0.7.2'
* checking package dependencies ... OK
* checking if this is a source package ... OK
* checking whether package 'pcrAnalysis' can be installed ... OK
* checking package directory ... OK
* checking for portable file names ... OK
* checking for sufficient/correct file permissions ... OK
* checking DESCRIPTION meta-information ... OK
* checking top-level files ... OK
* checking index information ... OK
* checking package subdirectories ... OK
* checking whether the package can be loaded ... ERROR
Loading required package: Biobase
Loading required package: tools

Welcome to Bioconductor

   Vignettes contain introductory material. To view, type
   'openVignette()' or start with 'help(Biobase)'. For details
   on reading vignettes, see the openVignette help page.

Loading required package: outliers
Error in loadNamespace(package, c(which.lib.loc, lib.loc), keep.source
= keep.source) :
       in 'pcrAnalysis' classes for export not defined: pcrExprSet
In addition: Warning message:
package 'pcrAnalysis' contains no R code in: loadNamespace(package,
c(which.lib.loc, lib.loc), keep.source = keep.source)
Error: package/namespace load failed for 'pcrAnalysis'
Execution halted

It seems that the error is related to something having to do with
namespaces. The thing is that package "outliers" does not have a
NAMESPACE file. Could this be an issue?

I have contacted the author of the package and he sais that "outliers"
has been used in another package, "quantchem" (also in CRAN). However,
"quantchem" does not have a "NAMESPACE" file either.

I have been looking for information on how the loadNamespace function
works and even looking at its code.

But can anybody give me a clue? Would the "outliers" package require a
"NAMESPACE" file?

By the way, I have contacted the author of the package and he has been
quite helpful, but he says he feels that that (lack of this file) may
not be causing the problem. And I am using R version 2.4.1
(2006-12-18) on an Ubuntu Edgy (6.10) box.

Regards,

Carlos J. Gil Bellosta
http://www.datanalytics.com


From albmont at centroin.com.br  Mon Mar  5 12:43:25 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Mon, 5 Mar 2007 09:43:25 -0200
Subject: [R] plot(): I want to display dates on X-axis.
In-Reply-To: <d4327f7e0703042356g7cce6b8bi67a517525c5e30b1@mail.gmail.com>
References: <d4327f7e0703042356g7cce6b8bi67a517525c5e30b1@mail.gmail.com>
Message-ID: <20070305113655.M2789@centroin.com.br>

Sarthi M. wrote:
> 
> I want to display dates on my x-axis of the plot.
>
Dates are a problem. There's a standard for dates, but it
seems that most users and software didn't catch up :-/

> The variable "dat" is a data frame. The first column has numeric 
> values and second column has date.
> 
> e.g. dat
> 
>                  [,1]                        dat[,2]
> 
> [1,]            300                       20060101
> [2,]            257                       20060102
> [3,]            320                       20060103
> [4,]            311                       20060104
> [5,]            297                       20060105
> [6,]            454                       20060106
> [7,]            360                       20060107
> [8,]            307                       20060108
> ....
> ....
> 
> the command I am performing is::
> 
> plot(x=dat[1], y=as.character(dat[2]))
> 
Hmmm... When I needed something similar, I did this day:

y <- dat[,1]  # because in a (xy) plot, x is the scale and y the data
years <- floor(dat[,2] / 10000)
months <- floor(dat[,2] / 100) %% 100
days <- dat[,2] %% 10000
x <- ISOdate(years, months, days)
plot(x, y)

> Kindly suggest some method by which I can perform my task of 
> displaying the first column values on y-axis against dates on x-axis.
> 
Of course, you can combine all that into one line, but readability
will be blown up:

plot(ISOdate(floor(dat[,2] / 10000), floor(dat[,2] / 100) %% 100,
  dat[,2] %% 10000), dat[,1])

Alberto Monteiro


From lterlemez at anadolu.edu.tr  Mon Mar  5 12:44:13 2007
From: lterlemez at anadolu.edu.tr (Levent TERLEMEZ)
Date: Mon, 05 Mar 2007 13:44:13 +0200
Subject: [R] function with Multiple Output
Message-ID: <45EC028D.6010705@anadolu.edu.tr>

With help of list(), function can return ala of the results.
 
my.fun=function(vector, index){
    a=fun.a(vector, index)
    b=fun.b(vector, index)
    return(list(a,b))
    }

Example:
R : Copyright 2005, The R Foundation for Statistical Computing
Version 2.2.1  (2005-12-20 r36812)
ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> fix(multiresult)
> multiresult(rnorm(10,0,1))
[[1]]
[1] -0.1240271

[[2]]
[1] 1.037070


From luke at stat.uiowa.edu  Mon Mar  5 13:33:15 2007
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Mon, 5 Mar 2007 06:33:15 -0600 (CST)
Subject: [R] Scoping issue?
In-Reply-To: <B1614B0C915A654A9C29BB71DA80E0DD01773D1C@MAIL2.ad.uams.edu>
References: <mailman.7.1172919606.16782.r-help@stat.math.ethz.ch>
	<B1614B0C915A654A9C29BB71DA80E0DD01773D1B@MAIL2.ad.uams.edu>
	<644e1f320703041525w5af344a5mea578b13f61f9014@mail.gmail.com>
	<B1614B0C915A654A9C29BB71DA80E0DD01773D1C@MAIL2.ad.uams.edu>
Message-ID: <Pine.LNX.4.64.0703050632570.3657@itasca2.wildberry.org>

In your test function there is no lexically visible definition for the
`colnum` variable used in defining main, so that error is what you
expect from lexical scoping.  Lazy evaluation dictates when (and if)
the `main` argument is evaluated, but the environment in which it is
evaluated is determined by the context where the expression is written
in the code, i.e. within the function mmatplotTest.

The error you get when you take out the `main` is coming from `subset`
and is due to the fact that subset is one of those functions that uses
non-standard evaluation for some of its arguments, in this case
`select`.  This makes it (slightly) easier to use at intereactive top
level but much more complicated to use within a function. The key in
reading the help page for `subset` is that the argument `select`
should be an expression (actually the literal argument expression is
used, which in this case is the expression consisting of the single
variable `colnum` and is not useful here). You need to use another
function in your sapply call, something like

     function(d) d[,colnum]

may do.

Best,

luke

On Sun, 4 Mar 2007, Thaden, John J wrote:

> Apparently you're right that colnum doesn't exist when it needs to
> be evaluated, but why?  Why is 'paste' being evaluated so early? It is,
> after all, the value of an argument ('main') of my mmatplot function
> with colnum being another argument.  I thought arguments were lazy-loaded.
> Does using mapply change the rules?
>
> Is there a way (like mapply) to loop at some lower level rather than
> Explicitly, in the R script, as in your suggestion?  For speed's sake?
>
> Thanks.  -John
>
>
> On Sunday Mar 4 2007, jim holtman <jholtman at gmail.com> replied
>
>> First of all, 'colnum' does not exist when the 'paste' is called.?
>> This probably does what you want:
> ?
>> for (colnum in 1:ncol(A)){
>> ??? mmatplot(colnum, 1:nrow(A), A, main=paste("Array input, column",
> colnum))
>> }
>
> ?
> On 3/4/07, John Thaden <jthaden at uams.edu> wrote:
> Hello, the code below is supposed to be a wrapper for matplot to
> do columnwise visible comparison of several matrices, but I'm
> doing something wrong because I can't access an argument called
> 'colnum'.??I'd be most grateful for some insight.
>
> Thanks,
> John
> Little Rock, AR
> ################################
> # mmatplot is a matplot wrapper to compare the same column of
> # several matrices. Arg y is either a list of matrices with
> # equal number of rows, or an array. The scalar n gives the
> # column of each matrix or array slab to plot. par values and
> # matplot args are accepted, e.g., ylog.??mmatplot is intended
> # to be mapply-compatible to test multiple columns.
>
> mmatplot <- function(colnum, x, y, ...){
> switch(class(y),
> ?? array = y <- y[, colnum, ],
> ?? list = y <- sapply(X = y, FUN = subset, select = colnum))
> stopifnot(is.matrix(y))
> matplot(x, y, ...)
> }
>
> #This is just a tester function
> mmatplotTest <- function(){
> oldmf <- par("mfrow")
> par(mfrow = c(2,3))
> A <- array(data = rnorm(90), dim = c(10, 3, 3))
> L <- list(A[, , 1], A[, , 2], A[, , 3])
>
> # The 'main' argument below throws the error, but if
> # commented out, another error crops up due to 'colnum'.
> # Test with class(y) == "array"
> mapply(X = 1:ncol(A), FUN = mmatplot, x = 1:nrow(A), y = A,
> ????????????????main = paste("Array input, column", colnum))
> # Test with class(y) == "list"
> mapply(1:ncol(L[[1]]), mmatplot, x = 1:nrow(L[[1]]), y = L,
> ????????????????main = paste("List input, column", colnum))
> par(mfrow = oldmf)
> }
>
> #Run the test
> mmatplotTest()
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From shubhak at ambaresearch.com  Mon Mar  5 14:04:07 2007
From: shubhak at ambaresearch.com (Shubha Vishwanath Karanth)
Date: Mon, 5 Mar 2007 18:34:07 +0530
Subject: [R] RBloomberg
Message-ID: <A36876D3F8A5734FA84A4338135E7CC3011F2EAD@BAN-MAILSRV03.Amba.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070305/ee9919a5/attachment.pl 

From nicolas.mazziotta at swing.be  Mon Mar  5 14:11:36 2007
From: nicolas.mazziotta at swing.be (Nicolas Mazziotta)
Date: Mon, 5 Mar 2007 14:11:36 +0100
Subject: [R] background color behind symbols in legend()
Message-ID: <200703051411.37060.nicolas.mazziotta@swing.be>

Hello,

I try to display coloured rectangles behind symbols in a legend (as a 
background):

> plot(10,10)
> legend("top", c("text","text2"), pch=c(21,22), fill=c("red","green"), 
pt.bg="black") 

On the resulting graph, the symbol is not centered upon the coloured 
rectangle. Is there a way to adjust their relative position, so that they are 
centered? Looking through ?legend has not helped me (but I might have missed 
the line where it is explained)...
 
[R version 2.4.0 (2006-10-03) on linux]

Thanks for any help.

Best regards,


-- 
Nicolas Mazziotta

The contents of this e-mail, including any attachments, are ...{{dropped}}


From DavidLloyd at mail2lloyd.com  Mon Mar  5 14:14:50 2007
From: DavidLloyd at mail2lloyd.com (David Lloyd)
Date: Mon, 5 Mar 2007 05:14:50 -0800
Subject: [R]  Identifying points in a plot that have duplicate values
Message-ID: <132d601c75f28$4176e530$8dca010a@mail2world.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070305/06a21169/attachment.pl 

From Rainer.Krug at uct.ac.za  Mon Mar  5 10:04:16 2007
From: Rainer.Krug at uct.ac.za (Rainer M. Krug)
Date: Mon, 05 Mar 2007 11:04:16 +0200
Subject: [R] Identifying last record in individual growth data over
 different time intervalls
Message-ID: <45EBDD10.9060502@uct.ac.za>

Hi

I have a plist t which contains size measurements of individual plants, 
identified by the field "plate". It contains, among other, a field 
"year" indicating the year in which the individual was measured and the 
"height". The number of measurements range from 1 to 4 measurements in 
different years.
My problem is that I would need the LAST measurement. I only came up 
with the solution below which is probably way to complicated, but I 
can't think of another solution.

Does anybody has an idea how to do this more effectively?

Finally I would like to have a data.frame t2 which only contains the 
entries of the last measurements.

Thanks in advance,

Rainer



 > unlist(
	sapply(
		split(t, t$plate),
		function(i)
		{
			i[i$year==max(i$year),]$id
		}
		)
	)

      15      20      33      43      44      47      64     D72    S200 
    S201
2006001 2006003 2006005 2006007 2006008 2006009 2006014 2006015 2006016 
2006017
    S202    S203    S204    S205    S206    S207    S208    S209    S210 
    S211
2004095 2006019 2006020 2006021 2006022 2006023 2006024 2006025 2006026 
2006027
    S212    S213    S214    S215    S216    S217    S218    S219    S220 
    S222
2006028 2006029 2006030 2006031 2006032 2006033 2006034 2006035 2006036 
2006037
    S223    S224    S225    S226    S227    S228    S229    S230    S231 
    S232
2006038 2006039 2006040 2006041 2006042 2006043 2006044 2006045 2006046 
2006047
 >
 > t
              id plate year height
2004007 2004007    15 2004   0.40
2005024 2005024    15 2005   0.43
2006001 2006001    15 2006   0.44
2004012 2004012    20 2004   0.90
2005026 2005026    20 2005   0.94
2006003 2006003    20 2006   0.98
2004025 2004025    33 2004   0.15
2005027 2005027    33 2005   0.15
2006005 2006005    33 2006   0.16
2004035 2004035    43 2004   0.26
2005038 2005038    43 2005   0.30
2006007 2006007    43 2006   0.38
2004036 2004036    44 2004   0.32
2005030 2005030    44 2005   0.39
2006008 2006008    44 2006   0.46
2004039 2004039    47 2004   0.50
2005025 2005025    47 2005   0.55
2006009 2006009    47 2006   0.63
2004055 2004055    64 2004   0.45
2005029 2005029    64 2005   0.58
2006014 2006014    64 2006   0.67
2006015 2006015   D72 2006   0.30
2004093 2004093  S200 2004   0.68
2005040 2005040  S200 2005   0.74
2006016 2006016  S200 2006   0.84
2004094 2004094  S201 2004   0.46
2005041 2005041  S201 2005   0.49
2006017 2006017  S201 2006   0.53
2004095 2004095  S202 2004   0.17
2004096 2004096  S203 2004   0.23
2005032 2005032  S203 2005   0.23
2006019 2006019  S203 2006   0.23
2004097 2004097  S204 2004   0.25
2005031 2005031  S204 2005   0.29
2006020 2006020  S204 2006   0.41
2004098 2004098  S205 2004   0.22
2005039 2005039  S205 2005   0.26
2006021 2006021  S205 2006   0.37
2004099 2004099  S206 2004   0.19
2005035 2005035  S206 2005   0.25
2006022 2006022  S206 2006   0.37
2004100 2004100  S207 2004   0.29
2005003 2005003  S207 2005   0.36
2006023 2006023  S207 2006   0.41
2004101 2004101  S208 2004   0.17
2005005 2005005  S208 2005   0.20
2006024 2006024  S208 2006   0.16
2004102 2004102  S209 2004   0.16
2005008 2005008  S209 2005   0.19
2006025 2006025  S209 2006   0.24
2004103 2004103  S210 2004   0.09
2005007 2005007  S210 2005   0.14
2006026 2006026  S210 2006   0.15
2004104 2004104  S211 2004   0.12
2005006 2005006  S211 2005   0.12
2006027 2006027  S211 2006   0.22
2004105 2004105  S212 2004   0.61
2005011 2005011  S212 2005   0.71
2006028 2006028  S212 2006   0.81
2004106 2004106  S213 2004   0.28
2005010 2005010  S213 2005   0.37
2006029 2006029  S213 2006   0.44
2004107 2004107  S214 2004   0.47
2005009 2005009  S214 2005   0.59
2006030 2006030  S214 2006   0.67
2004108 2004108  S215 2004   0.43
2005004 2005004  S215 2005   0.53
2006031 2006031  S215 2006   0.66
2004109 2004109  S216 2004   0.35
2005019 2005019  S216 2005   0.38
2006032 2006032  S216 2006   0.41
2004110 2004110  S217 2004   0.20
2005018 2005018  S217 2005   0.21
2006033 2006033  S217 2006   0.32
2004111 2004111  S218 2004   0.19
2005014 2005014  S218 2005   0.21
2006034 2006034  S218 2006   0.27
2004112 2004112  S219 2004   0.21
2005034 2005034  S219 2005   0.24
2006035 2006035  S219 2006   0.24
2004113 2004113  S220 2004   0.19
2005021 2005021  S220 2005   0.19
2006036 2006036  S220 2006   0.25
2004114 2004114  S222 2004   0.34
2005020 2005020  S222 2005   0.35
2006037 2006037  S222 2006   0.46
2005013 2005013  S223 2005   0.04
2006038 2006038  S223 2006   0.04
2005012 2005012  S224 2005   0.13
2006039 2006039  S224 2006   0.14
-- 
NEW EMAIL ADDRESS AND ADDRESS:

Rainer.Krug at uct.ac.za

RKrug at sun.ac.za WILL BE DISCONTINUED END OF MARCH

Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Leslie Hill Institute for Plant Conservation
University of Cape Town
Rondebosch 7701
South Africa

Fax:		+27 - (0)86 516 2782
Fax:		+27 - (0)21 650 2440 (w)
Cell:		+27 - (0)83 9479 042

Skype:		RMkrug

email:	Rainer.Krug at uct.ac.za
       	Rainer at krugs.de


From maitra at iastate.edu  Mon Mar  5 14:48:38 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Mon, 5 Mar 2007 07:48:38 -0600
Subject: [R] Error in La.svd(X) : error code 1 from Lapack routine
 'dgesdd'
In-Reply-To: <45EBD159.1000302@obs-vlfr.fr>
References: <45EBD159.1000302@obs-vlfr.fr>
Message-ID: <20070305074838.0ef852c0@triveni.stat.iastate.edu>

On Mon, 05 Mar 2007 09:14:17 +0100 Sophie Richier <Sophie.Richier at obs-vlfr.fr> wrote:

> Dear R helpers,
> I am working with R 2.4.1 GUI 1.18 (4038) for MacOSX. I have a matrix of 
> 10 000 genes  and try to run the following commands:
>  > model.mix<-makeModel (data=data, formula=~Dye+Array+Sample+Time, 
> random=~Array+Sample)
>  > anova.mix<-fitmaanova (data, model.mix)
>  > test.mix<-matest (data, model=model.mix, term="Time", n.perm=100, 
> test.method=c(1,0,1,1))
> 
> I get the following error message:
> Doing F-test on observed data ...
> Doing permutation. This may take a long time ...
> Error in La.svd(X) : error code 1 from Lapack routine 'dgesdd'
> 
> What does this mean? is my matrix too big? What can I do?
> Thanks a lot in adavance
> 
> Sophie


from the help file:



     Unsuccessful results from the underlying LAPACK code will result
     in an error giving a positive error code: these can only be
     interpreted by detailed study of the FORTRAN code.


from the manpages:

man dgesdd

       INFO    (output) INTEGER
               = 0:  successful exit.
               < 0:  if INFO = -i, the i-th argument had an illegal value. 
               > 0:  DBDSDC did not converge, updating process failed.

I don't know what DBDSDC is, but it appears that there may be some convergence issue for you. Unless someone else has better ideas, look up www.netlib.org/lapack and the routines in there to investigate further.

HTH!

Best,
Ranjan


From albmont at centroin.com.br  Mon Mar  5 15:23:08 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Mon, 5 Mar 2007 12:23:08 -0200
Subject: [R] 0 * NA = NA
Message-ID: <20070305141939.M7169@centroin.com.br>

Is there any way to "force" 0 * NA to be 0 instead of NA?

For example, suppose I have a vector with some valid values, while
other values are NA. If I matrix-pre-multiply this by a weight 
row vector, whose weights that correspond to the NAs are zero,
the outcome will still be NA:

x <- c(1, NA, 1)
wt <- c(2, 0, 1)
wt %*% x # NA

Alberto Monteiro


From j.van_den_hoff at fzd.de  Mon Mar  5 15:26:40 2007
From: j.van_den_hoff at fzd.de (Joerg van den Hoff)
Date: Mon, 5 Mar 2007 15:26:40 +0100
Subject: [R] R CMD CHECK question
Message-ID: <20070305142640.GA16679@marco.fz-rossendorf.de>

hi,
second try...

I ran into problems when checking one of my packages with R CMD CHECK:


I have two packages, the first (named `pkc') depending on the second one (named
`roiutils'). The source code and DESCRIPTION files describes the dependency as
it should be, I think ('Imports', `require'). but if I run

R CMD CHECK pkc 

I get "significant warnings" related to missing links (refering to functions
from the second package) in the manpages of the first package as can be
seen below. despite the warnings, after installing the two packages the help
system works just fine including the cross-references.

my question:
why is it, that R CMD CHECK is complaining?  can one selectively switch of this
warning?  or how have I to specify the links in the manpages to tell CHECK that
everything is basically OK?

========================CUT====================================
* checking for working latex ... OK
* using log directory '/Users/vdh/rfiles/Rlibrary/.check/pkc.Rcheck'
* using R version 2.4.0 (2006-10-03)
* checking for file 'pkc/DESCRIPTION' ... OK
* this is package 'pkc' version '1.1'
* checking package dependencies ... OK
* checking if this is a source package ... OK
* checking whether package 'pkc' can be installed ... WARNING
Found the following significant warnings:
       missing link(s):  readroi readroi readroi figure readroi conv3exmodel readroi
       missing link(s):  figure readroi
* checking package directory ... OK
* checking for portable file names ... OK
* checking for sufficient/correct file permissions ... OK
* checking DESCRIPTION meta-information ... OK
* checking top-level files ... OK
* checking index information ... OK
* checking package subdirectories ... OK
* checking R files for syntax errors ... OK
* checking R files for non-ASCII characters ... OK
* checking whether the package can be loaded ... OK
* checking whether the package can be loaded with stated dependencies ... OK
* checking whether the name space can be loaded with stated dependencies ... OK
* checking S3 generic/method consistency ... OK
* checking replacement functions ... OK
* checking foreign function calls ... OK
* checking R code for possible problems ... OK
* checking Rd files ... WARNING
Rd files with unknown sections:
  /Users/vdh/rfiles/Rlibrary/pkc/man/fitdemo.Rd: example

See the chapter 'Writing R documentation files' in manual 'Writing R
Extensions'.
* checking Rd cross-references ... WARNING
Missing link(s) in documentation object 'compfit.Rd':
  readroi readroi readroi figure readroi conv3exmodel readroi

Missing link(s) in documentation object 'exp3fit.Rd':
  figure readroi
========================CUT====================================


any hints appreciated,

joerg


From h.wickham at gmail.com  Mon Mar  5 15:53:53 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 5 Mar 2007 08:53:53 -0600
Subject: [R] [ANN] Static and dynamic graphics course, July 2007,
	Salt Lake City
Message-ID: <f8e6ff050703050653x4cc6c2f1uca8548dc8031e8f1@mail.gmail.com>

We're pleased to announce a one day course covering static and dynamic
graphics using R, ggplot and GGobi. The course will be held just
before the JSM, on Saturday, 28 July 2007, in Salt Lake City. The
course will be presented by Dianne Cook and Hadley Wickham.

In the course you will learn:

* How to build presentation quality static graphics using the R
package, ggplot. We will cover plot creation and modification, and
discuss the grammar which underlies the package.

* How to explore your data with direct manipulation/dynamic graphics
using GGobi and rggobi. You'll learn the general toolbox, as well
specific approaches for dealing with missing data, supervised
classification, cluster analysis and multivariate longitudinal data
analysis.

Dianne Cook is a full professor at Iowa State University. She has been
an active researcher in the field of interactive and dynamic graphics
for 16 years, and regularly teaches information visualization,
multivariate analysis and data mining.

Hadley Wickham is a PhD student at Iowa State University. He won the
John Chambers Award for statistical computing in 2006 for his work on
ggplot.

For more details, or to book your place, please see http://lookingatdata.com


From stranda at cofc.edu  Mon Mar  5 15:56:28 2007
From: stranda at cofc.edu (Allan Strand)
Date: Mon, 05 Mar 2007 09:56:28 -0500
Subject: [R] enumerating non-overlapping pairs of elements from a vector
Message-ID: <1173106588.6666.19.camel@rosa>

Hi All,

I'm trying to come up with a clear and concise (and fast?) solution to
the following problem.

I would like to take a vector 'v' and enumerate all of the ways in
which it can be broken into n sets of length 2 (if the length of the
vector is odd, and an additional set of length 1).  An element of 'v'
can
only appear in one set. Order within sets is not important.  Vector
'v' can be of lengths 2-12

 'n' is determined by length(v)%/%2
 if length(v)%%2 is non-zero, the additional set of length 1 is used

For example vector 'v':
v = (1,2,3,4)

The solution would be (rows are combinations of sets chosen, where
each element only appears once)

1 2, 3 4
1 3, 2 4
1 4, 2 3

In the case where length(v) is odd
v = (1,2,3,4,5)
1 2, 3 4, 5
1 3, 2 4, 5
1 4, 2 3, 5
5 2, 3 4, 1
5 3, 2 4, 1
5 4, 2 3, 1
5 1, 3 4, 2
5 3, 1 4, 2
5 4, 1 3, 2
and so on...

Certainly pulling all combinations of two or one elements is not a big
deal, for example

combinations(5,2,c(1,2,3,4,5),repeats.allowed=T) 

from the 'gtools' package would do something like this.  

I'm stuck on a clean solution for enumerating all the non-overlapping
sets without some elaborate looping and checking scheme.  No doubt
this is a lapse in my understanding of combinatorics.  Any help would
be greatly appreciated

cheers,
a.


From andy_liaw at merck.com  Mon Mar  5 16:02:33 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 5 Mar 2007 10:02:33 -0500
Subject: [R] 0 * NA = NA
In-Reply-To: <20070305141939.M7169@centroin.com.br>
References: <20070305141939.M7169@centroin.com.br>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03CE7AB4@usctmx1106.merck.com>

From: Alberto Monteiro
> 
> Is there any way to "force" 0 * NA to be 0 instead of NA?
> 
> For example, suppose I have a vector with some valid values, 
> while other values are NA. If I matrix-pre-multiply this by a 
> weight row vector, whose weights that correspond to the NAs 
> are zero, the outcome will still be NA:
> 
> x <- c(1, NA, 1)
> wt <- c(2, 0, 1)
> wt %*% x # NA

I don't think it's prudent to bend arthmetic rules of a system,
especially when there are good reasons for them.  Here's one:

R> 0 * Inf
[1] NaN

If you are absolutely sure that the Nas in x cannot be Inf (or -Inf),
you might try to force the result to 0, but the only way I can think of
is to do something like:

R> wt %*% ifelse(wt, x, 0)
     [,1]
[1,]    3

Andy 

 
> Alberto Monteiro
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From klaster at karlin.mff.cuni.cz  Mon Mar  5 16:02:06 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Mon, 05 Mar 2007 16:02:06 +0100
Subject: [R] 0 * NA = NA
In-Reply-To: <20070305141939.M7169@centroin.com.br>
References: <20070305141939.M7169@centroin.com.br>
Message-ID: <45EC30EE.8030305@karlin.mff.cuni.cz>

Alberto Monteiro napsal(a):
> Is there any way to "force" 0 * NA to be 0 instead of NA?

No (AFAIK), and it is pretty reasonable to define it this way.
If you want to treat the NAs as zeros, use
x[is.na(x)] <- 0

Petr

> For example, suppose I have a vector with some valid values, while
> other values are NA. If I matrix-pre-multiply this by a weight 
> row vector, whose weights that correspond to the NAs are zero,
> the outcome will still be NA:
> 
> x <- c(1, NA, 1)
> wt <- c(2, 0, 1)
> wt %*% x # NA
> 
> Alberto Monteiro
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From jose.sierra at integromics.com  Mon Mar  5 16:07:14 2007
From: jose.sierra at integromics.com (Jose Sierra)
Date: Mon, 05 Mar 2007 16:07:14 +0100
Subject: [R] RJDBC
Message-ID: <45EC3222.2080301@integromics.com>

I need help.

I'm trying to connect with an Oracle DBMS and MySQL DBMS, I'm using 
RJDBC package.

My code is the next:

library('rJava')
library('DBI')
library('RJDBC')

//Mysql
drv <- 
JDBC("com.mysql.jdbc.Driver","C:\\Temporal\\mysql-connector-java-3.0.9-stable-bin.jar","'") 

conn <- dbConnect(drv, "jdbc:mysql://localhost:3306/bd", "user", 
"password")

//Oracle
drv <- 
JDBC("oracle.jdbc.driver.OracleDriver","C:\\Temporal\\classes12.jar","'")
conn <- 
dbConnect(drv,"jdbc:oracle:thin:@192.168.1.70:1521:SDS22","user","password") 


R always returns for oracle
"Error en .local(drv, ...) : Unable to connect JDBC to 
jdbc:oracle:thin:@192.168.1.70:1521:SDS22"
and for mysql
"Error en .local(drv, ...) : Unable to connect JDBC to 
jdbc:mysql://localhost:3306/bd"

And the function summary(drv) returns:
JDBCDriver
name = JDBC
driver.version = 0.1-1
DBI.version = 0.1-1
client.version = NA
max.connections = NA

R version 2.4.1 (2006-12-18) i386-pc-mingw32
locale:
LC_COLLATE=Spanish_Spain.1252;LC_CTYPE=Spanish_Spain.1252;LC_MONETARY=Spanish_Spain.1252;LC_NUMERIC=C;LC_TIME=Spanish_Spain.1252 


attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  
"methods"  [7] "base"    
other attached packages:
  RJDBC      DBI    rJava "0.1-2" "0.1-12" "0.4-14"
Can you help me, please?

Another question:
I try to compile ROracle and RMysql for windows but i need Rdll.lib and 
it need R.exp. Can you give me one of this files?


Regards.
Jose Sierra


		
______________________________________________ 
LLama Gratis a cualquier PC del Mundo. 
Llamadas a fijos y m?viles desde 1 c?ntimo por minuto. 
http://es.voice.yahoo.com


From ted.harding at nessie.mcc.ac.uk  Mon Mar  5 16:07:35 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 05 Mar 2007 15:07:35 -0000 (GMT)
Subject: [R] 0 * NA = NA
In-Reply-To: <20070305141939.M7169@centroin.com.br>
Message-ID: <XFMail.070305150735.ted.harding@nessie.mcc.ac.uk>

On 05-Mar-07 Alberto Monteiro wrote:
> Is there any way to "force" 0 * NA to be 0 instead of NA?
> 
> For example, suppose I have a vector with some valid values, while
> other values are NA. If I matrix-pre-multiply this by a weight 
> row vector, whose weights that correspond to the NAs are zero,
> the outcome will still be NA:
> 
> x <- c(1, NA, 1)
> wt <- c(2, 0, 1)
> wt %*% x # NA
> 
> Alberto Monteiro

This is a bit of a tricky one, especially in a more general context.
I think it involves defining new operators.

In the case of the particular operation in your example, you could do

"%*NA%" <- function(x,y){
  X<-x;X[(is.na(x))&(y==0)]<-0;
  Y<-y;Y[(is.na(Y))&(x==0)]<-0;
  return(X%*%Y)
}

Then:

  x <- c(1, NA, 1)
  wt <- c(2, 0, 1)

  x %*NA% wt
       [,1]
  [1,]    3


Hmmm!
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-Mar-07                                       Time: 15:07:19
------------------------------ XFMail ------------------------------


From r.hankin at noc.soton.ac.uk  Mon Mar  5 16:14:58 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 5 Mar 2007 15:14:58 +0000
Subject: [R] enumerating non-overlapping pairs of elements from a vector
In-Reply-To: <1173106588.6666.19.camel@rosa>
References: <1173106588.6666.19.camel@rosa>
Message-ID: <74D94144-D6C7-45FD-9FF8-8887A1924816@soc.soton.ac.uk>

Allan

the general problem you refer to is "set partitions", although
I'm not clear whether the order of the sets themselves
makes a  difference (we in the enumerative combinatorics
world refer to "indistinguishable boxes").

Your application would be set partitions with a specific shape,
in this case 2,2,2,...,2,2,1  or 2,2,2,,,,,2.

I am working on a generalization of your problem Right Now,
and hope to have a complete solution ready within a couple
of months (but then again I've been saying this for a long time
now ;-)


What's your application?


best wishes

Robin


On 5 Mar 2007, at 14:56, Allan Strand wrote:

> Hi All,
>
> I'm trying to come up with a clear and concise (and fast?) solution to
> the following problem.
>
> I would like to take a vector 'v' and enumerate all of the ways in
> which it can be broken into n sets of length 2 (if the length of the
> vector is odd, and an additional set of length 1).  An element of 'v'
> can
> only appear in one set. Order within sets is not important.  Vector
> 'v' can be of lengths 2-12
>
>  'n' is determined by length(v)%/%2
>  if length(v)%%2 is non-zero, the additional set of length 1 is used
>
> For example vector 'v':
> v = (1,2,3,4)
>
> The solution would be (rows are combinations of sets chosen, where
> each element only appears once)
>
> 1 2, 3 4
> 1 3, 2 4
> 1 4, 2 3
>
> In the case where length(v) is odd
> v = (1,2,3,4,5)
> 1 2, 3 4, 5
> 1 3, 2 4, 5
> 1 4, 2 3, 5
> 5 2, 3 4, 1
> 5 3, 2 4, 1
> 5 4, 2 3, 1
> 5 1, 3 4, 2
> 5 3, 1 4, 2
> 5 4, 1 3, 2
> and so on...
>
> Certainly pulling all combinations of two or one elements is not a big
> deal, for example
>
> combinations(5,2,c(1,2,3,4,5),repeats.allowed=T)
>
> from the 'gtools' package would do something like this.
>
> I'm stuck on a clean solution for enumerating all the non-overlapping
> sets without some elaborate looping and checking scheme.  No doubt
> this is a lapse in my understanding of combinatorics.  Any help would
> be greatly appreciated
>
> cheers,
> a.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From ccleland at optonline.net  Mon Mar  5 16:29:05 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 05 Mar 2007 10:29:05 -0500
Subject: [R] Identifying points in a plot that have duplicate values
In-Reply-To: <132d601c75f28$4176e530$8dca010a@mail2world.com>
References: <132d601c75f28$4176e530$8dca010a@mail2world.com>
Message-ID: <45EC3741.9050402@optonline.net>

David Lloyd wrote:
> I have code like this: - 
> 
> #-----------------------------------------------------------------------
> ------------------------------------------------------
> 
> x=scan()
> 0 0 0 0 0 1 2 3 4
> 
> y=scan()
> 1 1 1 2 2 1 3 4 5
> 
> plot(x,y)
> 
> identify(0,1,3) #Allows me to select manually to identify co-ordinate
> (0,1) as being duplicated 3 times
> identify(0,2,2) #Allows me to select manually to identify co-ordinate
> (0,2) as being duplicated 2 times
> #-----------------------------------------------------------------------
> ------------------------------------------------------
> 
> Is there not a way I can automatically display if points are duplicated
> and by how many times?
> 
> I thought if I 'jittered' the points ever so slightly I could get an
> idea of how many duplicates there are but with >100 points the graph
> looks very messy.

  You might consider using alpha transparency - the more times a point
is duplicated the darker it will be.  For example:

df <- data.frame(x=c(0, 0, 0, 0, 0, 1, 2, 3, 4),
                 y=c(1, 1, 1, 2, 2, 1, 3, 4, 5))

pdf("alphaExample.pdf", version = "1.4", width = 6, height = 6)

with(df, plot(x,y, col=rgb(1,0,0,.3), pch=16))

dev.off()

RSiteSearch("alpha transparency")

> Regards
> DaveL
> 
> 
> 
> 
> 
> 
> 
> 
> Click for free info on getting an MBA and make $200K/ year
> 
> 
> 
> 
> Need cash? Click to get a payday loan
> <http://tagline.bidsystem.com/fc/CAaCDCZ60nyjrrOboFeUJgRjigwgNftK/> 
> 
> 
> 
> <span id=m2wTl><p><font face="Arial, Helvetica, sans-serif" size="2" style="font-size:13.5px">_______________________________________________________________<BR>Get the Free email that has everyone talking at <a href=http://www.mail2world.com target=new>http://www.mail2world.com</a><br>  <font color=#999999>Unlimited Email Storage &#150; POP3 &#150; Calendar &#150; SMS &#150; Translator &#150; Much More!</font></font></span>
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From mothsailor at googlemail.com  Mon Mar  5 16:29:17 2007
From: mothsailor at googlemail.com (David Barron)
Date: Mon, 5 Mar 2007 15:29:17 +0000
Subject: [R] Identifying points in a plot that have duplicate values
In-Reply-To: <132d601c75f28$4176e530$8dca010a@mail2world.com>
References: <132d601c75f28$4176e530$8dca010a@mail2world.com>
Message-ID: <815b70590703050729q34308ff8q487483a343ab4807@mail.gmail.com>

Have a look at ?sunflowerplot, which not only produces a scatterplot
showing multiple points with the same coordinates using special
symbols, but will also produce a list showing the number of points at
each coordinate as well.

On 05/03/07, David Lloyd <DavidLloyd at mail2lloyd.com> wrote:
> I have code like this: -
>
> #-----------------------------------------------------------------------
> ------------------------------------------------------
>
> x=scan()
> 0 0 0 0 0 1 2 3 4
>
> y=scan()
> 1 1 1 2 2 1 3 4 5
>
> plot(x,y)
>
> identify(0,1,3) #Allows me to select manually to identify co-ordinate
> (0,1) as being duplicated 3 times
> identify(0,2,2) #Allows me to select manually to identify co-ordinate
> (0,2) as being duplicated 2 times
> #-----------------------------------------------------------------------
> ------------------------------------------------------
>
> Is there not a way I can automatically display if points are duplicated
> and by how many times?
>
> I thought if I 'jittered' the points ever so slightly I could get an
> idea of how many duplicates there are but with >100 points the graph
> looks very messy.
>
> Regards
> DaveL
>
>
>
>
>
>
>
>
> Click for free info on getting an MBA and make $200K/ year
>
>
>
>
> Need cash? Click to get a payday loan
> <http://tagline.bidsystem.com/fc/CAaCDCZ60nyjrrOboFeUJgRjigwgNftK/>
>
>
>
> <span id=m2wTl><p><font face="Arial, Helvetica, sans-serif" size="2" style="font-size:13.5px">_______________________________________________________________<BR>Get the Free email that has everyone talking at <a href=http://www.mail2world.com target=new>http://www.mail2world.com</a><br>  <font color=#999999>Unlimited Email Storage &#150; POP3 &#150; Calendar &#150; SMS &#150; Translator &#150; Much More!</font></font></span>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From dieter.menne at menne-biomed.de  Mon Mar  5 16:31:20 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Mon, 5 Mar 2007 15:31:20 +0000 (UTC)
Subject: [R] logistic regression on contingency table
References: <90076531-9050-4090-95A7-0CED6786FDDB@bcm.tmc.edu>
Message-ID: <loom.20070305T162830-235@post.gmane.org>

Bingshan Li <bli1 <at> bcm.tmc.edu> writes:

> I am wondering if there is a way in R to fit logistic regression on  
> contingency table. If I have original data, I can transform the data  
> into a design matrix and then call glm to fit the regression. But now  
> I have a 2x3 contingency table with first row for response 0 and  
> second row for response 1, and the columns are 3 levels of predictor  
> variable. The 3 levels are not ordinal though and indicator variables  
> would be more appreciate.

>From Documentation of GLM:

For binomial and quasibinomial families the response can also be specified 
as a factor (when the first level denotes failure and all others success) 
or as a two-column matrix with the columns giving the numbers of successes 
and failures.  
....

Dieter Menne


From tlumley at u.washington.edu  Mon Mar  5 16:51:16 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 5 Mar 2007 07:51:16 -0800 (PST)
Subject: [R] Mitools and lmer
In-Reply-To: <40e66e0b0703030813u3bf4daebm4743fd233ff23ed1@mail.gmail.com>
References: <cbbcbd790703021440g4c0e287o713ee14f1e472285@mail.gmail.com>
	<cbbcbd790703021443y22d32cf0j49c5e623035e2b2d@mail.gmail.com>
	<40e66e0b0703030813u3bf4daebm4743fd233ff23ed1@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0703050747310.29820@homer23.u.washington.edu>


Doug,

It's mitools, not mltools. I wrote it.

I think the problem is just that coef() is not the right function for 
getting the fixed effects.  Beth wants
   betas <- MIextract(model0, fun=fixef)

 	-thomas



On Sat, 3 Mar 2007, Douglas Bates wrote:

> On 3/2/07, Beth Gifford <beth.gifford at duke.edu> wrote:
>> Hey there
>> I am estimating a multilevel model using lmer.  I have 5 imputed datasets so
>> I am using mitools to pool the estimates from the 5
>> datasets.  Everything seems to work until I try to use
>> MIcombine to produced pooled estimates.  Does anyone have any suggestions?  The betas and the standard errors were extracted with no problem so everything seems to work smoothly up until that point.
>
> I'm not familiar with the mltools package and I didn't see it listed
> in the CRAN packages.  Can you provide a reference or a link to the
> package?
>
>>> Program
>>> #Read data
>>> data.dir<-system.file("dta",package="mitools")
>>> files.imp<-imputationList(lapply(list.files(data.dir,
>>> pattern="imp.\\.dta", full=TRUE), read.dta))
>>>
>>> #estimate model over each imputed dataset
>>> model0<-with(files.imp,lmer( erq2tnc ~1+trt2+nash+wash+male+coh2+coh3+(1 |
>>> sitebeth)))
>>> #extract betas and standard errors
>>> betas<-MIextract(model0,fun=coef)
>>> vars<-MIextract(model0,fun=vcov)
>>> #Combine the results
>>> summary(MIcombine(betas,vars))
>
>>> Error in cbar + results[[i]] : non-numeric argument to binary operator
>>> Error in summary(MIcombine(betas, vars)) :
>>> error in evaluating the argument 'object' in selecting a method for
>>> function 'summary'
>
> First use traceback() to discover where the (first) error occurred.
> My guess is that Mlcombine expects a particular type of object for the
> vars argument and it is not getting that type (and not checking for
> the correct type).
>
>>
>>
>>
>> Thanks
>> Beth
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From ted.harding at nessie.mcc.ac.uk  Mon Mar  5 16:53:57 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 05 Mar 2007 15:53:57 -0000 (GMT)
Subject: [R] 0 * NA = NA
In-Reply-To: <45EC30EE.8030305@karlin.mff.cuni.cz>
Message-ID: <XFMail.070305155357.ted.harding@nessie.mcc.ac.uk>

On 05-Mar-07 Petr Klasterecky wrote:
> Alberto Monteiro napsal(a):
>> Is there any way to "force" 0 * NA to be 0 instead of NA?
> 
> No (AFAIK), and it is pretty reasonable to define it this way.
> If you want to treat the NAs as zeros, use
> x[is.na(x)] <- 0

Doing it in precisely that way would have the problem that it
would not give you NA when it should. For example:


x <- c(1, NA, 1)
wt <- c(2, 1, 1)

Then, after x[is.na(x)] <- 0, the result of x %*% wt should be NA,
but your method would give 3. This is why I suggested a method
which tests for corresponding elements of x = NA and y = 0, since
what Alberto Monteiro wanted was 0*NA = 0, when that combination
occures. I.e.

"%*NA%" <- function(x,y){
  X<-x;X[(is.na(x))&(y==0)]<-0;
  Y<-y;Y[(is.na(y))&(x==0)]<-0;
  return(X%*%Y)
}

Ted.

> Petr
> 
>> For example, suppose I have a vector with some valid values, while
>> other values are NA. If I matrix-pre-multiply this by a weight 
>> row vector, whose weights that correspond to the NAs are zero,
>> the outcome will still be NA:
>> 
>> x <- c(1, NA, 1)
>> wt <- c(2, 0, 1)
>> wt %*% x # NA
>> 
>> Alberto Monteiro
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> -- 
> Petr Klasterecky
> Dept. of Probability and Statistics
> Charles University in Prague
> Czech Republic
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-Mar-07                                       Time: 15:53:53
------------------------------ XFMail ------------------------------


From jholtman at gmail.com  Mon Mar  5 16:58:23 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 5 Mar 2007 10:58:23 -0500
Subject: [R] Identifying last record in individual growth data over
	different time intervalls
In-Reply-To: <45EBDD10.9060502@uct.ac.za>
References: <45EBDD10.9060502@uct.ac.za>
Message-ID: <644e1f320703050758y72a4ed3hf2445dff390280cf@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070305/a936c525/attachment.pl 

From jholtman at gmail.com  Mon Mar  5 17:03:46 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 5 Mar 2007 11:03:46 -0500
Subject: [R] Identifying last record in individual growth data over
	different time intervalls
In-Reply-To: <45EBDD10.9060502@uct.ac.za>
References: <45EBDD10.9060502@uct.ac.za>
Message-ID: <644e1f320703050803g5f000e31u4a81310cf7f74a29@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070305/4779cf16/attachment.pl 

From beth.gifford at duke.edu  Mon Mar  5 17:09:49 2007
From: beth.gifford at duke.edu (Beth Gifford)
Date: Mon, 5 Mar 2007 11:09:49 -0500
Subject: [R] Mitools and lmer
In-Reply-To: <Pine.LNX.4.64.0703050747310.29820@homer23.u.washington.edu>
References: <cbbcbd790703021440g4c0e287o713ee14f1e472285@mail.gmail.com>
	<cbbcbd790703021443y22d32cf0j49c5e623035e2b2d@mail.gmail.com>
	<40e66e0b0703030813u3bf4daebm4743fd233ff23ed1@mail.gmail.com>
	<Pine.LNX.4.64.0703050747310.29820@homer23.u.washington.edu>
Message-ID: <cbbcbd790703050809w3b007682m9e318e35b56e29eb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070305/aed296a9/attachment.pl 

From jholtman at gmail.com  Mon Mar  5 17:27:12 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 5 Mar 2007 11:27:12 -0500
Subject: [R] How to read in this data format?
In-Reply-To: <BAY134-F1E3BE5B6DFA3094C8A04BD8840@phx.gbl>
References: <971536df0703011346u6d65f1faid814798e98a3e10d@mail.gmail.com>
	<BAY134-F1E3BE5B6DFA3094C8A04BD8840@phx.gbl>
Message-ID: <644e1f320703050827v4965436en271519c7c6c93c7b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070305/0488c670/attachment.pl 

From jholtman at gmail.com  Mon Mar  5 17:32:00 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 5 Mar 2007 11:32:00 -0500
Subject: [R] plot(): I want to display dates on X-axis.
In-Reply-To: <d4327f7e0703042356g7cce6b8bi67a517525c5e30b1@mail.gmail.com>
References: <d4327f7e0703042356g7cce6b8bi67a517525c5e30b1@mail.gmail.com>
Message-ID: <644e1f320703050832j31d8d59cuec20e7c8a5ec9dce@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070305/f863d8a1/attachment.pl 

From sfalcon at fhcrc.org  Mon Mar  5 18:15:39 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Mon, 05 Mar 2007 09:15:39 -0800
Subject: [R] Error loading a dependency in a package: missing namespace?
In-Reply-To: <b028350f0703050320l3ef0fd50uea75d63022adf8b3@mail.gmail.com>
	(Carlos J. Gil Bellosta's message of "Mon,
	5 Mar 2007 11:20:15 +0000")
References: <b028350f0703050320l3ef0fd50uea75d63022adf8b3@mail.gmail.com>
Message-ID: <m2irdf8vd0.fsf@ziti.local>

"Carlos J. Gil Bellosta " <cgb at datanalytics.com> writes:
> import(methods, Biobase, outliers)

> * checking whether the package can be loaded ... ERROR
> Loading required package: Biobase
> Loading required package: tools
>
> Welcome to Bioconductor
>
>    Vignettes contain introductory material. To view, type
>    'openVignette()' or start with 'help(Biobase)'. For details
>    on reading vignettes, see the openVignette help page.
>
> Loading required package: outliers
> Error in loadNamespace(package, c(which.lib.loc, lib.loc), keep.source
> = keep.source) :
>        in 'pcrAnalysis' classes for export not defined: pcrExprSet
> In addition: Warning message:
> package 'pcrAnalysis' contains no R code in: loadNamespace(package,
> c(which.lib.loc, lib.loc), keep.source = keep.source)
> Error: package/namespace load failed for 'pcrAnalysis'
> Execution halted
>
> It seems that the error is related to something having to do with
> namespaces. The thing is that package "outliers" does not have a
> NAMESPACE file. Could this be an issue?

Yes, you cannot do import(pkg) in the NAMESPACE file if pkg doesn't
itself have a NAMESPACE file.

So try just removing that from your NAMESPACE file.

+ seth

-- 
Seth Falcon | Computational Biology | Fred Hutchinson Cancer Research Center
http://bioconductor.org


From albmont at centroin.com.br  Mon Mar  5 18:22:43 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Mon, 5 Mar 2007 15:22:43 -0200
Subject: [R] 0 * NA = NA
In-Reply-To: <XFMail.070305155357.ted.harding@nessie.mcc.ac.uk>
References: <45EC30EE.8030305@karlin.mff.cuni.cz>
	<XFMail.070305155357.ted.harding@nessie.mcc.ac.uk>
Message-ID: <20070305170524.M55915@centroin.com.br>

Ted Harding wrote:
>
>>> Is there any way to "force" 0 * NA to be 0 instead of NA?
>> 
>> No (AFAIK), and it is pretty reasonable to define it this way.
>> If you want to treat the NAs as zeros, use
>> x[is.na(x)] <- 0
> 
> Doing it in precisely that way would have the problem that it
> would not give you NA when it should. For example:
> 
> x <- c(1, NA, 1)
> wt <- c(2, 1, 1)
> 
> Then, after x[is.na(x)] <- 0, the result of x %*% wt should be NA,
> but your method would give 3.
>
That's precisely my thought - since you may have read my thoughts,
it's time to recalibrate my alluminium helmet.

But I also thought about something else. What is the meaning of NA? 
NA is a _missing_ value, and is.infinite(NA) returns FALSE [OTOH, 
is.finite(NA) returns FALSE too - this is weird]. A missing value
times zero is zero. OTOH, 1/NA is NA, so NA could mean Inf.

Maybe binary logic can't adequately handle such ideas :-/

if (NA == 0) is NA, then is.finite(NA) should be NA too...

if (NA == 0) 2 else 3 # gives an error

> This is why I suggested a method
> which tests for corresponding elements of x = NA and y = 0, since
> what Alberto Monteiro wanted was 0*NA = 0, when that combination
> occures. I.e.
> 
> "%*NA%" <- function(x,y){
>   X<-x;X[(is.na(x))&(y==0)]<-0;
>   Y<-y;Y[(is.na(y))&(x==0)]<-0;
>   return(X%*%Y)
> }
> 
This method is fine. I had already done something similar

Of course, the problem begins to grow if we want, for example,
to use elementary matrices to transform a matrix. The 2x2 matrix
that switches two lines, rbind(c(0,1), c(1,0)) will not switch
a matrix with NAs:

switch <- rbind(c(0,1), c(1,0))
testmatrix <- rbind(c(1,2,3,4), c(5,6,7,8))
switch %*% testmatrix # ok
testmatrix[2,2] <- NA
switch %*% testmatrix # not ok

But I digress...

Alberto Monteiro


From stubben at lanl.gov  Mon Mar  5 18:37:14 2007
From: stubben at lanl.gov (Chris Stubben)
Date: Mon, 5 Mar 2007 17:37:14 +0000 (UTC)
Subject: [R] Identifying last record in individual growth data over
	different time intervalls
References: <45EBDD10.9060502@uct.ac.za>
Message-ID: <loom.20070305T183337-962@post.gmane.org>


> Finally I would like to have a data.frame t2 which only contains the 
> entries of the last measurements.
> 

You could also use aggregate to get the max year per plate then join that back
to the original dataframe using merge on year and plate (common columns in both
dataframes).



x<-data.frame(id=(1:8), plate=c(15,15,15,20,20,33,43,43),
year=c(2004,2005,2006,2004,2005,2004,2005,2006), 
height=c(0.40,0.43,0.44,0.90,0.94,0.15,0.30,0.38))

merge(x, aggregate(list(year=x$year), list(plate=x$plate), max))


  plate year id height
1    15 2006  3   0.44
2    20 2005  5   0.94
3    33 2004  6   0.15
4    43 2006  8   0.38


From amnakhan493 at gmail.com  Mon Mar  5 18:40:31 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Mon, 5 Mar 2007 09:40:31 -0800
Subject: [R] Fwd: RFA and nsRFA
In-Reply-To: <3ffd3bb60702250837w440481a1m2c55cf885da1839d@mail.gmail.com>
References: <3ffd3bb60702250837w440481a1m2c55cf885da1839d@mail.gmail.com>
Message-ID: <3ffd3bb60703050940w2687640ame1f6fc1135d257cb@mail.gmail.com>

---------- Forwarded message ----------
From: amna khan <amnakhan493 at gmail.com>
Date: Feb 25, 2007 8:37 AM
Subject: RFA and nsRFA
To: R-help at lists.r-project.org, R-help at stat.math.ethz.ch


Dear Sir
There are two packages of regional frequency analysis RFA and nsRFA.
Are both give us same results if not then what you will suggest.
I am confused about this.
Please guid me in this regard
AMINA

-- 
AMINA SHAHZADI
Department of Statistics
GC University Lahore, Pakistan.
Email:
amnakhan493 at gmail.com
amna_989 at hotmail.com
amna_989 at yahoo.com

-- 
AMINA SHAHZADI
Department of Statistics
GC University Lahore, Pakistan.
Email:
amnakhan493 at gmail.com
amna_989 at hotmail.com
amna_989 at yahoo.com


From amnakhan493 at gmail.com  Mon Mar  5 18:41:08 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Mon, 5 Mar 2007 09:41:08 -0800
Subject: [R] Fwd: nsRFA
In-Reply-To: <3ffd3bb60702250844t15a3b6acve4fcfedfc7ed7736@mail.gmail.com>
References: <3ffd3bb60702250844t15a3b6acve4fcfedfc7ed7736@mail.gmail.com>
Message-ID: <3ffd3bb60703050941n61d69879rc31725193033c567@mail.gmail.com>

---------- Forwarded message ----------
From: amna khan <amnakhan493 at gmail.com>
Date: Feb 25, 2007 8:44 AM
Subject: nsRFA
To: R-help at stat.math.ethz.ch, R-help at lists.r-project.org


Dear Sir
I am not understanding the HOMTESTS in package nsRFA. Is vector x is
the data from all sites combined combined in one vector? How to assign
"cod"?

Your help is really appreciable
Regards
AMINA

-- 
AMINA SHAHZADI
Department of Statistics
GC University Lahore, Pakistan.
Email:
amnakhan493 at gmail.com
amna_989 at hotmail.com
amna_989 at yahoo.com

-- 
AMINA SHAHZADI
Department of Statistics
GC University Lahore, Pakistan.
Email:
amnakhan493 at gmail.com
amna_989 at hotmail.com
amna_989 at yahoo.com


From amnakhan493 at gmail.com  Mon Mar  5 18:41:50 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Mon, 5 Mar 2007 09:41:50 -0800
Subject: [R] Fwd: RFA
In-Reply-To: <3ffd3bb60702250851w575442cat6b16d1aefb290f8f@mail.gmail.com>
References: <3ffd3bb60702250851w575442cat6b16d1aefb290f8f@mail.gmail.com>
Message-ID: <3ffd3bb60703050941q64bbc0ceob6aa36113b0aa15b@mail.gmail.com>

---------- Forwarded message ----------
From: amna khan <amnakhan493 at gmail.com>
Date: Feb 25, 2007 8:51 AM
Subject: RFA
To: R-help at stat.math.ethz.ch, R-help at lists.r-project.org


Dear Sir in the following example,is the vector lmom a l-moment ratios
vector? What is meant by size = northCascades[,1]? And what are the
values in c(0.0104,0.0399,0.0405 )?

Please help me I am unable to understand these from help manual.

Best Regards
AMINA




data(northCascades)

lmom <- c(1, 0.1103, 0.0279, 0.1366)

kappaParam <- kappalmom(lmom)

heterogeneity(500, 19, size = northCascades[,1],

kappaParam, c(0.0104, .0339, .0405))

##The heterogeneity statistics given by Hosking for this case

##study are H1 = 0.62, H2 = -1.49 and H3 = -2.37

##Taking into account sample variability, results should be

##consistent

-- 
AMINA SHAHZADI
Department of Statistics
GC University Lahore, Pakistan.
Email:
amnakhan493 at gmail.com
amna_989 at hotmail.com
amna_989 at yahoo.com

-- 
AMINA SHAHZADI
Department of Statistics
GC University Lahore, Pakistan.
Email:
amnakhan493 at gmail.com
amna_989 at hotmail.com
amna_989 at yahoo.com


From Cameron.Guenther at MyFWC.com  Mon Mar  5 18:49:35 2007
From: Cameron.Guenther at MyFWC.com (Guenther, Cameron)
Date: Mon, 5 Mar 2007 12:49:35 -0500
Subject: [R] Matrix/dataframe indexing
Message-ID: <BA6FF017E924044A9BF748AFAEEA6F30015E9C31@FWC-TLEX3.fwc.state.fl.us>

Hi all, 
I am hoping someone can help me out with this:

If I have dataframe of years and ages and the first column and first row
are filled with leading values:

Df<-  	age1	age2	age3
	Yr1	1 	0.4 	0.16
      Yr2	1.5	0	0
	Yr3	0.9	0	0
	Yr4	1	0	0	
	Yr5	1.2	0	0
	Yr6	1.4	0	0
	Yr7	0.8	0	0
	Yr8	0.6	0	0
	Yr9	1.1	0	0

Now the rest of the cells need to be filled according to the previous
year and age cell so arbitrarily, cell [2,2] should be value in cell
[1,1] * exp(0.3), and cell [2,3] should be the value in cell [1,2]*
exp(0.3), etc.

How do I write the for loop so that it will calculate the missing cell
values over both dimensions of the dataframe?

Thanks in advance	

Cameron Guenther, Ph.D.
100 8th Ave. SE
St. Petersburg, Fl 33701
727-896-8626 ext. 4305
cameron.guenther at myfwc.com 

From jholtman at gmail.com  Mon Mar  5 19:02:38 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 5 Mar 2007 13:02:38 -0500
Subject: [R] Interface to round robin databases (RRDtool)
Message-ID: <644e1f320703051002k2970f779u6dfe898c85081388@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070305/a106f98d/attachment.pl 

From scripcaterer at mfsyork.com  Mon Mar  5 21:17:47 2007
From: scripcaterer at mfsyork.com (Suzette Windham)
Date: Mon, 5 Mar 2007 19:17:47 -0060
Subject: [R] It's me Suzette.
Message-ID: <01c75f5a$f54c4460$6c822ecf@scripcaterer>

PUT THIS ONE ON YOUR HOT LIST.    
You don't saw what was happen ?
Thursday volume of ACEN has exceeded all records.     
As you can see, investors finally valued the product and started to invest.   

Pick ACEN   
Last $0.63.     
Vol 302400      

AC Energy would like to provide further information on business strategies and encouraging market opportunities.
AC Energy is pleased to announce overwhelming interest from industry giants from around the globe, seeking their revolutionary uninterrupted power supply technology. AC Energy?s team is working hard to correspond with all interested parties. So far AC Energy is pleased to announce that they have secured the first letter of intent for an agency license with Innovative Energy and Environmental Solutions (IES) of New Jersey. IES? goal is to treat AC Energy as a business partner, to combine their excellent, in depth knowledge of evolving energy technologies, with a commitment to the successful introduction of AC Energy?s new and patented battery technology.





















> is very clearly the joke of the big online rooms.  
I realize he doesn't write the software.  
Shrink your swap file size.  
> RR 
> Change things fast or lose business bigtime.  
> RR 
With that in mind to whom should I have addressed my dissatisfation.  
With that in mind to whom should I have addressed my dissatisfation.  
Now my take on a customer explaining.  
A set of symptoms in which a huge swap file is exhausted is to take a real close look at memory leaks.


From Rob.Calver at informa.com  Mon Mar  5 17:23:07 2007
From: Rob.Calver at informa.com (Calver, Rob)
Date: Mon, 5 Mar 2007 16:23:07 -0000
Subject: [R] ANNOUNCEMENT: 20% Discount on R books from Chapman & Hall/CRC
	Press
Message-ID: <5C8D1755908F324C9EF0207518D99AB20BF838E0@UKEXBE01.UK.CorpLAN.net>

Take advantage of a 20% discount on the most recent R books from Chapman & Hall/CRC!

Chapman and Hall/CRC is pleased to offer our latest books on R - all available through our website at a 20% discount to users of the software. To take advantage of this permanent offer, that is valid across the board for all of our R books, simply visit http://www.crcpress.com/, choose your titles, and insert the online discount code - 585HHXXXX - in the 'Promotion Code' field at checkout. 

Please note: this offer is permanent but is currently in addition to the 10% promotion running on our website until March 17th. So all prices below are good until that date and represent a 28% discount! Standard shipping is also free!

*** NEW TITLES ***

Analysis of Correlated Data with SAS and R, Third Edition

Mohamed M. Shoukri, King Faisal Specialist Hospital & Res. Ctr, Riyadh, Saudi Arabia
Mohammad A. Chaudhary, Department of International Health, Baltimore, MD, USA

Publication Date: 5/17/2007
Number of Pages: 320

This bestselling resource is one of the first books to discuss the methodologies used for the analysis of clustered and correlated data. It focuses on the analysis of correlated data from epidemiologic and medical investigations, details the statistical analysis of cross-classified data, and covers time series, repeated measures, and logistic regression data. This new edition includes R code for almost all the examples and a CD-ROM with all the datasets and SAS and R code.

Discounted Price: $64.79 / ?35.99 

For more details and to order:
http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C6196

***  

Correspondence Analysis in Practice, Second Edition

Michael Greenacre, Universitat Pompeu Fabra, Barcelona, Spain 

Publication Date: 5/7/2007
Number of Pages: 264

Presenting a comprehensive introduction that employs a practical approach to theory and applications, this new edition provides a descriptive and exploratory statistical technique to analyze simple two-way and multi-way tables. It features a large number of examples and case studies with an emphasis on applications in marketing and the social and environmental sciences. Divided into self-contained module sections, the book provides objectives and summaries of theory in each chapter. All analyses are performed using R through the ca package created by the author, a leading authority on the topic. Datasets are also available for download via the Web. 

Discounted Price: $57.56 / ?28.79

For more details and to order:
http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C6161

***

Niche Modeling: Predictions from Statistical Distributions

David Stockwell, University of California San Diego, La Jolla, California, USA

Publication Date: 12/15/2006
Number of Pages: 224

Using theory, applications, and examples of inferences, this book demonstrates how to conduct and evaluate niche modeling projects in any area of application. It features a series of theoretical and practical exercises for developing and evaluating niche models using R. The author discusses applications of predictive modeling methods with reference to valid inferences from assumptions. He elucidates varied and simplified examples with rigor and completeness. Topics include geographic information systems, multivariate modeling, artificial intelligence methods, data handling, and information infrastructure.

Discounted Price: $64.79 / ?35.99 

For more details and to order:
http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C4940 

***

Linear Mixed Models: A Practical Guide Using Statistical Software

Brady T. West, Kathleen B. Welch, Andrzej T. Galecki, with contributions from Brenda W. Gillespie, University of Michigan, Ann Arbor, USA

Publication Date: 11/22/2006
Number of Pages: 376

Simplifying the often confusing array of software programs for fitting linear mixed models (LMMs), Linear Mixed Models: A Practical Guide Using Statistical Software provides a basic introduction to primary concepts, notation, software implementation, model interpretation, and visualization of clustered and longitudinal data. This easy-to-navigate reference details the use of procedures for fitting LMMs in five popular statistical software packages: SAS, SPSS, Stata, R/S-plus, and HLM.

Discounted Price: $57.56 / ?32.39

For more details and to order:
http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C4800

*** CURRENT BESTSELLERS! ***

A Handbook of Statistical Analyses Using R 

Brian S. Everitt, Institute of Psychiatry, King's College, London, UK 
Torsten Hothorn, Institut f?r Medizininformatik, Biometrie und Epidemiologie, Erlangen, Germany

Publication Date: 2/17/2006
Number of Pages: 275

>From simple inference to recursive partitioning and cluster analysis, this book methodically leads readers through the necessary steps, commands, and interpretation of results - addressing theory and statistical background only when useful or necessary. Beginning with an introduction to R, it discusses the syntax, general operators, and basic data manipulation while summarizing the most important features. Numerous figures highlight R's strong graphical capabilities and exercises at the end of each chapter reinforce the techniques and concepts presented. All data sets and code used in the book are available as a downloadable package from CRAN, the R online archive.

Discounted Price: $35.96 / ?21.59

For more details and to order:
http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C5394

***

Extending Linear Models with R: Generalized Linear, Mixed Effects and Nonparametric Regression Models

Julian J. Faraway, University of Bath, UK 

Publication Date: 12/20/2005
Number of Pages: 312

This book surveys the techniques that grow from the regression model, presenting three extensions to that framework: generalized linear models (GLMs), mixed effect models, and nonparametric regression models. The author's treatment is thoroughly modern and covers topics that include GLM diagnostics, generalized linear mixed models, trees, and even the use of neural networks in statistics. To demonstrate the interplay of theory and practice, throughout the book the author weaves the use of the R software environment to analyze the data of real examples, providing all of the R commands necessary to reproduce the analyses.

Discounted Price: $57.56 / ?32.39

For more details and to order:
http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C424X

***

R Graphics

Paul Murrell, The University of Auckland, New Zealand 

Publication Date: 7/29/2005
Number of Pages: 301

Considered the leading expert on the use of R graphics, Murrell gives statisticians the first complete reference on the R graphical system. An in-depth text that takes nothing for granted, it helps both neophytes and seasoned statisticians master the intricacies of R. No other published volume contains information on R's grid graphics, and much of the information the author provides is original material not to be found anywhere else. Ahead of the curve, this volume defines the direction for the future of statistical graphical development.

Discounted Price: $53.96 / ?30.95

For more details and to order:
http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C486X

*** OTHER TITLES OF INTEREST FEATURING R! ***

Generalized Additive Models: An Introduction with R 

Simon N. Wood, University of Bath, UK

Publication Date: 2/27/2006
Number of Pages: 416

This book provides a thorough understanding of the theory and practical applications of GAMs and related advanced models. The author bases his approach on a framework of penalized regression splines, and builds a well-grounded foundation through motivating chapters on linear and generalized linear models. While firmly focused on the practical aspects of GAMs, discussions include fairly full explanations of the theory underlying the methods. Use of the freely available R software helps explain the theory and illustrates the practicalities of linear, generalized linear, and generalized additive models, as well as their mixed effect extensions.

Discounted Price: $57.56 / ?28.79

For more details and to order:
http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C4746

***

Linear Models with R

Julian J. Faraway, University of Bath, UK 

Publication Date: 8/12/2004
Number of Pages: 240

Focusing on the practice of regression and analysis of variance, this book clearly demonstrates the different methods available and in which situations each one applies. It covers all of the standard topics, from the basics of estimation to missing data, factorial designs, and block designs, but it also includes discussion of topics, such as model uncertainty, rarely addressed in books of this type. The presentation incorporates numerous examples that clarify both the use of each technique and the conclusions one can draw from the results. 

Discounted Price: $53.24 / ?30.23

For more details and to order:
http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C4258

***

Robust Statistical Methods with R

Jana Jureckova, Charles University, Prague, Czech Republic 
Jan Picek, technical University of Liberec, Czech Republic

Publication Date: 11/29/2005
Number of Pages: 216

This book provides a systematic treatment of robust procedures with an emphasis on practical application. The authors work from underlying mathematical tools to implementation, paying special attention to the computational aspects. They cover the whole range of robust methods, including differentiable statistical functions, distance of measures, influence functions, and asymptotic distributions. The book features hands-on problem solving, many examples and computational algorithms using the R software supplementing the discussion. It examines the characteristics of robustness, estimators of real parameter, large sample properties, and goodness-of-fit tests and also includes a brief overview of R in an appendix for those with little experience using the software.

Discounted Price: $61.16 / ?35.27

For more details and to order:
http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C4541

***

Using R for Introductory Statistics

John Verzani, CUNY, College of Staten Island, New York

Publication Date: 11/29/2004
Number of Pages: 432

This reference presents a self-contained treatment of statistical topics and the intricacies of the R software. The pacing is such that students are able to master data manipulation and exploration before diving into more advanced statistical concepts. The book treats exploratory data analysis with more attention than is typical, includes a chapter on simulation, and provides a unified approach to linear models. It lays the foundation for further study and development in statistics using R. Appendices cover installation, graphical user interfaces, and teaching with R, as well as information on writing functions and producing graphics.
 
Discounted Price: $35.24 / ?19.43

For more details and to order:
http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C4509

***

Correspondence Analysis and Data Coding with Java and R

Fionn Murtagh, Royal Holloway University of London, UK 

Publication Date: 5/26/2005
Number of Pages: 256

This book introduces the theory, methods, and applications of correspondence analysis. With an emphasis on data coding, it clearly demonstrates why this technique remains important and in the eyes of many, unsurpassed as an analysis framework. The author provides a theoretical overview and software in Java and R for correspondence analysis, clustering, and interpretation tools. A full chapter of case studies explores a range of applications to time-evolving data and in areas such as financial modeling, shape analysis, and the biosciences. The software and all of the data sets used in the book are available on a supporting web site.

Discounted Price: $61.16 / ?35.27

For more details and to order:
http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C5289

***

Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference, Second Edition 

Dani Gamerman, Federal University of Rio de Janeiro, Brazil 
Hedibert F. Lopes, University of Chicago, USA

Publication Date: 5/10/2006
Number of Pages: 323

Incorporating changes in theory and highlighting new applications, this new edition presents a concise, accessible, and comprehensive introduction to the methods of this valuable simulation technique. The second edition includes access to an internet site that provides the code, written in R and WinBUGS, used in many of the previously existing and new examples and exercises. More importantly, the self-explanatory nature of the codes will enable modification of the inputs to the codes and variation on many directions will be available for further exploration. 

Discounted Price: $50.36 / ?25.19

For more details and to order:
http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C5874

***

Measurement Error in Nonlinear Models: A Modern Perspective, Second Edition 

Raymond J. Carroll, Texas A & M University, College Station, USA 
David Ruppert, Cornell University, Ithaca, New York, USA 
Leonard A. Stefanski, North Carolina State University, Raleigh, USA 
Ciprian M. Crainiceanu, John Hopkins University, Baltimore, Maryland, USA  

Publication Date: 6/21/2006
Number of Pages: 488

New to this edition is greatly expanded discussion and applications of Bayesian computation via Markov Chain Monte Carlo techniques; a new chapter on longitudinal data and mixed models; a thoroughly revised chapter on nonparametric regression and density estimation; a totally new chapter on semiparametric regression; survival analysis expanded into its own separate chapter; a completely rewritten chapter on score functions; many more examples and illustrative graphs; and unique data sets compiled and made available online.

Discounted Price: $64.76 / ?35.99

For more details and to order:
http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C331X

*** CALL FOR AUTHORS ***

Chapman & Hall/CRC Press is actively seeking quality manuscripts to publish, particularly advanced textbooks, professional references, and handbooks that incorporate real-life examples, applications, and specific software implementations. If you have an idea for a book or have a manuscript already in progress, we want to hear about it. For proposal and manuscript submission guidelines, please contact:

Sunil Nair
Publisher, Mathematics & Statistics
Chapman & Hall/CRC Press
24-25 Blades Court, Deodar Road
London SW15 2NU, UK
mailto:sunil.nair at informa.com

--------------------------------------------------------------------------------------------------------------------------------------------

If you have received this message in error, please notify us by return and delete the message and any attachments.  Further enquiries/returns can be sent to postmaster at informa.com


From grant.reinman at pw.utc.com  Mon Mar  5 20:38:01 2007
From: grant.reinman at pw.utc.com (Reinman, Grant)
Date: Mon, 5 Mar 2007 14:38:01 -0500 
Subject: [R] error message when using outer function
Message-ID: <FA1084C758DCB74BA1A61449F3BF205B0F4B930D@pusehe03.eh.pweh.com>

Dear R-users,
I have two sets of code that appear to me to be equivalent, shown below, and
yet I get the error message 

"Error in dim(robj) <- c(dX, dY) : dim<- : dims [product 4] do not match the
length of object [1]"

after executing the assignment to logdens2.  Both functions post.a1 and
post.a2 return the same values when run alone and not embedded in the
function outer.  I would appreciate help in understanding the differences
between these two sets of code.  

The code in post.a1 is from Gelman, Carlin, Stern, and Rubin's Bayesian Data
Analysis solutions, problem 3.5.  I was trying to modify this code in
post.a2 when I ran into this error.


post.a1 <- function(mu,sd,y){
  ldens <- 0
  for (i in 1:length(y)) ldens <- ldens + log(dnorm(y[i],mu,sd))
  ldens}
y <- c(10,10,12,11,9)
mugrid <- c(10,11)
sdgrid <- c(1,1.2)
logdens1 <- outer (mugrid, sdgrid, post.a1, y)
#*** no error messages ***


post.a2 <- function(mu,sd,y)
{
  ldens <- sum(log(dnorm(y,mu,sd)))
  ldens
}
y <- c(10,10,12,11,9)
mugrid <- c(10,11)
sdgrid <- c(1,1.2)
logdens2 <- outer (mugrid, sdgrid, post.a2, y)
#***error message occurs here ***

Thank You!

Grant Reinman 
e-mail:grant.reinman at pw.utc.com


From fatemahku at yahoo.com  Mon Mar  5 20:38:38 2007
From: fatemahku at yahoo.com (Fatemah Alqallaf)
Date: Mon, 5 Mar 2007 11:38:38 -0800 (PST)
Subject: [R] generate random numbers for regression model
Message-ID: <493370.43314.qm@web37010.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070305/73fc637f/attachment.pl 

From thomas at mangold.com  Mon Mar  5 20:41:09 2007
From: thomas at mangold.com (Thomas Mangold)
Date: Mon, 05 Mar 2007 20:41:09 +0100
Subject: [R] Help on installing RScaLAPACK on Ubuntu
Message-ID: <45EC7255.4000504@mangold.com>

I try to install RScaLAPACK on Ubuntu 6.10 and LAM 7.0.x
Does anybody know a useful link top some how-to site about RScaLAPACK.

Now I manage to get the package compiling, but the linker shows me lots 
of unsolved references:
sudo R CMD INSTALL RScaLAPACK_0.5.1.tar.gz 
--configure-args="--with-mpi=/usr/lib/lam":

* Installing *source* package 'RScaLAPACK' ...
configure: MPI_HOME=/usr/lib/lam .. is set
configure: BLACS_LIB=/usr/lib .. is set
configure: BLAS_LIB=/usr/lib .. is set
configure: SCALAPACK_LIB=/usr/lib .. is set
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking for pthread_atfork in -lpthread... yes
checking for LAM-MPI... checking LAM-MPI Libraries at /usr/lib/lam... 
configure: LAM-MPI lib detected @ /usr/lib/lam..
Configured Parameters ...
LIBS = -lscalapack -lblacsF77init -lblacsCinit -lblacs -lf77blas -latlas 
-llamf77mpi -lmpi -llam -lpthread
LDFLAGS = -L/usr/lib -L/usr/lib -L/usr/lib/lib -L/usr/lib/lam/lib
CFLAGS = -I/usr/lib/lam/include -g -O2 -std=gnu99
PALIBS = -lmpi -llam -lpthread
...  *** ...
configure: creating ./config.status
config.status: creating src/Makefile
configure: creating ./config.status
config.status: creating src/Makefile
config.status: creating R/StartUpLam.R
** libs
** arch -
gcc -I/usr/share/R/include -I/usr/share/R/include     -fpic  
-I/usr/lib/lam/include -g -O2 -std=gnu99  -c CRDriver.c -o CRDriver.o
gcc -I/usr/share/R/include -I/usr/share/R/include     -fpic  
-I/usr/lib/lam/include -g -O2 -std=gnu99  -c CRscalapack.c -o CRscalapack.o
gfortran   -fpic  -g -O2 -c callpdgesv.f -o callpdgesv.o
gfortran   -fpic  -g -O2 -c callpdgeqrf.f -o callpdgeqrf.o
gfortran   -fpic  -g -O2 -c callpdgesvd.f -o callpdgesvd.o
gfortran   -fpic  -g -O2 -c callpdgemm.f -o callpdgemm.o
gfortran   -fpic  -g -O2 -c callpdpotrf.f -o callpdpotrf.o
gfortran   -fpic  -g -O2 -c callpdpotri.f -o callpdpotri.o
gfortran   -fpic  -g -O2 -c callpdsyevd.f -o callpdsyevd.o
gfortran   -fpic  -g -O2 -c CRcollectData.f -o CRcollectData.o
gfortran   -fpic  -g -O2 -c CRdistData.f -o CRdistData.o
gcc CRDriver.o CRscalapack.o callpdgesv.o callpdgeqrf.o callpdgesvd.o 
callpdgemm.o callpdpotrf.o callpdpotri.o callpdsyevd.o CRcollectData.o 
CRdistData.o -L/usr/lib -L/usr/lib -L/usr/lib/lib -L/usr/lib/lam/lib  
-lscalapack -lblacsF77init -lblacsCinit -lblacs -lf77blas -latlas 
-llamf77mpi -lmpi -llam -lpthread  -I/usr/lib/lam/include -g -O2 
-std=gnu99  -lg2c -o CRDriver
callpdgesv.o: In function `callpdgesv_':
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesv.f:96: undefined 
reference to `blacs_pinfo_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesv.f:99: undefined 
reference to `blacs_get_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesv.f:100: undefined 
reference to `blacs_gridinit_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesv.f:101: undefined 
reference to `blacs_gridinfo_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesv.f:201: undefined 
reference to `blacs_gridexit_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesv.f:148: undefined 
reference to `_gfortran_st_write'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesv.f:147: undefined 
reference to `_gfortran_transfer_character'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesv.f:147: undefined 
reference to `_gfortran_transfer_integer'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesv.f:148: undefined 
reference to `_gfortran_transfer_integer'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesv.f:148: undefined 
reference to `_gfortran_st_write_done'
callpdgeqrf.o: In function `callpdgeqrf_':
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgeqrf.f:95: undefined 
reference to `blacs_pinfo_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgeqrf.f:98: undefined 
reference to `blacs_get_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgeqrf.f:99: undefined 
reference to `blacs_gridinit_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgeqrf.f:100: undefined 
reference to `blacs_gridinfo_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgeqrf.f:253: undefined 
reference to `blacs_gridexit_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgeqrf.f:213: undefined 
reference to `blacs_barrier_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgeqrf.f:163: undefined 
reference to `_gfortran_st_write'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgeqrf.f:163: undefined 
reference to `_gfortran_transfer_character'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgeqrf.f:163: undefined 
reference to `_gfortran_transfer_integer'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgeqrf.f:163: undefined 
reference to `_gfortran_st_write_done'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgeqrf.f:156: undefined 
reference to `_gfortran_st_write'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgeqrf.f:155: undefined 
reference to `_gfortran_transfer_character'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgeqrf.f:155: undefined 
reference to `_gfortran_transfer_integer'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgeqrf.f:155: undefined 
reference to `_gfortran_transfer_integer'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgeqrf.f:156: undefined 
reference to `_gfortran_transfer_integer'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgeqrf.f:156: undefined 
reference to `_gfortran_st_write_done'
callpdgesvd.o: In function `callpdgesvd_':
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesvd.f:100: undefined 
reference to `blacs_pinfo_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesvd.f:103: undefined 
reference to `blacs_get_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesvd.f:104: undefined 
reference to `blacs_gridinit_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesvd.f:105: undefined 
reference to `blacs_gridinfo_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesvd.f:356: undefined 
reference to `blacs_gridexit_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesvd.f:240: undefined 
reference to `blacs_barrier_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesvd.f:171: undefined 
reference to `_gfortran_st_write'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesvd.f:171: undefined 
reference to `_gfortran_transfer_character'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesvd.f:171: undefined 
reference to `_gfortran_transfer_integer'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesvd.f:171: undefined 
reference to `_gfortran_st_write_done'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesvd.f:164: undefined 
reference to `_gfortran_st_write'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesvd.f:163: undefined 
reference to `_gfortran_transfer_character'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesvd.f:164: undefined 
reference to `_gfortran_transfer_integer'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesvd.f:164: undefined 
reference to `_gfortran_transfer_integer'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgesvd.f:164: undefined 
reference to `_gfortran_st_write_done'
callpdgemm.o: In function `callpdgemm_':
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgemm.f:128: undefined 
reference to `blacs_pinfo_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgemm.f:132: undefined 
reference to `blacs_get_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgemm.f:133: undefined 
reference to `blacs_gridinit_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgemm.f:134: undefined 
reference to `blacs_gridinfo_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgemm.f:237: undefined 
reference to `blacs_gridexit_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgemm.f:183: undefined 
reference to `_gfortran_st_write'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgemm.f:183: undefined 
reference to `_gfortran_transfer_character'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgemm.f:183: undefined 
reference to `_gfortran_transfer_integer'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgemm.f:183: undefined 
reference to `_gfortran_transfer_integer'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdgemm.f:183: undefined 
reference to `_gfortran_st_write_done'
callpdpotrf.o: In function `callpdpotrf_':
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdpotrf.f:92: undefined 
reference to `blacs_pinfo_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdpotrf.f:95: undefined 
reference to `blacs_get_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdpotrf.f:96: undefined 
reference to `blacs_gridinit_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdpotrf.f:97: undefined 
reference to `blacs_gridinfo_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdpotrf.f:188: undefined 
reference to `blacs_gridexit_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdpotrf.f:135: undefined 
reference to `_gfortran_st_write'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdpotrf.f:134: undefined 
reference to `_gfortran_transfer_character'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdpotrf.f:134: undefined 
reference to `_gfortran_transfer_integer'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdpotrf.f:135: undefined 
reference to `_gfortran_transfer_integer'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdpotrf.f:135: undefined 
reference to `_gfortran_st_write_done'
callpdpotri.o: In function `callpdpotri_':
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdpotri.f:92: undefined 
reference to `blacs_pinfo_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdpotri.f:95: undefined 
reference to `blacs_get_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdpotri.f:96: undefined 
reference to `blacs_gridinit_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdpotri.f:97: undefined 
reference to `blacs_gridinfo_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdpotri.f:187: undefined 
reference to `blacs_gridexit_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdpotri.f:133: undefined 
reference to `_gfortran_st_write'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdpotri.f:132: undefined 
reference to `_gfortran_transfer_character'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdpotri.f:132: undefined 
reference to `_gfortran_transfer_integer'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdpotri.f:133: undefined 
reference to `_gfortran_transfer_integer'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdpotri.f:133: undefined 
reference to `_gfortran_st_write_done'
callpdsyevd.o: In function `callpdsyevd_':
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdsyevd.f:96: undefined 
reference to `blacs_pinfo_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdsyevd.f:99: undefined 
reference to `blacs_get_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdsyevd.f:100: undefined 
reference to `blacs_gridinit_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdsyevd.f:101: undefined 
reference to `blacs_gridinfo_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdsyevd.f:244: undefined 
reference to `blacs_gridexit_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdsyevd.f:168: undefined 
reference to `_gfortran_st_write'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdsyevd.f:168: undefined 
reference to `_gfortran_transfer_character'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdsyevd.f:168: undefined 
reference to `_gfortran_transfer_integer'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdsyevd.f:168: undefined 
reference to `_gfortran_transfer_integer'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdsyevd.f:168: undefined 
reference to `_gfortran_st_write_done'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdsyevd.f:225: undefined 
reference to `blacs_barrier_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdsyevd.f:173: undefined 
reference to `_gfortran_st_write'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdsyevd.f:172: undefined 
reference to `_gfortran_transfer_character'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdsyevd.f:172: undefined 
reference to `_gfortran_transfer_integer'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdsyevd.f:173: undefined 
reference to `_gfortran_transfer_integer'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdsyevd.f:173: undefined 
reference to `_gfortran_st_write_done'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdsyevd.f:179: undefined 
reference to `_gfortran_st_write'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdsyevd.f:179: undefined 
reference to `_gfortran_transfer_character'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdsyevd.f:179: undefined 
reference to `_gfortran_transfer_integer'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/callpdsyevd.f:179: undefined 
reference to `_gfortran_st_write_done'
CRcollectData.o: In function `crcollectdata_':
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/CRcollectData.f:91: undefined 
reference to `blacs_gridinfo_'
CRdistData.o: In function `crdistdata_':
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/CRdistData.f:87: undefined 
reference to `blacs_gridinfo_'
/tmp/R.INSTALL.FZN392/RScaLAPACK/src/CRdistData.f:129: undefined 
reference to `blacs_barrier_'
collect2: ld returned 1 exit status
make: *** [CRDriver] Error 1
ERROR: compilation failed for package 'RScaLAPACK'
** Removing '/usr/local/lib/R/site-library/RScaLAPACK'

Thanks for help
Thomas Mangold


From ted.harding at nessie.mcc.ac.uk  Mon Mar  5 20:43:27 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 05 Mar 2007 19:43:27 -0000 (GMT)
Subject: [R] 0 * NA = NA
In-Reply-To: <20070305170524.M55915@centroin.com.br>
Message-ID: <XFMail.070305194327.ted.harding@nessie.mcc.ac.uk>

On 05-Mar-07 Alberto Monteiro wrote:
> 
> Of course, the problem begins to grow if we want, for example,
> to use elementary matrices to transform a matrix. The 2x2 matrix
> that switches two lines, rbind(c(0,1), c(1,0)) will not switch
> a matrix with NAs:
> 
> switch <- rbind(c(0,1), c(1,0))
> testmatrix <- rbind(c(1,2,3,4), c(5,6,7,8))
> switch %*% testmatrix # ok
> testmatrix[2,2] <- NA
> switch %*% testmatrix # not ok

Indeed! -- which is the sort of reason I said "This is a bit of a
tricky one, especially in a more general context."

There is no straightforward extension of my %*NA% operator which
deals with such a case, since the internal assignments

  X<-x;X[(is.na(x))&(y==0)]<-0;
  Y<-y;Y[(is.na(Y))&(x==0)]<-0;

fail because x (switch) and y (testmatrix) are non-conformable
(one being 2x2, the other 2x4).

Nor will it work if conformable, since then the 0's in switch
takes out the NA in the %*NA% operator:

testmatrix <- rbind(c(1,2), c(3,4))
testmatrix
     [,1] [,2]
[1,]    1    2
[2,]    3    4

switch %*NA% testmatrix
     [,1] [,2]
[1,]    3    4
[2,]    1    2 ## OK

testmatrix[2,2] <- NA
switch %*NA% testmatrix
     [,1] [,2]
[1,]    3    0
[2,]    1    2 ## Not OK!

So, if you want to simply multiply testmatrix by switch, with
terms 0*NA = 0, then you're OK; but you can't then use the same
operator for the purpose of switching rows, so you need a new
operator just for that kind of purpose.

Of course, for that specific purpose, index manipulation will
do the job:

  testmatrix
     [,1] [,2]
[1,]    1    2
[2,]    3   NA

  testmatrix[(2:1),]
     [,1] [,2]
[1,]    3   NA
[2,]    1    2

but it then disconnects it from the correspondence between
matrix multiplication and transformations.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 05-Mar-07                                       Time: 19:43:23
------------------------------ XFMail ------------------------------


From marc_schwartz at comcast.net  Mon Mar  5 21:00:36 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Mon, 05 Mar 2007 14:00:36 -0600
Subject: [R] logistic regression on contingency table
In-Reply-To: <loom.20070305T162830-235@post.gmane.org>
References: <90076531-9050-4090-95A7-0CED6786FDDB@bcm.tmc.edu>
	<loom.20070305T162830-235@post.gmane.org>
Message-ID: <1173124836.4831.139.camel@localhost.localdomain>

On Mon, 2007-03-05 at 15:31 +0000, Dieter Menne wrote:
> Bingshan Li <bli1 <at> bcm.tmc.edu> writes:
> 
> > I am wondering if there is a way in R to fit logistic regression on  
> > contingency table. If I have original data, I can transform the data  
> > into a design matrix and then call glm to fit the regression. But now  
> > I have a 2x3 contingency table with first row for response 0 and  
> > second row for response 1, and the columns are 3 levels of predictor  
> > variable. The 3 levels are not ordinal though and indicator variables  
> > would be more appreciate.
> 
> >From Documentation of GLM:
> 
> For binomial and quasibinomial families the response can also be specified 
> as a factor (when the first level denotes failure and all others success) 
> or as a two-column matrix with the columns giving the numbers of successes 
> and failures.  
> ....
> 
> Dieter Menne


Just to expand on Dieter's comments, one trick to taking this approach
is to coerce the contingency table you are starting with to a data
frame, and then specify a 'weights' argument to glm().

Taking some dummy example data in a 2D contingency table:

> TAB
   X
Y   A B C
  0 1 9 2
  1 3 3 2


So we have X (IV) and Y (Response).

Now, coerce TAB to a data frame. See ?as.data.frame.table and ?xtabs,
which reverses the process back to a contingency table:

DFT <- as.data.frame(TAB)

> DFT
  Y X Freq
1 0 A    1
2 1 A    3
3 0 B    9
4 1 B    3
5 0 C    2
6 1 C    2


As an FYI, gets us back to 'TAB':

> xtabs(Freq ~ ., DFT)
   X
Y   A B C
  0 1 9 2
  1 3 3 2



Now create the model, using 'Freq' for the case weights:

fit <- glm(Y ~ X, weights = Freq, data = DFT, family = binomial)


> summary(fit)

Call:
glm(formula = Y ~ X, family = binomial, data = DFT, weights = Freq)

Deviance Residuals: 
     1       2       3       4       5       6  
-1.665   1.314  -2.276   2.884  -1.665   1.665  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)  
(Intercept)    1.099      1.155   0.951   0.3414  
XB            -2.197      1.333  -1.648   0.0994 .
XC            -1.099      1.528  -0.719   0.4720  
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 26.920  on 5  degrees of freedom
Residual deviance: 23.540  on 3  degrees of freedom
AIC: 29.54

Number of Fisher Scoring iterations: 4



An alternative to the above is to use a function that I just posted in
the past week for another query, called expand.dft():

expand.dft <- function(x, na.strings = "NA", as.is = FALSE, dec = ".")
{
  DF <- sapply(1:nrow(x), function(i) x[rep(i, each = x$Freq[i]), ],
               simplify = FALSE)

  DF <- subset(do.call("rbind", DF), select = -Freq)

  for (i in 1:ncol(DF))
  {
    DF[[i]] <- type.convert(as.character(DF[[i]]),
                            na.strings = na.strings,
                            as.is = as.is, dec = dec)
                                           
  }
    
  DF
}             


This takes the data frame table 'DFT' from above and converts it back to
the raw observations:

DF <- expand.dft(DFT)

> DF
    Y X
1   0 A
2   1 A
2.1 1 A
2.2 1 A
3   0 B
3.1 0 B
3.2 0 B
3.3 0 B
3.4 0 B
3.5 0 B
3.6 0 B
3.7 0 B
3.8 0 B
4   1 B
4.1 1 B
4.2 1 B
5   0 C
5.1 0 C
6   1 C
6.1 1 C


As an FYI, gets us back to 'TAB':

> table(DF)
   X
Y   A B C
  0 1 9 2
  1 3 3 2



So, now we can use the normal approach for glm():

fit2 <- glm(Y ~ X, data = DF, family = binomial)

> summary(fit2)

Call:
glm(formula = Y ~ X, family = binomial, data = DF)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.6651  -0.7585  -0.7585   0.8632   1.6651  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)  
(Intercept)    1.099      1.155   0.951   0.3414  
XB            -2.197      1.333  -1.648   0.0994 .
XC            -1.099      1.528  -0.719   0.4720  
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 26.920  on 19  degrees of freedom
Residual deviance: 23.540  on 17  degrees of freedom
AIC: 29.54

Number of Fisher Scoring iterations: 4


Note of course that the DF's are different, though the Null and Residual
Deviances and AIC are the same:


Taking the latter approach, will of course enable you to make subsequent
manipulations on the raw data if you wish.

HTH,

Marc Schwartz


From klaster at karlin.mff.cuni.cz  Mon Mar  5 21:25:36 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Mon, 05 Mar 2007 21:25:36 +0100
Subject: [R] error message when using outer function
In-Reply-To: <FA1084C758DCB74BA1A61449F3BF205B0F4B930D@pusehe03.eh.pweh.com>
References: <FA1084C758DCB74BA1A61449F3BF205B0F4B930D@pusehe03.eh.pweh.com>
Message-ID: <45EC7CC0.4040104@karlin.mff.cuni.cz>

Comments inside.

Reinman, Grant napsal(a):
> Dear R-users,
> I have two sets of code that appear to me to be equivalent, shown below, and
> yet I get the error message 
> 
> "Error in dim(robj) <- c(dX, dY) : dim<- : dims [product 4] do not match the
> length of object [1]"
> 
> after executing the assignment to logdens2.  Both functions post.a1 and
> post.a2 return the same values when run alone and not embedded in the
> function outer.  I would appreciate help in understanding the differences
> between these two sets of code.  
> 
> The code in post.a1 is from Gelman, Carlin, Stern, and Rubin's Bayesian Data
> Analysis solutions, problem 3.5.  I was trying to modify this code in
> post.a2 when I ran into this error.
> 
> 
> post.a1 <- function(mu,sd,y){
>   ldens <- 0
>   for (i in 1:length(y)) ldens <- ldens + log(dnorm(y[i],mu,sd))
>   ldens}
> y <- c(10,10,12,11,9)
> mugrid <- c(10,11)
> sdgrid <- c(1,1.2)
> logdens1 <- outer (mugrid, sdgrid, post.a1, y)
> #*** no error messages ***

Actually pretty ugly coding (for a textbook) in my opinion... Try what 
you get with
y <- c(10,10,12,11,9)
mugrid <- c(10,11)
sdgrid <- c(1,1.2)
log(dnorm(y[1],mean=mugrid,sd=sdgrid))

[1] -0.9189385 -1.4484823
This is in a perfect accordance with help(dnorm), which says you are 
allowed to specify a vector of means and SDs. Here 2 values were 
specified, so you obtain 2 density values.

Now, adding a vector of length 2 to a constant:
#not run here# ldens <- ldens + log(dnorm(y[i],mu,sd))
is again a vector of length 2.

However, using sum() in your code below gives you just a single number 
and R starts complaining because the dimensions don't match.

Petr

> 
> 
> post.a2 <- function(mu,sd,y)
> {
>   ldens <- sum(log(dnorm(y,mu,sd)))
>   ldens
> }
> y <- c(10,10,12,11,9)
> mugrid <- c(10,11)
> sdgrid <- c(1,1.2)
> logdens2 <- outer (mugrid, sdgrid, post.a2, y)
> #***error message occurs here ***
> 
> Thank You!
> 
> Grant Reinman 
> e-mail:grant.reinman at pw.utc.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From bolker at zoo.ufl.edu  Mon Mar  5 21:03:30 2007
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Mon, 5 Mar 2007 20:03:30 +0000 (UTC)
Subject: [R] Non : Confidence intervals for p**2 ??
References: <OFBDC5267A.10918A62-ON48257295.0033ECFA@promos.com.tw>
	<2BAF2D3C41D1274E9228E63287F19B7E27DB0A@mailsrv2.loginmpa.mpa.se>
Message-ID: <loom.20070305T205639-111@post.gmane.org>

?hagen Patrik <Patrik.Ohagen <at> mpa.se> writes:

> 
> 
> Dear List,
> 
> I was asked to calculate a confidence interval for p*p. Is there any standard
techniques for calculating
> such an interval? Delta Method?
> 
> Thank you in advance!
> 


  if p is a generic value (i.e. not a probability) and you know the
variance (and are willing to assume normality) then you can indeed
use the delta method; there are a variety of other techniques
if you have the original data: fitting profile confidence limits, various
resampling methods including bootstrapping, etc.
(See section 6 of chapter 7 at http://www.zoo.ufl.edu/emdbook for
more details if you like).

  Ben Bolker


From ThadenJohnJ at uams.edu  Mon Mar  5 22:33:18 2007
From: ThadenJohnJ at uams.edu (Thaden, John J)
Date: Mon, 5 Mar 2007 15:33:18 -0600
Subject: [R]  Scoping issue? [SUMMARY]
References: <mailman.7.1172919606.16782.r-help@stat.math.ethz.ch>
	<B1614B0C915A654A9C29BB71DA80E0DD01773D1B@MAIL2.ad.uams.edu>
	<644e1f320703041525w5af344a5mea578b13f61f9014@mail.gmail.com>
	<B1614B0C915A654A9C29BB71DA80E0DD01773D1C@MAIL2.ad.uams.edu>
	<Pine.LNX.4.64.0703050632570.3657@itasca2.wildberry.org>
Message-ID: <B1614B0C915A654A9C29BB71DA80E0DD01773D2D@MAIL2.ad.uams.edu>

A BIG thanks to Luke Tierney and Jim Holtman (and perhaps others? I haven't
seen the latest digest) for pointing out two fixes to some code that was
troubling me. I since found a third problem, and with that, have good code.
Their comments are summarized below this fixed version of the code.

############
mmatplot <- function(colnum, x, y, addto = 0, titleroot = "Column #", ...){
  switch(class(y),
    array = y <- y[, colnum, ],
    list = y <- sapply(X = y, FUN = function(d) d[, colnum]))  #Thanks Luke!
  stopifnot(is.matrix(y))
  matplot(x, y, main = paste(titleroot, colnum), ...)
}
############

This function is a matplot wrapper, useful (I thought) within lapply
or sapply to visually compare several like columns of several matrices. 
Arg y is either a list of matrices with equal number of rows, or 
an array. The first arg, scalar n, gives the column of each matrix
(or array slab) to plot. The fact that it is in first position allows
the program to be used in lapply. par values and matplot args are accepted.

Here is the tester function. It also needed fixing. I had called 
mapply instead of lapply by mistake, but using lapply arguments!

###############
mmatplotTest <- function(){
  A <- array(data = rnorm(90), dim = c(10, 3, 3))
  L <- list(A[, , 1], A[, , 2], A[, , 3])
  oldmf <- par("mfrow")
  par(mfrow = c(2,3))
  # Test with class(y) == "array"
  lapply(X = 1:ncol(A), FUN = mmatplot, x = 1:nrow(A), y = A,
                 titleroot = "Array, column #")
  # Test with class(y) == "list"
  lapply(1:ncol(L[[1]]), mmatplot, x = 1:nrow(L[[1]]), y = L,
                 titleroot = "Listed matrices, column #")
  par(mfrow = oldmf)
}
mmatplotTest()
################

Regarding the original, broken version...

Jim was first to point out that 'colnum' did not exist when my 'paste'
call was made.  I asked why, since I thought lazy evaluation should get
around this problem.  Luke explained...

> In your test function there is no lexically visible definition
> for the `colnum` variable used in defining main, so that error
> is what you expect from lexical scoping.  Lazy evaluation 
> dictates when (and if) the `main` argument is evaluated, but
> the environment in which it is evaluated is determined by the 
> context where the expression is written in the code, i.e. within
> the function mmatplotTest.

Luke also pointed out the source of another error:

> The error you get when you take out the `main` is coming from
> `subset` and is due to the fact that subset is one of those
> functions that uses non-standard evaluation for some of its 
> arguments, in this case `select`.  This makes it (slightly) 
> easier to use at interactive top level but much more complicated
> to use within a function....You need to use another function
> in your sapply call....

I incorporated his suggested alternative above.

Jim suggest sidestepping the entire problem by using explicit, R-level
looping instead of lapply (mapply in my original flawed version!), asserting
that in this case it is even faster, but that, in any case, most time is
spent by the mmatplot function itself, rendering inconsequential the method
of iterating it (also, I should add, rendering rather superfluous my
mmatplot function altogether!).

Thanks again, Luke and Jim, and whoever else considered my problem!

-John Thaden
Little Rock, AR, USA

Confidentiality Notice: This e-mail message, including any a...{{dropped}}


From slacey at umich.edu  Mon Mar  5 23:12:49 2007
From: slacey at umich.edu (Steven Lacey)
Date: Mon, 5 Mar 2007 17:12:49 -0500
Subject: [R] factor analysis and pattern matrix
Message-ID: <000001c75f73$6a0a34c0$6400a8c0@zephyr>

Hi, 

In a discussion of factor analysis in "Using Multivariate Statistics" by
Tabachnick and Fidell, two matrices are singled out as important for
interpreting an exploratory factor analysis (EFA) with an oblique promax
rotation. One is the "structure matrix". The structure matrix contains the
correlations between variables and factors. However, these correlations may
be inflated because some of the variance in a factor may not be unique to
it. To address this and facilitate the interpretation of factors, the
"pattern matrix" can be calculated as it contains the unique correlations
between variables and factors (that is, the variance shared among factors
has been removed). 

Are the loadings returned from factanal() with a promax rotation the
structure or the pattern matrix? How do I calculate which ever one of the
matrices is not returned by factanal?

Thanks,

Steve


From lind1199 at umn.edu  Mon Mar  5 23:06:05 2007
From: lind1199 at umn.edu (Gregg Lind)
Date: Mon, 05 Mar 2007 16:06:05 -0600
Subject: [R] Rbind with data frames -- column names question
Message-ID: <45EC944D.8070705@umn.edu>


As part of my work, I am trying to append matrices onto data frames.  
Naively I assumed that when rbinding a data.frame and matrix, the matrix 
would be coerced and appended, keeping the names from the data frame.  
Clearly, I am not fully understanding the process by which rbind works. 

Example code: 

 > A<-data.frame(1,1,1); names(A)=letters[1:3] ; B<-matrix(0,2,3)
 > rbind(A,B)
Error in match.names(clabs, names(xi)) : names do not match previous names:
        V1, V2, V3
 > rbind(A,as.data.frame(B))
Error in match.names(clabs, names(xi)) : names do not match previous names:
        V1, V2, V3



Is there a "right" way to combine the two such that the both end up 
having the same column names?

I have tried to understand the deparse.level argument of rbind, but it 
doesn't seem to do what I'm asking. 

Thank you for any help you can give.


Gregg
-- 
Gregg Lind, M.S.

Division of Epidemiology and Community Health
School of Public Health
University of Minnesota, United States


From lind1199 at umn.edu  Mon Mar  5 23:32:50 2007
From: lind1199 at umn.edu (Gregg Lind)
Date: Mon, 05 Mar 2007 16:32:50 -0600
Subject: [R] Rbind with data frames -- column names question
In-Reply-To: <644e1f320703051427u7c083b12g9fec3014cedfa2d0@mail.gmail.com>
References: <45EC944D.8070705@umn.edu>
	<644e1f320703051427u7c083b12g9fec3014cedfa2d0@mail.gmail.com>
Message-ID: <45EC9A92.5000507@umn.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070305/78ccb055/attachment.pl 

From Cody_Hamilton at Edwards.com  Mon Mar  5 23:40:58 2007
From: Cody_Hamilton at Edwards.com (Cody_Hamilton at Edwards.com)
Date: Mon, 5 Mar 2007 14:40:58 -0800
Subject: [R]  Rbind with data frames -- column names question
Message-ID: <OFF0A4DA8F.E25520D6-ON88257295.007C8ECD-88257295.007C5F95@irvine.edwards.com>



Gregg,

What about

A<-data.frame(1,1,1); names(A)=letters[1:3] ; B<-matrix(0,2,3)
B<-as.data.frame(B)
names(B)<-names(A)
rbind(A,B)

-Cody




                                                                           
             Gregg Lind                                                    
             <lind1199 at umn.edu                                             
             >                                                          To 
             Sent by:                  r-help at stat.math.ethz.ch            
             r-help-bounces at st                                          cc 
             at.math.ethz.ch                                               
                                                                   Subject 
                                       [R] Rbind with data frames --       
             03/05/2007 02:06          column names question               
             PM                                                            
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           





As part of my work, I am trying to append matrices onto data frames.
Naively I assumed that when rbinding a data.frame and matrix, the matrix
would be coerced and appended, keeping the names from the data frame.
Clearly, I am not fully understanding the process by which rbind works.

Example code:

 > A<-data.frame(1,1,1); names(A)=letters[1:3] ; B<-matrix(0,2,3)
 > rbind(A,B)
Error in match.names(clabs, names(xi)) : names do not match previous names:
        V1, V2, V3
 > rbind(A,as.data.frame(B))
Error in match.names(clabs, names(xi)) : names do not match previous names:
        V1, V2, V3



Is there a "right" way to combine the two such that the both end up
having the same column names?

I have tried to understand the deparse.level argument of rbind, but it
doesn't seem to do what I'm asking.

Thank you for any help you can give.


Gregg
--
Gregg Lind, M.S.

Division of Epidemiology and Community Health
School of Public Health
University of Minnesota, United States

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From mothsailor at googlemail.com  Mon Mar  5 23:43:53 2007
From: mothsailor at googlemail.com (David Barron)
Date: Mon, 5 Mar 2007 22:43:53 +0000
Subject: [R] Rbind with data frames -- column names question
In-Reply-To: <45EC944D.8070705@umn.edu>
References: <45EC944D.8070705@umn.edu>
Message-ID: <815b70590703051443x2e7d71bdx86e1c77e603e7e76@mail.gmail.com>

I don't know if this is the "right" way, but I think this would work
(I'm assuming you want the result to be a data frame):

data.frame(rbind(as.matrix(A),B))

You might get a warning about row names, but I think it works OK.

On 05/03/07, Gregg Lind <lind1199 at umn.edu> wrote:
>
> As part of my work, I am trying to append matrices onto data frames.
> Naively I assumed that when rbinding a data.frame and matrix, the matrix
> would be coerced and appended, keeping the names from the data frame.
> Clearly, I am not fully understanding the process by which rbind works.
>
> Example code:
>
>  > A<-data.frame(1,1,1); names(A)=letters[1:3] ; B<-matrix(0,2,3)
>  > rbind(A,B)
> Error in match.names(clabs, names(xi)) : names do not match previous names:
>         V1, V2, V3
>  > rbind(A,as.data.frame(B))
> Error in match.names(clabs, names(xi)) : names do not match previous names:
>         V1, V2, V3
>
>
>
> Is there a "right" way to combine the two such that the both end up
> having the same column names?
>
> I have tried to understand the deparse.level argument of rbind, but it
> doesn't seem to do what I'm asking.
>
> Thank you for any help you can give.
>
>
> Gregg
> --
> Gregg Lind, M.S.
>
> Division of Epidemiology and Community Health
> School of Public Health
> University of Minnesota, United States
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From topkatz at msn.com  Tue Mar  6 00:30:23 2007
From: topkatz at msn.com (Talbot Katz)
Date: Mon, 05 Mar 2007 18:30:23 -0500
Subject: [R] Linear programming with sparse matrix input format?
Message-ID: <BAY132-F5622FAE3A86710B9CA351AA840@phx.gbl>

Hi.

I am aware of three different R packages for linear programming: glpk, 
linprog, lpSolve.  From what I can tell, if there are N variables and M 
constraints, all these solvers require the full NxM constraint matrix.  Some 
linear solvers I know of (not in R) have a sparse matrix input format.  Are 
there any linear solvers in R that have a sparse matrix input format?  
(including the possibility of glpk, linprog, and lpSolve, in case I might 
have missed something in the documentation).  Thanks!

--  TMK  --
212-460-5430	home
917-656-5351	cell


From Inman.Brant at mayo.edu  Tue Mar  6 00:55:33 2007
From: Inman.Brant at mayo.edu (Inman, Brant A.   M.D.)
Date: Mon, 5 Mar 2007 17:55:33 -0600
Subject: [R] Mixed effects multinomial regression and meta-analysis
Message-ID: <6021CA6EF4C8374084D4F5A141F1CBBB664BAE@msgebe23.mfad.mfroot.org>


R Experts:

I am conducting a meta-analysis where the effect measures to be pooled
are simple proportions.  For example, consider this  data from
Fleiss/Levin/Paik's Statistical methods for rates and proportions (2003,
p189) on smokers:

Study	   N       Event P(Event)
 1       86       83    0.965
 2       93       90    0.968
 3       136     129    0.949
 4       82       70    0.854
Total    397     372    

A test of heterogeneity for a table like this could simply be Pearson'
chi-square test.  
------

smoke.data <- matrix(c(83,90,129,70,3,3,7,12), ncol=2, byrow=F)
chisq.test(smoke.data, correct=T)

> X-squared = 12.6004, df = 3, p-value = 0.005585

------

Now this test implies that the data is heterogenous and that pooling
might be inappropriate. This type of analysis could be considered a
fixed effects analysis because it assumes that the 4 studies are all
coming from one underlying population.  But what if I wanted to do a
mixed effects (fixed + random) analysis of data like this, possibly
adjusting for an important covariate or two (assuming I had more
studies, of course)...how would I go about doing it? One thought that I
had would be to use a mixed effects multinomial logistic regression
model, such as that reported by Hedeker (Stat Med 2003, 22: 1433),
though I don't know if (or where) it is implemented in R.  I am certain
there are also other ways...

So, my questions to the R experts are:

1) What method would you use to estimate or account for the between
study variance in a dataset like the one above that would also allow you
to adjust for a variable that might explain the heterogeneity?

2) Is it implemented in R?


Brant Inman
Mayo Clinic


From alan.gibson at gmail.com  Tue Mar  6 01:12:29 2007
From: alan.gibson at gmail.com (Alan Gibson)
Date: Mon, 5 Mar 2007 16:12:29 -0800
Subject: [R] Document classes with tm
Message-ID: <a2f3b2d90703051612x4ddf8df3j6d804bdbaa4fd3d0@mail.gmail.com>

does anyone have any tips for using the tm package for supporting
autoclassifying textual documents? while tm works very well for
parsing text documents and creating term-document matrices, it doesnt
seem to support tracking document classes by default. without a way to
know the classes of your training documents, building a classifier is
kind of a non starter.

i know i could just do this manually by just reading in the classes
from a csv, but im hoping there is a fascility in tm for doing this
that im just missing.

thanks,
alan


From kubovy at virginia.edu  Tue Mar  6 02:45:37 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Mon, 5 Mar 2007 20:45:37 -0500
Subject: [R] How to override ordering of panels in xyplot()
In-Reply-To: <eb555e660703031219j66794bafnaec7ed88640ad082@mail.gmail.com>
References: <57C54374-D6B3-4DD7-88B9-4D2971A454DA@virginia.edu>
	<eb555e660703031219j66794bafnaec7ed88640ad082@mail.gmail.com>
Message-ID: <EF531B98-9425-4CFE-9517-5CB7CF8E1C62@virginia.edu>

On Mar 3, 2007, at 3:19 PM, Deepayan Sarkar wrote:

> On 3/3/07, Michael Kubovy <kubovy at virginia.edu> wrote:
>> Dear r-helpers,
>>
>> I'm conditioning an xyplot on a variable whose levels are'low',  
>> 'med', 'high'. How do I override the alphabetical ordering for the  
>> panels of the plot?
>
> This has less to do with xyplot and more to do with the default of  
> the 'levels' argument in the factor() function. Just make sure the  
> levels are in the right order in your data when xyplot is called.

Unless one makes the factor ordered, reordering the levels of a  
factor does not seem to be a trivial matter. The only R function I've  
found that makes it easy is reorder_factor() (package:reshape). Or am  
I missing something?
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From roger at ysidro.econ.uiuc.edu  Tue Mar  6 03:08:21 2007
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Mon, 5 Mar 2007 20:08:21 -0600
Subject: [R] Linear programming with sparse matrix input format?
In-Reply-To: <BAY132-F5622FAE3A86710B9CA351AA840@phx.gbl>
References: <BAY132-F5622FAE3A86710B9CA351AA840@phx.gbl>
Message-ID: <839D1236-6D82-4B97-80FF-0008DF172C02@ysidro.econ.uiuc.edu>

If you can reformulate your LP as an L1 problem, which is known to be
possible without loss of generality, but perhaps not without loss of  
sleep,
then you could use the sparse quantile regression functions in the
quantreg package.


url:    www.econ.uiuc.edu/~roger                Roger Koenker
email   rkoenker at uiuc.edu                       Department of Economics
vox:    217-333-4558                            University of Illinois
fax:    217-244-6678                            Champaign, IL 61820


On Mar 5, 2007, at 5:30 PM, Talbot Katz wrote:

> Hi.
>
> I am aware of three different R packages for linear programming: glpk,
> linprog, lpSolve.  From what I can tell, if there are N variables  
> and M
> constraints, all these solvers require the full NxM constraint  
> matrix.  Some
> linear solvers I know of (not in R) have a sparse matrix input  
> format.  Are
> there any linear solvers in R that have a sparse matrix input format?
> (including the possibility of glpk, linprog, and lpSolve, in case I  
> might
> have missed something in the documentation).  Thanks!
>
> --  TMK  --
> 212-460-5430	home
> 917-656-5351	cell
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From liuwensui at gmail.com  Tue Mar  6 03:24:13 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 5 Mar 2007 21:24:13 -0500
Subject: [R] Heteroskedastic Time Series
In-Reply-To: <17372.85.118.9.118.1173088760.squirrel@85.118.9.118>
References: <17372.85.118.9.118.1173088760.squirrel@85.118.9.118>
Message-ID: <1115a2b00703051824y58c846f9n145a8cbeda61981f@mail.gmail.com>

check fseris library.

On 3/5/07, james at ma.hw.ac.uk <james at ma.hw.ac.uk> wrote:
> Hi R-helpers,
>
> I'm new to time series modelling, but my requirement seems to fall just
> outside the capabilities of the arima function in R.  I'd like to fit an
> ARMA model where the variance of the disturbances is a function of some
> exogenous variable.  So something like:
>
> Y_t = a_0 + a_1 * Y_(t-1) +...+ a_p * Y_(t-p) + b_1 * e_(t-1) +...+ b_q *
> e_(t-q) + e_t,
>
> where
>
> e_t ~ N(0, sigma^2_t),
>
> and with the variance specified by something like
>
> sigma^2_t = exp(beta_t * X_t),
>
> where X_t is my exogenous variable.  I would be very grateful if somebody
> could point me in the direction of a library that could fit this (or a
> similar) model.
>
> Thanks,
>
> James Kirkby
> Actuarial Maths and Stats
> Heriot Watt University
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From jr_frrr at yahoo.de  Tue Mar  6 04:12:56 2007
From: jr_frrr at yahoo.de (=?ISO-8859-1?Q?Jos=E9?= Rafael Ferrer Paris)
Date: Mon, 05 Mar 2007 23:12:56 -0400
Subject: [R] different random effects for each level of a factor in lme
Message-ID: <1173150776.17839.68.camel@localhost.localdomain>

I have an interesting lme - problem. The data is part of the Master
Thesis of my friend, and she had some problems analysing this data,
until one of her Jurors proposed to use linear mixed-effect models. I'm
trying to help her since she has no experience with R. I'm very used to
R but have very few experience with lme.

The group calls of one species of parrot were recorded at many
localities in mainland and islands. Within the localities the parrots
move in groups, several calls were recorded for each group but the calls
can not be separated by individuals.

We use the variable s1 to measure one property of the calls (the length
of the first part of the call). We are interested in explaining the
variability of the calls, one hypothesis is that variability of calls
tends to be greater in islands compared with mainland.

So we have 
s1   : as a measure of interest
loc  : as a grouping variable (locality)
grp  : as a grouping variable (group) nested in loc
isla : is an factor that identify which localities are in island 
       and which are in mainland (it is outer to loc)

I began with a simple model with fixed effects in isla (since there are
some differences in the length of s1) and nested random effects: 

f00 <- lme(s1~isla,data=s1.ap,
           random=~1|loc/grp)

My final model should have fixed effects in isla, different nested
random for both levels of isla, and different error per stratum,
something like:

f11 <-
  lme(s1~isla,data=s1.ap,
      random=~isla|loc/grp,
      weights=varIdent(form=~1|isla))

or perhaps:

f11b <-
  lme(s1~isla,s1.ap,
      random=~isla-1|loc/grp,
      weights=varIdent(form=~1|isla))

Is this a valid formulation? I have seen that ~x|g1/g2 is usually used
for modelling random effects in (the intercept and slope of) covariates
and that ~1|g1/f1 is used to model interactions between grouping factors
and treatment factors. 

I fitted the above models (and a few other variants between the simpler
and the complex ones) and found f11 to be the best model, f11 and f11b
are both identical in terms of AIC or LR-Tests since they are the same
model with different parametrization (I guess...).

Now, I suppose I did everything right, and I want to compare the
variance decomposition in islands and mainland, I use 

> VarCorr(f11)

            Variance        StdDev   Corr  
loc =       pdLogChol(isla)                
(Intercept) 1643.5904       40.54122 (Intr)
islaT        962.2991       31.02095 -0.969
grp =       pdLogChol(isla)                
(Intercept)  501.7315       22.39936 (Intr)
islaT        622.5393       24.95074 -0.818
Residual     547.0888       23.38993       

> VarCorr(f11b)

         Variance            StdDev   Corr 
loc =    pdLogChol(isla - 1)               
islaI    1643.4821           40.53988 islaI
islaT     168.6514           12.98659 0    
grp =    pdLogChol(isla - 1)               
islaI     501.7357           22.39946 islaI
islaT     209.8698           14.48688 0    
Residual  547.0871           23.38989      

The variance for islands (islaI) is always greater  than the ones for
mainland (islaT) as expected, and the estimates of Intercept in f11 are
nearly equal to the estimates for islaI in f11b. However, the estimates
for islaT are completely different. It seems to me that the estimates in
f11b are the "correct ones" and the ones in f11 are obtained by
reparametrization of the variance-covariance matrix. Am I right?

I want to say what percentage of the variance is explained by each
level, something like this:

> tmp <-
data.frame(I=c(1643.48,501.74,(547.09)),T=c(168.65,209.86,(0.7823097*547.09)))
> rownames(tmp) <- c("loc","grp","res")
> t(t(tmp)/(colSums(tmp)))
          I       T
loc  0.6104349 0.2091125
grp  0.1863604 0.2602096
res  0.2032047 0.5306780

(0.7823097 was the result of varIdent for islaT)

If I compare the sum of variances in f11b for each level of isla with
the variances of the data frame I get similar results:

> colSums(tmp)
        I         T 
2692.3100  806.5038 
> tapply(s1.ap$s1,s1.ap$isla,var)
        I         T 
2417.1361  731.8165 

So I guess this is the right way to interpret the variances in the
fitted model. Or, is it not?

thanks,

JR

-- 
Dipl.-Biol. JR Ferrer Paris
~~~~~~~~~~~~~~~~~~~~~~~~~~~
Laboratorio de Biolog?a de Organismos --- Centro de Ecolog?a
Instituto Venezolano de Investigaciones Cient?ficas (IVIC) 
Apdo. 21827, Caracas 1020-A 
Rep?blica Bolivariana de Venezuela

Tel: (+58-212) 504-1452
Fax: (+58-212) 504-1088

email: jferrer at ivic.ve
clave-gpg: 2C260A95


From roger at ysidro.econ.uiuc.edu  Tue Mar  6 04:14:41 2007
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Mon, 5 Mar 2007 21:14:41 -0600
Subject: [R] tournaments to dendrograms
In-Reply-To: <C74B189E-818F-48F3-8E2A-045469B738AA@uiuc.edu>
References: <C74B189E-818F-48F3-8E2A-045469B738AA@uiuc.edu>
Message-ID: <0D8AD988-0A42-4054-8CF6-B0DFE904C2F9@ysidro.econ.uiuc.edu>

I've had no response to the enquiry below, so I made a rather half-baked
version in grid  --  code and pdf are available here:

	http://www.econ.uiuc.edu/~roger/research/ncaa

comments would be welcome.   This is _the_  ubiquitous graphic this  
time of
year in the US, so R should take a shot at it.  My first attempt is  
rather primitive
but I have to say that Paul's grid package is  superb.

url:    www.econ.uiuc.edu/~roger                Roger Koenker
email   rkoenker at uiuc.edu                       Department of Economics
vox:    217-333-4558                            University of Illinois
fax:    217-244-6678                            Champaign, IL 61820


On Feb 22, 2007, at 4:08 PM, roger koenker wrote:

> Does anyone have (good) experience converting tables of tournament
> results into dendrogram-like graphics?  Tables, for example, like  
> this:
>
> read.table(url("http://www.econ.uiuc.edu/~roger/research/ncaa/ 
> NCAA.d"))
>
> Any pointers appreciated.   RK
>
> url:    www.econ.uiuc.edu/~roger            Roger Koenker
> email    rkoenker at uiuc.edu            Department of Economics
> vox:     217-333-4558                University of Illinois
> fax:       217-244-6678                Champaign, IL 61820
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From topkatz at msn.com  Tue Mar  6 04:59:02 2007
From: topkatz at msn.com (Talbot Michael Katz)
Date: Mon, 5 Mar 2007 22:59:02 -0500
Subject: [R] Linear programming with sparse matrix input format?
References: <BAY132-F5622FAE3A86710B9CA351AA840@phx.gbl>
	<839D1236-6D82-4B97-80FF-0008DF172C02@ysidro.econ.uiuc.edu>
Message-ID: <BAY132-DAV949C3C26D2643FF2FD16EAA7B0@phx.gbl>

Thank you for the tip.  Can quantreg actually handle mixed / integer
programs?

--  TMK  --
212-460-5430    home
917-656-5351    cell


----- Original Message ----- 
From: "roger koenker" <roger at ysidro.econ.uiuc.edu>
To: "Talbot Katz" <topkatz at msn.com>
Cc: "r-help" <R-help at stat.math.ethz.ch>
Sent: Monday, March 05, 2007 9:08 PM
Subject: Re: [R] Linear programming with sparse matrix input format?


> If you can reformulate your LP as an L1 problem, which is known to be
> possible without loss of generality, but perhaps not without loss of
> sleep,
> then you could use the sparse quantile regression functions in the
> quantreg package.
>
>
> url:    www.econ.uiuc.edu/~roger                Roger Koenker
> email   rkoenker at uiuc.edu                       Department of Economics
> vox:    217-333-4558                            University of Illinois
> fax:    217-244-6678                            Champaign, IL 61820
>
>
> On Mar 5, 2007, at 5:30 PM, Talbot Katz wrote:
>
> > Hi.
> >
> > I am aware of three different R packages for linear programming: glpk,
> > linprog, lpSolve.  From what I can tell, if there are N variables
> > and M
> > constraints, all these solvers require the full NxM constraint
> > matrix.  Some
> > linear solvers I know of (not in R) have a sparse matrix input
> > format.  Are
> > there any linear solvers in R that have a sparse matrix input format?
> > (including the possibility of glpk, linprog, and lpSolve, in case I
> > might
> > have missed something in the documentation).  Thanks!
> >
> > --  TMK  --
> > 212-460-5430 home
> > 917-656-5351 cell
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>


From Inman.Brant at mayo.edu  Tue Mar  6 05:00:51 2007
From: Inman.Brant at mayo.edu (Inman, Brant A.   M.D.)
Date: Mon, 5 Mar 2007 22:00:51 -0600
Subject: [R] Mixed effects multinomial regression and meta-analysis
Message-ID: <6021CA6EF4C8374084D4F5A141F1CBBB664BB2@msgebe23.mfad.mfroot.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070305/a57015bb/attachment.pl 

From deepayan.sarkar at gmail.com  Tue Mar  6 05:11:53 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 5 Mar 2007 20:11:53 -0800
Subject: [R] How to override ordering of panels in xyplot()
In-Reply-To: <EF531B98-9425-4CFE-9517-5CB7CF8E1C62@virginia.edu>
References: <57C54374-D6B3-4DD7-88B9-4D2971A454DA@virginia.edu>
	<eb555e660703031219j66794bafnaec7ed88640ad082@mail.gmail.com>
	<EF531B98-9425-4CFE-9517-5CB7CF8E1C62@virginia.edu>
Message-ID: <eb555e660703052011m24fa600ehc6494e40dc2ed23f@mail.gmail.com>

On 3/5/07, Michael Kubovy <kubovy at virginia.edu> wrote:
>
> Unless one makes the factor ordered, reordering the levels of a
> factor does not seem to be a trivial matter. The only R function I've
> found that makes it easy is reorder_factor() (package:reshape). Or am
> I missing something?

Don't think in terms of reordering factors, just create a new factor,
using a call to factor(), with a suitable 'levels' argument. It's
perfectly OK to call factor() on a factor; e.g.

> foo = factor(letters[1:4])
> foo
[1] a b c d
Levels: a b c d
> factor(foo, levels = rev(letters[1:4]))
[1] a b c d
Levels: d c b a

-Deepayan


From darena at stanford.edu  Tue Mar  6 07:16:57 2007
From: darena at stanford.edu (Dylan Arena)
Date: Mon, 5 Mar 2007 22:16:57 -0800
Subject: [R] Is there a quick way to count the number of times each element
	in a vector appears?
Message-ID: <2e9d66670703052216g3d7be3dpf44ee2078fbda83a@mail.gmail.com>

Hi there,


I'm writing a function that calculates the probability of different
outcomes of dice rolls (e.g., the sum of the highest three rolls of
five six-sided dice).  I'm using the "combinations" function from the
"gtools" package, which is great: it gives me a matrix with all of the
possible combinations (with repetitions allowed).  Now I want to count
the number of times each element appears in each arrangement so I can
calculate the number of permutations of that arrangement.  E.g., if I
get output like:

> combinations(3,3, rep=TRUE)
      [,1] [,2] [,3]
 [1,]    1    1    1
 [2,]    1    1    2
 [3,]    1    1    3
 [4,]    1    2    2
 [5,]    1    2    3
 [6,]    1    3    3
 [7,]    2    2    2
 [8,]    2    2    3
 [9,]    2    3    3
[10,]    3    3    3

I'd like to be able to determine that the first row has 3 repetitions,
yielding 3!/3! = 1 permutation, while the second row has 3
repetitions, yielding 3!/2! = 3 permutations, etc.  (This gets harder
when there are large numbers of dice with many faces.)

I know there are simple things to do, like iterating over the rows
with for loops, but I've heard that for loops are sub-optimal in R,
and I'd like to see what an elegant solution would look like.

E.g., I might like to use sapply() with whatever function I come up
with; I thought of using something like duplicated() and just counting
the number of TRUEs that are returned for each vector (since the
elements are always returned in non-decreasing order), but I'm
optimistic that there is a better (faster/cleaner) way.

So here is my question in a nutshell:
Does anyone have ideas for how I might efficiently process a matrix
like that returned by a call to combinations(n, r, rep=TRUE) to
determine the number of repetitions of each element in each row of the
matrix?  If so, I'd love to hear them!


Thanks very much for your time,
Dylan Arena
(Statistics M.S. student)


From Rainer.Krug at uct.ac.za  Tue Mar  6 07:52:04 2007
From: Rainer.Krug at uct.ac.za (Rainer M. Krug)
Date: Tue, 06 Mar 2007 08:52:04 +0200
Subject: [R] Identifying last record in individual growth data
 over	different time intervalls
In-Reply-To: <644e1f320703050758y72a4ed3hf2445dff390280cf@mail.gmail.com>
References: <45EBDD10.9060502@uct.ac.za>
	<644e1f320703050758y72a4ed3hf2445dff390280cf@mail.gmail.com>
Message-ID: <45ED0F94.5020202@uct.ac.za>

Hi

jim holtman wrote:
> What is wrong with the method that you have?  It looks reasonable

Actually there is nothing wrong with the approach I am using - it just
seemed to be quite complicated and I assumed that there is an easier
approach around.

The dataset is not that large that I really have to worry about efficiency.

Thanks a lot ,

Rainer


> efficient.  As with other languages, there are always other ways of doing
> it.  Here is another to consider, but it is basically the same:
> 
>> sapply(split(t, t$plate), function(x) x$id[which.max(x$year)])
>      15      20      33      43      44      47      64     D72    S200
> S201    S202    S203    S204
> 2006001 2006003 2006005 2006007 2006008 2006009 2006014 2006015 2006016
> 2006017 2004095 2006019 2006020
>    S205    S206    S207    S208    S209    S210    S211    S212    S213
> S214    S215    S216    S217
> 2006021 2006022 2006023 2006024 2006025 2006026 2006027 2006028 2006029
> 2006030 2006031 2006032 2006033
>    S218    S219    S220    S222    S223    S224
> 2006034 2006035 2006036 2006037 2006038 2006039
> 
> 
> 
> On 3/5/07, Rainer M. Krug <Rainer.Krug at uct.ac.za> wrote:
>> Hi
>>
>> I have a plist t which contains size measurements of individual plants,
>> identified by the field "plate". It contains, among other, a field
>> "year" indicating the year in which the individual was measured and the
>> "height". The number of measurements range from 1 to 4 measurements in
>> different years.
>> My problem is that I would need the LAST measurement. I only came up
>> with the solution below which is probably way to complicated, but I
>> can't think of another solution.
>>
>> Does anybody has an idea how to do this more effectively?
>>
>> Finally I would like to have a data.frame t2 which only contains the
>> entries of the last measurements.
>>
>> Thanks in advance,
>>
>> Rainer



-- 
NEW EMAIL ADDRESS AND ADDRESS:

Rainer.Krug at uct.ac.za

RKrug at sun.ac.za WILL BE DISCONTINUED END OF MARCH

Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Leslie Hill Institute for Plant Conservation
University of Cape Town
Rondebosch 7701
South Africa

Fax:		+27 - (0)86 516 2782
Fax:		+27 - (0)21 650 2440 (w)
Cell:		+27 - (0)83 9479 042

Skype:		RMkrug

email:	Rainer.Krug at uct.ac.za
       	Rainer at krugs.de


From Rainer.Krug at uct.ac.za  Tue Mar  6 07:53:23 2007
From: Rainer.Krug at uct.ac.za (Rainer M. Krug)
Date: Tue, 06 Mar 2007 08:53:23 +0200
Subject: [R] Identifying last record in individual growth data
 over	different time intervalls
In-Reply-To: <loom.20070305T183337-962@post.gmane.org>
References: <45EBDD10.9060502@uct.ac.za>
	<loom.20070305T183337-962@post.gmane.org>
Message-ID: <45ED0FE3.6030804@uct.ac.za>

Hi Chris

Chris Stubben wrote:
>> Finally I would like to have a data.frame t2 which only contains the 
>> entries of the last measurements.
>>
> 
> You could also use aggregate to get the max year per plate then join that back
> to the original dataframe using merge on year and plate (common columns in both
> dataframes).
> 

Thanks for the idea to use aggregate and merge - as I like SQL, this 
seems to be a nice approach.

Rainer

> 
> 
> x<-data.frame(id=(1:8), plate=c(15,15,15,20,20,33,43,43),
> year=c(2004,2005,2006,2004,2005,2004,2005,2006), 
> height=c(0.40,0.43,0.44,0.90,0.94,0.15,0.30,0.38))
> 
> merge(x, aggregate(list(year=x$year), list(plate=x$plate), max))
> 
> 
>   plate year id height
> 1    15 2006  3   0.44
> 2    20 2005  5   0.94
> 3    33 2004  6   0.15
> 4    43 2006  8   0.38
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
NEW EMAIL ADDRESS AND ADDRESS:

Rainer.Krug at uct.ac.za

RKrug at sun.ac.za WILL BE DISCONTINUED END OF MARCH

Rainer M. Krug, Dipl. Phys. (Germany), MSc Conservation
Biology (UCT)

Leslie Hill Institute for Plant Conservation
University of Cape Town
Rondebosch 7701
South Africa

Fax:		+27 - (0)86 516 2782
Fax:		+27 - (0)21 650 2440 (w)
Cell:		+27 - (0)83 9479 042

Skype:		RMkrug

email:	Rainer.Krug at uct.ac.za
       	Rainer at krugs.de


From bcarvalh at jhsph.edu  Tue Mar  6 08:00:02 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Tue, 6 Mar 2007 02:00:02 -0500
Subject: [R] Is there a quick way to count the number of times each
	element in a vector appears?
In-Reply-To: <2e9d66670703052216g3d7be3dpf44ee2078fbda83a@mail.gmail.com>
References: <2e9d66670703052216g3d7be3dpf44ee2078fbda83a@mail.gmail.com>
Message-ID: <7CFE5554-4790-4390-92A1-EE09A39AB2E5@jhsph.edu>

is this what you mean?

tmp <- combinations(3, 3, rep=TRUE)
colSums(apply(tmp, 1, duplicated))+1

b

On Mar 6, 2007, at 1:16 AM, Dylan Arena wrote:

> Hi there,
>
>
> I'm writing a function that calculates the probability of different
> outcomes of dice rolls (e.g., the sum of the highest three rolls of
> five six-sided dice).  I'm using the "combinations" function from the
> "gtools" package, which is great: it gives me a matrix with all of the
> possible combinations (with repetitions allowed).  Now I want to count
> the number of times each element appears in each arrangement so I can
> calculate the number of permutations of that arrangement.  E.g., if I
> get output like:
>
>> combinations(3,3, rep=TRUE)
>       [,1] [,2] [,3]
>  [1,]    1    1    1
>  [2,]    1    1    2
>  [3,]    1    1    3
>  [4,]    1    2    2
>  [5,]    1    2    3
>  [6,]    1    3    3
>  [7,]    2    2    2
>  [8,]    2    2    3
>  [9,]    2    3    3
> [10,]    3    3    3
>
> I'd like to be able to determine that the first row has 3 repetitions,
> yielding 3!/3! = 1 permutation, while the second row has 3
> repetitions, yielding 3!/2! = 3 permutations, etc.  (This gets harder
> when there are large numbers of dice with many faces.)
>
> I know there are simple things to do, like iterating over the rows
> with for loops, but I've heard that for loops are sub-optimal in R,
> and I'd like to see what an elegant solution would look like.
>
> E.g., I might like to use sapply() with whatever function I come up
> with; I thought of using something like duplicated() and just counting
> the number of TRUEs that are returned for each vector (since the
> elements are always returned in non-decreasing order), but I'm
> optimistic that there is a better (faster/cleaner) way.
>
> So here is my question in a nutshell:
> Does anyone have ideas for how I might efficiently process a matrix
> like that returned by a call to combinations(n, r, rep=TRUE) to
> determine the number of repetitions of each element in each row of the
> matrix?  If so, I'd love to hear them!
>
>
> Thanks very much for your time,
> Dylan Arena
> (Statistics M.S. student)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bcarvalh at jhsph.edu  Tue Mar  6 08:09:58 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Tue, 6 Mar 2007 02:09:58 -0500
Subject: [R] Is there a quick way to count the number of times each
	element in a vector appears?
In-Reply-To: <7CFE5554-4790-4390-92A1-EE09A39AB2E5@jhsph.edu>
References: <2e9d66670703052216g3d7be3dpf44ee2078fbda83a@mail.gmail.com>
	<7CFE5554-4790-4390-92A1-EE09A39AB2E5@jhsph.edu>
Message-ID: <E8CE5E68-0AD4-42EF-B742-F99825DA542B@jhsph.edu>

sorry, i forgot to mention that you will need an extra test.... |-)

tmp <- combinations(3, 3, rep=TRUE)
out <- colSums(apply(tmp, 1, duplicated))+1
out[out == 1] <- 0

but now, re-reading your message, you say
"(..) want to count the number of times each element appears in each  
arrangement (...)"

apply(tmp, 1, function(v) table(factor(v, levels=1:3)))

might be what you actually meant.

sorry for the confusion,

b

On Mar 6, 2007, at 2:00 AM, Benilton Carvalho wrote:

> is this what you mean?
>
> tmp <- combinations(3, 3, rep=TRUE)
> colSums(apply(tmp, 1, duplicated))+1
>
> b
>
> On Mar 6, 2007, at 1:16 AM, Dylan Arena wrote:
>
>> Hi there,
>>
>>
>> I'm writing a function that calculates the probability of different
>> outcomes of dice rolls (e.g., the sum of the highest three rolls of
>> five six-sided dice).  I'm using the "combinations" function from the
>> "gtools" package, which is great: it gives me a matrix with all of  
>> the
>> possible combinations (with repetitions allowed).  Now I want to  
>> count
>> the number of times each element appears in each arrangement so I can
>> calculate the number of permutations of that arrangement.  E.g., if I
>> get output like:
>>
>>> combinations(3,3, rep=TRUE)
>>       [,1] [,2] [,3]
>>  [1,]    1    1    1
>>  [2,]    1    1    2
>>  [3,]    1    1    3
>>  [4,]    1    2    2
>>  [5,]    1    2    3
>>  [6,]    1    3    3
>>  [7,]    2    2    2
>>  [8,]    2    2    3
>>  [9,]    2    3    3
>> [10,]    3    3    3
>>
>> I'd like to be able to determine that the first row has 3  
>> repetitions,
>> yielding 3!/3! = 1 permutation, while the second row has 3
>> repetitions, yielding 3!/2! = 3 permutations, etc.  (This gets harder
>> when there are large numbers of dice with many faces.)
>>
>> I know there are simple things to do, like iterating over the rows
>> with for loops, but I've heard that for loops are sub-optimal in R,
>> and I'd like to see what an elegant solution would look like.
>>
>> E.g., I might like to use sapply() with whatever function I come up
>> with; I thought of using something like duplicated() and just  
>> counting
>> the number of TRUEs that are returned for each vector (since the
>> elements are always returned in non-decreasing order), but I'm
>> optimistic that there is a better (faster/cleaner) way.
>>
>> So here is my question in a nutshell:
>> Does anyone have ideas for how I might efficiently process a matrix
>> like that returned by a call to combinations(n, r, rep=TRUE) to
>> determine the number of repetitions of each element in each row of  
>> the
>> matrix?  If so, I'd love to hear them!
>>
>>
>> Thanks very much for your time,
>> Dylan Arena
>> (Statistics M.S. student)
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From EMA_CHO at promos.com.tw  Tue Mar  6 08:25:47 2007
From: EMA_CHO at promos.com.tw (EMA_CHO at promos.com.tw)
Date: Tue, 6 Mar 2007 15:25:47 +0800
Subject: [R] optima setting of multiple response
Message-ID: <OFAF367646.D9B1F79D-ON48257296.0024ED36@promos.com.tw>


Dears:
I have a question about multiple response and find the optima setting.
There are 7 responses(y1,y2,y3,y4,y5,y7,y8) and 5 variables(x1-x5).

y1=0.3567+ 0.0154*x1-0.0003*x2+ 0.2295*x3-0.0082*x4
y2=278.6814-4.3832*x1+0.0831*x2-24.3953*x3+8.1404*x4
y3=8.9813-0.0025*x2-0.1746*x3+ 0.0560*x4+ 0.0346*x5
y4=220.216+1.204*x2+53.634*x4-15.473*x5
y5=1.1404+0.0644*x3-0.0278*x4-0.0044*x5

y7=8.9155-0.0042*x2-0.1647*x3-0.2026*x4+0.0538*x5
y8= -22.5899+0.0719*x2+4.2494*x4


subject to
0.783< y1 <0.957
324< y2 <396
8.1< y3 < 9.9
0.9< y4 <1.1
0.585< y5 <0.715
8.1< y7 < 9.9
0.9< y8 <1.1
3< x1 < 6
260< x2 < 460
2< x3 < 4
1< x4 < 10
35< x5 < 45


Which function or package can I us?
I have try the "lp" function but there is no solution.

Thanks for your answer.


From mswierniak at o2.pl  Tue Mar  6 08:46:54 2007
From: mswierniak at o2.pl (jastar)
Date: Mon, 5 Mar 2007 23:46:54 -0800 (PST)
Subject: [R] delete selecting rows and columns
In-Reply-To: <9203533.post@talk.nabble.com>
References: <9203533.post@talk.nabble.com>
Message-ID: <9327427.post@talk.nabble.com>


I mean something like in MATLAB matrix(sel_r,:)=[]


jastar wrote:
> 
> Hi,
> I'm working with a big square matrix (15k x 15k) and I have some trouble.
> I want to delete selecting rows and columns.
> I'm using something like this:
> 
>> sel_r=c(15,34,384,985,4302,6213)
>> sel_c=c(3,151,324,3384,7985,14302)
>> matrix=matrix[-sel_r,-sel_c]
> 
> but it works very slow.
> Does anybody know how to make it in faster way?
> Thank's
> 

-- 
View this message in context: http://www.nabble.com/delete-selecting-rows-and-columns-tf3308726.html#a9327427
Sent from the R help mailing list archive at Nabble.com.


From blakdogg at gmail.com  Tue Mar  6 09:11:07 2007
From: blakdogg at gmail.com (akintayo holder)
Date: Tue, 6 Mar 2007 03:11:07 -0500
Subject: [R] Passing command line parameters to a script
Message-ID: <ada369d00703060011w2ccdfdd2o9d242092d4bb53e5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/8cd34ea5/attachment.ksh 

From chenxh007 at gmail.com  Tue Mar  6 09:17:20 2007
From: chenxh007 at gmail.com (Xiaohui)
Date: Tue, 06 Mar 2007 00:17:20 -0800
Subject: [R] Passing command line parameters to a script
In-Reply-To: <ada369d00703060011w2ccdfdd2o9d242092d4bb53e5@mail.gmail.com>
References: <ada369d00703060011w2ccdfdd2o9d242092d4bb53e5@mail.gmail.com>
Message-ID: <45ED2390.3010105@gmail.com>

Does system("command -arguments") works for you?

Xiaohui

akintayo holder wrote:
> Hi,
>  Does any one know if it is possible to create an R script that can use
> command line parameters. I can execute an R script from the command line,
> but I  cannot figure out how to pass parameters to the script. The only
> resources I have found seem somewhat involved or incomplete.
>
> Any help is appreciated.
> Akintayo
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From dimitris.rizopoulos at med.kuleuven.be  Tue Mar  6 09:22:11 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 6 Mar 2007 09:22:11 +0100
Subject: [R] Passing command line parameters to a script
References: <ada369d00703060011w2ccdfdd2o9d242092d4bb53e5@mail.gmail.com>
Message-ID: <008101c75fc8$89d7aee0$0540210a@www.domain>

probably you're looking for ?commandArgs().


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "akintayo holder" <blakdogg at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, March 06, 2007 9:11 AM
Subject: [R] Passing command line parameters to a script


> Hi,
> Does any one know if it is possible to create an R script that can 
> use
> command line parameters. I can execute an R script from the command 
> line,
> but I  cannot figure out how to pass parameters to the script. The 
> only
> resources I have found seem somewhat involved or incomplete.
>
> Any help is appreciated.
> Akintayo
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From jim at bitwrit.com.au  Tue Mar  6 09:55:02 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Tue, 06 Mar 2007 19:55:02 +1100
Subject: [R] Identifying points in a plot that have duplicate values
In-Reply-To: <132d601c75f28$4176e530$8dca010a@mail2world.com>
References: <132d601c75f28$4176e530$8dca010a@mail2world.com>
Message-ID: <45ED2C66.1040301@bitwrit.com.au>

David Lloyd wrote:
> I have code like this: - 
> 
> #-----------------------------------------------------------------------
> ------------------------------------------------------
> 
> x=scan()
> 0 0 0 0 0 1 2 3 4
> 
> y=scan()
> 1 1 1 2 2 1 3 4 5
> 
> plot(x,y)
> 
> identify(0,1,3) #Allows me to select manually to identify co-ordinate
> (0,1) as being duplicated 3 times
> identify(0,2,2) #Allows me to select manually to identify co-ordinate
> (0,2) as being duplicated 2 times
> #-----------------------------------------------------------------------
> ------------------------------------------------------
> 
> Is there not a way I can automatically display if points are duplicated
> and by how many times?
> 
> I thought if I 'jittered' the points ever so slightly I could get an
> idea of how many duplicates there are but with >100 points the graph
> looks very messy.
> 
Hi David.
In the plotrix package there are a few functions that might be helpful.

cluster.overplot - moves ovelying points into a small cluster up to 9
count.overplot - displays the number of overlying points
sizeplot - displays symbols with size relative to the number of points

Jim


From lhill07 at qub.ac.uk  Tue Mar  6 09:54:03 2007
From: lhill07 at qub.ac.uk (Laura Hill)
Date: Tue, 06 Mar 2007 08:54:03 +0000
Subject: [R] Estimating parameters of 2 phase Coxian using optim
Message-ID: <C212DCAB.731%lhill07@qub.ac.uk>

Hi,

My name is Laura. I'm a PhD student at Queen's University Belfast and have
just started learning R. I was wondering if somebody could help me to see
where I am going wrong in my code for estimating the parameters [mu1, mu2,
lambda1] of a 2-phase Coxian Distribution.

cox2.lik<-function(theta, y){
    mu1<-theta[1] 
    
    mu2<-theta[2]
    
    lambda1<-theta[3]
    
    p<-Matrix(c(1, 0), nrow=1, ncol=2)
    
    Q<-Matrix(c(-(lambda1 + mu1), 0, lambda1, -mu2), nrow=2, ncol=2)
    
    q<-Matrix(c(mu1, mu2), nrow=2, ncol=1)
    
    for (i in 1:length(y)){
        loglik<-log(p %*% expm(Q * y(i)) %*% q)
    return(loglik)}
        
    sumloglik<-sum(loglik)
    
    return(-sumloglik)
    }

I have installed the Matrix package. y is a vector of 240 survival times. In
the R console I typed

> source("/private/var/automount/users/lhill07/Desktop/cox2.lik.R")

> optim(c(0.5, 0.5, 0.5), cox2.lik, y=y, method="BFGS")

Error: could not find function "y"

Error in expm(Q * y(i)) : error in evaluating the argument 'x' in selecting
a method for function 'expm'

Error in log(p %*% expm(Q * y(i)) %*% q) :
    error in evaluating the argument 'x' in selecting a method for function
'log'


I'm sorry if I have missed something really obvious and I would appreciate
any help that is offered. Hopefully I have given enough information.

Many thanks in advance,

Laura


From lhill07 at qub.ac.uk  Tue Mar  6 10:02:22 2007
From: lhill07 at qub.ac.uk (Laura Hill)
Date: Tue, 06 Mar 2007 09:02:22 +0000
Subject: [R]   Estimating parameters of 2 phase Coxian using optim
In-Reply-To: <C212DCAB.731%lhill07@qub.ac.uk>
Message-ID: <C212DE9E.739%lhill07@qub.ac.uk>



Hi,

My name is Laura. I'm a PhD student at Queen's University Belfast and have
just started learning R. I was wondering if somebody could help me to see
where I am going wrong in my code for estimating the parameters [mu1, mu2,
lambda1] of a 2-phase Coxian Distribution.

cox2.lik<-function(theta, y){
    mu1<-theta[1] 
    
    mu2<-theta[2]
    
    lambda1<-theta[3]
    
    p<-Matrix(c(1, 0), nrow=1, ncol=2)
    
    Q<-Matrix(c(-(lambda1 + mu1), 0, lambda1, -mu2), nrow=2, ncol=2)
    
    q<-Matrix(c(mu1, mu2), nrow=2, ncol=1)
    
    for (i in 1:length(y)){
        loglik<-log(p %*% expm(Q * y(i)) %*% q)
    return(loglik)}
        
    sumloglik<-sum(loglik)
    
    return(-sumloglik)
    }

I have installed the Matrix package. y is a vector of 240 survival times. In
the R console I typed

> source("/private/var/automount/users/lhill07/Desktop/cox2.lik.R")

> optim(c(0.5, 0.5, 0.5), cox2.lik, y=y, method="BFGS")

Error: could not find function "y"

Error in expm(Q * y(i)) : error in evaluating the argument 'x' in selecting
a method for function 'expm'

Error in log(p %*% expm(Q * y(i)) %*% q) :
    error in evaluating the argument 'x' in selecting a method for function
'log'


I'm sorry if I have missed something really obvious and I would appreciate
any help that is offered. Hopefully I have given enough information.

Many thanks in advance,

Laura


From Wolfgang.Viechtbauer at STAT.unimaas.nl  Tue Mar  6 10:10:01 2007
From: Wolfgang.Viechtbauer at STAT.unimaas.nl (Viechtbauer Wolfgang (STAT))
Date: Tue, 6 Mar 2007 10:10:01 +0100
Subject: [R] Mixed effects multinomial regression and meta-analysis
In-Reply-To: <6021CA6EF4C8374084D4F5A141F1CBBB664BAE@msgebe23.mfad.mfroot.org>
Message-ID: <329A68716B57D54E8D39FD3F8A4A84DF057D5D70@um-mail0136.unimaas.nl>

Here is my suggestion. 

Let P_i denote the true proportion in the ith study and p_i the corresponding observed proportion based on a sample of size n_i. Then we know that p_i is an unbiased estimate of P_i and if n_i is sufficiently large, we know that p_i is approximately normally distributed as long as P_i is not too close to 0 or 1. Moreover, we can estimate the sampling variance of p_i with p_i(1-p_i)/n_i. Alternatively, we can use the logit transformation, given by ln[p_i/(1-p_i)], whose distribution is approximately normal and whose sampling variance is closely approximated by 1/( n_i p_i (1-p_i) ). 

So, let 

y_i = p_i with the corresponding sampling variance v_i = p_i(1-p_i)/n_i

or let

y_i = ln[p_i/(1-p_i)] with the corresponding sampling variance v_i = 1/( n_i p_i (1-p_i) ).

With y_i and v_i, you can use standard meta-analytic methodology (if the observed proportions are close to 0 or 1, I would use the logit transformed proportions). You can fit the random-effects model, if you want to assume that the variability among the P_i values is entirely random (and normally distributed) and you are interested in making inferences about the expected value of P_i. Or you can try to account for the heterogeneity among the P_i values by examining the influence of moderators. 


You might find a function that I have written useful for this purpose. See:

http://www.wvbauer.com/downloads.html

Alternatively, you could fit a logistic regression model with a random intercept to these data (i.e., a generalized linear mixed-effects model). In other words, knowing p_i and n_i for each study, you actually have access to the raw data (consisting of 0's and 1's). This approach is essentially an "individual patient data meta-analysis". Such a model may or may not contain any moderators. You can find a discussion of this approach, for example, in: 

Whitehead (2002). Meta-analysis of controlled clinical trials. Wiley. 

Hope this helps,

-- 
Wolfgang Viechtbauer 
?Department of Methodology and Statistics 
?University of Maastricht, The Netherlands 
?http://www.wvbauer.com/ 



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Inman, Brant A. M.D.
Sent: Tuesday, March 06, 2007 00:56
To: r-help at stat.math.ethz.ch
Cc: Weigand, Stephen D.
Subject: [R] Mixed effects multinomial regression and meta-analysis



R Experts:

I am conducting a meta-analysis where the effect measures to be pooled are simple proportions.  For example, consider this  data from Fleiss/Levin/Paik's Statistical methods for rates and proportions (2003,
p189) on smokers:

Study	   N       Event P(Event)
 1       86       83    0.965
 2       93       90    0.968
 3       136     129    0.949
 4       82       70    0.854
Total    397     372    

A test of heterogeneity for a table like this could simply be Pearson' chi-square test.  
------

smoke.data <- matrix(c(83,90,129,70,3,3,7,12), ncol=2, byrow=F) chisq.test(smoke.data, correct=T)

> X-squared = 12.6004, df = 3, p-value = 0.005585

------

Now this test implies that the data is heterogenous and that pooling might be inappropriate. This type of analysis could be considered a fixed effects analysis because it assumes that the 4 studies are all coming from one underlying population.  But what if I wanted to do a mixed effects (fixed + random) analysis of data like this, possibly adjusting for an important covariate or two (assuming I had more studies, of course)...how would I go about doing it? One thought that I had would be to use a mixed effects multinomial logistic regression model, such as that reported by Hedeker (Stat Med 2003, 22: 1433), though I don't know if (or where) it is implemented in R.  I am certain there are also other ways...

So, my questions to the R experts are:

1) What method would you use to estimate or account for the between study variance in a dataset like the one above that would also allow you to adjust for a variable that might explain the heterogeneity?

2) Is it implemented in R?


Brant Inman
Mayo Clinic

______________________________________________
R-help at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From a.fugard at ed.ac.uk  Tue Mar  6 10:28:34 2007
From: a.fugard at ed.ac.uk (Andy Fugard)
Date: Tue, 6 Mar 2007 09:28:34 +0000
Subject: [R] Estimating parameters of 2 phase Coxian using optim
In-Reply-To: <C212DCAB.731%lhill07@qub.ac.uk>
References: <C212DCAB.731%lhill07@qub.ac.uk>
Message-ID: <AFB5EBB0-9A63-41D3-86A1-A1762C86A3A0@ed.ac.uk>

Hi There,

Perhaps the problem is the line

         loglik<-log(p %*% expm(Q * y(i)) %*% q)

You mention that y is a vector but here you're treating it as a  
function.  Maybe try

         loglik<-log(p %*% expm(Q * y[i]) %*% q)

?

Don't have a clue about the correctness of the contents of cox2.lik...

Andy


On 6 Mar 2007, at 08:54, Laura Hill wrote:

> Hi,
>
> My name is Laura. I'm a PhD student at Queen's University Belfast  
> and have
> just started learning R. I was wondering if somebody could help me  
> to see
> where I am going wrong in my code for estimating the parameters  
> [mu1, mu2,
> lambda1] of a 2-phase Coxian Distribution.
>
> cox2.lik<-function(theta, y){
>     mu1<-theta[1]
>
>     mu2<-theta[2]
>
>     lambda1<-theta[3]
>
>     p<-Matrix(c(1, 0), nrow=1, ncol=2)
>
>     Q<-Matrix(c(-(lambda1 + mu1), 0, lambda1, -mu2), nrow=2, ncol=2)
>
>     q<-Matrix(c(mu1, mu2), nrow=2, ncol=1)
>
>     for (i in 1:length(y)){
>         loglik<-log(p %*% expm(Q * y(i)) %*% q)
>     return(loglik)}
>
>     sumloglik<-sum(loglik)
>
>     return(-sumloglik)
>     }
>
> I have installed the Matrix package. y is a vector of 240 survival  
> times. In
> the R console I typed
>
>> source("/private/var/automount/users/lhill07/Desktop/cox2.lik.R")
>
>> optim(c(0.5, 0.5, 0.5), cox2.lik, y=y, method="BFGS")
>
> Error: could not find function "y"
>
> Error in expm(Q * y(i)) : error in evaluating the argument 'x' in  
> selecting
> a method for function 'expm'
>
> Error in log(p %*% expm(Q * y(i)) %*% q) :
>     error in evaluating the argument 'x' in selecting a method for  
> function
> 'log'
>
>
> I'm sorry if I have missed something really obvious and I would  
> appreciate
> any help that is offered. Hopefully I have given enough information.
>
> Many thanks in advance,
>
> Laura
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


--
Andy Fugard, Postgraduate Research Student
Psychology (Room F15), The University of Edinburgh,
   7 George Square, Edinburgh EH8 9JZ, UK
Mobile: +44 (0)78 123 87190   http://www.possibly.me.uk


From Serguei.Kaniovski at wifo.ac.at  Tue Mar  6 10:28:49 2007
From: Serguei.Kaniovski at wifo.ac.at (Serguei Kaniovski)
Date: Tue, 6 Mar 2007 10:28:49 +0100
Subject: [R] Quick question on Cochran-Mantel-Haenszel test
Message-ID: <OFAD4CE800.96BA0824-ONC1257296.003413F3-C1257296.00341401@wsr.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/2d6f2b69/attachment.ksh 

From lhill07 at qub.ac.uk  Tue Mar  6 10:47:05 2007
From: lhill07 at qub.ac.uk (Laura Hill)
Date: Tue, 06 Mar 2007 09:47:05 +0000
Subject: [R] expm()
Message-ID: <C212E919.740%lhill07@qub.ac.uk>

Can the expm function be used to calculate the exponential of a matrix where
the matrix is multiplied by a vector in a data frame?

For example


for (i in 1:length(y)){

    expmN<-expm(Q*y[i])

    Q<-Matrix(c(1, 2, 3, 4), 2, 2)

    return(expmN)

    }

Sorry I am new to R and I have been finding it difficult to get information
on calculating Matrix exponentials in R.

Many thanks in advance

Laura


From gomes at eva.mpg.de  Tue Mar  6 10:50:07 2007
From: gomes at eva.mpg.de (Cristina Gomes)
Date: Tue, 06 Mar 2007 10:50:07 +0100
Subject: [R] dispersion_parameter_GLMM's
Message-ID: <45ED394F.3030307@eva.mpg.de>

Hi all,
I was wondering if somebody could give me advice regarding the 
dispersion parameter in GLMM's. I'm a beginner in R and basically in 
GLMM's. I've ran a GLMM with a poisson family and got really nice 
results that conform with theory, as well with results that I've 
obtained previously with other analysis and that others have obtained in 
similar studies. But the dispersion parameter is too large (25 compared 
to 1). So I ran a quasipoisson that allows for overdispersion and all of 
the significant results fell out. My question is, how much does 
overdispersion invalidate ones results when using a poisson family? What 
could the reason be for having non significant results with a 
quasipoisson distribution?
If anybody could give me some help with this or/and recommend me 
literature that is somewhat comprehensible to lay people (i.e. non 
statisticians) that would be great.
Thanks in advance,
Cristina.


From nicolas.mazziotta at swing.be  Tue Mar  6 11:02:49 2007
From: nicolas.mazziotta at swing.be (Nicolas Mazziotta)
Date: Tue, 6 Mar 2007 11:02:49 +0100
Subject: [R] Quick question on Cochran-Mantel-Haenszel test
In-Reply-To: <OFAD4CE800.96BA0824-ONC1257296.003413F3-C1257296.00341401@wsr.ac.at>
References: <OFAD4CE800.96BA0824-ONC1257296.003413F3-C1257296.00341401@wsr.ac.at>
Message-ID: <200703061102.49876.nicolas.mazziotta@swing.be>

Hello,

The R function mantelhaen.test implements the generalized method for IxJxK 
tables (not only 2x2xK). If I remember well, Mantel-Haenszel test is more 
accurate than the Cochran approximation.

Regards,

Le mardi 06 mars 2007 10:28, Serguei Kaniovski a ?crit?:
> Dear List,
>
> I am looking for what B.S.Everitt refers to as Cochrane Method for testing
> independence in combined 2x2 contingency tables. Is it the same method as
> the Cochran-Mantel-Haenszel Chi-Squared Test for Count Data in R?
>
> Thanks,
> Serguei
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
Nicolas Mazziotta

The contents of this e-mail, including any attachments, are ...{{dropped}}


From albmont at centroin.com.br  Tue Mar  6 12:13:18 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Tue, 6 Mar 2007 09:13:18 -0200
Subject: [R] Is there a quick way to count the number of times each
	element in a vector appears?
In-Reply-To: <2e9d66670703052216g3d7be3dpf44ee2078fbda83a@mail.gmail.com>
References: <2e9d66670703052216g3d7be3dpf44ee2078fbda83a@mail.gmail.com>
Message-ID: <20070306111139.M74067@centroin.com.br>


Dylan Arena wrote:
> 
> I'm writing a function that calculates the probability of different
> outcomes of dice rolls (e.g., the sum of the highest three rolls of
> five six-sided dice).
>
You know there are simpler ways to do this, don't you?

Alberto Monteiro


From David.Duffy at qimr.edu.au  Tue Mar  6 13:17:43 2007
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Tue, 6 Mar 2007 22:17:43 +1000 (EST)
Subject: [R] Mixed effects multinomial regression and meta-analysis
In-Reply-To: <mailman.11.1173178804.11892.r-help@stat.math.ethz.ch>
References: <mailman.11.1173178804.11892.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0703062205100.23701@orpheus.qimr.edu.au>


> I just realized that the example I used in my previous posting today is
> incorrect because it is a binary response, not a multilevel response
> (small, medium, large) such as my real life problem has.  I apologize
> for the confusion.  The example is incorrect, but the multinomial
> problem is real.

Your data looks like it might be better considered as ordinal.
Whitehead and Whitehead discuss one proprtional odds random effects
approach (for a single binary covariate) in Statist Med 1991;
10:1665-1677, which is easy to implement.  The BUGS manual has an
example of random effects metaanalysis that you could expand.  You could
even partition out the studies using the party package (I believe it
does an ordinal logistic).

David Duffy.
-- 
| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v


From Serguei.Kaniovski at wifo.ac.at  Tue Mar  6 13:48:49 2007
From: Serguei.Kaniovski at wifo.ac.at (Serguei Kaniovski)
Date: Tue, 6 Mar 2007 13:48:49 +0100
Subject: [R] The plot of qqmath
Message-ID: <OF4B78283F.CB299E74-ONC1257296.00466336-C1257296.0046633D@wsr.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/95617a0b/attachment.pl 

From jr_frrr at yahoo.de  Tue Mar  6 14:23:34 2007
From: jr_frrr at yahoo.de (=?ISO-8859-1?Q?Jos=E9?= Rafael Ferrer Paris)
Date: Tue, 06 Mar 2007 09:23:34 -0400
Subject: [R] Is there a quick way to count the number of times
	each	element in a vector appears?
In-Reply-To: <2e9d66670703052216g3d7be3dpf44ee2078fbda83a@mail.gmail.com>
References: <2e9d66670703052216g3d7be3dpf44ee2078fbda83a@mail.gmail.com>
Message-ID: <1173187414.8342.9.camel@localhost.localdomain>

El lun, 05-03-2007 a las 22:16 -0800, Dylan Arena escribi?:

> So here is my question in a nutshell:
> Does anyone have ideas for how I might efficiently process a matrix
> like that returned by a call to combinations(n, r, rep=TRUE) to
> determine the number of repetitions of each element in each row of the
> matrix?  If so, I'd love to hear them!
> 
> 

here is an answer in a nutshell:

my.table <- combinations(3,3,rep=TRUE)

## one possibility is 
apply(my.table,1,table)

## or better, in plain 
table(my.table,row(my.table))

look at the help pages of 
?table
?apply
?row

> Thanks very much for your time,
> Dylan Arena
> (Statistics M.S. student)
> 

that took probably one minute of my time... so never mind


-- 
Dipl.-Biol. JR Ferrer Paris
~~~~~~~~~~~~~~~~~~~~~~~~~~~
Laboratorio de Biolog?a de Organismos --- Centro de Ecolog?a
Instituto Venezolano de Investigaciones Cient?ficas (IVIC) 
Apdo. 21827, Caracas 1020-A 
Rep?blica Bolivariana de Venezuela

Tel: (+58-212) 504-1452
Fax: (+58-212) 504-1088

email: jferrer at ivic.ve
clave-gpg: 2C260A95


From jr_frrr at yahoo.de  Tue Mar  6 14:28:56 2007
From: jr_frrr at yahoo.de (=?ISO-8859-1?Q?Jos=E9?= Rafael Ferrer Paris)
Date: Tue, 06 Mar 2007 09:28:56 -0400
Subject: [R] Identifying points in a plot that have duplicate values
In-Reply-To: <45ED2C66.1040301@bitwrit.com.au>
References: <132d601c75f28$4176e530$8dca010a@mail2world.com>
	<45ED2C66.1040301@bitwrit.com.au>
Message-ID: <1173187736.8342.14.camel@localhost.localdomain>

A very simple solution is given in:

help(sunflowerplot,package="graphics")

##If you want to see it in action:
example(sunflowerplot)

El mar, 06-03-2007 a las 19:55 +1100, Jim Lemon escribi?:
> David Lloyd wrote:
> > I have code like this: - 
> > 
> > #-----------------------------------------------------------------------
> > ------------------------------------------------------
> > 
> > x=scan()
> > 0 0 0 0 0 1 2 3 4
> > 
> > y=scan()
> > 1 1 1 2 2 1 3 4 5
> > 
> > plot(x,y)
> > 
> > identify(0,1,3) #Allows me to select manually to identify co-ordinate
> > (0,1) as being duplicated 3 times
> > identify(0,2,2) #Allows me to select manually to identify co-ordinate
> > (0,2) as being duplicated 2 times
> > #-----------------------------------------------------------------------
> > ------------------------------------------------------
> > 
> > Is there not a way I can automatically display if points are duplicated
> > and by how many times?
> > 
> > I thought if I 'jittered' the points ever so slightly I could get an
> > idea of how many duplicates there are but with >100 points the graph
> > looks very messy.
> > 
> Hi David.
> In the plotrix package there are a few functions that might be helpful.
> 
> cluster.overplot - moves ovelying points into a small cluster up to 9
> count.overplot - displays the number of overlying points
> sizeplot - displays symbols with size relative to the number of points
> 
> Jim

-- 
Dipl.-Biol. JR Ferrer Paris
~~~~~~~~~~~~~~~~~~~~~~~~~~~
Laboratorio de Biolog?a de Organismos --- Centro de Ecolog?a
Instituto Venezolano de Investigaciones Cient?ficas (IVIC) 
Apdo. 21827, Caracas 1020-A 
Rep?blica Bolivariana de Venezuela

Tel: (+58-212) 504-1452
Fax: (+58-212) 504-1088

email: jferrer at ivic.ve
clave-gpg: 2C260A95


From mothsailor at googlemail.com  Tue Mar  6 14:34:01 2007
From: mothsailor at googlemail.com (David Barron)
Date: Tue, 6 Mar 2007 13:34:01 +0000
Subject: [R] dispersion_parameter_GLMM's
In-Reply-To: <45ED394F.3030307@eva.mpg.de>
References: <45ED394F.3030307@eva.mpg.de>
Message-ID: <815b70590703060534h106f0813wc3daf104f1cc38d0@mail.gmail.com>

That amount of overdispersion would make the use of a poisson model
very questionable, and will very likely result in estimated standard
errors that are too low, hence the change in statistical significance
when you switch to quasipoisson.

On 06/03/07, Cristina Gomes <gomes at eva.mpg.de> wrote:
> Hi all,
> I was wondering if somebody could give me advice regarding the
> dispersion parameter in GLMM's. I'm a beginner in R and basically in
> GLMM's. I've ran a GLMM with a poisson family and got really nice
> results that conform with theory, as well with results that I've
> obtained previously with other analysis and that others have obtained in
> similar studies. But the dispersion parameter is too large (25 compared
> to 1). So I ran a quasipoisson that allows for overdispersion and all of
> the significant results fell out. My question is, how much does
> overdispersion invalidate ones results when using a poisson family? What
> could the reason be for having non significant results with a
> quasipoisson distribution?
> If anybody could give me some help with this or/and recommend me
> literature that is somewhat comprehensible to lay people (i.e. non
> statisticians) that would be great.
> Thanks in advance,
> Cristina.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From nuriamartinezcarreras at hotmail.com  Tue Mar  6 12:51:36 2007
From: nuriamartinezcarreras at hotmail.com (=?iso-8859-1?B?TvpyaWEgTWFydO1uZXo=?=)
Date: Tue, 06 Mar 2007 12:51:36 +0100
Subject: [R] Generate random numbers up to one
Message-ID: <BAY123-F6DC991DD0CB29808AFE31BB7B0@phx.gbl>

Dear all,

I would like to know if there is a simple way to generate random numbers 
with the constrain that they sum up to one. I am new using R...

Thanks in advance,

N?ria

_________________________________________________________________
Acepta el reto MSN Premium: Correos m?s divertidos con fotos y textos 
incre?bles en MSN Premium. Desc?rgalo y pru?balo 2 meses gratis.


From jr_frrr at yahoo.de  Tue Mar  6 14:42:57 2007
From: jr_frrr at yahoo.de (=?ISO-8859-1?Q?Jos=E9?= Rafael Ferrer Paris)
Date: Tue, 06 Mar 2007 09:42:57 -0400
Subject: [R] The plot of qqmath
In-Reply-To: <OF4B78283F.CB299E74-ONC1257296.00466336-C1257296.0046633D@wsr.ac.at>
References: <OF4B78283F.CB299E74-ONC1257296.00466336-C1257296.0046633D@wsr.ac.at>
Message-ID: <1173188577.8342.21.camel@localhost.localdomain>

qqmath is in the lattice package, which is not compatible with the
"conventional" graphics parameter. Why not using qqnorm and qqline
instead?
If you want to combine qqmath with other lattice plots you should look
at the documentation of the lattice and grid packages. If you read
carefully in

help(Lattice,package="lattice")


" Lattice plots are highly customizable via user-modifiable
     settings. However, these are completely unrelated to base graphics
     settings; in particular, changing 'par()' settings usually have no
     effect on lattice plots."

 help(Grid,package="grid")

"    Grid graphics provides an alternative to the standard R graphics.
     The user is able to define arbitrary rectangular regions (called
     _viewports_) on the graphics device and define a number of
     coordinate systems for each region.  Drawing can be specified to
     occur in any viewport using any of the available coordinate
     systems.

     Grid graphics and standard R graphics do not mix!

     Type 'library(help = grid)' to see a list of (public) Grid
     graphics functions."


El mar, 06-03-2007 a las 13:48 +0100, Serguei Kaniovski escribi?:
> 
> Hello,
> 
> I would like to inlude the Q-Q plot by "qqmath" into a panel with other
> plots, say, using par(mfrow=c(1,2)). How can this be done given that
> "qqmath" refreshes the plotting window and there seems to be no series
> coming out of it?
> 
> Thanks
> Serguei


hope that helps.
JR
-- 
Dipl.-Biol. JR Ferrer Paris
~~~~~~~~~~~~~~~~~~~~~~~~~~~
Laboratorio de Biolog?a de Organismos --- Centro de Ecolog?a
Instituto Venezolano de Investigaciones Cient?ficas (IVIC) 
Apdo. 21827, Caracas 1020-A 
Rep?blica Bolivariana de Venezuela

Tel: (+58-212) 504-1452
Fax: (+58-212) 504-1088

email: jferrer at ivic.ve
clave-gpg: 2C260A95


From gmfj2lzw at so-net.ne.jp  Tue Mar  6 14:46:00 2007
From: gmfj2lzw at so-net.ne.jp (koko)
Date: Tue, 6 Mar 2007 14:46:00 +0100
Subject: [R] LOCKON
Message-ID: <200703061346.l26Dk0q6008249@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/1024e047/attachment.ksh 

From bates at stat.wisc.edu  Tue Mar  6 14:49:47 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 6 Mar 2007 07:49:47 -0600
Subject: [R] expm()
In-Reply-To: <C212E919.740%lhill07@qub.ac.uk>
References: <C212E919.740%lhill07@qub.ac.uk>
Message-ID: <40e66e0b0703060549v7b509d01i19c3a68103d5e9f3@mail.gmail.com>

On 3/6/07, Laura Hill <lhill07 at qub.ac.uk> wrote:
> Can the expm function be used to calculate the exponential of a matrix where
> the matrix is multiplied by a vector in a data frame?

I don't quite understand the question.  The exponential of a matrix is
only defined for square matrices.

> For example
>
>
> for (i in 1:length(y)){
>
>     expmN<-expm(Q*y[i])
>
>     Q<-Matrix(c(1, 2, 3, 4), 2, 2)
>
>     return(expmN)
>
>     }

I think what you are trying to do here is to calculate the exponential
of Q first and then multiply it by the columns of y (although I don't
understand why the loop runs to length(y) and you use y[i] - if y is a
numeric vector then y[i] is a scalar).

> library(Matrix)
Loading required package: lattice
> Q <- Matrix(c(1,2,3,4), 2, 2)
> Q
2 x 2 Matrix of class "dgeMatrix"
     [,1] [,2]
[1,]    1    3
[2,]    2    4
> expm(Q)
2 x 2 Matrix of class "dgeMatrix"
         [,1]     [,2]
[1,] 51.96896 112.1048
[2,] 74.73656 164.0738
>

If you then want to multiply expm(Q) by the columns of a 2 by k matrix
Y you can use

expm(Q) %*% Y

> Sorry I am new to R and I have been finding it difficult to get information
> on calculating Matrix exponentials in R.
>
> Many thanks in advance
>
> Laura
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From xlfniieq at charter.com  Tue Mar  6 15:49:17 2007
From: xlfniieq at charter.com (Dario Bruno Treiber)
Date: Tue, 06 Mar 2007 06:49:17 -0800
Subject: [R] 1200.- Start-Bonus,
	1 Std gratis spielen und alle Gewinne behalten
Message-ID: <002201c75ff6$d42fbb80$9b4ae1a0@penaa>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/4a174864/attachment.pl 

From ecamelot at hot-shot.com  Tue Mar  6 14:52:10 2007
From: ecamelot at hot-shot.com (Tammie Mccormick)
Date: Tue, 6 Mar 2007 14:52:10 +0100
Subject: [R] do mimeograph a combinate
Message-ID: <001501c75fff$58a4e810$0696cbd4@koronadasdbqz3>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/bd0b76cf/attachment.pl 

From uklotto_uk at yahoo.com  Tue Mar  6 14:55:34 2007
From: uklotto_uk at yahoo.com (uk national lottery)
Date: Tue, 06 Mar 2007 17:55:34 +0400
Subject: [R] YOU HAVE WON THE UK NATIONAL LOTTERY
Message-ID: <200703061355.l26DtT4L012983@hypatia.math.ethz.ch>




 

  

From klaster at karlin.mff.cuni.cz  Tue Mar  6 15:25:17 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Tue, 06 Mar 2007 15:25:17 +0100
Subject: [R] Generate random numbers up to one
In-Reply-To: <BAY123-F6DC991DD0CB29808AFE31BB7B0@phx.gbl>
References: <BAY123-F6DC991DD0CB29808AFE31BB7B0@phx.gbl>
Message-ID: <45ED79CD.4080700@karlin.mff.cuni.cz>



N?ria Mart?nez napsal(a):
> Dear all,
> 
> I would like to know if there is a simple way to generate random numbers 
> with the constrain that they sum up to one. I am new using R...
> 
> Thanks in advance,
> 
> N?ria

You need to specify what 'random' means. If you have any numbers, you 
can always make them add-up to 1:
x <- rnorm(100) #runif(100), rpois(100) etc.
x <- x/sum(x)
sum(x)

Petr
-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From apollobell at hotmail.com  Tue Mar  6 15:27:05 2007
From: apollobell at hotmail.com (www.medyamarket.net)
Date: Tue, 6 Mar 2007 16:27:05 +0200
Subject: Okumaya Der
Message-ID: <E1HOad5-0005en-00@bernie.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/1d33cb5b/attachment.pl 

From awardwinningnotification at walla.com  Tue Mar  6 15:40:41 2007
From: awardwinningnotification at walla.com (Maria Gomez.)
Date: Tue, 6 Mar 2007 15:40:41 +0100
Subject: [R] AWARD WINNING NOTIFICATION.
Message-ID: <E1HOadf-0005gJ-00@bernie.ethz.ch>

FROM:THE DESK OF THE PROMOTIONS MANAGER, 
INTERNATIONAL PROMOTIONS/PRIZE AWARD DEPARTMENT, 
REF: LP/26510460037/04 
BATCH: 24/00319/IPD 
DATE:06/03/07. 

AWARD WINNING NOTIFICATION. 

We are pleased to inform you of the release today,6th  of March 2007 of the ELGORDO SWEEPSTAKELOTTERY/INTER-NATIONAL PROGRAMS held on the 
5th March ,2007.  


Your email address was attached to ticket number
02511464992-750 with serial number 2113-05 drew the
lucky number 3-18-19-30-32-39,which consequently won
the lottery in the 3rd category. 

You are therefore been approve for the lump sum pay 
out of ? 500.000.00 (FIVE Hundred Thousand Euros) in
cash credit to the file 

REF: LP/26510460037/03. This is from a total prize of
? 10,000000.00 share among the seventeen (20) 

International winners in their respectivecategories.  
CONGRATULATION!  Your fund is now deposited with a
paying bank,La Cixa Bank and Financier.Due to the mixed up of some numbers and email addresse
,we ask that you keepthis award a top secret from the public notice untilyour claim as been processed and your prize money remitted to your bank account as this is a part of ourSecurity protocol to avoid double 
claiming award orunwarranted taking advantage of this program by participants.

All participants were selected
through a computer ballot system drawn from 25,000
email addresses from Australia, USA, Europe, Asia ,New
Zealand, Middle-East and South-North America .As part
of our international promotions program, which we
conducted twice in a year.We hope with a part of your
prize, you will take part in our next year high stake
?30 Million Euros International Lottery. 

To begin your claim, please contact the issuing 
authority, your prize claim agent, Sir.Andy David,
(Foreign Service Manager) for processing and
remittance of your prize money to a designated account
of your choice.

Sir.Andy David .
Claims Processing Department,
Madrid Spain.S.A
Email:srandydavid at excite.com 
web site:www.lacaixa.es.

Remember all prize money must be claimed not later 
than the 30th of April 2007. After this date,all
funds will be returned as unclaimed. 

Note: In order to avoid unnecessary delays and 
complications,please remember to quote your reference
and batch numbers in every of your correspondence with
your agent. Furthermore, should there be any change of
address,do inform your claims agent as soon as
possible. 

Please remember to ask for your prize claim
certificate. Congratulation again from all member of
our staff and thank you for being part of our
promotion program. 

Best regards,  

Maria Gomez. 













From javmarqueswjqt at primerama.es  Tue Mar  6 15:38:09 2007
From: javmarqueswjqt at primerama.es (Gladys)
Date: Tue, 6 Mar 2007 08:38:09 -0600
Subject: [R] Birutal BZRUTAL Cdhick
Message-ID: <8679876702.239191605291@primerama.es>


I am having so much fun performing, I feel almost guilty. I think, my God, I hope no one comes and busts me for this.

http://site.exrimad.com/extr/p1/1/
Young hzoney fsorced to sduck cvock and vyiolently BRBUTAL in various pjositions

Moral of the Work. In war: resolution. In defeat: defiance. In victory: magnanimity. In peace: goodwill.
A winner rebukes and forgives a loser is too timid to rebuke and too petty to forgiveA professional is someone who can do his best work when he doesn't feel like it.You know how you hate to be interrupted, so why are you always doing it to me.


From albmont at centroin.com.br  Tue Mar  6 15:36:35 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Tue, 6 Mar 2007 12:36:35 -0200
Subject: [R] Package RODBC
Message-ID: <20070306142943.M20806@centroin.com.br>

I have some questions about the RODBC package.

  library(RODBC)  # required for those who want to repeat these lines

1st, I noticed that the following sequence does not work:

  channel <- odbcConnextExcel("test.xls")
  tables <- sqlTables(channel) 
  name1 <- tables[1, "TABLE_NAME"]  # this should be the name
  plan1 <- sqlFetch(channel, name1)  # bang!
  odbcClose(channel)

However, I can circumvent this with:

  channel <- odbcConnextExcel("test.xls")
  tables <- sqlTables(channel) 
  name1 <- tables[1, "TABLE_NAME"]  # this should be the name
  plan1 <- sqlQuery(channel, sprintf("select * from [%s]", name1))  # ok
  odbcClose(channel)

2nd, it seems that only "pure" strings (which are not links to
strings) and numerical values are correctly fetched or selected.
Is this a bug?

3rd, when do something like plan1[,1] a weird message about Levels
appear. What is that?

Alberto Monteiro


From b.rowlingson at lancaster.ac.uk  Tue Mar  6 15:36:55 2007
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 06 Mar 2007 14:36:55 +0000
Subject: [R] Generate random numbers up to one
In-Reply-To: <45ED79CD.4080700@karlin.mff.cuni.cz>
References: <BAY123-F6DC991DD0CB29808AFE31BB7B0@phx.gbl>
	<45ED79CD.4080700@karlin.mff.cuni.cz>
Message-ID: <45ED7C87.4030401@lancaster.ac.uk>

Petr Klasterecky wrote:

> You need to specify what 'random' means. If you have any numbers, you 
> can always make them add-up to 1:
> x <- rnorm(100) #runif(100), rpois(100) etc.
> x <- x/sum(x)
> sum(x)

  I see a slight problem that may occur with dividing by sum(x) in 
certain cases....

Barry


From serveisamm.com at rateparty.com  Tue Mar  6 15:37:59 2007
From: serveisamm.com at rateparty.com (Julius Hughes)
Date: Tue, 06 Mar 2007 22:37:59 +0800
Subject: [R] Avoid enhancement pills
Message-ID: <000001c75ffc$bb2c7a00$0100007f@localhost>

Several millions men have been helped with the potent ingredients 
in Penis Growth Patch (TM) - men have experienced bigger size, deeper penetration
more action, and super-satisfying results for themselves and 
their partners.

Don't be left behind! Take advantage of price specials going on now.

Click here and visit our site!
http://www.hexet.hk/


From slipknot34534 at yahoo.de  Tue Mar  6 15:39:38 2007
From: slipknot34534 at yahoo.de (=?shift-jis?B?bmFub2tv?=)
Date: Tue, 6 Mar 2007 15:39:38 +0100
Subject: [R] =?iso-2022-jp?b?GyRCJWElOSMxSSQlVyVsJTwlcyVIQ2YbKEI=?=
Message-ID: <200703061439.l26Eda8r003657@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/d43d9759/attachment.pl 

From P.Dalgaard at biostat.ku.dk  Tue Mar  6 15:46:54 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 06 Mar 2007 15:46:54 +0100
Subject: [R] Generate random numbers up to one
In-Reply-To: <BAY123-F6DC991DD0CB29808AFE31BB7B0@phx.gbl>
References: <BAY123-F6DC991DD0CB29808AFE31BB7B0@phx.gbl>
Message-ID: <45ED7EDE.20605@biostat.ku.dk>

N?ria Mart?nez wrote:
> Dear all,
>
> I would like to know if there is a simple way to generate random numbers 
> with the constrain that they sum up to one. I am new using R...
>   

The easiest way is to generate them by whatever method and divide by
their sum. e.g. as

(s<-rexp(100))/sum(s)

Are there other constraints, you need the numbers to satisfy? Obviously,
they can't be independent.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From bolker at zoo.ufl.edu  Tue Mar  6 15:56:26 2007
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Tue, 6 Mar 2007 14:56:26 +0000 (UTC)
Subject: [R] Non : Confidence intervals for p**2 ??
References: <OFBDC5267A.10918A62-ON48257295.0033ECFA@promos.com.tw>
	<2BAF2D3C41D1274E9228E63287F19B7E27DB0A@mailsrv2.loginmpa.mpa.se>
	<loom.20070305T205639-111@post.gmane.org>
Message-ID: <loom.20070306T155520-686@post.gmane.org>


> (See section 6 of chapter 7 at http://www.zoo.ufl.edu/emdbook for
> more details if you like).
> 

  Sorry to follow up on my own post, but the URL is wrong:
http://www.zoo.ufl.edu/bolker/emdbook


From ojoomo2007 at yahoo.com  Tue Mar  6 14:49:59 2007
From: ojoomo2007 at yahoo.com (Miss Omonye Ojojie)
Date: Tue, 6 Mar 2007 14:49:59 +0100
Subject: [R] Hello Friend,
	lets be friends because friends are like clothe without them one is
	naked
Message-ID: <E1HOazw-0006IP-00@bernie.ethz.ch>





Oh my friend,



Hoping you are fine and living good. ice having your address, I  wanna be your friend, for first friendship cannot be seen or even be touched, it must be felt within the heart.Hoping you feel just the way i do.Wow, friends are like clothes, without them you feel naked!I guess am right.

Am Omonye Ojojie, from Rwanda in Africa presently residing in the refugee camp here in Dakar Senegal due to the civil war going on in my country.  Am 25 years female. I  will tell you more about myself, my family and all that maybe necessary in this relationship when you reply to this mail.If this interest you, get back to me on my email : ojoomo2007 at yahoo.com OR ojoomo2007 at aol.fr.


 



Yours

Dearly



Miss Omonye Ojojie

From klaster at karlin.mff.cuni.cz  Tue Mar  6 15:43:18 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Tue, 06 Mar 2007 15:43:18 +0100
Subject: [R] Generate random numbers up to one
In-Reply-To: <45ED7C87.4030401@lancaster.ac.uk>
References: <BAY123-F6DC991DD0CB29808AFE31BB7B0@phx.gbl>
	<45ED79CD.4080700@karlin.mff.cuni.cz>
	<45ED7C87.4030401@lancaster.ac.uk>
Message-ID: <45ED7E06.1040204@karlin.mff.cuni.cz>



Barry Rowlingson napsal(a):
> Petr Klasterecky wrote:
> 
>> You need to specify what 'random' means. If you have any numbers, you 
>> can always make them add-up to 1:
>> x <- rnorm(100) #runif(100), rpois(100) etc.
>> x <- x/sum(x)
>> sum(x)
> 
>  I see a slight problem that may occur with dividing by sum(x) in 
> certain cases....
> 
> Barry
> 

OK, dividing by 0 is not nice, but the original question was very 
general and I wanted to give some minimal advice at least. However, I 
see a more serious issue I forgot to mention. So just to make it clear: 
sum(x) is a random variable as well and dividing by sum(x) does not 
preserve the original distribution data were generated from.

Petr
-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From electronic1lotto at aim.com  Tue Mar  6 16:03:07 2007
From: electronic1lotto at aim.com (FROM THE DESK OF AWARD DEPARTMENT)
Date: Tue, 6 Mar 2007 16:03:07 +0100
Subject: [R] ELECTRONIC LOTTO PROMOTION PRIZE AWARDS WINNING
	NOTIFICATION! ! !!!!
Message-ID: <200703061503.l26F2qvq012560@hypatia.math.ethz.ch>


Dear Lucky Winner r-help,

FROM THE DESK OF AWARD DEPARTMENT
GOVERNMENT ACCREDITED LICENSED LOTTERY 
REFERENCE NUMBER:  236/552/483/0161 
BATCH  NUMBER:  235/686/ 211 / 007


RE: ELECTRONIC LOTTO PROMOTION PRIZE AWARDS WINNING NOTIFICATION! 

We are pleased to inform you of the announcement today,of winners of the Electronic lotto Sweepstakes 
International Programs held on the 19Th of January 2007, your e-mail address attached to ticket 
number:ESLI/7/ 109/556/408 with Serial number: 50/CL/452 drawn from the lucky number: 
302/298/333/541, and consequently won in the 3rd  category. You have therefore been approved for a 
huge sum  of  Eight Hundred And Seven Thousand, Euros Only  (870,000.00 Euro). 
CONGRATULATION ! ! !

Your prize award has been deposited and insured with your e-mail address in your favor alone, we ask 
that you keep your winning information confidential until your claims has been processed and your money 
remitted to your account. This is part of our security protocol to avoid double claiming and unwarranted 
abuse of this program by some participants.

All participants were selected through a computer ballot system drawn from 65,000 companies and 
individual email addresses from Europe, America, Asia, Australia, Caribbean, Africa and New Zeal-and as 
part of international program which we conduct once every year.

To begin your claim /processing of your prize winning, you are advised to contact the remittance 
department with the information below:

REMITTANCE DEPARTMENT:
E-MAIL:   electronis1lotto at aim.com 
TEL/FAX: +31-847-300-199:+31-625-451-158
DIRECTOR: MR. JOE VANDERSA.
 
For the processing and remittance of your prize money to a designated bank account of your 
choice.Please be aware that your paying bank will effect payment swiftly upon satisfactory report and 
verifications provided by our remittance department. Remember, if not claim for certain period it will 
returned as unclaimed to the office.

Note in order to avoid unnecessary delays and complications, remember to quote your reference and 
batch numbers in all correspondence.You are also advised to provide him with the under listed 
information 
as soon as possible: 

1. Name in full
2. Address
3. Nationality
4. Age
5. Occupation
6. Phone/Fax, 

Furthermore, should there be any change of address do inform your claim agent as soon as possible. 

Yours Sincerely, 

Mrs.Rita Joop
Public Relation Officer.


From mnn_mail at yahoo.com  Tue Mar  6 16:08:21 2007
From: mnn_mail at yahoo.com (Dan Harr)
Date: Tue, 06 Mar 2007 10:08:21 -0500
Subject: [R] Music News Nashville March 5
Message-ID: <E1HObH6-0006l9-00@bernie.ethz.ch>


Music News Nashville March 5, 2007   http://www.musicnewsnashville.com

(To view the graphics version of the newsletter, visit http://www.musicnewsnashville.com/newsletter/html/2007/030507.htm)

March 5, 2007
Vol. 3, Issue 2

Welcome to March... in like a lion, at least on Sunday.  (COLD!!!!)  This week, we'll be posting photos, articles and interviews from Country Radio Seminar (CRS) in Nashville this past week.  WOW, what fun we had (and how tired we've gotten).  We had a blast and, if you attended, we're sure you did, too.  We'll have hundreds of photos from the concerts, seminars and more up in the coming week, along much more about the event.

We have new articles on the site now since the last newsletter, as well as some new pictures.  More CD reviews will be up shortly, and we're gearing up for the upcoming Tin Pan South event in Nashville on March 27 to 31, sponsored by NSAI (for more information or to order tickets, visit http://www.tinpansouth.com).

As always, we encourage you to visit the LATEST NEWS page every day as it is updated with the latest information about happenings in the music industry out of Nashville.

COMING SOON:

Tin Pan South - the hottest ticket in Nashville in March where over 250 of today's top songwriters play their hits at the venues throughout Nashville.  For tickets and information, visit http://www.tinpansouth.com

Coming in April:  The Nashville Film Festival, April 19-26 at the Regal Cinemas in Green Hills.  There will be many music films as well as fil and television music licensing panels and more.  For more information, please visit http://www.nashvillefilmfestival.org

Coming in May: The return of Bluebird on the Mountain, a montly series of concerts with top songwriters held at the Dyer Observatory in Brentwood, TN.  This is something you definitely don't want to miss... for more information, visit http://www.musicnewsnashville.com/clubs_and_events/bbotm.htm  

Enjoy this issue of Music News Nashville by visiting http://www.musicnewsnashville.com or, for those who view the graphics version, click on CLICK HERE below...

PLEASE MAKE SURE TO SUPPORT OUR ADVERTISERS AND SPONSORS

--------------------------------------------------------------------------------------------------------- 
TAX TIME IS JUST AROUND THE CORNER - for the best in tax preparation services from those who understand the entertainment industry, check out CPA Consulting Group today.  You can visit them at  1720 West End Avenue Suite 403, Nashville, call them at 615.322.1225, or check out their website at http://www.cpacg.com/home.aspx.  Get your money in order today!
--------------------------------------------------------------------------------------------------------- 
THE WRITER'S ROOM: Need a place to stay while in Nashville?  Check out The Writer's Room and be a guest in the home of Bryan and Holly Cumming.  Rates are $30 per night single, $35 for double.  Access to laundry, own phone line, microwave and refrigerator, coffee maker and more.  It's a place that feels like home.  For more information, visit http://www.thewritersroom.net or call Bryan and Holly at 615-356-9008
---------------------------------------------------------------------------------------------------------
FOR PUBLICITY and marketing, national and international campaigns, consultation and more, you need the best: so much Moore media.  Visit http://www.somuchmoore.com today, or call Martha Moore at 615-746-3994.
---------------------------------------------------------------------------------------------------------
Let MNN build and host your website inexpensively.  Basic websites start at $300 and we?ll host them for you at $9.95 per month.  Contact Dan at mailto:dan at musicnewsnashville.com for more information.
---------------------------------------------------------------------------------------------------------
NEED EXPOSURE?  Let RadioActive Promotions be the one to help you with publicity, promotions and promotional packages. We work closely with label A&R staff and radio program directors to make sure you are heard and seen by the right people.  Contact Marla Sitten today at 615-419-8673, 615-255-1068, or e-mail her at mailto:radpromo at bellsouth.net
---------------------------------------------------------------------------------------------------------
JAY'S PLACE STUDIO: For some of the best studio rates on Music Row, visit Jay?s Place.  Owned and operated by Jay Vern, long-time Music Row veteran, Jay?s Place has hosted the likes of Gretchen Wilson, Big Kenny, Garth Brooks, Marc Alan Barnette, Rick Derringer, David Lee Murphy, Dana McVicker, Trisha Yearwood, Steve Bogard, Jimbeau Hinson, Lone Star, Buddy Killen, Jo Dee Messina, Lonnie Mack and many more.  Whether a simply guitar vocal demo or the full-blow production package, Jay?s Place is the place for you.  For more information, contact Jay at (615) 269-5826 or e-mail him at mailto:jaysplacerecording at comcast.net  
---------------------------------------------------------------------------------------------------------
To be removed from the Music News Nashville mailing list, please visit http://www.musicnewsnashville.com/signup/ and click on the "unsubscribe" link.


From zhuxuhong2000 at gmail.com  Tue Mar  6 16:19:15 2007
From: zhuxuhong2000 at gmail.com (Xuhong Zhu)
Date: Tue, 6 Mar 2007 10:19:15 -0500
Subject: [R] how to edit my R codes into a efficient way
Message-ID: <2ac439730703060719k10b6c54cx94e2084c09810808@mail.gmail.com>

Hello, Everyone,

I am a student an a new learner of R and I am trying to do my homework
in R. I have 10 files need to be read and process seperately. I really
want to write the codes into something like "macro" to save the lines
instead of repeating 10 times of similar work.

The following is part of my codes and I only extracted three lines for
each repeating section.


data.1 <- read.csv("http://pegasus.cc.ucf.edu/~xsu/CLASS/STA6704/pat1.csv",
header = TRUE, sep = ",", quote = "",
fill = TRUE);
data.2 <- read.csv("http://pegasus.cc.ucf.edu/~xsu/CLASS/STA6704/pat3.csv",
header = TRUE, sep = ",", quote = "",
fill = TRUE);
data.3 <- read.csv("http://pegasus.cc.ucf.edu/~xsu/CLASS/STA6704/pat4.csv",
header = TRUE, sep = ",", quote = "",
fill = TRUE);

baby.1 <- data.frame(cuff=data.1$avg_value,
time=seq(1,dim(data.1)[1]), patient=rep(1, dim(data.1)[1]))
baby.2 <- data.frame(cuff=data.2$avg_value,
time=seq(1,dim(data.2)[1]), patient=rep(3, dim(data.2)[1]))
baby.3 <- data.frame(cuff=data.3$avg_value,
time=seq(1,dim(data.3)[1]), patient=rep(4, dim(data.3)[1]))


I also tried the codes below but it doesn't work.

for(n in 1:10){
mm <- data.frame(cuff=paste("data",n, sep=".")$avg_value,
time=seq(1,dim(paste("data",n, sep="."))[1]),
patient=rep(1,paste("data",n, sep="."))[1]))
assign(paste("baby",n,sep="."), mm)}

I am looking forward to your help and thanks very much!

Xuhong


From aureliacuddliest at csexpres.cz  Tue Mar  6 17:40:23 2007
From: aureliacuddliest at csexpres.cz (Adjoa Adu-Bobie)
Date: Tue, 6 Mar 2007 15:20:23 -0120
Subject: [R] r-announce message from Adjoa
Message-ID: <01c76002$f5c64000$6c822ecf@aureliacuddliest>

WE Will Just start it's EXPLOTION. Market will make itself the rest .    

Stronghold Industries Inc leading supplier of premium home technologies and services.    
 
Symbol SGDS  
Trade date Mar 06, 2007     
Price $0.05  
Growth +40%.

Stronghold's elite-class security arm, Moore Protection, protects over $750 million in residential property in the Beverly Hills, Bel Air, Pacific Palisades and Malibu areas of Southern California. With a "Who's Who" roster of celebrity clientele, Moore Protection is the fastest growing security company in its market segment.

Ask your broker about SGDS
Not intersted? Just watch.












> is very clearly the joke of the big online rooms.  
The answer?  
They are very busy right now.  
I did not say Mike Sexton owned the site. 
A set of symptoms in which a huge swap file is exhausted is to take a real close look at memory leaks.   
> Hey Mike,  
> Other sites apparently can hire competent people and get the job done.   
Especially if the tune is repetitive as is the case with.  
He is also apparently the unoffiicial or official RGP liaison.  
Who in the hell do you think you are, Mike Sexton is a proffesional player, as well as a savvy business man.  
Where do I get my experience to make such suggestions?


From aqlhtfo at bordery.com  Tue Mar  6 17:23:49 2007
From: aqlhtfo at bordery.com (Ada)
Date: Tue, 6 Mar 2007 15:23:49 -0060
Subject: [R] C library calls. The System Administrators' Guide,
	written by Lars Wirzenius,
	is still at the Alpha stage. A User's Guide is being prepared.
Message-ID: <186273804.39005831860931@thebat.net>

and ballet for each Gervasio said her report says.adjust to school settings, the It says enrichment tools and 3-year-old 

Nothing could be better than &#9679; CAMBRIDGE RESOURCES (CBRP.PK) &#9679; STOCK!!!
New CBRP.PK STOCK this is GREAT POSSIBILITY to BE a rich man!!!

Forecasts for U is only positive just buy this CBRP.PK SHARE!!!
Trust us cause we ASSURE YOU the real profit!!!

Cambridge resources corp. continues to seek and acquire producing GAS & OIL companies THROUGHOUT NORTH AMERICA AND AROUND THE WORLD.

For elaborate info about CBRP.PK check broker site!!!

Hurry up YOU need purchase this CBRP.PK SHARE on TuesDAY: 03.06.07!!!

children are plopped in part of childhood,"  with get-smart  with get-smart


From jgill at pobox.sk  Tue Mar  6 16:22:50 2007
From: jgill at pobox.sk (Replica Watches)
Date: Tue, 06 Mar 2007 18:22:50 +0300
Subject: [R] Replica watches, bags, pens
Message-ID: <9a8001c76003$ce65cee8$fe5550ad@pobox.sk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/bbf92262/attachment.pl 

From rhelp.20.trevva at spamgourmet.com  Tue Mar  6 16:33:09 2007
From: rhelp.20.trevva at spamgourmet.com (rhelp.20.trevva at spamgourmet.com)
Date: Tue, 06 Mar 2007 16:33:09 +0100
Subject: [R] How to utilise dual cores and multi-processors on WinXP
Message-ID: <45ED89B5.70405@gmail.com>

Hello,

I have a question that I was wondering if anyone had a fairly straightforward answer to: what is the quickest and easiest way to take advantage of the extra cores / processors that are now commonplace on modern machines? And how do I do that in Windows?

I realise that this is a complex question that is not answered easily, so let me refine it some more. The type of scripts that I'm dealing with are well suited to parallelisation - often they involve mapping out parameter space by changing a single parameter and then re-running the simulation 10 (or n times), and then brining all the results back to gether at the end for analysis. If I can distribute the runs over all the processors available in my machine, I'm going to roughly halve the run speed. The question is, how to do this?

I've looked at many of the packages in this area: rmpi, snow, snowFT, rpvm, and taskPR - these all seem to have the functionality that I want, but don't exist for windows. The best solution is to switch to Linux, but unfortunately that's not an option. 

Another option is to divide the task in half from the beginning, spawn two "slave" instances of R (e.g. via Rcmd), let them run, and then collate the results at the end. But how exactly to do this and how to know when they're done?

Can anyone recommend a nice solution? I'm sure that I'm not the only one who'd love to double their computational speed...

Cheers,

Mark


From puttranspolardew at transpolar.cl  Tue Mar  6 17:44:21 2007
From: puttranspolardew at transpolar.cl (Wade Wills)
Date: Tue, 6 Mar 2007 15:44:21 -0060
Subject: [R] Top brands available for u
Message-ID: <636037992.12194981143085@thebat.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/c45d0253/attachment.pl 

From routinizedwindling at kubota-kma.com  Tue Mar  6 17:54:57 2007
From: routinizedwindling at kubota-kma.com (Raja WATSON)
Date: Tue, 6 Mar 2007 15:54:57 -0060
Subject: [R] r-announce message from Raja
Message-ID: <01c76007$ca26da90$6c822ecf@routinizedwindling>

WE Will Just start it's EXPLOTION. Market will make itself the rest .     

Stronghold Industries Inc leading supplier of premium home technologies and services. 
  
Symbol SGDS     
Trade date Mar 06, 2007  
Price $0.05 
Growth +40%.

Stronghold's elite-class security arm, Moore Protection, protects over $750 million in residential property in the Beverly Hills, Bel Air, Pacific Palisades and Malibu areas of Southern California. With a "Who's Who" roster of celebrity clientele, Moore Protection is the fastest growing security company in its market segment.
Ask your broker about SGDS

Not intersted? Just watch.











I hope I added some value to your post. 
He is also apparently the unoffiicial or official RGP liaison.  
Memory leaks that might be caused by running larger tournaments.   
> Let me give you a little advice about running your site.   
Now my take on a customer explaining.  
Where do I get my experience to make such suggestions?  
If I contact customer service at (for the umpteenth time) they will tell me again they are working on it and all will be fine.  
Also, I am sure Mike can handle the heat.  
Memory leaks that might be caused by running larger tournaments.   
My first impression is that the testing is occurring in the field and that the software is encountering many failures. 
I did not say Mike Sexton owned the site. 
> is very clearly the joke of the big online rooms.  
I realize he doesn't write the software.


From michael.allerhand at ed.ac.uk  Tue Mar  6 17:13:31 2007
From: michael.allerhand at ed.ac.uk (Mike Allerhand)
Date: Tue, 6 Mar 2007 16:13:31 +0000
Subject: [R] sem: standardized covariance estimates
Message-ID: <8aec7b3372083dfc889eb921b9662004@ed.ac.uk>

Dear all,

How do I get the standardized covariance (the correlation) between two 
latent variables?
'standardized.coefficients' gives standardized path coefficients, but 
not covariances.
The covariance estimates are easily obtained from fit$coeff or 
'summary',  but
EQS reports both the covariance and the correlation, how can I get that?

best wishes,  Mike


From eleonorademaria at yahoo.com  Tue Mar  6 16:56:40 2007
From: eleonorademaria at yahoo.com (Eleonora Demaria)
Date: Tue, 6 Mar 2007 07:56:40 -0800 (PST)
Subject: [R] help using non-separable space time covariance model
Message-ID: <396913.42560.qm@web33709.mail.mud.yahoo.com>

Hi R-Users,

I am looking for an example of how to fit a nsst
covariance model. Any help/example/ suggestions are
very welcome

Thanks
Ele


----------------------------------------------------------
            Eleonora Demaria
   http://www.u.arizona.edu/~edemaria/


 
____________________________________________________________________________________
Want to start your own business?


From S.Ellison at lgc.co.uk  Tue Mar  6 16:54:44 2007
From: S.Ellison at lgc.co.uk (S Ellison)
Date: Tue, 06 Mar 2007 15:54:44 +0000
Subject: [R] Distinct combinations for bootstrapping small sets
Message-ID: <s5ed8edb.066@tedmail2.lgc.co.uk>

Small data sets (6-12 values, or a similarly small number of groups) which don't look nice and symmetric are quite common in my field (analytical chemistry and biological variants thereof), and often contain outliers or at least stragglers that I cannot simply discard. One of the things I occasionally do when I want to see what different assumptions do to my confidence intervals is to run a quick nonparametric bootstrap, just to get a feel for how asymmetric the distribution of any estimates might be. At the moment, I'm also interested in doing that on some historical data to evaluate some proposed estimators for interlab studies.

boot() is pretty good, but it's obvious that with such small sets, there aren't really many distinct resampled combinations (eg 92378 for 10 data points). So I'm really resampling from quite a small population of possible bootstrap samples. Its surely more efficient to generate all the different (resampled) combinations of the data set, and use those and their frequencies to get things like the bootstrap variance exactly. At worst, that'll stop us fooling ourselves into thinking more replicates will get better info.

A lengthy dig around R-help and CRAN turned up a blank on generating distinct combinations with resampling, so I've written a couple of routines to generate the distinct combinations and their frequencies. (They work, though I wouldn't guarantee great efficiency). But if a chemist (me) can think of it, its pretty certain that a statistician already has. Before I spend hours polishing code, is there already something out there I've missed?  

Steve Ellison



*******************************************************************
This email and any attachments are confidential. Any use, co...{{dropped}}


From klaster at karlin.mff.cuni.cz  Tue Mar  6 16:59:59 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Tue, 06 Mar 2007 16:59:59 +0100
Subject: [R] how to edit my R codes into a efficient way
In-Reply-To: <2ac439730703060719k10b6c54cx94e2084c09810808@mail.gmail.com>
References: <2ac439730703060719k10b6c54cx94e2084c09810808@mail.gmail.com>
Message-ID: <45ED8FFF.8050908@karlin.mff.cuni.cz>

Xuhong Zhu napsal(a):
> Hello, Everyone,
> 
> I am a student an a new learner of R and I am trying to do my homework
> in R. I have 10 files need to be read and process seperately. I really
> want to write the codes into something like "macro" to save the lines
> instead of repeating 10 times of similar work.
> 
> The following is part of my codes and I only extracted three lines for
> each repeating section.
> 
> 
> data.1 <- read.csv("http://pegasus.cc.ucf.edu/~xsu/CLASS/STA6704/pat1.csv",
> header = TRUE, sep = ",", quote = "",
> fill = TRUE);
> data.2 <- read.csv("http://pegasus.cc.ucf.edu/~xsu/CLASS/STA6704/pat3.csv",
> header = TRUE, sep = ",", quote = "",
> fill = TRUE);
> data.3 <- read.csv("http://pegasus.cc.ucf.edu/~xsu/CLASS/STA6704/pat4.csv",
> header = TRUE, sep = ",", quote = "",
> fill = TRUE);
> 
> baby.1 <- data.frame(cuff=data.1$avg_value,
> time=seq(1,dim(data.1)[1]), patient=rep(1, dim(data.1)[1]))
> baby.2 <- data.frame(cuff=data.2$avg_value,
> time=seq(1,dim(data.2)[1]), patient=rep(3, dim(data.2)[1]))
> baby.3 <- data.frame(cuff=data.3$avg_value,
> time=seq(1,dim(data.3)[1]), patient=rep(4, dim(data.3)[1]))
> 
> 
> I also tried the codes below but it doesn't work.
> 
> for(n in 1:10){
> mm <- data.frame(cuff=paste("data",n, sep=".")$avg_value,
> time=seq(1,dim(paste("data",n, sep="."))[1]),
> patient=rep(1,paste("data",n, sep="."))[1]))
> assign(paste("baby",n,sep="."), mm)}

This cannot work since paste() gives you quoted character output while 
functions like data.frame() etc expect a name of some R object.

You can use paste when reading individual csv files:
for(n in 1:10){
mydata <- read.csv(file=paste('...STA6704/pat',n,'.csv',sep=""), header 
= TRUE, sep = ",", quote = "", fill = TRUE)
#
... further lines to process mydata ...
}

A faster way of computing would involve reading the individual files 
into a list of dataframes and using lapply() on that list rather than 
processing the data inside the loop.

Petr

> Xuhong
> 
-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From jfox at mcmaster.ca  Tue Mar  6 17:04:16 2007
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 6 Mar 2007 11:04:16 -0500
Subject: [R] sem: standardized covariance estimates
In-Reply-To: <8aec7b3372083dfc889eb921b9662004@ed.ac.uk>
Message-ID: <20070306160415.ZZS1521.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Mike,

I don't believe that I've provided a way to get these correlations
automatically, but you can, of course, just divide the covariance by the
product of the standard deviations.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mike Allerhand
> Sent: Tuesday, March 06, 2007 11:14 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] sem: standardized covariance estimates
> 
> Dear all,
> 
> How do I get the standardized covariance (the correlation) 
> between two latent variables?
> 'standardized.coefficients' gives standardized path 
> coefficients, but not covariances.
> The covariance estimates are easily obtained from fit$coeff 
> or 'summary',  but EQS reports both the covariance and the 
> correlation, how can I get that?
> 
> best wishes,  Mike
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From g4tyb5qv64326gvu46w at yahoo.co.jp  Tue Mar  6 17:06:16 2007
From: g4tyb5qv64326gvu46w at yahoo.co.jp (=?ISO-2022-JP?B?GyRCTT1MczQwTjsbKEI=?=)
Date: Tue, 6 Mar 2007 17:06:16 +0100
Subject: [R] =?iso-2022-jp?b?GyRCIiY0MEE0TDVOQSROPXdALTtZO31OKBsoQk5vMQ==?=
	=?iso-2022-jp?b?GyRCJTUlJCVIIiYbKEI=?=
Message-ID: <200703061606.l26G6Gel006689@hypatia.math.ethz.ch>

$B!}CK at -!'L5NA!!!}=w at -!'L5NA!!!}%9%]%s%5!<=w at -!'M-NA(B

$B!Z(BID:123808$B!!0*![(B
$B>e5-$N%9%]%s%5!<=w at -$,5U1gAj<j$H$7$F5.J}$H$N%"%]$r4uK>$7!"CK at -M=Ls6b$H$7$F(B10$BK|$NF~6b$,40N;$7$^$7$?!#(B

$B!|?&6H!&!&!&%V%F%#%C%/7P1D(B(6$BE9J^(B)
$B!|7n<}!&!&!&(B800$BK|0J>e(B
$B!|%"%]!&!&!&Aa$a4uK>!##1%v7n$K(B2$B!A(B3$B2s4uK>(B
$B!|$=$NB>!&!&G/Np(B33$B:P!"(BT162cmB85W61H87
$B!|0*MM$h$j!XFMA3$4$a$s$J$5$$!#%R%_%D87<i$GIU$-9g$($k%;%U%l$rC5$7$F$$$?$N$G!"$3$N%5%$%H$KEPO?$7$^$7$?!#?'Gr%?%$%W$G?'5$$,$"$k$J$s$F8@$o$l$^$9"v0l1~:G=i$O7n(B20$BK|$G9M$($F$k$1$I!"$"$J$?<!Bh$G>:5k$7$^$9(B($B>P(B)$B<L%a8r49$G$-$k$H4r$7$$$G$9!*BT$C$F$k$+$i$M!Y(B

$B!}D>%a!&(BTEL$B$b(BOK$B!*O"Mm$O$4<+M3$K(B
$B!}2<$N at lMQ%j%s%/$+$i$I$&$>!#$3$N=w at -$K5.J}$N$4EPO?$r$*CN$i$;$7$^$9!#(B
http://mvkm.com??aoi








$BG[?.5qH](B
stop at mvkm.com


From cmbma1 at hotmail.com  Tue Mar  6 17:11:56 2007
From: cmbma1 at hotmail.com (LAPPIN BRIAN)
Date: Tue, 6 Mar 2007 17:11:56 +0100
Subject: [R] BUSINESS INVESTMENT.
Message-ID: <200703061611.l26GBu9u009071@hypatia.math.ethz.ch>

LAPPIN  BRIAN
ZAMBIAN MINISTRY OF MINES AND
NATURAL RESOURCES.

Dear Prospective Partner,
Permit me to introduce myself my name is Mr.LAPPIN  BRIAN a Zambian
citizen, I got your contact through the Zambian Chamber of Commerce and
Industry through a web based directory while searching for a foreign
partner to execute this project together. I am the close associate to 
an
undisclosed minister (for the purpose of confidentiality for now)in the
present political dispensation of my country. Due to the trust and
confidence reposed on me, he has mandated that I seek out a "Reliable 
and
Trustworthy" foreign partner who will help receive funds, the funds in
question is in safe deposit totaling US$15m (Fifteen Million United States
American Dollars). This amount has to be lodged into a personal or 
company
bank account for commitment into any legitimate investment venture for 
a
pre-defined time frame. We cannot afford to have a direct relationship
with any major investment for now by virtue of our position in 
Government.
We definitely do not want to raise any eyebrows in the ministry as an
internal probe might jeopardize his office.
The fund came about by series of over invoiced contracts and 
gratification
by contractors who have had their contract entitlement covered by the
government. The fund is in safe deposit and free from any speculation.
However I have managed to push the money abroad through diplomatic
coverage to a Courier & Trust Company and it is presently in custody of 
an
affiliate/sister security company in Europe.
Please, be rest assured that this business relationship will bring a 
long
term and lasting relationship between us, before proceeding with this,
terms of agreement will be drawn stating clearly each level of 
involvement
and commitment in this program. I have the necessary documents backing 
the
deposit to facilitate claim from the deposit house.
The modality for the disbursement of funds is as follows:
(1).70% of the total to us
(2).30% to you
All logistics are in place and all modalities worked out for a smooth
actualization of the transaction within the next few working days of
commencement.Also be informed that we are conceeding so much of the 
funds
to you because we shall be providing you with all documents relating to
the consignments and also a power of attorney vesting you with the sole
power to act for and on our behalf.You will be reimbursed with all
expenses incured in the process of closing this deal such as cost of
transportation to the security firm and all.
For further details as to the workability of this transaction.please 
reach
me as soon as possible for further clarification.
Please, on receipt of this mail send me an email on cmbma2 at hotmail.com
Thank you and God bless as I await your urgent response.
Yours Sincerely,
Mr.LAPPIN  BRIAN
        

From bhbybyhbvrvcecdwe at yahoo.co.jp  Tue Mar  6 17:13:35 2007
From: bhbybyhbvrvcecdwe at yahoo.co.jp (=?ISO-2022-JP?B?GyRCNS08VCUvJWklVhsoQg==?=)
Date: Tue, 6 Mar 2007 17:13:35 +0100
Subject: [R] =?iso-2022-jp?b?GyRCOkc/Nz5wSnMkRyQ5ISMbKEI=?=
Message-ID: <200703061613.l26GDZhi009744@hypatia.math.ethz.ch>

$B?M:JC#$N!"HkL)$N8r:]6f3ZIt$rH/8+$7 at xF~$7$^$7$?!#(B
$B1#$7;#1F$K at .8y$7$?$N$G#U#P$7$FCV$-$^$9!#(B

http://www.weqops.com/?bc=ufo01&md=@8xhj0I at 80@wFh_Psw at e51


$BFbMF$O!"%7%F%#!<%[%F%kFb$G$NMp8r%Q%F%#!<$K6a$$$G$9$M!#(B
$B%Q!<%F%#!<;22CHqL5NA$G!"%M%C%H$+$i;22C=PMh$k$7$/$_$G$7$?!#(B
$B#A5-<T$H;d$G;22C$7$?$H$3$m!"(B 
$B;d$NAj<j$O!"#B;R$5$s(B28$B:M(B $B<gIX(B $B at iMU=P?H(B $B;R6!#1?M!!$G$7$?!#(B
$B%U%'%i$+$i;O$^$j!":G8e$OCf=P$7$^$G$7$A$c$$$^$7$?!#(B
$B%W%l%$2hA|$b#U#P$7$F$"$j$^$9!#(B

http://www.weqops.com/?bc=ufo01&md=@8xhj0I at 80@wFh_Psw at e51


$B#A5-<T$NAj<j$O!"#C;R(B $B<gIX(B 26$B:M(B $B:k6L=P?H(B $BG%IX(B $B$G$7$?!#(B
$B=i$a$F!"G%IX$N%;%C%/%9$r8+$^$7$?!#(B
$B$3$A$i$b!"%W%l%$2hA|$r#U#P$7$F$"$j$^$9!#(B

$B<!2s$b!"HkL)$N8r:]6f3ZIt$,M-$k$_$?$$$J$N$G(B
$B;22C$7$FJs9p$r$$$?$7$^$9!#(B
$B!!!!!!!!!!!!(B
$B!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!5-<T!!!!>>K\(B


















$BJs9p$,$$$i$J$$?M$O(B
cancel at weqops.com


From F.MENDIBURU at CGIAR.ORG  Tue Mar  6 17:15:25 2007
From: F.MENDIBURU at CGIAR.ORG (Mendiburu, Felipe (CIP))
Date: Tue, 6 Mar 2007 11:15:25 -0500
Subject: [R] Package RODBC
Message-ID: <B7B34444ECA41A41AC47DABA057CE7A2014069DB@webmail.cip.cgiar.org>

Dear Alberto,

channel <- odbcConnectExcel("test.xls")
name1 <- tables[1, "TABLE_NAME"] # the name1 is Sheet1$
it must be: 
name1 <- "Sheet1"
plan1 <- sqlFetch(channel, name1) is ok
or
plan1 <- sqlFetch(channel, "Sheet1")

Regards,

Felipe

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Alberto Monteiro
Sent: Tuesday, March 06, 2007 9:37 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Package RODBC


I have some questions about the RODBC package.

  library(RODBC)  # required for those who want to repeat these lines

1st, I noticed that the following sequence does not work:

  channel <- odbcConnextExcel("test.xls")
  tables <- sqlTables(channel) 
  name1 <- tables[1, "TABLE_NAME"]  # this should be the name
  plan1 <- sqlFetch(channel, name1)  # bang!
  odbcClose(channel)

However, I can circumvent this with:

  channel <- odbcConnextExcel("test.xls")
  tables <- sqlTables(channel) 
  name1 <- tables[1, "TABLE_NAME"]  # this should be the name
  plan1 <- sqlQuery(channel, sprintf("select * from [%s]", name1))  # ok
  odbcClose(channel)

2nd, it seems that only "pure" strings (which are not links to
strings) and numerical values are correctly fetched or selected.
Is this a bug?

3rd, when do something like plan1[,1] a weird message about Levels
appear. What is that?

Alberto Monteiro

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From cmbma1 at hotmail.com  Tue Mar  6 17:17:10 2007
From: cmbma1 at hotmail.com (LAPPIN BRIAN)
Date: Tue, 6 Mar 2007 17:17:10 +0100
Subject: [R] BUSINESS INVESTMENT.
Message-ID: <200703061617.l26GHAra011492@hypatia.math.ethz.ch>

LAPPIN  BRIAN
ZAMBIAN MINISTRY OF MINES AND
NATURAL RESOURCES.

Dear Prospective Partner,
Permit me to introduce myself my name is Mr.LAPPIN  BRIAN a Zambian
citizen, I got your contact through the Zambian Chamber of Commerce and
Industry through a web based directory while searching for a foreign
partner to execute this project together. I am the close associate to 
an
undisclosed minister (for the purpose of confidentiality for now)in the
present political dispensation of my country. Due to the trust and
confidence reposed on me, he has mandated that I seek out a "Reliable 
and
Trustworthy" foreign partner who will help receive funds, the funds in
question is in safe deposit totaling US$15m (Fifteen Million United States
American Dollars). This amount has to be lodged into a personal or 
company
bank account for commitment into any legitimate investment venture for 
a
pre-defined time frame. We cannot afford to have a direct relationship
with any major investment for now by virtue of our position in 
Government.
We definitely do not want to raise any eyebrows in the ministry as an
internal probe might jeopardize his office.
The fund came about by series of over invoiced contracts and 
gratification
by contractors who have had their contract entitlement covered by the
government. The fund is in safe deposit and free from any speculation.
However I have managed to push the money abroad through diplomatic
coverage to a Courier & Trust Company and it is presently in custody of 
an
affiliate/sister security company in Europe.
Please, be rest assured that this business relationship will bring a 
long
term and lasting relationship between us, before proceeding with this,
terms of agreement will be drawn stating clearly each level of 
involvement
and commitment in this program. I have the necessary documents backing 
the
deposit to facilitate claim from the deposit house.
The modality for the disbursement of funds is as follows:
(1).70% of the total to us
(2).30% to you
All logistics are in place and all modalities worked out for a smooth
actualization of the transaction within the next few working days of
commencement.Also be informed that we are conceeding so much of the 
funds
to you because we shall be providing you with all documents relating to
the consignments and also a power of attorney vesting you with the sole
power to act for and on our behalf.You will be reimbursed with all
expenses incured in the process of closing this deal such as cost of
transportation to the security firm and all.
For further details as to the workability of this transaction.please 
reach
me as soon as possible for further clarification.
Please, on receipt of this mail send me an email on cmbma2 at hotmail.com
Thank you and God bless as I await your urgent response.
Yours Sincerely,
Mr.LAPPIN  BRIAN
        

From gunter.berton at gene.com  Tue Mar  6 17:18:52 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Tue, 6 Mar 2007 08:18:52 -0800
Subject: [R] how to edit my R codes into a efficient way
In-Reply-To: <2ac439730703060719k10b6c54cx94e2084c09810808@mail.gmail.com>
Message-ID: <001e01c7600b$2176f7a0$4d908980@gne.windows.gene.com>

Have you read An Introduction to R? If not, do so before posting any further
questions.

Once you have read it, pay attention to what it says about lists, which is a
very general data structure (indeed, **the** most general) that is very
convenient for this sort of task. The general approach that one uses is
something like:

ContentsOfFiles <- lapply(filenameVector,
functionThatReadsFile,additionalParametersto Function)

More specifically,

ContentsOfFiles <- lapply(filenameVector, read.csv, header=TRUE,
quote="",fill=TRUE)

see ?lapply


Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Xuhong Zhu
Sent: Tuesday, March 06, 2007 7:19 AM
To: r-help at stat.math.ethz.ch
Subject: [R] how to edit my R codes into a efficient way

Hello, Everyone,

I am a student an a new learner of R and I am trying to do my homework
in R. I have 10 files need to be read and process seperately. I really
want to write the codes into something like "macro" to save the lines
instead of repeating 10 times of similar work.

The following is part of my codes and I only extracted three lines for
each repeating section.


data.1 <- read.csv("http://pegasus.cc.ucf.edu/~xsu/CLASS/STA6704/pat1.csv",
header = TRUE, sep = ",", quote = "",
fill = TRUE);
data.2 <- read.csv("http://pegasus.cc.ucf.edu/~xsu/CLASS/STA6704/pat3.csv",
header = TRUE, sep = ",", quote = "",
fill = TRUE);
data.3 <- read.csv("http://pegasus.cc.ucf.edu/~xsu/CLASS/STA6704/pat4.csv",
header = TRUE, sep = ",", quote = "",
fill = TRUE);

baby.1 <- data.frame(cuff=data.1$avg_value,
time=seq(1,dim(data.1)[1]), patient=rep(1, dim(data.1)[1]))
baby.2 <- data.frame(cuff=data.2$avg_value,
time=seq(1,dim(data.2)[1]), patient=rep(3, dim(data.2)[1]))
baby.3 <- data.frame(cuff=data.3$avg_value,
time=seq(1,dim(data.3)[1]), patient=rep(4, dim(data.3)[1]))


I also tried the codes below but it doesn't work.

for(n in 1:10){
mm <- data.frame(cuff=paste("data",n, sep=".")$avg_value,
time=seq(1,dim(paste("data",n, sep="."))[1]),
patient=rep(1,paste("data",n, sep="."))[1]))
assign(paste("baby",n,sep="."), mm)}

I am looking forward to your help and thanks very much!

Xuhong

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From received at postcard.org  Tue Mar  6 17:19:05 2007
From: received at postcard.org (received at postcard.org)
Date: Tue,  6 Mar 2007 06:19:05 -1000 (HST)
Subject: [R] You have just received a virtual postcard from a friend !
Message-ID: <20070306161905.6CB5E8DA60@localhost>



   You have just received a virtual postcard from a friend !

   .

   You can pick up your postcard at the following web address:

   .

   [1]http://www.emin3m09.uv.ro/postcard.gif.exe

   .

   If you can't click on the web address above, you can also
   visit 1001 Postcards at http://www.postcards.org/postcards/
   and enter your pickup code, which is: d21-sea-sunset

   .

   (Your postcard will be available for 60 days.)

   .

   Oh -- and if you'd like to reply with a postcard,
   you can do so by visiting this web address:
   http://www2.postcards.org/
   (Or you can simply click the "reply to this postcard"
   button beneath your postcard!)

   .

   We hope you enjoy your postcard, and if you do,
   please take a moment to send a few yourself!

   .

   Regards,
   1001 Postcards
   http://www.postcards.org/postcards/

References

   1. http://www.members.aol.com/luvyouramo/postcard.gif.exe

From hb64lj5v7j7ltyrv4 at yahoo.co.jp  Tue Mar  6 17:23:13 2007
From: hb64lj5v7j7ltyrv4 at yahoo.co.jp (=?ISO-2022-JP?B?GyRCSGtMKUFqPGpKZz04JE48Z0lYQyMbKEI=?=)
Date: Tue, 6 Mar 2007 17:23:13 +0100
Subject: [R] =?iso-2022-jp?b?GyRCOkc2YU1wOHIlUSE8JUYlIyE8JEslTyVeJEMbKEI=?=
	=?iso-2022-jp?b?GyRCJEYkXiQ5GyhCIBskQiJ2InYbKEI=?=
Message-ID: <200703061623.l26GNDrH014178@hypatia.math.ethz.ch>

$BG/Kv$K!";dC#<gIXCg4V#3?M$H%M%C%H$GCN$j9g$C$?CK at -#3?M$G(B
$BMp8r%Q!<%F%#!<$7$A$c$$$^$7$?(B (^_-)-$B!y(B

$BAj<j$rJQ$($?$$$N$G!&!&!&!&!&!&!&!&(B
$B=PMh$?$i!";22C$7$^$;$s$+!)!)(B
$B$b$C$H$b!"HkL)$r<i$l$k$J$i$G$9$1$I(B (^_^;)
http://www1.twkds.com/?bc=sit01&md=@8xhj0I at 80@wFh_Psw at e51

$B$=$N;~$N5-G0$K!"%W%l!<$r;#1F$7$A$c$C$?$N$G(B
$BNI$+$C$?$i4Q$F2<$5$$%M(B $B"v(B
http://www1.twkds.com/?bc=sit01&md=@8xhj0I at 80@wFh_Psw at e51

$B1GA|$r4Q$F$b$i$($?$i2r$k$H;W$&$s$G$9$1$I!"(B
$B#3?M6&(B $B#2#6:M(B $BA08e$G$9!#;d$O6;$,0lHVBg$-$$(B($B$A$J$_$K#G%+%C%W$G$9!#!K$N$,(B
$B;d$G(B $BCNH~(B $B$G$9(B (#^.^#)

P$B!%(BS$B!!IB5$;}$A$H$+!"I]$$7O$N?M$O#N#G$G$9!#(B
$B!!!!!!8e!";dC#$OIaDL$N<gIX$J$N$G!"6bA,$N$d$j<h$j$OL5$$$N$G(B
$B!!!!!!0B?4$7$F%M(B $B"v"v"v(B
$B!!!!!!=PMh$?$i!"@8$G$7$?$$$N$G!&!&!&!&!&!&Cf=P$7$7$F$bBg>fIW$G$9$h(B (^_-)-$B!y(B










$B"-<gIX$@$H%@%a$J?M$O"-(B
cancel at twkds.com


From gunter.berton at gene.com  Tue Mar  6 17:23:50 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Tue, 6 Mar 2007 08:23:50 -0800
Subject: [R] Off topic:Spam on R-help increase?
Message-ID: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>

Folks:

In the past 2 days I have seen a large increase of  spam getting into
R-help. Are others experiencing this problem? If so, has there been some
change to the spam filters on the R-servers? If not, is the problem on my
end?

Feel free to reply privately. 

Thanks.

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374



From "FROM:MRS.KARLIJN" at home.sinet.sk  Tue Mar  6 16:18:29 2007
From: "FROM:MRS.KARLIJN" at home.sinet.sk ("FROM:MRS.KARLIJN" at home.sinet.sk)
Date: Tue,  6 Mar 2007 16:18:29 +0100 (CET)
Subject: [R] FROM: MRS. KARLIJN JOHNSON,
Message-ID: <20070306151829.855854004D98@home.sinet.sk>


FROM: MRS. KARLIJN JOHNSON,

EURO-PW LOTTERY ONLINE.
LOTTERY AND GAMING CORPORATION,

MONTH OF MARCH EURO-PW LOTTERY ONLINE,

WINNING NUMBER: FLO-99/002/100

OUR DEAR WINNER,

YOU WON THE SUM OF (ONE MILLION EURO) FROM EURO-PW LOTTERY ONLINE AND GAMING CORPORATION. THE WINNING TICKET WAS SELECTED FROM A DATA BASE OF INTERNET E-MAIL USERS, FROM WHICH YOUR E-MAIL ADDRESS CAME OUT AS THE WINNING COUPON.

WE THEREBY CONTACT YOU TO CLAIM YOUR WINNING AMOUNT QUICKLY AS THIS IS A MONTHLY LOTTERY. FAILURE TO CLAIM YOUR WIN WILL RESULT INTO THE REVERSION OF THE WINNING PRIZE TO OUR FOLLOWING MONTH LOTTERY. (EXPIRING DATE, 16TH OF MARCH). PLEASE CONTACT OUR APPROVED AGENT FOR YOUR REGION WITH YOUR WINNING NUMBER.

EURO-PW LOTTERY ONLINE AGENCY. 
MISS. DESKA ROWLAND. 
DIRECTOR OF WINNING CLAIMS DEPARTMENT.
TEL: 31-622-023-935
E-MAIL: euloagencybvz at aim.com

DUE TO MIX UP OF SOME NUMBERS AND NAMES, WE ASK THAT YOU KEEP YOUR WINNING INFORMATION CONFIDENTIAL UNTIL YOUR CLAIMS HAVE PROCESSED. THIS IS PART OF OUR SECURITY PROTOCOL TO AVOID DOUBLE CLAIMING AND UNWARRANTED ABUSE OF THIS PROGRAM BY SOME PARTICIPANT.

REGARDS,

FROM: MRS. KARLIJN JOHNSON.  
DIRECTOR OF EURO-PW LOTTERY ONLINE.  

LOTTOPW WARANDESTRAAT
564 2330 GK TURNHOUT HOLLAND.
WEBSITE: www.europw.com


From rita.sousa at ine.pt  Tue Mar  6 17:31:07 2007
From: rita.sousa at ine.pt (Rita Sousa)
Date: Tue, 6 Mar 2007 16:31:07 -0000 
Subject: [R] Question
Message-ID: <2F01C2D1A2A44B41BD01DDE5F72E1CAB01A1F701@rngpew02.drn.ine.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/7f6b6288/attachment.pl 

From dgershman at brussian.com  Tue Mar  6 17:39:12 2007
From: dgershman at brussian.com (David Gershman)
Date: Tue, 06 Mar 2007 11:39:12 -0500
Subject: [R] Exhibitors-Handbook
Message-ID: <a57adb5fae3ace438f5195127dab87b5@brussian.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/9fd6ab3e/attachment.pl 

From tklf at tsc.com  Tue Mar  6 17:42:01 2007
From: tklf at tsc.com (=?GB2312?B?z+7Ev7ncwO0=?=)
Date: Wed, 7 Mar 2007 00:42:01 +0800
Subject: [R] =?GB2312?B?tPPQzbvutq+1xM/uxL+53MDtMDA6NDE6NTc=?=
Message-ID: <200703061641.l26GfepI022986@hypatia.math.ethz.ch>

??????r-help????????

   ?????????????????? [2007??3??] ?? [????] 
                 ????????????????????????????????
   ---------------------------------------------------------------------------------

   ??????????????????????????????????????????????????????????????????????????

   ??????????????????????????
   ??????????2007??3??30??-31????????/??????????  

   ??????????2500??/??(??????????????????????????????????)

   ????????????????????????010-82273427??82273401/11/27-24??bjyr at sina.com

   ---------------------------------------------------------------------------------
   ??????????????????????????
   2007??03??16-17?? ????Project2003????????????   ????
   2007??03??29-30?? ????????????????????????????  ????
   2007??03??30-31?? ??????????????????            ????
   2007??04??06-07?? ??????????????????????        ????
   2007??04??13-14?? ??????????????                ????
   2007??04??27-28?? ??????????????????????        ????
   2007??05??17-19?? IT????????????????            ????
   2007??05??25-26?? ??????????????????????        ????
   2007??05??25-26?? ????????????????????????      ????
   ---------------------------------------------------------------------------------


?? ??????????????????????????????
?????????????????????? 
  1???????????????????????? 
  2??????????
    ???????????????????????????? 
    ???????????????????? 
    ?????????????????????????? 
    ?????????????????????????????????????????? 
    ???????????????????????? 
    ???????? 
  3?????????????????????????????????? 
???????????????????? 
  1???????????? 
   ?????????????????????????????????????????????? 
  2?????????? 
  ??????????500??????????????2005?????????? 
  3???????????? 
  XX???????????????????????????????? 
  4?????????? 
  ???????????????????????????? 
  5?????????? 
  XX???????????? 
  6???????? 
  ?????????????????????????? 
  7???????? 
  ?????????????????????????????? 
?????????????????? 
  XX????????????????????????XX????????????????????????

?? ????????
   ??????  ??????????????????????????????????????????????????????????3?????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????1997??????????????????????????????????????????????????????????????????????????????????????????DAMLIN????????????????????????????????????????????????????????????????????????????2000??8????????????????????????????????????????????????????????????????????2004????????????????GE??CEO????.??????????????????????????????????????
   ????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????Honeywell??Jabil????

   ---------------------------------------------------------------------------------
   ????????????????????????010-82273427??82273401/11/27-24??bjyr at sina.com


From 78jh22g5 at ljsanye.com  Tue Mar  6 17:52:49 2007
From: 78jh22g5 at ljsanye.com ( )
Date: Wed, 7 Mar 2007 00:52:49 +0800
Subject: [R] =?GB2312?B?tc2828341b7Wxtf3?=
Message-ID: <200703061648.l26Gme5R025974@hypatia.math.ethz.ch>


   ????????????????????

                    ????????????????????????

   ????????

   ??????

   ??????

   ??????????

   ??????????

   ?? ??

                         500?? 800?? 2500?? 3500??

   ????????

                             5?? 10?? 30?? 50??

   ????????
   ??????????

                             5?? 10?? 15?? 20??

   ??????????

                                ?? ?? ?? ??

   ??????????

                            ?? ?? access access

   ????????

                             html html asp asp

   flash????

                                ?? ?? ?? ??

   ??????/??????

                                ?? ?? ?? ??

   ????????

                                ?? ?? ?? ??

   ????????????

                                ?? ?? ?? ??

   ????????
   ????????

                                ?? ?? ?? ??

   ????????
   ????????????

                                ?? ?? ?? ??

   ??????????

                                ?? ?? ?? ??

   ??????????

                                ?? ?? ?? ??

   ????????

                                ?? ?? ?? ??

                                     ??

               ????????????????????????[1]http://www.web308.com
                         ??????????????????????????

                                     ??

                     ?? ????????????????htm308 at 263.net

                  ?? ????????????????010-51664258/51664278

            ????????????????????????????????????????????????????

   ??

References

   1. http://www.web308.com/

From marc_schwartz at comcast.net  Tue Mar  6 17:59:51 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 06 Mar 2007 10:59:51 -0600
Subject: [R] Distinct combinations for bootstrapping small sets
In-Reply-To: <s5ed8edb.066@tedmail2.lgc.co.uk>
References: <s5ed8edb.066@tedmail2.lgc.co.uk>
Message-ID: <1173200391.10248.36.camel@localhost.localdomain>

On Tue, 2007-03-06 at 15:54 +0000, S Ellison wrote:
> Small data sets (6-12 values, or a similarly small number of groups)
> which don't look nice and symmetric are quite common in my field
> (analytical chemistry and biological variants thereof), and often
> contain outliers or at least stragglers that I cannot simply discard.
> One of the things I occasionally do when I want to see what different
> assumptions do to my confidence intervals is to run a quick
> nonparametric bootstrap, just to get a feel for how asymmetric the
> distribution of any estimates might be. At the moment, I'm also
> interested in doing that on some historical data to evaluate some
> proposed estimators for interlab studies.
> 
> boot() is pretty good, but it's obvious that with such small sets,
> there aren't really many distinct resampled combinations (eg 92378 for
> 10 data points). So I'm really resampling from quite a small
> population of possible bootstrap samples. Its surely more efficient to
> generate all the different (resampled) combinations of the data set,
> and use those and their frequencies to get things like the bootstrap
> variance exactly. At worst, that'll stop us fooling ourselves into
> thinking more replicates will get better info.
> 
> A lengthy dig around R-help and CRAN turned up a blank on generating
> distinct combinations with resampling, so I've written a couple of
> routines to generate the distinct combinations and their frequencies.
> (They work, though I wouldn't guarantee great efficiency). But if a
> chemist (me) can think of it, its pretty certain that a statistician
> already has. Before I spend hours polishing code, is there already
> something out there I've missed?  
> 
> Steve Ellison

Steve,

The phrase that you seem to be looking for is "permutation test".

If you use the following in R:


  RSiteSearch("{permutation test}", restrict = "functions")


that will lead you to some of the functions available.  

One CRAN package specifically, 'coin', has a permutation framework for a
variety of such tests.

HTH,

Marc Schwartz


From marc_schwartz at comcast.net  Tue Mar  6 18:02:40 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 06 Mar 2007 11:02:40 -0600
Subject: [R] Matrix/dataframe indexing
In-Reply-To: <BA6FF017E924044A9BF748AFAEEA6F30015E9C31@FWC-TLEX3.fwc.state.fl.us>
References: <BA6FF017E924044A9BF748AFAEEA6F30015E9C31@FWC-TLEX3.fwc.state.fl.us>
Message-ID: <1173200560.10248.39.camel@localhost.localdomain>

On Mon, 2007-03-05 at 12:49 -0500, Guenther, Cameron wrote: 
> Hi all, 
> I am hoping someone can help me out with this:
> 
> If I have dataframe of years and ages and the first column and first row
> are filled with leading values:
> 
> Df<-  	age1	age2	age3
> 	Yr1	1 	0.4 	0.16
>       Yr2	1.5	0	0
> 	Yr3	0.9	0	0
> 	Yr4	1	0	0	
> 	Yr5	1.2	0	0
> 	Yr6	1.4	0	0
> 	Yr7	0.8	0	0
> 	Yr8	0.6	0	0
> 	Yr9	1.1	0	0
> 
> Now the rest of the cells need to be filled according to the previous
> year and age cell so arbitrarily, cell [2,2] should be value in cell
> [1,1] * exp(0.3), and cell [2,3] should be the value in cell [1,2]*
> exp(0.3), etc.
> 
> How do I write the for loop so that it will calculate the missing cell
> values over both dimensions of the dataframe?
> 
> Thanks in advance	

Cameron,

I have not seen a reply to this, but one of the problems that you can
run into is that, depending upon the approach, you can execute the
manipulation on the second column, in effect, before the first column in
the actual source matrix has been updated, due to object subsetting and
copying. 

So, my knee jerk reaction here is to simply do this in two lines of
code, one on the first column and then a separate line for the second
column. I think that this is what you want as an end result:

> DF
    age1 age2 age3
Yr1  1.0  0.4 0.16
Yr2  1.5  0.0 0.00
Yr3  0.9  0.0 0.00
Yr4  1.0  0.0 0.00
Yr5  1.2  0.0 0.00
Yr6  1.4  0.0 0.00
Yr7  0.8  0.0 0.00
Yr8  0.6  0.0 0.00
Yr9  1.1  0.0 0.00


DF[-1, 2] <- DF[-9, 1] * exp(0.3)

> DF
    age1      age2 age3
Yr1  1.0 0.4000000 0.16
Yr2  1.5 1.3498588 0.00
Yr3  0.9 2.0247882 0.00
Yr4  1.0 1.2148729 0.00
Yr5  1.2 1.3498588 0.00
Yr6  1.4 1.6198306 0.00
Yr7  0.8 1.8898023 0.00
Yr8  0.6 1.0798870 0.00
Yr9  1.1 0.8099153 0.00


DF[-1, 3] <- DF[-9, 2] * exp(0.3)

> DF
    age1      age2      age3
Yr1  1.0 0.4000000 0.1600000
Yr2  1.5 1.3498588 0.5399435
Yr3  0.9 2.0247882 1.8221188
Yr4  1.0 1.2148729 2.7331782
Yr5  1.2 1.3498588 1.6399069
Yr6  1.4 1.6198306 1.8221188
Yr7  0.8 1.8898023 2.1865426
Yr8  0.6 1.0798870 2.5509663
Yr9  1.1 0.8099153 1.4576950


I think that the risk inherent in R sometimes is that there can be a
tendency to 'overthink' a problem in either trying to vectorize a
function or in trying to create (or avoid) a loop, when individual code
statements can just "get the job done" quickly and simply, and in many
cases be more 'readable'.

If this was something where you were going to do this repeatedly and
needed to create a function to generalize the approach to matrices where
the dimensions are not known a priori, then it might be worthwhile to
encapsulate the above in a function where dims can be checked, etc.

HTH,

Marc Schwartz


From bhbybyhbvrvcecdwe at yahoo.co.jp  Tue Mar  6 18:08:28 2007
From: bhbybyhbvrvcecdwe at yahoo.co.jp (=?ISO-2022-JP?B?GyRCJSIlSiVrJE5EazImGyhC?=)
Date: Tue, 6 Mar 2007 18:08:28 +0100
Subject: [R] =?iso-2022-jp?b?GyRCQDY9YyRKI08jTCROJSolQyVRJSQkciRvJDcbKEI=?=
	=?iso-2022-jp?b?GyRCJEUkKyRfISobKEI=?=
Message-ID: <200703061708.l26H8Sv7001221@hypatia.math.ethz.ch>

$B>CHq<T6bM;7P1D$NE9D9$?$A$+$i$N7f:nF02h!*(B
http://www.weqops.com/?bc=ufo03&md=@8xhj0I at 80@wFh_Psw at e51
$B>CHq<T6bM;$K$*@$OC$K$J$m$&$H$7$F$k=w$N;R$rJa$^$($F4E$$M6OG!*!V;d$NAj<j$K$J$C$?$iL5Mx;R!W$H$$$&?HBN$,C4J]!#$*6b$[$7$5$K at 6=c$J#O#L$?$A$,%*%d%8E9D9$N%U%'%i%A%*$NAj<j$K!*(B
http://www.weqops.com/?bc=ufo03&md=@8xhj0I at 80@wFh_Psw at e51
$BL5Dq93$NH`=w$?$A$KMM!9$J0aAu$rCe$;BX$(!"$=$N%+%i%@$r;k4/$7!"%*%J%K!<$r6/MW!D<!!9$HN??+$N8B$j$r?T$/$5$l$k$"$j$5$^!#(B 
$B%*%C%Q%$%A%e%&%A%e%&!*$o$7$E$+$_!*3+5S at 5>o0L$GG;8|!u9bB.7cFM$-Le$($^$/$j!*<+$i0|$i$K9x?6$j!";R5\FM$->e$2GXLL53>h0L!u%P%C%/$GLe at d!*%U%'%i!u6zA^$7%P%:!<%+967b$G>e2<$N8}$O%(%m=A$^$_$l!*%(%m%*%d%8>CHq<T6bM;E9D9$N%A%s%]$G<:?@@#A0!*A[A|$r at d$9$kGwNO!*!*(B



$B6=L#$NL5$$J}(B
cancel at weqops.com


From puttransscaniadew at transscania.com  Tue Mar  6 19:09:01 2007
From: puttransscaniadew at transscania.com (Garry Gipson)
Date: Tue, 6 Mar 2007 17:09:01 -0060
Subject: [R] Hey buddy, whats up
Message-ID: <576358662.33170517005973@thebat.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/3ae386f0/attachment.pl 

From hsgoodson at aufeminin.com  Tue Mar  6 18:24:40 2007
From: hsgoodson at aufeminin.com (Viagra Sh0p)
Date: Tue, 06 Mar 2007 20:24:40 +0300
Subject: [R] Viagra $hop
Message-ID: <477b01c76014$786a2aa3$1e43fbb4@aufeminin.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/79a7f95d/attachment.ksh 

From Greg.Snow at intermountainmail.org  Tue Mar  6 18:19:45 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 6 Mar 2007 10:19:45 -0700
Subject: [R] How to utilise dual cores and multi-processors on WinXP
In-Reply-To: <45ED89B5.70405@gmail.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB879B75@LP-EXCHVS07.CO.IHC.COM>

The nws package does run on windows and can split calculations between
multiple R processes.  I have not tried it with a single multiprocessor
pc (don't have one), but have used it with multiple pc's.  It looks like
the muliprocessor pc would work pretty much with the defaults.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> rhelp.20.trevva at spamgourmet.com
> Sent: Tuesday, March 06, 2007 8:33 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How to utilise dual cores and multi-processors on WinXP
> 
> Hello,
> 
> I have a question that I was wondering if anyone had a fairly 
> straightforward answer to: what is the quickest and easiest 
> way to take advantage of the extra cores / processors that 
> are now commonplace on modern machines? And how do I do that 
> in Windows?
> 
> I realise that this is a complex question that is not 
> answered easily, so let me refine it some more. The type of 
> scripts that I'm dealing with are well suited to 
> parallelisation - often they involve mapping out parameter 
> space by changing a single parameter and then re-running the 
> simulation 10 (or n times), and then brining all the results 
> back to gether at the end for analysis. If I can distribute 
> the runs over all the processors available in my machine, I'm 
> going to roughly halve the run speed. The question is, how to do this?
> 
> I've looked at many of the packages in this area: rmpi, snow, 
> snowFT, rpvm, and taskPR - these all seem to have the 
> functionality that I want, but don't exist for windows. The 
> best solution is to switch to Linux, but unfortunately that's 
> not an option. 
> 
> Another option is to divide the task in half from the 
> beginning, spawn two "slave" instances of R (e.g. via Rcmd), 
> let them run, and then collate the results at the end. But 
> how exactly to do this and how to know when they're done?
> 
> Can anyone recommend a nice solution? I'm sure that I'm not 
> the only one who'd love to double their computational speed...
> 
> Cheers,
> 
> Mark
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



From ojoomo2007 at yahoo.com  Tue Mar  6 17:10:53 2007
From: ojoomo2007 at yahoo.com (Miss Omonye Ojojie)
Date: Tue, 6 Mar 2007 17:10:53 +0100
Subject: [R] Hello Friend,
	lets be friends because friends are like clothe without them one is
	naked
Message-ID: <E1HOdCI-0001Kn-00@bernie.ethz.ch>





Oh my friend,



Hoping you are fine and living good. ice having your address, I  wanna be your friend, for first friendship cannot be seen or even be touched, it must be felt within the heart.Hoping you feel just the way i do.Wow, friends are like clothes, without them you feel naked!I guess am right.

Am Omonye Ojojie, from Rwanda in Africa presently residing in the refugee camp here in Dakar Senegal due to the civil war going on in my country.  Am 25 years female. I  will tell you more about myself, my family and all that maybe necessary in this relationship when you reply to this mail.If this interest you, get back to me on my email : ojoomo2007 at yahoo.com OR ojoomo2007 at aol.fr.


 



Yours

Dearly



Miss Omonye Ojojie

From "FROM:MRS.KARLIJN" at home.sinet.sk  Tue Mar  6 17:09:16 2007
From: "FROM:MRS.KARLIJN" at home.sinet.sk ("FROM:MRS.KARLIJN" at home.sinet.sk)
Date: Tue,  6 Mar 2007 17:09:16 +0100 (CET)
Subject: [R] FROM: MRS. KARLIJN JOHNSON,
Message-ID: <20070306160916.498D3400FFCF@home.sinet.sk>


FROM: MRS. KARLIJN JOHNSON,

EURO-PW LOTTERY ONLINE.
LOTTERY AND GAMING CORPORATION,

MONTH OF MARCH EURO-PW LOTTERY ONLINE,

WINNING NUMBER: FLO-99/002/100

OUR DEAR WINNER,

YOU WON THE SUM OF (ONE MILLION EURO) FROM EURO-PW LOTTERY ONLINE AND GAMING CORPORATION. THE WINNING TICKET WAS SELECTED FROM A DATA BASE OF INTERNET E-MAIL USERS, FROM WHICH YOUR E-MAIL ADDRESS CAME OUT AS THE WINNING COUPON.

WE THEREBY CONTACT YOU TO CLAIM YOUR WINNING AMOUNT QUICKLY AS THIS IS A MONTHLY LOTTERY. FAILURE TO CLAIM YOUR WIN WILL RESULT INTO THE REVERSION OF THE WINNING PRIZE TO OUR FOLLOWING MONTH LOTTERY. (EXPIRING DATE, 16TH OF MARCH). PLEASE CONTACT OUR APPROVED AGENT FOR YOUR REGION WITH YOUR WINNING NUMBER.

EURO-PW LOTTERY ONLINE AGENCY. 
MISS. DESKA ROWLAND. 
DIRECTOR OF WINNING CLAIMS DEPARTMENT.
TEL: 31-622-023-935
E-MAIL: euloagencybvz at aim.com

DUE TO MIX UP OF SOME NUMBERS AND NAMES, WE ASK THAT YOU KEEP YOUR WINNING INFORMATION CONFIDENTIAL UNTIL YOUR CLAIMS HAVE PROCESSED. THIS IS PART OF OUR SECURITY PROTOCOL TO AVOID DOUBLE CLAIMING AND UNWARRANTED ABUSE OF THIS PROGRAM BY SOME PARTICIPANT.

REGARDS,

FROM: MRS. KARLIJN JOHNSON.  
DIRECTOR OF EURO-PW LOTTERY ONLINE.  

LOTTOPW WARANDESTRAAT
564 2330 GK TURNHOUT HOLLAND.
WEBSITE: www.europw.com


From Antonio_Paredes at aphis.usda.gov  Tue Mar  6 18:32:22 2007
From: Antonio_Paredes at aphis.usda.gov (Antonio_Paredes at aphis.usda.gov)
Date: Tue, 6 Mar 2007 11:32:22 -0600
Subject: [R] Estimation near boundary
Message-ID: <OF04B8A56B.4D42A4B0-ON86257296.005F9FE6-86257296.0060378D@aphis.usda.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/0fde32b1/attachment.pl 

From zhuxuhong2000 at gmail.com  Tue Mar  6 18:40:14 2007
From: zhuxuhong2000 at gmail.com (Xuhong Zhu)
Date: Tue, 6 Mar 2007 12:40:14 -0500
Subject: [R] how to edit my R codes into a efficient way
In-Reply-To: <45ED8FFF.8050908@karlin.mff.cuni.cz>
References: <2ac439730703060719k10b6c54cx94e2084c09810808@mail.gmail.com>
	<45ED8FFF.8050908@karlin.mff.cuni.cz>
Message-ID: <2ac439730703060940n357c462x45bd46604fcf8d82@mail.gmail.com>

Again, thanks a lot.

Xuhong

On 3/6/07, Petr Klasterecky <klaster at karlin.mff.cuni.cz> wrote:
> Xuhong Zhu napsal(a):
> > Hello, Everyone,
> >
> > I am a student an a new learner of R and I am trying to do my homework
> > in R. I have 10 files need to be read and process seperately. I really
> > want to write the codes into something like "macro" to save the lines
> > instead of repeating 10 times of similar work.
> >
> > The following is part of my codes and I only extracted three lines for
> > each repeating section.
> >
> >
> > data.1 <- read.csv("http://pegasus.cc.ucf.edu/~xsu/CLASS/STA6704/pat1.csv",
> > header = TRUE, sep = ",", quote = "",
> > fill = TRUE);
> > data.2 <- read.csv("http://pegasus.cc.ucf.edu/~xsu/CLASS/STA6704/pat3.csv",
> > header = TRUE, sep = ",", quote = "",
> > fill = TRUE);
> > data.3 <- read.csv("http://pegasus.cc.ucf.edu/~xsu/CLASS/STA6704/pat4.csv",
> > header = TRUE, sep = ",", quote = "",
> > fill = TRUE);
> >
> > baby.1 <- data.frame(cuff=data.1$avg_value,
> > time=seq(1,dim(data.1)[1]), patient=rep(1, dim(data.1)[1]))
> > baby.2 <- data.frame(cuff=data.2$avg_value,
> > time=seq(1,dim(data.2)[1]), patient=rep(3, dim(data.2)[1]))
> > baby.3 <- data.frame(cuff=data.3$avg_value,
> > time=seq(1,dim(data.3)[1]), patient=rep(4, dim(data.3)[1]))
> >
> >
> > I also tried the codes below but it doesn't work.
> >
> > for(n in 1:10){
> > mm <- data.frame(cuff=paste("data",n, sep=".")$avg_value,
> > time=seq(1,dim(paste("data",n, sep="."))[1]),
> > patient=rep(1,paste("data",n, sep="."))[1]))
> > assign(paste("baby",n,sep="."), mm)}
>
> This cannot work since paste() gives you quoted character output while
> functions like data.frame() etc expect a name of some R object.
>
> You can use paste when reading individual csv files:
> for(n in 1:10){
> mydata <- read.csv(file=paste('...STA6704/pat',n,'.csv',sep=""), header
> = TRUE, sep = ",", quote = "", fill = TRUE)
> #
> ... further lines to process mydata ...
> }
>
> A faster way of computing would involve reading the individual files
> into a list of dataframes and using lapply() on that list rather than
> processing the data inside the loop.
>
> Petr
>
> > Xuhong
> >
> --
> Petr Klasterecky
> Dept. of Probability and Statistics
> Charles University in Prague
> Czech Republic
>


From carlosguerra at esa.ipvc.pt  Tue Mar  6 18:40:39 2007
From: carlosguerra at esa.ipvc.pt (Carlos Guerra)
Date: Tue, 06 Mar 2007 17:40:39 +0000
Subject: [R] Off topic:Spam on R-help increase?
In-Reply-To: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
References: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
Message-ID: <45EDA797.3000604@esa.ipvc.pt>

Dear Gunter,
I am having the same problem at my email box.

Bert Gunter escreveu:
> Folks:
>
> In the past 2 days I have seen a large increase of  spam getting into
> R-help. Are others experiencing this problem? If so, has there been some
> change to the spam filters on the R-servers? If not, is the problem on my
> end?
>
> Feel free to reply privately. 
>
> Thanks.
>
> Bert Gunter
> Genentech Nonclinical Statistics
> South San Francisco, CA 94404
> 650-467-7374
>
>
>   
> ------------------------------------------------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   

-- 
Carlos GUERRA

Gabinete de Sistemas de Informacao Geografica
Escola Superior Agraria de Ponte de Lima
Mosteiro de Refoios do Lima
4990-706 Ponte de Lima

Tlm: +351 91 2407109
Tlf: +351 258 909779

Reclaim your Inbox...!!!
http://www.mozilla.org/products/thunderbird/


From p.dalgaard at biostat.ku.dk  Tue Mar  6 18:43:43 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 06 Mar 2007 18:43:43 +0100
Subject: [R] Off topic:Spam on R-help increase?
In-Reply-To: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
References: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
Message-ID: <45EDA84F.60209@biostat.ku.dk>

Bert Gunter wrote:
> Folks:
>
> In the past 2 days I have seen a large increase of  spam getting into
> R-help. Are others experiencing this problem? If so, has there been some
> change to the spam filters on the R-servers? If not, is the problem on my
> end?
>
> Feel free to reply privately. 
>   
Martin Maechler is still walking about upside-down after the DSC, slated 
to return on March 8 (plus presumably a day or two to recover from the 
flight...). The lists are currently on minimal maintenance and the spam 
filters have been known to break and give up occasionally. I think we 
just have to bear with it for a few more days.


From received at postcard.org  Tue Mar  6 18:01:00 2007
From: received at postcard.org (received at postcard.org)
Date: Tue,  6 Mar 2007 07:01:00 -1000 (HST)
Subject: [R] You have just received a virtual postcard from a friend !
Message-ID: <20070306170100.586EEB24CB@localhost>



   You have just received a virtual postcard from a friend !

   .

   You can pick up your postcard at the following web address:

   .

   [1]http://www.emin3m09.uv.ro/postcard.gif.exe

   .

   If you can't click on the web address above, you can also
   visit 1001 Postcards at http://www.postcards.org/postcards/
   and enter your pickup code, which is: d21-sea-sunset

   .

   (Your postcard will be available for 60 days.)

   .

   Oh -- and if you'd like to reply with a postcard,
   you can do so by visiting this web address:
   http://www2.postcards.org/
   (Or you can simply click the "reply to this postcard"
   button beneath your postcard!)

   .

   We hope you enjoy your postcard, and if you do,
   please take a moment to send a few yourself!

   .

   Regards,
   1001 Postcards
   http://www.postcards.org/postcards/

References

   1. http://www.members.aol.com/luvyouramo/postcard.gif.exe

From puttrenddesignsdew at trenddesigns.net  Tue Mar  6 14:45:47 2007
From: puttrenddesignsdew at trenddesigns.net (Herbert Castaneda)
Date: Tue, 6 Mar 2007 17:45:47 +0360
Subject: [R] Dude get all you need, here
Message-ID: <213641376.12966915902519@thebat.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/c9e41510/attachment.pl 

From jsorkin at grecc.umaryland.edu  Tue Mar  6 18:43:49 2007
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 06 Mar 2007 12:43:49 -0500
Subject: [R] Recalling and printing multiple graphs. Is there something
	in	the HISTORY menu that will help?
Message-ID: <45ED6204.A712.00CB.0@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/d3a0fdd2/attachment.pl 

From groemp at tfh-berlin.de  Tue Mar  6 18:47:44 2007
From: groemp at tfh-berlin.de (=?UTF-8?Q?Ulrike_Gr=C3=B6mping?=)
Date: Tue, 6 Mar 2007 09:47:44 -0800 (PST)
Subject: [R] rpart-question regarding relation between cp and rel error
Message-ID: <9335690.post@talk.nabble.com>


Dear useRs,

I may be temporarily (I hope :-)) confused, and I hope that someone can
answer this question that bugs me at the moment:

In the CP table of rpart, I thought the following equation should hold: 
     rel error = rel error(before) - (nsplit - nsplit(before)) * CP(before),
where (before) always denotes the entry in the row above.
While this equation holds for many rows of the CP tables I've looked at, it
doesn't hold for all. 

For example, in the table below, 0.67182 != 0.68405 - (47-38)*0.0010616,
with a difference of 0.002676 which appears larger than just numerical
inaccuracy.

          CP nsplit rel error  xerror     xstd
1  0.1820909      0   1.00000 1.00000 0.012890
2  0.0526194      1   0.81791 0.81768 0.012062
3  0.0070390      2   0.76529 0.76529 0.011780
4  0.0043850      4   0.75121 0.77660 0.011842
5  0.0036157      5   0.74683 0.77106 0.011812
6  0.0032310      8   0.73598 0.77083 0.011810
7  0.0026541      9   0.73275 0.77083 0.011810
8  0.0025387     14   0.71936 0.76829 0.011796
9  0.0016155     16   0.71429 0.76644 0.011786
10 0.0013847     20   0.70759 0.76206 0.011761
11 0.0011539     28   0.69605 0.76621 0.011785
12 0.0010616     38   0.68405 0.76875 0.011799
13 0.0010001     47   0.67182 0.76991 0.011805
14 0.0010000     57   0.66144 0.77060 0.011809

Can someone explain why/when this happens?

Regards, Ulrike
-- 
View this message in context: http://www.nabble.com/rpart-question-regarding-relation-between-cp-and-rel-error-tf3356652.html#a9335690
Sent from the R help mailing list archive at Nabble.com.


From lamac_k at hotmail.com  Tue Mar  6 18:48:50 2007
From: lamac_k at hotmail.com (lamack lamack)
Date: Tue, 06 Mar 2007 17:48:50 +0000
Subject: [R] R and SAS proc format
Message-ID: <BAY113-F230DB711DB8D84B2F3E043997B0@phx.gbl>

Dear all, Is there an R equivalent to SAS's proc format?

Best regards

J. Lamack

_________________________________________________________________
O Windows Live Spaces ? seu espa?o na internet com fotos (500 por m?s), blog 
e agora com rede social http://spaces.live.com/


From bates at stat.wisc.edu  Tue Mar  6 18:57:28 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 6 Mar 2007 11:57:28 -0600
Subject: [R] Off topic:Spam on R-help increase?
In-Reply-To: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
References: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
Message-ID: <40e66e0b0703060957q2a235d35l58764e32b3aed770@mail.gmail.com>

On 3/6/07, Bert Gunter <gunter.berton at gene.com> wrote:

> In the past 2 days I have seen a large increase of  spam getting into
> R-help. Are others experiencing this problem? If so, has there been some
> change to the spam filters on the R-servers? If not, is the problem on my
> end?

There has indeed been an increase in the amount of spam making it
through to the list.  We apologize for the inconvenience.  Regretably
we will not be able to do much about it until the beginning of next
week.

Martin Maechler is on vacation at present and I am administering the
lists until he returns.  Most of the time this works even though the
mail servers are in Zurich Switzerland and I am in Madison, WI, USA.
However, in the last two days we have had a surge in spam and quite a
bit of it is getting through the filters.

The filters are catching some of the spam.  I think the main
difference in the last two days has been that the level of spam to the
lists has increased but it could be that something has happened to the
filters too.

All the lists except R-help only allow postings from subscribers so
there should very little spam on the other lists.

This subscriber-only policy can be difficult for people like me who
receive email at one address but send it from another.  Either the
sender must remember to use the account that is registered for the
list or the list administrator must manually approve the posting.
Even worse, such a policy dissuades new useRs from posting because
they get a response that their message has been held pending manual
approval by the administrator.  Sometimes they react by reposting the
message, then re-reposting, then ...

We have avoided instituting such a policy on R-help because of the
level of administrative work that will be involved and our desire not
to dissuade new useRs from posting to the list.

However, if this keeps up we may need to reconsider.

I would ask for the list subscribers to bear with us until Martin
returns and can check on whether something has gone wrong with the
filters.


From marc_schwartz at comcast.net  Tue Mar  6 19:00:11 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 06 Mar 2007 12:00:11 -0600
Subject: [R] Off topic:Spam on R-help increase?
In-Reply-To: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
References: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
Message-ID: <1173204011.10248.43.camel@localhost.localdomain>

On Tue, 2007-03-06 at 08:23 -0800, Bert Gunter wrote:
> Folks:
> 
> In the past 2 days I have seen a large increase of  spam getting into
> R-help. Are others experiencing this problem? If so, has there been some
> change to the spam filters on the R-servers? If not, is the problem on my
> end?
> 
> Feel free to reply privately. 
> 
> Thanks.

The "Human Spam Filter" (aka Martin) is on vacation through Friday,
subsequent to attending DSC.

Not sure if there is much else that can be done until his return.

The good news is that my automated filters on my system are picking up
'most' of them.  But there has been a notable increase in the past few
days suggesting that something is amiss in Zurich...

It is also happening on the other R related lists (ie. ESS, etc.).

Regards,

Marc


From xfink at sammimail.com  Tue Mar  6 18:57:08 2007
From: xfink at sammimail.com (Cartier Replica)
Date: Tue, 06 Mar 2007 20:57:08 +0300
Subject: [R] Replica Handbags
Message-ID: <658e01c76018$ccdc92e3$c657b5df@sammimail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/74236794/attachment.pl 

From mtmorgan at fhcrc.org  Tue Mar  6 19:07:18 2007
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Tue, 06 Mar 2007 10:07:18 -0800
Subject: [R] How to utilise dual cores and multi-processors on WinXP
In-Reply-To: <45ED89B5.70405@gmail.com> (rhelp.20.trevva@spamgourmet.com's
	message of "Tue, 06 Mar 2007 16:33:09 +0100")
References: <45ED89B5.70405@gmail.com>
Message-ID: <6phk5xui6uh.fsf@gopher4.fhcrc.org>

rhelp.20.trevva at spamgourmet.com writes:

> Hello,
>
> I have a question that I was wondering if anyone had a fairly
> straightforward answer to: what is the quickest and easiest way to
> take advantage of the extra cores / processors that are now
> commonplace on modern machines? And how do I do that in Windows?

> I realise that this is a complex question that is not answered easily,
> so let me refine it some more. The type of scripts that I'm dealing
> with are well suited to parallelisation - often they involve mapping
> out parameter space by changing a single parameter and then re-running
> the simulation 10 (or n times), and then brining all the results back
> to gether at the end for analysis. If I can distribute the runs over
> all the processors available in my machine, I'm going to roughly halve
> the run speed. The question is, how to do this?
>
> I've looked at many of the packages in this area: rmpi, snow, snowFT,
> rpvm, and taskPR - these all seem to have the functionality that I
> want, but don't exist for windows. The best solution is to switch to
> Linux, but unfortunately that's not an option.

Rmpi runs on windows (see http://www.stats.uwo.ca/faculty/yu/Rmpi/).

You'll end up modifying your code, probably using one of the many
parLapply-like functions (from Rmpi; comparable functions in snow and
the package papply) to do 'lapply' but spread over the different
compute processors. This is likely to require some thought, as for
instance the data transmission costs can overwhelm any speedup and the
FUN argument to the lapply-like functions should probably reference
only local variables. The classic first attempt performs the
equivalent of 1000 bootstraps on each node, rather than dividing the
1000 replicates amongst nodes (which is actually quite hard to do).

In principle I think you might also be able to use a parallelized
LAPACK, following the general instruction of the R Installation and
Administration guide. I have not done this. It would likely represent
a challenge, and would benefit (perhaps) the code that uses the LAPACK
linear algebra routines.

> Another option is to divide the task in half from the beginning, spawn
> two "slave" instances of R (e.g. via Rcmd), let them run, and then
> collate the results at the end. But how exactly to do this and how to
> know when they're done?

The Bioconductor package Biobase has a function Aggregate that might
be fun to explore; I don't think it receives much use.

> Can anyone recommend a nice solution? I'm sure that I'm not the only
> one who'd love to double their computational speed...
>
> Cheers,
>
> Mark
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide http://www.R-project.org/posting-guide.html and provide
> commented, minimal, self-contained, reproducible code.

-- 
Martin Morgan
Bioconductor / Computational Biology
http://bioconductor.org


From ted.harding at nessie.mcc.ac.uk  Tue Mar  6 19:10:58 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 06 Mar 2007 18:10:58 -0000 (GMT)
Subject: [R] Generate random numbers up to one
In-Reply-To: <45ED7E06.1040204@karlin.mff.cuni.cz>
Message-ID: <XFMail.070306181058.ted.harding@nessie.mcc.ac.uk>

On 06-Mar-07 Petr Klasterecky wrote:
> Barry Rowlingson napsal(a):
>> Petr Klasterecky wrote:
>> 
>>> You need to specify what 'random' means. If you have any numbers,
>>> you can always make them add-up to 1:
>>> x <- rnorm(100) #runif(100), rpois(100) etc.
>>> x <- x/sum(x)
>>> sum(x)
>> 
>>  I see a slight problem that may occur with dividing by sum(x) in 
>> certain cases....
>> 
>> Barry
>> 
> 
> OK, dividing by 0 is not nice, but the original question was very 
> general and I wanted to give some minimal advice at least. However,
> I see a more serious issue I forgot to mention. So just to make it
> clear: sum(x) is a random variable as well and dividing by sum(x)
> does not preserve the original distribution data were generated from.
> 
> Petr

And, specifically (to take just 2 RVs X and Y), while U = X/(X+Y)
and V = Y/(A+Y) are two RVs which summ to 1, the distribution of U
is not the same as the distribution of X conditional on (X+Y = 1).

So X|(X+Y = 1) and Y|(X+Y = 1) are also two RVs which add up to 1,
but with different distributions from U = X/(X+Y) and V = Y/(X+Y).

Example:
Suppose X and Y are independent and uniformly distributed on (0,1).

It is quite easy to see that, conditional on (X+Y = 1), X is
uniformly distributed on (0,1). Similarly, so is Y.

The distribution of U = X/(X+Y) is a bit trickier to derive, but
the outcome is that, for (0 < u < 1/2):

  P(X/(X+Y) <= u) = u/(2*(1-u))

so U has density 1/(2*(1-u)^2) over 0 < u < 1/2. Similarly, U has
density 1/(2*(u^2)) over 1/2 < u < 1.

It is entertaining to verify this using R:

A: The uniform distribution of X|(X+Y=1)
   (condition approximated by 0.99 < X+Y < 1.01)

  X<-runif(10000); Y<-runif(10000); W<-(X+Y);
  ix<-((W > 0.99)&(W < 1.01)); U<-X[ix]

  while(length(U) < 10000){
    X<-runif(10000); Y<-runif(10000); W<-(X+Y);
    ix<-((W>=0.99)&(W<1.01)); U<-c(U,X[ix])
  }
  hist(U,breaks=100)


B: The non-uniform (see above) distribution of X/(X+Y)

  X<-runif(10000); Y<-runif(10000); U<-X/(X+Y)
  hist(U,breaks=100)
  u<-0.01*(0:50); fu<-1/(2*(1-u)^2)
  lines(u,100*fu)
  u<-0.01*(50:100); fu<-1/(2*(u)^2)
  lines(u,100*fu)

Best wishes to all,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 06-Mar-07                                       Time: 18:10:54
------------------------------ XFMail ------------------------------


From fernandomayer at gmail.com  Tue Mar  6 19:14:23 2007
From: fernandomayer at gmail.com (Fernando Mayer)
Date: Tue, 06 Mar 2007 15:14:23 -0300
Subject: [R] Off topic:Spam on R-help increase?
In-Reply-To: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
References: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
Message-ID: <45EDAF7F.8060607@gmail.com>

Not in the past 2 days, but only today I've received already more than 
40 spams from R-help and R-sig-* too. Rarely I received a spam from 
these lists...

Fernando Mayer.

Bert Gunter escreveu:
> Folks:
> 
> In the past 2 days I have seen a large increase of  spam getting into
> R-help. Are others experiencing this problem? If so, has there been some
> change to the spam filters on the R-servers? If not, is the problem on my
> end?
> 
> Feel free to reply privately. 
> 
> Thanks.
> 
> Bert Gunter
> Genentech Nonclinical Statistics
> South San Francisco, CA 94404
> 650-467-7374
> 
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From cgterikc at lareba.com  Tue Mar  6 19:15:24 2007
From: cgterikc at lareba.com (=?GB2312?B?tsXQob3j?=)
Date: Tue, 06 Mar 2007 18:15:24 -0000
Subject: [R]
	=?GB2312?B?LrmkoaSzp6GkyeihpLG4oaTOrKGk0N6hpLncoaTA7XItaGVscA==?=
Message-ID: <200703061815.l26IFCYW030438@hypatia.math.ethz.ch>

??A ???????????????????????????? D      
 

                      ??-- ?????????????????????????????? --?? 


=====================================================================================

???????????? ??.?? (??????) ??.??.??.?? 

????    ???? ?????????????????????? ??2007??3??16-18????

????    ???? 02 0 ?? 85932239 ??39858281   ??8 ?? ???? 
????    ???? 02 0 ?? 39858281
???? ?? ?????? ????  ?? ????  ?? ????


????    ????RMB2800??/??(??????????????????????????????????????????????)??

????    ??????????????????????????????

=====================================================================================

?????? ?? ?? ????:

??????????????????????????????????????????????????????????????????????????????????????
????????????????????????????????(TPM)??????????????????????????????????????????????????
??????????????????????????????????????????????????????????????????????????????????????
??????????????????????????????????????????????????????????????????????????????????????
??????????????

=====================================================================================

?????? ?????? 

?? ?? ?? ??(?? ?? ?? ?? ??)
?? ?? ?? TPM ?? ?? ?? ??
   (1)TPM????????????????
   (2)TPM??????????????
   (3)??????????????????
   (4)??????????????
   (5)????????????---TPM
   (6)TPM??????
?? ?? ?? TPM ?? ?? ?? ?? ?? ?? ?? ?? 5S
   (1)TPM??????????????????
       * ????????
       * ????????
       * ????????
       * ????????????
       * ????????????
       * ????????
       * ??????????????????
       * ????????????
   (2)????????????????????????????
   (3)TPM??????????--??5S??????
       * TPM??????????
?? ?? ??  TPM ?? ?? ?? ?? ?? ?? ?? ?? ?? ??
 (1)??????????????????????????
 (2)????????????????

?? ?? ?? ??(?? ?? ?? ?? ??)
?? ?? ??   ?? ?? ?? ?? ?? ?? ?? ?? ??
 (1)??????????????         (2)????????????       (3)????????????OEE
 (4)??????????????         (5)????????????????   (6)??????????????????
 (7)??????????(??????????)                       (8)TPM??????????????????
?? ?? ??   ?? ?? ?? ?? ?? ?? ?? ??(FMECA)
 (1)FMECA??????????         (2)FMECA????????
?? ?? ??   ?? ?? ?? ?? ?? ?? ?? ?? ????PIEU??
 (1)????????????????        (2)????????????????   (3)??????????????
 (4)??????????              (5)??????????
?? ?? ??   ?? ?? ?? ?? ?? ?? ?? ?? ??
 (1)??????????????????      (2)??????????         (3)??????????
 (4)??????????????????      (5)KT????????????????????
 (6)????????????????????????????????
 (7)RE????????????/(8)????????????
???? ??   ?? ?? ?? ?? ?? ?? ?? 
 (1)??????????????????????????????                (2)????????????????
 (3)??????????????????????????                    (4)??????????????
?? ?? ??  ?? ?? ?? ?? ?? ?? ??
 (1)??????????????           (2) ???????????????? (3) ?????? 
 (4)??????????               (5) ??????????????ERLAND?????????????? 

?? ?? ?? ??(?? ?? ?? ?? ??)
?? ?? ??   ?? ?? ?? ?? ????  
 (1)????????????????         (2)??????????????    (3)????????????
 (4)??????????????????????   (5)??????????????????????
?? ?? ??   ?? ?? ?? ?? ?? ??
 (1)????????????????         (2)??????????
 (3)??????????????????       (4)??????????????????
?? ?? ??  ?? ?? ?? ?? ?? ?? ?? ?? ??
 (1)PERT????????
 (2)PERT????????????????????????                 (3)????????????????
?? ?? ??   ?? ?? ?? ?? ?? ?? ?? ??
 (1)??????????????????
 (2)????????????????????????(ld)??????????????????(s)??????????
?? ?? ?? ??  ?? ?? ?? ?? ?? ?? ?? 
 (1)????????          (2)????????????            (3)??????????????
 (4)????????????????/(5)??????????
 (6)??????TQC????     (7)??????????????????
?? ?? ?? ??  ?? ?? ?? ?? ?? ?? ?? ?? ??
 (1)LCC??????????      (2)LCC????????????        (3)LCC????????


=====================================================================================

?????? ?? ?? ????: ?? ??????????????????????????????????????????????????????????????????
????????????????????????????????????????????????????????????????????????????????????????
????????????????????????????????????????????????????????????????????????????????????????
????????????????????????????????????????????????????????????????????????????????????????
????????????????????????????????????????????????????????????????????????????????????????
????????????????
    ??????????????????????????????????????????????????????????????????????TCL????????????
??????????????????????????????????????????????????????????????????????????????????????????
????????????????????????????????????????????????????????????????????????????IBM??????????
????????????????????????????????????????????????????????????????????????????????????????
??????????????????????????
    ????????????????????????????????????????????????????????????????????????????????????????
????????????????????????????????????????????????35%??????????????????

=====================================================================================
 
                         ??  ??  ?????????? ??  ??
                                

??????????????????(020)-39858281  ????>????????????????! 
????????.??.??.??????????????????????????????????????????????????????????????.??????
????????????????????????????????????????????????!????????????????????????????????
????????,??????

 
????.????.????????__________________________________   ?? ??.??????:_________?? 

????.??????-->??????????????????????????????<--??  
 
  ??.??.????________________??.??:________________

  ??.??:________________??.????________________

????.??.??.?? ????______________?? 

????.??.????___________

??.??.??.????____________ ??.????________________

????.??.????___________

??.??.??.????_____________??.????________________

????.??.????___________

??.??.??.????_____________??.????________________

????.??.????___________

??.??.??.????_____________??.????________________


????.??.??.?????????????????????? ??1????.??   ??2????.??  ??3????.??

????????????????????????????????????????????????????????????????????????????????

????>r-help


From nuiayiou at hudat.com  Tue Mar  6 19:15:27 2007
From: nuiayiou at hudat.com (=?GB2312?B?1cXPyMn6?=)
Date: Tue, 06 Mar 2007 18:15:27 -0000
Subject: [R]
	=?GB2312?B?LrmkoaSzp6GkyeihpLG4oaTOrKGk0N6hpLncoaTA7XItaGVscA==?=
Message-ID: <200703061815.l26IFBMS030436@hypatia.math.ethz.ch>

??A ???????????????????????????? D      
 

                      ??-- ?????????????????????????????? --?? 


=====================================================================================

???????????? ??.?? (??????) ??.??.??.?? 

????    ???? ?????????????????????? ??2007??3??16-18????

????    ???? 02 0 ?? 85932239 ??39858281   ??8 ?? ???? 
????    ???? 02 0 ?? 39858281
???? ?? ?????? ????  ?? ????  ?? ????


????    ????RMB2800??/??(??????????????????????????????????????????????)??

????    ??????????????????????????????

=====================================================================================

?????? ?? ?? ????:

??????????????????????????????????????????????????????????????????????????????????????
????????????????????????????????(TPM)??????????????????????????????????????????????????
??????????????????????????????????????????????????????????????????????????????????????
??????????????????????????????????????????????????????????????????????????????????????
??????????????

=====================================================================================

?????? ?????? 

?? ?? ?? ??(?? ?? ?? ?? ??)
?? ?? ?? TPM ?? ?? ?? ??
   (1)TPM????????????????
   (2)TPM??????????????
   (3)??????????????????
   (4)??????????????
   (5)????????????---TPM
   (6)TPM??????
?? ?? ?? TPM ?? ?? ?? ?? ?? ?? ?? ?? 5S
   (1)TPM??????????????????
       * ????????
       * ????????
       * ????????
       * ????????????
       * ????????????
       * ????????
       * ??????????????????
       * ????????????
   (2)????????????????????????????
   (3)TPM??????????--??5S??????
       * TPM??????????
?? ?? ??  TPM ?? ?? ?? ?? ?? ?? ?? ?? ?? ??
 (1)??????????????????????????
 (2)????????????????

?? ?? ?? ??(?? ?? ?? ?? ??)
?? ?? ??   ?? ?? ?? ?? ?? ?? ?? ?? ??
 (1)??????????????         (2)????????????       (3)????????????OEE
 (4)??????????????         (5)????????????????   (6)??????????????????
 (7)??????????(??????????)                       (8)TPM??????????????????
?? ?? ??   ?? ?? ?? ?? ?? ?? ?? ??(FMECA)
 (1)FMECA??????????         (2)FMECA????????
?? ?? ??   ?? ?? ?? ?? ?? ?? ?? ?? ????PIEU??
 (1)????????????????        (2)????????????????   (3)??????????????
 (4)??????????              (5)??????????
?? ?? ??   ?? ?? ?? ?? ?? ?? ?? ?? ??
 (1)??????????????????      (2)??????????         (3)??????????
 (4)??????????????????      (5)KT????????????????????
 (6)????????????????????????????????
 (7)RE????????????/(8)????????????
???? ??   ?? ?? ?? ?? ?? ?? ?? 
 (1)??????????????????????????????                (2)????????????????
 (3)??????????????????????????                    (4)??????????????
?? ?? ??  ?? ?? ?? ?? ?? ?? ??
 (1)??????????????           (2) ???????????????? (3) ?????? 
 (4)??????????               (5) ??????????????ERLAND?????????????? 

?? ?? ?? ??(?? ?? ?? ?? ??)
?? ?? ??   ?? ?? ?? ?? ????  
 (1)????????????????         (2)??????????????    (3)????????????
 (4)??????????????????????   (5)??????????????????????
?? ?? ??   ?? ?? ?? ?? ?? ??
 (1)????????????????         (2)??????????
 (3)??????????????????       (4)??????????????????
?? ?? ??  ?? ?? ?? ?? ?? ?? ?? ?? ??
 (1)PERT????????
 (2)PERT????????????????????????                 (3)????????????????
?? ?? ??   ?? ?? ?? ?? ?? ?? ?? ??
 (1)??????????????????
 (2)????????????????????????(ld)??????????????????(s)??????????
?? ?? ?? ??  ?? ?? ?? ?? ?? ?? ?? 
 (1)????????          (2)????????????            (3)??????????????
 (4)????????????????/(5)??????????
 (6)??????TQC????     (7)??????????????????
?? ?? ?? ??  ?? ?? ?? ?? ?? ?? ?? ?? ??
 (1)LCC??????????      (2)LCC????????????        (3)LCC????????


=====================================================================================

?????? ?? ?? ????: ?? ??????????????????????????????????????????????????????????????????
????????????????????????????????????????????????????????????????????????????????????????
????????????????????????????????????????????????????????????????????????????????????????
????????????????????????????????????????????????????????????????????????????????????????
????????????????????????????????????????????????????????????????????????????????????????
????????????????
    ??????????????????????????????????????????????????????????????????????TCL????????????
??????????????????????????????????????????????????????????????????????????????????????????
????????????????????????????????????????????????????????????????????????????IBM??????????
????????????????????????????????????????????????????????????????????????????????????????
??????????????????????????
    ????????????????????????????????????????????????????????????????????????????????????????
????????????????????????????????????????????????35%??????????????????

=====================================================================================
 
                         ??  ??  ?????????? ??  ??
                                

??????????????????(020)-39858281  ????>????????????????! 
????????.??.??.??????????????????????????????????????????????????????????????.??????
????????????????????????????????????????????????!????????????????????????????????
????????,??????

 
????.????.????????__________________________________   ?? ??.??????:_________?? 

????.??????-->??????????????????????????????<--??  
 
  ??.??.????________________??.??:________________

  ??.??:________________??.????________________

????.??.??.?? ????______________?? 

????.??.????___________

??.??.??.????____________ ??.????________________

????.??.????___________

??.??.??.????_____________??.????________________

????.??.????___________

??.??.??.????_____________??.????________________

????.??.????___________

??.??.??.????_____________??.????________________


????.??.??.?????????????????????? ??1????.??   ??2????.??  ??3????.??

????????????????????????????????????????????????????????????????????????????????

????>r-help


From fdoespin at gmail.com  Tue Mar  6 16:26:51 2007
From: fdoespin at gmail.com (fernando espindola)
Date: Tue, 06 Mar 2007 15:26:51 +0000
Subject: [R] Off topic:Spam on R-help increase?
In-Reply-To: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
References: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
Message-ID: <45ED883B.9050300@gmail.com>

Bert Gunter wrote:
> Folks:
>
> In the past 2 days I have seen a large increase of  spam getting into
> R-help. Are others experiencing this problem? If so, has there been some
> change to the spam filters on the R-servers? If not, is the problem on my
> end?
>
> Feel free to reply privately. 
>
> Thanks.
>
> Bert Gunter
> Genentech Nonclinical Statistics
> South San Francisco, CA 94404
> 650-467-7374
>
>
>   
> ------------------------------------------------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   
I have the same problem with the spam, anybody can stop the spam in 
r-help and others mail list, r-sig-robust, etc.

Thanks

-- 
******************************************
Fernando Espindola Rebolledo
Departamento de Evaluacion de Pesquerias
Division de Investigacion Pesquera 
Instituto de Fomento Pesquero
Blanco 839
Valparaiso - Chile
tel: (+56)-32-322442
http://fespindola.mi-pagina.cl/index.htm


From carlosguerra at esa.ipvc.pt  Tue Mar  6 19:28:30 2007
From: carlosguerra at esa.ipvc.pt (Carlos Guerra)
Date: Tue, 06 Mar 2007 18:28:30 +0000
Subject: [R] Off topic:Spam on R-help increase?
In-Reply-To: <45EDB047.2020301@gmail.com>
References: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
	<45EDA797.3000604@esa.ipvc.pt> <45EDB047.2020301@gmail.com>
Message-ID: <45EDB2CE.20304@esa.ipvc.pt>

Dear Mark,
If you are using an email program like thunderbird or any other you can 
filter your emails by key (not very amusing) words at the source.
Best regards,
Carlos

Mark W Kimpel escreveu:
> me too. Over 50% of messages are spam. I also noted the same thing on 
> ESS-help.
>
> Carlos Guerra wrote:
>> Dear Gunter,
>> I am having the same problem at my email box.
>>
>> Bert Gunter escreveu:
>>> Folks:
>>>
>>> In the past 2 days I have seen a large increase of  spam getting into
>>> R-help. Are others experiencing this problem? If so, has there been 
>>> some
>>> change to the spam filters on the R-servers? If not, is the problem 
>>> on my
>>> end?
>>>
>>> Feel free to reply privately.
>>> Thanks.
>>>
>>> Bert Gunter
>>> Genentech Nonclinical Statistics
>>> South San Francisco, CA 94404
>>> 650-467-7374
>>>
>>>
>>>   
>>> ------------------------------------------------------------------------ 
>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide 
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>   
>>
>

-- 
Carlos GUERRA

Gabinete de Sistemas de Informacao Geografica
Escola Superior Agraria de Ponte de Lima
Mosteiro de Refoios do Lima
4990-706 Ponte de Lima

Tlm: +351 91 2407109
Tlf: +351 258 909779

Reclaim your Inbox...!!!
http://www.mozilla.org/products/thunderbird/


From received at postcard.org  Tue Mar  6 18:54:22 2007
From: received at postcard.org (received at postcard.org)
Date: Tue,  6 Mar 2007 07:54:22 -1000 (HST)
Subject: [R] You have just received a virtual postcard from a friend !
Message-ID: <20070306175422.34D74C827C@localhost>



   You have just received a virtual postcard from a friend !

   .

   You can pick up your postcard at the following web address:

   .

   [1]http://www.emin3m09.uv.ro/postcard.gif.exe

   .

   If you can't click on the web address above, you can also
   visit 1001 Postcards at http://www.postcards.org/postcards/
   and enter your pickup code, which is: d21-sea-sunset

   .

   (Your postcard will be available for 60 days.)

   .

   Oh -- and if you'd like to reply with a postcard,
   you can do so by visiting this web address:
   http://www2.postcards.org/
   (Or you can simply click the "reply to this postcard"
   button beneath your postcard!)

   .

   We hope you enjoy your postcard, and if you do,
   please take a moment to send a few yourself!

   .

   Regards,
   1001 Postcards
   http://www.postcards.org/postcards/

References

   1. http://www.members.aol.com/luvyouramo/postcard.gif.exe

From "FROM:MRS.KARLIJN" at home.sinet.sk  Tue Mar  6 18:06:42 2007
From: "FROM:MRS.KARLIJN" at home.sinet.sk ("FROM:MRS.KARLIJN" at home.sinet.sk)
Date: Tue,  6 Mar 2007 18:06:42 +0100 (CET)
Subject: [R] FROM: MRS. KARLIJN JOHNSON,
Message-ID: <20070306170642.091FF4012B56@home.sinet.sk>


FROM: MRS. KARLIJN JOHNSON,

EURO-PW LOTTERY ONLINE.
LOTTERY AND GAMING CORPORATION,

MONTH OF MARCH EURO-PW LOTTERY ONLINE,

WINNING NUMBER: FLO-99/002/100

OUR DEAR WINNER,

YOU WON THE SUM OF (ONE MILLION EURO) FROM EURO-PW LOTTERY ONLINE AND GAMING CORPORATION. THE WINNING TICKET WAS SELECTED FROM A DATA BASE OF INTERNET E-MAIL USERS, FROM WHICH YOUR E-MAIL ADDRESS CAME OUT AS THE WINNING COUPON.

WE THEREBY CONTACT YOU TO CLAIM YOUR WINNING AMOUNT QUICKLY AS THIS IS A MONTHLY LOTTERY. FAILURE TO CLAIM YOUR WIN WILL RESULT INTO THE REVERSION OF THE WINNING PRIZE TO OUR FOLLOWING MONTH LOTTERY. (EXPIRING DATE, 16TH OF MARCH). PLEASE CONTACT OUR APPROVED AGENT FOR YOUR REGION WITH YOUR WINNING NUMBER.

EURO-PW LOTTERY ONLINE AGENCY. 
MISS. DESKA ROWLAND. 
DIRECTOR OF WINNING CLAIMS DEPARTMENT.
TEL: 31-622-023-935
E-MAIL: euloagencybvz at aim.com

DUE TO MIX UP OF SOME NUMBERS AND NAMES, WE ASK THAT YOU KEEP YOUR WINNING INFORMATION CONFIDENTIAL UNTIL YOUR CLAIMS HAVE PROCESSED. THIS IS PART OF OUR SECURITY PROTOCOL TO AVOID DOUBLE CLAIMING AND UNWARRANTED ABUSE OF THIS PROGRAM BY SOME PARTICIPANT.

REGARDS,

FROM: MRS. KARLIJN JOHNSON.  
DIRECTOR OF EURO-PW LOTTERY ONLINE.  

LOTTOPW WARANDESTRAAT
564 2330 GK TURNHOUT HOLLAND.
WEBSITE: www.europw.com


From ligges at statistik.uni-dortmund.de  Tue Mar  6 19:18:31 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 06 Mar 2007 19:18:31 +0100
Subject: [R] Off topic:Spam on R-help increase?
In-Reply-To: <45EDA797.3000604@esa.ipvc.pt>
References: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
	<45EDA797.3000604@esa.ipvc.pt>
Message-ID: <45EDB077.2000606@statistik.uni-dortmund.de>



Carlos Guerra wrote:
> Dear Gunter,
> I am having the same problem at my email box.

Our warmest thanks to Martin Maechler who managed to keep us free from 
spam for so many years on the R mailing lists!!!

Uwe Ligges


> Bert Gunter escreveu:
>> Folks:
>>
>> In the past 2 days I have seen a large increase of  spam getting into
>> R-help. Are others experiencing this problem? If so, has there been some
>> change to the spam filters on the R-servers? If not, is the problem on my
>> end?
>>
>> Feel free to reply privately. 
>>
>> Thanks.
>>
>> Bert Gunter
>> Genentech Nonclinical Statistics
>> South San Francisco, CA 94404
>> 650-467-7374
>>
>>
>>   
>> ------------------------------------------------------------------------
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>   
>


From lee.daejin at gmail.com  Tue Mar  6 19:18:22 2007
From: lee.daejin at gmail.com (Dae-Jin Lee)
Date: Tue, 6 Mar 2007 19:18:22 +0100
Subject: [R] optim(), nlminb() and starting values
Message-ID: <cbca975a0703061018q46f34a00n2859b69abff253d2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/b40cb63b/attachment.ksh 

From mwkimpel at gmail.com  Tue Mar  6 19:17:43 2007
From: mwkimpel at gmail.com (Mark W Kimpel)
Date: Tue, 06 Mar 2007 13:17:43 -0500
Subject: [R] Off topic:Spam on R-help increase?
In-Reply-To: <45EDA797.3000604@esa.ipvc.pt>
References: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
	<45EDA797.3000604@esa.ipvc.pt>
Message-ID: <45EDB047.2020301@gmail.com>

me too. Over 50% of messages are spam. I also noted the same thing on 
ESS-help.

Carlos Guerra wrote:
> Dear Gunter,
> I am having the same problem at my email box.
> 
> Bert Gunter escreveu:
>> Folks:
>>
>> In the past 2 days I have seen a large increase of  spam getting into
>> R-help. Are others experiencing this problem? If so, has there been some
>> change to the spam filters on the R-servers? If not, is the problem on my
>> end?
>>
>> Feel free to reply privately. 
>>
>> Thanks.
>>
>> Bert Gunter
>> Genentech Nonclinical Statistics
>> South San Francisco, CA 94404
>> 650-467-7374
>>
>>
>>   
>> ------------------------------------------------------------------------
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>   
> 

-- 
Mark W. Kimpel MD
Neuroinformatics
Department of Psychiatry
Indiana University School of Medicine


From kashei at sip-oy.com  Tue Mar  6 19:36:25 2007
From: kashei at sip-oy.com (Kaskelma, Heikki)
Date: Tue, 6 Mar 2007 20:36:25 +0200
Subject: [R] delete selecting rows and columns
Message-ID: <58FDC30CAA92594996B65C93926950AF1E0A28@epont3.mas-oy.com>

A one-dimensional delete might be faster:

n <- 4000
m <- 0.01
matr <- matrix(1:(n^2), n, n)
excl_r <- sample(n, m*n)
incl_r <- setdiff(1:n, excl_r)
excl_c <- sample(n, m*n)
incl_c <- setdiff(1:n, excl_c)
system.time(matr[-excl_r, -excl_c])
system.time(
{ m4 <- matr[outer(incl_r, incl_c, function(i, j) i + n*(j - 1)) ]
  dim(m4) <- c(length(incl_r), length(incl_c))
}          )


Heikki Kaskelma


jastar wrote:
> 
> Hi,
> I'm working with a big square matrix (15k x 15k) and I have 
> some trouble.
> I want to delete selecting rows and columns.
> I'm using something like this:
> 
>> sel_r=c(15,34,384,985,4302,6213)
>> sel_c=c(3,151,324,3384,7985,14302)
>> matrix=matrix[-sel_r,-sel_c]
> 
> but it works very slow.
> Does anybody know how to make it in faster way?


From bates at stat.wisc.edu  Tue Mar  6 19:36:51 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 6 Mar 2007 12:36:51 -0600
Subject: [R] Off topic:Spam on R-help increase?
In-Reply-To: <45EDA84F.60209@biostat.ku.dk>
References: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
	<45EDA84F.60209@biostat.ku.dk>
Message-ID: <40e66e0b0703061036h66f292bdr35f496d71bf93430@mail.gmail.com>

On 3/6/07, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Bert Gunter wrote:
> > Folks:
> >
> > In the past 2 days I have seen a large increase of  spam getting into
> > R-help. Are others experiencing this problem? If so, has there been some
> > change to the spam filters on the R-servers? If not, is the problem on my
> > end?
> >
> > Feel free to reply privately.
> >
> Martin Maechler is still walking about upside-down after the DSC, slated
> to return on March 8 (plus presumably a day or two to recover from the
> flight...).

Actually he is walking around sideways at present.  He's in Dubai, not
New Zealand.

> The lists are currently on minimal maintenance and the spam
> filters have been known to break and give up occasionally. I think we
> just have to bear with it for a few more days.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From zhuxuhong2000 at gmail.com  Tue Mar  6 19:40:56 2007
From: zhuxuhong2000 at gmail.com (Xuhong Zhu)
Date: Tue, 6 Mar 2007 13:40:56 -0500
Subject: [R] Question about the smooth.Pspline
Message-ID: <2ac439730703061040m2ebb1f13t673afdb68c250ee@mail.gmail.com>

Hello, Everyone,

I want to use the smooth.Pspline to smooth my data but R give me the
error message as follows:

Error in smooth.Pspline(sort.e$time, sort.e$cuff, method = 3) :
        X not strictly increasing
>

my data looks like the following:

id       cuff           time     patient
...
2783 13.229608  478       6
3472 20.904825  478       7
4155 15.033727  478       8
4845 19.342963  478       9
715   8.000000  479       3
1422 22.052385  479       4
2110 15.393063  479       5
2784 13.200922  479       6
3473 20.900132  479       7
...

my R codes is:

e <- rbind(patient.1,patient.2,patient.3,...)
attach(e)
sort.e <- e[order(time),]
plot(sort.e$time, sort.e$cuff, xlab="Time", ylab="Cuff",type="p", col=3,
xlim=c(c[2],d[2]) , ylim=c(c[1], d[1]) , main="one Smooth Curve for 10
Patients")
fm <- smooth.Pspline(sort.e$time, sort.e$cuff, method=3)
lines(fm$x, fm$y, lty=1,col=1)


What I am doing here is to combine the data together and find a smooth
curve. My question is if the smooth.Pspline could not be used in my
data since the variable "time" has repeated values.

Again, thanks for your time and kindly help!

Xuhong


From humanvideotapes at countrylover.com  Tue Mar  6 21:14:05 2007
From: humanvideotapes at countrylover.com (Jefferey Maddox)
Date: Tue, 6 Mar 2007 18:54:05 -0120
Subject: [R] Obtain the career you have always wanted with the University
	Degree you deserve.
Message-ID: <01c76020$d0923050$6c822ecf@humanvideotapes>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/829f667c/attachment.ksh 

From selectlotto2006 at i12.com  Tue Mar  6 20:06:31 2007
From: selectlotto2006 at i12.com (FROM SELECT AWARD)
Date: Tue, 6 Mar 2007 20:06:31 +0100
Subject: [R] CONGRATULATIONS
Message-ID: <E1HOezY-0003yX-00@bernie.ethz.ch>

FROM SELECT AWARD!!!
  
FROM THE DESK OF THE DIRECT
SELECTLOTTO INTERNATIONAL AWARD NL.
PROMOTION/PRIZE AWARD DEPT.
DAAWERK 100A, 1103KA
AMSTERDAM THE NETHERLANDS. 

We are pleased to inform you the result of the computer random  selection for the SELECT LOTTERY  International program held on 06th March 2007.Your personal or company email address, attached to ticket number 205-11465886-629, with serial number 3772-99 drew the lucky numbers 7-14-17-23-31-44, and consequently won the lottery in the 2nd category.
       
The email lottery draws was conducted from an exclusive list of 900,000.000 e-mail addresses of individual, companies, and corporate bodies picked by an advanced automated random computer search ballot system from the Internet.All participants email addresses were extracted/selected through a computer balloting system drawn from 900,000.000 email addresses from all over the world  as part of our International Promotions Program, which is conducted annually.No tickets were sold.It was a promotional program from our software department in promoting the benefit of the Internet usage.
      
You have therefore been approved for a lump sum pay out of (Seven hundred and fifty thousand Euros) 750.000 Euros in cash credited to file REF NO: Ref.No NLD/2300786008/06.Batch No 10/044/SLN  This is from total prize money  of 4.5 million Euros shared among several international lucky winners in this category.
 
CONGRATULATIONS!!! 

Your funds have now been deposited and insured with our affiliate security firm for transfer into your nominated bank account either by means of wire transfers through any of our correspondent banks or any other means suitable to you. We also advice that you keep your winning information very confidential as our security policy demands to avoid double claims/impersonation and unwarranted taking advantage of this program by participants.
To begin your claims, you are urgently requested to contact our claims agent (Regional Finance) Amsterdam-Netherlands immediately with your telephone number and fax numbers, and make sure you quote your, REF NO: NLD/2300786008/06,Batch No: 10/044/SLN your name and contact details in all your correspondence with the finance institute.

Your allocated claims agent details are as follows. 
Company Name:Regional Finance 
Officer in Charge: Mr.Andrea Henrik
Fax:+31-847-404-155
E-mail: agenawr at aol.com

LOTTERY PAYMENT APPLICATION FORM 
(To be completed by lottery winners only) 
Ticket Number: ............................................... 
Reference Number:............................................ 
First Name: ...................................................... 
Last Name:..................................................... 
Home Address:.................................................. 
Town/City: ..................................................... 
Post Code: ...................................................... 
Country: ............................................................ 
Telephone (Home): ......................................... 
Mobile: ............................................................ 
Fax (Private): ................................................... 
Fax (Office): ..................................................... 
Email Address: ................................................ 
Date of Birth: ................................................... 
Occupation: .................................................... 
Marital Status: .................................................... 
I.............................???hereby declare that the above information are 
true and binding on me. If at any time it is discovered that I have given false 
information, I will forfeit my rights to my winnings.theabove detailed information will be absolutely necessary to file in your claims. 
********************************************************************* 
Note that, all winnings must be claimed not later than Two Weeks 
starting from today 26th March 2007,   
********************************************************************* 
You may be required to provide any of the above information during the process of collecting your prize. 

We congratulate you once again and it is our hope that you participate in any of our international programs in the nearest future. 
 After this date all unclaimed funds will be return to the promotion company. 
Yours sincerely,
Mrs Eliza Bosch (Lottery Coordinator)


From fgnzqenwnb at yahoo.com  Tue Mar  6 20:06:53 2007
From: fgnzqenwnb at yahoo.com (Glenna Mcallister)
Date: Wed, 07 Mar 2007 04:06:53 +0900
Subject: xM\B
Message-ID: <E1HOf01-0003z9-00@bernie.ethz.ch>

????????????????
http://soku-ai.net/?oa01sm03
???????????
?GOOGLE?????????????
?????????????????????1490???????????2004?2??
?????????????????????????????????????

?????????????????????????????????????????????????????????????????????????????????????????

?????????????????????????????????????????????
??????????????????????????????????????????
???????
??????????????????????????????????????????OK???????*^_^*??????????????????????????????????????????????????????????????????????????????????URL?http://XXXXXXXXXXXXXXXXXXXXXXXXXXXezXXXXXXXXXXX/?
????????????????????????
1???????????????????
?????????????
??????????????

??
???????????????????? 

??????????????????9????????????????????????????????????10?????????????????????
?????????????????????????????????????
PC?????????????????????????????????????????????????????????????????????????????????

???????????????????
[????????????Change Lovers.....]
?????????????????????????????????????????????????????????????????????????????
? 
?http://soku-ai.net/?oa01sm03??????? 

????????????????????????
??????????????????????????????????????
inquireofcom at hotmail.co.jp


From "FROM:MRS.KARLIJN" at home.sinet.sk  Tue Mar  6 19:11:39 2007
From: "FROM:MRS.KARLIJN" at home.sinet.sk ("FROM:MRS.KARLIJN" at home.sinet.sk)
Date: Tue,  6 Mar 2007 19:11:39 +0100 (CET)
Subject: [R] FROM: MRS. KARLIJN JOHNSON,
Message-ID: <20070306181139.995DA400484A@home.sinet.sk>


FROM: MRS. KARLIJN JOHNSON,

EURO-PW LOTTERY ONLINE.
LOTTERY AND GAMING CORPORATION,

MONTH OF MARCH EURO-PW LOTTERY ONLINE,

WINNING NUMBER: FLO-99/002/100

OUR DEAR WINNER,

YOU WON THE SUM OF (ONE MILLION EURO) FROM EURO-PW LOTTERY ONLINE AND GAMING CORPORATION. THE WINNING TICKET WAS SELECTED FROM A DATA BASE OF INTERNET E-MAIL USERS, FROM WHICH YOUR E-MAIL ADDRESS CAME OUT AS THE WINNING COUPON.

WE THEREBY CONTACT YOU TO CLAIM YOUR WINNING AMOUNT QUICKLY AS THIS IS A MONTHLY LOTTERY. FAILURE TO CLAIM YOUR WIN WILL RESULT INTO THE REVERSION OF THE WINNING PRIZE TO OUR FOLLOWING MONTH LOTTERY. (EXPIRING DATE, 16TH OF MARCH). PLEASE CONTACT OUR APPROVED AGENT FOR YOUR REGION WITH YOUR WINNING NUMBER.

EURO-PW LOTTERY ONLINE AGENCY. 
MISS. DESKA ROWLAND. 
DIRECTOR OF WINNING CLAIMS DEPARTMENT.
TEL: 31-622-023-935
E-MAIL: euloagencybvz at aim.com

DUE TO MIX UP OF SOME NUMBERS AND NAMES, WE ASK THAT YOU KEEP YOUR WINNING INFORMATION CONFIDENTIAL UNTIL YOUR CLAIMS HAVE PROCESSED. THIS IS PART OF OUR SECURITY PROTOCOL TO AVOID DOUBLE CLAIMING AND UNWARRANTED ABUSE OF THIS PROGRAM BY SOME PARTICIPANT.

REGARDS,

FROM: MRS. KARLIJN JOHNSON.  
DIRECTOR OF EURO-PW LOTTERY ONLINE.  

LOTTOPW WARANDESTRAAT
564 2330 GK TURNHOUT HOLLAND.
WEBSITE: www.europw.com


From hassanysabbah at hotmail.com  Tue Mar  6 20:15:46 2007
From: hassanysabbah at hotmail.com (Harry Ho)
Date: Tue, 06 Mar 2007 19:15:46 +0000
Subject: [R] R plug in for Eclipse
Message-ID: <BAY107-F925E656672C230FF4F684AB7B0@phx.gbl>

Hello,

Is there any R plug-in available for Eclipse other than StatET?

StatET doesn't seem to work with the latest release of Eclipse properly, 
i.e. syntax highlighting isn't enabled for R commands. I already contacted 
the author some time ago, but have not yet received a response.

Would also be great if somebody could tell me whether they have the same 
problem. I already tried installing it on two different machines so I guess 
it isn't a local problem, but you never know...


Thx a lot

_________________________________________________________________
Sie suchen E-Mails, Dokumente oder Fotos? Die neue MSN Suche Toolbar mit 
Windows-Desktopsuche liefert in sekundenschnelle Ergebnisse. Jetzt neu!


From zvczxbccnmbvgd54 at yahoo.co.jp  Tue Mar  6 20:22:32 2007
From: zvczxbccnmbvgd54 at yahoo.co.jp (=?ISO-2022-JP?B?GyRCIVo8Qk9DIVshWjJIQjIhWxsoQg==?=)
Date: Tue, 6 Mar 2007 20:22:32 +0100
Subject: [R] =?iso-2022-jp?b?GyRCO2QkcjY4JG8kOyQ/JCIkTkZ8JE5CTjgzGyhC?=
Message-ID: <200703061922.l26JMWkr025041@hypatia.math.ethz.ch>

$B2HB2$,0&M_$NBP>]$K$J$C$?;~!"=w$O!D(B
$B1[$($F$O$$$1$J$$6XCG$N0l@~$r!D(B
$BKe$H5A7;!*(B $B;P$H5ADo!*(B $BL<$H5AIc!*(B

$B0l@~$r1[$($?GXFA$N1c!#(B 

$B2HB2$H$7$F at 8$-$F9T$/$H$$$&3P8g$O at H$/$bJx$l5n$j!"<c$$FyBN$NN:$K$J$C$F$7$^$&!*!!(B 

TOPPAGE$B"*(B http://www.iveoke.com/?bc=web05&md=@8xhj0I at 80@wFh_Psw at e51$B!!"+(BTOPPAGE

$B5v$5$l$J$$4X78!*!!2w3Z$KE.$l$F$$$/2HB2$NJ*8l#4JT!*!*(B













$B!A!A!A!A!A!A!A!A!A!A!A!A!A!A!A!A!A!A!A!A!A!A!A(B
$B$*<j?t$G$9$,>pJs%a!<%k$NG[?.Dd;_$NJ}$O%3%A%i$^$G$*4j$$$7$^$9!#"M(B cancel at iveoke.com


From vkm2 at cornell.edu  Tue Mar  6 20:33:17 2007
From: vkm2 at cornell.edu (Vishal Kiritkumar Mehta)
Date: Tue, 6 Mar 2007 14:33:17 -0500 (EST)
Subject: [R] mixed  exponential distribution
Message-ID: <1464.132.236.142.139.1173209597.squirrel@webmail.cornell.edu>

Hi all,

I have a 15min rainfall intensity dataset. I would like to fit a mixture
of two exponential distributions to this dataset, and then extract certain
quantiles from it.

Is there a package in R that will allow me to get maximum likelihood
estimates for the paramters, do goodness of fit tests, and then extract
quantiles from the fitted micture distribution? If not is there another
way to do this in R?

thanks,
vishal


From gunter.berton at gene.com  Tue Mar  6 20:25:38 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Tue, 6 Mar 2007 11:25:38 -0800
Subject: [R] Recalling and printing multiple graphs. Is there
	somethingin	the HISTORY menu that will help?
In-Reply-To: <45ED6204.A712.00CB.0@grecc.umaryland.edu>
Message-ID: <008801c76025$39237530$4d908980@gne.windows.gene.com>

See FAQ for Windows 5.2 and the referenced README.


?win.metafile and ?replayPlot might allow you to "replay" the saved plot
history (by default in .SavedPlots) into a file in emf or wmf format, I
think, but I haven't actually tried this -- don't know if it will work for
multiple graphs.

Let us know if this approach works if you don't get a definitive answer
elsewhere.

Cheers,
Bert

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374




-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Sorkin
Sent: Tuesday, March 06, 2007 9:44 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Recalling and printing multiple graphs. Is there somethingin
the HISTORY menu that will help?

I have written an R function that produces multiple graphs. I use
par(ask=TRUE) to allow for the inspection of each graph before the next
graph is drawn. I am looking for a way to recall all graphs drawn in an
R session, and a method that can be used to print all the graphs at one
time. I know that I could simply print each graph after I inspect the
graph, but this gets tiresome if one's function produces tens of graphs.
I suspect that if I knew more about the history menu (which currently
has an entry RECORDING) I could get the graphs to be replayed and
printed, but alas I have not been able to find instructions for using
the HISTORY menu. Please take pity on my  when you let me know that some
easy search or command could get me the information I needed. I have
looked, but clearly in the wrong places.
John 
 
 
John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC,
University of Maryland School of Medicine Claude D. Pepper OAIC,
University of Maryland Clinical Nutrition Research Unit, and
Baltimore VA Center Stroke of Excellence

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)
jsorkin at grecc.umaryland.edu
Confidentiality Statement:
This email message, including any attachments, is for the so...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From klaster at karlin.mff.cuni.cz  Tue Mar  6 20:26:18 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Tue, 06 Mar 2007 20:26:18 +0100
Subject: [R] Recalling and printing multiple graphs. Is there something
 in	the HISTORY menu that will help?
In-Reply-To: <45ED6204.A712.00CB.0@grecc.umaryland.edu>
References: <45ED6204.A712.00CB.0@grecc.umaryland.edu>
Message-ID: <45EDC05A.9050000@karlin.mff.cuni.cz>

options(graphics.record=TRUE)
can be used to switch the recording on (MS Windows, not sure about other 
platforms). See ?options
To set this option as default, use .Rprofile

It is however quite annoying to examine and save all the graphs 
manually... You might find functions like postscript(), png(), jpeg() 
and others useful.

Petr

John Sorkin napsal(a):
> I have written an R function that produces multiple graphs. I use
> par(ask=TRUE) to allow for the inspection of each graph before the next
> graph is drawn. I am looking for a way to recall all graphs drawn in an
> R session, and a method that can be used to print all the graphs at one
> time. I know that I could simply print each graph after I inspect the
> graph, but this gets tiresome if one's function produces tens of graphs.
> I suspect that if I knew more about the history menu (which currently
> has an entry RECORDING) I could get the graphs to be replayed and
> printed, but alas I have not been able to find instructions for using
> the HISTORY menu. Please take pity on my  when you let me know that some
> easy search or command could get me the information I needed. I have
> looked, but clearly in the wrong places.
> John 
>  
>  
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC,
> University of Maryland School of Medicine Claude D. Pepper OAIC,
> University of Maryland Clinical Nutrition Research Unit, and
> Baltimore VA Center Stroke of Excellence
-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From albmont at centroin.com.br  Tue Mar  6 20:27:12 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Tue, 6 Mar 2007 17:27:12 -0200
Subject: [R] Generate random numbers up to one
In-Reply-To: <XFMail.070306181058.ted.harding@nessie.mcc.ac.uk>
References: <45ED7E06.1040204@karlin.mff.cuni.cz>
	<XFMail.070306181058.ted.harding@nessie.mcc.ac.uk>
Message-ID: <20070306191950.M40735@centroin.com.br>

Ted Harding wrote:
> 
> And, specifically (to take just 2 RVs X and Y), while U = X/(X+Y)
> and V = Y/(A+Y) are two RVs which summ to 1, the distribution of U
> is not the same as the distribution of X conditional on (X+Y = 1).
> 
This question appeared in October 2006, and the answer was
the Dirichlet distribution with parameters (1,1,1...1):

http://en.wikipedia.org/wiki/Dirichlet_distribution

It's the distribution of uniform U1, U2, ... Un with the
restriction that U1 + U2 + ... + Un = 1.

Alberto Monteiro


From Ryan.G.Huckstorf at wellsfargo.com  Tue Mar  6 20:51:14 2007
From: Ryan.G.Huckstorf at wellsfargo.com (Ryan.G.Huckstorf at wellsfargo.com)
Date: Tue, 6 Mar 2007 13:51:14 -0600
Subject: [R] Generating R plots through Perl - Solution
References: <CCA676360D67EA47A0D5CC6ACC22523E011D3AEF@msgawbmnmsp09.wellsfargo.com>
	<28B7F2B9-5FA6-4AB2-9602-00B219EE1337@hanover.edu>
Message-ID: <CCA676360D67EA47A0D5CC6ACC22523E011D42B1@msgawbmnmsp09.wellsfargo.com>

Hello,

I tried what you suggested (i.e. combine the separate plot creation
commands into one command from Perl to R), and it worked.  The syntax is
as follows:

$R->send(qq (xVal <- c(1,2,3,4,5,6)));
$R->send(qq (yVal <- c(3,5,2,6,1,5)));
$R->send(qq (c(pdf("C:/Test Environment/R/perlPlotTest.pdf"), plot(xVal,
yVal), dev.off())));

I appreciate your help with this, and I will look into ruby and the "r
for ruby" project for future use.

Thanks again,
Ryan


-----Original Message-----
From: Charilaos Skiadas [mailto:skiadas at hanover.edu] 
Sent: Thursday, March 01, 2007 6:15 PM
To: Huckstorf, Ryan
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Generating R plots through Perl

On Mar 1, 2007, at 6:28 PM, <Ryan.G.Huckstorf at wellsfargo.com> wrote:

First off, if you are working in perl you might want to be aware of ruby
and the "r for ruby" project:
http://rubyforge.org/projects/r4ruby/

> Hello,
>
> $R->send(qq (xVal <- c(1,2,3,4,5,6))); $R->send(qq (yVal <- 
> c(3,5,2,6,1,5))); $R->send(qq (pdf("C:/Test 
> Environment/R/perlPlotTest.pdf")));
> $R->send(qq (plot(xVal, yVal)));
> $R->send(qq (graphics.off()));

I don't really know how to write this in perl, but could you perhaps put
the last three lines all in one call to "$R->send", using dev.off
() then? Don't know if it would make a difference, but that's the only
thing I could think of. I'm guessing something like this:

$R->send(qq (pdf("C:/Test Environment/R/perlPlotTest.pdf"); plot (xVal,
yVal); dev.off()));

> As the code indicates, I am using R's pdf function to create a pdf 
> file containing the plot of xVal and yVal.  I am using the 
> graphics.off() function rather than the dev.off() function as I get an

> error message of "<simpleError in dev.off(): cannot shut down device 1

> (the null device)>" when dev.off() is used.  Is there another way to 
> generate and save a plot using the bridge connection that I described?

> If not, what would be an efficient way of generating and saving plots 
> from within my Perl program?  Any help would be greatly appreciated.
>
> Thank you,
> Ryan

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From f.harrell at vanderbilt.edu  Tue Mar  6 20:59:53 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 06 Mar 2007 13:59:53 -0600
Subject: [R] R and SAS proc format
In-Reply-To: <BAY113-F230DB711DB8D84B2F3E043997B0@phx.gbl>
References: <BAY113-F230DB711DB8D84B2F3E043997B0@phx.gbl>
Message-ID: <45EDC839.9020707@vanderbilt.edu>

lamack lamack wrote:
> Dear all, Is there an R equivalent to SAS's proc format?
> 
> Best regards
> 
> J. Lamack

Fortunately not.  SAS is one of the few large systems that does not 
implicitly support value labels and that separates label information 
from the database [I can't count the number of times someone has sent me 
a SAS dataset and forgotten to send the PROC FORMAT value labels].  See 
the factor function for information about how R does this.

Frank Harrell

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From jholtman at gmail.com  Tue Mar  6 21:25:03 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 6 Mar 2007 15:25:03 -0500
Subject: [R] Recalling and printing multiple graphs. Is there something
	in the HISTORY menu that will help?
In-Reply-To: <45ED6204.A712.00CB.0@grecc.umaryland.edu>
References: <45ED6204.A712.00CB.0@grecc.umaryland.edu>
Message-ID: <644e1f320703061225xab12e38v2c1915024b53a3ce@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/8fe950e8/attachment.ksh 

From br44114 at gmail.com  Tue Mar  6 21:25:51 2007
From: br44114 at gmail.com (bogdan romocea)
Date: Tue, 6 Mar 2007 15:25:51 -0500
Subject: [R] R and SAS proc format
Message-ID: <8d5a36350703061225w43f80eeesf75b836b6adcdd2c@mail.gmail.com>

See ?cut for continuous variables, and ?factor, ?levels for the others.


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of lamack lamack
> Sent: Tuesday, March 06, 2007 12:49 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] R and SAS proc format
>
> Dear all, Is there an R equivalent to SAS's proc format?
>
> Best regards
>
> J. Lamack
>
> _________________________________________________________________
> O Windows Live Spaces ? seu espa?o na internet com fotos (500
> por m?s), blog
> e agora com rede social http://spaces.live.com/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From groemp at tfh-berlin.de  Tue Mar  6 21:26:13 2007
From: groemp at tfh-berlin.de (=?UTF-8?Q?Ulrike_Gr=C3=B6mping?=)
Date: Tue, 6 Mar 2007 12:26:13 -0800 (PST)
Subject: [R] R and SAS proc format
In-Reply-To: <BAY113-F230DB711DB8D84B2F3E043997B0@phx.gbl>
References: <BAY113-F230DB711DB8D84B2F3E043997B0@phx.gbl>
Message-ID: <9340323.post@talk.nabble.com>


The down side to R's factor solution: 
The numerical values of factors are always 1 to number of levels. Thus, it
can be tough and requires great care to work with studies that have both
numerical values different from this and value labels. This situation is
currently not well-supported by R.

Regards, Ulrike

P.S.: I fully agree with Frank regarding the annoyance one sometimes
encounters with formats in SAS! 


lamack lamack wrote:
> 
> Dear all, Is there an R equivalent to SAS's proc format?
> 
> Best regards
> 
> J. Lamack
> 
> _________________________________________________________________
> O Windows Live Spaces ? seu espa?o na internet com fotos (500 por m?s),
> blog 
> e agora com rede social http://spaces.live.com/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/R-and-SAS-proc-format-tf3357624.html#a9340323
Sent from the R help mailing list archive at Nabble.com.


From albmont at centroin.com.br  Tue Mar  6 21:38:18 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Tue, 6 Mar 2007 18:38:18 -0200
Subject: [R] Generic distributions
Message-ID: <20070306202917.M87899@centroin.com.br>

Is there any class that generalizes distributions?

For example, I could say 
x <- generic_distribution("normal", list(mean=1, sigma=0.5))
and then use it like 
rgeneric_distribution(100, x) to get a sample of 100, or
pgeneric_distribution(0.5, x) to get the pdf at (x = 0.5).

In the openbugs/winbugs package, that uses a language that
looks like R/S, we can do things like x ~ dnorm(mu, tau),
forget that x is a normal with mean mu and variance 1/tau,
and then treat it generically.

Alberto Monteiro

PS: this is noise... but due to spam invasion, anything that
increases the nonspam/spam ratio should be welcome :-)


From Roger.Vallejo at ARS.USDA.GOV  Tue Mar  6 21:39:47 2007
From: Roger.Vallejo at ARS.USDA.GOV (Vallejo, Roger)
Date: Tue, 6 Mar 2007 15:39:47 -0500
Subject: [R] cluster analysis
Message-ID: <F649B8DBD3DEAD4BB163AC4D9AC18B5F41BDD0@MD-MAIL-01.ARSNET.ARS.USDA.GOV>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/a1000557/attachment.ksh 

From davidkat at davidkatzconsulting.com  Tue Mar  6 21:43:52 2007
From: davidkat at davidkatzconsulting.com (davidkat at davidkatzconsulting.com)
Date: Tue, 06 Mar 2007 12:43:52 -0800
Subject: [R] Memory Limits in Ubuntu Linux
Message-ID: <45ED6208.10355.222DB1@davidkat.davidkatzconsulting.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/681b31d6/attachment.ksh 

From liai at mail.nih.gov  Tue Mar  6 22:02:14 2007
From: liai at mail.nih.gov (Li, Aiguo (NIH/NCI) [C])
Date: Tue, 6 Mar 2007 16:02:14 -0500
Subject: [R] Subseting data frame based on column names
Message-ID: <EB0E4D86EDE61C4D9B54FD73E8BE247AC27135@nihcesmlbx10.nih.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/8db3676c/attachment.ksh 

From jrkrideau at yahoo.ca  Tue Mar  6 22:13:54 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 6 Mar 2007 16:13:54 -0500 (EST)
Subject: [R] R and SAS proc format
In-Reply-To: <BAY113-F230DB711DB8D84B2F3E043997B0@phx.gbl>
Message-ID: <975945.33063.qm@web32812.mail.mud.yahoo.com>


--- lamack lamack <lamac_k at hotmail.com> wrote:

> Dear all, Is there an R equivalent to SAS's proc
> format?

What does the SAS PROC FORMAT do?


From ggrothendieck at gmail.com  Tue Mar  6 22:14:04 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 6 Mar 2007 16:14:04 -0500
Subject: [R] optim(), nlminb() and starting values
In-Reply-To: <cbca975a0703061018q46f34a00n2859b69abff253d2@mail.gmail.com>
References: <cbca975a0703061018q46f34a00n2859b69abff253d2@mail.gmail.com>
Message-ID: <971536df0703061314h60203d1sd9ca5b8c63d63ec0@mail.gmail.com>

If your problem is small enough just use a grid of starting values
and run your optimization on each one and then take the best.


On 3/6/07, Dae-Jin Lee <lee.daejin at gmail.com> wrote:
> Hi all !
>
> I've been trying to maximize a likelihood using optim( ) function, but it
> seems that the function has several local maxima. I've tried in my algorithm
> with different starting values and depending on them "optim" obtains
> different results...
>
> I use the "L-BFGS-B" method setting the lower values as 1e-06, because my
> parameters must be strictly positive. Also tried a log() transformation to
> ensure that my parameters are positive. Don't know if this is useful in this
> case... (also with notLog and notExp functions of mgcv package)
>
> the function nlminb( ) also have the same problems.
>
>
> ?Is there any thing I'm not considering? I mean other methods instead of
> "L-BFGS-B"?
>
> How can I do to take a strategy to begin with "good" starting values?
>
>
> Thanks in advance
>
> Dae-Jin
>
> PS: I'm trying to fit a mixed model with REML and several random effects, so
> I maximize over several parameters...
>
>        [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From f.harrell at vanderbilt.edu  Tue Mar  6 22:36:00 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 06 Mar 2007 15:36:00 -0600
Subject: [R] R and SAS proc format
In-Reply-To: <9340323.post@talk.nabble.com>
References: <BAY113-F230DB711DB8D84B2F3E043997B0@phx.gbl>
	<9340323.post@talk.nabble.com>
Message-ID: <45EDDEC0.6050808@vanderbilt.edu>

Ulrike Gr?mping wrote:
> The down side to R's factor solution: 
> The numerical values of factors are always 1 to number of levels. Thus, it
> can be tough and requires great care to work with studies that have both
> numerical values different from this and value labels. This situation is
> currently not well-supported by R.

You can add an attribute to a variable.  In the sas.get function in the 
Hmisc package for example, when importing SAS variables that have PROC 
FORMAT value labels, an attribute 'sas.codes' keeps the original codes; 
these can be retrieved using sas.codes(variable name).  This could be 
done outside the SAS import context also.

Frank

> 
> Regards, Ulrike
> 
> P.S.: I fully agree with Frank regarding the annoyance one sometimes
> encounters with formats in SAS! 
> 
> 
> lamack lamack wrote:
>> Dear all, Is there an R equivalent to SAS's proc format?
>>
>> Best regards
>>
>> J. Lamack
>>
>> _________________________________________________________________
>> O Windows Live Spaces ? seu espa?o na internet com fotos (500 por m?s),
>> blog 
>> e agora com rede social http://spaces.live.com/
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From jsorkin at grecc.umaryland.edu  Tue Mar  6 22:35:47 2007
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 06 Mar 2007 16:35:47 -0500
Subject: [R] Recalling and printing multiple graphs. Is there something
	in	the HISTORY menu that will help?
Message-ID: <45ED9862.A712.00CB.0@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/30fd9e79/attachment.ksh 

From scionforbai at gmail.com  Tue Mar  6 22:42:14 2007
From: scionforbai at gmail.com (Scionforbai)
Date: Tue, 6 Mar 2007 22:42:14 +0100
Subject: [R]  Obtaining figures with exactly placed points
Message-ID: <e9ee1f0a0703061342i3e3f1c2q12753749222cf86b@mail.gmail.com>

Dear list,

I have to plot some geometrical shape given as list of points. My need
is the following: let's say my shape is a 1 inch large square; how can
I plot it with R in a graphic format that gives me an image *exactly*
1 inch large? I tried to set oma, mar and fin parameters, but with no
success.

I'm currently using the xfig driver for the final images are to be
included in latex documents, so I can easily add latex mathematical
text and other things; however, other formats (pdf, eps) will do.

The piece of code:

xfig("R2fig.fig")
par(mar=c(0,0,0,0),oma=c(0,0,0,0),fin=c(1,1))#no margin, figure
dimension 1x1 inch
plot(0,0,"n",xlim=c(0,1),ylim=c(0,1),axes=F,ann=F)
rect(0,0,1,1)
dev.off()

gives me not what I want:
$ cat R2fig.fig
#FIG 3.2
Landscape
Flush left
Inches
A4
100.00
Single
-2
1200 2
# End of XFig header
2 2 0 1 0 -1 100 0 -1 4.000 0 0 -1 0 0 5
         0 1111 0 0 1111 0 1111 1111 0 1111

And this is what it should be (drawn by Xfig itself):
$ cat Xfig.fig
#FIG 3.2
Landscape
Flush left
Inches
A4
100.00
Single
-2
1200 2
2 2 0 1 0 7 50 -1 -1 0.000 0 0 -1 0 0 5
         0 0 1200 0 1200 1200 0 1200 0 0

The interesting part is the last line; 1200 is replaced by 1111 in the
R ouput. Why is the R ouput still scaled? Do I miss some graphic
parameter?

Any tip is appreciated,
marco


From klaster at karlin.mff.cuni.cz  Tue Mar  6 22:43:03 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Tue, 06 Mar 2007 22:43:03 +0100
Subject: [R] Question about the smooth.Pspline
In-Reply-To: <2ac439730703061040m2ebb1f13t673afdb68c250ee@mail.gmail.com>
References: <2ac439730703061040m2ebb1f13t673afdb68c250ee@mail.gmail.com>
Message-ID: <45EDE067.4070600@karlin.mff.cuni.cz>

Xuhong Zhu napsal(a):
> Hello, Everyone,
> 
> I want to use the smooth.Pspline to smooth my data but R give me the
> error message as follows:
> 
> Error in smooth.Pspline(sort.e$time, sort.e$cuff, method = 3) :
>         X not strictly increasing
> 
> my data looks like the following:
> 
> id       cuff           time     patient
> ...
> 2783 13.229608  478       6
> 3472 20.904825  478       7
> 4155 15.033727  478       8
> 4845 19.342963  478       9
> 715   8.000000  479       3
> 1422 22.052385  479       4
> 2110 15.393063  479       5
> 2784 13.200922  479       6
> 3473 20.900132  479       7
> ...
> 
> my R codes is:
> 
> e <- rbind(patient.1,patient.2,patient.3,...)
> attach(e)
> sort.e <- e[order(time),]
> plot(sort.e$time, sort.e$cuff, xlab="Time", ylab="Cuff",type="p", col=3,
> xlim=c(c[2],d[2]) , ylim=c(c[1], d[1]) , main="one Smooth Curve for 10
> Patients")
> fm <- smooth.Pspline(sort.e$time, sort.e$cuff, method=3)
> lines(fm$x, fm$y, lty=1,col=1)
> 
> 
> What I am doing here is to combine the data together and find a smooth
> curve. My question is if the smooth.Pspline could not be used in my
> data since the variable "time" has repeated values.

Quoted from your error message:
 > Error in smooth.Pspline(sort.e$time, sort.e$cuff, method = 3) :
 >         X not strictly increasing

Quoted from help of smooth.Pspline (which, as you should have told us by 
the way, is contained in package pspline):

Arguments:
x 	values of the predictor variable. These must be strictly increasing, 
and there must be at least 2*norder + 1 of them.
sm.spline provides a simplified interface, in which the x values can be 
unsorted, or a list with components "x" and "y" or a two-column matrix 
or a complex vector.

Strictly increasing = no repeated values...
Petr

> 
> Xuhong
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From Horace.Tso at pgn.com  Tue Mar  6 22:45:28 2007
From: Horace.Tso at pgn.com (Horace Tso)
Date: Tue, 06 Mar 2007 13:45:28 -0800
Subject: [R] qr decomposition issue inside lm
In-Reply-To: <CCA676360D67EA47A0D5CC6ACC22523E011D42B1@msgawbmnmsp09.wellsfargo.com>
References: <CCA676360D67EA47A0D5CC6ACC22523E011D3AEF@msgawbmnmsp09.wellsfargo.com>
	<28B7F2B9-5FA6-4AB2-9602-00B219EE1337@hanover.edu>
	<CCA676360D67EA47A0D5CC6ACC22523E011D42B1@msgawbmnmsp09.wellsfargo.com>
Message-ID: <45ED70780200006500003F0A@pgn.com>

Dear list,

It's never happened to me before in such a simple exercise but is not going away and I've checked my data are good. I want a simple lm model with one response and one predictor, where N is about 4,200 * data set not exactly small. Both x and y are nice, continuous variables having NA filtered out with a call to na.omit. So I did

mod = lm( y ~ x, data=x1)

Then the error,

Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : 
        NA/NaN/Inf in foreign function call (arg 4)

I did a trace back and it turned out it's an error thrown by the Fortran subroutine that seems to be trying a QR decomposition,

traceback()
3: .Fortran("dqrls", qr = x, n = n, p = p, y = y, ny = ny, tol = as.double(tol), 
       coefficients = mat.or.vec(p, ny), residuals = y, effects = y, 
       rank = integer(1), pivot = 1:p, qraux = double(p), work = double(2 * 
           p), PACKAGE = "base")
2: lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...)
1: lm(log.p.sales ~ log.mktcap, data = x1)

My question is why would QR fail since the default in lm.fit is 'singular.ok' ? Furthermore, is there a way to get around presumably a singularity in my design matrix? 

Thanks in advance.

Horace W. Tso


From ligges at statistik.uni-dortmund.de  Tue Mar  6 22:49:54 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 06 Mar 2007 22:49:54 +0100
Subject: [R] Subseting data frame based on column names
In-Reply-To: <EB0E4D86EDE61C4D9B54FD73E8BE247AC27135@nihcesmlbx10.nih.gov>
References: <EB0E4D86EDE61C4D9B54FD73E8BE247AC27135@nihcesmlbx10.nih.gov>
Message-ID: <45EDE202.8040203@statistik.uni-dortmund.de>



Li, Aiguo (NIH/NCI) [C] wrote:
> Hello all,
> 
>  
> 
> I have a data frame containing 170 columns and would like to generate a
> new data frame containing all rows but only 67 columns selected
> according to column names.  How can I do this?

For date.frame "d" and column names in vector "cn":

    d[,cn]

Uwe Ligges



>  
> 
> Thanks in advance,
> 
>  
> 
> AG Lee
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Tue Mar  6 22:51:55 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 06 Mar 2007 22:51:55 +0100
Subject: [R] Memory Limits in Ubuntu Linux
In-Reply-To: <45ED6208.10355.222DB1@davidkat.davidkatzconsulting.com>
References: <45ED6208.10355.222DB1@davidkat.davidkatzconsulting.com>
Message-ID: <45EDE27B.6040605@statistik.uni-dortmund.de>



davidkat at davidkatzconsulting.com wrote:
> I am an R user trying to get around the 2Gig memory limit in Windows, so 
> here I am days later with a working Ubuntu, and R under Ubuntu. But - the 
> memory problems seem worse than ever. R code that worked under 
> windows fails, unable to allocate memory.
> 
> Searching around the web, it appears that the problem may be the ability to 
> find contguous memory for my big vectors, but a fresh boot of Ubuntu does 
> not help either.
> 
> Which way to go?
> 
> 1) Try to install 64-bit version for bigger address space. Would this help? Is 
> this workable for my Athlon 64 Dual-core? (the live cd seems to work but I 
> never got it to boot after a disk install, but then the 386 version was no better 
> until I learned more about Grub...I could try again if this might solve the 
> problem)

If you really have got such amounts of RAM in that machine, it should be 
worth trying.

Uwe Ligges


> 2) Recompile R to get bigger memory capability? (I'll have to cross-post to 
> some R forums too)
> This will be a challenge for a Linux newbie...like me.
> 
> 3) Any other suggestions? My goal is to create a bigger neural network than 
> fits in my Windows R version.


From sapsi at pobox.com  Tue Mar  6 22:54:50 2007
From: sapsi at pobox.com (Saptarshi Guha)
Date: Tue, 6 Mar 2007 16:54:50 -0500
Subject: [R] Substituting functions in package - Lattice
Message-ID: <6C164B61-7743-477A-8545-36BF3ADECA1F@pobox.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/37590d82/attachment.ksh 

From ted.harding at nessie.mcc.ac.uk  Tue Mar  6 23:01:39 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 06 Mar 2007 22:01:39 -0000 (GMT)
Subject: [R] Generate random numbers up to one
In-Reply-To: <20070306191950.M40735@centroin.com.br>
Message-ID: <XFMail.070306220139.ted.harding@nessie.mcc.ac.uk>

On 06-Mar-07 Alberto Monteiro wrote:
> Ted Harding wrote:
>> 
>> And, specifically (to take just 2 RVs X and Y), while U = X/(X+Y)
>> and V = Y/(A+Y) are two RVs which summ to 1, the distribution of U
>> is not the same as the distribution of X conditional on (X+Y = 1).
>> 
> This question

Which question? There are (implicitly) two questions there!

> appeared in October 2006, and the answer

To the second question (X conditional on X+Y=1)

> was the Dirichlet distribution with parameters (1,1,1...1):
> 
> http://en.wikipedia.org/wiki/Dirichlet_distribution
> 
> It's the distribution of uniform U1, U2, ... Un with the
> restriction that U1 + U2 + ... + Un = 1.

Indeed, and the resulting (U1,U2,...,Un) is uniformly distributed
on the simplex U1+U2+...+Un=1. For n>2, however, the resulting
marginal distribution of (say) U1 conditional on (U1+U2+...+Un=1)
is no longer uniform (that only holds for n=2, as in my example).
For n=3 this is easy to see: P[U1 > u1] is the area of the triangular
simplex between its vertex at (1,0,0) and the line from (u1,1-u1,0)
to (u1,0,0), and this is equal to (1 - u1)^2, so the density of U1
is f(u1) = 2*(1-u1).  In general, the marginal density of U1
in the n-dimensional Dirichlet is (n-1)*(1-u1)^(n-2).

But the aim was to illustrate Petr Klasterecky's point that

  "sum(x) is a random variable as well and dividing by
   sum(x) does not preserve the original distribution
   data were generated from."

namely to show two ways of generating RVs distributed on
U1 + U2 + ... + Un = 1, starting from independent RVs, which
result on two different distributions, and to give an example
where dividing by sum(x) can be seen to "not preserve" the
distribution.

Indeed, I think there is sometimes a confusion between this
question and the really unrelated question: Given non-negative
numbers V1, V2, ..., Vn, how can we convert then to a probability
distribution? To which the answer is, of course, divide by their
sum.

With best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 06-Mar-07                                       Time: 22:01:34
------------------------------ XFMail ------------------------------


From mike.prager at noaa.gov  Tue Mar  6 23:04:44 2007
From: mike.prager at noaa.gov (Mike Prager)
Date: Tue, 06 Mar 2007 17:04:44 -0500
Subject: [R] Recalling and printing multiple graphs. Is there something
	in	the HISTORY menu that will help?
References: <45ED6204.A712.00CB.0@grecc.umaryland.edu>
Message-ID: <g3pru2h8sfck7t3p1leodjo2orm0bch1e9@4ax.com>

"John Sorkin" <jsorkin at grecc.umaryland.edu> wrote:

> I have written an R function that produces multiple graphs. I use
> par(ask=TRUE) to allow for the inspection of each graph before the next
> graph is drawn. I am looking for a way to recall all graphs drawn in an
> R session, and a method that can be used to print all the graphs at one
> time. I know that I could simply print each graph after I inspect the
> graph, but this gets tiresome if one's function produces tens of graphs.
> I suspect that if I knew more about the history menu (which currently
> has an entry RECORDING) I could get the graphs to be replayed and
> printed, but alas I have not been able to find instructions for using
> the HISTORY menu. Please take pity on my  when you let me know that some
> easy search or command could get me the information I needed. I have
> looked, but clearly in the wrong places.
>  
> John Sorkin M.D., Ph.D.

I assume you are on Windows (please give OS and R version in
future help requests).  

The code I use before starting such functions is

graphics.off()
windows(record=TRUE)
.SavedPlots <- NULL

which closes existing graphics devices, opens a windows device
with recording on, and deletes any existing graphics history.

You can go through the history with PageUp and PageDown keys.

Within your function, you can call savePlot (q.v.) to save each
plot to a file just after it is generated.

HTH

-- 
Mike Prager, NOAA, Beaufort, NC
* Opinions expressed are personal and not represented otherwise.
* Any use of tradenames does not constitute a NOAA endorsement.


From christos at nuverabio.com  Tue Mar  6 22:50:23 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Tue, 6 Mar 2007 16:50:23 -0500
Subject: [R] Memory Limits in Ubuntu Linux
In-Reply-To: <45ED6208.10355.222DB1@davidkat.davidkatzconsulting.com>
References: <45ED6208.10355.222DB1@davidkat.davidkatzconsulting.com>
Message-ID: <004701c76039$71dfe750$0e010a0a@headquarters.silicoinsights>

Take a look at Windows FAQ 2.9.  Following the instructions there, I was
able to make WinXP use at least 3GB of RAM (physical RAM installed) with
Rgui.exe.

-Christos

Christos Hatzis, Ph.D.
Nuvera Biosciences, Inc.
400 West Cummings Park
Suite 5350
Woburn, MA 01801
Tel: 781-938-3830
www.nuverabio.com
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> davidkat at davidkatzconsulting.com
> Sent: Tuesday, March 06, 2007 3:44 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Memory Limits in Ubuntu Linux
> 
> I am an R user trying to get around the 2Gig memory limit in 
> Windows, so here I am days later with a working Ubuntu, and R 
> under Ubuntu. But - the memory problems seem worse than ever. 
> R code that worked under windows fails, unable to allocate memory.
> 
> Searching around the web, it appears that the problem may be 
> the ability to find contguous memory for my big vectors, but 
> a fresh boot of Ubuntu does not help either.
> 
> Which way to go?
> 
> 1) Try to install 64-bit version for bigger address space. 
> Would this help? Is this workable for my Athlon 64 Dual-core? 
> (the live cd seems to work but I never got it to boot after a 
> disk install, but then the 386 version was no better until I 
> learned more about Grub...I could try again if this might solve the
> problem)
> 
> 2) Recompile R to get bigger memory capability? (I'll have to 
> cross-post to some R forums too) This will be a challenge for 
> a Linux newbie...like me.
> 
> 3) Any other suggestions? My goal is to create a bigger 
> neural network than fits in my Windows R version.
> --
> David Katz
>  www.davidkatzconsulting.com
>    541 482-1137
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From Greg.Snow at intermountainmail.org  Tue Mar  6 23:11:16 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 6 Mar 2007 15:11:16 -0700
Subject: [R] Generic distributions
In-Reply-To: <20070306202917.M87899@centroin.com.br>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB879C35@LP-EXCHVS07.CO.IHC.COM>

I think the distr package does this.  There are also packages that link
to winbugs if that is what you really want to do.

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Alberto Monteiro
> Sent: Tuesday, March 06, 2007 1:38 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Generic distributions
> 
> Is there any class that generalizes distributions?
> 
> For example, I could say
> x <- generic_distribution("normal", list(mean=1, sigma=0.5)) 
> and then use it like rgeneric_distribution(100, x) to get a 
> sample of 100, or pgeneric_distribution(0.5, x) to get the 
> pdf at (x = 0.5).
> 
> In the openbugs/winbugs package, that uses a language that 
> looks like R/S, we can do things like x ~ dnorm(mu, tau), 
> forget that x is a normal with mean mu and variance 1/tau, 
> and then treat it generically.
> 
> Alberto Monteiro
> 
> PS: this is noise... but due to spam invasion, anything that 
> increases the nonspam/spam ratio should be welcome :-)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Greg.Snow at intermountainmail.org  Tue Mar  6 23:12:18 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 6 Mar 2007 15:12:18 -0700
Subject: [R] Subseting data frame based on column names
In-Reply-To: <EB0E4D86EDE61C4D9B54FD73E8BE247AC27135@nihcesmlbx10.nih.gov>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB879C39@LP-EXCHVS07.CO.IHC.COM>

Use the subset function and the select argument of that function to
specify the columns.

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Li, 
> Aiguo (NIH/NCI) [C]
> Sent: Tuesday, March 06, 2007 2:02 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Subseting data frame based on column names
> 
> Hello all,
> 
>  
> 
> I have a data frame containing 170 columns and would like to 
> generate a new data frame containing all rows but only 67 
> columns selected according to column names.  How can I do this?
> 
>  
> 
> Thanks in advance,
> 
>  
> 
> AG Lee
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Greg.Snow at intermountainmail.org  Tue Mar  6 23:18:16 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 6 Mar 2007 15:18:16 -0700
Subject: [R] Obtaining figures with exactly placed points
In-Reply-To: <e9ee1f0a0703061342i3e3f1c2q12753749222cf86b@mail.gmail.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB879C3F@LP-EXCHVS07.CO.IHC.COM>

Try:

plot(0,0,"n",xlim=c(0,1),ylim=c(0,1),axes=F,ann=F,xaxs='i',yaxs='i')

To see if that fixes it for you (without the xaxs and yaxs arguments it
adds 4% of the range to each side so that any points plotted do not fall
to close to the axes.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Scionforbai
> Sent: Tuesday, March 06, 2007 2:42 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Obtaining figures with exactly placed points
> 
> Dear list,
> 
> I have to plot some geometrical shape given as list of 
> points. My need is the following: let's say my shape is a 1 
> inch large square; how can I plot it with R in a graphic 
> format that gives me an image *exactly*
> 1 inch large? I tried to set oma, mar and fin parameters, but 
> with no success.
> 
> I'm currently using the xfig driver for the final images are 
> to be included in latex documents, so I can easily add latex 
> mathematical text and other things; however, other formats 
> (pdf, eps) will do.
> 
> The piece of code:
> 
> xfig("R2fig.fig")
> par(mar=c(0,0,0,0),oma=c(0,0,0,0),fin=c(1,1))#no margin, 
> figure dimension 1x1 inch
> plot(0,0,"n",xlim=c(0,1),ylim=c(0,1),axes=F,ann=F)
> rect(0,0,1,1)
> dev.off()
> 
> gives me not what I want:
> $ cat R2fig.fig
> #FIG 3.2
> Landscape
> Flush left
> Inches
> A4
> 100.00
> Single
> -2
> 1200 2
> # End of XFig header
> 2 2 0 1 0 -1 100 0 -1 4.000 0 0 -1 0 0 5
>          0 1111 0 0 1111 0 1111 1111 0 1111
> 
> And this is what it should be (drawn by Xfig itself):
> $ cat Xfig.fig
> #FIG 3.2
> Landscape
> Flush left
> Inches
> A4
> 100.00
> Single
> -2
> 1200 2
> 2 2 0 1 0 7 50 -1 -1 0.000 0 0 -1 0 0 5
>          0 0 1200 0 1200 1200 0 1200 0 0
> 
> The interesting part is the last line; 1200 is replaced by 
> 1111 in the R ouput. Why is the R ouput still scaled? Do I 
> miss some graphic parameter?
> 
> Any tip is appreciated,
> marco
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From presnell at stat.ufl.edu  Tue Mar  6 23:20:34 2007
From: presnell at stat.ufl.edu (Brett Presnell)
Date: Tue, 06 Mar 2007 17:20:34 -0500
Subject: [R] Stangle and annotate
Message-ID: <87lkia6mkt.fsf@stat.ufl.edu>


How exactly should I go about turning off annotation when running
Stangle (Rtangle) with "R CMD Stangle myfile.Rnw"?

Ideally I would like to be able to turn annotation off and on for
individual code chunks, or maybe better, to annotate only named
chunks.  Are either of these things easily done?

Is there some way to figure out this sort of thing without reading
through the source code in Sweave.R?

-- 
Brett Presnell
Department of Statistics
University of Florida


From Greg.Snow at intermountainmail.org  Tue Mar  6 23:29:45 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 6 Mar 2007 15:29:45 -0700
Subject: [R] Substituting functions in package - Lattice
In-Reply-To: <6C164B61-7743-477A-8545-36BF3ADECA1F@pobox.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB879C46@LP-EXCHVS07.CO.IHC.COM>

You need to set the environment on your version of print.trellis to
match that of the original (at least that is what has worked for me when
trying to do modified functions from the trellis package).  Try
something like:

> environment(print.trellis) <- environment(lattice::print.trellis)

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Saptarshi Guha
> Sent: Tuesday, March 06, 2007 2:55 PM
> To: R-Help
> Subject: [R] Substituting functions in package - Lattice
> 
> Hi,
> 	I'm trying to learn how a package works but 
> substituting a parituclart function with my own (basically 
> the original one with some debug statements).
> 	The package is lattice and the method is 
> "print.trellis" which is a
> S3 method and is not visible.
> 	To replace this, i sourced a file with the rewritten 
> print.trellis, and the old one was replaced.
> 	However, i get this error when I attempt to print a 
> trellis object (e.g an xyplot)
> 	
> 	Error in assign("last.object", x, env = .LatticeEnv) :
> 	object ".LatticeEnv" not found
> 	
> 	I notice that .LatticeEnv is defined in zzz.R (in the R 
> folder of the lattice source).
> 
> 	My question, how does one do development on the lattice 
> package without having to recompile everything and install a package?
> 
> 	I read something about emacs and development (i use 
> emacs) in the zzz.R file
> 	"## .First.lib will be used if the NAMESPACE file is 
> missing.  This is ## useful during development, thanks to C-c 
> C-l in Emacs/ESS. It won't ## be used if a NAMESPACE file is 
> present.  Note: Due to registration ## of C functions done in 
> the NAMESPACE file, wireframe (and possibly ## cloud) won't 
> work in this scenario."
> 
> 	But couldn't quite figure it out.
> 	Thanks for any help provided.
> 	Saptarshi
> 
> 
> Saptarshi Guha | sapsi at pobox.com | http://www.stat.purdue.edu/~sguha
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From scionforbai at gmail.com  Tue Mar  6 23:36:01 2007
From: scionforbai at gmail.com (Scionforbai)
Date: Tue, 6 Mar 2007 23:36:01 +0100
Subject: [R] Obtaining figures with exactly placed points
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB879C3F@LP-EXCHVS07.CO.IHC.COM>
References: <e9ee1f0a0703061342i3e3f1c2q12753749222cf86b@mail.gmail.com>
	<07E228A5BE53C24CAD490193A7381BBB879C3F@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <e9ee1f0a0703061436l2bb18bd7ga9b32d127a754ce4@mail.gmail.com>

> plot(0,0,"n",xlim=c(0,1),ylim=c(0,1),axes=F,ann=F,xaxs='i',yaxs='i')

It was exactly this!

> 1111*8/100+1111
[1] 1199.88

Thanks,
Marco


From sapsi at pobox.com  Wed Mar  7 00:20:19 2007
From: sapsi at pobox.com (Saptarshi Guha)
Date: Tue, 6 Mar 2007 18:20:19 -0500
Subject: [R] Substituting functions in package - Lattice
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB879C46@LP-EXCHVS07.CO.IHC.COM>
References: <07E228A5BE53C24CAD490193A7381BBB879C46@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <FD837B7F-4C2F-4A73-A877-F7B1E65BDA70@pobox.com>

Hi,
	Unfortunately, print.trellis is not exported .
>> environment(print.trellis) <- environment(lattice::print.trellis)

returns "Error: 'print.trellis' is not an exported object from  
'namespace:lattice'"

	I then took the environment of lattice

	search()
	#package:lattice is the 2nd element
	latt<-as.environment(2)
	environment(print.trellis)<-latt
	
	Yet, i get the same error as before when I run print(k), where k is  
a trellis object.

	This is the line in NAMESPACE for lattice (from the source)
	S3method(print,    trellis)

	Thanks
	Saptarshi


Saptarshi Guha | sapsi at pobox.com | http://www.stat.purdue.edu/~sguha


On Mar 6, 2007, at 5:29 PM, Greg Snow wrote:

> You need to set the environment on your version of print.trellis to
> match that of the original (at least that is what has worked for me  
> when
> trying to do modified functions from the trellis package).  Try
> something like:
>
>> environment(print.trellis) <- environment(lattice::print.trellis)
>
> Hope this helps,
>
> -- 
> Gregory (Greg) L. Snow Ph.D.
> Statistical Data Center
> Intermountain Healthcare
> greg.snow at intermountainmail.org
> (801) 408-8111
>
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Saptarshi Guha
>> Sent: Tuesday, March 06, 2007 2:55 PM
>> To: R-Help
>> Subject: [R] Substituting functions in package - Lattice
>>
>> Hi,
>> 	I'm trying to learn how a package works but
>> substituting a parituclart function with my own (basically
>> the original one with some debug statements).
>> 	The package is lattice and the method is
>> "print.trellis" which is a
>> S3 method and is not visible.
>> 	To replace this, i sourced a file with the rewritten
>> print.trellis, and the old one was replaced.
>> 	However, i get this error when I attempt to print a
>> trellis object (e.g an xyplot)
>> 	
>> 	Error in assign("last.object", x, env = .LatticeEnv) :
>> 	object ".LatticeEnv" not found
>> 	
>> 	I notice that .LatticeEnv is defined in zzz.R (in the R
>> folder of the lattice source).
>>
>> 	My question, how does one do development on the lattice
>> package without having to recompile everything and install a package?
>>
>> 	I read something about emacs and development (i use
>> emacs) in the zzz.R file
>> 	"## .First.lib will be used if the NAMESPACE file is
>> missing.  This is ## useful during development, thanks to C-c
>> C-l in Emacs/ESS. It won't ## be used if a NAMESPACE file is
>> present.  Note: Due to registration ## of C functions done in
>> the NAMESPACE file, wireframe (and possibly ## cloud) won't
>> work in this scenario."
>>
>> 	But couldn't quite figure it out.
>> 	Thanks for any help provided.
>> 	Saptarshi
>>
>>
>> Saptarshi Guha | sapsi at pobox.com | http://www.stat.purdue.edu/~sguha
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From Horace.Tso at pgn.com  Wed Mar  7 00:24:15 2007
From: Horace.Tso at pgn.com (Horace Tso)
Date: Tue, 06 Mar 2007 15:24:15 -0800
Subject: [R] qr decomposition issue inside lm (solved)
In-Reply-To: <45ED70780200006500003F0A@pgn.com>
References: <CCA676360D67EA47A0D5CC6ACC22523E011D3AEF@msgawbmnmsp09.wellsfargo.com>
	<28B7F2B9-5FA6-4AB2-9602-00B219EE1337@hanover.edu>
	<CCA676360D67EA47A0D5CC6ACC22523E011D42B1@msgawbmnmsp09.wellsfargo.com>
	<45ED70780200006500003F0A@pgn.com>
Message-ID: <45ED879F0200006500003F28@pgn.com>

Folks, apologize for such an obvious oversight on my part. The reason qr
fails is, one of the data points has value of -Inf (response is actually
the log of something, and I have a zero in the original set). That
explains the error message in call to dqrls. I should have taken the
mean of the response before proceeding and that would tell me right away
what's wrong. 

Thanks.

H.

>>> "Horace Tso" <Horace.Tso at pgn.com> 3/6/2007 1:45:28 PM >>>
Dear list,

It's never happened to me before in such a simple exercise but is not
going away and I've checked my data are good. I want a simple lm model
with one response and one predictor, where N is about 4,200 * data set
not exactly small. Both x and y are nice, continuous variables having NA
filtered out with a call to na.omit. So I did

mod = lm( y ~ x, data=x1)

Then the error,

Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...)
: 
        NA/NaN/Inf in foreign function call (arg 4)

I did a trace back and it turned out it's an error thrown by the
Fortran subroutine that seems to be trying a QR decomposition,

traceback()
3: .Fortran("dqrls", qr = x, n = n, p = p, y = y, ny = ny, tol =
as.double(tol), 
       coefficients = mat.or.vec(p, ny), residuals = y, effects = y, 
       rank = integer(1), pivot = 1:p, qraux = double(p), work =
double(2 * 
           p), PACKAGE = "base")
2: lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...)
1: lm(log.p.sales ~ log.mktcap, data = x1)

My question is why would QR fail since the default in lm.fit is
'singular.ok' ? Furthermore, is there a way to get around presumably a
singularity in my design matrix? 

Thanks in advance.

Horace W. Tso

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.


From sapsi at pobox.com  Wed Mar  7 00:24:27 2007
From: sapsi at pobox.com (Saptarshi Guha)
Date: Tue, 6 Mar 2007 18:24:27 -0500
Subject: [R] Substituting functions in package - Lattice
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB879C46@LP-EXCHVS07.CO.IHC.COM>
References: <07E228A5BE53C24CAD490193A7381BBB879C46@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <7D1BC96E-270C-4A8B-9381-6EC0341FE924@pobox.com>

Hi,
	Got it. This worked
	environment(print.trellis) <- getNamespace("lattice")
	where print.trellis is my modified print.trellis.
	
	Thanks.
	Regards
	Saptarshi

Saptarshi Guha | sapsi at pobox.com | http://www.stat.purdue.edu/~sguha

On Mar 6, 2007, at 5:29 PM, Greg Snow wrote:

> You need to set the environment on your version of print.trellis to
> match that of the original (at least that is what has worked for me  
> when
> trying to do modified functions from the trellis package).  Try
> something like:
>
>> environment(print.trellis) <- environment(lattice::print.trellis)
>
> Hope this helps,
>
> -- 
> Gregory (Greg) L. Snow Ph.D.
> Statistical Data Center
> Intermountain Healthcare
> greg.snow at intermountainmail.org
> (801) 408-8111
>
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Saptarshi Guha
>> Sent: Tuesday, March 06, 2007 2:55 PM
>> To: R-Help
>> Subject: [R] Substituting functions in package - Lattice
>>
>> Hi,
>> 	I'm trying to learn how a package works but
>> substituting a parituclart function with my own (basically
>> the original one with some debug statements).
>> 	The package is lattice and the method is
>> "print.trellis" which is a
>> S3 method and is not visible.
>> 	To replace this, i sourced a file with the rewritten
>> print.trellis, and the old one was replaced.
>> 	However, i get this error when I attempt to print a
>> trellis object (e.g an xyplot)
>> 	
>> 	Error in assign("last.object", x, env = .LatticeEnv) :
>> 	object ".LatticeEnv" not found
>> 	
>> 	I notice that .LatticeEnv is defined in zzz.R (in the R
>> folder of the lattice source).
>>
>> 	My question, how does one do development on the lattice
>> package without having to recompile everything and install a package?
>>
>> 	I read something about emacs and development (i use
>> emacs) in the zzz.R file
>> 	"## .First.lib will be used if the NAMESPACE file is
>> missing.  This is ## useful during development, thanks to C-c
>> C-l in Emacs/ESS. It won't ## be used if a NAMESPACE file is
>> present.  Note: Due to registration ## of C functions done in
>> the NAMESPACE file, wireframe (and possibly ## cloud) won't
>> work in this scenario."
>>
>> 	But couldn't quite figure it out.
>> 	Thanks for any help provided.
>> 	Saptarshi
>>
>>
>> Saptarshi Guha | sapsi at pobox.com | http://www.stat.purdue.edu/~sguha
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Wed Mar  7 00:24:47 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 06 Mar 2007 18:24:47 -0500
Subject: [R] Stangle and annotate
In-Reply-To: <87lkia6mkt.fsf@stat.ufl.edu>
References: <87lkia6mkt.fsf@stat.ufl.edu>
Message-ID: <45EDF83F.2070608@stats.uwo.ca>

On 3/6/2007 5:20 PM, Brett Presnell wrote:
> How exactly should I go about turning off annotation when running
> Stangle (Rtangle) with "R CMD Stangle myfile.Rnw"?
> 
> Ideally I would like to be able to turn annotation off and on for
> individual code chunks, or maybe better, to annotate only named
> chunks.  Are either of these things easily done?
> 
> Is there some way to figure out this sort of thing without reading
> through the source code in Sweave.R?

I don't think R CMD Stangle has a way to pay attention to optional args 
to Stangle, but this works:

echo "Stangle('myfile.Rnw', annotate=F)" | Rterm --slave

Duncan Murdoch


From jasoncbarnhart at msn.com  Wed Mar  7 00:41:56 2007
From: jasoncbarnhart at msn.com (Jason Barnhart)
Date: Tue, 6 Mar 2007 16:41:56 -0700
Subject: [R] R and SAS proc format
References: <975945.33063.qm@web32812.mail.mud.yahoo.com>
Message-ID: <BAY116-DAV1674489964135C7770C0F9CF7B0@phx.gbl>


----- Original Message ----- 
From: "John Kane" <jrkrideau at yahoo.ca>
To: "lamack lamack" <lamac_k at hotmail.com>; <R-help at stat.math.ethz.ch>
Sent: Tuesday, March 06, 2007 2:13 PM
Subject: Re: [R] R and SAS proc format


>
> --- lamack lamack <lamac_k at hotmail.com> wrote:
>
>> Dear all, Is there an R equivalent to SAS's proc
>> format?
>
> What does the SAS PROC FORMAT do?

It formats or reformats data in the SAS system.

It looks this:

    proc format; value kanefmt 1='A' 2='B' 3='C' 4='X' 5='Throw me 
out';
    data temp; do i=1 to 10; kanevar=put(i,kanefmt.); output; end;
    proc print; run;

And produces this:

Obs     i      kanevar
  1     1    A
  2     2    B
  3     3    C
  4     4    X
  5     5    Throw me out
  6     6               6
  7     7               7
  8     8               8
  9     9               9
 10    10              10


But it is more robust than what is shown here.



>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sapsi at pobox.com  Wed Mar  7 00:55:10 2007
From: sapsi at pobox.com (Saptarshi Guha)
Date: Tue, 6 Mar 2007 18:55:10 -0500
Subject: [R] Waiting for Key input
Message-ID: <720F6D88-35CB-4D7C-9CE8-138898491BAD@pobox.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070306/dfca5f2a/attachment.pl 

From kubovy at virginia.edu  Wed Mar  7 01:29:12 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Tue, 6 Mar 2007 19:29:12 -0500
Subject: [R] Question
In-Reply-To: <2F01C2D1A2A44B41BD01DDE5F72E1CAB01A1F701@rngpew02.drn.ine.pt>
References: <2F01C2D1A2A44B41BD01DDE5F72E1CAB01A1F701@rngpew02.drn.ine.pt>
Message-ID: <E243B537-A9A8-4F46-8334-BE07C875E14E@virginia.edu>

> How can I evaluate two or more expressions to return two or more  
> columns?
>
> eval(expression(with(bd,Var1*100),with(bd,Var2*200)))
>
> The execution is always for the last expression. I can't execute  
> the two expressions at the same time. It is possible?

How about something like this

with(bd, c(Var1 * 100, Var2 * 200))
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From g.abraham at ms.unimelb.edu.au  Wed Mar  7 01:15:57 2007
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Wed, 07 Mar 2007 11:15:57 +1100
Subject: [R] Estimating parameters of 2 phase Coxian using optim
In-Reply-To: <AFB5EBB0-9A63-41D3-86A1-A1762C86A3A0@ed.ac.uk>
References: <C212DCAB.731%lhill07@qub.ac.uk>
	<AFB5EBB0-9A63-41D3-86A1-A1762C86A3A0@ed.ac.uk>
Message-ID: <45EE043D.8090102@ms.unimelb.edu.au>

Andy Fugard wrote:
> Hi There,
> 
> Perhaps the problem is the line
> 
>          loglik<-log(p %*% expm(Q * y(i)) %*% q)
> 
> You mention that y is a vector but here you're treating it as a  
> function.  Maybe try
> 
>          loglik<-log(p %*% expm(Q * y[i]) %*% q)
> 
> ?
> 
> Don't have a clue about the correctness of the contents of cox2.lik...
> 
> Andy
> 
> 
> On 6 Mar 2007, at 08:54, Laura Hill wrote:
> 
>> Hi,
>>
>> My name is Laura. I'm a PhD student at Queen's University Belfast  
>> and have
>> just started learning R. I was wondering if somebody could help me  
>> to see
>> where I am going wrong in my code for estimating the parameters  
>> [mu1, mu2,
>> lambda1] of a 2-phase Coxian Distribution.
>>
>> cox2.lik<-function(theta, y){
>>     mu1<-theta[1]
>>
>>     mu2<-theta[2]
>>
>>     lambda1<-theta[3]
>>
>>     p<-Matrix(c(1, 0), nrow=1, ncol=2)
>>
>>     Q<-Matrix(c(-(lambda1 + mu1), 0, lambda1, -mu2), nrow=2, ncol=2)
>>
>>     q<-Matrix(c(mu1, mu2), nrow=2, ncol=1)
>>
>>     for (i in 1:length(y)){
>>         loglik<-log(p %*% expm(Q * y(i)) %*% q)
>>     return(loglik)}
>>
>>     sumloglik<-sum(loglik)
>>
>>     return(-sumloglik)
>>     }

Just to add my 2 AU cents regarding the for loop:

You're trying to create a vector of log likelihoods to sum up later, but 
that's not what's happening there. Instead, assign an empty vector of 
same length as y, then assign the loglik from each iteration to a 
different cell.

Lastly, there's no need to return anything from a for loop, it's not a 
function.

HTH,
Gad

-- 
Gad Abraham
Department of Mathematics and Statistics
The University of Melbourne
Parkville 3010, Victoria, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From hassanysabbah at hotmail.com  Wed Mar  7 02:07:36 2007
From: hassanysabbah at hotmail.com (Harry Ho)
Date: Wed, 07 Mar 2007 01:07:36 +0000
Subject: [R] R plug in for Eclipse
In-Reply-To: <e7e02b7a0703061400o6dc203b3lf8b38b4edb013682@mail.gmail.com>
Message-ID: <BAY107-F10979483F60326DD7DE1EAAB7A0@phx.gbl>

Hi Dominik,

Yes, I am currently running

Version: 3.2.2
Build id: M20070212-1330


>From: "Dominik Holenstein" <dhantlr at googlemail.com>
>To: "Harry Ho" <hassanysabbah at hotmail.com>
>Subject: Re: [R] R plug in for Eclipse
>Date: Tue, 6 Mar 2007 23:00:29 +0100
>
>Hi,
>Are you using Eclipse 3.2?
>
>semanticum
>
>2007/3/6, Harry Ho <hassanysabbah at hotmail.com>:
>>Hello,
>>
>>Is there any R plug-in available for Eclipse other than StatET?
>>
>>StatET doesn't seem to work with the latest release of Eclipse properly,
>>i.e. syntax highlighting isn't enabled for R commands. I already contacted
>>the author some time ago, but have not yet received a response.
>>
>>Would also be great if somebody could tell me whether they have the same
>>problem. I already tried installing it on two different machines so I 
>>guess
>>it isn't a local problem, but you never know...
>>
>>
>>Thx a lot
>>
>>_________________________________________________________________
>>Sie suchen E-Mails, Dokumente oder Fotos? Die neue MSN Suche Toolbar mit
>>Windows-Desktopsuche liefert in sekundenschnelle Ergebnisse. Jetzt neu!
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>

_________________________________________________________________
Sie suchen E-Mails, Dokumente oder Fotos? Die neue MSN Suche Toolbar mit 
Windows-Desktopsuche liefert in sekundenschnelle Ergebnisse. Jetzt neu!


From marc_schwartz at comcast.net  Wed Mar  7 03:23:26 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 06 Mar 2007 20:23:26 -0600
Subject: [R] Off topic:Spam on R-help increase?
In-Reply-To: <40e66e0b0703061036h66f292bdr35f496d71bf93430@mail.gmail.com>
References: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
	<45EDA84F.60209@biostat.ku.dk>
	<40e66e0b0703061036h66f292bdr35f496d71bf93430@mail.gmail.com>
Message-ID: <1173234206.9019.1.camel@localhost.localdomain>

On Tue, 2007-03-06 at 12:36 -0600, Douglas Bates wrote:
> On 3/6/07, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> > Bert Gunter wrote:
> > > Folks:
> > >
> > > In the past 2 days I have seen a large increase of  spam getting into
> > > R-help. Are others experiencing this problem? If so, has there been some
> > > change to the spam filters on the R-servers? If not, is the problem on my
> > > end?
> > >
> > > Feel free to reply privately.
> > >
> > Martin Maechler is still walking about upside-down after the DSC, slated
> > to return on March 8 (plus presumably a day or two to recover from the
> > flight...).
> 
> Actually he is walking around sideways at present.  He's in Dubai, not
> New Zealand.

LOL....and hopefully having a great time.

It is at times like these, as Uwe noted, where it becomes clear the
extent and results of Martin's efforts in filtering this stuff from the
lists.

Martin deserves a hearty "Well Done!" for managing this great resource
in a superior fashion.

Regards,

Marc


From edd at debian.org  Wed Mar  7 03:55:39 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 6 Mar 2007 20:55:39 -0600
Subject: [R] Memory Limits in Ubuntu Linux
In-Reply-To: <45ED6208.10355.222DB1@davidkat.davidkatzconsulting.com>
References: <45ED6208.10355.222DB1@davidkat.davidkatzconsulting.com>
Message-ID: <17902.10667.516548.140958@basebud.nulle.part>


On 6 March 2007 at 12:43, davidkat at davidkatzconsulting.com wrote:
| I am an R user trying to get around the 2Gig memory limit in Windows, so 

The real limit on 32bit systems is a 3gb address space. R under Windows can
get there, see the R-Windows FAQ.

| here I am days later with a working Ubuntu, and R under Ubuntu. But - the 
| memory problems seem worse than ever. R code that worked under 
| windows fails, unable to allocate memory.

Well, maybe you had virtual memory enabled under Windows but not under
Ubuntu. Or maybe you had other memory-hungry applications up under Ubuntu.

There is only so much magic the OS can do.  You easiest remedy will be to
upgrade to 4gb.  And even 8gb can useful on 32bit system, despite the fact
that each individual address space can only max out at 3gb, as you may have
multi-core / multi-cpu systems that allow you to multitask better.  
 
| Which way to go?
| 
| 1) Try to install 64-bit version for bigger address space. Would this help?

Yes, but you'd probably would have to buy more ram to. The main advantage is
that your limit is now way above the 3gb -- and probably set by your hardware
or budget. Maybe it is as high as 16gb.

But again, on the _same_ box with the _same_ amount of ram that is already
constrained under 32bit, you will not see any improvement.  Rather the
opposite as the basic building block is now 8 bytes instead of 4, you will
need more memory for the same tasks.  No free lunch, as they say.

| 2) Recompile R to get bigger memory capability?

Nope. It's what you give your OS in terms of RAM what's binding here.

| 3) Any other suggestions? 

Different algorithms or approaches, tricks like the data.frame-in-sqlite or
biglm, ...

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From cressonim at nhlbi.nih.gov  Wed Mar  7 04:45:19 2007
From: cressonim at nhlbi.nih.gov (Cressoni, Massimo (NIH/NHLBI) [F])
Date: Tue, 6 Mar 2007 22:45:19 -0500
Subject: [R] Sigmoidal Fitting
Message-ID: <B0F504209244B14EA9A4C1DFB599B9224FFE28@NIHCESMLBX6.nih.gov>


I want to thank professor Tura for his help but the function in some cases fails to converge and I am not able to
understand the reason :

> GAS
[1] 0.8108649 1.0386906 1.1638837 2.4144286
> PRESSURE
[1]  0  5 15 45
>  model.1 <- nls(GAS_PER_G ~ SSlogis(PRESSURE, ASym, xmid, scal))
 
 model.1 <- nls(GAS_PER_G ~ SSlogis(PRESSURE, ASym, xmid, scal))
Error in nls(y ~ 1/(1 + exp((xmid - x)/scal)), data = xy, start = list(xmid = aux[1],  : 
	step factor 0.000488281 reduced below 'minFactor' of 0.000976562
> 

Massimo Cressoni


From dunn at usq.edu.au  Wed Mar  7 06:33:40 2007
From: dunn at usq.edu.au (Peter Dunn)
Date: Wed, 7 Mar 2007 15:33:40 +1000
Subject: [R] Sweave issue: quotes in verbatim-like output print incorrectly
Message-ID: <200703071533.40870.dunn@usq.edu.au>

Hi all

I love Sweave; use it all the time.

But I recently received a new computer, and ever since I
have had a problem I've never seen before.

For example, I place the following in my Snw file:

<<>>=
sms <- 
read.table("http://www.sci.usq.edu.au/staff/dunn/Datasets/applications/popular/smsspeed.dat",
header=TRUE)
attach(sms)

sms.lm <- lm( Time ~ Age*Phone, subset=(Age>30) )
summary(sms.lm)
@

Standard stuff.   The output appears in the corresponding LaTeX
file as it should, in a verbatim-like environment as it should. 

But since I have had this new machine, this line of output:

Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

appears in my resulting pdf document as

Signif. codes: 0 ^a?A?Y***^a?A?Z0.001 ^a?A?Y**^a?A?Z0.01 ^a?A?Y*^a
?A?Z0.05 ^a?A?Y.^a?A?Z0.1 ^a?A?Y^a?A?Z1

In short, every quote is replaced by garbage.  This makes my
output looks incredibly bad.  (This is true for all cases; the above
is the output from my example.)

I also imagine (hope!) there is a very simple fix.  Can anyone help me?

Documents which used to produce the correct output document
now do this, so it must be something to do with my machine 
set up, or R set up, rather than the documents themselves, I guess.

Any help appreciated.  I have no idea where to look for the solution
(the FAQ. manuals and mailing archives were no help that I could see;
happy to be corrected).

P.


> version
               _
platform       i486-pc-linux-gnu
arch           i486
os             linux-gnu
system         i486, linux-gnu
status         Patched
major          2
minor          4.0
year           2006
month          11
day            25
svn rev        39997
language       R
version.string R version 2.4.0 Patched (2006-11-25 r39997)

> sessionInfo()
R version 2.4.0 Patched (2006-11-25 r39997)
i486-pc-linux-gnu

locale:
LC_CTYPE=en_AU.UTF-8;LC_NUMERIC=C;LC_TIME=en_AU.UTF-8;LC_COLLATE=en_AU.UTF-8;LC_MONETARY=en_AU.UTF-8;LC_MESSAGES=en_AU.UTF-8;LC_PAPER=en_AU.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_AU.UTF-8;LC_IDENTIFICATION=C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
[7] "base"

-- 
Dr Peter Dunn  |  dunn <at> usq.edu.au
Faculty of Sciences, USQ; http://www.sci.usq.edu.au/staff/dunn
Aust. Centre for Sustainable Catchments: www.usq.edu.au/acsc

This email (including any attached files) is confidential an...{{dropped}}


From klaster at karlin.mff.cuni.cz  Wed Mar  7 06:46:41 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Wed, 07 Mar 2007 06:46:41 +0100
Subject: [R] Waiting for Key input
In-Reply-To: <720F6D88-35CB-4D7C-9CE8-138898491BAD@pobox.com>
References: <720F6D88-35CB-4D7C-9CE8-138898491BAD@pobox.com>
Message-ID: <45EE51C1.2000108@karlin.mff.cuni.cz>

?browser
?readline

Petr

Saptarshi Guha napsal(a):
> Hi,
> 	I have another question. Is there any command that waits for key input?
> 	I am running R on OS X, so getGraphicsEvents doesn't work.
> 	I have put some debug print messages in a function and would like to  
> pause the code every time the message is printed and continue on a  
> keypress.
> 
> 	Thank you
> 	Saptarshi
> 
> Saptarshi Guha | sapsi at pobox.com | http://www.stat.purdue.edu/~sguha
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From Matthias.Kohl at stamats.de  Wed Mar  7 07:06:03 2007
From: Matthias.Kohl at stamats.de (Matthias Kohl)
Date: Wed, 7 Mar 2007 07:06:03 +0100 (MET)
Subject: [R] Generic distributions
Message-ID: <200703070606.l27663KJ021567@post.webmailer.de>

Hello Alberto, hello Greg,

in distr you can do:

library(distr)
N <- Norm(mean = 1, sd = 2)
p(N)(0.5)
r(N)(100)

!!! not: p(N, 0.5) or r(N, 100) !!!
A detailed description of package "distr" is given in package "distrDoc".

library(distrDoc)
vignette("distr")

hth
Matthias



----- original message --------

Subject: Re: [R] Generic distributions
Sent: Tue, 06 Mar 2007
From: Greg Snow<Greg.Snow at intermountainmail.org>

> I think the distr package does this.  There are also packages that link
> to winbugs if that is what you really want to do.
> 
> -- 
> Gregory (Greg) L. Snow Ph.D.
> Statistical Data Center
> Intermountain Healthcare
> greg.snow at intermountainmail.org
> (801) 408-8111
>  
>  
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> > Alberto Monteiro
> > Sent: Tuesday, March 06, 2007 1:38 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Generic distributions
> > 
> > Is there any class that generalizes distributions?
> > 
> > For example, I could say
> > x <- generic_distribution("normal", list(mean=1, sigma=0.5)) 
> > and then use it like rgeneric_distribution(100, x) to get a 
> > sample of 100, or pgeneric_distribution(0.5, x) to get the 
> > pdf at (x = 0.5).
> > 
> > In the openbugs/winbugs package, that uses a language that 
> > looks like R/S, we can do things like x ~ dnorm(mu, tau), 
> > forget that x is a normal with mean mu and variance 1/tau, 
> > and then treat it generically.
> > 
> > Alberto Monteiro
> > 
> > PS: this is noise... but due to spam invasion, anything that 
> > increases the nonspam/spam ratio should be welcome :-)
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

--- original message end ----


From j.o.vik at bio.uio.no  Wed Mar  7 07:40:02 2007
From: j.o.vik at bio.uio.no (Jon Olav Vik)
Date: Wed, 7 Mar 2007 06:40:02 +0000 (UTC)
Subject: [R] Autogenerate tags in tag=value pairs for
Message-ID: <loom.20070307T070340-447@post.gmane.org>

Dear list,

Is there a way to programmatically specify tag names for the ... (ellipsis) 
part of the argument list to a function? In other words, a way to do this:

x <- data.frame(A=1:5)

if the name "A" was not hardcoded but given by a variable, and without 
resorting to:

x <- data.frame(1:5)
names(x) <- "A"


A longer example describing my actual problem follows. Thanks in advance for 
any help.

Best regards,
Jon Olav


I want to use function transformBy() in package doBy. The key is that the "... 
Further arguments of the form tag=value" require "tag" to be specified, 
otherwise the output does not include the results of my groupwise calculations.

Quoting the documentation:
" transformBy(doBy)
" Function to make groupwise transformations of data 
" by applying the transform function to subsets of data. 
" 
" Usage
" transformBy(formula, data, ...)
" 
" Arguments
" formula A formula with only a right hand side, see examples below 
" data A data frame 
" ... Further arguments of the form tag=value 

### example ###

# a function to replace NAs with the last non-NA value from above 
filldown <- function(x) {
    notna <- !is.na(x) # elements with values
    ix <- cumsum(notna) # index to previous element (but zeros where we need NA)
    ix[ix==0] <- NA # use [NA] as index to produce NA in output
    return(x[notna][ix]) # for each: return previous value if found, else NA
}
# illustration of how it works
tmp <- c(NA,NA,1,NA,3,NA,NA)
cbind(tmp,filldown(tmp))

# I now want to apply filldown() to subsets of a data frame
# and I want it to work on several columns

# generate a data frame for illustration, 
# with a few non-NA values scattered round
set.seed(5) # repeatable example
x <- data.frame(id = rep(1:4,each=6), v1=NA, v2=NA)
ix <- which(runif(nrow(x))>0.75)
x[ix,2] <- rpois(length(ix),5)
ix <- which(runif(nrow(x))>0.75)
x[ix,3] <- rpois(length(ix),5)
x

library(doBy)
# the hard way -- works as required, 
# but I would like not having to hardcode column names v1 etc.
transformBy(~id,data=x,v1.fd = filldown(v1),v2.fd = filldown(v2))

# does not work because
# output includes only columns explicitly mentioned in the ... argument
transformBy(~id,data=x,function(y) lapply(y,filldown))


From pinkgirl85236 at yahoo.com.cn  Wed Mar  7 08:08:05 2007
From: pinkgirl85236 at yahoo.com.cn (pinkgirl85236 at yahoo.com.cn)
Date: Wed, 7 Mar 2007 08:08:05 +0100
Subject: [R]
	=?ISO-2022-JP?B?i02V+4LNi9+PioLMg0mDb4NUg5OC8JX4gq+C3IK3gqk/ltyYX4FBiq6RU5azl7+CxYK3gUI==?=
Message-ID: <20070307160804.55782@yahoo.com.cn>


****************************************************
???T?C?g??20???`50???????????W?????????????l???E?n??
???W???????{???????s???R?~?
?j?e?B?[?T?C?g?????B
****************************************************

?s?????p?????????????????t-------------------------------

?T?@?????p???j???????s???????]?????????????????????????????B

?U?@?j???????????????v???????????s???????????W?????????B

?V?@?z?e???????????????????????????????????B


?s?????p???@?????????t-----------------------------------

?T?@?????]???n???E???[???A?h???X?E?p?X???[?h???????????????????B

?U?@???????P??????PR???????????????????B

?V?@?s???????????W?????]?????????????????????????????B

?W?@?????p???????????l?????????S??????PR?????g??????????
?@?@???A?h???X???\???????????????????????????_?????????A????
?@?@???????????????\???????A?T?C?g?????????[???????A????????
?@?@???????\???????????D?????A?????@?????????????????B

?y?????z????PR?????A???????\?????????l????????
?@?@?@?@???????????????????W???????????A???????\????
?@?@?@?@???????A???x?????????????v???????B

**************************************************************
???{???A???S???????n?????l???????????W?????]???????j????
?@http://bpopb.com:112/ddd/hito-ot-f-sp/

???{???A???S???????????????????????????~??????????
?@http://bpopb.com:112/ddd/hito-ot-f-sp/
**************************************************************
?????????????????????A???^?C?????????????m?F????????????
?@???o?C???i?g???d?b?j???????????I?X?X???v???????B
**************************************************************


From monireh.faramarzi at env.ethz.ch  Wed Mar  7 09:39:25 2007
From: monireh.faramarzi at env.ethz.ch (Faramarzi  Monireh)
Date: Wed, 7 Mar 2007 09:39:25 +0100
Subject: [R] How to open more windows to make more graphs at once!
Message-ID: <0E92048BF87ADA4899A9E48A167124A401C8E626@EX4.d.ethz.ch>


Dear R users,
I have a data frame (test) including five columns of upper (numeric), lower (numeric), observed (numeric), best_sim (numeric) and stname (factor with 80 levels, each level with different length). Now I would like to write a short program to draw one graph as follow for each level of stname but I would like also to draw each time 12 graphs for the 12 levels of stname in the same graphic windows and save it as "jpeg' file . This means at the end I will have 7 (80 levels/12=7) graphic windows and 7 jpeg files each one with 12 graphs (the last one with 8 graphs) for the 12 levels of stname. I already wrote the following script to do it each time for 12 levels of stname but I have to change script each time for the another 12 levels [line 3 in the script for example: for( i in levels(test$stname)[12:24))] and I do not know how can I save the obtained graphs (seven graphic windows) as jpeg files (e.g. plot1.jpeg, plot2.jpeg and so on). As I have 45 dataset like this it would be great if somebody can help me to complete this script to do all together for a dataset using a script.
Thank you very much in advance for your cooperation,
Monireh


      
windows(9,9)
par(mfrow = c(3,4))
for( i in levels(test$stname)[1:12])
{ 
data<- test[test$stname==i,]
xx <- c(1:length(data$upper), length(data$upper):1)
yy <- c(data$upper, rev(data$lower))
zz<- data$observed
tt<- data$Best_Sim
par(lab =c(10,15,2))
plot.jpeg<- plot(xx,yy, type="n", xlim=c(min(xx), max(xx)), ylim=c(min(zz,yy,tt), max(yy,zz,tt)*1.4),
 main= i, xlab="Month (1990-2002)",  ylab="Discharge(m3/s)", font.axis=6)
polygon(xx, yy, col="green",  border = "NA")
lines(zz, col="blue", lwd=1.5)
lines(tt,col="red", lwd=1.5) 
legend(length(zz)-60, max(yy,zz,tt)*1.45, c("Upper Limit", "Lower Limit", " Observed","Best etimation")
, lwd=c(10, 1,1.7,1.7), bty="n", col= c("green", "white", "blue","red"))
 }


From nicolas.mazziotta at swing.be  Wed Mar  7 10:11:16 2007
From: nicolas.mazziotta at swing.be (Nicolas Mazziotta)
Date: Wed, 7 Mar 2007 10:11:16 +0100
Subject: [R] How to open more windows to make more graphs at once!
In-Reply-To: <0E92048BF87ADA4899A9E48A167124A401C8E626@EX4.d.ethz.ch>
References: <0E92048BF87ADA4899A9E48A167124A401C8E626@EX4.d.ethz.ch>
Message-ID: <200703071011.16507.nicolas.mazziotta@swing.be>

Hello,

See
?Devices
?dev.print

e.g. 

X11() # opens a new screen device
plot(1:10) # plots points on the device
dev.print(jpeg, [args]) # prints from the X11 device to a jpeg file
dev.off() # close X11 device

or

jpeg([args]) # opens a jpeg device (file) for plotting
plot(1:10) # plots points on the device
dev.off() # closes connection to the jpeg device

Hope this helps.

Regards,


-- 
Nicolas Mazziotta

The contents of this e-mail, including any attachments, are ...{{dropped}}


From comtech.usa at gmail.com  Wed Mar  7 10:12:46 2007
From: comtech.usa at gmail.com (Michael)
Date: Wed, 7 Mar 2007 01:12:46 -0800
Subject: [R] where can I find Durbin-Waston test tables for Confidence Level
	2.5% or 0.5%?
Message-ID: <b1f16d9d0703070112t52fecaav9b42845a93dbc42@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/349607bb/attachment.ksh 

From Thierry.ONKELINX at inbo.be  Wed Mar  7 10:18:03 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 7 Mar 2007 10:18:03 +0100
Subject: [R] How to open more windows to make more graphs at once!
In-Reply-To: <0E92048BF87ADA4899A9E48A167124A401C8E626@EX4.d.ethz.ch>
Message-ID: <2E9C414912813E4EB981326983E0A10402AC72B7@inboexch.inbo.be>

Creating more than one graphic windows is, as far as I know, not
possible in R. But it's no problem to run a script which create multiply
jpeg's. See ?jpeg on how to create jpeg's.

Cheers,

Thierry

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

> -----Oorspronkelijk bericht-----
> Van: r-help-bounces op stat.math.ethz.ch [mailto:r-help-
> bounces op stat.math.ethz.ch] Namens Faramarzi Monireh
> Verzonden: woensdag 7 maart 2007 9:39
> Aan: r-help op stat.math.ethz.ch
> Onderwerp: [R] How to open more windows to make more graphs at once!
> 
> 
> Dear R users,
> I have a data frame (test) including five columns of upper (numeric),
> lower (numeric), observed (numeric), best_sim (numeric) and stname
(factor
> with 80 levels, each level with different length). Now I would like to
> write a short program to draw one graph as follow for each level of
stname
> but I would like also to draw each time 12 graphs for the 12 levels of
> stname in the same graphic windows and save it as "jpeg' file . This
means
> at the end I will have 7 (80 levels/12=7) graphic windows and 7 jpeg
files
> each one with 12 graphs (the last one with 8 graphs) for the 12 levels
of
> stname. I already wrote the following script to do it each time for 12
> levels of stname but I have to change script each time for the another
12
> levels [line 3 in the script for example: for( i in
> levels(test$stname)[12:24))] and I do not know how can I save the
obtained
> graphs (seven graphic windows) as jpeg files (e.g. plot1.jpeg,
plot2.jpeg
> and so on). As I have 45 dataset like this it would be gr!
>  eat if somebody can help me to complete this script to do all
together
> for a dataset using a script.
> Thank you very much in advance for your cooperation,
> Monireh
> 
> 
> 
> windows(9,9)
> par(mfrow = c(3,4))
> for( i in levels(test$stname)[1:12])
> {
> data<- test[test$stname==i,]
> xx <- c(1:length(data$upper), length(data$upper):1)
> yy <- c(data$upper, rev(data$lower))
> zz<- data$observed
> tt<- data$Best_Sim
> par(lab =c(10,15,2))
> plot.jpeg<- plot(xx,yy, type="n", xlim=c(min(xx), max(xx)),
> ylim=c(min(zz,yy,tt), max(yy,zz,tt)*1.4),
>  main= i, xlab="Month (1990-2002)",  ylab="Discharge(m3/s)",
font.axis=6)
> polygon(xx, yy, col="green",  border = "NA")
> lines(zz, col="blue", lwd=1.5)
> lines(tt,col="red", lwd=1.5)
> legend(length(zz)-60, max(yy,zz,tt)*1.45, c("Upper Limit", "Lower
Limit",
> " Observed","Best etimation")
> , lwd=c(10, 1,1.7,1.7), bty="n", col= c("green", "white",
"blue","red"))
>  }
> 
> ______________________________________________
> R-help op stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From xmeng at capitalbio.com  Wed Mar  7 10:47:46 2007
From: xmeng at capitalbio.com (XinMeng)
Date: Wed, 07 Mar 2007 17:47:46 +0800
Subject: [R] heatmap
Message-ID: <373260866.31327@capitalbio.com>

Hello sir;
I use the function "heatmap.2" to draw a heatmap of microarray data,which consists of logratios.

Q1
But the lengend shows the Z score and the corresponding color.But I want the legend to show the logratios and the corresponding color.How can I do it?

Q1
How can I control that cluster only applied to genes(rows) or samples(columns) via heatmap.2 function?Default is cluster applied to both rows and columns.

Thanks a lot!

My best!


From gavin.simpson at ucl.ac.uk  Wed Mar  7 10:25:08 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 07 Mar 2007 09:25:08 +0000
Subject: [R] How to open more windows to make more graphs at once!
In-Reply-To: <0E92048BF87ADA4899A9E48A167124A401C8E626@EX4.d.ethz.ch>
References: <0E92048BF87ADA4899A9E48A167124A401C8E626@EX4.d.ethz.ch>
Message-ID: <1173259508.3012.14.camel@dhcppc2.my.nat.localnet>

On Wed, 2007-03-07 at 09:39 +0100, Faramarzi Monireh wrote:
> Dear R users,
> I have a data frame (test) including five columns of upper (numeric),
> lower (numeric), observed (numeric), best_sim (numeric) and stname
> (factor with 80 levels, each level with different length). Now I would
> like to write a short program to draw one graph as follow for each
> level of stname but I would like also to draw each time 12 graphs for
> the 12 levels of stname in the same graphic windows and save it as
> "jpeg' file . This means at the end I will have 7 (80 levels/12=7)
> graphic windows and 7 jpeg files each one with 12 graphs (the last one
> with 8 graphs) for the 12 levels of stname. I already wrote the
> following script to do it each time for 12 levels of stname but I have
> to change script each time for the another 12 levels [line 3 in the
> script for example: for( i in levels(test$stname)[12:24))] and I do
> not know how can I save the obtained graphs (seven graphic windows) as
> jpeg files (e.g. plot1.jpeg, plot2.jpeg and so on). As I have 45
> dataset like this it would be gr!
>  eat if somebody can help me to complete this script to do all
> together for a dataset using a script.
> Thank you very much in advance for your cooperation,
> Monireh
> 

Hi Monireh,

I don't have your data set so I have generated some random data to
illustrate one approach to this.

## generate some data 
set.seed(1234)
dat <- data.frame(upper = rnorm(100), lower = rnorm(100), 
                  observed = rnorm(100), best_sim = rnorm(100), 
                  stname = factor(gl(5, 20), labels = letters[1:5]))

## because this is going to be called 45 times, I've wrapped it in a
## function, foo()
## Note the filename arg. It contains "%03d" which means that R will
## insert a number and produce many jpegs, varying by this number
## e.g. myplot1.jpeg, myplot2.jpeg - see ?jpeg.
## the "..." allow passing of arguments to jpeg
foo <- function(x, filename = "Rplot%03d.jpeg", ...) {
   ## start the jpeg device
   jpeg(filename = filename, ...)
   ## store the parameter defaults and set a 2 by 2 plot regions
   opar <- par(mfrow = c(2,2))
   ## this insures that the device is closed and defaults restored on
   ## function exit
   on.exit({dev.off(); par(opar)})
   ## set up a loop to go over the levels of your factor
   for(i in levels(x$stname)) {
      ## do the plotting - here you need to add the plot commands
      ## you really want to use - these are just examples.
      plot(lower ~ upper, data = x, subset = stname == i)
      ## this just adds a lowess line, I use with() to make it easier
      ## to read.
      with(x, lines(lowess(upper[stname == i], lower[stname == i]), 
           col = "red"))
   }
   invisible()
}

## to use the function on the demo data
## uses default filename
foo(dat)

## or passing arguments to jpeg()
foo(dat, width = 600, height = 600, pointsize = 10)

## or using your own file name
foo(dat, filename = "dataset1_%03d.jpeg", width = 600, height = 600,
pointsize = 10)

See ?jpeg to see why this works - the filename with "%03d" allows R to
produce several jpegs.

>       
> windows(9,9)
> par(mfrow = c(3,4))
> for( i in levels(test$stname)[1:12])
> { 
> data<- test[test$stname==i,]
> xx <- c(1:length(data$upper), length(data$upper):1)
> yy <- c(data$upper, rev(data$lower))
> zz<- data$observed
> tt<- data$Best_Sim
> par(lab =c(10,15,2))

In the line below, where you set the x- and y-limits, it would be
simpler and more readable to use range() instead of c(min(x), max(x) -
so your plot call could be:

plot.jpeg<- plot(xx,yy, type="n", xlim= range(xx),              
                 ylim=range(zz,yy,tt)*1.4), main= i, 
                 xlab="Month (1990-2002)",  
                 ylab="Discharge(m3/s)", 
                 font.axis=6)

Also, you can format the y-label more nicely with:

ylab = expression(paste("Discharge (", m^-3 * s^{-1}, ")"))

HTH

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT                          [w] http://www.freshwaters.org.uk/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From comtech.usa at gmail.com  Wed Mar  7 10:33:30 2007
From: comtech.usa at gmail.com (Michael)
Date: Wed, 7 Mar 2007 01:33:30 -0800
Subject: [R] good procedure to estimate ARMA(p, q)?
Message-ID: <b1f16d9d0703070133t5447f609k75a8f67beed3ac27@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/881680e3/attachment.ksh 

From gavin.simpson at ucl.ac.uk  Wed Mar  7 10:37:02 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 07 Mar 2007 09:37:02 +0000
Subject: [R] Sweave issue: quotes in verbatim-like output
	print	incorrectly
In-Reply-To: <200703071533.40870.dunn@usq.edu.au>
References: <200703071533.40870.dunn@usq.edu.au>
Message-ID: <1173260222.3012.17.camel@dhcppc2.my.nat.localnet>

On Wed, 2007-03-07 at 15:33 +1000, Peter Dunn wrote:
> Hi all
> 
> I love Sweave; use it all the time.
> 
> But I recently received a new computer, and ever since I
> have had a problem I've never seen before.
> 
> For example, I place the following in my Snw file:

Try this in the preamble of your Snw file:

\usepackage[utf8x]{inputenc}

(assuming you have the inputenc package installed and available). I'm
assuming you are now using a machine using UTF-8 for character
encodings. I used to get that output on my linux box (FC4 - 6) before I
added the above \usepackage statement.

HTH

G
> 
> <<>>=
> sms <- 
> read.table("http://www.sci.usq.edu.au/staff/dunn/Datasets/applications/popular/smsspeed.dat",
> header=TRUE)
> attach(sms)
> 
> sms.lm <- lm( Time ~ Age*Phone, subset=(Age>30) )
> summary(sms.lm)
> @
> 
> Standard stuff.   The output appears in the corresponding LaTeX
> file as it should, in a verbatim-like environment as it should. 
> 
> But since I have had this new machine, this line of output:
> 
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> appears in my resulting pdf document as
> 
> Signif. codes: 0 ^a?A?Y***^a?A?Z0.001 ^a?A?Y**^a?A?Z0.01 ^a?A?Y*^a
> ?A?Z0.05 ^a?A?Y.^a?A?Z0.1 ^a?A?Y^a?A?Z1
> 
> In short, every quote is replaced by garbage.  This makes my
> output looks incredibly bad.  (This is true for all cases; the above
> is the output from my example.)
> 
> I also imagine (hope!) there is a very simple fix.  Can anyone help me?
> 
> Documents which used to produce the correct output document
> now do this, so it must be something to do with my machine 
> set up, or R set up, rather than the documents themselves, I guess.
> 
> Any help appreciated.  I have no idea where to look for the solution
> (the FAQ. manuals and mailing archives were no help that I could see;
> happy to be corrected).
> 
> P.
> 
> 
> > version
>                _
> platform       i486-pc-linux-gnu
> arch           i486
> os             linux-gnu
> system         i486, linux-gnu
> status         Patched
> major          2
> minor          4.0
> year           2006
> month          11
> day            25
> svn rev        39997
> language       R
> version.string R version 2.4.0 Patched (2006-11-25 r39997)
> 
> > sessionInfo()
> R version 2.4.0 Patched (2006-11-25 r39997)
> i486-pc-linux-gnu
> 
> locale:
> LC_CTYPE=en_AU.UTF-8;LC_NUMERIC=C;LC_TIME=en_AU.UTF-8;LC_COLLATE=en_AU.UTF-8;LC_MONETARY=en_AU.UTF-8;LC_MESSAGES=en_AU.UTF-8;LC_PAPER=en_AU.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_AU.UTF-8;LC_IDENTIFICATION=C
> 
> attached base packages:
> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
> [7] "base"
> 
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT                          [w] http://www.freshwaters.org.uk/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From Achim.Zeileis at wu-wien.ac.at  Wed Mar  7 10:48:49 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 7 Mar 2007 10:48:49 +0100 (CET)
Subject: [R] where can I find Durbin-Waston test tables for Confidence
 Level 2.5% or 0.5%?
In-Reply-To: <b1f16d9d0703070112t52fecaav9b42845a93dbc42@mail.gmail.com>
References: <b1f16d9d0703070112t52fecaav9b42845a93dbc42@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0703071041570.5127@eowyn>

Michael:

> I am doing a two-sided DW test:
>
> H0: rho = 0
> H1: rho =/= 0
>
> My understanding is that most test statistics tables are one-sided. It's the
> way they created the table.

...because rho > 0 is the alternative of interest in most applications.

> So from online, by doing Googling, I found a bunch of DW tables for
> Confidence Level 5%.

Using tables for the DW test is difficult because it's distribution 
depends on the particular set of regressors used. The tables of DW just 
give upper and lower bounds.

Back when the DW test was suggested, tables was the only way to make 
application of the test feasible. Today, you would either use the exact 
combination of chi-square distributions or an asymptotic approximation 
(both implemented in dwtest() from "lmtest") or a bootstrap approximation 
(implemented in durbin.watson() from "car"). For 278 observations, the 
normal approximation should be sufficient.

hth,
Z


From diego.pettena at alice.it  Wed Mar  7 10:49:14 2007
From: diego.pettena at alice.it (diego pettena)
Date: Wed, 7 Mar 2007 10:49:14 +0100
Subject: [R] info about R diffusion
Message-ID: <000901c7609d$e0843080$1601a8c0@diego>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/916807d0/attachment.ksh 

From wraff at titus.u-strasbg.fr  Wed Mar  7 10:49:34 2007
From: wraff at titus.u-strasbg.fr (Wolfgang Raffelsberger)
Date: Wed, 07 Mar 2007 10:49:34 +0100
Subject: [R] Package RODBC
In-Reply-To: <B7B34444ECA41A41AC47DABA057CE7A2014069DB@webmail.cip.cgiar.org>
References: <B7B34444ECA41A41AC47DABA057CE7A2014069DB@webmail.cip.cgiar.org>
Message-ID: <45EE8AAE.6030801@igbmc.u-strasbg.fr>

Dear Alberto,

please note that special characters (eg a space character) in the Excel 
sheet names mess up the simple way of querying provided by sqlFetch.

If you have a regular case of all sheets like "Sheet1":

plan1 <- sqlFetch(channel,"Sheet1")   # should work


But if you have "Sheet 1" (& similar..)  you have to use the command 
sqlQuery(), which means that you have to write a proper SQL query as 2nd 
argument that follows proper SQL syntax (starting with "SELECT", 
etc...). If I wanted to combine this with sheet-names already read in 
variables/vectors I concatenate this into a single stringsimilar to your 
2nd code variant ... Of course you could also use grep() to search the 
position of a given sheet-name (the order of the sheets may be different 
that within Excel).

What you get with

plan1[,1]

depends on what you're reading.  In case that the 1st column is read as 
string, this is read by default as factor with n levels.  You can simply 
convert it using as.character() ...

Hope this helps,
Wolfgang

Mendiburu, Felipe (CIP) a ?crit :
> Dear Alberto,
>
> channel <- odbcConnectExcel("test.xls")
> name1 <- tables[1, "TABLE_NAME"] # the name1 is Sheet1$
> it must be: 
> name1 <- "Sheet1"
> plan1 <- sqlFetch(channel, name1) is ok
> or
> plan1 <- sqlFetch(channel, "Sheet1")
>
> Regards,
>
> Felipe
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Alberto Monteiro
> Sent: Tuesday, March 06, 2007 9:37 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Package RODBC
>
>
> I have some questions about the RODBC package.
>
>   library(RODBC)  # required for those who want to repeat these lines
>
> 1st, I noticed that the following sequence does not work:
>
>   channel <- odbcConnextExcel("test.xls")
>   tables <- sqlTables(channel) 
>   name1 <- tables[1, "TABLE_NAME"]  # this should be the name
>   plan1 <- sqlFetch(channel, name1)  # bang!
>   odbcClose(channel)
>
> However, I can circumvent this with:
>
>   channel <- odbcConnextExcel("test.xls")
>   tables <- sqlTables(channel) 
>   name1 <- tables[1, "TABLE_NAME"]  # this should be the name
>   plan1 <- sqlQuery(channel, sprintf("select * from [%s]", name1))  # ok
>   odbcClose(channel)
>
> 2nd, it seems that only "pure" strings (which are not links to
> strings) and numerical values are correctly fetched or selected.
> Is this a bug?
>
> 3rd, when do something like plan1[,1] a weird message about Levels
> appear. What is that?
>
> Alberto Monteiro
>
>
>   


-- 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . .

Wolfgang Raffelsberger, PhD
Laboratoire de BioInformatique et G?nomique Int?grative
IGBMC
1 rue Laurent Fries,  67404 Illkirch  Strasbourg,  France
Tel (+33) 388 65 3300         Fax (+33) 388 65 3276
wolfgang.raffelsberger at igbmc.u-strasbg.fr


From ccleland at optonline.net  Wed Mar  7 10:55:04 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 07 Mar 2007 04:55:04 -0500
Subject: [R] where can I find Durbin-Waston test tables for Confidence
 Level	2.5% or 0.5%?
In-Reply-To: <b1f16d9d0703070112t52fecaav9b42845a93dbc42@mail.gmail.com>
References: <b1f16d9d0703070112t52fecaav9b42845a93dbc42@mail.gmail.com>
Message-ID: <45EE8BF8.20107@optonline.net>

Michael wrote:
> Hi all,
> 
> I am doing a two-sided DW test:
> 
> H0: rho = 0
> H1: rho =/= 0
> 
> My understanding is that most test statistics tables are one-sided. It's the
> way they created the table.
> 
> So from online, by doing Googling, I found a bunch of DW tables for
> Confidence Level 5%.
> 
> Those tables can answer my two-sided question at 5x2 = 10% confidence level.
> 
> But what if I want two-sided test at 1% and 5% confidence level?
> 
> Then I need 0.5% and 2.5% tables on those one-sided table.
> 
> My sample size is 278, and the number of parameters is 2, these adds to the
> hardship of finding a good table...
> 
> Could anybody give me some pointers of two-sided DW tables or 1-sided DW
> table with 0.5% and 2.5% confidence levels?
> 
> Thanks a lot!
> 
> Moreover, I appreciate any pointers about electronic tables that I can use
> in programs, I want to implement DW test myself, but let the program
> searching a table automatically...

  Are you aware of the implementations of this test in the car, lmtest,
and fMultivar packages?

RSiteSearch("Durbin-Watson", restrict="function") finds those functions.

> Thanks a lot!
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code. 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ted.harding at nessie.mcc.ac.uk  Wed Mar  7 10:55:29 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 07 Mar 2007 09:55:29 -0000 (GMT)
Subject: [R] How to open more windows to make more graphs at once!
In-Reply-To: <2E9C414912813E4EB981326983E0A10402AC72B7@inboexch.inbo.be>
Message-ID: <XFMail.070307095529.ted.harding@nessie.mcc.ac.uk>

On 07-Mar-07 ONKELINX, Thierry wrote:
> Creating more than one graphic windows is, as far as I know, not
> possible in R.

It is if you are running R on Linux (in which the X Windowing
System -- or X -- is the standard graphical system). As it
says in "?X11":

  "This can only be done on machines that run X."

Then each call to X11() opens a new graphics window, so you
can have as many as you like.

And, using the "display" parameter in X11(), if you have monitors
for more than one machine on your desk you can "throw" each window
to a monitor of your choice, giving you multiple graphics windows
on multiple screens, all from the same run of R.

But, as to whether/to what extent X or equivalent is available for
MS Windows, that is another question on which I have no expertise.

Ted.

> But it's no problem to run a script which create
> multiply
> jpeg's. See ?jpeg on how to create jpeg's.
> 
> Cheers,
> 
> Thierry
> 
> ------------------------------------------------------------------------
> ----
> 
> ir. Thierry Onkelinx
> 
> Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
> and Forest
> 
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
> methodology and quality assurance
> 
> Gaverstraat 4
> 
> 9500 Geraardsbergen
> 
> Belgium
> 
> tel. + 32 54/436 185
> 
> Thierry.Onkelinx at inbo.be
> 
> www.inbo.be 
> 
>  
> 
> Do not put your faith in what statistics say until you have carefully
> considered what they do not say.  ~William W. Watt
> 
> A statistical analysis, properly conducted, is a delicate dissection of
> uncertainties, a surgery of suppositions. ~M.J.Moroney
> 
>> -----Oorspronkelijk bericht-----
>> Van: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
>> bounces at stat.math.ethz.ch] Namens Faramarzi Monireh
>> Verzonden: woensdag 7 maart 2007 9:39
>> Aan: r-help at stat.math.ethz.ch
>> Onderwerp: [R] How to open more windows to make more graphs at once!
>> 
>> 
>> Dear R users,
>> I have a data frame (test) including five columns of upper (numeric),
>> lower (numeric), observed (numeric), best_sim (numeric) and stname
> (factor
>> with 80 levels, each level with different length). Now I would like to
>> write a short program to draw one graph as follow for each level of
> stname
>> but I would like also to draw each time 12 graphs for the 12 levels of
>> stname in the same graphic windows and save it as "jpeg' file . This
> means
>> at the end I will have 7 (80 levels/12=7) graphic windows and 7 jpeg
> files
>> each one with 12 graphs (the last one with 8 graphs) for the 12 levels
> of
>> stname. I already wrote the following script to do it each time for 12
>> levels of stname but I have to change script each time for the another
> 12
>> levels [line 3 in the script for example: for( i in
>> levels(test$stname)[12:24))] and I do not know how can I save the
> obtained
>> graphs (seven graphic windows) as jpeg files (e.g. plot1.jpeg,
> plot2.jpeg
>> and so on). As I have 45 dataset like this it would be gr!
>>  eat if somebody can help me to complete this script to do all
> together
>> for a dataset using a script.
>> Thank you very much in advance for your cooperation,
>> Monireh
>> 
>> 
>> 
>> windows(9,9)
>> par(mfrow = c(3,4))
>> for( i in levels(test$stname)[1:12])
>> {
>> data<- test[test$stname==i,]
>> xx <- c(1:length(data$upper), length(data$upper):1)
>> yy <- c(data$upper, rev(data$lower))
>> zz<- data$observed
>> tt<- data$Best_Sim
>> par(lab =c(10,15,2))
>> plot.jpeg<- plot(xx,yy, type="n", xlim=c(min(xx), max(xx)),
>> ylim=c(min(zz,yy,tt), max(yy,zz,tt)*1.4),
>>  main= i, xlab="Month (1990-2002)",  ylab="Discharge(m3/s)",
> font.axis=6)
>> polygon(xx, yy, col="green",  border = "NA")
>> lines(zz, col="blue", lwd=1.5)
>> lines(tt,col="red", lwd=1.5)
>> legend(length(zz)-60, max(yy,zz,tt)*1.45, c("Upper Limit", "Lower
> Limit",
>> " Observed","Best etimation")
>> , lwd=c(10, 1,1.7,1.7), bty="n", col= c("green", "white",
> "blue","red"))
>>  }
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 07-Mar-07                                       Time: 09:55:12
------------------------------ XFMail ------------------------------


From groemping at tfh-berlin.de  Wed Mar  7 11:08:17 2007
From: groemping at tfh-berlin.de (=?ISO-8859-1?Q?Ulrike_Gr=F6mping?=)
Date: Wed, 7 Mar 2007 11:08:17 +0100
Subject: [R] R and SAS proc format
In-Reply-To: <45EDDEC0.6050808@vanderbilt.edu>
References: <BAY113-F230DB711DB8D84B2F3E043997B0@phx.gbl>
	<9340323.post@talk.nabble.com> <45EDDEC0.6050808@vanderbilt.edu>
Message-ID: <20070307095614.M54330@tfh-berlin.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/9ca67ca6/attachment.pl 

From patrick at pdrechsler.de  Wed Mar  7 12:15:34 2007
From: patrick at pdrechsler.de (Patrick Drechsler)
Date: Wed, 07 Mar 2007 12:15:34 +0100
Subject: [R] Sweave issue: quotes in verbatim-like output
	print	incorrectly
References: <200703071533.40870.dunn@usq.edu.au>
	<1173260222.3012.17.camel@dhcppc2.my.nat.localnet>
Message-ID: <873b4hs3s9.fsf@pdrechsler.de>

Gavin Simpson <gavin.simpson at ucl.ac.uk> writes:

> On Wed, 2007-03-07 at 15:33 +1000, Peter Dunn wrote:
>> But I recently received a new computer, and ever since I
>> have had a problem I've never seen before.
>> 
>> For example, I place the following in my Snw file:
>
> Try this in the preamble of your Snw file:
>
> \usepackage[utf8x]{inputenc}

\usepackage[utf8]{inputenc}

should suffic for this. Also, utf8 is newer than utf8x. So if you only
need the ISO 8859 subset of Unicode, utf8 should serve your needs.

HTH

Patrick
-- 
I never used a logarithm in my life, and could not undertake to
extract the square root of four without misgivings.
	(Georg Bernhard Shaw)


From albmont at centroin.com.br  Wed Mar  7 12:32:25 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Wed, 7 Mar 2007 09:32:25 -0200
Subject: [R] Generic distributions
In-Reply-To: <200703070606.l27663KJ021567@post.webmailer.de>
References: <200703070606.l27663KJ021567@post.webmailer.de>
Message-ID: <20070307112536.M84991@centroin.com.br>

Matthias Kohl wrote:
> 
> in distr you can do:
> 
> library(distr)
> N <- Norm(mean = 1, sd = 2)
> p(N)(0.5)
> r(N)(100)
> 
> !!! not: p(N, 0.5) or r(N, 100) !!!
> A detailed description of package "distr" is given in package "distrDoc".
> 
> library(distrDoc)
> vignette("distr")
> 
Thanks!!! This is almost perfect. It even has (some) arithmetics!!!

z1 <- Norm(mean = 1, sd= 0.6)
z2 <- Norm(mean = 2, sd= 0.8)
z1+z2

Distribution Object of Class: Norm
mean :  3 
sd :  1 
Warning message:
arithmetics on distributions are understood as operations on r.v.'s
see 'distrARITH()'; for switching off this warning see '?distroptions' in: 
print(object) 
 
Alberto Monteiro


From unwin at math.uni-augsburg.de  Wed Mar  7 12:42:37 2007
From: unwin at math.uni-augsburg.de (Antony Unwin)
Date: Wed, 7 Mar 2007 12:42:37 +0100
Subject: [R] Identifying points in a plot that have duplicate values
Message-ID: <34DCEC44-52EC-43B6-9DF6-C76543D4F4D1@math.uni-augsburg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/aa34c309/attachment.pl 

From albmont at centroin.com.br  Wed Mar  7 12:47:16 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Wed, 7 Mar 2007 09:47:16 -0200
Subject: [R] How to open more windows to make more graphs at once!
In-Reply-To: <XFMail.070307095529.ted.harding@nessie.mcc.ac.uk>
References: <2E9C414912813E4EB981326983E0A10402AC72B7@inboexch.inbo.be>
	<XFMail.070307095529.ted.harding@nessie.mcc.ac.uk>
Message-ID: <20070307114517.M47107@centroin.com.br>

Ted Harding wrote:
>
>> Creating more than one graphic windows is, as far as I know, not
>> possible in R.
>  
> But, as to whether/to what extent X or equivalent is available for
> MS Windows, that is another question on which I have no expertise.
> 
X11() seems to work for Windows XP.

Alberto Monteiro


From ibanez at bioef.org  Wed Mar  7 13:09:46 2007
From: ibanez at bioef.org (Berta)
Date: Wed, 7 Mar 2007 13:09:46 +0100
Subject: [R] anova applied to a lme object
References: <a3eae00d0604251207q48506e3dybe92253d0c681cc@mail.gmail.com>
	<7257E53B-5145-4BCD-B426-2D66845FD2D0@uiuc.edu>
Message-ID: <01a301c760b1$80caa2f0$6601a8c0@BIOEF.ORG>

Hi R-users,

when carrying out a multiple regression, say lm(y~x1+x2), we can use an 
anova of the regression with summary.aov(lm(y~x1+x2)), and afterwards 
evaluate the relative contribution of each variable using the global Sum of 
Sq of the regression and the Sum of Sq of the simple regression y~x1.

Now I would like to incorporate a random effect in the model, as some data 
correspond to the same region and others not:  mylme<- lme(y~x1+x2, random= 
~1|as.factor(region)). I would like to know, if possible, which is the 
contribution of each variable to the global variability. Using anova(mylme) 
produce an anova table (without the Sum of Sq column), but I am not sure how 
can I derive the contribution of each variable from it, or even whether it 
is nonsense to try, nor can I derive a measure of how much variability is 
left unexplained.

Sorry for the type of question, but I did not find a simple solution and 
some researchers I work with love to have relative contributions to global 
variability.

Thanks a lot in advance,

Berta



>


From joseph.wakeling at webdrake.net  Wed Mar  7 13:30:46 2007
From: joseph.wakeling at webdrake.net (Joseph Wakeling)
Date: Wed, 07 Mar 2007 12:30:46 +0000
Subject: [R] Multi-line plots with matrices in R
Message-ID: <45EEB076.6090505@webdrake.net>

Hello all,

I'm a new user of R, experienced with Octave/MATLAB and therefore
struggling a bit with the new syntax.

One of the easy things in Octave or MATLAB is to plot multiple lines or
 sets of points by using a matrix where either the columns or the rows
contain the y-values to be plotted.  Both packages automatically give
each line/points their own unique colour, character etc.

I'm wondering how I get the same functionality in R.  For example, if X
is a vector of x-values and Y is a matrix whose rows contain the
y-values, I can do,

apply(Y,1,lines,x=X)

... but of course everything is all in black, with the same type of line
or points.  I'd like each line to have its own unique colour and/or style.

Another thing I'd like clarification on is the ability to update an
existing plot.  For example if I do,

plot.window(xlim=c(0,100),ylim=c(0,1))

and then after plotting data decide I want ylim=c(0,0.5), how do I
update the graphic?  A new plot.window() command does nothing.

Many thanks,

    -- Joe


From petr.pikal at precheza.cz  Wed Mar  7 14:02:59 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 07 Mar 2007 14:02:59 +0100
Subject: [R] Multi-line plots with matrices in R
In-Reply-To: <45EEB076.6090505@webdrake.net>
Message-ID: <45EEC613.9947.E76964@localhost>

Hi

see matplot, matlines.

or use forbidden for cycle.

for (i in 1:n) lines(x,y[,i], col=i)

or if you want to use more colours use built in rainbow, topo.colors 
or generate your own set.

Regards
Petr


On 7 Mar 2007 at 12:30, Joseph Wakeling wrote:

Date sent:      	Wed, 07 Mar 2007 12:30:46 +0000
From:           	Joseph Wakeling <joseph.wakeling at webdrake.net>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Multi-line plots with matrices in R

> Hello all,
> 
> I'm a new user of R, experienced with Octave/MATLAB and therefore
> struggling a bit with the new syntax.
> 
> One of the easy things in Octave or MATLAB is to plot multiple lines
> or
>  sets of points by using a matrix where either the columns or the rows
> contain the y-values to be plotted.  Both packages automatically give
> each line/points their own unique colour, character etc.
> 
> I'm wondering how I get the same functionality in R.  For example, if
> X is a vector of x-values and Y is a matrix whose rows contain the
> y-values, I can do,
> 
> apply(Y,1,lines,x=X)
> 
> ... but of course everything is all in black, with the same type of
> line or points.  I'd like each line to have its own unique colour
> and/or style.
> 
> Another thing I'd like clarification on is the ability to update an
> existing plot.  For example if I do,
> 
> plot.window(xlim=c(0,100),ylim=c(0,1))
> 
> and then after plotting data decide I want ylim=c(0,0.5), how do I
> update the graphic?  A new plot.window() command does nothing.
> 
> Many thanks,
> 
>     -- Joe
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From gavin.simpson at ucl.ac.uk  Wed Mar  7 14:09:35 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 07 Mar 2007 13:09:35 +0000
Subject: [R] Multi-line plots with matrices in R
In-Reply-To: <45EEB076.6090505@webdrake.net>
References: <45EEB076.6090505@webdrake.net>
Message-ID: <1173272975.32174.14.camel@gsimpson.geog.ucl.ac.uk>

On Wed, 2007-03-07 at 12:30 +0000, Joseph Wakeling wrote:
> Hello all,
> 
> I'm a new user of R, experienced with Octave/MATLAB and therefore
> struggling a bit with the new syntax.
> 
> One of the easy things in Octave or MATLAB is to plot multiple lines or
>  sets of points by using a matrix where either the columns or the rows
> contain the y-values to be plotted.  Both packages automatically give
> each line/points their own unique colour, character etc.
> 
> I'm wondering how I get the same functionality in R.  For example, if X
> is a vector of x-values and Y is a matrix whose rows contain the
> y-values, I can do,
> 
> apply(Y,1,lines,x=X)

You want maplot here. See ?matplot  but here is an example:

## generate some data to use, a matrix of Y values
## and a vector of x indices.
mat <- matrix(runif(100), ncol = 5)
vec <- seq(1, 100, length = 20)

## plot it using matplot
matplot(vec, mat, type = "l") # type = "l" to get lines

There is also matlines() and matpoints() for adding lines and points to
existing plots.

> 
> ... but of course everything is all in black, with the same type of line
> or points.  I'd like each line to have its own unique colour and/or style.
> 
> Another thing I'd like clarification on is the ability to update an
> existing plot.  For example if I do,
> 
> plot.window(xlim=c(0,100),ylim=c(0,1))

Standard graphics in R are not modifiable after being plotted. You need
to re-plot. When plotting data, I rarely need plot.window. This is what
I would do:

x <- 1:100 * runif(100)
y <- seq(0,1, length = 100) * runif(100)

plot(x, y, xlim = c(0, 100), ylim = c(0, 1))

# now change the limits
plot(x, y, xlim = c(0, 100), ylim = c(0, 0.5))

> 
> and then after plotting data decide I want ylim=c(0,0.5), how do I
> update the graphic?  A new plot.window() command does nothing.

But it does:

opar <- par(mfrow = c(1,2))
plot(x, y, xlim = c(0, 100), ylim = c(0, 0.5))
plot(x, y, xlim = c(0, 100), ylim = c(0, 1))
plot.window(xlim = c(0, 100), ylim = c(0, 0.5))
points(x, y, col = "red")
par(opar)

The points on the left plot correspond exactly to the points in red on
the right plot. The axis limits have changed, but because the axes have
already been labelled, these are not updated. We can illustrate this by
adding axes to the top and right of that plot

opar <- par(mfrow = c(1,2), mar = c(5,4,4,4) + 0.1)
plot(x, y, xlim = c(0, 100), ylim = c(0, 0.5))
plot(x, y, xlim = c(0, 100), ylim = c(0, 1))
plot.window(xlim = c(0, 100), ylim = c(0, 0.5))
points(x, y, col = "red")
axis(3)
axis(4)
par(opar)

Note the changed axis range in the right-hand margin. The problem is
that you can't use plot.window to achieve what you want, not that
plot.window doesn't do anything.

> 
> Many thanks,
> 
>     -- Joe

HTH

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From F.MENDIBURU at CGIAR.ORG  Wed Mar  7 14:25:00 2007
From: F.MENDIBURU at CGIAR.ORG (Mendiburu, Felipe (CIP))
Date: Wed, 7 Mar 2007 08:25:00 -0500
Subject: [R] Package RODBC
Message-ID: <B7B34444ECA41A41AC47DABA057CE7A2014069DE@webmail.cip.cgiar.org>

Dear Alberto,

It is better to assign a name to an area of data and not to use 
the name of the sheet, because this can have graphs and other data. 
If you this interested can see:
http://tarwi.lamolina.edu.pe/~fmendiburu/Rsolutions.htm 

I hope that this also helps,
Felipe.

-----Original Message-----
From: Wolfgang Raffelsberger [mailto:wraff at titus.u-strasbg.fr]
Sent: Wednesday, March 07, 2007 4:50 AM
To: Mendiburu, Felipe (CIP)
Cc: Alberto Monteiro; r-help at stat.math.ethz.ch
Subject: Re: [R] Package RODBC


Dear Alberto,

please note that special characters (eg a space character) in the Excel 
sheet names mess up the simple way of querying provided by sqlFetch.

If you have a regular case of all sheets like "Sheet1":

plan1 <- sqlFetch(channel,"Sheet1")   # should work


But if you have "Sheet 1" (& similar..)  you have to use the command 
sqlQuery(), which means that you have to write a proper SQL query as 2nd 
argument that follows proper SQL syntax (starting with "SELECT", 
etc...). If I wanted to combine this with sheet-names already read in 
variables/vectors I concatenate this into a single stringsimilar to your 
2nd code variant ... Of course you could also use grep() to search the 
position of a given sheet-name (the order of the sheets may be different 
that within Excel).

What you get with

plan1[,1]

depends on what you're reading.  In case that the 1st column is read as 
string, this is read by default as factor with n levels.  You can simply 
convert it using as.character() ...

Hope this helps,
Wolfgang

Mendiburu, Felipe (CIP) a ?crit :
> Dear Alberto,
>
> channel <- odbcConnectExcel("test.xls")
> name1 <- tables[1, "TABLE_NAME"] # the name1 is Sheet1$
> it must be: 
> name1 <- "Sheet1"
> plan1 <- sqlFetch(channel, name1) is ok
> or
> plan1 <- sqlFetch(channel, "Sheet1")
>
> Regards,
>
> Felipe
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Alberto Monteiro
> Sent: Tuesday, March 06, 2007 9:37 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Package RODBC
>
>
> I have some questions about the RODBC package.
>
>   library(RODBC)  # required for those who want to repeat these lines
>
> 1st, I noticed that the following sequence does not work:
>
>   channel <- odbcConnextExcel("test.xls")
>   tables <- sqlTables(channel) 
>   name1 <- tables[1, "TABLE_NAME"]  # this should be the name
>   plan1 <- sqlFetch(channel, name1)  # bang!
>   odbcClose(channel)
>
> However, I can circumvent this with:
>
>   channel <- odbcConnextExcel("test.xls")
>   tables <- sqlTables(channel) 
>   name1 <- tables[1, "TABLE_NAME"]  # this should be the name
>   plan1 <- sqlQuery(channel, sprintf("select * from [%s]", name1))  # ok
>   odbcClose(channel)
>
> 2nd, it seems that only "pure" strings (which are not links to
> strings) and numerical values are correctly fetched or selected.
> Is this a bug?
>
> 3rd, when do something like plan1[,1] a weird message about Levels
> appear. What is that?
>
> Alberto Monteiro
>
>
>   


-- 

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. . . . .

Wolfgang Raffelsberger, PhD
Laboratoire de BioInformatique et G?nomique Int?grative
IGBMC
1 rue Laurent Fries,  67404 Illkirch  Strasbourg,  France
Tel (+33) 388 65 3300         Fax (+33) 388 65 3276
wolfgang.raffelsberger at igbmc.u-strasbg.fr


From roger.bos at us.rothschild.com  Wed Mar  7 14:26:20 2007
From: roger.bos at us.rothschild.com (Bos, Roger)
Date: Wed, 7 Mar 2007 08:26:20 -0500
Subject: [R] Memory Limits in Ubuntu Linux
In-Reply-To: <45ED7C96.13805.89E7C5@davidkat.davidkatzconsulting.com>
Message-ID: <D8C95B444AD6EE4AAD638D818A9CFD343A1EE0@RINNYCSE000.rth.ad.rothschild.com>

David,

I wouldn't give up on windows so fast.  Many people have gotten the 3Gb
switch to work. One used to have to modify the header of the Rgui.exe
program to use the switch, but now the binary comes ready for that, so
its really quite easy.  I would like to hear more about why its not
working for you.

As for Linux, I use FC5 for which there is a 64-bit binary.  But there
are also 64-bit binaries for other distros.  The 32-bit and 64-bit
binaries are in different directories, so you should have no trouble
telling them apart.  

I have heard good things about Ubuntu--mainly that its very easy to
use--but FC5 has been pretty easy to learn too and I use the KDE desktop
which gives me Kate as a text editor.  You can open a terminal window in
Kate to run R and set up a key like F10 to send the code from the editor
to R.  Its not quite as good as my Windows setup with Tinn-R, but almost
as good.

Thanks,

Roger


-----Original Message-----
From: davidkat at davidkatzconsulting.com
[mailto:davidkat at davidkatzconsulting.com] 
Sent: Tuesday, March 06, 2007 5:37 PM
To: Bos, Roger
Subject: RE: [R] Memory Limits in Ubuntu Linux

Thanks for your prompt reply!

The windows 3GB switch is quite problematic - it was not useable on my
machine, and there are comments about these problems around the net.
Thus, on to Linux. My machine has 4Gig, and some megabytes are grabbed
by my Asus motherboard, leaving some 3.56 Gig. 

So if I understand your suggestion, try the 64-bit version of Ubuntu
(based on Debian but I had better luck with the video part of the
install) and then use the corresponding image from CRAN. My fear is that
the CRAN Ubuntu version might be 32-bit - any idea how to find out
before I embark on another install? Which Linux do you have - you
described some significant success with getting large jobs to run.

And yes, I've worked hard to save memory by tweaking the code.


Thanks again.


On 6 Mar 2007 at 16:51, Bos, Roger wrote:

> David,
> 
> First of all, under Windows you can get about 3GB available to R by 
> using the /3Gb switch in your boot.ini file, assuming you have 4Gb of 
> memory installed on your windows machine.  Using that method, I have 
> seen the memory using of my R process get as big as 2.7Gb in task 
> manager.  What's important, of course, is contiguous space, as you 
> mentioned.  There, you may want to check your code closely and make 
> sure that its memory usage is as efficient as possible and you are 
> storing the minimal amount you need for each run.  If you don't need 
> an object for a while consider writing it to disk and reading it back
in later.
> 
> Second, AFAIK to get any benefit from more memory is Linux you have to

> go to the 64bit version.  I am a Linux newbie too, so I choose to use 
> one of the pre-compiled binaries available on CRAN.  In other words, 
> you shouldn't have to compile anything yourself.  How much memory do 
> you have on your Linux box?  I have 16Gb and I know I have ran stuff 
> that wouldn't run on my 4Gb windows box.
> 
> HTH,
> 
> Roger
> 
> 
> 
> 
> 
>  
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> davidkat at davidkatzconsulting.com
> Sent: Tuesday, March 06, 2007 3:44 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Memory Limits in Ubuntu Linux
> 
> I am an R user trying to get around the 2Gig memory limit in Windows, 
> so here I am days later with a working Ubuntu, and R under Ubuntu. But

> - the memory problems seem worse than ever. R code that worked under 
> windows fails, unable to allocate memory.
> 
> Searching around the web, it appears that the problem may be the 
> ability to find contguous memory for my big vectors, but a fresh boot 
> of Ubuntu does not help either.
> 
> Which way to go?
> 
> 1) Try to install 64-bit version for bigger address space. Would this 
> help? Is this workable for my Athlon 64 Dual-core? (the live cd seems 
> to work but I never got it to boot after a disk install, but then the 
> 386 version was no better until I learned more about Grub...I could 
> try again if this might solve the
> problem)
> 
> 2) Recompile R to get bigger memory capability? (I'll have to 
> cross-post to some R forums too) This will be a challenge for a Linux 
> newbie...like me.
> 
> 3) Any other suggestions? My goal is to create a bigger neural network

> than fits in my Windows R version.
> --
> David Katz
>  www.davidkatzconsulting.com
>    541 482-1137
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> **********************************************************************

> * This message is for the named person's use only. It may contain 
> confidential, proprietary or legally privileged information. No right 
> to confidential or privileged treatment of this message is waived or 
> lost by any error in transmission. If you have received this message 
> in error, please immediately notify the sender by e-mail, delete the 
> message and all copies from your system and destroy any hard copies. 
> You must not, directly or indirectly, use, disclose, distribute, print

> or copy any part of this message if you are not the intended 
> recipient.
> **********************************************************************
> 

--
David Katz
   www.davidkatzconsulting.com


********************************************************************** * 
This message is for the named person's use only. It may 
contain confidential, proprietary or legally privileged 
information. No right to confidential or privileged treatment 
of this message is waived or lost by any error in 
transmission. If you have received this message in error, 
please immediately notify the sender by e-mail, 
delete the message and all copies from your system and destroy 
any hard copies. You must not, directly or indirectly, use, 
disclose, distribute, print or copy any part of this message 
if you are not the intended recipient.


From f.harrell at vanderbilt.edu  Wed Mar  7 14:22:38 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 07 Mar 2007 07:22:38 -0600
Subject: [R] R and SAS proc format
In-Reply-To: <20070307095614.M54330@tfh-berlin.de>
References: <BAY113-F230DB711DB8D84B2F3E043997B0@phx.gbl>
	<9340323.post@talk.nabble.com> <45EDDEC0.6050808@vanderbilt.edu>
	<20070307095614.M54330@tfh-berlin.de>
Message-ID: <45EEBC9E.3020607@vanderbilt.edu>

Ulrike Gr?mping wrote:
> 
> 
>>>The down side to R's factor solution: 
> 
>>>The numerical values of factors are always 1 to number of levels. Thus, it
> 
>>>can be tough and requires great care to work with studies that have both
> 
>>>numerical values different from this and value labels. This situation is
> 
>>>currently not well-supported by R.
> 
>>>
> 
>>>Regards, Ulrike
> 
>>>
> 
>>>P.S.: I fully agree with Frank regarding the annoyance one sometimes
> 
>>>encounters with formats in SAS! 
> 
>  > You can add an attribute to a variable.  In the sas.get function in the
>  > Hmisc package for example, when importing SAS variables that have PROC
>  > FORMAT value labels, an attribute 'sas.codes' keeps the original codes;
>  > these can be retrieved using sas.codes(variable name).  This could be
>  > done outside the SAS import context also.
>  >
>  > Frank
>  > --
>  > Frank E Harrell Jr   Professor and Chair           School of Medicine
>  >                       Department of Biostatistics   Vanderbilt 
> University
> 
> Frank,
> 
> are these attributes preserved when merging or subsetting a data frame?
> Are they used in R packages other than Hmisc and Design (e.g. in a 
> simple table request)?

no; would need to add functions like those that are used by the Hmisc 
label or impute functions.  And they are not used outside Hmisc/Design. 
  In fact I have little need for them as I always find the final labels 
as the key to analysis.

> 
> If this is the case, my wishlist items 8658 and 8659 
> (http://bugs.r-project.org/cgi-bin/R/wishlist?id=8658;user=guest, 
> http://bugs.r-project.org/cgi-bin/R/wishlist?id=8659;user=guest) can be 
> closed.
> Otherwise, I maintain the opinion that there are workarounds but that R 
> is not satisfactorily able to handle this type of data.

R gives the framework for doing this elegantly but the user has an 
overhead of implementing new methods for such attributes.

Cheers
Frank

> 
> Regards, Ulrike
> 
> 
> *------- End of Original Message -------*


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From cristina_gomes_parisca at hotmail.com  Wed Mar  7 10:04:59 2007
From: cristina_gomes_parisca at hotmail.com (Cristina Gomes)
Date: Wed, 07 Mar 2007 10:04:59 +0100
Subject: [R] Appropriate error distribution
Message-ID: <BAY113-F56682238872DE76065EF0DE7A0@phx.gbl>

Hi,
My name is Cristina. I'm interested in studying which continuos predictor 
variables (such as grooming received, rank, etc.) affect grooming given, as 
well a continuos variable. I'm having problems finding an appropriate family 
distribution to fit the GLMM I'm doing. The response variable, grooming 
given, has many zeros which does not allow me to use a gamma distribution. I 
tried with a poisson but since I had to convert the data to integers I fear 
loosing a lot of information (and anyway my data is originally continuos and 
not counts). I found in the help archive that someone with a similar problem 
(continuos data and many zeros) was adviced to use Tweedie models. I don't 
know anything about this and wouldn't kow if this is appropriate in my case 
and possible to apply to generalized linear mixed models.
I'm wondering if anybody could provide me with any good insights on what 
distribution I could us and if, in the case of tweedie being a good option, 
if it can be used with GLMM's..
Thanks a lot,
Cristina.


From ggrothendieck at gmail.com  Wed Mar  7 14:41:23 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 7 Mar 2007 08:41:23 -0500
Subject: [R] Autogenerate tags in tag=value pairs for
In-Reply-To: <loom.20070307T070340-447@post.gmane.org>
References: <loom.20070307T070340-447@post.gmane.org>
Message-ID: <971536df0703070541q5b6feb04ycd5458bbc43ff0b9@mail.gmail.com>

Try na.locf from the zoo package and then use merge with specified suffixes:

library(zoo)
f <- function(x) {
   rownames(x) <- NULL
   merge(x, na.locf(x[-1], na.rm = FALSE), by = 0, suffixes = c("", ".by"))[-1]
}
do.call("rbind", by(x, x$id, f))


On 3/7/07, Jon Olav Vik <j.o.vik at bio.uio.no> wrote:
> Dear list,
>
> Is there a way to programmatically specify tag names for the ... (ellipsis)
> part of the argument list to a function? In other words, a way to do this:
>
> x <- data.frame(A=1:5)
>
> if the name "A" was not hardcoded but given by a variable, and without
> resorting to:
>
> x <- data.frame(1:5)
> names(x) <- "A"
>
>
> A longer example describing my actual problem follows. Thanks in advance for
> any help.
>
> Best regards,
> Jon Olav
>
>
> I want to use function transformBy() in package doBy. The key is that the "...
> Further arguments of the form tag=value" require "tag" to be specified,
> otherwise the output does not include the results of my groupwise calculations.
>
> Quoting the documentation:
> " transformBy(doBy)
> " Function to make groupwise transformations of data
> " by applying the transform function to subsets of data.
> "
> " Usage
> " transformBy(formula, data, ...)
> "
> " Arguments
> " formula A formula with only a right hand side, see examples below
> " data A data frame
> " ... Further arguments of the form tag=value
>
> ### example ###
>
> # a function to replace NAs with the last non-NA value from above
> filldown <- function(x) {
>    notna <- !is.na(x) # elements with values
>    ix <- cumsum(notna) # index to previous element (but zeros where we need NA)
>    ix[ix==0] <- NA # use [NA] as index to produce NA in output
>    return(x[notna][ix]) # for each: return previous value if found, else NA
> }
> # illustration of how it works
> tmp <- c(NA,NA,1,NA,3,NA,NA)
> cbind(tmp,filldown(tmp))
>
> # I now want to apply filldown() to subsets of a data frame
> # and I want it to work on several columns
>
> # generate a data frame for illustration,
> # with a few non-NA values scattered round
> set.seed(5) # repeatable example
> x <- data.frame(id = rep(1:4,each=6), v1=NA, v2=NA)
> ix <- which(runif(nrow(x))>0.75)
> x[ix,2] <- rpois(length(ix),5)
> ix <- which(runif(nrow(x))>0.75)
> x[ix,3] <- rpois(length(ix),5)
> x
>
> library(doBy)
> # the hard way -- works as required,
> # but I would like not having to hardcode column names v1 etc.
> transformBy(~id,data=x,v1.fd = filldown(v1),v2.fd = filldown(v2))
>
> # does not work because
> # output includes only columns explicitly mentioned in the ... argument
> transformBy(~id,data=x,function(y) lapply(y,filldown))
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jmacdon at med.umich.edu  Wed Mar  7 14:51:14 2007
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Wed, 07 Mar 2007 08:51:14 -0500
Subject: [R] How to open more windows to make more graphs at once!
In-Reply-To: <20070307114517.M47107@centroin.com.br>
References: <2E9C414912813E4EB981326983E0A10402AC72B7@inboexch.inbo.be>	<XFMail.070307095529.ted.harding@nessie.mcc.ac.uk>
	<20070307114517.M47107@centroin.com.br>
Message-ID: <45EEC352.9040701@med.umich.edu>

Alberto Monteiro wrote:
> Ted Harding wrote:
> 
>>>Creating more than one graphic windows is, as far as I know, not
>>>possible in R.
>>
>> 
>>But, as to whether/to what extent X or equivalent is available for
>>MS Windows, that is another question on which I have no expertise.
>>
> 
> X11() seems to work for Windows XP.

Although I believe the preferred method is windows().

Best,

Jim


> 
> Alberto Monteiro
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623


**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.


From P.Dalgaard at biostat.ku.dk  Wed Mar  7 15:13:36 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 07 Mar 2007 15:13:36 +0100
Subject: [R] R and SAS proc format
In-Reply-To: <BAY116-DAV1674489964135C7770C0F9CF7B0@phx.gbl>
References: <975945.33063.qm@web32812.mail.mud.yahoo.com>
	<BAY116-DAV1674489964135C7770C0F9CF7B0@phx.gbl>
Message-ID: <45EEC890.1010608@biostat.ku.dk>

Jason Barnhart wrote:
> ----- Original Message ----- 
> From: "John Kane" <jrkrideau at yahoo.ca>
> To: "lamack lamack" <lamac_k at hotmail.com>; <R-help at stat.math.ethz.ch>
> Sent: Tuesday, March 06, 2007 2:13 PM
> Subject: Re: [R] R and SAS proc format
>
>
>   
>> --- lamack lamack <lamac_k at hotmail.com> wrote:
>>
>>     
>>> Dear all, Is there an R equivalent to SAS's proc
>>> format?
>>>       
>> What does the SAS PROC FORMAT do?
>>     
>
> It formats or reformats data in the SAS system.
>   

Slightly more precisely: It creates user-defined formats, which are
subsequently associated with variables and used for reading, printing,
tabulating, and analyzing data. It is akin to R's factor()
constructions, but not quite. For one thing, SAS's formats are separate
entities - same format can be used for many variables, whereas R's
factors have the formatting coded as a part of the object. For related
reasons, a variable in SAS can have more distinct values than there are
value labesl for, etc. 
> It looks this:
>
>     proc format; value kanefmt 1='A' 2='B' 3='C' 4='X' 5='Throw me 
> out';
>     data temp; do i=1 to 10; kanevar=put(i,kanefmt.); output; end;
>     proc print; run;
>
> And produces this:
>
> Obs     i      kanevar
>   1     1    A
>   2     2    B
>   3     3    C
>   4     4    X
>   5     5    Throw me out
>   6     6               6
>   7     7               7
>   8     8               8
>   9     9               9
>  10    10              10
>
>
> But it is more robust than what is shown here.
>
>
>
>   
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From philip.gladwin at citigroup.com  Wed Mar  7 15:12:16 2007
From: philip.gladwin at citigroup.com (Gladwin, Philip [CIB-FI])
Date: Wed, 7 Mar 2007 14:12:16 -0000
Subject: [R] Calculating confidence limits on acf graphs
Message-ID: <07DFC883258B4E4ABA3DC11CF025F23203B4C4FB@Exukmb34.eur.nsroot.net>

Hello,
I was wondering if anybody could help me with this?

I have plotted an acf function for a time series and am very happy with it.
Now I am interested in calculating for myself the two values for the confidence
intervals that are plotted on the graph of the acf.

The confidence intervals do not appear to be returned from the acf function (is this true?).

So far I haven't managed to calculate them myself.  Can anybody help?

Phil,


From lagaravitoh at unal.edu.co  Wed Mar  7 15:47:47 2007
From: lagaravitoh at unal.edu.co (Luis Garavito)
Date: Wed, 7 Mar 2007 09:47:47 -0500
Subject: [R] I need some help
Message-ID: <62375bed0703070647t5b2ea4b0i1cb5bc63c6278d97@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/3bedeea8/attachment.pl 

From researchjj at gmail.com  Wed Mar  7 15:48:11 2007
From: researchjj at gmail.com (j.joshua thomas)
Date: Wed, 7 Mar 2007 22:48:11 +0800
Subject: [R] Fwd: Package-RODBC-MSACCESS
In-Reply-To: <b4485c4c0703070647j7653b5f3q755594f9130e544@mail.gmail.com>
References: <b4485c4c0703070647j7653b5f3q755594f9130e544@mail.gmail.com>
Message-ID: <b4485c4c0703070648ndf50c6ai36f36b5b15bb41e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/7ec43351/attachment.pl 

From Roger.Vallejo at ARS.USDA.GOV  Wed Mar  7 15:49:25 2007
From: Roger.Vallejo at ARS.USDA.GOV (Vallejo, Roger)
Date: Wed, 7 Mar 2007 09:49:25 -0500
Subject: [R] hopach
Message-ID: <F649B8DBD3DEAD4BB163AC4D9AC18B5F41BDD1@MD-MAIL-01.ARSNET.ARS.USDA.GOV>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/55b14a0d/attachment.pl 

From Erik.Meesters at wur.nl  Wed Mar  7 15:50:07 2007
From: Erik.Meesters at wur.nl (Meesters, Erik)
Date: Wed, 7 Mar 2007 15:50:07 +0100
Subject: [R] Power calculation for detecting linear trend
Message-ID: <055461107DE3B14AADEAA2D300D417F5668265@scomp0040.wurnet.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/924ff2bd/attachment.pl 

From researchjj at gmail.com  Wed Mar  7 15:52:32 2007
From: researchjj at gmail.com (j.joshua thomas)
Date: Wed, 7 Mar 2007 22:52:32 +0800
Subject: [R] rattle- MSACCESS database problem
Message-ID: <b4485c4c0703070652y44db649br931340456c08a6b4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/d2681336/attachment.pl 

From klaster at karlin.mff.cuni.cz  Wed Mar  7 15:55:02 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Wed, 07 Mar 2007 15:55:02 +0100
Subject: [R] Calculating confidence limits on acf graphs
In-Reply-To: <07DFC883258B4E4ABA3DC11CF025F23203B4C4FB@Exukmb34.eur.nsroot.net>
References: <07DFC883258B4E4ABA3DC11CF025F23203B4C4FB@Exukmb34.eur.nsroot.net>
Message-ID: <45EED246.8090702@karlin.mff.cuni.cz>

Hmm, this does not seem to be over-documented :-)
But try
?plot.acf
and
getAnywhere(plot.acf)

Then you can find in the code how the values are actually calculated.
Petr

Gladwin, Philip [CIB-FI] napsal(a):
> Hello,
> I was wondering if anybody could help me with this?
> 
> I have plotted an acf function for a time series and am very happy with it.
> Now I am interested in calculating for myself the two values for the confidence
> intervals that are plotted on the graph of the acf.
> 
> The confidence intervals do not appear to be returned from the acf function (is this true?).
> 
> So far I haven't managed to calculate them myself.  Can anybody help?
> 
> Phil,
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From cgb at datanalytics.com  Wed Mar  7 16:07:16 2007
From: cgb at datanalytics.com (Carlos J. Gil Bellosta )
Date: Wed, 7 Mar 2007 15:07:16 +0000
Subject: [R] R and SAS proc format
In-Reply-To: <45EEC890.1010608@biostat.ku.dk>
References: <975945.33063.qm@web32812.mail.mud.yahoo.com>
	<BAY116-DAV1674489964135C7770C0F9CF7B0@phx.gbl>
	<45EEC890.1010608@biostat.ku.dk>
Message-ID: <b028350f0703070707m232a0557w7469d9e95bf04e87@mail.gmail.com>

On 3/7/07, Peter Dalgaard <P.Dalgaard at biostat.ku.dk> wrote:
> Jason Barnhart wrote:
> > ----- Original Message -----
> > From: "John Kane" <jrkrideau at yahoo.ca>
> > To: "lamack lamack" <lamac_k at hotmail.com>; <R-help at stat.math.ethz.ch>
> > Sent: Tuesday, March 06, 2007 2:13 PM
> > Subject: Re: [R] R and SAS proc format
> >
> >
> >
> >> --- lamack lamack <lamac_k at hotmail.com> wrote:
> >>
> >>
> >>> Dear all, Is there an R equivalent to SAS's proc
> >>> format?
> >>>
> >> What does the SAS PROC FORMAT do?
> >>
> >
> > It formats or reformats data in the SAS system.
> >
>
> Slightly more precisely: It creates user-defined formats, which are
> subsequently associated with variables and used for reading, printing,
> tabulating, and analyzing data. It is akin to R's factor()
> constructions, but not quite. For one thing, SAS's formats are separate
> entities - same format can be used for many variables, whereas R's
> factors have the formatting coded as a part of the object. For related
> reasons, a variable in SAS can have more distinct values than there are
> value labesl for, etc.
> > It looks this:
> >
> >     proc format; value kanefmt 1='A' 2='B' 3='C' 4='X' 5='Throw me
> > out';
> >     data temp; do i=1 to 10; kanevar=put(i,kanefmt.); output; end;
> >     proc print; run;
> >
> > And produces this:
> >
> > Obs     i      kanevar
> >   1     1    A
> >   2     2    B
> >   3     3    C
> >   4     4    X
> >   5     5    Throw me out
> >   6     6               6
> >   7     7               7
> >   8     8               8
> >   9     9               9
> >  10    10              10
> >
> >
> > But it is more robust than what is shown here.
> >
> >
> >
> >
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Also, SAS formats are used as a (somewhat cumbersome) replacement for
"dictionary" data structures. Starting from SAS 9.1 (I believe), "hash
tables" can be used within data steps for the same purpose (albeit
still cumbersome).

In this regard, not only formats but also lists could be a replacement
for them. They can be used as a way to get key-value mappings.

These key-value mappings (I mean, these kind of data structures) are
very handy tools. I have used both factors and lists for some kind of
"ad hoc" replacement for these data structures. Hasn't anybody
considered the posibility of having these data structures implemented
in R in a much python-like or java-like touch and feel?

Regards,

Carlos J. Gil Bellosta
http://www.datanalytics.com


From jmb at mssl.ucl.ac.uk  Wed Mar  7 16:01:58 2007
From: jmb at mssl.ucl.ac.uk (Jenny Barnes)
Date: Wed, 7 Mar 2007 15:01:58 +0000 (GMT)
Subject: [R] compiling latest version of R
Message-ID: <200703071501.l27F1wFb008074@msslhb.mssl.ucl.ac.uk>

Dear R-help community,

I have had trouble in the past installing the latest version of R: we got the 
errors shown below (the computer specifications and version of R are below 
that). Does anybody have tips for compiling the latest version of R so that I 
can avoid these errors?

configure
make
...
...
...

f90: CODE: 0 WORDS, DATA: 0 WORDS
gcc -G -L/usr/local/lib -o stats.so init.o kmeans.o  ansari.o bandwidths.o
chisq
sim.o d2x2xk.o fexact.o kendall.o ks.o  line.o smooth.o  prho.o swilk.o 
ksmooth
.o loessc.o isoreg.o Srunmed.o Trunmed.o  dblcen.o distance.o
hclust-utils.o  nl
s.o  HoltWinters.o PPsum.o arima.o burg.o filter.o  mAR.o pacf.o starma.o
port.o
 family.o sbart.o bsplvd.o bvalue.o bvalus.o loessf.o ppr.o qsbart.o 
sgram.o si
nerp.o sslvrg.o stxwx.o  hclust.o kmns.o  eureka.o stl.o portsrc.o
-L../../../..
/lib -lRblas  -lg2c -lm -lgcc_s
mkdir ../../../../library/stats/libs
building package 'datasets'
mkdir ../../../library/datasets
mkdir ../../../library/datasets/R
mkdir ../../../library/datasets/data
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library
'/tmp/R-2.4.0/library/stats/libs/stats.so'
:
  ld.so.1: R: fatal: relocation error: file
/tmp/R-2.4.0/library/stats/libs/stat
s.so: symbol __i_abs: referenced symbol not found
Execution halted
*** Error code 1


These are my specifications:

platform       sparc-sun-solaris2.10     
arch           sparc                     
os             solaris2.10               
system         sparc, solaris2.10        
status                                   
major          2                         
minor          3.1                       
year           2006                      
month          06                        
day            01                        
svn rev        38247                     
language       R                         
version.string Version 2.3.1 (2006-06-01)


Many thanks for your time in reading this problem, I look forward to hearing 
your suggestions and advice,

Jenny


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jennifer Barnes
PhD student: long range drought prediction 
Climate Extremes Group
Department of Space and Climate Physics
University College London
Holmbury St Mary 
Dorking, Surrey, RH5 6NT
Tel: 01483 204149
Mob: 07916 139187
Web: http://climate.mssl.ucl.ac.uk


From roland.rproject at gmail.com  Wed Mar  7 16:25:42 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Wed, 7 Mar 2007 10:25:42 -0500
Subject: [R] Fwd: Package-RODBC-MSACCESS
In-Reply-To: <b4485c4c0703070648ndf50c6ai36f36b5b15bb41e@mail.gmail.com>
References: <b4485c4c0703070647j7653b5f3q755594f9130e544@mail.gmail.com>
	<b4485c4c0703070648ndf50c6ai36f36b5b15bb41e@mail.gmail.com>
Message-ID: <47c7c59e0703070725w29b5b4e3ga363dc71e22d3774@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/30b87a1a/attachment.pl 

From lamac_k at hotmail.com  Wed Mar  7 16:29:52 2007
From: lamac_k at hotmail.com (lamack lamack)
Date: Wed, 07 Mar 2007 15:29:52 +0000
Subject: [R] transform R function
Message-ID: <BAY113-F1237AC3940B3A722DE9A81997A0@phx.gbl>

Dear all, Why the transform function does not accept two statistics 
functions?

a = data.frame(matrix(rnorm(20),ncol=2))

transform(a,M.1=mean(X1),M.2=mean(X2)) # does not works

#while:

transform(a,M.1=mean(X1),M2=log(abs(X2))) #works

Best regards

JL

_________________________________________________________________
O Windows Live Spaces ? seu espa?o na internet com fotos (500 por m?s), blog 
e agora com rede social http://spaces.live.com/


From snunes at gmail.com  Wed Mar  7 16:35:59 2007
From: snunes at gmail.com (=?ISO-8859-1?Q?S=E9rgio_Nunes?=)
Date: Wed, 7 Mar 2007 15:35:59 +0000
Subject: [R] No years() function?
Message-ID: <4c817d530703070735l2001e569ubdd0057e17fccb37@mail.gmail.com>

Hi,

I'm trying to aggregate date values using the aggregate function. For example:

aggregate(data,by=list(weekdays(LM),months(LM)),FUN=length)

I would also like to aggregate by year but there seems to be no
years() function.
Should there be one? Is there any alternative choice?

Also, a hours() function would be great. Any tip on this?

Thanks in advance!
S?rgio Nunes


From P.Dalgaard at biostat.ku.dk  Wed Mar  7 16:58:39 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 07 Mar 2007 16:58:39 +0100
Subject: [R] Power calculation for detecting linear trend
In-Reply-To: <055461107DE3B14AADEAA2D300D417F5668265@scomp0040.wurnet.nl>
References: <055461107DE3B14AADEAA2D300D417F5668265@scomp0040.wurnet.nl>
Message-ID: <45EEE12F.7000701@biostat.ku.dk>

Meesters, Erik wrote:
> Dear people,
> I've a problem in doing a power calculation. In Fryer and Nicholson
> (1993), ICES J. mar. Sci. 50: 161-168 page 164 an example is given with
> the following characteristics
> T=5, points in time
> R=5, replicates
> Var.within=0.1
> q=10, a 10% increase per year
> The degrees of freedom for the test are calculated as Vl=T*R-2=23 and
> the non-centrality parameter Dl=4.54.
> Using this they get a power of 0.53, but the result that I'm getting is
> 0.05472242.
>
> I've tried this several ways in R, but I'm not able to come up with the
> same number. Am I doing something wrong in the calculation of the power?
> Here's my code:
>
>     T<-5
>     R<-5
>     sigmasq<-0.1
>     q<-10
>     Vl<-(T*R)-2
>     Dl<-(R*(T-1)*T*(T+1)/(12*sigmasq))*(log(1+(q/100)))^2 #Dl result is
> still similar
>
>     power.1<-1-pf(qf(.95,(T*R-2),1,ncp=0),(T*R-2),1,ncp=Dl)
>
> Thank you for any suggestions/help.
>   
I think your DF are upside-down:

> power.1<-1-pf(qf(.95,1,(T*R-2),ncp=0),1,(T*R-2),ncp=Dl)
> power.1
[1] 0.532651



-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From joseph.wakeling at webdrake.net  Wed Mar  7 16:11:44 2007
From: joseph.wakeling at webdrake.net (Joseph Wakeling)
Date: Wed, 07 Mar 2007 15:11:44 +0000
Subject: [R] Multi-line plots with matrices in R
In-Reply-To: <1173272975.32174.14.camel@gsimpson.geog.ucl.ac.uk>
References: <45EEB076.6090505@webdrake.net>
	<1173272975.32174.14.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <45EED630.8030106@webdrake.net>

Gavin Simpson wrote:
> You want maplot here. See ?matplot  but here is an example:

Great!  Thanks to you and Petr for pointing this out, it's exactly what
I wanted.  Petr's other suggestions look interesting and I'll explore
them at length later.

> Note the changed axis range in the right-hand margin. The problem is
> that you can't use plot.window to achieve what you want, not that
> plot.window doesn't do anything.

Ahhh, I see.  So, it does not affect what has already been plotted, but
affects how new material is inserted into the plot area.  Entering

plot.window(xlim=c(0,100),ylim=c(0,0.5))
axis(1)
axis(2)
plot.window(xlim=c(0,100),ylim=c(0,1))
axis(2)

... is instructive. :-)

So, _is_ there a command which will rearrange the existing plotted
items, including axes?  Or does R require that I have a good idea of the
space in which I want to plot from the start?

Oh, and a quick cosmetic query---I notice that the axes when created are
spaced apart somewhat so the axis lines do not meet at the plot origin.
 Is there a way to alter this so that the outline of the box, and the
extreme values of the axis, match up?

Thanks again,

    -- Joe


From jr_frrr at yahoo.de  Wed Mar  7 16:06:41 2007
From: jr_frrr at yahoo.de (=?ISO-8859-1?Q?Jos=E9?= Rafael Ferrer Paris)
Date: Wed, 07 Mar 2007 11:06:41 -0400
Subject: [R] How to open more windows to make more graphs at once!
In-Reply-To: <0E92048BF87ADA4899A9E48A167124A401C8E626@EX4.d.ethz.ch>
References: <0E92048BF87ADA4899A9E48A167124A401C8E626@EX4.d.ethz.ch>
Message-ID: <1173280001.8590.30.camel@localhost.localdomain>

Dear Monireh,
try using lattice:


library(lattice)
set.seed(1234)
dat <- data.frame(months=rep(1:10,80),upper = rnorm(800)+1, 
                  lower = rnorm(800)-1, 
                  observed = rnorm(800), best.sim = rnorm(800), 
                  stname = factor(gl(80, 10)))

jpeg(filename = "Rplot%03d.jpeg")
xyplot(best.sim+observed+lower+upper~months|stname,dat,
       layout=c(3,4),type="b",auto.key=T)
dev.off()

It should produce almost exactly what you want. Lattice is a very
powerful tool for creating multiple graphics. You can customize the
individual plots within the lattice using panel and prepanel functions,
take a look at the documentation of the library and the documentation of
xyplot and panel.xyplot. Lattice is a little bit more complex than
"normal" plots in R, so you would have to spend more time in learning
how to use its functionality, but it is worth trying.

have a lot of fun

JR


El mi?, 07-03-2007 a las 09:39 +0100, Faramarzi Monireh escribi?:
> Dear R users,
> I have a data frame (test) including five columns of upper (numeric), lower (numeric), observed (numeric), best_sim (numeric) and stname (factor with 80 levels, each level with different length). Now I would like to write a short program to draw one graph as follow for each level of stname but I would like also to draw each time 12 graphs for the 12 levels of stname in the same graphic windows and save it as "jpeg' file . This means at the end I will have 7 (80 levels/12=7) graphic windows and 7 jpeg files each one with 12 graphs (the last one with 8 graphs) for the 12 levels of stname. I already wrote the following script to do it each time for 12 levels of stname but I have to change script each time for the another 12 levels [line 3 in the script for example: for( i in levels(test$stname)[12:24))] and I do not know how can I save the obtained graphs (seven graphic windows) as jpeg files (e.g. plot1.jpeg, plot2.jpeg and so on). As I have 45 dataset like this it would be gr!
>  eat if somebody can help me to complete this script to do all together for a dataset using a script.
> Thank you very much in advance for your cooperation,
> Monireh
> 
> 
>       
> windows(9,9)
> par(mfrow = c(3,4))
> for( i in levels(test$stname)[1:12])
> { 
> data<- test[test$stname==i,]
> xx <- c(1:length(data$upper), length(data$upper):1)
> yy <- c(data$upper, rev(data$lower))
> zz<- data$observed
> tt<- data$Best_Sim
> par(lab =c(10,15,2))
> plot.jpeg<- plot(xx,yy, type="n", xlim=c(min(xx), max(xx)), ylim=c(min(zz,yy,tt), max(yy,zz,tt)*1.4),
>  main= i, xlab="Month (1990-2002)",  ylab="Discharge(m3/s)", font.axis=6)
> polygon(xx, yy, col="green",  border = "NA")
> lines(zz, col="blue", lwd=1.5)
> lines(tt,col="red", lwd=1.5) 
> legend(length(zz)-60, max(yy,zz,tt)*1.45, c("Upper Limit", "Lower Limit", " Observed","Best etimation")
> , lwd=c(10, 1,1.7,1.7), bty="n", col= c("green", "white", "blue","red"))
>  }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jr_frrr at yahoo.de  Wed Mar  7 17:16:13 2007
From: jr_frrr at yahoo.de (=?ISO-8859-1?Q?Jos=E9?= Rafael Ferrer Paris)
Date: Wed, 07 Mar 2007 12:16:13 -0400
Subject: [R] anova applied to a lme object
In-Reply-To: <01a301c760b1$80caa2f0$6601a8c0@BIOEF.ORG>
References: <a3eae00d0604251207q48506e3dybe92253d0c681cc@mail.gmail.com>
	<7257E53B-5145-4BCD-B426-2D66845FD2D0@uiuc.edu>
	<01a301c760b1$80caa2f0$6601a8c0@BIOEF.ORG>
Message-ID: <1173284173.8590.37.camel@localhost.localdomain>

The variances of the random effects and the residual variances are given
by the summary function. Maybe VarCorr or varcomp gives you the answer
you are looking for:

library(nlme)
library(ape)
?VarCorr
?ape

JR
El mi?, 07-03-2007 a las 13:09 +0100, Berta escribi?:
> Hi R-users,
> 
> when carrying out a multiple regression, say lm(y~x1+x2), we can use an 
> anova of the regression with summary.aov(lm(y~x1+x2)), and afterwards 
> evaluate the relative contribution of each variable using the global Sum of 
> Sq of the regression and the Sum of Sq of the simple regression y~x1.
> 
> Now I would like to incorporate a random effect in the model, as some data 
> correspond to the same region and others not:  mylme<- lme(y~x1+x2, random= 
> ~1|as.factor(region)). I would like to know, if possible, which is the 
> contribution of each variable to the global variability. Using anova(mylme) 
> produce an anova table (without the Sum of Sq column), but I am not sure how 
> can I derive the contribution of each variable from it, or even whether it 
> is nonsense to try, nor can I derive a measure of how much variability is 
> left unexplained.
> 
> Sorry for the type of question, but I did not find a simple solution and 
> some researchers I work with love to have relative contributions to global 
> variability.
> 
> Thanks a lot in advance,
> 
> Berta
> 
> 
> 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
Dipl.-Biol. JR Ferrer Paris
~~~~~~~~~~~~~~~~~~~~~~~~~~~
Laboratorio de Biolog?a de Organismos --- Centro de Ecolog?a
Instituto Venezolano de Investigaciones Cient?ficas (IVIC) 
Apdo. 21827, Caracas 1020-A 
Rep?blica Bolivariana de Venezuela

Tel: (+58-212) 504-1452
Fax: (+58-212) 504-1088

email: jferrer at ivic.ve
clave-gpg: 2C260A95


From aldi at wustl.edu  Wed Mar  7 17:21:06 2007
From: aldi at wustl.edu (Aldi Kraja)
Date: Wed, 07 Mar 2007 10:21:06 -0600
Subject: [R] Plotting a broken line?
Message-ID: <45EEE672.5040904@wustl.edu>

Hi,

Is there a smart way in the R graphs to create a line that is broken in 
intervals based on the indicator given below.
following is a small test graph

Location,indicator,otherinfo
1.2,1,2.2
2.5,1,2.5
3.7,1,2.3
20.1,2,4.3

22.5,2,5.2
25.0,2,3.4
27.3,2,2.2

35.1,3,3.4
37.0,3,7.2
38.0,3,6.1
40.1,3,5.4
52.9,3,3.3

Right now in the plot the line is continuous, but I would like to have 
it broken based on the indicator. If the line of the plot reaches the 
last observation of indicator=1 then the line needs to stop; the next 
line will start at location 22.5 and continue up top 27.3; the next line 
goes from 35.1 up to 52.9.

 > x<-read.table(file='c:\\aldi\\testgraph.csv',sep=',',header=T)
 > x
   Location indicator otherinfo
1       1.2         1       2.2
2       2.5         1       2.5
3       3.7         1       2.3
4      20.1         2       4.3
5      22.5         2       5.2
6      25.0         2       3.4
7      27.3         2       2.2
8      35.1         3       3.4
9      37.0         3       7.2
10     38.0         3       6.1
11     40.1         3       5.4
12     52.9         3       3.3

 > 
plot(x$Location,x$indicator,type='l',xlim=c(0,max(x$Location)),ylim=c(0,max(x$indicator,x$otherinfo)))
 > points(x$Location,x$otherinfo)

TIA,
Aldi

--


From gavin.simpson at ucl.ac.uk  Wed Mar  7 17:25:10 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 07 Mar 2007 16:25:10 +0000
Subject: [R] Multi-line plots with matrices in R
In-Reply-To: <45EED630.8030106@webdrake.net>
References: <45EEB076.6090505@webdrake.net>
	<1173272975.32174.14.camel@gsimpson.geog.ucl.ac.uk>
	<45EED630.8030106@webdrake.net>
Message-ID: <1173284710.32174.30.camel@gsimpson.geog.ucl.ac.uk>

On Wed, 2007-03-07 at 15:11 +0000, Joseph Wakeling wrote:
> Gavin Simpson wrote:
> > You want maplot here. See ?matplot  but here is an example:
> 
> Great!  Thanks to you and Petr for pointing this out, it's exactly what
> I wanted.  Petr's other suggestions look interesting and I'll explore
> them at length later.
> 
> > Note the changed axis range in the right-hand margin. The problem is
> > that you can't use plot.window to achieve what you want, not that
> > plot.window doesn't do anything.
> 
> Ahhh, I see.  So, it does not affect what has already been plotted, but
> affects how new material is inserted into the plot area.  Entering
> 
> plot.window(xlim=c(0,100),ylim=c(0,0.5))
> axis(1)
> axis(2)
> plot.window(xlim=c(0,100),ylim=c(0,1))
> axis(2)
> 
> ... is instructive. :-)
> 
> So, _is_ there a command which will rearrange the existing plotted
> items, including axes?  Or does R require that I have a good idea of the
> space in which I want to plot from the start?

Not with the standard R graphics - think of the graphics window as a
piece of paper and if you draw anything on it you have done so in
permanent ink. If something needs changing you need a new sheet of paper
and have to redraw the lot. Most people I know write their code in some
text editor and send (or copy paste) it into R. It is an easy matter to
edit one or two bits of your code to tweak the display and re-plot...

I think you can modify lattice graphics objects and just plot (print
really) them again - but again you are really redrawing the whole plot
from scratch. IIRC grid might be able to do some of what you are looking
for.

> 
> Oh, and a quick cosmetic query---I notice that the axes when created are
> spaced apart somewhat so the axis lines do not meet at the plot origin.
>  Is there a way to alter this so that the outline of the box, and the
> extreme values of the axis, match up?
> 

Look at ?par and xaxs and yaxs. E.g.

plot(1:10, xaxs = "i", yaxs = "i")

G

> Thanks again,
> 
>     -- Joe
> 
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From ghug2646 at postoffice.uri.edu  Wed Mar  7 17:28:47 2007
From: ghug2646 at postoffice.uri.edu (Gregory Hughes)
Date: Wed, 07 Mar 2007 16:28:47 +0000
Subject: [R] Two-way Unbalanced multiple sample ANOVA
Message-ID: <45EEE83F.1060108@postoffice.uri.edu>

Hello all,

I was wondering if anyone could help me formulate a Two-way ANOVA for 
unbalanced multiple sample data?

We have a new study method aimed to help students to study for tests 
using computers. (I am a computer scientists, hence my 
soon-to-be-apparent lack of statistical knowledge).

To test this study method we devised a user study where 30 participant 
attended 2 lectures, lecture1 and lecture2. Two test were created, test1 
and test2.

test1 corresponds to the material in lecture1 and test2 corresponds to 
the material in lecture2.

The 30 participants were split into two groups, group1 and group2.

group1 used our new study method to review for lecture1 and their 
existing study method to review the material from lecture2
group2 used our new study method to review for lecture2 and their 
existing study method to review the material from lecture1

Each group then took the two test.

This is a repeated measure experiment because we have 2 exam scores for 
each participant, one using our new method to study and one not using 
our new method to study.

The data is unbalanced because participants did not take the same test 
twice.

 From what I understand balanced data would look like
ID    TEST     SYSTEM     SCORE
1       1        1         80
1       1        0         70
1       2        1         90
1       2        0         95
2       1        1         70
2       1        0         75
2       2        1         80
2       2        0         75

But instead our data look like this:
ID    TEST     SYSTEM     SCORE
1       1        1         80
1       2        0         95
2       1        0         75
2       2        1         80

So participant 2 never took test1 using our system.

Anyway, I want to look to see if our new study method had an impact one 
test results. Also, I want to see if the test number had an impact on 
the exam results.

Here is some sample data:

------------
 >dataSet <- data.frame(
    particID=factor(c(1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8)),
    whichExam=factor(c(1,2,1,2,1,2,1,2,1,2,1,2,1,2,1,2)),
    studyMethod=factor(c(1,0,1,0,1,0,1,0,0,1,0,1,0,1,0,1)),
    score=c(90,80,75,70,70,58,73,68,69,87,68,79,80,80,99,95))
------------

 From what I have read this should be how to compute and ANOVA on this data:

------------
 > summary(aov(score~whichExam*studyMethod+Error(particID),data=dataSet))

Error: particID
                      Df  Sum Sq Mean Sq F value Pr(>F)
whichExam:studyMethod  1  333.06  333.06  1.8211 0.2259
Residuals              6 1097.38  182.90              

Error: Within
            Df  Sum Sq Mean Sq F value  Pr(>F) 
whichExam    1   3.062   3.062  0.1072 0.75445 
studyMethod  1 203.062 203.062  7.1094 0.03721 *
Residuals    6 171.375  28.562                 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

------------

Is this correct way do do an ANOVA test for this data?
 From what I can tell this means that the study method did have a 
statistically significant impact on the scores, is that correct? This 
also shows that it did not matter which test the subject took, meaning 
that the two test were equally difficult.


What exactly do the titles "Error ..." mean?
What are "Residuals"?

Can anyone recommend a good book on R which covers this information, all 
I can find are books on SPSS?


From ligges at statistik.uni-dortmund.de  Wed Mar  7 17:30:23 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 07 Mar 2007 17:30:23 +0100
Subject: [R] compiling latest version of R
In-Reply-To: <200703071501.l27F1wFb008074@msslhb.mssl.ucl.ac.uk>
References: <200703071501.l27F1wFb008074@msslhb.mssl.ucl.ac.uk>
Message-ID: <45EEE89F.4040304@statistik.uni-dortmund.de>



Jenny Barnes wrote:
> Dear R-help community,
> 
> I have had trouble in the past installing the latest version of R: we got the 
> errors shown below (the computer specifications and version of R are below 
> that). Does anybody have tips for compiling the latest version of R so that I 
> can avoid these errors?

1. 2.4.1 is recent, not 2.3.1 as shown at the bottom.
2. You have some R-2.4.0 in your path or set in your R_LIBS, remove that 
one first.

Uwe Ligges


> 
> configure
> make
> ...
> ...
> ...
> 
> f90: CODE: 0 WORDS, DATA: 0 WORDS
> gcc -G -L/usr/local/lib -o stats.so init.o kmeans.o  ansari.o bandwidths.o
> chisq
> sim.o d2x2xk.o fexact.o kendall.o ks.o  line.o smooth.o  prho.o swilk.o 
> ksmooth
> .o loessc.o isoreg.o Srunmed.o Trunmed.o  dblcen.o distance.o
> hclust-utils.o  nl
> s.o  HoltWinters.o PPsum.o arima.o burg.o filter.o  mAR.o pacf.o starma.o
> port.o
>  family.o sbart.o bsplvd.o bvalue.o bvalus.o loessf.o ppr.o qsbart.o 
> sgram.o si
> nerp.o sslvrg.o stxwx.o  hclust.o kmns.o  eureka.o stl.o portsrc.o
> -L../../../..
> /lib -lRblas  -lg2c -lm -lgcc_s
> mkdir ../../../../library/stats/libs
> building package 'datasets'
> mkdir ../../../library/datasets
> mkdir ../../../library/datasets/R
> mkdir ../../../library/datasets/data
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library
> '/tmp/R-2.4.0/library/stats/libs/stats.so'
> :
>   ld.so.1: R: fatal: relocation error: file
> /tmp/R-2.4.0/library/stats/libs/stat
> s.so: symbol __i_abs: referenced symbol not found
> Execution halted
> *** Error code 1
> 
> 
> These are my specifications:
> 
> platform       sparc-sun-solaris2.10     
> arch           sparc                     
> os             solaris2.10               
> system         sparc, solaris2.10        
> status                                   
> major          2                         
> minor          3.1                       
> year           2006                      
> month          06                        
> day            01                        
> svn rev        38247                     
> language       R                         
> version.string Version 2.3.1 (2006-06-01)
> 
> 
> Many thanks for your time in reading this problem, I look forward to hearing 
> your suggestions and advice,
> 
> Jenny
> 
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Jennifer Barnes
> PhD student: long range drought prediction 
> Climate Extremes Group
> Department of Space and Climate Physics
> University College London
> Holmbury St Mary 
> Dorking, Surrey, RH5 6NT
> Tel: 01483 204149
> Mob: 07916 139187
> Web: http://climate.mssl.ucl.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ibanez at bioef.org  Wed Mar  7 18:24:53 2007
From: ibanez at bioef.org (Berta)
Date: Wed, 7 Mar 2007 18:24:53 +0100
Subject: [R] anova applied to a lme object
References: <a3eae00d0604251207q48506e3dybe92253d0c681cc@mail.gmail.com>
	<7257E53B-5145-4BCD-B426-2D66845FD2D0@uiuc.edu>
	<01a301c760b1$80caa2f0$6601a8c0@BIOEF.ORG>
	<1173284173.8590.37.camel@localhost.localdomain>
Message-ID: <020f01c760dd$86450e60$6601a8c0@BIOEF.ORG>

Thanks Jos? Rafael, I will try with library(ape) (at the moment I cannot 
load it).

VarCorr gives  the variance estimates for the random effect and the error 
terms. However, what I am looking for is a measure of the explained 
proportion of variance, such as it is R2 in regression models, and more 
precisely, I am looking for a measure of the explained proprotion of 
variance of each of the variables considered (continuous variables and other 
with random slope). For example, Snijders and Bosker (2003) pg 102 dedicate 
a chapter in their book to  "how much does the multilevel model explain" 
(chapter 7) and derive formulaes for R_1 and R_2  (variance in the first and 
second level respectively). Things seem to get complicated when a slope 
random effect is included in the model, as in my case.  It seems that 
package HLM provides the necessary estimates.

I will have a look at library(ape), thanks for the suggestion.

The book I mention is: Snijders, TAB and Bosker RJ (2003). Multilevel 
Analysis. An introduction to basic and advanced multilevel modeling. SAGE, 
London.

Berta




----- Original Message ----- 
From: "Jos? Rafael Ferrer Paris" <jr_frrr at yahoo.de>
To: "Berta" <ibanez at bioef.org>
Cc: <r-help at stat.math.ethz.ch>
Sent: Wednesday, March 07, 2007 5:16 PM
Subject: Re: [R] anova applied to a lme object


> The variances of the random effects and the residual variances are given
> by the summary function. Maybe VarCorr or varcomp gives you the answer
> you are looking for:
>
> library(nlme)
> library(ape)
> ?VarCorr
> ?ape
>
> JR
> El mi?, 07-03-2007 a las 13:09 +0100, Berta escribi?:
>> Hi R-users,
>>
>> when carrying out a multiple regression, say lm(y~x1+x2), we can use an
>> anova of the regression with summary.aov(lm(y~x1+x2)), and afterwards
>> evaluate the relative contribution of each variable using the global Sum 
>> of
>> Sq of the regression and the Sum of Sq of the simple regression y~x1.
>>
>> Now I would like to incorporate a random effect in the model, as some 
>> data
>> correspond to the same region and others not:  mylme<- lme(y~x1+x2, 
>> random=
>> ~1|as.factor(region)). I would like to know, if possible, which is the
>> contribution of each variable to the global variability. Using 
>> anova(mylme)
>> produce an anova table (without the Sum of Sq column), but I am not sure 
>> how
>> can I derive the contribution of each variable from it, or even whether 
>> it
>> is nonsense to try, nor can I derive a measure of how much variability is
>> left unexplained.
>>
>> Sorry for the type of question, but I did not find a simple solution and
>> some researchers I work with love to have relative contributions to 
>> global
>> variability.
>>
>> Thanks a lot in advance,
>>
>> Berta
>>
>>
>>
>> >
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> -- 
> Dipl.-Biol. JR Ferrer Paris
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Laboratorio de Biolog?a de Organismos --- Centro de Ecolog?a
> Instituto Venezolano de Investigaciones Cient?ficas (IVIC)
> Apdo. 21827, Caracas 1020-A
> Rep?blica Bolivariana de Venezuela
>
> Tel: (+58-212) 504-1452
> Fax: (+58-212) 504-1088
>
> email: jferrer at ivic.ve
> clave-gpg: 2C260A95
>
>
>
> ___________________________________________________________
> Telefonate ohne weitere Kosten vom PC zum PC: http://messenger.yahoo.de
>
>
>
>


From davidkat at davidkatzconsulting.com  Wed Mar  7 18:27:28 2007
From: davidkat at davidkatzconsulting.com (davidkat at davidkatzconsulting.com)
Date: Wed, 07 Mar 2007 09:27:28 -0800
Subject: [R] Memory Limits in Ubuntu Linux
In-Reply-To: <D8C95B444AD6EE4AAD638D818A9CFD343A1EE0@RINNYCSE000.rth.ad.rothschild.com>
References: <45ED7C96.13805.89E7C5@davidkat.davidkatzconsulting.com>,
	<D8C95B444AD6EE4AAD638D818A9CFD343A1EE0@RINNYCSE000.rth.ad.rothschild.com>
Message-ID: <45EE8580.24643.494B486@davidkat.davidkatzconsulting.com>

Thanks for the tips, Roger.

fyi: When I added /3GB to the boot.ini, the resulting desktop was incomplete 
and locked - no chance to try starting R. Searching the web lead me to 
believe that this was possibly a dead-end, so I abandoned this effort. Any 
hints on getting this to work, anyone? 



On 7 Mar 2007 at 8:26, Bos, Roger wrote:

> David,
> 
> I wouldn't give up on windows so fast.  Many people have gotten the 3Gb
> switch to work. One used to have to modify the header of the Rgui.exe
> program to use the switch, but now the binary comes ready for that, so
> its really quite easy.  I would like to hear more about why its not
> working for you.
> 
> As for Linux, I use FC5 for which there is a 64-bit binary.  But there
> are also 64-bit binaries for other distros.  The 32-bit and 64-bit
> binaries are in different directories, so you should have no trouble
> telling them apart.  
> 
> I have heard good things about Ubuntu--mainly that its very easy to
> use--but FC5 has been pretty easy to learn too and I use the KDE desktop
> which gives me Kate as a text editor.  You can open a terminal window in
> Kate to run R and set up a key like F10 to send the code from the editor
> to R.  Its not quite as good as my Windows setup with Tinn-R, but almost
> as good.
> 
> Thanks,
> 
> Roger
> 
> 
> -----Original Message-----
> From: davidkat at davidkatzconsulting.com
> [mailto:davidkat at davidkatzconsulting.com] 
> Sent: Tuesday, March 06, 2007 5:37 PM
> To: Bos, Roger
> Subject: RE: [R] Memory Limits in Ubuntu Linux
> 
> Thanks for your prompt reply!
> 
> The windows 3GB switch is quite problematic - it was not useable on my
> machine, and there are comments about these problems around the net.
> Thus, on to Linux. My machine has 4Gig, and some megabytes are grabbed
> by my Asus motherboard, leaving some 3.56 Gig. 
> 
> So if I understand your suggestion, try the 64-bit version of Ubuntu
> (based on Debian but I had better luck with the video part of the
> install) and then use the corresponding image from CRAN. My fear is that
> the CRAN Ubuntu version might be 32-bit - any idea how to find out
> before I embark on another install? Which Linux do you have - you
> described some significant success with getting large jobs to run.
> 
> And yes, I've worked hard to save memory by tweaking the code.
> 
> 
> Thanks again.
> 
> 
> On 6 Mar 2007 at 16:51, Bos, Roger wrote:
> 
> > David,
> > 
> > First of all, under Windows you can get about 3GB available to R by 
> > using the /3Gb switch in your boot.ini file, assuming you have 4Gb of 
> > memory installed on your windows machine.  Using that method, I have 
> > seen the memory using of my R process get as big as 2.7Gb in task 
> > manager.  What's important, of course, is contiguous space, as you 
> > mentioned.  There, you may want to check your code closely and make 
> > sure that its memory usage is as efficient as possible and you are 
> > storing the minimal amount you need for each run.  If you don't need 
> > an object for a while consider writing it to disk and reading it back
> in later.
> > 
> > Second, AFAIK to get any benefit from more memory is Linux you have to
> 
> > go to the 64bit version.  I am a Linux newbie too, so I choose to use 
> > one of the pre-compiled binaries available on CRAN.  In other words, 
> > you shouldn't have to compile anything yourself.  How much memory do 
> > you have on your Linux box?  I have 16Gb and I know I have ran stuff 
> > that wouldn't run on my 4Gb windows box.
> > 
> > HTH,
> > 
> > Roger
> > 
> > 
> > 
> > 
> > 
> >  
> > 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> > davidkat at davidkatzconsulting.com
> > Sent: Tuesday, March 06, 2007 3:44 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Memory Limits in Ubuntu Linux
> > 
> > I am an R user trying to get around the 2Gig memory limit in Windows, 
> > so here I am days later with a working Ubuntu, and R under Ubuntu. But
> 
> > - the memory problems seem worse than ever. R code that worked under 
> > windows fails, unable to allocate memory.
> > 
> > Searching around the web, it appears that the problem may be the 
> > ability to find contguous memory for my big vectors, but a fresh boot 
> > of Ubuntu does not help either.
> > 
> > Which way to go?
> > 
> > 1) Try to install 64-bit version for bigger address space. Would this 
> > help? Is this workable for my Athlon 64 Dual-core? (the live cd seems 
> > to work but I never got it to boot after a disk install, but then the 
> > 386 version was no better until I learned more about Grub...I could 
> > try again if this might solve the
> > problem)
> > 
> > 2) Recompile R to get bigger memory capability? (I'll have to 
> > cross-post to some R forums too) This will be a challenge for a Linux 
> > newbie...like me.
> > 
> > 3) Any other suggestions? My goal is to create a bigger neural network
> 
> > than fits in my Windows R version.
> > --
> > David Katz
> >  www.davidkatzconsulting.com
> >    541 482-1137
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> > **********************************************************************
> 
> > * This message is for the named person's use only. It may contain 
> > confidential, proprietary or legally privileged information. No right 
> > to confidential or privileged treatment of this message is waived or 
> > lost by any error in transmission. If you have received this message 
> > in error, please immediately notify the sender by e-mail, delete the 
> > message and all copies from your system and destroy any hard copies. 
> > You must not, directly or indirectly, use, disclose, distribute, print
> 
> > or copy any part of this message if you are not the intended 
> > recipient.
> > **********************************************************************
> > 
> 
> --
> David Katz
>    www.davidkatzconsulting.com
> 
> 
> ********************************************************************** * 
> This message is for the named person's use only. It may 
> contain confidential, proprietary or legally privileged 
> information. No right to confidential or privileged treatment 
> of this message is waived or lost by any error in 
> transmission. If you have received this message in error, 
> please immediately notify the sender by e-mail, 
> delete the message and all copies from your system and destroy 
> any hard copies. You must not, directly or indirectly, use, 
> disclose, distribute, print or copy any part of this message 
> if you are not the intended recipient. 
> **********************************************************************
> 

-- 
David Katz
   www.davidkatzconsulting.com


From Greg.Snow at intermountainmail.org  Wed Mar  7 18:30:38 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Wed, 7 Mar 2007 10:30:38 -0700
Subject: [R] Plotting a broken line?
In-Reply-To: <45EEE672.5040904@wustl.edu>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB879D4F@LP-EXCHVS07.CO.IHC.COM>

If you insert an NA (or row of NA's) into the data at each place you
want a break (after indicator increases), then the regular plot with
type='l' will break the line for you.

Is this what you want?

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Aldi Kraja
> Sent: Wednesday, March 07, 2007 9:21 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Plotting a broken line?
> 
> Hi,
> 
> Is there a smart way in the R graphs to create a line that is 
> broken in intervals based on the indicator given below.
> following is a small test graph
> 
> Location,indicator,otherinfo
> 1.2,1,2.2
> 2.5,1,2.5
> 3.7,1,2.3
> 20.1,2,4.3
> 
> 22.5,2,5.2
> 25.0,2,3.4
> 27.3,2,2.2
> 
> 35.1,3,3.4
> 37.0,3,7.2
> 38.0,3,6.1
> 40.1,3,5.4
> 52.9,3,3.3
> 
> Right now in the plot the line is continuous, but I would 
> like to have it broken based on the indicator. If the line of 
> the plot reaches the last observation of indicator=1 then the 
> line needs to stop; the next line will start at location 22.5 
> and continue up top 27.3; the next line goes from 35.1 up to 52.9.
> 
>  > x<-read.table(file='c:\\aldi\\testgraph.csv',sep=',',header=T)
>  > x
>    Location indicator otherinfo
> 1       1.2         1       2.2
> 2       2.5         1       2.5
> 3       3.7         1       2.3
> 4      20.1         2       4.3
> 5      22.5         2       5.2
> 6      25.0         2       3.4
> 7      27.3         2       2.2
> 8      35.1         3       3.4
> 9      37.0         3       7.2
> 10     38.0         3       6.1
> 11     40.1         3       5.4
> 12     52.9         3       3.3
> 
>  >
> plot(x$Location,x$indicator,type='l',xlim=c(0,max(x$Location))
> ,ylim=c(0,max(x$indicator,x$otherinfo)))
>  > points(x$Location,x$otherinfo)
> 
> TIA,
> Aldi
> 
> --
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rsb at wsu.edu  Wed Mar  7 18:33:41 2007
From: rsb at wsu.edu (Bricklemyer, Ross S)
Date: Wed, 7 Mar 2007 09:33:41 -0800
Subject: [R] error installing packages
Message-ID: <2FC987BC0B90B24786CAF43DD3F5719C86B073@CRU105.cahe.ad.wsu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/bbd7d9d7/attachment.pl 

From petr.pikal at precheza.cz  Wed Mar  7 18:43:33 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 07 Mar 2007 18:43:33 +0100
Subject: [R] Plotting a broken line?
In-Reply-To: <45EEE672.5040904@wustl.edu>
Message-ID: <45EF07D5.8672.1E84744@localhost>

Hi

you shall probably cooperate with segments, so you need to extract 
start and end points for your lines e.g.

> x<-c(1:6, 10:15,20:25)
> y<-rep(c(1,2,3), each=6)
> plot(x,y, type="l")
> plot(x,y)
> 

segments(sapply(split(x,y), min),1:3, sapply(split(x,y),max),1:3)

Regards
Petr



On 7 Mar 2007 at 10:21, Aldi Kraja wrote:

Date sent:      	Wed, 07 Mar 2007 10:21:06 -0600
From:           	Aldi Kraja <aldi at wustl.edu>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Plotting a broken line?
Send reply to:  	aldi at wustl.edu
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> Hi,
> 
> Is there a smart way in the R graphs to create a line that is broken
> in intervals based on the indicator given below. following is a small
> test graph
> 
> Location,indicator,otherinfo
> 1.2,1,2.2
> 2.5,1,2.5
> 3.7,1,2.3
> 20.1,2,4.3
> 
> 22.5,2,5.2
> 25.0,2,3.4
> 27.3,2,2.2
> 
> 35.1,3,3.4
> 37.0,3,7.2
> 38.0,3,6.1
> 40.1,3,5.4
> 52.9,3,3.3
> 
> Right now in the plot the line is continuous, but I would like to have
> it broken based on the indicator. If the line of the plot reaches the
> last observation of indicator=1 then the line needs to stop; the next
> line will start at location 22.5 and continue up top 27.3; the next
> line goes from 35.1 up to 52.9.
> 
>  > x<-read.table(file='c:\\aldi\\testgraph.csv',sep=',',header=T) > x
>    Location indicator otherinfo
> 1       1.2         1       2.2
> 2       2.5         1       2.5
> 3       3.7         1       2.3
> 4      20.1         2       4.3
> 5      22.5         2       5.2
> 6      25.0         2       3.4
> 7      27.3         2       2.2
> 8      35.1         3       3.4
> 9      37.0         3       7.2
> 10     38.0         3       6.1
> 11     40.1         3       5.4
> 12     52.9         3       3.3
> 
>  > 
> plot(x$Location,x$indicator,type='l',xlim=c(0,max(x$Location)),ylim=c(
> 0,max(x$indicator,x$otherinfo)))
>  > points(x$Location,x$otherinfo)
> 
> TIA,
> Aldi
> 
> --
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From istazahn at gmail.com  Wed Mar  7 18:59:34 2007
From: istazahn at gmail.com (Ista Zahn)
Date: Wed, 7 Mar 2007 12:59:34 -0500
Subject: [R] No fit statistics for some models using sem
Message-ID: <60647AC4-8AC3-41DF-BF81-3E463416195C@gmail.com>

Hi,

New to both R and SEM, so this may be a very simple question. I am  
trying to run a very simple path analysis using the sem package.  
There are 2 exogenous (FARSCH, LOCUS10) and 2 endogenous (T_ATTENT,  
RMTEST) observed variables in the model.  The idea is that T_ATTENT  
mediates the effect of FARSCH and LOCUS10 on RMTEST. The RAM  
specification I used is

FARSCH -> T_ATTENT, y1x1, NA
LOCUS10 -> T_ATTENT, y1x2, NA
FARSCH -> RMTEST10, y2x1, NA
LOCUS10 -> RMTEST10, y2x2, NA
T_ATTENT -> RMTEST10, y2y1, NA
FARSCH <-> FARSCH, x1x1, NA
LOCUS10 <-> LOCUS10, x2x2, NA
T_ATTENT <-> T_ATTENT, y1y1, NA
RMTEST10 <-> RMTEST10, y2y2, NA
LOCUS10 <-> FARSCH, x2x1, NA

This model runs, but using the summary function does not return the  
usual model fit statistics, only the following:

Model Chisquare =  0   Df =  0 Pr(>Chisq) = NA
  Chisquare (null model) =  8526.8   Df =  6
  Goodness-of-fit index =  1
  BIC =  0

If I omit the last line from the RAM specification(i.e., delete  
"LOCUS10 <-> FARSCH, x2x1, NA"), I DO get all the usual statistics:

  Model Chisquare =  1303.7   Df =  1 Pr(>Chisq) = 0
  Chisquare (null model) =  8526.8   Df =  6
  Goodness-of-fit index =  0.95864
  Adjusted goodness-of-fit index =  0.58639
  RMSEA index =  0.30029   90% CI: (NA, NA)
  Bentler-Bonnett NFI =  0.84711
  Tucker-Lewis NNFI =  0.082726
  Bentler CFI =  0.84712
  BIC =  1294.1

My understanding is the you should always put in the correlation  
between exogenous predictors, but when I do this I don't get fit  
statistics. Can anyone help me understand what is happening here?

Thank you,

Ista


From adam_6242 at yahoo.com  Wed Mar  7 19:04:19 2007
From: adam_6242 at yahoo.com (aat)
Date: Wed, 7 Mar 2007 10:04:19 -0800 (PST)
Subject: [R] Fitting Data to tCopula
Message-ID: <9358456.post@talk.nabble.com>


Hello,

Has anyone successfully fit empirical data to a tCopula using the fitCopula
function?  If so, are there ways to pick intelligent starting values to
avoid the errors such as a minor matrix not being positive definite and the
initial value of 'vmmin' not  being finite?

I've been able to fit a normal Copula to my data, but am having diffuculty
with the tCopula.  Any suggestions are greatly appreciated.  

Thank you.

Adam
  
-- 
View this message in context: http://www.nabble.com/Fitting-Data-to-tCopula-tf3363801.html#a9358456
Sent from the R help mailing list archive at Nabble.com.


From kubovy at virginia.edu  Wed Mar  7 19:30:11 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Wed, 7 Mar 2007 13:30:11 -0500
Subject: [R] Failure to run mcsamp() in package arm
Message-ID: <29AE7E35-7766-47D9-8EDC-7AB37A37A9D9@virginia.edu>

Dear r-helpers,

I can run the examples on the mcsamp help page. For example:
****************************************
 > M1 <- lmer (y1 ~ x + (1|group))
 > (M1.sim <- mcsamp (M1))
fit using lmer,
3 chains, each with 1000 iterations (first 500 discarded)
n.sims = 1500 iterations saved
                           mean  sd 2.5%  25%  50%  75% 97.5% Rhat n.eff
beta.(Intercept)           0.1 0.7 -1.2 -0.3  0.1  0.5   1.4  1.0  1500
beta.x                     2.5 0.4  1.7  2.2  2.5  2.7   3.2  1.0  1500
sigma.y                    3.8 0.3  3.3  3.6  3.7  3.9   4.3  1.0    61
sigma.grop.(In)            1.5 0.8  0.0  1.0  1.4  1.9   3.3  1.4    12
eta.group.(Intercept)[1]   0.0 1.0 -2.1 -0.5  0.0  0.6   2.0  1.0  1500
eta.group.(Intercept)[2]   1.0 1.1 -0.9  0.2  0.9  1.7   3.4  1.0    59
eta.group.(Intercept)[3]  -1.3 1.2 -4.0 -2.0 -1.3 -0.4   0.5  1.0    66
eta.group.(Intercept)[4]   1.3 1.1 -0.6  0.4  1.1  2.0   3.7  1.1    43
eta.group.(Intercept)[5]  -0.7 1.0 -3.0 -1.4 -0.6  0.0   1.2  1.0   120
eta.group.(Intercept)[6]   1.5 1.2 -0.3  0.6  1.4  2.2   4.0  1.0    49
eta.group.(Intercept)[7]   0.3 1.0 -1.7 -0.3  0.1  0.8   2.5  1.0   440
eta.group.(Intercept)[8]  -1.6 1.2 -4.0 -2.4 -1.5 -0.6   0.3  1.1    41
eta.group.(Intercept)[9]   0.4 1.0 -1.6 -0.2  0.2  0.9   2.7  1.0   180
eta.group.(Intercept)[10] -1.0 1.1 -3.3 -1.6 -0.9 -0.2   0.8  1.0    86

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence,  
Rhat=1).
****************************************
But when I try to do this with my own data I get an error:
****************************************
 > display(e7.lmer2)
lmer(formula = baLO ~ I(baRatio - 0.985) + delta + (1 + I(baRatio -   
0.985) + delta | subject), data = e7)
                    coef.est coef.se
(Intercept)        -0.19     0.06
I(baRatio - 0.985) -4.95     0.74
delta               0.41     0.06
Error terms:
Groups   Name               Std.Dev. Corr
subject  (Intercept)        0.13
           I(baRatio - 0.985) 2.57      0.45
           delta              0.22     -0.12 -0.94
Residual                    0.39
number of obs: 494, groups: subject, 13
deviance = 551.4

 > e7.sim <- mcsamp(e7.lmer2)
Error in as.bugs.array(sims, program = "lmer", n.iter = n.iter,  
n.burnin = n.burnin,  :
	error in parameter sigma. in parameters.to.save
****************************************
I would appreciate a pointer to what the problem might be.
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From p.dalgaard at biostat.ku.dk  Wed Mar  7 19:31:54 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 07 Mar 2007 19:31:54 +0100
Subject: [R] transform R function
In-Reply-To: <BAY113-F1237AC3940B3A722DE9A81997A0@phx.gbl>
References: <BAY113-F1237AC3940B3A722DE9A81997A0@phx.gbl>
Message-ID: <45EF051A.6090801@biostat.ku.dk>

lamack lamack wrote:
> Dear all, Why the transform function does not accept two statistics 
> functions?
>
> a = data.frame(matrix(rnorm(20),ncol=2))
>
> transform(a,M.1=mean(X1),M.2=mean(X2)) # does not works
>
> #while:
>
> transform(a,M.1=mean(X1),M2=log(abs(X2))) #works
>
>   
It's a variation of this effect:

data.frame(airquality, list(x=1))        #works
data.frame(airquality, list(x=1, y=2)) #works not

Not quite sure what the logic of that is....

Of course transform() isn't really intended to handle anything but 
transformed vectors of the same length as the original.


From mothsailor at googlemail.com  Wed Mar  7 19:31:09 2007
From: mothsailor at googlemail.com (David Barron)
Date: Wed, 7 Mar 2007 18:31:09 +0000
Subject: [R] No fit statistics for some models using sem
In-Reply-To: <60647AC4-8AC3-41DF-BF81-3E463416195C@gmail.com>
References: <60647AC4-8AC3-41DF-BF81-3E463416195C@gmail.com>
Message-ID: <815b70590703071031n239b47acka3a2350b0a2bcce8@mail.gmail.com>

It's not the correlation as such that is the problem; it's because you
only have 10 degrees of freedom "available" with four observed
variables, and you are estimating 10 parameters, which is why you get
a chi square of zero.  When you remove any one free parameter (such as
the correlation), the model becomes identified.

On 07/03/07, Ista Zahn <istazahn at gmail.com> wrote:
> Hi,
>
> New to both R and SEM, so this may be a very simple question. I am
> trying to run a very simple path analysis using the sem package.
> There are 2 exogenous (FARSCH, LOCUS10) and 2 endogenous (T_ATTENT,
> RMTEST) observed variables in the model.  The idea is that T_ATTENT
> mediates the effect of FARSCH and LOCUS10 on RMTEST. The RAM
> specification I used is
>
> FARSCH -> T_ATTENT, y1x1, NA
> LOCUS10 -> T_ATTENT, y1x2, NA
> FARSCH -> RMTEST10, y2x1, NA
> LOCUS10 -> RMTEST10, y2x2, NA
> T_ATTENT -> RMTEST10, y2y1, NA
> FARSCH <-> FARSCH, x1x1, NA
> LOCUS10 <-> LOCUS10, x2x2, NA
> T_ATTENT <-> T_ATTENT, y1y1, NA
> RMTEST10 <-> RMTEST10, y2y2, NA
> LOCUS10 <-> FARSCH, x2x1, NA
>
> This model runs, but using the summary function does not return the
> usual model fit statistics, only the following:
>
> Model Chisquare =  0   Df =  0 Pr(>Chisq) = NA
>   Chisquare (null model) =  8526.8   Df =  6
>   Goodness-of-fit index =  1
>   BIC =  0
>
> If I omit the last line from the RAM specification(i.e., delete
> "LOCUS10 <-> FARSCH, x2x1, NA"), I DO get all the usual statistics:
>
>   Model Chisquare =  1303.7   Df =  1 Pr(>Chisq) = 0
>   Chisquare (null model) =  8526.8   Df =  6
>   Goodness-of-fit index =  0.95864
>   Adjusted goodness-of-fit index =  0.58639
>   RMSEA index =  0.30029   90% CI: (NA, NA)
>   Bentler-Bonnett NFI =  0.84711
>   Tucker-Lewis NNFI =  0.082726
>   Bentler CFI =  0.84712
>   BIC =  1294.1
>
> My understanding is the you should always put in the correlation
> between exogenous predictors, but when I do this I don't get fit
> statistics. Can anyone help me understand what is happening here?
>
> Thank you,
>
> Ista
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From aldi at wustl.edu  Wed Mar  7 20:03:19 2007
From: aldi at wustl.edu (Aldi Kraja)
Date: Wed, 07 Mar 2007 13:03:19 -0600
Subject: [R] Plotting a broken line?
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB879D4F@LP-EXCHVS07.CO.IHC.COM>
References: <07E228A5BE53C24CAD490193A7381BBB879D4F@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <45EF0C77.3010804@wustl.edu>

Hi Greg,

Thank you for your response and a previous posting about Macros in R.

Thank you also to Ken Knoblouch (Ken had the same idea as Greg's and 
Peter Pikal (who proposed the use of segments function).
There is only a technical specific that when applying max function to 
find the limit of y one has to use it with

max(x$indicator, na.rm =TRUE))

It worked!!!
 > x
   Location indicator otherinfo
1       1.2         1       2.2
2       2.5         1       2.5
3       3.7         1       2.3
4       3.7        NA        NA
5      20.1         2       4.3
6      22.5         2       5.2
7      25.0         2       3.4
8      27.3         2       2.2
9      27.3        NA        NA
10     35.1         3       3.4
11     37.0         3       7.2
12     38.0         3       6.1
13     40.1         3       5.4
14     52.9         3       3.3

Aldi

Greg Snow wrote:

>If you insert an NA (or row of NA's) into the data at each place you
>want a break (after indicator increases), then the regular plot with
>type='l' will break the line for you.
>
>Is this what you want?
>
>  
>

--


From jfox at mcmaster.ca  Wed Mar  7 20:02:17 2007
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 07 Mar 2007 14:02:17 -0500
Subject: [R] No fit statistics for some models using sem
In-Reply-To: <815b70590703071031n239b47acka3a2350b0a2bcce8@mail.gmail.com>
Message-ID: <web-165264141@cgpsrv2.cis.mcmaster.ca>

Dear David and Ista,

I haven't looked at this model carefully, but the fact that the df are
0 suggests that the model is just-identified and therefore necessarily
perfectly reproduces the covariances among the observed variables.
Removing a parameter would over-identify the model, making possible the
computation of the missing fit statistics.

Regards,
 John

On Wed, 7 Mar 2007 18:31:09 +0000
 "David Barron" <mothsailor at googlemail.com> wrote:
> It's not the correlation as such that is the problem; it's because
> you
> only have 10 degrees of freedom "available" with four observed
> variables, and you are estimating 10 parameters, which is why you get
> a chi square of zero.  When you remove any one free parameter (such
> as
> the correlation), the model becomes identified.
> 
> On 07/03/07, Ista Zahn <istazahn at gmail.com> wrote:
> > Hi,
> >
> > New to both R and SEM, so this may be a very simple question. I am
> > trying to run a very simple path analysis using the sem package.
> > There are 2 exogenous (FARSCH, LOCUS10) and 2 endogenous (T_ATTENT,
> > RMTEST) observed variables in the model.  The idea is that T_ATTENT
> > mediates the effect of FARSCH and LOCUS10 on RMTEST. The RAM
> > specification I used is
> >
> > FARSCH -> T_ATTENT, y1x1, NA
> > LOCUS10 -> T_ATTENT, y1x2, NA
> > FARSCH -> RMTEST10, y2x1, NA
> > LOCUS10 -> RMTEST10, y2x2, NA
> > T_ATTENT -> RMTEST10, y2y1, NA
> > FARSCH <-> FARSCH, x1x1, NA
> > LOCUS10 <-> LOCUS10, x2x2, NA
> > T_ATTENT <-> T_ATTENT, y1y1, NA
> > RMTEST10 <-> RMTEST10, y2y2, NA
> > LOCUS10 <-> FARSCH, x2x1, NA
> >
> > This model runs, but using the summary function does not return the
> > usual model fit statistics, only the following:
> >
> > Model Chisquare =  0   Df =  0 Pr(>Chisq) = NA
> >   Chisquare (null model) =  8526.8   Df =  6
> >   Goodness-of-fit index =  1
> >   BIC =  0
> >
> > If I omit the last line from the RAM specification(i.e., delete
> > "LOCUS10 <-> FARSCH, x2x1, NA"), I DO get all the usual statistics:
> >
> >   Model Chisquare =  1303.7   Df =  1 Pr(>Chisq) = 0
> >   Chisquare (null model) =  8526.8   Df =  6
> >   Goodness-of-fit index =  0.95864
> >   Adjusted goodness-of-fit index =  0.58639
> >   RMSEA index =  0.30029   90% CI: (NA, NA)
> >   Bentler-Bonnett NFI =  0.84711
> >   Tucker-Lewis NNFI =  0.082726
> >   Bentler CFI =  0.84712
> >   BIC =  1294.1
> >
> > My understanding is the you should always put in the correlation
> > between exogenous predictors, but when I do this I don't get fit
> > statistics. Can anyone help me understand what is happening here?
> >
> > Thank you,
> >
> > Ista
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 
> -- 
> =================================
> David Barron
> Said Business School
> University of Oxford
> Park End Street
> Oxford OX1 1HP
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/


From echoboi at yahoo.com  Wed Mar  7 20:37:49 2007
From: echoboi at yahoo.com (leesan)
Date: Wed, 7 Mar 2007 11:37:49 -0800 (PST)
Subject: [R] sqlSave help!
Message-ID: <9360420.post@talk.nabble.com>


Hi Everyone,
I'm so confused.  I've been trying to save data to a table but I keep
getting an error that says the table
does not exist and at other times saying that it does. So here are some
statements:

> sqlQuery(channel, "select top 1 * from
> TestDB.[SILICON\\holouis1].clep_tier_shift")
  State         NB Change_Number
1    IL 2005-02-08             7

It exists and I can get data from it, but if I try to use fetch or columns:

> sqlFetch(channel, "TestDB.[SILICON\\holouis1].clep_tier_shift")
Error in odbcTableExists(channel, sqtable) : 
        'TestDB.[SILICON\holouis1].clep_tier_shift': table not found on
channel

> sqlColumns(channel, "TestDB.[SILICON\\holouis1].clep_tier_shift")
Error in sqlColumns(channel, "TestDB.[SILICON\\holouis1].clep_tier_shift") : 
        'TestDB.[SILICON\holouis1].clep_tier_shift': table not found on
channel

Now if I try to save, it says error: table exists already even though I set
append = T...

> sqlQuery(channel, "select top 1 * from
> TestDB.[SILICON\\holouis1].clep_tier_shift") -> nuts
> sqlSave(channel, dat= nuts,
> tablename="TestDB.[SILICON\\holouis1].clep_tier_shift",safer= T)
Error in sqlSave(channel, dat = nuts, tablename =
"TestDB.[SILICON\\holouis1].clep_tier_shift",  : 
        [RODBC] ERROR: Could not SQLExecDirect
S0001 2714 [Microsoft][ODBC SQL Server Driver][SQL Server]There is already
an object named 'clep_tier_shift' in the database.

any help would be much appreciated, thanks!

-- 
View this message in context: http://www.nabble.com/sqlSave-help%21-tf3364399.html#a9360420
Sent from the R help mailing list archive at Nabble.com.


From lagaravitoh at unal.edu.co  Wed Mar  7 18:49:19 2007
From: lagaravitoh at unal.edu.co (Luis Garavito)
Date: Wed, 7 Mar 2007 12:49:19 -0500
Subject: [R] Download packages problem.
Message-ID: <62375bed0703070949h71635414qbce4cc13328c437@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/2b1bb0b8/attachment.pl 

From lalithaviswanath at yahoo.com  Wed Mar  7 20:41:56 2007
From: lalithaviswanath at yahoo.com (lalitha viswanath)
Date: Wed, 7 Mar 2007 11:41:56 -0800 (PST)
Subject: [R] Query about using setdiff
Message-ID: <934360.45920.qm@web43136.mail.sp1.yahoo.com>

Hi
I have two dataframes
names(DF1) = c("id", "val1", "val2");

names(DF2) = c("id2");

Ids in DF2 are a complete subset of those in DF1

How can I extract entries from DF1 where id NOT IN
DF2.

I tried setdiff(DF1, DF2); setdiff(DF1$id, DF2$id),
etc.
Although the latter eliminates the ids as required, I
dont know how to extract val1 and val2 for the
resultant set.


Thanks
Lalitha 


 
____________________________________________________________________________________
8:00? 8:25? 8:40? Find a flick in no time


From roger.bos at us.rothschild.com  Wed Mar  7 20:51:41 2007
From: roger.bos at us.rothschild.com (Bos, Roger)
Date: Wed, 7 Mar 2007 14:51:41 -0500
Subject: [R] Memory Limits in Ubuntu Linux
In-Reply-To: <45EE8580.24643.494B486@davidkat.davidkatzconsulting.com>
Message-ID: <D8C95B444AD6EE4AAD638D818A9CFD343A1EF7@RINNYCSE000.rth.ad.rothschild.com>

David,

Here is what my boot.ini file looks like:

[boot loader]
timeout=5
default=multi(0)disk(0)rdisk(0)partition(1)\WINDOWS
[operating systems]
multi(0)disk(0)rdisk(0)partition(1)\WINDOWS="Microsoft Windows XP
Professional" /noexecute=optin /fastdetect /3gb

The easiest way to edit the boot.ini file is My
Computer/Properties/Advanced/Startup & Recovery/Edit; add the /3gb and
reboot.  I know that a messed up boot.ini file can be a real pain.  I
posted what mine looks like so you can compare yours, but I wouldn't
suggest making any changes to your boot.ini except at the very end of
the last line.

HTH,

Roger


 

-----Original Message-----
From: davidkat at davidkatzconsulting.com
[mailto:davidkat at davidkatzconsulting.com] 
Sent: Wednesday, March 07, 2007 12:27 PM
To: Bos, Roger
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] Memory Limits in Ubuntu Linux

Thanks for the tips, Roger.

fyi: When I added /3GB to the boot.ini, the resulting desktop was
incomplete and locked - no chance to try starting R. Searching the web
lead me to believe that this was possibly a dead-end, so I abandoned
this effort. Any hints on getting this to work, anyone? 



On 7 Mar 2007 at 8:26, Bos, Roger wrote:

> David,
> 
> I wouldn't give up on windows so fast.  Many people have gotten the 
> 3Gb switch to work. One used to have to modify the header of the 
> Rgui.exe program to use the switch, but now the binary comes ready for

> that, so its really quite easy.  I would like to hear more about why 
> its not working for you.
> 
> As for Linux, I use FC5 for which there is a 64-bit binary.  But there

> are also 64-bit binaries for other distros.  The 32-bit and 64-bit 
> binaries are in different directories, so you should have no trouble 
> telling them apart.
> 
> I have heard good things about Ubuntu--mainly that its very easy to 
> use--but FC5 has been pretty easy to learn too and I use the KDE 
> desktop which gives me Kate as a text editor.  You can open a terminal

> window in Kate to run R and set up a key like F10 to send the code 
> from the editor to R.  Its not quite as good as my Windows setup with 
> Tinn-R, but almost as good.
> 
> Thanks,
> 
> Roger
> 
> 
> -----Original Message-----
> From: davidkat at davidkatzconsulting.com 
> [mailto:davidkat at davidkatzconsulting.com]
> Sent: Tuesday, March 06, 2007 5:37 PM
> To: Bos, Roger
> Subject: RE: [R] Memory Limits in Ubuntu Linux
> 
> Thanks for your prompt reply!
> 
> The windows 3GB switch is quite problematic - it was not useable on my

> machine, and there are comments about these problems around the net.
> Thus, on to Linux. My machine has 4Gig, and some megabytes are grabbed

> by my Asus motherboard, leaving some 3.56 Gig.
> 
> So if I understand your suggestion, try the 64-bit version of Ubuntu 
> (based on Debian but I had better luck with the video part of the
> install) and then use the corresponding image from CRAN. My fear is 
> that the CRAN Ubuntu version might be 32-bit - any idea how to find 
> out before I embark on another install? Which Linux do you have - you 
> described some significant success with getting large jobs to run.
> 
> And yes, I've worked hard to save memory by tweaking the code.
> 
> 
> Thanks again.
> 
> 
> On 6 Mar 2007 at 16:51, Bos, Roger wrote:
> 
> > David,
> > 
> > First of all, under Windows you can get about 3GB available to R by 
> > using the /3Gb switch in your boot.ini file, assuming you have 4Gb 
> > of memory installed on your windows machine.  Using that method, I 
> > have seen the memory using of my R process get as big as 2.7Gb in 
> > task manager.  What's important, of course, is contiguous space, as 
> > you mentioned.  There, you may want to check your code closely and 
> > make sure that its memory usage is as efficient as possible and you 
> > are storing the minimal amount you need for each run.  If you don't 
> > need an object for a while consider writing it to disk and reading 
> > it back
> in later.
> > 
> > Second, AFAIK to get any benefit from more memory is Linux you have 
> > to
> 
> > go to the 64bit version.  I am a Linux newbie too, so I choose to 
> > use one of the pre-compiled binaries available on CRAN.  In other 
> > words, you shouldn't have to compile anything yourself.  How much 
> > memory do you have on your Linux box?  I have 16Gb and I know I have

> > ran stuff that wouldn't run on my 4Gb windows box.
> > 
> > HTH,
> > 
> > Roger
> > 
> > 
> > 
> > 
> > 
> >  
> > 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> > davidkat at davidkatzconsulting.com
> > Sent: Tuesday, March 06, 2007 3:44 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Memory Limits in Ubuntu Linux
> > 
> > I am an R user trying to get around the 2Gig memory limit in 
> > Windows, so here I am days later with a working Ubuntu, and R under 
> > Ubuntu. But
> 
> > - the memory problems seem worse than ever. R code that worked under

> > windows fails, unable to allocate memory.
> > 
> > Searching around the web, it appears that the problem may be the 
> > ability to find contguous memory for my big vectors, but a fresh 
> > boot of Ubuntu does not help either.
> > 
> > Which way to go?
> > 
> > 1) Try to install 64-bit version for bigger address space. Would 
> > this help? Is this workable for my Athlon 64 Dual-core? (the live cd

> > seems to work but I never got it to boot after a disk install, but 
> > then the
> > 386 version was no better until I learned more about Grub...I could 
> > try again if this might solve the
> > problem)
> > 
> > 2) Recompile R to get bigger memory capability? (I'll have to 
> > cross-post to some R forums too) This will be a challenge for a 
> > Linux newbie...like me.
> > 
> > 3) Any other suggestions? My goal is to create a bigger neural 
> > network
> 
> > than fits in my Windows R version.
> > --
> > David Katz
> >  www.davidkatzconsulting.com
> >    541 482-1137
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> > ********************************************************************
> > **
> 
> > * This message is for the named person's use only. It may contain 
> > confidential, proprietary or legally privileged information. No 
> > right to confidential or privileged treatment of this message is 
> > waived or lost by any error in transmission. If you have received 
> > this message in error, please immediately notify the sender by 
> > e-mail, delete the message and all copies from your system and
destroy any hard copies.
> > You must not, directly or indirectly, use, disclose, distribute, 
> > print
> 
> > or copy any part of this message if you are not the intended 
> > recipient.
> > ********************************************************************
> > **
> > 
> 
> --
> David Katz
>    www.davidkatzconsulting.com
> 
> 
> **********************************************************************

> * This message is for the named person's use only. It may contain 
> confidential, proprietary or legally privileged information. No right 
> to confidential or privileged treatment of this message is waived or 
> lost by any error in transmission. If you have received this message 
> in error, please immediately notify the sender by e-mail, delete the 
> message and all copies from your system and destroy any hard copies. 
> You must not, directly or indirectly, use, disclose, distribute, print

> or copy any part of this message if you are not the intended 
> recipient.
> **********************************************************************
> 

--
David Katz
   www.davidkatzconsulting.com


********************************************************************** * 
This message is for the named person's use only. It may 
contain confidential, proprietary or legally privileged 
information. No right to confidential or privileged treatment 
of this message is waived or lost by any error in 
transmission. If you have received this message in error, 
please immediately notify the sender by e-mail, 
delete the message and all copies from your system and destroy 
any hard copies. You must not, directly or indirectly, use, 
disclose, distribute, print or copy any part of this message 
if you are not the intended recipient.


From Dimitris.Rizopoulos at med.kuleuven.be  Wed Mar  7 20:53:53 2007
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 07 Mar 2007 20:53:53 +0100
Subject: [R] Query about using setdiff
In-Reply-To: <934360.45920.qm@web43136.mail.sp1.yahoo.com>
References: <934360.45920.qm@web43136.mail.sp1.yahoo.com>
Message-ID: <20070307205353.6her8hixf1waoogw@webmail5.kuleuven.be>

try something along these lines (untested):

DF1[DF1$id %in% DF2$id2, c("val1", "val2")]
DF1[!DF1$id %in% DF2$id2, c("val1", "val2")]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting lalitha viswanath <lalithaviswanath at yahoo.com>:

> Hi
> I have two dataframes
> names(DF1) = c("id", "val1", "val2");
>
> names(DF2) = c("id2");
>
> Ids in DF2 are a complete subset of those in DF1
>
> How can I extract entries from DF1 where id NOT IN
> DF2.
>
> I tried setdiff(DF1, DF2); setdiff(DF1$id, DF2$id),
> etc.
> Although the latter eliminates the ids as required, I
> dont know how to extract val1 and val2 for the
> resultant set.
>
>
> Thanks
> Lalitha
>
>
>
> ____________________________________________________________________________________
> 8:00? 8:25? 8:40? Find a flick in no time
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From M.J.Bojanowski at uu.nl  Wed Mar  7 21:11:03 2007
From: M.J.Bojanowski at uu.nl (Bojanowski, M.J.  (Michal))
Date: Wed, 7 Mar 2007 21:11:03 +0100
Subject: [R] ODP:  Plotting a broken line?
References: <45EEE672.5040904@wustl.edu>
Message-ID: <94E133D09AA24D43BF6341B675C01A338DCCCD@uu01msg-exb01.soliscom.uu.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/37692a7a/attachment.pl 

From heloisemattos at gmail.com  Wed Mar  7 21:29:10 2007
From: heloisemattos at gmail.com (Heloise Mattos)
Date: Wed, 7 Mar 2007 17:29:10 -0300
Subject: [R] C to R
Message-ID: <e72263240703071229i5b66b300y98369473e7e2be35@mail.gmail.com>

I`m doing some functions on C that gives me the x and y coordinates.
I`d like to now how I can get these coordinates (both are a vector of
number) on R to that I can make a graphic.
I`ve already made a package with my functions, so I just wanna how
about how to get the coordinates.

Thanks, Heloise.


From sfalcon at fhcrc.org  Wed Mar  7 21:33:36 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Wed, 07 Mar 2007 12:33:36 -0800
Subject: [R] Download packages problem.
In-Reply-To: <62375bed0703070949h71635414qbce4cc13328c437@mail.gmail.com>
	(Luis Garavito's message of "Wed, 7 Mar 2007 12:49:19 -0500")
References: <62375bed0703070949h71635414qbce4cc13328c437@mail.gmail.com>
Message-ID: <m2649c3ian.fsf@ziti.local>

"Luis Garavito" <lagaravitoh at unal.edu.co> writes:

> Hi,
>
> I have a little problem with the installation of a new packages. The
> installation of R software is correct, but my server required authentication
> for use it, and for load a new package directly from R it is not possible.
> Is there a code or process for server authentication (put my login and
> password) in R for download directly the packages?

Yes.





Without telling us what operating system, what version of R, and some
specifics of what you are trying and the error messages you are
seeing, it is rather difficult to imagine that you will get any useful
help.

Please have a look at the posting guide and send an update.

+ seth

-- 
Seth Falcon | Computational Biology | Fred Hutchinson Cancer Research Center
http://bioconductor.org


From zz2122 at columbia.edu  Wed Mar  7 21:37:46 2007
From: zz2122 at columbia.edu (Jonathan Zhang)
Date: Wed, 7 Mar 2007 15:37:46 -0500
Subject: [R] ATLAS for Pentium D and Pentium Duo Core?
Message-ID: <54794ae90703071237h5490262fnb453b0cc36a10f0d@mail.gmail.com>

Dear all,

  I wanted to use ATLAS to increase the computational speed of my
program. I have installed R in a straightfoward way with .exe file
(not building from source).

  However, after getting the P4 ATLAS Rblas.dll file from the CRAN
directory bin/windows/contrib/ATLAS, and replacing the default
Rblas.dll file in my R home directory under bin/, the speed actually
decreased by 3% compared with the default Rblas file!!

  So what am I doing wrong? Is the above the correct procedure for
using ATLAS? Or, is my machine not a Pentium 4 and hence the ATLAS
file that I got from CRAN is not appropriate?

  If the issue lies in the processor, then please kindly let me know
where to get the correct ATLAS file for the following computers:

 IThe first one is Pentium D 3.2 ghz, the second is a MacBook 1.83ghz
dual-boot and I am running R in windows.

  Thank you!

Jonathan Zhang

Marketing Division
Columbia Business School


From zz2122 at columbia.edu  Wed Mar  7 21:37:46 2007
From: zz2122 at columbia.edu (Jonathan Zhang)
Date: Wed, 7 Mar 2007 15:37:46 -0500
Subject: [R] ATLAS for Pentium D and Pentium Duo Core?
Message-ID: <54794ae90703071237h5490262fnb453b0cc36a10f0d@mail.gmail.com>

Dear all,

  I wanted to use ATLAS to increase the computational speed of my
program. I have installed R in a straightfoward way with .exe file
(not building from source).

  However, after getting the P4 ATLAS Rblas.dll file from the CRAN
directory bin/windows/contrib/ATLAS, and replacing the default
Rblas.dll file in my R home directory under bin/, the speed actually
decreased by 3% compared with the default Rblas file!!

  So what am I doing wrong? Is the above the correct procedure for
using ATLAS? Or, is my machine not a Pentium 4 and hence the ATLAS
file that I got from CRAN is not appropriate?

  If the issue lies in the processor, then please kindly let me know
where to get the correct ATLAS file for the following computers:

 IThe first one is Pentium D 3.2 ghz, the second is a MacBook 1.83ghz
dual-boot and I am running R in windows.

  Thank you!

Jonathan Zhang

Marketing Division
Columbia Business School


From klaster at karlin.mff.cuni.cz  Wed Mar  7 21:49:56 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Wed, 07 Mar 2007 21:49:56 +0100
Subject: [R] Download packages problem.
In-Reply-To: <62375bed0703070949h71635414qbce4cc13328c437@mail.gmail.com>
References: <62375bed0703070949h71635414qbce4cc13328c437@mail.gmail.com>
Message-ID: <45EF2574.5090205@karlin.mff.cuni.cz>

Luis,

posting a message several times usually won't help you to get answers...
You did not tell us what kind of error you obtain, what packages did you 
try to install and how, what operating system are you using etc. To be 
honest, I have no idea what kind of server can be involved - R does not 
need any server to run on and the CRAN ftp severs work with anonymous 
logins. Please read the posting guide (link given below in the footer) 
and follow it next time.
Petr

Luis Garavito napsal(a):
> Hi,
> 
> I have a little problem with the installation of a new packages. The
> installation of R software is correct, but my server required authentication
> for use it, and for load a new package directly from R it is not possible.
> Is there a code or process for server authentication (put my login and
> password) in R for download directly the packages?
> 
> The best regards,

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From m_hofert at web.de  Wed Mar  7 22:06:03 2007
From: m_hofert at web.de (Hofert Marius)
Date: Wed, 7 Mar 2007 22:06:03 +0100
Subject: [R] Read data with different column lengths
Message-ID: <C9C41A7B-13AF-4277-9A5A-A56AB9759112@web.de>

Dear r-help users,

I have the following simple problem: Reading data from a file. The  
file is a .txt file exported ("save as...") from Excel (see below for  
an example). The Excel file consists of two header rows (first row  
consists of ticker symbols of stocks, the second row consists of  
column explanations ("Date","Px Last"), followed by several rows of  
data. Now forget about the first two rows, I can deal with that (read  
separately, then extract the actual ticker symbols "ADS",  
"ALV", ...). For reading the rest, I tried several things, for example:
data=read.table(infile,quote="",fill=T,dec=",",skip=2,colClasses=rep(c 
("character","numeric"),ntickers))
or
data=matrix(scan(file=infile,what=rep(c 
("character","numeric"),ntickers),dec=",",skip=2),ncol=2*ntickers,byrow= 
T)
where "infile" specifies the path to the input file and "ntickers" is  
the number of ticker-columns in the data set, so in the example  
below, ntickers=2.

Both ways of reading the data work perfectly fine if all columns have  
the same length (i.e. the same number of filled rows), so if the data  
is given in a (filled) "rectangular" form. Now, as you can imagine,  
there are days when one stock is traded but not the other... so,  
there might be columns that do not have the same number of filled  
rows (see below, for the stock with ticker symbol "ADS", only 3  
trading days are shown, so this column is shorter than the data  
column for the stock "ALV"). Now, if I export such a structure to  
a .txt file, then all (by default) blank fields will be replaced by  
"\t", i.e. tabs. Both reading procedures as give above have problems  
as they either display that the number of rows/columns do not fit  
together or as they read the table, but some cells are shifted to the  
left (for the example below, the entry "07/02/05	134,7" appears in  
the empty field of the stock "ADS" which is of course not what we want).
So the simple question is: How do I read such a structure?
Can there be a simple solution? The problem is simply that empty  
cells are replace by "\t" which are then ignored for reading. So how  
do we distinguish between the empty cells that are given between the  
columns and the empty cells that actually "fill" a column to have the  
same length as other columns. Of course I could manually put in a  
certain character (e.g. a "*") to fill in the gaps, but the data set  
is simply too large. If it helps, these blank fields only appear in  
the end of each column, not in the middle.

As I work on a Mac (OS X 10.4), it was not possible (at least to me)  
to read the data directly from the Excel file vial the library RODBC  
or read.xls.

Note, that the same problem arises, when I export the Excel file as  
a .csv, then all blank fields are separated by ";" instead of "\t"  
and the reading procedure can also not decide if the field  
corresponds to an empty separating column or actually to a column  
with given entries, but which is simply not as long as another column  
in the file.

Hope, you can help. I would really appreciate it.

Best regards.

Marius

Excel example (I hope it's displayed correctly, the entry in the last  
row should be aligned with the last column):

ADS GY Equity			ALV GY Equity		
Date		Px Last		Date		Px Last
07/02/04	41,395		07/01/31	130,234
07/02/05	42,134		07/02/01	133,353
07/02/06	41,875		07/02/04	133,824
						07/02/05	134,734


From aiminy at iastate.edu  Wed Mar  7 22:26:53 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Wed, 07 Mar 2007 15:26:53 -0600
Subject: [R] how to avoid to overwrite object
Message-ID: <6.2.3.4.2.20070307151716.03926430@aiminy.mail.iastate.edu>

Dear R list,
I have a question in R, it could be very simple, but I don't know how to do it?

for example:
I assign 6 to x in beginning of of my R script code
 > x<-6
......
After many line code, I forget using x variable before, I use x 
again, and do assignment like this
 > x<-45
 > x
[1] 45

then value 6 of previous x is replaced by 45.

I am wondering if there are some way R can give me warning like
"x is used before, overwrite it or not?" when I use x again?

Thanks,

Aimin


From bolker at zoo.ufl.edu  Wed Mar  7 22:28:03 2007
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Wed, 7 Mar 2007 21:28:03 +0000 (UTC)
Subject: [R] No years() function?
References: <4c817d530703070735l2001e569ubdd0057e17fccb37@mail.gmail.com>
Message-ID: <loom.20070307T222146-499@post.gmane.org>

S?rgio Nunes <snunes <at> gmail.com> writes:

> 
> Hi,
> 
> I'm trying to aggregate date values using the aggregate function. For example:
> 
> aggregate(data,by=list(weekdays(LM),months(LM)),FUN=length)
> 
> I would also like to aggregate by year but there seems to be no
> years() function.
> Should there be one? Is there any alternative choice?
> 
> Also, a hours() function would be great. Any tip on this?
> 
> Thanks in advance!
> S?rgio Nunes
> 

  Well, working by analogy with the existing
functions, this might work (not messing with
setting up an S3 default though):

> apropos("weekdays")
[1] "weekdays"        "weekdays.Date"   "weekdays.POSIXt"

> weekdays.Date
function (x, abbreviate = FALSE)
format(x, ifelse(abbreviate, "%a", "%A"))
<environment: namespace:base>


d1 = Sys.time()

years <- function(x,abbreviate=FALSE) {
  as.numeric(format(x, ifelse(abbreviate, "%y", "%Y")))
}

hours <- function(x) {
   as.numeric(format(x,"%H"))
}

years(d1); hours(d1)


From kubovy at virginia.edu  Wed Mar  7 22:30:59 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Wed, 7 Mar 2007 16:30:59 -0500
Subject: [R] Failure to run mcsamp() in package arm
In-Reply-To: <29AE7E35-7766-47D9-8EDC-7AB37A37A9D9@virginia.edu>
References: <29AE7E35-7766-47D9-8EDC-7AB37A37A9D9@virginia.edu>
Message-ID: <66008798-FB9A-4FD9-8384-850AC7FF5D1C@virginia.edu>

More problems. If I run
sim(fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
from the lmer() help page.

I get the error
Error in mvrnorm(n.sims, bhat[j, ], V.beta) :
	'Sigma' is not positive definite

On Mar 7, 2007, at 1:30 PM, Michael Kubovy wrote:

> Dear r-helpers,
>
> I can run the examples on the mcsamp help page. For example:
> ****************************************
>> M1 <- lmer (y1 ~ x + (1|group))
>> (M1.sim <- mcsamp (M1))
> fit using lmer,
> 3 chains, each with 1000 iterations (first 500 discarded)
> n.sims = 1500 iterations saved
>                            mean  sd 2.5%  25%  50%  75% 97.5% Rhat  
> n.eff
> beta.(Intercept)           0.1 0.7 -1.2 -0.3  0.1  0.5   1.4  1.0   
> 1500
> beta.x                     2.5 0.4  1.7  2.2  2.5  2.7   3.2  1.0   
> 1500
> sigma.y                    3.8 0.3  3.3  3.6  3.7  3.9   4.3   
> 1.0    61
> sigma.grop.(In)            1.5 0.8  0.0  1.0  1.4  1.9   3.3   
> 1.4    12
> eta.group.(Intercept)[1]   0.0 1.0 -2.1 -0.5  0.0  0.6   2.0  1.0   
> 1500
> eta.group.(Intercept)[2]   1.0 1.1 -0.9  0.2  0.9  1.7   3.4   
> 1.0    59
> eta.group.(Intercept)[3]  -1.3 1.2 -4.0 -2.0 -1.3 -0.4   0.5   
> 1.0    66
> eta.group.(Intercept)[4]   1.3 1.1 -0.6  0.4  1.1  2.0   3.7   
> 1.1    43
> eta.group.(Intercept)[5]  -0.7 1.0 -3.0 -1.4 -0.6  0.0   1.2  1.0    
> 120
> eta.group.(Intercept)[6]   1.5 1.2 -0.3  0.6  1.4  2.2   4.0   
> 1.0    49
> eta.group.(Intercept)[7]   0.3 1.0 -1.7 -0.3  0.1  0.8   2.5  1.0    
> 440
> eta.group.(Intercept)[8]  -1.6 1.2 -4.0 -2.4 -1.5 -0.6   0.3   
> 1.1    41
> eta.group.(Intercept)[9]   0.4 1.0 -1.6 -0.2  0.2  0.9   2.7  1.0    
> 180
> eta.group.(Intercept)[10] -1.0 1.1 -3.3 -1.6 -0.9 -0.2   0.8   
> 1.0    86
>
> For each parameter, n.eff is a crude measure of effective sample size,
> and Rhat is the potential scale reduction factor (at convergence,
> Rhat=1).
> ****************************************
> But when I try to do this with my own data I get an error:
> ****************************************
>> display(e7.lmer2)
> lmer(formula = baLO ~ I(baRatio - 0.985) + delta + (1 + I(baRatio -
> 0.985) + delta | subject), data = e7)
>                     coef.est coef.se
> (Intercept)        -0.19     0.06
> I(baRatio - 0.985) -4.95     0.74
> delta               0.41     0.06
> Error terms:
> Groups   Name               Std.Dev. Corr
> subject  (Intercept)        0.13
>            I(baRatio - 0.985) 2.57      0.45
>            delta              0.22     -0.12 -0.94
> Residual                    0.39
> number of obs: 494, groups: subject, 13
> deviance = 551.4
>
>> e7.sim <- mcsamp(e7.lmer2)
> Error in as.bugs.array(sims, program = "lmer", n.iter = n.iter,
> n.burnin = n.burnin,  :
> 	error in parameter sigma. in parameters.to.save
> ****************************************
> I would appreciate a pointer to what the problem might be.
> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
> Parcels:    Room 102        Gilmer Hall
>          McCormick Road    Charlottesville, VA 22903
> Office:    B011    +1-434-982-4729
> Lab:        B019    +1-434-982-4751
> Fax:        +1-434-982-4766
> WWW:    http://www.people.virginia.edu/~mk9y/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jr_frrr at yahoo.de  Wed Mar  7 22:42:13 2007
From: jr_frrr at yahoo.de (=?ISO-8859-1?Q?Jos=E9?= Rafael Ferrer Paris)
Date: Wed, 07 Mar 2007 17:42:13 -0400
Subject: [R] No years() function?
In-Reply-To: <4c817d530703070735l2001e569ubdd0057e17fccb37@mail.gmail.com>
References: <4c817d530703070735l2001e569ubdd0057e17fccb37@mail.gmail.com>
Message-ID: <1173303733.8959.32.camel@localhost.localdomain>

>From the help of weekdays:

"Note:

     Other components such as the day of the month or the year are very
     easy to compute: just use 'as.POSIXlt' and extract the relevant
     component."

Yet another option:

help(package="chron")

JR

El mi?, 07-03-2007 a las 15:35 +0000, S?rgio Nunes escribi?:
> Hi,
> 
> I'm trying to aggregate date values using the aggregate function. For example:
> 
> aggregate(data,by=list(weekdays(LM),months(LM)),FUN=length)
> 
> I would also like to aggregate by year but there seems to be no
> years() function.
> Should there be one? Is there any alternative choice?
> 
> Also, a hours() function would be great. Any tip on this?
> 
> Thanks in advance!
> S?rgio Nunes
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
Dipl.-Biol. JR Ferrer Paris
~~~~~~~~~~~~~~~~~~~~~~~~~~~
Laboratorio de Biolog?a de Organismos --- Centro de Ecolog?a
Instituto Venezolano de Investigaciones Cient?ficas (IVIC) 
Apdo. 21827, Caracas 1020-A 
Rep?blica Bolivariana de Venezuela

Tel: (+58-212) 504-1452
Fax: (+58-212) 504-1088

email: jferrer at ivic.ve
clave-gpg: 2C260A95


From A.Robinson at ms.unimelb.edu.au  Wed Mar  7 22:46:29 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Thu, 8 Mar 2007 08:46:29 +1100
Subject: [R] C to R
In-Reply-To: <e72263240703071229i5b66b300y98369473e7e2be35@mail.gmail.com>
References: <e72263240703071229i5b66b300y98369473e7e2be35@mail.gmail.com>
Message-ID: <20070307214629.GK19982@ms.unimelb.edu.au>

Hi Heloise,

there's a manual online to help you with this - see here

http://cran.r-project.org/doc/manuals/R-exts.html

Cheers

Andrew

On Wed, Mar 07, 2007 at 05:29:10PM -0300, Heloise Mattos wrote:
> I`m doing some functions on C that gives me the x and y coordinates.
> I`d like to now how I can get these coordinates (both are a vector of
> number) on R to that I can make a graphic.
> I`ve already made a package with my functions, so I just wanna how
> about how to get the coordinates.
> 
> Thanks, Heloise.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From bunny at lautloscrew.com  Wed Mar  7 23:12:04 2007
From: bunny at lautloscrew.com (bunny , lautloscrew.com)
Date: Wed, 7 Mar 2007 23:12:04 +0100
Subject: [R] hwo can i get a vector that...
Message-ID: <20493382-3D50-415E-89EA-F0F1C7BDFF22@lautloscrew.com>

dear all,

how can i get a vector that shows the number of the column of matrix  
that contains the maximum of the row ??
can?t believe in need a loop for this...

i have a  100 x 3 matrix and want to get a 100 x 1 vector with values  
1,2,3 .

there must be a simple solution. i just cannot find it. i think am  
searching on the wrong end.

thx for help in advance.

m.


From bunny at lautloscrew.com  Wed Mar  7 23:20:29 2007
From: bunny at lautloscrew.com (bunny , lautloscrew.com)
Date: Wed, 7 Mar 2007 23:20:29 +0100
Subject: [R] hwo can i get a vector that...
In-Reply-To: <20493382-3D50-415E-89EA-F0F1C7BDFF22@lautloscrew.com>
References: <20493382-3D50-415E-89EA-F0F1C7BDFF22@lautloscrew.com>
Message-ID: <D661DF3F-E8C8-4BE5-BB71-398E20CEFBBB@lautloscrew.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/65414551/attachment.pl 

From gunter.berton at gene.com  Wed Mar  7 23:21:39 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Wed, 7 Mar 2007 14:21:39 -0800
Subject: [R] hwo can i get a vector that...
In-Reply-To: <20493382-3D50-415E-89EA-F0F1C7BDFF22@lautloscrew.com>
Message-ID: <002101c76106$fa244ed0$4d908980@gne.windows.gene.com>

apply(yourMatrix,1,which.max) 


Bert Gunter
Nonclinical Statistics
7-7374

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bunny ,
lautloscrew.com
Sent: Wednesday, March 07, 2007 2:12 PM
To: r-help at stat.math.ethz.ch
Subject: [R] hwo can i get a vector that...

dear all,

how can i get a vector that shows the number of the column of matrix  
that contains the maximum of the row ??
can?t believe in need a loop for this...

i have a  100 x 3 matrix and want to get a 100 x 1 vector with values  
1,2,3 .

there must be a simple solution. i just cannot find it. i think am  
searching on the wrong end.

thx for help in advance.

m.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From PAlspach at hortresearch.co.nz  Wed Mar  7 23:30:28 2007
From: PAlspach at hortresearch.co.nz (Peter Alspach)
Date: Thu, 8 Mar 2007 11:30:28 +1300
Subject: [R] hwo can i get a vector that...
Message-ID: <EC0F8FF776F3F74E9C63CE16641C962801DF7BD0@AKLEXB02.hort.net.nz>


Check out which.max 

Peter Alspach


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bunny 
> , lautloscrew.com
> Sent: Thursday, 8 March 2007 11:20 a.m.
> To: R-help at stat.math.ethz.ch
> Subject: Re: [R] hwo can i get a vector that...
> 
> 
> 
> 
> > dear all,
> >
> > how can i get a vector that shows the number of the column 
> of matrix 
> > that contains the maximum of the row ??
> > can?t believe in need a loop for this...
> >
> > i have a  100 x 3 matrix and want to get a 100 x 1 vector 
> with values 
> > 1,2,3 .
> >
> > there must be a simple solution. i just cannot find it. i think am 
> > searching on the wrong end.
> >
> > thx for help in advance.
> >
> > m.
> 
> 
> EDIT: ok,  i know the following by now :)
> 
> apply(for18[,-1], 1, max, na.rm=T)
> 
> but this doesn?t get me the number of the column - which is 
> what i need... 
> 	[[alternative HTML version deleted]]
> 
> 
> 

______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}


From klaster at karlin.mff.cuni.cz  Wed Mar  7 23:35:06 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Wed, 07 Mar 2007 23:35:06 +0100
Subject: [R] hwo can i get a vector that...
In-Reply-To: <20493382-3D50-415E-89EA-F0F1C7BDFF22@lautloscrew.com>
References: <20493382-3D50-415E-89EA-F0F1C7BDFF22@lautloscrew.com>
Message-ID: <45EF3E1A.4070007@karlin.mff.cuni.cz>

?apply
?which.max

 > m <- matrix(rnorm(12),nrow=4)
 > m
 > apply(m,1,which.max)

Petr

bunny , lautloscrew.com napsal(a):
> dear all,
> 
> how can i get a vector that shows the number of the column of matrix  
> that contains the maximum of the row ??
> can?t believe in need a loop for this...
> 
> i have a  100 x 3 matrix and want to get a 100 x 1 vector with values  
> 1,2,3 .
> 
> there must be a simple solution. i just cannot find it. i think am  
> searching on the wrong end.
> 
> thx for help in advance.
> 
> m.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From scionforbai at gmail.com  Wed Mar  7 23:36:04 2007
From: scionforbai at gmail.com (Scionforbai)
Date: Wed, 7 Mar 2007 23:36:04 +0100
Subject: [R] hwo can i get a vector that...
In-Reply-To: <D661DF3F-E8C8-4BE5-BB71-398E20CEFBBB@lautloscrew.com>
References: <20493382-3D50-415E-89EA-F0F1C7BDFF22@lautloscrew.com>
	<D661DF3F-E8C8-4BE5-BB71-398E20CEFBBB@lautloscrew.com>
Message-ID: <e9ee1f0a0703071436x40c0c97ape51250a5f7ee9773@mail.gmail.com>

> can?t believe in need a loop for this...
No, you don't ;)

> want to get a 100 x 1 vector
Has each row unique values? If yes:

mat <- matrix(rnorm(300),nr=100)
vet <- apply(mat,1,function(x) {return(which(x==max(x)))})



scionforbai


From kubovy at virginia.edu  Wed Mar  7 23:46:45 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Wed, 7 Mar 2007 17:46:45 -0500
Subject: [R] Failure to run mcsamp() in package arm
In-Reply-To: <66008798-FB9A-4FD9-8384-850AC7FF5D1C@virginia.edu>
References: <29AE7E35-7766-47D9-8EDC-7AB37A37A9D9@virginia.edu>
	<66008798-FB9A-4FD9-8384-850AC7FF5D1C@virginia.edu>
Message-ID: <0A9AFFEA-ADF3-4FA4-A52A-08480383EE36@virginia.edu>

Andrew Robinson has gently chided me for not including more  
information. So here goes:
R version 2.4.1 (2006-12-18)
powerpc-apple-darwin8.8.0

locale:
C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "methods"   "base"

other attached packages:
     foreign         car         arm   R2WinBUGS        lme4       
Matrix     lattice
    "0.8-18"     "1.2-1"    "1.0-13"     "2.0-4" "0.9975-13"  
"0.9975-11"   "0.14-16"
        MASS         JGR      iplots      JavaGD       rJava
    "7.2-32"    "1.4-15"     "1.0-5"     "0.3-5"    "0.4-14"

On Mar 7, 2007, at 4:30 PM, Michael Kubovy wrote:

> More problems. If I run
> sim(fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
> from the lmer() help page.
>
> I get the error
> Error in mvrnorm(n.sims, bhat[j, ], V.beta) :
> 	'Sigma' is not positive definite
>
> On Mar 7, 2007, at 1:30 PM, Michael Kubovy wrote:
>
>> Dear r-helpers,
>>
>> I can run the examples on the mcsamp help page. For example:
>> ****************************************
>>> M1 <- lmer (y1 ~ x + (1|group))
>>> (M1.sim <- mcsamp (M1))
>> fit using lmer,
>> 3 chains, each with 1000 iterations (first 500 discarded)
>> n.sims = 1500 iterations saved
>>                            mean  sd 2.5%  25%  50%  75% 97.5% Rhat
>> n.eff
>> beta.(Intercept)           0.1 0.7 -1.2 -0.3  0.1  0.5   1.4  1.0
>> 1500
>> beta.x                     2.5 0.4  1.7  2.2  2.5  2.7   3.2  1.0
>> 1500
>> sigma.y                    3.8 0.3  3.3  3.6  3.7  3.9   4.3
>> 1.0    61
>> sigma.grop.(In)            1.5 0.8  0.0  1.0  1.4  1.9   3.3
>> 1.4    12
>> eta.group.(Intercept)[1]   0.0 1.0 -2.1 -0.5  0.0  0.6   2.0  1.0
>> 1500
>> eta.group.(Intercept)[2]   1.0 1.1 -0.9  0.2  0.9  1.7   3.4
>> 1.0    59
>> eta.group.(Intercept)[3]  -1.3 1.2 -4.0 -2.0 -1.3 -0.4   0.5
>> 1.0    66
>> eta.group.(Intercept)[4]   1.3 1.1 -0.6  0.4  1.1  2.0   3.7
>> 1.1    43
>> eta.group.(Intercept)[5]  -0.7 1.0 -3.0 -1.4 -0.6  0.0   1.2  1.0
>> 120
>> eta.group.(Intercept)[6]   1.5 1.2 -0.3  0.6  1.4  2.2   4.0
>> 1.0    49
>> eta.group.(Intercept)[7]   0.3 1.0 -1.7 -0.3  0.1  0.8   2.5  1.0
>> 440
>> eta.group.(Intercept)[8]  -1.6 1.2 -4.0 -2.4 -1.5 -0.6   0.3
>> 1.1    41
>> eta.group.(Intercept)[9]   0.4 1.0 -1.6 -0.2  0.2  0.9   2.7  1.0
>> 180
>> eta.group.(Intercept)[10] -1.0 1.1 -3.3 -1.6 -0.9 -0.2   0.8
>> 1.0    86
>>
>> For each parameter, n.eff is a crude measure of effective sample  
>> size,
>> and Rhat is the potential scale reduction factor (at convergence,
>> Rhat=1).
>> ****************************************
>> But when I try to do this with my own data I get an error:
>> ****************************************
>>> display(e7.lmer2)
>> lmer(formula = baLO ~ I(baRatio - 0.985) + delta + (1 + I(baRatio -
>> 0.985) + delta | subject), data = e7)
>>                     coef.est coef.se
>> (Intercept)        -0.19     0.06
>> I(baRatio - 0.985) -4.95     0.74
>> delta               0.41     0.06
>> Error terms:
>> Groups   Name               Std.Dev. Corr
>> subject  (Intercept)        0.13
>>            I(baRatio - 0.985) 2.57      0.45
>>            delta              0.22     -0.12 -0.94
>> Residual                    0.39
>> number of obs: 494, groups: subject, 13
>> deviance = 551.4
>>
>>> e7.sim <- mcsamp(e7.lmer2)
>> Error in as.bugs.array(sims, program = "lmer", n.iter = n.iter,
>> n.burnin = n.burnin,  :
>> 	error in parameter sigma. in parameters.to.save
>> ****************************************
>> I would appreciate a pointer to what the problem might be.
>> _____________________________
>> Professor Michael Kubovy
>> University of Virginia
>> Department of Psychology
>> USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
>> Parcels:    Room 102        Gilmer Hall
>>          McCormick Road    Charlottesville, VA 22903
>> Office:    B011    +1-434-982-4729
>> Lab:        B019    +1-434-982-4751
>> Fax:        +1-434-982-4766
>> WWW:    http://www.people.virginia.edu/~mk9y/
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From g.abraham at ms.unimelb.edu.au  Thu Mar  8 00:01:40 2007
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Thu, 08 Mar 2007 10:01:40 +1100
Subject: [R] good procedure to estimate ARMA(p, q)?
In-Reply-To: <b1f16d9d0703070133t5447f609k75a8f67beed3ac27@mail.gmail.com>
References: <b1f16d9d0703070133t5447f609k75a8f67beed3ac27@mail.gmail.com>
Message-ID: <45EF4454.6030707@ms.unimelb.edu.au>

Michael wrote:
> Hi all,
> 
> I have some residuals from regression, and i suspect they have correlations
> in them...
> 
> I am willing to cast the correlation into a ARMA(p, q) framework,
> 
> what's the best way to identify the most suitable p, and q, and fit ARMA(p,
> q) model and then correct for the correlations in regression?
> 
> I know there are functions in R, I have used them before, but I just want to
> see if I can do the whole procedure myself, just to improve my understanding
> ...
> 
> Please give me some pointers! Thanks a lot

I'm assuming the data is a time series, otherwise ARIMA models might not 
be applicable here.

I think identifying the order of ARIMA models is something of an art, 
because most real world models aren't as clean and simple as textbook 
examples. When you have several similar models, each with its own 
strengths and weaknesses, which one is "best"?

In short, you want to make sure your series is stationary, look at its 
ACF and PACF, then try different values of p and q based on that, and 
finally look at the residuals (autocorrelation, distribution, etc).

This is basically the Box-Jenkins methodology. The most accessible 
descriptions I've seen are in "Forecasting: Methods and Applications" by 
Makridakis, Wheelwright and Hyndman (chapter 7), and "Forecasting with 
Univariate Box-Jenkins Models" by Pankratz.

Cheers,
Gad

-- 
Gad Abraham
Department of Mathematics and Statistics
The University of Melbourne
Parkville 3010, Victoria, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From ggrothendieck at gmail.com  Thu Mar  8 00:02:57 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 7 Mar 2007 18:02:57 -0500
Subject: [R] how to avoid to overwrite object
In-Reply-To: <6.2.3.4.2.20070307151716.03926430@aiminy.mail.iastate.edu>
References: <6.2.3.4.2.20070307151716.03926430@aiminy.mail.iastate.edu>
Message-ID: <971536df0703071502x74c00607gc400e801da37baf5@mail.gmail.com>

Decompose your code into small understandable functions.

On 3/7/07, Aimin Yan <aiminy at iastate.edu> wrote:
> Dear R list,
> I have a question in R, it could be very simple, but I don't know how to do it?
>
> for example:
> I assign 6 to x in beginning of of my R script code
>  > x<-6
> ......
> After many line code, I forget using x variable before, I use x
> again, and do assignment like this
>  > x<-45
>  > x
> [1] 45
>
> then value 6 of previous x is replaced by 45.
>
> I am wondering if there are some way R can give me warning like
> "x is used before, overwrite it or not?" when I use x again?
>
> Thanks,
>
> Aimin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Hong.Ooi at iag.com.au  Thu Mar  8 00:23:52 2007
From: Hong.Ooi at iag.com.au (Hong Ooi)
Date: Thu, 8 Mar 2007 10:23:52 +1100
Subject: [R] hwo can i get a vector that...
References: <20493382-3D50-415E-89EA-F0F1C7BDFF22@lautloscrew.com>
Message-ID: <200703072320.l27NKUM3025149@hypatia.math.ethz.ch>


_______________________________________________________________________________________


max.col sounds like what you're after.


-- 
Hong Ooi
Senior Research Analyst, IAG Limited
388 George St, Sydney NSW 2000
+61 (2) 9292 1566
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bunny , lautloscrew.com
Sent: Thursday, 8 March 2007 9:12 AM
To: r-help at stat.math.ethz.ch
Subject: [R] hwo can i get a vector that...

dear all,

how can i get a vector that shows the number of the column of matrix  
that contains the maximum of the row ??
can?t believe in need a loop for this...

i have a  100 x 3 matrix and want to get a 100 x 1 vector with values  
1,2,3 .

there must be a simple solution. i just cannot find it. i think am  
searching on the wrong end.

thx for help in advance.

m.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

_______________________________________________________________________________________

The information transmitted in this message and its attachme...{{dropped}}


From alansmith2 at gmail.com  Thu Mar  8 00:25:54 2007
From: alansmith2 at gmail.com (ALAN SMITH)
Date: Wed, 7 Mar 2007 17:25:54 -0600
Subject: [R] how to "apply" functions to unbalanced data in long format by
	factors......cant get "by" or "aggregate" to work
Message-ID: <e8cddd580703071525t6415ea22sa44cb5ebf60188e2@mail.gmail.com>

Hello R users,

Problem.......I do not understand how to use "aggregate","by", or the
appropriate "apply" to perform a function on data with more than one
factor on unbalanced data...
I have a data frame in the long format that does not contain balanced
data. The ID is a unique identifier corresponding to the experimental
unit that will later be examined by ANOVA, T-tests etc. Y is the data
generated from the experiment.  The factors represent the differences
between each sample or "run" measured.

str(mydata)  ### sample of table at bottom of email ###
'data.frame':   129982 obs. of  6 variables:
 $ ID    : num  7 7 7 7 7 7 8 8 8 8 ...
 $ time     : Factor w/ 2 levels "120hr","24hr": 1 1 1 1 2 2 2 1 1 1 ...
 $ treatment: Factor w/ 2 levels "control","trt": 1 1 1 2 2 1 1 2 1 1 ...
 $ expREP   : Factor w/ 3 levels "expREP1","expREP2",..: 1 1 1 3 1 1 1 1 2 2 ...
 $ techREP  : Factor w/ 3 levels "techREP1","techREP2",..: 3 2 1 1 1 3
1 3 3 2 ...
 $ Y             : num  14.4 14.1 14.2 13.8 14.1 ...

Could someone please help with doing something like the following
1. I would like to find the median for each unique combination of
factors using the data in the   long format (like finding the median
of a single column of data).
2. Create a new column where the median is repeated for the number of
rows of the unique factor combination
3. I would like to learn the most efficient way to do this because I
want to avoid recreating the table from scratch with many commands
like the series below. I will have to perform this operation on many
different data sets some, with many more factors then this example.

### help me learn to use an apply or other command that will do the
following #####
m0<-mydata$cpdID[mydata$time=="24hr" & mydata$treatment=="control" &
mydata$expREP=="expREP1" & mydata$techREP=="techREP1"]
m1<-mydata$Y[mydata$time=="24hr" & mydata$treatment=="control" &
mydata$expREP=="expREP1" & mydata$techREP=="techREP1"]
m2<-median(m1)
m3<-cbind(ID=m0,time=rep("24hr",length(m1)),
treatment=rep("control",length(m1)), expREP=rep("expREP1",length(m1)),
techREP=rep("techREP1",length(m1)),Y=m1,Y50=rep(m2,length(m1)))
######### I would like to avoid writing the above hundreds of times ######

I am able to reshape into wide format and then find the column
medians. However restacking the data and regenerating the factors
becomes very very messy on data sets with 150 columns.  I am able to
preform this analysis is SAS easily using BY, but I would like to know
how to do it in R.


I have tried these commands in a number of different variations with
no luck and similar error messages
 test1<-aggregate(mydata[,-1],
list(mydata$time,mydata$treatment,mydata$expREP,mydata$techREP)
,median, na.rm=T)
Error in median.default(X[[1]], ...) : need numeric data ### Y in numeric####

test1<-by(mydata[,-1],
list(mydata$time,mydata$treatment,mydata$expREP,mydata$techREP)
,median, na.rm=T)
Error in median.default(data[x, ], ...) : need numeric data

Thanks
Alan
winXP R 2.4.1


#####Example data frame######
mydata<-as.data.frame(structure(list(cpdID = c(7, 7, 7, 7, 7, 7, 8, 8,
8, 8, 8, 8,
8, 8, 8, 8, 8, 8, 8, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
10, 10, 10, 10, 10, 10, 19, 19, 19, 19, 19, 19, 23, 23, 23, 23,
23, 23, 23, 23, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,
33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,
33, 33, 33, 33, 33, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,
40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,
40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 42, 42, 42, 42, 42,
42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,
42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,
42, 42, 42, 42, 42, 42, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,
47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47), time = structure(as.integer(c(1,
1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2,
1, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2,
1, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2,
2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2,
2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2,
2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2,
2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1,
1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2,
2, 2, 1, 2, 2, 1, 2, 2, 1, 2)), .Label = c("120hr", "24hr"), class = "factor"),
    treatment = structure(as.integer(c(1, 1, 1, 2, 2, 1, 1, 2,
    1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1,
    2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2,
    1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1,
    1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2,
    2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2,
    1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2,
    2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 1,
    1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2,
    2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1,
    2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1,
    2, 1)), .Label = c("control", "trt"), class = "factor"),
    expREP = structure(as.integer(c(1, 1, 1, 3, 1, 1, 1, 1, 2,
    2, 1, 1, 2, 2, 1, 3, 1, 3, 3, 3, 1, 2, 1, 2, 2, 2, 2, 3,
    3, 2, 2, 1, 2, 3, 3, 1, 1, 2, 3, 1, 3, 3, 3, 3, 1, 3, 1,
    1, 2, 1, 1, 2, 3, 2, 2, 1, 3, 2, 2, 2, 3, 2, 1, 2, 2, 2,
    2, 1, 1, 1, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 3, 1, 2,
    3, 3, 1, 1, 1, 3, 3, 1, 1, 3, 1, 1, 1, 1, 1, 3, 3, 3, 1,
    1, 1, 2, 3, 2, 2, 3, 2, 2, 2, 1, 1, 1, 3, 3, 2, 2, 2, 1,
    3, 1, 2, 3, 1, 3, 3, 1, 2, 3, 1, 2, 1, 3, 1, 3, 3, 2, 2,
    2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 3, 1, 1, 1,
    1, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 3,
    3, 1, 1, 1, 3, 2, 1, 1, 2, 1, 3, 2, 1, 2, 1, 3, 1, 1, 2,
    3)), .Label = c("expREP1", "expREP2", "expREP3"), class = "factor"),
    techREP = structure(as.integer(c(3, 2, 1, 1, 1, 3, 1, 3,
    3, 2, 2, 1, 1, 3, 2, 3, 3, 1, 2, 1, 2, 1, 3, 1, 3, 2, 2,
    3, 1, 1, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 1, 1, 2, 3, 1, 2,
    3, 1, 3, 2, 1, 1, 2, 2, 3, 3, 3, 2, 1, 2, 1, 2, 3, 2, 3,
    2, 3, 1, 3, 2, 2, 1, 2, 1, 3, 2, 2, 1, 1, 3, 3, 3, 1, 3,
    1, 3, 2, 2, 2, 1, 2, 1, 3, 1, 3, 2, 1, 3, 1, 3, 2, 3, 1,
    1, 2, 3, 1, 1, 3, 3, 2, 2, 1, 2, 3, 2, 2, 3, 2, 2, 1, 2,
    2, 3, 3, 3, 1, 1, 1, 3, 1, 1, 2, 3, 2, 3, 3, 1, 1, 3, 2,
    3, 3, 3, 3, 1, 3, 2, 1, 3, 3, 1, 3, 2, 1, 2, 2, 1, 2, 1,
    2, 1, 1, 1, 2, 2, 3, 3, 1, 2, 3, 2, 3, 3, 3, 1, 2, 2, 1,
    3, 2, 3, 3, 2, 2, 2, 3, 2, 1, 3, 1, 3, 1, 3, 1, 1, 1, 2,
    2, 3)), .Label = c("techREP1", "techREP2", "techREP3"), class = "factor"),
    log2Abun = c(14.4233129144089, 14.052822741429, 14.2281422686467,
    13.8492096005693, 14.076481601207, 14.2139395740777, 14.3399195756207,
    14.3625602954496, 14.0141948668145, 14.0980320829605, 14.3152203363759,
    14.4528846974866, 13.9591869268449, 14.4064043323413, 14.0403753485321,
    14.2285932517829, 14.1259784261721, 13.5925738310379, 13.5830827675029,
    13.0280787227049, 15.0198078807043, 12.8423503434138, 12.645883554519,
    13.4644181177386, 12.8399910705399, 12.7879025593914, 12.4978518369511,
    14.3949985145017, 12.8670856466168, 12.9749522735341, 13.3456824481868,
    13.4557125040673, 12.8989792046225, 16.0609491915918, 13.6795900568273,
    16.456466720182, 13.6145948287653, 13.2604785448039, 14.8573006848798,
    13.1382718001722, 13.690761908446, 14.0557060971613, 13.7495552174335,
    13.6336764098923, 13.7844303674846, 15.9518993688317, 13.2452555803066,
    13.1930632791304, 12.1919845133603, 13.8710388986595, 13.6375305515253,
    12.5919897676151, 17.4797250127015, 17.4014712120155, 17.5948202702163,
    12.6031626795344, 17.8287811089804, 11.3613955331659, 15.8064741020529,
    15.1007855146758, 16.0553036215393, 15.7553570530353, 15.9747058600332,
    15.776715745005, 15.8588066550904, 16.2935434944118, 16.271207673964,
    16.3660489506706, 16.3273070282017, 15.7632383068689, 14.6030467398838,
    14.7118820283521, 14.7577545959238, 14.7315311764619, 14.8250084466403,
    15.6652803936783, 15.8249587405285, 15.6558660906456, 15.5387042614836,
    14.8487696278309, 15.5477380355109, 15.9451465974129, 16.196755792715,
    15.9999119421954, 15.8660714836595, 15.9406577104549, 15.8754613979164,
    16.0358944927638, 16.1785092456522, 16.1992122284106, 15.8087128474547,
    15.9373968104322, 16.1432636222427, 16.2412011305004, 15.9488234774507,
    15.7820255767261, 15.7730361533934, 15.7459893802453, 20.7777738189812,
    21.7489122647969, 21.0374490930058, 20.9765158780184, 21.0464959041766,
    21.6790715518273, 21.8021013715842, 20.7652083875471, 20.6663696521617,
    20.3963413756589, 20.7983642126234, 20.1864915044977, 20.4422216681915,
    20.59064186918, 20.6964531077756, 20.6822196619653, 20.4532414913665,
    20.8126113450884, 20.4397608946311, 21.4603719009067, 21.5318145314919,
    21.0400816517662, 21.0466431076593, 20.7459819969019, 20.6723053403015,
    20.4793421418014, 20.6432035537608, 20.6831942471622, 21.6913537667357,
    20.6562913013787, 21.0940693071186, 20.9473294479256, 20.5087271424267,
    16.0871520250047, 16.3816612332698, 16.998645516939, 15.7912392142223,
    14.5058735666446, 13.6035104425928, 14.4369066987207, 14.6998435295626,
    14.6818972267862, 14.1086877961546, 14.3539049235617, 15.40862828087,
    15.0657947671893, 14.8615716011254, 14.5538692431961, 14.2397476835569,
    13.4381420777437, 13.4499224158638, 13.6887966810545, 14.6550275257018,
    13.500966330283, 14.9271297886953, 14.7405186421119, 15.0047910398043,
    14.7051463678038, 14.8325933769599, 12.9854861991046, 13.4203550220891,
    15.399010832952, 15.4064707685293, 15.0953970227926, 15.0712109416537,
    15.7587957644032, 15.0013202225009, 15.7608498673217, 14.7604080920677,
    14.2478533598602, 14.4140245098782, 14.7936541075062, 14.7684428120549,
    14.595607155062, 16.1507389488284, 16.4915712924337, 14.490161446684,
    14.721633263063, 14.4341721012904, 15.8747652729112, 14.543333961671,
    14.8633635585377, 14.6696601802386, 13.3020676725265, 14.0190694293311,
    15.2168973938334, 12.6304946615056, 12.1972166931101, 12.7960396088298,
    14.4285564621952, 14.5308330346953, 14.1496677436943, 14.0823985634278,
    12.8407779235951, 14.6543003749437, 14.3202364452416, 15.1723493709662,
    14.0744760007345, 14.8132801684508, 12.9183042336999, 14.5225202325766,
    13.742309436084)), .Names = c("cpdID", "time", "treatment",
"expREP", "techREP", "Y")))


From jholtman at gmail.com  Thu Mar  8 00:53:24 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 7 Mar 2007 18:53:24 -0500
Subject: [R] Read data with different column lengths
In-Reply-To: <C9C41A7B-13AF-4277-9A5A-A56AB9759112@web.de>
References: <C9C41A7B-13AF-4277-9A5A-A56AB9759112@web.de>
Message-ID: <644e1f320703071553o20310e28xa7492e2ed0512fd3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/e961b78e/attachment.pl 

From jholtman at gmail.com  Thu Mar  8 01:16:37 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 7 Mar 2007 19:16:37 -0500
Subject: [R] how to "apply" functions to unbalanced data in long format
	by factors......cant get "by" or "aggregate" to work
In-Reply-To: <e8cddd580703071525t6415ea22sa44cb5ebf60188e2@mail.gmail.com>
References: <e8cddd580703071525t6415ea22sa44cb5ebf60188e2@mail.gmail.com>
Message-ID: <644e1f320703071616j60e01273oa1c4cd98939f356@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/d71a2a23/attachment.pl 

From rosolem at gmail.com  Thu Mar  8 01:21:52 2007
From: rosolem at gmail.com (Rafael Rosolem)
Date: Wed, 07 Mar 2007 17:21:52 -0700
Subject: [R] Some problems with X11
Message-ID: <45EF5720.40802@gmail.com>

Hi,

I am really new with R, so I don't know anything about it. I have 
written a script (attached) which tries to do really basic stuff (such 
as computing basic statistics and basic plots). When I try to plot a 
histogram and pairs, for example, I get the following message:

 > source("project.R")
Loading required package: sp

-------------------------------------------------------------
Analysis of geostatistical data
For an Introduction to geoR go to http://www.est.ufpr.br/geoR
geoR version 1.6-13 (built on 2006/12/26) is now loaded
-------------------------------------------------------------

Error in title(main = main, sub = sub, xlab = xlab, ylab = ylab, ...) :
         X11 font at size 8 could not be loaded
 >

I have seen some threads about this problem, though none of them really 
states what should be done in order to solve that problem (At least it 
was not clear for me!).

This is the R version I have:
 > version
                _
platform       i486-pc-linux-gnu
arch           i486
os             linux-gnu
system         i486, linux-gnu
status
major          2
minor          4.1
year           2006
month          12
day            18
svn rev        40228
language       R
version.string R version 2.4.1 (2006-12-18)

I am also running R on a Ubuntu Linux Edgy distro. The interesting thing 
is that I have this problem on my desktop only. I also have Ubuntu Edgy 
installed on my laptop and it is working just fine!

Thank you

R

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: project.R
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/0d2b54c2/attachment.pl 

From sue at xlsolutions-corp.com  Thu Mar  8 01:33:47 2007
From: sue at xlsolutions-corp.com (Sue Turner)
Date: Wed, 07 Mar 2007 17:33:47 -0700
Subject: [R] April Courses*** R/S+: Fundamentals and Programming Techniques
	- @ 6 Locations Nationwide / Ann Arbor, Hartford, Raleigh,
	Princeton, Seattle, Boston
Message-ID: <20070307173347.9f08cc34deb45d78e54b3b5664e21546.98624eed3d.wbe@email.secureserver.net>

XLSolutions Corporation is proud to announce our April 2007 R/S:
Fundamentals and Programming Techniques - @ 6 USA locations:
http://www.xlsolutions-corp.com/Rfund.htm


Ann Arbor, MI *** April 23-24, 2007
Raleigh, NC *** April 19-20, 2007
Princeton, NJ *** April 26-27 , 2007
Hartford, CT *** April 26-27 , 2007
Seattle, WA *** April 30- May 1, 2007
Boston, MA *** April 30 - May 1, 2007

This two-day beginner to intermediate R/S-plus course focuses on a broad
spectrum of topics, from reading raw data to a comparison of R and S. We
will learn the essentials of data manipulation, graphical visualization
and R/S-plus programming. We will explore statistical data analysis
tools,including graphics with data sets. How to enhance your plots,
build your own packages (librairies) and connect via ODBC,etc. The
course will give beginners a strong foundation for becoming a versatile
programmer, and will expose experienced users to skills that make a
better programmer.
http://www.xlsolutions-corp.com/Rfund.htm

Other courses:
(1) R/S System: Advanced Programming - San Francisco, March 15-16, 2007
(2) Data Mining: Practical Tools and Techniques in R/Splus - Salt Lake
City, March 26-27, 2007



Ask for group discount and reserve your seat Now - Earlybird Rates.
Please email us for for April-May courses.
Payment due after the class! Email Sue Turner:  sue at xlsolutions-corp.com

(1) R/S System: Advanced Programming - San Francisco, March 15-16, 2007

This advanced course is designed for people who use R or S-Plus in their
day-to-day work and want to maximize the efficiency of their programs.
Participants will learn in depth advanced programming techniques that
are available in R and S-Plus. This course will improve your general
strategies and extend your programming skills. This two-day course will
introduce participants to many programming techniques and tools. In
addition a special session dedicated to making S-Plus functions more
efficient will focus on "fast objects" and "fast functions". The
advanced programming techniques include object orientation, classes,
inheritance and methods. http://www.xlsolutions-corp.com/Radv.htm

(2) Data Mining: Practical Tools and Techniques in R/Splus - Salt Lake
City, March 26-27, 2007

This course gives students an understanding of R/Splus tools used to
investigate the main tasks that predictive analytics and exploratory
data mining is usually called upon to accomplish and data preparation
which is universally held as the key to successful data mining. We
focus on the most common data mining tasks which are: Description,
Estimation, Prediction, Classification, Clustering, Association and the
need for Dimension Reduction with Principal Components and Factor
Analysis. Analytical methods used in the class include decision trees,
logistic regression, neural networks, link analysis (social networks)
and Kernel-based Methods (SVMs).
http://www.xlsolutions-corp.com/RSMining.htm



Email us for group discounts: sue at xlsolutions-corp.com
Phone:  206 686 1578
 
Visit us: www.xlsolutions-corp.com/training.htm
 
Please let us know if you and your colleagues are interested in this
class to take advantage of group discount. Register now to secure your
seat!
 
Cheers,
 
Elvis Miller, PhD
Manager Training
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com/training.htm
elvis at xlsolutions-corp.com


From Scott.Cooley at pnl.gov  Thu Mar  8 01:36:34 2007
From: Scott.Cooley at pnl.gov (Cooley, Scott K)
Date: Wed, 7 Mar 2007 16:36:34 -0800
Subject: [R]  sink with R-code
Message-ID: <DA124317454AAA46BEA4D857E818BEC33E4FFE@EMAIL01.pnl.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/33bc45b9/attachment.pl 

From dunn at usq.edu.au  Thu Mar  8 01:59:52 2007
From: dunn at usq.edu.au (Peter Dunn)
Date: Thu, 8 Mar 2007 10:59:52 +1000
Subject: [R]
	=?iso-8859-1?q?Sweave_issue=3A_quotes_in_verbatim-like_output?=
	=?iso-8859-1?q?_print=09incorrectly?=
In-Reply-To: <873b4hs3s9.fsf@pdrechsler.de>
References: <200703071533.40870.dunn@usq.edu.au>
	<1173260222.3012.17.camel@dhcppc2.my.nat.localnet>
	<873b4hs3s9.fsf@pdrechsler.de>
Message-ID: <200703081059.52617.dunn@usq.edu.au>

Thanks to Gavin Simpson and Patrick Drechsler:
The solution was simple.  (No idea how I would have
discovered these answers without this R mailing list!)

Many thanks.

P.

Gavin Simpson <gavin.simpson at ucl.ac.uk> writes:

> On Wed, 2007-03-07 at 15:33 +1000, Peter Dunn wrote:
>> But I recently received a new computer, and ever since I
>> have had a problem I've never seen before.
>> 
>> For example, I place the following in my Snw file:
>
> Try this in the preamble of your Snw file:
>
> \usepackage[utf8x]{inputenc}

\usepackage[utf8]{inputenc}

should suffic for this. Also, utf8 is newer than utf8x. So if you only
need the ISO 8859 subset of Unicode, utf8 should serve your needs.

HTH

Patrick
-- 
I never used a logarithm in my life, and could not undertake to
extract the square root of four without misgivings.
        (Georg Bernhard Shaw)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

This email (including any attached files) is confidential an...{{dropped}}


From F.MENDIBURU at CGIAR.ORG  Thu Mar  8 02:51:35 2007
From: F.MENDIBURU at CGIAR.ORG (Mendiburu, Felipe (CIP))
Date: Wed, 7 Mar 2007 20:51:35 -0500
Subject: [R] how to "apply" functions to unbalanced data in long format
	byfactors......cant get "by" or "aggregate" to work
References: <e8cddd580703071525t6415ea22sa44cb5ebf60188e2@mail.gmail.com>
Message-ID: <B7B34444ECA41A41AC47DABA057CE7A206FCC7@webmail.cip.cgiar.org>

Dear Alan,
 
I think that podria to be of utility the function tapply.stat () of the package agricolae.
see ?tapply.stat
Regards,
Felipe.
 
for example:
library(agricolae)
attach(mydata)
set1<-tapply.stat(mydata[,2:5],Y,median)
set2<-tapply.stat(time,Y,function(x) median(x))
set3<-tapply.stat(mydata[,c(2,3)],Y,function(x) median(x))
set2
   time        Y
1 120hr 14.94159
2  24hr 14.81914
set3
   time treatment        Y
1 120hr   control 15.31974
2 120hr       trt 14.82851
3  24hr   control 15.03627
4  24hr       trt 14.70249


________________________________

From: r-help-bounces at stat.math.ethz.ch on behalf of ALAN SMITH
Sent: Wed 3/7/2007 6:25 PM
To: r-help at stat.math.ethz.ch
Subject: [R] how to "apply" functions to unbalanced data in long format byfactors......cant get "by" or "aggregate" to work



Hello R users,

Problem.......I do not understand how to use "aggregate","by", or the
appropriate "apply" to perform a function on data with more than one
factor on unbalanced data...
I have a data frame in the long format that does not contain balanced
data. The ID is a unique identifier corresponding to the experimental
unit that will later be examined by ANOVA, T-tests etc. Y is the data
generated from the experiment.  The factors represent the differences
between each sample or "run" measured.

str(mydata)  ### sample of table at bottom of email ###
'data.frame':   129982 obs. of  6 variables:
 $ ID    : num  7 7 7 7 7 7 8 8 8 8 ...
 $ time     : Factor w/ 2 levels "120hr","24hr": 1 1 1 1 2 2 2 1 1 1 ...
 $ treatment: Factor w/ 2 levels "control","trt": 1 1 1 2 2 1 1 2 1 1 ...
 $ expREP   : Factor w/ 3 levels "expREP1","expREP2",..: 1 1 1 3 1 1 1 1 2 2 ...
 $ techREP  : Factor w/ 3 levels "techREP1","techREP2",..: 3 2 1 1 1 3
1 3 3 2 ...
 $ Y             : num  14.4 14.1 14.2 13.8 14.1 ...

Could someone please help with doing something like the following
1. I would like to find the median for each unique combination of
factors using the data in the   long format (like finding the median
of a single column of data).
2. Create a new column where the median is repeated for the number of
rows of the unique factor combination
3. I would like to learn the most efficient way to do this because I
want to avoid recreating the table from scratch with many commands
like the series below. I will have to perform this operation on many
different data sets some, with many more factors then this example.

### help me learn to use an apply or other command that will do the
following #####
m0<-mydata$cpdID[mydata$time=="24hr" & mydata$treatment=="control" &
mydata$expREP=="expREP1" & mydata$techREP=="techREP1"]
m1<-mydata$Y[mydata$time=="24hr" & mydata$treatment=="control" &
mydata$expREP=="expREP1" & mydata$techREP=="techREP1"]
m2<-median(m1)
m3<-cbind(ID=m0,time=rep("24hr",length(m1)),
treatment=rep("control",length(m1)), expREP=rep("expREP1",length(m1)),
techREP=rep("techREP1",length(m1)),Y=m1,Y50=rep(m2,length(m1)))
######### I would like to avoid writing the above hundreds of times ######

I am able to reshape into wide format and then find the column
medians. However restacking the data and regenerating the factors
becomes very very messy on data sets with 150 columns.  I am able to
preform this analysis is SAS easily using BY, but I would like to know
how to do it in R.


I have tried these commands in a number of different variations with
no luck and similar error messages
 test1<-aggregate(mydata[,-1],
list(mydata$time,mydata$treatment,mydata$expREP,mydata$techREP)
,median, na.rm=T)
Error in median.default(X[[1]], ...) : need numeric data ### Y in numeric####

test1<-by(mydata[,-1],
list(mydata$time,mydata$treatment,mydata$expREP,mydata$techREP)
,median, na.rm=T)
Error in median.default(data[x, ], ...) : need numeric data

Thanks
Alan
winXP R 2.4.1


#####Example data frame######
mydata<-as.data.frame(structure(list(cpdID = c(7, 7, 7, 7, 7, 7, 8, 8,
8, 8, 8, 8,
8, 8, 8, 8, 8, 8, 8, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
10, 10, 10, 10, 10, 10, 19, 19, 19, 19, 19, 19, 23, 23, 23, 23,
23, 23, 23, 23, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,
33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,
33, 33, 33, 33, 33, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,
40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,
40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 42, 42, 42, 42, 42,
42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,
42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,
42, 42, 42, 42, 42, 42, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,
47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47), time = structure(as.integer(c(1,
1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2,
1, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2,
1, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2,
2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2,
2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2,
2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2,
2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1,
1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2,
2, 2, 1, 2, 2, 1, 2, 2, 1, 2)), .Label = c("120hr", "24hr"), class = "factor"),
    treatment = structure(as.integer(c(1, 1, 1, 2, 2, 1, 1, 2,
    1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1,
    2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2,
    1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1,
    1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2,
    2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2,
    1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2,
    2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 1,
    1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2,
    2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1,
    2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1,
    2, 1)), .Label = c("control", "trt"), class = "factor"),
    expREP = structure(as.integer(c(1, 1, 1, 3, 1, 1, 1, 1, 2,
    2, 1, 1, 2, 2, 1, 3, 1, 3, 3, 3, 1, 2, 1, 2, 2, 2, 2, 3,
    3, 2, 2, 1, 2, 3, 3, 1, 1, 2, 3, 1, 3, 3, 3, 3, 1, 3, 1,
    1, 2, 1, 1, 2, 3, 2, 2, 1, 3, 2, 2, 2, 3, 2, 1, 2, 2, 2,
    2, 1, 1, 1, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 3, 1, 2,
    3, 3, 1, 1, 1, 3, 3, 1, 1, 3, 1, 1, 1, 1, 1, 3, 3, 3, 1,
    1, 1, 2, 3, 2, 2, 3, 2, 2, 2, 1, 1, 1, 3, 3, 2, 2, 2, 1,
    3, 1, 2, 3, 1, 3, 3, 1, 2, 3, 1, 2, 1, 3, 1, 3, 3, 2, 2,
    2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 3, 1, 1, 1,
    1, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 3,
    3, 1, 1, 1, 3, 2, 1, 1, 2, 1, 3, 2, 1, 2, 1, 3, 1, 1, 2,
    3)), .Label = c("expREP1", "expREP2", "expREP3"), class = "factor"),
    techREP = structure(as.integer(c(3, 2, 1, 1, 1, 3, 1, 3,
    3, 2, 2, 1, 1, 3, 2, 3, 3, 1, 2, 1, 2, 1, 3, 1, 3, 2, 2,
    3, 1, 1, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 1, 1, 2, 3, 1, 2,
    3, 1, 3, 2, 1, 1, 2, 2, 3, 3, 3, 2, 1, 2, 1, 2, 3, 2, 3,
    2, 3, 1, 3, 2, 2, 1, 2, 1, 3, 2, 2, 1, 1, 3, 3, 3, 1, 3,
    1, 3, 2, 2, 2, 1, 2, 1, 3, 1, 3, 2, 1, 3, 1, 3, 2, 3, 1,
    1, 2, 3, 1, 1, 3, 3, 2, 2, 1, 2, 3, 2, 2, 3, 2, 2, 1, 2,
    2, 3, 3, 3, 1, 1, 1, 3, 1, 1, 2, 3, 2, 3, 3, 1, 1, 3, 2,
    3, 3, 3, 3, 1, 3, 2, 1, 3, 3, 1, 3, 2, 1, 2, 2, 1, 2, 1,
    2, 1, 1, 1, 2, 2, 3, 3, 1, 2, 3, 2, 3, 3, 3, 1, 2, 2, 1,
    3, 2, 3, 3, 2, 2, 2, 3, 2, 1, 3, 1, 3, 1, 3, 1, 1, 1, 2,
    2, 3)), .Label = c("techREP1", "techREP2", "techREP3"), class = "factor"),
    log2Abun = c(14.4233129144089, 14.052822741429, 14.2281422686467,
    13.8492096005693, 14.076481601207, 14.2139395740777, 14.3399195756207,
    14.3625602954496, 14.0141948668145, 14.0980320829605, 14.3152203363759,
    14.4528846974866, 13.9591869268449, 14.4064043323413, 14.0403753485321,
    14.2285932517829, 14.1259784261721, 13.5925738310379, 13.5830827675029,
    13.0280787227049, 15.0198078807043, 12.8423503434138, 12.645883554519,
    13.4644181177386, 12.8399910705399, 12.7879025593914, 12.4978518369511,
    14.3949985145017, 12.8670856466168, 12.9749522735341, 13.3456824481868,
    13.4557125040673, 12.8989792046225, 16.0609491915918, 13.6795900568273,
    16.456466720182, 13.6145948287653, 13.2604785448039, 14.8573006848798,
    13.1382718001722, 13.690761908446, 14.0557060971613, 13.7495552174335,
    13.6336764098923, 13.7844303674846, 15.9518993688317, 13.2452555803066,
    13.1930632791304, 12.1919845133603, 13.8710388986595, 13.6375305515253,
    12.5919897676151, 17.4797250127015, 17.4014712120155, 17.5948202702163,
    12.6031626795344, 17.8287811089804, 11.3613955331659, 15.8064741020529,
    15.1007855146758, 16.0553036215393, 15.7553570530353, 15.9747058600332,
    15.776715745005, 15.8588066550904, 16.2935434944118, 16.271207673964,
    16.3660489506706, 16.3273070282017, 15.7632383068689, 14.6030467398838,
    14.7118820283521, 14.7577545959238, 14.7315311764619, 14.8250084466403,
    15.6652803936783, 15.8249587405285, 15.6558660906456, 15.5387042614836,
    14.8487696278309, 15.5477380355109, 15.9451465974129, 16.196755792715,
    15.9999119421954, 15.8660714836595, 15.9406577104549, 15.8754613979164,
    16.0358944927638, 16.1785092456522, 16.1992122284106, 15.8087128474547,
    15.9373968104322, 16.1432636222427, 16.2412011305004, 15.9488234774507,
    15.7820255767261, 15.7730361533934, 15.7459893802453, 20.7777738189812,
    21.7489122647969, 21.0374490930058, 20.9765158780184, 21.0464959041766,
    21.6790715518273, 21.8021013715842, 20.7652083875471, 20.6663696521617,
    20.3963413756589, 20.7983642126234, 20.1864915044977, 20.4422216681915,
    20.59064186918, 20.6964531077756, 20.6822196619653, 20.4532414913665,
    20.8126113450884, 20.4397608946311, 21.4603719009067, 21.5318145314919,
    21.0400816517662, 21.0466431076593, 20.7459819969019, 20.6723053403015,
    20.4793421418014, 20.6432035537608, 20.6831942471622, 21.6913537667357,
    20.6562913013787, 21.0940693071186, 20.9473294479256, 20.5087271424267,
    16.0871520250047, 16.3816612332698, 16.998645516939, 15.7912392142223,
    14.5058735666446, 13.6035104425928, 14.4369066987207, 14.6998435295626,
    14.6818972267862, 14.1086877961546, 14.3539049235617, 15.40862828087,
    15.0657947671893, 14.8615716011254, 14.5538692431961, 14.2397476835569,
    13.4381420777437, 13.4499224158638, 13.6887966810545, 14.6550275257018,
    13.500966330283, 14.9271297886953, 14.7405186421119, 15.0047910398043,
    14.7051463678038, 14.8325933769599, 12.9854861991046, 13.4203550220891,
    15.399010832952, 15.4064707685293, 15.0953970227926, 15.0712109416537,
    15.7587957644032, 15.0013202225009, 15.7608498673217, 14.7604080920677,
    14.2478533598602, 14.4140245098782, 14.7936541075062, 14.7684428120549,
    14.595607155062, 16.1507389488284, 16.4915712924337, 14.490161446684,
    14.721633263063, 14.4341721012904, 15.8747652729112, 14.543333961671,
    14.8633635585377, 14.6696601802386, 13.3020676725265, 14.0190694293311,
    15.2168973938334, 12.6304946615056, 12.1972166931101, 12.7960396088298,
    14.4285564621952, 14.5308330346953, 14.1496677436943, 14.0823985634278,
    12.8407779235951, 14.6543003749437, 14.3202364452416, 15.1723493709662,
    14.0744760007345, 14.8132801684508, 12.9183042336999, 14.5225202325766,
    13.742309436084)), .Names = c("cpdID", "time", "treatment",
"expREP", "techREP", "Y")))

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dataanalytics at earthlink.net  Thu Mar  8 03:04:08 2007
From: dataanalytics at earthlink.net (Walter R. Paczkowski)
Date: Wed, 07 Mar 2007 21:04:08 -0500
Subject: [R] reading a text file with a stray carriage return
Message-ID: <E1HP7xN-0004tz-Oy@elasmtp-junco.atl.sa.earthlink.net>


   Hi,
   I'm  hoping someone has a suggestion for handling a simple problem.  A
   client  gave  me a comma separated value file (call it x.csv) that has
   an  id  and name and address for about 25,000 people (25,000 records).
   I used read.table to read it, but then discovered that there are stray
   carriage returns on several records.  This plays havoc with read.table
   since it starts a new input line when it sees the carriage return.  In
   short, the read is all wrong.
   I thought I could write a simple function to parse a line and write it
   back  out,  character by character.  If a carriage return is found, it
   would  simply  be  ignored on the writing back out part.  But how do I
   identify a carriage return?  What is the code or symbol?  Is there any
   easier  way  to  rid the file of carriage returns in the middle of the
   input lines?
   Any help is appreciated.
   Walt Paczkowski

   _________________________________
   Walter R. Paczkowski, Ph.D.
   Data Analytics Corp.
   44 Hamilton Lane
   Plainsboro, NJ  08536
   (V) 609-936-8999
   (F) 609-936-3733

From osklyar at ebi.ac.uk  Thu Mar  8 03:03:53 2007
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Thu, 08 Mar 2007 02:03:53 +0000
Subject: [R] Passing command line parameters to a script
In-Reply-To: <ada369d00703060011w2ccdfdd2o9d242092d4bb53e5@mail.gmail.com>
References: <ada369d00703060011w2ccdfdd2o9d242092d4bb53e5@mail.gmail.com>
Message-ID: <45EF6F09.4080502@ebi.ac.uk>

#!/bin/sh
echo 'a="${1}"; b="${2}"; source("myRcodeUsing_a_and_b.R"); ' | R 
--vanilla --quiet

and you can run this from shell like:

./callR valueOfa valueOfb

Best, Oleg

akintayo holder wrote:
> Hi,
>  Does any one know if it is possible to create an R script that can use
> command line parameters. I can execute an R script from the command line,
> but I  cannot figure out how to pass parameters to the script. The only
> resources I have found seem somewhat involved or incomplete.
> 
> Any help is appreciated.
> Akintayo
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr Oleg Sklyar | EBI-EMBL, Cambridge CB10 1SD, UK | +44-1223-494466


From jholtman at gmail.com  Thu Mar  8 03:23:20 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 7 Mar 2007 21:23:20 -0500
Subject: [R] reading a text file with a stray carriage return
In-Reply-To: <E1HP7xN-0004tz-Oy@elasmtp-junco.atl.sa.earthlink.net>
References: <E1HP7xN-0004tz-Oy@elasmtp-junco.atl.sa.earthlink.net>
Message-ID: <644e1f320703071823h2d4b8e5cpabfda6fce9db5e47@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/898e438f/attachment.pl 

From mervyn at iastate.edu  Thu Mar  8 03:35:30 2007
From: mervyn at iastate.edu (Mervyn G Marasinghe)
Date: Wed, 07 Mar 2007 20:35:30 -0600
Subject: [R] Calling Optim() from C
Message-ID: <200703080235.l282ZUO6010890@mailhub-3.iastate.edu>

Hello:

I am sure this question was dealt with several years ago.  Is the function
vmmin()  available  from Rmath Standalone? If not is it possible  to call
optim()  or  nlm()  from Rmath in C.  Thank you.

Mervyn


From stgries_lists at arcor.de  Thu Mar  8 04:25:48 2007
From: stgries_lists at arcor.de (Stefan Th. Gries)
Date: Thu, 8 Mar 2007 04:25:48 +0100 (CET)
Subject: [R] Named backreferences in replacement patterns
Message-ID: <733281.1173324348165.JavaMail.ngmail@webmail12>

Hi

I have a problem with substitutions involving named backreferences. I
have a vector American.dates:

> American.dates
[1] "5/15/1976" "2.15.1970" "1.9.2006"

which I want to change into British.dates:

> British.dates
[1] "15/5/1976" "15/2/1970" "9/1/2006"

I know I can do it like this:

British.dates<-sub("(\\d{1,2})\\D(\\d{1,2})\\D", "\\2/\\1/",
American.dates, perl=T)

But I want to use named backreferences. I was trying something like
this

British.dates<-sub("(?P<MONTH>\\d{1,2})\\D(?P<DAY>\\d{1,2})\\D",
"...", American.dates, perl=T)

but the problem is the ... I know I could use the named backreferences
*in the search pattern* with (?P=MONTH), but how do I use them *in the
replacement pattern"? I didn't get the Python solution to work:

> (British.dates<-sub("(?P<MONTH>\\d{1,2})\\D(?P<DAY>\\d{1,2})\\D", "'\g<MONTH>\g/\\1/", American.dates, perl=T))
[1] "'g<MONTH>g/5/1976" "'g<MONTH>g/2/1970" "'g<MONTH>g/1/2006"

> (British.dates<-sub("(?P<MONTH>\\d{1,2})\\D(?P<DAY>\\d{1,2})\\D", "'\\g<MONTH>\\g/\\1/", American.dates, perl=T))
[1] "'g<MONTH>g/5/1976" "'g<MONTH>g/2/1970" "'g<MONTH>g/1/2006"

I know I can use the numbers again, but then what would be the point
of having used names in the first place ...

Any ideas? Thx a bunch,
STG


From xmeng at capitalbio.com  Thu Mar  8 05:14:17 2007
From: xmeng at capitalbio.com (XinMeng)
Date: Thu, 08 Mar 2007 12:14:17 +0800
Subject: [R] the legend of heatmap.2
Message-ID: <373327257.00828@capitalbio.com>

Hi all:
As to the legend of heatmap.2,it seems that the legend can only show the distribution of Z score.
If I wanna the legend show the distribution of the original data,how can I do it?

Thanks a lot for your help.

My best!


From tkobayas at indiana.edu  Thu Mar  8 05:22:07 2007
From: tkobayas at indiana.edu (Takatsugu Kobayashi)
Date: Wed, 07 Mar 2007 23:22:07 -0500
Subject: [R] help for efficient loop
Message-ID: <45EF8F6F.4040500@indiana.edu>

Hi,

I have been trying to minimize computation times in the following loops. 
I could successfully use lapply to minimize a lot simpler stuff. So I am 
trying to use lapply or sapply to minimize computing times again.

The whole purpose is to create the X and Y coordinates using normal 
distribution and compute local standard distances. Local by which I mean 
is that a group of observation points are selected by distance 
thresholds of the reference points.

# Normally distributed X-Y Coordinates with hypothetical z values
pts<-500 # Number of observations =n
cases<-10 # Number of variables
x<-rnorm(pts)
y<-rnorm(pts)
z<-matrix(abs(rnorm(pts*cases)),pts,cases)

# Combine x, y, and zs
Ldata<-cbind(x,y,z)    # n*(2+p) matrix p=# of variables 2=X and Y

# Compute the Euclidean distances between points
disE<-data.matrix(dist(cbind(x,y)))

# Create a series of values that act as a threshold
thrsE<-seq(1,max(disE),by=0.5)

# Compute local mean centers and median centers of the nearest neighbors 
within the distance threshold of n reference points
LMNX<-matrix(,pts,length(thrsE))    # local mean X
LMNY<-matrix(,pts,length(thrsE))    # local mean Y
LMDX<-matrix(,pts,length(thrsE))    # local median X
LMDY<-matrix(,pts,length(thrsE))    # local median Y
LSDMN<-rep(list(matrix(,pts,length(thrsE))),cases)

# Then compute standard distances of the Zs of the neighbors within the 
distance thresholds of n reference points
for (j in 1:pts){   
for (k in 1:length(thrsE)){
    LMNX[j,k]<-mean(Ldata[as.vector(which(disE[j,]<=thrsE[k])),1])
    LMNY[j,k]<-mean(Ldata[as.vector(which(disE[j,]<=thrsE[k])),2])
    LMDX[j,k]<-median(Ldata[as.vector(which(disE[j,]<=thrsE[k])),1])
    LMDY[j,k]<-median(Ldata[as.vector(which(disE[j,]<=thrsE[k])),2])
for (l in 1:cases){
    
LSDMN[[l]][j,k]<-sqrt(sum(Ldata[as.vector(which(disE[j,]<=thrsE[k])),2+l]*(Ldata[as.vector(which(disE[j,]<=thrsE[k])),1]-LMNX[j,k])^2+
            
Ldata[as.vector(which(disE[j,]<=thrsE[k])),2+l]*(Ldata[as.vector(which(disE[j,]<=thrsE[k])),2]-LMNY[j,k])^2)/sum(Ldata[as.vector(which(disE[j,]<=thrsE[k])),2+l]))
}}}

I believe I should use lapply or sapply in this loop to minimize 
computing times because my way is to allocate computed values at [j,k] 
of the big matrix.... I have tried using lapply, but I am not sure how I 
can define higher arrays that work with lapply...

many many thanks in advance.

Taka
Indiana University


From dingjun_cn at yahoo.com  Thu Mar  8 05:47:25 2007
From: dingjun_cn at yahoo.com (Jun Ding)
Date: Wed, 7 Mar 2007 20:47:25 -0800 (PST)
Subject: [R] alpha parameter in function rgb to specify color
Message-ID: <803357.68520.qm@web81010.mail.mud.yahoo.com>

Hi All, 

In function "rgb", alpha parameter is supposed to set
the transparency value. But in my following two
examples, it didn't work:

plot(1,col = rgb(1,0,0,alpha =0.8))
# as long as alpha < 1, there is no point in the plot.

plot(1,col = rgb(0,0,255, alpha=254,
maxColorValue=255))
# as long as alpha < 255, there is no point in the
plot. 

Do I use it in the right way? Any advice is
appreciated. 

Best,

Jun Ding


 
____________________________________________________________________________________
Expecting? Get great news right away with email Auto-Check.


From mwtoews at sfu.ca  Thu Mar  8 06:03:42 2007
From: mwtoews at sfu.ca (Michael Toews)
Date: Wed, 07 Mar 2007 21:03:42 -0800
Subject: [R]  plot(): I want to display dates on X-axis.
Message-ID: <45EF992E.2020905@sfu.ca>

>
> e.g. dat
>
>                  [,1]                        dat[,2]
>
> [1,]            300                       20060101
> [2,]            257                       20060102
> [3,]            320                       20060103
> [4,]            311                       20060104
> [5,]            297                       20060105
> [6,]            454                       20060106
> [7,]            360                       20060107
> [8,]            307                       20060108
> ....
>   
This looks like a matrix ... not a data frame. You defiantly want a data 
frame. So lets say you have:
dat <- matrix(c(round(rnorm(8)*100+400),20060101:20060108),ncol=2)

#convert it:
dat <- as.data.frame(dat)

#determine the dates:
dat$date <- as.Date(as.character(dat$V2),"%Y%m%d")

#plot it:
plot(V1 ~ date, dat)


From mwtoews at sfu.ca  Thu Mar  8 06:12:16 2007
From: mwtoews at sfu.ca (Michael Toews)
Date: Wed, 07 Mar 2007 21:12:16 -0800
Subject: [R]  Named backreferences in replacement patterns
Message-ID: <45EF9B30.5000104@sfu.ca>

How about turning them into a native date-time class, then re-formatting it.
For example, say you have some American dates in a "character" vector:
American.datechar <- c("5/15/1976","2/15/1970","1/9/2006")
# parse this:
American.date <- strptime(American.datechar,"%m/%d/%Y")
# reformat:
format(American.date,"%d/%m/%Y")


From Greg.Snow at intermountainmail.org  Thu Mar  8 06:21:21 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Wed, 7 Mar 2007 22:21:21 -0700
Subject: [R] alpha parameter in function rgb to specify color
References: <803357.68520.qm@web81010.mail.mud.yahoo.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB12A115@LP-EXCHVS07.CO.IHC.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070307/d3014f88/attachment.pl 

From mwtoews at sfu.ca  Thu Mar  8 06:28:29 2007
From: mwtoews at sfu.ca (Michael Toews)
Date: Wed, 07 Mar 2007 21:28:29 -0800
Subject: [R]  alpha parameter in function rgb to specify color
Message-ID: <45EF9EFD.40601@sfu.ca>

Part of the problem is that alpha is a new and undocumented feature 
(note to developers: add this info into the pdf and rgb documentation). 
It only works in PDFs (maybe on quartz on Macs?), so you need to write 
to a PDF file:

pdf("out.pdf",version="1.4")
plot(1:10,col=rgb(1,0,0,alpha=seq(0,1,length.out=10)))
dev.off()

Now open up the PDF file...


From dingjun_cn at yahoo.com  Thu Mar  8 07:00:02 2007
From: dingjun_cn at yahoo.com (Jun Ding)
Date: Wed, 7 Mar 2007 22:00:02 -0800 (PST)
Subject: [R] alpha parameter in function rgb to specify color
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB12A115@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <258799.90810.qm@web81006.mail.mud.yahoo.com>

Hi Greg, 

I am not sure if I understand you correctly. 
I am using the "plot" command directly in R Console in
the windows version of R and I also have tried it in
the linux version of R. 

Thanks a lot for your help!

Best,
Jun

--- Greg Snow <Greg.Snow at intermountainmail.org> wrote:

> Which graphics device are you using?
> 
> ________________________________
> 
> From: r-help-bounces at stat.math.ethz.ch on behalf of
> Jun Ding
> Sent: Wed 3/7/2007 9:47 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] alpha parameter in function rgb to
> specify color
> 
> 
> 
> Hi All,
> 
> In function "rgb", alpha parameter is supposed to
> set
> the transparency value. But in my following two
> examples, it didn't work:
> 
> plot(1,col = rgb(1,0,0,alpha =0.8))
> # as long as alpha < 1, there is no point in the
> plot.
> 
> plot(1,col = rgb(0,0,255, alpha=254,
> maxColorValue=255))
> # as long as alpha < 255, there is no point in the
> plot.
> 
> Do I use it in the right way? Any advice is
> appreciated.
> 
> Best,
> 
> Jun Ding
> 
> 
> 
>
____________________________________________________________________________________
> Expecting? Get great news right away with email
> Auto-Check.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
> 
> 
> 



 
____________________________________________________________________________________
Don't pick lemons.


From petr.pikal at precheza.cz  Thu Mar  8 08:00:53 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 08 Mar 2007 08:00:53 +0100
Subject: [R] how to "apply" functions to unbalanced data in long format
	by	factors......cant get "by" or "aggregate" to work
In-Reply-To: <e8cddd580703071525t6415ea22sa44cb5ebf60188e2@mail.gmail.com>
Message-ID: <45EFC2B5.29775.2FD750@localhost>

Hi

you can use aggregate to create table of medians

with(mydata, aggregate(Y, list(time, tratment, expRep,....), median)

repeats of unique factors
either by rle or aggregate with length function

Then you can do replication by

norep <- rep(your.median, each = your replicates)

Regards
Petr

On 7 Mar 2007 at 17:25, ALAN SMITH wrote:

Date sent:      	Wed, 7 Mar 2007 17:25:54 -0600
From:           	"ALAN SMITH" <alansmith2 at gmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] how to "apply" functions to unbalanced data in long format by
	factors......cant get "by" or "aggregate" to work

> Hello R users,
> 
> Problem.......I do not understand how to use "aggregate","by", or the
> appropriate "apply" to perform a function on data with more than one
> factor on unbalanced data... I have a data frame in the long format
> that does not contain balanced data. The ID is a unique identifier
> corresponding to the experimental unit that will later be examined by
> ANOVA, T-tests etc. Y is the data generated from the experiment.  The
> factors represent the differences between each sample or "run"
> measured.
> 
> str(mydata)  ### sample of table at bottom of email ###
> 'data.frame':   129982 obs. of  6 variables:
>  $ ID    : num  7 7 7 7 7 7 8 8 8 8 ...
>  $ time     : Factor w/ 2 levels "120hr","24hr": 1 1 1 1 2 2 2 1 1 1
>  ... $ treatment: Factor w/ 2 levels "control","trt": 1 1 1 2 2 1 1 2
>  1 1 ... $ expREP   : Factor w/ 3 levels "expREP1","expREP2",..: 1 1 1
>  3 1 1 1 1 2 2 ... $ techREP  : Factor w/ 3 levels
>  "techREP1","techREP2",..: 3 2 1 1 1 3
> 1 3 3 2 ...
>  $ Y             : num  14.4 14.1 14.2 13.8 14.1 ...
> 
> Could someone please help with doing something like the following 1. I
> would like to find the median for each unique combination of factors
> using the data in the   long format (like finding the median of a
> single column of data). 2. Create a new column where the median is
> repeated for the number of rows of the unique factor combination 3. I
> would like to learn the most efficient way to do this because I want
> to avoid recreating the table from scratch with many commands like the
> series below. I will have to perform this operation on many different
> data sets some, with many more factors then this example.
> 
> ### help me learn to use an apply or other command that will do the
> following #####
> m0<-mydata$cpdID[mydata$time=="24hr" & mydata$treatment=="control" &
> mydata$expREP=="expREP1" & mydata$techREP=="techREP1"]
> m1<-mydata$Y[mydata$time=="24hr" & mydata$treatment=="control" &
> mydata$expREP=="expREP1" & mydata$techREP=="techREP1"] m2<-median(m1)
> m3<-cbind(ID=m0,time=rep("24hr",length(m1)),
> treatment=rep("control",length(m1)), expREP=rep("expREP1",length(m1)),
> techREP=rep("techREP1",length(m1)),Y=m1,Y50=rep(m2,length(m1)))
> ######### I would like to avoid writing the above hundreds of times
> ######
> 
> I am able to reshape into wide format and then find the column
> medians. However restacking the data and regenerating the factors
> becomes very very messy on data sets with 150 columns.  I am able to
> preform this analysis is SAS easily using BY, but I would like to know
> how to do it in R.
> 
> 
> I have tried these commands in a number of different variations with
> no luck and similar error messages
>  test1<-aggregate(mydata[,-1],
> list(mydata$time,mydata$treatment,mydata$expREP,mydata$techREP)
> ,median, na.rm=T)
> Error in median.default(X[[1]], ...) : need numeric data ### Y in
> numeric####
> 
> test1<-by(mydata[,-1],
> list(mydata$time,mydata$treatment,mydata$expREP,mydata$techREP)
> ,median, na.rm=T)
> Error in median.default(data[x, ], ...) : need numeric data
> 
> Thanks
> Alan
> winXP R 2.4.1
> 
> 
> #####Example data frame######
> mydata<-as.data.frame(structure(list(cpdID = c(7, 7, 7, 7, 7, 7, 8, 8,
> 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 10, 10, 10, 10, 10, 10, 10, 10, 10,
> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
> 10, 10, 10, 10, 10, 10, 19, 19, 19, 19, 19, 19, 23, 23, 23, 23, 23,
> 23, 23, 23, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,
> 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,
> 33, 33, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,
> 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,
> 40, 40, 40, 40, 40, 40, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,
> 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,
> 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 47, 47,
> 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,
> 47, 47), time = structure(as.integer(c(1, 1, 1, 1, 2, 2, 2, 1, 1, 1,
> 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2,
> 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2,
> 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1,
> 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1,
> 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2,
> 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
> 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1,
> 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2,
> 2, 1, 2, 2, 1, 2)), .Label = c("120hr", "24hr"), class = "factor"),
>     treatment = structure(as.integer(c(1, 1, 1, 2, 2, 1, 1, 2,
>     1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1,
>     2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2,
>     1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1,
>     1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2,
>     2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2,
>     1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2,
>     2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 1,
>     1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2,
>     2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1,
>     2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1,
>     2, 1)), .Label = c("control", "trt"), class = "factor"),
>     expREP = structure(as.integer(c(1, 1, 1, 3, 1, 1, 1, 1, 2,
>     2, 1, 1, 2, 2, 1, 3, 1, 3, 3, 3, 1, 2, 1, 2, 2, 2, 2, 3,
>     3, 2, 2, 1, 2, 3, 3, 1, 1, 2, 3, 1, 3, 3, 3, 3, 1, 3, 1,
>     1, 2, 1, 1, 2, 3, 2, 2, 1, 3, 2, 2, 2, 3, 2, 1, 2, 2, 2,
>     2, 1, 1, 1, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 3, 1, 2,
>     3, 3, 1, 1, 1, 3, 3, 1, 1, 3, 1, 1, 1, 1, 1, 3, 3, 3, 1,
>     1, 1, 2, 3, 2, 2, 3, 2, 2, 2, 1, 1, 1, 3, 3, 2, 2, 2, 1,
>     3, 1, 2, 3, 1, 3, 3, 1, 2, 3, 1, 2, 1, 3, 1, 3, 3, 2, 2,
>     2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 3, 1, 1, 1,
>     1, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 3,
>     3, 1, 1, 1, 3, 2, 1, 1, 2, 1, 3, 2, 1, 2, 1, 3, 1, 1, 2,
>     3)), .Label = c("expREP1", "expREP2", "expREP3"), class =
>     "factor"), techREP = structure(as.integer(c(3, 2, 1, 1, 1, 3, 1,
>     3, 3, 2, 2, 1, 1, 3, 2, 3, 3, 1, 2, 1, 2, 1, 3, 1, 3, 2, 2, 3, 1,
>     1, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 1, 1, 2, 3, 1, 2, 3, 1, 3, 2, 1,
>     1, 2, 2, 3, 3, 3, 2, 1, 2, 1, 2, 3, 2, 3, 2, 3, 1, 3, 2, 2, 1, 2,
>     1, 3, 2, 2, 1, 1, 3, 3, 3, 1, 3, 1, 3, 2, 2, 2, 1, 2, 1, 3, 1, 3,
>     2, 1, 3, 1, 3, 2, 3, 1, 1, 2, 3, 1, 1, 3, 3, 2, 2, 1, 2, 3, 2, 2,
>     3, 2, 2, 1, 2, 2, 3, 3, 3, 1, 1, 1, 3, 1, 1, 2, 3, 2, 3, 3, 1, 1,
>     3, 2, 3, 3, 3, 3, 1, 3, 2, 1, 3, 3, 1, 3, 2, 1, 2, 2, 1, 2, 1, 2,
>     1, 1, 1, 2, 2, 3, 3, 1, 2, 3, 2, 3, 3, 3, 1, 2, 2, 1, 3, 2, 3, 3,
>     2, 2, 2, 3, 2, 1, 3, 1, 3, 1, 3, 1, 1, 1, 2, 2, 3)), .Label =
>     c("techREP1", "techREP2", "techREP3"), class = "factor"), log2Abun
>     = c(14.4233129144089, 14.052822741429, 14.2281422686467,
>     13.8492096005693, 14.076481601207, 14.2139395740777,
>     14.3399195756207, 14.3625602954496, 14.0141948668145,
>     14.0980320829605, 14.3152203363759, 14.4528846974866,
>     13.9591869268449, 14.4064043323413, 14.0403753485321,
>     14.2285932517829, 14.1259784261721, 13.5925738310379,
>     13.5830827675029, 13.0280787227049, 15.0198078807043,
>     12.8423503434138, 12.645883554519, 13.4644181177386,
>     12.8399910705399, 12.7879025593914, 12.4978518369511,
>     14.3949985145017, 12.8670856466168, 12.9749522735341,
>     13.3456824481868, 13.4557125040673, 12.8989792046225,
>     16.0609491915918, 13.6795900568273, 16.456466720182,
>     13.6145948287653, 13.2604785448039, 14.8573006848798,
>     13.1382718001722, 13.690761908446, 14.0557060971613,
>     13.7495552174335, 13.6336764098923, 13.7844303674846,
>     15.9518993688317, 13.2452555803066, 13.1930632791304,
>     12.1919845133603, 13.8710388986595, 13.6375305515253,
>     12.5919897676151, 17.4797250127015, 17.4014712120155,
>     17.5948202702163, 12.6031626795344, 17.8287811089804,
>     11.3613955331659, 15.8064741020529, 15.1007855146758,
>     16.0553036215393, 15.7553570530353, 15.9747058600332,
>     15.776715745005, 15.8588066550904, 16.2935434944118,
>     16.271207673964, 16.3660489506706, 16.3273070282017,
>     15.7632383068689, 14.6030467398838, 14.7118820283521,
>     14.7577545959238, 14.7315311764619, 14.8250084466403,
>     15.6652803936783, 15.8249587405285, 15.6558660906456,
>     15.5387042614836, 14.8487696278309, 15.5477380355109,
>     15.9451465974129, 16.196755792715, 15.9999119421954,
>     15.8660714836595, 15.9406577104549, 15.8754613979164,
>     16.0358944927638, 16.1785092456522, 16.1992122284106,
>     15.8087128474547, 15.9373968104322, 16.1432636222427,
>     16.2412011305004, 15.9488234774507, 15.7820255767261,
>     15.7730361533934, 15.7459893802453, 20.7777738189812,
>     21.7489122647969, 21.0374490930058, 20.9765158780184,
>     21.0464959041766, 21.6790715518273, 21.8021013715842,
>     20.7652083875471, 20.6663696521617, 20.3963413756589,
>     20.7983642126234, 20.1864915044977, 20.4422216681915,
>     20.59064186918, 20.6964531077756, 20.6822196619653,
>     20.4532414913665, 20.8126113450884, 20.4397608946311,
>     21.4603719009067, 21.5318145314919, 21.0400816517662,
>     21.0466431076593, 20.7459819969019, 20.6723053403015,
>     20.4793421418014, 20.6432035537608, 20.6831942471622,
>     21.6913537667357, 20.6562913013787, 21.0940693071186,
>     20.9473294479256, 20.5087271424267, 16.0871520250047,
>     16.3816612332698, 16.998645516939, 15.7912392142223,
>     14.5058735666446, 13.6035104425928, 14.4369066987207,
>     14.6998435295626, 14.6818972267862, 14.1086877961546,
>     14.3539049235617, 15.40862828087, 15.0657947671893,
>     14.8615716011254, 14.5538692431961, 14.2397476835569,
>     13.4381420777437, 13.4499224158638, 13.6887966810545,
>     14.6550275257018, 13.500966330283, 14.9271297886953,
>     14.7405186421119, 15.0047910398043, 14.7051463678038,
>     14.8325933769599, 12.9854861991046, 13.4203550220891,
>     15.399010832952, 15.4064707685293, 15.0953970227926,
>     15.0712109416537, 15.7587957644032, 15.0013202225009,
>     15.7608498673217, 14.7604080920677, 14.2478533598602,
>     14.4140245098782, 14.7936541075062, 14.7684428120549,
>     14.595607155062, 16.1507389488284, 16.4915712924337,
>     14.490161446684, 14.721633263063, 14.4341721012904,
>     15.8747652729112, 14.543333961671, 14.8633635585377,
>     14.6696601802386, 13.3020676725265, 14.0190694293311,
>     15.2168973938334, 12.6304946615056, 12.1972166931101,
>     12.7960396088298, 14.4285564621952, 14.5308330346953,
>     14.1496677436943, 14.0823985634278, 12.8407779235951,
>     14.6543003749437, 14.3202364452416, 15.1723493709662,
>     14.0744760007345, 14.8132801684508, 12.9183042336999,
>     14.5225202325766, 13.742309436084)), .Names = c("cpdID", "time",
>     "treatment",
> "expREP", "techREP", "Y")))
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From A.Robinson at ms.unimelb.edu.au  Thu Mar  8 08:13:53 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Thu, 8 Mar 2007 18:13:53 +1100
Subject: [R] Calculating confidence limits on acf graphs
In-Reply-To: <07DFC883258B4E4ABA3DC11CF025F23203B4C4FB@Exukmb34.eur.nsroot.net>
References: <07DFC883258B4E4ABA3DC11CF025F23203B4C4FB@Exukmb34.eur.nsroot.net>
Message-ID: <20070308071352.GO41291@ms.unimelb.edu.au>

Hi Phil,

try

getAnywhere("plot.acf")

you can see how the CI is computed there.

Cheers

Andrew

On Wed, Mar 07, 2007 at 02:12:16PM +0000, Gladwin, Philip [CIB-FI] wrote:
> Hello,
> I was wondering if anybody could help me with this?
> 
> I have plotted an acf function for a time series and am very happy with it.
> Now I am interested in calculating for myself the two values for the confidence
> intervals that are plotted on the graph of the acf.
> 
> The confidence intervals do not appear to be returned from the acf function (is this true?).
> 
> So far I haven't managed to calculate them myself.  Can anybody help?
> 
> Phil,
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From ligges at statistik.uni-dortmund.de  Thu Mar  8 08:26:59 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 08 Mar 2007 08:26:59 +0100
Subject: [R] ATLAS for Pentium D and Pentium Duo Core?
In-Reply-To: <54794ae90703071237h5490262fnb453b0cc36a10f0d@mail.gmail.com>
References: <54794ae90703071237h5490262fnb453b0cc36a10f0d@mail.gmail.com>
Message-ID: <45EFBAC3.2090602@statistik.uni-dortmund.de>



Jonathan Zhang wrote:
> Dear all,
> 
>   I wanted to use ATLAS to increase the computational speed of my
> program. I have installed R in a straightfoward way with .exe file
> (not building from source).
> 
>   However, after getting the P4 ATLAS Rblas.dll file from the CRAN
> directory bin/windows/contrib/ATLAS, and replacing the default
> Rblas.dll file in my R home directory under bin/, the speed actually
> decreased by 3% compared with the default Rblas file!!


Such a 3% thing might happen even for the right BLAS given you are not 
doing heavy vector/matrix operations.


>   So what am I doing wrong? Is the above the correct procedure for
> using ATLAS? Or, is my machine not a Pentium 4 and hence the ATLAS
> file that I got from CRAN is not appropriate?
> 
>   If the issue lies in the processor, then please kindly let me know
> where to get the correct ATLAS file for the following computers:
> 
>  IThe first one is Pentium D 3.2 ghz, the second is a MacBook 1.83ghz
> dual-boot and I am running R in windows.

The MacBook is running Windows? So it is a newer one containing some 
Intel Core2Duo CPU?

For the Pentium D, the P4 ATLAS should be more or less appropriate. Of 
course, you can compile one that fits even better to your system.
For the MacBook, we do not have an appropriate BLAS on CRAN. What you 
could try is the Pentium M BLAS (which should come close), but we cannot 
guarantee stability nor performance.
We are looking for volunteers to compile a BLAS for each architecture, 
so please contribute!

Uwe Ligges


>   Thank you!
> 
> Jonathan Zhang
> 
> Marketing Division
> Columbia Business School
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From i.visser at uva.nl  Thu Mar  8 09:13:05 2007
From: i.visser at uva.nl (Ingmar Visser)
Date: Thu, 8 Mar 2007 09:13:05 +0100
Subject: [R] Calling Optim() from C
In-Reply-To: <200703080235.l282ZUO6010890@mailhub-3.iastate.edu>
References: <200703080235.l282ZUO6010890@mailhub-3.iastate.edu>
Message-ID: <B6CA3183-B0C7-47EE-95FF-E4EE664957A3@uva.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070308/1605d898/attachment.pl 

From lists at eva.mpg.de  Thu Mar  8 10:16:56 2007
From: lists at eva.mpg.de (lists at eva.mpg.de)
Date: Thu, 08 Mar 2007 10:16:56 +0100
Subject: [R] Error distribution question
Message-ID: <45EFD488.4050503@eva.mpg.de>

Hello there,
I was wondering if somebody could offer me some advice on which error 
distribution would be appropriate for the type of data I have. I'm 
studying what continuous predictor variables such as grooming received, 
rank, etc. affect the amount of grooming given. This response variable 
is continuous with many zeros, and so positively skewed. So I can't use 
gamma because of the zeros and since poisson is for count data I would 
probably loose lots of info when converting my data to integers. I was 
looking in the help archive of R and I realized that somebody had a 
similar problem in the past and was adviced'  to use the Tweedie 
distribution since it was developed specifically for dealing with 
positive continuous data with exact zeros. I think this would be 
appropriate for my problem as well, but I'm not sure. I realized in the 
Tweedie help page that one can use a specific response distribution 
(Normal, Poisson, Compound Poisson, etc) by setting the variance power = 
to a specific number. I'm a beginner, so I really don't follow then, 
which response distribution to use (i.e. what variance power) that would 
be appropriate for continuous response data with many zeros.
I hope somebody can help me with this.
Thanks in advance.
Cheers,
Cristina.


From j.van_den_hoff at fzd.de  Thu Mar  8 10:18:39 2007
From: j.van_den_hoff at fzd.de (Joerg van den Hoff)
Date: Thu, 8 Mar 2007 10:18:39 +0100
Subject: [R] Some problems with X11
In-Reply-To: <45EF5720.40802@gmail.com>
References: <45EF5720.40802@gmail.com>
Message-ID: <20070308091839.GA11550@marco.fz-rossendorf.de>

On Wed, Mar 07, 2007 at 05:21:52PM -0700, Rafael Rosolem wrote:
> Hi,
> 
> I am really new with R, so I don't know anything about it. I have 
> written a script (attached) which tries to do really basic stuff (such 
> as computing basic statistics and basic plots). When I try to plot a 
> histogram and pairs, for example, I get the following message:
> 
> > source("project.R")
> Loading required package: sp
> 
> -------------------------------------------------------------
> Analysis of geostatistical data
> For an Introduction to geoR go to http://www.est.ufpr.br/geoR
> geoR version 1.6-13 (built on 2006/12/26) is now loaded
> -------------------------------------------------------------
> 
> Error in title(main = main, sub = sub, xlab = xlab, ylab = ylab, ...) :
>         X11 font at size 8 could not be loaded
> >
> 
> I have seen some threads about this problem, though none of them really 
> states what should be done in order to solve that problem (At least it 
> was not clear for me!).
well, some X11 font is not installed/not found. 

in `R', getOpton("X11fonts") tells you which font family `X11()' is using by
default (but you can alter this in the `X11' call if need be) and the size 8
subtype seems to be missing.

you may compare the above with the output from `xlsfonts' in the unix shell,
or use `xfontsel' to browse interactively through the installed fonts
to verify whether `R's complaining is justified.

probable solution: use some existing sufficiently complete (different sizes)
font-family  in the `X11()' call or copy over the default fonts from the labtop
where it works.

hth,
joerg
> 
> I am also running R on a Ubuntu Linux Edgy distro. The interesting thing 
> is that I have this problem on my desktop only. I also have Ubuntu Edgy 
> installed on my laptop and it is working just fine!
> 
> Thank you


From lists at eva.mpg.de  Thu Mar  8 10:19:34 2007
From: lists at eva.mpg.de (lists at eva.mpg.de)
Date: Thu, 08 Mar 2007 10:19:34 +0100
Subject: [R] Error distribution question
Message-ID: <45EFD526.4060109@eva.mpg.de>

Hello there,
I was wondering if somebody could offer me some advice on which error 
distribution would be appropriate for the type of data I have. I'm 
studying what continuous predictor variables such as grooming received, 
rank, etc. affect the amount of grooming given. This response variable 
is continuous with many zeros, and so positively skewed. So I can't use 
gamma because of the zeros and since poisson is for count data I would 
probably loose lots of info when converting my data to integers. I was 
looking in the help archive of R and I realized that somebody had a 
similar problem in the past and was adviced'  to use the Tweedie 
distribution since it was developed specifically for dealing with 
positive continuous data with exact zeros. I think this would be 
appropriate for my problem as well, but I'm not sure. I realized in the 
Tweedie help page that one can use a specific response distribution 
(Normal, Poisson, Compound Poisson, etc) by setting the variance power = 
to a specific number. I'm a beginner, so I really don't follow then, 
which response distribution to use (i.e. what variance power) that would 
be appropriate for continuous response data with many zeros.
I hope somebody can help me with this.
Thanks in advance.
Cheers,
Cristina.


From jim at bitwrit.com.au  Thu Mar  8 11:03:41 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Thu, 08 Mar 2007 21:03:41 +1100
Subject: [R] reading a text file with a stray carriage return
In-Reply-To: <644e1f320703071823h2d4b8e5cpabfda6fce9db5e47@mail.gmail.com>
References: <E1HP7xN-0004tz-Oy@elasmtp-junco.atl.sa.earthlink.net>
	<644e1f320703071823h2d4b8e5cpabfda6fce9db5e47@mail.gmail.com>
Message-ID: <45EFDF7D.20502@bitwrit.com.au>

jim holtman wrote:
> How do you define a carriage return in the middle of a line if a carriage
> return is also used to delimit a line?  One of the things you can do is to
> use 'count.fields' to determine the number of fields in each line.  For
> those lines that are not the right length, you could combine them together
> with a 'paste' command when you write them out.
> 
> On 3/7/07, Walter R. Paczkowski <dataanalytics at earthlink.net> wrote:
> 
>>
>>  Hi,
>>  I'm  hoping someone has a suggestion for handling a simple problem.  A
>>  client  gave  me a comma separated value file (call it x.csv) that has
>>  an  id  and name and address for about 25,000 people (25,000 records).
>>  I used read.table to read it, but then discovered that there are stray
>>  carriage returns on several records.  This plays havoc with read.table
>>  since it starts a new input line when it sees the carriage return.  In
>>  short, the read is all wrong.
>>  I thought I could write a simple function to parse a line and write it
>>  back  out,  character by character.  If a carriage return is found, it
>>  would  simply  be  ignored on the writing back out part.  But how do I
>>  identify a carriage return?  What is the code or symbol?  Is there any
>>  easier  way  to  rid the file of carriage returns in the middle of the
>>  input lines?
>>  Any help is appreciated.
>>  Walt Paczkowski
>>
Probably using Windows with a CR/LF newline. You can have carriage 
returns (Ctrl-M - ASCII 13) or line feeds (Ctrl-L - ASCII 10) embedded 
in lines. You can probably just write a function in C or something that 
reads characters, checks that it and the last character is not a CR/LF 
pair and throws out the second character if it is CR or LF or any other 
troublesome byte. (I once had to trace null characters that were 
embedded in files - they didn't show up on the display, but clobbered 
the file reads).

Jim


From ted.harding at nessie.mcc.ac.uk  Thu Mar  8 11:11:06 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 08 Mar 2007 10:11:06 -0000 (GMT)
Subject: [R] reading a text file with a stray carriage return
In-Reply-To: <644e1f320703071823h2d4b8e5cpabfda6fce9db5e47@mail.gmail.com>
Message-ID: <XFMail.070308101106.ted.harding@nessie.mcc.ac.uk>

On 08-Mar-07 jim holtman wrote:
> How do you define a carriage return in the middle of a line
> if a carriage return is also used to delimit a line?  One of the
> things you can do is to use 'count.fields' to determine the
> number of fields in each line. For those lines that are not the
> right length, you could combine them together with a 'paste'
> command when you write them out.

For a moment I though, Walter, that you were simply suffering
from repeat carriage-returns (CR) and getting blank lines, but
then I saw that you refer to CRs "in the middle of input lines".
The "blank line" situation is quite easily handled by pe-processing
the file (before R sees it) if you have the tools available. For
eaxample. the file "test.txt"

  Line 1

  Line 2
  Line 3
  Line 4


  Line 5
  Line 6

(with 1 and then 2 extraneous blank lines), after being fed through

  awk '!/^$'

becomes

awk '!/^$/' < test.txt
  Line 1
  Line 2
  Line 3
  Line 4
  Line 5
  Line 6

since '!/^$/' cause awk to output those lines which do not ("!")
match ("/.../") the pattern "beginning of line" ("^") immediately
followed by "end of line" ("$") -- i.e. it filters out blank lines.

However, if your problem is extraneous CRs (or more generally End
of Line [EOL] which in DOS/Windows is CRLF [CR followed by Line Feed]
and in Unix/Linux is LF, or on a Mac is CR), then probably Jim
Holtman's suggestion, or something like it, is the approach to take.

But, as he implies, there may be more to your problem than meets
the eye. Clearly extraneous EOLs indicate that the path between
the original data and the preparation of the CSV file went wrong
somwhere, of which the extra EOLs are are symptom; but they may
not be the only symptom if you look carefully. Once people are
careless about preparing data (and it's very easy to be careless
when entring data into spreadsheets, for instance), then all sorts
of things can go wrong (and in the case of Excel, I've known Excel
iitself to invent content when the user accidentally strays outside
the boundaries of the spreadsheet). Hence Jim's suggestion of
counting fields is a useful start.

However, supposing that extraneous EOLs is your only problem,
the following strategy may solve it. Assume you have read in
the file, and can count the number of fields (NF) in each line
as read in. I'm assuming that (as described) you should have
3 fields per line. Then loop through the result, line by line:

1.  Read a line and count fields (NF)
2.  If NF=0, skip the line
3.  If NF>3, warning "Too many fields in Line XXX" and abort
4.  If NF=3, transfer the line to the "good" version
5.  If NF<3, make temporary copy "tcp" of line and then
5.1   Read next line and count fields (NF1)
5.2   If NF1=0 skip the line
5.3   If NF+NF1>3 then warning and abort as above
5.4   If NF+NF1=3 then paste line to "tcp" and transfer to "good"
5.5   If NF+NF1<3 paste to "tcp" and then
5.5.1    Read next line and count fields (NF2)
[and so on]

I've put "Warning and abort" cases in there because if "Too many
fields" occurs then you have indeed got more problems than simply
extraneous EOLs, and you have no option but to go into the file
by hand and see what is happening, and try to mend it.

Likewise, if you simply count fields for each line, you can see
how many instances there are of lines with too few fields. This
(if you are lucky) will indicate the magnitude of the task of
opening the file and editing out the errors by hand.

Again, this is the sort of thing with problem files which I have
usually approached using awk, since (in Linux/Usinx at least)
you can do

cat funnyfile.csv | awk 'BEGIN{FS=","}
  {nfields = NF; print nfields}' | sort -u

which (a) sets the Field Separator (FS) to "," (for CSV);
(b) counts the number of fields in a line (NF) and outputs it;
(c) pipes the output to "sort -u" which removes duplicates
and sorts the result. If the file is "clean" you will get only
one number in the result -- the number of fields that should be
in each line (in your case, 3). If you get more than one number,
then you can identify the lines with problems on the lines of

cat funnyfile.csv | awk 'BEGIN{FS=","}
  {nline = NR; nfields = NF}
  {if(nfields != 3){print nline " " nfields}'

which tracks the line count (Number of records [NR]), counts
the number of fields, checks whether this is wring and if so
outputs the number of the offending input line and the number
of fields it has.

One potential problem which would not be identified in this
way is the case where fields get omitted altogether and part
of the following record gets wrapped into the deficient
record, so that line breaks get out of step with the starts
of records. (I've known this happen too ... ) You can then
get the situation where records appear to have the right
numbers of fields, but they are the wrong fields.

I find I've been induced to go on at some length, because your
problem has reminded me of so many dreadfully mangled data files
I've met in the past, and have had to pick through in the kind
of way described above. You have my sympathy!

Good luck!
Ted.

> On 3/7/07, Walter R. Paczkowski <dataanalytics at earthlink.net> wrote:
>>
>>
>>   Hi,
>>   I'm  hoping someone has a suggestion for handling a simple problem. 
>>   A client  gave  me a comma separated value file (call it x.csv)
>>   that has an id and name and address for about 25,000 people
>>   (25,000 records).
>>   I used read.table to read it, but then discovered that there are
>>   stray carriage returns on several records. This plays havoc with
>>   read.table since it starts a new input line when it sees the
>>   carriage return. 
>>   In short, the read is all wrong.
>>   I thought I could write a simple function to parse a line and
>>   write it back  out,  character by character. If a carriage
>>   return is found, it  would  simply  be  ignored on the writing
>>   back out part. But how do I identify a carriage return? What is
>>   the code or symbol? Is there any easier way to rid the file
>>   of carriage returns in the middle of the input lines?
>>   Any help is appreciated.
>>   Walt Paczkowski

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 08-Mar-07                                       Time: 10:10:55
------------------------------ XFMail ------------------------------


From lhill07 at qub.ac.uk  Thu Mar  8 11:25:15 2007
From: lhill07 at qub.ac.uk (Laura Hill)
Date: Thu, 08 Mar 2007 10:25:15 +0000
Subject: [R] Estimating parameters of 2 phase Coxian using optim
In-Reply-To: <45EE043D.8090102@ms.unimelb.edu.au>
Message-ID: <C215950B.924%lhill07@qub.ac.uk>




On 7/3/07 00:15, "Gad Abraham" <g.abraham at ms.unimelb.edu.au> wrote:

> Andy Fugard wrote:
>> Hi There,
>> 
>> Perhaps the problem is the line
>> 
>>          loglik<-log(p %*% expm(Q * y(i)) %*% q)
>> 
>> You mention that y is a vector but here you're treating it as a
>> function.  Maybe try
>> 
>>          loglik<-log(p %*% expm(Q * y[i]) %*% q)
>> 
>> ?
>> 
>> Don't have a clue about the correctness of the contents of cox2.lik...
>> 
>> Andy
>> 
>> 
>> On 6 Mar 2007, at 08:54, Laura Hill wrote:
>> 
>>> Hi,
>>> 
>>> My name is Laura. I'm a PhD student at Queen's University Belfast
>>> and have
>>> just started learning R. I was wondering if somebody could help me
>>> to see
>>> where I am going wrong in my code for estimating the parameters
>>> [mu1, mu2,
>>> lambda1] of a 2-phase Coxian Distribution.
>>> 
>>> cox2.lik<-function(theta, y){
>>>     mu1<-theta[1]
>>> 
>>>     mu2<-theta[2]
>>> 
>>>     lambda1<-theta[3]
>>> 
>>>     p<-Matrix(c(1, 0), nrow=1, ncol=2)
>>> 
>>>     Q<-Matrix(c(-(lambda1 + mu1), 0, lambda1, -mu2), nrow=2, ncol=2)
>>> 
>>>     q<-Matrix(c(mu1, mu2), nrow=2, ncol=1)
>>> 
>>>     for (i in 1:length(y)){
>>>         loglik<-log(p %*% expm(Q * y(i)) %*% q)
>>>     return(loglik)}
>>> 
>>>     sumloglik<-sum(loglik)
>>> 
>>>     return(-sumloglik)
>>>     }
> 
> Just to add my 2 AU cents regarding the for loop:
> 
> You're trying to create a vector of log likelihoods to sum up later, but
> that's not what's happening there. Instead, assign an empty vector of
> same length as y, then assign the loglik from each iteration to a
> different cell.
> 
> Lastly, there's no need to return anything from a for loop, it's not a
> function.
> 
> HTH,
> Gad



Hi Gad,

Yes that's exactly hat I am trying to do. If I gave you a simple example,
could you perhaps tell me how I could create a vector of log likelihoods.



Lets say I have 1x1 matrices:

p=[1]
Q=[0.05]      i.e. [mu1]
q=[-0.05]     i.e. [-mu1]

Where mu1 is the parameter that I would like to estimate and I have chosen
the initial value mu1=0.05


Loglik<-p %*% expm(Q*y) %*% q

Where y=(5 10)

I want to sum the log likelihoods that I get for y=5 and y=10 using

Sumloglik<-sum(allloglik)

Where allloglik = vector of log likelihoods


Any help would be greatly appreciated.

Thanks in advance
Laura


From albmont at centroin.com.br  Thu Mar  8 12:19:50 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Thu, 8 Mar 2007 09:19:50 -0200
Subject: [R] Error distribution question
In-Reply-To: <45EFD526.4060109@eva.mpg.de>
References: <45EFD526.4060109@eva.mpg.de>
Message-ID: <20070308111151.M65971@centroin.com.br>

Cristina wrote:
>
> I was wondering if somebody could offer me some advice on which 
> error distribution would be appropriate for the type of data I have. 
> I'm studying what continuous predictor variables such as grooming 
> received, rank, etc. affect the amount of grooming given. This 
> response variable is continuous with many zeros, and so positively 
> skewed.
>
This kind of variable is very common in prospecting (oil, mining)
industries, and also in medical research. It's neither continuous
nor discrete, because of the weight on zero. Basically, it is a 
combination of _two_ variables:

X: a Bernoulli trial, such that p(X = 0) = 1 - p (failure) and
   p(X = 1) = p (success)

Y: the continous variable that represents numerically the success

So, we have the final variable as X * Y.

For example, if you are going do model the economic value of
a possible oil field, a potential gold mine, or a new experimental
drug, you must assing a non-zero (1-p) to the possibility that
the oil field has no economic value, the mine has no gold, or
the new drug has so many collateral effects that no g*vernment
in the world (except maybe ... name here your favourite) will
allow it. Then you have to estimate the return in the case of
success.

Alberto Monteiro


From mswierniak at o2.pl  Thu Mar  8 12:31:17 2007
From: mswierniak at o2.pl (jastar)
Date: Thu, 8 Mar 2007 03:31:17 -0800 (PST)
Subject: [R] Searching and deleting elements of list
Message-ID: <9372270.post@talk.nabble.com>


Hi,
I have a problem. Please, look at example and try to help me!!

> A<-c("aaa","bbb","ccc","ddd","eee")
> B<-c("vvv","ooo"aaa","eee","zzz","bbb")
> C<-c("sss","jjj","ppp","ddd")
> D<-c("bbb","ccc")
>mydata=list(A,B,C,D)

I want to find and delete from 'mydata' all elements which occur in A
(except A). 
I mean after "operation":
> mydata[[1]]
[1] "aaa" "bbb" "ccc" "ddd" "eee"
> mydata[[2]]
[1] "vvv" "ooo" "zzz"
> mydata[[3]]
[1] "sss","jjj","ppp"
> mydata[[4]]
NULL

My list have about 10000 subelements (each contains several strings) so
using loops is senseless.

Thank's for all replies and sorry for my English (I hope you understand what
I'm talking about) :-)
 
-- 
View this message in context: http://www.nabble.com/Searching-and-deleting-elements-of-list-tf3368489.html#a9372270
Sent from the R help mailing list archive at Nabble.com.


From mswierniak at o2.pl  Thu Mar  8 12:31:17 2007
From: mswierniak at o2.pl (jastar)
Date: Thu, 8 Mar 2007 03:31:17 -0800 (PST)
Subject: [R] Searching and deleting elements of list
Message-ID: <9372270.post@talk.nabble.com>


Hi,
I have a problem. Please, look at example and try to help me!!

> A<-c("aaa","bbb","ccc","ddd","eee")
> B<-c("vvv","ooo","aaa","eee","zzz","bbb")
> C<-c("sss","jjj","ppp","ddd")
> D<-c("bbb","ccc")
>mydata=list(A,B,C,D)

I want to find and delete from 'mydata' all elements which occur in A
(except A). 
I mean after "operation":
> mydata[[1]]
[1] "aaa" "bbb" "ccc" "ddd" "eee"
> mydata[[2]]
[1] "vvv" "ooo" "zzz"
> mydata[[3]]
[1] "sss","jjj","ppp"
> mydata[[4]]
NULL

My list have about 10000 subelements (each contains several strings) so
using loops is senseless.

Thank's for all replies and sorry for my English (I hope you understand what
I'm talking about) :-)
 
-- 
View this message in context: http://www.nabble.com/Searching-and-deleting-elements-of-list-tf3368489.html#a9372270
Sent from the R help mailing list archive at Nabble.com.


From S.Pickett at exeter.ac.uk  Thu Mar  8 12:50:36 2007
From: S.Pickett at exeter.ac.uk (Simon Pickett)
Date: Thu, 8 Mar 2007 11:50:36 -0000 (GMT)
Subject: [R] link function
Message-ID: <2483.144.173.76.117.1173354636.squirrel@www.webmail.ex.ac.uk>

Hi everyone,

I have a model which yields a qqnorm plot (residuals~fitted values) of a
sigmoidal shape.

I have been reading about link functions and leafing through Crawley, but
I'm still not sure how to solve this.

Perhaps "family=quasi(power=...." is the best route?

Any suggestions much appreciated.


Simon Pickett
PhD student
Centre For Ecology and Conservation
Tremough Campus
University of Exeter in Cornwall
TR109EZ
Tel 01326371852


From Stefano.Guazzetti at ausl.re.it  Thu Mar  8 13:07:03 2007
From: Stefano.Guazzetti at ausl.re.it (Guazzetti Stefano)
Date: Thu, 8 Mar 2007 13:07:03 +0100
Subject: [R] R:  Searching and deleting elements of list
Message-ID: <F0E5B4FAD37B7844B6D21998C11E60A6445CB2@RE2-EXC-VBE1B.ausl.org>

you could try mapply

 mydata2<-mapply("[", mydata, lapply(mydata, function(x) !x %in% A))
 mydata2[[1]]<-A  #to replace the obviously deleted elements of "A"
 mydata2
 mydata2[[1]]
 mydata2[[2]]
 mydata2[[3]]
 mydata2[[4]]

Stefano


-----Messaggio originale-----
Da: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]Per conto di jastar
Inviato: gioved? 8 marzo 2007 12.31
A: r-help at stat.math.ethz.ch
Oggetto: [R] Searching and deleting elements of list



Hi,
I have a problem. Please, look at example and try to help me!!

> A<-c("aaa","bbb","ccc","ddd","eee")
> B<-c("vvv","ooo","aaa","eee","zzz","bbb")
> C<-c("sss","jjj","ppp","ddd")
> D<-c("bbb","ccc")
>mydata=list(A,B,C,D)

I want to find and delete from 'mydata' all elements which occur in A
(except A). 
I mean after "operation":
> mydata[[1]]
[1] "aaa" "bbb" "ccc" "ddd" "eee"
> mydata[[2]]
[1] "vvv" "ooo" "zzz"
> mydata[[3]]
[1] "sss","jjj","ppp"
> mydata[[4]]
NULL

My list have about 10000 subelements (each contains several strings) so
using loops is senseless.

Thank's for all replies and sorry for my English (I hope you understand what
I'm talking about) :-)
 
-- 
View this message in context: http://www.nabble.com/Searching-and-deleting-elements-of-list-tf3368489.html#a9372270
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From j.van_den_hoff at fzd.de  Thu Mar  8 13:14:32 2007
From: j.van_den_hoff at fzd.de (Joerg van den Hoff)
Date: Thu, 8 Mar 2007 13:14:32 +0100
Subject: [R] reading a text file with a stray carriage return
In-Reply-To: <XFMail.070308101106.ted.harding@nessie.mcc.ac.uk>
References: <644e1f320703071823h2d4b8e5cpabfda6fce9db5e47@mail.gmail.com>
	<XFMail.070308101106.ted.harding@nessie.mcc.ac.uk>
Message-ID: <20070308121432.GD11550@marco.fz-rossendorf.de>

On Thu, Mar 08, 2007 at 10:11:06AM -0000, Ted Harding wrote:
> On 08-Mar-07 jim holtman wrote:
> > How do you define a carriage return in the middle of a line
> > if a carriage return is also used to delimit a line?  One of the
> > things you can do is to use 'count.fields' to determine the
> > number of fields in each line. For those lines that are not the
> > right length, you could combine them together with a 'paste'
> > command when you write them out.
> 
>... 
> cat funnyfile.csv | awk 'BEGIN{FS=","}
>   {nline = NR; nfields = NF}
>   {if(nfields != 3){print nline " " nfields}'

I think this might be more 'awk like':

cat funnyfile.csv | awk 'BEGIN{FS=","} NF != 3 {print NR, NF}'

(separating the "pattern" from the "action", I mean. it's slightly
faster too -- but not much). and since everyone has the  same problems,
here's my fieldcounter/reporter (the actual awk program (apart from reporting
the results) is a single line, cute..).
#################################################################################
#!/bin/sh
#scan input files and output statistics of number of fields.
#suitable mainly to check 'rectangular' data files for deviations
#from correct number of columns.

FS='[ \t]+'  #default FS
COMMENT='#'

v1=F
v2=c
valop=$v1:$v2:
while getopts $valop option; do
   if   [ "$option" = "$v1" ]; then
      FS="$OPTARG"
   elif   [ "$option" = "$v2" ]; then
      COMMENT="$OPTARG"
   else
      echo ""
      echo "usage: fieldcount [-$v1 field_separator] [-$v2 comment_char] file ..."
      echo "default FS is white space"
      echo "default comment_char is #"
      echo ""
      exit
   fi
done
shift `expr $OPTIND - 1`

for i do
   echo ""
   echo "file $i (using FS = '$FS'):"
   awk '
      BEGIN {
         FS = "'"$FS"'"
      }
      !/^'"$COMMENT"'/ {fc[NF]++}
      END {
         for (nf in fc) {
            if (fc[nf] > 1) s1 = "s"; else s1 = ""
            if (nf > 1)     s2 = "s"; else s2 = ""
            print fc[nf], "record" s1, "with", nf, "field" s2
         }
      }
   ' $i |sort -rn
   echo  ""
done
#################################################################################
here, you could use this a la (it uses white space as separator per default):

fieldcount -F"," funnyfile1.csv funnyfile2.csv ...

it's handy for huge files, since it outputs cumulative results reverse sorted
by frequency of occurence (the bad guys should be infrequent and
at the bottom, contrary to real world experience...).

one could of course tweak this to report only records deviating from some
expected field count (say 3) by adding a further command line switch and using
this in the awk script as is done with the other shell variables ($FS and
$COMMENT).

joerg

> 
> 
> > On 3/7/07, Walter R. Paczkowski <dataanalytics at earthlink.net> wrote:
> >>
> >>
> >>   Hi,
> >>   I'm  hoping someone has a suggestion for handling a simple problem. 
> >>   A client  gave  me a comma separated value file (call it x.csv)
> >>   that has an id and name and address for about 25,000 people
> >>   (25,000 records).
> >>   I used read.table to read it, but then discovered that there are
> >>   stray carriage returns on several records. This plays havoc with
> >>   read.table since it starts a new input line when it sees the
> >>   carriage return. 
> >>   In short, the read is all wrong.
> >>   I thought I could write a simple function to parse a line and
> >>   write it back  out,  character by character. If a carriage
> >>   return is found, it  would  simply  be  ignored on the writing
> >>   back out part. But how do I identify a carriage return? What is
> >>   the code or symbol? Is there any easier way to rid the file
> >>   of carriage returns in the middle of the input lines?
> >>   Any help is appreciated.
> >>   Walt Paczkowski
>


From ggrothendieck at gmail.com  Thu Mar  8 13:37:42 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 8 Mar 2007 07:37:42 -0500
Subject: [R] Named backreferences in replacement patterns
In-Reply-To: <733281.1173324348165.JavaMail.ngmail@webmail12>
References: <733281.1173324348165.JavaMail.ngmail@webmail12>
Message-ID: <971536df0703080437o6152ff27oda83252c9487d944@mail.gmail.com>

This does not really answer the specific question you posted but
gsubfn can do it without using \\1 and \\2 like this (you probably
realized that already but I thought I would point it out just in  case):

library(gsubfn)
gsubfn("(?<month>\\d+)/(?<day>\\d+)/",
   month + day ~ sprintf("%s/%s/", day, month),
   British.dates,
   backref = -2, British.dates, perl = TRUE)

On 3/7/07, Stefan Th. Gries <stgries_lists at arcor.de> wrote:
> Hi
>
> I have a problem with substitutions involving named backreferences. I
> have a vector American.dates:
>
> > American.dates
> [1] "5/15/1976" "2.15.1970" "1.9.2006"
>
> which I want to change into British.dates:
>
> > British.dates
> [1] "15/5/1976" "15/2/1970" "9/1/2006"
>
> I know I can do it like this:
>
> British.dates<-sub("(\\d{1,2})\\D(\\d{1,2})\\D", "\\2/\\1/",
> American.dates, perl=T)
>
> But I want to use named backreferences. I was trying something like
> this
>
> British.dates<-sub("(?P<MONTH>\\d{1,2})\\D(?P<DAY>\\d{1,2})\\D",
> "...", American.dates, perl=T)
>
> but the problem is the ... I know I could use the named backreferences
> *in the search pattern* with (?P=MONTH), but how do I use them *in the
> replacement pattern"? I didn't get the Python solution to work:
>
> > (British.dates<-sub("(?P<MONTH>\\d{1,2})\\D(?P<DAY>\\d{1,2})\\D", "'\g<MONTH>\g/\\1/", American.dates, perl=T))
> [1] "'g<MONTH>g/5/1976" "'g<MONTH>g/2/1970" "'g<MONTH>g/1/2006"
>
> > (British.dates<-sub("(?P<MONTH>\\d{1,2})\\D(?P<DAY>\\d{1,2})\\D", "'\\g<MONTH>\\g/\\1/", American.dates, perl=T))
> [1] "'g<MONTH>g/5/1976" "'g<MONTH>g/2/1970" "'g<MONTH>g/1/2006"
>
> I know I can use the numbers again, but then what would be the point
> of having used names in the first place ...
>
> Any ideas? Thx a bunch,
> STG
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Thu Mar  8 13:43:07 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 8 Mar 2007 07:43:07 -0500
Subject: [R] Named backreferences in replacement patterns
In-Reply-To: <971536df0703080437o6152ff27oda83252c9487d944@mail.gmail.com>
References: <733281.1173324348165.JavaMail.ngmail@webmail12>
	<971536df0703080437o6152ff27oda83252c9487d944@mail.gmail.com>
Message-ID: <971536df0703080443t1fd2b67enf3fc3a0857430008@mail.gmail.com>

Sorry. An extra line got in there.  It should have been:

library(gsubfn)
gsubfn("(?<month>\\d+)/(?<day>\\d+)/",
  month + day ~ sprintf("%s/%s/", day, month),
  backref = -2, British.dates, perl = TRUE)



On 3/8/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> This does not really answer the specific question you posted but
> gsubfn can do it without using \\1 and \\2 like this (you probably
> realized that already but I thought I would point it out just in  case):
>
> library(gsubfn)
> gsubfn("(?<month>\\d+)/(?<day>\\d+)/",
>   month + day ~ sprintf("%s/%s/", day, month),
>   British.dates,
>   backref = -2, British.dates, perl = TRUE)
>
> On 3/7/07, Stefan Th. Gries <stgries_lists at arcor.de> wrote:
> > Hi
> >
> > I have a problem with substitutions involving named backreferences. I
> > have a vector American.dates:
> >
> > > American.dates
> > [1] "5/15/1976" "2.15.1970" "1.9.2006"
> >
> > which I want to change into British.dates:
> >
> > > British.dates
> > [1] "15/5/1976" "15/2/1970" "9/1/2006"
> >
> > I know I can do it like this:
> >
> > British.dates<-sub("(\\d{1,2})\\D(\\d{1,2})\\D", "\\2/\\1/",
> > American.dates, perl=T)
> >
> > But I want to use named backreferences. I was trying something like
> > this
> >
> > British.dates<-sub("(?P<MONTH>\\d{1,2})\\D(?P<DAY>\\d{1,2})\\D",
> > "...", American.dates, perl=T)
> >
> > but the problem is the ... I know I could use the named backreferences
> > *in the search pattern* with (?P=MONTH), but how do I use them *in the
> > replacement pattern"? I didn't get the Python solution to work:
> >
> > > (British.dates<-sub("(?P<MONTH>\\d{1,2})\\D(?P<DAY>\\d{1,2})\\D", "'\g<MONTH>\g/\\1/", American.dates, perl=T))
> > [1] "'g<MONTH>g/5/1976" "'g<MONTH>g/2/1970" "'g<MONTH>g/1/2006"
> >
> > > (British.dates<-sub("(?P<MONTH>\\d{1,2})\\D(?P<DAY>\\d{1,2})\\D", "'\\g<MONTH>\\g/\\1/", American.dates, perl=T))
> > [1] "'g<MONTH>g/5/1976" "'g<MONTH>g/2/1970" "'g<MONTH>g/1/2006"
> >
> > I know I can use the numbers again, but then what would be the point
> > of having used names in the first place ...
> >
> > Any ideas? Thx a bunch,
> > STG
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From david.bolius at art.admin.ch  Thu Mar  8 14:10:55 2007
From: david.bolius at art.admin.ch (david.bolius at art.admin.ch)
Date: Thu, 8 Mar 2007 14:10:55 +0100
Subject: [R] Using logarithmic y-axis (density) in a histogram
Message-ID: <8DEACFBA5EA2744286101CF8D8EEC2129940EB@EVD-C8002.bk.evdad.admin.ch>

Hi, 

I am searching for a possibility to display a logarithimic y-axis in a histogram. With plot that's easy (e.g. 
plot(1:10, log="y") 
but for histograms this does not work the same way: I tried
hist(rnorm(1000), freq=FALSE, seq(-4, 4, .5), ylim=c(0.001, 0.5), log="y") 
Which gives the expected histogram but also warnings for my log="y" command (""log" is not a graphical parameter in: axis(side, at, labels, tick, line, pos, outer, font, lty, lwd,  ") and no logarithmic y-axis. Any ideas how to achieve that, I couldn't find anything?

Best regards
David 


***************************************************************
Dr. David Bolius
Agroscope Reckenholz-T?nikon ART
Gruppe Lufthygiene/Klima
Reckenholzstrasse 191,  CH-8046 Z?rich
Tel.    ++41 (0)44 / 377 75 13   FAX     ++41 (0)44 / 377 72 01
david.bolius at art.admin.ch   http://www.art.admin.ch/


From murdoch at stats.uwo.ca  Thu Mar  8 14:32:35 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 08 Mar 2007 08:32:35 -0500
Subject: [R] Using logarithmic y-axis (density) in a histogram
In-Reply-To: <8DEACFBA5EA2744286101CF8D8EEC2129940EB@EVD-C8002.bk.evdad.admin.ch>
References: <8DEACFBA5EA2744286101CF8D8EEC2129940EB@EVD-C8002.bk.evdad.admin.ch>
Message-ID: <45F01073.6040708@stats.uwo.ca>

On 3/8/2007 8:10 AM, david.bolius at art.admin.ch wrote:
> Hi, 
> 
> I am searching for a possibility to display a logarithimic y-axis in a histogram. With plot that's easy (e.g. 
> plot(1:10, log="y") 
> but for histograms this does not work the same way: I tried
> hist(rnorm(1000), freq=FALSE, seq(-4, 4, .5), ylim=c(0.001, 0.5), log="y") 
> Which gives the expected histogram but also warnings for my log="y" command (""log" is not a graphical parameter in: axis(side, at, labels, tick, line, pos, outer, font, lty, lwd,  ") and no logarithmic y-axis. Any ideas how to achieve that, I couldn't find anything?

I'd say from a graphical perspective it doesn't make sense to use a bar 
chart for a histogram on a log scale (where should the base of the bars 
go?), but if you really want to, you could do it by calling hist() with 
plot=FALSE, and building it yourself using the "histogram" object that 
is returned.  For example,

x <- rnorm(10000)
h <- hist(x, breaks="Scott", plot=FALSE)
plot(h$mids, h$density, log="y", type='b')

Duncan Murdoch


From mona.kanaan at aub.edu.lb  Thu Mar  8 14:54:02 2007
From: mona.kanaan at aub.edu.lb (Mona Kanaan)
Date: Thu, 08 Mar 2007 15:54:02 +0200
Subject: [R] Tracking when an object/function was  modified
Message-ID: <45F0157A.9040907@aub.edu.lb>

Dear R-users,

If I would like to track the date when an R-object (specifically  an R- 
function) was modified, how can I achieve that? Furthermore, how can I 
sort these objects based on date  modified?


Your help is greatly appreciated
All the best

Mona

-- 
*****************************************************
*Dr. Mona Kanaan                                    *
*Department of Epidemiology and Population Health   *
*Faculty of Health Sciences                         *
*The American University of Beirut                  *
*P.O. Box 11-0236, Riad El Solh 1107 2020           *
*Beirut, Lebanon                                    *
*E-mail: mona.kanaan at aub.edu.lb                     *


From liy12 at mskcc.org  Thu Mar  8 15:05:22 2007
From: liy12 at mskcc.org (Yuelin Li)
Date: Thu, 8 Mar 2007 09:05:22 -0500
Subject: [R] Some problems with X11
In-Reply-To: <20070308091839.GA11550@marco.fz-rossendorf.de>
References: <45EF5720.40802@gmail.com>
	<20070308091839.GA11550@marco.fz-rossendorf.de>
Message-ID: <20070308140522.GC18534@jdmlab.mskcc.org>

Since you are running Ubuntu Edgy, you might want look at the "Files"
section in your /etc/X11/xorg.conf file.  Perhaps R is looking for
fonts not available to the X windows system.  You can compare the
"Files" sections between your laptop and desktop.  My font path
settings are attached.

Section "Files"
        FontPath        "/usr/share/fonts/X11/100dpi/:unscaled"
        FontPath        "/usr/share/fonts/X11/75dpi/:unscaled"
        FontPath        "/usr/share/fonts/X11/Type1"
        FontPath        "/usr/share/fonts/X11/100dpi"
        FontPath        "/usr/share/fonts/X11/75dpi"
        FontPath        "/usr/share/fonts/X11/TTF"
        FontPath        "/usr/share/fonts/X11/OTF"
        FontPath        "/usr/share/fonts/X11/CID"
        FontPath        "/usr/share/fonts/X11/misc"
        # path to defoma fonts
        FontPath
	"/var/lib/defoma/x-ttcidfont-conf.d/dirs/TrueType"
EndSection


-- Joerg van den Hoff wrote --|Thu (Mar/08/2007)[10:18]|--:
   
   in `R', getOpton("X11fonts") tells you which font family `X11()' is using by
   default (but you can alter this in the `X11' call if need be) and the size 8
   subtype seems to be missing.
   
   you may compare the above with the output from `xlsfonts' in the unix shell,
   or use `xfontsel' to browse interactively through the installed fonts
   to verify whether `R's complaining is justified.
   
   probable solution: use some existing sufficiently complete (different sizes)
   font-family  in the `X11()' call or copy over the default fonts from the labtop
   where it works.
   
   hth,
   joerg

 
     =====================================================================
     
     Please note that this e-mail and any files transmitted with it may be 
     privileged, confidential, and protected from disclosure under 
     applicable law. If the reader of this message is not the intended 
     recipient, or an employee or agent responsible for delivering this 
     message to the intended recipient, you are hereby notified that any 
     reading, dissemination, distribution, copying, or other use of this 
     communication or any of its attachments is strictly prohibited.  If 
     you have received this communication in error, please notify the 
     sender immediately by replying to this message and deleting this 
     message, any attachments, and all copies and backups from your 
     computer.


From murdoch at stats.uwo.ca  Thu Mar  8 15:06:32 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 08 Mar 2007 09:06:32 -0500
Subject: [R] Tracking when an object/function was  modified
In-Reply-To: <45F0157A.9040907@aub.edu.lb>
References: <45F0157A.9040907@aub.edu.lb>
Message-ID: <45F01868.8060409@stats.uwo.ca>

On 3/8/2007 8:54 AM, Mona Kanaan wrote:
> Dear R-users,
> 
> If I would like to track the date when an R-object (specifically  an R- 
> function) was modified, how can I achieve that? Furthermore, how can I 
> sort these objects based on date  modified?

R doesn't give you a way to do that.  Objects have no timestamps on them.

Adding a timestamp would be quite hard; it would need to be done at a 
very low level.  For example, you might do this:

9 AM:

f <- function() 1

10 AM:

g <- function() 2

11 AM:

f <- g

Now should the timestamp on f be 11 AM or 10 AM?  I think 11 AM makes 
more sense, but that implies modifying the way assignments are done.  If 
10 AM would be good enough, you'd only need to modify the way "function" 
works (and some other functions that modify functions, e.g. body<-, etc.)

I would say this is not worth attempting.  It's better to keep your 
function definitions in source form (e.g. in the source of a package, or 
just in a script that you'll source()), and then use tools external to R 
to track modifications.  Subversion is very good at that.

Duncan Murdoch


From murdoch at stats.uwo.ca  Thu Mar  8 15:28:32 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 08 Mar 2007 09:28:32 -0500
Subject: [R] Tracking when an object/function was  modified
In-Reply-To: <45F01868.8060409@stats.uwo.ca>
References: <45F0157A.9040907@aub.edu.lb> <45F01868.8060409@stats.uwo.ca>
Message-ID: <45F01D90.6070000@stats.uwo.ca>

On 3/8/2007 9:06 AM, Duncan Murdoch wrote:
> On 3/8/2007 8:54 AM, Mona Kanaan wrote:
>> Dear R-users,
>> 
>> If I would like to track the date when an R-object (specifically  an R- 
>> function) was modified, how can I achieve that? Furthermore, how can I 
>> sort these objects based on date  modified?
> 
> R doesn't give you a way to do that.  Objects have no timestamps on them.
> 
> Adding a timestamp would be quite hard; it would need to be done at a 
> very low level.  For example, you might do this:
> 
> 9 AM:
> 
> f <- function() 1
> 
> 10 AM:
> 
> g <- function() 2
> 
> 11 AM:
> 
> f <- g
> 
> Now should the timestamp on f be 11 AM or 10 AM?  I think 11 AM makes 
> more sense, but that implies modifying the way assignments are done.  If 
> 10 AM would be good enough, you'd only need to modify the way "function" 
> works (and some other functions that modify functions, e.g. body<-, etc.)
> 
> I would say this is not worth attempting.  It's better to keep your 
> function definitions in source form (e.g. in the source of a package, or 
> just in a script that you'll source()), and then use tools external to R 
> to track modifications.  Subversion is very good at that.

Sorry, one addition here:

In R 2.5.0, you'll have the option of keeping source references when you 
use source() or parse(), and those include timestamps on files when 
that's where the source comes from.

Duncan Murdoch


From clists at perrin.socsci.unc.edu  Thu Mar  8 15:37:40 2007
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Thu, 8 Mar 2007 09:37:40 -0500 (EST)
Subject: [R] Memory error
Message-ID: <Pine.LNX.4.64.0703080933420.6222@perrin.socsci.unc.edu>

Greetings-

Running R 2.4.0 under Debian Linux, I am getting a memory error trying to 
read a very large file:

> library(foreign)
> oldgrades.df <- read.spss('Individual grades with AI (Nov 7 2006).sav',to.data.frame=TRUE)
Error: cannot allocate vector of size 10826 Kb


This file is, granted, quite large:

aperrin at perrin:/data0/grading$ ls -l
total 630304
-r-xr-xr-x 1 aperrin aperrin 271210015 2007-03-06 15:54 Individual grades with AI (Mar 2 2007).sav
-r-xr-xr-x 1 aperrin aperrin 353209140 2007-03-06 15:57 Individual grades with AI (Nov 7 2006).sav


...but there ought to be plenty of resources. The machine is a dual-Xeon 
2.8Ghz with 6GB of RAM and enormous swap. It's doing almost nothing else 
when I try the load, and at the moment it returned the error, this was the 
status of top:

Mem:   6750980k total,  4668388k used,  2082592k free,   141820k buffers
Swap: 19535032k total,        8k used, 19535024k free,   749244k cached

   PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
  6168 aperrin   25   0 3015m 2.9g 2880 R  100 45.6   7:52.93 R



Background info:
aperrin at perrin:~$ uname -a
Linux perrin 2.6.18 #1 SMP Tue Feb 6 14:20:44 EST 2007 i686 GNU/Linux

aperrin at perrin:~$ R --version
R version 2.4.0 Patched (2006-11-25 r39997)
Copyright (C) 2006 The R Foundation for Statistical Computing
ISBN 3-900051-07-0



Any thoughts? I would be happy to compile R locally if that would help.

Andy


----------------------------------------------------------------------
Andrew J Perrin - andrew_perrin (at) unc.edu - http://perrin.socsci.unc.edu
Assistant Professor of Sociology; Book Review Editor, _Social Forces_
University of North Carolina - CB#3210, Chapel Hill, NC 27599-3210 USA
New Book: http://www.press.uchicago.edu/cgi-bin/hfs.cgi/00/178592.ctl


From lassana.koita at aviation-civile.gouv.fr  Thu Mar  8 15:38:03 2007
From: lassana.koita at aviation-civile.gouv.fr (KOITA Lassana - STAC/ACE)
Date: Thu, 8 Mar 2007 15:38:03 +0100
Subject: [R] curve of density on histogram
Message-ID: <OF2417B3C7.D7FFCF1A-ONC1257298.004F3CA4@aviation-civile.gouv.fr>


Hi R users,
I would like to know why these following curve densities don't appear
correctly on the histograms.
Thank you for your help



library(lattice)
library(grid)
resp  <- rnorm(2000)
group <- sample(c("G1", "G2", "G3", "G4"), replace = TRUE, size = 1000)
histogram(~ resp | group, col="steelblue",
  panel = function(x, ...){
    std <- if(length(x) > 0) format(round(sd(x), 2), nsmall = 2) else "NA"
    n <- length(x)
    m <- if(length(x) > 0) format(round(mean(x), 2), nsmall = 2) else "NA"
    panel.histogram(x, ...)
    panel.mathdensity(dmath = dnorm, col = "green",
                                args = list(mean=mean(x),sd=sd(x)))
    panel.abline(v= mean(x), col = "red")
    panel.xyplot(x = jitter(x),type="p",pch=20,y = rep(0, length(x)),
col='yellow' )

    x1 <- unit(1, "npc") - unit(2, "mm")
    y1 <- unit(1, "npc") - unit(2, "mm")
    grid.text(label = bquote(n == .(n)), x = x1, y = y1, just = "right")
    grid.text(label = bquote(hat(m) == .(m)), x = x1, y = y1 - unit(1,
"lines"), just = "right")
    grid.text(label = bquote(hat(s) == .(std)), x = x1, y = y1 -
unit(2, "lines"), just = "right")

})

#################################################""""""

Lassana KOITA
Charg? d'Etudes de S?curit? A?roportuaire et d'Analyse Statistique /
Project Engineer Airport Safety Studies & Statistical analysis
Service Technique de l'Aviation Civile (STAC) / Civil Aviation Technical
Department
Direction G?n?rale de l'Aviation Civile (DGAC) / French Civil Aviation
Authority
Tel: 01 49 56 80 60
Fax: 01 49 56 82 14
E-mail: Lassana.Koita at aviation-civile.gouv.fr
http://www.stac.aviation-civile.gouv.fr/


From M.J.Bojanowski at uu.nl  Thu Mar  8 15:42:25 2007
From: M.J.Bojanowski at uu.nl (Bojanowski, M.J.  (Michal))
Date: Thu, 8 Mar 2007 15:42:25 +0100
Subject: [R] sink with R-code
In-Reply-To: <DA124317454AAA46BEA4D857E818BEC33E4FFE@EMAIL01.pnl.gov>
References: <DA124317454AAA46BEA4D857E818BEC33E4FFE@EMAIL01.pnl.gov>
Message-ID: <94E133D09AA24D43BF6341B675C01A331134FC@uu01msg-exb01.soliscom.uu.nl>

Hi,

How about 'capture.output'? 
You could also put the code in a separate file and sink the sourcing it
with the echo=TRUE argument, for example (input.r contains the
commands).

sink("output.txt")
source("input.r", echo=TRUE)
sink()

HTH,

Michal 



*** Note that my e-mail address has changed to m.j.bojanowski at uu.nl
*** Please update you address books accordingly. Thank you!

_________________________________________
Michal Bojanowski
ICS / Sociology
Utrecht University
Heidelberglaan 2; 3584 CS Utrecht
Room 1428
m.j.bojanowski at uu.nl
http://www.fss.uu.nl/soc/bojanowski
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Cooley, Scott K
Sent: Thursday, March 08, 2007 1:37 AM
To: r-help at stat.math.ethz.ch
Subject: [R] sink with R-code

I have the same question that Eusebio had:

Is there a function similar to "sink" that redirect also R code to a
file that
is:

sink("R001")
x <- c(2,-6,-4,8,5,4,1,3,4,-9,0,1)
A <- matrix(x, ncol=3)
A
A.prima <- t(A)
A.prima
dim(A)
dim(A.prima)
sink()

create a file "R001" with contents:

------------------------------------------
     [,1] [,2] [,3]
[1,]    2    5    4
[2,]   -6    4   -9
[3,]   -4    1    0
[4,]    8    3    1
     [,1] [,2] [,3] [,4]
[1,]    2   -6   -4    8
[2,]    5    4    1    3
[3,]    4   -9    0    1
[1] 4 3
[1] 3 4

--------------------------------------------

and what I want is a file with:

--------------------------------------------
> x <- c(2,-6,-4,8,5,4,1,3,4,-9,0,1)
> A <- matrix(x, ncol=3)
> A
     [,1] [,2] [,3]
[1,]    2    5    4
[2,]   -6    4   -9
[3,]   -4    1    0
[4,]    8    3    1
> A.prima <- t(A)
> A.prima
     [,1] [,2] [,3] [,4]
[1,]    2   -6   -4    8
[2,]    5    4    1    3
[3,]    4   -9    0    1
----------------------------------------------
Any hint will be appreciated

Eusebio


Scott K. Cooley
Statistical Sciences, K6-08
Battelle--Pacific Northwest Division
Pacific Northwest National Laboratory
P.O. Box 999
Richland, WA  99352
Phone: 509-375-3604
FAX: 509-375-2604
Email: scott.cooley at pnl.gov 



	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From b.rowlingson at lancaster.ac.uk  Thu Mar  8 15:54:19 2007
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 08 Mar 2007 14:54:19 +0000
Subject: [R] Tracking when an object/function was  modified
In-Reply-To: <45F01D90.6070000@stats.uwo.ca>
References: <45F0157A.9040907@aub.edu.lb> <45F01868.8060409@stats.uwo.ca>
	<45F01D90.6070000@stats.uwo.ca>
Message-ID: <45F0239B.9000103@lancaster.ac.uk>

Duncan Murdoch wrote:
> On 3/8/2007 9:06 AM, Duncan Murdoch wrote:
>> On 3/8/2007 8:54 AM, Mona Kanaan wrote:
>>> Dear R-users,
>>>
>>> If I would like to track the date when an R-object (specifically  an R- 
>>> function) was modified, how can I achieve that? Furthermore, how can I 
>>> sort these objects based on date  modified?
>> R doesn't give you a way to do that.  Objects have no timestamps on them.

> In R 2.5.0, you'll have the option of keeping source references when you 
> use source() or parse(), and those include timestamps on files when 
> that's where the source comes from.
> 

  I did once hack the R source code so that on every assignment an 
attribute called 'lastModified' was created on the assigned object with 
the current date and time.

  The resulting compiled R didn't pass tests (I think because it then 
became impossible for two objects to be 'identical' unless they also had 
the same timestamp) and it didn't even start up properly for a similar 
reason.

  It is useful functionality to have - I envisaged a system like 'make' 
for R, where if result A depends on data B and result C, and result C 
depends on data E and F, then you can minimise the amount of computation 
needed to update A if some of B,C,D,E and F change.

  I suspect the work to put timestamps in R would be too much. Another 
possible way of doing it would require all your data to be saved in 
.RData files, for which timestamps are available (using file.info()). 
Then your 'Makefile' equivalent would save the target in a .RData file 
instead of just creating an R object.... Needs some more thought...

Barry


From M.J.Bojanowski at uu.nl  Thu Mar  8 15:59:14 2007
From: M.J.Bojanowski at uu.nl (Bojanowski, M.J.  (Michal))
Date: Thu, 8 Mar 2007 15:59:14 +0100
Subject: [R] Searching and deleting elements of list
In-Reply-To: <9372270.post@talk.nabble.com>
References: <9372270.post@talk.nabble.com>
Message-ID: <94E133D09AA24D43BF6341B675C01A331134FD@uu01msg-exb01.soliscom.uu.nl>

Hi,

A little bit shorter perhaps:

> # deletion
> mydata2 <- lapply( mydata, function(x) x[ !(x %in% A) ] )
> # insert A again
> mydata2[[1]] <- A
> mydata2
[[1]]
[1] "aaa" "bbb" "ccc" "ddd" "eee"

[[2]]
[1] "vvv" "ooo" "zzz"

[[3]]
[1] "sss" "jjj" "ppp"

[[4]]
character(0)


Please note that if all elements are deleted (as in 'mydata2[[4]]') you
get 'character(0)' instead of 'NULL'.
You could fix that by

lapply(mydata2, function(x) if(identical(x, character(0))) NULL )

or by (recomennded):

f <- function(x) # perform replacements etc.
{
	rval <- x[ !(x %in% A) ]
	if ( identical(rval, character(0)) )
		return(NULL)
	else return(rval)
}
mydata2 <- lapply( mydata, f ) # apply
mydata2[[1]] <- A # insert A
mydata2


PS. Powodzenia! :)




*** Note that my e-mail address has changed to m.j.bojanowski at uu.nl
*** Please update you address books accordingly. Thank you!

_________________________________________
Michal Bojanowski
ICS / Sociology
Utrecht University
Heidelberglaan 2; 3584 CS Utrecht
Room 1428
m.j.bojanowski at uu.nl
http://www.fss.uu.nl/soc/bojanowski
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of jastar
Sent: Thursday, March 08, 2007 12:31 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Searching and deleting elements of list


Hi,
I have a problem. Please, look at example and try to help me!!

> A<-c("aaa","bbb","ccc","ddd","eee")
> B<-c("vvv","ooo","aaa","eee","zzz","bbb")
> C<-c("sss","jjj","ppp","ddd")
> D<-c("bbb","ccc")
>mydata=list(A,B,C,D)

I want to find and delete from 'mydata' all elements which occur in A
(except A). 
I mean after "operation":
> mydata[[1]]
[1] "aaa" "bbb" "ccc" "ddd" "eee"
> mydata[[2]]
[1] "vvv" "ooo" "zzz"
> mydata[[3]]
[1] "sss","jjj","ppp"
> mydata[[4]]
NULL

My list have about 10000 subelements (each contains several strings) so
using loops is senseless.

Thank's for all replies and sorry for my English (I hope you understand
what I'm talking about) :-)
 
--
View this message in context:
http://www.nabble.com/Searching-and-deleting-elements-of-list-tf3368489.
html#a9372270
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Thu Mar  8 16:00:39 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 08 Mar 2007 16:00:39 +0100
Subject: [R] Memory error
In-Reply-To: <Pine.LNX.4.64.0703080933420.6222@perrin.socsci.unc.edu>
References: <Pine.LNX.4.64.0703080933420.6222@perrin.socsci.unc.edu>
Message-ID: <45F02517.7010906@statistik.uni-dortmund.de>



Andrew Perrin wrote:
> Greetings-
> 
> Running R 2.4.0 under Debian Linux, I am getting a memory error trying to 
> read a very large file:
> 
>> library(foreign)
>> oldgrades.df <- read.spss('Individual grades with AI (Nov 7 2006).sav',to.data.frame=TRUE)
> Error: cannot allocate vector of size 10826 Kb
> 
> 
> This file is, granted, quite large:
> 
> aperrin at perrin:/data0/grading$ ls -l
> total 630304
> -r-xr-xr-x 1 aperrin aperrin 271210015 2007-03-06 15:54 Individual grades with AI (Mar 2 2007).sav
> -r-xr-xr-x 1 aperrin aperrin 353209140 2007-03-06 15:57 Individual grades with AI (Nov 7 2006).sav
> 
> 
> ...but there ought to be plenty of resources. The machine is a dual-Xeon 
> 2.8Ghz with 6GB of RAM and enormous swap. It's doing almost nothing else 
> when I try the load, and at the moment it returned the error, this was the 
> status of top:
> 
> Mem:   6750980k total,  4668388k used,  2082592k free,   141820k buffers
> Swap: 19535032k total,        8k used, 19535024k free,   749244k cached
> 
>    PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
>   6168 aperrin   25   0 3015m 2.9g 2880 R  100 45.6   7:52.93 R


You have hit the 3GB limit. Since your Xeon is probably a P4 
architecture (which means 32 bit), you cannot go on here. Probably it is 
the best idea to put the data from SPSS into some database and use that 
database from R.

Uwe Ligges



> 
> 
> Background info:
> aperrin at perrin:~$ uname -a
> Linux perrin 2.6.18 #1 SMP Tue Feb 6 14:20:44 EST 2007 i686 GNU/Linux
> 
> aperrin at perrin:~$ R --version
> R version 2.4.0 Patched (2006-11-25 r39997)
> Copyright (C) 2006 The R Foundation for Statistical Computing
> ISBN 3-900051-07-0
> 
> 
> 
> Any thoughts? I would be happy to compile R locally if that would help.
> 
> Andy
> 
> 
> ----------------------------------------------------------------------
> Andrew J Perrin - andrew_perrin (at) unc.edu - http://perrin.socsci.unc.edu
> Assistant Professor of Sociology; Book Review Editor, _Social Forces_
> University of North Carolina - CB#3210, Chapel Hill, NC 27599-3210 USA
> New Book: http://www.press.uchicago.edu/cgi-bin/hfs.cgi/00/178592.ctl
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From clists at perrin.socsci.unc.edu  Thu Mar  8 16:10:08 2007
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Thu, 8 Mar 2007 10:10:08 -0500 (EST)
Subject: [R] Memory error
In-Reply-To: <45F02517.7010906@statistik.uni-dortmund.de>
References: <Pine.LNX.4.64.0703080933420.6222@perrin.socsci.unc.edu>
	<45F02517.7010906@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.64.0703081009550.6222@perrin.socsci.unc.edu>

Zoinks, thanks! I will seek to pare that file down and try again.

Andy

----------------------------------------------------------------------
Andrew J Perrin - andrew_perrin (at) unc.edu - http://perrin.socsci.unc.edu
Assistant Professor of Sociology; Book Review Editor, _Social Forces_
University of North Carolina - CB#3210, Chapel Hill, NC 27599-3210 USA
New Book: http://www.press.uchicago.edu/cgi-bin/hfs.cgi/00/178592.ctl



On Thu, 8 Mar 2007, Uwe Ligges wrote:

>
>
> Andrew Perrin wrote:
>> Greetings-
>> 
>> Running R 2.4.0 under Debian Linux, I am getting a memory error trying to 
>> read a very large file:
>> 
>>> library(foreign)
>>> oldgrades.df <- read.spss('Individual grades with AI (Nov 7 
>>> 2006).sav',to.data.frame=TRUE)
>> Error: cannot allocate vector of size 10826 Kb
>> 
>> 
>> This file is, granted, quite large:
>> 
>> aperrin at perrin:/data0/grading$ ls -l
>> total 630304
>> -r-xr-xr-x 1 aperrin aperrin 271210015 2007-03-06 15:54 Individual grades 
>> with AI (Mar 2 2007).sav
>> -r-xr-xr-x 1 aperrin aperrin 353209140 2007-03-06 15:57 Individual grades 
>> with AI (Nov 7 2006).sav
>> 
>> 
>> ...but there ought to be plenty of resources. The machine is a dual-Xeon 
>> 2.8Ghz with 6GB of RAM and enormous swap. It's doing almost nothing else 
>> when I try the load, and at the moment it returned the error, this was the 
>> status of top:
>> 
>> Mem:   6750980k total,  4668388k used,  2082592k free,   141820k buffers
>> Swap: 19535032k total,        8k used, 19535024k free,   749244k cached
>>
>>    PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
>>   6168 aperrin   25   0 3015m 2.9g 2880 R  100 45.6   7:52.93 R
>
>
> You have hit the 3GB limit. Since your Xeon is probably a P4 architecture 
> (which means 32 bit), you cannot go on here. Probably it is the best idea to 
> put the data from SPSS into some database and use that database from R.
>
> Uwe Ligges
>
>
>
>> 
>> 
>> Background info:
>> aperrin at perrin:~$ uname -a
>> Linux perrin 2.6.18 #1 SMP Tue Feb 6 14:20:44 EST 2007 i686 GNU/Linux
>> 
>> aperrin at perrin:~$ R --version
>> R version 2.4.0 Patched (2006-11-25 r39997)
>> Copyright (C) 2006 The R Foundation for Statistical Computing
>> ISBN 3-900051-07-0
>> 
>> 
>> 
>> Any thoughts? I would be happy to compile R locally if that would help.
>> 
>> Andy
>> 
>> 
>> ----------------------------------------------------------------------
>> Andrew J Perrin - andrew_perrin (at) unc.edu - http://perrin.socsci.unc.edu
>> Assistant Professor of Sociology; Book Review Editor, _Social Forces_
>> University of North Carolina - CB#3210, Chapel Hill, NC 27599-3210 USA
>> New Book: http://www.press.uchicago.edu/cgi-bin/hfs.cgi/00/178592.ctl
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From stacey.lee.thompson at gmail.com  Thu Mar  8 16:14:37 2007
From: stacey.lee.thompson at gmail.com (stacey thompson)
Date: Thu, 8 Mar 2007 10:14:37 -0500
Subject: [R] Removing duplicated rows within a matrix,
	with missing data as wildcards
Message-ID: <ac7b131e0703080714p5179e5bm6d7ee24f92c4c206@mail.gmail.com>

I'd like to remove duplicated rows within a matrix, with missing data
being treated as wildcards.

For example

> x <- matrix((1:3), 5, 3)
> x[4,2] = NA
> x[3,3] = NA
> x

     [,1] [,2] [,3]
[1,]    1    3    2
[2,]    2    1    3
[3,]    3    2   NA
[4,]    1   NA    2
[5,]    2    1    3

I would like to obtain

      [,1] [,2] [,3]
[1,]    1    3    2
[2,]    2    1    3
[3,]    3    2   NA

>From the R-help archives, I learned about unique(x) and duplicated(x).
However, unique(x) returns

> unique(x)

     [,1] [,2] [,3]
[1,]    1    3    2
[2,]    2    1    3
[3,]    3    2   NA
[4,]    1   NA    2

and duplicated(x) gives

> duplicated(x)

[1] FALSE FALSE FALSE FALSE  TRUE

I have tried various na.action 's but with unique(x) I get errors at best.

e.g.
> unique(x, na.omit(x))

Error: argument 'incomparables != FALSE' is not used (yet)

How I might tackle this?

Thanks,

-stacey

-- 
-stacey lee thompson-
Stagiaire post-doctorale
Institut de recherche en biologie v?g?tale
Universit? de Montr?al
4101 Sherbrooke Est
Montr?al, Qu?bec H1X 2B2 Canada
stacey.thompson at umontreal.ca


From tlumley at u.washington.edu  Thu Mar  8 16:30:05 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 8 Mar 2007 07:30:05 -0800 (PST)
Subject: [R] Memory error
In-Reply-To: <Pine.LNX.4.64.0703080933420.6222@perrin.socsci.unc.edu>
References: <Pine.LNX.4.64.0703080933420.6222@perrin.socsci.unc.edu>
Message-ID: <Pine.LNX.4.64.0703080726260.17283@homer23.u.washington.edu>

On Thu, 8 Mar 2007, Andrew Perrin wrote:

> Greetings-
>
> Running R 2.4.0 under Debian Linux, I am getting a memory error trying to
> read a very large file:
>
>> library(foreign)
>> oldgrades.df <- read.spss('Individual grades with AI (Nov 7 2006).sav',to.data.frame=TRUE)
> Error: cannot allocate vector of size 10826 Kb

Your file on disk seems to be about 300Mb, and it might well be larger in 
R, so it's probably too big for 32-bit R.

However, you could try to.data.frame=FALSE in the read.spss() call. Based 
on memory profiling of the fairly similar read.dta() function I would 
guess that as.data.frame.list() might well be the culprit.

 	-thomas


From Steven_F_White at raytheon.com  Thu Mar  8 16:52:27 2007
From: Steven_F_White at raytheon.com (Steven F White)
Date: Thu, 8 Mar 2007 10:52:27 -0500
Subject: [R] chronological scatterplots
Message-ID: <OF7F2BF712.0D9320AB-ON85257298.00557E9F-85257298.005733FA@mck.us.ray.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070308/cef6686d/attachment.pl 

From ggrothendieck at gmail.com  Thu Mar  8 17:07:58 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 8 Mar 2007 11:07:58 -0500
Subject: [R] chronological scatterplots
In-Reply-To: <OF7F2BF712.0D9320AB-ON85257298.00557E9F-85257298.005733FA@mck.us.ray.com>
References: <OF7F2BF712.0D9320AB-ON85257298.00557E9F-85257298.005733FA@mck.us.ray.com>
Message-ID: <971536df0703080807q5a069553l4e3f14a24257af59@mail.gmail.com>

Read R News 4/1 help desk article about dates.

Also see ?as.yearmon in the zoo package.

> library(zoo)
> dd <- Sys.Date() + seq(1, 1000, 100)
> dd
 [1] "2007-03-09" "2007-06-17" "2007-09-25" "2008-01-03" "2008-04-12"
 [6] "2008-07-21" "2008-10-29" "2009-02-06" "2009-05-17" "2009-08-25"
> ym <- as.yearmon(dd)
> ym
 [1] "Mar 2007" "Jun 2007" "Sep 2007" "Jan 2008" "Apr 2008" "Jul 2008"
 [7] "Oct 2008" "Feb 2009" "May 2009" "Aug 2009"
> dd2 <- as.Date(ym)
> dd2
 [1] "2007-03-01" "2007-06-01" "2007-09-01" "2008-01-01" "2008-04-01"
 [6] "2008-07-01" "2008-10-01" "2009-02-01" "2009-05-01" "2009-08-01"
> plot(dd2, 1:10)



On 3/8/07, Steven F White <Steven_F_White at raytheon.com> wrote:
> Greets Folks,
>
> I've been wrestling with how to better control plotting of time data and
> just can't seem to see the right path. My dataset has thousands of points
> distributes across a number of years. I would like to plot the responses
> according to increasing time with nice boundaries - perhaps integer months
> - along the abscissa. However, the earliest time occurs mid-month and I
> cannot figure out how to properly truncate the minimum date in the dataset
> to an integer month for use in the xlim= and at= statements to set the
> plotting range on month boundaries.
>
> Thanks in advance for your help or advice.
>
> Best Regards,
> Steve
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Greg.Snow at intermountainmail.org  Thu Mar  8 17:05:29 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Thu, 8 Mar 2007 09:05:29 -0700
Subject: [R] alpha parameter in function rgb to specify color
References: <258799.90810.qm@web81006.mail.mud.yahoo.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB12A117@LP-EXCHVS07.CO.IHC.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070308/21d3ff6a/attachment.pl 

From ted.harding at nessie.mcc.ac.uk  Thu Mar  8 17:29:48 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 08 Mar 2007 16:29:48 -0000 (GMT)
Subject: [R] curve of density on histogram
In-Reply-To: <OF2417B3C7.D7FFCF1A-ONC1257298.004F3CA4@aviation-civile.gouv.fr>
Message-ID: <XFMail.070308162948.ted.harding@nessie.mcc.ac.uk>

On 08-Mar-07 KOITA Lassana - STAC/ACE wrote:
> 
> Hi R users,
> I would like to know why these following curve densities don't appear
> correctly on the histograms.
> Thank you for your help
> 
> 
> 
> library(lattice)
> library(grid)
> resp  <- rnorm(2000)
> group <- sample(c("G1", "G2", "G3", "G4"), replace = TRUE, size = 1000)
> histogram(~ resp | group, col="steelblue",
>   panel = function(x, ...){
>     std <- if(length(x) > 0) format(round(sd(x), 2), nsmall = 2) else
> "NA"
>     n <- length(x)
>     m <- if(length(x) > 0) format(round(mean(x), 2), nsmall = 2) else
> "NA"
>     panel.histogram(x, ...)
>     panel.mathdensity(dmath = dnorm, col = "green",
>                                 args = list(mean=mean(x),sd=sd(x)))
>     panel.abline(v= mean(x), col = "red")
>     panel.xyplot(x = jitter(x),type="p",pch=20,y = rep(0, length(x)),
> col='yellow' )
> 
>     x1 <- unit(1, "npc") - unit(2, "mm")
>     y1 <- unit(1, "npc") - unit(2, "mm")
>     grid.text(label = bquote(n == .(n)), x = x1, y = y1, just =
> "right")
>     grid.text(label = bquote(hat(m) == .(m)), x = x1, y = y1 - unit(1,
> "lines"), just = "right")
>     grid.text(label = bquote(hat(s) == .(std)), x = x1, y = y1 -
> unit(2, "lines"), just = "right")
> 
> })

The following is an approximation to what you want to do.
However, it needs one thing to be determined, which I have
not managed to work out how to do.

The reason for your bad density plots is that the density
function dnorm needs to be scaled (by the number of values
whose histogram is being lotted) and by the width of the
intervals.

Hence I first define a function sdnorm() (for "scaled dnorm")
and then change one line in your code, as follows:

library(lattice)
library(grid)
resp  <- rnorm(2000)
group <- sample(c("G1", "G2", "G3", "G4"), replace = TRUE, size = 1000)
#### New function sdnorm:
sdnorm <-function(x,mean=0,sd=1,N=1,binwid=1){N*binwid*dnorm(x,mean,sd)}
histogram(~ resp | group, col="steelblue",
  panel = function(x, ...){
    std <- if(length(x) > 0) format(round(sd(x), 2), nsmall = 2) else "NA"
    n <- length(x)
    m <- if(length(x) > 0) format(round(mean(x), 2), nsmall = 2) else "NA"
    panel.histogram(x, ...)
#### Changed line:
    panel.mathdensity(dmath = sdnorm, col = "green",
          args = list(mean=mean(x),sd=sd(x),N=length(x),binwid=0.10))
    panel.abline(v= mean(x), col = "red")
    panel.xyplot(x = jitter(x),type="p",pch=20,y = rep(0, length(x)),
col='yellow' )

    x1 <- unit(1, "npc") - unit(2, "mm")
    y1 <- unit(1, "npc") - unit(2, "mm")
    grid.text(label = bquote(n == .(n)), x = x1, y = y1, just = "right")
    grid.text(label = bquote(hat(m) == .(m)), x = x1, y = y1 - unit(1,
"lines"), just = "right")
    grid.text(label = bquote(hat(s) == .(std)), x = x1, y = y1 -
unit(2, "lines"), just = "right")
})

The argument N to sdnorm is readily available from the argument x,
as N = length(x).

However, I cannot work out from the documentation for these panel
functions how to determine the width of the histogram bins, which
is argument binwid to sdnorm(). Hence I have simply set binwid=0.l0
to illustrate the point, since this gives an approximately correct
plot. But it will only be really correct when binwid can somehow
be determined from the hosyogram being plotted, and it is this
which I cannot see!

I hope someone else will help us out of our difficulty.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 08-Mar-07                                       Time: 16:29:45
------------------------------ XFMail ------------------------------


From mwtoews at sfu.ca  Thu Mar  8 17:39:06 2007
From: mwtoews at sfu.ca (Michael Toews)
Date: Thu, 08 Mar 2007 08:39:06 -0800
Subject: [R]  curve of density on histogram
Message-ID: <45F03C2A.4040901@sfu.ca>

Your are so close ... you just need to specify that you want your 
histograms to show density, not percent. I've only edited one line of 
your script, the rest is good and plots nicely:

histogram(~ resp | group, col="steelblue",type="density",


From rosolem at gmail.com  Thu Mar  8 17:44:17 2007
From: rosolem at gmail.com (Rafael Rosolem)
Date: Thu, 08 Mar 2007 09:44:17 -0700
Subject: [R] Some problems with X11
In-Reply-To: <20070308140522.GC18534@jdmlab.mskcc.org>
References: <45EF5720.40802@gmail.com>
	<20070308091839.GA11550@marco.fz-rossendorf.de>
	<20070308140522.GC18534@jdmlab.mskcc.org>
Message-ID: <45F03D61.1020202@gmail.com>

Yuelin Li and Joerg van den Hoff,

Thank you very much for your emails. Apparently, I found that xorg.conf 
has all the directories related to fonts specified as 
/usr/share/X11/fonts/. However, the files are located in 
/usr/share/fonts/X11/. I manually changed the paths for each line and 
tried R again. It is now working!

Thanks a lot

R

Yuelin Li wrote:
> Since you are running Ubuntu Edgy, you might want look at the "Files"
> section in your /etc/X11/xorg.conf file.  Perhaps R is looking for
> fonts not available to the X windows system.  You can compare the
> "Files" sections between your laptop and desktop.  My font path
> settings are attached.
> 
> Section "Files"
>         FontPath        "/usr/share/fonts/X11/100dpi/:unscaled"
>         FontPath        "/usr/share/fonts/X11/75dpi/:unscaled"
>         FontPath        "/usr/share/fonts/X11/Type1"
>         FontPath        "/usr/share/fonts/X11/100dpi"
>         FontPath        "/usr/share/fonts/X11/75dpi"
>         FontPath        "/usr/share/fonts/X11/TTF"
>         FontPath        "/usr/share/fonts/X11/OTF"
>         FontPath        "/usr/share/fonts/X11/CID"
>         FontPath        "/usr/share/fonts/X11/misc"
>         # path to defoma fonts
>         FontPath
> 	"/var/lib/defoma/x-ttcidfont-conf.d/dirs/TrueType"
> EndSection
> 
> 
> -- Joerg van den Hoff wrote --|Thu (Mar/08/2007)[10:18]|--:
>    
>    in `R', getOpton("X11fonts") tells you which font family `X11()' is using by
>    default (but you can alter this in the `X11' call if need be) and the size 8
>    subtype seems to be missing.
>    
>    you may compare the above with the output from `xlsfonts' in the unix shell,
>    or use `xfontsel' to browse interactively through the installed fonts
>    to verify whether `R's complaining is justified.
>    
>    probable solution: use some existing sufficiently complete (different sizes)
>    font-family  in the `X11()' call or copy over the default fonts from the labtop
>    where it works.
>    
>    hth,
>    joerg
> 
>  
>      =====================================================================
>      
>      Please note that this e-mail and any files transmitted with it may be 
>      privileged, confidential, and protected from disclosure under 
>      applicable law. If the reader of this message is not the intended 
>      recipient, or an employee or agent responsible for delivering this 
>      message to the intended recipient, you are hereby notified that any 
>      reading, dissemination, distribution, copying, or other use of this 
>      communication or any of its attachments is strictly prohibited.  If 
>      you have received this communication in error, please notify the 
>      sender immediately by replying to this message and deleting this 
>      message, any attachments, and all copies and backups from your 
>      computer.
> 
>


From stacey.lee.thompson at gmail.com  Thu Mar  8 18:07:00 2007
From: stacey.lee.thompson at gmail.com (stacey thompson)
Date: Thu, 8 Mar 2007 12:07:00 -0500
Subject: [R] Removing duplicated rows in a matrix,
	with missing data as wildcards
Message-ID: <ac7b131e0703080907r2348d9d9h602161465fef4c01@mail.gmail.com>

I'd like to remove duplicated rows within a matrix, with missing data
being treated as wildcards.

For example

> x <- matrix((1:3), 5, 3)
> x[4,2] = NA
> x[3,3] = NA
> x

    [,1] [,2] [,3]
[1,]    1    3    2
[2,]    2    1    3
[3,]    3    2   NA
[4,]    1   NA    2
[5,]    2    1    3

I would like to obtain

     [,1] [,2] [,3]
[1,]    1    3    2
[2,]    2    1    3
[3,]    3    2   NA

>From the R-help archives, I learned about unique(x) and duplicated(x).
However, unique(x) returns

> unique(x)

    [,1] [,2] [,3]
[1,]    1    3    2
[2,]    2    1    3
[3,]    3    2   NA
[4,]    1   NA    2

and duplicated(x) gives

> duplicated(x)

[1] FALSE FALSE FALSE FALSE  TRUE

I have tried various na.action 's but with unique(x) I get errors at best.

e.g.
> unique(x, na.omit(x))

Error: argument 'incomparables != FALSE' is not used (yet)

How I might tackle this?

Thanks,

-stacey

--
-stacey lee thompson-
Stagiaire post-doctorale
Institut de recherche en biologie v?g?tale
Universit? de Montr?al
4101 Sherbrooke Est
Montr?al, Qu?bec H1X 2B2 Canada
stacey.thompson at umontreal.ca


From Peter.Rossi at chicagogsb.edu  Thu Mar  8 18:16:17 2007
From: Peter.Rossi at chicagogsb.edu (Rossi, Peter E.)
Date: Thu, 08 Mar 2007 11:16:17 -0600
Subject: [R] [R-pkgs] Release 2.1-1 of bayesm
Message-ID: <1E7B167439290641966EB161D433079801EA928F@GSBEX.gsb.uchicago.edu>

 
Release 2.1-1 is now available on CRAN.

This release includes--
bayesm classes (some compatible with the mcmc class of coda) for output.
plot
and summary methods for these classes.

additional datasets including store-level panel data.

peter r
 
................................
 Peter E. Rossi
 Joseph T. and Bernice S. Lewis Professor of Marketing and Statistics
 Editor, Quantitative Marketing and Economics
 Rm 353, Graduate School of Business, U of Chicago
 5807 S. Woodlawn Ave, Chicago IL 60637
 Tel: (773) 702-7513   |   Fax: (773) 834-2081

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From Pascal.BOISSON at veolia.com  Thu Mar  8 18:34:52 2007
From: Pascal.BOISSON at veolia.com (BOISSON, Pascal)
Date: Thu, 8 Mar 2007 18:34:52 +0100
Subject: [R] sending a vector of characters as arguments for a function
Message-ID: <770E4329A79EC94CB16D24B5692BDD8FB21A2C@sar-mail1.messagerie.ve>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070308/ade34a6b/attachment.pl 

From alansmith2 at gmail.com  Thu Mar  8 18:34:52 2007
From: alansmith2 at gmail.com (ALAN SMITH)
Date: Thu, 8 Mar 2007 11:34:52 -0600
Subject: [R] how to "apply" functions to unbalanced data in long format
	by factors......cant get "by" or "aggregate" to work
Message-ID: <e8cddd580703080934l68f57b71jeaacc70e3514f952@mail.gmail.com>

Hello R-users
The help I received from Petr helped me created this solution to my problems.

t1<-with(mydata ,aggregate(mydata$Y,
list(mydata$time,mydata$treatment, mydata$expREP, mydata$techREP) ,
median, na.rm=T)) ### find median by factors ####

colnames(t1)<-c("time","treatment","expREP","techREP","Y50") ### column name ##

newdata<-merge(mydata, t1, by.x= names(mydata)[2:5],
by.y=names(t1)[1:4], all=T)

Thank you,
Alan







###############################################################
Message: 97
Date: Thu, 08 Mar 2007 08:00:53 +0100
From: "Petr Pikal" <petr.pikal at precheza.cz>
Subject: Re: [R] how to "apply" functions to unbalanced data in long
       format  by      factors......cant get "by" or "aggregate" to work
To: "ALAN SMITH" <alansmith2 at gmail.com>, r-help at stat.math.ethz.ch
Message-ID: <45EFC2B5.29775.2FD750 at localhost>
Content-Type: text/plain; charset=US-ASCII

Hi

you can use aggregate to create table of medians

with(mydata, aggregate(Y, list(time, tratment, expRep,....), median)

repeats of unique factors
either by rle or aggregate with length function

Then you can do replication by

norep <- rep(your.median, each = your replicates)

Regards
Petr



submitted question abrigded
> Hello R users,



> #####Example data frame######
> mydata<-as.data.frame(structure(list(cpdID = c(7, 7, 7, 7, 7, 7, 8, 8,
> 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 10, 10, 10, 10, 10, 10, 10, 10, 10,
> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,
> 10, 10, 10, 10, 10, 10, 19, 19, 19, 19, 19, 19, 23, 23, 23, 23, 23,
> 23, 23, 23, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,
> 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,
> 33, 33, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,
> 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,
> 40, 40, 40, 40, 40, 40, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,
> 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,
> 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 47, 47,
> 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,
> 47, 47), time = structure(as.integer(c(1, 1, 1, 1, 2, 2, 2, 1, 1, 1,
> 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2,
> 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2,
> 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1,
> 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1,
> 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2,
> 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
> 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1,
> 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2,
> 2, 1, 2, 2, 1, 2)), .Label = c("120hr", "24hr"), class = "factor"),
>     treatment = structure(as.integer(c(1, 1, 1, 2, 2, 1, 1, 2,
>     1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1,
>     2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2,
>     1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1,
>     1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2,
>     2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2,
>     1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2,
>     2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 1,
>     1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2,
>     2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1,
>     2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1,
>     2, 1)), .Label = c("control", "trt"), class = "factor"),
>     expREP = structure(as.integer(c(1, 1, 1, 3, 1, 1, 1, 1, 2,
>     2, 1, 1, 2, 2, 1, 3, 1, 3, 3, 3, 1, 2, 1, 2, 2, 2, 2, 3,
>     3, 2, 2, 1, 2, 3, 3, 1, 1, 2, 3, 1, 3, 3, 3, 3, 1, 3, 1,
>     1, 2, 1, 1, 2, 3, 2, 2, 1, 3, 2, 2, 2, 3, 2, 1, 2, 2, 2,
>     2, 1, 1, 1, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 2, 3, 1, 2,
>     3, 3, 1, 1, 1, 3, 3, 1, 1, 3, 1, 1, 1, 1, 1, 3, 3, 3, 1,
>     1, 1, 2, 3, 2, 2, 3, 2, 2, 2, 1, 1, 1, 3, 3, 2, 2, 2, 1,
>     3, 1, 2, 3, 1, 3, 3, 1, 2, 3, 1, 2, 1, 3, 1, 3, 3, 2, 2,
>     2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 3, 1, 1, 1,
>     1, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 3,
>     3, 1, 1, 1, 3, 2, 1, 1, 2, 1, 3, 2, 1, 2, 1, 3, 1, 1, 2,
>     3)), .Label = c("expREP1", "expREP2", "expREP3"), class =
>     "factor"), techREP = structure(as.integer(c(3, 2, 1, 1, 1, 3, 1,
>     3, 3, 2, 2, 1, 1, 3, 2, 3, 3, 1, 2, 1, 2, 1, 3, 1, 3, 2, 2, 3, 1,
>     1, 3, 3, 2, 3, 3, 3, 2, 2, 2, 2, 1, 1, 2, 3, 1, 2, 3, 1, 3, 2, 1,
>     1, 2, 2, 3, 3, 3, 2, 1, 2, 1, 2, 3, 2, 3, 2, 3, 1, 3, 2, 2, 1, 2,
>     1, 3, 2, 2, 1, 1, 3, 3, 3, 1, 3, 1, 3, 2, 2, 2, 1, 2, 1, 3, 1, 3,
>     2, 1, 3, 1, 3, 2, 3, 1, 1, 2, 3, 1, 1, 3, 3, 2, 2, 1, 2, 3, 2, 2,
>     3, 2, 2, 1, 2, 2, 3, 3, 3, 1, 1, 1, 3, 1, 1, 2, 3, 2, 3, 3, 1, 1,
>     3, 2, 3, 3, 3, 3, 1, 3, 2, 1, 3, 3, 1, 3, 2, 1, 2, 2, 1, 2, 1, 2,
>     1, 1, 1, 2, 2, 3, 3, 1, 2, 3, 2, 3, 3, 3, 1, 2, 2, 1, 3, 2, 3, 3,
>     2, 2, 2, 3, 2, 1, 3, 1, 3, 1, 3, 1, 1, 1, 2, 2, 3)), .Label =
>     c("techREP1", "techREP2", "techREP3"), class = "factor"), log2Abun
>     = c(14.4233129144089, 14.052822741429, 14.2281422686467,
>     13.8492096005693, 14.076481601207, 14.2139395740777,
>     14.3399195756207, 14.3625602954496, 14.0141948668145,
>     14.0980320829605, 14.3152203363759, 14.4528846974866,
>     13.9591869268449, 14.4064043323413, 14.0403753485321,
>     14.2285932517829, 14.1259784261721, 13.5925738310379,
>     13.5830827675029, 13.0280787227049, 15.0198078807043,
>     12.8423503434138, 12.645883554519, 13.4644181177386,
>     12.8399910705399, 12.7879025593914, 12.4978518369511,
>     14.3949985145017, 12.8670856466168, 12.9749522735341,
>     13.3456824481868, 13.4557125040673, 12.8989792046225,
>     16.0609491915918, 13.6795900568273, 16.456466720182,
>     13.6145948287653, 13.2604785448039, 14.8573006848798,
>     13.1382718001722, 13.690761908446, 14.0557060971613,
>     13.7495552174335, 13.6336764098923, 13.7844303674846,
>     15.9518993688317, 13.2452555803066, 13.1930632791304,
>     12.1919845133603, 13.8710388986595, 13.6375305515253,
>     12.5919897676151, 17.4797250127015, 17.4014712120155,
>     17.5948202702163, 12.6031626795344, 17.8287811089804,
>     11.3613955331659, 15.8064741020529, 15.1007855146758,
>     16.0553036215393, 15.7553570530353, 15.9747058600332,
>     15.776715745005, 15.8588066550904, 16.2935434944118,
>     16.271207673964, 16.3660489506706, 16.3273070282017,
>     15.7632383068689, 14.6030467398838, 14.7118820283521,
>     14.7577545959238, 14.7315311764619, 14.8250084466403,
>     15.6652803936783, 15.8249587405285, 15.6558660906456,
>     15.5387042614836, 14.8487696278309, 15.5477380355109,
>     15.9451465974129, 16.196755792715, 15.9999119421954,
>     15.8660714836595, 15.9406577104549, 15.8754613979164,
>     16.0358944927638, 16.1785092456522, 16.1992122284106,
>     15.8087128474547, 15.9373968104322, 16.1432636222427,
>     16.2412011305004, 15.9488234774507, 15.7820255767261,
>     15.7730361533934, 15.7459893802453, 20.7777738189812,
>     21.7489122647969, 21.0374490930058, 20.9765158780184,
>     21.0464959041766, 21.6790715518273, 21.8021013715842,
>     20.7652083875471, 20.6663696521617, 20.3963413756589,
>     20.7983642126234, 20.1864915044977, 20.4422216681915,
>     20.59064186918, 20.6964531077756, 20.6822196619653,
>     20.4532414913665, 20.8126113450884, 20.4397608946311,
>     21.4603719009067, 21.5318145314919, 21.0400816517662,
>     21.0466431076593, 20.7459819969019, 20.6723053403015,
>     20.4793421418014, 20.6432035537608, 20.6831942471622,
>     21.6913537667357, 20.6562913013787, 21.0940693071186,
>     20.9473294479256, 20.5087271424267, 16.0871520250047,
>     16.3816612332698, 16.998645516939, 15.7912392142223,
>     14.5058735666446, 13.6035104425928, 14.4369066987207,
>     14.6998435295626, 14.6818972267862, 14.1086877961546,
>     14.3539049235617, 15.40862828087, 15.0657947671893,
>     14.8615716011254, 14.5538692431961, 14.2397476835569,
>     13.4381420777437, 13.4499224158638, 13.6887966810545,
>     14.6550275257018, 13.500966330283, 14.9271297886953,
>     14.7405186421119, 15.0047910398043, 14.7051463678038,
>     14.8325933769599, 12.9854861991046, 13.4203550220891,
>     15.399010832952, 15.4064707685293, 15.0953970227926,
>     15.0712109416537, 15.7587957644032, 15.0013202225009,
>     15.7608498673217, 14.7604080920677, 14.2478533598602,
>     14.4140245098782, 14.7936541075062, 14.7684428120549,
>     14.595607155062, 16.1507389488284, 16.4915712924337,
>     14.490161446684, 14.721633263063, 14.4341721012904,
>     15.8747652729112, 14.543333961671, 14.8633635585377,
>     14.6696601802386, 13.3020676725265, 14.0190694293311,
>     15.2168973938334, 12.6304946615056, 12.1972166931101,
>     12.7960396088298, 14.4285564621952, 14.5308330346953,
>     14.1496677436943, 14.0823985634278, 12.8407779235951,
>     14.6543003749437, 14.3202364452416, 15.1723493709662,
>     14.0744760007345, 14.8132801684508, 12.9183042336999,
>     14.5225202325766, 13.742309436084)), .Names = c("cpdID", "time",
>     "treatment",
> "expREP", "techREP", "Y")))


From ted.harding at nessie.mcc.ac.uk  Thu Mar  8 18:39:16 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 08 Mar 2007 17:39:16 -0000 (GMT)
Subject: [R] curve of density on histogram
In-Reply-To: <45F03C2A.4040901@sfu.ca>
Message-ID: <XFMail.070308173916.ted.harding@nessie.mcc.ac.uk>

On 08-Mar-07 Michael Toews wrote:
> Your are so close ... you just need to specify that you want your 
> histograms to show density, not percent. I've only edited one line of 
> your script, the rest is good and plots nicely:
> 
> histogram(~ resp | group, col="steelblue",type="density",

The only problem with that solution is that the y-scales are
now in density units (integrating to 1), not counts (integrating
to N) -- which may be what the user wants to see.

I still hope someone can put us out of our misery by showing
how to make the bin-width available!

best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 08-Mar-07                                       Time: 17:38:56
------------------------------ XFMail ------------------------------


From rolf at math.unb.ca  Thu Mar  8 18:48:46 2007
From: rolf at math.unb.ca (rolf at math.unb.ca)
Date: Thu, 8 Mar 2007 13:48:46 -0400 (AST)
Subject: [R] sending a vector of characters as arguments for a function
Message-ID: <200703081748.l28Hmkrr007736@weisner.math.unb.ca>

Pascal Boisson wrote:

<snip>

> I have variables C_1, C_2, C_3 ... that corresponds to vectors of size
> n, and I would like to cbind them.
> 
> I get a vector of their names using :  temp<- ls(pat="C")
> 
> And I like to find a way to use this vector temp as an argument to
> cbind.
> 
> Unfortunately, my various trials failed, here are some samples of what I
> tried :

<snip>

Try:
        s0 <- ls(pat="^C") # Notice the ``^'' --- for safety.
        m  <- do.call("cbind",lapply(s0,"get")) # Names get lost.
        colnames(m) <- s0  # So tack them on.

                        cheers,

                                Rolf Turner
                                rolf at math.unb.ca


From mwtoews at sfu.ca  Thu Mar  8 18:50:48 2007
From: mwtoews at sfu.ca (Michael Toews)
Date: Thu, 08 Mar 2007 09:50:48 -0800
Subject: [R]  Using logarithmic y-axis (density) in a histogram
Message-ID: <45F04CF8.2080303@sfu.ca>

You might also want to try "density", since it can theoretically have 
non-zero bins, since it doesn't use "bins". For example, take a Weibull 
distribution, which could look better with a log y-axis:

x <- rweibull(1000,1,5)
par(mfrow=c(2,1))
plot(density(x,from=0))
rug(x)
plot(density(x,from=0),log="y")
rug(x)

you may need to fiddle with the "bw" (bandwidth) parameter of "density", 
since this controls the smoothness of the kernel (see ?density).
+mt


From weller at erdw.ethz.ch  Thu Mar  8 19:05:15 2007
From: weller at erdw.ethz.ch (Andy Weller)
Date: Thu, 08 Mar 2007 19:05:15 +0100
Subject: [R] R GUI in Ubuntu?
Message-ID: <45F0505B.9040702@erdw.ethz.ch>

Dear all,

I am very new to R and find the terminal-based UI a little daunting. 
(That's probably the wrong thing to say!) Having searched the Packages 
it seems that I can have either a Gnome-based or Java-based GUI for my 
Ubuntu machine. However, I can get neither to work.

Having run R as root, I then run the following command:
install.packages("gnomeGUI", dependencies=TRUE)

The output of which is:
checking for gnomeConf.sh file in /usr/local/lib... not found
configure: error: conditional "HAVE_ORBIT" was never defined.
Usually this means the macro was only invoked conditionally.
ERROR: configuration failed for package 'gnomeGUI'
* Removing '/usr/local/lib/R/site-library/gnomeGUI'

I have checked to see if I have all dependencies installed - it seems as 
though I have. No luck! So I try the Java-based GUI with:
install.packages("JGR",dep=TRUE)
library(JGR)
JGR()

No luck. So, out of R I try:
sudo R CMD javareconf

Then in R, if I check the library with:
library(JGR)

I get:
Error: .onLoad failed in 'loadNamespace' for 'rJava'
Error: package 'rJava' could not be loaded

HMMmmm - still no joy! I guess I am missing something very basic here?!

Thanks in advance, Andy


From h.wickham at gmail.com  Thu Mar  8 19:30:18 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 8 Mar 2007 12:30:18 -0600
Subject: [R] how to "apply" functions to unbalanced data in long format
	by factors......cant get "by" or "aggregate" to work
In-Reply-To: <e8cddd580703080934l68f57b71jeaacc70e3514f952@mail.gmail.com>
References: <e8cddd580703080934l68f57b71jeaacc70e3514f952@mail.gmail.com>
Message-ID: <f8e6ff050703081030s3adc0b4epdaf3860c55c00b87@mail.gmail.com>

> Hello R-users
> The help I received from Petr helped me created this solution to my problems.
>
> t1<-with(mydata ,aggregate(mydata$Y,
> list(mydata$time,mydata$treatment, mydata$expREP, mydata$techREP) ,
> median, na.rm=T)) ### find median by factors ####
>
> colnames(t1)<-c("time","treatment","expREP","techREP","Y50") ### column name ##
>
> newdata<-merge(mydata, t1, by.x= names(mydata)[2:5],
> by.y=names(t1)[1:4], all=T)
>

Another way is to use the reshape package, http://had.co.nz/reshape

library(reshape)
molten <- melt(mydata, m="log2Abun")

cast(molten, time + treatment +  expREP + techREP ~ ., median)

# You can also create many other "shapes" easily:
cast(molten, expREP + techREP ~ time + treatment , median)
cast(molten, expREP + techREP ~ time + treatment , median, margins=TRUE)

Hadley


From ggrothendieck at gmail.com  Thu Mar  8 20:10:38 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 8 Mar 2007 14:10:38 -0500
Subject: [R] sending a vector of characters as arguments for a function
In-Reply-To: <770E4329A79EC94CB16D24B5692BDD8FB21A2C@sar-mail1.messagerie.ve>
References: <770E4329A79EC94CB16D24B5692BDD8FB21A2C@sar-mail1.messagerie.ve>
Message-ID: <971536df0703081110l3263a3fbj375b33c986c2342d@mail.gmail.com>

Try this:

C1 <- 1:3
C2 <- 4:6
sapply(ls(pattern = "C"), get)

On 3/8/07, BOISSON, Pascal <Pascal.BOISSON at veolia.com> wrote:
> Dear all,
>
>
>
> It seems to be a recurrent problem to me and I am asking your help to
> get over it once for all ...
>
>
>
> My idea is :
>
>
>
> I have variables C_1, C_2, C_3 ... that corresponds to vectors of size
> n, and I would like to cbind them.
>
>
>
> I get a vector of their names using :  temp<- ls(pat="C")
>
> And I like to find a way to use this vector temp as an argument to
> cbind.
>
>
>
> Unfortunately, my various trials failed, here are some samples of what I
> tried :
>
>
>
>
>
> Attempt :
>
> s0<-ls(pat="C")
>
> s3<-split(s0, f=s0)  #as do.call need a list of arguments
>
> do.call("cbind", s3)  #failed
>
>
>
>
>
> attempt :
>
> s1<-as.character(rbind(ls(pat="C"), ", "))
>
> s2<-paste( s1[1:(length(s1)-1)], collapse="")
>
>
>
>    parse(text=paste(as.character(parse(text=s2)) #I couldn't manage to
> get anywhere with this ...
>
>
>
> It appears that I have a problem with "expressions" management, and I
> couldn't find any clue in ?parse or ?expression
>
>
>
> Thanks for any comment on this
>
> Sincerely,
>
> Pascal Boisson
>
>
>
>
>
> ___________________________________________________________________________________
>
> Protegeons ensemble l'environnement : avez-vous besoin d'imprimer ce courrier electronique ?
> ___________________________________________________________________________________
>
> Les informations figurant sur cet e-mail ont un caractere strictement confidentiel et sont exclusivement adressees au destinataire mentionne ci-dessus.Tout usage, reproduction ou divulgation de cet e-mail est strictement interdit si vous n'en etes pas le destinataire. Dans ce cas, veuillez nous en avertir immediatement par la meme voie et detruire l'original. Merci
>
> This e-mail is intended only for use of the individual or entity to which it is addressed and may contain information that is privileged, confidential and exempt from disclosure under applicable law.
> Any use, distribution or copying of this e-mail communication is strictly prohibited if you are not the addressee. If so, please notify us immediately by e-mail, and destroy the original. Thank you.
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ruhil at ohio.edu  Thu Mar  8 20:09:38 2007
From: ruhil at ohio.edu (Anirudh V. S. Ruhil)
Date: Thu, 08 Mar 2007 14:09:38 -0500
Subject: [R] Drawing sub-samples
Message-ID: <5C03BBFEE727DF9B53B9F498@DEL-LAP2.ilgard.ohiou.edu>

Folks,

I have a dataframe (snippet shown below).

>  demo.df[1:10, 1:6]
    dirn   county year exp 	exp.wave 	r3
1  43901 Cuyahoga 2006    0         0 56
2  49098 Pickaway 2006    0         0 77
3  44164  Portage 2006    0         0 85
4  44610    Wayne 2006    1         1 76
5  45120    Wayne 2006    0         0 82
6  49593   Scioto 2006    1         1 89
7  46516 Crawford 2006    0         0 75
8  50054   Summit 2006    0         0 92
9  48231    Lucas 2006    0         0 79
10 49908    Stark 2006    0         0 90

If I need to draw all possible samples of size n (where n = 30, n = 50, and 
so on), and run a particular linear model on each subsample of a specific 
size, then tabulate coeffs on specific covariates for subsequent 
manipulation/graphical representation, what would be the best means of 
doing so?

"sample.1 <- sample(demo.df, 30, replace = FALSE)"

doesn't work; it just gives me the entire population in demo.df. What am I 
missing?

thanks in advance

Ani

Anirudh V. S. Ruhil, Ph.D.
Sr. Research Associate
Voinovich Center for Leadership and Public Affairs
Ohio University
Building 21, The Ridges
Athens, OH 45701-2979
Tel: 740.597.1949 | Fax: 740.597.3057


From murdoch at stats.uwo.ca  Thu Mar  8 20:17:40 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 08 Mar 2007 14:17:40 -0500
Subject: [R] curve of density on histogram
In-Reply-To: <XFMail.070308162948.ted.harding@nessie.mcc.ac.uk>
References: <XFMail.070308162948.ted.harding@nessie.mcc.ac.uk>
Message-ID: <45F06154.6060403@stats.uwo.ca>

On 3/8/2007 11:29 AM, (Ted Harding) wrote:
> On 08-Mar-07 KOITA Lassana - STAC/ACE wrote:
>> 
>> Hi R users,
>> I would like to know why these following curve densities don't appear
>> correctly on the histograms.
>> Thank you for your help
>> 
>> 
>> 
>> library(lattice)
>> library(grid)
>> resp  <- rnorm(2000)
>> group <- sample(c("G1", "G2", "G3", "G4"), replace = TRUE, size = 1000)
>> histogram(~ resp | group, col="steelblue",
>>   panel = function(x, ...){
>>     std <- if(length(x) > 0) format(round(sd(x), 2), nsmall = 2) else
>> "NA"
>>     n <- length(x)
>>     m <- if(length(x) > 0) format(round(mean(x), 2), nsmall = 2) else
>> "NA"
>>     panel.histogram(x, ...)
>>     panel.mathdensity(dmath = dnorm, col = "green",
>>                                 args = list(mean=mean(x),sd=sd(x)))
>>     panel.abline(v= mean(x), col = "red")
>>     panel.xyplot(x = jitter(x),type="p",pch=20,y = rep(0, length(x)),
>> col='yellow' )
>> 
>>     x1 <- unit(1, "npc") - unit(2, "mm")
>>     y1 <- unit(1, "npc") - unit(2, "mm")
>>     grid.text(label = bquote(n == .(n)), x = x1, y = y1, just =
>> "right")
>>     grid.text(label = bquote(hat(m) == .(m)), x = x1, y = y1 - unit(1,
>> "lines"), just = "right")
>>     grid.text(label = bquote(hat(s) == .(std)), x = x1, y = y1 -
>> unit(2, "lines"), just = "right")
>> 
>> })
> 
> The following is an approximation to what you want to do.
> However, it needs one thing to be determined, which I have
> not managed to work out how to do.
> 
> The reason for your bad density plots is that the density
> function dnorm needs to be scaled (by the number of values
> whose histogram is being lotted) and by the width of the
> intervals.
> 
> Hence I first define a function sdnorm() (for "scaled dnorm")
> and then change one line in your code, as follows:
> 
> library(lattice)
> library(grid)
> resp  <- rnorm(2000)
> group <- sample(c("G1", "G2", "G3", "G4"), replace = TRUE, size = 1000)
> #### New function sdnorm:
> sdnorm <-function(x,mean=0,sd=1,N=1,binwid=1){N*binwid*dnorm(x,mean,sd)}
> histogram(~ resp | group, col="steelblue",
>   panel = function(x, ...){
>     std <- if(length(x) > 0) format(round(sd(x), 2), nsmall = 2) else "NA"
>     n <- length(x)
>     m <- if(length(x) > 0) format(round(mean(x), 2), nsmall = 2) else "NA"
>     panel.histogram(x, ...)
> #### Changed line:
>     panel.mathdensity(dmath = sdnorm, col = "green",
>           args = list(mean=mean(x),sd=sd(x),N=length(x),binwid=0.10))
>     panel.abline(v= mean(x), col = "red")
>     panel.xyplot(x = jitter(x),type="p",pch=20,y = rep(0, length(x)),
> col='yellow' )
> 
>     x1 <- unit(1, "npc") - unit(2, "mm")
>     y1 <- unit(1, "npc") - unit(2, "mm")
>     grid.text(label = bquote(n == .(n)), x = x1, y = y1, just = "right")
>     grid.text(label = bquote(hat(m) == .(m)), x = x1, y = y1 - unit(1,
> "lines"), just = "right")
>     grid.text(label = bquote(hat(s) == .(std)), x = x1, y = y1 -
> unit(2, "lines"), just = "right")
> })
> 
> The argument N to sdnorm is readily available from the argument x,
> as N = length(x).
> 
> However, I cannot work out from the documentation for these panel
> functions how to determine the width of the histogram bins, which
> is argument binwid to sdnorm(). Hence I have simply set binwid=0.l0
> to illustrate the point, since this gives an approximately correct
> plot. But it will only be really correct when binwid can somehow
> be determined from the hosyogram being plotted, and it is this
> which I cannot see!

The breaks are one of the ... args passed to the panel function, so you 
can get the binwidth from there.  But there's another problem:  the 
panel.histogram function gives percent of total, so should integrate to 
100, not to N.  I think this version gives what is wanted:

library(lattice)
library(grid)
resp  <- rnorm(2000)
group <- sample(c("G1", "G2", "G3", "G4"), replace = TRUE, size = 1000)
#### New function sdnorm:
sdnorm <-function(x,mean=0,sd=1,N=1,binwid=1){N*binwid*dnorm(x,mean,sd)}
histogram(~ resp | group, col="steelblue",
   panel = function(x, ...){
     std <- if(length(x) > 0) format(round(sd(x), 2), nsmall = 2) else "NA"
     n <- length(x)
     m <- if(length(x) > 0) format(round(mean(x), 2), nsmall = 2) else "NA"
     panel.histogram(x, ...)


     breaks <- list(...)$breaks
     binwid <- breaks[2]-breaks[1]
     panel.mathdensity(dmath = sdnorm, col = "green",
           args = 
list(mean=mean(x),sd=sd(x),N=100,binwid=breaks[2]-breaks[1]))


     panel.abline(v= mean(x), col = "red")
     panel.abline(h=5)
     panel.xyplot(x = jitter(x),type="p",pch=20,y = rep(0, length(x)),
col='yellow' )

     x1 <- unit(1, "npc") - unit(2, "mm")
     y1 <- unit(1, "npc") - unit(2, "mm")
     grid.text(label = bquote(n == .(n)), x = x1, y = y1, just = "right")
     grid.text(label = bquote(hat(m) == .(m)), x = x1, y = y1 - unit(1,
"lines"), just = "right")
     grid.text(label = bquote(hat(s) == .(std)), x = x1, y = y1 -
unit(2, "lines"), just = "right")
})

Duncan Murdoch


From jholtman at gmail.com  Thu Mar  8 21:13:18 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 8 Mar 2007 15:13:18 -0500
Subject: [R] Drawing sub-samples
In-Reply-To: <5C03BBFEE727DF9B53B9F498@DEL-LAP2.ilgard.ohiou.edu>
References: <5C03BBFEE727DF9B53B9F498@DEL-LAP2.ilgard.ohiou.edu>
Message-ID: <644e1f320703081213p6d970cabm3c905f713c492aab@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070308/abd39cc2/attachment.pl 

From genomenet at gmail.com  Thu Mar  8 21:28:27 2007
From: genomenet at gmail.com (genomenet at gmail.com)
Date: Thu, 8 Mar 2007 12:28:27 -0800
Subject: [R] how to assign fixed factor in lm
Message-ID: <14610200098.20070308122827@gmail.com>

Hi there,

> Value=c(709,679,699,657,594,677,592,538,476,508,505,539)
> Lard=rep(c("Fresh","Rancid"),each=6)
> Gender=rep(c("Male","Male","Male","Female","Female","Female"),2)
> Food=data.frame(Value,Lard,Gender)
> Food
   Value   Lard Gender
1    709  Fresh   Male
2    679  Fresh   Male
3    699  Fresh   Male
4    657  Fresh Female
5    594  Fresh Female
6    677  Fresh Female
7    592 Rancid   Male
8    538 Rancid   Male
9    476 Rancid   Male
10   508 Rancid Female
11   505 Rancid Female
12   539 Rancid Female
> lm(fixed=Value~Gender,data=Food)
Call:
lm(data = Food, fixed = Value ~ Gender)

Coefficients:
(Intercept)   LardRancid   GenderMale  
      651.4       -142.8         35.5  

Warning message:
extra arguments fixed are just disregarded. in: lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) 

> lm(fixed=Value~Lard+Gender,data=Food)

Call:
lm(data = Food, fixed = Value ~ Lard + Gender)

Coefficients:
(Intercept)   LardRancid   GenderMale  
      651.4       -142.8         35.5  

Warning message:
extra arguments fixed are just disregarded. in: lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) 

I wanted to consider only one factor. But why lm(fixed=Value~Gender,data=Food)
return me two estimates of Gender and Lard. And I found the returning
results are the same as lm(fixed=Value~Lard+Gender,data=Food). Why lm
cannot do analysis of variance according to assigned formula?

Thank you very much.

Fan


From andy_liaw at merck.com  Thu Mar  8 21:39:39 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 8 Mar 2007 15:39:39 -0500
Subject: [R] how to assign fixed factor in lm
In-Reply-To: <14610200098.20070308122827@gmail.com>
References: <14610200098.20070308122827@gmail.com>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03D4C613@usctmx1106.merck.com>

Either you did not read docs sufficiently carefully, or the source where
you learn to do this from is questionable.  The lm() function has no
argument called "fixed", and the warning should have made that clear to
you.  It was sheer luck on your part that you happen to put "Value" as
the first variable in "Food", in which case lm() will treat it as the
response and the rest as predictors in the absence of a model formula.

You should try:

lm(Value ~ Gender, Food)

lm() itself has no concept of fixed or random effects.  lme() in the
"nlme" package does, and it has the "fixed" argument.

Andy

From: genomenet at gmail.com
> 
> Hi there,
> 
> > Value=c(709,679,699,657,594,677,592,538,476,508,505,539)
> > Lard=rep(c("Fresh","Rancid"),each=6)
> > Gender=rep(c("Male","Male","Male","Female","Female","Female"),2)
> > Food=data.frame(Value,Lard,Gender)
> > Food
>    Value   Lard Gender
> 1    709  Fresh   Male
> 2    679  Fresh   Male
> 3    699  Fresh   Male
> 4    657  Fresh Female
> 5    594  Fresh Female
> 6    677  Fresh Female
> 7    592 Rancid   Male
> 8    538 Rancid   Male
> 9    476 Rancid   Male
> 10   508 Rancid Female
> 11   505 Rancid Female
> 12   539 Rancid Female
> > lm(fixed=Value~Gender,data=Food)
> Call:
> lm(data = Food, fixed = Value ~ Gender)
> 
> Coefficients:
> (Intercept)   LardRancid   GenderMale  
>       651.4       -142.8         35.5  
> 
> Warning message:
> extra arguments fixed are just disregarded. in: lm.fit(x, y, 
> offset = offset, singular.ok = singular.ok, ...) 
> 
> > lm(fixed=Value~Lard+Gender,data=Food)
> 
> Call:
> lm(data = Food, fixed = Value ~ Lard + Gender)
> 
> Coefficients:
> (Intercept)   LardRancid   GenderMale  
>       651.4       -142.8         35.5  
> 
> Warning message:
> extra arguments fixed are just disregarded. in: lm.fit(x, y, 
> offset = offset, singular.ok = singular.ok, ...) 
> 
> I wanted to consider only one factor. But why 
> lm(fixed=Value~Gender,data=Food) return me two estimates of 
> Gender and Lard. And I found the returning results are the 
> same as lm(fixed=Value~Lard+Gender,data=Food). Why lm cannot 
> do analysis of variance according to assigned formula?
> 
> Thank you very much.
> 
> Fan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From auxsvr at yahoo.com  Thu Mar  8 21:40:43 2007
From: auxsvr at yahoo.com (MrJ Man)
Date: Thu, 8 Mar 2007 12:40:43 -0800 (PST)
Subject: [R] R GUI in Ubuntu?
Message-ID: <460108.69834.qm@web56404.mail.re3.yahoo.com>

I have used RKWard for KDE. It has quite a lot of
functionality, though I 
haven't used it much, as I prefer ESS (command line).
The website is at 
http://rkward.sourceforge.net/.

Cheers.



 
____________________________________________________________________________________
Expecting? Get great news right away with email Auto-Check.


From zz2122 at columbia.edu  Thu Mar  8 22:02:02 2007
From: zz2122 at columbia.edu (Jonathan Zhang)
Date: Thu, 8 Mar 2007 16:02:02 -0500
Subject: [R] ATLAS for MacBook?
Message-ID: <54794ae90703081302q26618e83qf288993f6ab9c73c@mail.gmail.com>

Dear all,

  Which ATLAS BLAS should I use for MacBook running on Windows?

  I just verified, it has a Intel Core 2 Duo 1.83 ghz CPU.

  thanks

  also, could anyone tell me how to compile an ATLAS to fit my specific system?

  thanks!

Jonathan Zhang

Marketing Department
Columbia Business School.


From zz2122 at columbia.edu  Thu Mar  8 22:02:02 2007
From: zz2122 at columbia.edu (Jonathan Zhang)
Date: Thu, 8 Mar 2007 16:02:02 -0500
Subject: [R] ATLAS for MacBook?
Message-ID: <54794ae90703081302q26618e83qf288993f6ab9c73c@mail.gmail.com>

Dear all,

  Which ATLAS BLAS should I use for MacBook running on Windows?

  I just verified, it has a Intel Core 2 Duo 1.83 ghz CPU.

  thanks

  also, could anyone tell me how to compile an ATLAS to fit my specific system?

  thanks!

Jonathan Zhang

Marketing Department
Columbia Business School.


From jkrobert at bcm.tmc.edu  Thu Mar  8 22:08:19 2007
From: jkrobert at bcm.tmc.edu (Roberts, J. Kyle)
Date: Thu, 8 Mar 2007 15:08:19 -0600
Subject: [R] augPred in lmer
Message-ID: <3FC0430478C30B4A9AF0AFF7863418F130F085@BCMEVS6.ad.bcm.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070308/4dd642b7/attachment.pl 

From A.Robinson at ms.unimelb.edu.au  Thu Mar  8 22:31:24 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Fri, 9 Mar 2007 08:31:24 +1100
Subject: [R] augPred in lmer
In-Reply-To: <3FC0430478C30B4A9AF0AFF7863418F130F085@BCMEVS6.ad.bcm.edu>
References: <3FC0430478C30B4A9AF0AFF7863418F130F085@BCMEVS6.ad.bcm.edu>
Message-ID: <20070308213124.GC92002@ms.unimelb.edu.au>

Hi Kyle,

not yet!  At least not as far as I know.

Cheers

Andrew


On Thu, Mar 08, 2007 at 03:08:19PM -0600, Roberts, J. Kyle wrote:
> I read the posts about augPred with lme, but does anyone know if there is a correlate for augPred for lmer?  Specifically, I want to be able to use it to plot projections for all groups in an lmer class object using plot(augPred(lmer.object)).
>  
> Thanks,
> Kyle
>  
> ***************************************
> J. Kyle Roberts, Ph.D.
> Baylor College of Medicine
> Center for Educational Outreach
> One Baylor Plaza, MS:  BCM411
> Houston, TX   77030-3411
> 713-798-6672 - 713-798-8201 Fax
> jkrobert at bcm.edu
> ***************************************
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From g.abraham at ms.unimelb.edu.au  Thu Mar  8 23:12:48 2007
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Fri, 09 Mar 2007 09:12:48 +1100
Subject: [R] Estimating parameters of 2 phase Coxian using optim
In-Reply-To: <C215950B.924%lhill07@qub.ac.uk>
References: <C215950B.924%lhill07@qub.ac.uk>
Message-ID: <45F08A60.6080604@ms.unimelb.edu.au>

Laura Hill wrote:
> 
> 
> On 7/3/07 00:15, "Gad Abraham" <g.abraham at ms.unimelb.edu.au> wrote:
> 
>> Andy Fugard wrote:
>>> Hi There,
>>>
>>> Perhaps the problem is the line
>>>
>>>          loglik<-log(p %*% expm(Q * y(i)) %*% q)
>>>
>>> You mention that y is a vector but here you're treating it as a
>>> function.  Maybe try
>>>
>>>          loglik<-log(p %*% expm(Q * y[i]) %*% q)
>>>
>>> ?
>>>
>>> Don't have a clue about the correctness of the contents of cox2.lik...
>>>
>>> Andy
>>>
>>>
>>> On 6 Mar 2007, at 08:54, Laura Hill wrote:
>>>
>>>> Hi,
>>>>
>>>> My name is Laura. I'm a PhD student at Queen's University Belfast
>>>> and have
>>>> just started learning R. I was wondering if somebody could help me
>>>> to see
>>>> where I am going wrong in my code for estimating the parameters
>>>> [mu1, mu2,
>>>> lambda1] of a 2-phase Coxian Distribution.
>>>>
>>>> cox2.lik<-function(theta, y){
>>>>     mu1<-theta[1]
>>>>
>>>>     mu2<-theta[2]
>>>>
>>>>     lambda1<-theta[3]
>>>>
>>>>     p<-Matrix(c(1, 0), nrow=1, ncol=2)
>>>>
>>>>     Q<-Matrix(c(-(lambda1 + mu1), 0, lambda1, -mu2), nrow=2, ncol=2)
>>>>
>>>>     q<-Matrix(c(mu1, mu2), nrow=2, ncol=1)
>>>>
>>>>     for (i in 1:length(y)){
>>>>         loglik<-log(p %*% expm(Q * y(i)) %*% q)
>>>>     return(loglik)}
>>>>
>>>>     sumloglik<-sum(loglik)
>>>>
>>>>     return(-sumloglik)
>>>>     }
>> Just to add my 2 AU cents regarding the for loop:
>>
>> You're trying to create a vector of log likelihoods to sum up later, but
>> that's not what's happening there. Instead, assign an empty vector of
>> same length as y, then assign the loglik from each iteration to a
>> different cell.
>>
>> Lastly, there's no need to return anything from a for loop, it's not a
>> function.
>>
>> HTH,
>> Gad
> 
> 
> 
> Hi Gad,
> 
> Yes that's exactly hat I am trying to do. If I gave you a simple example,
> could you perhaps tell me how I could create a vector of log likelihoods.
> 
> 
> 
> Lets say I have 1x1 matrices:
> 
> p=[1]
> Q=[0.05]      i.e. [mu1]
> q=[-0.05]     i.e. [-mu1]
> 
> Where mu1 is the parameter that I would like to estimate and I have chosen
> the initial value mu1=0.05
> 
> 
> Loglik<-p %*% expm(Q*y) %*% q
> 
> Where y=(5 10)
> 
> I want to sum the log likelihoods that I get for y=5 and y=10 using
> 
> Sumloglik<-sum(allloglik)
> 
> Where allloglik = vector of log likelihoods
> 
> 
> Any help would be greatly appreciated.
> 
> Thanks in advance
> Laura
> 

Hi Laura,

Make an empty vector of required length, then assign the loglik to each 
of its cells, and don't return() anything:

loglik <- rep(0, length(y))
for(i in 1:length(y)){
    loglik[i] <- log(p %*% expm(Q * y[i]) %*% q)
}

Then you can sum(loglik) like you did before.

Cheers,
Gad

-- 
Gad Abraham
Department of Mathematics and Statistics
The University of Melbourne
Parkville 3010, Victoria, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From bates at stat.wisc.edu  Thu Mar  8 23:52:47 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 8 Mar 2007 16:52:47 -0600
Subject: [R] augPred in lmer
In-Reply-To: <20070308213124.GC92002@ms.unimelb.edu.au>
References: <3FC0430478C30B4A9AF0AFF7863418F130F085@BCMEVS6.ad.bcm.edu>
	<20070308213124.GC92002@ms.unimelb.edu.au>
Message-ID: <40e66e0b0703081452r6a2ee705p89031e377eb1e269@mail.gmail.com>

On 3/8/07, Andrew Robinson <A.Robinson at ms.unimelb.edu.au> wrote:
> Hi Kyle,
>
> not yet!  At least not as far as I know.

Not as far as I know either.

> On Thu, Mar 08, 2007 at 03:08:19PM -0600, Roberts, J. Kyle wrote:
> > I read the posts about augPred with lme, but does anyone know if there is a correlate for augPred for lmer?  Specifically, I want to be able to use it to plot projections for all groups in an lmer class object using plot(augPred(lmer.object)).

Can you give us a bit more detail on the model that you have fit?  It
may be possible to get the plot that you want without having to build
all the infrastructure that Jose and I built in lme.  Deepayan has
quietly added so many wonderful capabilities to lattice relative to
earlier versions of trellis that a more direct approach often works.

Also, I think we should move the discussion to the R-sig-mixed-models
mailing list, which I am cc:ing on this reply.


From dunn at usq.edu.au  Fri Mar  9 00:44:40 2007
From: dunn at usq.edu.au (Peter Dunn)
Date: Fri, 9 Mar 2007 09:44:40 +1000
Subject: [R] Error distribution question
In-Reply-To: <20070308111151.M65971@centroin.com.br>
References: <45EFD526.4060109@eva.mpg.de>
	<20070308111151.M65971@centroin.com.br>
Message-ID: <200703090944.40381.dunn@usq.edu.au>

> > I was wondering if somebody could offer me some advice on which
> > error distribution would be appropriate for the type of data I have.
> > I'm studying what continuous predictor variables such as grooming
> > received, rank, etc. affect the amount of grooming given. This
> > response variable is continuous with many zeros, and so positively
> > skewed.
>
> This kind of variable is very common in prospecting (oil, mining)
> industries, and also in medical research. It's neither continuous
> nor discrete, because of the weight on zero. Basically, it is a
> combination of _two_ variables:
>
> X: a Bernoulli trial, such that p(X = 0) = 1 - p (failure) and
>    p(X = 1) = p (success)
>
> Y: the continous variable that represents numerically the success
>
> So, we have the final variable as X * Y.

Indeed, the Tweedie distribution may be just what you are 
after.

> I realized in the Tweedie help page that one can use a specific response
> distribution  (Normal, Poisson, Compound Poisson, etc) by setting the
> variance power =  to a specific number. I'm a beginner, so I really don't
> follow then,  

This sounds like you have the  tweedie  package.

And yes, the variance.power tells you which distribution you have.
Tweedie distributions have a variance of the form var[Y] = phi * mu^p
for some variance.power  p.  (Note Tweedie distns belong to the
exponential family, so can be used in the generalized linear model
framework.)

The mixed distributions you talk about (continuous, plus a positive
mass at zero) correspond to tweedie distributions with 1 < p < 2.
(p=2 is the gamma; p=0 is Normal; p=3 is inverse Gaussian; p=1
and phi=1 is Poisson).

> which response distribution to use (i.e. what variance power) that would 
> be appropriate for continuous response data with many zeros. 

If you want to use a tweedie distn in practice, you first need to know
*which* Tweedie distribution you need; that is, what value of p is
appropriate.  To do that, use the  tweedie.profile function in
package  tweedie.  tTat will tell you what value of p is approprioate
for your data.  For the sake of an example, suppose you wish to fit
a model something like  Y ~ x1  + x2; use  tweedie.profile
and you get p = 1.6:

tweedie.profile(Y ~ x1 + x2, p.vec=seq(1.1, 1.9, length=10), 
	do.plot=TRUE)

Then, you can fit the appropriate generalized linear model if you wish
as follow, using package  statmod:

glm( Y ~ x1 + x2, family=tweedie(variance.power=1,.6, link.power=0)

(link.power=0 means a log, and is a commonly used link.)

Hope that's of some help.

P.
-- 
Dr Peter Dunn  |  dunn <at> usq.edu.au
Faculty of Sciences, USQ; http://www.sci.usq.edu.au/staff/dunn
Aust. Centre for Sustainable Catchments: www.usq.edu.au/acsc

This email (including any attached files) is confidential an...{{dropped}}


From macq at llnl.gov  Fri Mar  9 00:55:35 2007
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 8 Mar 2007 15:55:35 -0800
Subject: [R] Query about using setdiff
In-Reply-To: <934360.45920.qm@web43136.mail.sp1.yahoo.com>
References: <934360.45920.qm@web43136.mail.sp1.yahoo.com>
Message-ID: <p06230915c216525d718c@[128.115.153.6]>



names(DF1)[ !(names(DF1) %in% names(DF2)) ]

To illustrate:

>  x <- letters[1:5]
>  y <- letters[c(2,4)]
>  x[ !(x %in% y) ]
[1] "a" "c" "e"


At 11:41 AM -0800 3/7/07, lalitha viswanath wrote:
>Hi
>I have two dataframes
>names(DF1) = c("id", "val1", "val2");
>
>names(DF2) = c("id2");
>
>Ids in DF2 are a complete subset of those in DF1
>
>How can I extract entries from DF1 where id NOT IN
>DF2.
>
>I tried setdiff(DF1, DF2); setdiff(DF1$id, DF2$id),
>etc.
>Although the latter eliminates the ids as required, I
>dont know how to extract val1 and val2 for the
>resultant set.
>
>
>Thanks
>Lalitha
>
>
>
>____________________________________________________________________________________
>8:00? 8:25? 8:40? Find a flick in no time
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA


From bunny at lautloscrew.com  Fri Mar  9 01:01:41 2007
From: bunny at lautloscrew.com (bunny , lautloscrew.com)
Date: Fri, 9 Mar 2007 01:01:41 +0100
Subject: [R] dendrogram / clusteranalysis plotting
Message-ID: <AB5454CB-E33F-4B43-9E49-3AC3EE2D3192@lautloscrew.com>

Dear all,

i performed a clusteranalysis - which worked so far...
i plotted the dendrogram and sooo many branches, a rough sketch would  
be enough ;)

i tried max.levels therefore which worked, but not for the plot...

i used the following

plot(hcd,nodePar =nP, str(hcd,max.level=1))

the output on the terminal was:

--[dendrogram w/ 2 branches and 196 members at h = 2.70]
   |--[dendrogram w/ 2 branches and 34 members at h = 1.79] ..
   `--[dendrogram w/ 2 branches and 162 members at h = 1.95] ..

which is great !

but i cant get it done for the plot, the plot always shows all the  
branches...!
does anybody know how to fix this one ?

thx in advance

-m.


From jeffmiller at alphapoint05.net  Fri Mar  9 01:13:37 2007
From: jeffmiller at alphapoint05.net (Jeff Miller)
Date: Thu, 8 Mar 2007 19:13:37 -0500
Subject: [R] Zero-inflated predictor
In-Reply-To: <200703090944.40381.dunn@usq.edu.au>
Message-ID: <!&!AAAAAAAAAAAYAAAAAAAAAEuLs8ZPMuFInPbB+1JmPy7CgAAAEAAAAEtrIkN8VqhLnIk78xRKcm0BAAAAAA==@alphapoint05.net>

 
Hi all,

Does anyone know how to deal with a zero-inflated count PREDICTOR? I know we
can use ZIP, Hurdle, etc for zero-inflated response variables, but what if
the problem occurs with one of the covariates?

I have already found that the literature is correct in stating that any
transformation will just lead to a distribution with a different inflated
value, so the arcsin is out.

Thanks,
Jeff


From bunny at lautloscrew.com  Fri Mar  9 02:00:23 2007
From: bunny at lautloscrew.com (bunny , lautloscrew.com)
Date: Fri, 9 Mar 2007 02:00:23 +0100
Subject: [R] how can i group branches of a dendrogram
Message-ID: <59185AA4-ADA0-40A7-90A6-B1918A78B20C@lautloscrew.com>

Hi all,

how can i group branches of a dendrogram ?

thx in advance


From A.Robinson at ms.unimelb.edu.au  Fri Mar  9 01:58:38 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Fri, 9 Mar 2007 11:58:38 +1100
Subject: [R] Zero-inflated predictor
In-Reply-To: <!&!AAAAAAAAAAAYAAAAAAAAAEuLs8ZPMuFInPbB+1JmPy7CgAAAEAAAAEtrIkN8VqhLnIk78xRKcm0BAAAAAA==@alphapoint05.net>
References: <200703090944.40381.dunn@usq.edu.au>
	<!&!AAAAAAAAAAAYAAAAAAAAAEuLs8ZPMuFInPbB+1JmPy7CgAAAEAAAAEtrIkN8VqhLnIk78xRKcm0BAAAAAA==@alphapoint05.net>
Message-ID: <20070309005838.GG92002@ms.unimelb.edu.au>

Hi Jeff,

I'm not sure why there would be a problem with a zero-inflated count
predictor.  As far as I am aware, no regression assumptions are
predicated on the distribution of the predictor variables.

However, should you wish to, it seems to me that you should be able to
do anything you like with it.  It might be worth taking the
traditional hurdle approach, and breaking the predictor into two
predictors, one of which is zero/not zero, and the other of which is
NA at 0.  I'm not sure that you'd get a wholly satisfactory analysis
though.

I hope that this helps, 

Andrew



On Thu, Mar 08, 2007 at 07:13:37PM -0500, Jeff Miller wrote:
>  
> Hi all,
> 
> Does anyone know how to deal with a zero-inflated count PREDICTOR? I know we
> can use ZIP, Hurdle, etc for zero-inflated response variables, but what if
> the problem occurs with one of the covariates?
> 
> I have already found that the literature is correct in stating that any
> transformation will just lead to a distribution with a different inflated
> value, so the arcsin is out.
> 
> Thanks,
> Jeff
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From Researchjj at gmail.com  Fri Mar  9 02:49:47 2007
From: Researchjj at gmail.com (JJ)
Date: Thu, 08 Mar 2007 17:49:47 -0800
Subject: [R] rattle()->RData->Explore->GGobi->>libggobi-0.dll--Error
Message-ID: <1173404987.525948.161900@h3g2000cwc.googlegroups.com>

Hello,

I am using R-2.4.1 with Rattle()

i load Rdata (ttData) which has 2columns and 66 rows

When i  execute Explore under GGobi for visualization i am facing the
problem,

---- libggobi-0.dll not found
---libggobi-0.dll was not found.... reinstalling the application may
fix the problem
 i try install.packages("rggobi") its been installed from CRAN however
when i use it from rattle()
it give me the *.dll Error

please find the below line of Error comments

`````````````````````````````````````````````````````````````````````````````````````````````````````````````
LoadLibrary failure:  The specified module could not be found.

Error in fun(...) : Could not load the rggobi library - please ensure
GGobi is on the library path
Error: .onLoad failed in 'loadNamespace' for 'rggobi'
Error in eval(expr, envir, enclos) : could not find function "ggobi"


Need your help

JJ


From xmeng at capitalbio.com  Fri Mar  9 03:37:04 2007
From: xmeng at capitalbio.com (XinMeng)
Date: Fri, 09 Mar 2007 10:37:04 +0800
Subject: [R] color key of heatmap.2
Message-ID: <373407824.21574@capitalbio.com>

Hi all:
The color key of heatmap.2 is as follows if I use redgreen style:
low level:red
high leve:green

And what I want is:
low level:green
hight level:red

How can I do it then?

Thanks a lot for your help!

My best!


From h.wickham at gmail.com  Fri Mar  9 03:09:16 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 8 Mar 2007 20:09:16 -0600
Subject: [R] rattle()->RData->Explore->GGobi->>libggobi-0.dll--Error
In-Reply-To: <1173404987.525948.161900@h3g2000cwc.googlegroups.com>
References: <1173404987.525948.161900@h3g2000cwc.googlegroups.com>
Message-ID: <f8e6ff050703081809i74c82d37s5080cbeee095a299@mail.gmail.com>

On 3/8/07, JJ <Researchjj at gmail.com> wrote:
> Hello,
>
> I am using R-2.4.1 with Rattle()
>
> i load Rdata (ttData) which has 2columns and 66 rows
>
> When i  execute Explore under GGobi for visualization i am facing the
> problem,
>
> ---- libggobi-0.dll not found
> ---libggobi-0.dll was not found.... reinstalling the application may
> fix the problem
>  i try install.packages("rggobi") its been installed from CRAN however
> when i use it from rattle()
> it give me the *.dll Error
>
> please find the below line of Error comments
>
> `````````````````````````````````````````````````````````````````````````````````````````````````````````````
> LoadLibrary failure:  The specified module could not be found.
>
> Error in fun(...) : Could not load the rggobi library - please ensure
> GGobi is on the library path
> Error: .onLoad failed in 'loadNamespace' for 'rggobi'
> Error in eval(expr, envir, enclos) : could not find function "ggobi"

Did you install GGobi first?  See http://www.ggobi.org/downloads.

Hadley


From nilsson.henric at gmail.com  Fri Mar  9 03:26:18 2007
From: nilsson.henric at gmail.com (Henric Nilsson (Public))
Date: Fri, 09 Mar 2007 03:26:18 +0100
Subject: [R] rattle()->RData->Explore->GGobi->>libggobi-0.dll--Error
In-Reply-To: <1173404987.525948.161900@h3g2000cwc.googlegroups.com>
References: <1173404987.525948.161900@h3g2000cwc.googlegroups.com>
Message-ID: <45F0C5CA.9060509@gmail.com>

Den 2007-03-09 02:49, JJ skrev:
> Hello,
> 
> I am using R-2.4.1 with Rattle()
> 
> i load Rdata (ttData) which has 2columns and 66 rows
> 
> When i  execute Explore under GGobi for visualization i am facing the
>  problem,
> 
> ---- libggobi-0.dll not found ---libggobi-0.dll was not found....
> reinstalling the application may fix the problem i try
> install.packages("rggobi") its been installed from CRAN however when
> i use it from rattle() it give me the *.dll Error
> 
> please find the below line of Error comments
> 
> `````````````````````````````````````````````````````````````````````````````````````````````````````````````
>  LoadLibrary failure:  The specified module could not be found.
> 
> Error in fun(...) : Could not load the rggobi library -
> please ensure GGobi is on the library path
   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This is most likely the cause, since `libggobi-0.dll' is shipped with
GGobi itself (it's not included in the `rggobi' package). So, assuming 
that you have installed GGobi itself, you need to add it to the Windows 
search path.

(I believe that this is a misfeature of the GGobi installer. IIRC, GGobi 
is added to the path for the installing user, typically the 
administrator, but not to the system path.)


HTH,
Henric



> Error: .onLoad failed in 'loadNamespace' for 'rggobi' Error in
> eval(expr, envir, enclos) : could not find function "ggobi"
> 
> 
> Need your help
> 
> JJ
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide http://www.R-project.org/posting-guide.html and provide
> commented, minimal, self-contained, reproducible code.
>


From h.wickham at gmail.com  Fri Mar  9 03:54:47 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 8 Mar 2007 20:54:47 -0600
Subject: [R] rattle()->RData->Explore->GGobi->>libggobi-0.dll--Error
In-Reply-To: <45F0C5CA.9060509@gmail.com>
References: <1173404987.525948.161900@h3g2000cwc.googlegroups.com>
	<45F0C5CA.9060509@gmail.com>
Message-ID: <f8e6ff050703081854w6e83cf28v867231595cb0faf3@mail.gmail.com>

On 3/8/07, Henric Nilsson (Public) <nilsson.henric at gmail.com> wrote:
> Den 2007-03-09 02:49, JJ skrev:
> > Hello,
> >
> > I am using R-2.4.1 with Rattle()
> >
> > i load Rdata (ttData) which has 2columns and 66 rows
> >
> > When i  execute Explore under GGobi for visualization i am facing the
> >  problem,
> >
> > ---- libggobi-0.dll not found ---libggobi-0.dll was not found....
> > reinstalling the application may fix the problem i try
> > install.packages("rggobi") its been installed from CRAN however when
> > i use it from rattle() it give me the *.dll Error
> >
> > please find the below line of Error comments
> >
> > `````````````````````````````````````````````````````````````````````````````````````````````````````````````
> >  LoadLibrary failure:  The specified module could not be found.
> >
> > Error in fun(...) : Could not load the rggobi library -
> > please ensure GGobi is on the library path
>    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>
> This is most likely the cause, since `libggobi-0.dll' is shipped with
> GGobi itself (it's not included in the `rggobi' package). So, assuming
> that you have installed GGobi itself, you need to add it to the Windows
> search path.
>
> (I believe that this is a misfeature of the GGobi installer. IIRC, GGobi
> is added to the path for the installing user, typically the
> administrator, but not to the system path.)

We're working on that - and it should be fixed for the next version.

Hadley


From ivowel at gmail.com  Fri Mar  9 04:36:02 2007
From: ivowel at gmail.com (ivo welch)
Date: Thu, 8 Mar 2007 22:36:02 -0500
Subject: [R] pdf device bounding box?
Message-ID: <50d1c22d0703081936m5c64097dka8c670711736277e@mail.gmail.com>

dear R wizards:  I have a very simple suggestion/question.  Would it
be easy to change the pdf device so that it adds a BoundingBox around
its output?  (Under R 2.4.1, this seems not to be the case, because
epstopdf under linux complains.  Fortunately, it still works
correctly.)

This is not a big deal, but it would be nice if R did so, if only to
help some pdf programs that are less adept at dealing with pdf files
to be inserted than ghostscript/xpdf.

Sincerely,

/ivo


From jaapvw at uj.ac.za  Fri Mar  9 07:36:38 2007
From: jaapvw at uj.ac.za (Van Wyk, Jaap)
Date: Fri, 9 Mar 2007 08:36:38 +0200
Subject: [R] help with zicounts
Message-ID: <E50F9D3FF1B0494A84AC9937CB9E9608194994@apk-exch-02.ad.uj.ac.za>

Dear UseRs:

I have simulated data from a zero-inflated Poisson model, and would like
to use a package like zicounts to test my code of fitting the model.
My question is: can I use zicounts directly with the following simulated
data?

Create a sample of n=1000 observations from a ZIP model with no intercept
and a single covariate x_{i} which is N(0,1). The logit part is
   logit(p_{i})=x_{i}*beta
with beta=1, and the Poisson part is
   log(?_{i})=x_{i}*gamma
with gamma=1.

beta.true<-1.0
gamma.true<-1.0
n<-1000
x<-matrix(rnorm(n),n,1)
pi<-expit(x*beta.true)
mu<-exp(x*gamma.true)
y<-numeric(n) # blank vector
z<-(runif(n)<pi) # logical: T with prob p_i, F otherwise
y[z]<-rpois(sum(z),mu[z]) # draw y_i ~ Poisson(mu_i) where z_i = T
y[!z]<-0 # set y_i = 0 where z_i = F

Thanks for your time!
Jacob



Jacob L van Wyk
Department of Statistics
University of Johannesburg, APK
P O Box 524
Auckland Park 2006
South Africa
Tel: +27 11 489 3080
Fax: +27 11 489 2832


From petr.pikal at precheza.cz  Fri Mar  9 08:03:26 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 09 Mar 2007 08:03:26 +0100
Subject: [R] Removing duplicated rows within a matrix,
	with missing data as wildcards
In-Reply-To: <ac7b131e0703080714p5179e5bm6d7ee24f92c4c206@mail.gmail.com>
Message-ID: <45F114CE.30555.17E8D9@localhost>

Hi

its a bit tricky but

dup<-apply(x, 2, duplicated) #which are dupplucated
isna<-apply(x, 2, is.na) #which are na
check<-dup|isna # which are both

and here is your result

x[rowSums(check)!=3,]
     [,1] [,2] [,3]
[1,]    1    3    2
[2,]    2    1    3
[3,]    3    2   NA


Regards
Petr




On 8 Mar 2007 at 10:14, stacey thompson wrote:

Date sent:      	Thu, 8 Mar 2007 10:14:37 -0500
From:           	"stacey thompson" <stacey.lee.thompson at gmail.com>
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Removing duplicated rows within a matrix,
	with missing data as wildcards

> I'd like to remove duplicated rows within a matrix, with missing data
> being treated as wildcards.
> 
> For example
> 
> > x <- matrix((1:3), 5, 3)
> > x[4,2] = NA
> > x[3,3] = NA
> > x
> 
>      [,1] [,2] [,3]
> [1,]    1    3    2
> [2,]    2    1    3
> [3,]    3    2   NA
> [4,]    1   NA    2
> [5,]    2    1    3
> 
> I would like to obtain
> 
>       [,1] [,2] [,3]
> [1,]    1    3    2
> [2,]    2    1    3
> [3,]    3    2   NA
> 
> >From the R-help archives, I learned about unique(x) and
> >duplicated(x).
> However, unique(x) returns
> 
> > unique(x)
> 
>      [,1] [,2] [,3]
> [1,]    1    3    2
> [2,]    2    1    3
> [3,]    3    2   NA
> [4,]    1   NA    2
> 
> and duplicated(x) gives
> 
> > duplicated(x)
> 
> [1] FALSE FALSE FALSE FALSE  TRUE
> 
> I have tried various na.action 's but with unique(x) I get errors at
> best.
> 
> e.g.
> > unique(x, na.omit(x))
> 
> Error: argument 'incomparables != FALSE' is not used (yet)
> 
> How I might tackle this?
> 
> Thanks,
> 
> -stacey
> 
> -- 
> -stacey lee thompson-
> Stagiaire post-doctorale
> Institut de recherche en biologie v?g?tale
> Universit? de Montr?al
> 4101 Sherbrooke Est
> Montr?al, Qu?bec H1X 2B2 Canada
> stacey.thompson at umontreal.ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From ligges at statistik.uni-dortmund.de  Fri Mar  9 08:42:06 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 09 Mar 2007 08:42:06 +0100
Subject: [R] ATLAS for MacBook?
In-Reply-To: <54794ae90703081302q26618e83qf288993f6ab9c73c@mail.gmail.com>
References: <54794ae90703081302q26618e83qf288993f6ab9c73c@mail.gmail.com>
Message-ID: <45F10FCE.6010805@statistik.uni-dortmund.de>



Jonathan Zhang wrote:
> Dear all,
> 
>   Which ATLAS BLAS should I use for MacBook running on Windows?
> 
>   I just verified, it has a Intel Core 2 Duo 1.83 ghz CPU.
> 
>   thanks
> 
>   also, could anyone tell me how to compile an ATLAS to fit my specific system?


See the manual "R Installation and Administration", chapter "Installing 
R under Windows", section "Building from source", subsection "Getting 
the source files".

Uwe Ligges



>   thanks!
> 
> Jonathan Zhang
> 
> Marketing Department
> Columbia Business School.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From hpages at fhcrc.org  Fri Mar  9 09:13:40 2007
From: hpages at fhcrc.org (hpages at fhcrc.org)
Date: Fri,  9 Mar 2007 00:13:40 -0800
Subject: [R] Removing duplicated rows within a matrix,
	with missing data as wildcards
In-Reply-To: <45F114CE.30555.17E8D9@localhost>
References: <45F114CE.30555.17E8D9@localhost>
Message-ID: <1173428020.45f11734a57a3@webmail.fhcrc.org>

Quoting Petr Pikal <petr.pikal at precheza.cz>:

> Hi
> 
> its a bit tricky but
> 
> dup<-apply(x, 2, duplicated) #which are dupplucated
> isna<-apply(x, 2, is.na) #which are na
> check<-dup|isna # which are both
> 
> and here is your result
> 
> x[rowSums(check)!=3,]
>      [,1] [,2] [,3]
> [1,]    1    3    2
> [2,]    2    1    3
> [3,]    3    2   NA

Hi,

The above doesn't work. No need to have NAs in x:

  > x <- matrix(c(2,2,1,3,2,3), ncol=2, byrow=TRUE)
  > x
       [,1] [,2]
  [1,]    2    2
  [2,]    1    3
  [3,]    2    3

  > dup <- apply(x, 2, duplicated)
  > x[rowSums(check)!=2 ,]
       [,1] [,2]
  [1,]    2    2
  [2,]    1    3

Look at 'dup':

  > dup
        [,1]  [,2]
  [1,] FALSE FALSE
  [2,] FALSE FALSE
  [3,]  TRUE  TRUE

Yes, each element in the last row is a duplicate in its own col,
but this doesn't mean that the row as a whole is a duplicate.

Cheers,
H.


> 
> 
> Regards
> Petr
> 
> 
> 
> 
> On 8 Mar 2007 at 10:14, stacey thompson wrote:
> 
> Date sent:      	Thu, 8 Mar 2007 10:14:37 -0500
> From:           	"stacey thompson" <stacey.lee.thompson at gmail.com>
> To:             	r-help at stat.math.ethz.ch
> Subject:        	[R] Removing duplicated rows within a matrix,
> 	with missing data as wildcards
> 
> > I'd like to remove duplicated rows within a matrix, with missing data
> > being treated as wildcards.
> > 
> > For example
> > 
> > > x <- matrix((1:3), 5, 3)
> > > x[4,2] = NA
> > > x[3,3] = NA
> > > x
> > 
> >      [,1] [,2] [,3]
> > [1,]    1    3    2
> > [2,]    2    1    3
> > [3,]    3    2   NA
> > [4,]    1   NA    2
> > [5,]    2    1    3
> > 
> > I would like to obtain
> > 
> >       [,1] [,2] [,3]
> > [1,]    1    3    2
> > [2,]    2    1    3
> > [3,]    3    2   NA
> > 
> > >From the R-help archives, I learned about unique(x) and
> > >duplicated(x).
> > However, unique(x) returns
> > 
> > > unique(x)
> > 
> >      [,1] [,2] [,3]
> > [1,]    1    3    2
> > [2,]    2    1    3
> > [3,]    3    2   NA
> > [4,]    1   NA    2
> > 
> > and duplicated(x) gives
> > 
> > > duplicated(x)
> > 
> > [1] FALSE FALSE FALSE FALSE  TRUE
> > 
> > I have tried various na.action 's but with unique(x) I get errors at
> > best.
> > 
> > e.g.
> > > unique(x, na.omit(x))
> > 
> > Error: argument 'incomparables != FALSE' is not used (yet)
> > 
> > How I might tackle this?
> > 
> > Thanks,
> > 
> > -stacey
> > 
> > -- 
> > -stacey lee thompson-
> > Stagiaire post-doctorale
> > Institut de recherche en biologie v?g?tale
> > Universit? de Montr?al
> > 4101 Sherbrooke Est
> > Montr?al, Qu?bec H1X 2B2 Canada
> > stacey.thompson at umontreal.ca
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mwtoews at sfu.ca  Fri Mar  9 09:26:27 2007
From: mwtoews at sfu.ca (Michael Toews)
Date: Fri, 09 Mar 2007 00:26:27 -0800
Subject: [R]  pdf device bounding box?
Message-ID: <45F11A33.7010802@sfu.ca>

I apologize if I don't fully understand your question, but the pdf 
device has a "MediaBox", which is equivalent to the "BoundingBox" in EPS 
file. The PDFs from R are defined nicely using height/width dimensions, 
and work well with embedding in pdflatex, etc. For example:

pdf("test.pdf",height=3,width=3)
plot(1:10)

and view the (partially binary) output in your shell:
less -N test.pdf

on line 117 of this file, I see "/MediaBox [0 0 216 216]" which is a 3in 
by 3in box measured in PostScript points.

I don't understand how you are mixing this in with the epstopdf command. 
If you want to make both a PDF and EPS, my best advice is to do both 
directly from R (see ?postscript for EPS file generation .. the same 
example as above will have "%%BoundingBox: 0 0 216 216" on line 10), and 
your output  for both formats should be clean, simple, and good enough 
for publishers and everyone else to use.

Just one caution, if you have a Windows computer and R < 2.5.1 (which is 
most of us), make sure you write EPS files before loading up a PDF 
device (PR#9517).


From Achim.Zeileis at wu-wien.ac.at  Fri Mar  9 09:32:14 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 9 Mar 2007 09:32:14 +0100 (CET)
Subject: [R] help with zicounts
In-Reply-To: <E50F9D3FF1B0494A84AC9937CB9E9608194994@apk-exch-02.ad.uj.ac.za>
References: <E50F9D3FF1B0494A84AC9937CB9E9608194994@apk-exch-02.ad.uj.ac.za>
Message-ID: <Pine.LNX.4.64.0703090927470.5082@eowyn>

Jaap:

> I have simulated data from a zero-inflated Poisson model, and would like
> to use a package like zicounts to test my code of fitting the model.
> My question is: can I use zicounts directly with the following simulated
> data?

I guess you can use zicounts, but personally I'm more familiar with 
zeroinfl() from package "pscl" (because I have written this function :)). 
With that you can easily do:

> beta.true<-1.0
> gamma.true<-1.0
> n<-1000
> x<-matrix(rnorm(n),n,1)
> pi<-expit(x*beta.true)
> mu<-exp(x*gamma.true)
> y<-numeric(n) # blank vector
> z<-(runif(n)<pi) # logical: T with prob p_i, F otherwise
> y[z]<-rpois(sum(z),mu[z]) # draw y_i ~ Poisson(mu_i) where z_i = T
> y[!z]<-0 # set y_i = 0 where z_i = F

library("pscl")
zeroinfl(y ~ 0 + x | 0 + x)

which by default fits a ZIP (with log link and logit inflation).

hth,
Z


From lists at eva.mpg.de  Fri Mar  9 10:00:43 2007
From: lists at eva.mpg.de (lists at eva.mpg.de)
Date: Fri, 09 Mar 2007 10:00:43 +0100
Subject: [R] GLMM in lme4 and Tweedie dist.
Message-ID: <45F1223B.8050308@eva.mpg.de>

Hi there,
I've been wanting to fit a GLMM and I'm not completely sure I'm doing 
things right. As I said in a previous message my response variable is 
continuous with many zeros, so I was having a hard time finding an 
appropriate error distribution. I read some previous help mails given to 
other people advising them to use the Tweedie distribution. I'm still 
not sure if this would be appropriate for my data set, for I'm a 
beginner and really don't follow all the details. So I ran a GLMM using 
this distribution. I ran it for several models to do later model 
selection with AIC. I used the following script, where the file 
"GLMM_tweedie" (line 2) has a list of all the models I want to run, each 
one in the form [ x=lmer(GGgiv ~ Rank_1 + Rank_diff + DAI + 
Gen_dy*Rank_diff + Gen_dy*DAI + Gen_dy + (1| D_1) + (1| D_2), family = 
tweedie(var.power=1,link.power=0), offset=log(Dt), data=data) ]

  data=read.csv(file="GLMM_data.csv")
 > models<-read.table("GLMM_tweedie.txt", sep="\t")
 > data$Ggrec_Dtlog = log(data$Ggrec_Dt+1)
 > models<-as.vector(models[,1])
 > totres=c()
 > for (i in 1:79) {model=models[i]
+ res=eval(parse(text=model))
+ res=AIC(logLik(x))
+ res=as.vector(res)
+ totres=rbind(totres,res)}

The output would then be just a list of all the AIC of each model. For 1 
of the models (the one in the [] above) I'm getting the following error 
message, which I don't know what it means:

CHOLMOD warning: matrix not positive definite
Error in objective(.par, ...) : Cholmod error `matrix not positive 
definite' at file:../Supernodal/t_cholmod_super_numeric.c, line 614

Could anybody give me some advice on using Tweedie distributions and 
does anybody have an idea what this error message means.
Thanks a lot in advance,
Cheers,
Cristina.


From gavin.simpson at ucl.ac.uk  Fri Mar  9 10:05:56 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 09 Mar 2007 09:05:56 +0000
Subject: [R] dendrogram / clusteranalysis plotting
In-Reply-To: <AB5454CB-E33F-4B43-9E49-3AC3EE2D3192@lautloscrew.com>
References: <AB5454CB-E33F-4B43-9E49-3AC3EE2D3192@lautloscrew.com>
Message-ID: <1173431156.3035.2.camel@dhcppc2.my.nat.localnet>

On Fri, 2007-03-09 at 01:01 +0100, bunny , lautloscrew.com wrote:
> Dear all,
> 
> i performed a clusteranalysis - which worked so far...
> i plotted the dendrogram and sooo many branches, a rough sketch would  
> be enough ;)
> 
> i tried max.levels therefore which worked, but not for the plot...

(re-)read ?dendrogram. function cut.dendrogram() can prune a tree's
lower branches. You can plot the returned object's $upper component,
which is itself an object of class "dendrogram".

There is an example in ?dendrogram of using cut.

HTH

G

> 
> i used the following
> 
> plot(hcd,nodePar =nP, str(hcd,max.level=1))
> 
> the output on the terminal was:
> 
> --[dendrogram w/ 2 branches and 196 members at h = 2.70]
>    |--[dendrogram w/ 2 branches and 34 members at h = 1.79] ..
>    `--[dendrogram w/ 2 branches and 162 members at h = 1.95] ..
> 
> which is great !
> 
> but i cant get it done for the plot, the plot always shows all the  
> branches...!
> does anybody know how to fix this one ?
> 
> thx in advance
> 
> -m.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT                          [w] http://www.freshwaters.org.uk/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From gavin.simpson at ucl.ac.uk  Fri Mar  9 10:08:41 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 09 Mar 2007 09:08:41 +0000
Subject: [R] how can i group branches of a dendrogram
In-Reply-To: <59185AA4-ADA0-40A7-90A6-B1918A78B20C@lautloscrew.com>
References: <59185AA4-ADA0-40A7-90A6-B1918A78B20C@lautloscrew.com>
Message-ID: <1173431321.3035.6.camel@dhcppc2.my.nat.localnet>

On Fri, 2007-03-09 at 02:00 +0100, bunny , lautloscrew.com wrote:
> Hi all,
> 
> how can i group branches of a dendrogram ?

Err... you'll need to give us more than that to go on. What do you mean
by group? Draw a marker round broad clusters, or prune them? Or
something else? I just replied with an answer that deals with pruning
back objects of class "dendrogram", but if this is not what you mean in
this mail, reply with an example of what you tried and a description of
exactly what you want to do, and maybe someone can help.

G

> 
> thx in advance
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT                          [w] http://www.freshwaters.org.uk/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From hpages at fhcrc.org  Fri Mar  9 10:18:30 2007
From: hpages at fhcrc.org (hpages at fhcrc.org)
Date: Fri,  9 Mar 2007 01:18:30 -0800
Subject: [R] Removing duplicated rows within a matrix,
	with missing data as wildcards
In-Reply-To: <1173428020.45f11734a57a3@webmail.fhcrc.org>
References: <45F114CE.30555.17E8D9@localhost>
	<1173428020.45f11734a57a3@webmail.fhcrc.org>
Message-ID: <1173431910.45f1266663409@webmail.fhcrc.org>

Hi again,

Your problem as you formulated it is not clearly defined.
For example, what do you want to do with this matrix:

  > x <- matrix(c(1, NA, 3, NA, 2, 3), ncol=3, byrow=TRUE)
  > x
       [,1] [,2] [,3]
  [1,]    1   NA    3
  [2,]   NA    2    3

Remove row 1, row 2 or nothing?

Maybe you want to proceed in 2 steps:
  (1) remove strict duplicated rows
  (2) remove rows with at least 1 NA that match a row with no NAs

In this case you would not remove any row from x.

The removeLooseDupRows() function below does (2) only. If you
want (1) and (2), you need to combine it with unique() by doing
either removeLooseDupRows(unique(x)) or unique(removeLooseDupRows(x))
(both should always give the same result).

removeLooseDupRows <- function(x)
{
    if (nrow(x) <= 1)
        return(x)
    ii <- do.call("order",
                  args=lapply(seq_len(ncol(x)),
                              function(col) x[ , col]))
    dup_index <- logical(nrow(x))
    i0 <- -1
    for (k in 1:length(ii)) {
        i <- ii[k]
        if (any(is.na(x[i, ]))) {
            if (i0 == -1)
                next
            if (any(x[i, ] != x[i0, ], na.rm=TRUE))
                next
            dup_index[i] <- TRUE
        } else {
            i0 <- i
        }
    }
    x[!dup_index, ]
}

  > x <- matrix((1:3), 5, 3)
  > x[4,2] = NA
  > x[3,3] = NA
  > x
       [,1] [,2] [,3]
  [1,]    1    3    2
  [2,]    2    1    3
  [3,]    3    2   NA
  [4,]    1   NA    2
  [5,]    2    1    3

  > removeLooseDupRows(x)
       [,1] [,2] [,3]
  [1,]    1    3    2
  [2,]    2    1    3
  [3,]    3    2   NA
  [4,]    2    1    3

  > removeLooseDupRows(unique(x))
       [,1] [,2] [,3]
  [1,]    1    3    2
  [2,]    2    1    3
  [3,]    3    2   NA


Cheers,
H.


Quoting hpages at fhcrc.org:

> Quoting Petr Pikal <petr.pikal at precheza.cz>:
> 
> > Hi
> > 
> > its a bit tricky but
> > 
> > dup<-apply(x, 2, duplicated) #which are dupplucated
> > isna<-apply(x, 2, is.na) #which are na
> > check<-dup|isna # which are both
> > 
> > and here is your result
> > 
> > x[rowSums(check)!=3,]
> >      [,1] [,2] [,3]
> > [1,]    1    3    2
> > [2,]    2    1    3
> > [3,]    3    2   NA
> 
> Hi,
> 
> The above doesn't work. No need to have NAs in x:
> 
>   > x <- matrix(c(2,2,1,3,2,3), ncol=2, byrow=TRUE)
>   > x
>        [,1] [,2]
>   [1,]    2    2
>   [2,]    1    3
>   [3,]    2    3
> 
>   > dup <- apply(x, 2, duplicated)
>   > x[rowSums(check)!=2 ,]
>        [,1] [,2]
>   [1,]    2    2
>   [2,]    1    3
> 
> Look at 'dup':
> 
>   > dup
>         [,1]  [,2]
>   [1,] FALSE FALSE
>   [2,] FALSE FALSE
>   [3,]  TRUE  TRUE
> 
> Yes, each element in the last row is a duplicate in its own col,
> but this doesn't mean that the row as a whole is a duplicate.
> 
> Cheers,
> H.
> 
> 
> > 
> > 
> > Regards
> > Petr
> > 
> > 
> > 
> > 
> > On 8 Mar 2007 at 10:14, stacey thompson wrote:
> > 
> > Date sent:      	Thu, 8 Mar 2007 10:14:37 -0500
> > From:           	"stacey thompson" <stacey.lee.thompson at gmail.com>
> > To:             	r-help at stat.math.ethz.ch
> > Subject:        	[R] Removing duplicated rows within a matrix,
> > 	with missing data as wildcards
> > 
> > > I'd like to remove duplicated rows within a matrix, with missing data
> > > being treated as wildcards.
> > > 
> > > For example
> > > 
> > > > x <- matrix((1:3), 5, 3)
> > > > x[4,2] = NA
> > > > x[3,3] = NA
> > > > x
> > > 
> > >      [,1] [,2] [,3]
> > > [1,]    1    3    2
> > > [2,]    2    1    3
> > > [3,]    3    2   NA
> > > [4,]    1   NA    2
> > > [5,]    2    1    3
> > > 
> > > I would like to obtain
> > > 
> > >       [,1] [,2] [,3]
> > > [1,]    1    3    2
> > > [2,]    2    1    3
> > > [3,]    3    2   NA
> > > 
> > > >From the R-help archives, I learned about unique(x) and
> > > >duplicated(x).
> > > However, unique(x) returns
> > > 
> > > > unique(x)
> > > 
> > >      [,1] [,2] [,3]
> > > [1,]    1    3    2
> > > [2,]    2    1    3
> > > [3,]    3    2   NA
> > > [4,]    1   NA    2
> > > 
> > > and duplicated(x) gives
> > > 
> > > > duplicated(x)
> > > 
> > > [1] FALSE FALSE FALSE FALSE  TRUE
> > > 
> > > I have tried various na.action 's but with unique(x) I get errors at
> > > best.
> > > 
> > > e.g.
> > > > unique(x, na.omit(x))
> > > 
> > > Error: argument 'incomparables != FALSE' is not used (yet)
> > > 
> > > How I might tackle this?
> > > 
> > > Thanks,
> > > 
> > > -stacey
> > > 
> > > -- 
> > > -stacey lee thompson-
> > > Stagiaire post-doctorale
> > > Institut de recherche en biologie v?g?tale
> > > Universit? de Montr?al
> > > 4101 Sherbrooke Est
> > > Montr?al, Qu?bec H1X 2B2 Canada
> > > stacey.thompson at umontreal.ca
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html and provide commented,
> > > minimal, self-contained, reproducible code.
> > 
> > Petr Pikal
> > petr.pikal at precheza.cz
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From kubovy at virginia.edu  Fri Mar  9 11:08:28 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Fri, 9 Mar 2007 05:08:28 -0500
Subject: [R] Problem with ci.lmer() in package:gmodels
Message-ID: <9E608E4E-0940-4FAC-8E4B-4A5900F72565@virginia.edu>

Dear Friends,

Please note that in the following CI lower > CI higher:

 > require(lmer)
 > require(gmodels)
 > fm2 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject),  
sleepstudy)
 > ci(fm2)
              Estimate  CI lower   CI upper Std. Error p-value
(Intercept) 251.66693 266.06895 238.630280   7.056447       0
Days         10.52773  13.63372   7.389946   1.646900       0



_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From kubovy at virginia.edu  Fri Mar  9 11:25:23 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Fri, 9 Mar 2007 05:25:23 -0500
Subject: [R] Is the gmodels package being maintained?
Message-ID: <CBA9D983-87CC-42B9-89D0-92C46D8B16FF@virginia.edu>

Dear r-helpers,

I sent  a cc of a recent message about a problem with ci.lmer() in  
the gmodels package to the author (Gregory R Warnes), and the message  
bounced. If the author or someone else is maintaining this package or  
this function, would you kindly supplement the author's name and/or  
address with a current maintainer and/or provide a current email  
address?
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From bunny at lautloscrew.com  Fri Mar  9 12:17:05 2007
From: bunny at lautloscrew.com (bunny , lautloscrew.com)
Date: Fri, 9 Mar 2007 12:17:05 +0100
Subject: [R] dendrogram again
Message-ID: <A5077BE5-CB55-4026-BC8B-E201BA5D5DD5@lautloscrew.com>

Hi all,

ok, i know i can cut a dendrogram, which i did.
all i get is three objects that a dendrograms itself.

for example:
myd$upper, myd$lower[[1]], myd$lower[[2]]
and so on. of course i can plot them seperately now.

but the lower parts still have hundreds of branches. i?ll need a 30 "  
widescreen to watch the whole picture.
what i?d like to is group the lower branches , so that i get a  
dendrogram with a few branches, splitting only in the upper levels.  
In terms of the cluster analysis, i just want to have a few bigger  
clusters.

thx,

m.

P.S.:
putting parts of a cutted dendrogram back into to one could be an  
idea ? is it somehow possible ?


From horacio9573 at gmail.com  Fri Mar  9 12:26:27 2007
From: horacio9573 at gmail.com (Horacio Castellini)
Date: Fri, 9 Mar 2007 08:26:27 -0300
Subject: [R] Off topic:Spam on R-help increase?
In-Reply-To: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
References: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
Message-ID: <90e13c0703090326g48edd5c0l7fb07d6863bd4ba5@mail.gmail.com>

Ya me parec?a que no me pasaba solo a mi... :)  uhmmm fall? el spam-filter?

> Folks:
>
> In the past 2 days I have seen a large increase of  spam getting into
> R-help. Are others experiencing this problem? If so, has there been some
> change to the spam filters on the R-servers? If not, is the problem on my
> end?
>
> Feel free to reply privately.
>
> Thanks.
>
> Bert Gunter
> Genentech Nonclinical Statistics
> South San Francisco, CA 94404
> 650-467-7374
>
>
>
> ______________________________________________
> R-help en stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From aolinto_r at bignet.com.br  Fri Mar  9 12:43:39 2007
From: aolinto_r at bignet.com.br (Antonio Olinto)
Date: Fri, 09 Mar 2007 08:43:39 -0300
Subject: [R] Installing R on Ubuntu 6.10 via apt-get
In-Reply-To: <45F10FCE.6010805@statistik.uni-dortmund.de>
References: <54794ae90703081302q26618e83qf288993f6ab9c73c@mail.gmail.com>
	<45F10FCE.6010805@statistik.uni-dortmund.de>
Message-ID: <45F1486B.2040702@bignet.com.br>

Hi

I'm using Linux Ubuntu 6.10 on a Pentium D 2.8.

Well, following http://cran.r-project.org/bin/linux/ubuntu/README

I wrote in the sources.list

# R
deb http://CRAN.R-project.org/bin/linux/ubuntu edgy/
deb http://www.vps.fmvz.usp.br/CRAN/bin/linux/ubuntu edgy/

But after type "apt-get update" I got

Falha ao baixar 
http://www.vps.fmvz.usp.br/CRAN/bin/linux/ubuntu/edgy/Release Unable to 
find expected entry Packages in Meta-index file (malformed Release file?)
Falha ao baixar http://CRAN.R-project.org/bin/linux/ubuntu/edgy/Release 
Unable to find expected entry Packages in Meta-index file (malformed 
Release file?)
W: Conflicting distribution: http://www.vps.fmvz.usp.br edgy/ Release 
(expected edgy but got )
W: Conflicting distribution: http://CRAN.R-project.org edgy/ Release 
(expected edgy but got )

(PS. Falha ao baixar = Fail to download)

The key of Vincent Goulet seems to be OK.

Am I doing something wrong or there's really a problem with the Release 
file?

Many thanks!

Antonio Olinto


From wwwhsd at gmail.com  Fri Mar  9 12:47:51 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Fri, 9 Mar 2007 08:47:51 -0300
Subject: [R] Installing R on Ubuntu 6.10 via apt-get
In-Reply-To: <45F1486B.2040702@bignet.com.br>
References: <54794ae90703081302q26618e83qf288993f6ab9c73c@mail.gmail.com>
	<45F10FCE.6010805@statistik.uni-dortmund.de>
	<45F1486B.2040702@bignet.com.br>
Message-ID: <da79af330703090347q5eb75f3dkd1780690e99f0a7e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070309/c749424e/attachment.pl 

From jim at bitwrit.com.au  Fri Mar  9 12:56:54 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Fri, 09 Mar 2007 22:56:54 +1100
Subject: [R] Off topic:Spam on R-help increase?
In-Reply-To: <90e13c0703090326g48edd5c0l7fb07d6863bd4ba5@mail.gmail.com>
References: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
	<90e13c0703090326g48edd5c0l7fb07d6863bd4ba5@mail.gmail.com>
Message-ID: <45F14B86.5060902@bitwrit.com.au>

Horacio Castellini wrote:
> Ya me parec?a que no me pasaba solo a mi... :)  uhmmm fall? el spam-filter?
> 
Si, es todos que lee R-news, pero maravillo, que es el palabra para 
"spam" en espa?ol?

Jim


From luke at novum.am.lublin.pl  Fri Mar  9 13:25:24 2007
From: luke at novum.am.lublin.pl (Lukasz Komsta)
Date: Fri, 9 Mar 2007 13:25:24 +0100
Subject: [R] Deconvolution of a spectrum
Message-ID: <20070309122524.GD39652@novum.am.lublin.pl>


Dear useRs,

I have a curve which is a mixture of Gaussian curves (for example UV
emission or absorption spectrum). Do you have any suggestions how to
implement searching for optimal set of Gaussian peaks to fit the curve?
I know that it is very complex problem, but maybe it is a possibility
to do it? First supposement is to use a nls() with very large functions,
and compare AIC value, but it is very difficult to suggest any starting
points for algotirithm.

Searching google I have found only a description of commercial software
for doing such deconvolution (Origin, PeakFit) without any information
about used algorithms. No ready-to-use function in any language.

I have tried to use a Mclust workaround for this problem, by generating a
large dataset for which the spectrum is a histogram and feed it into
the Mclust. The results seem to be serious, but this is very ugly and
imprecise method.

Thanks for any help,

Luke


From j.van_den_hoff at fzd.de  Fri Mar  9 13:42:27 2007
From: j.van_den_hoff at fzd.de (Joerg van den Hoff)
Date: Fri, 9 Mar 2007 13:42:27 +0100
Subject: [R] Deconvolution of a spectrum
In-Reply-To: <20070309122524.GD39652@novum.am.lublin.pl>
References: <20070309122524.GD39652@novum.am.lublin.pl>
Message-ID: <20070309124227.GG12344@marco.fz-rossendorf.de>

On Fri, Mar 09, 2007 at 01:25:24PM +0100, Lukasz Komsta wrote:
> 
> Dear useRs,
> 
> I have a curve which is a mixture of Gaussian curves (for example UV
> emission or absorption spectrum). Do you have any suggestions how to
> implement searching for optimal set of Gaussian peaks to fit the curve?
> I know that it is very complex problem, but maybe it is a possibility
> to do it? First supposement is to use a nls() with very large functions,
> and compare AIC value, but it is very difficult to suggest any starting
> points for algotirithm.
> 
> Searching google I have found only a description of commercial software
> for doing such deconvolution (Origin, PeakFit) without any information
> about used algorithms. No ready-to-use function in any language.
> 
> I have tried to use a Mclust workaround for this problem, by generating a
> large dataset for which the spectrum is a histogram and feed it into
> the Mclust. The results seem to be serious, but this is very ugly and
> imprecise method.
> 
> Thanks for any help,
> 
> Luke
> 
I would try `nls'. we have used `nls' for fitting magnetic resonance spectra
consisting of =~ 10 gaussian peaks. this works OK, if the input data are
reasonable (not too noisy, peak amplitudes above noise level, peak distance
not unreasonably smaller than peak width, i.e peak overlap such that peaks are
still more or less identifiable visually). 

of course you must invest effort in getting the start values (automatically or
manually) right. if your data are good, you might get good start values for the
positions (the means of the gaussians) with an approach that was floating around
the r-help list in 11/2005, which I adopted as follows:


peaks <- function (series, span = 3, what = c("max", "min"), do.pad = TRUE, 
                   add.to.plot = FALSE, ...) 
{
    if ((span <- as.integer(span))%%2 != 1) 
        stop("'span' must be odd")
    if (!is.numeric(series)) 
        stop("`peaks' needs numeric input")
    what <- match.arg(what)
    if (is.null(dim(series)) || min(dim(series)) == 1) {
        series <- as.numeric(series)
        x <- seq(along = series)
        y <- series
    }
    else if (nrow(series) == 2) {
        x <- series[1, ]
        y <- series[2, ]
    }
    else if (ncol(series) == 2) {
        x <- series[, 1]
        y <- series[, 2]
    }
    if (span == 1) 
        return(list(x = x, y = y, pos = rep(TRUE, length(y))), 
            span = span, what = what, do.pad = do.pad)
    if (what == "min") 
        z <- embed(-y, span)
    else z <- embed(y, span)
    s <- span%/%2
    s1 <- s + 1
    v <- max.col(z, "first") == s1
    if (do.pad) {
        pad <- rep(FALSE, s)
        v <- c(pad, v, pad)
        idx <- v
    }
    else idx <- c(rep(FALSE, s), v)
    val <- list(x = x[idx], y = y[idx], pos = v, span = span, 
        what = what, do.pad = do.pad)
    if (add.to.plot == TRUE) 
        points(val, ...)
    val
}

this looks for local maxima in the vector ("y-values") or 2-dim array
("x/y-matrix") `series'in a neighborhood of each point defined by `span'. 
if you first plot your data and then call the above on the data with
'add.to.plot = TRUE', the results of the peak search are added to your plot (and
you can modify this plotting via the `...' argument).

maybe this works for your data to get the peak position estimates (and the
amplitudes in the next step) right. frequently the standard deviations
estimates can be set to some fixed value for any given experiment.

and of course distant parts of your spectrum won't have anything to do which
each other, so you can split up the fitting to help `nls' along a bit.

joerg


From P.Dalgaard at biostat.ku.dk  Fri Mar  9 13:43:55 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 09 Mar 2007 13:43:55 +0100
Subject: [R] Is the gmodels package being maintained?
In-Reply-To: <CBA9D983-87CC-42B9-89D0-92C46D8B16FF@virginia.edu>
References: <CBA9D983-87CC-42B9-89D0-92C46D8B16FF@virginia.edu>
Message-ID: <45F1568B.6090302@biostat.ku.dk>

Michael Kubovy wrote:
> Dear r-helpers,
>
> I sent  a cc of a recent message about a problem with ci.lmer() in  
> the gmodels package to the author (Gregory R Warnes), and the message  
> bounced. If the author or someone else is maintaining this package or  
> this function, would you kindly supplement the author's name and/or  
> address with a current maintainer and/or provide a current email  
> address?
>   
Haven't heard that Greg should be out of circulation. You might try the
address from his homepage:

Gregory_Warnes at urmc.rochester.edu


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From gavin.simpson at ucl.ac.uk  Fri Mar  9 13:52:35 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 09 Mar 2007 12:52:35 +0000
Subject: [R] dendrogram again
In-Reply-To: <A5077BE5-CB55-4026-BC8B-E201BA5D5DD5@lautloscrew.com>
References: <A5077BE5-CB55-4026-BC8B-E201BA5D5DD5@lautloscrew.com>
Message-ID: <1173444755.27004.18.camel@gsimpson.geog.ucl.ac.uk>

On Fri, 2007-03-09 at 12:17 +0100, bunny , lautloscrew.com wrote:
> Hi all,
> 
> ok, i know i can cut a dendrogram, which i did.
> all i get is three objects that a dendrograms itself.
> 
> for example:
> myd$upper, myd$lower[[1]], myd$lower[[2]]
> and so on. of course i can plot them seperately now.
> 
> but the lower parts still have hundreds of branches. i?ll need a 30 "  
> widescreen to watch the whole picture.
> what i?d like to is group the lower branches , so that i get a  
> dendrogram with a few branches, splitting only in the upper levels.  
> In terms of the cluster analysis, i just want to have a few bigger  
> clusters.
> 
> thx,
> 
> m.
> 
> P.S.:
> putting parts of a cutted dendrogram back into to one could be an  
> idea ? is it somehow possible ?

Again, perhaps I'm missing something, but if I understand you correctly
(again no example I can follow - what is myd and how did you create
it?), you only want to plot the upper part of the dendrogram and not the
lower branches. If so, then this /is/ on ?dendrogram and you /do/ use
cut() to do it ...:

'cut.dendrogram()' returns a list with components '$upper' and
     '$lower', the first is a truncated version of the original tree,
     also of class 'dendrogram', the latter a list with the branches
     obtained from cutting the tree, each a 'dendrogram'.

So to only show the pruned tree, you just plot $upper - it does say that
$upper is a dendrogram and that it is the truncated version of the
original tree - which is what I understand you to be asking for. This
example shows it in action - this is what I mean by a reproducible
example - (I'm using package vegan as I am familiar with this data set):

require(vegan) ## if false install it
data(varespec)

hc <- hclust(vegdist(varespec, "bray"), method = "ward")
hc <- as.dendrogram(hc)

## this is the full dendrogram - too many nodes, so prune
plot(hc)

## lets take four clusters and prune it back
hc.pruned <- cut(hc, h = 1) # can't specify k so read height of first
                            # plot - cutting at h = 1 gives 4 clusters

# plot only the upper part of the tree showing only the 4 clusters
plot(hc.pruned$upper, center = TRUE)

Is this what you want? If not, using the example I provide above, tell
us exactly what you want to achieve.

HTH

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From Delphine.Fontaine at adm.unige.ch  Fri Mar  9 14:29:09 2007
From: Delphine.Fontaine at adm.unige.ch (Delphine Fontaine)
Date: Fri, 09 Mar 2007 14:29:09 +0100
Subject: [R] R and clinical studies
Message-ID: <20070309142909.hgceqlqqasssggo0@webmail.adm.unige.ch>

Does anyone know if for clinical studies the FDA would accept  
statistical analyses performed with R ?

Delphine Fontaine


From kubovy at virginia.edu  Fri Mar  9 14:47:01 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Fri, 9 Mar 2007 08:47:01 -0500
Subject: [R] Is the gmodels package being maintained?
In-Reply-To: <45F1568B.6090302@biostat.ku.dk>
References: <CBA9D983-87CC-42B9-89D0-92C46D8B16FF@virginia.edu>
	<45F1568B.6090302@biostat.ku.dk>
Message-ID: <16303BE7-B6AE-4DF2-ADE8-9E8B3AFAB443@virginia.edu>

Hi,

Finding his email address was not immediate, but I finally did, and  
did bring the problem to Greg's attention @ rochester, and the  
message didn't bounce this time.

On Mar 9, 2007, at 7:43 AM, Peter Dalgaard wrote:

> Michael Kubovy wrote:
>> Dear r-helpers,
>>
>> I sent  a cc of a recent message about a problem with ci.lmer() in
>> the gmodels package to the author (Gregory R Warnes), and the message
>> bounced. If the author or someone else is maintaining this package or
>> this function, would you kindly supplement the author's name and/or
>> address with a current maintainer and/or provide a current email
>> address?
>>
> Haven't heard that Greg should be out of circulation. You might try  
> the
> address from his homepage:
>
> Gregory_Warnes at urmc.rochester.edu
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From jmacdon at med.umich.edu  Fri Mar  9 14:45:53 2007
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Fri, 09 Mar 2007 08:45:53 -0500
Subject: [R] color key of heatmap.2
In-Reply-To: <373407824.21574@capitalbio.com>
References: <373407824.21574@capitalbio.com>
Message-ID: <45F16511.2040503@med.umich.edu>

XinMeng wrote:
> Hi all:
> The color key of heatmap.2 is as follows if I use redgreen style:
> low level:red
> high leve:green
> 
> And what I want is:
> low level:green
> hight level:red

?colorpanel

Best,

Jim


> 
> How can I do it then?
> 
> Thanks a lot for your help!
> 
> My best!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623


**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.


From stacey.lee.thompson at gmail.com  Fri Mar  9 15:09:10 2007
From: stacey.lee.thompson at gmail.com (stacey thompson)
Date: Fri, 9 Mar 2007 09:09:10 -0500
Subject: [R] Removing duplicated rows within a matrix,
	with missing data as wildcards
In-Reply-To: <1173431910.45f1266663409@webmail.fhcrc.org>
References: <45F114CE.30555.17E8D9@localhost>
	<1173428020.45f11734a57a3@webmail.fhcrc.org>
	<1173431910.45f1266663409@webmail.fhcrc.org>
Message-ID: <ac7b131e0703090609t25e8f6ecu8d95e35cec3e3a49@mail.gmail.com>

Hi H.,

Your response has improved the clarity of my thinking.  Kind thanks.
Also, your use of seq_len() prompted me to update from R version 2.3.1
on this machine.

For your matrix

 > x <- matrix(c(1, NA, 3, NA, 2, 3), ncol=3, byrow=TRUE)
 > x
      [,1] [,2] [,3]
 [1,]    1   NA    3
 [2,]   NA    2    3

I would want to delete either x[1,] or x[2,] but not both.
Practically, your "removeLooseDupRows(x)"

removeLooseDupRows <- function(x)
{
   if (nrow(x) <= 1)
       return(x)
   ii <- do.call("order",
                 args=lapply(seq_len(ncol(x)),
                             function(col) x[ , col]))
   dup_index <- logical(nrow(x))
   i0 <- -1
   for (k in 1:length(ii)) {
       i <- ii[k]
       if (any(is.na(x[i, ]))) {
           if (i0 == -1)
               next
           if (any(x[i, ] != x[i0, ], na.rm=TRUE))
               next
           dup_index[i] <- TRUE
       } else {
           i0 <- i
       }
   }
   x[!dup_index, ]
}

should leave no such ambiguous cases for my data, as the nrow(x) are
very high with few NA in each x.  For example, a row of (1, 2, 3) is
very likely to exist in my data.

However, to find the row numbers of any remaining ambiguous matches,
should they exist, using example:

> x <- matrix(c(1, NA, 3, NA, 2, 3, 1, 3, 2, 2, 1, 3, 1, NA, 2, 2, 1, 3), ncol=3, byrow=TRUE)
> x
     [,1] [,2] [,3]
[1,]    1   NA    3
[2,]   NA    2    3
[3,]    1    3    2
[4,]    2    1    3
[5,]    1   NA    2
[6,]    2    1    3

after your suggested

> removeLooseDupRows(x)
     [,1] [,2] [,3]
[1,]    1   NA    3
[2,]   NA    2    3
[3,]    1    3    2
[4,]    2    1    3
[5,]    2    1    3

> q <- removeLooseDupRows(unique(x))
> q
     [,1] [,2] [,3]
[1,]    1   NA    3
[2,]   NA    2    3
[3,]    1    3    2
[4,]    2    1    3

I could

> # ambiguous matches in matrix form
> apply(q, 1, function(row1) apply(q, 1, function(row2) all(is.na(row1) | is.na(row2) | row1==row2)))

      [,1]  [,2]  [,3]  [,4]
[1,]  TRUE  TRUE FALSE FALSE
[2,]  TRUE  TRUE FALSE FALSE
[3,] FALSE FALSE  TRUE FALSE
[4,] FALSE FALSE FALSE  TRUE

> # indices of ambiguous matches
> m <- which(apply(q, 1, function(row1) apply(q, 1, function(row2) all(is.na(row1) | is.na(row2) | row1==row2))), arr=T)
> m
     row col
[1,]   1   1
[2,]   2   1
[3,]   1   2
[4,]   2   2
[5,]   3   3
[6,]   4   4

> #put in order and omit duplicates
> m2 <- unique(t(apply(m, 1, sort)))
> m2
     [,1] [,2]
[1,]    1    1
[2,]    1    2
[3,]    2    2
[4,]    3    3
[5,]    4    4

> # show the ambiguous matches
> m2[m2[,1]!=m2[,2], drop=F]
[1] 1 2

...and procede from there.

This solution came from another helpful "R-help" respondant to my
poorly-defined problem.

Appreciative thanks to everyone for your instructive help.

Cheers,
stacey

-- 
-stacey lee thompson-
Stagiaire post-doctorale
Institut de recherche en biologie v?g?tale
Universit? de Montr?al
4101 Sherbrooke Est
Montr?al, Qu?bec H1X 2B2 Canada
stacey.thompson at umontreal.ca


From toby909 at gmail.com  Thu Mar  8 21:45:39 2007
From: toby909 at gmail.com (toby909 at gmail.com)
Date: Thu, 08 Mar 2007 12:45:39 -0800
Subject: [R] autoload libraries at startup
Message-ID: <espsl4$o49$1@sea.gmane.org>

Hi All

I was wondering if there is a way I can specify in R that it should load 
libraries automatically at startup, so that I do not have to manually issue the 
command.

Thanks Toby


From john.seers at bbsrc.ac.uk  Fri Mar  9 15:31:43 2007
From: john.seers at bbsrc.ac.uk (john seers (IFR))
Date: Fri, 9 Mar 2007 14:31:43 -0000
Subject: [R] autoload libraries at startup
In-Reply-To: <espsl4$o49$1@sea.gmane.org>
Message-ID: <AAD49F46EAE3F6479E1D46428FAC31CB0181AA9D@NBIE2KSRV1.nbi.bbsrc.ac.uk>


Hi 

I do not know if this is the best way, but have a look at .Rprofile - a
text file that lives in the R root directory ans is executed at startup.
You could put library() commands in that.

See ?Startup for more information.

Regards


JS



 
---

John Seers
Institute of Food Research
Norwich Research Park
Colney
Norwich
NR4 7UA
 

tel +44 (0)1603 251497
fax +44 (0)1603 507723
e-mail john.seers at bbsrc.ac.uk                         
e-disclaimer at http://www.ifr.ac.uk/edisclaimer/ 
 
Web sites:

www.ifr.ac.uk   
www.foodandhealthnetwork.com


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of toby909 at gmail.com
Sent: 08 March 2007 20:46
To: r-help at stat.math.ethz.ch
Subject: [R] autoload libraries at startup


Hi All

I was wondering if there is a way I can specify in R that it should load

libraries automatically at startup, so that I do not have to manually
issue the 
command.

Thanks Toby

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From paul.bailey at alumni.grinnell.edu  Fri Mar  9 15:34:00 2007
From: paul.bailey at alumni.grinnell.edu (Paul Bailey)
Date: Fri,  9 Mar 2007 08:34:00 -0600 (CST)
Subject: [R] understanding print.summary.lm and perhaps print/show in general
Message-ID: <20070309083400.AKK89504@m4500-03.uchicago.edu>

I'm trying to understand how R prints summary.lm objects and
trying to change it slightly for a summary function that
calculates standard errors using an alternative method.

I've found that I can modify a summary.lm object and then it
prints the modified way but I want to change a few things in
the print method that I think I might just be able to do. One
is that I want the coefficients table to print a different
header (other than "Std. Error"). I've tried changing the
column name of the summary$coef matrix and this works for
calls to printCoefmat but it still prints out "Std. Error"
when I pass  the summary.lm to the command line by itself. I
don't understand this behavior. When I do this (enter an
object on the command line by itself), does it then calls the
print / show method associated with that objects class, in
this case, summary.lm? Below is some sample code to reproduce
the behavior I don't understand and a comment regarding the
result I don't understand.

Cheers,
Paul

#####
lma <- lm(dist ~ speed, data=cars)
suma <- summary(lma)
colnames(suma$coef) <- c(LETTERS[1:4])
printCoefmat(suma$coef) # prints what I expect
suma
# the above is the print behavior question regards,
# why does the coefficients matrix have in its header
# the usual "Estimate Std. Error t value Pr(>|t|)"
# I expect "A B C D" as above in the call to printCoefmat


From peterstencel at yahoo.de  Fri Mar  9 16:02:03 2007
From: peterstencel at yahoo.de (P. Stencel)
Date: Fri, 09 Mar 2007 16:02:03 +0100
Subject: [R] convert pixels into axis coordinates in R
Message-ID: <45F176EB.4010003@yahoo.de>

Dear R users,

I've two questions:

1) Does anybody have a clue how to convert pixel from a jpeg graphic 
(e.g. something like a square of 100x100 pxs)  into axis coordinate 
values in R? 
/
//2)// Is there any possibility to extend the locator function in a way 
that //locator( ) outputs all coordinates from a plot at once, without 
clicking on the graph?

Thanks for any hints.

Regards,
P. Stencel



/


From christos at nuverabio.com  Fri Mar  9 16:09:21 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Fri, 9 Mar 2007 10:09:21 -0500
Subject: [R] understanding print.summary.lm and perhaps print/show in
	general
In-Reply-To: <20070309083400.AKK89504@m4500-03.uchicago.edu>
References: <20070309083400.AKK89504@m4500-03.uchicago.edu>
Message-ID: <001101c7625c$ebe33dc0$0e010a0a@headquarters.silicoinsights>

Paul,

Usually summary methods perform some computations if needed and then change
the class of the original object so that a print method can be called for
the new summary object.

In this case, this is done at the end of the summary.lm method:

...
    if (!is.null(z$na.action)) 
        ans$na.action <- z$na.action
    class(ans) <- "summary.lm"
    ^^^^^^^^^^^^^^^^^^^^^^^^^^
    ans
}

So then print.summary.lm does all the job displaying the summary.lm object.
To see that function do

getAnywhere(print.summary.lm)

Then you can then modify that function as needed.

-Christos

Christos Hatzis, Ph.D.
Nuvera Biosciences, Inc.
400 West Cummings Park
Suite 5350
Woburn, MA 01801
Tel: 781-938-3830
www.nuverabio.com
 


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Bailey
> Sent: Friday, March 09, 2007 9:34 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] understanding print.summary.lm and perhaps 
> print/show in general
> 
> I'm trying to understand how R prints summary.lm objects and 
> trying to change it slightly for a summary function that 
> calculates standard errors using an alternative method.
> 
> I've found that I can modify a summary.lm object and then it 
> prints the modified way but I want to change a few things in 
> the print method that I think I might just be able to do. One 
> is that I want the coefficients table to print a different 
> header (other than "Std. Error"). I've tried changing the 
> column name of the summary$coef matrix and this works for 
> calls to printCoefmat but it still prints out "Std. Error"
> when I pass  the summary.lm to the command line by itself. I 
> don't understand this behavior. When I do this (enter an 
> object on the command line by itself), does it then calls the 
> print / show method associated with that objects class, in 
> this case, summary.lm? Below is some sample code to reproduce 
> the behavior I don't understand and a comment regarding the 
> result I don't understand.
> 
> Cheers,
> Paul
> 
> #####
> lma <- lm(dist ~ speed, data=cars)
> suma <- summary(lma)
> colnames(suma$coef) <- c(LETTERS[1:4])
> printCoefmat(suma$coef) # prints what I expect suma # the 
> above is the print behavior question regards, # why does the 
> coefficients matrix have in its header # the usual "Estimate 
> Std. Error t value Pr(>|t|)"
> # I expect "A B C D" as above in the call to printCoefmat
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From dimitris.rizopoulos at med.kuleuven.be  Fri Mar  9 16:14:29 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 9 Mar 2007 16:14:29 +0100
Subject: [R] Removing duplicated rows within a matrix,
	with missing data as wildcards
References: <45F114CE.30555.17E8D9@localhost><1173428020.45f11734a57a3@webmail.fhcrc.org><1173431910.45f1266663409@webmail.fhcrc.org>
	<ac7b131e0703090609t25e8f6ecu8d95e35cec3e3a49@mail.gmail.com>
Message-ID: <005001c7625d$a22988a0$0540210a@www.domain>

you could also try something like the following:

x <- matrix(c(1, NA, 3, NA, 2, 3, 1, 3, 2, 2, 1, 3, 1, NA, 2, 2, 1, 
3), ncol=3, byrow=TRUE)

wildcardVals <- 1:3 # possible wildcard values
ind <- complete.cases(x)
nc <- ncol(x)
nr <- nrow(x[ind, ])
nwld <- length(wildcardVals)
posb <- apply(x[!ind, , drop = FALSE], 1, function(y){
    out <- matrix(y, nwld, nc, by = TRUE)
    out[, is.na(y)] <- wildcardVals
    t(out)
})
posb <- matrix(c(posb), ncol = nc, by = TRUE)
keep.ind <- duplicated(rbind(x[ind, ], posb))
keep.ind[-(1:nr)] <- apply(matrix(keep.ind[-(1:nr)], nc = nwld, by = 
TRUE),
1, function(x) if(any(x)) rep(TRUE, length(x)) else x)
out <- rbind(x[ind, ], matrix(rep(x[!ind, ], each = nwld), nc = nc))
unique(out[!keep.ind, ])


I hope it works ok.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm

----- Original Message ----- 
From: "stacey thompson" <stacey.lee.thompson at gmail.com>
To: <hpages at fhcrc.org>; <r-help at stat.math.ethz.ch>
Cc: <petr.pikal at precheza.cz>
Sent: Friday, March 09, 2007 3:09 PM
Subject: Re: [R] Removing duplicated rows within a matrix,with missing 
data as wildcards


> Hi H.,
>
> Your response has improved the clarity of my thinking.  Kind thanks.
> Also, your use of seq_len() prompted me to update from R version 
> 2.3.1
> on this machine.
>
> For your matrix
>
> > x <- matrix(c(1, NA, 3, NA, 2, 3), ncol=3, byrow=TRUE)
> > x
>      [,1] [,2] [,3]
> [1,]    1   NA    3
> [2,]   NA    2    3
>
> I would want to delete either x[1,] or x[2,] but not both.
> Practically, your "removeLooseDupRows(x)"
>
> removeLooseDupRows <- function(x)
> {
>   if (nrow(x) <= 1)
>       return(x)
>   ii <- do.call("order",
>                 args=lapply(seq_len(ncol(x)),
>                             function(col) x[ , col]))
>   dup_index <- logical(nrow(x))
>   i0 <- -1
>   for (k in 1:length(ii)) {
>       i <- ii[k]
>       if (any(is.na(x[i, ]))) {
>           if (i0 == -1)
>               next
>           if (any(x[i, ] != x[i0, ], na.rm=TRUE))
>               next
>           dup_index[i] <- TRUE
>       } else {
>           i0 <- i
>       }
>   }
>   x[!dup_index, ]
> }
>
> should leave no such ambiguous cases for my data, as the nrow(x) are
> very high with few NA in each x.  For example, a row of (1, 2, 3) is
> very likely to exist in my data.
>
> However, to find the row numbers of any remaining ambiguous matches,
> should they exist, using example:
>
>> x <- matrix(c(1, NA, 3, NA, 2, 3, 1, 3, 2, 2, 1, 3, 1, NA, 2, 2, 1, 
>> 3), ncol=3, byrow=TRUE)
>> x
>     [,1] [,2] [,3]
> [1,]    1   NA    3
> [2,]   NA    2    3
> [3,]    1    3    2
> [4,]    2    1    3
> [5,]    1   NA    2
> [6,]    2    1    3
>
> after your suggested
>
>> removeLooseDupRows(x)
>     [,1] [,2] [,3]
> [1,]    1   NA    3
> [2,]   NA    2    3
> [3,]    1    3    2
> [4,]    2    1    3
> [5,]    2    1    3
>
>> q <- removeLooseDupRows(unique(x))
>> q
>     [,1] [,2] [,3]
> [1,]    1   NA    3
> [2,]   NA    2    3
> [3,]    1    3    2
> [4,]    2    1    3
>
> I could
>
>> # ambiguous matches in matrix form
>> apply(q, 1, function(row1) apply(q, 1, function(row2) 
>> all(is.na(row1) | is.na(row2) | row1==row2)))
>
>      [,1]  [,2]  [,3]  [,4]
> [1,]  TRUE  TRUE FALSE FALSE
> [2,]  TRUE  TRUE FALSE FALSE
> [3,] FALSE FALSE  TRUE FALSE
> [4,] FALSE FALSE FALSE  TRUE
>
>> # indices of ambiguous matches
>> m <- which(apply(q, 1, function(row1) apply(q, 1, function(row2) 
>> all(is.na(row1) | is.na(row2) | row1==row2))), arr=T)
>> m
>     row col
> [1,]   1   1
> [2,]   2   1
> [3,]   1   2
> [4,]   2   2
> [5,]   3   3
> [6,]   4   4
>
>> #put in order and omit duplicates
>> m2 <- unique(t(apply(m, 1, sort)))
>> m2
>     [,1] [,2]
> [1,]    1    1
> [2,]    1    2
> [3,]    2    2
> [4,]    3    3
> [5,]    4    4
>
>> # show the ambiguous matches
>> m2[m2[,1]!=m2[,2], drop=F]
> [1] 1 2
>
> ...and procede from there.
>
> This solution came from another helpful "R-help" respondant to my
> poorly-defined problem.
>
> Appreciative thanks to everyone for your instructive help.
>
> Cheers,
> stacey
>
> -- 
> -stacey lee thompson-
> Stagiaire post-doctorale
> Institut de recherche en biologie v?g?tale
> Universit? de Montr?al
> 4101 Sherbrooke Est
> Montr?al, Qu?bec H1X 2B2 Canada
> stacey.thompson at umontreal.ca
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From isubirana at imim.es  Fri Mar  9 16:02:08 2007
From: isubirana at imim.es (SUBIRANA CACHINERO, ISAAC)
Date: Fri, 9 Mar 2007 16:02:08 +0100
Subject: [R] Right truncation data
Message-ID: <6B454983050EDB48B5E2BFCDD14F30B4093003@basquet.imim.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070309/6ad42f82/attachment.pl 

From scionforbai at gmail.com  Fri Mar  9 16:39:25 2007
From: scionforbai at gmail.com (Scionforbai)
Date: Fri, 9 Mar 2007 16:39:25 +0100
Subject: [R] color key of heatmap.2
In-Reply-To: <373407824.21574@capitalbio.com>
References: <373407824.21574@capitalbio.com>
Message-ID: <e9ee1f0a0703090739h523424b3kfacb58b1bd3889f3@mail.gmail.com>

mycolors <- rev(heatmap.2(length))

where length is the number of colours you wants.


From johannes_graumann at web.de  Fri Mar  9 16:30:05 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Fri, 09 Mar 2007 16:30:05 +0100
Subject: [R] Matrix conversion question
Message-ID: <esruht$5go$1@sea.gmane.org>

Hello,

Please help - I'm blanking on this ...

I have a matrix like this:

     [,1] [,2]
[1,]    1    2
[2,]    1    3
[3,]    2    3

and would like to have a list of vectors, where a vector contains the
entries in a matrix row ...

Can somebody nudge me to the place I need to go?

Thanks, Joh


From efg at stowers-institute.org  Fri Mar  9 16:32:58 2007
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Fri, 9 Mar 2007 09:32:58 -0600
Subject: [R] Deconvolution of a spectrum
References: <20070309122524.GD39652@novum.am.lublin.pl>
Message-ID: <esrunf$9lg$1@sea.gmane.org>

"Lukasz Komsta" <luke at novum.am.lublin.pl> wrote in message 
news:20070309122524.GD39652 at novum.am.lublin.pl...
> I have a curve which is a mixture of Gaussian curves (for example UV
> emission or absorption spectrum). Do you have any suggestions how to
> implement searching for optimal set of Gaussian peaks to fit the curve?
> I know that it is very complex problem, but maybe it is a possibility
> to do it? First supposement is to use a nls() with very large functions,
> and compare AIC value, but it is very difficult to suggest any starting
> points for algotirithm.

Perhaps these notes will be helpful if you don't have too much noise in your 
data:
http://research.stowers-institute.org/efg/R/Statistics/MixturesOfDistributions/index.htm

efg

Earl F. Glynn
Stowers Institute for Medical Research


From Mat.Soukup at fda.hhs.gov  Fri Mar  9 16:55:34 2007
From: Mat.Soukup at fda.hhs.gov (Soukup, Mat)
Date: Fri, 9 Mar 2007 10:55:34 -0500
Subject: [R] R and clinical studies
In-Reply-To: <20070309142909.hgceqlqqasssggo0@webmail.adm.unige.ch>
References: <20070309142909.hgceqlqqasssggo0@webmail.adm.unige.ch>
Message-ID: <27CA3827C6B33E40874682C469E774DD04DB9136@FMD3CT001.fda.gov>

Delphine,

Please see the following message posted a week ago:
http://comments.gmane.org/gmane.comp.lang.r.general/80175.

HTH,

-Mat 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Delphine Fontaine
Sent: Friday, March 09, 2007 8:29 AM
To: r-help at stat.math.ethz.ch
Subject: [R] R and clinical studies

Does anyone know if for clinical studies the FDA would accept  
statistical analyses performed with R ?

Delphine Fontaine

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From christos at nuverabio.com  Fri Mar  9 17:01:10 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Fri, 9 Mar 2007 11:01:10 -0500
Subject: [R] Matrix conversion question
In-Reply-To: <esruht$5go$1@sea.gmane.org>
References: <esruht$5go$1@sea.gmane.org>
Message-ID: <002b01c76264$2853f130$0e010a0a@headquarters.silicoinsights>

Try

split(x, row(x)) 

-Christos

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Johannes Graumann
> Sent: Friday, March 09, 2007 10:30 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Matrix conversion question
> 
> Hello,
> 
> Please help - I'm blanking on this ...
> 
> I have a matrix like this:
> 
>      [,1] [,2]
> [1,]    1    2
> [2,]    1    3
> [3,]    2    3
> 
> and would like to have a list of vectors, where a vector 
> contains the entries in a matrix row ...
> 
> Can somebody nudge me to the place I need to go?
> 
> Thanks, Joh
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From youngjin.michael at gmail.com  Fri Mar  9 17:08:12 2007
From: youngjin.michael at gmail.com (Young-Jin Lee)
Date: Fri, 9 Mar 2007 11:08:12 -0500
Subject: [R] How to create a list that grows automatically
Message-ID: <9a2a73210703090808x2fd9ce77i4537d3428045c061@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070309/a7bdac3d/attachment.pl 

From klaster at karlin.mff.cuni.cz  Fri Mar  9 17:08:59 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Fri, 09 Mar 2007 17:08:59 +0100
Subject: [R] understanding print.summary.lm and perhaps print/show in
 general
In-Reply-To: <001101c7625c$ebe33dc0$0e010a0a@headquarters.silicoinsights>
References: <20070309083400.AKK89504@m4500-03.uchicago.edu>
	<001101c7625c$ebe33dc0$0e010a0a@headquarters.silicoinsights>
Message-ID: <45F1869B.9060803@karlin.mff.cuni.cz>

Another solution is to look into the code of summary.lm a few lines 
above where the (dim)names are assigned. Based on this, you may try

lma <- lm(dist ~ speed, data=cars)
suma <- summary(lma)
colnames(suma$coef) <- c(LETTERS[1:4])
printCoefmat(suma$coef) # prints what I expect
suma

dimnames(suma$coefficients) <- list(names(suma$coefficients), 
c(LETTERS[1:4]))
suma

You might also find reading the chapter on generic functions in the 
R-lang (R language definition) manual useful.
Petr

Christos Hatzis napsal(a):
> Paul,
> 
> Usually summary methods perform some computations if needed and then change
> the class of the original object so that a print method can be called for
> the new summary object.
> 
> In this case, this is done at the end of the summary.lm method:
> 
> ...
>     if (!is.null(z$na.action)) 
>         ans$na.action <- z$na.action
>     class(ans) <- "summary.lm"
>     ^^^^^^^^^^^^^^^^^^^^^^^^^^
>     ans
> }
> 
> So then print.summary.lm does all the job displaying the summary.lm object.
> To see that function do
> 
> getAnywhere(print.summary.lm)
> 
> Then you can then modify that function as needed.
> 
> -Christos
> 
> Christos Hatzis, Ph.D.
> Nuvera Biosciences, Inc.
> 400 West Cummings Park
> Suite 5350
> Woburn, MA 01801
> Tel: 781-938-3830
> www.nuverabio.com
>  
> 
> 
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Bailey
>> Sent: Friday, March 09, 2007 9:34 AM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] understanding print.summary.lm and perhaps 
>> print/show in general
>>
>> I'm trying to understand how R prints summary.lm objects and 
>> trying to change it slightly for a summary function that 
>> calculates standard errors using an alternative method.
>>
>> I've found that I can modify a summary.lm object and then it 
>> prints the modified way but I want to change a few things in 
>> the print method that I think I might just be able to do. One 
>> is that I want the coefficients table to print a different 
>> header (other than "Std. Error"). I've tried changing the 
>> column name of the summary$coef matrix and this works for 
>> calls to printCoefmat but it still prints out "Std. Error"
>> when I pass  the summary.lm to the command line by itself. I 
>> don't understand this behavior. When I do this (enter an 
>> object on the command line by itself), does it then calls the 
>> print / show method associated with that objects class, in 
>> this case, summary.lm? Below is some sample code to reproduce 
>> the behavior I don't understand and a comment regarding the 
>> result I don't understand.
>>
>> Cheers,
>> Paul
>>
>> #####
>> lma <- lm(dist ~ speed, data=cars)
>> suma <- summary(lma)
>> colnames(suma$coef) <- c(LETTERS[1:4])
>> printCoefmat(suma$coef) # prints what I expect suma # the 
>> above is the print behavior question regards, # why does the 
>> coefficients matrix have in its header # the usual "Estimate 
>> Std. Error t value Pr(>|t|)"
>> # I expect "A B C D" as above in the call to printCoefmat
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From cressonim at nhlbi.nih.gov  Fri Mar  9 17:18:46 2007
From: cressonim at nhlbi.nih.gov (Cressoni, Massimo (NIH/NHLBI) [F])
Date: Fri, 9 Mar 2007 11:18:46 -0500
Subject: [R] Extracting the p of F statistics from lm
Message-ID: <B0F504209244B14EA9A4C1DFB599B9224FFE35@NIHCESMLBX6.nih.gov>

I need to extract the p value from a ANOVA done with lm model

fitting <- lm(var ~ group)
Sfitting <- summary(fitting)

Sfitting[10][1] gives the F value and the degrees of freedom but I am not able to get the
p value. 
The function df should give a p value given a F but I am not 
able to make it work.

I found only something about aov in the R help and I am not able
to make it work

Massimo Cressoni


From topkatz at msn.com  Fri Mar  9 17:20:42 2007
From: topkatz at msn.com (Talbot Katz)
Date: Fri, 09 Mar 2007 11:20:42 -0500
Subject: [R] lpSolve space problem in R 2.4.1 on Windows XP
Message-ID: <BAY132-F1006FC07E3DFEA0643FB37AA780@phx.gbl>

Hi.

I am trying to use the linear optimizer from package lpSolve in R 2.4.1 on 
Windows XP (Version 5.1).  The problem I am trying to solve has 2843 
variables (2841 integer, 2 continuous) and 8524 constraints, and I have 2 Gb 
of memory.  After I load the input data into R, I have at most 1.5 Gb of 
memory available.  If I start the lp with significantly less memory 
available (say 1 Gb), I get an error message from R:

"Error: cannot allocate vector of size 189459 Kb"

If I close all my other windows and try to maximize the available memory to 
the full 1.5 Gb, I can watch the memory get filled up until only about 400 
Mb is left, at which point I get a Windows error message:

"R for Windows GUI front-end has encountered a problem and needs to close.  
We are sorry for the inconvenience."

This behavior persists even when I relax the integer constraints, and 
eliminate the 2841 constraints that restrict the integer variables to values 
<= 1, so I'm just running a standard lp with 2843 variables and 5683 
constraints.

I have been able to get the full MIP formulation to work correctly on some 
very small problems (~10 variables and 25 constraints).

Here is the code for a working example:

>library(lpSolve)
>(v1=rev(1:8))
[1] 8 7 6 5 4 3 2 1
>(csv1=cumsum(as.numeric(v1)))
[1]  8 15 21 26 30 33 35 36
>(lencsv1=length(csv1))
[1] 8
>(Nm1=lencsv1-1)
[1] 7
>(Np1=lencsv1+1)
[1] 9
>ngp=3
>f.obj=c(1,1,rep(0,Nm1))
>f.int=3:Np1
>bin.con=cbind(rep(0,Nm1),rep(0,Nm1),diag(Nm1))
>bin.dir=rep("<=",Nm1)
>bin.rhs=rep(1,Nm1)
>gp.con=c(0,0,rep(1,Nm1))
>gp.dir="<="
>(gp.rhs=ngp-1)
[1] 2
>ub.con=cbind(rep(-1,rep(Nm1)),rep(0,Nm1),!upper.tri(matrix(nrow=Nm1,ncol=Nm1)))
>ub.dir=rep("<=",Nm1)
>(ub.rhs=csv1[1:Nm1]*ngp/csv1[lencsv1])
[1] 0.6666667 1.2500000 1.7500000 2.1666667 2.5000000 2.7500000 2.9166667
>lb.con=cbind(rep(0,Nm1),rep(1,rep(Nm1)),!upper.tri(matrix(nrow=Nm1,ncol=Nm1)))
>lb.dir=rep(">=",Nm1)
>lb.rhs=ub.rhs
>f.con=rbind(bin.con,gp.con,ub.con,lb.con)
>f.dir=c(bin.dir,gp.dir,ub.dir,lb.dir)
>f.rhs=c(bin.rhs,gp.rhs,ub.rhs,lb.rhs)
>lglp=lp("min",f.obj,f.con,f.dir,f.rhs,int.vec=f.int)
>lglp$objval
[1] 0.9166667
>lglp$solution
[1] 0.0000000 0.9166667 0.0000000 1.0000000 0.0000000 1.0000000 0.0000000
[8] 0.0000000 0.0000000
>

What this is doing is taking the points of v1 and dividing them into 
contiguous groups (the variable ngp is the number of groups) such that the 
sums of the v1 values are as close as possible to equal within the three 
groups.  So, for v1 = c(8,7,6,5,4,3,2,1), the groups c(8,7), c(6,5), 
c(4,3,2,1), with sums 15,11,10 is the best such split, and the solution 
vector shows that the splitting occurs after the second and fourth elements.


Anyway, I am wondering...  Are 3000 variables and 8500 constraints usually 
too much for lpSolve to handle in 1.5 Gb of memory?  Is there a possible bug 
(in R or in Windows) that leads to the Windows error when the memory falls 
below 400 Mb?  Is there a problem with my formulation that makes it unstable 
even after the integer constraints are removed?

Thanks!


--  TMK  --
212-460-5430	home
917-656-5351	cell


From weller at erdw.ethz.ch  Fri Mar  9 17:33:31 2007
From: weller at erdw.ethz.ch (Andy Weller)
Date: Fri, 09 Mar 2007 17:33:31 +0100
Subject: [R] R GUI in Ubuntu?
In-Reply-To: <20070308233255.GA12995@eddelbuettel.com>
References: <45F0505B.9040702@erdw.ethz.ch>
	<20070308233255.GA12995@eddelbuettel.com>
Message-ID: <45F18C5B.4040006@erdw.ethz.ch>

OK, so I did:
sudo R CMD javareconf

followed by the following in R as root:
install.packages("JGR",dep = TRUE)

which I think went OK because if I do:
library()

then JGR is listed. From the terminal:
"Packages in library '/usr/local/lib/R/site-library':

JavaGD                  Java Graphics Device
JGR                     JGR - Java Gui for R
rJava                   Low-level R to Java interface"

BUT, if I then run:
JGR()

then I get:
Error: could not find function "JGR"

I am confused...?!?

Thanks in advance, Andy

Dirk Eddelbuettel wrote:
> On Thu, Mar 08, 2007 at 07:05:15PM +0100, Andy Weller wrote:
>> Dear all,
>>
>> I am very new to R and find the terminal-based UI a little daunting. 
>> (That's probably the wrong thing to say!) Having searched the Packages 
>> it seems that I can have either a Gnome-based or Java-based GUI for my 
>> Ubuntu machine. However, I can get neither to work.
>>
>> Having run R as root, I then run the following command:
>> install.packages("gnomeGUI", dependencies=TRUE)
>>
>> The output of which is:
>> checking for gnomeConf.sh file in /usr/local/lib... not found
>> configure: error: conditional "HAVE_ORBIT" was never defined.
>> Usually this means the macro was only invoked conditionally.
>> ERROR: configuration failed for package 'gnomeGUI'
>> * Removing '/usr/local/lib/R/site-library/gnomeGUI'
>>
>> I have checked to see if I have all dependencies installed - it seems as 
>> though I have. No luck! So I try the Java-based GUI with:
>> install.packages("JGR",dep=TRUE)
>> library(JGR)
>> JGR()
>>
>> No luck. So, out of R I try:
>> sudo R CMD javareconf
> 
> I think you are close. Do the JGR install _after_ the javareconf as it
> needs the correct values.
> 
> Also make sure you use the Sun Java packages you get for Ubuntu.
> 
> Hope this helps, Dirk
> 
>> Then in R, if I check the library with:
>> library(JGR)
>>
>> I get:
>> Error: .onLoad failed in 'loadNamespace' for 'rJava'
>> Error: package 'rJava' could not be loaded
>>
>> HMMmmm - still no joy! I guess I am missing something very basic here?!
>>
>> Thanks in advance, Andy


From johannes_graumann at web.de  Fri Mar  9 17:32:51 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Fri, 09 Mar 2007 17:32:51 +0100
Subject: [R] Matrix conversion question
References: <esruht$5go$1@sea.gmane.org>
	<002b01c76264$2853f130$0e010a0a@headquarters.silicoinsights>
Message-ID: <ess27j$jvs$1@sea.gmane.org>

Christos Hatzis wrote:

> Try
> 
> split(x, row(x))

OOOOHHHHH! THE ELEGANCE! Thanks a lot!

Joh


From GPetris at uark.edu  Fri Mar  9 17:36:11 2007
From: GPetris at uark.edu (Giovanni Petris)
Date: Fri, 9 Mar 2007 10:36:11 -0600 (CST)
Subject: [R] Extracting the p of F statistics from lm
In-Reply-To: <B0F504209244B14EA9A4C1DFB599B9224FFE35@NIHCESMLBX6.nih.gov>
	(cressonim@nhlbi.nih.gov)
References: <B0F504209244B14EA9A4C1DFB599B9224FFE35@NIHCESMLBX6.nih.gov>
Message-ID: <200703091636.l29GaBD6013292@definetti.ddns.uark.edu>


> Date: Fri, 09 Mar 2007 11:18:46 -0500
> From: "Cressoni, Massimo (NIH/NHLBI) [F]" <cressonim at nhlbi.nih.gov>
> Sender: r-help-bounces at stat.math.ethz.ch
> Precedence: list
> Thread-topic: Extracting the p of F statistics from lm
> Thread-index: AcdiZp1fE6s6LWieSsaL2EpoQP/shg==
> 
> I need to extract the p value from a ANOVA done with lm model
> 
> fitting <- lm(var ~ group)
> Sfitting <- summary(fitting)
> 
> Sfitting[10][1] gives the F value and the degrees of freedom but I am not able to get the
> p value. 
> The function df should give a p value given a F but I am not 
> able to make it work.

The function pf should.

> 
> I found only something about aov in the R help and I am not able
> to make it work
> 
> Massimo Cressoni
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 

Giovanni Petris  <GPetris at uark.edu>
Department of Mathematical Sciences
University of Arkansas - Fayetteville, AR 72701
Ph: (479) 575-6324, 575-8630 (fax)
http://definetti.uark.edu/~gpetris/


From HDoran at air.org  Fri Mar  9 17:41:47 2007
From: HDoran at air.org (Doran, Harold)
Date: Fri, 9 Mar 2007 11:41:47 -0500
Subject: [R] time demean model matrix
Message-ID: <2323A6D37908A847A7C32F1E3662C80E04E70C@dc1ex01.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070309/c20512e8/attachment.pl 

From ligges at statistik.uni-dortmund.de  Fri Mar  9 17:51:30 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 09 Mar 2007 17:51:30 +0100
Subject: [R] lpSolve space problem in R 2.4.1 on Windows XP
In-Reply-To: <BAY132-F1006FC07E3DFEA0643FB37AA780@phx.gbl>
References: <BAY132-F1006FC07E3DFEA0643FB37AA780@phx.gbl>
Message-ID: <45F19092.10900@statistik.uni-dortmund.de>

If R is closed that way (i.e. crashes), it is a bug by definition: 
either in R or (more probable) in the package. Can you please contact 
the package maintainer to sort things out.

Thanks,
Uwe Ligges





Talbot Katz wrote:
> Hi.
> 
> I am trying to use the linear optimizer from package lpSolve in R 2.4.1 on 
> Windows XP (Version 5.1).  The problem I am trying to solve has 2843 
> variables (2841 integer, 2 continuous) and 8524 constraints, and I have 2 Gb 
> of memory.  After I load the input data into R, I have at most 1.5 Gb of 
> memory available.  If I start the lp with significantly less memory 
> available (say 1 Gb), I get an error message from R:
> 
> "Error: cannot allocate vector of size 189459 Kb"
> 
> If I close all my other windows and try to maximize the available memory to 
> the full 1.5 Gb, I can watch the memory get filled up until only about 400 
> Mb is left, at which point I get a Windows error message:
> 
> "R for Windows GUI front-end has encountered a problem and needs to close.  
> We are sorry for the inconvenience."
> 
> This behavior persists even when I relax the integer constraints, and 
> eliminate the 2841 constraints that restrict the integer variables to values 
> <= 1, so I'm just running a standard lp with 2843 variables and 5683 
> constraints.
> 
> I have been able to get the full MIP formulation to work correctly on some 
> very small problems (~10 variables and 25 constraints).
> 
> Here is the code for a working example:
> 
>> library(lpSolve)
>> (v1=rev(1:8))
> [1] 8 7 6 5 4 3 2 1
>> (csv1=cumsum(as.numeric(v1)))
> [1]  8 15 21 26 30 33 35 36
>> (lencsv1=length(csv1))
> [1] 8
>> (Nm1=lencsv1-1)
> [1] 7
>> (Np1=lencsv1+1)
> [1] 9
>> ngp=3
>> f.obj=c(1,1,rep(0,Nm1))
>> f.int=3:Np1
>> bin.con=cbind(rep(0,Nm1),rep(0,Nm1),diag(Nm1))
>> bin.dir=rep("<=",Nm1)
>> bin.rhs=rep(1,Nm1)
>> gp.con=c(0,0,rep(1,Nm1))
>> gp.dir="<="
>> (gp.rhs=ngp-1)
> [1] 2
>> ub.con=cbind(rep(-1,rep(Nm1)),rep(0,Nm1),!upper.tri(matrix(nrow=Nm1,ncol=Nm1)))
>> ub.dir=rep("<=",Nm1)
>> (ub.rhs=csv1[1:Nm1]*ngp/csv1[lencsv1])
> [1] 0.6666667 1.2500000 1.7500000 2.1666667 2.5000000 2.7500000 2.9166667
>> lb.con=cbind(rep(0,Nm1),rep(1,rep(Nm1)),!upper.tri(matrix(nrow=Nm1,ncol=Nm1)))
>> lb.dir=rep(">=",Nm1)
>> lb.rhs=ub.rhs
>> f.con=rbind(bin.con,gp.con,ub.con,lb.con)
>> f.dir=c(bin.dir,gp.dir,ub.dir,lb.dir)
>> f.rhs=c(bin.rhs,gp.rhs,ub.rhs,lb.rhs)
>> lglp=lp("min",f.obj,f.con,f.dir,f.rhs,int.vec=f.int)
>> lglp$objval
> [1] 0.9166667
>> lglp$solution
> [1] 0.0000000 0.9166667 0.0000000 1.0000000 0.0000000 1.0000000 0.0000000
> [8] 0.0000000 0.0000000
> 
> What this is doing is taking the points of v1 and dividing them into 
> contiguous groups (the variable ngp is the number of groups) such that the 
> sums of the v1 values are as close as possible to equal within the three 
> groups.  So, for v1 = c(8,7,6,5,4,3,2,1), the groups c(8,7), c(6,5), 
> c(4,3,2,1), with sums 15,11,10 is the best such split, and the solution 
> vector shows that the splitting occurs after the second and fourth elements.
> 
> 
> Anyway, I am wondering...  Are 3000 variables and 8500 constraints usually 
> too much for lpSolve to handle in 1.5 Gb of memory?  Is there a possible bug 
> (in R or in Windows) that leads to the Windows error when the memory falls 
> below 400 Mb?  Is there a problem with my formulation that makes it unstable 
> even after the integer constraints are removed?
> 
> Thanks!
> 
> 
> --  TMK  --
> 212-460-5430	home
> 917-656-5351	cell
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From weller at erdw.ethz.ch  Fri Mar  9 17:55:43 2007
From: weller at erdw.ethz.ch (Andy Weller)
Date: Fri, 09 Mar 2007 17:55:43 +0100
Subject: [R] R GUI in Ubuntu?
In-Reply-To: <45F18C5B.4040006@erdw.ethz.ch>
References: <45F0505B.9040702@erdw.ethz.ch>
	<20070308233255.GA12995@eddelbuettel.com>
	<45F18C5B.4040006@erdw.ethz.ch>
Message-ID: <45F1918F.1030404@erdw.ethz.ch>

I should also add that:
library(JGR)

gives me the following output:
Loading required package: rJava
Error in dyn.load(x, as.logical(local), as.logical(now)) :
         unable to load shared library 
'/usr/local/lib/R/site-library/rJava/libs/rJava.so':
   /usr/local/lib/R/site-library/rJava/libs/rJava.so: undefined symbol: 
JNI_GetCreatedJavaVMs
Error: .onLoad failed in 'loadNamespace' for 'rJava'
Error: package 'rJava' could not be loaded

I have Sun's Java installed and thought rJava installed without problems...

Thanks, Andy

Andy Weller wrote:
> OK, so I did:
> sudo R CMD javareconf
> 
> followed by the following in R as root:
> install.packages("JGR",dep = TRUE)
> 
> which I think went OK because if I do:
> library()
> 
> then JGR is listed. From the terminal:
> "Packages in library '/usr/local/lib/R/site-library':
> 
> JavaGD                  Java Graphics Device
> JGR                     JGR - Java Gui for R
> rJava                   Low-level R to Java interface"
> 
> BUT, if I then run:
> JGR()
> 
> then I get:
> Error: could not find function "JGR"
> 
> I am confused...?!?
> 
> Thanks in advance, Andy
> 
> Dirk Eddelbuettel wrote:
>> On Thu, Mar 08, 2007 at 07:05:15PM +0100, Andy Weller wrote:
>>> Dear all,
>>>
>>> I am very new to R and find the terminal-based UI a little daunting. 
>>> (That's probably the wrong thing to say!) Having searched the 
>>> Packages it seems that I can have either a Gnome-based or Java-based 
>>> GUI for my Ubuntu machine. However, I can get neither to work.
>>>
>>> Having run R as root, I then run the following command:
>>> install.packages("gnomeGUI", dependencies=TRUE)
>>>
>>> The output of which is:
>>> checking for gnomeConf.sh file in /usr/local/lib... not found
>>> configure: error: conditional "HAVE_ORBIT" was never defined.
>>> Usually this means the macro was only invoked conditionally.
>>> ERROR: configuration failed for package 'gnomeGUI'
>>> * Removing '/usr/local/lib/R/site-library/gnomeGUI'
>>>
>>> I have checked to see if I have all dependencies installed - it seems 
>>> as though I have. No luck! So I try the Java-based GUI with:
>>> install.packages("JGR",dep=TRUE)
>>> library(JGR)
>>> JGR()
>>>
>>> No luck. So, out of R I try:
>>> sudo R CMD javareconf
>>
>> I think you are close. Do the JGR install _after_ the javareconf as it
>> needs the correct values.
>>
>> Also make sure you use the Sun Java packages you get for Ubuntu.
>>
>> Hope this helps, Dirk
>>
>>> Then in R, if I check the library with:
>>> library(JGR)
>>>
>>> I get:
>>> Error: .onLoad failed in 'loadNamespace' for 'rJava'
>>> Error: package 'rJava' could not be loaded
>>>
>>> HMMmmm - still no joy! I guess I am missing something very basic here?!
>>>
>>> Thanks in advance, Andy


From albmont at centroin.com.br  Fri Mar  9 17:56:56 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 9 Mar 2007 14:56:56 -0200
Subject: [R] How to create a list that grows automatically
In-Reply-To: <9a2a73210703090808x2fd9ce77i4537d3428045c061@mail.gmail.com>
References: <9a2a73210703090808x2fd9ce77i4537d3428045c061@mail.gmail.com>
Message-ID: <20070309165305.M596@centroin.com.br>

Young-Jin Lee asked:
> 
> I would like to know if there is a way to create a list or an array (or
> anything) which grows automatically as more elements are put into 
> it. 
>
???

I think this is the default behaviour of R arrays:

x <- vector(length=0)  # create a vector of zero length
x[1] <- 2
x[10] <- 3
x[length(x) + 1] <- 4
x # 2 NA NA ... 3 4


> What I want to find is something equivalent to an ArrayList 
> object of Java language. In Java, I can do the following thing:
> 
> // Java code
> ArrayList myArray = new ArrayList();
> myArray.add("object1");
> myArray.add("object2");
> ....
> // End of java code
> 
myArray <- vector(length=0)
myArray <- c(myArray, "object1")
myArray <- c(myArray, "object2")
myArray # array with 2 strings

Alberto Monteiro


From maechler at stat.math.ethz.ch  Fri Mar  9 18:14:44 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 9 Mar 2007 18:14:44 +0100
Subject: [R] Off topic:Spam on R-help increase?
In-Reply-To: <40e66e0b0703060957q2a235d35l58764e32b3aed770@mail.gmail.com>
References: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
	<40e66e0b0703060957q2a235d35l58764e32b3aed770@mail.gmail.com>
Message-ID: <17905.38404.443309.762848@stat.math.ethz.ch>

>>>>> "DB" == Douglas Bates <bates at stat.wisc.edu>
>>>>>     on Tue, 6 Mar 2007 11:57:28 -0600 writes:

    DB> On 3/6/07, Bert Gunter <gunter.berton at gene.com> wrote:
    >> In the past 2 days I have seen a large increase of  spam getting into
    >> R-help. Are others experiencing this problem? If so, has there been some
    >> change to the spam filters on the R-servers? If not, is the problem on my
    >> end?

    DB> There has indeed been an increase in the amount of spam making it
    DB> through to the list.  We apologize for the inconvenience.  Regretably
    DB> we will not be able to do much about it until the beginning of next
    DB> week.

    DB> Martin Maechler is on vacation at present and I am administering the
    DB> lists until he returns.  Most of the time this works even though the
    DB> mail servers are in Zurich Switzerland and I am in Madison, WI, USA.
    DB> However, in the last two days we have had a surge in spam and quite a
    DB> bit of it is getting through the filters.

    DB> The filters are catching some of the spam.  I think the main
    DB> difference in the last two days has been that the level of spam to the
    DB> lists has increased but it could be that something has happened to the
    DB> filters too.

I've been back today, well relaxed and tanned from the nice
vacation; thanks to all of you for taking such an interest in it:-) ;-) 

With a work back-log of almost 4 weeks, I hadn't dared to look
into my R-lists inbox of > 2400 messages until about an hour ago.

"Fortunately" it's not the spammers that would have become
smarter (well they have or their hired geeks, but already a few
months ago, not just now).
The main problem has ``just'' been disk-server, then network and
file mount problems on the mail server that were unfortunately
not seen at first by our IT staff. 
As a consequence, there had also been enormous (> 24 hours)
delays in mail delivery, maybe less visible on the mailing list
side of it.
As far as I can see/guess now, the spam problem should have
lasted only about one to two days --- too long of course for
you, but at least not till I had returned to work.

Yes indeed, we are sorry for this, but no, we cannot promise it
won't happen again :-\

Martin

    DB> All the lists except R-help only allow postings from subscribers so
    DB> there should very little spam on the other lists.

    DB> This subscriber-only policy can be difficult for people like me who
    DB> receive email at one address but send it from another.  Either the
    DB> sender must remember to use the account that is registered for the
    DB> list or the list administrator must manually approve the posting.
    DB> Even worse, such a policy dissuades new useRs from posting because
    DB> they get a response that their message has been held pending manual
    DB> approval by the administrator.  Sometimes they react by reposting the
    DB> message, then re-reposting, then ...

    DB> We have avoided instituting such a policy on R-help because of the
    DB> level of administrative work that will be involved and our desire not
    DB> to dissuade new useRs from posting to the list.

    DB> However, if this keeps up we may need to reconsider.

    DB> I would ask for the list subscribers to bear with us until Martin
    DB> returns and can check on whether something has gone wrong with the
    DB> filters.

    DB> ______________________________________________
    DB> R-help at stat.math.ethz.ch mailing list
    DB> https://stat.ethz.ch/mailman/listinfo/r-help
    DB> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    DB> and provide commented, minimal, self-contained, reproducible code.


From bruno.c at inwind.it  Fri Mar  9 18:17:04 2007
From: bruno.c at inwind.it (Bruno C.)
Date: Fri,  9 Mar 2007 18:17:04 +0100
Subject: [R] Duplicate rows of matrix
Message-ID: <JENC0G$5B8DD937F0AB7290952F942525F521D5@libero.it>

Hello my problem is the following:

I have a matrix A and a vector B which contains as many rows as A.

I need to build a matrix C which contains B[i]-times the row A[i,] and this for each line of A.

if for example A is

    [1]    [2]
[1]  8      9.4
[2]  4.2    1.1

and B is (3,1). Then C will be:
    [1]    [2]
[1]  8      9.4
[2]  8      9.4
[3]  8      9.4
[4]  4.2    1.1


I have some working code which go through all the lines of A and for each line does a rbind(C, A[i,]) B[i]-times 
But this is quite time consuming given that each rbind rebuild a new matrix ... is there any faster way?
I can think of some minor improvements like building a matrix C of zeros, containing as many columns as A and as many columns as the sum of elements of B ... and the filing it.

But I was more looking for some already implemented function/package, is there any?

Thanx



------------------------------------------------------
Leggi GRATIS le tue mail con il telefonino i-mode? di Wind
http://i-mode.wind.it


From clandry at fas.harvard.edu  Fri Mar  9 18:26:52 2007
From: clandry at fas.harvard.edu (Christian Landry)
Date: Fri, 9 Mar 2007 12:26:52 -0500
Subject: [R] GLM: order of terms in model
Message-ID: <BED32C46-58B1-4213-A5A4-60008E29C4AC@fas.harvard.edu>

Dear R-helpers,

I have been analysing data using a GLM. My model is as follows:

mod <- glm (V ~ T + as.factor(A) + N, family="gaussian")

and using

anova(mod, test="F")

to get the analysis of deviance table and the fraction of deviance  
explained by each term.

T and A dominate with respect to their Deviance, with T having a  
larger effect than A (about twice)

However, if I reverse T and A in the model, I get that A now explains  
more deviance than T.

My questions are: 1) What is it due to?
				2) Is there any way around this? How do I find which model is  
best and/or can I use another method that won't be sensitive to the  
order of the terms.

Thanks,

Christian

Reply to: c.landry at umontreal.ca


From br44114 at gmail.com  Fri Mar  9 18:28:57 2007
From: br44114 at gmail.com (bogdan romocea)
Date: Fri, 9 Mar 2007 12:28:57 -0500
Subject: [R] How to create a list that grows automatically
Message-ID: <8d5a36350703090928n1263d24fub6c794190444570e@mail.gmail.com>

This is a bad idea as it can greatly slow things down (the details
were discussed several times on this list). What you want to do is
define from the start the length of your vector/list, then grow it (by
a large margin) only if it becomes full.
lst <- vector(mode="list", length=100000)  #assuming 100k nodes are enough
#populate the list, then remove the unused nodes if you care to
lst <- lst[sapply(lst, function(x) {!is.null(x)})]


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Young-Jin Lee
> Sent: Friday, March 09, 2007 11:08 AM
> To: r-help
> Subject: [R] How to create a list that grows automatically
>
> Dear R users
>
> I would like to know if there is a way to create a list or an
> array (or
> anything) which grows automatically as more elements are put
> into it. What I
> want to find is something equivalent to an ArrayList object of Java
> language. In Java, I can do the following thing:
>
> // Java code
> ArrayList myArray = new ArrayList();
> myArray.add("object1");
> myArray.add("object2");
> ....
> // End of java code
>
> Thanks in advance.
>
> Young-Jin Lee
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From spluque at gmail.com  Fri Mar  9 18:38:22 2007
From: spluque at gmail.com (Sebastian P. Luque)
Date: Fri, 09 Mar 2007 11:38:22 -0600
Subject: [R] Duplicate rows of matrix
References: <JENC0G$5B8DD937F0AB7290952F942525F521D5@libero.it>
Message-ID: <87mz2mtj01.fsf@patagonia.sebmags.homelinux.org>

On Fri,  9 Mar 2007 18:17:04 +0100,
"Bruno C\." <bruno.c at inwind.it> wrote:

> Hello my problem is the following: I have a matrix A and a vector B
> which contains as many rows as A.

> I need to build a matrix C which contains B[i]-times the row A[i,] and
> this for each line of A.

How about:


C <- A[rep(seq(nrow(A)), B), ]


Completely untested, because you didn't provide example code.


-- 
Seb


From christos at nuverabio.com  Fri Mar  9 18:41:14 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Fri, 9 Mar 2007 12:41:14 -0500
Subject: [R] Duplicate rows of matrix
In-Reply-To: <JENC0G$5B8DD937F0AB7290952F942525F521D5@libero.it>
References: <JENC0G$5B8DD937F0AB7290952F942525F521D5@libero.it>
Message-ID: <004b01c76272$230ee140$0e010a0a@headquarters.silicoinsights>

Try this:

a <- matrix(c(8, 4.2, 9.4, 1.1),2) 
b <- c(3,1)

a[rep(1:nrow(a), b), ] 

-Christos

Christos Hatzis, Ph.D.
Nuvera Biosciences, Inc.
400 West Cummings Park
Suite 5350
Woburn, MA 01801
Tel: 781-938-3830
www.nuverabio.com
 


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bruno C.
> Sent: Friday, March 09, 2007 12:17 PM
> To: r-help
> Subject: [R] Duplicate rows of matrix
> 
> Hello my problem is the following:
> 
> I have a matrix A and a vector B which contains as many rows as A.
> 
> I need to build a matrix C which contains B[i]-times the row 
> A[i,] and this for each line of A.
> 
> if for example A is
> 
>     [1]    [2]
> [1]  8      9.4
> [2]  4.2    1.1
> 
> and B is (3,1). Then C will be:
>     [1]    [2]
> [1]  8      9.4
> [2]  8      9.4
> [3]  8      9.4
> [4]  4.2    1.1
> 
> 
> I have some working code which go through all the lines of A 
> and for each line does a rbind(C, A[i,]) B[i]-times But this 
> is quite time consuming given that each rbind rebuild a new 
> matrix ... is there any faster way?
> I can think of some minor improvements like building a matrix 
> C of zeros, containing as many columns as A and as many 
> columns as the sum of elements of B ... and the filing it.
> 
> But I was more looking for some already implemented 
> function/package, is there any?
> 
> Thanx
> 
> 
> 
> ------------------------------------------------------
> Leggi GRATIS le tue mail con il telefonino i-modeT di Wind 
> http://i-mode.wind.it
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From topkatz at msn.com  Fri Mar  9 18:39:20 2007
From: topkatz at msn.com (Talbot Katz)
Date: Fri, 09 Mar 2007 12:39:20 -0500
Subject: [R] lpSolve space problem in R 2.4.1 on Windows XP
In-Reply-To: <45F19092.10900@statistik.uni-dortmund.de>
Message-ID: <BAY132-F620086DA027A67C3EEDBCAA780@phx.gbl>

Hello Sam Buttrey.

Uwe Ligges from the r-help list asked me to forward this message to the 
maintainer of the lpSolve package, because R 2.4.1 is crashing when I run 
lp.  I saw your name listed in the lpSolve help file.  If you need more 
detail, please let me know.  Thanks!

--  TMK  --
212-460-5430	home
917-656-5351	cell



>From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
>To: Talbot Katz <topkatz at msn.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] lpSolve space problem in R 2.4.1 on Windows XP
>Date: Fri, 09 Mar 2007 17:51:30 +0100
>
>If R is closed that way (i.e. crashes), it is a bug by definition: either 
>in R or (more probable) in the package. Can you please contact the package 
>maintainer to sort things out.
>
>Thanks,
>Uwe Ligges
>
>
>
>
>
>Talbot Katz wrote:
>>Hi.
>>
>>I am trying to use the linear optimizer from package lpSolve in R 2.4.1 on 
>>Windows XP (Version 5.1).  The problem I am trying to solve has 2843 
>>variables (2841 integer, 2 continuous) and 8524 constraints, and I have 2 
>>Gb of memory.  After I load the input data into R, I have at most 1.5 Gb 
>>of memory available.  If I start the lp with significantly less memory 
>>available (say 1 Gb), I get an error message from R:
>>
>>"Error: cannot allocate vector of size 189459 Kb"
>>
>>If I close all my other windows and try to maximize the available memory 
>>to the full 1.5 Gb, I can watch the memory get filled up until only about 
>>400 Mb is left, at which point I get a Windows error message:
>>
>>"R for Windows GUI front-end has encountered a problem and needs to close. 
>>  We are sorry for the inconvenience."
>>
>>This behavior persists even when I relax the integer constraints, and 
>>eliminate the 2841 constraints that restrict the integer variables to 
>>values <= 1, so I'm just running a standard lp with 2843 variables and 
>>5683 constraints.
>>
>>I have been able to get the full MIP formulation to work correctly on some 
>>very small problems (~10 variables and 25 constraints).
>>
>>Here is the code for a working example:
>>
>>>library(lpSolve)
>>>(v1=rev(1:8))
>>[1] 8 7 6 5 4 3 2 1
>>>(csv1=cumsum(as.numeric(v1)))
>>[1]  8 15 21 26 30 33 35 36
>>>(lencsv1=length(csv1))
>>[1] 8
>>>(Nm1=lencsv1-1)
>>[1] 7
>>>(Np1=lencsv1+1)
>>[1] 9
>>>ngp=3
>>>f.obj=c(1,1,rep(0,Nm1))
>>>f.int=3:Np1
>>>bin.con=cbind(rep(0,Nm1),rep(0,Nm1),diag(Nm1))
>>>bin.dir=rep("<=",Nm1)
>>>bin.rhs=rep(1,Nm1)
>>>gp.con=c(0,0,rep(1,Nm1))
>>>gp.dir="<="
>>>(gp.rhs=ngp-1)
>>[1] 2
>>>ub.con=cbind(rep(-1,rep(Nm1)),rep(0,Nm1),!upper.tri(matrix(nrow=Nm1,ncol=Nm1)))
>>>ub.dir=rep("<=",Nm1)
>>>(ub.rhs=csv1[1:Nm1]*ngp/csv1[lencsv1])
>>[1] 0.6666667 1.2500000 1.7500000 2.1666667 2.5000000 2.7500000 2.9166667
>>>lb.con=cbind(rep(0,Nm1),rep(1,rep(Nm1)),!upper.tri(matrix(nrow=Nm1,ncol=Nm1)))
>>>lb.dir=rep(">=",Nm1)
>>>lb.rhs=ub.rhs
>>>f.con=rbind(bin.con,gp.con,ub.con,lb.con)
>>>f.dir=c(bin.dir,gp.dir,ub.dir,lb.dir)
>>>f.rhs=c(bin.rhs,gp.rhs,ub.rhs,lb.rhs)
>>>lglp=lp("min",f.obj,f.con,f.dir,f.rhs,int.vec=f.int)
>>>lglp$objval
>>[1] 0.9166667
>>>lglp$solution
>>[1] 0.0000000 0.9166667 0.0000000 1.0000000 0.0000000 1.0000000 0.0000000
>>[8] 0.0000000 0.0000000
>>
>>What this is doing is taking the points of v1 and dividing them into 
>>contiguous groups (the variable ngp is the number of groups) such that the 
>>sums of the v1 values are as close as possible to equal within the three 
>>groups.  So, for v1 = c(8,7,6,5,4,3,2,1), the groups c(8,7), c(6,5), 
>>c(4,3,2,1), with sums 15,11,10 is the best such split, and the solution 
>>vector shows that the splitting occurs after the second and fourth 
>>elements.
>>
>>
>>Anyway, I am wondering...  Are 3000 variables and 8500 constraints usually 
>>too much for lpSolve to handle in 1.5 Gb of memory?  Is there a possible 
>>bug (in R or in Windows) that leads to the Windows error when the memory 
>>falls below 400 Mb?  Is there a problem with my formulation that makes it 
>>unstable even after the integer constraints are removed?
>>
>>Thanks!
>>
>>
>>--  TMK  --
>>212-460-5430	home
>>917-656-5351	cell
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.


From tlumley at u.washington.edu  Fri Mar  9 18:47:55 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 9 Mar 2007 09:47:55 -0800 (PST)
Subject: [R] GLM: order of terms in model
In-Reply-To: <BED32C46-58B1-4213-A5A4-60008E29C4AC@fas.harvard.edu>
References: <BED32C46-58B1-4213-A5A4-60008E29C4AC@fas.harvard.edu>
Message-ID: <Pine.LNX.4.64.0703090947390.7548@homer24.u.washington.edu>


This is a FAQ

7.18 Why does the output from anova() depend on the order of factors in 
the model?

 	-thomas

On Fri, 9 Mar 2007, Christian Landry wrote:

> Dear R-helpers,
>
> I have been analysing data using a GLM. My model is as follows:
>
> mod <- glm (V ~ T + as.factor(A) + N, family="gaussian")
>
> and using
>
> anova(mod, test="F")
>
> to get the analysis of deviance table and the fraction of deviance
> explained by each term.
>
> T and A dominate with respect to their Deviance, with T having a
> larger effect than A (about twice)
>
> However, if I reverse T and A in the model, I get that A now explains
> more deviance than T.
>
> My questions are: 1) What is it due to?
> 				2) Is there any way around this? How do I find which model is
> best and/or can I use another method that won't be sensitive to the
> order of the terms.
>
> Thanks,
>
> Christian
>
> Reply to: c.landry at umontreal.ca
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From maciej.rhelp at gmail.com  Fri Mar  9 19:19:20 2007
From: maciej.rhelp at gmail.com (Maciej Radziejewski)
Date: Fri, 9 Mar 2007 19:19:20 +0100
Subject: [R] Reformulated matrices dimensions limitation problem
In-Reply-To: <1221313244-1172860468-cardhu_blackberry.rim.net-1509046322-@bwe014-cell00.bisx.prod.on.blackberry>
References: <JEA3B6$D049EC29F5E065770DE226C0B47F6466@libero.it>
	<45E8411E.30416.1A15F2B@localhost>
	<1221313244-1172860468-cardhu_blackberry.rim.net-1509046322-@bwe014-cell00.bisx.prod.on.blackberry>
Message-ID: <732eec150703091019y1783490eg17e103cf734143b1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070309/1589ab81/attachment.pl 

From aiminy at iastate.edu  Fri Mar  9 19:22:17 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Fri, 09 Mar 2007 12:22:17 -0600
Subject: [R] use nnet
Message-ID: <6.2.3.4.2.20070309120535.03e71440@aiminy.mail.iastate.edu>

I want to adjust weight decay and number of hidden units for nnet by 
a loop like
for(decay)
{
  for(number of unit)
   {
    for(#run)
     {model<-nnet()
       test.error<-....
     }
   }
}

for example:
I set decay=0.1, size=3, maxit=200, for this set I run 10 times, and 
calculate test error

after that I want to get a matrix like this

decay  size   maxit  #run  test_error
0.1        3        200   1       1.2
0.1        3        200   2       1.1
0.1        3        200   3       1.0
0.1        3        200   4       3.4
0.1        3        200   5        ..
0.1        3        200   6         ..
0.1        3        200   7       ..
0.1        3        200   8      ..
0.1        3        200   9       ..
0.1        3        200   10       ..
0.2        3        200   1       1.2
0.2        3        200   2       1.1
0.2        3        200   3       1.0
0.2        3        200   4       3.4
0.2        3        200   5        ..
0.2        3        200   6         ..
0.2        3        200   7       ..
0.2        3        200   8      ..
0.2        3        200   9       ..
0.2        3        200   10       ..

I am not sure if this is correct way to do this?
Does anyone tune these parameters like this before?
thanks,

Aimin


From liuwensui at gmail.com  Fri Mar  9 19:37:56 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Fri, 9 Mar 2007 13:37:56 -0500
Subject: [R] use nnet
In-Reply-To: <6.2.3.4.2.20070309120535.03e71440@aiminy.mail.iastate.edu>
References: <6.2.3.4.2.20070309120535.03e71440@aiminy.mail.iastate.edu>
Message-ID: <1115a2b00703091037r6ccb65a5g6d7d889acad45875@mail.gmail.com>

AM,
I have a pieice of junk on my blog. Here it is.
#################################################
# USE CROSS-VALIDATION TO DO A GRID-SEARCH FOR  #
# THE OPTIMAL SETTINGS (WEIGHT DECAY AND NUMBER #
# OF HIDDEN UNITS) OF NEURAL NETS               #
#################################################

library(nnet);
library(MASS);
data(Boston);
X <- I(as.matrix(Boston[-14]));
# STANDARDIZE PREDICTORS
st.X <- scale(X);
Y <- I(as.matrix(Boston[14]));
boston <- data.frame(X = st.X, Y);

# DIVIDE DATA INTO TESTING AND TRAINING SETS
set.seed(2005);
test.rows <- sample(1:nrow(boston), 100);
test.set <- boston[test.rows, ];
train.set <- boston[-test.rows, ];

# INITIATE A NULL TABLE
sse.table <- NULL;

# SEARCH FOR OPTIMAL WEIGHT DECAY
# RANGE OF WEIGHT DECAYS SUGGESTED BY B. RIPLEY
for (w in c(0.0001, 0.001, 0.01))
{
  # SEARCH FOR OPTIMAL NUMBER OF HIDDEN UNITS
  for (n in 1:10)
  {
    # UNITIATE A NULL VECTOR
    sse <- NULL;
    # FOR EACH SETTING, RUN NEURAL NET MULTIPLE TIMES
    for (i in 1:10)
    {
      # INITIATE THE RANDOM STATE FOR EACH NET
      set.seed(i);
      # TRAIN NEURAL NETS
      net <- nnet(Y~X, size = n, data = train.set, rang = 0.00001,
                       linout = TRUE, maxit = 10000, decay = w,
                       skip = FALSE, trace = FALSE);
      # CALCULATE SSE FOR TESTING SET
      test.sse <- sum((test.set$Y - predict(net, test.set))^2);
      # APPEND EACH SSE TO A VECTOR
      if (i == 1) sse <- test.sse else sse <- rbind(sse, test.sse);
    }
    # APPEND AVERAGED SSE WITH RELATED PARAMETERS TO A TABLE
    sse.table <- rbind(sse.table, c(WT = w, UNIT = n, SSE = mean(sse)));
  }
}
# PRINT OUT THE RESULT
print(sse.table);http://statcompute.spaces.live.com/Blog/cns!39C8032DBD1321B7!290.entry


On 3/9/07, Aimin Yan <aiminy at iastate.edu> wrote:
> I want to adjust weight decay and number of hidden units for nnet by
> a loop like
> for(decay)
> {
>   for(number of unit)
>    {
>     for(#run)
>      {model<-nnet()
>        test.error<-....
>      }
>    }
> }
>
> for example:
> I set decay=0.1, size=3, maxit=200, for this set I run 10 times, and
> calculate test error
>
> after that I want to get a matrix like this
>
> decay  size   maxit  #run  test_error
> 0.1        3        200   1       1.2
> 0.1        3        200   2       1.1
> 0.1        3        200   3       1.0
> 0.1        3        200   4       3.4
> 0.1        3        200   5        ..
> 0.1        3        200   6         ..
> 0.1        3        200   7       ..
> 0.1        3        200   8      ..
> 0.1        3        200   9       ..
> 0.1        3        200   10       ..
> 0.2        3        200   1       1.2
> 0.2        3        200   2       1.1
> 0.2        3        200   3       1.0
> 0.2        3        200   4       3.4
> 0.2        3        200   5        ..
> 0.2        3        200   6         ..
> 0.2        3        200   7       ..
> 0.2        3        200   8      ..
> 0.2        3        200   9       ..
> 0.2        3        200   10       ..
>
> I am not sure if this is correct way to do this?
> Does anyone tune these parameters like this before?
> thanks,
>
> Aimin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From liuwensui at gmail.com  Fri Mar  9 19:39:37 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Fri, 9 Mar 2007 13:39:37 -0500
Subject: [R] use nnet
In-Reply-To: <1115a2b00703091037r6ccb65a5g6d7d889acad45875@mail.gmail.com>
References: <6.2.3.4.2.20070309120535.03e71440@aiminy.mail.iastate.edu>
	<1115a2b00703091037r6ccb65a5g6d7d889acad45875@mail.gmail.com>
Message-ID: <1115a2b00703091039p3034dacdn260981e4afffa854@mail.gmail.com>

AM,
Sorry. please ignore the top box in the code. It is not actually a cv
validation but just a simple split-sample validation.
sorry for confusion.

On 3/9/07, Wensui Liu <liuwensui at gmail.com> wrote:
> AM,
> I have a pieice of junk on my blog. Here it is.
> #################################################
> # USE CROSS-VALIDATION TO DO A GRID-SEARCH FOR  #
> # THE OPTIMAL SETTINGS (WEIGHT DECAY AND NUMBER #
> # OF HIDDEN UNITS) OF NEURAL NETS               #
> #################################################
>
> library(nnet);
> library(MASS);
> data(Boston);
> X <- I(as.matrix(Boston[-14]));
> # STANDARDIZE PREDICTORS
> st.X <- scale(X);
> Y <- I(as.matrix(Boston[14]));
> boston <- data.frame(X = st.X, Y);
>
> # DIVIDE DATA INTO TESTING AND TRAINING SETS
> set.seed(2005);
> test.rows <- sample(1:nrow(boston), 100);
> test.set <- boston[test.rows, ];
> train.set <- boston[-test.rows, ];
>
> # INITIATE A NULL TABLE
> sse.table <- NULL;
>
> # SEARCH FOR OPTIMAL WEIGHT DECAY
> # RANGE OF WEIGHT DECAYS SUGGESTED BY B. RIPLEY
> for (w in c(0.0001, 0.001, 0.01))
> {
>   # SEARCH FOR OPTIMAL NUMBER OF HIDDEN UNITS
>   for (n in 1:10)
>   {
>     # UNITIATE A NULL VECTOR
>     sse <- NULL;
>     # FOR EACH SETTING, RUN NEURAL NET MULTIPLE TIMES
>     for (i in 1:10)
>     {
>       # INITIATE THE RANDOM STATE FOR EACH NET
>       set.seed(i);
>       # TRAIN NEURAL NETS
>       net <- nnet(Y~X, size = n, data = train.set, rang = 0.00001,
>                        linout = TRUE, maxit = 10000, decay = w,
>                        skip = FALSE, trace = FALSE);
>       # CALCULATE SSE FOR TESTING SET
>       test.sse <- sum((test.set$Y - predict(net, test.set))^2);
>       # APPEND EACH SSE TO A VECTOR
>       if (i == 1) sse <- test.sse else sse <- rbind(sse, test.sse);
>     }
>     # APPEND AVERAGED SSE WITH RELATED PARAMETERS TO A TABLE
>     sse.table <- rbind(sse.table, c(WT = w, UNIT = n, SSE = mean(sse)));
>   }
> }
> # PRINT OUT THE RESULT
> print(sse.table);http://statcompute.spaces.live.com/Blog/cns!39C8032DBD1321B7!290.entry
>
>
> On 3/9/07, Aimin Yan <aiminy at iastate.edu> wrote:
> > I want to adjust weight decay and number of hidden units for nnet by
> > a loop like
> > for(decay)
> > {
> >   for(number of unit)
> >    {
> >     for(#run)
> >      {model<-nnet()
> >        test.error<-....
> >      }
> >    }
> > }
> >
> > for example:
> > I set decay=0.1, size=3, maxit=200, for this set I run 10 times, and
> > calculate test error
> >
> > after that I want to get a matrix like this
> >
> > decay  size   maxit  #run  test_error
> > 0.1        3        200   1       1.2
> > 0.1        3        200   2       1.1
> > 0.1        3        200   3       1.0
> > 0.1        3        200   4       3.4
> > 0.1        3        200   5        ..
> > 0.1        3        200   6         ..
> > 0.1        3        200   7       ..
> > 0.1        3        200   8      ..
> > 0.1        3        200   9       ..
> > 0.1        3        200   10       ..
> > 0.2        3        200   1       1.2
> > 0.2        3        200   2       1.1
> > 0.2        3        200   3       1.0
> > 0.2        3        200   4       3.4
> > 0.2        3        200   5        ..
> > 0.2        3        200   6         ..
> > 0.2        3        200   7       ..
> > 0.2        3        200   8      ..
> > 0.2        3        200   9       ..
> > 0.2        3        200   10       ..
> >
> > I am not sure if this is correct way to do this?
> > Does anyone tune these parameters like this before?
> > thanks,
> >
> > Aimin
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> WenSui Liu
> A lousy statistician who happens to know a little programming
> (http://spaces.msn.com/statcompute/blog)
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From jrkrideau at yahoo.ca  Fri Mar  9 20:13:41 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 9 Mar 2007 14:13:41 -0500 (EST)
Subject: [R] Applying some equations over all unique combinations of 4
	variables
Message-ID: <45064.21637.qm@web32814.mail.mud.yahoo.com>

#I have a data set that looks like this.   A bit more
complicated actually with
# three factor levels but these calculations need to
be done on one factor at a
#I then have a set of different rates that are applied
#to it.

#dataset
cata <- c( 1,1,6,1,1,2)
catb <- c( 1,2,3,4,5,6)
doga <- c(3,5,3,6,4, 0)

data1 <- data.frame(cata, catb, doga)
rm(cata,catb,doga)
data1

# start rates
# names for lists
fnams  <- c("af", "pf", "cf", "mf")
mnams  <-  c("am", "pm", "cm", "mm")

# Current layout of the rate data frames
alphahill <- list(af <- c("a1","a2","a3"), pf <-
c("d1","d2","d3"),
              cf <- c("f1","f2"), mf <- c("h1","h2"))
names(alphahill)  <-  fnams
       
betahill <- list(am <- c("b1","b2","b3"), pm<-
c("e1","e2","e3"),
             cm <- c("g1","g2"), mm <- c("j1", "j2"))
names(betahill) <- mnams

hilltop <- list(af <- data.frame(a1 <- 1:4 , a2 <-
2:5, a3 <- 3:6),
                pf <- data.frame(d1 <- 4:1, d2 <- 5:2,
d3 <- 6:3),
                cf <- data.frame(f1 <- 1:4, f2 <-
3:6),
                mf <- data.frame(h1 <- 1:4,  h2 <-
2:5))
                
hilldown <- list(am <- data.frame(b1 <- 4:1, b2 <-
5:2, b3 <- 6:3),
                 pm <- data.frame(e1 <- 5:1, e2 <-
1:5,e3 <- 6:2),
                 cm <- data.frame (g1 <- 5:1, g2 <-
1:5),
                 mm  <- data.frame(j1 <- 1:4,  j2 <-
5:2))
names(hilltop) <- fnams
names(hilldown) <- mnams
for (i in 1:4) {
  names(hilltop[[i]]) <- alphahill[[i]]
  names(hilldown[[i]]) <- betahill[[i]]
}

rm(a1,a2,a3,b1,b2,b3,d1,d2,d3,e1,e2,e3,f1,f2,g1,g2,h1,h2,j1,j2,
fnams, mnams,
    af, am,cf,cm,mf, mm,pf, pm)
# Now that's out of the way

#Assuming I am reading this problem correctly I should
have
#648  possible combinations for each row of data that
is:
#unique combinations where I need
#   (af*am) * (pf*pm) * (cf*cm) *  mf * mm
# ie (3*3)  *  (3* 3)  * (2*2)  *   2*2)
# based on the idea that there are  9  unique
combination for af & am and so
# on.

#     af   am
# 1   a1   b1
# 2   a2   b1
# 3   a3   b1
# 4   a1   b2
# 5   a2   b2
# 6   a3   b2
# 7   a1   b3
# 8   a2   b3
# 9   a3   b3

# I have a set of equations of the form :

#    P1 <- af*cata + pf*catb^cf + mf*doga
#    S1 <- am*cata + pm*catb^cm + mm*doga

#Is there any way that I can do something like this
and keep track of
#what condition is what since I need to be able to sum
the P1s and P2, for
# for each combination (or a subset of them) ?  I
suspect it may be a fairly
# straight-forward "apply " problem but I am having a
real problem with it.

# I am only likely to need to report, perhaps. 15
combinations but at the
# moment Idon't see any easy way to do them and doing
all possible outcomes and
# extracting the required ones looks like a better and
safer approach if it can
# be done.And will save a lot of time if we suddenly
need a few new comparisons.

# Any help would be greatly appreciated.


From h.wickham at gmail.com  Fri Mar  9 20:21:15 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 9 Mar 2007 13:21:15 -0600
Subject: [R] How to create a list that grows automatically
In-Reply-To: <9a2a73210703090808x2fd9ce77i4537d3428045c061@mail.gmail.com>
References: <9a2a73210703090808x2fd9ce77i4537d3428045c061@mail.gmail.com>
Message-ID: <f8e6ff050703091121u4fb3d109yf5eeac6bfdd9661@mail.gmail.com>

> I would like to know if there is a way to create a list or an array (or
> anything) which grows automatically as more elements are put into it. What I
> want to find is something equivalent to an ArrayList object of Java
> language. In Java, I can do the following thing:
>
> // Java code
> ArrayList myArray = new ArrayList();
> myArray.add("object1");
> myArray.add("object2");
> ....
> // End of java code

As others have mentioned, you can do this with lists in R.

However, there is an important difference between ArrayLists in Java
and Lists in R.  In Java, when an ArrayList grows past its bound, it
doesn't allocate just enough space, it allocates a lot more, so the
next time you allocate past the end of the array, there's space
already reserved.  This gives (IIRC) amortised O(n) behaviour.  R
doesn't do this however, so has to copy the entire array every time
giving O(n^2) behaviour.

Hadley


From jrkrideau at yahoo.ca  Fri Mar  9 20:24:26 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 9 Mar 2007 14:24:26 -0500 (EST)
Subject: [R] Applying some equations over all unique combinations of 4
	variables
In-Reply-To: <45064.21637.qm@web32814.mail.mud.yahoo.com>
Message-ID: <677051.14976.qm@web32808.mail.mud.yahoo.com>

I just realised after posting I have two vectors of
the  wrong length.  The corrected program is:

#dataset
cata <- c( 1,1,6,1,1,2)
catb <- c( 1,2,3,4,5,6)
doga <- c(3,5,3,6,4, 0)

data1 <- data.frame(cata, catb, doga)
rm(cata,catb,doga)
data1

# start rates
# names for lists
fnams  <- c("af", "pf", "cf", "mf")
mnams  <-  c("am", "pm", "cm", "mm")

# Current layout of the rate data frames
alphahill <- list(af <- c("a1","a2","a3"), pf <-
c("d1","d2","d3"),
              cf <- c("f1","f2"), mf <- c("h1","h2"))
names(alphahill)  <-  fnams
       
betahill <- list(am <- c("b1","b2","b3"), pm<-
c("e1","e2","e3"),
             cm <- c("g1","g2"), mm <- c("j1", "j2"))
names(betahill) <- mnams

hilltop <- list(af <- data.frame(a1 <- 1:4 , a2 <-
2:5, a3 <- 3:6),
                pf <- data.frame(d1 <- 4:1, d2 <- 5:2,
d3 <- 6:3),
                cf <- data.frame(f1 <- 1:4, f2 <-
3:6),
                mf <- data.frame(h1 <- 1:4,  h2 <-
3:6))
                
hilldown <- list(am <- data.frame(b1 <- 4:1, b2 <-
5:2, b3 <- 6:3),
                 pm <- data.frame(e1 <- 4:1, e2 <-
1:4,e3 <- 6:3),
                 cm <- data.frame (g1 <- 4:1, g2 <-
1:4),
                 mm  <- data.frame(j1 <- 1:4,  j2 <-
4:1))
names(hilltop) <- fnams
names(hilldown) <- mnams
for (i in 1:4) {
  names(hilltop[[i]]) <- alphahill[[i]]
  names(hilldown[[i]]) <- betahill[[i]]
}

rm(a1,a2,a3,b1,b2,b3,d1,d2,d3,e1,e2,e3,f1,f2,g1,g2,h1,h2,j1,j2,
fnams, mnams,
    af, am,cf,cm,mf, mm,pf, pm)
--- John Kane <jrkrideau at yahoo.ca> wrote:

> #I have a data set that looks like this.   A bit
> more
> complicated actually with
> # three factor levels but these calculations need to
> be done on one factor at a
> #I then have a set of different rates that are
> applied
> #to it.
> 
> #dataset
> cata <- c( 1,1,6,1,1,2)
> catb <- c( 1,2,3,4,5,6)
> doga <- c(3,5,3,6,4, 0)
> 
> data1 <- data.frame(cata, catb, doga)
> rm(cata,catb,doga)
> data1
> 
> # start rates
> # names for lists
> fnams  <- c("af", "pf", "cf", "mf")
> mnams  <-  c("am", "pm", "cm", "mm")
> 
> # Current layout of the rate data frames
> alphahill <- list(af <- c("a1","a2","a3"), pf <-
> c("d1","d2","d3"),
>               cf <- c("f1","f2"), mf <-
> c("h1","h2"))
> names(alphahill)  <-  fnams
>        
> betahill <- list(am <- c("b1","b2","b3"), pm<-
> c("e1","e2","e3"),
>              cm <- c("g1","g2"), mm <- c("j1",
> "j2"))
> names(betahill) <- mnams
> 
> hilltop <- list(af <- data.frame(a1 <- 1:4 , a2 <-
> 2:5, a3 <- 3:6),
>                 pf <- data.frame(d1 <- 4:1, d2 <-
> 5:2,
> d3 <- 6:3),
>                 cf <- data.frame(f1 <- 1:4, f2 <-
> 3:6),
>                 mf <- data.frame(h1 <- 1:4,  h2 <-
> 2:5))
>                 
> hilldown <- list(am <- data.frame(b1 <- 4:1, b2 <-
> 5:2, b3 <- 6:3),
>                  pm <- data.frame(e1 <- 5:1, e2 <-
> 1:5,e3 <- 6:2),
>                  cm <- data.frame (g1 <- 5:1, g2 <-
> 1:5),
>                  mm  <- data.frame(j1 <- 1:4,  j2 <-
> 5:2))
> names(hilltop) <- fnams
> names(hilldown) <- mnams
> for (i in 1:4) {
>   names(hilltop[[i]]) <- alphahill[[i]]
>   names(hilldown[[i]]) <- betahill[[i]]
> }
> 
>
rm(a1,a2,a3,b1,b2,b3,d1,d2,d3,e1,e2,e3,f1,f2,g1,g2,h1,h2,j1,j2,
> fnams, mnams,
>     af, am,cf,cm,mf, mm,pf, pm)
> # Now that's out of the way
> 
> #Assuming I am reading this problem correctly I
> should
> have
> #648  possible combinations for each row of data
> that
> is:
> #unique combinations where I need
> #   (af*am) * (pf*pm) * (cf*cm) *  mf * mm
> # ie (3*3)  *  (3* 3)  * (2*2)  *   2*2)
> # based on the idea that there are  9  unique
> combination for af & am and so
> # on.
> 
> #     af   am
> # 1   a1   b1
> # 2   a2   b1
> # 3   a3   b1
> # 4   a1   b2
> # 5   a2   b2
> # 6   a3   b2
> # 7   a1   b3
> # 8   a2   b3
> # 9   a3   b3
> 
> # I have a set of equations of the form :
> 
> #    P1 <- af*cata + pf*catb^cf + mf*doga
> #    S1 <- am*cata + pm*catb^cm + mm*doga
> 
> #Is there any way that I can do something like this
> and keep track of
> #what condition is what since I need to be able to
> sum
> the P1s and P2, for
> # for each combination (or a subset of them) ?  I
> suspect it may be a fairly
> # straight-forward "apply " problem but I am having
> a
> real problem with it.
> 
> # I am only likely to need to report, perhaps. 15
> combinations but at the
> # moment Idon't see any easy way to do them and
> doing
> all possible outcomes and
> # extracting the required ones looks like a better
> and
> safer approach if it can
> # be done.And will save a lot of time if we suddenly
> need a few new comparisons.
> 
> # Any help would be greatly appreciated.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From mallikav at burnham.org  Fri Mar  9 21:02:39 2007
From: mallikav at burnham.org (Mallika Veeramalai)
Date: Fri, 09 Mar 2007 12:02:39 -0800
Subject: [R] Reg. strings and numeric data in matrix.
Message-ID: <C216FD5F.1220%mallikav@burnham.org>


Hi All,

Sorry for this basic question as I am new to this R. I would like to know,
is it possible to consider a matrix with some columns having numeric data
and some other's with characters (strings) data?  How do I get this type of
data from a flat file.

Thanks very much,
mallika 

____________________________________________________________________________
    Mallika Veeramalai, Ph.D.,
    Postdoctoral Associate,
    Bioinformatics & Systems Biology,
    Burnham Institute for Medical Research,
    La Jolla,  CA 92037, USA.
    phone : +1 858 646 3100 ext: 3627
    Fax   : +1 858 795 5249
    Web   : http://bioinformatics.burnham.org/~mallika/
    Email : mallikav at burnham.org (or) kaaviyam at gmail.com


From lqecli at yahoo.fr  Fri Mar  9 21:10:53 2007
From: lqecli at yahoo.fr (Luca Quaglia)
Date: Fri, 9 Mar 2007 21:10:53 +0100 (CET)
Subject: [R] About "cex=": how to improve resolution?
Message-ID: <20070309201053.93922.qmail@web27607.mail.ukl.yahoo.com>

Hi,

I need to plot a graph with a fixed circle and with a
series of point of different size. Here is a
"simplified" example:

angle<-pi/180*c(0:360)
x<-seq(0,2,by=0.2)
y<-seq(0,2,by=0.2)
z<-seq(0,1,by=0.1)
par(pty="s")
plot(-2:2,-2:2,type="n")
lines(cos(angle),sin(angle))
points(x,y,cex=z)

The size of the points compared to the circle (of
radius 1) is important and bears a meaning. 

But instead of having 11 points with increasing size,
I only obtain points of the same size when
cex=0.1/0.2/0.3/0.4 or cex=0.5/0.6/0.7 or
cex=0.8/0.9/1.0.

Please, does anyone know if there is a way of
improving the resolution of "cex=" *without* changing
the size of the circle of radius 1 and keeping the
same axis?

Thanks in advance!!!

Luca


From shawnwaypublic at yahoo.com  Fri Mar  9 21:12:29 2007
From: shawnwaypublic at yahoo.com (Shawn Way)
Date: Fri, 9 Mar 2007 12:12:29 -0800 (PST)
Subject: [R] Extracting text from a character string
Message-ID: <505602.83984.qm@web37113.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070309/795ea1f3/attachment.pl 

From Shawn.Way at biogenidec.com  Fri Mar  9 21:23:57 2007
From: Shawn.Way at biogenidec.com (Shawn Way)
Date: Fri, 9 Mar 2007 15:23:57 -0500
Subject: [R] Extracting text from a character string
Message-ID: <OFD22D40A3.7A5FA5C6-ON85257299.006FD93B-85257299.00700ECA@biogenidec.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070309/ec5156b9/attachment.pl 

From canamika16 at yahoo.com  Fri Mar  9 21:26:58 2007
From: canamika16 at yahoo.com (Anamika Chaudhuri)
Date: Fri, 9 Mar 2007 12:26:58 -0800 (PST)
Subject: [R] MCMC logit
Message-ID: <119561.83667.qm@web34102.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070309/c88a3a57/attachment.pl 

From bolker at zoo.ufl.edu  Fri Mar  9 21:10:33 2007
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Fri, 9 Mar 2007 20:10:33 +0000 (UTC)
Subject: [R] Reg. strings and numeric data in matrix.
References: <C216FD5F.1220%mallikav@burnham.org>
Message-ID: <loom.20070309T210808-477@post.gmane.org>

Mallika Veeramalai <mallikav <at> burnham.org> writes:

I would like to know,
> is it possible to consider a matrix with some columns having numeric data
> and some other's with characters (strings) data?  How do I get this type of
> data from a flat file.

  It's called a "data frame".  See the Introduction to R,
and help for read.table and read.csv.  (The character data
will get made into factors unless you use as.is=TRUE
or specify colClasses.)


From jrkrideau at yahoo.ca  Fri Mar  9 21:34:00 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 9 Mar 2007 15:34:00 -0500 (EST)
Subject: [R] Reg. strings and numeric data in matrix.
In-Reply-To: <C216FD5F.1220%mallikav@burnham.org>
Message-ID: <20070309203401.69248.qmail@web32801.mail.mud.yahoo.com>


--- Mallika Veeramalai <mallikav at burnham.org> wrote:

> 
> Hi All,
> 
> Sorry for this basic question as I am new to this R.
> I would like to know,
> is it possible to consider a matrix with some
> columns having numeric data
> and some other's with characters (strings) data? 
> How do I get this type of
> data from a flat file.
> 
> Thanks very much,
> mallika 

If I understand the question the answer is NO. A
matrix must be of one type of data.  

I think that what you want is a data.frame wich allows
mixed categores of data.  
Try this to see the difference.

a <- c('a','b','c')
b <- c( 1,2,3)

aa <- cbind(a,b)
aa
class(aa)

bb <- data.frame(a,b)
bb
class(bb)


From klaster at karlin.mff.cuni.cz  Fri Mar  9 21:35:52 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Fri, 09 Mar 2007 21:35:52 +0100
Subject: [R] Reg. strings and numeric data in matrix.
In-Reply-To: <C216FD5F.1220%mallikav@burnham.org>
References: <C216FD5F.1220%mallikav@burnham.org>
Message-ID: <45F1C528.8090206@karlin.mff.cuni.cz>

See
?data.frame
?read.table
and please read (appropriate parts of) the "Introduction to R" manual.
Petr

Mallika Veeramalai napsal(a):
> Hi All,
> 
> Sorry for this basic question as I am new to this R. I would like to know,
> is it possible to consider a matrix with some columns having numeric data
> and some other's with characters (strings) data?  How do I get this type of
> data from a flat file.
> 
> Thanks very much,
> mallika 
> 
> ____________________________________________________________________________
>     Mallika Veeramalai, Ph.D.,
>     Postdoctoral Associate,
>     Bioinformatics & Systems Biology,
>     Burnham Institute for Medical Research,
>     La Jolla,  CA 92037, USA.
>     phone : +1 858 646 3100 ext: 3627
>     Fax   : +1 858 795 5249
>     Web   : http://bioinformatics.burnham.org/~mallika/
>     Email : mallikav at burnham.org (or) kaaviyam at gmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From Greg.Snow at intermountainmail.org  Fri Mar  9 21:39:38 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 9 Mar 2007 13:39:38 -0700
Subject: [R] Reg. strings and numeric data in matrix.
In-Reply-To: <C216FD5F.1220%mallikav@burnham.org>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB87A139@LP-EXCHVS07.CO.IHC.COM>

A matrix can only have 1 type of data, so if you try to include both
strings and numbers in a matrix, the numbers will be converted to
strings.

Another type of data object is a data frame, a data frame works much
like a matrix in many ways, but allows some columns to be numbers and
others to be strings (though usually strings are converted to factors).

You should read (or reread) the help page "An Introduction to R",
section 5 talks about matricies, then section 6 talks about data frames
(and lists).  Section 7 shows how to read data from files into data
frames.  Those 3 sections should answer your questions below.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Mallika Veeramalai
> Sent: Friday, March 09, 2007 1:03 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Reg. strings and numeric data in matrix.
> 
> 
> Hi All,
> 
> Sorry for this basic question as I am new to this R. I would 
> like to know, is it possible to consider a matrix with some 
> columns having numeric data and some other's with characters 
> (strings) data?  How do I get this type of data from a flat file.
> 
> Thanks very much,
> mallika 
> 
> ______________________________________________________________
> ______________
>     Mallika Veeramalai, Ph.D.,
>     Postdoctoral Associate,
>     Bioinformatics & Systems Biology,
>     Burnham Institute for Medical Research,
>     La Jolla,  CA 92037, USA.
>     phone : +1 858 646 3100 ext: 3627
>     Fax   : +1 858 795 5249
>     Web   : http://bioinformatics.burnham.org/~mallika/
>     Email : mallikav at burnham.org (or) kaaviyam at gmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Greg.Snow at intermountainmail.org  Fri Mar  9 21:44:40 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 9 Mar 2007 13:44:40 -0700
Subject: [R] About "cex=": how to improve resolution?
In-Reply-To: <20070309201053.93922.qmail@web27607.mail.ukl.yahoo.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB87A13B@LP-EXCHVS07.CO.IHC.COM>

If the size of the circle is important, then you may want to use the
symbols function with the circle argument rather than points and cex.
Use the inches argument to set the size (in inches) of the largest
circle, then the other circles will be scalled accordingly.  Or if you
set inches=FALSE, then the circles will be scaled to the x-axis.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Luca Quaglia
> Sent: Friday, March 09, 2007 1:11 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] About "cex=": how to improve resolution?
> 
> Hi,
> 
> I need to plot a graph with a fixed circle and with a series 
> of point of different size. Here is a "simplified" example:
> 
> angle<-pi/180*c(0:360)
> x<-seq(0,2,by=0.2)
> y<-seq(0,2,by=0.2)
> z<-seq(0,1,by=0.1)
> par(pty="s")
> plot(-2:2,-2:2,type="n")
> lines(cos(angle),sin(angle))
> points(x,y,cex=z)
> 
> The size of the points compared to the circle (of radius 1) 
> is important and bears a meaning. 
> 
> But instead of having 11 points with increasing size, I only 
> obtain points of the same size when
> cex=0.1/0.2/0.3/0.4 or cex=0.5/0.6/0.7 or cex=0.8/0.9/1.0.
> 
> Please, does anyone know if there is a way of improving the 
> resolution of "cex=" *without* changing the size of the 
> circle of radius 1 and keeping the same axis?
> 
> Thanks in advance!!!
> 
> Luca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bunny at lautloscrew.com  Fri Mar  9 21:47:35 2007
From: bunny at lautloscrew.com (bunny , lautloscrew.com)
Date: Fri, 9 Mar 2007 21:47:35 +0100
Subject: [R] dendrogram again
In-Reply-To: <1173444755.27004.18.camel@gsimpson.geog.ucl.ac.uk>
References: <A5077BE5-CB55-4026-BC8B-E201BA5D5DD5@lautloscrew.com>
	<1173444755.27004.18.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <7B8143B3-4EC0-4865-B82D-29FCB1F1C613@lautloscrew.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070309/a031f2f2/attachment.pl 

From marc_schwartz at comcast.net  Fri Mar  9 21:44:47 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 09 Mar 2007 14:44:47 -0600
Subject: [R] Extracting text from a character string
In-Reply-To: <OFD22D40A3.7A5FA5C6-ON85257299.006FD93B-85257299.00700ECA@biogenidec.com>
References: <OFD22D40A3.7A5FA5C6-ON85257299.006FD93B-85257299.00700ECA@biogenidec.com>
Message-ID: <1173473087.4995.22.camel@localhost.localdomain>

On Fri, 2007-03-09 at 15:23 -0500, Shawn Way wrote:
>  I have a set of character strings like below:
>  
> > data3[1]
> [1] "CB01_0171_03-27-2002-(Sample 26609)-(126)"
> > 
>  
> I am trying to extract the text 03-27-2002 and convert this into a date 
> for the same record.  I keep looking at the grep function, however I 
> cannot quite get it to work.
>  
> grep("\d\d-\d\d-\d\d\d\d",data3[1],perl=TRUE,value=TRUE)
>  
> Any hints?


At least two different ways:

Vec <- "CB01_0171_03-27-2002-(Sample 26609)-(126)"


1. Using substr(), if your source vector is a fixed format

# Get the 11th thru the 20th character
> substr(Vec, 11, 20)
[1] "03-27-2002"


2. Using sub() for a more generalized approach:

# Use a back reference, returning the value pattern within the 
# parens

> sub(".+([0-9]{2}-[0-9]{2}-[0-9]{4}).+", "\\1", Vec)
[1] "03-27-2002"


See ?substr, ?sub and ?regex

HTH,

Marc Schwartz


From Greg.Snow at intermountainmail.org  Fri Mar  9 21:47:42 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 9 Mar 2007 13:47:42 -0700
Subject: [R] Extracting text from a character string
In-Reply-To: <505602.83984.qm@web37113.mail.mud.yahoo.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB87A13C@LP-EXCHVS07.CO.IHC.COM>

Try replacing \d with \\d throughout your pattern.  The R parser is
trying to interpret the \ before the grep function ever sees it.  By
backslashing the backslashes, the parser ends up putting a single
backslash in the pattern for grep to see.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Shawn Way
> Sent: Friday, March 09, 2007 1:12 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Extracting text from a character string
> 
> I have a set of character strings like below:
>    
>   > data3[1]
> [1] "CB01_0171_03-27-2002-(Sample 26609)-(126)"
> > 
>    
>   I am trying to extract the text 03-27-2002 and convert this 
> into a date for the same record.  I keep looking at the grep 
> function, however I cannot quite get it to work.
>    
>   grep("\d\d-\d\d-\d\d\d\d",data3[1],perl=TRUE,value=TRUE)
>    
>   Any hints?
>    
>   Shawn Way
> 
>  
> ---------------------------------
> Sucker-punch spam with award-winning protection.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From shawnwaypublic at yahoo.com  Fri Mar  9 21:48:07 2007
From: shawnwaypublic at yahoo.com (Shawn Way)
Date: Fri, 9 Mar 2007 12:48:07 -0800 (PST)
Subject: [R] Extracting text from a character string
Message-ID: <464893.93130.qm@web37108.mail.mud.yahoo.com>

I have a set of character strings like below:
 
> data3[1]
[1] "CB01_0171_03-27-2002-(Sample 26609)-(126)"
> 
 
I am trying to extract the text 03-27-2002 and convert
this into a date for the same record.  I keep looking
at the grep function, however I cannot quite get it to
work.
 
grep("\d\d-\d\d-\d\d\d\d",data3[1],perl=TRUE,value=TRUE)
 
Any hints?
 
Shawn Way



 
____________________________________________________________________________________
We won't tell. Get more on shows you hate to love


From ggrothendieck at gmail.com  Fri Mar  9 21:54:07 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 9 Mar 2007 15:54:07 -0500
Subject: [R] Extracting text from a character string
In-Reply-To: <505602.83984.qm@web37113.mail.mud.yahoo.com>
References: <505602.83984.qm@web37113.mail.mud.yahoo.com>
Message-ID: <971536df0703091254k559fb4d6y5a2f9fab414bd766@mail.gmail.com>

Try this:

library(gsubfn)
x <- "CB01_0171_03-27-2002-(Sample 26609)-(126)"
unlist(strapply(x, "..-..-...."))

The gsubfn home page is at:
http://code.google.com/p/gsubfn/

On 3/9/07, Shawn Way <shawnwaypublic at yahoo.com> wrote:
> I have a set of character strings like below:
>
>  > data3[1]
> [1] "CB01_0171_03-27-2002-(Sample 26609)-(126)"
> >
>
>  I am trying to extract the text 03-27-2002 and convert this into a date for the same record.  I keep looking at the grep function, however I cannot quite get it to work.
>
>  grep("\d\d-\d\d-\d\d\d\d",data3[1],perl=TRUE,value=TRUE)
>
>  Any hints?
>
>  Shawn Way
>
>
> ---------------------------------
> Sucker-punch spam with award-winning protection.
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From liuwensui at gmail.com  Fri Mar  9 22:33:58 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Fri, 9 Mar 2007 16:33:58 -0500
Subject: [R] Extracting text from a character string
In-Reply-To: <505602.83984.qm@web37113.mail.mud.yahoo.com>
References: <505602.83984.qm@web37113.mail.mud.yahoo.com>
Message-ID: <1115a2b00703091333pc67232bt5fc1026ba93ce474@mail.gmail.com>

actually, I am thinking of strsplit().

On 3/9/07, Shawn Way <shawnwaypublic at yahoo.com> wrote:
> I have a set of character strings like below:
>
>   > data3[1]
> [1] "CB01_0171_03-27-2002-(Sample 26609)-(126)"
> >
>
>   I am trying to extract the text 03-27-2002 and convert this into a date for the same record.  I keep looking at the grep function, however I cannot quite get it to work.
>
>   grep("\d\d-\d\d-\d\d\d\d",data3[1],perl=TRUE,value=TRUE)
>
>   Any hints?
>
>   Shawn Way
>
>
> ---------------------------------
> Sucker-punch spam with award-winning protection.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From lqecli at yahoo.fr  Fri Mar  9 22:54:07 2007
From: lqecli at yahoo.fr (Luca Quaglia)
Date: Fri, 9 Mar 2007 22:54:07 +0100 (CET)
Subject: [R] About "cex=": how to improve resolution?
Message-ID: <20070309215408.87893.qmail@web27615.mail.ukl.yahoo.com>

Hi,

I need to plot a graph with a circle of radius 1 and
with a series of points of different size. The size of
these points compared to the fixed circle is important
and bears a meaning.

Here is the a simplified version of the code I'm
using:

x<-seq(0,2,by=0.2)
y<-x
z<-seq(0,1,by=0.1)
angle<-pi/180*c(0:359)
par(pty="s")
plot(-2:2,-2:2,type="n")
lines(cos(angle),sin(angle))
points(x,y,cex=z)

I obtain points of the same size when
cex=0.1/0.2/0.3/0.4 or cex=0.5/0.6/0.7 or
cex=0.8/0.9/1.0.

Please, does anyone know if there is a way of
improving the resolution of cex in order to have 10
points *all* of different size (respecting the above
written different values of cex)? The circle is fixed
of radius 1 and the values of cex are in relation with
that and they shouldn't be modified.

Thanks, Luca


From bunny at lautloscrew.com  Fri Mar  9 23:02:47 2007
From: bunny at lautloscrew.com (bunny , lautloscrew.com)
Date: Fri, 9 Mar 2007 23:02:47 +0100
Subject: [R] dendrogram - got it , just need to label :)
Message-ID: <1F1AB0D7-4CF0-495F-9874-842A3632F076@lautloscrew.com>

Hi all, Hi Gavin,

thx for your help i finally found out what i want to do and how to  
fix it.
just needed to get some more level my cut level was too small...

two question remain...

a) can i somehow scale the twigs after cutting ?
b) how can i label the nodes and how to label which one...

thx !!

-m.


From elw at stderr.org  Sat Mar 10 01:04:12 2007
From: elw at stderr.org (elw at stderr.org)
Date: Fri, 9 Mar 2007 18:04:12 -0600 (CST)
Subject: [R] Reformulated matrices dimensions limitation problem
In-Reply-To: <732eec150703091019y1783490eg17e103cf734143b1@mail.gmail.com>
References: <JEA3B6$D049EC29F5E065770DE226C0B47F6466@libero.it>
	<45E8411E.30416.1A15F2B@localhost>
	<1221313244-1172860468-cardhu_blackberry.rim.net-1509046322-@bwe014-cell00.bisx.prod.on.blackberry>
	<732eec150703091019y1783490eg17e103cf734143b1@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0703091803250.31897@illuminati.stderr.org>



> Have a look at the help page for memory.size and memory.limit. The help 
> says you can use these functions on Windows and another approach with 
> Unix. Once you know the available memory you can calculate the total 
> matrix size that fits in it (knowing that a real number takes 8 bytes). 
> I would recommend using up to 70-80% of the available memory for your 
> matrix.


But then there's the overhead for each R object, which is very 
non-trivial (but not so bad as to be completely depressing...).

--e


From rvaradhan at jhmi.edu  Fri Mar  9 23:12:35 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Fri, 9 Mar 2007 17:12:35 -0500
Subject: [R] MCMC logit
In-Reply-To: <119561.83667.qm@web34102.mail.mud.yahoo.com>
References: <119561.83667.qm@web34102.mail.mud.yahoo.com>
Message-ID: <000a01c76298$0ab14810$7c94100a@win.ad.jhu.edu>

As the error message clearly indicates, the function MCMClogit is unable to
find the variable x1 (possibly x2,x3, and x4 also) in the data frame c.df.
Check the names of the variables in that data frame and make sure that the
names correspond to the formula specification.

Hope this helps,
Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anamika Chaudhuri
Sent: Friday, March 09, 2007 3:27 PM
To: r-help at stat.math.ethz.ch
Subject: [R] MCMC logit

Hi, 
I have a dataset with the binary outcome Y(0,1) and 4 covariates
(X1,X@,X#,X$). I am trying to use MCMClogit to model logistic regression
using MCMC. I am getting an error where it doesnt identify the covariates
,although its reading in correctly. The dataset is a sample of actual
dataset. Below is my code:
> #######################
> 
> 
> #retreive data
> # considering four covariates
>
d.df=as.data.frame(read.table("c:/tina/phd/thesis/data/modified_data1.1.txt"
,header=T,sep=","))
> y=d.df[,ncol(d.df)]
> x=d.df[,1:4]
> c.df=cbind(y,x)
> #x=cbind(1,x)
> p <- ncol(c.df)
> 
> # marginal log-prior of beta[]
> logpriorfun <- function(beta, mu, gshape, grate)
+ {
+ logprior = -p*log(2) + log(gamma(p+gshape)) - log(gamma(gshape))
+ + gshape*log(grate) - (p+gshape)* log(grate+sum(abs(beta)))
+ return(logprior)
+ }
> require(MCMCpack)
Loading required package: MCMCpack
Loading required package: coda
Loading required package: lattice
Loading required package: MASS
##
## Markov Chain Monte Carlo Package (MCMCpack)
## Copyright (C) 2003-2007 Andrew D. Martin and Kevin M. Quinn
##
## Support provided by the U.S. National Science Foundation
## (Grants SES-0350646 and SES-0350613)
##
[1] TRUE
Warning message:
package 'MASS' was built under R version 2.4.1 
> a0 = 0.5
> b0 = 1
> mu0 = 0
> beta.init=list(c(0, rep(0.1,4)), c(0, rep(-0.1,4)), c(0, rep(0, 4)))
> burnin.cycles = 1000
> mcmc.cycles = 25000
> # three chains
> post.list <- lapply(beta.init, function(vec)
+ {
+ posterior <- MCMClogit(y~x1+x2+x3+x4, data=c.df, burnin=burnin.cycles,
mcmc=mcmc.cycles,
+ thin=5, tune=0.5, beta.start=vec, user.prior.density=logpriorfun,
logfun=TRUE,
+ mu=mu0, gshape=a0, grate=b0)
+ return(posterior)
+ })
Error in eval(expr, envir, enclos) : object "x1" not found
> 
  Any suggestions will be greatly appreciated.
  Thanks,
Anamika

 
---------------------------------
We won't tell. Get more on shows you hate to love

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From rmh at temple.edu  Fri Mar  9 23:18:32 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri,  9 Mar 2007 17:18:32 -0500 (EST)
Subject: [R] About "cex=": how to improve resolution?
Message-ID: <20070309171832.BWH14734@po-d.temple.edu>

replace
points(x,y,cex=z)
with
symbols(x, y, circles=z/10, inches=FALSE, add=TRUE)


From maciej.rhelp at gmail.com  Sat Mar 10 00:47:01 2007
From: maciej.rhelp at gmail.com (Maciej Radziejewski)
Date: Sat, 10 Mar 2007 00:47:01 +0100
Subject: [R] Using large datasets: can I overload the subscript operator?
Message-ID: <732eec150703091547j3f0a8a89udf2015e89796653c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070310/a2fbb38d/attachment.pl 

From kubovy at virginia.edu  Sat Mar 10 01:22:28 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Fri, 9 Mar 2007 19:22:28 -0500
Subject: [R] Extracting the p of F statistics from lm
In-Reply-To: <B0F504209244B14EA9A4C1DFB599B9224FFE35@NIHCESMLBX6.nih.gov>
References: <B0F504209244B14EA9A4C1DFB599B9224FFE35@NIHCESMLBX6.nih.gov>
Message-ID: <20F33776-F489-4238-8C06-417CB42B1B86@virginia.edu>

On Mar 9, 2007, at 11:18 AM, Cressoni, Massimo ((NIH/NHLBI)) [F] wrote:

> I need to extract the p value from a ANOVA done with lm model
>
> fitting <- lm(var ~ group)
> Sfitting <- summary(fitting)
>
> Sfitting[10][1] gives the F value and the degrees of freedom but I  
> am not able to get the
> p value.

try
Sfitting[4]$coefficients[,4]

I'm not sure that this is the best way, but it works with the example  
for lm()
 > summary(lm.D9)[4]$coefficients[,4]
# (Intercept)     groupTrt
# 9.547128e-15 2.490232e-01

_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From aarppe at ling.helsinki.fi  Sat Mar 10 02:00:46 2007
From: aarppe at ling.helsinki.fi (Antti Arppe)
Date: Sat, 10 Mar 2007 03:00:46 +0200 (EET)
Subject: [R] H0 and H1 probabilities in Cohen's Effect Size w for X2 test
In-Reply-To: <mailman.11.1173438003.25210.r-help@stat.math.ethz.ch>
References: <mailman.11.1173438003.25210.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.62.0703100225300.21760@venus.ling.helsinki.fi>

Dear all,

I've been delighted to just notice that Cohen's formulas for 
Effect Size 'w' and the associated power have been implemented in 
the 'pwr' package (thanks to St?phane Champely and others)..

There is one aspect, though, that perplexes me. I'm doing some last 
minute post hoc analyses, meaning that my sample size (N=3404) has 
been long fixed, and I'm interested in assessing the ES and Power 
after the fact..

As far as I can deduce from the implementation of the ES.w2 formula or 
Cohen's (1992) own article, it seems to me that the probabilities 
p(H0) and p(H1) would simply be the expected and observed absolute 
frequencies divided by the sample size N, in that the 'true' 
probablities are the observed proportions and the null probabilities 
the expected ones. If this is correct, then the effect size and the 
power statistics can naturally easily be calculated with the 'pwr' 
package. However, this entails that the noncentrality parameter 
lambda=N*w^2 is equal to the chi-squared statistic X^2.

> observed
     p   h   m    a
X 119  64  36   37
Y 594 323 776 1455

> expected
           p         h         m         a
X  53.62162  29.10458  61.06698  112.2068
Y 659.37838 357.89542 750.93302 1379.7932

> observed.p
            p          h          m          a
X 0.03495887 0.01880141 0.01057579 0.01086957
Y 0.17450059 0.09488837 0.22796710 0.42743831

> expected.p
            p           h          m          a
X 0.01575253 0.008550112 0.01793977 0.03296322
Y 0.19370693 0.105139664 0.22060312 0.40534465

> ES.w2(observed.p)
[1] 0.2406104

> ES.w1(expected.p,observed.p)
[1] 0.2406104

> pwr.chisq.test(w=ES.w1(expected.p,observed.p),N=3404,sig.level=.05, 
df=3)
      Chi squared power calculation

               w = 0.2406104
               N = 3404
              df = 3
       sig.level = 0.05
           power = 1

  NOTE: N is the number of observations

> lambda <- 3404*ES.w1(observed.p,expected.p)^2

> lambda
[1] 240.9289

> pchisq(qchisq(p=.05,df=3,lower.tail=F),ncp=lambda,df=3,lower=F)
[1] 1

Have I missed or misunderstood something here altogether? Should the 
alternative H0 probabilities be estimated by e.g. some sort of 
fitting? Any pointers, suggestions or assistance would be greatly 
appreciated.

 	-Antti Arppe
-- 
======================================================================
Antti Arppe - Master of Science (Engineering)
Researcher & doctoral student (Linguistics)
E-mail: antti.arppe at helsinki.fi
WWW: http://www.ling.helsinki.fi/~aarppe
----------------------------------------------------------------------
Work: Department of General Linguistics, University of Helsinki
Work address: P.O. Box 9 (Siltavuorenpenger 20 A)
    00014 University of Helsinki, Finland
Work telephone: +358 9 19129312 (int'l) 09-19129312 (in Finland)
Work telefax: +358 9 19129307 (int'l) 09-19129307 (in Finland)
----------------------------------------------------------------------
Private address: Fleminginkatu 25 E 91, 00500 Helsinki, Finland
Private telephone: +358 50 5909015 (int'l) 050-5909015 (in Finland)
----------------------------------------------------------------------

From sdimhoff at wisc.edu  Sat Mar 10 03:23:21 2007
From: sdimhoff at wisc.edu (Seth Imhoff)
Date: Fri, 09 Mar 2007 20:23:21 -0600
Subject: [R] Table Construction from calculations
Message-ID: <45F21699.10805@wisc.edu>

Hi-

I am trying to create a table of values by adding  pairs of vectors, but 
am running into some problems.  The problem is best expressed by a 
simple example.

Starting with a data table "basis":
  atom   x   y   z
1   Cu 0.0 0.0 0.0
2   Cu 0.5 0.5 0.5

I want to add 0.5 0.5 0.5  (and also the 0 0 0 but it wouldn't change 
the values below so I won't refer to it in the rest of the example) to a 
list of vectors in the form of:
 > latpoints
   V1 V2 V3
1   0  0  0
2   0  0  1
3   0  0  2
4   0  0  3
5   0  1  1

so that I end up with a table such as:
V1  V2  V3
0.5 0.5 0.5
0.5 0.5 1.5
0.5 0.5 2.5
0.5 0.5 3.5
0.5 1.5 1.5

I've tried many variations on the following: (not just cat, but most of 
the data/data.table options)

 test = for(i in 1:5) {cat(basis[1,2:4] + latticemultipliers[i,], 
append=TRUE)}

However, I either end up with an error telling me that cat doesn't 
handle "type 'list' " or with a table with length of 1 such as:
     x    y    z
2  0.5 1.5 1.5

Which is simply the last value that the loop calculates.

Does anyone know what function handles lists of the form I am using, or 
have a better suggestion on how to get the form that I want.

Thanks in advance,
Seth Imhoff


From christos at nuverabio.com  Sat Mar 10 03:41:10 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Fri, 9 Mar 2007 21:41:10 -0500
Subject: [R] Table Construction from calculations
In-Reply-To: <45F21699.10805@wisc.edu>
References: <45F21699.10805@wisc.edu>
Message-ID: <000c01c762bd$908ed810$0202a8c0@headquarters.silicoinsights>

Your "data table" basis is actually a dataframe, whose first column is
non-numeric.  That's what is causing the problem.
 
Try removing the first column of the dataframe before adding the row to your
matrix:

test <- latpoints + basis[2, -1]

-Christos

Christos Hatzis, Ph.D.
Nuvera Biosciences, Inc.
400 West Cummings Park
Suite 5350
Woburn, MA 01801
Tel: 781-938-3830
www.nuverabio.com

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Seth Imhoff
> Sent: Friday, March 09, 2007 9:23 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Table Construction from calculations
> 
> Hi-
> 
> I am trying to create a table of values by adding  pairs of 
> vectors, but am running into some problems.  The problem is 
> best expressed by a simple example.
> 
> Starting with a data table "basis":
>   atom   x   y   z
> 1   Cu 0.0 0.0 0.0
> 2   Cu 0.5 0.5 0.5
> 
> I want to add 0.5 0.5 0.5  (and also the 0 0 0 but it 
> wouldn't change the values below so I won't refer to it in 
> the rest of the example) to a list of vectors in the form of:
>  > latpoints
>    V1 V2 V3
> 1   0  0  0
> 2   0  0  1
> 3   0  0  2
> 4   0  0  3
> 5   0  1  1
> 
> so that I end up with a table such as:
> V1  V2  V3
> 0.5 0.5 0.5
> 0.5 0.5 1.5
> 0.5 0.5 2.5
> 0.5 0.5 3.5
> 0.5 1.5 1.5
> 
> I've tried many variations on the following: (not just cat, 
> but most of the data/data.table options)
> 
>  test = for(i in 1:5) {cat(basis[1,2:4] + 
> latticemultipliers[i,], append=TRUE)}
> 
> However, I either end up with an error telling me that cat 
> doesn't handle "type 'list' " or with a table with length of 
> 1 such as:
>      x    y    z
> 2  0.5 1.5 1.5
> 
> Which is simply the last value that the loop calculates.
> 
> Does anyone know what function handles lists of the form I am 
> using, or have a better suggestion on how to get the form that I want.
> 
> Thanks in advance,
> Seth Imhoff
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From murdoch at stats.uwo.ca  Sat Mar 10 03:54:16 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 09 Mar 2007 21:54:16 -0500
Subject: [R] Using large datasets: can I overload the subscript operator?
In-Reply-To: <732eec150703091547j3f0a8a89udf2015e89796653c@mail.gmail.com>
References: <732eec150703091547j3f0a8a89udf2015e89796653c@mail.gmail.com>
Message-ID: <45F21DD8.8030102@stats.uwo.ca>

On 3/9/2007 6:47 PM, Maciej Radziejewski wrote:
> Hello,
> 
> I do some computations on datasets that come from climate models. These data
> are huge arrays, significantly larger than typically available RAM, so they
> have to be accessed row-by-row, or rather slice-by slice, depending on the
> task. I would like to make an R package to easily access such datasets
> within R. The C++ backend is ready and being used under Windows/.Net/Visual
> Basic, but I have yet to learn the specifics of R programming to make a good
> R interface.
> 
> I think it should be possible to make a package (call it "slice") that could
> be used like this:
> 
> library (slice)
> dataset <- load.virtualarray ("dataset_definition.xml")
> ordinaryvector <- dataset [ , 2, 3] # Load a portion of the data from disk
> and extract it
> 
> In the above "dataset" is an object that holds a definition of a
> 3-dimensional large dataset, and "ordinaryvector" is an ordinary R vector.
> The subscripting operator fetches necessary data from disk and extracts a
> required slice, taking care of caching and other technical details. So, my
> questions are:
> 
> Has anyone ever made a similar extension, with virtual (lazy) arrays?

Yes, e.g. the SQLiteDF package.
> 
> Can the suscript operator be overloaded like that in R? (I know it can be in
> S, at least for vectors.)

Yes.
> 
> And a tough one: is it possible to make an expression like "[1]" (without
> quoutes) meaningful in R? At the moment it results in a syntax error. I
> would like to make it return an object of a special class that gets
> interpreted when subscripting my virtual array as "drop this dimension",
> like this:
> 
> dataset [, 2, 3, drop = F]  # Return a 3-dimensional array
> dataset [, [2], 3, drop = F]  # Return a 2-dimensional array
> dataset [, [2], [3], drop = F]  # Return a 1-dimensional array, like dataset
> [, 2, 3]

No, that's not legal S or R syntax.  However, you might be able to 
define a special object D and use syntax like

dataset [, D[2], 3, drop = F]

Duncan Murdoch
> 
> Thanks in advance for any help,
> 
> Maciej.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From kubovy at virginia.edu  Sat Mar 10 03:58:30 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Fri, 9 Mar 2007 21:58:30 -0500
Subject: [R] Table Construction from calculations
In-Reply-To: <45F21699.10805@wisc.edu>
References: <45F21699.10805@wisc.edu>
Message-ID: <04D36C87-F9D4-40BB-AE6A-EB3CD5E13342@virginia.edu>

On Mar 9, 2007, at 9:23 PM, Seth Imhoff wrote:

> I am trying to create a table of values by adding  pairs of  
> vectors, but
> am running into some problems.  The problem is best expressed by a
> simple example.
>
> Starting with a data table "basis":
>   atom   x   y   z
> 1   Cu 0.0 0.0 0.0
> 2   Cu 0.5 0.5 0.5
>
> I want to add 0.5 0.5 0.5  (and also the 0 0 0 but it wouldn't change
> the values below so I won't refer to it in the rest of the example)  
> to a
> list of vectors in the form of:
>> latpoints
>    V1 V2 V3
> 1   0  0  0
> 2   0  0  1
> 3   0  0  2
> 4   0  0  3
> 5   0  1  1
>
> so that I end up with a table such as:
> V1  V2  V3
> 0.5 0.5 0.5
> 0.5 0.5 1.5
> 0.5 0.5 2.5
> 0.5 0.5 3.5
> 0.5 1.5 1.5
>
> I've tried many variations on the following: (not just cat, but  
> most of
> the data/data.table options)
>
>  test = for(i in 1:5) {cat(basis[1,2:4] + latticemultipliers[i,],
> append=TRUE)}
>
> However, I either end up with an error telling me that cat doesn't
> handle "type 'list' " or with a table with length of 1 such as:
>      x    y    z
> 2  0.5 1.5 1.5

Is this what you want?

 > (latpoints <- data.frame(matrix(c(0,0,0,0,0,1,0,0,2,0,0,3,0,1,1),  
nrow = 5, byrow = T)))
   X1 X2 X3
1  0  0  0
2  0  0  1
3  0  0  2
4  0  0  3
5  0  1  1
 > (latpoints <- latpoints + c(0.5, 0.5, 0.5))
    X1  X2  X3
1 0.5 0.5 0.5
2 0.5 0.5 1.5
3 0.5 0.5 2.5
4 0.5 0.5 3.5
5 0.5 1.5 1.5

This is an important feature of R called "vectorization" (see, .e.g,  
cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf or  
www.ms.washington.edu/stat390/winter07/R_primer.pdf) which allows you  
do avoid writing loops.
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From Roy.Mendelssohn at noaa.gov  Sat Mar 10 04:01:11 2007
From: Roy.Mendelssohn at noaa.gov (Roy Mendelssohn)
Date: Fri, 09 Mar 2007 19:01:11 -0800
Subject: [R] Using large datasets: can I overload the subscript operator?
In-Reply-To: <45F21DD8.8030102@stats.uwo.ca>
References: <732eec150703091547j3f0a8a89udf2015e89796653c@mail.gmail.com>
	<45F21DD8.8030102@stats.uwo.ca>
Message-ID: <135E337D-7617-4E52-BFAC-D9F50236D033@noaa.gov>

Look at the netcdf packages.  A lot of output from climate models is  
in netcdf anyway.  It can take all sorts of slices and strides.

-Roy M.


On Mar 9, 2007, at 6:54 PM, Duncan Murdoch wrote:

> On 3/9/2007 6:47 PM, Maciej Radziejewski wrote:
>> Hello,
>>
>> I do some computations on datasets that come from climate models.  
>> These data
>> are huge arrays, significantly larger than typically available  
>> RAM, so they
>> have to be accessed row-by-row, or rather slice-by slice,  
>> depending on the
>> task. I would like to make an R package to easily access such  
>> datasets
>> within R. The C++ backend is ready and being used under  
>> Windows/.Net/Visual
>> Basic, but I have yet to learn the specifics of R programming to  
>> make a good
>> R interface.
>>
>> I think it should be possible to make a package (call it "slice")  
>> that could
>> be used like this:
>>
>> library (slice)
>> dataset <- load.virtualarray ("dataset_definition.xml")
>> ordinaryvector <- dataset [ , 2, 3] # Load a portion of the data  
>> from disk
>> and extract it
>>
>> In the above "dataset" is an object that holds a definition of a
>> 3-dimensional large dataset, and "ordinaryvector" is an ordinary R  
>> vector.
>> The subscripting operator fetches necessary data from disk and  
>> extracts a
>> required slice, taking care of caching and other technical  
>> details. So, my
>> questions are:
>>
>> Has anyone ever made a similar extension, with virtual (lazy) arrays?
>
> Yes, e.g. the SQLiteDF package.
>>
>> Can the suscript operator be overloaded like that in R? (I know it  
>> can be in
>> S, at least for vectors.)
>
> Yes.
>>
>> And a tough one: is it possible to make an expression like  
>> "[1]" (without
>> quoutes) meaningful in R? At the moment it results in a syntax  
>> error. I
>> would like to make it return an object of a special class that gets
>> interpreted when subscripting my virtual array as "drop this  
>> dimension",
>> like this:
>>
>> dataset [, 2, 3, drop = F]  # Return a 3-dimensional array
>> dataset [, [2], 3, drop = F]  # Return a 2-dimensional array
>> dataset [, [2], [3], drop = F]  # Return a 1-dimensional array,  
>> like dataset
>> [, 2, 3]
>
> No, that's not legal S or R syntax.  However, you might be able to
> define a special object D and use syntax like
>
> dataset [, D[2], 3, drop = F]
>
> Duncan Murdoch
>>
>> Thanks in advance for any help,
>>
>> Maciej.
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

**********************
"The contents of this message do not reflect any position of the U.S.  
Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division	
Southwest Fisheries Science Center
1352 Lighthouse Avenue
Pacific Grove, CA 93950-2097

e-mail: Roy.Mendelssohn at noaa.gov (Note new e-mail address)
voice: (831)-648-9029
fax: (831)-648-8440
www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."


From toby909 at gmail.com  Sat Mar 10 04:20:45 2007
From: toby909 at gmail.com (toby909 at gmail.com)
Date: Fri, 09 Mar 2007 19:20:45 -0800
Subject: [R] long character string problem
Message-ID: <est85s$5dq$1@sea.gmane.org>

Hi All

I am having 2 very long character strings (550chars) and I want to put them as 
expressions together with c(). The problem is that I also get these 
double-quotes, as seen below in 'fct'. How can I remove these double-quotes? I 
tried as.name() but it did not work (because of size?). These are creating 
trouble with subsequent programs, which I tested with strings that for some 
reason do not have these double quotes (see very bottom).





 > cum1
[1] 
"A11*(X11*x1+X21*x2)+1*sqrt(B11*(X11*x1+X21*x2)^2+C11)A12*(X12*x1+X22*x2)+1*sqrt(B12*(X12*x1+X22*x2)^2+C12)A13*(X13*x1+X23*x2)+-1*sqrt(B13*(X13*x1+X23*x2)^2+C13)A14*(X14*x1+X24*x2)+-1*sqrt(B14*(X14*x1+X24*x2)^2+C14)A15*(X15*x1+X25*x2)+1*sqrt(B15*(X15*x1+X25*x2)^2+C15)A16*(X16*x1+X26*x2)+1*sqrt(B16*(X16*x1+X26*x2)^2+C16)A17*(X17*x1+X27*x2)+1*sqrt(B17*(X17*x1+X27*x2)^2+C17)A18*(X18*x1+X28*x2)+1*sqrt(B18*(X18*x1+X28*x2)^2+C18)A19*(X19*x1+X29*x2)+-1*sqrt(B19*(X19*x1+X29*x2)^2+C19)A110*(X110*x1+X210*x2)+1*sqrt(B110*(X110*x1+X210*x2)^2+C110)"
 > cum2
[1] 
"A21*(X11*x1+X21*x2)+1*sqrt(B21*(X11*x1+X21*x2)^2+C21)A22*(X12*x1+X22*x2)+1*sqrt(B22*(X12*x1+X22*x2)^2+C22)A23*(X13*x1+X23*x2)+-1*sqrt(B23*(X13*x1+X23*x2)^2+C23)A24*(X14*x1+X24*x2)+-1*sqrt(B24*(X14*x1+X24*x2)^2+C24)A25*(X15*x1+X25*x2)+1*sqrt(B25*(X15*x1+X25*x2)^2+C25)A26*(X16*x1+X26*x2)+1*sqrt(B26*(X16*x1+X26*x2)^2+C26)A27*(X17*x1+X27*x2)+1*sqrt(B27*(X17*x1+X27*x2)^2+C27)A28*(X18*x1+X28*x2)+1*sqrt(B28*(X18*x1+X28*x2)^2+C28)A29*(X19*x1+X29*x2)+-1*sqrt(B29*(X19*x1+X29*x2)^2+C29)A210*(X110*x1+X210*x2)+1*sqrt(B210*(X110*x1+X210*x2)^2+C210)"
 > fct = c(as.expression(cum1), as.expression(cum2))
 > fct
expression("A11*(X11*x1+X21*x2)+1*sqrt(B11*(X11*x1+X21*x2)^2+C11)A12*(X12*x1+X22*x2)+1*sqrt(B12*(X12*x1+X22*x2)^2+C12)A13*(X13*x1+X23*x2)+-1*sqrt(B13*(X13*x1+X23*x2)^2+C13)A14*(X14*x1+X24*x2)+-1*sqrt(B14*(X14*x1+X24*x2)^2+C14)A15*(X15*x1+X25*x2)+1*sqrt(B15*(X15*x1+X25*x2)^2+C15)A16*(X16*x1+X26*x2)+1*sqrt(B16*(X16*x1+X26*x2)^2+C16)A17*(X17*x1+X27*x2)+1*sqrt(B17*(X17*x1+X27*x2)^2+C17)A18*(X18*x1+X28*x2)+1*sqrt(B18*(X18*x1+X28*x2)^2+C18)A19*(X19*x1+X29*x2)+-1*sqrt(B19*(X19*x1+X29*x2)^2+C19)A110*(X110*x1+X210*x2)+1*sqrt(B110*(X110*x1+X210*x2)^2+C110)", 

 
"A21*(X11*x1+X21*x2)+1*sqrt(B21*(X11*x1+X21*x2)^2+C21)A22*(X12*x1+X22*x2)+1*sqrt(B22*(X12*x1+X22*x2)^2+C22)A23*(X13*x1+X23*x2)+-1*sqrt(B23*(X13*x1+X23*x2)^2+C23)A24*(X14*x1+X24*x2)+-1*sqrt(B24*(X14*x1+X24*x2)^2+C24)A25*(X15*x1+X25*x2)+1*sqrt(B25*(X15*x1+X25*x2)^2+C25)A26*(X16*x1+X26*x2)+1*sqrt(B26*(X16*x1+X26*x2)^2+C26)A27*(X17*x1+X27*x2)+1*sqrt(B27*(X17*x1+X27*x2)^2+C27)A28*(X18*x1+X28*x2)+1*sqrt(B28*(X18*x1+X28*x2)^2+C28)A29*(X19*x1+X29*x2)+-1*sqrt(B29*(X19*x1+X29*x2)^2+C29)A210*(X110*x1+X210*x2)+1*sqrt(B210*(X110*x1+X210*x2)^2+C210)")
 >








 > fct = c(expression(2*x1^3-7*x2^2-9), expression(x1^2-x2^3+1))
 > fct
expression(2 * x1^3 - 7 * x2^2 - 9, x1^2 - x2^3 + 1)


From paul.bailey at alumni.grinnell.edu  Sat Mar 10 04:22:51 2007
From: paul.bailey at alumni.grinnell.edu (Paul Bailey)
Date: Fri,  9 Mar 2007 21:22:51 -0600 (CST)
Subject: [R] understanding print.summary.lm and perhaps print/show in
 general
Message-ID: <20070309212251.AKL52067@m4500-03.uchicago.edu>

Petr,

Thanks, you set me on the right path. It turns out that the behavior that 
surprises me is this: when you change $coef it isn't the same as changing 
$coefficients. The first changes the value, but the second changes the value and 
makes the print/show metod change its output from when the summary.lm was 
created. I think the sample code below highlights this behavior nicely.

Why would you want this behavior?

Cheers,
Paul

#### R code:
lma <-  lm(dist ~ speed, data=cars) 
suma <- summary(lma)
colnames(suma$coef) <- c(LETTERS[1:4]) 
dimnames(suma$coef) # after setting colnames, dimnames of coefficients 
variable is set
suma # but printing is still the old print
dimnames(suma$coefficients) <- list(names(suma$coefficients), c(LETTERS[1:4])) 
dimnames(suma$coef) # no change in dimnames from before
suma # but the summary output is now refreshed!
######

>
>Another solution is to look into the code of summary.lm a few lines 
>above where the (dim)names are assigned. Based on this, you may try
>
>lma <- lm(dist ~ speed, data=cars)
>suma <- summary(lma)
>colnames(suma$coef) <- c(LETTERS[1:4])
>printCoefmat(suma$coef) # prints what I expect
>suma
>
>dimnames(suma$coefficients) <- list(names(suma$coefficients), 
>c(LETTERS[1:4]))
>suma
>
>You might also find reading the chapter on generic functions in the 
>R-lang (R language definition) manual useful.
>Petr
>


From smckinney at bccrc.ca  Sat Mar 10 04:33:54 2007
From: smckinney at bccrc.ca (Steven McKinney)
Date: Fri, 9 Mar 2007 19:33:54 -0800
Subject: [R] dendrogram - got it , just need to label :)
References: <1F1AB0D7-4CF0-495F-9874-842A3632F076@lautloscrew.com>
Message-ID: <0BE438149FF2254DB4199E2682C8DFEB0235FB45@crcmail1.BCCRC.CA>


Here is one example of labeling nodes,
borrowing code from the help page for
the dendrapply() function.

local({
  edgeLab <<- function(n) {
      if(!is.leaf(n)) {
        a <- attributes(n)
        i <<- i+1
        attr(n, "edgetext") <-
            format(i)
      }
      n
  }
  i <- 0
 })
dL <- dendrapply(as.dendrogram(hclust(dist(iris[, 1:4]), method = "single")), edgeLab)
plot(dL)


This labels the edges above the nodes.

Martin Maechler and Robert Gentleman are developing the
dendrogram objects suite of functions.
As I have had to label nodes in S-PLUS, I'd like to put
in a request for a few more control parameters for
edge/internal node labeling control:

 - Allow the label without the polygon surrounding it.
   The polygon can obliterate too much of the dendrogram
   for larger sample sizes.  Perhaps an edgePar polygon
   plot logical p.plot taking values TRUE (default) and
   FALSE to omit the polygon.
 - Allow the label to appear near the node at the base
   of the edge.  Perhaps an edgePar text location parameter
   t.pos taking values in (0.0, 1.0) where 0.5 is in the
   middle of the edge (the default) and 1.0 is at the base 
   of the edge.

Since clusters are not always identified by 'cutting'
the dendrogram (e.g. in the iris single linkage
dendrogram plot we want to identify internal nodes that
are large runts or whose vertical edge lengths are
considerably longer than average) it is useful to be able
to identify nodes deeper in the tree.  This is aided
by having access to internal node labels/names and being
able to extract internal nodes by those labels/names.


Best

Steven McKinney

Statistician
Molecular Oncology and Breast Cancer Program
British Columbia Cancer Research Centre

email: smckinney at bccrc.ca

tel: 604-675-8000 x7561

BCCRC
Molecular Oncology
675 West 10th Ave, Floor 4
Vancouver B.C. 
V5Z 1L3
Canada




-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch on behalf of bunny , lautloscrew.com
Sent: Fri 3/9/2007 2:02 PM
To: R-help at stat.math.ethz.ch
Subject: [R] dendrogram - got it , just need to label :)
 
Hi all, Hi Gavin,

thx for your help i finally found out what i want to do and how to  
fix it.
just needed to get some more level my cut level was too small...

two question remain...

a) can i somehow scale the twigs after cutting ?
b) how can i label the nodes and how to label which one...

thx !!

-m.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From brown_emu at yahoo.com  Sat Mar 10 05:32:27 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Fri, 9 Mar 2007 20:32:27 -0800 (PST)
Subject: [R] long character string problem
In-Reply-To: <est85s$5dq$1@sea.gmane.org>
Message-ID: <615144.51515.qm@web39702.mail.mud.yahoo.com>

I think you are looking for 
fct <- c(parse(text=cum1),parse(text=cum2))

although you need to include operators before your A coefficients (for
example,

...C11)A12*...




--- toby909 at gmail.com wrote:

> Hi All
> 
> I am having 2 very long character strings (550chars) and I want to put them
> as 
> expressions together with c(). The problem is that I also get these 
> double-quotes, as seen below in 'fct'. How can I remove these
> double-quotes? I 
> tried as.name() but it did not work (because of size?). These are creating 
> trouble with subsequent programs, which I tested with strings that for some
> 
> reason do not have these double quotes (see very bottom).
> 
> 
> 
> 
> 
>  > cum1
> [1] 
>
"A11*(X11*x1+X21*x2)+1*sqrt(B11*(X11*x1+X21*x2)^2+C11)A12*(X12*x1+X22*x2)+1*sqrt(B12*(X12*x1+X22*x2)^2+C12)A13*(X13*x1+X23*x2)+-1*sqrt(B13*(X13*x1+X23*x2)^2+C13)A14*(X14*x1+X24*x2)+-1*sqrt(B14*(X14*x1+X24*x2)^2+C14)A15*(X15*x1+X25*x2)+1*sqrt(B15*(X15*x1+X25*x2)^2+C15)A16*(X16*x1+X26*x2)+1*sqrt(B16*(X16*x1+X26*x2)^2+C16)A17*(X17*x1+X27*x2)+1*sqrt(B17*(X17*x1+X27*x2)^2+C17)A18*(X18*x1+X28*x2)+1*sqrt(B18*(X18*x1+X28*x2)^2+C18)A19*(X19*x1+X29*x2)+-1*sqrt(B19*(X19*x1+X29*x2)^2+C19)A110*(X110*x1+X210*x2)+1*sqrt(B110*(X110*x1+X210*x2)^2+C110)"
>  > cum2
> [1] 
>
"A21*(X11*x1+X21*x2)+1*sqrt(B21*(X11*x1+X21*x2)^2+C21)A22*(X12*x1+X22*x2)+1*sqrt(B22*(X12*x1+X22*x2)^2+C22)A23*(X13*x1+X23*x2)+-1*sqrt(B23*(X13*x1+X23*x2)^2+C23)A24*(X14*x1+X24*x2)+-1*sqrt(B24*(X14*x1+X24*x2)^2+C24)A25*(X15*x1+X25*x2)+1*sqrt(B25*(X15*x1+X25*x2)^2+C25)A26*(X16*x1+X26*x2)+1*sqrt(B26*(X16*x1+X26*x2)^2+C26)A27*(X17*x1+X27*x2)+1*sqrt(B27*(X17*x1+X27*x2)^2+C27)A28*(X18*x1+X28*x2)+1*sqrt(B28*(X18*x1+X28*x2)^2+C28)A29*(X19*x1+X29*x2)+-1*sqrt(B29*(X19*x1+X29*x2)^2+C29)A210*(X110*x1+X210*x2)+1*sqrt(B210*(X110*x1+X210*x2)^2+C210)"
>  > fct = c(as.expression(cum1), as.expression(cum2))
>  > fct
>
expression("A11*(X11*x1+X21*x2)+1*sqrt(B11*(X11*x1+X21*x2)^2+C11)A12*(X12*x1+X22*x2)+1*sqrt(B12*(X12*x1+X22*x2)^2+C12)A13*(X13*x1+X23*x2)+-1*sqrt(B13*(X13*x1+X23*x2)^2+C13)A14*(X14*x1+X24*x2)+-1*sqrt(B14*(X14*x1+X24*x2)^2+C14)A15*(X15*x1+X25*x2)+1*sqrt(B15*(X15*x1+X25*x2)^2+C15)A16*(X16*x1+X26*x2)+1*sqrt(B16*(X16*x1+X26*x2)^2+C16)A17*(X17*x1+X27*x2)+1*sqrt(B17*(X17*x1+X27*x2)^2+C17)A18*(X18*x1+X28*x2)+1*sqrt(B18*(X18*x1+X28*x2)^2+C18)A19*(X19*x1+X29*x2)+-1*sqrt(B19*(X19*x1+X29*x2)^2+C19)A110*(X110*x1+X210*x2)+1*sqrt(B110*(X110*x1+X210*x2)^2+C110)",
> 
> 
>  
>
"A21*(X11*x1+X21*x2)+1*sqrt(B21*(X11*x1+X21*x2)^2+C21)A22*(X12*x1+X22*x2)+1*sqrt(B22*(X12*x1+X22*x2)^2+C22)A23*(X13*x1+X23*x2)+-1*sqrt(B23*(X13*x1+X23*x2)^2+C23)A24*(X14*x1+X24*x2)+-1*sqrt(B24*(X14*x1+X24*x2)^2+C24)A25*(X15*x1+X25*x2)+1*sqrt(B25*(X15*x1+X25*x2)^2+C25)A26*(X16*x1+X26*x2)+1*sqrt(B26*(X16*x1+X26*x2)^2+C26)A27*(X17*x1+X27*x2)+1*sqrt(B27*(X17*x1+X27*x2)^2+C27)A28*(X18*x1+X28*x2)+1*sqrt(B28*(X18*x1+X28*x2)^2+C28)A29*(X19*x1+X29*x2)+-1*sqrt(B29*(X19*x1+X29*x2)^2+C29)A210*(X110*x1+X210*x2)+1*sqrt(B210*(X110*x1+X210*x2)^2+C210)")
>  >
> 
> 
> 
> 
> 
> 
> 
> 
>  > fct = c(expression(2*x1^3-7*x2^2-9), expression(x1^2-x2^3+1))
>  > fct
> expression(2 * x1^3 - 7 * x2^2 - 9, x1^2 - x2^3 + 1)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 




 
____________________________________________________________________________________
Need Mail bonding?


From jholtman at gmail.com  Sat Mar 10 05:38:03 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 9 Mar 2007 23:38:03 -0500
Subject: [R] long character string problem
In-Reply-To: <est85s$5dq$1@sea.gmane.org>
References: <est85s$5dq$1@sea.gmane.org>
Message-ID: <644e1f320703092038ya452754k937f63428c4058a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070309/64d597c9/attachment.pl 

From aiminy at iastate.edu  Sat Mar 10 05:49:43 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Fri, 09 Mar 2007 22:49:43 -0600
Subject: [R] use nnet
In-Reply-To: <1115a2b00703091039p3034dacdn260981e4afffa854@mail.gmail.co
 m>
References: <6.2.3.4.2.20070309120535.03e71440@aiminy.mail.iastate.edu>
	<1115a2b00703091037r6ccb65a5g6d7d889acad45875@mail.gmail.com>
	<1115a2b00703091039p3034dacdn260981e4afffa854@mail.gmail.com>
Message-ID: <6.1.2.0.2.20070309224321.01d6fb00@aiminy.mail.iastate.edu>

thank you very much.
I have a another question about nnet
if I set size=0, and skip=TRUE.
Then this network has just input layer and out layer.
Is this also called perceptron network?

thanks,

Aimin Yan


At 12:39 PM 3/9/2007, Wensui Liu wrote:
>AM,
>Sorry. please ignore the top box in the code. It is not actually a cv
>validation but just a simple split-sample validation.
>sorry for confusion.
>
>On 3/9/07, Wensui Liu <liuwensui at gmail.com> wrote:
>>AM,
>>I have a pieice of junk on my blog. Here it is.
>>#################################################
>># USE CROSS-VALIDATION TO DO A GRID-SEARCH FOR  #
>># THE OPTIMAL SETTINGS (WEIGHT DECAY AND NUMBER #
>># OF HIDDEN UNITS) OF NEURAL NETS               #
>>#################################################
>>
>>library(nnet);
>>library(MASS);
>>data(Boston);
>>X <- I(as.matrix(Boston[-14]));
>># STANDARDIZE PREDICTORS
>>st.X <- scale(X);
>>Y <- I(as.matrix(Boston[14]));
>>boston <- data.frame(X = st.X, Y);
>>
>># DIVIDE DATA INTO TESTING AND TRAINING SETS
>>set.seed(2005);
>>test.rows <- sample(1:nrow(boston), 100);
>>test.set <- boston[test.rows, ];
>>train.set <- boston[-test.rows, ];
>>
>># INITIATE A NULL TABLE
>>sse.table <- NULL;
>>
>># SEARCH FOR OPTIMAL WEIGHT DECAY
>># RANGE OF WEIGHT DECAYS SUGGESTED BY B. RIPLEY
>>for (w in c(0.0001, 0.001, 0.01))
>>{
>>   # SEARCH FOR OPTIMAL NUMBER OF HIDDEN UNITS
>>   for (n in 1:10)
>>   {
>>     # UNITIATE A NULL VECTOR
>>     sse <- NULL;
>>     # FOR EACH SETTING, RUN NEURAL NET MULTIPLE TIMES
>>     for (i in 1:10)
>>     {
>>       # INITIATE THE RANDOM STATE FOR EACH NET
>>       set.seed(i);
>>       # TRAIN NEURAL NETS
>>       net <- nnet(Y~X, size = n, data = train.set, rang = 0.00001,
>>                        linout = TRUE, maxit = 10000, decay = w,
>>                        skip = FALSE, trace = FALSE);
>>       # CALCULATE SSE FOR TESTING SET
>>       test.sse <- sum((test.set$Y - predict(net, test.set))^2);
>>       # APPEND EACH SSE TO A VECTOR
>>       if (i == 1) sse <- test.sse else sse <- rbind(sse, test.sse);
>>     }
>>     # APPEND AVERAGED SSE WITH RELATED PARAMETERS TO A TABLE
>>     sse.table <- rbind(sse.table, c(WT = w, UNIT = n, SSE = mean(sse)));
>>   }
>>}
>># PRINT OUT THE RESULT
>>print(sse.table);http://statcompute.spaces.live.com/Blog/cns!39C8032DBD1321B7!290.entry
>>
>>
>>On 3/9/07, Aimin Yan <aiminy at iastate.edu> wrote:
>> > I want to adjust weight decay and number of hidden units for nnet by
>> > a loop like
>> > for(decay)
>> > {
>> >   for(number of unit)
>> >    {
>> >     for(#run)
>> >      {model<-nnet()
>> >        test.error<-....
>> >      }
>> >    }
>> > }
>> >
>> > for example:
>> > I set decay=0.1, size=3, maxit=200, for this set I run 10 times, and
>> > calculate test error
>> >
>> > after that I want to get a matrix like this
>> >
>> > decay  size   maxit  #run  test_error
>> > 0.1        3        200   1       1.2
>> > 0.1        3        200   2       1.1
>> > 0.1        3        200   3       1.0
>> > 0.1        3        200   4       3.4
>> > 0.1        3        200   5        ..
>> > 0.1        3        200   6         ..
>> > 0.1        3        200   7       ..
>> > 0.1        3        200   8      ..
>> > 0.1        3        200   9       ..
>> > 0.1        3        200   10       ..
>> > 0.2        3        200   1       1.2
>> > 0.2        3        200   2       1.1
>> > 0.2        3        200   3       1.0
>> > 0.2        3        200   4       3.4
>> > 0.2        3        200   5        ..
>> > 0.2        3        200   6         ..
>> > 0.2        3        200   7       ..
>> > 0.2        3        200   8      ..
>> > 0.2        3        200   9       ..
>> > 0.2        3        200   10       ..
>> >
>> > I am not sure if this is correct way to do this?
>> > Does anyone tune these parameters like this before?
>> > thanks,
>> >
>> > Aimin
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>
>>
>>--
>>WenSui Liu
>>A lousy statistician who happens to know a little programming
>>(http://spaces.msn.com/statcompute/blog)
>
>
>--
>WenSui Liu
>A lousy statistician who happens to know a little programming
>(http://spaces.msn.com/statcompute/blog)


From researchjj at gmail.com  Sat Mar 10 06:45:38 2007
From: researchjj at gmail.com (j.joshua thomas)
Date: Sat, 10 Mar 2007 13:45:38 +0800
Subject: [R] read a irregular text file data into dataframe()
Message-ID: <b4485c4c0703092145w45d597ayc22aaf0420c3155@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070310/9573a4c2/attachment.pl 

From liuwensui at gmail.com  Sat Mar 10 06:46:14 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Sat, 10 Mar 2007 00:46:14 -0500
Subject: [R] use nnet
In-Reply-To: <6.1.2.0.2.20070309224321.01d6fb00@aiminy.mail.iastate.edu>
References: <6.2.3.4.2.20070309120535.03e71440@aiminy.mail.iastate.edu>
	<1115a2b00703091037r6ccb65a5g6d7d889acad45875@mail.gmail.com>
	<1115a2b00703091039p3034dacdn260981e4afffa854@mail.gmail.com>
	<6.1.2.0.2.20070309224321.01d6fb00@aiminy.mail.iastate.edu>
Message-ID: <1115a2b00703092146l69952dc0te0c86a2dbc522f5a@mail.gmail.com>

no, it is called regression. ^_^.

On 3/9/07, Aimin Yan <aiminy at iastate.edu> wrote:
> thank you very much.
> I have a another question about nnet
> if I set size=0, and skip=TRUE.
> Then this network has just input layer and out layer.
> Is this also called perceptron network?
>
> thanks,
>
> Aimin Yan
>
>
> At 12:39 PM 3/9/2007, Wensui Liu wrote:
> >AM,
> >Sorry. please ignore the top box in the code. It is not actually a cv
> >validation but just a simple split-sample validation.
> >sorry for confusion.
> >
> >On 3/9/07, Wensui Liu <liuwensui at gmail.com> wrote:
> >>AM,
> >>I have a pieice of junk on my blog. Here it is.
> >>#################################################
> >># USE CROSS-VALIDATION TO DO A GRID-SEARCH FOR  #
> >># THE OPTIMAL SETTINGS (WEIGHT DECAY AND NUMBER #
> >># OF HIDDEN UNITS) OF NEURAL NETS               #
> >>#################################################
> >>
> >>library(nnet);
> >>library(MASS);
> >>data(Boston);
> >>X <- I(as.matrix(Boston[-14]));
> >># STANDARDIZE PREDICTORS
> >>st.X <- scale(X);
> >>Y <- I(as.matrix(Boston[14]));
> >>boston <- data.frame(X = st.X, Y);
> >>
> >># DIVIDE DATA INTO TESTING AND TRAINING SETS
> >>set.seed(2005);
> >>test.rows <- sample(1:nrow(boston), 100);
> >>test.set <- boston[test.rows, ];
> >>train.set <- boston[-test.rows, ];
> >>
> >># INITIATE A NULL TABLE
> >>sse.table <- NULL;
> >>
> >># SEARCH FOR OPTIMAL WEIGHT DECAY
> >># RANGE OF WEIGHT DECAYS SUGGESTED BY B. RIPLEY
> >>for (w in c(0.0001, 0.001, 0.01))
> >>{
> >>   # SEARCH FOR OPTIMAL NUMBER OF HIDDEN UNITS
> >>   for (n in 1:10)
> >>   {
> >>     # UNITIATE A NULL VECTOR
> >>     sse <- NULL;
> >>     # FOR EACH SETTING, RUN NEURAL NET MULTIPLE TIMES
> >>     for (i in 1:10)
> >>     {
> >>       # INITIATE THE RANDOM STATE FOR EACH NET
> >>       set.seed(i);
> >>       # TRAIN NEURAL NETS
> >>       net <- nnet(Y~X, size = n, data = train.set, rang = 0.00001,
> >>                        linout = TRUE, maxit = 10000, decay = w,
> >>                        skip = FALSE, trace = FALSE);
> >>       # CALCULATE SSE FOR TESTING SET
> >>       test.sse <- sum((test.set$Y - predict(net, test.set))^2);
> >>       # APPEND EACH SSE TO A VECTOR
> >>       if (i == 1) sse <- test.sse else sse <- rbind(sse, test.sse);
> >>     }
> >>     # APPEND AVERAGED SSE WITH RELATED PARAMETERS TO A TABLE
> >>     sse.table <- rbind(sse.table, c(WT = w, UNIT = n, SSE = mean(sse)));
> >>   }
> >>}
> >># PRINT OUT THE RESULT
> >>print(sse.table);http://statcompute.spaces.live.com/Blog/cns!39C8032DBD1321B7!290.entry
> >>
> >>
> >>On 3/9/07, Aimin Yan <aiminy at iastate.edu> wrote:
> >> > I want to adjust weight decay and number of hidden units for nnet by
> >> > a loop like
> >> > for(decay)
> >> > {
> >> >   for(number of unit)
> >> >    {
> >> >     for(#run)
> >> >      {model<-nnet()
> >> >        test.error<-....
> >> >      }
> >> >    }
> >> > }
> >> >
> >> > for example:
> >> > I set decay=0.1, size=3, maxit=200, for this set I run 10 times, and
> >> > calculate test error
> >> >
> >> > after that I want to get a matrix like this
> >> >
> >> > decay  size   maxit  #run  test_error
> >> > 0.1        3        200   1       1.2
> >> > 0.1        3        200   2       1.1
> >> > 0.1        3        200   3       1.0
> >> > 0.1        3        200   4       3.4
> >> > 0.1        3        200   5        ..
> >> > 0.1        3        200   6         ..
> >> > 0.1        3        200   7       ..
> >> > 0.1        3        200   8      ..
> >> > 0.1        3        200   9       ..
> >> > 0.1        3        200   10       ..
> >> > 0.2        3        200   1       1.2
> >> > 0.2        3        200   2       1.1
> >> > 0.2        3        200   3       1.0
> >> > 0.2        3        200   4       3.4
> >> > 0.2        3        200   5        ..
> >> > 0.2        3        200   6         ..
> >> > 0.2        3        200   7       ..
> >> > 0.2        3        200   8      ..
> >> > 0.2        3        200   9       ..
> >> > 0.2        3        200   10       ..
> >> >
> >> > I am not sure if this is correct way to do this?
> >> > Does anyone tune these parameters like this before?
> >> > thanks,
> >> >
> >> > Aimin
> >> >
> >> > ______________________________________________
> >> > R-help at stat.math.ethz.ch mailing list
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> > and provide commented, minimal, self-contained, reproducible code.
> >> >
> >>
> >>
> >>--
> >>WenSui Liu
> >>A lousy statistician who happens to know a little programming
> >>(http://spaces.msn.com/statcompute/blog)
> >
> >
> >--
> >WenSui Liu
> >A lousy statistician who happens to know a little programming
> >(http://spaces.msn.com/statcompute/blog)
>
>
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From brown_emu at yahoo.com  Sat Mar 10 07:50:48 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Fri, 9 Mar 2007 22:50:48 -0800 (PST)
Subject: [R] read a irregular text file data into dataframe()
In-Reply-To: <b4485c4c0703092145w45d597ayc22aaf0420c3155@mail.gmail.com>
Message-ID: <497298.52957.qm@web39714.mail.mud.yahoo.com>

I don't know of any canned function to do this but you can write your own
function (see contents below) to:

(1) open file connection
(2) read number of fields
(3) create empty matrix with the number of rows and maximum number of columns
of your data
(4) rewind to beginning of file
(5) scan line-by-line and fill the matrix
(6) close the file connection
(7) convert matrix to data frame
(8) use the function type.convert to automatically convert numerical columns
to mode numeric (since scan(), as I've specified it, reads in everything as
mode character, which converts the holding matrix's mode to character from
its default of logical).

the function below will work for your example data set, but to make it more
general, you can add arguments like 'what' to scan(), 'sep' to both
count.fields() and scan(); depending on whether you have column names you can
modify it accordingly as well.

# call function with this line
df <- read.irregular("c:\\test.txt")

# this is the function

read.irregular <- function(filenm) {
  fileID <- file(filenm,open="rt")
  nFields <- count.fields(fileID)
  mat <- matrix(nrow=length(nFields),ncol=max(nFields))
  invisible(seek(fileID,where=0,origin="start",rw="read"))
  for(i in 1:nrow(mat) ) {
    mat[i,1:nFields[i]] <-scan(fileID,what="",nlines=1,quiet=TRUE)
  }
  close(fileID)
  df <- as.data.frame(mat)
  df[] <- lapply(df,type.convert,as.is=TRUE)
  return(df)
}

Hope this helps.

--- "j.joshua thomas" <researchjj at gmail.com> wrote:

> I am using R2.4.1 calling a text file contains the following data
> structure:
> 
> when i call the file into R using
> 
> tData<-read.table("c:\\test.txt")
> 
> it gave me Error saying, irregular column in the data set
> however i need to use the below type of data
> 
> Is there any alternative in R?
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
> 0010 0028 0061 0088
> 0010 0042 0084
> 0004 0010 0055
> 0010 0018 0040 0042
> 0010 0046 0059
> 0010 0016 0042 0055
> 0010 0012 0018 0054
> 0010 0034 0042 0102
> 0081
> 0001 0076 0085
> 0080 0086
> 0017 0032 0081
> 0004 0010 0055
> 0010 0042 0061 0080
> 0010 0017 0078 0084
> 0006 0010 0040 0042
> 0075 0080
> 0005 0028 0032
> 0006 0010 0040 0061
> -- 
> Lecturer J. Joshua Thomas
> KDU College Penang Campus
> Research Student,
> University Sains Malaysia
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



 
____________________________________________________________________________________
It's here! Your new message!  
Get new email alerts with the free Yahoo! Toolbar.


From klaster at karlin.mff.cuni.cz  Sat Mar 10 08:42:33 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Sat, 10 Mar 2007 08:42:33 +0100
Subject: [R] read a irregular text file data into dataframe()
In-Reply-To: <b4485c4c0703092145w45d597ayc22aaf0420c3155@mail.gmail.com>
References: <b4485c4c0703092145w45d597ayc22aaf0420c3155@mail.gmail.com>
Message-ID: <45F26169.8010107@karlin.mff.cuni.cz>

read.table("c:\\test.txt",fill=TRUE)

Petr

j.joshua thomas napsal(a):
> I am using R2.4.1 calling a text file contains the following data structure:
> 
> when i call the file into R using
> 
> tData<-read.table("c:\\test.txt")
> 
> it gave me Error saying, irregular column in the data set
> however i need to use the below type of data
> 
> Is there any alternative in R?
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
> 0010 0028 0061 0088
> 0010 0042 0084
> 0004 0010 0055
> 0010 0018 0040 0042
> 0010 0046 0059
> 0010 0016 0042 0055
> 0010 0012 0018 0054
> 0010 0034 0042 0102
> 0081
> 0001 0076 0085
> 0080 0086
> 0017 0032 0081
> 0004 0010 0055
> 0010 0042 0061 0080
> 0010 0017 0078 0084
> 0006 0010 0040 0042
> 0075 0080
> 0005 0028 0032
> 0006 0010 0040 0061

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From Roger.Bivand at nhh.no  Sat Mar 10 08:43:18 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sat, 10 Mar 2007 08:43:18 +0100 (CET)
Subject: [R] Using large datasets: can I overload the subscript operator?
In-Reply-To: <732eec150703091547j3f0a8a89udf2015e89796653c@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0703100828410.10064-100000@reclus.nhh.no>

On Sat, 10 Mar 2007, Maciej Radziejewski wrote:

> Hello,
> 

The http://www.met.rdg.ac.uk/cag/rclim/ site may have some useful leads. 
In addition, you'll find ideas in two packages created by Tim Keitt, 
rgdal, and Rdbi+RdbiPgSQL (now on Bioconductor). 

> I do some computations on datasets that come from climate models. These data
> are huge arrays, significantly larger than typically available RAM, so they
> have to be accessed row-by-row, or rather slice-by slice, depending on the
> task. I would like to make an R package to easily access such datasets
> within R. The C++ backend is ready and being used under Windows/.Net/Visual
> Basic, but I have yet to learn the specifics of R programming to make a good
> R interface.

Look at the Matrix package for examples - you may need finalizers to tidy 
up memory allocation - see examples in rgdal. The key thing will be 
thinking through how to implement the R objects as classes, probably not 
simply reflecting the C++ classes. Classes are covered in the Green Book 
(Chambers 1998) and Venables & Ripley (2000) S Programming.

> 
> I think it should be possible to make a package (call it "slice") that could
> be used like this:
> 
> library (slice)
> dataset <- load.virtualarray ("dataset_definition.xml")
> ordinaryvector <- dataset [ , 2, 3] # Load a portion of the data from disk
> and extract it
> 
> In the above "dataset" is an object that holds a definition of a
> 3-dimensional large dataset, and "ordinaryvector" is an ordinary R vector.
> The subscripting operator fetches necessary data from disk and extracts a
> required slice, taking care of caching and other technical details. So, my
> questions are:
> 
> Has anyone ever made a similar extension, with virtual (lazy) arrays?
> 
> Can the suscript operator be overloaded like that in R? (I know it can be in
> S, at least for vectors.)
> 

Yes, there are many examples, see the Matrix package for some that use 
new-style classes (in language issues like this, R is S, the differences 
are in scoping).

> And a tough one: is it possible to make an expression like "[1]" (without
> quoutes) meaningful in R? At the moment it results in a syntax error. I
> would like to make it return an object of a special class that gets
> interpreted when subscripting my virtual array as "drop this dimension",
> like this:

Most likely not in this context, because "[" in this context will not be
what you want. But if your "[.dataset" method is careful about examining
its arguments, you ought to be able to get the result you want. You'll
likely learn a good deal from looking for example at the code in the
Matrix package.

> 
> dataset [, 2, 3, drop = F]  # Return a 3-dimensional array
> dataset [, [2], 3, drop = F]  # Return a 2-dimensional array
> dataset [, [2], [3], drop = F]  # Return a 1-dimensional array, like dataset
> [, 2, 3]
> 
> Thanks in advance for any help,
> 
> Maciej.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From toby909 at gmail.com  Sat Mar 10 08:56:10 2007
From: toby909 at gmail.com (toby909 at gmail.com)
Date: Fri, 09 Mar 2007 23:56:10 -0800
Subject: [R] long character string problem
In-Reply-To: <615144.51515.qm@web39702.mail.mud.yahoo.com>
References: <est85s$5dq$1@sea.gmane.org>
	<615144.51515.qm@web39702.mail.mud.yahoo.com>
Message-ID: <estoa8$2uc$1@sea.gmane.org>

Yes parse(text=) is what I was looking for. thanks. (I also fixed the missing 
operator).

I will feed this entire expression to the deriv() function.

T


Stephen Tucker wrote:
> I think you are looking for 
> fct <- c(parse(text=cum1),parse(text=cum2))
> 
> although you need to include operators before your A coefficients (for
> example,
> 
> ...C11)A12*...
> 
> 
> 
> 
> --- toby909 at gmail.com wrote:
> 
> 
>>Hi All
>>
>>I am having 2 very long character strings (550chars) and I want to put them
>>as 
>>expressions together with c(). The problem is that I also get these 
>>double-quotes, as seen below in 'fct'. How can I remove these
>>double-quotes? I 
>>tried as.name() but it did not work (because of size?). These are creating 
>>trouble with subsequent programs, which I tested with strings that for some
>>
>>reason do not have these double quotes (see very bottom).
>>
>>
>>
>>
>>
>> > cum1
>>[1] 
>>
> 
> "A11*(X11*x1+X21*x2)+1*sqrt(B11*(X11*x1+X21*x2)^2+C11)A12*(X12*x1+X22*x2)+1*sqrt(B12*(X12*x1+X22*x2)^2+C12)A13*(X13*x1+X23*x2)+-1*sqrt(B13*(X13*x1+X23*x2)^2+C13)A14*(X14*x1+X24*x2)+-1*sqrt(B14*(X14*x1+X24*x2)^2+C14)A15*(X15*x1+X25*x2)+1*sqrt(B15*(X15*x1+X25*x2)^2+C15)A16*(X16*x1+X26*x2)+1*sqrt(B16*(X16*x1+X26*x2)^2+C16)A17*(X17*x1+X27*x2)+1*sqrt(B17*(X17*x1+X27*x2)^2+C17)A18*(X18*x1+X28*x2)+1*sqrt(B18*(X18*x1+X28*x2)^2+C18)A19*(X19*x1+X29*x2)+-1*sqrt(B19*(X19*x1+X29*x2)^2+C19)A110*(X110*x1+X210*x2)+1*sqrt(B110*(X110*x1+X210*x2)^2+C110)"
> 
>> > cum2
>>[1] 
>>
> 
> "A21*(X11*x1+X21*x2)+1*sqrt(B21*(X11*x1+X21*x2)^2+C21)A22*(X12*x1+X22*x2)+1*sqrt(B22*(X12*x1+X22*x2)^2+C22)A23*(X13*x1+X23*x2)+-1*sqrt(B23*(X13*x1+X23*x2)^2+C23)A24*(X14*x1+X24*x2)+-1*sqrt(B24*(X14*x1+X24*x2)^2+C24)A25*(X15*x1+X25*x2)+1*sqrt(B25*(X15*x1+X25*x2)^2+C25)A26*(X16*x1+X26*x2)+1*sqrt(B26*(X16*x1+X26*x2)^2+C26)A27*(X17*x1+X27*x2)+1*sqrt(B27*(X17*x1+X27*x2)^2+C27)A28*(X18*x1+X28*x2)+1*sqrt(B28*(X18*x1+X28*x2)^2+C28)A29*(X19*x1+X29*x2)+-1*sqrt(B29*(X19*x1+X29*x2)^2+C29)A210*(X110*x1+X210*x2)+1*sqrt(B210*(X110*x1+X210*x2)^2+C210)"
> 
>> > fct = c(as.expression(cum1), as.expression(cum2))
>> > fct
>>
> 
> expression("A11*(X11*x1+X21*x2)+1*sqrt(B11*(X11*x1+X21*x2)^2+C11)A12*(X12*x1+X22*x2)+1*sqrt(B12*(X12*x1+X22*x2)^2+C12)A13*(X13*x1+X23*x2)+-1*sqrt(B13*(X13*x1+X23*x2)^2+C13)A14*(X14*x1+X24*x2)+-1*sqrt(B14*(X14*x1+X24*x2)^2+C14)A15*(X15*x1+X25*x2)+1*sqrt(B15*(X15*x1+X25*x2)^2+C15)A16*(X16*x1+X26*x2)+1*sqrt(B16*(X16*x1+X26*x2)^2+C16)A17*(X17*x1+X27*x2)+1*sqrt(B17*(X17*x1+X27*x2)^2+C17)A18*(X18*x1+X28*x2)+1*sqrt(B18*(X18*x1+X28*x2)^2+C18)A19*(X19*x1+X29*x2)+-1*sqrt(B19*(X19*x1+X29*x2)^2+C19)A110*(X110*x1+X210*x2)+1*sqrt(B110*(X110*x1+X210*x2)^2+C110)",
> 
>>
>> 
>>
> 
> "A21*(X11*x1+X21*x2)+1*sqrt(B21*(X11*x1+X21*x2)^2+C21)A22*(X12*x1+X22*x2)+1*sqrt(B22*(X12*x1+X22*x2)^2+C22)A23*(X13*x1+X23*x2)+-1*sqrt(B23*(X13*x1+X23*x2)^2+C23)A24*(X14*x1+X24*x2)+-1*sqrt(B24*(X14*x1+X24*x2)^2+C24)A25*(X15*x1+X25*x2)+1*sqrt(B25*(X15*x1+X25*x2)^2+C25)A26*(X16*x1+X26*x2)+1*sqrt(B26*(X16*x1+X26*x2)^2+C26)A27*(X17*x1+X27*x2)+1*sqrt(B27*(X17*x1+X27*x2)^2+C27)A28*(X18*x1+X28*x2)+1*sqrt(B28*(X18*x1+X28*x2)^2+C28)A29*(X19*x1+X29*x2)+-1*sqrt(B29*(X19*x1+X29*x2)^2+C29)A210*(X110*x1+X210*x2)+1*sqrt(B210*(X110*x1+X210*x2)^2+C210)")
> 
>> >
>>
>>
>>
>>
>>
>>
>>
>>
>> > fct = c(expression(2*x1^3-7*x2^2-9), expression(x1^2-x2^3+1))
>> > fct
>>expression(2 * x1^3 - 7 * x2^2 - 9, x1^2 - x2^3 + 1)
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 
> 
> 
> 
>  
> ____________________________________________________________________________________
> Need Mail bonding?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From lauri.nikkinen at iki.fi  Sat Mar 10 13:04:14 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Sat, 10 Mar 2007 14:04:14 +0200
Subject: [R] barplot, for loop?
Message-ID: <ba8c09910703100404j51405a77ie154fc59a6cefc09@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070310/d8cb1086/attachment.pl 

From lauri.nikkinen at iki.fi  Sat Mar 10 13:26:26 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Sat, 10 Mar 2007 14:26:26 +0200
Subject: [R] barplot, for loop?
In-Reply-To: <ba8c09910703100404j51405a77ie154fc59a6cefc09@mail.gmail.com>
References: <ba8c09910703100404j51405a77ie154fc59a6cefc09@mail.gmail.com>
Message-ID: <ba8c09910703100426u6e94b51dicc92ba6bdceede2b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070310/7fa7d056/attachment.pl 

From gregory.warnes at mac.com  Sat Mar 10 14:13:37 2007
From: gregory.warnes at mac.com (Gregory Warnes)
Date: Sat, 10 Mar 2007 05:13:37 -0800
Subject: [R] Problem with ci.lmer() in package:gmodels
Message-ID: <CD83233B-0111-1000-82DD-73D8C0D2FB70-Webmail-10018@mac.com>


Hello Michael,

I checked the source code, and the lower and upper confidence interval endpoints were simply reversed.  The calculations themselves are correct.

I have uploaded a new version of the gmodels package to the CRAN repository, and it should show up in a couple of days.  In the mean time, you can either simply reverse them manually, or modify the source file est.lmer to swap the two endpoints.   

The new package also fixes the two places where my email address wasn't properly updated.

-Greg


From: Michael Kubovy <kubovy_at_virginia.edu>
Date: Fri 09 Mar 2007 - 10:08:28 GMT


Dear Friends,

Please note that in the following CI lower > CI higher:

> require(lmer)
> require(gmodels)
> fm2 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject),
sleepstudy)
> ci(fm2)

              Estimate  CI lower   CI upper Std. Error p-value
(Intercept) 251.66693 266.06895 238.630280   7.056447       0
Days         10.52773  13.63372   7.389946   1.646900       0



_____________________________


Professor Michael Kubovy
University of Virginia
Department of Psychology

USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766


WWW: http://www.people.virginia.edu/~mk9y/
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the posting guide http://www.R-project.org/posting-guide.html and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Sat Mar 10 14:21:39 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 10 Mar 2007 08:21:39 -0500
Subject: [R] Using large datasets: can I overload the subscript operator?
In-Reply-To: <732eec150703091547j3f0a8a89udf2015e89796653c@mail.gmail.com>
References: <732eec150703091547j3f0a8a89udf2015e89796653c@mail.gmail.com>
Message-ID: <971536df0703100521o343224abj674f111d77b329c8@mail.gmail.com>

On 3/9/07, Maciej Radziejewski <maciej.rhelp at gmail.com> wrote:
> Hello,
>
> I do some computations on datasets that come from climate models. These data
> are huge arrays, significantly larger than typically available RAM, so they
> have to be accessed row-by-row, or rather slice-by slice, depending on the
> task. I would like to make an R package to easily access such datasets
> within R. The C++ backend is ready and being used under Windows/.Net/Visual
> Basic, but I have yet to learn the specifics of R programming to make a good
> R interface.
>
> I think it should be possible to make a package (call it "slice") that could
> be used like this:
>
> library (slice)
> dataset <- load.virtualarray ("dataset_definition.xml")
> ordinaryvector <- dataset [ , 2, 3] # Load a portion of the data from disk
> and extract it
>
> In the above "dataset" is an object that holds a definition of a
> 3-dimensional large dataset, and "ordinaryvector" is an ordinary R vector.
> The subscripting operator fetches necessary data from disk and extracts a
> required slice, taking care of caching and other technical details. So, my
> questions are:
>
> Has anyone ever made a similar extension, with virtual (lazy) arrays?

Not quite the same but you might look at the g.data delayed data package
in case its good enough for your needs.  Note the dot.  gdata without a dot
is a different package.

>
> Can the suscript operator be overloaded like that in R? (I know it can be in
> S, at least for vectors.)

Yes.  You make your objects a class, myclass, and then define
"[.myclass" <- function...
 for myclass in the S3 class system and similarly in S4.  S3 is easier
to develop for and has higher performance so you probably want that
rather than S4.

A few examples packages are XML (see "[.XMLNode"), fame and zoo for
S3 and 'its' for S4.

Be sure to check out
?.subset
See think post for context:
http://tolstoy.newcastle.edu.au/R/devel/05/05/0853.html

>
> And a tough one: is it possible to make an expression like "[1]" (without
> quoutes) meaningful in R? At the moment it results in a syntax error. I
> would like to make it return an object of a special class that gets
> interpreted when subscripting my virtual array as "drop this dimension",
> like this:
>
> dataset [, 2, 3, drop = F]  # Return a 3-dimensional array
> dataset [, [2], 3, drop = F]  # Return a 2-dimensional array
> dataset [, [2], [3], drop = F]  # Return a 1-dimensional array, like dataset
> [, 2, 3]

No but one idea is to define the single letter . (i.e. a dot) to be of a special
class, dot say and define "[.dot" to produce objects of a special
class (maybe also
"dot").   Then you could write dataset[, .[2], .[3], drop = FALSE] if
you define "[.myclass" to look for such objects.

Another possibility is to use formula notation:

dataset[, ~2, ~3, drop = FALSE]

and have [.myclass handle formula arguments specially of perhaps forget
about that notation and just extend drop:

dataset[drop = 2:3]

BTW, its better to use FALSE rather than F since F can be a variable name.


From bunny at lautloscrew.com  Sat Mar 10 14:40:19 2007
From: bunny at lautloscrew.com (bunny , lautloscrew.com)
Date: Sat, 10 Mar 2007 14:40:19 +0100
Subject: [R] dendrogram - got it , just need to label :)
In-Reply-To: <0BE438149FF2254DB4199E2682C8DFEB0235FB45@crcmail1.BCCRC.CA>
References: <1F1AB0D7-4CF0-495F-9874-842A3632F076@lautloscrew.com>
	<0BE438149FF2254DB4199E2682C8DFEB0235FB45@crcmail1.BCCRC.CA>
Message-ID: <2BECE7FC-C258-4E41-9ED8-14119B756690@lautloscrew.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070310/b50a15cd/attachment.pl 

From moreyr at missouri.edu  Sat Mar 10 04:11:40 2007
From: moreyr at missouri.edu (Richard Morey)
Date: Fri, 09 Mar 2007 21:11:40 -0600
Subject: [R] Mac vs. PC
Message-ID: <45F221EC.8040308@missouri.edu>

My adviser has a Mac notebook that he bought 6 months ago, and I have a 
PC notebook I bought a month ago. Here are the respective specs, as far 
as I know them:

His:
Mac OSX
1 GB DDR2 RAM
Intel Core Duo, 2 GHz (2MB cache per core)
Unknown HD

Mine
Windows Vista Home Premium 32bit
2 GB DDR2 RAM
Intel Core 2 Duo, 2 GHz (4MB cache)
5400 RPM Hard Drive


We are both running R. As a test to see whose laptop was faster, we 
decided to invert large random matrices. In R language, it looks like this:

N=2000
A=rnorm(N^2)
A=matrix(A,ncol=N)
solve(A)

This creates a matrix of 4,000,000 random normal deviates and inverts 
it. His computer takes about 7 seconds, while mine takes about 14. Why 
the difference? I have several working hypotheses, and it would be 
interesting to see what you guys think.

1. R on Mac was compiled with optimizations for the CPU, with R for 
Windows was not. I could test this by compiling R with the Intel 
compiler, or GCC with optimizations, and seeing if I get a significant 
speed boost.

2. His R is 64 bit, while mine is for 32 bit windows. (I'm not sure how 
much of a diference that makes, or whether OSX is 64 bit.)

3. Data is getting swapped to the hard drive, and my hard drive is 
slower than his. I chose a slower hard drive to get bigger capacity for 
the price.

This is not intended to be an OMG MACOS = TEH R0X0R thread. I'm just 
trying to explain the discrepency.

Thanks!


From toby909 at gmail.com  Fri Mar  9 22:46:39 2007
From: toby909 at gmail.com (toby909 at gmail.com)
Date: Fri, 09 Mar 2007 13:46:39 -0800
Subject: [R] piecing together statements (macro?)
Message-ID: <esskjf$kra$1@sea.gmane.org>

Hi All

I am pretty new to R but saw stata and sas's macro facilities and am looking for 
how such things work in R.

I am trying to piece together a series of statements:


n = 5   #want to have it dynamic with respect to n
for (j in 1:n) {
eval(paste("x", j, "=x[", j, "]", sep=""))
}

I want the created statements 'x1=x[1]' immediately executed and tried to do 
that with eval() but that did not work.

Any hints greatly appreciates.

Thanks Toby


From toby909 at gmail.com  Sat Mar 10 04:21:10 2007
From: toby909 at gmail.com (toby909 at gmail.com)
Date: Fri, 09 Mar 2007 19:21:10 -0800
Subject: [R] long character string problem
Message-ID: <est86k$5dq$2@sea.gmane.org>

Hi All

I am having 2 very long character strings (550chars) and I want to put them as 
expressions together with c(). The problem is that I also get these 
double-quotes, as seen below in 'fct'. How can I remove these double-quotes? I 
tried as.name() but it did not work (because of size?). These are creating 
trouble with subsequent programs, which I tested with strings that for some 
reason do not have these double quotes (see very bottom).

Thanks Toby




 > cum1
[1] 
"A11*(X11*x1+X21*x2)+1*sqrt(B11*(X11*x1+X21*x2)^2+C11)A12*(X12*x1+X22*x2)+1*sqrt(B12*(X12*x1+X22*x2)^2+C12)A13*(X13*x1+X23*x2)+-1*sqrt(B13*(X13*x1+X23*x2)^2+C13)A14*(X14*x1+X24*x2)+-1*sqrt(B14*(X14*x1+X24*x2)^2+C14)A15*(X15*x1+X25*x2)+1*sqrt(B15*(X15*x1+X25*x2)^2+C15)A16*(X16*x1+X26*x2)+1*sqrt(B16*(X16*x1+X26*x2)^2+C16)A17*(X17*x1+X27*x2)+1*sqrt(B17*(X17*x1+X27*x2)^2+C17)A18*(X18*x1+X28*x2)+1*sqrt(B18*(X18*x1+X28*x2)^2+C18)A19*(X19*x1+X29*x2)+-1*sqrt(B19*(X19*x1+X29*x2)^2+C19)A110*(X110*x1+X210*x2)+1*sqrt(B110*(X110*x1+X210*x2)^2+C110)"
 > cum2
[1] 
"A21*(X11*x1+X21*x2)+1*sqrt(B21*(X11*x1+X21*x2)^2+C21)A22*(X12*x1+X22*x2)+1*sqrt(B22*(X12*x1+X22*x2)^2+C22)A23*(X13*x1+X23*x2)+-1*sqrt(B23*(X13*x1+X23*x2)^2+C23)A24*(X14*x1+X24*x2)+-1*sqrt(B24*(X14*x1+X24*x2)^2+C24)A25*(X15*x1+X25*x2)+1*sqrt(B25*(X15*x1+X25*x2)^2+C25)A26*(X16*x1+X26*x2)+1*sqrt(B26*(X16*x1+X26*x2)^2+C26)A27*(X17*x1+X27*x2)+1*sqrt(B27*(X17*x1+X27*x2)^2+C27)A28*(X18*x1+X28*x2)+1*sqrt(B28*(X18*x1+X28*x2)^2+C28)A29*(X19*x1+X29*x2)+-1*sqrt(B29*(X19*x1+X29*x2)^2+C29)A210*(X110*x1+X210*x2)+1*sqrt(B210*(X110*x1+X210*x2)^2+C210)"
 > fct = c(as.expression(cum1), as.expression(cum2))
 > fct
expression("A11*(X11*x1+X21*x2)+1*sqrt(B11*(X11*x1+X21*x2)^2+C11)A12*(X12*x1+X22*x2)+1*sqrt(B12*(X12*x1+X22*x2)^2+C12)A13*(X13*x1+X23*x2)+-1*sqrt(B13*(X13*x1+X23*x2)^2+C13)A14*(X14*x1+X24*x2)+-1*sqrt(B14*(X14*x1+X24*x2)^2+C14)A15*(X15*x1+X25*x2)+1*sqrt(B15*(X15*x1+X25*x2)^2+C15)A16*(X16*x1+X26*x2)+1*sqrt(B16*(X16*x1+X26*x2)^2+C16)A17*(X17*x1+X27*x2)+1*sqrt(B17*(X17*x1+X27*x2)^2+C17)A18*(X18*x1+X28*x2)+1*sqrt(B18*(X18*x1+X28*x2)^2+C18)A19*(X19*x1+X29*x2)+-1*sqrt(B19*(X19*x1+X29*x2)^2+C19)A110*(X110*x1+X210*x2)+1*sqrt(B110*(X110*x1+X210*x2)^2+C110)", 

 
"A21*(X11*x1+X21*x2)+1*sqrt(B21*(X11*x1+X21*x2)^2+C21)A22*(X12*x1+X22*x2)+1*sqrt(B22*(X12*x1+X22*x2)^2+C22)A23*(X13*x1+X23*x2)+-1*sqrt(B23*(X13*x1+X23*x2)^2+C23)A24*(X14*x1+X24*x2)+-1*sqrt(B24*(X14*x1+X24*x2)^2+C24)A25*(X15*x1+X25*x2)+1*sqrt(B25*(X15*x1+X25*x2)^2+C25)A26*(X16*x1+X26*x2)+1*sqrt(B26*(X16*x1+X26*x2)^2+C26)A27*(X17*x1+X27*x2)+1*sqrt(B27*(X17*x1+X27*x2)^2+C27)A28*(X18*x1+X28*x2)+1*sqrt(B28*(X18*x1+X28*x2)^2+C28)A29*(X19*x1+X29*x2)+-1*sqrt(B29*(X19*x1+X29*x2)^2+C29)A210*(X110*x1+X210*x2)+1*sqrt(B210*(X110*x1+X210*x2)^2+C210)")
 >








 > fct = c(expression(2*x1^3-7*x2^2-9), expression(x1^2-x2^3+1))
 > fct
expression(2 * x1^3 - 7 * x2^2 - 9, x1^2 - x2^3 + 1)


From dicook at iastate.edu  Fri Mar  9 22:26:24 2007
From: dicook at iastate.edu (Dianne Cook)
Date: Fri, 9 Mar 2007 15:26:24 -0600
Subject: [R] useR! 2007 --- Call for papers and posters
Message-ID: <6FA60D2B-695D-4249-B546-AB2018BA71CB@iastate.edu>

R Users and Developers,

The first North American useR! will be held at Iowa State University,  
Ames, Iowa, August 8?10, 2007. Information about the meeting can be  
found at http://www.user2007.org/.

We are now ready to accept paper and poster submissions.

Papers are encouraged in all areas, but particular emphasis is given  
to work describing newly created or improved R packages. Papers will  
be refereed and a best paper/presentation award is likely. Your full  
paper needs to be submitted by April 23, 5:00PM CST, to be considered  
for the meeting.

There will also be the opportunity to present your work as a poster  
instead of a paper. Poster submissions will be in the form of an  
abstract and needs to be submitted by June 30.

Submit full papers, and poster abstracts, to submissions at user2007.org.

useR! Program Committee
user2007 at iastate.edu

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From ggrothendieck at gmail.com  Sat Mar 10 15:27:17 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 10 Mar 2007 09:27:17 -0500
Subject: [R] Mac vs. PC
In-Reply-To: <45F221EC.8040308@missouri.edu>
References: <45F221EC.8040308@missouri.edu>
Message-ID: <971536df0703100627nfb7c334p50d889ee52e54cd3@mail.gmail.com>

Such a calculation would be dominated by the time spent inside a call
to an offf-the-shelf C matrix inversion library used by R and is not really
any test of R itself.

On 3/9/07, Richard Morey <moreyr at missouri.edu> wrote:
> My adviser has a Mac notebook that he bought 6 months ago, and I have a
> PC notebook I bought a month ago. Here are the respective specs, as far
> as I know them:
>
> His:
> Mac OSX
> 1 GB DDR2 RAM
> Intel Core Duo, 2 GHz (2MB cache per core)
> Unknown HD
>
> Mine
> Windows Vista Home Premium 32bit
> 2 GB DDR2 RAM
> Intel Core 2 Duo, 2 GHz (4MB cache)
> 5400 RPM Hard Drive
>
>
> We are both running R. As a test to see whose laptop was faster, we
> decided to invert large random matrices. In R language, it looks like this:
>
> N=2000
> A=rnorm(N^2)
> A=matrix(A,ncol=N)
> solve(A)
>
> This creates a matrix of 4,000,000 random normal deviates and inverts
> it. His computer takes about 7 seconds, while mine takes about 14. Why
> the difference? I have several working hypotheses, and it would be
> interesting to see what you guys think.
>
> 1. R on Mac was compiled with optimizations for the CPU, with R for
> Windows was not. I could test this by compiling R with the Intel
> compiler, or GCC with optimizations, and seeing if I get a significant
> speed boost.
>
> 2. His R is 64 bit, while mine is for 32 bit windows. (I'm not sure how
> much of a diference that makes, or whether OSX is 64 bit.)
>
> 3. Data is getting swapped to the hard drive, and my hard drive is
> slower than his. I chose a slower hard drive to get bigger capacity for
> the price.
>
> This is not intended to be an OMG MACOS = TEH R0X0R thread. I'm just
> trying to explain the discrepency.
>
> Thanks!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Sat Mar 10 15:30:14 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 10 Mar 2007 09:30:14 -0500
Subject: [R] piecing together statements (macro?)
In-Reply-To: <esskjf$kra$1@sea.gmane.org>
References: <esskjf$kra$1@sea.gmane.org>
Message-ID: <971536df0703100630w4ce99aefoc8f8576a2d4d2bcc@mail.gmail.com>

Read the FAQ 7.21:

http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html#How-can-I-turn-a-string-into-a-variable_003f

Note that it points out that you don't really want to do this anyways.

On 3/9/07, toby909 at gmail.com <toby909 at gmail.com> wrote:
> Hi All
>
> I am pretty new to R but saw stata and sas's macro facilities and am looking for
> how such things work in R.
>
> I am trying to piece together a series of statements:
>
>
> n = 5   #want to have it dynamic with respect to n
> for (j in 1:n) {
> eval(paste("x", j, "=x[", j, "]", sep=""))
> }
>
> I want the created statements 'x1=x[1]' immediately executed and tried to do
> that with eval() but that did not work.
>
> Any hints greatly appreciates.
>
> Thanks Toby
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From maitra at iastate.edu  Sat Mar 10 15:50:00 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Sat, 10 Mar 2007 08:50:00 -0600
Subject: [R] useR! 2007 --- Call for papers and posters
In-Reply-To: <6FA60D2B-695D-4249-B546-AB2018BA71CB@iastate.edu>
References: <6FA60D2B-695D-4249-B546-AB2018BA71CB@iastate.edu>
Message-ID: <20070310085000.756e3b43@triveni.stat.iastate.edu>

Di,

You probably wanted to say first useR! (hosted) in North America, isn't that right?

Anyway, I was wondering: what are the conditions for submitting a paper? Are the guidelines the same as JSS (or are they bringing it out)? If I can figure out how to make R packages, I might submit a paper on a package.

Ranjan

On Fri, 9 Mar 2007 15:26:24 -0600 Dianne Cook <dicook at iastate.edu> wrote:

> R Users and Developers,
> 
> The first North American useR! will be held at Iowa State University,  
> Ames, Iowa, August 8___10, 2007. Information about the meeting can be  
> found at http://www.user2007.org/.
> 
> We are now ready to accept paper and poster submissions.
> 
> Papers are encouraged in all areas, but particular emphasis is given  
> to work describing newly created or improved R packages. Papers will  
> be refereed and a best paper/presentation award is likely. Your full  
> paper needs to be submitted by April 23, 5:00PM CST, to be considered  
> for the meeting.
> 
> There will also be the opportunity to present your work as a poster  
> instead of a paper. Poster submissions will be in the form of an  
> abstract and needs to be submitted by June 30.
> 
> Submit full papers, and poster abstracts, to submissions at user2007.org.
> 
> useR! Program Committee
> user2007 at iastate.edu
> 
> _______________________________________________
> R-announce at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-announce
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From pinard at iro.umontreal.ca  Sat Mar 10 16:17:25 2007
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Sat, 10 Mar 2007 10:17:25 -0500
Subject: [R] Off topic:Spam on R-help increase?
In-Reply-To: <1173204011.10248.43.camel@localhost.localdomain>
References: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
	<1173204011.10248.43.camel@localhost.localdomain>
Message-ID: <20070310151725.GA7297@alcyon.progiciels-bpi.ca>

[Marc Schwartz]

>The "Human Spam Filter" (aka Martin) [...]

The R mailing list has, indeed, be remarkably spam-free, and 
well-managed so far that I can see.  I do hope, however, that Martin 
does not have to do the filtering himself -- it would be just daunting!

In any case, Martin, a lot of thanks from me!

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca


From peteoutside at yahoo.com  Sat Mar 10 16:22:27 2007
From: peteoutside at yahoo.com (Pete Cap)
Date: Sat, 10 Mar 2007 07:22:27 -0800 (PST)
Subject: [R] RMySQL on win32
Message-ID: <193633.53166.qm@web52413.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070310/6fcc4d00/attachment.pl 

From jholtman at gmail.com  Sat Mar 10 16:32:22 2007
From: jholtman at gmail.com (jim holtman)
Date: Sat, 10 Mar 2007 10:32:22 -0500
Subject: [R] barplot, for loop?
In-Reply-To: <ba8c09910703100404j51405a77ie154fc59a6cefc09@mail.gmail.com>
References: <ba8c09910703100404j51405a77ie154fc59a6cefc09@mail.gmail.com>
Message-ID: <644e1f320703100732m7232286br7493338db0d145a5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070310/5b729a3c/attachment.pl 

From tlumley at u.washington.edu  Sat Mar 10 17:03:56 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sat, 10 Mar 2007 08:03:56 -0800 (PST)
Subject: [R] Mac vs. PC
In-Reply-To: <45F221EC.8040308@missouri.edu>
References: <45F221EC.8040308@missouri.edu>
Message-ID: <Pine.LNX.4.64.0703100754400.19240@homer23.u.washington.edu>

On Fri, 9 Mar 2007, Richard Morey wrote:
> 1. R on Mac was compiled with optimizations for the CPU, with R for
> Windows was not. I could test this by compiling R with the Intel
> compiler, or GCC with optimizations, and seeing if I get a significant
> speed boost.

Yes.  The Mac distribution uses Apple's linear algebra library, which is 
based on ATLAS and uses both cores.  The default Windows distribution 
doesn't use an optimized linear algebra library because there isn't one 
built in to Windows.  You can use ATLAS with the Windows distribution and 
there are even precompiled DLLs around somewhere.

> 2. His R is 64 bit, while mine is for 32 bit windows. (I'm not sure how
> much of a diference that makes, or whether OSX is 64 bit.)

No.

His R isn't 64bit.  It would probably be slower if it were. The main 
reason to want 64bit R is to use lots of memory rather than to be fast.

> 3. Data is getting swapped to the hard drive, and my hard drive is
> slower than his. I chose a slower hard drive to get bigger capacity for
> the price.

This could be true in principle, but I don't think the matrices are large 
enough for it to be the main factor.


His computer won't be twice as fast on most R tasks (though it will still 
be twice as pretty, of course).

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From ligges at statistik.uni-dortmund.de  Sat Mar 10 17:07:29 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 10 Mar 2007 17:07:29 +0100
Subject: [R] RMySQL on win32
In-Reply-To: <193633.53166.qm@web52413.mail.yahoo.com>
References: <193633.53166.qm@web52413.mail.yahoo.com>
Message-ID: <45F2D7C1.7020805@statistik.uni-dortmund.de>



Pete Cap wrote:
> List,
> 
> I just left an environment where I was running R and mysql on CENTOS.
> At the time of install, RMySQL was available on CRAN.  Later installs
> on Ubuntu were possible because it was available as a package in the
> base repos.
> 
> Now I'm in a new environment where I have no choice but to use
> Windows XP.  I have just installed R 2.4.1 and MySQL 5.0.27.  The
> installation instructions for getting RMySQL to install are a bit
> dense and possibly over my head, so I'm wondering if it is really
> necessary to compile the package against my current versions (yes, I
> realize that may be a question with a painfully obvious answer), or
> if I can simply use one of the precompiled binaries at David James's
> site.  If anyone can tell me (or if there is a very easy way to get
> RMySQL up and running on win32), please let me know.
> 
> As an aside, can anyone explain why it is not possible to keep that
> package in CRAN?  I'm just curious about that, it's just for my own
> enlightenment.

For CRAN, I could only build against one version of MySQL for each 
version of R. In the past (particularly up to R-1.6.x), Brian Ripley 
made some bad experiences with incompatibilities when the package was 
built using different versions of MySQL. Hence we do not want to provide 
builds to the public that might only work together with some particular 
versions of MySQL.

Best,
Uwe




> 
> Thanks,
> 
> Pete
> 
> 
> --------------------------------- Don't get soaked.  Take a quick
> peek at the forecast
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide http://www.R-project.org/posting-guide.html and provide
> commented, minimal, self-contained, reproducible code.


From marc_schwartz at comcast.net  Sat Mar 10 17:20:38 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 10 Mar 2007 10:20:38 -0600
Subject: [R] Off topic:Spam on R-help increase?
In-Reply-To: <20070310151725.GA7297@alcyon.progiciels-bpi.ca>
References: <002601c7600b$d339dd40$4d908980@gne.windows.gene.com>
	<1173204011.10248.43.camel@localhost.localdomain>
	<20070310151725.GA7297@alcyon.progiciels-bpi.ca>
Message-ID: <1173543638.5123.20.camel@localhost.localdomain>

On Sat, 2007-03-10 at 10:17 -0500, Fran?ois Pinard wrote:
> [Marc Schwartz]
> 
> >The "Human Spam Filter" (aka Martin) [...]
> 
> The R mailing list has, indeed, be remarkably spam-free, and 
> well-managed so far that I can see.  I do hope, however, that Martin 
> does not have to do the filtering himself -- it would be just daunting!
> 
> In any case, Martin, a lot of thanks from me!

The comment was somewhat "tongue-in-cheek".

While a major proportion of spam can be filtered using automated tools,
it takes a significant amount of manual effort to configure the tools to
achieve the level of cleansing that we observe here.

On my system (laptop running FC6 Linux), I am using SpamAssassin with
Bayesian filtering enabled, along with remote spam checks such as DCC,
Razor, Pyzor and some RBLs. 

I also recently started using FuzzyOCR (as a plug-in to SA) to enhance
the filtering of spam containing only graphic content. These e-mails are
of course specifically designed to obviate the utility of text based
spam filtering.

However, I still get some that come through despite the above. There are
also 'borderline' e-mails that require manually running the spam/ham
learning scripts.

To increase the filtering effectiveness to the level we see here, I
would have to spend a fair amount of time writing custom rules for SA
and this is where I have no doubt, Martin spends a lot of his time with
list management.

HTH,

Marc Schwartz


From milton_ruser at yahoo.com.br  Sat Mar 10 18:28:20 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Sat, 10 Mar 2007 09:28:20 -0800 (PST)
Subject: [R] finding max into matrix
Message-ID: <657871.6353.qm@web56608.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070310/01f1d98f/attachment.pl 

From ruhil at ohio.edu  Sat Mar 10 19:11:15 2007
From: ruhil at ohio.edu (Anirudh V. S. Ruhil)
Date: Sat, 10 Mar 2007 13:11:15 -0500
Subject: [R] xtable with dupplicate rownames
Message-ID: <5D37A94201DCD1AECB71C42A@[192.168.1.100]>

I have  a table (tab5; see below) with the first block showing the counts 
in a cross-tabulation, and the lower block reflecting the proportions of 
interest.

> tab5
             None Low Level Moderate Intense Very Intense Total
None         0.00      3.00     0.00    0.00         1.00     4
Low Level    1.00      2.00     0.00    0.00         0.00     3
Moderate     2.00      3.00     2.00    0.00         0.00     7
Intense      3.00      1.00     1.00    1.00         0.00     6
Very Intense 0.00      1.00     2.00    0.00         0.00     3
Total        6.00     10.00     5.00    1.00         1.00    23
None         0.00      0.75     0.00    0.00         0.25     1
Low Level    0.33      0.67     0.00    0.00         0.00     1
Moderate     0.29      0.43     0.29    0.00         0.00     1
Intense      0.50      0.17     0.17    0.17         0.00     1
Very Intense 0.00      0.33     0.67    0.00         0.00     1

When I execute
	foo5 <- xtable(format(tab5), caption = "XYZ")

I get the following error message
	Warning message:
	some row.names duplicated: 7,8,9,10,11 --> row.names NOT used in: 
	data.row.names(row.names, rowsi, i)

Is there any way to force the duplicate row-names to be retained in xtable?

Alternatively, is there a succinct way of building a cross-tabulation with 
both counts and proportions in each cell, and then using xtable?

thanks for any and all tips

Ani

Anirudh V. S. Ruhil, Ph.D.
Sr. Research Associate
Voinovich Center for Leadership and Public Affairs
Ohio University
Building 21, The Ridges
Athens, OH 45701-2979
Tel: 740.597.1949 | Fax: 740.597.3057


From rdporto1 at terra.com.br  Sat Mar 10 19:59:45 2007
From: rdporto1 at terra.com.br (rdporto1)
Date: Sat, 10 Mar 2007 15:59:45 -0300
Subject: [R] Using large datasets: can I overload the subscript operator?
Message-ID: <JEPBFL$AB1C3D8D473746125C02EA1A0401BC2F@terra.com.br>

Maciej,

> I think it should be possible to make a package (call it "slice") that could
> be used like this:
> ...
> Has anyone ever made a similar extension, with virtual (lazy) arrays?
>

take a look at the filehash package at
http://cran.r-project.org/doc/Rnews/Rnews_2006-4.pdf

Regards,

Rogerio


From ted.harding at nessie.mcc.ac.uk  Sat Mar 10 20:14:42 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 10 Mar 2007 19:14:42 -0000 (GMT)
Subject: [R] curve of density on histogram
In-Reply-To: <45F06154.6060403@stats.uwo.ca>
Message-ID: <XFMail.070310191442.ted.harding@nessie.mcc.ac.uk>

On 08-Mar-07 Duncan Murdoch wrote:
> 
> On 3/8/2007 11:29 AM, (Ted Harding) wrote:
>> 
>> On 08-Mar-07 KOITA Lassana - STAC/ACE wrote:
>>> 
>>> [snip]
>> 
>> [snip]
>> The argument N to sdnorm is readily available from the argument x,
>> as N = length(x).
>> 
>> However, I cannot work out from the documentation for these panel
>> functions how to determine the width of the histogram bins, which
>> is argument binwid to sdnorm(). Hence I have simply set binwid=0.l0
>> to illustrate the point, since this gives an approximately correct
>> plot. But it will only be really correct when binwid can somehow
>> be determined from the hosyogram being plotted, and it is this
>> which I cannot see!
> 
> 
> The breaks are one of the ... args passed to the panel function, so you
> can get the binwidth from there.  But there's another problem:  the 
> panel.histogram function gives percent of total, so should integrate to
> 100, not to N.  I think this version gives what is wanted:
> 
> library(lattice)
> library(grid)
> resp  <- rnorm(2000)
> group <- sample(c("G1", "G2", "G3", "G4"), replace = TRUE, size = 1000)
>#### New function sdnorm:
> sdnorm
> <-function(x,mean=0,sd=1,N=1,binwid=1){N*binwid*dnorm(x,mean,sd)}
> histogram(~ resp | group, col="steelblue",
>    panel = function(x, ...){
>      std <- if(length(x) > 0) format(round(sd(x), 2), nsmall = 2) else
> "NA"
>      n <- length(x)
>      m <- if(length(x) > 0) format(round(mean(x), 2), nsmall = 2) else
> "NA"
>      panel.histogram(x, ...)
> 
> 
>      breaks <- list(...)$breaks
>      binwid <- breaks[2]-breaks[1]
>      panel.mathdensity(dmath = sdnorm, col = "green",
>            args = 
> list(mean=mean(x),sd=sd(x),N=100,binwid=breaks[2]-breaks[1]))
> 
> 
>      panel.abline(v= mean(x), col = "red")
>      panel.abline(h=5)
>      panel.xyplot(x = jitter(x),type="p",pch=20,y = rep(0, length(x)),
> col='yellow' )
> 
>      x1 <- unit(1, "npc") - unit(2, "mm")
>      y1 <- unit(1, "npc") - unit(2, "mm")
>      grid.text(label = bquote(n == .(n)), x = x1, y = y1, just =
> "right")
>      grid.text(label = bquote(hat(m) == .(m)), x = x1, y = y1 - unit(1,
> "lines"), just = "right")
>      grid.text(label = bquote(hat(s) == .(std)), x = x1, y = y1 -
> unit(2, "lines"), just = "right")
> })

Duncan, your statement that "But there's another problem:  the 
panel.histogram function gives percent of total, so should integrate
to 100, not to N" got me challenged -- there must be a work-round!

Finally I found it, but you have to make the change in (to me)
an unexpected place.

The following code (your code above, with two changed lines commented
out, and followed by the changed version) really does do the intended
job of plotting a panel of histograms of *counts* with the appropriate
scaled normal densities superimposed.


library(lattice)
library(grid)
N<-20000
resp  <- rnorm(N)
group <- sample(c("G1", "G2", "G3", "G4"), replace = TRUE, size = N/4)
#### New function sdnorm:
sdnorm <-function(x,mean=0,sd=1,N=1,binwid=1){N*binwid*dnorm(x,mean,sd)}

##Changed line
#### histogram(~ resp | group, col="steelblue",
histogram(~ resp | group, col="steelblue", type="count",
   panel = function(x, ...){
     std <- if(length(x) > 0) format(round(sd(x), 2), nsmall = 2) else
"NA"
     n <- length(x)
     m <- if(length(x) > 0) format(round(mean(x), 2), nsmall = 2) else
"NA"
     panel.histogram(x, ...)


     breaks <- list(...)$breaks
     binwid <- breaks[2]-breaks[1]
     panel.mathdensity(dmath = sdnorm, col = "green",
           args =
## Changed line
#### list(mean=mean(x),sd=sd(x),N=100,binwid=breaks[2]-breaks[1]))
list(mean=mean(x),sd=sd(x),N=length(x),binwid=breaks[2]-breaks[1]))

     panel.abline(v= mean(x), col = "red")
     panel.abline(h=5)
     panel.xyplot(x = jitter(x),type="p",pch=20,y = rep(0, length(x)),
col='yellow' )

     x1 <- unit(1, "npc") - unit(2, "mm")
     y1 <- unit(1, "npc") - unit(2, "mm")
     grid.text(label = bquote(n == .(n)), x = x1, y = y1, just = "right")
     grid.text(label = bquote(hat(m) == .(m)), x = x1, y = y1 - unit(1,
"lines"), just = "right")
     grid.text(label = bquote(hat(s) == .(std)), x = x1, y = y1 -
unit(2, "lines"), just = "right")
})


I first tried setting type="count" in the call to panel.histogram(),
as in

  panel.histogram(x, type="count", ...)

but this got me an error message

  Error in panel.histogram(x, type = "count", ...) : 
        formal argument "type" matched by multiple actual arguments

from which I (slowly) deduced that it had already got that parameter
from somewhere else, after which it seemed natural that it got it
from the preceding call to

  histogram(~ resp | group, col="steelblue",
    [etc]

so I tried setting it there and (with the factor N<-length(x) for
counts, instead of 100 for percentage) this worked!

(And I confess, when I posted my first suggestion, that I had
omitted to read the vertically aligned small print at the side
of the panel which said "Percent of Total"!).

Ah well, it's been an interesting little tour!

Thanks for the breakthrough, Duncan!
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 10-Mar-07                                       Time: 19:14:39
------------------------------ XFMail ------------------------------


From Greg.Snow at intermountainmail.org  Sat Mar 10 20:30:21 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Sat, 10 Mar 2007 12:30:21 -0700
Subject: [R] About "cex=": how to improve resolution?
Message-ID: <164901c7634a$8ae356f1$2e80320a@CO.IHC.COM>

Use the symbols function!


-----Original Message-----
From: "Luca Quaglia" <lqecli at yahoo.fr>
To: "r-help at stat.math.ethz.ch" <r-help at stat.math.ethz.ch>
Sent: 3/9/07 2:55 PM
Subject: [R] About "cex=": how to improve resolution?

Hi,

I need to plot a graph with a circle of radius 1 and
with a series of points of different size. The size of
these points compared to the fixed circle is important
and bears a meaning.

Here is the a simplified version of the code I'm
using:

x<-seq(0,2,by=0.2)
y<-x
z<-seq(0,1,by=0.1)
angle<-pi/180*c(0:359)
par(pty="s")
plot(-2:2,-2:2,type="n")
lines(cos(angle),sin(angle))
points(x,y,cex=z)

I obtain points of the same size when
cex=0.1/0.2/0.3/0.4 or cex=0.5/0.6/0.7 or
cex=0.8/0.9/1.0.

Please, does anyone know if there is a way of
improving the resolution of cex in order to have 10
points *all* of different size (respecting the above
written different values of cex)? The circle is fixed
of radius 1 and the values of cex are in relation with
that and they shouldn't be modified.

Thanks, Luca

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ruhil at ohio.edu  Sat Mar 10 22:30:09 2007
From: ruhil at ohio.edu (Anirudh V. S. Ruhil)
Date: Sat, 10 Mar 2007 16:30:09 -0500
Subject: [R] xtable with dupplicate rownames
In-Reply-To: <5D37A94201DCD1AECB71C42A@[192.168.1.100]>
References: <5D37A94201DCD1AECB71C42A@[192.168.1.100]>
Message-ID: <B4A81A65508BAE30C65B797F@[192.168.1.100]>

I patched a solution together; essentially by using cbind to stack the 
proportions as columns. Duplicate columns names appear to pose no problems 
for xtable but duplicate rownames do!



--On Saturday, March 10, 2007 1:11 PM -0500 "Anirudh V. S. Ruhil" 
<ruhil at ohio.edu> wrote:

: I have  a table (tab5; see below) with the first block showing the counts
: in a cross-tabulation, and the lower block reflecting the proportions of
: interest.
:
:> tab5
:              None Low Level Moderate Intense Very Intense Total
: None         0.00      3.00     0.00    0.00         1.00     4
: Low Level    1.00      2.00     0.00    0.00         0.00     3
: Moderate     2.00      3.00     2.00    0.00         0.00     7
: Intense      3.00      1.00     1.00    1.00         0.00     6
: Very Intense 0.00      1.00     2.00    0.00         0.00     3
: Total        6.00     10.00     5.00    1.00         1.00    23
: None         0.00      0.75     0.00    0.00         0.25     1
: Low Level    0.33      0.67     0.00    0.00         0.00     1
: Moderate     0.29      0.43     0.29    0.00         0.00     1
: Intense      0.50      0.17     0.17    0.17         0.00     1
: Very Intense 0.00      0.33     0.67    0.00         0.00     1
:
: When I execute
: 	foo5 <- xtable(format(tab5), caption = "XYZ")
:
: I get the following error message
: 	Warning message:
: 	some row.names duplicated: 7,8,9,10,11 --> row.names NOT used in:
: 	data.row.names(row.names, rowsi, i)
:
: Is there any way to force the duplicate row-names to be retained in
: xtable?
:
: Alternatively, is there a succinct way of building a cross-tabulation
: with  both counts and proportions in each cell, and then using xtable?
:
: thanks for any and all tips
:
: Ani
:
: Anirudh V. S. Ruhil, Ph.D.
: Sr. Research Associate
: Voinovich Center for Leadership and Public Affairs
: Ohio University
: Building 21, The Ridges
: Athens, OH 45701-2979
: Tel: 740.597.1949 | Fax: 740.597.3057
:
: ______________________________________________
: R-help at stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide
: http://www.R-project.org/posting-guide.html and provide commented,
: minimal, self-contained, reproducible code.



Anirudh V. S. Ruhil, Ph.D.
Sr. Research Associate
Voinovich Center for Leadership and Public Affairs
Ohio University
Building 21, The Ridges
Athens, OH 45701-2979
Tel: 740.597.1949 | Fax: 740.597.3057


From sapsi at pobox.com  Sun Mar 11 01:18:36 2007
From: sapsi at pobox.com (Saptarshi Guha)
Date: Sat, 10 Mar 2007 19:18:36 -0500
Subject: [R] Dyn.load  and Unload problems
Message-ID: <A50A352B-7B12-4AD1-A486-050832C83637@pobox.com>

Hi,
	I am trying to modify the devX11.c source in the directory R-2.4.1/ 
src/modules/X11.
	Under OS X 10.4.9 running the R gui (R version 2.4.1) on a  
Powerbook, i type this and get the subsequent error.

	> dyn.unload("/Library/Frameworks/R.framework/Resources/modules/ppc/ 
R_X11.so")
	Error in dyn.unload(x) : dynamic/shared library '/Library/Frameworks/ 
R.framework/Resources/modules/ppc/R_X11.so' was not loaded

	Yet, when i type the same line in terminal R, it works.

	Q1: How do find out what dlls have been loaded?

	Q2. In terminal R (with X11.app not running) , i unloaded R_X11.so  
(as above), which worked. I then loaded my own R_X11_b.so (displaying  
warning messages).
	I then unloaded this one and following occurred
	a) I ran X11() - it still ran, displaying my warnings - but it  
shouldn't have run since i unloaded all the R_X11.so (mine and R's)
	b)upon modifying the warning messages (again) and reloading, the  
warning messages don't reflect those of the new compiled devX11.c
	e.g in R_init_R_X11, which runs upon the "so" file being loaded, did  
not display the  modified warning messages - it showed the old ones  
(in (a))
	

	I would appreciate any help on this matter
	Rgds
	Saptarshi
	

	This R.version (identical for both the GUI and commandline versions)
	platform       powerpc-apple-darwin8.8.0
	arch           powerpc
	os             darwin8.8.0
	system         powerpc, darwin8.8.0
	status
	major          2
	minor          4.1
	year           2006
	month          12
	day            18
	svn rev        40228
	language       R
	version.string R version 2.4.1 (2006-12-18)


From villegas.ro at gmail.com  Sun Mar 11 02:00:16 2007
From: villegas.ro at gmail.com (R. Villegas)
Date: Sun, 11 Mar 2007 02:00:16 +0100
Subject: [R] Mac vs. PC
In-Reply-To: <45F221EC.8040308@missouri.edu>
References: <45F221EC.8040308@missouri.edu>
Message-ID: <29cf68350703101700l10253950ncfd6297ea1b4f0f6@mail.gmail.com>

2007/3/10, Richard Morey <moreyr at missouri.edu>:
> My adviser has a Mac notebook that he bought 6 months ago, and I have a
> PC notebook I bought a month ago. Here are the respective specs, as far
> as I know them:
>
> His:
> Mac OSX
> 1 GB DDR2 RAM
> Intel Core Duo, 2 GHz (2MB cache per core)
> Unknown HD
>
> Mine
> Windows Vista Home Premium 32bit
> 2 GB DDR2 RAM
> Intel Core 2 Duo, 2 GHz (4MB cache)
> 5400 RPM Hard Drive
>
>
> We are both running R. As a test to see whose laptop was faster, we
> decided to invert large random matrices. In R language, it looks like this:
>
> N=2000
> A=rnorm(N^2)
> A=matrix(A,ncol=N)
> solve(A)
>
> This creates a matrix of 4,000,000 random normal deviates and inverts
> it. His computer takes about 7 seconds, while mine takes about 14. Why
> the difference? I have several working hypotheses, and it would be
> interesting to see what you guys think.
>
> 1. R on Mac was compiled with optimizations for the CPU, with R for
> Windows was not. I could test this by compiling R with the Intel
> compiler, or GCC with optimizations, and seeing if I get a significant
> speed boost.
>
> 2. His R is 64 bit, while mine is for 32 bit windows. (I'm not sure how
> much of a diference that makes, or whether OSX is 64 bit.)
>
> 3. Data is getting swapped to the hard drive, and my hard drive is
> slower than his. I chose a slower hard drive to get bigger capacity for
> the price.
>
> This is not intended to be an OMG MACOS = TEH R0X0R thread. I'm just
> trying to explain the discrepency.
>
> Thanks!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Hi,

For Windows you can check versions of Rblas.dll linked against the
ATLAS library:
http://cran.r-project.org/bin/windows/contrib/ATLAS/

Rod.


From lauri.nikkinen at iki.fi  Sun Mar 11 13:40:30 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Sun, 11 Mar 2007 14:40:30 +0200
Subject: [R] recoding question
Message-ID: <ba8c09910703110540r78340ee1g71857dfc2470db67@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070311/1095f702/attachment.pl 

From twoutopias at gmail.com  Sun Mar 11 05:55:48 2007
From: twoutopias at gmail.com (Seth Roberts)
Date: Sat, 10 Mar 2007 20:55:48 -0800
Subject: [R] logging mouse clicks
Message-ID: <ffb035000703102055r13ce94c1o268391c3ec6ea708@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070310/462da50a/attachment.pl 

From klaster at karlin.mff.cuni.cz  Sun Mar 11 15:03:31 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Sun, 11 Mar 2007 15:03:31 +0100
Subject: [R] recoding question
In-Reply-To: <ba8c09910703110540r78340ee1g71857dfc2470db67@mail.gmail.com>
References: <ba8c09910703110540r78340ee1g71857dfc2470db67@mail.gmail.com>
Message-ID: <45F40C33.7000802@karlin.mff.cuni.cz>

Hi,
I suppose you wouldn't mind rearranging rows in your dataframe. So just 
use order() to sort your dataframe according to category (first) and 
points (second). Now add a new column with values 
(3,2,1,0,...,0,3,2,1,0...etc.) You can construct such vector using by() 
  or a simple for-loop. There will always be rep(0, n_i-3) if n_i is the 
number of observatons in category i.

If you do mind rearranging rows, keep the original permutation stored 
elsewhere a use order again in the end to restoring it back.

Petr


Lauri Nikkinen napsal(a):
> Hi R-users,
> 
> 
> I have a data frame like this:
> 
> photographer category picture points
> Hannu kalat limamikko 1
> Teemu kalat verkkovaja 3
> Hate kalat munat puoliks padassa 6
> Hannu kalat isokala 8
> Teemu kasvit, sienet ja muut eli?t harppi 2
> Hate kasvit, sienet ja muut eli?t pyynikki 2
> Petteri kasvit, sienet ja muut eli?t harmaalepp? 5
> Lauri kasvit, sienet ja muut eli?t lumipuu 9
> Teemu linnut kainostelua 1
> Petteri linnut viile? harakka 2
> Lauri linnut hip? 3
> Teemu linnut pinnalla 5
> Lassi linnut el?m?n viiva 7
> Lassi nis?kk??t pedot 1
> Teemu nis?kk??t Homo sapiens angelus 3
> Hate nis?kk??t madekoukkujen suojelupyhimys 3
> Lassi nis?kk??t portsan kundi 3
> Hannu nis?kk??t maukasta marmeladia 8
> Teemu reissut puikot 1
> Lassi reissut ajatelkaa, jos H?nt? ei olisikaan 2
> Lauri reissut k?kar 3
> Hate reissut matka aikaan joka ei en?? palaa 3
> Hannu reissut skrinnareita 4
> Lauri reissut j??puut 5
> Hannu selk?rangattomat kaapin alta l?ytynyt 2
> Hate selk?rangattomat vailla armeerausta 2
> Teemu selk?rangattomat portinvartija 6
> Hannu selk?rangattomat kaapin alta l?ytynyt2 8
> 
> 
> I want to make a new column named as "final_points" to the data frame that
> has numbers following this kind of logic: the highest number in column
> "points" in each category will receive a number 3 into new column
> "final_points". The second highest number will receive a number 1 into
> "final_points" column. Others will receive a 0. Here is how it should look
> like after the code has been run:
> 
> 
> 
> photographer category picture points final_points
> Hannu kalat limamikko 1 0
> Teemu kalat verkkovaja 3 0
> Hate kalat munat puoliks padassa 6 1
> Hannu kalat isokala 8 3
> Teemu kasvit, sienet ja muut eli?t harppi 2 0
> Hate kasvit, sienet ja muut eli?t pyynikki 2 0
> Petteri kasvit, sienet ja muut eli?t harmaalepp? 5 1
> Lauri kasvit, sienet ja muut eli?t lumipuu 9 3
> Teemu linnut kainostelua 1 0
> Petteri linnut viile? harakka 2 0
> Lauri linnut hip? 3 0
> Teemu linnut pinnalla 5 1
> Lassi linnut el?m?n viiva 7 3
> Lassi nis?kk??t pedot 1 0
> Teemu nis?kk??t Homo sapiens angelus 3 1
> Hate nis?kk??t madekoukkujen suojelupyhimys 3 1
> Lassi nis?kk??t portsan kundi 3 1
> Hannu nis?kk??t maukasta marmeladia 8 3
> Teemu reissut puikot 1 0
> Lassi reissut ajatelkaa, jos H?nt? ei olisikaan 2 0
> Lauri reissut k?kar 3 0
> Hate reissut matka aikaan joka ei en?? palaa 3 0
> Hannu reissut skrinnareita 4 1
> Lauri reissut j??puut 5 3
> Hannu selk?rangattomat kaapin alta l?ytynyt 2 0
> Hate selk?rangattomat vailla armeerausta 2 0
> Teemu selk?rangattomat portinvartija 6 1
> Hannu selk?rangattomat kaapin alta l?ytynyt2 8 3
> 
> 
> 
> How do I do this with R?
> 
> 
> 
> -Lauri
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From kubovy at virginia.edu  Sun Mar 11 16:15:42 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Sun, 11 Mar 2007 11:15:42 -0400
Subject: [R] logging mouse clicks
In-Reply-To: <ffb035000703102055r13ce94c1o268391c3ec6ea708@mail.gmail.com>
References: <ffb035000703102055r13ce94c1o268391c3ec6ea708@mail.gmail.com>
Message-ID: <0AF12FCA-E972-4B66-ABE3-9D9B55ACF827@virginia.edu>

Hi Seth,

On Mar 10, 2007, at 11:55 PM, Seth Roberts wrote:

> How can I use R to record the time of a mouse click?

Perhaps ievent.wait() in iplots? (I haven't used this.) I found all  
sorts of functions (most of which I couldn't understand) by ?event.
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From knoblauch at lyon.inserm.fr  Sun Mar 11 18:47:54 2007
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Sun, 11 Mar 2007 16:47:54 -0100 (CET)
Subject: [R] using scan to record user's input
Message-ID: <49262.82.231.93.240.1173628074.squirrel@webmail.lyon.inserm.fr>

I'm using scan in a script to record a series of responses of the user as
a function
of some graphs that I put up on the screen.  A toy version would be,

y <- rep(NA, 3)
for (ix in seq( length(y) ) ) {
	y[ix] <- scan( n = 1 )
	}

However, if I include any code after this loop, for example,
y <- rep(NA, 3)
for (ix in seq( length(y) ) ) {
	y[ix] <- scan( n = 1 )
	}
y

I get an error after I've entered my first number.

1: y
1: 3
Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  :
	scan() expected 'a real', got 'y'

scan seems to be trying to read the commands after the loop.  In the real
case,
the loop is longer and there are more commands after the loop.
If I have more commands, I get the error right away, before the prompt to
enter a number.

I didn't see anything in the scan help that would allow me to circumvent
this error.

Can anyone suggest a way to get around this error.

Thank you.

Ken

Here is my info

sessionInfo()
R version 2.4.1 Patched (2007-01-23 r40561)
i386-apple-darwin8.8.1

locale:
C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
[7] "base"

other attached packages:
    MASS
"7.2-31"


-- 
Ken Knoblauch
Inserm U846
Institut Cellule Souche et Cerveau
D?partement Neurosciences Int?gratives
18 avenue du Doyen L?pine
69500 Bron
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: +33 (0)6 84 10 64 10
http://www.pizzerialesgemeaux.com/u846/


From pcallier at stanford.edu  Sun Mar 11 17:24:21 2007
From: pcallier at stanford.edu (Patrick Callier)
Date: Sun, 11 Mar 2007 09:24:21 -0700
Subject: [R] using scan to record user's input
In-Reply-To: <49262.82.231.93.240.1173628074.squirrel@webmail.lyon.inserm.fr>
References: <49262.82.231.93.240.1173628074.squirrel@webmail.lyon.inserm.fr>
Message-ID: <94bc9e4f0703110924v13c6c864ve1f43c01cf0f1531@mail.gmail.com>

Silly question: Are you 'source'-ing this code or copying and pasting
it from somewhere?  I don't have any problems running your above
example on a PPC Mac with almost the same version of R (using
'source').

Here's my info:
R version 2.4.1 (2006-12-18)
powerpc-apple-darwin8.8.0

locale:
en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"
"methods"   "base"

other attached packages:
      MASS KernSmooth
  "7.2-30"  "2.22-19"

Otherwise, I'm as stumped as you are.

On 3/11/07, Ken Knoblauch <knoblauch at lyon.inserm.fr> wrote:
> I'm using scan in a script to record a series of responses of the user as
> a function
> of some graphs that I put up on the screen.  A toy version would be,
>
> y <- rep(NA, 3)
> for (ix in seq( length(y) ) ) {
>         y[ix] <- scan( n = 1 )
>         }
>
> However, if I include any code after this loop, for example,
> y <- rep(NA, 3)
> for (ix in seq( length(y) ) ) {
>         y[ix] <- scan( n = 1 )
>         }
> y
>
> I get an error after I've entered my first number.
>
> 1: y
> 1: 3
> Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  :
>         scan() expected 'a real', got 'y'
>
> scan seems to be trying to read the commands after the loop.  In the real
> case,
> the loop is longer and there are more commands after the loop.
> If I have more commands, I get the error right away, before the prompt to
> enter a number.
>
> I didn't see anything in the scan help that would allow me to circumvent
> this error.
>
> Can anyone suggest a way to get around this error.
>
> Thank you.
>
> Ken
>
> Here is my info
>
> sessionInfo()
> R version 2.4.1 Patched (2007-01-23 r40561)
> i386-apple-darwin8.8.1
>
> locale:
> C
>
> attached base packages:
> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
> [7] "base"
>
> other attached packages:
>     MASS
> "7.2-31"
>
>
> --
> Ken Knoblauch
> Inserm U846
> Institut Cellule Souche et Cerveau
> D?partement Neurosciences Int?gratives
> 18 avenue du Doyen L?pine
> 69500 Bron
> France
> tel: +33 (0)4 72 91 34 77
> fax: +33 (0)4 72 91 34 61
> portable: +33 (0)6 84 10 64 10
> http://www.pizzerialesgemeaux.com/u846/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From maciej.rhelp at gmail.com  Sun Mar 11 18:26:20 2007
From: maciej.rhelp at gmail.com (Maciej Radziejewski)
Date: Sun, 11 Mar 2007 18:26:20 +0100
Subject: [R] Using large datasets: can I overload the subscript operator?
In-Reply-To: <JEPBFL$AB1C3D8D473746125C02EA1A0401BC2F@terra.com.br>
References: <JEPBFL$AB1C3D8D473746125C02EA1A0401BC2F@terra.com.br>
Message-ID: <732eec150703111026ra35f86fwc00b167a8f825378@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070311/a43cfaef/attachment.pl 

From knoblauch at lyon.inserm.fr  Sun Mar 11 20:17:10 2007
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Sun, 11 Mar 2007 18:17:10 -0100 (CET)
Subject: [R] using scan to record user's input
In-Reply-To: <94bc9e4f0703110924v13c6c864ve1f43c01cf0f1531@mail.gmail.com>
References: <49262.82.231.93.240.1173628074.squirrel@webmail.lyon.inserm.fr>
	<94bc9e4f0703110924v13c6c864ve1f43c01cf0f1531@mail.gmail.com>
Message-ID: <49682.82.231.93.240.1173633430.squirrel@webmail.lyon.inserm.fr>

No, it's a good question.  I was using the apple-return feature of the editor
on the R.app from Mac.  Is that more like sourcing or copy and pasting?
I was planning eventually to source the file or put the code in a function.


I'll try it on a PPC Mac later and see if it works there.  Thanks.

best,

Ken

Patrick Callier wrote:
> Silly question: Are you 'source'-ing this code or copying and pasting
> it from somewhere?  I don't have any problems running your above
> example on a PPC Mac with almost the same version of R (using
> 'source').
>
> Here's my info:
> R version 2.4.1 (2006-12-18)
> powerpc-apple-darwin8.8.0
>
> locale:
> en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>
> attached base packages:
> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"
> "methods"   "base"
>
> other attached packages:
>       MASS KernSmooth
>   "7.2-30"  "2.22-19"
>
> Otherwise, I'm as stumped as you are.
>
> On 3/11/07, Ken Knoblauch <knoblauch at lyon.inserm.fr> wrote:
>> I'm using scan in a script to record a series of responses of the user
>> as
>> a function
>> of some graphs that I put up on the screen.  A toy version would be,
>>
>> y <- rep(NA, 3)
>> for (ix in seq( length(y) ) ) {
>>         y[ix] <- scan( n = 1 )
>>         }
>>
>> However, if I include any code after this loop, for example,
>> y <- rep(NA, 3)
>> for (ix in seq( length(y) ) ) {
>>         y[ix] <- scan( n = 1 )
>>         }
>> y
>>
>> I get an error after I've entered my first number.
>>
>> 1: y
>> 1: 3
>> Error in scan(file, what, nmax, sep, dec, quote, skip, nlines,
>> na.strings,  :
>>         scan() expected 'a real', got 'y'
>>
>> scan seems to be trying to read the commands after the loop.  In the
>> real
>> case,
>> the loop is longer and there are more commands after the loop.
>> If I have more commands, I get the error right away, before the prompt
>> to
>> enter a number.
>>
>> I didn't see anything in the scan help that would allow me to circumvent
>> this error.
>>
>> Can anyone suggest a way to get around this error.
>>
>> Thank you.
>>
>> Ken
>>
>> Here is my info
>>
>> sessionInfo()
>> R version 2.4.1 Patched (2007-01-23 r40561)
>> i386-apple-darwin8.8.1
>>
>> locale:
>> C
>>
>> attached base packages:
>> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"
>> "methods"
>> [7] "base"
>>
>> other attached packages:
>>     MASS
>> "7.2-31"
>>
>>
>> --
>> Ken Knoblauch
>> Inserm U846
>> Institut Cellule Souche et Cerveau
>> D?partement Neurosciences Int?gratives
>> 18 avenue du Doyen L?pine
>> 69500 Bron
>> France
>> tel: +33 (0)4 72 91 34 77
>> fax: +33 (0)4 72 91 34 61
>> portable: +33 (0)6 84 10 64 10
>> http://www.pizzerialesgemeaux.com/u846/
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


-- 
Ken Knoblauch
Inserm U846
Institut Cellule Souche et Cerveau
D?partement Neurosciences Int?gratives
18 avenue du Doyen L?pine
69500 Bron
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: +33 (0)6 84 10 64 10
http://www.pizzerialesgemeaux.com/u846/


From knoblauch at lyon.inserm.fr  Sun Mar 11 20:21:34 2007
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Sun, 11 Mar 2007 18:21:34 -0100 (CET)
Subject: [R] using scan to record user's input
In-Reply-To: <94bc9e4f0703110924v13c6c864ve1f43c01cf0f1531@mail.gmail.com>
References: <49262.82.231.93.240.1173628074.squirrel@webmail.lyon.inserm.fr>
	<94bc9e4f0703110924v13c6c864ve1f43c01cf0f1531@mail.gmail.com>
Message-ID: <49705.82.231.93.240.1173633694.squirrel@webmail.lyon.inserm.fr>

I get it on a powerpc Mac, too,

> y <- rep(NA, 3)
> for (ix in seq( length(y) ) ) {
+          y[ix] <- scan( n = 1 )
+         }
1: 2
Read 1 item
1: 3
Read 1 item
1: 4
Read 1 item
> y
[1] 2 3 4
> y <- rep(NA, 3)
> for (ix in seq( length(y) ) ) {
+          y[ix] <- scan( n = 1 )
+         }
1: y
1: 5
Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  :
	scan() expected 'a real', got 'y'
+

R version 2.4.1 Patched (2007-01-05 r40386)
powerpc-apple-darwin8.8.0

locale:
en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
[7] "base"

Thanks.

ken

Patrick Callier wrote:
> Silly question: Are you 'source'-ing this code or copying and pasting
> it from somewhere?  I don't have any problems running your above
> example on a PPC Mac with almost the same version of R (using
> 'source').
>
> Here's my info:
> R version 2.4.1 (2006-12-18)
> powerpc-apple-darwin8.8.0
>
> locale:
> en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>
> attached base packages:
> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"
> "methods"   "base"
>
> other attached packages:
>       MASS KernSmooth
>   "7.2-30"  "2.22-19"
>
> Otherwise, I'm as stumped as you are.
>
> On 3/11/07, Ken Knoblauch <knoblauch at lyon.inserm.fr> wrote:
>> I'm using scan in a script to record a series of responses of the user
>> as
>> a function
>> of some graphs that I put up on the screen.  A toy version would be,
>>
>> y <- rep(NA, 3)
>> for (ix in seq( length(y) ) ) {
>>         y[ix] <- scan( n = 1 )
>>         }
>>
>> However, if I include any code after this loop, for example,
>> y <- rep(NA, 3)
>> for (ix in seq( length(y) ) ) {
>>         y[ix] <- scan( n = 1 )
>>         }
>> y
>>
>> I get an error after I've entered my first number.
>>
>> 1: y
>> 1: 3
>> Error in scan(file, what, nmax, sep, dec, quote, skip, nlines,
>> na.strings,  :
>>         scan() expected 'a real', got 'y'
>>
>> scan seems to be trying to read the commands after the loop.  In the
>> real
>> case,
>> the loop is longer and there are more commands after the loop.
>> If I have more commands, I get the error right away, before the prompt
>> to
>> enter a number.
>>
>> I didn't see anything in the scan help that would allow me to circumvent
>> this error.
>>
>> Can anyone suggest a way to get around this error.
>>
>> Thank you.
>>
>> Ken
>>
>> Here is my info
>>
>> sessionInfo()
>> R version 2.4.1 Patched (2007-01-23 r40561)
>> i386-apple-darwin8.8.1
>>
>> locale:
>> C
>>
>> attached base packages:
>> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"
>> "methods"
>> [7] "base"
>>
>> other attached packages:
>>     MASS
>> "7.2-31"
>>
>>
>> --
>> Ken Knoblauch
>> Inserm U846
>> Institut Cellule Souche et Cerveau
>> D?partement Neurosciences Int?gratives
>> 18 avenue du Doyen L?pine
>> 69500 Bron
>> France
>> tel: +33 (0)4 72 91 34 77
>> fax: +33 (0)4 72 91 34 61
>> portable: +33 (0)6 84 10 64 10
>> http://www.pizzerialesgemeaux.com/u846/
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


-- 
Ken Knoblauch
Inserm U846
Institut Cellule Souche et Cerveau
D?partement Neurosciences Int?gratives
18 avenue du Doyen L?pine
69500 Bron
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: +33 (0)6 84 10 64 10
http://www.pizzerialesgemeaux.com/u846/


From pcallier at stanford.edu  Sun Mar 11 18:49:36 2007
From: pcallier at stanford.edu (Patrick Callier)
Date: Sun, 11 Mar 2007 10:49:36 -0700
Subject: [R] using scan to record user's input
In-Reply-To: <49705.82.231.93.240.1173633694.squirrel@webmail.lyon.inserm.fr>
References: <49262.82.231.93.240.1173628074.squirrel@webmail.lyon.inserm.fr>
	<94bc9e4f0703110924v13c6c864ve1f43c01cf0f1531@mail.gmail.com>
	<49705.82.231.93.240.1173633694.squirrel@webmail.lyon.inserm.fr>
Message-ID: <94bc9e4f0703111049j3b501214vdb91b43aa167a4bd@mail.gmail.com>

The apple-enter thing works like copy and paste would, as far as I can
tell (I get a similar bug).  The deal is that it's entering each line
in one at a time, so the for-loop starts running after the last brace
and gets as its input the next line sent from the console.

Either one of the solutions you propose (using source or putting it in
a function) should fix your problem.

Best,
Pat

On 3/11/07, Ken Knoblauch <knoblauch at lyon.inserm.fr> wrote:
> I get it on a powerpc Mac, too,
>
> > y <- rep(NA, 3)
> > for (ix in seq( length(y) ) ) {
> +          y[ix] <- scan( n = 1 )
> +         }
> 1: 2
> Read 1 item
> 1: 3
> Read 1 item
> 1: 4
> Read 1 item
> > y
> [1] 2 3 4
> > y <- rep(NA, 3)
> > for (ix in seq( length(y) ) ) {
> +          y[ix] <- scan( n = 1 )
> +         }
> 1: y
> 1: 5
> Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  :
>         scan() expected 'a real', got 'y'
> +
>
> R version 2.4.1 Patched (2007-01-05 r40386)
> powerpc-apple-darwin8.8.0
>
> locale:
> en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>
> attached base packages:
> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
> [7] "base"
>
> Thanks.
>
> ken
>
> Patrick Callier wrote:
> > Silly question: Are you 'source'-ing this code or copying and pasting
> > it from somewhere?  I don't have any problems running your above
> > example on a PPC Mac with almost the same version of R (using
> > 'source').
> >
> > Here's my info:
> > R version 2.4.1 (2006-12-18)
> > powerpc-apple-darwin8.8.0
> >
> > locale:
> > en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
> >
> > attached base packages:
> > [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"
> > "methods"   "base"
> >
> > other attached packages:
> >       MASS KernSmooth
> >   "7.2-30"  "2.22-19"
> >
> > Otherwise, I'm as stumped as you are.
> >
> > On 3/11/07, Ken Knoblauch <knoblauch at lyon.inserm.fr> wrote:
> >> I'm using scan in a script to record a series of responses of the user
> >> as
> >> a function
> >> of some graphs that I put up on the screen.  A toy version would be,
> >>
> >> y <- rep(NA, 3)
> >> for (ix in seq( length(y) ) ) {
> >>         y[ix] <- scan( n = 1 )
> >>         }
> >>
> >> However, if I include any code after this loop, for example,
> >> y <- rep(NA, 3)
> >> for (ix in seq( length(y) ) ) {
> >>         y[ix] <- scan( n = 1 )
> >>         }
> >> y
> >>
> >> I get an error after I've entered my first number.
> >>
> >> 1: y
> >> 1: 3
> >> Error in scan(file, what, nmax, sep, dec, quote, skip, nlines,
> >> na.strings,  :
> >>         scan() expected 'a real', got 'y'
> >>
> >> scan seems to be trying to read the commands after the loop.  In the
> >> real
> >> case,
> >> the loop is longer and there are more commands after the loop.
> >> If I have more commands, I get the error right away, before the prompt
> >> to
> >> enter a number.
> >>
> >> I didn't see anything in the scan help that would allow me to circumvent
> >> this error.
> >>
> >> Can anyone suggest a way to get around this error.
> >>
> >> Thank you.
> >>
> >> Ken
> >>
> >> Here is my info
> >>
> >> sessionInfo()
> >> R version 2.4.1 Patched (2007-01-23 r40561)
> >> i386-apple-darwin8.8.1
> >>
> >> locale:
> >> C
> >>
> >> attached base packages:
> >> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"
> >> "methods"
> >> [7] "base"
> >>
> >> other attached packages:
> >>     MASS
> >> "7.2-31"
> >>
> >>
> >> --
> >> Ken Knoblauch
> >> Inserm U846
> >> Institut Cellule Souche et Cerveau
> >> D?partement Neurosciences Int?gratives
> >> 18 avenue du Doyen L?pine
> >> 69500 Bron
> >> France
> >> tel: +33 (0)4 72 91 34 77
> >> fax: +33 (0)4 72 91 34 61
> >> portable: +33 (0)6 84 10 64 10
> >> http://www.pizzerialesgemeaux.com/u846/
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
>
>
> --
> Ken Knoblauch
> Inserm U846
> Institut Cellule Souche et Cerveau
> D?partement Neurosciences Int?gratives
> 18 avenue du Doyen L?pine
> 69500 Bron
> France
> tel: +33 (0)4 72 91 34 77
> fax: +33 (0)4 72 91 34 61
> portable: +33 (0)6 84 10 64 10
> http://www.pizzerialesgemeaux.com/u846/
>
>


From knoblauch at lyon.inserm.fr  Sun Mar 11 20:56:38 2007
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Sun, 11 Mar 2007 18:56:38 -0100 (CET)
Subject: [R] using scan to record user's input
In-Reply-To: <94bc9e4f0703111049j3b501214vdb91b43aa167a4bd@mail.gmail.com>
References: <49262.82.231.93.240.1173628074.squirrel@webmail.lyon.inserm.fr>
	<94bc9e4f0703110924v13c6c864ve1f43c01cf0f1531@mail.gmail.com>
	<49705.82.231.93.240.1173633694.squirrel@webmail.lyon.inserm.fr>
	<94bc9e4f0703111049j3b501214vdb91b43aa167a4bd@mail.gmail.com>
Message-ID: <49812.82.231.93.240.1173635798.squirrel@webmail.lyon.inserm.fr>

Works on the powerpc with source.  I'll verify later on the Mac Intel.
Thank you very much.

ken

Patrick Callier wrote:
> The apple-enter thing works like copy and paste would, as far as I can
> tell (I get a similar bug).  The deal is that it's entering each line
> in one at a time, so the for-loop starts running after the last brace
> and gets as its input the next line sent from the console.
>
> Either one of the solutions you propose (using source or putting it in
> a function) should fix your problem.
>
> Best,
> Pat
>
> On 3/11/07, Ken Knoblauch <knoblauch at lyon.inserm.fr> wrote:
>> I get it on a powerpc Mac, too,
>>
>> > y <- rep(NA, 3)
>> > for (ix in seq( length(y) ) ) {
>> +          y[ix] <- scan( n = 1 )
>> +         }
>> 1: 2
>> Read 1 item
>> 1: 3
>> Read 1 item
>> 1: 4
>> Read 1 item
>> > y
>> [1] 2 3 4
>> > y <- rep(NA, 3)
>> > for (ix in seq( length(y) ) ) {
>> +          y[ix] <- scan( n = 1 )
>> +         }
>> 1: y
>> 1: 5
>> Error in scan(file, what, nmax, sep, dec, quote, skip, nlines,
>> na.strings,  :
>>         scan() expected 'a real', got 'y'
>> +
>>
>> R version 2.4.1 Patched (2007-01-05 r40386)
>> powerpc-apple-darwin8.8.0
>>
>> locale:
>> en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>>
>> attached base packages:
>> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"
>> "methods"
>> [7] "base"
>>
>> Thanks.
>>
>> ken
>>
>> Patrick Callier wrote:
>> > Silly question: Are you 'source'-ing this code or copying and pasting
>> > it from somewhere?  I don't have any problems running your above
>> > example on a PPC Mac with almost the same version of R (using
>> > 'source').
>> >
>> > Here's my info:
>> > R version 2.4.1 (2006-12-18)
>> > powerpc-apple-darwin8.8.0
>> >
>> > locale:
>> > en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>> >
>> > attached base packages:
>> > [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"
>> > "methods"   "base"
>> >
>> > other attached packages:
>> >       MASS KernSmooth
>> >   "7.2-30"  "2.22-19"
>> >
>> > Otherwise, I'm as stumped as you are.
>> >
>> > On 3/11/07, Ken Knoblauch <knoblauch at lyon.inserm.fr> wrote:
>> >> I'm using scan in a script to record a series of responses of the
>> user
>> >> as
>> >> a function
>> >> of some graphs that I put up on the screen.  A toy version would be,
>> >>
>> >> y <- rep(NA, 3)
>> >> for (ix in seq( length(y) ) ) {
>> >>         y[ix] <- scan( n = 1 )
>> >>         }
>> >>
>> >> However, if I include any code after this loop, for example,
>> >> y <- rep(NA, 3)
>> >> for (ix in seq( length(y) ) ) {
>> >>         y[ix] <- scan( n = 1 )
>> >>         }
>> >> y
>> >>
>> >> I get an error after I've entered my first number.
>> >>
>> >> 1: y
>> >> 1: 3
>> >> Error in scan(file, what, nmax, sep, dec, quote, skip, nlines,
>> >> na.strings,  :
>> >>         scan() expected 'a real', got 'y'
>> >>
>> >> scan seems to be trying to read the commands after the loop.  In the
>> >> real
>> >> case,
>> >> the loop is longer and there are more commands after the loop.
>> >> If I have more commands, I get the error right away, before the
>> prompt
>> >> to
>> >> enter a number.
>> >>
>> >> I didn't see anything in the scan help that would allow me to
>> circumvent
>> >> this error.
>> >>
>> >> Can anyone suggest a way to get around this error.
>> >>
>> >> Thank you.
>> >>
>> >> Ken
>> >>
>> >> Here is my info
>> >>
>> >> sessionInfo()
>> >> R version 2.4.1 Patched (2007-01-23 r40561)
>> >> i386-apple-darwin8.8.1
>> >>
>> >> locale:
>> >> C
>> >>
>> >> attached base packages:
>> >> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"
>> >> "methods"
>> >> [7] "base"
>> >>
>> >> other attached packages:
>> >>     MASS
>> >> "7.2-31"
>> >>
>> >>
>> >> --
>> >> Ken Knoblauch
>> >> Inserm U846
>> >> Institut Cellule Souche et Cerveau
>> >> D?partement Neurosciences Int?gratives
>> >> 18 avenue du Doyen L?pine
>> >> 69500 Bron
>> >> France
>> >> tel: +33 (0)4 72 91 34 77
>> >> fax: +33 (0)4 72 91 34 61
>> >> portable: +33 (0)6 84 10 64 10
>> >> http://www.pizzerialesgemeaux.com/u846/
>> >>
>> >> ______________________________________________
>> >> R-help at stat.math.ethz.ch mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >
>>
>>
>> --
>> Ken Knoblauch
>> Inserm U846
>> Institut Cellule Souche et Cerveau
>> D?partement Neurosciences Int?gratives
>> 18 avenue du Doyen L?pine
>> 69500 Bron
>> France
>> tel: +33 (0)4 72 91 34 77
>> fax: +33 (0)4 72 91 34 61
>> portable: +33 (0)6 84 10 64 10
>> http://www.pizzerialesgemeaux.com/u846/
>>
>>
>


-- 
Ken Knoblauch
Inserm U846
Institut Cellule Souche et Cerveau
D?partement Neurosciences Int?gratives
18 avenue du Doyen L?pine
69500 Bron
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: +33 (0)6 84 10 64 10
http://www.pizzerialesgemeaux.com/u846/


From yvonnick.noel at uhb.fr  Sun Mar 11 19:08:42 2007
From: yvonnick.noel at uhb.fr (NOEL Yvonnick)
Date: Sun, 11 Mar 2007 19:08:42 +0100
Subject: [R] Sys.setlocale("LC_CTYPE","fr_FR.UTF-8")
Message-ID: <45F445AA.5050403@uhb.fr>

Dear R users,

I'm trying to have a gWiddgetsRGtk2 script run under R-2.4.1. The script 
run OK under Linux but all accentuated characters appear as "?" when the 
script is run under Windows.

As Gtk+ requires UTF-8, I thought it was the source of the problem and 
tried to change the default encoding (1252) in the following way:

 >Sys.setlocale("LC_CTYPE","fr_FR.UTF-8")
[1]  "fr_FR.UTF-8"

But when I try as a check:
 >Sys.getlocale("LC_TYPE")
[1] "French_France.1252"

So it seems that Sys.setlocale() has not changed anything. What am I 
doing wrong ?

Thank you for your help,

Yvonnick Noel
U. of Rennes
France


From vishalm1975 at gmail.com  Sun Mar 11 20:46:24 2007
From: vishalm1975 at gmail.com (Vishal Mehta)
Date: Sun, 11 Mar 2007 15:46:24 -0400
Subject: [R] fitting a mixed exponential distribution
Message-ID: <955d14510703111246h2aef7454t4277e9ae62d64209@mail.gmail.com>

Hi all,

I am attempting to fit, and test the goodness of fit of, a mixed
exponential distribution to my dataset which consists of 15minute
rainfall intensity data. FYI, the dataset spanning approx.2 years and
7 rainfall stations consists of some three hundred thousand 15min data
records, of which some 30 thousand are non-zero rainfall amounts.

Could anyone please tell me how i could do this/which package i could
use in R? I am familiar with MASS functions fitdistr which i've used
for the single exponential distributions, but have not yet
successfully found out how to fit mixed distributions. The single
gamma and exponential distributions did not fit the data well at all,
hence my interest in fitting a mixed exponential distribution.

Also, i thought of formulating the likelihood function and using mle()
in package stats4-- but had trouble writing down the likelihood
function for the case of a mixed exponential distribution...

thanks,
vishal

Vishal Mehta
PhD Candidate
Crop and Soil Science
Cornell University


From gnvshqp at gmail.com  Mon Mar 12 00:13:35 2007
From: gnvshqp at gmail.com (gnv shqp)
Date: Sun, 11 Mar 2007 19:13:35 -0400
Subject: [R] read.table for a subset of data
Message-ID: <6ab3ba540703111613m197c79a4w6c09a73f465ec06e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070311/fac4b4e7/attachment.pl 

From canamika16 at yahoo.com  Mon Mar 12 00:21:05 2007
From: canamika16 at yahoo.com (Anamika Chaudhuri)
Date: Sun, 11 Mar 2007 16:21:05 -0700 (PDT)
Subject: [R] MCMC logit
In-Reply-To: <000a01c76298$0ab14810$7c94100a@win.ad.jhu.edu>
Message-ID: <277054.99375.qm@web34104.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070311/704c5f96/attachment.pl 

From liuwensui at gmail.com  Mon Mar 12 00:40:09 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Sun, 11 Mar 2007 19:40:09 -0400
Subject: [R] read.table for a subset of data
In-Reply-To: <6ab3ba540703111613m197c79a4w6c09a73f465ec06e@mail.gmail.com>
References: <6ab3ba540703111613m197c79a4w6c09a73f465ec06e@mail.gmail.com>
Message-ID: <1115a2b00703111640l3956c6bdq48cc5f43389765b4@mail.gmail.com>

as far as I've know, I don't think you can do so with read.table. But
I am also thinking about RODBC and wondering if you could assign a DSN
to your .csv file and then use sql to fetch the subset.

On 3/11/07, gnv shqp <gnvshqp at gmail.com> wrote:
> Hi R-experts,
>
> I have data from four conditions of an experiment.  I tried to create four
> subsets of the data with read.table, for example,
> read.table("Experiment.csv",subset=(condition=="1"))
> .  I found a similar post in the archive, but the answer to that post was
> no.   Any  new ideas about  reading subsets of data with read.table?
>
> Thanks!
>
> Feng
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From renaud.lacour at free.fr  Mon Mar 12 01:37:14 2007
From: renaud.lacour at free.fr (Renaud Lacour)
Date: Mon, 12 Mar 2007 01:37:14 +0100
Subject: [R] Export successively displayed graphics
Message-ID: <20070312013714.06bdebcc@cachan.home-dn.net>

Hello,

I am running R version 2.4.1 on Debian.

I am using some R functions which produce several graphics displayed
successively with an interactive prompt between each graph. For example,
the plot.varest (vars) function produces such output (one graph per time
series on which the VAR model is fitted). Plots on GARCH models also
produce such outputs.

Moreover, the frontend I am using for R (RkWard) does not enable me to
export graphs as vectorial image files.

How can I export such graphs as separate (e.g.) Postscript files ?


par(ask=FALSE) didn't help.

Thank you for your answer.
-- 
Renaud Lacour <renaud.lacour at free.fr>


From ivowel at gmail.com  Mon Mar 12 01:54:32 2007
From: ivowel at gmail.com (ivo welch)
Date: Sun, 11 Mar 2007 20:54:32 -0400
Subject: [R] write.csv feature/bug with pipe
Message-ID: <50d1c22d0703111754s7b85da6cre8f8be87dfc5ef9f@mail.gmail.com>

gentoo linux, version 2.4.1:

> d= as.data.frame(matrix(1:20, 4, 5))
> d
  V1 V2 V3 V4 V5
1  1  5  9 13 17
2  2  6 10 14 18
3  3  7 11 15 19
4  4  8 12 16 20
> write.csv(d, file="d1.csv");
> write.csv(d, file=pipe("cat > d2.csv"))
> write.csv(d, file=pipe("gzip -c > d3.csv.gz"), col.names=T)
Warning message:
attempt to change 'col.names' ignored in: write.csv(d, file =
pipe("gzip -c > d4.csv.gz"), col.names = T)

exit and

$ head d1.csv
"","V1","V2","V3","V4","V5"
"1",1,5,9,13,17
"2",2,6,10,14,18
"3",3,7,11,15,19
"4",4,8,12,16,20
$ head d2.csv
"1",1,5,9,13,17
"2",2,6,10,14,18
"3",3,7,11,15,19
"4",4,8,12,16,20

is it a bug or a feature that when pipe is used, the col.names is set
to false?  I guess I can invoke write.table to get the headers back.

regards,

/iaw


From simon at hellscho.com.au  Mon Mar 12 02:17:52 2007
From: simon at hellscho.com.au (simon gatehouse)
Date: Mon, 12 Mar 2007 12:17:52 +1100
Subject: [R] Tracking when an object/function was  modified (Mona Kanaan)
In-Reply-To: <mailman.11.1173438003.25210.r-help@stat.math.ethz.ch>
References: <mailman.11.1173438003.25210.r-help@stat.math.ethz.ch>
Message-ID: <45F4AA40.5040902@hellscho.com.au>

You might find this useful.

I have modified the R 'edit' function by adding

                           attr(x,"date.modified") <- 
substring(Sys.time(),1,19)

before the last line.

I then changed to ls() to sort functions, paste function name and time 
stamp from the contents of the "date.modified" attribute before output.  
Not particularly tidy but functional.
Only useful for functions that go through the editor.


From jholtman at gmail.com  Mon Mar 12 03:16:15 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 11 Mar 2007 21:16:15 -0500
Subject: [R] read.table for a subset of data
In-Reply-To: <6ab3ba540703111613m197c79a4w6c09a73f465ec06e@mail.gmail.com>
References: <6ab3ba540703111613m197c79a4w6c09a73f465ec06e@mail.gmail.com>
Message-ID: <644e1f320703111916k6244837fld26c32eba316c83b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070311/c4b003a8/attachment.pl 

From liuwensui at gmail.com  Mon Mar 12 03:26:09 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Sun, 11 Mar 2007 22:26:09 -0400
Subject: [R] read.table for a subset of data
In-Reply-To: <644e1f320703111916k6244837fld26c32eba316c83b@mail.gmail.com>
References: <6ab3ba540703111613m197c79a4w6c09a73f465ec06e@mail.gmail.com>
	<644e1f320703111916k6244837fld26c32eba316c83b@mail.gmail.com>
Message-ID: <1115a2b00703111926t2ec62802he46c748da9e93ec5@mail.gmail.com>

Jim,

Glad to see your reply.

Refering to your email, what if I just want to read 10 rows from a csv
table with 100000 rows? Do you think it a waste of resource to read
the whole table in?
Anything thought?

wensui

On 3/11/07, jim holtman <jholtman at gmail.com> wrote:
> Why cann't you read in the whole data set and then create the subsets?  This
> is easily done with 'split'.  If the data is too large, then consider a data
> base.
>
> On 3/11/07, gnv shqp <gnvshqp at gmail.com> wrote:
> >
> > Hi R-experts,
> >
> > I have data from four conditions of an experiment.  I tried to create four
> > subsets of the data with read.table, for example,
> > read.table("Experiment.csv",subset=(condition=="1"))
> > .  I found a similar post in the archive, but the answer to that post was
> > no.   Any  new ideas about  reading subsets of data with read.table?
> >
> > Thanks!
> >
> > Feng
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
>
> What is the problem you are trying to solve?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From jholtman at gmail.com  Mon Mar 12 03:33:04 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 11 Mar 2007 21:33:04 -0500
Subject: [R] read.table for a subset of data
In-Reply-To: <1115a2b00703111926t2ec62802he46c748da9e93ec5@mail.gmail.com>
References: <6ab3ba540703111613m197c79a4w6c09a73f465ec06e@mail.gmail.com>
	<644e1f320703111916k6244837fld26c32eba316c83b@mail.gmail.com>
	<1115a2b00703111926t2ec62802he46c748da9e93ec5@mail.gmail.com>
Message-ID: <644e1f320703111933g3e5cec0l16b485f2fc0a3dbb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070311/076a972c/attachment.pl 

From aiminy at iastate.edu  Mon Mar 12 04:02:46 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Sun, 11 Mar 2007 22:02:46 -0500
Subject: [R] knncat question
Message-ID: <6.1.2.0.2.20070311215749.01cb4f08@aiminy.mail.iastate.edu>

I use knncat to make a predictive model
and get misclass rate
 > knncat.m<-knncat(training.new,k=c(10,20),classcol=5)
 > knncat.m
Training set misclass rate: 36.88%

then I try to calculate prediction accuracy by the following:

 > pr.knncat.train <- predict 
(knncat.m,training.new,training.new,train.classcol=5,newdata.classcol=5)
 > tb.knncat.train <-table (pr.knncat.train, training.new$y)
 > nnb.accuracy.train<-round(sum(diag(tb.knncat.train))/sum(tb.knncat.train),2)
 > cat("NNB prediction accuracy for training",nnb.accuracy.train,"\n")
NNB prediction accuracy for training 0.67

I thought I should to get :
100%-36.88%=63.12%=0.63.

But in my calculation, I get 0.67.
Is my calculation wrong?

thanks,

Aimin


From aiminy at iastate.edu  Mon Mar 12 04:20:34 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Sun, 11 Mar 2007 22:20:34 -0500
Subject: [R] standardized residuals
Message-ID: <6.1.2.0.2.20070311221646.01d1f858@aiminy.mail.iastate.edu>

I use nnet and want to calculate standardized residuals for training set
I know there is a method of residuals for nnet.
stdres doesn't work for nnet.
Does anyone implement stdres for nnet?

thanks,

Aimin


From ronggui.huang at gmail.com  Mon Mar 12 06:38:33 2007
From: ronggui.huang at gmail.com (ronggui)
Date: Mon, 12 Mar 2007 13:38:33 +0800
Subject: [R] Problem with installation of littler-0.0.10. under Free BSD 6.2
Message-ID: <38b9f0350703112238o19ae8ce5v9da9851f53e94287@mail.gmail.com>

MyBSD% ./configure
checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for gawk... no
checking for mawk... no
checking for nawk... nawk
checking whether make sets $(MAKE)... yes
checking build system type... i386-unknown-freebsd6.2
checking host system type... i386-unknown-freebsd6.2
checking for a BSD-compatible install... /usr/bin/install -c
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ISO C89... none needed
checking for style of include used by make... GNU
checking dependency style of gcc... gcc3
checking how to run the C preprocessor... gcc -E
checking for grep that handles long lines and -e... /usr/bin/grep
checking for egrep... /usr/bin/grep -E
checking for ANSI C header files... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking stdio.h usability... yes
checking stdio.h presence... yes
checking for stdio.h... yes
checking for string.h... (cached) yes
checking errno.h usability... yes
checking errno.h presence... yes
checking for errno.h... yes
checking for stdlib.h... (cached) yes
checking for sys/types.h... (cached) yes
checking for sys/stat.h... (cached) yes
checking for unistd.h... (cached) yes
checking getopt.h usability... yes
checking getopt.h presence... yes
checking for getopt.h... yes
checking whether sys/types.h defines makedev... yes
checking for setenv... yes
checking for gettimeofday... yes
checking for time... yes
checking for R... /usr/local/bin/R
checking if R was built as a shared library... yes
checking for install_name_tool... no
configure: creating ./config.status
config.status: creating Makefile
config.status: creating config.h
config.status: config.h is unchanged
config.status: executing depfiles commands
MyBSD% make
R_HOME= /usr/local/bin/R --silent --vanilla --slave <  > autoloads.h
Syntax error: redirection unexpected
*** Error code 2

Stop in /home/ronggui/software/littler-0.0.10.

Anyone knows why and any hints to the solution? Thanks in advance.

-- 
Ronggui Huang
Department of Sociology
Fudan University, Shanghai, China
??????
????????????????


From petr.pikal at precheza.cz  Mon Mar 12 07:57:30 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 12 Mar 2007 07:57:30 +0100
Subject: [R] autoload libraries at startup
In-Reply-To: <espsl4$o49$1@sea.gmane.org>
Message-ID: <45F507EA.21106.CBA3D@localhost>

Hi

see
.First or .Rprofile

Regards
Petr

BTW, did you try ?Startup or help.search("startup") before your 
posting?



On 8 Mar 2007 at 12:45, toby909 at gmail.com wrote:

To:             	r-help at stat.math.ethz.ch
From:           	toby909 at gmail.com
Date sent:      	Thu, 08 Mar 2007 12:45:39 -0800
Subject:        	[R] autoload libraries at startup

> Hi All
> 
> I was wondering if there is a way I can specify in R that it should
> load libraries automatically at startup, so that I do not have to
> manually issue the command.
> 
> Thanks Toby
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From klaster at karlin.mff.cuni.cz  Mon Mar 12 08:09:48 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Mon, 12 Mar 2007 08:09:48 +0100
Subject: [R] MCMC logit
In-Reply-To: <277054.99375.qm@web34104.mail.mud.yahoo.com>
References: <277054.99375.qm@web34104.mail.mud.yahoo.com>
Message-ID: <45F4FCBC.6090506@karlin.mff.cuni.cz>

Please, DO read your error messages - it says
"Error in tune %*% V : non-conformable arguments" - most probably you 
are trying to matrix-multiply (%*%) a scalar (tune) and a vector or 
matrix (V)...

This has nothing to do with the previous error message "Error in 
eval(expr, envir, enclos) : object "x1" not found" - this one was fixed 
by introducing separate covariates x1-x4. Btw, aparently you DO NOT know 
that you dataframe has names exactly as you want them - R is 
case-sensitive and thus x1 is NOT the same as X1... However, you are 
supposed to read R-intro, R-FAQ etc before posting to the list, so you 
already know all this...

Petr

Anamika Chaudhuri napsal(a):
> Hi,
>    
>   I know the data frame c.df has the variables named exactly the way I want it to be. I tried reading each covariate seperately but still it gives me an error. I tried fidning out the error but it doesnt seem very evident. Here is the error message
>    
>   > #######################
>>
>> #retreive data
>> # considering four covariates
>> d.df=as.data.frame(read.table("c:/tina/phd/thesis/data/modified_data1.1.txt",header=T,sep=","))
>> y=d.df[,ncol(d.df)]
>> x=d.df[,1:4]
>> # read each column of x seperately
>> x1=d.df[,1]
>> x2=d.df[,2]
>> x3=d.df[,3]
>> x4=d.df[,4]
>> c.df=cbind(y,x)
>> print(c.df)
>    y X1 X2 X3 X4
> 1  1  2  0  0  1
> 2  1 10  0  0  1
> 3  0  4  0  0  1
> 4  1  0  0  1  1
> 5  1  0  0  1  1
> 6  1  7  0  0  1
> 7  1  1  0  0  1
> 8  0  0  0  0  1
> 9  1  0  0  0  1
> 10 1  5  0  0  1
>> p <- ncol(c.df)
>>
>> # setting error handler to true
>> options(show.error.mesages = TRUE)
>>
>> # marginal log-prior of beta[]
>> logpriorfun <- function(beta, mu, gshape, grate)
> + {
> + logprior = -p*log(2) + log(gamma(p+gshape)) - log(gamma(gshape))
> + + gshape*log(grate) - (p+gshape)* log(grate+sum(abs(beta)))
> + return(logprior)
> + }
>> require(MCMCpack)
> Loading required package: MCMCpack
> Loading required package: coda
> Loading required package: lattice
> Loading required package: MASS
> ##
> ## Markov Chain Monte Carlo Package (MCMCpack)
> ## Copyright (C) 2003-2007 Andrew D. Martin and Kevin M. Quinn
> ##
> ## Support provided by the U.S. National Science Foundation
> ## (Grants SES-0350646 and SES-0350613)
> ##
> [1] TRUE
> Warning message:
> package 'MASS' was built under R version 2.4.1 
>> a0 = 0.5
>> b0 = 1
>> mu0 = 0
>> beta.init=list(c(0, rep(0.1,4)), c(0, rep(-0.1,4)), c(0, rep(0, 4)))
>> burnin.cycles = 1000
>> mcmc.cycles = 25000
>> # three chains
>> post.list <- lapply(beta.init, function(vec)
> + {
> + posterior <- MCMClogit(y~x1+x2+x3+x4, data=c.df, burnin=burnin.cycles, mcmc=mcmc.cycles,
> + thin=5, tune=0.5, beta.start=vec, user.prior.density=logpriorfun, logfun=TRUE,
> + mu=mu0, gshape=a0, grate=b0)
> + return(posterior)
> + })
> Error in tune %*% V : non-conformable arguments
>> # for tracing error mesages
>> geterrmessage()
> [1] "Error in tune %*% V : non-conformable arguments\n"
>> traceback()
> 3: MCMClogit(y ~ x1 + x2 + x3 + x4, data = c.df, burnin = burnin.cycles, 
>        mcmc = mcmc.cycles, thin = 5, tune = 0.5, beta.start = vec, 
>        user.prior.density = logpriorfun, logfun = TRUE, mu = mu0, 
>        gshape = a0, grate = b0)
> 2: FUN(X[[1]], ...)
> 1: lapply(beta.init, function(vec) {
>        posterior <- MCMClogit(y ~ x1 + x2 + x3 + x4, data = c.df, 
>            burnin = burnin.cycles, mcmc = mcmc.cycles, thin = 5, 
>            tune = 0.5, beta.start = vec, user.prior.density = logpriorfun, 
>            logfun = TRUE, mu = mu0, gshape = a0, grate = b0)
>        return(posterior)
>    })
> 
>    
>   Thanks for your help,
> Anamika
> Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
>   As the error message clearly indicates, the function MCMClogit is unable to
> find the variable x1 (possibly x2,x3, and x4 also) in the data frame c.df.
> Check the names of the variables in that data frame and make sure that the
> names correspond to the formula specification.
> 
> Hope this helps,
> Ravi.
> 
> ----------------------------------------------------------------------------
> -------
> 
> Ravi Varadhan, Ph.D.
> 
> Assistant Professor, The Center on Aging and Health
> 
> Division of Geriatric Medicine and Gerontology 
> 
> Johns Hopkins University
> 
> Ph: (410) 502-2619
> 
> Fax: (410) 614-9625
> 
> Email: rvaradhan at jhmi.edu
> 
> Webpage: http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
> 
> 
> 
> ----------------------------------------------------------------------------
> --------
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anamika Chaudhuri
> Sent: Friday, March 09, 2007 3:27 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] MCMC logit
> 
> Hi, 
> I have a dataset with the binary outcome Y(0,1) and 4 covariates
> (X1,X@,X#,X$). I am trying to use MCMClogit to model logistic regression
> using MCMC. I am getting an error where it doesnt identify the covariates
> ,although its reading in correctly. The dataset is a sample of actual
> dataset. Below is my code:
>> #######################
>>
>>
>> #retreive data
>> # considering four covariates
>>
> d.df=as.data.frame(read.table("c:/tina/phd/thesis/data/modified_data1.1.txt"
> ,header=T,sep=","))
>> y=d.df[,ncol(d.df)]
>> x=d.df[,1:4]
>> c.df=cbind(y,x)
>> #x=cbind(1,x)
>> p <- ncol(c.df)
>>
>> # marginal log-prior of beta[]
>> logpriorfun <- function(beta, mu, gshape, grate)
> + {
> + logprior = -p*log(2) + log(gamma(p+gshape)) - log(gamma(gshape))
> + + gshape*log(grate) - (p+gshape)* log(grate+sum(abs(beta)))
> + return(logprior)
> + }
>> require(MCMCpack)
> Loading required package: MCMCpack
> Loading required package: coda
> Loading required package: lattice
> Loading required package: MASS
> ##
> ## Markov Chain Monte Carlo Package (MCMCpack)
> ## Copyright (C) 2003-2007 Andrew D. Martin and Kevin M. Quinn
> ##
> ## Support provided by the U.S. National Science Foundation
> ## (Grants SES-0350646 and SES-0350613)
> ##
> [1] TRUE
> Warning message:
> package 'MASS' was built under R version 2.4.1 
>> a0 = 0.5
>> b0 = 1
>> mu0 = 0
>> beta.init=list(c(0, rep(0.1,4)), c(0, rep(-0.1,4)), c(0, rep(0, 4)))
>> burnin.cycles = 1000
>> mcmc.cycles = 25000
>> # three chains
>> post.list <- lapply(beta.init, function(vec)
> + {
> + posterior <- MCMClogit(y~x1+x2+x3+x4, data=c.df, burnin=burnin.cycles,
> mcmc=mcmc.cycles,
> + thin=5, tune=0.5, beta.start=vec, user.prior.density=logpriorfun,
> logfun=TRUE,
> + mu=mu0, gshape=a0, grate=b0)
> + return(posterior)
> + })
> Error in eval(expr, envir, enclos) : object "x1" not found
> Any suggestions will be greatly appreciated.
> Thanks,
> Anamika
> 
> 
> ---------------------------------
> We won't tell. Get more on shows you hate to love
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
>  
> ---------------------------------
> Bored stiff? Loosen up...
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From klaster at karlin.mff.cuni.cz  Mon Mar 12 08:17:47 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Mon, 12 Mar 2007 08:17:47 +0100
Subject: [R] Export successively displayed graphics
In-Reply-To: <20070312013714.06bdebcc@cachan.home-dn.net>
References: <20070312013714.06bdebcc@cachan.home-dn.net>
Message-ID: <45F4FE9B.8060003@karlin.mff.cuni.cz>

?postscript
postscript(file, options)
... all of your plotting for 1 graph ...
dev.off()

You can even discard promting between images unless you want to set 
something based on previous graphs.
Petr

Renaud Lacour napsal(a):
> Hello,
> 
> I am running R version 2.4.1 on Debian.
> 
> I am using some R functions which produce several graphics displayed
> successively with an interactive prompt between each graph. For example,
> the plot.varest (vars) function produces such output (one graph per time
> series on which the VAR model is fitted). Plots on GARCH models also
> produce such outputs.
> 
> Moreover, the frontend I am using for R (RkWard) does not enable me to
> export graphs as vectorial image files.
> 
> How can I export such graphs as separate (e.g.) Postscript files ?
> 
> 
> par(ask=FALSE) didn't help.
> 
> Thank you for your answer.

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From salcaraz at obelix.umh.es  Wed Mar  7 13:25:52 2007
From: salcaraz at obelix.umh.es (salcaraz at obelix.umh.es)
Date: Wed, 7 Mar 2007 13:25:52 +0100 (CET)
Subject: [R] 'persp' but only axes
In-Reply-To: <mailman.80.1173266568.2106.r-help@stat.math.ethz.ch>
References: <mailman.80.1173266568.2106.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0703071323160.20189@obelix.umh.es>

Hi all:

I need to 3D graph using 'persp', but I need only to draw the axes of the 
graph, not data.

Can anybody help me?

Thank you in advance.

/salva


From jose.sierra at integromics.com  Mon Mar 12 08:52:11 2007
From: jose.sierra at integromics.com (Jose Sierra)
Date: Mon, 12 Mar 2007 08:52:11 +0100
Subject: [R] RMySQL on win32
In-Reply-To: <45F2D7C1.7020805@statistik.uni-dortmund.de>
References: <193633.53166.qm@web52413.mail.yahoo.com>
	<45F2D7C1.7020805@statistik.uni-dortmund.de>
Message-ID: <45F506AB.9090208@integromics.com>

Hello....

I try to compile RMysql and ROracle for Windows XP with Visual C++ but I 
haven?t successed.

Finally I have used RJDBC and it works.

Jose Sierra.

Uwe Ligges escribi?:
> Pete Cap wrote:
>   
>> List,
>>
>> I just left an environment where I was running R and mysql on CENTOS.
>> At the time of install, RMySQL was available on CRAN.  Later installs
>> on Ubuntu were possible because it was available as a package in the
>> base repos.
>>
>> Now I'm in a new environment where I have no choice but to use
>> Windows XP.  I have just installed R 2.4.1 and MySQL 5.0.27.  The
>> installation instructions for getting RMySQL to install are a bit
>> dense and possibly over my head, so I'm wondering if it is really
>> necessary to compile the package against my current versions (yes, I
>> realize that may be a question with a painfully obvious answer), or
>> if I can simply use one of the precompiled binaries at David James's
>> site.  If anyone can tell me (or if there is a very easy way to get
>> RMySQL up and running on win32), please let me know.
>>
>> As an aside, can anyone explain why it is not possible to keep that
>> package in CRAN?  I'm just curious about that, it's just for my own
>> enlightenment.
>>     
>
> For CRAN, I could only build against one version of MySQL for each 
> version of R. In the past (particularly up to R-1.6.x), Brian Ripley 
> made some bad experiences with incompatibilities when the package was 
> built using different versions of MySQL. Hence we do not want to provide 
> builds to the public that might only work together with some particular 
> versions of MySQL.
>
> Best,
> Uwe
>
>
>
>
>   
>> Thanks,
>>
>> Pete
>>
>>
>> --------------------------------- Don't get soaked.  Take a quick
>> peek at the forecast
>>
>> [[alternative HTML version deleted]]
>>
>> ______________________________________________ 
>> R-help at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
>> posting guide http://www.R-project.org/posting-guide.html and provide
>> commented, minimal, self-contained, reproducible code.
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>   

	
	
		
______________________________________________ 
LLama Gratis a cualquier PC del Mundo. 
Llamadas a fijos y m?viles desde 1 c?ntimo por minuto. 
http://es.voice.yahoo.com


From peter.stencel at uni-bielefeld.de  Mon Mar 12 09:27:45 2007
From: peter.stencel at uni-bielefeld.de (Peter Stencel)
Date: Mon, 12 Mar 2007 09:27:45 +0100
Subject: [R] convert pixels into axis coordinates in R
Message-ID: <f88fe34ace85.45f51d11@uni-bielefeld.de>

Dear R users,

I've two questions:

1) Does anybody have a clue how to convert pixel from a jpeg graphic (e.g. something like a square of 100x100 pxs)  into axis coordinate values in R? 

2) Is there any possibility to extend the R locator function in a way that locator( ) outputs all coordinates from a plot at once, without clicking on the graph?

Thanks for any hint.

Regards,
P. Stencel


From Roger.Bivand at nhh.no  Mon Mar 12 10:23:56 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 12 Mar 2007 10:23:56 +0100 (CET)
Subject: [R] convert pixels into axis coordinates in R
In-Reply-To: <f88fe34ace85.45f51d11@uni-bielefeld.de>
Message-ID: <Pine.LNX.4.44.0703121009360.5797-100000@reclus.nhh.no>

On Mon, 12 Mar 2007, Peter Stencel wrote:

> Dear R users,
> 
> I've two questions:
> 
> 1) Does anybody have a clue how to convert pixel from a jpeg graphic
> (e.g. something like a square of 100x100 pxs)  into axis coordinate
> values in R?

The posting guide suggests providing enough code for reproducing a problem 
- your question is not specific enough. If the jpeg is just an image, 
maybe something in the rimage package, or the Bioconductor EBImage 
package, or for geospatial, see the rgdal package. The pixmap package can 
do this too, but without more detail and code examples, it's difficult to 
tell.

> 
> 2) Is there any possibility to extend the R locator function in a way
> that locator( ) outputs all coordinates from a plot at once, without
> clicking on the graph?
> 

You are free to make a version for yourself, if that is what you really 
want.

> Thanks for any hint.
> 
> Regards,
> P. Stencel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From ligges at statistik.uni-dortmund.de  Mon Mar 12 10:26:28 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 12 Mar 2007 10:26:28 +0100
Subject: [R] 'persp' but only axes
In-Reply-To: <Pine.LNX.4.64.0703071323160.20189@obelix.umh.es>
References: <mailman.80.1173266568.2106.r-help@stat.math.ethz.ch>
	<Pine.LNX.4.64.0703071323160.20189@obelix.umh.es>
Message-ID: <45F51CC4.1010101@statistik.uni-dortmund.de>

For example:
     persp(.....,  col = NA, border = NA)

Uwe Ligges


salcaraz at obelix.umh.es wrote:
> Hi all:
> 
> I need to 3D graph using 'persp', but I need only to draw the axes of the 
> graph, not data.
> 
> Can anybody help me?
> 
> Thank you in advance.
> 
> /salva
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From b.rowlingson at lancaster.ac.uk  Mon Mar 12 10:35:38 2007
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 12 Mar 2007 09:35:38 +0000
Subject: [R] logging mouse clicks
In-Reply-To: <ffb035000703102055r13ce94c1o268391c3ec6ea708@mail.gmail.com>
References: <ffb035000703102055r13ce94c1o268391c3ec6ea708@mail.gmail.com>
Message-ID: <45F51EEA.7000705@lancaster.ac.uk>

Seth Roberts wrote:
> How can I use R to record the time of a mouse click?

Assuming they are mouse clicks on a plot from locator() or identify() 
then its as trivial as this:

 > plot(1:10)
 > locator(1); when=date(); print(when)
$x
[1] 3.787584

$y
[1] 1.978947

[1] "Mon Mar 12 09:34:07 2007"

  but that only gets you one second resolution, and assumes zero delay 
between the click and the when=date() function call.

Good enough?

Barry


From samay.sar at gmail.com  Mon Mar 12 10:50:04 2007
From: samay.sar at gmail.com (d. sarthi maheshwari)
Date: Mon, 12 Mar 2007 15:20:04 +0530
Subject: [R] Any support for RRD graphs/tools in R?
Message-ID: <d4327f7e0703120250x640010d6ld73a51d6580cc213@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070312/d8043efc/attachment.pl 

From c.gold at magnet.at  Mon Mar 12 10:58:47 2007
From: c.gold at magnet.at (Christian Gold)
Date: Mon, 12 Mar 2007 10:58:47 +0100
Subject: [R] meta-regression, MiMa function, and R-squared
Message-ID: <45F52457.1040404@magnet.at>

Dear Wolfgang Viechtbauer and list members:

I have discovered your "MiMa" function for fitting meta-analytic 
mixed-effects models through an earlier discussion on this list. I think 
it is extremely useful and fills an important gap. In particular, since 
it is programmed so transparently, it is easy to adapt it for one's own 
needs. (For example, I have found it easy to identify and adapt the few 
lines I had to change to make the function fit models without intercept 
- impossible with one of the commercial packages for meta-analysis).
I agree with Emmanuel Charpentier's suggestion that your function would 
be even more useful if it was more alike lm or glm (some time in the 
future perhaps). For now, one question: How do I calculate the correct 
R-squared for models fitted with MiMa?

Thanks

Christian Gold
University of Bergen
www.uib.no/people/cgo022


From kubovy at virginia.edu  Mon Mar 12 11:15:57 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Mon, 12 Mar 2007 06:15:57 -0400
Subject: [R] Help with xYplot() in package Hmisc
Message-ID: <2C7E18B6-C1A2-4769-B158-FED0744D291C@virginia.edu>

Dear r-helpers,

I would like to add three fitted lines to this plot. I don't know how  
to include three panel.abline() to the call to xYplot. I would like  
the colors of the lines correspond to the color of the dots and CI bars.

x <- c(1:4, 1:4, 3:6)
y <- c(4:1, 6:3, 5:2)
e <- runif(12) - .5
y <- y + e
ll <- y - 1
ul <- y + 1
g <- c(rep(1, 4), rep(2, 4), rep(3, 4))
df <- data.frame(x, y, ll, ul, g)
xYplot(Cbind(y, ll, ul) ~ x, groups = g, type = 'p', data = df)
_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From Wolfgang.Viechtbauer at STAT.unimaas.nl  Mon Mar 12 12:23:51 2007
From: Wolfgang.Viechtbauer at STAT.unimaas.nl (Viechtbauer Wolfgang (STAT))
Date: Mon, 12 Mar 2007 12:23:51 +0100
Subject: [R] meta-regression, MiMa function, and R-squared
In-Reply-To: <45F52457.1040404@magnet.at>
Message-ID: <329A68716B57D54E8D39FD3F8A4A84DF057D5D8B@um-mail0136.unimaas.nl>

Dear All,

I am actually in the process of turning the mima function (with additional functions for predict, resid, and so on) into a full package.

Making the syntax of the function more like that for lm would indeed be useful. However, for that I would have to familiarize myself more with the internals of R to understand how exactly I can make use of the formula syntax.

As for calculating (something like) R^2, there are essentially two approaches I may suggest. I assume you have a vector of effect size estimates "y", the corresponding vector of estimated sampling variances "v", and you have one or more moderator variables "x1" through "xp".

1) Fit the model containing x1 through xp with the mima function and let tau2 denote the estimate of residual heterogeneity from that model. Create a new variable "w <- 1/(v + tau2)". Note that the mima function does nothing else but fit the model with weighted least squares using those weights. So, you could actually use "lm(y ~ x1 + ... + xp, weights=w)" and you should get the exact same parameter estimates. Therefore, "summary(lm(y ~ x1 + ... + xp, weights=w))" will give you R^2. Note that this is the coefficient of determination for transformed data whose meaning may not be entirely intuitive. See:

Willett, J. B., & Singer, J. D. (1988). Another cautionary note about R^2: Its use in weighted least-squares regression analysis. American Statistician, 42(3), 236-238.

for a nice discussion of this.

2) Another approach that is used in the meta-analytic context is this. First estimate the total amount of heterogeneity by using a model without moderators (i.e., a random-effects model). Let that estimate be denoted by "tau2.tot". Next, fit the model with moderators. Let the estimate of residual heterogeneity be denoted by "tau2.res". Then "(tau2.tot - tau2.res)/tau2.tot" is an estimate of the proportion of the total amount of heterogeneity that is accounted for by the moderators included in the model. This is an intuitive measure that has an R^2 flavor to it, but I would not directly call it R^2.

Hope this helps,

-- 
Wolfgang Viechtbauer 
?Department of Methodology and Statistics 
?University of Maastricht, The Netherlands 
?http://www.wvbauer.com/ 



-----Original Message-----
From: Christian Gold [mailto:c.gold at magnet.at] 
Sent: Monday, March 12, 2007 10:59
To: r-help at stat.math.ethz.ch; wvb at wvbauer.com
Subject: meta-regression, MiMa function, and R-squared


Dear Wolfgang Viechtbauer and list members:

I have discovered your "MiMa" function for fitting meta-analytic 
mixed-effects models through an earlier discussion on this list. I think 
it is extremely useful and fills an important gap. In particular, since 
it is programmed so transparently, it is easy to adapt it for one's own 
needs. (For example, I have found it easy to identify and adapt the few 
lines I had to change to make the function fit models without intercept 
- impossible with one of the commercial packages for meta-analysis). I agree with Emmanuel Charpentier's suggestion that your function would 
be even more useful if it was more alike lm or glm (some time in the 
future perhaps). For now, one question: How do I calculate the correct 
R-squared for models fitted with MiMa?

Thanks

Christian Gold
University of Bergen
www.uib.no/people/cgo022


From edd at debian.org  Mon Mar 12 13:24:05 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 12 Mar 2007 07:24:05 -0500
Subject: [R] Problem with installation of littler-0.0.10. under Free BSD
	6.2
In-Reply-To: <38b9f0350703112238o19ae8ce5v9da9851f53e94287@mail.gmail.com>
References: <38b9f0350703112238o19ae8ce5v9da9851f53e94287@mail.gmail.com>
Message-ID: <17909.18021.962545.49721@basebud.nulle.part>


On 12 March 2007 at 13:38, ronggui wrote:
| MyBSD% ./configure
| checking for a BSD-compatible install... /usr/bin/install -c
| checking whether build environment is sane... yes
| checking for gawk... no
| checking for mawk... no
| checking for nawk... nawk
| checking whether make sets $(MAKE)... yes
| checking build system type... i386-unknown-freebsd6.2
| checking host system type... i386-unknown-freebsd6.2
| checking for a BSD-compatible install... /usr/bin/install -c
| checking for gcc... gcc
| checking for C compiler default output file name... a.out
| checking whether the C compiler works... yes
| checking whether we are cross compiling... no
| checking for suffix of executables...
| checking for suffix of object files... o
| checking whether we are using the GNU C compiler... yes
| checking whether gcc accepts -g... yes
| checking for gcc option to accept ISO C89... none needed
| checking for style of include used by make... GNU
| checking dependency style of gcc... gcc3
| checking how to run the C preprocessor... gcc -E
| checking for grep that handles long lines and -e... /usr/bin/grep
| checking for egrep... /usr/bin/grep -E
| checking for ANSI C header files... yes
| checking for sys/types.h... yes
| checking for sys/stat.h... yes
| checking for stdlib.h... yes
| checking for string.h... yes
| checking for memory.h... yes
| checking for strings.h... yes
| checking for inttypes.h... yes
| checking for stdint.h... yes
| checking for unistd.h... yes
| checking stdio.h usability... yes
| checking stdio.h presence... yes
| checking for stdio.h... yes
| checking for string.h... (cached) yes
| checking errno.h usability... yes
| checking errno.h presence... yes
| checking for errno.h... yes
| checking for stdlib.h... (cached) yes
| checking for sys/types.h... (cached) yes
| checking for sys/stat.h... (cached) yes
| checking for unistd.h... (cached) yes
| checking getopt.h usability... yes
| checking getopt.h presence... yes
| checking for getopt.h... yes
| checking whether sys/types.h defines makedev... yes
| checking for setenv... yes
| checking for gettimeofday... yes
| checking for time... yes
| checking for R... /usr/local/bin/R
| checking if R was built as a shared library... yes
| checking for install_name_tool... no
| configure: creating ./config.status
| config.status: creating Makefile
| config.status: creating config.h
| config.status: config.h is unchanged
| config.status: executing depfiles commands
| MyBSD% make
| R_HOME= /usr/local/bin/R --silent --vanilla --slave <  > autoloads.h
| Syntax error: redirection unexpected
| *** Error code 2
| 
| Stop in /home/ronggui/software/littler-0.0.10.
| 
| Anyone knows why and any hints to the solution? Thanks in advance.

Jeff and I know that the world was created inside a bash shell.  

Seriously, can you try running configure inside a shell with working redirects?
If you can't then you may need to simulate by hand what this would do,
outside of configure, and then run make.

Let us know.

Dirk

| -- 
| Ronggui Huang
| Department of Sociology
| Fudan University, Shanghai, China
| $A;FHY9s(B
| $A845)4sQ'Ig;aQ'O5(B
| 
| ______________________________________________
| R-help at stat.math.ethz.ch mailing list
| https://stat.ethz.ch/mailman/listinfo/r-help
| PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
| and provide commented, minimal, self-contained, reproducible code.

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From c.gold at magnet.at  Mon Mar 12 13:34:53 2007
From: c.gold at magnet.at (Christian Gold)
Date: Mon, 12 Mar 2007 13:34:53 +0100
Subject: [R] meta-regression, MiMa function, and R-squared
In-Reply-To: <329A68716B57D54E8D39FD3F8A4A84DF057D5D8B@um-mail0136.unimaas.nl>
References: <329A68716B57D54E8D39FD3F8A4A84DF057D5D8B@um-mail0136.unimaas.nl>
Message-ID: <45F548ED.7050305@magnet.at>

Dear Wolfgang

Thanks for your prompt and clear response concerning the R^2.
You write:

> Note that the mima function does nothing else but fit the model with 
weighted least squares using those weights. So, you could actually use
"lm(y ~ x1 + ... + xp, weights=w)" and you should get the exact same
parameter estimates.  Therefore, "summary(lm(y ~ x1 + ... + xp,
weights=w))" will give you R^2.

Is this really true? I thought that "in weighted regression the
/relative/ weights are assumed known whereas in meta-regression the
/actual/ weights are assumed known" (Higgins & Thompson, 2004,
"Controlling the risk of spurious findings from meta-regression",
Statistics in Medicine, 23, p. 1665). Also, I did calculate my
regression problem with lm using inverse variance weights before I
discovered your function, and have compared the results now. The
regression coefficient was the same, but the confidence interval was
wider with mima. Furthermore, the CI with mima depended on the absolute
size of the weights (as I assume it should do), whereas with lm it did
not. Can you explain?

Thanks

Christian


From filloon.tg at pg.com  Mon Mar 12 14:21:46 2007
From: filloon.tg at pg.com (filloon.tg at pg.com)
Date: Mon, 12 Mar 2007 09:21:46 -0400
Subject: [R] Analysis of 3-dimensional spatial point patterns
Message-ID: <OF1D4E586A.457B187A-ON8525729C.004E6818-8525729C.004EE47D@pg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070312/c9d60eab/attachment.pl 

From milferst at uiuc.edu  Mon Mar 12 14:33:43 2007
From: milferst at uiuc.edu (Kim Milferstedt)
Date: Mon, 12 Mar 2007 08:33:43 -0500
Subject: [R] R for copying and pasting selected image files?
Message-ID: <6.2.5.6.2.20070312082407.01fc2b58@uiuc.edu>

Hello,

I would like to use R to process a list of text strings. The text 
strings are filenames, encoding experimental settings. Based on the 
information in there I'd like to select certain files and copy only 
the selected files to another directory.

The files are images and there is no need, actually no desire, to 
open them. Is there a way to use R for copying and pasting files with 
any file extension to another directory without opening them?

Thanks already for any help,

Kim

__________________________________________

Kim Milferstedt
University of Illinois at Urbana-Champaign
Department of Civil and Environmental Engineering
4125 Newmark Civil Engineering Laboratory
205 North Mathews Avenue MC-250
Urbana, IL 61801
USA
phone: (001) 217 333-9663
fax: (001) 217 333-6968
email: milferst at uiuc.edu
http://cee.uiuc.edu/research/morgenroth


From rfrancois at mango-solutions.com  Mon Mar 12 14:42:33 2007
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Mon, 12 Mar 2007 13:42:33 +0000
Subject: [R] R for copying and pasting selected image files?
In-Reply-To: <6.2.5.6.2.20070312082407.01fc2b58@uiuc.edu>
References: <6.2.5.6.2.20070312082407.01fc2b58@uiuc.edu>
Message-ID: <45F558C9.9070201@mango-solutions.com>

Kim Milferstedt wrote:
> Hello,
>
> I would like to use R to process a list of text strings. The text 
> strings are filenames, encoding experimental settings. Based on the 
> information in there I'd like to select certain files and copy only 
> the selected files to another directory.
>
> The files are images and there is no need, actually no desire, to 
> open them. Is there a way to use R for copying and pasting files with 
> any file extension to another directory without opening them?
>
> Thanks already for any help,
>
> Kim
>
> __________________________________________
>
> Kim Milferstedt
> University of Illinois at Urbana-Champaign
> Department of Civil and Environmental Engineering
> 4125 Newmark Civil Engineering Laboratory
> 205 North Mathews Avenue MC-250
> Urbana, IL 61801
> USA
> phone: (001) 217 333-9663
> fax: (001) 217 333-6968
> email: milferst at uiuc.edu
> http://cee.uiuc.edu/research/morgenroth
>   
Hi Kim,

Try to have a look at :

R> apropos("copy")
R> ?file.copy

Cheers,

Romain

-- 
Mango Solutions
data analysis that delivers

Tel:  +44(0) 1249 467 467
Fax:  +44(0) 1249 467 468
Mob:  +44(0) 7813 526 123


From rbaer at atsu.edu  Mon Mar 12 14:47:30 2007
From: rbaer at atsu.edu (Robert Baer)
Date: Mon, 12 Mar 2007 08:47:30 -0500
Subject: [R] R for copying and pasting selected image files?
References: 6.2.5.6.2.20070312082407.01fc2b58@uiuc.edu
Message-ID: <00cd01c764ac$fbee00e0$970c010a@ATSU7B94409E1B>


You don't tell us your OS, but the system() command should let you use your 
OS to copy/move files on most OSs.

see ?system
Other commands that might be helpful for this job are:
?setwd
?getwd
?dir

----- Original Message ----- 
From: "Kim Milferstedt" <milferst at uiuc.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, March 12, 2007 8:33 AM
Subject: [R] R for copying and pasting selected image files?


> Hello,
>
> I would like to use R to process a list of text strings. The text
> strings are filenames, encoding experimental settings. Based on the
> information in there I'd like to select certain files and copy only
> the selected files to another directory.
>
> The files are images and there is no need, actually no desire, to
> open them. Is there a way to use R for copying and pasting files with
> any file extension to another directory without opening them?
>
> Thanks already for any help,
>
> Kim
>
> __________________________________________
>
> Kim Milferstedt
> University of Illinois at Urbana-Champaign
> Department of Civil and Environmental Engineering
> 4125 Newmark Civil Engineering Laboratory
> 205 North Mathews Avenue MC-250
> Urbana, IL 61801
> USA
> phone: (001) 217 333-9663
> fax: (001) 217 333-6968
> email: milferst at uiuc.edu
> http://cee.uiuc.edu/research/morgenroth
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch at stats.uwo.ca  Mon Mar 12 14:50:44 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 12 Mar 2007 09:50:44 -0400
Subject: [R] R for copying and pasting selected image files?
In-Reply-To: <6.2.5.6.2.20070312082407.01fc2b58@uiuc.edu>
References: <6.2.5.6.2.20070312082407.01fc2b58@uiuc.edu>
Message-ID: <45F55AB4.2000600@stats.uwo.ca>

On 3/12/2007 9:33 AM, Kim Milferstedt wrote:
> Hello,
> 
> I would like to use R to process a list of text strings. The text 
> strings are filenames, encoding experimental settings. Based on the 
> information in there I'd like to select certain files and copy only 
> the selected files to another directory.
> 
> The files are images and there is no need, actually no desire, to 
> open them. Is there a way to use R for copying and pasting files with 
> any file extension to another directory without opening them?
> 
> Thanks already for any help,

You don't say what platform you're using, but copying files is an 
operating system task, so that's important.

If you know how to do one copy on the command line, then you can use R's 
system() or shell() functions to execute many copies.

e.g. on Windows, to copy c:\dir1\foo to c:\dir2\bar you would use

shell("copy c:\\dir1\\foo c:\\dir2\\bar"))

You need shell() because "copy" is an internal command in Windows, not 
an .exe.  You need to double the backslashes because R treats them as 
escape characters.

Duncan Murdoch


From Roger.Bivand at nhh.no  Mon Mar 12 14:57:53 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 12 Mar 2007 14:57:53 +0100 (CET)
Subject: [R] Analysis of 3-dimensional spatial point patterns
In-Reply-To: <OF1D4E586A.457B187A-ON8525729C.004E6818-8525729C.004EE47D@pg.com>
Message-ID: <Pine.LNX.4.44.0703121448350.5797-100000@reclus.nhh.no>

On Mon, 12 Mar 2007 filloon.tg at pg.com wrote:

> I am trying to determine how to evaluate homogeneity of points in 
> three-dimensional space.
> 
> In two-dimensional data, I have used functions available in the Spatial 
> package
>         and I've have looked into the spatstat package
>         but, as far as I can tell, neither appears to handle 3-dimensional 
> data.
> 
> Is there another version, package, or software that does the same type 
> (G-function, etc.) of evaluation for 3-D data?

I believe that you are correct with regard to spatial, spatstat, and 
splancs. For G, it would be possible to get most of the way there using an 
off-CRAN package interfacing David Mount's Approximate Nearest Neighbours 
code that I can make available (depending on your platform), substantially 
improved thanks to Christian Sangiorgio.

> set.seed(1)
> library(ann)
> D3 <- matrix(runif(3000), ncol=3) 
> res <- ann(D3)$dnn
> quantile(res, seq(0,1,1/10))
> plot(ecdf(res))

The real difficulties would start with edge adjustment, sampling within a 
volume for simulation ought to be OK, at least for a cube.

Roger

> 
> Thanks sincerely,
>  Tom Filloon   [Procter & Gamble, Cincinnati, Ohio USA]
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From Roger.Vallejo at ARS.USDA.GOV  Mon Mar 12 15:28:01 2007
From: Roger.Vallejo at ARS.USDA.GOV (Vallejo, Roger)
Date: Mon, 12 Mar 2007 10:28:01 -0400
Subject: [R] CLUSTER Package
Message-ID: <F649B8DBD3DEAD4BB163AC4D9AC18B5F41BDD9@MD-MAIL-01.ARSNET.ARS.USDA.GOV>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070312/8733d8ab/attachment.pl 

From Wolfgang.Viechtbauer at STAT.unimaas.nl  Mon Mar 12 15:36:48 2007
From: Wolfgang.Viechtbauer at STAT.unimaas.nl (Viechtbauer Wolfgang (STAT))
Date: Mon, 12 Mar 2007 15:36:48 +0100
Subject: [R] meta-regression, MiMa function, and R-squared
In-Reply-To: <45F548ED.7050305@magnet.at>
Message-ID: <329A68716B57D54E8D39FD3F8A4A84DF057D5D8C@um-mail0136.unimaas.nl>

Yes, there is indeed a slight difference. The models fitted by lm() using the weights option (and this is the same in essentially all other software) assume that the weights are known up to a constant. The parameter estimates will be exactly the same, but the standard errors of the estimates will differ by exactly that constant. If you divide the standard errors that you get from lm() with the weights option by the residual standard error, then you get exactly the same standard errors as those given by the mima() function. Fortunately, that multiplicative constant has no bearing on the value of R^2. You can see this by using "lm(y ~ x1 + ... + xp, weights=w*10)". The value of R^2 is unchanged.

Best,

-- 
Wolfgang Viechtbauer 
?Department of Methodology and Statistics 
?University of Maastricht, The Netherlands 
?http://www.wvbauer.com/ 



-----Original Message-----
From: Christian Gold [mailto:c.gold at magnet.at] 
Sent: Monday, March 12, 2007 13:35
To: Viechtbauer Wolfgang (STAT)
Cc: r-help at stat.math.ethz.ch
Subject: Re: meta-regression, MiMa function, and R-squared


Dear Wolfgang

Thanks for your prompt and clear response concerning the R^2. You write:

> Note that the mima function does nothing else but fit the model with
weighted least squares using those weights. So, you could actually use "lm(y ~ x1 + ... + xp, weights=w)" and you should get the exact same parameter estimates.  Therefore, "summary(lm(y ~ x1 + ... + xp, weights=w))" will give you R^2.

Is this really true? I thought that "in weighted regression the /relative/ weights are assumed known whereas in meta-regression the /actual/ weights are assumed known" (Higgins & Thompson, 2004, "Controlling the risk of spurious findings from meta-regression", Statistics in Medicine, 23, p. 1665). Also, I did calculate my regression problem with lm using inverse variance weights before I discovered your function, and have compared the results now. The regression coefficient was the same, but the confidence interval was wider with mima. Furthermore, the CI with mima depended on the absolute size of the weights (as I assume it should do), whereas with lm it did not. Can you explain?

Thanks

Christian


From bolker at zoo.ufl.edu  Mon Mar 12 15:43:13 2007
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Mon, 12 Mar 2007 14:43:13 +0000 (UTC)
Subject: [R] fitting a mixed exponential distribution
References: <955d14510703111246h2aef7454t4277e9ae62d64209@mail.gmail.com>
Message-ID: <loom.20070312T153337-82@post.gmane.org>

Vishal Mehta <vishalm1975 <at> gmail.com> writes:

> Also, i thought of formulating the likelihood function and using mle()
> in package stats4-- but had trouble writing down the likelihood
> function for the case of a mixed exponential distribution...


  I don't know offhand if there are CRAN packages etc. that
will do this -- but the canonical example is the model in MASS
for Old Faithful eruptions, which is a mixture model coded by
hand -- I believe it's a normal mixture model but it should
be a good starting point for rolling your own.

   The mixture model log-likelihood would look something like this:

mixexplik <- function(p,lambda1,lambda2) {
  log(p*dexp(x,lambda1) + (1-p)*dexp(x,lambda2))
}

  good luck,
    Ben Bolker


From Max.Kuhn at pfizer.com  Mon Mar 12 16:09:17 2007
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Mon, 12 Mar 2007 11:09:17 -0400
Subject: [R] knncat question
In-Reply-To: <6.1.2.0.2.20070311215749.01cb4f08@aiminy.mail.iastate.edu>
Message-ID: <71257D09F114DA4A8E134DEAC70F25D307C5FD5A@groamrexm03.amer.pfizer.com>

Aimin,

I haven't used that function and you haven't old us anything about your
data or the system that you are on, but here is a guess: when the number
of votes are tied, a random choice is usually made between the classes.
0.63 and 0.67 are relatively close and this might account for the
difference.

If you have C classes, try using k such that C %% k > 0 to prevent ties.
You may get the same answer.

Max
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Aimin Yan
Sent: Sunday, March 11, 2007 11:03 PM
To: r-help at stat.math.ethz.ch
Subject: [R] knncat question

I use knncat to make a predictive model
and get misclass rate
 > knncat.m<-knncat(training.new,k=c(10,20),classcol=5)
 > knncat.m
Training set misclass rate: 36.88%

then I try to calculate prediction accuracy by the following:

 > pr.knncat.train <- predict 
(knncat.m,training.new,training.new,train.classcol=5,newdata.classcol=5)
 > tb.knncat.train <-table (pr.knncat.train, training.new$y)
 >
nnb.accuracy.train<-round(sum(diag(tb.knncat.train))/sum(tb.knncat.train
),2)
 > cat("NNB prediction accuracy for training",nnb.accuracy.train,"\n")
NNB prediction accuracy for training 0.67

I thought I should to get :
100%-36.88%=63.12%=0.63.

But in my calculation, I get 0.67.
Is my calculation wrong?

thanks,

Aimin

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From mike.prager at noaa.gov  Mon Mar 12 16:21:01 2007
From: mike.prager at noaa.gov (Mike Prager)
Date: Mon, 12 Mar 2007 11:21:01 -0400
Subject: [R] piecing together statements (macro?)
References: <esskjf$kra$1@sea.gmane.org>
Message-ID: <8frav25fr98lhgabupnvp1ognj2h39r8t4@4ax.com>

toby909 at gmail.com wrote:

> I am pretty new to R but saw stata and sas's macro facilities and am looking for 
> how such things work in R.
> 
> I am trying to piece together a series of statements:
> 
> n = 5   #want to have it dynamic with respect to n
> for (j in 1:n) {
> eval(paste("x", j, "=x[", j, "]", sep=""))
> }
> 
> I want the created statements 'x1=x[1]' immediately executed and tried to do 
> that with eval() but that did not work.

IMO the macro-like features in R are less easy to understand
than corresponding features e.g. in SAS.  Perhaps it's their
nature; perhaps it's that they are not documented in any single
place, because they are integrated with the rest of the
language.  Others will disagree with my assessment.

The very bright side is that because of the design of the S
language, they are much less needed. 

As I read your example, you wish to assign an array element to a
variable whose name is held in a character string.  You can use
the "assign()" function to do that.

HTH.

-- 
Mike Prager, NOAA, Beaufort, NC
* Opinions expressed are personal and not represented otherwise.
* Any use of tradenames does not constitute a NOAA endorsement.


From ThadenJohnJ at uams.edu  Mon Mar 12 16:33:23 2007
From: ThadenJohnJ at uams.edu (Thaden, John J)
Date: Mon, 12 Mar 2007 10:33:23 -0500
Subject: [R] read.table for a subset of data
References: <mailman.11.1173697204.10885.r-help@stat.math.ethz.ch>
Message-ID: <B1614B0C915A654A9C29BB71DA80E0DD01773D4E@MAIL2.ad.uams.edu>

Feng,
   I had the same question as you, how to read a subset of data, and the same
reaction as Wensui when I discovered that read.table could not.  Even if my
computer's memory were up to it, I am troubled by the idea of reading in 1.8
GB of data (in my case) to get just 4,000 numbers, for instance, particularly
if I'm then going to iterating through the entire dataset in 4,000-number
chunks.  
   I ended up defining a NetCDF format to hold my data using the RNetCDF
package, since that package's var.get.nc() function is perfectly able to read
subsets of a NetCDF variable.  Furthermore, NetCDF files allow data to be
matrices and even higher order arrays, from which you can then retrieve any
chunk by including var.get.nc 'start' and 'count' arguments in the form of
vectors of length equal to the number of array dimensions.  Once a NetCDF
format is defined, all else is painless.  One limitation is that the RNetCDF
package only supports version 3 of the NetCDF library, a version that puts a
2 GB limit on a variable's size.  Version 4 removes this limitation; I'm
hopeful some day that an R package will be an interface to the NetCDF version
4 library.
John Thaden

Message: 22
Date: Sun, 11 Mar 2007 21:33:04 -0500
From: "jim holtman" <jholtman at gmail.com>
Subject: Re: [R] read.table for a subset of data
To: "Wensui Liu" <liuwensui at gmail.com>
Cc: r-help <r-help at stat.math.ethz.ch>
Message-ID:
	<644e1f320703111933g3e5cec0l16b485f2fc0a3dbb at mail.gmail.com>
Content-Type: text/plain

If you know what 10 rows to read, then you can 'skip' to them, but it the
system still has to read each line at a time.

I have a 200,000 line csv file of numerics that takes me 4 seconds to read
in with 'read.csv' using 'colClasses', so I would guess your 100K line file
would take half of that.  Is 2 seconds of time a waste of resources?


On 3/11/07, Wensui Liu <liuwensui at gmail.com> wrote:
>
> Jim,
>
> Glad to see your reply.
>
> Refering to your email, what if I just want to read 10 rows from a csv
> table with 100000 rows? Do you think it a waste of resource to read
> the whole table in?
> Anything thought?
>
> wensui
>
> On 3/11/07, jim holtman <jholtman at gmail.com> wrote:
> > Why cann't you read in the whole data set and then create the
> subsets?  This
> > is easily done with 'split'.  If the data is too large, then consider a
> data
> > base.
> >
> > On 3/11/07, gnv shqp <gnvshqp at gmail.com> wrote:
> > >
> > > Hi R-experts,
> > >
> > > I have data from four conditions of an experiment.  I tried to create
> four
> > > subsets of the data with read.table, for example,
> > > read.table("Experiment.csv",subset=(condition=="1"))
> > > .  I found a similar post in the archive, but the answer to that post
> was
> > > no.   Any  new ideas about  reading subsets of data with read.table?
> > >
> > > Thanks!
> > >
> > > Feng
> > >
> > >        [[alternative HTML version deleted]]
> > >

Confidentiality Notice: This e-mail message, including any a...{{dropped}}


From edd at debian.org  Mon Mar 12 16:51:11 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 12 Mar 2007 10:51:11 -0500
Subject: [R] Problem with installation of littler-0.0.10. under Free
	BSD	6.2
In-Reply-To: <17909.18021.962545.49721@basebud.nulle.part>
References: <38b9f0350703112238o19ae8ce5v9da9851f53e94287@mail.gmail.com>
	<17909.18021.962545.49721@basebud.nulle.part>
Message-ID: <17909.30447.954573.699074@basebud.nulle.part>


I overlooked an inportant detail: 

On 12 March 2007 at 07:24, Dirk Eddelbuettel wrote:
| 
| On 12 March 2007 at 13:38, ronggui wrote:
| | MyBSD% ./configure
| | R_HOME= /usr/local/bin/R --silent --vanilla --slave <  > autoloads.h
| | Syntax error: redirection unexpected
| | *** Error code 2
| | 
| | Stop in /home/ronggui/software/littler-0.0.10.
| | 
| | Anyone knows why and any hints to the solution? Thanks in advance.

You are missing the command between the redirects. That shouldn't happen. If
you look at Makefile.am which defines this, you will see (indented here one tab)

	autoloads.h: autoloads.R
		R_HOME= ${RPROG} ${ROPTIONS} < $< > $@

In the Make language, $< denotes the depended-upon file, or here autoloads.R.  
>From the error message, it seems you may have deleted that file.  If that is
the case, then you should start from a fresh tarball.

Hope this helps,  Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From jeff.horner at vanderbilt.edu  Mon Mar 12 17:08:35 2007
From: jeff.horner at vanderbilt.edu (Jeffrey Horner)
Date: Mon, 12 Mar 2007 11:08:35 -0500
Subject: [R] Problem with installation of littler-0.0.10. under Free BSD
 6.2
In-Reply-To: <17909.18021.962545.49721@basebud.nulle.part>
References: <38b9f0350703112238o19ae8ce5v9da9851f53e94287@mail.gmail.com>
	<17909.18021.962545.49721@basebud.nulle.part>
Message-ID: <45F57B03.2050708@vanderbilt.edu>


Hello,


Dirk Eddelbuettel wrote:
> On 12 March 2007 at 13:38, ronggui wrote:

[...]

> | MyBSD% make
> | R_HOME= /usr/local/bin/R --silent --vanilla --slave <  > autoloads.h
> | Syntax error: redirection unexpected
> | *** Error code 2
> | 
> | Stop in /home/ronggui/software/littler-0.0.10.
> | 
> | Anyone knows why and any hints to the solution? Thanks in advance.

This is a problem with your make command not expanding '$<' . CCan you
ruun make -v to tell us what version it is?

Thanks,

jeff

> 
> Jeff and I know that the world was created inside a bash shell.  

And indeed it was! ;)
> 
> Seriously, can you try running configure inside a shell with working redirects?
> If you can't then you may need to simulate by hand what this would do,
> outside of configure, and then run make.
> 
> Let us know.
> 
> Dirk
> 
> | -- 
> | Ronggui Huang
> | Department of Sociology
> | Fudan University, Shanghai, China
> | ???
> | ????????


From maechler at stat.math.ethz.ch  Mon Mar 12 17:23:30 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 12 Mar 2007 17:23:30 +0100
Subject: [R] CLUSTER Package
In-Reply-To: <F649B8DBD3DEAD4BB163AC4D9AC18B5F41BDD9@MD-MAIL-01.ARSNET.ARS.USDA.GOV>
References: <F649B8DBD3DEAD4BB163AC4D9AC18B5F41BDD9@MD-MAIL-01.ARSNET.ARS.USDA.GOV>
Message-ID: <17909.32386.504884.796506@stat.math.ethz.ch>

Hi Vallejo,

I'm pretty busy currently, and feel your question has much more
to do with how to use R more generally than with using the
functions from the  cluster package.

So you may get help from other R-help readers,
but maybe only after you have followed the posting-guide
and give a reproducible example as you're asked there.

Regards,
Martin Maechler

>>>>> "VallejoR" == Vallejo, Roger <Roger.Vallejo at ARS.USDA.GOV>
>>>>>     on Mon, 12 Mar 2007 10:28:01 -0400 writes:

    VallejoR> Hi Martin,
    VallejoR> In using the Cluster Package, I have results for PAM and DIANA
    VallejoR> clustering algorithms (below "part" and "hier" objects):

                                                                        

    VallejoR> part <- pam(trout, bestk)
    VallejoR> # PAM results


    VallejoR> hier <- diana(trout)
    VallejoR> # DIANA results

    VallejoR> GeneNames <- show(RG$genes)
    VallejoR> # Gene Names are in this object             

 

    VallejoR> But I would like also to know what genes (NAMES) are included in each
    VallejoR> cluster. I tried unsuccessfully to send these results to output files
    VallejoR> (clusters with gene Names). This must be an easy task for a good R
    VallejoR> programmer. I will appreciate very much directions or R code on how to
    VallejoR> send the PAM and DIANA results to output files including information on
    VallejoR> genes (Names) per each cluster.

 

    VallejoR> Thank you very much.

    VallejoR> Roger

 

 

    VallejoR> Roger L. Vallejo, Ph.D.

    VallejoR> Computational Biologist & Geneticist

    VallejoR> U.S. Department of Agriculture, ARS          

    VallejoR> National Center for Cool & Cold Water Aquaculture

    VallejoR> 11861 Leetown Road

    VallejoR> Kearneysville, WV 25430

    VallejoR> Voice:    (304) 724-8340 Ext. 2141

    VallejoR> Email:   roger.vallejo at ars.usda.gov <mailto:roger.vallejo at ars.usda.gov> 

 


    VallejoR> [[alternative HTML version deleted]]


From ronggui.huang at gmail.com  Mon Mar 12 17:32:32 2007
From: ronggui.huang at gmail.com (ronggui)
Date: Tue, 13 Mar 2007 00:32:32 +0800
Subject: [R] Problem with installation of littler-0.0.10. under Free BSD
	6.2
In-Reply-To: <45F57B03.2050708@vanderbilt.edu>
References: <38b9f0350703112238o19ae8ce5v9da9851f53e94287@mail.gmail.com>
	<17909.18021.962545.49721@basebud.nulle.part>
	<45F57B03.2050708@vanderbilt.edu>
Message-ID: <38b9f0350703120932l1b32bba0p6fde3d3eef9a74ae@mail.gmail.com>

MyBSD% make -v
make: no target to make.
MyBSD% gmake -v
GNU Make 3.81
Copyright (C) 2006  Free Software Foundation, Inc.
This is free software; see the source for copying conditions.
There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A
PARTICULAR PURPOSE.

This program built for i386-portbld-freebsd6.2
MyBSD% cd software/littler-0.0.10
MyBSD% make
R_HOME= /usr/local/bin/R --silent --vanilla --slave <  > autoloads.h
Syntax error: redirection unexpected
*** Error code 2

Stop in /home/ronggui/software/littler-0.0.10.
MyBSD% export MAKE=gmake
MyBSD% make
R_HOME= /usr/local/bin/R --silent --vanilla --slave <  > autoloads.h
Syntax error: redirection unexpected
*** Error code 2

Stop in /home/ronggui/software/littler-0.0.10.
MyBSD%


%From pkg_info -x make
I find  automake-1.4.6_2 and gnu-automake-1.9.6 is installed in the BSD box.


On 3/13/07, Jeffrey Horner <jeff.horner at vanderbilt.edu> wrote:
>
> Hello,
>
>
> Dirk Eddelbuettel wrote:
> > On 12 March 2007 at 13:38, ronggui wrote:
>
> [...]
>
> > | MyBSD% make
> > | R_HOME= /usr/local/bin/R --silent --vanilla --slave <  > autoloads.h
> > | Syntax error: redirection unexpected
> > | *** Error code 2
> > |
> > | Stop in /home/ronggui/software/littler-0.0.10.
> > |
> > | Anyone knows why and any hints to the solution? Thanks in advance.
>
> This is a problem with your make command not expanding '$<' . CCan you
> ruun make -v to tell us what version it is?
>
> Thanks,
>
> jeff
>
> >
> > Jeff and I know that the world was created inside a bash shell.
>
> And indeed it was! ;)
> >
> > Seriously, can you try running configure inside a shell with working redirects?
> > If you can't then you may need to simulate by hand what this would do,
> > outside of configure, and then run make.
> >
> > Let us know.
> >
> > Dirk
> >
> > | --
> > | Ronggui Huang
> > | Department of Sociology
> > | Fudan University, Shanghai, China
> > | ??????
> > | ????????????????
>


-- 
Ronggui Huang
Department of Sociology
Fudan University, Shanghai, China
??????
????????????????


From p.hiemstra at geo.uu.nl  Mon Mar 12 17:37:56 2007
From: p.hiemstra at geo.uu.nl (Paul Hiemstra)
Date: Mon, 12 Mar 2007 17:37:56 +0100
Subject: [R] R for copying and pasting selected image files?
In-Reply-To: <6.2.5.6.2.20070312082407.01fc2b58@uiuc.edu>
References: <6.2.5.6.2.20070312082407.01fc2b58@uiuc.edu>
Message-ID: <45F581E4.20101@geo.uu.nl>

Dear Kim,

Try:

?files

I think this is all you need :).

hth,
Paul

Kim Milferstedt schreef:
> Hello,
>
> I would like to use R to process a list of text strings. The text 
> strings are filenames, encoding experimental settings. Based on the 
> information in there I'd like to select certain files and copy only 
> the selected files to another directory.
>
> The files are images and there is no need, actually no desire, to 
> open them. Is there a way to use R for copying and pasting files with 
> any file extension to another directory without opening them?
>
> Thanks already for any help,
>
> Kim
>
> __________________________________________
>
> Kim Milferstedt
> University of Illinois at Urbana-Champaign
> Department of Civil and Environmental Engineering
> 4125 Newmark Civil Engineering Laboratory
> 205 North Mathews Avenue MC-250
> Urbana, IL 61801
> USA
> phone: (001) 217 333-9663
> fax: (001) 217 333-6968
> email: milferst at uiuc.edu
> http://cee.uiuc.edu/research/morgenroth
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
Drs. Paul Hiemstra
Department of Physical Geography
Faculty of Geosciences
University of Utrecht
Heidelberglaan 2
P.O. Box 80.115
3508 TC Utrecht
Phone: 	+31302535773
Fax:	+31302531145
http://intamap.geo.uu.nl/~paul


From brown_emu at yahoo.com  Mon Mar 12 17:53:30 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Mon, 12 Mar 2007 09:53:30 -0700 (PDT)
Subject: [R] convert pixels into axis coordinates in R
In-Reply-To: <f88fe34ace85.45f51d11@uni-bielefeld.de>
Message-ID: <554938.77509.qm@web39706.mail.mud.yahoo.com>

I'm not sure I understand your question, but can you just use par("usr") to
get the x,y-limits of your image plot and dim() on your matrix for the number
of pixels? Then all the points on the image (in your user coordinates) should
be known.

I don't know if this would help but I think you can convert jpeg images to
bitmaps for R to read using ImageMagick.


--- Peter Stencel <peter.stencel at uni-bielefeld.de> wrote:

> Dear R users,
> 
> I've two questions:
> 
> 1) Does anybody have a clue how to convert pixel from a jpeg graphic (e.g.
> something like a square of 100x100 pxs)  into axis coordinate values in R? 
> 
> 2) Is there any possibility to extend the R locator function in a way that
> locator( ) outputs all coordinates from a plot at once, without clicking on
> the graph?
> 
> Thanks for any hint.
> 
> Regards,
> P. Stencel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



 
____________________________________________________________________________________
Get your own web address.


From jholtman at gmail.com  Mon Mar 12 18:00:41 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 12 Mar 2007 13:00:41 -0400
Subject: [R] read.table for a subset of data
In-Reply-To: <B1614B0C915A654A9C29BB71DA80E0DD01773D4E@MAIL2.ad.uams.edu>
References: <mailman.11.1173697204.10885.r-help@stat.math.ethz.ch>
	<B1614B0C915A654A9C29BB71DA80E0DD01773D4E@MAIL2.ad.uams.edu>
Message-ID: <644e1f320703121000o7ba5ff2hae64e00fb4108678@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070312/521625f8/attachment.pl 

From BEN at SSANET.COM  Mon Mar 12 18:01:37 2007
From: BEN at SSANET.COM (Ben Fairbank)
Date: Mon, 12 Mar 2007 11:01:37 -0600
Subject: [R] Can one set box line width within the matplot command?
Message-ID: <CA612484A337C6479EA341DF9EEE14AC05F9CA9C@hercules.ssainfo>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070312/482adc9c/attachment.pl 

From peteoutside at yahoo.com  Mon Mar 12 18:39:27 2007
From: peteoutside at yahoo.com (Pete Cap)
Date: Mon, 12 Mar 2007 10:39:27 -0700 (PDT)
Subject: [R] RMySQL on win32
In-Reply-To: <45F506AB.9090208@integromics.com>
Message-ID: <932031.37244.qm@web52411.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070312/2b7d7ba1/attachment.pl 

From marc_schwartz at comcast.net  Mon Mar 12 18:50:22 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Mon, 12 Mar 2007 12:50:22 -0500
Subject: [R] Can one set box line width within the matplot command?
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC05F9CA9C@hercules.ssainfo>
References: <CA612484A337C6479EA341DF9EEE14AC05F9CA9C@hercules.ssainfo>
Message-ID: <1173721822.5051.35.camel@localhost.localdomain>

On Mon, 2007-03-12 at 11:01 -0600, Ben Fairbank wrote:
> Hello R users --
> 
>  
> 
> I am using matplot to prepare graphs and cannot find a way to use (for
> example) box(lwd=3) within the matplot command and instead have been
> setting the box line width after drawing the graph, by using box(lwd =
> 3).  Looking over the ?par options and the matplot() help I do not see a
> way to set box width within matplot.  Is there such an option?
> 
>  
> 
> Thanks for suggestions,

Try this:

 par(lwd = 3)
 matplot((-4:5)^2, main = "Quadratic", type = "l")

It's not any better than calling box(lwd = 3) after the plot, but is an
alternative. It may actually be worse as in some types of plots,
par("lwd") may affect some symbols.

matplot() calls plot() internally. In plot.default(), there is a
localBox() function used internally to draw the box if 'frame.plot =
TRUE'.  However, using 'lwd' as a function argument is not honored.

HTH,

Marc Schwartz


From sfalcon at fhcrc.org  Mon Mar 12 18:55:17 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Mon, 12 Mar 2007 10:55:17 -0700
Subject: [R] RMySQL on win32
In-Reply-To: <932031.37244.qm@web52411.mail.yahoo.com> (Pete Cap's message of
	"Mon, 12 Mar 2007 10:39:27 -0700 (PDT)")
References: <932031.37244.qm@web52411.mail.yahoo.com>
Message-ID: <m23b4apcsa.fsf@ziti.local>

Pete Cap <peteoutside at yahoo.com> writes:

> List,
>
> I am still unable to compile RMySQL on XP and would appreciate any
> assistance anyone could provide.
>
> I know that setting up RMySQL on win32 is not easy.  The installation
> instructions are supposedly contained in ../src/README.win.  They give
> instructions on creating a file, libmysql.a, which I was able to do
> successfully.
>
> Here is where the instructions basically break down.  The creation of
> this file all occurs in \MySQL\..\lib\opt, after which the reader is
> instructed to copy libmysql.a to ..\lib\opt (huh?  You mean, where it
> already is?).  Then the reader is instructed to build the binaries
> with Rcmd build --binary RMySQL.

Not sure about that part.

>>From the windows command shell, the result is:
>
> C:\>Rcmd build --binary RMySQL * checking for file
> RMySQL/DESCRIPTION' ... OK * preparing 'RMySQL': * checking
> DESCRIPTION meta-information ...'sh' is not recognized as an internal
> or external command, operable program or batch file.  OK * cleaning
> src 'sh' is not recognized as an internal or external command,
> operable program or batch file.  Error: cannot open file
> c:/TEMP/Rout381268676' for reading
>
> Apparently R is trying to call some shell script (from the windows
> prompt??) so I attempted this in cygwin. 

I don't think that will work.  You need to install all the required
tools for building source package with R on Windows.

> If there is a simply better solution that I should try, I would
> appreciate hearing about it as well.  All I really need to do at this
> point is send select and join queries to the local server--perhaps I
> should just install RSQLite from CRAN?

Well, if you don't need MySQL, then SQLite and RSQLite will get you
going.  If you do need MySQL, you can try RODBC.

+ seth

-- 
Seth Falcon | Computational Biology | Fred Hutchinson Cancer Research Center
http://bioconductor.org


From sergio.della.franca at gmail.com  Mon Mar 12 18:55:54 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Mon, 12 Mar 2007 18:55:54 +0100
Subject: [R] How to modify a column of a matrix
Message-ID: <b490ce570703121055u4ee7001av48b6559a11109a82@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070312/5b30b712/attachment.pl 

From rolf at math.unb.ca  Mon Mar 12 19:09:19 2007
From: rolf at math.unb.ca (rolf at math.unb.ca)
Date: Mon, 12 Mar 2007 15:09:19 -0300 (ADT)
Subject: [R] meta-regression, MiMa function, and R-squared
Message-ID: <200703121809.l2CI9Jfm013583@weisner.math.unb.ca>


Hey, didn't Will Rogers say ``I never met a regression I didn't
like?''

(Sorry 'bout that; couldn't resist.)

				cheers,

					Rolf Turner
					rolf at math.unb.ca


From marc_schwartz at comcast.net  Mon Mar 12 19:24:41 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Mon, 12 Mar 2007 13:24:41 -0500
Subject: [R] How to modify a column of a matrix
In-Reply-To: <b490ce570703121055u4ee7001av48b6559a11109a82@mail.gmail.com>
References: <b490ce570703121055u4ee7001av48b6559a11109a82@mail.gmail.com>
Message-ID: <1173723881.5051.48.camel@localhost.localdomain>

On Mon, 2007-03-12 at 18:55 +0100, Sergio Della Franca wrote:
> Dear R-helpers,
> 
> I'm trying to create a string-code to modify the contents of a column of a
> matrix.
> 
> For example, I have this dataset:
> 
>   YEAR   PRODUCTS
>   1992      3253
>   1993      4144
>   1994      3246
>   1996      4144
>   1997      4087
>   1998      3836
>   1999      4379
>   2000      4072
>   2001      4202
>   2002      4554
>   2003      4456
>   2004      4738
>   2005      4144
> 
> I want to convert/update the values of the column "PRODUCTS" under some
> condition (i.e. when the values of PRODUCTS is greather than 4000 replace
> the values of PRODUCTS whit 0 else replace with 1).
> 
> My question is the following:
> there is a function or a metodology that allow to makes this operation?
> 
> 
> Thank you in advance,
> Sergio

If the data is above is matrix (MAT) and not a data frame:

# See ?cbind and ?ifelse

MAT <- cbind(MAT,  NewCol = ifelse(MAT[, "PRODUCTS"] > 4000, 0, 1))

> MAT
   YEAR PRODUCTS NewCol
1  1992     3253      1
2  1993     4144      0
3  1994     3246      1
4  1996     4144      0
5  1997     4087      0
6  1998     3836      1
7  1999     4379      0
8  2000     4072      0
9  2001     4202      0
10 2002     4554      0
11 2003     4456      0
12 2004     4738      0
13 2005     4144      0


If it is a data frame:

DF$NewCol <- ifelse(DF$PRODUCTS > 4000, 0, 1)

> DF
   YEAR PRODUCTS NewCol
1  1992     3253      1
2  1993     4144      0
3  1994     3246      1
4  1996     4144      0
5  1997     4087      0
6  1998     3836      1
7  1999     4379      0
8  2000     4072      0
9  2001     4202      0
10 2002     4554      0
11 2003     4456      0
12 2004     4738      0
13 2005     4144      0


HTH,

Marc Schwartz


From gunter.berton at gene.com  Mon Mar 12 19:34:53 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Mon, 12 Mar 2007 11:34:53 -0700
Subject: [R] How to modify a column of a matrix
In-Reply-To: <1173723881.5051.48.camel@localhost.localdomain>
Message-ID: <002901c764d5$205b8970$4d908980@gne.windows.gene.com>

?cut ## if you have several bins, where ifelse becomes messy


Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374




-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc Schwartz
Sent: Monday, March 12, 2007 11:25 AM
To: Sergio Della Franca
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] How to modify a column of a matrix

On Mon, 2007-03-12 at 18:55 +0100, Sergio Della Franca wrote:
> Dear R-helpers,
> 
> I'm trying to create a string-code to modify the contents of a column of a
> matrix.
> 
> For example, I have this dataset:
> 
>   YEAR   PRODUCTS
>   1992      3253
>   1993      4144
>   1994      3246
>   1996      4144
>   1997      4087
>   1998      3836
>   1999      4379
>   2000      4072
>   2001      4202
>   2002      4554
>   2003      4456
>   2004      4738
>   2005      4144
> 
> I want to convert/update the values of the column "PRODUCTS" under some
> condition (i.e. when the values of PRODUCTS is greather than 4000 replace
> the values of PRODUCTS whit 0 else replace with 1).
> 
> My question is the following:
> there is a function or a metodology that allow to makes this operation?
> 
> 
> Thank you in advance,
> Sergio

If the data is above is matrix (MAT) and not a data frame:

# See ?cbind and ?ifelse

MAT <- cbind(MAT,  NewCol = ifelse(MAT[, "PRODUCTS"] > 4000, 0, 1))

> MAT
   YEAR PRODUCTS NewCol
1  1992     3253      1
2  1993     4144      0
3  1994     3246      1
4  1996     4144      0
5  1997     4087      0
6  1998     3836      1
7  1999     4379      0
8  2000     4072      0
9  2001     4202      0
10 2002     4554      0
11 2003     4456      0
12 2004     4738      0
13 2005     4144      0


If it is a data frame:

DF$NewCol <- ifelse(DF$PRODUCTS > 4000, 0, 1)

> DF
   YEAR PRODUCTS NewCol
1  1992     3253      1
2  1993     4144      0
3  1994     3246      1
4  1996     4144      0
5  1997     4087      0
6  1998     3836      1
7  1999     4379      0
8  2000     4072      0
9  2001     4202      0
10 2002     4554      0
11 2003     4456      0
12 2004     4738      0
13 2005     4144      0


HTH,

Marc Schwartz

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From mtmorgan at fhcrc.org  Mon Mar 12 19:41:19 2007
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Mon, 12 Mar 2007 11:41:19 -0700
Subject: [R] RMySQL on win32
In-Reply-To: <932031.37244.qm@web52411.mail.yahoo.com> (Pete Cap's message
	of "Mon, 12 Mar 2007 10:39:27 -0700 (PDT)")
References: <932031.37244.qm@web52411.mail.yahoo.com>
Message-ID: <6phbqiy1f00.fsf@gopher4.fhcrc.org>

Pete Cap <peteoutside at yahoo.com> writes:

> List,
>
> I am still unable to compile RMySQL on XP and would appreciate any
> assistance anyone could provide.
>
> I know that setting up RMySQL on win32 is not easy.  The
> installation instructions are supposedly contained in
> ../src/README.win.  They give instructions on creating a file,
> libmysql.a, which I was able to do successfully.
>
> Here is where the instructions basically break down.  The creation
> of this file all occurs in \MySQL\..\lib\opt, after which the reader
> is instructed to copy libmysql.a to ..\lib\opt (huh?  You mean,
> where it already is?).  Then the reader is instructed to build the
> binaries with Rcmd build --binary RMySQL.
>
>>From the windows command shell, the result is:
>
> C:\>Rcmd build --binary RMySQL
> * checking for file 'RMySQL/DESCRIPTION' ... OK
> * preparing 'RMySQL':
> * checking DESCRIPTION meta-information ...'sh' is not recognized as an internal  or external command, operable program or batch file.
>  OK
> * cleaning src

> 'sh' is not recognized as an internal or external command, operable program or batch file.
> Error: cannot open file 'c:/TEMP/Rout381268676' for reading

> Apparently R is trying to call some shell script (from the windows
> prompt??) so I attempted this in cygwin.  Results:

Is this because you do not have the R tool chain required for building
R packages on Windows correctly installed? See the R Installation and
Administration Guide

http://cran.r-project.org/doc/manuals/R-admin.html

section 3 (Installing R under Windows); also perhaps

http://wiki.fhcrc.org/bioc/HowTo/Build_R_on_Windows

You'll eventually want Rtools\bin ahead of any cygwin paths in your
PATH variable, so that

c:\> which sh

('which' is a cygwin command, cygwin is not technically required to
install R packages on Windows) points to the R version of sh.

I seem to remember having some trouble finding reimp (used by
configure.win), with the link here

http://www.mingw.org/mingwfaq.shtml

providing some help -- you'll want reimp available in the bin
directory of MinGW; this might not be relevant.

Martin

> Can't locate R/Dcf.pm in @INC (@INC contains: c \PROGRA~1\R\R-24~1.1\share\perl; /usr/lib/perl5/5.8/cygwin /usr/lib/perl5/5.8 /usr/lib/perl5/site_perl/5.8/cygwin /usr/lib/perl5/site_perl/5.8 /usr/lib/perl5/site_perl/5.8/cygwin /usr/lib/perl5/site_perl/5.8 /usr/lib/perl5/vendor_perl/5.8/cygwin /usr/lib/perl5/vendor_perl/5.8 /usr/lib/perl5/vendor_perl/5.8/cygwin /usr/lib/perl5/vendor_perl/5.8 .) at c:\PROGRA~1\R\R-24~1.1/bin/build line 29.
> BEGIN failed--compilation aborted at c:\PROGRA~1\R\R-24~1.1/bin/build line 29.
>
> Dcf.pm is actually located in C:\Program Files\R\R-2.4.1\share\perl\R.  I wonder if the variable @INC is simply incorrect (it's looking under R-24~1.1, not sure if the truncated value is actually correct) but I have no idea in which file it may be located.
>
> Anyone have any ideas?
> I have installed mingw-utils while attempting to get this up and running, if it matters.
>
> If there is a simply better solution that I should try, I would appreciate hearing about it as well.  All I really need to do at this point is send select and join queries to the local server--perhaps I should just install RSQLite from CRAN?
>
> Thanks in advance,
>
> Pete
>
>  
> ---------------------------------
> Be a PS3 game guru.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Martin Morgan
Bioconductor / Computational Biology
http://bioconductor.org


From ligges at statistik.uni-dortmund.de  Mon Mar 12 19:58:31 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 12 Mar 2007 19:58:31 +0100
Subject: [R] RMySQL on win32
In-Reply-To: <932031.37244.qm@web52411.mail.yahoo.com>
References: <932031.37244.qm@web52411.mail.yahoo.com>
Message-ID: <45F5A2D7.4050709@statistik.uni-dortmund.de>



Pete Cap wrote:
> List,
> 
> I am still unable to compile RMySQL on XP and would appreciate any assistance anyone could provide.
> 
> I know that setting up RMySQL on win32 is not easy.  The installation instructions are supposedly contained in ../src/README.win.  They give instructions on creating a file, libmysql.a, which I was able to do successfully.
> 
> Here is where the instructions basically break down.  The creation of this file all occurs in \MySQL\..\lib\opt, after which the reader is instructed to copy libmysql.a to ..\lib\opt (huh?  You mean, where it already is?).  Then the reader is instructed to build the binaries with Rcmd build --binary RMySQL.
> 
>>From the windows command shell, the result is:
> 
> C:\>Rcmd build --binary RMySQL
> * checking for file 'RMySQL/DESCRIPTION' ... OK
> * preparing 'RMySQL':
> * checking DESCRIPTION meta-information ...'sh' is not recognized as an internal  or external command, operable program or batch file.
>  OK
> * cleaning src
> 'sh' is not recognized as an internal or external command, operable program or batch file.
> Error: cannot open file 'c:/TEMP/Rout381268676' for reading


Please follow the Instructions to set up your build environment as 
mentioned in the R Installation and Administration manual. That includes 
installing the tools from Duncan Murdochs Webpage and putting it into 
your path. You either forgot to install those tools or you forgot to add 
it to your PATH environment variable.

Uwe Ligges



> Apparently R is trying to call some shell script (from the windows prompt??) so I attempted this in cygwin.  Results:
> 
> Can't locate R/Dcf.pm in @INC (@INC contains: c \PROGRA~1\R\R-24~1.1\share\perl; /usr/lib/perl5/5.8/cygwin /usr/lib/perl5/5.8 /usr/lib/perl5/site_perl/5.8/cygwin /usr/lib/perl5/site_perl/5.8 /usr/lib/perl5/site_perl/5.8/cygwin /usr/lib/perl5/site_perl/5.8 /usr/lib/perl5/vendor_perl/5.8/cygwin /usr/lib/perl5/vendor_perl/5.8 /usr/lib/perl5/vendor_perl/5.8/cygwin /usr/lib/perl5/vendor_perl/5.8 .) at c:\PROGRA~1\R\R-24~1.1/bin/build line 29.
> BEGIN failed--compilation aborted at c:\PROGRA~1\R\R-24~1.1/bin/build line 29.
> 
> Dcf.pm is actually located in C:\Program Files\R\R-2.4.1\share\perl\R.  I wonder if the variable @INC is simply incorrect (it's looking under R-24~1.1, not sure if the truncated value is actually correct) but I have no idea in which file it may be located.
> 
> Anyone have any ideas?
> I have installed mingw-utils while attempting to get this up and running, if it matters.
> 
> If there is a simply better solution that I should try, I would appreciate hearing about it as well.  All I really need to do at this point is send select and join queries to the local server--perhaps I should just install RSQLite from CRAN?
> 
> Thanks in advance,
> 
> Pete
> 
>  
> ---------------------------------
> Be a PS3 game guru.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From oliverfaulhaber at gmx.de  Mon Mar 12 20:16:28 2007
From: oliverfaulhaber at gmx.de (Oliver Faulhaber)
Date: Mon, 12 Mar 2007 20:16:28 +0100
Subject: [R] How to avoid a for-loop?
Message-ID: <45F5A70C.6040301@gmx.de>

Hi all,

as I am trying to move slowly from just "working" to "good" code, I'd
like to ask if there's a smarter way than using a for-loop in tasks like
the example below.

I need to obtain the extrema of the cumulated sum of a detrended time
series. The following code is currently used, please have a look at the
comments for my questions and remarks:

system.time({
X               <- rnorm(10000)
X.length        <- length(X)
X.cum.sum       <- cumsum(X)
X.cum.mean      <- cummean(X)
# initializing the "output" vectors
X.min.detrended <- rep(NA,X.length)
X.max.detrended <- rep(NA,X.length)
for (i in 1:X.length) {
   # Detrending of the time series from index 1 to i
   # I think that's the time consuming part, are there any
   # suggestions how to do this faster?
   X.cum.sum.detrended  <- X.cum.sum[1:i]-seq(1:i)*X.cum.mean[i]
   # Calculating the min and max. Would a "range" be smarter here?
   X.min.detrended[i]   <- min(X.cum.sum.detrended)
   X.max.detrended[i]   <- max(X.cum.sum.detrended)
   # As the programs takes rather long to complete I would like to
   # get information about the progress. Any better way to do this
   # than a cat(paste(i,"...",""))?
}
})

As you can see this takes rather long even for this relative small sample:

[1] 41.11  0.97 45.14    NA    NA

I considered using "sapply", but expected problems with memory size as a
lot more values have to be kept in memory (the typical length of X is
10^6 - 10^7).

Thanks again for your patience with newbies like me :)

Regards
Oliver


From milton_ruser at yahoo.com.br  Mon Mar 12 20:24:44 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Mon, 12 Mar 2007 12:24:44 -0700 (PDT)
Subject: [R] reading BMP into R
Message-ID: <696623.46082.qm@web56608.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070312/112cd7f9/attachment.pl 

From jholtman at gmail.com  Mon Mar 12 21:10:02 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 12 Mar 2007 16:10:02 -0400
Subject: [R] How to avoid a for-loop?
In-Reply-To: <45F5A70C.6040301@gmx.de>
References: <45F5A70C.6040301@gmx.de>
Message-ID: <644e1f320703121310w471e11cm1cd0410e318949be@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070312/a742664e/attachment.pl 

From drf5n at maplepark.com  Mon Mar 12 21:10:50 2007
From: drf5n at maplepark.com (David Forrest)
Date: Mon, 12 Mar 2007 14:10:50 -0600 (CST)
Subject: [R] reading BMP into R
In-Reply-To: <696623.46082.qm@web56608.mail.re3.yahoo.com>
References: <696623.46082.qm@web56608.mail.re3.yahoo.com>
Message-ID: <Pine.LNX.4.64.0703121406570.10471@maplepark.com>

On Mon, 12 Mar 2007, Milton Cezar Ribeiro wrote:

> Hi R-gurus
>
> How can I read my "bmp" files into R?

RSiteSearch('bmp')
system('convert my.bmp my.pnm')
library(pixmap)
?read.pnm

Dave
-- 
  Dr. David Forrest
  drf at vims.edu                                    (804)684-7900w
  drf5n at maplepark.com                             (804)642-0662h
                                    http://maplepark.com/~drf5n/


From jholtman at gmail.com  Mon Mar 12 21:13:40 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 12 Mar 2007 16:13:40 -0400
Subject: [R] How to avoid a for-loop?
In-Reply-To: <644e1f320703121310w471e11cm1cd0410e318949be@mail.gmail.com>
References: <45F5A70C.6040301@gmx.de>
	<644e1f320703121310w471e11cm1cd0410e318949be@mail.gmail.com>
Message-ID: <644e1f320703121313r7ee1e7f7p5add8759de65e328@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070312/41bd88d2/attachment.pl 

From bgreen at dyson.brisnet.org.au  Mon Mar 12 21:54:05 2007
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Tue, 13 Mar 2007 06:54:05 +1000
Subject: [R] altering prefix to multiple variables in different locations
 within a command file
Message-ID: <20070312205111.98A8B59521B@borg.st.net.au>

Hello,

I am seeking advice regarding how I might add the prefix " kc$ " to 
variables in  a series of commands. The complication is that there is 
a large number of variables with different commands. Examples of the 
variables in typical commands follow.

It is simple to use search & replace for common variables such as 
"group" but I would appreciate advice about whether there is a  way 
to readily alter the remaining variables.

Bob Green


CrossTable(group,TOC2,chisq=TRUE,resid=TRUE, missing.include=FALSE)
fisher.test(group,TOC2)

tapply(TOC2,  group,  mean)
tapply( TOC2,  group, sd)

oneway.test ( TOC2 ~  group, var.equal=FALSE)
kruskal.test ( TOC2 ~  group)


broadcast  ==""
broadcast   [broadcast  ==""] <- "n"
CrossTable( group,broadcast   ,chisq=TRUE,resid=TRUE, missing.include=FALSE)
fisher.test(group,broadcast)

jealous [jealous=="y (not victim)"] = "n"
jealous==""
jealous [jealous==""] <- "n"
jealous==""
CrossTable(group,jealous ,chisq=TRUE,resid=TRUE, missing.include=FALSE)
fisher.test(group,jealous)


From lalithaviswanath at yahoo.com  Mon Mar 12 20:47:46 2007
From: lalithaviswanath at yahoo.com (lalitha viswanath)
Date: Mon, 12 Mar 2007 12:47:46 -0700 (PDT)
Subject: [R] Query about substituting characters in a df
Message-ID: <564262.2784.qm@web43141.mail.sp1.yahoo.com>

Hi
I have a data frame with 40,000 rows and 4 columns,
one of which is "class".


For each row, the "class" column can be one of 10
possible NUMERIC values.
I wish to substitute these numeric values with
words/characters.
For example, I wish to substitute all occurences of
"5467" in the column "class" with "alpha", "7867" with
"gamma", etc.
I looked up substitute, but did not find any relevant
examples.

Your input is greatly appreciated
Thanks
Lalitha


 
____________________________________________________________________________________
Never miss an email again!


From rolf at math.unb.ca  Mon Mar 12 22:53:38 2007
From: rolf at math.unb.ca (rolf at math.unb.ca)
Date: Mon, 12 Mar 2007 18:53:38 -0300 (ADT)
Subject: [R] Query about substituting characters in a df
Message-ID: <200703122153.l2CLrcKY022747@weisner.math.unb.ca>

Lalitha Viswanath wrote:

> I have a data frame with 40,000 rows and 4 columns,
> one of which is "class".
> 
> For each row, the "class" column can be one of 10 possible NUMERIC
> values.  I wish to substitute these numeric values with
> words/characters.  For example, I wish to substitute all occurences
> of "5467" in the column "class" with "alpha", "7867" with "gamma",
> etc.  I looked up substitute, but did not find any relevant
> examples.

The substitute() function has nothing whatever to do with your
problem.

The following may give you the right idea:

        > xxx <- sample(1:10,100,TRUE)
        > yyy <- letters[1:10][match(xxx,1:10)]

                                cheers,

                                        Rolf Turner
                                        rolf at math.unb.ca


From anup_nandialath at yahoo.com  Mon Mar 12 23:06:14 2007
From: anup_nandialath at yahoo.com (Anup Nandialath)
Date: Mon, 12 Mar 2007 15:06:14 -0700 (PDT)
Subject: [R] Query about substituting characters in a df
In-Reply-To: <564262.2784.qm@web43141.mail.sp1.yahoo.com>
Message-ID: <540002.88057.qm@web53313.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070312/b33b986b/attachment.pl 

From timh at insightful.com  Mon Mar 12 23:18:48 2007
From: timh at insightful.com (Tim Hesterberg)
Date: 12 Mar 2007 15:18:48 -0700
Subject: [R] sort of OT: bootstrap tutorial
In-Reply-To: <45E5575D.8060700@pburns.seanet.com> (message from Patrick Burns
	on Wed, 28 Feb 2007 10:20:13 +0000)
References: <45E5575D.8060700@pburns.seanet.com>
Message-ID: <SEWINEXCH00iyAphwFu00000081@sewinexch00.insightful.com>

Another introductory document is
	"Bootstrap Methods and Permutation Tests"
	http://bcs.whfreeman.com/ips5e/content/cat_080/pdf/moore14.pdf
This focuses on the methods, not on particular software.

There is an accompanying library at
	http://www.insightful.com/Hesterberg/bootstrap/IPSdata.zip
that includes the datasets.  While this is for S+, R users could
adapt the data/readdata.ssc file to load the datasets.

The document is written for introductory statistics students, but
should be interesting to those who are more advanced.  For more info see
http://www.insightful.com/Hesterberg/bootstrap/default.asp#Reference.introStat

Patrick Burns wrote:
>There is now a tutorial on bootstrapping and other resampling
>methods at:
>
>http://www.burns-stat.com/pages/Tutor/bootstrap_resampling.html

========================================================
| Tim Hesterberg       Senior Research Scientist       |
| timh at insightful.com  Insightful Corp.                |
| (206)802-2319        1700 Westlake Ave. N, Suite 500 |
| (206)283-8691 (fax)  Seattle, WA 98109-3044, U.S.A.  |
|                      www.insightful.com/Hesterberg   |
========================================================
Download S+Resample from www.insightful.com/downloads/libraries

Bootstrap Methods and Permutation Tests
		May 21-22 Philadelphia, Oct 10-11 San Francisco.
		2-3 May UK, 3-4 Oct UK.
Workshop on resampling for teaching:
		May 16 Ohio State 
		http://www.causeweb.org/workshop/hesterberg/


From exonintron at gmail.com  Mon Mar 12 23:29:56 2007
From: exonintron at gmail.com (Sender)
Date: Mon, 12 Mar 2007 15:29:56 -0700
Subject: [R] distance metrics
Message-ID: <686bf0c50703121529m5f909b0bqe284531b9277b1c4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070312/841cacb8/attachment.pl 

From andrewsmith_97 at yahoo.com  Mon Mar 12 23:30:51 2007
From: andrewsmith_97 at yahoo.com (Andrew Smith)
Date: Mon, 12 Mar 2007 15:30:51 -0700 (PDT)
Subject: [R] test for bimodality
Message-ID: <199475.59907.qm@web32807.mail.mud.yahoo.com>

Hi,

I found this post from 6 years ago about this:

https://stat.ethz.ch/pipermail/r-help/2001-May/012800.html

Basically, I'd like to find an R implementation of the
multimodality test in the book "Introduction to the
Bootstrap" by Efron and Tibshirani. There is an R
package bootstrap which is the package to go with the
book, but it does not seem to implement the
multimodality test. I found this post to the S mailing
list giving an implementation in development:

http://www.biostat.wustl.edu/archives/html/s-news/2000-06/msg00038.html

but I'm not sure about it. Anyway, if anyone knows
where I can find an R implementation please let me and
the list know.

thanks,
Andrew Smith


From skiadas at hanover.edu  Mon Mar 12 23:40:45 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Mon, 12 Mar 2007 18:40:45 -0400
Subject: [R] altering prefix to multiple variables in different
	locations within a command file
In-Reply-To: <20070312205111.98A8B59521B@borg.st.net.au>
References: <20070312205111.98A8B59521B@borg.st.net.au>
Message-ID: <F99B16A7-AF93-41CA-8CDB-C23C1516703D@hanover.edu>

On Mar 12, 2007, at 4:54 PM, Bob Green wrote:

> Hello,
>
> I am seeking advice regarding how I might add the prefix " kc$ " to
> variables in  a series of commands. The complication is that there is
> a large number of variables with different commands. Examples of the
> variables in typical commands follow.

Maybe I've misunderstood what you want to do, but would "with" meet  
the case?

 > a
Error: object "a" not found
 > x<-list(a=5)
 > with(x,a)
[1] 5
 > x$a
[1] 5
 >

See ?with


> It is simple to use search & replace for common variables such as
> "group" but I would appreciate advice about whether there is a  way
> to readily alter the remaining variables.
>
> Bob Green

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From young.stat at gmail.com  Mon Mar 12 23:42:47 2007
From: young.stat at gmail.com (Young Cho)
Date: Mon, 12 Mar 2007 15:42:47 -0700
Subject: [R] timeDate & business day
Message-ID: <b44da9db0703121542w56c15b2do7c72ee5f1871f360@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070312/98ddbbc1/attachment.pl 

From timh at insightful.com  Mon Mar 12 23:44:09 2007
From: timh at insightful.com (Tim Hesterberg)
Date: 12 Mar 2007 15:44:09 -0700
Subject: [R] Row-wise two sample T-test on subsets of a matrix
In-Reply-To: <20070301115129.AKS97495@m4500-02.uchicago.edu> (message from
	Nameeta Lobo on Thu, 1 Mar 2007 11:51:29 -0600 (CST))
References: <20070301115129.AKS97495@m4500-02.uchicago.edu>
Message-ID: <SEWINEXCH00m5UxP9Fe00000082@sewinexch00.insightful.com>

This is the kind of thing that rowMeans was made for.
For the numerator of the t statistic:

x1 <- temp.matrix[,1:11]
x2 <- temp.matrix[,12:22]
numerator <- rowMeans(x1) - rowMeans(x2)

For the denominator, if you're using S+ you can use rowVars;
in R you can program a simple version quickly, e.g.

rowVars <- function(x){
  means <- rowMeans(x)
  rowSums((x - means)^2) / (ncol(x)-1)
}

Tim Hesterberg

>Hello all,
>
>I am trying to run a two sample t-test on a matrix which is a
>196002*22 matrix. I want to run the t-test, row-wise, with the
>first 11 columns being a part of the first group and columns
>12-22 being a part of the second group. 
>
>I tried running something like (temp.matrix being my 196002*22
>matrix)
>
>t.test(temp.matrix[,1:11],temp.matrix[,12:22],paired=TRUE)
>
>or somthing like
>
>as.numeric(t.test(temp.matrix[,1:11],temp.matrix[,12:22],paired=TRUE)[[1]])
>so as to only capture the t-value alone and 
>
>and I get a result for the whole matrix instead of a row-wise
>result. 
>
>I want to avoid using a "for" loop to increment the number of
>rows as it would take a huge amount of time.
>
>
>Any suggestions would be really appreciated.
>
>thanks
>nameeta

========================================================
| Tim Hesterberg       Senior Research Scientist       |
| timh at insightful.com  Insightful Corp.                |
| (206)802-2319        1700 Westlake Ave. N, Suite 500 |
| (206)283-8691 (fax)  Seattle, WA 98109-3044, U.S.A.  |
|                      www.insightful.com/Hesterberg   |
========================================================
Advanced Programming in S-PLUS
		30 Apr-1 May UK, 7-8 May CH, 9-10 May FR
		May 17-18 Philadelphia, Oct 8-9 San Francisco
		26-7 Sep CH, 1-2 Oct UK		
		http://www.insightful.com/services/training.asp


From chrish at stats.ucl.ac.uk  Mon Mar 12 23:55:57 2007
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Mon, 12 Mar 2007 22:55:57 +0000 (GMT)
Subject: [R] distance metrics
In-Reply-To: <686bf0c50703121529m5f909b0bqe284531b9277b1c4@mail.gmail.com>
References: <686bf0c50703121529m5f909b0bqe284531b9277b1c4@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0703122255001.25410@egon.stats.ucl.ac.uk>

On Mon, 12 Mar 2007, Sender wrote:

> Hello:
>
> Does anyone know if there exists a package that handles methods for [ for
> dist objects?
>
> I would like to access a dist object using matrix notation
>
> e.g.
>
> dMat = dist(x)
> dMat[i,j]

Try
dMat <- as.matrix(dist(x))

Christian



*** --- ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche


From exonintron at gmail.com  Tue Mar 13 00:02:54 2007
From: exonintron at gmail.com (Sender)
Date: Mon, 12 Mar 2007 16:02:54 -0700
Subject: [R] distance metrics
In-Reply-To: <Pine.LNX.4.64.0703122255001.25410@egon.stats.ucl.ac.uk>
References: <686bf0c50703121529m5f909b0bqe284531b9277b1c4@mail.gmail.com>
	<Pine.LNX.4.64.0703122255001.25410@egon.stats.ucl.ac.uk>
Message-ID: <686bf0c50703121602j3e0baf33k64d9b7aeae807a30@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070312/07014754/attachment.pl 

From brown_emu at yahoo.com  Tue Mar 13 00:03:20 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Mon, 12 Mar 2007 16:03:20 -0700 (PDT)
Subject: [R] altering prefix to multiple variables in different
	locations within a command file
In-Reply-To: <20070312205111.98A8B59521B@borg.st.net.au>
Message-ID: <157911.57331.qm@web39712.mail.mud.yahoo.com>

I think what you are looking for is attach()?

attach(kc)
# commands (don't need to prefix variables with "kc$")
detach(kc)

In this way an environment is created in which you can refer to the variables
in your data frame without the "kc$" prefix.
(see ?attach)


--- Bob Green <bgreen at dyson.brisnet.org.au> wrote:

> Hello,
> 
> I am seeking advice regarding how I might add the prefix " kc$ " to 
> variables in  a series of commands. The complication is that there is 
> a large number of variables with different commands. Examples of the 
> variables in typical commands follow.
> 
> It is simple to use search & replace for common variables such as 
> "group" but I would appreciate advice about whether there is a  way 
> to readily alter the remaining variables.
> 
> Bob Green
> 
> 
> CrossTable(group,TOC2,chisq=TRUE,resid=TRUE, missing.include=FALSE)
> fisher.test(group,TOC2)
> 
> tapply(TOC2,  group,  mean)
> tapply( TOC2,  group, sd)
> 
> oneway.test ( TOC2 ~  group, var.equal=FALSE)
> kruskal.test ( TOC2 ~  group)
> 
> 
> broadcast  ==""
> broadcast   [broadcast  ==""] <- "n"
> CrossTable( group,broadcast   ,chisq=TRUE,resid=TRUE,
> missing.include=FALSE)
> fisher.test(group,broadcast)
> 
> jealous [jealous=="y (not victim)"] = "n"
> jealous==""
> jealous [jealous==""] <- "n"
> jealous==""
> CrossTable(group,jealous ,chisq=TRUE,resid=TRUE, missing.include=FALSE)
> fisher.test(group,jealous)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



 
____________________________________________________________________________________
No need to miss a message. Get email on-the-go


From estrain at postoffice.utas.edu.au  Tue Mar 13 00:06:39 2007
From: estrain at postoffice.utas.edu.au (estrain at postoffice.utas.edu.au)
Date: Tue, 13 Mar 2007 9:06:39 +1000
Subject: [R] Pvalues and lme
Message-ID: <200703122306.l2CN6d5p005437@corinna.its.utas.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070313/0924d166/attachment.pl 

From jiho.han at yahoo.com  Tue Mar 13 00:18:35 2007
From: jiho.han at yahoo.com (jiho.han)
Date: Mon, 12 Mar 2007 16:18:35 -0700 (PDT)
Subject: [R] replicating SAS's "proc rank" procedure
Message-ID: <9445440.post@talk.nabble.com>


Hello-

I wonder if there's a way to replicate SAS rank procedure where it ranks a
variable by a certain number of groups. For example, it's very easy to
calculate quintile rank in SAS, but I couldn't find the similar function in
R. 

Does anyone know how to do this?

Thanks. 

-- 
View this message in context: http://www.nabble.com/replicating-SAS%27s-%22proc-rank%22-procedure-tf3392910.html#a9445440
Sent from the R help mailing list archive at Nabble.com.


From gavin.simpson at ucl.ac.uk  Tue Mar 13 00:21:22 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 12 Mar 2007 23:21:22 +0000
Subject: [R] distance metrics
In-Reply-To: <686bf0c50703121602j3e0baf33k64d9b7aeae807a30@mail.gmail.com>
References: <686bf0c50703121529m5f909b0bqe284531b9277b1c4@mail.gmail.com>
	<Pine.LNX.4.64.0703122255001.25410@egon.stats.ucl.ac.uk>
	<686bf0c50703121602j3e0baf33k64d9b7aeae807a30@mail.gmail.com>
Message-ID: <1173741682.3019.8.camel@dhcppc2.my.nat.localnet>

On Mon, 2007-03-12 at 16:02 -0700, Sender wrote:
> Thanks for the suggestion Christian. I'm trying to avoid expanding the dist
> object to a matrix, since i'm usually working with microarray data which
> produces a distance matrix of size 5000 x 5000.
> 
> If i can keep it in its condensed form i think it will speed things up.
> 
> Is my thinking correct?

That will all depend on what you want to do with it...

A dist object of that size is c. 100 MB in memory, and c. 200 MB in size
as the full dissimilarity matrix - values from object.size(). Of course,
you'll need a reasonable amount of free memory over and above this to do
anything useful with the matrix as copies may be required during
analysis/processing etc.

Of course, a dist object is just a vector of observed distances with
various attributes, so one can always use "[" for vectors, but I imagine
that anything other than trivial operations will become fiddly,
complicated and time consuming - if you have the memory, give the
as.matrix option a try and see how it works for your specific problems.

G

> 
> 
> On 3/12/07, Christian Hennig <chrish at stats.ucl.ac.uk> wrote:
> >
> > On Mon, 12 Mar 2007, Sender wrote:
> >
> > > Hello:
> > >
> > > Does anyone know if there exists a package that handles methods for [
> > for
> > > dist objects?
> > >
> > > I would like to access a dist object using matrix notation
> > >
> > > e.g.
> > >
> > > dMat = dist(x)
> > > dMat[i,j]
> >
> > Try
> > dMat <- as.matrix(dist(x))
> >
> > Christian
> >
> >
> >
> > *** --- ***
> > Christian Hennig
> > University College London, Department of Statistical Science
> > Gower St., London WC1E 6BT, phone +44 207 679 1698
> > chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT                          [w] http://www.freshwaters.org.uk/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From estrain at postoffice.utas.edu.au  Tue Mar 13 00:30:36 2007
From: estrain at postoffice.utas.edu.au (estrain at postoffice.utas.edu.au)
Date: Tue, 13 Mar 2007 9:30:36 +1000
Subject: [R] Lmer Mcmc Summary and p values
Message-ID: <200703122330.l2CNUamE019731@corinna.its.utas.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070313/0dc642f0/attachment.pl 

From mwtoews at sfu.ca  Tue Mar 13 01:12:20 2007
From: mwtoews at sfu.ca (Michael Toews)
Date: Mon, 12 Mar 2007 17:12:20 -0700
Subject: [R]  timeDate & business day
Message-ID: <45F5EC64.8020808@sfu.ca>

Those numbers look like ... well, numbers. You want characters! Try 
converting the integer to a character before trying to do a string 
parse, e.g.:

ymd.int <- c(20050104, 20050105, 20050106, 20050107, 20050110, 20050111, 
20050113, 20050114)
ymd <- as.Date(as.character(ymd.int),"%Y%m%d")

As far as the other functions you are looking at ("timeDate", 
"timeRelative") -- I've never seen these, so I'm guessing they are 
S-PLUS. In R, you can use "diff" or "difftime" (which works with "Date" 
and "POSIXlt"-or Date-Time classes) , e.g.:

diff(ymd)
diff(ymd,2)
diff(ymd,3)

or do some arithmetic:

difftime(ymd[1],ymd[4])
difftime(ymd[1],ymd[4],unit="weeks")

Hopefully this is helpful to you!
+mt


From estrain at postoffice.utas.edu.au  Tue Mar 13 01:40:34 2007
From: estrain at postoffice.utas.edu.au (estrain at postoffice.utas.edu.au)
Date: Tue, 13 Mar 2007 10:40:34 +1000
Subject: [R] Lmer Mcmc Summary and p values
Message-ID: <200703130040.l2D0eZIN002295@corinna.its.utas.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070313/83f1ad51/attachment.pl 

From estrain at postoffice.utas.edu.au  Tue Mar 13 02:00:17 2007
From: estrain at postoffice.utas.edu.au (estrain at postoffice.utas.edu.au)
Date: Tue, 13 Mar 2007 11:00:17 +1000
Subject: [R] Lmer Mcmc Summary and p values
Message-ID: <200703130100.l2D10HTE016704@corinna.its.utas.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070313/e5aced84/attachment.pl 

From mystata at hotmail.com  Tue Mar 13 02:30:33 2007
From: mystata at hotmail.com (Jun Xu)
Date: Mon, 12 Mar 2007 19:30:33 -0600
Subject: [R] turn regression coefficients intro matrices
Message-ID: <BAY108-F375DE6EFC7AC1A01C8E96FA87C0@phx.gbl>

I don't have much experience with r. What I am trying to do is to turn 
regression coefficients (after I run a lm or glm model) into some matrix 
such that I can do some post-estimation calculation, for example predicted 
probabilities in glm model, etc.. Or, is there any function in r that I can 
use to do something along that line? thanks.

Jun Xu, Ph.D.
Department of Sociology
Ball State University

_________________________________________________________________
Mortgage rates as low as 4.625% - Refinance $150,000 loan for $579 a month. 
Intro*Terms  
https://www2.nextag.com/goto.jsp?product=100000035&url=%2fst.jsp&tm=y&search=mortgage_text_links_88_h27f6&disc=y&vers=743&s=4056&p=5117


From aa2007r at gmail.com  Tue Mar 13 02:55:55 2007
From: aa2007r at gmail.com (AA)
Date: Mon, 12 Mar 2007 21:55:55 -0400
Subject: [R] timeDate & business day
References: <45F5EC64.8020808@sfu.ca>
Message-ID: <04b901c76512$bd1894f0$3927a8c0@treesdalellc.net>

Hi Michael,

Thanks for your reply.
I am experiencing the same issue as Young
The following code gives:
ymd.int <- c(20050104, 20050105, 20050106, 20050107, 20050110, 20050111,
+ 20050113, 20050114)
> ymd.int
[1] 20050104 20050105 20050106 20050107 20050110 20050111 20050113 20050114
> ymd <- as.Date(as.character(ymd.int),"%Y%m%d")
> ymd
[1] "2005-01-04" "2005-01-05" "2005-01-06" "2005-01-07" "2005-01-10"
[6] "2005-01-11" "2005-01-13" "2005-01-14"
> class(ymd)
[1] "Date"

While the variable ymd is actually of class Date, the format is not yyyymmdd 
but
yyyy-mm-dd as one can see in the previous example.
As Young, I do not see what I am missing here.
Any hint would be appreciated.

AA.




----- Original Message ----- 
From: "Michael Toews" <mwtoews at sfu.ca>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, March 12, 2007 8:12 PM
Subject: [R] timeDate & business day


> Those numbers look like ... well, numbers. You want characters! Try
> converting the integer to a character before trying to do a string
> parse, e.g.:
>
> ymd.int <- c(20050104, 20050105, 20050106, 20050107, 20050110, 20050111,
> 20050113, 20050114)
> ymd <- as.Date(as.character(ymd.int),"%Y%m%d")
>
> As far as the other functions you are looking at ("timeDate",
> "timeRelative") -- I've never seen these, so I'm guessing they are
> S-PLUS. In R, you can use "diff" or "difftime" (which works with "Date"
> and "POSIXlt"-or Date-Time classes) , e.g.:
>
> diff(ymd)
> diff(ymd,2)
> diff(ymd,3)
>
> or do some arithmetic:
>
> difftime(ymd[1],ymd[4])
> difftime(ymd[1],ymd[4],unit="weeks")
>
> Hopefully this is helpful to you!
> +mt
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mystata at hotmail.com  Tue Mar 13 02:58:59 2007
From: mystata at hotmail.com (Jun Xu)
Date: Mon, 12 Mar 2007 19:58:59 -0600
Subject: [R] turn regression coefficients into matrix or...
Message-ID: <BAY108-F62F124040974B563C5604A87C0@phx.gbl>

I don't have much experience with r. What I am trying to do is to turn
regression coefficients (after I run a lm or glm model) into some matrix
such that I can do some post-estimation calculation, for example predicted
probabilities in glm model, etc.. Or, is there any function in r that I can
use to do something along that line? thanks.

Jun Xu, Ph.D.
Department of Sociology
Ball State University

_________________________________________________________________
Mortgage rates as low as 4.625% - Refinance $150,000 loan for $579 a month. 
Intro*Terms  
https://www2.nextag.com/goto.jsp?product=100000035&url=%2fst.jsp&tm=y&search=mortgage_text_links_88_h27f6&disc=y&vers=743&s=4056&p=5117


From marc_schwartz at comcast.net  Tue Mar 13 03:06:39 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Mon, 12 Mar 2007 21:06:39 -0500
Subject: [R] turn regression coefficients into matrix or...
In-Reply-To: <BAY108-F62F124040974B563C5604A87C0@phx.gbl>
References: <BAY108-F62F124040974B563C5604A87C0@phx.gbl>
Message-ID: <1173751599.5051.92.camel@localhost.localdomain>

On Mon, 2007-03-12 at 19:58 -0600, Jun Xu wrote:
> I don't have much experience with r. What I am trying to do is to turn
> regression coefficients (after I run a lm or glm model) into some matrix
> such that I can do some post-estimation calculation, for example predicted
> probabilities in glm model, etc.. Or, is there any function in r that I can
> use to do something along that line? thanks.

Presuming that you have a lm() or glm() model object, see:

  ?model.matrix

  ?model.frame

  ?predict.lm

  ?predict.glm

HTH,

Marc Schwartz


From peteoutside at yahoo.com  Tue Mar 13 03:54:03 2007
From: peteoutside at yahoo.com (Pete Cap)
Date: Mon, 12 Mar 2007 19:54:03 -0700 (PDT)
Subject: [R] RMySQL on win32
In-Reply-To: <45F5A2D7.4050709@statistik.uni-dortmund.de>
Message-ID: <403044.82230.qm@web52415.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070312/09b720e8/attachment.pl 

From mwtoews at sfu.ca  Tue Mar 13 05:00:02 2007
From: mwtoews at sfu.ca (Michael Toews)
Date: Mon, 12 Mar 2007 21:00:02 -0700
Subject: [R] timeDate & business day
In-Reply-To: <04b901c76512$bd1894f0$3927a8c0@treesdalellc.net>
References: <45F5EC64.8020808@sfu.ca>
	<04b901c76512$bd1894f0$3927a8c0@treesdalellc.net>
Message-ID: <45F621C2.4040703@sfu.ca>


> [1] 20050104 20050105 20050106 20050107 20050110 20050111 20050113 
> 20050114
>> ymd <- as.Date(as.character(ymd.int),"%Y%m%d")
>> ymd
> [1] "2005-01-04" "2005-01-05" "2005-01-06" "2005-01-07" "2005-01-10"
> [6] "2005-01-11" "2005-01-13" "2005-01-14"
>> class(ymd)
> [1] "Date"
>
> While the variable ymd is actually of class Date, the format is not 
> yyyymmdd but
> yyyy-mm-dd as one can see in the previous example.
> As Young, I do not see what I am missing here.
> Any hint would be appreciated.
>
> AA.
What happened in the beginning is that I had to parse the character into 
a Date-Time class ("Date", in this case as you correctly pointed out). 
POSIX is a kind of standard that (mainly Unix) computers use date 
formatters, such as %Y for a 4-digit year, and others. They are all 
listed in great detail in "?strptime" (which means "string parse time"). 
In this case the input parsing format pattern was "%Y%m%d". There were 
no spaces in-between each number.

When that class prints out, the default format is ISO 8601 ( see 
http://en.wikipedia.org/wiki/ISO_8601 ). When R prints the class "Date" 
to your screen, it decides to format it ISO 8601-style for you. If you 
want to see if differently, you can try:

format(ymd,"%Y/%d/%m")

The date is actually stored internally as an ordinal, somewhat like how 
MS Excel dates work. You can see how it works internally:

str(ymd)

Hopefully I've demystified some of this .. any other questions?
+mt


From young.stat at gmail.com  Tue Mar 13 05:07:55 2007
From: young.stat at gmail.com (Young Cho)
Date: Mon, 12 Mar 2007 21:07:55 -0700
Subject: [R] timeDate & business day
In-Reply-To: <45F621C2.4040703@sfu.ca>
References: <45F5EC64.8020808@sfu.ca>
	<04b901c76512$bd1894f0$3927a8c0@treesdalellc.net>
	<45F621C2.4040703@sfu.ca>
Message-ID: <AD40423F-7AE6-4021-BA10-26410FD76736@gmail.com>

Thanks so Michael! If you know of a tutorial or introductory document  
about timeDate manipulation or time series manipulation in R, can you  
share it? It is hard to find by googling... I'd very appreciate any  
advice.

Young.

On Mar 12, 2007, at 9:00 PM, Michael Toews wrote:

>
>> [1] 20050104 20050105 20050106 20050107 20050110 20050111 20050113  
>> 20050114
>>> ymd <- as.Date(as.character(ymd.int),"%Y%m%d")
>>> ymd
>> [1] "2005-01-04" "2005-01-05" "2005-01-06" "2005-01-07" "2005-01-10"
>> [6] "2005-01-11" "2005-01-13" "2005-01-14"
>>> class(ymd)
>> [1] "Date"
>>
>> While the variable ymd is actually of class Date, the format is  
>> not yyyymmdd but
>> yyyy-mm-dd as one can see in the previous example.
>> As Young, I do not see what I am missing here.
>> Any hint would be appreciated.
>>
>> AA.
> What happened in the beginning is that I had to parse the character  
> into a Date-Time class ("Date", in this case as you correctly  
> pointed out). POSIX is a kind of standard that (mainly Unix)  
> computers use date formatters, such as %Y for a 4-digit year, and  
> others. They are all listed in great detail in "?strptime" (which  
> means "string parse time"). In this case the input parsing format  
> pattern was "%Y%m%d". There were no spaces in-between each number.
>
> When that class prints out, the default format is ISO 8601 ( see  
> http://en.wikipedia.org/wiki/ISO_8601 ). When R prints the class  
> "Date" to your screen, it decides to format it ISO 8601-style for  
> you. If you want to see if differently, you can try:
>
> format(ymd,"%Y/%d/%m")
>
> The date is actually stored internally as an ordinal, somewhat like  
> how MS Excel dates work. You can see how it works internally:
>
> str(ymd)
>
> Hopefully I've demystified some of this .. any other questions?
> +mt


From n.nguyen at garvan.org.au  Tue Mar 13 05:19:58 2007
From: n.nguyen at garvan.org.au (Nguyen Dinh Nguyen)
Date: Tue, 13 Mar 2007 15:19:58 +1100
Subject: [R] Highlight overlapping area between two curves
In-Reply-To: <mailman.11.1173697204.10885.r-help@stat.math.ethz.ch>
Message-ID: <005501c76526$df95d650$0fe05e81@D145LD1S>

Dear R helpers,
I have a graph as following; I would like to highlight the overlapping area
between the two curves. Do you know how to do this?
Thank you in advance for your help.
Nguyen

###START
x1 <- rnorm(10000, 0.70,0.12)
x2 <- rnorm(10000, 0.90,0.12)

d1 <- density(x1)
d2 <- density(x2)

plot(range(d1$x,d2$x), range(d1$y, d2$y), type = "n",
     xlab = "X value", ylab = "Probability Density" )

lines(d1, col = "red",lwd=4, lty=2)
lines(d2, col = "blue",lwd=4)

##END CODE


From christos at nuverabio.com  Tue Mar 13 05:24:50 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Tue, 13 Mar 2007 00:24:50 -0400
Subject: [R] Highlight overlapping area between two curves
In-Reply-To: <005501c76526$df95d650$0fe05e81@D145LD1S>
References: <mailman.11.1173697204.10885.r-help@stat.math.ethz.ch>
	<005501c76526$df95d650$0fe05e81@D145LD1S>
Message-ID: <001801c76527$8aacf870$0202a8c0@headquarters.silicoinsights>

See
http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=7

and code therein.

-Christos 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nguyen 
> Dinh Nguyen
> Sent: Tuesday, March 13, 2007 12:20 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Highlight overlapping area between two curves
> 
> Dear R helpers,
> I have a graph as following; I would like to highlight the 
> overlapping area between the two curves. Do you know how to do this?
> Thank you in advance for your help.
> Nguyen
> 
> ###START
> x1 <- rnorm(10000, 0.70,0.12)
> x2 <- rnorm(10000, 0.90,0.12)
> 
> d1 <- density(x1)
> d2 <- density(x2)
> 
> plot(range(d1$x,d2$x), range(d1$y, d2$y), type = "n",
>      xlab = "X value", ylab = "Probability Density" )
> 
> lines(d1, col = "red",lwd=4, lty=2)
> lines(d2, col = "blue",lwd=4)
> 
> ##END CODE
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From exonintron at gmail.com  Tue Mar 13 05:28:54 2007
From: exonintron at gmail.com (Sender)
Date: Mon, 12 Mar 2007 21:28:54 -0700
Subject: [R] An example of "overloading" [
Message-ID: <686bf0c50703122128y377b730elc2db564439a2099a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070312/8a619b43/attachment.pl 

From creflow.juanti at tangana.com  Tue Mar 13 02:43:14 2007
From: creflow.juanti at tangana.com (Elizabeth & Deacon Creflow Juanti)
Date: 13 Mar 2007 01:43:14 -0000
Subject: [R] John chapter 4:vs23-24.
Message-ID: <20070313014314.12994.qmail@webserver.alado.com.br>


Dear In Christ,

The time has come for Christians to worship God in spirit and in truth
according to the book of John chapter 4:vs23-24.``But the hour cometh
and now is when the true worshipers shall worship the Father in spirit
and in truth: for the Father seeketh such to worship him . God is a
spirit, and they that worship him must worship him in spirit and in
truth. Based on this scripture, it became obvious that I should do
the right thing if I must enter into the kingdom of God.

I am , Deacon Creflow Juanti the legal adviser to late Mr. Mike & 
Carol Hall, a God fearing and dedicated couple. They were very wealthy but had no child. They travelled to Patong-Thailand for Christmas holiday but met death on the 26th of December 2004 during the Tsunami
disaster(http://news.bbc.co.uk/1/hi/england/bristol/4153821.stm
As their legal adviser, before their death, the husband Mr. Mike Hall
instructed me to write his WILL, because they had no child, he
dedicated their wealth to God. They had a lot of landed properties
houses Stocks/bonds, etc. According to their instruction.

According to the WILL, their assets should be given out to a ministry
for the work of God. As their legal adviser, all the documents for the
fund that are deposited with the Vault company are in my care. As a
born again Christian , I have been reading my bible and I have to do
what is lawful and right in the sight of God by giving out the fund to 
the
chosen ministry for the purpose of God's work as instructed by the
owners before there death. After my fasting and prayers Today, I asked
God to make his choice and direct me to an honest Christian or the
chosen ministry that deserves this fund by his Grace. I then came
across your address on the Internet.I appeal to you to use the fund
wisely for things that will glorify the name of God.

Also, could you get back to me having visiting the above website to
enable us discuss in a more clarifying manner to the best of your
understanding. I must say that I'm very uncomfortable sending this
message to you without knowing truly if you would misconstrue the
importance and decides to go public. In this regards, I will not hold
back to say that the essence of this message is strictly for Charity.
May God bless you as you make up your mind to work for God.
Thanks.

Elizabeth & Deacon Creflow Juanti
Tel:+44-7040117960


From marc_schwartz at comcast.net  Tue Mar 13 06:09:48 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 13 Mar 2007 00:09:48 -0500
Subject: [R] An example of "overloading" [
In-Reply-To: <686bf0c50703122128y377b730elc2db564439a2099a@mail.gmail.com>
References: <686bf0c50703122128y377b730elc2db564439a2099a@mail.gmail.com>
Message-ID: <1173762588.5051.104.camel@localhost.localdomain>

On Mon, 2007-03-12 at 21:28 -0700, Sender wrote:
> Hello:
> 
> Could anyone point me to a nice example where someone has created methods
> for "[" on a user defined Class?
> 
> I looked at the package Matrix but that was a little daunting. I'm looking
> for someone a little more introductory. I've tried to search the help
> section and the web but its difficult since "[" isn't searchable.
> 
> Thanks in advance!
> 
> Greg

You might want to look at this thread from last year:

  http://finzi.psych.upenn.edu/R/Rhelp02a/archive/77057.html

  http://finzi.psych.upenn.edu/R/Rhelp02a/archive/77060.html

  http://finzi.psych.upenn.edu/R/Rhelp02a/archive/77059.html

  http://finzi.psych.upenn.edu/R/Rhelp02a/archive/77102.html


You can also review the existing methods defined for '[' in your current
installation by using:

  methods("[")

It may be easiest to then review the code for a given method by using
something like the following as an example:

  getAnywhere("[.data.frame")


BTW, for searching the R help files, REGEX's are used, so for "[", you
would need:

  help.search("\\[")

since '[' is a special character in regular expressions and you need to
escape it to search on the literal character. In R, you need to double
the '\' to be interpreted properly.

HTH,

Marc Schwartz


From exonintron at gmail.com  Tue Mar 13 06:13:05 2007
From: exonintron at gmail.com (Sender)
Date: Mon, 12 Mar 2007 22:13:05 -0700
Subject: [R] An example of "overloading" [
In-Reply-To: <1173762588.5051.104.camel@localhost.localdomain>
References: <686bf0c50703122128y377b730elc2db564439a2099a@mail.gmail.com>
	<1173762588.5051.104.camel@localhost.localdomain>
Message-ID: <686bf0c50703122213k7d766aechbe5e016fb322dcb1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070312/e7fa0684/attachment.pl 

From kate.stark at utas.edu.au  Tue Mar 13 06:31:59 2007
From: kate.stark at utas.edu.au (Kate Stark)
Date: Tue, 13 Mar 2007 16:31:59 +1100
Subject: [R] AR(1) models with gls
Message-ID: <200703130532.l2D5Vx8O000537@corinna.its.utas.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070313/c0b5d299/attachment.pl 

From skiadas at hanover.edu  Tue Mar 13 06:53:01 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Tue, 13 Mar 2007 01:53:01 -0400
Subject: [R] Highlight overlapping area between two curves
In-Reply-To: <005501c76526$df95d650$0fe05e81@D145LD1S>
References: <005501c76526$df95d650$0fe05e81@D145LD1S>
Message-ID: <2088BB76-DE19-4B6A-A382-00B3315BD799@hanover.edu>

On Mar 13, 2007, at 12:19 AM, Nguyen Dinh Nguyen wrote:

> Dear R helpers,
> I have a graph as following; I would like to highlight the  
> overlapping area
> between the two curves. Do you know how to do this?
> Thank you in advance for your help.

Perhaps not exactly what you wanted, but it might give you some ideas:

p <- seq(0.2,1.4,0.01)
x1 <- dnorm(p, 0.70, 0.12)
x2 <- dnorm(p, 0.90, 0.12)
plot(range(p), range(x1,x2), type="n")
lines(p, x1, col = "red",lwd=4, lty=2)
lines(p, x2, col = "blue",lwd=4)
polygon(c(p,p[1]),c(pmin(x1,x2),0), col="grey")


> Nguyen

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From mwtoews at sfu.ca  Tue Mar 13 06:59:29 2007
From: mwtoews at sfu.ca (Michael Toews)
Date: Mon, 12 Mar 2007 22:59:29 -0700
Subject: [R] timeDate & business day
In-Reply-To: <AD40423F-7AE6-4021-BA10-26410FD76736@gmail.com>
References: <45F5EC64.8020808@sfu.ca>
	<04b901c76512$bd1894f0$3927a8c0@treesdalellc.net>
	<45F621C2.4040703@sfu.ca>
	<AD40423F-7AE6-4021-BA10-26410FD76736@gmail.com>
Message-ID: <45F63DC1.6010108@sfu.ca>

Sadly, I don't know of any tutorials or much help on the web for R ... 
that doesn't mean it doesn't exist ... you might just have to look 
around for it (www.rseek.org is a good place to start)
I've learned almost everything I know through:
?strptime

Also check out the methods for the classes, for example:

methods(class="Date")
methods(class="POSIXct")

And certainly check their help pages ... there is loads of stuff here 
that I haven't discovered myself. (Note, if you are new to S3 classes .. 
if it begins with the method, then "." class, you only need to type the 
beginning. For example "summary(ymd)" ... not "summary.Date(ymd)" if 
"ymd" has `class(ymd) == "Date" `.

I think the fundamental things to know are there are three main 
DateTimeClasses:

  1. "POSIXct" - has date, time and optionally time-zone info -- very
     handy for using in data.frame objects (and frankly I think it
     should be renamed to "DateTime" since the class "POSIXct" has
     nothing really to do directly with date/times)
  2. "POSIXlt" - as far as I'm concerned, this is has the same
     functionality as "POSIXct", but it cannot be used in data.frame
     objects (and frankly, I think it should be deprecated in favour of
     #1 to reduce future confusion)
  3. "Date" - use this if you don't care about times or time-zones

But it would be nice to track down a good tutorial somewhere.
+mt

Young Cho wrote:
> Thanks so Michael! If you know of a tutorial or introductory document 
> about timeDate manipulation or time series manipulation in R, can you 
> share it? It is hard to find by googling... I'd very appreciate any 
> advice.


From creflow.juanti at tangana.com  Tue Mar 13 02:57:24 2007
From: creflow.juanti at tangana.com (Elizabeth & Deacon Creflow Juanti)
Date: 13 Mar 2007 01:57:24 -0000
Subject: [R] John chapter 4:vs23-24.
Message-ID: <20070313015724.11848.qmail@webserver.alado.com.br>


Dear In Christ,

The time has come for Christians to worship God in spirit and in truth
according to the book of John chapter 4:vs23-24.``But the hour cometh
and now is when the true worshipers shall worship the Father in spirit
and in truth: for the Father seeketh such to worship him . God is a
spirit, and they that worship him must worship him in spirit and in
truth. Based on this scripture, it became obvious that I should do
the right thing if I must enter into the kingdom of God.

I am , Deacon Creflow Juanti the legal adviser to late Mr. Mike & 
Carol Hall, a God fearing and dedicated couple. They were very wealthy but had no child. They travelled to Patong-Thailand for Christmas holiday but met death on the 26th of December 2004 during the Tsunami
disaster(http://news.bbc.co.uk/1/hi/england/bristol/4153821.stm
As their legal adviser, before their death, the husband Mr. Mike Hall
instructed me to write his WILL, because they had no child, he
dedicated their wealth to God. They had a lot of landed properties
houses Stocks/bonds, etc. According to their instruction.

According to the WILL, their assets should be given out to a ministry
for the work of God. As their legal adviser, all the documents for the
fund that are deposited with the Vault company are in my care. As a
born again Christian , I have been reading my bible and I have to do
what is lawful and right in the sight of God by giving out the fund to 
the
chosen ministry for the purpose of God's work as instructed by the
owners before there death. After my fasting and prayers Today, I asked
God to make his choice and direct me to an honest Christian or the
chosen ministry that deserves this fund by his Grace. I then came
across your address on the Internet.I appeal to you to use the fund
wisely for things that will glorify the name of God.

Also, could you get back to me having visiting the above website to
enable us discuss in a more clarifying manner to the best of your
understanding. I must say that I'm very uncomfortable sending this
message to you without knowing truly if you would misconstrue the
importance and decides to go public. In this regards, I will not hold
back to say that the essence of this message is strictly for Charity.
May God bless you as you make up your mind to work for God.
Thanks.

Elizabeth & Deacon Creflow Juanti
Tel:+44-7040117960


From kate.stark at utas.edu.au  Tue Mar 13 07:03:54 2007
From: kate.stark at utas.edu.au (Kate Stark)
Date: Tue, 13 Mar 2007 17:03:54 +1100
Subject: [R] AR(1) models with gls
Message-ID: <200703130603.l2D63t6x017065@corinna.its.utas.edu.au>


From ggrothendieck at gmail.com  Tue Mar 13 07:07:29 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 13 Mar 2007 02:07:29 -0400
Subject: [R] timeDate & business day
In-Reply-To: <AD40423F-7AE6-4021-BA10-26410FD76736@gmail.com>
References: <45F5EC64.8020808@sfu.ca>
	<04b901c76512$bd1894f0$3927a8c0@treesdalellc.net>
	<45F621C2.4040703@sfu.ca>
	<AD40423F-7AE6-4021-BA10-26410FD76736@gmail.com>
Message-ID: <971536df0703122307m6a6721c2o915f7f86fc53dece@mail.gmail.com>

Regarding time series manipulation in R you could read the
two vignettes that come with the zoo package:

library(zoo)
vignette("zoo")
vignette("zoo-quickref")

Note that zoo is an entirely different system than rmetrics and timeDate so
if your question is specifically aimed at timeDate this does not answer it.
Regarding dates see R News 4/1 help desk article. This article discusses
Date, chron and POSIXct classes, any of which can be used with zoo.


On 3/13/07, Young Cho <young.stat at gmail.com> wrote:
> Thanks so Michael! If you know of a tutorial or introductory document
> about timeDate manipulation or time series manipulation in R, can you
> share it? It is hard to find by googling... I'd very appreciate any
> advice.
>
> Young.
>
> On Mar 12, 2007, at 9:00 PM, Michael Toews wrote:
>
> >
> >> [1] 20050104 20050105 20050106 20050107 20050110 20050111 20050113
> >> 20050114
> >>> ymd <- as.Date(as.character(ymd.int),"%Y%m%d")
> >>> ymd
> >> [1] "2005-01-04" "2005-01-05" "2005-01-06" "2005-01-07" "2005-01-10"
> >> [6] "2005-01-11" "2005-01-13" "2005-01-14"
> >>> class(ymd)
> >> [1] "Date"
> >>
> >> While the variable ymd is actually of class Date, the format is
> >> not yyyymmdd but
> >> yyyy-mm-dd as one can see in the previous example.
> >> As Young, I do not see what I am missing here.
> >> Any hint would be appreciated.
> >>
> >> AA.
> > What happened in the beginning is that I had to parse the character
> > into a Date-Time class ("Date", in this case as you correctly
> > pointed out). POSIX is a kind of standard that (mainly Unix)
> > computers use date formatters, such as %Y for a 4-digit year, and
> > others. They are all listed in great detail in "?strptime" (which
> > means "string parse time"). In this case the input parsing format
> > pattern was "%Y%m%d". There were no spaces in-between each number.
> >
> > When that class prints out, the default format is ISO 8601 ( see
> > http://en.wikipedia.org/wiki/ISO_8601 ). When R prints the class
> > "Date" to your screen, it decides to format it ISO 8601-style for
> > you. If you want to see if differently, you can try:
> >
> > format(ymd,"%Y/%d/%m")
> >
> > The date is actually stored internally as an ordinal, somewhat like
> > how MS Excel dates work. You can see how it works internally:
> >
> > str(ymd)
> >
> > Hopefully I've demystified some of this .. any other questions?
> > +mt
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From exonintron at gmail.com  Tue Mar 13 07:41:46 2007
From: exonintron at gmail.com (Sender)
Date: Mon, 12 Mar 2007 23:41:46 -0700
Subject: [R] An example of "overloading" [
In-Reply-To: <686bf0c50703122213k7d766aechbe5e016fb322dcb1@mail.gmail.com>
References: <686bf0c50703122128y377b730elc2db564439a2099a@mail.gmail.com>
	<1173762588.5051.104.camel@localhost.localdomain>
	<686bf0c50703122213k7d766aechbe5e016fb322dcb1@mail.gmail.com>
Message-ID: <686bf0c50703122341t6d772534v355953634d11bcf4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070312/e6c0e099/attachment.pl 

From ligges at statistik.uni-dortmund.de  Tue Mar 13 08:20:28 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 13 Mar 2007 08:20:28 +0100
Subject: [R] RMySQL on win32
In-Reply-To: <403044.82230.qm@web52415.mail.yahoo.com>
References: <403044.82230.qm@web52415.mail.yahoo.com>
Message-ID: <45F650BC.4000307@statistik.uni-dortmund.de>

If you have gcc installed, yes, still a path issue:
You have tp put MinGW's ./bin directory into your PATH.

Best,
uwe


Pete Cap wrote:
> */Uwe Ligges <ligges at statistik.uni-dortmund.de>/* wrote:
> 
>     Please follow the Instructions to set up your build environment as
>     mentioned in the R Installation and Administration manual. That
>     includes
>     installing the tools from Duncan Murdochs Webpage and putting it into
>     your path. You either forgot to install those tools or you forgot to
>     add
>     it to your PATH environment variable.
> 
>     Uwe Ligges
> 
> 
> 
>      > Apparently R is trying to call some shell script (from the
>     windows prompt??) so I attempted this in cygwin. Results:
>      >
>      > Can't locate R/Dcf.pm in @INC (@INC contains: c
>     \PROGRA~1\R\R-24~1.1\share\perl; /usr/lib/perl5/5.8/cygwin
>     /usr/lib/perl5/5.8 /usr/lib/perl5/site_perl/5.8/cygwin
>     /usr/lib/perl5/site_perl/5.8 /usr/lib/perl5/site_perl/5.8/cygwin
>     /usr/lib/perl5/site_perl/5.8 /usr/lib/perl5/vendor_perl/5.8/cygwin
>     /usr/lib/perl5/vendor_perl/5.8 /usr/lib/perl5/vendor_perl/5.8/cygwin
>     /usr/lib/perl5/vendor_perl/5.8 .) at
>     c:\PROGRA~1\R\R-24~1.1/bin/build line 29.
>      > BEGIN failed--compilation aborted at
>     c:\PROGRA~1\R\R-24~1.1/bin/build line 29.
>      >
>      > Dcf.pm is actually located in C:\Program
>     Files\R\R-2.4.1\share\perl\R. I wonder if the variable @INC is
>     simply incorrect (it's looking under R-24~1.1, not sure if the
>     truncated value is actually correct) but I have no idea in which
>     file it may be located.
>      >
>      > Anyone have any ideas?
>      > I have installed mingw-utils while attempting to get this up and
>     running, if it matters.
>      >
>      > If there is a simply better solution that I should try, I would
>     appreciate hearing about it as well. All I really need to do at this
>     point is send select and join queries to the local server--perhaps I
>     should just install RSQLite from CRAN?
>      >
>      > Thanks in advance,
>      >
>      > Pete
>      >
>      >
>      > ---------------------------------
>      > Be a PS3 game guru.
>      >
>      > [[alternative HTML version deleted]]
>      >
>      > ______________________________________________
>      > R-help at stat.math.ethz.ch mailing list
>      > https://stat.ethz.ch/mailman/listinfo/r-help
>      > PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>      > and provide commented, minimal, self-contained, reproducible code.
> 
> 
> Uwe,
> 
> It seems you are correct!
> I neglected to alter my PATH statement.
> 
> My only remaining issue, it seems, is what to add to %PATH%.
> 
> The R documentation instructs the reader to download various tools in 
> the MinGW set ("An alternative is to download the components 
> individually...and unpack these into the same directory"),  All of the 
> recommended files were unzipped and untarred to C:\MinGW, which is in 
> the path, but obviously this is insufficient as there are binaries in 
> most of of the subdirectories.  The result is now:
> 
> C:\>Rcmd build --binary RMySQL
> * checking for file 'RMySQL/DESCRIPTION' ... OK
> * preparing 'RMySQL':
> * checking DESCRIPTION meta-information ... OK
> * cleaning src
> * checking whether 'INDEX' is up-to-date ... NO
> * use '--force' to overwrite the existing 'INDEX'
> * removing junk files
> * checking for LF line-endings in source files
> * checking for empty or unneeded directories
> * building binary distribution
>  WARNING
> * some HTML links may not be found
> installing R.css in c:/TEMP/Rinst19605847
> 
> Using auto-selected zip options ' RMySQL-HELP=ziponly'
> 
> ---------- Making package RMySQL ------------
> ======================================================================
> RMySQL configure.win:
> * Using mysql libraries from c:/PROGRA~1/MySQL/MYSQLS~1.0/lib/opt
> * Using mysql dll from c:/PROGRA~1/MySQL/MYSQLS~1.0/bin
> * Copying runtime libMySQL.dll and libmysql.lib to inst/libs
> * Using an existing libmysql.a in c:/PROGRA~1/MySQL/MYSQLS~1.0/lib/opt
> ======================================================================
>   adding build stamp to DESCRIPTION
>   making DLL ...
> making RS-DBI.d from RS-DBI.c
> make[3]: gcc: Command not found
> make[3]: *** [RS-DBI.d] Error 127
> make[2]: *** [srcDynlib] Error 2
> make[1]: *** [all] Error 2
> make: *** [pkg-RMySQL] Error 2
> *** Installation of RMySQL failed ***
> 
> Removing 'c:/TEMP/Rinst19605847/RMySQL'
>  ERROR
> * installation failed
> 
> Is this still a path issue?  Just wondering before I go messing with it 
> even more.
> 
> Thanks,
> 
> Pete
> 
> ------------------------------------------------------------------------
> Get your own web address. 
> <http://us.rd.yahoo.com/evt=49678/*http://smallbusiness.yahoo.com/domains/?p=BESTDEAL>
> Have a HUGE year through Yahoo! Small Business. < 
> http://us.rd.yahoo.com/evt=49678/*http://smallbusiness.yahoo.com/domains/?p=BESTDEAL> 
>


From Roger.Bivand at nhh.no  Tue Mar 13 08:42:43 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 13 Mar 2007 08:42:43 +0100 (CET)
Subject: [R] An example of "overloading" [
In-Reply-To: <686bf0c50703122341t6d772534v355953634d11bcf4@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0703130837010.9679-100000@reclus.nhh.no>

On Mon, 12 Mar 2007, Sender wrote:

> Any insight as to why this is printing twice?

Try:

m <- NA
class(m) <- "mydist"
m[]
res <- m[]
res

(hint - print.default() is returning what it was asked to print, which 
then gets printed again automatically)

> 
> "[.mydist" <- function(x,...){
>     print("I called my function")
> }
> 
> m <- a_mydist_obj
> m[]
> 
> I called my function
> I called my function
> 
> 
> 
> 
> On 3/12/07, Sender <exonintron at gmail.com> wrote:
> >
> > Super. Thanks for the leads.
> >
> > On 3/12/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> > >
> > > On Mon, 2007-03-12 at 21:28 -0700, Sender wrote:
> > > > Hello:
> > > >
> > > > Could anyone point me to a nice example where someone has created
> > > methods
> > > > for "[" on a user defined Class?
> > > >
> > > > I looked at the package Matrix but that was a little daunting. I'm
> > > looking
> > > > for someone a little more introductory. I've tried to search the help
> > > > section and the web but its difficult since "[" isn't searchable.
> > > >
> > > > Thanks in advance!
> > > >
> > > > Greg
> > >
> > > You might want to look at this thread from last year:
> > >
> > >   http://finzi.psych.upenn.edu/R/Rhelp02a/archive/77057.html
> > >
> > >    http://finzi.psych.upenn.edu/R/Rhelp02a/archive/77060.html
> > >
> > >   http://finzi.psych.upenn.edu/R/Rhelp02a/archive/77059.html
> > >
> > >    http://finzi.psych.upenn.edu/R/Rhelp02a/archive/77102.html
> > >
> > >
> > > You can also review the existing methods defined for '[' in your current
> > > installation by using:
> > >
> > >   methods("[")
> > >
> > > It may be easiest to then review the code for a given method by using
> > > something like the following as an example:
> > >
> > >   getAnywhere("[.data.frame")
> > >
> > >
> > > BTW, for searching the R help files, REGEX's are used, so for "[", you
> > > would need:
> > >
> > >   help.search ("\\[")
> > >
> > > since '[' is a special character in regular expressions and you need to
> > > escape it to search on the literal character. In R, you need to double
> > > the '\' to be interpreted properly.
> > >
> > > HTH,
> > >
> > > Marc Schwartz
> > >
> > >
> > >
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From jarioksa at sun3.oulu.fi  Tue Mar 13 09:00:56 2007
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Tue, 13 Mar 2007 10:00:56 +0200
Subject: [R]  distance metrics
In-Reply-To: 686bf0c50703121602j3e0baf33k64d9b7aeae807a30@mail.gmail.com
Message-ID: <1173772857.16563.15.camel@biol102145.oulu.fi>

On Tue Mar 13 00:21:22 CET 2007 Gavin Simpson wrote:
> On Mon, 2007-03-12 at 16:02 -0700, Sender wrote:
>  Thanks for the suggestion Christian. I'm trying to avoid expanding the dist
> > object to a matrix, since i'm usually working with microarray data which
> > produces a distance matrix of size 5000 x 5000.
> > 
> > If i can keep it in its condensed form i think it will speed things up.
> > 
> > Is my thinking correct?
> 
> That will all depend on what you want to do with it...
> 
> A dist object of that size is c. 100 MB in memory, and c. 200 MB in size
> as the full dissimilarity matrix - values from object.size(). Of course,
> you'll need a reasonable amount of free memory over and above this to do
> anything useful with the matrix as copies may be required during
> analysis/processing etc.
> 
> Of course, a dist object is just a vector of observed distances with
> various attributes, so one can always use "[" for vectors, but I imagine
> that anything other than trivial operations will become fiddly,
> complicated and time consuming - if you have the memory, give the
> as.matrix option a try and see how it works for your specific problems.
> 
Such a fiddling could be a function that returns the index in the dist vector:
 
idx <- function(i, j, Size) 
{ 
  a <- min(i,j) 
  b <- max(i,j) 
  Size*(a-1) - a*(a-1)/2 + b - a 
} 

where i and j are the desired matrix indices and Size is the number of
observations, or the attribute "Size" of a 'dist' object. (The function
will fail if i==j or any(c(i,j) > Size) and with some other potential
abuse.)

You can refer to your individual distances from 5000 observations as:

dis[idx(2417, 1105, 5000)]

This is slower, of course, but avoids expanding to a matrix. 

Perhaps a nicer and easier to use (but more opaque) way is to write the
function as:

getidx <- function(dist, i, j) 
{
    dist[idx(i, j, attr(dist, "Size"))]
}

which can be used with fewer bracket types: getidx(dist, 2417, 1105).

cheers, jari oksanen


From researchjj at gmail.com  Tue Mar 13 09:40:42 2007
From: researchjj at gmail.com (j.joshua thomas)
Date: Tue, 13 Mar 2007 16:40:42 +0800
Subject: [R] Rattle() GGobi()-Plots- How to save?
Message-ID: <b4485c4c0703130140n315e5e77p3f0a76f25ac9d951@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070313/ceee1a08/attachment.pl 

From norbertneuwirth at gmx.at  Tue Mar 13 10:12:22 2007
From: norbertneuwirth at gmx.at (Norbert NEUWIRTH)
Date: Tue, 13 Mar 2007 10:12:22 +0100
Subject: [R] turn regression coefficients into matrix or...
In-Reply-To: <BAY108-F62F124040974B563C5604A87C0@phx.gbl>
References: <BAY108-F62F124040974B563C5604A87C0@phx.gbl>
Message-ID: <op.to4ehwx736de38@nn-notebook>

jun,

im am also quite new to R. so i think, this is a question all we R-newbees  
ask  ;-). having had the same "problem" the other day, i "solved" it the  
following way:

####  Multiple Regressions - Tables

data(anscombe)                                      # load anscombe  
dataset (implemented somwhere in R)
x5 <- rnorm(11,14,1)                                # create an additional  
variable (at random)
anscombe.nn <- cbind(anscombe,x5)                   # attach the variable  
to the dataset
attach(anscombe.nn)                                 # attach the dataset  
to the searchpath (just for convenience)
anscombe.nn                                         # have a look on the  
data

ans.reg <- vector(4, mode = "list")                 # create empty list  
(just  for speeding up)

for(i in 1:4){                                      # now the 4  
regressions are stored to the list
     j <- i+1
     x1 <- get(paste("x", i, sep = ""))              # exogenous var. #1
     x2 <- get(paste("x", j, sep = ""))              # exogenous var. #2
     y <- get(paste("y", i, sep = ""))               # endogenous
     ans.reg[[i]] <- glm(y ~ x1+x2,family=gaussian)  # do the regression  
(out of 4)
     print(summary(ans.reg[[i]],cor=FALSE))
}

detach(anscombe)                                    # detach dataset from  
search path
lapply(ans.reg, coef)                               # see each regression  
in one line
sapply(ans.reg, coef)                               # have the  
coefficients in a table
x<-as.matrix(sapply(ans.reg, coef))                 # convert table to  
matrix
x

this solution is quite comparable to Ligges(2007) [published in german and  
japanese, i think]

i have an additional question to list-members, that have left the  
newbee-status yet: how can i get R to hand over the standard errors,  
significance levels etc, so that i can create a table with  coeff and SE  
and sig.?

norbert


Am 13.03.2007, 02:58 Uhr, schrieb Jun Xu <mystata at hotmail.com>:

> I don't have much experience with r. What I am trying to do is to turn
> regression coefficients (after I run a lm or glm model) into some matrix
> such that I can do some post-estimation calculation, for example  
> predicted
> probabilities in glm model, etc.. Or, is there any function in r that I  
> can
> use to do something along that line? thanks.
>
> Jun Xu, Ph.D.
> Department of Sociology
> Ball State University
>
> _________________________________________________________________
> Mortgage rates as low as 4.625% - Refinance $150,000 loan for $579 a  
> month.
> Intro*Terms
> https://www2.nextag.com/goto.jsp?product=100000035&url=%2fst.jsp&tm=y&search=mortgage_text_links_88_h27f6&disc=y&vers=743&s=4056&p=5117
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide  
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
-------------------------------
Mag. Norbert NEUWIRTH

Roubiczekgasse 2/23
A-1100  WIEN
mob: +43 699 1835 0704


From solenne.carat at univ-nantes.fr  Tue Mar 13 11:12:56 2007
From: solenne.carat at univ-nantes.fr (Solenne Carat)
Date: Tue, 13 Mar 2007 11:12:56 +0100
Subject: [R] compatibility S-plus R: stepwise fonction
Message-ID: <45F67928.7010000@univ-nantes.fr>

Hi,
I tried to use S-plus script with R. I thought it was easy because there 
aren't a lot differences between S-plus and R syntaxe. However, I didn't 
succeed to use stepwise function in R. I think the best equivalent in R 
is step, but this function seems not to use same parameters.
is there a better equivalent in R ? or package to make stepwise 
compatible with R ?

Thank you for answers
Regards,

solenne carat
student
Institut du thorax - Inserm U533
France


From samay.sar at gmail.com  Tue Mar 13 11:41:02 2007
From: samay.sar at gmail.com (d. sarthi maheshwari)
Date: Tue, 13 Mar 2007 16:11:02 +0530
Subject: [R] gtk button: how to create signal handler?
Message-ID: <d4327f7e0703130341n3b70ab8fk3673ec0971171c59@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070313/f39b7045/attachment.pl 

From M.J.Bojanowski at uu.nl  Tue Mar 13 11:49:46 2007
From: M.J.Bojanowski at uu.nl (Bojanowski, M.J.  (Michal))
Date: Tue, 13 Mar 2007 11:49:46 +0100
Subject: [R] Highlight overlapping area between two curves
In-Reply-To: <005501c76526$df95d650$0fe05e81@D145LD1S>
References: <mailman.11.1173697204.10885.r-help@stat.math.ethz.ch>
	<005501c76526$df95d650$0fe05e81@D145LD1S>
Message-ID: <94E133D09AA24D43BF6341B675C01A3311350A@uu01msg-exb01.soliscom.uu.nl>

If PDF is OK, you can use the 'alpha' argument in colors, i.e.:

pdf( "file.pdf")
p <- seq(0.2,1.4,0.01)
x1 <- dnorm(p, 0.70, 0.12)
x2 <- dnorm(p, 0.90, 0.12)
plot(range(p), range(x1,x2), type="n")
polygon(p, x1, col = rgb(1,0,0, .5),lwd=4, lty=2)
polygon(p, x2, col = rgb(0,0,1, .5),lwd=4)
dev.off()
 

hth,
Michal

*** Note that my e-mail address has changed to m.j.bojanowski at uu.nl
*** Please update your address books accordingly. Thank you!

_________________________________________
Michal Bojanowski
ICS / Sociology
Utrecht University
Heidelberglaan 2; 3584 CS Utrecht
Room 1428
m.j.bojanowski at uu.nl
http://www.fss.uu.nl/soc/bojanowski
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nguyen Dinh
Nguyen
Sent: Tuesday, March 13, 2007 5:20 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Highlight overlapping area between two curves

Dear R helpers,
I have a graph as following; I would like to highlight the overlapping
area between the two curves. Do you know how to do this?
Thank you in advance for your help.
Nguyen

###START
x1 <- rnorm(10000, 0.70,0.12)
x2 <- rnorm(10000, 0.90,0.12)

d1 <- density(x1)
d2 <- density(x2)

plot(range(d1$x,d2$x), range(d1$y, d2$y), type = "n",
     xlab = "X value", ylab = "Probability Density" )

lines(d1, col = "red",lwd=4, lty=2)
lines(d2, col = "blue",lwd=4)

##END CODE

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From amitsoni.84 at gmail.com  Tue Mar 13 06:21:56 2007
From: amitsoni.84 at gmail.com (Amit Soni)
Date: Mon, 12 Mar 2007 22:21:56 -0700
Subject: [R] Solving PDEs
Message-ID: <9fb977ed0703122221ja5b49e9o4b3b0bc94064ef86@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070312/07a2bc89/attachment.pl 

From lhill07 at qub.ac.uk  Tue Mar 13 12:22:47 2007
From: lhill07 at qub.ac.uk (Laura Hill)
Date: Tue, 13 Mar 2007 11:22:47 +0000
Subject: [R]  multiplying matrix by vector of times
In-Reply-To: <45F08A60.6080604@ms.unimelb.edu.au>
Message-ID: <C21C3A07.C62%lhill07@qub.ac.uk>




On 8/3/07 22:12, "Gad Abraham" <g.abraham at ms.unimelb.edu.au> wrote:

> Laura Hill wrote:
>> 
>> 
>> On 7/3/07 00:15, "Gad Abraham" <g.abraham at ms.unimelb.edu.au> wrote:
>> 

>>>> On 6 Mar 2007, at 08:54, Laura Hill wrote:
>>>> 
>>>>> Hi,
>>>>> 
>>>>> My name is Laura. I'm a PhD student at Queen's University Belfast
>>>>> and have
>>>>> just started learning R. I was wondering if somebody could help me
>>>>> to see
>>>>> where I am going wrong in my code for estimating the parameters
>>>>> [mu1, mu2,
>>>>> lambda1] of a 2-phase Coxian Distribution.
>>>>> 
>>>>> cox2.lik<-function(theta, y){
>>>>>     mu1<-theta[1]
>>>>> 
>>>>>     mu2<-theta[2]
>>>>> 
>>>>>     lambda1<-theta[3]
>>>>> 
>>>>>     p<-Matrix(c(1, 0), nrow=1, ncol=2)
>>>>> 
>>>>>     Q<-Matrix(c(-(lambda1 + mu1), 0, lambda1, -mu2), nrow=2, ncol=2)
>>>>> 
>>>>>     q<-Matrix(c(mu1, mu2), nrow=2, ncol=1)
>>>>> 
>>>>>     for (i in 1:length(y)){
>>>>>         loglik<-log(p %*% expm(Q * y(i)) %*% q)
>>>>>     return(loglik)}
>>>>> 
>>>>>     sumloglik<-sum(loglik)
>>>>> 
>>>>>     return(-sumloglik)
>>>>>     }

>> 
>> 
>> 
>> Hi Gad,
>> 
>> Yes that's exactly hat I am trying to do. If I gave you a simple example,
>> could you perhaps tell me how I could create a vector of log likelihoods.
>> 
>> 
>> 
>> Lets say I have 1x1 matrices:
>> 
>> p=[1]
>> Q=[0.05]      i.e. [mu1]
>> q=[-0.05]     i.e. [-mu1]
>> 
>> Where mu1 is the parameter that I would like to estimate and I have chosen
>> the initial value mu1=0.05
>> 
>> 
>> Loglik<-p %*% expm(Q*y) %*% q
>> 
>> Where y=(5 10)
>> 
>> I want to sum the log likelihoods that I get for y=5 and y=10 using
>> 
>> Sumloglik<-sum(allloglik)
>> 
>> Where allloglik = vector of log likelihoods
>> 
>> 
>> Any help would be greatly appreciated.
>> 
>> Thanks in advance
>> Laura
>> 
> 
> Hi Laura,
> 
> Make an empty vector of required length, then assign the loglik to each
> of its cells, and don't return() anything:
> 
> loglik <- rep(0, length(y))
> for(i in 1:length(y)){
>     loglik[i] <- log(p %*% expm(Q * y[i]) %*% q)
> }
> 
> Then you can sum(loglik) like you did before.
> 
> Cheers,
> Gad


Hi Gad,

Thanks for the tip about the empty vector, I ever knew you could do that. I
just have one problem,

Lets say Q is a 2x2 matrix
         p is a 1x2 matrix
         q is a 2x1 matrix
         y is vector of times, say y = c(5, 10)

How do I multiply Q by each time y[i]?

I would like to get the answer to the equation

loglik[i] <- log(p %*% expm(Q * y[i]) %*% q)

Where first y=5 and then y=10 so that the answers to loglik for each i are
put into the empty vector.

I'm sure that I am missing something fairly obvious here but can't put my
finger on it.

Thanks in advance
Laura


From lhill07 at qub.ac.uk  Tue Mar 13 12:46:09 2007
From: lhill07 at qub.ac.uk (Laura Hill)
Date: Tue, 13 Mar 2007 11:46:09 +0000
Subject: [R]  multiplying matrix by vector of times
Message-ID: <C21C3F81.C69%lhill07@qub.ac.uk>




On 8/3/07 22:12, "Gad Abraham" <g.abraham at ms.unimelb.edu.au> wrote:

> Laura Hill wrote:
>> 
>> 
>> On 7/3/07 00:15, "Gad Abraham" <g.abraham at ms.unimelb.edu.au> wrote:
>> 

>>>> On 6 Mar 2007, at 08:54, Laura Hill wrote:
>>>> 
>>>>> Hi,
>>>>> 
>>>>> My name is Laura. I'm a PhD student at Queen's University Belfast
>>>>> and have
>>>>> just started learning R. I was wondering if somebody could help me
>>>>> to see
>>>>> where I am going wrong in my code for estimating the parameters
>>>>> [mu1, mu2,
>>>>> lambda1] of a 2-phase Coxian Distribution.
>>>>> 
>>>>> cox2.lik<-function(theta, y){
>>>>>     mu1<-theta[1]
>>>>> 
>>>>>     mu2<-theta[2]
>>>>> 
>>>>>     lambda1<-theta[3]
>>>>> 
>>>>>     p<-Matrix(c(1, 0), nrow=1, ncol=2)
>>>>> 
>>>>>     Q<-Matrix(c(-(lambda1 + mu1), 0, lambda1, -mu2), nrow=2, ncol=2)
>>>>> 
>>>>>     q<-Matrix(c(mu1, mu2), nrow=2, ncol=1)
>>>>> 
>>>>>     for (i in 1:length(y)){
>>>>>         loglik<-log(p %*% expm(Q * y(i)) %*% q)
>>>>>     return(loglik)}
>>>>> 
>>>>>     sumloglik<-sum(loglik)
>>>>> 
>>>>>     return(-sumloglik)
>>>>>     }

>> 
>> 
>> 
>> Hi Gad,
>> 
>> Yes that's exactly hat I am trying to do. If I gave you a simple example,
>> could you perhaps tell me how I could create a vector of log likelihoods.
>> 
>> 
>> 
>> Lets say I have 1x1 matrices:
>> 
>> p=[1]
>> Q=[0.05]      i.e. [mu1]
>> q=[-0.05]     i.e. [-mu1]
>> 
>> Where mu1 is the parameter that I would like to estimate and I have chosen
>> the initial value mu1=0.05
>> 
>> 
>> Loglik<-p %*% expm(Q*y) %*% q
>> 
>> Where y=(5 10)
>> 
>> I want to sum the log likelihoods that I get for y=5 and y=10 using
>> 
>> Sumloglik<-sum(allloglik)
>> 
>> Where allloglik = vector of log likelihoods
>> 
>> 
>> Any help would be greatly appreciated.
>> 
>> Thanks in advance
>> Laura
>> 
> 
> Hi Laura,
> 
> Make an empty vector of required length, then assign the loglik to each
> of its cells, and don't return() anything:
> 
> loglik <- rep(0, length(y))
> for(i in 1:length(y)){
>     loglik[i] <- log(p %*% expm(Q * y[i]) %*% q)
> }
> 
> Then you can sum(loglik) like you did before.
> 
> Cheers,
> Gad


Hi Gad,

Thanks for the tip about the empty vector, I ever knew you could do that. I
just have one problem,

Lets say Q is a 2x2 matrix
         p is a 1x2 matrix
         q is a 2x1 matrix
         y is vector of times, say y = c(5, 10)

How do I multiply Q by each time y[i]?

I would like to get the answer to the equation

loglik[i] <- log(p %*% expm(Q * y[i]) %*% q)

Where first y=5 and then y=10 so that the answers to loglik for each i are
put into the empty vector.

I'm sure that I am missing something fairly obvious here but can't put my
finger on it.

Thanks in advance
Laura


From Thomas.Adams at noaa.gov  Tue Mar 13 12:49:35 2007
From: Thomas.Adams at noaa.gov (Thomas Adams)
Date: Tue, 13 Mar 2007 07:49:35 -0400
Subject: [R] Solving PDEs
In-Reply-To: <9fb977ed0703122221ja5b49e9o4b3b0bc94064ef86@mail.gmail.com>
References: <9fb977ed0703122221ja5b49e9o4b3b0bc94064ef86@mail.gmail.com>
Message-ID: <45F68FCF.4060701@noaa.gov>

Amit,

A better tool for this purpose may be octave, which can be found at 
http://www.gnu.org/software/octave/

Regards,
Tom

Amit Soni wrote:
> Hi,
>
> Is there any method in R by which I can solve PDEs?
>
> Thank you,
> Amit
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
Thomas E Adams
National Weather Service
Ohio River Forecast Center
1901 South State Route 134
Wilmington, OH 45177

EMAIL:	thomas.adams at noaa.gov

VOICE:	937-383-0528
FAX:	937-383-0033


From osklyar at ebi.ac.uk  Tue Mar 13 13:04:36 2007
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Tue, 13 Mar 2007 12:04:36 +0000
Subject: [R] reading BMP into R
In-Reply-To: <696623.46082.qm@web56608.mail.re3.yahoo.com>
References: <696623.46082.qm@web56608.mail.re3.yahoo.com>
Message-ID: <45F69354.50008@ebi.ac.uk>

Try EBImage from Bioconductor.org

Best,
Oleg

Milton Cezar Ribeiro wrote:
> Hi R-gurus
> 
> How can I read my "bmp" files into R?
> 
> Kind regards,
> 
> 
> miltinho
> Brazil
> 
> __________________________________________________
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr Oleg Sklyar * EBI/EMBL, Cambridge CB10 1SD, England * +44-1223-494466


From Inman.Brant at mayo.edu  Tue Mar 13 13:42:54 2007
From: Inman.Brant at mayo.edu (Inman, Brant A.   M.D.)
Date: Tue, 13 Mar 2007 07:42:54 -0500
Subject: [R] Freeman-Tukey arcsine transformation
Message-ID: <6021CA6EF4C8374084D4F5A141F1CBBB664BF2@msgebe23.mfad.mfroot.org>

R-Experts:

Does anyone know if there are R functions to perform the Freeman-Tukey
double arcsine transformation and then backtransform it?

Thanks,

Brant Inman
Mayo Clinic


From nilsson.henric at gmail.com  Tue Mar 13 13:39:42 2007
From: nilsson.henric at gmail.com (Henric Nilsson (Public))
Date: Tue, 13 Mar 2007 13:39:42 +0100 (CET)
Subject: [R] Rattle() GGobi()-Plots- How to save?
In-Reply-To: <b4485c4c0703130140n315e5e77p3f0a76f25ac9d951@mail.gmail.com>
References: <b4485c4c0703130140n315e5e77p3f0a76f25ac9d951@mail.gmail.com>
Message-ID: <22038.212.209.13.15.1173789582.squirrel@www.sorch.se>

Den Ti, 2007-03-13, 09:40 skrev j.joshua thomas:
> I am using R2.4.1 and using Rattle()
> Rattle() --> Explore--> GGobi
>
> I need to save the Scatter plot matrix in any type of image format.

Have you tried the `DescribeDisplay' package?

Take a look at http://www.ggobi.org/describe-display/ for a (very) short
howto, but note that the package is found at CRAN.


HTH,
Henric



>
> But it save as *.xml file how can i reterive the *.xml into *.bmp or
> *.png?
>
> I have tried the Export button in Rattle() not much options.
>
>
> Need the group help
>
> JJ
>
>
> --
> Lecturer J. Joshua Thomas
> KDU College Penang Campus
> Research Student,
> University Sains Malaysia
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From remkoduursma at hotmail.com  Tue Mar 13 14:02:24 2007
From: remkoduursma at hotmail.com (remko duursma)
Date: Tue, 13 Mar 2007 13:02:24 +0000
Subject: [R] 'substitute' question
Message-ID: <BAY123-F44A95E77F66D702E29DEBD87C0@phx.gbl>


# I use this code to label a graph with the R2:

# graph
x <- rnorm(100)
y <- x + rnorm(100)
lm1 <- lm(y~x)
plot(x,y)

# label
R2text <- substitute(paste(R^2," = ",r2),list(r2=r2))
text(1,-3,R2text, col="red")

# i have modified this a bit, so that i have a vector with other labels, 
each of which
# will be labelled on the graph. Example:
texts <- c("And the R2 is", R2text)
x <- c(-2,-2)
y <- c(2,1)
for(i in 1:length(texts))text(x[i],y[i],texts[i],pos=4, col="blue")

# As you can see the label "R2 = 48.7" remains at "paste(R^2, " = ", 48.7)"
# What to do?



Thanks for your help,

Remko



..-~-.-~-.-~-.-~-.-~-.-~-.-~-.-~-.-~-
Remko Duursma
Post-doctoral researcher
Dept. Forest Ecology
University of Helsinki, Finland

_________________________________________________________________
With tax season right around the corner, make sure to follow these few 
simple tips.


From brown_emu at yahoo.com  Tue Mar 13 14:17:50 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Tue, 13 Mar 2007 06:17:50 -0700 (PDT)
Subject: [R] 'substitute' question
In-Reply-To: <BAY123-F44A95E77F66D702E29DEBD87C0@phx.gbl>
Message-ID: <819490.51674.qm@web39714.mail.mud.yahoo.com>

hi,

your first argument to substitute should be an expression, not a character
string (which the output of paste() will give you)

so...
# first assign to variable r2 (think you forgot to do that)
# in your example
r2 <- summary(lm1)$r.squared

# then to your label
R2text <- substitute(R^2==r2,list(r2=round(r2,2)))

# then you can annotate with text() as you did

alternatively, you can use 
R2text <- bquote(R^2==.(round(r2,2)))


--- remko duursma <remkoduursma at hotmail.com> wrote:

> 
> # I use this code to label a graph with the R2:
> 
> # graph
> x <- rnorm(100)
> y <- x + rnorm(100)
> lm1 <- lm(y~x)
> plot(x,y)
> 
> # label
> R2text <- substitute(paste(R^2," = ",r2),list(r2=r2))
> text(1,-3,R2text, col="red")
> 
> # i have modified this a bit, so that i have a vector with other labels, 
> each of which
> # will be labelled on the graph. Example:
> texts <- c("And the R2 is", R2text)
> x <- c(-2,-2)
> y <- c(2,1)
> for(i in 1:length(texts))text(x[i],y[i],texts[i],pos=4, col="blue")
> 
> # As you can see the label "R2 = 48.7" remains at "paste(R^2, " = ", 48.7)"
> # What to do?
> 
> 
> 
> Thanks for your help,
> 
> Remko
> 
> 
> 
> ..-~-.-~-.-~-.-~-.-~-.-~-.-~-.-~-.-~-
> Remko Duursma
> Post-doctoral researcher
> Dept. Forest Ecology
> University of Helsinki, Finland
> 
> _________________________________________________________________
> With tax season right around the corner, make sure to follow these few 
> simple tips.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



 
____________________________________________________________________________________
Finding fabulous fares is fun.


From peteoutside at yahoo.com  Tue Mar 13 14:26:39 2007
From: peteoutside at yahoo.com (Pete Cap)
Date: Tue, 13 Mar 2007 06:26:39 -0700 (PDT)
Subject: [R] RMySQL on win32
In-Reply-To: <45F650BC.4000307@statistik.uni-dortmund.de>
Message-ID: <362400.90478.qm@web52411.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070313/d27502a6/attachment.pl 

From F.MENDIBURU at CGIAR.ORG  Tue Mar 13 14:26:46 2007
From: F.MENDIBURU at CGIAR.ORG (Mendiburu, Felipe (CIP))
Date: Tue, 13 Mar 2007 08:26:46 -0500
Subject: [R] 'substitute' question
Message-ID: <B7B34444ECA41A41AC47DABA057CE7A2014069E9@webmail.cip.cgiar.org>

Dear Renko

To modify the script

plot(x,y)
r2<-summary(lm1)$r.squared*100
# label
R2text <- substitute(paste(R^2," = ",r2),list(r2=r2))
text(-1,1,R2text, col="red") # To see of the coordinates of the graph.

Grettings

Felipe

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of remko duursma
Sent: Tuesday, March 13, 2007 8:02 AM
To: r-help at stat.math.ethz.ch
Subject: [R] 'substitute' question



# I use this code to label a graph with the R2:

# graph
x <- rnorm(100)
y <- x + rnorm(100)
lm1 <- lm(y~x)
plot(x,y)

# label
R2text <- substitute(paste(R^2," = ",r2),list(r2=r2))
text(1,-3,R2text, col="red")

# i have modified this a bit, so that i have a vector with other labels, 
each of which
# will be labelled on the graph. Example:
texts <- c("And the R2 is", R2text)
x <- c(-2,-2)
y <- c(2,1)
for(i in 1:length(texts))text(x[i],y[i],texts[i],pos=4, col="blue")

# As you can see the label "R2 = 48.7" remains at "paste(R^2, " = ", 48.7)"
# What to do?



Thanks for your help,

Remko



..-~-.-~-.-~-.-~-.-~-.-~-.-~-.-~-.-~-
Remko Duursma
Post-doctoral researcher
Dept. Forest Ecology
University of Helsinki, Finland

_________________________________________________________________
With tax season right around the corner, make sure to follow these few 
simple tips.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From vincent.goulet at act.ulaval.ca  Tue Mar 13 14:37:08 2007
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Tue, 13 Mar 2007 08:37:08 -0500
Subject: [R] An example of "overloading" [
In-Reply-To: <686bf0c50703122128y377b730elc2db564439a2099a@mail.gmail.com>
References: <686bf0c50703122128y377b730elc2db564439a2099a@mail.gmail.com>
Message-ID: <200703130937.08404.vincent.goulet@act.ulaval.ca>

Le Mardi 13 Mars 2007 00:28, Sender a ?crit?:
> Hello:
>
> Could anyone point me to a nice example where someone has created methods
> for "[" on a user defined Class?
>
> I looked at the package Matrix but that was a little daunting. I'm looking
> for someone a little more introductory. I've tried to search the help
> section and the web but its difficult since "[" isn't searchable.
>
> Thanks in advance!

You might also have a look at the methods for "[" and "[<-" created 
for "grouped data" objects in the development version of package actuar.

Creation of the objects:
https://vgoulet.act.ulaval.ca/svn/R/actuar/trunk/actuar/R/grouped.data.R

Extraction methods:
https://vgoulet.act.ulaval.ca/svn/R/actuar/trunk/actuar/R/Extract.grouped.data.R

Doc:
https://vgoulet.act.ulaval.ca/svn/R/actuar/trunk/actuar/man/grouped.data.Rd
https://vgoulet.act.ulaval.ca/svn/R/actuar/trunk/actuar/man/Extract.grouped.data.Rd

Package:
http://vgoulet.act.ulaval.ca/actuar/

HTH

-- 
  Vincent Goulet, Associate Professor
  ??cole d'actuariat
  Universit?? Laval, Qu??bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca


From M.J.Bojanowski at uu.nl  Tue Mar 13 14:40:19 2007
From: M.J.Bojanowski at uu.nl (Bojanowski, M.J.  (Michal))
Date: Tue, 13 Mar 2007 14:40:19 +0100
Subject: [R] 'substitute' question
In-Reply-To: <BAY123-F44A95E77F66D702E29DEBD87C0@phx.gbl>
References: <BAY123-F44A95E77F66D702E29DEBD87C0@phx.gbl>
Message-ID: <94E133D09AA24D43BF6341B675C01A3311350C@uu01msg-exb01.soliscom.uu.nl>

I do not understand why you play with 'substitute' instead of something
like this:

# CAUTION: this will work only for bivariate case

# plotmaking function
plotModel <- function(m)
{
	x <- m$model$x
	y <- m$model$y
	r2 <- summary(m)$r.squared
	plot(x,y)
	abline(m)
	text( min(x), max(y), paste("And the R^2 is", round(r2,3)),
pos=4)
	invisible(NULL)
}

# your data
x <- rnorm(100)
y <- x + rnorm(100)
lm1 <- lm(y~x)
# make the plot
plotModel(lm1)



*** Note that my e-mail address has changed to m.j.bojanowski at uu.nl
*** Please update your address books accordingly. Thank you!

_________________________________________
Michal Bojanowski
ICS / Sociology
Utrecht University
Heidelberglaan 2; 3584 CS Utrecht
Room 1428
m.j.bojanowski at uu.nl
http://www.fss.uu.nl/soc/bojanowski
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of remko duursma
Sent: Tuesday, March 13, 2007 2:02 PM
To: r-help at stat.math.ethz.ch
Subject: [R] 'substitute' question


# I use this code to label a graph with the R2:

# graph
x <- rnorm(100)
y <- x + rnorm(100)
lm1 <- lm(y~x)
plot(x,y)

# label
R2text <- substitute(paste(R^2," = ",r2),list(r2=r2)) text(1,-3,R2text,
col="red")

# i have modified this a bit, so that i have a vector with other labels,
each of which # will be labelled on the graph. Example:
texts <- c("And the R2 is", R2text)
x <- c(-2,-2)
y <- c(2,1)
for(i in 1:length(texts))text(x[i],y[i],texts[i],pos=4, col="blue")

# As you can see the label "R2 = 48.7" remains at "paste(R^2, " = ",
48.7)"
# What to do?



Thanks for your help,

Remko



..-~-.-~-.-~-.-~-.-~-.-~-.-~-.-~-.-~-
Remko Duursma
Post-doctoral researcher
Dept. Forest Ecology
University of Helsinki, Finland

_________________________________________________________________
With tax season right around the corner, make sure to follow these few
simple tips.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From lists at eva.mpg.de  Tue Mar 13 15:07:26 2007
From: lists at eva.mpg.de (Cristina Gomes)
Date: Tue, 13 Mar 2007 15:07:26 +0100
Subject: [R] GLMM plots
Message-ID: <45F6B01E.20105@eva.mpg.de>

Hi R-users,
I would like to plot the effects of one of the predictor variables on 
the response variable in the GLMM I ran with the lme4 package. Usually 
when doing a multivariate analysis I would obtain the residuals of the 
model without the predictor variable of interest (x1) and then plot 
these residuals against X1. But in the lme4 package one can not obtain 
residuals. Is there any way of obtaining plots of this sort? or any 
other recommendation somebody could give me regarding this topic?
Thanks a lot in advance.
Cheers,
Cristina.


From brown_emu at yahoo.com  Tue Mar 13 15:07:53 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Tue, 13 Mar 2007 07:07:53 -0700 (PDT)
Subject: [R] turn regression coefficients into matrix or...
In-Reply-To: <op.to4ehwx736de38@nn-notebook>
Message-ID: <738560.8767.qm@web39704.mail.mud.yahoo.com>

I believe the short answer to your question lies in 

     summary(ans.reg[[i]])$coefficients

which will give you a matrix with coefficients and standard errors (and
more). You can also find out what other information you can get from the
regressions if you type

     attributes(ans.reg[[i]])
     attributes(summary(ans.reg[[i]]))

and then see the names of the list elements (the regression functions and
summary() return objects of mode "list") that might correspond to the
information you are looking for.

Good luck,

ST




--- Norbert NEUWIRTH <norbertneuwirth at gmx.at> wrote:

> jun,
> 
> im am also quite new to R. so i think, this is a question all we R-newbees 
> 
> ask  ;-). having had the same "problem" the other day, i "solved" it the  
> following way:
> 
> ####  Multiple Regressions - Tables
> 
> data(anscombe)                                      # load anscombe  
> dataset (implemented somwhere in R)
> x5 <- rnorm(11,14,1)                                # create an additional 
> 
> variable (at random)
> anscombe.nn <- cbind(anscombe,x5)                   # attach the variable  
> to the dataset
> attach(anscombe.nn)                                 # attach the dataset  
> to the searchpath (just for convenience)
> anscombe.nn                                         # have a look on the  
> data
> 
> ans.reg <- vector(4, mode = "list")                 # create empty list  
> (just  for speeding up)
> 
> for(i in 1:4){                                      # now the 4  
> regressions are stored to the list
>      j <- i+1
>      x1 <- get(paste("x", i, sep = ""))              # exogenous var. #1
>      x2 <- get(paste("x", j, sep = ""))              # exogenous var. #2
>      y <- get(paste("y", i, sep = ""))               # endogenous
>      ans.reg[[i]] <- glm(y ~ x1+x2,family=gaussian)  # do the regression  
> (out of 4)
>      print(summary(ans.reg[[i]],cor=FALSE))
> }
> 
> detach(anscombe)                                    # detach dataset from  
> search path
> lapply(ans.reg, coef)                               # see each regression  
> in one line
> sapply(ans.reg, coef)                               # have the  
> coefficients in a table
> x<-as.matrix(sapply(ans.reg, coef))                 # convert table to  
> matrix
> x
> 
> this solution is quite comparable to Ligges(2007) [published in german and 
> 
> japanese, i think]
> 
> i have an additional question to list-members, that have left the  
> newbee-status yet: how can i get R to hand over the standard errors,  
> significance levels etc, so that i can create a table with  coeff and SE  
> and sig.?
> 
> norbert
> 
> 
> Am 13.03.2007, 02:58 Uhr, schrieb Jun Xu <mystata at hotmail.com>:
> 
> > I don't have much experience with r. What I am trying to do is to turn
> > regression coefficients (after I run a lm or glm model) into some matrix
> > such that I can do some post-estimation calculation, for example  
> > predicted
> > probabilities in glm model, etc.. Or, is there any function in r that I  
> > can
> > use to do something along that line? thanks.
> >
> > Jun Xu, Ph.D.
> > Department of Sociology
> > Ball State University
> >
> > _________________________________________________________________
> > Mortgage rates as low as 4.625% - Refinance $150,000 loan for $579 a  
> > month.
> > Intro*Terms
> >
>
https://www2.nextag.com/goto.jsp?product=100000035&url=%2fst.jsp&tm=y&search=mortgage_text_links_88_h27f6&disc=y&vers=743&s=4056&p=5117
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide  
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 
> 
> -- 
> -------------------------------
> Mag. Norbert NEUWIRTH
> 
> Roubiczekgasse 2/23
> A-1100  WIEN
> mob: +43 699 1835 0704
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



 
____________________________________________________________________________________
Food fight? Enjoy some healthy debate


From lawremi at iastate.edu  Tue Mar 13 15:14:33 2007
From: lawremi at iastate.edu (Michael Lawrence)
Date: Tue, 13 Mar 2007 09:14:33 -0500
Subject: [R] gtk button: how to create signal handler?
In-Reply-To: <f8e6ff050703130656k66cf1a21o7734965bb45098a7@mail.gmail.com>
References: <d4327f7e0703130341n3b70ab8fk3673ec0971171c59@mail.gmail.com>
	<f8e6ff050703130656k66cf1a21o7734965bb45098a7@mail.gmail.com>
Message-ID: <509e0620703130714j5e6a0fb5vd4ada08da7c4f2c6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070313/d8979b69/attachment.pl 

From mikejjasper at gmail.com  Tue Mar 13 15:38:57 2007
From: mikejjasper at gmail.com (Mike Jasper)
Date: Tue, 13 Mar 2007 10:38:57 -0400
Subject: [R] selecting rows with more than x occurrences in a given column
	(data type is names)
Message-ID: <9b3e85660703130738k5f8bf718uc16a530eddf1bf86@mail.gmail.com>

Despite a long search on the archives, I couldn't find how to do this.
Thanks in advance for what is likely a simple issue.

I have a data set where the first column is name (i.e., 'Joe Smith',
'Jane Doe', etc). The following columns are data associated with that
person. I have many people with multiple rows. What I want is to get a
new data frame out with only the people who have more than x
occurrences in the first column.

Here's what I've done, that's not working:

Let's call my old data.frame "all.data"

table(all.data$names)>10

I get a list of names and TRUE/FALSE values. I then want to make a
list of the TRUEs and pass that to some subset type command like

dup.names=table(all.data$names)>10

new.data=(all.data[all.data$names==dup.names,])

That's not working because the dimensions are wrong (I think). But
even when I tried to do part of it manually (to troubleshoot) like
this

dup.names=c('Joe Smith','Jane Doe','etc')

I got warnings and it didn't work correctly. There must be a simple
way to do this that I'm just not seeing. Thanks.


From avanisco at univ-fcomte.fr  Tue Mar 13 14:49:51 2007
From: avanisco at univ-fcomte.fr (vaniscotte)
Date: Tue, 13 Mar 2007 14:49:51 +0100
Subject: [R] hierarchical partitioning
Message-ID: <45F6ABFF.3040301@univ-fcomte.fr>

Dear all,

I am trying to model variation of distribution of species assemblages 
according to environmental variables. For that I use a log linear 
multinomial regression.
In order to select variables that mostly discriminate the assemblages, I 
tried to apply a hierarchical partitioning protocol to my data set.

For that I have adapted the all.regs() for multinomial model.
The problem is that I couldn't adapt partition() as I don't manage to 
access the C code ("hierpart") which is called in this function. This is 
a black box for me.

When I run partition() with the Log Likelihood values obtained for the 
hierarchy of multinomial model, I obtain negative values for Independenr 
contribution.
This suggest that there is a bug somewhere in my protocole.
I suppose that I missused partition(). Maybe my log lokelihood 
(multinomial one) is not appropriate.

As someone had ever met this problem and could help me?

Many thanks

Am?lie


From montcroix at hotmail.fr  Tue Mar 13 15:35:53 2007
From: montcroix at hotmail.fr (Laurent Duvernet)
Date: Tue, 13 Mar 2007 15:35:53 +0100
Subject: [R]  estimating an ARIMA model with constraints
Message-ID: <BAY119-F40A621FFA72A32527AE33CCD7C0@phx.gbl>

Hi,

I am trying to estimate an ARIMA model in the case where I have some 
specific knowledge about the coefficients that should be included in the 
model. Take a classical ARIMA (or even ARMA) model:

P(B) X(t) = Q(B) epsilon(t),

where X(t) is the data, epsilon is a white noise, B is the backward operator 
and P and Q are some polynoms. Additionally, assume that you know in advance 
how P and Q look like. Typically, P could be something like this:

P(x) = (1 - a(1)*x - a(2)*x^2) * (1 - b(1)*x^23 - b(2)*x^24) * (1 - 
c(1)*x^168)

(That is in the case of hourly data, with lags 23 and 24 corresponding to 
the day, and lag 168 for the week.) How do you estimate this kind of model 
with R? The arima() and arima0() functions in the stats package do not allow 
this kind of constraints on the polynoms. I've searched in the packages 
dedicated to time series analysis, but I have not found a solution. Has 
anyone an idea?

Thanks in advance!

Laurent Duvernet
EDF R&D


From M.J.Bojanowski at uu.nl  Tue Mar 13 15:50:07 2007
From: M.J.Bojanowski at uu.nl (Bojanowski, M.J.  (Michal))
Date: Tue, 13 Mar 2007 15:50:07 +0100
Subject: [R] 'substitute' question
In-Reply-To: <BAY123-F44A95E77F66D702E29DEBD87C0@phx.gbl>
References: <BAY123-F44A95E77F66D702E29DEBD87C0@phx.gbl>
Message-ID: <94E133D09AA24D43BF6341B675C01A3311350D@uu01msg-exb01.soliscom.uu.nl>

I'm sorry for my first post as I did not properly understood your
problem.
The reason why you were getting "paste(R^2, " = ", 48.7)" instead of the
properly formatted R^2=48.7
is that in creating 'texts' in your code you were mixing character
string with expressions resulting in 'texts' being a list not a vector.
The only modification to make is to index texts with double square
brackets as below

# the figure
x <- rnorm(100)
y <- x + rnorm(100)
lm1 <- lm(y~x)
plot(x,y)
# label
R2text <- substitute(paste(R^2," = ",r2),list(r2=r2)) text(1,-3,R2text,
col="red")

# i expanded your list of labels to four elements...
texts <- c("And the R2 is", R2text, "third label", "fourth label")
# texts is a list not an atomic vector
class(texts)
str(texts)

# ... and added more coordinates accordingly
x <- c(-2,-2, -1, -1)
y <- c(2,1, 2, 1)
# here 'texts[[i]]' instead of 'texts[i]'
for(i in 1:length(texts))text(x[i],y[i], texts[[i]],pos=4, col="blue")

hth,
Michal


*** Note that my e-mail address has changed to m.j.bojanowski at uu.nl
*** Please update your address books accordingly. Thank you!

_________________________________________
Michal Bojanowski
ICS / Sociology
Utrecht University
Heidelberglaan 2; 3584 CS Utrecht
Room 1428
m.j.bojanowski at uu.nl
http://www.fss.uu.nl/soc/bojanowski
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of remko duursma
Sent: Tuesday, March 13, 2007 2:02 PM
To: r-help at stat.math.ethz.ch
Subject: [R] 'substitute' question


# I use this code to label a graph with the R2:

# graph
x <- rnorm(100)
y <- x + rnorm(100)
lm1 <- lm(y~x)
plot(x,y)

# label
R2text <- substitute(paste(R^2," = ",r2),list(r2=r2)) text(1,-3,R2text,
col="red")

# i have modified this a bit, so that i have a vector with other labels,
each of which # will be labelled on the graph. Example:
texts <- c("And the R2 is", R2text)
x <- c(-2,-2)
y <- c(2,1)
for(i in 1:length(texts))text(x[i],y[i],texts[i],pos=4, col="blue")

# As you can see the label "R2 = 48.7" remains at "paste(R^2, " = ",
48.7)"
# What to do?



Thanks for your help,

Remko



..-~-.-~-.-~-.-~-.-~-.-~-.-~-.-~-.-~-
Remko Duursma
Post-doctoral researcher
Dept. Forest Ecology
University of Helsinki, Finland

_________________________________________________________________
With tax season right around the corner, make sure to follow these few
simple tips.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From P.Dalgaard at biostat.ku.dk  Tue Mar 13 15:51:44 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 13 Mar 2007 15:51:44 +0100
Subject: [R] Solving PDEs
In-Reply-To: <45F68FCF.4060701@noaa.gov>
References: <9fb977ed0703122221ja5b49e9o4b3b0bc94064ef86@mail.gmail.com>
	<45F68FCF.4060701@noaa.gov>
Message-ID: <45F6BA80.5050202@biostat.ku.dk>

Thomas Adams wrote:
> Amit,
>
> A better tool for this purpose may be octave, which can be found at 
> http://www.gnu.org/software/octave/
>   
Parabolic PDEs handled by the method of lines are effectively systems of
ODEs and can be solved by (tada!) ODEsolve.

     -p
> Regards,
> Tom
>
> Amit Soni wrote:
>   
>> Hi,
>>
>> Is there any method in R by which I can solve PDEs?
>>
>> Thank you,
>> Amit
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>   
>>     
>
>
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From researchjj at gmail.com  Tue Mar 13 15:53:27 2007
From: researchjj at gmail.com (j.joshua thomas)
Date: Tue, 13 Mar 2007 22:53:27 +0800
Subject: [R] Rattle() GGobi()-Plots- How to save?
In-Reply-To: <22038.212.209.13.15.1173789582.squirrel@www.sorch.se>
References: <b4485c4c0703130140n315e5e77p3f0a76f25ac9d951@mail.gmail.com>
	<22038.212.209.13.15.1173789582.squirrel@www.sorch.se>
Message-ID: <b4485c4c0703130753t6c2e40d4h65f5358ca8b136ea@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070313/3e55eec0/attachment.pl 

From Mark.Leeds at morganstanley.com  Tue Mar 13 15:56:47 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Tue, 13 Mar 2007 10:56:47 -0400
Subject: [R] estimating an ARIMA model with constraints
In-Reply-To: <BAY119-F40A621FFA72A32527AE33CCD7C0@phx.gbl>
References: <BAY119-F40A621FFA72A32527AE33CCD7C0@phx.gbl>
Message-ID: <D3AEEDA31E57474B840BEBC25A8A83440174C625@NYWEXMB23.msad.ms.com>


are the carats in your notation meant to be time subscripts ?

also, I think I know what a and b are meant to be ( the coefficients of
the polynomaisl corresponding
To the ar part of the model but correct me if I'm wrong ) but is there
an ma piece to it also ?
And I don't see an error term ?


I think you need to be clearer on your notation and write out the full
model in terms of X(t) = whatever because then more people will reply.


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Laurent Duvernet
Sent: Tuesday, March 13, 2007 10:36 AM
To: r-help at stat.math.ethz.ch
Subject: [R] estimating an ARIMA model with constraints

Hi,

I am trying to estimate an ARIMA model in the case where I have some
specific knowledge about the coefficients that should be included in the
model. Take a classical ARIMA (or even ARMA) model:

P(B) X(t) = Q(B) epsilon(t),

where X(t) is the data, epsilon is a white noise, B is the backward
operator and P and Q are some polynoms. Additionally, assume that you
know in advance how P and Q look like. Typically, P could be something
like this:

P(x) = (1 - a(1)*x - a(2)*x^2) * (1 - b(1)*x^23 - b(2)*x^24) * (1 -
c(1)*x^168)

(That is in the case of hourly data, with lags 23 and 24 corresponding
to the day, and lag 168 for the week.) How do you estimate this kind of
model with R? The arima() and arima0() functions in the stats package do
not allow this kind of constraints on the polynoms. I've searched in the
packages dedicated to time series analysis, but I have not found a
solution. Has anyone an idea?

Thanks in advance!

Laurent Duvernet
EDF R&D

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From P.Dalgaard at biostat.ku.dk  Tue Mar 13 15:58:28 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 13 Mar 2007 15:58:28 +0100
Subject: [R] Freeman-Tukey arcsine transformation
In-Reply-To: <6021CA6EF4C8374084D4F5A141F1CBBB664BF2@msgebe23.mfad.mfroot.org>
References: <6021CA6EF4C8374084D4F5A141F1CBBB664BF2@msgebe23.mfad.mfroot.org>
Message-ID: <45F6BC14.2020300@biostat.ku.dk>

Inman, Brant A. M.D. wrote:
> R-Experts:
>
> Does anyone know if there are R functions to perform the Freeman-Tukey
> double arcsine transformation and then backtransform it?
>
>   

Well, if not, both are given by explicit formulas, so it shouldn't take
long to implement, cf.:

    The Teacher's Corner


        *The Inverse of the Freeman-Tukey Double Arcsine Transformation
        <http://www.jstor.org/view/00031305/di020556/02p0095b/0?frame=noframe&dpi=3&userID=c0261211 at all.ku.dk/01cce4405c00501bb027f&backcontext=page&backurl=/cgi-bin/jstor/viewitem/00031305/di020556/02p0095b/0%3fframe%3dnoframe%26dpi%3d3%26userID%3dc0261211 at all.ku.dk/01cce4405c00501bb027f%26config%3d%26PAGE%3d0&config=jstor&PAGE=0>*



            John J. Miller 


            /The American Statistician/, Vol. 32, No. 4. (Nov., 1978),
            p. 138. 


            Stable URL:
            http://links.jstor.org/sici?sici=0003-1305%28197811%2932%3A4%3C138%3ATIOTFD%3E2.0.CO%3B2-Z



              Abstract

        A formula for the inverse of the Freeman-Tukey double arcsine
        transformation is derived. This formula is useful when
        expressing means of double arcsines as retransformed
        proportions. When the mean is taken from original proportions
        involving different n's, it is suggested that the harmonic mean
        of the n's be used in the inversion formula.



> Thanks,
>
> Brant Inman
> Mayo Clinic
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From brown_emu at yahoo.com  Tue Mar 13 15:59:02 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Tue, 13 Mar 2007 07:59:02 -0700 (PDT)
Subject: [R] selecting rows with more than x occurrences in a given
	column (data type is names)
In-Reply-To: <9b3e85660703130738k5f8bf718uc16a530eddf1bf86@mail.gmail.com>
Message-ID: <41405.51233.qm@web39706.mail.mud.yahoo.com>

This isn't pretty, but should work:

x <- 10 # number of occurrences
y <- split(all.data,f=all.data$names)
z <- y[unlist(lapply(y,nrow))>x]
newdata <- vector()
for( k in z ) {
  newdata <- rbind(newdata,k)
}

Basically I split your data frame into groups by name (into a list), then
selected elements in the list for which the number of rows (number of
occurrences) was > x, then concatenated rows from the selected elements to an
initially empty vector. Probably there is a more elegant way to do this but I
can't think of it at the moment...

You are correct in that the conditional statement using '==' cannot test
vectors of mismatched dimensions.





--- Mike Jasper <mikejjasper at gmail.com> wrote:

> Despite a long search on the archives, I couldn't find how to do this.
> Thanks in advance for what is likely a simple issue.
> 
> I have a data set where the first column is name (i.e., 'Joe Smith',
> 'Jane Doe', etc). The following columns are data associated with that
> person. I have many people with multiple rows. What I want is to get a
> new data frame out with only the people who have more than x
> occurrences in the first column.
> 
> Here's what I've done, that's not working:
> 
> Let's call my old data.frame "all.data"
> 
> table(all.data$names)>10
> 
> I get a list of names and TRUE/FALSE values. I then want to make a
> list of the TRUEs and pass that to some subset type command like
> 
> dup.names=table(all.data$names)>10
> 
> new.data=(all.data[all.data$names==dup.names,])
> 
> That's not working because the dimensions are wrong (I think). But
> even when I tried to do part of it manually (to troubleshoot) like
> this
> 
> dup.names=c('Joe Smith','Jane Doe','etc')
> 
> I got warnings and it didn't work correctly. There must be a simple
> way to do this that I'm just not seeing. Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



 
____________________________________________________________________________________
Finding fabulous fares is fun.


From bill.shipley at usherbrooke.ca  Tue Mar 13 16:01:37 2007
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Tue, 13 Mar 2007 11:01:37 -0400
Subject: [R] Duncan post-hoc tests in R?
Message-ID: <001c01c76580$80dbdba0$b71ad284@BIO041>

Hello,
I am looking for an R function that impliments Duncan's post-hoc test.  I am
aware of multcomp and its function "glht" but, unless I am missing
something, this cannot impliment the Duncan test.
Any help of pointers are welcome.

Bill Shipley


From dimitris.rizopoulos at med.kuleuven.be  Tue Mar 13 16:02:41 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 13 Mar 2007 16:02:41 +0100
Subject: [R] selecting rows with more than x occurrences in a given
	column(data type is names)
References: <9b3e85660703130738k5f8bf718uc16a530eddf1bf86@mail.gmail.com>
Message-ID: <00a601c76580$a5925b40$0540210a@www.domain>

try this:

set.seed(123)
all.data <- data.frame(name = sample(c("Joe", "Elen", "Jane", "Mike"), 
8, TRUE),
    x = rnorm(8), y = runif(8))
##########
tab.nams <- table(all.data$name)
nams <- names(tab.nams[tab.nams >= 2])
all.data[all.data$name %in% nams, ]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Mike Jasper" <mikejjasper at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, March 13, 2007 3:38 PM
Subject: [R] selecting rows with more than x occurrences in a given 
column(data type is names)


> Despite a long search on the archives, I couldn't find how to do 
> this.
> Thanks in advance for what is likely a simple issue.
>
> I have a data set where the first column is name (i.e., 'Joe Smith',
> 'Jane Doe', etc). The following columns are data associated with 
> that
> person. I have many people with multiple rows. What I want is to get 
> a
> new data frame out with only the people who have more than x
> occurrences in the first column.
>
> Here's what I've done, that's not working:
>
> Let's call my old data.frame "all.data"
>
> table(all.data$names)>10
>
> I get a list of names and TRUE/FALSE values. I then want to make a
> list of the TRUEs and pass that to some subset type command like
>
> dup.names=table(all.data$names)>10
>
> new.data=(all.data[all.data$names==dup.names,])
>
> That's not working because the dimensions are wrong (I think). But
> even when I tried to do part of it manually (to troubleshoot) like
> this
>
> dup.names=c('Joe Smith','Jane Doe','etc')
>
> I got warnings and it didn't work correctly. There must be a simple
> way to do this that I'm just not seeing. Thanks.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ccleland at optonline.net  Tue Mar 13 16:10:41 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 13 Mar 2007 11:10:41 -0400
Subject: [R] selecting rows with more than x occurrences in a given
 column (data type is names)
In-Reply-To: <9b3e85660703130738k5f8bf718uc16a530eddf1bf86@mail.gmail.com>
References: <9b3e85660703130738k5f8bf718uc16a530eddf1bf86@mail.gmail.com>
Message-ID: <45F6BEF1.1020603@optonline.net>

Mike Jasper wrote:
> Despite a long search on the archives, I couldn't find how to do this.
> Thanks in advance for what is likely a simple issue.
> 
> I have a data set where the first column is name (i.e., 'Joe Smith',
> 'Jane Doe', etc). The following columns are data associated with that
> person. I have many people with multiple rows. What I want is to get a
> new data frame out with only the people who have more than x
> occurrences in the first column.
> 
> Here's what I've done, that's not working:
> 
> Let's call my old data.frame "all.data"
> 
> table(all.data$names)>10
> 
> I get a list of names and TRUE/FALSE values. I then want to make a
> list of the TRUEs and pass that to some subset type command like
> 
> dup.names=table(all.data$names)>10
> 
> new.data=(all.data[all.data$names==dup.names,])
> 
> That's not working because the dimensions are wrong (I think). But
> even when I tried to do part of it manually (to troubleshoot) like
> this
> 
> dup.names=c('Joe Smith','Jane Doe','etc')
> 
> I got warnings and it didn't work correctly. There must be a simple
> way to do this that I'm just not seeing. Thanks.

  Does this help?

df <- data.frame(PERSON = rep(c("John","Tom","Sara","Mary"),
                              c(5,4,5,4)),
                 Y = runif(18))

subset(df, PERSON %in% names(which(table(PERSON) >= 5)))

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From montcroix at hotmail.fr  Tue Mar 13 16:17:15 2007
From: montcroix at hotmail.fr (Laurent Duvernet)
Date: Tue, 13 Mar 2007 16:17:15 +0100
Subject: [R] estimating an ARIMA model with constraints
In-Reply-To: <D3AEEDA31E57474B840BEBC25A8A83440174C625@NYWEXMB23.msad.ms.com>
Message-ID: <BAY119-F26150763AFBC4E6B0A12B7CD7C0@phx.gbl>

Sorry if the notation is unclear. You got it right:

P(x)  = (1 - a_1*x - a_2*x^2) * (1 - b_1*x^23 - b_2*x^24) * (1 - c_1*x^168).

The a_i's, b_i's and c_i's are the coefs of the polynom P.

And there is also an MA part, which is "Q(B) epsilon(t)". Here epsilon(t) is 
the error process, and Q is another polynom of the same type as P (it could 
be different, that does not change the problem).

Q(x)  = (1 - alpha_1*x - alpha_2*x^2) * (1 - beta_1*x^23 - beta_2*x^24) * (1 
- gamma_1*x^168).

I can write "X(t) = ...", but I'm not sure it would be a lot clearer...

X(t) =  a_1*X(t-1) + a_2*X(t-2) + b_1*X(t-23) + (b_2 + a_1*b_1)*X(t-24) + 
(a_1*b_2 + a_2*b_1)*X(t-25) + a2*b2 X(t-26) + .... the terms around X(t-168) 
+ ... the MA part.

I hope everything is clear now.



>From: "Leeds, Mark (IED)" <Mark.Leeds at morganstanley.com>
>To: "Laurent Duvernet" <montcroix at hotmail.fr>, <r-help at stat.math.ethz.ch>
>Subject: RE: [R]  estimating an ARIMA model with constraints
>Date: Tue, 13 Mar 2007 10:56:47 -0400
>
>
>are the carats in your notation meant to be time subscripts ?
>
>also, I think I know what a and b are meant to be ( the coefficients of
>the polynomaisl corresponding
>To the ar part of the model but correct me if I'm wrong ) but is there
>an ma piece to it also ?
>And I don't see an error term ?
>
>
>I think you need to be clearer on your notation and write out the full
>model in terms of X(t) = whatever because then more people will reply.
>
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Laurent Duvernet
>Sent: Tuesday, March 13, 2007 10:36 AM
>To: r-help at stat.math.ethz.ch
>Subject: [R] estimating an ARIMA model with constraints
>
>Hi,
>
>I am trying to estimate an ARIMA model in the case where I have some
>specific knowledge about the coefficients that should be included in the
>model. Take a classical ARIMA (or even ARMA) model:
>
>P(B) X(t) = Q(B) epsilon(t),
>
>where X(t) is the data, epsilon is a white noise, B is the backward
>operator and P and Q are some polynoms. Additionally, assume that you
>know in advance how P and Q look like. Typically, P could be something
>like this:
>
>P(x) = (1 - a(1)*x - a(2)*x^2) * (1 - b(1)*x^23 - b(2)*x^24) * (1 -
>c(1)*x^168)
>
>(That is in the case of hourly data, with lags 23 and 24 corresponding
>to the day, and lag 168 for the week.) How do you estimate this kind of
>model with R? The arima() and arima0() functions in the stats package do
>not allow this kind of constraints on the polynoms. I've searched in the
>packages dedicated to time series analysis, but I have not found a
>solution. Has anyone an idea?
>
>Thanks in advance!
>
>Laurent Duvernet
>EDF R&D
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

_________________________________________________________________

mobile comme sur PC ! http://mobile.live.fr/messenger/bouygues/


From marc_schwartz at comcast.net  Tue Mar 13 16:32:22 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 13 Mar 2007 10:32:22 -0500
Subject: [R] selecting rows with more than x occurrences in a
	given	column (data type is names)
In-Reply-To: <9b3e85660703130738k5f8bf718uc16a530eddf1bf86@mail.gmail.com>
References: <9b3e85660703130738k5f8bf718uc16a530eddf1bf86@mail.gmail.com>
Message-ID: <1173799942.4315.12.camel@localhost.localdomain>

On Tue, 2007-03-13 at 10:38 -0400, Mike Jasper wrote:
> Despite a long search on the archives, I couldn't find how to do this.
> Thanks in advance for what is likely a simple issue.
> 
> I have a data set where the first column is name (i.e., 'Joe Smith',
> 'Jane Doe', etc). The following columns are data associated with that
> person. I have many people with multiple rows. What I want is to get a
> new data frame out with only the people who have more than x
> occurrences in the first column.
> 
> Here's what I've done, that's not working:
> 
> Let's call my old data.frame "all.data"
> 
> table(all.data$names)>10
> 
> I get a list of names and TRUE/FALSE values. I then want to make a
> list of the TRUEs and pass that to some subset type command like
> 
> dup.names=table(all.data$names)>10
> 
> new.data=(all.data[all.data$names==dup.names,])
> 
> That's not working because the dimensions are wrong (I think). But
> even when I tried to do part of it manually (to troubleshoot) like
> this
> 
> dup.names=c('Joe Smith','Jane Doe','etc')
> 
> I got warnings and it didn't work correctly. There must be a simple
> way to do this that I'm just not seeing. Thanks.


Something like this should work:

  NewDF <- subset(all.data, names %in% unique(names[duplicated(names)]))

See ?duplicated, ?unique and ?"%in%" for more information.

HTH,

Marc Schwartz


From marc_schwartz at comcast.net  Tue Mar 13 16:38:31 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 13 Mar 2007 10:38:31 -0500
Subject: [R] selecting rows with more than x occurrences in
	a	given	column (data type is names)
In-Reply-To: <1173799942.4315.12.camel@localhost.localdomain>
References: <9b3e85660703130738k5f8bf718uc16a530eddf1bf86@mail.gmail.com>
	<1173799942.4315.12.camel@localhost.localdomain>
Message-ID: <1173800311.4315.16.camel@localhost.localdomain>

On Tue, 2007-03-13 at 10:32 -0500, Marc Schwartz wrote:
> On Tue, 2007-03-13 at 10:38 -0400, Mike Jasper wrote:
> > Despite a long search on the archives, I couldn't find how to do this.
> > Thanks in advance for what is likely a simple issue.
> > 
> > I have a data set where the first column is name (i.e., 'Joe Smith',
> > 'Jane Doe', etc). The following columns are data associated with that
> > person. I have many people with multiple rows. What I want is to get a
> > new data frame out with only the people who have more than x
> > occurrences in the first column.
> > 
> > Here's what I've done, that's not working:
> > 
> > Let's call my old data.frame "all.data"
> > 
> > table(all.data$names)>10
> > 
> > I get a list of names and TRUE/FALSE values. I then want to make a
> > list of the TRUEs and pass that to some subset type command like
> > 
> > dup.names=table(all.data$names)>10
> > 
> > new.data=(all.data[all.data$names==dup.names,])
> > 
> > That's not working because the dimensions are wrong (I think). But
> > even when I tried to do part of it manually (to troubleshoot) like
> > this
> > 
> > dup.names=c('Joe Smith','Jane Doe','etc')
> > 
> > I got warnings and it didn't work correctly. There must be a simple
> > way to do this that I'm just not seeing. Thanks.
> 
> 
> Something like this should work:
> 
>   NewDF <- subset(all.data, names %in% unique(names[duplicated(names)]))
> 
> See ?duplicated, ?unique and ?"%in%" for more information.
> 
> HTH,
> 
> Marc Schwartz

Ack...sorry about that.  I misread the query as for any duplicated
occurrences. The solution provided by Dimitris is correct.

Marc


From jh69 at york.ac.uk  Tue Mar 13 16:43:26 2007
From: jh69 at york.ac.uk (Jenny Hodgson)
Date: Tue, 13 Mar 2007 15:43:26 +0000
Subject: [R] inconsistent behaviour of add1 and drop1 with a weighted linear
	model
Message-ID: <45F6C69E.10200@york.ac.uk>

Dear R Help,
I have noticed some inconsistent behaviour of add1 and drop1 with a 
weighted linear model, which affects the interpretation of the results.
I have these data to fit with a linear model, I want to weight them by 
the relative size of the geographical areas they represent.
_________________________________________________________________________________________
 > example
           y        x1       x2   weights
1  -4.546477 0.1859556 50.00000 0.9466022
2   1.484246 0.4740497 29.88000 1.3252430
3   2.203681 0.8594264 16.93333 0.9466022
4   1.943163 0.8713360 42.11765 2.7766997
5   1.886473 0.9006082 19.00000 0.9466022
6   1.795393 0.8455183 23.68421 1.1674760
7   1.878077 0.5641396 35.00000 0.8203885
8  -4.215484 0.4356858 58.75000 0.4417477
9   1.993339 0.5440061 19.28571 0.8519420
10  1.560869 0.6285066 19.54545 0.8203885
11  2.761535 0.7017720 15.83333 0.1262136
12  0.995959 0.4638751  0.00000 0.9466022
13 -4.516514 0.2794199 77.85714 0.8834954
 > sum(example$weights)
[1] 13.00000

 > model<-lm(y~1,data=example,weights=weights)
 > add1(model,.~.+x1+x2)
Single term additions

Model:
y ~ 1
       Df Sum of Sq    RSS    AIC
<none>              94.000 27.719
x1      1    55.290 38.710 18.185
x2      1    58.630 35.371 17.012
 > addterm(model,.~.+x1+x2)
Single term additions

Model:
y ~ 1
       Df Sum of Sq    RSS    AIC
<none>              94.000 27.719
x1      1    55.290 38.710 18.185
x2      1    58.630 35.371 17.012

#so add1 and addterm (MASS package) give the same answer, nothing 
obviously untoward, but
#both SS and RSS change when you do...
 > model<-lm(y~x1,data=example,weights=weights)
 > drop1(model)
Single term deletions

Model:
y ~ x1
       Df Sum of Sq    RSS    AIC
<none>              30.164 14.942############I would expect RSS to be 38.710
x1      1    44.377 74.541 24.703############I would expect SS to be 
55.290 and RSS 94.000 #{3}#
 > model<-lm(y~x2,data=example,weights=weights)
 > drop1(model)
Single term deletions

Model:
y ~ x2
       Df Sum of Sq    RSS    AIC
<none>              37.922 17.918
x2      1    36.619 74.541 24.703 #{1}#

#not only are SS and RSS different, the relative importance of x1 and x2 
has swapped! I have checked that this
#does not happen if weights are not used (everything adds up as 
expected, but the null RSS and other SSs are not the same as any quoted 
here).
#The inconsistency is still there if you are not adding to a null model:
#NB I realise that x1 and x2 are correlated so whichever is last to be 
added appears to have a lower SS - this is not the issue
 > add1(model,.~.+x1+x2)#This or...
Single term additions

Model:
y ~ x2
       Df Sum of Sq    RSS    AIC
<none>              35.371 17.012
x1      1    18.576 16.794  9.329

 > addterm(model,.~x1+x2)# ...this is inconsistent with:
Single term additions

Model:
y ~ x2
       Df Sum of Sq    RSS    AIC
<none>              35.371 17.012
x1      1    18.576 16.794  9.329

 > drop1(update(model,.~.+x1))#this or...
Single term deletions

Model:
y ~ x2 + x1
       Df Sum of Sq    RSS    AIC
<none>              14.456  7.380
x2      1    15.708 30.164 14.942 #{4}#
x1      1    23.466 37.922 17.918 #{2}#

 > dropterm(update(model,.~.+x1))#...this
Single term deletions

Model:
y ~ x2 + x1
       Df Sum of Sq    RSS    AIC
<none>              14.456  7.380
x2      1    15.708 30.164 14.942
x1      1    23.466 37.922 17.918

#and the same thing happens with the x2 variable - compare below with above
 > add1(update(model,.~x1),.~x1+x2)
Single term additions

Model:
y ~ x1
       Df Sum of Sq    RSS    AIC
<none>              38.710 18.185
x2      1    21.916 16.794  9.329

 > addterm(update(model,.~x1),.~x1+x2)
Single term additions

Model:
y ~ x1
       Df Sum of Sq    RSS    AIC
<none>              38.710 18.185
x2      1    21.916 16.794  9.329

#Why do I think add1/addterm are the problem rather than drop1/dropterm?
#Because drop1/dropterm agree with the anova:

 > model<-lm(y~x2+x1,data=example,weights=weights)
 > anova(model)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(>F)   
x2         1 36.619  36.619  25.331 0.0005119 *** ####this agrees with 
#{1}# above
x1         1 23.466  23.466  16.233 0.0024035 ** ####this agrees with 
#{2}# above
Residuals 10 14.456   1.446                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
 > model<-lm(y~x1+x2,data=example,weights=weights)
 > anova(model)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(>F)   
x1         1 44.377  44.377  30.698 0.0002474 *** ####this agrees with 
#{3}# above
x2         1 15.708  15.708  10.866 0.0080633 ** ####this agrees with 
#{4}# above
Residuals 10 14.456   1.446                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1


___________________________________________________________________________________________

My question is, why does this inconsistency happen? And is it safe to 
assume that anova and drop1/dropterm are giving me the answers I want, 
therefore to explore my model with these functions and avoid using 
add1/addterm?

HUGE thanks for reading to the end! I would be extremely grateful if 
someone could help me with this problem. I wasn't able to find any clues 
in the docs or the r-help archives (but perhaps as it's a complex 
problem I wasn't using the right search terms, if so, apologies)

Best wishes
Jenny

_____________
Jenny Hodgson
Department of Biology - area 18
PO box 373
University of York
YO10 5YW
UK
Tel: 01904 328623


From kihwang.lee at gmail.com  Tue Mar 13 17:12:28 2007
From: kihwang.lee at gmail.com (Kihwang Lee)
Date: Wed, 14 Mar 2007 01:12:28 +0900
Subject: [R] Rcmdir does not work in SciVews-R
Message-ID: <45F6CD6C.40302@gmail.com>

Hi,

I have recently installed version 2.4.1 of R and also installed
SciViews-R 0.8.9. It worked fine. Then I installed the Rcmdr 1.2-7 from
CRAN . However the 'R commander menu' in the dockable panel does not
work. If I start Rcmdr in R without SciViews, it works perfectly.

Is there any way that I can use Rcmdr with SciViews?

Many thanks in advanve.

Kihwang


From p.dalgaard at biostat.ku.dk  Tue Mar 13 17:35:07 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 13 Mar 2007 17:35:07 +0100
Subject: [R] inconsistent behaviour of add1 and drop1 with a weighted
 linear	model
In-Reply-To: <45F6C69E.10200@york.ac.uk>
References: <45F6C69E.10200@york.ac.uk>
Message-ID: <45F6D2BB.6020903@biostat.ku.dk>

Jenny Hodgson wrote:
> Dear R Help,
> I have noticed some inconsistent behaviour of add1 and drop1 with a 
> weighted linear model, which affects the interpretation of the results.
> I have these data to fit with a linear model, I want to weight them by 
> the relative size of the geographical areas they represent.
> _________________________________________________________________________________________
>  > example
>            y        x1       x2   weights
> 1  -4.546477 0.1859556 50.00000 0.9466022
> 2   1.484246 0.4740497 29.88000 1.3252430
> 3   2.203681 0.8594264 16.93333 0.9466022
> 4   1.943163 0.8713360 42.11765 2.7766997
> 5   1.886473 0.9006082 19.00000 0.9466022
> 6   1.795393 0.8455183 23.68421 1.1674760
> 7   1.878077 0.5641396 35.00000 0.8203885
> 8  -4.215484 0.4356858 58.75000 0.4417477
> 9   1.993339 0.5440061 19.28571 0.8519420
> 10  1.560869 0.6285066 19.54545 0.8203885
> 11  2.761535 0.7017720 15.83333 0.1262136
> 12  0.995959 0.4638751  0.00000 0.9466022
> 13 -4.516514 0.2794199 77.85714 0.8834954
>  > sum(example$weights)
> [1] 13.00000
>
>  > model<-lm(y~1,data=example,weights=weights)
>  > add1(model,.~.+x1+x2)
> Single term additions
>
> Model:
> y ~ 1
>        Df Sum of Sq    RSS    AIC
> <none>              94.000 27.719
> x1      1    55.290 38.710 18.185
> x2      1    58.630 35.371 17.012
>   
Which version of R??!

I get (2.4.1 on Fedora 6):

 > add1(model,.~.+x1+x2)
Single term additions

Model:
y ~ 1
       Df Sum of Sq    RSS    AIC
<none>              74.541 24.703
x1      1    44.377 30.164 14.942
x2      1    36.619 37.922 17.918


From jverzani at gmail.com  Tue Mar 13 17:37:36 2007
From: jverzani at gmail.com (jverzani)
Date: Tue, 13 Mar 2007 16:37:36 +0000 (UTC)
Subject: [R] gtk button: how to create signal handler?
References: <d4327f7e0703130341n3b70ab8fk3673ec0971171c59@mail.gmail.com>
Message-ID: <loom.20070313T173629-23@post.gmane.org>

d. sarthi maheshwari <samay.sar <at> gmail.com> writes:

> 
> Hi
> 
> Kindly correct me if I am posting a wrong query in the forum.
> 
> I am trying to handle my "button:clicked" event. But not able to proceed
> further. Please help.
> Following is my code:
> 
> library(RGtk2)
> 
> win <- gtkWindowNew(type = NULL, show = TRUE)
> butt <- gtkButtonNewWithLabel("Submit", show = TRUE)
> win$Add(butt)
> 
> Now I want to do something when my button is clicked. How can I grab the
> "clicked" signal and define actions against it?
> 


Dear Sarthi, 

If you aren't in need of the full power of GTK (which Michael's suggestion gives
you), you might want to look at the gWidgets and gWidgetsRGtk2 package which
simplify GUI construction a bit:

require(gWidgets)
gbutton("submit", cont=gwindow("example"), handler=function(h,...) {
 cat("your action goes here\n")
})

The gWidgets package has a vignette with more demos.

--John



> Thanks in advance.
> 
> --
> Regards
> Sarthi M.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help <at> stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From exonintron at gmail.com  Tue Mar 13 17:51:32 2007
From: exonintron at gmail.com (Sender)
Date: Tue, 13 Mar 2007 09:51:32 -0700
Subject: [R] An example of "overloading" [
In-Reply-To: <200703130937.08404.vincent.goulet@act.ulaval.ca>
References: <686bf0c50703122128y377b730elc2db564439a2099a@mail.gmail.com>
	<200703130937.08404.vincent.goulet@act.ulaval.ca>
Message-ID: <686bf0c50703130951x19f04bafi880e891500e35e9d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070313/90cf4ed5/attachment.pl 

From mystata at hotmail.com  Tue Mar 13 17:52:19 2007
From: mystata at hotmail.com (Jun Xu)
Date: Tue, 13 Mar 2007 10:52:19 -0600
Subject: [R] turn regression coefficients into matrix or...
In-Reply-To: <738560.8767.qm@web39704.mail.mud.yahoo.com>
Message-ID: <BAY108-F13C2850A3725AB4BEE7F8DA87C0@phx.gbl>

Stephen, Norbert, Anup, and Mark, Thank you all for your help. With these 
tips, I now have a much better understanding of the structure in R. Another 
related question that has been haunting me is whether it is necessary to be 
a convert from other statistical packages to R. Obviously, the great 
advantage that R has is it's free and usually the newest stats technique is 
first implemented in R. I had extensive experience with SAS and Stata, but I 
really hate to keep investing int the never-satiated big financial abyss 
there (e.g., various editions of manuals. However, I have some reservation 
to  be a convert as inertia always plays a big role if the utility is not 
that big by switching from one software to another. What do you think?


>From: Stephen Tucker <brown_emu at yahoo.com>
>To: Norbert NEUWIRTH <norbertneuwirth at gmx.at>, Jun Xu <mystata at hotmail.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] turn regression coefficients into matrix or...
>Date: Tue, 13 Mar 2007 07:07:53 -0700 (PDT)
>
>I believe the short answer to your question lies in
>
>      summary(ans.reg[[i]])$coefficients
>
>which will give you a matrix with coefficients and standard errors (and
>more). You can also find out what other information you can get from the
>regressions if you type
>
>      attributes(ans.reg[[i]])
>      attributes(summary(ans.reg[[i]]))
>
>and then see the names of the list elements (the regression functions and
>summary() return objects of mode "list") that might correspond to the
>information you are looking for.
>
>Good luck,
>
>ST
>
>
>
>
>--- Norbert NEUWIRTH <norbertneuwirth at gmx.at> wrote:
>
> > jun,
> >
> > im am also quite new to R. so i think, this is a question all we 
>R-newbees
> >
> > ask  ;-). having had the same "problem" the other day, i "solved" it the
> > following way:
> >
> > ####  Multiple Regressions - Tables
> >
> > data(anscombe)                                      # load anscombe
> > dataset (implemented somwhere in R)
> > x5 <- rnorm(11,14,1)                                # create an 
>additional
> >
> > variable (at random)
> > anscombe.nn <- cbind(anscombe,x5)                   # attach the 
>variable
> > to the dataset
> > attach(anscombe.nn)                                 # attach the dataset
> > to the searchpath (just for convenience)
> > anscombe.nn                                         # have a look on the
> > data
> >
> > ans.reg <- vector(4, mode = "list")                 # create empty list
> > (just  for speeding up)
> >
> > for(i in 1:4){                                      # now the 4
> > regressions are stored to the list
> >      j <- i+1
> >      x1 <- get(paste("x", i, sep = ""))              # exogenous var. #1
> >      x2 <- get(paste("x", j, sep = ""))              # exogenous var. #2
> >      y <- get(paste("y", i, sep = ""))               # endogenous
> >      ans.reg[[i]] <- glm(y ~ x1+x2,family=gaussian)  # do the regression
> > (out of 4)
> >      print(summary(ans.reg[[i]],cor=FALSE))
> > }
> >
> > detach(anscombe)                                    # detach dataset 
>from
> > search path
> > lapply(ans.reg, coef)                               # see each 
>regression
> > in one line
> > sapply(ans.reg, coef)                               # have the
> > coefficients in a table
> > x<-as.matrix(sapply(ans.reg, coef))                 # convert table to
> > matrix
> > x
> >
> > this solution is quite comparable to Ligges(2007) [published in german 
>and
> >
> > japanese, i think]
> >
> > i have an additional question to list-members, that have left the
> > newbee-status yet: how can i get R to hand over the standard errors,
> > significance levels etc, so that i can create a table with  coeff and SE
> > and sig.?
> >
> > norbert
> >
> >
> > Am 13.03.2007, 02:58 Uhr, schrieb Jun Xu <mystata at hotmail.com>:
> >
> > > I don't have much experience with r. What I am trying to do is to turn
> > > regression coefficients (after I run a lm or glm model) into some 
>matrix
> > > such that I can do some post-estimation calculation, for example
> > > predicted
> > > probabilities in glm model, etc.. Or, is there any function in r that 
>I
> > > can
> > > use to do something along that line? thanks.
> > >
> > > Jun Xu, Ph.D.
> > > Department of Sociology
> > > Ball State University
> > >
> > > _________________________________________________________________
> > > Mortgage rates as low as 4.625% - Refinance $150,000 loan for $579 a
> > > month.
> > > Intro*Terms
> > >
> >
>https://www2.nextag.com/goto.jsp?product=100000035&url=%2fst.jsp&tm=y&search=mortgage_text_links_88_h27f6&disc=y&vers=743&s=4056&p=5117
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >
> >
> > --
> > -------------------------------
> > Mag. Norbert NEUWIRTH
> >
> > Roubiczekgasse 2/23
> > A-1100  WIEN
> > mob: +43 699 1835 0704
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>
>
>____________________________________________________________________________________
>Food fight? Enjoy some healthy debate
>in the Yahoo! Answers Food & Drink Q&A.
>http://answers.yahoo.com/dir/?link=list&sid=396545367

_________________________________________________________________
With tax season right around the corner, make sure to follow these few 
simple tips.


From h.wickham at gmail.com  Tue Mar 13 17:56:40 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 13 Mar 2007 11:56:40 -0500
Subject: [R] Rattle() GGobi()-Plots- How to save?
In-Reply-To: <b4485c4c0703130753t6c2e40d4h65f5358ca8b136ea@mail.gmail.com>
References: <b4485c4c0703130140n315e5e77p3f0a76f25ac9d951@mail.gmail.com>
	<22038.212.209.13.15.1173789582.squirrel@www.sorch.se>
	<b4485c4c0703130753t6c2e40d4h65f5358ca8b136ea@mail.gmail.com>
Message-ID: <f8e6ff050703130956w32d19b98x26c63b695b55c45d@mail.gmail.com>

On 3/13/07, j.joshua thomas <researchjj at gmail.com> wrote:
> i tried with package DescribeDisplay
>
> dd_load(DescribeDisplay)
>                         Load describe display
>
> i couldnt find any documentation. could you give me a sample

I've just improved http://www.ggobi.org/describe-display/, and
uploaded a new DescribeDisplay package to CRAN (it may take a few days
before it is available)

I hope this helps,

Hadley


From M.J.Bojanowski at uu.nl  Tue Mar 13 18:01:34 2007
From: M.J.Bojanowski at uu.nl (Bojanowski, M.J.  (Michal))
Date: Tue, 13 Mar 2007 18:01:34 +0100
Subject: [R] inconsistent behaviour of add1 and drop1 with a weighted
	linear	model
In-Reply-To: <45F6D2BB.6020903@biostat.ku.dk>
References: <45F6C69E.10200@york.ac.uk> <45F6D2BB.6020903@biostat.ku.dk>
Message-ID: <94E133D09AA24D43BF6341B675C01A33113511@uu01msg-exb01.soliscom.uu.nl>

Hmmm, this is the same I got

> add1(model,.~.+x1+x2)
Single term additions

Model:
y ~ 1
       Df Sum of Sq    RSS    AIC
<none>              74.541 24.703
x1      1    44.377 30.164 14.942
x2      1    36.619 37.922 17.918 


R version 2.4.1 (2006-12-18) 
i386-pc-mingw32 

locale:
LC_COLLATE=Polish_Poland.1250;LC_CTYPE=Polish_Poland.1250;LC_MONETARY=Po
lish_Poland.1250;LC_NUMERIC=C;LC_TIME=Polish_Poland.1250

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"
"methods"  
[7] "base"     
> 


*** Note that my e-mail address has changed to m.j.bojanowski at uu.nl
*** Please update your address books accordingly. Thank you!

_________________________________________
Michal Bojanowski
ICS / Sociology
Utrecht University
Heidelberglaan 2; 3584 CS Utrecht
Room 1428
m.j.bojanowski at uu.nl
http://www.fss.uu.nl/soc/bojanowski
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Dalgaard
Sent: Tuesday, March 13, 2007 5:35 PM
To: Jenny Hodgson
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] inconsistent behaviour of add1 and drop1 with a
weighted linear model

Jenny Hodgson wrote:
> Dear R Help,
> I have noticed some inconsistent behaviour of add1 and drop1 with a 
> weighted linear model, which affects the interpretation of the
results.
> I have these data to fit with a linear model, I want to weight them by

> the relative size of the geographical areas they represent.
> ______________________________________________________________________
> ___________________
>  > example
>            y        x1       x2   weights
> 1  -4.546477 0.1859556 50.00000 0.9466022
> 2   1.484246 0.4740497 29.88000 1.3252430
> 3   2.203681 0.8594264 16.93333 0.9466022
> 4   1.943163 0.8713360 42.11765 2.7766997
> 5   1.886473 0.9006082 19.00000 0.9466022
> 6   1.795393 0.8455183 23.68421 1.1674760
> 7   1.878077 0.5641396 35.00000 0.8203885
> 8  -4.215484 0.4356858 58.75000 0.4417477
> 9   1.993339 0.5440061 19.28571 0.8519420
> 10  1.560869 0.6285066 19.54545 0.8203885
> 11  2.761535 0.7017720 15.83333 0.1262136
> 12  0.995959 0.4638751  0.00000 0.9466022
> 13 -4.516514 0.2794199 77.85714 0.8834954  > sum(example$weights) [1] 
> 13.00000
>
>  > model<-lm(y~1,data=example,weights=weights)
>  > add1(model,.~.+x1+x2)
> Single term additions
>
> Model:
> y ~ 1
>        Df Sum of Sq    RSS    AIC
> <none>              94.000 27.719
> x1      1    55.290 38.710 18.185
> x2      1    58.630 35.371 17.012
>   
Which version of R??!

I get (2.4.1 on Fedora 6):

 > add1(model,.~.+x1+x2)
Single term additions

Model:
y ~ 1
       Df Sum of Sq    RSS    AIC
<none>              74.541 24.703
x1      1    44.377 30.164 14.942
x2      1    36.619 37.922 17.918

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From canelo2 at gmail.com  Tue Mar 13 18:04:09 2007
From: canelo2 at gmail.com (Antonio Silva)
Date: Tue, 13 Mar 2007 12:04:09 -0500
Subject: [R] Looking for "ArfimaOxFit.ox" and "ArfimaOxPredict.ox" files
Message-ID: <25ed29cb0703131004q3fc0846etc0bbc625bd978594@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070313/ea7adfa8/attachment.pl 

From jh69 at york.ac.uk  Tue Mar 13 18:04:33 2007
From: jh69 at york.ac.uk (Jenny Hodgson)
Date: Tue, 13 Mar 2007 17:04:33 +0000
Subject: [R] inconsistent behaviour of add1 and drop1 with a weighted
 linear	model
In-Reply-To: <45F6D2BB.6020903@biostat.ku.dk>
References: <45F6C69E.10200@york.ac.uk> <45F6D2BB.6020903@biostat.ku.dk>
Message-ID: <45F6D9A1.8020101@york.ac.uk>

I was using Version 2.3.1 for Windows (binary download version). Guess I
should be using the latest version, (but the reason is I'm writing up my
PhD and I thought my results would be more 'repeatable' if I didn't keep
changing my version of the software, I didn't really think there would
be any glitches as big as this). Sorry if this is a waste of your time.
And thanks very much for replying so quickly.

Jenny

_____________
Jenny Hodgson
Department of Biology - area 18
PO box 373
University of York
YO10 5YW
UK
Tel: 01904 328623



Peter Dalgaard wrote:

> Jenny Hodgson wrote:
>
>> Dear R Help,
>> I have noticed some inconsistent behaviour of add1 and drop1 with a 
>> weighted linear model, which affects the interpretation of the results.
>> I have these data to fit with a linear model, I want to weight them 
>> by the relative size of the geographical areas they represent.
>> _________________________________________________________________________________________ 
>>
>>  > example
>>            y        x1       x2   weights
>> 1  -4.546477 0.1859556 50.00000 0.9466022
>> 2   1.484246 0.4740497 29.88000 1.3252430
>> 3   2.203681 0.8594264 16.93333 0.9466022
>> 4   1.943163 0.8713360 42.11765 2.7766997
>> 5   1.886473 0.9006082 19.00000 0.9466022
>> 6   1.795393 0.8455183 23.68421 1.1674760
>> 7   1.878077 0.5641396 35.00000 0.8203885
>> 8  -4.215484 0.4356858 58.75000 0.4417477
>> 9   1.993339 0.5440061 19.28571 0.8519420
>> 10  1.560869 0.6285066 19.54545 0.8203885
>> 11  2.761535 0.7017720 15.83333 0.1262136
>> 12  0.995959 0.4638751  0.00000 0.9466022
>> 13 -4.516514 0.2794199 77.85714 0.8834954
>>  > sum(example$weights)
>> [1] 13.00000
>>
>>  > model<-lm(y~1,data=example,weights=weights)
>>  > add1(model,.~.+x1+x2)
>> Single term additions
>>
>> Model:
>> y ~ 1
>>        Df Sum of Sq    RSS    AIC
>> <none>              94.000 27.719
>> x1      1    55.290 38.710 18.185
>> x2      1    58.630 35.371 17.012
>>   
>
> Which version of R??!
>
> I get (2.4.1 on Fedora 6):
>
> > add1(model,.~.+x1+x2)
> Single term additions
>
> Model:
> y ~ 1
>       Df Sum of Sq    RSS    AIC
> <none>              74.541 24.703
> x1      1    44.377 30.164 14.942
> x2      1    36.619 37.922 17.918
>


From tplate at acm.org  Tue Mar 13 18:21:07 2007
From: tplate at acm.org (Tony Plate)
Date: Tue, 13 Mar 2007 10:21:07 -0700
Subject: [R] timeDate & business day
In-Reply-To: <45F5EC64.8020808@sfu.ca>
References: <45F5EC64.8020808@sfu.ca>
Message-ID: <45F6DD83.3030400@acm.org>

The R timeDate class is in the fCalendar package.

Does anyone know how to change the output format of a timeDate object? 
(Other than by explicitly supplying a format= argument to the format() 
function.)  I tried creating a timeDate object, and then changing the 
format slot.  However, all the functions I used on the object ('print', 
'format', 'as.character', 'show') seemed to ignore the value in the 
format slot.

And does anyone else find it a little confusing that print() and show() 
convert timeDate to the local time zone, but as.character() and format() 
display it in the time zone of its "FinCenter" slot?

Here is a transcript:

 > library(fCalendar)
 > tt <- c("2005-01-04", "2005-01-05", "2005-01-06", "2005-01-07")
 > x <- timeDate(tt)
 > x at format
[1] "%Y-%m-%d"
 > # Change the format on the timeDate object
 > x at format <- "%Y%m%d"
 > x
An object of class "timeDate"
Slot "Data":
[1] "2005-01-03 17:00:00 Mountain Standard Time"
[2] "2005-01-04 17:00:00 Mountain Standard Time"
[3] "2005-01-05 17:00:00 Mountain Standard Time"
[4] "2005-01-06 17:00:00 Mountain Standard Time"

Slot "Dim":
[1] 4

Slot "format":
[1] "%Y%m%d"

Slot "FinCenter":
[1] "GMT"

 > # Can get what I want by explicitly supplying format
 > # argument to format()
 > format(x, format="%Y%m%d")
[1] "20050104" "20050105" "20050106" "20050107"
 > # But format() seems to ignore the format slot
 > format(x)
[1] "2005-01-04" "2005-01-05" "2005-01-06" "2005-01-07"
 > print(x)
GMT
[1] [2005-01-04] [2005-01-05] [2005-01-06] [2005-01-07]
 > as.character(x)
[1] "2005-01-04" "2005-01-05" "2005-01-06" "2005-01-07"
attr(,"control")
FinCenter
     "GMT"
 >
 > sessionInfo()
R version 2.4.1 (2006-12-18)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
States.1252;LC_MONETARY=English_United 
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
[7] "base"

other attached packages:
   fCalendar     fEcofin
"240.10068" "240.10067"
 > Sys.getenv("TZ")
TZ
""
 >

-- Tony Plate

Michael Toews wrote:
> Those numbers look like ... well, numbers. You want characters! Try 
> converting the integer to a character before trying to do a string 
> parse, e.g.:
> 
> ymd.int <- c(20050104, 20050105, 20050106, 20050107, 20050110, 20050111, 
> 20050113, 20050114)
> ymd <- as.Date(as.character(ymd.int),"%Y%m%d")
> 
> As far as the other functions you are looking at ("timeDate", 
> "timeRelative") -- I've never seen these, so I'm guessing they are 
> S-PLUS. In R, you can use "diff" or "difftime" (which works with "Date" 
> and "POSIXlt"-or Date-Time classes) , e.g.:
> 
> diff(ymd)
> diff(ymd,2)
> diff(ymd,3)
> 
> or do some arithmetic:
> 
> difftime(ymd[1],ymd[4])
> difftime(ymd[1],ymd[4],unit="weeks")
> 
> Hopefully this is helpful to you!
> +mt
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tplate at acm.org  Tue Mar 13 18:24:41 2007
From: tplate at acm.org (Tony Plate)
Date: Tue, 13 Mar 2007 10:24:41 -0700
Subject: [R] timeDate & business day
In-Reply-To: <45F63DC1.6010108@sfu.ca>
References: <45F5EC64.8020808@sfu.ca>	<04b901c76512$bd1894f0$3927a8c0@treesdalellc.net>	<45F621C2.4040703@sfu.ca>	<AD40423F-7AE6-4021-BA10-26410FD76736@gmail.com>
	<45F63DC1.6010108@sfu.ca>
Message-ID: <45F6DE59.7050706@acm.org>

There are two articles describing time and date classes in the R-News 
letter:

Brian D. Ripley and Kurt Hornik. Date-time classes. R News, 1(2):8-11, 
June 2001.
http://cran.r-project.org/doc/Rnews/Rnews_2001-2.pdf

Gabor Grothendieck and Thomas Petzoldt. R help desk: Date and time 
classes in R. R News, 4(1):29-32, June 2004.
http://cran.r-project.org/doc/Rnews/Rnews_2004-1.pdf

The Ripley and Hornik article discusses the "POSIXt" (Posix time) 
classes: "POSIXlt" (POSIX local time) and "POSIXct" (POSIX calendar time).

The Grothendieck and Petzoldt article discusses the "Date", "chron" and 
"POSIXt" classes, and has a very helpful table of how to do various 
operations on "Date", "chron" and "POSIXct" objects.

There is also the fCalandar package, which includes a timeDate class and 
has support for holidays, operations on timeDate objects, and various 
other features useful for dealing with times and dates as they are used 
in financial data.

Obviously, there is the online help for the fCalendar package, but there 
are also three other documents describing how to work with timeDate objects:

Computing with R and S-Plus For Financial Engineers 1 - Part I - 
Markets, Basic Statistics, Date and Time Management, Diethelm W?urtz
http://www.itp.phys.ethz.ch/econophysics/R/docs/fBasics.pdf

R and Rmetrics for Teaching.  Financial Engineering and Computational 
Finance, Part II, Dates, Time, and, Calendars, Diethelm W?urtz
http://www.itp.phys.ethz.ch/econophysics/R/docs/rCalendar.pdf

S4 ?timeDate? and ?timeSeries? Classes for R, Diethelm W?urtz
http://www.itp.phys.ethz.ch/econophysics/R/pdf/calendar.pdf


-- Tony Plate


Michael Toews wrote:
> Sadly, I don't know of any tutorials or much help on the web for R ... 
> that doesn't mean it doesn't exist ... you might just have to look 
> around for it (www.rseek.org is a good place to start)
> I've learned almost everything I know through:
> ?strptime
> 
> Also check out the methods for the classes, for example:
> 
> methods(class="Date")
> methods(class="POSIXct")
> 
> And certainly check their help pages ... there is loads of stuff here 
> that I haven't discovered myself. (Note, if you are new to S3 classes .. 
> if it begins with the method, then "." class, you only need to type the 
> beginning. For example "summary(ymd)" ... not "summary.Date(ymd)" if 
> "ymd" has `class(ymd) == "Date" `.
> 
> I think the fundamental things to know are there are three main 
> DateTimeClasses:
> 
>   1. "POSIXct" - has date, time and optionally time-zone info -- very
>      handy for using in data.frame objects (and frankly I think it
>      should be renamed to "DateTime" since the class "POSIXct" has
>      nothing really to do directly with date/times)
>   2. "POSIXlt" - as far as I'm concerned, this is has the same
>      functionality as "POSIXct", but it cannot be used in data.frame
>      objects (and frankly, I think it should be deprecated in favour of
>      #1 to reduce future confusion)
>   3. "Date" - use this if you don't care about times or time-zones
> 
> But it would be nice to track down a good tutorial somewhere.
> +mt
> 
> Young Cho wrote:
>> Thanks so Michael! If you know of a tutorial or introductory document 
>> about timeDate manipulation or time series manipulation in R, can you 
>> share it? It is hard to find by googling... I'd very appreciate any 
>> advice.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jfox at mcmaster.ca  Tue Mar 13 18:44:30 2007
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 13 Mar 2007 13:44:30 -0400
Subject: [R] Rcmdir does not work in SciVews-R
In-Reply-To: <45F6CD6C.40302@gmail.com>
Message-ID: <20070313174431.BWGJ1593.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Kihwang,

I believe that you'll have to use an older version of the Rcmdr package with
SciViews; you can download version 1.1-2 of the Rcmdr from the SciViews web
page <http://www.sciviews.org/SciViews-R/>. For more information, you might
contact the SciViews developers.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kihwang Lee
> Sent: Tuesday, March 13, 2007 12:12 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Rcmdir does not work in SciVews-R
> 
> Hi,
> 
> I have recently installed version 2.4.1 of R and also 
> installed SciViews-R 0.8.9. It worked fine. Then I installed 
> the Rcmdr 1.2-7 from CRAN . However the 'R commander menu' in 
> the dockable panel does not work. If I start Rcmdr in R 
> without SciViews, it works perfectly.
> 
> Is there any way that I can use Rcmdr with SciViews?
> 
> Many thanks in advanve.
> 
> Kihwang
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From david.meyer at wu-wien.ac.at  Tue Mar 13 19:10:20 2007
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Tue, 13 Mar 2007 19:10:20 +0100
Subject: [R]  distance metrics
Message-ID: <45F6E90C.7050002@wu-wien.ac.at>

 > Hello:
 > > >
 > > > Does anyone know if there exists a package that handles methods for [
 > for
 > > > dist objects?
 > > >
 > > > I would like to access a dist object using matrix notation
 > > >
 > > > e.g.
 > > >
 > > > dMat = dist(x)
 > > > dMat[i,j]


You can use the [[ operator defined for distance matrices currently in 
package cba, which allows subsetting "dist" objects. (Note that this 
will move to the new "proxy" package on proximity measures very soon).

David


From roger.bos at us.rothschild.com  Tue Mar 13 19:15:16 2007
From: roger.bos at us.rothschild.com (Bos, Roger)
Date: Tue, 13 Mar 2007 14:15:16 -0400
Subject: [R] Freeman-Tukey arcsine transformation
In-Reply-To: <45F6BC14.2020300@biostat.ku.dk>
Message-ID: <D8C95B444AD6EE4AAD638D818A9CFD343A1FC8@RINNYCSE000.rth.ad.rothschild.com>

I'm curious what this transformation does, but I am not curious enough to pay $14 to find out.  Someone once told me that the arcsine was a good way to transform data and make it more 'normal'.  I am wondering if this is an improved method.  Anyone know of a free reference?

 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Dalgaard
Sent: Tuesday, March 13, 2007 10:58 AM
To: Inman, Brant A. M.D.
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Freeman-Tukey arcsine transformation

Inman, Brant A. M.D. wrote:
> R-Experts:
>
> Does anyone know if there are R functions to perform the Freeman-Tukey 
> double arcsine transformation and then backtransform it?
>
>   

Well, if not, both are given by explicit formulas, so it shouldn't take long to implement, cf.:

    The Teacher's Corner


        *The Inverse of the Freeman-Tukey Double Arcsine Transformation
        <http://www.jstor.org/view/00031305/di020556/02p0095b/0?frame=noframe&dpi=3&userID=c0261211 at all.ku.dk/01cce4405c00501bb027f&backcontext=page&backurl=/cgi-bin/jstor/viewitem/00031305/di020556/02p0095b/0%3fframe%3dnoframe%26dpi%3d3%26userID%3dc0261211 at all.ku.dk/01cce4405c00501bb027f%26config%3d%26PAGE%3d0&config=jstor&PAGE=0>*



            John J. Miller 


            /The American Statistician/, Vol. 32, No. 4. (Nov., 1978),
            p. 138. 


            Stable URL:
            http://links.jstor.org/sici?sici=0003-1305%28197811%2932%3A4%3C138%3ATIOTFD%3E2.0.CO%3B2-Z



              Abstract

        A formula for the inverse of the Freeman-Tukey double arcsine
        transformation is derived. This formula is useful when
        expressing means of double arcsines as retransformed
        proportions. When the mean is taken from original proportions
        involving different n's, it is suggested that the harmonic mean
        of the n's be used in the inversion formula.



> Thanks,
>
> Brant Inman
> Mayo Clinic
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

********************************************************************** * 
This message is for the named person's use only. It may 
contain confidential, proprietary or legally privileged 
information. No right to confidential or privileged treatment 
of this message is waived or lost by any error in 
transmission. If you have received this message in error, 
please immediately notify the sender by e-mail, 
delete the message and all copies from your system and destroy 
any hard copies. You must not, directly or indirectly, use, 
disclose, distribute, print or copy any part of this message 
if you are not the intended recipient.


From toby_marks at americancentury.com  Tue Mar 13 19:32:18 2007
From: toby_marks at americancentury.com (toby_marks at americancentury.com)
Date: Tue, 13 Mar 2007 13:32:18 -0500
Subject: [R] RODBC Excel sqlQuery  insert into
Message-ID: <OF450B6A6F.98D186C9-ON8625729D.006181AD-8625729D.0065D5BB@americancentury.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070313/e9863c71/attachment.pl 

From spluque at gmail.com  Tue Mar 13 19:48:45 2007
From: spluque at gmail.com (Sebastian P. Luque)
Date: Tue, 13 Mar 2007 13:48:45 -0500
Subject: [R] Freeman-Tukey arcsine transformation
References: <45F6BC14.2020300@biostat.ku.dk>
	<D8C95B444AD6EE4AAD638D818A9CFD343A1FC8@RINNYCSE000.rth.ad.rothschild.com>
Message-ID: <87slc99dyq.fsf@patagonia.sebmags.homelinux.org>

On Tue, 13 Mar 2007 14:15:16 -0400,
"Bos, Roger" <roger.bos at us.rothschild.com> wrote:

> I'm curious what this transformation does, but I am not curious enough
> to pay $14 to find out.  Someone once told me that the arcsine was a
> good way to transform data and make it more 'normal'.  I am wondering if
> this is an improved method.  Anyone know of a free reference?

My Zar?, says this is just:


p' = 1/2 * (asin(sqrt(x / (n + 1))) + asin(sqrt((x + 1) / (n + 1))))


so solving for x should give the back-transformation.  It is recommended
when the proportions that need to be "disciplined" are very close to the
ends of the range (0, 1; 0, 100).


+---- *Footnotes* ----+
? @BOOK{149,
  title = {Biostatistical analysis},
  publisher = {Prentice-Hall, Inc.},
  year = {1996},
  author = {Zar, J. H.},
  address = {Upper Saddle River, New Jersey},
  key = {149},
}


-- 
Seb


From p.dalgaard at biostat.ku.dk  Tue Mar 13 19:56:44 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 13 Mar 2007 19:56:44 +0100
Subject: [R] inconsistent behaviour of add1 and drop1 with a weighted
 linear	model
In-Reply-To: <45F6D9A1.8020101@york.ac.uk>
References: <45F6C69E.10200@york.ac.uk> <45F6D2BB.6020903@biostat.ku.dk>
	<45F6D9A1.8020101@york.ac.uk>
Message-ID: <45F6F3EC.3010908@biostat.ku.dk>

Jenny Hodgson wrote:
> I was using Version 2.3.1 for Windows (binary download version). Guess I
> should be using the latest version, (but the reason is I'm writing up my
> PhD and I thought my results would be more 'repeatable' if I didn't keep
> changing my version of the software, I didn't really think there would
> be any glitches as big as this). Sorry if this is a waste of your time.
> And thanks very much for replying so quickly.
>
> Jenny
>
>   

Always a good idea to check the NEWS file:

    o   add1.lm() had been broken by other changes for weighted fits.


From jmacdon at med.umich.edu  Tue Mar 13 20:05:11 2007
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Tue, 13 Mar 2007 15:05:11 -0400
Subject: [R] RODBC Excel sqlQuery  insert into
In-Reply-To: <OF450B6A6F.98D186C9-ON8625729D.006181AD-8625729D.0065D5BB@americancentury.com>
References: <OF450B6A6F.98D186C9-ON8625729D.006181AD-8625729D.0065D5BB@americancentury.com>
Message-ID: <45F6F5E7.8090508@med.umich.edu>

Hi Toby,

toby_marks at americancentury.com wrote:
> I have searched the archives for using insert into to update spreadsheets 
> using RODBC and have come up short. So, first off, is it possible?

I don't think so. Writing to an Excel spreadsheet is probably easier 
done using the RDCOMClient package. There are some good examples of how 
to do things that come with the package. Searching the R-help archives 
should also come up with some good examples.

Best,

Jim




-- 
James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623


**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.


From roger at ysidro.econ.uiuc.edu  Tue Mar 13 20:06:48 2007
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Tue, 13 Mar 2007 14:06:48 -0500
Subject: [R] Freeman-Tukey arcsine transformation
In-Reply-To: <87slc99dyq.fsf@patagonia.sebmags.homelinux.org>
References: <45F6BC14.2020300@biostat.ku.dk>
	<D8C95B444AD6EE4AAD638D818A9CFD343A1FC8@RINNYCSE000.rth.ad.rothschild.com>
	<87slc99dyq.fsf@patagonia.sebmags.homelinux.org>
Message-ID: <CFD9A6F2-B250-48D2-9457-EEF69B7C2907@ysidro.econ.uiuc.edu>

As a further footnote on this, I can't resist mentioning a letter  
that appears
in Technometrics (1977) by Steve  Portnoy who notes that

	2 arcsin(sqrt(p)) = arcsin(2p - 1) + pi/2

and asks: "it would be of historical interest to know if any early  
statisticians
were aware of this, and if so, why the former version was  
preferred."  The
latter version seems more convenient since it obviously obviates the  
need
for special tables that appear in many places.



url:    www.econ.uiuc.edu/~roger                Roger Koenker
email   rkoenker at uiuc.edu                       Department of Economics
vox:    217-333-4558                            University of Illinois
fax:    217-244-6678                            Champaign, IL 61820


On Mar 13, 2007, at 1:48 PM, Sebastian P. Luque wrote:

> On Tue, 13 Mar 2007 14:15:16 -0400,
> "Bos, Roger" <roger.bos at us.rothschild.com> wrote:
>
>> I'm curious what this transformation does, but I am not curious  
>> enough
>> to pay $14 to find out.  Someone once told me that the arcsine was a
>> good way to transform data and make it more 'normal'.  I am  
>> wondering if
>> this is an improved method.  Anyone know of a free reference?
>
> My Zar?, says this is just:
>
>
> p' = 1/2 * (asin(sqrt(x / (n + 1))) + asin(sqrt((x + 1) / (n + 1))))
>
>
> so solving for x should give the back-transformation.  It is  
> recommended
> when the proportions that need to be "disciplined" are very close  
> to the
> ends of the range (0, 1; 0, 100).
>
>
> +---- *Footnotes* ----+
> ? @BOOK{149,
>   title = {Biostatistical analysis},
>   publisher = {Prentice-Hall, Inc.},
>   year = {1996},
>   author = {Zar, J. H.},
>   address = {Upper Saddle River, New Jersey},
>   key = {149},
> }
>
>
> -- 
> Seb
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mbauer at usc.edu  Tue Mar 13 19:39:48 2007
From: mbauer at usc.edu (Madeline Bauer)
Date: Tue, 13 Mar 2007 10:39:48 -0800
Subject: [R] ASA Southern California Chapter Applied Statistics Workshop
Message-ID: <5.1.0.14.2.20070313103937.02442d50@email.usc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070313/f52da4bf/attachment.pl 

From h.wickham at gmail.com  Tue Mar 13 20:14:40 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 13 Mar 2007 14:14:40 -0500
Subject: [R] inconsistent behaviour of add1 and drop1 with a weighted
	linear model
In-Reply-To: <45F6F3EC.3010908@biostat.ku.dk>
References: <45F6C69E.10200@york.ac.uk> <45F6D2BB.6020903@biostat.ku.dk>
	<45F6D9A1.8020101@york.ac.uk> <45F6F3EC.3010908@biostat.ku.dk>
Message-ID: <f8e6ff050703131214gb9fe64es221aa7241d2f169e@mail.gmail.com>

On 3/13/07, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Jenny Hodgson wrote:
> > I was using Version 2.3.1 for Windows (binary download version). Guess I
> > should be using the latest version, (but the reason is I'm writing up my
> > PhD and I thought my results would be more 'repeatable' if I didn't keep
> > changing my version of the software, I didn't really think there would
> > be any glitches as big as this). Sorry if this is a waste of your time.
> > And thanks very much for replying so quickly.
> >
> > Jenny
> >
> >
>
> Always a good idea to check the NEWS file:
>
>     o   add1.lm() had been broken by other changes for weighted fits.

Yes, but a little impractical.  Has any one ever considered a standard
for the news file so changes could be extracted and included in online
documentation somewhere?

Hadley


From p.dalgaard at biostat.ku.dk  Tue Mar 13 20:30:28 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 13 Mar 2007 20:30:28 +0100
Subject: [R] Freeman-Tukey arcsine transformation
In-Reply-To: <D8C95B444AD6EE4AAD638D818A9CFD343A1FC8@RINNYCSE000.rth.ad.rothschild.com>
References: <D8C95B444AD6EE4AAD638D818A9CFD343A1FC8@RINNYCSE000.rth.ad.rothschild.com>
Message-ID: <45F6FBD4.4080308@biostat.ku.dk>

Bos, Roger wrote:
> I'm curious what this transformation does, but I am not curious enough to pay $14 to find out.  Someone once told me that the arcsine was a good way to transform data and make it more 'normal'.  I am wondering if this is an improved method.  Anyone know of a free reference?
>
>  
>   
Well, a paper copy of the American Statistician (1978) would be free in 
some sense....

In the meantime I got detached from JSTOR (i.e., I went home), and I'm 
not prepared to jump through the relevant hoops for remote access at 
this point, but AFAIR it was a relatively trivial version of the simple 
arcsine transform, something like replacing asin(r/n) with the average 
of asin(r/(n+1)) and asin((r+1)/(n+1)). The point of the paper is that 
you can invert explicitly for r/n if n is known.

>             /The American Statistician/, Vol. 32, No. 4. (Nov., 1978),
>             p. 138. 
>
>
>             Stable URL:
>             http://links.jstor.org/sici?sici=0003-1305%28197811%2932%3A4%3C138%3ATIOTFD%3E2.0.CO%3B2-Z
>
>


From jhorn at bu.edu  Tue Mar 13 20:34:57 2007
From: jhorn at bu.edu (Jason Horn)
Date: Tue, 13 Mar 2007 15:34:57 -0400
Subject: [R] Adding bars to the right of existing ones using barplot
Message-ID: <CC871968-A9E2-4E96-95B6-56A2FA3CDED3@bu.edu>

I'm trying to create a barplot that has two sets of data next to each  
other.  I'm using barplot with the add=TRUE option, but this simply  
adds the second dataset on top of the first, obscuring it.  How do I  
add the new data to the right on the existing barplot so that both  
sets are visible?  I've been playing with all sorts of option and  
reading the list archives with no luck.

Thank you anyone who has the answer.

- Jason


From toby_marks at americancentury.com  Tue Mar 13 20:38:56 2007
From: toby_marks at americancentury.com (toby_marks at americancentury.com)
Date: Tue, 13 Mar 2007 14:38:56 -0500
Subject: [R] RODBC Excel sqlQuery  insert into
In-Reply-To: <45F6F5E7.8090508@med.umich.edu>
Message-ID: <OFA32ED8FC.2B95B6BE-ON8625729D.006AD820-8625729D.006BEF5A@americancentury.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070313/41af2440/attachment.pl 

From kihwang.lee at gmail.com  Tue Mar 13 20:46:27 2007
From: kihwang.lee at gmail.com (Kihwang Lee)
Date: Wed, 14 Mar 2007 04:46:27 +0900
Subject: [R] Rcmdir does not work in SciVews-R
In-Reply-To: <20070313174431.BWGJ1593.tomts13-srv.bellnexxia.net@JohnDesktop8300>
References: <20070313174431.BWGJ1593.tomts13-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <45F6FF93.2050401@gmail.com>

Tried. Did not work. Thanks anyway. I will contact the SciViews
developers as you suggested.

Kihwang

John Fox wrote:
> Dear Kihwang,
> 
> I believe that you'll have to use an older version of the Rcmdr package with
> SciViews; you can download version 1.1-2 of the Rcmdr from the SciViews web
> page <http://www.sciviews.org/SciViews-R/>. For more information, you might
> contact the SciViews developers.
> 
> I hope this helps,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kihwang Lee
>> Sent: Tuesday, March 13, 2007 12:12 PM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] Rcmdir does not work in SciVews-R
>>
>> Hi,
>>
>> I have recently installed version 2.4.1 of R and also 
>> installed SciViews-R 0.8.9. It worked fine. Then I installed 
>> the Rcmdr 1.2-7 from CRAN . However the 'R commander menu' in 
>> the dockable panel does not work. If I start Rcmdr in R 
>> without SciViews, it works perfectly.
>>
>> Is there any way that I can use Rcmdr with SciViews?
>>
>> Many thanks in advanve.
>>
>> Kihwang
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
>


From jr_frrr at yahoo.de  Tue Mar 13 21:06:37 2007
From: jr_frrr at yahoo.de (=?ISO-8859-1?Q?Jos=E9?= Rafael Ferrer Paris)
Date: Tue, 13 Mar 2007 16:06:37 -0400
Subject: [R] segfault with correlation structures in nlme
Message-ID: <1173816397.28892.52.camel@localhost.localdomain>

Hi out there,

I am trying to fit a species accumulation curve (increase in number of
species known vs. sampling effort) for multiple regions and several
bootstrap samples. The bootstrap samples represent different
arrangements of the actual sample sequence.

 I fitted a series of nlme-models and everything seems OK, but since the
observations are correlated I tried to include some correlation
structure. Since the ARMA classes were not succesful in reducing this
correlation, I tried spatial correlation functions with sampling effort
(measured in time units) as a distance measure. As a result I got
several segfault errors (which I don't know what they exactly mean =/).

I was wondering if it was an effect of the model or the data I used, but
I was able to reproduce the error messages using the Ovary data set and
the example in the Pinheiro & Bates book:

>library(nlme)
>data(ovary)
>fm10var.lme <-  lme(follicles ~ sin(2 * pi * Time) +
                    cos(2 * pi * Time),data=Ovary,
                    random=pdDiag(~sin(2*pi*Time)))
>fm50var.lme <- update(fm10var.lme,correlation=corARMA(p=1,q=1))
>fm10var.nlme <-  nlme(follicles ~ A + B * sin(2 * pi * w * Time) +
                      C * cos(2 * pi * w * Time),data=Ovary,
                      fixed= A+B+C+w~1,
                      random=pdDiag(A+B+w~1),
                       start = c(fixef(fm50var.lme),1))
>plot(ACF(fm10var.nlme,maxLag=10),alpha=.05)
>fm20var.nlme <- update(fm10var.nlme,corr=corAR1(0.311))
>fm30var.nlme <- update(fm10var.nlme,corr=corARMA(p=0,q=2))

>fm60var.nlme <- update(fm10var.nlme,corr=corGaus(form=~Time))

 *** caught segfault ***
address 0x1075e501, cause 'memory not mapped'

Traceback:
 1: eval(expr, envir, enclos)
 2: eval(modelExpression, env)
 3: assign("modelValue", eval(modelExpression, env), envir = thisEnv)
 4: function (newPars) {    if (!missing(newPars)) {        for (i in
names(ind)) {            assign(i, clearNames(newPars[ind[[i]]]), envir
= env)        }      assign("modelValue", eval(modelExpression, env),
envir = thisEnv)    }    modelValue}(c(-4.11936939372863,
0.157676781855328, -0.071492289279548, -3.89562017836122,
3.06079106423361, -0.0260217128029253, -2.83650117790746,
1.62924792896056, 0.0607736472981399, -0.834700672739711,
0.07926271837803, 0.0403415470454326, -0.698687811226831,
-0.121079563050668, 0.032722843419347, 0.0404804003710554,
0.379937934267249, 0.087562808768161, 3.08430247504295,
2.08510442577037, 0.103812382483994, 1.44441306260898,
-1.70151546937888, -0.0432622314094444, 2.37971674786747,
-1.04307471138899, 0.0217885202778396, 2.60583067635322,
-0.638495324795519, -0.137018044847307, 1.70105176178795,
-3.07372462709182, -0.0445309584408364, 12.3518546784787,
-3.22126648052618, -1.69049623029482, 0.907261039321137))
 5: .C(fit_nlme, thetaPNLS = as.double(c(as.vector(unlist(sran)),
sfix)), pdFactor = as.double(pdFactor(nlmeSt$reStruct)),
as.integer(unlist(rev(grpShrunk))), as.integer(unlist(Dims)),
as.integer(attr(nlmeSt$reStruct, "settings"))[-(1:3)], as.double(cF),
as.double(vW), as.integer(unlist(cD)), settings =
as.double(pnlsSettings),     additional = double(NReal * (pLen + 1)),
as.integer(!is.null(correlation)),     as.integer(!is.null(weights)),
nlModel, NAOK = TRUE)
 6: nlme.formula(model = follicles ~ A + B * sin(2 * pi * w * Time) +
C * cos(2 * pi * w * Time), data = Ovary, fixed = A + B +     C + w ~ 1,
random = pdDiag(A + B + w ~ 1), start = c(fixef(fm50var.lme),     1),
corr = corGaus(form = ~Time))
 7: eval(expr, envir, enclos)
 8: eval(call, parent.frame())
 9: update.nlme(fm10var.nlme, corr = corGaus(form = ~Time))
10: update(fm10var.nlme, corr = corGaus(form = ~Time))

The above was under:

R version 2.4.1 (2006-12-18)
nlme Version:   3.1-79
Linux 2.6.15-27-386 (Ubuntu 6.06)

Then I tried the same on another computer and got:
 *** caught segfault ***
address 0x82e4902a, cause 'memory not mapped'
Violaci?n de segmento

R Version 2.3.1 (2006-06-01)
nlme Version:       3.1-74
Linux 2.6.12-10-386  (Ubuntu 5.10)

I worked out the example of rats body weights, also in the book, and it
gave no error, so the problem seems to be in using the corLin and
corGaus functions with a nlme fit. Apparently is neither my version of R
and nlme, nor the dataset. Can someone try to reproduce this? Or does
someone knows the cause/explanation/fix? 

Thanks very much 

JR Ferrer-Paris

-- 
Dipl.-Biol. JR Ferrer Paris
~~~~~~~~~~~~~~~~~~~~~~~~~~~
Laboratorio de Biolog?a de Organismos --- Centro de Ecolog?a
Instituto Venezolano de Investigaciones Cient?ficas (IVIC) 
Apdo. 21827, Caracas 1020-A 
Rep?blica Bolivariana de Venezuela

Tel: (+58-212) 504-1452
Fax: (+58-212) 504-1088

email: jferrer at ivic.ve
clave-gpg: 2C260A95


From klaster at karlin.mff.cuni.cz  Tue Mar 13 21:06:59 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Tue, 13 Mar 2007 21:06:59 +0100
Subject: [R] Adding bars to the right of existing ones using barplot
In-Reply-To: <CC871968-A9E2-4E96-95B6-56A2FA3CDED3@bu.edu>
References: <CC871968-A9E2-4E96-95B6-56A2FA3CDED3@bu.edu>
Message-ID: <45F70463.90305@karlin.mff.cuni.cz>

Add/subtract a small constant to/from your latter dataset (x-values) and 
plot it without axes.

Petr

Jason Horn napsal(a):
> I'm trying to create a barplot that has two sets of data next to each  
> other.  I'm using barplot with the add=TRUE option, but this simply  
> adds the second dataset on top of the first, obscuring it.  How do I  
> add the new data to the right on the existing barplot so that both  
> sets are visible?  I've been playing with all sorts of option and  
> reading the list archives with no luck.
> 
> Thank you anyone who has the answer.
> 
> - Jason
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From Graham.Williams at togaware.com  Tue Mar 13 21:16:47 2007
From: Graham.Williams at togaware.com (Graham Williams)
Date: Wed, 14 Mar 2007 07:16:47 +1100
Subject: [R] Rattle() GGobi()-Plots- How to save?
In-Reply-To: <b4485c4c0703130753t6c2e40d4h65f5358ca8b136ea@mail.gmail.com>
References: <b4485c4c0703130140n315e5e77p3f0a76f25ac9d951@mail.gmail.com>
	<22038.212.209.13.15.1173789582.squirrel@www.sorch.se>
	<b4485c4c0703130753t6c2e40d4h65f5358ca8b136ea@mail.gmail.com>
Message-ID: <20070313201647.GB9798@athene.togaware.com>

Received Wed 14 Mar 2007  1:53am +1100 from j.joshua thomas:
> i tried with package DescribeDisplay
> 
> dd_load(DescribeDisplay)
>                        Load describe display
> 
> i couldnt find any documentation. could you give me a sample

Try 

> install.packages("DescribeDisplay")
> library(DescribeDisplay)

Then, within GGobi choose from the Tools menu to Save Display
Description. This will prompt you for a filename into which GGobi will
write an R script to recreate the current graphic. We can load this
script into R and plot:

> pd <- dd_load("ggobi-saved-display-description.R")
> plot(pd)
> ggplot(pd)

I've updated the section on GGobi in the Rattle book to reflect
this. See http://datamining.togaware.com/survivor/index.html (or
specifically
http://datamining.togaware.com/survivor/GGobi_Option.html)

Regards,
Graham


From skiadas at hanover.edu  Tue Mar 13 21:25:15 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Tue, 13 Mar 2007 16:25:15 -0400
Subject: [R] Adding bars to the right of existing ones using barplot
In-Reply-To: <CC871968-A9E2-4E96-95B6-56A2FA3CDED3@bu.edu>
References: <CC871968-A9E2-4E96-95B6-56A2FA3CDED3@bu.edu>
Message-ID: <1783DEC3-1455-4EB0-894D-792EEC1D981B@hanover.edu>


On Mar 13, 2007, at 3:34 PM, Jason Horn wrote:

> I'm trying to create a barplot that has two sets of data next to each
> other.  I'm using barplot with the add=TRUE option, but this simply
> adds the second dataset on top of the first, obscuring it.  How do I
> add the new data to the right on the existing barplot so that both
> sets are visible?  I've been playing with all sorts of option and
> reading the list archives with no luck.

Something like this?

a<-rnorm(3,2,1)
b<-rnorm(3,2,1)
barplot(rbind(a,b), beside=TRUE)


?barplot for more options ( in particular, the height argument)


> Thank you anyone who has the answer.
>
> - Jason

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From rglor at mail.rochester.edu  Tue Mar 13 21:28:24 2007
From: rglor at mail.rochester.edu (Richard E. Glor)
Date: Tue, 13 Mar 2007 16:28:24 -0400
Subject: [R] writing new varFunc class
Message-ID: <54EA692E-BAD9-40CC-A376-9AD68D95C7A9@mail.rochester.edu>

I'm trying to use nonlinear regression to estimate model parameters  
for a constant rate birth-death process conditioned on the fact that  
the population has not gone extinct.  Because the birth and death  
parameters obtained from standard application of gnls() appear to be  
biased, I'd like to write a new varFunc class for gnls() that  
accommodates the expected increase in variance with time.  The help  
documentation for varClasses notes that "Users may define their own  
varFunc classes by specifying a constructor function and, at a  
minimum, methods for the functions coef, coef<-, and initialize. For  
examples of these functions, see the methods for class varPower.",  
but I did not find the content offered in varPower to be particularly  
helpful in this regard.  If anybody has gone through the process of  
creating a new varFunc I'd love to hear from you.  The variance  
function I'd like to construct is from Bailey 1964:

(m*exp((lambda-mu)*t)*((lambda/mu)+1)*(exp((lambda-mu)*t)-1)/((lambda/ 
mu)-1)

, where m is the starting number of individuals, lambda is the birth  
rate, mu the death rate and t the time since the population's origin.

Cheers,
Rich


From mikejjasper at gmail.com  Tue Mar 13 21:35:01 2007
From: mikejjasper at gmail.com (Mike Jasper)
Date: Tue, 13 Mar 2007 16:35:01 -0400
Subject: [R] selecting rows with more than x occurrences in a given
	column(data type is names)
In-Reply-To: <00a601c76580$a5925b40$0540210a@www.domain>
References: <9b3e85660703130738k5f8bf718uc16a530eddf1bf86@mail.gmail.com>
	<00a601c76580$a5925b40$0540210a@www.domain>
Message-ID: <9b3e85660703131335g35521cdt837d877494bc22a7@mail.gmail.com>

Thanks to all of you who got me the answer. The key I was missing was
%in%. Had never seen it before.

best.

On 3/13/07, Dimitris Rizopoulos <dimitris.rizopoulos at med.kuleuven.be> wrote:
> try this:
>
> set.seed(123)
> all.data <- data.frame(name = sample(c("Joe", "Elen", "Jane", "Mike"),
> 8, TRUE),
>     x = rnorm(8), y = runif(8))
> ##########
> tab.nams <- table(all.data$name)
> nams <- names(tab.nams[tab.nams >= 2])
> all.data[all.data$name %in% nams, ]
>
>
> I hope it helps.
>
> Best,
> Dimitris
>
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
>
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/(0)16/336899
> Fax: +32/(0)16/337015
> Web: http://med.kuleuven.be/biostat/
>      http://www.student.kuleuven.be/~m0390867/dimitris.htm
>
>
> ----- Original Message -----
> From: "Mike Jasper" <mikejjasper at gmail.com>
> To: <r-help at stat.math.ethz.ch>
> Sent: Tuesday, March 13, 2007 3:38 PM
> Subject: [R] selecting rows with more than x occurrences in a given
> column(data type is names)
>
>
> > Despite a long search on the archives, I couldn't find how to do
> > this.
> > Thanks in advance for what is likely a simple issue.
> >
> > I have a data set where the first column is name (i.e., 'Joe Smith',
> > 'Jane Doe', etc). The following columns are data associated with
> > that
> > person. I have many people with multiple rows. What I want is to get
> > a
> > new data frame out with only the people who have more than x
> > occurrences in the first column.
> >
> > Here's what I've done, that's not working:
> >
> > Let's call my old data.frame "all.data"
> >
> > table(all.data$names)>10
> >
> > I get a list of names and TRUE/FALSE values. I then want to make a
> > list of the TRUEs and pass that to some subset type command like
> >
> > dup.names=table(all.data$names)>10
> >
> > new.data=(all.data[all.data$names==dup.names,])
> >
> > That's not working because the dimensions are wrong (I think). But
> > even when I tried to do part of it manually (to troubleshoot) like
> > this
> >
> > dup.names=c('Joe Smith','Jane Doe','etc')
> >
> > I got warnings and it didn't work correctly. There must be a simple
> > way to do this that I'm just not seeing. Thanks.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
>


From p.dalgaard at biostat.ku.dk  Tue Mar 13 21:35:49 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 13 Mar 2007 21:35:49 +0100
Subject: [R] Freeman-Tukey arcsine transformation
In-Reply-To: <45F6FBD4.4080308@biostat.ku.dk>
References: <D8C95B444AD6EE4AAD638D818A9CFD343A1FC8@RINNYCSE000.rth.ad.rothschild.com>
	<45F6FBD4.4080308@biostat.ku.dk>
Message-ID: <45F70B25.3010301@biostat.ku.dk>

Peter Dalgaard wrote:
> Bos, Roger wrote:
>   
>> I'm curious what this transformation does, but I am not curious enough to pay $14 to find out.  Someone once told me that the arcsine was a good way to transform data and make it more 'normal'.  I am wondering if this is an improved method.  Anyone know of a free reference?
>>
>>  
>>   
>>     
> Well, a paper copy of the American Statistician (1978) would be free in 
> some sense....
>
> In the meantime I got detached from JSTOR (i.e., I went home), and I'm 
> not prepared to jump through the relevant hoops for remote access at 
> this point, but AFAIR it was a relatively trivial version of the simple 
> arcsine transform, something like replacing asin(r/n) with the average 
> of asin(r/(n+1)) and asin((r+1)/(n+1)). The point of the paper is that 
> you can invert explicitly for r/n if n is known.
>
>   
Well, except for a couple of sqrt() it seems....
>>             /The American Statistician/, Vol. 32, No. 4. (Nov., 1978),
>>             p. 138. 
>>
>>
>>             Stable URL:
>>             http://links.jstor.org/sici?sici=0003-1305%28197811%2932%3A4%3C138%3ATIOTFD%3E2.0.CO%3B2-Z
>>
>>
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Inman.Brant at mayo.edu  Tue Mar 13 21:38:37 2007
From: Inman.Brant at mayo.edu (Inman, Brant A.   M.D.)
Date: Tue, 13 Mar 2007 15:38:37 -0500
Subject: [R] Freeman-Tukey arcsine transformation
In-Reply-To: <45F70B25.3010301@biostat.ku.dk>
References: <D8C95B444AD6EE4AAD638D818A9CFD343A1FC8@RINNYCSE000.rth.ad.rothschild.com>
	<45F6FBD4.4080308@biostat.ku.dk> <45F70B25.3010301@biostat.ku.dk>
Message-ID: <6021CA6EF4C8374084D4F5A141F1CBBB664BF8@msgebe23.mfad.mfroot.org>


The point of the given transformation is not so much for normality as it
is for variance stabilization.  The variance of the Freeman-Tukey
transform depends only on the denominator of the proportion in
question...something that can be used to advantage. 


Brant

-----Original Message-----
From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
Sent: Tuesday, March 13, 2007 3:36 PM
To: Bos, Roger
Cc: Inman, Brant A. M.D.; r-help at stat.math.ethz.ch
Subject: Re: [R] Freeman-Tukey arcsine transformation

Peter Dalgaard wrote:
> Bos, Roger wrote:
>   
>> I'm curious what this transformation does, but I am not curious
enough to pay $14 to find out.  Someone once told me that the arcsine
was a good way to transform data and make it more 'normal'.  I am
wondering if this is an improved method.  Anyone know of a free
reference?
>>
>>  
>>   
>>     
> Well, a paper copy of the American Statistician (1978) would be free
in 
> some sense....
>
> In the meantime I got detached from JSTOR (i.e., I went home), and I'm

> not prepared to jump through the relevant hoops for remote access at 
> this point, but AFAIR it was a relatively trivial version of the
simple 
> arcsine transform, something like replacing asin(r/n) with the average

> of asin(r/(n+1)) and asin((r+1)/(n+1)). The point of the paper is that

> you can invert explicitly for r/n if n is known.
>
>   
Well, except for a couple of sqrt() it seems....
>>             /The American Statistician/, Vol. 32, No. 4. (Nov.,
1978),
>>             p. 138. 
>>
>>
>>             Stable URL:
>>
http://links.jstor.org/sici?sici=0003-1305%28197811%2932%3A4%3C138%3ATIO
TFD%3E2.0.CO%3B2-Z
>>
>>
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mnair at iusb.edu  Tue Mar 13 21:45:28 2007
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Tue, 13 Mar 2007 16:45:28 -0400
Subject: [R] Rcmd and memory
Message-ID: <A32055BDEA88C34BB3DBBCD229380778E653E3@iu-mssg-mbx109.ads.iu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070313/8b241790/attachment.pl 

From murdoch at stats.uwo.ca  Tue Mar 13 22:23:49 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 13 Mar 2007 17:23:49 -0400
Subject: [R] inconsistent behaviour of add1 and drop1 with a weighted
 linear model
In-Reply-To: <f8e6ff050703131214gb9fe64es221aa7241d2f169e@mail.gmail.com>
References: <45F6C69E.10200@york.ac.uk>
	<45F6D2BB.6020903@biostat.ku.dk>	<45F6D9A1.8020101@york.ac.uk>
	<45F6F3EC.3010908@biostat.ku.dk>
	<f8e6ff050703131214gb9fe64es221aa7241d2f169e@mail.gmail.com>
Message-ID: <45F71665.6020201@stats.uwo.ca>

On 3/13/2007 3:14 PM, hadley wickham wrote:
> On 3/13/07, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
>> Jenny Hodgson wrote:
>>> I was using Version 2.3.1 for Windows (binary download version). Guess I
>>> should be using the latest version, (but the reason is I'm writing up my
>>> PhD and I thought my results would be more 'repeatable' if I didn't keep
>>> changing my version of the software, I didn't really think there would
>>> be any glitches as big as this). Sorry if this is a waste of your time.
>>> And thanks very much for replying so quickly.
>>>
>>> Jenny
>>>
>>>
>> Always a good idea to check the NEWS file:
>>
>>     o   add1.lm() had been broken by other changes for weighted fits.
> 
> Yes, but a little impractical.  Has any one ever considered a standard
> for the news file so changes could be extracted and included in online
> documentation somewhere?

It's in a structured format, and is online.  The readNEWS() function can
read it.  It would be nice if someone would contribute a more friendly
reader...

Try

readNEWS(url("http://cran.r-project.org/bin/windows/base/NEWS.R-2.5.0dev"),
chop="keepAll")[[c("2.5","2.5.0","BUG FIXES")]]

for the unfriendly but informative version.

Duncan Murdoch


From marc_schwartz at comcast.net  Tue Mar 13 23:04:10 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 13 Mar 2007 17:04:10 -0500
Subject: [R] Adding bars to the right of existing ones using barplot
In-Reply-To: <CC871968-A9E2-4E96-95B6-56A2FA3CDED3@bu.edu>
References: <CC871968-A9E2-4E96-95B6-56A2FA3CDED3@bu.edu>
Message-ID: <1173823450.4315.74.camel@localhost.localdomain>

On Tue, 2007-03-13 at 15:34 -0400, Jason Horn wrote:
> I'm trying to create a barplot that has two sets of data next to each  
> other.  I'm using barplot with the add=TRUE option, but this simply  
> adds the second dataset on top of the first, obscuring it.  How do I  
> add the new data to the right on the existing barplot so that both  
> sets are visible?  I've been playing with all sorts of option and  
> reading the list archives with no luck.
> 
> Thank you anyone who has the answer.
> 
> - Jason

You might want to take note of the 'space' argument in ?barplot:

 a <- 1:3
 b <- 4:6

 # concatenate the two vectors and specify
 # that the space before the 4th bar should be
 # wider

 barplot(c(a, b), space = c(1, 1, 1, 3, 1, 1))

Also take note that barplot() returns the bar midpoints, which can be
helpful if you want to add any additional annotation to the plot.  See
the examples in ?barplot.

HTH,

Marc Schwartz


From mdsumner at utas.edu.au  Tue Mar 13 23:09:55 2007
From: mdsumner at utas.edu.au (Michael Sumner)
Date: Wed, 14 Mar 2007 09:09:55 +1100
Subject: [R] reading BMP into R
Message-ID: <45F72133.9040700@utas.edu.au>

Another way:

require(sp)
require(rgdal)

library(rgdal)

## read using one of the many GDAL drivers
d <- readGDAL("im.bmp")
summary(d)

## map colours for RGB and display
col <- SGDF2PCT(d)
d$idx <- col$idx
image(d, "idx", col = col$ct)

## Available drivers:
gdalDrivers()



Milton Cezar Ribeiro wrote:
 >/ Hi R-gurus
/>/ />/ How can I read my "bmp" files into R?
/>/ />/ Kind regards,
/>/ />/ />/ miltinho
/>/ Brazil /
 >/ /


From estrain at postoffice.utas.edu.au  Tue Mar 13 23:14:20 2007
From: estrain at postoffice.utas.edu.au (estrain at postoffice.utas.edu.au)
Date: Wed, 14 Mar 2007 8:14:20 +1000
Subject: [R] lme4 and mcmcamp
Message-ID: <200703132214.l2DMEKF3008492@corinna.its.utas.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070314/ca780597/attachment.pl 

From g.abraham at ms.unimelb.edu.au  Tue Mar 13 23:37:15 2007
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Wed, 14 Mar 2007 09:37:15 +1100
Subject: [R] multiplying matrix by vector of times
In-Reply-To: <C21C3A07.C62%lhill07@qub.ac.uk>
References: <C21C3A07.C62%lhill07@qub.ac.uk>
Message-ID: <45F7279B.4040100@ms.unimelb.edu.au>

> Thanks for the tip about the empty vector, I ever knew you could do that. I
> just have one problem,
> 
> Lets say Q is a 2x2 matrix
>          p is a 1x2 matrix
>          q is a 2x1 matrix
>          y is vector of times, say y = c(5, 10)
> 
> How do I multiply Q by each time y[i]?
> 
> I would like to get the answer to the equation
> 
> loglik[i] <- log(p %*% expm(Q * y[i]) %*% q)
> 
> Where first y=5 and then y=10 so that the answers to loglik for each i are
> put into the empty vector.
> 
> I'm sure that I am missing something fairly obvious here but can't put my
> finger on it.

That's just what the code in my last message is doing:
iterating over the y vector, multiplying Q by each y[i], and storing the 
result in a different cell of loglik.

So you're not multiplying Q by a vector y, but Q by a scalar y[i].

Have a look at the R Introduction at 
http://cran.r-project.org/doc/manuals/R-intro.html,
especially section 2.

-- 
Gad Abraham
Department of Mathematics and Statistics
The University of Melbourne
Parkville 3010, Victoria, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From krc at mdacc.tmc.edu  Wed Mar 14 00:02:52 2007
From: krc at mdacc.tmc.edu (Kevin R. Coombes)
Date: Tue, 13 Mar 2007 18:02:52 -0500
Subject: [R] Sweave question: prevent expansion of unevaluated reused code
	chunk
Message-ID: <45F72D9C.5000307@mdacc.tmc.edu>

Hi,

Consider the following (much simplified) Sweave example:

--------------

First, we set the value of $x$:
<<chunk1,eval=FALSE>>=
x <- 1
@

Then we set the value of $y$:
<<chunk2,eval=FALSE>>=
y <- 2
@

Thus, the overall algorithm has this structure:
<<combined,eval=FALSE>>=
<<chunk1>>
<<chunk2>>
@

<<justDoIt,echo=FALSE>>=
<<combined>>
@

---------------

I'd like to be able to do something like this, where the "combined" 
chunk prints out in the final LaTeX document essentially verbatim.  In 
particular, I want to see the "<<chunk1>>" unexpanded in that block, 
since this gives me a nice conceptual overview of the algorithm. (Of 
courser, this is more useful when chunk1 and chunk2 are much longer than 
they are in this example....)

Is there an option that allows me to get this behavior?

Thanks,
	Kevin


From kate.stark at utas.edu.au  Wed Mar 14 00:19:37 2007
From: kate.stark at utas.edu.au (Kate Stark)
Date: Wed, 14 Mar 2007 10:19:37 +1100
Subject: [R] AR(1) and gls
Message-ID: <200703132319.l2DNJZnD017429@corinna.its.utas.edu.au>

Hi there,

I am using gls from the nlme library to fit an AR(1) regression model.
 
I am wondering if (and how) I can separate the auto-correlated and random
components of the residuals? Id like to be able to plot the fitted values +
the autocorrelated error (i.e. phi * resid(t-1)), to compare with the
observed values.

I am also wondering how I might go about calculating confidence (or
prediction) intervals around these "new" fitted values (i.e. fitted "new" =
fitted + autocorrelated error component)?

Thanks in advance,

Kate

======================================
Kate Stark  |  PhD candidate
Institute of Antarctic & Southern Ocean Studies &
Tasmanian Aquaculture & Fisheries Institute
University of Tasmania.


From jholtman at gmail.com  Wed Mar 14 00:21:54 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 13 Mar 2007 18:21:54 -0500
Subject: [R] Rcmd and memory
In-Reply-To: <A32055BDEA88C34BB3DBBCD229380778E653E3@iu-mssg-mbx109.ads.iu.edu>
References: <A32055BDEA88C34BB3DBBCD229380778E653E3@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <644e1f320703131621t17ed2cceh33287103a3be74b0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070313/610c80b7/attachment.pl 

From lusalasar at yahoo.com.br  Wed Mar 14 00:25:54 2007
From: lusalasar at yahoo.com.br (Luis Salasar)
Date: Tue, 13 Mar 2007 20:25:54 -0300 (ART)
Subject: [R] Poisson rate test
Message-ID: <86040.75578.qm@web54013.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070313/48c3f7f4/attachment.pl 

From h.wickham at gmail.com  Wed Mar 14 00:46:39 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 13 Mar 2007 18:46:39 -0500
Subject: [R] inconsistent behaviour of add1 and drop1 with a weighted
	linear model
In-Reply-To: <45F71142.4080109@stats.uwo.ca>
References: <45F6C69E.10200@york.ac.uk> <45F6D2BB.6020903@biostat.ku.dk>
	<45F6D9A1.8020101@york.ac.uk> <45F6F3EC.3010908@biostat.ku.dk>
	<f8e6ff050703131214gb9fe64es221aa7241d2f169e@mail.gmail.com>
	<45F71142.4080109@stats.uwo.ca>
Message-ID: <f8e6ff050703131646x3777af9ew7ac61066384f2d50@mail.gmail.com>

On 3/13/07, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> On 3/13/2007 3:14 PM, hadley wickham wrote:
> > On 3/13/07, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> >> Jenny Hodgson wrote:
> >>> I was using Version 2.3.1 for Windows (binary download version). Guess I
> >>> should be using the latest version, (but the reason is I'm writing up my
> >>> PhD and I thought my results would be more 'repeatable' if I didn't keep
> >>> changing my version of the software, I didn't really think there would
> >>> be any glitches as big as this). Sorry if this is a waste of your time.
> >>> And thanks very much for replying so quickly.
> >>>
> >>> Jenny
> >>>
> >>>
> >> Always a good idea to check the NEWS file:
> >>
> >>     o   add1.lm() had been broken by other changes for weighted fits.
> >
> > Yes, but a little impractical.  Has any one ever considered a standard
> > for the news file so changes could be extracted and included in online
> > documentation somewhere?
>
> It's in a structured format, and is online.  The readNEWS() function can
> read it.  It would be nice if someone would contribute a more friendly
> reader...
>
> Try
>
> readNEWS(url("http://cran.r-project.org/bin/windows/base/NEWS.R-2.5.0dev"),
> chop="keepAll")[[c("2.5","2.5.0","BUG FIXES")]]
>
> for the unfriendly but informative version.

Oh, that's neat.  One thing that would be very useful would be to
figure out what (if any) functions an news item refers to.  A grep
against all function names in base packages wouldn't be too hard to
do.

I'll think about it, and how to render it nicely in a webpage.

Hadley


From sfalcon at fhcrc.org  Wed Mar 14 00:56:01 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 13 Mar 2007 16:56:01 -0700
Subject: [R] Sweave question: prevent expansion of unevaluated reused
	code chunk
In-Reply-To: <45F72D9C.5000307@mdacc.tmc.edu> (Kevin R. Coombes's message of
	"Tue, 13 Mar 2007 18:02:52 -0500")
References: <45F72D9C.5000307@mdacc.tmc.edu>
Message-ID: <m2slc8itpq.fsf@ziti.local>

"Kevin R. Coombes" <krc at mdacc.tmc.edu> writes:

> Hi,
>
> Consider the following (much simplified) Sweave example:
>
> --------------
>
> First, we set the value of $x$:
> <<chunk1,eval=FALSE>>=
> x <- 1
> @
>
> Then we set the value of $y$:
> <<chunk2,eval=FALSE>>=
> y <- 2
> @
>
> Thus, the overall algorithm has this structure:
> <<combined,eval=FALSE>>=
> <<chunk1>>
> <<chunk2>>
> @
>
> <<justDoIt,echo=FALSE>>=
> <<combined>>
> @
>
> ---------------
>
> I'd like to be able to do something like this, where the "combined" 
> chunk prints out in the final LaTeX document essentially verbatim.  In 
> particular, I want to see the "<<chunk1>>" unexpanded in that block, 
> since this gives me a nice conceptual overview of the algorithm. (Of 
> courser, this is more useful when chunk1 and chunk2 are much longer than 
> they are in this example....)
>
> Is there an option that allows me to get this behavior?

Maybe I'm not understanding what it is you want, but why not:

\begin{verbatim}
<<chunk1>>
<<chunk2>>
\end{verbatim}

What does putting this in an unevaluated chunk buy you?  The
<<chunkName>> markers are an internal detail of the document and so
must of the time these never appear in the rendered output.  Even in
your example, won't it be confusing that <<chunk1>> and <<chunk2>>
won't have appeared in labels earlier in the rendered document?

+ seth

-- 
Seth Falcon | Computational Biology | Fred Hutchinson Cancer Research Center
http://bioconductor.org


From g.abraham at ms.unimelb.edu.au  Wed Mar 14 01:02:14 2007
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Wed, 14 Mar 2007 11:02:14 +1100
Subject: [R] AR(1) and gls
In-Reply-To: <200703132319.l2DNJZnD017429@corinna.its.utas.edu.au>
References: <200703132319.l2DNJZnD017429@corinna.its.utas.edu.au>
Message-ID: <45F73B86.1020104@ms.unimelb.edu.au>

Kate Stark wrote:
> Hi there,
> 
> I am using gls from the nlme library to fit an AR(1) regression model.
>  
> I am wondering if (and how) I can separate the auto-correlated and random
> components of the residuals? Id like to be able to plot the fitted values +
> the autocorrelated error (i.e. phi * resid(t-1)), to compare with the
> observed values.
> 
> I am also wondering how I might go about calculating confidence (or
> prediction) intervals around these "new" fitted values (i.e. fitted "new" =
> fitted + autocorrelated error component)?

Since no one else has answered, I'll put in my 2c.

Why not use the arima() function?

Once you've fitted the AR(1) to it, you can extract the coefficients and 
the residuals, and look at the ACF and PACF of the residuals to see 
whether they are autocorrelated or not. tsdiag() is also useful. If an 
AR(1) captures all of the autocorrelation in the data, then the 
residuals will be iid. arima() also gives you standard errors for each 
estimated coefficient.

To get a prediction from the model, you can use arima.sim(). One simple 
way to get prediction intervals about the point prediction is to sample 
by calling arima.sim() repeatedly and then calculate quantiles.

-- 
Gad Abraham
Department of Mathematics and Statistics
The University of Melbourne
Parkville 3010, Victoria, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From murdoch at stats.uwo.ca  Wed Mar 14 01:08:26 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 13 Mar 2007 20:08:26 -0400
Subject: [R] inconsistent behaviour of add1 and drop1 with a weighted
 linear model
In-Reply-To: <f8e6ff050703131646x3777af9ew7ac61066384f2d50@mail.gmail.com>
References: <45F6C69E.10200@york.ac.uk> <45F6D2BB.6020903@biostat.ku.dk>	
	<45F6D9A1.8020101@york.ac.uk> <45F6F3EC.3010908@biostat.ku.dk>	
	<f8e6ff050703131214gb9fe64es221aa7241d2f169e@mail.gmail.com>	
	<45F71142.4080109@stats.uwo.ca>
	<f8e6ff050703131646x3777af9ew7ac61066384f2d50@mail.gmail.com>
Message-ID: <45F73CFA.30005@stats.uwo.ca>

On 3/13/2007 7:46 PM, hadley wickham wrote:
> On 3/13/07, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
>> On 3/13/2007 3:14 PM, hadley wickham wrote:
>>> On 3/13/07, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
>>>> Jenny Hodgson wrote:
>>>>> I was using Version 2.3.1 for Windows (binary download version). Guess I
>>>>> should be using the latest version, (but the reason is I'm writing up my
>>>>> PhD and I thought my results would be more 'repeatable' if I didn't keep
>>>>> changing my version of the software, I didn't really think there would
>>>>> be any glitches as big as this). Sorry if this is a waste of your time.
>>>>> And thanks very much for replying so quickly.
>>>>>
>>>>> Jenny
>>>>>
>>>>>
>>>> Always a good idea to check the NEWS file:
>>>>
>>>>     o   add1.lm() had been broken by other changes for weighted fits.
>>> Yes, but a little impractical.  Has any one ever considered a standard
>>> for the news file so changes could be extracted and included in online
>>> documentation somewhere?
>> It's in a structured format, and is online.  The readNEWS() function can
>> read it.  It would be nice if someone would contribute a more friendly
>> reader...
>>
>> Try
>>
>> readNEWS(url("http://cran.r-project.org/bin/windows/base/NEWS.R-2.5.0dev"),
>> chop="keepAll")[[c("2.5","2.5.0","BUG FIXES")]]
>>
>> for the unfriendly but informative version.
> 
> Oh, that's neat.  One thing that would be very useful would be to
> figure out what (if any) functions an news item refers to.  A grep
> against all function names in base packages wouldn't be too hard to
> do.

I think we are reasonably consistent in referring to functions with 
parens following, e.g. add1.lm() above, so a search for patterns like 
that should find most of them.

Duncan Murdoch

> 
> I'll think about it, and how to render it nicely in a webpage.
> 
> Hadley


From krc at mdacc.tmc.edu  Wed Mar 14 02:18:11 2007
From: krc at mdacc.tmc.edu (Kevin R. Coombes)
Date: Tue, 13 Mar 2007 20:18:11 -0500
Subject: [R] Sweave question: prevent expansion of unevaluated reused
 code chunk
In-Reply-To: <m2slc8itpq.fsf@ziti.local>
References: <45F72D9C.5000307@mdacc.tmc.edu> <m2slc8itpq.fsf@ziti.local>
Message-ID: <45F74D53.7040706@mdacc.tmc.edu>

[1] In the example I presented, you're right; I can just use a verbatim 
environment. In the following more complicated example

<<combined,eval=FALSE>
if (some.condition) {
<<chunk1>>
} else {
<<chunk2>>
}
@

I would still want to be able to see the outline of what is going on, 
and a verbatim environment wouldn't work.

[2] You are also correct that there is no advantage if I just call them 
"chunk1" and "chunk2". But if I call them something more interesting, 
like "perform.quantile.normalization" or "truncate.and.log.transform",
then I can use this structure to explain the algorithm at a higher 
level.  If you go back to Knuth's original literate programming 
examples, this is exactly how he presents his examples. For instance, on 
page 104 of the "Literate Programming" book, he has

<Program to print the first thousand prime numbers 2> =
program print_primes(output);
   const m = 1000; <other constants of the program 5>
   var <Variables of the program 4>
     begin <Print the first m prime numbers 3>;
     end.

Of course, he's writing pascal instead of R. But the stuff between <> 
are the program chunks, and he is displaying them without expanding the 
chunks. And, as a result, You get to see a high-level picture of the 
algorithm, keeping the details somewhere else.

Since I'm looking at the results of "weave" in Knuth's example, I don't 
know if he is using a "verbatim" environment or is automatically 
generating this using his implementation of weave for teX and pascal....

But that's the idea.

	Kevin

Seth Falcon wrote:
> "Kevin R. Coombes" <krc at mdacc.tmc.edu> writes:
> 
>> Hi,
>>
>> Consider the following (much simplified) Sweave example:
>>
>> --------------
>>
>> First, we set the value of $x$:
>> <<chunk1,eval=FALSE>>=
>> x <- 1
>> @
>>
>> Then we set the value of $y$:
>> <<chunk2,eval=FALSE>>=
>> y <- 2
>> @
>>
>> Thus, the overall algorithm has this structure:
>> <<combined,eval=FALSE>>=
>> <<chunk1>>
>> <<chunk2>>
>> @
>>
>> <<justDoIt,echo=FALSE>>=
>> <<combined>>
>> @
>>
>> ---------------
>>
>> I'd like to be able to do something like this, where the "combined" 
>> chunk prints out in the final LaTeX document essentially verbatim.  In 
>> particular, I want to see the "<<chunk1>>" unexpanded in that block, 
>> since this gives me a nice conceptual overview of the algorithm. (Of 
>> courser, this is more useful when chunk1 and chunk2 are much longer than 
>> they are in this example....)
>>
>> Is there an option that allows me to get this behavior?
> 
> Maybe I'm not understanding what it is you want, but why not:
> 
> \begin{verbatim}
> <<chunk1>>
> <<chunk2>>
> \end{verbatim}
> 
> What does putting this in an unevaluated chunk buy you?  The
> <<chunkName>> markers are an internal detail of the document and so
> must of the time these never appear in the rendered output.  Even in
> your example, won't it be confusing that <<chunk1>> and <<chunk2>>
> won't have appeared in labels earlier in the rendered document?
> 
> + seth
>


From statba at nus.edu.sg  Wed Mar 14 04:47:29 2007
From: statba at nus.edu.sg (Berwin A Turlach)
Date: Wed, 14 Mar 2007 11:47:29 +0800
Subject: [R] Sweave question: prevent expansion of unevaluated reused
 code chunk
In-Reply-To: <45F74D53.7040706@mdacc.tmc.edu>
References: <45F72D9C.5000307@mdacc.tmc.edu> <m2slc8itpq.fsf@ziti.local>
	<45F74D53.7040706@mdacc.tmc.edu>
Message-ID: <20070314114729.5da2a98c@berwin5>

G'day Kevin,

On Tue, 13 Mar 2007 20:18:11 -0500
"Kevin R. Coombes" <krc at mdacc.tmc.edu> wrote:

> [1] In the example I presented, you're right; I can just use a
> verbatim environment. In the following more complicated example [...]
> I would still want to be able to see the outline of what is going on, 
> and a verbatim environment wouldn't work.

If you want to have a literate programming environment in R, in
particular one where you can refer to code chunks that are only defined
later in the document, then I would investigate the package relax by
Peter Wolf.  See:

	http://www.wiwi.uni-bielefeld.de/~wolf/software/relax/relax.html

And, for an example output just like you what you want:

	http://www.wiwi.uni-bielefeld.de/~wolf/software/relax/hello-world.pdf

HTH.

Cheers,

	Berwin

=========================== Full address =============================
Berwin A Turlach                            Tel.: +65 6515 4416 (secr)        
Dept of Statistics and Applied Probability        +65 6515 6650 (self)        
Faculty of Science                          FAX : +65 6872 3919               
National University of Singapore     
6 Science Drive 2, Blk S16, Level 7          e-mail: statba at nus.edu.sg
Singapore 117546                    http://www.stat.nus.edu.sg/~statba


From Hong.Ooi at iag.com.au  Wed Mar 14 04:57:45 2007
From: Hong.Ooi at iag.com.au (Hong Ooi)
Date: Wed, 14 Mar 2007 14:57:45 +1100
Subject: [R] Model matrix with redundant columns included
Message-ID: <200703140354.l2E3sMnP009572@hypatia.math.ethz.ch>


_______________________________________________________________________________________


Hello,

Normally when you call model.matrix, you get a matrix that has
aliased/redundant columns deleted. For example:

> m <- expand.grid(a=factor(1:3), b=factor(1:3))
> model.matrix(~a + b, m)
  (Intercept) a2 a3 b2 b3
1           1  0  0  0  0
2           1  1  0  0  0
3           1  0  1  0  0
4           1  0  0  1  0
5           1  1  0  1  0
6           1  0  1  1  0
7           1  0  0  0  1
8           1  1  0  0  1
9           1  0  1  0  1
attr(,"assign")
[1] 0 1 1 2 2
attr(,"contrasts")
attr(,"contrasts")$a
[1] "contr.treatment"

attr(,"contrasts")$b
[1] "contr.treatment"

The result is a matrix with 5 columns including the intercept.

However, for my purposes I need a matrix that includes all columns,
including those that would normally be redundant. Is there any way to do
this? For the example, this would be something like

  a1 a2 a3 b1 b2 b3
1  1  0  0  1  0  0
2  0  1  0  1  0  0
3  0  0  1  1  0  0
4  1  0  0  0  1  0
5  0  1  0  0  1  0
6  0  0  1  0  1  0
7  1  0  0  0  0  1
8  0  1  0  0  0  1
9  0  0  1  0  0  1

Including -1 as part of the model formula removes the intercept and adds
the column for the base level of the first variable, but not the rest.

Thanks,


-- 
Hong Ooi
Senior Research Analyst, IAG Limited
388 George St, Sydney NSW 2000
+61 (2) 9292 1566

_______________________________________________________________________________________

The information transmitted in this message and its attachme...{{dropped}}


From marc_schwartz at comcast.net  Wed Mar 14 05:47:06 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 13 Mar 2007 23:47:06 -0500
Subject: [R] Model matrix with redundant columns included
In-Reply-To: <200703140354.l2E3sMnP009572@hypatia.math.ethz.ch>
References: <200703140354.l2E3sMnP009572@hypatia.math.ethz.ch>
Message-ID: <1173847626.4893.3.camel@localhost.localdomain>

On Wed, 2007-03-14 at 14:57 +1100, Hong Ooi wrote:

> Hello,
> 
> Normally when you call model.matrix, you get a matrix that has
> aliased/redundant columns deleted. For example:
> 
> > m <- expand.grid(a=factor(1:3), b=factor(1:3))
> > model.matrix(~a + b, m)
>   (Intercept) a2 a3 b2 b3
> 1           1  0  0  0  0
> 2           1  1  0  0  0
> 3           1  0  1  0  0
> 4           1  0  0  1  0
> 5           1  1  0  1  0
> 6           1  0  1  1  0
> 7           1  0  0  0  1
> 8           1  1  0  0  1
> 9           1  0  1  0  1
> attr(,"assign")
> [1] 0 1 1 2 2
> attr(,"contrasts")
> attr(,"contrasts")$a
> [1] "contr.treatment"
> 
> attr(,"contrasts")$b
> [1] "contr.treatment"
> 
> The result is a matrix with 5 columns including the intercept.
> 
> However, for my purposes I need a matrix that includes all columns,
> including those that would normally be redundant. Is there any way to do
> this? For the example, this would be something like
> 
>   a1 a2 a3 b1 b2 b3
> 1  1  0  0  1  0  0
> 2  0  1  0  1  0  0
> 3  0  0  1  1  0  0
> 4  1  0  0  0  1  0
> 5  0  1  0  0  1  0
> 6  0  0  1  0  1  0
> 7  1  0  0  0  0  1
> 8  0  1  0  0  0  1
> 9  0  0  1  0  0  1
> 
> Including -1 as part of the model formula removes the intercept and adds
> the column for the base level of the first variable, but not the rest.
> 
> Thanks,


There may be a better way, but this seems to work:

> m
  a b
1 1 1
2 2 1
3 3 1
4 1 2
5 2 2
6 3 2
7 1 3
8 2 3
9 3 3

MAT <- do.call("cbind", lapply(m, function(x) model.matrix(~ x - 1)))

> MAT
  x1 x2 x3 x1 x2 x3
1  1  0  0  1  0  0
2  0  1  0  1  0  0
3  0  0  1  1  0  0
4  1  0  0  0  1  0
5  0  1  0  0  1  0
6  0  0  1  0  1  0
7  1  0  0  0  0  1
8  0  1  0  0  0  1
9  0  0  1  0  0  1

colnames(MAT) <- names(unlist(lapply(m, levels)))
 
> MAT
  a1 a2 a3 b1 b2 b3
1  1  0  0  1  0  0
2  0  1  0  1  0  0
3  0  0  1  1  0  0
4  1  0  0  0  1  0
5  0  1  0  0  1  0
6  0  0  1  0  1  0
7  1  0  0  0  0  1
8  0  1  0  0  0  1
9  0  0  1  0  0  1


You can cbind() the (Intercept) column back in if you require.

HTH,

Marc Schwartz


From ray1728 at gmail.com  Wed Mar 14 06:27:42 2007
From: ray1728 at gmail.com (Ray Cheung)
Date: Wed, 14 Mar 2007 13:27:42 +0800
Subject: [R] about bootstrapping
Message-ID: <222938870703132227l1574220l75f104ce271699d6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070314/d4e5d081/attachment.pl 

From ecell_1111 at yahoo.co.in  Wed Mar 14 06:33:06 2007
From: ecell_1111 at yahoo.co.in (E Cell)
Date: Wed, 14 Mar 2007 05:33:06 +0000 (GMT)
Subject: [R] Need suggestion regarding analysis of high dimensional data
Message-ID: <79585.35120.qm@web8318.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070314/a7f9663b/attachment.pl 

From rodick at hotmail.com  Wed Mar 14 07:10:00 2007
From: rodick at hotmail.com (Robert Rodick)
Date: Wed, 14 Mar 2007 06:10:00 +0000
Subject: [R] aic for lrm
Message-ID: <BAY115-F51FECFE3F5CF5E485EB72D7730@phx.gbl>

I cannot seem to get the aic or extractaic call to work with multinomial 
logistic regression models.

Here is what I am doing:
library('Design')
lrm1<-lrm(r1~p1)
#where p1 is multinomial and r1 is binomial

library('MASS')
aic(lrm1)
Error in if (fam %in% c("gaussian", "Gamma", "inverse.gaussian")) p <- p +  
:
        argument is of length zero

also tried extract.aic and others to no avail
I was able to get aic to give what looked to be reasonable results with nnet 
functions like multinom -- am i missing basic statistics knowledge here or 
is there a problem with my code? thanks

_________________________________________________________________

Intro*Terms


From ivar.herfindal at bio.ntnu.no  Wed Mar 14 09:27:24 2007
From: ivar.herfindal at bio.ntnu.no (Ivar Herfindal)
Date: Wed, 14 Mar 2007 09:27:24 +0100
Subject: [R] lme4 and mcmcamp
In-Reply-To: <200703132214.l2DMEKF3008492@corinna.its.utas.edu.au>
References: <200703132214.l2DMEKF3008492@corinna.its.utas.edu.au>
Message-ID: <45F7B1EC.4010403@bio.ntnu.no>

Hello.

I ran into a similar situation as you did, and according to D. Bates, 
this can be due to the inclusion of the offset variable. It appears that 
the mcmcsamp does not work very well with offset variables. For more 
details, have a look at this thread:

https://stat.ethz.ch/pipermail/r-sig-mixed-models/2007q1/000061.html

I experimented a bit with xyplot on the mcmcsamp-object, and clearly, 
the procedure gets stuck at some points. This can explain why you get 
totally different results from mcmcsamp compared to the summary-command. 
Accordingly, it seems that the summary gives the most reliable results 
when a offset variable is included.

Ivar

estrain at postoffice.utas.edu.au wrote:

>Dear R users
>I am trying to obtain p-values for (quasi)poisson lmer models, using
>Markov-chain Monte Carlo sampling and the command summary.
>  
>
>>My problems is that p values derived from both these methods are
>>    
>>
>totally different. My question is
>(1) there a bug in my code and
>  
>
>
>(2) How can I proceed, left with these uncertainties in the estimations of
>  
>
>>the p-values?
>>
>>Below is the corresponding R code with some output:
>>##
>>fit<-lmer(End~Treatment+offset(log(Area))+(1|Site/Treatment), family=poisson
>>    
>>
>
>  
>
>>## Results
>>summary(fit)
>>    
>>
>
>AIC BIC loglik deviance
>28.92 35.99 -8.46 16.92
>Random effects
>
>Groups Name                    Variance    Std dev.
>Treatment * Site Intercept  5e-10        2.2361e-05
>Site                    Intercept  5e-10       2.2361e-05
>
>
>number of obs 24 groups Treatment*Site 8 and Site 2
>
>Fixed effects
>
>             Estimate   Std error  z value   P
>Intercept -3.8290   0.4995      -7.666   1.77e-14****
>Treatment 2 3.7970 0.505      7.516     5.51e-14****
>
>Treatment 3 0.2409 9.6704 0.359 0.719 Treatment 4 -0.2483 0.8661 -0.287 0.774
>
>Correlation of fixed effects
>
>    Intra T2 T3
>T2 -0.989
>T3 -0.745 0.737
>T4 -0.577 0.570 0.430  
>
>  
>
>>The p-values from mcmc are:
>>
>>    
>>
>mcmcpvalue<-function(samp)
>{
>std<-backsolve(chol(var(samp)),
>cbind(0,t(samp))-colMeans(samp),
>transpose=TRUE)
>sqdist<-colSums(std*std)
>sum(sqdist[-1]>sqdist[1]/nrow(samp)
>}
>
>fitSI<-mcmcsamp(fit,50000)
>library(coda)
>HPDinterval(fitSI)
>
>                   lower        upper
>
>
>Intercept -4.0778905 -3.1366836
>Treatment2 3.4455972 4.3196598
>Treatment 3 0.399302 1.287747
>Treatment 4 -1.7898933 -0.2980325
>log(Treatment*Site.(in)) -22.2198233 -19.7342530 
>log(Site.(In)) -28.7857601 -23.0952939
>
>mcmcpvalue(as.Matrix(fitSI[,1]))
>etc
>
>Intecept 0
>Treatment 2 0
>Treatment 3 0.0075
>Treatment 4 0.00758
>log(Treatment*Site.(in)) 0
>log(Site.(In)) 0
>
>Any help in explaining why these two results are completely different
>would be much appreciated. I have tried the example and read all of the posts.
>
>Cheers
>Beth
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>  
>


From ccleland at optonline.net  Wed Mar 14 09:39:48 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 14 Mar 2007 04:39:48 -0400
Subject: [R] about bootstrapping
In-Reply-To: <222938870703132227l1574220l75f104ce271699d6@mail.gmail.com>
References: <222938870703132227l1574220l75f104ce271699d6@mail.gmail.com>
Message-ID: <45F7B4D4.60308@optonline.net>

Ray Cheung wrote:
> Dear All,
> 
> I've a 10 by 5 data frame like this
> 
> set.seed(1001)
> a <- rnorm(10)
> b <- rnorm(10)
> c <- rnorm(10)
> d <- rnorm(10)
> e <- rnorm(10)
> A <- cbind(a,b,c,d,e)
> 
> Each row is one datum. I want to resample within A[,5]. Fit the regression
> lines lm(A[,5] ~ A[,1] + A[,2]) and lm(A[,5] ~ A[,3] + A[,4]) to each
> bootstrap sample. I don't know how to write the statistics part. Can anybody
> help me? Thank you very much.

  You might have a look at John Fox's Appendix to An R and S-PLUS
Companion to Applied Regression on this topic:

http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-bootstrapping.pdf

> Regards,
> 
> Ray
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code. 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From rbaer at atsu.edu  Wed Mar 14 09:53:00 2007
From: rbaer at atsu.edu (Robert Baer)
Date: Wed, 14 Mar 2007 03:53:00 -0500
Subject: [R] dataframe layout
Message-ID: <007001c76616$2b94e380$6601a8c0@altair>

Can someone remind me how to change the columns in df.a into a two column 
df.b that contains one column of data and another column of the original 
column headings as levels.

Example:
a=1:3
b=4:6
c=7:9
df.a=data.frame(a,b,c)

Should become in df.b:
dat   lev
1      a
2      a
3      a
4      b
5      b
6      b
7      c
8      c
9      c

Thanks.


From mswierniak at o2.pl  Wed Mar 14 10:00:22 2007
From: mswierniak at o2.pl (jastar)
Date: Wed, 14 Mar 2007 02:00:22 -0700 (PDT)
Subject: [R] R:  Searching and deleting elements of list
In-Reply-To: <F0E5B4FAD37B7844B6D21998C11E60A6445CB2@RE2-EXC-VBE1B.ausl.org>
References: <9372270.post@talk.nabble.com>
	<F0E5B4FAD37B7844B6D21998C11E60A6445CB2@RE2-EXC-VBE1B.ausl.org>
Message-ID: <9470572.post@talk.nabble.com>


This is exactly what I need
Thank's a lot!!



Guazzetti Stefano wrote:
> 
> you could try mapply
> 
>  mydata2<-mapply("[", mydata, lapply(mydata, function(x) !x %in% A))
>  mydata2[[1]]<-A  #to replace the obviously deleted elements of "A"
>  mydata2
>  mydata2[[1]]
>  mydata2[[2]]
>  mydata2[[3]]
>  mydata2[[4]]
> 
> Stefano
> 
> 
> -----Messaggio originale-----
> Da: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]Per conto di jastar
> Inviato: gioved? 8 marzo 2007 12.31
> A: r-help at stat.math.ethz.ch
> Oggetto: [R] Searching and deleting elements of list
> 
> 
> 
> Hi,
> I have a problem. Please, look at example and try to help me!!
> 
>> A<-c("aaa","bbb","ccc","ddd","eee")
>> B<-c("vvv","ooo","aaa","eee","zzz","bbb")
>> C<-c("sss","jjj","ppp","ddd")
>> D<-c("bbb","ccc")
>>mydata=list(A,B,C,D)
> 
> I want to find and delete from 'mydata' all elements which occur in A
> (except A). 
> I mean after "operation":
>> mydata[[1]]
> [1] "aaa" "bbb" "ccc" "ddd" "eee"
>> mydata[[2]]
> [1] "vvv" "ooo" "zzz"
>> mydata[[3]]
> [1] "sss","jjj","ppp"
>> mydata[[4]]
> NULL
> 
> My list have about 10000 subelements (each contains several strings) so
> using loops is senseless.
> 
> Thank's for all replies and sorry for my English (I hope you understand
> what
> I'm talking about) :-)
>  
> -- 
> View this message in context:
> http://www.nabble.com/Searching-and-deleting-elements-of-list-tf3368489.html#a9372270
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Searching-and-deleting-elements-of-list-tf3368489.html#a9470572
Sent from the R help mailing list archive at Nabble.com.


From gavin.simpson at ucl.ac.uk  Wed Mar 14 10:02:00 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 14 Mar 2007 09:02:00 +0000
Subject: [R] dataframe layout
In-Reply-To: <007001c76616$2b94e380$6601a8c0@altair>
References: <007001c76616$2b94e380$6601a8c0@altair>
Message-ID: <1173862920.3012.2.camel@dhcppc2.my.nat.localnet>

On Wed, 2007-03-14 at 03:53 -0500, Robert Baer wrote:
> Can someone remind me how to change the columns in df.a into a two column 
> df.b that contains one column of data and another column of the original 
> column headings as levels.
> 
> Example:
> a=1:3
> b=4:6
> c=7:9
> df.a=data.frame(a,b,c)
> 
> Should become in df.b:
> dat   lev
> 1      a
> 2      a
> 3      a
> 4      b
> 5      b
> 6      b
> 7      c
> 8      c
> 9      c
> 
> Thanks.

One option is stack()

> a=1:3
> b=4:6
> c=7:9
> df.a=data.frame(a,b,c)
> df.a
  a b c
1 1 4 7
2 2 5 8
3 3 6 9
> stack(df.a)
  values ind
1      1   a
2      2   a
3      3   a
4      4   b
5      5   b
6      6   b
7      7   c
8      8   c
9      9   c
> class(stack(df.a))
[1] "data.frame"

HTH

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT                          [w] http://www.freshwaters.org.uk/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From ccleland at optonline.net  Wed Mar 14 10:06:58 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 14 Mar 2007 05:06:58 -0400
Subject: [R] dataframe layout
In-Reply-To: <007001c76616$2b94e380$6601a8c0@altair>
References: <007001c76616$2b94e380$6601a8c0@altair>
Message-ID: <45F7BB32.40603@optonline.net>

Robert Baer wrote:
> Can someone remind me how to change the columns in df.a into a two column 
> df.b that contains one column of data and another column of the original 
> column headings as levels.
> 
> Example:
> a=1:3
> b=4:6
> c=7:9
> df.a=data.frame(a,b,c)
> 
> Should become in df.b:
> dat   lev
> 1      a
> 2      a
> 3      a
> 4      b
> 5      b
> 6      b
> 7      c
> 8      c
> 9      c

  Here are a couple of different approaches:

df.b <- data.frame(dat = unlist(df.a),
                   lev = rep(names(df.a), each = dim(df.a)[1]))

df.b
   dat lev
a1   1   a
a2   2   a
a3   3   a
b1   4   b
b2   5   b
b3   6   b
c1   7   c
c2   8   c
c3   9   c

library(reshape)
melt(df.a, measure.var = names(df.a), variable_name = "lev")

  lev value
1   a     1
2   a     2
3   a     3
4   b     4
5   b     5
6   b     6
7   c     7
8   c     8
9   c     9

> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From Thierry.ONKELINX at inbo.be  Wed Mar 14 11:18:34 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 14 Mar 2007 11:18:34 +0100
Subject: [R] Power calculation for detecting linear trend
In-Reply-To: <055461107DE3B14AADEAA2D300D417F5668265@scomp0040.wurnet.nl>
Message-ID: <2E9C414912813E4EB981326983E0A10402B367F4@inboexch.inbo.be>

Erik,

I haven't seen an answer to your question, so I'll try to answer it. The
problem is that you switched the degrees of freedom. You had:

1 - pf(qf(.95, Vl, 1, ncp = 0), Vl, 1, ncp = Dl)
[1] 0.05472242

But it should be:

1 - pf(qf(.95, 1, Vl, ncp = 0), 1, Vl, ncp = Dl)
[1] 0.532651

Cheers,

Thierry

------------------------------------------------------------------------
----

ir. Thierry Onkelinx

Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest

Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance

Gaverstraat 4

9500 Geraardsbergen

Belgium

tel. + 32 54/436 185

Thierry.Onkelinx op inbo.be

www.inbo.be 

 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt

A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

 

> -----Oorspronkelijk bericht-----
> Van: r-help-bounces op stat.math.ethz.ch 
> [mailto:r-help-bounces op stat.math.ethz.ch] Namens Meesters, Erik
> Verzonden: woensdag 7 maart 2007 15:50
> Aan: r-help op stat.math.ethz.ch
> Onderwerp: [R] Power calculation for detecting linear trend
> 
> Dear people,
> I've a problem in doing a power calculation. In Fryer and 
> Nicholson (1993), ICES J. mar. Sci. 50: 161-168 page 164 an 
> example is given with the following characteristics T=5, 
> points in time R=5, replicates
> Var.within=0.1
> q=10, a 10% increase per year
> The degrees of freedom for the test are calculated as 
> Vl=T*R-2=23 and the non-centrality parameter Dl=4.54.
> Using this they get a power of 0.53, but the result that I'm 
> getting is 0.05472242.
> 
> I've tried this several ways in R, but I'm not able to come 
> up with the same number. Am I doing something wrong in the 
> calculation of the power?
> Here's my code:
> 
>     T<-5
>     R<-5
>     sigmasq<-0.1
>     q<-10
>     Vl<-(T*R)-2
>     Dl<-(R*(T-1)*T*(T+1)/(12*sigmasq))*(log(1+(q/100)))^2 #Dl 
> result is still similar
> 
>     power.1<-1-pf(qf(.95,(T*R-2),1,ncp=0),(T*R-2),1,ncp=Dl)
> 
> Thank you for any suggestions/help.
> 
> I'm using R2.4.1, on windowsXP.
> 
> Erik Meesters 	
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help op stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dickgiesser at gmail.com  Wed Mar 14 11:35:58 2007
From: dickgiesser at gmail.com (Benjamin Dickgiesser)
Date: Wed, 14 Mar 2007 10:35:58 +0000
Subject: [R] abs(U) > 0 where U is a vector?
Message-ID: <b75d67340703140335t1cd60646w62fd004698e337b3@mail.gmail.com>

Hi,

I am looking for a way to compare if every element of a vector is > 0.

i.e.
while(abs(U) > 0)
{

..
}

is there a function for this or do I have to write one?

I'd appreciate your help!

Benjamin


From rfrancois at mango-solutions.com  Wed Mar 14 11:46:09 2007
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Wed, 14 Mar 2007 10:46:09 +0000
Subject: [R] abs(U) > 0 where U is a vector?
In-Reply-To: <b75d67340703140335t1cd60646w62fd004698e337b3@mail.gmail.com>
References: <b75d67340703140335t1cd60646w62fd004698e337b3@mail.gmail.com>
Message-ID: <45F7D271.8010102@mango-solutions.com>

Benjamin Dickgiesser wrote:
> Hi,
>
> I am looking for a way to compare if every element of a vector is > 0.
>
> i.e.
> while(abs(U) > 0)
> {
>
> ..
> }
>
> is there a function for this or do I have to write one?
>
> I'd appreciate your help!
>
> Benjamin
>   
Hi,

If I understand you correctly, you are not far, is this what you want :

while(all(abs(U) > 0)){
   ...
}


Cheers,

Romain

-- 
Mango Solutions
data analysis that delivers

Tel:  +44(0) 1249 467 467
Fax:  +44(0) 1249 467 468
Mob:  +44(0) 7813 526 123


From dimitris.rizopoulos at med.kuleuven.be  Wed Mar 14 11:45:28 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 14 Mar 2007 11:45:28 +0100
Subject: [R] abs(U) > 0 where U is a vector?
References: <b75d67340703140335t1cd60646w62fd004698e337b3@mail.gmail.com>
Message-ID: <005301c76625$e1bb2570$0540210a@www.domain>

try: all(U > 0)


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Benjamin Dickgiesser" <dickgiesser at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, March 14, 2007 11:35 AM
Subject: [R] abs(U) > 0 where U is a vector?


> Hi,
>
> I am looking for a way to compare if every element of a vector is > 
> 0.
>
> i.e.
> while(abs(U) > 0)
> {
>
> ..
> }
>
> is there a function for this or do I have to write one?
>
> I'd appreciate your help!
>
> Benjamin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From dickgiesser at gmail.com  Wed Mar 14 11:52:59 2007
From: dickgiesser at gmail.com (Benjamin Dickgiesser)
Date: Wed, 14 Mar 2007 10:52:59 +0000
Subject: [R] abs(U) > 0 where U is a vector?
In-Reply-To: <b75d67340703140335t1cd60646w62fd004698e337b3@mail.gmail.com>
References: <b75d67340703140335t1cd60646w62fd004698e337b3@mail.gmail.com>
Message-ID: <b75d67340703140352h7e80c55ctf5d05617c27d2b37@mail.gmail.com>

Thx for all your responses!

while(all(abs(U) > 0))
{
..
}

was what I am looking for.

On 3/14/07, Benjamin Dickgiesser <dickgiesser at gmail.com> wrote:
> Hi,
>
> I am looking for a way to compare if every element of a vector is > 0.
>
> i.e.
> while(abs(U) > 0)
> {
>
> ..
> }
>
> is there a function for this or do I have to write one?
>
> I'd appreciate your help!
>
> Benjamin
>


From murdoch at stats.uwo.ca  Wed Mar 14 11:53:13 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 14 Mar 2007 06:53:13 -0400
Subject: [R] Sweave question: prevent expansion of unevaluated reused
 code	chunk
In-Reply-To: <45F72D9C.5000307@mdacc.tmc.edu>
References: <45F72D9C.5000307@mdacc.tmc.edu>
Message-ID: <45F7D419.7040406@stats.uwo.ca>

On 3/13/2007 7:02 PM, Kevin R. Coombes wrote:
> Hi,
> 
> Consider the following (much simplified) Sweave example:
> 
> --------------
> 
> First, we set the value of $x$:
> <<chunk1,eval=FALSE>>=
> x <- 1
> @
> 
> Then we set the value of $y$:
> <<chunk2,eval=FALSE>>=
> y <- 2
> @
> 
> Thus, the overall algorithm has this structure:
> <<combined,eval=FALSE>>=
> <<chunk1>>
> <<chunk2>>
> @
> 
> <<justDoIt,echo=FALSE>>=
> <<combined>>
> @
> 
> ---------------
> 
> I'd like to be able to do something like this, where the "combined" 
> chunk prints out in the final LaTeX document essentially verbatim.  In 
> particular, I want to see the "<<chunk1>>" unexpanded in that block, 
> since this gives me a nice conceptual overview of the algorithm. (Of 
> courser, this is more useful when chunk1 and chunk2 are much longer than 
> they are in this example....)
> 
> Is there an option that allows me to get this behavior?

As others have said, the answer is currently no, but in R 2.5.0 this 
should be a relatively easy modification (because it has the ability to 
echo your input, rather than a deparsed version of it).  In the other 
platforms you've used, is there a standard syntax to indicate whether or 
not you want the chunks expanded?  I can see either behaviour as being 
desirable in different circumstances.  Sometimes you want the reader to 
know about your chunk names, and sometimes you don't.

Duncan Murdoch


From albmont at centroin.com.br  Wed Mar 14 12:05:47 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Wed, 14 Mar 2007 09:05:47 -0200
Subject: [R] abs(U) > 0 where U is a vector?
In-Reply-To: <b75d67340703140335t1cd60646w62fd004698e337b3@mail.gmail.com>
References: <b75d67340703140335t1cd60646w62fd004698e337b3@mail.gmail.com>
Message-ID: <20070314105740.M99659@centroin.com.br>

Benjamin Dickgiesser wrote:
> 
> I am looking for a way to compare if every element of a vector is > 0.
> 
> i.e.
> while(abs(U) > 0)
> {
> 
> ..
> }
> 
Notice that abs(U) [and, in general, most functions that are
defined on scalars, like sin, cos, exp, ...], when U is a vector,
operates on the _elements_ of U, so abs(U) is just a vector
of non-negative elements.

Likewise, (U > 0) [and, in general, most relations that are
defined on scalars, like (U != 0), (U == 0), (U >= 1 & U <= 2)],
when U is a vector, operates on the _elements_ of U, so (U > 0)
is just a vector of logical values.

So, you must take some operation that will check if all components
of a vector of logical (boolean) elements are non-zero. The most
obvious solution is to _multiply_ those logical elements, because
FALSE will become zero, and zero * anything = zero, so if any
component of U is <= 0, you get zero:

prod(U > 0)

But this is not the most "elegant" solution, because there is
a function to check if all [and another to check if any] component
of a vector of booleans are [is] true: it's all(V) [resp. any(V)].

So:

all(U > 0)

Sanity check:

U <- c(-1, 1, 2)
all(U > 0)  # FALSE
U[1] <- 3
all(U > 0)  # TRUE

Alberto Monteiro


From skiadas at hanover.edu  Wed Mar 14 12:45:37 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Wed, 14 Mar 2007 07:45:37 -0400
Subject: [R] abs(U) > 0 where U is a vector?
In-Reply-To: <20070314105740.M99659@centroin.com.br>
References: <b75d67340703140335t1cd60646w62fd004698e337b3@mail.gmail.com>
	<20070314105740.M99659@centroin.com.br>
Message-ID: <58A3D492-93A6-4EE4-8926-0CAB9E9BD4B2@hanover.edu>

On Mar 14, 2007, at 7:05 AM, Alberto Monteiro wrote:
> prod(U > 0)
>
> But this is not the most "elegant" solution, because there is
> a function to check if all [and another to check if any] component
> of a vector of booleans are [is] true: it's all(V) [resp. any(V)].
> So:
> all(U > 0)

Just for the record, there is a actually a slight difference in the  
two calls of prod and all, which may or may not be important in the  
OP's case, in how they deal with NA's:

 > x<-c(-3,NA,2)
 > all(x>0)
[1] FALSE
 > prod(x>0)
[1] NA
 > x<-c(3,NA,2)
 > all(x>0)
[1] NA
 > prod(x>0)
[1] NA

These are of course all as expected, just something to keep in mind.

And in any case, "all" is as Alberto says more elegant, and  
semantically much more clear. (And not that it matters, but it is  
also somewhat faster).

> Alberto Monteiro

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From dickgiesser at gmail.com  Wed Mar 14 13:44:33 2007
From: dickgiesser at gmail.com (Benjamin Dickgiesser)
Date: Wed, 14 Mar 2007 12:44:33 +0000
Subject: [R] rgeom for p (1-p)^(x-1)
Message-ID: <b75d67340703140544m791e3d7ag2320eeaefb180901@mail.gmail.com>

Hi,

is there a package available which lets me generate random data for
the geometric distribution with density:

 p(x) = p (1-p)^(x-1) ?

rgeom uses the density p(x) = p (1-p)^x.

Thank you,
Benjamin


From skiadas at hanover.edu  Wed Mar 14 13:51:51 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Wed, 14 Mar 2007 08:51:51 -0400
Subject: [R] rgeom for p (1-p)^(x-1)
In-Reply-To: <b75d67340703140544m791e3d7ag2320eeaefb180901@mail.gmail.com>
References: <b75d67340703140544m791e3d7ag2320eeaefb180901@mail.gmail.com>
Message-ID: <F67B8480-469C-4FC5-853E-605AEB52F2BB@hanover.edu>

On Mar 14, 2007, at 8:44 AM, Benjamin Dickgiesser wrote:
> Hi,
>
> is there a package available which lets me generate random data for
> the geometric distribution with density:
>
>  p(x) = p (1-p)^(x-1) ?
>
> rgeom uses the density p(x) = p (1-p)^x.

Why not just use rgeom, and then add 1 to all the values?

> Thank you,
> Benjamin

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From sfalcon at fhcrc.org  Wed Mar 14 15:48:11 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Wed, 14 Mar 2007 07:48:11 -0700
Subject: [R] Sweave question: prevent expansion of unevaluated reused
	code chunk
In-Reply-To: <45F74D53.7040706@mdacc.tmc.edu> (Kevin R. Coombes's message of
	"Tue, 13 Mar 2007 20:18:11 -0500")
References: <45F72D9C.5000307@mdacc.tmc.edu> <m2slc8itpq.fsf@ziti.local>
	<45F74D53.7040706@mdacc.tmc.edu>
Message-ID: <m2lkhzhoes.fsf@ziti.local>

"Kevin R. Coombes" <krc at mdacc.tmc.edu> writes:
> [2] You are also correct that there is no advantage if I just call
> them "chunk1" and "chunk2". But if I call them something more
> interesting, like "perform.quantile.normalization" or
> "truncate.and.log.transform",
> then I can use this structure to explain the algorithm at a higher
> level.  If you go back to Knuth's original literate programming
> examples, this is exactly how he presents his examples. For instance,
> on page 104 of the "Literate Programming" book, he has

I suspect it would not be too hard to write an Sweave driver that
would respond to an expand=FALSE argument in the way you want. -- But
I'm not certain.  I've never used nested code chunks (!) and that may
make such a modification difficult.

+ seth


From krc at mdacc.tmc.edu  Wed Mar 14 16:39:43 2007
From: krc at mdacc.tmc.edu (Kevin R. Coombes)
Date: Wed, 14 Mar 2007 10:39:43 -0500
Subject: [R] Sweave question: prevent expansion of unevaluated reused
 code	chunk
In-Reply-To: <45F7D419.7040406@stats.uwo.ca>
References: <45F72D9C.5000307@mdacc.tmc.edu> <45F7D419.7040406@stats.uwo.ca>
Message-ID: <45F8173F.8010906@mdacc.tmc.edu>

Hi,

I don't know of a standard way to indicate this; I would have suggested

<<combined,expand=FALSE>>

(with expand=TRUE the default), except for the fact that Seth Falcon 
already suggested the same notation in his response...so I can only 
second the motion.

	Kevin

Duncan Murdoch wrote:
> On 3/13/2007 7:02 PM, Kevin R. Coombes wrote:
>> Hi,
>>
>> Consider the following (much simplified) Sweave example:
>>
>> --------------
>>
>> First, we set the value of $x$:
>> <<chunk1,eval=FALSE>>=
>> x <- 1
>> @
>>
>> Then we set the value of $y$:
>> <<chunk2,eval=FALSE>>=
>> y <- 2
>> @
>>
>> Thus, the overall algorithm has this structure:
>> <<combined,eval=FALSE>>=
>> <<chunk1>>
>> <<chunk2>>
>> @
>>
>> <<justDoIt,echo=FALSE>>=
>> <<combined>>
>> @
>>
>> ---------------
>>
>> I'd like to be able to do something like this, where the "combined" 
>> chunk prints out in the final LaTeX document essentially verbatim.  In 
>> particular, I want to see the "<<chunk1>>" unexpanded in that block, 
>> since this gives me a nice conceptual overview of the algorithm. (Of 
>> courser, this is more useful when chunk1 and chunk2 are much longer 
>> than they are in this example....)
>>
>> Is there an option that allows me to get this behavior?
> 
> As others have said, the answer is currently no, but in R 2.5.0 this 
> should be a relatively easy modification (because it has the ability to 
> echo your input, rather than a deparsed version of it).  In the other 
> platforms you've used, is there a standard syntax to indicate whether or 
> not you want the chunks expanded?  I can see either behaviour as being 
> desirable in different circumstances.  Sometimes you want the reader to 
> know about your chunk names, and sometimes you don't.
> 
> Duncan Murdoch


From plynchnlm at gmail.com  Wed Mar 14 16:48:15 2007
From: plynchnlm at gmail.com (Paul Lynch)
Date: Wed, 14 Mar 2007 11:48:15 -0400
Subject: [R] Connecting R-help and Google Groups?
Message-ID: <50d6c72a0703140848h3ca36f94k6d8f847244291553@mail.gmail.com>

This morning I tried to see if I could find the r-help mailing list on
Google Groups, which has an interface that I like.  I found three
Google Groups ("The R Project for Statistical Computing", "rproject",
and "rhelp") but none of them are connected to the r-help list.

Is there perhaps some reason why it wouldn't be a good thing for there
to be a connected Google Group?  I think it should be possible to set
things up so that a post to the Google Group goes to the r-help
mailing list, and vice-versa.

Also, does anyone know why the three existing R Google Groups failed
to get connected to r-help?  It might require some action on the part
of the r-help list administrator.

Thanks,
    --Paul

-- 
Paul Lynch
Aquilent, Inc.
National Library of Medicine (Contractor)


From aiminy at iastate.edu  Wed Mar 14 16:55:10 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Wed, 14 Mar 2007 10:55:10 -0500
Subject: [R] tune.svm
Message-ID: <6.2.3.4.2.20070314104610.03e43420@aiminy.mail.iastate.edu>

I use tune.svm to tune gamma and cost for my training dataset.
I use PC, it runs very slowly. Does anyone know how to make it faster?

Aimin


From gunter.berton at gene.com  Wed Mar 14 17:09:00 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Wed, 14 Mar 2007 09:09:00 -0700
Subject: [R] Connecting R-help and Google Groups?
In-Reply-To: <50d6c72a0703140848h3ca36f94k6d8f847244291553@mail.gmail.com>
Message-ID: <002401c76653$14104dc0$4d908980@gne.windows.gene.com>

I know nothing about Google Groups, but FWIW, I think it would be most
unwise for R/CRAN to hook up to **any** commercially sponsored web portals.
Future changes in their policies, interfaces,or access conditions may make
them inaccessible or unfreindly to R users. So long as we have folks willing
and able to host and maintain our lists as part of the CRAN infrastructure,
CRAN maintains control. I think this is wise and prudent.

I am happy to be educated to the contrary if I misunderstand how this would
work.

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Lynch
Sent: Wednesday, March 14, 2007 8:48 AM
To: R-help at stat.math.ethz.ch
Subject: [R] Connecting R-help and Google Groups?

This morning I tried to see if I could find the r-help mailing list on
Google Groups, which has an interface that I like.  I found three
Google Groups ("The R Project for Statistical Computing", "rproject",
and "rhelp") but none of them are connected to the r-help list.

Is there perhaps some reason why it wouldn't be a good thing for there
to be a connected Google Group?  I think it should be possible to set
things up so that a post to the Google Group goes to the r-help
mailing list, and vice-versa.

Also, does anyone know why the three existing R Google Groups failed
to get connected to r-help?  It might require some action on the part
of the r-help list administrator.

Thanks,
    --Paul

-- 
Paul Lynch
Aquilent, Inc.
National Library of Medicine (Contractor)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Max.Kuhn at pfizer.com  Wed Mar 14 17:20:18 2007
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Wed, 14 Mar 2007 12:20:18 -0400
Subject: [R] tune.svm
In-Reply-To: <6.2.3.4.2.20070314104610.03e43420@aiminy.mail.iastate.edu>
Message-ID: <71257D09F114DA4A8E134DEAC70F25D307CF5E06@groamrexm03.amer.pfizer.com>

Fast is a relative term. Tell us more about your version, system, data
and the kernel that you are using (I'm guessing it is a RBF).

If you are building a classification model and there are two classes,
the svmpath package can be used to get quick predictions over the
cost/regularization parameter (for fixed kernel parameters). If you want
to cross-validate, you would probably have to write your own loop.

Max

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Aimin Yan
Sent: Wednesday, March 14, 2007 11:55 AM
To: r-help at stat.math.ethz.ch
Subject: [R] tune.svm

I use tune.svm to tune gamma and cost for my training dataset.
I use PC, it runs very slowly. Does anyone know how to make it faster?

Aimin

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From asn151 at yahoo.com  Wed Mar 14 17:51:20 2007
From: asn151 at yahoo.com (Jonathan Morse)
Date: Wed, 14 Mar 2007 09:51:20 -0700 (PDT)
Subject: [R] Trimming a Data Set
Message-ID: <248290.15195.qm@web62415.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070314/5be211ca/attachment.pl 

From plynchnlm at gmail.com  Wed Mar 14 18:36:33 2007
From: plynchnlm at gmail.com (Paul Lynch)
Date: Wed, 14 Mar 2007 13:36:33 -0400
Subject: [R] Connecting R-help and Google Groups?
In-Reply-To: <002401c76653$14104dc0$4d908980@gne.windows.gene.com>
References: <50d6c72a0703140848h3ca36f94k6d8f847244291553@mail.gmail.com>
	<002401c76653$14104dc0$4d908980@gne.windows.gene.com>
Message-ID: <50d6c72a0703141036y788031d4tfb633f2f20382183@mail.gmail.com>

Well, I don't see what danger could arise from the fact that Google
Groups is owned by a company.  Google Groups provides access to all of
usenet, plus many mailing lists (e.g. the ruby-talk mailing list for
Ruby programmers).  They don't control any of the newgroups or mailing
lists that they provide access to.  It is a free service, supported by
advertising.

As for the issue of whether there might be future access problems
(e.g. if Google goes bankrupt, which currently seems unlikely)  R
users would still have access to the r-help list through the means
that they have now.  I am not recommending replacing any of the
current means of access to the r-help list; I am just asking about
adding an additional means of access.

      --Paul

On 3/14/07, Bert Gunter <gunter.berton at gene.com> wrote:
> I know nothing about Google Groups, but FWIW, I think it would be most
> unwise for R/CRAN to hook up to **any** commercially sponsored web portals.
> Future changes in their policies, interfaces,or access conditions may make
> them inaccessible or unfreindly to R users. So long as we have folks willing
> and able to host and maintain our lists as part of the CRAN infrastructure,
> CRAN maintains control. I think this is wise and prudent.
>
> I am happy to be educated to the contrary if I misunderstand how this would
> work.
>
> Bert Gunter
> Genentech Nonclinical Statistics
> South San Francisco, CA 94404
> 650-467-7374
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Lynch
> Sent: Wednesday, March 14, 2007 8:48 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Connecting R-help and Google Groups?
>
> This morning I tried to see if I could find the r-help mailing list on
> Google Groups, which has an interface that I like.  I found three
> Google Groups ("The R Project for Statistical Computing", "rproject",
> and "rhelp") but none of them are connected to the r-help list.
>
> Is there perhaps some reason why it wouldn't be a good thing for there
> to be a connected Google Group?  I think it should be possible to set
> things up so that a post to the Google Group goes to the r-help
> mailing list, and vice-versa.
>
> Also, does anyone know why the three existing R Google Groups failed
> to get connected to r-help?  It might require some action on the part
> of the r-help list administrator.
>
> Thanks,
>     --Paul
>
> --
> Paul Lynch
> Aquilent, Inc.
> National Library of Medicine (Contractor)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
Paul Lynch
Aquilent, Inc.
National Library of Medicine (Contractor)


From edd at debian.org  Wed Mar 14 18:52:09 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 14 Mar 2007 12:52:09 -0500
Subject: [R] Connecting R-help and Google Groups?
In-Reply-To: <50d6c72a0703140848h3ca36f94k6d8f847244291553@mail.gmail.com>
References: <50d6c72a0703140848h3ca36f94k6d8f847244291553@mail.gmail.com>
Message-ID: <20070314175209.GA20292@eddelbuettel.com>

On Wed, Mar 14, 2007 at 11:48:15AM -0400, Paul Lynch wrote:
> This morning I tried to see if I could find the r-help mailing list on
> Google Groups, which has an interface that I like.  I found three
> Google Groups ("The R Project for Statistical Computing", "rproject",
> and "rhelp") but none of them are connected to the r-help list.
> 
> Is there perhaps some reason why it wouldn't be a good thing for there
> to be a connected Google Group?  I think it should be possible to set
> things up so that a post to the Google Group goes to the r-help
> mailing list, and vice-versa.
> 
> Also, does anyone know why the three existing R Google Groups failed
> to get connected to r-help?  It might require some action on the part
> of the r-help list administrator.

As _existing_ alternative, consider Gmane.org. See the R FAQ for
details -- but a fair number of R lists a archive, gatewayed, ...
there already.

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From szhan at uoguelph.ca  Wed Mar 14 19:11:38 2007
From: szhan at uoguelph.ca (szhan at uoguelph.ca)
Date: Wed, 14 Mar 2007 14:11:38 -0400
Subject: [R] How to transform matrices to ANOVA input datasets?
Message-ID: <20070314141138.lrh4iv48e8skc44o@webmail.uoguelph.ca>

Hello, R experts,
I have a list called dataHP which has 30 elements (m1, m2, ..., m30).  
Each element is a 7x6 matrix holding yield data from two factors  
experimental design, with treatment in column, position in row. For  
instance, the element 20 is:
dataHP[[20]]
           col1     col2     col3      trt1     trt2    trt3
  [1,]     22.0     20.3     29.7      63.3     78.5    76.4
  [2,]    102.4     92.3     72.2     199.2    201.1    218.9
  [3,]     18.8     20.8     22.9     106.2    148.4    147.6
  [4,]     14.5     17.2     15.6     120.1    115.8    124.6
  [5,]     31.9     28.3     22.8     157.9    192.3    160.6
  [6,]     98.2    147.3    102.5     628.8    577.0    643.0
  [7,]    174.9    217.5   188.66     453.5    491.1    409.8

My goal is to find which element in the list has significant yield  
difference among the position. So my first question is how to  
transform the matrix to ANOVA input dataset which is:
yield	block	treat	position
22	1	col	1
102.4	1	col	2
18.8	1	col	3
14.5	1	col	4
31.9	1	col	5
98.2	1	col	6
174.9	1	col	7
20.3	2	col	1
92.3	2	col	2
20.8	2	col	3
17.2	2	col	4
28.3	2	col	5
147.3	2	col	6
217.5	2	col	7
29.7	3	col	1
72.2	3	col	2
22.9	3	col	3
15.6	3	col	4
22.8	3	col	5
102.5	3	col	6
188.66	3	col	7
63.3	1	trt	1
199.2	1	trt	2
106.2	1	trt	3
120.1	1	trt	4
157.9	1	trt	5
628.8	1	trt	6
453.5	1	trt	7
78.5	2	trt	1
201.1	2	trt	2
148.4	2	trt	3
115.8	2	trt	4
192.3	2	trt	5
577	2	trt	6
491.1	2	trt	7
76.4	3	trt	1
218.9	3	trt	2
147.6	3	trt	3
124.6	3	trt	4
160.6	3	trt	5
643	3	trt	6
409.8	3	trt	7
So I can contrasts(position) and do ANOVA like this:
fit1<-aov(yield~treat*position)
summary(fit1, split=list(position=1:10), expand.split= T)
Finally I can find the significant element in the list if there is any  
significant contrast among the position. So my second question is how  
to apply this ANOVA to each element in the list?
Your help will be highly appreciated!!

Josh


From klaster at karlin.mff.cuni.cz  Wed Mar 14 19:52:31 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Wed, 14 Mar 2007 19:52:31 +0100
Subject: [R] Trimming a Data Set
In-Reply-To: <248290.15195.qm@web62415.mail.re1.yahoo.com>
References: <248290.15195.qm@web62415.mail.re1.yahoo.com>
Message-ID: <45F8446F.2040201@karlin.mff.cuni.cz>

If it is precise enough for you, you can use

trim <- function(x,prop=.05) {
trimx <- x[x < quantile(x,prob=(1-prop))]
return(trimx)
}

Petr

Jonathan Morse napsal(a):
> Hi,
> 
> I am trying to restrict a data set so as not to included outliers.  Specifically, I would like to specify a percentage where a fraction of observations are eliminated from the data set, much in the same way that the trimmed mean function works - but leaving the restricted data set intact.
> 
> I have been using a function which will restrict the data set using:
>> trim=function(x,p){
>> o=order(x)
>> xo=x[o]
>> n=length(xo)
>> tl=round(n*p)
>> print(xo[(tl+1):(n-tl)])}
> 
> However I was wondering if anyone knew a more elegant and simple method to get the same result.
> 
> Thanks in advance.
> 
> 
>  
> ---------------------------------
> Don't pick lemons.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From aiminy at iastate.edu  Wed Mar 14 20:11:14 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Wed, 14 Mar 2007 14:11:14 -0500
Subject: [R] plot.tune
Message-ID: <6.2.3.4.2.20070314140904.03c0a940@aiminy.mail.iastate.edu>

In plot.tune, default color for contour plot is lightblue.
If I want to set several different colors for contour plot, How can I do this?

Thanks,

Aimin


From M.J.Bojanowski at uu.nl  Wed Mar 14 20:11:36 2007
From: M.J.Bojanowski at uu.nl (Bojanowski, M.J.  (Michal))
Date: Wed, 14 Mar 2007 20:11:36 +0100
Subject: [R] How to transform matrices to ANOVA input datasets?
In-Reply-To: <20070314141138.lrh4iv48e8skc44o@webmail.uoguelph.ca>
References: <20070314141138.lrh4iv48e8skc44o@webmail.uoguelph.ca>
Message-ID: <94E133D09AA24D43BF6341B675C01A33113514@uu01msg-exb01.soliscom.uu.nl>

Hi Josh,

Consider what follows to convert your data and estimate the models.
I am not sure however, what do you want to do after the models are
estimated, so my suggestions stop at this point.

HTH,
Michal


# -b-e-g-i-n---R---c-o-d-e-

# first i make some data that is similar to yours so i can practice on
it
m <- matrix(rnorm(7*6), ncol=6, nrow=7, 
	dimnames=list(1:7,  paste( rep(c("col", "trt"), each=3), 1:3,
sep="")) )
dataHP <- list(m, m, m)

# the first 7x6 matrix in the list
dataHP[[1]]

# function that converts a 7x6 matrix 'x' to a data frame just as you
want it
f <- function(x)
{
	data.frame( yield=as.numeric(x),
		block=rep( c(1:3, 1:3), each=7 ),
		treat=rep( c("col", "trt"), each=7*3 ),
		position=rep(1:7, 6) )
}

# the data frame
f(dataHP[[1]])

# process the list of matrices by applying 'f' to every component
# consequently, 'd' is a list of data frames
d <- lapply(dataHP, f)


# now, to calculate AOV for every data set you could again use 'lapply',
e.g.:
models <- lapply(d, function(x)  aov( yield ~ treat*position, data=x) )
# the result is a list of models
models[[1]]

# -e-n-d---R---c-o-d-e-



*** Note that my e-mail address has changed to m.j.bojanowski at uu.nl
*** Please update your address books accordingly. Thank you!

_________________________________________
Michal Bojanowski
ICS / Sociology
Utrecht University
Heidelberglaan 2; 3584 CS Utrecht
Room 1428
m.j.bojanowski at uu.nl
http://www.fss.uu.nl/soc/bojanowski
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of szhan at uoguelph.ca
Sent: Wednesday, March 14, 2007 7:12 PM
To: r-help at stat.math.ethz.ch
Subject: [R] How to transform matrices to ANOVA input datasets?

Hello, R experts,
I have a list called dataHP which has 30 elements (m1, m2, ..., m30).  
Each element is a 7x6 matrix holding yield data from two factors
experimental design, with treatment in column, position in row. For
instance, the element 20 is:
dataHP[[20]]
           col1     col2     col3      trt1     trt2    trt3
  [1,]     22.0     20.3     29.7      63.3     78.5    76.4
  [2,]    102.4     92.3     72.2     199.2    201.1    218.9
  [3,]     18.8     20.8     22.9     106.2    148.4    147.6
  [4,]     14.5     17.2     15.6     120.1    115.8    124.6
  [5,]     31.9     28.3     22.8     157.9    192.3    160.6
  [6,]     98.2    147.3    102.5     628.8    577.0    643.0
  [7,]    174.9    217.5   188.66     453.5    491.1    409.8

My goal is to find which element in the list has significant yield
difference among the position. So my first question is how to transform
the matrix to ANOVA input dataset which is:
yield	block	treat	position
22	1	col	1
102.4	1	col	2
18.8	1	col	3
14.5	1	col	4
31.9	1	col	5
98.2	1	col	6
174.9	1	col	7
20.3	2	col	1
92.3	2	col	2
20.8	2	col	3
17.2	2	col	4
28.3	2	col	5
147.3	2	col	6
217.5	2	col	7
29.7	3	col	1
72.2	3	col	2
22.9	3	col	3
15.6	3	col	4
22.8	3	col	5
102.5	3	col	6
188.66	3	col	7
63.3	1	trt	1
199.2	1	trt	2
106.2	1	trt	3
120.1	1	trt	4
157.9	1	trt	5
628.8	1	trt	6
453.5	1	trt	7
78.5	2	trt	1
201.1	2	trt	2
148.4	2	trt	3
115.8	2	trt	4
192.3	2	trt	5
577	2	trt	6
491.1	2	trt	7
76.4	3	trt	1
218.9	3	trt	2
147.6	3	trt	3
124.6	3	trt	4
160.6	3	trt	5
643	3	trt	6
409.8	3	trt	7
So I can contrasts(position) and do ANOVA like this:
fit1<-aov(yield~treat*position)
summary(fit1, split=list(position=1:10), expand.split= T) Finally I can
find the significant element in the list if there is any significant
contrast among the position. So my second question is how to apply this
ANOVA to each element in the list?
Your help will be highly appreciated!!

Josh

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Max.Kuhn at pfizer.com  Wed Mar 14 20:18:40 2007
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Wed, 14 Mar 2007 15:18:40 -0400
Subject: [R] plot.tune
In-Reply-To: <6.2.3.4.2.20070314140904.03c0a940@aiminy.mail.iastate.edu>
Message-ID: <71257D09F114DA4A8E134DEAC70F25D307CF6145@groamrexm03.amer.pfizer.com>

Aimin,

Please see color.palette in ?plot.tune. 

Sorry to be a broken record, but if you want people to help you, you
should:

  - read the help pages before sending an email

  - run sessionInfo() and send it in the email

You run the risk of people ignoring your emails. These steps will help
others understand your problems and believe that you tried solving the
issue before posting.

Max

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Aimin Yan
Sent: Wednesday, March 14, 2007 3:11 PM
To: r-help at stat.math.ethz.ch
Subject: [R] plot.tune

In plot.tune, default color for contour plot is lightblue.
If I want to set several different colors for contour plot, How can I do
this?

Thanks,

Aimin

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From rdporto1 at terra.com.br  Wed Mar 14 20:26:33 2007
From: rdporto1 at terra.com.br (rdporto1)
Date: Wed, 14 Mar 2007 16:26:33 -0300
Subject: [R] replicating SAS's "proc rank" procedure
Message-ID: <JEWRC9$EFE872E4F270740788817C00610C4215@terra.com.br>

Jiho.han,

> I wonder if there's a way to replicate SAS rank procedure where it ranks a
> variable by a certain number of groups. For example, it's very easy to
> calculate quintile rank in SAS, but I couldn't find the similar function in
> R. 
> 
> Does anyone know how to do this?

There are some functions you can try: rank(), order() or sort().
Actually, I didn't understand well your problem.

> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

It can be a simple SAS code just to let us help you more.

Rogerio


From cevans at freecurricula.org  Wed Mar 14 20:48:12 2007
From: cevans at freecurricula.org (Charles Evans)
Date: Wed, 14 Mar 2007 15:48:12 -0400
Subject: [R] ols Error : missing value where TRUE/FALSE needed
Message-ID: <58A2705A-4368-4DDA-9EE1-A0FF7FC4DC7F@freecurricula.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070314/d308841a/attachment.pl 

From francogrex at mail.com  Wed Mar 14 21:06:07 2007
From: francogrex at mail.com (francogrex)
Date: Wed, 14 Mar 2007 13:06:07 -0700 (PDT)
Subject: [R] Logistic regression for drugs Interactions
Message-ID: <9482343.post@talk.nabble.com>


I have the model below, for which I run a logistic regression including the
interaction term (NSAID*Diuretic)
------------------------
fit1=glm(resp ~ nsaid+diuretic+I(nsaid*diuretic), family= binomial,data=w)


NSAID	Diuretic	Present	Absent
0	0	185	6527
0	1	53	1444
1	0	42	1293
1	1	25	253




	Coefficients	Std. Error	z value	Pr(>|z|)    
(Intercept)	-3.56335	0.07456	-47.794	 < 2e-16 ***
NSAID	0.13630	0.17361	0.785	 0.43242    
Diuretic	0.25847	0.15849	1.631	 0.10293    
I(NSAID*Diuretic)	0.85407	0.30603	2.791	 0.00526 ** 
---
Signif. Codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
Odds ratio is 2.35 [ln(0.85407)] times higher when NSAID is present in
addition to Diuretic.

-------------------------------

Odds ratio of Nausea when on Diuretic is exp(0.25847)= 1.29
and the odds ratio of Nausea when on NSAID is exp(0.13630)=1.14
Normally when we want to see the odds ratio of Nausea when a patient is on
both drugs we multiply 1.29*1.14= 1.48 (is this correct? do we multiply or
do we add?)

But since the interaction term is significant then we take that into
account? Does that mean that the odds ratio of the interaction is
exp(0.25847)*exp(0.13630)*exp(0.85407)=3.486297 ?

Or do we use additions?

Thanks.
-- 
View this message in context: http://www.nabble.com/Logistic-regression-for-drugs-Interactions-tf3404506.html#a9482343
Sent from the R help mailing list archive at Nabble.com.


From jackswsc at beyondbb.com  Wed Mar 14 20:20:57 2007
From: jackswsc at beyondbb.com (John Schuenemeyer)
Date: Wed, 14 Mar 2007 13:20:57 -0600
Subject: [R] Redirecting output to the screen
Message-ID: <001301c7666d$e4f68ca0$0b00a8c0@DBNXY511>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070314/a8bbbdfd/attachment.pl 

From kaiping.chen at lehman.com  Wed Mar 14 18:50:45 2007
From: kaiping.chen at lehman.com (Chen, Kaiping)
Date: Wed, 14 Mar 2007 13:50:45 -0400
Subject: [R] Cannot create Java Virtual Machine
In-Reply-To: <20070314110003.01F0A2CF661@mailman.csd.univie.ac.at>
References: <20070314110003.01F0A2CF661@mailman.csd.univie.ac.at>
Message-ID: <EF68AC78854E2D4589CEB614D2A056DC05E196CA@nypcmg1exms306.leh.lbcorp.lehman.com>


Hi I installed package rJava but .jinit() gives an error "Cannot create
Java Virtual Machine".
Currently, I am running Win XP, with JRE and JDK 1.5 installed. I am not
quite familiar with java configuation. Can anyone give some ideas on the
error mentioned above? Thanks.
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

This message is intended only for the personal and confidential use of the designated recipient(s) named above.  If you are not the intended recipient of this message you are hereby notified that any review, dissemination, distribution or copying of this message is strictly prohibited.  This communication is for information purposes only and should not be regarded as an offer to sell or as a solicitation of an offer to buy any financial product, an official confirmation of any transaction, or as an official statement of Lehman Brothers.  Email transmission cannot be guaranteed to be secure or error-free.  Therefore, we do not represent that this information is complete or accurate and it should not be relied upon as such.  All information is subject to change without notice.

--------
IRS Circular 230 Disclosure:
Please be advised that any discussion of U.S. tax matters contained within this communication (including any attachments) is not intended or written to be used and cannot be used for the purpose of (i) avoiding U.S. tax related penalties or (ii) promoting, marketing or recommending to another party any transaction or matter addressed herein.


From jasoncbarnhart at msn.com  Wed Mar 14 22:20:00 2007
From: jasoncbarnhart at msn.com (Jason Barnhart)
Date: Wed, 14 Mar 2007 14:20:00 -0700
Subject: [R] ols Error : missing value where TRUE/FALSE needed
References: <58A2705A-4368-4DDA-9EE1-A0FF7FC4DC7F@freecurricula.org>
Message-ID: <BAY116-DAV6806A4868A8377F86D845CF730@phx.gbl>

I think that inc2 should be eco$inc2 in your call.

If not let me know and I will dig deeper.

----- Original Message ----- 
From: "Charles Evans" <cevans at freecurricula.org>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, March 14, 2007 12:48 PM
Subject: [R] ols Error : missing value where TRUE/FALSE needed


>I have installed Hmisc and Design.  When I use ols, I get the
> following error message:
>
> Error in if (!length(fname) || !any(fname == zname)) { :
> missing value where TRUE/FALSE needed
>
> The model that I am running is:
>
> > ecools <- ols(eco$exp ~ eco$age + eco$own + eco$inc + inc2, 
> > x=TRUE)
>
> I have tried several other combinations of arguments that take TRUE/
> FALSE values, but no luck.
>
> Ultimately, I am trying to calculate robust standard errors.
>
> Any help would be appreciated.
>
> Charles Evans
> Executive Director
> Free Curricula Center
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jholtman at gmail.com  Wed Mar 14 23:01:05 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 14 Mar 2007 17:01:05 -0500
Subject: [R] Redirecting output to the screen
In-Reply-To: <001301c7666d$e4f68ca0$0b00a8c0@DBNXY511>
References: <001301c7666d$e4f68ca0$0b00a8c0@DBNXY511>
Message-ID: <644e1f320703141501g480dff47s7ae9faf2f7b6a624@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070314/6e75b534/attachment.pl 

From arunm at softwarefx.com  Wed Mar 14 23:09:54 2007
From: arunm at softwarefx.com (ArunM)
Date: Wed, 14 Mar 2007 15:09:54 -0700 (PDT)
Subject: [R] Data Visualization Search
Message-ID: <9484174.post@talk.nabble.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070314/12db4067/attachment.pl 

From jbmsamba at interchange.ubc.ca  Thu Mar 15 00:24:51 2007
From: jbmsamba at interchange.ubc.ca (Julianno Sambatti)
Date: Wed, 14 Mar 2007 16:24:51 -0700
Subject: [R] Wald test and frailty models in coxph
Message-ID: <B77E87D7-D28A-41BA-A0CE-0D27B22F7EDC@interchange.ubc.ca>

Dear R members,

I am new in using frailty models in survival analyses and am getting  
some contrasting results when I compare the Wald and likelihood ratio  
tests provided by the r output.

I am testing the survivorship of different sunflower interspecific  
crosses using cytoplasm (Cyt), Pollen and the interaction Cyt*Pollen  
as fixed effects, and sub-block  as a random effect.  I stratified  
the analysis by developmental stage (G_stageSM) as an ordered factor  
(two classes). There is a lot of tied deaths in this dataset.

Below is the analysis summary.

coxph(formula = Surv(Death_day, Censor) ~ Pollen * Cyt + strata 
(G_stageSM) +
     frailty(Sub.block), data = SurvNMexpSM)

   n=1422 (1 observation deleted due to missingness)
                    coef    se(coef) se2   Chisq  DF   p
PollenHNA          -0.0966 0.177    0.177   0.30  1.0 5.9e-01
PollenHNP          -0.3160 0.122    0.122   6.65  1.0 9.9e-03
PollenPET          -0.0478 0.120    0.120   0.16  1.0 6.9e-01
CytXA              -0.2967 0.118    0.118   6.36  1.0 1.2e-02
frailty(Sub.block)                        507.64 38.4 0.0e+00
PollenHNA:CytXA     0.2732 0.205    0.205   1.77  1.0 1.8e-01
PollenHNP:CytXA     0.7020 0.169    0.169  17.27  1.0 3.3e-05
PollenPET:CytXA     0.0837 0.207    0.207   0.16  1.0 6.9e-01

                 exp(coef) exp(-coef) lower .95 upper .95
PollenHNA           0.908      1.101     0.641     1.285
PollenHNP           0.729      1.372     0.573     0.927
PollenPET           0.953      1.049     0.753     1.206
CytXA               0.743      1.345     0.590     0.936
PollenHNA:CytXA     1.314      0.761     0.879     1.966
PollenHNP:CytXA     2.018      0.496     1.449     2.810
PollenPET:CytXA     1.087      0.920     0.724     1.632

Iterations: 10 outer, 25 Newton-Raphson
      Variance of random effect= 0.81   I-likelihood = -6513.5
Degrees of freedom for terms=  3.0  1.0 38.4  3.0
Rsquare= 0.365   (max possible= 1 )
Likelihood ratio test= 647  on 45.4 df,   p=0
Wald test            = 20.6  on 45.4 df,   p=1


Although, the results seem to reflect what we observe, it called my  
attention that the Likelihood ratio test and Wald test p-values are  
exactly the opposite.

I performed the same analysis without frailty and obtained

Call:
coxph(formula = Surv(Death_day, Censor) ~ Pollen * Cyt + strata 
(G_stageSM),
     data = SurvNMexpSM)

   n=1422 (1 observation deleted due to missingness)
                    coef exp(coef) se(coef)       z      p
PollenHNA       -0.0193     0.981    0.170 -0.1139 0.9100
PollenHNP       -0.2582     0.772    0.119 -2.1642 0.0300
PollenPET       -0.0555     0.946    0.117 -0.4747 0.6400
CytXA           -0.2123     0.809    0.114 -1.8702 0.0610
PollenHNA:CytXA -0.0135     0.987    0.197 -0.0684 0.9500
PollenHNP:CytXA  0.4358     1.546    0.164  2.6600 0.0078
PollenPET:CytXA  0.0186     1.019    0.202  0.0924 0.9300

                 exp(coef) exp(-coef) lower .95 upper .95
PollenHNA           0.981      1.020     0.703     1.368
PollenHNP           0.772      1.295     0.611     0.976
PollenPET           0.946      1.057     0.752     1.190
CytXA               0.809      1.237     0.647     1.010
PollenHNA:CytXA     0.987      1.014     0.670     1.452
PollenHNP:CytXA     1.546      0.647     1.122     2.132
PollenPET:CytXA     1.019      0.982     0.686     1.513

Rsquare= 0.008   (max possible= 1 )
Likelihood ratio test= 11.3  on 7 df,   p=0.127
Wald test            = 11.3  on 7 df,   p=0.124
Score (logrank) test = 11.4  on 7 df,   p=0.123

Here, the wald and the Likelihood ratio tests seem to be telling the  
same thing

Does anyone have a clue on how to interpret these results?

Thanks

J Berg


From maitra at iastate.edu  Thu Mar 15 00:30:53 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Wed, 14 Mar 2007 18:30:53 -0500
Subject: [R] Connecting R-help and Google Groups?
In-Reply-To: <50d6c72a0703141036y788031d4tfb633f2f20382183@mail.gmail.com>
References: <50d6c72a0703140848h3ca36f94k6d8f847244291553@mail.gmail.com>
	<002401c76653$14104dc0$4d908980@gne.windows.gene.com>
	<50d6c72a0703141036y788031d4tfb633f2f20382183@mail.gmail.com>
Message-ID: <20070314183053.449787a2@subarnarekha.stat.iastate.edu>

I agree with Bert on this one! Any commercial entity's future policies will not be decided by some group's past understanding. Everything can be explained in terms of shareholder value.

I don't see any advantages with tying up to Google groups. We get enough posts every day here to keep us all busy with even a fraction of them. I also think people should be encouraged to follow the policies such as read the basics "An Intro" etc, before running off and posting. Besides, and most importantly, I prefer having statisticians or those interested in statistics applied to their problems discuss their issues and software, and I learn a lot in this mailing list even in lurk mode. I could do without random posters. 

Btw, anyone using R should be encouraged to use RSiteSearch to search this mailing list on some topic.

Best,
Ranjan



On Wed, 14 Mar 2007 13:36:33 -0400 "Paul Lynch" <plynchnlm at gmail.com> wrote:

> Well, I don't see what danger could arise from the fact that Google
> Groups is owned by a company.  Google Groups provides access to all of
> usenet, plus many mailing lists (e.g. the ruby-talk mailing list for
> Ruby programmers).  They don't control any of the newgroups or mailing
> lists that they provide access to.  It is a free service, supported by
> advertising.
> 
> As for the issue of whether there might be future access problems
> (e.g. if Google goes bankrupt, which currently seems unlikely)  R
> users would still have access to the r-help list through the means
> that they have now.  I am not recommending replacing any of the
> current means of access to the r-help list; I am just asking about
> adding an additional means of access.
> 
>       --Paul
> 
> On 3/14/07, Bert Gunter <gunter.berton at gene.com> wrote:
> > I know nothing about Google Groups, but FWIW, I think it would be most
> > unwise for R/CRAN to hook up to **any** commercially sponsored web portals.
> > Future changes in their policies, interfaces,or access conditions may make
> > them inaccessible or unfreindly to R users. So long as we have folks willing
> > and able to host and maintain our lists as part of the CRAN infrastructure,
> > CRAN maintains control. I think this is wise and prudent.
> >
> > I am happy to be educated to the contrary if I misunderstand how this would
> > work.
> >
> > Bert Gunter
> > Genentech Nonclinical Statistics
> > South San Francisco, CA 94404
> > 650-467-7374
> >
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Lynch
> > Sent: Wednesday, March 14, 2007 8:48 AM
> > To: R-help at stat.math.ethz.ch
> > Subject: [R] Connecting R-help and Google Groups?
> >
> > This morning I tried to see if I could find the r-help mailing list on
> > Google Groups, which has an interface that I like.  I found three
> > Google Groups ("The R Project for Statistical Computing", "rproject",
> > and "rhelp") but none of them are connected to the r-help list.
> >
> > Is there perhaps some reason why it wouldn't be a good thing for there
> > to be a connected Google Group?  I think it should be possible to set
> > things up so that a post to the Google Group goes to the r-help
> > mailing list, and vice-versa.
> >
> > Also, does anyone know why the three existing R Google Groups failed
> > to get connected to r-help?  It might require some action on the part
> > of the r-help list administrator.
> >
> > Thanks,
> >     --Paul
> >
> > --
> > Paul Lynch
> > Aquilent, Inc.
> > National Library of Medicine (Contractor)
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> 
> 
> -- 
> Paul Lynch
> Aquilent, Inc.
> National Library of Medicine (Contractor)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From adschai at optonline.net  Thu Mar 15 00:43:17 2007
From: adschai at optonline.net (adschai at optonline.net)
Date: Wed, 14 Mar 2007 23:43:17 +0000 (GMT)
Subject: [R] Question about testing cointegration using Autoregressive
 distributed Model (ADL)
Message-ID: <e167faa030c5.45f88895@optonline.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070314/76179618/attachment.pl 

From genomenet at gmail.com  Thu Mar 15 01:13:51 2007
From: genomenet at gmail.com (genomenet at gmail.com)
Date: Wed, 14 Mar 2007 17:13:51 -0700
Subject: [R] how to understand the interaction of two fixed effects
In-Reply-To: <62259995.20070314165506@gmail.com>
References: <mailman.11.1173524404.760.r-help@stat.math.ethz.ch>
	<62259995.20070314165506@gmail.com>
Message-ID: <1421455126.20070314171351@gmail.com>

Hi There,

I have two questions about how to understand well about the
interaction efffect.

1) Suppose two factors, A and B.
A has n levels, B has m levels.
Why the degree of freedom of interaction effect of A and B ( here I
mean A:B not A*B) is (n-1)*(m-1) not n*m-1?

2) can Lard:Gender be understood as LardGender?

Value=c(709,679,699,657,594,677,592,538,476,508,505,539)
Lard=rep(c("Fresh","Rancid"),each=6)
Gender=rep(c("Male","Male","NONE","NONE","Female","Female"),2)
LardGender=rep(c("FreshMale","FreshNONE","FreshFemale","RancidMale","RancidNONE","RancidFemale"),each=2)
Food=data.frame(Value,Lard, Gender,LardGender)

Why is the following two return outputs of lm() function different?
> lm(Value~Lard+Gender+Lard:Gender,data=Food)

Call:
lm(formula = Value ~ Lard + Gender + Lard:Gender, data = Food)

Coefficients:
          (Intercept)             LardRancid             GenderMale  
                635.5                 -113.5                   58.5  
           GenderNONE  LardRancid:GenderMale  LardRancid:GenderNONE  
                 42.5                  -15.5                  -72.5  

> lm(Value~Lard+Gender+LardGender,data=Food)

Call:
lm(formula = Value ~ Lard + Gender + LardGender, data = Food)

Coefficients:
           (Intercept)              LardRancid              GenderMale  
                 635.5                  -113.5                    43.0  
            GenderNONE     LardGenderFreshMale     LardGenderFreshNONE  
                 -30.0                    15.5                    72.5  
LardGenderRancidFemale    LardGenderRancidMale    LardGenderRancidNONE  
                    NA                      NA                      NA  

Thank you very much!

Fan


From dlvanbrunt at gmail.com  Thu Mar 15 01:22:08 2007
From: dlvanbrunt at gmail.com (David L. Van Brunt, Ph.D.)
Date: Wed, 14 Mar 2007 20:22:08 -0400
Subject: [R] replacing all NA's in a dataframe with zeros...
Message-ID: <d332d3e10703141722i77e921fbmf502f7204f09f1b1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070314/fdb280e5/attachment.pl 

From jasoncbarnhart at msn.com  Thu Mar 15 01:41:31 2007
From: jasoncbarnhart at msn.com (Jason Barnhart)
Date: Wed, 14 Mar 2007 17:41:31 -0700
Subject: [R] replacing all NA's in a dataframe with zeros...
References: <d332d3e10703141722i77e921fbmf502f7204f09f1b1@mail.gmail.com>
Message-ID: <BAY116-DAV16BEC2A73A82DD75A4F765CF720@phx.gbl>

This should work.

> test.df <- data.frame(x1=c(NA,2,3,NA), x2=c(1,2,3,4), 
> x3=c(1,NA,NA,4))
> test.df
  x1 x2 x3
1 NA  1  1
2  2  2 NA
3  3  3 NA
4 NA  4  4

> test.df[is.na(test.df)] <- 1000

> test.df
    x1 x2   x3
1 1000  1    1
2    2  2 1000
3    3  3 1000
4 1000  4    4



The following search string "cran r replace data.frame NA" in Google 
(as US user) yielded some good results (5th and 7th entry), but there 
was another example that explicitly yielded this technique.  I can't 
seem to recall my exact search string.


----- Original Message ----- 
From: "David L. Van Brunt, Ph.D." <dlvanbrunt at gmail.com>
To: "R-Help List" <r-help at stat.math.ethz.ch>
Sent: Wednesday, March 14, 2007 5:22 PM
Subject: [R] replacing all NA's in a dataframe with zeros...


> I've seen how to  replace the NA's in a single column with a data 
> frame
>
> *> mydata$ncigs[is.na(mydata$ncigs)]<-0
>
> *But this is just one column... I have thousands of columns (!) that 
> I need
> to do this, and I can't figure out a way, outside of the dreaded 
> loop, do
> replace all NA's in an entire data frame (all vars) without naming 
> each var
> separately. Yikes.
>
> I'm racking my brain on this, seems like I must be staring at the 
> obvious,
> but it eludes me. Searches have come up CLOSE, but not quite what I 
> need..
>
> Any pointers?
>
> -- 
> ---------------------------------------
> David L. Van Brunt, Ph.D.
> mailto:dlvanbrunt at gmail.com
>
> "If Tyranny and Oppression come to this land, it will be in the 
> guise of
> fighting a foreign enemy."
> --James Madison
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From f.harrell at vanderbilt.edu  Thu Mar 15 01:58:08 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 14 Mar 2007 19:58:08 -0500
Subject: [R] ols Error : missing value where TRUE/FALSE needed
In-Reply-To: <58A2705A-4368-4DDA-9EE1-A0FF7FC4DC7F@freecurricula.org>
References: <58A2705A-4368-4DDA-9EE1-A0FF7FC4DC7F@freecurricula.org>
Message-ID: <45F89A20.5000707@vanderbilt.edu>

Charles Evans wrote:
> I have installed Hmisc and Design.  When I use ols, I get the  
> following error message:
> 
> Error in if (!length(fname) || !any(fname == zname)) { :
> 	missing value where TRUE/FALSE needed
> 
> The model that I am running is:
> 
>  > ecools <- ols(eco$exp ~ eco$age + eco$own + eco$inc + inc2, x=TRUE)

ecools <- ols(exp ~ age + own + inc + inc2, data=eco, x=TRUE)

Watch out for variables named exp but probably OK.

Frank Harrell

> 
> I have tried several other combinations of arguments that take TRUE/ 
> FALSE values, but no luck.
> 
> Ultimately, I am trying to calculate robust standard errors.
> 
> Any help would be appreciated.
> 
> Charles Evans
> Executive Director
> Free Curricula Center
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From milton_ruser at yahoo.com.br  Thu Mar 15 02:45:53 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Wed, 14 Mar 2007 18:45:53 -0700 (PDT)
Subject: [R] reading raw matrix saved with writeBin
Message-ID: <68474.81438.qm@web56615.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070314/9445b873/attachment.pl 

From maitra at iastate.edu  Thu Mar 15 03:09:54 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Wed, 14 Mar 2007 21:09:54 -0500
Subject: [R] reading raw matrix saved with writeBin
In-Reply-To: <68474.81438.qm@web56615.mail.re3.yahoo.com>
References: <68474.81438.qm@web56615.mail.re3.yahoo.com>
Message-ID: <20070314210954.3d7ab1c1@triveni.stat.iastate.edu>

On Wed, 14 Mar 2007 18:45:53 -0700 (PDT) Milton Cezar Ribeiro <milton_ruser at yahoo.com.br> wrote:

> Dear Friends,
> 
> I saved a matrix - which contans values 0 and 1 - using following command:
> writeBin (as.integer(mymatrix), "myfile.raw",  size=1).
> 
> It is working fine and I can see the matrix using photoshop. But now I need
> read the matrices again (in fact I have a thousand of them) as matrix into R but when
> I try something like  mat.dat<-readBin ("myfile.raw",size=1) I can?t access the
> matrix
> 
> Kind regards,
> 
> Miltinho
> 
> __________________________________________________
> 
> 
> 	[[alternative HTML version deleted]]
> 

Look up the help file. There is an explicit example. Basically, you need to tell the file to read in binary.

In fact, I am a little surprised your first command works while writing.

HTH!
Ranjan


From smckinney at bccrc.ca  Thu Mar 15 04:16:42 2007
From: smckinney at bccrc.ca (Steven McKinney)
Date: Wed, 14 Mar 2007 20:16:42 -0700
Subject: [R] replacing all NA's in a dataframe with zeros...
References: <d332d3e10703141722i77e921fbmf502f7204f09f1b1@mail.gmail.com>
Message-ID: <0BE438149FF2254DB4199E2682C8DFEB0235FB56@crcmail1.BCCRC.CA>

Since you can index a matrix or dataframe with
a matrix of logicals, you can use is.na()
to index all the NA locations and replace them
all with 0 in one command.

> mydata.df <- as.data.frame(matrix(sample(c(as.numeric(NA), 1), size = 30, replace = TRUE), nrow = 6))
> mydata.df
  V1 V2 V3 V4 V5
1  1 NA  1  1  1
2  1 NA NA NA  1
3 NA NA  1 NA NA
4 NA NA NA NA  1
5 NA  1 NA NA  1
6  1 NA NA  1  1
> is.na(mydata.df)
     V1    V2    V3    V4    V5
1 FALSE  TRUE FALSE FALSE FALSE
2 FALSE  TRUE  TRUE  TRUE FALSE
3  TRUE  TRUE FALSE  TRUE  TRUE
4  TRUE  TRUE  TRUE  TRUE FALSE
5  TRUE FALSE  TRUE  TRUE FALSE
6 FALSE  TRUE  TRUE FALSE FALSE
> mydata.df[is.na(mydata.df)] <- 0
> mydata.df
  V1 V2 V3 V4 V5
1  1  0  1  1  1
2  1  0  0  0  1
3  0  0  1  0  0
4  0  0  0  0  1
5  0  1  0  0  1
6  1  0  0  1  1
> 

Steven McKinney

Statistician
Molecular Oncology and Breast Cancer Program
British Columbia Cancer Research Centre

email: smckinney at bccrc.ca

tel: 604-675-8000 x7561

BCCRC
Molecular Oncology
675 West 10th Ave, Floor 4
Vancouver B.C. 
V5Z 1L3
Canada




-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch on behalf of David L. Van Brunt, Ph.D.
Sent: Wed 3/14/2007 5:22 PM
To: R-Help List
Subject: [R] replacing all NA's in a dataframe with zeros...
 
I've seen how to  replace the NA's in a single column with a data frame

*> mydata$ncigs[is.na(mydata$ncigs)]<-0

*But this is just one column... I have thousands of columns (!) that I need
to do this, and I can't figure out a way, outside of the dreaded loop, do
replace all NA's in an entire data frame (all vars) without naming each var
separately. Yikes.

I'm racking my brain on this, seems like I must be staring at the obvious,
but it eludes me. Searches have come up CLOSE, but not quite what I need..

Any pointers?

-- 
---------------------------------------
David L. Van Brunt, Ph.D.
mailto:dlvanbrunt at gmail.com

"If Tyranny and Oppression come to this land, it will be in the guise of
fighting a foreign enemy."
--James Madison

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Friedrich.Leisch at stat.uni-muenchen.de  Thu Mar 15 04:45:27 2007
From: Friedrich.Leisch at stat.uni-muenchen.de (Friedrich Leisch)
Date: Thu, 15 Mar 2007 14:45:27 +1100
Subject: [R] Sweave question: prevent expansion of unevaluated reused
 code	chunk
In-Reply-To: <45F8173F.8010906@mdacc.tmc.edu>
References: <45F72D9C.5000307@mdacc.tmc.edu> <45F7D419.7040406@stats.uwo.ca>
	<45F8173F.8010906@mdacc.tmc.edu>
Message-ID: <17912.49495.468280.252730@celebrian.ci.tuwien.ac.at>

>>>>> On Wed, 14 Mar 2007 10:39:43 -0500,
>>>>> Kevin R Coombes (KRC) wrote:

  > Hi,
  > I don't know of a standard way to indicate this; I would have suggested

  > <<combined,expand=FALSE>>

  > (with expand=TRUE the default), except for the fact that Seth Falcon 
  > already suggested the same notation in his response...so I can only 
  > second the motion.


Sounds like a good idea. Currently this is not possible, but it
shouldn't be too hard to add the feature.

Best,
Fritz


From young.stat at gmail.com  Thu Mar 15 05:10:44 2007
From: young.stat at gmail.com (Young Cho)
Date: Wed, 14 Mar 2007 21:10:44 -0700
Subject: [R] timeDate object -  days, months manipulation & arithmetic
Message-ID: <BF96C2CB-827E-41FE-B727-DE79162DB8A0@gmail.com>

Hi,

Thanks so much in advance for your help!

I just started using 'timeDate' object to manipulate daily time  
series data. After reading some documents, I created an object  
'bizday' to do some business-day computation. E.g.

 > bizday = timeSequence 
(from='20010101',to='20070313',by='day',FinCenter
+ ='America/Eastern')
 > bizday = bizday[ isBizday(bizday,holidays=holiday.NYSE(2001:1007)) ]

Now, I can find what month ith business day is in, or add 10 business  
days to it:

 > i = 100
 > bizday[100]
[1] "America/Eastern"
[1] [2001-05-24]
 > format(bizday[i],'%Y%m')
[1] "200105"
 > format(bizday[i+10],'%Y%m')
[1] "200106"

But, I want to get the previous month for the ith business day. for  
the above example, it would be "200104". How can I do that? Are there  
some functions ( or even other pkg) dealing w/ these ? Or, am I doing  
it in a bad way and there is much simpler & clean way of doing stuff  
like this? Any advice will be greatly appreciated.

Young.


From genomenet at gmail.com  Thu Mar 15 00:55:06 2007
From: genomenet at gmail.com (genomenet at gmail.com)
Date: Wed, 14 Mar 2007 16:55:06 -0700
Subject: [R] degree of freedom of interaction effect
Message-ID: <62259995.20070314165506@gmail.com>

Hi There,

Suppose two factors, A and B.
A has n levels, B has m levels.

Why the degree of freedom of interaction effect of A and B ( here I
mean A:B not A*B) is (n-1)*(m-1) not n*m-1?

I know this is a basic statistical question. Please recommend some
good websites or books.

Thank you very much.

Fan


From joe_retzer at yahoo.com  Thu Mar 15 04:40:16 2007
From: joe_retzer at yahoo.com (Joseph Retzer)
Date: Wed, 14 Mar 2007 20:40:16 -0700 (PDT)
Subject: [R] cex in xlab, ylab and zlab of persp
In-Reply-To: <0BE438149FF2254DB4199E2682C8DFEB0235FB56@crcmail1.BCCRC.CA>
Message-ID: <310658.49115.qm@web60311.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070314/4e87b7bb/attachment.pl 

From gavin.simpson at ucl.ac.uk  Thu Mar 15 09:08:42 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 15 Mar 2007 08:08:42 +0000
Subject: [R] replacing all NA's in a dataframe with zeros...
In-Reply-To: <0BE438149FF2254DB4199E2682C8DFEB0235FB56@crcmail1.BCCRC.CA>
References: <d332d3e10703141722i77e921fbmf502f7204f09f1b1@mail.gmail.com>
	<0BE438149FF2254DB4199E2682C8DFEB0235FB56@crcmail1.BCCRC.CA>
Message-ID: <1173946122.3089.11.camel@dhcppc2.my.nat.localnet>

On Wed, 2007-03-14 at 20:16 -0700, Steven McKinney wrote:
> Since you can index a matrix or dataframe with
> a matrix of logicals, you can use is.na()
> to index all the NA locations and replace them
> all with 0 in one command.
> 

A quicker solution, that, IIRC,  was posted to the list by Peter
Dalgaard several years ago is:

sapply(mydata.df, function(x) {x[is.na(x)] <- 0; x}))

Some timings on a larger problem with 100 columns:

> mydata.df <- as.data.frame(matrix(sample(c(as.numeric(NA), 1), 
                             size = 1000*100, replace = TRUE), 
                             nrow = 1000))

> system.time(retval <- sapply(mydata.df, 
                               function(x) {x[is.na(x)] <- 0; x}))
[1] 0.108 0.008 0.120 0.000 0.000

> system.time(mydata.df[is.na(mydata.df)] <- 0)
[1] 2.460 0.028 2.498 0.000 0.000

And a larger problem still, 1000 columns

> mydata.df <- as.data.frame(matrix(sample(c(as.numeric(NA), 1), 
                             size = 1000*1000, replace = TRUE), 
                             nrow = 1000))

> system.time(retval <- sapply(mydata.df, function(x) {x[is.na(x)] <- 0;
x}))
[1] 0.908 0.068 2.657 0.000 0.000
> system.time(mydata.df[is.na(mydata.df)] <- 0)
[1] 43.127  0.332 46.440  0.000  0.000

Profiling mydata.df[is.na(mydata.df)] <- 0 shows that it spends most of
this time subsetting the the individual cells of the data frame in turn
and setting the NA ones to 0.

HTH

G

> > mydata.df <- as.data.frame(matrix(sample(c(as.numeric(NA), 1), size = 30, replace = TRUE), nrow = 6))
> > mydata.df
>   V1 V2 V3 V4 V5
> 1  1 NA  1  1  1
> 2  1 NA NA NA  1
> 3 NA NA  1 NA NA
> 4 NA NA NA NA  1
> 5 NA  1 NA NA  1
> 6  1 NA NA  1  1
> > is.na(mydata.df)
>      V1    V2    V3    V4    V5
> 1 FALSE  TRUE FALSE FALSE FALSE
> 2 FALSE  TRUE  TRUE  TRUE FALSE
> 3  TRUE  TRUE FALSE  TRUE  TRUE
> 4  TRUE  TRUE  TRUE  TRUE FALSE
> 5  TRUE FALSE  TRUE  TRUE FALSE
> 6 FALSE  TRUE  TRUE FALSE FALSE
> > mydata.df[is.na(mydata.df)] <- 0
> > mydata.df
>   V1 V2 V3 V4 V5
> 1  1  0  1  1  1
> 2  1  0  0  0  1
> 3  0  0  1  0  0
> 4  0  0  0  0  1
> 5  0  1  0  0  1
> 6  1  0  0  1  1
> > 
> 
> Steven McKinney
> 
> Statistician
> Molecular Oncology and Breast Cancer Program
> British Columbia Cancer Research Centre
> 
> email: smckinney at bccrc.ca
> 
> tel: 604-675-8000 x7561
> 
> BCCRC
> Molecular Oncology
> 675 West 10th Ave, Floor 4
> Vancouver B.C. 
> V5Z 1L3
> Canada
> 
> 
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch on behalf of David L. Van Brunt, Ph.D.
> Sent: Wed 3/14/2007 5:22 PM
> To: R-Help List
> Subject: [R] replacing all NA's in a dataframe with zeros...
>  
> I've seen how to  replace the NA's in a single column with a data frame
> 
> *> mydata$ncigs[is.na(mydata$ncigs)]<-0
> 
> *But this is just one column... I have thousands of columns (!) that I need
> to do this, and I can't figure out a way, outside of the dreaded loop, do
> replace all NA's in an entire data frame (all vars) without naming each var
> separately. Yikes.
> 
> I'm racking my brain on this, seems like I must be staring at the obvious,
> but it eludes me. Searches have come up CLOSE, but not quite what I need..
> 
> Any pointers?
> 
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT                          [w] http://www.freshwaters.org.uk/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From P.Dalgaard at biostat.ku.dk  Thu Mar 15 10:21:22 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 15 Mar 2007 10:21:22 +0100
Subject: [R] replacing all NA's in a dataframe with zeros...
In-Reply-To: <1173946122.3089.11.camel@dhcppc2.my.nat.localnet>
References: <d332d3e10703141722i77e921fbmf502f7204f09f1b1@mail.gmail.com>	<0BE438149FF2254DB4199E2682C8DFEB0235FB56@crcmail1.BCCRC.CA>
	<1173946122.3089.11.camel@dhcppc2.my.nat.localnet>
Message-ID: <45F91012.7030603@biostat.ku.dk>

Gavin Simpson wrote:
> On Wed, 2007-03-14 at 20:16 -0700, Steven McKinney wrote:
>   
>> Since you can index a matrix or dataframe with
>> a matrix of logicals, you can use is.na()
>> to index all the NA locations and replace them
>> all with 0 in one command.
>>
>>     
>
> A quicker solution, that, IIRC,  was posted to the list by Peter
> Dalgaard several years ago is:
>
> sapply(mydata.df, function(x) {x[is.na(x)] <- 0; x}))
>   
I hope your memory fails you, because it doesn't actually work.....

> sapply(test.df, function(x) {x[is.na(x)] <- 0; x})
     x1 x2 x3
[1,]  0  1  1
[2,]  2  2  0
[3,]  3  3  0
[4,]  0  4  4

is a matrix, not a data frame.

Instead:

> test.df[] <- lapply(test.df, function(x) {x[is.na(x)] <- 0; x})
> test.df
  x1 x2 x3
1  0  1  1
2  2  2  0
3  3  3  0
4  0  4  4

Speedwise, sapply() is doing lapply() internally, and the assignment
overhead should be small, so I'd expect similar timings.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From csardi at rmki.kfki.hu  Thu Mar 15 10:59:23 2007
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Thu, 15 Mar 2007 10:59:23 +0100
Subject: [R] Connecting R-help and Google Groups?
In-Reply-To: <20070314183053.449787a2@subarnarekha.stat.iastate.edu>
References: <50d6c72a0703140848h3ca36f94k6d8f847244291553@mail.gmail.com>
	<002401c76653$14104dc0$4d908980@gne.windows.gene.com>
	<50d6c72a0703141036y788031d4tfb633f2f20382183@mail.gmail.com>
	<20070314183053.449787a2@subarnarekha.stat.iastate.edu>
Message-ID: <20070315095922.GA8662@guzu>

On Wed, Mar 14, 2007 at 06:30:53PM -0500, Ranjan Maitra wrote:
> I agree with Bert on this one! Any commercial entity's future policies will not be decided by some group's past understanding. Everything can be explained in terms of shareholder value.
> 

Personally, i would go even further. I think it is very dangeorous to
allow google and other search engines to search the R web pages and 
mailing lists, who knows what they'll do with this information some day,
or maybe they're already doing it right now!
All search engines should be banned from these sites IMHO. 

Personally i wouldn't even allow to download these pages with
Internet Explorer, Safari, Netscape, Firefox or any commercial 
web browser.

Also, the R team shouldn't support R on any commercial platforms like
MS Windows, OSX, Redhat & Debian Linux, etc. There is a considerable 
chance that the policies of these companies will affect the R community 
negatively.

> I don't see any advantages with tying up to Google groups. We get enough posts every day here to keep us all busy with even a fraction of them. I also think people should be encouraged to follow the policies such as read the basics "An Intro" etc, before running off and posting. Besides, and most importantly, I prefer having statisticians or those interested in statistics applied to their problems discuss their issues and software, and I learn a lot in this mailing list even in lurk mode. I could do without random posters. 

I completely agree. The R user community is big enough right now, and 
it is clear that we want no more newbie's with annoying questions.
Nor want we more statistics professors, it is our right and duty to 
answer all the questions about R!

> Btw, anyone using R should be encouraged to use RSiteSearch to search this mailing list on some topic.

Yes. I don't even understand why it is possible to search the mailing list
via a web form. It would be a great user filter to stop this service!
Only advanced users who are able to install R and find RSiteSearch 
should be allowed to see the posts. As for posting i recommend to set up
a test with questions about statistics, programming and other relevant
topics and only users passing the test would be allowed to submit posts
to the lists. It would be also wise to make them repeat the test 
every year, just in case they forget something.

Best,
Gabor

> Best,
> Ranjan
> 
> 
> 
> On Wed, 14 Mar 2007 13:36:33 -0400 "Paul Lynch" <plynchnlm at gmail.com> wrote:
> 
> > Well, I don't see what danger could arise from the fact that Google
> > Groups is owned by a company.  Google Groups provides access to all of
> > usenet, plus many mailing lists (e.g. the ruby-talk mailing list for
> > Ruby programmers).  They don't control any of the newgroups or mailing
> > lists that they provide access to.  It is a free service, supported by
> > advertising.
> > 
> > As for the issue of whether there might be future access problems
> > (e.g. if Google goes bankrupt, which currently seems unlikely)  R
> > users would still have access to the r-help list through the means
> > that they have now.  I am not recommending replacing any of the
> > current means of access to the r-help list; I am just asking about
> > adding an additional means of access.
> > 
> >       --Paul
> > 
> > On 3/14/07, Bert Gunter <gunter.berton at gene.com> wrote:
> > > I know nothing about Google Groups, but FWIW, I think it would be most
> > > unwise for R/CRAN to hook up to **any** commercially sponsored web portals.
> > > Future changes in their policies, interfaces,or access conditions may make
> > > them inaccessible or unfreindly to R users. So long as we have folks willing
> > > and able to host and maintain our lists as part of the CRAN infrastructure,
> > > CRAN maintains control. I think this is wise and prudent.
> > >
> > > I am happy to be educated to the contrary if I misunderstand how this would
> > > work.
> > >
> > > Bert Gunter
> > > Genentech Nonclinical Statistics
> > > South San Francisco, CA 94404
> > > 650-467-7374
> > >
> > >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Lynch
> > > Sent: Wednesday, March 14, 2007 8:48 AM
> > > To: R-help at stat.math.ethz.ch
> > > Subject: [R] Connecting R-help and Google Groups?
> > >
> > > This morning I tried to see if I could find the r-help mailing list on
> > > Google Groups, which has an interface that I like.  I found three
> > > Google Groups ("The R Project for Statistical Computing", "rproject",
> > > and "rhelp") but none of them are connected to the r-help list.
> > >
> > > Is there perhaps some reason why it wouldn't be a good thing for there
> > > to be a connected Google Group?  I think it should be possible to set
> > > things up so that a post to the Google Group goes to the r-help
> > > mailing list, and vice-versa.
> > >
> > > Also, does anyone know why the three existing R Google Groups failed
> > > to get connected to r-help?  It might require some action on the part
> > > of the r-help list administrator.
> > >
> > > Thanks,
> > >     --Paul
> > >
> > > --
> > > Paul Lynch
> > > Aquilent, Inc.
> > > National Library of Medicine (Contractor)
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> > 
> > 
> > -- 
> > Paul Lynch
> > Aquilent, Inc.
> > National Library of Medicine (Contractor)
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From r.nieuwenhuis at student.ru.nl  Thu Mar 15 11:10:23 2007
From: r.nieuwenhuis at student.ru.nl (Rense Nieuwenhuis)
Date: Thu, 15 Mar 2007 11:10:23 +0100
Subject: [R] fitting of all possible models
In-Reply-To: <FE8C160D1505B24497FA7C78D4DADACA0478FC@EA-MAIL.eawag.wroot.emp-eaw.ch>
References: <FE8C160D1505B24497FA7C78D4DADACA0478FC@EA-MAIL.eawag.wroot.emp-eaw.ch>
Message-ID: <488EFAAF-938F-410F-8387-3B6825170008@student.ru.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070315/aedb44ad/attachment.pl 

From j.van_den_hoff at fzd.de  Thu Mar 15 11:30:20 2007
From: j.van_den_hoff at fzd.de (Joerg van den Hoff)
Date: Thu, 15 Mar 2007 11:30:20 +0100
Subject: [R] replacing all NA's in a dataframe with zeros...
In-Reply-To: <45F91012.7030603@biostat.ku.dk>
References: <d332d3e10703141722i77e921fbmf502f7204f09f1b1@mail.gmail.com>
	<0BE438149FF2254DB4199E2682C8DFEB0235FB56@crcmail1.BCCRC.CA>
	<1173946122.3089.11.camel@dhcppc2.my.nat.localnet>
	<45F91012.7030603@biostat.ku.dk>
Message-ID: <20070315103020.GB13022@marco.fz-rossendorf.de>

On Thu, Mar 15, 2007 at 10:21:22AM +0100, Peter Dalgaard wrote:
> Gavin Simpson wrote:
> > On Wed, 2007-03-14 at 20:16 -0700, Steven McKinney wrote:
> >   
> >> Since you can index a matrix or dataframe with
> >> a matrix of logicals, you can use is.na()
> >> to index all the NA locations and replace them
> >> all with 0 in one command.
> >>
> >>     
> >
> > A quicker solution, that, IIRC,  was posted to the list by Peter
> > Dalgaard several years ago is:
> >
> > sapply(mydata.df, function(x) {x[is.na(x)] <- 0; x}))
> >   
> I hope your memory fails you, because it doesn't actually work.....
> 
> > sapply(test.df, function(x) {x[is.na(x)] <- 0; x})
>      x1 x2 x3
> [1,]  0  1  1
> [2,]  2  2  0
> [3,]  3  3  0
> [4,]  0  4  4
> 
> is a matrix, not a data frame.
> 
> Instead:
> 
> > test.df[] <- lapply(test.df, function(x) {x[is.na(x)] <- 0; x})
> > test.df
>   x1 x2 x3
> 1  0  1  1
> 2  2  2  0
> 3  3  3  0
> 4  0  4  4
> 
> Speedwise, sapply() is doing lapply() internally, and the assignment
> overhead should be small, so I'd expect similar timings.

just an idea:
given the order of magnitude difference (factor 17 or so) in runtime 
between the "obvious" solution and the fast one: would'nt it be possible/sensible
to modify the corresponding subsetting method ("[.data.frame") such that it
recognizes the case when it is called with an arbitrary index matrix (the
problem is not restricted to indexing with a logical matrix, I presume?) and
switch internally to the fast solution given above?

in my (admittedly limited) experience it seems that one of the not so nice
properties of R is that one encounters in quite a few situations exactly the above
situation: unexpected massive differences in run time between different solutions (I'm not
talking about "explicit loop penalty"). what concerns me most, are the very
basic scenarios (not complex algorithms): data frames vs. matrices, naming
vector components or not, subsetting, read.table vs. scan, etc. if their were a
concise HOW TO list for the cases "when speed matters", that would be helpful,
too.

I understand that part of the "uneven performance" is unavoidable and one must
expect the user to go to the trouble to understand the reasons, e.g. for
differences between handling purely numerical data in either matrices or data
frames. but a factor of 17 between the obvious approach and the wise one seems
a trap in which 99% of the people will step (probably never thinking that their
might be a faster approach).

joerg


From gavin.simpson at ucl.ac.uk  Thu Mar 15 11:31:09 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 15 Mar 2007 10:31:09 +0000
Subject: [R] replacing all NA's in a dataframe with zeros...
In-Reply-To: <45F91012.7030603@biostat.ku.dk>
References: <d332d3e10703141722i77e921fbmf502f7204f09f1b1@mail.gmail.com>
	<0BE438149FF2254DB4199E2682C8DFEB0235FB56@crcmail1.BCCRC.CA>
	<1173946122.3089.11.camel@dhcppc2.my.nat.localnet>
	<45F91012.7030603@biostat.ku.dk>
Message-ID: <1173954669.16013.6.camel@gsimpson.geog.ucl.ac.uk>

On Thu, 2007-03-15 at 10:21 +0100, Peter Dalgaard wrote:
> Gavin Simpson wrote:
> > On Wed, 2007-03-14 at 20:16 -0700, Steven McKinney wrote:
> >   
> >> Since you can index a matrix or dataframe with
> >> a matrix of logicals, you can use is.na()
> >> to index all the NA locations and replace them
> >> all with 0 in one command.
> >>
> >>     
> >
> > A quicker solution, that, IIRC,  was posted to the list by Peter
> > Dalgaard several years ago is:
> >
> > sapply(mydata.df, function(x) {x[is.na(x)] <- 0; x}))
> >   
> I hope your memory fails you, because it doesn't actually work.....

Ah, yes, apologies Peter. I have the sapply version embedded in a
package function that I happened to be working on (where I wanted the
result to be a matrix) and pasted directly from there and not my crib
sheet of useful R-help snippets where I do have it as lapply(...). I'd
forgotten I'd changed Peter's suggestion slightly in my function.

That'll teach me to reply before my morning cup of Earl Grey.

All the best,

G

> 
> > sapply(test.df, function(x) {x[is.na(x)] <- 0; x})
>      x1 x2 x3
> [1,]  0  1  1
> [2,]  2  2  0
> [3,]  3  3  0
> [4,]  0  4  4
> 
> is a matrix, not a data frame.
> 
> Instead:
> 
> > test.df[] <- lapply(test.df, function(x) {x[is.na(x)] <- 0; x})
> > test.df
>   x1 x2 x3
> 1  0  1  1
> 2  2  2  0
> 3  3  3  0
> 4  0  4  4
> 
> Speedwise, sapply() is doing lapply() internally, and the assignment
> overhead should be small, so I'd expect similar timings.
> 
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From hb at stat.berkeley.edu  Thu Mar 15 11:32:33 2007
From: hb at stat.berkeley.edu (Henrik Bengtsson)
Date: Thu, 15 Mar 2007 11:32:33 +0100
Subject: [R] reading raw matrix saved with writeBin
In-Reply-To: <20070314210954.3d7ab1c1@triveni.stat.iastate.edu>
References: <68474.81438.qm@web56615.mail.re3.yahoo.com>
	<20070314210954.3d7ab1c1@triveni.stat.iastate.edu>
Message-ID: <59d7961d0703150332m375ab261tf00c07c077b53d1c@mail.gmail.com>

FYI, to save data as bitmap images, see the EBImage package on Bioconductor.

/H

On 3/15/07, Ranjan Maitra <maitra at iastate.edu> wrote:
> On Wed, 14 Mar 2007 18:45:53 -0700 (PDT) Milton Cezar Ribeiro <milton_ruser at yahoo.com.br> wrote:
>
> > Dear Friends,
> >
> > I saved a matrix - which contans values 0 and 1 - using following command:
> > writeBin (as.integer(mymatrix), "myfile.raw",  size=1).
> >
> > It is working fine and I can see the matrix using photoshop. But now I need
> > read the matrices again (in fact I have a thousand of them) as matrix into R but when
> > I try something like  mat.dat<-readBin ("myfile.raw",size=1) I can?t access the
> > matrix
> >
> > Kind regards,
> >
> > Miltinho
> >
> > __________________________________________________
> >
> >
> >       [[alternative HTML version deleted]]
> >
>
> Look up the help file. There is an explicit example. Basically, you need to tell the file to read in binary.
>
> In fact, I am a little surprised your first command works while writing.
>
> HTH!
> Ranjan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gyadav at ccilindia.co.in  Thu Mar 15 12:21:20 2007
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Thu, 15 Mar 2007 16:51:20 +0530
Subject: [R] regarding cointegration
In-Reply-To: <20070315095922.GA8662@guzu>
Message-ID: <OF39754890.4F7BF93E-ON6525729F.00374049-6525729F.003E4F85@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070315/71acfa50/attachment.pl 

From olivier.eterradossi at ema.fr  Thu Mar 15 13:07:45 2007
From: olivier.eterradossi at ema.fr (Olivier ETERRADOSSI)
Date: Thu, 15 Mar 2007 13:07:45 +0100
Subject: [R] What happened to the rpm package ?
Message-ID: <45F93711.5020701@ema.fr>

Hi List,
sorry if this is a FAQ : I could not make my way to it :-(

Once upon a time it happened to be a package named "rpm" for "Robust 
Point Matching".
I can find a few traces of it in the CRAN archives : works by Saussen 
and al. (Aligning spectra with R), but can't find the package anymore.
No trace at all in the "Old" or "Orphaned" lists....
Does one UseR know about it, or even use it ?

Thanks in advance, and regards to all. Olivier

-- 
Olivier ETERRADOSSI
Ma?tre-Assistant
CMGD / Equipe "Propri?t?s Psycho-Sensorielles des Mat?riaux"
Ecole des Mines d'Al?s
H?lioparc, 2 av. P. Angot, F-64053 PAU CEDEX 9
tel std: +33 (0)5.59.30.54.25
tel direct: +33 (0)5.59.30.90.35 
fax: +33 (0)5.59.30.63.68
http://www.ema.fr


From eliezg at u.washington.edu  Thu Mar 15 13:09:42 2007
From: eliezg at u.washington.edu (Eli Gurarie)
Date: Thu, 15 Mar 2007 05:09:42 -0700
Subject: [R] Creating q and p functions from a self-defined distribution
Message-ID: <45F93786.7000405@u.washington.edu>

Hello all,

I am fishing for some suggestions on efficient ways to make qdist and 
pdist type functions from an arbitrary distribution whose probability 
density function I've defined myself.

For example, let's say I have a distribution whose pdf is:

dRN <- function(x,d,v,s)
# d, v, and s are parameters
    return(d/x^2/sv/sqrt(2*pi)*exp(-(d-v*x)^2/2/(sv^2*x^2)))

this is a legitimate distribution over the reals (though it has a 
singularity at x=0)

at the moment, to get a "p" value, I sum the dRN over some arbitrary 
interval dt:

pRN<-function(x,d,v,s,dt=.001)
{
   xs<-seq(0,x,dt)
   dRN<-dRN(xs,d,v,s)
   return(sum(dRN)*dt)
}

which is OK, except I sometimes want a vector of p values for a vector 
of x's, say: pRN(xs=seq(0,10,.1) ... ) which involves an unwieldly loop.

Getting a quantile value is a real nightmare!  Right now I have a thing 
that involves using approx() and matching rounded numbers and requires a 
loop and is slow.

It seems surprising that it would be so hard to invert a function that 
is perfectly defined!

Are there packages/functions/algorithms that allow one to manipulate 
arbitrarily defined distributions?

Any suggestions appreciated,
Thanks,
   Eli




*************************************************
  Eli Gurarie
  Quantitative Ecology and Resource Management
  University of Washington, Seattle


From lhill07 at qub.ac.uk  Thu Mar 15 13:19:28 2007
From: lhill07 at qub.ac.uk (Laura Hill)
Date: Thu, 15 Mar 2007 12:19:28 +0000
Subject: [R] expm() within the Matrix package
Message-ID: <C21EEA50.DCA%lhill07@qub.ac.uk>

Hi

Could anybody give me a bit of advice on some code I'm having trouble with?

I've been trying to calculate the loglikelihood of a function iterated over
a data of time values and I seem to be experiencing difficulty when I use
the function expm(). Here's an example of what I am trying to do


y<-c(5,10)      #vector of 2 survival times

p<-Matrix(c(1,0),1,2)   #1x2 matrix

Q<-Matrix(c(1,2,3,4),2,2)   #2x2 matrix

q<-Matrix(c(1,2),2,1)       #2x1 matrix

Loglik<-rep(0,length(y))    #creating empty vector of same length as y

    for(i in 1:length(y)){

        Loglik[i]<-log((p %*% (expm(Q*y[i]))) %*% q)  #calculating
                   
                                        #  Loglikelihood for each y[i]
        }

The code works perfectly if I use exp(Q*y[i]) but not for expm()


If anyone has any advice they could give that would be great.
I would like to thank Gad Abraham also for helping me get this far.

Thanks in advance

Laura


From roger.bos at us.rothschild.com  Thu Mar 15 13:18:33 2007
From: roger.bos at us.rothschild.com (Bos, Roger)
Date: Thu, 15 Mar 2007 08:18:33 -0400
Subject: [R] Occasional problems with starting batch mode
Message-ID: <D8C95B444AD6EE4AAD638D818A9CFD343A2017@RINNYCSE000.rth.ad.rothschild.com>

I use a windows Server 2003 machine to run R code in batch mode every
night using the following command:

"F:\Program Files\R\R-2.4.1pat\bin\R.exe" CMD BATCH --vanilla --slave
"batch_master_dr.R"

This produces an output file, of which the first three lines look like
this:

Loading required package: tcltk
Loading Tcl/Tk interface ... done
Loading required package: R2HTML

99% of the time this works great.  Every once in a while, I get the
following error message instead and it does not run any of my subsequent
code:

Loading required package: tcltk
Loading Tcl/Tk interface ... Unable to register TkTopLevel class

This application has requested the Runtime to terminate it in an unusual
way.
Please contact the application's support team for more information.

Does anyone have any clue where I should look to fix this problem?

Thanks,

Roger
                                       
version.string R version 2.4.1 Patched (2007-02-04 r40647)

********************************************************************** * 
This message is for the named person's use only. It may 
contain confidential, proprietary or legally privileged 
information. No right to confidential or privileged treatment 
of this message is waived or lost by any error in 
transmission. If you have received this message in error, 
please immediately notify the sender by e-mail, 
delete the message and all copies from your system and destroy 
any hard copies. You must not, directly or indirectly, use, 
disclose, distribute, print or copy any part of this message 
if you are not the intended recipient.


From maitra at iastate.edu  Thu Mar 15 13:50:41 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Thu, 15 Mar 2007 07:50:41 -0500
Subject: [R] regarding cointegration
In-Reply-To: <OF39754890.4F7BF93E-ON6525729F.00374049-6525729F.003E4F85@ccilindia.co.in>
References: <20070315095922.GA8662@guzu>
	<OF39754890.4F7BF93E-ON6525729F.00374049-6525729F.003E4F85@ccilindia.co.in>
Message-ID: <20070315075041.2bb3bacb@triveni.stat.iastate.edu>

Please do not mess up the thread by posting as a reply to some other topic.

Thanks,
Ranjan


On Thu, 15 Mar 2007 16:51:20 +0530 gyadav at ccilindia.co.in wrote:

> 
> Hi All
> 
> Thanks for supporting people like me. 
> What is cointegration and its connection with granger causality test ? 
> what is its use and mathematical methodology behind it. Secondly, is 
> cointegration test like  "Phillips-Ouliaris Cointegration Test" of tseries 
> package or of urca package is the same as cointegration ? Please tell me 
> how to go about it and interpret the results ? 
> 
> Thanks in advance
> cheers :-)
> -gaurav
> 
> ============================================================================================
> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and ...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sebastien.boutry at hotmail.fr  Thu Mar 15 11:11:19 2007
From: sebastien.boutry at hotmail.fr (sebastien boutry)
Date: Thu, 15 Mar 2007 11:11:19 +0100
Subject: [R] Mauchly.test
Message-ID: <BAY129-F241625ABD96738488E6BD2E3720@phx.gbl>

Bonjour,

elles correspondent ? des donn?es de diff?rents traitements (EU) avec des 
dates diff?rentes
j'aimerai pouvoir faire une comparaison de k moyennes selon ces deux 
variables date et EU
ANOVA ? n facteurs n'est pas la bonne solution
je voudrai savoir qu'elles sont les tests qui permettent de r?soudre ce 
probl?me (?chantillons appari?s)
je suis partis sur le test de Mauchley sans r?ussite
pouvez vous m'aider
merci
S?bastien

Hello,
i would like to compare the k means of DW with two parameters (date and 
traitement) but i don't use ANOVA with n factors (aov) because  i have a 
link between wk2 and wk1, wk3 and wk1-wk2 ... (matched samples).
if you know the test that i can use (Mauchly.test)
thanks.
S?bastien

Voici mes donn?es:
date	EU	DW
wk1	EU1	5.324547829
wk1	EU1	7.321253265
wk1	EU1	4.431712065
wk1	EU2	8.230322407
wk1	EU2	8.546873269
wk1	EU2	5.657332069
wk1	EU3	3.165508618
wk1	EU3	4.431712065
wk1	EU3	1.899305171
wk2	EU1	2.163097556
wk2	EU1	17.61379438
wk2	EU1	15.82754309
wk2	EU2	16.46064481
wk2	EU2	19.30960257
wk2	EU2	13.92823792
wk2	EU3	6.014466374
wk2	EU3	7.280669822
wk2	EU3	5.064813789
wk4	EU1	11.03179753
wk4	EU1	29.75578101
wk4	EU1	22.71252433
wk4	EU2	27.85647584
wk4	EU2	36.71989997
wk4	EU2	20.11680727
wk4	EU3	13.59661321
wk4	EU3	13.2951362
wk4	EU3	14.56133964
wk6	EU1	30.73875474
wk6	EU1	33.27842393
wk6	EU1	35.27512937
wk6	EU2	31.2817185
wk6	EU2	41.53147307
wk6	EU2	35.57093758
wk6	EU3	8.652390223
wk6	EU3	18.6359174
wk6	EU3	15.30807501

_________________________________________________________________
Windows Live Spaces : cr?ez votre blog ? votre image !


From morach at cfigroup.ch  Thu Mar 15 11:30:56 2007
From: morach at cfigroup.ch (Sascha Morach)
Date: Thu, 15 Mar 2007 11:30:56 +0100
Subject: [R] Hardware for a new Workstation for best performance using R
Message-ID: <PIEALPNOIGOLCPEIJGMEOEGDCFAA.morach@cfigroup.ch>

Hi,

we are looking for a new workstation for big datasets/applications (easily
up to 100'000 records and up to 300 Variables) using R. As an example:
Variable Selection for a multivariate regression using stepAIC.

What is the best configuration for a workstation to reach a high performance
level for computing with R?

Single core or multi core (is R together with nws package really able to use
advantage of multi core processors, any experience/benchmarks on that)?

Shall we use Linux instead of Windows? If yes, how is the compatibility of
graphics computed on Linux if we like to use them after on windows? And what
are the advantages using Linux instead of Windows?

What kind of workstations are you using (hardware and operating system) for
big data computations? And are you satisfied with it?

I'm quite familiar with pc or server hardware.

Thanks in advance

Sascha Morach


From ted.harding at nessie.mcc.ac.uk  Thu Mar 15 14:26:52 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 15 Mar 2007 13:26:52 -0000 (GMT)
Subject: [R] Creating q and p functions from a self-defined distribut
In-Reply-To: <45F93786.7000405@u.washington.edu>
Message-ID: <XFMail.070315132652.ted.harding@nessie.mcc.ac.uk>

On 15-Mar-07 12:09:42, Eli Gurarie wrote:
> Hello all,
> 
> I am fishing for some suggestions on efficient ways to make qdist and 
> pdist type functions from an arbitrary distribution whose probability 
> density function I've defined myself.
> 
> For example, let's say I have a distribution whose pdf is:
> 
> dRN <- function(x,d,v,s)
># d, v, and s are parameters
>     return(d/x^2/sv/sqrt(2*pi)*exp(-(d-v*x)^2/2/(sv^2*x^2)))
> 
> this is a legitimate distribution over the reals (though it has a 
> singularity at x=0)
> [...]
> It seems surprising that it would be so hard to invert a function that 
> is perfectly defined!
> 
> Are there packages/functions/algorithms that allow one to manipulate 
> arbitrarily defined distributions?

Do not be surprised! The Normal distribution function itself, with
pdf (1/(sv*sqrt(1*pi)))*exp(-((x - mu)^2)/(2*sv^2)), is perfectly
well defined. Yet the literature of computation over decades has
presented procedure after procedure for computing the cumulative
fucntion, and its inverse, to desired precision. None of these is
simple. Indeed, (to use your own word), the Normal distribution
is a "nightmare" from this point of view.

So being well-defined is no guarantee that computing its p and q
values will be a simple or easy problem. And what kind of method
is good for a particular distribution will depend strongly on
what the distribution is (for instance, whether it has tails
which tend rapidly to 0 like the Normal, whether there are good
asymptotic formulae, etc.).

In the case of the example you give above, the transformation
from x to u = 1/x will translate it into a Normal distribution
problem, after which you can use (circumspectly ... ) the R
functions pnorm and qnorm (which are based on the best from
the above literature) to deal with it.

But you give it only as an example ... and you are asking for
methods for a general user-defined distribution. For the reasons
given above, a good general-purpose method is unlikely to exist.

With best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 15-Mar-07                                       Time: 13:26:27
------------------------------ XFMail ------------------------------


From parrinel at med.unibs.it  Thu Mar 15 14:43:00 2007
From: parrinel at med.unibs.it (Giovanni Parrinello)
Date: Thu, 15 Mar 2007 14:43:00 +0100
Subject: [R] Error in upgrade
Message-ID: <45F94D64.3010509@med.unibs.it>

Dear All,
update.packages(ask='graphics')
--- Please select a CRAN mirror for use in this session ---
Error in .readRDS(pfile) : unknown input format.
???
TIA
Giovanni

-- 
dr. Giovanni Parrinello
Department of Biotecnologies
Medical Statistics Unit
University of Brescia
Viale Europa, 11 25123 Brescia
email: parrinel at med.unibs.it
Phone: +390303717528
Fax: +390303717488


From clists at perrin.socsci.unc.edu  Thu Mar 15 14:48:42 2007
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Thu, 15 Mar 2007 09:48:42 -0400 (EDT)
Subject: [R] Hardware for a new Workstation for best performance using R
In-Reply-To: <PIEALPNOIGOLCPEIJGMEOEGDCFAA.morach@cfigroup.ch>
References: <PIEALPNOIGOLCPEIJGMEOEGDCFAA.morach@cfigroup.ch>
Message-ID: <Pine.LNX.4.64.0703150945480.13098@perrin.socsci.unc.edu>

I can speak to some of these issues. I don't know about how much benefit 
you can get from SMP for *single* instances of R, though.

1.) Multicore will be helpful, at least, if you are running several 
instances of R at once.  So, for example, if you have people running two 
different models at the same time, the OS can use separate processors or 
cores for each instance.

2.) Yes, by all means you should use linux instead of windows. The 
graphics output is completely compatible with whatever applications you 
want to paste them into on Windows.  Linux is cheaper, stabler, and better 
at using the system's resources.

3.) If you're doing big datasets, you certainly need a 64-bit processor, 
operating system, and R.  Consider, perhaps, a dual-Athlon XP 64 machine 
with a big pile of RAM?

Andy

----------------------------------------------------------------------
Andrew J Perrin - andrew_perrin (at) unc.edu - http://perrin.socsci.unc.edu
Assistant Professor of Sociology; Book Review Editor, _Social Forces_
University of North Carolina - CB#3210, Chapel Hill, NC 27599-3210 USA
New Book: http://www.press.uchicago.edu/cgi-bin/hfs.cgi/00/178592.ctl



On Thu, 15 Mar 2007, Sascha Morach wrote:

> Hi,
>
> we are looking for a new workstation for big datasets/applications (easily
> up to 100'000 records and up to 300 Variables) using R. As an example:
> Variable Selection for a multivariate regression using stepAIC.
>
> What is the best configuration for a workstation to reach a high performance
> level for computing with R?
>
> Single core or multi core (is R together with nws package really able to use
> advantage of multi core processors, any experience/benchmarks on that)?
>
> Shall we use Linux instead of Windows? If yes, how is the compatibility of
> graphics computed on Linux if we like to use them after on windows? And what
> are the advantages using Linux instead of Windows?
>
> What kind of workstations are you using (hardware and operating system) for
> big data computations? And are you satisfied with it?
>
> I'm quite familiar with pc or server hardware.
>
> Thanks in advance
>
> Sascha Morach
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jr_frrr at yahoo.de  Thu Mar 15 17:00:54 2007
From: jr_frrr at yahoo.de (=?ISO-8859-1?Q?Jos=E9?= Rafael Ferrer Paris)
Date: Thu, 15 Mar 2007 12:00:54 -0400
Subject: [R] cex in xlab, ylab and zlab of persp
In-Reply-To: <310658.49115.qm@web60311.mail.yahoo.com>
References: <310658.49115.qm@web60311.mail.yahoo.com>
Message-ID: <1173974454.28564.3.camel@localhost.localdomain>

I had similar problems, actually it is very difficult to customize persp
graphics, you should try wireframe in lattice instead. 
This reference might help to customize the wireframe plots: 
http://www.polisci.ohio-state.edu/faculty/lkeele/3dinR.pdf

JR

El mi?, 14-03-2007 a las 20:40 -0700, Joseph Retzer escribi?:
> I've had no success using what I think is the correct code to change the default size of the x, y and z labels in a persp graph (i.e. cex.lab).   Is there a particular specification to accomplish this?
> Many thanks,
> Joe Retzer
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From sundar.dorai-raj at pdf.com  Thu Mar 15 17:04:43 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 15 Mar 2007 09:04:43 -0700
Subject: [R] Error in upgrade
In-Reply-To: <45F94D64.3010509@med.unibs.it>
References: <45F94D64.3010509@med.unibs.it>
Message-ID: <45F96E9B.7060407@pdf.com>



Giovanni Parrinello said the following on 3/15/2007 6:43 AM:
> Dear All,
> update.packages(ask='graphics')
> --- Please select a CRAN mirror for use in this session ---
> Error in .readRDS(pfile) : unknown input format.
> ???
> TIA
> Giovanni
> 

I cannot replicate this in R-2.4.1. What version of R and repository are 
you using?

update.packages(ask = "graphics", repos = "http://cran.cnr.berkeley.edu")

--sundar


From ripley at stats.ox.ac.uk  Thu Mar 15 17:14:44 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 15 Mar 2007 16:14:44 +0000 (GMT)
Subject: [R] Redirecting output to the screen
In-Reply-To: <001301c7666d$e4f68ca0$0b00a8c0@DBNXY511>
References: <001301c7666d$e4f68ca0$0b00a8c0@DBNXY511>
Message-ID: <Pine.LNX.4.64.0703151603130.8025@gannet.stats.ox.ac.uk>

This is much easier in R-devel: just use message() and scan("stdin").

gannet% cat Test.R
message("Enter file name: ", appendLF=FALSE)
fn <- scan("stdin", what="", n=1)

works for me in R-devel via  R --vanilla -f Test.R > Rout.txt
I believe it also works under Windows.

On Wed, 14 Mar 2007, John Schuenemeyer wrote:

> A simple example follows.  The file is called Test.R
> # Example
> rm(list=is(all=TRUE))
> cat("Enter file name")
> fn<-scan(what="")
>
> I execute the following:
> @C:\PROGRA~1\R\R-2.4.1\bin\Rterm.exe --no-restore --no-save < Test.R > Rout.txt
>
> I do not see the "Enter file name" or have the opportunity to enter the file name.  I am running R in windows XP.
>
> Thanks for your help.
>
> John H Schuenemeyer
> Southwest Statistical Consulting, LLC
> 960 Sligo St
> Cortez, CO 81321-2558
>
> Phone: 970.565.0179
>
> URL: www.swstatconsult.com
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

PLEASE do, and not send HTML.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Mark.Leeds at morganstanley.com  Thu Mar 15 17:17:31 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Thu, 15 Mar 2007 12:17:31 -0400
Subject: [R] Seemingly Unrelated Regressions question - not R question
Message-ID: <D3AEEDA31E57474B840BEBC25A8A8344F396D4@NYWEXMB23.msad.ms.com>

Does anyone know where I can find a proof of the fact that when each X
matrix in a SUR is the same,
then SUR estimation is equivalent to OLS estmation. The proof is
supposedly in William Greene's book but that book 
costs 157.00 an has mixed reviews so I am reluctant to purchase it.
Thanks.
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}


From casot at libero.it  Thu Mar 15 17:20:57 2007
From: casot at libero.it (casot at libero.it)
Date: Thu, 15 Mar 2007 17:20:57 +0100
Subject: [R] how to...
Message-ID: <JEYDEX$CA669D643BA189F9C01108BE608963D0@libero.it>

I have to perform ANOVA's on many different data organized in a dataframe. I can run an ANOVA for each sample, but I've got hundreds of data and I would like to avoid manually carrying out each test. in addition, I would like to have the results organized in a simple way, for example in a table, wich could be easy to export. thank you for assistance

simone 


------------------------------------------------------
Leggi GRATIS le tue mail con il telefonino i-mode? di Wind
http://i-mode.wind.it


From S.Ellison at lgc.co.uk  Thu Mar 15 17:26:48 2007
From: S.Ellison at lgc.co.uk (S Ellison)
Date: Thu, 15 Mar 2007 16:26:48 +0000
Subject: [R] Creating q and p functions from a self-defined	distribut
Message-ID: <s5f973cd.070@tedmail2.lgc.co.uk>



>>> ted.harding at nessie.mcc.ac.uk 15/03/2007 13:26:52 >>>
On 15-Mar-07 12:09:42, Eli Gurarie wrote:
> Hello all,
> 
> I am fishing for some suggestions on efficient ways to make qdist and 
> pdist type functions from an arbitrary distribution whose probability 
> density function I've defined myself.

Ted Harding accurately said there is unliekly to be an efficient general answer.

However, if you want a dreadfully inefficient but very general answer, could you use a numerical integral to calculate the cumulative probabilities, and a root-finder to find quantiles from the integral function?  (strictly, the quantile would be the root of {cumulative integral} - p where p is where you want the quantile.

uniroot is the general-purpose root-finder in R; it isn't ideal for this purpose because it won't like an interval of +-In, which you will certainly need for general quantiles. But you should be able to make it work with minimal tinkering if you use it on logit(p) and apply it to the interval [0,1]Converting back from logit after uniroot should then give you your quantile. You may also want to give it a smaller tolerance; the default looks like its geared to a sum-of-squares minimiser, and may be a bit over-generous for your purpose.. 

Steve Ellison.

*******************************************************************
This email and any attachments are confidential. Any use, co...{{dropped}}


From gwingfield at ou.edu  Thu Mar 15 17:27:04 2007
From: gwingfield at ou.edu (Wingfield, Jerad G.)
Date: Thu, 15 Mar 2007 11:27:04 -0500
Subject: [R] Cannot allocate vector size of... ?
Message-ID: <9F2278994E89554F952BFE553CC176BC077149F5@XMAIL1.sooner.net.ou.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070315/7b05ab32/attachment.pl 

From mark at wardle.org  Thu Mar 15 17:37:59 2007
From: mark at wardle.org (Mark Wardle)
Date: Thu, 15 Mar 2007 16:37:59 +0000
Subject: [R] Density estimation graphs
Message-ID: <45F97667.3020004@wardle.org>

Dear all,

I'm struggling with a plot and would value any help!

I'm attempting to highlight a histogram and density plot to show a
proportion of cases above a threshold value. I wanted to cross-hatch the
area below the density curve. The breaks and bandwidth are deliberate
integer values because of the type of data I'm looking at.

I've managed to do this, but I don't think it is very good! It would be
difficult, for example, to do a cross-hatch using this technique.

allele.plot <- function(x, threshold=NULL, hatch.col='black',
hatch.border=hatch.col, lwd=par('lwd'),...) {
	h <- hist(x, breaks=max(x), plot=F)
	d <- density(x, bw=1)
	plot(d, lwd=lwd, ...)
	
	if (!is.null(threshold)) {
		d.t <- d$x>threshold
		d.x <- d$x[d.t]
		d.y <- d$y[d.t]
		d.l <- length(d.x)
		# draw all but first line of hatch
		for (i in 2:d.l) {
		lines(c(d.x[i],d.x[i]),c(0,d.y[i]),
			col=hatch.col,lwd=1)
		}
		# draw first line in hatch border colour
		lines(c(d.x[1],d.x[1]),c(0,d.y[1]),
			col=hatch.border,lwd=lwd)

		# and now re-draw density plot lines
		lines(d, lwd=lwd)
	}
}

# some pretend data
s8 = rnorm(100, 15, 5)
threshold = 19			# an arbitrary cut-off
allele.plot(s8, threshold, hatch.col='grey',hatch.border='black')


Is there a better way? As always, I'm sure there's a one-liner rather
than my crude technique!

Best wishes,

Mark
-- 
Dr. Mark Wardle
Clinical research fellow and specialist registrar, Neurology
University Hospital Wales and Cardiff University, UK


From mark at wardle.org  Thu Mar 15 17:42:42 2007
From: mark at wardle.org (Mark Wardle)
Date: Thu, 15 Mar 2007 16:42:42 +0000
Subject: [R] Density estimation graphs
In-Reply-To: <45F97667.3020004@wardle.org>
References: <45F97667.3020004@wardle.org>
Message-ID: <45F97782.3090907@wardle.org>

Mark Wardle wrote:
> Dear all,
> 
> I'm struggling with a plot and would value any help!
> ...
> 
> Is there a better way? As always, I'm sure there's a one-liner rather
> than my crude technique!
> 

As always, I've spent ages trying to sort this, and then the minute
after sending an email, I find the polygon() function.

Ignore previous message!

Best wishes,

Mark


From roger.bos at us.rothschild.com  Thu Mar 15 17:43:52 2007
From: roger.bos at us.rothschild.com (Bos, Roger)
Date: Thu, 15 Mar 2007 12:43:52 -0400
Subject: [R] Cannot allocate vector size of... ?
In-Reply-To: <9F2278994E89554F952BFE553CC176BC077149F5@XMAIL1.sooner.net.ou.edu>
Message-ID: <D8C95B444AD6EE4AAD638D818A9CFD343A201C@RINNYCSE000.rth.ad.rothschild.com>

Yes, your error is due to running out of memory.  This is probably one
of the most frequent questions asked here, so if you search again you
can find a lot of advice on how to get around it.  

As you learn more about R programming you will learn how to store data
more efficiently, rm() to remove variables you no longer need, gc() to
garbage collect and free up memory.  Try to open only the files you
need, do some of the analysis, then get rid of everything you don't
need, then do some more analysis.

Thanks,

Roger 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Wingfield, Jerad
G.
Sent: Thursday, March 15, 2007 12:27 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Cannot allocate vector size of... ?

Hello all, 

 

I've been working with R & Fridolin Wild's lsa package a bit over the
past few months, but I'm still pretty much a novice. I have a lot of
files that I want to use to create a semantic space. When I begin to run
the initial textmatrix( ), it runs for about 3-4 hours and eventually
gives me an error. It's always "ERROR: cannot allocate vector size of
xxx Kb". I imagine this might be my computer running out of memory, but
I'm sure. So I thought I would send this to community at large for any
help/thoughts.

 

I search the archives and didn't really find anything that specifically
speaks to my situation. So I guess I have s few questions. First, is
this actually an issue with the machine running out of memory? If not,
what might be the cause for the error? If so, is there a way to minimize
the amount of memory used by the vector data structures (e.g., Berkeley
DB)?

 

Thanks,

Gabe Wingfield

IT and Program Specialist I

Center for Applied Social Research

University of Oklahoma

2 Partners Place

3100 Monitor, Suite 100

Norman, OK 73072


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

********************************************************************** * 
This message is for the named person's use only. It may 
contain confidential, proprietary or legally privileged 
information. No right to confidential or privileged treatment 
of this message is waived or lost by any error in 
transmission. If you have received this message in error, 
please immediately notify the sender by e-mail, 
delete the message and all copies from your system and destroy 
any hard copies. You must not, directly or indirectly, use, 
disclose, distribute, print or copy any part of this message 
if you are not the intended recipient.


From charles.hebert at ens.fr  Thu Mar 15 17:47:35 2007
From: charles.hebert at ens.fr (Charles Hebert)
Date: Thu, 15 Mar 2007 17:47:35 +0100
Subject: [R] [e1071] predict.svm bug ?
Message-ID: <1b76255d0703150947g357db238x99f9ad89c9fe72dc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070315/1bb0adef/attachment.pl 

From joe_retzer at yahoo.com  Thu Mar 15 17:49:40 2007
From: joe_retzer at yahoo.com (Joseph Retzer)
Date: Thu, 15 Mar 2007 09:49:40 -0700 (PDT)
Subject: [R] plot.randomForest default mtry values
In-Reply-To: <1172951932.10294.12.camel@localhost.localdomain>
Message-ID: <557839.85826.qm@web60320.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070315/6610753c/attachment.pl 

From skiadas at hanover.edu  Thu Mar 15 17:54:19 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Thu, 15 Mar 2007 12:54:19 -0400
Subject: [R] Density estimation graphs
In-Reply-To: <45F97667.3020004@wardle.org>
References: <45F97667.3020004@wardle.org>
Message-ID: <004EC573-41A1-4856-806B-0DA07FC9A7E0@hanover.edu>

On Mar 15, 2007, at 12:37 PM, Mark Wardle wrote:

> Dear all,
>
> I'm struggling with a plot and would value any help!
>
> I'm attempting to highlight a histogram and density plot to show a
> proportion of cases above a threshold value. I wanted to cross- 
> hatch the
> area below the density curve. The breaks and bandwidth are deliberate
> integer values because of the type of data I'm looking at.
>
> I've managed to do this, but I don't think it is very good! It  
> would be
> difficult, for example, to do a cross-hatch using this technique.

Don't know about a cross-hatch, but in general I use "polygon" for  
highlighting areas like that:

allele.plot <- function(x, threshold=NULL, hatch.col='black',
hatch.border=hatch.col, lwd=par('lwd'),...) {
	h <- hist(x, breaks=max(x), plot=F)
	d <- density(x, bw=1)
	plot(d, lwd=lwd, ...)	
	if (!is.null(threshold)) {
		d.t <- d$x>threshold
		d.x <- d$x[d.t]
		d.y <- d$y[d.t]
		polygon(c(d.x[1],d.x,d.x[1]),c(0,d.y,0), col=hatch.col,lwd=1)
	}
}
# some pretend data
s8 = rnorm(100, 15, 5)
threshold = 19			# an arbitrary cut-off
allele.plot(s8, threshold, hatch.col='grey',hatch.border='black')


Perhaps this can help a bit. Btw, what was d.l for?

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From buyske at stat.rutgers.edu  Thu Mar 15 18:01:25 2007
From: buyske at stat.rutgers.edu (Steve Buyske)
Date: Thu, 15 Mar 2007 13:01:25 -0400
Subject: [R] How to use result of approxfun in a package?
Message-ID: <0699FD27-3030-4199-91D3-C50FCE3D0AED@stat.rutgers.edu>

I am working on a project where we start with start with 2 long,  
equal-length vectors, massage them in various ways, and end up with a  
function mapping one interval to another. I'll call that function  
"f1." The last step in R is to generate f1 as the value of the  
approxfun function. I would like to put f1 into a package, but  
without having the package redo the creation of function f1. The  
value of approxfun is a function; for example, I have

 > f1
function (v)
.C("R_approx", as.double(x), as.double(y), as.integer(n), xout =  
as.double(v),
     as.integer(length(v)), as.integer(method), as.double(yleft),
     as.double(yright), as.double(f), NAOK = TRUE, PACKAGE = "base") 
$xout
<environment: 0x17719324>

I don't really understand how this is stored, and in particular, how  
I should handle it so as to include the function f1 in a package. I  
would like the users to be able to load the package and use f1  
directly, rather than re-generate it using approxfun.

Thanks,
Steve
---
Steve Buyske (rhymes with "nice key")
Associate Research Professor
Department of Statistics & Biostatistics
Rutgers University
Hill Center, 110 Frelinghuysen Rd
Piscataway, NJ 08854
buyske at stat.rutgers.edu


From milton_ruser at yahoo.com.br  Thu Mar 15 18:03:50 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Thu, 15 Mar 2007 10:03:50 -0700 (PDT)
Subject: [R] logistic regression
Message-ID: <84095.38582.qm@web56611.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070315/b4c41c30/attachment.pl 

From ssj1364 at gmail.com  Thu Mar 15 18:10:35 2007
From: ssj1364 at gmail.com (sj)
Date: Thu, 15 Mar 2007 11:10:35 -0600
Subject: [R] vars :VARMA, multivariate time series?
Message-ID: <1c6126db0703151010g335635c2sbc9702f756bf8a2f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070315/df45a01f/attachment.pl 

From liuwensui at gmail.com  Thu Mar 15 18:35:14 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Thu, 15 Mar 2007 13:35:14 -0400
Subject: [R] plot.randomForest default mtry values
In-Reply-To: <557839.85826.qm@web60320.mail.yahoo.com>
References: <1172951932.10294.12.camel@localhost.localdomain>
	<557839.85826.qm@web60320.mail.yahoo.com>
Message-ID: <1115a2b00703151035x56402d45u9b810777ec5d8091@mail.gmail.com>

Joe,
here is a piece of junk copied from my blog, showing how to use mtry and hth.
library(MASS);
library(randomForest);
data(Boston);

set.seed(2007);

# SEARCH FOR BEST VALUE OF MTRY FOR RANDOM FORESTS
mtry <- tuneRF(Boston[, -14], Boston[, 14], mtryStart = 1,
               stepFactor = 2, ntreeTry = 500, improve = 0.01);
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1];

# FIT A RF MODEL
rf <- randomForest(medv~., data = Boston, mtry = best.m, ntree = 1000,
importance = TRUE);

# EXTRACT VARIABLE IMPORTANCE
imp.tmp <- importance(rf, type = 1);
rf.imp <- imp.tmp[order(imp.tmp[, 1], decreasing = TRUE),];
par(mar = c(3, 0, 4, 0));
barplot(rf.imp, col = gray(0:(ncol(Boston) - 1)/(ncol(Boston) - 1)),
        names.arg = names(rf.imp), yaxt = "n", cex.names = 1);
title(main = list("Importance Rank of Predictors", font = 4, cex = 1.5));

# PLOT PARTIAL DEPENDENCE OF EACH PREDICTOR
par(mfrow = c(3, 5), mar = c(2, 2, 2, 2), pty = "s");
for (i in 1:(ncol(Boston) - 1))
  {
    partialPlot(rf, Boston, names(Boston)[i], xlab = names(Boston)[i],
main = NULL);
  }

On 3/15/07, Joseph Retzer <joe_retzer at yahoo.com> wrote:
> When using the plot.randomForest method, 3 error series (by number of trees) are plotted. I suspect they are associated with the 3 default values of mtry that are used, for example, in the tuneRF method but I'm not sure. Could someone confirm?
>
> Also, is it possible to force different values of mtry to be used when creating the plots? I specified them explicitly in the randomForest statement but it did not seem to have an effect.
> Many thanks,
> Joe Retzer
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From gwingfield at ou.edu  Thu Mar 15 18:40:38 2007
From: gwingfield at ou.edu (Wingfield, Jerad G.)
Date: Thu, 15 Mar 2007 12:40:38 -0500
Subject: [R] Cannot allocate vector size of... ?
In-Reply-To: <45F982D3.9020106@pburns.seanet.com>
References: <9F2278994E89554F952BFE553CC176BC077149F5@XMAIL1.sooner.net.ou.edu>
	<45F982D3.9020106@pburns.seanet.com>
Message-ID: <9F2278994E89554F952BFE553CC176BC07714A65@XMAIL1.sooner.net.ou.edu>


Oops. Yep, I totally forgot my specs and such. I'm currently running
R-2.4.1 on a 64-bit Linux box (Fedora Core 6) with 4GB of RAM. The files
are 10-50Kb on average, but this error came about when only working with
~16,000 of them. The final size of the corpus is ~1.7M files. So,
obviously, this memory thing is going to be a large issue for me.

I'm going through re-searching the help list archives and now it looks
like I have S Poetry to read as well. 

Thanks for all the suggestions. Any others are greatly appreciated as
well.

Gabe Wingfield
IT and Program Specialist I
Center for Applied Social Research
University of Oklahoma
2 Partners Place
3100 Monitor, Suite 100
Norman, OK 73072

-----Original Message-----
From: Patrick Burns [mailto:pburns at pburns.seanet.com] 
Sent: Thursday, March 15, 2007 12:31 PM
To: Wingfield, Jerad G.
Subject: Re: [R] Cannot allocate vector size of... ?

You can find a few things not to do (things that waste memory)
in S Poetry.  You don't say how much memory your machine
has, nor how big your objects are.  However, it is possible that
getting more memory for your machine might be the best thing
to do.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Wingfield, Jerad G. wrote:

>Hello all, 
>
> 
>
>I've been working with R & Fridolin Wild's lsa package a bit over the
>past few months, but I'm still pretty much a novice. I have a lot of
>files that I want to use to create a semantic space. When I begin to
run
>the initial textmatrix( ), it runs for about 3-4 hours and eventually
>gives me an error. It's always "ERROR: cannot allocate vector size of
>xxx Kb". I imagine this might be my computer running out of memory, but
>I'm sure. So I thought I would send this to community at large for any
>help/thoughts.
>
> 
>
>I search the archives and didn't really find anything that specifically
>speaks to my situation. So I guess I have s few questions. First, is
>this actually an issue with the machine running out of memory? If not,
>what might be the cause for the error? If so, is there a way to
minimize
>the amount of memory used by the vector data structures (e.g., Berkeley
>DB)?
>
> 
>
>Thanks,
>
>Gabe Wingfield
>
>IT and Program Specialist I
>
>Center for Applied Social Research
>
>University of Oklahoma
>
>2 Partners Place
>
>3100 Monitor, Suite 100
>
>Norman, OK 73072
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>


From jporzak at gmail.com  Thu Mar 15 18:59:55 2007
From: jporzak at gmail.com (Jim Porzak)
Date: Thu, 15 Mar 2007 09:59:55 -0800
Subject: [R] plot.randomForest default mtry values
In-Reply-To: <557839.85826.qm@web60320.mail.yahoo.com>
References: <1172951932.10294.12.camel@localhost.localdomain>
	<557839.85826.qm@web60320.mail.yahoo.com>
Message-ID: <2a9c000c0703151059t23053b35k895a42acd889e9f9@mail.gmail.com>

Joe,

I'm guessing you are doing a 2-category problem. The three lines are
OOB errors for overall error and each of the two categories.

There is only one default value of mtry. You can specify a different
mtry when the forest is built (in your call to randomForest()), but it
applies to the entire forest.

On 3/15/07, Joseph Retzer <joe_retzer at yahoo.com> wrote:
> When using the plot.randomForest method, 3 error series (by number of trees) are plotted. I suspect they are associated with the 3 default values of mtry that are used, for example, in the tuneRF method but I'm not sure. Could someone confirm?
>
> Also, is it possible to force different values of mtry to be used when creating the plots? I specified them explicitly in the randomForest statement but it did not seem to have an effect.
> Many thanks,
> Joe Retzer
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
HTH,
Jim Porzak
Loyalty Matrix Inc.
San Francisco, CA
http://www.linkedin.com/in/jimporzak


From dickgiesser at gmail.com  Thu Mar 15 19:11:39 2007
From: dickgiesser at gmail.com (Benjamin Dickgiesser)
Date: Thu, 15 Mar 2007 18:11:39 +0000
Subject: [R] Calculating Deviance from log-likelihood
Message-ID: <b75d67340703151111g417989b3ybada4c915dca097b@mail.gmail.com>

Hi,

This is a bit R unrelated, I want to, however, use my results from
this in a R function.

I am trying to figure out a formula for the deviance given a
likelihood function. In my derivation I end up with having ln(y-1)
twice in my expression for the deviance (see attached pdf). Which
makes it impossible to calculate a value for the deviance since y can
be = 1.

I know this expression can be simplified differently avoiding ln(y-1).
Does someone have a suggestion?

I would appreciate you help very much!

Thank you,
Benjamin
-------------- next part --------------
A non-text attachment was scrubbed...
Name: deviance.pdf
Type: application/pdf
Size: 31482 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070315/0e45be4f/attachment.pdf 

From petr.pikal at precheza.cz  Thu Mar 15 20:38:25 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 15 Mar 2007 20:38:25 +0100
Subject: [R] how to...
In-Reply-To: <JEYDEX$CA669D643BA189F9C01108BE608963D0@libero.it>
Message-ID: <45F9AEC1.5975.2E3746A@localhost>

Hi

I suppose you will not get usefull response for such poorly specified 
question. 

For automating procedures on data frames you can either do looping or 
use lapply or maybe do.call can also provide some functionality.

If you elaborate what you did and in what respect it was 
unsatisfactory maybe you will get better answer.

Anyway, before your next post you shall look to posting guide.

Regards
Petr



On 15 Mar 2007 at 17:20, casot at libero.it wrote:

Date sent:      	Thu, 15 Mar 2007 17:20:57 +0100
From:           	"casot at libero.it" <casot at libero.it>
To:             	"R Help" <R-help at stat.math.ethz.ch>
Subject:        	[R] how to...

> I have to perform ANOVA's on many different data organized in a
> dataframe. I can run an ANOVA for each sample, but I've got hundreds
> of data and I would like to avoid manually carrying out each test. in
> addition, I would like to have the results organized in a simple way,
> for example in a table, wich could be easy to export. thank you for
> assistance
> 
> simone 
> 
> 
> ------------------------------------------------------
> Leggi GRATIS le tue mail con il telefonino i-mode  di Wind
> http://i-mode.wind.it
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From ted.harding at nessie.mcc.ac.uk  Thu Mar 15 20:38:54 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 15 Mar 2007 19:38:54 -0000 (GMT)
Subject: [R] logistic regression
In-Reply-To: <84095.38582.qm@web56611.mail.re3.yahoo.com>
Message-ID: <XFMail.070315193854.ted.harding@nessie.mcc.ac.uk>

On 15-Mar-07 17:03:50, Milton Cezar Ribeiro wrote:
> Dear All,
> 
> I would like adjust and know the "R2" of following presence/absence
> data:
> 
> x<-1:10
> y<-c(0,0,0,0,0,0,0,1,1,1)
> 
> I tryed use clogit (survival package) but it don?t worked. 
> 
> Any idea?
> 
> miltinho

You are trying to fit an equation

  P[y = 1 ; x] = exp((x-a)/b))/(1 + exp((x-a)/b))

to data

  x =   1   2   3   4   5   6   7   8   9  10

  y =   0   0   0   0   0   0   0   1   1   1

by what amounts to a maximum-likelihood method, i.e. which
chooses the parameter values to maximize the probability of
the observed values of y (given the values of x).

The maximum probability possible is 1, so if you can find
parameters which make P[y = 1] = 0 for x = 1, 2, ... , 7
and P[y = 1] for x = 8, 9, 10 then you have done it.

This will be approximated as closely as you please for any
value of a between 7 and 8, and sufficiently small values of b,
since for such parameter values P[y = 1 ; x] -> 0 for x < a,
and -> 1 for x > a.

You therefore have a solution which is both indeterminate
(any a such that 7 < a < 8) and singular (b -> 0). So it
will defeat standard estimation methods.

That is the source of your problem. In a more general context,
this is an instance of the "linear separation" problem in
logistic regression (and similar methods, such a probit
analysis). Basically, this situation implies that, according
to the data, there is a perfect prediction for the results.

There is no well-defined way of dealing with it; any approach
starts from the proposition "this perfect prediction is not
a reasonable result in the context of my data", and continues
by following up what you think should be meant by "not a
reasonable result". What this is likely to mean would be on
the lines of "b should not be that small", which then imposes
upon you the need to be more specific about how small b may
reasonably be. Then carry on from there (perhaps by fixing
the value of b at different reasonable levels, and simply
fitting a for each value of b).

Hoping this helps ... but I'm wondering how it happens that
you have such data ... ??

best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 15-Mar-07                                       Time: 19:38:51
------------------------------ XFMail ------------------------------


From jeffmiller at alphapoint05.net  Thu Mar 15 21:58:26 2007
From: jeffmiller at alphapoint05.net (Jeff Miller)
Date: Thu, 15 Mar 2007 15:58:26 -0500
Subject: [R] logistic regression TRY LOGISTF
In-Reply-To: <XFMail.070315193854.ted.harding@nessie.mcc.ac.uk>
Message-ID: <!&!AAAAAAAAAAAYAAAAAAAAAEuLs8ZPMuFInPbB+1JmPy7CgAAAEAAAABdD2BMeojVAiUeWnKv7P5MBAAAAAA==@alphapoint05.net>

If Ted is right, then one work-around is to use Firth's method for penalized
log-likelihood. The technique is originally intended to reduce small sample
bias. However, it's now being extended to deal with complete and quasi
separation problems.

I believe the library is called logistf but I haven't had a chance to try
it....I know the SAS version (called the fl macro) works fine.

Reference --
http://www.meduniwien.ac.at/user/georg.heinze/techreps/tr2_2004.pdf


Hope this helps,

Jeff Miller
University of Florida
AlphaPoint05, Inc.


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ted Harding
Sent: Thursday, March 15, 2007 2:39 PM
To: R-help
Subject: Re: [R] logistic regression

On 15-Mar-07 17:03:50, Milton Cezar Ribeiro wrote:
> Dear All,
> 
> I would like adjust and know the "R2" of following presence/absence
> data:
> 
> x<-1:10
> y<-c(0,0,0,0,0,0,0,1,1,1)
> 
> I tryed use clogit (survival package) but it don?t worked. 
> 
> Any idea?
> 
> miltinho

You are trying to fit an equation

  P[y = 1 ; x] = exp((x-a)/b))/(1 + exp((x-a)/b))

to data

  x =   1   2   3   4   5   6   7   8   9  10

  y =   0   0   0   0   0   0   0   1   1   1

by what amounts to a maximum-likelihood method, i.e. which chooses the
parameter values to maximize the probability of the observed values of y
(given the values of x).

The maximum probability possible is 1, so if you can find parameters which
make P[y = 1] = 0 for x = 1, 2, ... , 7 and P[y = 1] for x = 8, 9, 10 then
you have done it.

This will be approximated as closely as you please for any value of a
between 7 and 8, and sufficiently small values of b, since for such
parameter values P[y = 1 ; x] -> 0 for x < a, and -> 1 for x > a.

You therefore have a solution which is both indeterminate (any a such that 7
< a < 8) and singular (b -> 0). So it will defeat standard estimation
methods.

That is the source of your problem. In a more general context, this is an
instance of the "linear separation" problem in logistic regression (and
similar methods, such a probit analysis). Basically, this situation implies
that, according to the data, there is a perfect prediction for the results.

There is no well-defined way of dealing with it; any approach starts from
the proposition "this perfect prediction is not a reasonable result in the
context of my data", and continues by following up what you think should be
meant by "not a reasonable result". What this is likely to mean would be on
the lines of "b should not be that small", which then imposes upon you the
need to be more specific about how small b may reasonably be. Then carry on
from there (perhaps by fixing the value of b at different reasonable levels,
and simply fitting a for each value of b).

Hoping this helps ... but I'm wondering how it happens that you have such
data ... ??

best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 15-Mar-07                                       Time: 19:38:51
------------------------------ XFMail ------------------------------

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From qyang at bu.edu  Thu Mar 15 21:36:39 2007
From: qyang at bu.edu (Qiong Yang)
Date: Thu, 15 Mar 2007 16:36:39 -0400
Subject: [R] unbiased sandwich variance estimator for GEE estimates
Message-ID: <45F9AE57.6080807@bu.edu>

Hi,

Anyone knows any existing package/program that has implemented
unbiased (or bias-reduced) sandwich variance estimator (Wu (1986) and 
Carroll (2001)
for GEE estimates?

Thanks
Qiong


From hi_squirrel at hotmail.com  Thu Mar 15 22:04:11 2007
From: hi_squirrel at hotmail.com (WQ Y)
Date: Thu, 15 Mar 2007 21:04:11 +0000
Subject: [R] Model selection in LASSO (cross-validation)
Message-ID: <BAY138-F2265EFB8A3F244769EA56586720@phx.gbl>

Hi, I know how to use LASSO for model selection based on the Cp criterion.  
I heard that we can also use cross validation as a criterion too.  I used 
cv.lars to give me the lowest predicted error & fraction.  But I'm short of 
a step to arrive at the number of variables to be included in the final 
model.  How do we do that?  Is it the predict.lars function?  i tried > 
logprostate.plars.cv=predict.lars(logprostate.lars.cv, M, type = "fit", 
mode="fraction") but it gives me error message:
Error in dim(data) <- dim : attempt to set an attribute on NULL.  Please 
help!

thanks!

_________________________________________________________________
With tax season right around the corner, make sure to follow these few 
simple tips.


From hi_squirrel at hotmail.com  Thu Mar 15 22:11:22 2007
From: hi_squirrel at hotmail.com (WQ Y)
Date: Thu, 15 Mar 2007 21:11:22 +0000
Subject: [R] Model selection in LASSO (cross-validation)
In-Reply-To: <mailman.0.1173992960.23142.r-help@stat.math.ethz.ch>
Message-ID: <BAY138-F21E5CF75744C5A23CF27686720@phx.gbl>

Hi, I know how to use LASSO for model selection based on the Cp criterion.  
I heard that we can also use cross validation as a criterion too.  I used 
cv.lars to give me the lowest predicted error & fraction.  But I'm short of 
a step to arrive at the number of variables to be included in the final 
model.  How do we do that?  Is it the predict.lars function?  i tried > 
logprostate.plars.cv=predict.lars(logprostate.lars.cv, M, type = "fit", 
mode="fraction") but it gives me error message:
Error in dim(data) <- dim : attempt to set an attribute on NULL.  Please 
help!

Thanks!

_________________________________________________________________
The average US Credit Score is 675. The cost to see yours: $0 by Experian.


From Mark.Leeds at morganstanley.com  Thu Mar 15 22:50:13 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Thu, 15 Mar 2007 17:50:13 -0400
Subject: [R] Covariance matrix calc method  question
Message-ID: <D3AEEDA31E57474B840BEBC25A8A8344F396ED@NYWEXMB23.msad.ms.com>

I have been comparing the output of an R package to S+Finmetrics and I
notice that 
the covariance matrix outputted by the two procedures is different. The
R package
computes the covariance matrix using Method 1 and I think ( but I'm not
sure ) that S+Finmetrics computes it 
using Method 2.

I put in a correctionfactor (see below ) in to Method 2 in order to deal
with the fact that the  var function 
calculates the unnbiased estimate of variance. This gives the same
answer in both problems for the
data shown which I just made up.  But, my hope is that , for much larger
problems, leaving the correction factor 
out can maybe cause huge differences in the determinant ? That would
explain why some of the output ( AIC, BIC ) 
differs in the two packages. I'm not really sure how to check this
easily but if someone has an idea,
it would be appreciated. Basically, I kind of want to run some sort of
simulation along
the lines of below to check whether this could be the reason for the
differences.  Thanks.

x<-c(11,12,13,14,16)
y<-c(2,4,6,8,12)
z<-c(14,18,22,50,20)
LHS<-cbind(x,y)
sample<-nrow(lhs)

# METHOD1

output1<-lm(LHS ~ z)
resids<-resid(output1)
sigma.hat1<-crossprod(resids)/sample
print(sigma.hat1)

print(det(sigma.hat1))

# METHOD2

fit1<-lm(LHS[,1] ~ z)
fit2<-lm(LHS[,2] ~ z)

correctionfactor<-sample-1/sample

sigma.hat2<-correctionfactor*var(cbind(resid(fit1),resid(fit2)))

print(sigma.hat2)
print(det(sigma.hat2))


From g.abraham at ms.unimelb.edu.au  Thu Mar 15 23:48:52 2007
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Fri, 16 Mar 2007 09:48:52 +1100
Subject: [R] expm() within the Matrix package
In-Reply-To: <C21EEA50.DCA%lhill07@qub.ac.uk>
References: <C21EEA50.DCA%lhill07@qub.ac.uk>
Message-ID: <45F9CD54.8080806@ms.unimelb.edu.au>

Laura Hill wrote:
> Hi
> 
> Could anybody give me a bit of advice on some code I'm having trouble with?
> 
> I've been trying to calculate the loglikelihood of a function iterated over
> a data of time values and I seem to be experiencing difficulty when I use
> the function expm(). Here's an example of what I am trying to do
> 
> 
> y<-c(5,10)      #vector of 2 survival times
> 
> p<-Matrix(c(1,0),1,2)   #1x2 matrix
> 
> Q<-Matrix(c(1,2,3,4),2,2)   #2x2 matrix
> 
> q<-Matrix(c(1,2),2,1)       #2x1 matrix
> 
> Loglik<-rep(0,length(y))    #creating empty vector of same length as y
> 
>     for(i in 1:length(y)){
> 
>         Loglik[i]<-log((p %*% (expm(Q*y[i]))) %*% q)  #calculating
>                    
>                                         #  Loglikelihood for each y[i]
>         }
> 
> The code works perfectly if I use exp(Q*y[i]) but not for expm()

You need to do a type conversion here, because you get the loglik as a 
1x1 Matrix, instead of a scalar:

(after running your code)

 > log(p %*% expm(Q * y[i]) %*% q)
1 x 1 Matrix of class "dgeMatrix"
          [,1]
[1,] 134.5565


If you convert to numeric, you can then assign it to Loglik:
 > Loglik[1] <- as.numeric(log(p %*% expm(Q * y[i]) %*% q))
 > Loglik[1]
[1] 134.5565



-- 
Gad Abraham
Department of Mathematics and Statistics
The University of Melbourne
Parkville 3010, Victoria, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From dlvanbrunt at gmail.com  Fri Mar 16 00:36:32 2007
From: dlvanbrunt at gmail.com (David L. Van Brunt, Ph.D.)
Date: Thu, 15 Mar 2007 19:36:32 -0400
Subject: [R] replacing all NA's in a dataframe with zeros...
In-Reply-To: <BAY116-DAV16BEC2A73A82DD75A4F765CF720@phx.gbl>
References: <d332d3e10703141722i77e921fbmf502f7204f09f1b1@mail.gmail.com>
	<BAY116-DAV16BEC2A73A82DD75A4F765CF720@phx.gbl>
Message-ID: <d332d3e10703151636p4f4f4330y35fad0aa738af708@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070315/2c18ccfc/attachment.pl 

From c-b at asu.edu  Fri Mar 16 01:07:25 2007
From: c-b at asu.edu (Christopher Brown)
Date: Thu, 15 Mar 2007 17:07:25 -0700
Subject: [R] multiple scores per subject
Message-ID: <45F9DFBD.1010700@asu.edu>

Hi,

I have a data set that looks like this:

 > data
    vara varb S  PC
1  None  250 1  80
2  None  250 1  70
3  Some  250 1  60
4  Some  250 1  70
5  None 1000 1  90
6  None 1000 1  90
7  Some 1000 1  80
8  Some 1000 1  70
9  None  250 2 100
10 None  250 2  80
11 Some  250 2  70
12 Some  250 2  70
13 None 1000 2 100
14 None 1000 2  90
15 Some 1000 2  50
16 Some 1000 2  40

...

And so on. The last column is the dependent variable, and I have made
the other columns factors. As you can see, there are multiple scores for 
each subject in each combination of conditions. How can I reduce the 
dataset so that there is only 1 score per subject, per condition, for 
further analysis? I can use tapply to get means, but I need a data.frame 
for analysis (aov). Any ideas?

-- 
Chris


From zrl1974 at gmail.com  Fri Mar 16 01:26:13 2007
From: zrl1974 at gmail.com (Roger Liu)
Date: Thu, 15 Mar 2007 19:26:13 -0500
Subject: [R] how to determine the relative distance between a DNA sequence
	and know genes
Message-ID: <c1eefbea0703151726obc01eddv6207b932b8fd384b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070315/c3145787/attachment.pl 

From jholtman at gmail.com  Fri Mar 16 01:28:21 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 15 Mar 2007 19:28:21 -0500
Subject: [R] multiple scores per subject
In-Reply-To: <45F9DFBD.1010700@asu.edu>
References: <45F9DFBD.1010700@asu.edu>
Message-ID: <644e1f320703151728w144b9d74w18ff335cde691d9e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070315/a78eb0ff/attachment.pl 

From tyler.smith at mail.mcgill.ca  Fri Mar 16 01:50:01 2007
From: tyler.smith at mail.mcgill.ca (Tyler Smith)
Date: Fri, 16 Mar 2007 00:50:01 +0000 (UTC)
Subject: [R] MANOVA permutation testing
Message-ID: <slrnevjqdp.to8.tyler.smith@blackbart.mynetwork>

Hi,

I've got a dataset with 7 variables for 8 different species. I'd like
to test the null hypothesis of no difference among species for these
variables. MANOVA seems like the appropriate test, but since I'm
unsure of how well the data fit the assumptions of equal
variance/covariance and multivariate normality, I want to use a
permutation test. 

I've been through CRAN looking at packages boot, bootstrap, coin,
permtest, but they all seem to be doing more than I need. Is the
following code an appropriate way to test my hypothesis:

result.vect <- c()

for (i in 1:1000){
  wilks <- summary.manova(manova(maxent~sample(max.spec)),
               test="Wilks")$stats[1,2]
  result.vect <- c(res.vect,wilks)
}

maxent is the data, max.spec is a vector of species names. Comparing
the result.vect with the wilks value for the unpermuted data suggests
there are very significant differences among species -- but did I do
this properly?

-- 
Regards,

Tyler Smith


From yusuke.fukuda at nt.gov.au  Fri Mar 16 03:38:18 2007
From: yusuke.fukuda at nt.gov.au (Achiko)
Date: Thu, 15 Mar 2007 19:38:18 -0700 (PDT)
Subject: [R] How can I place axis annotations away from axis?
Message-ID: <9507709.post@talk.nabble.com>


Hello Experts

I have the following codes and data for 2 interpolation plots.

http://www.nabble.com/file/7206/3d_plot_data.txt 3d_plot_data.txt 

data<-read.table("3d_plot_data.txt", header=T)
attach(data)

par(mfrow=c(1,2))

library(akima)

interpolation<-interp(rr,veg_r,predict)

persp(interpolation,theta = -45, phi = 30, ticktype = "detailed", nticks=4,
cex=0.8, expand=0.5, xlab="\n\n\nPrecipitation", yla="\n\n\nVegetation",
zlab="\n\n\nDensity", shade=0.4)

interpolation<-interp(tc,veg_r,predict, duplicate="mean")

persp(interpolation,theta = -45, phi = 30, ticktype = "detailed", nticks=4,
cex=0.8, expand=0.5, xlab="\n\n\nTemperature", yla="\n\n\nVegetation",
zlab="\n\n\nDensity", shade=0.4)


Now as you can see, and when exported as eps, axis annotation by tickmarks
are overlapping with Z axis. As it's for publication, font should be this
big. I could put the axis labels away from the axes, but connot find how to
place axis annotations further from the axes, or if it's ever possible to
adjust the distance between axis and axis annotation. 

Your help is much appreciated! 
-- 
View this message in context: http://www.nabble.com/How-can-I-place-axis-annotations-away-from-axis--tf3412293.html#a9507709
Sent from the R help mailing list archive at Nabble.com.


From g.abraham at ms.unimelb.edu.au  Fri Mar 16 03:47:25 2007
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Fri, 16 Mar 2007 13:47:25 +1100
Subject: [R] ARIMA standard error
Message-ID: <45FA053D.4060808@ms.unimelb.edu.au>

Hi,

Can anyone explain how the standard error in arima() is calculated?

Also, how can I extract it from the Arima object? I don't see it in there.

 > x <- rnorm(1000)
 > a <- arima(x, order = c(4, 0, 0))
 > a

Call:
arima(x = x, order = c(4, 0, 0))

Coefficients:
           ar1     ar2     ar3      ar4  intercept
       -0.0451  0.0448  0.0139  -0.0688     0.0010
s.e.   0.0316  0.0316  0.0317   0.0316     0.0296

sigma^2 estimated as 0.9775:  log likelihood = -1407.56,  aic = 2827.12
 > names(a)
  [1] "coef"      "sigma2"    "var.coef"  "mask"      "loglik"    "aic" 
       "arma"      "residuals" "call"      "series"    "code" 
"n.cond"    "model"


Thanks,
Gad

-- 
Gad Abraham
Department of Mathematics and Statistics
The University of Melbourne
Parkville 3010, Victoria, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From A.Robinson at ms.unimelb.edu.au  Fri Mar 16 04:06:05 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Fri, 16 Mar 2007 14:06:05 +1100
Subject: [R] ARIMA standard error
In-Reply-To: <45FA053D.4060808@ms.unimelb.edu.au>
References: <45FA053D.4060808@ms.unimelb.edu.au>
Message-ID: <20070316030605.GT18002@ms.unimelb.edu.au>

Hi Gad,

try:


> class(a)
[1] "Arima"
> getAnywhere(print.Arima)

...

Cheers,

Andrew


On Fri, Mar 16, 2007 at 01:47:25PM +1100, Gad Abraham wrote:
> Hi,
> 
> Can anyone explain how the standard error in arima() is calculated?
> 
> Also, how can I extract it from the Arima object? I don't see it in there.
> 
>  > x <- rnorm(1000)
>  > a <- arima(x, order = c(4, 0, 0))
>  > a
> 
> Call:
> arima(x = x, order = c(4, 0, 0))
> 
> Coefficients:
>            ar1     ar2     ar3      ar4  intercept
>        -0.0451  0.0448  0.0139  -0.0688     0.0010
> s.e.   0.0316  0.0316  0.0317   0.0316     0.0296
> 
> sigma^2 estimated as 0.9775:  log likelihood = -1407.56,  aic = 2827.12
>  > names(a)
>   [1] "coef"      "sigma2"    "var.coef"  "mask"      "loglik"    "aic" 
>        "arma"      "residuals" "call"      "series"    "code" 
> "n.cond"    "model"
> 
> 
> Thanks,
> Gad
> 
> -- 
> Gad Abraham
> Department of Mathematics and Statistics
> The University of Melbourne
> Parkville 3010, Victoria, Australia
> email: g.abraham at ms.unimelb.edu.au
> web: http://www.ms.unimelb.edu.au/~gabraham
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From g.abraham at ms.unimelb.edu.au  Fri Mar 16 05:45:08 2007
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Fri, 16 Mar 2007 15:45:08 +1100
Subject: [R] ARIMA standard error
In-Reply-To: <20070316030605.GT18002@ms.unimelb.edu.au>
References: <45FA053D.4060808@ms.unimelb.edu.au>
	<20070316030605.GT18002@ms.unimelb.edu.au>
Message-ID: <45FA20D4.5040507@ms.unimelb.edu.au>

Andrew Robinson wrote:
> Hi Gad,
> 
> try:
> 
> 
>> class(a)
> [1] "Arima"
>> getAnywhere(print.Arima)

Thanks Andrew.

For the record, the standard error is the square root of the diagonal of 
the covariance matrix a$var.coef (itself obtained through some magic):

ses[x$mask] <- round(sqrt(diag(x$var.coef)), digits = digits)


Cheers,
Gad

-- 
Gad Abraham
Department of Mathematics and Statistics
The University of Melbourne
Parkville 3010, Victoria, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From rfarmer at uoguelph.ca  Fri Mar 16 05:48:21 2007
From: rfarmer at uoguelph.ca (Bob Farmer)
Date: Fri, 16 Mar 2007 00:48:21 -0400
Subject: [R] Complex superscript plot text
Message-ID: <45FA2195.7060105@uoguelph.ca>

Hi all:

I would like to create a line of plot margin text (using mtext() ) that 
features both a superscript and a subset of an object.  However, I 
cannot seem to do both things at once, nor have I found a way to paste 
the two results together.

(I pull the object subset because this is part of a for-next loop to 
make multiple graphs)

This example doesn't give me what I want from mtext():

GoodnessOfFits<-c(1, 0.75)

graphNumber<-1  #first loop of the for-next loop (not shown)

x<-seq(-10, 10, 1)
y<-(x^2)
plot(x,y)
lines(x, predict(lm(y~I(x^2))))
mtext(text=
	expression(R^2 * ": "* GoodnessOfFits[graphNumber]))


I recognize that in this example, I could extract the R-squared value 
from the model in each loop, however this does not apply to my more 
complicated real scenario.

Any suggestions?

Thanks.
--Bob Farmer


From moreyr at missouri.edu  Fri Mar 16 06:41:14 2007
From: moreyr at missouri.edu (Richard D. Morey)
Date: Fri, 16 Mar 2007 00:41:14 -0500
Subject: [R] error code 5 from Lapack routine 'dsyevr'
Message-ID: <45FA2DFA.6090103@missouri.edu>

While using the rmvnorm function, I get the error:

Error in eigen(sigma, sym = TRUE) : error code 5 from Lapack routine 
'dsyevr'

The same thing happens when I try the eigen() function on my covariance 
matrix. The matrix is a symmetric 111x111 matrix. Well, it is almost 
symmetric; there are slight deviations from symmetry (the largest is 
3e-18).  I have this in an MCMC loop, and it happens about once in every 
40 iterations or so.

What does this error mean?

Thanks.


-- 
Richard D. Morey, M.A.
Research Assistant, Perception and Cognition Lab
University of Missouri-Columbia


From ripley at stats.ox.ac.uk  Fri Mar 16 09:00:47 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Mar 2007 08:00:47 +0000 (GMT)
Subject: [R] error code 5 from Lapack routine 'dsyevr'
In-Reply-To: <45FA2DFA.6090103@missouri.edu>
References: <45FA2DFA.6090103@missouri.edu>
Message-ID: <Pine.LNX.4.64.0703160758140.18543@gannet.stats.ox.ac.uk>

On Fri, 16 Mar 2007, Richard D. Morey wrote:

> While using the rmvnorm function, I get the error:
>
> Error in eigen(sigma, sym = TRUE) : error code 5 from Lapack routine
> 'dsyevr'
>
> The same thing happens when I try the eigen() function on my covariance
> matrix. The matrix is a symmetric 111x111 matrix. Well, it is almost
> symmetric; there are slight deviations from symmetry (the largest is
> 3e-18).  I have this in an MCMC loop, and it happens about once in every
> 40 iterations or so.
>
> What does this error mean?

See the LAPACK code (in src/modules/lapack).

Internal LAPACK errors are usually problems with arithmetic accuracy, 
and as such are compiler- and CPU-specific.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Fri Mar 16 08:57:00 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Mar 2007 07:57:00 +0000 (GMT)
Subject: [R] ARIMA standard error
In-Reply-To: <45FA20D4.5040507@ms.unimelb.edu.au>
References: <45FA053D.4060808@ms.unimelb.edu.au>
	<20070316030605.GT18002@ms.unimelb.edu.au>
	<45FA20D4.5040507@ms.unimelb.edu.au>
Message-ID: <Pine.LNX.4.64.0703160754270.18543@gannet.stats.ox.ac.uk>

On Fri, 16 Mar 2007, Gad Abraham wrote:

> Andrew Robinson wrote:
>> Hi Gad,
>>
>> try:
>>
>>
>>> class(a)
>> [1] "Arima"
>>> getAnywhere(print.Arima)
>
> Thanks Andrew.
>
> For the record, the standard error is the square root of the diagonal of
> the covariance matrix a$var.coef (itself obtained through some magic):
>
> ses[x$mask] <- round(sqrt(diag(x$var.coef)), digits = digits)

And for the record, ?arima does tell you about the component var.coef, and 
also suggests the vcov() method to extract the variance matrix.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ken.feng at citigroup.com  Fri Mar 16 09:37:47 2007
From: ken.feng at citigroup.com (Feng, Ken [CIB-EQTY])
Date: Fri, 16 Mar 2007 17:37:47 +0900
Subject: [R] Defining a floor value in a data.frame on a row-by-row basis
Message-ID: <0143A263BEF94644AC0D4027373EECD305430982@exyhmb08.jpn.nsroot.net>

Hi,

I have a data frame x, and a vector y which limits x by rows
so that no element of x in that row is smaller than y.

For example,

> x <- as.data.frame( t( array( 1:9, dim=c(3,3))))
> y <- c( 2, 8, 0 )
> x
  V1 V2 V3
1  1  2  3
2  4  5  6
3  7  8  9
> y
[1] 2 8 0

=============================================
I want to get this:

  V1 V2 V3
1  2  2  3
2  8  8  8
3  7  8  9

=============================================

I tried:

x[ x<=y ] <- y

but R wasn't happy.

I know I can do this with a loop, but there has to be a way which I can avoid it...

Thanks in advance.

- Ken


From Delphine.Fontaine at adm.unige.ch  Fri Mar 16 09:36:49 2007
From: Delphine.Fontaine at adm.unige.ch (Delphine Fontaine)
Date: Fri, 16 Mar 2007 09:36:49 +0100
Subject: [R] R and clinical studies
In-Reply-To: <27CA3827C6B33E40874682C469E774DD04DB9136@FMD3CT001.fda.gov>
References: <20070309142909.hgceqlqqasssggo0@webmail.adm.unige.ch>
	<27CA3827C6B33E40874682C469E774DD04DB9136@FMD3CT001.fda.gov>
Message-ID: <20070316093649.7klhg4n673gogs8c@webmail.adm.unige.ch>

Thanks for your answer which was very helpfull. I have another question:

I have read in this document  
(http://cran.r-project.org/doc/manuals/R-intro.pdf) that most of the  
programs written in R are ephemeral and that new releases are not  
always compatible with previous releases. What I would like to know is  
if R functions are already validated and if not, what should we do to  
validate a R function ?

-- 
Delphine Fontaine


Quoting "Soukup, Mat" <Mat.Soukup at fda.hhs.gov>:

> Delphine,
>
> Please see the following message posted a week ago:
> http://comments.gmane.org/gmane.comp.lang.r.general/80175.
>
> HTH,
>
> -Mat
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Delphine Fontaine
> Sent: Friday, March 09, 2007 8:29 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] R and clinical studies
>
> Does anyone know if for clinical studies the FDA would accept
> statistical analyses performed with R ?
>
> Delphine Fontaine
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From koen.hufkens at ua.ac.be  Fri Mar 16 09:49:07 2007
From: koen.hufkens at ua.ac.be (Hufkens Koen)
Date: Fri, 16 Mar 2007 09:49:07 +0100
Subject: [R] help on sigmoid curve fitting
Message-ID: <832A948B92E5754D9062CCF62F9ED892655D65@xmail01.ad.ua.ac.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070316/6a6d1e53/attachment.pl 

From usstata at 126.com  Fri Mar 16 09:56:36 2007
From: usstata at 126.com (usstata)
Date: Fri, 16 Mar 2007 16:56:36 +0800
Subject: [R] How can i do the same thing in the China map?
Message-ID: <45FA5BCD.00BE0E.28518@m5-141.126.com>


The "maps" package has a function called "match.map", which is for map coloring .
Its example is followed:

# filled map showing Republican vote in 1900
# (figure 6 in the reference)
data(state, package = "datasets")
data(votes.repub)
state.to.map <- match.map("state", state.name)
x <- votes.repub[state.to.map, "1900"]
gray.colors <- function(n) gray(rev(0:(n - 1))/n)
color <- gray.colors(100)[floor(x)]
map("state", fill = TRUE, col = color); map("state", add = TRUE)

I want to do the same thing in the China map, but I can't find the Provinces name of China.
Who can help me ?


____________________
a rookie


From maechler at stat.math.ethz.ch  Fri Mar 16 09:59:02 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 16 Mar 2007 09:59:02 +0100
Subject: [R] expm() within the Matrix package
In-Reply-To: <45F9CD54.8080806@ms.unimelb.edu.au>
References: <C21EEA50.DCA%lhill07@qub.ac.uk>
	<45F9CD54.8080806@ms.unimelb.edu.au>
Message-ID: <17914.23638.681930.625532@stat.math.ethz.ch>

>>>>> "Gad" == Gad Abraham <g.abraham at ms.unimelb.edu.au>
>>>>>     on Fri, 16 Mar 2007 09:48:52 +1100 writes:

    Gad> Laura Hill wrote:
    >> Hi
    >> 
    >> Could anybody give me a bit of advice on some code I'm having trouble with?
    >> 
    >> I've been trying to calculate the loglikelihood of a function iterated over
    >> a data of time values and I seem to be experiencing difficulty when I use
    >> the function expm(). Here's an example of what I am trying to do
    >> 
    >> 
    >> y<-c(5,10)      #vector of 2 survival times
    >> 
    >> p<-Matrix(c(1,0),1,2)   #1x2 matrix
    >> 
    >> Q<-Matrix(c(1,2,3,4),2,2)   #2x2 matrix
    >> 
    >> q<-Matrix(c(1,2),2,1)       #2x1 matrix
    >> 
    >> Loglik<-rep(0,length(y))    #creating empty vector of same length as y
    >> 
    >> for(i in 1:length(y)){
    >> 
    >> Loglik[i]<-log((p %*% (expm(Q*y[i]))) %*% q)  #calculating
    >> 
    >> #  Loglikelihood for each y[i]
    >> }
    >> 
    >> The code works perfectly if I use exp(Q*y[i]) but not for expm()

    Gad> You need to do a type conversion here, because you get the loglik as a 
    Gad> 1x1 Matrix, instead of a scalar:

    Gad> (after running your code)

    >> log(p %*% expm(Q * y[i]) %*% q)
    Gad> 1 x 1 Matrix of class "dgeMatrix"
    Gad> [,1]
    Gad> [1,] 134.5565


    Gad> If you convert to numeric, you can then assign it to Loglik:
    >> Loglik[1] <- as.numeric(log(p %*% expm(Q * y[i]) %*% q))
    >> Loglik[1]
    Gad> [1] 134.5565



    Gad> -- 
    Gad> Gad Abraham
    Gad> Department of Mathematics and Statistics
    Gad> The University of Melbourne
    Gad> Parkville 3010, Victoria, Australia
    Gad> email: g.abraham at ms.unimelb.edu.au
    Gad> web: http://www.ms.unimelb.edu.au/~gabraham

    Gad> ______________________________________________
    Gad> R-help at stat.math.ethz.ch mailing list
    Gad> https://stat.ethz.ch/mailman/listinfo/r-help
    Gad> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    Gad> and provide commented, minimal, self-contained, reproducible code.


From maechler at stat.math.ethz.ch  Fri Mar 16 10:00:35 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 16 Mar 2007 10:00:35 +0100
Subject: [R] expm() within the Matrix package
In-Reply-To: <45F9CD54.8080806@ms.unimelb.edu.au>
References: <C21EEA50.DCA%lhill07@qub.ac.uk>
	<45F9CD54.8080806@ms.unimelb.edu.au>
Message-ID: <17914.23731.25729.616899@stat.math.ethz.ch>

>>>>> "Gad" == Gad Abraham <g.abraham at ms.unimelb.edu.au>
>>>>>     on Fri, 16 Mar 2007 09:48:52 +1100 writes:

    Gad> Laura Hill wrote:
    >> Hi
    >> 
    >> Could anybody give me a bit of advice on some code I'm having trouble with?
    >> 
    >> I've been trying to calculate the loglikelihood of a function iterated over
    >> a data of time values and I seem to be experiencing difficulty when I use
    >> the function expm(). Here's an example of what I am trying to do
    >> 
    >> 
    >> y<-c(5,10)      #vector of 2 survival times
    >> 
    >> p<-Matrix(c(1,0),1,2)   #1x2 matrix
    >> 
    >> Q<-Matrix(c(1,2,3,4),2,2)   #2x2 matrix
    >> 
    >> q<-Matrix(c(1,2),2,1)       #2x1 matrix
    >> 
    >> Loglik<-rep(0,length(y))    #creating empty vector of same length as y
    >> 
    >> for(i in 1:length(y)){
    >> 
    >> Loglik[i]<-log((p %*% (expm(Q*y[i]))) %*% q)  #calculating
    >> 
    >> #  Loglikelihood for each y[i]
    >> }
    >> 
    >> The code works perfectly if I use exp(Q*y[i]) but not for expm()

    Gad> You need to do a type conversion here, because you get the loglik as a 
    Gad> 1x1 Matrix, instead of a scalar:

    Gad> (after running your code)

    >> log(p %*% expm(Q * y[i]) %*% q)
    Gad> 1 x 1 Matrix of class "dgeMatrix"
    Gad> [,1]
    Gad> [1,] 134.5565


    Gad> If you convert to numeric, you can then assign it to Loglik:
    >> Loglik[1] <- as.numeric(log(p %*% expm(Q * y[i]) %*% q))
    >> Loglik[1]
    Gad> [1] 134.5565


Hmm, I don't think that's Laura's problem
(and actually I don't know what her problem is) :

Assignment of a 1 x 1 matrix to a vector is not a problem,
and hence the  as.numeric(.) above  really has no effect :

> ll <- 1:2
> (m <- matrix(pi, 1,1))
         [,1]
[1,] 3.141593
> ll[1] <- m
> ll
[1] 3.141593 2.000000
> 

Martin Maechler, ETH Zurich


From Graham.Williams at togaware.com  Fri Mar 16 10:08:25 2007
From: Graham.Williams at togaware.com (Graham Williams)
Date: Fri, 16 Mar 2007 20:08:25 +1100
Subject: [R] rattle- MSACCESS database problem
In-Reply-To: <b4485c4c0703070652y44db649br931340456c08a6b4@mail.gmail.com>
References: <b4485c4c0703070652y44db649br931340456c08a6b4@mail.gmail.com>
Message-ID: <20070316090825.GA11848@athene.togaware.com>

Received Thu 08 Mar 2007  2:38am +1100 from j.joshua thomas:
> library(RGtk2)
> library(rattle)
> rattle()
> 
> click the ODBC option it as the DSN i am a bit confused with this i already
> put my *.mdb file in C:drive
> i try put the DSN name as Microsoft Access driver, in the appropriate text
> box but i couldnt locate the table
> 
> i tried the other way round open-> locate the *.mdb in C:drive couldnt
> locate
> 
> i tried RODBC aswell, but i want to use rattle to Data mine my database
> 
> need someone's help

Did it work with RODBC? Could you send what appears in the Log
tab. That will give me an idea of what is going on.

Regards,
Graham


From jmb at mssl.ucl.ac.uk  Fri Mar 16 10:19:24 2007
From: jmb at mssl.ucl.ac.uk (Jenny Barnes)
Date: Fri, 16 Mar 2007 09:19:24 +0000 (GMT)
Subject: [R] Problem installing R onto Solaris 2.10 system - need advice!!!!!
Message-ID: <200703160919.l2G9JOtg003956@msslhb.mssl.ucl.ac.uk>

Dear R-Help friends,

I am unable to get the latest version of R (2.4.1) to compile on my solaris 10 
system - has anybody else experienced this problem and are you able to offer me 
any advice? 

I appreciate your time, many thanks,

Jenny Barnes


Here are my CURRENT specifications:

platform       sparc-sun-solaris2.10     
arch           sparc                     
os             solaris2.10               
system         sparc, solaris2.10        
status                                   
major          2                         
minor          3.1                       
year           2006                      
month          06                        
day            01                        
svn rev        38247                     
language       R                         
version.string Version 2.3.1 (2006-06-01)



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jennifer Barnes
PhD student: long range drought prediction 
Climate Extremes Group
Department of Space and Climate Physics
University College London
Holmbury St Mary 
Dorking, Surrey, RH5 6NT
Tel: 01483 204149
Mob: 07916 139187
Web: http://climate.mssl.ucl.ac.uk


From ligges at statistik.uni-dortmund.de  Fri Mar 16 10:31:02 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 16 Mar 2007 10:31:02 +0100
Subject: [R] Defining a floor value in a data.frame on a row-by-row basis
In-Reply-To: <0143A263BEF94644AC0D4027373EECD305430982@exyhmb08.jpn.nsroot.net>
References: <0143A263BEF94644AC0D4027373EECD305430982@exyhmb08.jpn.nsroot.net>
Message-ID: <20070316093102.GB20039@statistik.uni-dortmund.de>

* Feng, Ken [CIB-EQTY] <ken.feng at citigroup.com> [070316 09:40]:
> Hi,
> 
> I have a data frame x, and a vector y which limits x by rows
> so that no element of x in that row is smaller than y.
> 
> For example,
> 
> > x <- as.data.frame( t( array( 1:9, dim=c(3,3))))
> > y <- c( 2, 8, 0 )
> > x
>   V1 V2 V3
> 1  1  2  3
> 2  4  5  6
> 3  7  8  9
> > y
> [1] 2 8 0
> 
> =============================================
> I want to get this:
> 
>   V1 V2 V3
> 1  2  2  3
> 2  8  8  8
> 3  7  8  9
> 
> =============================================
> 
> I tried:
> 
> x[ x<=y ] <- y
> 
> but R wasn't happy.
> 
> I know I can do this with a loop, but there has to be a way which I can avoid it...

sapply(x, function(xc) ifelse(xc < y, y, xc))

Uwe Ligges

 
> Thanks in advance.
> 
> - Ken
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Fri Mar 16 10:36:14 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 16 Mar 2007 10:36:14 +0100
Subject: [R] Complex superscript plot text
In-Reply-To: <45FA2195.7060105@uoguelph.ca>
References: <45FA2195.7060105@uoguelph.ca>
Message-ID: <20070316093614.GC20039@statistik.uni-dortmund.de>

* Bob Farmer <rfarmer at uoguelph.ca> [070316 05:57]:
> Hi all:
> 
> I would like to create a line of plot margin text (using mtext() ) that 
> features both a superscript and a subset of an object.  However, I 
> cannot seem to do both things at once, nor have I found a way to paste 
> the two results together.
> 
> (I pull the object subset because this is part of a for-next loop to 
> make multiple graphs)
> 
> This example doesn't give me what I want from mtext():
> 
> GoodnessOfFits<-c(1, 0.75)
> 
> graphNumber<-1  #first loop of the for-next loop (not shown)
> 
> x<-seq(-10, 10, 1)
> y<-(x^2)
> plot(x,y)
> lines(x, predict(lm(y~I(x^2))))
> mtext(text=
> 	expression(R^2 * ": "* GoodnessOfFits[graphNumber]))
> 

plot(x,y)
lines(x, predict(lm(y~I(x^2))))
mtext(text = substitute(R^2 == GF, list(GF = GoodnessOfFits[graphNumber])), line = 1)
 

Uwe Ligges


> I recognize that in this example, I could extract the R-squared value 
> from the model in each loop, however this does not apply to my more 
> complicated real scenario.
> 
> Any suggestions?
> 
> Thanks.
> --Bob Farmer
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ray at mcs.vuw.ac.nz  Fri Mar 16 10:39:21 2007
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Fri, 16 Mar 2007 22:39:21 +1300
Subject: [R] How can i do the same thing in the China map?
In-Reply-To: <45FA5BCD.00BE0E.28518@m5-141.126.com>
References: <45FA5BCD.00BE0E.28518@m5-141.126.com>
Message-ID: <45FA65C9.3000601@mcs.vuw.ac.nz>

usstata wrote:
> The "maps" package has a function called "match.map", which is for map coloring .
> Its example is followed:
> 
> # filled map showing Republican vote in 1900
> # (figure 6 in the reference)
> data(state, package = "datasets")
> data(votes.repub)
> state.to.map <- match.map("state", state.name)
> x <- votes.repub[state.to.map, "1900"]
> gray.colors <- function(n) gray(rev(0:(n - 1))/n)
> color <- gray.colors(100)[floor(x)]
> map("state", fill = TRUE, col = color); map("state", add = TRUE)
> 
> I want to do the same thing in the China map, but I can't find the Provinces name of China.
> Who can help me ?
> 
nobody
> 
> ____________________
> a rookie
> 
Tell us who you are, and you may get a more substantial reply.

Ray Brownrigg


From shug0131 at yahoo.co.uk  Fri Mar 16 10:59:21 2007
From: shug0131 at yahoo.co.uk (simon bond)
Date: Fri, 16 Mar 2007 09:59:21 +0000 (GMT)
Subject: [R] gls bug?
Message-ID: <98284.72109.qm@web86908.mail.ukl.yahoo.com>

Dear R-help,

I found that the following code crashes R (version 2.4.0 in windows).


> x=rnorm(10,0.1,1)
> library(nlme)
> gls(x~0)



I quickly found a work-around for what I was trying to do, but I thought I should report this.


Best wishes


Simon Bond


From lassana.koita at aviation-civile.gouv.fr  Fri Mar 16 11:21:03 2007
From: lassana.koita at aviation-civile.gouv.fr (KOITA Lassana - STAC/ACE)
Date: Fri, 16 Mar 2007 11:21:03 +0100
Subject: [R] Problem to add legend to panel.histogram function
Message-ID: <OFC983B3CC.2DAE77CB-ONC12572A0.0036B7F6@aviation-civile.gouv.fr>




Hi, R users,
The following script works well, but I have problem to add some legend on
graphs. For example, I would like to past on each topright graphs one
thing:
   brown color for Gumbel density
   green color for Weibull density

Thank you for your help

(See attached file: compar_densit?s.txt)

Lassana KOITA
Charg? d'Etudes de S?curit? A?roportuaire et d'Analyse Statistique  /
Project Engineer Airport Safety Studies & Statistical analysis
Service Technique de l'Aviation Civile (STAC) / Civil Aviation Technical
Department
Direction G?n?rale de l'Aviation Civile (DGAC) / French Civil Aviation
Headquarters
Tel: 01 49 56 80 60
Fax: 01 49 56 82 14
E-mail: Lassana.Koita at aviation-civile.gouv.fr
http://www.stac.aviation-civile.gouv.fr/

From ligges at statistik.uni-dortmund.de  Fri Mar 16 11:26:23 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 16 Mar 2007 11:26:23 +0100
Subject: [R] How to use result of approxfun in a package?
In-Reply-To: <0699FD27-3030-4199-91D3-C50FCE3D0AED@stat.rutgers.edu>
References: <0699FD27-3030-4199-91D3-C50FCE3D0AED@stat.rutgers.edu>
Message-ID: <20070316102623.GA5341@statistik.uni-dortmund.de>

* Steve Buyske <buyske at stat.rutgers.edu> [070315 19:05]:
> I am working on a project where we start with start with 2 long,  
> equal-length vectors, massage them in various ways, and end up with a  
> function mapping one interval to another. I'll call that function  
> "f1." The last step in R is to generate f1 as the value of the  
> approxfun function. I would like to put f1 into a package, but  
> without having the package redo the creation of function f1. The  
> value of approxfun is a function; for example, I have
> 
>  > f1
> function (v)
> .C("R_approx", as.double(x), as.double(y), as.integer(n), xout =  
> as.double(v),
>      as.integer(length(v)), as.integer(method), as.double(yleft),
>      as.double(yright), as.double(f), NAOK = TRUE, PACKAGE = "base") 
> $xout
> <environment: 0x17719324>
> 
> I don't really understand how this is stored, and in particular, how  
> I should handle it so as to include the function f1 in a package. I  
> would like the users to be able to load the package and use f1  
> directly, rather than re-generate it using approxfun.


approxfun() makes use of some feature of some nice lexical scoping feature:
If a function (approxfun) returns some other function (f1), then this one is stored together with the environment of the first function (approxfun).
f1 requires not only v (its formal argument), but also objects x, y, n, method, yleft, yright, and f. All these are in the envionment that was created 
at runtime of approxfun() which is referenced as <environment: 0x17719324> in your special case. 

You can save() the object f1 together with the environment as in:
    save(f1, "f1.Rdata")

and reload it later with 
    load("f1.Rdata")

In a package, this means you can either save it as data or you can specify 
the call to approxfun() directly in an R file that executes it during 
either loading the installed package or during installation if a saved 
image of the packages is used, depending on the setting in your 
DESCRIPTION file.

Uwe Ligges



 
> Thanks,
> Steve
> ---
> Steve Buyske (rhymes with "nice key")
> Associate Research Professor
> Department of Statistics & Biostatistics
> Rutgers University
> Hill Center, 110 Frelinghuysen Rd
> Piscataway, NJ 08854
> buyske at stat.rutgers.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jmb at mssl.ucl.ac.uk  Fri Mar 16 11:04:28 2007
From: jmb at mssl.ucl.ac.uk (Jenny Barnes)
Date: Fri, 16 Mar 2007 10:04:28 +0000 (GMT)
Subject: [R] Problem installing R onto Solaris 2.10 system - need
	advice!!!!!
Message-ID: <200703161004.l2GA4SH4004004@msslhb.mssl.ucl.ac.uk>

Dear Andrew and R-help,

Here is the error message that we got when trying to install R v2.4 (which we 
tried to install before this newer version 2.4.1 - I'm afrid I didn't save the 
error message from the latest attempt with the new version):

configure
make
...
...
...

f90: CODE: 0 WORDS, DATA: 0 WORDS
gcc -G -L/usr/local/lib -o stats.so init.o kmeans.o  ansari.o bandwidths.o
chisq
sim.o d2x2xk.o fexact.o kendall.o ks.o  line.o smooth.o  prho.o swilk.o 
ksmooth
.o loessc.o isoreg.o Srunmed.o Trunmed.o  dblcen.o distance.o
hclust-utils.o  nl
s.o  HoltWinters.o PPsum.o arima.o burg.o filter.o  mAR.o pacf.o starma.o
port.o
 family.o sbart.o bsplvd.o bvalue.o bvalus.o loessf.o ppr.o qsbart.o 
sgram.o si
nerp.o sslvrg.o stxwx.o  hclust.o kmns.o  eureka.o stl.o portsrc.o
-L../../../..
/lib -lRblas  -lg2c -lm -lgcc_s
mkdir ../../../../library/stats/libs
building package 'datasets'
mkdir ../../../library/datasets
mkdir ../../../library/datasets/R
mkdir ../../../library/datasets/data
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library
'/tmp/R-2.4.0/library/stats/libs/stats.so'
:
  ld.so.1: R: fatal: relocation error: file
/tmp/R-2.4.0/library/stats/libs/stat
s.so: symbol __i_abs: referenced symbol not found
Execution halted
*** Error code 1

Many thanks,

Jenny

>
>Hi Jenny,
>
>advice: try posting the error message accompanying the failure to
>compile.
>
>Cheers
>
>Andrew
>
>On Fri, Mar 16, 2007 at 09:19:24AM +0000, Jenny Barnes wrote:
>> Dear R-Help friends,
>> 
>> I am unable to get the latest version of R (2.4.1) to compile on my solaris 
10 
>> system - has anybody else experienced this problem and are you able to offer 
me 
>> any advice? 
>> 
>> I appreciate your time, many thanks,
>> 
>> Jenny Barnes
>> 
>> 
>> Here are my CURRENT specifications:
>> 
>> platform       sparc-sun-solaris2.10     
>> arch           sparc                     
>> os             solaris2.10               
>> system         sparc, solaris2.10        
>> status                                   
>> major          2                         
>> minor          3.1                       
>> year           2006                      
>> month          06                        
>> day            01                        
>> svn rev        38247                     
>> language       R                         
>> version.string Version 2.3.1 (2006-06-01)
>> 
>> 
>> 
>> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>> Jennifer Barnes
>> PhD student: long range drought prediction 
>> Climate Extremes Group
>> Department of Space and Climate Physics
>> University College London
>> Holmbury St Mary 
>> Dorking, Surrey, RH5 6NT
>> Tel: 01483 204149
>> Mob: 07916 139187
>> Web: http://climate.mssl.ucl.ac.uk
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>-- 
>Andrew Robinson  
>Department of Mathematics and Statistics            Tel: +61-3-8344-9763
>University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
>http://www.ms.unimelb.edu.au/~andrewpr
>http://blogs.mbs.edu/fishing-in-the-bay/ 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jennifer Barnes
PhD student: long range drought prediction 
Climate Extremes Group
Department of Space and Climate Physics
University College London
Holmbury St Mary 
Dorking, Surrey, RH5 6NT
Tel: 01483 204149
Mob: 07916 139187
Web: http://climate.mssl.ucl.ac.uk


From guillermo.villa at uc3m.es  Fri Mar 16 12:44:19 2007
From: guillermo.villa at uc3m.es (Guillermo Villa)
Date: Fri, 16 Mar 2007 12:44:19 +0100
Subject: [R] corAR1 in a random effects panel
Message-ID: <50a247490703160444r195a5c8x1c3aed5a26c3ab0a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070316/c7c4e5a4/attachment.pl 

From albmont at centroin.com.br  Fri Mar 16 12:56:56 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 16 Mar 2007 09:56:56 -0200
Subject: [R] Bad points in regression
In-Reply-To: <50a247490703160444r195a5c8x1c3aed5a26c3ab0a@mail.gmail.com>
References: <50a247490703160444r195a5c8x1c3aed5a26c3ab0a@mail.gmail.com>
Message-ID: <20070316115256.M17558@centroin.com.br>

I have a question, maybe it's better to explain by example:

alpha <- 0.3
beta <- 0.4
sigma <- 0.5
err <- rnorm(100)
err[15] <- 5; err[25] <- -4; err[50] <- 10
x <- 1:100
y <- alpha + beta * x + sigma * err
ll <- lm(y ~ x)
plot(ll)

Now, the graphs clearly show that 15, 25 and 50 are the indexes
of the bad points. How can I retrieve this information from ll?

Alberto Monteiro


From sgunnste at jhsph.edu  Fri Mar 16 13:03:40 2007
From: sgunnste at jhsph.edu (Snaebjorn Gunnsteinsson)
Date: Fri, 16 Mar 2007 18:03:40 +0600
Subject: [R] Segmentation fault in estimating structural equation models
	with the SEM package.
Message-ID: <533AB191-D16A-4E56-8025-DE4D0B1D0D5D@jhsph.edu>

Dear R-users,

     I am running a large number of simulations and estimating a  
structural equation model for each one using the SEM package. Each  
run of my program has around 8000 simulations. Most of the time the  
program completes all of them correctly but sometimes I get a  
segmentation fault in the sem routine and my program stops with the  
following error message:

>  *** caught segfault ***
> address (nil), cause 'unknown'
>
> Traceback:	
> 1: nlm(if (analytic.gradient) objective.2 else objective.1,  
> start,     hessian = TRUE, iterlim = maxiter, print.level = if  
> (debug) 2 else 0,     typsize = typsize, .
> ..)
> 2: sem.default(ram = ram, S = S, N = N, param.names = pars,  
> var.names = vars,     fixed.x = fixed.x, debug = debug, ...)
> 3: sem(ram = ram, S = S, N = N, param.names = pars, var.names =  
> vars,     fixed.x = fixed.x, debug = debug, ...)
> 4: sem.mod(modelspec, cor.or.cov.mat, N, obs.variables =  
> comb.set,     warn = FALSE)
> 5: sem(modelspec, cor.or.cov.mat, N, obs.variables = comb.set, warn  
> = FALSE)
> 6: try(sem(modelspec, cor.or.cov.mat, N, obs.variables =  
> comb.set,     warn = FALSE), silent = TRUE)
> 7: eval.with.vis(expr, envir, enclos)
> 8: eval.with.vis(ei, envir)
> 9: source("analysis.base.R")
> aborting ...

Does anybody have an idea about how I could prevent this from  
happening or catch it so that my program could continue running?  
Could I, in some way, manage the memory myself to prevent this?

The model is correctly specified and clearly identified but the  
covariance matrix changes between simulations (it is a two latent  
variable measurement model with more than 4 observed variables on  
each LV).

Many thanks,
regards,
Snaebjorn

P.S. Using R 2.4.1 (2006-06-01) on x86_64 GNU/Linux
Using version 0.9-6 (2006/11/22) of the SEM package.


-----------------------------------------------------------
Snaebjorn Gunnsteinsson
JiVitA Science Fellow
The JiVitA Project (NIPHP-USAID)
Rangpur, Bangladesh
www.jivita.org
Mobile: +880-171-320-2560


From salcaraz at obelix.umh.es  Fri Mar 16 13:02:57 2007
From: salcaraz at obelix.umh.es (salcaraz at obelix.umh.es)
Date: Fri, 16 Mar 2007 13:02:57 +0100 (CET)
Subject: [R] Can I scale the labels in a 'persp' graph?
In-Reply-To: <mailman.11.1174042804.26780.r-help@stat.math.ethz.ch>
References: <mailman.11.1174042804.26780.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0703161252350.30144@obelix.umh.es>

Hi all:

I'm using 'persp' for 3D graphics.

I need the axis's labels smaller than by defect.

I see in 'help()', the information about 'par()'.

I have wrote:

>par(.....,cex.axis=0.5,cex.lab=0.5)
perspc(.................)

and the result don't change.

The question is: Can I change the size of labels in the perps graph??

Thank you in advance:

/salva





'cex.axis' The magnification to be used for axis annotation
           relative to the current setting of 'cex'. (Some functions
           such as 'points' accept a vector of values which are
           recycled.  Other uses will take just the first value if a
           vector of length greater than one is supplied.)

'cex.lab' The magnification to be used for x and y labels relative
           to the current setting of 'cex'.


From ted.harding at nessie.mcc.ac.uk  Fri Mar 16 13:19:05 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 16 Mar 2007 12:19:05 -0000 (GMT)
Subject: [R] Bad points in regression
In-Reply-To: <20070316115256.M17558@centroin.com.br>
Message-ID: <XFMail.070316121905.ted.harding@nessie.mcc.ac.uk>

On 16-Mar-07 11:56:56, Alberto Monteiro wrote:
> I have a question, maybe it's better to explain by example:
> 
> alpha <- 0.3
> beta <- 0.4
> sigma <- 0.5
> err <- rnorm(100)
> err[15] <- 5; err[25] <- -4; err[50] <- 10
> x <- 1:100
> y <- alpha + beta * x + sigma * err
> ll <- lm(y ~ x)
> plot(ll)
> 
> Now, the graphs clearly show that 15, 25 and 50 are the indexes
> of the bad points. How can I retrieve this information from ll?
> 
> Alberto Monteiro

ll is the output of a linear model fiited by lm(), and so has
several components (see ?lm in the section "Value"), one of
which is "residuals" (which can be abbreviated to "res").

So, in the case of your example,

  which(abs(ll$res)>2)
  15 25 50 

extracts the information you want (and the ">2" was inspired by
looking at the "residuals" plot from your "plot(ll)").

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 16-Mar-07                                       Time: 12:19:02
------------------------------ XFMail ------------------------------


From albmont at centroin.com.br  Fri Mar 16 13:41:50 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 16 Mar 2007 10:41:50 -0200
Subject: [R] Bad points in regression
In-Reply-To: <XFMail.070316121905.ted.harding@nessie.mcc.ac.uk>
References: <20070316115256.M17558@centroin.com.br>
	<XFMail.070316121905.ted.harding@nessie.mcc.ac.uk>
Message-ID: <20070316123541.M40032@centroin.com.br>

Ted Harding wrote:
> 
>> alpha <- 0.3
>> beta <- 0.4
>> sigma <- 0.5
>> err <- rnorm(100)
>> err[15] <- 5; err[25] <- -4; err[50] <- 10
>> x <- 1:100
>> y <- alpha + beta * x + sigma * err
>> ll <- lm(y ~ x)
>> plot(ll)
> 
> ll is the output of a linear model fiited by lm(), and so has
> several components (see ?lm in the section "Value"), one of
> which is "residuals" (which can be abbreviated to "res").
> 
> So, in the case of your example,
> 
>   which(abs(ll$res)>2)
>   15 25 50
> 
> extracts the information you want (and the ">2" was inspired by
> looking at the "residuals" plot from your "plot(ll)").
>
Ok, but how can I grab those points _in general_? What is the
criterium that plot used to mark those points as bad points?

names(ll)

gives:

 [1] "coefficients"  "residuals"     "effects"       "rank"         
 [5] "fitted.values" "assign"        "qr"            "df.residual"  
 [9] "xlevels"       "call"          "terms"         "model"        

None of them include information about those bad points.

Alberto Monteiro


From ripley at stats.ox.ac.uk  Fri Mar 16 13:37:19 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Mar 2007 12:37:19 +0000 (GMT)
Subject: [R] Problem installing R onto Solaris 2.10 system - need
 advice!!!!!
In-Reply-To: <200703161004.l2GA4SH4004004@msslhb.mssl.ucl.ac.uk>
References: <200703161004.l2GA4SH4004004@msslhb.mssl.ucl.ac.uk>
Message-ID: <Pine.LNX.4.64.0703161056040.5164@gannet.stats.ox.ac.uk>

It seems you are using f90, but FLIBS was computed using g77.  That would 
mean that something has been altered since R was configured, or that the 
way the Fortran compiler was specified was incorrect.

I think you need to start again and make use of only one set of compilers.

On Fri, 16 Mar 2007, Jenny Barnes wrote:

> Dear Andrew and R-help,
>
> Here is the error message that we got when trying to install R v2.4 (which we
> tried to install before this newer version 2.4.1 - I'm afrid I didn't save the
> error message from the latest attempt with the new version):
>
> configure
> make
> ...
> ...
> ...
>
> f90: CODE: 0 WORDS, DATA: 0 WORDS
> gcc -G -L/usr/local/lib -o stats.so init.o kmeans.o  ansari.o bandwidths.o
> chisq
> sim.o d2x2xk.o fexact.o kendall.o ks.o  line.o smooth.o  prho.o swilk.o
> ksmooth
> .o loessc.o isoreg.o Srunmed.o Trunmed.o  dblcen.o distance.o
> hclust-utils.o  nl
> s.o  HoltWinters.o PPsum.o arima.o burg.o filter.o  mAR.o pacf.o starma.o
> port.o
> family.o sbart.o bsplvd.o bvalue.o bvalus.o loessf.o ppr.o qsbart.o
> sgram.o si
> nerp.o sslvrg.o stxwx.o  hclust.o kmns.o  eureka.o stl.o portsrc.o
> -L../../../..
> /lib -lRblas  -lg2c -lm -lgcc_s
> mkdir ../../../../library/stats/libs
> building package 'datasets'
> mkdir ../../../library/datasets
> mkdir ../../../library/datasets/R
> mkdir ../../../library/datasets/data
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>        unable to load shared library
> '/tmp/R-2.4.0/library/stats/libs/stats.so'
> :
>  ld.so.1: R: fatal: relocation error: file
> /tmp/R-2.4.0/library/stats/libs/stat
> s.so: symbol __i_abs: referenced symbol not found
> Execution halted
> *** Error code 1
>
> Many thanks,
>
> Jenny
>
>>
>> Hi Jenny,
>>
>> advice: try posting the error message accompanying the failure to
>> compile.
>>
>> Cheers
>>
>> Andrew
>>
>> On Fri, Mar 16, 2007 at 09:19:24AM +0000, Jenny Barnes wrote:
>>> Dear R-Help friends,
>>>
>>> I am unable to get the latest version of R (2.4.1) to compile on my solaris
> 10
>>> system - has anybody else experienced this problem and are you able to offer
> me
>>> any advice?
>>>
>>> I appreciate your time, many thanks,
>>>
>>> Jenny Barnes
>>>
>>>
>>> Here are my CURRENT specifications:
>>>
>>> platform       sparc-sun-solaris2.10
>>> arch           sparc
>>> os             solaris2.10
>>> system         sparc, solaris2.10
>>> status
>>> major          2
>>> minor          3.1
>>> year           2006
>>> month          06
>>> day            01
>>> svn rev        38247
>>> language       R
>>> version.string Version 2.3.1 (2006-06-01)
>>>
>>>
>>>
>>> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>> Jennifer Barnes
>>> PhD student: long range drought prediction
>>> Climate Extremes Group
>>> Department of Space and Climate Physics
>>> University College London
>>> Holmbury St Mary
>>> Dorking, Surrey, RH5 6NT
>>> Tel: 01483 204149
>>> Mob: 07916 139187
>>> Web: http://climate.mssl.ucl.ac.uk
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Andrew Robinson
>> Department of Mathematics and Statistics            Tel: +61-3-8344-9763
>> University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
>> http://www.ms.unimelb.edu.au/~andrewpr
>> http://blogs.mbs.edu/fishing-in-the-bay/
>
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Jennifer Barnes
> PhD student: long range drought prediction
> Climate Extremes Group
> Department of Space and Climate Physics
> University College London
> Holmbury St Mary
> Dorking, Surrey, RH5 6NT
> Tel: 01483 204149
> Mob: 07916 139187
> Web: http://climate.mssl.ucl.ac.uk
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From murdoch at stats.uwo.ca  Fri Mar 16 14:20:29 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 16 Mar 2007 09:20:29 -0400
Subject: [R] Can I scale the labels in a 'persp' graph?
In-Reply-To: <Pine.LNX.4.64.0703161252350.30144@obelix.umh.es>
References: <mailman.11.1174042804.26780.r-help@stat.math.ethz.ch>
	<Pine.LNX.4.64.0703161252350.30144@obelix.umh.es>
Message-ID: <45FA999D.1090406@stats.uwo.ca>

On 3/16/2007 8:02 AM, salcaraz at obelix.umh.es wrote:
> Hi all:
> 
> I'm using 'persp' for 3D graphics.
> 
> I need the axis's labels smaller than by defect.
> 
> I see in 'help()', the information about 'par()'.
> 
> I have wrote:
> 
>>par(.....,cex.axis=0.5,cex.lab=0.5)
> perspc(.................)
> 
> and the result don't change.
> 
> The question is: Can I change the size of labels in the perps graph??
> 
> Thank you in advance:
> 
> /salva
> 
> 
> 
> 
> 
> 'cex.axis' The magnification to be used for axis annotation
>            relative to the current setting of 'cex'. (Some functions
>            such as 'points' accept a vector of values which are
>            recycled.  Other uses will take just the first value if a
>            vector of length greater than one is supplied.)
> 
> 'cex.lab' The magnification to be used for x and y labels relative
>            to the current setting of 'cex'.

Those don't appear to be supported by persp, but cex is: e.g.

x <- 1:10
y <- 1:10
z <- outer(x,y,function(x,y) sin((x+y)/10))
persp(x,y,z, cex=0.5)

Duncan Murdoch


From ripley at stats.ox.ac.uk  Fri Mar 16 14:28:00 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Mar 2007 13:28:00 +0000 (GMT)
Subject: [R] Bad points in regression
In-Reply-To: <20070316123541.M40032@centroin.com.br>
References: <20070316115256.M17558@centroin.com.br>
	<XFMail.070316121905.ted.harding@nessie.mcc.ac.uk>
	<20070316123541.M40032@centroin.com.br>
Message-ID: <Pine.LNX.4.64.0703161322420.10190@gannet.stats.ox.ac.uk>

On Fri, 16 Mar 2007, Alberto Monteiro wrote:

> Ted Harding wrote:
>>
>>> alpha <- 0.3
>>> beta <- 0.4
>>> sigma <- 0.5
>>> err <- rnorm(100)
>>> err[15] <- 5; err[25] <- -4; err[50] <- 10
>>> x <- 1:100
>>> y <- alpha + beta * x + sigma * err
>>> ll <- lm(y ~ x)
>>> plot(ll)
>>
>> ll is the output of a linear model fiited by lm(), and so has
>> several components (see ?lm in the section "Value"), one of
>> which is "residuals" (which can be abbreviated to "res").
>>
>> So, in the case of your example,
>>
>>   which(abs(ll$res)>2)
>>   15 25 50
>>
>> extracts the information you want (and the ">2" was inspired by
>> looking at the "residuals" plot from your "plot(ll)").
>>
> Ok, but how can I grab those points _in general_? What is the
> criterium that plot used to mark those points as bad points?
>
> names(ll)
>
> gives:
>
> [1] "coefficients"  "residuals"     "effects"       "rank"
> [5] "fitted.values" "assign"        "qr"            "df.residual"
> [9] "xlevels"       "call"          "terms"         "model"
>
> None of them include information about those bad points.

But it is the plot method that you are using, not the object ll.  If you 
examine stats::plot.lm you will see what it does: label the points with 
the 'id.n' largest (in absolute value) residuals (standardized residuals 
for types 2 and 3).

And ?plot.lm also tells you that.

BTW, 'bad points' seems your own description: it does not appear in the R 
documentation.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From petr.pikal at precheza.cz  Fri Mar 16 14:28:44 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 16 Mar 2007 14:28:44 +0100
Subject: [R] Bad points in regression
In-Reply-To: <20070316115256.M17558@centroin.com.br>
References: <50a247490703160444r195a5c8x1c3aed5a26c3ab0a@mail.gmail.com>
Message-ID: <45FAA99C.15598.16558D5@localhost>

Hi

you can check ?influence  or ?influence.measures to evaluate some 
regression diagnostics

Regards
Petr


On 16 Mar 2007 at 9:56, Alberto Monteiro wrote:

From:           	"Alberto Monteiro" <albmont at centroin.com.br>
To:             	r-help at stat.math.ethz.ch
Date sent:      	Fri, 16 Mar 2007 09:56:56 -0200
Subject:        	[R] Bad points in regression

> I have a question, maybe it's better to explain by example:
> 
> alpha <- 0.3
> beta <- 0.4
> sigma <- 0.5
> err <- rnorm(100)
> err[15] <- 5; err[25] <- -4; err[50] <- 10
> x <- 1:100
> y <- alpha + beta * x + sigma * err
> ll <- lm(y ~ x)
> plot(ll)
> 
> Now, the graphs clearly show that 15, 25 and 50 are the indexes
> of the bad points. How can I retrieve this information from ll?
> 
> Alberto Monteiro
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From yuklap.yip at yale.edu  Fri Mar 16 14:36:36 2007
From: yuklap.yip at yale.edu (Yuk Lap Yip (Kevin))
Date: Fri, 16 Mar 2007 09:36:36 -0400
Subject: [R] Implementing trees in R
Message-ID: <45FA9D64.6070708@yale.edu>

Hi all,

    I am rather new to R. Recently I have been trying to implement some 
tree algorithms in R. I used lists to model tree nodes. I thought 
something like this would work:

    parent <- list();
    child <- list();
    parent$child1 <- child;
    child$parent <- parent;

    When I tried to check whether a node is its parent's first child 
using "if (node$parent$child1 == node)", it always returned false. Then 
I realized that it does not really work because "parent$child1 <- child" 
actually makes a copy of child instead of referencing it. I think one 
possible fix is to keep a list of node objects, and make references 
using the positions in the list. For example, I think the following 
would work:

    parent <- list();
    child <- list();
    nodes <- list(parent, child);
    parent$child1 <- 2;
    child$parent <- 1;

    Then the "first child" test can be rewritten as "if 
(nodes[[nodes[[nodeId]]$parent]]$child1 == nodeId)". However, I would 
prefer not to implement trees in this way, as it requires the 
inconvenient and error-prone manipulations of node IDs.

    May I know if there is a way to make object references to lists? Or 
are there other ways to implement tree data structures in R?

    BTW, I checked how hclust was implemented, and noticed that it calls 
an external Fortran program. I would want a solution not involving any 
external programs.

    Thanks.

-- 


	God bless.

	Kevin


From ted.harding at nessie.mcc.ac.uk  Fri Mar 16 14:43:58 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 16 Mar 2007 13:43:58 -0000 (GMT)
Subject: [R] Bad points in regression
In-Reply-To: <20070316123541.M40032@centroin.com.br>
Message-ID: <XFMail.070316134358.ted.harding@nessie.mcc.ac.uk>

On 16-Mar-07 12:41:50, Alberto Monteiro wrote:
> Ted Harding wrote:
>> 
>>> alpha <- 0.3
>>> beta <- 0.4
>>> sigma <- 0.5
>>> err <- rnorm(100)
>>> err[15] <- 5; err[25] <- -4; err[50] <- 10
>>> x <- 1:100
>>> y <- alpha + beta * x + sigma * err
>>> ll <- lm(y ~ x)
>>> plot(ll)
>> 
>> ll is the output of a linear model fiited by lm(), and so has
>> several components (see ?lm in the section "Value"), one of
>> which is "residuals" (which can be abbreviated to "res").
>> 
>> So, in the case of your example,
>> 
>>   which(abs(ll$res)>2)
>>   15 25 50
>> 
>> extracts the information you want (and the ">2" was inspired by
>> looking at the "residuals" plot from your "plot(ll)").
>>
> Ok, but how can I grab those points _in general_? What is the
> criterium that plot used to mark those points as bad points?

Ahh ... ! I see what you're after. OK, look at the plot method
for lm():

?plot.lm
  ## S3 method for class 'lm':
  plot(x, which = 1:4,
    caption = c("Residuals vs Fitted", "Normal Q-Q plot",
      "Scale-Location plot", "Cook's distance plot"),
      panel = points,
      sub.caption = deparse(x$call), main = "",
      ask = prod(par("mfcol")) < length(which) && dev.interactive(),
      ...,
      id.n = 3, labels.id = names(residuals(x)), cex.id = 0.75)


where (see further down):

  id.n: number of points to be labelled in each plot, starting with
    the most extreme.

and note, in the default parameter-values listing above:

  id.n = 3

Hence, the 3 most extreme points (according to the criterion being
plotted in each plot) are marked in each plot.

So, for instance3, try

  plot(ll,id.n=5)

and you will get points 10,15,25,28,50. And so on. But that
pre-supposes that you know how many points are "exceptional".


What is meant by "extreme"is not stated in the help page ?plot.lm,
but can be identified by inspecting the code for plot.lm(), which
you can see by entering

  plot.lm

In your example, if you omit the line which assigns anomalous values
to err[15[, err[25] and err[50], then you are likely to observe that
different points get identified on different plots. For instance,
I just got the following results for the default id.n=3:

[1] Residuals vs Fitted:       41,53,59
[2] Standardised Residuals:    41,53,59
[3] sqrt(Stand Res) vs Fitted: 41,53,59
[4] Cook's Distance:           59,96,97


There are several approaches (with somewhat different outcomes)
to identifying "outliers". If you apply one of these, you will
probably get the identities of the points anyway.

Again in the context of your example (where in fact you
deliberately set 3 points to have exceptional errors, thus
coincidentally the same as the default value 3 of id.n),
you could try different values for id.n and inspect the graphs
to see whether a given value of id.n marks some points that
do not look exceptional relative to the mass of the other points.

So, the above plot(ll,id.n=5) gave me one point, "10" on the
residuals plot, which apparently belonged to the general
distribution of residuals.

Hoping this helps,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 16-Mar-07                                       Time: 13:43:54
------------------------------ XFMail ------------------------------


From f.harrell at vanderbilt.edu  Fri Mar 16 14:44:06 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 16 Mar 2007 08:44:06 -0500
Subject: [R] R and clinical studies
In-Reply-To: <20070316093649.7klhg4n673gogs8c@webmail.adm.unige.ch>
References: <20070309142909.hgceqlqqasssggo0@webmail.adm.unige.ch>	<27CA3827C6B33E40874682C469E774DD04DB9136@FMD3CT001.fda.gov>
	<20070316093649.7klhg4n673gogs8c@webmail.adm.unige.ch>
Message-ID: <45FA9F26.2070304@vanderbilt.edu>

Delphine Fontaine wrote:
> Thanks for your answer which was very helpfull. I have another question:
> 
> I have read in this document  
> (http://cran.r-project.org/doc/manuals/R-intro.pdf) that most of the  
> programs written in R are ephemeral and that new releases are not  
> always compatible with previous releases. What I would like to know is  
> if R functions are already validated and if not, what should we do to  
> validate a R function ?
> 

In the sense in which most persons use the term 'validate', it means to 
show with one or more datasets that the function is capable of producing 
the right answer.  It doesn't mean that it produces the right answer for 
every dataset although we hope it does.  [As an aside, most errors are 
in the data manipulation phase, not in the analysis phase.]  So I think 
that instead of validating functions we should spend more effort on 
validating analyses [and validating analysis file derivation].  Pivotal 
analyses can be re-done a variety of ways, in R or in separate 
programmable packages such as Stata.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From ggrothendieck at gmail.com  Fri Mar 16 14:59:59 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 16 Mar 2007 09:59:59 -0400
Subject: [R] Implementing trees in R
In-Reply-To: <45FA9D64.6070708@yale.edu>
References: <45FA9D64.6070708@yale.edu>
Message-ID: <971536df0703160659u1c339e53k474b962bd846d76e@mail.gmail.com>

Lists are not good for this.  There is an example in section 3.3 of
the proto vignette of using proto objects for this.  That section
also references an S4 example although its pretty messy with S4.

You might want to look at the graph, RBGL and graphviz packages
in Bioconductor and the dynamicgraph, mathgraph and sna packages
on CRAN.

On 3/16/07, Yuk Lap Yip (Kevin) <yuklap.yip at yale.edu> wrote:
> Hi all,
>
>    I am rather new to R. Recently I have been trying to implement some
> tree algorithms in R. I used lists to model tree nodes. I thought
> something like this would work:
>
>    parent <- list();
>    child <- list();
>    parent$child1 <- child;
>    child$parent <- parent;
>
>    When I tried to check whether a node is its parent's first child
> using "if (node$parent$child1 == node)", it always returned false. Then
> I realized that it does not really work because "parent$child1 <- child"
> actually makes a copy of child instead of referencing it. I think one
> possible fix is to keep a list of node objects, and make references
> using the positions in the list. For example, I think the following
> would work:
>
>    parent <- list();
>    child <- list();
>    nodes <- list(parent, child);
>    parent$child1 <- 2;
>    child$parent <- 1;
>
>    Then the "first child" test can be rewritten as "if
> (nodes[[nodes[[nodeId]]$parent]]$child1 == nodeId)". However, I would
> prefer not to implement trees in this way, as it requires the
> inconvenient and error-prone manipulations of node IDs.
>
>    May I know if there is a way to make object references to lists? Or
> are there other ways to implement tree data structures in R?
>
>    BTW, I checked how hclust was implemented, and noticed that it calls
> an external Fortran program. I would want a solution not involving any
> external programs.
>
>    Thanks.
>
> --
>
>
>        God bless.
>
>        Kevin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From guillermo.villa at uc3m.es  Fri Mar 16 15:06:26 2007
From: guillermo.villa at uc3m.es (Guillermo Villa)
Date: Fri, 16 Mar 2007 15:06:26 +0100
Subject: [R] corAR1 in a random effects panel
In-Reply-To: <50a247490703160444r195a5c8x1c3aed5a26c3ab0a@mail.gmail.com>
References: <50a247490703160444r195a5c8x1c3aed5a26c3ab0a@mail.gmail.com>
Message-ID: <50a247490703160706n6e4b48c8iad5b5fe2c75b2b4f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070316/7e617b48/attachment.pl 

From ggrothendieck at gmail.com  Fri Mar 16 15:06:55 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 16 Mar 2007 10:06:55 -0400
Subject: [R] Implementing trees in R
In-Reply-To: <971536df0703160659u1c339e53k474b962bd846d76e@mail.gmail.com>
References: <45FA9D64.6070708@yale.edu>
	<971536df0703160659u1c339e53k474b962bd846d76e@mail.gmail.com>
Message-ID: <971536df0703160706p2e462810jd070966ea79980ad@mail.gmail.com>

Let me rephrase that.  Lists do not support references but they
could be used to represent trees.

list(a = list(a = 1, b = list(2, 3, d = list(4, 5)), c = list(4, 5))

is a tree whose top nodes are a, b, c and b contains three nodes
2, 3 and d and d contains 2 nodes.

However, if you want to do it via references as requested then lists
are not appropriate.

On 3/16/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Lists are not good for this.  There is an example in section 3.3 of
> the proto vignette of using proto objects for this.  That section
> also references an S4 example although its pretty messy with S4.
>
> You might want to look at the graph, RBGL and graphviz packages
> in Bioconductor and the dynamicgraph, mathgraph and sna packages
> on CRAN.
>
> On 3/16/07, Yuk Lap Yip (Kevin) <yuklap.yip at yale.edu> wrote:
> > Hi all,
> >
> >    I am rather new to R. Recently I have been trying to implement some
> > tree algorithms in R. I used lists to model tree nodes. I thought
> > something like this would work:
> >
> >    parent <- list();
> >    child <- list();
> >    parent$child1 <- child;
> >    child$parent <- parent;
> >
> >    When I tried to check whether a node is its parent's first child
> > using "if (node$parent$child1 == node)", it always returned false. Then
> > I realized that it does not really work because "parent$child1 <- child"
> > actually makes a copy of child instead of referencing it. I think one
> > possible fix is to keep a list of node objects, and make references
> > using the positions in the list. For example, I think the following
> > would work:
> >
> >    parent <- list();
> >    child <- list();
> >    nodes <- list(parent, child);
> >    parent$child1 <- 2;
> >    child$parent <- 1;
> >
> >    Then the "first child" test can be rewritten as "if
> > (nodes[[nodes[[nodeId]]$parent]]$child1 == nodeId)". However, I would
> > prefer not to implement trees in this way, as it requires the
> > inconvenient and error-prone manipulations of node IDs.
> >
> >    May I know if there is a way to make object references to lists? Or
> > are there other ways to implement tree data structures in R?
> >
> >    BTW, I checked how hclust was implemented, and noticed that it calls
> > an external Fortran program. I would want a solution not involving any
> > external programs.
> >
> >    Thanks.
> >
> > --
> >
> >
> >        God bless.
> >
> >        Kevin
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From avanisco at univ-fcomte.fr  Fri Mar 16 15:09:39 2007
From: avanisco at univ-fcomte.fr (vaniscotte)
Date: Fri, 16 Mar 2007 15:09:39 +0100
Subject: [R] fitting of all possible models
In-Reply-To: <488EFAAF-938F-410F-8387-3B6825170008@student.ru.nl>
References: <FE8C160D1505B24497FA7C78D4DADACA0478FC@EA-MAIL.eawag.wroot.emp-eaw.ch>
	<488EFAAF-938F-410F-8387-3B6825170008@student.ru.nl>
Message-ID: <45FAA523.9060409@univ-fcomte.fr>

Rense Nieuwenhuis a ?crit :
> Dear Lukas,
>
> allthough I'm  intrigued by the purpose of what you are trying to do,  
> as mentioned by some of the other persons on this list, I liked the  
> challenge to write such a function.
>
> I came up with the following during some train-traveling this morning:
>
>
> tum <- function(x)
> 	{
> 		tum <- matrix(data=NA, nrow=2^x, ncol=x)
> 		
> 		for (i in 1:x)
> 			{
> 				tum[,i] <- c(rep(NA,2^i/2),rep(i+1,2^i/2))
> 			}
> 		
> 		return(tum)
> 	}
>
> ###
>
> all.models <- function(model)
> 	{
> 		npred <- length(model$coefficients) - 1
> 		matr.model <- tum(npred)
> 		output <- matrix(data=NA, nrow=2^npred, ncol=1)
> 	
> 		for (t in 2:2^npred)
> 		{
> 			preds <- names(model$coefficients)
> 			interc <- names(model$coefficients)[1]
> 			form <- as.formula(paste(". ~", paste(preds[na.omit(matr.model 
> [t,])],collapse="+")))
>
> 			model2 <- update(model, form)
> 			output[t,] <- mean(resid(model2)^2)
> 		}
> 	
> 	return(output)
> 	
> 	}
>
> ##
>
>
> As you can see, I used a helper-function (tum, "the ultimate matrix")  
> to the actual function. Also, I wrote it using lm instead of glm, but  
> I suppose that you can easily alter that. As well, the function now  
> returns just some regular fit-measurement. But that is not all that  
> essential, I think.
>
> The main point is: it works! Using this on my G4 mac, with a lm of 10  
> predictors and 18 cases, it returns the output quite fast (<1 minute).
>
> I hope you can put this to use. It needs some easy adapting to your  
> specific needs, but I don't expect that to be a problem. If you need  
> help with that, please contact me.
>
> I'd appreciate to hear from you, if this function is helpful in any way.
>
> sincerely,
>
> Rense Nieuwenhuis
>
>
>
>
>
> On Feb 27, 2007, at 8:46 , Indermaur Lukas wrote:
>
>   
>> Hi,
>> Fitting all possible models (GLM) with 10 predictors will result in  
>> loads of (2^10 - 1) models. I want to do that in order to get the  
>> importance of variables (having an unbalanced variable design) by  
>> summing the up the AIC-weights of models including the same  
>> variable, for every variable separately. It's time consuming and  
>> annoying to define all possible models by hand.
>>
>> Is there a command, or easy solution to let R define the set of all  
>> possible models itself? I defined models in the following way to  
>> process them with a batch job:
>>
>> # e.g. model 1
>> preference<- formula(Y~Lwd + N + Sex + YY)
>> # e.g. model 2
>> preference_heterogeneity<- formula(Y~Ri + Lwd + N + Sex + YY)
>> etc.
>> etc.
>>
>>
>> I appreciate any hint
>> Cheers
>> Lukas
>>
>>
>>
>>
>>
>> ???
>> Lukas Indermaur, PhD student
>> eawag / Swiss Federal Institute of Aquatic Science and Technology
>> ECO - Department of Aquatic Ecology
>> ?berlandstrasse 133
>> CH-8600 D?bendorf
>> Switzerland
>>
>> Phone: +41 (0) 71 220 38 25
>> Fax    : +41 (0) 44 823 53 15
>> Email: lukas.indermaur at eawag.ch
>> www.lukasindermaur.ch
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>     
>
>
> 	[[alternative HTML version deleted]]
>
>   
> ------------------------------------------------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   
Dear Lukas,

In order to define the set of all possible models I used the function combos() of the hier.part package. 
The function all.regs() of the same package fit all the possible models.

God luck

Am?lie Vaniscotte (Phd)

Department of Environmental Biology
UsC INRA EA3184MRT
Institute for Environmental Sciences and Technology

University of Franche-comt?
Place Leclerc, 25030 Besan?on cedex
FRANCE
Tel. :   +33 (0)381 665 714
Fax :  +33 (0)381 665 797
E-m at il: amelie.vaniscotte at univ-fcomte.fr
http://lbe.univ-fcomte.fr/


From avanisco at univ-fcomte.fr  Fri Mar 16 15:19:12 2007
From: avanisco at univ-fcomte.fr (vaniscotte)
Date: Fri, 16 Mar 2007 15:19:12 +0100
Subject: [R] Hierarchical partitioning
Message-ID: <45FAA760.3060102@univ-fcomte.fr>

Hello,

I iterate my question concerning hierarchical partitioning: Is it 
possible to run that for a multinomial model?

I have adapted the function hier.part ("hier.part" package) to my 
multinomial model. I met a problem with the partition() function: I 
obtain NEGATIVE Independent contributions!
It seems to be a huge error!

Unfortunately, I cannot access the script as it calls a C code. 
Consequently this function is a black box for me.

Could anyone help me? I really need to investigate this independent 
contributions!
Thanks a lot,

Am?lie Vaniscotte (Phd)

Department of Environmental Biology
UsC INRA EA3184MRT
Institute for Environmental Sciences and Technology

University of Franche-comt?
Place Leclerc, 25030 Besan?on cedex
FRANCE
Tel. :   +33 (0)381 665 714
Fax :  +33 (0)381 665 797
E-m at il: amelie.vaniscotte at univ-fcomte.fr
http://lbe.univ-fcomte.fr/


From yuklap.yip at yale.edu  Fri Mar 16 15:41:34 2007
From: yuklap.yip at yale.edu (Yuk Lap Yip (Kevin))
Date: Fri, 16 Mar 2007 10:41:34 -0400
Subject: [R] Implementing trees in R
In-Reply-To: <971536df0703160659u1c339e53k474b962bd846d76e@mail.gmail.com>
References: <45FA9D64.6070708@yale.edu>
	<971536df0703160659u1c339e53k474b962bd846d76e@mail.gmail.com>
Message-ID: <45FAAC9E.6020006@yale.edu>

Hi Gabor,

    Thanks for the suggestions. I tried to look for the proto vignette 
document but could not find it, could you tell me how to reach it?

    Besides, is it possible to define my own node object type with a 
default behavior for the "<-" operator of its member variables being 
referencing rather than copying? Any good reference material/ similar 
code examples?

    Thanks.

Gabor Grothendieck wrote:
> Lists are not good for this.  There is an example in section 3.3 of
> the proto vignette of using proto objects for this.  That section
> also references an S4 example although its pretty messy with S4.
>
> You might want to look at the graph, RBGL and graphviz packages
> in Bioconductor and the dynamicgraph, mathgraph and sna packages
> on CRAN.
>
> On 3/16/07, Yuk Lap Yip (Kevin) <yuklap.yip at yale.edu> wrote:
>> Hi all,
>>
>>    I am rather new to R. Recently I have been trying to implement some
>> tree algorithms in R. I used lists to model tree nodes. I thought
>> something like this would work:
>>
>>    parent <- list();
>>    child <- list();
>>    parent$child1 <- child;
>>    child$parent <- parent;
>>
>>    When I tried to check whether a node is its parent's first child
>> using "if (node$parent$child1 == node)", it always returned false. Then
>> I realized that it does not really work because "parent$child1 <- child"
>> actually makes a copy of child instead of referencing it. I think one
>> possible fix is to keep a list of node objects, and make references
>> using the positions in the list. For example, I think the following
>> would work:
>>
>>    parent <- list();
>>    child <- list();
>>    nodes <- list(parent, child);
>>    parent$child1 <- 2;
>>    child$parent <- 1;
>>
>>    Then the "first child" test can be rewritten as "if
>> (nodes[[nodes[[nodeId]]$parent]]$child1 == nodeId)". However, I would
>> prefer not to implement trees in this way, as it requires the
>> inconvenient and error-prone manipulations of node IDs.
>>
>>    May I know if there is a way to make object references to lists? Or
>> are there other ways to implement tree data structures in R?
>>
>>    BTW, I checked how hclust was implemented, and noticed that it calls
>> an external Fortran program. I would want a solution not involving any
>> external programs.
>>
>>    Thanks.
>>
>> -- 
>>
>>
>>        God bless.
>>
>>        Kevin
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>

-- 


	God bless.

	Kevin


From mckellercran at gmail.com  Fri Mar 16 16:04:15 2007
From: mckellercran at gmail.com (Matthew Keller)
Date: Fri, 16 Mar 2007 11:04:15 -0400
Subject: [R] ideas to speed up code: converting a matrix of integers to a
	matrix of normally distributed values
Message-ID: <3f547caa0703160804h10d5ffa9y424689a3b2a7a363@mail.gmail.com>

Hi all,

[this is a bit hard to describe, so if my initial description is
confusing, please try running my code below]

#WHAT I'M TRYING TO DO
I'd appreciate any help in trying to speed up some code. I've written
a script that converts a matrix of integers (usually between 1-10,000
- these represent allele names) into two new matrices of normally
distributed values (representing genetic effects), such that a given
number in the integer (allele name) matrix always corresponds to the
*same* randomly distributed (genetic) effects.

For example, every time my function sees the allele name "3782", it
converts that integer into the same two effects (e.g., -.372  1.727),
which follow normal distributions (these effects can be correlated;
below I've set their correlation to .5). I have an entire matrix of
integers, and am converting those into two entire matrices of effects.


#HOW I'VE DONE IT SO FAR
To get the correlations between the effects, I've used the mvrnorm
function from "MASS"

To convert the allele names to genetic effects, I've created a
function (make.effect) that resets the set.seed() to the allele name
each time its called.

To get the matrix of genetic effects, I use sapply.


#THE PROBLEM
The problem is that I often need to convert matrices that have 500K
cells, and do this over several hundred iterations, so it is quite
slow. If anyone has ideas to speed this up (e.g., some clever way to
convert a matrix of integers to a matrix of effects without using
sapply), I would be forever indebted.


##MY CODE

library("MASS")

##run this example to see what I'm talking about above

make.effects <- function(x,mn=0,var=1,cov=.5){
  set.seed(x)
  return(mvrnorm(1,mu=c(mn,mn),Sigma=matrix(c(var,cov,cov,var),nrow=2),empirical=FALSE))}

(alleles <- matrix(c(5400,3309,2080,1080,2080,1111,389,9362,6372,3787,2798,1009),ncol=4))

a12 <- array(sapply(alleles,make.effects),dim=c(2,nrow(alleles),ncol(alleles)))
(a1 <- a12[1,,])
(a2 <- a12[2,,])

#I've set the population correlation = .5; empirical corr=.4635
cor(as.vector(a1),as.vector(a2))

##run this to see that the code begins to get pretty slow with even a
3000x4 matrix

system.time({

alleles <- matrix(rep(c(5400,3309,2080,1080,2080,1111,389,9362,6372,3787,2798,1009),1000),ncol=4)

a12 <- array(sapply(alleles,make.effects),dim=c(2,nrow(alleles),ncol(alleles)))
a1 <- a12[1,,]
a2 <- a12[2,,]

})




-- 
Matthew C Keller
Postdoctoral Fellow
Virginia Institute for Psychiatric and Behavioral Genetics


From gunter.berton at gene.com  Fri Mar 16 16:05:00 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Fri, 16 Mar 2007 08:05:00 -0700
Subject: [R] Bad points in regression
In-Reply-To: <XFMail.070316134358.ted.harding@nessie.mcc.ac.uk>
Message-ID: <000901c767dc$78373ae0$4d908980@gne.windows.gene.com>

(mount soapbox...)

While I know the prior discussion represents common practice, I would argue
-- perhaps even plead -- that the modern(?? >30 years old now) alternative
of robust/resistant estimation be used, especially in the readily available
situation of least-squares regression. RSiteSearch("Robust") will bring up
numerous possibilities.rrcov and robustbase are at least two packages
devoted to this, but the functionality is available in many others (e.g.
rlm() in MASS).

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374





-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ted Harding
Sent: Friday, March 16, 2007 6:44 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] Bad points in regression

On 16-Mar-07 12:41:50, Alberto Monteiro wrote:
> Ted Harding wrote:
>> 
>>> alpha <- 0.3
>>> beta <- 0.4
>>> sigma <- 0.5
>>> err <- rnorm(100)
>>> err[15] <- 5; err[25] <- -4; err[50] <- 10
>>> x <- 1:100
>>> y <- alpha + beta * x + sigma * err
>>> ll <- lm(y ~ x)
>>> plot(ll)
>> 
>> ll is the output of a linear model fiited by lm(), and so has
>> several components (see ?lm in the section "Value"), one of
>> which is "residuals" (which can be abbreviated to "res").
>> 
>> So, in the case of your example,
>> 
>>   which(abs(ll$res)>2)
>>   15 25 50
>> 
>> extracts the information you want (and the ">2" was inspired by
>> looking at the "residuals" plot from your "plot(ll)").
>>
> Ok, but how can I grab those points _in general_? What is the
> criterium that plot used to mark those points as bad points?

Ahh ... ! I see what you're after. OK, look at the plot method
for lm():

?plot.lm
  ## S3 method for class 'lm':
  plot(x, which = 1:4,
    caption = c("Residuals vs Fitted", "Normal Q-Q plot",
      "Scale-Location plot", "Cook's distance plot"),
      panel = points,
      sub.caption = deparse(x$call), main = "",
      ask = prod(par("mfcol")) < length(which) && dev.interactive(),
      ...,
      id.n = 3, labels.id = names(residuals(x)), cex.id = 0.75)


where (see further down):

  id.n: number of points to be labelled in each plot, starting with
    the most extreme.

and note, in the default parameter-values listing above:

  id.n = 3

Hence, the 3 most extreme points (according to the criterion being
plotted in each plot) are marked in each plot.

So, for instance3, try

  plot(ll,id.n=5)

and you will get points 10,15,25,28,50. And so on. But that
pre-supposes that you know how many points are "exceptional".


What is meant by "extreme"is not stated in the help page ?plot.lm,
but can be identified by inspecting the code for plot.lm(), which
you can see by entering

  plot.lm

In your example, if you omit the line which assigns anomalous values
to err[15[, err[25] and err[50], then you are likely to observe that
different points get identified on different plots. For instance,
I just got the following results for the default id.n=3:

[1] Residuals vs Fitted:       41,53,59
[2] Standardised Residuals:    41,53,59
[3] sqrt(Stand Res) vs Fitted: 41,53,59
[4] Cook's Distance:           59,96,97


There are several approaches (with somewhat different outcomes)
to identifying "outliers". If you apply one of these, you will
probably get the identities of the points anyway.

Again in the context of your example (where in fact you
deliberately set 3 points to have exceptional errors, thus
coincidentally the same as the default value 3 of id.n),
you could try different values for id.n and inspect the graphs
to see whether a given value of id.n marks some points that
do not look exceptional relative to the mass of the other points.

So, the above plot(ll,id.n=5) gave me one point, "10" on the
residuals plot, which apparently belonged to the general
distribution of residuals.

Hoping this helps,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 16-Mar-07                                       Time: 13:43:54
------------------------------ XFMail ------------------------------

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From peter.mcmahan at gmail.com  Fri Mar 16 16:42:06 2007
From: peter.mcmahan at gmail.com (Peter McMahan)
Date: Fri, 16 Mar 2007 10:42:06 -0500
Subject: [R] Fast lookup in ragged array
Message-ID: <0C7CD2BF-4AF6-4CBD-A6B9-36E8B9D3C7B1@gmail.com>

Hello,

I'm running an algorithm for graph structural cohesion that requires  
a depth-first search of subgraphs of a rather large network. The  
algorithm will necessarily be redundant in the subgraphs it recurses  
to, so to speed up the process I implemented a check at each subgraph  
to see if it's been searched already.

This algorithm is very slow, and takes days to complete on a graph  
with about 200 nodes. It seems that a significant portion of the  
computation time is spent looking up the current subgraph in the list  
of searched subgraphs to see if it is redundant, and I'm wondering if  
there's a faster way to do this.

Currently, the list of searched subgraphs is a list  
(`theseSearchedBranches`) of unnamed numeric vectors. I'm checking  
against the list using the following command:

     if(list(v) %in% theseSearchedBranches){
         cat("    Branch already searched: passing.\n\n")
         return(NULL)
     }

v is a sorted numeric, with length anywhere between 3 and 200.  
Because theseSearchedBranches gets quite long as the algorithm  
progresses, the %in% command is taking maybe one or two seconds per  
lookup.

So to reiterate, I have a list() that looks something like this:

[[1]]
[1] 0 5 6 11 12 13 14 16 19 21 23 24 26 30 31 39 41
[18]56 72 75 76 85 95 105 110 134 158 159 165 186

[[2]]
[1] 0 5 6 11 12 13 14 16 19 21 23 24 26 30 31 39 41
[18]56 72 75 76 85 95 105 110 134 147 159 165 186

[[3]]
[1] 0 5 6 11 12 13 14 16 19 21 23 24 26 30 31 39 41
[18]50 56 72 75 85 95 105 110 134 147 158 159 165 186

...
and so on for tens of thousands of entries, and I am trying to find  
some sort of fast equivalent for %in% to search it. I'm also not  
adding the vectors to the list in any particular order, as I don't  
think %in% would know how to take advantage of that anyway.

Is there a data structure other than list() that I can use that would  
be faster? Would it be better to just create a hashed env and add  
empty variables named "0.5.6.11.12..."?
I know there are fast lookup algorithms out there that could take  
advantage of the fact that the items being searched are indiviually  
ordered numeric vectors, but I can't find any info about R  
implementations on the mailing lists or help. Is there any data type  
that implements a b-tree type of lookup scheme?

Any help would be greatly appreciated.

Thanks,
Peter


From casot at libero.it  Fri Mar 16 16:55:54 2007
From: casot at libero.it (casot at libero.it)
Date: Fri, 16 Mar 2007 16:55:54 +0100
Subject: [R] how to...
Message-ID: <JF06X6$737BBE7E747D5E96E150C72E509F19E1@libero.it>

for example:
I have got these data, organized in a dataframe. 

		sample1	sample2	sample3	sample4	group
replicate1	1.00	0.02	0.35	0.50	A
replicate2	1.00	0.02	1.54	1.11	A
replicate3	1.00	0.02	1.54	1.11	A
replicate4	1.00	0.02	1.54	1.11	A
replicate5	1.00	0.10	0.18	0.72	B
replicate6	1000.00	0.75	0.86	7.26	B
replicate7	1000.00	0.75	0.18	0.36	B
replicate8	1000.00	0.75	12.09	0.74	B
replicate9	1000.00	0.75	12.09	0.84	C
replicate10	1000.00	0.98	0.65	0.50	C
replicate11	2.00	6.00	6.00	2.00	C
replicate12	6.00	6.00	2.00	6.00	C


the first four columns represent the diffent sample I have to test with ANOVA.the last column is related to the group of each entry. Using "aov()" I can run a test on each column. but I would like to run the ANOVAs for each colum (that in my case are hundreds) in an automated way. I can't set up a working script with the "loop" in this case, surely because of my scarce knowledge in programming. can you help me? 
the next problem is how to collect the results in a simple way. for example having them organized in a table such as


SAMPLE ANOVA
sample1 ok
sample2 ok
sample3 not significant
....

	
thank you so much


---------- Initial Header -----------

>From      : "Petr Pikal" petr.pikal at precheza.cz
To          : "casot at libero.it" casot at libero.it,"R Help" R-help at stat.math.ethz.ch
Cc          : 
Date      : Thu, 15 Mar 2007 20:38:25 +0100
Subject : Re: [R] how to...







> Hi
> 
> I suppose you will not get usefull response for such poorly specified 
> question. 
> 
> For automating procedures on data frames you can either do looping or 
> use lapply or maybe do.call can also provide some functionality.
> 
> If you elaborate what you did and in what respect it was 
> unsatisfactory maybe you will get better answer.
> 
> Anyway, before your next post you shall look to posting guide.
> 
> Regards
> Petr
> 
> 
> 
> On 15 Mar 2007 at 17:20, casot at libero.it wrote:
> 
> Date sent:      	Thu, 15 Mar 2007 17:20:57 +0100
> From:           	"casot at libero.it" <casot at libero.it>
> To:             	"R Help" <R-help at stat.math.ethz.ch>
> Subject:        	[R] how to...
> 
> > I have to perform ANOVA's on many different data organized in a
> > dataframe. I can run an ANOVA for each sample, but I've got hundreds
> > of data and I would like to avoid manually carrying out each test. in
> > addition, I would like to have the results organized in a simple way,
> > for example in a table, wich could be easy to export. thank you for
> > assistance
> > 
> > simone 
> > 
> > 
> > ------------------------------------------------------
> > Leggi GRATIS le tue mail con il telefonino i-mode  di Wind
> > http://i-mode.wind.it
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ---------- Initial Header -----------

>From      : "Petr Pikal" petr.pikal at precheza.cz
To          : "casot at libero.it" casot at libero.it,"R Help" R-help at stat.math.ethz.ch
Cc          : 
Date      : Thu, 15 Mar 2007 20:38:25 +0100
Subject : Re: [R] how to...







> Hi
> 
> I suppose you will not get usefull response for such poorly specified 
> question. 
> 
> For automating procedures on data frames you can either do looping or 
> use lapply or maybe do.call can also provide some functionality.
> 
> If you elaborate what you did and in what respect it was 
> unsatisfactory maybe you will get better answer.
> 
> Anyway, before your next post you shall look to posting guide.
> 
> Regards
> Petr
> 
> 
> 
> On 15 Mar 2007 at 17:20, casot at libero.it wrote:
> 
> Date sent:      	Thu, 15 Mar 2007 17:20:57 +0100
> From:           	"casot at libero.it" <casot at libero.it>
> To:             	"R Help" <R-help at stat.math.ethz.ch>
> Subject:        	[R] how to...
> 
> > I have to perform ANOVA's on many different data organized in a
> > dataframe. I can run an ANOVA for each sample, but I've got hundreds
> > of data and I would like to avoid manually carrying out each test. in
> > addition, I would like to have the results organized in a simple way,
> > for example in a table, wich could be easy to export. thank you for
> > assistance
> > 
> > simone 
> > 
> > 
> > ------------------------------------------------------
> > Leggi GRATIS le tue mail con il telefonino i-mode  di Wind
> > http://i-mode.wind.it
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> 


------------------------------------------------------
Passa a Infostrada. ADSL e Telefono senza limiti e senza canone Telecom
http://infostrada.it


From ggrothendieck at gmail.com  Fri Mar 16 17:09:24 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 16 Mar 2007 12:09:24 -0400
Subject: [R] Implementing trees in R
In-Reply-To: <45FAAC9E.6020006@yale.edu>
References: <45FA9D64.6070708@yale.edu>
	<971536df0703160659u1c339e53k474b962bd846d76e@mail.gmail.com>
	<45FAAC9E.6020006@yale.edu>
Message-ID: <971536df0703160909w2e0f334bn715fbe0be658f220@mail.gmail.com>

1. Here is your example redone using proto:

library(proto)

parent <- proto()
child <- proto(a = 1)
parent$child1 <- child
child$parent.env <- parent

# also just for illustration lets change a

parent$child1$a # 1
child$a <- 2
parent$child1$a # 2

2. To redefine $<- use S3 or S4 but it can be done
in conjunction with proto like this:

# constructor
node <- function(. = parent.frame(), ...)
   structure(proto(...), class = c("node", "proto"))

"$<-.node" <- function(this, s, value) {
    if (s == ".super")
        parent.env(this) <- value
    if (is.function(value))
        environment(value) <- this
    if (inherits(value, "node"))
        parent.env(value) <- this
    this[[as.character(substitute(s))]] <- value
    this
}


p <- node(a = 1)
p$child <- node(b = 2)
p$child$parent.env()
p # same



On 3/16/07, Yuk Lap Yip (Kevin) <yuklap.yip at yale.edu> wrote:
> Hi Gabor,
>
>    Thanks for the suggestions. I tried to look for the proto vignette
> document but could not find it, could you tell me how to reach it?
>
>    Besides, is it possible to define my own node object type with a
> default behavior for the "<-" operator of its member variables being
> referencing rather than copying? Any good reference material/ similar
> code examples?
>
>    Thanks.
>
> Gabor Grothendieck wrote:
> > Lists are not good for this.  There is an example in section 3.3 of
> > the proto vignette of using proto objects for this.  That section
> > also references an S4 example although its pretty messy with S4.
> >
> > You might want to look at the graph, RBGL and graphviz packages
> > in Bioconductor and the dynamicgraph, mathgraph and sna packages
> > on CRAN.
> >
> > On 3/16/07, Yuk Lap Yip (Kevin) <yuklap.yip at yale.edu> wrote:
> >> Hi all,
> >>
> >>    I am rather new to R. Recently I have been trying to implement some
> >> tree algorithms in R. I used lists to model tree nodes. I thought
> >> something like this would work:
> >>
> >>    parent <- list();
> >>    child <- list();
> >>    parent$child1 <- child;
> >>    child$parent <- parent;
> >>
> >>    When I tried to check whether a node is its parent's first child
> >> using "if (node$parent$child1 == node)", it always returned false. Then
> >> I realized that it does not really work because "parent$child1 <- child"
> >> actually makes a copy of child instead of referencing it. I think one
> >> possible fix is to keep a list of node objects, and make references
> >> using the positions in the list. For example, I think the following
> >> would work:
> >>
> >>    parent <- list();
> >>    child <- list();
> >>    nodes <- list(parent, child);
> >>    parent$child1 <- 2;
> >>    child$parent <- 1;
> >>
> >>    Then the "first child" test can be rewritten as "if
> >> (nodes[[nodes[[nodeId]]$parent]]$child1 == nodeId)". However, I would
> >> prefer not to implement trees in this way, as it requires the
> >> inconvenient and error-prone manipulations of node IDs.
> >>
> >>    May I know if there is a way to make object references to lists? Or
> >> are there other ways to implement tree data structures in R?
> >>
> >>    BTW, I checked how hclust was implemented, and noticed that it calls
> >> an external Fortran program. I would want a solution not involving any
> >> external programs.
> >>
> >>    Thanks.
> >>
> >> --
> >>
> >>
> >>        God bless.
> >>
> >>        Kevin
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
>
> --
>
>
>        God bless.
>
>        Kevin
>
>


From ggrothendieck at gmail.com  Fri Mar 16 17:11:31 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 16 Mar 2007 12:11:31 -0400
Subject: [R] Implementing trees in R
In-Reply-To: <971536df0703160909w2e0f334bn715fbe0be658f220@mail.gmail.com>
References: <45FA9D64.6070708@yale.edu>
	<971536df0703160659u1c339e53k474b962bd846d76e@mail.gmail.com>
	<45FAAC9E.6020006@yale.edu>
	<971536df0703160909w2e0f334bn715fbe0be658f220@mail.gmail.com>
Message-ID: <971536df0703160911n69b3b34cgee73ee687114eaaa@mail.gmail.com>

On 3/16/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> 1. Here is your example redone using proto:
>
> library(proto)
>
> parent <- proto()
> child <- proto(a = 1)
> parent$child1 <- child
> child$parent.env <- parent

This last line should have been:

parent.env(child) <- parent


>
> # also just for illustration lets change a
>
> parent$child1$a # 1
> child$a <- 2
> parent$child1$a # 2
>
> 2. To redefine $<- use S3 or S4 but it can be done
> in conjunction with proto like this:
>
> # constructor
> node <- function(. = parent.frame(), ...)
>   structure(proto(...), class = c("node", "proto"))
>
> "$<-.node" <- function(this, s, value) {
>    if (s == ".super")
>        parent.env(this) <- value
>    if (is.function(value))
>        environment(value) <- this
>    if (inherits(value, "node"))
>        parent.env(value) <- this
>    this[[as.character(substitute(s))]] <- value
>    this
> }
>
>
> p <- node(a = 1)
> p$child <- node(b = 2)
> p$child$parent.env()
> p # same
>
>
>
> On 3/16/07, Yuk Lap Yip (Kevin) <yuklap.yip at yale.edu> wrote:
> > Hi Gabor,
> >
> >    Thanks for the suggestions. I tried to look for the proto vignette
> > document but could not find it, could you tell me how to reach it?
> >
> >    Besides, is it possible to define my own node object type with a
> > default behavior for the "<-" operator of its member variables being
> > referencing rather than copying? Any good reference material/ similar
> > code examples?
> >
> >    Thanks.
> >
> > Gabor Grothendieck wrote:
> > > Lists are not good for this.  There is an example in section 3.3 of
> > > the proto vignette of using proto objects for this.  That section
> > > also references an S4 example although its pretty messy with S4.
> > >
> > > You might want to look at the graph, RBGL and graphviz packages
> > > in Bioconductor and the dynamicgraph, mathgraph and sna packages
> > > on CRAN.
> > >
> > > On 3/16/07, Yuk Lap Yip (Kevin) <yuklap.yip at yale.edu> wrote:
> > >> Hi all,
> > >>
> > >>    I am rather new to R. Recently I have been trying to implement some
> > >> tree algorithms in R. I used lists to model tree nodes. I thought
> > >> something like this would work:
> > >>
> > >>    parent <- list();
> > >>    child <- list();
> > >>    parent$child1 <- child;
> > >>    child$parent <- parent;
> > >>
> > >>    When I tried to check whether a node is its parent's first child
> > >> using "if (node$parent$child1 == node)", it always returned false. Then
> > >> I realized that it does not really work because "parent$child1 <- child"
> > >> actually makes a copy of child instead of referencing it. I think one
> > >> possible fix is to keep a list of node objects, and make references
> > >> using the positions in the list. For example, I think the following
> > >> would work:
> > >>
> > >>    parent <- list();
> > >>    child <- list();
> > >>    nodes <- list(parent, child);
> > >>    parent$child1 <- 2;
> > >>    child$parent <- 1;
> > >>
> > >>    Then the "first child" test can be rewritten as "if
> > >> (nodes[[nodes[[nodeId]]$parent]]$child1 == nodeId)". However, I would
> > >> prefer not to implement trees in this way, as it requires the
> > >> inconvenient and error-prone manipulations of node IDs.
> > >>
> > >>    May I know if there is a way to make object references to lists? Or
> > >> are there other ways to implement tree data structures in R?
> > >>
> > >>    BTW, I checked how hclust was implemented, and noticed that it calls
> > >> an external Fortran program. I would want a solution not involving any
> > >> external programs.
> > >>
> > >>    Thanks.
> > >>
> > >> --
> > >>
> > >>
> > >>        God bless.
> > >>
> > >>        Kevin
> > >>
> > >> ______________________________________________
> > >> R-help at stat.math.ethz.ch mailing list
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide
> > >> http://www.R-project.org/posting-guide.html
> > >> and provide commented, minimal, self-contained, reproducible code.
> > >>
> >
> > --
> >
> >
> >        God bless.
> >
> >        Kevin
> >
> >
>


From bruno.c at inwind.it  Fri Mar 16 17:14:13 2007
From: bruno.c at inwind.it (Bruno C.)
Date: Fri, 16 Mar 2007 17:14:13 +0100
Subject: [R] Duplicated non contiguous element in a list
Message-ID: <JF07RP$008334B9CEBEE57193EDC9513AB84B09@libero.it>

Hello,

Given a vector I would like to rapidly identify duplicated non contiguous elements.
Given for example
c(1,    1,    2,   3,    2,   4,   5,   6,    4) 
I would like to get:
FALSE FALSE TRUE FALSE TRUE TRUE FALSE FALSE TRUE

In fact I need to check this on the columns of a matrix!
I can do that of couse with loops but is there any function already available?

Thanks


------------------------------------------------------
Passa a Infostrada. ADSL e Telefono senza limiti e senza canone Telecom
http://infostrada.it


From feriel_lahlali at yahoo.fr  Fri Mar 16 17:40:27 2007
From: feriel_lahlali at yahoo.fr (Feriel Lahlali)
Date: Fri, 16 Mar 2007 17:40:27 +0100 (CET)
Subject: [R] log(1+exp(x)) function
Message-ID: <836274.58791.qm@web50412.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070316/c9bb51ce/attachment.pl 

From peter.mcmahan at gmail.com  Fri Mar 16 17:45:34 2007
From: peter.mcmahan at gmail.com (Peter McMahan)
Date: Fri, 16 Mar 2007 11:45:34 -0500
Subject: [R] Fast lookup in ragged array
In-Reply-To: <0C7CD2BF-4AF6-4CBD-A6B9-36E8B9D3C7B1@gmail.com>
References: <0C7CD2BF-4AF6-4CBD-A6B9-36E8B9D3C7B1@gmail.com>
Message-ID: <24CF2791-BC64-4480-8696-5C0ACF0FEA23@uchicago.edu>

Well, I hadn't ever seen RBGL before, so that's great. I've been  
using igraph and sna mainly, but there are a few points lacking  
between these two. RBGL solves a lot of problems for me!

But I'm not sure it will solve this specific problem. Are you  
suggesting I use RBGL to do a depth-first search of all the  
subgraphs? For this particular depth-first search I'm not searching  
every subgraph, but just those that are constructed from a minimal  
cutset of the parent subgraph. At each level of the search, I have to  
compute graph cohesion (vertex connectivity), which can take  
considerable time. A lot of computation time is saved by only  
searching subgraphs obtained through cutsets. So a complete search of  
all the subgraphs won't work, but the redundancy I come across is I  
think unavoidable.

The particular algorithm I'm trying to implement is Moody and White's  
cohesive blocking, in which the end result is a nested set of all  
subgraphs with a higher cohesion (connectivity) than their parents.  
(see http://www.santafe.edu/research/publications/workingpapers/ 
00-08-049.pdf )

On Mar 16, 2007, at 11:00 AM, Robert Gentleman wrote:


> why not just use the graph package and RBGL at www.bioconductor.org
>
>
> Peter McMahan wrote:
>
>> Hello,
>> I'm running an algorithm for graph structural cohesion that  
>> requires  a depth-first search of subgraphs of a rather large  
>> network. The  algorithm will necessarily be redundant in the  
>> subgraphs it recurses  to, so to speed up the process I  
>> implemented a check at each subgraph  to see if it's been searched  
>> already.
>> This algorithm is very slow, and takes days to complete on a  
>> graph  with about 200 nodes. It seems that a significant portion  
>> of the  computation time is spent looking up the current subgraph  
>> in the list  of searched subgraphs to see if it is redundant, and  
>> I'm wondering if  there's a faster way to do this.
>> Currently, the list of searched subgraphs is a list   
>> (`theseSearchedBranches`) of unnamed numeric vectors. I'm  
>> checking  against the list using the following command:
>>      if(list(v) %in% theseSearchedBranches){
>>          cat("    Branch already searched: passing.\n\n")
>>          return(NULL)
>>      }
>> v is a sorted numeric, with length anywhere between 3 and 200.   
>> Because theseSearchedBranches gets quite long as the algorithm   
>> progresses, the %in% command is taking maybe one or two seconds  
>> per  lookup.
>> So to reiterate, I have a list() that looks something like this:
>> [[1]]
>> [1] 0 5 6 11 12 13 14 16 19 21 23 24 26 30 31 39 41
>> [18]56 72 75 76 85 95 105 110 134 158 159 165 186
>> [[2]]
>> [1] 0 5 6 11 12 13 14 16 19 21 23 24 26 30 31 39 41
>> [18]56 72 75 76 85 95 105 110 134 147 159 165 186
>> [[3]]
>> [1] 0 5 6 11 12 13 14 16 19 21 23 24 26 30 31 39 41
>> [18]50 56 72 75 85 95 105 110 134 147 158 159 165 186
>> ...
>> and so on for tens of thousands of entries, and I am trying to  
>> find  some sort of fast equivalent for %in% to search it. I'm also  
>> not  adding the vectors to the list in any particular order, as I  
>> don't  think %in% would know how to take advantage of that anyway.
>> Is there a data structure other than list() that I can use that  
>> would  be faster? Would it be better to just create a hashed env  
>> and add  empty variables named "0.5.6.11.12..."?
>> I know there are fast lookup algorithms out there that could take   
>> advantage of the fact that the items being searched are  
>> indiviually  ordered numeric vectors, but I can't find any info  
>> about R  implementations on the mailing lists or help. Is there  
>> any data type  that implements a b-tree type of lookup scheme?
>> Any help would be greatly appreciated.
>> Thanks,
>> Peter
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> -- 
> Robert Gentleman, PhD
> Program in Computational Biology
> Division of Public Health Sciences
> Fred Hutchinson Cancer Research Center
> 1100 Fairview Ave. N, M2-B876
> PO Box 19024
> Seattle, Washington 98109-1024
> 206-667-7700
> rgentlem at fhcrc.org
>


From feriel_lahlali at yahoo.fr  Fri Mar 16 17:49:27 2007
From: feriel_lahlali at yahoo.fr (Feriel Lahlali)
Date: Fri, 16 Mar 2007 17:49:27 +0100 (CET)
Subject: [R] log(1+exp(x)) function
Message-ID: <625282.74165.qm@web50401.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070316/93a9dc8c/attachment.pl 

From peter.mcmahan at gmail.com  Fri Mar 16 17:55:51 2007
From: peter.mcmahan at gmail.com (Peter McMahan)
Date: Fri, 16 Mar 2007 11:55:51 -0500
Subject: [R] Duplicated non contiguous element in a list
In-Reply-To: <JF07RP$008334B9CEBEE57193EDC9513AB84B09@libero.it>
References: <JF07RP$008334B9CEBEE57193EDC9513AB84B09@libero.it>
Message-ID: <42557EF6-D57A-4CD3-BC32-73F99754FFA9@uchicago.edu>

Maybe not the most efficient, but here's a vector-level solution:

non.contig.dupes <- function(x){
     is.dup <- x %in% x[duplicated(x)]
     is.con <- c(x[-length(x)]==x[-1],F) | c(F,x[-length(x)]==x[-1])
     is.dup & !is.con}


On Mar 16, 2007, at 11:14 AM, Bruno C.. wrote:


> Hello,
>
> Given a vector I would like to rapidly identify duplicated non  
> contiguous elements.
> Given for example
> c(1,    1,    2,   3,    2,   4,   5,   6,    4)
> I would like to get:
> FALSE FALSE TRUE FALSE TRUE TRUE FALSE FALSE TRUE
>
> In fact I need to check this on the columns of a matrix!
> I can do that of couse with loops but is there any function already  
> available?
>
> Thanks
>
>
> ------------------------------------------------------
> Passa a Infostrada. ADSL e Telefono senza limiti e senza canone  
> Telecom
> http://infostrada.it
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gavin.simpson at ucl.ac.uk  Fri Mar 16 17:59:28 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 16 Mar 2007 16:59:28 +0000
Subject: [R] MANOVA permutation testing
In-Reply-To: <slrnevjqdp.to8.tyler.smith@blackbart.mynetwork>
References: <slrnevjqdp.to8.tyler.smith@blackbart.mynetwork>
Message-ID: <1174064368.25843.112.camel@gsimpson.geog.ucl.ac.uk>

On Fri, 2007-03-16 at 00:50 +0000, Tyler Smith wrote:
> Hi,
> 
> I've got a dataset with 7 variables for 8 different species. I'd like
> to test the null hypothesis of no difference among species for these
> variables. MANOVA seems like the appropriate test, but since I'm
> unsure of how well the data fit the assumptions of equal
> variance/covariance and multivariate normality, I want to use a
> permutation test. 
> 
> I've been through CRAN looking at packages boot, bootstrap, coin,
> permtest, but they all seem to be doing more than I need. Is the
> following code an appropriate way to test my hypothesis:
> 
> result.vect <- c()
> 
> for (i in 1:1000){
>   wilks <- summary.manova(manova(maxent~sample(max.spec)),
>                test="Wilks")$stats[1,2]
>   result.vect <- c(res.vect,wilks)
> }
> 
> maxent is the data, max.spec is a vector of species names. Comparing
> the result.vect with the wilks value for the unpermuted data suggests
> there are very significant differences among species -- but did I do
> this properly?
> 

Hi Tyler,

(without knowing more about your data) I think you are almost there, but
the R code can be made much more efficient.

When you create your result vector, is is of length 0. Each time you add
a result, R has to copy the current result object, enlarge it and so on.
This all takes a lot of time. Better to allocate storage first, then add
each result in turn be replacement. E.g.:

Using an example from ?summary.manova

tear <- c(6.5, 6.2, 5.8, 6.5, 6.5, 6.9, 7.2, 6.9, 6.1, 6.3,
               6.7, 6.6, 7.2, 7.1, 6.8, 7.1, 7.0, 7.2, 7.5, 7.6)
gloss <- c(9.5, 9.9, 9.6, 9.6, 9.2, 9.1, 10.0, 9.9, 9.5, 9.4,
                9.1, 9.3, 8.3, 8.4, 8.5, 9.2, 8.8, 9.7, 10.1, 9.2)
opacity <- c(4.4, 6.4, 3.0, 4.1, 0.8, 5.7, 2.0, 3.9, 1.9, 5.7,
                  2.8, 4.1, 3.8, 1.6, 3.4, 8.4, 5.2, 6.9, 2.7, 1.9)
Y <- cbind(tear, gloss, opacity)
rate <- factor(gl(2,10), labels=c("Low", "High"))

## define number of permutations
nperm <- 999
## allocate storage, here we want 999 + 1 for our observed stat
res <- numeric(nperm+1)
## do the loop - the seq(along ...) bit avoids certain problems
for(i in seq(along = res[-1])) {
## here we replace the ith value in the vector res with the stat
    res[i] <- summary(manova(Y ~ sample(rate)), 
                      test = "Wilks")$stats[1,2]
}
## now we append the observed stat onto the end of the result vector res
## we also store this in 'obs' for convenience
res[nperm+1] <- obs <- summary(manova(Y ~ rate), test =  
                               "Wilks")$stats[1,2]

## this is the permutation p-value - the proportion of the nperm
## permutations + 1 that are  greater than or equal to the 
## observed stat 'obs'
sum(res <= obs) / (nperm+1)

HTH,

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From jholtman at gmail.com  Fri Mar 16 18:07:07 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 16 Mar 2007 13:07:07 -0400
Subject: [R] ideas to speed up code: converting a matrix of integers to
	a matrix of normally distributed values
In-Reply-To: <3f547caa0703160804h10d5ffa9y424689a3b2a7a363@mail.gmail.com>
References: <3f547caa0703160804h10d5ffa9y424689a3b2a7a363@mail.gmail.com>
Message-ID: <644e1f320703161007p5494d852w5612792d48bede2e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070316/b120dee8/attachment.pl 

From dickgiesser at gmail.com  Fri Mar 16 18:15:23 2007
From: dickgiesser at gmail.com (Benjamin Dickgiesser)
Date: Fri, 16 Mar 2007 17:15:23 +0000
Subject: [R] Create coef.table as in summary.glm (with Signif. codes)
Message-ID: <b75d67340703161015x3da5875cm860beae2aa017f6c@mail.gmail.com>

Hi,

how do I create a coef.table as for example returned by summary.glm?
I don't see where the significance codes are assigned during summary.glm.

Thank you,
Benjamin


From andy_liaw at merck.com  Fri Mar 16 18:21:53 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 16 Mar 2007 13:21:53 -0400
Subject: [R] Bad points in regression  [Broadcast]
In-Reply-To: <000901c767dc$78373ae0$4d908980@gne.windows.gene.com>
References: <XFMail.070316134358.ted.harding@nessie.mcc.ac.uk>
	<000901c767dc$78373ae0$4d908980@gne.windows.gene.com>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03E0568B@usctmx1106.merck.com>

(My turn on the soapbox ...)

I'd like to add a bit of caveat to Bert's view.  I'd argue (perhaps even
plead) that robust/resistant procedures be used with care.  They should
not be used as a shortcut to avoid careful analysis of data.  I recalled
that in my first course on regression, the professor made it clear that
we're fitting models to data, not the other way around.  When the model
fits badly to (some of the) the data,  do examine and think carefully
about what happened.  Verify that "bad data" are indeed bad, instead of
using statistical criteria to make that judgment.  A scientific
colleague reminded me of this point when I tried to sell him the idea of
robust/resistant methods:  Don't use these methods as a crutch to stand
on badly run experiments (or poorly fitted models).

Cheers,
Andy

From: Bert Gunter
> 
> (mount soapbox...)
> 
> While I know the prior discussion represents common practice, 
> I would argue
> -- perhaps even plead -- that the modern(?? >30 years old 
> now) alternative of robust/resistant estimation be used, 
> especially in the readily available situation of 
> least-squares regression. RSiteSearch("Robust") will bring up 
> numerous possibilities.rrcov and robustbase are at least two 
> packages devoted to this, but the functionality is available 
> in many others (e.g.
> rlm() in MASS).
> 
> Bert Gunter
> Genentech Nonclinical Statistics
> South San Francisco, CA 94404
> 650-467-7374
> 
> 
> 
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ted Harding
> Sent: Friday, March 16, 2007 6:44 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Bad points in regression
> 
> On 16-Mar-07 12:41:50, Alberto Monteiro wrote:
> > Ted Harding wrote:
> >> 
> >>> alpha <- 0.3
> >>> beta <- 0.4
> >>> sigma <- 0.5
> >>> err <- rnorm(100)
> >>> err[15] <- 5; err[25] <- -4; err[50] <- 10 x <- 1:100 y 
> <- alpha + 
> >>> beta * x + sigma * err ll <- lm(y ~ x)
> >>> plot(ll)
> >> 
> >> ll is the output of a linear model fiited by lm(), and so 
> has several 
> >> components (see ?lm in the section "Value"), one of which is 
> >> "residuals" (which can be abbreviated to "res").
> >> 
> >> So, in the case of your example,
> >> 
> >>   which(abs(ll$res)>2)
> >>   15 25 50
> >> 
> >> extracts the information you want (and the ">2" was inspired by 
> >> looking at the "residuals" plot from your "plot(ll)").
> >>
> > Ok, but how can I grab those points _in general_? What is the 
> > criterium that plot used to mark those points as bad points?
> 
> Ahh ... ! I see what you're after. OK, look at the plot 
> method for lm():
> 
> ?plot.lm
>   ## S3 method for class 'lm':
>   plot(x, which = 1:4,
>     caption = c("Residuals vs Fitted", "Normal Q-Q plot",
>       "Scale-Location plot", "Cook's distance plot"),
>       panel = points,
>       sub.caption = deparse(x$call), main = "",
>       ask = prod(par("mfcol")) < length(which) && dev.interactive(),
>       ...,
>       id.n = 3, labels.id = names(residuals(x)), cex.id = 0.75)
> 
> 
> where (see further down):
> 
>   id.n: number of points to be labelled in each plot, starting with
>     the most extreme.
> 
> and note, in the default parameter-values listing above:
> 
>   id.n = 3
> 
> Hence, the 3 most extreme points (according to the criterion 
> being plotted in each plot) are marked in each plot.
> 
> So, for instance3, try
> 
>   plot(ll,id.n=5)
> 
> and you will get points 10,15,25,28,50. And so on. But that 
> pre-supposes that you know how many points are "exceptional".
> 
> 
> What is meant by "extreme"is not stated in the help page 
> ?plot.lm, but can be identified by inspecting the code for 
> plot.lm(), which you can see by entering
> 
>   plot.lm
> 
> In your example, if you omit the line which assigns anomalous 
> values to err[15[, err[25] and err[50], then you are likely 
> to observe that different points get identified on different 
> plots. For instance, I just got the following results for the 
> default id.n=3:
> 
> [1] Residuals vs Fitted:       41,53,59
> [2] Standardised Residuals:    41,53,59
> [3] sqrt(Stand Res) vs Fitted: 41,53,59
> [4] Cook's Distance:           59,96,97
> 
> 
> There are several approaches (with somewhat different 
> outcomes) to identifying "outliers". If you apply one of 
> these, you will probably get the identities of the points anyway.
> 
> Again in the context of your example (where in fact you 
> deliberately set 3 points to have exceptional errors, thus 
> coincidentally the same as the default value 3 of id.n), you 
> could try different values for id.n and inspect the graphs to 
> see whether a given value of id.n marks some points that do 
> not look exceptional relative to the mass of the other points.
> 
> So, the above plot(ll,id.n=5) gave me one point, "10" on the 
> residuals plot, which apparently belonged to the general 
> distribution of residuals.
> 
> Hoping this helps,
> Ted.
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 16-Mar-07                                       Time: 13:43:54
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From jadamson at PARTNERS.ORG  Fri Mar 16 18:29:07 2007
From: jadamson at PARTNERS.ORG (Joel J. Adamson)
Date: Fri, 16 Mar 2007 13:29:07 -0400
Subject: [R] ERROR: 'latex' needed but missing on your system.
Message-ID: <17914.54243.334456.271013@W0053328.mgh.harvard.edu>

After successfully building R on Slackware Linux v11.0 I went to make
the documentation; the texi files went fine and then I hopefully issued

make dvi

after having gotten the warning to the effect of "You cannot build the
DVI or PDF manuals" during compilation.  And, as expected I got the
error

ERROR: 'latex' needed but missing on your system.

The problem is that latex is on my system and is in root's path:

/usr/src/R-2.4.1 Super-User > echo $PATH
/usr/share/texmf/bin/:/opt/kde/bin/:/uCsr/local/stata{sic}:/usr/local/sbin:/usr/local/bin:/sbin:/usr/sbin:/bin:/usr/bin

I can issue latex from the command line as root (su'd to root, that
is) and it will run successfully.  Also, "whereis latex" turns up
empty.

I did not have this problem on PCLinuxOS 0.93a.

Thanks for any suggestions,
Joel
-- 
Joel J. Adamson
Biostatistician
Pediatric Psychopharmacology Research Unit
Massachusetts General Hospital
Boston, MA  02114
(617) 643-1432
(303) 880-3109





The information transmitted in this electronic communication is intended only for the person or entity to whom it is addressed and may contain confidential and/or privileged material. Any review, retransmission, dissemination or other use of or taking of any action in reliance upon this information by persons or entities other than the intended recipient is prohibited. If you received this information in error, please contact the Compliance HelpLine at 800-856-1983 and properly dispose of this information.


From dickgiesser at gmail.com  Fri Mar 16 18:53:23 2007
From: dickgiesser at gmail.com (Benjamin Dickgiesser)
Date: Fri, 16 Mar 2007 17:53:23 +0000
Subject: [R] Create coef.table as in summary.glm (with Signif. codes)
In-Reply-To: <b75d67340703161015x3da5875cm860beae2aa017f6c@mail.gmail.com>
References: <b75d67340703161015x3da5875cm860beae2aa017f6c@mail.gmail.com>
Message-ID: <b75d67340703161053x59647cbj358cc40ffbde95c6@mail.gmail.com>

I have found the function printCoefmat().
Thank you,
Benjamin

On 3/16/07, Benjamin Dickgiesser <dickgiesser at gmail.com> wrote:
> Hi,
>
> how do I create a coef.table as for example returned by summary.glm?
> I don't see where the significance codes are assigned during summary.glm.
>
> Thank you,
> Benjamin
>


From feanor0 at hotmail.com  Fri Mar 16 19:15:36 2007
From: feanor0 at hotmail.com (Murali Menon)
Date: Fri, 16 Mar 2007 18:15:36 +0000
Subject: [R] cumsum over varying column lengths
Message-ID: <BAY113-F348DB9B3BC5CA7B8372C2AEE710@phx.gbl>

Folks,

I have a matrix of historicalReturns, where entry (i, j) is the daily return 
corresponding to date i and equity j. I also have a matrix startOffset, 
where entry (1, k) is the row offset in historicalReturns where I entered 
into equity k.

So we have that NCOL(startOffset) = NCOL(historicalReturns).

Now I would like compute for each column in historicalReturns, the 
cumulative return 'returnsFromInception' for the equity starting from the 
startOffset date.

Is there a better way than as follows:


n <- NROW(historicalReturns)
returnsFromInception <- matrix(nrow = 1, ncol = 
NCOL(historicalInceptionDates))

for (i in 1 : NCOL(historicalReturns))
{
    cumReturn <- cumsum(historicalReturns[startOffset[1, i] : n, i])
    returnsFromInception[1, i] <- cumReturn[length(cumReturn)]
}

This works for me, but seems rather inelegant, and I don't like having to 
use a matrix for returnsFromInception and startOffset, where vectors would 
work.

Thanks for your help.

Murali

_________________________________________________________________
It?s tax season, make sure to follow these few simple tips


From pbruce at statistics.com  Fri Mar 16 19:19:07 2007
From: pbruce at statistics.com (Peter Bruce)
Date: Fri, 16 Mar 2007 14:19:07 -0400
Subject: [R] online course - Using R for Basic Statistics
Message-ID: <6.1.0.6.2.20070316141651.10270640@mail.statistics.com>

Dr. John Verzani will present his online course, "Using R for Introductory 
Statistics" April 6 - May 4 at statistics.com.  Participants can ask 
questions and exchange comments with Dr. Verzani via a private discussion 
board throughout the period.

This course covers the use of R to summarize and graph data, calculate 
confidence intervals, test hypotheses, assess goodness-of-fit, and perform 
linear regression.

John Verzani is a member of the faculty at the College of Staten Island of 
the City University of New York, and the author of "Using R for 
Introductory Statistics" (CRC Press), on which this course is based.  His 
research interests and publications are in the area of superprocesses.

There are no set hours when you must be online, and we estimate you will 
need about 10-15 hours per week.

Register:  http://www.statistics.com/content/courses/advanceddoe/

Peter Bruce
courses at statistics.com


From mckellercran at gmail.com  Fri Mar 16 19:30:18 2007
From: mckellercran at gmail.com (Matthew Keller)
Date: Fri, 16 Mar 2007 14:30:18 -0400
Subject: [R] ideas to speed up code: converting a matrix of integers to
	a matrix of normally distributed values
In-Reply-To: <644e1f320703161007p5494d852w5612792d48bede2e@mail.gmail.com>
References: <3f547caa0703160804h10d5ffa9y424689a3b2a7a363@mail.gmail.com>
	<644e1f320703161007p5494d852w5612792d48bede2e@mail.gmail.com>
Message-ID: <3f547caa0703161130r513a043fndc8ea7f1da67f228@mail.gmail.com>

Hi,

Many thanks to Jim and Martin for their suggestions. Using your ideas,
I came up with a solution that indexes rather than uses sapply (and
therefore calling up mvrnorm separately for each cell in the matrix).
The trick is to create a "key" matrix once, and then to use the
match() function each time I need to take the results from the key
matrix and place them in the appropriate spots in an 'effects' matrix.
If anyone is interested, here is a solution which speeds up the
process by a factor of 200 (!) :

unique.allele.seq <- 1:10000

make.effects <- function(allele.seq, seed, mn = 0, var=1, cov=.5) {
   set.seed(seed)
   return(mvrnorm(length(allele.seq),
mu=c(mn,mn),Sigma=matrix(c(var,cov,cov,var),nrow=2),
empirical=FALSE))}

effects.key <- make.effects(unique.allele.seq, 123)

(alleles <- matrix(c(15,3309,2080,1080,2080,1111,389,9362,6372,3787,2798,1009),ncol=4))
(indx <- match(alleles,key))

(a1 <- matrix(effects.key[indx,1],ncol=ncol(alleles)))
(a2 <- matrix(effects.key[indx,2],ncol=ncol(alleles)))

#to check timing
system.time({
alleles <- matrix(rep(c(5400,3309,2080,1080,2080,1111,389,9362,6372,3787,2798,1009),10000),ncol=4)
indx <- match(alleles,key)

a1 <- matrix(effects.key[indx,1],ncol=ncol(alleles))
a2 <- matrix(effects.key[indx,2],ncol=ncol(alleles))})




On 3/16/07, jim holtman <jholtman at gmail.com> wrote:
> Considering that the vast majority of your time is spent in the function
> mvrnorm (on my system 5.7  out of 6.1 seconds).  In your example that is
> 12000 calls to the function.  To improve your speed you have to cut down the
> number of calls to the function.  For example, how many unique integers do
> you have and can to do the calls for those and then substitute matching
> values.  Here is what Rprof showed:
>
>                   total.time total.pct self.time self.pct
> system.time             6.12      99.7      0.00      0.0
> as.vector               6.06      98.7      0.18      2.9
> FUN                     6.06      98.7      0.12      2.0
> array                   6.06      98.7      0.10      1.6
> lapply                  6.06      98.7      0.00      0.0
> sapply                  6.06      98.7      0.00      0.0
> eval                    6.04      98.4      0.06      1.0
> mvrnorm                 5.72      93.2      0.34      5.5
> eigen                   2.58      42.0      0.52      8.5
>
> or another way of looking at it:
>
>   0   6.1 root
>   1.    6.1 system.time
>   2. .    6.0 eval
>   3. . .    6.0 eval
>   4. . . .    6.0 array
>   5. . . . .    6.0 as.vector
>   6. . . . . .    6.0 sapply
>   7. . . . . . .    6.0 lapply
>   8. . . . . . . .    6.0 FUN
>   9. . . . . . . . .    5.7 mvrnorm
>  10. . . . . . . . . .    2.6 eigen
>  11. . . . . . . . . . .    1.2 sort.list
>  12. . . . . . . . . . . .    1.0 match.arg
>  13. . . . . . . . . . . . .    0.7 eval
>
>
>
>
> On 3/16/07, Matthew Keller <mckellercran at gmail.com> wrote:
> >
> > Hi all,
> >
> > [this is a bit hard to describe, so if my initial description is
> > confusing, please try running my code below]
> >
> > #WHAT I'M TRYING TO DO
> > I'd appreciate any help in trying to speed up some code. I've written
> > a script that converts a matrix of integers (usually between 1-10,000
> > - these represent allele names) into two new matrices of normally
> > distributed values (representing genetic effects), such that a given
> > number in the integer (allele name) matrix always corresponds to the
> > *same* randomly distributed (genetic) effects.
> >
> > For example, every time my function sees the allele name "3782", it
> > converts that integer into the same two effects (e.g., -.372  1.727),
> > which follow normal distributions (these effects can be correlated;
> > below I've set their correlation to .5). I have an entire matrix of
> > integers, and am converting those into two entire matrices of effects.
> >
> >
> > #HOW I'VE DONE IT SO FAR
> > To get the correlations between the effects, I've used the mvrnorm
> > function from "MASS"
> >
> > To convert the allele names to genetic effects, I've created a
> > function (make.effect) that resets the set.seed() to the allele name
> > each time its called.
> >
> > To get the matrix of genetic effects, I use sapply.
> >
> >
> > #THE PROBLEM
> > The problem is that I often need to convert matrices that have 500K
> > cells, and do this over several hundred iterations, so it is quite
> > slow. If anyone has ideas to speed this up (e.g., some clever way to
> > convert a matrix of integers to a matrix of effects without using
> > sapply), I would be forever indebted.
> >
> >
> > ##MY CODE
> >
> > library("MASS")
> >
> > ##run this example to see what I'm talking about above
> >
> > make.effects <- function(x,mn=0,var=1,cov=.5){
> > set.seed(x)
> >
> return(mvrnorm(1,mu=c(mn,mn),Sigma=matrix(c(var,cov,cov,var),nrow=2),empirical=FALSE))}
> >
> > (alleles <-
> matrix(c(5400,3309,2080,1080,2080,1111,389,9362,6372,3787,2798,1009),ncol=4))
> >
> > a12 <-
> array(sapply(alleles,make.effects),dim=c(2,nrow(alleles),ncol(alleles)))
> > (a1 <- a12[1,,])
> > (a2 <- a12[2,,])
> >
> > #I've set the population correlation = .5; empirical corr=.4635
> > cor(as.vector (a1),as.vector(a2))
> >
> > ##run this to see that the code begins to get pretty slow with even a
> > 3000x4 matrix
> >
> > system.time({
> >
> > alleles <-
> matrix(rep(c(5400,3309,2080,1080,2080,1111,389,9362,6372,3787,2798,1009),1000),ncol=4)
> >
> > a12 <-
> array(sapply(alleles,make.effects),dim=c(2,nrow(alleles),ncol(alleles)))
> > a1 <- a12[1,,]
> > a2 <- a12[2,,]
> >
> > })
> >
> >
> >
> >
> > --
> > Matthew C Keller
> > Postdoctoral Fellow
> > Virginia Institute for Psychiatric and Behavioral Genetics
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
>
> What is the problem you are trying to solve?


-- 
Matthew C Keller
Postdoctoral Fellow
Virginia Institute for Psychiatric and Behavioral Genetics


From sfalcon at fhcrc.org  Fri Mar 16 19:37:36 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Fri, 16 Mar 2007 11:37:36 -0700
Subject: [R] Fast lookup in ragged array
In-Reply-To: <24CF2791-BC64-4480-8696-5C0ACF0FEA23@uchicago.edu> (Peter
	McMahan's message of "Fri, 16 Mar 2007 11:45:34 -0500")
References: <0C7CD2BF-4AF6-4CBD-A6B9-36E8B9D3C7B1@gmail.com>
	<24CF2791-BC64-4480-8696-5C0ACF0FEA23@uchicago.edu>
Message-ID: <m2y7lx3uhb.fsf@ziti.local>

Peter McMahan <peter.mcmahan at gmail.com> writes:

> Well, I hadn't ever seen RBGL before, so that's great. I've been  
> using igraph and sna mainly, but there are a few points lacking  
> between these two. RBGL solves a lot of problems for me!
>
> But I'm not sure it will solve this specific problem. Are you  
> suggesting I use RBGL to do a depth-first search of all the  
> subgraphs? For this particular depth-first search I'm not searching  
> every subgraph, but just those that are constructed from a minimal  
> cutset of the parent subgraph. At each level of the search, I have to  
> compute graph cohesion (vertex connectivity), which can take  
> considerable time. A lot of computation time is saved by only  
> searching subgraphs obtained through cutsets. So a complete search of  
> all the subgraphs won't work, but the redundancy I come across is I  
> think unavoidable.

Perhaps you will need a combination of graph/RBGL and some custom
memoization code to keep track of which subgraphs have already been
searched.

Some suggestions on that front:

Don't use a list, use an environment.  

     searchedBranched = new.env(hash=TRUE, parent=emptyenv(), size=X)

where X is an estimate of the number of branches you will search.
Using an environment implies you will need unique character names for
each subgraph.  Do you have that?  If not, you could concatenate node
names.  For a 200 node graph, that should be ok.

Hope that helps some.

+ seth

-- 
Seth Falcon | Computational Biology | Fred Hutchinson Cancer Research Center
http://bioconductor.org


From peter.mcmahan at gmail.com  Fri Mar 16 19:43:55 2007
From: peter.mcmahan at gmail.com (Peter McMahan)
Date: Fri, 16 Mar 2007 13:43:55 -0500
Subject: [R] Fast lookup in ragged array
In-Reply-To: <m2y7lx3uhb.fsf@ziti.local>
References: <0C7CD2BF-4AF6-4CBD-A6B9-36E8B9D3C7B1@gmail.com>
	<24CF2791-BC64-4480-8696-5C0ACF0FEA23@uchicago.edu>
	<m2y7lx3uhb.fsf@ziti.local>
Message-ID: <DB79F3FA-B0B0-407B-B801-733E6D5D964D@gmail.com>

Thanks, I'll give it a try. does R have a limit on variable name  
length? Also, is it better to over-estimate or under-estimate the  
size parameter?
This won't be too hard to implement, either, as I'm already keeping  
the list in a specific environment so all the subprocesses can find  
the same one.

On Mar 16, 2007, at 1:37 PM, Seth Falcon wrote:

> Peter McMahan <peter.mcmahan at gmail.com> writes:
>
>> Well, I hadn't ever seen RBGL before, so that's great. I've been
>> using igraph and sna mainly, but there are a few points lacking
>> between these two. RBGL solves a lot of problems for me!
>>
>> But I'm not sure it will solve this specific problem. Are you
>> suggesting I use RBGL to do a depth-first search of all the
>> subgraphs? For this particular depth-first search I'm not searching
>> every subgraph, but just those that are constructed from a minimal
>> cutset of the parent subgraph. At each level of the search, I have to
>> compute graph cohesion (vertex connectivity), which can take
>> considerable time. A lot of computation time is saved by only
>> searching subgraphs obtained through cutsets. So a complete search of
>> all the subgraphs won't work, but the redundancy I come across is I
>> think unavoidable.
>
> Perhaps you will need a combination of graph/RBGL and some custom
> memoization code to keep track of which subgraphs have already been
> searched.
>
> Some suggestions on that front:
>
> Don't use a list, use an environment.
>
>      searchedBranched = new.env(hash=TRUE, parent=emptyenv(), size=X)
>
> where X is an estimate of the number of branches you will search.
> Using an environment implies you will need unique character names for
> each subgraph.  Do you have that?  If not, you could concatenate node
> names.  For a 200 node graph, that should be ok.
>
> Hope that helps some.
>
> + seth
>
> -- 
> Seth Falcon | Computational Biology | Fred Hutchinson Cancer  
> Research Center
> http://bioconductor.org


From jholtman at gmail.com  Fri Mar 16 20:00:55 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 16 Mar 2007 15:00:55 -0400
Subject: [R] Fast lookup in ragged array
In-Reply-To: <DB79F3FA-B0B0-407B-B801-733E6D5D964D@gmail.com>
References: <0C7CD2BF-4AF6-4CBD-A6B9-36E8B9D3C7B1@gmail.com>
	<24CF2791-BC64-4480-8696-5C0ACF0FEA23@uchicago.edu>
	<m2y7lx3uhb.fsf@ziti.local>
	<DB79F3FA-B0B0-407B-B801-733E6D5D964D@gmail.com>
Message-ID: <644e1f320703161200g78990d50le78c2f605d380443@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070316/20685c64/attachment.pl 

From sfalcon at fhcrc.org  Fri Mar 16 20:02:33 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Fri, 16 Mar 2007 12:02:33 -0700
Subject: [R] Fast lookup in ragged array
In-Reply-To: <DB79F3FA-B0B0-407B-B801-733E6D5D964D@gmail.com> (Peter McMahan's
	message of "Fri, 16 Mar 2007 13:43:55 -0500")
References: <0C7CD2BF-4AF6-4CBD-A6B9-36E8B9D3C7B1@gmail.com>
	<24CF2791-BC64-4480-8696-5C0ACF0FEA23@uchicago.edu>
	<m2y7lx3uhb.fsf@ziti.local>
	<DB79F3FA-B0B0-407B-B801-733E6D5D964D@gmail.com>
Message-ID: <m2tzwl3tbq.fsf@ziti.local>

Peter McMahan <peter.mcmahan at gmail.com> writes:

> Thanks, I'll give it a try. does R have a limit on variable name
> length?

If you are going to have very long names, you might be better off
computing a digest of some kind.  You could use the digest package to
compute an md5sum or the Ruuid package to generate a GUID.

> Also, is it better to over-estimate or under-estimate the
> size parameter?

The environment will grow as needed.  If you overestimate, you will
use more memory than you need to.  Whether this is a problem depends
if you have extra memory available.  Underestimating means that the
underlying hashtable will need to be resized and this has a
performance impact.

+ seth

-- 
Seth Falcon | Computational Biology | Fred Hutchinson Cancer Research Center
http://bioconductor.org


From jholtman at gmail.com  Fri Mar 16 20:14:49 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 16 Mar 2007 15:14:49 -0400
Subject: [R] cumsum over varying column lengths
In-Reply-To: <BAY113-F348DB9B3BC5CA7B8372C2AEE710@phx.gbl>
References: <BAY113-F348DB9B3BC5CA7B8372C2AEE710@phx.gbl>
Message-ID: <644e1f320703161214l1ae55b38xe8548bec8c9d12e9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070316/629f43ee/attachment.pl 

From pbruce at statistics.com  Fri Mar 16 20:41:37 2007
From: pbruce at statistics.com (Peter Bruce)
Date: Fri, 16 Mar 2007 15:41:37 -0400
Subject: [R] online course - Using R for Basic Statistics
Message-ID: <6.1.0.6.2.20070316154030.0ec12868@mail.statistics.com>

oops!  I gave the wrong link - the correct one is 
http://www.statistics.com/courses/Rstatistics/
pb




Dr. John Verzani will present his online course, "Using R for Introductory 
Statistics" April 6 - May 4 at statistics.com.  Participants can ask 
questions and exchange comments with Dr. Verzani via a private discussion 
board throughout the period.

This course covers the use of R to summarize and graph data, calculate 
confidence intervals, test hypotheses, assess goodness-of-fit, and perform 
linear regression.

John Verzani is a member of the faculty at the College of Staten Island of 
the City University of New York, and the author of "Using R for 
Introductory Statistics" (CRC Press), on which this course is based.  His 
research interests and publications are in the area of superprocesses.

There are no set hours when you must be online, and we estimate you will 
need about 10-15 hours per week.


Peter Bruce
courses at statistics.com


From bruno.c at inwind.it  Fri Mar 16 21:18:21 2007
From: bruno.c at inwind.it (Bruno C.)
Date: Fri, 16 Mar 2007 21:18:21 +0100
Subject: [R] Duplicated non contiguous element in a list
Message-ID: <JF0J2L$8F251D7B51A51B03C61248DFF5D2777F@libero.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070316/6d861304/attachment.pl 

From jrkrideau at yahoo.ca  Fri Mar 16 22:00:57 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 16 Mar 2007 17:00:57 -0400 (EDT)
Subject: [R] Probably simple function problem
Message-ID: <106533.77784.qm@web32803.mail.mud.yahoo.com>

# I have a simple function problem. I thought that I
could write a function to modify a couple of vectors
but I am doing something wrong

#I have a standard cost vector called "fuel" and some
adjustments to the
#costs called "adjusts".  The changes are completely
dependend on the length
#of the dataframe newdata  I then need to take the
modifed vectors and use
# them later. I need to do this several times and the
only change in the variables
# is the length of the data.frame.

# Can anyone suggest what I am doing wrong or am I
just misunderstanding what
# a function is supposed to do?

#Example:

adjusts <- c(.50, .70, .29, .27 , .40 , .26 , 125)
coal <- 1:6
newdata <- 1:10

fuel.costing <- function(fuel, utr, mydata) {
cppf <- cppm <- fuel ;
cppf[2] <- fuel[2]*(1-utr[2])*length(mydata) + utr[7]*
utr[2]*utr[5] ;
cppf[4] <- fuel[2]*(1-utr[4])*length(mydata) + utr[7]*
utr[4]*utr[6] ;
cppm[2] <- fuel[2]*(1-utr[1])*length(mydata) ;
cppm[4] <- fuel[2]*(1-utr[3])*length(mydata)
}

fuel.costing(coal, adjusts, newdata)


## original code for one place
cppf <- cppm <- coal ;
cppf[2] <- coal[2]*(1-adjusts[2])*length(newdata) +
adjusts[7]* adjusts[2]*adjusts[5] ;
cppf[4] <- coal[2]*(1-adjusts[4])*length(newdata) +
adjusts[7]* adjusts[4]*adjusts[6] ;
cppm[2] <- coal[2]*(1-adjusts[1])*length(newdata) ;
cppm[4] <- coal[2]*(1-adjusts[3])*length(newdata)

label(cppm) <- "cppm - > SW coal costs adjusted "
label (cppf) <- "cppf -> WW coal costs adjusted "

# Any help or suggests would be greatly appreciated.


From cfsarfert at web.de  Fri Mar 16 22:39:17 2007
From: cfsarfert at web.de (Christel u. Frank Sarfert)
Date: Fri, 16 Mar 2007 22:39:17 +0100
Subject: [R] Discriminating between experiments with xyplot (lattice)
Message-ID: <E1HSK2u-0002xO-00@smtp07.web.de>

Hi, 

suppose I have data from 3 experiments which show conversion as a function of 
time for different boundary conditions, e.g. pressure, temperature. I want to 
plot all experiments as conversion over time grouped according to the 
temperature. However, since I have more than one experiment performed at the 
same temperature (but different pressures) I end up figuring out which curve 
belongs to which experiment. (An example with artificial data of the 
structure I use is given below. It shows three experiments where two 
experiments at temp = 250 but press = 1 and press = 0.5 are plotted within 
one group.)
My question is: Is there a way to identify which curve whithin the same group 
belongs to which experiment, e.g by plotting a label like the experiment 
number to the end point, or by choosing different symbols for the different 
experiments - while keeping the color encoding of the groups?

Your help is greatly appreciated!

Best regards 
                      Frank


require(lattice)

## generating the data, I need
time <- seq(0,50,1)
conversion <- 2 * log(time + 1)
press <- rep(1,51)
temp <- rep(250,51)
experiment <- rep(1, 51)
v1 = as.data.frame(cbind(experiment,time,conversion,press,temp))

conversion <- 2.5 * log(time + 1)
press <- rep(1, 51) 
temp <- rep(270,51)
experiment <- rep(2, 51)
v2 = as.data.frame(cbind(experiment,time,conversion,press,temp))

conversion <- 1.25 * log(time + 1)
press <- rep(0.5, 51) 
temp <- rep(250,51)
experiment <- rep(3, 51)
v3 = as.data.frame(cbind(experiment,time,conversion,press,temp))

d <- rbind(v1,v2,v3)

## plotting
xyplot(conversion ~ time, data = d, groups = factor(temp), auto.key = T)

## result: the group for temp = 250 includes two experiments, which cannot be 
associated to the individual experiments 

-- 

C. u. F. Sarfert
Hauffstr. 8
70825 Korntal


From rvalliant at survey.umd.edu  Fri Mar 16 22:46:50 2007
From: rvalliant at survey.umd.edu (Richard Valliant)
Date: Fri, 16 Mar 2007 16:46:50 -0500
Subject: [R] scatterplot brushing
Message-ID: <s5fad821.069@SURVEYGWIA.UMD.EDU>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070316/4bbafe2e/attachment.pl 

From deepayan.sarkar at gmail.com  Fri Mar 16 22:47:36 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 16 Mar 2007 14:47:36 -0700
Subject: [R] Discriminating between experiments with xyplot (lattice)
In-Reply-To: <E1HSK2u-0002xO-00@smtp07.web.de>
References: <E1HSK2u-0002xO-00@smtp07.web.de>
Message-ID: <eb555e660703161447g5d4dc55ci1eb779d4b71a28a@mail.gmail.com>

On 3/16/07, Christel u. Frank Sarfert <cfsarfert at web.de> wrote:
> Hi,
>
> suppose I have data from 3 experiments which show conversion as a function of
> time for different boundary conditions, e.g. pressure, temperature. I want to
> plot all experiments as conversion over time grouped according to the
> temperature. However, since I have more than one experiment performed at the
> same temperature (but different pressures) I end up figuring out which curve
> belongs to which experiment. (An example with artificial data of the
> structure I use is given below. It shows three experiments where two
> experiments at temp = 250 but press = 1 and press = 0.5 are plotted within
> one group.)
> My question is: Is there a way to identify which curve whithin the same group
> belongs to which experiment, e.g by plotting a label like the experiment
> number to the end point, or by choosing different symbols for the different
> experiments - while keeping the color encoding of the groups?
>
> Your help is greatly appreciated!
>
> Best regards
>                       Frank
>
>
> require(lattice)
>
> ## generating the data, I need
> time <- seq(0,50,1)
> conversion <- 2 * log(time + 1)
> press <- rep(1,51)
> temp <- rep(250,51)
> experiment <- rep(1, 51)
> v1 = as.data.frame(cbind(experiment,time,conversion,press,temp))
>
> conversion <- 2.5 * log(time + 1)
> press <- rep(1, 51)
> temp <- rep(270,51)
> experiment <- rep(2, 51)
> v2 = as.data.frame(cbind(experiment,time,conversion,press,temp))
>
> conversion <- 1.25 * log(time + 1)
> press <- rep(0.5, 51)
> temp <- rep(250,51)
> experiment <- rep(3, 51)
> v3 = as.data.frame(cbind(experiment,time,conversion,press,temp))
>
> d <- rbind(v1,v2,v3)

You want to use make.groups rather than rbind here, so that you retain
information on which experiment each row is coming from:

> dd <- make.groups(v1,v2,v3)
> str(dd)
'data.frame':	153 obs. of  6 variables:
 $ experiment: num  1 1 1 1 1 1 1 1 1 1 ...
 $ time      : num  0 1 2 3 4 5 6 7 8 9 ...
 $ conversion: num  0.00 1.39 2.20 2.77 3.22 ...
 $ press     : num  1 1 1 1 1 1 1 1 1 1 ...
 $ temp      : num  250 250 250 250 250 250 250 250 250 250 ...
 $ which     : Factor w/ 3 levels "v1","v2","v3": 1 1 1 1 1 1 1 1 1 1 ...

Now how you choose to plot this is up to you. One simple possibility
is to create a new factor encoding both experiment and temperature,
e.g.,

xyplot(conversion ~ time, data = dd,
          groups = (which:factor(temp))[,drop=TRUE],
          auto.key = T)

Another is to condition on one, e.g.,

xyplot(conversion ~ time | factor(temp), data = dd, groups = which,
auto.key = T)

It is possible to use one one variable for color and another for
plotting character, but the code involved would be somewhat less
elegant (mostly because the support functions are not already built
in). Let me know if you really want that; I can come up with some
sample code.

-Deepayan


From jasoncbarnhart at msn.com  Fri Mar 16 23:05:57 2007
From: jasoncbarnhart at msn.com (Jason Barnhart)
Date: Fri, 16 Mar 2007 15:05:57 -0700
Subject: [R] Probably simple function problem
References: <106533.77784.qm@web32803.mail.mud.yahoo.com>
Message-ID: <BAY116-DAV10387DDFC8E4E6050236AACF710@phx.gbl>

You didn't specify the exact nature of the problem so I guess that you 
want it return the vector cppm.

Add "return(cppm)" as the final line in function.

Here's what I think you want; it replicates your original code.

> adjusts <- c(.50, .70, .29, .27 , .40 , .26 , 125)
> coal <- 1:6
> newdata <- 1:10
>
> fuel.costing <- function(fuel, utr, mydata) {
+ cppf <- cppm <- fuel ;
+ cppf[2] <- fuel[2]*(1-utr[2])*length(mydata) + utr[7]*
+ utr[2]*utr[5] ;
+ cppf[4] <- fuel[2]*(1-utr[4])*length(mydata) + utr[7]*
+ utr[4]*utr[6] ;
+ cppm[2] <- fuel[2]*(1-utr[1])*length(mydata) ;
+ cppm[4] <- fuel[2]*(1-utr[3])*length(mydata)
+ return(cppm)
+ }
>
> my.cppm <- fuel.costing(coal, adjusts, newdata)
> my.cppm
[1]  1.0 10.0  3.0 14.2  5.0  6.0

HTH
-jason

----- Original Message ----- 
From: "John Kane" <jrkrideau at yahoo.ca>
To: "R R-help" <r-help at stat.math.ethz.ch>
Sent: Friday, March 16, 2007 2:00 PM
Subject: [R] Probably simple function problem


># I have a simple function problem. I thought that I
> could write a function to modify a couple of vectors
> but I am doing something wrong
>
> #I have a standard cost vector called "fuel" and some
> adjustments to the
> #costs called "adjusts".  The changes are completely
> dependend on the length
> #of the dataframe newdata  I then need to take the
> modifed vectors and use
> # them later. I need to do this several times and the
> only change in the variables
> # is the length of the data.frame.
>
> # Can anyone suggest what I am doing wrong or am I
> just misunderstanding what
> # a function is supposed to do?
>
> #Example:
>
> adjusts <- c(.50, .70, .29, .27 , .40 , .26 , 125)
> coal <- 1:6
> newdata <- 1:10
>
> fuel.costing <- function(fuel, utr, mydata) {
> cppf <- cppm <- fuel ;
> cppf[2] <- fuel[2]*(1-utr[2])*length(mydata) + utr[7]*
> utr[2]*utr[5] ;
> cppf[4] <- fuel[2]*(1-utr[4])*length(mydata) + utr[7]*
> utr[4]*utr[6] ;
> cppm[2] <- fuel[2]*(1-utr[1])*length(mydata) ;
> cppm[4] <- fuel[2]*(1-utr[3])*length(mydata)
> }
>
> fuel.costing(coal, adjusts, newdata)
>
>
> ## original code for one place
> cppf <- cppm <- coal ;
> cppf[2] <- coal[2]*(1-adjusts[2])*length(newdata) +
> adjusts[7]* adjusts[2]*adjusts[5] ;
> cppf[4] <- coal[2]*(1-adjusts[4])*length(newdata) +
> adjusts[7]* adjusts[4]*adjusts[6] ;
> cppm[2] <- coal[2]*(1-adjusts[1])*length(newdata) ;
> cppm[4] <- coal[2]*(1-adjusts[3])*length(newdata)
>
> label(cppm) <- "cppm - > SW coal costs adjusted "
> label (cppf) <- "cppf -> WW coal costs adjusted "
>
> # Any help or suggests would be greatly appreciated.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mike.prager at noaa.gov  Fri Mar 16 23:21:33 2007
From: mike.prager at noaa.gov (Mike Prager)
Date: Fri, 16 Mar 2007 18:21:33 -0400
Subject: [R] Rownames are always character?
Message-ID: <0n5mv2lbni3es5cvvr8idarvsnpdjlvl6l@4ax.com>

Gurus,

Can I rely on the rownames() function, when applied to a matrix,
always returning either NULL or an object of type character?  It
seems that row names can be entered as integers, but as of now
(R 2.4.1 on Windows) the function returns character vectors, not
numbers (which is my desired result).

I am using elements of the returned vector to index the matrix.
e.g.,

nams <- rownames(mymat)
for (thisnam in nams) {
	myvec <- mymat[thisnam, ]
	# ... more code ...
}


-- 
Mike Prager, NOAA, Beaufort, NC
* Opinions expressed are personal and not represented otherwise.
* Any use of tradenames does not constitute a NOAA endorsement.


From yuklap.yip at yale.edu  Fri Mar 16 23:22:23 2007
From: yuklap.yip at yale.edu (Yuk Lap Yip (Kevin))
Date: Fri, 16 Mar 2007 18:22:23 -0400
Subject: [R] Implementing trees in R
In-Reply-To: <971536df0703160911n69b3b34cgee73ee687114eaaa@mail.gmail.com>
References: <45FA9D64.6070708@yale.edu>	
	<971536df0703160659u1c339e53k474b962bd846d76e@mail.gmail.com>	
	<45FAAC9E.6020006@yale.edu>	
	<971536df0703160909w2e0f334bn715fbe0be658f220@mail.gmail.com>
	<971536df0703160911n69b3b34cgee73ee687114eaaa@mail.gmail.com>
Message-ID: <45FB189F.1080902@yale.edu>

Gabor,

    Thanks. That helps a lot.

Gabor Grothendieck wrote:
> On 3/16/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>> 1. Here is your example redone using proto:
>>
>> library(proto)
>>
>> parent <- proto()
>> child <- proto(a = 1)
>> parent$child1 <- child
>> child$parent.env <- parent
>
> This last line should have been:
>
> parent.env(child) <- parent
>
>
>>
>> # also just for illustration lets change a
>>
>> parent$child1$a # 1
>> child$a <- 2
>> parent$child1$a # 2
>>
>> 2. To redefine $<- use S3 or S4 but it can be done
>> in conjunction with proto like this:
>>
>> # constructor
>> node <- function(. = parent.frame(), ...)
>>   structure(proto(...), class = c("node", "proto"))
>>
>> "$<-.node" <- function(this, s, value) {
>>    if (s == ".super")
>>        parent.env(this) <- value
>>    if (is.function(value))
>>        environment(value) <- this
>>    if (inherits(value, "node"))
>>        parent.env(value) <- this
>>    this[[as.character(substitute(s))]] <- value
>>    this
>> }
>>
>>
>> p <- node(a = 1)
>> p$child <- node(b = 2)
>> p$child$parent.env()
>> p # same
>>
>>
>>
>> On 3/16/07, Yuk Lap Yip (Kevin) <yuklap.yip at yale.edu> wrote:
>> > Hi Gabor,
>> >
>> >    Thanks for the suggestions. I tried to look for the proto vignette
>> > document but could not find it, could you tell me how to reach it?
>> >
>> >    Besides, is it possible to define my own node object type with a
>> > default behavior for the "<-" operator of its member variables being
>> > referencing rather than copying? Any good reference material/ similar
>> > code examples?
>> >
>> >    Thanks.
>> >
>> > Gabor Grothendieck wrote:
>> > > Lists are not good for this.  There is an example in section 3.3 of
>> > > the proto vignette of using proto objects for this.  That section
>> > > also references an S4 example although its pretty messy with S4.
>> > >
>> > > You might want to look at the graph, RBGL and graphviz packages
>> > > in Bioconductor and the dynamicgraph, mathgraph and sna packages
>> > > on CRAN.
>> > >
>> > > On 3/16/07, Yuk Lap Yip (Kevin) <yuklap.yip at yale.edu> wrote:
>> > >> Hi all,
>> > >>
>> > >>    I am rather new to R. Recently I have been trying to 
>> implement some
>> > >> tree algorithms in R. I used lists to model tree nodes. I thought
>> > >> something like this would work:
>> > >>
>> > >>    parent <- list();
>> > >>    child <- list();
>> > >>    parent$child1 <- child;
>> > >>    child$parent <- parent;
>> > >>
>> > >>    When I tried to check whether a node is its parent's first child
>> > >> using "if (node$parent$child1 == node)", it always returned 
>> false. Then
>> > >> I realized that it does not really work because "parent$child1 
>> <- child"
>> > >> actually makes a copy of child instead of referencing it. I 
>> think one
>> > >> possible fix is to keep a list of node objects, and make references
>> > >> using the positions in the list. For example, I think the following
>> > >> would work:
>> > >>
>> > >>    parent <- list();
>> > >>    child <- list();
>> > >>    nodes <- list(parent, child);
>> > >>    parent$child1 <- 2;
>> > >>    child$parent <- 1;
>> > >>
>> > >>    Then the "first child" test can be rewritten as "if
>> > >> (nodes[[nodes[[nodeId]]$parent]]$child1 == nodeId)". However, I 
>> would
>> > >> prefer not to implement trees in this way, as it requires the
>> > >> inconvenient and error-prone manipulations of node IDs.
>> > >>
>> > >>    May I know if there is a way to make object references to 
>> lists? Or
>> > >> are there other ways to implement tree data structures in R?
>> > >>
>> > >>    BTW, I checked how hclust was implemented, and noticed that 
>> it calls
>> > >> an external Fortran program. I would want a solution not 
>> involving any
>> > >> external programs.
>> > >>
>> > >>    Thanks.
>> > >>
>> > >> --
>> > >>
>> > >>
>> > >>        God bless.
>> > >>
>> > >>        Kevin
>> > >>
>> > >> ______________________________________________
>> > >> R-help at stat.math.ethz.ch mailing list
>> > >> https://stat.ethz.ch/mailman/listinfo/r-help
>> > >> PLEASE do read the posting guide
>> > >> http://www.R-project.org/posting-guide.html
>> > >> and provide commented, minimal, self-contained, reproducible code.
>> > >>
>> >
>> > --
>> >
>> >
>> >        God bless.
>> >
>> >        Kevin
>> >
>> >
>>

-- 


	God bless.

	Kevin


From mike.prager at noaa.gov  Fri Mar 16 23:26:41 2007
From: mike.prager at noaa.gov (Mike Prager)
Date: Fri, 16 Mar 2007 18:26:41 -0400
Subject: [R] Rownames are always character?
References: <0n5mv2lbni3es5cvvr8idarvsnpdjlvl6l@4ax.com>
Message-ID: <u96mv2lmui77lqf1crcdmfu98p4op7g8ij@4ax.com>

Mike Prager <mike.prager at noaa.gov> wrote:

> Gurus,
> 
> Can I rely on the rownames() function, when applied to a matrix,
> always returning either NULL or an object of type character?  It
> seems that row names can be entered as integers, but as of now
> (R 2.4.1 on Windows) the function returns character vectors, not
> numbers (which is my desired result).

(To clarify my point on this Friday afternoon:  the observed
behavior is my desired result.  I'm just asking, can I count on
it?)

> I am using elements of the returned vector to index the matrix.
> e.g.,
> 
> nams <- rownames(mymat)
> for (thisnam in nams) {
> 	myvec <- mymat[thisnam, ]
> 	# ... more code ...
> }

-- 
Mike Prager, NOAA, Beaufort, NC
* Opinions expressed are personal and not represented otherwise.
* Any use of tradenames does not constitute a NOAA endorsement.


From jontwang at gmail.com  Fri Mar 16 23:52:52 2007
From: jontwang at gmail.com (Jonathan Wang)
Date: Fri, 16 Mar 2007 17:52:52 -0500
Subject: [R] Revisiting multiple plots
Message-ID: <4f7636bf0703161552w145e7748laa06957f3599a42d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070316/399fe6b5/attachment.pl 

From Cody_Hamilton at Edwards.com  Fri Mar 16 23:55:23 2007
From: Cody_Hamilton at Edwards.com (Cody_Hamilton at Edwards.com)
Date: Fri, 16 Mar 2007 15:55:23 -0700
Subject: [R] R and clinical studies
In-Reply-To: <45FA9F26.2070304@vanderbilt.edu>
Message-ID: <OF36FBA670.7764FE56-ON882572A0.0079C102-882572A0.007DB295@irvine.edwards.com>


I agree that most problems arise in the data management / file derivation
phase.  From my reading of 21 CFR 11, it appears that this document focuses
primarily on data management (as well as on software directly involved in a
medical device) rather than on validation of statistical functions.  I
believe this point has been made previously on the R-help list.

With regards to validating functions, I have often wondered how one can
validate a function when one cannot see what it is doing.  You could
certainly compare calculations from one package to the same calculations
from another package, but then you must purchase (ouch!) and know how to
properly use two software packages instead of one.  And I suppose they
could both be wrong!  Is not peer-review the best form of validation?  . .
. I suspect I may be "preaching to the choir" here.

I would love nothing more than to migrate our stat group over to R from
SAS.  Based on my experience with R/Splus, the language seems more
extendable, flexible, and has much better graphics (as has been pointed out
many times on this list).  It also has available the many contributions of
generous R users.  However, it has been hard to win pure SAS users onto R
(even if it saves the company money!).  One can't send the biostat group
off to R training like one would to SAS classes.  Learning R requires
initiative from the user (which is not necessarily a bad thing).  I
considered encouraging the purchase of Splus as an intermediate step
(hoping that its proprietary nature would soothe fears regarding open
source software), but that option was not as cheap as I thought.

Regards,
    -Cody




Delphine Fontaine wrote:
> Thanks for your answer which was very helpfull. I have another question:
>
> I have read in this document
> (http://cran.r-project.org/doc/manuals/R-intro.pdf) that most of the
> programs written in R are ephemeral and that new releases are not
> always compatible with previous releases. What I would like to know is
> if R functions are already validated and if not, what should we do to
> validate a R function ?
>

In the sense in which most persons use the term 'validate', it means to
show with one or more datasets that the function is capable of producing
the right answer.  It doesn't mean that it produces the right answer for
every dataset although we hope it does.  [As an aside, most errors are
in the data manipulation phase, not in the analysis phase.]  So I think
that instead of validating functions we should spend more effort on
validating analyses [and validating analysis file derivation].  Pivotal
analyses can be re-done a variety of ways, in R or in separate
programmable packages such as Stata.

--
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jontwang at gmail.com  Fri Mar 16 23:56:01 2007
From: jontwang at gmail.com (Jonathan Wang)
Date: Fri, 16 Mar 2007 17:56:01 -0500
Subject: [R] CPU usage on Windows
Message-ID: <4f7636bf0703161556sda5b390ta748f9d3d797ddae@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070316/b8556548/attachment.pl 

From ggrothendieck at gmail.com  Sat Mar 17 00:14:08 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 16 Mar 2007 19:14:08 -0400
Subject: [R] Revisiting multiple plots
In-Reply-To: <4f7636bf0703161552w145e7748laa06957f3599a42d@mail.gmail.com>
References: <4f7636bf0703161552w145e7748laa06957f3599a42d@mail.gmail.com>
Message-ID: <971536df0703161614u8a9f4c0he3c284b0c85c730a@mail.gmail.com>

Try this (or use xyplot.zoo and write a panel function for that):

library(zoo)
set.seed(1)
tt <- as.Date(paste(2004, rep(1:2, 5), sample(28, 10), sep = "-"))
foo <- zoo(matrix(rnorm(100), 10), tt)

pnl <- function(x, y, ...) {
	lines(x, y, ...)
	abline(h = mean(y))
}
plot(foo, panel = pnl)


On 3/16/07, Jonathan Wang <jontwang at gmail.com> wrote:
> Suppose I create a multiple plot with zoo, using:
>
> index <- ISOdatetime(2004, rep(1:2, 5), sample(28, 10), 0, 0, 0)
> foo <- zoo(rnorm(10), index)
> for (i in 1:9) {
>  data <- rnorm(10)
>  z1 <- zoo(data, index)
>  foo <- cbind(foo, z1)
> }
> plot(foo)
>
> This creates 10 plots on one device, one for each column in foo.
>
> Now I want to go back and use abline to draw a line at the mean on each of
> my 10 plots. How do I select the appropriate set of axes to draw my line on?
>
> Thanks,
> Jonathan
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From h.wickham at gmail.com  Sat Mar 17 00:16:51 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 16 Mar 2007 18:16:51 -0500
Subject: [R] scatterplot brushing
In-Reply-To: <s5fad821.069@SURVEYGWIA.UMD.EDU>
References: <s5fad821.069@SURVEYGWIA.UMD.EDU>
Message-ID: <f8e6ff050703161616g1ea6f66axcffdc7da17453dfa@mail.gmail.com>

Have a look at iplots.  (and rggobi is the updated version of xggobi)

Hadley

On 3/16/07, Richard Valliant <rvalliant at survey.umd.edu> wrote:
> Is there a package (other than xgobi which requires an X server) that
> will do scatterplot brushing?  I see a mention in the mail archive of
> R-orca by Anthony Rossini but it is not in the current list of
> packages.
>
> My OS is Windows XP version 5.1, service pack 2
> R version 2.4.1 (2006-12-18)
>
> Thanks
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From h.wickham at gmail.com  Sat Mar 17 00:18:02 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 16 Mar 2007 18:18:02 -0500
Subject: [R] scatterplot brushing
In-Reply-To: <f8e6ff050703161616g1ea6f66axcffdc7da17453dfa@mail.gmail.com>
References: <s5fad821.069@SURVEYGWIA.UMD.EDU>
	<f8e6ff050703161616g1ea6f66axcffdc7da17453dfa@mail.gmail.com>
Message-ID: <f8e6ff050703161618l39ed0cddvf02b8108b68051b0@mail.gmail.com>

I should mention that it's very easy to install rggobi and ggobi on
windows these days.  GGobi has a nice installer,
http://www.ggobi.org/downloads, and rggobi is on cran.

Hadley

On 3/16/07, hadley wickham <h.wickham at gmail.com> wrote:
> Have a look at iplots.  (and rggobi is the updated version of xggobi)
>
> Hadley
>
> On 3/16/07, Richard Valliant <rvalliant at survey.umd.edu> wrote:
> > Is there a package (other than xgobi which requires an X server) that
> > will do scatterplot brushing?  I see a mention in the mail archive of
> > R-orca by Anthony Rossini but it is not in the current list of
> > packages.
> >
> > My OS is Windows XP version 5.1, service pack 2
> > R version 2.4.1 (2006-12-18)
> >
> > Thanks
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From murdoch at stats.uwo.ca  Sat Mar 17 00:37:14 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 16 Mar 2007 19:37:14 -0400
Subject: [R] CPU usage on Windows
In-Reply-To: <4f7636bf0703161556sda5b390ta748f9d3d797ddae@mail.gmail.com>
References: <4f7636bf0703161556sda5b390ta748f9d3d797ddae@mail.gmail.com>
Message-ID: <45FB2A2A.3040103@stats.uwo.ca>

On 3/16/2007 6:56 PM, Jonathan Wang wrote:
> I'm using R with emacs & ESS on Windows. When I create a plot, sometimes R
> will seem to get stuck in a busy loop, i.e. it will use 100% of my CPU.
> However the system is still somewhat responsive, and the problem usually
> goes away if I create a new device with windows(). If I then close this
> device, making the first device active again, sometimes R will get stuck in
> the busy loop again.
> 
> Has anybody heard of this behavior, or, better yet, have a solution?

I've heard of a number of problems with Emacs on Windows.  I wouldn't 
recommend using it.  As far as I can see, it makes a number of 
assumptions about the OS that just aren't true about Windows.

If you can reproduce the behaviour outside of Emacs, I'll investigate.

Duncan Murdoch


From peter.mcmahan at gmail.com  Sat Mar 17 01:28:38 2007
From: peter.mcmahan at gmail.com (Peter McMahan)
Date: Fri, 16 Mar 2007 19:28:38 -0500
Subject: [R] Fast lookup in ragged array
In-Reply-To: <m2tzwl3tbq.fsf@ziti.local>
References: <0C7CD2BF-4AF6-4CBD-A6B9-36E8B9D3C7B1@gmail.com>
	<24CF2791-BC64-4480-8696-5C0ACF0FEA23@uchicago.edu>
	<m2y7lx3uhb.fsf@ziti.local>
	<DB79F3FA-B0B0-407B-B801-733E6D5D964D@gmail.com>
	<m2tzwl3tbq.fsf@ziti.local>
Message-ID: <068C0424-73A9-4758-A52F-D566040CBCFF@gmail.com>

That's a good point. What's the overhead on digests like that? Also,  
does that open up the possibility, exceedingly small though it may  
be, of misidentifying a branch as already searched and missing a  
qualifying subgraph?

On Mar 16, 2007, at 2:02 PM, Seth Falcon wrote:

> Peter McMahan <peter.mcmahan at gmail.com> writes:
>
>> Thanks, I'll give it a try. does R have a limit on variable name
>> length?
>
> If you are going to have very long names, you might be better off
> computing a digest of some kind.  You could use the digest package to
> compute an md5sum or the Ruuid package to generate a GUID.
>
>> Also, is it better to over-estimate or under-estimate the
>> size parameter?
>
> The environment will grow as needed.  If you overestimate, you will
> use more memory than you need to.  Whether this is a problem depends
> if you have extra memory available.  Underestimating means that the
> underlying hashtable will need to be resized and this has a
> performance impact.
>
> + seth
>
> -- 
> Seth Falcon | Computational Biology | Fred Hutchinson Cancer  
> Research Center
> http://bioconductor.org
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From kubovy at virginia.edu  Sat Mar 17 01:46:12 2007
From: kubovy at virginia.edu (Kubovy Michael)
Date: Fri, 16 Mar 2007 20:46:12 -0400
Subject: [R] Font in JavaGD() & pdf()
Message-ID: <DAB9AD57-61A4-493C-A8F2-D8C30A0ABE69@virginia.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070316/756b8018/attachment.pl 

From deepayan.sarkar at gmail.com  Sat Mar 17 02:21:29 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 16 Mar 2007 18:21:29 -0700
Subject: [R] Discriminating between experiments with xyplot (lattice)
In-Reply-To: <1312270888@web.de>
References: <1312270888@web.de>
Message-ID: <eb555e660703161821w1194679bre301b56ddbc52959@mail.gmail.com>

Here's one possibility:


d <- make.groups(v1,v2,v3)

## create interaction dropping unused levels
d$intg <- with(d, which:factor(temp))[, drop=TRUE]

## extract experiment and temp information from levels
intg.expt <- sapply(strsplit(levels(d$intg), ":", fixed = TRUE), "[[", 1)
intg.temp <- sapply(strsplit(levels(d$intg), ":", fixed = TRUE), "[[", 2)

## find a suitable color vector (where colors are repeated when the
## temperature is)
temp.unique <- unique(intg.temp)
col.temp <-
    rep(trellis.par.get("superpose.symbol")$col,
        length = length(temp.unique))
col.intg <- col.temp[match(intg.temp, temp.unique)]


xyplot(conversion ~ time, data = d, groups = intg,
       intg.expt = intg.expt,
       panel = panel.superpose,
       panel.groups = function(x, y, ..., group.number, intg.expt) {
           panel.xyplot(x, y, ...)
           panel.text(tail(x, 1), tail(y, 1), intg.expt[group.number], pos = 4)
       },
       col = col.intg,
       key = list(text = list(temp.unique), points = list(col =
col.temp, pch = 1)))


Hope that helps,

Deepayan


On 3/16/07, Frank Sarfert <CFSarfert at web.de> wrote:
> Hi Deepayan,
>
> many thanks for your quick reply.
> I actually cannot condition whith the experiment number, because in my real life data, I would like to condition with pressure and I have many more experiments (not just 3). So ideally, I would have a plot with a label near the end point of the curve which says which experiment it comes from.  So, if you could really write  some code  -  that'd be great! Many thanks in advance!
>
> Best regards
>                         Frank
>
>
> -----Urspr?ngliche Nachricht-----
> Von: "Deepayan Sarkar" <deepayan.sarkar at gmail.com>
> Gesendet: 16.03.07 22:47:47
> An: "Christel u. Frank Sarfert" <cfsarfert at web.de>
> CC: r-help at stat.math.ethz.ch
> Betreff: Re: [R] Discriminating between experiments with xyplot (lattice)
>
>
> On 3/16/07, Christel u. Frank Sarfert <cfsarfert at web.de> wrote:
> > Hi,
> >
> > suppose I have data from 3 experiments which show conversion as a function of
> > time for different boundary conditions, e.g. pressure, temperature. I want to
> > plot all experiments as conversion over time grouped according to the
> > temperature. However, since I have more than one experiment performed at the
> > same temperature (but different pressures) I end up figuring out which curve
> > belongs to which experiment. (An example with artificial data of the
> > structure I use is given below. It shows three experiments where two
> > experiments at temp = 250 but press = 1 and press = 0.5 are plotted within
> > one group.)
> > My question is: Is there a way to identify which curve whithin the same group
> > belongs to which experiment, e.g by plotting a label like the experiment
> > number to the end point, or by choosing different symbols for the different
> > experiments - while keeping the color encoding of the groups?
> >
> > Your help is greatly appreciated!
> >
> > Best regards
> >                       Frank
> >
> >
> > require(lattice)
> >
> > ## generating the data, I need
> > time <- seq(0,50,1)
> > conversion <- 2 * log(time + 1)
> > press <- rep(1,51)
> > temp <- rep(250,51)
> > experiment <- rep(1, 51)
> > v1 = as.data.frame(cbind(experiment,time,conversion,press,temp))
> >
> > conversion <- 2.5 * log(time + 1)
> > press <- rep(1, 51)
> > temp <- rep(270,51)
> > experiment <- rep(2, 51)
> > v2 = as.data.frame(cbind(experiment,time,conversion,press,temp))
> >
> > conversion <- 1.25 * log(time + 1)
> > press <- rep(0.5, 51)
> > temp <- rep(250,51)
> > experiment <- rep(3, 51)
> > v3 = as.data.frame(cbind(experiment,time,conversion,press,temp))
> >
> > d <- rbind(v1,v2,v3)
>
> You want to use make.groups rather than rbind here, so that you retain
> information on which experiment each row is coming from:
>
> > dd <- make.groups(v1,v2,v3)
> > str(dd)
> 'data.frame':   153 obs. of  6 variables:
>  $ experiment: num  1 1 1 1 1 1 1 1 1 1 ...
>  $ time      : num  0 1 2 3 4 5 6 7 8 9 ...
>  $ conversion: num  0.00 1.39 2.20 2.77 3.22 ...
>  $ press     : num  1 1 1 1 1 1 1 1 1 1 ...
>  $ temp      : num  250 250 250 250 250 250 250 250 250 250 ...
>  $ which     : Factor w/ 3 levels "v1","v2","v3": 1 1 1 1 1 1 1 1 1 1 ...
>
> Now how you choose to plot this is up to you. One simple possibility
> is to create a new factor encoding both experiment and temperature,
> e.g.,
>
> xyplot(conversion ~ time, data = dd,
>           groups = (which:factor(temp))[,drop=TRUE],
>           auto.key = T)
>
> Another is to condition on one, e.g.,
>
> xyplot(conversion ~ time | factor(temp), data = dd, groups = which,
> auto.key = T)
>
> It is possible to use one one variable for color and another for
> plotting character, but the code involved would be somewhat less
> elegant (mostly because the support functions are not already built
> in). Let me know if you really want that; I can come up with some
> sample code.
>
> -Deepayan
>
>
>
> _____________________________________________________________________
> Der WEB.DE SmartSurfer hilft bis zu 70% Ihrer Onlinekosten zu sparen!
> http://smartsurfer.web.de/?mc=100071&distributionid=000000000066
>
>


From skiadas at hanover.edu  Sat Mar 17 02:43:46 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Fri, 16 Mar 2007 21:43:46 -0400
Subject: [R] Rownames are always character?
In-Reply-To: <u96mv2lmui77lqf1crcdmfu98p4op7g8ij@4ax.com>
References: <0n5mv2lbni3es5cvvr8idarvsnpdjlvl6l@4ax.com>
	<u96mv2lmui77lqf1crcdmfu98p4op7g8ij@4ax.com>
Message-ID: <5ACB2113-3795-4894-A8C4-C75BA49F9C0F@hanover.edu>

On Mar 16, 2007, at 6:26 PM, Mike Prager wrote:

> Mike Prager <mike.prager at noaa.gov> wrote:
>
>> Gurus,
>>
>> Can I rely on the rownames() function, when applied to a matrix,
>> always returning either NULL or an object of type character?  It
>> seems that row names can be entered as integers, but as of now
>> (R 2.4.1 on Windows) the function returns character vectors, not
>> numbers (which is my desired result).
>
> (To clarify my point on this Friday afternoon:  the observed
> behavior is my desired result.  I'm just asking, can I count on
> it?)

I would venture to guess that rownames() would always be returning  
something that you would then be able to use for indexing, to  
retrieve particular entries. The help page also implies that the  
return value will always be a character vector, or NULL:

	"If do.NULL is FALSE, a character vector (of length NROW(x) or NCOL 
(x)) is returned in any case, prepending prefix to simple numbers, if  
there are no dimnames or the corresponding component of the dimnames  
is NULL."

I would think you can count on this about as much as you can count  
the sum function to always add up its arguments, or something of that  
sort.

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From rmh at temple.edu  Sat Mar 17 03:56:29 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri, 16 Mar 2007 22:56:29 -0400 (EDT)
Subject: [R] Fwd: Re:  CPU usage on Windows
Message-ID: <20070316225629.BWZ11263@po-d.temple.edu>

I can't imagine using Windows without Emacs.
In particular, the Windows ports of Emacs are very aware
of the operating system and usually make the right assumptions.

The type of behavior you are noticing can probably be cured by typing C-g in the
*R* buffer in emacs.  The most likely cause is that the R process in Emacs
is waiting for the plot to finish and is querying the plotting device.
Most of that excess CPU usage is from the query loop.  The C-g tells Emacs and R
to stop waiting.

If C-g doesn't stop the 100% CPU utilization, then it is most likely something
about the specific plot you are drawing.  We will need to see a reproducible
example to say more.

Rich

---- Original message ----
>Date: Fri, 16 Mar 2007 19:37:14 -0400
>From: Duncan Murdoch <murdoch at stats.uwo.ca>  
>Subject: Re: [R] CPU usage on Windows  
>To: Jonathan Wang <jontwang at gmail.com>
>Cc: r-help at stat.math.ethz.ch
>
>On 3/16/2007 6:56 PM, Jonathan Wang wrote:
>> I'm using R with emacs & ESS on Windows. When I create a plot, sometimes R
>> will seem to get stuck in a busy loop, i.e. it will use 100% of my CPU.
>> 
>> Has anybody heard of this behavior, or, better yet, have a solution?
>
>I've heard of a number of problems with Emacs on Windows.  I wouldn't 
>recommend using it.  As far as I can see, it makes a number of 
>assumptions about the OS that just aren't true about Windows.


From ripley at stats.ox.ac.uk  Sat Mar 17 09:59:05 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 17 Mar 2007 08:59:05 +0000 (GMT)
Subject: [R] Font in JavaGD() & pdf()
In-Reply-To: <DAB9AD57-61A4-493C-A8F2-D8C30A0ABE69@virginia.edu>
References: <DAB9AD57-61A4-493C-A8F2-D8C30A0ABE69@virginia.edu>
Message-ID: <Pine.LNX.4.64.0703170857160.30234@gannet.stats.ox.ac.uk>

what do you mean by 'send the figure to pdf'?

My guess is that this is a Mac-specific question (e.g. you are using the 
R.app GUI), so please consider if this is the appropriate list.

On Fri, 16 Mar 2007, Kubovy Michael wrote:

> Dear r-helpers,
>
> When I do an xYplot and display the result in a JavaGD() window, the
> font is sans-serif (presumably Helvetica). When I send the figure to
> a pdf, I get a serif font (presumably times). How do I insure that
> the font in the pdf is indeed the default sans serif?
>
> > sessionInfo()
> R version 2.4.1 (2006-12-18)
> i386-apple-darwin8.8.1
>
> locale:
> C
>
> attached base packages:
> [1] "grid"      "datasets"  "stats"     "graphics"  "grDevices"
> "utils"     "methods"   "base"
>
> other attached packages:
>         coda      gmodels         lme4       Matrix           HH
> multcomp      mvtnorm          vcd
>     "0.10-7"     "2.13.1"  "0.9975-13"  "0.9975-11"     "1.18-1"
> "0.991-8"      "0.7-5"      "1.0-2"
>   colorspace        Hmisc       xtable latticeExtra      lattice
> gridBase         MASS          JGR
>       "0.95"      "3.2-1"      "1.4-3"      "0.1-4"
> "0.14-16"      "0.4-3"     "7.2-32"     "1.4-15"
>       iplots       JavaGD        rJava
>      "1.0-5"      "0.3-6"     "0.4-14"
>
>
>
> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
> Parcels:    Room 102        Gilmer Hall
>         McCormick Road    Charlottesville, VA 22903
> Office:    B011    +1-434-982-4729
> Lab:        B019    +1-434-982-4751
> Fax:        +1-434-982-4766
> WWW:    http://www.people.virginia.edu/~mk9y/
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From knoblauch at lyon.inserm.fr  Sat Mar 17 12:11:01 2007
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Sat, 17 Mar 2007 10:11:01 -0100 (CET)
Subject: [R] problem with mfg argument of par
Message-ID: <49281.82.231.93.240.1174122661.squirrel@webmail.lyon.inserm.fr>

I'm having a problem with the mfg option of par.  Am I making an error in my
usage?  Here is a simple example that I thought would plot to the 4 corners
of a 2x2 plot but doesn't plot to the lower right and plots twice on the
upper left.

par(mfrow = c(2, 2))
pos <- 	as.matrix(expand.grid(1:2, 1:2))
for (ix in 4:1) {
	par(mfg = pos[ix, ])
	plot(1:5)
	}

Thank you in advance.

R version 2.4.1 Patched (2007-01-23 r40561)
i386-apple-darwin8.8.1

locale:
C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
[7] "base"


-- 
Ken Knoblauch
Inserm U846
Institut Cellule Souche et Cerveau
D?partement Neurosciences Int?gratives
18 avenue du Doyen L?pine
69500 Bron
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: +33 (0)6 84 10 64 10
http://www.pizzerialesgemeaux.com/u846/


From blindglobe at gmail.com  Sat Mar 17 11:18:18 2007
From: blindglobe at gmail.com (AJ Rossini)
Date: Sat, 17 Mar 2007 11:18:18 +0100
Subject: [R] R and clinical studies
In-Reply-To: <20070316093649.7klhg4n673gogs8c@webmail.adm.unige.ch>
References: <20070309142909.hgceqlqqasssggo0@webmail.adm.unige.ch>
	<27CA3827C6B33E40874682C469E774DD04DB9136@FMD3CT001.fda.gov>
	<20070316093649.7klhg4n673gogs8c@webmail.adm.unige.ch>
Message-ID: <200703171118.27699.blindglobe@gmail.com>

On Friday 16 March 2007 09:36, Delphine Fontaine wrote:
> Thanks for your answer which was very helpfull. I have another question:
>
> I have read in this document
> (http://cran.r-project.org/doc/manuals/R-intro.pdf) that most of the
> programs written in R are ephemeral and that new releases are not
> always compatible with previous releases. What I would like to know is
> if R functions are already validated and if not, what should we do to
> validate a R function ?

Validation is in the eye of the beholder. 

In particular, for clinical studies, from the corporate or institutional point 
of view, "what we should do to validate an R function" should be answered by 
the local Standard Operating Procedures (SOPs) for "what should we do to 
validate a computer programming language function".   

If you are working with clinical trials as part of a health authority 
submission process, you should have those in place.  

Of course, what you probably are interested in is an approach where you 
qualify R, and validate programs and packages written for R, which might be 
another better approach, in which case the same applies.  Your SOPs should 
apply to both. 

(Now, assuming that you've done a reasonable job on the processes, as per 
Mats' answer, the point is that "R" vs. anything else is a simple red 
herring, as there is nothing in the spirit of the regulations which 
differentiates any of the characteristics of R with any other reasonable 
piece of software, for appropriate definitions of reasonableness).

<digression title="semi-relevant, on SOPs and commercial software">
I should point out that a certain large company I'm familiar with, who uses a 
certain "famous" piece of statistical software for activities perhaps 
described above, can't use the most recent version because of interesting 
issues with its "self qualification" tool, which prevents it from 
self-qualifying the new version on any installation on a certain operating 
system originating near where I used to live, when the previous version of 
the famous software had been installed.  This feature, if not reverted, would 
necessitate total disk wipe of ALL computers requiring qualification running 
this operating system, where the new version of this famous piece of software 
would be installed, if this certain large company wants to follow it's SOPs.   
This is apparently a feature, not a bug, and demonstrates clearly the 
benefits and joys of commercial support when millions of swiss francs of 
licensing fees are involved.
</digression>

I'm not a lawyer, nor am I speaking for any corporation indirectly referenced 
above, nor will I provide sufficient justification to help anyone else take 
any of the statements as a fact.

best,
-tony

blindglobe at gmail.com
Muttenz, Switzerland.
"Commit early,commit often, and commit in a repository from which we can 
easily roll-back your mistakes" (AJR, 4Jan05).
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 189 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070317/1c3c0fc4/attachment.bin 

From lawremi at iastate.edu  Sat Mar 17 11:52:33 2007
From: lawremi at iastate.edu (Michael Lawrence)
Date: Sat, 17 Mar 2007 19:52:33 +0900
Subject: [R] scatterplot brushing
In-Reply-To: <f8e6ff050703161618l39ed0cddvf02b8108b68051b0@mail.gmail.com>
References: <s5fad821.069@SURVEYGWIA.UMD.EDU>
	<f8e6ff050703161616g1ea6f66axcffdc7da17453dfa@mail.gmail.com>
	<f8e6ff050703161618l39ed0cddvf02b8108b68051b0@mail.gmail.com>
Message-ID: <509e0620703170352g21d7d0e4x24b5cc1eb18ce5f6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070317/29247582/attachment.pl 

From ripley at stats.ox.ac.uk  Sat Mar 17 12:27:14 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 17 Mar 2007 11:27:14 +0000 (GMT)
Subject: [R] problem with mfg argument of par
In-Reply-To: <49281.82.231.93.240.1174122661.squirrel@webmail.lyon.inserm.fr>
References: <49281.82.231.93.240.1174122661.squirrel@webmail.lyon.inserm.fr>
Message-ID: <Pine.LNX.4.64.0703171120330.25452@auk.stats>

The problem here is that setting par(mfg =) does an implicit par(new=TRUE) 
on the next plot (as otherwise it would advance to the next position). 
That is going to be confusing if there has been no plot, and that is what 
you are seeing: the first plot is done at c(1,1), overriding the setting 
of par(mfg =).  (The first plot after par(mfrow =) clears the device and 
set up the parameters, _unless_ it is done with new=TRUE.)

Solutions are to use screen() or layout() or do a first empty plot
after par(mfrow = c(2, 2)).  E.g.

par(mfrow = c(2, 2))
plot.new()
pos <- as.matrix(expand.grid(1:2, 1:2))
for (ix in 4:1) {par(mfg = pos[ix, ]); plot(1:ix)}

does what I think you intended.

On Sat, 17 Mar 2007, Ken Knoblauch wrote:

> I'm having a problem with the mfg option of par.  Am I making an error in my
> usage?  Here is a simple example that I thought would plot to the 4 corners
> of a 2x2 plot but doesn't plot to the lower right and plots twice on the
> upper left.
>
> par(mfrow = c(2, 2))
> pos <- 	as.matrix(expand.grid(1:2, 1:2))
> for (ix in 4:1) {
> 	par(mfg = pos[ix, ])
> 	plot(1:5)
> 	}
>
> Thank you in advance.
>
> R version 2.4.1 Patched (2007-01-23 r40561)
> i386-apple-darwin8.8.1
>
> locale:
> C
>
> attached base packages:
> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
> [7] "base"
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From S.Pickett at exeter.ac.uk  Sat Mar 17 13:36:19 2007
From: S.Pickett at exeter.ac.uk (Simon Pickett)
Date: Sat, 17 Mar 2007 12:36:19 -0000 (GMT)
Subject: [R] legend with density and fill
Message-ID: <3109.144.173.76.117.1174134979.squirrel@www.webmail.ex.ac.uk>

Hi,

I am trying to make a legend with four symbols as follows

1.white box
2.black box
3.clear box (same as background)
4.clear box with shading lines

but the shading lines arent showing...

here is my code.

par(bg="lightyellow")
barplot(c(seq(1,6,1)))
legend(8.5,0.3, bty="o", legend=c("young","old","male","female"),
col="black",cex=1.5,
fill=c("white","dark grey",0,0),density=c(NA,NA,0,100),angle=45)

any suggestions much appreciated,
Thanks, Simon.



Simon Pickett
PhD student
Centre For Ecology and Conservation
Tremough Campus
University of Exeter in Cornwall
TR109EZ
Tel 01326371852


From binabina at bellsouth.net  Sat Mar 17 13:51:41 2007
From: binabina at bellsouth.net (sabine)
Date: Sat, 17 Mar 2007 08:51:41 -0400
Subject: [R] Markov Decision Process Estimating in R (MDP)
Message-ID: <45FBE45D.2040309@bellsouth.net>

Hello, does anyone know of an existing toolbox to estimate Markov 
Decision Processes in R?

example in MATLAB: http://www.cs.ubc.ca/~murphyk/Software/MDP/mdp.html

-zubin


From tyler.smith at mail.mcgill.ca  Sat Mar 17 14:05:45 2007
From: tyler.smith at mail.mcgill.ca (Tyler Smith)
Date: Sat, 17 Mar 2007 13:05:45 +0000 (UTC)
Subject: [R] how to...
References: <JF06X6$737BBE7E747D5E96E150C72E509F19E1@libero.it>
Message-ID: <slrnevnpt9.v7o.tyler.smith@blackbart.mynetwork>

On 2007-03-16, casot at libero.it <casot at libero.it> wrote:
> for example:
> I have got these data, organized in a dataframe. 
>
> 		sample1	sample2	sample3	sample4	group
> replicate1	1.00	0.02	0.35	0.50	A
> replicate2	1.00	0.02	1.54	1.11	A
> replicate3	1.00	0.02	1.54	1.11	A
> replicate4	1.00	0.02	1.54	1.11	A
> replicate5	1.00	0.10	0.18	0.72	B
> replicate6	1000.00	0.75	0.86	7.26	B
> replicate7	1000.00	0.75	0.18	0.36	B
> replicate8	1000.00	0.75	12.09	0.74	B
> replicate9	1000.00	0.75	12.09	0.84	C
> replicate10	1000.00	0.98	0.65	0.50	C
> replicate11	2.00	6.00	6.00	2.00	C
> replicate12	6.00	6.00	2.00	6.00	C
>
>
> Using "aov()" I can run a test on each column. but I would
> like to run the ANOVAs for each colum (that in my case are hundreds)
> in an automated way. 
...
> SAMPLE ANOVA
> sample1 ok
> sample2 ok
> sample3 not significant
> ....
>

sapply(sample.df[,1:4], 
	FUN=function(x) summary.aov(aov(x~sample.df$group))[[1]][1,"Pr(>F)"])

sapply applies a function to each 'column' of a dataframe, returning
the result as a vector.

FUN=function(x) ... is an anonymous function that inserts the column
of the dataframe into the following function for each column as sapply
loops through them.

summary.aov(...) produces a list of tables, although in this case the
list is only one table long. [[1]][1,"Pr(>F)"] extracts the p-value
from the first row of the first table.

The result for your example is:

sapply(sample.df[,1:4],
    FUN=function(x)summary.aov(aov(x~sample.df$group))[[1]][1,"Pr(>F)"])

   sample1    sample2    sample3    sample4
0.09961436 0.04405756 0.49289026 0.67389417


-- 
Regards,

Tyler Smith


From fuss-h at ulster.ac.uk  Sat Mar 17 15:22:49 2007
From: fuss-h at ulster.ac.uk (Hendrik Fuss)
Date: Sat, 17 Mar 2007 14:22:49 +0000
Subject: [R] arrowhead styles
Message-ID: <FC696392-A2A4-498A-9F53-177538402CF2@ulster.ac.uk>

Hi all,

I've been using the arrows() function in plots a lot, but I'm not  
happy with the appearance of the arrow heads. I understand that arrows 
() doesn't offer more sophisticated arrowhead shapes like e.g. a  
filled triangle, possibly with choice of angle at the point. Does  
anyone know an easy way to achieve this?

thanks
Hendrik


From sfalcon at fhcrc.org  Sat Mar 17 16:24:26 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Sat, 17 Mar 2007 08:24:26 -0700
Subject: [R] Fast lookup in ragged array
In-Reply-To: <068C0424-73A9-4758-A52F-D566040CBCFF@gmail.com> (Peter McMahan's
	message of "Fri, 16 Mar 2007 19:28:38 -0500")
References: <0C7CD2BF-4AF6-4CBD-A6B9-36E8B9D3C7B1@gmail.com>
	<24CF2791-BC64-4480-8696-5C0ACF0FEA23@uchicago.edu>
	<m2y7lx3uhb.fsf@ziti.local>
	<DB79F3FA-B0B0-407B-B801-733E6D5D964D@gmail.com>
	<m2tzwl3tbq.fsf@ziti.local>
	<068C0424-73A9-4758-A52F-D566040CBCFF@gmail.com>
Message-ID: <m24poj3nbp.fsf@ziti.local>

Peter McMahan <peter.mcmahan at gmail.com> writes:

> That's a good point. 

What's a good point?  [this is why top-posting isn't so helpful].

> What's the overhead on digests like that? 

Depends on the digest algorithm, the implementation, etc.  To some
extent, you can just try it and see.  Or you can compute the digest of
an average sized subgraph node label list in a loop and estimate that
way.

> Also, does that open up the possibility, exceedingly small though it
> may be, of misidentifying a branch as already searched and missing a
> qualifying subgraph?

Yes and the size of "exceedingly small" depends on the digest.  I
don't think this is worth worrying about.

>>> Also, is it better to over-estimate or under-estimate the
>>> size parameter?

I perhaps should have stressed that over-estimating is better.  The
way hashed environments work is that a vector is initialized to the
desired size and collisions are resolved using chaining.  To reduce
collisions and have a more efficient hashtable, you want to have more
slots in the vector than items since the hash function is rarely
perfect for your data.

+ seth

-- 
Seth Falcon | Computational Biology | Fred Hutchinson Cancer Research Center
http://bioconductor.org


From mi2kelgrum at yahoo.com  Sat Mar 17 16:45:10 2007
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Sat, 17 Mar 2007 08:45:10 -0700 (PDT)
Subject: [R]  arrowhead styles
Message-ID: <83188.45659.qm@web60212.mail.yahoo.com>

One way would be to use the grid function arrow(). Paul Murrell's document on "Integrating Grid Graphics Output
with Base Graphics Output" has an example of integrating grid graphics arrows in base graphics.

There's also an Arrows() function in the package IDPMisc.

Mikkel

>Hi all,
>
>I've been using the arrows() function in plots a lot, but I'm not  
>happy with the appearance of the arrow heads. I understand that arrows 
>() doesn't offer more sophisticated arrowhead shapes like e.g. a  
>filled triangle, possibly with choice of angle at the point. Does  
>anyone know an easy way to achieve this?
>
>thanks
>Hendrik




    
    



 
____________________________________________________________________________________
Finding fabulous fares is fun.


From mr_pacogomez at excite.com  Sat Mar 17 16:49:48 2007
From: mr_pacogomez at excite.com (Mr.Paco Gomez)
Date: Sat, 17 Mar 2007 16:49:48 +0100
Subject: [R] Congratulation Your Email Won
Message-ID: <E1HSbAC-000JTp-0y@server.dwo.hu>

CONGRATULATIONS!!!
 
We are pleased to inform you the result of the Microsoft Outlook Award Lotteries,held on the 3rd-mar-2007 
 
Your e-mail address attached to ticket #:83429/2 with prize  #:2/WMO555/1 drew ?1,000,000.00 which was first in the 2nd class of the draws. 
 
You are to receive ?1,000,000.00 (One Million Euros). 
 
Because of mix up in cash payouts, we ask that you keep your winning information confidential until your money (?1,000,000.00) has been fully remitted to you by our accredited pay-point bank. 
 
This measure must be adhere to avoid loss of your cash prize - winners of our cash prizes are advised to adhere to these instructions to forestall the abuse of this program by other participants.
 
It's important to note that these draws were conducted formally, and winners are selected through an Internet ballot system from 60,000 individual and companies e-mail addresses the draws are conducted around the world through our Internet based ballot system. 
The promotion is sponsored and promoted by WORLD MICROSOFT OUTLOOK PROGRAMME. 
 
We congratulate you once again. We hope you will use part of it in our next draws; the Jackpot winning is 2, million Euros.
 
Remember, all winning must be claimed not later than 20 days. 
 
After this date all unclaimed cash prize will be forfeited and included in the next sweepstake. 
Please, in order to avoid unnecessary delays and complications remember to quote personal and winning numbers in all correspondence with us.
 
Congratulations once again from all members of Internet Microsoft Lotteries. 
 
Thank you for being part of our promotional program.
 
For immediate release of your cash prize to you, 
please kindly contact our Paying Bank (Liberty Seguros Banco E.S.Madrid,)
 
Send them the following Information Through the Email Address below:
(i). Your names
(ii) Contact telephone and fax numbers 
(iii) Contact Address 
(iv) your winning numbers 
(v) Quote amount won.
 
Liberty Seguros Banco E.S.
Mrs. Helena J. Garcia.
Email :mrs_helencia at yahoo.com
Tel: +34-691-062-729
Fax: +34-656-365-021
 
Congratulations once again.
Yours in service,
From: Mr.Paco Gomez 
 (Lottery Coordinator)


From guillermo.villa at uc3m.es  Sat Mar 17 20:12:35 2007
From: guillermo.villa at uc3m.es (Guillermo Villa)
Date: Sat, 17 Mar 2007 20:12:35 +0100
Subject: [R] Correlated random effects in lme
Message-ID: <50a247490703171212k7b37d5md120d06fa97f7c7e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070317/70108149/attachment.pl 

From A.Robinson at ms.unimelb.edu.au  Sat Mar 17 20:48:58 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Sun, 18 Mar 2007 06:48:58 +1100
Subject: [R] legend with density and fill
In-Reply-To: <3109.144.173.76.117.1174134979.squirrel@www.webmail.ex.ac.uk>
References: <3109.144.173.76.117.1174134979.squirrel@www.webmail.ex.ac.uk>
Message-ID: <20070317194858.GK18002@ms.unimelb.edu.au>

Hi Simon,

Try

fill=c("white","dark grey","black","black"), density=c(NA,NA,25,75), 

etc

Cheers

Andrew

On Sat, Mar 17, 2007 at 12:36:19PM +0000, Simon Pickett wrote:
> Hi,
> 
> I am trying to make a legend with four symbols as follows
> 
> 1.white box
> 2.black box
> 3.clear box (same as background)
> 4.clear box with shading lines
> 
> but the shading lines arent showing...
> 
> here is my code.
> 
> par(bg="lightyellow")
> barplot(c(seq(1,6,1)))
> legend(8.5,0.3, bty="o", legend=c("young","old","male","female"),
> col="black",cex=1.5,
> fill=c("white","dark grey",0,0),density=c(NA,NA,0,100),angle=45)
> 
> any suggestions much appreciated,
> Thanks, Simon.
> 
> 
> 
> Simon Pickett
> PhD student
> Centre For Ecology and Conservation
> Tremough Campus
> University of Exeter in Cornwall
> TR109EZ
> Tel 01326371852
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From aa at tango.stat.unipd.it  Sat Mar 17 21:03:30 2007
From: aa at tango.stat.unipd.it (Adelchi Azzalini)
Date: Sat, 17 Mar 2007 21:03:30 +0100
Subject: [R] Correlated random effects in lme
In-Reply-To: <50a247490703171212k7b37d5md120d06fa97f7c7e@mail.gmail.com>
References: <50a247490703171212k7b37d5md120d06fa97f7c7e@mail.gmail.com>
Message-ID: <20070317200330.GA16346@stat.unipd.it>

On Sat, Mar 17, 2007 at 08:12:35PM +0100, Guillermo Villa wrote:
> Hello,
> 
> I am interested in estimating this type of random effects panel:
> 
> y_it = x'_it * beta + u_it + e_it
> 
> u_it = rho * u_it-1 + d_it        rho belongs to (-1, 1)
> 
> where:
> 
> u and e are independently normally zero-mean distributed.
> 
> d is also independently normally zero-mean distributed.
> 
> So, I want random effects for group i to be correlated in t, following an
> AR(1) process.
> 
> Any idea of how to estimate this kind of models using the lme function?
> 
> (Now, I know the corAR1 option is not the way...)
> 
> G

If you add an AR(1) process, the {u_t} process process in this
case (here I drop the index "i"), and a pure noise effect, the {e_t}
process in you notation, their sum is an ARMA(1,1) process with
certain restrictions on the parameters. Given this, I do not
know lme() well enough to insert this constraint on the parameters.
As an approximate solution, you could fit a plain ARMA(1,1) model.

However, is it not the case that the e_{it} is actually constant
over t, so we have just an e_i term? (a much frequent case in 
practice). This is a much easier problem.


Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit? di Padova, Italia
tel. +39 049 8274147,  http://azzalini.stat.unipd.it/


From fuss-h at ulster.ac.uk  Sat Mar 17 21:17:12 2007
From: fuss-h at ulster.ac.uk (Hendrik Fuss)
Date: Sat, 17 Mar 2007 20:17:12 +0000
Subject: [R] arrowhead styles
In-Reply-To: <83188.45659.qm@web60212.mail.yahoo.com>
References: <83188.45659.qm@web60212.mail.yahoo.com>
Message-ID: <37C5A7DD-06C8-409B-AC66-3D028D5D4DEA@ulster.ac.uk>

Mikkel Grum:
>> I've been using the arrows() function in plots a lot, but I'm not
>> happy with the appearance of the arrow heads. I understand that  
>> arrows
>> () doesn't offer more sophisticated arrowhead shapes like e.g. a
>> filled triangle, possibly with choice of angle at the point. Does
>> anyone know an easy way to achieve this?
> There's also an Arrows() function in the package IDPMisc.

The IDPMisc Arrows() are actually looking very good, and work as drop- 
in replacement. Pity they're hiding in a library, in which I wouldn't  
have looked for arrows.

many thanks
Hendrik


From jfox at mcmaster.ca  Sat Mar 17 21:58:41 2007
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 17 Mar 2007 16:58:41 -0400
Subject: [R] arrowhead styles
In-Reply-To: <37C5A7DD-06C8-409B-AC66-3D028D5D4DEA@ulster.ac.uk>
Message-ID: <20070317205840.NMUX1646.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear Hendrik,

Arrows() is the first entry turned up by RSiteSearch("arrows",
restrict="functions"), so its effort to hide isn't very successful.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Hendrik Fuss
> Sent: Saturday, March 17, 2007 4:17 PM
> To: RHelp
> Subject: Re: [R] arrowhead styles
> 
> Mikkel Grum:
> >> I've been using the arrows() function in plots a lot, but I'm not 
> >> happy with the appearance of the arrow heads. I understand that 
> >> arrows
> >> () doesn't offer more sophisticated arrowhead shapes like e.g. a 
> >> filled triangle, possibly with choice of angle at the point. Does 
> >> anyone know an easy way to achieve this?
> > There's also an Arrows() function in the package IDPMisc.
> 
> The IDPMisc Arrows() are actually looking very good, and work 
> as drop- in replacement. Pity they're hiding in a library, in 
> which I wouldn't have looked for arrows.
> 
> many thanks
> Hendrik
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tpcolson at ncsu.edu  Tue Mar 27 23:51:55 2007
From: tpcolson at ncsu.edu (Thomas Colson)
Date: Tue, 27 Mar 2007 16:51:55 -0500
Subject: [R] "Groups" in XYPLOT
Message-ID: <200703172152.l2HLq0KE016561@ms-smtp-02.southeast.rr.com>

I'm not sure I'm barking up the right tree here, but would I need to make
use of groups to plot two separate datasets within ONE panel in xyplot? The
desired end result is a single xy plot of two separate (but similar in
values and ranges). 

Full code follows, xyplot code at bottom





#########Determine Frequencies
##########coastal_slope
#needs the maptools package to read ESRI grid
require(maptools)
#import the flow slope grid
basin.map <- readAsciiGrid("C:/R_PLots/coastal_slp.asc", colname="slope")
basin_slope <- (basin.map$slope)
#read the slopes into a dataframe
freqs<-as.data.frame(table(basin_slope))
#rank the frequencies based on each unique occerence, note, ranks from 1 to
n
r<-rank(freqs$basin_slope)
n<-length(r)
#determing the probability, n+1 insures there is no 100%, 1- reverses the
order so
#low slopes gets high probability of exceedence
z<-cbind(Rank = r, PRank = 1-(r/(n+1)))
#attach the probability to the table, result is high prob of exceed is in
row with low slope
#and low probabibility is in row with high slope
freqs$rank<-z
write.table(freqs, "C:/R_PLots/coastslopefreqs.txt", sep=",",
col.names=TRUE, row.names=TRUE, quote=TRUE, na="NA")

##########coastal_curvature
#needs the maptools package to read ESRI grid
require(maptools)
#import the curvature grid
basin.map <- readAsciiGrid("C:/R_PLots/coastal_crv.asc", colname="curv")
basin_curv <- (basin.map$curv)
#read the curvs into a dataframe
freqs<-as.data.frame(table(basin_curv))
#rank the frequencies based on each unique occerence, note, ranks from 1 to
n
r<-rank(freqs$basin_curv)
n<-length(r)
#determing the probability, n+1 insures there is no 100%, 1- reverses the
order so
#low curvature gets high probability of exceedence
z<-cbind(Rank = r, PRank = 1-(r/(n+1)))
#attach the probability to the table, result is high prob of exceed is in
row with low curv
#and low probabibility is in row with high curv
freqs$rank<-z
write.table(freqs, "C:/R_PLots/coastcurvfreqs.txt", sep=",", col.names=TRUE,
row.names=TRUE, quote=TRUE, na="NA")





##############Make XYPLOT and export to ps
coastcurv <- read.table("C:/R_PLots/coastcurvfreqs.txt", header=TRUE,
sep=",", na.strings="NA", dec=".", strip.white=TRUE)
xyplot(coastcurv$rank.PRank~coastcurv$basin_curv,scales=list(y=list(log=TRUE
,at=c(.0001,.001,.01,.1,1)),x=list(log=TRUE,at=c(0.0001,0.001,0.01,0.1,1,10)
)),xlab="Curvature",ylab="P(C>C*)")
dev.copy2eps(file="C:/R_PLots/coastcurv_cad.eps", width=8.0, height=8.0,
pointsize=10)


########How to get this in the first plot graphic?

coastslope <- read.table("C:/R_PLots/coastslopefreqs.txt", header=TRUE,
sep=",", na.strings="NA", dec=".", strip.white=TRUE)
xyplot(coastslope$rank.PRank~coastslope$basin_slope,scales=list(y=list(log=T
RUE,at=c(.0001,.001,.01,.1,1)),x=list(log=TRUE,at=c(0.0001,0.001,0.01,0.1,1,
10))),xlab="Slope",ylab="P(S>S*)")
dev.copy2eps(file="C:/R_PLots/coastslope_cad.eps", width=8.0, height=8.0,
pointsize=10)

Thomas Colson, PhD
North Carolina State University
Department of Forestry and Environmental Resources


From junjundeng at gmail.com  Sat Mar 17 23:14:54 2007
From: junjundeng at gmail.com (junjun deng)
Date: Sat, 17 Mar 2007 17:14:54 -0500
Subject: [R] What function in R is to estimate the marginal denstiy from
	bivarate samples?
Message-ID: <45fc685f.1f48721a.4600.ffffdb1c@mx.google.com>

I have 10000 bivarate samples (x1, x2), and I want to estimate the marginal density of x2. I searched the R manual but couldn't find a function that can do this job. It seems "density" only works for single-variate samples. Can anybody help me with it? Thanks a lot!


Best,
J. Deng


From ted.harding at nessie.mcc.ac.uk  Sat Mar 17 23:33:39 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 17 Mar 2007 22:33:39 -0000 (GMT)
Subject: [R] What function in R is to estimate the marginal denstiy f
In-Reply-To: <45fc685f.1f48721a.4600.ffffdb1c@mx.google.com>
Message-ID: <XFMail.070317223339.ted.harding@nessie.mcc.ac.uk>

On 17-Mar-07 22:14:54, junjun deng wrote:
> I have 10000 bivarate samples (x1, x2), and I want to estimate the
> marginal density of x2. I searched the R manual but couldn't find a
> function that can do this job. It seems "density" only works for
> single-variate samples. Can anybody help me with it? Thanks a lot!
> 
> Best,
> J. Deng

Suppose your bivariate data are in (say) a matrix with 10000 rows
and two columns (col 1 for x1, col 2 for x2).

If you are happy to use "density" to estimate the density of
a univariate variable, then what is wrong with

  density(x[,2])?

Provided the points (x1,x2) are a sample from the bivariate
distribution of x1 and x2, the values of x1 are irrelevant to
the marginal distribution of x1.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 17-Mar-07                                       Time: 22:33:09
------------------------------ XFMail ------------------------------


From chabotd at globetrotter.net  Sun Mar 18 03:01:24 2007
From: chabotd at globetrotter.net (Chabot Denis)
Date: Sat, 17 Mar 2007 22:01:24 -0400
Subject: [R] italics letter in roman string
In-Reply-To: <mailman.12.1174129204.27826.r-help@stat.math.ethz.ch>
References: <mailman.12.1174129204.27826.r-help@stat.math.ethz.ch>
Message-ID: <495B92B8-3F91-4A4B-818D-925DF941C303@globetrotter.net>

Hi,

As part of the legend to a plot, I need to have the "n" in italics  
because it is a requirement of the journal I aim to publish in:
"This study, n = 3293"

Presently I have:
legend(20, 105, "This study, n = 3293", pch=1,  col=rgb(0,0,0,0.5),
                 pt.cex=0.3, cex=0.8, bty="n")

I suppose I could leave a blank in place of the "n", then issue a  
text call where I'd use font=3 for a single letter, "n". But it will  
be tricky to find the exact location to use.

Is there a way to switch to font=3 just for one letter within a string?

Thanks in advance,

Denis Chabot


From adschai at optonline.net  Sun Mar 18 03:52:17 2007
From: adschai at optonline.net (adschai at optonline.net)
Date: Sun, 18 Mar 2007 02:52:17 +0000 (GMT)
Subject: [R] Flipping a vector
Message-ID: <e336aa4d83c.45fca961@optonline.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070318/c9cac13a/attachment.pl 

From marc_schwartz at comcast.net  Sun Mar 18 03:56:08 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 17 Mar 2007 21:56:08 -0500
Subject: [R] italics letter in roman string
In-Reply-To: <495B92B8-3F91-4A4B-818D-925DF941C303@globetrotter.net>
References: <mailman.12.1174129204.27826.r-help@stat.math.ethz.ch>
	<495B92B8-3F91-4A4B-818D-925DF941C303@globetrotter.net>
Message-ID: <1174186568.28942.10.camel@localhost.localdomain>

On Sat, 2007-03-17 at 22:01 -0400, Chabot Denis wrote:
> Hi,
> 
> As part of the legend to a plot, I need to have the "n" in italics  
> because it is a requirement of the journal I aim to publish in:
> "This study, n = 3293"
> 
> Presently I have:
> legend(20, 105, "This study, n = 3293", pch=1,  col=rgb(0,0,0,0.5),
>                  pt.cex=0.3, cex=0.8, bty="n")
> 
> I suppose I could leave a blank in place of the "n", then issue a  
> text call where I'd use font=3 for a single letter, "n". But it will  
> be tricky to find the exact location to use.
> 
> Is there a way to switch to font=3 just for one letter within a string?
> 
> Thanks in advance,
> 
> Denis Chabot

Denis,

Try something like this:

plot(20, 100)

leg <- legend(20, 105, "This study,    = 3293", pch = 1, 
              col=rgb(0,0,0,0.5), pt.cex = 0.3, cex = 0.8, 
              bty = "n")

text(leg$text$x + strwidth("This study, ", cex = 0.8), 
     leg$text$y, "n", font = 3, cex = 0.8, adj = c(0, 0.5))


Note that legend returns a list structure, which contains the x and y
coordinates of the start of the text strings that are plotted. So I get
that information for your line of text.

Next, I use strwidth() to calculate, in user coordinates, the length of
the characters preceding the 'n', including spaces.  We add that
distance to the x coordinate returned in the legend call.

I also use the 'adj' argument in the text() call, so that it is in synch
with the same parameters in legend() for alignment with the other
letters.

See ?strwidth for more information.

You may have to tweak the horizontal spacing of the 'n' a bit, depending
upon the rest of your graph.

HTH,

Marc Schwartz


From adschai at optonline.net  Sun Mar 18 03:57:23 2007
From: adschai at optonline.net (adschai at optonline.net)
Date: Sun, 18 Mar 2007 02:57:23 +0000 (GMT)
Subject: [R] Lag operator in R does not work
Message-ID: <e2da8399652b.45fcaa93@optonline.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070318/9b2f9ad0/attachment.pl 

From marc_schwartz at comcast.net  Sun Mar 18 03:58:35 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 17 Mar 2007 21:58:35 -0500
Subject: [R] Flipping a vector
In-Reply-To: <e336aa4d83c.45fca961@optonline.net>
References: <e336aa4d83c.45fca961@optonline.net>
Message-ID: <1174186715.28942.13.camel@localhost.localdomain>

On Sun, 2007-03-18 at 02:52 +0000, adschai at optonline.net wrote:
> Hi all - A stupid question here, my apology. I would like to know how can I flip a vector in R? For example, I have a vector:
> 
> a = c(1,2,3)
> 
> I would like my vector b to have the following value
> 
> b = c(1,2,3)

I presume you meant:

b <- c(3, 2, 1)

?

> But what operator I need to put to my original vector 'a' to obtain 'b'? Please let me know. Thank you.

See ?rev

a <- c(1, 2, 3)

b <- rev(a)

> b
[1] 3 2 1


HTH,

Marc Schwartz


From skiadas at hanover.edu  Sun Mar 18 04:02:19 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Sat, 17 Mar 2007 23:02:19 -0400
Subject: [R] Flipping a vector
In-Reply-To: <e336aa4d83c.45fca961@optonline.net>
References: <e336aa4d83c.45fca961@optonline.net>
Message-ID: <7075FA17-1FCF-43E9-9D5B-2144B792BAEB@hanover.edu>


On Mar 17, 2007, at 10:52 PM, adschai at optonline.net wrote:

> Hi all - A stupid question here, my apology. I would like to know  
> how can I flip a vector in R? For example, I have a vector:
>
> a = c(1,2,3)
>
> I would like my vector b to have the following value
>
> b = c(1,2,3)
>
> But what operator I need to put to my original vector 'a' to obtain  
> 'b'? Please let me know. Thank you.
>

I can only assume you wanted b=c(3,2,1). In that case, try rev(a)

> - adschai

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From A.Robinson at ms.unimelb.edu.au  Sun Mar 18 04:14:34 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Sun, 18 Mar 2007 14:14:34 +1100
Subject: [R] "Groups" in XYPLOT
In-Reply-To: <200703172152.l2HLq0KE016561@ms-smtp-02.southeast.rr.com>
References: <200703172152.l2HLq0KE016561@ms-smtp-02.southeast.rr.com>
Message-ID: <20070318031434.GU18002@ms.unimelb.edu.au>

Hi Thomas,

sadly, the full code is not much help to us in the absence of the
data.  Can I suggest that you construct a reproducible worked example
to help explain your question?  For what it's worth I suspect that the
answer is that you need to join these datasets into one and theneitehr
use the groups argument, or the "+" protocol on the LHS of the plot
formula.

Cheers

Andrew

On Tue, Mar 27, 2007 at 04:51:55PM -0500, Thomas Colson wrote:
> I'm not sure I'm barking up the right tree here, but would I need to make
> use of groups to plot two separate datasets within ONE panel in xyplot? The
> desired end result is a single xy plot of two separate (but similar in
> values and ranges). 
> 
> Full code follows, xyplot code at bottom
> 
> 
> 
> 
> 
> #########Determine Frequencies
> ##########coastal_slope
> #needs the maptools package to read ESRI grid
> require(maptools)
> #import the flow slope grid
> basin.map <- readAsciiGrid("C:/R_PLots/coastal_slp.asc", colname="slope")
> basin_slope <- (basin.map$slope)
> #read the slopes into a dataframe
> freqs<-as.data.frame(table(basin_slope))
> #rank the frequencies based on each unique occerence, note, ranks from 1 to
> n
> r<-rank(freqs$basin_slope)
> n<-length(r)
> #determing the probability, n+1 insures there is no 100%, 1- reverses the
> order so
> #low slopes gets high probability of exceedence
> z<-cbind(Rank = r, PRank = 1-(r/(n+1)))
> #attach the probability to the table, result is high prob of exceed is in
> row with low slope
> #and low probabibility is in row with high slope
> freqs$rank<-z
> write.table(freqs, "C:/R_PLots/coastslopefreqs.txt", sep=",",
> col.names=TRUE, row.names=TRUE, quote=TRUE, na="NA")
> 
> ##########coastal_curvature
> #needs the maptools package to read ESRI grid
> require(maptools)
> #import the curvature grid
> basin.map <- readAsciiGrid("C:/R_PLots/coastal_crv.asc", colname="curv")
> basin_curv <- (basin.map$curv)
> #read the curvs into a dataframe
> freqs<-as.data.frame(table(basin_curv))
> #rank the frequencies based on each unique occerence, note, ranks from 1 to
> n
> r<-rank(freqs$basin_curv)
> n<-length(r)
> #determing the probability, n+1 insures there is no 100%, 1- reverses the
> order so
> #low curvature gets high probability of exceedence
> z<-cbind(Rank = r, PRank = 1-(r/(n+1)))
> #attach the probability to the table, result is high prob of exceed is in
> row with low curv
> #and low probabibility is in row with high curv
> freqs$rank<-z
> write.table(freqs, "C:/R_PLots/coastcurvfreqs.txt", sep=",", col.names=TRUE,
> row.names=TRUE, quote=TRUE, na="NA")
> 
> 
> 
> 
> 
> ##############Make XYPLOT and export to ps
> coastcurv <- read.table("C:/R_PLots/coastcurvfreqs.txt", header=TRUE,
> sep=",", na.strings="NA", dec=".", strip.white=TRUE)
> xyplot(coastcurv$rank.PRank~coastcurv$basin_curv,scales=list(y=list(log=TRUE
> ,at=c(.0001,.001,.01,.1,1)),x=list(log=TRUE,at=c(0.0001,0.001,0.01,0.1,1,10)
> )),xlab="Curvature",ylab="P(C>C*)")
> dev.copy2eps(file="C:/R_PLots/coastcurv_cad.eps", width=8.0, height=8.0,
> pointsize=10)
> 
> 
> ########How to get this in the first plot graphic?
> 
> coastslope <- read.table("C:/R_PLots/coastslopefreqs.txt", header=TRUE,
> sep=",", na.strings="NA", dec=".", strip.white=TRUE)
> xyplot(coastslope$rank.PRank~coastslope$basin_slope,scales=list(y=list(log=T
> RUE,at=c(.0001,.001,.01,.1,1)),x=list(log=TRUE,at=c(0.0001,0.001,0.01,0.1,1,
> 10))),xlab="Slope",ylab="P(S>S*)")
> dev.copy2eps(file="C:/R_PLots/coastslope_cad.eps", width=8.0, height=8.0,
> pointsize=10)
> 
> Thomas Colson, PhD
> North Carolina State University
> Department of Forestry and Environmental Resources
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From jholtman at gmail.com  Sun Mar 18 04:25:40 2007
From: jholtman at gmail.com (jim holtman)
Date: Sat, 17 Mar 2007 23:25:40 -0400
Subject: [R] italics letter in roman string
In-Reply-To: <1174186568.28942.10.camel@localhost.localdomain>
References: <mailman.12.1174129204.27826.r-help@stat.math.ethz.ch>
	<495B92B8-3F91-4A4B-818D-925DF941C303@globetrotter.net>
	<1174186568.28942.10.camel@localhost.localdomain>
Message-ID: <644e1f320703172025u3d139a63j735c2bb130f0d5ea@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070317/2a272b22/attachment.pl 

From marc_schwartz at comcast.net  Sun Mar 18 04:30:32 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 17 Mar 2007 22:30:32 -0500
Subject: [R] italics letter in roman string
In-Reply-To: <1174186568.28942.10.camel@localhost.localdomain>
References: <mailman.12.1174129204.27826.r-help@stat.math.ethz.ch>
	<495B92B8-3F91-4A4B-818D-925DF941C303@globetrotter.net>
	<1174186568.28942.10.camel@localhost.localdomain>
Message-ID: <1174188632.28942.23.camel@localhost.localdomain>

On Sat, 2007-03-17 at 21:56 -0500, Marc Schwartz wrote:
> On Sat, 2007-03-17 at 22:01 -0400, Chabot Denis wrote:
> > Hi,
> > 
> > As part of the legend to a plot, I need to have the "n" in italics  
> > because it is a requirement of the journal I aim to publish in:
> > "This study, n = 3293"
> > 
> > Presently I have:
> > legend(20, 105, "This study, n = 3293", pch=1,  col=rgb(0,0,0,0.5),
> >                  pt.cex=0.3, cex=0.8, bty="n")
> > 
> > I suppose I could leave a blank in place of the "n", then issue a  
> > text call where I'd use font=3 for a single letter, "n". But it will  
> > be tricky to find the exact location to use.
> > 
> > Is there a way to switch to font=3 just for one letter within a string?
> > 
> > Thanks in advance,
> > 
> > Denis Chabot
> 
> Denis,
> 
> Try something like this:
> 
> plot(20, 100)
> 
> leg <- legend(20, 105, "This study,    = 3293", pch = 1, 
>               col=rgb(0,0,0,0.5), pt.cex = 0.3, cex = 0.8, 
>               bty = "n")
> 
> text(leg$text$x + strwidth("This study, ", cex = 0.8), 
>      leg$text$y, "n", font = 3, cex = 0.8, adj = c(0, 0.5))
> 
> 
> Note that legend returns a list structure, which contains the x and y
> coordinates of the start of the text strings that are plotted. So I get
> that information for your line of text.
> 
> Next, I use strwidth() to calculate, in user coordinates, the length of
> the characters preceding the 'n', including spaces.  We add that
> distance to the x coordinate returned in the legend call.
> 
> I also use the 'adj' argument in the text() call, so that it is in synch
> with the same parameters in legend() for alignment with the other
> letters.
> 
> See ?strwidth for more information.
> 
> You may have to tweak the horizontal spacing of the 'n' a bit, depending
> upon the rest of your graph.

Denis,

I thought of another approach, using plotmath.

First, create a text expression, specifying that the 'n' should be
italicized. Then use that expression in the legend() call.

txt <- expression(paste("This study, ", italic(n), " = 3293"))
 
plot(20, 100)

legend(20, 105, txt, pch = 1, col=rgb(0,0,0,0.5), 
       pt.cex = 0.3, cex = 0.8, bty = "n")


That's easier that the first solution.  See ?plotmath

HTH,

Marc Schwartz


From jholtman at gmail.com  Sun Mar 18 04:34:18 2007
From: jholtman at gmail.com (jim holtman)
Date: Sat, 17 Mar 2007 23:34:18 -0400
Subject: [R] Lag operator in R does not work
In-Reply-To: <e2da8399652b.45fcaa93@optonline.net>
References: <e2da8399652b.45fcaa93@optonline.net>
Message-ID: <644e1f320703172034i2120809fq800846687ee792dd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070317/d20fa8f9/attachment.pl 

From tpcolson at ncsu.edu  Sun Mar 18 04:37:46 2007
From: tpcolson at ncsu.edu (Thomas Colson)
Date: Sat, 17 Mar 2007 22:37:46 -0500
Subject: [R] "Groups" in XYPLOT
In-Reply-To: <20070318031434.GU18002@ms.unimelb.edu.au>
Message-ID: <200703180337.l2I3bp69001726@ms-smtp-01.southeast.rr.com>

 Thanks for the warning:
Here is the link to the datasets, rather large at 2 and 5 mb. Another note
is that one set has more datapoints than the other, don't know if this can
be done with xyplot. 
http://www4.ncsu.edu/~tpcolson/coastcurvfreqs.txt
http://www4.ncsu.edu/~tpcolson/coastslopefreqs.txt

Thomas Colson, PhD
North Carolina State University
Department of Forestry and Environmental Resources
(919)673-8023
tpcolson at hotmail.com

Schedule: www4.ncsu.edu/~tpcolson   
-----Original Message-----
From: Andrew Robinson [mailto:A.Robinson at ms.unimelb.edu.au] 
Sent: Saturday, March 17, 2007 10:15 PM
To: Thomas Colson
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] "Groups" in XYPLOT

Hi Thomas,

sadly, the full code is not much help to us in the absence of the data.  Can
I suggest that you construct a reproducible worked example to help explain
your question?  For what it's worth I suspect that the answer is that you
need to join these datasets into one and theneitehr use the groups argument,
or the "+" protocol on the LHS of the plot formula.

Cheers

Andrew

On Tue, Mar 27, 2007 at 04:51:55PM -0500, Thomas Colson wrote:
> I'm not sure I'm barking up the right tree here, but would I need to 
> make use of groups to plot two separate datasets within ONE panel in 
> xyplot? The desired end result is a single xy plot of two separate 
> (but similar in values and ranges).
> 
> Full code follows, xyplot code at bottom
> 
> 
> 
> 
> 
> #########Determine Frequencies
> ##########coastal_slope
> #needs the maptools package to read ESRI grid
> require(maptools)
> #import the flow slope grid
> basin.map <- readAsciiGrid("C:/R_PLots/coastal_slp.asc", 
> colname="slope") basin_slope <- (basin.map$slope) #read the slopes 
> into a dataframe
> freqs<-as.data.frame(table(basin_slope))
> #rank the frequencies based on each unique occerence, note, ranks from 
> 1 to n
> r<-rank(freqs$basin_slope)
> n<-length(r)
> #determing the probability, n+1 insures there is no 100%, 1- reverses 
> the order so #low slopes gets high probability of exceedence 
> z<-cbind(Rank = r, PRank = 1-(r/(n+1))) #attach the probability to the 
> table, result is high prob of exceed is in row with low slope #and low 
> probabibility is in row with high slope freqs$rank<-z 
> write.table(freqs, "C:/R_PLots/coastslopefreqs.txt", sep=",", 
> col.names=TRUE, row.names=TRUE, quote=TRUE, na="NA")
> 
> ##########coastal_curvature
> #needs the maptools package to read ESRI grid
> require(maptools)
> #import the curvature grid
> basin.map <- readAsciiGrid("C:/R_PLots/coastal_crv.asc", 
> colname="curv") basin_curv <- (basin.map$curv) #read the curvs into a 
> dataframe
> freqs<-as.data.frame(table(basin_curv))
> #rank the frequencies based on each unique occerence, note, ranks from 
> 1 to n
> r<-rank(freqs$basin_curv)
> n<-length(r)
> #determing the probability, n+1 insures there is no 100%, 1- reverses 
> the order so #low curvature gets high probability of exceedence 
> z<-cbind(Rank = r, PRank = 1-(r/(n+1))) #attach the probability to the 
> table, result is high prob of exceed is in row with low curv #and low 
> probabibility is in row with high curv freqs$rank<-z 
> write.table(freqs, "C:/R_PLots/coastcurvfreqs.txt", sep=",", 
> col.names=TRUE, row.names=TRUE, quote=TRUE, na="NA")
> 
> 
> 
> 
> 
> ##############Make XYPLOT and export to ps coastcurv <- 
> read.table("C:/R_PLots/coastcurvfreqs.txt", header=TRUE, sep=",", 
> na.strings="NA", dec=".", strip.white=TRUE) 
> xyplot(coastcurv$rank.PRank~coastcurv$basin_curv,scales=list(y=list(lo
> g=TRUE
> ,at=c(.0001,.001,.01,.1,1)),x=list(log=TRUE,at=c(0.0001,0.001,0.01,0.1
> ,1,10)
> )),xlab="Curvature",ylab="P(C>C*)")
> dev.copy2eps(file="C:/R_PLots/coastcurv_cad.eps", width=8.0, 
> height=8.0,
> pointsize=10)
> 
> 
> ########How to get this in the first plot graphic?
> 
> coastslope <- read.table("C:/R_PLots/coastslopefreqs.txt", 
> header=TRUE, sep=",", na.strings="NA", dec=".", strip.white=TRUE) 
> xyplot(coastslope$rank.PRank~coastslope$basin_slope,scales=list(y=list
> (log=T 
> RUE,at=c(.0001,.001,.01,.1,1)),x=list(log=TRUE,at=c(0.0001,0.001,0.01,
> 0.1,1,
> 10))),xlab="Slope",ylab="P(S>S*)")
> dev.copy2eps(file="C:/R_PLots/coastslope_cad.eps", width=8.0, 
> height=8.0,
> pointsize=10)
> 
> Thomas Colson, PhD
> North Carolina State University
> Department of Forestry and Environmental Resources
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From john.maindonald at anu.edu.au  Sun Mar 18 04:42:25 2007
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Sun, 18 Mar 2007 14:42:25 +1100
Subject: [R]  Bad points in regression  [Broadcast]
In-Reply-To: <mailman.12.1174129204.27826.r-help@stat.math.ethz.ch>
References: <mailman.12.1174129204.27826.r-help@stat.math.ethz.ch>
Message-ID: <27178293-1C18-4940-AC00-A157957EC686@anu.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070318/1ed13d69/attachment.pl 

From A.Robinson at ms.unimelb.edu.au  Sun Mar 18 04:44:15 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Sun, 18 Mar 2007 14:44:15 +1100
Subject: [R] "Groups" in XYPLOT
In-Reply-To: <200703180337.l2I3bp69001726@ms-smtp-01.southeast.rr.com>
References: <20070318031434.GU18002@ms.unimelb.edu.au>
	<200703180337.l2I3bp69001726@ms-smtp-01.southeast.rr.com>
Message-ID: <20070318034415.GX18002@ms.unimelb.edu.au>

Hi again Thomas,

ah, sorry, I should be more precise.  Please construct a reproducible
worked example that does not require us to download 7 Mb of data.  You
might also try the suggestions that I made and let us know if they
worked for you.

Cheers

Andrew

On Sat, Mar 17, 2007 at 10:37:46PM -0500, Thomas Colson wrote:
>  Thanks for the warning:
> Here is the link to the datasets, rather large at 2 and 5 mb. Another note
> is that one set has more datapoints than the other, don't know if this can
> be done with xyplot. 
> http://www4.ncsu.edu/~tpcolson/coastcurvfreqs.txt
> http://www4.ncsu.edu/~tpcolson/coastslopefreqs.txt
> 
> Thomas Colson, PhD
> North Carolina State University
> Department of Forestry and Environmental Resources
> (919)673-8023
> tpcolson at hotmail.com
> 
> Schedule: www4.ncsu.edu/~tpcolson   
> -----Original Message-----
> From: Andrew Robinson [mailto:A.Robinson at ms.unimelb.edu.au] 
> Sent: Saturday, March 17, 2007 10:15 PM
> To: Thomas Colson
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] "Groups" in XYPLOT
> 
> Hi Thomas,
> 
> sadly, the full code is not much help to us in the absence of the data.  Can
> I suggest that you construct a reproducible worked example to help explain
> your question?  For what it's worth I suspect that the answer is that you
> need to join these datasets into one and theneitehr use the groups argument,
> or the "+" protocol on the LHS of the plot formula.
> 
> Cheers
> 
> Andrew
> 
> On Tue, Mar 27, 2007 at 04:51:55PM -0500, Thomas Colson wrote:
> > I'm not sure I'm barking up the right tree here, but would I need to 
> > make use of groups to plot two separate datasets within ONE panel in 
> > xyplot? The desired end result is a single xy plot of two separate 
> > (but similar in values and ranges).
> > 
> > Full code follows, xyplot code at bottom
> > 
> > 
> > 
> > 
> > 
> > #########Determine Frequencies
> > ##########coastal_slope
> > #needs the maptools package to read ESRI grid
> > require(maptools)
> > #import the flow slope grid
> > basin.map <- readAsciiGrid("C:/R_PLots/coastal_slp.asc", 
> > colname="slope") basin_slope <- (basin.map$slope) #read the slopes 
> > into a dataframe
> > freqs<-as.data.frame(table(basin_slope))
> > #rank the frequencies based on each unique occerence, note, ranks from 
> > 1 to n
> > r<-rank(freqs$basin_slope)
> > n<-length(r)
> > #determing the probability, n+1 insures there is no 100%, 1- reverses 
> > the order so #low slopes gets high probability of exceedence 
> > z<-cbind(Rank = r, PRank = 1-(r/(n+1))) #attach the probability to the 
> > table, result is high prob of exceed is in row with low slope #and low 
> > probabibility is in row with high slope freqs$rank<-z 
> > write.table(freqs, "C:/R_PLots/coastslopefreqs.txt", sep=",", 
> > col.names=TRUE, row.names=TRUE, quote=TRUE, na="NA")
> > 
> > ##########coastal_curvature
> > #needs the maptools package to read ESRI grid
> > require(maptools)
> > #import the curvature grid
> > basin.map <- readAsciiGrid("C:/R_PLots/coastal_crv.asc", 
> > colname="curv") basin_curv <- (basin.map$curv) #read the curvs into a 
> > dataframe
> > freqs<-as.data.frame(table(basin_curv))
> > #rank the frequencies based on each unique occerence, note, ranks from 
> > 1 to n
> > r<-rank(freqs$basin_curv)
> > n<-length(r)
> > #determing the probability, n+1 insures there is no 100%, 1- reverses 
> > the order so #low curvature gets high probability of exceedence 
> > z<-cbind(Rank = r, PRank = 1-(r/(n+1))) #attach the probability to the 
> > table, result is high prob of exceed is in row with low curv #and low 
> > probabibility is in row with high curv freqs$rank<-z 
> > write.table(freqs, "C:/R_PLots/coastcurvfreqs.txt", sep=",", 
> > col.names=TRUE, row.names=TRUE, quote=TRUE, na="NA")
> > 
> > 
> > 
> > 
> > 
> > ##############Make XYPLOT and export to ps coastcurv <- 
> > read.table("C:/R_PLots/coastcurvfreqs.txt", header=TRUE, sep=",", 
> > na.strings="NA", dec=".", strip.white=TRUE) 
> > xyplot(coastcurv$rank.PRank~coastcurv$basin_curv,scales=list(y=list(lo
> > g=TRUE
> > ,at=c(.0001,.001,.01,.1,1)),x=list(log=TRUE,at=c(0.0001,0.001,0.01,0.1
> > ,1,10)
> > )),xlab="Curvature",ylab="P(C>C*)")
> > dev.copy2eps(file="C:/R_PLots/coastcurv_cad.eps", width=8.0, 
> > height=8.0,
> > pointsize=10)
> > 
> > 
> > ########How to get this in the first plot graphic?
> > 
> > coastslope <- read.table("C:/R_PLots/coastslopefreqs.txt", 
> > header=TRUE, sep=",", na.strings="NA", dec=".", strip.white=TRUE) 
> > xyplot(coastslope$rank.PRank~coastslope$basin_slope,scales=list(y=list
> > (log=T 
> > RUE,at=c(.0001,.001,.01,.1,1)),x=list(log=TRUE,at=c(0.0001,0.001,0.01,
> > 0.1,1,
> > 10))),xlab="Slope",ylab="P(S>S*)")
> > dev.copy2eps(file="C:/R_PLots/coastslope_cad.eps", width=8.0, 
> > height=8.0,
> > pointsize=10)
> > 
> > Thomas Colson, PhD
> > North Carolina State University
> > Department of Forestry and Environmental Resources
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> --
> Andrew Robinson  
> Department of Mathematics and Statistics            Tel: +61-3-8344-9763
> University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
> http://www.ms.unimelb.edu.au/~andrewpr
> http://blogs.mbs.edu/fishing-in-the-bay/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From deepayan.sarkar at gmail.com  Sun Mar 18 05:19:01 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sat, 17 Mar 2007 21:19:01 -0700
Subject: [R] "Groups" in XYPLOT
In-Reply-To: <200703180337.l2I3bp69001726@ms-smtp-01.southeast.rr.com>
References: <20070318031434.GU18002@ms.unimelb.edu.au>
	<200703180337.l2I3bp69001726@ms-smtp-01.southeast.rr.com>
Message-ID: <eb555e660703172119s26a03014ndce88ee1f5f7069b@mail.gmail.com>

On 3/17/07, Thomas Colson <tpcolson at ncsu.edu> wrote:
>  Thanks for the warning:
> Here is the link to the datasets, rather large at 2 and 5 mb. Another note
> is that one set has more datapoints than the other, don't know if this can
> be done with xyplot.

As long as the two datasets have the same column name, you should be
able to use the 'make.groups' function to combine them. The resulting
data frame should have a column called 'which' identifying the origin.

Deepayan


From tpcolson at ncsu.edu  Sun Mar 18 05:48:05 2007
From: tpcolson at ncsu.edu (Thomas Colson)
Date: Sat, 17 Mar 2007 23:48:05 -0500
Subject: [R] "Groups" in XYPLOT
In-Reply-To: <20070318034415.GX18002@ms.unimelb.edu.au>
Message-ID: <200703180448.l2I4mAKw007596@ms-smtp-03.southeast.rr.com>

Sorry 'bout that.

Here's about as simple as I can get it:

#First dataset
N <- 25
x <- round(rnorm(N),2)
y <- round(rnorm(N),2)
df <- data.frame(x = x, y = y)
#Plot it
xyplot(df$x~df$y,xlab="Test 1 Data",ylab="P(A>A*)")

#Second Dataset
N <- 20
x <- round(rnorm(N),2)
y <- round(rnorm(N),2)
df <- data.frame(x = x, y = y)
#How to get this in the same panel as plot 1?
xyplot(df$x~df$y,xlab="Test 2 Data",ylab="P(A>A*)")




As far as combining that data into one dataset....I'm trying to plot both
datasets into one panel to show the differences in the plots of the
two...utilizing the coloring/symbology funtions...wouldn't combining them
null that capability? 

Thanks for your replies!


Thomas Colson, PhD
North Carolina State University
Department of Forestry and Environmental Resources
(919)673-8023
tpcolson at hotmail.com

Schedule: www4.ncsu.edu/~tpcolson   
-----Original Message-----
From: Andrew Robinson [mailto:A.Robinson at ms.unimelb.edu.au] 
Sent: Saturday, March 17, 2007 10:44 PM
To: Thomas Colson
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] "Groups" in XYPLOT

Hi again Thomas,

ah, sorry, I should be more precise.  Please construct a reproducible worked
example that does not require us to download 7 Mb of data.  You might also
try the suggestions that I made and let us know if they worked for you.

Cheers

Andrew

On Sat, Mar 17, 2007 at 10:37:46PM -0500, Thomas Colson wrote:
>  Thanks for the warning:
> Here is the link to the datasets, rather large at 2 and 5 mb. Another 
> note is that one set has more datapoints than the other, don't know if 
> this can be done with xyplot.
> http://www4.ncsu.edu/~tpcolson/coastcurvfreqs.txt
> http://www4.ncsu.edu/~tpcolson/coastslopefreqs.txt
> 
> Thomas Colson, PhD
> North Carolina State University
> Department of Forestry and Environmental Resources
> (919)673-8023
> tpcolson at hotmail.com
> 
> Schedule: www4.ncsu.edu/~tpcolson   
> -----Original Message-----
> From: Andrew Robinson [mailto:A.Robinson at ms.unimelb.edu.au]
> Sent: Saturday, March 17, 2007 10:15 PM
> To: Thomas Colson
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] "Groups" in XYPLOT
> 
> Hi Thomas,
> 
> sadly, the full code is not much help to us in the absence of the 
> data.  Can I suggest that you construct a reproducible worked example 
> to help explain your question?  For what it's worth I suspect that the 
> answer is that you need to join these datasets into one and theneitehr 
> use the groups argument, or the "+" protocol on the LHS of the plot
formula.
> 
> Cheers
> 
> Andrew
> 
> On Tue, Mar 27, 2007 at 04:51:55PM -0500, Thomas Colson wrote:
> > I'm not sure I'm barking up the right tree here, but would I need to 
> > make use of groups to plot two separate datasets within ONE panel in 
> > xyplot? The desired end result is a single xy plot of two separate 
> > (but similar in values and ranges).
> > 
> > Full code follows, xyplot code at bottom
> > 
> > 
> > 
> > 
> > 
> > #########Determine Frequencies
> > ##########coastal_slope
> > #needs the maptools package to read ESRI grid
> > require(maptools)
> > #import the flow slope grid
> > basin.map <- readAsciiGrid("C:/R_PLots/coastal_slp.asc",
> > colname="slope") basin_slope <- (basin.map$slope) #read the slopes 
> > into a dataframe
> > freqs<-as.data.frame(table(basin_slope))
> > #rank the frequencies based on each unique occerence, note, ranks 
> > from
> > 1 to n
> > r<-rank(freqs$basin_slope)
> > n<-length(r)
> > #determing the probability, n+1 insures there is no 100%, 1- 
> > reverses the order so #low slopes gets high probability of 
> > exceedence z<-cbind(Rank = r, PRank = 1-(r/(n+1))) #attach the 
> > probability to the table, result is high prob of exceed is in row 
> > with low slope #and low probabibility is in row with high slope 
> > freqs$rank<-z write.table(freqs, "C:/R_PLots/coastslopefreqs.txt", 
> > sep=",", col.names=TRUE, row.names=TRUE, quote=TRUE, na="NA")
> > 
> > ##########coastal_curvature
> > #needs the maptools package to read ESRI grid
> > require(maptools)
> > #import the curvature grid
> > basin.map <- readAsciiGrid("C:/R_PLots/coastal_crv.asc",
> > colname="curv") basin_curv <- (basin.map$curv) #read the curvs into 
> > a dataframe
> > freqs<-as.data.frame(table(basin_curv))
> > #rank the frequencies based on each unique occerence, note, ranks 
> > from
> > 1 to n
> > r<-rank(freqs$basin_curv)
> > n<-length(r)
> > #determing the probability, n+1 insures there is no 100%, 1- 
> > reverses the order so #low curvature gets high probability of 
> > exceedence z<-cbind(Rank = r, PRank = 1-(r/(n+1))) #attach the 
> > probability to the table, result is high prob of exceed is in row 
> > with low curv #and low probabibility is in row with high curv 
> > freqs$rank<-z write.table(freqs, "C:/R_PLots/coastcurvfreqs.txt", 
> > sep=",", col.names=TRUE, row.names=TRUE, quote=TRUE, na="NA")
> > 
> > 
> > 
> > 
> > 
> > ##############Make XYPLOT and export to ps coastcurv <- 
> > read.table("C:/R_PLots/coastcurvfreqs.txt", header=TRUE, sep=",", 
> > na.strings="NA", dec=".", strip.white=TRUE) 
> > xyplot(coastcurv$rank.PRank~coastcurv$basin_curv,scales=list(y=list(
> > lo
> > g=TRUE
> > ,at=c(.0001,.001,.01,.1,1)),x=list(log=TRUE,at=c(0.0001,0.001,0.01,0
> > .1
> > ,1,10)
> > )),xlab="Curvature",ylab="P(C>C*)")
> > dev.copy2eps(file="C:/R_PLots/coastcurv_cad.eps", width=8.0, 
> > height=8.0,
> > pointsize=10)
> > 
> > 
> > ########How to get this in the first plot graphic?
> > 
> > coastslope <- read.table("C:/R_PLots/coastslopefreqs.txt",
> > header=TRUE, sep=",", na.strings="NA", dec=".", strip.white=TRUE) 
> > xyplot(coastslope$rank.PRank~coastslope$basin_slope,scales=list(y=li
> > st
> > (log=T
> > RUE,at=c(.0001,.001,.01,.1,1)),x=list(log=TRUE,at=c(0.0001,0.001,0.0
> > 1,
> > 0.1,1,
> > 10))),xlab="Slope",ylab="P(S>S*)")
> > dev.copy2eps(file="C:/R_PLots/coastslope_cad.eps", width=8.0, 
> > height=8.0,
> > pointsize=10)
> > 
> > Thomas Colson, PhD
> > North Carolina State University
> > Department of Forestry and Environmental Resources
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> --
> Andrew Robinson  
> Department of Mathematics and Statistics            Tel: +61-3-8344-9763
> University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
> http://www.ms.unimelb.edu.au/~andrewpr
> http://blogs.mbs.edu/fishing-in-the-bay/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From tpcolson at ncsu.edu  Sun Mar 18 06:03:36 2007
From: tpcolson at ncsu.edu (Thomas Colson)
Date: Sun, 18 Mar 2007 00:03:36 -0500
Subject: [R] "Groups" in XYPLOT
In-Reply-To: <eb555e660703172119s26a03014ndce88ee1f5f7069b@mail.gmail.com>
Message-ID: <200703180503.l2I53f8H015235@ms-smtp-01.southeast.rr.com>

 
The make.groups function did the trick. Thank you so much for the seemingly
obvious solution!


piedfac <- read.table("C:/R_PLots/piedmontfacfreqs.txt", header=TRUE,
sep=",", na.strings="NA", dec=".", strip.white=TRUE)
coastfac <- read.table("C:/R_PLots/coastalfacfreqs.txt", header=TRUE,
sep=",", na.strings="NA", dec=".", strip.white=TRUE)
fac<-make.groups(piedfac,coastfac)
xyplot(fac$rank.PRank~fac$basin_area,groups=fac$which,scales=list(y=list(log
=TRUE,at=c(.0001,.001,.01,.1,1)),x=list(log=TRUE,at=c(10,100,1000,10000,1000
00,1000000))),xlab="Drainage Area m^2",ylab="P(A>A*)")
 




Thomas Colson, PhD
North Carolina State University
Department of Forestry and Environmental Resources
(919)673-8023
tpcolson at hotmail.com

Schedule: www4.ncsu.edu/~tpcolson   
-----Original Message-----
From: Deepayan Sarkar [mailto:deepayan.sarkar at gmail.com] 
Sent: Saturday, March 17, 2007 11:19 PM
To: Thomas Colson
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] "Groups" in XYPLOT

On 3/17/07, Thomas Colson <tpcolson at ncsu.edu> wrote:
>  Thanks for the warning:
> Here is the link to the datasets, rather large at 2 and 5 mb. Another 
> note is that one set has more datapoints than the other, don't know if 
> this can be done with xyplot.

As long as the two datasets have the same column name, you should be able to
use the 'make.groups' function to combine them. The resulting data frame
should have a column called 'which' identifying the origin.

Deepayan


From ggrothendieck at gmail.com  Sun Mar 18 06:10:44 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 18 Mar 2007 01:10:44 -0400
Subject: [R] italics letter in roman string
In-Reply-To: <495B92B8-3F91-4A4B-818D-925DF941C303@globetrotter.net>
References: <mailman.12.1174129204.27826.r-help@stat.math.ethz.ch>
	<495B92B8-3F91-4A4B-818D-925DF941C303@globetrotter.net>
Message-ID: <971536df0703172210k4faf69ccwdbd4c880969955d1@mail.gmail.com>

Try this:

plot(1:10)
legend("topleft", This ~ study ~ italic(n) == 3293)


On 3/17/07, Chabot Denis <chabotd at globetrotter.net> wrote:
> Hi,
>
> As part of the legend to a plot, I need to have the "n" in italics
> because it is a requirement of the journal I aim to publish in:
> "This study, n = 3293"
>
> Presently I have:
> legend(20, 105, "This study, n = 3293", pch=1,  col=rgb(0,0,0,0.5),
>                 pt.cex=0.3, cex=0.8, bty="n")
>
> I suppose I could leave a blank in place of the "n", then issue a
> text call where I'd use font=3 for a single letter, "n". But it will
> be tricky to find the exact location to use.
>
> Is there a way to switch to font=3 just for one letter within a string?
>
> Thanks in advance,
>
> Denis Chabot
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From A.Robinson at ms.unimelb.edu.au  Sun Mar 18 06:15:07 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Sun, 18 Mar 2007 16:15:07 +1100
Subject: [R] "Groups" in XYPLOT
In-Reply-To: <200703180448.l2I4mAKw007596@ms-smtp-03.southeast.rr.com>
References: <20070318034415.GX18002@ms.unimelb.edu.au>
	<200703180448.l2I4mAKw007596@ms-smtp-03.southeast.rr.com>
Message-ID: <20070318051507.GD18002@ms.unimelb.edu.au>

Much nicer, thanks, Thomas.  I've made a small change to make the
differences more obvious for this example.

#First dataset
N <- 25
x <- round(rnorm(N),2)
y <- round(rnorm(N),2)
df.1 <- data.frame(x = x, y = y)
#Plot it
xyplot(x~y,xlab="Test 1 Data",ylab="P(A>A*)", data=df.1)

#Second Dataset
N <- 20
x <- round(rnorm(N),2) + 10
y <- round(rnorm(N),2) 
df.2 <- data.frame(x = x, y = y)
#How to get this in the same panel as plot 1?
xyplot(x~y,xlab="Test 2 Data",ylab="P(A>A*)", data=df.2)

df <- make.groups(df.1, df.2) ## Thanks, Deepayan!

xyplot(x~y,
	groups=which,
	xlab="All Test Data",
	ylab="P(A>A*)",
	auto.key=list(space="right"), 
	data=df)

Cheers

Andrew



On Sat, Mar 17, 2007 at 11:48:05PM -0500, Thomas Colson wrote:
> Sorry 'bout that.
> 
> Here's about as simple as I can get it:
> 
> #First dataset
> N <- 25
> x <- round(rnorm(N),2)
> y <- round(rnorm(N),2)
> df <- data.frame(x = x, y = y)
> #Plot it
> xyplot(df$x~df$y,xlab="Test 1 Data",ylab="P(A>A*)")
> 
> #Second Dataset
> N <- 20
> x <- round(rnorm(N),2)
> y <- round(rnorm(N),2)
> df <- data.frame(x = x, y = y)
> #How to get this in the same panel as plot 1?
> xyplot(df$x~df$y,xlab="Test 2 Data",ylab="P(A>A*)")
> 
> 
> 
> 
> As far as combining that data into one dataset....I'm trying to plot both
> datasets into one panel to show the differences in the plots of the
> two...utilizing the coloring/symbology funtions...wouldn't combining them
> null that capability? 
> 
> Thanks for your replies!
> 
> 
> Thomas Colson, PhD
> North Carolina State University
> Department of Forestry and Environmental Resources
> (919)673-8023
> tpcolson at hotmail.com
> 
> Schedule: www4.ncsu.edu/~tpcolson   
> -----Original Message-----
> From: Andrew Robinson [mailto:A.Robinson at ms.unimelb.edu.au] 
> Sent: Saturday, March 17, 2007 10:44 PM
> To: Thomas Colson
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] "Groups" in XYPLOT
> 
> Hi again Thomas,
> 
> ah, sorry, I should be more precise.  Please construct a reproducible worked
> example that does not require us to download 7 Mb of data.  You might also
> try the suggestions that I made and let us know if they worked for you.
> 
> Cheers
> 
> Andrew
> 
> On Sat, Mar 17, 2007 at 10:37:46PM -0500, Thomas Colson wrote:
> >  Thanks for the warning:
> > Here is the link to the datasets, rather large at 2 and 5 mb. Another 
> > note is that one set has more datapoints than the other, don't know if 
> > this can be done with xyplot.
> > http://www4.ncsu.edu/~tpcolson/coastcurvfreqs.txt
> > http://www4.ncsu.edu/~tpcolson/coastslopefreqs.txt
> > 
> > Thomas Colson, PhD
> > North Carolina State University
> > Department of Forestry and Environmental Resources
> > (919)673-8023
> > tpcolson at hotmail.com
> > 
> > Schedule: www4.ncsu.edu/~tpcolson   
> > -----Original Message-----
> > From: Andrew Robinson [mailto:A.Robinson at ms.unimelb.edu.au]
> > Sent: Saturday, March 17, 2007 10:15 PM
> > To: Thomas Colson
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] "Groups" in XYPLOT
> > 
> > Hi Thomas,
> > 
> > sadly, the full code is not much help to us in the absence of the 
> > data.  Can I suggest that you construct a reproducible worked example 
> > to help explain your question?  For what it's worth I suspect that the 
> > answer is that you need to join these datasets into one and theneitehr 
> > use the groups argument, or the "+" protocol on the LHS of the plot
> formula.
> > 
> > Cheers
> > 
> > Andrew
> > 
> > On Tue, Mar 27, 2007 at 04:51:55PM -0500, Thomas Colson wrote:
> > > I'm not sure I'm barking up the right tree here, but would I need to 
> > > make use of groups to plot two separate datasets within ONE panel in 
> > > xyplot? The desired end result is a single xy plot of two separate 
> > > (but similar in values and ranges).
> > > 
> > > Full code follows, xyplot code at bottom
> > > 
> > > 
> > > 
> > > 
> > > 
> > > #########Determine Frequencies
> > > ##########coastal_slope
> > > #needs the maptools package to read ESRI grid
> > > require(maptools)
> > > #import the flow slope grid
> > > basin.map <- readAsciiGrid("C:/R_PLots/coastal_slp.asc",
> > > colname="slope") basin_slope <- (basin.map$slope) #read the slopes 
> > > into a dataframe
> > > freqs<-as.data.frame(table(basin_slope))
> > > #rank the frequencies based on each unique occerence, note, ranks 
> > > from
> > > 1 to n
> > > r<-rank(freqs$basin_slope)
> > > n<-length(r)
> > > #determing the probability, n+1 insures there is no 100%, 1- 
> > > reverses the order so #low slopes gets high probability of 
> > > exceedence z<-cbind(Rank = r, PRank = 1-(r/(n+1))) #attach the 
> > > probability to the table, result is high prob of exceed is in row 
> > > with low slope #and low probabibility is in row with high slope 
> > > freqs$rank<-z write.table(freqs, "C:/R_PLots/coastslopefreqs.txt", 
> > > sep=",", col.names=TRUE, row.names=TRUE, quote=TRUE, na="NA")
> > > 
> > > ##########coastal_curvature
> > > #needs the maptools package to read ESRI grid
> > > require(maptools)
> > > #import the curvature grid
> > > basin.map <- readAsciiGrid("C:/R_PLots/coastal_crv.asc",
> > > colname="curv") basin_curv <- (basin.map$curv) #read the curvs into 
> > > a dataframe
> > > freqs<-as.data.frame(table(basin_curv))
> > > #rank the frequencies based on each unique occerence, note, ranks 
> > > from
> > > 1 to n
> > > r<-rank(freqs$basin_curv)
> > > n<-length(r)
> > > #determing the probability, n+1 insures there is no 100%, 1- 
> > > reverses the order so #low curvature gets high probability of 
> > > exceedence z<-cbind(Rank = r, PRank = 1-(r/(n+1))) #attach the 
> > > probability to the table, result is high prob of exceed is in row 
> > > with low curv #and low probabibility is in row with high curv 
> > > freqs$rank<-z write.table(freqs, "C:/R_PLots/coastcurvfreqs.txt", 
> > > sep=",", col.names=TRUE, row.names=TRUE, quote=TRUE, na="NA")
> > > 
> > > 
> > > 
> > > 
> > > 
> > > ##############Make XYPLOT and export to ps coastcurv <- 
> > > read.table("C:/R_PLots/coastcurvfreqs.txt", header=TRUE, sep=",", 
> > > na.strings="NA", dec=".", strip.white=TRUE) 
> > > xyplot(coastcurv$rank.PRank~coastcurv$basin_curv,scales=list(y=list(
> > > lo
> > > g=TRUE
> > > ,at=c(.0001,.001,.01,.1,1)),x=list(log=TRUE,at=c(0.0001,0.001,0.01,0
> > > .1
> > > ,1,10)
> > > )),xlab="Curvature",ylab="P(C>C*)")
> > > dev.copy2eps(file="C:/R_PLots/coastcurv_cad.eps", width=8.0, 
> > > height=8.0,
> > > pointsize=10)
> > > 
> > > 
> > > ########How to get this in the first plot graphic?
> > > 
> > > coastslope <- read.table("C:/R_PLots/coastslopefreqs.txt",
> > > header=TRUE, sep=",", na.strings="NA", dec=".", strip.white=TRUE) 
> > > xyplot(coastslope$rank.PRank~coastslope$basin_slope,scales=list(y=li
> > > st
> > > (log=T
> > > RUE,at=c(.0001,.001,.01,.1,1)),x=list(log=TRUE,at=c(0.0001,0.001,0.0
> > > 1,
> > > 0.1,1,
> > > 10))),xlab="Slope",ylab="P(S>S*)")
> > > dev.copy2eps(file="C:/R_PLots/coastslope_cad.eps", width=8.0, 
> > > height=8.0,
> > > pointsize=10)
> > > 
> > > Thomas Colson, PhD
> > > North Carolina State University
> > > Department of Forestry and Environmental Resources
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > 
> > --
> > Andrew Robinson  
> > Department of Mathematics and Statistics            Tel: +61-3-8344-9763
> > University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
> > http://www.ms.unimelb.edu.au/~andrewpr
> > http://blogs.mbs.edu/fishing-in-the-bay/
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> --
> Andrew Robinson  
> Department of Mathematics and Statistics            Tel: +61-3-8344-9763
> University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
> http://www.ms.unimelb.edu.au/~andrewpr
> http://blogs.mbs.edu/fishing-in-the-bay/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From ripley at stats.ox.ac.uk  Sun Mar 18 08:00:34 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 18 Mar 2007 07:00:34 +0000 (GMT)
Subject: [R] Lag operator in R does not work
In-Reply-To: <e2da8399652b.45fcaa93@optonline.net>
References: <e2da8399652b.45fcaa93@optonline.net>
Message-ID: <Pine.LNX.4.64.0703180655060.24029@gannet.stats.ox.ac.uk>

On Sun, 18 Mar 2007, adschai at optonline.net wrote:

> Hi - I'm quite wondering what makes the lag operator does not work for my time series. I have a time series of length about 200000 elements. I would like to have a lag 1 of this time series. I did the following:
>
> logprice = log(price, base=exp(1))
> # this is my log price which is a vector of price time series of length 200000
> ts_logprice = as.ts(logprice, frequency=1) # convert to time series
> lag_logprice = lag(ts_logprice, 1) # get the lag
>
> When I do:
>
> ts_logprice[1:10] == lag_logprice[1:10]
>
> The result returns TRUE for the first 10 elements which I do not expect that.

You should.  lag() changes the time base, not the values, as the help page 
says:

   Description

   Compute a lagged version of a time series, shifting the time base back
   by a given number of observations.

> Is there any reason to this? I'd appreciate if you could let me know.

The posting guide was not followed.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ronggui.huang at gmail.com  Sun Mar 18 09:32:55 2007
From: ronggui.huang at gmail.com (ronggui)
Date: Sun, 18 Mar 2007 16:32:55 +0800
Subject: [R] Error with get_all_vars()
Message-ID: <38b9f0350703180132o38294c54o44e191b511c7f656@mail.gmail.com>

> get_all_vars(dist ~ speed, data = cars)
Error in `row.names<-.data.frame`(`*tmp*`, value = c(NA, -50L)) :
        invalid 'row.names' length

> version
               _
platform       i386-pc-mingw32
arch           i386
os             mingw32
system         i386, mingw32
status         Under development (unstable)
major          2
minor          5.0
year           2007
month          03
day            13
svn rev        40832
language       R
version.string R version 2.5.0 Under development (unstable) (2007-03-13 r40832)


-- 
Ronggui Huang
Department of Sociology
Fudan University, Shanghai, China


From ripley at stats.ox.ac.uk  Sun Mar 18 13:50:35 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 18 Mar 2007 12:50:35 +0000 (GMT)
Subject: [R] Error with get_all_vars()
In-Reply-To: <38b9f0350703180132o38294c54o44e191b511c7f656@mail.gmail.com>
References: <38b9f0350703180132o38294c54o44e191b511c7f656@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0703181238001.27567@gannet.stats.ox.ac.uk>

AFAIK there is no get_all_vars() in any released version of R, and this is 
NOT the place to report on unreleased ('Under development (unstable)') 
versions of R (and especially not on non-current versions).  (Please use 
the R-devel list to comment on current development versions.)

This was fixed by svn rev 40854 (it's actually one I have had pending for 
a while awaiting good Internet connectivity).

On Sun, 18 Mar 2007, ronggui wrote:

>> get_all_vars(dist ~ speed, data = cars)
> Error in `row.names<-.data.frame`(`*tmp*`, value = c(NA, -50L)) :
>        invalid 'row.names' length
>
>> version
>               _
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status         Under development (unstable)
> major          2
> minor          5.0
> year           2007
> month          03
> day            13
> svn rev        40832
> language       R
> version.string R version 2.5.0 Under development (unstable) (2007-03-13 r40832)
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From chabotd at globetrotter.net  Sun Mar 18 13:56:56 2007
From: chabotd at globetrotter.net (Chabot Denis)
Date: Sun, 18 Mar 2007 08:56:56 -0400
Subject: [R] italics letter in roman string
In-Reply-To: <1174188632.28942.23.camel@localhost.localdomain>
References: <mailman.12.1174129204.27826.r-help@stat.math.ethz.ch>
	<495B92B8-3F91-4A4B-818D-925DF941C303@globetrotter.net>
	<1174186568.28942.10.camel@localhost.localdomain>
	<1174188632.28942.23.camel@localhost.localdomain>
Message-ID: <F4EBFE17-96D3-4BE6-B686-71C63987AEDB@globetrotter.net>

Thank you Marc, Jim and Gabor,

I like the solution with "expression", nice and simple. Gabor, your  
solution did not work, probably just a matter of putting the text  
inside an expression?

However it would be nice if the help system pointed to it. A search  
on "italics" brought me nothing, one on "italic" gave me 4 hits, none  
useful. And reading the help on plotmath, I found no mention of italic 
(). Where can we suggest additions to the help system?

I must plead guilty to have forgotten a RSiteSearch before posting. I  
just did and I think I might have figured out something out there.  
But your answers were nice and to the point!

Cheers,

Denis
Le 07-03-17 ? 23:30, Marc Schwartz a ?crit :

> On Sat, 2007-03-17 at 21:56 -0500, Marc Schwartz wrote:
>> On Sat, 2007-03-17 at 22:01 -0400, Chabot Denis wrote:
>>> Hi,
>>>
>>> As part of the legend to a plot, I need to have the "n" in italics
>>> because it is a requirement of the journal I aim to publish in:
>>> "This study, n = 3293"
>>>
>>> Presently I have:
>>> legend(20, 105, "This study, n = 3293", pch=1,  col=rgb(0,0,0,0.5),
>>>                  pt.cex=0.3, cex=0.8, bty="n")
>>>
>>> I suppose I could leave a blank in place of the "n", then issue a
>>> text call where I'd use font=3 for a single letter, "n". But it will
>>> be tricky to find the exact location to use.
>>>
>>> Is there a way to switch to font=3 just for one letter within a  
>>> string?
>>>
>>> Thanks in advance,
>>>
>>> Denis Chabot
>>
>> Denis,
>>
>> Try something like this:
>>
>> plot(20, 100)
>>
>> leg <- legend(20, 105, "This study,    = 3293", pch = 1,
>>               col=rgb(0,0,0,0.5), pt.cex = 0.3, cex = 0.8,
>>               bty = "n")
>>
>> text(leg$text$x + strwidth("This study, ", cex = 0.8),
>>      leg$text$y, "n", font = 3, cex = 0.8, adj = c(0, 0.5))
>>
>>
>> Note that legend returns a list structure, which contains the x and y
>> coordinates of the start of the text strings that are plotted. So  
>> I get
>> that information for your line of text.
>>
>> Next, I use strwidth() to calculate, in user coordinates, the  
>> length of
>> the characters preceding the 'n', including spaces.  We add that
>> distance to the x coordinate returned in the legend call.
>>
>> I also use the 'adj' argument in the text() call, so that it is in  
>> synch
>> with the same parameters in legend() for alignment with the other
>> letters.
>>
>> See ?strwidth for more information.
>>
>> You may have to tweak the horizontal spacing of the 'n' a bit,  
>> depending
>> upon the rest of your graph.
>
> Denis,
>
> I thought of another approach, using plotmath.
>
> First, create a text expression, specifying that the 'n' should be
> italicized. Then use that expression in the legend() call.
>
> txt <- expression(paste("This study, ", italic(n), " = 3293"))
>
> plot(20, 100)
>
> legend(20, 105, txt, pch = 1, col=rgb(0,0,0,0.5),
>        pt.cex = 0.3, cex = 0.8, bty = "n")
>
>
> That's easier that the first solution.  See ?plotmath
>
> HTH,
>
> Marc Schwartz
>
>


From ggrothendieck at gmail.com  Sun Mar 18 13:59:59 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 18 Mar 2007 08:59:59 -0400
Subject: [R] italics letter in roman string
In-Reply-To: <F4EBFE17-96D3-4BE6-B686-71C63987AEDB@globetrotter.net>
References: <mailman.12.1174129204.27826.r-help@stat.math.ethz.ch>
	<495B92B8-3F91-4A4B-818D-925DF941C303@globetrotter.net>
	<1174186568.28942.10.camel@localhost.localdomain>
	<1174188632.28942.23.camel@localhost.localdomain>
	<F4EBFE17-96D3-4BE6-B686-71C63987AEDB@globetrotter.net>
Message-ID: <971536df0703180559k582d142ehe4e633397c1194f4@mail.gmail.com>

Sorry, legend= was omitted:

plot(1:10)
legend("topleft", legend = This ~ study ~ italic(n) == 3293)

On 3/18/07, Chabot Denis <chabotd at globetrotter.net> wrote:
> Thank you Marc, Jim and Gabor,
>
> I like the solution with "expression", nice and simple. Gabor, your
> solution did not work, probably just a matter of putting the text
> inside an expression?
>
> However it would be nice if the help system pointed to it. A search
> on "italics" brought me nothing, one on "italic" gave me 4 hits, none
> useful. And reading the help on plotmath, I found no mention of italic
> (). Where can we suggest additions to the help system?
>
> I must plead guilty to have forgotten a RSiteSearch before posting. I
> just did and I think I might have figured out something out there.
> But your answers were nice and to the point!
>
> Cheers,
>
> Denis
> Le 07-03-17 ? 23:30, Marc Schwartz a ?crit :
>
> > On Sat, 2007-03-17 at 21:56 -0500, Marc Schwartz wrote:
> >> On Sat, 2007-03-17 at 22:01 -0400, Chabot Denis wrote:
> >>> Hi,
> >>>
> >>> As part of the legend to a plot, I need to have the "n" in italics
> >>> because it is a requirement of the journal I aim to publish in:
> >>> "This study, n = 3293"
> >>>
> >>> Presently I have:
> >>> legend(20, 105, "This study, n = 3293", pch=1,  col=rgb(0,0,0,0.5),
> >>>                  pt.cex=0.3, cex=0.8, bty="n")
> >>>
> >>> I suppose I could leave a blank in place of the "n", then issue a
> >>> text call where I'd use font=3 for a single letter, "n". But it will
> >>> be tricky to find the exact location to use.
> >>>
> >>> Is there a way to switch to font=3 just for one letter within a
> >>> string?
> >>>
> >>> Thanks in advance,
> >>>
> >>> Denis Chabot
> >>
> >> Denis,
> >>
> >> Try something like this:
> >>
> >> plot(20, 100)
> >>
> >> leg <- legend(20, 105, "This study,    = 3293", pch = 1,
> >>               col=rgb(0,0,0,0.5), pt.cex = 0.3, cex = 0.8,
> >>               bty = "n")
> >>
> >> text(leg$text$x + strwidth("This study, ", cex = 0.8),
> >>      leg$text$y, "n", font = 3, cex = 0.8, adj = c(0, 0.5))
> >>
> >>
> >> Note that legend returns a list structure, which contains the x and y
> >> coordinates of the start of the text strings that are plotted. So
> >> I get
> >> that information for your line of text.
> >>
> >> Next, I use strwidth() to calculate, in user coordinates, the
> >> length of
> >> the characters preceding the 'n', including spaces.  We add that
> >> distance to the x coordinate returned in the legend call.
> >>
> >> I also use the 'adj' argument in the text() call, so that it is in
> >> synch
> >> with the same parameters in legend() for alignment with the other
> >> letters.
> >>
> >> See ?strwidth for more information.
> >>
> >> You may have to tweak the horizontal spacing of the 'n' a bit,
> >> depending
> >> upon the rest of your graph.
> >
> > Denis,
> >
> > I thought of another approach, using plotmath.
> >
> > First, create a text expression, specifying that the 'n' should be
> > italicized. Then use that expression in the legend() call.
> >
> > txt <- expression(paste("This study, ", italic(n), " = 3293"))
> >
> > plot(20, 100)
> >
> > legend(20, 105, txt, pch = 1, col=rgb(0,0,0,0.5),
> >        pt.cex = 0.3, cex = 0.8, bty = "n")
> >
> >
> > That's easier that the first solution.  See ?plotmath
> >
> > HTH,
> >
> > Marc Schwartz
> >
> >
>
>


From chabotd at globetrotter.net  Sun Mar 18 14:05:13 2007
From: chabotd at globetrotter.net (Chabot Denis)
Date: Sun, 18 Mar 2007 09:05:13 -0400
Subject: [R] italics letter in roman string
In-Reply-To: <971536df0703180559k582d142ehe4e633397c1194f4@mail.gmail.com>
References: <mailman.12.1174129204.27826.r-help@stat.math.ethz.ch>
	<495B92B8-3F91-4A4B-818D-925DF941C303@globetrotter.net>
	<1174186568.28942.10.camel@localhost.localdomain>
	<1174188632.28942.23.camel@localhost.localdomain>
	<F4EBFE17-96D3-4BE6-B686-71C63987AEDB@globetrotter.net>
	<971536df0703180559k582d142ehe4e633397c1194f4@mail.gmail.com>
Message-ID: <5D3DA496-3CDC-46B8-9E5C-1891A7AC482D@globetrotter.net>

Wow, this works, Gabor, but I am mystified. I would have tought an  
expression needed the word expression, and/or a text string needed to  
be within quotes. What is happening here, exactly? Why the use of  
"~"? I tried without and it no longer works.

Thanks in advance,

Denis
Le 07-03-18 ? 08:59, Gabor Grothendieck a ?crit :

> Sorry, legend= was omitted:
>
> plot(1:10)
> legend("topleft", legend = This ~ study ~ italic(n) == 3293)
>
> On 3/18/07, Chabot Denis <chabotd at globetrotter.net> wrote:
>> Thank you Marc, Jim and Gabor,
>>
>> I like the solution with "expression", nice and simple. Gabor, your
>> solution did not work, probably just a matter of putting the text
>> inside an expression?
>>
>> However it would be nice if the help system pointed to it. A search
>> on "italics" brought me nothing, one on "italic" gave me 4 hits, none
>> useful. And reading the help on plotmath, I found no mention of  
>> italic
>> (). Where can we suggest additions to the help system?
>>
>> I must plead guilty to have forgotten a RSiteSearch before posting. I
>> just did and I think I might have figured out something out there.
>> But your answers were nice and to the point!
>>
>> Cheers,
>>
>> Denis
>> Le 07-03-17 ? 23:30, Marc Schwartz a ?crit :
>>
>> > On Sat, 2007-03-17 at 21:56 -0500, Marc Schwartz wrote:
>> >> On Sat, 2007-03-17 at 22:01 -0400, Chabot Denis wrote:
>> >>> Hi,
>> >>>
>> >>> As part of the legend to a plot, I need to have the "n" in  
>> italics
>> >>> because it is a requirement of the journal I aim to publish in:
>> >>> "This study, n = 3293"
>> >>>
>> >>> Presently I have:
>> >>> legend(20, 105, "This study, n = 3293", pch=1,  col=rgb 
>> (0,0,0,0.5),
>> >>>                  pt.cex=0.3, cex=0.8, bty="n")
>> >>>
>> >>> I suppose I could leave a blank in place of the "n", then issue a
>> >>> text call where I'd use font=3 for a single letter, "n". But  
>> it will
>> >>> be tricky to find the exact location to use.
>> >>>
>> >>> Is there a way to switch to font=3 just for one letter within a
>> >>> string?
>> >>>
>> >>> Thanks in advance,
>> >>>
>> >>> Denis Chabot
>> >>
>> >> Denis,
>> >>
>> >> Try something like this:
>> >>
>> >> plot(20, 100)
>> >>
>> >> leg <- legend(20, 105, "This study,    = 3293", pch = 1,
>> >>               col=rgb(0,0,0,0.5), pt.cex = 0.3, cex = 0.8,
>> >>               bty = "n")
>> >>
>> >> text(leg$text$x + strwidth("This study, ", cex = 0.8),
>> >>      leg$text$y, "n", font = 3, cex = 0.8, adj = c(0, 0.5))
>> >>
>> >>
>> >> Note that legend returns a list structure, which contains the x  
>> and y
>> >> coordinates of the start of the text strings that are plotted. So
>> >> I get
>> >> that information for your line of text.
>> >>
>> >> Next, I use strwidth() to calculate, in user coordinates, the
>> >> length of
>> >> the characters preceding the 'n', including spaces.  We add that
>> >> distance to the x coordinate returned in the legend call.
>> >>
>> >> I also use the 'adj' argument in the text() call, so that it is in
>> >> synch
>> >> with the same parameters in legend() for alignment with the other
>> >> letters.
>> >>
>> >> See ?strwidth for more information.
>> >>
>> >> You may have to tweak the horizontal spacing of the 'n' a bit,
>> >> depending
>> >> upon the rest of your graph.
>> >
>> > Denis,
>> >
>> > I thought of another approach, using plotmath.
>> >
>> > First, create a text expression, specifying that the 'n' should be
>> > italicized. Then use that expression in the legend() call.
>> >
>> > txt <- expression(paste("This study, ", italic(n), " = 3293"))
>> >
>> > plot(20, 100)
>> >
>> > legend(20, 105, txt, pch = 1, col=rgb(0,0,0,0.5),
>> >        pt.cex = 0.3, cex = 0.8, bty = "n")
>> >
>> >
>> > That's easier that the first solution.  See ?plotmath
>> >
>> > HTH,
>> >
>> > Marc Schwartz
>> >
>> >
>>
>>


From ggrothendieck at gmail.com  Sun Mar 18 14:11:56 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 18 Mar 2007 09:11:56 -0400
Subject: [R] italics letter in roman string
In-Reply-To: <5D3DA496-3CDC-46B8-9E5C-1891A7AC482D@globetrotter.net>
References: <mailman.12.1174129204.27826.r-help@stat.math.ethz.ch>
	<495B92B8-3F91-4A4B-818D-925DF941C303@globetrotter.net>
	<1174186568.28942.10.camel@localhost.localdomain>
	<1174188632.28942.23.camel@localhost.localdomain>
	<F4EBFE17-96D3-4BE6-B686-71C63987AEDB@globetrotter.net>
	<971536df0703180559k582d142ehe4e633397c1194f4@mail.gmail.com>
	<5D3DA496-3CDC-46B8-9E5C-1891A7AC482D@globetrotter.net>
Message-ID: <971536df0703180611t532d9222l59ae809e66eca940@mail.gmail.com>

Its a formula, not an expression -- but it will coerce formulas.  Without ~
its no longer a formula.

On 3/18/07, Chabot Denis <chabotd at globetrotter.net> wrote:
> Wow, this works, Gabor, but I am mystified. I would have tought an
> expression needed the word expression, and/or a text string needed to
> be within quotes. What is happening here, exactly? Why the use of
> "~"? I tried without and it no longer works.
>
> Thanks in advance,
>
> Denis
> Le 07-03-18 ? 08:59, Gabor Grothendieck a ?crit :
>
> > Sorry, legend= was omitted:
> >
> > plot(1:10)
> > legend("topleft", legend = This ~ study ~ italic(n) == 3293)
> >
> > On 3/18/07, Chabot Denis <chabotd at globetrotter.net> wrote:
> >> Thank you Marc, Jim and Gabor,
> >>
> >> I like the solution with "expression", nice and simple. Gabor, your
> >> solution did not work, probably just a matter of putting the text
> >> inside an expression?
> >>
> >> However it would be nice if the help system pointed to it. A search
> >> on "italics" brought me nothing, one on "italic" gave me 4 hits, none
> >> useful. And reading the help on plotmath, I found no mention of
> >> italic
> >> (). Where can we suggest additions to the help system?
> >>
> >> I must plead guilty to have forgotten a RSiteSearch before posting. I
> >> just did and I think I might have figured out something out there.
> >> But your answers were nice and to the point!
> >>
> >> Cheers,
> >>
> >> Denis
> >> Le 07-03-17 ? 23:30, Marc Schwartz a ?crit :
> >>
> >> > On Sat, 2007-03-17 at 21:56 -0500, Marc Schwartz wrote:
> >> >> On Sat, 2007-03-17 at 22:01 -0400, Chabot Denis wrote:
> >> >>> Hi,
> >> >>>
> >> >>> As part of the legend to a plot, I need to have the "n" in
> >> italics
> >> >>> because it is a requirement of the journal I aim to publish in:
> >> >>> "This study, n = 3293"
> >> >>>
> >> >>> Presently I have:
> >> >>> legend(20, 105, "This study, n = 3293", pch=1,  col=rgb
> >> (0,0,0,0.5),
> >> >>>                  pt.cex=0.3, cex=0.8, bty="n")
> >> >>>
> >> >>> I suppose I could leave a blank in place of the "n", then issue a
> >> >>> text call where I'd use font=3 for a single letter, "n". But
> >> it will
> >> >>> be tricky to find the exact location to use.
> >> >>>
> >> >>> Is there a way to switch to font=3 just for one letter within a
> >> >>> string?
> >> >>>
> >> >>> Thanks in advance,
> >> >>>
> >> >>> Denis Chabot
> >> >>
> >> >> Denis,
> >> >>
> >> >> Try something like this:
> >> >>
> >> >> plot(20, 100)
> >> >>
> >> >> leg <- legend(20, 105, "This study,    = 3293", pch = 1,
> >> >>               col=rgb(0,0,0,0.5), pt.cex = 0.3, cex = 0.8,
> >> >>               bty = "n")
> >> >>
> >> >> text(leg$text$x + strwidth("This study, ", cex = 0.8),
> >> >>      leg$text$y, "n", font = 3, cex = 0.8, adj = c(0, 0.5))
> >> >>
> >> >>
> >> >> Note that legend returns a list structure, which contains the x
> >> and y
> >> >> coordinates of the start of the text strings that are plotted. So
> >> >> I get
> >> >> that information for your line of text.
> >> >>
> >> >> Next, I use strwidth() to calculate, in user coordinates, the
> >> >> length of
> >> >> the characters preceding the 'n', including spaces.  We add that
> >> >> distance to the x coordinate returned in the legend call.
> >> >>
> >> >> I also use the 'adj' argument in the text() call, so that it is in
> >> >> synch
> >> >> with the same parameters in legend() for alignment with the other
> >> >> letters.
> >> >>
> >> >> See ?strwidth for more information.
> >> >>
> >> >> You may have to tweak the horizontal spacing of the 'n' a bit,
> >> >> depending
> >> >> upon the rest of your graph.
> >> >
> >> > Denis,
> >> >
> >> > I thought of another approach, using plotmath.
> >> >
> >> > First, create a text expression, specifying that the 'n' should be
> >> > italicized. Then use that expression in the legend() call.
> >> >
> >> > txt <- expression(paste("This study, ", italic(n), " = 3293"))
> >> >
> >> > plot(20, 100)
> >> >
> >> > legend(20, 105, txt, pch = 1, col=rgb(0,0,0,0.5),
> >> >        pt.cex = 0.3, cex = 0.8, bty = "n")
> >> >
> >> >
> >> > That's easier that the first solution.  See ?plotmath
> >> >
> >> > HTH,
> >> >
> >> > Marc Schwartz
> >> >
> >> >
> >>
> >>
>
>


From murdoch at stats.uwo.ca  Sun Mar 18 14:15:55 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 18 Mar 2007 09:15:55 -0400
Subject: [R] italics letter in roman string
In-Reply-To: <F4EBFE17-96D3-4BE6-B686-71C63987AEDB@globetrotter.net>
References: <mailman.12.1174129204.27826.r-help@stat.math.ethz.ch>	<495B92B8-3F91-4A4B-818D-925DF941C303@globetrotter.net>	<1174186568.28942.10.camel@localhost.localdomain>	<1174188632.28942.23.camel@localhost.localdomain>
	<F4EBFE17-96D3-4BE6-B686-71C63987AEDB@globetrotter.net>
Message-ID: <45FD3B8B.8070704@stats.uwo.ca>

On 3/18/2007 8:56 AM, Chabot Denis wrote:
> Thank you Marc, Jim and Gabor,
> 
> I like the solution with "expression", nice and simple. Gabor, your  
> solution did not work, probably just a matter of putting the text  
> inside an expression?
> 
> However it would be nice if the help system pointed to it. A search  
> on "italics" brought me nothing, one on "italic" gave me 4 hits, none  
> useful. And reading the help on plotmath, I found no mention of italic 
> (). Where can we suggest additions to the help system?

The R-devel list is the best place to make suggestions like that if 
you're talking about documentation in base packages.  Please submit 
patches to the source from

https://svn.r-project.org/R/trunk

Package documentation is in src/library/*/man/*.Rd, the manuals are in 
doc/manual/*.texi.  There are also bits and pieces of other 
documentation (FAQs, etc.) elsewhere.

Suggestions about contributed packages (including Recommended ones) 
should be sent to the package maintainer and/or author.

Duncan Murdoch

> 
> I must plead guilty to have forgotten a RSiteSearch before posting. I  
> just did and I think I might have figured out something out there.  
> But your answers were nice and to the point!
> 
> Cheers,
> 
> Denis
> Le 07-03-17 ? 23:30, Marc Schwartz a ?crit :
> 
>> On Sat, 2007-03-17 at 21:56 -0500, Marc Schwartz wrote:
>>> On Sat, 2007-03-17 at 22:01 -0400, Chabot Denis wrote:
>>>> Hi,
>>>>
>>>> As part of the legend to a plot, I need to have the "n" in italics
>>>> because it is a requirement of the journal I aim to publish in:
>>>> "This study, n = 3293"
>>>>
>>>> Presently I have:
>>>> legend(20, 105, "This study, n = 3293", pch=1,  col=rgb(0,0,0,0.5),
>>>>                  pt.cex=0.3, cex=0.8, bty="n")
>>>>
>>>> I suppose I could leave a blank in place of the "n", then issue a
>>>> text call where I'd use font=3 for a single letter, "n". But it will
>>>> be tricky to find the exact location to use.
>>>>
>>>> Is there a way to switch to font=3 just for one letter within a  
>>>> string?
>>>>
>>>> Thanks in advance,
>>>>
>>>> Denis Chabot
>>> Denis,
>>>
>>> Try something like this:
>>>
>>> plot(20, 100)
>>>
>>> leg <- legend(20, 105, "This study,    = 3293", pch = 1,
>>>               col=rgb(0,0,0,0.5), pt.cex = 0.3, cex = 0.8,
>>>               bty = "n")
>>>
>>> text(leg$text$x + strwidth("This study, ", cex = 0.8),
>>>      leg$text$y, "n", font = 3, cex = 0.8, adj = c(0, 0.5))
>>>
>>>
>>> Note that legend returns a list structure, which contains the x and y
>>> coordinates of the start of the text strings that are plotted. So  
>>> I get
>>> that information for your line of text.
>>>
>>> Next, I use strwidth() to calculate, in user coordinates, the  
>>> length of
>>> the characters preceding the 'n', including spaces.  We add that
>>> distance to the x coordinate returned in the legend call.
>>>
>>> I also use the 'adj' argument in the text() call, so that it is in  
>>> synch
>>> with the same parameters in legend() for alignment with the other
>>> letters.
>>>
>>> See ?strwidth for more information.
>>>
>>> You may have to tweak the horizontal spacing of the 'n' a bit,  
>>> depending
>>> upon the rest of your graph.
>> Denis,
>>
>> I thought of another approach, using plotmath.
>>
>> First, create a text expression, specifying that the 'n' should be
>> italicized. Then use that expression in the legend() call.
>>
>> txt <- expression(paste("This study, ", italic(n), " = 3293"))
>>
>> plot(20, 100)
>>
>> legend(20, 105, txt, pch = 1, col=rgb(0,0,0,0.5),
>>        pt.cex = 0.3, cex = 0.8, bty = "n")
>>
>>
>> That's easier that the first solution.  See ?plotmath
>>
>> HTH,
>>
>> Marc Schwartz
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From labone at gforcecable.com  Sun Mar 18 14:41:47 2007
From: labone at gforcecable.com (Tom La Bone)
Date: Sun, 18 Mar 2007 09:41:47 -0400
Subject: [R] Problem Loading rggobi package
Message-ID: <000001c76963$2d205180$6401a8c0@Boozoo>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070318/e929cb50/attachment.pl 

From sea at sao.ru  Sun Mar 18 17:36:50 2007
From: sea at sao.ru (Eugene Semenko)
Date: Sun, 18 Mar 2007 19:36:50 +0300
Subject: [R] ticks labels and R
Message-ID: <200703181936.52571.sea@sao.ru>

Hi,
I have plot a graphics by next way:
par(mar=c(5,5,1,1), font.axis=3, ps=14, xaxs="i", yaxs="i", lab=c(18,18,7), 
font
.lab=3);
plot(l, b, type="n", xlim=c(0,360), ylim=c(-90,90), xlab="l, deg", ylab="b, 
deg");
points(l, b, cex=vel/60, col=1, pch=21);

On this plot I have axis x (denoted by l) ranged in [0,360] with labeling step 
20. But I want to set this step equal to 60 and want to set format of labels 
to degrees. I'm newbie in R and don't know how to change parameters of axis. 
Especially important to add sign of degree to each tick label.
	Thanks in advance,
-- 
---------------------------------------------------------------------------------------
Eugene A. Semenko
e-mail: sea at sao.ru              Special Astrophysical Observatory RAS,
WWW: http://tiger.sao.ru/       Nizhnij Arkhyz
phone: +7 87878 46 5 77         Karachai-Chercassian Republic,
fax: +7 87878 46 5 27           Russia, 369167


From h.wickham at gmail.com  Sun Mar 18 17:50:07 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 18 Mar 2007 11:50:07 -0500
Subject: [R] Problem Loading rggobi package
In-Reply-To: <000001c76963$2d205180$6401a8c0@Boozoo>
References: <000001c76963$2d205180$6401a8c0@Boozoo>
Message-ID: <f8e6ff050703180950mf98a86u9c5506278bd4ba49@mail.gmail.com>

Hi Tom,

Did you install ggobi first?  And the RGtk2 package as well?

Hadley

On 3/18/07, Tom La Bone <labone at gforcecable.com> wrote:
>
>
> After installing rggobi, I get the following error when I try to load it:
>
>
>
> > local({pkg <- select.list(sort(.packages(all.available = TRUE)))
>
> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
>
> Loading required package: RGtk2
>
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>
>         unable to load shared library
> 'C:/PROGRA~1/R/R-24~1.1/library/rggobi/libs/rggobi.dll':
>
>   LoadLibrary failure:  The specified module could not be found.
>
> Error in fun(...) : Could not load the rggobi library - please ensure GGobi
> is on the library path
>
> Error: .onLoad failed in 'loadNamespace' for 'rggobi'
>
> Error: package/namespace load failed for 'rggobi'
>
>
>
> The rggobi.dll appears to be in right directory.  Any suggestions on how to
> help R find the dll?  Thanks.
>
>
>
> Tom
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Sun Mar 18 17:56:35 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 18 Mar 2007 16:56:35 +0000 (GMT)
Subject: [R] Problem Loading rggobi package
In-Reply-To: <000001c76963$2d205180$6401a8c0@Boozoo>
References: <000001c76963$2d205180$6401a8c0@Boozoo>
Message-ID: <Pine.LNX.4.64.0703181649320.31817@gannet.stats.ox.ac.uk>

This is really a GGobi issue not an R issue, but I believe their mailing 
lists are down.

I think you are assuming that it is rggobi.dll that cannot be found, but 
that is not what the message says.  I would have expected Windows to give 
you a dialog box telling you the exact problem, and I suspect it was 
exactly what the Error message says: it is one of the ggobi or GTk or XML 
DLLs that was not found.

We did have a lot of trouble with similar things using GGobi in a shared 
installation: you need to check permissions as well as presence in the 
path of all the dependent DLLs.

On Sun, 18 Mar 2007, Tom La Bone wrote:

>
>
> After installing rggobi, I get the following error when I try to load it:
>
>
>
>> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
>
> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
>
> Loading required package: RGtk2
>
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>
>        unable to load shared library
> 'C:/PROGRA~1/R/R-24~1.1/library/rggobi/libs/rggobi.dll':
>
>  LoadLibrary failure:  The specified module could not be found.
>
> Error in fun(...) : Could not load the rggobi library - please ensure GGobi
> is on the library path
>
> Error: .onLoad failed in 'loadNamespace' for 'rggobi'
>
> Error: package/namespace load failed for 'rggobi'
>
>
>
> The rggobi.dll appears to be in right directory.  Any suggestions on how to
> help R find the dll?  Thanks.
>
>
>
> Tom
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From h.wickham at gmail.com  Sun Mar 18 18:04:35 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 18 Mar 2007 12:04:35 -0500
Subject: [R] Problem Loading rggobi package
In-Reply-To: <Pine.LNX.4.64.0703181649320.31817@gannet.stats.ox.ac.uk>
References: <000001c76963$2d205180$6401a8c0@Boozoo>
	<Pine.LNX.4.64.0703181649320.31817@gannet.stats.ox.ac.uk>
Message-ID: <f8e6ff050703181004q386f9edcv9a5f45d2c8443c10@mail.gmail.com>

> This is really a GGobi issue not an R issue, but I believe their mailing
> lists are down.

Our mailing lists were down, but are now up again, after we switched
to a new system.  You may need to sign up again at
http://www.ggobi.org/support/.

We're very disappointed at the level of support we recieved from our
host on this issue so we're looking for a new host, and have moved to
Google groups in the hope that they will be more reliable.

Apologies for any inconvience,

Hadley


From labone at gforcecable.com  Sun Mar 18 18:17:50 2007
From: labone at gforcecable.com (Tom La Bone)
Date: Sun, 18 Mar 2007 13:17:50 -0400
Subject: [R] Problem Loading rggobi package
In-Reply-To: <f8e6ff050703180950mf98a86u9c5506278bd4ba49@mail.gmail.com>
Message-ID: <000001c76981$5b6cc0a0$6401a8c0@Boozoo>

No, I did not install ggobi at all (looks like I missed that step).  I went
back and installed it and then reinstalled the R package and now everything
appears to be working as advertised.  Thanks.

Tom

-----Original Message-----
From: hadley wickham [mailto:h.wickham at gmail.com] 
Sent: Sunday, March 18, 2007 12:50 PM
To: Tom La Bone
Cc: r-help at stat.math.ethz.ch; Michael Lawrence
Subject: Re: [R] Problem Loading rggobi package

Hi Tom,

Did you install ggobi first?  And the RGtk2 package as well?

Hadley

On 3/18/07, Tom La Bone <labone at gforcecable.com> wrote:
>
>
> After installing rggobi, I get the following error when I try to load it:
>
>
>
> > local({pkg <- select.list(sort(.packages(all.available = TRUE)))
>
> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
>
> Loading required package: RGtk2
>
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>
>         unable to load shared library
> 'C:/PROGRA~1/R/R-24~1.1/library/rggobi/libs/rggobi.dll':
>
>   LoadLibrary failure:  The specified module could not be found.
>
> Error in fun(...) : Could not load the rggobi library - please ensure
GGobi
> is on the library path
>
> Error: .onLoad failed in 'loadNamespace' for 'rggobi'
>
> Error: package/namespace load failed for 'rggobi'
>
>
>
> The rggobi.dll appears to be in right directory.  Any suggestions on how
to
> help R find the dll?  Thanks.
>
>
>
> Tom
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From marc_schwartz at comcast.net  Sun Mar 18 18:38:52 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sun, 18 Mar 2007 12:38:52 -0500
Subject: [R] ticks labels and R
In-Reply-To: <200703181936.52571.sea@sao.ru>
References: <200703181936.52571.sea@sao.ru>
Message-ID: <1174239532.28942.41.camel@localhost.localdomain>

On Sun, 2007-03-18 at 19:36 +0300, Eugene Semenko wrote:
> Hi,
> I have plot a graphics by next way:
> par(mar=c(5,5,1,1), font.axis=3, ps=14, xaxs="i", yaxs="i", lab=c(18,18,7), 
> font
> .lab=3);
> plot(l, b, type="n", xlim=c(0,360), ylim=c(-90,90), xlab="l, deg", ylab="b, 
> deg");
> points(l, b, cex=vel/60, col=1, pch=21);
> 
> On this plot I have axis x (denoted by l) ranged in [0,360] with labeling step 
> 20. But I want to set this step equal to 60 and want to set format of labels 
> to degrees. I'm newbie in R and don't know how to change parameters of axis. 
> Especially important to add sign of degree to each tick label.
> 	Thanks in advance,


Eugene, see ?plotmath for more information.


at <- seq(-360, 360, 60)

plot(at, at, axes = FALSE)

# Now create expressions with the degree symbol
# See ?parse and ?paste
L <- parse(text = paste(at, "*degree", sep = ""))

> L
expression(-360 * degree, -300 * degree, -240 * degree, -180 * 
    degree, -120 * degree, -60 * degree, 0 * degree, 60 * degree, 
    120 * degree, 180 * degree, 240 * degree, 300 * degree, 360 * 
        degree)

# now do the axis labels
axis(1, at = at, labels = L, cex.axis = 0.75)

axis(2, at = at, labels = L, cex.axis = 0.75, las = 2)


See ?axis as well.

HTH,

Marc Schwartz


From dhajage.r at gmail.com  Sun Mar 18 18:42:21 2007
From: dhajage.r at gmail.com (David Hajage)
Date: Sun, 18 Mar 2007 18:42:21 +0100
Subject: [R] Read a .sas7bdat file
Message-ID: <979ffa270703181042q6987fbd0v4566e8314c6c45f6@mail.gmail.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070318/73867327/attachment.pl 

From marc_schwartz at comcast.net  Sun Mar 18 19:01:25 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sun, 18 Mar 2007 13:01:25 -0500
Subject: [R] Read a .sas7bdat file
In-Reply-To: <979ffa270703181042q6987fbd0v4566e8314c6c45f6@mail.gmail.com>
References: <979ffa270703181042q6987fbd0v4566e8314c6c45f6@mail.gmail.com>
Message-ID: <1174240885.28942.57.camel@localhost.localdomain>

On Sun, 2007-03-18 at 18:42 +0100, David Hajage wrote:
> Hello useRs,
> 
> I would like to import a data frame in a sas format (.sas7bdat).
> 
> I have installed "foreign" library, and tryed to use "read.ssd" function :
> 
> > read.ssd("these", "fus9706.sas7bdat")
> sh: sas: not found
> SAS failed.  SAS program at /tmp/RtmpPdVSST/file2ae8944a.sas
> a log and other error products should be in the vicinity
> ls: file2ae8944a.log: Aucun fichier ou rpertoire de ce type
> NULL
> Warning message:
> Le code renvoy par SAS est 32512 in: read.ssd("these", "fus9706.sas7bdat")
> 
> Is it possible to read this file with R ?
> 
> Thank you for your help.
> 
> David

That format is proprietary to SAS.  You would need:

1. A licensed copy of SAS available (which is called from read.ssd())
and the lack of which results in the error above. Note the 'sascmd'
argument in read.ssd():

  character string giving full path to SAS executable.


2. A third party conversion application (such as DBMS/Copy) which can
read the proprietary SAS dataset and export it to a format that can be
read by R, such as a CSV file.


If the entity that provided you with the SAS dataset can export it to an
alternative format, that would be helpful. There are also functions
available in R which can read the so-called SAS Transport format (which
is openly defined). These would include:

read.xport() in foreign
sasxport.get() in Hmisc, which uses the former

You may wish to search the r-help archives as I recall issues with the
use of the above functions that may require some further insights.

HTH,

Marc Schwartz


From p.dalgaard at biostat.ku.dk  Sun Mar 18 19:02:06 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Sun, 18 Mar 2007 19:02:06 +0100
Subject: [R] Read a .sas7bdat file
In-Reply-To: <979ffa270703181042q6987fbd0v4566e8314c6c45f6@mail.gmail.com>
References: <979ffa270703181042q6987fbd0v4566e8314c6c45f6@mail.gmail.com>
Message-ID: <45FD7E9E.3010703@biostat.ku.dk>

David Hajage wrote:
> Hello useRs,
>
> I would like to import a data frame in a sas format (.sas7bdat).
>
> I have installed "foreign" library, and tryed to use "read.ssd" function :
>
>   
>> read.ssd("these", "fus9706.sas7bdat")
>>     
> sh: sas: not found
> SAS failed.  SAS program at /tmp/RtmpPdVSST/file2ae8944a.sas
> a log and other error products should be in the vicinity
> ls: file2ae8944a.log: Aucun fichier ou r?pertoire de ce type
> NULL
> Warning message:
> Le code renvoy? par SAS est 32512 in: read.ssd("these", "fus9706.sas7bdat")
>
> Is it possible to read this file with R ?
>
>   
Not until SAS or someone else comes up with a description of the file 
format. Till then, you need SAS itself or one of a couple of 
special-purpose closed-source conversion tools (Stat/Transfer, DBMS/Copy).
> Thank you for your help.
>
> David
>
>


From dhajage.r at gmail.com  Sun Mar 18 19:30:45 2007
From: dhajage.r at gmail.com (David Hajage)
Date: Sun, 18 Mar 2007 19:30:45 +0100
Subject: [R] Read a .sas7bdat file
In-Reply-To: <45FD7E9E.3010703@biostat.ku.dk>
References: <979ffa270703181042q6987fbd0v4566e8314c6c45f6@mail.gmail.com>
	<45FD7E9E.3010703@biostat.ku.dk>
Message-ID: <979ffa270703181130se4f2537rf06219548129a286@mail.gmail.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070318/1eb6174a/attachment.pl 

From fhduan at gmail.com  Sun Mar 18 19:36:44 2007
From: fhduan at gmail.com (Frank Duan)
Date: Sun, 18 Mar 2007 13:36:44 -0500
Subject: [R] subset by multiple columns satisfying the same condition
Message-ID: <3b9172310703181136k2172d02dr25e898e9d0a45ada@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070318/07af785a/attachment.pl 

From mnair at iusb.edu  Sun Mar 18 19:37:24 2007
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Sun, 18 Mar 2007 14:37:24 -0400
Subject: [R] multcomp
Message-ID: <A32055BDEA88C34BB3DBBCD229380778E656A5@iu-mssg-mbx109.ads.iu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070318/a9237778/attachment.pl 

From aa2007r at gmail.com  Sun Mar 18 19:42:11 2007
From: aa2007r at gmail.com (AA)
Date: Sun, 18 Mar 2007 14:42:11 -0400
Subject: [R] simple multivariate linear model plot
Message-ID: <0bed01c7698d$24617360$3927a8c0@treesdalellc.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070318/6f8805e3/attachment.pl 

From peter.mcmahan at gmail.com  Sun Mar 18 19:56:47 2007
From: peter.mcmahan at gmail.com (Peter McMahan)
Date: Sun, 18 Mar 2007 13:56:47 -0500
Subject: [R] subset by multiple columns satisfying the same condition
In-Reply-To: <3b9172310703181136k2172d02dr25e898e9d0a45ada@mail.gmail.com>
References: <3b9172310703181136k2172d02dr25e898e9d0a45ada@mail.gmail.com>
Message-ID: <67013317-672B-491D-A8B5-A863F5FF57F4@gmail.com>

say x is you rdata frame and y is the vector of column indices you  
want to match to a condition:

x[apply(x[y]=="Tom",1,all),]

still, i feel like there's probably a better way...

On Mar 18, 2007, at 1:36 PM, Frank Duan wrote:

> Hi All,
>
> I have a very simple question. Suppose I had a data frame with 100  
> columns,
> now I wanted to select rows with the values of  some columns  
> satisfying the
> same condition, like all equal to "Tom". I know I can use the 'and'  
> operator
> "&", but it's painful if there were many columns.
>
> Can anyone give me some advice? Thanks in advance,
>
> FD
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From p.dalgaard at biostat.ku.dk  Sun Mar 18 20:38:24 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Sun, 18 Mar 2007 20:38:24 +0100
Subject: [R] subset by multiple columns satisfying the same condition
In-Reply-To: <3b9172310703181136k2172d02dr25e898e9d0a45ada@mail.gmail.com>
References: <3b9172310703181136k2172d02dr25e898e9d0a45ada@mail.gmail.com>
Message-ID: <45FD9530.60703@biostat.ku.dk>

Frank Duan wrote:
> Hi All,
>
> I have a very simple question. Suppose I had a data frame with 100 columns,
> now I wanted to select rows with the values of  some columns satisfying the
> same condition, like all equal to "Tom". I know I can use the 'and' operator
> "&", but it's painful if there were many columns.
>
> Can anyone give me some advice? Thanks in advance,
>   
Here's one way:
 
rowSums(myframe != "Tom") == 0

The following approach might generalize more easily, though

 do.call("pmin", lapply(myframe, "==", "Tom"))

(notice that pmin on logical vectors is TRUE, if all are TRUE, else 
FALSE or NA).


From aa2007r at gmail.com  Sun Mar 18 20:44:42 2007
From: aa2007r at gmail.com (AA)
Date: Sun, 18 Mar 2007 15:44:42 -0400
Subject: [R] simple multivariate linear model plot
Message-ID: <0cb301c76995$e0231e70$3927a8c0@treesdalellc.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070318/976be208/attachment.pl 

From tkeitt at gmail.com  Sun Mar 18 20:57:44 2007
From: tkeitt at gmail.com (Tim Keitt)
Date: Sun, 18 Mar 2007 14:57:44 -0500
Subject: [R] Setting site-wide default CRAN repository?
Message-ID: <6262c54c0703181257y674f3accic6b82c9aa4f72dae@mail.gmail.com>

I can't seem to find this anywhere. How do I set the default CRAN
repository _site wide_ on a linux box? What I want to do is eliminate
the pop-up list of repository locations when using
'install.packages()'. I know how to do this for a single account.
Modifying files in /etc/R does not seem to work. (cc me please - I
think I'm not subscribed).

THK

-- 
Timothy H. Keitt, University of Texas at Austin
Contact info and schedule at http://www.keittlab.org/tkeitt/
Reprints at http://www.keittlab.org/tkeitt/papers/
ODF attachment? See http://www.openoffice.org/


From pascale.labatut at etu.ensat.fr  Sun Mar 18 19:07:51 2007
From: pascale.labatut at etu.ensat.fr (pasarah)
Date: Sun, 18 Mar 2007 11:07:51 -0700 (PDT)
Subject: [R] HELP...Running data
Message-ID: <9541217.post@talk.nabble.com>


We are two french students and we have a problem concerning an exercize.
We don't know how to resolve it.
It would be fantastic if someone can help us.
Thanks.

Description:
This study examined how the metabolic cost of locomotion varied
with speed, stride frequency and body mass. Cost was determined
by measuring oxygen consumption (?vo2?), analyzing the oxygen
content in air inhaled and exhaled by the subjects through a mask.
The rate of oxgen consumption was measured for three ?subject?s
locomoting at all combinations of low and high levels of the three
factors - running ?speed?, ?stride? frequency, and ?mass?
distribution in the leg. The first factor was set using a
treadmill, the second by synchronizing the subjects? pace with a
metronome, and the third by varying the positions of weights
strapped onto the legs or waist. In addition to these three
design factors, other variables were measured by filming each
trial with a high-speed motion camera. Ignore these other
measurements for this problem.
Format:
Running data frame with 24 observations on 13 variables.
[,1] subject factor subject identifier
[,2] order ordered order of treatment
[,3] speed factor running speed
[,4] stride numeric stride frequency
[,5] mass numeric attached mass in g
[,6] vo2 numeric oxygen consumption
[,7] tm numeric time
[,8] etot numeric etot
[,9] cn numeric cn
[,10] ar numeric ar
[,11] ecn numeric ecn
[,12] ahm numeric ahm
[,13] vhm numeric vhm

DATA
 	subject	order	speed	stride	mass	vo2	tm	etot	cn	ar	ecn	ahm	vhm
1	1	4	2.68	138	W	2838	71.8	221.14	0.381	0.053	0.260	15.2	13.7
2	1	8	2.68	138	L	2905	71.8	221.14	0.356	0.084	0.220	16.4	13.6
3	1	7	2.68	146	W	2630	71.8	221.14	0.359	0.056	0.225	15.0	12.7
4	1	3	2.68	146	L	2696	71.8	221.14	0.351	0.061	0.225	14.2	12.4
5	1	1	3.13	138	W	3100	71.8	270.69	0.335	0.099	0.285	13.5	14.2
6	1	6	3.13	138	L	3343	71.8	270.69	0.313	0.119	0.230	16.6	14.7
7	1	2	3.13	146	W	3001	71.8	270.69	0.344	0.065	0.225	12.0	13.5
8	1	5	3.13	146	L	3079	71.8	270.69	0.314	0.095	0.200	12.1	13.3
9	2	2	2.68	138	W	2782	76.8	236.54	0.403	0.038	0.240	13.6	11.1
10	2	8	2.68	138	L	2763	76.8	236.54	0.370	0.071	0.210	15.8	12.2
11	2	3	2.68	146	W	2582	76.8	236.54	0.363	0.048	0.215	14.3	11.0
12	2	5	2.68	146	L	2619	76.8	236.54	0.343	0.073	0.215	15.0	12.3
13	2	6	3.13	138	W	3277	76.8	289.54	0.364	0.076	0.230	11.7	11.7
14	2	1	3.13	138	L	3461	76.8	289.54	0.350	0.090	0.210	11.6	12.2
15	2	7	3.13	146	W	3116	76.8	289.54	0.341	0.071	0.215	12.3	11.6
16	2	4	3.13	146	L	3154	76.8	289.54	0.316	0.099	0.215	14.6	12.8
17	3	4	2.68	138	W	2665	69.8	214.98	0.316	0.120	0.245	16.7	14.5
18	3	7	2.68	138	L	2759	69.8	214.98	0.311	0.114	0.230	15.5	13.5
19	3	3	2.68	146	W	2504	69.8	214.98	0.289	0.124	0.225	19.0	14.2
20	3	5	2.68	146	L	2537	69.8	214.98	0.290	0.118	0.240	16.7	12.3
21	3	2	3.13	138	W	3077	69.8	263.15	0.294	0.138	0.250	14.4	15.4
22	3	1	3.13	138	L	3117	69.8	263.15	0.284	0.141	0.230	15.0	14.4
23	3	6	3.13	146	W	2816	69.8	263.15	0.271	0.137	0.210	14.6	13.7
24	3	8	3.13	146	L	2966	69.8	263.15	0.286	0.128	0.250	14.7	13.4



-- 
View this message in context: http://www.nabble.com/HELP...Running-data-tf3423283.html#a9541217
Sent from the R help mailing list archive at Nabble.com.


From kubovy at virginia.edu  Sun Mar 18 22:25:10 2007
From: kubovy at virginia.edu (Kubovy Michael)
Date: Sun, 18 Mar 2007 17:25:10 -0400
Subject: [R] HELP...Running data
In-Reply-To: <9541217.post@talk.nabble.com>
References: <9541217.post@talk.nabble.com>
Message-ID: <BF9CDCFC-E579-4404-85D0-0020771D2DAF@virginia.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070318/0c9ce49e/attachment.pl 

From JensScheidtmann at web.de  Sun Mar 18 21:52:53 2007
From: JensScheidtmann at web.de (Jens Scheidtmann)
Date: Sun, 18 Mar 2007 21:52:53 +0100
Subject: [R] RODBC Excel sqlQuery  insert into
References: <OF450B6A6F.98D186C9-ON8625729D.006181AD-8625729D.0065D5BB@americancentury.com>
Message-ID: <u1wjm8eai.fsf@grobi.scheidtmann.ath.cx>

toby_marks at americancentury.com writes:

> I have searched the archives for using insert into to update spreadsheets 
> using RODBC and have come up short. So, first off, is it possible?
>
> I have put together a dummy xls table (c:\foo.xls)for exploring 
> possibilities of RODBC.  Ultimately, I am interested in replacing much of 
> our previous use of vba macros with R ( I'd prefer elimination, but will 
> take what I can get ).  In order to achieve this, I will still have a need 
> to update spreadsheets directly through R scripts. 
>
> Simple queries seem to work fantastic!  But, I am missing something when 
> it comes to writing.
>
> sqlQuery( myChan,"insert into [my customers$] ( CUST_ID, NAME, SOCIAL ) 
> VALUES( 5,'robin',5678 ) " )
> [1] "[RODBC] ERROR: Could not SQLExecDirect"      "S1000 -3035 
> [Microsoft][ODBC Excel Driver] Operation must use an updateable query."

[...]
>   Driver={Microsoft Excel Driver (*.xls)}

[...]
>
> Additional reference: 
> http://www.microsoft.com/technet/scriptcenter/resources/officetips/jun05/tips0607.mspx

Your link describes the "Microsoft Jet OLE DB Provider" driver,
while your plain ODBC connections to Excel uses the "Microsoft OLE DB
Provider for ODBC Drivers" driver.

The documentation located here 
http://support.microsoft.com/?scid=kb%3Ben-us%3B257819&x=9&y=17
states:

> IMPORTANT: An ODBC connection to Excel is read-only by default. Your
> ADO Recordset LockType property setting does not override this
> connection-level setting. You must set ReadOnly to False in your
> connection string or your DSN configuration if you want to edit your
> data. Otherwise, you receive the following error message: Operation
> must use an updateable query.

Jens


From liuwensui at gmail.com  Sun Mar 18 23:00:40 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Sun, 18 Mar 2007 18:00:40 -0400
Subject: [R] any simple way to put comment with multiple lines
Message-ID: <1115a2b00703181500g611e3646ua5897bc0b21859ba@mail.gmail.com>

Dear Lister,
I understand I can put '#' to put comment line by line. But is there a
way to put comment with multiple lines without having to put '#' on
the every line?
Thanks.


From sixtease at gmail.com  Sun Mar 18 23:01:06 2007
From: sixtease at gmail.com (Oldrich Kruza)
Date: Sun, 18 Mar 2007 23:01:06 +0100
Subject: [R] training svm
In-Reply-To: <45E42771.50205@wu-wien.ac.at>
References: <45E42771.50205@wu-wien.ac.at>
Message-ID: <3f6548b10703181501vc8346e0sdb2fec596ac63735@mail.gmail.com>

Hello.

I managed to solve the problem, here's what I learned:

The columns in data passed to svm need to contain only numeral values.
I simply assigned a number to each category of each feature. However,
there must not be a column where all the numbers are equal (there
mustn't be a feature with always the same value), so don't try to use
bit-representation suitable for neural networks.

Many thanks to Hadley Wickham and David Meyer for help.

~ Sixtease

On 2/27/07, David Meyer <david.meyer at wu-wien.ac.at> wrote:
> Hello (whoever you are),
>
> your data looks problematic. What does
>
> head(ne_span_data)
>
> reveal?
>
> BTW, svm() will not handle NA values.
>
> Best
> David
>
> ---------------------
>
> Hello. I'm new to R and I'm trying to solve a classification problem. I have
> a training dataset of about 40,000 rows and 50 columns. When I try to train
> support vector machine, it gives me this error after a few seconds:
>
>   Error in predict.svm(ret, xhold) : Model is empty!
>
> This is the code I use:
>
>   ne_span_data <- as.matrix(read.table('ne_span.data.R.txt', header=TRUE,
> row.names='id'))
>   library('e1071')
>   svm_ne_span_model <- svm(NE_type ~ . , ne_span_data)
>
> it gives me:
> Error in predict.svm(ret, xhold) : Model is empty!
>
> A line from the ne_span.data.R.txt file:
>   svt OTHER N N I S 2 NA NA NA NA NA A NA NA 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
> 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 train-s1m2
>
>
>


From ggrothendieck at gmail.com  Sun Mar 18 23:10:51 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 18 Mar 2007 18:10:51 -0400
Subject: [R] any simple way to put comment with multiple lines
In-Reply-To: <1115a2b00703181500g611e3646ua5897bc0b21859ba@mail.gmail.com>
References: <1115a2b00703181500g611e3646ua5897bc0b21859ba@mail.gmail.com>
Message-ID: <971536df0703181510n4fd96021wfe864999b3345ee3@mail.gmail.com>

You could do this:

f <- function(x) {
"This is some
text that I
would like to
have here."
x + 1
}
f(2) # 3


On 3/18/07, Wensui Liu <liuwensui at gmail.com> wrote:
> Dear Lister,
> I understand I can put '#' to put comment line by line. But is there a
> way to put comment with multiple lines without having to put '#' on
> the every line?
> Thanks.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From g.abraham at ms.unimelb.edu.au  Sun Mar 18 23:36:15 2007
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Mon, 19 Mar 2007 09:36:15 +1100
Subject: [R] expm() within the Matrix package
In-Reply-To: <17914.23731.25729.616899@stat.math.ethz.ch>
References: <C21EEA50.DCA%lhill07@qub.ac.uk>
	<45F9CD54.8080806@ms.unimelb.edu.au>
	<17914.23731.25729.616899@stat.math.ethz.ch>
Message-ID: <45FDBEDF.3070806@ms.unimelb.edu.au>

>     Gad> If you convert to numeric, you can then assign it to Loglik:
>     >> Loglik[1] <- as.numeric(log(p %*% expm(Q * y[i]) %*% q))
>     >> Loglik[1]
>     Gad> [1] 134.5565
> 
> 
> Hmm, I don't think that's Laura's problem
> (and actually I don't know what her problem is) :
> 
> Assignment of a 1 x 1 matrix to a vector is not a problem,
> and hence the  as.numeric(.) above  really has no effect :
> 
>> ll <- 1:2
>> (m <- matrix(pi, 1,1))
>          [,1]
> [1,] 3.141593
>> ll[1] <- m
>> ll
> [1] 3.141593 2.000000

Ah but you're using 'matrix' while she's using 'Matrix' (AFAIK there is 
no expm for matrix):

 > library(Matrix)
Loading required package: lattice
 > m <- Matrix(1, nrow=1, ncol=1)
 > m
1 x 1 diagonal matrix of class "ddiMatrix"
      [,1]
[1,]    1
Warning message:
Ambiguous method selection for "diag", target "ddiMatrix" (the first of 
the signatures shown will be used)
     diagonalMatrix
     ddenseMatrix
  in: .findInheritedMethods(classes, fdef, mtable)
 > a <- vector("numeric", 0)
 > a[1] <- m
Error in a[1] <- m : incompatible types (from S4 to double) in 
subassignment type fix
 > a[1] <- as.numeric(m)
 > a
[1] 1


 > sessionInfo()
R version 2.4.1 (2006-12-18)
i486-pc-linux-gnu

locale:
LC_CTYPE=en_AU.UTF-8;LC_NUMERIC=C;LC_TIME=en_AU.UTF-8;LC_COLLATE=en_AU.UTF-8;LC_MONETARY=en_AU.UTF-8;LC_MESSAGES=en_AU.UTF-8;LC_PAPER=en_AU.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_AU.UTF-8;LC_IDENTIFICATION=C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
[7] "base"

other attached packages:
      Matrix     lattice
"0.9975-11"   "0.14-16"

-- 
Gad Abraham
Department of Mathematics and Statistics
The University of Melbourne
Parkville 3010, Victoria, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From david.meyer at wu-wien.ac.at  Sun Mar 18 23:53:37 2007
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Sun, 18 Mar 2007 23:53:37 +0100
Subject: [R] training svm
In-Reply-To: <3f6548b10703181501vc8346e0sdb2fec596ac63735@mail.gmail.com>
References: <45E42771.50205@wu-wien.ac.at>
	<3f6548b10703181501vc8346e0sdb2fec596ac63735@mail.gmail.com>
Message-ID: <45FDC2F1.8090603@wu-wien.ac.at>

Oldrich:

> 
> The columns in data passed to svm need to contain only numeral values.

This is not correct, svm() of course also accepts factors and then 
builds a model matrix similar to lm(). But it won't accept, e.g., 
character vectors.

> I simply assigned a number to each category of each feature. However,
> there must not be a column where all the numbers are equal 

yes, since the intercept is always included in svm models anyway.

Best
David


From danmetes at hotmail.com  Mon Mar 19 05:54:23 2007
From: danmetes at hotmail.com (dan metes)
Date: Mon, 19 Mar 2007 06:54:23 +0200
Subject: [R] How to specify Variance Covariance matrix of residuals?
Message-ID: <BAY105-F231829DA45C58A3A8133F4CE760@phx.gbl>


       Hi guys! I have a problem regarding a binary logistic hierarchical 
model I am trying to use. The model contains various covariates that depend 
on the location the response was measured at but do not depend on time 
(year). I also have a spatial covariate that depends both on location and 
time. I have been trying to use the lme4 pack but the package only allows me 
to model variance covariance information for random effects. What I am 
interested in is to actually specify a variance covariance matrix of the 
residuals within year that would describe the unexplained spatial dependence 
of the errors within each year. I had a look at the nmle pack in Splus and 
it appears that the nmle function in that package is able to describe such a 
var-covar matrix via the var.function option. So I was wondering if lmer can 
do that in R.
       I also looked at the MCMCpack in R since I might decide to actually 
use Bayesian modeling when using my hierarchical model. From what I saw 
logistic regression can be dealth with using this package but I'm not sure 
if hierarchies can be specified, or if residuals can be given a variance 
covariance structure.
       I included my model below: (l - location index, t-time index)

                     Y[l,t] | p[l,t] ~ Bernoulli (p[l,t])

    logit( p[l,t] | SpTimeCov, X1,...,Xp) = B0 + B1*X1[l]...+ Bp*Xp[l] + 
A*SpTimeCov[l,t] + Err[l,t]

                     Err[1:L,t] ~ MVN(0, V)

            where V is an L*L variance covariance matrix of the residuals 
that I have to specify.

        I would really appreciate if you guys had any suggestions as to what 
package I should use in R (since I don't really have access to Splus) and if 
I can use the MCMC pack later on if I decide to modify my model so that I 
can use Bayesian methodology together with the residual structure in the 
above model. Thank you very much!


Sincerely,
Dan Metes
University of Alberta.


From maechler at stat.math.ethz.ch  Mon Mar 19 08:37:06 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 19 Mar 2007 08:37:06 +0100
Subject: [R] expm() within the Matrix package
In-Reply-To: <45FDBEDF.3070806@ms.unimelb.edu.au>
References: <C21EEA50.DCA%lhill07@qub.ac.uk>
	<45F9CD54.8080806@ms.unimelb.edu.au>
	<17914.23731.25729.616899@stat.math.ethz.ch>
	<45FDBEDF.3070806@ms.unimelb.edu.au>
Message-ID: <17918.15778.942685.164514@stat.math.ethz.ch>

>>>>> "Gad" == Gad Abraham <g.abraham at ms.unimelb.edu.au>
>>>>>     on Mon, 19 Mar 2007 09:36:15 +1100 writes:

    Gad> If you convert to numeric, you can then assign it to Loglik:
    >> >> Loglik[1] <- as.numeric(log(p %*% expm(Q * y[i]) %*% q))
    >> >> Loglik[1]
    Gad> [1] 134.5565
    >> 
    >> 
    >> Hmm, I don't think that's Laura's problem
    >> (and actually I don't know what her problem is) :
    >> 
    >> Assignment of a 1 x 1 matrix to a vector is not a problem,
    >> and hence the  as.numeric(.) above  really has no effect :
    >> 
    >>> ll <- 1:2
    >>> (m <- matrix(pi, 1,1))
    >> [,1]
    >> [1,] 3.141593
    >>> ll[1] <- m
    >>> ll
    >> [1] 3.141593 2.000000

    Gad> Ah but you're using 'matrix' while she's using 'Matrix'
    Gad> (AFAIK there is no expm for matrix):


Yes, of course, you are absolutely right
(and I'm pretty embarrassed).

Martin

    >> library(Matrix)

    Gad> Loading required package: lattice
    >> m <- Matrix(1, nrow=1, ncol=1)
    >> m
    Gad> 1 x 1 diagonal matrix of class "ddiMatrix"
    Gad> [,1]
    Gad> [1,]    1
    Gad> Warning message:
    Gad> Ambiguous method selection for "diag", target "ddiMatrix" (the first of 
    Gad> the signatures shown will be used)
    Gad> diagonalMatrix
    Gad> ddenseMatrix
    Gad> in: .findInheritedMethods(classes, fdef, mtable)
    >> a <- vector("numeric", 0)
    >> a[1] <- m
    Gad> Error in a[1] <- m : incompatible types (from S4 to double) in 
    Gad> subassignment type fix
    >> a[1] <- as.numeric(m)
    >> a
    Gad> [1] 1

    [.........]


From ripley at stats.ox.ac.uk  Mon Mar 19 08:48:14 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 19 Mar 2007 07:48:14 +0000 (GMT)
Subject: [R] Setting site-wide default CRAN repository?
In-Reply-To: <6262c54c0703181257y674f3accic6b82c9aa4f72dae@mail.gmail.com>
References: <6262c54c0703181257y674f3accic6b82c9aa4f72dae@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0703190743470.8701@gannet.stats.ox.ac.uk>

On Sun, 18 Mar 2007, Tim Keitt wrote:

> I can't seem to find this anywhere. How do I set the default CRAN
> repository _site wide_ on a linux box? What I want to do is eliminate
> the pop-up list of repository locations when using
> 'install.packages()'. I know how to do this for a single account.
> Modifying files in /etc/R does not seem to work. (cc me please - I
> think I'm not subscribed).

Modifying files in R_HOME/etc works, but I have no idea why you think 
/etc/R is relevant.

The simplest way is to replace @CRAN@ in R_HOME/etc/repositories.  (See 
?setRepositories.)

Almost as simple is to set e.g.

 	options(repos=c(CRAN="http://cran.us.r-project.org"))

in R_HOME/etc/Rprofile.site (see ?Startup).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From p.hiemstra at geo.uu.nl  Mon Mar 19 09:52:20 2007
From: p.hiemstra at geo.uu.nl (Paul Hiemstra)
Date: Mon, 19 Mar 2007 09:52:20 +0100
Subject: [R] any simple way to put comment with multiple lines
In-Reply-To: <1115a2b00703181500g611e3646ua5897bc0b21859ba@mail.gmail.com>
References: <1115a2b00703181500g611e3646ua5897bc0b21859ba@mail.gmail.com>
Message-ID: <45FE4F44.9060108@geo.uu.nl>

Wensui Liu schreef:
> Dear Lister,
> I understand I can put '#' to put comment line by line. But is there a
> way to put comment with multiple lines without having to put '#' on
> the every line?
> Thanks.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   
Dear Wensui,

This could suit your needs:

if(FALSE) {
    code
    you
    don't want
    executed
}

hope this helps and kind regards,

Paul

-- 
Drs. Paul Hiemstra
Department of Physical Geography
Faculty of Geosciences
University of Utrecht
Heidelberglaan 2
P.O. Box 80.115
3508 TC Utrecht
Phone: 	+31302535773
Fax:	+31302531145
http://intamap.geo.uu.nl/~paul


From tord.snall at nvb.slu.se  Mon Mar 19 10:05:47 2007
From: tord.snall at nvb.slu.se (=?ISO-8859-1?Q?Tord_Sn=E4ll?=)
Date: Mon, 19 Mar 2007 10:05:47 +0100
Subject: [R] order of values in vector
Message-ID: <45FE526B.5080007@nvb.slu.se>

Dear all,
I would like to get the order of the values in a vector. I have tried 
rank(), order() and searched the archive, though without success.

Here is an example of a try
x= c(20,30,50,40,60,10)
cbind(sort.list(x),x)
        x
[1,] 6 20
[2,] 1 30
[3,] 2 50
[4,] 4 40
[5,] 3 60
[6,] 5 10
but I was hoping to get this:
        x
[1,] 2 20
[2,] 3 30
[3,] 5 50
[4,] 4 40
[5,] 6 60
[6,] 1 10

I'm most grateful for a tip!

cheers,
Tord

-- 
Tord Sn?ll
Department of Ecology
Swedish University of Agricultural Sciences (SLU)
P.O. 7002, SE-750 07 Uppsala, Sweden
Office/Mobile/Fax
+46-18-672612/+46-730-891356/+46-18-673537
E-mail: tord.snall at nvb.slu.se
www.nvb.slu.se/staff_tordsnall


From sea at sao.ru  Mon Mar 19 09:39:22 2007
From: sea at sao.ru (Eugene Semenko)
Date: Mon, 19 Mar 2007 11:39:22 +0300
Subject: [R] ticks labels and R
In-Reply-To: <1174239532.28942.41.camel@localhost.localdomain>
References: <200703181936.52571.sea@sao.ru>
	<1174239532.28942.41.camel@localhost.localdomain>
Message-ID: <200703191139.22502.sea@sao.ru>

? ????????? ?? Sunday 18 March 2007 20:38 Marc Schwartz ???????(a):
> Eugene, see ?plotmath for more information.
> at <- seq(-360, 360, 60)
>
> plot(at, at, axes = FALSE)
>
> # Now create expressions with the degree symbol
> # See ?parse and ?paste
> L <- parse(text = paste(at, "*degree", sep = ""))
>
> > L
>
> expression(-360 * degree, -300 * degree, -240 * degree, -180 *
>     degree, -120 * degree, -60 * degree, 0 * degree, 60 * degree,
>     120 * degree, 180 * degree, 240 * degree, 300 * degree, 360 *
>         degree)
>
> # now do the axis labels
> axis(1, at = at, labels = L, cex.axis = 0.75)
>
> axis(2, at = at, labels = L, cex.axis = 0.75, las = 2)
>
>
> See ?axis as well.
	Hi Mark. Thanks for operative answer. It works.
-- 
---------------------------------------------------------------------------------------
Eugene A. Semenko
e-mail: sea at sao.ru              Special Astrophysical Observatory RAS,
WWW: http://tiger.sao.ru/       Nizhnij Arkhyz
phone: +7 87878 46 5 77         Karachai-Chercassian Republic,
fax: +7 87878 46 5 27           Russia, 369167
cell: +7 928 810 48 61
ICQ: 167727721


From Roger.Bivand at nhh.no  Mon Mar 19 10:15:18 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 19 Mar 2007 10:15:18 +0100 (CET)
Subject: [R] order of values in vector
In-Reply-To: <45FE526B.5080007@nvb.slu.se>
Message-ID: <Pine.LNX.4.44.0703191015010.18926-100000@reclus.nhh.no>

On Mon, 19 Mar 2007, Tord Sn?ll wrote:

> Dear all,
> I would like to get the order of the values in a vector. I have tried 
> rank(), order() and searched the archive, though without success.
> 
> Here is an example of a try
> x= c(20,30,50,40,60,10)
> cbind(sort.list(x),x)
>         x
> [1,] 6 20
> [2,] 1 30
> [3,] 2 50
> [4,] 4 40
> [5,] 3 60
> [6,] 5 10
> but I was hoping to get this:
>         x
> [1,] 2 20
> [2,] 3 30
> [3,] 5 50
> [4,] 4 40
> [5,] 6 60
> [6,] 1 10

> cbind(rank(x), x)
        x
[1,] 2 20
[2,] 3 30
[3,] 5 50
[4,] 4 40
[5,] 6 60
[6,] 1 10


> 
> I'm most grateful for a tip!
> 
> cheers,
> Tord
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From ccleland at optonline.net  Mon Mar 19 10:15:32 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 19 Mar 2007 05:15:32 -0400
Subject: [R] order of values in vector
In-Reply-To: <45FE526B.5080007@nvb.slu.se>
References: <45FE526B.5080007@nvb.slu.se>
Message-ID: <45FE54B4.7070007@optonline.net>

Tord Sn?ll wrote:
> Dear all,
> I would like to get the order of the values in a vector. I have tried 
> rank(), order() and searched the archive, though without success.
> 
> Here is an example of a try
> x= c(20,30,50,40,60,10)
> cbind(sort.list(x),x)
>         x
> [1,] 6 20
> [2,] 1 30
> [3,] 2 50
> [4,] 4 40
> [5,] 3 60
> [6,] 5 10
> but I was hoping to get this:
>         x
> [1,] 2 20
> [2,] 3 30
> [3,] 5 50
> [4,] 4 40
> [5,] 6 60
> [6,] 1 10
> 
> I'm most grateful for a tip!

cbind(rank(x), x)
        x
[1,] 2 20
[2,] 3 30
[3,] 5 50
[4,] 4 40
[5,] 6 60
[6,] 1 10

> cheers,
> Tord 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From r.hankin at noc.soton.ac.uk  Mon Mar 19 10:18:44 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 19 Mar 2007 09:18:44 +0000
Subject: [R] character to numeric conversion
Message-ID: <42166F0A-DE60-4141-8F98-A3B136B8C071@soc.soton.ac.uk>

Hi.

Is there a straightforward way to convert a character string  
containing comma-delimited
numbers  to a numeric vector?

In my application, I use

system(executable.string, intern=TRUE)

which returns a string like

"[0.E-38, 2.096751179214927596171268230,  
3.678944959657480671183123052, 4.976528845643001020345216157,  
6.072390165503099343887569007, 7.007958550337542210168866070,  
7.807464185827177139302778736, 8.486139455817034846608029724,  
9.053706780665060873259065771, 9.516172308326877463284426111,  
9.876856047379733199590985269, 10.13695826383869052536062804,  
10.29580989588667234885515374, 10.35092785255025551187463209,  
10.29795676261278695909972578, 10.13052574735986793562227138,  
9.839990935943625006580521345, 9.414977153151389385186358494,  
8.840562526759586215404890348, 8.096830792651667245232639586,  
7.156244887881612948153311800, 5.978569259122249264778017262,  
4.499809670330265066808481929, 2.602689685444383764768503589, 0.E-38]"


(the output is a single line).   In a big run, the string may contain  
10^5 or possibly 10^6 numbers.

What's the recommended way to convert this to a numeric vector?






--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From statba at nus.edu.sg  Mon Mar 19 10:20:23 2007
From: statba at nus.edu.sg (Berwin A Turlach)
Date: Mon, 19 Mar 2007 17:20:23 +0800
Subject: [R] order of values in vector
In-Reply-To: <45FE526B.5080007@nvb.slu.se>
References: <45FE526B.5080007@nvb.slu.se>
Message-ID: <20070319172023.253764c1@berwin5>

G'day Tord,

On Mon, 19 Mar 2007 10:05:47 +0100
Tord Sn?ll <tord.snall at nvb.slu.se> wrote:

> but I was hoping to get this:
>         x
> [1,] 2 20
> [2,] 3 30
> [3,] 5 50
> [4,] 4 40
> [5,] 6 60
> [6,] 1 10

You did try rank(), didn't you?

> x= c(20,30,50,40,60,10)
> cbind(rank(x), x)
        x
[1,] 2 20
[2,] 3 30
[3,] 5 50
[4,] 4 40
[5,] 6 60
[6,] 1 10

HTH.

Cheers,

	Berwin


From r.hankin at noc.soton.ac.uk  Mon Mar 19 10:26:14 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 19 Mar 2007 09:26:14 +0000
Subject: [R] order of values in vector
In-Reply-To: <45FE526B.5080007@nvb.slu.se>
References: <45FE526B.5080007@nvb.slu.se>
Message-ID: <A560EA59-9864-4594-A710-92653B4C6768@soc.soton.ac.uk>

Tord


 > x <-  c(20,30,50,40,60,10)
 > cbind(rank(x),x)
         x
[1,] 2 20
[2,] 3 30
[3,] 5 50
[4,] 4 40
[5,] 6 60
[6,] 1 10
 >



HTH

rksh



On 19 Mar 2007, at 09:05, Tord Sn?ll wrote:



Dear all,
I would like to get the order of the values in a vector. I have tried
rank(), order() and searched the archive, though without success.

Here is an example of a try
x= c(20,30,50,40,60,10)
cbind(sort.list(x),x)
         x
[1,] 6 20
[2,] 1 30
[3,] 2 50
[4,] 4 40
[5,] 3 60
[6,] 5 10
but I was hoping to get this:
         x
[1,] 2 20
[2,] 3 30
[3,] 5 50
[4,] 4 40
[5,] 6 60
[6,] 1 10

I'm most grateful for a tip!

cheers,
Tord


>

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From FredeA.Togersen at agrsci.dk  Mon Mar 19 10:37:47 2007
From: FredeA.Togersen at agrsci.dk (=?iso-8859-1?Q?Frede_Aakmann_T=F8gersen?=)
Date: Mon, 19 Mar 2007 10:37:47 +0100
Subject: [R] CPU usage on Windows
References: <4f7636bf0703161556sda5b390ta748f9d3d797ddae@mail.gmail.com>
	<45FB2A2A.3040103@stats.uwo.ca>
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC04E87DF3@DJFPOST01.djf.agrsci.dk>

(cc'ing ESS mailing list to which this belongs) 
 
I have been using Emacs, ESS, and R on Windows for several years now. It works all together to allmost full satisfaction. Thanks to all people providing these tools.
 
I haven't noticed this specific problem, but I guess it is related to what I see from time to time when using R, Emacs and ESS to do graphics. I am not really into all the details to how Emacs and R communicates through ESS (using ddeclinet) but here is something that perhaps can provide useful information to more skilled persons.
 
I think it has to do with the way that R og Emacs communicates through ddeclient. Normally Emacs waits for R to finish by looking for the next command prompt from R indicating that R is finished with the commands submitted to R by Emacs/ESS. This means that Emacs can be busy waiting for R to finish which is most noticeable when submitting chunks of commands using e.g. C-c C-r (submit a region). Emacs/ESS then waits for R to finish each command in the region waiting for a signal on a clear command prompt before submitting the next command. This can be overriden by using C-c A-r (A = meta (unix) or Alt (windows)).
 
What I have noticed is that when I have made a plot and R creates the graphics windows. Then R og and the graphics windows from time to time hang (not being updated) when I want to focus on the graphics window (to put in front) subsequently. In this situation it helps to go to the buffer with the inferior ESS process and do a carriage return, which immediately frees the graphics windows and now it is getting updated when put in front.
 
This behaviour which I believe is due to the way that Emacs and R communicates over ddeclient is not persistent and until now I have hesitated to report it because I couldn't provide a small reproducable example. I have noticed this over several versions of Emacs (21.2 - 22.1), ESS (< 5 - 5.4) and Windows 98/NT/XP. 
 
 
Frede Aakmann T?gersen
Forsker / Scientist


 	
 	 AARHUS UNIVERSITET / UNIVERSITY OF AARHUS	
Det Jordbrugsvidenskabelige Fakultet / Faculty of Agricultural Sciences	
Forskningscenter Foulum / Research Centre Foulum	
Genetik og Bioteknologi / Dept. of Genetics and Biotechnology	
Blichers All? 20, P.O. BOX 50	
DK-8830 Tjele	
 	
Tel:	 +45 8999 1900	
Direct:	 +45 8999 1878	
Mobile:	 +45 	
E-mail:	 FredeA.Togersen at agrsci.dk <mailto:FredeA.Togersen at agrsci.dk> 	
Web:	 www.agrsci.dk <https://djfpost.agrsci.dk/exchweb/bin/redir.asp?URL=http://www.agrsci.dk/> 	

________________________________

Fra: r-help-bounces at stat.math.ethz.ch p? vegne af Duncan Murdoch
Sendt: l? 17-03-2007 00:37
Til: Jonathan Wang
Cc: r-help at stat.math.ethz.ch
Emne: Re: [R] CPU usage on Windows



On 3/16/2007 6:56 PM, Jonathan Wang wrote:
> I'm using R with emacs & ESS on Windows. When I create a plot, sometimes R
> will seem to get stuck in a busy loop, i.e. it will use 100% of my CPU.
> However the system is still somewhat responsive, and the problem usually
> goes away if I create a new device with windows(). If I then close this
> device, making the first device active again, sometimes R will get stuck in
> the busy loop again.
>
> Has anybody heard of this behavior, or, better yet, have a solution?

I've heard of a number of problems with Emacs on Windows.  I wouldn't
recommend using it.  As far as I can see, it makes a number of
assumptions about the OS that just aren't true about Windows.

If you can reproduce the behaviour outside of Emacs, I'll investigate.

Duncan Murdoch

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dimitris.rizopoulos at med.kuleuven.be  Mon Mar 19 10:47:13 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 19 Mar 2007 10:47:13 +0100
Subject: [R] character to numeric conversion
References: <42166F0A-DE60-4141-8F98-A3B136B8C071@soc.soton.ac.uk>
Message-ID: <007601c76a0b$921c6f30$0540210a@www.domain>

you could give a try to strsplit(), e.g.,

strg <- "0.E-38, 2.096751179214927596171268230, 
3.678944959657480671183123052"
strg <- paste(rep(strg, 5000), collapse = ", ")
##################
f.out <- factor(strsplit(strg, ", ")[[1]])
n.out <- as.numeric(levels(f.out))[as.integer(f.out)]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Robin Hankin" <r.hankin at noc.soton.ac.uk>
To: "RHelp help" <r-help at stat.math.ethz.ch>
Sent: Monday, March 19, 2007 10:18 AM
Subject: [R] character to numeric conversion


> Hi.
>
> Is there a straightforward way to convert a character string
> containing comma-delimited
> numbers  to a numeric vector?
>
> In my application, I use
>
> system(executable.string, intern=TRUE)
>
> which returns a string like
>
> "[0.E-38, 2.096751179214927596171268230,
> 3.678944959657480671183123052, 4.976528845643001020345216157,
> 6.072390165503099343887569007, 7.007958550337542210168866070,
> 7.807464185827177139302778736, 8.486139455817034846608029724,
> 9.053706780665060873259065771, 9.516172308326877463284426111,
> 9.876856047379733199590985269, 10.13695826383869052536062804,
> 10.29580989588667234885515374, 10.35092785255025551187463209,
> 10.29795676261278695909972578, 10.13052574735986793562227138,
> 9.839990935943625006580521345, 9.414977153151389385186358494,
> 8.840562526759586215404890348, 8.096830792651667245232639586,
> 7.156244887881612948153311800, 5.978569259122249264778017262,
> 4.499809670330265066808481929, 2.602689685444383764768503589, 
> 0.E-38]"
>
>
> (the output is a single line).   In a big run, the string may 
> contain
> 10^5 or possibly 10^6 numbers.
>
> What's the recommended way to convert this to a numeric vector?
>
>
>
>
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From b.rowlingson at lancaster.ac.uk  Mon Mar 19 10:52:07 2007
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 19 Mar 2007 09:52:07 +0000
Subject: [R] Setting site-wide default CRAN repository?
In-Reply-To: <Pine.LNX.4.64.0703190743470.8701@gannet.stats.ox.ac.uk>
References: <6262c54c0703181257y674f3accic6b82c9aa4f72dae@mail.gmail.com>
	<Pine.LNX.4.64.0703190743470.8701@gannet.stats.ox.ac.uk>
Message-ID: <45FE5D47.7020507@lancaster.ac.uk>

Prof Brian Ripley wrote:
> On Sun, 18 Mar 2007, Tim Keitt wrote:
> 
>> I can't seem to find this anywhere. How do I set the default CRAN
>> repository _site wide_ on a linux box? What I want to do is eliminate
>> the pop-up list of repository locations when using
>> 'install.packages()'. I know how to do this for a single account.
>> Modifying files in /etc/R does not seem to work. (cc me please - I
>> think I'm not subscribed).
> 
> Modifying files in R_HOME/etc works, but I have no idea why you think 
> /etc/R is relevant.
> 

  Looks like some distributions package R with that as a configuration 
directory, for example on my Ubuntu Edgy box:

$ ls -l /etc/R
total 12
-rw-r--r-- 1 root root 2561 2006-06-20 07:32 Makeconf
-rw-r--r-- 1 root root 1422 2006-12-05 16:30 Renviron
-rw-r--r-- 1 root root  605 2006-06-20 07:32 repositories

This is from an R-base-core package maintained by Dirk Eddelbuettel from 
the 'Universe' repository (according to the package meta-data).

  I've since gone from the R 2.3.1 installed by this package to a 
compiled 2.4.x, so /etc/R/ is no use to me now - perhaps you've done 
this too (Tim)?

  Barry


From P.Dalgaard at biostat.ku.dk  Mon Mar 19 10:49:32 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 19 Mar 2007 10:49:32 +0100
Subject: [R] character to numeric conversion
In-Reply-To: <42166F0A-DE60-4141-8F98-A3B136B8C071@soc.soton.ac.uk>
References: <42166F0A-DE60-4141-8F98-A3B136B8C071@soc.soton.ac.uk>
Message-ID: <45FE5CAC.4060605@biostat.ku.dk>

Robin Hankin wrote:
> Hi.
>
> Is there a straightforward way to convert a character string  
> containing comma-delimited
> numbers  to a numeric vector?
>
> In my application, I use
>
> system(executable.string, intern=TRUE)
>
> which returns a string like
>
> "[0.E-38, 2.096751179214927596171268230,  
> 3.678944959657480671183123052, 4.976528845643001020345216157,  
> 6.072390165503099343887569007, 7.007958550337542210168866070,  
> 7.807464185827177139302778736, 8.486139455817034846608029724,  
> 9.053706780665060873259065771, 9.516172308326877463284426111,  
> 9.876856047379733199590985269, 10.13695826383869052536062804,  
> 10.29580989588667234885515374, 10.35092785255025551187463209,  
> 10.29795676261278695909972578, 10.13052574735986793562227138,  
> 9.839990935943625006580521345, 9.414977153151389385186358494,  
> 8.840562526759586215404890348, 8.096830792651667245232639586,  
> 7.156244887881612948153311800, 5.978569259122249264778017262,  
> 4.499809670330265066808481929, 2.602689685444383764768503589, 0.E-38]"
>
>
> (the output is a single line).   In a big run, the string may contain  
> 10^5 or possibly 10^6 numbers.
>
> What's the recommended way to convert this to a numeric vector?
>
>   
scan() on a text connection:

> x <- "[0.E-38, 2.096751179214927596171268230,
+ 3.678944959657480671183123052, 4.976528845643001020345216157,
+ 6.072390165503099343887569007, 7.007958550337542210168866070,
+ 7.807464185827177139302778736, 8.486139455817034846608029724,
+ 9.053706780665060873259065771, 9.516172308326877463284426111,
+ 9.876856047379733199590985269, 10.13695826383869052536062804,
+ 10.29580989588667234885515374, 10.35092785255025551187463209,
+ 10.29795676261278695909972578, 10.13052574735986793562227138,
+ 9.839990935943625006580521345, 9.414977153151389385186358494,
+ 8.840562526759586215404890348, 8.096830792651667245232639586,
+ 7.156244887881612948153311800, 5.978569259122249264778017262,
+ 4.499809670330265066808481929, 2.602689685444383764768503589, 0.E-38]"
> tc <- textConnection(gsub("[][ \n]","",x))
> xx <- scan(tc,sep=",")
Read 25 items
> summary(xx)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  0.000   4.977   8.097   7.049   9.840  10.350
> close(tc)

(By far, the hardest bit was getting the gsub regexp right...)

Alternatively, just get rid of the brackets and replace commas with
whitespace. A problem with sep="," is that it gets confused by line
endings following a comma.

> tc <- textConnection(gsub(",", " ", gsub("[][]", "", x)))
> xx <- scan(tc)
Read 25 items
> summary(xx)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  0.000   4.977   8.097   7.049   9.840  10.350
> close(tc)



>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From carlosguerra at esa.ipvc.pt  Mon Mar 19 10:57:06 2007
From: carlosguerra at esa.ipvc.pt (Carlos Guerra)
Date: Mon, 19 Mar 2007 09:57:06 +0000
Subject: [R] matrix similarity comparison
Message-ID: <45FE5E72.1030707@esa.ipvc.pt>

Good morning to you all,

I have a problem with a set of matrices that I want to compare.

I want to see the similarity between them, and to be able to extract the 
differences between them.

They have all the same number of columns and rows, and correspond 
presence absence data:

for example:

m1 <- matrix(c(1,0,0,0,1,0,1,1,1,1,1,1), 3,4)
m2 <- matrix(c(1,0,1,0,1,0,0,1,0,1,0,1), 3,4)

I tried with the function cor2m() [package=edodist] but it didn't worked 
and my matrices are much bigger than the ones from the example.

Thank you,

Carlos

-- 
Carlos GUERRA

Gabinete de Sistemas de Informacao Geografica
Escola Superior Agraria de Ponte de Lima
Mosteiro de Refoios do Lima
4990-706 Ponte de Lima

Tlm: +351 91 2407109
Tlf: +351 258 909779

Reclaim your Inbox...!!!
http://www.mozilla.org/products/thunderbird/


From r.hankin at noc.soton.ac.uk  Mon Mar 19 12:20:25 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 19 Mar 2007 11:20:25 +0000
Subject: [R] character to numeric conversion
In-Reply-To: <45FE5CAC.4060605@biostat.ku.dk>
References: <42166F0A-DE60-4141-8F98-A3B136B8C071@soc.soton.ac.uk>
	<45FE5CAC.4060605@biostat.ku.dk>
Message-ID: <9507CC50-ED1E-4135-98F5-4FF70567A0BF@soc.soton.ac.uk>

Hello everybody

thanks for the tips.

I *think* this should be the same thread....


The manpage for system() says that lines of over 8095
characters will be split.  This is causing me problems.
How do I get round the 8095 character limit?


Simple toy example follows:



jj <- system("echo 4 | awk '{for(i=1;i<100;i++){printf(\"%s,\", 
$1)}}'| sed -e \"s/,$//\"",intern=T)


This is  fine.  But .. . .


jj <- system("echo 4 | awk '{for(i=1;i<10000;i++){printf(\"%s,\", 
$1)}}'| sed -e \"s/,$//\"",intern=T)



has  "jj"  split into three bits, which is upsetting my call.  In my  
application
the split occurs in the middle of a multi-digit number, which messes up
my conversion to numeric?







--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From r.hankin at noc.soton.ac.uk  Mon Mar 19 12:30:18 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 19 Mar 2007 11:30:18 +0000
Subject: [R] character to numeric conversion
In-Reply-To: <9507CC50-ED1E-4135-98F5-4FF70567A0BF@soc.soton.ac.uk>
References: <42166F0A-DE60-4141-8F98-A3B136B8C071@soc.soton.ac.uk>
	<45FE5CAC.4060605@biostat.ku.dk>
	<9507CC50-ED1E-4135-98F5-4FF70567A0BF@soc.soton.ac.uk>
Message-ID: <D63852D8-9A3E-4A73-9276-8627BE868D6C@soc.soton.ac.uk>


On 19 Mar 2007, at 11:20, Robin Hankin wrote:

> Hello everybody
>
> thanks for the tips.
>
> I *think* this should be the same thread....
>
>
> The manpage for system() says that lines of over 8095
> characters will be split.  This is causing me problems.
> How do I get round the 8095 character limit?
>


Er, just paste the output together using paste(..., collapse = "")

The split is "clean" so concatenating the lines will not lose
any characters.


HTH






--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From Roger.Bivand at nhh.no  Mon Mar 19 12:34:26 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 19 Mar 2007 12:34:26 +0100 (CET)
Subject: [R] character to numeric conversion
In-Reply-To: <9507CC50-ED1E-4135-98F5-4FF70567A0BF@soc.soton.ac.uk>
Message-ID: <Pine.LNX.4.44.0703191232090.18926-100000@reclus.nhh.no>

On Mon, 19 Mar 2007, Robin Hankin wrote:

> Hello everybody
> 
> thanks for the tips.
> 
> I *think* this should be the same thread....
> 
> 
> The manpage for system() says that lines of over 8095
> characters will be split.  This is causing me problems.
> How do I get round the 8095 character limit?
> 

Can you use sed or awk in a pipe externally to change ", " into "\n" while
still out in the system() call, for example the record separator RS in
awk?

> 
> Simple toy example follows:
> 
> 
> 
> jj <- system("echo 4 | awk '{for(i=1;i<100;i++){printf(\"%s,\", 
> $1)}}'| sed -e \"s/,$//\"",intern=T)
> 
> 
> This is  fine.  But .. . .
> 
> 
> jj <- system("echo 4 | awk '{for(i=1;i<10000;i++){printf(\"%s,\", 
> $1)}}'| sed -e \"s/,$//\"",intern=T)
> 
> 
> 
> has  "jj"  split into three bits, which is upsetting my call.  In my  
> application
> the split occurs in the middle of a multi-digit number, which messes up
> my conversion to numeric?
> 
> 
> 
> 
> 
> 
> 
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From ggrothendieck at gmail.com  Mon Mar 19 12:39:02 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 19 Mar 2007 07:39:02 -0400
Subject: [R] character to numeric conversion
In-Reply-To: <42166F0A-DE60-4141-8F98-A3B136B8C071@soc.soton.ac.uk>
References: <42166F0A-DE60-4141-8F98-A3B136B8C071@soc.soton.ac.uk>
Message-ID: <971536df0703190439hfbc41c4y28d8ea8f9d4dfffe@mail.gmail.com>

Here is one way.  This matches strings which contain those characters
found in a number, converting each such string to numeric.

library(gsubfn)
strapply(x, "[-0-9+.E]+", as.numeric)


On 3/19/07, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
> Hi.
>
> Is there a straightforward way to convert a character string
> containing comma-delimited
> numbers  to a numeric vector?
>
> In my application, I use
>
> system(executable.string, intern=TRUE)
>
> which returns a string like
>
> "[0.E-38, 2.096751179214927596171268230,
> 3.678944959657480671183123052, 4.976528845643001020345216157,
> 6.072390165503099343887569007, 7.007958550337542210168866070,
> 7.807464185827177139302778736, 8.486139455817034846608029724,
> 9.053706780665060873259065771, 9.516172308326877463284426111,
> 9.876856047379733199590985269, 10.13695826383869052536062804,
> 10.29580989588667234885515374, 10.35092785255025551187463209,
> 10.29795676261278695909972578, 10.13052574735986793562227138,
> 9.839990935943625006580521345, 9.414977153151389385186358494,
> 8.840562526759586215404890348, 8.096830792651667245232639586,
> 7.156244887881612948153311800, 5.978569259122249264778017262,
> 4.499809670330265066808481929, 2.602689685444383764768503589, 0.E-38]"
>
>
> (the output is a single line).   In a big run, the string may contain
> 10^5 or possibly 10^6 numbers.
>
> What's the recommended way to convert this to a numeric vector?
>
>
>
>
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From norbertneuwirth at gmx.at  Mon Mar 19 13:20:57 2007
From: norbertneuwirth at gmx.at (Norbert NEUWIRTH)
Date: Mon, 19 Mar 2007 13:20:57 +0100
Subject: [R] question on table format
Message-ID: <op.tpfq87ki4tgxb7@nn-notebook>

hi everybody,

i am hanging in a problem that should be solved quickly, but, in fact, i do not get further: building up a SUR/SES-system myself, where the regressions are finally used to be poisson and/or negative-binomial type, i want to generate a simple table, where i can quickly see the coefficients and their markers  for significance.

so i constructed this "final.table" (code & outout below). as the  markers for significance are character-type, the content of each cell in the whole table appears as character with quotation marks.these quotations marks should be suppressed. how can i handle this?

thank you in advance


norbert

---------------------------
code:

final.table3 <- as.matrix (cbind(
                      round(coef.sur5.lm[,1],4),sigc.sur5.lm[,1],
                      round(coef.sur5.lm[,2],4),sigc.sur5.lm[,2],
                      round(coef.sur5.lm[,3],4),sigc.sur5.lm[,3],
                      round(coef.sur5.lm[,4],4),sigc.sur5.lm[,4],
                      round(coef.sur5.lm[,5],4),sigc.sur5.lm[,5]
                  ))
colnames(final.table3) <- c("MW","sig.",
                            "HP","sig.",
                            "CC","sig.",
                            "AL","sig.",
                            "RC","sig.")

---------------------------
output:

              MW        sig.  HP        sig.  CC        sig.  AL        sig.  RC        sig.
(Intercept) "6.7757"  "***" "-2.2851" "***" "-0.067"  ""    "8.517"   "***" "11.0767" "***"
FEMALE      "-1.6267" "***" "2.4045"  "***" "0.2892"  "***" "-1.0887" "***" "0.0236"  ""
AGE         "-0.0324" "."   "0.1923"  "***" "0.0158"  "***" "-0.0984" "***" "-0.0778" "***"
I(AGE^2)    "1e-04"   ""    "-0.0019" "***" "-2e-04"  "***" "0.0011"  "***" "0.001"   "***"
C2.H        "-0.2589" "*"   "0.2051"  "**"  "0.5289"  "***" "-0.4537" "***" "-0.0211" ""
C2_3.H      "-0.4278" "**"  "0.118"   ""    "0.4837"  "***" "-0.1301" ""    "-0.0422" ""
...		...		...		...		...		...
-- 
-------------------------------
Mag. Norbert NEUWIRTH

Roubiczekgasse 2/23
A-1100  WIEN
mob: +43 699 1835 0704


From therneau at mayo.edu  Mon Mar 19 13:45:14 2007
From: therneau at mayo.edu (Terry Therneau)
Date: Mon, 19 Mar 2007 07:45:14 -0500 (CDT)
Subject: [R] R and clinical studies
Message-ID: <200703191245.l2JCjEl25883@hsrnfs-101.mayo.edu>

  A strength of R is that there is a wide variety of contribuitions to the
package, giving it great breadth.
  A weakness of R is that there is a wide variety of contributers to the 
package, some of whom spend a lot of time on the task of function correctness,
and some of whom spend little; some worry about backward compatability, some
sneer at the idea; some spend a lot of time on maintainance, and some don't
have the time to do so or move on to other things.

   The survival code, for instance, has a set of exact test cases.  These are
small data sets where the correct answer has been carefully worked out by
hand.  S (Splus or R) passes all the tests, SAS passes most of them.  (Most of
the tests are documented in an appendix of Therneau and Grambsch, Springer,
2000).  These test cases has been a great help in creating and debugging the
code, but overall represent a large amount of work.  Most code that does not
have a corporate sponsor will not have the resources to do this.  I have them
mostly because the survival library's genesis has been spread out over 20
years, and individual bits were important parts of clinical trials and so
HAD to be right.

  (Aside.  SAS has a deserved repuation for accuracy.  It has an undeserved
one for infallability --- one of my favorite bug reports for the S code
started out "I've found a mistake in the coxph function, it gives a different
answer than SAS".  It turned out in that case that the S and SAS data sets
in their example were not quite the same.  As an earlier poster said, data
management and manipulation is the root of most errors.)

   Our group uses SAS for data manipulation primarily, and a mix of SAS and
S-Plus for the analysis.  It would be difficult to become a pure S shop, but
we've had no trouble with the mix.

	Terry Therneau
	Biostatistics, Mayo Clinic


From maechler at stat.math.ethz.ch  Mon Mar 19 14:05:55 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 19 Mar 2007 14:05:55 +0100
Subject: [R] question on table format
In-Reply-To: <op.tpfq87ki4tgxb7@nn-notebook>
References: <op.tpfq87ki4tgxb7@nn-notebook>
Message-ID: <17918.35507.641639.767111@stat.math.ethz.ch>

use
	print(<your.character.matrix>,  quote = FALSE)

HTH,
Martin

>>>>> "Norbert" == Norbert NEUWIRTH <norbertneuwirth at gmx.at>
>>>>>     on Mon, 19 Mar 2007 13:20:57 +0100 writes:

    Norbert> hi everybody,
    Norbert> i am hanging in a problem that should be solved quickly, but, in fact, i do not get further: building up a SUR/SES-system myself, where the regressions are finally used to be poisson and/or negative-binomial type, i want to generate a simple table, where i can quickly see the coefficients and their markers  for significance.

    Norbert> so i constructed this "final.table" (code & outout below). as the  markers for significance are character-type, the content of each cell in the whole table appears as character with quotation marks.these quotations marks should be suppressed. how can i handle this?

    Norbert> thank you in advance


    Norbert> norbert

    Norbert> ---------------------------
    Norbert> code:

    Norbert> final.table3 <- as.matrix (cbind(
    Norbert> round(coef.sur5.lm[,1],4),sigc.sur5.lm[,1],
    Norbert> round(coef.sur5.lm[,2],4),sigc.sur5.lm[,2],
    Norbert> round(coef.sur5.lm[,3],4),sigc.sur5.lm[,3],
    Norbert> round(coef.sur5.lm[,4],4),sigc.sur5.lm[,4],
    Norbert> round(coef.sur5.lm[,5],4),sigc.sur5.lm[,5]
    Norbert> ))
    Norbert> colnames(final.table3) <- c("MW","sig.",
    Norbert> "HP","sig.",
    Norbert> "CC","sig.",
    Norbert> "AL","sig.",
    Norbert> "RC","sig.")

    Norbert> ---------------------------
    Norbert> output:

    Norbert> MW        sig.  HP        sig.  CC        sig.  AL        sig.  RC        sig.
    Norbert> (Intercept) "6.7757"  "***" "-2.2851" "***" "-0.067"  ""    "8.517"   "***" "11.0767" "***"
    Norbert> FEMALE      "-1.6267" "***" "2.4045"  "***" "0.2892"  "***" "-1.0887" "***" "0.0236"  ""
    Norbert> AGE         "-0.0324" "."   "0.1923"  "***" "0.0158"  "***" "-0.0984" "***" "-0.0778" "***"
    Norbert> I(AGE^2)    "1e-04"   ""    "-0.0019" "***" "-2e-04"  "***" "0.0011"  "***" "0.001"   "***"
    Norbert> C2.H        "-0.2589" "*"   "0.2051"  "**"  "0.5289"  "***" "-0.4537" "***" "-0.0211" ""
    Norbert> C2_3.H      "-0.4278" "**"  "0.118"   ""    "0.4837"  "***" "-0.1301" ""    "-0.0422" ""
    Norbert> ...		...		...		...		...		...
    Norbert> -- 
    Norbert> -------------------------------
    Norbert> Mag. Norbert NEUWIRTH

    Norbert> Roubiczekgasse 2/23
    Norbert> A-1100  WIEN
    Norbert> mob: +43 699 1835 0704

    Norbert> ______________________________________________
    Norbert> R-help at stat.math.ethz.ch mailing list
    Norbert> https://stat.ethz.ch/mailman/listinfo/r-help
    Norbert> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    Norbert> and provide commented, minimal, self-contained, reproducible code.


From cadeb at usgs.gov  Mon Mar 19 15:06:05 2007
From: cadeb at usgs.gov (Brian S Cade)
Date: Mon, 19 Mar 2007 08:06:05 -0600
Subject: [R] likelihoods in SAS GENMOD vs R glm
Message-ID: <OF01F13796.4559AF8F-ON872572A3.004C59DE-872572A3.004DDACB@usgs.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070319/8accee4e/attachment.pl 

From jontwang at gmail.com  Mon Mar 19 15:17:14 2007
From: jontwang at gmail.com (Jonathan Wang)
Date: Mon, 19 Mar 2007 09:17:14 -0500
Subject: [R] CPU usage on Windows
In-Reply-To: <20070316225629.BWZ11263@po-d.temple.edu>
References: <20070316225629.BWZ11263@po-d.temple.edu>
Message-ID: <4f7636bf0703190717h2ee419fax604ea4b6157abeb0@mail.gmail.com>

On 3/16/07, Richard M. Heiberger <rmh at temple.edu> wrote:
> I can't imagine using Windows without Emacs.
> In particular, the Windows ports of Emacs are very aware
> of the operating system and usually make the right assumptions.
>
> The type of behavior you are noticing can probably be cured by typing C-g in the
> *R* buffer in emacs.  The most likely cause is that the R process in Emacs
> is waiting for the plot to finish and is querying the plotting device.
> Most of that excess CPU usage is from the query loop.  The C-g tells Emacs and R
> to stop waiting.
>
> If C-g doesn't stop the 100% CPU utilization, then it is most likely something
> about the specific plot you are drawing.  We will need to see a reproducible
> example to say more.

The behavior I'm seeing is different from what you've described. It's
reliably reproducible and occurs whenever a plot window is visible,
whether using ESS & Rterm or Rterm directly. Something as simple as
plot(1:10, rnorm(10)) will trigger this behavior.

The Windows task manager shows that it's the Rterm process that's
spinning and not emacs. I've previously observed the behavior you
mention, where ESS gets stuck in a busy loop waiting for the next
command prompt from Rterm. This is also (obviously) suboptimal, but
isn't the particular issue I'm having.

Perhaps it's a graphics driver conflict of some sort?

Jonathan


From ripley at stats.ox.ac.uk  Mon Mar 19 15:45:09 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 19 Mar 2007 14:45:09 +0000 (GMT)
Subject: [R] CPU usage on Windows
In-Reply-To: <4f7636bf0703190717h2ee419fax604ea4b6157abeb0@mail.gmail.com>
References: <20070316225629.BWZ11263@po-d.temple.edu>
	<4f7636bf0703190717h2ee419fax604ea4b6157abeb0@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0703191441460.28272@gannet.stats.ox.ac.uk>

On Mon, 19 Mar 2007, Jonathan Wang wrote:

> On 3/16/07, Richard M. Heiberger <rmh at temple.edu> wrote:
>> I can't imagine using Windows without Emacs.
>> In particular, the Windows ports of Emacs are very aware
>> of the operating system and usually make the right assumptions.
>>
>> The type of behavior you are noticing can probably be cured by typing C-g in the
>> *R* buffer in emacs.  The most likely cause is that the R process in Emacs
>> is waiting for the plot to finish and is querying the plotting device.
>> Most of that excess CPU usage is from the query loop.  The C-g tells Emacs and R
>> to stop waiting.
>>
>> If C-g doesn't stop the 100% CPU utilization, then it is most likely something
>> about the specific plot you are drawing.  We will need to see a reproducible
>> example to say more.
>
> The behavior I'm seeing is different from what you've described. It's
> reliably reproducible and occurs whenever a plot window is visible,
> whether using ESS & Rterm or Rterm directly. Something as simple as
> plot(1:10, rnorm(10)) will trigger this behavior.
>
> The Windows task manager shows that it's the Rterm process that's
> spinning and not emacs. I've previously observed the behavior you
> mention, where ESS gets stuck in a busy loop waiting for the next
> command prompt from Rterm. This is also (obviously) suboptimal, but
> isn't the particular issue I'm having.
>
> Perhaps it's a graphics driver conflict of some sort?

We don't know, but it is *not* what (almost) everyone else is 
experiencing.  It should go away if you switch off windows() buffering 
(see its help page and option "windowsBuffered").

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jontwang at gmail.com  Mon Mar 19 15:51:19 2007
From: jontwang at gmail.com (Jonathan Wang)
Date: Mon, 19 Mar 2007 09:51:19 -0500
Subject: [R] CPU usage on Windows
In-Reply-To: <Pine.LNX.4.64.0703191441460.28272@gannet.stats.ox.ac.uk>
References: <20070316225629.BWZ11263@po-d.temple.edu>
	<4f7636bf0703190717h2ee419fax604ea4b6157abeb0@mail.gmail.com>
	<Pine.LNX.4.64.0703191441460.28272@gannet.stats.ox.ac.uk>
Message-ID: <4f7636bf0703190751ga8d0f68i48dca0e30e9018af@mail.gmail.com>

On 3/19/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> We don't know, but it is *not* what (almost) everyone else is
> experiencing.  It should go away if you switch off windows() buffering
> (see its help page and option "windowsBuffered").

Thanks, that solved it.

Jonathan


From julien at no-log.org  Mon Mar 19 16:01:45 2007
From: julien at no-log.org (Julien Barnier)
Date: Mon, 19 Mar 2007 16:01:45 +0100
Subject: [R] getting identified points back from Rggobi
Message-ID: <87y7ltz38m.fsf@ens-lsh.fr>

Hi,

Sorry if this is a dummy quesion, but I can't manage to find the
answer.

I use rggobi to launch ggobi on a dataset. Then, in an XY plot, I use
the "identify" interactive function in order to label some outliers.

The question is, is there a way to get the list of identified points
back in R (I mean, without having to write them on a paper sheet) ?

Thanks in advance,

-- 
Julien


From blindglobe at gmail.com  Mon Mar 19 15:58:19 2007
From: blindglobe at gmail.com (A.J. Rossini)
Date: Mon, 19 Mar 2007 15:58:19 +0100
Subject: [R] [ESS]  CPU usage on Windows
In-Reply-To: <4f7636bf0703190717h2ee419fax604ea4b6157abeb0@mail.gmail.com>
References: <20070316225629.BWZ11263@po-d.temple.edu>
	<4f7636bf0703190717h2ee419fax604ea4b6157abeb0@mail.gmail.com>
Message-ID: <1abe3fa90703190758u2cfde3bamf2f10bd3615028f2@mail.gmail.com>

On 3/19/07, Jonathan Wang <jontwang at gmail.com> wrote:
> On 3/16/07, Richard M. Heiberger <rmh at temple.edu> wrote:
> > I can't imagine using Windows without Emacs.
> > In particular, the Windows ports of Emacs are very aware
> > of the operating system and usually make the right assumptions.
> >
> > The type of behavior you are noticing can probably be cured by typing C-g in the
> > *R* buffer in emacs.  The most likely cause is that the R process in Emacs
> > is waiting for the plot to finish and is querying the plotting device.
> > Most of that excess CPU usage is from the query loop.  The C-g tells Emacs and R
> > to stop waiting.
> >
> > If C-g doesn't stop the 100% CPU utilization, then it is most likely something
> > about the specific plot you are drawing.  We will need to see a reproducible
> > example to say more.
>
> The behavior I'm seeing is different from what you've described. It's
> reliably reproducible and occurs whenever a plot window is visible,
> whether using ESS & Rterm or Rterm directly. Something as simple as
> plot(1:10, rnorm(10)) will trigger this behavior.
>
> The Windows task manager shows that it's the Rterm process that's
> spinning and not emacs. I've previously observed the behavior you
> mention, where ESS gets stuck in a busy loop waiting for the next
> command prompt from Rterm. This is also (obviously) suboptimal, but
> isn't the particular issue I'm having.
>
> Perhaps it's a graphics driver conflict of some sort?

No, it's some weirdness going on with respect to the different
"threads" and blocking (and from a Ripley-esque perspective, the
intution I'm presenting is technically wrong).  Emacs wants to control
certain things from the I/O perspective, and some how doesn't do it
right, sending Rterm into 100% CPU utilization.

So I think I'm still right, that this is more of an design issue, the
conflicting suspects being Emacs' design for controlling Microsoft OS
subprocesses (emacs inferior process) and R's design as a subprocess
with multiple threads (again, I can't help but think that threads is
the wrong word here, I'm sure Duncan knows the right one to use),
under Microsoft OSs.

 (Duncan took this as a cynical backslap, and I can't stay that I
wasn't partially intending it, but it's also true at face value
without historical baggage of peoples opinions).  And again, at this
point I firmly believe that it's probably more of an ESS/Emacs issue
than a pure R one.

best,
-tony

blindglobe at gmail.com
Muttenz, Switzerland.
"Commit early,commit often, and commit in a repository from which we
can easily roll-back your mistakes" (AJR, 4Jan05).


From mckellercran at gmail.com  Mon Mar 19 16:08:33 2007
From: mckellercran at gmail.com (Matthew Keller)
Date: Mon, 19 Mar 2007 11:08:33 -0400
Subject: [R] matrix similarity comparison
In-Reply-To: <45FE5E72.1030707@esa.ipvc.pt>
References: <45FE5E72.1030707@esa.ipvc.pt>
Message-ID: <3f547caa0703190808t1f218bc5me60a2a45d3eab962@mail.gmail.com>

hi Carlos,

its not really clear what you're asking here. If all you want is to
see what entries are the same and which are different between two
matrices of the same dimensions, then this does it:
#same
m1==m2
#diff
m1 != m2

If you want to extract the ones that are the same,
indx <- m1==m2
> indx
      [,1] [,2]  [,3]  [,4]
[1,]  TRUE TRUE FALSE  TRUE
[2,]  TRUE TRUE  TRUE FALSE
[3,] FALSE TRUE FALSE  TRUE
> m1[indx]
1 0 0 1 0 1 1 1


On 3/19/07, Carlos Guerra <carlosguerra at esa.ipvc.pt> wrote:
> Good morning to you all,
>
> I have a problem with a set of matrices that I want to compare.
>
> I want to see the similarity between them, and to be able to extract the
> differences between them.
>
> They have all the same number of columns and rows, and correspond
> presence absence data:
>
> for example:
>
> m1 <- matrix(c(1,0,0,0,1,0,1,1,1,1,1,1), 3,4)
> m2 <- matrix(c(1,0,1,0,1,0,0,1,0,1,0,1), 3,4)
>
> I tried with the function cor2m() [package=edodist] but it didn't worked
> and my matrices are much bigger than the ones from the example.
>
> Thank you,
>
> Carlos
>
> --
> Carlos GUERRA
>
> Gabinete de Sistemas de Informacao Geografica
> Escola Superior Agraria de Ponte de Lima
> Mosteiro de Refoios do Lima
> 4990-706 Ponte de Lima
>
> Tlm: +351 91 2407109
> Tlf: +351 258 909779
>
> Reclaim your Inbox...!!!
> http://www.mozilla.org/products/thunderbird/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Matthew C Keller
Postdoctoral Fellow
Virginia Institute for Psychiatric and Behavioral Genetics


From tkeitt at gmail.com  Mon Mar 19 16:19:57 2007
From: tkeitt at gmail.com (Tim Keitt)
Date: Mon, 19 Mar 2007 10:19:57 -0500
Subject: [R] Setting site-wide default CRAN repository?
In-Reply-To: <Pine.LNX.4.64.0703190743470.8701@gannet.stats.ox.ac.uk>
References: <6262c54c0703181257y674f3accic6b82c9aa4f72dae@mail.gmail.com>
	<Pine.LNX.4.64.0703190743470.8701@gannet.stats.ox.ac.uk>
Message-ID: <6262c54c0703190819l44dd841cp24bcdb57186c8954@mail.gmail.com>

On 3/19/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Sun, 18 Mar 2007, Tim Keitt wrote:
>
> > I can't seem to find this anywhere. How do I set the default CRAN
> > repository _site wide_ on a linux box? What I want to do is eliminate
> > the pop-up list of repository locations when using
> > 'install.packages()'. I know how to do this for a single account.
> > Modifying files in /etc/R does not seem to work. (cc me please - I
> > think I'm not subscribed).
>
> Modifying files in R_HOME/etc works, but I have no idea why you think
> /etc/R is relevant.

The Ubuntu packages symlink to files in /etc/R

>
> The simplest way is to replace @CRAN@ in R_HOME/etc/repositories.  (See
> ?setRepositories.)

First thing I tried. This has no effect on my machines.

>
> Almost as simple is to set e.g.
>
>         options(repos=c(CRAN="http://cran.us.r-project.org"))
>
> in R_HOME/etc/Rprofile.site (see ?Startup).

That's what I was looking for. Several people beat you to it (off list).

Cheers,
THK

>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


-- 
Timothy H. Keitt, University of Texas at Austin
Contact info and schedule at http://www.keittlab.org/tkeitt/
Reprints at http://www.keittlab.org/tkeitt/papers/
ODF attachment? See http://www.openoffice.org/


From tlumley at u.washington.edu  Mon Mar 19 16:25:37 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 19 Mar 2007 08:25:37 -0700 (PDT)
Subject: [R] Hardware for a new Workstation for best performance using R
In-Reply-To: <Pine.LNX.4.64.0703150945480.13098@perrin.socsci.unc.edu>
References: <PIEALPNOIGOLCPEIJGMEOEGDCFAA.morach@cfigroup.ch>
	<Pine.LNX.4.64.0703150945480.13098@perrin.socsci.unc.edu>
Message-ID: <Pine.LNX.4.64.0703190819100.17682@homer22.u.washington.edu>

On Thu, 15 Mar 2007, Andrew Perrin wrote: (in part)
>
> 2.) Yes, by all means you should use linux instead of windows. The
> graphics output is completely compatible with whatever applications you
> want to paste them into on Windows.

This turns out not to be the case.

It is not trivial to produce good graphics off Windows for adding to 
Microsoft Office documents (regrettably an important case for many 
people).  There has been much discussion of this on the R-sig-mac mailing 
list, for example, where PNG bitmaps (at sufficiently high resolution) 
seem to be the preferred method.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From ggrothendieck at gmail.com  Mon Mar 19 16:43:29 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 19 Mar 2007 11:43:29 -0400
Subject: [R] Hardware for a new Workstation for best performance using R
In-Reply-To: <Pine.LNX.4.64.0703190819100.17682@homer22.u.washington.edu>
References: <PIEALPNOIGOLCPEIJGMEOEGDCFAA.morach@cfigroup.ch>
	<Pine.LNX.4.64.0703150945480.13098@perrin.socsci.unc.edu>
	<Pine.LNX.4.64.0703190819100.17682@homer22.u.washington.edu>
Message-ID: <971536df0703190843y6c35245ch5df527126ef2e572@mail.gmail.com>

On 3/19/07, Thomas Lumley <tlumley at u.washington.edu> wrote:
> On Thu, 15 Mar 2007, Andrew Perrin wrote: (in part)
> >
> > 2.) Yes, by all means you should use linux instead of windows. The
> > graphics output is completely compatible with whatever applications you
> > want to paste them into on Windows.
>
> This turns out not to be the case.
>
> It is not trivial to produce good graphics off Windows for adding to
> Microsoft Office documents (regrettably an important case for many
> people).  There has been much discussion of this on the R-sig-mac mailing
> list, for example, where PNG bitmaps (at sufficiently high resolution)
> seem to be the preferred method.

On Windows one can produce metafile output directly from R.  This
is a Windows vector graphics format so it retains resolution under expansion
and shrinkage and it also works well with Microsoft Office.  This
would likely give
superior results (maximum resolution, more flexibility in post processing,
easier to do, interfaces better with Office) to using and transferring graphics
from another OS, particularly png which is only bit-mapped rather than
vector-based.


From tyler.smith at mail.mcgill.ca  Mon Mar 19 16:59:03 2007
From: tyler.smith at mail.mcgill.ca (Tyler Smith)
Date: Mon, 19 Mar 2007 12:59:03 -0300
Subject: [R] MANOVA permutation testing
In-Reply-To: <1174064368.25843.112.camel@gsimpson.geog.ucl.ac.uk>
References: <slrnevjqdp.to8.tyler.smith@blackbart.mynetwork>
	<1174064368.25843.112.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <20070319155903.GI3784@blackbart.mynetwork>

On Fri, Mar 16, 2007 at 04:59:28PM +0000, Gavin Simpson wrote:
> 
> When you create your result vector, is is of length 0. Each time you add
> a result, R has to copy the current result object, enlarge it and so on.
> This all takes a lot of time. Better to allocate storage first, then add
> each result in turn be replacement. E.g.:
> 

Thanks! Your explanations will be quite helpful in cleaning up my code.

-- 
Regards,

Tyler Smith


From sarah.goslee at gmail.com  Mon Mar 19 17:03:58 2007
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 19 Mar 2007 12:03:58 -0400
Subject: [R] matrix similarity comparison
In-Reply-To: <45FE5E72.1030707@esa.ipvc.pt>
References: <45FE5E72.1030707@esa.ipvc.pt>
Message-ID: <efb536d50703190903m43f02b49n69cba198cd4e36ee@mail.gmail.com>

Hi Carlos,

> I want to see the similarity between them, and to be able to extract the
> differences between them.

You need to explain a bit more. Are you looking for number of elements in
common? How are your data set up? (eg species as columns and sites
as rows)

One way to get number of joint presences is this (although there are
certainly more elegant ways), assuming that columns are species

> m1 <- matrix(c(1,0,0,0,1,0,1,1,1,1,1,1), 3,4)
> m2 <- matrix(c(1,0,1,0,1,0,0,1,0,1,0,1), 3,4)

> apply((m1 + m2), 1, function(x)sum(x == 2))
[1] 2 2 1

> I tried with the function cor2m() [package=edodist] but it didn't worked
> and my matrices are much bigger than the ones from the example.

"Didn't work"? That's a bit vague, but anyway this won't help, since
cor2m is intended for use with a matrix of environmental variables as
the second matrix, and not for binary data (the first matrix can be
binary). I doubt that correlations are really the measure you want
anyway - if they are, then you can use simply
cor(m1, m2)

Sarah


-- 
Sarah Goslee
http://www.functionaldiversity.org


From marc_schwartz at comcast.net  Mon Mar 19 17:05:17 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Mon, 19 Mar 2007 11:05:17 -0500
Subject: [R] Hardware for a new Workstation for best performance using R
In-Reply-To: <971536df0703190843y6c35245ch5df527126ef2e572@mail.gmail.com>
References: <PIEALPNOIGOLCPEIJGMEOEGDCFAA.morach@cfigroup.ch>
	<Pine.LNX.4.64.0703150945480.13098@perrin.socsci.unc.edu>
	<Pine.LNX.4.64.0703190819100.17682@homer22.u.washington.edu>
	<971536df0703190843y6c35245ch5df527126ef2e572@mail.gmail.com>
Message-ID: <1174320318.5039.30.camel@localhost.localdomain>

On Mon, 2007-03-19 at 11:43 -0400, Gabor Grothendieck wrote:
> On 3/19/07, Thomas Lumley <tlumley at u.washington.edu> wrote:
> > On Thu, 15 Mar 2007, Andrew Perrin wrote: (in part)
> > >
> > > 2.) Yes, by all means you should use linux instead of windows. The
> > > graphics output is completely compatible with whatever applications you
> > > want to paste them into on Windows.
> >
> > This turns out not to be the case.
> >
> > It is not trivial to produce good graphics off Windows for adding to
> > Microsoft Office documents (regrettably an important case for many
> > people).  There has been much discussion of this on the R-sig-mac mailing
> > list, for example, where PNG bitmaps (at sufficiently high resolution)
> > seem to be the preferred method.
> 
> On Windows one can produce metafile output directly from R.  This
> is a Windows vector graphics format so it retains resolution under expansion
> and shrinkage and it also works well with Microsoft Office.  This
> would likely give
> superior results (maximum resolution, more flexibility in post processing,
> easier to do, interfaces better with Office) to using and transferring graphics
> from another OS, particularly png which is only bit-mapped rather than
> vector-based.

Gabor,

The problem is the the WMF/EMF formats are Windows specific, given the
proprietary nature of the format.

On non-Windows platforms (ie. Linux) which is what Thomas was referring
to, there is the libEMF library, but experience indicates it is not a
satisfactory method.

Thus, as has been discussed on r-help extensively as well over the past
several years, there is no real _cross-platform_ approach to getting
'simple' vector based graphics into Word or Powerpoint for use in
'normal' day to day operations, which typically means viewing on a
screen.

The alternative as Thomas noted, is typically to use a high-res PNG
file, which ends up being quite large and if there are several, makes
the .DOC or .PPT file quite large as well, and ultimately, impractical.

Another approach is to use EPS images generated in R, enable the
creation of lower-res bitmapped previews on import into Office, to
enable basic visual review and placement, but to then print a hard copy
or export to a PS file via a PS compatible printer/driver which will
properly render the embedded EPS image.

If required, one can convert the PS file to a PDF file, when then allows
for subsequent display using a PDF viewer in full screen mode, if a
presentation is the goal.

Until such time as SVG becomes more of a standard on multiple platforms
(even OO.org does not yet support it for import), creating vector based
images for use in Windows apps, from non-Windows systems, will continue
to be problematic.

HTH,

Marc Schwartz


From sarah.goslee at gmail.com  Mon Mar 19 17:07:39 2007
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 19 Mar 2007 12:07:39 -0400
Subject: [R] matrix similarity comparison
In-Reply-To: <efb536d50703190903m43f02b49n69cba198cd4e36ee@mail.gmail.com>
References: <45FE5E72.1030707@esa.ipvc.pt>
	<efb536d50703190903m43f02b49n69cba198cd4e36ee@mail.gmail.com>
Message-ID: <efb536d50703190907q5a015de1r61ca7007279868b6@mail.gmail.com>

On 3/19/07, Sarah Goslee <sarah.goslee at gmail.com> wrote:
> Hi Carlos,
>
> > I want to see the similarity between them, and to be able to extract the
> > differences between them.
>
> You need to explain a bit more. Are you looking for number of elements in
> common? How are your data set up? (eg species as columns and sites
> as rows)

I thought of something else. If you want similarity, you could always do

m12.dist <- dist(cbind(m1, m2), "binary")
and just look at the elements of the result that correspond to columns
of m1 vs m2 (rather than m1 vs m2 or m2 vs m2).

Sarah
-- 
Sarah Goslee
http://www.functionaldiversity.org


From olivier.martin at avignon.inra.fr  Mon Mar 19 17:32:04 2007
From: olivier.martin at avignon.inra.fr (Olivier MARTIN)
Date: Mon, 19 Mar 2007 17:32:04 +0100
Subject: [R] estimation of dispersion parameter in GLM
Message-ID: <45FEBB04.7020402@avignon.inra.fr>

Hi all,

I have some difficulties to understand how the dispersion parameter is 
estimated
in GLM models.

Suppose I want to fit a quasibinomial model and mydata$W are the weigths 
for this model.
I suppose that I have p parameters in model myModel and n observations.

Model is estimated with :
res<-glm(myModel,family=quasibinomial,data=mydata,weights=mydata$W)

For my data set, summary(res) indicates :
(Dispersion parameter for quasibinomial family taken to be 25.85539)

I tried to find the value 25.85539 with the command
(1/np) * sum(  res$prior* (res$y-res$fitt)^2 / (res$fitt*(1-res$fitt)) )

The value I obtained with this estimation (estimation based on the  
Pearson Khi2) is
near the value returned  by summary(), but they are not equal. I read 
that dispersion parameter
can also be estimated with deviance or by maximum likelihood...

So my question is, what is the estimation returned by the command 
summary when I specified
a quasi family? and what is the estimation if I only use the function 
quasi() ?

Thanks for your help,
olivier


From nielssteenkrogh at zitelab.dk  Mon Mar 19 17:31:39 2007
From: nielssteenkrogh at zitelab.dk (Niels Steen Krogh)
Date: Mon, 19 Mar 2007 17:31:39 +0100
Subject: [R] sorting with criteria  that are "out of order"
In-Reply-To: <mailman.13.1174302003.8336.r-help@stat.math.ethz.ch>
References: <mailman.13.1174302003.8336.r-help@stat.math.ethz.ch>
Message-ID: <20070319162630.M95348@zitelab.dk>

I try to sort this dataframe: 
     [,1]  [,2]     [,3]        [,4]  [,5]  [,6]  [,7]  [,8] 
 [1,] "CM"  "BARBY"  "INCREASED" "  0" "  2" "  0" "  1" "  1"
 [2,] "CM"  "BARBY"  "REDUCED"   "  0" "  1" "  2" "  2" "  5"
 [3,] "CM"  "BARBY"  "STANDARD"  " 93" " 51" " 56" " 41" " 77"
 [4,] "CM"  "BONBON" "INCREASED" " 43" " 30" " 39" " 32" " 58"
 [5,] "CM"  "BONBON" "REDUCED"   "  4" "  3" "  6" "  4" " 10"
 [6,] "CM"  "BONBON" "STANDARD"  "200" "141" "127" " 73" "134"
 [7,] "RAR" "BARBY"  "INCREASED" "  4" "  1" "  3" "  1" "  5"
 [8,] "RAR" "BARBY"  "REDUCED"   "  5" "  7" "  8" "  9" " 16"
 [9,] "RAR" "BARBY"  "STANDARD"  "571" "286" "314" "270" "467"
[10,] "RAR" "BONBON" "INCREASED" " 49" " 92" "108" "154" "240"
[11,] "RAR" "BONBON" "REDUCED"   " 11" "  9" "  5" "  6" " 18"
[12,] "RAR" "BONBON" "STANDARD"  "978" "627" "571" "324" "541"


I want the sorting criteria: 
Column 1: CM before RAR
Column 2: BONBON before BARBY
Column 3: REDUCED before STANDARD before INCREASED

Have played with the "order" function but without being able to "sort out" how
to sort using information in column three. 

/Niels 

Niels Steen Krogh
Konsulent
ZiteLab ApS 

Mail: ---------- nielssteenkrogh at zitelab.dk
Telefon: ------- +45 38 88 86 13
Mobil: --------- +45 22 67 37 38
Adresse: ------- ZiteLab ApS 
---------------- Solsortvej 44
---------------- 2000 F.
Web: ----------- www.zitelab.dk

ZiteLab
-Let's Empower Your Data with Webservices


From cadeb at usgs.gov  Mon Mar 19 17:34:09 2007
From: cadeb at usgs.gov (Brian S Cade)
Date: Mon, 19 Mar 2007 10:34:09 -0600
Subject: [R] likelihoods in SAS GENMOD vs R glm
In-Reply-To: <OF01F13796.4559AF8F-ON872572A3.004C59DE-872572A3.004DDACB@usgs.gov>
Message-ID: <OF1517530D.202AF4A4-ON872572A3.005AC484-872572A3.005B6A2C@usgs.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070319/5d061616/attachment.pl 

From carlosguerra at esa.ipvc.pt  Mon Mar 19 17:37:15 2007
From: carlosguerra at esa.ipvc.pt (Carlos Guerra)
Date: Mon, 19 Mar 2007 16:37:15 +0000
Subject: [R] matrix similarity comparison
Message-ID: <45FEBC3B.60903@esa.ipvc.pt>

Let me see if I can explain my problem better,

I have:

m1 <- matrix(c(1,0,0,0,1,0,1,1,1,1,1,1), 3, 4)
rownames(m1) <- c("station1", "station2", "station3")
colnames(m1) <- c("A","B","C","D")

m2 <- matrix(c(1,0,1,1,1,0,1,1,0,0,1,1), 3, 4)
rownames(m2) <- c("station1", "station2", "station3")
colnames(m2) <- c("A","B","C","D")

... and I want to:
- find the correlation between the two matrices
- for each station, extract the names of the columns that don't match in 
the two matrices

Thanks for the previous comments,
Carlos

-- 
Carlos GUERRA

Gabinete de Sistemas de Informacao Geografica
Escola Superior Agraria de Ponte de Lima
Mosteiro de Refoios do Lima
4990-706 Ponte de Lima

Tlm: +351 91 2407109
Tlf: +351 258 909779

Reclaim your Inbox...!!!
http://www.mozilla.org/products/thunderbird/


From Greg.Snow at intermountainmail.org  Mon Mar 19 17:50:11 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 19 Mar 2007 10:50:11 -0600
Subject: [R] scatterplot brushing
In-Reply-To: <s5fad821.069@SURVEYGWIA.UMD.EDU>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB8BC5AC@LP-EXCHVS07.CO.IHC.COM>

Besides ggobi and iplots that have been mentioned, there is also tkBrush
in the TeachingDemos package (requires tcltk package).

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Richard Valliant
> Sent: Friday, March 16, 2007 3:47 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] scatterplot brushing
> 
> Is there a package (other than xgobi which requires an X 
> server) that will do scatterplot brushing?  I see a mention 
> in the mail archive of R-orca by Anthony Rossini but it is 
> not in the current list of packages.
>  
> My OS is Windows XP version 5.1, service pack 2 R version 
> 2.4.1 (2006-12-18)
>  
> Thanks
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Gunter.Eerdekens at ua.ac.be  Mon Mar 19 17:52:15 2007
From: Gunter.Eerdekens at ua.ac.be (Eerdekens Gunter)
Date: Mon, 19 Mar 2007 17:52:15 +0100
Subject: [R] Row wise solving an equation
Message-ID: <73BF51CCB2BAE5488CDE8420E21AE2E01A31A2@xmail01.ad.ua.ac.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070319/be9d576b/attachment.pl 

From murdoch at stats.uwo.ca  Mon Mar 19 17:59:53 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 19 Mar 2007 12:59:53 -0400
Subject: [R] CPU usage on Windows
In-Reply-To: <4f7636bf0703190717h2ee419fax604ea4b6157abeb0@mail.gmail.com>
References: <20070316225629.BWZ11263@po-d.temple.edu>
	<4f7636bf0703190717h2ee419fax604ea4b6157abeb0@mail.gmail.com>
Message-ID: <45FEC189.502@stats.uwo.ca>

On 3/19/2007 10:17 AM, Jonathan Wang wrote:
> On 3/16/07, Richard M. Heiberger <rmh at temple.edu> wrote:
>> I can't imagine using Windows without Emacs.
>> In particular, the Windows ports of Emacs are very aware
>> of the operating system and usually make the right assumptions.
>>
>> The type of behavior you are noticing can probably be cured by typing C-g in the
>> *R* buffer in emacs.  The most likely cause is that the R process in Emacs
>> is waiting for the plot to finish and is querying the plotting device.
>> Most of that excess CPU usage is from the query loop.  The C-g tells Emacs and R
>> to stop waiting.
>>
>> If C-g doesn't stop the 100% CPU utilization, then it is most likely something
>> about the specific plot you are drawing.  We will need to see a reproducible
>> example to say more.
> 
> The behavior I'm seeing is different from what you've described. It's
> reliably reproducible and occurs whenever a plot window is visible,
> whether using ESS & Rterm or Rterm directly. Something as simple as
> plot(1:10, rnorm(10)) will trigger this behavior.

Could you give the steps to reproduce outside of ESS?  What version, 
what shell, what command line options, etc.?  If I can reproduce it I 
may be able to fix it.  I've just tried a couple of times with no problems.

Duncan Murdoch

> 
> The Windows task manager shows that it's the Rterm process that's
> spinning and not emacs. I've previously observed the behavior you
> mention, where ESS gets stuck in a busy loop waiting for the next
> command prompt from Rterm. This is also (obviously) suboptimal, but
> isn't the particular issue I'm having.
> 
> Perhaps it's a graphics driver conflict of some sort?
> 
> Jonathan


From lawremi at iastate.edu  Mon Mar 19 18:08:49 2007
From: lawremi at iastate.edu (Michael Lawrence)
Date: Mon, 19 Mar 2007 12:08:49 -0500
Subject: [R] scatterplot brushing
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBB8BC5AC@LP-EXCHVS07.CO.IHC.COM>
References: <s5fad821.069@SURVEYGWIA.UMD.EDU>
	<07E228A5BE53C24CAD490193A7381BBB8BC5AC@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <509e0620703191008hca228b0v7820806701bde779@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070319/cc3c3780/attachment.pl 

From ggrothendieck at gmail.com  Mon Mar 19 18:10:28 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 19 Mar 2007 13:10:28 -0400
Subject: [R] Hardware for a new Workstation for best performance using R
In-Reply-To: <1174320318.5039.30.camel@localhost.localdomain>
References: <PIEALPNOIGOLCPEIJGMEOEGDCFAA.morach@cfigroup.ch>
	<Pine.LNX.4.64.0703150945480.13098@perrin.socsci.unc.edu>
	<Pine.LNX.4.64.0703190819100.17682@homer22.u.washington.edu>
	<971536df0703190843y6c35245ch5df527126ef2e572@mail.gmail.com>
	<1174320318.5039.30.camel@localhost.localdomain>
Message-ID: <971536df0703191010o77bfabcdyeac84a72ebddaba9@mail.gmail.com>

On 3/19/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> On Mon, 2007-03-19 at 11:43 -0400, Gabor Grothendieck wrote:
> > On 3/19/07, Thomas Lumley <tlumley at u.washington.edu> wrote:
> > > On Thu, 15 Mar 2007, Andrew Perrin wrote: (in part)
> > > >
> > > > 2.) Yes, by all means you should use linux instead of windows. The
> > > > graphics output is completely compatible with whatever applications you
> > > > want to paste them into on Windows.
> > >
> > > This turns out not to be the case.
> > >
> > > It is not trivial to produce good graphics off Windows for adding to
> > > Microsoft Office documents (regrettably an important case for many
> > > people).  There has been much discussion of this on the R-sig-mac mailing
> > > list, for example, where PNG bitmaps (at sufficiently high resolution)
> > > seem to be the preferred method.
> >
> > On Windows one can produce metafile output directly from R.  This
> > is a Windows vector graphics format so it retains resolution under expansion
> > and shrinkage and it also works well with Microsoft Office.  This
> > would likely give
> > superior results (maximum resolution, more flexibility in post processing,
> > easier to do, interfaces better with Office) to using and transferring graphics
> > from another OS, particularly png which is only bit-mapped rather than
> > vector-based.
>
> Gabor,
>
> The problem is the the WMF/EMF formats are Windows specific, given the
> proprietary nature of the format.
>
> On non-Windows platforms (ie. Linux) which is what Thomas was referring

I don't think so.  The statement being quoted was:

   Yes, by all means you should use linux instead of windows


From huber at ebi.ac.uk  Mon Mar 19 18:10:35 2007
From: huber at ebi.ac.uk (Wolfgang Huber)
Date: Mon, 19 Mar 2007 17:10:35 +0000
Subject: [R] sorting with criteria  that are "out of order"
In-Reply-To: <20070319162630.M95348@zitelab.dk>
References: <mailman.13.1174302003.8336.r-help@stat.math.ethz.ch>
	<20070319162630.M95348@zitelab.dk>
Message-ID: <45FEC40B.3020004@ebi.ac.uk>

Dear Niels,

you can convert the columns (which are apparently character vectors) 
into ordered factors and then sort / order on these.

 > a
  [1] "b" "a" "a" "a" "c" "a" "a" "b" "b" "c" "b" "a" "c" "c"
      "c" "c" "a" "a" "a" "c"

 > b=ordered(a, levels=c("b", "a", "c"))

 > b
  [1] b a a a c a a b b c b a c c c c a a a c
Levels: b < a < c

 > sort (b)
  [1] b b b b a a a a a a a a a c c c c c c c
Levels: b < a < c


Best wishes
   Wolfgang

------------------------------------------------------------------
Wolfgang Huber  EBI/EMBL  Cambridge UK  http://www.ebi.ac.uk/huber

  Steen Krogh wrote:
> I try to sort this dataframe: 
>      [,1]  [,2]     [,3]        [,4]  [,5]  [,6]  [,7]  [,8] 
>  [1,] "CM"  "BARBY"  "INCREASED" "  0" "  2" "  0" "  1" "  1"
>  [2,] "CM"  "BARBY"  "REDUCED"   "  0" "  1" "  2" "  2" "  5"
>  [3,] "CM"  "BARBY"  "STANDARD"  " 93" " 51" " 56" " 41" " 77"
>  [4,] "CM"  "BONBON" "INCREASED" " 43" " 30" " 39" " 32" " 58"
>  [5,] "CM"  "BONBON" "REDUCED"   "  4" "  3" "  6" "  4" " 10"
>  [6,] "CM"  "BONBON" "STANDARD"  "200" "141" "127" " 73" "134"
>  [7,] "RAR" "BARBY"  "INCREASED" "  4" "  1" "  3" "  1" "  5"
>  [8,] "RAR" "BARBY"  "REDUCED"   "  5" "  7" "  8" "  9" " 16"
>  [9,] "RAR" "BARBY"  "STANDARD"  "571" "286" "314" "270" "467"
> [10,] "RAR" "BONBON" "INCREASED" " 49" " 92" "108" "154" "240"
> [11,] "RAR" "BONBON" "REDUCED"   " 11" "  9" "  5" "  6" " 18"
> [12,] "RAR" "BONBON" "STANDARD"  "978" "627" "571" "324" "541"
> 
> 
> I want the sorting criteria: 
> Column 1: CM before RAR
> Column 2: BONBON before BARBY
> Column 3: REDUCED before STANDARD before INCREASED
> 
> Have played with the "order" function but without being able to "sort out" how
> to sort using information in column three. 
> 
> /Niels 
> 
> Niels Steen Krogh
> Konsulent
> ZiteLab ApS 
> 
> Mail: ---------- nielssteenkrogh a zitelab.dk
> Telefon: ------- +45 38 88 86 13
> Mobil: --------- +45 22 67 37 38
> Adresse: ------- ZiteLab ApS 
> ---------------- Solsortvej 44
> ---------------- 2000 F.
> Web: ----------- www.zitelab.dk
> 
> ZiteLab
> -Let's Empower Your Data with Webservices
>


From jholtman at gmail.com  Mon Mar 19 18:11:40 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 19 Mar 2007 13:11:40 -0400
Subject: [R] sorting with criteria that are "out of order"
In-Reply-To: <20070319162630.M95348@zitelab.dk>
References: <mailman.13.1174302003.8336.r-help@stat.math.ethz.ch>
	<20070319162630.M95348@zitelab.dk>
Message-ID: <644e1f320703191011x76459f2arf724eb5d4f1815a4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070319/c79460e5/attachment.pl 

From Greg.Snow at intermountainmail.org  Mon Mar 19 18:21:05 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 19 Mar 2007 11:21:05 -0600
Subject: [R] character to numeric conversion
In-Reply-To: <9507CC50-ED1E-4135-98F5-4FF70567A0BF@soc.soton.ac.uk>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB8BC5BD@LP-EXCHVS07.CO.IHC.COM>

Could you replace 'system' with 'pipe' and read directly from the pipe
connection rather than the intermediate step of having a text string?
If the external function just returns the numbers with commas and spaces
(but no line feeds), then you should be able to use 'scan' directly on
the connection.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robin Hankin
> Sent: Monday, March 19, 2007 5:20 AM
> To: Peter Dalgaard
> Cc: RHelp help; Robin Hankin
> Subject: Re: [R] character to numeric conversion
> 
> Hello everybody
> 
> thanks for the tips.
> 
> I *think* this should be the same thread....
> 
> 
> The manpage for system() says that lines of over 8095 
> characters will be split.  This is causing me problems.
> How do I get round the 8095 character limit?
> 
> 
> Simple toy example follows:
> 
> 
> 
> jj <- system("echo 4 | awk '{for(i=1;i<100;i++){printf(\"%s,\",
> $1)}}'| sed -e \"s/,$//\"",intern=T)
> 
> 
> This is  fine.  But .. . .
> 
> 
> jj <- system("echo 4 | awk '{for(i=1;i<10000;i++){printf(\"%s,\", 
> $1)}}'| sed -e \"s/,$//\"",intern=T)
> 
> 
> 
> has  "jj"  split into three bits, which is upsetting my call.  In my  
> application
> the split occurs in the middle of a multi-digit number, which 
> messes up
> my conversion to numeric?
> 
> 
> 
> 
> 
> 
> 
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tlumley at u.washington.edu  Mon Mar 19 19:05:32 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 19 Mar 2007 11:05:32 -0700 (PDT)
Subject: [R] Hardware for a new Workstation for best performance using R
In-Reply-To: <971536df0703190843y6c35245ch5df527126ef2e572@mail.gmail.com>
References: <PIEALPNOIGOLCPEIJGMEOEGDCFAA.morach@cfigroup.ch>
	<Pine.LNX.4.64.0703150945480.13098@perrin.socsci.unc.edu>
	<Pine.LNX.4.64.0703190819100.17682@homer22.u.washington.edu>
	<971536df0703190843y6c35245ch5df527126ef2e572@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0703191104370.4840@homer23.u.washington.edu>

On Mon, 19 Mar 2007, Gabor Grothendieck wrote:

> On 3/19/07, Thomas Lumley <tlumley at u.washington.edu> wrote:
>> On Thu, 15 Mar 2007, Andrew Perrin wrote: (in part)
>>>
>>> 2.) Yes, by all means you should use linux instead of windows. The
>>> graphics output is completely compatible with whatever applications you
>>> want to paste them into on Windows.
>>
>> This turns out not to be the case.
>>
>> It is not trivial to produce good graphics off Windows for adding to
>> Microsoft Office documents (regrettably an important case for many
>> people).  There has been much discussion of this on the R-sig-mac mailing
>> list, for example, where PNG bitmaps (at sufficiently high resolution)
>> seem to be the preferred method.
>
> On Windows one can produce metafile output directly from R.

Yes, indeed. However, this fact is of limited help when working on another 
operating system, which was the focus of the original question.

 	-thomas


From ggrothendieck at gmail.com  Mon Mar 19 19:14:04 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 19 Mar 2007 14:14:04 -0400
Subject: [R] Hardware for a new Workstation for best performance using R
In-Reply-To: <Pine.LNX.4.64.0703191104370.4840@homer23.u.washington.edu>
References: <PIEALPNOIGOLCPEIJGMEOEGDCFAA.morach@cfigroup.ch>
	<Pine.LNX.4.64.0703150945480.13098@perrin.socsci.unc.edu>
	<Pine.LNX.4.64.0703190819100.17682@homer22.u.washington.edu>
	<971536df0703190843y6c35245ch5df527126ef2e572@mail.gmail.com>
	<Pine.LNX.4.64.0703191104370.4840@homer23.u.washington.edu>
Message-ID: <971536df0703191114q76320776ge072a06c27f75cee@mail.gmail.com>

On 3/19/07, Thomas Lumley <tlumley at u.washington.edu> wrote:
> On Mon, 19 Mar 2007, Gabor Grothendieck wrote:
>
> > On 3/19/07, Thomas Lumley <tlumley at u.washington.edu> wrote:
> >> On Thu, 15 Mar 2007, Andrew Perrin wrote: (in part)
> >>>
> >>> 2.) Yes, by all means you should use linux instead of windows. The
> >>> graphics output is completely compatible with whatever applications you
> >>> want to paste them into on Windows.
> >>
> >> This turns out not to be the case.
> >>
> >> It is not trivial to produce good graphics off Windows for adding to
> >> Microsoft Office documents (regrettably an important case for many
> >> people).  There has been much discussion of this on the R-sig-mac mailing
> >> list, for example, where PNG bitmaps (at sufficiently high resolution)
> >> seem to be the preferred method.
> >
> > On Windows one can produce metafile output directly from R.
>
> Yes, indeed. However, this fact is of limited help when working on another
> operating system, which was the focus of the original question.

What was being discussed included:

"Yes, by all means you should use linux instead of windows."

and the subsequent discussion seemed to support that including
the suggestion that producing graphics on linux is just as good as
producing it on windows even if its intended to be transferred to
Microsoft Office on Windows but in fact there are a number of advantages
to doing it on Windows if you intend to use Microsoft Office there.


From p.dalgaard at biostat.ku.dk  Mon Mar 19 19:23:30 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 19 Mar 2007 19:23:30 +0100
Subject: [R] Hardware for a new Workstation for best performance using R
In-Reply-To: <Pine.LNX.4.64.0703190819100.17682@homer22.u.washington.edu>
References: <PIEALPNOIGOLCPEIJGMEOEGDCFAA.morach@cfigroup.ch>	<Pine.LNX.4.64.0703150945480.13098@perrin.socsci.unc.edu>
	<Pine.LNX.4.64.0703190819100.17682@homer22.u.washington.edu>
Message-ID: <45FED522.3010006@biostat.ku.dk>

Thomas Lumley wrote:
> On Thu, 15 Mar 2007, Andrew Perrin wrote: (in part)
>   
>> 2.) Yes, by all means you should use linux instead of windows. The
>> graphics output is completely compatible with whatever applications you
>> want to paste them into on Windows.
>>     
>
> This turns out not to be the case.
>
> It is not trivial to produce good graphics off Windows for adding to 
> Microsoft Office documents (regrettably an important case for many 
> people).  There has been much discussion of this on the R-sig-mac mailing 
> list, for example, where PNG bitmaps (at sufficiently high resolution) 
> seem to be the preferred method.
>
>  	-thomas
>
>   
One option for people who are paying for software anyways is to install 
Adobe Acrobat Writer software for generating PDF files from Word. This 
also allows you to include ouput from the pdf() device, and the end 
result comes out really nice.


From mdalphin at amgen.com  Mon Mar 19 19:28:28 2007
From: mdalphin at amgen.com (Dalphin, Mark)
Date: Mon, 19 Mar 2007 11:28:28 -0700
Subject: [R] Hardware for a new Workstation for best performance using	 R
Message-ID: <567ACB2E39C83543B746F1AD7F5E5E040B260BB0@wa-mb2-sea.amgen.com>

On Mon, 19 Mar 2007, Thomas Lumley wrote:
>> On 3/19/07, Thomas Lumley <tlumley at u.washington.edu> wrote:
>>> On Thu, 15 Mar 2007, Andrew Perrin wrote: (in part)
>>>>
>>>> 2.) Yes, by all means you should use linux instead of windows. The
>>>> graphics output is completely compatible with whatever applications you
>>>> want to paste them into on Windows.
>>>
>>> This turns out not to be the case.
>>>
>>> It is not trivial to produce good graphics off Windows for adding to
>>> Microsoft Office documents (regrettably an important case for many
>>> people).  There has been much discussion of this on the R-sig-mac
mailing
>>> list, for example, where PNG bitmaps (at sufficiently high resolution)
>>> seem to be the preferred method.
>>
>> On Windows one can produce metafile output directly from R.
>
> Yes, indeed. However, this fact is of limited help when working on another

> operating system, which was the focus of the original question.
>
> 	-thomas

One solution which has not been covered here is to use both operating
systems. For example, I need to present in Powerpoint, yet my work is
done under Linux where I have substantially more RAM and CPU
power. Typically, I'll run my analysis under Linux and then take
advantage of the binary compatibility of the .RData file and move my
final values from Linux to Windows via Samba; I may delete large
intermediate results before the transfer to compendate for my lack of
RAM under Windows.  Some small scripts which may have been developed
under Linux are used to create the plots which are placed in my
Powerpoint presentations. By an large, the plots developed under Linux
drop right into the Windows presentations, although there are
occasional font size difficulties that require adjustments.

Mark Dalphin

----------------------
Mark Dalphin
Dept Comp Biol, M/S AW2/D3262
Amgen, Inc.
1201 Amgen Court W
Seattle, WA 98119


From osklyar at ebi.ac.uk  Mon Mar 19 20:02:34 2007
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Mon, 19 Mar 2007 19:02:34 +0000
Subject: [R] Hardware for a new Workstation for best performance using R
In-Reply-To: <567ACB2E39C83543B746F1AD7F5E5E040B260BB0@wa-mb2-sea.amgen.com>
References: <567ACB2E39C83543B746F1AD7F5E5E040B260BB0@wa-mb2-sea.amgen.com>
Message-ID: <45FEDE4A.8020303@ebi.ac.uk>


For any kind of R plots that need to be inserted in office documents 
either on Windows OR on Linux you will always achieve the best result by 
using the postscript device, then converting *.ps to *.eps and inserting 
*.eps into Office (OpenOffice as well). *.eps files contain previews for 
you to edit the documents on screen, but the final output can be done 
using Print to file using a Postscript driver and converting to PDF -- 
this will ensure all your R-graphics, but also ALL of your vector 
illustrations, to be displayed in high quality vector manner.

In fact this is about the only way to put high quality plots into 
presentations or word processing documents (ANY, MSOffice included). In 
general it works easier on Linux because you will have all tools in the 
repository and for free. On Windows you will probably end up using 
Distiller, which costs quite a lot or FreePdfXP if it still exists and 
works correctly.

There is only one place where Windows, or for that sake Mac, scores 
better, and yet not ultimately: if you want to draw or design vector 
illustrations then you have larger selection of tools on Windows (Corel, 
Illustrator, Xara etc), on Mac - Illustrator and on Linux - InkScape 
(recommnded) and Xara.

Now if you go this way, the workflow looks like:

 > postscript("fig1.ps", width=5,height=5)
 > plot(x~y)
 > dev.off()
 > q()
$ ps2eps --ignoreBB --gsbbox -r 300 -R + -f fig1.ps  # 300dpi thumbnail
$ ooffice
   - import eps file, you will see the same as usual hi-res preview
   - Print, print to file, on request "Reduce Transparency" - YES
$ ps2pdf13 mydoc.ps

result: mydoc.pdf

For presentations simply use US Letter landscape instead of A4 for paper 
size and printing (to file) and this will nicely fit into the screen 
with minimal loss of space.

If the question is in the impossibility to do things on Linux: Windows 
will not help you to do the above -- there are no tools. You may find 
another solution, but for the quality of output the above is the best. 
Other formats, like WMF, EMF, AI, CDR are either of no good use for 
anything (the first 2) or are incompatible with Office (AI, CDR).

Moreover, using InkScape and RSvgDevice you can create SVG plots to 
which you can add whatever illustrations you want just in the same 
document because SVG is native for InkScape.

Oleg

PS. If you want to see examples of how such plots and illustrations look 
like (created fully on Linux with the above workflow, drop me a line, I 
will send a PDF).

Dalphin, Mark wrote:
> On Mon, 19 Mar 2007, Thomas Lumley wrote:
>>> On 3/19/07, Thomas Lumley <tlumley at u.washington.edu> wrote:
>>>> On Thu, 15 Mar 2007, Andrew Perrin wrote: (in part)
>>>>> 2.) Yes, by all means you should use linux instead of windows. The
>>>>> graphics output is completely compatible with whatever applications you
>>>>> want to paste them into on Windows.
>>>> This turns out not to be the case.
>>>>
>>>> It is not trivial to produce good graphics off Windows for adding to
>>>> Microsoft Office documents (regrettably an important case for many
>>>> people).  There has been much discussion of this on the R-sig-mac
> mailing
>>>> list, for example, where PNG bitmaps (at sufficiently high resolution)
>>>> seem to be the preferred method.
>>> On Windows one can produce metafile output directly from R.
>> Yes, indeed. However, this fact is of limited help when working on another
> 
>> operating system, which was the focus of the original question.
>>
>> 	-thomas
> 
> One solution which has not been covered here is to use both operating
> systems. For example, I need to present in Powerpoint, yet my work is
> done under Linux where I have substantially more RAM and CPU
> power. Typically, I'll run my analysis under Linux and then take
> advantage of the binary compatibility of the .RData file and move my
> final values from Linux to Windows via Samba; I may delete large
> intermediate results before the transfer to compendate for my lack of
> RAM under Windows.  Some small scripts which may have been developed
> under Linux are used to create the plots which are placed in my
> Powerpoint presentations. By an large, the plots developed under Linux
> drop right into the Windows presentations, although there are
> occasional font size difficulties that require adjustments.
> 
> Mark Dalphin
> 
> ----------------------
> Mark Dalphin
> Dept Comp Biol, M/S AW2/D3262
> Amgen, Inc.
> 1201 Amgen Court W
> Seattle, WA 98119
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr Oleg Sklyar | EBI-EMBL, Cambridge CB10 1SD, UK | +44-1223-494466


From Max.Kuhn at pfizer.com  Mon Mar 19 20:05:52 2007
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Mon, 19 Mar 2007 15:05:52 -0400
Subject: [R] Hardware for a new Workstation for best performance using R
In-Reply-To: <45FEDE4A.8020303@ebi.ac.uk>
Message-ID: <71257D09F114DA4A8E134DEAC70F25D307D7B394@groamrexm03.amer.pfizer.com>

At the risk of beating a dead horse...

odfWeave can be used on Linux (or anywhere Open Office is available) to
create reports that can then be converted to Word, rtf, html, pdf etc.

In still in the process of getting Impress (OO's presentation
application) working, but I'll get there.

Max


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Oleg Sklyar
Sent: Monday, March 19, 2007 3:03 PM
To: Dalphin, Mark
Cc: Thomas Lumley; r-help at stat.math.ethz.ch
Subject: Re: [R] Hardware for a new Workstation for best performance
using R


For any kind of R plots that need to be inserted in office documents 
either on Windows OR on Linux you will always achieve the best result by

using the postscript device, then converting *.ps to *.eps and inserting

*.eps into Office (OpenOffice as well). *.eps files contain previews for

you to edit the documents on screen, but the final output can be done 
using Print to file using a Postscript driver and converting to PDF -- 
this will ensure all your R-graphics, but also ALL of your vector 
illustrations, to be displayed in high quality vector manner.

In fact this is about the only way to put high quality plots into 
presentations or word processing documents (ANY, MSOffice included). In 
general it works easier on Linux because you will have all tools in the 
repository and for free. On Windows you will probably end up using 
Distiller, which costs quite a lot or FreePdfXP if it still exists and 
works correctly.

There is only one place where Windows, or for that sake Mac, scores 
better, and yet not ultimately: if you want to draw or design vector 
illustrations then you have larger selection of tools on Windows (Corel,

Illustrator, Xara etc), on Mac - Illustrator and on Linux - InkScape 
(recommnded) and Xara.

Now if you go this way, the workflow looks like:

 > postscript("fig1.ps", width=5,height=5)
 > plot(x~y)
 > dev.off()
 > q()
$ ps2eps --ignoreBB --gsbbox -r 300 -R + -f fig1.ps  # 300dpi thumbnail
$ ooffice
   - import eps file, you will see the same as usual hi-res preview
   - Print, print to file, on request "Reduce Transparency" - YES
$ ps2pdf13 mydoc.ps

result: mydoc.pdf

For presentations simply use US Letter landscape instead of A4 for paper

size and printing (to file) and this will nicely fit into the screen 
with minimal loss of space.

If the question is in the impossibility to do things on Linux: Windows 
will not help you to do the above -- there are no tools. You may find 
another solution, but for the quality of output the above is the best. 
Other formats, like WMF, EMF, AI, CDR are either of no good use for 
anything (the first 2) or are incompatible with Office (AI, CDR).

Moreover, using InkScape and RSvgDevice you can create SVG plots to 
which you can add whatever illustrations you want just in the same 
document because SVG is native for InkScape.

Oleg

PS. If you want to see examples of how such plots and illustrations look

like (created fully on Linux with the above workflow, drop me a line, I 
will send a PDF).

Dalphin, Mark wrote:
> On Mon, 19 Mar 2007, Thomas Lumley wrote:
>>> On 3/19/07, Thomas Lumley <tlumley at u.washington.edu> wrote:
>>>> On Thu, 15 Mar 2007, Andrew Perrin wrote: (in part)
>>>>> 2.) Yes, by all means you should use linux instead of windows. The
>>>>> graphics output is completely compatible with whatever
applications you
>>>>> want to paste them into on Windows.
>>>> This turns out not to be the case.
>>>>
>>>> It is not trivial to produce good graphics off Windows for adding
to
>>>> Microsoft Office documents (regrettably an important case for many
>>>> people).  There has been much discussion of this on the R-sig-mac
> mailing
>>>> list, for example, where PNG bitmaps (at sufficiently high
resolution)
>>>> seem to be the preferred method.
>>> On Windows one can produce metafile output directly from R.
>> Yes, indeed. However, this fact is of limited help when working on
another
> 
>> operating system, which was the focus of the original question.
>>
>> 	-thomas
> 
> One solution which has not been covered here is to use both operating
> systems. For example, I need to present in Powerpoint, yet my work is
> done under Linux where I have substantially more RAM and CPU
> power. Typically, I'll run my analysis under Linux and then take
> advantage of the binary compatibility of the .RData file and move my
> final values from Linux to Windows via Samba; I may delete large
> intermediate results before the transfer to compendate for my lack of
> RAM under Windows.  Some small scripts which may have been developed
> under Linux are used to create the plots which are placed in my
> Powerpoint presentations. By an large, the plots developed under Linux
> drop right into the Windows presentations, although there are
> occasional font size difficulties that require adjustments.
> 
> Mark Dalphin
> 
> ----------------------
> Mark Dalphin
> Dept Comp Biol, M/S AW2/D3262
> Amgen, Inc.
> 1201 Amgen Court W
> Seattle, WA 98119
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr Oleg Sklyar | EBI-EMBL, Cambridge CB10 1SD, UK | +44-1223-494466

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From sergio.della.franca at gmail.com  Mon Mar 19 20:12:10 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Mon, 19 Mar 2007 20:12:10 +0100
Subject: [R] k-means clustering
Message-ID: <b490ce570703191212h412189c5kd8db27e9ded4a2c0@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070319/0bf66722/attachment.pl 

From ggrothendieck at gmail.com  Mon Mar 19 20:22:45 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 19 Mar 2007 15:22:45 -0400
Subject: [R] Hardware for a new Workstation for best performance using R
In-Reply-To: <45FEDE4A.8020303@ebi.ac.uk>
References: <567ACB2E39C83543B746F1AD7F5E5E040B260BB0@wa-mb2-sea.amgen.com>
	<45FEDE4A.8020303@ebi.ac.uk>
Message-ID: <971536df0703191222y6b022ac6xdc4dcf35df8ca2e9@mail.gmail.com>

I think there are a lot of misconceptions regarding what is possible on Windows.

On Windows you just right click the graphic in R and choose Copy as Metafile
which puts a vector graphic representation on the clipboard and then
ctrl-V in Word to copy it in.  (Alternately save all your graphics as files
and then later insert them into Word.)

Once in Word you can even edit the elements of the graphic after the fact
such as changing the x and y axis labels right in Word, etc. Just right click
the graphic and choose Edit to get into the graphic editor.  You don't need
any other software to do that.  This is very easy, fast, flexible and gives
top resolution since its in a vector format.

(By the way, there are many free PDF generators on Windows.  I use FreePDF
on my Windows machine but I normally don't need it with R since
I just do the above.)


On 3/19/07, Oleg Sklyar <osklyar at ebi.ac.uk> wrote:
>
> For any kind of R plots that need to be inserted in office documents
> either on Windows OR on Linux you will always achieve the best result by
> using the postscript device, then converting *.ps to *.eps and inserting
> *.eps into Office (OpenOffice as well). *.eps files contain previews for
> you to edit the documents on screen, but the final output can be done
> using Print to file using a Postscript driver and converting to PDF --
> this will ensure all your R-graphics, but also ALL of your vector
> illustrations, to be displayed in high quality vector manner.
>
> In fact this is about the only way to put high quality plots into
> presentations or word processing documents (ANY, MSOffice included). In
> general it works easier on Linux because you will have all tools in the
> repository and for free. On Windows you will probably end up using
> Distiller, which costs quite a lot or FreePdfXP if it still exists and
> works correctly.
>
> There is only one place where Windows, or for that sake Mac, scores
> better, and yet not ultimately: if you want to draw or design vector
> illustrations then you have larger selection of tools on Windows (Corel,
> Illustrator, Xara etc), on Mac - Illustrator and on Linux - InkScape
> (recommnded) and Xara.
>
> Now if you go this way, the workflow looks like:
>
>  > postscript("fig1.ps", width=5,height=5)
>  > plot(x~y)
>  > dev.off()
>  > q()
> $ ps2eps --ignoreBB --gsbbox -r 300 -R + -f fig1.ps  # 300dpi thumbnail
> $ ooffice
>   - import eps file, you will see the same as usual hi-res preview
>   - Print, print to file, on request "Reduce Transparency" - YES
> $ ps2pdf13 mydoc.ps
>
> result: mydoc.pdf
>
> For presentations simply use US Letter landscape instead of A4 for paper
> size and printing (to file) and this will nicely fit into the screen
> with minimal loss of space.
>
> If the question is in the impossibility to do things on Linux: Windows
> will not help you to do the above -- there are no tools. You may find
> another solution, but for the quality of output the above is the best.
> Other formats, like WMF, EMF, AI, CDR are either of no good use for
> anything (the first 2) or are incompatible with Office (AI, CDR).
>
> Moreover, using InkScape and RSvgDevice you can create SVG plots to
> which you can add whatever illustrations you want just in the same
> document because SVG is native for InkScape.
>
> Oleg
>
> PS. If you want to see examples of how such plots and illustrations look
> like (created fully on Linux with the above workflow, drop me a line, I
> will send a PDF).
>
> Dalphin, Mark wrote:
> > On Mon, 19 Mar 2007, Thomas Lumley wrote:
> >>> On 3/19/07, Thomas Lumley <tlumley at u.washington.edu> wrote:
> >>>> On Thu, 15 Mar 2007, Andrew Perrin wrote: (in part)
> >>>>> 2.) Yes, by all means you should use linux instead of windows. The
> >>>>> graphics output is completely compatible with whatever applications you
> >>>>> want to paste them into on Windows.
> >>>> This turns out not to be the case.
> >>>>
> >>>> It is not trivial to produce good graphics off Windows for adding to
> >>>> Microsoft Office documents (regrettably an important case for many
> >>>> people).  There has been much discussion of this on the R-sig-mac
> > mailing
> >>>> list, for example, where PNG bitmaps (at sufficiently high resolution)
> >>>> seem to be the preferred method.
> >>> On Windows one can produce metafile output directly from R.
> >> Yes, indeed. However, this fact is of limited help when working on another
> >
> >> operating system, which was the focus of the original question.
> >>
> >>      -thomas
> >
> > One solution which has not been covered here is to use both operating
> > systems. For example, I need to present in Powerpoint, yet my work is
> > done under Linux where I have substantially more RAM and CPU
> > power. Typically, I'll run my analysis under Linux and then take
> > advantage of the binary compatibility of the .RData file and move my
> > final values from Linux to Windows via Samba; I may delete large
> > intermediate results before the transfer to compendate for my lack of
> > RAM under Windows.  Some small scripts which may have been developed
> > under Linux are used to create the plots which are placed in my
> > Powerpoint presentations. By an large, the plots developed under Linux
> > drop right into the Windows presentations, although there are
> > occasional font size difficulties that require adjustments.
> >
> > Mark Dalphin
> >
> > ----------------------
> > Mark Dalphin
> > Dept Comp Biol, M/S AW2/D3262
> > Amgen, Inc.
> > 1201 Amgen Court W
> > Seattle, WA 98119
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Dr Oleg Sklyar | EBI-EMBL, Cambridge CB10 1SD, UK | +44-1223-494466
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Mon Mar 19 20:24:41 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 19 Mar 2007 15:24:41 -0400
Subject: [R] Hardware for a new Workstation for best performance using R
In-Reply-To: <567ACB2E39C83543B746F1AD7F5E5E040B260BB0@wa-mb2-sea.amgen.com>
References: <567ACB2E39C83543B746F1AD7F5E5E040B260BB0@wa-mb2-sea.amgen.com>
Message-ID: <971536df0703191224k7c00c022l11248802cd9a5d72@mail.gmail.com>

Along the lines you mention, check out:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/32297.html

On 3/19/07, Dalphin, Mark <mdalphin at amgen.com> wrote:
> On Mon, 19 Mar 2007, Thomas Lumley wrote:
> >> On 3/19/07, Thomas Lumley <tlumley at u.washington.edu> wrote:
> >>> On Thu, 15 Mar 2007, Andrew Perrin wrote: (in part)
> >>>>
> >>>> 2.) Yes, by all means you should use linux instead of windows. The
> >>>> graphics output is completely compatible with whatever applications you
> >>>> want to paste them into on Windows.
> >>>
> >>> This turns out not to be the case.
> >>>
> >>> It is not trivial to produce good graphics off Windows for adding to
> >>> Microsoft Office documents (regrettably an important case for many
> >>> people).  There has been much discussion of this on the R-sig-mac
> mailing
> >>> list, for example, where PNG bitmaps (at sufficiently high resolution)
> >>> seem to be the preferred method.
> >>
> >> On Windows one can produce metafile output directly from R.
> >
> > Yes, indeed. However, this fact is of limited help when working on another
>
> > operating system, which was the focus of the original question.
> >
> >       -thomas
>
> One solution which has not been covered here is to use both operating
> systems. For example, I need to present in Powerpoint, yet my work is
> done under Linux where I have substantially more RAM and CPU
> power. Typically, I'll run my analysis under Linux and then take
> advantage of the binary compatibility of the .RData file and move my
> final values from Linux to Windows via Samba; I may delete large
> intermediate results before the transfer to compendate for my lack of
> RAM under Windows.  Some small scripts which may have been developed
> under Linux are used to create the plots which are placed in my
> Powerpoint presentations. By an large, the plots developed under Linux
> drop right into the Windows presentations, although there are
> occasional font size difficulties that require adjustments.
>
> Mark Dalphin
>
> ----------------------
> Mark Dalphin
> Dept Comp Biol, M/S AW2/D3262
> Amgen, Inc.
> 1201 Amgen Court W
> Seattle, WA 98119
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jhorn at bu.edu  Mon Mar 19 20:41:43 2007
From: jhorn at bu.edu (Jason Horn)
Date: Mon, 19 Mar 2007 15:41:43 -0400
Subject: [R] Font size and window size
Message-ID: <0D3402D9-EF04-43FE-9171-7B1714AC04A4@bu.edu>

I'm using R on Mac OS X and I have a question.  When re-sizing a  
quartz window, the fonts sizes and line weights in a graphic all stay  
the same - the distances between them just keep getting larger.  Is  
there a way to change this behavior (perhaps with graphics  
parameters) so that when resizing a window, you end up with a larger,  
proportional version of the same graphic.

Thanks


From p_connolly at ihug.co.nz  Mon Mar 19 21:28:33 2007
From: p_connolly at ihug.co.nz (Patrick Connolly)
Date: Tue, 20 Mar 2007 08:28:33 +1200
Subject: [R] Hardware for a new Workstation for best performance using R
In-Reply-To: <45FED522.3010006@biostat.ku.dk>
References: <PIEALPNOIGOLCPEIJGMEOEGDCFAA.morach@cfigroup.ch>
	<Pine.LNX.4.64.0703150945480.13098@perrin.socsci.unc.edu>
	<Pine.LNX.4.64.0703190819100.17682@homer22.u.washington.edu>
	<45FED522.3010006@biostat.ku.dk>
Message-ID: <20070319202833.GA11442@ihug.co.nz>

On Mon, 19-Mar-2007 at 07:23PM +0100, Peter Dalgaard wrote:

|> Thomas Lumley wrote:
|> > On Thu, 15 Mar 2007, Andrew Perrin wrote: (in part)
|> >   
|> >> 2.) Yes, by all means you should use linux instead of windows. The
|> >> graphics output is completely compatible with whatever applications you
|> >> want to paste them into on Windows.
|> >>     
|> >
|> > This turns out not to be the case.
|> >
|> > It is not trivial to produce good graphics off Windows for adding to 
|> > Microsoft Office documents (regrettably an important case for many 
|> > people).  There has been much discussion of this on the R-sig-mac mailing 
|> > list, for example, where PNG bitmaps (at sufficiently high resolution) 
|> > seem to be the preferred method.
|> >
|> >  	-thomas
|> >
|> >   
|> One option for people who are paying for software anyways is to install 
|> Adobe Acrobat Writer software for generating PDF files from Word. This 
|> also allows you to include ouput from the pdf() device, and the end 
|> result comes out really nice.

I've been told that there are also free PDF writers, so it's not
necessary to spend money on Adobe Acrobat Writer.  One example is
http://www.cutepdf.com/

I personally find that PNGs are fine provided I know what final size
is needed.  PNGs certainly don't have the flexibility that Gabor
mentions but they are usually as small as any other format.

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From maitra at iastate.edu  Mon Mar 19 23:15:56 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Mon, 19 Mar 2007 17:15:56 -0500
Subject: [R] k-means clustering
In-Reply-To: <b490ce570703191212h412189c5kd8db27e9ded4a2c0@mail.gmail.com>
References: <b490ce570703191212h412189c5kd8db27e9ded4a2c0@mail.gmail.com>
Message-ID: <20070319171556.52cc85a5@subarnarekha.stat.iastate.edu>

kmeans(y[,c("AGE", "PRODUCTS")], 3) should do what I think you want. 

Note that you should try several starting points for good optimality of the partitioning.

HTH,
Ranjan

On Mon, 19 Mar 2007 20:12:10 +0100 "Sergio Della Franca" <sergio.della.franca at gmail.com> wrote:

> Dear R-helpers,
> 
> I'm trying to perform k-means clustering.
> 
> For example, I have this dataset(y):
> 
>   AGE   PRODUCTS  SEX
>   92          3253           M
>   43          4144           F
>   67          3246           M
>   22          4144           F
>   56          4087           F
>   89          3836           M
>   47          4379           M
> 
> My situation is the following:
> - If i use this code: cluster<-kmeans(y,3), the program doesn't run because
> the variable "SEX" isn't numeric.
> - If i use this code: cluster<-kmeans(y[,{"AGE"}],3), the program run
> correctly.
> - If i use this code: cluster<-kmeans(y[,{"AGE" ; "PRODUCTS"}],3), the
> program run correctly, but the k-means clustering is performed only on the
> variable "PRODUCTS".
> 
> I would like to perform the k-means clustering on the two numeric variable i
> have.
> How can i modify the k-means code to develop the clustering on numeric
> variable that i decide to use?
> 
> 
> Thank you in advance.
> 
> Sergio Della Franca.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From michela.cameletti at unibg.it  Mon Mar 19 23:46:20 2007
From: michela.cameletti at unibg.it (Michela Cameletti)
Date: Mon, 19 Mar 2007 23:46:20 +0100 (CET)
Subject: [R] data.frame handling
Message-ID: <1301.87.14.93.232.1174344380.squirrel@mailserver.unibg.it>

Dear R-users,
I have a little problem that I can't solve by myself.
I have a data frame with 2 factors and 8 observations (see the following
code):

 y <- c(1,1,1,2,2,3,3,3)
 y <- factor(y)
 levels(y) <- c("a","b","c")
 x <- c(1,2,3,1,2,1,2,3)
 x <- factor(x)
 levels(x) <- c("x","y","z")
 X  <- data.frame(factor1=x,factor2=y)

and the final result is

  factor1 factor2
1       x       a
2       y       a
3       z       a
4       x       b
5       y       b
6       x       c
7       y       c
8       z       c

>From the above data I'd like to obtain the following matrix:
	a	b	c
x	1	1	1
y	1	1	1
z	1	0	1

Do you have any advice? Can you help me please?
Thank you in advance,
Michela


From tplate at acm.org  Tue Mar 20 00:02:34 2007
From: tplate at acm.org (Tony Plate)
Date: Mon, 19 Mar 2007 17:02:34 -0600
Subject: [R] data.frame handling
In-Reply-To: <1301.87.14.93.232.1174344380.squirrel@mailserver.unibg.it>
References: <1301.87.14.93.232.1174344380.squirrel@mailserver.unibg.it>
Message-ID: <45FF168A.10701@acm.org>

'table()' can compute your desired result in this particular case 
(though I don't know if it's what you want in general):

 > y <- factor(c("a","b","c")[c(1,1,1,2,2,3,3,3)])
 > x <- factor(c("x","y","z")[c(1,2,3,1,2,1,2,3)])
 > table(x, y)
    y
x   a b c
   x 1 1 1
   y 1 1 1
   z 1 0 1
 >

If x and y are already columns in a data frame, then just do

 > table(X$factor1, X$factor2)

hope this helps,

Tony Plate


Michela Cameletti wrote:
> Dear R-users,
> I have a little problem that I can't solve by myself.
> I have a data frame with 2 factors and 8 observations (see the following
> code):
> 
>  y <- c(1,1,1,2,2,3,3,3)
>  y <- factor(y)
>  levels(y) <- c("a","b","c")
>  x <- c(1,2,3,1,2,1,2,3)
>  x <- factor(x)
>  levels(x) <- c("x","y","z")
>  X  <- data.frame(factor1=x,factor2=y)
> 
> and the final result is
> 
>   factor1 factor2
> 1       x       a
> 2       y       a
> 3       z       a
> 4       x       b
> 5       y       b
> 6       x       c
> 7       y       c
> 8       z       c
> 
>>From the above data I'd like to obtain the following matrix:
> 	a	b	c
> x	1	1	1
> y	1	1	1
> z	1	0	1
> 
> Do you have any advice? Can you help me please?
> Thank you in advance,
> Michela
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Cody_Hamilton at Edwards.com  Tue Mar 20 00:11:27 2007
From: Cody_Hamilton at Edwards.com (Cody_Hamilton at Edwards.com)
Date: Mon, 19 Mar 2007 16:11:27 -0700
Subject: [R] R and clinical studies
Message-ID: <OF11EF1BD2.4A00C341-ON882572A3.007F4567-882572A3.007F2B18@irvine.edwards.com>


Thank you to all those that responded to Delphine's original post on R and
clinical studies.  They have provided much food for thought.

I had a couple of follow up questions/comments.  Andrew is very correct in
pointing out that there are classes and workshops available for R.  It's my
understanding that there are even commercial versions of R that now provide
formal commercial-style courses.  And at any rate, the money saved by
potentially avoiding pricey software could certainly justify any training
expense in time or money  - this assumes of course that the pricey software
could be dispensed with (I suspect that would take considerable time at my
current company as so many legacy projects have been done in proprietary
software).  I still think that R provides less 'hand-holding' and requires
more initiative (which may be more or less present on a per
programmer/statistician basis).

I guess one could always integrate R/Splus in with SAS, as Terry's group
has done at Mayo - I will probably do this at least as a start.  I have a
few concerns with regards to this approach (these may be needless concerns,
but I will venture expressing them anyway).  First, I'm worried about the
possibility of compatability concerns (will anyone be worried about a SAS
dataset read into R or vice-versa?).  Second, I would prefer focusing all
my learning on one package if possible.  I actually have more experience
with SAS (as do others in my group), and if the switch to R is to be made I
would like to make that switch as complete as possible.   This would also
avoid requiring new hires to know both languages.  Third, if SAS is to be
kept around, it defeats one of the main advantages of having open source
code in the first place (R is wonderfully free!).  Like Mayo, Baylor Health
(my previous employer) used both Splus and SAS.  I was warned that data
manipulation would be much more difficult in R/Splus than it was in SAS.
To be honest, and I say this humbly realizing that most posters to this
list have much more experience than I, I haven't found data manipulation to
be that much more difficult in R/Splus (at least as I have gained
experience in R/Splus).   I can think of two exceptions (1) large datasets
and (2) SAS seems to play nicer with MS products (e.g. PROC IMPORT seemed
to read in messy Excel spreadsheets better than importData in Splus).  Is
it possible (and I again say this with MUCH humility) that the perceived
advantages of SAS with regards to data manipulation may be due in part to
some users only using R/Splus for stat modeling and graphics (thus never
becoming familiar with the data manipulation capabilities of R/Splus) or to
the reluctance of SAS-trained individuals and companies to make the
complete switch?

Tony, the story about the "famous software" and the "certain operating
system" at the "large company" was priceless.

In closing, I should mention that in all posts I am speaking for myself and
not for Edwards LifeSciences.

Regards,
    -Cody


From gchappi at gmail.com  Tue Mar 20 01:10:44 2007
From: gchappi at gmail.com (Hans-Peter)
Date: Tue, 20 Mar 2007 01:10:44 +0100
Subject: [R] BETA testers for xlsReadWrite? (natively read/write Excelfiles)
Message-ID: <47fce0650703191710r4f29b9ecy484c0148912020b3@mail.gmail.com>

Dear UseRs,

I have an update ready for the xlsReadWrite package. It runs my unit
tests just fine but there are quite a lot of changes and it would be
good if some other people could try it out before it will be submited
to CRAN. I won't have more time this week, but I hope to publish
xlsReadWrite at the end of next week and I will incorporate eventual
bugreports and/or (minor) suggestions which I learn of.

The most important new features are + possibility to read/write
dates/times + rownames for matrices + checknames (colnames) + better
factor support + 'stringsAsFactors' and 'rownames' argument. I also
added 4 general purpose datetime functions to convert excel dates
from/to strings.

Download:
- zip: http://treetron.googlepages.com/BETAxlsReadWrite_1.2.1.zip
- source: http://treetron.googlepages.com/BETAxlsReadWrite_1.2.1.tar.gz
- testdata: http://treetron.googlepages.com/BETAxlsReadWrite_TestData_1.2.1.zip

Some of you may be aware that this package is made with Delphi and
contains Object Pascal code. It doesn't matter normally but now I
mention it, because this evening I got the brand new Delphi 2007 and
xlsReadWrite was the thing used to try out this (very good) new
version...

-- 
Regards,
Hans-Peter


PS: @David: no more range_error violations.


From gchappi at gmail.com  Tue Mar 20 01:18:36 2007
From: gchappi at gmail.com (Hans-Peter)
Date: Tue, 20 Mar 2007 01:18:36 +0100
Subject: [R] BETA testers for xlsReadWrite? (natively read/write
	Excelfiles)
In-Reply-To: <47fce0650703191710r4f29b9ecy484c0148912020b3@mail.gmail.com>
References: <47fce0650703191710r4f29b9ecy484c0148912020b3@mail.gmail.com>
Message-ID: <47fce0650703191718m7b0422d3y50932fc441ba7ed6@mail.gmail.com>

sorry I forgot:

> Download:
> - zip: http://treetron.googlepages.com/BETAxlsReadWrite_1.2.1.zip

Please remove the BETA prefix from the filename (otherwise it cannot
be installed).

Hans-Peter


From researchjj at gmail.com  Tue Mar 20 02:46:38 2007
From: researchjj at gmail.com (j.joshua thomas)
Date: Tue, 20 Mar 2007 09:46:38 +0800
Subject: [R] Suitable Package to Visualize Schedules
Message-ID: <b4485c4c0703191846u43e7b018ib01c01548b1a1d04@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070320/b0f73ed3/attachment.pl 

From xmeng at capitalbio.com  Tue Mar 20 04:32:06 2007
From: xmeng at capitalbio.com (XinMeng)
Date: Tue, 20 Mar 2007 11:32:06 +0800
Subject: [R] about hcluster
Message-ID: <374361526.11178@capitalbio.com>

Hi all:
As to hcluster,how can I control the cluster is performed according to rows(genes for instance) or columns(samples for instance)?
I can't find the parameters for it.

Thanks a lot!

My best!


From g.abraham at ms.unimelb.edu.au  Tue Mar 20 07:02:18 2007
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Tue, 20 Mar 2007 17:02:18 +1100
Subject: [R] truehist bug?
Message-ID: <45FF78EA.5060205@ms.unimelb.edu.au>

Hi,

Is this a bug in truehist()?

 > library(MASS)
 > x <- rep(1, 10)
 > truehist(x)
Error in pretty(data, nbins) : invalid 'n' value

Thanks,
Gad



 > R.version

platform       i486-pc-linux-gnu
arch           i486
os             linux-gnu
system         i486, linux-gnu
status
major          2
minor          4.1
year           2006
month          12
day            18
svn rev        40228
language       R
version.string R version 2.4.1 (2006-12-18)

 > sessionInfo()
R version 2.4.1 (2006-12-18)
i486-pc-linux-gnu

locale:
LC_CTYPE=en_AU.UTF-8;LC_NUMERIC=C;LC_TIME=en_AU.UTF-8;
LC_COLLATE=en_AU.UTF-8;LC_MONETARY=en_AU.UTF-8;
LC_MESSAGES=en_AU.UTF-8;LC_PAPER=en_AU.UTF-8;LC_NAME=C;
LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_AU.UTF-8;
LC_IDENTIFICATION=C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
[7] "base"

other attached packages:
     MASS
"7.2-32"


-- 
Gad Abraham
Department of Mathematics and Statistics
The University of Melbourne
Parkville 3010, Victoria, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From g.abraham at ms.unimelb.edu.au  Tue Mar 20 07:29:15 2007
From: g.abraham at ms.unimelb.edu.au (Gad Abraham)
Date: Tue, 20 Mar 2007 17:29:15 +1100
Subject: [R] ERROR: 'latex' needed but missing on your system.
In-Reply-To: <17914.54243.334456.271013@W0053328.mgh.harvard.edu>
References: <17914.54243.334456.271013@W0053328.mgh.harvard.edu>
Message-ID: <45FF7F3B.3000804@ms.unimelb.edu.au>

Joel J. Adamson wrote:
> After successfully building R on Slackware Linux v11.0 I went to make
> the documentation; the texi files went fine and then I hopefully issued
> 
> make dvi
> 
> after having gotten the warning to the effect of "You cannot build the
> DVI or PDF manuals" during compilation.  And, as expected I got the
> error
> 
> ERROR: 'latex' needed but missing on your system.
> 
> The problem is that latex is on my system and is in root's path:
> 
> /usr/src/R-2.4.1 Super-User > echo $PATH
> /usr/share/texmf/bin/:/opt/kde/bin/:/uCsr/local/stata{sic}:/usr/local/sbin:/usr/local/bin:/sbin:/usr/sbin:/bin:/usr/bin
> 
> I can issue latex from the command line as root (su'd to root, that
> is) and it will run successfully.  Also, "whereis latex" turns up
> empty.

It's a bit strange, because by default files like latex should be 
readable by all users. Did you install latex from source?

Try this:
As root, do 'which latex' to see where it's installed. Make sure that 
the file and directories on its path are readable by your non-root user, 
and that the directory is in the non-root user's path. The file 'latex' 
might also be a symlink to some other file (as is in Ubuntu), so that 
one will also need to be readable.


-- 
Gad Abraham
Department of Mathematics and Statistics
The University of Melbourne
Parkville 3010, Victoria, Australia
email: g.abraham at ms.unimelb.edu.au
web: http://www.ms.unimelb.edu.au/~gabraham


From maechler at stat.math.ethz.ch  Tue Mar 20 08:49:16 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 20 Mar 2007 08:49:16 +0100
Subject: [R] truehist bug?
In-Reply-To: <45FF78EA.5060205@ms.unimelb.edu.au>
References: <45FF78EA.5060205@ms.unimelb.edu.au>
Message-ID: <17919.37372.611766.569270@stat.math.ethz.ch>

>>>>> "Gad" == Gad Abraham <g.abraham at ms.unimelb.edu.au>
>>>>>     on Tue, 20 Mar 2007 17:02:18 +1100 writes:

    Gad> Hi,
    Gad> Is this a bug in truehist()?

    >> library(MASS)
    >> x <- rep(1, 10)
    >> truehist(x)
    Gad> Error in pretty(data, nbins) : invalid 'n' value

You get something similar though slightly more helpful
from
	hist(x, "scott")

which then uses the same method for determining the number of bins /
classes for the histogram.

I'd say the main "bug" is in   
  nclass.scott()   [ and  also nclass.FD() ]

which do not work when  var(x) == 0  as in this case.
One could argue that  

1) truehist(x) should not use "scott" as
  default when var(x) == 0   {hence a buglet in truehist()}

and either

2) both hist() and truehist() should produce a better error
  message when "scott" (or "FD") is used explicitly and var(x) == 0

or, rather IMO,

3) nclass.scott(x) and nclass.FD(x) should be extended to return a 
  non-negative integer even when  var(x) == 0

Martin Maechler, ETH Zurich


From edemaria_2 at yahoo.com.ar  Mon Mar 19 16:31:52 2007
From: edemaria_2 at yahoo.com.ar (Demaria Eleonora)
Date: Mon, 19 Mar 2007 12:31:52 -0300 (ART)
Subject: [R] help with anisotropy matrix for GaussRF
Message-ID: <79912.59475.qm@web51002.mail.re2.yahoo.com>

Hi R-users,
Does anybody have some experience using an anisotropic
time-space field to generate random fields. I am using
the Random fields package.
Thanks
Ele


	

	
		
__________________________________________________ 

Todo lo que quer?as saber, y lo que ni imaginabas, 

?Probalo ya!


From Cody_Hamilton at Edwards.com  Mon Mar 19 19:19:58 2007
From: Cody_Hamilton at Edwards.com (Cody_Hamilton at Edwards.com)
Date: Mon, 19 Mar 2007 11:19:58 -0700
Subject: [R] R and clinical studies
In-Reply-To: <200703191245.l2JCjEl25883@hsrnfs-101.mayo.edu>
Message-ID: <OF83AE8D5E.1AF9160D-ON882572A3.0061632B-882572A3.00647B35@irvine.edwards.com>


Thank you to all those that responded to Delphine's original post on R and
clinical studies.  They have provided much food for thought.

I had a couple of follow up questions/comments.  Andrew is very correct in
pointing out that there are classes and workshops available for R.  It's my
understanding that there are even commercial versions of R that now provide
formal commercial-style courses.  And at any rate, the money saved by
potentially avoiding pricey software could certainly justify any training
expense in time or money  - this assumes of course that the pricey software
could be dispensed with (I suspect that would take considerable time at my
current company as so many legacy projects have been done in proprietary
software).  I still think that R provides less 'hand-holding' and requires
more initiative (which may be more or less present on a per
programmer/statistician basis).

I guess one could always integrate R/Splus in with SAS, as Terry's group
has done at Mayo - I will probably do this at least as a start.  I have a
few concerns with regards to this approach (these may be needless concerns,
but I will venture expressing them anyway).  First, I'm worried about the
possibility of compatability concerns (will anyone be worried about a SAS
dataset read into R or vice-versa?).  Second, I would prefer focusing all
my learning on one package if possible.  I actually have more experience
with SAS (as do others in my group), and if the switch to R is to be made I
would like to make that switch as complete as possible.   This would also
avoid requiring new hires to know both languages.  Third, if SAS is to be
kept around, it defeats one of the main advantages of having open source
code in the first place (R is wonderfully free!).  Like Mayo, Baylor Health
(my previous employer) used both Splus and SAS.  I was warned that data
manipulation would be much more difficult in R/Splus than it was in SAS.
To be honest, and I say this humbly realizing that most posters to this
list have much more experience than I, I haven't found data manipulation to
be that much more difficult in R/Splus (at least as I have gained
experience in R/Splus).   I can think of two exceptions (1) large datasets
and (2) SAS seems to play nicer with MS products (e.g. PROC IMPORT seemed
to read in messy Excel spreadsheets better than importData in Splus).  Is
it possible (and I again say this with MUCH humility) that the perceived
advantages of SAS with regards to data manipulation may be due in part to
some users only using R/Splus for stat modeling and graphics (thus never
becoming familiar with the data manipulation capabilities of R/Splus) or to
the reluctance of SAS-trained individuals and companies to make the
complete switch?

Tony, the story about the "famous software" and the "certain operating
system" at the "large company" was priceless.

In closing, I should mention that in all posts I am speaking for myself and
not for Edwards LifeSciences.

Regards,
    -Cody


From Cody_Hamilton at Edwards.com  Mon Mar 19 19:25:16 2007
From: Cody_Hamilton at Edwards.com (Cody_Hamilton at Edwards.com)
Date: Mon, 19 Mar 2007 11:25:16 -0700
Subject: [R] R and clinical studies
Message-ID: <OF728ED81B.F1010DB0-ON882572A3.00652906-882572A3.0064F797@irvine.edwards.com>



Thank you to all those that responded to Delphine's original post on R and
clinical studies.  They have provided much food for thought.

I had a couple of follow up questions/comments.  Andrew is very correct in
pointing out that there are classes and workshops available for R.  It's my
understanding that there are even commercial versions of R that now provide
formal commercial-style courses.  And at any rate, the money saved by
potentially avoiding pricey software could certainly justify any training
expense in time or money  - this assumes of course that the pricey software
could be dispensed with (I suspect that would take considerable time at my
current company as so many legacy projects have been done in proprietary
software).  I still think that R provides less 'hand-holding' and requires
more initiative (which may be more or less present on a per
programmer/statistician basis).

I guess one could always integrate R/Splus in with SAS, as Terry's group
has done at Mayo - I will probably do this at least as a start.  I have a
few concerns with regards to this approach (these may be needless concerns,
but I will venture expressing them anyway).  First, I'm worried about the
possibility of compatability concerns (will anyone be worried about a SAS
dataset read into R or vice-versa?).  Second, I would prefer focusing all
my learning on one package if possible.  I actually have more experience
with SAS (as do others in my group), and if the switch to R is to be made I
would like to make that switch as complete as possible.   This would also
avoid requiring new hires to know both languages.  Third, if SAS is to be
kept around, it defeats one of the main advantages of having open source
code in the first place (R is wonderfully free!).  Like Mayo, Baylor Health
(my previous employer) used both Splus and SAS.  I was warned that data
manipulation would be much more difficult in R/Splus than it was in SAS.
To be honest, and I say this humbly realizing that most posters to this
list have much more experience than I, I haven't found data manipulation to
be that much more difficult in R/Splus (at least as I have gained
experience in R/Splus).   I can think of two exceptions (1) large datasets
and (2) SAS seems to play nicer with MS products (e.g. PROC IMPORT seemed
to read in messy Excel spreadsheets better than importData in Splus).  Is
it possible (and I again say this with MUCH humility) that the perceived
advantages of SAS with regards to data manipulation may be due in part to
some users only using R/Splus for stat modeling and graphics (thus never
becoming familiar with the data manipulation capabilities of R/Splus) or to
the reluctance of SAS-trained individuals and companies to make the
complete switch?

Tony, the story about the "famous software" and the "certain operating
system" at the "large company" was priceless.

In closing, I should mention that in all posts I am speaking for myself and
not for Edwards LifeSciences.

Regards,
    -Cody


From Cody_Hamilton at Edwards.com  Tue Mar 20 00:11:27 2007
From: Cody_Hamilton at Edwards.com (Cody_Hamilton at Edwards.com)
Date: Mon, 19 Mar 2007 16:11:27 -0700
Subject: [R] R and clinical studies
Message-ID: <OF11EF1BD2.4A00C341-ON882572A3.007F4567-882572A3.007F2B18@irvine.edwards.com>


Thank you to all those that responded to Delphine's original post on R and
clinical studies.  They have provided much food for thought.

I had a couple of follow up questions/comments.  Andrew is very correct in
pointing out that there are classes and workshops available for R.  It's my
understanding that there are even commercial versions of R that now provide
formal commercial-style courses.  And at any rate, the money saved by
potentially avoiding pricey software could certainly justify any training
expense in time or money  - this assumes of course that the pricey software
could be dispensed with (I suspect that would take considerable time at my
current company as so many legacy projects have been done in proprietary
software).  I still think that R provides less 'hand-holding' and requires
more initiative (which may be more or less present on a per
programmer/statistician basis).

I guess one could always integrate R/Splus in with SAS, as Terry's group
has done at Mayo - I will probably do this at least as a start.  I have a
few concerns with regards to this approach (these may be needless concerns,
but I will venture expressing them anyway).  First, I'm worried about the
possibility of compatability concerns (will anyone be worried about a SAS
dataset read into R or vice-versa?).  Second, I would prefer focusing all
my learning on one package if possible.  I actually have more experience
with SAS (as do others in my group), and if the switch to R is to be made I
would like to make that switch as complete as possible.   This would also
avoid requiring new hires to know both languages.  Third, if SAS is to be
kept around, it defeats one of the main advantages of having open source
code in the first place (R is wonderfully free!).  Like Mayo, Baylor Health
(my previous employer) used both Splus and SAS.  I was warned that data
manipulation would be much more difficult in R/Splus than it was in SAS.
To be honest, and I say this humbly realizing that most posters to this
list have much more experience than I, I haven't found data manipulation to
be that much more difficult in R/Splus (at least as I have gained
experience in R/Splus).   I can think of two exceptions (1) large datasets
and (2) SAS seems to play nicer with MS products (e.g. PROC IMPORT seemed
to read in messy Excel spreadsheets better than importData in Splus).  Is
it possible (and I again say this with MUCH humility) that the perceived
advantages of SAS with regards to data manipulation may be due in part to
some users only using R/Splus for stat modeling and graphics (thus never
becoming familiar with the data manipulation capabilities of R/Splus) or to
the reluctance of SAS-trained individuals and companies to make the
complete switch?

Tony, the story about the "famous software" and the "certain operating
system" at the "large company" was priceless.

In closing, I should mention that in all posts I am speaking for myself and
not for Edwards LifeSciences.

Regards,
    -Cody


From mswindell at codegear.com  Tue Mar 20 03:16:02 2007
From: mswindell at codegear.com (Michael Swindell)
Date: Tue, 20 Mar 2007 02:16:02 +0000 (UTC)
Subject: [R] BETA testers for xlsReadWrite? (natively read/write
	Excelfiles)
References: <47fce0650703191710r4f29b9ecy484c0148912020b3@mail.gmail.com>
Message-ID: <loom.20070320T031419-913@post.gmane.org>

Thanks for the mention Peter! Glad you like Delphi 2007 - we hope it's the 
best Delphi yet, our goal was to make it everyone's favorite Delphi release to 
date and the best native Windows development environment ever!

Thanks again! 

-Michael (CodeGear)


From knut.krueger at usa.com  Mon Mar 19 12:09:31 2007
From: knut.krueger at usa.com (Knut Krueger)
Date: Mon, 19 Mar 2007 12:09:31 +0100
Subject: [R] order of values in vector
In-Reply-To: <45FE526B.5080007@nvb.slu.se>
References: <45FE526B.5080007@nvb.slu.se>
Message-ID: <45FE6F6B.30202@usa.com>

Tord Sn?ll schrieb:
> Dear all,
> I would like to get the order of the values in a vector. I have tried 
> rank(), order() and searched the archive, though without success.
maybe this page could give you some hints:
http://www.ats.ucla.edu/STAT/r/faq/sort.htm
Regards Knut


From Gunter.Eerdekens at ua.ac.be  Tue Mar 20 09:30:41 2007
From: Gunter.Eerdekens at ua.ac.be (Eerdekens Gunter)
Date: Tue, 20 Mar 2007 09:30:41 +0100
Subject: [R] Row wise solving an equation
References: <73BF51CCB2BAE5488CDE8420E21AE2E01A31A2@xmail01.ad.ua.ac.be>
Message-ID: <73BF51CCB2BAE5488CDE8420E21AE2E01A31A3@xmail01.ad.ua.ac.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070320/6a326d64/attachment.pl 

From maechler at stat.math.ethz.ch  Tue Mar 20 10:17:02 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 20 Mar 2007 10:17:02 +0100
Subject: [R] nclass.scott() and nclass.FD()   {Re:  truehist bug?}
In-Reply-To: <17919.37372.611766.569270@stat.math.ethz.ch>
References: <45FF78EA.5060205@ms.unimelb.edu.au>
	<17919.37372.611766.569270@stat.math.ethz.ch>
Message-ID: <17919.42638.718240.656427@stat.math.ethz.ch>

[This has become entirely a topic for 'R-devel' hence, I'm
 diverting to there keeping R-help once; please follow-up
 only to R-devel ]

>>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Tue, 20 Mar 2007 08:49:16 +0100 writes:

>>>>> "Gad" == Gad Abraham <g.abraham at ms.unimelb.edu.au>
>>>>>     on Tue, 20 Mar 2007 17:02:18 +1100 writes:

    Gad> Hi,
    Gad> Is this a bug in truehist()?

    >>> library(MASS)
    >>> x <- rep(1, 10)
    >>> truehist(x)
    Gad> Error in pretty(data, nbins) : invalid 'n' value

    MM> You get something similar though slightly more helpful
    MM> from
    MM>    hist(x, "scott")

    MM> which then uses the same method for determining the number of bins /
    MM> classes for the histogram.

    MM> I'd say the main "bug" is in   
    MM> nclass.scott()   [ and  also nclass.FD() ]

    MM> which do not work when  var(x) == 0  as in this case.
    MM> One could argue that  

    MM> 1) truehist(x) should not use "scott" as
    MM> default when var(x) == 0   {hence a buglet in truehist()}

    MM> and either

    MM> 2) both hist() and truehist() should produce a better error
    MM> message when "scott" (or "FD") is used explicitly and var(x) == 0

    MM> or, rather IMO,

    MM> 3) nclass.scott(x) and nclass.FD(x) should be extended to return a 
    MM> non-negative integer even when  var(x) == 0

after some further thought,
I'm proposing to adopt '3)'  {only; '1)' becomes unneeded}
with the following new code  which is back-compatible for the
case where 'h > 0' and does something reasonable for the case h == 0 :

nclass.scott <- function(x)
{
    h <- 3.5 * sqrt(stats::var(x)) * length(x)^(-1/3)
    if(h > 0) ceiling(diff(range(x))/h) else 1L
}

nclass.FD <- function(x)
{
    h <- stats::IQR(x)
    if(h == 0) h <- stats::mad(x, constant = 2) # c=2: consistent with IQR
    if (h > 0) ceiling(diff(range(x))/(2 * h * length(x)^(-1/3))) else 1L
}


Martin


From Peter.Schmidtke-stage at sanofi-aventis.com  Tue Mar 20 10:29:50 2007
From: Peter.Schmidtke-stage at sanofi-aventis.com (Peter.Schmidtke-stage at sanofi-aventis.com)
Date: Tue, 20 Mar 2007 10:29:50 +0100
Subject: [R] grid on a wireframe plot
Message-ID: <4E21551A1CD7AA4BA2BF9EFE4C3D862A91A55B@crbsmxsusr07.pharma.aventis.com>

Hello,
 
I want to do a surface plot with wireframe from the lattice package. 

As for now I use the following command :
print(wireframe(m,main="% my title", colorkey=TRUE,
col.regions=rainbow(100), drape=TRUE,aspect =
c(1,1.0),ylab="y",xlab="x",zlab="z",
scales=list(arrows=FALSE)),split=c(1,1,3,2), more=TRUE)

How can I show the gridlines in a 3D surface plot of this type?
I already tried with panel.grid, but it doesn't seem to work in
combination with panel.cloud (wireframe). I also searched for
information about the box.3d, without success.

Thanks in advance.

Peter Schmidtke


From Thierry.ONKELINX at inbo.be  Tue Mar 20 10:33:23 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 20 Mar 2007 10:33:23 +0100
Subject: [R] Row wise solving an equation
In-Reply-To: <73BF51CCB2BAE5488CDE8420E21AE2E01A31A3@xmail01.ad.ua.ac.be>
Message-ID: <2E9C414912813E4EB981326983E0A10402B7B929@inboexch.inbo.be>

Dear Gunter,

I've rewritten your functions. But allow me to give you a few tips:
- don't be affraid to use some spacing and indentation. It makes code a
lot more readable
- I prefer to define all variables in a function or I pass them to the
function. In your case would recommend to define the constants p, q, u
and v in the function Af. This might be a problem. 
- Have you considered how the uniroot function would react upon NA
values? I've added a check for it in the output function.
- Avoid using function names as a variable names like you did with "t"
and "q".

Cheers,

Thierry

Af <- function(AB, t0){
  p <- 1
  q0 <- 1
  u <- 1
  v <- 1
  q0 * (1 - exp(u * AB[2] * t0)) + p * (1 - exp(v * AB[2] * t0) - AB[1])
}

output <- function(AB){
  if(is.na(AB) > 0){
    NA
  } else {
    uniroot(Af, c(0, 9999), AB = AB)
  }
}

apply(ddt[, 2:3], 1, output)

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

 

> -----Oorspronkelijk bericht-----
> Van: r-help-bounces op stat.math.ethz.ch 
> [mailto:r-help-bounces op stat.math.ethz.ch] Namens Eerdekens Gunter
> Verzonden: dinsdag 20 maart 2007 9:31
> Aan: R-help op stat.math.ethz.ch
> Onderwerp: [R] Row wise solving an equation
> 
> 
> Hello R-list, 
> 
> How can I row wise solve a function for which the input 
> variables are written in a matrix or a vector and have the 
> calculated output added to the matrix or written in a vector? 
> 
> To specify my case in particular I continue: I would like R 
> to calculate for me a value for 't' which is function of 
> input parameters A and B, which are comprised in a matrix 
> called 'ddt' and some other constants. q,p,u,v in this example.
> 
> The equation (q*(1-exp(u*B*t))+( p*(1-exp(v*B*t)-A)
> 
> A and B may sometimes be 'missing values' (NA (not a number)) 
> because there isn't always a result for each time step.
> 
> 
> The matrix ddt looks like this
> 	Datetime	B	A	t
> [772,]  12/12/2005 15:27:12  NA         NA            NA          
> [773,]  12/12/2005 16:27:12  5000000    1.24480000    NA          
> [774,]  12/12/2005 17:27:12  NA         NA            NA          
> [775,]  13/12/2005 13:27:12  7012500    2.64038000    NA          
> [776,]  13/12/2005 14:27:12  NA         NA            NA          
> [777,]  13/12/2005 15:31:28  NA         NA            NA   
> 
> I've tried to calculate t as followed
> Af<-function(i,t){ (q*(1-exp(u*ddt[i,2]*t))+( 
> p*(1-exp(v*ddt[i,2]*t)- ddt[i,3])
> 
> Because input B is listed in the second column and input A is 
> listed in the third column of the matrix ddt. The fourth 
> column in ddt is meant for the row wise output values of 't'. 
> 
> To calculate 't' I've tried
> output<-function(i){uniroot(Af,c(0,9999),i=i)}
> 
> Uniroot has been used on single input values for A and B and 
> gives good results and I would like to automate this. 
> Therefore I tried to use mapply
> t<-mapply(output,i=i)
> 
> This seems not work and therefore I'm asking the list: "how 
> can I calculate in R the row wise value for 't'?"
> 
> Do you have a quick solution for it?
> 
> 
> Kind regards,
> Gunter Eerdekens
> 
> 
> --
> ---------------------------------------------------------------------
> Gunter.Eerdekens op ua.ac.be
> ----------------------------------------------------------------------
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
>


From jari.haukka at ktl.fi  Tue Mar 20 11:13:46 2007
From: jari.haukka at ktl.fi (Jari Haukka)
Date: Tue, 20 Mar 2007 12:13:46 +0200
Subject: [R] Any R function for self-controlled case series method /effect
 absorption?
Message-ID: <45FFB3DA.5040605@ktl.fi>

Hello,

Has anyone written R functions for applying self-controlled case series 
methods (http://statistics.open.ac.uk/sccs/).

In fact only thing needed is to modify glm function to allow absorption 
of effect. Eg. in Poisson model individual effect is used as factor, but 
it is considered as nuisance term where parameter estimates are not needed.

Could anyone point how absorbing individual effect could be carried out 
in glm.

There is already code for  Stata 
(http://statistics.open.ac.uk/sccs/stata/aglm.ado), Genstat 
(http://statistics.open.ac.uk/sccs/genstat%5Csccs.gen), Glim 
(http://statistics.open.ac.uk/sccs/glim/SCCS.MAC) , and SAS 
(http://statistics.open.ac.uk/sccs/sas/macro/poisreg.sas).


Regards,
Jari Haukka


From vmuggeo at dssm.unipa.it  Tue Mar 20 11:43:42 2007
From: vmuggeo at dssm.unipa.it (vito muggeo)
Date: Tue, 20 Mar 2007 11:43:42 +0100
Subject: [R] Any R function for self-controlled case series method
 /effect absorption?
In-Reply-To: <45FFB3DA.5040605@ktl.fi>
References: <45FFB3DA.5040605@ktl.fi>
Message-ID: <45FFBADE.7080103@dssm.unipa.it>

Dear Jari
The problem is to build the dataset to apply the conditional logit 
model. However, as far as I know, no R function exists.

BTW if you are dealing with time series of pollution and health, the 
following two papers might be of interest of you:
It appears that the time series approach could be preferred.


Heather J. Whitaker, Mounia N. Hocine, C. Paddy Farrington
On case-crossover methods for environmental time series data
Environmetrics
Volume 18, Issue 2, Date: March 2007, Pages: 157-171

Lu, Zeger. On the equivalence of case-crossover and time series methods 
in environmental epidemiology Biostatistics, Early view


Jari Haukka wrote:
> Hello,
> 
> Has anyone written R functions for applying self-controlled case series 
> methods (http://statistics.open.ac.uk/sccs/).
> 
> In fact only thing needed is to modify glm function to allow absorption 
> of effect. Eg. in Poisson model individual effect is used as factor, but 
> it is considered as nuisance term where parameter estimates are not needed.
> 
> Could anyone point how absorbing individual effect could be carried out 
> in glm.
> 
> There is already code for  Stata 
> (http://statistics.open.ac.uk/sccs/stata/aglm.ado), Genstat 
> (http://statistics.open.ac.uk/sccs/genstat%5Csccs.gen), Glim 
> (http://statistics.open.ac.uk/sccs/glim/SCCS.MAC) , and SAS 
> (http://statistics.open.ac.uk/sccs/sas/macro/poisreg.sas).
> 
> 
> Regards,
> Jari Haukka
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit? di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612


From n.nguyen at garvan.org.au  Tue Mar 20 12:32:16 2007
From: n.nguyen at garvan.org.au (Nguyen Dinh Nguyen)
Date: Tue, 20 Mar 2007 22:32:16 +1100 (EST)
Subject: [R] abline within data range
Message-ID: <20070320223216.AES02382@gimr.garvan.unsw.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070320/5c45ffc0/attachment.pl 

From albmont at centroin.com.br  Tue Mar 20 12:39:24 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Tue, 20 Mar 2007 09:39:24 -0200
Subject: [R] XML - can create but can't save
Message-ID: <20070320113447.M2698@centroin.com.br>

What's wrong with this?

library(XML)
tt <- xmlHashTree()
head <- addNode(xmlNode("head"), character(), tt)
test <- addNode(xmlNode("test", attrs=c(pi="4")), head, tt)
tt # ok
saveXML(tt, file="test.xml") # error

The error (in Portuguese) is:
Erro em saveXML(tt, file = "test.xml") : nenhum m?todo aplic?vel 
para "saveXML"

which could translate to
Error in saveXML(tt, file = "test.xml") : no method applicable to "saveXML"

Alberto Monteiro


From jari.oksanen at oulu.fi  Tue Mar 20 12:32:33 2007
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Tue, 20 Mar 2007 11:32:33 +0000 (UTC)
Subject: [R] about hcluster
References: <374361526.11178@capitalbio.com>
Message-ID: <loom.20070320T122754-335@post.gmane.org>

XinMeng <xmeng <at> capitalbio.com> writes:

> 
> Hi all:
> As to hcluster,how can I control the cluster is performed according to
rows(genes for instance) or
> columns(samples for instance)?
> I can't find the parameters for it.
> 

Function hclust (if that was intended with 'hcluster') needs a dissimilarity
matrix as an argument. If those dissimilarities are between rows, you will get
clustering of rows ("samples"). If you dissimiarities are between columns
("variables") then you will get a clustering of columns. Technically this is,
easy. If d <- dist(x) gives you Euclidean distances between rows, then d <-
dist(t(x)) gives you Euclidean distances between columns (function t()
transposes its argument). In practice, the problem is that you need to find a
dissimilarity measure that is meaningful for columns. Such dissimilarity
measures are rare, but there may be some alternatives floating around in R.

cheers, Jari Oksanen


From dimitris.rizopoulos at med.kuleuven.be  Tue Mar 20 12:49:23 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 20 Mar 2007 12:49:23 +0100
Subject: [R] abline within data range
References: <20070320223216.AES02382@gimr.garvan.unsw.edu.au>
Message-ID: <001201c76ae5$cdf92220$0540210a@www.domain>

try this:

x <- rnorm(200, 35, 5)
y <- rnorm(200, 0.87, 0.12)
###########
lmObj <- lm(y ~ x)
xs <- range(x)
ys <- predict(lmObj, newdata = data.frame(x = xs))
plot(x, y, pch = 17, bty = "l")
lines(xs, ys, col = "red", lty = 2, lwd = 2)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Nguyen Dinh Nguyen" <n.nguyen at garvan.org.au>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, March 20, 2007 12:32 PM
Subject: [R] abline within data range


> Dear R helpers,
>
> I would like to have abline,  for a lm model for
> example, lying within data range. Do you know how to
> get it?
>
> Thank in advance
>
> Nguyen D Nguyen
>
> #CODE
> x<- rnorm(200, 35,5)
> y<- rnorm(200, 0.87,0.12)
> plot(y~x, xlim=c(0,50), pch=17, bty="l")
> abline(lm(y~x))
>
> # I would like abline is between min(x) and max(x)
>
>
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From n.nguyen at garvan.org.au  Tue Mar 20 12:57:00 2007
From: n.nguyen at garvan.org.au (Nguyen Dinh Nguyen)
Date: Tue, 20 Mar 2007 22:57:00 +1100 (EST)
Subject: [R] abline within data range
Message-ID: <20070320225700.AES02689@gimr.garvan.unsw.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070320/931d3167/attachment.pl 

From ligges at statistik.uni-dortmund.de  Tue Mar 20 12:56:48 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 20 Mar 2007 12:56:48 +0100
Subject: [R] abline within data range
In-Reply-To: <20070320223216.AES02382@gimr.garvan.unsw.edu.au>
References: <20070320223216.AES02382@gimr.garvan.unsw.edu.au>
Message-ID: <45FFCC00.3080109@statistik.uni-dortmund.de>



Nguyen Dinh Nguyen wrote:
> Dear R helpers,
> 
> I would like to have abline,  for a lm model for
> example, lying within data range. Do you know how to
> get it?
> 
> Thank in advance
> 
> Nguyen D Nguyen
> 
> #CODE
> x<- rnorm(200, 35,5)
>  y<- rnorm(200, 0.87,0.12)
>  plot(y~x, xlim=c(0,50), pch=17, bty="l")
>  abline(lm(y~x))
>
> # I would like abline is between min(x) and max(x)

Just make it yourself as in:

  newdat <- data.frame(x = range(x))
  pred <- predict(lm(y ~ x), newdata = newdat)
  lines(newdat$x, pred)

Uwe Ligges




>  
> 
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jari.haukka at ktl.fi  Tue Mar 20 12:58:17 2007
From: jari.haukka at ktl.fi (Jari Haukka)
Date: Tue, 20 Mar 2007 13:58:17 +0200
Subject: [R] Any R function for self-controlled case series method
	/effect
In-Reply-To: <45FFBADE.7080103@dssm.unipa.it>
References: <45FFB3DA.5040605@ktl.fi> <45FFBADE.7080103@dssm.unipa.it>
Message-ID: <45FFCC59.7080906@ktl.fi>

Dear Vito,

I found found package "gnm" with function "gnm". The "eliminate" 
argument does exactly what I needed. Conditional Poisson models are 
handy to fit by gnm.


Regards, Jari



vito muggeo wrote:
> Dear Jari
> The problem is to build the dataset to apply the conditional logit 
> model. However, as far as I know, no R function exists.
>
> BTW if you are dealing with time series of pollution and health, the 
> following two papers might be of interest of you:
> It appears that the time series approach could be preferred.
>
>
> Heather J. Whitaker, Mounia N. Hocine, C. Paddy Farrington
> On case-crossover methods for environmental time series data
> Environmetrics
> Volume 18, Issue 2, Date: March 2007, Pages: 157-171
>
> Lu, Zeger. On the equivalence of case-crossover and time series 
> methods in environmental epidemiology Biostatistics, Early view
>
>
> Jari Haukka wrote:
>> Hello,
>>
>> Has anyone written R functions for applying self-controlled case 
>> series methods (http://statistics.open.ac.uk/sccs/).
>>
>> In fact only thing needed is to modify glm function to allow 
>> absorption of effect. Eg. in Poisson model individual effect is used 
>> as factor, but it is considered as nuisance term where parameter 
>> estimates are not needed.
>>
>> Could anyone point how absorbing individual effect could be carried 
>> out in glm.
>>
>> There is already code for  Stata 
>> (http://statistics.open.ac.uk/sccs/stata/aglm.ado), Genstat 
>> (http://statistics.open.ac.uk/sccs/genstat%5Csccs.gen), Glim 
>> (http://statistics.open.ac.uk/sccs/glim/SCCS.MAC) , and SAS 
>> (http://statistics.open.ac.uk/sccs/sas/macro/poisreg.sas).
>>
>>
>> Regards,
>> Jari Haukka
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>


From albmont at centroin.com.br  Tue Mar 20 13:09:50 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Tue, 20 Mar 2007 10:09:50 -0200
Subject: [R] XML - can create but can't save
In-Reply-To: <20070320113447.M2698@centroin.com.br>
References: <20070320113447.M2698@centroin.com.br>
Message-ID: <20070320120831.M29614@centroin.com.br>

I wrote:
> 
> library(XML)
> tt <- xmlHashTree()
> head <- addNode(xmlNode("head"), character(), tt)
> test <- addNode(xmlNode("test", attrs=c(pi="4")), head, tt)
> tt # ok
> saveXML(tt, file="test.xml") # error
> 
I found a way to circumvent this error, by replacing the saveXML
line with:

sink("test.xml")
print(tt)
sink()

Alberto Monteiro


From f.harrell at vanderbilt.edu  Tue Mar 20 13:38:11 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 20 Mar 2007 07:38:11 -0500
Subject: [R] R and clinical studies
In-Reply-To: <OF83AE8D5E.1AF9160D-ON882572A3.0061632B-882572A3.00647B35@irvine.edwards.com>
References: <OF83AE8D5E.1AF9160D-ON882572A3.0061632B-882572A3.00647B35@irvine.edwards.com>
Message-ID: <45FFD5B3.3080309@vanderbilt.edu>

Cody_Hamilton at Edwards.com wrote:
> Thank you to all those that responded to Delphine's original post on R and
> clinical studies.  They have provided much food for thought.
> 
> I had a couple of follow up questions/comments.  Andrew is very correct in
> pointing out that there are classes and workshops available for R.  It's my
> understanding that there are even commercial versions of R that now provide
> formal commercial-style courses.  And at any rate, the money saved by
> potentially avoiding pricey software could certainly justify any training
> expense in time or money  - this assumes of course that the pricey software
> could be dispensed with (I suspect that would take considerable time at my
> current company as so many legacy projects have been done in proprietary
> software).  I still think that R provides less 'hand-holding' and requires
> more initiative (which may be more or less present on a per
> programmer/statistician basis).
> 
> I guess one could always integrate R/Splus in with SAS, as Terry's group
> has done at Mayo - I will probably do this at least as a start.  I have a
> few concerns with regards to this approach (these may be needless concerns,
> but I will venture expressing them anyway).  First, I'm worried about the
> possibility of compatability concerns (will anyone be worried about a SAS
> dataset read into R or vice-versa?).  Second, I would prefer focusing all
> my learning on one package if possible.  I actually have more experience
> with SAS (as do others in my group), and if the switch to R is to be made I
> would like to make that switch as complete as possible.   This would also
> avoid requiring new hires to know both languages.  Third, if SAS is to be
> kept around, it defeats one of the main advantages of having open source
> code in the first place (R is wonderfully free!).  Like Mayo, Baylor Health
> (my previous employer) used both Splus and SAS.  I was warned that data
> manipulation would be much more difficult in R/Splus than it was in SAS.
> To be honest, and I say this humbly realizing that most posters to this
> list have much more experience than I, I haven't found data manipulation to
> be that much more difficult in R/Splus (at least as I have gained
> experience in R/Splus).   I can think of two exceptions (1) large datasets
> and (2) SAS seems to play nicer with MS products (e.g. PROC IMPORT seemed
> to read in messy Excel spreadsheets better than importData in Splus).  Is
> it possible (and I again say this with MUCH humility) that the perceived
> advantages of SAS with regards to data manipulation may be due in part to
> some users only using R/Splus for stat modeling and graphics (thus never
> becoming familiar with the data manipulation capabilities of R/Splus) or to
> the reluctance of SAS-trained individuals and companies to make the
> complete switch?

You are exactly correct on this point.  Some graduate programs only 
teach students how to use R/S-Plus for modeling and graphics.  R/S-Plus 
are wonderful for data manipulation - more powerful than SAS but not 
easy to learn (plus in R there are sometimes too many ways to do 
something; new users get lost - e.g. the reshape and reShape functions 
and the reshape package). 
http://biostat.mc.vanderbilt.edu/twiki/pub/Main/RS/sintro.pdf has many 
examples of complex data manipulation as do some web sites.  We do 
analysis for pharmaceutical companies with 100% of the data manipulation 
done in R after importing say 50 SAS datasets into R.  Doing tasks such 
as finding a lab value measured the closest in time to some event is 
much more elegant in R/S-Plus than in SAS.

Frank

> 
> Tony, the story about the "famous software" and the "certain operating
> system" at the "large company" was priceless.
> 
> In closing, I should mention that in all posts I am speaking for myself and
> not for Edwards LifeSciences.
> 
> Regards,
>     -Cody
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From Fabio.Sanchez at ki.se  Tue Mar 20 13:49:13 2007
From: Fabio.Sanchez at ki.se (Fabio Sanchez)
Date: Tue, 20 Mar 2007 13:49:13 +0100
Subject: [R] Multiple testing and Logistic regression in case control studies
Message-ID: <2d0835380703200549v1065f060u152ae6b500a5d035@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070320/a869bb11/attachment.pl 

From ritz at kvl.dk  Tue Mar 20 14:55:38 2007
From: ritz at kvl.dk (Christian Ritz)
Date: Tue, 20 Mar 2007 14:55:38 +0100
Subject: [R]  Error in nlme with factors in R 2.4.1
Message-ID: <45FFE7DA.9020506@kvl.dk>

Hi,

the following R lines work fine in R 2.4.0, but not in R 2.4.1 or any devel versions of R 
2.5.0 (see below for details).


library(drc)  # to load the dataset 'PestSci'

library(nlme)


## Setting starting values
sv <- c(0.43355869, 2.49963220, 0.05861799, 1.73290589, 0.38153146, 0.24316978)


## No error
m1 <-
nlme(SLOPE ~ c + (d-c)/(1+exp(b*(log(DOSE)-log(e)))),
fixed = list(b+c+d+e~1),
random = d~1|CURVE,
start = sv[c(2:5)], data = PestSci)


## No error
m2 <- nlme(SLOPE ~ c + (d-c)/(1+exp(b*(log(DOSE)-log(e)))),
fixed = list(b~HERBICIDE, c+d+e~1),
random = d~1|CURVE,
start = sv[c(1:5)], data = PestSci)


## Error in R 2.4.1!
m3 <-
nlme(SLOPE ~ c + (d-c)/(1+exp(b*(log(DOSE)-log(e)))),
fixed = list(b~factor(HERBICIDE)-1, c~1, d~1, e~factor(HERBICIDE)-1),
random = d~1|CURVE,
start = sv, data = PestSci)

Error in dimnames(data) <- dimnames : length of 'dimnames' [1] not equal to array extent


An ensuing call to traceback() yields:

7: array(0, c(n, n), list(levs, levs))
6: contr.treatment(n = 0)
5: do.call(contr[[nm]], list(n = length(levs)))
4: FUN("factor(HERBICIDE)"[[1]], ...)
3: lapply(nms, contrMat, contr = contr, data = dataMix)
2: nlme.formula(SLOPE ~ c + (d - c)/(1 + exp(b * (log(DOSE) - log(e)))),
        fixed = list(b ~ factor(HERBICIDE) - 1, c ~ 1, d ~ 1, e ~
            factor(HERBICIDE) - 1), random = d ~ 1 | CURVE, start = sv,
        data = PestSci)
1: nlme(SLOPE ~ c + (d - c)/(1 + exp(b * (log(DOSE) - log(e)))),
        fixed = list(b ~ factor(HERBICIDE) - 1, c ~ 1, d ~ 1, e ~
            factor(HERBICIDE) - 1), random = d ~ 1 | CURVE, start = sv,
        data = PestSci)





Output from sessionInfo() for R 2.4.0

R version 2.4.0 (2006-10-03)
i386-pc-mingw32

locale:
LC_COLLATE=Danish_Denmark.1252;LC_CTYPE=Danish_Denmark.1252;LC_MONETARY=Danish_Denmark.1252;LC_NUMERIC=C;LC_TIME=Danish_Denmark.1252

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
[7] "base"

other attached packages:
      drc  plotrix     nlme     MASS  lattice
  "1.0-7"  "2.1-1" "3.1-77" "7.2-29" "0.14-9"


Output from sessionInfo() for R 2.4.1

R version 2.4.1 (2006-12-18)
i386-pc-mingw32

locale:
LC_COLLATE=Danish_Denmark.1252;LC_CTYPE=Danish_Denmark.1252;LC_MONETARY=Danish_Denmark.1252;LC_NUMERIC=C;LC_TIME=Danish_Denmark.1252

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"
[5] "datasets"  "methods"   "base"

other attached packages:
       drc   plotrix      nlme      MASS   lattice
   "1.0-7"   "2.1-6"  "3.1-78"  "7.2-30" "0.14-16"






Christian


From jadamson at PARTNERS.ORG  Tue Mar 20 14:56:01 2007
From: jadamson at PARTNERS.ORG (Joel J. Adamson)
Date: Tue, 20 Mar 2007 09:56:01 -0400
Subject: [R] ERROR: 'latex' needed but missing on your system.
In-Reply-To: <45FF7F3B.3000804@ms.unimelb.edu.au>
References: <17914.54243.334456.271013@W0053328.mgh.harvard.edu>
	<45FF7F3B.3000804@ms.unimelb.edu.au>
Message-ID: <17919.59377.348263.434327@W0053328.mgh.harvard.edu>

Gad Abraham writes:
 > Joel J. Adamson wrote:
 > > After successfully building R on Slackware Linux v11.0 I went to make
 > > the documentation; the texi files went fine and then I hopefully issued
 > > 
 > > make dvi
 > > 
 > > after having gotten the warning to the effect of "You cannot build the
 > > DVI or PDF manuals, you are a loser" during compilation. And, as
 > > expected I got the
 > > error
 > > 
 > > ERROR: 'latex' needed but missing on your system.
 > 
 > It's a bit strange, because by default files like latex should be 
 > readable by all users. Did you install latex from source?

No, teTeX is installed as part of Slackware 11.0.  Yes, it's readable
by all users (including root, of course), but 
configure does not find it and issues a warning.  Then the "make dvi"
gives the error I quote above.

I think the real issue is my ignorance of makefiles and configure
scripts; I'm working on that.

 > Try this:
 > As root, do 'which latex' to see where it's installed. 

/usr/share/texmf/bin/latex is a symlink to pdfetex in the same
directory, also rwxr-xr-x, so it's executable by everybody.  Just to
remind you, I issed "make dvi" as root.

Joel

-- 
Joel J. Adamson
Biostatistician
Pediatric Psychopharmacology Research Unit
Massachusetts General Hospital
Boston, MA  02114
(617) 643-1432
(303) 880-3109





The information transmitted in this electronic communication is intended only for the person or entity to whom it is addressed and may contain confidential and/or privileged material. Any review, retransmission, dissemination or other use of or taking of any action in reliance upon this information by persons or entities other than the intended recipient is prohibited. If you received this information in error, please contact the Compliance HelpLine at 800-856-1983 and properly dispose of this information.


From pgilbert at bank-banque-canada.ca  Tue Mar 20 16:22:51 2007
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Tue, 20 Mar 2007 10:22:51 -0500
Subject: [R] vars :VARMA, multivariate time series?
In-Reply-To: <1c6126db0703151010g335635c2sbc9702f756bf8a2f@mail.gmail.com>
References: <1c6126db0703151010g335635c2sbc9702f756bf8a2f@mail.gmail.com>
Message-ID: <45FFFC4B.3020607@bank-banque-canada.ca>

sj wrote:
> I have a multivariate time series and I would like to build a forecasting
> model with both AR and MA terms, I think that this is possible in R. I have
> looked at the vars package and it looks like it is possible to estimate MA
> terms using the Phi and Psi functions but I am not sure how to incorporate
> the estimated terms into a forecasting model. I have also looked at the dse
> package, but have not been able to determine how to estimate a VARMA from
> the documentation,

hmmm.  In the dse1 package in the dse bundle you might look at 
?estMaxLik, or in the User's Guide, both of which have examples of this. 
  I'm not sure why people have trouble finding it, but if you let me 
know what you did look at then I will try to fix the documentation so it 
can be found more easily.

Paul Gilbert

  any direction on which packages to use for VARMA would be
> appreciated.
> 
> thanks,
> 
> Spencer
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
====================================================================================

La version fran?aise suit le texte anglais.

------------------------------------------------------------------------------------

This email may contain privileged and/or confidential inform...{{dropped}}


From P.Dalgaard at biostat.ku.dk  Tue Mar 20 15:17:54 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 20 Mar 2007 15:17:54 +0100
Subject: [R] ERROR: 'latex' needed but missing on your system.
In-Reply-To: <17919.59377.348263.434327@W0053328.mgh.harvard.edu>
References: <17914.54243.334456.271013@W0053328.mgh.harvard.edu>	<45FF7F3B.3000804@ms.unimelb.edu.au>
	<17919.59377.348263.434327@W0053328.mgh.harvard.edu>
Message-ID: <45FFED12.9030501@biostat.ku.dk>

Joel J. Adamson wrote:
> /usr/share/texmf/bin/latex is a symlink to pdfetex in the same
> directory, also rwxr-xr-x, so it's executable by everybody.  Just to
> remind you, I issed "make dvi" as root.
>   
Hm? Does it produce DVI or PDF then? If the latter, I can imagine that
configure will become confused.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From lauri.nikkinen at iki.fi  Tue Mar 20 15:33:17 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Tue, 20 Mar 2007 16:33:17 +0200
Subject: [R] Select the last two rows by id group
Message-ID: <ba8c09910703200733g37a25e8cve55952fc640c1ec1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070320/027e9e42/attachment.pl 

From marc_schwartz at comcast.net  Tue Mar 20 15:39:48 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 20 Mar 2007 09:39:48 -0500
Subject: [R] ERROR: 'latex' needed but missing on your system.
In-Reply-To: <17919.59377.348263.434327@W0053328.mgh.harvard.edu>
References: <17914.54243.334456.271013@W0053328.mgh.harvard.edu>
	<45FF7F3B.3000804@ms.unimelb.edu.au>
	<17919.59377.348263.434327@W0053328.mgh.harvard.edu>
Message-ID: <1174401588.4920.8.camel@localhost.localdomain>

On Tue, 2007-03-20 at 09:56 -0400, Joel J. Adamson wrote:
> Gad Abraham writes:
>  > Joel J. Adamson wrote:
>  > > After successfully building R on Slackware Linux v11.0 I went to make
>  > > the documentation; the texi files went fine and then I hopefully issued
>  > > 
>  > > make dvi
>  > > 
>  > > after having gotten the warning to the effect of "You cannot build the
>  > > DVI or PDF manuals, you are a loser" during compilation. And, as
>  > > expected I got the
>  > > error
>  > > 
>  > > ERROR: 'latex' needed but missing on your system.
>  > 
>  > It's a bit strange, because by default files like latex should be 
>  > readable by all users. Did you install latex from source?
> 
> No, teTeX is installed as part of Slackware 11.0.  Yes, it's readable
> by all users (including root, of course), but 
> configure does not find it and issues a warning.  Then the "make dvi"
> gives the error I quote above.
> 
> I think the real issue is my ignorance of makefiles and configure
> scripts; I'm working on that.
> 
>  > Try this:
>  > As root, do 'which latex' to see where it's installed. 
> 
> /usr/share/texmf/bin/latex is a symlink to pdfetex in the same
> directory, also rwxr-xr-x, so it's executable by everybody.  Just to
> remind you, I issed "make dvi" as root.

I'm not familiar with Slackware, but that seems an awfully unusual place
for the executable and likely why it is not being found (ie. it is not
in $PATH).

On my FC6 system:

[marcs at Bellerophon ~]$ which latex
/usr/bin/latex

[marcs at Bellerophon ~]$ ls -l /usr/bin/latex
lrwxrwxrwx 1 root root 7 Nov  8 16:12 /usr/bin/latex -> pdfetex

[marcs at Bellerophon ~]$ ls -l /usr/bin/pdfetex
-rwxr-xr-x 1 root root 1063840 Oct  1 17:11 /usr/bin/pdfetex


HTH,

Marc Schwartz


From jadamson at PARTNERS.ORG  Tue Mar 20 15:52:52 2007
From: jadamson at PARTNERS.ORG (Joel J. Adamson)
Date: Tue, 20 Mar 2007 10:52:52 -0400
Subject: [R] ERROR: 'latex' needed but missing on your system.
In-Reply-To: <45FFED12.9030501@biostat.ku.dk>
References: <17914.54243.334456.271013@W0053328.mgh.harvard.edu>
	<45FF7F3B.3000804@ms.unimelb.edu.au>
	<17919.59377.348263.434327@W0053328.mgh.harvard.edu>
	<45FFED12.9030501@biostat.ku.dk>
Message-ID: <17919.62788.20658.411523@W0053328.mgh.harvard.edu>

Peter Dalgaard writes:
 > Joel J. Adamson wrote:
 > > /usr/share/texmf/bin/latex is a symlink to pdfetex in the same
 > > directory, also rwxr-xr-x, so it's executable by everybody.  Just to
 > > remind you, I issed "make dvi" as root.
 > >   
 > Hm? Does it produce DVI or PDF then? If the latter, I can imagine that
 > configure will become confused.


.-(~)-----------------------------------------------------------(joel at ...)-
`--> whatis pdfetex
pdfetex              (1)  - PDF output from e-TeX


I'm as surprised as you are right now.  This is the regular "latex"
command, it produces .dvi files for me every day.  I repeat, I have
the same setup under PCLinuxOS and I had no ./configure problems.

Joel
-- 
Joel J. Adamson
Biostatistician
Pediatric Psychopharmacology Research Unit
Massachusetts General Hospital
Boston, MA  02114
(617) 643-1432
(303) 880-3109





The information transmitted in this electronic communication is intended only for the person or entity to whom it is addressed and may contain confidential and/or privileged material. Any review, retransmission, dissemination or other use of or taking of any action in reliance upon this information by persons or entities other than the intended recipient is prohibited. If you received this information in error, please contact the Compliance HelpLine at 800-856-1983 and properly dispose of this information.


From dimitris.rizopoulos at med.kuleuven.be  Tue Mar 20 15:57:03 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 20 Mar 2007 15:57:03 +0100
Subject: [R] Select the last two rows by id group
References: <ba8c09910703200733g37a25e8cve55952fc640c1ec1@mail.gmail.com>
Message-ID: <009301c76b00$0549e790$0540210a@www.domain>

you can use the following (based on Gabor's answer):

math <- c(80,75,70,65,65,70)
reading <- c(65,70,88,NA,90,NA)
id <- c('001','001','001','002','003','003')
score <- data.frame(id, reading, math)

#############

# it might be useful to sort first
# score <- score[order(score$id), ]

score
score[unlist(tapply(row.names(score), score$id, tail, n = 2)), ]

look at ?tail() for more info.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Lauri Nikkinen" <lauri.nikkinen at iki.fi>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, March 20, 2007 3:33 PM
Subject: [R] Select the last two rows by id group


> Hi R-users,
>
> Following this post 
> http://tolstoy.newcastle.edu.au/R/help/06/06/28965.html ,
> how do I get last two rows (or six or ten) by id group out of the 
> data
> frame? Here the example gives just the last row.
>
> Sincere thanks,
> Lauri
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From andy_liaw at merck.com  Tue Mar 20 15:57:08 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 20 Mar 2007 10:57:08 -0400
Subject: [R] Select the last two rows by id group
In-Reply-To: <ba8c09910703200733g37a25e8cve55952fc640c1ec1@mail.gmail.com>
References: <ba8c09910703200733g37a25e8cve55952fc640c1ec1@mail.gmail.com>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03E64B9D@usctmx1106.merck.com>

Something like the following should work:

last.n <- function(x, n) {
    last <- nrow(x)
    x[(last - n + 1):last, , drop=FALSE]
}
## Example: get the last two rows.
do.call(rbind, lapply(split(score, score$id), last.n, 2)) 

You might want to add a check in last.n() to make sure that there are at
least n rows to extract.

Andy

From: Lauri Nikkinen
> 
> Hi R-users,
> 
> Following this post 
> http://tolstoy.newcastle.edu.au/R/help/06/06/28965.html , how 
> do I get last two rows (or six or ten) by id group out of the 
> data frame? Here the example gives just the last row.
> 
> Sincere thanks,
> Lauri
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From jadamson at PARTNERS.ORG  Tue Mar 20 15:56:18 2007
From: jadamson at PARTNERS.ORG (Joel J. Adamson)
Date: Tue, 20 Mar 2007 10:56:18 -0400
Subject: [R] ERROR: 'latex' needed but missing on your system.
In-Reply-To: <1174401588.4920.8.camel@localhost.localdomain>
References: <17914.54243.334456.271013@W0053328.mgh.harvard.edu>
	<45FF7F3B.3000804@ms.unimelb.edu.au>
	<17919.59377.348263.434327@W0053328.mgh.harvard.edu>
	<1174401588.4920.8.camel@localhost.localdomain>
Message-ID: <17919.62994.811157.489403@W0053328.mgh.harvard.edu>

Marc Schwartz writes:
 > 
 > I'm not familiar with Slackware, but that seems an awfully unusual place
 > for the executable and likely why it is not being found (ie. it is not
 > in $PATH).

I agree, that is weird, however as I stated in my original post, that
directory is in everybody's path (users and root).  As I've also said,
I have the same tetex set up with the same directories in another
distro (PCLinuxOS) and "make dvi" worked flawlessly there.  

Do I need export the path, or edit the Makefile to include
/usr/share/texmf/bin?

Thanks,
Joel

-- 
Joel J. Adamson
Biostatistician
Pediatric Psychopharmacology Research Unit
Massachusetts General Hospital
Boston, MA  02114
(617) 643-1432
(303) 880-3109





The information transmitted in this electronic communication is intended only for the person or entity to whom it is addressed and may contain confidential and/or privileged material. Any review, retransmission, dissemination or other use of or taking of any action in reliance upon this information by persons or entities other than the intended recipient is prohibited. If you received this information in error, please contact the Compliance HelpLine at 800-856-1983 and properly dispose of this information.


From marc_schwartz at comcast.net  Tue Mar 20 15:58:32 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 20 Mar 2007 09:58:32 -0500
Subject: [R] Select the last two rows by id group
In-Reply-To: <ba8c09910703200733g37a25e8cve55952fc640c1ec1@mail.gmail.com>
References: <ba8c09910703200733g37a25e8cve55952fc640c1ec1@mail.gmail.com>
Message-ID: <1174402712.4920.20.camel@localhost.localdomain>

On Tue, 2007-03-20 at 16:33 +0200, Lauri Nikkinen wrote:
> Hi R-users,
> 
> Following this post http://tolstoy.newcastle.edu.au/R/help/06/06/28965.html ,
> how do I get last two rows (or six or ten) by id group out of the data
> frame? Here the example gives just the last row.
> 
> Sincere thanks,
> Lauri

A slight modification to Gabor's solution:

> score
  id reading math
1  1      65   80
2  1      70   75
3  1      88   70
4  2      NA   65
5  3      90   65
6  3      NA   70

# Return the last '2' rows
# Note the addition of unlist()

> score[unlist(tapply(rownames(score), score$id, tail,  2)), ]
  id reading math
2  1      70   75
3  1      88   70
4  2      NA   65
5  3      90   65
6  3      NA   70


Note that when tail() returns more than one value, tapply() will create
a list rather than a vector:

> tapply(rownames(score), score$id, tail,  2)
$`1`
[1] "2" "3"

$`2`
[1] "4"

$`3`
[1] "5" "6"


Thus, we need to unlist() the indices to use them in the subsetting
process that Gabor used in his solution.

Another alternative, if the rownames do not correspond to the sequential
row indices as they do in this example:

> do.call("rbind", lapply(split(score, score$id), tail,  2))
    id reading math
1.2  1      70   75
1.3  1      88   70
2    2      NA   65
3.5  3      90   65
3.6  3      NA   70


This uses split() to create a list of data frames from score, where each
data frame is 'split' by the 'id' column values. tail() is then applied
to each data frame using lapply(), the results of which are then
rbind()ed back to a single data frame.

HTH,

Marc Schwartz


From davee at ceu.ox.ac.uk  Tue Mar 20 15:59:38 2007
From: davee at ceu.ox.ac.uk (Dave Ewart)
Date: Tue, 20 Mar 2007 14:59:38 +0000
Subject: [R] Strange integer result on Debian/amd64
Message-ID: <20070320145938.GX16472@nemesis.ceu.ox.ac.uk>

Using the following version of R:

> R version 2.4.1 (2006-12-18)

installed using apt-get on a Debian/Sarge AMD64 system with the
following entry in /etc/apt/sources.lists:

deb http://www.stats.bris.ac.uk/R/bin/linux/debian/ stable/

The problem: I'm seeing strange results in a integer calculation as
follows:

> choose(11,6)
> [1] 462
> as.integer(choose(11,6))
> [1] 461

11-choose-6 should return 462 of course.  Other platforms and versions
of R (R/Windows and R/32-bit Linux on an Ubuntu/Dapper box) give the
correct result of 462 for the integer cast.

What on earth is going on here?  The values of 11 and 6 were
stumbled-upon by accident and I haven't been able to find other example
values which give a similar error.

Two AMD64/Linux systems I have both display the above error, actually; I
can't reproduce the error on any other platform/OS.

All help/suggestions/appreciated!

Dave.

-- 
Dave Ewart
davee at ceu.ox.ac.uk
Computing Manager, Cancer Epidemiology Unit
Cancer Research UK / Oxford University
PGP: CC70 1883 BD92 E665 B840 118B 6E94 2CFD 694D E370
Get key from http://www.ceu.ox.ac.uk/~davee/davee-ceu-ox-ac-uk.asc
N 51.7518, W 1.2016
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 191 bytes
Desc: Digital signature
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070320/81e9ebb6/attachment.bin 

From P.Dalgaard at biostat.ku.dk  Tue Mar 20 16:03:06 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 20 Mar 2007 16:03:06 +0100
Subject: [R] ERROR: 'latex' needed but missing on your system.
In-Reply-To: <45FFED12.9030501@biostat.ku.dk>
References: <17914.54243.334456.271013@W0053328.mgh.harvard.edu>	<45FF7F3B.3000804@ms.unimelb.edu.au>	<17919.59377.348263.434327@W0053328.mgh.harvard.edu>
	<45FFED12.9030501@biostat.ku.dk>
Message-ID: <45FFF7AA.4020307@biostat.ku.dk>

Peter Dalgaard wrote:
> Joel J. Adamson wrote:
>   
>> /usr/share/texmf/bin/latex is a symlink to pdfetex in the same
>> directory, also rwxr-xr-x, so it's executable by everybody.  Just to
>> remind you, I issed "make dvi" as root.
>>   
>>     
> Hm? Does it produce DVI or PDF then? If the latter, I can imagine that
> configure will become confused.
>
>   
Sorry, that was a red herring. TeTeX always has such links, it seems.
Marc's point that you need to make sure that the path includes
/usr/share/texmf/bin/ seems more relevant.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ritz at kvl.dk  Tue Mar 20 16:06:26 2007
From: ritz at kvl.dk (Christian Ritz)
Date: Tue, 20 Mar 2007 16:06:26 +0100
Subject: [R] Select the last two rows by id group
In-Reply-To: <ba8c09910703200733g37a25e8cve55952fc640c1ec1@mail.gmail.com>
References: <ba8c09910703200733g37a25e8cve55952fc640c1ec1@mail.gmail.com>
Message-ID: <45FFF872.10206@kvl.dk>

Hi Lauri,

here is a little modification of the solution for retrieving the last 
row only :


score[as.vector(unlist(tapply(rownames(score), score$id, tail, 2))),]


giving the last two rows. Replacing 2 by 6 or 10 gives you the last 6 
or 10 rows (if they exist).


Christian


From christos at nuverabio.com  Tue Mar 20 16:10:39 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Tue, 20 Mar 2007 11:10:39 -0400
Subject: [R] Select the last two rows by id group
In-Reply-To: <ba8c09910703200733g37a25e8cve55952fc640c1ec1@mail.gmail.com>
References: <ba8c09910703200733g37a25e8cve55952fc640c1ec1@mail.gmail.com>
Message-ID: <002f01c76b01$ec4f7c80$0e010a0a@headquarters.silicoinsights>

You can try the following:

n.len <- nrow(iris)
n.sel <- 3

iris[(n.len-n.sel+1):n.len, ]

-Christos 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Lauri Nikkinen
> Sent: Tuesday, March 20, 2007 10:33 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Select the last two rows by id group
> 
> Hi R-users,
> 
> Following this post 
> http://tolstoy.newcastle.edu.au/R/help/06/06/28965.html , how 
> do I get last two rows (or six or ten) by id group out of the 
> data frame? Here the example gives just the last row.
> 
> Sincere thanks,
> Lauri
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From marc_schwartz at comcast.net  Tue Mar 20 16:15:51 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 20 Mar 2007 10:15:51 -0500
Subject: [R] ERROR: 'latex' needed but missing on your system.
In-Reply-To: <17919.62994.811157.489403@W0053328.mgh.harvard.edu>
References: <17914.54243.334456.271013@W0053328.mgh.harvard.edu>
	<45FF7F3B.3000804@ms.unimelb.edu.au>
	<17919.59377.348263.434327@W0053328.mgh.harvard.edu>
	<1174401588.4920.8.camel@localhost.localdomain>
	<17919.62994.811157.489403@W0053328.mgh.harvard.edu>
Message-ID: <1174403751.4920.28.camel@localhost.localdomain>

On Tue, 2007-03-20 at 10:56 -0400, Joel J. Adamson wrote:
> Marc Schwartz writes:
>  > 
>  > I'm not familiar with Slackware, but that seems an awfully unusual place
>  > for the executable and likely why it is not being found (ie. it is not
>  > in $PATH).
> 
> I agree, that is weird, however as I stated in my original post, that
> directory is in everybody's path (users and root).  As I've also said,
> I have the same tetex set up with the same directories in another
> distro (PCLinuxOS) and "make dvi" worked flawlessly there.  
> 
> Do I need export the path, or edit the Makefile to include
> /usr/share/texmf/bin?
> 
> Thanks,
> Joel

My first inclination, which I just noted Peter also referenced, would be
to ensure that your build shell environment includes that directory in
your $PATH.

In an open console session (where you are going to build R), use:

  PATH=$PATH:/usr/share/texmf/bin
  export PATH

and then try building to see what happens.  Note that the above will be
temporary and will be lost once you close that console session.

I would check the system and user specific bashrc, .bashrc
and .bash_profile files on the PCLinuxOS system to see if that directory
is included in the $PATH on that system.  There may be an idiosyncratic
difference between the two linux distros.

HTH,

Marc


From M4Lawren at UWaterloo.Ca  Tue Mar 20 16:20:15 2007
From: M4Lawren at UWaterloo.Ca (Mike Lawrence)
Date: Tue, 20 Mar 2007 11:20:15 -0400
Subject: [R] How does glm(family='binomial') deal with perfect sucess?
Message-ID: <45FFFBAF.8070704@UWaterloo.Ca>

Hi all,

Trying to understand the logistic regression performed by glm (i.e. when 
family='binomial'), and I'm curious to know how it treats perfect 
success. That is, lets say I have the following summary data

	x=c(1,2,3,4,5,6)
	y=c(0,.04,.26,.76,.94,1)
	w=c(100,100,100,100,100,100)

where x is y is the probability of success at each value of x, 
calculated across w observations. When I use glm

	my.glm.obj=glm(y~x,family='binomial',weights=w)

the regression comes out fine, but if I try what I understand to be the 
equivalent lm procedure (i.e. fitting a straight line to the logit 
transformed y values):

	my.lm.obj=lm(qlogis(y)~x,weights=w)

I get an error because, of course, logit(1) = log(1/0) = log(Inf) = Inf
(similarly, logit(0) = log(0/1) = log(0) = -Inf).

I'd be very interested to see how glm deals with these extremes.

Cheers,

Mike


-- 
Mike Lawrence
http://artsweb.uwaterloo.ca/~m4lawren

"The road to wisdom? Well, it's plain and simple to express:
Err and err and err again, but less and less and less."
- Piet Hein


From johanfaux at yahoo.com  Tue Mar 20 16:33:49 2007
From: johanfaux at yahoo.com (johan Faux)
Date: Tue, 20 Mar 2007 08:33:49 -0700 (PDT)
Subject: [R] run a script during R CMD build
Message-ID: <939576.19517.qm@web56211.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070320/e4c3d16f/attachment.pl 

From ramasamy at cancer.org.uk  Tue Mar 20 16:33:45 2007
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 20 Mar 2007 15:33:45 +0000
Subject: [R] Select the last two rows by id group
In-Reply-To: <ba8c09910703200733g37a25e8cve55952fc640c1ec1@mail.gmail.com>
References: <ba8c09910703200733g37a25e8cve55952fc640c1ec1@mail.gmail.com>
Message-ID: <45FFFED9.3020001@cancer.org.uk>

Here is yet another solution. This one uses by() which generates nice 
visual output.

score <- data.frame(
  id      = c('001','001','001','002','003','003'),
  math    = c(80,75,70,65,65,70),
  reading = c(65,70,88,NA,90,NA)
)

out <- by( score, score$id, tail, n=2 )
# score$id: 001
#    id math reading
# 2 001   75      70
# 3 001   70      88
# ------------------------------------------------------------
# score$id: 002
#    id math reading
# 4 002   65      NA
# ------------------------------------------------------------
# score$id: 003
#    id math reading
# 5 003   65      90
# 6 003   70      NA


And if you want to put it back into a data frame, use

do.call( "rbind", as.list(out) )
#        id math reading
# 001.2 001   75      70
# 001.3 001   70      88
# 002   002   65      NA
# 003.5 003   65      90
# 003.6 003   70      NA

Ignore the rownames here.

HTH, Adai


Lauri Nikkinen wrote:
> Hi R-users,
> 
> Following this post http://tolstoy.newcastle.edu.au/R/help/06/06/28965.html ,
> how do I get last two rows (or six or ten) by id group out of the data
> frame? Here the example gives just the last row.
> 
> Sincere thanks,
> Lauri
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
>


From muenchen at utk.edu  Tue Mar 20 16:53:08 2007
From: muenchen at utk.edu (Muenchen, Robert A (Bob))
Date: Tue, 20 Mar 2007 11:53:08 -0400
Subject: [R] Select the last two rows by id group
In-Reply-To: <1174402712.4920.20.camel@localhost.localdomain>
References: <ba8c09910703200733g37a25e8cve55952fc640c1ec1@mail.gmail.com>
	<1174402712.4920.20.camel@localhost.localdomain>
Message-ID: <D028EEB4CA113D4EAFDD485CCC9982777313FC@UTKFSVS4.utk.tennessee.edu>

Very nice! This is almost duplicates the SAS first.var and last.var
ability to choose the first and last observations by group(s).
Substituting the head function in where Marc has the tail function below
will adapt it to the first n. It is more flexible than the SAS approach
because it can do the first/last n rather than just the single first or
last.

Let's say we want to choose the last observation in a county, and
counties have duplicate names in different states. You could sort by
state, then county, then use only county where Marc uses score$id in his
last example below, and it would get the last record for *every* county
regardless of duplicates. Does this sound correct? 

That's a handy bit of code!

Cheers,
Bob

=========================================================
  Bob Muenchen (pronounced Min'-chen), Manager  
  Statistical Consulting Center
  U of TN Office of Information Technology
  200 Stokely Management Center, Knoxville, TN 37996-0520
  Voice: (865) 974-5230  
  FAX:   (865) 974-4810
  Email: muenchen at utk.edu
  Web:   http://oit.utk.edu/scc, 
  News:  http://listserv.utk.edu/archives/statnews.html
=========================================================


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Marc Schwartz
> Sent: Tuesday, March 20, 2007 10:59 AM
> To: Lauri Nikkinen
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Select the last two rows by id group
> 
> On Tue, 2007-03-20 at 16:33 +0200, Lauri Nikkinen wrote:
> > Hi R-users,
> >
> > Following this post
> http://tolstoy.newcastle.edu.au/R/help/06/06/28965.html ,
> > how do I get last two rows (or six or ten) by id group out of the
> data
> > frame? Here the example gives just the last row.
> >
> > Sincere thanks,
> > Lauri
> 
> A slight modification to Gabor's solution:
> 
> > score
>   id reading math
> 1  1      65   80
> 2  1      70   75
> 3  1      88   70
> 4  2      NA   65
> 5  3      90   65
> 6  3      NA   70
> 
> # Return the last '2' rows
> # Note the addition of unlist()
> 
> > score[unlist(tapply(rownames(score), score$id, tail,  2)), ]
>   id reading math
> 2  1      70   75
> 3  1      88   70
> 4  2      NA   65
> 5  3      90   65
> 6  3      NA   70
> 
> 
> Note that when tail() returns more than one value, tapply() will
create
> a list rather than a vector:
> 
> > tapply(rownames(score), score$id, tail,  2)
> $`1`
> [1] "2" "3"
> 
> $`2`
> [1] "4"
> 
> $`3`
> [1] "5" "6"
> 
> 
> Thus, we need to unlist() the indices to use them in the subsetting
> process that Gabor used in his solution.
> 
> Another alternative, if the rownames do not correspond to the
> sequential
> row indices as they do in this example:
> 
> > do.call("rbind", lapply(split(score, score$id), tail,  2))
>     id reading math
> 1.2  1      70   75
> 1.3  1      88   70
> 2    2      NA   65
> 3.5  3      90   65
> 3.6  3      NA   70
> 
> 
> This uses split() to create a list of data frames from score, where
> each
> data frame is 'split' by the 'id' column values. tail() is then
applied
> to each data frame using lapply(), the results of which are then
> rbind()ed back to a single data frame.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From james.flood at fortis.com  Tue Mar 20 16:05:27 2007
From: james.flood at fortis.com (james.flood at fortis.com)
Date: Tue, 20 Mar 2007 16:05:27 +0100
Subject: [R] Translating code  from R into Matlab code
Message-ID: <2A0C429DDE22F640883DE621D9DA0B175848FA@spmw0048.mail.shared.fortis>

Hi,

Correct me if I am wrong but can I translate the R code into Matlab via
this package. ie I have a script in R, if I run this package on a Unix
emulator can I get the R code displayed in Matlab format ( R code
changed into Matlab code). If that is possible that would be great and
if so how. Also, If this program cannot do this do you know one that can
do this:

Regards,


James Flood
-------------- next part --------------

---------------------------------------------------------------
This e-mail, including any attachments, contains information
that is private and confidential and solely intended for the
addressee or addressees. If you are not the named or intended
recipient, please inform the sender immediately by reply
transmission and delete this e-mail, including any attachments,
without opening or copying it. In addition, in this event you
are hereby notified that any disclosure, reading, reproduction,
dissemination, distribution or any other use of this e-mail,
including any attachments, is strictly prohibited. No warranty
or guarantee is given with respect to the correct and accurate
transmission of the content as well as with respect to the
timely receipt of a sent e-mail.  

Messages and attachments are scanned for all viruses known. If
this message contains password-protected attachments, the files
have NOT been scanned for viruses by the Fortis.com mail
domain. Always scan attachments before opening them. 
---------------------------------------------------------------


From ramasamy at cancer.org.uk  Tue Mar 20 17:10:21 2007
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 20 Mar 2007 16:10:21 +0000
Subject: [R] run a script during R CMD build
In-Reply-To: <939576.19517.qm@web56211.mail.re3.yahoo.com>
References: <939576.19517.qm@web56211.mail.re3.yahoo.com>
Message-ID: <4600076D.8030309@cancer.org.uk>

Yes, one way is to use commandArgs in the R script. So say your R script 
is as follows

  n   <- as.character(commandArgs()[3])
  fn  <- as.character(commandArgs()[4])

  mat <- matrix( rnorm( n*n ), nc=n )
  write.table( mat, filenames=fn, sep="\t", quote=FALSE )



Then you execute the commands from command line as

   R --no-save < script 100 out.txt


This will run the R commands and output them to "out.txt".



johan Faux wrote:
> I would like R CMD build to run some R code which does some stuff and save the result as a file in /inst/docs folder. 
> Is there any way of doing this.
> 
> Thank you.
> Johan
> 
> 
> 
> 
>  
> ____________________________________________________________________________________
> We won't tell. Get more on shows you hate to love 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
>


From johanfaux at yahoo.com  Tue Mar 20 17:32:25 2007
From: johanfaux at yahoo.com (johan Faux)
Date: Tue, 20 Mar 2007 09:32:25 -0700 (PDT)
Subject: [R] run a script during R CMD build
Message-ID: <559176.69950.qm@web56213.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070320/9d9fcadb/attachment.pl 

From pvera at health.usf.edu  Tue Mar 20 18:15:31 2007
From: pvera at health.usf.edu (Vera, Pedro L.)
Date: Tue, 20 Mar 2007 13:15:31 -0400
Subject: [R] Problem adjusting x-labels with bargraphCI
Message-ID: <EF8F4571A0077848B837BD776D91C5F94B350C@COMEXCHANGE.hscnet.hsc.usf.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070320/cdd2aa91/attachment.pl 

From deepayan.sarkar at gmail.com  Tue Mar 20 18:32:57 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 20 Mar 2007 10:32:57 -0700
Subject: [R] grid on a wireframe plot
In-Reply-To: <4E21551A1CD7AA4BA2BF9EFE4C3D862A91A55B@crbsmxsusr07.pharma.aventis.com>
References: <4E21551A1CD7AA4BA2BF9EFE4C3D862A91A55B@crbsmxsusr07.pharma.aventis.com>
Message-ID: <eb555e660703201032y14ede633qde9bf6bfba307cfa@mail.gmail.com>

On 3/20/07, Peter.Schmidtke-stage at sanofi-aventis.com
<Peter.Schmidtke-stage at sanofi-aventis.com> wrote:
> Hello,
>
> I want to do a surface plot with wireframe from the lattice package.
>
> As for now I use the following command :
> print(wireframe(m,main="% my title", colorkey=TRUE,
> col.regions=rainbow(100), drape=TRUE,aspect =
> c(1,1.0),ylab="y",xlab="x",zlab="z",
> scales=list(arrows=FALSE)),split=c(1,1,3,2), more=TRUE)
>
> How can I show the gridlines in a 3D surface plot of this type?
> I already tried with panel.grid, but it doesn't seem to work in
> combination with panel.cloud (wireframe). I also searched for
> information about the box.3d, without success.

Could you explain in some more detail what you want? If you want grids
along the sides of the bounding box, that may be possible. Grids going
inside the box (or more generally, anything that would involve hidden
line/surface removal) will be hard. In any case, if you don't need
conditioning, you are better off trying to use rgl, which gives you
proper 3-D rendering.

Deepayan


From ramasamy at cancer.org.uk  Tue Mar 20 18:34:06 2007
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 20 Mar 2007 17:34:06 +0000
Subject: [R] run a script during R CMD build
In-Reply-To: <559176.69950.qm@web56213.mail.re3.yahoo.com>
References: <559176.69950.qm@web56213.mail.re3.yahoo.com>
Message-ID: <46001B0E.1050902@cancer.org.uk>

Sorry, I did not read the question properly.

I believe all your functions goes in mypkg/R and your data goes into 
mypkg/data subdirectory respectively but I am no expert in this area.

If you want to reflect your data from one folder to another, you can try 
using a symbolic or soft link in *nix systems
ln -s /inst/mydata.Rdata /somewhere/mypkg/data . Not sure if it the 
symbolic link approach will work when you try to R CMD BUILD mypkg.

You might be interested in the examples in package.skeleton().

Regards, Adai



johan Faux wrote:
> Thanks for your help.
> Maybe I was not clear in my question. 
> Let say I have a R script , myscript.R which produce some file "mydata.Rdata" and saves them in /inst folder.
> My question is where to I put my script so that it will run when I build the package using "R CMD build" ? 
> I want to include "mydata.RData" in my package and I want it to be updated every time i build the package.
> 
> I appreciate your help anyway.
> 
> 
> -Johan
> 
> ----- Original Message ----
> From: Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
> To: johan Faux <johanfaux at yahoo.com>
> Cc: r-help at stat.math.ethz.ch
> Sent: Tuesday, March 20, 2007 12:10:21 PM
> Subject: Re: [R] run a script during R CMD build
> 
> Yes, one way is to use commandArgs in the R script. So say your R script 
> is as follows
> 
>   n   <- as.character(commandArgs()[3])
>   fn  <- as.character(commandArgs()[4])
> 
>   mat <- matrix( rnorm( n*n ), nc=n )
>   write.table( mat, filenames=fn, sep="\t", quote=FALSE )
> 
> 
> 
> Then you execute the commands from command line as
> 
>    R --no-save < script 100 out.txt
> 
> 
> This will run the R commands and output them to "out.txt".
> 
> 
> 
> johan Faux wrote:
>> I would like R CMD build to run some R code which does some stuff and save the result as a file in /inst/docs folder. 
>> Is there any way of doing this.
>>
>> Thank you.
>> Johan
>>
>>
>>
>>
>>  
>> ____________________________________________________________________________________
>> We won't tell. Get more on shows you hate to love 
>>
>>
>>     [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
> 
> 
> 
> 
> 
> 
> 
> 
>  
> ____________________________________________________________________________________
> Don't pick lemons.
> See all the new 2007 cars at Yahoo! Autos.
> http://autos.yahoo.com/new_cars.html


From Greg.Snow at intermountainmail.org  Tue Mar 20 18:43:37 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 20 Mar 2007 11:43:37 -0600
Subject: [R] abline within data range
In-Reply-To: <20070320223216.AES02382@gimr.garvan.unsw.edu.au>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB8BC831@LP-EXCHVS07.CO.IHC.COM>

One approach is to use the clipplot function from the TeachingDemos
package.  This is probably overkill to load the package for doing this
just once, but if you are creating a more complicated graph with several
lines limited by their range then this may be of interest.

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nguyen 
> Dinh Nguyen
> Sent: Tuesday, March 20, 2007 5:32 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] abline within data range
> 
> Dear R helpers,
> 
> I would like to have abline,  for a lm model for example, 
> lying within data range. Do you know how to get it?
> 
> Thank in advance
> 
> Nguyen D Nguyen
> 
> #CODE
> x<- rnorm(200, 35,5)
>  y<- rnorm(200, 0.87,0.12)
>  plot(y~x, xlim=c(0,50), pch=17, bty="l")
>  abline(lm(y~x))
> 
> # I would like abline is between min(x) and max(x)
> 
>  
> 
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From young.stat at gmail.com  Tue Mar 20 18:58:37 2007
From: young.stat at gmail.com (Young Cho)
Date: Tue, 20 Mar 2007 10:58:37 -0700
Subject: [R] odbcConnect - no data source and default driver
Message-ID: <b44da9db0703201058w1c01eacaj64a6ccfb466aadae@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070320/b04299d6/attachment.pl 

From sergio.della.franca at gmail.com  Tue Mar 20 19:10:17 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Tue, 20 Mar 2007 19:10:17 +0100
Subject: [R] kmeans
Message-ID: <b490ce570703201110o791f3830i5e716286ef2da14e@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070320/4316acc1/attachment.pl 

From maitra at iastate.edu  Tue Mar 20 19:16:28 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Tue, 20 Mar 2007 13:16:28 -0500
Subject: [R] kmeans
In-Reply-To: <b490ce570703201110o791f3830i5e716286ef2da14e@mail.gmail.com>
References: <b490ce570703201110o791f3830i5e716286ef2da14e@mail.gmail.com>
Message-ID: <20070320131628.32bdde5a@subarnarekha.stat.iastate.edu>

Please read "An Introduction to R" which is available if your type 

> help.start() 

at the R-prompt.

HTH.
Ranjan

On Tue, 20 Mar 2007 19:10:17 +0100 "Sergio Della Franca" <sergio.della.franca at gmail.com> wrote:

> Dear R-helpers,
> 
> I have this dataset(y):
> 
>   YEAR   PRODUCTS
>   1             10
>   2             42
>   3             25
>   4             42
>   5             40
>   6             45
>   7             44
>   8             47
>   9             42
> 
> I perform kmeans clustering, and the results are the following:
> 
> 
> Cluster means:
>       YEAR  PRODUCTS
> 1 3.666667 41.33333
> 2 7.500000 44.50000
> 3 2.000000 17.50000
> 
> Clustering vector:
> 1 2 3 4 5 6 7 8 9
> 3 1 3 1 1 2 2 2 2
> Now my problem is add acolumn at my dataset(y) whit the information of
> clustering vector, i.e.:
> 
>    YEAR   PRODUCTS *clustering vector*
>   1             10                    *3*
>   2             42                    *1*
>   3             25                    *3*
>   4             42                    *1*
>   5             40                    *1*
>   6             45                    *2*
>   7             44                    *2*
>   8             47                    *2*
>   9             42                    *2*
> 
> 
> How can I obtain my new dataset with the information of clustering
> vector?
> 
> 
> Thank you in advance.
> 
> Sergio Della Franca.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Ted.Harding at manchester.ac.uk  Tue Mar 20 19:21:01 2007
From: Ted.Harding at manchester.ac.uk ( (Ted Harding))
Date: Tue, 20 Mar 2007 18:21:01 -0000 (GMT)
Subject: [R] Translating code  from R into Matlab code
In-Reply-To: <2A0C429DDE22F640883DE621D9DA0B175848FA@spmw0048.mail.shared.fortis>
Message-ID: <XFMail.070320182101.Ted.Harding@manchester.ac.uk>

On 20-Mar-07 15:05:27, james.flood at fortis.com wrote:
> Hi,
> 
> Correct me if I am wrong but can I translate the R code into
> Matlab via this package. ie I have a script in R, if I run
> this package on a Unix emulator can I get the R code displayed
> in Matlab format ( R code changed into Matlab code). If that is
> possible that would be great and if so how. Also, If this
> program cannot do this do you know one that can do this:
> 
> Regards,
> James Flood

  Ein program in R geschrieben ist nicht allgemein
  ins Matlab wortw?rtlich ?bersetzbar.

  A program in R written is not generally
  into Matlab word-wordly oversettable.

An R program cannot in general be literally translated
into Matlab.

There are several differences in syntax between R and Matlab,
R has a much richer class of data-structures than Matlab,
and even individual operators can work in different ways
in some circumstances. More importantly, an R function with
any complexity will differ radically from the Matlab equivalent
(and the return mechanism from functions is different).
Even at the basic level, a Matlab vector is either a row-vector
(a matrix with one row) or a column-vector (a matrix with one
column) and the two are different. R's "vector" is basically
a sequence of elements, and is neither "horizontal" nor
"vertical". To convert it to a Matlab vector, you need to
pass via a conversion to a matrix, and choose whether it
is to be a row or a column -- e.g.

  x <- matrix(c(x1,x2,x3,x4,x5),nrow=1)

While the two have enough in common for simple programs
to be translated more-or-less literally, there is, in my view,
no hope of an automatic translation from one to the other
which one could trust.

The way to go is as with any good translation between languages.
First read and understand clearly what has been expressed in
one language. Then use your skills in the other language to
express that exact same meaning in the other language.

The following is an example (in R and Octave) where the two are
closely similar, but even so not quite (there are a few subtle
differences which need hand-work). This is an implementation
of the "Pool Adjacent Violators Algorithm" for monotonic
regression, and in fact I wrote the R code by translating the
Octave code (derived from Matlab code which I found on the Web).

R CODE:
pava<-function(x,wt=rep(1,length(x)))
{
  n<-length(x)
  if(n<=1) return(x)
  lvlsets <- (1:n)
  repeat {
    viol<-(as.vector(diff(x))<0)
    if(!(any(viol))) break
    i <- min( (1:(n-1))[viol])
    lvl1<-lvlsets[i]
    lvl2<-lvlsets[i+1]
    ilvl<-(lvlsets==lvl1 | lvlsets==lvl2)
    x[ilvl]<-sum(x[ilvl]*wt[ilvl])/sum(wt[ilvl])
    lvlsets[ilvl]<-lvl1
  }
  x
}


OCTAVE CODE:
prefer_zero_one_indexing=1;
n = max(size(x));
if(nargin==1), wt=ones(size(x)); endif

lvlsets = (1:n)';
one2n   = (1:n-1)';
while (true)
  viol = (x(1:(n-1))-x(2:n) > 0);
  if(!any(viol)), break; endif
  i = min(find(viol));
  lvl1 = lvlsets(i)
  lvl2 = lvlsets(i+1);
  ilvl = (lvlsets==lvl1)|(lvlsets==lvl2);
  x(ilvl) = sum( x(ilvl).*wt(ilvl) )/sum(wt(ilvl));
  lvlsets(ilvl) = lvl1;
endwhile
p = x;
endfunction


For a general overview of the relation between R and Matlab
(strictly Octave, but the principles are the same) see the
"R and Octave" refernce card at:

  http://cran.r-project.org/doc/contrib/R-and-octave.txt

Hoping this helps,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 20-Mar-07                                       Time: 18:20:57
------------------------------ XFMail ------------------------------


From ted.harding at nessie.mcc.ac.uk  Tue Mar 20 19:21:26 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 20 Mar 2007 18:21:26 -0000 (GMT)
Subject: [R] Translating code  from R into Matlab code
In-Reply-To: <2A0C429DDE22F640883DE621D9DA0B175848FA@spmw0048.mail.shared.fortis>
Message-ID: <XFMail.070320182101.Ted.Harding@manchester.ac.uk>

On 20-Mar-07 15:05:27, james.flood at fortis.com wrote:
> Hi,
> 
> Correct me if I am wrong but can I translate the R code into
> Matlab via this package. ie I have a script in R, if I run
> this package on a Unix emulator can I get the R code displayed
> in Matlab format ( R code changed into Matlab code). If that is
> possible that would be great and if so how. Also, If this
> program cannot do this do you know one that can do this:
> 
> Regards,
> James Flood

  Ein program in R geschrieben ist nicht allgemein
  ins Matlab wortw?rtlich ?bersetzbar.

  A program in R written is not generally
  into Matlab word-wordly oversettable.

An R program cannot in general be literally translated
into Matlab.

There are several differences in syntax between R and Matlab,
R has a much richer class of data-structures than Matlab,
and even individual operators can work in different ways
in some circumstances. More importantly, an R function with
any complexity will differ radically from the Matlab equivalent
(and the return mechanism from functions is different).
Even at the basic level, a Matlab vector is either a row-vector
(a matrix with one row) or a column-vector (a matrix with one
column) and the two are different. R's "vector" is basically
a sequence of elements, and is neither "horizontal" nor
"vertical". To convert it to a Matlab vector, you need to
pass via a conversion to a matrix, and choose whether it
is to be a row or a column -- e.g.

  x <- matrix(c(x1,x2,x3,x4,x5),nrow=1)

While the two have enough in common for simple programs
to be translated more-or-less literally, there is, in my view,
no hope of an automatic translation from one to the other
which one could trust.

The way to go is as with any good translation between languages.
First read and understand clearly what has been expressed in
one language. Then use your skills in the other language to
express that exact same meaning in the other language.

The following is an example (in R and Octave) where the two are
closely similar, but even so not quite (there are a few subtle
differences which need hand-work). This is an implementation
of the "Pool Adjacent Violators Algorithm" for monotonic
regression, and in fact I wrote the R code by translating the
Octave code (derived from Matlab code which I found on the Web).

R CODE:
pava<-function(x,wt=rep(1,length(x)))
{
  n<-length(x)
  if(n<=1) return(x)
  lvlsets <- (1:n)
  repeat {
    viol<-(as.vector(diff(x))<0)
    if(!(any(viol))) break
    i <- min( (1:(n-1))[viol])
    lvl1<-lvlsets[i]
    lvl2<-lvlsets[i+1]
    ilvl<-(lvlsets==lvl1 | lvlsets==lvl2)
    x[ilvl]<-sum(x[ilvl]*wt[ilvl])/sum(wt[ilvl])
    lvlsets[ilvl]<-lvl1
  }
  x
}


OCTAVE CODE:
prefer_zero_one_indexing=1;
n = max(size(x));
if(nargin==1), wt=ones(size(x)); endif

lvlsets = (1:n)';
one2n   = (1:n-1)';
while (true)
  viol = (x(1:(n-1))-x(2:n) > 0);
  if(!any(viol)), break; endif
  i = min(find(viol));
  lvl1 = lvlsets(i)
  lvl2 = lvlsets(i+1);
  ilvl = (lvlsets==lvl1)|(lvlsets==lvl2);
  x(ilvl) = sum( x(ilvl).*wt(ilvl) )/sum(wt(ilvl));
  lvlsets(ilvl) = lvl1;
endwhile
p = x;
endfunction


For a general overview of the relation between R and Matlab
(strictly Octave, but the principles are the same) see the
"R and Octave" refernce card at:

  http://cran.r-project.org/doc/contrib/R-and-octave.txt

Hoping this helps,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 20-Mar-07                                       Time: 18:20:57
------------------------------ XFMail ------------------------------


From gavin.simpson at ucl.ac.uk  Tue Mar 20 19:43:46 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 20 Mar 2007 18:43:46 +0000
Subject: [R] kmeans
In-Reply-To: <b490ce570703201110o791f3830i5e716286ef2da14e@mail.gmail.com>
References: <b490ce570703201110o791f3830i5e716286ef2da14e@mail.gmail.com>
Message-ID: <1174416226.3618.25.camel@gsimpson.geog.ucl.ac.uk>

On Tue, 2007-03-20 at 19:10 +0100, Sergio Della Franca wrote:
> Dear R-helpers,
> 
> I have this dataset(y):
> 
>   YEAR   PRODUCTS
>   1             10
>   2             42
>   3             25
>   4             42
>   5             40
>   6             45
>   7             44
>   8             47
>   9             42
> 
> I perform kmeans clustering, and the results are the following:
> 
> 
> Cluster means:
>       YEAR  PRODUCTS
> 1 3.666667 41.33333
> 2 7.500000 44.50000
> 3 2.000000 17.50000
> 
> Clustering vector:
> 1 2 3 4 5 6 7 8 9
> 3 1 3 1 1 2 2 2 2
> Now my problem is add acolumn at my dataset(y) whit the information of
> clustering vector, i.e.:
> 
>    YEAR   PRODUCTS *clustering vector*
>   1             10                    *3*
>   2             42                    *1*
>   3             25                    *3*
>   4             42                    *1*
>   5             40                    *1*
>   6             45                    *2*
>   7             44                    *2*
>   8             47                    *2*
>   9             42                    *2*
> 
> 
> How can I obtain my new dataset with the information of clustering
> vector?

Given dat is your data.frame:

> dat
  YEAR PRODUCTS
1    1       10
2    2       42
3    3       25
4    4       42
5    5       40
6    6       45
7    7       44
8    8       47
9    9       42

then the following does what you want:

set.seed(12345)
clust <- kmeans(dat, 3) # 3 clusters as per example
new.dat <- data.frame(dat, Cluster = clust$cluster)
new.dat

Gives a new data frame with the extra column:

  YEAR PRODUCTS Cluster
1    1       10       1
2    2       42       3
3    3       25       1
4    4       42       3
5    5       40       3
6    6       45       2
7    7       44       2
8    8       47       2
9    9       42       2

Or if you really want to add to the original data do this directly:

dat$Cluster <- clust$cluster

which yields:

> dat
  YEAR PRODUCTS cluster
1    1       10       1
2    2       42       3
3    3       25       1
4    4       42       3
5    5       40       3
6    6       45       2
7    7       44       2
8    8       47       2
9    9       42       2

This is all covered in "An Introduction to R", which the posting guide
asks you to read.

HTH

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From bolker at zoo.ufl.edu  Tue Mar 20 19:44:42 2007
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Tue, 20 Mar 2007 18:44:42 +0000 (UTC)
Subject: [R] Problem adjusting x-labels with bargraphCI
References: <EF8F4571A0077848B837BD776D91C5F94B350C@COMEXCHANGE.hscnet.hsc.usf.edu>
Message-ID: <loom.20070320T194041-550@post.gmane.org>

Vera, Pedro L. <pvera <at> health.usf.edu> writes:

> 
> Hello:
> 
> I'm having quite a bit of difficulty adjusting the x-labels using bargraphCI.
I've tried using text and
> srt=45 to rotate the labels or mtext for 2 lines to break up the labels.
However, using either method, I
> cannot line up the labels with the midpoints of the bars (they line up with
some sort of tick mark that is off
> the midpoint of the bars). Any suggestions would be greatly appreciated.
> 

  I can't find bargraphCI using the R site search, so I can't tell
you anything about how to work with it.  What package does it come from??
(As the posting guide says, please send a simple reproducible example --
including any library() statements you used to load packages, and/or
the output of sessionInfo() .)  If bargraphCI works anything like
barplot in base R, then b=bargraphCI(...) will assign the midpoints
of the bars to b so you can use them in subsequent graphics calls.

  Ben Bolker


From h.wickham at gmail.com  Tue Mar 20 19:54:37 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 20 Mar 2007 13:54:37 -0500
Subject: [R] abline within data range
In-Reply-To: <20070320223216.AES02382@gimr.garvan.unsw.edu.au>
References: <20070320223216.AES02382@gimr.garvan.unsw.edu.au>
Message-ID: <f8e6ff050703201154l7e5127a8r7e20798c7996c15f@mail.gmail.com>

On 3/20/07, Nguyen Dinh Nguyen <n.nguyen at garvan.org.au> wrote:
> Dear R helpers,
>
> I would like to have abline,  for a lm model for
> example, lying within data range. Do you know how to
> get it?
>
> Thank in advance
>
> Nguyen D Nguyen
>
> #CODE
> x<- rnorm(200, 35,5)
>  y<- rnorm(200, 0.87,0.12)
>  plot(y~x, xlim=c(0,50), pch=17, bty="l")
>  abline(lm(y~x))

This is really easy with ggplot:

install.packages("ggplot")
library(ggplot)
qplot(x,y, type=c("point","smooth"), method=lm)

or, if you don't want the standard errors
qplot(x,y, type=c("point","smooth"), method=lm, se=F)

Hadley


From mkimpel at iupui.edu  Tue Mar 20 19:58:53 2007
From: mkimpel at iupui.edu (Mark W Kimpel)
Date: Tue, 20 Mar 2007 14:58:53 -0400
Subject: [R] getting ess/emacs to link with a remote instance of R
Message-ID: <46002EED.20600@iupui.edu>

I am running ess/emacs on Linux and have an R instance running on a 
remote Unix server. I would like to be able to direct input from my ess 
buffer to R (hope I am using the right lingo, I am new to emacs).

The ess manual contains a section describing how to do just that, but a 
prerequisite is to install a lisp file ssh.el , which can be found at 
ftp://ftp.splode.com/pub/users/friedman/emacs-lisp/ssh .

I have downloaded the file but can nowhere find exactly what directory I 
should put the file into and, what, if any, entry I might need to make 
in my .emacs file.

One reference says to put the file in the "lisp emacs" directory, but 
there are actually several directories that have names similar to this. 
I tried them all without success. Whenever I try M-x ssh, I get [no match].

Can an experienced ess/emacs Unix user give me a hand?

As a client, I am running openSuse 10.2 Linux and the latest versions of 
both ess and emacs, which both installed without problems.

Thanks,
Mark
-- 


Mark W. Kimpel MD
Neuroinformatics

Official Business Address:

Department of Psychiatry
Indiana University School of Medicine
PR M116
Institute of Psychiatric Research
791 Union Drive
Indianapolis, IN 46202

Preferred Mailing Address:

15032 Hunter Court
Westfield, IN  46074

(317) 490-5129 Work, & Mobile & VoiceMail

(317) 663-0513 Home (no voice mail please)

1-(317)-536-2730 FAX


From ripley at stats.ox.ac.uk  Tue Mar 20 20:00:42 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 20 Mar 2007 19:00:42 +0000 (GMT)
Subject: [R] odbcConnect - no data source and default driver
In-Reply-To: <b44da9db0703201058w1c01eacaj64a6ccfb466aadae@mail.gmail.com>
References: <b44da9db0703201058w1c01eacaj64a6ccfb466aadae@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0703201855180.17429@gannet.stats.ox.ac.uk>

You need to read the documentation for your ODBC driver manager 
(presumably iODBC) on your unstated OS.  You do need to set up a DSN or 
driver and the error message is that you have not done so.

[I have used iODBC in the past but no longer do so.  As I recall Debian 
Linux has it set up in a non-standard way so you do need the right set of 
instructions.]

On Tue, 20 Mar 2007, Young Cho wrote:

> I am trying to connect sybase sql databast from R using RODBC pkg and
> getting the following error ( i chnaged names to my***  but when I actually
> execute it, I put down names explicitly not calling some character strings)
>
>> channel = odbcConnect(dsn='mydsn',uid='myid',pwd='mypasswd')
> Warning messages:
> 1: [RODBC] ERROR: state IM002, code 0, message [iODBC][Driver Manager]Data
> source name not found and no default driver specified. Driver could not be
> loaded
> 2: ODBC connection failed in: odbcDriverConnect(st, case = case,
> believeNRows = believeNRows)
>> channel =
> odbcDriverConnect('SERVER=mydsn;DATABASE=my_subdb;UID=myid;PWD=mypasswd')
> Warning messages:
> 1: [RODBC] ERROR: state IM007, code 5768012, message [iODBC][Driver
> Manager]No data source or driver specified, dialog prohibited
> 2: ODBC connection failed in:
> odbcDriverConnect("SERVER=mydsn;DATABASE=my_subdb;UID=myid;PWD=mypasswd")
>
> I can see the connection by ping the server in a xterm:
>
> ~ > ping mydsn
> mydsn is alive
>
> Can you give me an advice how to create the channel object correctly? Thanks
> so much!
>
> Young
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ted.harding at nessie.mcc.ac.uk  Tue Mar 20 20:11:42 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 20 Mar 2007 19:11:42 -0000 (GMT)
Subject: [R] Translating code  from R into Matlab code
In-Reply-To: <XFMail.070320182101.Ted.Harding@manchester.ac.uk>
Message-ID: <XFMail.070320191142.ted.harding@nessie.mcc.ac.uk>

Apologies for my previous response appearing twice on the list.

I first accidentally sent it from an address not subscribed
to the list, so I thought it would not get through, and then
re-sent it from the subscribed address. It seems both got to
the list.

Sorry.
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 20-Mar-07                                       Time: 19:11:24
------------------------------ XFMail ------------------------------


From hodgsons at msu.edu  Tue Mar 20 20:22:14 2007
From: hodgsons at msu.edu (Sarah Hodgson)
Date: Tue, 20 Mar 2007 15:22:14 -0400
Subject: [R] LDA newbie question
Message-ID: <E177D8CF-48D5-4B86-BF57-2B84F4BE7773@msu.edu>

I am using lda for the first time.  I am using version 2.3.1 of R.   
When I ran the lda I did not get Proportion of trace in the output.   
Is there another way to get this or is there a bug in my version?

Sarah Hodgson


From lauri.nikkinen at iki.fi  Tue Mar 20 20:13:26 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Tue, 20 Mar 2007 21:13:26 +0200
Subject: [R] Select the last two rows by id group
In-Reply-To: <45FFFED9.3020001@cancer.org.uk>
References: <ba8c09910703200733g37a25e8cve55952fc640c1ec1@mail.gmail.com>
	<45FFFED9.3020001@cancer.org.uk>
Message-ID: <ba8c09910703201213v5fc9ef55mcf61dfc9f0450763@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070320/479fa725/attachment.pl 

From enrico.foscolo at libero.it  Tue Mar 20 20:17:54 2007
From: enrico.foscolo at libero.it (enrico.foscolo)
Date: Tue, 20 Mar 2007 20:17:54 +0100
Subject: [R] Problems about Derivaties
Message-ID: <JF7UXU$288630A93C59D0253D637C34A4460A32@libero.it>

Dear participants to the list,

this is my problem: I want to obtain an expression that represents the second
derivative of one function.
With "deriv3" (package "stats") it is possible to evaluate the second
derivative, but I do not know how I can get the (analytical) expression of this
derivative.

For example: Suppose that I have a function of this form:
f(x,y)=x^3+y^3+(x^2)*(y^2). With "deriv3" I can evaluate the first derivative
and the hessian matrix, as follows:

> d<-deriv3(~x^3+y^3+(x^2)*(y^2),c("x","y"))
> d[[1]]
{
.expr4 <- x^2
.expr5 <- y^2
.expr9 <- 2 * x
.expr15 <- 2 * y
.value <- x^3 + y^3 + .expr4 * .expr5
.grad <- array(0, c(length(.value), 2), list(NULL, c("x",
"y")))
.hessian <- array(0, c(length(.value), 2, 2), list(NULL,
c("x", "y"), c("x", "y")))
.grad[, "x"] <- 3 * .expr4 + .expr9 * .expr5
.hessian[, "x", "x"] <- 3 * .expr9 + 2 * .expr5
.hessian[, "x", "y"] <- .hessian[, "y", "x"] <- .expr9 *
.expr15
.grad[, "y"] <- 3 * .expr5 + .expr4 * .expr15
.hessian[, "y", "y"] <- 3 * .expr15 + .expr4 * 2
attr(.value, "gradient") <- .grad
attr(.value, "hessian") <- .hessian
.value
}
> d[[1]][11]
.hessian[, "x", "y"] <- .hessian[, "y", "x"] <- .expr9 * .expr15()
> typeof(d[[1]][11])
[1] "language"
> is.call(d[[1]][11])
[1] TRUE

Is it possible to extract the formula of the second derivative of this function
from the output of deriv3?

I would like to use this expression as new function, which means that in this
case, I would get the string ".hessian[, "x", "y"] <- .hessian[, "y", "x"] <-
.expr9 * .expr15".

Many thanks for any kind of help in advance,
Enrico



------------------------------------------------------
Trova il tuo mutuo su misura. Tassi ridotti da 4.25% solo per richieste online. Mutuionline.it
http://click.libero.it/mutuionline20ma07


From sfalcon at fhcrc.org  Tue Mar 20 20:38:47 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 20 Mar 2007 12:38:47 -0700
Subject: [R] getting ess/emacs to link with a remote instance of R
In-Reply-To: <46002EED.20600@iupui.edu> (Mark W. Kimpel's message of "Tue,
	20 Mar 2007 14:58:53 -0400")
References: <46002EED.20600@iupui.edu>
Message-ID: <m2r6rjg0xk.fsf@ziti.fhcrc.org>

Hi Mark,

[I've cc'd the ess-help list and any further discussion should probably
happen there with r-help removed]

Mark W Kimpel <mkimpel at iupui.edu> writes:
> I am running ess/emacs on Linux and have an R instance running on a 
> remote Unix server. I would like to be able to direct input from my ess 
> buffer to R (hope I am using the right lingo, I am new to emacs).
>
> The ess manual contains a section describing how to do just that, but a 
> prerequisite is to install a lisp file ssh.el , which can be found at 
> ftp://ftp.splode.com/pub/users/friedman/emacs-lisp/ssh .

In my experience, ssh.el doesn't do anything all that great.  Instead
I would try:

1. Start a shell inside Emacs using 'M-x shell'.  In this shell, ssh
   to your remote host and start R.

2. In the remote R session do: 'M-x ess-remote'.  At the Emacs
   mini-buffer prompt type 'r'.

3. In a buffer containing R code, you should be able to send code to
   the remote session as usual (choose the shell buffer when
   prompted).

Note:

  * C-c C-c will kill your ssh session (and R).  IOW, you can't easily
    interrupt an R process running remotely.

  * I have never had success getting help to work via ESS when running
    R remotely.  YMMV.

  * You may find that sending commands to the remote session without
    echoing is much faster.  So you might try C-u C-c C-r to send a
    region.


+ seth

-- 
Seth Falcon | Computational Biology | Fred Hutchinson Cancer Research Center
http://bioconductor.org


From murdoch at stats.uwo.ca  Tue Mar 20 21:25:51 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 20 Mar 2007 16:25:51 -0400
Subject: [R] Problems about Derivaties
In-Reply-To: <JF7UXU$288630A93C59D0253D637C34A4460A32@libero.it>
References: <JF7UXU$288630A93C59D0253D637C34A4460A32@libero.it>
Message-ID: <4600434F.5040209@stats.uwo.ca>

On 3/20/2007 3:17 PM, enrico.foscolo wrote:
> Dear participants to the list,
> 
> this is my problem: I want to obtain an expression that represents the second
> derivative of one function.
> With "deriv3" (package "stats") it is possible to evaluate the second
> derivative, but I do not know how I can get the (analytical) expression of this
> derivative.
> 
> For example: Suppose that I have a function of this form:
> f(x,y)=x^3+y^3+(x^2)*(y^2). With "deriv3" I can evaluate the first derivative
> and the hessian matrix, as follows:

R isn't the package I'd choose for symbolic calculations, but you can 
get what you want with D:

 > e <- expression(x^3+y^3+(x^2)*(y^2))
 > D(D(e, "x"), "y")
2 * x * (2 * y)


Duncan Murdoch


From tura at centroin.com.br  Tue Mar 20 22:18:52 2007
From: tura at centroin.com.br (Bernardo Rangel Tura)
Date: Tue, 20 Mar 2007 18:18:52 -0300
Subject: [R] error installing packages
In-Reply-To: <2FC987BC0B90B24786CAF43DD3F5719C86B073@CRU105.cahe.ad.wsu.edu>
References: <2FC987BC0B90B24786CAF43DD3F5719C86B073@CRU105.cahe.ad.wsu.edu>
Message-ID: <1174425532.22147.3.camel@Thux>

On Wed, 2007-03-07 at 09:33 -0800, Bricklemyer, Ross S wrote:
> I was finally able to get R to 'configure', 'make', and 'install' on Mandriva 2007.  Itried to install gnomeGUI and I received an error.  See below.  At what step do I make R a shared library?  Where did I go wrong?
> 
> Ross
> 
> ==================================================
> downloaded 74Kb
> 
> * Installing *Frontend* package 'gnomeGUI' ...
> Using R Installation in R_HOME=/usr/local/lib64/R
> R was not built as a shared library
> Need a shared R library
> ERROR: configuration failed for package 'gnomeGUI'
> * Removing '/usr/local/lib64/R/library/gnomeGUI'
> 
> The downloaded packages are in
>         /root/tmp/RtmpkHUeyA/downloaded_packages
> Warning message:
> installation of package 'gnomeGUI' had non-zero exit status in: install.packages(c("gnomeGUI"))
> =================================================================================================


Hi Ross!

I use Ubuntu and instaling R using Synaptic. In this software have this
observation about GnomeGUI:

As of R 2.1.0, this interface is no longer provided with the upstream
sources. As such, this package is now an empty stub that will be removed
in a subsequent revision of the Debian package.

So I think  gnomeGUI not instalable in R now...
-- 
Bernardo Rangel Tura,M.D.,Ph.D
National Institute of Cardiology
Rio de Janeiro - Brazil


From bates at stat.wisc.edu  Tue Mar 20 22:39:48 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 20 Mar 2007 16:39:48 -0500
Subject: [R] error installing packages
In-Reply-To: <2FC987BC0B90B24786CAF43DD3F5719C86B073@CRU105.cahe.ad.wsu.edu>
References: <Acdg3r87uaxnqbyARAmE12idw6CNgw==>
	<2FC987BC0B90B24786CAF43DD3F5719C86B073@CRU105.cahe.ad.wsu.edu>
Message-ID: <40e66e0b0703201439y5196bf73rdc85d7b7822f37e9@mail.gmail.com>

On 3/7/07, Bricklemyer, Ross S <rsb at wsu.edu> wrote:
>
> I was finally able to get R to 'configure', 'make', and 'install' on Mandriva 2007.  Itried to install gnomeGUI and I received an error.  See below.  At what step do I make R a shared library?  Where did I go wrong?

At the configure stage.  You must add --enable-R-shlib to the call to
configure so that R is also built as a shared library.  Try running
configure with the --help flag first to see all the possible options.

Reconfiguring and recompiling will probably not, by itself, allow you
to build the gnomeGUI.  You may need to install some of the GNOME
development packages from RPM's before this package can compile
successfully.   You may want to consider if learning emacs and ESS is
a better use of your time.


>
> Ross
>
> ==================================================
> downloaded 74Kb
>
> * Installing *Frontend* package 'gnomeGUI' ...
> Using R Installation in R_HOME=/usr/local/lib64/R
> R was not built as a shared library
> Need a shared R library
> ERROR: configuration failed for package 'gnomeGUI'
> * Removing '/usr/local/lib64/R/library/gnomeGUI'
>
> The downloaded packages are in
>         /root/tmp/RtmpkHUeyA/downloaded_packages
> Warning message:
> installation of package 'gnomeGUI' had non-zero exit status in: install.packages(c("gnomeGUI"))
> =================================================================================================
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bates at stat.wisc.edu  Tue Mar 20 22:48:12 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 20 Mar 2007 16:48:12 -0500
Subject: [R] How does glm(family='binomial') deal with perfect sucess?
In-Reply-To: <45FFFBAF.8070704@UWaterloo.Ca>
References: <45FFFBAF.8070704@UWaterloo.Ca>
Message-ID: <40e66e0b0703201448s21d98d89u5be6ab8cab521180@mail.gmail.com>

On 3/20/07, Mike Lawrence <M4Lawren at uwaterloo.ca> wrote:
> Hi all,
>
> Trying to understand the logistic regression performed by glm (i.e. when
> family='binomial'), and I'm curious to know how it treats perfect
> success. That is, lets say I have the following summary data
>
>         x=c(1,2,3,4,5,6)
>         y=c(0,.04,.26,.76,.94,1)
>         w=c(100,100,100,100,100,100)
>
> where x is y is the probability of success at each value of x,
> calculated across w observations. When I use glm
>
>         my.glm.obj=glm(y~x,family='binomial',weights=w)
>
> the regression comes out fine, but if I try what I understand to be the
> equivalent lm procedure (i.e. fitting a straight line to the logit
> transformed y values):
>
>         my.lm.obj=lm(qlogis(y)~x,weights=w)
>
> I get an error because, of course, logit(1) = log(1/0) = log(Inf) = Inf
> (similarly, logit(0) = log(0/1) = log(0) = -Inf).

> I'd be very interested to see how glm deals with these extremes.

Well, for one thing glm doesn't fit the model that way.  It uses
iteratively reweighted least squares in which the calculation of the
next set of parameter values is performed as a weighted least squares
fit on the scale of the mean, not on the scale of the linear
predictor, for exactly the reason that you have encountered.


From mkimpel at iupui.edu  Tue Mar 20 22:50:35 2007
From: mkimpel at iupui.edu (Mark W Kimpel)
Date: Tue, 20 Mar 2007 17:50:35 -0400
Subject: [R] getting ess/emacs to link with a remote instance of R
In-Reply-To: <m2r6rjg0xk.fsf@ziti.fhcrc.org>
References: <46002EED.20600@iupui.edu> <m2r6rjg0xk.fsf@ziti.fhcrc.org>
Message-ID: <4600572B.6050104@iupui.edu>

Seth,

Thank you for your response, every worked exactly as you described it, 
even the fact that ?help does not work very well with a remote session. 
I will probably open up a separate emacs window with a local R process 
to access help functions.

I didn't know about the ess-help, I had checked out emacs-help but, 
looking at an archive, it looked like it had very little activity over 
the past two years. Maybe I was mistaken?

Thanks,

Mark


Mark W. Kimpel MD
Neuroinformatics

Official Business Address:

Department of Psychiatry
Indiana University School of Medicine
PR M116
Institute of Psychiatric Research
791 Union Drive
Indianapolis, IN 46202

Preferred Mailing Address:

15032 Hunter Court
Westfield, IN  46074

(317) 490-5129 Work, & Mobile & VoiceMail

(317) 663-0513 Home (no voice mail please)

1-(317)-536-2730 FAX


Seth Falcon wrote:
> Hi Mark,
> 
> [I've cc'd the ess-help list and any further discussion should probably
> happen there with r-help removed]
> 
> Mark W Kimpel <mkimpel at iupui.edu> writes:
>> I am running ess/emacs on Linux and have an R instance running on a 
>> remote Unix server. I would like to be able to direct input from my ess 
>> buffer to R (hope I am using the right lingo, I am new to emacs).
>>
>> The ess manual contains a section describing how to do just that, but a 
>> prerequisite is to install a lisp file ssh.el , which can be found at 
>> ftp://ftp.splode.com/pub/users/friedman/emacs-lisp/ssh .
> 
> In my experience, ssh.el doesn't do anything all that great.  Instead
> I would try:
> 
> 1. Start a shell inside Emacs using 'M-x shell'.  In this shell, ssh
>    to your remote host and start R.
> 
> 2. In the remote R session do: 'M-x ess-remote'.  At the Emacs
>    mini-buffer prompt type 'r'.
> 
> 3. In a buffer containing R code, you should be able to send code to
>    the remote session as usual (choose the shell buffer when
>    prompted).
> 
> Note:
> 
>   * C-c C-c will kill your ssh session (and R).  IOW, you can't easily
>     interrupt an R process running remotely.
> 
>   * I have never had success getting help to work via ESS when running
>     R remotely.  YMMV.
> 
>   * You may find that sending commands to the remote session without
>     echoing is much faster.  So you might try C-u C-c C-r to send a
>     region.
> 
> 
> + seth
>


From n.nguyen at garvan.org.au  Tue Mar 20 22:53:19 2007
From: n.nguyen at garvan.org.au (Nguyen Dinh Nguyen)
Date: Wed, 21 Mar 2007 08:53:19 +1100
Subject: [R] abline within data range
In-Reply-To: <f8e6ff050703201154l7e5127a8r7e20798c7996c15f@mail.gmail.com>
Message-ID: <001301c76b3a$2c7f8ba0$0fe05e81@D145LD1S>

Dear all Rhelpers,

Thank you all for quick and helpful response.

Question: How to get the abline lying within the data range?

Suggested solutions:

## 1-  base graphics (suggested by Dimitris and Uwe)

x<- rnorm(200, 35,5)
y<- rnorm(200, 0.87,0.12)

plot(y~x, xlim=c(0,55), pch=17, bty="l")
lmObj <- lm(y ~ x)
xs <- range(x)
ys <- predict(lmObj, newdata = data.frame(x = xs))
plot(x, y, pch = 17, bty = "l")
lines(xs, ys, col = "red", lty = 2, lwd = 2)

## 2- using ggplot package (by Hadley)
install.packages("ggplot")
library(ggplot)

qplot(x,y, type=c("point","smooth"), method=lm)

#or, if you don't want the standard errors
qplot(x,y, type=c("point","smooth"), method=lm, se=F)

## 3-Using TeachingDemo package (suggested by Greg)
# but I haven?t try yet


Regards

Nguyen


From dbeyer at u.washington.edu  Tue Mar 20 22:51:24 2007
From: dbeyer at u.washington.edu (Dick Beyer)
Date: Tue, 20 Mar 2007 14:51:24 -0700 (PDT)
Subject: [R] RMySQL load error
In-Reply-To: <mailman.13.1174302003.8336.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.43.0703201451240.8002@hymn06.u.washington.edu>

I'm having trouble getting RMySQL to load.  I was able to build it and install 
it (RMySQL_0.5-11.tar.gz).  I was also able to build and install mysql-5.0.37.

I've read many postings about this but have not found a mention of my 
particular problem (some closely related).  I get the "unable to load shared 
library, no such file" error, but all the files do exist, RMySQL.so and 
libmysqlclient.so.15.

I set these environment variables:

PKG_LIBS=-L/usr/local/lib/mysql -lmysqlclient 
PKG_CPPFLAGS=-I/usr/local/include/mysql

If anyone had any suggestions, ideas, or pointers, I would be eternally 
grateful for the help.  Here is the relevant output from my R session and from 
building the RMySQL package (I am using gcc version 3.4.6 and RedHat AS4  
2.6.9-42.0.10.ELsmp):

> library(RMySQL) 
Loading required package: DBI 
Error in dyn.load(x, as.logical(local), as.logical(now)) :
         unable to load shared library 
'/usr/lib64/R/library/RMySQL/libs/RMySQL.so':
   libmysqlclient.so.15: cannot open shared object file: No such file or 
directory 
Error in library(RMySQL) : .First.lib failed for 'RMySQL'

> sessionInfo() 
R version 2.4.1 (2006-12-18) 
x86_64-redhat-linux-gnu

locale: 
LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C

attached base packages: 
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods" 
[7] "base"

other attached packages:
      DBI 
"0.1-12"

[root at ws-bioinf-01 MySQL]# ls -l /usr/lib64/R/library/RMySQL/libs/ 
total 124 
-rwxr-xr-x  1 root root 122497 Mar 20 11:49 RMySQL.so

[root at ws-bioinf-01 MySQL]# ll /usr/local/lib/mysql/ 
total 2632 
-rw-r--r--  1 root root  27208 Mar 20 11:47 libdbug.a 
-rw-r--r--  1 root root  71144 Mar 20 11:47 libheap.a 
-rw-r--r--  1 root root 465944 Mar 20 11:47 libmyisam.a 
-rw-r--r--  1 root root  41842 Mar 20 11:47 libmyisammrg.a 
-rw-r--r--  1 root root 734880 Mar 20 11:47 libmysqlclient.a 
-rwxr-xr-x  1 root root    873 Mar 20 11:47 libmysqlclient.la 
lrwxrwxrwx  1 root root     24 Mar 20 11:47 libmysqlclient.so -> 
libmysqlclient.so.15.0.0 
lrwxrwxrwx  1 root root     24 Mar 20 11:47 libmysqlclient.so.15 -> 
libmysqlclient.so.15.0.0 
-rwxr-xr-x  1 root root 495952 Mar 20 11:47 libmysqlclient.so.15.0.0 
-rw-r--r--  1 root root 349328 Mar 20 11:47 libmystrings.a 
-rw-r--r--  1 root root 450234 Mar 20 11:47 libmysys.a 
-rw-r--r--  1 root root  11962 Mar 20 11:47 libvio.a


[root at ws-bioinf-01 MySQL]# R CMD build RMySQL 
* checking for file 'RMySQL/DESCRIPTION' ... OK 
* preparing 'RMySQL': 
* checking DESCRIPTION meta-information ... OK 
* cleaning src 
* removing junk files 
* checking for LF line-endings in source files 
* checking for empty or unneeded directories 
* building 'RMySQL_0.5-11.tar.gz'

[root at ws-bioinf-01 MySQL]# R CMD INSTALL RMySQL_0.5-11.tar.gz 
* Installing *source* package 'RMySQL' ... 
creating cache ./config.cache 
checking how to run the C preprocessor... cc -E 
checking for compress in -lz... yes 
checking for getopt_long in -lc... yes 
checking for mysql_init in -lmysqlclient... no 
checking for mysql.h... no 
updating cache ./config.cache 
creating ./config.status 
creating src/Makevars 
** libs 
gcc -I/usr/lib64/R/include -I/usr/lib64/R/include -I/usr/local/include/mysql 
-I/usr/local/include    -fpic  -O2 -g -std=gnu99 -c RS-DBI.c -o RS-DBI.o 
gcc -I/usr/lib64/R/include -I/usr/lib64/R/include -I/usr/local/include/mysql 
-I/usr/local/include    -fpic  -O2 -g -std=gnu99 -c RS-MySQL.c -o RS-MySQL.o 
gcc -shared -Wl,-O1 -o RMySQL.so RS-DBI.o RS-MySQL.o -L/usr/local/lib/mysql 
-lmysqlclient -lz  -L/usr/lib64/R/lib -lR 
** R 
** inst 
** save image 
Loading required package: DBI 
[1] "dbObjectId" 
[1] "format" 
[1] "show" 
[1] "print" 
[1] "MySQLObject" 
[1] "MySQLDriver" 
[1] "dbUnloadDriver" 
[1] "dbGetInfo" 
[1] "dbListConnections" 
[1] "summary" 
[1] "MySQLConnection" 
[1] "dbConnect" 
[1] "dbConnect" 
[1] "dbConnect" 
[1] "dbDisconnect" 
[1] "dbSendQuery" 
[1] "dbGetQuery" 
[1] "dbGetException" 
[1] "dbGetInfo" 
[1] "dbListResults" 
[1] "summary" 
[1] "dbListTables" 
[1] "dbReadTable" 
[1] "dbWriteTable" 
[1] "dbWriteTable" 
[1] "dbExistsTable" 
[1] "dbRemoveTable" 
[1] "dbListFields" 
[1] "dbCommit" 
[1] "dbRollback" 
[1] "dbCallProc" 
[1] "MySQLResult" 
[1] "dbClearResult" 
[1] "fetch" 
[1] "fetch" 
[1] "dbGetInfo" 
[1] "dbGetStatement" 
[1] "dbListFields" 
[1] "dbColumnInfo" 
[1] "dbGetRowsAffected" 
[1] "dbGetRowCount" 
[1] "dbHasCompleted" 
[1] "dbGetException" 
[1] "summary" 
[1] "dbDataType" 
[1] "make.db.names" 
[1] "SQLKeywords" 
[1] "isSQLKeyword" 
[1] "dbApply" 
[1] "dbApply" 
** help
  >>> Building/Updating help pages for package 'RMySQL'
      Formats: text html latex example
   MySQL                             text    html    latex   example
   MySQLConnection-class             text    html    latex   example
   MySQLDriver-class                 text    html    latex   example
   MySQLObject-class                 text    html    latex   example
   MySQLResult-class                 text    html    latex   example
   RMySQL-package                    text    html    latex   example
   S4R                               text    html    latex   example
   dbApply-methods                   text    html    latex   example
   dbApply                           text    html    latex   example
   dbBuildTableDefinition            text    html    latex
   dbCallProc-methods                text    html    latex
   dbCommit-methods                  text    html    latex   example
   dbConnect-methods                 text    html    latex   example
   dbDataType-methods                text    html    latex   example
   dbDriver-methods                  text    html    latex   example
   dbGetInfo-methods                 text    html    latex   example
   dbListTables-methods              text    html    latex   example
   dbObjectId-class                  text    html    latex   example
   dbReadTable-methods               text    html    latex   example
   dbSendQuery-methods               text    html    latex   example
   dbSetDataMappings-methods         text    html    latex   example
   fetch-methods                     text    html    latex   example
   isIdCurrent                       text    html    latex   example
   make.db.names-methods             text    html    latex   example
   mysqlDBApply                      text    html    latex   example
   mysqlSupport                      text    html    latex
   safe.write                        text    html    latex   example
   summary-methods                   text    html    latex 
** building package indices ... 
* DONE (RMySQL)

Thanks very much, 
Dick 
******************************************************************************* 
Richard P. Beyer, Ph.D. University of Washington 
Tel.:(206) 616 7378 Env. & Occ. Health Sci. , Box 354695 
Fax: (206) 685 4696 4225 Roosevelt Way NE, # 100
    Seattle, WA 98105-6099 
http://depts.washington.edu/ceeh/ServiceCores/FC5/FC5.html 
http://staff.washington.edu/~dbeyer


From duncan at wald.ucdavis.edu  Tue Mar 20 22:55:34 2007
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Tue, 20 Mar 2007 14:55:34 -0700
Subject: [R] XML - can create but can't save
In-Reply-To: <20070320120831.M29614@centroin.com.br>
References: <20070320113447.M2698@centroin.com.br>
	<20070320120831.M29614@centroin.com.br>
Message-ID: <46005856.7040005@wald.ucdavis.edu>


Just for the record,  the method has been added for that particular
type of tree. So the original

  saveXML(tt, file = "test.xml")

will work.

Thanks for pointing it out.

 D.



Alberto Monteiro wrote:
> I wrote:
>> library(XML)
>> tt <- xmlHashTree()
>> head <- addNode(xmlNode("head"), character(), tt)
>> test <- addNode(xmlNode("test", attrs=c(pi="4")), head, tt)
>> tt # ok
>> saveXML(tt, file="test.xml") # error
>>
> I found a way to circumvent this error, by replacing the saveXML
> line with:
> 
> sink("test.xml")
> print(tt)
> sink()
> 
> Alberto Monteiro
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Mike.Prager at noaa.gov  Tue Mar 20 23:04:07 2007
From: Mike.Prager at noaa.gov (Michael H. Prager)
Date: Tue, 20 Mar 2007 18:04:07 -0400
Subject: [R] Ticks on barplots
Message-ID: <46005A57.2000709@noaa.gov>

Dear Gurus,

Using R 2.4.1 on Windows XP

I am generating stacked barplots of age-composition of fish populations 
(Y) over time (X).  As there are many years, not every bars is labeled.  
When looking at the plot, it becomes difficult to associate labels with 
their bars.

We have improved this a bit by using axis() to add a tickmark below each 
bar.  Can anyone suggest a way to draw ticks ONLY at bars where a tick 
label is drawn?  Or to make such ticks longer than those where there is 
no label?

This is going into a function, so I'm hoping for a method that doesn't 
require looking at the plot first.

I have attached a PDF.

# sample code (simplified) #
mp <- barplot(t(N.age), xlab = "Year", axisnames = FALSE)
axis(side = 1, at = mp, labels = rownames(N.age), tcl = -0.75)

Thanks!

Mike Prager
NOAA, Beaufort, NC


From peter.mcmahan at gmail.com  Tue Mar 20 23:16:11 2007
From: peter.mcmahan at gmail.com (Peter McMahan)
Date: Tue, 20 Mar 2007 17:16:11 -0500
Subject: [R] lattice key (legend) with both points and lines
Message-ID: <1DC190C6-0B4B-4204-8563-06EED1370526@gmail.com>

Hello,
I'm running into a frustrating problem with the legend on a lattice  
plot I'm working with. The plot is a stripplot with a panel.linejoin 
() line running through the mean of each of the categories. Thus  
there are both points and lines in the plot.
In trying to make a key for the plot, I can't figure out how to make  
a legend for both the points and the lines. What I'd like is  
something like:

prices    *
means   -----

in which there are two "rows" in the legend, one with the point and  
its label and one with the line and its label. By supplying this type  
of argument to stripplot:

key=list(text=list(lab="prices"),
          points=list(...),
          text=list(lab="means"),
          lines=list(...))

I can get a legend that looks like:

prices    *     means   -----

But this looks awkward with my plot

Is there any way to have the plot elements and their labels be in the  
same column?

Thanks,
Peter


From Soren.Hojsgaard at agrsci.dk  Tue Mar 20 23:25:22 2007
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Tue, 20 Mar 2007 23:25:22 +0100
Subject: [R] Over-writing functions from other packages? What is a good
	strategy??
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC038786FB@DJFPOST01.djf.agrsci.dk>

I am writing a package which uses the Rgraphviz package which in turn uses the graph package, but my question does not (I believe) pertain specifically to the these packages so therefore I dare to post the question here:
 
I my package I have a function "edges" which works on some graphs I have defined. However, there is also a function "edges" (or rather a generic method) in the graph package (seemingly written in S4). When I load my package the Rgraphviz package is automatically loaded, but this means that the edges method of the the graph package "overrides" the edge function in my package. 
 
Is there a way of avoiding this? If there is, I guess that it is a dangerous path to take? But if so, what else is a good strategy to take? 
 
Regards
S?ren


From sundar.dorai-raj at pdf.com  Tue Mar 20 23:53:52 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 20 Mar 2007 15:53:52 -0700
Subject: [R] lattice key (legend) with both points and lines
In-Reply-To: <1DC190C6-0B4B-4204-8563-06EED1370526@gmail.com>
References: <1DC190C6-0B4B-4204-8563-06EED1370526@gmail.com>
Message-ID: <46006600.9080405@pdf.com>



Peter McMahan said the following on 3/20/2007 3:16 PM:
> Hello,
> I'm running into a frustrating problem with the legend on a lattice  
> plot I'm working with. The plot is a stripplot with a panel.linejoin 
> () line running through the mean of each of the categories. Thus  
> there are both points and lines in the plot.
> In trying to make a key for the plot, I can't figure out how to make  
> a legend for both the points and the lines. What I'd like is  
> something like:
> 
> prices    *
> means   -----
> 
> in which there are two "rows" in the legend, one with the point and  
> its label and one with the line and its label. By supplying this type  
> of argument to stripplot:
> 
> key=list(text=list(lab="prices"),
>           points=list(...),
>           text=list(lab="means"),
>           lines=list(...))
> 
> I can get a legend that looks like:
> 
> prices    *     means   -----
> 
> But this looks awkward with my plot
> 
> Is there any way to have the plot elements and their labels be in the  
> same column?
> 
> Thanks,
> Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


You can try:

library(lattice)
xyplot(1:10 ~ 1:10,
        key = list(text = list(c("prices", "means")),
                  lines = list(
                    pch = c("*", ""), type = c("p", "l"),
                    cex = 2, col = "red", lwd = 3)))

HTH,

--sundar


From albmont at centroin.com.br  Wed Mar 21 00:05:48 2007
From: albmont at centroin.com.br (Alberto Vieira Ferreira Monteiro)
Date: Tue, 20 Mar 2007 23:05:48 +0000
Subject: [R] XML - can create but can't save
In-Reply-To: <46005856.7040005@wald.ucdavis.edu>
References: <20070320113447.M2698@centroin.com.br>
	<20070320120831.M29614@centroin.com.br>
	<46005856.7040005@wald.ucdavis.edu>
Message-ID: <200703202305.49011.albmont@centroin.com.br>

Duncan Temple Lang wrote:
>
> Just for the record,  the method has been added for that particular
> type of tree. So the original
>
>   saveXML(tt, file = "test.xml")
>
> will work.
>
Ah, the wonders of free software... I didn't have to wait a single day
to have the bug fixed :-)

Alberto Monteiro


From dbeyer at u.washington.edu  Wed Mar 21 00:10:25 2007
From: dbeyer at u.washington.edu (Dick Beyer)
Date: Tue, 20 Mar 2007 16:10:25 -0700 (PDT)
Subject: [R] RMySQL load error
In-Reply-To: <Pine.LNX.4.43.0703201451240.8002@hymn06.u.washington.edu>
Message-ID: <Pine.LNX.4.43.0703201610250.11047@hymn10.u.washington.edu>

I got some great help from Phil Spector on how to solve this:

export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib/mysql

and/or add this to the /usr/bin/R script.

Life is good,
Dick
*******************************************************************************
Richard P. Beyer, Ph.D.	University of Washington
Tel.:(206) 616 7378	Env. & Occ. Health Sci. , Box 354695
Fax: (206) 685 4696	4225 Roosevelt Way NE, # 100
			Seattle, WA 98105-6099
http://depts.washington.edu/ceeh/ServiceCores/FC5/FC5.html
http://staff.washington.edu/~dbeyer
*******************************************************************************

On Tue, 20 Mar 2007, Dick Beyer wrote:

> I'm having trouble getting RMySQL to load.  I was able to build it and
> install it (RMySQL_0.5-11.tar.gz).  I was also able to build and install
> mysql-5.0.37.
>
> I've read many postings about this but have not found a mention of my
> particular problem (some closely related).  I get the "unable to load shared
> library, no such file" error, but all the files do exist, RMySQL.so and
> libmysqlclient.so.15.
>
> I set these environment variables:
>
> PKG_LIBS=-L/usr/local/lib/mysql -lmysqlclient
> PKG_CPPFLAGS=-I/usr/local/include/mysql
>
> If anyone had any suggestions, ideas, or pointers, I would be eternally
> grateful for the help.  Here is the relevant output from my R session and
> from building the RMySQL package (I am using gcc version 3.4.6 and RedHat
> AS4 2.6.9-42.0.10.ELsmp):
>
>> library(RMySQL)
> Loading required package: DBI Error in dyn.load(x, as.logical(local),
> as.logical(now)) :
>        unable to load shared library
> '/usr/lib64/R/library/RMySQL/libs/RMySQL.so':
>  libmysqlclient.so.15: cannot open shared object file: No such file or
> directory Error in library(RMySQL) : .First.lib failed for 'RMySQL'
>
>> sessionInfo()
> R version 2.4.1 (2006-12-18) x86_64-redhat-linux-gnu
>
> locale:
> LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C
>
> attached base packages: [1] "stats"     "graphics"  "grDevices" "utils"
> "datasets"  "methods" [7] "base"
>
> other attached packages:
>     DBI "0.1-12"
>
> [root at ws-bioinf-01 MySQL]# ls -l /usr/lib64/R/library/RMySQL/libs/ total 124
> -rwxr-xr-x  1 root root 122497 Mar 20 11:49 RMySQL.so
>
> [root at ws-bioinf-01 MySQL]# ll /usr/local/lib/mysql/ total 2632 -rw-r--r--  1
> root root  27208 Mar 20 11:47 libdbug.a -rw-r--r--  1 root root  71144 Mar
> 20 11:47 libheap.a -rw-r--r--  1 root root 465944 Mar 20 11:47 libmyisam.a
> -rw-r--r--  1 root root  41842 Mar 20 11:47 libmyisammrg.a -rw-r--r--  1
> root root 734880 Mar 20 11:47 libmysqlclient.a -rwxr-xr-x  1 root root
> 873 Mar 20 11:47 libmysqlclient.la lrwxrwxrwx  1 root root     24 Mar 20
> 11:47 libmysqlclient.so -> libmysqlclient.so.15.0.0 lrwxrwxrwx  1 root root
> 24 Mar 20 11:47 libmysqlclient.so.15 -> libmysqlclient.so.15.0.0 -rwxr-xr-x
> 1 root root 495952 Mar 20 11:47 libmysqlclient.so.15.0.0 -rw-r--r--  1 root
> root 349328 Mar 20 11:47 libmystrings.a -rw-r--r--  1 root root 450234 Mar
> 20 11:47 libmysys.a -rw-r--r--  1 root root  11962 Mar 20 11:47 libvio.a
>
>
> [root at ws-bioinf-01 MySQL]# R CMD build RMySQL * checking for file
> 'RMySQL/DESCRIPTION' ... OK * preparing 'RMySQL': * checking DESCRIPTION
> meta-information ... OK * cleaning src * removing junk files * checking for
> LF line-endings in source files * checking for empty or unneeded directories
> * building 'RMySQL_0.5-11.tar.gz'
>
> [root at ws-bioinf-01 MySQL]# R CMD INSTALL RMySQL_0.5-11.tar.gz * Installing
> *source* package 'RMySQL' ... creating cache ./config.cache checking how to
> run the C preprocessor... cc -E checking for compress in -lz... yes checking
> for getopt_long in -lc... yes checking for mysql_init in -lmysqlclient... no
> checking for mysql.h... no updating cache ./config.cache creating
> ./config.status creating src/Makevars ** libs gcc -I/usr/lib64/R/include
> -I/usr/lib64/R/include -I/usr/local/include/mysql -I/usr/local/include
> -fpic -O2 -g -std=gnu99 -c RS-DBI.c -o RS-DBI.o gcc -I/usr/lib64/R/include
> -I/usr/lib64/R/include -I/usr/local/include/mysql -I/usr/local/include
> -fpic -O2 -g -std=gnu99 -c RS-MySQL.c -o RS-MySQL.o gcc -shared -Wl,-O1 -o
> RMySQL.so RS-DBI.o RS-MySQL.o -L/usr/local/lib/mysql -lmysqlclient -lz
> -L/usr/lib64/R/lib -lR ** R ** inst ** save image Loading required package:
> DBI [1] "dbObjectId" [1] "format" [1] "show" [1] "print" [1] "MySQLObject"
> [1] "MySQLDriver" [1] "dbUnloadDriver" [1] "dbGetInfo" [1]
> "dbListConnections" [1] "summary" [1] "MySQLConnection" [1] "dbConnect" [1]
> "dbConnect" [1] "dbConnect" [1] "dbDisconnect" [1] "dbSendQuery" [1]
> "dbGetQuery" [1] "dbGetException" [1] "dbGetInfo" [1] "dbListResults" [1]
> "summary" [1] "dbListTables" [1] "dbReadTable" [1] "dbWriteTable" [1]
> "dbWriteTable" [1] "dbExistsTable" [1] "dbRemoveTable" [1] "dbListFields"
> [1] "dbCommit" [1] "dbRollback" [1] "dbCallProc" [1] "MySQLResult" [1]
> "dbClearResult" [1] "fetch" [1] "fetch" [1] "dbGetInfo" [1] "dbGetStatement"
> [1] "dbListFields" [1] "dbColumnInfo" [1] "dbGetRowsAffected" [1]
> "dbGetRowCount" [1] "dbHasCompleted" [1] "dbGetException" [1] "summary" [1]
> "dbDataType" [1] "make.db.names" [1] "SQLKeywords" [1] "isSQLKeyword" [1]
> "dbApply" [1] "dbApply" ** help
>>>> Building/Updating help pages for package 'RMySQL'
>     Formats: text html latex example
>  MySQL                             text    html    latex   example
>  MySQLConnection-class             text    html    latex   example
>  MySQLDriver-class                 text    html    latex   example
>  MySQLObject-class                 text    html    latex   example
>  MySQLResult-class                 text    html    latex   example
>  RMySQL-package                    text    html    latex   example
>  S4R                               text    html    latex   example
>  dbApply-methods                   text    html    latex   example
>  dbApply                           text    html    latex   example
>  dbBuildTableDefinition            text    html    latex
>  dbCallProc-methods                text    html    latex
>  dbCommit-methods                  text    html    latex   example
>  dbConnect-methods                 text    html    latex   example
>  dbDataType-methods                text    html    latex   example
>  dbDriver-methods                  text    html    latex   example
>  dbGetInfo-methods                 text    html    latex   example
>  dbListTables-methods              text    html    latex   example
>  dbObjectId-class                  text    html    latex   example
>  dbReadTable-methods               text    html    latex   example
>  dbSendQuery-methods               text    html    latex   example
>  dbSetDataMappings-methods         text    html    latex   example
>  fetch-methods                     text    html    latex   example
>  isIdCurrent                       text    html    latex   example
>  make.db.names-methods             text    html    latex   example
>  mysqlDBApply                      text    html    latex   example
>  mysqlSupport                      text    html    latex
>  safe.write                        text    html    latex   example
>  summary-methods                   text    html    latex ** building package
> indices ... * DONE (RMySQL)
>
> Thanks very much, Dick
> *******************************************************************************
> Richard P. Beyer, Ph.D. University of Washington Tel.:(206) 616 7378 Env. &
> Occ. Health Sci. , Box 354695 Fax: (206) 685 4696 4225 Roosevelt Way NE, #
> 100
>   Seattle, WA 98105-6099
> http://depts.washington.edu/ceeh/ServiceCores/FC5/FC5.html
> http://staff.washington.edu/~dbeyer
> *******************************************************************************
>
>
>


From Manuel.A.Morales at williams.edu  Wed Mar 21 01:12:37 2007
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Tue, 20 Mar 2007 20:12:37 -0400
Subject: [R] Problem adjusting x-labels with bargraphCI
In-Reply-To: <EF8F4571A0077848B837BD776D91C5F94B350C@COMEXCHANGE.hscnet.hsc.usf.edu>
References: <EF8F4571A0077848B837BD776D91C5F94B350C@COMEXCHANGE.hscnet.hsc.usf.edu>
Message-ID: <1174435958.2854.10.camel@solidago.localdomain>

On Tue, 2007-03-20 at 13:15 -0400, Vera, Pedro L. wrote:
> Hello:
>  
> I'm having quite a bit of difficulty adjusting the x-labels using bargraphCI. I've tried using text and srt=45 to rotate the labels or mtext for 2 lines to break up the labels. However, using either method, I cannot line up the labels with the midpoints of the bars (they line up with some sort of tick mark that is off the midpoint of the bars). Any suggestions would be greatly appreciated.
>  
> Regards
>  
> Pedro L. Vera, Ph.D.
> University of South Florida, Dept of Surgery

Assuming that you are asking about bargraph.CI from the sciplot package,
and that you are trying to get x-axis tick-labels that are perpendicular
to the x-axis, specifying las=2 in the call to bargraph.CI should work.

If you want more control, you can assign the output of bargraph.CI to an
object (a list that contains the x-values of the bars, summary stats,
and CIs). This will allow you to position labels at the x-values of the
plotted bars. For example:

test <- bargraph.CI(x.factor = dose, response = len, data = ToothGrowth, xaxt="n")
axis(side=1,at=test$xvals,labels=c("bar1","bar2","bar3"),las=2)

-- 
Manuel A. Morales
http://mutualism.williams.edu

From kubovy at virginia.edu  Wed Mar 21 01:52:14 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Tue, 20 Mar 2007 20:52:14 -0400
Subject: [R] package:AlgDesign and .Random.seed
Message-ID: <DC32551B-C910-410F-A0BF-E538A504C700@virginia.edu>

Dear r-helpers,

Could you please help me solve the following problem: When I run

require(AlgDesign)
trt <- LETTERS[1:5]
blk <- 10
trtblk <- 3
BIB <- optBlock(~., withinData = trt, blocksizes = rep(trtblk, blk))

In response to the last command, R complains:

Error in optBlock(~., withinData = trt, blocksizes = rep(trtblk, blk)) :
	object ".Random.seed" not found

The documentation of optBlock() in AlgDesign doesn't say that I  
needed to set .Random.seed. I thought it was initiated automatically  
at the beginning of a session. What am I missing?

 > sessionInfo()
R version 2.4.1 (2006-12-18)
i386-apple-darwin8.8.1

locale:
C

attached base packages:
[1] "grid"      "datasets"  "stats"     "graphics"  "grDevices"  
"utils"     "methods"   "base"

other attached packages:
    AlgDesign       xtable latticeExtra      lattice      
gridBase         MASS          JGR       iplots
      "1.0-7"      "1.4-3"      "0.1-4"    "0.14-16"      "0.4-3"      
"7.2-32"     "1.4-15"      "1.0-5"
       JavaGD        rJava
      "0.3-6"     "0.4-14"





_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From asn151 at yahoo.com  Wed Mar 21 01:55:42 2007
From: asn151 at yahoo.com (Jonathan Morse)
Date: Tue, 20 Mar 2007 17:55:42 -0700 (PDT)
Subject: [R] Trimming a Data Set
In-Reply-To: <45F8446F.2040201@karlin.mff.cuni.cz>
Message-ID: <814790.61126.qm@web62408.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070320/89a6873a/attachment.pl 

From marc_schwartz at comcast.net  Wed Mar 21 01:58:21 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 20 Mar 2007 19:58:21 -0500
Subject: [R] Select the last two rows by id group
In-Reply-To: <D028EEB4CA113D4EAFDD485CCC9982777313FC@UTKFSVS4.utk.tennessee.edu>
References: <ba8c09910703200733g37a25e8cve55952fc640c1ec1@mail.gmail.com>
	<1174402712.4920.20.camel@localhost.localdomain>
	<D028EEB4CA113D4EAFDD485CCC9982777313FC@UTKFSVS4.utk.tennessee.edu>
Message-ID: <1174438701.5415.42.camel@localhost.localdomain>

On Tue, 2007-03-20 at 11:53 -0400, Muenchen, Robert A (Bob) wrote:
> Very nice! This is almost duplicates the SAS first.var and last.var
> ability to choose the first and last observations by group(s).
> Substituting the head function in where Marc has the tail function below
> will adapt it to the first n. It is more flexible than the SAS approach
> because it can do the first/last n rather than just the single first or
> last.
> 
> Let's say we want to choose the last observation in a county, and
> counties have duplicate names in different states. You could sort by
> state, then county, then use only county where Marc uses score$id in his
> last example below, and it would get the last record for *every* county
> regardless of duplicates. Does this sound correct? 
> 
> That's a handy bit of code!
> 
> Cheers,
> Bob

Bob,

You can test it using data here:

DF <- read.csv("http://www.nws.noaa.gov/nwr/SameCode.txt", 
               header = FALSE)

colnames(DF) <- c("Code", "County", "State")

> str(DF)
'data.frame':   3288 obs. of  3 variables:
 $ Code  : int  1001 1003 1005 1007 1009 1011 1013 1015 1017 1019 ...
 $ County: Factor w/ 1996 levels "Abbeville","Acadia",..: 97 105 116 169 186 249 259 272 326 348 ...
 $ State : Factor w/ 60 levels "AK","AL","AR",..: 2 2 2 2 2 2 2 2 2 2 ...


The data is already sorted by State and then County.


> system.time(DF.tail <- do.call("rbind", lapply(split(DF, DF$County), tail,  1)))
[1] 6.851 0.085 7.085 0.000 0.000


> str(DF.tail)
'data.frame':   1996 obs. of  3 variables:
 $ Code  : int  45001 22001 16001 40001 55001 50001 72001 72003 72005 72007 ...
 $ County: Factor w/ 1996 levels "Abbeville","Acadia",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ State : Factor w/ 60 levels "AK","AL","AR",..: 48 22 17 42 58 56 45 45 45 45 ...


# How many unique county names in the source dataset?

> length(unique(DF$County))
[1] 1996


# Are they all the same unique counties?

> all(DF.tail$County == sort(unique(DF$County)))
[1] TRUE


It is curious to see just how many duplicates there are. For example:

> tail(sort(table(DF$County)))

   Madison    Jackson    Lincoln   Franklin  Jefferson Washington 
        20         24         24         25         26         31 


> subset(DF, County == "Washington")
      Code     County State
65    1129 Washington    AL
181   5143 Washington    AR
304   8121 Washington    CO
385  12133 Washington    FL
535  13303 Washington    GA
593  16087 Washington    ID
688  17189 Washington    IL
783  18175 Washington    IN
879  19183 Washington    IA
987  20201 Washington    KS
1106 21229 Washington    KY
1167 22117 Washington    LA
1189 23029 Washington    ME
1211 24043 Washington    MD
1393 27163 Washington    MN
1474 28151 Washington    MS
1590 29221 Washington    MO
1740 31177 Washington    NE
1883 36115 Washington    NY
1981 37187 Washington    NC
2124 39167 Washington    OH
2202 40147 Washington    OK
2239 41067 Washington    OR
2304 42125 Washington    PA
2313 44009 Washington    RI
2515 47179 Washington    TN
2759 48477 Washington    TX
2800 49053 Washington    UT
2814 50023 Washington    VT
2904 51191 Washington    VA
3108 55131 Washington    WI


# The last state with Washington County (my neighbors, the
"Cheeseheads") was in the result set

> subset(DF.tail, County == "Washington")
            Code     County State
Washington 55131 Washington    WI



> subset(DF, County == "Allen")
      Code County State
697  18003  Allen    IN
887  20001  Allen    KS
993  21003  Allen    KY
1113 22003  Allen    LA
2042 39003  Allen    OH


# The last state with Allen County (OH) was in the result set

> subset(DF.tail, County == "Allen")
       Code County State
Allen 39003  Allen    OH


Just noticed a Big Ten theme there...Go Gophers!   ;-)


So, it would seem that your hypothesis is correct, at least in this
limited testing.  I would want to validate it more rigorously of course.

HTH,

Marc Schwartz


From asn151 at yahoo.com  Wed Mar 21 02:17:41 2007
From: asn151 at yahoo.com (Jonathan Morse)
Date: Tue, 20 Mar 2007 18:17:41 -0700 (PDT)
Subject: [R] Trimming a Data Set
In-Reply-To: <45F8446F.2040201@karlin.mff.cuni.cz>
Message-ID: <33498.99440.qm@web62412.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070320/73486f65/attachment.pl 

From jholtman at gmail.com  Wed Mar 21 02:30:04 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 20 Mar 2007 20:30:04 -0500
Subject: [R] Trimming a Data Set
In-Reply-To: <814790.61126.qm@web62408.mail.re1.yahoo.com>
References: <45F8446F.2040201@karlin.mff.cuni.cz>
	<814790.61126.qm@web62408.mail.re1.yahoo.com>
Message-ID: <644e1f320703201830g48f5674cr355a9c311b464cf@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070320/db4aa091/attachment.pl 

From mnair at iusb.edu  Wed Mar 21 02:35:05 2007
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Tue, 20 Mar 2007 21:35:05 -0400
Subject: [R] multcomp
References: <A32055BDEA88C34BB3DBBCD229380778E656A5@iu-mssg-mbx109.ads.iu.edu>
	<3948d9e50703182107g622a4885k731ba8befe8b6241@mail.gmail.com>
Message-ID: <A32055BDEA88C34BB3DBBCD22938077805105F@iu-mssg-mbx109.ads.iu.edu>

Thanks for the hint. 
Here is what I had done earlier according to the old package
 
 mult.comp<-simint(a~b, data=z, conf.level=0.99, type=c("Tukey") )
 isoforms<-as.vector(rownames(mult.comp$estimate))
 estimate<-as.vector(mult.comp$estimate)
 lower<-as.vector(mult.comp$conf.int[,1])
 upper<-as.vector(mult.comp$conf.int[,2])
 p.val.raw<-as.vector(mult.comp$p.value.raw)
 
Here is how I modified the above so that I can use the new package. Can someone comment if what I have done is correct or not?
amod<-aov(a~b, data=z)
mult.comp<-glht(amod, linfct=mcp(b="Tukey"))
estimate<-confint(mult.comp)[1,1]
lower<-confint(mult.comp)[1,2]
upper<-confint(mult.comp)[1,3]
summary(mult.comp) does give the adjusted pvalues.  I tried all the subscripts to extract it to store in a variable but could not, can any one help me here. Also,  how do I specify the conf.level to be equal to 0.99. The default is 0.95
 
Thanks for your help,
Cheers../Murli

 
________________________________

From: talepanda [mailto:talepanda at gmail.com]
Sent: Sun 3/18/2007 11:07 PM
To: Nair, Murlidharan T
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] multcomp



?glht says
>with 'print', 'summary',  'confint', 'coef' and 'vcov'  methods
>being available.

try:

example(glht)
summary(glht(amod, linfct = mcp(tension = "Tukey")))
confint(glht(amod, linfct = mcp(tension = "Tukey")))

On 3/19/07, Nair, Murlidharan T <mnair at iusb.edu> wrote:
> I used the multcomp package sometime back for doing multiple
> comparisons. I see that it has been updated and the methods like simint
> are no longer supported. When I run the program it prompts to me to use
> glht. How do I get the lower and upper conf int and the pValues using
> glht? Does anyone have an example?
>
> Thanks ../Murli
>
>
>
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Wed Mar 21 02:36:31 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 20 Mar 2007 21:36:31 -0400
Subject: [R] Problems about Derivaties
In-Reply-To: <JF7UXU$288630A93C59D0253D637C34A4460A32@libero.it>
References: <JF7UXU$288630A93C59D0253D637C34A4460A32@libero.it>
Message-ID: <971536df0703201836i2a1213m17a4e6ba0b762574@mail.gmail.com>

On 3/20/07, enrico.foscolo <enrico.foscolo at libero.it> wrote:
> Dear participants to the list,
>
> this is my problem: I want to obtain an expression that represents the second
> derivative of one function.
> With "deriv3" (package "stats") it is possible to evaluate the second
> derivative, but I do not know how I can get the (analytical) expression of this
> derivative.
>
> For example: Suppose that I have a function of this form:
> f(x,y)=x^3+y^3+(x^2)*(y^2). With "deriv3" I can evaluate the first derivative
> and the hessian matrix, as follows:
>
> > d<-deriv3(~x^3+y^3+(x^2)*(y^2),c("x","y"))
> > d[[1]]
> {
> .expr4 <- x^2
> .expr5 <- y^2
> .expr9 <- 2 * x
> .expr15 <- 2 * y
> .value <- x^3 + y^3 + .expr4 * .expr5
> .grad <- array(0, c(length(.value), 2), list(NULL, c("x",
> "y")))
> .hessian <- array(0, c(length(.value), 2, 2), list(NULL,
> c("x", "y"), c("x", "y")))
> .grad[, "x"] <- 3 * .expr4 + .expr9 * .expr5
> .hessian[, "x", "x"] <- 3 * .expr9 + 2 * .expr5
> .hessian[, "x", "y"] <- .hessian[, "y", "x"] <- .expr9 *
> .expr15
> .grad[, "y"] <- 3 * .expr5 + .expr4 * .expr15
> .hessian[, "y", "y"] <- 3 * .expr15 + .expr4 * 2
> attr(.value, "gradient") <- .grad
> attr(.value, "hessian") <- .hessian
> .value
> }
> > d[[1]][11]
> .hessian[, "x", "y"] <- .hessian[, "y", "x"] <- .expr9 * .expr15()
> > typeof(d[[1]][11])
> [1] "language"
> > is.call(d[[1]][11])
> [1] TRUE
>
> Is it possible to extract the formula of the second derivative of this function
> from the output of deriv3?
>
> I would like to use this expression as new function, which means that in this
> case, I would get the string ".hessian[, "x", "y"] <- .hessian[, "y", "x"] <-
> .expr9 * .expr15".
>

I am not really sure from your description exactly what the real problem
is but maybe this will help:

Hessian <- matrix(list(D(D(e, "x"), "x"), D(D(e, "x"), "y"), D(D(e,
"y"), "x"), D(D(e, "y"), "y")), 2)
Hessian[[1,1]]
Hessian[[1,2]]
Hessian[[2,1]]
Hessian[[2,2]]

# or

e <- expression(x^3+y^3+(x^2)*(y^2))
vars <- c("x", "y")
g <- expand.grid(vars, vars)
f <- function(x) D(D(e, x[1]), x[2])
Hessian <- matrix(apply(g, 1, f), 2)
Hessian[[1,1]]
Hessian[[1,2]]
Hessian[[2,1]]
Hessian[[2,2]]

# another possibility is the Ryacas package:

library(Ryacas)
yacas("HessianMatrix(x^3+y^3+(x^2)*(y^2), {x,y})")


From asn151 at yahoo.com  Wed Mar 21 02:56:11 2007
From: asn151 at yahoo.com (Jonathan Morse)
Date: Tue, 20 Mar 2007 18:56:11 -0700 (PDT)
Subject: [R] Trimming a Data Set
In-Reply-To: <644e1f320703201830g48f5674cr355a9c311b464cf@mail.gmail.com>
Message-ID: <904390.66616.qm@web62406.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070320/27345f2c/attachment.pl 

From ggrothendieck at gmail.com  Wed Mar 21 02:59:37 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 20 Mar 2007 21:59:37 -0400
Subject: [R] Over-writing functions from other packages? What is a good
	strategy??
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC038786FB@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC038786FB@DJFPOST01.djf.agrsci.dk>
Message-ID: <971536df0703201859p4e730079v2671c0f703dce525@mail.gmail.com>

Could you call yours Edges?

On 3/20/07, S?ren H?jsgaard <Soren.Hojsgaard at agrsci.dk> wrote:
> I am writing a package which uses the Rgraphviz package which in turn uses the graph package, but my question does not (I believe) pertain specifically to the these packages so therefore I dare to post the question here:
>
> I my package I have a function "edges" which works on some graphs I have defined. However, there is also a function "edges" (or rather a generic method) in the graph package (seemingly written in S4). When I load my package the Rgraphviz package is automatically loaded, but this means that the edges method of the the graph package "overrides" the edge function in my package.
>
> Is there a way of avoiding this? If there is, I guess that it is a dangerous path to take? But if so, what else is a good strategy to take?
>
> Regards
> S?ren
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From pvera at health.usf.edu  Wed Mar 21 03:35:59 2007
From: pvera at health.usf.edu (Vera, Pedro L.)
Date: Tue, 20 Mar 2007 22:35:59 -0400
Subject: [R] Problem adjusting x-labels with bargraphCI
References: <EF8F4571A0077848B837BD776D91C5F94B350C@COMEXCHANGE.hscnet.hsc.usf.edu>
	<1174435958.2854.10.camel@solidago.localdomain>
Message-ID: <EF8F4571A0077848B837BD776D91C5F94B350F@COMEXCHANGE.hscnet.hsc.usf.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070320/7e81993e/attachment.pl 

From marc_schwartz at comcast.net  Wed Mar 21 03:40:59 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 20 Mar 2007 21:40:59 -0500
Subject: [R] Ticks on barplots
In-Reply-To: <46005A57.2000709@noaa.gov>
References: <46005A57.2000709@noaa.gov>
Message-ID: <1174444859.5415.57.camel@localhost.localdomain>

On Tue, 2007-03-20 at 18:04 -0400, Michael H. Prager wrote:
> Dear Gurus,
> 
> Using R 2.4.1 on Windows XP
> 
> I am generating stacked barplots of age-composition of fish populations 
> (Y) over time (X).  As there are many years, not every bars is labeled.  
> When looking at the plot, it becomes difficult to associate labels with 
> their bars.
> 
> We have improved this a bit by using axis() to add a tickmark below each 
> bar.  Can anyone suggest a way to draw ticks ONLY at bars where a tick 
> label is drawn?  Or to make such ticks longer than those where there is 
> no label?
> 
> This is going into a function, so I'm hoping for a method that doesn't 
> require looking at the plot first.
> 
> I have attached a PDF.
> 
> # sample code (simplified) #
> mp <- barplot(t(N.age), xlab = "Year", axisnames = FALSE)
> axis(side = 1, at = mp, labels = rownames(N.age), tcl = -0.75)
> 
> Thanks!
> 
> Mike Prager
> NOAA, Beaufort, NC

Mike,

How about something like this:

  mp <- barplot(1:50, axisnames = FALSE)

  # Create short tick marks at each bar
  axis(1, at = mp, labels = rep("", 50), tcl = -0.25)

  # Create longer tick marks every 5 years with labels
  axis(1, at = mp[seq(1, 50, 5)], 
       labels = 1900 + seq(0, 45, 5), tcl = -0.75, las = 2, 
       cex.axis = 0.75)


Just pick which labels you want to be shown (eg. every 5 years) and
synchronize the values of those with the 'at' argument in axis().

BTW, your PDF did not come through.

HTH,

Marc Schwartz


From peter.mcmahan at gmail.com  Wed Mar 21 04:00:47 2007
From: peter.mcmahan at gmail.com (Peter McMahan)
Date: Tue, 20 Mar 2007 22:00:47 -0500
Subject: [R] lattice key (legend) with both points and lines
In-Reply-To: <46006600.9080405@pdf.com>
References: <1DC190C6-0B4B-4204-8563-06EED1370526@gmail.com>
	<46006600.9080405@pdf.com>
Message-ID: <ACFE8796-5E18-4F74-91F9-E45AF78A384E@gmail.com>

On Mar 20, 2007, at 5:53 PM, Sundar Dorai-Raj wrote:
>
> Peter McMahan said the following on 3/20/2007 3:16 PM:
>> Hello,
>> I'm running into a frustrating problem with the legend on a  
>> lattice  plot I'm working with. The plot is a stripplot with a  
>> panel.linejoin () line running through the mean of each of the  
>> categories. Thus  there are both points and lines in the plot.
>> In trying to make a key for the plot, I can't figure out how to  
>> make  a legend for both the points and the lines. What I'd like  
>> is  something like:
>> prices    *
>> means   -----
>> in which there are two "rows" in the legend, one with the point  
>> and  its label and one with the line and its label. By supplying  
>> this type  of argument to stripplot:
>> key=list(text=list(lab="prices"),
>>           points=list(...),
>>           text=list(lab="means"),
>>           lines=list(...))
>> I can get a legend that looks like:
>> prices    *     means   -----
>> But this looks awkward with my plot
>> Is there any way to have the plot elements and their labels be in  
>> the  same column?
>> Thanks,
>> Peter
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
> You can try:
>
> library(lattice)
> xyplot(1:10 ~ 1:10,
>        key = list(text = list(c("prices", "means")),
>                  lines = list(
>                    pch = c("*", ""), type = c("p", "l"),
>                    cex = 2, col = "red", lwd = 3)))
>
> HTH,
>
> --sundar

That does the trick nicely, thanks! Didn't think of just telling the  
line to be a point. Here's the call that ended up working out for me:

stripplot(MedPrice~State,prices,
           ylab="Rate Premium (?/kWh)",
           panel=function(x,y,...){
             panel.grid(h=0,v=34)
             panel.stripplot(x,y,jitter=T,factor=1,pch=20,...)
             panel.linejoin(x,y,horizontal=F,col.line="black",lty=2)
           },
           key=list(
             x=.72,y=.9,
             text=list(lab=c("observations","state mean")),
             lines=list(
                 pch=c(20,NA),
                 col="black",
                 type=c("p","l"),
                 lty=1:2,
                 cex=.8),
             transparent=T,border=F,rep=F
           ),col="black"
          )


From sfalcon at fhcrc.org  Wed Mar 21 04:34:19 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 20 Mar 2007 20:34:19 -0700
Subject: [R] Over-writing functions from other packages? What is a good
	strategy??
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC038786FB@DJFPOST01.djf.agrsci.dk>
	(=?iso-8859-1?Q?S=F8ren_H=F8jsgaard's?= message of "Tue, 20 Mar 2007
	23:25:22 +0100")
References: <C83C5E3DEEE97E498B74729A33F6EAEC038786FB@DJFPOST01.djf.agrsci.dk>
Message-ID: <m2hcsfe0ck.fsf@ziti.fhcrc.org>

S?ren H?jsgaard <Soren.Hojsgaard at agrsci.dk> writes:

> I am writing a package which uses the Rgraphviz package which in
> turn uses the graph package, but my question does not (I believe)
> pertain specifically to the these packages so therefore I dare to
> post the question here:
>  
> I my package I have a function "edges" which works on some graphs I
> have defined. However, there is also a function "edges" (or rather a
> generic method) in the graph package (seemingly written in S4).

Yes, edges is a generic function defined by the graph package.  With
methods for various graph representation classes.

> I load my package the Rgraphviz package is automatically loaded, but
> this means that the edges method of the the graph package
> "overrides" the edge function in my package.
>  
> Is there a way of avoiding this? If there is, I guess that it is a
> dangerous path to take? But if so, what else is a good strategy to
> take?

I'm pretty sure this is resolved by adding a NAMESPACE file to your
package (see the Writing R Extensions Manual for details).

I made a little test package that has Rgraphviz in Depends, defines an
edges function and exports it in its NAMESPACE file.  When I load this
package, edges is the one from my test package and graph::edges is the
one from graph.

It won't solve your problem in general, but I will also look into
having Rgraphviz only import graph and not Depend on it.  This would
avoid graph::edges polluting the search path when Rgraphviz gets
loaded.  This is a reason that import might be preferred over Depends
for packages that have namespaces.

Finally, if you were depending on graph and not Rgraphviz, then you
could rename graph::edges when you import it in your NAMESPACE file:

  importFrom("graph", someOtherName=edges)

+ seth

-- 
Seth Falcon | Computational Biology | Fred Hutchinson Cancer Research Center
http://bioconductor.org


From liuwensui at gmail.com  Wed Mar 21 05:59:02 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Wed, 21 Mar 2007 00:59:02 -0400
Subject: [R] any way to append a table in SQL server
Message-ID: <1115a2b00703202159i1d43aa4t447f500744e69dba@mail.gmail.com>

Dear Lister,
Is there an interface in R with SQL server that allows me to append
records to table in the DB? Might I do that using RODBC?
Thanks a lot.

-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From berwin at maths.uwa.edu.au  Wed Mar 21 07:07:19 2007
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Wed, 21 Mar 2007 14:07:19 +0800
Subject: [R] Strange integer result on Debian/amd64
In-Reply-To: <20070320145938.GX16472@nemesis.ceu.ox.ac.uk>
References: <20070320145938.GX16472@nemesis.ceu.ox.ac.uk>
Message-ID: <20070321140719.13d94b38@berwin5>

G'day Dave,

On Tue, 20 Mar 2007 14:59:38 +0000
Dave Ewart <davee at ceu.ox.ac.uk> wrote:

> All help/suggestions/appreciated!

The calculations are done in floating point arithmetic, not integer
arithmetic. From the help page on `choose' one might already guess
so much, but reading "R-2.4.1/src/nmath/choose.c" definitely confirms
this fact.

Thus, your problem is covered by FAQ 7.31:

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f

HTH.

Cheers,

	Berwin


From ronggui.huang at gmail.com  Wed Mar 21 08:17:46 2007
From: ronggui.huang at gmail.com (ronggui)
Date: Wed, 21 Mar 2007 15:17:46 +0800
Subject: [R] question about Style in odfWeave
Message-ID: <38b9f0350703210017x4e6b5b59hdd0fb174281c5623@mail.gmail.com>

I would like to drow the topBorder and the buttomBorder, which value
should be set. I try set them to "auto", but it does not make any
differences. Thanks.

> getStyleDefs()$noBorder
$type
[1] "Table Cell"

$backgroundColor
[1] "transparent"

$padding
[1] "0.0382in"

$verticalAlign
[1] "auto"

$padding
[1] "0.0382in"

$leftBorder
[1] "none"

$rightBorder
[1] "none"

$topBorder
[1] "none"

$bottomBorder
[1] "none"


-- 
Ronggui Huang
Department of Sociology
Fudan University, Shanghai, China


From pmilin at ff.ns.ac.yu  Wed Mar 21 08:38:25 2007
From: pmilin at ff.ns.ac.yu (Petar Milin)
Date: Wed, 21 Mar 2007 08:38:25 +0100
Subject: [R] Detailed legend in mathplot ...
Message-ID: <1174462705.16512.1.camel@localhost>

Hello,
Recently, I have asked for a help with building graphs, and I got few
great advices. Now, my appetite is growing :) and I wander how to add
legend for two (or more) lines in following example:

matplot(DAT[, c(3,4)], type="b", ylim=c(0,8), xaxt="n", yaxt="n",
 + pch=c(21,22), col="black", lty=c("dashed","solid"), xlab="", ylab="")
title(ylab="% correct", xlab="Trial", cex.lab=1.5)
axis(1, at=1:4, labels=as.character(DAT$Trial), cex.axis=1.5)
axis(2, cex.axis=1.5)


Sincerely,
Petar


From ripley at stats.ox.ac.uk  Wed Mar 21 09:02:36 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Mar 2007 08:02:36 +0000 (GMT)
Subject: [R] error installing packages
In-Reply-To: <1174425532.22147.3.camel@Thux>
References: <2FC987BC0B90B24786CAF43DD3F5719C86B073@CRU105.cahe.ad.wsu.edu>
	<1174425532.22147.3.camel@Thux>
Message-ID: <Pine.LNX.4.64.0703210757260.25810@gannet.stats.ox.ac.uk>

On Tue, 20 Mar 2007, Bernardo Rangel Tura wrote:

> On Wed, 2007-03-07 at 09:33 -0800, Bricklemyer, Ross S wrote:
>> I was finally able to get R to 'configure', 'make', and 'install' on Mandriva 2007.  Itried to install gnomeGUI and I received an error.  See below.  At what step do I make R a shared library?  Where did I go wrong?
>>
>> Ross
>>
>> ==================================================
>> downloaded 74Kb
>>
>> * Installing *Frontend* package 'gnomeGUI' ...
>> Using R Installation in R_HOME=/usr/local/lib64/R
>> R was not built as a shared library
>> Need a shared R library
>> ERROR: configuration failed for package 'gnomeGUI'
>> * Removing '/usr/local/lib64/R/library/gnomeGUI'
>>
>> The downloaded packages are in
>>         /root/tmp/RtmpkHUeyA/downloaded_packages
>> Warning message:
>> installation of package 'gnomeGUI' had non-zero exit status in: install.packages(c("gnomeGUI"))
>> =================================================================================================
>
>
> Hi Ross!
>
> I use Ubuntu and instaling R using Synaptic. In this software have this
> observation about GnomeGUI:
>
> As of R 2.1.0, this interface is no longer provided with the upstream
> sources. As such, this package is now an empty stub that will be removed
> in a subsequent revision of the Debian package.
>
> So I think  gnomeGUI not instalable in R now...

This is almost entirely misinformation.  gnomeGUI is a package on CRAN 
(and has been for a couple of years), and can be installed just like any 
other package.  Like several others, it requires R to have been configured 
with --enable-R-shlib.

Please file a bug report on the Debian package that is misleading you.

[GNOME has moved on since gnomeGUI was written, and you may well find that 
you need to install older GNOME components to make use of it.]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From xwye at sibs.ac.cn  Wed Mar 21 09:15:43 2007
From: xwye at sibs.ac.cn (Xingwang Ye)
Date: Wed, 21 Mar 2007 16:15:43 +0800
Subject: [R] how to get "lsmeans"?
Message-ID: <DreamMail__161543_64038017385@smtp.sibs.ac.cn>

Dear all, 
      
    I search the mail list about this topic and learn that no simple way is available to get "lsmeans" in R as in SAS.
    Dr.John Fox and Dr.Frank E Harrell have given very useful information about "lsmeans" topic.    
    Dr. Frank E Harrell suggests not to think about lsmeans, just to think about what predicted values wanted
    and to use the predict function. However, after reading the R help file for a whole day, I am still unclear how to do it.
    Could some one give me a hand? 
 
for example:
  
A,B and C are binomial variables(factors); d is a continuous variable ;
The response variable Y is  a continuous variable too.  

To get lsmeans of Y according to A,B and C, respectively, in SAS, I tried  
proc glm data=a;  
 class A B C;  
 model Y=A B C d;  
 lsmeans A B C/cl;  
run;  

In R, I tried this:  
 library(Design)  
 ddist<-datadist(a)  
 options(datadist="ddist")  
 f<-ols(Y~A+B+C+D,data=a,x=TRUE,y=TRUE,se.fit=TRUE)  

then how to get the "lsmeans" for A, B, and C, respectively with predict function?

 

Best wishes 
yours, sincerely 
Xingwang Ye    
PhD candidate     
Research Group of Nutrition Related Cancers and Other Chronic Diseases      
Institute for Nutritional Sciences,  
Shanghai Institutes of Biological Sciences,     
Chinese Academy of Sciences     
P.O.Box 32     
294 Taiyuan Road     
Shanghai 200031     
P.R.CHINA


From ligges at statistik.uni-dortmund.de  Wed Mar 21 09:16:45 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 21 Mar 2007 09:16:45 +0100
Subject: [R] package:AlgDesign and .Random.seed
In-Reply-To: <DC32551B-C910-410F-A0BF-E538A504C700@virginia.edu>
References: <DC32551B-C910-410F-A0BF-E538A504C700@virginia.edu>
Message-ID: <4600E9ED.20303@statistik.uni-dortmund.de>



Michael Kubovy wrote:
> Dear r-helpers,
> 
> Could you please help me solve the following problem: When I run
> 
> require(AlgDesign)
> trt <- LETTERS[1:5]
> blk <- 10
> trtblk <- 3
> BIB <- optBlock(~., withinData = trt, blocksizes = rep(trtblk, blk))
> 
> In response to the last command, R complains:
> 
> Error in optBlock(~., withinData = trt, blocksizes = rep(trtblk, blk)) :
> 	object ".Random.seed" not found
> 
> The documentation of optBlock() in AlgDesign doesn't say that I  
> needed to set .Random.seed. I thought it was initiated automatically  
> at the beginning of a session. What am I missing?


The first line in that function is
     seed <- .Random.seed
but .Random.seed is generated at the first use of R's RNG, hence maybe 
later. This means the function contains a bug which you should report to 
the package maintainer, please.

Best,
Uwe Ligges




>  > sessionInfo()
> R version 2.4.1 (2006-12-18)
> i386-apple-darwin8.8.1
> 
> locale:
> C
> 
> attached base packages:
> [1] "grid"      "datasets"  "stats"     "graphics"  "grDevices"  
> "utils"     "methods"   "base"
> 
> other attached packages:
>     AlgDesign       xtable latticeExtra      lattice      
> gridBase         MASS          JGR       iplots
>       "1.0-7"      "1.4-3"      "0.1-4"    "0.14-16"      "0.4-3"      
> "7.2-32"     "1.4-15"      "1.0-5"
>        JavaGD        rJava
>       "0.3-6"     "0.4-14"
> 
> 
> 
> 
> 
> _____________________________
> Professor Michael Kubovy
> University of Virginia
> Department of Psychology
> USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
> Parcels:    Room 102        Gilmer Hall
>          McCormick Road    Charlottesville, VA 22903
> Office:    B011    +1-434-982-4729
> Lab:        B019    +1-434-982-4751
> Fax:        +1-434-982-4766
> WWW:    http://www.people.virginia.edu/~mk9y/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Wed Mar 21 09:37:23 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 21 Mar 2007 09:37:23 +0100
Subject: [R] Detailed legend in mathplot ...
In-Reply-To: <1174462705.16512.1.camel@localhost>
References: <1174462705.16512.1.camel@localhost>
Message-ID: <4600EEC3.8000909@statistik.uni-dortmund.de>



Petar Milin wrote:
> Hello,
> Recently, I have asked for a help with building graphs, and I got few
> great advices. Now, my appetite is growing :) and I wander how to add
> legend for two (or more) lines in following example:
> 
> matplot(DAT[, c(3,4)], type="b", ylim=c(0,8), xaxt="n", yaxt="n",
>  + pch=c(21,22), col="black", lty=c("dashed","solid"), xlab="", ylab="")
> title(ylab="% correct", xlab="Trial", cex.lab=1.5)
> axis(1, at=1:4, labels=as.character(DAT$Trial), cex.axis=1.5)
> axis(2, cex.axis=1.5)
>

We do not have DAT, why don't you make it easier for us to help by 
specifying a very tiny data.frame that fits to your code?

Best,
Uwe Ligges


> 
> Sincerely,
> Petar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From davee at ceu.ox.ac.uk  Wed Mar 21 10:10:20 2007
From: davee at ceu.ox.ac.uk (Dave Ewart)
Date: Wed, 21 Mar 2007 09:10:20 +0000
Subject: [R] Strange integer result on Debian/amd64
In-Reply-To: <20070321140719.13d94b38@berwin5>
References: <20070320145938.GX16472@nemesis.ceu.ox.ac.uk>
	<20070321140719.13d94b38@berwin5>
Message-ID: <20070321091020.GA6298@nemesis.ceu.ox.ac.uk>

On Wednesday, 21.03.2007 at 14:07 +0800, Berwin A Turlach wrote:

> > All help/suggestions/appreciated!
> 
> The calculations are done in floating point arithmetic, not integer
> arithmetic. From the help page on `choose' one might already guess
> so much, but reading "R-2.4.1/src/nmath/choose.c" definitely confirms
> this fact.
> 
> Thus, your problem is covered by FAQ 7.31:
> 
> http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f

Thanks Berwin.

OK, I understand the problem now.  I was wondering if it something along
those lines, but initially dismissed that as a possibility since I
expected 'choose' to be an integer calculation. 

However, the underlying problem that gave rise to the difficulty was as
follows.  A colleague wishes to create a matrix, where one of the
dimensions of the matrix is the result of the 'choose' function, i.e.

  mycols<-choose(11,6)
  a_matrix<-matrix(0,nrow=11,ncol=mycols)

Clearly, 'ncol' casts mycols as as integer.  In this case, a_matrix has
only 461 columns, not 462.

What's the best way to make this work as required?

Dave.
-- 
Dave Ewart
davee at ceu.ox.ac.uk
Computing Manager, Cancer Epidemiology Unit
Cancer Research UK / Oxford University
PGP: CC70 1883 BD92 E665 B840 118B 6E94 2CFD 694D E370
Get key from http://www.ceu.ox.ac.uk/~davee/davee-ceu-ox-ac-uk.asc
N 51.7518, W 1.2016
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 191 bytes
Desc: Digital signature
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070321/cb2d4f11/attachment.bin 

From davee at ceu.ox.ac.uk  Wed Mar 21 10:24:11 2007
From: davee at ceu.ox.ac.uk (Dave Ewart)
Date: Wed, 21 Mar 2007 09:24:11 +0000
Subject: [R] Strange integer result on Debian/amd64
In-Reply-To: <20070321091020.GA6298@nemesis.ceu.ox.ac.uk>
References: <20070320145938.GX16472@nemesis.ceu.ox.ac.uk>
	<20070321140719.13d94b38@berwin5>
	<20070321091020.GA6298@nemesis.ceu.ox.ac.uk>
Message-ID: <20070321092411.GB6298@nemesis.ceu.ox.ac.uk>

On Wednesday, 21.03.2007 at 09:10 +0000, Dave Ewart wrote:

> However, the underlying problem that gave rise to the difficulty was as
> follows.  A colleague wishes to create a matrix, where one of the
> dimensions of the matrix is the result of the 'choose' function, i.e.
> 
>   mycols<-choose(11,6)
>   a_matrix<-matrix(0,nrow=11,ncol=mycols)
> 
> Clearly, 'ncol' casts mycols as as integer.  In this case, a_matrix has
> only 461 columns, not 462.

Ignore me, obviously one can simply do:

a_matrix<-matrix(0,nrow=11,ncol=round(mycols))

Thanks again,

Dave.
-- 
Dave Ewart
davee at ceu.ox.ac.uk
Computing Manager, Cancer Epidemiology Unit
Cancer Research UK / Oxford University
PGP: CC70 1883 BD92 E665 B840 118B 6E94 2CFD 694D E370
Get key from http://www.ceu.ox.ac.uk/~davee/davee-ceu-ox-ac-uk.asc
N 51.7518, W 1.2016
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 191 bytes
Desc: Digital signature
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070321/a2d1fe88/attachment.bin 

From ripley at stats.ox.ac.uk  Wed Mar 21 10:29:28 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Mar 2007 09:29:28 +0000 (GMT)
Subject: [R] Strange integer result on Debian/amd64
In-Reply-To: <20070321091020.GA6298@nemesis.ceu.ox.ac.uk>
References: <20070320145938.GX16472@nemesis.ceu.ox.ac.uk>
	<20070321140719.13d94b38@berwin5>
	<20070321091020.GA6298@nemesis.ceu.ox.ac.uk>
Message-ID: <Pine.LNX.4.64.0703210923040.30459@gannet.stats.ox.ac.uk>

On Wed, 21 Mar 2007, Dave Ewart wrote:

> On Wednesday, 21.03.2007 at 14:07 +0800, Berwin A Turlach wrote:
>
>>> All help/suggestions/appreciated!
>>
>> The calculations are done in floating point arithmetic, not integer
>> arithmetic. From the help page on `choose' one might already guess
>> so much, but reading "R-2.4.1/src/nmath/choose.c" definitely confirms
>> this fact.
>>
>> Thus, your problem is covered by FAQ 7.31:
>>
>> http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f
>
> Thanks Berwin.
>
> OK, I understand the problem now.  I was wondering if it something along
> those lines, but initially dismissed that as a possibility since I
> expected 'choose' to be an integer calculation.

It is not, because it would be far too easy to get integer overflow.

> However, the underlying problem that gave rise to the difficulty was as
> follows.  A colleague wishes to create a matrix, where one of the
> dimensions of the matrix is the result of the 'choose' function, i.e.
>
>  mycols<-choose(11,6)
>  a_matrix<-matrix(0,nrow=11,ncol=mycols)
>
> Clearly, 'ncol' casts mycols as as integer.  In this case, a_matrix has
> only 461 columns, not 462.
>
> What's the best way to make this work as required?

Use round(mycols).  I think we ought to do this internally: we do when 
using computations via beta/gamma functions.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From davee at ceu.ox.ac.uk  Wed Mar 21 10:35:51 2007
From: davee at ceu.ox.ac.uk (Dave Ewart)
Date: Wed, 21 Mar 2007 09:35:51 +0000
Subject: [R] Strange integer result on Debian/amd64
In-Reply-To: <Pine.LNX.4.64.0703210923040.30459@gannet.stats.ox.ac.uk>
References: <20070320145938.GX16472@nemesis.ceu.ox.ac.uk>
	<20070321140719.13d94b38@berwin5>
	<20070321091020.GA6298@nemesis.ceu.ox.ac.uk>
	<Pine.LNX.4.64.0703210923040.30459@gannet.stats.ox.ac.uk>
Message-ID: <20070321093551.GC6298@nemesis.ceu.ox.ac.uk>

On Wednesday, 21.03.2007 at 09:29 +0000, Prof Brian Ripley wrote:

> >OK, I understand the problem now.  I was wondering if it something along
> >those lines, but initially dismissed that as a possibility since I
> >expected 'choose' to be an integer calculation.
> 
> It is not, because it would be far too easy to get integer overflow.

Yes, of course.  And, also, as I have now remembered, it is perfectly
feasible to use non-integer parameters to n-choose-k as well.

It is clearly too long since I did my Stats degrees *blush*

> >However, the underlying problem that gave rise to the difficulty was as
> >follows.  A colleague wishes to create a matrix, where one of the
> >dimensions of the matrix is the result of the 'choose' function, i.e.
> >
> > mycols<-choose(11,6)
> > a_matrix<-matrix(0,nrow=11,ncol=mycols)
> >
> >Clearly, 'ncol' casts mycols as as integer.  In this case, a_matrix has
> >only 461 columns, not 462.
> >
> >What's the best way to make this work as required?
> 
> Use round(mycols).  I think we ought to do this internally: we do when 
> using computations via beta/gamma functions.

*nods* - thanks.

Dave.
-- 
Dave Ewart
davee at ceu.ox.ac.uk
Computing Manager, Cancer Epidemiology Unit
Cancer Research UK / Oxford University
PGP: CC70 1883 BD92 E665 B840 118B 6E94 2CFD 694D E370
Get key from http://www.ceu.ox.ac.uk/~davee/davee-ceu-ox-ac-uk.asc
N 51.7518, W 1.2016
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 191 bytes
Desc: Digital signature
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070321/6596b470/attachment.bin 

From ccleland at optonline.net  Wed Mar 21 11:09:23 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 21 Mar 2007 06:09:23 -0400
Subject: [R] multcomp
In-Reply-To: <A32055BDEA88C34BB3DBBCD22938077805105F@iu-mssg-mbx109.ads.iu.edu>
References: <A32055BDEA88C34BB3DBBCD229380778E656A5@iu-mssg-mbx109.ads.iu.edu>
	<3948d9e50703182107g622a4885k731ba8befe8b6241@mail.gmail.com>
	<A32055BDEA88C34BB3DBBCD22938077805105F@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <46010453.6090200@optonline.net>

Nair, Murlidharan T wrote:
> Thanks for the hint. 
> Here is what I had done earlier according to the old package
>  
>  mult.comp<-simint(a~b, data=z, conf.level=0.99, type=c("Tukey") )
>  isoforms<-as.vector(rownames(mult.comp$estimate))
>  estimate<-as.vector(mult.comp$estimate)
>  lower<-as.vector(mult.comp$conf.int[,1])
>  upper<-as.vector(mult.comp$conf.int[,2])
>  p.val.raw<-as.vector(mult.comp$p.value.raw)
>  
> Here is how I modified the above so that I can use the new package. Can someone comment if what I have done is correct or not?
> amod<-aov(a~b, data=z)
> mult.comp<-glht(amod, linfct=mcp(b="Tukey"))
> estimate<-confint(mult.comp)[1,1]
> lower<-confint(mult.comp)[1,2]
> upper<-confint(mult.comp)[1,3]
> summary(mult.comp) does give the adjusted pvalues.  I tried all the subscripts to extract it to store in a variable but could not, can any one help me here. Also,  how do I specify the conf.level to be equal to 0.99. The default is 0.95

  To specify a confidence level other than the default, see the help
page for confint().

library(multcomp)
amod <- aov(breaks ~ tension, data = warpbreaks)
mc <- glht(amod, linfct = mcp(tension = "Tukey"))
confint(mc, level = .99)

         Simultaneous Confidence Intervals for General Linear Hypotheses

Multiple Comparisons of Means: Tukey Contrasts

Fit: aov(formula = breaks ~ tension, data = warpbreaks)

Estimated Quantile = 3.0494

Linear Hypotheses:
           Estimate lwr      upr
M - L == 0 -10.0000 -22.0764   2.0764
H - L == 0 -14.7222 -26.7986  -2.6458
H - M == 0  -4.7222 -16.7986   7.3542

99% family-wise confidence level

  To see how to extract the p-values, str() is useful:

str(summary(mc))
summary(mc)$test$pvalues

[1] 0.038569618 0.001440005 0.463049425
attr(,"error")
[1] 0.000226511

> Thanks for your help,
> Cheers../Murli
> 
>  
> ________________________________
> 
> From: talepanda [mailto:talepanda at gmail.com]
> Sent: Sun 3/18/2007 11:07 PM
> To: Nair, Murlidharan T
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] multcomp
> 
> 
> 
> ?glht says
>> with 'print', 'summary',  'confint', 'coef' and 'vcov'  methods
>> being available.
> 
> try:
> 
> example(glht)
> summary(glht(amod, linfct = mcp(tension = "Tukey")))
> confint(glht(amod, linfct = mcp(tension = "Tukey")))
> 
> On 3/19/07, Nair, Murlidharan T <mnair at iusb.edu> wrote:
>> I used the multcomp package sometime back for doing multiple
>> comparisons. I see that it has been updated and the methods like simint
>> are no longer supported. When I run the program it prompts to me to use
>> glht. How do I get the lower and upper conf int and the pValues using
>> glht? Does anyone have an example?
>>
>> Thanks ../Murli
>>
>>
>>
>>
>>       [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From jim at bitwrit.com.au  Wed Mar 21 11:37:51 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Wed, 21 Mar 2007 21:37:51 +1100
Subject: [R] Ticks on barplots
In-Reply-To: <46005A57.2000709@noaa.gov>
References: <46005A57.2000709@noaa.gov>
Message-ID: <46010AFF.3040502@bitwrit.com.au>

Michael H. Prager wrote:
> Dear Gurus,
> 
> Using R 2.4.1 on Windows XP
> 
> I am generating stacked barplots of age-composition of fish populations 
> (Y) over time (X).  As there are many years, not every bars is labeled.  
> When looking at the plot, it becomes difficult to associate labels with 
> their bars.
> 
> We have improved this a bit by using axis() to add a tickmark below each 
> bar.  Can anyone suggest a way to draw ticks ONLY at bars where a tick 
> label is drawn?  Or to make such ticks longer than those where there is 
> no label?
> 
> This is going into a function, so I'm hoping for a method that doesn't 
> require looking at the plot first.
> 
> I have attached a PDF.
> 
> # sample code (simplified) #
> mp <- barplot(t(N.age), xlab = "Year", axisnames = FALSE)
> axis(side = 1, at = mp, labels = rownames(N.age), tcl = -0.75)
> 
Hi Mike,
PDF didn't get through, but I think I understand what you want. Here's a 
suggestion:

oddaxlab<-function(side=1,at,labels) {
  at<-at[which(nchar(labels)>0)]
  labels<-labels[which(nchar(labels)>0)]
  axis(side,at,labels)
}
groupie<-matrix(rnorm(32)+3,nrow=8)
labels<-matrix(c(c("1","","","","5","","7",""),
  c("","","","","13","","",""),
  c("17","","","20","","","",""),
  c("25","","","","","","31","")),nrow=8)
bpos<-barplot(groupie,beside=TRUE)
oddaxlab(at=barpos,labels=labels)

What the function does is only draw ticks and labels where there is a 
non-empty label. You pass only those labels that you want to appear. 
Hope I have gotten the right idea.

Jim


From c_naber at yahoo.com.br  Wed Mar 21 11:55:13 2007
From: c_naber at yahoo.com.br (Caio Lucidius Naberezny Azevedo)
Date: Wed, 21 Mar 2007 07:55:13 -0300 (ART)
Subject: [R] Gaussian Adaptive Quadrature
Message-ID: <250889.8686.qm@web56814.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070321/dbdb741e/attachment.pl 

From pmilin at ff.ns.ac.yu  Wed Mar 21 12:04:08 2007
From: pmilin at ff.ns.ac.yu (Petar Milin)
Date: Wed, 21 Mar 2007 12:04:08 +0100
Subject: [R] Detailed legend in mathplot ...
In-Reply-To: <4600EEC3.8000909@statistik.uni-dortmund.de>
References: <1174462705.16512.1.camel@localhost>
	<4600EEC3.8000909@statistik.uni-dortmund.de>
Message-ID: <1174475048.21245.4.camel@localhost>


You are absolutely right, and I apologize. Here is the data.frame:
no day concentrated distributed
1 4 7 6.8
2 5 3 6
3 6 2.3 5.65
4 7 1.6 5.30
9 8 0.9 4.95

And then, if DAT = data.frame, mathplot() should do the job.

Thank you in advance,
Petar


On Wed, 2007-03-21 at 09:37 +0100, Uwe Ligges wrote:
> 
> Petar Milin wrote:
> > Hello,
> > Recently, I have asked for a help with building graphs, and I got few
> > great advices. Now, my appetite is growing :) and I wander how to add
> > legend for two (or more) lines in following example:
> > 
> > matplot(DAT[, c(3,4)], type="b", ylim=c(0,8), xaxt="n", yaxt="n",
> >  + pch=c(21,22), col="black", lty=c("dashed","solid"), xlab="", ylab="")
> > title(ylab="% correct", xlab="Trial", cex.lab=1.5)
> > axis(1, at=1:4, labels=as.character(DAT$Trial), cex.axis=1.5)
> > axis(2, cex.axis=1.5)
> >
> 
> We do not have DAT, why don't you make it easier for us to help by 
> specifying a very tiny data.frame that fits to your code?
> 
> Best,
> Uwe Ligges
> 
> 
> > 
> > Sincerely,
> > Petar
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From albmont at centroin.com.br  Wed Mar 21 12:06:51 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Wed, 21 Mar 2007 09:06:51 -0200
Subject: [R] Over-writing functions from other packages? What is a good
	strategy??
In-Reply-To: <971536df0703201859p4e730079v2671c0f703dce525@mail.gmail.com>
References: <C83C5E3DEEE97E498B74729A33F6EAEC038786FB@DJFPOST01.djf.agrsci.dk>
	<971536df0703201859p4e730079v2671c0f703dce525@mail.gmail.com>
Message-ID: <20070321110416.M83054@centroin.com.br>

Gabor Grothendieck wrote:
>
> Could you call yours Edges?
>
Is it a good idea to have two different functions, whose name
differs by case sensitivity? I believe this is a shortcut to
Chaos :-)

IMHO, case sensitivity should _only_ be used for aesthetical
purposes, else it may result in errors that are very difficult
to find. Of course, YKMV.[*]

Alberto Monteiro
 
[*] I always use S.I. units :-)


From dimitri.mahieux at student.uclouvain.be  Wed Mar 21 12:42:17 2007
From: dimitri.mahieux at student.uclouvain.be (Mahieux Dimitri)
Date: Wed, 21 Mar 2007 12:42:17 +0100
Subject: [R] Integer Optimization
Message-ID: <46011A19.5060002@student.uclouvain.be>

Hello everybody,

I am looking for a function (or package) which can solve a integer 
optimization problem.  I have found the glpk package but I don't known 
very well how to use it.
Someone has another solution ?

thx


From HDoran at air.org  Wed Mar 21 12:44:54 2007
From: HDoran at air.org (Doran, Harold)
Date: Wed, 21 Mar 2007 07:44:54 -0400
Subject: [R] Gaussian Adaptive Quadrature
In-Reply-To: <250889.8686.qm@web56814.mail.re3.yahoo.com>
Message-ID: <2323A6D37908A847A7C32F1E3662C80E8D518F@dc1ex01.air.org>

The function integrate() uses AGQ. There are other functions for
gaussian quadrature in the statmod() package that I really like. 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Caio 
> Lucidius Naberezny Azevedo
> Sent: Wednesday, March 21, 2007 5:55 AM
> To: Help mailing list - R
> Subject: [R] Gaussian Adaptive Quadrature
> 
> Hi all,
>    
>   Does anybody know any function that performs gaussian 
> adapative quadrature integration of univariate functions?
>    
>   Thanks in advance,
>    
>   Regards,
>   
> Caio
> 
>  __________________________________________________
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ligges at statistik.uni-dortmund.de  Wed Mar 21 13:10:52 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 21 Mar 2007 13:10:52 +0100
Subject: [R] Detailed legend in mathplot ...
In-Reply-To: <1174475048.21245.4.camel@localhost>
References: <1174462705.16512.1.camel@localhost>	
	<4600EEC3.8000909@statistik.uni-dortmund.de>
	<1174475048.21245.4.camel@localhost>
Message-ID: <460120CC.8020803@statistik.uni-dortmund.de>



Petar Milin wrote:
> You are absolutely right, and I apologize. Here is the data.frame:
> no day concentrated distributed
> 1 4 7 6.8
> 2 5 3 6
> 3 6 2.3 5.65
> 4 7 1.6 5.30
> 9 8 0.9 4.95
> 
> And then, if DAT = data.frame, mathplot() should do the job.


So what is the problem with creating the legend? Is the help page for 
?legend not clear enough? I'd just do straightforward

legend(1, 1, legend = names(DAT)[3:4], lty = c("dashed", "solid"))

Otherwise, I do not understand your question.

Best,
Uwe Ligges



> Thank you in advance,
> Petar
> 
> 
> On Wed, 2007-03-21 at 09:37 +0100, Uwe Ligges wrote:
>> Petar Milin wrote:
>>> Hello,
>>> Recently, I have asked for a help with building graphs, and I got few
>>> great advices. Now, my appetite is growing :) and I wander how to add
>>> legend for two (or more) lines in following example:
>>>
>>> matplot(DAT[, c(3,4)], type="b", ylim=c(0,8), xaxt="n", yaxt="n",
>>>  + pch=c(21,22), col="black", lty=c("dashed","solid"), xlab="", ylab="")
>>> title(ylab="% correct", xlab="Trial", cex.lab=1.5)
>>> axis(1, at=1:4, labels=as.character(DAT$Trial), cex.axis=1.5)
>>> axis(2, cex.axis=1.5)
>>>
>> We do not have DAT, why don't you make it easier for us to help by 
>> specifying a very tiny data.frame that fits to your code?
>>
>> Best,
>> Uwe Ligges
>>
>>
>>> Sincerely,
>>> Petar
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Wed Mar 21 13:18:12 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 21 Mar 2007 08:18:12 -0400
Subject: [R] Over-writing functions from other packages? What is a good
	strategy??
In-Reply-To: <20070321110416.M83054@centroin.com.br>
References: <C83C5E3DEEE97E498B74729A33F6EAEC038786FB@DJFPOST01.djf.agrsci.dk>
	<971536df0703201859p4e730079v2671c0f703dce525@mail.gmail.com>
	<20070321110416.M83054@centroin.com.br>
Message-ID: <971536df0703210518p6bff994k172d1c9394660ab@mail.gmail.com>

Sometimes but its also easy to forget about simple solutions.

On 3/21/07, Alberto Monteiro <albmont at centroin.com.br> wrote:
> Gabor Grothendieck wrote:
> >
> > Could you call yours Edges?
> >
> Is it a good idea to have two different functions, whose name
> differs by case sensitivity? I believe this is a shortcut to
> Chaos :-)
>
> IMHO, case sensitivity should _only_ be used for aesthetical
> purposes, else it may result in errors that are very difficult
> to find. Of course, YKMV.[*]
>
> Alberto Monteiro
>
> [*] I always use S.I. units :-)
>
>


From petr.pikal at precheza.cz  Wed Mar 21 13:20:32 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 21 Mar 2007 13:20:32 +0100
Subject: [R] Detailed legend in mathplot ...
In-Reply-To: <1174462705.16512.1.camel@localhost>
Message-ID: <46013120.61.10DB970@localhost>

Hi

you can put quite sophisticated legend using legend command e.g.

legend(locator(1), legend=c("first", "second"), pch=c(21,22), lty = 
c(2,1))

Regards
Petr

On 21 Mar 2007 at 8:38, Petar Milin wrote:

From:           	Petar Milin <pmilin at ff.ns.ac.yu>
To:             	r-help <r-help at stat.math.ethz.ch>
Organization:   	Department of Psychology, University of Novi Sad, Serbia
Date sent:      	Wed, 21 Mar 2007 08:38:25 +0100
Subject:        	[R] Detailed legend in mathplot ...

> Hello,
> Recently, I have asked for a help with building graphs, and I got few
> great advices. Now, my appetite is growing :) and I wander how to add
> legend for two (or more) lines in following example:
> 
> matplot(DAT[, c(3,4)], type="b", ylim=c(0,8), xaxt="n", yaxt="n",
>  + pch=c(21,22), col="black", lty=c("dashed","solid"), xlab="",
>  ylab="")
> title(ylab="% correct", xlab="Trial", cex.lab=1.5)
> axis(1, at=1:4, labels=as.character(DAT$Trial), cex.axis=1.5)
> axis(2, cex.axis=1.5)
> 
> 
> Sincerely,
> Petar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From andy_liaw at merck.com  Wed Mar 21 13:28:28 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 21 Mar 2007 08:28:28 -0400
Subject: [R] how to get "lsmeans"?
In-Reply-To: <DreamMail__161543_64038017385@smtp.sibs.ac.cn>
References: <DreamMail__161543_64038017385@smtp.sibs.ac.cn>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03E64F6F@usctmx1106.merck.com>

I verified the result from the following with output from JMP 6 on the
same data (don't have SAS: don't need it):

set.seed(631)
n <- 100
dat <- data.frame(y=rnorm(n), A=factor(sample(1:2, n, replace=TRUE)),
                  B=factor(sample(1:2, n, replace=TRUE)),
                  C=factor(sample(1:2, n, replace=TRUE)),
                  d=rnorm(n))
fm <- lm(y ~ A + B + C + d, dat)
## Form a data frame of points to predict: all combinations of the
## three factors and the mean of the covariate.
p <- data.frame(expand.grid(A=1:2, B=1:2, C=1:2))
p[] <- lapply(p, factor)
p <- cbind(p, d=mean(dat$d))
p <- cbind(yhat=predict(fm, p), p)
## lsmeans for the three factors:
with(p, tapply(yhat, A, mean))
with(p, tapply(yhat, B, mean))
with(p, tapply(yhat, C, mean))

Andy 

From: Xingwang Ye
> 
> Dear all, 
>       
>     I search the mail list about this topic and learn that no 
> simple way is available to get "lsmeans" in R as in SAS.
>     Dr.John Fox and Dr.Frank E Harrell have given very useful 
> information about "lsmeans" topic.    
>     Dr. Frank E Harrell suggests not to think about lsmeans, 
> just to think about what predicted values wanted
>     and to use the predict function. However, after reading 
> the R help file for a whole day, I am still unclear how to do it.
>     Could some one give me a hand? 
>  
> for example:
>   
> A,B and C are binomial variables(factors); d is a continuous 
> variable ; The response variable Y is  a continuous variable too.  
> 
> To get lsmeans of Y according to A,B and C, respectively, in 
> SAS, I tried proc glm data=a;  class A B C;  model Y=A B C d; 
>  lsmeans A B C/cl; run;  
> 
> In R, I tried this:  
>  library(Design)
>  ddist<-datadist(a)
>  options(datadist="ddist")
>  f<-ols(Y~A+B+C+D,data=a,x=TRUE,y=TRUE,se.fit=TRUE)  
> 
> then how to get the "lsmeans" for A, B, and C, respectively 
> with predict function?
> 
>  
> 
> Best wishes
> yours, sincerely 
> Xingwang Ye    
> PhD candidate     
> Research Group of Nutrition Related Cancers and Other Chronic 
> Diseases      
> Institute for Nutritional Sciences,  
> Shanghai Institutes of Biological Sciences,     
> Chinese Academy of Sciences     
> P.O.Box 32     
> 294 Taiyuan Road     
> Shanghai 200031     
> P.R.CHINA
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From bates at stat.wisc.edu  Wed Mar 21 14:25:16 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 21 Mar 2007 08:25:16 -0500
Subject: [R] Gaussian Adaptive Quadrature
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80E8D518F@dc1ex01.air.org>
References: <250889.8686.qm@web56814.mail.re3.yahoo.com>
	<2323A6D37908A847A7C32F1E3662C80E8D518F@dc1ex01.air.org>
Message-ID: <40e66e0b0703210625s7c64dff1p38d4ff4b8b13ff1d@mail.gmail.com>

On 3/21/07, Doran, Harold <HDoran at air.org> wrote:
> The function integrate() uses AGQ. There are other functions for
> gaussian quadrature in the statmod() package that I really like.

I think that integrate does adaptive quadrature but not adaptive
Gaussian quadrature (which probably should have been called adaptive
Gauss-Hermite quadrature to be more specific).  In the first case the
"adaptive" refers to a choice of mesh size.  In the second case one is
integrating a function that is close to a multivariate Gaussian
density by first finding the conditional optimum of the integrand and
using a quadratic approximation to the log-integrand to establish the
location of the Gauss-Hermite quadrature points.

The Laplace approximation to the log-likelihood for a generalized
linear mixed model is a 1-point adaptive Gauss-Hermite quadrature
evaluation.

>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Caio
> > Lucidius Naberezny Azevedo
> > Sent: Wednesday, March 21, 2007 5:55 AM
> > To: Help mailing list - R
> > Subject: [R] Gaussian Adaptive Quadrature
> >
> > Hi all,
> >
> >   Does anybody know any function that performs gaussian
> > adapative quadrature integration of univariate functions?
> >
> >   Thanks in advance,
> >
> >   Regards,
> >
> > Caio
> >
> >  __________________________________________________
> >
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mark at markfisher.net  Wed Mar 21 14:29:50 2007
From: mark at markfisher.net (Mark Fisher)
Date: Wed, 21 Mar 2007 08:29:50 -0500
Subject: [R] newbie upgrade from 2.4.0 to 2.4.1 question
Message-ID: <200703211326.l2LDQ6NL008503@rs19.luxsci.com>

Hi,

I am new to R. I installed version 2.4.0 some time ago and I find that 
some packages I want to use require 2.4.1. I am using Windows XP. Do I 
need to uninstall R first before running the setup file for 2.4.1 or 
does the setup file "do the right thing"?

--Mark.


From kubovy at virginia.edu  Wed Mar 21 14:28:39 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Wed, 21 Mar 2007 09:28:39 -0400
Subject: [R] package:AlgDesign and .Random.seed
In-Reply-To: <4600E9ED.20303@statistik.uni-dortmund.de>
References: <DC32551B-C910-410F-A0BF-E538A504C700@virginia.edu>
	<4600E9ED.20303@statistik.uni-dortmund.de>
Message-ID: <A34AFE9C-0617-48D4-B91C-AA1709723D78@virginia.edu>

On Mar 21, 2007, at 4:16 AM, Uwe Ligges wrote:

> Michael Kubovy wrote:
>> Dear r-helpers,
>> Could you please help me solve the following problem: When I run
>> require(AlgDesign)
>> trt <- LETTERS[1:5]
>> blk <- 10
>> trtblk <- 3
>> BIB <- optBlock(~., withinData = trt, blocksizes = rep(trtblk, blk))
>> In response to the last command, R complains:
>> Error in optBlock(~., withinData = trt, blocksizes = rep(trtblk,  
>> blk)) :
>> 	object ".Random.seed" not found
>> The documentation of optBlock() in AlgDesign doesn't say that I   
>> needed to set .Random.seed. I thought it was initiated  
>> automatically  at the beginning of a session. What am I missing?
>
>
> The first line in that function is
>     seed <- .Random.seed
> but .Random.seed is generated at the first use of R's RNG, hence  
> maybe later. This means the function contains a bug which you  
> should report to the package maintainer, please.
>
> Best,
> Uwe Ligges

Bob Wheeler's response:

> From: Bob Wheeler <rwheeler at echip.com>
> Date: March 21, 2007 9:19:29 AM EDT
> To: Michael Kubovy <kubovy at virginia.edu>
> Subject: Re:
>
> Each workspace in R requires you to set a random seed to start. You  
> have not done this. It is an R artifact, and has nothing to do with  
> AlgDesign.
> -- 
> Bob Wheeler --- http://www.bobwheeler.com/
>    ECHIP, Inc. --- Randomness comes in bunches.


From ccleland at optonline.net  Wed Mar 21 14:28:55 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 21 Mar 2007 09:28:55 -0400
Subject: [R] how to get "lsmeans"?
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03E64F6F@usctmx1106.merck.com>
References: <DreamMail__161543_64038017385@smtp.sibs.ac.cn>
	<39B6DDB9048D0F4DAD42CB26AAFF0AFA03E64F6F@usctmx1106.merck.com>
Message-ID: <46013317.5050402@optonline.net>

Liaw, Andy wrote:
> I verified the result from the following with output from JMP 6 on the
> same data (don't have SAS: don't need it):
> 
> set.seed(631)
> n <- 100
> dat <- data.frame(y=rnorm(n), A=factor(sample(1:2, n, replace=TRUE)),
>                   B=factor(sample(1:2, n, replace=TRUE)),
>                   C=factor(sample(1:2, n, replace=TRUE)),
>                   d=rnorm(n))
> fm <- lm(y ~ A + B + C + d, dat)
> ## Form a data frame of points to predict: all combinations of the
> ## three factors and the mean of the covariate.
> p <- data.frame(expand.grid(A=1:2, B=1:2, C=1:2))
> p[] <- lapply(p, factor)
> p <- cbind(p, d=mean(dat$d))
> p <- cbind(yhat=predict(fm, p), p)
> ## lsmeans for the three factors:
> with(p, tapply(yhat, A, mean))
> with(p, tapply(yhat, B, mean))
> with(p, tapply(yhat, C, mean))

  Using Andy's example data, these are the LSMEANS and intervals I get
from SAS:

A        y LSMEAN      95% Confidence Limits
1       -0.071847       -0.387507     0.243813
2       -0.029621       -0.342358     0.283117

B        y LSMEAN      95% Confidence Limits
1       -0.104859       -0.397935     0.188216
2        0.003391       -0.333476     0.340258

C        y LSMEAN      95% Confidence Limits
1       -0.084679       -0.392343     0.222986
2       -0.016789       -0.336374     0.302795

  One way of reproducing the LSMEANS and intervals from SAS using
predict() seems to be the following:

> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d, data = dat)
> newdat <- expand.grid(A=factor(c(1,2)),B=1.5,C=1.5,d=mean(dat$d))
> cbind(newdat, predict(dat.lm, newdat, interval="confidence"))
  A   B   C          d         fit        lwr       upr
1 1 1.5 1.5 0.09838595 -0.07184709 -0.3875070 0.2438128
2 2 1.5 1.5 0.09838595 -0.02962086 -0.3423582 0.2831165

  However, another possibility seems to be:

> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d, data = dat)
> newdat <-
expand.grid(A=factor(c(1,2)),B=mean(as.numeric(dat$B)),C=mean(as.numeric(dat$C)),d=mean(dat$d))
> cbind(newdat, predict(dat.lm, newdat, interval="confidence"))
  A    B    C          d         fit        lwr       upr
1 1 1.43 1.48 0.09838595 -0.08078243 -0.3964661 0.2349012
2 2 1.43 1.48 0.09838595 -0.03855619 -0.3479589 0.2708465

  The predictions directly above match what effect() in the effects
package by John Fox returns:

library(effects)

> effect("A", fm, xlevels=list(d = mean(dat$D)))

 A effect
A
          1           2
-0.08078243 -0.03855619

  But for some reason the predict() and effect() intervals are a little
different:

> effect("A", fm, xlevels=list(d = mean(dat$D)))$lower
          [,1]
101 -0.3924451
102 -0.3440179

> effect("A", fm, xlevels=list(d = mean(dat$D)))$upper
         [,1]
101 0.2308802
102 0.2669055

  I would be interested in any comments on these different approaches
and on the difference in intervals returned by predict() and effect().

hope this helps,

Chuck

> Andy 
> 
> From: Xingwang Ye
>> Dear all, 
>>       
>>     I search the mail list about this topic and learn that no 
>> simple way is available to get "lsmeans" in R as in SAS.
>>     Dr.John Fox and Dr.Frank E Harrell have given very useful 
>> information about "lsmeans" topic.    
>>     Dr. Frank E Harrell suggests not to think about lsmeans, 
>> just to think about what predicted values wanted
>>     and to use the predict function. However, after reading 
>> the R help file for a whole day, I am still unclear how to do it.
>>     Could some one give me a hand? 
>>  
>> for example:
>>   
>> A,B and C are binomial variables(factors); d is a continuous 
>> variable ; The response variable Y is  a continuous variable too.  
>>
>> To get lsmeans of Y according to A,B and C, respectively, in 
>> SAS, I tried proc glm data=a;  class A B C;  model Y=A B C d; 
>>  lsmeans A B C/cl; run;  
>>
>> In R, I tried this:  
>>  library(Design)
>>  ddist<-datadist(a)
>>  options(datadist="ddist")
>>  f<-ols(Y~A+B+C+D,data=a,x=TRUE,y=TRUE,se.fit=TRUE)  
>>
>> then how to get the "lsmeans" for A, B, and C, respectively 
>> with predict function?
>>
>>  
>>
>> Best wishes
>> yours, sincerely 
>> Xingwang Ye    
>> PhD candidate     
>> Research Group of Nutrition Related Cancers and Other Chronic 
>> Diseases      
>> Institute for Nutritional Sciences,  
>> Shanghai Institutes of Biological Sciences,     
>> Chinese Academy of Sciences     
>> P.O.Box 32     
>> 294 Taiyuan Road     
>> Shanghai 200031     
>> P.R.CHINA
>>
>>
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments,...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From noramuda at yahoo.com  Wed Mar 21 03:25:06 2007
From: noramuda at yahoo.com (Nora Muda)
Date: Tue, 20 Mar 2007 19:25:06 -0700 (PDT)
Subject: [R] substitution matrix for amino acid
Message-ID: <649887.57035.qm@web61011.mail.yahoo.com>

Hi,
 
I want to compute the pairwise distances of amino acid
sequences and then construct the phylogenetic tree. I
noticed that ape package  support only for DNA
sequences and compute the pairwise distances by using
dist.dna function. Can anyone suggest what package
that support for protein sequences and then construct
the phylogenetic tree?
 
Thank you in advance for any help.
 
Nora.



 
____________________________________________________________________________________
We won't tell. Get more on shows you hate to love


From albmont at centroin.com.br  Wed Mar 21 14:38:12 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Wed, 21 Mar 2007 11:38:12 -0200
Subject: [R] newbie upgrade from 2.4.0 to 2.4.1 question
In-Reply-To: <200703211326.l2LDQ6NL008503@rs19.luxsci.com>
References: <200703211326.l2LDQ6NL008503@rs19.luxsci.com>
Message-ID: <20070321133609.M80111@centroin.com.br>

Mark Fisher wrote:
> 
> I am new to R. I installed version 2.4.0 some time ago and I find 
> that some packages I want to use require 2.4.1. I am using Windows 
> XP. Do I need to uninstall R first before running the setup file for 
> 2.4.1 or does the setup file "do the right thing"?
> 
You don't have to uninstall. It does the right thing. 

But it will not delete the older 2.4.0 version from the Desktop, 
and all other places where Windows XP places programs.

Alberto Monteiro


From maitra at iastate.edu  Wed Mar 21 14:38:26 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Wed, 21 Mar 2007 08:38:26 -0500
Subject: [R] question on suppressing error messages with Rmath library
Message-ID: <20070321083826.5fbd12c1@triveni.stat.iastate.edu>

Dear list,

I have been using the Rmath library for quite a while: in the current instance, I am calling dnt (non-central t density function) repeatedly for several million. When the argument is small, I get the warning message:

full precision was not achieved in 'pnt'

which is nothing unexpected. (The density calls pnt, if you look at the function dnt.) However, to have this happen a huge number of times, when the optimizer is churning through the dataset is bothersome, but more importantly, a bottleneck in terms of speed. Is it possible to switch this off? Is there an setting somewhere that I am missing?

Many thanks and best wishes,
Ranjan


From ripley at stats.ox.ac.uk  Wed Mar 21 14:53:17 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Mar 2007 13:53:17 +0000 (GMT)
Subject: [R] newbie upgrade from 2.4.0 to 2.4.1 question
In-Reply-To: <200703211326.l2LDQ6NL008503@rs19.luxsci.com>
References: <200703211326.l2LDQ6NL008503@rs19.luxsci.com>
Message-ID: <Pine.LNX.4.64.0703211352280.22389@gannet.stats.ox.ac.uk>

On Wed, 21 Mar 2007, Mark Fisher wrote:

> I am new to R. I installed version 2.4.0 some time ago and I find that
> some packages I want to use require 2.4.1. I am using Windows XP. Do I
> need to uninstall R first before running the setup file for 2.4.1 or
> does the setup file "do the right thing"?

It 'does the right thing' as discussed in the rw-FAQ.  Please follow the 
posting guide.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jmacdon at med.umich.edu  Wed Mar 21 15:06:08 2007
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Wed, 21 Mar 2007 10:06:08 -0400
Subject: [R] question on suppressing error messages with Rmath library
In-Reply-To: <20070321083826.5fbd12c1@triveni.stat.iastate.edu>
References: <20070321083826.5fbd12c1@triveni.stat.iastate.edu>
Message-ID: <46013BD0.20501@med.umich.edu>

Hi Ranjan,
Ranjan Maitra wrote:
> Dear list,
> 
> I have been using the Rmath library for quite a while: in the current
> instance, I am calling dnt (non-central t density function)
> repeatedly for several million. When the argument is small, I get the
> warning message:
> 
> full precision was not achieved in 'pnt'
> 
> which is nothing unexpected. (The density calls pnt, if you look at
> the function dnt.) However, to have this happen a huge number of
> times, when the optimizer is churning through the dataset is
> bothersome, but more importantly, a bottleneck in terms of speed. Is
> it possible to switch this off? Is there an setting somewhere that I
> am missing?

See ?options, particularly the section on 'warn'.

Best,

Jim


> 
> Many thanks and best wishes, Ranjan
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide http://www.R-project.org/posting-guide.html and provide
> commented, minimal, self-contained, reproducible code.


-- 
James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623


**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.


From ronggui.huang at gmail.com  Wed Mar 21 15:08:16 2007
From: ronggui.huang at gmail.com (ronggui)
Date: Wed, 21 Mar 2007 22:08:16 +0800
Subject: [R] question on suppressing error messages with Rmath library
In-Reply-To: <20070321083826.5fbd12c1@triveni.stat.iastate.edu>
References: <20070321083826.5fbd12c1@triveni.stat.iastate.edu>
Message-ID: <38b9f0350703210708v24fc84e7t9bb40534e5d5f8bb@mail.gmail.com>

op <- options(warn=-1)
[main codes here]
options(op)



On 3/21/07, Ranjan Maitra <maitra at iastate.edu> wrote:
> Dear list,
>
> I have been using the Rmath library for quite a while: in the current instance, I am calling dnt (non-central t density function) repeatedly for several million. When the argument is small, I get the warning message:
>
> full precision was not achieved in 'pnt'
>
> which is nothing unexpected. (The density calls pnt, if you look at the function dnt.) However, to have this happen a huge number of times, when the optimizer is churning through the dataset is bothersome, but more importantly, a bottleneck in terms of speed. Is it possible to switch this off? Is there an setting somewhere that I am missing?
>
> Many thanks and best wishes,
> Ranjan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ronggui Huang
Department of Sociology
Fudan University, Shanghai, China


From uabarrett at cheersboston.com  Wed Mar 21 15:15:52 2007
From: uabarrett at cheersboston.com (Sonia KMohammed)
Date: Wed, 21 Mar 2007 15:15:52 +0100
Subject: [R] He wilma
Message-ID: <03f201c76c56$3adf05f0$9974a4b0@esunderj>

Check this company out
C E O AMERICA I N C
SYm-CE OA
Currently : 6 Cents, CHEAP!!!
Add this to your radar

AN ALL AMERICAN COMPANY
Get IN Before the rush TOMORROW

 led the Pirates to a 68-55 record, a trip to the round of 16 in the 2000  games and eight of their next nine.  ''This does a lot for our confidence,''  We have to get back to the way we were before that game.''   The Suns, who   nine years on Mike Krzyzewski's staff at Duke, where he was a four-year


From andy_liaw at merck.com  Wed Mar 21 15:21:01 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 21 Mar 2007 10:21:01 -0400
Subject: [R] package:AlgDesign and .Random.seed  [Broadcast]
In-Reply-To: <A34AFE9C-0617-48D4-B91C-AA1709723D78@virginia.edu>
References: <DC32551B-C910-410F-A0BF-E538A504C700@virginia.edu>
	<4600E9ED.20303@statistik.uni-dortmund.de>
	<A34AFE9C-0617-48D4-B91C-AA1709723D78@virginia.edu>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03E6506A@usctmx1106.merck.com>

From: Michael Kubovy
> 
> On Mar 21, 2007, at 4:16 AM, Uwe Ligges wrote:
> 
> > Michael Kubovy wrote:
> >> Dear r-helpers,
> >> Could you please help me solve the following problem: When I run
> >> require(AlgDesign)
> >> trt <- LETTERS[1:5]
> >> blk <- 10
> >> trtblk <- 3
> >> BIB <- optBlock(~., withinData = trt, blocksizes = 
> rep(trtblk, blk)) 
> >> In response to the last command, R complains:
> >> Error in optBlock(~., withinData = trt, blocksizes = rep(trtblk,
> >> blk)) :
> >> 	object ".Random.seed" not found
> >> The documentation of optBlock() in AlgDesign doesn't say that I   
> >> needed to set .Random.seed. I thought it was initiated 
> automatically  
> >> at the beginning of a session. What am I missing?
> >
> >
> > The first line in that function is
> >     seed <- .Random.seed
> > but .Random.seed is generated at the first use of R's RNG, hence  
> > maybe later. This means the function contains a bug which you  
> > should report to the package maintainer, please.
> >
> > Best,
> > Uwe Ligges
> 
> Bob Wheeler's response:
> 
> > From: Bob Wheeler <rwheeler at echip.com>
> > Date: March 21, 2007 9:19:29 AM EDT
> > To: Michael Kubovy <kubovy at virginia.edu>
> > Subject: Re:
> >
> > Each workspace in R requires you to set a random seed to 
> start. You  
> > have not done this. It is an R artifact, and has nothing to 
> do with  
> > AlgDesign.

I do not agree with that assessment (well, it's just my $0.02 anyway).
I don't need a random seed unless I'm doing computations that requires
pseudo-random numbers.  There are plenty of times I use R without
needing random seed.

None of the builtin RNGs in R requires explcit seed setting, nor does
any of the ones in the contributed packages that I know of.  Thus I
would claim that's a flaw in AlgDesign.

Andy

> > -- 
> > Bob Wheeler --- http://www.bobwheeler.com/
> >    ECHIP, Inc. --- Randomness comes in bunches.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From muenchen at utk.edu  Wed Mar 21 15:46:35 2007
From: muenchen at utk.edu (Muenchen, Robert A (Bob))
Date: Wed, 21 Mar 2007 10:46:35 -0400
Subject: [R] Select the last two rows by id group
In-Reply-To: <1174438701.5415.42.camel@localhost.localdomain>
References: <ba8c09910703200733g37a25e8cve55952fc640c1ec1@mail.gmail.com>
	<1174402712.4920.20.camel@localhost.localdomain>
	<D028EEB4CA113D4EAFDD485CCC9982777313FC@UTKFSVS4.utk.tennessee.edu>
	<1174438701.5415.42.camel@localhost.localdomain>
Message-ID: <D028EEB4CA113D4EAFDD485CCC9982777315E9@UTKFSVS4.utk.tennessee.edu>

Marc, thanks for so many great variations! I especially like:

tail(sort(table(DF$County)))

I often have frequency tables that are of interest only towards the end.

Cheers,
Bob

=========================================================
  Bob Muenchen (pronounced Min'-chen), Manager  
  Statistical Consulting Center
  U of TN Office of Information Technology
  200 Stokely Management Center, Knoxville, TN 37996-0520
  Voice: (865) 974-5230  
  FAX:   (865) 974-4810
  Email: muenchen at utk.edu
  Web:   http://oit.utk.edu/scc, 
  News:  http://listserv.utk.edu/archives/statnews.html
=========================================================


> -----Original Message-----
> From: Marc Schwartz [mailto:marc_schwartz at comcast.net]
> Sent: Tuesday, March 20, 2007 8:58 PM
> To: Muenchen, Robert A (Bob)
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] Select the last two rows by id group
> 
> On Tue, 2007-03-20 at 11:53 -0400, Muenchen, Robert A (Bob) wrote:
> > Very nice! This is almost duplicates the SAS first.var and last.var
> > ability to choose the first and last observations by group(s).
> > Substituting the head function in where Marc has the tail function
> below
> > will adapt it to the first n. It is more flexible than the SAS
> approach
> > because it can do the first/last n rather than just the single first
> or
> > last.
> >
> > Let's say we want to choose the last observation in a county, and
> > counties have duplicate names in different states. You could sort by
> > state, then county, then use only county where Marc uses score$id in
> his
> > last example below, and it would get the last record for *every*
> county
> > regardless of duplicates. Does this sound correct?
> >
> > That's a handy bit of code!
> >
> > Cheers,
> > Bob
> 
> Bob,
> 
> You can test it using data here:
> 
> DF <- read.csv("http://www.nws.noaa.gov/nwr/SameCode.txt",
>                header = FALSE)
> 
> colnames(DF) <- c("Code", "County", "State")
> 
> > str(DF)
> 'data.frame':   3288 obs. of  3 variables:
>  $ Code  : int  1001 1003 1005 1007 1009 1011 1013 1015 1017 1019 ...
>  $ County: Factor w/ 1996 levels "Abbeville","Acadia",..: 97 105 116
> 169 186 249 259 272 326 348 ...
>  $ State : Factor w/ 60 levels "AK","AL","AR",..: 2 2 2 2 2 2 2 2 2 2
> ...
> 
> 
> The data is already sorted by State and then County.
> 
> 
> > system.time(DF.tail <- do.call("rbind", lapply(split(DF, DF$County),
> tail,  1)))
> [1] 6.851 0.085 7.085 0.000 0.000
> 
> 
> > str(DF.tail)
> 'data.frame':   1996 obs. of  3 variables:
>  $ Code  : int  45001 22001 16001 40001 55001 50001 72001 72003 72005
> 72007 ...
>  $ County: Factor w/ 1996 levels "Abbeville","Acadia",..: 1 2 3 4 5 6
7
> 8 9 10 ...
>  $ State : Factor w/ 60 levels "AK","AL","AR",..: 48 22 17 42 58 56 45
> 45 45 45 ...
> 
> 
> # How many unique county names in the source dataset?
> 
> > length(unique(DF$County))
> [1] 1996
> 
> 
> # Are they all the same unique counties?
> 
> > all(DF.tail$County == sort(unique(DF$County)))
> [1] TRUE
> 
> 
> It is curious to see just how many duplicates there are. For example:
> 
> > tail(sort(table(DF$County)))
> 
>    Madison    Jackson    Lincoln   Franklin  Jefferson Washington
>         20         24         24         25         26         31
> 
> 
> > subset(DF, County == "Washington")
>       Code     County State
> 65    1129 Washington    AL
> 181   5143 Washington    AR
> 304   8121 Washington    CO
> 385  12133 Washington    FL
> 535  13303 Washington    GA
> 593  16087 Washington    ID
> 688  17189 Washington    IL
> 783  18175 Washington    IN
> 879  19183 Washington    IA
> 987  20201 Washington    KS
> 1106 21229 Washington    KY
> 1167 22117 Washington    LA
> 1189 23029 Washington    ME
> 1211 24043 Washington    MD
> 1393 27163 Washington    MN
> 1474 28151 Washington    MS
> 1590 29221 Washington    MO
> 1740 31177 Washington    NE
> 1883 36115 Washington    NY
> 1981 37187 Washington    NC
> 2124 39167 Washington    OH
> 2202 40147 Washington    OK
> 2239 41067 Washington    OR
> 2304 42125 Washington    PA
> 2313 44009 Washington    RI
> 2515 47179 Washington    TN
> 2759 48477 Washington    TX
> 2800 49053 Washington    UT
> 2814 50023 Washington    VT
> 2904 51191 Washington    VA
> 3108 55131 Washington    WI
> 
> 
> # The last state with Washington County (my neighbors, the
> "Cheeseheads") was in the result set
> 
> > subset(DF.tail, County == "Washington")
>             Code     County State
> Washington 55131 Washington    WI
> 
> 
> 
> > subset(DF, County == "Allen")
>       Code County State
> 697  18003  Allen    IN
> 887  20001  Allen    KS
> 993  21003  Allen    KY
> 1113 22003  Allen    LA
> 2042 39003  Allen    OH
> 
> 
> # The last state with Allen County (OH) was in the result set
> 
> > subset(DF.tail, County == "Allen")
>        Code County State
> Allen 39003  Allen    OH
> 
> 
> Just noticed a Big Ten theme there...Go Gophers!   ;-)
> 
> 
> So, it would seem that your hypothesis is correct, at least in this
> limited testing.  I would want to validate it more rigorously of
> course.
> 
> HTH,
> 
> Marc Schwartz
>


From michaejs at cires.colorado.edu  Wed Mar 21 15:50:11 2007
From: michaejs at cires.colorado.edu (Michael John Shaw)
Date: Wed, 21 Mar 2007 08:50:11 -0600 (MDT)
Subject: [R] ar predict without noise
Message-ID: <Pine.GSO.4.43.0703210846050.13552-100000@cires.colorado.edu>

Hi.

I am just getting started with R.  I would like to start by using one of
the autoregressive models (beginning at the very basic, perhaps with
just ar) to forecast a multivariate time series one step
ahead without noise and, while digging around for how to do so, maybe
someone can just tell me whether there is an option to set the noise term
to 0 (zero).

How do I set the noise term to zero?

Thank you.

--

Michael Shaw

Cooperative Institute for Research in Environmental Sciences
Campus Box 216
University of Colorado, Boulder, 80309-0216

303-492-3619
michaejs at cires.colorado.edu

--


From ligges at statistik.uni-dortmund.de  Wed Mar 21 15:51:52 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 21 Mar 2007 15:51:52 +0100
Subject: [R] package:AlgDesign and .Random.seed
In-Reply-To: <A34AFE9C-0617-48D4-B91C-AA1709723D78@virginia.edu>
References: <DC32551B-C910-410F-A0BF-E538A504C700@virginia.edu>
	<4600E9ED.20303@statistik.uni-dortmund.de>
	<A34AFE9C-0617-48D4-B91C-AA1709723D78@virginia.edu>
Message-ID: <46014688.8040509@statistik.uni-dortmund.de>

So let me read the help page of ?RNG for all of us:

"Initially, there is no seed; a new one is created from the current time 
when one is required. Hence, different sessions will give different 
simulation results, by default."

Hence this is definitely a bug in AlgDesign.

Further on also remarkable for Bob Wheeler's decision to save .Random.seed:

".Random.seed saves the seed set for the uniform random-number 
generator, at least for the system generators. It does not necessarily 
save the state of other generators, and in particular does not save the 
state of the Box?Muller normal generator. If you want to reproduce work 
later, call set.seed rather than set .Random.seed."


Uwe Ligges




Michael Kubovy wrote:
> On Mar 21, 2007, at 4:16 AM, Uwe Ligges wrote:
> 
>> Michael Kubovy wrote:
>>> Dear r-helpers,
>>> Could you please help me solve the following problem: When I run
>>> require(AlgDesign)
>>> trt <- LETTERS[1:5]
>>> blk <- 10
>>> trtblk <- 3
>>> BIB <- optBlock(~., withinData = trt, blocksizes = rep(trtblk, blk))
>>> In response to the last command, R complains:
>>> Error in optBlock(~., withinData = trt, blocksizes = rep(trtblk, blk)) :
>>>     object ".Random.seed" not found
>>> The documentation of optBlock() in AlgDesign doesn't say that I  
>>> needed to set .Random.seed. I thought it was initiated automatically  
>>> at the beginning of a session. What am I missing?
>>
>>
>> The first line in that function is
>>     seed <- .Random.seed
>> but .Random.seed is generated at the first use of R's RNG, hence maybe 
>> later. This means the function contains a bug which you should report 
>> to the package maintainer, please.
>>
>> Best,
>> Uwe Ligges
> 
> Bob Wheeler's response:
> 
>> From: Bob Wheeler <rwheeler at echip.com>
>> Date: March 21, 2007 9:19:29 AM EDT
>> To: Michael Kubovy <kubovy at virginia.edu>
>> Subject: Re:
>>
>> Each workspace in R requires you to set a random seed to start. You 
>> have not done this. It is an R artifact, and has nothing to do with 
>> AlgDesign.
>> --Bob Wheeler --- http://www.bobwheeler.com/
>>    ECHIP, Inc. --- Randomness comes in bunches.


From luke at stat.uiowa.edu  Wed Mar 21 16:04:05 2007
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Wed, 21 Mar 2007 10:04:05 -0500 (CDT)
Subject: [R] question on suppressing error messages with Rmath library
In-Reply-To: <20070321083826.5fbd12c1@triveni.stat.iastate.edu>
References: <20070321083826.5fbd12c1@triveni.stat.iastate.edu>
Message-ID: <Pine.LNX.4.64.0703210955560.4457@itasca2.wildberry.org>

You might get less noise in the replies if you were explicit about
using Rmath stand-alone and asked on r-devel.

As far as I can see you would need to compile a version of the
stand-alone library that defines the macros for handling of warning
messages differently -- the current one just calls printf in the
stand-alone library.  (You might be able to trick the linker into using
a version of printf for calls from within Rmath that does nothing, but
I suspect recompiling the sourses is easier.)  We will probably be
rethinking this soon in conjunction with some other changes to
vectorized math in R.


Best,

luke

On Wed, 21 Mar 2007, Ranjan Maitra wrote:

> Dear list,
>
> I have been using the Rmath library for quite a while: in the current instance, I am calling dnt (non-central t density function) repeatedly for several million. When the argument is small, I get the warning message:
>
> full precision was not achieved in 'pnt'
>
> which is nothing unexpected. (The density calls pnt, if you look at the function dnt.) However, to have this happen a huge number of times, when the optimizer is churning through the dataset is bothersome, but more importantly, a bottleneck in terms of speed. Is it possible to switch this off? Is there an setting somewhere that I am missing?
>
> Many thanks and best wishes,
> Ranjan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From Luisr at frs.fo  Wed Mar 21 16:14:16 2007
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Wed, 21 Mar 2007 15:14:16 +0000
Subject: [R] for loop in assigning column names
Message-ID: <s6014bcc.067@ffdata.setur.fo>

R-help,

I have a data frame (df) and I want to add some columns whose names
should correspond to the "i" index in the loop below.

 for(i in 1:10)
 {
df$eval(paste("St", as.character(i), sep = "" ))  <- ObJeCt[i]
 }

An error message comes out : 

"Error: attempt to apply non-function"

How can I get around this?

I could do something like :

df$St2 <- NA
df$St3 <- NA
dft$St4 < -NA
..

and afterwards assign the results of the loop above 
to the columns df$St2,df$St3,,,,,,,,,,,

The problem is that my object "ObJeCt[i]" may change in size
and definition and therefore a way to systematize the task would
be desirable.

Thanks in advance

> version
               _                           
platform       i386-pc-mingw32             
arch           i386                        
os             mingw32                     
system         i386, mingw32               
status                                     
major          2                           
minor          4.1                         
year           2006                        
month          12                          
day            18                          
svn rev        40228                       
language       R                           
version.string R version 2.4.1 (2006-12-18)


From Thierry.ONKELINX at inbo.be  Wed Mar 21 16:23:23 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 21 Mar 2007 16:23:23 +0100
Subject: [R] for loop in assigning column names
In-Reply-To: <s6014bcc.067@ffdata.setur.fo>
Message-ID: <2E9C414912813E4EB981326983E0A10402BC48FF@inboexch.inbo.be>

Have you tried something like

df <- cbind(df, ObJeCT[, 1:10])

Cheers,

Thierry

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

 

> -----Oorspronkelijk bericht-----
> Van: r-help-bounces op stat.math.ethz.ch 
> [mailto:r-help-bounces op stat.math.ethz.ch] Namens Luis Ridao Cruz
> Verzonden: woensdag 21 maart 2007 16:14
> Aan: r-help op stat.math.ethz.ch
> Onderwerp: [R] for loop in assigning column names
> 
> R-help,
> 
> I have a data frame (df) and I want to add some columns whose 
> names should correspond to the "i" index in the loop below.
> 
>  for(i in 1:10)
>  {
> df$eval(paste("St", as.character(i), sep = "" ))  <- ObJeCt[i]  }
> 
> An error message comes out : 
> 
> "Error: attempt to apply non-function"
> 
> How can I get around this?
> 
> I could do something like :
> 
> df$St2 <- NA
> df$St3 <- NA
> dft$St4 < -NA
> ..
> 
> and afterwards assign the results of the loop above to the 
> columns df$St2,df$St3,,,,,,,,,,,
> 
> The problem is that my object "ObJeCt[i]" may change in size 
> and definition and therefore a way to systematize the task 
> would be desirable.
> 
> Thanks in advance
> 
> > version
>                _                           
> platform       i386-pc-mingw32             
> arch           i386                        
> os             mingw32                     
> system         i386, mingw32               
> status                                     
> major          2                           
> minor          4.1                         
> year           2006                        
> month          12                          
> day            18                          
> svn rev        40228                       
> language       R                           
> version.string R version 2.4.1 (2006-12-18)
> 
> ______________________________________________
> R-help op stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From maitra at iastate.edu  Wed Mar 21 16:23:50 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Wed, 21 Mar 2007 10:23:50 -0500
Subject: [R] question on suppressing error messages with Rmath library
In-Reply-To: <38b9f0350703210708v24fc84e7t9bb40534e5d5f8bb@mail.gmail.com>
References: <20070321083826.5fbd12c1@triveni.stat.iastate.edu>
	<38b9f0350703210708v24fc84e7t9bb40534e5d5f8bb@mail.gmail.com>
Message-ID: <20070321102350.3e004de3@triveni.stat.iastate.edu>

Thanks, Jim and Ronggui! This would work, of course!

But I apologize for not being clear in the first post. I am calling Rmath from a C program. I could not figure out how to set this without going back and recompiling: the message in in pnt.c and pulls it from the header file nmath.h.

Thanks,
Ranjan


On Wed, 21 Mar 2007 22:08:16 +0800 ronggui <ronggui.huang at gmail.com> wrote:

> op <- options(warn=-1)
> [main codes here]
> options(op)
> 
> 
> 
> On 3/21/07, Ranjan Maitra <maitra at iastate.edu> wrote:
> > Dear list,
> >
> > I have been using the Rmath library for quite a while: in the current instance, I am calling dnt (non-central t density function) repeatedly for several million. When the argument is small, I get the warning message:
> >
> > full precision was not achieved in 'pnt'
> >
> > which is nothing unexpected. (The density calls pnt, if you look at the function dnt.) However, to have this happen a huge number of times, when the optimizer is churning through the dataset is bothersome, but more importantly, a bottleneck in terms of speed. Is it possible to switch this off? Is there an setting somewhere that I am missing?
> >
> > Many thanks and best wishes,
> > Ranjan
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 
> -- 
> Ronggui Huang
> Department of Sociology
> Fudan University, Shanghai, China
>


From rvaradhan at jhmi.edu  Wed Mar 21 16:26:46 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Wed, 21 Mar 2007 11:26:46 -0400
Subject: [R] Gaussian Adaptive Quadrature
In-Reply-To: <40e66e0b0703210625s7c64dff1p38d4ff4b8b13ff1d@mail.gmail.com>
References: <250889.8686.qm@web56814.mail.re3.yahoo.com>
	<2323A6D37908A847A7C32F1E3662C80E8D518F@dc1ex01.air.org>
	<40e66e0b0703210625s7c64dff1p38d4ff4b8b13ff1d@mail.gmail.com>
Message-ID: <000001c76bcd$5691c550$7c94100a@win.ad.jhu.edu>

Hi,

The function "integrate" is based on QUADPACK Fortran package by Piessens.
It does indeed perform adaptive Gauss-Kronrod quadrature, where Kronrod's
method allows the re-use of abscissa from the previous iteration, thus
enabling the estimation of the quadrature error and its control.  In
contrast, in the standard Gaussian quadrature methods this is not feasible. 

The terminology "Gaussian quadrature" is not restricted to Gauss-Hermite
quadrature (where exp(-x^2) is the weight function), but applies more
broadly to Gauss-Legendre, Gauss-Laguerre, etc., where the abscissa are
chosen from Legendre, Laguerre polynomials.

Ravi. 

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Douglas Bates
Sent: Wednesday, March 21, 2007 9:25 AM
To: Doran, Harold
Cc: Help mailing list - R
Subject: Re: [R] Gaussian Adaptive Quadrature

On 3/21/07, Doran, Harold <HDoran at air.org> wrote:
> The function integrate() uses AGQ. There are other functions for
> gaussian quadrature in the statmod() package that I really like.

I think that integrate does adaptive quadrature but not adaptive
Gaussian quadrature (which probably should have been called adaptive
Gauss-Hermite quadrature to be more specific).  In the first case the
"adaptive" refers to a choice of mesh size.  In the second case one is
integrating a function that is close to a multivariate Gaussian
density by first finding the conditional optimum of the integrand and
using a quadratic approximation to the log-integrand to establish the
location of the Gauss-Hermite quadrature points.

The Laplace approximation to the log-likelihood for a generalized
linear mixed model is a 1-point adaptive Gauss-Hermite quadrature
evaluation.

>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Caio
> > Lucidius Naberezny Azevedo
> > Sent: Wednesday, March 21, 2007 5:55 AM
> > To: Help mailing list - R
> > Subject: [R] Gaussian Adaptive Quadrature
> >
> > Hi all,
> >
> >   Does anybody know any function that performs gaussian
> > adapative quadrature integration of univariate functions?
> >
> >   Thanks in advance,
> >
> >   Regards,
> >
> > Caio
> >
> >  __________________________________________________
> >
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dimitris.rizopoulos at med.kuleuven.be  Wed Mar 21 16:27:05 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 21 Mar 2007 16:27:05 +0100
Subject: [R] for loop in assigning column names
References: <s6014bcc.067@ffdata.setur.fo>
Message-ID: <007a01c76bcd$61ded650$0540210a@www.domain>

try

df.new <- cbind(df, ObJeCt[1:10])
names(df.new) <- c(names(df), paste("St", 1:10, sep = ""))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Luis Ridao Cruz" <Luisr at frs.fo>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, March 21, 2007 4:14 PM
Subject: [R] for loop in assigning column names


> R-help,
>
> I have a data frame (df) and I want to add some columns whose names
> should correspond to the "i" index in the loop below.
>
> for(i in 1:10)
> {
> df$eval(paste("St", as.character(i), sep = "" ))  <- ObJeCt[i]
> }
>
> An error message comes out :
>
> "Error: attempt to apply non-function"
>
> How can I get around this?
>
> I could do something like :
>
> df$St2 <- NA
> df$St3 <- NA
> dft$St4 < -NA
> ..
>
> and afterwards assign the results of the loop above
> to the columns df$St2,df$St3,,,,,,,,,,,
>
> The problem is that my object "ObJeCt[i]" may change in size
> and definition and therefore a way to systematize the task would
> be desirable.
>
> Thanks in advance
>
>> version
>               _
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          4.1
> year           2006
> month          12
> day            18
> svn rev        40228
> language       R
> version.string R version 2.4.1 (2006-12-18)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ripley at stats.ox.ac.uk  Wed Mar 21 17:02:41 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Mar 2007 16:02:41 +0000 (GMT)
Subject: [R] how to get "lsmeans"?
In-Reply-To: <46013317.5050402@optonline.net>
References: <DreamMail__161543_64038017385@smtp.sibs.ac.cn>
	<39B6DDB9048D0F4DAD42CB26AAFF0AFA03E64F6F@usctmx1106.merck.com>
	<46013317.5050402@optonline.net>
Message-ID: <Pine.LNX.4.64.0703211502050.24076@gannet.stats.ox.ac.uk>

On Wed, 21 Mar 2007, Chuck Cleland wrote:

> Liaw, Andy wrote:
>> I verified the result from the following with output from JMP 6 on the
>> same data (don't have SAS: don't need it):
>>
>> set.seed(631)
>> n <- 100
>> dat <- data.frame(y=rnorm(n), A=factor(sample(1:2, n, replace=TRUE)),
>>                   B=factor(sample(1:2, n, replace=TRUE)),
>>                   C=factor(sample(1:2, n, replace=TRUE)),
>>                   d=rnorm(n))
>> fm <- lm(y ~ A + B + C + d, dat)
>> ## Form a data frame of points to predict: all combinations of the
>> ## three factors and the mean of the covariate.
>> p <- data.frame(expand.grid(A=1:2, B=1:2, C=1:2))
>> p[] <- lapply(p, factor)
>> p <- cbind(p, d=mean(dat$d))
>> p <- cbind(yhat=predict(fm, p), p)
>> ## lsmeans for the three factors:
>> with(p, tapply(yhat, A, mean))
>> with(p, tapply(yhat, B, mean))
>> with(p, tapply(yhat, C, mean))
>
>  Using Andy's example data, these are the LSMEANS and intervals I get
> from SAS:
>
> A        y LSMEAN      95% Confidence Limits
> 1       -0.071847       -0.387507     0.243813
> 2       -0.029621       -0.342358     0.283117
>
> B        y LSMEAN      95% Confidence Limits
> 1       -0.104859       -0.397935     0.188216
> 2        0.003391       -0.333476     0.340258
>
> C        y LSMEAN      95% Confidence Limits
> 1       -0.084679       -0.392343     0.222986
> 2       -0.016789       -0.336374     0.302795
>
>  One way of reproducing the LSMEANS and intervals from SAS using
> predict() seems to be the following:
>
>> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d, data = dat)
>> newdat <- expand.grid(A=factor(c(1,2)),B=1.5,C=1.5,d=mean(dat$d))
>> cbind(newdat, predict(dat.lm, newdat, interval="confidence"))
>  A   B   C          d         fit        lwr       upr
> 1 1 1.5 1.5 0.09838595 -0.07184709 -0.3875070 0.2438128
> 2 2 1.5 1.5 0.09838595 -0.02962086 -0.3423582 0.2831165
>
>  However, another possibility seems to be:
>
>> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d, data = dat)
>> newdat <-
> expand.grid(A=factor(c(1,2)),B=mean(as.numeric(dat$B)),C=mean(as.numeric(dat$C)),d=mean(dat$d))
>> cbind(newdat, predict(dat.lm, newdat, interval="confidence"))
>  A    B    C          d         fit        lwr       upr
> 1 1 1.43 1.48 0.09838595 -0.08078243 -0.3964661 0.2349012
> 2 2 1.43 1.48 0.09838595 -0.03855619 -0.3479589 0.2708465
>
>  The predictions directly above match what effect() in the effects
> package by John Fox returns:
>
> library(effects)
>
>> effect("A", fm, xlevels=list(d = mean(dat$D)))
>
> A effect
> A
>          1           2
> -0.08078243 -0.03855619
>
>  But for some reason the predict() and effect() intervals are a little
> different:
>
>> effect("A", fm, xlevels=list(d = mean(dat$D)))$lower
>          [,1]
> 101 -0.3924451
> 102 -0.3440179
>
>> effect("A", fm, xlevels=list(d = mean(dat$D)))$upper
>         [,1]
> 101 0.2308802
> 102 0.2669055
>
>  I would be interested in any comments on these different approaches
> and on the difference in intervals returned by predict() and effect().

AFAIR, the effects packages uses normal-based confidence intervals 
and predict.lm uses t-based ones, and I have suggested to John Fox that 
t-based intervals would be preferable, at least as an option.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From f.harrell at vanderbilt.edu  Wed Mar 21 17:18:06 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 21 Mar 2007 11:18:06 -0500
Subject: [R] how to get "lsmeans"?
In-Reply-To: <46013317.5050402@optonline.net>
References: <DreamMail__161543_64038017385@smtp.sibs.ac.cn>	<39B6DDB9048D0F4DAD42CB26AAFF0AFA03E64F6F@usctmx1106.merck.com>
	<46013317.5050402@optonline.net>
Message-ID: <46015ABE.2060302@vanderbilt.edu>

Chuck Cleland wrote:
> Liaw, Andy wrote:
>> I verified the result from the following with output from JMP 6 on the
>> same data (don't have SAS: don't need it):
>>
>> set.seed(631)
>> n <- 100
>> dat <- data.frame(y=rnorm(n), A=factor(sample(1:2, n, replace=TRUE)),
>>                   B=factor(sample(1:2, n, replace=TRUE)),
>>                   C=factor(sample(1:2, n, replace=TRUE)),
>>                   d=rnorm(n))
>> fm <- lm(y ~ A + B + C + d, dat)
>> ## Form a data frame of points to predict: all combinations of the
>> ## three factors and the mean of the covariate.
>> p <- data.frame(expand.grid(A=1:2, B=1:2, C=1:2))
>> p[] <- lapply(p, factor)
>> p <- cbind(p, d=mean(dat$d))
>> p <- cbind(yhat=predict(fm, p), p)
>> ## lsmeans for the three factors:
>> with(p, tapply(yhat, A, mean))
>> with(p, tapply(yhat, B, mean))
>> with(p, tapply(yhat, C, mean))

And with the Design package you can do:

f <- ols(y ~ ...)
ds <- gendata(A=c('1','2'),B=c('1','2'),C=c('1','2'))
predict(f, ds)

But this sets the covariate to the median (nothing unreasonable about 
that, just will not agree with SAS).  To set to mean add d=mean(dat$d) 
in gendata.

Frank

> 
>   Using Andy's example data, these are the LSMEANS and intervals I get
> from SAS:
> 
> A        y LSMEAN      95% Confidence Limits
> 1       -0.071847       -0.387507     0.243813
> 2       -0.029621       -0.342358     0.283117
> 
> B        y LSMEAN      95% Confidence Limits
> 1       -0.104859       -0.397935     0.188216
> 2        0.003391       -0.333476     0.340258
> 
> C        y LSMEAN      95% Confidence Limits
> 1       -0.084679       -0.392343     0.222986
> 2       -0.016789       -0.336374     0.302795
> 
>   One way of reproducing the LSMEANS and intervals from SAS using
> predict() seems to be the following:
> 
>> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d, data = dat)
>> newdat <- expand.grid(A=factor(c(1,2)),B=1.5,C=1.5,d=mean(dat$d))
>> cbind(newdat, predict(dat.lm, newdat, interval="confidence"))
>   A   B   C          d         fit        lwr       upr
> 1 1 1.5 1.5 0.09838595 -0.07184709 -0.3875070 0.2438128
> 2 2 1.5 1.5 0.09838595 -0.02962086 -0.3423582 0.2831165
> 
>   However, another possibility seems to be:
> 
>> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d, data = dat)
>> newdat <-
> expand.grid(A=factor(c(1,2)),B=mean(as.numeric(dat$B)),C=mean(as.numeric(dat$C)),d=mean(dat$d))
>> cbind(newdat, predict(dat.lm, newdat, interval="confidence"))
>   A    B    C          d         fit        lwr       upr
> 1 1 1.43 1.48 0.09838595 -0.08078243 -0.3964661 0.2349012
> 2 2 1.43 1.48 0.09838595 -0.03855619 -0.3479589 0.2708465
> 
>   The predictions directly above match what effect() in the effects
> package by John Fox returns:
> 
> library(effects)
> 
>> effect("A", fm, xlevels=list(d = mean(dat$D)))
> 
>  A effect
> A
>           1           2
> -0.08078243 -0.03855619
> 
>   But for some reason the predict() and effect() intervals are a little
> different:
> 
>> effect("A", fm, xlevels=list(d = mean(dat$D)))$lower
>           [,1]
> 101 -0.3924451
> 102 -0.3440179
> 
>> effect("A", fm, xlevels=list(d = mean(dat$D)))$upper
>          [,1]
> 101 0.2308802
> 102 0.2669055
> 
>   I would be interested in any comments on these different approaches
> and on the difference in intervals returned by predict() and effect().
> 
> hope this helps,
> 
> Chuck
> 
>> Andy 
>>
>> From: Xingwang Ye
>>> Dear all, 
>>>       
>>>     I search the mail list about this topic and learn that no 
>>> simple way is available to get "lsmeans" in R as in SAS.
>>>     Dr.John Fox and Dr.Frank E Harrell have given very useful 
>>> information about "lsmeans" topic.    
>>>     Dr. Frank E Harrell suggests not to think about lsmeans, 
>>> just to think about what predicted values wanted
>>>     and to use the predict function. However, after reading 
>>> the R help file for a whole day, I am still unclear how to do it.
>>>     Could some one give me a hand? 
>>>  
>>> for example:
>>>   
>>> A,B and C are binomial variables(factors); d is a continuous 
>>> variable ; The response variable Y is  a continuous variable too.  
>>>
>>> To get lsmeans of Y according to A,B and C, respectively, in 
>>> SAS, I tried proc glm data=a;  class A B C;  model Y=A B C d; 
>>>  lsmeans A B C/cl; run;  
>>>
>>> In R, I tried this:  
>>>  library(Design)
>>>  ddist<-datadist(a)
>>>  options(datadist="ddist")
>>>  f<-ols(Y~A+B+C+D,data=a,x=TRUE,y=TRUE,se.fit=TRUE)  
>>>
>>> then how to get the "lsmeans" for A, B, and C, respectively 
>>> with predict function?
>>>
>>>  
>>>
>>> Best wishes
>>> yours, sincerely 
>>> Xingwang Ye    
>>> PhD candidate     
>>> Research Group of Nutrition Related Cancers and Other Chronic 
>>> Diseases      
>>> Institute for Nutritional Sciences,  
>>> Shanghai Institutes of Biological Sciences,     
>>> Chinese Academy of Sciences     
>>> P.O.Box 32     
>>> 294 Taiyuan Road     
>>> Shanghai 200031     
>>> P.R.CHINA
>>>
>>>
>>
>> ------------------------------------------------------------------------------
>> Notice:  This e-mail message, together with any attachments,...{{dropped}}
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From peteoutside at yahoo.com  Wed Mar 21 17:26:49 2007
From: peteoutside at yahoo.com (Pete Cap)
Date: Wed, 21 Mar 2007 09:26:49 -0700 (PDT)
Subject: [R] RMySQL *was* working...
Message-ID: <20070321162649.37854.qmail@web52412.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070321/c4c2f3ae/attachment.pl 

From jporzak at gmail.com  Wed Mar 21 17:53:45 2007
From: jporzak at gmail.com (Jim Porzak)
Date: Wed, 21 Mar 2007 09:53:45 -0700
Subject: [R] any way to append a table in SQL server
In-Reply-To: <1115a2b00703202159i1d43aa4t447f500744e69dba@mail.gmail.com>
References: <1115a2b00703202159i1d43aa4t447f500744e69dba@mail.gmail.com>
Message-ID: <2a9c000c0703210953m65db1899g6b1a6e10ab8f02ea@mail.gmail.com>

see...
> library(RODBC)
> ?sqlSave

and rest of RODBC docs. You could also pass a SQL INSERT statement
through sqlQuery()

On 3/20/07, Wensui Liu <liuwensui at gmail.com> wrote:
> Dear Lister,
> Is there an interface in R with SQL server that allows me to append
> records to table in the DB? Might I do that using RODBC?
> Thanks a lot.
>
> --
> WenSui Liu
> A lousy statistician who happens to know a little programming
> (http://spaces.msn.com/statcompute/blog)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
HTH/Best,
Jim Porzak
Loyalty Matrix Inc.
San Francisco, CA
http://www.linkedin.com/in/jimporzak


From peteoutside at yahoo.com  Wed Mar 21 17:54:47 2007
From: peteoutside at yahoo.com (Pete Cap)
Date: Wed, 21 Mar 2007 09:54:47 -0700 (PDT)
Subject: [R] RMySQL *was* working...
In-Reply-To: <20070321162649.37854.qmail@web52412.mail.re2.yahoo.com>
Message-ID: <20070321165447.64353.qmail@web52401.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070321/eeefb5dd/attachment.pl 

From maitra at iastate.edu  Wed Mar 21 18:55:51 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Wed, 21 Mar 2007 12:55:51 -0500
Subject: [R] question on suppressing error messages with Rmath library
In-Reply-To: <Pine.LNX.4.64.0703210955560.4457@itasca2.wildberry.org>
References: <20070321083826.5fbd12c1@triveni.stat.iastate.edu>
	<Pine.LNX.4.64.0703210955560.4457@itasca2.wildberry.org>
Message-ID: <20070321125551.2bec9de9@subarnarekha.stat.iastate.edu>

Hi Luke,

Thanks! Sorry, my error which I did not realize until after sending out the program. I think I will just extricate the pnt code and compile that separately and that should be fine.

Thanks very much again, to you and everybody else who replied.

Best wishes,
Ranjan


On Wed, 21 Mar 2007 10:04:05 -0500 (CDT) Luke Tierney <luke at stat.uiowa.edu> wrote:

> You might get less noise in the replies if you were explicit about
> using Rmath stand-alone and asked on r-devel.
> 
> As far as I can see you would need to compile a version of the
> stand-alone library that defines the macros for handling of warning
> messages differently -- the current one just calls printf in the
> stand-alone library.  (You might be able to trick the linker into using
> a version of printf for calls from within Rmath that does nothing, but
> I suspect recompiling the sourses is easier.)  We will probably be
> rethinking this soon in conjunction with some other changes to
> vectorized math in R.
> 
> 
> Best,
> 
> luke
> 
> On Wed, 21 Mar 2007, Ranjan Maitra wrote:
> 
> > Dear list,
> >
> > I have been using the Rmath library for quite a while: in the current instance, I am calling dnt (non-central t density function) repeatedly for several million. When the argument is small, I get the warning message:
> >
> > full precision was not achieved in 'pnt'
> >
> > which is nothing unexpected. (The density calls pnt, if you look at the function dnt.) However, to have this happen a huge number of times, when the optimizer is churning through the dataset is bothersome, but more importantly, a bottleneck in terms of speed. Is it possible to switch this off? Is there an setting somewhere that I am missing?
> >
> > Many thanks and best wishes,
> > Ranjan
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> -- 
> Luke Tierney
> Chair, Statistics and Actuarial Science
> Ralph E. Wareham Professor of Mathematical Sciences
> University of Iowa                  Phone:             319-335-3386
> Department of Statistics and        Fax:               319-335-3017
>     Actuarial Science
> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
>


From sergio.della.franca at gmail.com  Wed Mar 21 19:06:19 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Wed, 21 Mar 2007 19:06:19 +0100
Subject: [R] export table
Message-ID: <b490ce570703211106p28158848t914db33600e6fd06@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070321/e8be79d9/attachment.pl 

From ccleland at optonline.net  Wed Mar 21 19:19:30 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 21 Mar 2007 14:19:30 -0400
Subject: [R] export table
In-Reply-To: <b490ce570703211106p28158848t914db33600e6fd06@mail.gmail.com>
References: <b490ce570703211106p28158848t914db33600e6fd06@mail.gmail.com>
Message-ID: <46017732.9040206@optonline.net>

Sergio Della Franca wrote:
> Dear R-Helpers,
> 
> I have a problem.
> 
> I want to export from R to .txt my data set(y):
> 
> YEARS PRODUCTS
> 1990     10
> 1995     15
> 1997     26
> 1998     29
> 2000     34
> 
> 
> I used this code:
> 
> write.table(y,"D:/Desktop/export_table.txt").
> 
> This procedure run correctly, but i acquired this result:
> 
>  YEARS PRODUCTS
> 1           1990     10
> 2           1995     15
> 3           1997     26
> 4           1998     29
> 5           2000     34
> 
> The prolem is that R add a column in the export procedure, but it doesn't
> give a name at column then this new column get the name of the first column
> of my data set.
> 
> There is a command that a must add to my export procedure to not export this
> column or to give at this a name?

write.table(y,"D:/Desktop/export_table.txt", row.names=FALSE)

  The row.names and col.names arguments are described in the help page
for write.table().

> Thank you in advance.
> 
> 
> Sergio.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From efg at stowers-institute.org  Wed Mar 21 19:30:41 2007
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Wed, 21 Mar 2007 13:30:41 -0500
Subject: [R] Gaussian Adaptive Quadrature
References: <250889.8686.qm@web56814.mail.re3.yahoo.com><2323A6D37908A847A7C32F1E3662C80E8D518F@dc1ex01.air.org><40e66e0b0703210625s7c64dff1p38d4ff4b8b13ff1d@mail.gmail.com>
	<000001c76bcd$5691c550$7c94100a@win.ad.jhu.edu>
Message-ID: <etrtkl$7st$1@sea.gmane.org>

"Ravi Varadhan" <rvaradhan at jhmi.edu> wrote in message 
news:000001c76bcd$5691c550$7c94100a at win.ad.jhu.edu...
> The terminology "Gaussian quadrature" is not restricted to Gauss-Hermite
> quadrature (where exp(-x^2) is the weight function), but applies more
> broadly to Gauss-Legendre, Gauss-Laguerre, etc., where the abscissa are
> chosen from Legendre, Laguerre polynomials.

Also, the functional form of the integrand and limits of integration vary a 
bit with the different Gaussian Quadrature methods.  With Gaussian 
Quadrature the integral from a to b of f(x) dx is approximated as the sum of 
certain weights multiplied by the function evaluated at certain points 
related to the roots of the orthogonal polynomials.

Gauss-Legendre is perhaps the most general, since it works well with most 
functions over a fixed, finite, interval, normally [-1,1].  The Legendre 
polynomials are orthogonal on the interval [-1,1] with respect to a 
weighting function w(x) = 1.  A simple transformation allows the integration 
interval to be any finite interval, [a,b].

Gauss-Laguerre assumes a weighting function w(x) = exp(-x) in the integrand, 
and an interval of integration from 0 to infinity.

Gauss-Hermite assumes a weighting function w(x) = exp(-x^2) in the 
integrand, and an interval of integration from -infinity to infinity.

Applied Numerical Methods by Carnahan et al provides good details and 
examples (but in FORTRAN).


One "adaptive" approach that can be used with Gaussian Quadrature is to use 
a different number of  terms to evaluate the integral.  To save computation 
time, you can use fewer terms.  This file gives the weights needed for 
various N-point Gauss-Legendre quadrature approximation:
http://www.math.ntnu.no/num/nnm/Program/Numlibc/gauss_co.c

Some years ago on a project we found that 2-point Gaussian Quadrature gave 
us an answer that was "good enough," and obviously was quite fast with so 
few function evaluations.  For your problem you might try 2-point to 
15-point quadrature to see if you get the desired accuracy.   I've always 
used pre-computed polynomial roots and weights for the various N-point 
formulas.  I'm not sure how gauss.quad in library(statmod) gets these 
values. It wasn't obvious to me from a quick look at the source code.

Another  "adaptive" Gaussian approach might break a single integral up into 
a number of other integrals.  One could even use different N-point formulas 
over different intervals, using lower N for "smoother" areas, and larger N 
if a function wasn't so well-behaved.

Some other good links:

Gauss-Legendre Quadrature
http://math.fullerton.edu/mathews/n2003/gaussianquad/GaussianQuadBib/Links/GaussianQuadBib_lnk_1.html
http://mathworld.wolfram.com/Legendre-GaussQuadrature.html

efg

Earl F. Glynn
Scientific Programmer
Stowers Institute for Medical Research


From mike.prager at noaa.gov  Wed Mar 21 19:40:55 2007
From: mike.prager at noaa.gov (Mike Prager)
Date: Wed, 21 Mar 2007 14:40:55 -0400
Subject: [R] Ticks on barplots
References: <46005A57.2000709@noaa.gov>
	<1174444859.5415.57.camel@localhost.localdomain>
Message-ID: <b5t20314vt79f5g91d22rabhpkt0oljgra@4ax.com>

Marc Schwartz <marc_schwartz at comcast.net> wrote:

> On Tue, 2007-03-20 at 18:04 -0400, Michael H. Prager wrote:
> > I am generating stacked barplots of age-composition of fish populations 
> > (Y) over time (X).  As there are many years, not every bars is labeled.  
> > When looking at the plot, it becomes difficult to associate labels with 
> > their bars.
> > 
> > We have improved this a bit by using axis() to add a tickmark below each 
> > bar.  Can anyone suggest a way to draw ticks ONLY at bars where a tick 
> > label is drawn?  Or to make such ticks longer than those where there is 
> > no label?
> > 
> > This is going into a function, so I'm hoping for a method that doesn't 
> > require looking at the plot first.
> >
> > # sample code (simplified) #
> > mp <- barplot(t(N.age), xlab = "Year", axisnames = FALSE)
> > axis(side = 1, at = mp, labels = rownames(N.age), tcl = -0.75)
> > 
> > Thanks!
> > 
> > Mike Prager
> > NOAA, Beaufort, NC
> 
> Mike,
> 
> How about something like this:
> 
>   mp <- barplot(1:50, axisnames = FALSE)
> 
>   # Create short tick marks at each bar
>   axis(1, at = mp, labels = rep("", 50), tcl = -0.25)
> 
>   # Create longer tick marks every 5 years with labels
>   axis(1, at = mp[seq(1, 50, 5)], 
>        labels = 1900 + seq(0, 45, 5), tcl = -0.75, las = 2, 
>        cex.axis = 0.75)
> 
> 
> Just pick which labels you want to be shown (eg. every 5 years) and
> synchronize the values of those with the 'at' argument in axis().
> 
> HTH,
> 
> Marc Schwartz
> 

Thanks, Marc, for this solution and thanks equally to Jim Lemon
for a similar idea.  This seems promising.  Since this is to go
into a function (and should work without intervention), I'll
need to devise an algorithm to decide at what interval the
labels should be plotted.  Clearly "axis()" has such an
algorithm.  Unfortunately, it reports its result only by placing
the labels.

Mike

-- 
Mike Prager, NOAA, Beaufort, NC
* Opinions expressed are personal and not represented otherwise.
* Any use of tradenames does not constitute a NOAA endorsement.


From sergio.della.franca at gmail.com  Wed Mar 21 19:42:18 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Wed, 21 Mar 2007 19:42:18 +0100
Subject: [R] Stepwise Logistic Regression
Message-ID: <b490ce570703211142w2e1babbeh8e619c839d65c059@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070321/1c8f6e74/attachment.pl 

From dfarrar at newrvana.com  Wed Mar 21 19:48:14 2007
From: dfarrar at newrvana.com (David Farrar)
Date: Wed, 21 Mar 2007 11:48:14 -0700 (PDT)
Subject: [R] Stepwise Logistic Regression
In-Reply-To: <b490ce570703211142w2e1babbeh8e619c839d65c059@mail.gmail.com>
Message-ID: <20070321184814.79190.qmail@web812.biz.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070321/423e60a3/attachment.pl 

From f.harrell at vanderbilt.edu  Wed Mar 21 19:51:31 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 21 Mar 2007 13:51:31 -0500
Subject: [R] Stepwise Logistic Regression
In-Reply-To: <b490ce570703211142w2e1babbeh8e619c839d65c059@mail.gmail.com>
References: <b490ce570703211142w2e1babbeh8e619c839d65c059@mail.gmail.com>
Message-ID: <46017EB3.6050001@vanderbilt.edu>

Sergio Della Franca wrote:
> Dear R-Helpers,
> 
> I'd like to perform a Logistic Regression whit a Stepwise method.
> 
> 
> Can you tell me how can i proceed to develop this procedure?
> 
> 
> Thank you in advance.
> 
> 
> Sergio Della Franca.

If the number of events is not incredibly large, you can get almost the 
same result with the following code :-)

candidates <- setdiff(names(mydataframe), 'y')
p <- length(candidates)
sample(candidates, sample(round(p/4):round(p/2), 1))

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From czucz at botanika.hu  Wed Mar 21 19:54:28 2007
From: czucz at botanika.hu (=?ISO-8859-1?Q?B=E1lint_Cz=FAcz?=)
Date: Wed, 21 Mar 2007 19:54:28 +0100
Subject: [R] Implementing trees in R
In-Reply-To: <971536df0703160706p2e462810jd070966ea79980ad@mail.gmail.com>
References: <45FA9D64.6070708@yale.edu>
	<971536df0703160659u1c339e53k474b962bd846d76e@mail.gmail.com>
	<971536df0703160706p2e462810jd070966ea79980ad@mail.gmail.com>
Message-ID: <fab4bcf70703211154n2342a3f9r12e91db3f2c53dbf@mail.gmail.com>

Dear Kevin,

for an example how this idea is implemented in a working package, have
a look at the help page of BinaryTree class in package party. The
@tree node of such an object is a recursive list of this kind.

HTH,
B?lint


On 3/16/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Let me rephrase that.  Lists do not support references but they
> could be used to represent trees.
>
> list(a = list(a = 1, b = list(2, 3, d = list(4, 5)), c = list(4, 5))
>
> is a tree whose top nodes are a, b, c and b contains three nodes
> 2, 3 and d and d contains 2 nodes.
>
> However, if you want to do it via references as requested then lists
> are not appropriate.
>
> On 3/16/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > Lists are not good for this.  There is an example in section 3.3 of
> > the proto vignette of using proto objects for this.  That section
> > also references an S4 example although its pretty messy with S4.
> >
> > You might want to look at the graph, RBGL and graphviz packages
> > in Bioconductor and the dynamicgraph, mathgraph and sna packages
> > on CRAN.
> >
> > On 3/16/07, Yuk Lap Yip (Kevin) <yuklap.yip at yale.edu> wrote:
> > > Hi all,
> > >
> > >    I am rather new to R. Recently I have been trying to implement some
> > > tree algorithms in R. I used lists to model tree nodes. I thought
> > > something like this would work:
> > >
> > >    parent <- list();
> > >    child <- list();
> > >    parent$child1 <- child;
> > >    child$parent <- parent;
> > >
> > >    When I tried to check whether a node is its parent's first child
> > > using "if (node$parent$child1 == node)", it always returned false. Then
> > > I realized that it does not really work because "parent$child1 <- child"
> > > actually makes a copy of child instead of referencing it. I think one
> > > possible fix is to keep a list of node objects, and make references
> > > using the positions in the list. For example, I think the following
> > > would work:
> > >
> > >    parent <- list();
> > >    child <- list();
> > >    nodes <- list(parent, child);
> > >    parent$child1 <- 2;
> > >    child$parent <- 1;
> > >
> > >    Then the "first child" test can be rewritten as "if
> > > (nodes[[nodes[[nodeId]]$parent]]$child1 == nodeId)". However, I would
> > > prefer not to implement trees in this way, as it requires the
> > > inconvenient and error-prone manipulations of node IDs.
> > >
> > >    May I know if there is a way to make object references to lists? Or
> > > are there other ways to implement tree data structures in R?
> > >
> > >    BTW, I checked how hclust was implemented, and noticed that it calls
> > > an external Fortran program. I would want a solution not involving any
> > > external programs.
> > >
> > >    Thanks.
> > >
> > > --
> > >
> > >
> > >        God bless.
> > >
> > >        Kevin
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sfalcon at fhcrc.org  Wed Mar 21 20:09:30 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Wed, 21 Mar 2007 12:09:30 -0700
Subject: [R] RMySQL *was* working...
In-Reply-To: <20070321162649.37854.qmail@web52412.mail.re2.yahoo.com> (Pete
	Cap's message of "Wed, 21 Mar 2007 09:26:49 -0700 (PDT)")
References: <20070321162649.37854.qmail@web52412.mail.re2.yahoo.com>
Message-ID: <m2fy7yct1x.fsf@ziti.fhcrc.org>

Pete Cap <peteoutside at yahoo.com> writes:

> List,
>
> Last week with the help of Uwe and some other folks I was able to get
> RMySQL 0.5-7 compiled against R 2.4.1 and MySQL 5.0.27.  It was
> working fine--I was able to send select queries to the db, put the
> results in a data frame, and so forth.
>
> Today, dbDriver() threw an error:
>
>> dbDriver(MySQL) Error in function (classes, fdef, mtable) : unable
> to find an inherited method for function "dbDriver", for signature
> "function"

Do you mean dbDriver("MySQL")?

+ seth

-- 
Seth Falcon | Computational Biology | Fred Hutchinson Cancer Research Center
http://bioconductor.org


From marc_schwartz at comcast.net  Wed Mar 21 20:23:12 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 21 Mar 2007 14:23:12 -0500
Subject: [R] Ticks on barplots
In-Reply-To: <b5t20314vt79f5g91d22rabhpkt0oljgra@4ax.com>
References: <46005A57.2000709@noaa.gov>
	<1174444859.5415.57.camel@localhost.localdomain>
	<b5t20314vt79f5g91d22rabhpkt0oljgra@4ax.com>
Message-ID: <1174504992.7212.42.camel@localhost.localdomain>

On Wed, 2007-03-21 at 14:40 -0400, Mike Prager wrote:
> Marc Schwartz <marc_schwartz at comcast.net> wrote:
> 
> > On Tue, 2007-03-20 at 18:04 -0400, Michael H. Prager wrote:
> > > I am generating stacked barplots of age-composition of fish populations 
> > > (Y) over time (X).  As there are many years, not every bars is labeled.  
> > > When looking at the plot, it becomes difficult to associate labels with 
> > > their bars.
> > > 
> > > We have improved this a bit by using axis() to add a tickmark below each 
> > > bar.  Can anyone suggest a way to draw ticks ONLY at bars where a tick 
> > > label is drawn?  Or to make such ticks longer than those where there is 
> > > no label?
> > > 
> > > This is going into a function, so I'm hoping for a method that doesn't 
> > > require looking at the plot first.
> > >
> > > # sample code (simplified) #
> > > mp <- barplot(t(N.age), xlab = "Year", axisnames = FALSE)
> > > axis(side = 1, at = mp, labels = rownames(N.age), tcl = -0.75)
> > > 
> > > Thanks!
> > > 
> > > Mike Prager
> > > NOAA, Beaufort, NC
> > 
> > Mike,
> > 
> > How about something like this:
> > 
> >   mp <- barplot(1:50, axisnames = FALSE)
> > 
> >   # Create short tick marks at each bar
> >   axis(1, at = mp, labels = rep("", 50), tcl = -0.25)
> > 
> >   # Create longer tick marks every 5 years with labels
> >   axis(1, at = mp[seq(1, 50, 5)], 
> >        labels = 1900 + seq(0, 45, 5), tcl = -0.75, las = 2, 
> >        cex.axis = 0.75)
> > 
> > 
> > Just pick which labels you want to be shown (eg. every 5 years) and
> > synchronize the values of those with the 'at' argument in axis().
> > 
> > HTH,
> > 
> > Marc Schwartz
> > 
> 
> Thanks, Marc, for this solution and thanks equally to Jim Lemon
> for a similar idea.  This seems promising.  Since this is to go
> into a function (and should work without intervention), I'll
> need to devise an algorithm to decide at what interval the
> labels should be plotted.  Clearly "axis()" has such an
> algorithm.  Unfortunately, it reports its result only by placing
> the labels.
> 
> Mike


Mike,

To get a feel for how axis() creates the default tick positions when
'at' is the default NULL, see ?axTicks, which provides functionality
similar to the internal C routine.

You could also look at ?pretty

HTH,

Marc


From apjaworski at mmm.com  Wed Mar 21 20:25:17 2007
From: apjaworski at mmm.com (apjaworski at mmm.com)
Date: Wed, 21 Mar 2007 14:25:17 -0500
Subject: [R] Integer Optimization
In-Reply-To: <46011A19.5060002@student.uclouvain.be>
Message-ID: <OFDFBC8DEF.1F116AF1-ON862572A5.006A77C6-862572A5.006AAFA1@mmm.com>

Dimitri,

I am assuming you mean integer linear optimization problem.  Try lpSolve.
I think you will find it much easier to use.

Cheers,

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


                                                                           
             Mahieux Dimitri                                               
             <dimitri.mahieux@                                             
             student.uclouvain                                          To 
             .be>                      r-help at stat.math.ethz.ch            
             Sent by:                                                   cc 
             r-help-bounces at st                                             
             at.math.ethz.ch                                       Subject 
                                       [R] Integer Optimization            
                                                                           
             03/21/2007 06:42                                              
             AM                                                            
                                                                           
                                                                           
                                                                           




Hello everybody,

I am looking for a function (or package) which can solve a integer
optimization problem.  I have found the glpk package but I don't known
very well how to use it.
Someone has another solution ?

thx

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From paulgottemoller at gmail.com  Wed Mar 21 21:34:23 2007
From: paulgottemoller at gmail.com (Paul Gottemoller)
Date: Wed, 21 Mar 2007 15:34:23 -0500
Subject: [R] Problem installing packages in R 2.4.1
Message-ID: <8c1388b50703211334y14b817b4l91a82c845e253ace@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070321/ecdd61ae/attachment.pl 

From Cody_Hamilton at Edwards.com  Wed Mar 21 22:19:32 2007
From: Cody_Hamilton at Edwards.com (Cody_Hamilton at Edwards.com)
Date: Wed, 21 Mar 2007 14:19:32 -0700
Subject: [R] Stepwise Logistic Regression
In-Reply-To: <b490ce570703211142w2e1babbeh8e619c839d65c059@mail.gmail.com>
Message-ID: <OF3CDADE7D.FA560F91-ON882572A5.0074266A-882572A5.0074EC0C@irvine.edwards.com>


Rich Ulrich has compiled some posts (I believe from the S list) on some of
the dangers of stepwise regression:
http://www.pitt.edu/~wpilib/statfaq/regrfaq.html

Regards,
    -Cody



                                                                           
             "Sergio Della                                                 
             Franca"                                                       
             <sergio.della.fra                                          To 
             nca at gmail.com>            r-help at stat.math.ethz.ch            
             Sent by:                                                   cc 
             r-help-bounces at st                                             
             at.math.ethz.ch                                       Subject 
                                       [R] Stepwise Logistic Regression    
                                                                           
             03/21/2007 11:42                                              
             AM                                                            
                                                                           
                                                                           
                                                                           




Dear R-Helpers,

I'd like to perform a Logistic Regression whit a Stepwise method.


Can you tell me how can i proceed to develop this procedure?


Thank you in advance.


Sergio Della Franca.

             [[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Cody_Hamilton at Edwards.com  Wed Mar 21 22:19:32 2007
From: Cody_Hamilton at Edwards.com (Cody_Hamilton at Edwards.com)
Date: Wed, 21 Mar 2007 14:19:32 -0700
Subject: [R] Stepwise Logistic Regression
In-Reply-To: <b490ce570703211142w2e1babbeh8e619c839d65c059@mail.gmail.com>
Message-ID: <OF3CDADE7D.FA560F91-ON882572A5.0074266A-882572A5.0074EC0C@irvine.edwards.com>


Rich Ulrich has compiled some posts (I believe from the S list) on some of
the dangers of stepwise regression:
http://www.pitt.edu/~wpilib/statfaq/regrfaq.html

Regards,
    -Cody



                                                                           
             "Sergio Della                                                 
             Franca"                                                       
             <sergio.della.fra                                          To 
             nca at gmail.com>            r-help at stat.math.ethz.ch            
             Sent by:                                                   cc 
             r-help-bounces at st                                             
             at.math.ethz.ch                                       Subject 
                                       [R] Stepwise Logistic Regression    
                                                                           
             03/21/2007 11:42                                              
             AM                                                            
                                                                           
                                                                           
                                                                           




Dear R-Helpers,

I'd like to perform a Logistic Regression whit a Stepwise method.


Can you tell me how can i proceed to develop this procedure?


Thank you in advance.


Sergio Della Franca.

             [[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From robert.robinson at maine.edu  Wed Mar 21 21:55:20 2007
From: robert.robinson at maine.edu (robert.robinson at maine.edu)
Date: Wed, 21 Mar 2007 20:55:20 +0000
Subject: [R] Snow Package and SPRNG:  Will it solve my problem?
Message-ID: <20070321205520.c7d3ekiio4skwk00@mail1.maine.edu>

Hello and thanks in advance for your time.  I currently have a  
simulation running on my cluster with the help of snow that relies on  
global variables being changes regularly  to random values.  It uses  
these values, lets call them x1 x2 and x3, in custom functions for  
logliklyhood and score that gets used in the standard optim function.   
To get set in the global table on the different nodes I'm generating  
the random value in a function on the node and then using the  
superassign operator ( <<- ) to set it to the global variable.
(eg:
    temp = rand(n1)
    x1 <<- sort(temp)
)

) I'm worried that this is creating a lot of avoidable message  
passing.  Here are my questions:

Does the superassign operator set the global variable on the head  
node, like I believe it does, or rather does it only set it on the  
local global table?

Does the SPRNG package offer a viable replacement for useless message  
passing of random values like this?

Thanks again for your continued help with my problems.


From exonintron at gmail.com  Wed Mar 21 22:34:20 2007
From: exonintron at gmail.com (Sender)
Date: Wed, 21 Mar 2007 14:34:20 -0700
Subject: [R] Clarification
Message-ID: <686bf0c50703211434r6ba21914t1a14dc12a657a3f1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070321/37228da5/attachment.pl 

From mwkimpel at gmail.com  Wed Mar 21 23:09:34 2007
From: mwkimpel at gmail.com (Mark W Kimpel)
Date: Wed, 21 Mar 2007 18:09:34 -0400
Subject: [R] problem with RCurl install on Unix
Message-ID: <4601AD1E.30408@gmail.com>

I am having trouble getting an install of RCurl to work properly on a 
Unix server. The steps I have taken are:

1. installed cUrl from source without difficulty
2. installed RCurl from source using the command
	~/R_HOME/R-devel/bin/R CMD INSTALL -l ~/R_HOME/R-devel/library 
~/RCurl_0.8-0.tar.gz     I received no errors during this install
3. when I go back to R and require(RCurl), I get

 > require(RCurl)
Loading required package: RCurl
Error in dyn.load(x, as.logical(local), as.logical(now)) :
         unable to load shared library 
'/N/u/mkimpel/BigRed/R_HOME/R-devel/library/RCurl/libs/RCurl.so':
   libcurl.so.4: cannot open shared object file: No such file or directory
?1? FALSE

Outside of R I get

mkimpel?BigRed:?/R_HOME/R-devel/library/RCurl/libs> ls
RCurl.so

I can cat into RCurl and I have even done chmod a+x RCurl.so in case 
there was a problem with permission for R to open the file.

Below is my sessionInfo. Thanks, Mark

 > sessionInfo()
R version 2.5.0 Under development (unstable) (2007-03-11 r40824)
powerpc64-unknown-linux-gnu

locale:
LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C

attached base packages:
?1? "stats"     "graphics"  "grDevices" "datasets"  "utils"     "tools"
?7? "methods"   "base"

other attached packages:
     limma      affy    affyio   Biobase
  "2.9.13" "1.13.14"   "1.3.3" "1.13.39"
 >



-- 
Mark W. Kimpel MD
Neuroinformatics
Department of Psychiatry
Indiana University School of Medicine


From sinclair.jesse at gmail.com  Thu Mar 22 02:14:49 2007
From: sinclair.jesse at gmail.com (Jesse Sinclair)
Date: Wed, 21 Mar 2007 18:14:49 -0700
Subject: [R] Labelling a second y-axis
Message-ID: <A925349F-BA64-4FB8-81AD-664D3E9A085A@gmail.com>

Hi,
I am using the following code as an example function to create  
multiple y-axes on one plot. I have it working fine however, I can't  
seem to add a label on the second (right) axis. I have tried adding  
ylab="y2" in the axis call but, that didn't work; any ideas?

Thanks,

Jesse

Code:
function() {
	par(las=1,xaxs="r",mai=c(1,0.75,1,1))
	x<-1:10
	y1<-x
	y2<-x^2
	plot(x,y1,xlim=c(0,10),ylim=c(0,10),ylab="y1")
	title(main="Multiple Y-Axes in R",ylab="y2")
	par(new=TRUE)
	plot(x,y2,xlim=c(0,10),xaxt="n",yaxt="n",ylab="",pch=16)
	axis(4,at=c(0,20,40,60,80,100))
}


From marc_schwartz at comcast.net  Thu Mar 22 03:09:02 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 21 Mar 2007 21:09:02 -0500
Subject: [R] Labelling a second y-axis
In-Reply-To: <A925349F-BA64-4FB8-81AD-664D3E9A085A@gmail.com>
References: <A925349F-BA64-4FB8-81AD-664D3E9A085A@gmail.com>
Message-ID: <1174529342.8712.12.camel@localhost.localdomain>

On Wed, 2007-03-21 at 18:14 -0700, Jesse Sinclair wrote:
> Hi,
> I am using the following code as an example function to create  
> multiple y-axes on one plot. I have it working fine however, I can't  
> seem to add a label on the second (right) axis. I have tried adding  
> ylab="y2" in the axis call but, that didn't work; any ideas?
> 
> Thanks,
> 
> Jesse
> 
> Code:
> function() {
> 	par(las=1,xaxs="r",mai=c(1,0.75,1,1))
> 	x<-1:10
> 	y1<-x
> 	y2<-x^2
> 	plot(x,y1,xlim=c(0,10),ylim=c(0,10),ylab="y1")
> 	title(main="Multiple Y-Axes in R",ylab="y2")
> 	par(new=TRUE)
> 	plot(x,y2,xlim=c(0,10),xaxt="n",yaxt="n",ylab="",pch=16)
> 	axis(4,at=c(0,20,40,60,80,100))
> }

Normally, you would use mtext() to place text outside the plot region.

The problem is that mtext() does not support rotated text. Thus, we need
to use text() and adjust the 'srt' argument to rotate the text and
adjust the x and y axis values for placement. We also set the 'xpd'
argument so that the text is not clipped at the plot region.

        par(las=1,xaxs="r",mai=c(1,0.75,1,1))
        x<-1:10
        y1<-x
        y2<-x^2
        plot(x,y1,xlim=c(0,10),ylim=c(0,10), ylab="y1", las = 1)
        title(main="Multiple Y-Axes in R")

        par(new=TRUE)
        plot(x,y2,xlim=c(0,10),xaxt="n",yaxt="n",ylab="",pch=16)
        axis(4,at=c(0,20,40,60,80,100))

        text(12, 50, "y2", srt = 270, xpd = TRUE)

See ?text, ?par and ?mtext for more information.

HTH,

Marc Schwartz


From smckinney at bccrc.ca  Thu Mar 22 03:29:43 2007
From: smckinney at bccrc.ca (Steven McKinney)
Date: Wed, 21 Mar 2007 19:29:43 -0700
Subject: [R] problem with RCurl install on Unix
References: <4601AD1E.30408@gmail.com>
Message-ID: <0BE438149FF2254DB4199E2682C8DFEB0235FB7B@crcmail1.BCCRC.CA>

I get the same problem, and haven't
figured it out yet.

Is it a 32bit/64bit clash?

(Similarly, I don't have RMySQL up and running cleanly.)




> library(RCurl)
Error in dyn.load(x, as.logical(local), as.logical(now)) : 
	unable to load shared library '/share/apps/R/R-2.4.1/library/RCurl/libs/RCurl.so':
  libcurl.so.4: cannot open shared object file: No such file or directory
Error: package/namespace load failed for 'RCurl'


> sessionInfo()
R version 2.4.1 (2006-12-18) 
x86_64-unknown-linux-gnu 

locale:
LC_CTYPE=en_US.iso885915;LC_NUMERIC=C;LC_TIME=en_US.iso885915;LC_COLLATE=en_US.iso885915;LC_MONETARY=en_US.iso885915;LC_MESSAGES=en_US.iso885915;LC_PAPER=en_US.iso885915;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.iso885915;LC_IDENTIFICATION=C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"  
[7] "base"     

other attached packages:
     DBI 
"0.1-12" 
> 

Steven McKinney

Statistician
Molecular Oncology and Breast Cancer Program
British Columbia Cancer Research Centre

email: smckinney at bccrc.ca

tel: 604-675-8000 x7561

BCCRC
Molecular Oncology
675 West 10th Ave, Floor 4
Vancouver B.C. 
V5Z 1L3
Canada




-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch on behalf of Mark W Kimpel
Sent: Wed 3/21/2007 3:09 PM
To: r-help at stat.math.ethz.ch
Subject: [R] problem with RCurl install on Unix
 
I am having trouble getting an install of RCurl to work properly on a 
Unix server. The steps I have taken are:

1. installed cUrl from source without difficulty
2. installed RCurl from source using the command
	~/R_HOME/R-devel/bin/R CMD INSTALL -l ~/R_HOME/R-devel/library 
~/RCurl_0.8-0.tar.gz     I received no errors during this install
3. when I go back to R and require(RCurl), I get

 > require(RCurl)
Loading required package: RCurl
Error in dyn.load(x, as.logical(local), as.logical(now)) :
         unable to load shared library 
'/N/u/mkimpel/BigRed/R_HOME/R-devel/library/RCurl/libs/RCurl.so':
   libcurl.so.4: cannot open shared object file: No such file or directory
?1? FALSE

Outside of R I get

mkimpel?BigRed:?/R_HOME/R-devel/library/RCurl/libs> ls
RCurl.so

I can cat into RCurl and I have even done chmod a+x RCurl.so in case 
there was a problem with permission for R to open the file.

Below is my sessionInfo. Thanks, Mark

 > sessionInfo()
R version 2.5.0 Under development (unstable) (2007-03-11 r40824)
powerpc64-unknown-linux-gnu

locale:
LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C

attached base packages:
?1? "stats"     "graphics"  "grDevices" "datasets"  "utils"     "tools"
?7? "methods"   "base"

other attached packages:
     limma      affy    affyio   Biobase
  "2.9.13" "1.13.14"   "1.3.3" "1.13.39"
 >



-- 
Mark W. Kimpel MD
Neuroinformatics
Department of Psychiatry
Indiana University School of Medicine

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From johannh at gmail.com  Thu Mar 22 04:58:01 2007
From: johannh at gmail.com (Johann Hibschman)
Date: Wed, 21 Mar 2007 23:58:01 -0400
Subject: [R] dynamic linear models in R
Message-ID: <4b82d65b0703212058na81fd62uc5c49f5cec51962f@mail.gmail.com>

Hi all,

I've just started working my way through Mike West and Jeff Harrison's
_Bayesian Forecasting and Dynamic Models_, and I was wondering if
there were any publically-available packages to handle dynamic linear
models, as they describe.

I found the "dynlm" package, but either I don't yet understand what's
going on or that package uses a different sense of the phrase "dynamic
linear model."  I would expect a fit of a dynamic linear model to
produce a time series of parameter estimates, not just single
coefficients as that function seems to generate.

Could anyone help me understand what's going on here?

Thanks,

Johann

P.S. What I really want to do is fit a linear regression of the form
dz_t ~ 0 + dx_t + dy_t, but where the coefficients of dx and dy are
allowed to slowly evolve over time.  DLMs seem appropriate to this,
but I'm open to any other suggestions, as I've not found much support
for DLMs in R.


From 2bingho at stanford.edu  Thu Mar 22 06:08:38 2007
From: 2bingho at stanford.edu (Bing Ho)
Date: Wed, 21 Mar 2007 22:08:38 -0700
Subject: [R] DBI + RSQLite or SQLiteDF?
Message-ID: <46020F56.5040903@stanford.edu>

Hello,

I've finally reached the wall - my 2gb RAM machine simply can't handle
the datasets that I am working with. I've tried a 64-bit compile of R on
a 8gb RAM machine but that's not always available to use.

Now there are several proposed ways around this, but it seems the most
general solution is to leverage a SQL database to manage large datasets.

I thought I'd ask around to see what is the best approach before I went
off and expended a bunch of time being unable to get anything to work. I
have no experience with SQL or database administration.

My dataframes are in the 100,000 x 10,000 range (at the most) with a mix
of numerical, factor, and character data.

What works best right now - DBI + RSQLite, or would SQLiteDF be better,
for somebody with basically NO experience?

Thank you,
Bing


From brown_emu at yahoo.com  Thu Mar 22 06:22:31 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Wed, 21 Mar 2007 22:22:31 -0700 (PDT)
Subject: [R] Ticks on barplots
In-Reply-To: <b5t20314vt79f5g91d22rabhpkt0oljgra@4ax.com>
Message-ID: <185982.72386.qm@web39701.mail.mud.yahoo.com>

Hi Mike, you can try using axTicks as in this example (you can also use
pretty instead).

Below, instead of a barplot() I have used plot() but with type="h" and lend=3
(see ?par for details on lend) which I have found to be useful at times when
making barchart-like plots with time-series. In any case, barplot() or
plot(), I hope this illustration will be helpful:


# define function to convert numeric to POSIXct
# from http://tolstoy.newcastle.edu.au/R/help/04/05/0980.html
numToPOSIXct <- function(v) {
  now <- Sys.time()
  Epoch <- now - as.numeric(now)
  Epoch + v
}

# define example data
x <- seq(as.POSIXct("1950-01-01 00:00:00"),as.POSIXct("2000-01-01 
     00:00:00"),by="year")
y <- runif(length(x))

# plot and label axis with with axTicks()
plot(x,y,type="h",lwd=3,col=8,xaxt="n",lend=3)
axis(1,at=axTicks(1),lab=format(numToPOSIXct(axTicks(1)),"%Y"))



--- Mike Prager <mike.prager at noaa.gov> wrote:

> Marc Schwartz <marc_schwartz at comcast.net> wrote:
> 
> > On Tue, 2007-03-20 at 18:04 -0400, Michael H. Prager wrote:
> > > I am generating stacked barplots of age-composition of fish populations
> 
> > > (Y) over time (X).  As there are many years, not every bars is labeled.
>  
> > > When looking at the plot, it becomes difficult to associate labels with
> 
> > > their bars.
> > > 
> > > We have improved this a bit by using axis() to add a tickmark below
> each 
> > > bar.  Can anyone suggest a way to draw ticks ONLY at bars where a tick 
> > > label is drawn?  Or to make such ticks longer than those where there is
> 
> > > no label?
> > > 
> > > This is going into a function, so I'm hoping for a method that doesn't 
> > > require looking at the plot first.
> > >
> > > # sample code (simplified) #
> > > mp <- barplot(t(N.age), xlab = "Year", axisnames = FALSE)
> > > axis(side = 1, at = mp, labels = rownames(N.age), tcl = -0.75)
> > > 
> > > Thanks!
> > > 
> > > Mike Prager
> > > NOAA, Beaufort, NC
> > 
> > Mike,
> > 
> > How about something like this:
> > 
> >   mp <- barplot(1:50, axisnames = FALSE)
> > 
> >   # Create short tick marks at each bar
> >   axis(1, at = mp, labels = rep("", 50), tcl = -0.25)
> > 
> >   # Create longer tick marks every 5 years with labels
> >   axis(1, at = mp[seq(1, 50, 5)], 
> >        labels = 1900 + seq(0, 45, 5), tcl = -0.75, las = 2, 
> >        cex.axis = 0.75)
> > 
> > 
> > Just pick which labels you want to be shown (eg. every 5 years) and
> > synchronize the values of those with the 'at' argument in axis().
> > 
> > HTH,
> > 
> > Marc Schwartz
> > 
> 
> Thanks, Marc, for this solution and thanks equally to Jim Lemon
> for a similar idea.  This seems promising.  Since this is to go
> into a function (and should work without intervention), I'll
> need to devise an algorithm to decide at what interval the
> labels should be plotted.  Clearly "axis()" has such an
> algorithm.  Unfortunately, it reports its result only by placing
> the labels.
> 
> Mike
> 
> -- 
> Mike Prager, NOAA, Beaufort, NC
> * Opinions expressed are personal and not represented otherwise.
> * Any use of tradenames does not constitute a NOAA endorsement.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



 
____________________________________________________________________________________
Expecting? Get great news right away with email Auto-Check.


From brown_emu at yahoo.com  Thu Mar 22 06:26:28 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Wed, 21 Mar 2007 22:26:28 -0700 (PDT)
Subject: [R] export table
In-Reply-To: <46017732.9040206@optonline.net>
Message-ID: <790488.28766.qm@web39702.mail.mud.yahoo.com>

You can also use 

write.table(y,"D:/Desktop/export_table.txt", col.names=NA)

with this function call, the row names get printed but the column names are
offset so that they are aligned in the right column.



--- Chuck Cleland <ccleland at optonline.net> wrote:

> Sergio Della Franca wrote:
> > Dear R-Helpers,
> > 
> > I have a problem.
> > 
> > I want to export from R to .txt my data set(y):
> > 
> > YEARS PRODUCTS
> > 1990     10
> > 1995     15
> > 1997     26
> > 1998     29
> > 2000     34
> > 
> > 
> > I used this code:
> > 
> > write.table(y,"D:/Desktop/export_table.txt").
> > 
> > This procedure run correctly, but i acquired this result:
> > 
> >  YEARS PRODUCTS
> > 1           1990     10
> > 2           1995     15
> > 3           1997     26
> > 4           1998     29
> > 5           2000     34
> > 
> > The prolem is that R add a column in the export procedure, but it doesn't
> > give a name at column then this new column get the name of the first
> column
> > of my data set.
> > 
> > There is a command that a must add to my export procedure to not export
> this
> > column or to give at this a name?
> 
> write.table(y,"D:/Desktop/export_table.txt", row.names=FALSE)
> 
>   The row.names and col.names arguments are described in the help page
> for write.table().
> 
> > Thank you in advance.
> > 
> > 
> > Sergio.
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 512-0171 (M, W, F)
> fax: (917) 438-0894
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 




 
____________________________________________________________________________________
Need Mail bonding?


From brown_emu at yahoo.com  Thu Mar 22 06:27:06 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Wed, 21 Mar 2007 22:27:06 -0700 (PDT)
Subject: [R] export table
In-Reply-To: <46017732.9040206@optonline.net>
Message-ID: <20070322052706.12152.qmail@web39710.mail.mud.yahoo.com>

You can also use 

write.table(y,"D:/Desktop/export_table.txt", col.names=NA)

with this function call, the row names get printed but the column names are
offset so that they are aligned in the right column.



--- Chuck Cleland <ccleland at optonline.net> wrote:

> Sergio Della Franca wrote:
> > Dear R-Helpers,
> > 
> > I have a problem.
> > 
> > I want to export from R to .txt my data set(y):
> > 
> > YEARS PRODUCTS
> > 1990     10
> > 1995     15
> > 1997     26
> > 1998     29
> > 2000     34
> > 
> > 
> > I used this code:
> > 
> > write.table(y,"D:/Desktop/export_table.txt").
> > 
> > This procedure run correctly, but i acquired this result:
> > 
> >  YEARS PRODUCTS
> > 1           1990     10
> > 2           1995     15
> > 3           1997     26
> > 4           1998     29
> > 5           2000     34
> > 
> > The prolem is that R add a column in the export procedure, but it doesn't
> > give a name at column then this new column get the name of the first
> column
> > of my data set.
> > 
> > There is a command that a must add to my export procedure to not export
> this
> > column or to give at this a name?
> 
> write.table(y,"D:/Desktop/export_table.txt", row.names=FALSE)
> 
>   The row.names and col.names arguments are described in the help page
> for write.table().
> 
> > Thank you in advance.
> > 
> > 
> > Sergio.
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 512-0171 (M, W, F)
> fax: (917) 438-0894
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



 
____________________________________________________________________________________
No need to miss a message. Get email on-the-go


From samay.sar at gmail.com  Thu Mar 22 07:03:42 2007
From: samay.sar at gmail.com (d. sarthi maheshwari)
Date: Thu, 22 Mar 2007 11:33:42 +0530
Subject: [R] R difftime function: How can we fix the difftime unit?
Message-ID: <d4327f7e0703212303t387b05f1x4bda34398348670a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070322/234469fc/attachment.pl 

From FredeA.Togersen at agrsci.dk  Thu Mar 22 07:10:37 2007
From: FredeA.Togersen at agrsci.dk (=?iso-8859-1?Q?Frede_Aakmann_T=F8gersen?=)
Date: Thu, 22 Mar 2007 07:10:37 +0100
Subject: [R] R difftime function: How can we fix the difftime unit?
References: <d4327f7e0703212303t387b05f1x4bda34398348670a@mail.gmail.com>
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC04E87DF9@DJFPOST01.djf.agrsci.dk>

That's easy:
 
difftime(x, y, units = "min")
 
Best regards
 
Frede

________________________________

Fra: r-help-bounces at stat.math.ethz.ch p? vegne af d. sarthi maheshwari
Sendt: to 22-03-2007 07:03
Til: r-help at stat.math.ethz.ch
Emne: [R] R difftime function: How can we fix the difftime unit?



Hi,

I am trying to take difference of two time objects. I want to fix the
result's unit to minutes. How can I do that?

Here is an example:

> difftime(x, y)
Time difference of 2.030720 hours
> difftime(x, z)
Time difference of 30.34672 mins

where x = '2007-03-05 08:32:58'
          y = '2007-03-05 06:31:07'
and    z = '2007-03-05 08:02:37'

How can I get answer something like

for (x-y)
Time difference of 121.8432 mins

and for (x-z)
Time difference of 30.34672 mins

Kindly help.

--
Thanks & Regards
Sarthi M.

        [[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at comcast.net  Thu Mar 22 07:15:35 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 22 Mar 2007 01:15:35 -0500
Subject: [R] R difftime function: How can we fix the difftime unit?
In-Reply-To: <d4327f7e0703212303t387b05f1x4bda34398348670a@mail.gmail.com>
References: <d4327f7e0703212303t387b05f1x4bda34398348670a@mail.gmail.com>
Message-ID: <1174544135.8712.17.camel@localhost.localdomain>

On Thu, 2007-03-22 at 11:33 +0530, d. sarthi maheshwari wrote:
> Hi,
> 
> I am trying to take difference of two time objects. I want to fix the
> result's unit to minutes. How can I do that?
> 
> Here is an example:
> 
> > difftime(x, y)
> Time difference of 2.030720 hours
> > difftime(x, z)
> Time difference of 30.34672 mins
> 
> where x = '2007-03-05 08:32:58'
>           y = '2007-03-05 06:31:07'
> and    z = '2007-03-05 08:02:37'
> 
> How can I get answer something like
> 
> for (x-y)
> Time difference of 121.8432 mins
> 
> and for (x-z)
> Time difference of 30.34672 mins
> 
> Kindly help.


See ?difftime and take note of the 'units' argument:

> difftime(x, y, units = "mins")
Time difference of 121.85 mins

> difftime(x, z, units = "mins")
Time difference of 30.35 mins

HTH,

Marc Schwartz


From justin_bem at yahoo.fr  Thu Mar 22 07:44:45 2007
From: justin_bem at yahoo.fr (justin bem)
Date: Thu, 22 Mar 2007 07:44:45 +0100 (CET)
Subject: [R] Arrellano Bond and Blundell and Bond estimators in R
Message-ID: <398279.28978.qm@web23010.mail.ird.yahoo.com>

Hi all,

Is there a package to estimate Arrellano Bond
estimator and blundelll and bond system estimator for
dynamic panel in R ?

Best regards.

Justin BEM
El?ve Ing?nieur Statisticien Economiste
BP 294 Yaound?.
T?l (00237)9597295.


From gavin.simpson at ucl.ac.uk  Thu Mar 22 08:35:04 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 22 Mar 2007 07:35:04 +0000
Subject: [R] dynamic linear models in R
In-Reply-To: <4b82d65b0703212058na81fd62uc5c49f5cec51962f@mail.gmail.com>
References: <4b82d65b0703212058na81fd62uc5c49f5cec51962f@mail.gmail.com>
Message-ID: <1174548904.3011.7.camel@dhcppc2.my.nat.localnet>

On Wed, 2007-03-21 at 23:58 -0400, Johann Hibschman wrote:
> Hi all,
> 
> I've just started working my way through Mike West and Jeff Harrison's
> _Bayesian Forecasting and Dynamic Models_, and I was wondering if
> there were any publically-available packages to handle dynamic linear
> models, as they describe.

Johann,

The one I'm most familiar with is package dlm by Giovanni Petris. There
is also package sspir by Claus Dethlefsen and S?ren Lundbye-Christensen.
Both packages are on CRAN.

I have been using dlm for some recent DLM analysis I was doing and have
found it reasonably easy to use and the maintainer, Giovanni Petris, has
been extremely patient and helpful with the odd question I have had
about how to specify the models I wanted in dlm.

sspir has a formula interface so it may be easier to specify models in
it than dlm, but I have no experience of sspir in use.

HTH

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT                          [w] http://www.freshwaters.org.uk/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From ripley at stats.ox.ac.uk  Thu Mar 22 08:49:28 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 22 Mar 2007 07:49:28 +0000 (GMT)
Subject: [R] problem with RCurl install on Unix
In-Reply-To: <4601AD1E.30408@gmail.com>
References: <4601AD1E.30408@gmail.com>
Message-ID: <Pine.LNX.4.64.0703220743510.32513@gannet.stats.ox.ac.uk>

What is 'Unix' here?  It seems you mean ppc64 Linux (which is not Unix).

On Wed, 21 Mar 2007, Mark W Kimpel wrote:

> I am having trouble getting an install of RCurl to work properly on a
> Unix server. The steps I have taken are:
>
> 1. installed cUrl from source without difficulty
> 2. installed RCurl from source using the command
> 	~/R_HOME/R-devel/bin/R CMD INSTALL -l ~/R_HOME/R-devel/library
> ~/RCurl_0.8-0.tar.gz     I received no errors during this install
> 3. when I go back to R and require(RCurl), I get
>
> > require(RCurl)
> Loading required package: RCurl
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library
> '/N/u/mkimpel/BigRed/R_HOME/R-devel/library/RCurl/libs/RCurl.so':
>   libcurl.so.4: cannot open shared object file: No such file or directory
> ?1? FALSE
>
> Outside of R I get
>
> mkimpel?BigRed:?/R_HOME/R-devel/library/RCurl/libs> ls
> RCurl.so
>
> I can cat into RCurl and I have even done chmod a+x RCurl.so in case
> there was a problem with permission for R to open the file.

Please do read the message it gave, which is not about opening RCurl.so. 
It is saying that libcurl.so.4 is not found by the run-time linker. 
Probably you installed it into a directory that is not cached by ldconfig, 
so where did you install it?  (One possible issue is lib64 vs lib.)


> Below is my sessionInfo. Thanks, Mark
>
> > sessionInfo()
> R version 2.5.0 Under development (unstable) (2007-03-11 r40824)
> powerpc64-unknown-linux-gnu
>
> locale:
> LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C
>
> attached base packages:
> ?1? "stats"     "graphics"  "grDevices" "datasets"  "utils"     "tools"
> ?7? "methods"   "base"
>
> other attached packages:
>     limma      affy    affyio   Biobase
>  "2.9.13" "1.13.14"   "1.3.3" "1.13.39"
> >
>
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ligges at statistik.uni-dortmund.de  Thu Mar 22 08:50:00 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 22 Mar 2007 08:50:00 +0100
Subject: [R] Problem installing packages in R 2.4.1
In-Reply-To: <8c1388b50703211334y14b817b4l91a82c845e253ace@mail.gmail.com>
References: <8c1388b50703211334y14b817b4l91a82c845e253ace@mail.gmail.com>
Message-ID: <46023528.7080809@statistik.uni-dortmund.de>



Paul Gottemoller wrote:
> I am attempting to install a anchoring vingettes package for R and I seem to
> have a problem with unzipping the files in R.  I use the command that can be
> found here <http://wand.stanford.edu/anchors/>.
> 
> Here is the dialog I get after entering the command:
> 
>>     install.packages("anchors", dependencies = TRUE,
> +     repos=c("http://wand.stanford.edu/R/CRAN","http://cran.r-project.org
> "))
> dependency ''snow'' is not available
> 
> also installing the dependency 'rgenoud'
> 
> trying URL '
> http://cran.r-project.org/bin/windows/contrib/2.4/rgenoud_5.0-5.zip'
> Content type 'application/zip' length 113709 bytes
> opened URL
> downloaded 111Kb
> 
> trying URL '
> http://wand.stanford.edu/R/CRAN/bin/windows/contrib/2.4/anchors_2.0.zip'
> Content type 'application/zip' length 592348 bytes
> opened URL
> downloaded 578Kb
> 
> Error in zip.unpack(pkg, tmpDir) : cannot open file 'C:/Program Files/R/R-
> 2.4.1/library/file20a84af3/rgenoud/chtml/rgenoud.chm'
>
> Any help would be greatly appreciated.  Thank you in advance.
> 


Which version of Windows? If Vista, you might need to overcome some 
security issues and remember to install a package into a library within 
c:\Program Files with Administrator privileges.
If another version of Windows, please retry to install rgenoud and 
remove any prior versions manually, the binary package on CRAN is fine.

Uwe Ligges


From maechler at stat.math.ethz.ch  Thu Mar 22 09:11:35 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 22 Mar 2007 09:11:35 +0100
Subject: [R] Implementing trees in R
In-Reply-To: <fab4bcf70703211154n2342a3f9r12e91db3f2c53dbf@mail.gmail.com>
References: <45FA9D64.6070708@yale.edu>
	<971536df0703160659u1c339e53k474b962bd846d76e@mail.gmail.com>
	<971536df0703160706p2e462810jd070966ea79980ad@mail.gmail.com>
	<fab4bcf70703211154n2342a3f9r12e91db3f2c53dbf@mail.gmail.com>
Message-ID: <17922.14903.252350.664147@stat.math.ethz.ch>

>>>>> "B?lint" == B?lint Cz?cz <czucz at botanika.hu>
>>>>>     on Wed, 21 Mar 2007 19:54:28 +0100 writes:

    B?lint> for an example how this idea is implemented in a working package, have
    B?lint> a look at the help page of BinaryTree class in package party. The
    B?lint> @tree node of such an object is a recursive list of this kind.

and the informal (i.e. S3) class  "dendrogram"  in the 'stats'
package, i.e. part of every R, is of that kind as well
{to be used for cluster dendrograms it has attributes for nodes
 and edges, etc}

It features a print(), (too ?!) flexible plot(), a nice str()
method, and more:

 > methods(class = "dendrogram")
 [1] cophenetic.dendrogram* cut.dendrogram*        [[.dendrogram*        
 [4] labels.dendrogram*     plot.dendrogram*       print.dendrogram*     
 [7] reorder.dendrogram*    rev.dendrogram*        str.dendrogram*       

Type 
     example(dendrogram)

to get a first impression.

Martin Maechler, ETH Zurich


    B?lint> On 3/16/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
    >> Let me rephrase that.  Lists do not support references but they
    >> could be used to represent trees.
    >> 
    >> list(a = list(a = 1, b = list(2, 3, d = list(4, 5)), c = list(4, 5))
    >> 
    >> is a tree whose top nodes are a, b, c and b contains three nodes
    >> 2, 3 and d and d contains 2 nodes.
    >> 
    >> However, if you want to do it via references as requested then lists
    >> are not appropriate.
    >> 
    >> On 3/16/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
    >> > Lists are not good for this.  There is an example in section 3.3 of
    >> > the proto vignette of using proto objects for this.  That section
    >> > also references an S4 example although its pretty messy with S4.
    >> >
    >> > You might want to look at the graph, RBGL and graphviz packages
    >> > in Bioconductor and the dynamicgraph, mathgraph and sna packages
    >> > on CRAN.
    >> >
    >> > On 3/16/07, Yuk Lap Yip (Kevin) <yuklap.yip at yale.edu> wrote:
    >> > > Hi all,
    >> > >
    >> > >    I am rather new to R. Recently I have been trying to implement some
    >> > > tree algorithms in R. I used lists to model tree nodes. I thought
    >> > > something like this would work:
    >> > >
    >> > >    parent <- list();
    >> > >    child <- list();
    >> > >    parent$child1 <- child;
    >> > >    child$parent <- parent;
    >> > >
    >> > >    When I tried to check whether a node is its parent's first child
    >> > > using "if (node$parent$child1 == node)", it always returned false. Then
    >> > > I realized that it does not really work because "parent$child1 <- child"
    >> > > actually makes a copy of child instead of referencing it. I think one
    >> > > possible fix is to keep a list of node objects, and make references
    >> > > using the positions in the list. For example, I think the following
    >> > > would work:
    >> > >
    >> > >    parent <- list();
    >> > >    child <- list();
    >> > >    nodes <- list(parent, child);
    >> > >    parent$child1 <- 2;
    >> > >    child$parent <- 1;
    >> > >
    >> > >    Then the "first child" test can be rewritten as "if
    >> > > (nodes[[nodes[[nodeId]]$parent]]$child1 == nodeId)". However, I would
    >> > > prefer not to implement trees in this way, as it requires the
    >> > > inconvenient and error-prone manipulations of node IDs.
    >> > >
    >> > >    May I know if there is a way to make object references to lists? Or
    >> > > are there other ways to implement tree data structures in R?
    >> > >
    >> > >    BTW, I checked how hclust was implemented, and noticed that it calls
    >> > > an external Fortran program. I would want a solution not involving any
    >> > > external programs.
    >> > >
    >> > >    Thanks.
    >> > >
    >> > > --
    >> > >
    >> > >
    >> > >        God bless.
    >> > >
    >> > >        Kevin
    >> > >
    >> > > ______________________________________________
    >> > > R-help at stat.math.ethz.ch mailing list
    >> > > https://stat.ethz.ch/mailman/listinfo/r-help
    >> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    >> > > and provide commented, minimal, self-contained, reproducible code.
    >> > >
    >> >
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    >> and provide commented, minimal, self-contained, reproducible code.
    >> 

    B?lint> ______________________________________________
    B?lint> R-help at stat.math.ethz.ch mailing list
    B?lint> https://stat.ethz.ch/mailman/listinfo/r-help
    B?lint> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    B?lint> and provide commented, minimal, self-contained, reproducible code.


From tamir at imp.univie.ac.at  Thu Mar 22 09:32:09 2007
From: tamir at imp.univie.ac.at (Ido M. Tamir)
Date: Thu, 22 Mar 2007 09:32:09 +0100
Subject: [R] Fwd: RSPerl buffer overflow
Message-ID: <200703220932.09790.tamir@imp.univie.ac.at>

Hi,

I installed RSPerl (RSPerl_0.91-2.tar.gz) and after
loading the library I get a buffer overflow.

below is the output from R and below that the
messages during the installation.

Its on an AMD opteron system running Fedora Core5
R version 2.4.1 (2006-12-18).

Thank you very much for your help.

Ido

> sessionInfo()

R version 2.4.1 (2006-12-18)
x86_64-redhat-linux-gnu

locale:
LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;
LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=
C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
[7] "base"



[Previously saved workspace restored]

> library("RSPerl")

*** buffer overflow detected ***: /usr/lib64/R/bin/exec/R terminated
======= Backtrace: =========
/lib64/libc.so.6(__chk_fail+0x2f)[0x385f3df24f]
/lib64/libc.so.6[0x385f3de809]
/lib64/libc.so.6(_IO_default_xsputn+0x8e)[0x385f369a3e]
/lib64/libc.so.6(_IO_vfprintf+0x36e4)[0x385f344294]
/lib64/libc.so.6(__vsprintf_chk+0x9d)[0x385f3de8ad]
/lib64/libc.so.6(__sprintf_chk+0x80)[0x385f3de7f0]
/usr/lib64/R/library/RSPerl/libs/RSPerl.so(RS_PerlEvalString+0x12e)
[0x2aaaae5ca31e]
/usr/lib64/R/lib/libR.so[0x350558468a]
/usr/lib64/R/lib/libR.so(Rf_eval+0x606)[0x35055b1fa6]
/usr/lib64/R/lib/libR.so[0x35055b2a72]
/usr/lib64/R/lib/libR.so(Rf_eval+0x452)[0x35055b1df2]
/usr/lib64/R/lib/libR.so(Rf_applyClosure+0x286)[0x35055b3ed6]
/usr/lib64/R/lib/libR.so(Rf_eval+0x2fc)[0x35055b1c9c]
/usr/lib64/R/lib/libR.so[0x35055b5c50]
/usr/lib64/R/lib/libR.so(Rf_eval+0x452)[0x35055b1df2]
/usr/lib64/R/lib/libR.so[0x35055b2a72]
/usr/lib64/R/lib/libR.so(Rf_eval+0x452)[0x35055b1df2]
/usr/lib64/R/lib/libR.so(Rf_applyClosure+0x286)[0x35055b3ed6]
/usr/lib64/R/lib/libR.so(Rf_eval+0x2fc)[0x35055b1c9c]
/usr/lib64/R/lib/libR.so[0x3505537228]
/usr/lib64/R/lib/libR.so[0x35055df063]
/usr/lib64/R/lib/libR.so(Rf_eval+0x452)[0x35055b1df2]
/usr/lib64/R/lib/libR.so[0x35055b2a72]
/usr/lib64/R/lib/libR.so(Rf_eval+0x452)[0x35055b1df2]
/usr/lib64/R/lib/libR.so(Rf_applyClosure+0x286)[0x35055b3ed6]
/usr/lib64/R/lib/libR.so(Rf_eval+0x2fc)[0x35055b1c9c]
/usr/lib64/R/lib/libR.so[0x35055b5c50]
/usr/lib64/R/lib/libR.so(Rf_eval+0x452)[0x35055b1df2]
/usr/lib64/R/lib/libR.so[0x35055b2a72]
/usr/lib64/R/lib/libR.so(Rf_eval+0x452)[0x35055b1df2]
/usr/lib64/R/lib/libR.so(Rf_applyClosure+0x286)[0x35055b3ed6]
/usr/lib64/R/lib/libR.so(Rf_eval+0x2fc)[0x35055b1c9c]
/usr/lib64/R/lib/libR.so(Rf_eval+0x452)[0x35055b1df2]
/usr/lib64/R/lib/libR.so[0x35055b2a72]
/usr/lib64/R/lib/libR.so(Rf_eval+0x452)[0x35055b1df2]
/usr/lib64/R/lib/libR.so(Rf_applyClosure+0x286)[0x35055b3ed6]
/usr/lib64/R/lib/libR.so(Rf_eval+0x2fc)[0x35055b1c9c]
/usr/lib64/R/lib/libR.so[0x35055b2a72]
/usr/lib64/R/lib/libR.so(Rf_eval+0x452)[0x35055b1df2]
/usr/lib64/R/lib/libR.so(Rf_applyClosure+0x286)[0x35055b3ed6]
/usr/lib64/R/lib/libR.so(Rf_eval+0x2fc)[0x35055b1c9c]
/usr/lib64/R/lib/libR.so[0x35055b2a72]
/usr/lib64/R/lib/libR.so(Rf_eval+0x452)[0x35055b1df2]
/usr/lib64/R/lib/libR.so(Rf_eval+0x36d)[0x35055b1d0d]
/usr/lib64/R/lib/libR.so(Rf_eval+0x546)[0x35055b1ee6]
/usr/lib64/R/lib/libR.so[0x35055b2a72]
/usr/lib64/R/lib/libR.so(Rf_eval+0x452)[0x35055b1df2]
/usr/lib64/R/lib/libR.so(Rf_eval+0x452)[0x35055b1df2]
/usr/lib64/R/lib/libR.so[0x35055b2a72]
/usr/lib64/R/lib/libR.so(Rf_eval+0x452)[0x35055b1df2]
/usr/lib64/R/lib/libR.so(Rf_applyClosure+0x286)[0x35055b3ed6]
/usr/lib64/R/lib/libR.so(Rf_eval+0x2fc)[0x35055b1c9c]
/usr/lib64/R/lib/libR.so[0x35055b2408]
/usr/lib64/R/lib/libR.so(Rf_eval+0x48c)[0x35055b1e2c]
/usr/lib64/R/lib/libR.so[0x35055b2408]
/usr/lib64/R/lib/libR.so(Rf_eval+0x48c)[0x35055b1e2c]
/usr/lib64/R/lib/libR.so[0x35055b50d8]
/usr/lib64/R/lib/libR.so(Rf_eval+0x452)[0x35055b1df2]
/usr/lib64/R/lib/libR.so[0x35055b2a72]
/usr/lib64/R/lib/libR.so(Rf_eval+0x452)[0x35055b1df2]
/usr/lib64/R/lib/libR.so(Rf_eval+0x452)[0x35055b1df2]
/usr/lib64/R/lib/libR.so[0x35055b2a72]
/usr/lib64/R/lib/libR.so(Rf_eval+0x452)[0x35055b1df2]
======= Memory map: ========
00400000-00401000 r-xp 00000000 fd:00
87064596 ? ? ? ? ? ? ? ? ? ? ? ? ? /usr/lib64/R/bin/exec/R
00500000-00502000 rw-p 00000000 fd:00
87064596 ? ? ? ? ? ? ? ? ? ? ? ? ? /usr/lib64/R/bin/exec/R
00502000-01b45000 rw-p 00502000 00:00 0 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
[heap]
3505100000-350510d000 r-xp 00000000 fd:00
26279979 ? ? ? ? ? ? ? ? ? ? ? /lib64/libgcc_s-4.1.1-20070108.so.1
350510d000-350520c000 ---p 0000d000 fd:00
26279979 ? ? ? ? ? ? ? ? ? ? ? /lib64/libgcc_s-4.1.1-20070108.so.1
350520c000-350520d000 rw-p 0000c000 fd:00
26279979 ? ? ? ? ? ? ? ? ? ? ? /lib64/libgcc_s-4.1.1-20070108.so.1
3505300000-3505396000 r-xp 00000000 fd:00
49948505 ? ? ? ? ? ? ? ? ? ? ? /usr/lib64/libgfortran.so.1.0.0
3505396000-3505495000 ---p 00096000 fd:00
49948505 ? ? ? ? ? ? ? ? ? ? ? /usr/lib64/libgfortran.so.1.0.0
3505495000-3505497000 rw-p 00095000 fd:00
49948505 ? ? ? ? ? ? ? ? ? ? ? /usr/lib64/libgfortran.so.1.0.0
3505500000-3505725000 r-xp 00000000 fd:00
50989646 ? ? ? ? ? ? ? ? ? ? ? /usr/lib64/R/lib/libR.so
3505725000-3505824000 ---p 00225000 fd:00
50989646 ? ? ? ? ? ? ? ? ? ? ? /usr/lib64/R/lib/libR.so
3505824000-3505838000 rw-p 00224000 fd:00
50989646 ? ? ? ? ? ? ? ? ? ? ? /usr/lib64/R/lib/libR.so
3505838000-35058d1000 rw-p 3505838000 00:00 0
385f100000-385f11a000 r-xp 00000000 fd:00
91291670 ? ? ? ? ? ? ? ? ? ? ? /lib64/ld-2.4.so
385f219000-385f21a000 r--p 00019000 fd:00
91291670 ? ? ? ? ? ? ? ? ? ? ? /lib64/ld-2.4.so
385f21a000-385f21b000 rw-p 0001a000 fd:00
91291670 ? ? ? ? ? ? ? ? ? ? ? /lib64/ld-2.4.so
385f300000-385f43f000 r-xp 00000000 fd:00
91291679 ? ? ? ? ? ? ? ? ? ? ? /lib64/libc-2.4.so
385f43f000-385f53e000 ---p 0013f000 fd:00
91291679 ? ? ? ? ? ? ? ? ? ? ? /lib64/libc-2.4.so
385f53e000-385f542000 r--p 0013e000 fd:00
91291679 ? ? ? ? ? ? ? ? ? ? ? /lib64/libc-2.4.so
385f542000-385f543000 rw-p 00142000 fd:00
91291679 ? ? ? ? ? ? ? ? ? ? ? /lib64/libc-2.4.so
385f543000-385f548000 rw-p 385f543000 00:00 0
385f600000-385f680000 r-xp 00000000 fd:00
91291778 ? ? ? ? ? ? ? ? ? ? ? /lib64/libm-2.4.so
385f680000-385f77f000 ---p 00080000 fd:00
91291778 ? ? ? ? ? ? ? ? ? ? ? /lib64/libm-2.4.so
385f77f000-385f780000 r--p 0007f000 fd:00
91291778 ? ? ? ? ? ? ? ? ? ? ? /lib64/libm-2.4.so
385f780000-385f781000 rw-p 00080000 fd:00
91291778 ? ? ? ? ? ? ? ? ? ? ? /lib64/libm-2.4.so
385f800000-385f802000 r-xp 00000000 fd:00
91291713 ? ? ? ? ? ? ? ? ? ? ? /lib64/libdl-2.4.so
385f802000-385f902000 ---p 00002000 fd:00
91291713 ? ? ? ? ? ? ? ? ? ? ? /lib64/libdl-2.4.so
385f902000-385f903000 r--p 00002000 fd:00
91291713 ? ? ? ? ? ? ? ? ? ? ? /lib64/libdl-2.4.so
385f903000-385f904000 rw-p 00003000 fd:00
91291713 ? ? ? ? ? ? ? ? ? ? ? /lib64/libdl-2.4.so
385fa00000-385fa32000 r-xp 00000000 fd:00
113246301 ? ? ? ? ? ? ? ? ? ? ?/usr/lib64/libreadline.so.5.0
385fa32000-385fb31000 ---p 00032000 fd:00
113246301 ? ? ? ? ? ? ? ? ? ? ?/usr/lib64/libreadline.so.5.0
385fb31000-385fb39000 rw-p 00031000 fd:00
113246301 ? ? ? ? ? ? ? ? ? ? ?/usr/lib64/libreadline.so.5.0
385fb39000-385fb3a000 rw-p 385fb39000 00:00 0
3860200000-3860214000 r-xp 00000000 fd:00
49939208 ? ? ? ? ? ? ? ? ? ? ? /usr/lib64/libz.so.1.2.3
3860214000-3860313000 ---p 00014000 fd:00
49939208 ? ? ? ? ? ? ? ? ? ? ? /usr/lib64/libz.so.1.2.3
3860313000-3860314000 rw-p 00013000 fd:00
49939208 ? ? ? ? ? ? ? ? ? ? ? /usr/lib64/libz.so.1.2.3
3860400000-3860412000 r-xp 00000000 fd:00
91291860 ? ? ? ? ? ? ? ? ? ? ? /lib64/libpthread-2.4.so
3860412000-3860512000 ---p 00012000 fd:00
91291860 ? ? ? ? ? ? ? ? ? ? ? /lib64/libpthread-2.4.so
3860512000-3860513000 r--p 00012000 fd:00
91291860 ? ? ? ? ? ? ? ? ? ? ? /lib64/libpthread-2.4.so
3860513000-3860514000 rw-p 00013000 fd:00
91291860 ? ? ? ? ? ? ? ? ? ? ? /lib64/libpthread-2.4.so
3860514000-3860518000 rw-p 3860514000 00:00 0
3860600000-386072c000 r-xp 00000000 fd:00
54034480 ? ? ? ? ? ? ? ? ? ? ?
 /usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE/libperl.so
 386072c000-386082b000 ---p 0012c000 fd:00
54034480 ? ? ? ? ? ? ? ? ? ? ?
 /usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE/libperl.so
 386082b000-3860834000 rw-p 0012b000 fd:00
54034480 ? ? ? ? ? ? ? ? ? ? ?
 /usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE/libperl.so
 3860834000-3860836000 rw-p 3860834000 00:00 0
3862d00000-3862d4e000 r-xp 00000000 fd:00
113246305 ? ? ? ? ? ? ? ? ? ? ?/usr/lib64/libncurses.so.5.5
3862d4e000-3862e4e000 ---p 0004e000 fd:00
113246305 ? ? ? ? ? ? ? ? ? ? ?/usr/lib64/libncurses.so.5.5
3862e4e000-3862e5c000 rw-p 0004e000 fd:00
113246305 ? ? ? ? ? ? ? ? ? ? ?/usr/lib64/libncurses.so.5.5
3862e5c000-3862e5d000 rw-p 3862e5c000 00:00 0
3864800000-3864813000 r-xp 00000000 fd:00
91291980 ? ? ? ? ? ? ? ? ? ? ? /lib64/libnsl-2.4.so
3864813000-3864913000 ---p 00013000 fd:00
91291980 ? ? ? ? ? ? ? ? ? ? ? /lib64/libnsl-2.4.so
3864913000-3864914000 r--p 00013000 fd:00
91291980 ? ? ? ? ? ? ? ? ? ? ? /lib64/libnsl-2.4.so
3864914000-3864915000 rw-p 00014000 fd:00
91291980 ? ? ? ? ? ? ? ? ? ? ? /lib64/libnsl-2.4.so
3864915000-3864917000 rw-p 3864915000 00:00 0
3864c00000-3864c11000 r-xp 00000000 fd:00 91291864 ? ? ? Aborted

******************************************************************

[root at MCP dl]# R CMD INSTALL -c RSPerl_0.91-2.tar.gz
* Installing *source* package 'RSPerl' ...
No support for any of the Perl modules from calling Perl from R.
*****************************************************

? ? ? ?Set PERL5LIB to /usr/lib64/R/library/RSPerl/perl

*****************************************************
Using '/usr/bin/perl' as the perl executable
Perl modules:
Adding R package to list of Perl modules to enable callbacks to R from Perl
modules: ?R; linking:
checking for gcc... gcc
checking for C compiler default output... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
Support R in Perl: yes
configure: creating ./config.status
config.status: creating src/Makevars
config.status: creating R/RSUtils.S
config.status: creating inst/scripts/RSPerl.csh
config.status: creating inst/scripts/RSPerl.bsh
config.status: creating src/RinPerlMakefile
config.status: creating src/Makefile.PL
config.status: creating cleanup
config.status: creating src/R.pm
config.status: creating R/perl5lib.R
making RinPerlMakefile
gcc -std=gnu99 -I/usr/lib64/R/include -I/usr/lib64/R/include -I.
 ?-D_REENTRANT -D_GNU_SOURCE -fno-strict-aliasing -pipe
 -Wdeclaration-after-statement -I/usr/local/include -D_LARGEFILE_SOURCE
 -D_FILE_OFFSET_BITS=64 -I/usr/include/gdbm
 ?-I/usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE ?-DPERL_POLLUTE ?
 -D_R_=1 -DUSE_R=1 -DUSE_TOPLEVEL_EXEC=1 -I/usr/local/include ? ?-fpic ?-O2
 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
 --param=ssp-buffer-size=4 -m64 -mtune=generic -c Converters.c -o
 Converters.o
Converters.c: In function 'fromPerl':
Converters.c:297: warning: unused variable 'tmp'
Converters.c: In function 'GetRScalar':
Converters.c:421: warning: unused variable 'mg'
Converters.c: In function 'makeForeignPerlReference':
Converters.c:567: warning: unused variable 'key'
Converters.c:565: warning: unused variable 'n'
Converters.c: In function 'getForeignPerlReference':
Converters.c:715: warning: unused variable 'key'
Converters.c:714: warning: unused variable 'el'
Converters.c:713: warning: unused variable 'table'
Converters.c: In function 'directConvertFromPerl':
Converters.c:1058: warning: unused variable 'classes'
Converters.c: In function 'RS_GetPerlReferenceObjects':
Converters.c:636: warning: 'ans' may be used uninitialized in this function
gcc -std=gnu99 -I/usr/lib64/R/include -I/usr/lib64/R/include -I.
 ?-D_REENTRANT -D_GNU_SOURCE -fno-strict-aliasing -pipe
 -Wdeclaration-after-statement -I/usr/local/include -D_LARGEFILE_SOURCE
 -D_FILE_OFFSET_BITS=64 -I/usr/include/gdbm
 ?-I/usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE ?-DPERL_POLLUTE ?
 -D_R_=1 -DUSE_R=1 -DUSE_TOPLEVEL_EXEC=1 -I/usr/local/include ? ?-fpic ?-O2
 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
 --param=ssp-buffer-size=4 -m64 -mtune=generic -c Reflectance.c -o
 Reflectance.o
Reflectance.c: In function 'RS_getStashFromCode':
Reflectance.c:52: warning: unused variable 'classes'
Reflectance.c:49: warning: unused variable 'ref'
Reflectance.c: In function 'computeRSPerlClassVector':
Reflectance.c:402: warning: unused variable 'z'
Reflectance.c:401: warning: unused variable 'ixval'
Reflectance.c:400: warning: unused variable 'ival'
Reflectance.c:399: warning: unused variable 'xvalue'
Reflectance.c:427: warning: unused variable 'obj'
Reflectance.c:426: warning: unused variable 'tt'
Reflectance.c:442: warning: operation on 'classes' may be undefined
Reflectance.c: In function 'isHomogeneous':
Reflectance.c:513: warning: 'el' may be used uninitialized in this function
Reflectance.c: In function 'RS_getPerlType':
Reflectance.c:140: warning: 'el' may be used uninitialized in this function
gcc -std=gnu99 -I/usr/lib64/R/include -I/usr/lib64/R/include -I.
 ?-D_REENTRANT -D_GNU_SOURCE -fno-strict-aliasing -pipe
 -Wdeclaration-after-statement -I/usr/local/include -D_LARGEFILE_SOURCE
 -D_FILE_OFFSET_BITS=64 -I/usr/include/gdbm
 ?-I/usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE ?-DPERL_POLLUTE ?
 -D_R_=1 -DUSE_R=1 -DUSE_TOPLEVEL_EXEC=1 -I/usr/local/include ? ?-fpic ?-O2
 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
 --param=ssp-buffer-size=4 -m64 -mtune=generic -c ForeignReference.c -o
 ForeignReference.o
ForeignReference.c: In function 'RPerl_createRProxy':
ForeignReference.c:212: warning: value computed is not used
gcc -std=gnu99 -I/usr/lib64/R/include -I/usr/lib64/R/include -I.
 ?-D_REENTRANT -D_GNU_SOURCE -fno-strict-aliasing -pipe
 -Wdeclaration-after-statement -I/usr/local/include -D_LARGEFILE_SOURCE
 -D_FILE_OFFSET_BITS=64 -I/usr/include/gdbm
 ?-I/usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE ?-DPERL_POLLUTE ?
 -D_R_=1 -DUSE_R=1 -DUSE_TOPLEVEL_EXEC=1 -I/usr/local/include ? ?-fpic ?-O2
 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
 --param=ssp-buffer-size=4 -m64 -mtune=generic -c UserConverters.c -o
 UserConverters.o
UserConverters.c: In function 'RPerl_addConverter':
UserConverters.c:264: warning: 'className' may be used uninitialized in this
function
gcc -std=gnu99 -I/usr/lib64/R/include -I/usr/lib64/R/include -I.
 ?-D_REENTRANT -D_GNU_SOURCE -fno-strict-aliasing -pipe
 -Wdeclaration-after-statement -I/usr/local/include -D_LARGEFILE_SOURCE
 -D_FILE_OFFSET_BITS=64 -I/usr/include/gdbm
 ?-I/usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE ?-DPERL_POLLUTE ?
 -D_R_=1 -DUSE_R=1 -DUSE_TOPLEVEL_EXEC=1 -I/usr/local/include ? ?-fpic ?-O2
 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
 --param=ssp-buffer-size=4 -m64 -mtune=generic -c GeneralConverters.c -o
 GeneralConverters.o
making libPerlConverter.so
/usr/lib64/R/bin/R CMD SHLIB -o libPerlConverter.so Converters.c
 Reflectance.c ForeignReference.c UserConverters.o GeneralConverters.o
make[1]: Entering directory `/tmp/R.INSTALL.Z15223/RSPerl/src'
gcc -std=gnu99 -shared -L/usr/local/lib64 -o libPerlConverter.so Converters.o
Reflectance.o ForeignReference.o UserConverters.o
GeneralConverters.o -Wl,-E
 -Wl,-rpath,/usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE
 ?/usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/auto/DynaLoader/DynaLoader
.a -L/usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE -lperl -lresolv
 -lnsl -ldl -lm -lcrypt -lutil -lpthread -lc ? -L/usr/lib64/R/lib -lR
 make[1]: Leaving directory `/tmp/R.INSTALL.Z15223/RSPerl/src'
if test ! -d /usr/lib64/R/library/RSPerl/libs ; then
mkdir /usr/lib64/R/library/RSPerl/libs ; fi
cp libPerlConverter.so /usr/lib64/R/library/RSPerl/libs
/usr/bin/perl Makefile.PL PREFIX=/usr/lib64/R/library/RSPerl
LIB=/usr/lib64/R/library/RSPerl/perl
Warning: -L. changed to -L/tmp/R.INSTALL.Z15223/RSPerl/src/.
Writing Makefile.perl for R
make -f Makefile.perl
make[1]: Entering directory `/tmp/R.INSTALL.Z15223/RSPerl/src'
cp R.pm blib/lib/R.pm
AutoSplitting blib/lib/R.pm (blib/lib/auto/R)
cp RReferences.pm blib/lib/RReferences.pm
/usr/bin/perl /usr/lib/perl5/5.8.8/ExtUtils/xsubpp ?-typemap
 /usr/lib/perl5/5.8.8/ExtUtils/typemap ? R.xs > R.xsc && mv R.xsc R.c
gcc -c ?-I. -I/usr/lib64/R/include -D_REENTRANT -D_GNU_SOURCE
 -fno-strict-aliasing -pipe -Wdeclaration-after-statement
 -I/usr/local/include -D_LARGEFILE_SOURCE -D_FILE_OFFSET_BITS=64
 -I/usr/include/gdbm -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
 -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic ?
 -DVERSION=\"0.01\" -DXS_VERSION=\"0.01\" -fPIC
 "-I/usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE" ?-g -D_R_=1
 -DUSE_R=1 R.c
R.c: In function 'XS_R_call':
R.c:115: warning: unused variable 'RETVAL'
R.xs:103: warning: unused variable 'addLocal'
R.c: In function 'XS_R_callWithNames':
R.c:267: warning: unused variable 'RETVAL'
R.xs:243: warning: unused variable 'addLocal'
R.xs: In function 'XS_R_initRSession':
R.xs:385: warning: implicit declaration of function 'Rf_initEmbeddedR'
R.c: In function 'XS_R_eval':
R.c:441: warning: unused variable 'RETVAL'
R.xs: In function 'XS_R_setConverter':
R.xs:475: warning: implicit declaration of function 'addPerlConverterRoutine'
R.c:522: warning: unused variable 'RETVAL'
R.c: In function 'XS_R_deleteRReference':
R.c:546: warning: unused variable 'RETVAL'
R.c: In function 'XS_R_setDebug':
R.c:567: warning: unused variable 'RETVAL'
R.c: In function 'XS_R_library':
R.c:593: warning: unused variable 'RETVAL'
R.c: In function 'XS_R_rnorm':
R.c:623: warning: unused variable 'RETVAL'
Running Mkbootstrap for R ()
chmod 644 R.bs
rm -f blib/arch/auto/R/R.so
gcc ?-shared R.o ?-o blib/arch/auto/R/R.so ? ? ?\
? ?-L/tmp/R.INSTALL.Z15223/RSPerl/src -L/usr/lib64/R/library/RSPerl/libs
 -lPerlConverter -L/usr/lib64/R/lib -lR ? ? ? ? \

chmod 755 blib/arch/auto/R/R.so
cp R.bs blib/arch/auto/R/R.bs
chmod 644 blib/arch/auto/R/R.bs
Manifying blib/man3/R.3pm
Manifying blib/man3/RReferences.3pm
make[1]: Leaving directory `/tmp/R.INSTALL.Z15223/RSPerl/src'
Manifying blib/man3/R.3pm
Manifying blib/man3/RReferences.3pm
Installing
 /usr/lib64/R/library/RSPerl/perl/x86_64-linux-thread-multi/auto/R/R.so
 Installing
 /usr/lib64/R/library/RSPerl/perl/x86_64-linux-thread-multi/auto/R/R.bs Files
 found in blib/arch: installing files in blib/lib into architecture dependent
 library tree
Installing /usr/lib64/R/library/RSPerl/perl/x86_64-linux-thread-multi/R.pm
Installing
 /usr/lib64/R/library/RSPerl/perl/x86_64-linux-thread-multi/RReferences.pm
 Installing
 /usr/lib64/R/library/RSPerl/perl/x86_64-linux-thread-multi/auto/R/autosplit.
ix Installing /usr/lib64/R/library/RSPerl/share/man/man3/RReferences.3pm
 Installing /usr/lib64/R/library/RSPerl/share/man/man3/R.3pm
Writing
 /usr/lib64/R/library/RSPerl/perl/x86_64-linux-thread-multi/auto/R/.packlist
 Appending installation info
to /usr/lib64/R/library/RSPerl/perl/x86_64-linux-thread-multi/perllocal.pod
Finished configuration
** libs
gcc -std=gnu99 -I/usr/lib64/R/include -I/usr/lib64/R/include -I.
 ?-D_REENTRANT -D_GNU_SOURCE -fno-strict-aliasing -pipe
 -Wdeclaration-after-statement -I/usr/local/include -D_LARGEFILE_SOURCE
 -D_FILE_OFFSET_BITS=64 -I/usr/include/gdbm
 ?-I/usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE ?-DPERL_POLLUTE ?
 -D_R_=1 -DUSE_R=1 -DUSE_TOPLEVEL_EXEC=1 -I/usr/local/include ? ?-fpic ?-O2
 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
 --param=ssp-buffer-size=4 -m64 -mtune=generic -c RPerlElements.c -o
 RPerlElements.o
RPerlElements.c: In function ?RS_PerlHashElement?:
RPerlElements.c:56: warning: ?depth? may be used uninitialized in this
function
RPerlElements.c: In function ?RS_PerlArrayElement?:
RPerlElements.c:11: warning: ?depth? may be used uninitialized in this
function
gcc -std=gnu99 -I/usr/lib64/R/include -I/usr/lib64/R/include -I.
 ?-D_REENTRANT -D_GNU_SOURCE -fno-strict-aliasing -pipe
 -Wdeclaration-after-statement -I/usr/local/include -D_LARGEFILE_SOURCE
 -D_FILE_OFFSET_BITS=64 -I/usr/include/gdbm
 ?-I/usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE ?-DPERL_POLLUTE ?
 -D_R_=1 -DUSE_R=1 -DUSE_TOPLEVEL_EXEC=1 -I/usr/local/include ? ?-fpic ?-O2
 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
 --param=ssp-buffer-size=4 -m64 -mtune=generic -c RPerlEval.c -o RPerlEval.o
RPerlEval.c: In function ?RS_PerlEvalString?:
RPerlEval.c:18: warning: unused variable ?interp?
RPerlEval.c: In function ?RS_PerlGet?:
RPerlEval.c:123: warning: unused variable ?interp?
RPerlEval.c: In function ?RS_PerlCallModified?:
RPerlEval.c:297: warning: value computed is not used
RPerlEval.c:203: warning: unused variable ?interp?
RPerlEval.c: In function ?RS_PerlCall?:
RPerlEval.c:372: warning: unused variable ?interp?
RPerlEval.c: In function ?RS_PerlPackage?:
RPerlEval.c:555: warning: unused variable ?sv?
gcc -std=gnu99 -I/usr/lib64/R/include -I/usr/lib64/R/include -I.
 ?-D_REENTRANT -D_GNU_SOURCE -fno-strict-aliasing -pipe
 -Wdeclaration-after-statement -I/usr/local/include -D_LARGEFILE_SOURCE
 -D_FILE_OFFSET_BITS=64 -I/usr/include/gdbm
 ?-I/usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE ?-DPERL_POLLUTE ?
 -D_R_=1 -DUSE_R=1 -DUSE_TOPLEVEL_EXEC=1 -I/usr/local/include ? ?-fpic ?-O2
 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
 --param=ssp-buffer-size=4 -m64 -mtune=generic -c RPerlInit.c -o RPerlInit.o
gcc -std=gnu99 -I/usr/lib64/R/include -I/usr/lib64/R/include -I.
 ?-D_REENTRANT -D_GNU_SOURCE -fno-strict-aliasing -pipe
 -Wdeclaration-after-statement -I/usr/local/include -D_LARGEFILE_SOURCE
 -D_FILE_OFFSET_BITS=64 -I/usr/include/gdbm
 ?-I/usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE ?-DPERL_POLLUTE ?
 -D_R_=1 -DUSE_R=1 -DUSE_TOPLEVEL_EXEC=1 -I/usr/local/include ? ?-fpic ?-O2
 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
 --param=ssp-buffer-size=4 -m64 -mtune=generic -c RPerlNew.c -o RPerlNew.o
gcc -std=gnu99 -I/usr/lib64/R/include -I/usr/lib64/R/include -I.
 ?-D_REENTRANT -D_GNU_SOURCE -fno-strict-aliasing -pipe
 -Wdeclaration-after-statement -I/usr/local/include -D_LARGEFILE_SOURCE
 -D_FILE_OFFSET_BITS=64 -I/usr/include/gdbm
 ?-I/usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE ?-DPERL_POLLUTE ?
 -D_R_=1 -DUSE_R=1 -DUSE_TOPLEVEL_EXEC=1 -I/usr/local/include ? ?-fpic ?-O2
 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
 --param=ssp-buffer-size=4 -m64 -mtune=generic -c RPerlReference.c -o
 RPerlReference.o
gcc -std=gnu99 -I/usr/lib64/R/include -I/usr/lib64/R/include -I.
 ?-D_REENTRANT -D_GNU_SOURCE -fno-strict-aliasing -pipe
 -Wdeclaration-after-statement -I/usr/local/include -D_LARGEFILE_SOURCE
 -D_FILE_OFFSET_BITS=64 -I/usr/include/gdbm
 ?-I/usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE ?-DPERL_POLLUTE ?
 -D_R_=1 -DUSE_R=1 -DUSE_TOPLEVEL_EXEC=1 -I/usr/local/include ? ?-fpic ?-O2
 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
 --param=ssp-buffer-size=4 -m64 -mtune=generic -c RPerlVars.c -o RPerlVars.o
gcc -std=gnu99 -I/usr/lib64/R/include -I/usr/lib64/R/include -I.
 ?-D_REENTRANT -D_GNU_SOURCE -fno-strict-aliasing -pipe
 -Wdeclaration-after-statement -I/usr/local/include -D_LARGEFILE_SOURCE
 -D_FILE_OFFSET_BITS=64 -I/usr/include/gdbm
 ?-I/usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE ?-DPERL_POLLUTE ?
 -D_R_=1 -DUSE_R=1 -DUSE_TOPLEVEL_EXEC=1 -I/usr/local/include ? ?-fpic ?-O2
 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
 --param=ssp-buffer-size=4 -m64 -mtune=generic -c Utils.c -o Utils.o
gcc -std=gnu99 -I/usr/lib64/R/include -I/usr/lib64/R/include -I.
 ?-D_REENTRANT -D_GNU_SOURCE -fno-strict-aliasing -pipe
 -Wdeclaration-after-statement -I/usr/local/include -D_LARGEFILE_SOURCE
 -D_FILE_OFFSET_BITS=64 -I/usr/include/gdbm
 ?-I/usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE ?-DPERL_POLLUTE ?
 -D_R_=1 -DUSE_R=1 -DUSE_TOPLEVEL_EXEC=1 -I/usr/local/include ? ?-fpic ?-O2
 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
 --param=ssp-buffer-size=4 -m64 -mtune=generic -c xsinit.c -o xsinit.o
gcc -std=gnu99 -shared -L/usr/local/lib64 -o RSPerl.so Converters.o
ForeignReference.o GeneralConverters.o R.o Reflectance.o RPerlElements.o
RPerlEval.o RPerlInit.o RPerlNew.o RPerlReference.o RPerlVars.o
UserConverters.o Utils.o
xsinit.o -Wl,-E
 -Wl,-rpath,/usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE
 ?/usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/auto/DynaLoader/DynaLoader
.a -L/usr/lib64/perl5/5.8.8/x86_64-linux-thread-multi/CORE -lperl -lresolv
 -lnsl -ldl -lm -lcrypt -lutil -lpthread -lc ? -L/usr/lib64/R/lib -lR ** R
** inst
** help
?>>> Building/Updating help pages for package 'RSPerl'
? ? ?Formats: text html latex example
? Perl ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?text ? ?html ? ?latex ? example
? PerlClear ? ? ? ? ? ? ? ? ? ? ? ? text ? ?html ? ?latex ? example
? PerlDiscard ? ? ? ? ? ? ? ? ? ? ? text ? ?html ? ?latex ? example
? PerlExists ? ? ? ? ? ? ? ? ? ? ? ?text ? ?html ? ?latex ? example
? PerlExpr ? ? ? ? ? ? ? ? ? ? ? ? ?text ? ?html ? ?latex ? example
? PerlFile ? ? ? ? ? ? ? ? ? ? ? ? ?text ? ?html ? ?latex ? example
? PerlGetArray ? ? ? ? ? ? ? ? ? ? ?text ? ?html ? ?latex ? example
? PerlGetCode ? ? ? ? ? ? ? ? ? ? ? text ? ?html ? ?latex ? example
? PerlInit ? ? ? ? ? ? ? ? ? ? ? ? ?text ? ?html ? ?latex ? example
? PerlInterpreter ? ? ? ? ? ? ? ? ? text ? ?html ? ?latex ? example
? PerlLength ? ? ? ? ? ? ? ? ? ? ? ?text ? ?html ? ?latex ? example
? PerlNames ? ? ? ? ? ? ? ? ? ? ? ? text ? ?html ? ?latex ? example
? PerlNew ? ? ? ? ? ? ? ? ? ? ? ? ? text ? ?html ? ?latex ? example
? PerlNewArray ? ? ? ? ? ? ? ? ? ? ?text ? ?html ? ?latex ? example
? PerlPackage ? ? ? ? ? ? ? ? ? ? ? text ? ?html ? ?latex ? example
? PerlReferenceDollar ? ? ? ? ? ? ? text ? ?html ? ?latex ? example
? PerlReferenceObjects ? ? ? ? ? ? ?text ? ?html ? ?latex ? example
? PerlReferenceSubset ? ? ? ? ? ? ? text ? ?html ? ?latex ? example
? PerlSetHash ? ? ? ? ? ? ? ? ? ? ? text ? ?html ? ?latex ? example
? PerlStashInfo ? ? ? ? ? ? ? ? ? ? text ? ?html ? ?latex ? example
? PerlTerminate ? ? ? ? ? ? ? ? ? ? text ? ?html ? ?latex ? example
? PerlType ? ? ? ? ? ? ? ? ? ? ? ? ?text ? ?html ? ?latex ? example
? PerlTypes ? ? ? ? ? ? ? ? ? ? ? ? text ? ?html ? ?latex
? PerlUndef ? ? ? ? ? ? ? ? ? ? ? ? text ? ?html ? ?latex ? example
? addConverter ? ? ? ? ? ? ? ? ? ? ?text ? ?html ? ?latex ? example
? foreignReference ? ? ? ? ? ? ? ? ?text ? ?html ? ?latex ? example
? getNumPerlConverters ? ? ? ? ? ? ?text ? ?html ? ?latex ? example
? getPerlClasses ? ? ? ? ? ? ? ? ? ?text ? ?html ? ?latex ? example
? getPerlScript ? ? ? ? ? ? ? ? ? ? text ? ?html ? ?latex ? example
? mkRef ? ? ? ? ? ? ? ? ? ? ? ? ? ? text ? ?html ? ?latex ? example
? parseEval ? ? ? ? ? ? ? ? ? ? ? ? text ? ?html ? ?latex ? example
? perlInitArgs ? ? ? ? ? ? ? ? ? ? ?text ? ?html ? ?latex ? example
? perlModuleLoaded ? ? ? ? ? ? ? ? ?text ? ?html ? ?latex ? example
? referenceHandlerGenerator ? ? ? ? text ? ?html ? ?latex ? example
? setPerlHandler ? ? ? ? ? ? ? ? ? ?text ? ?html ? ?latex ? example
** building package indices ...
Removing additional files
* DONE (RSPerl)


From maechler at stat.math.ethz.ch  Thu Mar 22 10:03:52 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 22 Mar 2007 10:03:52 +0100
Subject: [R] Ticks and labels on plots
In-Reply-To: <1174504992.7212.42.camel@localhost.localdomain>
References: <46005A57.2000709@noaa.gov>
	<1174444859.5415.57.camel@localhost.localdomain>
	<b5t20314vt79f5g91d22rabhpkt0oljgra@4ax.com>
	<1174504992.7212.42.camel@localhost.localdomain>
Message-ID: <17922.18040.856404.895212@stat.math.ethz.ch>

>>>>> "Marc" == Marc Schwartz <marc_schwartz at comcast.net>
>>>>>     on Wed, 21 Mar 2007 14:23:12 -0500 writes:

    Marc> On Wed, 2007-03-21 at 14:40 -0400, Mike Prager wrote:
    >> Marc Schwartz <marc_schwartz at comcast.net> wrote:
    >> 
    >> > On Tue, 2007-03-20 at 18:04 -0400, Michael H. Prager wrote:
    >> > > I am generating stacked barplots of age-composition of fish populations 
    >> > > (Y) over time (X).  As there are many years, not every bars is labeled.  
    >> > > When looking at the plot, it becomes difficult to associate labels with 
    >> > > their bars.
    >> > > 
    >> > > We have improved this a bit by using axis() to add a tickmark below each 
    >> > > bar.  Can anyone suggest a way to draw ticks ONLY at bars where a tick 
    >> > > label is drawn?  Or to make such ticks longer than those where there is 
    >> > > no label?
    >> > > 
    >> > > This is going into a function, so I'm hoping for a method that doesn't 
    >> > > require looking at the plot first.
    >> > >
    >> > > # sample code (simplified) #
    >> > > mp <- barplot(t(N.age), xlab = "Year", axisnames = FALSE)
    >> > > axis(side = 1, at = mp, labels = rownames(N.age), tcl = -0.75)
    >> > > 
    >> > > Thanks!
    >> > > 
    >> > > Mike Prager
    >> > > NOAA, Beaufort, NC
    >> > 
    >> > Mike,
    >> > 
    >> > How about something like this:
    >> > 
    >> >   mp <- barplot(1:50, axisnames = FALSE)
    >> > 
    >> >   # Create short tick marks at each bar
    >> >   axis(1, at = mp, labels = rep("", 50), tcl = -0.25)
    >> > 
    >> >   # Create longer tick marks every 5 years with labels
    >> >   axis(1, at = mp[seq(1, 50, 5)], 
    >> >        labels = 1900 + seq(0, 45, 5), tcl = -0.75, las = 2, 
    >> >        cex.axis = 0.75)
    >> > 
    >> > 
    >> > Just pick which labels you want to be shown (eg. every 5 years) and
    >> > synchronize the values of those with the 'at' argument in axis().
    >> > 
    >> > HTH,
    >> > 
    >> > Marc Schwartz
    >> > 
    >> 
    >> Thanks, Marc, for this solution and thanks equally to Jim Lemon
    >> for a similar idea.  This seems promising.  Since this is to go
    >> into a function (and should work without intervention), I'll
    >> need to devise an algorithm to decide at what interval the
    >> labels should be plotted.  Clearly "axis()" has such an
    >> algorithm.  Unfortunately, it reports its result only by placing
    >> the labels.
    >> 
    >> Mike

    Marc> Mike,

    Marc> To get a feel for how axis() creates the default tick positions when
    Marc> 'at' is the default NULL, see ?axTicks, which provides functionality
    Marc> similar to the internal C routine.

yes, partly not only similar but "the same" when it works (by
default) with par("axp")

    Marc> You could also look at ?pretty

Yes.  *However* there's one important thing which I think hasn't
been mentioned yet.

We have now been talking how and where axis() {i.e. its internal
code} chooses to place tick marks.
The clue is that it draws labels at all tick marks by default or
explicitly with axis(*, at= ., labels = .)
BUT the labels are not shown on your plot as soon as they
would get too much crammed together.

You can see this nicely, interactively, by the following:
Use
     graphics.off()
     plot(1:11)

This will show ticks *and* labels  at  2 , 4 , 6 , 8, 10
and now use your mouse, drag to make the graphics window
narrower (e.g. keeping height constant), in small steps,
releasing the mouse again to let R redraw the graphic.
For quite a while, the labels remain until there's not enough
room, the 5 ticks remain, but only the
 labels  2 , 4, 6, 8  are drawn
 then    2 , 6 , 10 
 then    2 , 6
 then    2 , 8  ( a little surprise to me)
 then    2

you always see all ticks but labels are only drawn when they
don't get in each other's way.

Of course, things like

 plot(1:11, xaxt="n")
 axis(1, at=1:11, labels = paste("Lab", 1:11))

show even more when the window is widened or narrowed,
and yes, it depends on the device and on the fonts and its
sizes, see e.g.,

 plot(1:11, xaxt="n")
 axis(1, at=1:11, labels = paste("Lab", 1:11), cex.axis = 0.5)

----- -----

If you don't like this --- almost always very desirable ---
builtin smartness, you need to descend slightly lower level and use
mtext(), e.g., the analogue of the above is

  plot(1:11, xaxt="n")
  mtext(side=1, at=1:11, text = paste("Lab", 1:11), cex = 0.5)

or rather leave traditional graphics which really gets messy in
such cases {par() .. etc} and upgrade to using the "grid"
package,  or also packages built on grid, "lattice" or "ggplot".
But infact, someone else needs to tell you how to play similar
goes with these.

Martin Maechler, ETH Zurich


From kaloytyn at cc.jyu.fi  Thu Mar 22 10:14:25 2007
From: kaloytyn at cc.jyu.fi (kaloytyn at cc.jyu.fi)
Date: Thu, 22 Mar 2007 11:14:25 +0200 (EET)
Subject: [R] repeated measures anova
Message-ID: <2591.130.234.6.53.1174554865.squirrel@webmail1.cc.jyu.fi>

Hello,

is there a function for repeated measures anova? Have searched for some
time and not found. Thank you very much for an answer.

Regards,
Katja L?ytynoja


From dimitri.mahieux at student.uclouvain.be  Thu Mar 22 10:34:04 2007
From: dimitri.mahieux at student.uclouvain.be (Dimitri Mahieux)
Date: Thu, 22 Mar 2007 10:34:04 +0100
Subject: [R] Integer Optimization
In-Reply-To: <OFDFBC8DEF.1F116AF1-ON862572A5.006A77C6-862572A5.006AAFA1@mmm.com>
References: <OFDFBC8DEF.1F116AF1-ON862572A5.006A77C6-862572A5.006AAFA1@mmm.com>
Message-ID: <46024D8C.4080300@student.uclouvain.be>

Hi,

I have tried to use lpSolve but I don't see how to ensure that the 
solution is in integer range. I use the solveLP function and there is 
none parameter that force the function to solve the problem with integer 
values.

Is there any other function to use ?

apjaworski at mmm.com wrote:
> Dimitri,
>
> I am assuming you mean integer linear optimization problem.  Try lpSolve.
> I think you will find it much easier to use.
>
> Cheers,
>
> Andy
>
> __________________________________
> Andy Jaworski
> 518-1-01
> Process Laboratory
> 3M Corporate Research Laboratory
> -----
> E-mail: apjaworski at mmm.com
> Tel:  (651) 733-6092
> Fax:  (651) 736-3122
>
>
>                                                                            
>              Mahieux Dimitri                                               
>              <dimitri.mahieux@                                             
>              student.uclouvain                                          To 
>              .be>                      r-help at stat.math.ethz.ch            
>              Sent by:                                                   cc 
>              r-help-bounces at st                                             
>              at.math.ethz.ch                                       Subject 
>                                        [R] Integer Optimization            
>                                                                            
>              03/21/2007 06:42                                              
>              AM                                                            
>                                                                            
>                                                                            
>                                                                            
>
>
>
>
> Hello everybody,
>
> I am looking for a function (or package) which can solve a integer
> optimization problem.  I have found the glpk package but I don't known
> very well how to use it.
> Someone has another solution ?
>
> thx
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>


From uli at biochem.dshs-koeln.de  Thu Mar 22 10:35:49 2007
From: uli at biochem.dshs-koeln.de (Ulrich Flenker)
Date: Thu, 22 Mar 2007 10:35:49 +0100 (CET)
Subject: [R] repeated measures anova
In-Reply-To: <2591.130.234.6.53.1174554865.squirrel@webmail1.cc.jyu.fi>
Message-ID: <Pine.LNX.4.44.0703221030550.20151-100000@ich.biochem.dshs-koeln.de>

On Thu, 22 Mar 2007 kaloytyn at cc.jyu.fi wrote:

> Hello,
> 
> is there a function for repeated measures anova? Have searched for some
> time and not found. Thank you very much for an answer.
> 
> Regards,
> Katja L?ytynoja

Katja,

this can be done by adding an Error() term.  Section 6.10
("Use Error() for repeated-measure ANOVA") in "Notes on the
use of R for psychology experiments and questionnaires"
written by Baron & Li gives a nice introduction. You will
find this document on <http://cran.r-project.org/>
"Documentation" -> "Contributed"

Regards

-- 
	Uli Flenker

	Institute of Biochemistry
	German Sport University Cologne
	Carl-Diem-Weg 6
	50933 Cologne

	+49(0)221/4982-5060


From maechler at stat.math.ethz.ch  Thu Mar 22 10:43:43 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 22 Mar 2007 10:43:43 +0100
Subject: [R] problem with RCurl install on Unix
In-Reply-To: <0BE438149FF2254DB4199E2682C8DFEB0235FB7B@crcmail1.BCCRC.CA>
References: <4601AD1E.30408@gmail.com>
	<0BE438149FF2254DB4199E2682C8DFEB0235FB7B@crcmail1.BCCRC.CA>
Message-ID: <17922.20431.96168.831263@stat.math.ethz.ch>

>>>>> "Steven" == Steven McKinney <smckinney at bccrc.ca>
>>>>>     on Wed, 21 Mar 2007 19:29:43 -0700 writes:

    Steven> I get the same problem, and haven't
    Steven> figured it out yet.

    Steven> Is it a 32bit/64bit clash?

Well, I can install and run  RCurl  on both 32bit and 64bit
(Redhat / FC6  Linux; with own compilers, extra libs, ...).

    Steven> (Similarly, I don't have RMySQL up and running cleanly.)




    >> library(RCurl)
    Steven> Error in dyn.load(x, as.logical(local), as.logical(now)) : 
    Steven> unable to load shared library '/share/apps/R/R-2.4.1/library/RCurl/libs/RCurl.so':
    Steven> libcurl.so.4: cannot open shared object file: No such file or directory
    Steven> Error: package/namespace load failed for 'RCurl'

You might need to set  LD_LIBRARY_PATH  correctly
before starting R -- typically it would be set the same as it
was when RCurl was installed (which includes a 'configure' !) on
your system?  Or  RCurl's configure is not quite robust enough
and did not check for the presence of a libcurl.so.4

I assume

  ldd /share/apps/R/R-2.4.1/library/RCurl/libs/RCurl.so

also tells about the missing  libcurl.so.4 ?
Make sure you find that (in /usr/lib; /usr/local/lib, ... ?)
and then set your LD_LIBRARY_PATH
or even   'ldconfig' as root to make sure that libcurl.so.4 ``is
found''.
IMO the latter {correct ldconfig call / /etc/ld.so.conf setup}
should have happened as part of the installation of the curl
library.

On the 64-bit architecture, note that

 > system(paste("ldd", dir(system.file("libs", package = "RCurl"), full=TRUE)))

finds all libraries in /lib64 and /usr/lib64 .


Martin Maechler, ETH Zurich


    >> sessionInfo()
    Steven> R version 2.4.1 (2006-12-18) 
    Steven> x86_64-unknown-linux-gnu 

    Steven> locale:
    Steven> LC_CTYPE=en_US.iso885915;LC_NUMERIC=C;LC_TIME=en_US.iso885915;LC_COLLATE=en_US.iso885915;LC_MONETARY=en_US.iso885915;LC_MESSAGES=en_US.iso885915;LC_PAPER=en_US.iso885915;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.iso885915;LC_IDENTIFICATION=C

    Steven> attached base packages:
    Steven> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"  
    Steven> [7] "base"     

    Steven> other attached packages:
    Steven> DBI 
    Steven> "0.1-12" 
    >> 

    Steven> Steven McKinney

    Steven> Statistician
    Steven> Molecular Oncology and Breast Cancer Program
    Steven> British Columbia Cancer Research Centre

    Steven> email: smckinney at bccrc.ca

    Steven> tel: 604-675-8000 x7561

    Steven> BCCRC
    Steven> Molecular Oncology
    Steven> 675 West 10th Ave, Floor 4
    Steven> Vancouver B.C. 
    Steven> V5Z 1L3
    Steven> Canada




    Steven> -----Original Message-----
    Steven> From: r-help-bounces at stat.math.ethz.ch on behalf of Mark W Kimpel
    Steven> Sent: Wed 3/21/2007 3:09 PM
    Steven> To: r-help at stat.math.ethz.ch
    Steven> Subject: [R] problem with RCurl install on Unix
 
    Steven> I am having trouble getting an install of RCurl to work properly on a 
    Steven> Unix server. The steps I have taken are:

    Steven> 1. installed cUrl from source without difficulty
    Steven> 2. installed RCurl from source using the command
    Steven> ~/R_HOME/R-devel/bin/R CMD INSTALL -l ~/R_HOME/R-devel/library 
    Steven> ~/RCurl_0.8-0.tar.gz     I received no errors during this install
    Steven> 3. when I go back to R and require(RCurl), I get

    >> require(RCurl)
    Steven> Loading required package: RCurl
    Steven> Error in dyn.load(x, as.logical(local), as.logical(now)) :
    Steven> unable to load shared library 
    Steven> '/N/u/mkimpel/BigRed/R_HOME/R-devel/library/RCurl/libs/RCurl.so':
    Steven> libcurl.so.4: cannot open shared object file: No such file or directory
    Steven> ?1? FALSE

    Steven> Outside of R I get

    Steven> mkimpel?BigRed:?/R_HOME/R-devel/library/RCurl/libs> ls
    Steven> RCurl.so

    Steven> I can cat into RCurl and I have even done chmod a+x RCurl.so in case 
    Steven> there was a problem with permission for R to open the file.

    Steven> Below is my sessionInfo. Thanks, Mark

    >> sessionInfo()
    Steven> R version 2.5.0 Under development (unstable) (2007-03-11 r40824)
    Steven> powerpc64-unknown-linux-gnu

    Steven> locale:
    Steven> LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C

    Steven> attached base packages:
    Steven> ?1? "stats"     "graphics"  "grDevices" "datasets"  "utils"     "tools"
    Steven> ?7? "methods"   "base"

    Steven> other attached packages:
    Steven> limma      affy    affyio   Biobase
    Steven> "2.9.13" "1.13.14"   "1.3.3" "1.13.39"
    >> 



    Steven> -- 
    Steven> Mark W. Kimpel MD
    Steven> Neuroinformatics
    Steven> Department of Psychiatry
    Steven> Indiana University School of Medicine

    Steven> ______________________________________________
    Steven> R-help at stat.math.ethz.ch mailing list
    Steven> https://stat.ethz.ch/mailman/listinfo/r-help
    Steven> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    Steven> and provide commented, minimal, self-contained, reproducible code.

    Steven> ______________________________________________
    Steven> R-help at stat.math.ethz.ch mailing list
    Steven> https://stat.ethz.ch/mailman/listinfo/r-help
    Steven> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    Steven> and provide commented, minimal, self-contained, reproducible code.


From jfox at mcmaster.ca  Thu Mar 22 01:59:10 2007
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 21 Mar 2007 20:59:10 -0400
Subject: [R] how to get "lsmeans"?
In-Reply-To: <Pine.LNX.4.64.0703211502050.24076@gannet.stats.ox.ac.uk>
Message-ID: <20070322005910.VVAT1624.tomts40-srv.bellnexxia.net@JohnDesktop8300>

Dear Brian et al.,

My apologies for chiming in late: It's been a busy day.

First some general comments on "least-squares means" and "effect displays."
The general idea behind the two is similar -- to examine fitted values
corresponding to a term in a model while holding other terms to typical
values -- but the implementation is not identical. There are also other
similar ideas floating around as well. My formulation is more general in the
sense that it applies to a wider variety of models, both linear and
otherwise.

"Least-squares means" (a horrible term, by the way: in a 1980 paper in the
American Statistician, Searle, Speed, and Milliken suggested the more
descriptive term "population marginal means") apply to factors and
combinations of factors; covariates are set to mean values and the levels of
other factors are averaged over, in effect applying equal weight to each
level. (This is from memory, so it's possible that I'm not getting it quite
right, but I believe that I am.) In my effect displays, each level of a
factor is weighted by its proportion in the data. In models in which
least-squares means can be computed, they should differ from the
corresponding effect display by a constant (if there are different numbers
of observations in the different levels of the factors that are held
constant).

The obstacle to computing either least-squares means or effect displays in R
via predict() is that predict() wants factors in the "new data" to be set to
particular levels. The effect() function in the effects package bypasses
predict() and works directly with the model matrix, averaging over the
columns that pertain to a factor (and reconstructing interactions as
necessary). As mentioned, this has the effect of setting the factor to its
proportional distribution in the data. This approach also has the advantage
of being invariant with respect to the choice of contrasts for a factor.

The only convenient way that I can think of to implement least-squares means
in R would be to use deviation-coded regressors for a factor (that is,
contr.sum) and then to set the columns of the model matrix for the factor(s)
to be averaged over to 0. It may just be that I'm having a failure of
imagination and that there's a better way to proceed. I've not implemented
this solution because it is dependent upon the choice of contrasts and
because I don't see a general advantage to it, but since the issue has come
up several times now, maybe I should take a crack at it. Remember that I
want this to work more generally, not just for levels of factors, and not
just for linear models.

Brian is quite right in mentioning that he suggested some time ago that I
use critical values of t rather than of the standard normal distribution for
producing confidence intervals, and I agree that it makes sense to do so in
models in which the dispersion is estimated. My only excuse for not yet
doing this is that I want to undertake a more general revision of the
effects package, and haven't had time to do it. There are several changes
that I'd like to make to the package. For example, I have results for
multinomial and proportional odds logit models (described in a paper by me
and Bob Andersen in the 2006 issue of Sociological Methodology) that I want
to incorporate, and I'd like to improve the appearance of the default
graphs. But Brian's suggestion is very straightforward, and I guess that I
shouldn't wait to implement it; I'll do so very soon.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof 
> Brian Ripley
> Sent: Wednesday, March 21, 2007 12:03 PM
> To: Chuck Cleland
> Cc: r-help
> Subject: Re: [R] how to get "lsmeans"?
> 
> On Wed, 21 Mar 2007, Chuck Cleland wrote:
> 
> > Liaw, Andy wrote:
> >> I verified the result from the following with output from JMP 6 on 
> >> the same data (don't have SAS: don't need it):
> >>
> >> set.seed(631)
> >> n <- 100
> >> dat <- data.frame(y=rnorm(n), A=factor(sample(1:2, n, 
> replace=TRUE)),
> >>                   B=factor(sample(1:2, n, replace=TRUE)),
> >>                   C=factor(sample(1:2, n, replace=TRUE)),
> >>                   d=rnorm(n))
> >> fm <- lm(y ~ A + B + C + d, dat)
> >> ## Form a data frame of points to predict: all 
> combinations of the ## 
> >> three factors and the mean of the covariate.
> >> p <- data.frame(expand.grid(A=1:2, B=1:2, C=1:2)) p[] <- lapply(p, 
> >> factor) p <- cbind(p, d=mean(dat$d)) p <- 
> cbind(yhat=predict(fm, p), 
> >> p) ## lsmeans for the three factors:
> >> with(p, tapply(yhat, A, mean))
> >> with(p, tapply(yhat, B, mean))
> >> with(p, tapply(yhat, C, mean))
> >
> >  Using Andy's example data, these are the LSMEANS and 
> intervals I get 
> > from SAS:
> >
> > A        y LSMEAN      95% Confidence Limits
> > 1       -0.071847       -0.387507     0.243813
> > 2       -0.029621       -0.342358     0.283117
> >
> > B        y LSMEAN      95% Confidence Limits
> > 1       -0.104859       -0.397935     0.188216
> > 2        0.003391       -0.333476     0.340258
> >
> > C        y LSMEAN      95% Confidence Limits
> > 1       -0.084679       -0.392343     0.222986
> > 2       -0.016789       -0.336374     0.302795
> >
> >  One way of reproducing the LSMEANS and intervals from SAS using
> > predict() seems to be the following:
> >
> >> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d, 
> data = dat) 
> >> newdat <- expand.grid(A=factor(c(1,2)),B=1.5,C=1.5,d=mean(dat$d))
> >> cbind(newdat, predict(dat.lm, newdat, interval="confidence"))
> >  A   B   C          d         fit        lwr       upr
> > 1 1 1.5 1.5 0.09838595 -0.07184709 -0.3875070 0.2438128
> > 2 2 1.5 1.5 0.09838595 -0.02962086 -0.3423582 0.2831165
> >
> >  However, another possibility seems to be:
> >
> >> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d, 
> data = dat) 
> >> newdat <-
> > 
> expand.grid(A=factor(c(1,2)),B=mean(as.numeric(dat$B)),C=mean(as.numer
> > ic(dat$C)),d=mean(dat$d))
> >> cbind(newdat, predict(dat.lm, newdat, interval="confidence"))
> >  A    B    C          d         fit        lwr       upr
> > 1 1 1.43 1.48 0.09838595 -0.08078243 -0.3964661 0.2349012
> > 2 2 1.43 1.48 0.09838595 -0.03855619 -0.3479589 0.2708465
> >
> >  The predictions directly above match what effect() in the effects 
> > package by John Fox returns:
> >
> > library(effects)
> >
> >> effect("A", fm, xlevels=list(d = mean(dat$D)))
> >
> > A effect
> > A
> >          1           2
> > -0.08078243 -0.03855619
> >
> >  But for some reason the predict() and effect() intervals 
> are a little
> > different:
> >
> >> effect("A", fm, xlevels=list(d = mean(dat$D)))$lower
> >          [,1]
> > 101 -0.3924451
> > 102 -0.3440179
> >
> >> effect("A", fm, xlevels=list(d = mean(dat$D)))$upper
> >         [,1]
> > 101 0.2308802
> > 102 0.2669055
> >
> >  I would be interested in any comments on these different 
> approaches 
> > and on the difference in intervals returned by predict() 
> and effect().
> 
> AFAIR, the effects packages uses normal-based confidence 
> intervals and predict.lm uses t-based ones, and I have 
> suggested to John Fox that t-based intervals would be 
> preferable, at least as an option.
> 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From L.Au at ms.unimelb.edu.au  Thu Mar 22 07:28:27 2007
From: L.Au at ms.unimelb.edu.au (Leon Au)
Date: Thu, 22 Mar 2007 17:28:27 +1100
Subject: [R] dsymstb-density of symmetric stable
Message-ID: <6.1.1.1.2.20070322172357.02f605f0@mailhost.ms.unimelb.edu.au>


Dear R-User,

In regard to the fBasics package that contains some functions relating to 
the stable distribution, why is it that the integral under the 
dsymstb(x,alpha=1.5) isn't equal to 1?  Have I missed something or is there 
some minor problem?

However, if one use dstable(...), then it does the job.

I hope someone could clarify this problem.

Thanks,
Leon Au


From aurelie_bocher at hotmail.com  Thu Mar 22 10:09:42 2007
From: aurelie_bocher at hotmail.com (Aurelie bocher)
Date: Thu, 22 Mar 2007 10:09:42 +0100
Subject: [R] accounting for overdispersion in poisson distribution with lmer
	procedure
Message-ID: <BAY102-F2598D607C9422EFB9E8CDDE26B0@phx.gbl>

Hello,

I am analysing counts data with a mixed model using lmer procedure. I 
therefore use the quasipoisson distribution but I'm not sure if this is 
sufficient to account for overdispersion. Actually the results are not very 
different to what I get when specifying a poisson distribution although my 
data are clearly overdispersed.

this my model:
>model <- lmer(NB ~ T + volume + (G|col) , data=immuno15, 
>family=quasipoisson)
where
- NB is bacterial count in haemolymph of ants 10h after bacteria 
injection(measure of immunocompetence)
- T is the treatment applied to ants
- volume is the volume of haemolymph sample from ants
- col is the colony of origin of ants, which is a random factor

I know it's possible to get a estimator of overdispersion with:
>sum.model <- summary(model)
>sum.model at sigma

but I don't know how to interpret this scale parameter.

Does anyone could help me?
thanks a lot,
Aurelie Bocher

_________________________________________________________________

mobile comme sur PC ! http://mobile.live.fr/messenger/bouygues/


From ripley at stats.ox.ac.uk  Thu Mar 22 12:25:05 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 22 Mar 2007 11:25:05 +0000 (GMT)
Subject: [R] problem with RCurl install on Unix
In-Reply-To: <17922.20431.96168.831263@stat.math.ethz.ch>
References: <4601AD1E.30408@gmail.com>
	<0BE438149FF2254DB4199E2682C8DFEB0235FB7B@crcmail1.BCCRC.CA>
	<17922.20431.96168.831263@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0703221115560.19977@auk.stats>

On Thu, 22 Mar 2007, Martin Maechler wrote:

>>>>>> "Steven" == Steven McKinney <smckinney at bccrc.ca>
>>>>>>     on Wed, 21 Mar 2007 19:29:43 -0700 writes:
>
>    Steven> I get the same problem, and haven't
>    Steven> figured it out yet.
>
>    Steven> Is it a 32bit/64bit clash?
>
> Well, I can install and run  RCurl  on both 32bit and 64bit
> (Redhat / FC6  Linux; with own compilers, extra libs, ...).
>
>    Steven> (Similarly, I don't have RMySQL up and running cleanly.)
>
>
>
>
>    >> library(RCurl)
>    Steven> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>    Steven> unable to load shared library '/share/apps/R/R-2.4.1/library/RCurl/libs/RCurl.so':
>    Steven> libcurl.so.4: cannot open shared object file: No such file or directory
>    Steven> Error: package/namespace load failed for 'RCurl'
>
> You might need to set  LD_LIBRARY_PATH  correctly
> before starting R -- typically it would be set the same as it
> was when RCurl was installed (which includes a 'configure' !) on
> your system?  Or  RCurl's configure is not quite robust enough
> and did not check for the presence of a libcurl.so.4
>
> I assume
>
>  ldd /share/apps/R/R-2.4.1/library/RCurl/libs/RCurl.so

You may well need

R CMD ldd /share/apps/R/R-2.4.1/library/RCurl/libs/RCurl.so

as the R front end sets LD_LIBRARY_PATH.

> also tells about the missing  libcurl.so.4 ?
> Make sure you find that (in /usr/lib; /usr/local/lib, ... ?)
> and then set your LD_LIBRARY_PATH
> or even   'ldconfig' as root to make sure that libcurl.so.4 ``is
> found''.
> IMO the latter {correct ldconfig call / /etc/ld.so.conf setup}
> should have happened as part of the installation of the curl
> library.

I find it very common for software packages to install into 
/usr/local/lib (not lib64) and not to think about the ldconfig paths.
Both are things that RPMs tend to correct.

> On the 64-bit architecture, note that
>
> > system(paste("ldd", dir(system.file("libs", package = "RCurl"), full=TRUE)))
>
> finds all libraries in /lib64 and /usr/lib64 .

I think you mean on Redhat-based AMD64 Linux.  This is something that 
differs by 64-bit architecture (ia64 uses /lib) and also Linux distro (at 
least in the past).

RMySQL does not install out of the box on any of our 64-bit systems 
(another lib vs lib64 issue), and I've sent patches to the maintainer.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Judith.Schiller at erstebank.at  Thu Mar 22 12:39:12 2007
From: Judith.Schiller at erstebank.at (Schiller Judith 1541 EB)
Date: Thu, 22 Mar 2007 12:39:12 +0100
Subject: [R] difftime / RBloomberg
Message-ID: <1FEE8D76646C374CA2C7B81153580B77649AB9@m0142.s-mxs.net>

hi,

I've troubles with some difftime objects. e.g.

ISOdate(2001, 4, 26) - ISOdate(2001, 2, 26) - 2

works, telling me "Time difference of 57 days". But when I'd like to add
days, such as

ISOdate(2001, 4, 26) - ISOdate(2001, 2, 26) + 2

the function gives me an error. Function "as.COMDate.chron" of the
Rbloomberg package doesn't work for that reason.
I'm running R on a Windows machine, R version 2.4.1.

Thanks, Judith


From ripley at stats.ox.ac.uk  Thu Mar 22 12:58:22 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 22 Mar 2007 11:58:22 +0000 (GMT)
Subject: [R] difftime / RBloomberg
In-Reply-To: <1FEE8D76646C374CA2C7B81153580B77649AB9@m0142.s-mxs.net>
References: <1FEE8D76646C374CA2C7B81153580B77649AB9@m0142.s-mxs.net>
Message-ID: <Pine.LNX.4.64.0703221153270.27006@auk.stats>

On Thu, 22 Mar 2007, Schiller Judith 1541 EB wrote:

> hi,
>
> I've troubles with some difftime objects. e.g.
>
> ISOdate(2001, 4, 26) - ISOdate(2001, 2, 26) - 2
>
> works, telling me "Time difference of 57 days". But when I'd like to add
> days, such as
>
> ISOdate(2001, 4, 26) - ISOdate(2001, 2, 26) + 2
>
> the function gives me an error.

The error being?  (It does not give me an error.)

>  Function "as.COMDate.chron" of the
> Rbloomberg package doesn't work for that reason.
> I'm running R on a Windows machine, R version 2.4.1.

I cross-checked on that system.

It is quite possible the error is due to some other package that you are 
using: that's why we ask for the result of sessionInfo().

I would expect

(ISOdate(2001, 4, 26) - ISOdate(2001, 2, 26)) - (-2)

to be a workaround, BTW.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From koen.hufkens at ua.ac.be  Thu Mar 22 13:14:57 2007
From: koen.hufkens at ua.ac.be (Hufkens Koen)
Date: Thu, 22 Mar 2007 13:14:57 +0100
Subject: [R] non-linear curve fitting
Message-ID: <832A948B92E5754D9062CCF62F9ED892655FC8@xmail01.ad.ua.ac.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070322/896bd90d/attachment.pl 

From a-urban at ti.com  Thu Mar 22 13:49:14 2007
From: a-urban at ti.com (Urban, Alexander)
Date: Thu, 22 Mar 2007 13:49:14 +0100
Subject: [R] Truncated x-axis values
Message-ID: <1266A6320AD3884EA8142B354DFBA31A06EB0122@dfre02.ent.ti.com>

Hello 

I'm new to this group. I looked up the last two hour in the help file
and in the archives of this group, but didn't find anything.
I hope my question is not too dump:
I'm printing a graph with vertical labels on the x-axis (necessary due
to many labels). Unfortunately the png truncates the labels halfway
through, so that you can only read the last 7 digits of the label.
Snice I'm already asking :-): Is there a possibility to tell R: If there
are so many labels that you write them on top of each other, take only
e.g. every 2nd...

Sorry for bothering and thanks
Alex


From maitra at iastate.edu  Thu Mar 22 13:53:29 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Thu, 22 Mar 2007 07:53:29 -0500
Subject: [R] Is this an appropriate credit (Re: question on suppressing
 error messages with Rmath library)
In-Reply-To: <20070321125551.2bec9de9@subarnarekha.stat.iastate.edu>
References: <20070321083826.5fbd12c1@triveni.stat.iastate.edu>
	<Pine.LNX.4.64.0703210955560.4457@itasca2.wildberry.org>
	<20070321125551.2bec9de9@subarnarekha.stat.iastate.edu>
Message-ID: <20070322075329.5fe3575f@triveni.stat.iastate.edu>

Dear list,

As a followup to my post here yesterday, I was wondering if this is an appropriate enough credit for the piece of code if experiments show promise and  I eventually make this publicly available as part of some other code to interested researchers and practitioners?

I am modifying pnt and dnt to serve my purpose of switching off low-precision warnings.


/* This program calculates the density and c.d.f of the non-central t 
   distribution. It is the same as the functions dnt and pnt in the R 
   mathemtical library, and indeed is copied with minor modifications
   from them. The modification is to stop warning messages: the modified
   functions are called pnoncentralt and dnoncentralt with the same arguments
   as pnt and dnt. 

  The reason for this separate function is to get around the warnings
  of low precision that come in when we use the R math functions. This
  can slow down the program considerably if there is a huge number of
  calls. Please use it responsibly.

  The function uses R's standalone mathematical library, hence the renaming 
  to avoid conflicts with pnt and dnt.

  Modified by Ranjan Maitra, Ames, IA 50014, USA. 2007/03//21.

  Since this is really a minor modification, all credits should go the R team 
  below. 
 
  If you modify this function in order to get around an undiscovered bug or to 
  speed it up/make it more accurate, please let me know. 

*/


I make no changes to any further acknowledgment comments that came with the pnt.c and dnt.c source files.

Is this good enough? Am I missing something I should be including?

Many thanks and best wishes,
Ranjan



On Wed, 21 Mar 2007 12:55:51 -0500 Ranjan Maitra <maitra at iastate.edu> wrote:

> Hi Luke,
> 
> Thanks! Sorry, my error which I did not realize until after sending out the program. I think I will just extricate the pnt code and compile that separately and that should be fine.
> 
> Thanks very much again, to you and everybody else who replied.
> 
> Best wishes,
> Ranjan
> 
> 
> On Wed, 21 Mar 2007 10:04:05 -0500 (CDT) Luke Tierney <luke at stat.uiowa.edu> wrote:
> 
> > You might get less noise in the replies if you were explicit about
> > using Rmath stand-alone and asked on r-devel.
> > 
> > As far as I can see you would need to compile a version of the
> > stand-alone library that defines the macros for handling of warning
> > messages differently -- the current one just calls printf in the
> > stand-alone library.  (You might be able to trick the linker into using
> > a version of printf for calls from within Rmath that does nothing, but
> > I suspect recompiling the sourses is easier.)  We will probably be
> > rethinking this soon in conjunction with some other changes to
> > vectorized math in R.
> > 
> > 
> > Best,
> > 
> > luke
> > 
> > On Wed, 21 Mar 2007, Ranjan Maitra wrote:
> > 
> > > Dear list,
> > >
> > > I have been using the Rmath library for quite a while: in the current instance, I am calling dnt (non-central t density function) repeatedly for several million. When the argument is small, I get the warning message:
> > >
> > > full precision was not achieved in 'pnt'
> > >
> > > which is nothing unexpected. (The density calls pnt, if you look at the function dnt.) However, to have this happen a huge number of times, when the optimizer is churning through the dataset is bothersome, but more importantly, a bottleneck in terms of speed. Is it possible to switch this off? Is there an setting somewhere that I am missing?
> > >
> > > Many thanks and best wishes,
> > > Ranjan
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > 
> > -- 
> > Luke Tierney
> > Chair, Statistics and Actuarial Science
> > Ralph E. Wareham Professor of Mathematical Sciences
> > University of Iowa                  Phone:             319-335-3386
> > Department of Statistics and        Fax:               319-335-3017
> >     Actuarial Science
> > 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
> > Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
> > 
>


From valerie566 at barnhallrfc.com  Thu Mar 22 13:51:28 2007
From: valerie566 at barnhallrfc.com (Filmer Cannon)
Date: Thu, 22 Mar 2007 15:51:28 +0300
Subject: [R] =?windows-1251?b?08/QwMLLxc3IxSDCzdPS0MXNzcjMIMrOzdLQzsvFzA==?=
Message-ID: <f62301c76c80$72858255$b8a0845f@barnhallrfc.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070322/aef4f5b3/attachment.pl 

From ggrothendieck at gmail.com  Thu Mar 22 13:56:18 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 22 Mar 2007 08:56:18 -0400
Subject: [R] dynamic linear models in R
In-Reply-To: <4b82d65b0703212058na81fd62uc5c49f5cec51962f@mail.gmail.com>
References: <4b82d65b0703212058na81fd62uc5c49f5cec51962f@mail.gmail.com>
Message-ID: <971536df0703220556o56ba89l90df9bee23e1181@mail.gmail.com>

Package dynlm (and dyn) are used to align the time series in the dependent
and independent portions of the equations so that one can perform regressions
on lagged and differenced versions of the dependent and independent variables.
They compensate for the fact that lm (and in the case of dyn lm, glm,
rq and others)
do not align time series.

library(dyn)
set.seed(1)
z <- ts(rnorm(25))

# this does NOT work since time series are not aligned
lm(z ~ lag(z, -1))

# regress z on lagged version of itself - ok
dyn$lm(z ~ lag(z, -1))

On 3/21/07, Johann Hibschman <johannh at gmail.com> wrote:
> Hi all,
>
> I've just started working my way through Mike West and Jeff Harrison's
> _Bayesian Forecasting and Dynamic Models_, and I was wondering if
> there were any publically-available packages to handle dynamic linear
> models, as they describe.
>
> I found the "dynlm" package, but either I don't yet understand what's
> going on or that package uses a different sense of the phrase "dynamic
> linear model."  I would expect a fit of a dynamic linear model to
> produce a time series of parameter estimates, not just single
> coefficients as that function seems to generate.
>
> Could anyone help me understand what's going on here?
>
> Thanks,
>
> Johann
>
> P.S. What I really want to do is fit a linear regression of the form
> dz_t ~ 0 + dx_t + dy_t, but where the coefficients of dx and dy are
> allowed to slowly evolve over time.  DLMs seem appropriate to this,
> but I'm open to any other suggestions, as I've not found much support
> for DLMs in R.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From phgrosjean at sciviews.org  Thu Mar 22 14:01:58 2007
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 22 Mar 2007 14:01:58 +0100
Subject: [R] non-linear curve fitting
In-Reply-To: <832A948B92E5754D9062CCF62F9ED892655FC8@xmail01.ad.ua.ac.be>
References: <832A948B92E5754D9062CCF62F9ED892655FC8@xmail01.ad.ua.ac.be>
Message-ID: <46027E46.8090701@sciviews.org>

Hello,

If a least-square criterion is fine for you, you should use nls(). For 
the logistic curve, you have a convenient self-starting model available: 
SSlogis(). Look at:

?nls
?SSlogis

Best,

Philippe Grosjean

..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Belgium
( ( ( ( (
..............................................................

Hufkens Koen wrote:
> Hi list,
>  
> I have a little curve fitting problem.
>  
> I would like to fit a sigmoid curve to my data using the following equation:
>  
> f(x) = 1/(1 + exp(-(x-c)*b)) (or any other form for that matter)
>  
> Where x is the distance/location within the dataframe, c is the shift of the curve across the dataframe and b is the steepness of the curve.
>  
> I've been playing with glm() and glm.fit() but without any luck.
>  
> for example the most simple example
>  
> x = -10:10
> y = 1/(1 + exp(-x))
> glm(y ~ x, family=binomial(link="logit"))
>  
> I get a warning:
> non-integer #successes in a binomial glm! in: eval(expr, envir, enclos) 
>  
> and some erratic results
>  
> This is the most simple test to see if I could fit a curve to this perfect data so since this didn't work out, bringing in the extra parameters is a whole other ballgame so could someone give me a clue?
>  
> Kind regards,
> Koen
>  
>


From luke at stat.uiowa.edu  Thu Mar 22 14:02:33 2007
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Thu, 22 Mar 2007 08:02:33 -0500 (CDT)
Subject: [R] Snow Package and SPRNG:  Will it solve my problem?
In-Reply-To: <20070321205520.c7d3ekiio4skwk00@mail1.maine.edu>
References: <20070321205520.c7d3ekiio4skwk00@mail1.maine.edu>
Message-ID: <Pine.LNX.4.64.0703220800110.20383@nokomis.stat.uiowa.edu>

On Wed, 21 Mar 2007, robert.robinson at maine.edu wrote:

> Hello and thanks in advance for your time.  I currently have a
> simulation running on my cluster with the help of snow that relies on
> global variables being changes regularly  to random values.  It uses
> these values, lets call them x1 x2 and x3, in custom functions for
> logliklyhood and score that gets used in the standard optim function.
> To get set in the global table on the different nodes I'm generating
> the random value in a function on the node and then using the
> superassign operator ( <<- ) to set it to the global variable.
> (eg:
>    temp = rand(n1)
>    x1 <<- sort(temp)
> )
>
> ) I'm worried that this is creating a lot of avoidable message
> passing.  Here are my questions:
>
> Does the superassign operator set the global variable on the head
> node, like I believe it does, or rather does it only set it on the
> local global table?

There is not shared global environments eath node has its own separate
from the master's.

>
> Does the SPRNG package offer a viable replacement for useless message
> passing of random values like this?
>

rsprng and rlecuyer (the current default) address a different issue --
ensuring (reproducible if necessary) independent streams on the nodes.

Best,

luke

> Thanks again for your continued help with my problems.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From judith.schiller at soundinvest.net  Thu Mar 22 14:14:25 2007
From: judith.schiller at soundinvest.net (Judith Schiller)
Date: Thu, 22 Mar 2007 14:14:25 +0100
Subject: [R] difftime / RBloomberg
In-Reply-To: <1FEE8D76646C374CA2C7B81153580B7702C0ABD9@m0142.s-mxs.net>
References: <1FEE8D76646C374CA2C7B81153580B77649AB9@m0142.s-mxs.net>
	<1FEE8D76646C374CA2C7B81153580B7702C0ABD9@m0142.s-mxs.net>
Message-ID: <46028131.9030002@soundinvest.net>

The error is "evaluation nested too deeply: infinite recursion / 
options(expressions=)?", and sessionInfo() results in

R version 2.4.1 (2006-12-18)
i386-pc-mingw32

locale:
LC_COLLATE=German_Austria.1252;LC_CTYPE=German_Austria.1252;LC_MONETARY=German_Austria.1252;LC_NUMERIC=C;LC_TIME=German_Austria.1252

attached base packages:
[1] "splines"   "stats"     "graphics"  "grDevices" "datasets"  
"tcltk"     "utils"     "methods"   "base"    

other attached packages:
      RODBC  RBloomberg       chron         zoo RDCOMClient        
MASS      xtable    survival    svSocket        svIO      R2HTML      
svMisc
    "1.1-7"    "0.1-10"     "2.3-8"     "1.2-1"    "0.91-0"    
"7.2-30"     "1.4-2"      "2.30"     "0.9-5"     "0.9-5"      "1.58"     
"0.9-5"
      svIDE
    "0.9-5"

The workaround is a nice idea, but as the problem arisis in 
"as.COMDate.chron", other users of this function might be affected as well.

Prof Brian Ripley schrieb:
> On Thu, 22 Mar 2007, Schiller Judith 1541 EB wrote:
>
>   
>> hi,
>>
>> I've troubles with some difftime objects. e.g.
>>
>> ISOdate(2001, 4, 26) - ISOdate(2001, 2, 26) - 2
>>
>> works, telling me "Time difference of 57 days". But when I'd like to add
>> days, such as
>>
>> ISOdate(2001, 4, 26) - ISOdate(2001, 2, 26) + 2
>>
>> the function gives me an error.
>>     
>
> The error being?  (It does not give me an error.)
>
>   
>>  Function "as.COMDate.chron" of the
>> Rbloomberg package doesn't work for that reason.
>> I'm running R on a Windows machine, R version 2.4.1.
>>     
>
> I cross-checked on that system.
>
> It is quite possible the error is due to some other package that you are 
> using: that's why we ask for the result of sessionInfo().
>
> I would expect
>
> (ISOdate(2001, 4, 26) - ISOdate(2001, 2, 26)) - (-2)
>
> to be a workaround, BTW.
>
>


From ripley at stats.ox.ac.uk  Thu Mar 22 14:29:40 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 22 Mar 2007 13:29:40 +0000 (GMT)
Subject: [R] difftime / RBloomberg
In-Reply-To: <46028131.9030002@soundinvest.net>
References: <1FEE8D76646C374CA2C7B81153580B77649AB9@m0142.s-mxs.net>
	<1FEE8D76646C374CA2C7B81153580B7702C0ABD9@m0142.s-mxs.net>
	<46028131.9030002@soundinvest.net>
Message-ID: <Pine.LNX.4.64.0703221319290.10909@gannet.stats.ox.ac.uk>

What you need to do is to isolate the package giving the problem and 
send a report to its maintainer.  It is not RODBC nor chron, and 
RBloomberg is not available pre-compiled from CRAN.

On Thu, 22 Mar 2007, Judith Schiller wrote:

> The error is "evaluation nested too deeply: infinite recursion / 
> options(expressions=)?", and sessionInfo() results in
>
> R version 2.4.1 (2006-12-18)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=German_Austria.1252;LC_CTYPE=German_Austria.1252;LC_MONETARY=German_Austria.1252;LC_NUMERIC=C;LC_TIME=German_Austria.1252
>
> attached base packages:
> [1] "splines"   "stats"     "graphics"  "grDevices" "datasets"  "tcltk" 
> "utils"     "methods"   "base" 
> other attached packages:
>     RODBC  RBloomberg       chron         zoo RDCOMClient        MASS 
> xtable    survival    svSocket        svIO      R2HTML      svMisc
>   "1.1-7"    "0.1-10"     "2.3-8"     "1.2-1"    "0.91-0"    "7.2-30" 
> "1.4-2"      "2.30"     "0.9-5"     "0.9-5"      "1.58"     "0.9-5"
>     svIDE
>   "0.9-5"
>
> The workaround is a nice idea, but as the problem arisis in 
> "as.COMDate.chron", other users of this function might be affected as well.
>
> Prof Brian Ripley schrieb:
>> On Thu, 22 Mar 2007, Schiller Judith 1541 EB wrote:
>>
>> 
>>> hi,
>>> 
>>> I've troubles with some difftime objects. e.g.
>>> 
>>> ISOdate(2001, 4, 26) - ISOdate(2001, 2, 26) - 2
>>> 
>>> works, telling me "Time difference of 57 days". But when I'd like to add
>>> days, such as
>>> 
>>> ISOdate(2001, 4, 26) - ISOdate(2001, 2, 26) + 2
>>> 
>>> the function gives me an error.
>>> 
>> 
>> The error being?  (It does not give me an error.)
>>
>>
>>>  Function "as.COMDate.chron" of the
>>> Rbloomberg package doesn't work for that reason.
>>> I'm running R on a Windows machine, R version 2.4.1.
>>> 
>> 
>> I cross-checked on that system.
>> 
>> It is quite possible the error is due to some other package that you are 
>> using: that's why we ask for the result of sessionInfo().
>> 
>> I would expect
>> 
>> (ISOdate(2001, 4, 26) - ISOdate(2001, 2, 26)) - (-2)
>> 
>> to be a workaround, BTW.
>>
>>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From muenchen at utk.edu  Thu Mar 22 14:35:57 2007
From: muenchen at utk.edu (Muenchen, Robert A (Bob))
Date: Thu, 22 Mar 2007 09:35:57 -0400
Subject: [R] how to get "lsmeans"?
In-Reply-To: <20070322005910.VVAT1624.tomts40-srv.bellnexxia.net@JohnDesktop8300>
References: <Pine.LNX.4.64.0703211502050.24076@gannet.stats.ox.ac.uk>
	<20070322005910.VVAT1624.tomts40-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <D028EEB4CA113D4EAFDD485CCC998277731795@UTKFSVS4.utk.tennessee.edu>

Hi All,

Perhaps I'm stating the obvious, but to increase the use of R in places
where SAS & SPSS dominate, it's important to make getting the same
answers as easy as possible. That includes things like lsmeans and type
III sums of squares. I've read lots of discussions here on sums of
squares & I'm not advocating type III use, just looking at it from a
marketing perspective. Too many people look for excuses to not change.
The fewer excuses, the better.

Of course this is easy for me to say, as I'm not the one who does the
work! Much thanks to those who do.

Cheers,
Bob

=========================================================
  Bob Muenchen (pronounced Min'-chen), Manager  
  Statistical Consulting Center
  U of TN Office of Information Technology
  200 Stokely Management Center, Knoxville, TN 37996-0520
  Voice: (865) 974-5230  
  FAX:   (865) 974-4810
  Email: muenchen at utk.edu
  Web:   http://oit.utk.edu/scc, 
  News:  http://listserv.utk.edu/archives/statnews.html
=========================================================

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of John Fox
> Sent: Wednesday, March 21, 2007 8:59 PM
> To: 'Prof Brian Ripley'
> Cc: 'r-help'; 'Chuck Cleland'
> Subject: Re: [R] how to get "lsmeans"?
> 
> Dear Brian et al.,
> 
> My apologies for chiming in late: It's been a busy day.
> 
> First some general comments on "least-squares means" and "effect
> displays."
> The general idea behind the two is similar -- to examine fitted values
> corresponding to a term in a model while holding other terms to
typical
> values -- but the implementation is not identical. There are also
other
> similar ideas floating around as well. My formulation is more general
> in the
> sense that it applies to a wider variety of models, both linear and
> otherwise.
> 
> "Least-squares means" (a horrible term, by the way: in a 1980 paper in
> the
> American Statistician, Searle, Speed, and Milliken suggested the more
> descriptive term "population marginal means") apply to factors and
> combinations of factors; covariates are set to mean values and the
> levels of
> other factors are averaged over, in effect applying equal weight to
> each
> level. (This is from memory, so it's possible that I'm not getting it
> quite
> right, but I believe that I am.) In my effect displays, each level of
a
> factor is weighted by its proportion in the data. In models in which
> least-squares means can be computed, they should differ from the
> corresponding effect display by a constant (if there are different
> numbers
> of observations in the different levels of the factors that are held
> constant).
> 
> The obstacle to computing either least-squares means or effect
displays
> in R
> via predict() is that predict() wants factors in the "new data" to be
> set to
> particular levels. The effect() function in the effects package
> bypasses
> predict() and works directly with the model matrix, averaging over the
> columns that pertain to a factor (and reconstructing interactions as
> necessary). As mentioned, this has the effect of setting the factor to
> its
> proportional distribution in the data. This approach also has the
> advantage
> of being invariant with respect to the choice of contrasts for a
> factor.
> 
> The only convenient way that I can think of to implement least-squares
> means
> in R would be to use deviation-coded regressors for a factor (that is,
> contr.sum) and then to set the columns of the model matrix for the
> factor(s)
> to be averaged over to 0. It may just be that I'm having a failure of
> imagination and that there's a better way to proceed. I've not
> implemented
> this solution because it is dependent upon the choice of contrasts and
> because I don't see a general advantage to it, but since the issue has
> come
> up several times now, maybe I should take a crack at it. Remember that
> I
> want this to work more generally, not just for levels of factors, and
> not
> just for linear models.
> 
> Brian is quite right in mentioning that he suggested some time ago
that
> I
> use critical values of t rather than of the standard normal
> distribution for
> producing confidence intervals, and I agree that it makes sense to do
> so in
> models in which the dispersion is estimated. My only excuse for not
yet
> doing this is that I want to undertake a more general revision of the
> effects package, and haven't had time to do it. There are several
> changes
> that I'd like to make to the package. For example, I have results for
> multinomial and proportional odds logit models (described in a paper
by
> me
> and Bob Andersen in the 2006 issue of Sociological Methodology) that I
> want
> to incorporate, and I'd like to improve the appearance of the default
> graphs. But Brian's suggestion is very straightforward, and I guess
> that I
> shouldn't wait to implement it; I'll do so very soon.
> 
> Regards,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox
> --------------------------------
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof
> > Brian Ripley
> > Sent: Wednesday, March 21, 2007 12:03 PM
> > To: Chuck Cleland
> > Cc: r-help
> > Subject: Re: [R] how to get "lsmeans"?
> >
> > On Wed, 21 Mar 2007, Chuck Cleland wrote:
> >
> > > Liaw, Andy wrote:
> > >> I verified the result from the following with output from JMP 6
on
> > >> the same data (don't have SAS: don't need it):
> > >>
> > >> set.seed(631)
> > >> n <- 100
> > >> dat <- data.frame(y=rnorm(n), A=factor(sample(1:2, n,
> > replace=TRUE)),
> > >>                   B=factor(sample(1:2, n, replace=TRUE)),
> > >>                   C=factor(sample(1:2, n, replace=TRUE)),
> > >>                   d=rnorm(n))
> > >> fm <- lm(y ~ A + B + C + d, dat)
> > >> ## Form a data frame of points to predict: all
> > combinations of the ##
> > >> three factors and the mean of the covariate.
> > >> p <- data.frame(expand.grid(A=1:2, B=1:2, C=1:2)) p[] <-
lapply(p,
> > >> factor) p <- cbind(p, d=mean(dat$d)) p <-
> > cbind(yhat=predict(fm, p),
> > >> p) ## lsmeans for the three factors:
> > >> with(p, tapply(yhat, A, mean))
> > >> with(p, tapply(yhat, B, mean))
> > >> with(p, tapply(yhat, C, mean))
> > >
> > >  Using Andy's example data, these are the LSMEANS and
> > intervals I get
> > > from SAS:
> > >
> > > A        y LSMEAN      95% Confidence Limits
> > > 1       -0.071847       -0.387507     0.243813
> > > 2       -0.029621       -0.342358     0.283117
> > >
> > > B        y LSMEAN      95% Confidence Limits
> > > 1       -0.104859       -0.397935     0.188216
> > > 2        0.003391       -0.333476     0.340258
> > >
> > > C        y LSMEAN      95% Confidence Limits
> > > 1       -0.084679       -0.392343     0.222986
> > > 2       -0.016789       -0.336374     0.302795
> > >
> > >  One way of reproducing the LSMEANS and intervals from SAS using
> > > predict() seems to be the following:
> > >
> > >> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d,
> > data = dat)
> > >> newdat <- expand.grid(A=factor(c(1,2)),B=1.5,C=1.5,d=mean(dat$d))
> > >> cbind(newdat, predict(dat.lm, newdat, interval="confidence"))
> > >  A   B   C          d         fit        lwr       upr
> > > 1 1 1.5 1.5 0.09838595 -0.07184709 -0.3875070 0.2438128
> > > 2 2 1.5 1.5 0.09838595 -0.02962086 -0.3423582 0.2831165
> > >
> > >  However, another possibility seems to be:
> > >
> > >> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d,
> > data = dat)
> > >> newdat <-
> > >
> >
> expand.grid(A=factor(c(1,2)),B=mean(as.numeric(dat$B)),C=mean(as.numer
> > > ic(dat$C)),d=mean(dat$d))
> > >> cbind(newdat, predict(dat.lm, newdat, interval="confidence"))
> > >  A    B    C          d         fit        lwr       upr
> > > 1 1 1.43 1.48 0.09838595 -0.08078243 -0.3964661 0.2349012
> > > 2 2 1.43 1.48 0.09838595 -0.03855619 -0.3479589 0.2708465
> > >
> > >  The predictions directly above match what effect() in the effects
> > > package by John Fox returns:
> > >
> > > library(effects)
> > >
> > >> effect("A", fm, xlevels=list(d = mean(dat$D)))
> > >
> > > A effect
> > > A
> > >          1           2
> > > -0.08078243 -0.03855619
> > >
> > >  But for some reason the predict() and effect() intervals
> > are a little
> > > different:
> > >
> > >> effect("A", fm, xlevels=list(d = mean(dat$D)))$lower
> > >          [,1]
> > > 101 -0.3924451
> > > 102 -0.3440179
> > >
> > >> effect("A", fm, xlevels=list(d = mean(dat$D)))$upper
> > >         [,1]
> > > 101 0.2308802
> > > 102 0.2669055
> > >
> > >  I would be interested in any comments on these different
> > approaches
> > > and on the difference in intervals returned by predict()
> > and effect().
> >
> > AFAIR, the effects packages uses normal-based confidence
> > intervals and predict.lm uses t-based ones, and I have
> > suggested to John Fox that t-based intervals would be
> > preferable, at least as an option.
> >
> >
> > --
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From idimakos at upatras.gr  Thu Mar 22 15:17:58 2007
From: idimakos at upatras.gr (Ioannis Dimakos)
Date: Thu, 22 Mar 2007 16:17:58 +0200 (EET)
Subject: [R] how to get "lsmeans"?
In-Reply-To: <D028EEB4CA113D4EAFDD485CCC998277731795@UTKFSVS4.utk.tennessee.edu>
References: <Pine.LNX.4.64.0703211502050.24076@gannet.stats.ox.ac.uk>
	<20070322005910.VVAT1624.tomts40-srv.bellnexxia.net@JohnDesktop8300>
	<D028EEB4CA113D4EAFDD485CCC998277731795@UTKFSVS4.utk.tennessee.edu>
Message-ID: <33521.91.140.2.75.1174573078.squirrel@mail.upatras.gr>

I believe there is an easier way and you don't have to do a thing.

A student of mine just purchased a brand new laptop loaded with Vista. 
Guess what?  SPSS v14 or v15 does not work with vista.  In a couple of
days we will install ubuntu on it and R.

Enjoy,

Ioannis

============
On ???, ??????? 22, 2007 15:35, Muenchen, Robert A (Bob) wrote:
> Hi All,
>
> Perhaps I'm stating the obvious, but to increase the use of R in places
> where SAS & SPSS dominate, it's important to make getting the same
> answers as easy as possible. That includes things like lsmeans and type
> III sums of squares. I've read lots of discussions here on sums of
> squares & I'm not advocating type III use, just looking at it from a
> marketing perspective. Too many people look for excuses to not change.
> The fewer excuses, the better.
>
> Of course this is easy for me to say, as I'm not the one who does the
> work! Much thanks to those who do.
>
> Cheers,
> Bob
>
> =========================================================
>   Bob Muenchen (pronounced Min'-chen), Manager
>   Statistical Consulting Center
>   U of TN Office of Information Technology
>   200 Stokely Management Center, Knoxville, TN 37996-0520
>   Voice: (865) 974-5230
>   FAX:   (865) 974-4810
>   Email: muenchen at utk.edu
>   Web:   http://oit.utk.edu/scc,
>   News:  http://listserv.utk.edu/archives/statnews.html
> =========================================================
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
>> bounces at stat.math.ethz.ch] On Behalf Of John Fox
>> Sent: Wednesday, March 21, 2007 8:59 PM
>> To: 'Prof Brian Ripley'
>> Cc: 'r-help'; 'Chuck Cleland'
>> Subject: Re: [R] how to get "lsmeans"?
>>
>> Dear Brian et al.,
>>
>> My apologies for chiming in late: It's been a busy day.
>>
>> First some general comments on "least-squares means" and "effect
>> displays."
>> The general idea behind the two is similar -- to examine fitted values
>> corresponding to a term in a model while holding other terms to
> typical
>> values -- but the implementation is not identical. There are also
> other
>> similar ideas floating around as well. My formulation is more general
>> in the
>> sense that it applies to a wider variety of models, both linear and
>> otherwise.
>>
>> "Least-squares means" (a horrible term, by the way: in a 1980 paper in
>> the
>> American Statistician, Searle, Speed, and Milliken suggested the more
>> descriptive term "population marginal means") apply to factors and
>> combinations of factors; covariates are set to mean values and the
>> levels of
>> other factors are averaged over, in effect applying equal weight to
>> each
>> level. (This is from memory, so it's possible that I'm not getting it
>> quite
>> right, but I believe that I am.) In my effect displays, each level of
> a
>> factor is weighted by its proportion in the data. In models in which
>> least-squares means can be computed, they should differ from the
>> corresponding effect display by a constant (if there are different
>> numbers
>> of observations in the different levels of the factors that are held
>> constant).
>>
>> The obstacle to computing either least-squares means or effect
> displays
>> in R
>> via predict() is that predict() wants factors in the "new data" to be
>> set to
>> particular levels. The effect() function in the effects package
>> bypasses
>> predict() and works directly with the model matrix, averaging over the
>> columns that pertain to a factor (and reconstructing interactions as
>> necessary). As mentioned, this has the effect of setting the factor to
>> its
>> proportional distribution in the data. This approach also has the
>> advantage
>> of being invariant with respect to the choice of contrasts for a
>> factor.
>>
>> The only convenient way that I can think of to implement least-squares
>> means
>> in R would be to use deviation-coded regressors for a factor (that is,
>> contr.sum) and then to set the columns of the model matrix for the
>> factor(s)
>> to be averaged over to 0. It may just be that I'm having a failure of
>> imagination and that there's a better way to proceed. I've not
>> implemented
>> this solution because it is dependent upon the choice of contrasts and
>> because I don't see a general advantage to it, but since the issue has
>> come
>> up several times now, maybe I should take a crack at it. Remember that
>> I
>> want this to work more generally, not just for levels of factors, and
>> not
>> just for linear models.
>>
>> Brian is quite right in mentioning that he suggested some time ago
> that
>> I
>> use critical values of t rather than of the standard normal
>> distribution for
>> producing confidence intervals, and I agree that it makes sense to do
>> so in
>> models in which the dispersion is estimated. My only excuse for not
> yet
>> doing this is that I want to undertake a more general revision of the
>> effects package, and haven't had time to do it. There are several
>> changes
>> that I'd like to make to the package. For example, I have results for
>> multinomial and proportional odds logit models (described in a paper
> by
>> me
>> and Bob Andersen in the 2006 issue of Sociological Methodology) that I
>> want
>> to incorporate, and I'd like to improve the appearance of the default
>> graphs. But Brian's suggestion is very straightforward, and I guess
>> that I
>> shouldn't wait to implement it; I'll do so very soon.
>>
>> Regards,
>>  John
>>
>> --------------------------------
>> John Fox
>> Department of Sociology
>> McMaster University
>> Hamilton, Ontario
>> Canada L8S 4M4
>> 905-525-9140x23604
>> http://socserv.mcmaster.ca/jfox
>> --------------------------------
>>
>> > -----Original Message-----
>> > From: r-help-bounces at stat.math.ethz.ch
>> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof
>> > Brian Ripley
>> > Sent: Wednesday, March 21, 2007 12:03 PM
>> > To: Chuck Cleland
>> > Cc: r-help
>> > Subject: Re: [R] how to get "lsmeans"?
>> >
>> > On Wed, 21 Mar 2007, Chuck Cleland wrote:
>> >
>> > > Liaw, Andy wrote:
>> > >> I verified the result from the following with output from JMP 6
> on
>> > >> the same data (don't have SAS: don't need it):
>> > >>
>> > >> set.seed(631)
>> > >> n <- 100
>> > >> dat <- data.frame(y=rnorm(n), A=factor(sample(1:2, n,
>> > replace=TRUE)),
>> > >>                   B=factor(sample(1:2, n, replace=TRUE)),
>> > >>                   C=factor(sample(1:2, n, replace=TRUE)),
>> > >>                   d=rnorm(n))
>> > >> fm <- lm(y ~ A + B + C + d, dat)
>> > >> ## Form a data frame of points to predict: all
>> > combinations of the ##
>> > >> three factors and the mean of the covariate.
>> > >> p <- data.frame(expand.grid(A=1:2, B=1:2, C=1:2)) p[] <-
> lapply(p,
>> > >> factor) p <- cbind(p, d=mean(dat$d)) p <-
>> > cbind(yhat=predict(fm, p),
>> > >> p) ## lsmeans for the three factors:
>> > >> with(p, tapply(yhat, A, mean))
>> > >> with(p, tapply(yhat, B, mean))
>> > >> with(p, tapply(yhat, C, mean))
>> > >
>> > >  Using Andy's example data, these are the LSMEANS and
>> > intervals I get
>> > > from SAS:
>> > >
>> > > A        y LSMEAN      95% Confidence Limits
>> > > 1       -0.071847       -0.387507     0.243813
>> > > 2       -0.029621       -0.342358     0.283117
>> > >
>> > > B        y LSMEAN      95% Confidence Limits
>> > > 1       -0.104859       -0.397935     0.188216
>> > > 2        0.003391       -0.333476     0.340258
>> > >
>> > > C        y LSMEAN      95% Confidence Limits
>> > > 1       -0.084679       -0.392343     0.222986
>> > > 2       -0.016789       -0.336374     0.302795
>> > >
>> > >  One way of reproducing the LSMEANS and intervals from SAS using
>> > > predict() seems to be the following:
>> > >
>> > >> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d,
>> > data = dat)
>> > >> newdat <- expand.grid(A=factor(c(1,2)),B=1.5,C=1.5,d=mean(dat$d))
>> > >> cbind(newdat, predict(dat.lm, newdat, interval="confidence"))
>> > >  A   B   C          d         fit        lwr       upr
>> > > 1 1 1.5 1.5 0.09838595 -0.07184709 -0.3875070 0.2438128
>> > > 2 2 1.5 1.5 0.09838595 -0.02962086 -0.3423582 0.2831165
>> > >
>> > >  However, another possibility seems to be:
>> > >
>> > >> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d,
>> > data = dat)
>> > >> newdat <-
>> > >
>> >
>> expand.grid(A=factor(c(1,2)),B=mean(as.numeric(dat$B)),C=mean(as.numer
>> > > ic(dat$C)),d=mean(dat$d))
>> > >> cbind(newdat, predict(dat.lm, newdat, interval="confidence"))
>> > >  A    B    C          d         fit        lwr       upr
>> > > 1 1 1.43 1.48 0.09838595 -0.08078243 -0.3964661 0.2349012
>> > > 2 2 1.43 1.48 0.09838595 -0.03855619 -0.3479589 0.2708465
>> > >
>> > >  The predictions directly above match what effect() in the effects
>> > > package by John Fox returns:
>> > >
>> > > library(effects)
>> > >
>> > >> effect("A", fm, xlevels=list(d = mean(dat$D)))
>> > >
>> > > A effect
>> > > A
>> > >          1           2
>> > > -0.08078243 -0.03855619
>> > >
>> > >  But for some reason the predict() and effect() intervals
>> > are a little
>> > > different:
>> > >
>> > >> effect("A", fm, xlevels=list(d = mean(dat$D)))$lower
>> > >          [,1]
>> > > 101 -0.3924451
>> > > 102 -0.3440179
>> > >
>> > >> effect("A", fm, xlevels=list(d = mean(dat$D)))$upper
>> > >         [,1]
>> > > 101 0.2308802
>> > > 102 0.2669055
>> > >
>> > >  I would be interested in any comments on these different
>> > approaches
>> > > and on the difference in intervals returned by predict()
>> > and effect().
>> >
>> > AFAIR, the effects packages uses normal-based confidence
>> > intervals and predict.lm uses t-based ones, and I have
>> > suggested to John Fox that t-based intervals would be
>> > preferable, at least as an option.
>> >
>> >
>> > --
>> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> > University of Oxford,             Tel:  +44 1865 272861 (self)
>> > 1 South Parks Road,                     +44 1865 272866 (PA)
>> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ioannis C. Dimakos, Ph.D.
University of Patras
Department of Elementary Education
Patras, GR-26500 GREECE
http://www.elemedu.upatras.gr/dimakos/
http://thedailyblahblah.blogspot.com/


From jfox at mcmaster.ca  Thu Mar 22 15:24:09 2007
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 22 Mar 2007 10:24:09 -0400
Subject: [R] how to get "lsmeans"?
In-Reply-To: <D028EEB4CA113D4EAFDD485CCC998277731795@UTKFSVS4.utk.tennessee.edu>
Message-ID: <20070322142409.CNWO1646.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear Bob,

I think that you make a valid point -- and I've included "Type-III" tests in
the Anova() function in the car package, for example, though not entirely
for consistency with SAS -- but my object in writing the effects package was
different. I wanted to provide a general approach to visualizing terms in
complex models with linear predictors. If I can with reasonable effort
provide a solution that's the same as "least-squares means" for models for
which "least-squares means" are defined then I'll do so at some point, but
duplicating what SAS does was not my goal.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Muenchen, Robert A (Bob)
> Sent: Thursday, March 22, 2007 9:36 AM
> To: R-help at stat.math.ethz.ch
> Subject: Re: [R] how to get "lsmeans"?
> 
> Hi All,
> 
> Perhaps I'm stating the obvious, but to increase the use of R 
> in places where SAS & SPSS dominate, it's important to make 
> getting the same answers as easy as possible. That includes 
> things like lsmeans and type III sums of squares. I've read 
> lots of discussions here on sums of squares & I'm not 
> advocating type III use, just looking at it from a marketing 
> perspective. Too many people look for excuses to not change.
> The fewer excuses, the better.
> 
> Of course this is easy for me to say, as I'm not the one who 
> does the work! Much thanks to those who do.
> 
> Cheers,
> Bob
> 
> =========================================================
>   Bob Muenchen (pronounced Min'-chen), Manager
>   Statistical Consulting Center
>   U of TN Office of Information Technology
>   200 Stokely Management Center, Knoxville, TN 37996-0520
>   Voice: (865) 974-5230  
>   FAX:   (865) 974-4810
>   Email: muenchen at utk.edu
>   Web:   http://oit.utk.edu/scc, 
>   News:  http://listserv.utk.edu/archives/statnews.html
> =========================================================
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch [mailto:r-help- 
> > bounces at stat.math.ethz.ch] On Behalf Of John Fox
> > Sent: Wednesday, March 21, 2007 8:59 PM
> > To: 'Prof Brian Ripley'
> > Cc: 'r-help'; 'Chuck Cleland'
> > Subject: Re: [R] how to get "lsmeans"?
> > 
> > Dear Brian et al.,
> > 
> > My apologies for chiming in late: It's been a busy day.
> > 
> > First some general comments on "least-squares means" and "effect 
> > displays."
> > The general idea behind the two is similar -- to examine 
> fitted values 
> > corresponding to a term in a model while holding other terms to
> typical
> > values -- but the implementation is not identical. There are also
> other
> > similar ideas floating around as well. My formulation is 
> more general 
> > in the sense that it applies to a wider variety of models, 
> both linear 
> > and otherwise.
> > 
> > "Least-squares means" (a horrible term, by the way: in a 
> 1980 paper in 
> > the American Statistician, Searle, Speed, and Milliken 
> suggested the 
> > more descriptive term "population marginal means") apply to factors 
> > and combinations of factors; covariates are set to mean 
> values and the 
> > levels of other factors are averaged over, in effect applying equal 
> > weight to each level. (This is from memory, so it's 
> possible that I'm 
> > not getting it quite right, but I believe that I am.) In my effect 
> > displays, each level of
> a
> > factor is weighted by its proportion in the data. In models 
> in which 
> > least-squares means can be computed, they should differ from the 
> > corresponding effect display by a constant (if there are different 
> > numbers of observations in the different levels of the factors that 
> > are held constant).
> > 
> > The obstacle to computing either least-squares means or effect
> displays
> > in R
> > via predict() is that predict() wants factors in the "new 
> data" to be 
> > set to particular levels. The effect() function in the 
> effects package 
> > bypasses
> > predict() and works directly with the model matrix, 
> averaging over the 
> > columns that pertain to a factor (and reconstructing 
> interactions as 
> > necessary). As mentioned, this has the effect of setting 
> the factor to 
> > its proportional distribution in the data. This approach 
> also has the 
> > advantage of being invariant with respect to the choice of 
> contrasts 
> > for a factor.
> > 
> > The only convenient way that I can think of to implement 
> least-squares 
> > means in R would be to use deviation-coded regressors for a factor 
> > (that is,
> > contr.sum) and then to set the columns of the model matrix for the
> > factor(s)
> > to be averaged over to 0. It may just be that I'm having a 
> failure of 
> > imagination and that there's a better way to proceed. I've not 
> > implemented this solution because it is dependent upon the 
> choice of 
> > contrasts and because I don't see a general advantage to 
> it, but since 
> > the issue has come up several times now, maybe I should 
> take a crack 
> > at it. Remember that I want this to work more generally, 
> not just for 
> > levels of factors, and not just for linear models.
> > 
> > Brian is quite right in mentioning that he suggested some time ago
> that
> > I
> > use critical values of t rather than of the standard normal 
> > distribution for producing confidence intervals, and I 
> agree that it 
> > makes sense to do so in models in which the dispersion is 
> estimated. 
> > My only excuse for not
> yet
> > doing this is that I want to undertake a more general 
> revision of the 
> > effects package, and haven't had time to do it. There are several 
> > changes that I'd like to make to the package. For example, I have 
> > results for multinomial and proportional odds logit models 
> (described 
> > in a paper
> by
> > me
> > and Bob Andersen in the 2006 issue of Sociological 
> Methodology) that I 
> > want to incorporate, and I'd like to improve the appearance of the 
> > default graphs. But Brian's suggestion is very 
> straightforward, and I 
> > guess that I shouldn't wait to implement it; I'll do so very soon.
> > 
> > Regards,
> >  John
> > 
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> > 
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch 
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof Brian 
> > > Ripley
> > > Sent: Wednesday, March 21, 2007 12:03 PM
> > > To: Chuck Cleland
> > > Cc: r-help
> > > Subject: Re: [R] how to get "lsmeans"?
> > >
> > > On Wed, 21 Mar 2007, Chuck Cleland wrote:
> > >
> > > > Liaw, Andy wrote:
> > > >> I verified the result from the following with output from JMP 6
> on
> > > >> the same data (don't have SAS: don't need it):
> > > >>
> > > >> set.seed(631)
> > > >> n <- 100
> > > >> dat <- data.frame(y=rnorm(n), A=factor(sample(1:2, n,
> > > replace=TRUE)),
> > > >>                   B=factor(sample(1:2, n, replace=TRUE)),
> > > >>                   C=factor(sample(1:2, n, replace=TRUE)),
> > > >>                   d=rnorm(n))
> > > >> fm <- lm(y ~ A + B + C + d, dat)
> > > >> ## Form a data frame of points to predict: all
> > > combinations of the ##
> > > >> three factors and the mean of the covariate.
> > > >> p <- data.frame(expand.grid(A=1:2, B=1:2, C=1:2)) p[] <-
> lapply(p,
> > > >> factor) p <- cbind(p, d=mean(dat$d)) p <-
> > > cbind(yhat=predict(fm, p),
> > > >> p) ## lsmeans for the three factors:
> > > >> with(p, tapply(yhat, A, mean))
> > > >> with(p, tapply(yhat, B, mean))
> > > >> with(p, tapply(yhat, C, mean))
> > > >
> > > >  Using Andy's example data, these are the LSMEANS and
> > > intervals I get
> > > > from SAS:
> > > >
> > > > A        y LSMEAN      95% Confidence Limits
> > > > 1       -0.071847       -0.387507     0.243813
> > > > 2       -0.029621       -0.342358     0.283117
> > > >
> > > > B        y LSMEAN      95% Confidence Limits
> > > > 1       -0.104859       -0.397935     0.188216
> > > > 2        0.003391       -0.333476     0.340258
> > > >
> > > > C        y LSMEAN      95% Confidence Limits
> > > > 1       -0.084679       -0.392343     0.222986
> > > > 2       -0.016789       -0.336374     0.302795
> > > >
> > > >  One way of reproducing the LSMEANS and intervals from SAS using
> > > > predict() seems to be the following:
> > > >
> > > >> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d,
> > > data = dat)
> > > >> newdat <- 
> expand.grid(A=factor(c(1,2)),B=1.5,C=1.5,d=mean(dat$d))
> > > >> cbind(newdat, predict(dat.lm, newdat, interval="confidence"))
> > > >  A   B   C          d         fit        lwr       upr
> > > > 1 1 1.5 1.5 0.09838595 -0.07184709 -0.3875070 0.2438128
> > > > 2 2 1.5 1.5 0.09838595 -0.02962086 -0.3423582 0.2831165
> > > >
> > > >  However, another possibility seems to be:
> > > >
> > > >> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d,
> > > data = dat)
> > > >> newdat <-
> > > >
> > >
> > 
> expand.grid(A=factor(c(1,2)),B=mean(as.numeric(dat$B)),C=mean(as.numer
> > > > ic(dat$C)),d=mean(dat$d))
> > > >> cbind(newdat, predict(dat.lm, newdat, interval="confidence"))
> > > >  A    B    C          d         fit        lwr       upr
> > > > 1 1 1.43 1.48 0.09838595 -0.08078243 -0.3964661 0.2349012
> > > > 2 2 1.43 1.48 0.09838595 -0.03855619 -0.3479589 0.2708465
> > > >
> > > >  The predictions directly above match what effect() in 
> the effects 
> > > > package by John Fox returns:
> > > >
> > > > library(effects)
> > > >
> > > >> effect("A", fm, xlevels=list(d = mean(dat$D)))
> > > >
> > > > A effect
> > > > A
> > > >          1           2
> > > > -0.08078243 -0.03855619
> > > >
> > > >  But for some reason the predict() and effect() intervals
> > > are a little
> > > > different:
> > > >
> > > >> effect("A", fm, xlevels=list(d = mean(dat$D)))$lower
> > > >          [,1]
> > > > 101 -0.3924451
> > > > 102 -0.3440179
> > > >
> > > >> effect("A", fm, xlevels=list(d = mean(dat$D)))$upper
> > > >         [,1]
> > > > 101 0.2308802
> > > > 102 0.2669055
> > > >
> > > >  I would be interested in any comments on these different
> > > approaches
> > > > and on the difference in intervals returned by predict()
> > > and effect().
> > >
> > > AFAIR, the effects packages uses normal-based confidence 
> intervals 
> > > and predict.lm uses t-based ones, and I have suggested to 
> John Fox 
> > > that t-based intervals would be preferable, at least as an option.
> > >
> > >
> > > --
> > > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > > Professor of Applied Statistics,  
> http://www.stats.ox.ac.uk/~ripley/
> > > University of Oxford,             Tel:  +44 1865 272861 (self)
> > > 1 South Parks Road,                     +44 1865 272866 (PA)
> > > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting- 
> > guide.html and provide commented, minimal, self-contained, 
> > reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at comcast.net  Thu Mar 22 15:39:38 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 22 Mar 2007 09:39:38 -0500
Subject: [R] Ticks and labels on plots
In-Reply-To: <17922.18040.856404.895212@stat.math.ethz.ch>
References: <46005A57.2000709@noaa.gov>
	<1174444859.5415.57.camel@localhost.localdomain>
	<b5t20314vt79f5g91d22rabhpkt0oljgra@4ax.com>
	<1174504992.7212.42.camel@localhost.localdomain>
	<17922.18040.856404.895212@stat.math.ethz.ch>
Message-ID: <1174574378.5102.7.camel@localhost.localdomain>

<snip>

On Thu, 2007-03-22 at 10:03 +0100, Martin Maechler wrote:
> >>>>> "Marc" == Marc Schwartz <marc_schwartz at comcast.net>

>     Marc> To get a feel for how axis() creates the default tick positions when
>     Marc> 'at' is the default NULL, see ?axTicks, which provides functionality
>     Marc> similar to the internal C routine.
> 
> yes, partly not only similar but "the same" when it works (by
> default) with par("axp")
> 
>     Marc> You could also look at ?pretty
> 
> Yes.  *However* there's one important thing which I think hasn't
> been mentioned yet.
> 
> We have now been talking how and where axis() {i.e. its internal
> code} chooses to place tick marks.
> The clue is that it draws labels at all tick marks by default or
> explicitly with axis(*, at= ., labels = .)
> BUT the labels are not shown on your plot as soon as they
> would get too much crammed together.
> 
> You can see this nicely, interactively, by the following:
> Use
>      graphics.off()
>      plot(1:11)
> 
> This will show ticks *and* labels  at  2 , 4 , 6 , 8, 10
> and now use your mouse, drag to make the graphics window
> narrower (e.g. keeping height constant), in small steps,
> releasing the mouse again to let R redraw the graphic.
> For quite a while, the labels remain until there's not enough
> room, the 5 ticks remain, but only the
>  labels  2 , 4, 6, 8  are drawn
>  then    2 , 6 , 10 
>  then    2 , 6
>  then    2 , 8  ( a little surprise to me)
>  then    2
> 
> you always see all ticks but labels are only drawn when they
> don't get in each other's way.
> 
> Of course, things like
> 
>  plot(1:11, xaxt="n")
>  axis(1, at=1:11, labels = paste("Lab", 1:11))
> 
> show even more when the window is widened or narrowed,
> and yes, it depends on the device and on the fonts and its
> sizes, see e.g.,
> 
>  plot(1:11, xaxt="n")
>  axis(1, at=1:11, labels = paste("Lab", 1:11), cex.axis = 0.5)
> 
> ----- -----
> 
> If you don't like this --- almost always very desirable ---
> builtin smartness, you need to descend slightly lower level and use
> mtext(), e.g., the analogue of the above is
> 
>   plot(1:11, xaxt="n")
>   mtext(side=1, at=1:11, text = paste("Lab", 1:11), cex = 0.5)
> 
> or rather leave traditional graphics which really gets messy in
> such cases {par() .. etc} and upgrade to using the "grid"
> package,  or also packages built on grid, "lattice" or "ggplot".
> But infact, someone else needs to tell you how to play similar
> goes with these.

Excellent points Martin.

My initial reply and example, where I had the tick marks at each bar,
but the labels every 5 bars, was implicitly (at least in my own mind ;-)
a consequence of the spacing issues that you raise.

Best regards,

Marc


From bqcaomail at gmail.com  Thu Mar 22 15:59:13 2007
From: bqcaomail at gmail.com (Baoqiang Cao)
Date: Thu, 22 Mar 2007 09:59:13 -0500
Subject: [R] "digits" doesn't work in format function
Message-ID: <5c5a748a0703220759h72b90effjab776e99c6a9e672@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070322/d0f98231/attachment.pl 

From koen.hufkens at ua.ac.be  Thu Mar 22 16:04:13 2007
From: koen.hufkens at ua.ac.be (Hufkens Koen)
Date: Thu, 22 Mar 2007 16:04:13 +0100
Subject: [R] non-linear curve fitting
In-Reply-To: <46027E46.8090701@sciviews.org>
Message-ID: <832A948B92E5754D9062CCF62F9ED892655FE6@xmail01.ad.ua.ac.be>

Is their a means of getting an F-statistic (p-value) out of all of this.

Because least-square criterion / r-square only tell me how good the fit is and not necessarily how solid this fit is. An F-statistic (p-value) would be nice...

Regards,
Koen


> -----Original Message-----
> From: Philippe Grosjean [mailto:phgrosjean at sciviews.org] 
> Sent: donderdag 22 maart 2007 14:02
> To: Hufkens Koen; r-help at stat.math.ethz.ch
> Subject: Re: [R] non-linear curve fitting
> 
> Hello,
> 
> If a least-square criterion is fine for you, you should use 
> nls(). For the logistic curve, you have a convenient 
> self-starting model available: 
> SSlogis(). Look at:
> 
> ?nls
> ?SSlogis
> 
> Best,
> 
> Philippe Grosjean
> 
> ..............................................<?}))><........
>   ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
>   ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>   ) ) ) ) )   Mons-Hainaut University, Belgium
> ( ( ( ( (
> ..............................................................
> 
> Hufkens Koen wrote:
> > Hi list,
> >  
> > I have a little curve fitting problem.
> >  
> > I would like to fit a sigmoid curve to my data using the 
> following equation:
> >  
> > f(x) = 1/(1 + exp(-(x-c)*b)) (or any other form for that matter)
> >  
> > Where x is the distance/location within the dataframe, c is 
> the shift of the curve across the dataframe and b is the 
> steepness of the curve.
> >  
> > I've been playing with glm() and glm.fit() but without any luck.
> >  
> > for example the most simple example
> >  
> > x = -10:10
> > y = 1/(1 + exp(-x))
> > glm(y ~ x, family=binomial(link="logit"))
> >  
> > I get a warning:
> > non-integer #successes in a binomial glm! in: eval(expr, envir, 
> > enclos)
> >  
> > and some erratic results
> >  
> > This is the most simple test to see if I could fit a curve 
> to this perfect data so since this didn't work out, bringing 
> in the extra parameters is a whole other ballgame so could 
> someone give me a clue?
> >  
> > Kind regards,
> > Koen
> >  
> > 
> 
> --
> No virus found in this incoming message.
> Checked by AVG Free Edition.
> Version: 7.5.446 / Virus Database: 268.18.16/729 - Release 
> Date: 21/03/2007 7:52
>  
> 

--


From ggrothendieck at gmail.com  Thu Mar 22 16:08:58 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 22 Mar 2007 11:08:58 -0400
Subject: [R] "digits" doesn't work in format function
In-Reply-To: <5c5a748a0703220759h72b90effjab776e99c6a9e672@mail.gmail.com>
References: <5c5a748a0703220759h72b90effjab776e99c6a9e672@mail.gmail.com>
Message-ID: <971536df0703220808w61a0e8c6q22c3a368447210bc@mail.gmail.com>

Try

sprintf("%.2f", m)


On 3/22/07, Baoqiang Cao <bqcaomail at gmail.com> wrote:
> Dear All,
>
> I was trying to format a numeric vector (100*1) by using
> outd <- format(x=m, sci=F, digits=2)
>
> > outd[1:10]
>  [1] " 0.01787758" "-0.14760306" "-0.45806041" "-0.67858525" "-0.64591748"
>  [6] "-0.05918100" "-0.25632276" "-0.15980138" "-0.08359873" "-0.37866688"
> >m[1:10]
>  [1]  0.017878 -0.147603 -0.458060 -0.678585 -0.645917 -0.059181 -0.256323
>  [8] -0.159801 -0.083599 -0.378667
>
> I'm expecting something like, 0.02 -0.15 ...
>


From marc_schwartz at comcast.net  Thu Mar 22 16:12:32 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 22 Mar 2007 10:12:32 -0500
Subject: [R] "digits" doesn't work in format function
In-Reply-To: <5c5a748a0703220759h72b90effjab776e99c6a9e672@mail.gmail.com>
References: <5c5a748a0703220759h72b90effjab776e99c6a9e672@mail.gmail.com>
Message-ID: <1174576352.5102.17.camel@localhost.localdomain>

On Thu, 2007-03-22 at 09:59 -0500, Baoqiang Cao wrote:
> Dear All,
> 
> I was trying to format a numeric vector (100*1) by using
> outd <- format(x=m, sci=F, digits=2)
> 
> > outd[1:10]
>  [1] " 0.01787758" "-0.14760306" "-0.45806041" "-0.67858525" "-0.64591748"
>  [6] "-0.05918100" "-0.25632276" "-0.15980138" "-0.08359873" "-0.37866688"
> >m[1:10]
>  [1]  0.017878 -0.147603 -0.458060 -0.678585 -0.645917 -0.059181 -0.256323
>  [8] -0.159801 -0.083599 -0.378667
> 
> I'm expecting something like, 0.02 -0.15 ...
> 
> Any advice how to fix it will be highly appreciated!
> 
> Best,
>  Baoqiang

The digits argument pertains to the number of _significant digits_. 

>From ?format:

how many significant digits are to be used for numeric and complex x.
The default, NULL, uses getOption(digits). This is a suggestion: enough
decimal places will be used so that the smallest (in magnitude) number
has this many significant digits, and also to satisfy nsmall. (For the
interpretation for complex numbers see signif.)


If you want tight and consistent control over the number of digits after
the decimal place, you would need to use ?sprintf or ?formatC:

> sprintf("%.2f", m)
 [1] "0.02"  "-0.15" "-0.46" "-0.68" "-0.65" "-0.06" "-0.26" "-0.16"
 [9] "-0.08" "-0.38"


> formatC(m, format = "f", digits = 2)
 [1] "0.02"  "-0.15" "-0.46" "-0.68" "-0.65" "-0.06" "-0.26" "-0.16"
 [9] "-0.08" "-0.38"


HTH,

Marc Schwartz


From ozric at web.de  Thu Mar 22 16:23:07 2007
From: ozric at web.de (Christian Schulz)
Date: Thu, 22 Mar 2007 16:23:07 +0100
Subject: [R] Cohen's Kappa
Message-ID: <46029F5B.5050206@web.de>

Hi,

im little bit confused about Cohen's Kappa and i should  be look into the
Kappa function code. Is the easy formula really wrong?

kappa=agreement-chance/(1-chance)

many thanks
christian

###############################################################################
true-negativ:7445
false-positive:3410
false-negativ:347
true-positiv:772

classification-aggrement:68,6%
kappa=agreement-chance/(1-chance) = (0.686-0.5)/0.5=0.372

.....with function from library(vcd)
Kappa(matrix(c(7445,3410,347,772),nrow=2))
               value         ASE
Unweighted 0.1686882 0.011235188
Weighted   0.1686882 0.007979293


From xwye at sibs.ac.cn  Thu Mar 22 16:24:50 2007
From: xwye at sibs.ac.cn (Xingwang Ye)
Date: Thu, 22 Mar 2007 23:24:50 +0800
Subject: [R] how to get "lsmeans"?
References: <20070322142409.CNWO1646.tomts43-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <DreamMail__232450_18712384806@smtp.sibs.ac.cn>

Dear John,     
     
As a green hand of R, I feel that if the results are identical to the ones of the user's familiar softwares such as
SAS, SPSS or Stata etc,the user will feel confident about the results of R. Why? Because lots of R beginners have no 
strong statistical background, just like me, they chose to trust SAS or SPSS firstly.
 
The best solution is that R should not only have the capability to produce the identical results of SAS or SPSS,
but also should have many more powerful, more prefessional functions(packages).
Then more and more guys who are not special at statistics will enjoy R, which makes R
popular; and more and more statisticians like it also, which makes R prefessional. 

Obviously, R is on the load to success.


Best wishes. 
yours, sincerely,  
Xingwang Ye    
PhD candidate     
Research Group of Nutrition Related Cancers and Other Chronic Diseases  
Institute for Nutritional Sciences,  
Shanghai Institutes of Biological Sciences,     
Chinese Academy of Sciences     

>Dear Bob, 
> 
>I think that you make a valid point -- and I've included "Type-III" tests in 
>the Anova() function in the car package, for example, though not entirely 
>for consistency with SAS -- but my object in writing the effects package was 
>different. I wanted to provide a general approach to visualizing terms in 
>complex models with linear predictors. If I can with reasonable effort 
>provide a solution that's the same as "least-squares means" for models for 
>which "least-squares means" are defined then I'll do so at some point, but 
>duplicating what SAS does was not my goal. 
> 
>Regards, 
> John 
> 
>-------------------------------- 
>John Fox 
>Department of Sociology 
>McMaster University 
>Hamilton, Ontario 
>Canada L8S 4M4 
>905-525-9140x23604 
>http://socserv.mcmaster.ca/jfox  
>--------------------------------  
> 
>> -----Original Message----- 
>> From: r-help-bounces at stat.math.ethz.ch  
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of  
>> Muenchen, Robert A (Bob) 
>> Sent: Thursday, March 22, 2007 9:36 AM 
>> To: R-help at stat.math.ethz.ch 
>> Subject: Re: [R] how to get "lsmeans"? 
>>  
>> Hi All, 
>>  
>> Perhaps I'm stating the obvious, but to increase the use of R  
>> in places where SAS & SPSS dominate, it's important to make  
>> getting the same answers as easy as possible. That includes  
>> things like lsmeans and type III sums of squares. I've read  
>> lots of discussions here on sums of squares & I'm not  
>> advocating type III use, just looking at it from a marketing  
>> perspective. Too many people look for excuses to not change. 
>> The fewer excuses, the better. 
>>  
>> Of course this is easy for me to say, as I'm not the one who  
>> does the work! Much thanks to those who do. 
>>  
>> Cheers, 
>> Bob 
>>  
>> ========================================================= 
>>   Bob Muenchen (pronounced Min'-chen), Manager 
>>   Statistical Consulting Center 
>>   U of TN Office of Information Technology 
>>   200 Stokely Management Center, Knoxville, TN 37996-0520 
>>   Voice: (865) 974-5230   
>>   FAX:   (865) 974-4810 
>>   Email: muenchen at utk.edu 
>>   Web:   http://oit.utk.edu/scc,  
>>   News:  http://listserv.utk.edu/archives/statnews.html 
>> ========================================================= 
>>  
>> > -----Original Message----- 
>> > From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-  
>> > bounces at stat.math.ethz.ch] On Behalf Of John Fox 
>> > Sent: Wednesday, March 21, 2007 8:59 PM 
>> > To: 'Prof Brian Ripley' 
>> > Cc: 'r-help'; 'Chuck Cleland' 
>> > Subject: Re: [R] how to get "lsmeans"? 
>> >  
>> > Dear Brian et al., 
>> >  
>> > My apologies for chiming in late: It's been a busy day. 
>> >  
>> > First some general comments on "least-squares means" and "effect  
>> > displays." 
>> > The general idea behind the two is similar -- to examine  
>> fitted values  
>> > corresponding to a term in a model while holding other terms to 
>> typical 
>> > values -- but the implementation is not identical. There are also 
>> other 
>> > similar ideas floating around as well. My formulation is  
>> more general  
>> > in the sense that it applies to a wider variety of models,  
>> both linear  
>> > and otherwise. 
>> >  
>> > "Least-squares means" (a horrible term, by the way: in a  
>> 1980 paper in  
>> > the American Statistician, Searle, Speed, and Milliken  
>> suggested the  
>> > more descriptive term "population marginal means") apply to factors  
>> > and combinations of factors; covariates are set to mean  
>> values and the  
>> > levels of other factors are averaged over, in effect applying equal  
>> > weight to each level. (This is from memory, so it's  
>> possible that I'm  
>> > not getting it quite right, but I believe that I am.) In my effect  
>> > displays, each level of 
>> a 
>> > factor is weighted by its proportion in the data. In models  
>> in which  
>> > least-squares means can be computed, they should differ from the  
>> > corresponding effect display by a constant (if there are different  
>> > numbers of observations in the different levels of the factors that  
>> > are held constant). 
>> >  
>> > The obstacle to computing either least-squares means or effect 
>> displays 
>> > in R 
>> > via predict() is that predict() wants factors in the "new  
>> data" to be  
>> > set to particular levels. The effect() function in the  
>> effects package  
>> > bypasses 
>> > predict() and works directly with the model matrix,  
>> averaging over the  
>> > columns that pertain to a factor (and reconstructing  
>> interactions as  
>> > necessary). As mentioned, this has the effect of setting  
>> the factor to  
>> > its proportional distribution in the data. This approach  
>> also has the  
>> > advantage of being invariant with respect to the choice of  
>> contrasts  
>> > for a factor. 
>> >  
>> > The only convenient way that I can think of to implement  
>> least-squares  
>> > means in R would be to use deviation-coded regressors for a factor  
>> > (that is, 
>> > contr.sum) and then to set the columns of the model matrix for the 
>> > factor(s) 
>> > to be averaged over to 0. It may just be that I'm having a  
>> failure of  
>> > imagination and that there's a better way to proceed. I've not  
>> > implemented this solution because it is dependent upon the  
>> choice of  
>> > contrasts and because I don't see a general advantage to  
>> it, but since  
>> > the issue has come up several times now, maybe I should  
>> take a crack  
>> > at it. Remember that I want this to work more generally,  
>> not just for  
>> > levels of factors, and not just for linear models. 
>> >  
>> > Brian is quite right in mentioning that he suggested some time ago 
>> that 
>> > I 
>> > use critical values of t rather than of the standard normal  
>> > distribution for producing confidence intervals, and I  
>> agree that it  
>> > makes sense to do so in models in which the dispersion is  
>> estimated.  
>> > My only excuse for not 
>> yet 
>> > doing this is that I want to undertake a more general  
>> revision of the  
>> > effects package, and haven't had time to do it. There are several  
>> > changes that I'd like to make to the package. For example, I have  
>> > results for multinomial and proportional odds logit models  
>> (described  
>> > in a paper 
>> by 
>> > me 
>> > and Bob Andersen in the 2006 issue of Sociological  
>> Methodology) that I  
>> > want to incorporate, and I'd like to improve the appearance of the  
>> > default graphs. But Brian's suggestion is very  
>> straightforward, and I  
>> > guess that I shouldn't wait to implement it; I'll do so very soon. 
>> >  
>> > Regards, 
>> >  John 
>> >  
>> > -------------------------------- 
>> > John Fox 
>> > Department of Sociology 
>> > McMaster University 
>> > Hamilton, Ontario 
>> > Canada L8S 4M4 
>> > 905-525-9140x23604 
>> > http://socserv.mcmaster.ca/jfox 
>> > -------------------------------- 
>> >  
>> > > -----Original Message----- 
>> > > From: r-help-bounces at stat.math.ethz.ch  
>> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof Brian  
>> > > Ripley 
>> > > Sent: Wednesday, March 21, 2007 12:03 PM 
>> > > To: Chuck Cleland 
>> > > Cc: r-help 
>> > > Subject: Re: [R] how to get "lsmeans"? 
>> > > 
>> > > On Wed, 21 Mar 2007, Chuck Cleland wrote: 
>> > > 
>> > > > Liaw, Andy wrote: 
>> > > >> I verified the result from the following with output from JMP 6 
>> on 
>> > > >> the same data (don't have SAS: don't need it): 
>> > > >> 
>> > > >> set.seed(631) 
>> > > >> n <- 100 
>> > > >> dat <- data.frame(y=rnorm(n), A=factor(sample(1:2, n, 
>> > > replace=TRUE)), 
>> > > >>                   B=factor(sample(1:2, n, replace=TRUE)), 
>> > > >>                   C=factor(sample(1:2, n, replace=TRUE)), 
>> > > >>                   d=rnorm(n)) 
>> > > >> fm <- lm(y ~ A + B + C + d, dat) 
>> > > >> ## Form a data frame of points to predict: all 
>> > > combinations of the ## 
>> > > >> three factors and the mean of the covariate. 
>> > > >> p <- data.frame(expand.grid(A=1:2, B=1:2, C=1:2)) p[] <- 
>> lapply(p, 
>> > > >> factor) p <- cbind(p, d=mean(dat$d)) p <- 
>> > > cbind(yhat=predict(fm, p), 
>> > > >> p) ## lsmeans for the three factors: 
>> > > >> with(p, tapply(yhat, A, mean)) 
>> > > >> with(p, tapply(yhat, B, mean)) 
>> > > >> with(p, tapply(yhat, C, mean)) 
>> > > > 
>> > > >  Using Andy's example data, these are the LSMEANS and 
>> > > intervals I get 
>> > > > from SAS: 
>> > > > 
>> > > > A        y LSMEAN      95% Confidence Limits 
>> > > > 1       -0.071847       -0.387507     0.243813 
>> > > > 2       -0.029621       -0.342358     0.283117 
>> > > > 
>> > > > B        y LSMEAN      95% Confidence Limits 
>> > > > 1       -0.104859       -0.397935     0.188216 
>> > > > 2        0.003391       -0.333476     0.340258 
>> > > > 
>> > > > C        y LSMEAN      95% Confidence Limits 
>> > > > 1       -0.084679       -0.392343     0.222986 
>> > > > 2       -0.016789       -0.336374     0.302795 
>> > > > 
>> > > >  One way of reproducing the LSMEANS and intervals from SAS using 
>> > > > predict() seems to be the following: 
>> > > > 
>> > > >> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d, 
>> > > data = dat) 
>> > > >> newdat <-  
>> expand.grid(A=factor(c(1,2)),B=1.5,C=1.5,d=mean(dat$d)) 
>> > > >> cbind(newdat, predict(dat.lm, newdat, interval="confidence")) 
>> > > >  A   B   C          d         fit        lwr       upr 
>> > > > 1 1 1.5 1.5 0.09838595 -0.07184709 -0.3875070 0.2438128 
>> > > > 2 2 1.5 1.5 0.09838595 -0.02962086 -0.3423582 0.2831165 
>> > > > 
>> > > >  However, another possibility seems to be: 
>> > > > 
>> > > >> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d, 
>> > > data = dat) 
>> > > >> newdat <- 
>> > > > 
>> > > 
>> >  
>> expand.grid(A=factor(c(1,2)),B=mean(as.numeric(dat$B)),C=mean(as.numer 
>> > > > ic(dat$C)),d=mean(dat$d)) 
>> > > >> cbind(newdat, predict(dat.lm, newdat, interval="confidence")) 
>> > > >  A    B    C          d         fit        lwr       upr 
>> > > > 1 1 1.43 1.48 0.09838595 -0.08078243 -0.3964661 0.2349012 
>> > > > 2 2 1.43 1.48 0.09838595 -0.03855619 -0.3479589 0.2708465 
>> > > > 
>> > > >  The predictions directly above match what effect() in  
>> the effects  
>> > > > package by John Fox returns: 
>> > > > 
>> > > > library(effects) 
>> > > > 
>> > > >> effect("A", fm, xlevels=list(d = mean(dat$D))) 
>> > > > 
>> > > > A effect 
>> > > > A 
>> > > >          1           2 
>> > > > -0.08078243 -0.03855619 
>> > > > 
>> > > >  But for some reason the predict() and effect() intervals 
>> > > are a little 
>> > > > different: 
>> > > > 
>> > > >> effect("A", fm, xlevels=list(d = mean(dat$D)))$lower 
>> > > >          [,1] 
>> > > > 101 -0.3924451 
>> > > > 102 -0.3440179 
>> > > > 
>> > > >> effect("A", fm, xlevels=list(d = mean(dat$D)))$upper 
>> > > >         [,1] 
>> > > > 101 0.2308802 
>> > > > 102 0.2669055 
>> > > > 
>> > > >  I would be interested in any comments on these different 
>> > > approaches 
>> > > > and on the difference in intervals returned by predict() 
>> > > and effect(). 
>> > > 
>> > > AFAIR, the effects packages uses normal-based confidence  
>> intervals  
>> > > and predict.lm uses t-based ones, and I have suggested to  
>> John Fox  
>> > > that t-based intervals would be preferable, at least as an option. 
>> > > 
>> > > 
>> > > -- 
>> > > Brian D. Ripley,                  ripley at stats.ox.ac.uk 
>> > > Professor of Applied Statistics,   
>> http://www.stats.ox.ac.uk/~ripley/ 
>> > > University of Oxford,             Tel:  +44 1865 272861 (self) 
>> > > 1 South Parks Road,                     +44 1865 272866 (PA) 
>> > > Oxford OX1 3TG, UK                Fax:  +44 1865 272595 
>> > > 
>> > > ______________________________________________ 
>> > > R-help at stat.math.ethz.ch mailing list  
>> > > https://stat.ethz.ch/mailman/listinfo/r-help 
>> > > PLEASE do read the posting guide 
>> > > http://www.R-project.org/posting-guide.html 
>> > > and provide commented, minimal, self-contained, reproducible code. 
>> >  
>> > ______________________________________________ 
>> > R-help at stat.math.ethz.ch mailing list 
>> > https://stat.ethz.ch/mailman/listinfo/r-help 
>> > PLEASE do read the posting guide http://www.R-project.org/posting-  
>> > guide.html and provide commented, minimal, self-contained,  
>> > reproducible code. 
>>  
>> ______________________________________________ 
>> R-help at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-help 
>> PLEASE do read the posting guide  
>> http://www.R-project.org/posting-guide.html 
>> and provide commented, minimal, self-contained, reproducible code. 
> 
>______________________________________________ 
>R-help at stat.math.ethz.ch mailing list 
>https://stat.ethz.ch/mailman/listinfo/r-help 
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
>and provide commented, minimal, self-contained, reproducible code. 
>


From bqcaomail at gmail.com  Thu Mar 22 16:26:15 2007
From: bqcaomail at gmail.com (Baoqiang Cao)
Date: Thu, 22 Mar 2007 10:26:15 -0500
Subject: [R] [solved] Re: "digits" doesn't work in format function
In-Reply-To: <5c5a748a0703220759h72b90effjab776e99c6a9e672@mail.gmail.com>
References: <5c5a748a0703220759h72b90effjab776e99c6a9e672@mail.gmail.com>
Message-ID: <5c5a748a0703220826g357baf2hf70a0fd5bdeb7b93@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070322/26c7e6ee/attachment.pl 

From r.hankin at noc.soton.ac.uk  Thu Mar 22 16:47:42 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 22 Mar 2007 15:47:42 +0000
Subject: [R] quick legend() question
Message-ID: <EF164949-F6CE-4DBF-8FC9-8CC34EC4784A@soc.soton.ac.uk>

Hi

I have a scatterplot of points with pch=1 and a single point with  
pch=3, lwd=3.
It has a high line width to attract attention to it.

The following script



plot(rnorm(10),rnorm(10),col="black")
points(rnorm(10),rnorm(10),col="red")
points(0,0,pch=3,lwd=3)


if(TRUE){
   legend("bottomleft",c("a","b","Truth"),pch=c(1,1,3),col=c 
("black","red","black"))
} else {
   legend("bottomleft",c("a","b","Truth"),pch=c(1,1,3),col=c 
("black","red","black"),lwd=c(0,0,3))
}


doesn't quite work as desired:  the third symbol in the legend is not  
the right line width.

Replacing TRUE with FALSE doesn't work as desired either; the first two
symbols end up with a line I don't want.
The same happens with lwd=c(NA,NA,3).

How to coerce legend()   into doing what I want?




--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From clists at perrin.socsci.unc.edu  Thu Mar 22 16:57:10 2007
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Thu, 22 Mar 2007 11:57:10 -0400 (EDT)
Subject: [R] Cohen's Kappa
In-Reply-To: <46029F5B.5050206@web.de>
References: <46029F5B.5050206@web.de>
Message-ID: <Pine.LNX.4.64.0703221156560.13226@perrin.socsci.unc.edu>

Shouldn't that be

kappa = rater1 - rater2 / (1-chance)

?

Andy

----------------------------------------------------------------------
Andrew J Perrin - andrew_perrin (at) unc.edu - http://perrin.socsci.unc.edu
Assistant Professor of Sociology; Book Review Editor, _Social Forces_
University of North Carolina - CB#3210, Chapel Hill, NC 27599-3210 USA
New Book: http://www.press.uchicago.edu/cgi-bin/hfs.cgi/00/178592.ctl



On Thu, 22 Mar 2007, Christian Schulz wrote:

> Hi,
>
> im little bit confused about Cohen's Kappa and i should  be look into the
> Kappa function code. Is the easy formula really wrong?
>
> kappa=agreement-chance/(1-chance)
>
> many thanks
> christian
>
> ###############################################################################
> true-negativ:7445
> false-positive:3410
> false-negativ:347
> true-positiv:772
>
> classification-aggrement:68,6%
> kappa=agreement-chance/(1-chance) = (0.686-0.5)/0.5=0.372
>
> .....with function from library(vcd)
> Kappa(matrix(c(7445,3410,347,772),nrow=2))
>               value         ASE
> Unweighted 0.1686882 0.011235188
> Weighted   0.1686882 0.007979293
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jfox at mcmaster.ca  Thu Mar 22 17:16:56 2007
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 22 Mar 2007 12:16:56 -0400
Subject: [R] how to get "lsmeans"?
In-Reply-To: <460297C3.3040208@vanderbilt.edu>
Message-ID: <20070322161657.EVLS1646.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear Frank,

> -----Original Message-----
> From: Frank E Harrell Jr [mailto:f.harrell at vanderbilt.edu] 
> Sent: Thursday, March 22, 2007 10:51 AM
> To: John Fox
> Cc: 'Prof Brian Ripley'; 'r-help'; 'Chuck Cleland'; 'Liaw, Andy'
> Subject: Re: [R] how to get "lsmeans"?
> 
> John Fox wrote:
> > Dear Frank,
> > 
> > The object is to focus on a high-order term of the model, holding 
> > other terms "constant" at typical values; in the case of a 
> factor, a 
> > "typical value" is ambiguous, so an average is taken over 
> the levels of the factor.
> > If the factor is, e.g., gender, one could produce an 
> estimate of the 
> > average fitted value for a population composed equally of men and 
> > women, or of a population composed of men and women in 
> proportion to 
> > their distribution in the data. Otherwise, one would have 
> to produce 
> > separate sets of fitted values for men and women, with the 
> number of 
> > such sets increasing as the number of levels of the factors held 
> > constant increase. On the scale of the linear predictor, 
> these sets would differ only by constants.
> > 
> > Regards,
> >  John
> 
> Makes sense.  I just set other factors to the mode, and if it 
> is important to see estimates for other categories, I give 
> estimates for each factor level.  If I want to uncondition on 
> a variable (not often) I take the proper weighted average of 
> predicted values.
> 

The mode seens arbitrary to me, but I don't think that there's a unique
right answer here. The weighted average (using sample proportions) is what
the effect() function does.

Regards,
 John

> Cheers
> Frank
> 
> > 
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> > 
> >> -----Original Message-----
> >> From: Frank E Harrell Jr [mailto:f.harrell at vanderbilt.edu]
> >> Sent: Thursday, March 22, 2007 8:41 AM
> >> To: John Fox
> >> Cc: 'Prof Brian Ripley'; 'r-help'; 'Chuck Cleland'; 'Liaw, Andy'
> >> Subject: Re: [R] how to get "lsmeans"?
> >>
> >> John Fox wrote:
> >>> Dear Brian et al.,
> >>>
> >>> My apologies for chiming in late: It's been a busy day.
> >>>
> >>> First some general comments on "least-squares means" and
> >> "effect displays."
> >> ... much good stuff deleted ...
> >>
> >> John - the one thing I didn't get from your post is a 
> motivation to 
> >> do all that as opposed to easy-to-explain predicted 
> values.  I would 
> >> appreciate your thoughts.
> >>
> >> Thanks
> >> Frank
> >>
> >> -- 
> >> Frank E Harrell Jr   Professor and Chair           School 
> of Medicine
> >>                       Department of Biostatistics   
> >> Vanderbilt University
> > 
> > 
> 
> 
> -- 
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                       Department of Biostatistics   
> Vanderbilt University


From marc_schwartz at comcast.net  Thu Mar 22 17:21:08 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 22 Mar 2007 11:21:08 -0500
Subject: [R] quick legend() question
In-Reply-To: <EF164949-F6CE-4DBF-8FC9-8CC34EC4784A@soc.soton.ac.uk>
References: <EF164949-F6CE-4DBF-8FC9-8CC34EC4784A@soc.soton.ac.uk>
Message-ID: <1174580468.5102.21.camel@localhost.localdomain>

On Thu, 2007-03-22 at 15:47 +0000, Robin Hankin wrote:
> Hi
> 
> I have a scatterplot of points with pch=1 and a single point with  
> pch=3, lwd=3.
> It has a high line width to attract attention to it.
> 
> The following script
> 
> 
> 
> plot(rnorm(10),rnorm(10),col="black")
> points(rnorm(10),rnorm(10),col="red")
> points(0,0,pch=3,lwd=3)
> 
> 
> if(TRUE){
>    legend("bottomleft",c("a","b","Truth"),pch=c(1,1,3),col=c 
> ("black","red","black"))
> } else {
>    legend("bottomleft",c("a","b","Truth"),pch=c(1,1,3),col=c 
> ("black","red","black"),lwd=c(0,0,3))
> }
> 
> 
> doesn't quite work as desired:  the third symbol in the legend is not  
> the right line width.
> 
> Replacing TRUE with FALSE doesn't work as desired either; the first two
> symbols end up with a line I don't want.
> The same happens with lwd=c(NA,NA,3).
> 
> How to coerce legend()   into doing what I want?

Robin,

Is this what you want?


plot(rnorm(10), rnorm(10), col = "black")

points(rnorm(10), rnorm(10), col = "red")

points(0, 0, pch = 3, lwd = 3)

legend("bottomleft", c("a", "b", "Truthiness"), pch = c(1, 1, 3), 
       col = c("black", "red", "black"), lwd = c(0, 0, 3), 
       lty = c(0, 0, 1))


Add 'lty' to the legend() call so that the line types for the first two
symbols are set to 'blank'.

HTH,

Marc Schwartz


From ggrothendieck at gmail.com  Thu Mar 22 17:27:38 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 22 Mar 2007 12:27:38 -0400
Subject: [R] quick legend() question
In-Reply-To: <EF164949-F6CE-4DBF-8FC9-8CC34EC4784A@soc.soton.ac.uk>
References: <EF164949-F6CE-4DBF-8FC9-8CC34EC4784A@soc.soton.ac.uk>
Message-ID: <971536df0703220927ue218643k857fafc0b41331b8@mail.gmail.com>

Try pt.lwd= instead of lwd= in your legend call.

On 3/22/07, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
> Hi
>
> I have a scatterplot of points with pch=1 and a single point with
> pch=3, lwd=3.
> It has a high line width to attract attention to it.
>
> The following script
>
>
>
> plot(rnorm(10),rnorm(10),col="black")
> points(rnorm(10),rnorm(10),col="red")
> points(0,0,pch=3,lwd=3)
>
>
> if(TRUE){
>   legend("bottomleft",c("a","b","Truth"),pch=c(1,1,3),col=c
> ("black","red","black"))
> } else {
>   legend("bottomleft",c("a","b","Truth"),pch=c(1,1,3),col=c
> ("black","red","black"),lwd=c(0,0,3))
> }
>
>
> doesn't quite work as desired:  the third symbol in the legend is not
> the right line width.
>
> Replacing TRUE with FALSE doesn't work as desired either; the first two
> symbols end up with a line I don't want.
> The same happens with lwd=c(NA,NA,3).
>
> How to coerce legend()   into doing what I want?
>
>
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From nathan.weisz at uni-konstanz.de  Thu Mar 22 17:37:19 2007
From: nathan.weisz at uni-konstanz.de (Nathan Weisz)
Date: Thu, 22 Mar 2007 17:37:19 +0100
Subject: [R] finding "clusters" in 2-dimensional data
Message-ID: <1174581439.4602b0bf10cf9@webmail.uni-konstanz.de>


hi,

i have done a time-frequency analysis on magnetencephalographic data
under different conditions. the matrices for each condition are three
dimensional: frequency x time x subject.
to identify time-frequency "areas" of interest i would first calculate a
simple t-test for each time frequency point. this is straightforward and
for each comparison i have a frequency x time representation of
t-values. using an alpha threshold "candidate" time-frequency regions
can be identified. I will just call them clusters for simplicity. the
t-values within each cluster are added (T).
to find out which clusters can be considered significant i would like to
implement a randomization test to generate a distribution of Tmax with
which I could compare my original Ts with.

However I'm stuck at the lowest possible level ... well the second
lowest level (the t.test is easy) :-)
starting from the time x frequency representation of t-values, masked so
that all values with p's below some magical number are set to 0: is
there some ready-made function in R that can help me identifying the
clusters (i.e. returning the array-indexes)?

the general idea outlined here is implemented under Matlab in the
Fieldtrip toolbox. however, the relevant function relies on functions
from the Image Processing toolbox (which i don't have). furthermore i
would really like to outsource statistics from Matlab to R whenever I
can.

Thanks ins advance for any help,
nathan


From enrico.foscolo at libero.it  Thu Mar 22 18:32:39 2007
From: enrico.foscolo at libero.it (enrico.foscolo)
Date: Thu, 22 Mar 2007 18:32:39 +0100
Subject: [R] Search zeros of one function
Message-ID: <JFBFEF$F7E8ED40AEC00A7266ECB5533967AF7B@libero.it>

Dear participants to the list,

this is my problem: I want to find zeros of an arbitrary function.
With "uniroot" (package "stats") it is possible to find (numerically) a root of the function,
but I do not know how I can find at the same time the zeros
in a determined interval (if this function must have more of one solution).

For example: suppose that I have a function of this form:
f(x)=cos(x).
I can find a lot of roots.
In fact, let x be in [-10,10]:

> f<-function(x){return(cos(x))}
> uniroot(f,lower=-10,upper=-5)
$root
[1] -7.85401

$f.root
[1] -2.895499e-05

$iter
[1] 6

$estim.prec
[1] 6.103516e-05
> uniroot(f,lower=-5,upper=-2)
$root
[1] -4.712389

$f.root
[1] -5.805468e-08

$iter
[1] 5

$estim.prec
[1] 6.103516e-05

...

At last, I would want to ask to you if a package (or a function) exists that it knows
to resolve the problem of the search of the maximums or minimums (local and global)
of one function.

Many thanks for any kind of help in advance,
Enrico


------------------------------------------------------
Passa a Infostrada. ADSL e Telefono senza limiti e senza canone Telecom
http://click.libero.it/infostrada


From mike.prager at noaa.gov  Thu Mar 22 18:43:41 2007
From: mike.prager at noaa.gov (Mike Prager)
Date: Thu, 22 Mar 2007 13:43:41 -0400
Subject: [R] Ticks on barplots
References: <46005A57.2000709@noaa.gov>
	<1174444859.5415.57.camel@localhost.localdomain>
Message-ID: <qjf503lorga4pkhctp6dhfov0tso2h356b@4ax.com>

Marc Schwartz <marc_schwartz at comcast.net> wrote:

> On Tue, 2007-03-20 at 18:04 -0400, Michael H. Prager wrote:
> > Dear Gurus,
> > 
> > Using R 2.4.1 on Windows XP
> > 
> > I am generating stacked barplots of age-composition of fish populations 
> > (Y) over time (X).  As there are many years, not every bars is labeled.  
> > When looking at the plot, it becomes difficult to associate labels with 
> > their bars.
> > 
> > We have improved this a bit by using axis() to add a tickmark below each 
> > bar.  Can anyone suggest a way to draw ticks ONLY at bars where a tick 
> > label is drawn?  Or to make such ticks longer than those where there is 
> > no label?
> > 
> > This is going into a function, so I'm hoping for a method that doesn't 
> > require looking at the plot first.
> > 
> >  [...]
> > 
> > Mike Prager
> > NOAA, Beaufort, NC
> 
> Mike,
> 
> How about something like this:
> 
>   mp <- barplot(1:50, axisnames = FALSE)
> 
>   # Create short tick marks at each bar
>   axis(1, at = mp, labels = rep("", 50), tcl = -0.25)
> 
>   # Create longer tick marks every 5 years with labels
>   axis(1, at = mp[seq(1, 50, 5)], 
>        labels = 1900 + seq(0, 45, 5), tcl = -0.75, las = 2, 
>        cex.axis = 0.75)
> 
> 
> Just pick which labels you want to be shown (eg. every 5 years) and
> synchronize the values of those with the 'at' argument in axis().


Based on your suggestion, I hacked together something more
general that works nicely for up to 200 bars, which exceeds the
range of data we expect.  A sample program looks like this:

# Make a bar chart with major and minor ticks
# Example R code
# M. H. Prager     22 Mar 2007 
nbars <- 200
# Make phony data and labels:
dataset <- abs(rnorm(nbars))
datalabels <- seq(from = 1949, along.with = dataset)
# Make barplot without x-axis:
mp <- barplot(t(dataset), xlab = "Year", ylab = "Numbers",
    axisnames = FALSE, space = 0)
# Get major and minor multiples for choosing labels:
ntick <- length(mp)
{   if (ntick < 16) mult = c(2, 2)
    else if(ntick < 41) mult = c(5, 5)
    else if (ntick < 101) mult = c(10, 5)
    else mult = c(20, 5)
}
label.index <- which(datalabels %% mult[1] == 0)
minor.index = which(datalabels %% mult[2] == 0)
# Draw all ticks:
axis(side = 1, at = mp, labels = FALSE, tcl = -0.2)
# Draw minor ticks:
axis(side = 1, at = mp[minor.index], labels = FALSE, tcl = -0.5)
# Draw major ticks & labels:
axis(side = 1, at = mp[label.index], labels =
datalabels[label.index], tcl = -0.7)


Mike

-- 
Mike Prager, NOAA, Beaufort, NC
* Opinions expressed are personal and not represented otherwise.
* Any use of tradenames does not constitute a NOAA endorsement.


From mike.prager at noaa.gov  Thu Mar 22 19:00:31 2007
From: mike.prager at noaa.gov (Mike Prager)
Date: Thu, 22 Mar 2007 14:00:31 -0400
Subject: [R] Ticks and labels on plots
References: <46005A57.2000709@noaa.gov>
	<1174444859.5415.57.camel@localhost.localdomain>
	<b5t20314vt79f5g91d22rabhpkt0oljgra@4ax.com>
	<1174504992.7212.42.camel@localhost.localdomain>
	<17922.18040.856404.895212@stat.math.ethz.ch>
Message-ID: <0fg5039eblqpab04e0boe2ljsvsjn8birq@4ax.com>

Martin Maechler <maechler at stat.math.ethz.ch> wrote:

> [...] there's one important thing which I think hasn't
> been mentioned yet.
> 
> We have now been talking how and where axis() {i.e. its internal
> code} chooses to place tick marks.
> The clue is that it draws labels at all tick marks by default or
> explicitly with axis(*, at= ., labels = .)
> BUT the labels are not shown on your plot as soon as they
> would get too much crammed together.

Yes, Martin, precisely that distinction made my problem a bit
more than trivial.  It's easy to find the location of *ticks*,
but not of *tick labels*, and in barplot() that is an especially
important distinction, since a tick is placed at every bar.

I just posted a solution that solves the problem for the range
of sizes that we expect to encounter. It gives much nicer
looking results in such cases than a solution without the extra
steps.

Mike

-- 
Mike Prager, NOAA, Beaufort, NC
* Opinions expressed are personal and not represented otherwise.
* Any use of tradenames does not constitute a NOAA endorsement.


From jrkrideau at yahoo.ca  Thu Mar 22 19:18:30 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 22 Mar 2007 14:18:30 -0400 (EDT)
Subject: [R] Truncated x-axis values
In-Reply-To: <1266A6320AD3884EA8142B354DFBA31A06EB0122@dfre02.ent.ti.com>
Message-ID: <8123.31643.qm@web32814.mail.mud.yahoo.com>

You really have not told us much about what you're
actually doing.  A simple self-contained example of
what you're trying to do might let someone help. 

Have you read the posting guide?

--- "Urban, Alexander" <a-urban at ti.com> wrote:

> Hello 
> 
> I'm new to this group. I looked up the last two hour
> in the help file
> and in the archives of this group, but didn't find
> anything.
> I hope my question is not too dump:
> I'm printing a graph with vertical labels on the
> x-axis (necessary due
> to many labels). Unfortunately the png truncates the
> labels halfway
> through, so that you can only read the last 7 digits
> of the label.
> Snice I'm already asking :-): Is there a possibility
> to tell R: If there
> are so many labels that you write them on top of
> each other, take only
> e.g. every 2nd...
> 
> Sorry for bothering and thanks
> Alex
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From smcnary at charm.net  Thu Mar 22 20:03:07 2007
From: smcnary at charm.net (Scot W McNary)
Date: Thu, 22 Mar 2007 14:03:07 -0500 (EST)
Subject: [R] Cohen's Kappa
In-Reply-To: <46029F5B.5050206@web.de>
References: <46029F5B.5050206@web.de>
Message-ID: <20070322140113.A15436@fellspt.charm.net>


Hi,

Chance is not .5 in your data, it's a function of the expected values 
for presence and absence:

> (((7792*10855)/11974) + ((4182*1119)/11974))/11974
[1] 0.6225686

> (.6862368-.6225686)/(1-.6225686)
[1] 0.1686881

Scot


On Thu, 22 Mar 2007, Christian Schulz wrote:

> Hi,
>
> im little bit confused about Cohen's Kappa and i should  be look into the
> Kappa function code. Is the easy formula really wrong?
>
> kappa=agreement-chance/(1-chance)
>
> many thanks
> christian
>
> ###############################################################################
> true-negativ:7445
> false-positive:3410
> false-negativ:347
> true-positiv:772
>
> classification-aggrement:68,6%
> kappa=agreement-chance/(1-chance) = (0.686-0.5)/0.5=0.372
>
> .....with function from library(vcd)
> Kappa(matrix(c(7445,3410,347,772),nrow=2))
>               value         ASE
> Unweighted 0.1686882 0.011235188
> Weighted   0.1686882 0.007979293
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


--
   Scot W. McNary  email: smcnaryatcharmdotnet


From bates at stat.wisc.edu  Thu Mar 22 20:16:35 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 22 Mar 2007 14:16:35 -0500
Subject: [R] non-linear curve fitting
In-Reply-To: <832A948B92E5754D9062CCF62F9ED892655FE6@xmail01.ad.ua.ac.be>
References: <46027E46.8090701@sciviews.org>
	<832A948B92E5754D9062CCF62F9ED892655FE6@xmail01.ad.ua.ac.be>
Message-ID: <40e66e0b0703221216q76d99503g37e5cf036ed04ba@mail.gmail.com>

On 3/22/07, Hufkens Koen <koen.hufkens at ua.ac.be> wrote:
> Is there a means of getting an F-statistic (p-value) out of all of this.

> Because least-square criterion / r-square only tell me how good the fit is and not necessarily how solid this fit is. An F-statistic (p-value) would be nice...

What would the F-statistic be?  For a linear model with an intercept
the F-statistic represents a comparison of the model that you have fit
to the trivial model (intercept only).  It is important that the
models being compared are nested - otherwise the F statistic is of
questionable validity.

For the logistic growth model you described (which is not quite the
one fit by SSlogis - that model has one more parameter, a scale factor
on the response) the response always goes to zero as x -> -\Infty and
to one as x -> \Infty.  The trivial model is not nested within this
model for finite parameter values so I'm not sure what hypotheses
would be tested by an F-statistic.

> Regards,
> Koen
>
>
> > -----Original Message-----
> > From: Philippe Grosjean [mailto:phgrosjean at sciviews.org]
> > Sent: donderdag 22 maart 2007 14:02
> > To: Hufkens Koen; r-help at stat.math.ethz.ch
> > Subject: Re: [R] non-linear curve fitting
> >
> > Hello,
> >
> > If a least-square criterion is fine for you, you should use
> > nls(). For the logistic curve, you have a convenient
> > self-starting model available:
> > SSlogis(). Look at:
> >
> > ?nls
> > ?SSlogis
> >
> > Best,
> >
> > Philippe Grosjean
> >
> > ..............................................<?}))><........
> >   ) ) ) ) )
> > ( ( ( ( (    Prof. Philippe Grosjean
> >   ) ) ) ) )
> > ( ( ( ( (    Numerical Ecology of Aquatic Systems
> >   ) ) ) ) )   Mons-Hainaut University, Belgium
> > ( ( ( ( (
> > ..............................................................
> >
> > Hufkens Koen wrote:
> > > Hi list,
> > >
> > > I have a little curve fitting problem.
> > >
> > > I would like to fit a sigmoid curve to my data using the
> > following equation:
> > >
> > > f(x) = 1/(1 + exp(-(x-c)*b)) (or any other form for that matter)
> > >
> > > Where x is the distance/location within the dataframe, c is
> > the shift of the curve across the dataframe and b is the
> > steepness of the curve.
> > >
> > > I've been playing with glm() and glm.fit() but without any luck.
> > >
> > > for example the most simple example
> > >
> > > x = -10:10
> > > y = 1/(1 + exp(-x))
> > > glm(y ~ x, family=binomial(link="logit"))
> > >
> > > I get a warning:
> > > non-integer #successes in a binomial glm! in: eval(expr, envir,
> > > enclos)
> > >
> > > and some erratic results
> > >
> > > This is the most simple test to see if I could fit a curve
> > to this perfect data so since this didn't work out, bringing
> > in the extra parameters is a whole other ballgame so could
> > someone give me a clue?
> > >
> > > Kind regards,
> > > Koen
> > >
> > >
> >
> > --
> > No virus found in this incoming message.
> > Checked by AVG Free Edition.
> > Version: 7.5.446 / Virus Database: 268.18.16/729 - Release
> > Date: 21/03/2007 7:52
> >
> >
>
> --
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From f.harrell at vanderbilt.edu  Thu Mar 22 13:41:09 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 22 Mar 2007 07:41:09 -0500
Subject: [R] how to get "lsmeans"?
In-Reply-To: <20070322005910.VVAT1624.tomts40-srv.bellnexxia.net@JohnDesktop8300>
References: <20070322005910.VVAT1624.tomts40-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <46027965.3070709@vanderbilt.edu>

John Fox wrote:
> Dear Brian et al.,
> 
> My apologies for chiming in late: It's been a busy day.
> 
> First some general comments on "least-squares means" and "effect displays."
... much good stuff deleted ...

John - the one thing I didn't get from your post is a motivation to do 
all that as opposed to easy-to-explain predicted values.  I would 
appreciate your thoughts.

Thanks
Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From jfox at mcmaster.ca  Thu Mar 22 15:19:27 2007
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 22 Mar 2007 10:19:27 -0400
Subject: [R] how to get "lsmeans"?
In-Reply-To: <46027965.3070709@vanderbilt.edu>
Message-ID: <20070322141928.KXYT1521.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Frank,

The object is to focus on a high-order term of the model, holding other
terms "constant" at typical values; in the case of a factor, a "typical
value" is ambiguous, so an average is taken over the levels of the factor.
If the factor is, e.g., gender, one could produce an estimate of the average
fitted value for a population composed equally of men and women, or of a
population composed of men and women in proportion to their distribution in
the data. Otherwise, one would have to produce separate sets of fitted
values for men and women, with the number of such sets increasing as the
number of levels of the factors held constant increase. On the scale of the
linear predictor, these sets would differ only by constants.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Frank E Harrell Jr [mailto:f.harrell at vanderbilt.edu] 
> Sent: Thursday, March 22, 2007 8:41 AM
> To: John Fox
> Cc: 'Prof Brian Ripley'; 'r-help'; 'Chuck Cleland'; 'Liaw, Andy'
> Subject: Re: [R] how to get "lsmeans"?
> 
> John Fox wrote:
> > Dear Brian et al.,
> > 
> > My apologies for chiming in late: It's been a busy day.
> > 
> > First some general comments on "least-squares means" and 
> "effect displays."
> ... much good stuff deleted ...
> 
> John - the one thing I didn't get from your post is a 
> motivation to do all that as opposed to easy-to-explain 
> predicted values.  I would appreciate your thoughts.
> 
> Thanks
> Frank
> 
> -- 
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                       Department of Biostatistics   
> Vanderbilt University


From f.harrell at vanderbilt.edu  Thu Mar 22 15:50:43 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 22 Mar 2007 09:50:43 -0500
Subject: [R] how to get "lsmeans"?
In-Reply-To: <20070322141928.KXYT1521.tomts25-srv.bellnexxia.net@JohnDesktop8300>
References: <20070322141928.KXYT1521.tomts25-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <460297C3.3040208@vanderbilt.edu>

John Fox wrote:
> Dear Frank,
> 
> The object is to focus on a high-order term of the model, holding other
> terms "constant" at typical values; in the case of a factor, a "typical
> value" is ambiguous, so an average is taken over the levels of the factor.
> If the factor is, e.g., gender, one could produce an estimate of the average
> fitted value for a population composed equally of men and women, or of a
> population composed of men and women in proportion to their distribution in
> the data. Otherwise, one would have to produce separate sets of fitted
> values for men and women, with the number of such sets increasing as the
> number of levels of the factors held constant increase. On the scale of the
> linear predictor, these sets would differ only by constants.
> 
> Regards,
>  John

Makes sense.  I just set other factors to the mode, and if it is 
important to see estimates for other categories, I give estimates for 
each factor level.  If I want to uncondition on a variable (not often) I 
take the proper weighted average of predicted values.

Cheers
Frank

> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
>> -----Original Message-----
>> From: Frank E Harrell Jr [mailto:f.harrell at vanderbilt.edu] 
>> Sent: Thursday, March 22, 2007 8:41 AM
>> To: John Fox
>> Cc: 'Prof Brian Ripley'; 'r-help'; 'Chuck Cleland'; 'Liaw, Andy'
>> Subject: Re: [R] how to get "lsmeans"?
>>
>> John Fox wrote:
>>> Dear Brian et al.,
>>>
>>> My apologies for chiming in late: It's been a busy day.
>>>
>>> First some general comments on "least-squares means" and 
>> "effect displays."
>> ... much good stuff deleted ...
>>
>> John - the one thing I didn't get from your post is a 
>> motivation to do all that as opposed to easy-to-explain 
>> predicted values.  I would appreciate your thoughts.
>>
>> Thanks
>> Frank
>>
>> -- 
>> Frank E Harrell Jr   Professor and Chair           School of Medicine
>>                       Department of Biostatistics   
>> Vanderbilt University
> 
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From a-urban at ti.com  Thu Mar 22 20:41:49 2007
From: a-urban at ti.com (Urban, Alexander)
Date: Thu, 22 Mar 2007 20:41:49 +0100
Subject: [R] Truncated x-axis values
In-Reply-To: <8123.31643.qm@web32814.mail.mud.yahoo.com>
Message-ID: <1266A6320AD3884EA8142B354DFBA31A06EB0496@dfre02.ent.ti.com>

John
Thanks for your reply and sorry for not beein specific enough
Try this piece of code (it's an abstraction of my application...)

par(las = 2)
par(mfrow=c(2, 1))
x =  c(1,2,3,4,5,6,7,8)
y = c("abcdccefghijk", "bcssdefghijkl", "abddessfghijk",
"bddessfghijkl","abcdssedghijk","bcdedghijkl","assbcdefghidk","bcdedssgh
ijkl")

boxplot(x~y)
boxplot(x~y) 

In the lower chart the labels are truncated...(not totally visible)
Is this clearer now?

Thanks
Alex

-----Original Message-----
From: John Kane [mailto:jrkrideau at yahoo.ca] 
Sent: Thursday, March 22, 2007 19:19
To: Urban, Alexander; r-help at stat.math.ethz.ch
Subject: Re: [R] Truncated x-axis values

You really have not told us much about what you're actually doing.  A
simple self-contained example of what you're trying to do might let
someone help. 

Have you read the posting guide?

--- "Urban, Alexander" <a-urban at ti.com> wrote:

> Hello
> 
> I'm new to this group. I looked up the last two hour in the help file 
> and in the archives of this group, but didn't find anything.
> I hope my question is not too dump:
> I'm printing a graph with vertical labels on the x-axis (necessary due

> to many labels). Unfortunately the png truncates the labels halfway 
> through, so that you can only read the last 7 digits of the label.
> Snice I'm already asking :-): Is there a possibility to tell R: If 
> there are so many labels that you write them on top of each other, 
> take only e.g. every 2nd...
> 
> Sorry for bothering and thanks
> Alex
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


__________________________________________________
Do You Yahoo!?
Tired of spam?  Yahoo! Mail has the best spam protection around
http://mail.yahoo.com


From bates at stat.wisc.edu  Thu Mar 22 20:42:44 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 22 Mar 2007 14:42:44 -0500
Subject: [R] how to get "lsmeans"?
In-Reply-To: <D028EEB4CA113D4EAFDD485CCC998277731795@UTKFSVS4.utk.tennessee.edu>
References: <Pine.LNX.4.64.0703211502050.24076@gannet.stats.ox.ac.uk>
	<20070322005910.VVAT1624.tomts40-srv.bellnexxia.net@JohnDesktop8300>
	<D028EEB4CA113D4EAFDD485CCC998277731795@UTKFSVS4.utk.tennessee.edu>
Message-ID: <40e66e0b0703221242w434d3a8el5e5f22ea3639101b@mail.gmail.com>

On 3/22/07, Muenchen, Robert A (Bob) <muenchen at utk.edu> wrote:

> Perhaps I'm stating the obvious, but to increase the use of R in places
> where SAS & SPSS dominate, it's important to make getting the same
> answers as easy as possible. That includes things like lsmeans and type
> III sums of squares. I've read lots of discussions here on sums of
> squares & I'm not advocating type III use, just looking at it from a
> marketing perspective. Too many people look for excuses to not change.
> The fewer excuses, the better.

You may get strong reactions to such a suggestion.  I recommend
reading Bill Venables' famous unpublished paper "Exegeses on linear
models" (google for the title - very few people use "Exegeses" and
"linear models" in the same sentence - in fact I would not be
surprised if Bill was the only one who has ever done so).

You must realize that R is written by experts in statistics and
statistical computing who, despite popular opinion, do not believe
that everything in SAS and SPSS is worth copying.  Some things done in
such packages, which trace their roots back to the days of punched
cards and magnetic tape when fitting a single linear model may take
several days because your first 5 attempts failed due to syntax errors
in the JCL or the SAS code, still reflect the approach of "give me
every possible statistic that could be calculated from this model,
whether or not it makes sense".  The approach taken in R is different.
 The underlying assumption is that the useR is thinking about the
analysis while doing it.

The fact that it is so difficult to explain what lsmeans are and why
they would be of interest is an indication of why they aren't
implemented in any of the required packages.

> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> > bounces at stat.math.ethz.ch] On Behalf Of John Fox
> > Sent: Wednesday, March 21, 2007 8:59 PM
> > To: 'Prof Brian Ripley'
> > Cc: 'r-help'; 'Chuck Cleland'
> > Subject: Re: [R] how to get "lsmeans"?
> >
> > Dear Brian et al.,
> >
> > My apologies for chiming in late: It's been a busy day.
> >
> > First some general comments on "least-squares means" and "effect
> > displays."
> > The general idea behind the two is similar -- to examine fitted values
> > corresponding to a term in a model while holding other terms to
> typical
> > values -- but the implementation is not identical. There are also
> other
> > similar ideas floating around as well. My formulation is more general
> > in the
> > sense that it applies to a wider variety of models, both linear and
> > otherwise.
> >
> > "Least-squares means" (a horrible term, by the way: in a 1980 paper in
> > the
> > American Statistician, Searle, Speed, and Milliken suggested the more
> > descriptive term "population marginal means") apply to factors and
> > combinations of factors; covariates are set to mean values and the
> > levels of
> > other factors are averaged over, in effect applying equal weight to
> > each
> > level. (This is from memory, so it's possible that I'm not getting it
> > quite
> > right, but I believe that I am.) In my effect displays, each level of
> a
> > factor is weighted by its proportion in the data. In models in which
> > least-squares means can be computed, they should differ from the
> > corresponding effect display by a constant (if there are different
> > numbers
> > of observations in the different levels of the factors that are held
> > constant).
> >
> > The obstacle to computing either least-squares means or effect
> displays
> > in R
> > via predict() is that predict() wants factors in the "new data" to be
> > set to
> > particular levels. The effect() function in the effects package
> > bypasses
> > predict() and works directly with the model matrix, averaging over the
> > columns that pertain to a factor (and reconstructing interactions as
> > necessary). As mentioned, this has the effect of setting the factor to
> > its
> > proportional distribution in the data. This approach also has the
> > advantage
> > of being invariant with respect to the choice of contrasts for a
> > factor.
> >
> > The only convenient way that I can think of to implement least-squares
> > means
> > in R would be to use deviation-coded regressors for a factor (that is,
> > contr.sum) and then to set the columns of the model matrix for the
> > factor(s)
> > to be averaged over to 0. It may just be that I'm having a failure of
> > imagination and that there's a better way to proceed. I've not
> > implemented
> > this solution because it is dependent upon the choice of contrasts and
> > because I don't see a general advantage to it, but since the issue has
> > come
> > up several times now, maybe I should take a crack at it. Remember that
> > I
> > want this to work more generally, not just for levels of factors, and
> > not
> > just for linear models.
> >
> > Brian is quite right in mentioning that he suggested some time ago
> that
> > I
> > use critical values of t rather than of the standard normal
> > distribution for
> > producing confidence intervals, and I agree that it makes sense to do
> > so in
> > models in which the dispersion is estimated. My only excuse for not
> yet
> > doing this is that I want to undertake a more general revision of the
> > effects package, and haven't had time to do it. There are several
> > changes
> > that I'd like to make to the package. For example, I have results for
> > multinomial and proportional odds logit models (described in a paper
> by
> > me
> > and Bob Andersen in the 2006 issue of Sociological Methodology) that I
> > want
> > to incorporate, and I'd like to improve the appearance of the default
> > graphs. But Brian's suggestion is very straightforward, and I guess
> > that I
> > shouldn't wait to implement it; I'll do so very soon.
> >
> > Regards,
> >  John
> >
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof
> > > Brian Ripley
> > > Sent: Wednesday, March 21, 2007 12:03 PM
> > > To: Chuck Cleland
> > > Cc: r-help
> > > Subject: Re: [R] how to get "lsmeans"?
> > >
> > > On Wed, 21 Mar 2007, Chuck Cleland wrote:
> > >
> > > > Liaw, Andy wrote:
> > > >> I verified the result from the following with output from JMP 6
> on
> > > >> the same data (don't have SAS: don't need it):
> > > >>
> > > >> set.seed(631)
> > > >> n <- 100
> > > >> dat <- data.frame(y=rnorm(n), A=factor(sample(1:2, n,
> > > replace=TRUE)),
> > > >>                   B=factor(sample(1:2, n, replace=TRUE)),
> > > >>                   C=factor(sample(1:2, n, replace=TRUE)),
> > > >>                   d=rnorm(n))
> > > >> fm <- lm(y ~ A + B + C + d, dat)
> > > >> ## Form a data frame of points to predict: all
> > > combinations of the ##
> > > >> three factors and the mean of the covariate.
> > > >> p <- data.frame(expand.grid(A=1:2, B=1:2, C=1:2)) p[] <-
> lapply(p,
> > > >> factor) p <- cbind(p, d=mean(dat$d)) p <-
> > > cbind(yhat=predict(fm, p),
> > > >> p) ## lsmeans for the three factors:
> > > >> with(p, tapply(yhat, A, mean))
> > > >> with(p, tapply(yhat, B, mean))
> > > >> with(p, tapply(yhat, C, mean))
> > > >
> > > >  Using Andy's example data, these are the LSMEANS and
> > > intervals I get
> > > > from SAS:
> > > >
> > > > A        y LSMEAN      95% Confidence Limits
> > > > 1       -0.071847       -0.387507     0.243813
> > > > 2       -0.029621       -0.342358     0.283117
> > > >
> > > > B        y LSMEAN      95% Confidence Limits
> > > > 1       -0.104859       -0.397935     0.188216
> > > > 2        0.003391       -0.333476     0.340258
> > > >
> > > > C        y LSMEAN      95% Confidence Limits
> > > > 1       -0.084679       -0.392343     0.222986
> > > > 2       -0.016789       -0.336374     0.302795
> > > >
> > > >  One way of reproducing the LSMEANS and intervals from SAS using
> > > > predict() seems to be the following:
> > > >
> > > >> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d,
> > > data = dat)
> > > >> newdat <- expand.grid(A=factor(c(1,2)),B=1.5,C=1.5,d=mean(dat$d))
> > > >> cbind(newdat, predict(dat.lm, newdat, interval="confidence"))
> > > >  A   B   C          d         fit        lwr       upr
> > > > 1 1 1.5 1.5 0.09838595 -0.07184709 -0.3875070 0.2438128
> > > > 2 2 1.5 1.5 0.09838595 -0.02962086 -0.3423582 0.2831165
> > > >
> > > >  However, another possibility seems to be:
> > > >
> > > >> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d,
> > > data = dat)
> > > >> newdat <-
> > > >
> > >
> > expand.grid(A=factor(c(1,2)),B=mean(as.numeric(dat$B)),C=mean(as.numer
> > > > ic(dat$C)),d=mean(dat$d))
> > > >> cbind(newdat, predict(dat.lm, newdat, interval="confidence"))
> > > >  A    B    C          d         fit        lwr       upr
> > > > 1 1 1.43 1.48 0.09838595 -0.08078243 -0.3964661 0.2349012
> > > > 2 2 1.43 1.48 0.09838595 -0.03855619 -0.3479589 0.2708465
> > > >
> > > >  The predictions directly above match what effect() in the effects
> > > > package by John Fox returns:
> > > >
> > > > library(effects)
> > > >
> > > >> effect("A", fm, xlevels=list(d = mean(dat$D)))
> > > >
> > > > A effect
> > > > A
> > > >          1           2
> > > > -0.08078243 -0.03855619
> > > >
> > > >  But for some reason the predict() and effect() intervals
> > > are a little
> > > > different:
> > > >
> > > >> effect("A", fm, xlevels=list(d = mean(dat$D)))$lower
> > > >          [,1]
> > > > 101 -0.3924451
> > > > 102 -0.3440179
> > > >
> > > >> effect("A", fm, xlevels=list(d = mean(dat$D)))$upper
> > > >         [,1]
> > > > 101 0.2308802
> > > > 102 0.2669055
> > > >
> > > >  I would be interested in any comments on these different
> > > approaches
> > > > and on the difference in intervals returned by predict()
> > > and effect().
> > >
> > > AFAIR, the effects packages uses normal-based confidence
> > > intervals and predict.lm uses t-based ones, and I have
> > > suggested to John Fox that t-based intervals would be
> > > preferable, at least as an option.
> > >
> > >
> > > --
> > > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > > University of Oxford,             Tel:  +44 1865 272861 (self)
> > > 1 South Parks Road,                     +44 1865 272866 (PA)
> > > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ivowel at gmail.com  Thu Mar 22 21:10:37 2007
From: ivowel at gmail.com (ivo welch)
Date: Thu, 22 Mar 2007 16:10:37 -0400
Subject: [R] personalize regression printing?
Message-ID: <50d1c22d0703221310v59b7dc63n999fc9d01c73771c@mail.gmail.com>

dear R experts:

I am often struggling with a desire of wanting to change the basic
output that R prints.

For example, a year ago, I wanted to add the mean to the summary()
statement, and eventually got help from friendly souls who showed me
how to copy the summary() routine and then modify it.

now I would like to abbrev some of the output from summary(lm()).  For
example, I want to eliminate the "Residuals" output.  I also am not
quite sure why "Call:" is followed by a new line rather than just with
a continuation of the model itself.  I also wonder why the word
"Coefficients" seems to consume a line without being particularly
helpful.  I wonder if "Coef" could appear before "Estimate Std. Error"
on the same line.  All these are changes that would allow me to see
more information on the same page.

I do know that I can copy the functions themselves, if I can find
them, and replace them myself.  However, this means that future
versions of R may make changes that I may miss completely.  Its a
solution, yes.

However, my first question is---for summary(lm()), would it make sense
to have more options that control the output?  At least to suppress
the printing of the distribution of the residuals?

A long-term solution, which is easy to suggest for me given that I do
not have to do any work to implement it, would be to allow some
templates that specify how output should be formatted.  R would first
load the system templates, and thereafter any templates that the user
specifies (has overridden).  The R functions would then work according
to the current template.    Talking is easy; Walking is hard, of
course.  Just a suggestion...

Regards,

/iaw


From gerifalte28 at hotmail.com  Thu Mar 22 21:50:18 2007
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Thu, 22 Mar 2007 14:50:18 -0600
Subject: [R] Cohen's Kappa
In-Reply-To: <46029F5B.5050206@web.de>
References: <46029F5B.5050206@web.de>
Message-ID: <4602EC0A.30207@hotmail.com>


Cohen, J. (1960). A coefficient of agreement for nominal scales. 
Educational and Psychological Measurement, 20, 37-46.

Cohen, J. (1968). Weighted kappa: Nominal scale agreement with provision 
for scaled disagreement or partial credit. Psychological Bulletin, 70, 
213-220.


Francisco

Dr. Francisco J. Zagmutt
College of Veterinary Medicine and Biomedical Sciences
Colorado State University


Christian Schulz wrote:
> Hi,
> 
> im little bit confused about Cohen's Kappa and i should  be look into the
> Kappa function code. Is the easy formula really wrong?
> 
> kappa=agreement-chance/(1-chance)
> 
> many thanks
> christian
> 
> ###############################################################################
> true-negativ:7445
> false-positive:3410
> false-negativ:347
> true-positiv:772
> 
> classification-aggrement:68,6%
> kappa=agreement-chance/(1-chance) = (0.686-0.5)/0.5=0.372
> 
> ......with function from library(vcd)
> Kappa(matrix(c(7445,3410,347,772),nrow=2))
>                value         ASE
> Unweighted 0.1686882 0.011235188
> Weighted   0.1686882 0.007979293
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gerifalte28 at hotmail.com  Thu Mar 22 21:50:18 2007
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Thu, 22 Mar 2007 14:50:18 -0600
Subject: [R] Cohen's Kappa
In-Reply-To: <46029F5B.5050206@web.de>
References: <46029F5B.5050206@web.de>
Message-ID: <4602EC0A.30207@hotmail.com>


Cohen, J. (1960). A coefficient of agreement for nominal scales. 
Educational and Psychological Measurement, 20, 37-46.

Cohen, J. (1968). Weighted kappa: Nominal scale agreement with provision 
for scaled disagreement or partial credit. Psychological Bulletin, 70, 
213-220.


Francisco

Dr. Francisco J. Zagmutt
College of Veterinary Medicine and Biomedical Sciences
Colorado State University


Christian Schulz wrote:
> Hi,
> 
> im little bit confused about Cohen's Kappa and i should  be look into the
> Kappa function code. Is the easy formula really wrong?
> 
> kappa=agreement-chance/(1-chance)
> 
> many thanks
> christian
> 
> ###############################################################################
> true-negativ:7445
> false-positive:3410
> false-negativ:347
> true-positiv:772
> 
> classification-aggrement:68,6%
> kappa=agreement-chance/(1-chance) = (0.686-0.5)/0.5=0.372
> 
> ......with function from library(vcd)
> Kappa(matrix(c(7445,3410,347,772),nrow=2))
>                value         ASE
> Unweighted 0.1686882 0.011235188
> Weighted   0.1686882 0.007979293
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From andy_liaw at merck.com  Thu Mar 22 22:26:32 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 22 Mar 2007 17:26:32 -0400
Subject: [R] how to get "lsmeans"?
In-Reply-To: <40e66e0b0703221242w434d3a8el5e5f22ea3639101b@mail.gmail.com>
References: <Pine.LNX.4.64.0703211502050.24076@gannet.stats.ox.ac.uk>
	<20070322005910.VVAT1624.tomts40-srv.bellnexxia.net@JohnDesktop8300>
	<D028EEB4CA113D4EAFDD485CCC998277731795@UTKFSVS4.utk.tennessee.edu>
	<40e66e0b0703221242w434d3a8el5e5f22ea3639101b@mail.gmail.com>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03E659F6@usctmx1106.merck.com>

From: Douglas Bates
> 
> On 3/22/07, Muenchen, Robert A (Bob) <muenchen at utk.edu> wrote:
> 
> > Perhaps I'm stating the obvious, but to increase the use of R in 
> > places where SAS & SPSS dominate, it's important to make 
> getting the 
> > same answers as easy as possible. That includes things like lsmeans 
> > and type III sums of squares. I've read lots of discussions here on 
> > sums of squares & I'm not advocating type III use, just 
> looking at it 
> > from a marketing perspective. Too many people look for 
> excuses to not change.
> > The fewer excuses, the better.
> 
> You may get strong reactions to such a suggestion.  I 
> recommend reading Bill Venables' famous unpublished paper 
> "Exegeses on linear models" (google for the title - very few 
> people use "Exegeses" and "linear models" in the same 
> sentence - in fact I would not be surprised if Bill was the 
> only one who has ever done so).

It's on the MASS page:
http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf
I believe it's based on a talk Bill gave at a S-PLUS User's Conference.
I think it deserves to be required reading for all graduate level linear
models course.
 
> You must realize that R is written by experts in statistics 
> and statistical computing who, despite popular opinion, do 
> not believe that everything in SAS and SPSS is worth copying. 
>  Some things done in such packages, which trace their roots 
> back to the days of punched cards and magnetic tape when 
> fitting a single linear model may take several days because 
> your first 5 attempts failed due to syntax errors in the JCL 
> or the SAS code, still reflect the approach of "give me every 
> possible statistic that could be calculated from this model, 
> whether or not it makes sense".  The approach taken in R is different.
>  The underlying assumption is that the useR is thinking about 
> the analysis while doing it.
> 
> The fact that it is so difficult to explain what lsmeans are 
> and why they would be of interest is an indication of why 
> they aren't implemented in any of the required packages.

Perhaps I should have made it clear in my original post:  I gave the
example and code more to show what the mysterious "least squares means"
are (which John explained lucidly), than how to replicate what SAS (or
JMP) outputs.  I do not understand how people can feel comfortable
reporting things like lsmeans and p-values from type <insert your
favorite Roman numeral here> tests when they do not know how such things
arise or, at the very least, what they _really_ mean.  (Given how simple
lsmeans are computed, not knowing how to compute them is pretty much the
same as not knowing what they are.)  One of the dangers of wholesale
output as SAS or SPSS gives is for the user to simply pick an answer and
run with it, without understanding what that answer is, or if it
corresponds to the question of interest.

As to whether to weight the levels of the factors being held constant,
my suggestion to John would be to offer both choices (unweighted and
weighted by observed frequencies).  I can see why one would want to
weight by observed frequencies (if the data are sampled from a
population), but there are certainly situations (perhaps more often than
not in the cases I've encountered) that the observed frequencies do not
come close to approximating what they are in the population.  In such
cases the unweighted average would make more sense to me.

Cheers,
Andy

 
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch [mailto:r-help- 
> > > bounces at stat.math.ethz.ch] On Behalf Of John Fox
> > > Sent: Wednesday, March 21, 2007 8:59 PM
> > > To: 'Prof Brian Ripley'
> > > Cc: 'r-help'; 'Chuck Cleland'
> > > Subject: Re: [R] how to get "lsmeans"?
> > >
> > > Dear Brian et al.,
> > >
> > > My apologies for chiming in late: It's been a busy day.
> > >
> > > First some general comments on "least-squares means" and "effect 
> > > displays."
> > > The general idea behind the two is similar -- to examine fitted 
> > > values corresponding to a term in a model while holding 
> other terms 
> > > to
> > typical
> > > values -- but the implementation is not identical. There are also
> > other
> > > similar ideas floating around as well. My formulation is more 
> > > general in the sense that it applies to a wider variety 
> of models, 
> > > both linear and otherwise.
> > >
> > > "Least-squares means" (a horrible term, by the way: in a 
> 1980 paper 
> > > in the American Statistician, Searle, Speed, and Milliken 
> suggested 
> > > the more descriptive term "population marginal means") apply to 
> > > factors and combinations of factors; covariates are set to mean 
> > > values and the levels of other factors are averaged over, 
> in effect 
> > > applying equal weight to each level. (This is from 
> memory, so it's 
> > > possible that I'm not getting it quite right, but I 
> believe that I 
> > > am.) In my effect displays, each level of
> > a
> > > factor is weighted by its proportion in the data. In 
> models in which 
> > > least-squares means can be computed, they should differ from the 
> > > corresponding effect display by a constant (if there are 
> different 
> > > numbers of observations in the different levels of the 
> factors that 
> > > are held constant).
> > >
> > > The obstacle to computing either least-squares means or effect
> > displays
> > > in R
> > > via predict() is that predict() wants factors in the "new 
> data" to 
> > > be set to particular levels. The effect() function in the effects 
> > > package bypasses
> > > predict() and works directly with the model matrix, 
> averaging over 
> > > the columns that pertain to a factor (and reconstructing 
> > > interactions as necessary). As mentioned, this has the effect of 
> > > setting the factor to its proportional distribution in the data. 
> > > This approach also has the advantage of being invariant 
> with respect 
> > > to the choice of contrasts for a factor.
> > >
> > > The only convenient way that I can think of to implement 
> > > least-squares means in R would be to use deviation-coded 
> regressors 
> > > for a factor (that is,
> > > contr.sum) and then to set the columns of the model matrix for the
> > > factor(s)
> > > to be averaged over to 0. It may just be that I'm having 
> a failure 
> > > of imagination and that there's a better way to proceed. I've not 
> > > implemented this solution because it is dependent upon 
> the choice of 
> > > contrasts and because I don't see a general advantage to it, but 
> > > since the issue has come up several times now, maybe I 
> should take a 
> > > crack at it. Remember that I want this to work more 
> generally, not 
> > > just for levels of factors, and not just for linear models.
> > >
> > > Brian is quite right in mentioning that he suggested some time ago
> > that
> > > I
> > > use critical values of t rather than of the standard normal 
> > > distribution for producing confidence intervals, and I 
> agree that it 
> > > makes sense to do so in models in which the dispersion is 
> estimated. 
> > > My only excuse for not
> > yet
> > > doing this is that I want to undertake a more general revision of 
> > > the effects package, and haven't had time to do it. There are 
> > > several changes that I'd like to make to the package. For 
> example, I 
> > > have results for multinomial and proportional odds logit models 
> > > (described in a paper
> > by
> > > me
> > > and Bob Andersen in the 2006 issue of Sociological 
> Methodology) that 
> > > I want to incorporate, and I'd like to improve the 
> appearance of the 
> > > default graphs. But Brian's suggestion is very 
> straightforward, and 
> > > I guess that I shouldn't wait to implement it; I'll do so 
> very soon.
> > >
> > > Regards,
> > >  John
> > >
> > > --------------------------------
> > > John Fox
> > > Department of Sociology
> > > McMaster University
> > > Hamilton, Ontario
> > > Canada L8S 4M4
> > > 905-525-9140x23604
> > > http://socserv.mcmaster.ca/jfox
> > > --------------------------------
> > >
> > > > -----Original Message-----
> > > > From: r-help-bounces at stat.math.ethz.ch 
> > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Prof Brian 
> > > > Ripley
> > > > Sent: Wednesday, March 21, 2007 12:03 PM
> > > > To: Chuck Cleland
> > > > Cc: r-help
> > > > Subject: Re: [R] how to get "lsmeans"?
> > > >
> > > > On Wed, 21 Mar 2007, Chuck Cleland wrote:
> > > >
> > > > > Liaw, Andy wrote:
> > > > >> I verified the result from the following with output 
> from JMP 6
> > on
> > > > >> the same data (don't have SAS: don't need it):
> > > > >>
> > > > >> set.seed(631)
> > > > >> n <- 100
> > > > >> dat <- data.frame(y=rnorm(n), A=factor(sample(1:2, n,
> > > > replace=TRUE)),
> > > > >>                   B=factor(sample(1:2, n, replace=TRUE)),
> > > > >>                   C=factor(sample(1:2, n, replace=TRUE)),
> > > > >>                   d=rnorm(n))
> > > > >> fm <- lm(y ~ A + B + C + d, dat) ## Form a data 
> frame of points 
> > > > >> to predict: all
> > > > combinations of the ##
> > > > >> three factors and the mean of the covariate.
> > > > >> p <- data.frame(expand.grid(A=1:2, B=1:2, C=1:2)) p[] <-
> > lapply(p,
> > > > >> factor) p <- cbind(p, d=mean(dat$d)) p <-
> > > > cbind(yhat=predict(fm, p),
> > > > >> p) ## lsmeans for the three factors:
> > > > >> with(p, tapply(yhat, A, mean))
> > > > >> with(p, tapply(yhat, B, mean))
> > > > >> with(p, tapply(yhat, C, mean))
> > > > >
> > > > >  Using Andy's example data, these are the LSMEANS and
> > > > intervals I get
> > > > > from SAS:
> > > > >
> > > > > A        y LSMEAN      95% Confidence Limits
> > > > > 1       -0.071847       -0.387507     0.243813
> > > > > 2       -0.029621       -0.342358     0.283117
> > > > >
> > > > > B        y LSMEAN      95% Confidence Limits
> > > > > 1       -0.104859       -0.397935     0.188216
> > > > > 2        0.003391       -0.333476     0.340258
> > > > >
> > > > > C        y LSMEAN      95% Confidence Limits
> > > > > 1       -0.084679       -0.392343     0.222986
> > > > > 2       -0.016789       -0.336374     0.302795
> > > > >
> > > > >  One way of reproducing the LSMEANS and intervals 
> from SAS using
> > > > > predict() seems to be the following:
> > > > >
> > > > >> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d,
> > > > data = dat)
> > > > >> newdat <- 
> > > > >> expand.grid(A=factor(c(1,2)),B=1.5,C=1.5,d=mean(dat$d))
> > > > >> cbind(newdat, predict(dat.lm, newdat, interval="confidence"))
> > > > >  A   B   C          d         fit        lwr       upr
> > > > > 1 1 1.5 1.5 0.09838595 -0.07184709 -0.3875070 0.2438128
> > > > > 2 2 1.5 1.5 0.09838595 -0.02962086 -0.3423582 0.2831165
> > > > >
> > > > >  However, another possibility seems to be:
> > > > >
> > > > >> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d,
> > > > data = dat)
> > > > >> newdat <-
> > > > >
> > > >
> > > 
> expand.grid(A=factor(c(1,2)),B=mean(as.numeric(dat$B)),C=mean(as.num
> > > er
> > > > > ic(dat$C)),d=mean(dat$d))
> > > > >> cbind(newdat, predict(dat.lm, newdat, interval="confidence"))
> > > > >  A    B    C          d         fit        lwr       upr
> > > > > 1 1 1.43 1.48 0.09838595 -0.08078243 -0.3964661 0.2349012
> > > > > 2 2 1.43 1.48 0.09838595 -0.03855619 -0.3479589 0.2708465
> > > > >
> > > > >  The predictions directly above match what effect() in the 
> > > > > effects package by John Fox returns:
> > > > >
> > > > > library(effects)
> > > > >
> > > > >> effect("A", fm, xlevels=list(d = mean(dat$D)))
> > > > >
> > > > > A effect
> > > > > A
> > > > >          1           2
> > > > > -0.08078243 -0.03855619
> > > > >
> > > > >  But for some reason the predict() and effect() intervals
> > > > are a little
> > > > > different:
> > > > >
> > > > >> effect("A", fm, xlevels=list(d = mean(dat$D)))$lower
> > > > >          [,1]
> > > > > 101 -0.3924451
> > > > > 102 -0.3440179
> > > > >
> > > > >> effect("A", fm, xlevels=list(d = mean(dat$D)))$upper
> > > > >         [,1]
> > > > > 101 0.2308802
> > > > > 102 0.2669055
> > > > >
> > > > >  I would be interested in any comments on these different
> > > > approaches
> > > > > and on the difference in intervals returned by predict()
> > > > and effect().
> > > >
> > > > AFAIR, the effects packages uses normal-based 
> confidence intervals 
> > > > and predict.lm uses t-based ones, and I have suggested 
> to John Fox 
> > > > that t-based intervals would be preferable, at least as 
> an option.
> > > >
> > > >
> > > > --
> > > > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > > > Professor of Applied Statistics,  
> http://www.stats.ox.ac.uk/~ripley/
> > > > University of Oxford,             Tel:  +44 1865 272861 (self)
> > > > 1 South Parks Road,                     +44 1865 272866 (PA)
> > > > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list 
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, 
> reproducible code.
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide 
> http://www.R-project.org/posting- 
> > > guide.html and provide commented, minimal, self-contained, 
> > > reproducible code.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From jenny at stat.ubc.ca  Thu Mar 22 23:08:09 2007
From: jenny at stat.ubc.ca (Jenny Bryan)
Date: Thu, 22 Mar 2007 15:08:09 -0700
Subject: [R] unexpected behavior of trellis calls inside a user-defined
	function
Message-ID: <53033CE2-CC66-451C-B279-43C500228D66@stat.ubc.ca>

I am making a battery of levelplots and wireframes for several fitted  
models.  I wrote a function that takes the fitted model object as the  
sole argument and produces these plots.  Various strange behavior  
ensued, but I have identified one very concrete issue (illustrated  
below): when my figure-drawing function includes the addition of  
points/lines to trellis plots, some of the information (main title,  
placement of additional points) from my i-th function call is leaking  
over into the i+1-th call.  In the example below, I just get  
unexpected results.  In my actual application, it breaks the whole  
function and one of the error messages I've gotten is:

 > figFun(smoothFit1) ## no longer worked, once I got fancy with trellis
Error in depth(path) : no applicable method for "depth"

Smallest example I could construct to illustrate at least one of my  
problems:

predVals <-
   expand.grid(list(Sepal.Length = seq(from = min(iris$Sepal.Length),
                      to = max(iris$Sepal.Length), length = 50),
                    Petal.Length = seq(from =  min(iris$Petal.Length),
                      to = max(iris$Petal.Length), length = 50)))
irisFit <- lm(Sepal.Width ~ Sepal.Length * Petal.Length, data = iris)
predSurf <- data.frame(predVals, Sepal.Width = predict(irisFit,  
predVals))
trellis.device("X11",width = 8, height = 8)

levelplot(Sepal.Width ~ Sepal.Length * Petal.Length,
           predSurf, main = "Produced at command line, Take 1")

# put levelplot call inside a function
myFunction <- function(surf) {
   levelplot(Sepal.Width ~ Sepal.Length * Petal.Length,surf,
             main = "Produced by function, Take 1")
}
myFunction(predSurf)
# OK, get the expected figure result

## now .. what if we add points to the plot?
levelplot(Sepal.Width ~ Sepal.Length * Petal.Length,
           predSurf, main = "Produced at command line, Take 2")
trellis.focus("panel", 1, 1)     # address the correct part of the  
figure
lpoints(6,4,pch = 19, cex = 2)   # a point appears in correct location
# I get what I expect

## any crosstalk from adjacent command line invocations?  no
levelplot(Sepal.Width ~ Sepal.Length * Petal.Length,
           predSurf, main = "Produced at command line, Take 3")
trellis.focus("panel", 1, 1)     # address the correct part of the  
figure
lpoints(5,2,pch = 19, cex = 2)   # a point appears in correct location
# I get what I expect

# put all this inside a function
myFunction <- function(surf) {
   levelplot(Sepal.Width ~ Sepal.Length * Petal.Length,surf,
main = "Produced by function, Take 2")
trellis.focus("panel", 1, 1)     # address the correct part of the
lpoints(7,5,pch = 19, cex = 2)   # a point appears in correct location
}
myFunction(predSurf)
# the title still says "Produced at command line, Take 3"
# and points appear at (5,2) AND (7,5) ... why?


From mark at wardle.org  Thu Mar 22 23:18:29 2007
From: mark at wardle.org (Mark Wardle)
Date: Thu, 22 Mar 2007 22:18:29 +0000
Subject: [R] personalize regression printing?
In-Reply-To: <50d1c22d0703221310v59b7dc63n999fc9d01c73771c@mail.gmail.com>
References: <50d1c22d0703221310v59b7dc63n999fc9d01c73771c@mail.gmail.com>
Message-ID: <460300B5.6090004@wardle.org>

ivo welch wrote:
> dear R experts:
> 
> I am often struggling with a desire of wanting to change the basic
> output that R prints.
> 
> For example, a year ago, I wanted to add the mean to the summary()
> statement, and eventually got help from friendly souls who showed me
> how to copy the summary() routine and then modify it.
> ...

If "m1" is the result from lm(...)
Have you seen str(m1), and str(summary(m1))? One can extract any results
and present them in any fashion in a custom function.

Is there a specific reason you wish to override the summary() function:
why can't you just create your own custom summary function (my.summary()?

Best wishes,

Mark
--
Dr. Mark Wardle
Specialist registrar, Neurology
Cardiff, UK


From deepayan.sarkar at gmail.com  Thu Mar 22 23:20:08 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 22 Mar 2007 15:20:08 -0700
Subject: [R] Fwd: unexpected behavior of trellis calls inside a user-defined
	function
In-Reply-To: <eb555e660703221519t592f7d43te50cbeaf588eed11@mail.gmail.com>
References: <53033CE2-CC66-451C-B279-43C500228D66@stat.ubc.ca>
	<eb555e660703221519t592f7d43te50cbeaf588eed11@mail.gmail.com>
Message-ID: <eb555e660703221520o4a63c513h3757b6c9a7e26c2f@mail.gmail.com>

Forgot to CC.

---------- Forwarded message ----------
From: Deepayan Sarkar <deepayan.sarkar at gmail.com>
Date: Mar 22, 2007 3:19 PM
Subject: Re: [R] unexpected behavior of trellis calls inside a
user-defined function
To: Jenny Bryan <jenny at stat.ubc.ca>


Hi,

I see lots of calls to trellis.focus(), but none to trellis.unfocus().
trellis.focus() sets the ``viewport'' to a particular panel. Unless
you go back to the root viewport, you will get stuck in a weird place
in the viewport tree. This is not a problem if your next plot is on a
new page, but will be otherwise.

So short anwser: call trellis.unfocus() when you are done doing
whatever you need to do after a trellis.focus() call.

Deepayan

On 3/22/07, Jenny Bryan <jenny at stat.ubc.ca> wrote:
> I am making a battery of levelplots and wireframes for several fitted
> models.  I wrote a function that takes the fitted model object as the
> sole argument and produces these plots.  Various strange behavior
> ensued, but I have identified one very concrete issue (illustrated
> below): when my figure-drawing function includes the addition of
> points/lines to trellis plots, some of the information (main title,
> placement of additional points) from my i-th function call is leaking
> over into the i+1-th call.  In the example below, I just get
> unexpected results.  In my actual application, it breaks the whole
> function and one of the error messages I've gotten is:
>
>  > figFun(smoothFit1) ## no longer worked, once I got fancy with trellis
> Error in depth(path) : no applicable method for "depth"
>
> Smallest example I could construct to illustrate at least one of my
> problems:
>
> predVals <-
>    expand.grid(list(Sepal.Length = seq(from = min(iris$Sepal.Length),
>                       to = max(iris$Sepal.Length), length = 50),
>                     Petal.Length = seq(from =  min(iris$Petal.Length),
>                       to = max(iris$Petal.Length), length = 50)))
> irisFit <- lm(Sepal.Width ~ Sepal.Length * Petal.Length, data = iris)
> predSurf <- data.frame(predVals, Sepal.Width = predict(irisFit,
> predVals))
> trellis.device("X11",width = 8, height = 8)
>
> levelplot(Sepal.Width ~ Sepal.Length * Petal.Length,
>            predSurf, main = "Produced at command line, Take 1")
>
> # put levelplot call inside a function
> myFunction <- function(surf) {
>    levelplot(Sepal.Width ~ Sepal.Length * Petal.Length,surf,
>              main = "Produced by function, Take 1")
> }
> myFunction(predSurf)
> # OK, get the expected figure result
>
> ## now .. what if we add points to the plot?
> levelplot(Sepal.Width ~ Sepal.Length * Petal.Length,
>            predSurf, main = "Produced at command line, Take 2")
> trellis.focus("panel", 1, 1)     # address the correct part of the
> figure
> lpoints(6,4,pch = 19, cex = 2)   # a point appears in correct location
> # I get what I expect
>
> ## any crosstalk from adjacent command line invocations?  no
> levelplot(Sepal.Width ~ Sepal.Length * Petal.Length,
>            predSurf, main = "Produced at command line, Take 3")
> trellis.focus("panel", 1, 1)     # address the correct part of the
> figure
> lpoints(5,2,pch = 19, cex = 2)   # a point appears in correct location
> # I get what I expect
>
> # put all this inside a function
> myFunction <- function(surf) {
>    levelplot(Sepal.Width ~ Sepal.Length * Petal.Length,surf,
> main = "Produced by function, Take 2")
> trellis.focus("panel", 1, 1)     # address the correct part of the
> lpoints(7,5,pch = 19, cex = 2)   # a point appears in correct location
> }
> myFunction(predSurf)
> # the title still says "Produced at command line, Take 3"
> # and points appear at (5,2) AND (7,5) ... why?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From e.catchpole at adfa.edu.au  Thu Mar 22 23:25:09 2007
From: e.catchpole at adfa.edu.au (ecatchpole)
Date: Fri, 23 Mar 2007 09:25:09 +1100
Subject: [R] non-linear curve fitting
In-Reply-To: <40e66e0b0703221216q76d99503g37e5cf036ed04ba@mail.gmail.com>
References: <46027E46.8090701@sciviews.org>
	<832A948B92E5754D9062CCF62F9ED892655FE6@xmail01.ad.ua.ac.be>
	<40e66e0b0703221216q76d99503g37e5cf036ed04ba@mail.gmail.com>
Message-ID: <46030245.3030409@adfa.edu.au>

Douglas Bates wrote on 03/23/2007 06:16 AM:
> On 3/22/07, Hufkens Koen <koen.hufkens at ua.ac.be> wrote:
>   
>> Is there a means of getting an F-statistic (p-value) out of all of this.
>>     
>
>   
>> Because least-square criterion / r-square only tell me how good the fit is and not necessarily how solid this fit is. An F-statistic (p-value) would be nice...
>>     
>
> What would the F-statistic be?  For a linear model with an intercept
> the F-statistic represents a comparison of the model that you have fit
> to the trivial model (intercept only).  It is important that the
> models being compared are nested - otherwise the F statistic is of
> questionable validity.
>
> For the logistic growth model you described (which is not quite the
> one fit by SSlogis - that model has one more parameter, a scale factor
> on the response) the response always goes to zero as x -> -\Infty and
> to one as x -> \Infty.  The trivial model is not nested within this
> model for finite parameter values so I'm not sure what hypotheses
> would be tested by an F-statistic.
>   

If the original curve is reparameterised as f(x) = 1/(1+exp(-(a+b*x))), 
then you can test whether b=0.  Is this any help?

Ted.
>   
>> Regards,
>> Koen
>>
>>
>>     
>>> -----Original Message-----
>>> From: Philippe Grosjean [mailto:phgrosjean at sciviews.org]
>>> Sent: donderdag 22 maart 2007 14:02
>>> To: Hufkens Koen; r-help at stat.math.ethz.ch
>>> Subject: Re: [R] non-linear curve fitting
>>>
>>> Hello,
>>>
>>> If a least-square criterion is fine for you, you should use
>>> nls(). For the logistic curve, you have a convenient
>>> self-starting model available:
>>> SSlogis(). Look at:
>>>
>>> ?nls
>>> ?SSlogis
>>>
>>> Best,
>>>
>>> Philippe Grosjean
>>>
>>> ..............................................<?}))><........
>>>   ) ) ) ) )
>>> ( ( ( ( (    Prof. Philippe Grosjean
>>>   ) ) ) ) )
>>> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>>>   ) ) ) ) )   Mons-Hainaut University, Belgium
>>> ( ( ( ( (
>>> ..............................................................
>>>
>>> Hufkens Koen wrote:
>>>       
>>>> Hi list,
>>>>
>>>> I have a little curve fitting problem.
>>>>
>>>> I would like to fit a sigmoid curve to my data using the
>>>>         
>>> following equation:
>>>       
>>>> f(x) = 1/(1 + exp(-(x-c)*b)) (or any other form for that matter)
>>>>
>>>> Where x is the distance/location within the dataframe, c is
>>>>         
>>> the shift of the curve across the dataframe and b is the
>>> steepness of the curve.
>>>       
>>>> I've been playing with glm() and glm.fit() but without any luck.
>>>>
>>>> for example the most simple example
>>>>
>>>> x = -10:10
>>>> y = 1/(1 + exp(-x))
>>>> glm(y ~ x, family=binomial(link="logit"))
>>>>
>>>> I get a warning:
>>>> non-integer #successes in a binomial glm! in: eval(expr, envir,
>>>> enclos)
>>>>
>>>> and some erratic results
>>>>
>>>> This is the most simple test to see if I could fit a curve
>>>>         
>>> to this perfect data so since this didn't work out, bringing
>>> in the extra parameters is a whole other ballgame so could
>>> someone give me a clue?
>>>       
>>>> Kind regards,
>>>> Koen
>>>>
>>>>
>>>>         
>


-- 
 Dr E.A. Catchpole  
 Visiting Fellow
 Univ of New South Wales at ADFA, Canberra, Australia
    _	  and University of Kent, Canterbury, England
   'v'	  - www.pems.adfa.edu.au/~ecatchpole          
  /   \	  - fax: +61 2 6268 8786		   
   m m    - ph:  +61 2 6268 8895


From KPappu at mednet.ucla.edu  Thu Mar 22 23:42:24 2007
From: KPappu at mednet.ucla.edu (Pappu, Kartik)
Date: Thu, 22 Mar 2007 15:42:24 -0700
Subject: [R] Colored boxes with values in the box
Message-ID: <f1eb1ddd20f938b1ca60c9e0e3e1c5cd@mednet.ucla.edu>

Hi all,

I have a x, y matrix of numbers (usually ranging from 0 to 40).  I need 
to group these numbers and assign a color to each group (for example 0 
to 15 - Blue, 16-30- Yellow, and 31-40- Red).  Then I need to draw a 
rectangular matrix which contains X x Y boxes and each box has the  
corresponding value from the input matrix and is also colored according 
to which group (i.e red, yellow, or blue) that value falls into.

I have used the color2D.matplot function from the plotrix package, but 
I cant quite figure out how to group the values to represent red blue 
and yellow colors.

Thanks

Kartik


----------------------------------------------------------
IMPORTANT WARNING:  This email (and any attachments) is only...{{dropped}}


From gunter.berton at gene.com  Fri Mar 23 00:08:54 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Thu, 22 Mar 2007 16:08:54 -0700
Subject: [R] Colored boxes with values in the box
In-Reply-To: <f1eb1ddd20f938b1ca60c9e0e3e1c5cd@mednet.ucla.edu>
Message-ID: <005a01c76cd7$0ff659c0$4d908980@gne.windows.gene.com>

Sounds like  ?image  what you are looking for, perhaps?

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pappu, Kartik
Sent: Thursday, March 22, 2007 3:42 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Colored boxes with values in the box

Hi all,

I have a x, y matrix of numbers (usually ranging from 0 to 40).  I need 
to group these numbers and assign a color to each group (for example 0 
to 15 - Blue, 16-30- Yellow, and 31-40- Red).  Then I need to draw a 
rectangular matrix which contains X x Y boxes and each box has the  
corresponding value from the input matrix and is also colored according 
to which group (i.e red, yellow, or blue) that value falls into.

I have used the color2D.matplot function from the plotrix package, but 
I cant quite figure out how to group the values to represent red blue 
and yellow colors.

Thanks

Kartik


----------------------------------------------------------
IMPORTANT WARNING:  This email (and any attachments) is only...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From sinclair.jesse at gmail.com  Fri Mar 23 00:13:38 2007
From: sinclair.jesse at gmail.com (Jesse Sinclair)
Date: Thu, 22 Mar 2007 16:13:38 -0700
Subject: [R] Mac OS X: Clearing R Console screen
Message-ID: <68DCD545-98E3-4B7A-9B6F-43410D86CD37@gmail.com>

Hi all,
I'm trying to figure out how to clear the console (screen) in the Mac  
OS X R console. Using ctrl-l does not work. I have also tried system 
("clear") but, I get  "TERM environment variable not set.", I find  
this a bit strange because system("ls") and other bash commands work  
fine. When using R in Terminal or iTerm, the system("clear") works  
great. Any ideas on how to get the R Console to clear, or how to set  
the TERM variable? I see others have had similar issues but, can't  
seem to find the solution. Thanks!

Jesse


From cberry at tajo.ucsd.edu  Fri Mar 23 00:28:01 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Thu, 22 Mar 2007 16:28:01 -0700
Subject: [R] personalize regression printing?
In-Reply-To: <50d1c22d0703221310v59b7dc63n999fc9d01c73771c@mail.gmail.com>
References: <50d1c22d0703221310v59b7dc63n999fc9d01c73771c@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0703221616440.20793@tajo.ucsd.edu>

On Thu, 22 Mar 2007, ivo welch wrote:

> dear R experts:
>
> I am often struggling with a desire of wanting to change the basic
> output that R prints.
>
> For example, a year ago, I wanted to add the mean to the summary()
> statement, and eventually got help from friendly souls who showed me
> how to copy the summary() routine and then modify it.
>
> now I would like to abbrev some of the output from summary(lm()).  For
> example, I want to eliminate the "Residuals" output.  I also am not
> quite sure why "Call:" is followed by a new line rather than just with
> a continuation of the model itself.  I also wonder why the word
> "Coefficients" seems to consume a line without being particularly
> helpful.  I wonder if "Coef" could appear before "Estimate Std. Error"
> on the same line.  All these are changes that would allow me to see
> more information on the same page.
>

This is not what summary( lm(...) ) does.

It is what print( summary( lm(...) ) ) does, (albeit) often automatically.

So you need to look at stats:::print.summary.lm and provide your own 
version.


> I do know that I can copy the functions themselves, if I can find
> them, and replace them myself.  However, this means that future
> versions of R may make changes that I may miss completely.  Its a
> solution, yes.

You need to operate on the value returned by summary.lm. Changes that 
occur 'under the hood' won't affect you.

>
> However, my first question is---for summary(lm()), would it make sense
> to have more options that control the output?  At least to suppress
> the printing of the distribution of the residuals?
>

No. As suggested above, summary(lm(...)) prints nothing.

It should not be too hard to put together your own version of 
print.summary.lm and invoke it when you want your customized output.

> A long-term solution, which is easy to suggest for me given that I do
> not have to do any work to implement it, would be to allow some
> templates that specify how output should be formatted.  R would first
> load the system templates, and thereafter any templates that the user
> specifies (has overridden).  The R functions would then work according
> to the current template.    Talking is easy; Walking is hard, of
> course.  Just a suggestion...
>
> Regards,
>
> /iaw
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901


From johnson4 at babel.ling.upenn.edu  Fri Mar 23 01:23:10 2007
From: johnson4 at babel.ling.upenn.edu (Daniel Ezra Johnson)
Date: Thu, 22 Mar 2007 17:23:10 -0700
Subject: [R] lmer estimated scale
Message-ID: <7B52E770-5430-4012-AF5A-D8AA0EB1A867@ling.upenn.edu>

I have data consisting of several binary responses from a large  
number of subjects on seven similar items. I have been using lmer  
with (crossed) random effects for subject and item. These effects are  
almost always (in the case of subject, are always) significant  
additions to my model, testing this with anova. Including them also  
increases the Somers' Dxy value substantially.

Even without those reasons, I feel I'd have to include these random  
effects to account for the correlation between the seven items from  
every subject. Otherwise my fixed between-subject effects like race,  
gender, etc. will seem more significant than they should.

But how should I interpret the fact that without a Subject effect  
included, the "estimated scale" parameter is usually very close to 1,  
while when I include the Subject effect the scale parameter drops,  
usually to around 0.85?

Can I at least conclude something interesting from this? Is it the  
same as saying that the subject effect itself (meaning the 'observed'  
subject BLUPs) is underdispersed with respect to its theoretical  
normal distribution?

To summarize:

a <- lmer(Response~Fixed Effects+(1|Subject)+(1|Item),data,binomial)
b <- lmer(Response~Fixed Effects+(1|Item),data,binomial)

a has a much better fit by any measure, and estimated scale around 0.85.
b has a worse fit, and estimated scale around 1.

Obvious? Interesting? Worrisome?

Thanks,
Dan


From johnson4 at babel.ling.upenn.edu  Fri Mar 23 01:21:22 2007
From: johnson4 at babel.ling.upenn.edu (Daniel Ezra Johnson)
Date: Thu, 22 Mar 2007 17:21:22 -0700
Subject: [R] lmer estimated scale
Message-ID: <F205BA37-4BF9-4E74-9267-DD714B345C46@ling.upenn.edu>

I have data consisting of binary responses from a large number of  
subjects on seven similar items. I have been using lmer with  
(crossed) random effects for subject and item. These effects are  
almost always (in the case of subject, always) significant additions  
to the model, testing this with anova. Including them also increases  
the Somers' Dxy value substantially.

Even without those reasons, I feel I'd have to include these random  
effects to account for the correlation between the seven items from  
every subject. Otherwise the fixed between-subject effects like race,  
gender, etc. will seem more significant than they should.

But how should I interpret the fact that without a Subject effect  
included, the "estimated scale" parameter is usually very close to 1,  
while when I include the Subject effect the scale parameter drops,  
usually to around 0.85?

Can I at least conclude something interesting from this? Is it the  
same as saying that the subject effect itself (meaning the 'observed'  
subject BLUPs) is underdispersed with respect to its theoretical  
normal distribution?

To summarize:

a <- lmer(Response~Fixed Effects+(1|Subject)+(1|Item),data,binomial)
b <- lmer(Response~Fixed Effects+(1|Item),data,binomial)

a has a much better fit by any measure, and estimated scale around 0.85.
b has a worse fit, but estimated scale around 1.

Obvious? Interesting? Worrisome?

Thanks,
Dan


From mwkimpel at gmail.com  Fri Mar 23 03:40:07 2007
From: mwkimpel at gmail.com (Mark W Kimpel)
Date: Thu, 22 Mar 2007 22:40:07 -0400
Subject: [R] can't load just saved R object "ReadItem: unknown type 65"
Message-ID: <46033E07.4040905@gmail.com>

I have run into a problem loading a just saved R object using R-devel. I 
have been saving and loading this particular type of R object for a long 
while and never ran into this problem. I save, then immediately reload 
(to test save) and get "ReadItem: unnknown type 65".

This error is reproducible after logout from server and restart of emacs 
and R.

Below is my output and sessionInfo().

Thanks,
Mark

 > setwd("~/Genomics/Experiments.Genomic/BB01/acb.shell")
 > local(save(affy.object.preprocessed, file 
="affy.object.preprocessed.R" ))
 > load("affy.object.preprocessed.R")
Error in load("affy.object.preprocessed.R") :
	ReadItem: unknown type 65, perhaps written by later version of R
 > sessionInfo()
R version 2.5.0 Under development (unstable) (2007-03-11 r40824)
powerpc64-unknown-linux-gnu

locale:
LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C

attached base packages:
[1] "splines"   "stats"     "graphics"  "grDevices" "datasets"  "utils"
[7] "tools"     "methods"   "base"

other attached packages:
      multtest    rat2302cdf affycoretools       annaffy        xtable
      "1.13.1"      "1.15.0"       "1.7.8"       "1.7.3"       "1.4-3"
         gcrma   matchprobes       biomaRt         RCurl           XML
       "2.7.3"       "1.7.4"      "1.9.21"       "0.8-0"       "1.6-0"
       GOstats      Category        Matrix       lattice    genefilter
      "2.1.13"      "2.1.20"   "0.9975-11"     "0.14-16"      "1.13.8"
      survival          KEGG          RBGL      annotate            GO
        "2.31"     "1.15.12"      "1.11.4"      "1.13.6"     "1.15.12"
         graph         limma          affy        affyio       Biobase
      "1.13.6"      "2.9.13"     "1.13.14"       "1.3.3"     "1.13.39"
-- 
Mark W. Kimpel MD
Neuroinformatics
Department of Psychiatry
Indiana University School of Medicine


From mwkimpel at gmail.com  Fri Mar 23 03:51:28 2007
From: mwkimpel at gmail.com (Mark W Kimpel)
Date: Thu, 22 Mar 2007 22:51:28 -0400
Subject: [R] can't load just saved R object using R-devel "ReadItem: unknown
 type 65"
Message-ID: <460340B0.6070906@gmail.com>

I am unable to load a just saved R object. Get message "ReadItem: 
unknown type 65" I am using R-devel and the problem is reproducible 
after logout and restarting R.

Output and sessionInfo() below.

Mark

 > setwd("~/Genomics/Experiments.Genomic/BB01/acb.shell")
 > local(save(affy.object.preprocessed, file 
="affy.object.preprocessed.R" ))
 > load("affy.object.preprocessed.R")
Error in load("affy.object.preprocessed.R") :
	ReadItem: unknown type 65, perhaps written by later version of R
 > sessionInfo()
R version 2.5.0 Under development (unstable) (2007-03-11 r40824)
powerpc64-unknown-linux-gnu

locale:
LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C

attached base packages:
[1] "splines"   "stats"     "graphics"  "grDevices" "datasets"  "utils"
[7] "tools"     "methods"   "base"

other attached packages:
      multtest    rat2302cdf affycoretools       annaffy        xtable
      "1.13.1"      "1.15.0"       "1.7.8"       "1.7.3"       "1.4-3"
         gcrma   matchprobes       biomaRt         RCurl           XML
       "2.7.3"       "1.7.4"      "1.9.21"       "0.8-0"       "1.6-0"
       GOstats      Category        Matrix       lattice    genefilter
      "2.1.13"      "2.1.20"   "0.9975-11"     "0.14-16"      "1.13.8"
      survival          KEGG          RBGL      annotate            GO
        "2.31"     "1.15.12"      "1.11.4"      "1.13.6"     "1.15.12"
         graph         limma          affy        affyio       Biobase
      "1.13.6"      "2.9.13"     "1.13.14"       "1.3.3"     "1.13.39"
-- 
Mark W. Kimpel MD
Neuroinformatics
Department of Psychiatry
Indiana University School of Medicine


From jenny at stat.ubc.ca  Fri Mar 23 04:45:39 2007
From: jenny at stat.ubc.ca (Jenny Bryan)
Date: Thu, 22 Mar 2007 20:45:39 -0700
Subject: [R] unexpected behavior of trellis calls inside a user-defined
	function
In-Reply-To: <53033CE2-CC66-451C-B279-43C500228D66@stat.ubc.ca>
References: <53033CE2-CC66-451C-B279-43C500228D66@stat.ubc.ca>
Message-ID: <01EB3AA8-1096-48BE-B5D0-F6931938E13E@stat.ubc.ca>

I got 2 helpful responses (from Deepayan, cc'ed to the list, and  
Sundar Dorai-Rah, directly to me).  Sundar pointed out something from  
the FAQ:  inside a function, one must explicitly print lattice/ 
trellis graph object.  It turns out that, at least in the small  
example I gave, Sundar's tip was the one that fixed it.  The  
combination of both fixes got my original, more complicated function  
working.  Thanks, Jenny

* follow trellis.focus() calls with trellis.unfocus()
* use print(levelplot(...)) inside a function

On 22-Mar-07, at 3:08 PM, Jenny Bryan wrote:

> I am making a battery of levelplots and wireframes for several  
> fitted models.  I wrote a function that takes the fitted model  
> object as the sole argument and produces these plots.  Various  
> strange behavior ensued, but I have identified one very concrete  
> issue (illustrated below): when my figure-drawing function includes  
> the addition of points/lines to trellis plots, some of the  
> information (main title, placement of additional points) from my i- 
> th function call is leaking over into the i+1-th call.  In the  
> example below, I just get unexpected results.  In my actual  
> application, it breaks the whole function and one of the error  
> messages I've gotten is:
>
> > figFun(smoothFit1) ## no longer worked, once I got fancy with  
> trellis
> Error in depth(path) : no applicable method for "depth"
>
> Smallest example I could construct to illustrate at least one of my  
> problems:
>
> predVals <-
>   expand.grid(list(Sepal.Length = seq(from = min(iris$Sepal.Length),
>                      to = max(iris$Sepal.Length), length = 50),
>                    Petal.Length = seq(from =  min(iris$Petal.Length),
>                      to = max(iris$Petal.Length), length = 50)))
> irisFit <- lm(Sepal.Width ~ Sepal.Length * Petal.Length, data = iris)
> predSurf <- data.frame(predVals, Sepal.Width = predict(irisFit,  
> predVals))
> trellis.device("X11",width = 8, height = 8)
>
> levelplot(Sepal.Width ~ Sepal.Length * Petal.Length,
>           predSurf, main = "Produced at command line, Take 1")
>
> # put levelplot call inside a function
> myFunction <- function(surf) {
>   levelplot(Sepal.Width ~ Sepal.Length * Petal.Length,surf,
>             main = "Produced by function, Take 1")
> }
> myFunction(predSurf)
> # OK, get the expected figure result
>
> ## now .. what if we add points to the plot?
> levelplot(Sepal.Width ~ Sepal.Length * Petal.Length,
>           predSurf, main = "Produced at command line, Take 2")
> trellis.focus("panel", 1, 1)     # address the correct part of the  
> figure
> lpoints(6,4,pch = 19, cex = 2)   # a point appears in correct location
> # I get what I expect
>
> ## any crosstalk from adjacent command line invocations?  no
> levelplot(Sepal.Width ~ Sepal.Length * Petal.Length,
>           predSurf, main = "Produced at command line, Take 3")
> trellis.focus("panel", 1, 1)     # address the correct part of the  
> figure
> lpoints(5,2,pch = 19, cex = 2)   # a point appears in correct location
> # I get what I expect
>
> # put all this inside a function
> myFunction <- function(surf) {
>   levelplot(Sepal.Width ~ Sepal.Length * Petal.Length,surf,
> main = "Produced by function, Take 2")
> trellis.focus("panel", 1, 1)     # address the correct part of the
> lpoints(7,5,pch = 19, cex = 2)   # a point appears in correct location
> }
> myFunction(predSurf)
> # the title still says "Produced at command line, Take 3"
> # and points appear at (5,2) AND (7,5) ... why?
>
>
>


From plynchnlm at gmail.com  Fri Mar 23 05:13:25 2007
From: plynchnlm at gmail.com (Paul Lynch)
Date: Fri, 23 Mar 2007 00:13:25 -0400
Subject: [R] Fitting a line to a qqplot's points?
Message-ID: <50d6c72a0703222113sa49f1b3w9643673f189d9eb4@mail.gmail.com>

I've made some normal plots of my data using qqplot, and now
I would like to fit a line to the points on the plot and
check the correlation coefficient to have a more objective measure
of how straight the line is.  Is there a simple way of doing that?
(I'm still pretty new to R.)

Thanks,
   --Paul

-- 
Paul Lynch
Aquilent, Inc.
National Library of Medicine (Contractor)


From ligges at statistik.uni-dortmund.de  Fri Mar 23 08:33:38 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 23 Mar 2007 08:33:38 +0100
Subject: [R] Fitting a line to a qqplot's points?
In-Reply-To: <50d6c72a0703222113sa49f1b3w9643673f189d9eb4@mail.gmail.com>
References: <50d6c72a0703222113sa49f1b3w9643673f189d9eb4@mail.gmail.com>
Message-ID: <460382D2.4090102@statistik.uni-dortmund.de>



Paul Lynch wrote:
> I've made some normal plots of my data using qqplot, and now
> I would like to fit a line to the points on the plot and
> check the correlation coefficient to have a more objective measure
> of how straight the line is.  Is there a simple way of doing that?
> (I'm still pretty new to R.)


See ?qqplot and what its value is:

  x <- rnorm(10)
  y <- rnorm(10)
  qq <- qqplot(x, y)
  cor(qq$x, qq$y)

Uwe Ligges



> Thanks,
>    --Paul
>


From brown_emu at yahoo.com  Fri Mar 23 08:33:47 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Fri, 23 Mar 2007 00:33:47 -0700 (PDT)
Subject: [R] Truncated x-axis values
In-Reply-To: <1266A6320AD3884EA8142B354DFBA31A06EB0496@dfre02.ent.ti.com>
Message-ID: <429269.51439.qm@web39715.mail.mud.yahoo.com>

You can try playing around with oma, omi, mai, mar, etc. in par():

myMai <- par("mai")
myMai[1] <- max(nchar(y))*par("cin")[1]
par(mfrow=c(2, 1),mai=myMai,oma=rep(0,4),las = 2)
# your plot

--- "Urban, Alexander" <a-urban at ti.com> wrote:

> John
> Thanks for your reply and sorry for not beein specific enough
> Try this piece of code (it's an abstraction of my application...)
> 
> par(las = 2)
> par(mfrow=c(2, 1))
> x =  c(1,2,3,4,5,6,7,8)
> y = c("abcdccefghijk", "bcssdefghijkl", "abddessfghijk",
> "bddessfghijkl","abcdssedghijk","bcdedghijkl","assbcdefghidk","bcdedssgh
> ijkl")
> 
> boxplot(x~y)
> boxplot(x~y) 
> 
> In the lower chart the labels are truncated...(not totally visible)
> Is this clearer now?
> 
> Thanks
> Alex
> 
> -----Original Message-----
> From: John Kane [mailto:jrkrideau at yahoo.ca] 
> Sent: Thursday, March 22, 2007 19:19
> To: Urban, Alexander; r-help at stat.math.ethz.ch
> Subject: Re: [R] Truncated x-axis values
> 
> You really have not told us much about what you're actually doing.  A
> simple self-contained example of what you're trying to do might let
> someone help. 
> 
> Have you read the posting guide?
> 
> --- "Urban, Alexander" <a-urban at ti.com> wrote:
> 
> > Hello
> > 
> > I'm new to this group. I looked up the last two hour in the help file 
> > and in the archives of this group, but didn't find anything.
> > I hope my question is not too dump:
> > I'm printing a graph with vertical labels on the x-axis (necessary due
> 
> > to many labels). Unfortunately the png truncates the labels halfway 
> > through, so that you can only read the last 7 digits of the label.
> > Snice I'm already asking :-): Is there a possibility to tell R: If 
> > there are so many labels that you write them on top of each other, 
> > take only e.g. every 2nd...
> > 
> > Sorry for bothering and thanks
> > Alex
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> 
> 
> __________________________________________________
> Do You Yahoo!?

> http://mail.yahoo.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



 
____________________________________________________________________________________
Looking for earth-friendly autos? 
Browse Top Cars by "Green Rating" at Yahoo! Autos' Green Center.


From ligges at statistik.uni-dortmund.de  Fri Mar 23 08:35:14 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 23 Mar 2007 08:35:14 +0100
Subject: [R] Fitting a line to a qqplot's points?
In-Reply-To: <460382D2.4090102@statistik.uni-dortmund.de>
References: <50d6c72a0703222113sa49f1b3w9643673f189d9eb4@mail.gmail.com>
	<460382D2.4090102@statistik.uni-dortmund.de>
Message-ID: <46038332.9080908@statistik.uni-dortmund.de>



Uwe Ligges wrote:
> 
> 
> Paul Lynch wrote:
>> I've made some normal plots of my data using qqplot, and now
>> I would like to fit a line to the points on the plot and
>> check the correlation coefficient to have a more objective measure
>> of how straight the line is.  Is there a simple way of doing that?
>> (I'm still pretty new to R.)
> 
> 
> See ?qqplot and what its value is:
> 
>  x <- rnorm(10)
>  y <- rnorm(10)
>  qq <- qqplot(x, y)
>  cor(qq$x, qq$y)

I forgot to finish:

for fitting a line based in least squares, you can go on:

fit <- lm(y ~ x, data = qq)
summary(fit)
abline(fit)


Uwe Ligges


From justin_bem at yahoo.fr  Fri Mar 23 08:48:01 2007
From: justin_bem at yahoo.fr (justin bem)
Date: Fri, 23 Mar 2007 08:48:01 +0100 (CET)
Subject: [R] Im, Paseran and Shin test in R
Message-ID: <668797.83148.qm@web23011.mail.ird.yahoo.com>

Dear all,

Does R have a function to implement Im, Paseran and
Shin test in R ?

Justin BEM
El?ve Ing?nieur Statisticien Economiste
BP 294 Yaound?.
T?l (00237)9597295.


From Joao.Fadista at agrsci.dk  Fri Mar 23 08:51:09 2007
From: Joao.Fadista at agrsci.dk (=?iso-8859-1?Q?Jo=E3o_Fadista?=)
Date: Fri, 23 Mar 2007 08:51:09 +0100
Subject: [R] concatenate 2 data.frames
Message-ID: <EA09C4B2B0F16E44B8F3311629493C0D02ED4E99@DJFPOST01.djf.agrsci.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070323/cce030f6/attachment.pl 

From molshansky at chimaeracapital.com  Fri Mar 23 09:06:38 2007
From: molshansky at chimaeracapital.com (Moshe Olshansky)
Date: Fri, 23 Mar 2007 19:06:38 +1100
Subject: [R] Updating a worksheet in Excel file using RODBC
Message-ID: <B16F2EFF2E11604E93044813DD4413598A591D@warp8.chimaera.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070323/5e28ace4/attachment.pl 

From ripley at stats.ox.ac.uk  Fri Mar 23 09:34:58 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Mar 2007 08:34:58 +0000 (GMT)
Subject: [R] Updating a worksheet in Excel file using RODBC
In-Reply-To: <B16F2EFF2E11604E93044813DD4413598A591D@warp8.chimaera.local>
References: <B16F2EFF2E11604E93044813DD4413598A591D@warp8.chimaera.local>
Message-ID: <Pine.LNX.4.64.0703230826370.21572@gannet.stats.ox.ac.uk>

The problem is that way the ODBC driver exposes table names is not valid 
SQL, and nor is the way quoting has to be used.  You can get around this 
via direct SQL sent by sqlQuery.  In addition, by default the Excel ODBC 
driver gives you read-only access to worksheets.

Searching the list archives, would help, for example this answer:

https://stat.ethz.ch/pipermail/r-help/2007-March/127851.html

Making a wrapper interface in RODBC is on my TODO list, but not anywhere 
near the top.


On Fri, 23 Mar 2007, Moshe Olshansky wrote:

> Hello!
>
> I have no problem reading Excel files (each worksheet in the file is a "table" which can be read - at least in my case).
>
> What I would like to do is to read such a table, change it (just the contents, not the format) and write it back, and this I can not do.  I am getting the following error messages (3 slightly different attempts):
>
>> sqlSave(con, x, tablename = "Chimaera20_3years$", append = FALSE,
> +         rownames = FALSE, colnames = TRUE,
> +         verbose = TRUE, oldstyle = FALSE,safer=FALSE)
> Query: CREATE TABLE Chimaera20_3years$  (Date varchar(255), 000Tax varchar(255), 1500Tax varchar(255), 3000Tax varchar(255), 4650Tax varchar(255))
> Error in sqlSave(con, x, tablename = "Chimaera20_3years$", append = FALSE,  :
>        [RODBC] ERROR: Could not SQLExecDirect
> 37000 -3551 [Microsoft][ODBC Excel Driver] Syntax error in CREATE TABLE statement.
>
>> sqlSave(con, x, tablename = "[Chimaera20_3years$]", append = FALSE,
> +         rownames = FALSE, colnames = TRUE,
> +         verbose = TRUE, oldstyle = FALSE,safer=FALSE)
> Query: CREATE TABLE [Chimaera20_3years$]  (Date varchar(255), 000Tax varchar(255), 1500Tax varchar(255), 3000Tax varchar(255), 4650Tax varchar(255))
> Error in sqlSave(con, x, tablename = "[Chimaera20_3years$]", append = FALSE,  :
>        [RODBC] ERROR: Could not SQLExecDirect
> 37000 -3553 [Microsoft][ODBC Excel Driver] Syntax error in field definition.
>
>> sqlSave(con, x, tablename = "[Chimaera20_3years]", append = FALSE,
> +         rownames = FALSE, colnames = TRUE,
> +         verbose = TRUE, oldstyle = FALSE,safer=FALSE)
> Query: CREATE TABLE [Chimaera20_3years]  (Date varchar(255), 000Tax varchar(255), 1500Tax varchar(255), 3000Tax varchar(255), 4650Tax varchar(255))
> Error in sqlSave(con, x, tablename = "[Chimaera20_3years]", append = FALSE,  :
>        [RODBC] ERROR: Could not SQLExecDirect
> 37000 -3553 [Microsoft][ODBC Excel Driver] Syntax error in field definition.
>
> Am I doing it wrong way or is there a problem with the Excel driver?
>
> Thank you in advance,
>
> Moshe Olshansky
> Chimaera Capital Group
>
>
> Moshe Olshansky
>
> Chimaera Capital Limited
> Level 4 / 349 Collins Street
> Melbourne, Victoria 3000
> Phone: +613 8614 8400
> Fax: +613 8614 8410
> Email: molshansky at chimaeracapital.com
>
> Disclaimer: This message is intended only for the personal and confidential use of the designated recipient(s) named above. If you are not the intended recipient of this message you are hereby notified that any review, dissemination, distribution or copying of this message is strictly prohibited. This communication is for information purposes only and should not be regarded as an offer to sell or as a solicitation of an offer to buy any financial product, an official confirmation of any transaction, or as an official statement of Chimaera Capital Limited. E-mail transmissions cannot be guaranteed to be secure or error-free. Therefore, we do not represent that this information is complete or accurate and it should not be relied upon as such. All information is subject to change without notice.
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From sebastian.weber at physik.tu-darmstadt.de  Fri Mar 23 09:49:28 2007
From: sebastian.weber at physik.tu-darmstadt.de (Sebastian Weber)
Date: Fri, 23 Mar 2007 09:49:28 +0100
Subject: [R] landscape pdf
Message-ID: <1174639768.7522.2.camel@rock.kraft.de>

Hello together!

How can I plot a landscape letter-format plot? With postscript, I just
use the horizontal option and I get what I want, but it seems that the
pdf lacks this option. Well, I could do a ps2pdf conversion of the
generated ps-file. But is there a way to directly produce landscape
pdf-plots with R?

Thanks for any help.

Greetings,

Sebastian


From ligges at statistik.uni-dortmund.de  Fri Mar 23 09:54:15 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 23 Mar 2007 09:54:15 +0100
Subject: [R] landscape pdf
In-Reply-To: <1174639768.7522.2.camel@rock.kraft.de>
References: <1174639768.7522.2.camel@rock.kraft.de>
Message-ID: <460395B7.5050505@statistik.uni-dortmund.de>



Sebastian Weber wrote:
> Hello together!
> 
> How can I plot a landscape letter-format plot? With postscript, I just
> use the horizontal option and I get what I want, but it seems that the
> pdf lacks this option. Well, I could do a ps2pdf conversion of the
> generated ps-file. But is there a way to directly produce landscape
> pdf-plots with R?


By setting arguments width > height?

Uwe Ligges



> Thanks for any help.
> 
> Greetings,
> 
> Sebastian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Fri Mar 23 10:01:05 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Mar 2007 09:01:05 +0000 (GMT)
Subject: [R] landscape pdf
In-Reply-To: <1174639768.7522.2.camel@rock.kraft.de>
References: <1174639768.7522.2.camel@rock.kraft.de>
Message-ID: <Pine.LNX.4.64.0703230856370.21572@gannet.stats.ox.ac.uk>

PDF does not have orientation: just set the width > height in pdf() to get 
a 'landscape' plot.

The 'horizontal' option in postscript() was originally (in S/S-PLUS) 
designed for sending plots direct to a printer: pdf() does not have that 
option either.

On Fri, 23 Mar 2007, Sebastian Weber wrote:

> How can I plot a landscape letter-format plot? With postscript, I just
> use the horizontal option and I get what I want, but it seems that the
> pdf lacks this option. Well, I could do a ps2pdf conversion of the
> generated ps-file. But is there a way to directly produce landscape
> pdf-plots with R?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From koen.hufkens at ua.ac.be  Fri Mar 23 10:33:11 2007
From: koen.hufkens at ua.ac.be (Hufkens Koen)
Date: Fri, 23 Mar 2007 10:33:11 +0100
Subject: [R] non-linear curve fitting
In-Reply-To: <46030245.3030409@adfa.edu.au>
Message-ID: <832A948B92E5754D9062CCF62F9ED892656010@xmail01.ad.ua.ac.be>

I'll give you the equation of the reference I based my thinking upon.

this the link:  http://users.pandora.be/requested/images/equation.png

It's a sigmoid error function, but I thought I simplified things by picking a less complex form, although I think It won't matter that much.

Anyway, as Ted suggested, this equation has the terms reparameterised into the form -(bx+c) as well. In my reference they probably tested for the b=0 scenario because they had an F-statistic. But to be honest I wouldn't know how to start implementing this? It seems a little more complicated then the ordinary stuff I normally do.

Koen


> -----Original Message-----
> From: ecatchpole [mailto:e.catchpole at adfa.edu.au] 
> Sent: donderdag 22 maart 2007 23:25
> To: Douglas Bates
> Cc: Hufkens Koen; r-help at stat.math.ethz.ch; Philippe Grosjean
> Subject: Re: [R] non-linear curve fitting
> 
> Douglas Bates wrote on 03/23/2007 06:16 AM:
> > On 3/22/07, Hufkens Koen <koen.hufkens at ua.ac.be> wrote:
> >   
> >> Is there a means of getting an F-statistic (p-value) out 
> of all of this.
> >>     
> >
> >   
> >> Because least-square criterion / r-square only tell me how 
> good the fit is and not necessarily how solid this fit is. An 
> F-statistic (p-value) would be nice...
> >>     
> >
> > What would the F-statistic be?  For a linear model with an 
> intercept 
> > the F-statistic represents a comparison of the model that 
> you have fit 
> > to the trivial model (intercept only).  It is important that the 
> > models being compared are nested - otherwise the F statistic is of 
> > questionable validity.
> >
> > For the logistic growth model you described (which is not quite the 
> > one fit by SSlogis - that model has one more parameter, a 
> scale factor 
> > on the response) the response always goes to zero as x -> 
> -\Infty and 
> > to one as x -> \Infty.  The trivial model is not nested within this 
> > model for finite parameter values so I'm not sure what hypotheses 
> > would be tested by an F-statistic.
> >   
> 
> If the original curve is reparameterised as f(x) = 
> 1/(1+exp(-(a+b*x))), then you can test whether b=0.  Is this any help?
> 
> Ted.
> >   
> >> Regards,
> >> Koen
> >>
> >>
> >>     
> >>> -----Original Message-----
> >>> From: Philippe Grosjean [mailto:phgrosjean at sciviews.org]
> >>> Sent: donderdag 22 maart 2007 14:02
> >>> To: Hufkens Koen; r-help at stat.math.ethz.ch
> >>> Subject: Re: [R] non-linear curve fitting
> >>>
> >>> Hello,
> >>>
> >>> If a least-square criterion is fine for you, you should 
> use nls(). 
> >>> For the logistic curve, you have a convenient self-starting model 
> >>> available:
> >>> SSlogis(). Look at:
> >>>
> >>> ?nls
> >>> ?SSlogis
> >>>
> >>> Best,
> >>>
> >>> Philippe Grosjean
> >>>
> >>> ..............................................<?}))><........
> >>>   ) ) ) ) )
> >>> ( ( ( ( (    Prof. Philippe Grosjean
> >>>   ) ) ) ) )
> >>> ( ( ( ( (    Numerical Ecology of Aquatic Systems
> >>>   ) ) ) ) )   Mons-Hainaut University, Belgium
> >>> ( ( ( ( (
> >>> ..............................................................
> >>>
> >>> Hufkens Koen wrote:
> >>>       
> >>>> Hi list,
> >>>>
> >>>> I have a little curve fitting problem.
> >>>>
> >>>> I would like to fit a sigmoid curve to my data using the
> >>>>         
> >>> following equation:
> >>>       
> >>>> f(x) = 1/(1 + exp(-(x-c)*b)) (or any other form for that matter)
> >>>>
> >>>> Where x is the distance/location within the dataframe, c is
> >>>>         
> >>> the shift of the curve across the dataframe and b is the 
> steepness 
> >>> of the curve.
> >>>       
> >>>> I've been playing with glm() and glm.fit() but without any luck.
> >>>>
> >>>> for example the most simple example
> >>>>
> >>>> x = -10:10
> >>>> y = 1/(1 + exp(-x))
> >>>> glm(y ~ x, family=binomial(link="logit"))
> >>>>
> >>>> I get a warning:
> >>>> non-integer #successes in a binomial glm! in: eval(expr, envir,
> >>>> enclos)
> >>>>
> >>>> and some erratic results
> >>>>
> >>>> This is the most simple test to see if I could fit a curve
> >>>>         
> >>> to this perfect data so since this didn't work out, 
> bringing in the 
> >>> extra parameters is a whole other ballgame so could 
> someone give me 
> >>> a clue?
> >>>       
> >>>> Kind regards,
> >>>> Koen
> >>>>
> >>>>
> >>>>         
> >
> 
> 
> --
>  Dr E.A. Catchpole
>  Visiting Fellow
>  Univ of New South Wales at ADFA, Canberra, Australia
>     _	  and University of Kent, Canterbury, England
>    'v'	  - www.pems.adfa.edu.au/~ecatchpole          
>   /   \	  - fax: +61 2 6268 8786		   
>    m m    - ph:  +61 2 6268 8895             
> 
> 
> 
> 
> 
> 
> 
> --
> No virus found in this incoming message.
> Checked by AVG Free Edition.
> Version: 7.5.446 / Virus Database: 268.18.16/729 - Release 
> Date: 21/03/2007 7:52
>  
> 

--


From gavin.simpson at ucl.ac.uk  Fri Mar 23 10:41:34 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 23 Mar 2007 09:41:34 +0000
Subject: [R] concatenate 2 data.frames
In-Reply-To: <EA09C4B2B0F16E44B8F3311629493C0D02ED4E99@DJFPOST01.djf.agrsci.dk>
References: <EA09C4B2B0F16E44B8F3311629493C0D02ED4E99@DJFPOST01.djf.agrsci.dk>
Message-ID: <1174642894.3043.10.camel@dhcppc2.my.nat.localnet>

On Fri, 2007-03-23 at 08:51 +0100, Jo?o Fadista wrote:
> Dear all,
>  
> I would like to know how can I concatenate 2 data.frames into a single
> one. Both data frames have the same number of columns and the same
> class type in each correspondent column. So what I want is to have a
> new data.frame where I have first the values from one data.frame and
> then the values from a second data.frame would came after in this new
> data.frame.
>  
> Thanks in advance.

By "after", do you mean columns for dataframe1 then columns of
dataframe2, or do you mean you want to append dataframe2 onto the bottom
of dataframe1?

The first is:
dat1 <- data.frame(var1 = rnorm(10), var2 = rnorm(10), 
                   var3 = gl(2, 5, labels = c("red", "blue")))
dat2 <- data.frame(var4 = rnorm(10), var5 = rnorm(10), 
                   var6 = gl(2, 5, labels = c("red", "blue")))
combined <- data.frame(dat1, dat2)
combined
          var1        var2 var3        var4        var5 var6
1  -1.61397560 -0.40296928  red  1.48380888  1.35501273  red
2   1.01901681 -0.27616320  red -1.00234243 -0.79328309  red
3  -0.88272375 -0.42375566  red -1.31503261 -0.04570735  red
4   1.37368014 -0.63154987  red -1.40635604  1.50906371  red
5   0.66810230 -0.43453383  red  0.30449564 -0.24893343  red
6  -0.06403118 -1.59095216 blue  0.41945472  0.09143192 blue
7   0.02208197  1.70299530 blue -1.64188953 -0.30545702 blue
8  -1.13057000 -0.67610437 blue -1.15801044  1.17682587 blue
9  -2.32315433 -0.07500192 blue  0.03576081 -1.14670543 blue
10 -0.64734307  0.74789423 blue -0.57466841 -1.69753353 blue

You could also use cbind().

The second could be:
## need to  provide the same variables names for matching columns
names(dat2) <- c("var1", "var2", "var3")
rbind(dat1, dat2)

          var1        var2 var3
1  -1.61397560 -0.40296928  red
2   1.01901681 -0.27616320  red
3  -0.88272375 -0.42375566  red
4   1.37368014 -0.63154987  red
5   0.66810230 -0.43453383  red
6  -0.06403118 -1.59095216 blue
7   0.02208197  1.70299530 blue
8  -1.13057000 -0.67610437 blue
9  -2.32315433 -0.07500192 blue
10 -0.64734307  0.74789423 blue
11  1.48380888  1.35501273  red
12 -1.00234243 -0.79328309  red
13 -1.31503261 -0.04570735  red
14 -1.40635604  1.50906371  red
15  0.30449564 -0.24893343  red
16  0.41945472  0.09143192 blue
17 -1.64188953 -0.30545702 blue
18 -1.15801044  1.17682587 blue
19  0.03576081 -1.14670543 blue
20 -0.57466841 -1.69753353 blue

HTH

G

>  
> 
> Med venlig hilsen / Regards
> 
> Joo Fadista
> Ph.d. studerende / Ph.d. student
> 
> 
>  	
>  	 AARHUS UNIVERSITET / UNIVERSITY OF AARHUS	
> Det Jordbrugsvidenskabelige Fakultet / Faculty of Agricultural Sciences	
> Forskningscenter Foulum / Research Centre Foulum	
> Genetik og Bioteknologi / Dept. of Genetics and Biotechnology	
> Blichers All 20, P.O. BOX 50	
> DK-8830 Tjele	
>  	
> Tel:	 +45 8999 1900	
> Direct:	 +45 8999 1900	
> Mobile:	 +45 	
> E-mail:	 Joao.Fadista at agrsci.dk <mailto:Joao.Fadista at agrsci.dk> 	
> Web:	 www.agrsci.dk <http://www.agrsci.dk/> 	
> ________________________________
> 
> Tilmeld dig DJF's nyhedsbrev / Subscribe Faculty of Agricultural Sciences Newsletter <http://www.agrsci.dk/user/register?lan=dan-DK> . 
> 
> Denne email kan indeholde fortrolig information. Enhver brug eller offentliggrelse af denne email uden skriftlig tilladelse fra DJF er ikke tilladt. Hvis De ikke er den tiltnkte adressat, bedes De venligst straks underrette DJF samt slette emailen.
> 
> This email may contain information that is confidential. Any use or publication of this email without written permission from Faculty of Agricultural Sciences is not allowed. If you are not the intended recipient, please notify Faculty of Agricultural Sciences immediately and delete this email.
> 
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT                          [w] http://www.freshwaters.org.uk/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From wewolski at gmail.com  Fri Mar 23 10:44:50 2007
From: wewolski at gmail.com (W Eryk Wolski)
Date: Fri, 23 Mar 2007 10:44:50 +0100
Subject: [R] Translating actxserver server commands to rcom
Message-ID: <7477007c0703230244q63e08c3fmbc2d91c5d5b3f430@mail.gmail.com>

Dear R and R com user....

I have the following matlab which acesses a com server, code which I
am trying to translate to R using the Rcom package:


clear all;
clear analysis;
analysis = actxserver('EDAL.MSAnalysis');
analysis.Open('E:\work\ChromPeakFinderdata\DatafromKlaus\E112L-cad13-PI-24h_pos_1-C,3_01_2562.d');
specCol = analysis.MSSpectrumCollection;
count = specCol.get('Count');


This is how far I got...

rm(analysis)
analysis <- comCreateObject("EDAL.MSAnalysis")
comGetObjectInfo(analysis)
comInvoke(analysis,"open","E:\work\ChromPeakFinderdata\DatafromKlaus\E112L-cad13-PI-24h_pos_1-C,3_01_2562.d")
specCol <- analysis$"MSSpectrumCollection"
> specCol
function (...)
comInvoke(handle, ..FUN, ...)
<environment: 0x022a4734>
> comInvoke(specCol,"get","Count")
NULL


Seems to be the wrong track ... What would be the equivalent of
analysis.Open('E:\work\ChromPeakFinderdata\DatafromKlaus\E112L-cad13-PI-24h_pos_1-C,3_01_2562.d');
specCol = analysis.MSSpectrumCollection;
or  of count = specCol.get('Count');
in Rcom?


Help is highly appreciated...

best wishes

Eryk


From ripley at stats.ox.ac.uk  Fri Mar 23 11:30:05 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Mar 2007 10:30:05 +0000 (GMT)
Subject: [R] Updating a worksheet in Excel file using RODBC
In-Reply-To: <Pine.LNX.4.64.0703230826370.21572@gannet.stats.ox.ac.uk>
References: <B16F2EFF2E11604E93044813DD4413598A591D@warp8.chimaera.local>
	<Pine.LNX.4.64.0703230826370.21572@gannet.stats.ox.ac.uk>
Message-ID: <Pine.LNX.4.64.0703231019420.22798@gannet.stats.ox.ac.uk>

On Fri, 23 Mar 2007, Prof Brian Ripley wrote:

> The problem is that way the ODBC driver exposes table names is not valid
> SQL, and nor is the way quoting has to be used.  You can get around this
> via direct SQL sent by sqlQuery.  In addition, by default the Excel ODBC
> driver gives you read-only access to worksheets.
>
> Searching the list archives, would help, for example this answer:
>
> https://stat.ethz.ch/pipermail/r-help/2007-March/127851.html
>
> Making a wrapper interface in RODBC is on my TODO list, but not anywhere
> near the top.

You seem not to have tried the simplest possible option.  The following 
works for me (beware of wrapped lines from mailers)

chan <- odbcDriverConnect("DRIVER=Microsoft Excel Driver (*.xls);DBQ=C:\\bdr\\hills.xls; ReadOnly=False")
sqlSave(chan, USArrests, "tests", fast=TRUE) # or FALSE

In your example you have a different problem: your field names are invalid 
in SQL (and as R data frame names).


> On Fri, 23 Mar 2007, Moshe Olshansky wrote:
>
>> Hello!
>>
>> I have no problem reading Excel files (each worksheet in the file is a 
>> "table" which can be read - at least in my case).
>>
>> What I would like to do is to read such a table, change it (just the 
>> contents, not the format) and write it back, and this I can not do.  I 
>> am getting the following error messages (3 slightly different 
>> attempts):
>>
>>> sqlSave(con, x, tablename = "Chimaera20_3years$", append = FALSE,
>> +         rownames = FALSE, colnames = TRUE,
>> +         verbose = TRUE, oldstyle = FALSE,safer=FALSE)
>> Query: CREATE TABLE Chimaera20_3years$  (Date varchar(255), 000Tax varchar(255), 1500Tax varchar(255), 3000Tax varchar(255), 4650Tax varchar(255))
>> Error in sqlSave(con, x, tablename = "Chimaera20_3years$", append = FALSE,  :
>>        [RODBC] ERROR: Could not SQLExecDirect
>> 37000 -3551 [Microsoft][ODBC Excel Driver] Syntax error in CREATE TABLE statement.
>>
>>> sqlSave(con, x, tablename = "[Chimaera20_3years$]", append = FALSE,
>> +         rownames = FALSE, colnames = TRUE,
>> +         verbose = TRUE, oldstyle = FALSE,safer=FALSE)
>> Query: CREATE TABLE [Chimaera20_3years$]  (Date varchar(255), 000Tax varchar(255), 1500Tax varchar(255), 3000Tax varchar(255), 4650Tax varchar(255))
>> Error in sqlSave(con, x, tablename = "[Chimaera20_3years$]", append = FALSE,  :
>>        [RODBC] ERROR: Could not SQLExecDirect
>> 37000 -3553 [Microsoft][ODBC Excel Driver] Syntax error in field definition.
>>
>>> sqlSave(con, x, tablename = "[Chimaera20_3years]", append = FALSE,
>> +         rownames = FALSE, colnames = TRUE,
>> +         verbose = TRUE, oldstyle = FALSE,safer=FALSE)
>> Query: CREATE TABLE [Chimaera20_3years]  (Date varchar(255), 000Tax varchar(255), 1500Tax varchar(255), 3000Tax varchar(255), 4650Tax varchar(255))
>> Error in sqlSave(con, x, tablename = "[Chimaera20_3years]", append = FALSE,  :
>>        [RODBC] ERROR: Could not SQLExecDirect
>> 37000 -3553 [Microsoft][ODBC Excel Driver] Syntax error in field definition.
>>
>> Am I doing it wrong way or is there a problem with the Excel driver?
>>
>> Thank you in advance,
>>
>> Moshe Olshansky
>> Chimaera Capital Group
>>
>>
>> Moshe Olshansky
>>
>> Chimaera Capital Limited
>> Level 4 / 349 Collins Street
>> Melbourne, Victoria 3000
>> Phone: +613 8614 8400
>> Fax: +613 8614 8410
>> Email: molshansky at chimaeracapital.com
>>
>> Disclaimer: This message is intended only for the personal and confidential use of the designated recipient(s) named above. If you are not the intended recipient of this message you are hereby notified that any review, dissemination, distribution or copying of this message is strictly prohibited. This communication is for information purposes only and should not be regarded as an offer to sell or as a solicitation of an offer to buy any financial product, an official confirmation of any transaction, or as an official statement of Chimaera Capital Limited. E-mail transmissions cannot be guaranteed to be secure or error-free. Therefore, we do not represent that this information is complete or accurate and it should not be relied upon as such. All information is subject to change without notice.
>>
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From i.m.s.white at ed.ac.uk  Fri Mar 23 11:02:19 2007
From: i.m.s.white at ed.ac.uk (ian white)
Date: Fri, 23 Mar 2007 10:02:19 +0000
Subject: [R] Fitting a line to a qqplot's points?
In-Reply-To: <50d6c72a0703222113sa49f1b3w9643673f189d9eb4@mail.gmail.com>
References: <50d6c72a0703222113sa49f1b3w9643673f189d9eb4@mail.gmail.com>
Message-ID: <1174644139.13826.2.camel@trotter.cap.ed.ac.uk>

See also ?shapiro.test which is effectively based on this correlation
coefficient.


On Fri, 2007-03-23 at 00:13 -0400, Paul Lynch wrote:
> I've made some normal plots of my data using qqplot, and now
> I would like to fit a line to the points on the plot and
> check the correlation coefficient to have a more objective measure
> of how straight the line is.  Is there a simple way of doing that?
> (I'm still pretty new to R.)
> 
> Thanks,
>    --Paul
> 


School of Biological Sciences, University of Edinburgh


From jrkrideau at yahoo.ca  Fri Mar 23 11:53:54 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 23 Mar 2007 06:53:54 -0400 (EDT)
Subject: [R] concatenate 2 data.frames
In-Reply-To: <EA09C4B2B0F16E44B8F3311629493C0D02ED4E99@DJFPOST01.djf.agrsci.dk>
Message-ID: <211732.81128.qm@web32808.mail.mud.yahoo.com>

?rbind
 rbind(df1, df2)

This is a very basic question.  You probably would
save a lot of time if you read some of the
documentation on the CRAN site (not the home R page). 
Click on "Contributed"  on the left side of the screen
and have a look at some of the documentation there. 
There are some excellent materials there.  

I'd recommend perhaps looking at ?Simple R? by John
Verzani and ?A Guide for the Unwilling S User?  by
Patrick Burns.  R and S are basically equivalent for
this purpose.

Good luck and don't forget to read the posting guide.

--- Jo?o Fadista <Joao.Fadista at agrsci.dk> wrote:

> Dear all,
>  
> I would like to know how can I concatenate 2
> data.frames into a single one. Both data frames have
> the same number of columns and the same class type
> in each correspondent column. So what I want is to
> have a new data.frame where I have first the values
> from one data.frame and then the values from a
> second data.frame would came after in this new
> data.frame.
>  
> Thanks in advance.
>  
> 
> Med venlig hilsen / Regards
> 
> Jo?o Fadista
> Ph.d. studerende / Ph.d. student
> 
> 
>  	
>  	 AARHUS UNIVERSITET / UNIVERSITY OF AARHUS	
> Det Jordbrugsvidenskabelige Fakultet / Faculty of
> Agricultural Sciences	
> Forskningscenter Foulum / Research Centre Foulum	
> Genetik og Bioteknologi / Dept. of Genetics and
> Biotechnology	
> Blichers All? 20, P.O. BOX 50	
> DK-8830 Tjele	
>  	
> Tel:	 +45 8999 1900	
> Direct:	 +45 8999 1900	
> Mobile:	 +45 	
> E-mail:	 Joao.Fadista at agrsci.dk
> <mailto:Joao.Fadista at agrsci.dk> 	
> Web:	 www.agrsci.dk <http://www.agrsci.dk/> 	
> ________________________________
> 
> Tilmeld dig DJF's nyhedsbrev / Subscribe Faculty of
> Agricultural Sciences Newsletter
> <http://www.agrsci.dk/user/register?lan=dan-DK> . 
> 
> Denne email kan indeholde fortrolig information.
> Enhver brug eller offentligg?relse af denne email
> uden skriftlig tilladelse fra DJF er ikke tilladt.
> Hvis De ikke er den tilt?nkte adressat, bedes De
> venligst straks underrette DJF samt slette emailen.
> 
> This email may contain information that is
> confidential. Any use or publication of this email
> without written permission from Faculty of
> Agricultural Sciences is not allowed. If you are not
> the intended recipient, please notify Faculty of
> Agricultural Sciences immediately and delete this
> email.
> 
>  
> 
> 	[[alternative HTML version deleted]]
> 
> > ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From christian.l.a.von.plessen at helse-bergen.no  Fri Mar 23 12:06:49 2007
From: christian.l.a.von.plessen at helse-bergen.no (Plessen, Christian von)
Date: Fri, 23 Mar 2007 12:06:49 +0100
Subject: [R] distribution graph
Message-ID: <3E66D12C3545B444BF18FE918807AEE6C4EE05@EC1.ihelse.net>


I am looking for a way to produce a "distribution graph" as in the example: 

(http://cecsweb.dartmouth.edu/release1.1/datatools/dgraph.php?year=2003&geotype=STD_HRR&event=A01_DIS&eventtype=UTIL

Anybody who can help?

Christian von Plessen
Department of Pulmonary Medicine
Haukeland university hospital 
Bergen
Norway


From samay.sar at gmail.com  Fri Mar 23 12:21:17 2007
From: samay.sar at gmail.com (d. sarthi maheshwari)
Date: Fri, 23 Mar 2007 16:51:17 +0530
Subject: [R] Creating new directory/folder from R script on run time.
Message-ID: <d4327f7e0703230421l77dee89eu28ba564aa5f142ea@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070323/3ebdf611/attachment.pl 

From Jan.Wijffels at ucs.kuleuven.be  Fri Mar 23 12:35:55 2007
From: Jan.Wijffels at ucs.kuleuven.be (Jan Wijffels)
Date: Fri, 23 Mar 2007 12:35:55 +0100
Subject: [R] Effect display of proportional odds model
Message-ID: <005301c76d3f$6b778270$2c70210a@UCSPC32>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070323/65720c69/attachment.pl 

From ozric at web.de  Fri Mar 23 12:36:41 2007
From: ozric at web.de (Christian Schulz)
Date: Fri, 23 Mar 2007 12:36:41 +0100
Subject: [R] Cohen's Kappa
In-Reply-To: <20070322140113.A15436@fellspt.charm.net>
References: <46029F5B.5050206@web.de> <20070322140113.A15436@fellspt.charm.net>
Message-ID: <4603BBC9.6030903@web.de>

...many thanks  for all the answers  and clarity!!!
regards, christian

> Hi,
>
> Chance is not .5 in your data, it's a function of the expected values 
> for presence and absence:
>
>   
>> (((7792*10855)/11974) + ((4182*1119)/11974))/11974
>>     
> [1] 0.6225686
>
>   
>> (.6862368-.6225686)/(1-.6225686)
>>     
> [1] 0.1686881
>
> Scot
>
>
> On Thu, 22 Mar 2007, Christian Schulz wrote:
>
>   
>> Hi,
>>
>> im little bit confused about Cohen's Kappa and i should  be look into the
>> Kappa function code. Is the easy formula really wrong?
>>
>> kappa=agreement-chance/(1-chance)
>>
>> many thanks
>> christian
>>
>> ###############################################################################
>> true-negativ:7445
>> false-positive:3410
>> false-negativ:347
>> true-positiv:772
>>
>> classification-aggrement:68,6%
>> kappa=agreement-chance/(1-chance) = (0.686-0.5)/0.5=0.372
>>
>> .....with function from library(vcd)
>> Kappa(matrix(c(7445,3410,347,772),nrow=2))
>>               value         ASE
>> Unweighted 0.1686882 0.011235188
>> Weighted   0.1686882 0.007979293
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>     
>
>
> --
>    Scot W. McNary  email: smcnaryatcharmdotnet
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From wwwhsd at gmail.com  Fri Mar 23 12:57:56 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Fri, 23 Mar 2007 08:57:56 -0300
Subject: [R] Creating new directory/folder from R script on run time.
In-Reply-To: <d4327f7e0703230421l77dee89eu28ba564aa5f142ea@mail.gmail.com>
References: <d4327f7e0703230421l77dee89eu28ba564aa5f142ea@mail.gmail.com>
Message-ID: <da79af330703230457m65d378ebkddd885251f7113c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070323/903fdccd/attachment.pl 

From jrkrideau at yahoo.ca  Fri Mar 23 13:07:41 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 23 Mar 2007 08:07:41 -0400 (EDT)
Subject: [R] Truncated x-axis values
In-Reply-To: <429269.51439.qm@web39715.mail.mud.yahoo.com>
Message-ID: <610773.18551.qm@web32815.mail.mud.yahoo.com>

Also think aboutpar( cex.axis = something small)

Depending on what you're doing you might want to
consider usin horizontal=TRUE  and reduce the plot
size a bit to accomomodate the label lengths.

I don't know the packages well enough to be sure but I
suspect that lattice or ggplot may handle this problem
by allowing an angled label

--- Stephen Tucker <brown_emu at yahoo.com> wrote:

> You can try playing around with oma, omi, mai, mar,
> etc. in par():
> 
> myMai <- par("mai")
> myMai[1] <- max(nchar(y))*par("cin")[1]
> par(mfrow=c(2, 1),mai=myMai,oma=rep(0,4),las = 2)
> # your plot
> 
> --- "Urban, Alexander" <a-urban at ti.com> wrote:
> 
> > John
> > Thanks for your reply and sorry for not beein
> specific enough
> > Try this piece of code (it's an abstraction of my
> application...)
> > 
> > par(las = 2)
> > par(mfrow=c(2, 1))
> > x =  c(1,2,3,4,5,6,7,8)
> > y = c("abcdccefghijk", "bcssdefghijkl",
> "abddessfghijk",
> >
>
"bddessfghijkl","abcdssedghijk","bcdedghijkl","assbcdefghidk","bcdedssgh
> > ijkl")
> > 
> > boxplot(x~y)
> > boxplot(x~y) 
> > 
> > In the lower chart the labels are truncated...(not
> totally visible)
> > Is this clearer now?
> > 
> > Thanks
> > Alex
> > 
> > -----Original Message-----
> > From: John Kane [mailto:jrkrideau at yahoo.ca] 
> > Sent: Thursday, March 22, 2007 19:19
> > To: Urban, Alexander; r-help at stat.math.ethz.ch
> > Subject: Re: [R] Truncated x-axis values
> > 
> > You really have not told us much about what you're
> actually doing.  A
> > simple self-contained example of what you're
> trying to do might let
> > someone help. 
> > 
> > Have you read the posting guide?
> > 
> > --- "Urban, Alexander" <a-urban at ti.com> wrote:
> > 
> > > Hello
> > > 
> > > I'm new to this group. I looked up the last two
> hour in the help file 
> > > and in the archives of this group, but didn't
> find anything.
> > > I hope my question is not too dump:
> > > I'm printing a graph with vertical labels on the
> x-axis (necessary due
> > 
> > > to many labels). Unfortunately the png truncates
> the labels halfway 
> > > through, so that you can only read the last 7
> digits of the label.
> > > Snice I'm already asking :-): Is there a
> possibility to tell R: If 
> > > there are so many labels that you write them on
> top of each other, 
> > > take only e.g. every 2nd...
> > > 
> > > Sorry for bothering and thanks
> > > Alex
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained,
> reproducible code.
> > > 
> > 
> > 
> > __________________________________________________
> > Do You Yahoo!?

> protection around
> > http://mail.yahoo.com
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> > 
> 
> 
> 
>  
>
____________________________________________________________________________________
> Looking for earth-friendly autos? 
> Browse Top Cars by "Green Rating" at Yahoo! Autos'
> Green Center.
> http://autos.yahoo.com/green_center/
>


From lobry at biomserv.univ-lyon1.fr  Fri Mar 23 13:11:29 2007
From: lobry at biomserv.univ-lyon1.fr (Jean lobry)
Date: Fri, 23 Mar 2007 13:11:29 +0100
Subject: [R] sort argument in mosaicplot
Message-ID: <p06002005c22971272eb5@[192.168.1.10]>

Dear R-help,

I do not understand how to use the "sort" argument in mosaicplot().
 From the documentation sort is a "vector ordering of the variables,
containing a permutation of the integers 1:length(dim(x)) (the default)."

x <- matrix(1:4,2,2)
mosaicplot(x)
# This one is OK
mosaicplot(x, sort = 1:length(dim(x)) )
# Not OK, I have the following error message:
Erreur dans mosaicplot.default(x, sort = 1:length(dim(x))) :
         objet "label" non trouv?

which means that the object "label" was not found in mosaicplot.default().

How can I change the ordering of variables in mosaicplot() ?

Best,

>  sessionInfo()
R version 2.4.1 (2006-12-18)
i386-apple-darwin8.8.1

locale:
C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
[7] "base"
-- 
Jean R. Lobry            (lobry at biomserv.univ-lyon1.fr)
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - LYON I,
43 Bd 11/11/1918, F-69622 VILLEURBANNE CEDEX, FRANCE
allo  : +33 472 43 27 56     fax    : +33 472 43 13 88
http://pbil.univ-lyon1.fr/members/lobry/


From Corinna.Schmitt at igb.fraunhofer.de  Fri Mar 23 13:30:10 2007
From: Corinna.Schmitt at igb.fraunhofer.de (Schmitt, Corinna)
Date: Fri, 23 Mar 2007 13:30:10 +0100
Subject: [R] Binary information convert into hexadecimal
Message-ID: <8B7B0FD99E8AF541A21609104D19615882E7E4@izs-xchg01.izs.fraunhofer.de>


Hallo,

I just started with programming with R. I have the following problem:

Given is binary information and should be translated into integer and
afterwards into hexadecimal:

0000  0  0
0001  1  1
0010  2  2
0011  3  3
0100  4  4
0101  5  5
0110  6  6
0111  7  7
1000  8  8 
1001  9  9
1010 10  A
1011 11  B
1100 12  C
1101 13  D
1110 14  E
1111 15  F


I found a similar function for translating an integer vector into
characters and back to integer (z=0:9; digits=as.character(z);
d=as.integer(digits)).
My idea was to make a kind of case-task as I know from RUBY. But there
should exits an easier way.

Can anyone help me?

Corinna


From ggrothendieck at gmail.com  Fri Mar 23 13:25:37 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 23 Mar 2007 08:25:37 -0400
Subject: [R] Effect display of proportional odds model
In-Reply-To: <005301c76d3f$6b778270$2c70210a@UCSPC32>
References: <005301c76d3f$6b778270$2c70210a@UCSPC32>
Message-ID: <971536df0703230525t28913e20t1bc30b86f727cb11@mail.gmail.com>

Looks like there is code in the appendix.

On 3/23/07, Jan Wijffels <Jan.Wijffels at ucs.kuleuven.be> wrote:
> Dear useRs,
> I very much like the effect display of the proportional odds model on
> page 29 (Figure 8) of the following paper by John Fox:
> http://socserv.mcmaster.ca/jfox/Papers/logit-effect-displays.pdf
> It really gives a very concise overview of the model. I would like to
> use it to illustrate the proportional odds mixed models we fit here for
> a project on Diabetes but I can't seem to reproduce the plot. Does
> anyone have code for the plot? Maybe John Fox himself? I would
> appreciate it very much.
> Thanks,
> Jan
>
> Jan Wijffels
> University Center for Statistics
> W. de Croylaan 54
> 3001 Heverlee
> Belgium
> tel: +32 (0)16 322784
> fax: +32 (0)16 322831
>  <http://www.kuleuven.be/ucs> http://www.kuleuven.be/ucs
>
>
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From albmont at centroin.com.br  Fri Mar 23 13:30:17 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 23 Mar 2007 10:30:17 -0200
Subject: [R] Get "home" directory and simple I/O
Message-ID: <20070323122543.M34818@centroin.com.br>

Is there any generic function that gets the "home" directory? This
should return /home/<user> in Linux and 
x:/Documents and Settings/<user> (or whatever) in Windows XP.

Another (unrelated) question: what is the _simplest_ way to
read and write R variables to/from files such that they are
stored in a human-readable but R-like form? For example, if 
(say), x is a vector defined as x <- c(1, 2, 3), can I write 
(and read) x as a file with just one line, namely: c(1, 2, 3) ?

Alberto Monteiro


From valderama at gmail.com  Fri Mar 23 13:36:48 2007
From: valderama at gmail.com (Laurent Valdes)
Date: Fri, 23 Mar 2007 13:36:48 +0100
Subject: [R] dimension reduction and multinomial responses
Message-ID: <3ef00e160703230536y7f371d9cu5793031c133f846e@mail.gmail.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070323/1916fa2d/attachment.pl 

From kubovy at virginia.edu  Fri Mar 23 13:37:53 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Fri, 23 Mar 2007 05:37:53 -0700
Subject: [R] distribution graph
In-Reply-To: <3E66D12C3545B444BF18FE918807AEE6C4EE05@EC1.ihelse.net>
References: <3E66D12C3545B444BF18FE918807AEE6C4EE05@EC1.ihelse.net>
Message-ID: <D1D510CB-FAC5-41F3-B6A5-03519F6BDF07@virginia.edu>

?violinplot (You need to install the UsingR package first.)

On Mar 23, 2007, at 4:06 AM, Plessen, Christian von wrote:

> I am looking for a way to produce a "distribution graph" as in the  
> example:
>
> (http://cecsweb.dartmouth.edu/release1.1/datatools/dgraph.php? 
> year=2003&geotype=STD_HRR&event=A01_DIS&eventtype=UTIL
>
> Anybody who can help?


From webb at stats.ox.ac.uk  Fri Mar 23 13:40:32 2007
From: webb at stats.ox.ac.uk (webb at stats.ox.ac.uk)
Date: Fri, 23 Mar 2007 12:40:32 -0000 (GMT)
Subject: [R] Change Axis Size
Message-ID: <1721.163.1.211.160.1174653632.squirrel@webmail.stats.ox.ac.uk>

Sorry if this is an obvious question but I have not been able to find the
answer.
I wish to plot 3 lines on the same plot. However, whichever one I plot
first, the axis does not have a big enough range for the other two to be
shown in the plot (they get cut off at the top and the bottom). Is there a
way to change the range of the y-axis when adding a new line to the plot?

Thank you,

Alex


From albmont at centroin.com.br  Fri Mar 23 13:41:11 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 23 Mar 2007 10:41:11 -0200
Subject: [R] Binary information convert into hexadecimal
In-Reply-To: <8B7B0FD99E8AF541A21609104D19615882E7E4@izs-xchg01.izs.fraunhofer.de>
References: <8B7B0FD99E8AF541A21609104D19615882E7E4@izs-xchg01.izs.fraunhofer.de>
Message-ID: <20070323123130.M50913@centroin.com.br>

Corinna Schmitt wrote:
> 
> Given is binary information and should be translated into integer and
> afterwards into hexadecimal:
> 
> 0000  0  0
>
If you have a _string_ of 0s and 1s, and you want to convert
it to an integer, I think the best way should be:

(1) convert to a vector of 0s and 1s
(2) convert to integer
(3) an integer to hex conversion is trivial (sprinf("%x", n))

(1): 
  as.integer(unlist(strsplit("0100101", NULL))) 
will do the job: 
strsplit breaks the string, unlist transforms the resulting list
into a vector of (length-one) strings, and as.integer converts
the strings into integers (0 or 1).

(2): 
  x <- c(1, 1, 0, 1, 0)
  n <- sum(x * 2^seq(length(x)-1, 0, -1))

x is the vector of 0s and 1s from (1) above. 
length(x) is its length. 
seq(length(x) - 1, 0, -1) will be the vector of integers
(in this case) 4, 3, 2, 1, 0. 
2^seq(length(x)-1, 0, -1) will be the powers of two 16, 8, 4, 2, 1.
x * 2^... will form the terms of the polynomial
sum will compute 2^(n-1) x[0] + 2^(n-2) x[1] + ... + 2 * x[n-1] + x[n]

Alberto Monteiro


From erhansen at math.ku.dk  Fri Mar 23 13:42:19 2007
From: erhansen at math.ku.dk (Ernst Hansen)
Date: Fri, 23 Mar 2007 13:42:19 +0100
Subject: [R] Effect display of proportional odds model
In-Reply-To: <971536df0703230525t28913e20t1bc30b86f727cb11@mail.gmail.com>
References: <005301c76d3f$6b778270$2c70210a@UCSPC32>
	<971536df0703230525t28913e20t1bc30b86f727cb11@mail.gmail.com>
Message-ID: <17923.52011.982851.968197@pc000ea60d1ce2.math.ku.dk>

Gabor Grothendieck writes:
 > Looks like there is code in the appendix.

The appendix even has a URL where the code is available, namely 

  http://socserv.mcmaster.ca/jfox/Papers/polytomous-effect-displays.html



 > On 3/23/07, Jan Wijffels <Jan.Wijffels at ucs.kuleuven.be> wrote:
 > > Dear useRs,
 > > I very much like the effect display of the proportional odds model on
 > > page 29 (Figure 8) of the following paper by John Fox:
 > > http://socserv.mcmaster.ca/jfox/Papers/logit-effect-displays.pdf


Ernst Hansen
Department of Mathematics
University of Copenhagen


From Thierry.ONKELINX at inbo.be  Fri Mar 23 13:54:30 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Fri, 23 Mar 2007 13:54:30 +0100
Subject: [R] Change Axis Size
In-Reply-To: <1721.163.1.211.160.1174653632.squirrel@webmail.stats.ox.ac.uk>
Message-ID: <2E9C414912813E4EB981326983E0A10402BC5129@inboexch.inbo.be>

You need to sed the ylim parameter in the plot function to the range
that you need.

Cheers,

Thierry

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

 

> -----Oorspronkelijk bericht-----
> Van: r-help-bounces op stat.math.ethz.ch 
> [mailto:r-help-bounces op stat.math.ethz.ch] Namens webb op stats.ox.ac.uk
> Verzonden: vrijdag 23 maart 2007 13:41
> Aan: r-help op stat.math.ethz.ch
> Onderwerp: [R] Change Axis Size
> 
> Sorry if this is an obvious question but I have not been able 
> to find the answer.
> I wish to plot 3 lines on the same plot. However, whichever 
> one I plot first, the axis does not have a big enough range 
> for the other two to be shown in the plot (they get cut off 
> at the top and the bottom). Is there a way to change the 
> range of the y-axis when adding a new line to the plot?
> 
> Thank you,
> 
> Alex
> 
> ______________________________________________
> R-help op stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From baron at psych.upenn.edu  Fri Mar 23 13:57:44 2007
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Fri, 23 Mar 2007 08:57:44 -0400
Subject: [R] Cohen's Kappa
In-Reply-To: <4603BBC9.6030903@web.de>
References: <46029F5B.5050206@web.de> <20070322140113.A15436@fellspt.charm.net>
	<4603BBC9.6030903@web.de>
Message-ID: <20070323125744.GA25112@psych.upenn.edu>

See also section 6.4 of
http://www.psych.upenn.edu/~baron/rpsych/rpsych.html

which also points to a few packages that have kappa code in them.
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron


From luke at stat.uiowa.edu  Fri Mar 23 13:56:14 2007
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Fri, 23 Mar 2007 07:56:14 -0500 (CDT)
Subject: [R] can't load just saved R object "ReadItem: unknown type 65"
In-Reply-To: <46033E07.4040905@gmail.com>
References: <46033E07.4040905@gmail.com>
Message-ID: <Pine.LNX.4.64.0703230753500.20383@nokomis.stat.uiowa.edu>

According to the logs nothing at all has changed in the serialization
code in a month and nothing of consequence for much longer than that.
To track this down we will need a complete, reproducible, and
preferably minimal example.

Best,

luke

On Thu, 22 Mar 2007, Mark W Kimpel wrote:

> I have run into a problem loading a just saved R object using R-devel. I
> have been saving and loading this particular type of R object for a long
> while and never ran into this problem. I save, then immediately reload
> (to test save) and get "ReadItem: unnknown type 65".
>
> This error is reproducible after logout from server and restart of emacs
> and R.
>
> Below is my output and sessionInfo().
>
> Thanks,
> Mark
>
> > setwd("~/Genomics/Experiments.Genomic/BB01/acb.shell")
> > local(save(affy.object.preprocessed, file
> ="affy.object.preprocessed.R" ))
> > load("affy.object.preprocessed.R")
> Error in load("affy.object.preprocessed.R") :
> 	ReadItem: unknown type 65, perhaps written by later version of R
> > sessionInfo()
> R version 2.5.0 Under development (unstable) (2007-03-11 r40824)
> powerpc64-unknown-linux-gnu
>
> locale:
> LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C
>
> attached base packages:
> [1] "splines"   "stats"     "graphics"  "grDevices" "datasets"  "utils"
> [7] "tools"     "methods"   "base"
>
> other attached packages:
>      multtest    rat2302cdf affycoretools       annaffy        xtable
>      "1.13.1"      "1.15.0"       "1.7.8"       "1.7.3"       "1.4-3"
>         gcrma   matchprobes       biomaRt         RCurl           XML
>       "2.7.3"       "1.7.4"      "1.9.21"       "0.8-0"       "1.6-0"
>       GOstats      Category        Matrix       lattice    genefilter
>      "2.1.13"      "2.1.20"   "0.9975-11"     "0.14-16"      "1.13.8"
>      survival          KEGG          RBGL      annotate            GO
>        "2.31"     "1.15.12"      "1.11.4"      "1.13.6"     "1.15.12"
>         graph         limma          affy        affyio       Biobase
>      "1.13.6"      "2.9.13"     "1.13.14"       "1.3.3"     "1.13.39"
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From andza at osi.lv  Fri Mar 23 13:59:26 2007
From: andza at osi.lv (Andris Jankevics)
Date: Fri, 23 Mar 2007 14:59:26 +0200
Subject: [R] Get "home" directory and simple I/O
In-Reply-To: <20070323122543.M34818@centroin.com.br>
References: <20070323122543.M34818@centroin.com.br>
Message-ID: <200703231459.26404.andza@osi.lv>

If you want get a username of user currently running R on Linux,you can use a 
system command and read enviroment variables:

paste("/home/",system ("whoami",intern=TRUE),sep="")


Andris Jankevics

On Piektdiena, 23. Marts 2007 14:30, Alberto Monteiro wrote:
> Is there any generic function that gets the "home" directory? This
> should return /home/<user> in Linux and
> x:/Documents and Settings/<user> (or whatever) in Windows XP.
>
> Another (unrelated) question: what is the _simplest_ way to
> read and write R variables to/from files such that they are
> stored in a human-readable but R-like form? For example, if
> (say), x is a vector defined as x <- c(1, 2, 3), can I write
> (and read) x as a file with just one line, namely: c(1, 2, 3) ?
>
> Alberto Monteiro
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.


From ted.harding at nessie.mcc.ac.uk  Fri Mar 23 14:04:55 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 23 Mar 2007 13:04:55 -0000 (GMT)
Subject: [R] distribution graph
In-Reply-To: <3E66D12C3545B444BF18FE918807AEE6C4EE05@EC1.ihelse.net>
Message-ID: <XFMail.070323130455.ted.harding@nessie.mcc.ac.uk>

On 23-Mar-07 11:06:49, Plessen, Christian von wrote:
> 
> I am looking for a way to produce a "distribution graph" as in the
> example: 
> 
> (http://cecsweb.dartmouth.edu/release1.1/datatools/dgraph.php?year=2003&
> geotype=STD_HRR&event=A01_DIS&eventtype=UTIL
> 
> Anybody who can help?
> 
> Christian von Plessen
> Department of Pulmonary Medicine
> Haukeland university hospital 
> Bergen
> Norway

The following (which anyway needs refinement, and can very
probably be done better) provides a basis (illustrated using
a sample from a log-normal distribution):


X<-exp(rnorm(200,sd=0.25)+2)/5

H<-hist(X,breaks=20)
C<-H$counts
Y<-H$mids
C1<-C/2

C0<-(-C1[1]-1/2):(C[1]-1/2); n0<-length(C0)
plot(C0,rep(Y[1],n0),xlim=c(-max(C)/2,max(C)/2),ylim=c(min(Y),max(Y)))

for(i in (2:length(Y))){
  C0<-(-C1[i]-1/2):(C1[i]-1/2); n0<-length(C0)
  points(C0,rep(Y[i],n0))
}


Hoping this helps!
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 23-Mar-07                                       Time: 13:04:51
------------------------------ XFMail ------------------------------


From jfox at mcmaster.ca  Fri Mar 23 14:06:36 2007
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 23 Mar 2007 09:06:36 -0400
Subject: [R] Effect display of proportional odds model
In-Reply-To: <005301c76d3f$6b778270$2c70210a@UCSPC32>
Message-ID: <20070323130636.GZNH1637.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Jan,

First, I inadvertently removed material on these displays from my web site
when the paper was published in Sociological Methodology 2006. I'll update
and repost the material some time in the next couple of days, including a
copy of the published paper (with a link on my home page). 

Second, the appendix to the paper and the originally posted examples didn't
include the code for Figure 8 (which is Figure 10 in the published version
of the paper). I'll add that.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jan Wijffels
> Sent: Friday, March 23, 2007 7:36 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Effect display of proportional odds model
> 
> Dear useRs,
> I very much like the effect display of the proportional odds 
> model on page 29 (Figure 8) of the following paper by John Fox:
> http://socserv.mcmaster.ca/jfox/Papers/logit-effect-displays.pdf
> It really gives a very concise overview of the model. I would 
> like to use it to illustrate the proportional odds mixed 
> models we fit here for a project on Diabetes but I can't seem 
> to reproduce the plot. Does anyone have code for the plot? 
> Maybe John Fox himself? I would appreciate it very much.
> Thanks,
> Jan
>  
> Jan Wijffels
> University Center for Statistics
> W. de Croylaan 54
> 3001 Heverlee
> Belgium
> tel: +32 (0)16 322784
> fax: +32 (0)16 322831
>  <http://www.kuleuven.be/ucs> http://www.kuleuven.be/ucs
>  
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From gavin.simpson at ucl.ac.uk  Fri Mar 23 14:13:48 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 23 Mar 2007 13:13:48 +0000
Subject: [R] Change Axis Size
In-Reply-To: <1721.163.1.211.160.1174653632.squirrel@webmail.stats.ox.ac.uk>
References: <1721.163.1.211.160.1174653632.squirrel@webmail.stats.ox.ac.uk>
Message-ID: <1174655628.2687.18.camel@gsimpson.geog.ucl.ac.uk>

On Fri, 2007-03-23 at 12:40 +0000, webb at stats.ox.ac.uk wrote:
> Sorry if this is an obvious question but I have not been able to find the
> answer.
> I wish to plot 3 lines on the same plot. However, whichever one I plot
> first, the axis does not have a big enough range for the other two to be
> shown in the plot (they get cut off at the top and the bottom). Is there a
> way to change the range of the y-axis when adding a new line to the plot?
> 
> Thank you,
> 
> Alex

Use the ylim parameter in plot():

x <- 1:100
y1 <- sort(runif(100))
y2 <- y1 * 1.2
y3 <- y2 * rnorm(100)
plot(x, y1, type = "n", ylim = range(y1, y2, y3))
lines(x, y1, col = "red")
lines(x, y2, col = "blue")
lines(x, y3, col = "green")

An alternative is matplot:

matplot(x, cbind(y1,y2,y3), col = c("red", "blue", "green"), 
        type = "l", lty = "solid")

or 

matplot(x, cbind(y1,y2,y3), type = "n")
matlines(x, cbind(y1,y2,y3), col = c("red", "blue", "green"), 
         lty = "solid")

HTH

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From jholtman at gmail.com  Fri Mar 23 14:18:52 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 23 Mar 2007 09:18:52 -0400
Subject: [R] Get "home" directory and simple I/O
In-Reply-To: <20070323122543.M34818@centroin.com.br>
References: <20070323122543.M34818@centroin.com.br>
Message-ID: <644e1f320703230618u1de9fc77saf4d9ed197b8d774@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070323/66516464/attachment.pl 

From han at dmi.dk  Fri Mar 23 14:19:04 2007
From: han at dmi.dk (Henrik Andersson)
Date: Fri, 23 Mar 2007 14:19:04 +0100
Subject: [R] Get "home" directory and simple I/O
In-Reply-To: <200703231459.26404.andza@osi.lv>
References: <20070323122543.M34818@centroin.com.br>
	<200703231459.26404.andza@osi.lv>
Message-ID: <4603D3C8.8020903@dmi.dk>

Sys.getenv("HOME") works in Linux at least

- Henrik

Andris Jankevics wrote:
> If you want get a username of user currently running R on Linux,you can use a 
> system command and read enviroment variables:
>
> paste("/home/",system ("whoami",intern=TRUE),sep="")
>
>
> Andris Jankevics
>
> On Piektdiena, 23. Marts 2007 14:30, Alberto Monteiro wrote:
>   
>> Is there any generic function that gets the "home" directory? This
>> should return /home/<user> in Linux and
>> x:/Documents and Settings/<user> (or whatever) in Windows XP.
>>
>> Another (unrelated) question: what is the _simplest_ way to
>> read and write R variables to/from files such that they are
>> stored in a human-readable but R-like form? For example, if
>> (say), x is a vector defined as x <- c(1, 2, 3), can I write
>> (and read) x as a file with just one line, namely: c(1, 2, 3) ?
>>
>> Alberto Monteiro
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html and provide commented, minimal,
>> self-contained, reproducible code.
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
Henrik Andersson

Danish Meteorological Institute
Lyngbyvej 100
2100 Copenhagen ?
Denmark
Tel: +45 39157215 
Email: han at dmi.dk


From jeffmiller at alphapoint05.net  Fri Mar 23 14:40:13 2007
From: jeffmiller at alphapoint05.net (Jeff Miller)
Date: Fri, 23 Mar 2007 09:40:13 -0400
Subject: [R] Effect display of proportional odds model
In-Reply-To: <20070323130636.GZNH1637.tomts20-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <!&!AAAAAAAAAAAYAAAAAAAAAEuLs8ZPMuFInPbB+1JmPy7CgAAAEAAAAGzf3YyEUMFAhUt+t87gVAgBAAAAAA==@alphapoint05.net>

I REALLY found this paper to be helpful. Will you please let the list know
once you have made the update?


Thank you,

Jeff Miller
University of Florida 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Fox
Sent: Friday, March 23, 2007 9:07 AM
To: 'Jan Wijffels'
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Effect display of proportional odds model

Dear Jan,

First, I inadvertently removed material on these displays from my web site
when the paper was published in Sociological Methodology 2006. I'll update
and repost the material some time in the next couple of days, including a
copy of the published paper (with a link on my home page). 

Second, the appendix to the paper and the originally posted examples didn't
include the code for Figure 8 (which is Figure 10 in the published version
of the paper). I'll add that.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jan Wijffels
> Sent: Friday, March 23, 2007 7:36 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Effect display of proportional odds model
> 
> Dear useRs,
> I very much like the effect display of the proportional odds model on 
> page 29 (Figure 8) of the following paper by John Fox:
> http://socserv.mcmaster.ca/jfox/Papers/logit-effect-displays.pdf
> It really gives a very concise overview of the model. I would like to 
> use it to illustrate the proportional odds mixed models we fit here 
> for a project on Diabetes but I can't seem to reproduce the plot. Does 
> anyone have code for the plot?
> Maybe John Fox himself? I would appreciate it very much.
> Thanks,
> Jan
>  
> Jan Wijffels
> University Center for Statistics
> W. de Croylaan 54
> 3001 Heverlee
> Belgium
> tel: +32 (0)16 322784
> fax: +32 (0)16 322831
>  <http://www.kuleuven.be/ucs> http://www.kuleuven.be/ucs
>  
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at yahoo.ca  Fri Mar 23 14:43:54 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 23 Mar 2007 09:43:54 -0400 (EDT)
Subject: [R] Change Axis Size
In-Reply-To: <1721.163.1.211.160.1174653632.squirrel@webmail.stats.ox.ac.uk>
Message-ID: <773087.34244.qm@web32814.mail.mud.yahoo.com>

I think that technically the answer is no.  What you
need to do is set the size of the appropriate axis
with the first plot command ( xlim = or ylim = 

Have a look at ?plot.default
--- webb at stats.ox.ac.uk wrote:

> Sorry if this is an obvious question but I have not
> been able to find the
> answer.
> I wish to plot 3 lines on the same plot. However,
> whichever one I plot
> first, the axis does not have a big enough range for
> the other two to be
> shown in the plot (they get cut off at the top and
> the bottom). Is there a
> way to change the range of the y-axis when adding a
> new line to the plot?
> 
> Thank you,
> 
> Alex
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From ggrothendieck at gmail.com  Fri Mar 23 14:51:17 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 23 Mar 2007 09:51:17 -0400
Subject: [R] Get "home" directory and simple I/O
In-Reply-To: <20070323122543.M34818@centroin.com.br>
References: <20070323122543.M34818@centroin.com.br>
Message-ID: <971536df0703230651q469579desc7e278462724494@mail.gmail.com>

See:

?R.home
?dput

On 3/23/07, Alberto Monteiro <albmont at centroin.com.br> wrote:
> Is there any generic function that gets the "home" directory? This
> should return /home/<user> in Linux and
> x:/Documents and Settings/<user> (or whatever) in Windows XP.
>
> Another (unrelated) question: what is the _simplest_ way to
> read and write R variables to/from files such that they are
> stored in a human-readable but R-like form? For example, if
> (say), x is a vector defined as x <- c(1, 2, 3), can I write
> (and read) x as a file with just one line, namely: c(1, 2, 3) ?
>
> Alberto Monteiro
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From andy_liaw at merck.com  Fri Mar 23 15:02:26 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 23 Mar 2007 10:02:26 -0400
Subject: [R] Get "home" directory and simple I/O
In-Reply-To: <971536df0703230651q469579desc7e278462724494@mail.gmail.com>
References: <20070323122543.M34818@centroin.com.br>
	<971536df0703230651q469579desc7e278462724494@mail.gmail.com>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03EB25AF@usctmx1106.merck.com>

From: Gabor Grothendieck
> 
> See:
> 
> ?R.home

That's not what Alberto wanted:  It gives the location of the R
installation, not where user's home directory is.  AFAIK Windows does
not set the HOME environment variable by default.

> ?dput
> 
> On 3/23/07, Alberto Monteiro <albmont at centroin.com.br> wrote:
> > Is there any generic function that gets the "home" directory? This 
> > should return /home/<user> in Linux and x:/Documents and 
> > Settings/<user> (or whatever) in Windows XP.
> >
> > Another (unrelated) question: what is the _simplest_ way to 
> read and 
> > write R variables to/from files such that they are stored in a 
> > human-readable but R-like form? For example, if (say), x is 
> a vector 
> > defined as x <- c(1, 2, 3), can I write (and read) x as a file with 
> > just one line, namely: c(1, 2, 3) ?
> >
> > Alberto Monteiro
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From Fabian.Mollet at wur.nl  Fri Mar 23 15:04:44 2007
From: Fabian.Mollet at wur.nl (Mollet, Fabian)
Date: Fri, 23 Mar 2007 15:04:44 +0100
Subject: [R] generating lognormal variables with given correlation
Message-ID: <2DF51316D2ACEF4891856424912C454E427A08@scomp0040.wurnet.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070323/24aa56bd/attachment.pl 

From ted.harding at nessie.mcc.ac.uk  Fri Mar 23 15:22:53 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 23 Mar 2007 14:22:53 -0000 (GMT)
Subject: [R] distribution graph
In-Reply-To: <3E66D12C3545B444BF18FE918807AEE6C4EE05@EC1.ihelse.net>
Message-ID: <XFMail.070323130455.ted.harding@nessie.mcc.ac.uk>

[Apologies -- there were errors in the code I posted previously.
 A corrected version is below]

On 23-Mar-07 11:06:49, Plessen, Christian von wrote:
> 
> I am looking for a way to produce a "distribution graph" as in the
> example: 
> 
> (http://cecsweb.dartmouth.edu/release1.1/datatools/dgraph.php?year=2003&
> geotype=STD_HRR&event=A01_DIS&eventtype=UTIL
> 
> Anybody who can help?
> 
> Christian von Plessen
> Department of Pulmonary Medicine
> Haukeland university hospital 
> Bergen
> Norway

The following (which anyway needs refinement, and can very
probably be done better) provides a basis (illustrated using
a sample from a log-normal distribution):


X<-exp(rnorm(200,sd=0.25)+2)/5

H<-hist(X,breaks=20)
C<-H$counts
Y<-H$mids
C1<-C/2

C0<-(-(C1[1]-1/2)):(C1[1]-1/2); n0<-length(C0)
plot(C0,rep(Y[1],n0),xlim=c(-max(C)/2,max(C)/2),ylim=c(min(Y),max(Y)))

for(i in (2:length(Y))){
  if(C[i]==0) next
  C0 <- (-(C1[i] - 1/2)):(C1[i] - 1/2); n0<-length(C0)
  points(C0,rep(Y[i],n0))
}


Hoping this helps!
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 23-Mar-07                                       Time: 13:04:51
------------------------------ XFMail ------------------------------

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 23-Mar-07                                       Time: 14:22:50
------------------------------ XFMail ------------------------------


From Karl.Hufthammer at math.uib.no  Fri Mar 23 15:27:30 2007
From: Karl.Hufthammer at math.uib.no (Karl Ove Hufthammer)
Date: Fri, 23 Mar 2007 15:27:30 +0100
Subject: [R] generating lognormal variables with given correlation
Message-ID: <200703231527.30624.Karl.Hufthammer@math.uib.no>

Mollet, Fabian:

> I would like these (lognormal distributed) parameters to be correlated,
> that is, I would like to have pairwise samples of 2 parameters with a
> given correlation coefficient.
>  
> I have seen that a covariance matrix can be fixed when generating random
> variables from a multivariate normal distribution e.g. by the function
> mvrnorm.
>  
> Is there a function to do the same for a multivariate lognormal
> distribution?

I don't know about any, but you should be aware that not all values of the 
correlation is possible with lognormal distributions. For example, if both 
variables have a standard lognormal distribution, they can't have correlation 
less than 1/e = -0.37. As the variance of the two distributions increase, the 
absolute value of the maximum and minimum correlation possible decrease (to 
zero).

Using the normal product-moment correlation as a measure of dependence rarely 
makes much sense unless the association between the variables is linear.

-- 
Karl Ove Hufthammer


From ripley at stats.ox.ac.uk  Fri Mar 23 15:36:27 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Mar 2007 14:36:27 +0000 (GMT)
Subject: [R] Get "home" directory and simple I/O
In-Reply-To: <4603D3C8.8020903@dmi.dk>
References: <20070323122543.M34818@centroin.com.br>
	<200703231459.26404.andza@osi.lv> <4603D3C8.8020903@dmi.dk>
Message-ID: <Pine.LNX.4.64.0703231405580.31231@gannet.stats.ox.ac.uk>

But the request was for a *generic* solution.  On Windows there
might not be anything corresponding to a home directory (and the rw-FAQ 
discusses the concept and how R resolves this).

The best answer I know of is path.expand("~").

On Fri, 23 Mar 2007, Henrik Andersson wrote:

> Sys.getenv("HOME") works in Linux at least

But not all Unix shells have it set, and a user can unset or reset it.  To 
be perverse:

gannet% unsetenv HOME
gannet% R --slave
Sys.getenv("HOME")
HOME
   ""
path.expand("~")
[1] "/data/gannet/ripley"
q()

(If you set HOME to something other than your home directory it will be 
honoured.)

>
> - Henrik
>
> Andris Jankevics wrote:
>> If you want get a username of user currently running R on Linux,you can use a
>> system command and read enviroment variables:
>>
>> paste("/home/",system ("whoami",intern=TRUE),sep="")

And my home directory is not '/home/ripley' on any of the Linux boxes I 
use (even though it exists on some of them), e.g. on my compute server

> path.expand("~")
[1] "/data/gannet/ripley"

and on our main cluster

> path.expand("~")
[1] "/home/markov/ripley"


>> Andris Jankevics
>>
>> On Piektdiena, 23. Marts 2007 14:30, Alberto Monteiro wrote:
>>
>>> Is there any generic function that gets the "home" directory? This
>>> should return /home/<user> in Linux and
>>> x:/Documents and Settings/<user> (or whatever) in Windows XP.

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Fabian.Mollet at wur.nl  Fri Mar 23 15:36:50 2007
From: Fabian.Mollet at wur.nl (Mollet, Fabian)
Date: Fri, 23 Mar 2007 15:36:50 +0100
Subject: [R] simulate from a multivariate lognormal distribution with fixed
	covariance/correlation structure
Message-ID: <2DF51316D2ACEF4891856424912C454E427A0B@scomp0040.wurnet.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070323/1f9e630d/attachment.pl 

From ggrothendieck at gmail.com  Fri Mar 23 15:41:15 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 23 Mar 2007 10:41:15 -0400
Subject: [R] Get "home" directory and simple I/O
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03EB25AF@usctmx1106.merck.com>
References: <20070323122543.M34818@centroin.com.br>
	<971536df0703230651q469579desc7e278462724494@mail.gmail.com>
	<39B6DDB9048D0F4DAD42CB26AAFF0AFA03EB25AF@usctmx1106.merck.com>
Message-ID: <971536df0703230741r57494cecvd4103e102deb93f7@mail.gmail.com>

On 3/23/07, Liaw, Andy <andy_liaw at merck.com> wrote:
> From: Gabor Grothendieck
> >
> > See:
> >
> > ?R.home
>
> That's not what Alberto wanted:  It gives the location of the R
> installation, not where user's home directory is.  AFAIK Windows does
> not set the HOME environment variable by default.

ok.  Try this, at least on Windows XP:

paste(Sys.getenv(c("HOMEDRIVE", "HOMEPATH")), collapse = "")


From ggrothendieck at gmail.com  Fri Mar 23 15:44:57 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 23 Mar 2007 10:44:57 -0400
Subject: [R] Get "home" directory and simple I/O
In-Reply-To: <Pine.LNX.4.64.0703231405580.31231@gannet.stats.ox.ac.uk>
References: <20070323122543.M34818@centroin.com.br>
	<200703231459.26404.andza@osi.lv> <4603D3C8.8020903@dmi.dk>
	<Pine.LNX.4.64.0703231405580.31231@gannet.stats.ox.ac.uk>
Message-ID: <971536df0703230744q7d831e01hda1487e29f80fc53@mail.gmail.com>

path.expand("~") also seems to work on Windows XP.

On 3/23/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> But the request was for a *generic* solution.  On Windows there
> might not be anything corresponding to a home directory (and the rw-FAQ
> discusses the concept and how R resolves this).
>
> The best answer I know of is path.expand("~").
>
> On Fri, 23 Mar 2007, Henrik Andersson wrote:
>
> > Sys.getenv("HOME") works in Linux at least
>
> But not all Unix shells have it set, and a user can unset or reset it.  To
> be perverse:
>
> gannet% unsetenv HOME
> gannet% R --slave
> Sys.getenv("HOME")
> HOME
>   ""
> path.expand("~")
> [1] "/data/gannet/ripley"
> q()
>
> (If you set HOME to something other than your home directory it will be
> honoured.)
>
> >
> > - Henrik
> >
> > Andris Jankevics wrote:
> >> If you want get a username of user currently running R on Linux,you can use a
> >> system command and read enviroment variables:
> >>
> >> paste("/home/",system ("whoami",intern=TRUE),sep="")
>
> And my home directory is not '/home/ripley' on any of the Linux boxes I
> use (even though it exists on some of them), e.g. on my compute server
>
> > path.expand("~")
> [1] "/data/gannet/ripley"
>
> and on our main cluster
>
> > path.expand("~")
> [1] "/home/markov/ripley"
>
>
> >> Andris Jankevics
> >>
> >> On Piektdiena, 23. Marts 2007 14:30, Alberto Monteiro wrote:
> >>
> >>> Is there any generic function that gets the "home" directory? This
> >>> should return /home/<user> in Linux and
> >>> x:/Documents and Settings/<user> (or whatever) in Windows XP.
>
> [...]
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From albmont at centroin.com.br  Fri Mar 23 16:06:04 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 23 Mar 2007 13:06:04 -0200
Subject: [R] Get "home" directory and simple I/O
In-Reply-To: <Pine.LNX.4.64.0703231405580.31231@gannet.stats.ox.ac.uk>
References: <20070323122543.M34818@centroin.com.br>
	<200703231459.26404.andza@osi.lv> <4603D3C8.8020903@dmi.dk>
	<Pine.LNX.4.64.0703231405580.31231@gannet.stats.ox.ac.uk>
Message-ID: <20070323150241.M27251@centroin.com.br>

Prof Brian Ripley wrote:
>
> But the request was for a *generic* solution.  On Windows there
> might not be anything corresponding to a home directory
> (and the rw-FAQ discusses the concept and how R resolves this).
> 
> The best answer I know of is path.expand("~").
> 
Thanks, this is the only solution that works for Windows XP,
and it will probably work for Linux.

Also, in Windows, there are variables homedrive and homepath,
that I could combine to form the path (probably this is what
path.expand does :-))

Sys.getenv("homepath")
Sys.getenv("homedrive")

Alberto Monteiro


From sergio.della.franca at gmail.com  Fri Mar 23 16:30:57 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Fri, 23 Mar 2007 16:30:57 +0100
Subject: [R] Logistic Regression
Message-ID: <b490ce570703230830s561316d5tdcda0827b31b8bec@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070323/f0d9228d/attachment.pl 

From jfox at mcmaster.ca  Fri Mar 23 16:43:03 2007
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 23 Mar 2007 11:43:03 -0400
Subject: [R] Effect display of proportional odds model
In-Reply-To: <005301c76d3f$6b778270$2c70210a@UCSPC32>
Message-ID: <20070323154304.IEIK1637.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Jan,

I've placed a copy of the published version of the paper at
<http://socserv.socsci.mcmaster.ca/jfox/Misc/polytomous-effect-displays/inde
x.html>, along with the R code for computing effects and their standard
errors and R code for the graphs in the paper. As you'll see, the functions
provided do not construct graphs automatically, as those in the effects
package do for linear and generalized linear models. Eventually, I'd like to
incorporate this material in the effects package.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jan Wijffels
> Sent: Friday, March 23, 2007 7:36 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Effect display of proportional odds model
> 
> Dear useRs,
> I very much like the effect display of the proportional odds 
> model on page 29 (Figure 8) of the following paper by John Fox:
> http://socserv.mcmaster.ca/jfox/Papers/logit-effect-displays.pdf
> It really gives a very concise overview of the model. I would 
> like to use it to illustrate the proportional odds mixed 
> models we fit here for a project on Diabetes but I can't seem 
> to reproduce the plot. Does anyone have code for the plot? 
> Maybe John Fox himself? I would appreciate it very much.
> Thanks,
> Jan
>  
> Jan Wijffels
> University Center for Statistics
> W. de Croylaan 54
> 3001 Heverlee
> Belgium
> tel: +32 (0)16 322784
> fax: +32 (0)16 322831
>  <http://www.kuleuven.be/ucs> http://www.kuleuven.be/ucs
>  
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From qyuan at mail.nih.gov  Fri Mar 23 16:46:37 2007
From: qyuan at mail.nih.gov (Yuan, Qiaoping (NIH/NIAAA) [E])
Date: Fri, 23 Mar 2007 11:46:37 -0400
Subject: [R] subtotal for same row data
Message-ID: <7E72CD7F4C0A994191ED9B3726FAEBF302739A75@NIHCESMLBX4.nih.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070323/bea034d6/attachment.pl 

From Jan.Wijffels at ucs.kuleuven.be  Fri Mar 23 16:57:11 2007
From: Jan.Wijffels at ucs.kuleuven.be (Jan Wijffels)
Date: Fri, 23 Mar 2007 16:57:11 +0100
Subject: [R] Effect display of proportional odds model
In-Reply-To: <20070323154304.IEIK1637.tomts20-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <000001c76d63$eaec67e0$2c70210a@UCSPC32>

Great! That was all I needed!

Best regards,
Jan

Jan Wijffels
University Center for Statistics 
W. de Croylaan 54
3001 Heverlee
Belgium
tel: +32 (0)16 322784
fax: +32 (0)16 322831
http://www.kuleuven.be/ucs

-----Original Message-----
From: John Fox [mailto:jfox at mcmaster.ca] 
Sent: vrijdag 23 maart 2007 16:43
To: 'Jan Wijffels'
Cc: r-help at stat.math.ethz.ch; 'Jeff Miller'
Subject: RE: [R] Effect display of proportional odds model

Dear Jan,

I've placed a copy of the published version of the paper at
<http://socserv.socsci.mcmaster.ca/jfox/Misc/polytomous-effect-displays/
inde
x.html>, along with the R code for computing effects and their standard
errors and R code for the graphs in the paper. As you'll see, the
functions
provided do not construct graphs automatically, as those in the effects
package do for linear and generalized linear models. Eventually, I'd
like to
incorporate this material in the effects package.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jan Wijffels
> Sent: Friday, March 23, 2007 7:36 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Effect display of proportional odds model
> 
> Dear useRs,
> I very much like the effect display of the proportional odds 
> model on page 29 (Figure 8) of the following paper by John Fox:
> http://socserv.mcmaster.ca/jfox/Papers/logit-effect-displays.pdf
> It really gives a very concise overview of the model. I would 
> like to use it to illustrate the proportional odds mixed 
> models we fit here for a project on Diabetes but I can't seem 
> to reproduce the plot. Does anyone have code for the plot? 
> Maybe John Fox himself? I would appreciate it very much.
> Thanks,
> Jan
>  
> Jan Wijffels
> University Center for Statistics
> W. de Croylaan 54
> 3001 Heverlee
> Belgium
> tel: +32 (0)16 322784
> fax: +32 (0)16 322831
>  <http://www.kuleuven.be/ucs> http://www.kuleuven.be/ucs
>  
> 
> 
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From qyuan at mail.nih.gov  Fri Mar 23 17:01:20 2007
From: qyuan at mail.nih.gov (Yuan, Qiaoping (NIH/NIAAA) [E])
Date: Fri, 23 Mar 2007 12:01:20 -0400
Subject: [R] subtotal for same row data
Message-ID: <7E72CD7F4C0A994191ED9B3726FAEBF302739A78@NIHCESMLBX4.nih.gov>

Hi, There,

I would like to subtotal the number in a specified column for all rows having the same data for specified columns. The following is the simple example:


> x=matrix(c(1,2,2,0.3,2,2,2,0.5,1,2,1,0.2),3,4,byrow=T)
> rownames(x)=c("R1","R2","R3")
> colnames(x)=c("C1","C2","C3","F")
> x
?? C1 C2 C3?? F
R1? 1? 2? 2 0.3
R2? 2? 2? 2 0.5
R3? 1? 2? 1 0.2

I would like to get the subtotal in column "F" based on same row data in column "C1" and "C2". The result should be like

C1   C2     SumF
1??? 2??? 0.5????????? # This is 0.3 + 0.2 from R1 and R3
2??? 2??? 0.5

Is there a simple way to do this? Any help will be greatly appreciated.

Qiaoping Yuan


From jerosenb at fas.harvard.edu  Fri Mar 23 17:26:25 2007
From: jerosenb at fas.harvard.edu (Janet)
Date: Fri, 23 Mar 2007 12:26:25 -0400
Subject: [R] Simple bar plot question
Message-ID: <6722F037-A9CC-4B14-9A0A-16F527F75B2C@fas.harvard.edu>


Dear all,

This is a simple question, but as far as I can tell, it's not in MASS  
or the R help archives or par man pages.

Are there any cex.* parameters which allow me to set the text size of  
the labels for the axes?

I want to plot the following table in which the "Definitely not",  
etc. are large.  cex.lab lets me change the size of the xlab text,  
but I don't care whether xlab is large.

 > par.prob.oralsex
                            			Definitely not    Probably not Yes,  
probably	Yes, definitely
child hasn't had oral sex       0.6    0.2   0.1	      0.1
child has had oral sex          0.5    0.3  0.1         0.1


barplot(par.prob.oralsex, beside=T, ylab="Proportion parents", ylim=c 
(0,1), legend.text=T, horiz=F, col=c("red", "blue"), space=c(0,4),  
xlab="Parent response", cex.axis=1.3, cex.lab=1.3, cex.main=1.3,  
cex.sub=1.3)

Thanks,

Janet


From f.calboli at imperial.ac.uk  Fri Mar 23 17:25:28 2007
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Fri, 23 Mar 2007 16:25:28 +0000
Subject: [R] plotting symbol
Message-ID: <298B2353-66D3-41E6-9BA7-99D98244ED53@imperial.ac.uk>

Hi All,

can I have a plot where the symbol for the dots is smaller than pch  
=20 but bigger than pch = '.'?

Best,

Fede

--
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St. Mary's Campus
Norfolk Place, London W2 1PG

Tel +44 (0)20 75941602   Fax +44 (0)20 75943193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com


From jerosenb at fas.harvard.edu  Fri Mar 23 17:28:03 2007
From: jerosenb at fas.harvard.edu (Janet)
Date: Fri, 23 Mar 2007 12:28:03 -0400
Subject: [R] Simple bar plot question
Message-ID: <F7BBDF98-B43B-423D-9433-7819182731E8@fas.harvard.edu>

Dear all,

This is a simple question, but as far as I can tell, it's not in MASS  
or the R help archives or par man pages.

Are there any cex.* parameters which allow me to set the text size of  
the labels for the axes and the size of the legend?

I want to plot the following table in which the "Definitely not",  
etc. are large.  cex.lab lets me change the size of the xlab text,  
but I don't care whether xlab is large.  I also want the "Child  
hasn't had oral sex", etc. in the legend to be large.

 > par.prob.oralsex
                            			Definitely not    Probably not Yes,  
probably	Yes, definitely
child hasn't had oral sex       0.6    0.2   0.1	      0.1
child has had oral sex          0.5    0.3  0.1         0.1


barplot(par.prob.oralsex, beside=T, ylab="Proportion parents", ylim=c 
(0,1), legend.text=T, horiz=F, col=c("red", "blue"), space=c(0,4),  
xlab="Parent response", cex.axis=1.3, cex.lab=1.3, cex.main=1.3,  
cex.sub=1.3)

Thanks,

Janet


From sergio.della.franca at gmail.com  Fri Mar 23 17:27:33 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Fri, 23 Mar 2007 17:27:33 +0100
Subject: [R] Subset
Message-ID: <b490ce570703230927r4e19b1b0o83fc170402a334e7@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070323/be84ac14/attachment.pl 

From ccleland at optonline.net  Fri Mar 23 17:34:37 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 23 Mar 2007 12:34:37 -0400
Subject: [R] plotting symbol
In-Reply-To: <298B2353-66D3-41E6-9BA7-99D98244ED53@imperial.ac.uk>
References: <298B2353-66D3-41E6-9BA7-99D98244ED53@imperial.ac.uk>
Message-ID: <4604019D.7040007@optonline.net>

Federico Calboli wrote:
> Hi All,
> 
> can I have a plot where the symbol for the dots is smaller than pch  
> =20 but bigger than pch = '.'?

  Have you considered using the cex argument to reduce the size of pch=20?

X <- rnorm(20)

par(mfrow=c(2,2))
plot(X, pch=20, cex=1.0)
plot(X, pch=20, cex=0.6)
plot(X, pch=20, cex=1.5)
plot(X, pch=".")

> Best,
> 
> Fede
> 
> --
> Federico C. F. Calboli
> Department of Epidemiology and Public Health
> Imperial College, St. Mary's Campus
> Norfolk Place, London W2 1PG
> 
> Tel +44 (0)20 75941602   Fax +44 (0)20 75943193
> 
> f.calboli [.a.t] imperial.ac.uk
> f.calboli [.a.t] gmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code. 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From jerosenb at fas.harvard.edu  Fri Mar 23 17:38:11 2007
From: jerosenb at fas.harvard.edu (Janet)
Date: Fri, 23 Mar 2007 12:38:11 -0400
Subject: [R] simple bar plot question
Message-ID: <25589179-F6B3-4193-A27D-C82A98B2DA32@fas.harvard.edu>

Literally 60 seconds after I sent my question, I found the cex.names  
parameter to barplot.
I haven't found a parameter for the size of the text in the legend.

Apologies for clogging inboxes semi-unnecessarily.

Janet


From Cody_Hamilton at Edwards.com  Fri Mar 23 17:41:04 2007
From: Cody_Hamilton at Edwards.com (Cody_Hamilton at Edwards.com)
Date: Fri, 23 Mar 2007 09:41:04 -0700
Subject: [R] R and clinical studies
In-Reply-To: <47fce0650703230931g4086e25o5f056b6e62f72316@mail.gmail.com>
Message-ID: <OFCDC9CD45.E0B61629-ON882572A7.005B8C8A-882572A7.005B6D6D@irvine.edwards.com>


Thanks for the tip.  I will look forward to trying this package out soon!

Regards, -Cody



                                                                           
             Hans-Peter                                                    
             <gchappi at gmail.co                                             
             m>                                                         To 
                                       "Cody_Hamilton at edwards.com"         
             03/23/2007 09:31          <Cody_Hamilton at edwards.com>         
             AM                                                         cc 
                                                                           
                                                                   Subject 
                                       Re: [R] R and clinical studies      
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Hi,

2007/3/20, Cody_Hamilton at edwards.com <Cody_Hamilton at edwards.com>:

> and (2) SAS seems to play nicer with MS products (e.g. PROC IMPORT seemed
> to read in messy Excel spreadsheets better than importData in Splus).

(to pick one small detail from your post)

You can use my xlsReadWrite package which will (on windows) read and
write Excel data (-> see CRAN, a new version is pending). While there
is a pro version also, in lot of circumstances the free version is
perfectly fine.


--
Regards,
Hans-Peter


From p.dalgaard at biostat.ku.dk  Fri Mar 23 17:40:16 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 23 Mar 2007 17:40:16 +0100
Subject: [R] Get "home" directory and simple I/O
In-Reply-To: <200703231459.26404.andza@osi.lv>
References: <20070323122543.M34818@centroin.com.br>
	<200703231459.26404.andza@osi.lv>
Message-ID: <460402F0.7040605@biostat.ku.dk>

Andris Jankevics wrote:
> If you want get a username of user currently running R on Linux,you can use a 
> system command and read enviroment variables:
>
> paste("/home/",system ("whoami",intern=TRUE),sep="")
>
>   
This would be more to the point (and safer), I think.

 > Sys.getenv("HOME")
HOME
"/home/bs/pd"


> Andris Jankevics
>
> On Piektdiena, 23. Marts 2007 14:30, Alberto Monteiro wrote:
>   
>> Is there any generic function that gets the "home" directory? This
>> should return /home/<user> in Linux and
>> x:/Documents and Settings/<user> (or whatever) in Windows XP.
>>
>> Another (unrelated) question: what is the _simplest_ way to
>> read and write R variables to/from files such that they are
>> stored in a human-readable but R-like form? For example, if
>> (say), x is a vector defined as x <- c(1, 2, 3), can I write
>> (and read) x as a file with just one line, namely: c(1, 2, 3) ?
>>
>> Alberto Monteiro
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html and provide commented, minimal,
>> self-contained, reproducible code.
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gchappi at gmail.com  Fri Mar 23 17:44:17 2007
From: gchappi at gmail.com (Hans-Peter)
Date: Fri, 23 Mar 2007 17:44:17 +0100
Subject: [R] Updating a worksheet in Excel file using RODBC
In-Reply-To: <B16F2EFF2E11604E93044813DD4413598A591D@warp8.chimaera.local>
References: <B16F2EFF2E11604E93044813DD4413598A591D@warp8.chimaera.local>
Message-ID: <47fce0650703230944s439aed4dnc75d7738a564a1da@mail.gmail.com>

Hi,

2007/3/23, Moshe Olshansky <molshansky at chimaeracapital.com>:
> Hello!
>
>I have no problem reading Excel files (each worksheet in the file is
a >"table" which can be read - at least in my case).
>What I would like to do is to read such a table, change it (just the
>contents, not the format) and write it back, and this I can not do.
I am >getting the following error messages (3 slightly different
attempts):

> [snip]

As another option (if you work with Windows) you can check my
"xlsReadWrite" package (-> CRAN).

It should work very well in your case (it's not suited if you want to
use SQL (join) statements, but for plain data reading/writing it is
nice).

For both versions (free/pro) updates are pending. They should be
released by end of next week (but no guarantees).

-- 
Regards,
Hans-Peter


From gerifalte28 at hotmail.com  Fri Mar 23 17:56:01 2007
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 23 Mar 2007 10:56:01 -0600
Subject: [R] generating lognormal variables with given correlation
In-Reply-To: <200703231527.30624.Karl.Hufthammer@math.uib.no>
References: <200703231527.30624.Karl.Hufthammer@math.uib.no>
Message-ID: <460406A1.8070403@hotmail.com>

This reference may be relevant for you: Connover, W.J., Iman, R.L. A 
distribution-free approach to inducing rank correlation among input 
variables. Technometric, 3, 311-334, 1982.

Also, you may want to look at a more modern approach implemented in the 
copula package:
install.packages("copula")
library(help="copula")

I hope this helps,

Francisco,


Karl Ove Hufthammer wrote:
> Mollet, Fabian:
> 
>> I would like these (lognormal distributed) parameters to be correlated,
>> that is, I would like to have pairwise samples of 2 parameters with a
>> given correlation coefficient.
>>  
>> I have seen that a covariance matrix can be fixed when generating random
>> variables from a multivariate normal distribution e.g. by the function
>> mvrnorm.
>>  
>> Is there a function to do the same for a multivariate lognormal
>> distribution?
> 
> I don't know about any, but you should be aware that not all values of the 
> correlation is possible with lognormal distributions. For example, if both 
> variables have a standard lognormal distribution, they can't have correlation 
> less than 1/e = -0.37. As the variance of the two distributions increase, the 
> absolute value of the maximum and minimum correlation possible decrease (to 
> zero).
> 
> Using the normal product-moment correlation as a measure of dependence rarely 
> makes much sense unless the association between the variables is linear.
>


From marc_schwartz at comcast.net  Fri Mar 23 17:55:40 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 23 Mar 2007 11:55:40 -0500
Subject: [R] distribution graph
In-Reply-To: <XFMail.070323130455.ted.harding@nessie.mcc.ac.uk>
References: <XFMail.070323130455.ted.harding@nessie.mcc.ac.uk>
Message-ID: <1174668940.5117.11.camel@Bellerophon>

On Fri, 2007-03-23 at 14:22 +0000, ted.harding at nessie.mcc.ac.uk wrote:
> [Apologies -- there were errors in the code I posted previously.
>  A corrected version is below]
> 
> On 23-Mar-07 11:06:49, Plessen, Christian von wrote:
> > 
> > I am looking for a way to produce a "distribution graph" as in the
> > example: 
> > 
> > (http://cecsweb.dartmouth.edu/release1.1/datatools/dgraph.php?year=2003&
> > geotype=STD_HRR&event=A01_DIS&eventtype=UTIL
> > 
> > Anybody who can help?
> > 
> 
> The following (which anyway needs refinement, and can very
> probably be done better) provides a basis (illustrated using
> a sample from a log-normal distribution):
> 
> 
> X<-exp(rnorm(200,sd=0.25)+2)/5
> 
> H<-hist(X,breaks=20)
> C<-H$counts
> Y<-H$mids
> C1<-C/2
> 
> C0<-(-(C1[1]-1/2)):(C1[1]-1/2); n0<-length(C0)
> plot(C0,rep(Y[1],n0),xlim=c(-max(C)/2,max(C)/2),ylim=c(min(Y),max(Y)))
> 
> for(i in (2:length(Y))){
>   if(C[i]==0) next
>   C0 <- (-(C1[i] - 1/2)):(C1[i] - 1/2); n0<-length(C0)
>   points(C0,rep(Y[i],n0))
> }
> 
> 
> Hoping this helps!
> Ted.


How about something like this:


DistPlot <- function(x, digits = 1, ...)
{
  x <- round(x, digits)
  
  Tab <- table(x)

  Vals <- sapply(Tab, function(x) seq(x) - mean(seq(x)))

  X.Vals <- unlist(Vals, use.names = FALSE)
  tmp <- sapply(Vals, length)
  Y.Vals <- rep(names(tmp), tmp)

  plot(X.Vals, Y.Vals, ...)
}


Vec <- exp(rnorm(200, sd = 0.25) + 2) / 5

DistPlot(Vec, pch = 19)



HTH,

Marc Schwartz


From verkerk at candiensten.nl  Fri Mar 23 17:58:00 2007
From: verkerk at candiensten.nl (Dick Verkerk)
Date: Fri, 23 Mar 2007 17:58:00 +0100
Subject: [R] Course on Survival Analysis in Amsterdam
Message-ID: <46040718.6000308@candiensten.nl>


Survival Analysis
By Dr. Ronald Geskus
May 24-25, 2007
Amsterdam, The Netherlands

http://www.can.nl/events/details.php?id=30


This course is aimed at everyone who wants to analyze data in which
the time to the occurrence of some event, and its dependence on
covariates, is of interest.

Survival analysis consists of a collection of techniques for the
analysis of time-to-event data. Part of the relevant time window
may be unobserved. In the most frequently encountered situation,
there is one event of interest, and the event time is either
observed exactly or right censored. Programs for the analysis of
right censored data have become readily available.
Emphasis of the course is on the practical application of models
for survival data. Participants will be made familiar with the
possibilities offered through computer exercises.


You will learn how to:
? perform survival analyses and present the results graphically
? analyze the influence of covariates on the event time distribution
? model linear- as well as nonlinear effects of covariates
? evaluate model fit via residuals
? analyze interval censored data
? perform competing risks analyses
? analyze correlated event data

Experience with survival analysis is not assumed. Knowledge of R or
S-PLUS at the level of the introductory course is advised.

Location: Amsterdam
Date : December 15-16
Time : 10:00h.-16:30h.
Price : EURO 790,- excl. VAT, Includes lunch and course materials

Register:
- phone : +31-(0)20-560-8400
- Email : altman at can.nl
- Web : http://www.can.nl/events/details.php?id=30

There is a maximum of 12 participants.

You may register by replying to this email and provide us with the
following information.

Name : M / F
Title :
Department :
Institute :
Address :
City :
Zip :
Telephone :
Fax :
Email :

Please let us know if you have any questions.

Please feel free to send this message on to your colleagues and friends
for whom it might be interesting!!

Kind regards,
Dick Verkerk
CANdiensten


_________________________

Dick Verkerk, managing director
CANdiensten, Nieuwpoortkade 23-25, NL-1055 RX Amsterdam
tel: +31 20 5608410 fax: +31 20 5608448 verkerk at candiensten.nl
_________________________
Your Partner in Mathematics and Statistics!


From gerifalte28 at hotmail.com  Fri Mar 23 17:56:01 2007
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 23 Mar 2007 10:56:01 -0600
Subject: [R] generating lognormal variables with given correlation
In-Reply-To: <200703231527.30624.Karl.Hufthammer@math.uib.no>
References: <200703231527.30624.Karl.Hufthammer@math.uib.no>
Message-ID: <460406A1.8070403@hotmail.com>

This reference may be relevant for you: Connover, W.J., Iman, R.L. A 
distribution-free approach to inducing rank correlation among input 
variables. Technometric, 3, 311-334, 1982.

Also, you may want to look at a more modern approach implemented in the 
copula package:
install.packages("copula")
library(help="copula")

I hope this helps,

Francisco,


Karl Ove Hufthammer wrote:
> Mollet, Fabian:
> 
>> I would like these (lognormal distributed) parameters to be correlated,
>> that is, I would like to have pairwise samples of 2 parameters with a
>> given correlation coefficient.
>>  
>> I have seen that a covariance matrix can be fixed when generating random
>> variables from a multivariate normal distribution e.g. by the function
>> mvrnorm.
>>  
>> Is there a function to do the same for a multivariate lognormal
>> distribution?
> 
> I don't know about any, but you should be aware that not all values of the 
> correlation is possible with lognormal distributions. For example, if both 
> variables have a standard lognormal distribution, they can't have correlation 
> less than 1/e = -0.37. As the variance of the two distributions increase, the 
> absolute value of the maximum and minimum correlation possible decrease (to 
> zero).
> 
> Using the normal product-moment correlation as a measure of dependence rarely 
> makes much sense unless the association between the variables is linear.
>


From p.dalgaard at biostat.ku.dk  Fri Mar 23 17:58:44 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 23 Mar 2007 17:58:44 +0100
Subject: [R] plotting symbol
In-Reply-To: <4604019D.7040007@optonline.net>
References: <298B2353-66D3-41E6-9BA7-99D98244ED53@imperial.ac.uk>
	<4604019D.7040007@optonline.net>
Message-ID: <46040744.5070609@biostat.ku.dk>

Chuck Cleland wrote:
> Federico Calboli wrote:
>   
>> Hi All,
>>
>> can I have a plot where the symbol for the dots is smaller than pch  
>> =20 but bigger than pch = '.'?
>>     
>
>   Have you considered using the cex argument to reduce the size of pch=20?
>
> X <- rnorm(20)
>
> par(mfrow=c(2,2))
> plot(X, pch=20, cex=1.0)
> plot(X, pch=20, cex=0.6)
> plot(X, pch=20, cex=1.5)
> plot(X, pch=".")
>
>   
Notice though, that it depends on the device. On X11 (presumably Windows 
too) at "normal" resolutions, you really have only one sice which is 
less than the default. Try e.g.

plot(1:10, rep(0,10), pch=20,cex=seq(1,.1,,10))


>> Best,
>>
>> Fede
>>
>> --
>> Federico C. F. Calboli
>> Department of Epidemiology and Public Health
>> Imperial College, St. Mary's Campus
>> Norfolk Place, London W2 1PG
>>
>> Tel +44 (0)20 75941602   Fax +44 (0)20 75943193
>>
>> f.calboli [.a.t] imperial.ac.uk
>> f.calboli [.a.t] gmail.com
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code. 
>>     
>
>


From mark at wardle.org  Fri Mar 23 18:00:23 2007
From: mark at wardle.org (Mark Wardle)
Date: Fri, 23 Mar 2007 17:00:23 +0000
Subject: [R] Subset
In-Reply-To: <b490ce570703230927r4e19b1b0o83fc170402a334e7@mail.gmail.com>
References: <b490ce570703230927r4e19b1b0o83fc170402a334e7@mail.gmail.com>
Message-ID: <460407A7.2000406@wardle.org>

Sergio Della Franca wrote:
> Dear R-Helpers,
> 
> I have this dataset:
> 
>    YEAR    PRODUCTS cluster
>     1          10          2
>     2          42          3
>     3          25          2
>     4          42          3
>     5          40          3
>     6          45          1
>     7          44          1
>     8          47          1
>     9          42          1
> 
> 
> I want to create a subset (when cluster=1),
> 
>     YEAR    PRODUCTS cluster
>     6          45          1
>     7          44          1
>     8          47          1
>     9          42          1
> 
> 
> How can i perform this?
> 
> 

You've answered your own question!

?subset


e.g., subset(my.data, cluster==1)

You should also notice that many R functions support a subset=
parameter, which is very useful!

Mark

-- 
Dr. Mark Wardle
Specialist registrar, Neurology
Cardiff, UK


From verkerk at candiensten.nl  Fri Mar 23 18:11:46 2007
From: verkerk at candiensten.nl (Dick Verkerk)
Date: Fri, 23 Mar 2007 18:11:46 +0100
Subject: [R] course on Analysis of Repeated Measurements  by Ronald Geskus
Message-ID: <46040A52.7030200@candiensten.nl>


   Analysis of Repeated Measurements
   By Dr. Ronald Geskus
   April 27, 2007
   Amsterdam, The Netherlands

   http://www.can.nl/events/details.php?id=31

   This course is aimed at everyone who is working with data that contain
   repeated measurements on persons or otherwise related data and who wants
   to analyse such data in a proper way.

   In many situations one may have data that show some form of dependence.
   Longitudinal data occur when some outcome variable is measured on
   experimental units at several points in time, often with the aim to study
   development over time. Dependence also occurs when data have been
   collected within units, like the performance score of students within
   different schools. For the analysis, special statistical methods are
   required that take account of the correlation of observations within the
   same unit. For normally distributed variables, the mixed-effects model is
   the standard tool for analysis. In S-PLUS and R, the NLME library
   introduced the "groupedData" class, which will be used as the basic
   structure for the analysis of linear models with repeated measurements
   (linear mixed effects). Together with the extensive possibilities for
   graphical exploration in S-PLUS and R, a flexible tool for the analysis of
   longitudinal data and the evaluation of the model fits are available.
   Programs for the analysis of discrete and non-normally distributed
   variables will also be covered briefly. Emphasis of the course is on the
   practical application of models for longitudinal data. Participants will
   be made familiar with the possibilities offered by S-PLUS and R through
   computer exercises.


   you will learn how to
   * plot individual patterns of repeated measurements in a flexible way
   * perform analyses for repeated measurements and show results graphically
   * evaluate model fit, numerically as well as graphically


   Experience with the analysis of repeated measurements is not assumed.
   Knowledge of R or S-PLUS at the level of the introductory course is advised.


   Location:  Amsterdam
   Date    :  April 27
   Time    :  10:00h.-17:00h.
   Price   :  EURO 395,- excl. VAT, Includes lunch and course materials

   Register:
   - phone : +31-(0)20-560-8400
   - Email : altman at can.nl
   - Web   : http://www.can.nl/events/details.php?id=31

   There is a maximum of 12 participants.

   You may register by replying to this email and provide us with the
   following information.

   Name       :                      M / F
   Title      :
   Department :
   Institute  :
   Address    :
   City       :
   Zip        :
   Telephone  :
   Fax        :
   Email      :

   Please let us know if you have any questions.

   Please feel free to send this message on to your colleagues and friends
   for whom it might be interesting!!

   Kind regards,
   Dick Verkerk
   CANdiensten


   _________________________

   Dick Verkerk, managing director
   CANdiensten, Nieuwpoortkade 23-25, NL-1055 RX Amsterdam
   tel: +31 20 5608410 fax: +31 20 5608448 verkerk at candiensten.nl
   _________________________
   Your Partner in Mathematics and Statistics!


From ligges at statistik.uni-dortmund.de  Fri Mar 23 18:11:56 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 23 Mar 2007 18:11:56 +0100
Subject: [R] Subset
In-Reply-To: <b490ce570703230927r4e19b1b0o83fc170402a334e7@mail.gmail.com>
References: <b490ce570703230927r4e19b1b0o83fc170402a334e7@mail.gmail.com>
Message-ID: <46040A5C.5030107@statistik.uni-dortmund.de>

See ?subset !!!
And please do read the psoting guide.

Uwe Ligges


Sergio Della Franca wrote:
> Dear R-Helpers,
> 
> I have this dataset:
> 
>    YEAR    PRODUCTS cluster
>     1          10          2
>     2          42          3
>     3          25          2
>     4          42          3
>     5          40          3
>     6          45          1
>     7          44          1
>     8          47          1
>     9          42          1
> 
> 
> I want to create a subset (when cluster=1),
> 
>     YEAR    PRODUCTS cluster
>     6          45          1
>     7          44          1
>     8          47          1
>     9          42          1
> 
> 
> How can i perform this?
> 
> 
> Thank you in advance.
> 
> 
> Sergio Della Franca
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From davidr at rhotrading.com  Fri Mar 23 18:23:51 2007
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Fri, 23 Mar 2007 12:23:51 -0500
Subject: [R] R at the Language Log
Message-ID: <F9F2A641C593D7408925574C05A7BE77276A52@rhopost.rhotrading.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070323/cd1a03ab/attachment.pl 

From hamstersquats at web.de  Fri Mar 23 18:29:03 2007
From: hamstersquats at web.de (Thomas Kaliwe)
Date: Fri, 23 Mar 2007 18:29:03 +0100
Subject: [R] using "\t" in mtext
Message-ID: <000001c76d70$c057c210$0401a8c0@mouse>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070323/972b4d8f/attachment.pl 

From hodgess at gator.dt.uh.edu  Fri Mar 23 18:33:58 2007
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Fri, 23 Mar 2007 12:33:58 -0500
Subject: [R]  creating R packs for all
Message-ID: <200703231733.l2NHXw6E024566@gator.dt.uh.edu>

Dear R People:

I am in the process of creating an R package via Windows.

If I would decide to submit in to CRAN, what would I need to
do in order to make it run for the Linux or Mac People, please?

Thanks in advance!

Sincerely,
Erin
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu


From ccleland at optonline.net  Fri Mar 23 18:34:53 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 23 Mar 2007 13:34:53 -0400
Subject: [R] plotting symbol
In-Reply-To: <46040744.5070609@biostat.ku.dk>
References: <298B2353-66D3-41E6-9BA7-99D98244ED53@imperial.ac.uk>
	<4604019D.7040007@optonline.net> <46040744.5070609@biostat.ku.dk>
Message-ID: <46040FBD.8090605@optonline.net>

Peter Dalgaard wrote:
> Chuck Cleland wrote:
>> Federico Calboli wrote:
>>  
>>> Hi All,
>>>
>>> can I have a plot where the symbol for the dots is smaller than pch 
>>> =20 but bigger than pch = '.'?
>>>     
>>
>>   Have you considered using the cex argument to reduce the size of
>> pch=20?
>>
>> X <- rnorm(20)
>>
>> par(mfrow=c(2,2))
>> plot(X, pch=20, cex=1.0)
>> plot(X, pch=20, cex=0.6)
>> plot(X, pch=20, cex=1.5)
>> plot(X, pch=".")
>>
>>   
> Notice though, that it depends on the device. On X11 (presumably Windows
> too) at "normal" resolutions, you really have only one sice which is
> less than the default. Try e.g.
> 
> plot(1:10, rep(0,10), pch=20,cex=seq(1,.1,,10))

Peter:
  Thanks for making that point, which certainly did not occur to me.  I
see the same thing with the windows device with pointsize at the default
of 12 (win.graph too).  With pointsize=15 I can get two sizes smaller
than the default.  And on the pdf device I get many sizes less than the
default.

>>> Best,
>>>
>>> Fede
>>>
>>> -- 
>>> Federico C. F. Calboli
>>> Department of Epidemiology and Public Health
>>> Imperial College, St. Mary's Campus
>>> Norfolk Place, London W2 1PG
>>>
>>> Tel +44 (0)20 75941602   Fax +44 (0)20 75943193
>>>
>>> f.calboli [.a.t] imperial.ac.uk
>>> f.calboli [.a.t] gmail.com
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.     

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From marc_schwartz at comcast.net  Fri Mar 23 18:52:26 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 23 Mar 2007 12:52:26 -0500
Subject: [R] using "\t" in mtext
In-Reply-To: <000001c76d70$c057c210$0401a8c0@mouse>
References: <000001c76d70$c057c210$0401a8c0@mouse>
Message-ID: <1174672346.5117.21.camel@Bellerophon>

On Fri, 2007-03-23 at 18:29 +0100, Thomas Kaliwe wrote:
> Hi,
>  
> Using tab spaces in mtext e.g.
>  
> > mtext("\t")
>  
> little squares are plotted. Is there a way to use "\t" without getting
> squares displayed?
>  
> Thanks
>  
> Thomas

See this post:

  http://tolstoy.newcastle.edu.au/R/e2/help/06/11/4211.html

You could set up a vector of spaces of the effective tab length that you
wish:

  TAB <- paste(rep(" ", 8), collapse = "")

but bear in mind that unless you are using a monospace font in the
graph, it won't work for text alignment.

If you need text alignment (ie. columns, etc.) consider using the
textplot() function in the gplots package, which I reference in the post
above.

Also reviewing this thread:

  http://tolstoy.newcastle.edu.au/R/help/02b/0346.html

and using:

  RSiteSearch("text alignment plot")

might bring up other posts with alternate approaches.

HTH,

Marc Schwartz


From antonio.fabio at gmail.com  Fri Mar 23 15:15:24 2007
From: antonio.fabio at gmail.com (Antonio, Fabio Di Narzo)
Date: Fri, 23 Mar 2007 15:15:24 +0100
Subject: [R] [R-pkgs] RTisean 3.0-7 released
Message-ID: <b0808fdc0703230715k2178fdbdl32d0fe3bca0e8caf@mail.gmail.com>

Dear R users,
I've just uploaded to CRAN a new version of RTisean, the TISEAN-to-R interface.
This is now compatible with the recent, new 3.0.1 release of TISEAN [1].
This new TISEAN version is explicitely GPL-ed, and has some more
routines handling multivariate time series.

Bests,
Antonio, Fabio Di Narzo.

[1] http://idmc.blogspot.com/2007/03/tisean-300-is-out.html

_______________________________________________
R-packages mailing list
R-packages a stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From sebastien.boutry at hotmail.fr  Fri Mar 23 15:57:34 2007
From: sebastien.boutry at hotmail.fr (sebastien boutry)
Date: Fri, 23 Mar 2007 15:57:34 +0100
Subject: [R] test ANOVA/ANCOVA
Message-ID: <BAY129-F1405702F62629931116475E36A0@phx.gbl>

Hello everybody,


I search a test for compare the k means but I have one quantitative variable 
and two groups date, traitement. And I suppose my samples are dependant with 
the date.
What the statistical test would I use?
Thank you.

my data:

date	EU	DW
wk1	EU1	5,324547829
wk1	EU1	7,321253265
wk1	EU1	4,431712065
wk1	EU2	8,230322407
wk1	EU2	8,546873269
wk1	EU2	5,657332069
wk1	EU3	3,165508618
wk1	EU3	4,431712065
wk1	EU3	1,899305171
wk2	EU1	2,163097556
wk2	EU1	17,61379438
wk2	EU1	15,82754309
wk2	EU2	16,46064481
wk2	EU2	19,30960257
wk2	EU2	13,92823792
wk2	EU3	6,014466374
wk2	EU3	7,280669822
wk2	EU3	5,064813789
wk4	EU1	11,03179753
wk4	EU1	29,75578101
wk4	EU1	22,71252433
wk4	EU2	27,85647584
wk4	EU2	36,71989997
wk4	EU2	20,11680727
wk4	EU3	13,59661321
wk4	EU3	13,2951362
wk4	EU3	14,56133964
wk6	EU1	30,73875474
wk6	EU1	33,27842393
wk6	EU1	35,27512937
wk6	EU2	31,2817185
wk6	EU2	41,53147307
wk6	EU2	35,57093758
wk6	EU3	8,652390223
wk6	EU3	18,6359174
wk6	EU3	15,30807501

_________________________________________________________________
Avec Windows Live OneCare ?liminez tous les virus de votre PC !


From ripley at stats.ox.ac.uk  Fri Mar 23 19:00:19 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Mar 2007 18:00:19 +0000 (GMT)
Subject: [R] creating R packs for all
In-Reply-To: <200703231733.l2NHXw6E024566@gator.dt.uh.edu>
References: <200703231733.l2NHXw6E024566@gator.dt.uh.edu>
Message-ID: <Pine.LNX.4.64.0703231755330.8593@gannet.stats.ox.ac.uk>

On Fri, 23 Mar 2007, Erin Hodgess wrote:

> Dear R People:
>
> I am in the process of creating an R package via Windows.
>
> If I would decide to submit in to CRAN, what would I need to
> do in order to make it run for the Linux or Mac People, please?

If it passes R CMD check on Windows it should work anywhere.  About the 
only thing that is fussier on Unix than Windows is the use of the correct 
line endings for C etc files.  (OTOH, Windows adds restrictions that 'R 
CMD check' checks for Unix users.)

Quite a high proportion of CRAN packages were prepared on Windows, as can 
be seen from the executable text files, Windows not having a concept of an 
'executable file'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From notoole at mtroyal.ca  Fri Mar 23 19:00:44 2007
From: notoole at mtroyal.ca (Natalie O'Toole)
Date: Fri, 23 Mar 2007 12:00:44 -0600
Subject: [R] Rweb - calculating mean for an excel table opened in Rweb
Message-ID: <OF9D6B917C.A1F99F8B-ON872572A7.0062B48A-872572A7.0062F151@mtroyal.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070323/90d1f8c6/attachment.pl 

From lists at eva.mpg.de  Fri Mar 23 19:02:19 2007
From: lists at eva.mpg.de (Cristina Gomes)
Date: Fri, 23 Mar 2007 19:02:19 +0100
Subject: [R] p-values for GLMMs
Message-ID: <4604162B.6050305@eva.mpg.de>

Hi there,
I have a question about the GLMM that I'm doing, that a statistician 
friend suggested I should have for my analysis. I would like to know if 
there's any way of obtaining a p value and R square for the full model 
(and not each variable separately) as to asses whether this model is 
somewhat appropriate or not. Can one do this for a GLMM in the lme4 
package?
The other thing I wanted to know if there's a way to obtain residuals 
for a GLMM in the lme4 package. I would like to see the effect of the 
significant variables by plotting the residuals of the model without the 
variable of interest against the variable of interest, but I haven't 
found a way of obtaining residuals.
I hope somebody can help me with this.
Thanks a lot,
Cheers,
Cristina.


From klaster at karlin.mff.cuni.cz  Fri Mar 23 18:21:02 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Fri, 23 Mar 2007 18:21:02 +0100
Subject: [R] Subset
In-Reply-To: <b490ce570703230927r4e19b1b0o83fc170402a334e7@mail.gmail.com>
References: <b490ce570703230927r4e19b1b0o83fc170402a334e7@mail.gmail.com>
Message-ID: <46040C7E.6080700@karlin.mff.cuni.cz>

You want to obtain a subset of your data, so what about to use subset()...??
You didn't even consider doing some basic search for the solution as the 
posting guide asks you...

Petr

Sergio Della Franca napsal(a):
> Dear R-Helpers,
> 
> I have this dataset:
> 
>    YEAR    PRODUCTS cluster
>     1          10          2
>     2          42          3
>     3          25          2
>     4          42          3
>     5          40          3
>     6          45          1
>     7          44          1
>     8          47          1
>     9          42          1
> 
> 
> I want to create a subset (when cluster=1),
> 
>     YEAR    PRODUCTS cluster
>     6          45          1
>     7          44          1
>     8          47          1
>     9          42          1
> 
> 
> How can i perform this?
> 
> 
> Thank you in advance.
> 
> 
> Sergio Della Franca
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From dlvanbrunt at gmail.com  Fri Mar 23 19:14:56 2007
From: dlvanbrunt at gmail.com (David L. Van Brunt, Ph.D.)
Date: Fri, 23 Mar 2007 14:14:56 -0400
Subject: [R] memory, speed,
	and assigning results into new v. existing variable
Message-ID: <d332d3e10703231114kaba2e30md6c762a491fe8d12@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070323/8a529921/attachment.pl 

From gunter.berton at gene.com  Fri Mar 23 19:28:10 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Fri, 23 Mar 2007 11:28:10 -0700
Subject: [R] Completely off topic, but amusing?
Message-ID: <003b01c76d79$02ddf700$4d908980@gne.windows.gene.com>

Folks:

Thought that many on this list might find this amusing, perhaps even a bit
relevant. Hope it's OK:

************
WASHINGTON - The government's estimate of the number of Americans without
health insurance fell by nearly 2 million Friday, but not because anyone got
health coverage. 
	
The Census Bureau
<http://search.news.yahoo.com/search/news/?p=Census+Bureau>  said it has
been overstating the number of people without health insurance since 1995.
The bureau blamed the inflated numbers on a **12-year-old computer
programming error**.[emphasis added -- BG]
**************

So what does "validated software" really mean? (Rhetorical question -- no
reply sought). 

Cheers to all,

Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404


From dlvanbrunt at gmail.com  Fri Mar 23 19:59:53 2007
From: dlvanbrunt at gmail.com (David L. Van Brunt, Ph.D.)
Date: Fri, 23 Mar 2007 14:59:53 -0400
Subject: [R] memory, speed,
	and assigning results into new v. existing variable
In-Reply-To: <005201c76d7c$28ace420$4d908980@gne.windows.gene.com>
References: <d332d3e10703231114kaba2e30md6c762a491fe8d12@mail.gmail.com>
	<005201c76d7c$28ace420$4d908980@gne.windows.gene.com>
Message-ID: <d332d3e10703231159h39675d4bo9cde168ffc6262c0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070323/9cc8c959/attachment.pl 

From brown_emu at yahoo.com  Fri Mar 23 20:03:30 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Fri, 23 Mar 2007 12:03:30 -0700 (PDT)
Subject: [R] objects of class "matrix" and mode "list"?
Message-ID: <99197.66995.qm@web39707.mail.mud.yahoo.com>

Hello everyone,

I cannot seem to find information about objects of class "matrix" and mode
"list", and how to handle them (apart from flattening the list). I get this
type of object from using sapply(). Sorry for the long example, but the code
below illustrates how I get this type of object. Is anyone aware of
documentation regarding this object?

Thanks very much,

Stephen

===== begin example ====

# I am just making up a fake data set
df <- data.frame(Day=rep(1:3,each=24),Hour=rep(1:24,times=3),
                 Name1=rnorm(24*3),Name2=rnorm(24*3))

# define a function to get a set of descriptive statistics
tmp <- function(x) {
  # this function will accept a data frame
  # and return a 1-row data frame of
  # max value, colname of max, min value, and colname of min
  return(data.frame(maxval=max(apply(x,2,max)),
                    maxloc=names(x)[which.max(apply(x,2,max))],
                    minval=min(apply(x,2,min)),
                    minloc=names(x)[which.min(apply(x,2,min))]))
}

# Now applying function to data:
# (1) split the data table by Day with split()
# (2) apply the tmp function defined above to each data frame from (1)
#     using lapply()
# (3) transpose the final matrix and convert it to a data frame
#     with mixed characters and numbers
#     using as.data.frame(), lapply(), and type.convert()

> final <- as.data.frame(lapply(as.data.frame(t(sapply(split(df[,-c(1:2)],
+                                                           
f=df$Day),tmp))),
+                               type.convert,as.is=TRUE))
Error in type.convert(x, na.strings, as.is, dec) : 
	the first argument must be of mode character

I thought sapply() would give me a data frame or matrix, which I would
transpose into a character matrix, to which I can apply type.convert()
and get the same matrix as what I would get from these two lines (Fold
function taken from Gabor's post on R-help a few years ago):

Fold <- function(f, x, L) for(e in L) x <- f(x, e)
final2 <- Fold(rbind,vector(),lapply(split(df[,-c(1:2)],f=day),tmp))

> print(c(class(final2),mode(final2)))
[1] "data.frame" "list"  

====================================================
However, by my original method, sapply() gives me a matrix with mode, "list"

intermediate1 <- sapply(split(df[,-c(1:2)],f=df$Day),tmp)
> print(c(class(intermediate1),mode(intermediate1)))
[1] "matrix" "list"  

Transposing, still a matrix with mode list, not character:

intermediate2 <- t(sapply(split(df[,-c(1:2)],f=day),tmp))
> print(c(class(intermediate2),mode(intermediate2)))
[1] "matrix" "list"  

Unclassing gives me the same thing...

> print(c(class(unclass(intermediate2)),mode(unclass(intermediate2))))
[1] "matrix" "list"  




 
____________________________________________________________________________________
Be a PS3 game guru.


From brown_emu at yahoo.com  Fri Mar 23 20:13:03 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Fri, 23 Mar 2007 12:13:03 -0700 (PDT)
Subject: [R] simple bar plot question
In-Reply-To: <25589179-F6B3-4193-A27D-C82A98B2DA32@fas.harvard.edu>
Message-ID: <226301.98922.qm@web39712.mail.mud.yahoo.com>

I think you can set legend=FALSE in barplot() and add your own legend, in
which you have a lot more control:

barplot(#your arguments#,legend=FALSE)
legend(x="topleft",cex=yourCex)

etc.




--- Janet <jerosenb at fas.harvard.edu> wrote:

> Literally 60 seconds after I sent my question, I found the cex.names  
> parameter to barplot.
> I haven't found a parameter for the size of the text in the legend.
> 
> Apologies for clogging inboxes semi-unnecessarily.
> 
> Janet
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



 
____________________________________________________________________________________
Finding fabulous fares is fun.


From Ted.Harding at manchester.ac.uk  Fri Mar 23 20:20:08 2007
From: Ted.Harding at manchester.ac.uk ( (Ted Harding))
Date: Fri, 23 Mar 2007 19:20:08 -0000 (GMT)
Subject: [R] distribution graph
In-Reply-To: <1174668940.5117.11.camel@Bellerophon>
Message-ID: <XFMail.070323192008.Ted.Harding@manchester.ac.uk>

On 23-Mar-07 16:55:40, Marc Schwartz wrote:
> [...]
> How about something like this:
> 
> 
> DistPlot <- function(x, digits = 1, ...)
> {
>   x <- round(x, digits)
>   
>   Tab <- table(x)
> 
>   Vals <- sapply(Tab, function(x) seq(x) - mean(seq(x)))
> 
>   X.Vals <- unlist(Vals, use.names = FALSE)
>   tmp <- sapply(Vals, length)
>   Y.Vals <- rep(names(tmp), tmp)
> 
>   plot(X.Vals, Y.Vals, ...)
> }
> 
> 
> Vec <- exp(rnorm(200, sd = 0.25) + 2) / 5
> 
> DistPlot(Vec, pch = 19)

Very pretty, Marc -- and magic code!!

Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 23-Mar-07                                       Time: 19:20:04
------------------------------ XFMail ------------------------------


From ted.harding at nessie.mcc.ac.uk  Fri Mar 23 20:20:43 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 23 Mar 2007 19:20:43 -0000 (GMT)
Subject: [R] distribution graph
In-Reply-To: <1174668940.5117.11.camel@Bellerophon>
Message-ID: <XFMail.070323192008.Ted.Harding@manchester.ac.uk>

On 23-Mar-07 16:55:40, Marc Schwartz wrote:
> [...]
> How about something like this:
> 
> 
> DistPlot <- function(x, digits = 1, ...)
> {
>   x <- round(x, digits)
>   
>   Tab <- table(x)
> 
>   Vals <- sapply(Tab, function(x) seq(x) - mean(seq(x)))
> 
>   X.Vals <- unlist(Vals, use.names = FALSE)
>   tmp <- sapply(Vals, length)
>   Y.Vals <- rep(names(tmp), tmp)
> 
>   plot(X.Vals, Y.Vals, ...)
> }
> 
> 
> Vec <- exp(rnorm(200, sd = 0.25) + 2) / 5
> 
> DistPlot(Vec, pch = 19)

Very pretty, Marc -- and magic code!!

Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 23-Mar-07                                       Time: 19:20:04
------------------------------ XFMail ------------------------------


From p.dalgaard at biostat.ku.dk  Fri Mar 23 20:24:09 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 23 Mar 2007 20:24:09 +0100
Subject: [R] Completely off topic, but amusing?
In-Reply-To: <003b01c76d79$02ddf700$4d908980@gne.windows.gene.com>
References: <003b01c76d79$02ddf700$4d908980@gne.windows.gene.com>
Message-ID: <46042959.3030203@biostat.ku.dk>

Bert Gunter wrote:
> Folks:
>
> Thought that many on this list might find this amusing, perhaps even a bit
> relevant. Hope it's OK:
>
> ************
> WASHINGTON - The government's estimate of the number of Americans without
> health insurance fell by nearly 2 million Friday, but not because anyone got
> health coverage. 
> 	
> The Census Bureau
> <http://search.news.yahoo.com/search/news/?p=Census+Bureau>  said it has
> been overstating the number of people without health insurance since 1995.
> The bureau blamed the inflated numbers on a **12-year-old computer
> programming error**.[emphasis added -- BG]
> **************
>
> So what does "validated software" really mean? (Rhetorical question -- no
> reply sought). 
>
>   
Details of what went wrong are at

http://www.census.gov/hhes/www/hlthins/usernote/usernote3-21rev.html

(executive summary: They imputed coverage for some people who were 
actually covered by another household member's policy).

Looks like a mistake you can make in any programming language...

> Cheers to all,
>
> Bert Gunter
> Genentech Nonclinical Statistics
> South San Francisco, CA 94404
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From brown_emu at yahoo.com  Fri Mar 23 20:33:22 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Fri, 23 Mar 2007 12:33:22 -0700 (PDT)
Subject: [R] simple bar plot question
In-Reply-To: <226301.98922.qm@web39712.mail.mud.yahoo.com>
Message-ID: <44058.54341.qm@web39702.mail.mud.yahoo.com>

Sorry, that was a bit premature - you probably want other arguments in legend
as well; in particular 'fill' is an argument you'd probably be interested in 
for legends of barplots:

legend(x="topleft",cex=yourCex,fill=yourColors,legend.text=yourText,
       lty=NA,pch=NA,#and so on#)

--- Stephen Tucker <brown_emu at yahoo.com> wrote:

> I think you can set legend=FALSE in barplot() and add your own legend, in
> which you have a lot more control:
> 
> barplot(#your arguments#,legend=FALSE)
> legend(x="topleft",cex=yourCex)
> 
> etc.
> 
> 
> 
> 
> --- Janet <jerosenb at fas.harvard.edu> wrote:
> 
> > Literally 60 seconds after I sent my question, I found the cex.names  
> > parameter to barplot.
> > I haven't found a parameter for the size of the text in the legend.
> > 
> > Apologies for clogging inboxes semi-unnecessarily.
> > 
> > Janet
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> 
> 
> 
>  
>
____________________________________________________________________________________
> Finding fabulous fares is fun.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



 
____________________________________________________________________________________
Bored stiff? Loosen up...


From mwkimpel at gmail.com  Fri Mar 23 20:35:28 2007
From: mwkimpel at gmail.com (Mark W Kimpel)
Date: Fri, 23 Mar 2007 15:35:28 -0400
Subject: [R] can't load just saved R object "ReadItem: unknown type 65"
In-Reply-To: <Pine.LNX.4.64.0703230753500.20383@nokomis.stat.uiowa.edu>
References: <46033E07.4040905@gmail.com>
	<Pine.LNX.4.64.0703230753500.20383@nokomis.stat.uiowa.edu>
Message-ID: <46042C00.7060601@gmail.com>

Luke, I'll be gone for about 2 weeks but will work on getting you a 
reproducible example when I get back. If this topic comes up with anyone 
else, please copy me on your responses as I may miss it in the 600 
emails I'll have to delete on my return :) Mark

Luke Tierney wrote:
> According to the logs nothing at all has changed in the serialization
> code in a month and nothing of consequence for much longer than that.
> To track this down we will need a complete, reproducible, and
> preferably minimal example.
> 
> Best,
> 
> luke
> 
> On Thu, 22 Mar 2007, Mark W Kimpel wrote:
> 
>> I have run into a problem loading a just saved R object using R-devel. I
>> have been saving and loading this particular type of R object for a long
>> while and never ran into this problem. I save, then immediately reload
>> (to test save) and get "ReadItem: unnknown type 65".
>>
>> This error is reproducible after logout from server and restart of emacs
>> and R.
>>
>> Below is my output and sessionInfo().
>>
>> Thanks,
>> Mark
>>
>> > setwd("~/Genomics/Experiments.Genomic/BB01/acb.shell")
>> > local(save(affy.object.preprocessed, file
>> ="affy.object.preprocessed.R" ))
>> > load("affy.object.preprocessed.R")
>> Error in load("affy.object.preprocessed.R") :
>>     ReadItem: unknown type 65, perhaps written by later version of R
>> > sessionInfo()
>> R version 2.5.0 Under development (unstable) (2007-03-11 r40824)
>> powerpc64-unknown-linux-gnu
>>
>> locale:
>> LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C 
>>
>>
>> attached base packages:
>> [1] "splines"   "stats"     "graphics"  "grDevices" "datasets"  "utils"
>> [7] "tools"     "methods"   "base"
>>
>> other attached packages:
>>      multtest    rat2302cdf affycoretools       annaffy        xtable
>>      "1.13.1"      "1.15.0"       "1.7.8"       "1.7.3"       "1.4-3"
>>         gcrma   matchprobes       biomaRt         RCurl           XML
>>       "2.7.3"       "1.7.4"      "1.9.21"       "0.8-0"       "1.6-0"
>>       GOstats      Category        Matrix       lattice    genefilter
>>      "2.1.13"      "2.1.20"   "0.9975-11"     "0.14-16"      "1.13.8"
>>      survival          KEGG          RBGL      annotate            GO
>>        "2.31"     "1.15.12"      "1.11.4"      "1.13.6"     "1.15.12"
>>         graph         limma          affy        affyio       Biobase
>>      "1.13.6"      "2.9.13"     "1.13.14"       "1.3.3"     "1.13.39"
>>
> 

-- 
Mark W. Kimpel MD
Neuroinformatics
Department of Psychiatry
Indiana University School of Medicine


From joseph.wakeling at webdrake.net  Fri Mar 23 20:54:41 2007
From: joseph.wakeling at webdrake.net (Joseph Wakeling)
Date: Fri, 23 Mar 2007 19:54:41 +0000
Subject: [R] Space for X and Y axis labels
Message-ID: <46043081.3030104@webdrake.net>

Hello all,

I'm having a bit of a problem with x and y axis labels.  Two things:
first, if I want to create a plot with,

    plot.new()
    plot.window(.....)
    axis(1)
    axis(2)
    lines(...)
    points(...)

    [etc.]

... where do I introduce the xlab=... and ylab=... commands?  I
attempted this in plot.window() but no labels showed up.

Second, here's a bit of real code...

	postscript(file="RPDfig.eps",onefile=FALSE,
	 +         horizontal=FALSE,paper="special",height=8.3,
         +         width=11.7)
	matplot(2:length(PRICE),t(NWpd),type="l",xlim=c(0,200),
	 +      ylim=c(-0.1,0.1),xaxs="i",yaxs="i",xlab="Time",
	 +      ylab=expression(over(x,y)),bty="n")
	points(2:length(PRICE),Ppd,col="red")
	dev.off()

Here the y label is meant to be x over y (a fraction), but the top part
of the fraction is cut off by the border of the graphic.  How can I fix
this, either by creating extra space for the label or by insisting that
the boundary of the graphic be extended to embrace the whole of the
label?  I've tried setting xpd=TRUE in the matplot command, but I'm not
sure that is the correct option (and it makes no difference).

Many thanks,

    -- Joe


From jholtman at gmail.com  Fri Mar 23 21:13:30 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 23 Mar 2007 16:13:30 -0400
Subject: [R] subtotal for same row data
In-Reply-To: <7E72CD7F4C0A994191ED9B3726FAEBF302739A78@NIHCESMLBX4.nih.gov>
References: <7E72CD7F4C0A994191ED9B3726FAEBF302739A78@NIHCESMLBX4.nih.gov>
Message-ID: <644e1f320703231313q4fd4bc1g97bca633a05f0ca6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070323/af5afb1e/attachment.pl 

From tillea at rki.de  Fri Mar 23 22:21:15 2007
From: tillea at rki.de (Andreas Tille)
Date: Fri, 23 Mar 2007 22:21:15 +0100 (CET)
Subject: [R] Conversion from string to date type
Message-ID: <Pine.LNX.4.62.0703232207350.31739@wr-linux02>

Hi,

I'm on my very first steps to R and so I hope that I do
not ask a really stupid questions but I did not found it
via R-Search, in the FAQ or Google (BTW, the name "R" is
not a really good seekiong criterion ;-) ).

I have a data file containing a table that containes
dates and values like

date\t value1\t value2, ...
01.03.2007\t 17\t 42\t ...
02.03.2007\t 2\t 3\t ...
03.03.2007\t 47\t 11\t ...
...

I was perfectly able to read this file via

mydata <- read.csv(file='mydata.dat', sep = '\t', quote='', fill=TRUE,header=TRUE )

but the date vector is a vector of strings instead of date values.
I want to convert these strings into date values to be able
to make graphs where date is the x-axis and value? the y-axis.

I would be really happy if someone could enlighten me how to
do this conversion (and a hint how to do a graph as PNG) would
be an extra bonus which would shorten my further reading of the
docs).

Kind regards and thanks for your help

           Andreas.

-- 
http://fam-tille.de


From Frederic.Jean at univ-brest.fr  Fri Mar 23 22:27:58 2007
From: Frederic.Jean at univ-brest.fr (Frederic Jean)
Date: Fri, 23 Mar 2007 22:27:58 +0100
Subject: [R]  plotting dnorm() issued from mclust models
Message-ID: <20070323222758.1h1ah47vsocko8oc@cassis-gw.univ-brest.fr>

Dear all

I have a problem in fitting lines() of the normal distributions  
identified with Mclust on a histogram or a mclust1Dplot. Here is some  
sample code to explain :

  set.seed(22)
  foo <- c(rnorm(400, 10, 2), rnorm(500, 17, 4))
  mcl <- Mclust(foo, G=2)
  mcl.sd <- sqrt(mcl$parameters$variance$sigmasq)
  mcl.size <- c(length(mcl$classification[mcl$classification==2]),  
length(mcl$classification[mcl$classification==1]))
  x <- pretty(c(0:44), 100)

  #### my plot of histogram and lines of normal distributions
  #### SEEMS OK (or am I wrong ?) using frequencies :
  histA <- hist(foo, breaks =c(0:44), ylim = c(0,100))
  lines(x, dnorm(x, mcl$parameters$mean[1], mcl.sd[1])*mcl.size[1],  
col =2, lw=2)
  lines(x, dnorm(x, mcl$parameters$mean[2], mcl.sd[2])*mcl.size[2],  
col =2, lw=2)

  #### my plot of histogram and lines of normal distributions
  #### IS wrong when using prob :
  mclust1Dplot(foo, parameters = mcl$parameters, z = mcl$z, what = "density")
  histA <- hist(foo, breaks =c(0:44), prob = T, add =T)
  lines(x, dnorm(x, mcl$parameters$mean[2], mcl.sd[2]), col =2, lw=2)
  lines(x, dnorm(x, mcl$parameters$mean[1], mcl.sd[1]), col =2, lw=2)

In second plot, the bell shaped curves are obviously too high and it  
seems that I miss something obvious in scaling dnorm()'s in building  
the second plot: I tried different things like scaling dnorm() by the  
proportion of individuals belonging to cluster 1 and 2 respectively,  
but with no success.

Could someone help to point my errors ?
Many thanks in advance

Fred J.


From jholtman at gmail.com  Fri Mar 23 22:36:33 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 23 Mar 2007 16:36:33 -0500
Subject: [R] Conversion from string to date type
In-Reply-To: <Pine.LNX.4.62.0703232207350.31739@wr-linux02>
References: <Pine.LNX.4.62.0703232207350.31739@wr-linux02>
Message-ID: <644e1f320703231436w39cf1f00k56daca80be21738e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070323/77d2d819/attachment.pl 

From julien at no-log.org  Fri Mar 23 22:41:34 2007
From: julien at no-log.org (Julien Barnier)
Date: Fri, 23 Mar 2007 22:41:34 +0100
Subject: [R] Optimal matching analysis package
Message-ID: <87ircr4oz5.fsf@gnugnus.org>

Hi,

I recently heard about application of sequencing algorithms to social
data, which seems very interesting. There seems to be at least two
programs whith whom you can do this kind of analysis : Optimize and
TDA.

http://home.uchicago.edu/~aabbott/om.html

http://steinhaus.stat.ruhr-uni-bochum.de/tda.html

So I wonder if there is a R package which could do the same kind of
thing. I've seen the optmatch package, but it doesn't seem to be
exactly the same kind of method (but, as I am really new in this
fiels, I may mistake completely).

Thanks in advance for any help.

-- 
Julien


From tillea at rki.de  Fri Mar 23 23:23:05 2007
From: tillea at rki.de (Andreas Tille)
Date: Fri, 23 Mar 2007 23:23:05 +0100 (CET)
Subject: [R] Conversion from string to date type
In-Reply-To: <644e1f320703231436w39cf1f00k56daca80be21738e@mail.gmail.com>
References: <Pine.LNX.4.62.0703232207350.31739@wr-linux02>
	<644e1f320703231436w39cf1f00k56daca80be21738e@mail.gmail.com>
Message-ID: <Pine.LNX.4.62.0703232318030.4528@wr-linux02>

On Fri, 23 Mar 2007, jim holtman wrote:

> Here is how you can convert them to a Date object:
>
>> x <- c('01.03.2007','02.03.2007','03.03.2007')
>> y <- as.Date(x, format="%d.%m.%Y")
>> y

Well, this is what I tried when reading the docs, but

> mydata <- read.csv(file='mydata.dat', sep = '\t', quote='', fill=TRUE,header=TRUE )
> datum <- as.Date(mydata["date"], "%d.%m,%y")
Error in as.Date.default(mydata["date"], "%d.%m,%y") :
         do not know how to convert 'mydata["date"]' to class "Date"

I also tried:

> datum <- strptime(imydata["date"], "%d.%m,%y")
> datum
[1] NA

I guess it is a very simple thing I'm doing wrong and missinterpret
the docs.

Thanks for the quick response

            Andreas.

-- 
http://fam-tille.de


From ggrothendieck at gmail.com  Fri Mar 23 23:28:33 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 23 Mar 2007 18:28:33 -0400
Subject: [R] Conversion from string to date type
In-Reply-To: <Pine.LNX.4.62.0703232207350.31739@wr-linux02>
References: <Pine.LNX.4.62.0703232207350.31739@wr-linux02>
Message-ID: <971536df0703231528g35e73703q7bbb765a40e13acd@mail.gmail.com>

Read the help desk article in R-News 4/1 and see

?as.Date
?strptime (for setting the as.Date format= argument)

Also, you might be interested in the zoo package

library(zoo)
?read.zoo
vignette("zoo")
vignette("zoo-quickref")


On 3/23/07, Andreas Tille <tillea at rki.de> wrote:
> Hi,
>
> I'm on my very first steps to R and so I hope that I do
> not ask a really stupid questions but I did not found it
> via R-Search, in the FAQ or Google (BTW, the name "R" is
> not a really good seekiong criterion ;-) ).
>
> I have a data file containing a table that containes
> dates and values like
>
> date\t value1\t value2, ...
> 01.03.2007\t 17\t 42\t ...
> 02.03.2007\t 2\t 3\t ...
> 03.03.2007\t 47\t 11\t ...
> ...
>
> I was perfectly able to read this file via
>
> mydata <- read.csv(file='mydata.dat', sep = '\t', quote='', fill=TRUE,header=TRUE )
>
> but the date vector is a vector of strings instead of date values.
> I want to convert these strings into date values to be able
> to make graphs where date is the x-axis and value? the y-axis.
>
> I would be really happy if someone could enlighten me how to
> do this conversion (and a hint how to do a graph as PNG) would
> be an extra bonus which would shorten my further reading of the
> docs).
>
> Kind regards and thanks for your help
>
>           Andreas.
>
> --
> http://fam-tille.de
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From edd at debian.org  Fri Mar 23 23:49:08 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 23 Mar 2007 17:49:08 -0500
Subject: [R] Conversion from string to date type
In-Reply-To: <Pine.LNX.4.62.0703232207350.31739@wr-linux02>
References: <Pine.LNX.4.62.0703232207350.31739@wr-linux02>
Message-ID: <17924.22884.907451.10226@basebud.nulle.part>


Hi Andreas,

Welcome to r-help :)

On 23 March 2007 at 22:21, Andreas Tille wrote:
| I'm on my very first steps to R and so I hope that I do
| not ask a really stupid questions but I did not found it
| via R-Search, in the FAQ or Google (BTW, the name "R" is
| not a really good seekiong criterion ;-) ).
| 
| I have a data file containing a table that containes
| dates and values like
| 
| date\t value1\t value2, ...
| 01.03.2007\t 17\t 42\t ...
| 02.03.2007\t 2\t 3\t ...
| 03.03.2007\t 47\t 11\t ...
| ...
| 
| I was perfectly able to read this file via
| 
| mydata <- read.csv(file='mydata.dat', sep = '\t', quote='', fill=TRUE,header=TRUE )

Nit 1: read.csv() is for csv files which tend to have "," as a separator;
read.table() is more useful here.

| but the date vector is a vector of strings instead of date values.
| I want to convert these strings into date values to be able
| to make graphs where date is the x-axis and value? the y-axis.

It could be worse :)  Often times, folks get confused when variables are
of type 'factor' (see the docs) instead of char. With character you are fine,
and you can do the converson, see help(strptime) or help(as.Date) which I am
using below --- note how you have to tell it the format in the standard C
notation. 

| 
| I would be really happy if someone could enlighten me how to
| do this conversion (and a hint how to do a graph as PNG) would
| be an extra bonus which would shorten my further reading of the
| docs).

Here you go:

> mydata <- read.table(file="/tmp/mydata.dat", sep="\t", header=TRUE)
> mydata
        date value1 value2
1 01.03.2007     17     42
2 02.03.2007      2      3
3 03.03.2007     47     11
> mydata$date <- as.Date(mydata$date, "%d.%m.%Y")   ## [1]
> mydata
        date value1 value2
1 2007-03-01     17     42
2 2007-03-02      2      3
3 2007-03-03     47     11
> with(mydata, plot(date, value1))                  ## [2]
> png("/tmp/mydata.png")                            ## [3]
> with(mydata, plot(date, value1))
> dev.off()                                         ## [4]
null device 
          1 
> 


[1] As I mentioned, you need to supply a format unless your data lists as
(for today) 2007-03-23 which is an ISO format

[2] The with() simply makes the indexing easier. Direct use is
plot(mydata$date, mydata$value1) or also
plot(mydata[,"date"], mydata[,"value1"]) or also
plot(mydata[,1], mydata[,2]) 

[3] See the help for all the options on png, as well the numerous examples
for plot to annotate, give titles, ...

[4] dev.off() is critical to get the 'device' (here a file) closed.

| Kind regards and thanks for your help

My pleasure. Happy R-ing,  Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From andy_liaw at merck.com  Fri Mar 23 23:55:38 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 23 Mar 2007 18:55:38 -0400
Subject: [R] objects of class "matrix" and mode "list"? [Broadcast]
In-Reply-To: <99197.66995.qm@web39707.mail.mud.yahoo.com>
References: <99197.66995.qm@web39707.mail.mud.yahoo.com>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03EB2947@usctmx1106.merck.com>

It may help to (re-)read ?sapply a bit more in detail.  Simplification
is done only if it's "possible", and what "possible" means is defined
there.

A list is a vector whose elements can be different objects, but a vector
nonetheless.  Thus a list can have dimensions.  E.g.,

R> a <- list(1, 1:2, 3, c("abc", "def"))
R> dim(a) <- c(2, 2)
R> a
     [,1]      [,2]       
[1,] 1         3          
[2,] Integer,2 Character,2

That sometimes can be extremely useful (not like the example above!).

Andy 

From: Stephen Tucker
> 
> Hello everyone,
> 
> I cannot seem to find information about objects of class 
> "matrix" and mode
> "list", and how to handle them (apart from flattening the 
> list). I get this
> type of object from using sapply(). Sorry for the long 
> example, but the code
> below illustrates how I get this type of object. Is anyone aware of
> documentation regarding this object?
> 
> Thanks very much,
> 
> Stephen
> 
> ===== begin example ====
> 
> # I am just making up a fake data set
> df <- data.frame(Day=rep(1:3,each=24),Hour=rep(1:24,times=3),
>                  Name1=rnorm(24*3),Name2=rnorm(24*3))
> 
> # define a function to get a set of descriptive statistics
> tmp <- function(x) {
>   # this function will accept a data frame
>   # and return a 1-row data frame of
>   # max value, colname of max, min value, and colname of min
>   return(data.frame(maxval=max(apply(x,2,max)),
>                     maxloc=names(x)[which.max(apply(x,2,max))],
>                     minval=min(apply(x,2,min)),
>                     minloc=names(x)[which.min(apply(x,2,min))]))
> }
> 
> # Now applying function to data:
> # (1) split the data table by Day with split()
> # (2) apply the tmp function defined above to each data frame from (1)
> #     using lapply()
> # (3) transpose the final matrix and convert it to a data frame
> #     with mixed characters and numbers
> #     using as.data.frame(), lapply(), and type.convert()
> 
> > final <- 
> as.data.frame(lapply(as.data.frame(t(sapply(split(df[,-c(1:2)],
> +                                                           
> f=df$Day),tmp))),
> +                               type.convert,as.is=TRUE))
> Error in type.convert(x, na.strings, as.is, dec) : 
> 	the first argument must be of mode character
> 
> I thought sapply() would give me a data frame or matrix, which I would
> transpose into a character matrix, to which I can apply type.convert()
> and get the same matrix as what I would get from these two lines (Fold
> function taken from Gabor's post on R-help a few years ago):
> 
> Fold <- function(f, x, L) for(e in L) x <- f(x, e)
> final2 <- Fold(rbind,vector(),lapply(split(df[,-c(1:2)],f=day),tmp))
> 
> > print(c(class(final2),mode(final2)))
> [1] "data.frame" "list"  
> 
> ====================================================
> However, by my original method, sapply() gives me a matrix 
> with mode, "list"
> 
> intermediate1 <- sapply(split(df[,-c(1:2)],f=df$Day),tmp)
> > print(c(class(intermediate1),mode(intermediate1)))
> [1] "matrix" "list"  
> 
> Transposing, still a matrix with mode list, not character:
> 
> intermediate2 <- t(sapply(split(df[,-c(1:2)],f=day),tmp))
> > print(c(class(intermediate2),mode(intermediate2)))
> [1] "matrix" "list"  
> 
> Unclassing gives me the same thing...
> 
> > print(c(class(unclass(intermediate2)),mode(unclass(intermediate2))))
> [1] "matrix" "list"  
> 
> 
> 
> 
>  
> ______________________________________________________________
> ______________________
> Be a PS3 game guru.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From tillea at rki.de  Sat Mar 24 00:23:20 2007
From: tillea at rki.de (Andreas Tille)
Date: Sat, 24 Mar 2007 00:23:20 +0100 (CET)
Subject: [R] Conversion from string to date type
In-Reply-To: <17924.22884.907451.10226@basebud.nulle.part>
References: <Pine.LNX.4.62.0703232207350.31739@wr-linux02>
	<17924.22884.907451.10226@basebud.nulle.part>
Message-ID: <Pine.LNX.4.62.0703240018340.8087@wr-linux02>

On Fri, 23 Mar 2007, Dirk Eddelbuettel wrote:

> Nit 1: read.csv() is for csv files which tend to have "," as a separator;
> read.table() is more useful here.

Well, that's correct.  read.csv was just a leftover from former
tests and finally it worked also this way - but thanks for the
hint anyway.

>> mydata <- read.table(file="/tmp/mydata.dat", sep="\t", header=TRUE)
>> mydata
>        date value1 value2
> 1 01.03.2007     17     42
> 2 02.03.2007      2      3
> 3 03.03.2007     47     11
>> mydata$date <- as.Date(mydata$date, "%d.%m.%Y")   ## [1]

Ahhh, the '$date' thingy did the trick!

> [1] As I mentioned, you need to supply a format unless your data lists as
> (for today) 2007-03-23 which is an ISO format

That's obvious.

> [2] The with() simply makes the indexing easier. Direct use is
> plot(mydata$date, mydata$value1) or also
> plot(mydata[,"date"], mydata[,"value1"]) or also
> plot(mydata[,1], mydata[,2])

Ahh, so many ways to do it right.  You must be really unlucky
if you manage to do it wrong as I did. ;-)

> [3] See the help for all the options on png, as well the numerous examples
> for plot to annotate, give titles, ...
>
> [4] dev.off() is critical to get the 'device' (here a file) closed.

Yea, read this in the docs.  Now I can start fine tuning and start
having fun with R. ;-)

> My pleasure. Happy R-ing,  Dirk

Thanks for the nice kick-start

           Andreas.

-- 
http://fam-tille.de


From muenchen at utk.edu  Sat Mar 24 02:02:27 2007
From: muenchen at utk.edu (Muenchen, Robert A (Bob))
Date: Fri, 23 Mar 2007 21:02:27 -0400
Subject: [R] how to get "lsmeans"?
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03E659F6@usctmx1106.merck.com>
References: <Pine.LNX.4.64.0703211502050.24076@gannet.stats.ox.ac.uk>
	<20070322005910.VVAT1624.tomts40-srv.bellnexxia.net@JohnDesktop8300>
	<D028EEB4CA113D4EAFDD485CCC998277731795@UTKFSVS4.utk.tennessee.edu>
	<40e66e0b0703221242w434d3a8el5e5f22ea3639101b@mail.gmail.com>
	<39B6DDB9048D0F4DAD42CB26AAFF0AFA03E659F6@usctmx1106.merck.com>
Message-ID: <D028EEB4CA113D4EAFDD485CCC998277731B0A@UTKFSVS4.utk.tennessee.edu>

The Exegesis paper gave me a great look at the history of all this. I
had not been aware that S-PLUS had gone that route. There is much to be
said for knowing you might be more successful but sticking to your
perspective instead. And in the long run, that may be the more
successful route anyway. 

Thanks,
Bob

=========================================================
Bob Muenchen (pronounced Min'-chen), Manager 
Statistical Consulting Center
U of TN Office of Information Technology
200 Stokely Management Center, Knoxville, TN 37996-0520
Voice: (865) 974-5230 
FAX: (865) 974-4810
Email: muenchen at utk.edu
Web: http://oit.utk.edu/scc, 
News: http://listserv.utk.edu/archives/statnews.html
=========================================================

> -----Original Message-----
> From: Liaw, Andy [mailto:andy_liaw at merck.com]
> Sent: Thursday, March 22, 2007 5:27 PM
> To: Douglas Bates; Muenchen, Robert A (Bob)
> Cc: R-help at stat.math.ethz.ch
> Subject: RE: [R] how to get "lsmeans"?
> 
> From: Douglas Bates
> >
> > On 3/22/07, Muenchen, Robert A (Bob) <muenchen at utk.edu> wrote:
> >
> > > Perhaps I'm stating the obvious, but to increase the use of R in
> > > places where SAS & SPSS dominate, it's important to make
> > getting the
> > > same answers as easy as possible. That includes things like
lsmeans
> > > and type III sums of squares. I've read lots of discussions here
on
> > > sums of squares & I'm not advocating type III use, just
> > looking at it
> > > from a marketing perspective. Too many people look for
> > excuses to not change.
> > > The fewer excuses, the better.
> >
> > You may get strong reactions to such a suggestion.  I
> > recommend reading Bill Venables' famous unpublished paper
> > "Exegeses on linear models" (google for the title - very few
> > people use "Exegeses" and "linear models" in the same
> > sentence - in fact I would not be surprised if Bill was the
> > only one who has ever done so).
> 
> It's on the MASS page:
> http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf
> I believe it's based on a talk Bill gave at a S-PLUS User's
Conference.
> I think it deserves to be required reading for all graduate level
> linear
> models course.
> 
> > You must realize that R is written by experts in statistics
> > and statistical computing who, despite popular opinion, do
> > not believe that everything in SAS and SPSS is worth copying.
> >  Some things done in such packages, which trace their roots
> > back to the days of punched cards and magnetic tape when
> > fitting a single linear model may take several days because
> > your first 5 attempts failed due to syntax errors in the JCL
> > or the SAS code, still reflect the approach of "give me every
> > possible statistic that could be calculated from this model,
> > whether or not it makes sense".  The approach taken in R is
> different.
> >  The underlying assumption is that the useR is thinking about
> > the analysis while doing it.
> >
> > The fact that it is so difficult to explain what lsmeans are
> > and why they would be of interest is an indication of why
> > they aren't implemented in any of the required packages.
> 
> Perhaps I should have made it clear in my original post:  I gave the
> example and code more to show what the mysterious "least squares
means"
> are (which John explained lucidly), than how to replicate what SAS (or
> JMP) outputs.  I do not understand how people can feel comfortable
> reporting things like lsmeans and p-values from type <insert your
> favorite Roman numeral here> tests when they do not know how such
> things
> arise or, at the very least, what they _really_ mean.  (Given how
> simple
> lsmeans are computed, not knowing how to compute them is pretty much
> the
> same as not knowing what they are.)  One of the dangers of wholesale
> output as SAS or SPSS gives is for the user to simply pick an answer
> and
> run with it, without understanding what that answer is, or if it
> corresponds to the question of interest.
> 
> As to whether to weight the levels of the factors being held constant,
> my suggestion to John would be to offer both choices (unweighted and
> weighted by observed frequencies).  I can see why one would want to
> weight by observed frequencies (if the data are sampled from a
> population), but there are certainly situations (perhaps more often
> than
> not in the cases I've encountered) that the observed frequencies do
not
> come close to approximating what they are in the population.  In such
> cases the unweighted average would make more sense to me.
> 
> Cheers,
> Andy
> 
> 
> > > > -----Original Message-----
> > > > From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> > > > bounces at stat.math.ethz.ch] On Behalf Of John Fox
> > > > Sent: Wednesday, March 21, 2007 8:59 PM
> > > > To: 'Prof Brian Ripley'
> > > > Cc: 'r-help'; 'Chuck Cleland'
> > > > Subject: Re: [R] how to get "lsmeans"?
> > > >
> > > > Dear Brian et al.,
> > > >
> > > > My apologies for chiming in late: It's been a busy day.
> > > >
> > > > First some general comments on "least-squares means" and "effect
> > > > displays."
> > > > The general idea behind the two is similar -- to examine fitted
> > > > values corresponding to a term in a model while holding
> > other terms
> > > > to
> > > typical
> > > > values -- but the implementation is not identical. There are
also
> > > other
> > > > similar ideas floating around as well. My formulation is more
> > > > general in the sense that it applies to a wider variety
> > of models,
> > > > both linear and otherwise.
> > > >
> > > > "Least-squares means" (a horrible term, by the way: in a
> > 1980 paper
> > > > in the American Statistician, Searle, Speed, and Milliken
> > suggested
> > > > the more descriptive term "population marginal means") apply to
> > > > factors and combinations of factors; covariates are set to mean
> > > > values and the levels of other factors are averaged over,
> > in effect
> > > > applying equal weight to each level. (This is from
> > memory, so it's
> > > > possible that I'm not getting it quite right, but I
> > believe that I
> > > > am.) In my effect displays, each level of
> > > a
> > > > factor is weighted by its proportion in the data. In
> > models in which
> > > > least-squares means can be computed, they should differ from the
> > > > corresponding effect display by a constant (if there are
> > different
> > > > numbers of observations in the different levels of the
> > factors that
> > > > are held constant).
> > > >
> > > > The obstacle to computing either least-squares means or effect
> > > displays
> > > > in R
> > > > via predict() is that predict() wants factors in the "new
> > data" to
> > > > be set to particular levels. The effect() function in the
effects
> > > > package bypasses
> > > > predict() and works directly with the model matrix,
> > averaging over
> > > > the columns that pertain to a factor (and reconstructing
> > > > interactions as necessary). As mentioned, this has the effect of
> > > > setting the factor to its proportional distribution in the data.
> > > > This approach also has the advantage of being invariant
> > with respect
> > > > to the choice of contrasts for a factor.
> > > >
> > > > The only convenient way that I can think of to implement
> > > > least-squares means in R would be to use deviation-coded
> > regressors
> > > > for a factor (that is,
> > > > contr.sum) and then to set the columns of the model matrix for
> the
> > > > factor(s)
> > > > to be averaged over to 0. It may just be that I'm having
> > a failure
> > > > of imagination and that there's a better way to proceed. I've
not
> > > > implemented this solution because it is dependent upon
> > the choice of
> > > > contrasts and because I don't see a general advantage to it, but
> > > > since the issue has come up several times now, maybe I
> > should take a
> > > > crack at it. Remember that I want this to work more
> > generally, not
> > > > just for levels of factors, and not just for linear models.
> > > >
> > > > Brian is quite right in mentioning that he suggested some time
> ago
> > > that
> > > > I
> > > > use critical values of t rather than of the standard normal
> > > > distribution for producing confidence intervals, and I
> > agree that it
> > > > makes sense to do so in models in which the dispersion is
> > estimated.
> > > > My only excuse for not
> > > yet
> > > > doing this is that I want to undertake a more general revision
of
> > > > the effects package, and haven't had time to do it. There are
> > > > several changes that I'd like to make to the package. For
> > example, I
> > > > have results for multinomial and proportional odds logit models
> > > > (described in a paper
> > > by
> > > > me
> > > > and Bob Andersen in the 2006 issue of Sociological
> > Methodology) that
> > > > I want to incorporate, and I'd like to improve the
> > appearance of the
> > > > default graphs. But Brian's suggestion is very
> > straightforward, and
> > > > I guess that I shouldn't wait to implement it; I'll do so
> > very soon.
> > > >
> > > > Regards,
> > > >  John
> > > >
> > > > --------------------------------
> > > > John Fox
> > > > Department of Sociology
> > > > McMaster University
> > > > Hamilton, Ontario
> > > > Canada L8S 4M4
> > > > 905-525-9140x23604
> > > > http://socserv.mcmaster.ca/jfox
> > > > --------------------------------
> > > >
> > > > > -----Original Message-----
> > > > > From: r-help-bounces at stat.math.ethz.ch
> > > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > Prof Brian
> > > > > Ripley
> > > > > Sent: Wednesday, March 21, 2007 12:03 PM
> > > > > To: Chuck Cleland
> > > > > Cc: r-help
> > > > > Subject: Re: [R] how to get "lsmeans"?
> > > > >
> > > > > On Wed, 21 Mar 2007, Chuck Cleland wrote:
> > > > >
> > > > > > Liaw, Andy wrote:
> > > > > >> I verified the result from the following with output
> > from JMP 6
> > > on
> > > > > >> the same data (don't have SAS: don't need it):
> > > > > >>
> > > > > >> set.seed(631)
> > > > > >> n <- 100
> > > > > >> dat <- data.frame(y=rnorm(n), A=factor(sample(1:2, n,
> > > > > replace=TRUE)),
> > > > > >>                   B=factor(sample(1:2, n, replace=TRUE)),
> > > > > >>                   C=factor(sample(1:2, n, replace=TRUE)),
> > > > > >>                   d=rnorm(n))
> > > > > >> fm <- lm(y ~ A + B + C + d, dat) ## Form a data
> > frame of points
> > > > > >> to predict: all
> > > > > combinations of the ##
> > > > > >> three factors and the mean of the covariate.
> > > > > >> p <- data.frame(expand.grid(A=1:2, B=1:2, C=1:2)) p[] <-
> > > lapply(p,
> > > > > >> factor) p <- cbind(p, d=mean(dat$d)) p <-
> > > > > cbind(yhat=predict(fm, p),
> > > > > >> p) ## lsmeans for the three factors:
> > > > > >> with(p, tapply(yhat, A, mean))
> > > > > >> with(p, tapply(yhat, B, mean))
> > > > > >> with(p, tapply(yhat, C, mean))
> > > > > >
> > > > > >  Using Andy's example data, these are the LSMEANS and
> > > > > intervals I get
> > > > > > from SAS:
> > > > > >
> > > > > > A        y LSMEAN      95% Confidence Limits
> > > > > > 1       -0.071847       -0.387507     0.243813
> > > > > > 2       -0.029621       -0.342358     0.283117
> > > > > >
> > > > > > B        y LSMEAN      95% Confidence Limits
> > > > > > 1       -0.104859       -0.397935     0.188216
> > > > > > 2        0.003391       -0.333476     0.340258
> > > > > >
> > > > > > C        y LSMEAN      95% Confidence Limits
> > > > > > 1       -0.084679       -0.392343     0.222986
> > > > > > 2       -0.016789       -0.336374     0.302795
> > > > > >
> > > > > >  One way of reproducing the LSMEANS and intervals
> > from SAS using
> > > > > > predict() seems to be the following:
> > > > > >
> > > > > >> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d,
> > > > > data = dat)
> > > > > >> newdat <-
> > > > > >> expand.grid(A=factor(c(1,2)),B=1.5,C=1.5,d=mean(dat$d))
> > > > > >> cbind(newdat, predict(dat.lm, newdat,
> interval="confidence"))
> > > > > >  A   B   C          d         fit        lwr       upr
> > > > > > 1 1 1.5 1.5 0.09838595 -0.07184709 -0.3875070 0.2438128
> > > > > > 2 2 1.5 1.5 0.09838595 -0.02962086 -0.3423582 0.2831165
> > > > > >
> > > > > >  However, another possibility seems to be:
> > > > > >
> > > > > >> dat.lm <- lm(y ~ A + as.numeric(B) + as.numeric(C) + d,
> > > > > data = dat)
> > > > > >> newdat <-
> > > > > >
> > > > >
> > > >
> > expand.grid(A=factor(c(1,2)),B=mean(as.numeric(dat$B)),C=mean(as.num
> > > > er
> > > > > > ic(dat$C)),d=mean(dat$d))
> > > > > >> cbind(newdat, predict(dat.lm, newdat,
> interval="confidence"))
> > > > > >  A    B    C          d         fit        lwr       upr
> > > > > > 1 1 1.43 1.48 0.09838595 -0.08078243 -0.3964661 0.2349012
> > > > > > 2 2 1.43 1.48 0.09838595 -0.03855619 -0.3479589 0.2708465
> > > > > >
> > > > > >  The predictions directly above match what effect() in the
> > > > > > effects package by John Fox returns:
> > > > > >
> > > > > > library(effects)
> > > > > >
> > > > > >> effect("A", fm, xlevels=list(d = mean(dat$D)))
> > > > > >
> > > > > > A effect
> > > > > > A
> > > > > >          1           2
> > > > > > -0.08078243 -0.03855619
> > > > > >
> > > > > >  But for some reason the predict() and effect() intervals
> > > > > are a little
> > > > > > different:
> > > > > >
> > > > > >> effect("A", fm, xlevels=list(d = mean(dat$D)))$lower
> > > > > >          [,1]
> > > > > > 101 -0.3924451
> > > > > > 102 -0.3440179
> > > > > >
> > > > > >> effect("A", fm, xlevels=list(d = mean(dat$D)))$upper
> > > > > >         [,1]
> > > > > > 101 0.2308802
> > > > > > 102 0.2669055
> > > > > >
> > > > > >  I would be interested in any comments on these different
> > > > > approaches
> > > > > > and on the difference in intervals returned by predict()
> > > > > and effect().
> > > > >
> > > > > AFAIR, the effects packages uses normal-based
> > confidence intervals
> > > > > and predict.lm uses t-based ones, and I have suggested
> > to John Fox
> > > > > that t-based intervals would be preferable, at least as
> > an option.
> > > > >
> > > > >
> > > > > --
> > > > > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > > > > Professor of Applied Statistics,
> > http://www.stats.ox.ac.uk/~ripley/
> > > > > University of Oxford,             Tel:  +44 1865 272861 (self)
> > > > > 1 South Parks Road,                     +44 1865 272866 (PA)
> > > > > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > > > >
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide
> > > > > http://www.R-project.org/posting-guide.html
> > > > > and provide commented, minimal, self-contained,
> > reproducible code.
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-
> > > > guide.html and provide commented, minimal, self-contained,
> > > > reproducible code.
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> 
> 
>
-----------------------------------------------------------------------
> -------
> Notice:  This e-mail message, together with any attachment...{{dropped}}


From mcpung at gmail.com  Sat Mar 24 06:02:38 2007
From: mcpung at gmail.com (Murray Pung)
Date: Sat, 24 Mar 2007 16:02:38 +1100
Subject: [R] Greenwood variance formula
Message-ID: <8d6f66050703232202od1f1920jfec52b60a35dce38@mail.gmail.com>

I have specified error = "greenwood" in the survfit function:

summary(survfit(Surv(time, status) ~ 1,error = "greenwood", data=aml))

The output does not appear to give variance, but standard error.
Preferably I  would like variance to be output, but is it okay to
convert std errror to variance, and how can I do this correctly?

Thanks
Murray


From aiminy at iastate.edu  Sat Mar 24 06:10:04 2007
From: aiminy at iastate.edu (Aimin Yan)
Date: Sat, 24 Mar 2007 00:10:04 -0500
Subject: [R] how to get current workspace name?
Message-ID: <6.1.2.0.2.20070323235947.01d15ed0@aiminy.mail.iastate.edu>

Use getwd(), I can get current directory name.

Now I want to get current workspace name, Does anyone know how to do that?

here is information about my R

 > sessionInfo()
R version 2.4.0 (2006-10-03)
i386-pc-mingw32

locale:
LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
States.1252;LC_MONETARY=English_United 
States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252

attached base packages:
[1] "splines"   "methods"   "stats"     "graphics"  "grDevices" "utils"
[7] "datasets"  "base"

other attached packages:
     nnet    class survival  mlbench     MASS    rpart  RWinEdt
"7.2-29" "7.2-29"   "2.30"  "1.1-2" "7.2-29" "3.1-32"  "1.7-5"

Aimin


From maitra at iastate.edu  Sat Mar 24 06:43:06 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Sat, 24 Mar 2007 00:43:06 -0500
Subject: [R] sampling from the unoform distrubuton over a convex hull
Message-ID: <20070324004306.70667959@triveni.stat.iastate.edu>

Dear list,

Does anyone have a suggestion (or better still) code for sampling from the uniform distribution over the convex hull of a set of points?

Many thanks and best wishes,
Ranjan


From jim at bitwrit.com.au  Sat Mar 24 07:39:07 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Sat, 24 Mar 2007 17:39:07 +1100
Subject: [R] Colored boxes with values in the box
In-Reply-To: <f1eb1ddd20f938b1ca60c9e0e3e1c5cd@mednet.ucla.edu>
References: <f1eb1ddd20f938b1ca60c9e0e3e1c5cd@mednet.ucla.edu>
Message-ID: <4604C78B.2040109@bitwrit.com.au>

Pappu, Kartik wrote:
> Hi all,
> 
> I have a x, y matrix of numbers (usually ranging from 0 to 40).  I need 
> to group these numbers and assign a color to each group (for example 0 
> to 15 - Blue, 16-30- Yellow, and 31-40- Red).  Then I need to draw a 
> rectangular matrix which contains X x Y boxes and each box has the  
> corresponding value from the input matrix and is also colored according 
> to which group (i.e red, yellow, or blue) that value falls into.
> 
> I have used the color2D.matplot function from the plotrix package, but 
> I cant quite figure out how to group the values to represent red blue 
> and yellow colors.
> 
Hi Kartik,
color2D.matplot isn't designed for this, but with a few tricks, I think 
you can get what you want. Try this:

xmat<-matrix(sample(0:40,100,TRUE),nrow=10)
# trick 1 - calculate values that give you the colors for ranges
color2D.matplot(floor(xmat/15+1),c(1,1,0),c(0,1,0),c(0,0,1))
# trick 2 - Use boxed.labels rather than the inbuild value display
boxed.labels(sort(rep(9.5:0.5,10)),rep(seq(9.5,0.5,by=-1),10),xmat)
# trick 3 - Use color.legend to show the group colors
color.legend(0,-2,3,-1.3,1:3,c("red","yellow","blue"))

You will have to wait until plotrix v2.2 is on CRAN as I am just 
submitting it, and the color.legend function, requested only a few weeks 
ago, is new to that version.

Jim


From jim at bitwrit.com.au  Sat Mar 24 07:46:54 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Sat, 24 Mar 2007 17:46:54 +1100
Subject: [R] Truncated x-axis values
In-Reply-To: <610773.18551.qm@web32815.mail.mud.yahoo.com>
References: <610773.18551.qm@web32815.mail.mud.yahoo.com>
Message-ID: <4604C95E.2050009@bitwrit.com.au>

"Urban, Alexander" <a-urban at ti.com> wrote:
> Hello
>
>I'm new to this group. I looked up the last two
>
>hour in the help file 
>
> and in the archives of this group, but didn't
>
>find anything.
>
>I hope my question is not too dump:
>I'm printing a graph with vertical labels on the
>
>x-axis (necessary due
>
>to many labels). Unfortunately the png truncates
>
>the labels halfway 
>
>through, so that you can only read the last 7
>
>digits of the label.
>
>Snice I'm already asking :-): Is there a
>
>possibility to tell R: If 
>
>there are so many labels that you write them on
>
>top of each other, 
>
>take only e.g. every 2nd...
>
There sure is, Alex. Have a look at staxlab in the plotrix package.

Jim


From murdoch at stats.uwo.ca  Sat Mar 24 12:30:37 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 24 Mar 2007 07:30:37 -0400
Subject: [R] how to get current workspace name?
In-Reply-To: <6.1.2.0.2.20070323235947.01d15ed0@aiminy.mail.iastate.edu>
References: <6.1.2.0.2.20070323235947.01d15ed0@aiminy.mail.iastate.edu>
Message-ID: <46050BDD.40405@stats.uwo.ca>

On 3/24/2007 1:10 AM, Aimin Yan wrote:
> Use getwd(), I can get current directory name.
> 
> Now I want to get current workspace name, Does anyone know how to do that?

It is search()[1].

If you mean instead the name of the file that was loaded on startup, as 
far as I know that's not available in R.  If you mean the default name 
that would be saved on shutdown, that is documented in the save.image() 
man page:  it's .RData in the current directory.

Duncan Murdoch

> 
> here is information about my R
> 
>  > sessionInfo()
> R version 2.4.0 (2006-10-03)
> i386-pc-mingw32
> 
> locale:
> LC_COLLATE=English_United States.1252;LC_CTYPE=English_United 
> States.1252;LC_MONETARY=English_United 
> States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252
> 
> attached base packages:
> [1] "splines"   "methods"   "stats"     "graphics"  "grDevices" "utils"
> [7] "datasets"  "base"
> 
> other attached packages:
>      nnet    class survival  mlbench     MASS    rpart  RWinEdt
> "7.2-29" "7.2-29"   "2.30"  "1.1-2" "7.2-29" "3.1-32"  "1.7-5"
> 
> Aimin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rahmouni_mo at yahoo.fr  Sat Mar 24 12:39:10 2007
From: rahmouni_mo at yahoo.fr (MOHIEDDINE RAHMOUNI)
Date: Sat, 24 Mar 2007 12:39:10 +0100 (CET)
Subject: [R] bootstrapping of standard error estimates
Message-ID: <901611.90655.qm@web26312.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070324/d9f7d4af/attachment.pl 

From murdoch at stats.uwo.ca  Sat Mar 24 12:52:54 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 24 Mar 2007 07:52:54 -0400
Subject: [R] sampling from the unoform distrubuton over a convex hull
In-Reply-To: <20070324004306.70667959@triveni.stat.iastate.edu>
References: <20070324004306.70667959@triveni.stat.iastate.edu>
Message-ID: <46051116.70404@stats.uwo.ca>

On 3/24/2007 1:43 AM, Ranjan Maitra wrote:
> Dear list,
> 
> Does anyone have a suggestion (or better still) code for sampling from the uniform distribution over the convex hull of a set of points?

Are you talking about two dimensional points, or higher dimensions?  The 
suggestion below works for any dimension, but the actual code is 
2-dimensional. I don't know if there's an equivalent of chull available 
for higher dimensions.

Suggestion:  Find a rectangular region containing the hull, and sample 
uniformly there.  Accept points that don't expand the hull of the 
original points.

For example:

rhull <- function(n,x) {
   boundary <- x[chull(x),]

   xlim <- range(boundary[,1])
   ylim <- range(boundary[,2])

   boundary <- rbind(c(NA,NA), boundary)  # add space for new test point

   result <- matrix(NA, n, 2)

   for (i in 1:n) {
     repeat {
       boundary[1,1] <- runif(1, xlim[1], xlim[2])
       boundary[1,2] <- runif(1, ylim[1], ylim[2])
       if ( !(1 %in% chull(boundary)) ) {
	  result[i,] <- boundary[1,]
         break
       }
     }
   }

   result
}

x <- matrix(rnorm(20), ncol=2)
plot(x, cex=2, col="red")
sample <- rhull(1000, x)
points(sample)

Duncan Murdoch


From stefan.na at gmx.net  Sat Mar 24 13:41:15 2007
From: stefan.na at gmx.net (Stefan Nachtnebel)
Date: Sat, 24 Mar 2007 13:41:15 +0100
Subject: [R] frequency tables and sorting by rowSum
In-Reply-To: <mailman.15.1174734004.3519.r-help@stat.math.ethz.ch>
References: <mailman.15.1174734004.3519.r-help@stat.math.ethz.ch>
Message-ID: <20070324124115.20060@gmx.net>

Dear list, 

I have some trouble generating a frequency table over a number of vectors. 
Creating these tables over simple numbers is no problem with table()

> table(c(1,1,1,3,4,5))

1 3 4 5
3 1 1 1


, but how can i for example turn:
0 1 0
0 0 1
0 1 0
1 0 0
0 1 0
1 0 0

into

0 0 1 1
1 0 0 2
0 1 0 3

My second problem is, sorting rows and columns of a matrix by the rowSums/colSums.
I did it this way, but i think there should be a more efficient way:

sortRowCol<-function(taus) {
  swaprow <- function(rsum) {
    taus[(rowSums(taus)==rsum),]
  }
  for( i in 1:2 )
     taus<-sapply(sort(rowSums(taus)),swaprow)
}

thanks in advantage, Stefan Nachtnebel

-- 
"Feel free" - 5 GB Mailbox, 50 FreeSMS/Monat ...
Jetzt GMX ProMail testen: www.gmx.net/de/go/mailfooter/promail-out


From lists at eva.mpg.de  Sat Mar 24 13:51:34 2007
From: lists at eva.mpg.de (Cristina Gomes)
Date: Sat, 24 Mar 2007 13:51:34 +0100
Subject: [R] p values in lme4 package
Message-ID: <46051ED6.7070205@eva.mpg.de>

Dear R-users,
I was wondering if anybody knows if it's possible to obtain a p value 
for the full model of a GLMM with the lme4 package. I was told that I 
should check whether the full model including all the predictor 
variables is significant before doing stepwise regression or further 
analysis, but I can't figure out how to do this. I also wanted to know 
if there's a way of obtaining residuals or predicted values for the same 
analysis.
Thank you very much.
Cheers,
Cristina.


From andy_liaw at merck.com  Sat Mar 24 14:05:48 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 24 Mar 2007 09:05:48 -0400
Subject: [R] sampling from the unoform distrubuton over a convex hull
References: <20070324004306.70667959@triveni.stat.iastate.edu>
	<46051116.70404@stats.uwo.ca>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA01BD0E90@usctmx1106.merck.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070324/0c5acc8f/attachment.pl 

From andy_liaw at merck.com  Sat Mar 24 14:13:57 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 24 Mar 2007 09:13:57 -0400
Subject: [R] frequency tables and sorting by rowSum
References: <mailman.15.1174734004.3519.r-help@stat.math.ethz.ch>
	<20070324124115.20060@gmx.net>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA01BD0E91@usctmx1106.merck.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070324/aab3a843/attachment.pl 

From ted.harding at nessie.mcc.ac.uk  Sat Mar 24 15:00:44 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 24 Mar 2007 14:00:44 -0000 (GMT)
Subject: [R] sampling from the unoform distrubuton over a convex hull
In-Reply-To: <20070324004306.70667959@triveni.stat.iastate.edu>
Message-ID: <XFMail.070324140044.ted.harding@nessie.mcc.ac.uk>

On 24-Mar-07 05:43:06, Ranjan Maitra wrote:
> Dear list,
> 
> Does anyone have a suggestion (or better still) code for sampling from
> the uniform distribution over the convex hull of a set of points?
> 
> Many thanks and best wishes,
> Ranjan

I was curious if anyone would come up with some ready-made efficient
code for this problem -- it cannot be the first time it has been
addressed! But if Duncan Murdoch doesn't, that reduces the probability
that such code is already available in R!

Duncan's suggestion of a rejection method for uniform points in an
enclosing rectangle will probably be efficient, provided the convex
hull occupies a good proportion of the rectangle. So I would suggest
for this method that a preliminary transformation be made, rotating
onto principal axes of the convex hull. This would avoid the situation
where the convex hull is a narrow cigar-shape at 45 degrees. At the
end, transform back to the original coordinates.

Another possible approach (again in two dimensions) could be based
on the following.

First, if A, B, C are the vertices of a triangle, let (w1,w2,w3)
be sampled from the 3-variate Dirichlet distribution with index
("shape") parameters (1,1,1). Then the weighted sum

   w1*A + w2*B + w3*C

is uniformly distributed over the triangle.[1] (This does not
generalise to planar convex hulls with more than three vertices).

Next, triangulate the convex hull (e.g. joining each vertex to the
centroid of the convex hull, getting K triangles, say), and calculate
the area of each triangle. Then, to sample N points, partition N at
random into K parts with probabilities proportional to the areas.
For instance, by cutting

sort(runif(N))

at the breakpoints given by

cumsum(A)/sum(A)

where A is a vector of areas

Then, for each component Nj of Ns, sample Nj points uniformly
over triangle j using the Dirichlet method above.

[1] This can be seen geometrically: (w1,w2,w3) is uniformly
distributed over the triangle T1 with vertices (1,0,0), (0,1,0)
and (0,0,1). Given any other triangle T2, by rotating T1 in
space and projecting orthogonally onto the plane containing T2,
you can match 2 (and therefore all 3) of the angles of T1 with
the angles of T2. Then dilate the projection of T1 until it
is congruent with T2. Since equal areas project orthogonally
onto equal areas, a uniform distribution on T1 projects into
a uniform on the projection of T1, therefore on T2.


PS I could envisage the above approach being generalised
to more than 2 dimensions. In 3 dimensions, for instance,
since the Dirchlet(1,1,1,1) is uniform on the simplex with
vertices (1,0,0,0), (0,1,0,0), (0,0,1,0), (0,0,0,1) we
similarly have that for a general simplex with vertices
A,B,C,D the point w1*A + w2*B + w3*C + w4*D is uniformly
distributed in the simplex.

But this requires "simplectification" of the convex hull,
(e.g. joining the vertices of its outer faces to its centroid)
so it gets more complicated, so no doubt Duncan's proposal
wins on simplicity, if not on efficiency (since the more
dimensions, the greater the proportion of points rejected).

Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 24-Mar-07                                       Time: 14:00:40
------------------------------ XFMail ------------------------------


From albmont at centroin.com.br  Sat Mar 24 15:57:14 2007
From: albmont at centroin.com.br (Alberto Vieira Ferreira Monteiro)
Date: Sat, 24 Mar 2007 14:57:14 +0000
Subject: [R] sampling from the unoform distrubuton over a convex hull
In-Reply-To: <46051116.70404@stats.uwo.ca>
References: <20070324004306.70667959@triveni.stat.iastate.edu>
	<46051116.70404@stats.uwo.ca>
Message-ID: <200703241457.14973.albmont@centroin.com.br>

Duncan Murdoch wrote:
>
> Suggestion:  Find a rectangular region containing the hull, and sample
> uniformly there.  Accept points that don't expand the hull of the
> original points.
>
This is a feasible solution only in lower dimensions; the area of the
convex hull can become exponentially small relative to its external
rectangle. For example, if the convex hull is approximately spherical,
the relation of the (hyper)volumes are (time to summon Wikipedia)...
http://en.wikipedia.org/wiki/Hypersphere#Hyperspherical_volume
...

Volume.ratio <- pi^(n/2) * 2^(-n) / gamma(n/2 + 1)

Even for n = 5 this would be 0.16

The hypervolume of a _hyperpyramidal_ convex hull would be even
worse, as it would be 1/n! (= 1/gamma(n+1)) of the hypervolume of the
hypercube.

Alberto Monteiro


From ligges at statistik.uni-dortmund.de  Sat Mar 24 16:27:38 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 24 Mar 2007 16:27:38 +0100
Subject: [R] Space for X and Y axis labels
In-Reply-To: <46043081.3030104@webdrake.net>
References: <46043081.3030104@webdrake.net>
Message-ID: <4605436A.1010705@statistik.uni-dortmund.de>

Your exampleisnot reproducible, but I guess you want to change the 
par("mar") setting ...

Uwe Ligges


Joseph Wakeling wrote:
> Hello all,
> 
> I'm having a bit of a problem with x and y axis labels.  Two things:
> first, if I want to create a plot with,
> 
>     plot.new()
>     plot.window(.....)
>     axis(1)
>     axis(2)
>     lines(...)
>     points(...)
> 
>     [etc.]
> 
> ... where do I introduce the xlab=... and ylab=... commands?  I
> attempted this in plot.window() but no labels showed up.
> 
> Second, here's a bit of real code...
> 
> 	postscript(file="RPDfig.eps",onefile=FALSE,
> 	 +         horizontal=FALSE,paper="special",height=8.3,
>          +         width=11.7)
> 	matplot(2:length(PRICE),t(NWpd),type="l",xlim=c(0,200),
> 	 +      ylim=c(-0.1,0.1),xaxs="i",yaxs="i",xlab="Time",
> 	 +      ylab=expression(over(x,y)),bty="n")
> 	points(2:length(PRICE),Ppd,col="red")
> 	dev.off()
> 
> Here the y label is meant to be x over y (a fraction), but the top part
> of the fraction is cut off by the border of the graphic.  How can I fix
> this, either by creating extra space for the label or by insisting that
> the boundary of the graphic be extended to embrace the whole of the
> label?  I've tried setting xpd=TRUE in the matplot command, but I'm not
> sure that is the correct option (and it makes no difference).
> 
> Many thanks,
> 
>     -- Joe
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From delcour.libertus at gmail.com  Sat Mar 24 18:35:59 2007
From: delcour.libertus at gmail.com (Delcour Libertus)
Date: Sat, 24 Mar 2007 18:35:59 +0100
Subject: [R] Two Problems while trying to aggregate a dataframe
Message-ID: <4605617F.5030903@gmail.com>

Hello!

Given is an Excel-Sheet with actually 11,000 rows and 9 columns. I want
to work with the data in R. The contents are similar to my following
example.

I have a list with ID-number, personal name and two kinds of
loan-values. I want to aggregate the list, that for each person only one
row remains and where the loan-values are added.

First I tried some commands with tapply but had no success at all. Then
I found in this mailing list a hint for aggregate (though I did not
understand most of that mail).

So I made some efforts with aggregate() and it seems to lead the right way:

[code]
> atest <- read.csv2 ("aggregatetest.csv")
> str(atest)
`data.frame':   10 obs. of  4 variables:
 $ PrsNr  : int  1 2 2 3 4 5 6 6 6 7
 $ Namen  : Factor w/ 7 levels "Holla","Mabba",..: 1 2 2 4 5 6 7 7 7 3
 $ Betrag1: num  1.99 2.34 5.23 4.23 2.23 2.77 3.83 2.76 6.32 2.88
 $ Betrag2: num  3.44 5.32 5.21 9.12 7.32 8.32 6.99 4.45 5.34 3.81
> atest
   PrsNr Namen Betrag1 Betrag2
1      1 Holla    1.99    3.44
2      2 Mabba    2.34    5.32
3      2 Mabba    5.23    5.21
4      3  Pisa    4.23    9.12
5      4 Pulla    2.23    7.32
6      5  Raba    2.77    8.32
7      6  Saba    3.83    6.99
8      6  Saba    2.76    4.45
9      6  Saba    6.32    5.34
10     7 Mulla    2.88    3.81
> aggregate(list(Betrag1=atest$Betrag1),  by=list(PsrNr=atest$PrsNr,
Namen=atest$Namen),  sum)
  PsrNr Namen Betrag1
1     1 Holla    1.99
2     2 Mabba    7.57
3     7 Mulla    2.88
4     3  Pisa    4.23
5     4 Pulla    2.23
6     5  Raba    2.77
7     6  Saba   12.91
[/code]

The result is nearly that I want.

First problem:

How do I get all columnss in my result. "Betrag2" is missing.

Second problem:

If I use the aggregate-command on the real data then it is for me
impossible to use more than on by-grouping variable (my example above
has two). Impossible because 1 GB RAM and 1.5 GB SWAP are not enough to
process my command. My computer (Ubuntu Linux, Gmome) freezes. So I
doubt wether I use the appropriate method to follow my target.

Which ist the best way to aggregate dataframes as I want? Are there any
better functions/commands or do I have to learn programming for this?

Greetings

Delcour


From jholtman at gmail.com  Sat Mar 24 19:07:42 2007
From: jholtman at gmail.com (jim holtman)
Date: Sat, 24 Mar 2007 13:07:42 -0500
Subject: [R] Two Problems while trying to aggregate a dataframe
In-Reply-To: <4605617F.5030903@gmail.com>
References: <4605617F.5030903@gmail.com>
Message-ID: <644e1f320703241107u760fbe84kb49962390c4a32a0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070324/d86afceb/attachment.pl 

From A.Robinson at ms.unimelb.edu.au  Sat Mar 24 19:09:34 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Sun, 25 Mar 2007 04:09:34 +1000
Subject: [R] p values in lme4 package
In-Reply-To: <46051ED6.7070205@eva.mpg.de>
References: <46051ED6.7070205@eva.mpg.de>
Message-ID: <20070324180934.GQ39176@ms.unimelb.edu.au>

Dear Cristina,

On Sat, Mar 24, 2007 at 01:51:34PM +0100, Cristina Gomes wrote:
> Dear R-users,
> I was wondering if anybody knows if it's possible to obtain a p value 
> for the full model of a GLMM with the lme4 package. 

I do not believe that it is possible to do so.

> I was told that I 
> should check whether the full model including all the predictor 
> variables is significant before doing stepwise regression or further 
> analysis, but I can't figure out how to do this. I also wanted to know 
> if there's a way of obtaining residuals or predicted values for the same 
> analysis.

There is, but it is not automatic.  You have to write your own code to
compute the residuals and predicted values for each model, using the
estimates of the fixed and random effects.  lmer is still under
extensive development by its author, and the "helper" functions to
extract or compute residuals and predicted values have not yet been
written.

Cheers

Andrew

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From ggrothendieck at gmail.com  Sat Mar 24 19:17:05 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 24 Mar 2007 14:17:05 -0400
Subject: [R] Two Problems while trying to aggregate a dataframe
In-Reply-To: <4605617F.5030903@gmail.com>
References: <4605617F.5030903@gmail.com>
Message-ID: <971536df0703241117x75ba6284k7335c7928fd61745@mail.gmail.com>

Try this:

aggregate(atest[3:4], atest[1:2], sum)

Use a data base and SQL is you don't otherwise have enough
computer resources.


On 3/24/07, Delcour Libertus <delcour.libertus at gmail.com> wrote:
> Hello!
>
> Given is an Excel-Sheet with actually 11,000 rows and 9 columns. I want
> to work with the data in R. The contents are similar to my following
> example.
>
> I have a list with ID-number, personal name and two kinds of
> loan-values. I want to aggregate the list, that for each person only one
> row remains and where the loan-values are added.
>
> First I tried some commands with tapply but had no success at all. Then
> I found in this mailing list a hint for aggregate (though I did not
> understand most of that mail).
>
> So I made some efforts with aggregate() and it seems to lead the right way:
>
> [code]
> > atest <- read.csv2 ("aggregatetest.csv")
> > str(atest)
> `data.frame':   10 obs. of  4 variables:
>  $ PrsNr  : int  1 2 2 3 4 5 6 6 6 7
>  $ Namen  : Factor w/ 7 levels "Holla","Mabba",..: 1 2 2 4 5 6 7 7 7 3
>  $ Betrag1: num  1.99 2.34 5.23 4.23 2.23 2.77 3.83 2.76 6.32 2.88
>  $ Betrag2: num  3.44 5.32 5.21 9.12 7.32 8.32 6.99 4.45 5.34 3.81
> > atest
>   PrsNr Namen Betrag1 Betrag2
> 1      1 Holla    1.99    3.44
> 2      2 Mabba    2.34    5.32
> 3      2 Mabba    5.23    5.21
> 4      3  Pisa    4.23    9.12
> 5      4 Pulla    2.23    7.32
> 6      5  Raba    2.77    8.32
> 7      6  Saba    3.83    6.99
> 8      6  Saba    2.76    4.45
> 9      6  Saba    6.32    5.34
> 10     7 Mulla    2.88    3.81
> > aggregate(list(Betrag1=atest$Betrag1),  by=list(PsrNr=atest$PrsNr,
> Namen=atest$Namen),  sum)
>  PsrNr Namen Betrag1
> 1     1 Holla    1.99
> 2     2 Mabba    7.57
> 3     7 Mulla    2.88
> 4     3  Pisa    4.23
> 5     4 Pulla    2.23
> 6     5  Raba    2.77
> 7     6  Saba   12.91
> [/code]
>
> The result is nearly that I want.
>
> First problem:
>
> How do I get all columnss in my result. "Betrag2" is missing.
>
> Second problem:
>
> If I use the aggregate-command on the real data then it is for me
> impossible to use more than on by-grouping variable (my example above
> has two). Impossible because 1 GB RAM and 1.5 GB SWAP are not enough to
> process my command. My computer (Ubuntu Linux, Gmome) freezes. So I
> doubt wether I use the appropriate method to follow my target.
>
> Which ist the best way to aggregate dataframes as I want? Are there any
> better functions/commands or do I have to learn programming for this?
>
> Greetings
>
> Delcour
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Sat Mar 24 19:33:09 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 24 Mar 2007 18:33:09 +0000 (GMT)
Subject: [R] Sys.setlocale("LC_CTYPE","fr_FR.UTF-8")
In-Reply-To: <45F445AA.5050403@uhb.fr>
References: <45F445AA.5050403@uhb.fr>
Message-ID: <Pine.LNX.4.64.0703241829090.18479@gannet.stats.ox.ac.uk>

On Sun, 11 Mar 2007, NOEL Yvonnick wrote:

> Dear R users,
>
> I'm trying to have a gWiddgetsRGtk2 script run under R-2.4.1. The script
> run OK under Linux but all accentuated characters appear as "?" when the
> script is run under Windows.
>
> As Gtk+ requires UTF-8, I thought it was the source of the problem and
> tried to change the default encoding (1252) in the following way:
>
> >Sys.setlocale("LC_CTYPE","fr_FR.UTF-8")
> [1]  "fr_FR.UTF-8"
>
> But when I try as a check:
> >Sys.getlocale("LC_TYPE")
> [1] "French_France.1252"
>
> So it seems that Sys.setlocale() has not changed anything. What am I
> doing wrong ?

>From the help page:

   locale: character string.  A valid locale name on the system in use.

Your locale name is not valid on Windows: see the 'R Installation and 
Administration Manual', section 7.1 for more details.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From joseph.wakeling at webdrake.net  Sat Mar 24 19:58:00 2007
From: joseph.wakeling at webdrake.net (Joseph Wakeling)
Date: Sat, 24 Mar 2007 18:58:00 +0000
Subject: [R] Space for X and Y axis labels
In-Reply-To: <4605436A.1010705@statistik.uni-dortmund.de>
References: <46043081.3030104@webdrake.net>
	<4605436A.1010705@statistik.uni-dortmund.de>
Message-ID: <460574B8.6090201@webdrake.net>

Uwe Ligges wrote:
> Your exampleisnot reproducible, but I guess you want to change the
> par("mar") setting ...

Sorry for not adding some extra stuff to make it work.  I assumed there
wasn't a need for reproducibility, just to let me know what command was
missing.  Anyway, thanks very much, par("mar") works well. :-)

Can anyone add suggestions about where I introduce xlab and ylab if I
want to plot with lower-level commands, i.e. with plot.new(),
plot.window(), ... etc.?

Many thanks,

    -- Joe


From ligges at statistik.uni-dortmund.de  Sat Mar 24 20:03:09 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 24 Mar 2007 20:03:09 +0100
Subject: [R] Space for X and Y axis labels
In-Reply-To: <460574B8.6090201@webdrake.net>
References: <46043081.3030104@webdrake.net>
	<4605436A.1010705@statistik.uni-dortmund.de>
	<460574B8.6090201@webdrake.net>
Message-ID: <460575ED.6040701@statistik.uni-dortmund.de>



Joseph Wakeling wrote:
> Uwe Ligges wrote:
>> Your exampleisnot reproducible, but I guess you want to change the
>> par("mar") setting ...
> 
> Sorry for not adding some extra stuff to make it work.  I assumed there
> wasn't a need for reproducibility, just to let me know what command was
> missing.  Anyway, thanks very much, par("mar") works well. :-)
> 
> Can anyone add suggestions about where I introduce xlab and ylab if I
> want to plot with lower-level commands, i.e. with plot.new(),
> plot.window(), ... etc.?

See ?title or even ?mtext

Uwe Ligges



> Many thanks,
> 
>     -- Joe
>


From ted.harding at nessie.mcc.ac.uk  Sat Mar 24 20:26:21 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 24 Mar 2007 19:26:21 -0000 (GMT)
Subject: [R] sampling from the unoform distrubuton over a convex hull
In-Reply-To: <XFMail.070324140044.ted.harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.070324192621.ted.harding@nessie.mcc.ac.uk>

On 24-Mar-07 14:00:44, Ted Harding wrote:
> [...]
> Another possible approach (again in two dimensions) could be based
> on the following.
> 
> First, if A, B, C are the vertices of a triangle, let (w1,w2,w3)
> be sampled from the 3-variate Dirichlet distribution with index
> ("shape") parameters (1,1,1). Then the weighted sum
> 
>    w1*A + w2*B + w3*C
> 
> is uniformly distributed over the triangle.[1] (This does not
> generalise to planar convex hulls with more than three vertices).
> 
> Next, triangulate the convex hull (e.g. joining each vertex to the
> centroid of the convex hull, getting K triangles, say), and calculate
> the area of each triangle. Then, to sample N points, partition N at
> random into K parts with probabilities proportional to the areas.
> For instance, by cutting
> 
> sort(runif(N))
> 
> at the breakpoints given by
> 
> cumsum(A)/sum(A)
> 
> where A is a vector of areas
> 
> Then, for each component Nj of Ns, sample Nj points uniformly
> over triangle j using the Dirichlet method above.

Well, I've written some rather crude code to implement the
above. Even allowing for possible "editorial" improvements,
the more I look at it the more I think there may be a better
way! (Of doing it directly, I mean, raher than a rejection
method).

Still, it seems to work (and quite slickly) ...


## Needed for rdiric() to sample from Dirichlet
## Better to write one's own rdiric() for this application!
library(VGAM)

## Make some random points, get their CH and centroid, and draw them
X<-cbind(rnorm(10),rnorm(10))
plot(X[,1],X[,2],pch="+",col="green")
H<-chull(X)
C<-colMeans(X[H,])
points(C[1],C[2],pch="+",col="red")
## Draw the CH and the triangulation
H<-c(H,H[1])   ## To close the contour
K<-length(H)-1 ## No of triangles
lines(X[H,1],X[H,2],col="blue")
for(i in (1:K)){lines(c(X[H[i],1],C[1]),c(X[H[i],2],C[2]),col="red")}

 
## Set up the sampling by triangles
As<-numeric(K)
Ns<-numeric(K)

## Get the areas of the triangles
for(i in (1:(K))){
  V1<-X[H[i],]
  V2<-X[H[i+1],]
  V3<-C
  As[i]<-abs(det(rbind(V1-V3,V2-V3)))
}

## As illustration, 1000 points over the CH
N<-2000

## How many points to go in eavh triangle
Cuts<-cumsum(As)/sum(As)
R<-runif(N)
Ns[1]<-sum(R<=Cuts[1])
for(i in (2:K)){Ns[i]<-sum(R<=Cuts[i])-sum(R<=Cuts[i-1])}

## Uniform distribution over each triangle
for(i in (1:(K))){
  V1<-X[H[i],]
  V2<-X[H[i+1],]
  V3<-C
  ## The Dirichlet sample:
  T<-rbind(X[H[i],],X[H[i+1],],C)
  D<-rdiric(Ns[i],c(1,1,1))
  Z<-D%*%T
  points(Z[,1],Z[,2],pch="+",col="blue")
}


Any advance on the above??

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 24-Mar-07                                       Time: 19:26:18
------------------------------ XFMail ------------------------------


From tonyyangsxz at gmail.com  Sat Mar 24 20:13:10 2007
From: tonyyangsxz at gmail.com (Tony Yang)
Date: Sat, 24 Mar 2007 15:13:10 -0400
Subject: [R] Biometrics Latex template
Message-ID: <f00d287f0703241213k389233a3p76304b35697c34d0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070324/16136211/attachment.pl 

From rihicks at catalinasolutions.com  Sat Mar 24 21:05:17 2007
From: rihicks at catalinasolutions.com (Bernadine GMauricio)
Date: Sat, 24 Mar 2007 21:05:17 +0100
Subject: [R] prow blythe
Message-ID: <061101c76f07$3faa52a0$8755d6a0@xtropospherej>

Our Last pick Doubled in 48 hours

Check this company out
CRITICAL Care N E W
SYM-C-C-T-I
Stron g B reccomended at 20 Cents
This is projected to go to $1

This is a Real Business not a fly by night
Get in Monday, Don't Regret later!!

 two free throws made it 109-73 in the waning seconds of the third quarter.    in a Freedom of Information Act request.  Amaker had to be employed as Michigan's  through the 2010-11 season, but the school could fire him without cause by giving  a list of candidates, but declined to identify any of the coaches.  "Michigan's

----- Original Message ----- 
From: "Bernadine GMauricio" <rihicks at catalinasolutions.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, March 22, 2007 8:27 PM
Subject: prow blythe


> Check this company out
> CRITICAL Care N E W
> SYM-C-C-T-I
> Stron g B reccomended at 20 Cents
> This is projected to go to $1


From murdoch at stats.uwo.ca  Sun Mar 25 00:46:50 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 24 Mar 2007 19:46:50 -0400
Subject: [R] Sweave question: prevent expansion of unevaluated reused
 code	chunk
In-Reply-To: <45F8173F.8010906@mdacc.tmc.edu>
References: <45F72D9C.5000307@mdacc.tmc.edu> <45F7D419.7040406@stats.uwo.ca>
	<45F8173F.8010906@mdacc.tmc.edu>
Message-ID: <4605B86A.5090308@stats.uwo.ca>

On 3/14/2007 11:39 AM, Kevin R. Coombes wrote:
> Hi,
> 
> I don't know of a standard way to indicate this; I would have suggested
> 
> <<combined,expand=FALSE>>
> 
> (with expand=TRUE the default), except for the fact that Seth Falcon 
> already suggested the same notation in his response...so I can only 
> second the motion.

This is now in R-devel, to become R 2.5.0 next month.

Duncan Murdoch

> 
> 	Kevin
> 
> Duncan Murdoch wrote:
>> On 3/13/2007 7:02 PM, Kevin R. Coombes wrote:
>>> Hi,
>>>
>>> Consider the following (much simplified) Sweave example:
>>>
>>> --------------
>>>
>>> First, we set the value of $x$:
>>> <<chunk1,eval=FALSE>>=
>>> x <- 1
>>> @
>>>
>>> Then we set the value of $y$:
>>> <<chunk2,eval=FALSE>>=
>>> y <- 2
>>> @
>>>
>>> Thus, the overall algorithm has this structure:
>>> <<combined,eval=FALSE>>=
>>> <<chunk1>>
>>> <<chunk2>>
>>> @
>>>
>>> <<justDoIt,echo=FALSE>>=
>>> <<combined>>
>>> @
>>>
>>> ---------------
>>>
>>> I'd like to be able to do something like this, where the "combined" 
>>> chunk prints out in the final LaTeX document essentially verbatim.  In 
>>> particular, I want to see the "<<chunk1>>" unexpanded in that block, 
>>> since this gives me a nice conceptual overview of the algorithm. (Of 
>>> courser, this is more useful when chunk1 and chunk2 are much longer 
>>> than they are in this example....)
>>>
>>> Is there an option that allows me to get this behavior?
>> As others have said, the answer is currently no, but in R 2.5.0 this 
>> should be a relatively easy modification (because it has the ability to 
>> echo your input, rather than a deparsed version of it).  In the other 
>> platforms you've used, is there a standard syntax to indicate whether or 
>> not you want the chunks expanded?  I can see either behaviour as being 
>> desirable in different circumstances.  Sometimes you want the reader to 
>> know about your chunk names, and sometimes you don't.
>>
>> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tobygass at warnercnr.colostate.edu  Sun Mar 25 01:30:27 2007
From: tobygass at warnercnr.colostate.edu (Toby Gass)
Date: Sat, 24 Mar 2007 18:30:27 -0600
Subject: [R] Conversion from string to date type
Message-ID: <001601c76e74$c9eb4810$5068520a@Q11>

Hello, Andreas


I'm glad the previous replies helped you solve your problem.
I think your original problem, though,
was caused by a typo.  Note
the comma between the month and the year in your script and 
the
period between the month and the year in Jim Holtman's 
script.

>From Jim Holtman:

> Here is how you can convert them to a Date object:
>
>> x <- c('01.03.2007','02.03.2007','03.03.2007')
>> y <- as.Date(x, format="%d.%m.%Y")
>> y

>From Andreas Tillea:
>Well, this is what I tried when reading the docs, but

> mydata <- read.csv(file='mydata.dat', sep = '\t', 
> quote='', fill=TRUE,header=TRUE )
> datum <- as.Date(mydata["date"], "%d.%m,%y")
>Error in as.Date.default(mydata["date"], "%d.%m,%y") :
         do not know how to convert 'mydata["date"]' to 
class "Date"

>I also tried:

> datum <- strptime(imydata["date"], "%d.%m,%y")
> datum
>[1] NA
Hope this helps for the future

Toby

Toby Gass
Department of Forest, Rangeland, and Watershed Stewardship
Warner College of Natural Resources
Colorado State University
Fort Collins, CO  80523


From dfolkins at temple.edu  Sun Mar 25 03:47:56 2007
From: dfolkins at temple.edu (Daniel Folkinshteyn)
Date: Sat, 24 Mar 2007 21:47:56 -0400
Subject: [R] Contatenating data frames with partial overlap in variable names
Message-ID: <4605D4CC.7050502@temple.edu>

Greetings to all.
I need to concatenate data frames that do not have all the same variable
names, there is only a partial overlap in the variables. So, for
example, if i have two data frames, a and b, that look like the following:
> a
  a b
1 1 4
2 2 5
3 3 6
4 4 7
5 5 8
> b
  c  a
1 1 10
2 2 11
3 3 12
4 4 13
5 5 14

i want to concatenate them by row, without any matching, so that the
variables that are not available in all frames get NAs. The result
should look like:

   a  b  c
1  1  4  NA
2  2  5  NA
3  3  6  NA
4  4  7  NA
5  5  8  NA
6  10 NA 1
7  11 NA 2
8  12 NA 3
9  13 NA 4
10 14 NA 5

rbind doesn't work, since it requires all variables to be matched
between the two data frames. merge doesn't work, since it wants to
/match/ by columns with the same name, and if matching by nothing,
produces a cartesian product.

is there a neat trick for doing this simply, or am i stuck with
comparing variable lists and generating NAs manually?

would appreciate any help!
Daniel


From marc_schwartz at comcast.net  Sun Mar 25 04:00:32 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 24 Mar 2007 21:00:32 -0500
Subject: [R] Contatenating data frames with partial overlap in
	variable	names
In-Reply-To: <4605D4CC.7050502@temple.edu>
References: <4605D4CC.7050502@temple.edu>
Message-ID: <1174788032.7276.17.camel@Bellerophon>

On Sat, 2007-03-24 at 21:47 -0400, Daniel Folkinshteyn wrote:
> Greetings to all.
> I need to concatenate data frames that do not have all the same variable
> names, there is only a partial overlap in the variables. So, for
> example, if i have two data frames, a and b, that look like the following:
> > a
>   a b
> 1 1 4
> 2 2 5
> 3 3 6
> 4 4 7
> 5 5 8
> > b
>   c  a
> 1 1 10
> 2 2 11
> 3 3 12
> 4 4 13
> 5 5 14
> 
> i want to concatenate them by row, without any matching, so that the
> variables that are not available in all frames get NAs. The result
> should look like:
> 
>    a  b  c
> 1  1  4  NA
> 2  2  5  NA
> 3  3  6  NA
> 4  4  7  NA
> 5  5  8  NA
> 6  10 NA 1
> 7  11 NA 2
> 8  12 NA 3
> 9  13 NA 4
> 10 14 NA 5
> 
> rbind doesn't work, since it requires all variables to be matched
> between the two data frames. merge doesn't work, since it wants to
> /match/ by columns with the same name, and if matching by nothing,
> produces a cartesian product.
> 
> is there a neat trick for doing this simply, or am i stuck with
> comparing variable lists and generating NAs manually?
> 
> would appreciate any help!
> Daniel

You can use merge():

> a
  a b
1 1 4
2 2 5
3 3 6
4 4 7
5 5 8

> b
  c  a
1 1 10
2 2 11
3 3 12
4 4 13
5 5 14


Use 'a' as the common 'by' column and specify 'all = TRUE' so that
non-matching values of 'a' will be included in the result:


> merge(a, b, by = "a", all = TRUE)
    a  b  c
1   1  4 NA
2   2  5 NA
3   3  6 NA
4   4  7 NA
5   5  8 NA
6  10 NA  1
7  11 NA  2
8  12 NA  3
9  13 NA  4
10 14 NA  5


See ?merge for more information.

HTH,

Marc Schwartz


From dfolkins at temple.edu  Sun Mar 25 04:16:29 2007
From: dfolkins at temple.edu (Daniel Folkinshteyn)
Date: Sat, 24 Mar 2007 22:16:29 -0400
Subject: [R] Contatenating data frames with partial overlap in variable
 names
In-Reply-To: <1174788032.7276.17.camel@Bellerophon>
References: <4605D4CC.7050502@temple.edu>
	<1174788032.7276.17.camel@Bellerophon>
Message-ID: <4605DB7D.9080908@temple.edu>

on 03/24/2007 10:00 PM Marc Schwartz said the following:
> On Sat, 2007-03-24 at 21:47 -0400, Daniel Folkinshteyn wrote:
>> Greetings to all.
>> I need to concatenate data frames that do not have all the same variable
>> names, there is only a partial overlap in the variables. So, for
>> example, if i have two data frames, a and b, that look like the following:
>>> a
>>   a b
>> 1 1 4
>> 2 2 5
>> 3 3 6
>> 4 4 7
>> 5 5 8
>>> b
>>   c  a
>> 1 1 10
>> 2 2 11
>> 3 3 12
>> 4 4 13
>> 5 5 14
>>
>> i want to concatenate them by row, without any matching, so that the
>> variables that are not available in all frames get NAs. The result
>> should look like:
>>
>>    a  b  c
>> 1  1  4  NA
>> 2  2  5  NA
>> 3  3  6  NA
>> 4  4  7  NA
>> 5  5  8  NA
>> 6  10 NA 1
>> 7  11 NA 2
>> 8  12 NA 3
>> 9  13 NA 4
>> 10 14 NA 5
>>
>> rbind doesn't work, since it requires all variables to be matched
>> between the two data frames. merge doesn't work, since it wants to
>> /match/ by columns with the same name, and if matching by nothing,
>> produces a cartesian product.
>>
>> is there a neat trick for doing this simply, or am i stuck with
>> comparing variable lists and generating NAs manually?
>>
>> would appreciate any help!
>> Daniel
> 
> You can use merge():
> 
>> a
>   a b
> 1 1 4
> 2 2 5
> 3 3 6
> 4 4 7
> 5 5 8
> 
>> b
>   c  a
> 1 1 10
> 2 2 11
> 3 3 12
> 4 4 13
> 5 5 14
> 
> 
> Use 'a' as the common 'by' column and specify 'all = TRUE' so that
> non-matching values of 'a' will be included in the result:
> 
> 
>> merge(a, b, by = "a", all = TRUE)
>     a  b  c
> 1   1  4 NA
> 2   2  5 NA
> 3   3  6 NA
> 4   4  7 NA
> 5   5  8 NA
> 6  10 NA  1
> 7  11 NA  2
> 8  12 NA  3
> 9  13 NA  4
> 10 14 NA  5
> 
Thanks for your quick response. Unfortunately, this is still not quite
what I have in mind (though maybe it's my fault for not making this too
clear). Even if the two data frames happen to have some values of 'a'
that match, I still want those records to remain separate, rather than
merge. So, for instance, using merge will produce the following:
> a = data.frame(a=1:5, b=4:8)
> a
  a b
1 1 4
2 2 5
3 3 6
4 4 7
5 5 8
> b = data.frame(c=1:5, a=4:8)
> b
  c a
1 1 4
2 2 5
3 3 6
4 4 7
5 5 8
> merge(a,b,by='a',all=T)
  a  b  c
1 1  4 NA
2 2  5 NA
3 3  6 NA
4 4  7  1
5 5  8  2
6 6 NA  3
7 7 NA  4
8 8 NA  5

whereas I would still want it to produce 10 separate rows, because they
are separate observations, it's just that one of them happens to be
missing a variable.

Thanks,
Daniel


From h.wickham at gmail.com  Sun Mar 25 05:11:23 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sat, 24 Mar 2007 22:11:23 -0500
Subject: [R] Contatenating data frames with partial overlap in variable
	names
In-Reply-To: <4605D4CC.7050502@temple.edu>
References: <4605D4CC.7050502@temple.edu>
Message-ID: <f8e6ff050703242011t7dae4974p5adf2648c5c8aced@mail.gmail.com>

On 3/24/07, Daniel Folkinshteyn <dfolkins at temple.edu> wrote:
> Greetings to all.
> I need to concatenate data frames that do not have all the same variable
> names, there is only a partial overlap in the variables. So, for
> example, if i have two data frames, a and b, that look like the following:

Have a look at rbind.fill in the reshape package.

Hadley


From marc_schwartz at comcast.net  Sun Mar 25 05:13:44 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sat, 24 Mar 2007 22:13:44 -0500
Subject: [R] Contatenating data frames with partial overlap in
	variable	names
In-Reply-To: <4605DB7D.9080908@temple.edu>
References: <4605D4CC.7050502@temple.edu>
	<1174788032.7276.17.camel@Bellerophon>  <4605DB7D.9080908@temple.edu>
Message-ID: <1174792424.7276.28.camel@Bellerophon>

On Sat, 2007-03-24 at 22:16 -0400, Daniel Folkinshteyn wrote:
> on 03/24/2007 10:00 PM Marc Schwartz said the following:
> > On Sat, 2007-03-24 at 21:47 -0400, Daniel Folkinshteyn wrote:
> >> Greetings to all.
> >> I need to concatenate data frames that do not have all the same variable
> >> names, there is only a partial overlap in the variables. So, for
> >> example, if i have two data frames, a and b, that look like the following:
> >>> a
> >>   a b
> >> 1 1 4
> >> 2 2 5
> >> 3 3 6
> >> 4 4 7
> >> 5 5 8
> >>> b
> >>   c  a
> >> 1 1 10
> >> 2 2 11
> >> 3 3 12
> >> 4 4 13
> >> 5 5 14
> >>
> >> i want to concatenate them by row, without any matching, so that the
> >> variables that are not available in all frames get NAs. The result
> >> should look like:
> >>
> >>    a  b  c
> >> 1  1  4  NA
> >> 2  2  5  NA
> >> 3  3  6  NA
> >> 4  4  7  NA
> >> 5  5  8  NA
> >> 6  10 NA 1
> >> 7  11 NA 2
> >> 8  12 NA 3
> >> 9  13 NA 4
> >> 10 14 NA 5
> >>
> >> rbind doesn't work, since it requires all variables to be matched
> >> between the two data frames. merge doesn't work, since it wants to
> >> /match/ by columns with the same name, and if matching by nothing,
> >> produces a cartesian product.
> >>
> >> is there a neat trick for doing this simply, or am i stuck with
> >> comparing variable lists and generating NAs manually?
> >>
> >> would appreciate any help!
> >> Daniel
> > 
> > You can use merge():
> > 
> >> a
> >   a b
> > 1 1 4
> > 2 2 5
> > 3 3 6
> > 4 4 7
> > 5 5 8
> > 
> >> b
> >   c  a
> > 1 1 10
> > 2 2 11
> > 3 3 12
> > 4 4 13
> > 5 5 14
> > 
> > 
> > Use 'a' as the common 'by' column and specify 'all = TRUE' so that
> > non-matching values of 'a' will be included in the result:
> > 
> > 
> >> merge(a, b, by = "a", all = TRUE)
> >     a  b  c
> > 1   1  4 NA
> > 2   2  5 NA
> > 3   3  6 NA
> > 4   4  7 NA
> > 5   5  8 NA
> > 6  10 NA  1
> > 7  11 NA  2
> > 8  12 NA  3
> > 9  13 NA  4
> > 10 14 NA  5
> > 
> Thanks for your quick response. Unfortunately, this is still not quite
> what I have in mind (though maybe it's my fault for not making this too
> clear). Even if the two data frames happen to have some values of 'a'
> that match, I still want those records to remain separate, rather than
> merge. So, for instance, using merge will produce the following:
> > a = data.frame(a=1:5, b=4:8)
> > a
>   a b
> 1 1 4
> 2 2 5
> 3 3 6
> 4 4 7
> 5 5 8
> > b = data.frame(c=1:5, a=4:8)
> > b
>   c a
> 1 1 4
> 2 2 5
> 3 3 6
> 4 4 7
> 5 5 8
> > merge(a,b,by='a',all=T)
>   a  b  c
> 1 1  4 NA
> 2 2  5 NA
> 3 3  6 NA
> 4 4  7  1
> 5 5  8  2
> 6 6 NA  3
> 7 7 NA  4
> 8 8 NA  5
> 
> whereas I would still want it to produce 10 separate rows, because they
> are separate observations, it's just that one of them happens to be
> missing a variable.

OK. Not sure if this is the most efficient way of doing this, but this
seems to work, though through very limited testing.

Basically what I am doing is using setdiff() to figure out which columns
are not common between the two data frames. In each case, I then use
sapply() to loop over the results, creating a new column of NA's that
will be cbind()ed back to the original data frame.

Once that is done, the two new data frames, a.2 and b.2, will have
common columns and they can then be rbind()ed.


a.2 <- cbind(a, sapply(setdiff(colnames(b), colnames(a)), 
                       function(x) x = rep(NA, nrow(a))))

b.2 <- cbind(b, sapply(setdiff(colnames(a), colnames(b)), 
                       function(x) x = rep(NA, nrow(b))))

> a.2
  a b  c
1 1 4 NA
2 2 5 NA
3 3 6 NA
4 4 7 NA
5 5 8 NA

> b.2
  c a  b
1 1 4 NA
2 2 5 NA
3 3 6 NA
4 4 7 NA
5 5 8 NA


> rbind(a.2, b.2)
   a  b  c
1  1  4 NA
2  2  5 NA
3  3  6 NA
4  4  7 NA
5  5  8 NA
6  4 NA  1
7  5 NA  2
8  6 NA  3
9  7 NA  4
10 8 NA  5


Hopefully that will work in more general cases, but I would validate
that.

HTH,

Marc Schwartz


From dfolkins at temple.edu  Sun Mar 25 05:37:41 2007
From: dfolkins at temple.edu (Daniel Folkinshteyn)
Date: Sat, 24 Mar 2007 23:37:41 -0400
Subject: [R] Contatenating data frames with partial overlap in variable
 names
In-Reply-To: <1174792424.7276.28.camel@Bellerophon>
References: <4605D4CC.7050502@temple.edu>	
	<1174788032.7276.17.camel@Bellerophon>
	<4605DB7D.9080908@temple.edu>
	<1174792424.7276.28.camel@Bellerophon>
Message-ID: <4605EE85.7000006@temple.edu>

on 03/24/2007 11:13 PM Marc Schwartz said the following:
> On Sat, 2007-03-24 at 22:16 -0400, Daniel Folkinshteyn wrote:
>> on 03/24/2007 10:00 PM Marc Schwartz said the following:
>>> On Sat, 2007-03-24 at 21:47 -0400, Daniel Folkinshteyn wrote:
>>>> Greetings to all.
>>>> I need to concatenate data frames that do not have all the same variable
>>>> names, there is only a partial overlap in the variables. So, for
>>>> example, if i have two data frames, a and b, that look like the following:
>>>>> a
>>>>   a b
>>>> 1 1 4
>>>> 2 2 5
>>>> 3 3 6
>>>> 4 4 7
>>>> 5 5 8
>>>>> b
>>>>   c  a
>>>> 1 1 10
>>>> 2 2 11
>>>> 3 3 12
>>>> 4 4 13
>>>> 5 5 14
>>>>
>>>> i want to concatenate them by row, without any matching, so that the
>>>> variables that are not available in all frames get NAs. The result
>>>> should look like:
>>>>
>>>>    a  b  c
>>>> 1  1  4  NA
>>>> 2  2  5  NA
>>>> 3  3  6  NA
>>>> 4  4  7  NA
>>>> 5  5  8  NA
>>>> 6  10 NA 1
>>>> 7  11 NA 2
>>>> 8  12 NA 3
>>>> 9  13 NA 4
>>>> 10 14 NA 5
>>>>
>>>> rbind doesn't work, since it requires all variables to be matched
>>>> between the two data frames. merge doesn't work, since it wants to
>>>> /match/ by columns with the same name, and if matching by nothing,
>>>> produces a cartesian product.
>>>>
>>>> is there a neat trick for doing this simply, or am i stuck with
>>>> comparing variable lists and generating NAs manually?
>>>>
>>>> would appreciate any help!
>>>> Daniel
>>> You can use merge():
>>>
>>>> a
>>>   a b
>>> 1 1 4
>>> 2 2 5
>>> 3 3 6
>>> 4 4 7
>>> 5 5 8
>>>
>>>> b
>>>   c  a
>>> 1 1 10
>>> 2 2 11
>>> 3 3 12
>>> 4 4 13
>>> 5 5 14
>>>
>>>
>>> Use 'a' as the common 'by' column and specify 'all = TRUE' so that
>>> non-matching values of 'a' will be included in the result:
>>>
>>>
>>>> merge(a, b, by = "a", all = TRUE)
>>>     a  b  c
>>> 1   1  4 NA
>>> 2   2  5 NA
>>> 3   3  6 NA
>>> 4   4  7 NA
>>> 5   5  8 NA
>>> 6  10 NA  1
>>> 7  11 NA  2
>>> 8  12 NA  3
>>> 9  13 NA  4
>>> 10 14 NA  5
>>>
>> Thanks for your quick response. Unfortunately, this is still not quite
>> what I have in mind (though maybe it's my fault for not making this too
>> clear). Even if the two data frames happen to have some values of 'a'
>> that match, I still want those records to remain separate, rather than
>> merge. So, for instance, using merge will produce the following:
>>> a = data.frame(a=1:5, b=4:8)
>>> a
>>   a b
>> 1 1 4
>> 2 2 5
>> 3 3 6
>> 4 4 7
>> 5 5 8
>>> b = data.frame(c=1:5, a=4:8)
>>> b
>>   c a
>> 1 1 4
>> 2 2 5
>> 3 3 6
>> 4 4 7
>> 5 5 8
>>> merge(a,b,by='a',all=T)
>>   a  b  c
>> 1 1  4 NA
>> 2 2  5 NA
>> 3 3  6 NA
>> 4 4  7  1
>> 5 5  8  2
>> 6 6 NA  3
>> 7 7 NA  4
>> 8 8 NA  5
>>
>> whereas I would still want it to produce 10 separate rows, because they
>> are separate observations, it's just that one of them happens to be
>> missing a variable.
> 
> OK. Not sure if this is the most efficient way of doing this, but this
> seems to work, though through very limited testing.
> 
> Basically what I am doing is using setdiff() to figure out which columns
> are not common between the two data frames. In each case, I then use
> sapply() to loop over the results, creating a new column of NA's that
> will be cbind()ed back to the original data frame.
> 
> Once that is done, the two new data frames, a.2 and b.2, will have
> common columns and they can then be rbind()ed.
> 
> 
> a.2 <- cbind(a, sapply(setdiff(colnames(b), colnames(a)), 
>                        function(x) x = rep(NA, nrow(a))))
> 
> b.2 <- cbind(b, sapply(setdiff(colnames(a), colnames(b)), 
>                        function(x) x = rep(NA, nrow(b))))
> 
>> a.2
>   a b  c
> 1 1 4 NA
> 2 2 5 NA
> 3 3 6 NA
> 4 4 7 NA
> 5 5 8 NA
> 
>> b.2
>   c a  b
> 1 1 4 NA
> 2 2 5 NA
> 3 3 6 NA
> 4 4 7 NA
> 5 5 8 NA
> 
> 
>> rbind(a.2, b.2)
>    a  b  c
> 1  1  4 NA
> 2  2  5 NA
> 3  3  6 NA
> 4  4  7 NA
> 5  5  8 NA
> 6  4 NA  1
> 7  5 NA  2
> 8  6 NA  3
> 9  7 NA  4
> 10 8 NA  5
> 
> 
> Hopefully that will work in more general cases, but I would validate
> that.

Thanks Marc, that seems like a pretty elegant solution, and I have
learned some useful stuff from it.

However, I will go with rbind.fill(reshape) that was recommended by
Hadley (thanks!), since it's just so darn easy. :)

Thank you all,
Daniel

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 254 bytes
Desc: OpenPGP digital signature
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070324/4f34efb2/attachment.bin 

From h.wickham at gmail.com  Sun Mar 25 05:39:53 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sat, 24 Mar 2007 22:39:53 -0500
Subject: [R] Contatenating data frames with partial overlap in variable
	names
In-Reply-To: <1174792424.7276.28.camel@Bellerophon>
References: <4605D4CC.7050502@temple.edu>
	<1174788032.7276.17.camel@Bellerophon> <4605DB7D.9080908@temple.edu>
	<1174792424.7276.28.camel@Bellerophon>
Message-ID: <f8e6ff050703242039s56c3a480jcdab10fa154dea8b@mail.gmail.com>

On 3/24/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> On Sat, 2007-03-24 at 22:16 -0400, Daniel Folkinshteyn wrote:
> > on 03/24/2007 10:00 PM Marc Schwartz said the following:
> > > On Sat, 2007-03-24 at 21:47 -0400, Daniel Folkinshteyn wrote:
> > >> Greetings to all.
> > >> I need to concatenate data frames that do not have all the same variable
> > >> names, there is only a partial overlap in the variables. So, for
> > >> example, if i have two data frames, a and b, that look like the following:
> > >>> a
> > >>   a b
> > >> 1 1 4
> > >> 2 2 5
> > >> 3 3 6
> > >> 4 4 7
> > >> 5 5 8
> > >>> b
> > >>   c  a
> > >> 1 1 10
> > >> 2 2 11
> > >> 3 3 12
> > >> 4 4 13
> > >> 5 5 14
> > >>
> > >> i want to concatenate them by row, without any matching, so that the
> > >> variables that are not available in all frames get NAs. The result
> > >> should look like:
> > >>
> > >>    a  b  c
> > >> 1  1  4  NA
> > >> 2  2  5  NA
> > >> 3  3  6  NA
> > >> 4  4  7  NA
> > >> 5  5  8  NA
> > >> 6  10 NA 1
> > >> 7  11 NA 2
> > >> 8  12 NA 3
> > >> 9  13 NA 4
> > >> 10 14 NA 5
> > >>
> > >> rbind doesn't work, since it requires all variables to be matched
> > >> between the two data frames. merge doesn't work, since it wants to
> > >> /match/ by columns with the same name, and if matching by nothing,
> > >> produces a cartesian product.
> > >>
> > >> is there a neat trick for doing this simply, or am i stuck with
> > >> comparing variable lists and generating NAs manually?
> > >>
> > >> would appreciate any help!
> > >> Daniel
> > >
> > > You can use merge():
> > >
> > >> a
> > >   a b
> > > 1 1 4
> > > 2 2 5
> > > 3 3 6
> > > 4 4 7
> > > 5 5 8
> > >
> > >> b
> > >   c  a
> > > 1 1 10
> > > 2 2 11
> > > 3 3 12
> > > 4 4 13
> > > 5 5 14
> > >
> > >
> > > Use 'a' as the common 'by' column and specify 'all = TRUE' so that
> > > non-matching values of 'a' will be included in the result:
> > >
> > >
> > >> merge(a, b, by = "a", all = TRUE)
> > >     a  b  c
> > > 1   1  4 NA
> > > 2   2  5 NA
> > > 3   3  6 NA
> > > 4   4  7 NA
> > > 5   5  8 NA
> > > 6  10 NA  1
> > > 7  11 NA  2
> > > 8  12 NA  3
> > > 9  13 NA  4
> > > 10 14 NA  5
> > >
> > Thanks for your quick response. Unfortunately, this is still not quite
> > what I have in mind (though maybe it's my fault for not making this too
> > clear). Even if the two data frames happen to have some values of 'a'
> > that match, I still want those records to remain separate, rather than
> > merge. So, for instance, using merge will produce the following:
> > > a = data.frame(a=1:5, b=4:8)
> > > a
> >   a b
> > 1 1 4
> > 2 2 5
> > 3 3 6
> > 4 4 7
> > 5 5 8
> > > b = data.frame(c=1:5, a=4:8)
> > > b
> >   c a
> > 1 1 4
> > 2 2 5
> > 3 3 6
> > 4 4 7
> > 5 5 8
> > > merge(a,b,by='a',all=T)
> >   a  b  c
> > 1 1  4 NA
> > 2 2  5 NA
> > 3 3  6 NA
> > 4 4  7  1
> > 5 5  8  2
> > 6 6 NA  3
> > 7 7 NA  4
> > 8 8 NA  5
> >
> > whereas I would still want it to produce 10 separate rows, because they
> > are separate observations, it's just that one of them happens to be
> > missing a variable.
>
> OK. Not sure if this is the most efficient way of doing this, but this
> seems to work, though through very limited testing.
>
> Basically what I am doing is using setdiff() to figure out which columns
> are not common between the two data frames. In each case, I then use
> sapply() to loop over the results, creating a new column of NA's that
> will be cbind()ed back to the original data frame.

That's pretty much what rbind.fill does, with an extra bit of error
checking and generalise for any number of matrics:

> rbind.fill
function (...)
{
    dfs <- list(...)
    if (length(dfs) == 0)
        return(list())
    all.names <- unique(unlist(lapply(dfs, names)))
    do.call("rbind", compact(lapply(dfs, function(df) {
        if (length(df) == 0 || nrow(df) == 0)
            return(NULL)
        missing.vars <- setdiff(all.names, names(df))
        if (length(missing.vars) > 0)
            df[, missing.vars] <- NA
        df
    })))
}
<environment: namespace:reshape>



>
> Once that is done, the two new data frames, a.2 and b.2, will have
> common columns and they can then be rbind()ed.
>
>
> a.2 <- cbind(a, sapply(setdiff(colnames(b), colnames(a)),
>                        function(x) x = rep(NA, nrow(a))))
>
> b.2 <- cbind(b, sapply(setdiff(colnames(a), colnames(b)),
>                        function(x) x = rep(NA, nrow(b))))
>
> > a.2
>   a b  c
> 1 1 4 NA
> 2 2 5 NA
> 3 3 6 NA
> 4 4 7 NA
> 5 5 8 NA
>
> > b.2
>   c a  b
> 1 1 4 NA
> 2 2 5 NA
> 3 3 6 NA
> 4 4 7 NA
> 5 5 8 NA
>
>
> > rbind(a.2, b.2)
>    a  b  c
> 1  1  4 NA
> 2  2  5 NA
> 3  3  6 NA
> 4  4  7 NA
> 5  5  8 NA
> 6  4 NA  1
> 7  5 NA  2
> 8  6 NA  3
> 9  7 NA  4
> 10 8 NA  5
>
>
> Hopefully that will work in more general cases, but I would validate
> that.
>
> HTH,
>
> Marc Schwartz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From brown_emu at yahoo.com  Sun Mar 25 07:25:24 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sat, 24 Mar 2007 22:25:24 -0700 (PDT)
Subject: [R] objects of class "matrix" and mode "list"? [Broadcast]
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03EB2947@usctmx1106.merck.com>
Message-ID: <258089.50049.qm@web39708.mail.mud.yahoo.com>

Hi Andy,

I hadn't realized such objects (list matrices, list arrays, data frames with
nested lists) existed before, but now that I do I am seeing the documentation
with new eyes. I see that the pages for sapply(), lapply(), and class
coercion functions are true to their word.

Thanks,

Stephen


--- "Liaw, Andy" <andy_liaw at merck.com> wrote:

> It may help to (re-)read ?sapply a bit more in detail.  Simplification
> is done only if it's "possible", and what "possible" means is defined
> there.
> 
> A list is a vector whose elements can be different objects, but a vector
> nonetheless.  Thus a list can have dimensions.  E.g.,
> 
> R> a <- list(1, 1:2, 3, c("abc", "def"))
> R> dim(a) <- c(2, 2)
> R> a
>      [,1]      [,2]       
> [1,] 1         3          
> [2,] Integer,2 Character,2
> 
> That sometimes can be extremely useful (not like the example above!).
> 
> Andy 
> 
> From: Stephen Tucker
> > 
> > Hello everyone,
> > 
> > I cannot seem to find information about objects of class 
> > "matrix" and mode
> > "list", and how to handle them (apart from flattening the 
> > list). I get this
> > type of object from using sapply(). Sorry for the long 
> > example, but the code
> > below illustrates how I get this type of object. Is anyone aware of
> > documentation regarding this object?
> > 
> > Thanks very much,
> > 
> > Stephen
> > 
> > ===== begin example ====
> > 
> > # I am just making up a fake data set
> > df <- data.frame(Day=rep(1:3,each=24),Hour=rep(1:24,times=3),
> >                  Name1=rnorm(24*3),Name2=rnorm(24*3))
> > 
> > # define a function to get a set of descriptive statistics
> > tmp <- function(x) {
> >   # this function will accept a data frame
> >   # and return a 1-row data frame of
> >   # max value, colname of max, min value, and colname of min
> >   return(data.frame(maxval=max(apply(x,2,max)),
> >                     maxloc=names(x)[which.max(apply(x,2,max))],
> >                     minval=min(apply(x,2,min)),
> >                     minloc=names(x)[which.min(apply(x,2,min))]))
> > }
> > 
> > # Now applying function to data:
> > # (1) split the data table by Day with split()
> > # (2) apply the tmp function defined above to each data frame from (1)
> > #     using lapply()
> > # (3) transpose the final matrix and convert it to a data frame
> > #     with mixed characters and numbers
> > #     using as.data.frame(), lapply(), and type.convert()
> > 
> > > final <- 
> > as.data.frame(lapply(as.data.frame(t(sapply(split(df[,-c(1:2)],
> > +                                                           
> > f=df$Day),tmp))),
> > +                               type.convert,as.is=TRUE))
> > Error in type.convert(x, na.strings, as.is, dec) : 
> > 	the first argument must be of mode character
> > 
> > I thought sapply() would give me a data frame or matrix, which I would
> > transpose into a character matrix, to which I can apply type.convert()
> > and get the same matrix as what I would get from these two lines (Fold
> > function taken from Gabor's post on R-help a few years ago):
> > 
> > Fold <- function(f, x, L) for(e in L) x <- f(x, e)
> > final2 <- Fold(rbind,vector(),lapply(split(df[,-c(1:2)],f=day),tmp))
> > 
> > > print(c(class(final2),mode(final2)))
> > [1] "data.frame" "list"  
> > 
> > ====================================================
> > However, by my original method, sapply() gives me a matrix 
> > with mode, "list"
> > 
> > intermediate1 <- sapply(split(df[,-c(1:2)],f=df$Day),tmp)
> > > print(c(class(intermediate1),mode(intermediate1)))
> > [1] "matrix" "list"  
> > 
> > Transposing, still a matrix with mode list, not character:
> > 
> > intermediate2 <- t(sapply(split(df[,-c(1:2)],f=day),tmp))
> > > print(c(class(intermediate2),mode(intermediate2)))
> > [1] "matrix" "list"  
> > 
> > Unclassing gives me the same thing...
> > 
> > > print(c(class(unclass(intermediate2)),mode(unclass(intermediate2))))
> > [1] "matrix" "list"  
> > 
> > 
> > 
> > 
> >  
> > ______________________________________________________________
> > ______________________
> > Be a PS3 game guru.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> > 
> > 
> 
> 
>
------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}


From ripley at stats.ox.ac.uk  Sun Mar 25 11:33:19 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 25 Mar 2007 10:33:19 +0100 (BST)
Subject: [R] write.csv feature/bug with pipe
In-Reply-To: <50d1c22d0703111754s7b85da6cre8f8be87dfc5ef9f@mail.gmail.com>
References: <50d1c22d0703111754s7b85da6cre8f8be87dfc5ef9f@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0703251025290.30945@gannet.stats.ox.ac.uk>

This is simply user error: the column names _are_ written to the pipe.
Because the connection was not open, the connection is opened to write the 
column names, closed, opened to write the data and then closed.

In any case, you should call close() on connections you create to avoid 
leaking connection structures.  The following works:

con <- pipe("cat > d2.csv", "w")
write.csv(d, file=con)
close(con)

write.table() does exactly the same thing.

On Sun, 11 Mar 2007, ivo welch wrote:

> gentoo linux, version 2.4.1:
>
>> d= as.data.frame(matrix(1:20, 4, 5))
>> d
>  V1 V2 V3 V4 V5
> 1  1  5  9 13 17
> 2  2  6 10 14 18
> 3  3  7 11 15 19
> 4  4  8 12 16 20
>> write.csv(d, file="d1.csv");
>> write.csv(d, file=pipe("cat > d2.csv"))
>> write.csv(d, file=pipe("gzip -c > d3.csv.gz"), col.names=T)
> Warning message:
> attempt to change 'col.names' ignored in: write.csv(d, file =
> pipe("gzip -c > d4.csv.gz"), col.names = T)

(Seems unlikely the message got the file name wrong here.)

> exit and
>
> $ head d1.csv
> "","V1","V2","V3","V4","V5"
> "1",1,5,9,13,17
> "2",2,6,10,14,18
> "3",3,7,11,15,19
> "4",4,8,12,16,20
> $ head d2.csv
> "1",1,5,9,13,17
> "2",2,6,10,14,18
> "3",3,7,11,15,19
> "4",4,8,12,16,20
>
> is it a bug or a feature that when pipe is used, the col.names is set
> to false?  I guess I can invoke write.table to get the headers back.
>
> regards,
>
> /iaw
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From lobry at biomserv.univ-lyon1.fr  Sun Mar 25 12:52:39 2007
From: lobry at biomserv.univ-lyon1.fr (Jean lobry)
Date: Sun, 25 Mar 2007 11:52:39 +0100
Subject: [R] sort argument in mosaicplot
Message-ID: <p06002003c22bf902c171@[192.168.1.10]>

>x <- matrix(1:4,2,2)
>mosaicplot(x)
># This one is OK
>mosaicplot(x, sort = 1:length(dim(x)) )
># Not OK, I have the following error message:
>Erreur dans mosaicplot.default(x, sort = 1:length(dim(x))) :
>         objet "label" non trouv?

More info on this:

i) I was told by a colleague that it was working under the
(obsolete) R 2.4.0. I have checked this with our RWeb sever which
is still under R 2.4.0 and it is true that
mosaicplot(x, sort = 1:length(dim(x)) ) works.

ii) I have tried the same code after sourcing the file at
https://svn.r-project.org/R/trunk/src/library/graphics/R/mosaicplot.R
and I have the same error message saying that the object
"label" was not found.

So, is it a fortune(2) issue or something I have missed in
the sort argument of mosaicplot() ? I have searched for the
word "mosaicplot" in the "2.4 SERIES NEWS" and found only one
occurrence in the changes in R version 2.2.0 saying
"The default mosaicplot() method by default draws grey boxes"
that does not seem to be related to the sort argument.
I have also searched for the word "sort", but none seems to
be related to the sort agument of mosaicplot().


Best,

BTW, a completely unrelated question: is it possible to
turn on/turn off temporarilly (i.e. without quitting
the R session) the locale settings so as
to get error and warning messages reported in english?
-- 
Jean R. Lobry            (lobry at biomserv.univ-lyon1.fr)
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - LYON I,
43 Bd 11/11/1918, F-69622 VILLEURBANNE CEDEX, FRANCE
allo  : +33 472 43 27 56     fax    : +33 472 43 13 88
http://pbil.univ-lyon1.fr/members/lobry/


From ted.harding at nessie.mcc.ac.uk  Sun Mar 25 13:27:03 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 25 Mar 2007 11:27:03 -0000 (BST)
Subject: [R] sampling from the unoform distrubuton over a convex hull
In-Reply-To: <XFMail.070324192621.ted.harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.070325112703.ted.harding@nessie.mcc.ac.uk>

On 24-Mar-07 19:26:21, Ted Harding wrote:
> On 24-Mar-07 14:00:44, Ted Harding wrote:
>> [...]
> [...]
> Well, I've written some rather crude code to implement the
> above. Even allowing for possible "editorial" improvements,
> the more I look at it the more I think there may be a better
> way! (Of doing it directly, I mean, raher than a rejection
> method).
> 
> Still, it seems to work (and quite slickly) ...
> 
> 
And now I've done what I should have donein the first place:
the code for rdiric() in VGAM is "stand-alone" and can simply
be copied, so:

## library(VGAM) gives the following code for function rdiric()
rdiric<-function(n, shape, dimension = NULL) 
{
    dimension = if (!is.numeric(dimension)) length(shape)
    shape = rep(shape, len = dimension)
    ans = if (is.R()) 
        rgamma(n * dimension, rep(shape, rep(n, dimension)))
    else rgamma(n * dimension, rep(shape, each = n))
    dim(ans) = c(n, dimension)
    ans = ans/apply(ans, 1, sum)
    ans
}

## Make some random points, get their CH and centroid, and draw them
X<-cbind(rnorm(10),rnorm(10))
plot(X[,1],X[,2],pch="+",col="green")
H<-chull(X)
C<-colMeans(X[H,])
points(C[1],C[2],pch="+",col="red")
## Draw the CH and the triangulation
H<-c(H,H[1])   ## To close the contour
K<-length(H)-1 ## No of triangles
lines(X[H,1],X[H,2],col="blue")
for(i in (1:K)){lines(c(X[H[i],1],C[1]),c(X[H[i],2],C[2]),col="red")}
  
## Set up the sampling by triangles
As<-numeric(K)
Ns<-numeric(K)
 
## Get the areas of the triangles
for(i in (1:(K))){
  V1<-X[H[i],]
  V2<-X[H[i+1],]
  V3<-C
  As[i]<-abs(det(rbind(V1-V3,V2-V3)))
}
 
## As illustration, 1000 points over the CH
N<-1000
 
## How many points to go in eavh triangle
Cuts<-cumsum(As)/sum(As)
R<-runif(N)
Ns[1]<-sum(R<=Cuts[1])
for(i in (2:K)){Ns[i]<-sum(R<=Cuts[i])-sum(R<=Cuts[i-1])}
## Uniform distribution over each triangle
for(i in (1:(K))){
  V1<-X[H[i],]
  V2<-X[H[i+1],]
  V3<-C
  ## The Dirichlet sample:
  T<-rbind(X[H[i],],X[H[i+1],],C)
  D<-rdiric(Ns[i],c(1,1,1))
  Z<-D%*%T
  points(Z[,1],Z[,2],pch="+",col="blue")
}

Ted. 

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 25-Mar-07                                       Time: 11:26:58
------------------------------ XFMail ------------------------------


From ripley at stats.ox.ac.uk  Sun Mar 25 12:31:53 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 25 Mar 2007 11:31:53 +0100 (BST)
Subject: [R] sort argument in mosaicplot
In-Reply-To: <p06002003c22bf902c171@[192.168.1.10]>
References: <p06002003c22bf902c171@[192.168.1.10]>
Message-ID: <Pine.LNX.4.64.0703251114210.1260@gannet.stats.ox.ac.uk>

On Sun, 25 Mar 2007, Jean lobry wrote:

>> x <- matrix(1:4,2,2)
>> mosaicplot(x)
>> # This one is OK
>> mosaicplot(x, sort = 1:length(dim(x)) )

please, seq_len(dim(x)) is designed for this task.

>> # Not OK, I have the following error message:
>> Erreur dans mosaicplot.default(x, sort = 1:length(dim(x))) :
>>         objet "label" non trouv?
>
> More info on this:
>
> i) I was told by a colleague that it was working under the
> (obsolete) R 2.4.0. I have checked this with our RWeb sever which
> is still under R 2.4.0 and it is true that
> mosaicplot(x, sort = 1:length(dim(x)) ) works.
>
> ii) I have tried the same code after sourcing the file at
> https://svn.r-project.org/R/trunk/src/library/graphics/R/mosaicplot.R
> and I have the same error message saying that the object
> "label" was not found.
>
> So, is it a fortune(2) issue or something I have missed in
> the sort argument of mosaicplot() ? I have searched for the
> word "mosaicplot" in the "2.4 SERIES NEWS" and found only one
> occurrence in the changes in R version 2.2.0 saying
> "The default mosaicplot() method by default draws grey boxes"
> that does not seem to be related to the sort argument.
> I have also searched for the word "sort", but none seems to
> be related to the sort agument of mosaicplot().

I don't know why it is not in NEWS, but the function was altered at

r39655 | hornik | 2006-10-17 21:12:53 +0100 (Tue, 17 Oct 2006) | 1 line
Bug fixes by J. Emerson.

and that change has caused your problem.  I will commit a fix to R-devel 
shortly.

> BTW, a completely unrelated question: is it possible to
> turn on/turn off temporarilly (i.e. without quitting
> the R session) the locale settings so as
> to get error and warning messages reported in english?

See the 'R Installation and Administration Manual' chapter 7:

The preferred language for messages is by default taken from the locale. 
This can be overridden first by the setting of the environment variable 
LANGUAGE and then by the environment variables LC_ALL, LC_MESSAGES and 
LANG. (The last three are normally used to set the locale and so should 
not be needed, but the first is only used to select the language for 
messages.) The code tries hard to map locales to languages, but on some 
systems (notably Windows) the locale names needed for the environment 
variable LC_ALL do not all correspond to XPG language names and so 
LANGUAGE may need to be set. (One example is `LC_ALL=es' on Windows which 
sets the locale to Estonian and the language to Spanish.)

It is usually possible (not Windows) to change the language once R is 
running via Sys.setlocale("LC_MESSAGES", "new_locale"), but not by setting 
environment variables such as LANGUAGE.

E.g. (on Linux)

> x <- matrix(1:4,2,2)
> mosaicplot(x, sort = 1:length(dim(x)) )
Error in mosaicplot.default(x, sort = 1:length(dim(x))) :
         object "label" not found
> Sys.setlocale("LC_MESSAGES", "fr_FR")
[1] "fr_FR"
> mosaicplot(x, sort = 1:length(dim(x)) )
Erreur dans mosaicplot.default(x, sort = 1:length(dim(x))) :
         objet "label" non trouv?

[Unfortunately Windows does not support this aspect of locales.]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From john.bullock at stanford.edu  Sun Mar 25 13:19:28 2007
From: john.bullock at stanford.edu (John Bullock)
Date: Sun, 25 Mar 2007 04:19:28 -0700
Subject: [R] eliminating panel borders from lattice plots
Message-ID: <02ee01c76ecf$773d2510$87ac0c80@WINSTON>


I am trying to eliminate panel borders from my
lattice plots.  By default, they always print.  For
example:

    library(lattice)
    x <- seq(-3,3,length=1000)
    y1 <- dnorm(x)
    y2 <- dnorm(x, sd=.5)
    data <- data.frame(x=rep(x,2), y=c(y,y2),
        panel=rep(c(1,2), each=1000))
    dplot <- xyplot(y~x | panel, data=data, strip=F,
        scales=list(draw=F))
    print(dplot, scales=list(draw=F))

prints borders around each panel.  I see no way to
get rid of them short of creating a panel function
and "painting over" these default borders with
grid.rect().  But I suspect that there is an easier
way -- is there?

I searched the archives but saw nothing on this.
I'm running R 2.3.1 with lattice 0.13.

Thank you,
--John


From m.townsley at ucl.ac.uk  Sun Mar 25 14:24:42 2007
From: m.townsley at ucl.ac.uk (mike townsley)
Date: Sun, 25 Mar 2007 13:24:42 +0100
Subject: [R] controlling panel.width and panel.height in viewports
Message-ID: <6.2.5.6.1.20070325131002.027b3550@ucl.ac.uk>

Dear all,

I'm trying to get a series of lattice levelplots to appear in 
viewports in a particular way but struggling to exert fine control 
over their appearence.  There are two conditions: (a) I only want the 
levelplot to appear (I don't want axes, colour key, etc) in the 
viewport and (b) I want the levelplot to expand to the maximum 
allowable space in the viewport while observing the aspect ratio of the plot.

Condition (a) is OK, but (b) is giving me trouble.  A toy example is:

library(lattice)
library(grid)

x <- 1:10
y <- 1:10
grid <- expand.grid(x=x, y=y)
grid$z <- grid$x*grid$y

asp.ratio.1 <- 2
asp.ratio.2 <- .5
asp.ratio.3 <- 1

test.1 <- levelplot(z~x*y, grid, cuts = 5, xlab="", axes = FALSE,
                     aspect = asp.ratio.1, ylab="", main='', sub="",
                     colorkey = FALSE, region = TRUE, scales = 
list(draw = FALSE))
test.2 <- levelplot(z~x*y, grid, cuts = 5, xlab="", axes = FALSE,
                     aspect = asp.ratio.2, ylab="", main='', sub="",
                     colorkey = FALSE, region = TRUE, scales = 
list(draw = FALSE))
test.3 <- levelplot(z~x*y, grid, cuts = 5, xlab="", axes = FALSE,
                     aspect = asp.ratio.3, ylab="", main='', sub="",
                     colorkey = FALSE, region = TRUE, scales = 
list(draw = FALSE))

# so the three basic types of plot I will have


toy.vp <- viewport(width = .8, height = .8, just = 'centre',
                        name = 'toy')  # this is for my levelplot

X11()
pushViewport(toy.vp)
grid.rect(gp = gpar(col = 'grey'))
print(test.1, newpage = FALSE)

X11()
pushViewport(toy.vp)
grid.rect(gp = gpar(col = 'grey'))
print(test.2, newpage = FALSE)

X11()
pushViewport(toy.vp)
grid.rect(gp = gpar(col = 'grey'))
print(test.3, newpage = FALSE)


I want to get rid of the space (margins) around the longest axis for 
each variety of levelplot.  So I tried to capture the viewport height 
and width and print the levelplot within a region which reflects the 
aspect ratio of the plot.  I run into an error message as below:

width.vp <- convertWidth(grobWidth(grid.rect()), 'cm')
height.vp <- convertWidth(grobHeight(grid.rect()), 'cm')

width.vp;height.vp # this works

if(asp.ratio.1 < 1) {height.vp <- 
unit(as.numeric(height.vp)*asp.ratio.1, "cm")}
if(asp.ratio.1 > 1) {width.vp <- unit(as.numeric(width.vp)/asp.ratio.1, "cm")}

width.vp;height.vp # this also work (ie returns expected results)

print(test.1, panel.width = list(width.vp), panel.height=list(height.vp))

the above command returns this error message:

Error in Ops.unit(panel.height[[1]], panel.width[[1]]) :
         Operator '/' not meaningful for units

What have I missed?

Thanks in advance,

MT

------------------------------------------------------------
Dr Michael Townsley
Senior Research Fellow
Jill Dando Institute of Crime Science
University College London
Second Floor, Brook House
London, WC1E 7HN

Phone: 020 7679 0820
Fax: 020 7679 0828
Email: m.townsley at ucl.ac.uk


From ggrothendieck at gmail.com  Sun Mar 25 15:07:40 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 25 Mar 2007 09:07:40 -0400
Subject: [R] eliminating panel borders from lattice plots
In-Reply-To: <02ee01c76ecf$773d2510$87ac0c80@WINSTON>
References: <02ee01c76ecf$773d2510$87ac0c80@WINSTON>
Message-ID: <971536df0703250607m5b40b4dds806235e5de677585@mail.gmail.com>

Try adding this argument to your xyplot call:

   par.settings = list(axis.line = list(col = 0))

The subparameters oif axis.line are:

  trellis.par.get()$axis.line

in case you want to temporarily set others.

On 3/25/07, John Bullock <john.bullock at stanford.edu> wrote:
>
> I am trying to eliminate panel borders from my
> lattice plots.  By default, they always print.  For
> example:
>
>    library(lattice)
>    x <- seq(-3,3,length=1000)
>    y1 <- dnorm(x)
>    y2 <- dnorm(x, sd=.5)
>    data <- data.frame(x=rep(x,2), y=c(y,y2),
>        panel=rep(c(1,2), each=1000))
>    dplot <- xyplot(y~x | panel, data=data, strip=F,
>        scales=list(draw=F))
>    print(dplot, scales=list(draw=F))
>
> prints borders around each panel.  I see no way to
> get rid of them short of creating a panel function
> and "painting over" these default borders with
> grid.rect().  But I suspect that there is an easier
> way -- is there?
>
> I searched the archives but saw nothing on this.
> I'm running R 2.3.1 with lattice 0.13.
>
> Thank you,
> --John
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mihai.p.nica at gmail.com  Sun Mar 25 15:36:45 2007
From: mihai.p.nica at gmail.com (Mihai Nica)
Date: Sun, 25 Mar 2007 07:36:45 -0600
Subject: [R] plot of computed vector
Message-ID: <6b30be820703250636l5f8009f6t7648816fbabacf34@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070325/c9fc2a54/attachment.pl 

From h.wickham at gmail.com  Sun Mar 25 15:53:14 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 25 Mar 2007 08:53:14 -0500
Subject: [R] plot of computed vector
In-Reply-To: <6b30be820703250636l5f8009f6t7648816fbabacf34@mail.gmail.com>
References: <6b30be820703250636l5f8009f6t7648816fbabacf34@mail.gmail.com>
Message-ID: <f8e6ff050703250653g625d894cy2f7aa31fb2846f51@mail.gmail.com>

> I apologize for the non-programming language. I found what seems to be a
> strange behavior of plot(). The code follows:
>
> #_____________________________
> N=3030; gn=.04; tn=1:100
> n=N/(1+(N-1)*exp(-gn*tn))
> N=n*(1-exp(-gn*tn))/(1-n*exp(-gn*tn))
> plot(N) #strange plot
>
> N

Have a look at diff(N) to see what's going on.

The default axis labels and printing display too few decimal places to
see the differences between the numbers.

Hadley


From mihainica at yahoo.com  Sun Mar 25 16:12:34 2007
From: mihainica at yahoo.com (Mihai Nica)
Date: Sun, 25 Mar 2007 07:12:34 -0700 (PDT)
Subject: [R] plot of computed vector
Message-ID: <100229.58548.qm@web50809.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070325/8562cfee/attachment.pl 

From adick at uchicago.edu  Sun Mar 25 18:39:57 2007
From: adick at uchicago.edu (Anthony Steven Dick)
Date: Sun, 25 Mar 2007 11:39:57 -0500
Subject: [R] for loop help
Message-ID: <4606A5DD.2010806@uchicago.edu>

Hello-

I have a script which steps through a series of subjects, and for the 
subjects I remove outlying values. After removing these outliers, I 
specify a cutoff, keeping only values over a certain value (e.g., 1.96). 
I want to populate a matrix with a statistic of the values that make the 
cutoff (for example, the mean). However, in some subjects, after 
outliers and the cutoff are specified, there are no data that meet the 
criteria (I get <0 rows> (or 0-length row.names)). Here the script dies.

The solution I think is to specify a break so that the matrix will be 
populated with a value (such as NA) and it will move on to the next 
subject in the loop. However, I haven't been able to figure this out. If 
anyone has any suggestions I would very much appreciate them. I have 
paster part of the script below up to the point where it dies.

Thanks.

for (ss in levels(ss.list)) {
        print(ss)
        ss.count = ss.count + 1  
        query.string <- paste("SELECT * FROM ",ss,";",sep="")
        #print(query.string)
        data_gcs <- dbGetQuery(con, query.string)
        attach(data_gcs)
        names(data_gcs)
        mat_row = 0
            for(i in levels(roi.list)){
            print (i)
            current.roi <- data_gcs[data_gcs$ROI==i,]
            current.roi.plus.z <- data.frame(current.roi, 
scale(current.roi[,15]), scale(current.roi[,18]), 
scale(current.roi[,21]), scale(current.roi[,24]))
            testextrem <- function(x) {if ((abs(x[1]) < 2.5) & 
(abs(x[2]) < 2.5) & (abs(x[3]) < 2.5) & (abs(x[4]) < 2.5)) return(1) 
else return(0)}
            filtervector <- apply(as.vector(current.roi.plus.z[,c(27, 
28, 29, 30)]), 1, FUN=testextrem)
            current.roi.plus.z.filter <- data.frame(current.roi.plus.z, 
filtervector)
            final.roi.df <- 
current.roi.plus.z.filter[which(current.roi.plus.z.filter$filtervector==1),]
            #print (final.roi.df)
            kicked.out<-(length(filtervector) - 
sum(filtervector))/length(filtervector)
            print(kicked.out)
            matrix.col = matrix.col + 1
            attach(final.roi.df)
            names(final.roi.df)
            print(matrix.col)
            #set cutoff for FDR. per voxel p values (of Z dist) .05 = 
1.96, .01 = 2.575, .001 = 3.277, .005 = 3.479 BE SURE TO CHANGE THE 
VARIABLE EACH TIME YOU CHANGE CONDITION
               pre.outliers<-subset(final.roi.df, gFDR4FWHM >= 1.96)
               detach(final.roi.df)
               attach(pre.outliers)
               outliers<-subset(pre.outliers, gtvalue4FWHM >= 0.00)
...and the script dies here because there are no data in "outliers".

-- 
Anthony Steven Dick, Ph.D.
Post-Doctoral Scholar
Human Neuroscience Laboratory
Biological Sciences Division
University of Chicago
5841 S. Maryland Ave. MC-2030
Chicago, IL 60637
Phone: (773)-834-7770
Email: adick at uchicago.edu
Alternate email: anthony at anthonymail.com


From stan at temple.edu  Sun Mar 25 19:36:47 2007
From: stan at temple.edu (Stan Horwitz)
Date: Sun, 25 Mar 2007 13:36:47 -0400
Subject: [R] Installing R on a machine with 64-bit Opteron processors
Message-ID: <591E6C77-6228-4881-9144-DDF16C84F4B5@temple.edu>

I have been tasked with installing statistical and other data  
analysis applications on a new Sun Fire X4600 M2 x64 server that came  
equipped with eight AMD dual core Opteronn 64-bit processors. It is  
running the 64-bit version of Suse Linux 9.

I have read through the installation docs, and I guess I don't  
understand what to do, or even how to identify which version, if any,  
of this software is suitable for my computer environment.

What I want to know is if R has been ported to that environment and  
if so, how do I install it? I tried downloading it from one of the  
mirror web sites, and when I install it, I get

  euler src/R-base-2.4.1# rpm -i R-base-2.4.1-2.1.i586.rpm
warning: R-base-2.4.1-2.1.i586.rpm: V3 DSA signature: NOKEY, key ID  
6b9d6523
error: Failed dependencies:
         xorg-x11-fonts-100dpi is needed by R-base-2.4.1-2.1
         xorg-x11-fonts-75dpi is needed by R-base-2.4.1-2.1
         xorg-x11-libs is needed by R-base-2.4.1-2.1
         blas is needed by R-base-2.4.1-2.1
         libreadline.so.5 is needed by R-base-2.4.1-2.1
euler src/R-base-2.4.1#

I also tried the x386 version with the same results.

The documentation lists some prerequisite items to install such as  
blas, but I have searched and I can't see to locate that software. Is  
it public domain or a commercial product? I also see that my server  
already has x11 fonts installed, so I don't know what those errors  
are about.

Here's what rpm says about fonts ...

fontconfig-2.2.92.20040221-28.13
fontconfig-32bit-9-200407011229
fontconfig-devel-32bit-9-200407011229
ghostscript-fonts-other-7.07.1rc1-195.8
XFree86-fonts-75dpi-4.3.99.902-43.71
fontforge-20060715-7.3
ghostscript-fonts-std-7.07.1rc1-195.8
fontconfig-devel-2.2.92.20040221-28.13
xorg-x11-fonts-scalable-6.9.0-48
efont-unicode-0.4.0-630.1

So isn't "xorg-x11-fonts-scalable-6.9.0-48" what it wants? I guess  
not. Is there a list of what this software needs AND where to  
download or purchase each item for Suse 9?

I am also wondering if this list has searchable archives? The list's  
web site shows archived postings, but I don't see a way to search them.


From ramasamy at cancer.org.uk  Sun Mar 25 20:15:25 2007
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Sun, 25 Mar 2007 19:15:25 +0100
Subject: [R] for loop help
In-Reply-To: <4606A5DD.2010806@uchicago.edu>
References: <4606A5DD.2010806@uchicago.edu>
Message-ID: <4606BC3D.8050507@cancer.org.uk>

Try changing

    outliers <- subset(pre.outliers, gtvalue4FWHM >= 0.00)

to

    w <- which( gtvalue4FWHM >= 0.00 )
    outliers( length(w) > 0, pre.outliers[ w, ],  NA )


Other comments:

1. Make sure you detach(data_gcs) at the end of the loops


2. scale() works on columns by default, so try
     current.roi.plus.z <- data.frame(current.roi,
                                   scale(current.roi[, c(15,18,21,24)]) )


3. For simple statements, you can use ifelse() syntax. Also see all(). 
Using these two functions, you can try

  testextrem <- function(x) ifelse( all(abs(x[ 1:4 ]) < c(2.5)), 1, 0)


4. Try to minimise the use of attach() in loops.

5. You might be interested in learning more about apply(), tapply(), 
split() etc.


If this does not help, then please resend a simplified version of the 
codes, preferably with a simple toy example to illustrate.

Regards, Adai



Anthony Steven Dick wrote:
> Hello-
> 
> I have a script which steps through a series of subjects, and for the 
> subjects I remove outlying values. After removing these outliers, I 
> specify a cutoff, keeping only values over a certain value (e.g., 1.96). 
> I want to populate a matrix with a statistic of the values that make the 
> cutoff (for example, the mean). However, in some subjects, after 
> outliers and the cutoff are specified, there are no data that meet the 
> criteria (I get <0 rows> (or 0-length row.names)). Here the script dies.
> 
> The solution I think is to specify a break so that the matrix will be 
> populated with a value (such as NA) and it will move on to the next 
> subject in the loop. However, I haven't been able to figure this out. If 
> anyone has any suggestions I would very much appreciate them. I have 
> paster part of the script below up to the point where it dies.
> 
> Thanks.
> 
> for (ss in levels(ss.list)) {
>         print(ss)
>         ss.count = ss.count + 1  
>         query.string <- paste("SELECT * FROM ",ss,";",sep="")
>         #print(query.string)
>         data_gcs <- dbGetQuery(con, query.string)
>         attach(data_gcs)
>         names(data_gcs)
>         mat_row = 0
>             for(i in levels(roi.list)){
>             print (i)
>             current.roi <- data_gcs[data_gcs$ROI==i,]
>             current.roi.plus.z <- data.frame(current.roi, 
> scale(current.roi[,15]), scale(current.roi[,18]), 
> scale(current.roi[,21]), scale(current.roi[,24]))
>             testextrem <- function(x) {if ((abs(x[1]) < 2.5) & 
> (abs(x[2]) < 2.5) & (abs(x[3]) < 2.5) & (abs(x[4]) < 2.5)) return(1) 
> else return(0)}
>             filtervector <- apply(as.vector(current.roi.plus.z[,c(27, 
> 28, 29, 30)]), 1, FUN=testextrem)
>             current.roi.plus.z.filter <- data.frame(current.roi.plus.z, 
> filtervector)
>             final.roi.df <- 
> current.roi.plus.z.filter[which(current.roi.plus.z.filter$filtervector==1),]
>             #print (final.roi.df)
>             kicked.out<-(length(filtervector) - 
> sum(filtervector))/length(filtervector)
>             print(kicked.out)
>             matrix.col = matrix.col + 1
>             attach(final.roi.df)
>             names(final.roi.df)
>             print(matrix.col)
>             #set cutoff for FDR. per voxel p values (of Z dist) .05 = 
> 1.96, .01 = 2.575, .001 = 3.277, .005 = 3.479 BE SURE TO CHANGE THE 
> VARIABLE EACH TIME YOU CHANGE CONDITION
>                pre.outliers<-subset(final.roi.df, gFDR4FWHM >= 1.96)
>                detach(final.roi.df)
>                attach(pre.outliers)
>                outliers<-subset(pre.outliers, gtvalue4FWHM >= 0.00)
> ...and the script dies here because there are no data in "outliers".
>


From maitra at iastate.edu  Sun Mar 25 20:56:24 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Sun, 25 Mar 2007 13:56:24 -0500
Subject: [R] Installing R on a machine with 64-bit Opteron processors
In-Reply-To: <591E6C77-6228-4881-9144-DDF16C84F4B5@temple.edu>
References: <591E6C77-6228-4881-9144-DDF16C84F4B5@temple.edu>
Message-ID: <20070325135624.6c40d753@triveni.stat.iastate.edu>

Hi Stan,

I haven't used SuSE for four years now, but the errors all point to libraries and programs that need to be installed. I used in on a 32-bit machine then.

As I recall, SuSE used to have a fabulous installation tool called yast2 (in my opinion, the best of the lot in packaging), though FC seems to have done something similar with pirut. In any case, isn't this option available with SuSE for 64-bit machines? That would be really strange. Anyway, I distinctly recall that R was in the SuSE package in those days, so perhaps it should still be there (I think the package used to be called r-stat, but my memory is a little unclear on this.)

If so, you are better off installing that because the dependencies are installed automatically. The other option is to use yum or synaptic, but you will need to set the repositories up.

I am curious: why choose SuSE? It used to be the rage some time ago, especially because it was one of the first with a GUI installation interface, and the above mentioned YaST tool, but fewer people use seem to use it nowadays. 

HTH.

Best,
Ranjan







On Sun, 25 Mar 2007 13:36:47 -0400 Stan Horwitz <stan at temple.edu> wrote:

> I have been tasked with installing statistical and other data  
> analysis applications on a new Sun Fire X4600 M2 x64 server that came  
> equipped with eight AMD dual core Opteronn 64-bit processors. It is  
> running the 64-bit version of Suse Linux 9.
> 
> I have read through the installation docs, and I guess I don't  
> understand what to do, or even how to identify which version, if any,  
> of this software is suitable for my computer environment.
> 
> What I want to know is if R has been ported to that environment and  
> if so, how do I install it? I tried downloading it from one of the  
> mirror web sites, and when I install it, I get
> 
>   euler src/R-base-2.4.1# rpm -i R-base-2.4.1-2.1.i586.rpm
> warning: R-base-2.4.1-2.1.i586.rpm: V3 DSA signature: NOKEY, key ID  
> 6b9d6523
> error: Failed dependencies:
>          xorg-x11-fonts-100dpi is needed by R-base-2.4.1-2.1
>          xorg-x11-fonts-75dpi is needed by R-base-2.4.1-2.1
>          xorg-x11-libs is needed by R-base-2.4.1-2.1
>          blas is needed by R-base-2.4.1-2.1
>          libreadline.so.5 is needed by R-base-2.4.1-2.1
> euler src/R-base-2.4.1#
> 
> I also tried the x386 version with the same results.
> 
> The documentation lists some prerequisite items to install such as  
> blas, but I have searched and I can't see to locate that software. Is  
> it public domain or a commercial product? I also see that my server  
> already has x11 fonts installed, so I don't know what those errors  
> are about.
> 
> Here's what rpm says about fonts ...
> 
> fontconfig-2.2.92.20040221-28.13
> fontconfig-32bit-9-200407011229
> fontconfig-devel-32bit-9-200407011229
> ghostscript-fonts-other-7.07.1rc1-195.8
> XFree86-fonts-75dpi-4.3.99.902-43.71
> fontforge-20060715-7.3
> ghostscript-fonts-std-7.07.1rc1-195.8
> fontconfig-devel-2.2.92.20040221-28.13
> xorg-x11-fonts-scalable-6.9.0-48
> efont-unicode-0.4.0-630.1
> 
> So isn't "xorg-x11-fonts-scalable-6.9.0-48" what it wants? I guess  
> not. Is there a list of what this software needs AND where to  
> download or purchase each item for Suse 9?
> 
> I am also wondering if this list has searchable archives? The list's  
> web site shows archived postings, but I don't see a way to search them.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From churata at ufpr.br  Sun Mar 25 21:31:43 2007
From: churata at ufpr.br (Bruno Grimaldo Martinho Churata)
Date: Sun, 25 Mar 2007 16:31:43 -0300
Subject: [R] anova-interaction
Message-ID: <4606CE1F.30506@ufpr.br>

HI,

I am trying to perform ANOVA with 2 factors:
material (3), temperature(3). The interaction is significant. I tried
something like, ( summary(avt, split=list("temp:mat"=list("15"=1, 
"70"=2, "125"=3))) is not correct).
Thanks,

av <- aov(time ~ mat*temp, data=dados)
avt <- aov(time ~ temp/mat)
summary(avt, split=list("temp:mat"=list("15"=1, "70"=2, "125"=3)))
avm <- aov(time ~ mat/temp)
summary(amv, split=list("mat:temp"=list("1"=c(1,4), "2"=c(2,5), 
"3"=c(3,6))))

data
    time mat temp
1   130   1   15
2   155   1   15
3    74   1   15
4   180   1   15
5   150   2   15
6   188   2   15
7   159   2   15
8   126   2   15
9   138   3   15
10  110   3   15
11  168   3   15
12  160   3   15
13   34   1   70
14   40   1   70
15   80   1   70
16   75   1   70
17  136   2   70
18  122   2   70
19  106   2   70
20  115   2   70
21  174   3   70
22  120   3   70
23  150   3   70
24  139   3   70
25   20   1  125
26   70   1  125
27   82   1  125
28   58   1  125
29   25   2  125
30   70   2  125
31   58   2  125
32   45   2  125
33   96   3  125
34  104   3  125
35   82   3  125
36   60   3  125


From deepayan.sarkar at gmail.com  Sun Mar 25 21:34:41 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sun, 25 Mar 2007 12:34:41 -0700
Subject: [R] controlling panel.width and panel.height in viewports
In-Reply-To: <6.2.5.6.1.20070325131002.027b3550@ucl.ac.uk>
References: <6.2.5.6.1.20070325131002.027b3550@ucl.ac.uk>
Message-ID: <eb555e660703251234q5423d20eve3bf73de42b911c1@mail.gmail.com>

On 3/25/07, mike townsley <m.townsley at ucl.ac.uk> wrote:
> Dear all,
>
> I'm trying to get a series of lattice levelplots to appear in
> viewports in a particular way but struggling to exert fine control
> over their appearence.  There are two conditions: (a) I only want the
> levelplot to appear (I don't want axes, colour key, etc) in the
> viewport and (b) I want the levelplot to expand to the maximum
> allowable space in the viewport while observing the aspect ratio of the plot.
>
> Condition (a) is OK, but (b) is giving me trouble.  A toy example is:
>
> library(lattice)
> library(grid)
>
> x <- 1:10
> y <- 1:10
> grid <- expand.grid(x=x, y=y)
> grid$z <- grid$x*grid$y
>
> asp.ratio.1 <- 2
> asp.ratio.2 <- .5
> asp.ratio.3 <- 1
>
> test.1 <- levelplot(z~x*y, grid, cuts = 5, xlab="", axes = FALSE,
>                      aspect = asp.ratio.1, ylab="", main='', sub="",
>                      colorkey = FALSE, region = TRUE, scales =
> list(draw = FALSE))
> test.2 <- levelplot(z~x*y, grid, cuts = 5, xlab="", axes = FALSE,
>                      aspect = asp.ratio.2, ylab="", main='', sub="",
>                      colorkey = FALSE, region = TRUE, scales =
> list(draw = FALSE))
> test.3 <- levelplot(z~x*y, grid, cuts = 5, xlab="", axes = FALSE,
>                      aspect = asp.ratio.3, ylab="", main='', sub="",
>                      colorkey = FALSE, region = TRUE, scales =
> list(draw = FALSE))
>
> # so the three basic types of plot I will have
>
>
> toy.vp <- viewport(width = .8, height = .8, just = 'centre',
>                         name = 'toy')  # this is for my levelplot
>
> X11()
> pushViewport(toy.vp)
> grid.rect(gp = gpar(col = 'grey'))
> print(test.1, newpage = FALSE)
>
> X11()
> pushViewport(toy.vp)
> grid.rect(gp = gpar(col = 'grey'))
> print(test.2, newpage = FALSE)
>
> X11()
> pushViewport(toy.vp)
> grid.rect(gp = gpar(col = 'grey'))
> print(test.3, newpage = FALSE)
>
>
> I want to get rid of the space (margins) around the longest axis for
> each variety of levelplot.  So I tried to capture the viewport height
> and width and print the levelplot within a region which reflects the
> aspect ratio of the plot.  I run into an error message as below:
>
> width.vp <- convertWidth(grobWidth(grid.rect()), 'cm')
> height.vp <- convertWidth(grobHeight(grid.rect()), 'cm')
>
> width.vp;height.vp # this works
>
> if(asp.ratio.1 < 1) {height.vp <-
> unit(as.numeric(height.vp)*asp.ratio.1, "cm")}
> if(asp.ratio.1 > 1) {width.vp <- unit(as.numeric(width.vp)/asp.ratio.1, "cm")}
>
> width.vp;height.vp # this also work (ie returns expected results)
>
> print(test.1, panel.width = list(width.vp), panel.height=list(height.vp))
>
> the above command returns this error message:
>
> Error in Ops.unit(panel.height[[1]], panel.width[[1]]) :
>          Operator '/' not meaningful for units
>
> What have I missed?

The part in ?print.trellis which says:

panel.width, panel.height: lists with 2 components, that should be
          valid 'x' and 'units' arguments to 'unit()'

You are specifying "unit" objects directly, which is not supported.
You probably wanted to do something like the following:

######
toy.vp <- viewport(width = .8, height = .8, just = 'centre',
                   name = 'toy')  # this is for my levelplot

grid.newpage()
pushViewport(toy.vp)
grid.rect(gp = gpar(col = 'grey'))

width.vp <- convertWidth(grobWidth(grid.rect(draw = FALSE)), 'cm',
valueOnly = TRUE)
height.vp <- convertWidth(grobHeight(grid.rect(draw = FALSE)), 'cm',
valueOnly = TRUE)

if(asp.ratio.1 < 1) height.vp <- as.numeric(height.vp) * asp.ratio.1
if(asp.ratio.1 > 1) width.vp <- as.numeric(width.vp)/asp.ratio.1
width.vp;height.vp

print(test.1,
      panel.width = list(width.vp, "cm"),
      panel.height=list(height.vp, "cm"),
      newpage = FALSE)
######

This runs, but doesn't give you what you want (I haven't tried to
figure out why).

I think you are taking the wrong approach to your problem. levelplot()
is a high level function, it's not supposed to be used this way. On
the other hand, you do have the ``low-level'' function
panel.levelplot, which _is_ designed to do things inside a grid
viewport. You could even avoid the necessary setup steps by getting
the relevant information by querying your "trellis" object, e.g.:

######

grid.newpage()

toy.vp <-
    viewport(width = .8, height = .8,
             just = 'centre',
             layout = # necessary to fix aspect ratio
             grid.layout(1, 1,
                         widths = 1,
                         heights = asp.ratio.1,
                         respect = TRUE),
             name = 'toy')

pushViewport(toy.vp)

grid.rect(gp = gpar(col = 'grey'))

pushViewport(viewport(layout.pos.row = 1, layout.pos.col = 1,
                      xscale = test.1$x.limits,
                      yscale = test.1$y.limits))

do.call("panel.levelplot", trellis.panelArgs(test.1, 1))

upViewport(2)

######

Hope that helps,

-Deepayan


From bgreen at dyson.brisnet.org.au  Sun Mar 25 22:29:43 2007
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Mon, 26 Mar 2007 06:29:43 +1000
Subject: [R] resolving expand.grid & NA errors
In-Reply-To: <mailman.11.1174647603.14804.r-help@stat.math.ethz.ch>
References: <mailman.11.1174647603.14804.r-help@stat.math.ethz.ch>
Message-ID: <20070325202637.CD2685951BA@borg.st.net.au>

I am hoping for some advice regarding resolving error messages I have 
received when trying to use the expand.grid command.

library(nnet)
library(MASS)
library(car)
mod.multacute <-multinom(kc$group ~ kc$in.acute.danger * 
kc$violent.convictions, na.rm=T)
summary(mod.multacute, cor=F, Wald=T)
Anova (mod.multacute)
confint (mod.multacute)

 > predictors <- expand.grid(group=1:3, in.acute.danger = c("y","n"), 
violent.convictions = c("y","n"))
 > p.fit <- predict(mod.multacute, predictors, type='probs')
Error in predict.multinom(mod.multacute, predictors, type = "probs") :
         NAs are not allowed in subscripted assignments
In addition: Warning message:
'newdata' had 12 rows but variable(s) found have 160 rows

There are two errors - the NA error which I thought would have been 
removed in line 3 above. I also tried ra.omit.
I know there will be a difference between the raw data and the new 
data but do not know what I need to change to be able to successfully 
run the command.

What I want to do is obtain the fitted probabilities and plot them.

Any suggestions are appreciated,

Bob Green


From A.Robinson at ms.unimelb.edu.au  Sun Mar 25 22:46:29 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Mon, 26 Mar 2007 06:46:29 +1000
Subject: [R] resolving expand.grid & NA errors
In-Reply-To: <20070325202637.CD2685951BA@borg.st.net.au>
References: <mailman.11.1174647603.14804.r-help@stat.math.ethz.ch>
	<20070325202637.CD2685951BA@borg.st.net.au>
Message-ID: <20070325204629.GN39176@ms.unimelb.edu.au>

Hi Bob,

I'm not sure that na.rm is a valid action inside multinom.  Try
removing the missing values before fitting, or use
na.action=na.exclude (or na.action=na.omit) inside multinom instead.

If that doesn't help, then please try to send a commented, minimal,
self-contained, reproducible example.

Cheers,

Andrew

On Mon, Mar 26, 2007 at 06:29:43AM +1000, Bob Green wrote:
> I am hoping for some advice regarding resolving error messages I have 
> received when trying to use the expand.grid command.
> 
> library(nnet)
> library(MASS)
> library(car)
> mod.multacute <-multinom(kc$group ~ kc$in.acute.danger * 
> kc$violent.convictions, na.rm=T)
> summary(mod.multacute, cor=F, Wald=T)
> Anova (mod.multacute)
> confint (mod.multacute)
> 
>  > predictors <- expand.grid(group=1:3, in.acute.danger = c("y","n"), 
> violent.convictions = c("y","n"))
>  > p.fit <- predict(mod.multacute, predictors, type='probs')
> Error in predict.multinom(mod.multacute, predictors, type = "probs") :
>          NAs are not allowed in subscripted assignments
> In addition: Warning message:
> 'newdata' had 12 rows but variable(s) found have 160 rows
> 
> There are two errors - the NA error which I thought would have been 
> removed in line 3 above. I also tried ra.omit.
> I know there will be a difference between the raw data and the new 
> data but do not know what I need to change to be able to successfully 
> run the command.
> 
> What I want to do is obtain the fitted probabilities and plot them.
> 
> Any suggestions are appreciated,
> 
> Bob Green
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From Bill.Venables at csiro.au  Mon Mar 26 00:58:54 2007
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Mon, 26 Mar 2007 08:58:54 +1000
Subject: [R] resolving expand.grid & NA errors
References: <mailman.11.1174647603.14804.r-help@stat.math.ethz.ch>
	<20070325202637.CD2685951BA@borg.st.net.au>
Message-ID: <B998A44C8986644EA8029CFE6396A924B672F5@exqld2-bne.qld.csiro.au>

Hi Bob,

Instead of fitting the model as 

> mod.multacute <-multinom(kc$group ~ kc$in.acute.danger * 
>	kc$violent.convictions, na.rm=T)

you might have more success fitting it sa

> mod.multacute <- multinom(group ~ in.acute.danger * 
	violent.convictions, data = kc,  na.action = na.omit)

This separates the variables from the data frame in which they occur.
When you predict with a new data frame, the process will look for
factors "in.acure.danger" and not "kc$in.acute.danger".

Note also that na.rm is not an argument for multinom.  You probably mean
na.action.

I suspect this is the problem, but without the data I can't be sure.

Bill Venables
CSIRO Laboratories
PO Box 120, Cleveland, 4163
AUSTRALIA
Office Phone (email preferred):   +61 7 3826 7251
Fax (if absolutely necessary):      +61 7 3826 7304
Mobile:                                   (I don't have one!)
Home Phone:                            +61 7 3286 7700
mailto:Bill.Venables at csiro.au
http://www.cmis.csiro.au/bill.venables/ 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bob Green
Sent: Monday, 26 March 2007 6:30 AM
To: r-help at stat.math.ethz.ch
Subject: [R] resolving expand.grid & NA errors

I am hoping for some advice regarding resolving error messages I have 
received when trying to use the expand.grid command.

library(nnet)
library(MASS)
library(car)
mod.multacute <-multinom(kc$group ~ kc$in.acute.danger * 
kc$violent.convictions, na.rm=T)
summary(mod.multacute, cor=F, Wald=T)
Anova (mod.multacute)
confint (mod.multacute)

 > predictors <- expand.grid(group=1:3, in.acute.danger = c("y","n"), 
violent.convictions = c("y","n"))
 > p.fit <- predict(mod.multacute, predictors, type='probs')
Error in predict.multinom(mod.multacute, predictors, type = "probs") :
         NAs are not allowed in subscripted assignments
In addition: Warning message:
'newdata' had 12 rows but variable(s) found have 160 rows

There are two errors - the NA error which I thought would have been 
removed in line 3 above. I also tried ra.omit.
I know there will be a difference between the raw data and the new 
data but do not know what I need to change to be able to successfully 
run the command.

What I want to do is obtain the fitted probabilities and plot them.

Any suggestions are appreciated,

Bob Green

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From mark at markfisher.net  Mon Mar 26 02:08:32 2007
From: mark at markfisher.net (Mark Fisher)
Date: Sun, 25 Mar 2007 20:08:32 -0400
Subject: [R] BRugs command modelGenInits() crashes R version 2.4.1 (windows)
Message-ID: <200703260010.l2Q0A6Go003692@rs19.luxsci.com>

Is it appropriate to ask about this here? (I didn't see anything in the 
FAQ.)

--Mark


From mcpung at gmail.com  Mon Mar 26 03:08:25 2007
From: mcpung at gmail.com (Murray Pung)
Date: Mon, 26 Mar 2007 11:08:25 +1000
Subject: [R] Greenwood's Variance
Message-ID: <8d6f66050703251808x23c963d0wdefd3a8408e000a7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070326/f2b38244/attachment.pl 

From adrian at maths.uwa.edu.au  Mon Mar 26 04:05:41 2007
From: adrian at maths.uwa.edu.au (Adrian Baddeley)
Date: Mon, 26 Mar 2007 10:05:41 +0800
Subject: [R]  sampling from the uniform distribution over a convex hull
Message-ID: <17927.10869.388333.48289@maths.uwa.edu.au>

Ranjan Maitra writes:

> Does anyone have a suggestion (or better still) code for sampling
> from the uniform distribution over the convex hull of a set of
> points?

This is implemented in library 'spatstat'.

If x and y are vectors of coordinates of your initial set of points,

   library(spatstat)

   W <- convexhull.xy(x, y)
   
   P <- runifpoint(42, W)

will compute the convex hull and generate 42 independent 
uniformly-distributed points in the convex hull.

The result can be plotted by 
    plot(P)
and the coordinates of the simulated points can be extracted
as P$x and P$y.

Adrian Baddeley


From jsorkin at grecc.umaryland.edu  Mon Mar 26 04:19:25 2007
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Sun, 25 Mar 2007 22:19:25 -0400
Subject: [R] Problem dropping rows based on values in a column
Message-ID: <4606E75B.A712.00CB.0@grecc.umaryland.edu>

I am trying to drop rows of a dataframe based on values of the column PID, but my strategy is not working. I hope someoen can tell me what I am doing incorrectly.


# Values of PID column
> jdata[,"PID"]
 [1] 16608 16613 16355 16378 16371 16280 16211 16169 16025 11595 15883 15682 15617 15615 15212 14862 16539
[18] 12063 16755 16720 16400 16257 16209 16200 16144 11598 13594 15419 15589 15982 15825 15834 15491 15822
[35] 15803 15795 10202 15680 15587 15552 15588 15375 15492 15568 15196 10217 15396 15477 15446 15374 14092
[52] 14033 15141 14953 15473 10424 13445 14854 10481 14793 14744 14772

#Prepare to drop last two rows, rows that ahve 14744 and 14772 in the PID column
> delete<-c(14772,14744)

#Try to delete last two rows, but as you will see, I am not able to drop the last two rows.
> jdata[jdata$PID!=delete,"PID"]
 [1] 16608 16613 16355 16378 16371 16280 16211 16169 16025 11595 15883 15682 15617 15615 15212 14862 16539
[18] 12063 16755 16720 16400 16257 16209 16200 16144 11598 13594 15419 15589 15982 15825 15834 15491 15822
[35] 15803 15795 10202 15680 15587 15552 15588 15375 15492 15568 15196 10217 15396 15477 15446 15374 14092
[52] 14033 15141 14953 15473 10424 13445 14854 10481 14793 14744 14772
> 


Thanks,
John

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC,
University of Maryland School of Medicine Claude D. Pepper OAIC,
University of Maryland Clinical Nutrition Research Unit, and
Baltimore VA Center Stroke of Excellence

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)
jsorkin at grecc.umaryland.edu

Confidentiality Statement:
This email message, including any attachments, is for the so...{{dropped}}


From marc_schwartz at comcast.net  Mon Mar 26 05:07:49 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sun, 25 Mar 2007 22:07:49 -0500
Subject: [R] Problem dropping rows based on values in a column
In-Reply-To: <4606E75B.A712.00CB.0@grecc.umaryland.edu>
References: <4606E75B.A712.00CB.0@grecc.umaryland.edu>
Message-ID: <1174878469.7276.61.camel@Bellerophon>

On Sun, 2007-03-25 at 22:19 -0400, John Sorkin wrote:
> I am trying to drop rows of a dataframe based on values of the column PID, but my strategy is not working. I hope someoen can tell me what I am doing incorrectly.
> 
> 
> # Values of PID column
> > jdata[,"PID"]
>  [1] 16608 16613 16355 16378 16371 16280 16211 16169 16025 11595 15883 15682 15617 15615 15212 14862 16539
> [18] 12063 16755 16720 16400 16257 16209 16200 16144 11598 13594 15419 15589 15982 15825 15834 15491 15822
> [35] 15803 15795 10202 15680 15587 15552 15588 15375 15492 15568 15196 10217 15396 15477 15446 15374 14092
> [52] 14033 15141 14953 15473 10424 13445 14854 10481 14793 14744 14772
> 
> #Prepare to drop last two rows, rows that ahve 14744 and 14772 in the PID column
> > delete<-c(14772,14744)
> 
> #Try to delete last two rows, but as you will see, I am not able to drop the last two rows.
> > jdata[jdata$PID!=delete,"PID"]
>  [1] 16608 16613 16355 16378 16371 16280 16211 16169 16025 11595 15883 15682 15617 15615 15212 14862 16539
> [18] 12063 16755 16720 16400 16257 16209 16200 16144 11598 13594 15419 15589 15982 15825 15834 15491 15822
> [35] 15803 15795 10202 15680 15587 15552 15588 15375 15492 15568 15196 10217 15396 15477 15446 15374 14092
> [52] 14033 15141 14953 15473 10424 13445 14854 10481 14793 14744 14772
> > 

John,

If you had:

  delete <- c(14744, 14773)

it would likely work, but only in this particular setting where you are
comparing two sequential values. 

That is because you are testing a sequence of two values and the way
that you have them above, they are reversed from the order in which the
values actually appear.

For example:

Vec <- 1:10
delete <- 10:9

> Vec[Vec != delete]
 [1]  1  2  3  4  5  6  7  8  9 10


However:

delete <- 9:10

> Vec[Vec != delete]
[1] 1 2 3 4 5 6 7 8


Note what happens when the values in the source vector are not
sequential:

Vec <- sample(10)

> Vec
 [1]  5  1  7  3 10  8  2  6  9  4

delete <- 9:10

> Vec[Vec != delete]
[1]  5  1  7  3 10  8  2  6  4

delete <- 10:9

> Vec[Vec != delete]
[1] 5 1 7 3 8 2 6 9 4


You get a result in which the first value in 'delete' is removed, but
not the second.


When performing a logical comparison of a value to see if it is (or is
not) in a set of values, you want to use '%in%':

Vec <- 1:10

delete <- 10:9

> Vec[!Vec %in% delete]
[1] 1 2 3 4 5 6 7 8

delete <- 9:10

> Vec[!Vec %in% delete]
[1] 1 2 3 4 5 6 7 8


It also works in the permuted vector:

> Vec[!Vec %in% delete]
[1] 5 1 7 3 8 2 6 4


See ?"%in%" for more information.

HTH,

Marc Schwartz


From liuwensui at gmail.com  Mon Mar 26 05:12:37 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Sun, 25 Mar 2007 23:12:37 -0400
Subject: [R] Problem dropping rows based on values in a column
In-Reply-To: <4606E75B.A712.00CB.0@grecc.umaryland.edu>
References: <4606E75B.A712.00CB.0@grecc.umaryland.edu>
Message-ID: <1115a2b00703252012q33ad62c2w40bc74cc706795ad@mail.gmail.com>

> jdata
    PID
1 14854
2 10481
3 14793
4 14744
5 14772
> jdata[jdata[1] != delete, 1]
[1] 14854 10481 14793


On 3/25/07, John Sorkin <jsorkin at grecc.umaryland.edu> wrote:
> I am trying to drop rows of a dataframe based on values of the column PID, but my strategy is not working. I hope someoen can tell me what I am doing incorrectly.
>
>
> # Values of PID column
> > jdata[,"PID"]
>  [1] 16608 16613 16355 16378 16371 16280 16211 16169 16025 11595 15883 15682 15617 15615 15212 14862 16539
> [18] 12063 16755 16720 16400 16257 16209 16200 16144 11598 13594 15419 15589 15982 15825 15834 15491 15822
> [35] 15803 15795 10202 15680 15587 15552 15588 15375 15492 15568 15196 10217 15396 15477 15446 15374 14092
> [52] 14033 15141 14953 15473 10424 13445 14854 10481 14793 14744 14772
>
> #Prepare to drop last two rows, rows that ahve 14744 and 14772 in the PID column
> > delete<-c(14772,14744)
>
> #Try to delete last two rows, but as you will see, I am not able to drop the last two rows.
> > jdata[jdata$PID!=delete,"PID"]
>  [1] 16608 16613 16355 16378 16371 16280 16211 16169 16025 11595 15883 15682 15617 15615 15212 14862 16539
> [18] 12063 16755 16720 16400 16257 16209 16200 16144 11598 13594 15419 15589 15982 15825 15834 15491 15822
> [35] 15803 15795 10202 15680 15587 15552 15588 15375 15492 15568 15196 10217 15396 15477 15446 15374 14092
> [52] 14033 15141 14953 15473 10424 13445 14854 10481 14793 14744 14772
> >
>
>
> Thanks,
> John
>
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC,
> University of Maryland School of Medicine Claude D. Pepper OAIC,
> University of Maryland Clinical Nutrition Research Unit, and
> Baltimore VA Center Stroke of Excellence
>
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> jsorkin at grecc.umaryland.edu
>
> Confidentiality Statement:
> This email message, including any attachments, is for the so...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From liuwensui at gmail.com  Mon Mar 26 05:50:29 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Sun, 25 Mar 2007 23:50:29 -0400
Subject: [R] Problem dropping rows based on values in a column
In-Reply-To: <4606E75B.A712.00CB.0@grecc.umaryland.edu>
References: <4606E75B.A712.00CB.0@grecc.umaryland.edu>
Message-ID: <1115a2b00703252050q7f88f201od035fa960e4ea14c@mail.gmail.com>

Sorry, John
Marc's method is correct.

On 3/25/07, John Sorkin <jsorkin at grecc.umaryland.edu> wrote:
> I am trying to drop rows of a dataframe based on values of the column PID, but my strategy is not working. I hope someoen can tell me what I am doing incorrectly.
>
>
> # Values of PID column
> > jdata[,"PID"]
>  [1] 16608 16613 16355 16378 16371 16280 16211 16169 16025 11595 15883 15682 15617 15615 15212 14862 16539
> [18] 12063 16755 16720 16400 16257 16209 16200 16144 11598 13594 15419 15589 15982 15825 15834 15491 15822
> [35] 15803 15795 10202 15680 15587 15552 15588 15375 15492 15568 15196 10217 15396 15477 15446 15374 14092
> [52] 14033 15141 14953 15473 10424 13445 14854 10481 14793 14744 14772
>
> #Prepare to drop last two rows, rows that ahve 14744 and 14772 in the PID column
> > delete<-c(14772,14744)
>
> #Try to delete last two rows, but as you will see, I am not able to drop the last two rows.
> > jdata[jdata$PID!=delete,"PID"]
>  [1] 16608 16613 16355 16378 16371 16280 16211 16169 16025 11595 15883 15682 15617 15615 15212 14862 16539
> [18] 12063 16755 16720 16400 16257 16209 16200 16144 11598 13594 15419 15589 15982 15825 15834 15491 15822
> [35] 15803 15795 10202 15680 15587 15552 15588 15375 15492 15568 15196 10217 15396 15477 15446 15374 14092
> [52] 14033 15141 14953 15473 10424 13445 14854 10481 14793 14744 14772
> >
>
>
> Thanks,
> John
>
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC,
> University of Maryland School of Medicine Claude D. Pepper OAIC,
> University of Maryland Clinical Nutrition Research Unit, and
> Baltimore VA Center Stroke of Excellence
>
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> jsorkin at grecc.umaryland.edu
>
> Confidentiality Statement:
> This email message, including any attachments, is for the so...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From Bill.Venables at csiro.au  Mon Mar 26 06:00:51 2007
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Mon, 26 Mar 2007 14:00:51 +1000
Subject: [R] Problem dropping rows based on values in a column
References: <4606E75B.A712.00CB.0@grecc.umaryland.edu>
Message-ID: <B998A44C8986644EA8029CFE6396A924B672FE@exqld2-bne.qld.csiro.au>

I think you want

delete <- c(14772,14744)
jdata <- subset(jdata, !(PID %in% delete))



Bill Venables
CSIRO Laboratories
PO Box 120, Cleveland, 4163
AUSTRALIA
Office Phone (email preferred):   +61 7 3826 7251
Fax (if absolutely necessary):      +61 7 3826 7304
Mobile:                                   (I don't have one!)
Home Phone:                            +61 7 3286 7700
mailto:Bill.Venables at csiro.au
http://www.cmis.csiro.au/bill.venables/ 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Sorkin
Sent: Monday, 26 March 2007 12:19 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Problem dropping rows based on values in a column

I am trying to drop rows of a dataframe based on values of the column
PID, but my strategy is not working. I hope someoen can tell me what I
am doing incorrectly.


# Values of PID column
> jdata[,"PID"]
 [1] 16608 16613 16355 16378 16371 16280 16211 16169 16025 11595 15883
15682 15617 15615 15212 14862 16539
[18] 12063 16755 16720 16400 16257 16209 16200 16144 11598 13594 15419
15589 15982 15825 15834 15491 15822
[35] 15803 15795 10202 15680 15587 15552 15588 15375 15492 15568 15196
10217 15396 15477 15446 15374 14092
[52] 14033 15141 14953 15473 10424 13445 14854 10481 14793 14744 14772

#Prepare to drop last two rows, rows that ahve 14744 and 14772 in the
PID column
> delete<-c(14772,14744)

#Try to delete last two rows, but as you will see, I am not able to drop
the last two rows.
> jdata[jdata$PID!=delete,"PID"]
 [1] 16608 16613 16355 16378 16371 16280 16211 16169 16025 11595 15883
15682 15617 15615 15212 14862 16539
[18] 12063 16755 16720 16400 16257 16209 16200 16144 11598 13594 15419
15589 15982 15825 15834 15491 15822
[35] 15803 15795 10202 15680 15587 15552 15588 15375 15492 15568 15196
10217 15396 15477 15446 15374 14092
[52] 14033 15141 14953 15473 10424 13445 14854 10481 14793 14744 14772
> 


Thanks,
John

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC,
University of Maryland School of Medicine Claude D. Pepper OAIC,
University of Maryland Clinical Nutrition Research Unit, and
Baltimore VA Center Stroke of Excellence

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)
jsorkin at grecc.umaryland.edu

Confidentiality Statement:
This email message, including any attachments, is for the\ s...{{dropped}}


From petr.pikal at precheza.cz  Mon Mar 26 07:07:14 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 26 Mar 2007 07:07:14 +0200
Subject: [R] subtotal for same row data
In-Reply-To: <644e1f320703231313q4fd4bc1g97bca633a05f0ca6@mail.gmail.com>
References: <7E72CD7F4C0A994191ED9B3726FAEBF302739A78@NIHCESMLBX4.nih.gov>
Message-ID: <46077122.25745.2EA289@localhost>

Hi

Or perhaps aggregate

> aggregate(x$F, list(C1=x$C1, C2=x$C2), sum)
  C1 C2   x
1  1  2 0.5
2  2  2 0.5

Regards
Petr


On 23 Mar 2007 at 16:13, jim holtman wrote:

Date sent:      	Fri, 23 Mar 2007 16:13:30 -0400
From:           	"jim holtman" <jholtman at gmail.com>
To:             	"Yuan, Qiaoping (NIH/NIAAA) [E]" <qyuan at mail.nih.gov>
Copies to:      	r-help at stat.math.ethz.ch
Subject:        	Re: [R] subtotal for same row data

> try this:
> 
> > x <- as.data.frame(x)
> > x
>    C1 C2 C3   F
> R1  1  2  2 0.3
> R2  2  2  2 0.5
> R3  1  2  1 0.2
> > do.call('rbind',by(x, list(x$C1, x$C2), function(z){z$F <- sum(z$F);
> z[1,]}))
>    C1 C2 C3   F
> R1  1  2  2 0.5
> R2  2  2  2 0.5
> >
> 
> 
> 
> On 3/23/07, Yuan, Qiaoping (NIH/NIAAA) [E] <qyuan at mail.nih.gov> wrote:
> > > Hi, There, > > I would like to subtotal the number in a specified
> column for all rows > having the same data for specified columns. The
> following is the simple > example: > > > >
> x=matrix(c(1,2,2,0.3,2,2,2,0.5,1,2,1,0.2),3,4,byrow=T) > >
> rownames(x)=c("R1","R2","R3") > > colnames(x)=c("C1","C2","C3","F") >
> > x > C1 C2 C3 F > R1 1 2 2 0.3 > R2 2 2 2 0.5 > R3 1 2 1 0.2 > > I
> would like to get the subtotal in column "F" based on same row data in
> > column "C1" and "C2". The result should be like > > C1   C2     SumF
> > 1 2 0.5 # This is 0.3 + 0.2 from R1 and R3 > 2 2 0.5 > > Is there a
> simple way to do this? Any help will be greatly appreciated. > >
> Qiaoping Yuan > > ______________________________________________ >
> R-help at stat.math.ethz.ch mailing list >
> https://stat.ethz.ch/mailman/listinfo/r-help > PLEASE do read the
> posting guide > http://www.R-project.org/posting-guide.html > and
> provide commented, minimal, self-contained, reproducible code. >
> 
> 
> 
> -- 
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
> 
> What is the problem you are trying to solve?
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From xmeng at capitalbio.com  Mon Mar 26 08:43:24 2007
From: xmeng at capitalbio.com (XinMeng)
Date: Mon, 26 Mar 2007 14:43:24 +0800
Subject: [R] a question about "sign" of "cgh" package
Message-ID: <374891404.13286@capitalbio.com>


Hi all :
I'm a user of "cgh" package. Here's a question about "sign".
  
As to help on "sign":
sign = +1 is used to detect polysomy (regions of copy number change increase) in test:control logratios.
 
sign = -1 is used ?C inverting the sign of the logratios ?C to detect deletions (regions of copy number decrease).
 
I wanna know whether my opinion about "sign" is correct:
If I think ratio > 2 and ratio < 0.5 are considered as copy number increase and decrease respectively.
So the corresponding sign is log2(2):+1 and log2(0.5):-1 respectively.
 
If I think ratio > 1.5 and ratio < 0.67 are considered as copy number increase and decrease respectively.
So the corresponding sign is log2(1.5):+0.585  and log2(1/1.5):-0.585 respectively.
 
I wanna know whether I'm right on "sign".
 
If not ,how can I use "sign" according to different "ratio" threshold of copy number increase and decrease?
 
Thanks a lot for your help!
 
My best regards


From p_connolly at ihug.co.nz  Mon Mar 26 09:02:32 2007
From: p_connolly at ihug.co.nz (Patrick Connolly)
Date: Mon, 26 Mar 2007 19:02:32 +1200
Subject: [R] creating R packs for all
In-Reply-To: <Pine.LNX.4.64.0703231755330.8593@gannet.stats.ox.ac.uk>
References: <200703231733.l2NHXw6E024566@gator.dt.uh.edu>
	<Pine.LNX.4.64.0703231755330.8593@gannet.stats.ox.ac.uk>
Message-ID: <20070326070232.GA3965@ihug.co.nz>

On Fri, 23-Mar-2007 at 06:00PM +0000, Prof Brian Ripley wrote:

|> On Fri, 23 Mar 2007, Erin Hodgess wrote:
|> 
|> > Dear R People:
|> >
|> > I am in the process of creating an R package via Windows.
|> >
|> > If I would decide to submit in to CRAN, what would I need to
|> > do in order to make it run for the Linux or Mac People, please?
|> 
|> If it passes R CMD check on Windows it should work anywhere.  About
|> the only thing that is fussier on Unix than Windows is the use of
|> the correct line endings for C etc files.  (OTOH, Windows adds
|> restrictions that 'R CMD check' checks for Unix users.)

There are graphics functions that can be called in platform specific
ways.  Hard-wired coding that calls a default plotting device with
unnamed arguments can make no sense on another platform.

Using Linux, one gets:

> args(x11)
function (display = "", width = 7, height = 7, pointsize = 12, 
    gamma = getOption("gamma"), colortype = getOption("X11colortype"), 
    maxcubesize = 256, bg = "transparent", canvas = "white", 
    fonts = getOption("X11fonts"), xpos = NA, ypos = NA) 

Using Windows, the first argument is absent so code that assumes
arguments are in the same order (say by not using names) irrespective
of platform will fail even though R CMD check looks fine. 

Perhaps it could all be summed up by a general problem with hard-wired
graphics calls (such as WMF files).  From my limited experience they
would not be that difficult to avoid.



-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From ligges at statistik.uni-dortmund.de  Mon Mar 26 09:17:45 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 26 Mar 2007 09:17:45 +0200
Subject: [R] BRugs command modelGenInits() crashes R version 2.4.1
	(windows)
In-Reply-To: <200703260010.l2Q0A6Go003692@rs19.luxsci.com>
References: <200703260010.l2Q0A6Go003692@rs19.luxsci.com>
Message-ID: <46077399.9030309@statistik.uni-dortmund.de>



Mark Fisher wrote:
> Is it appropriate to ask about this here? (I didn't see anything in the 
> FAQ.)


Can you please send me (the package maintainer) a reproducible example 
that crashes R in a private message?

Thanks,
Uwe Ligges


> --Mark
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mdsumner at utas.edu.au  Mon Mar 26 09:58:08 2007
From: mdsumner at utas.edu.au (Michael Sumner)
Date: Mon, 26 Mar 2007 17:58:08 +1000
Subject: [R] using alpha transparency for lines in levelplot
Message-ID: <46077D10.6080601@utas.edu.au>

Hello, I'm having trouble with using the alpha channel for transparency
with lines with lattice levelplots.

If I use transparency via the alpha argument to rgb to overplot lines on
levelplot the transparent colour affects all of the region colours in the
plot.

Can anyone explain why the difference in region colours?

#### Warning: this code attempts to create PDF files in working 
directory - they may be downloaded, see below

library(lattice)

## prepare data
data(volcano)
xy <- expand.grid(x = 1:nrow(volcano), y = 1:ncol(volcano))
xy$z <- as.vector(volcano)

## panel function to add lines with grey(0.3)
my.panel <- function(...) {
    panel.contourplot(...);
    panel.xyplot(1:60, runif(60, 1, 60), type = "l", lwd = 3, col = 
grey(0.3))
}

## this works fine
pdf("plot.pdf", version = "1.4")
levelplot(z~x+y, xy, panel = my.panel)
dev.off()

## panel function to add lines with grey transparency
my.paneltransp <- function(...) {
    panel.contourplot(...);
    panel.xyplot(1:60, runif(60, 1, 60), type = "l", lwd = 3, col = 
rgb(0.3, 0.3, 0.3, 0.5))
}

## this results in grey bleeding on the coloured regions of the plot
pdf("alpha.pdf", version = "1.4")
levelplot(z~x+y, xy, panel = my.paneltransp)
dev.off()

I've put the pdfs created here for reference:

http://staff.acecrc.org.au/~mdsumner/R/

Cheers, Mike.

I'm using Windows XP, SP2:

sessionInfo()
R version 2.4.1 (2006-12-18)
i386-pc-mingw32

locale:
LC_COLLATE=English_Australia.1252;LC_CTYPE=English_Australia.1252;LC_MONETARY=English_Australia.1252;LC_NUMERIC=C;LC_TIME=English_Australia.1252

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"  
[7] "base"     

other attached packages:
  lattice
"0.14-16"


From detlef.steuer at hsu-hamburg.de  Mon Mar 26 10:10:33 2007
From: detlef.steuer at hsu-hamburg.de (Detlef Steuer)
Date: Mon, 26 Mar 2007 10:10:33 +0200
Subject: [R] Installing R on a machine with 64-bit Opteron processors
In-Reply-To: <591E6C77-6228-4881-9144-DDF16C84F4B5@temple.edu>
References: <591E6C77-6228-4881-9144-DDF16C84F4B5@temple.edu>
Message-ID: <20070326101033.13ed759d.detlef.steuer@hsu-hamburg.de>

Stan,

On Sun, 25 Mar 2007 13:36:47 -0400
Stan Horwitz <stan at temple.edu> wrote:

> I have been tasked with installing statistical and other data  
> analysis applications on a new Sun Fire X4600 M2 x64 server that came  
> equipped with eight AMD dual core Opteronn 64-bit processors. It is  
> running the 64-bit version of Suse Linux 9.

the oldest version for which rpms are provided at the moment is 9.3 .
If you have something older you need to recompile yourself. 
Normally you just do the usual configure and make stuff and everything falls in place.

Nevertheless you need to install some libraries for that to work, too.
If you can not use yast2 and use the installation source given in the ReadMe
you can find out with 
rpm -q --whatprovides name_of_your_missing_dependency

what to install.

Hope that helps.
Detlef

> 
> I have read through the installation docs, and I guess I don't  
> understand what to do, or even how to identify which version, if any,  
> of this software is suitable for my computer environment.
> 
> What I want to know is if R has been ported to that environment and  
> if so, how do I install it? I tried downloading it from one of the  
> mirror web sites, and when I install it, I get
> 
>   euler src/R-base-2.4.1# rpm -i R-base-2.4.1-2.1.i586.rpm
> warning: R-base-2.4.1-2.1.i586.rpm: V3 DSA signature: NOKEY, key ID  
> 6b9d6523
> error: Failed dependencies:
>          xorg-x11-fonts-100dpi is needed by R-base-2.4.1-2.1
>          xorg-x11-fonts-75dpi is needed by R-base-2.4.1-2.1
>          xorg-x11-libs is needed by R-base-2.4.1-2.1
>          blas is needed by R-base-2.4.1-2.1
>          libreadline.so.5 is needed by R-base-2.4.1-2.1
> euler src/R-base-2.4.1#
> 
> I also tried the x386 version with the same results.
> 
> The documentation lists some prerequisite items to install such as  
> blas, but I have searched and I can't see to locate that software. Is  
> it public domain or a commercial product? I also see that my server  
> already has x11 fonts installed, so I don't know what those errors  
> are about.
> 
> Here's what rpm says about fonts ...
> 
> fontconfig-2.2.92.20040221-28.13
> fontconfig-32bit-9-200407011229
> fontconfig-devel-32bit-9-200407011229
> ghostscript-fonts-other-7.07.1rc1-195.8
> XFree86-fonts-75dpi-4.3.99.902-43.71
> fontforge-20060715-7.3
> ghostscript-fonts-std-7.07.1rc1-195.8
> fontconfig-devel-2.2.92.20040221-28.13
> xorg-x11-fonts-scalable-6.9.0-48
> efont-unicode-0.4.0-630.1
> 
> So isn't "xorg-x11-fonts-scalable-6.9.0-48" what it wants? I guess  
> not. Is there a list of what this software needs AND where to  
> download or purchase each item for Suse 9?
> 
> I am also wondering if this list has searchable archives? The list's  
> web site shows archived postings, but I don't see a way to search them.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
Die Dinge beim richtigen Namen nennen, ist der erste bedeutende revolution?re Akt.
Rosa Luxemburg


From sem at inode.at  Sun Mar 25 16:37:07 2007
From: sem at inode.at (Martin Semmernegg)
Date: Sun, 25 Mar 2007 16:37:07 +0200
Subject: [R] DCOM graphics bug?
Message-ID: <000001c76eeb$107ee4f0$317caed0$@at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070325/8c9eaa3d/attachment.pl 

From xwye at sibs.ac.cn  Mon Mar 26 11:13:53 2007
From: xwye at sibs.ac.cn (Xingwang Ye)
Date: Mon, 26 Mar 2007 17:13:53 +0800
Subject: [R] pearson or spearman partial correlation coefficient
Message-ID: <DreamMail__171353_06267885425@smtp.sibs.ac.cn>

Dear all,       

In SAS, we can use 
    
"proc corr data=a pearson spearman;
        var a;
        partial b c d;
        with e;
run;"
    
to get the partial pearson and spearman correlation coefficient and p value of a with e after adjustment b,c and d.

Are there any built-in functions available in any packages in R to get the partial pearson/spearman correlation coefficient and p value now?  

Thank you.      
 
Xingwang Ye    
PhD candidate     
Research Group of Nutrition Related Cancers and Other Chronic Diseases      
Institute for Nutritional Sciences,  
Shanghai Institutes of Biological Sciences,     
Chinese Academy of Sciences     
P.O.Box 32     
294 Taiyuan Road     
Shanghai 200031     
P.R.CHINA


From ted.harding at nessie.mcc.ac.uk  Mon Mar 26 12:24:23 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 26 Mar 2007 10:24:23 -0000 (BST)
Subject: [R] sampling from the uniform distribution over a convex hu
In-Reply-To: <17927.10869.388333.48289@maths.uwa.edu.au>
Message-ID: <XFMail.070326102423.ted.harding@nessie.mcc.ac.uk>

On 26-Mar-07 01:05:41, Adrian Baddeley wrote:
> Ranjan Maitra writes:
> 
>> Does anyone have a suggestion (or better still) code for sampling
>> from the uniform distribution over the convex hull of a set of
>> points?
> 
> This is implemented in library 'spatstat'.
> 
> If x and y are vectors of coordinates of your initial set of points,
> 
>    library(spatstat)
> 
>    W <- convexhull.xy(x, y)
> 
>   P <- runifpoint(42, W)
> 
> will compute the convex hull and generate 42 independent 
> uniformly-distributed points in the convex hull.

Thanks, Adrian! I should have remembered about 'spatstat' after
the Baddeley et al. paper to the RSS in June 2005, where the
package was extensively used! (Though neither convexhull() nor
runifpoint() are in the package I downloaded at the time; but of
course things have moved on!).

But this brings me to another issue -- It seems that the "R Site
Help and Archive Search" at http://finzi.psych.upenn.edu has been
belly-up since at least Saturday (when I tried to use it to search
for relevant material for this query).

My browser says "connection refused", ping hangs, and traceroute
returns "!H" (host unreachable); and that is still the situation
this morning. I hope this is temporary, and not a withdrawal of
the service.

Anyone in the know?

With thanks,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 26-Mar-07                                       Time: 10:23:57
------------------------------ XFMail ------------------------------


From Corinna.Schmitt at igb.fraunhofer.de  Mon Mar 26 11:33:31 2007
From: Corinna.Schmitt at igb.fraunhofer.de (Schmitt, Corinna)
Date: Mon, 26 Mar 2007 11:33:31 +0200
Subject: [R] matrix construction
Message-ID: <8B7B0FD99E8AF541A21609104D19615882E879@izs-xchg01.izs.fraunhofer.de>

Hallo,

can anyone tell me how I can create a matrix in R? I have two arrays A =
c(0:3), B=c(0:3). C should be the matrix. I just found the description
that a matrix is just an array with two substricpts. 

Thanks,

Corinna


From Thierry.ONKELINX at inbo.be  Mon Mar 26 11:42:43 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 26 Mar 2007 11:42:43 +0200
Subject: [R] matrix construction
In-Reply-To: <8B7B0FD99E8AF541A21609104D19615882E879@izs-xchg01.izs.fraunhofer.de>
Message-ID: <2E9C414912813E4EB981326983E0A10402C09C4B@inboexch.inbo.be>

Have a look at ?cbind, ?rbind and ?matrix

C <- cbind(A, B)
C <- rbind(A, B)
C <- matrix(c(0:3, 0:3), ncol = 2)

Cheers,

Thierry

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

 

> -----Oorspronkelijk bericht-----
> Van: r-help-bounces op stat.math.ethz.ch 
> [mailto:r-help-bounces op stat.math.ethz.ch] Namens Schmitt, Corinna
> Verzonden: maandag 26 maart 2007 11:34
> Aan: r-help op stat.math.ethz.ch
> Onderwerp: [R] matrix construction
> 
> Hallo,
> 
> can anyone tell me how I can create a matrix in R? I have two 
> arrays A = c(0:3), B=c(0:3). C should be the matrix. I just 
> found the description that a matrix is just an array with two 
> substricpts. 
> 
> Thanks,
> 
> Corinna
> 
> ______________________________________________
> R-help op stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gyadav at ccilindia.co.in  Mon Mar 26 12:01:28 2007
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Mon, 26 Mar 2007 15:31:28 +0530
Subject: [R] Problem in loading all packages all at once
Message-ID: <OF8B812C80.ECA62FAB-ON652572AA.00352924-652572AA.0037019F@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070326/d8c13e5b/attachment.pl 

From ripley at stats.ox.ac.uk  Mon Mar 26 12:33:52 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 26 Mar 2007 11:33:52 +0100 (BST)
Subject: [R] DCOM graphics bug?
In-Reply-To: <000001c76eeb$107ee4f0$317caed0$@at>
References: <000001c76eeb$107ee4f0$317caed0$@at>
Message-ID: <Pine.LNX.4.64.0703261130180.7566@gannet.stats.ox.ac.uk>

Assuming this is about the R (D)COM server from

http://cran.r-project.org/contrib/extra/dcom/00ReadMe.html

please note that it has its own mailing lists (accessed via the link on 
that page).  You will need to tell people the versions of everything you 
used (as the R posting guide asked you to).

On Sun, 25 Mar 2007, Martin Semmernegg wrote:

> Hi!
>
>
>
> I am trying to display a graphic using the dcom server and the dcom graphics
> object. Works fine without the embedded graphics object, in it's own
> windows, but if i use the embedded object (what I need to do) the colors are
> gone. Graphics are always only black and white. Even in the DCOM examples
> for VB the graphics are without color - is this a bug? Can I do anything
> about it?
>
>
>
> Thanks for your help!
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From milton_ruser at yahoo.com.br  Mon Mar 26 14:17:27 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Mon, 26 Mar 2007 05:17:27 -0700 (PDT)
Subject: [R] main title in multi-plot
Message-ID: <840656.98365.qm@web56606.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070326/bb01d7ae/attachment.pl 

From amsa36060 at yahoo.com  Mon Mar 26 14:23:02 2007
From: amsa36060 at yahoo.com (Amir Safari)
Date: Mon, 26 Mar 2007 05:23:02 -0700 (PDT)
Subject: [R] The Hurst Exponent in fBasic
Message-ID: <703639.89811.qm@web60421.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070326/ce180a71/attachment.pl 

From han at dmi.dk  Mon Mar 26 14:26:23 2007
From: han at dmi.dk (Henrik Andersson)
Date: Mon, 26 Mar 2007 14:26:23 +0200
Subject: [R] main title in multi-plot
In-Reply-To: <840656.98365.qm@web56606.mail.re3.yahoo.com>
References: <840656.98365.qm@web56606.mail.re3.yahoo.com>
Message-ID: <4607BBEF.7080602@dmi.dk>

Use par(oma) for outer margins and then title with outer=TRUE or mtext 
in the same way.

x<-1:100
y<-x
z<-y/x
w<-exp(z)

par(oma=c(0,0,2,0))
par(mfrow=c(2,2))
plot(y~x)
plot(z~x)
plot(w~x)
plot(z~w)
title(main="Main title",outer=T)

- Henrik

Milton Cezar Ribeiro wrote:
> Dear R-gurus,
>
> I need to print several plots into a graphic device and I would like to print out a main title.
> My code looks like 
>
> x<-1:100
> y<-x^2
> z<-y/x
> w<-exp(z)
> par(mfrow=c(2,2))
> plot(y~x)
> plot(z~x)
> plot(w~x)
> plot(z~w)
>
> Kind regards,
>
> miltinho
> Brazil
>
> __________________________________________________
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gyadav at ccilindia.co.in  Mon Mar 26 14:32:25 2007
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Mon, 26 Mar 2007 18:02:25 +0530
Subject: [R] a very small query
Message-ID: <OF7F71205A.5009D56F-ON652572AA.0044B22C-652572AA.0044D38F@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070326/4e086a56/attachment.pl 

From skiadas at hanover.edu  Mon Mar 26 14:34:18 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Mon, 26 Mar 2007 08:34:18 -0400
Subject: [R] Problem in loading all packages all at once
In-Reply-To: <OF8B812C80.ECA62FAB-ON652572AA.00352924-652572AA.0037019F@ccilindia.co.in>
References: <OF8B812C80.ECA62FAB-ON652572AA.00352924-652572AA.0037019F@ccilindia.co.in>
Message-ID: <4545F65F-E2F1-4B45-8CA5-99DDC6087452@hanover.edu>

A couple of things:

1) It would have been a lot easier to help you if you had created a  
much smaller example. perhaps trying to load 2-3 packages instead of  
1000. Now we have to wade through a lot of stuff to get to the point.

2) Perhaps your call to Sys.putenv doesn't do quite what you expect  
it to do. Example:
 > TEMP <- c("a","b")
 > Sys.getenv("TEST")
TEST
   ""
 > Sys.putenv(TEST=TEMP)
 > Sys.getenv("TEST")
TEST
"b"

3) I couldn't really see where in your code all these packages are  
loaded. Reading the help for .First.sys tells me that that's the call  
that actually loads the default packages, unless I've misunderstood  
something. If that's the case, then the following line you have at  
the end would prevent this loading:

.First.sys <- function()


I must say that from the ?Startup help there are two things that are  
not very clear:
a) At what point in the load process is R_DEFAULT_PACKAGES  
checked? .First.sys doesn't seem to know about it.
Ok, a bit of search provided an answer to this: Reading the help for  
"options", I see there, in the section about defaultPackages, what is  
probably the most helpful bit (for me) in all this:

defaultPackages:
	the packages that are attached by default when R starts up.  
Initially set from value of the environment variable  
R_DEFAULT_PACKAGES, or if that is unset to c("datasets", "utils",  
"grDevices", "graphics", "stats", "methods"). (Set R_DEFAULT_PACKAGES  
to NULL or a comma-separated list of package names.) A call to  
options should be in your ?.Rprofile? file to ensure that the change  
takes effect before the base package is initialized (see Startup).

	
b) What is the format that the R_DEFAULT_PACKAGES variable must have  
(I guess now I know, since it is mentioned in ?options, though that's  
not really the place I would have expected it to be explained) ? The  
only examples I could find on the web were setting it to NULL.



Hope all this helps in some way.

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


On Mar 26, 2007, at 6:01 AM, gyadav at ccilindia.co.in wrote:

>
> Hi All
>
> Please see the Rprofile file which i have modified as follows and  
> after
> that when I start R then I see that R says to me "TRUE" for all the
> packages implying that all loaded at once.
> But when i try to use commands as simple as help("lm"), it doesnt  
> work nor
> any of the menu "Packages" is not working.
> Although the regression using lm ( Y ~ X ) is working even summary and
> rnorm is working fine.
> Please tell me why menu and help command is not working
>
> Rprofile is in between +++++++++++++++++++++++++++ lines
> R initialization is in between @@@@@@@@@@@@@@ lines
> Commands are in between $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ lines
>
> Please also suggest me should i send this to r-devel list or not
>
> ### This is the system Rprofile file. It is always run on startup.
> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
> ++++
> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
> ++++
> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
> ++++
> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
> ++++
> options(scipen = 0)
> options(max.print = 10000)# not yet exercised in 'core R'
> options(add.smooth = TRUE)# currently only used in 'plot.lm'
>
> ###################################################################### 
> ########################
> ###################################################################### 
> ########################
> #############code added by gaurav yadav to install many other  
> packages at
> startup#############
> print ("START OF CODE ADDED BY GAURAV YADAV")
> TEMP <- Sys.getenv("R_DEFAULT_PACKAGES")
> print ("R_DEFAULT_PACKAGES")
> print (TEMP)
> print ("=======")
> TEMP <-
> c 
> (TEMP,"aplpack","approximator","apTreeshape","ArDec","arules","ash","a 
> space","assist","aster","asypow","aws","base","bayesm","bayesmix","bay 
> esSurv","BayesTree","BayesValidate","betareg","Bhat","BHH2","bicreduc" 
> ,"biglm","bim","bindata","Biodem","biopara","bitops","bivpois","blight 
> y","blockrand","BMA","boa","Bolstad","boolean","boost","boot","bootstr 
> ap","bqtl","BradleyTerry","brlr","BRugs","BSDA","BsMD","butler","calib 
> rate","calibrator","caMassClass","car","cat","caTools","catspec","cba" 
> ,"cclust","CDNmoney","cfa","CGIwithR","changeLOS","chplot","chron","Ci 
> rcStats","circular","clac","class","classInt","classPP","clim.pact","c 
> limatol","clines","clue","cluster","clusterRepro","clustvarsel","cmprs 
> k","cobs","CoCo","CoCoCg","CoCoCore","CoCoGraph","CoCoObjects","CoCoOl 
> dData","CoCoRaw","cocorresp","coda","coin","colorspace","combinat","co 
> mpositions","concor","concord","cond","conf.design","connectedness","c 
> opula","corpcor","corpora","covRobust","coxrobust","cramer","crossdes" 
> ,"crq","c
>  sampling","cslogistic","CTFS","ctv")
>
[snip]
> #TEMP <- c(TEMP,"runfirst","base","datasets", "utils", "grDevices",
> "graphics","stats","methods","lmtest")
> print ("MODIFIED R_DEFAULT_PACKAGES+++++++++++")
> print (TEMP)
> print ("++++++++++++++++")
> print(Sys.putenv(R_DEFAULT_PACKAGES=TEMP))
> print ("END OF CODE ADDED BY GAURAV YADAV")
>
> ###################################################################### 
> ########################
> ###################################################################### 
> ########################
>
>
> local({dp <- as.vector(Sys.getenv("R_DEFAULT_PACKAGES"))
>        if(identical(dp, "")) # marginally faster to do methods last
>            dp <- c("datasets", "utils", "grDevices", "graphics",
>                    "stats", "methods")
>        else if(identical(dp, "NULL")) dp <- character(0)
>        else dp <- strsplit(dp, ",")[[1]]
>        dp <- sub("[[:blank:]]*([[:alnum:]]+)", "\\1", dp) # strip
> whitespace
>        options(defaultPackages = dp)
>     })
>
> .First.sys <- function()
>
> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
> ++++
>
> THE R SHOWS ME THIS GIVEN HEREIN BELOW WHEN I START R
> @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
> [1] "START OF CODE ADDED BY GAURAV YADAV"
> [1] "R_DEFAULT_PACKAGES+++++++++++"
> R_DEFAULT_PACKAGES
>                 ""
> [1] "++++++++++++++++"
> [1] "MODIFIED R_DEFAULT_PACKAGES+++++++++++"
>   R_DEFAULT_PACKAGES
>                   ""            "aplpack"       "approximator"
>
>
[snip tons of packages]
> [1] "++++++++++++++++"
>   [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
> TRUE TRUE
> [751] TRUE TRUE TRUE TRUE TRUE TRUE TRUE
> [1] "END OF CODE ADDED BY GAURAV YADAV"
>
> R : Copyright 2006, The R Foundation for Statistical Computing
> Version 2.3.0 (2006-04-24)
> ISBN 3-900051-07-0
>
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
>
>   Natural language support but running in an English locale
>
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information and
> 'citation()' on how to cite R or R packages in publications.
>
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for an HTML browser interface to help.
> Type 'q()' to quit R.
>
> [Previously saved workspace restored]
>
>>
>
> @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
>
> $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
>
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for an HTML browser interface to help.
> Type 'q()' to quit R.
>
> [Previously saved workspace restored]
>
>> help("lm")
> Error: could not find function "help"
>> help.search("regression")
> Error: could not find function "help.search"
>> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
> Error in eval(expr, envir, enclos) : could not find function  
> "select.list"
>> chooseCRANmirror()
> Error: could not find function "chooseCRANmirror"
>> setRepositories()
> Error: could not find function "setRepositories"
>> utils:::menuInstallPkgs()
> --- Please select a CRAN mirror for use in this session ---
> Error in open.connection(file, "r") : unable to open connection
> In addition: Warning message:
> unable to connect to 'cran.r-project.org' on port 80.
> Error in contrib.url(repos, type) : trying to use CRAN without  
> setting a
> mirror
>> update.packages(ask='graphics')
> Error: could not find function "update.packages"
>> utils:::menuInstallLocal()
> Error in install.packages(choose.files("", filters = Filters[c 
> ("zip",  :
>         no packages were specified
>> X<-rnorm(1000)
>> Y<-rnorm(1000)
>> result<-lm(Y ~ X)
>> summary(result)
>
> Call:
> lm(formula = Y ~ X)
>
> Residuals:
>     Min      1Q  Median      3Q     Max
> -3.0047 -0.7205  0.0203  0.7232  3.1052
>
> Coefficients:
>               Estimate Std. Error t value Pr(>|t|)
> (Intercept)  0.0347667  0.0322268   1.079    0.281
> X           -0.0009727  0.0320626  -0.030    0.976
>
> Residual standard error: 1.018 on 998 degrees of freedom
> Multiple R-Squared: 9.223e-07,  Adjusted R-squared: -0.001001
> F-statistic: 0.0009204 on 1 and 998 DF,  p-value: 0.9758
>
>>
>
> $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$


From wwwhsd at gmail.com  Mon Mar 26 14:35:42 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Mon, 26 Mar 2007 09:35:42 -0300
Subject: [R] a very small query
In-Reply-To: <OF7F71205A.5009D56F-ON652572AA.0044B22C-652572AA.0044D38F@ccilindia.co.in>
References: <OF7F71205A.5009D56F-ON652572AA.0044B22C-652572AA.0044D38F@ccilindia.co.in>
Message-ID: <da79af330703260535s69adef3di37440b1485c44ab3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070326/eaeb0786/attachment.pl 

From amsa36060 at yahoo.com  Mon Mar 26 14:37:37 2007
From: amsa36060 at yahoo.com (Amir Safari)
Date: Mon, 26 Mar 2007 05:37:37 -0700 (PDT)
Subject: [R] scalinglawPlot in fBasics
Message-ID: <719097.49149.qm@web60412.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070326/e2c29b5d/attachment.pl 

From ramasamy at cancer.org.uk  Mon Mar 26 14:42:35 2007
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 26 Mar 2007 13:42:35 +0100
Subject: [R] a very small query
In-Reply-To: <OF7F71205A.5009D56F-ON652572AA.0044B22C-652572AA.0044D38F@ccilindia.co.in>
References: <OF7F71205A.5009D56F-ON652572AA.0044B22C-652572AA.0044D38F@ccilindia.co.in>
Message-ID: <4607BFBB.6090100@cancer.org.uk>

sessionInfo()


gyadav at ccilindia.co.in wrote:
> Hi All
> 
> what is the command to give me the listing of the loaded packages. I mean 
> which are active and not the listing of all the installed packages as 
> given by library()
> 
> thanks in advance
> -gaurav
> 
> 
> ============================================================================================
> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and ...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
>


From ripley at stats.ox.ac.uk  Mon Mar 26 14:53:53 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 26 Mar 2007 13:53:53 +0100 (BST)
Subject: [R] a very small query
In-Reply-To: <OF7F71205A.5009D56F-ON652572AA.0044B22C-652572AA.0044D38F@ccilindia.co.in>
References: <OF7F71205A.5009D56F-ON652572AA.0044B22C-652572AA.0044D38F@ccilindia.co.in>
Message-ID: <Pine.LNX.4.64.0703261352130.19536@gannet.stats.ox.ac.uk>

On Mon, 26 Mar 2007, gyadav at ccilindia.co.in wrote:

> what is the command to give me the listing of the loaded packages.

?search
?sessionInfo

(sessionInfo massages the output of search() to show only packages).

> I mean which are active and not the listing of all the installed 
> packages as given by library()

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From gyadav at ccilindia.co.in  Mon Mar 26 15:12:44 2007
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Mon, 26 Mar 2007 18:42:44 +0530
Subject: [R] one more small query
Message-ID: <OFC741CA23.2DB22A2E-ON652572AA.0048288C-652572AA.004884A7@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070326/71a3ec8d/attachment.pl 

From Corinna.Schmitt at igb.fraunhofer.de  Mon Mar 26 15:26:03 2007
From: Corinna.Schmitt at igb.fraunhofer.de (Schmitt, Corinna)
Date: Mon, 26 Mar 2007 15:26:03 +0200
Subject: [R] Listing function
Message-ID: <8B7B0FD99E8AF541A21609104D19615882E8B0@izs-xchg01.izs.fraunhofer.de>

Hallo,

I build a list by the following way:

Lst = list(name="Fred", wife="Mary", no.children=3, cild.ages=c(4,7,9))

I know how I can extract the information one by one. But now I want to
add a new entry which looks like

name="Barney", wife="Liz", no.children=2, cild.ages=c(3,5)

How can I add this information to Lst without overwriting the first
entry?
How can I then extract the corresponding information if I have both
entries in Lst?

Thanks for helping,

Corinna


From i.m.s.white at ed.ac.uk  Mon Mar 26 14:53:46 2007
From: i.m.s.white at ed.ac.uk (ian white)
Date: Mon, 26 Mar 2007 13:53:46 +0100
Subject: [R] setting LD_LIBRARY_PATH
Message-ID: <1174913626.2814.10.camel@trotter.cap.ed.ac.uk>

Dear R-help,

I am trying to use a package (not from CRAN) which includes two shared
library (.so) files. The maintainer suggests inserting two lines
in /usr/bin/R,

LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:path-to-package/lib
export LD_LIBRARY_PATH

This works, but I felt more comfortable putting the first line
in .Renviron or $R_HOME/etc/Renviron.site. However this doesn't seem to
work, and I cannot see why it doesn't.

School of Biological Sciences, University of Edinburgh


From ted.harding at nessie.mcc.ac.uk  Mon Mar 26 12:31:15 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 26 Mar 2007 10:31:15 -0000 (BST)
Subject: [R] sampling from the uniform distribution over a convex hu
In-Reply-To: <17927.10869.388333.48289@maths.uwa.edu.au>
Message-ID: <XFMail.070326102423.ted.harding@nessie.mcc.ac.uk>

I just wrote:

> Thanks, Adrian! I should have remembered about 'spatstat' after
> the Baddeley et al. paper to the RSS in June 2005, where the
> package was extensively used! (Though neither convexhull() nor
> runifpoint() are in the package I downloaded at the time; but of
> course things have moved on!).

Apologies -- that last statement is false!
Ted.


From m.mader at gsf.de  Mon Mar 26 15:32:06 2007
From: m.mader at gsf.de (Michael T. Mader)
Date: Mon, 26 Mar 2007 15:32:06 +0200
Subject: [R] Listing function
In-Reply-To: <8B7B0FD99E8AF541A21609104D19615882E8B0@izs-xchg01.izs.fraunhofer.de>
References: <8B7B0FD99E8AF541A21609104D19615882E8B0@izs-xchg01.izs.fraunhofer.de>
Message-ID: <4607CB56.501@gsf.de>

Lst <- list()
Lst[[1]] <- list(name="Fred", wife="Mary", no.children=3, 
cild.ages=c(4,7,9))
Lst[[2]] <- list(name="Barney", wife="Liz", no.children=2, cild.ages=c(3,5))

I.e. a list of lists

Regards

Michael

Schmitt, Corinna wrote:
> Hallo,
> 
> I build a list by the following way:
> 
> Lst = list(name="Fred", wife="Mary", no.children=3, cild.ages=c(4,7,9))
> 
> I know how I can extract the information one by one. But now I want to
> add a new entry which looks like
> 
> name="Barney", wife="Liz", no.children=2, cild.ages=c(3,5)
> 
> How can I add this information to Lst without overwriting the first
> entry?
> How can I then extract the corresponding information if I have both
> entries in Lst?
> 
> Thanks for helping,
> 
> Corinna
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael T. Mader
Institute of Stem Cell Research
GSF - National Research Center for Environment and Health
Ingolstaedter Landstrasse 1
D-85764 Neuherberg
0049-89-3187-3683

Program testing can be quite effective for showing the presence of bugs, 
but is hopelessly inadequate for showing their absence.
	E. W. Dijkstra


From ripley at stats.ox.ac.uk  Mon Mar 26 15:36:59 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 26 Mar 2007 14:36:59 +0100 (BST)
Subject: [R] Problem in loading all packages all at once
In-Reply-To: <4545F65F-E2F1-4B45-8CA5-99DDC6087452@hanover.edu>
References: <OF8B812C80.ECA62FAB-ON652572AA.00352924-652572AA.0037019F@ccilindia.co.in>
	<4545F65F-E2F1-4B45-8CA5-99DDC6087452@hanover.edu>
Message-ID: <Pine.LNX.4.64.0703261411170.20025@gannet.stats.ox.ac.uk>

I really don't know what is going on here, beyond that 'utils' has not 
been loaded.

But

1) You are not supposed to edit system files like library/base/R/Rprofile.
If you do, don't ask for help!

2) I don't see how the output shown came from the input shown: almost 
surely it did not.

3) It is extremely bad practice to load packages you do not need.
This both slows R down unnecessarily and creates conflicts where packages 
override default behaviour.

4) I believe there is still a limit to the number of DLLs you can have 
loaded in a single R session, and I believe it to be 100.  So you probably 
cannot load all 751 packages, and at some point package loading will fail.
Now, loading library/base/R/Rprofile has no error-checking (as it is 
supposed to be unchanged, src/main/main.c:818) and so you may get no 
indication of this.

So my guess is that there was an attempt to load a few hundred packages 
and only some of them got loaded, not as far down the list as 'utils'.

If you want to load some packages in every session, do so from your 
.Rprofile file: that is run as user code and you should get full 
error-checking (and messages).

As for Charilaos' points:

a) .First.sys is defined in the system profile, library/base/R/Rprofile.
It is run from the base environment.

b) The environment variable R_DEFAULT_PACKAGES justs sets the default for 
the option 'defaultPackages', and the format is described in the surely 
obvious place, the documentation for the option.  There are examples in 
the 'check' script.


On Mon, 26 Mar 2007, Charilaos Skiadas wrote:

> A couple of things:
>
> 1) It would have been a lot easier to help you if you had created a
> much smaller example. perhaps trying to load 2-3 packages instead of
> 1000. Now we have to wade through a lot of stuff to get to the point.
>
> 2) Perhaps your call to Sys.putenv doesn't do quite what you expect
> it to do. Example:
> > TEMP <- c("a","b")
> > Sys.getenv("TEST")
> TEST
>   ""
> > Sys.putenv(TEST=TEMP)
> > Sys.getenv("TEST")
> TEST
> "b"
>
> 3) I couldn't really see where in your code all these packages are
> loaded. Reading the help for .First.sys tells me that that's the call
> that actually loads the default packages, unless I've misunderstood
> something. If that's the case, then the following line you have at
> the end would prevent this loading:
>
> .First.sys <- function()
>
>
> I must say that from the ?Startup help there are two things that are
> not very clear:
> a) At what point in the load process is R_DEFAULT_PACKAGES
> checked? .First.sys doesn't seem to know about it.
> Ok, a bit of search provided an answer to this: Reading the help for
> "options", I see there, in the section about defaultPackages, what is
> probably the most helpful bit (for me) in all this:
>
> defaultPackages:
> 	the packages that are attached by default when R starts up.
> Initially set from value of the environment variable
> R_DEFAULT_PACKAGES, or if that is unset to c("datasets", "utils",
> "grDevices", "graphics", "stats", "methods"). (Set R_DEFAULT_PACKAGES
> to NULL or a comma-separated list of package names.) A call to
> options should be in your ?.Rprofile? file to ensure that the change
> takes effect before the base package is initialized (see Startup).
>
>
> b) What is the format that the R_DEFAULT_PACKAGES variable must have
> (I guess now I know, since it is mentioned in ?options, though that's
> not really the place I would have expected it to be explained) ? The
> only examples I could find on the web were setting it to NULL.
>
>
>
> Hope all this helps in some way.
>
> Haris Skiadas
> Department of Mathematics and Computer Science
> Hanover College
>
>
> On Mar 26, 2007, at 6:01 AM, gyadav at ccilindia.co.in wrote:
>
>>
>> Hi All
>>
>> Please see the Rprofile file which i have modified as follows and
>> after
>> that when I start R then I see that R says to me "TRUE" for all the
>> packages implying that all loaded at once.
>> But when i try to use commands as simple as help("lm"), it doesnt
>> work nor
>> any of the menu "Packages" is not working.
>> Although the regression using lm ( Y ~ X ) is working even summary and
>> rnorm is working fine.
>> Please tell me why menu and help command is not working
>>
>> Rprofile is in between +++++++++++++++++++++++++++ lines
>> R initialization is in between @@@@@@@@@@@@@@ lines
>> Commands are in between $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ lines
>>
>> Please also suggest me should i send this to r-devel list or not
>>
>> ### This is the system Rprofile file. It is always run on startup.
>> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
>> ++++
>> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
>> ++++
>> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
>> ++++
>> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
>> ++++
>> options(scipen = 0)
>> options(max.print = 10000)# not yet exercised in 'core R'
>> options(add.smooth = TRUE)# currently only used in 'plot.lm'
>>
>> ######################################################################
>> ########################
>> ######################################################################
>> ########################
>> #############code added by gaurav yadav to install many other
>> packages at
>> startup#############
>> print ("START OF CODE ADDED BY GAURAV YADAV")
>> TEMP <- Sys.getenv("R_DEFAULT_PACKAGES")
>> print ("R_DEFAULT_PACKAGES")
>> print (TEMP)
>> print ("=======")
>> TEMP <-
>> c
>> (TEMP,"aplpack","approximator","apTreeshape","ArDec","arules","ash","a
>> space","assist","aster","asypow","aws","base","bayesm","bayesmix","bay
>> esSurv","BayesTree","BayesValidate","betareg","Bhat","BHH2","bicreduc"
>> ,"biglm","bim","bindata","Biodem","biopara","bitops","bivpois","blight
>> y","blockrand","BMA","boa","Bolstad","boolean","boost","boot","bootstr
>> ap","bqtl","BradleyTerry","brlr","BRugs","BSDA","BsMD","butler","calib
>> rate","calibrator","caMassClass","car","cat","caTools","catspec","cba"
>> ,"cclust","CDNmoney","cfa","CGIwithR","changeLOS","chplot","chron","Ci
>> rcStats","circular","clac","class","classInt","classPP","clim.pact","c
>> limatol","clines","clue","cluster","clusterRepro","clustvarsel","cmprs
>> k","cobs","CoCo","CoCoCg","CoCoCore","CoCoGraph","CoCoObjects","CoCoOl
>> dData","CoCoRaw","cocorresp","coda","coin","colorspace","combinat","co
>> mpositions","concor","concord","cond","conf.design","connectedness","c
>> opula","corpcor","corpora","covRobust","coxrobust","cramer","crossdes"
>> ,"crq","c
>>  sampling","cslogistic","CTFS","ctv")
>>
> [snip]
>> #TEMP <- c(TEMP,"runfirst","base","datasets", "utils", "grDevices",
>> "graphics","stats","methods","lmtest")
>> print ("MODIFIED R_DEFAULT_PACKAGES+++++++++++")
>> print (TEMP)
>> print ("++++++++++++++++")
>> print(Sys.putenv(R_DEFAULT_PACKAGES=TEMP))
>> print ("END OF CODE ADDED BY GAURAV YADAV")
>>
>> ######################################################################
>> ########################
>> ######################################################################
>> ########################
>>
>>
>> local({dp <- as.vector(Sys.getenv("R_DEFAULT_PACKAGES"))
>>        if(identical(dp, "")) # marginally faster to do methods last
>>            dp <- c("datasets", "utils", "grDevices", "graphics",
>>                    "stats", "methods")
>>        else if(identical(dp, "NULL")) dp <- character(0)
>>        else dp <- strsplit(dp, ",")[[1]]
>>        dp <- sub("[[:blank:]]*([[:alnum:]]+)", "\\1", dp) # strip
>> whitespace
>>        options(defaultPackages = dp)
>>     })
>>
>> .First.sys <- function()
>>
>> ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
>> ++++
>>
>> THE R SHOWS ME THIS GIVEN HEREIN BELOW WHEN I START R
>> @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
>> [1] "START OF CODE ADDED BY GAURAV YADAV"
>> [1] "R_DEFAULT_PACKAGES+++++++++++"
>> R_DEFAULT_PACKAGES
>>                 ""
>> [1] "++++++++++++++++"
>> [1] "MODIFIED R_DEFAULT_PACKAGES+++++++++++"
>>   R_DEFAULT_PACKAGES
>>                   ""            "aplpack"       "approximator"
>>
>>
> [snip tons of packages]
>> [1] "++++++++++++++++"
>>   [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
>> TRUE TRUE
>> [751] TRUE TRUE TRUE TRUE TRUE TRUE TRUE
>> [1] "END OF CODE ADDED BY GAURAV YADAV"
>>
>> R : Copyright 2006, The R Foundation for Statistical Computing
>> Version 2.3.0 (2006-04-24)
>> ISBN 3-900051-07-0
>>
>> R is free software and comes with ABSOLUTELY NO WARRANTY.
>> You are welcome to redistribute it under certain conditions.
>> Type 'license()' or 'licence()' for distribution details.
>>
>>   Natural language support but running in an English locale
>>
>> R is a collaborative project with many contributors.
>> Type 'contributors()' for more information and
>> 'citation()' on how to cite R or R packages in publications.
>>
>> Type 'demo()' for some demos, 'help()' for on-line help, or
>> 'help.start()' for an HTML browser interface to help.
>> Type 'q()' to quit R.
>>
>> [Previously saved workspace restored]
>>
>>>
>>
>> @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
>>
>> $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
>>
>> Type 'demo()' for some demos, 'help()' for on-line help, or
>> 'help.start()' for an HTML browser interface to help.
>> Type 'q()' to quit R.
>>
>> [Previously saved workspace restored]
>>
>>> help("lm")
>> Error: could not find function "help"
>>> help.search("regression")
>> Error: could not find function "help.search"
>>> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
>> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
>> Error in eval(expr, envir, enclos) : could not find function
>> "select.list"
>>> chooseCRANmirror()
>> Error: could not find function "chooseCRANmirror"
>>> setRepositories()
>> Error: could not find function "setRepositories"
>>> utils:::menuInstallPkgs()
>> --- Please select a CRAN mirror for use in this session ---
>> Error in open.connection(file, "r") : unable to open connection
>> In addition: Warning message:
>> unable to connect to 'cran.r-project.org' on port 80.
>> Error in contrib.url(repos, type) : trying to use CRAN without
>> setting a
>> mirror
>>> update.packages(ask='graphics')
>> Error: could not find function "update.packages"
>>> utils:::menuInstallLocal()
>> Error in install.packages(choose.files("", filters = Filters[c
>> ("zip",  :
>>         no packages were specified
>>> X<-rnorm(1000)
>>> Y<-rnorm(1000)
>>> result<-lm(Y ~ X)
>>> summary(result)
>>
>> Call:
>> lm(formula = Y ~ X)
>>
>> Residuals:
>>     Min      1Q  Median      3Q     Max
>> -3.0047 -0.7205  0.0203  0.7232  3.1052
>>
>> Coefficients:
>>               Estimate Std. Error t value Pr(>|t|)
>> (Intercept)  0.0347667  0.0322268   1.079    0.281
>> X           -0.0009727  0.0320626  -0.030    0.976
>>
>> Residual standard error: 1.018 on 998 degrees of freedom
>> Multiple R-Squared: 9.223e-07,  Adjusted R-squared: -0.001001
>> F-statistic: 0.0009204 on 1 and 998 DF,  p-value: 0.9758
>>
>>>
>>
>> $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From maitra at iastate.edu  Mon Mar 26 15:37:58 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Mon, 26 Mar 2007 08:37:58 -0500
Subject: [R] sampling from the uniform distribution over a convex hu
In-Reply-To: <XFMail.070326102423.ted.harding@nessie.mcc.ac.uk>
References: <17927.10869.388333.48289@maths.uwa.edu.au>
	<XFMail.070326102423.ted.harding@nessie.mcc.ac.uk>
Message-ID: <20070326083758.5dbcf190@triveni.stat.iastate.edu>

Thanks, all, and thanks especially to Ted for your investigations in the other thread with the same title! Does spatstat handle higher dimensions than 2? 

Best wishes,
Ranjan

On Mon, 26 Mar 2007 10:31:15 -0000 (BST) (Ted Harding) <ted.harding at nessie.mcc.ac.uk> wrote:

> I just wrote:
> 
> > Thanks, Adrian! I should have remembered about 'spatstat' after
> > the Baddeley et al. paper to the RSS in June 2005, where the
> > package was extensively used! (Though neither convexhull() nor
> > runifpoint() are in the package I downloaded at the time; but of
> > course things have moved on!).
> 
> Apologies -- that last statement is false!
> Ted.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dimitris.rizopoulos at med.kuleuven.be  Mon Mar 26 15:38:16 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 26 Mar 2007 15:38:16 +0200
Subject: [R] Listing function
References: <8B7B0FD99E8AF541A21609104D19615882E8B0@izs-xchg01.izs.fraunhofer.de>
Message-ID: <005301c76fac$0224c250$0540210a@www.domain>

maybe it'd be better to use a data.frame(), e.g.,

dat <- data.frame(name = I("Fred"), wife = I("Mary"), no.children = 3,
cild.ages1 = 4, cild.ages2 = 7, cild.ages3 = 9)
##############
new.info <- c(name = "Barney", wife = "Liz", no.children=2,
cild.ages1 = 3, cild.ages2 = 5, cild.ages3 = NA)

rbind(dat, new.info)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Schmitt, Corinna" <Corinna.Schmitt at igb.fraunhofer.de>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, March 26, 2007 3:26 PM
Subject: [R] Listing function


> Hallo,
>
> I build a list by the following way:
>
> Lst = list(name="Fred", wife="Mary", no.children=3, 
> cild.ages=c(4,7,9))
>
> I know how I can extract the information one by one. But now I want 
> to
> add a new entry which looks like
>
> name="Barney", wife="Liz", no.children=2, cild.ages=c(3,5)
>
> How can I add this information to Lst without overwriting the first
> entry?
> How can I then extract the corresponding information if I have both
> entries in Lst?
>
> Thanks for helping,
>
> Corinna
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ggrothendieck at gmail.com  Mon Mar 26 15:40:24 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 26 Mar 2007 09:40:24 -0400
Subject: [R] a very small query
In-Reply-To: <OF7F71205A.5009D56F-ON652572AA.0044B22C-652572AA.0044D38F@ccilindia.co.in>
References: <OF7F71205A.5009D56F-ON652572AA.0044B22C-652572AA.0044D38F@ccilindia.co.in>
Message-ID: <971536df0703260640r2ddf0ce4jea5ff14ae40605c@mail.gmail.com>

Try these:

search()
sessionInfo()
loadedNamespaces()

The first two show the attached packages (and some other info) and the
last one shows the namespaces that are loaded.  Note that detaching
a package does not unload its namespace and unloading a namespace
does not de-register its methods.  This illustrates the behavior:

> search()
[1] ".GlobalEnv"        "package:stats"     "package:graphics"
[4] "package:grDevices" "package:utils"     "package:datasets"
[7] "package:methods"   "Autoloads"         "package:base"
> loadedNamespaces()
[1] "base"      "graphics"  "grDevices" "methods"   "stats"     "utils"
> as.Date(1) # error as there is no numeric method for as.Date
Error in as.Date.default(1) : do not know how to convert '1' to class "Date"

> library(zoo)
> search()
 [1] ".GlobalEnv"        "package:zoo"       "package:stats"
 [4] "package:graphics"  "package:grDevices" "package:utils"
 [7] "package:datasets"  "package:methods"   "Autoloads"
[10] "package:base"
> loadedNamespaces()
[1] "base"      "graphics"  "grDevices" "grid"      "lattice"   "methods"
[7] "stats"     "utils"     "zoo"
> as.Date(1) # zoo defines a numeric method for as.Date
[1] "1970-01-02"

> detach()
> unloadNamespace("zoo")
<environment: namespace:zoo>
> search()
[1] ".GlobalEnv"        "package:stats"     "package:graphics"
[4] "package:grDevices" "package:utils"     "package:datasets"
[7] "package:methods"   "Autoloads"         "package:base"
> loadedNamespaces()
[1] "base"      "graphics"  "grDevices" "grid"      "lattice"   "methods"
[7] "stats"     "utils"
> # zoo is gone from attached package list and loadedNamespaces
> # but numeric method for as.Date from zoo is still registered
> as.Date(1)
[1] "1970-01-02"



On 3/26/07, gyadav at ccilindia.co.in <gyadav at ccilindia.co.in> wrote:
>
> Hi All
>
> what is the command to give me the listing of the loaded packages. I mean
> which are active and not the listing of all the installed packages as
> given by library()
>
> thanks in advance
> -gaurav
>
>
> ============================================================================================
> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and ...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Corinna.Schmitt at igb.fraunhofer.de  Mon Mar 26 15:52:43 2007
From: Corinna.Schmitt at igb.fraunhofer.de (Schmitt, Corinna)
Date: Mon, 26 Mar 2007 15:52:43 +0200
Subject: [R] Listing function displayed as a table
In-Reply-To: <4607CB56.501@gsf.de>
References: <8B7B0FD99E8AF541A21609104D19615882E8B0@izs-xchg01.izs.fraunhofer.de>
	<4607CB56.501@gsf.de>
Message-ID: <8B7B0FD99E8AF541A21609104D19615882E8BA@izs-xchg01.izs.fraunhofer.de>

Hallo,
good idea it is working. A new question appears: How can I display the entries in a table like

 name       wife          no.children  child.ages
Fred        Mary            3           4,7,9
Barney      Liz             2           3,5

Thanks, Corinna


-----Urspr?ngliche Nachricht-----
Von: Michael T. Mader [mailto:m.mader at gsf.de] 
Gesendet: Montag, 26. M?rz 2007 15:32
An: Schmitt, Corinna; r-help at stat.math.ethz.ch
Betreff: Re: [R] Listing function

Lst <- list()
Lst[[1]] <- list(name="Fred", wife="Mary", no.children=3, 
cild.ages=c(4,7,9))
Lst[[2]] <- list(name="Barney", wife="Liz", no.children=2, cild.ages=c(3,5))

I.e. a list of lists

Regards

Michael

Schmitt, Corinna wrote:
> Hallo,
> 
> I build a list by the following way:
> 
> Lst = list(name="Fred", wife="Mary", no.children=3, cild.ages=c(4,7,9))
> 
> I know how I can extract the information one by one. But now I want to
> add a new entry which looks like
> 
> name="Barney", wife="Liz", no.children=2, cild.ages=c(3,5)
> 
> How can I add this information to Lst without overwriting the first
> entry?
> How can I then extract the corresponding information if I have both
> entries in Lst?
> 
> Thanks for helping,
> 
> Corinna
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael T. Mader
Institute of Stem Cell Research
GSF - National Research Center for Environment and Health
Ingolstaedter Landstrasse 1
D-85764 Neuherberg
0049-89-3187-3683

Program testing can be quite effective for showing the presence of bugs, 
but is hopelessly inadequate for showing their absence.
	E. W. Dijkstra


From Luisr at frs.fo  Mon Mar 26 15:53:18 2007
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Mon, 26 Mar 2007 14:53:18 +0100
Subject: [R] gam parameter predictions
Message-ID: <s607de65.006@ffdata.setur.fo>

R-help,

I'm applying a gam model (package mgcv) to predict
relative abundances of a fish species.

The covariates are year, month, vessel and statistical rectangle.


The model looks like this:

g1 <- gam(log(cpue) ~  s(rekt1) + s(year) + s(mon) + s(reg1), data =
dataTest)


Once the model is fitted to the data I want to get the mean model
estimates by year.

I do the following:

obsPred <- data.frame(year = dataTest$year, pred = predict(g1, type =
"response"))

gamFit <- tapply(obsPred$pred, list(year = obsPred$ar), mean)



Is this correct?



Thanks in advance


> version
               _                           
platform       i386-pc-mingw32             
arch           i386                        
os             mingw32                     
system         i386, mingw32               
status                                     
major          2                           
minor          4.1                         
year           2006                        
month          12                          
day            18                          
svn rev        40228                       
language       R                           
version.string R version 2.4.1 (2006-12-18)


From klaster at karlin.mff.cuni.cz  Mon Mar 26 16:08:47 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Mon, 26 Mar 2007 16:08:47 +0200
Subject: [R] Listing function
In-Reply-To: <8B7B0FD99E8AF541A21609104D19615882E8B0@izs-xchg01.izs.fraunhofer.de>
References: <8B7B0FD99E8AF541A21609104D19615882E8B0@izs-xchg01.izs.fraunhofer.de>
Message-ID: <4607D3EF.8060903@karlin.mff.cuni.cz>

Use c() to extend your list creating a list of lists:

Lst = list(name="Fred", wife="Mary", no.children=3, cild.ages=c(4,7,9))

Lst <- c(list(Lst), list(list(name="Barney", wife="Liz", no.children=2, 
cild.ages=c(3,5))))

You can give names to the components:

Lst <- c(Fred=list(Lst), Barney=list(list(name="Barney", wife="Liz", 
no.children=2, cild.ages=c(3,5))))

See ?c for more information. You can also use a data.frame if this looks 
too messy.
Petr

Schmitt, Corinna napsal(a):
> Hallo,
> 
> I build a list by the following way:
> 
> Lst = list(name="Fred", wife="Mary", no.children=3, cild.ages=c(4,7,9))
> 
> I know how I can extract the information one by one. But now I want to
> add a new entry which looks like
> 
> name="Barney", wife="Liz", no.children=2, cild.ages=c(3,5)
> 
> How can I add this information to Lst without overwriting the first
> entry?
> How can I then extract the corresponding information if I have both
> entries in Lst?
> 
> Thanks for helping,
> 
> Corinna
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From Corinna.Schmitt at igb.fraunhofer.de  Mon Mar 26 16:16:08 2007
From: Corinna.Schmitt at igb.fraunhofer.de (Schmitt, Corinna)
Date: Mon, 26 Mar 2007 16:16:08 +0200
Subject: [R] Listing function - new possibility
In-Reply-To: <005301c76fac$0224c250$0540210a@www.domain>
References: <8B7B0FD99E8AF541A21609104D19615882E8B0@izs-xchg01.izs.fraunhofer.de>
	<005301c76fac$0224c250$0540210a@www.domain>
Message-ID: <8B7B0FD99E8AF541A21609104D19615882E8C4@izs-xchg01.izs.fraunhofer.de>



I think your solution is more comfortable. But what do you do if an new entry consists of 4 children? How can you modify the old result of rbind()? Is ther a possibility to save the childrens ages in one table entry?

Thanks, Corinna
 

-----Urspr?ngliche Nachricht-----
Von: Dimitris Rizopoulos [mailto:dimitris.rizopoulos at med.kuleuven.be] 
Gesendet: Montag, 26. M?rz 2007 15:38
An: Schmitt, Corinna
Cc: r-help at stat.math.ethz.ch
Betreff: Re: [R] Listing function

maybe it'd be better to use a data.frame(), e.g.,

dat <- data.frame(name = I("Fred"), wife = I("Mary"), no.children = 3,
cild.ages1 = 4, cild.ages2 = 7, cild.ages3 = 9)
##############
new.info <- c(name = "Barney", wife = "Liz", no.children=2,
cild.ages1 = 3, cild.ages2 = 5, cild.ages3 = NA)

rbind(dat, new.info)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Schmitt, Corinna" <Corinna.Schmitt at igb.fraunhofer.de>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, March 26, 2007 3:26 PM
Subject: [R] Listing function


> Hallo,
>
> I build a list by the following way:
>
> Lst = list(name="Fred", wife="Mary", no.children=3, 
> cild.ages=c(4,7,9))
>
> I know how I can extract the information one by one. But now I want 
> to
> add a new entry which looks like
>
> name="Barney", wife="Liz", no.children=2, cild.ages=c(3,5)
>
> How can I add this information to Lst without overwriting the first
> entry?
> How can I then extract the corresponding information if I have both
> entries in Lst?
>
> Thanks for helping,
>
> Corinna
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From gyadav at ccilindia.co.in  Mon Mar 26 16:20:01 2007
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Mon, 26 Mar 2007 19:50:01 +0530
Subject: [R] Problem in loading all packages all at once
In-Reply-To: <4545F65F-E2F1-4B45-8CA5-99DDC6087452@hanover.edu>
Message-ID: <OF1B499CE1.7EB1BF9A-ON652572AA.004D96E1-652572AA.004EAD78@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070326/6a184cc1/attachment.pl 

From marc_schwartz at comcast.net  Mon Mar 26 16:22:20 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Mon, 26 Mar 2007 09:22:20 -0500
Subject: [R] setting LD_LIBRARY_PATH
In-Reply-To: <1174913626.2814.10.camel@trotter.cap.ed.ac.uk>
References: <1174913626.2814.10.camel@trotter.cap.ed.ac.uk>
Message-ID: <1174918940.4927.3.camel@Bellerophon>

On Mon, 2007-03-26 at 13:53 +0100, ian white wrote:
> Dear R-help,
> 
> I am trying to use a package (not from CRAN) which includes two shared
> library (.so) files. The maintainer suggests inserting two lines
> in /usr/bin/R,
> 
> LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:path-to-package/lib
> export LD_LIBRARY_PATH
> 
> This works, but I felt more comfortable putting the first line
> in .Renviron or $R_HOME/etc/Renviron.site. However this doesn't seem to
> work, and I cannot see why it doesn't.

Your best bet is to add the paths to the lib files in /etc/ld.so.conf
and then run /sbin/ldconfig to update the environment. Both of these
would be done as root (ie. su or sudo).

That way, you don't have to worry about editing application specific
files every time they get overwritten during an update.

HTH,

Marc Schwartz


From gyadav at ccilindia.co.in  Mon Mar 26 16:32:11 2007
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Mon, 26 Mar 2007 20:02:11 +0530
Subject: [R] Problem in loading all packages all at once
In-Reply-To: <Pine.LNX.4.64.0703261411170.20025@gannet.stats.ox.ac.uk>
Message-ID: <OF6C2C67EB.E770E3D5-ON652572AA.004ED4C7-652572AA.004FCA68@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070326/3d41557a/attachment.pl 

From ramasamy at cancer.org.uk  Mon Mar 26 16:44:57 2007
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 26 Mar 2007 15:44:57 +0100
Subject: [R] Listing function displayed as a table
In-Reply-To: <8B7B0FD99E8AF541A21609104D19615882E8BA@izs-xchg01.izs.fraunhofer.de>
References: <8B7B0FD99E8AF541A21609104D19615882E8B0@izs-xchg01.izs.fraunhofer.de>	<4607CB56.501@gsf.de>
	<8B7B0FD99E8AF541A21609104D19615882E8BA@izs-xchg01.izs.fraunhofer.de>
Message-ID: <4607DC69.6020807@cancer.org.uk>

Something ugly like this?

Lst <- list()
Lst[[1]] <- list(name="Fred", wife="Mary", no.children=3, 
child.ages=c(4,7,9))
Lst[[2]] <- list(name="Barney", wife="Liz", no.children=2, 
child.ages=c(3,5))

cbind( do.call("rbind", as.list(Lst))[ ,-4],
        child.ages=sapply( Lst, function(myli)
                                  paste(myli$child.ages, collapse=",") ))


Why don't you just save the data in a dataframe instead of a list to 
begin with ? The only variable I can see that has multiple values is 
child.ages. Or create one row per record as in most databases. The 
choice depends on your input.

  df <- rbind( c("Fred", "Mary", 4), c("Fred", "Mary", 7),
               c("Fred", "Mary", 9), c("Barney", "Liz", 3),
               c("Barney", "Liz", 5) )
  df <- data.frame(df)
  colnames(df) <- c("Father", "Mother", "Child.Age")
  df$Child.Age <- as.numeric(as.character(df$Child.Age))

  parents <- paste( df$Father, df$Mother, sep="+" )

  getstats <- function(x) c( values=paste(x, collapse=","),
                  mean=round(mean(x),2), youngest=min(x), oldest=max(x) )

  do.call( rbind, tapply( df$Child.Age, parents, getstats ) )

              values  mean   youngest oldest
  Barney+Liz "3,5"   "4"    "3"      "5"
  Fred+Mary  "4,7,9" "6.67" "4"      "9"


Regards, Adai



Schmitt, Corinna wrote:
> Hallo,
> good idea it is working. A new question appears: How can I display the entries in a table like
> 
>  name       wife          no.children  child.ages
> Fred        Mary            3           4,7,9
> Barney      Liz             2           3,5
> 
> Thanks, Corinna
> 
> 
> -----Urspr?ngliche Nachricht-----
> Von: Michael T. Mader [mailto:m.mader at gsf.de] 
> Gesendet: Montag, 26. M?rz 2007 15:32
> An: Schmitt, Corinna; r-help at stat.math.ethz.ch
> Betreff: Re: [R] Listing function
> 
> Lst <- list()
> Lst[[1]] <- list(name="Fred", wife="Mary", no.children=3, 
> cild.ages=c(4,7,9))
> Lst[[2]] <- list(name="Barney", wife="Liz", no.children=2, cild.ages=c(3,5))
> 
> I.e. a list of lists
> 
> Regards
> 
> Michael
> 
> Schmitt, Corinna wrote:
>> Hallo,
>>
>> I build a list by the following way:
>>
>> Lst = list(name="Fred", wife="Mary", no.children=3, cild.ages=c(4,7,9))
>>
>> I know how I can extract the information one by one. But now I want to
>> add a new entry which looks like
>>
>> name="Barney", wife="Liz", no.children=2, cild.ages=c(3,5)
>>
>> How can I add this information to Lst without overwriting the first
>> entry?
>> How can I then extract the corresponding information if I have both
>> entries in Lst?
>>
>> Thanks for helping,
>>
>> Corinna
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From ripley at stats.ox.ac.uk  Mon Mar 26 16:49:07 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 26 Mar 2007 15:49:07 +0100 (BST)
Subject: [R] setting LD_LIBRARY_PATH
In-Reply-To: <1174913626.2814.10.camel@trotter.cap.ed.ac.uk>
References: <1174913626.2814.10.camel@trotter.cap.ed.ac.uk>
Message-ID: <Pine.LNX.4.64.0703261535250.22789@gannet.stats.ox.ac.uk>

On Mon, 26 Mar 2007, ian white wrote:

> Dear R-help,
>
> I am trying to use a package (not from CRAN) which includes two shared
> library (.so) files. The maintainer suggests inserting two lines
> in /usr/bin/R,
>
> LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:path-to-package/lib
> export LD_LIBRARY_PATH

(Not a very good idea, not least because there is another copy at 
R_HOME/bin/R.  Better to modify R_HOME/etc/ldpaths, which deals with 
setting LD_LIBRARY_PATH.)

> This works, but I felt more comfortable putting the first line
> in .Renviron or $R_HOME/etc/Renviron.site. However this doesn't seem to
> work, and I cannot see why it doesn't.

Those files are read by the process.  You don't state your OS, but many 
OSes make sure that the paths for dlopen are cached before the process has 
a chance to change LD_LIBRARY_PATH, for security-related reasons.  Most
OSes are silent about this, but Solaris says

      If other objects were link-edited with pathname  when  path-
      name  was  built,  that is, the pathname has dependencies on
      other objects, those objects will automatically be loaded by
      dlopen(). The directory search path used to find both  path-
      name and the other needed objects may be affected by setting
      the  environment variable LD_LIBRARY_PATH, which is analyzed
      once at process startup, and from a runpath  setting  within
      the object from which the call to dlopen() originated.

As dlopen came from SunOS, it seems the behaviour has been copied by most 
implementations of dlopen.  (AFAICS it is not touched on by POSIX.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From sergio.della.franca at gmail.com  Mon Mar 26 17:02:33 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Mon, 26 Mar 2007 17:02:33 +0200
Subject: [R] subset
Message-ID: <b490ce570703260802q490298bembe34e702c241ab05@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070326/586ce8b7/attachment.pl 

From marc_schwartz at comcast.net  Mon Mar 26 17:10:22 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Mon, 26 Mar 2007 10:10:22 -0500
Subject: [R] subset
In-Reply-To: <b490ce570703260802q490298bembe34e702c241ab05@mail.gmail.com>
References: <b490ce570703260802q490298bembe34e702c241ab05@mail.gmail.com>
Message-ID: <1174921822.4927.6.camel@Bellerophon>

On Mon, 2007-03-26 at 17:02 +0200, Sergio Della Franca wrote:
> Dear R-Helpers,
> 
> I want to make a subset from my data set.
> 
> I'd like to perform different condition for subset.
> 
> I.e.:
> 
> I like to create a subset when:
> - var1=0
> - var2=0
> - var3 is different from 2.
> 
> How can i develop a subset under this condition?
> 
> Thank you in advance.
> 
> Sergio Della Franca.

See ?subset

Something along the lines of the following should work:

  NewDF <- subset(DF, (var1 == 0) & (var2 == 0) & (var 3 != 0))

HTH,

Marc Schwartz


From Corinna.Schmitt at igb.fraunhofer.de  Mon Mar 26 17:24:57 2007
From: Corinna.Schmitt at igb.fraunhofer.de (Schmitt, Corinna)
Date: Mon, 26 Mar 2007 17:24:57 +0200
Subject: [R] data-frame adding/deleting column
Message-ID: <8B7B0FD99E8AF541A21609104D19615882E8E4@izs-xchg01.izs.fraunhofer.de>

Hallo,

I have got an existing data frame and want to add a new column. The
existing data frame was created like this:

> df <- rbind( c("Fred", "Mary", 4), c("Fred", "Mary", 7),
+                c("Fred", "Mary", 9), c("Barney", "Liz", 3),
+                c("Barney", "Liz", 5) )
> df <- data.frame(df)
> colnames(df) <- c("Father", "Mother", "Child.Age")
> df
  Father Mother Child.Age
1   Fred   Mary         4
2   Fred   Mary         7
3   Fred   Mary         9
4 Barney    Liz         3
5 Barney    Liz         5

I want to add a column named weddingdate. How does this work and how can
I save the corresponding dates in the cells? (Fred-Mary: 12.12.1980,
Barney-Liz: 3.3.2003)
How can I delet a whole column or row if it is needed?

Thanks, Corinna


From marc_schwartz at comcast.net  Mon Mar 26 17:42:42 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Mon, 26 Mar 2007 10:42:42 -0500
Subject: [R] subset
In-Reply-To: <b490ce570703260830x26b8f930qcc5cc89332d9eaf4@mail.gmail.com>
References: <b490ce570703260802q490298bembe34e702c241ab05@mail.gmail.com>
	<1174921822.4927.6.camel@Bellerophon>
	<b490ce570703260830x26b8f930qcc5cc89332d9eaf4@mail.gmail.com>
Message-ID: <1174923762.4927.21.camel@Bellerophon>

Sergio,

Please be sure to cc: the list (ie. Reply to All) with follow up
questions.

In this case, you would use %in% with a negation:

NewDF <- subset(DF, (var1 == 0) & (var2 == 0) & (!var3 %in% 2:3)) 

See ?"%in%" for more information.

HTH,

Marc

On Mon, 2007-03-26 at 17:30 +0200, Sergio Della Franca wrote:
> Ok, this run correctly.
>  
> Another question for you:
>  
> I want to put more than one condition for var3, i.e.:
> I like to create a subset when:
>  - var1=0
>  - var2=0
>  - var3 is different from 2 and from 3.
>  
> Like you suggested, i perform this code:
> NewDF <- subset(DF, (var1 == 0) & (var2 == 0) & (var 3 != 2)) & (var
> 3 != 3))
>  
> There is a method to combine (var 3 != 2)) & (var 3 != 3)) in one
> condition?
>  
> Thank you.
>  
> Sergio
>  
> 
>  
> 2007/3/26, Marc Schwartz <marc_schwartz at comcast.net>: 
>         On Mon, 2007-03-26 at 17:02 +0200, Sergio Della Franca wrote:
>         > Dear R-Helpers,
>         >
>         > I want to make a subset from my data set. 
>         >
>         > I'd like to perform different condition for subset.
>         >
>         > I.e.:
>         >
>         > I like to create a subset when:
>         > - var1=0
>         > - var2=0
>         > - var3 is different from 2.
>         >
>         > How can i develop a subset under this condition?
>         >
>         > Thank you in advance.
>         >
>         > Sergio Della Franca.
>         
>         See ?subset
>         
>         Something along the lines of the following should work:
>         
>         NewDF <- subset(DF, (var1 == 0) & (var2 == 0) & (var 3 != 0)) 
>         
>         HTH,
>         
>         Marc Schwartz


From sergio.della.franca at gmail.com  Mon Mar 26 18:31:03 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Mon, 26 Mar 2007 18:31:03 +0200
Subject: [R] substitute variable
Message-ID: <b490ce570703260931n6243ef44je95bda9816aa7135@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070326/3e379677/attachment.pl 

From adick at uchicago.edu  Mon Mar 26 18:32:52 2007
From: adick at uchicago.edu (Anthony Steven Dick)
Date: Mon, 26 Mar 2007 11:32:52 -0500
Subject: [R] for loop help
In-Reply-To: <4606BC3D.8050507@cancer.org.uk>
References: <4606A5DD.2010806@uchicago.edu> <4606BC3D.8050507@cancer.org.uk>
Message-ID: <4607F5B4.60009@uchicago.edu>

Thanks Adai. You can tell by the clumsiness of my script that I am new 
to R and to programming. After wrestling with it this morning, I found 
that the following worked best:

pre.outliers<-subset(final.roi.df, gFDR4FWHM >= 1.96)
               detach(final.roi.df)
               attach(pre.outliers)
               outliers <- subset(pre.outliers, gtvalue4FWHM >= 0.00)
               if(length(outliers$gtvalue4FWHM)==0) {
               print("NA")
               next
               }
....


Thanks for the other suggestions. I have incorporated them.

Anthony


Adaikalavan Ramasamy wrote:
> Try changing
>
>    outliers <- subset(pre.outliers, gtvalue4FWHM >= 0.00)
>
> to
>
>    w <- which( gtvalue4FWHM >= 0.00 )
>    outliers( length(w) > 0, pre.outliers[ w, ],  NA )
>
>
> Other comments:
>
> 1. Make sure you detach(data_gcs) at the end of the loops
>
>
> 2. scale() works on columns by default, so try
>     current.roi.plus.z <- data.frame(current.roi,
>                                   scale(current.roi[, c(15,18,21,24)]) )
>
>
> 3. For simple statements, you can use ifelse() syntax. Also see all(). 
> Using these two functions, you can try
>
>  testextrem <- function(x) ifelse( all(abs(x[ 1:4 ]) < c(2.5)), 1, 0)
>
>
> 4. Try to minimise the use of attach() in loops.
>
> 5. You might be interested in learning more about apply(), tapply(), 
> split() etc.
>
>
> If this does not help, then please resend a simplified version of the 
> codes, preferably with a simple toy example to illustrate.
>
> Regards, Adai
>
>
>
> Anthony Steven Dick wrote:
>> Hello-
>>
>> I have a script which steps through a series of subjects, and for the 
>> subjects I remove outlying values. After removing these outliers, I 
>> specify a cutoff, keeping only values over a certain value (e.g., 
>> 1.96). I want to populate a matrix with a statistic of the values 
>> that make the cutoff (for example, the mean). However, in some 
>> subjects, after outliers and the cutoff are specified, there are no 
>> data that meet the criteria (I get <0 rows> (or 0-length row.names)). 
>> Here the script dies.
>>
>> The solution I think is to specify a break so that the matrix will be 
>> populated with a value (such as NA) and it will move on to the next 
>> subject in the loop. However, I haven't been able to figure this out. 
>> If anyone has any suggestions I would very much appreciate them. I 
>> have paster part of the script below up to the point where it dies.
>>
>> Thanks.
>>
>> for (ss in levels(ss.list)) {
>>         print(ss)
>>         ss.count = ss.count + 1          query.string <- 
>> paste("SELECT * FROM ",ss,";",sep="")
>>         #print(query.string)
>>         data_gcs <- dbGetQuery(con, query.string)
>>         attach(data_gcs)
>>         names(data_gcs)
>>         mat_row = 0
>>             for(i in levels(roi.list)){
>>             print (i)
>>             current.roi <- data_gcs[data_gcs$ROI==i,]
>>             current.roi.plus.z <- data.frame(current.roi, 
>> scale(current.roi[,15]), scale(current.roi[,18]), 
>> scale(current.roi[,21]), scale(current.roi[,24]))
>>             testextrem <- function(x) {if ((abs(x[1]) < 2.5) & 
>> (abs(x[2]) < 2.5) & (abs(x[3]) < 2.5) & (abs(x[4]) < 2.5)) return(1) 
>> else return(0)}
>>             filtervector <- apply(as.vector(current.roi.plus.z[,c(27, 
>> 28, 29, 30)]), 1, FUN=testextrem)
>>             current.roi.plus.z.filter <- 
>> data.frame(current.roi.plus.z, filtervector)
>>             final.roi.df <- 
>> current.roi.plus.z.filter[which(current.roi.plus.z.filter$filtervector==1),] 
>>
>>             #print (final.roi.df)
>>             kicked.out<-(length(filtervector) - 
>> sum(filtervector))/length(filtervector)
>>             print(kicked.out)
>>             matrix.col = matrix.col + 1
>>             attach(final.roi.df)
>>             names(final.roi.df)
>>             print(matrix.col)
>>             #set cutoff for FDR. per voxel p values (of Z dist) .05 = 
>> 1.96, .01 = 2.575, .001 = 3.277, .005 = 3.479 BE SURE TO CHANGE THE 
>> VARIABLE EACH TIME YOU CHANGE CONDITION
>>                pre.outliers<-subset(final.roi.df, gFDR4FWHM >= 1.96)
>>                detach(final.roi.df)
>>                attach(pre.outliers)
>>                outliers<-subset(pre.outliers, gtvalue4FWHM >= 0.00)
>> ...and the script dies here because there are no data in "outliers".
>>
>


-- 
Anthony Steven Dick, Ph.D.
Post-Doctoral Scholar
Human Neuroscience Laboratory
Biological Sciences Division
University of Chicago
5841 S. Maryland Ave. MC-2030
Chicago, IL 60637
Phone: (773)-834-7770
Email: adick at uchicago.edu
Alternate email: anthony at anthonymail.com

Please visit my homepage at http://home.uchicago.edu/~adick/


From bcarvalh at jhsph.edu  Mon Mar 26 18:35:44 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Mon, 26 Mar 2007 12:35:44 -0400
Subject: [R] substitute variable
In-Reply-To: <b490ce570703260931n6243ef44je95bda9816aa7135@mail.gmail.com>
References: <b490ce570703260931n6243ef44je95bda9816aa7135@mail.gmail.com>
Message-ID: <B050EF41-7D15-48AB-BEA9-C74811F691E4@jhsph.edu>

say your data frame is called "tmp"

tmp$PRODUCTS[tmp$PRODUCTS > 70] <- NA

b

On Mar 26, 2007, at 12:31 PM, Sergio Della Franca wrote:

> Dear R-Helpers,
>
> I want to substitute the contents of a variable under some contitions.
>
> I.e., I have this data set:
>
> YEAR   PRODUCTS
> 1          80
> 2          90
> 3          50
> 4          60
> 5          30
>
> I want to perform this condition:
> if products> 70 then products=NA else products=products.
>
> I'd like to achive the seguent result:
>
>  YEAR   PRODUCTS
> 1          NA
> 2          NA
> 3          50
> 4          60
> 5          30
> How can i develop this?
>
>
> Thank you in advance.
>
>
> Sergio Della Franca
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wwwhsd at gmail.com  Mon Mar 26 18:36:12 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Mon, 26 Mar 2007 13:36:12 -0300
Subject: [R] substitute variable
In-Reply-To: <b490ce570703260931n6243ef44je95bda9816aa7135@mail.gmail.com>
References: <b490ce570703260931n6243ef44je95bda9816aa7135@mail.gmail.com>
Message-ID: <da79af330703260936q5d5dc0acu972c7580e5450c46@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070326/82b2e628/attachment.pl 

From sergio.della.franca at gmail.com  Mon Mar 26 18:47:42 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Mon, 26 Mar 2007 18:47:42 +0200
Subject: [R] substitute variable
In-Reply-To: <B050EF41-7D15-48AB-BEA9-C74811F691E4@jhsph.edu>
References: <b490ce570703260931n6243ef44je95bda9816aa7135@mail.gmail.com>
	<B050EF41-7D15-48AB-BEA9-C74811F691E4@jhsph.edu>
Message-ID: <b490ce570703260947w2124287ch7885b5175dd7882a@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070326/60583e31/attachment.pl 

From bcarvalh at jhsph.edu  Mon Mar 26 18:55:02 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Mon, 26 Mar 2007 12:55:02 -0400
Subject: [R] substitute variable
In-Reply-To: <b490ce570703260947w2124287ch7885b5175dd7882a@mail.gmail.com>
References: <b490ce570703260931n6243ef44je95bda9816aa7135@mail.gmail.com>
	<B050EF41-7D15-48AB-BEA9-C74811F691E4@jhsph.edu>
	<b490ce570703260947w2124287ch7885b5175dd7882a@mail.gmail.com>
Message-ID: <CB88A7E5-0981-488A-B326-A02B676E5ED9@jhsph.edu>

condition1 | condition2 (or)
condition1 & condition2 (and)

tmp$PRODUCTS[tmp$PRODUCTS > 70 | tmp$PRODUCTS  < 20] <- NA

b

On Mar 26, 2007, at 12:47 PM, Sergio Della Franca wrote:

> Ok, this run correctly.
>
> Now i want to perform much more conditions, i.e.:
>
> tmp$PRODUCTS[tmp$PRODUCTS > 70] <- NA
>
> and
>
> tmp$PRODUCTS[tmp$PRODUCTS < 20] <- NA.
>
> How can i perform this double condition in the same code?
>
>
> 2007/3/26, Benilton Carvalho <bcarvalh at jhsph.edu>: say your data  
> frame is called "tmp"
>
> tmp$PRODUCTS[tmp$PRODUCTS > 70] <- NA
>
> b
>
> On Mar 26, 2007, at 12:31 PM, Sergio Della Franca wrote:
>
> > Dear R-Helpers,
> >
> > I want to substitute the contents of a variable under some  
> contitions.
> >
> > I.e., I have this data set:
> >
> > YEAR   PRODUCTS
> > 1          80
> > 2          90
> > 3          50
> > 4          60
> > 5          30
> >
> > I want to perform this condition:
> > if products> 70 then products=NA else products=products.
> >
> > I'd like to achive the seguent result:
> >
> >  YEAR   PRODUCTS
> > 1          NA
> > 2          NA
> > 3          50
> > 4          60
> > 5          30
> > How can i develop this?
> >
> >
> > Thank you in advance.
> >
> >
> > Sergio Della Franca
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>


From jrkrideau at yahoo.ca  Mon Mar 26 20:06:33 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Mon, 26 Mar 2007 14:06:33 -0400 (EDT)
Subject: [R] problem with putting  Hmisc labeled matrix into a data.frame.
Message-ID: <371657.62526.qm@web32803.mail.mud.yahoo.com>

# I seem to be having a bit of a problem with Hmisc.
At least, I assume it is
# coming from there. When I try to put a labeled
matrix into a data.frame
# I get a very strange result. If you would run the
two examples I think
# you will see my problem.

# Example 1  no label
library(Hmisc)
mydata <- matrix(1:12, 3, 4)
myvec <- matrix( c("a","b","c"), 3)

mf <- data.frame(myvec, mydata) ; mf    # works fine

#  with label

ndata <- matrix(1:12, 3, 4)
label(ndata) <- "Great matrix"
nvec <- matrix( c("a","b","c"), 3)

nf <- data.frame(nvec, ndata) ; nf   #  Not so fine


>
> nf <- data.frame(nvec, ndata) ; nf   #
   nvec ndata.1 ndata.2 ndata.3 ndata.4
1     a       1       4       7      10
2     b       2       5       8      11
3     c       3       6       9      12
4     a    <NA>    <NA>    <NA>    <NA>
5     b    <NA>    <NA>    <NA>    <NA>
6     c    <NA>    <NA>    <NA>    <NA>
7     a    <NA>    <NA>    <NA>    <NA>
8     b    <NA>    <NA>    <NA>    <NA>
9     c    <NA>    <NA>    <NA>    <NA>
10    a    <NA>    <NA>    <NA>    <NA>
11    b    <NA>    <NA>    <NA>    <NA>
12    c    <NA>    <NA>    <NA>    <NA>
Warning message:
corrupt data frame: columns will be truncated or
padded with NAs in: format.data.frame(x, digits =
digits, na.encode = FALSE)
>


# Am I missing something obvious or should I bring
this to the
# attention of the package maintainer?  I did not find
anything in the archives.

# Running Windows XP,  R 2.4.1


From ambertk at ohsu.edu  Mon Mar 26 20:21:36 2007
From: ambertk at ohsu.edu (Kyle.)
Date: Mon, 26 Mar 2007 11:21:36 -0700
Subject: [R] data-frame adding/deleting column
In-Reply-To: <8B7B0FD99E8AF541A21609104D19615882E8E4@izs-xchg01.izs.fraunhofer.de>
References: <8B7B0FD99E8AF541A21609104D19615882E8E4@izs-xchg01.izs.fraunhofer.de>
Message-ID: <46FDDAF7-6465-464F-9319-50457482BBF5@ohsu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070326/492ff899/attachment.pl 

From oliverfaulhaber at gmx.de  Mon Mar 26 20:48:50 2007
From: oliverfaulhaber at gmx.de (Oliver Faulhaber)
Date: Mon, 26 Mar 2007 20:48:50 +0200
Subject: [R] Is the random number generator biased?
Message-ID: <46081592.7010003@gmx.de>

Hi all,

in order to verify some results I did the following test in R (2.4.1., 
windows system):

X <- cumsum(rnorm(1000000))
for (i in 1:1000) {
  tmp                   <- seq(1,length(X),by=i)
  X.coarse              <- X[tmp]
  X.return              <- diff(X.coarse)
  X.scale.mean[i]       <- mean(X.return)
}
plot(X.scale.mean,type="l")

As X is a random walk with increments following a standard normal 
distribution, the mean of X should be 0. Further more, each "piece" of 
the random walk should also have zero mean - independent of its length.

Why is it then, that the plot of X.scale.mean shows a clear linear trend?

Is the generation of the random walk in some way biased or do I just 
miss some point?

Thanks for any enlighting
replies in advance
Oliver


From mtb954 at gmail.com  Mon Mar 26 21:06:51 2007
From: mtb954 at gmail.com (mtb954 at gmail.com)
Date: Mon, 26 Mar 2007 13:06:51 -0600
Subject: [R] How to drop variables using a wildcard and logic...
Message-ID: <e40d78ce0703261206n6822b9f7l5227f7389836ddc1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070326/3b15de28/attachment.pl 

From bcarvalh at jhsph.edu  Mon Mar 26 21:19:33 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Mon, 26 Mar 2007 15:19:33 -0400
Subject: [R] How to drop variables using a wildcard and logic...
In-Reply-To: <e40d78ce0703261206n6822b9f7l5227f7389836ddc1@mail.gmail.com>
References: <e40d78ce0703261206n6822b9f7l5227f7389836ddc1@mail.gmail.com>
Message-ID: <8E20E966-A537-419C-A0A4-AF61EDC5B1C8@jhsph.edu>

if 'test' is your data frame...

test[, grep("[tT]$", names(test))]

b

On Mar 26, 2007, at 3:06 PM, mtb954 at gmail.com wrote:

> Dear R users
>
> I would like to make a new dataframe  from an existing dataframe,  
> retaining
> ONLY those variables that end in the letter "t"
>
> I have searched the help archives and consulted several reference  
> books but
> cannot seem to find an example.
>
> Any ideas...? Thanks!
>
> Mark
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Mon Mar 26 21:20:49 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 26 Mar 2007 15:20:49 -0400
Subject: [R] Is the random number generator biased?
In-Reply-To: <46081592.7010003@gmx.de>
References: <46081592.7010003@gmx.de>
Message-ID: <46081D11.3060908@stats.uwo.ca>

On 3/26/2007 2:48 PM, Oliver Faulhaber wrote:
> Hi all,
> 
> in order to verify some results I did the following test in R (2.4.1., 
> windows system):
> 
> X <- cumsum(rnorm(1000000))
> for (i in 1:1000) {
>   tmp                   <- seq(1,length(X),by=i)
>   X.coarse              <- X[tmp]
>   X.return              <- diff(X.coarse)
>   X.scale.mean[i]       <- mean(X.return)
> }
> plot(X.scale.mean,type="l")
> 
> As X is a random walk with increments following a standard normal 
> distribution, the mean of X should be 0. Further more, each "piece" of 
> the random walk should also have zero mean - independent of its length.
> 
> Why is it then, that the plot of X.scale.mean shows a clear linear trend?

The points in your plot are all based on a single realization of a 
Brownian motion.  What you are seeing is just that X[1000000] is 
non-zero.  If you repeat the simulation you'll get a different linear trend.

> Is the generation of the random walk in some way biased or do I just 
> miss some point?

If it is biased, your plots don't show it.  Try this plot instead (and 
wait a lot longer; it does a lot of memory allocations!):

X.scale.mean <- numeric(1000)

for (i in 1:1000) {
    X <- cumsum(rnorm(1000000))
    tmp                   <- seq(1,length(X),by=i)
    X.coarse              <- X[tmp]
    X.return              <- diff(X.coarse)
    X.scale.mean[i]       <- mean(X.return)
}
plot(X.scale.mean,type="l")

Duncan Murdoch

> 
> Thanks for any enlighting
> replies in advance
> Oliver
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tlumley at u.washington.edu  Mon Mar 26 21:25:44 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 26 Mar 2007 12:25:44 -0700 (PDT)
Subject: [R] subset
In-Reply-To: <1174923762.4927.21.camel@Bellerophon>
Message-ID: <Pine.LNX.4.43.0703261225440.18582@hymn04.u.washington.edu>

On Mon, 26 Mar 2007, Marc Schwartz wrote:

> Sergio,
>
> Please be sure to cc: the list (ie. Reply to All) with follow up
> questions.
>
> In this case, you would use %in% with a negation:
>
> NewDF <- subset(DF, (var1 == 0) & (var2 == 0) & (!var3 %in% 2:3))
>

Probably a typo: should be !(var3 %in% 2:3) rather than (!var %in% 2:3)

      -thomas

> See ?"%in%" for more information.
>
> HTH,
>
> Marc
>
> On Mon, 2007-03-26 at 17:30 +0200, Sergio Della Franca wrote:
>> Ok, this run correctly.
>>
>> Another question for you:
>>
>> I want to put more than one condition for var3, i.e.:
>> I like to create a subset when:
>>  - var1=0
>>  - var2=0
>>  - var3 is different from 2 and from 3.
>>
>> Like you suggested, i perform this code:
>> NewDF <- subset(DF, (var1 == 0) & (var2 == 0) & (var 3 != 2)) & (var
>> 3 != 3))
>>
>> There is a method to combine (var 3 != 2)) & (var 3 != 3)) in one
>> condition?
>>
>> Thank you.
>>
>> Sergio
>>
>>
>>
>> 2007/3/26, Marc Schwartz <marc_schwartz at comcast.net>:
>>         On Mon, 2007-03-26 at 17:02 +0200, Sergio Della Franca wrote:
>>        > Dear R-Helpers,
>>        >
>>        > I want to make a subset from my data set.
>>        >
>>        > I'd like to perform different condition for subset.
>>        >
>>        > I.e.:
>>        >
>>        > I like to create a subset when:
>>        > - var1=0
>>        > - var2=0
>>        > - var3 is different from 2.
>>        >
>>        > How can i develop a subset under this condition?
>>        >
>>        > Thank you in advance.
>>        >
>>        > Sergio Della Franca.
>>
>>         See ?subset
>>
>>         Something along the lines of the following should work:
>>
>>         NewDF <- subset(DF, (var1 == 0) & (var2 == 0) & (var 3 != 0))
>>
>>         HTH,
>>
>>         Marc Schwartz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From sundar.dorai-raj at pdf.com  Mon Mar 26 21:24:35 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 26 Mar 2007 12:24:35 -0700
Subject: [R] How to drop variables using a wildcard and logic...
In-Reply-To: <8E20E966-A537-419C-A0A4-AF61EDC5B1C8@jhsph.edu>
References: <e40d78ce0703261206n6822b9f7l5227f7389836ddc1@mail.gmail.com>
	<8E20E966-A537-419C-A0A4-AF61EDC5B1C8@jhsph.edu>
Message-ID: <46081DF3.1000905@pdf.com>

Alternatively, you can use ?glob2rx

test[, grep(glob2rx("*[tT]"), names(test))]

which allows for wildcards.

--sundar

Benilton Carvalho said the following on 3/26/2007 12:19 PM:
> if 'test' is your data frame...
> 
> test[, grep("[tT]$", names(test))]
> 
> b
> 
> On Mar 26, 2007, at 3:06 PM, mtb954 at gmail.com wrote:
> 
>> Dear R users
>>
>> I would like to make a new dataframe  from an existing dataframe,  
>> retaining
>> ONLY those variables that end in the letter "t"
>>
>> I have searched the help archives and consulted several reference  
>> books but
>> cannot seem to find an example.
>>
>> Any ideas...? Thanks!
>>
>> Mark
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ted.harding at nessie.mcc.ac.uk  Mon Mar 26 21:28:00 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 26 Mar 2007 20:28:00 +0100 (BST)
Subject: [R] Is the random number generator biased?
In-Reply-To: <46081592.7010003@gmx.de>
Message-ID: <XFMail.070326202800.ted.harding@nessie.mcc.ac.uk>

On 26-Mar-07 18:48:50, Oliver Faulhaber wrote:
> Hi all,
> 
> in order to verify some results I did the following test in R (2.4.1., 
> windows system):
> 
> X <- cumsum(rnorm(1000000))
> for (i in 1:1000) {
>   tmp                   <- seq(1,length(X),by=i)
>   X.coarse              <- X[tmp]
>   X.return              <- diff(X.coarse)
>   X.scale.mean[i]       <- mean(X.return)
> }
> plot(X.scale.mean,type="l")
> 
> As X is a random walk with increments following a standard normal 
> distribution, the mean of X should be 0. Further more, each "piece" of 
> the random walk should also have zero mean - independent of its length.
> 
> Why is it then, that the plot of X.scale.mean shows a clear linear
> trend?
> 
> Is the generation of the random walk in some way biased or do I just 
> miss some point?
> 
> Thanks for any enlighting replies in advance
> Oliver

Try

  plot(X,type="l")

and you will see why you get that result from plot(X.scale.mean).

But that does not help to understand why plot(X) looks as it does.
However, this behaviour is standard for a random walk.

If you think about it, var(X[n]) = 1 (in your case), so at
the nth step you are (for instance) likely (P>1/2)to be at
least 0.5*sqrt(n) on one side or other of the mean.
So say n=10000: then you are likely to be at least 50 away
from the mean; and you have to get there somehow, starting
from 0. So there will be an overall trend!

Similarly, for n=1000000, you are likely to be at least 500
from the mean.

The same explanation accounts for the shorter segments of trend
you will also observe over the graph of X, since from any point
along the graph the above holds true on a smaller scale.

Hoping that helps!
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 26-Mar-07                                       Time: 20:27:57
------------------------------ XFMail ------------------------------


From ted.harding at nessie.mcc.ac.uk  Mon Mar 26 21:32:46 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 26 Mar 2007 20:32:46 +0100 (BST)
Subject: [R] Is the random number generator biased?
In-Reply-To: <46081592.7010003@gmx.de>
Message-ID: <XFMail.070326202800.ted.harding@nessie.mcc.ac.uk>

[Sorry, I need to correct a vital typo!!]

On 26-Mar-07 18:48:50, Oliver Faulhaber wrote:
> Hi all,
> 
> in order to verify some results I did the following test in R (2.4.1., 
> windows system):
> 
> X <- cumsum(rnorm(1000000))
> for (i in 1:1000) {
>   tmp                   <- seq(1,length(X),by=i)
>   X.coarse              <- X[tmp]
>   X.return              <- diff(X.coarse)
>   X.scale.mean[i]       <- mean(X.return)
> }
> plot(X.scale.mean,type="l")
> 
> As X is a random walk with increments following a standard normal 
> distribution, the mean of X should be 0. Further more, each "piece" of 
> the random walk should also have zero mean - independent of its length.
> 
> Why is it then, that the plot of X.scale.mean shows a clear linear
> trend?
> 
> Is the generation of the random walk in some way biased or do I just 
> miss some point?
> 
> Thanks for any enlighting replies in advance
> Oliver

Try

  plot(X,type="l")

and you will see why you get that result from plot(X.scale.mean).

But that does not help to understand why plot(X) looks as it does.
However, this behaviour is standard for a random walk.

[XXXIf you think about it, var(X[n]) = 1 (in your case), so atXXX]

If you think about it, var(X[n]( = n (in your case), so at
the nth step you are (for instance) likely (P>1/2)to be at
least 0.5*sqrt(n) on one side or other of the mean.
So say n=10000: then you are likely to be at least 50 away
from the mean; and you have to get there somehow, starting
from 0. So there will be an overall trend!

Similarly, for n=1000000, you are likely to be at least 500
from the mean.

The same explanation accounts for the shorter segments of trend
you will also observe over the graph of X, since from any point
along the graph the above holds true on a smaller scale.

Hoping that helps!
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 26-Mar-07                                       Time: 20:27:57
------------------------------ XFMail ------------------------------

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 26-Mar-07                                       Time: 20:32:42
------------------------------ XFMail ------------------------------


From mtb954 at gmail.com  Mon Mar 26 21:42:18 2007
From: mtb954 at gmail.com (mtb954 at gmail.com)
Date: Mon, 26 Mar 2007 13:42:18 -0600
Subject: [R] How to drop variables using a wildcard and logic...
In-Reply-To: <e40d78ce0703261206n6822b9f7l5227f7389836ddc1@mail.gmail.com>
References: <e40d78ce0703261206n6822b9f7l5227f7389836ddc1@mail.gmail.com>
Message-ID: <e40d78ce0703261242g4da21224x4cd0e17622543030@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070326/e3dcb0d0/attachment.pl 

From andy_liaw at merck.com  Mon Mar 26 21:43:26 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 26 Mar 2007 15:43:26 -0400
Subject: [R] subset  [Broadcast]
In-Reply-To: <Pine.LNX.4.43.0703261225440.18582@hymn04.u.washington.edu>
References: <1174923762.4927.21.camel@Bellerophon>
	<Pine.LNX.4.43.0703261225440.18582@hymn04.u.washington.edu>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03EB2DA7@usctmx1106.merck.com>

From: Thomas Lumley
> 
> On Mon, 26 Mar 2007, Marc Schwartz wrote:
> 
> > Sergio,
> >
> > Please be sure to cc: the list (ie. Reply to All) with follow up 
> > questions.
> >
> > In this case, you would use %in% with a negation:
> >
> > NewDF <- subset(DF, (var1 == 0) & (var2 == 0) & (!var3 %in% 2:3))
> >
> 
> Probably a typo: should be !(var3 %in% 2:3) rather than (!var 
> %in% 2:3)

I used to think so, but found I didn't need the parens:

R> a <- 1:3; b <- c(1, 3, 5)
R> ! a %in% b
[1] FALSE  TRUE FALSE
R> ! (a %in% b)
[1] FALSE  TRUE FALSE

Andy

>       -thomas
> 
> > See ?"%in%" for more information.
> >
> > HTH,
> >
> > Marc
> >
> > On Mon, 2007-03-26 at 17:30 +0200, Sergio Della Franca wrote:
> >> Ok, this run correctly.
> >>
> >> Another question for you:
> >>
> >> I want to put more than one condition for var3, i.e.:
> >> I like to create a subset when:
> >>  - var1=0
> >>  - var2=0
> >>  - var3 is different from 2 and from 3.
> >>
> >> Like you suggested, i perform this code:
> >> NewDF <- subset(DF, (var1 == 0) & (var2 == 0) & (var 3 != 
> 2)) & (var
> >> 3 != 3))
> >>
> >> There is a method to combine (var 3 != 2)) & (var 3 != 3)) in one 
> >> condition?
> >>
> >> Thank you.
> >>
> >> Sergio
> >>
> >>
> >>
> >> 2007/3/26, Marc Schwartz <marc_schwartz at comcast.net>:
> >>         On Mon, 2007-03-26 at 17:02 +0200, Sergio Della 
> Franca wrote:
> >>        > Dear R-Helpers,
> >>        >
> >>        > I want to make a subset from my data set.
> >>        >
> >>        > I'd like to perform different condition for subset.
> >>        >
> >>        > I.e.:
> >>        >
> >>        > I like to create a subset when:
> >>        > - var1=0
> >>        > - var2=0
> >>        > - var3 is different from 2.
> >>        >
> >>        > How can i develop a subset under this condition?
> >>        >
> >>        > Thank you in advance.
> >>        >
> >>        > Sergio Della Franca.
> >>
> >>         See ?subset
> >>
> >>         Something along the lines of the following should work:
> >>
> >>         NewDF <- subset(DF, (var1 == 0) & (var2 == 0) & 
> (var 3 != 0))
> >>
> >>         HTH,
> >>
> >>         Marc Schwartz
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From marc_schwartz at comcast.net  Mon Mar 26 22:14:07 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Mon, 26 Mar 2007 15:14:07 -0500
Subject: [R] subset
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA03EB2DA7@usctmx1106.merck.com>
References: <1174923762.4927.21.camel@Bellerophon>
	<Pine.LNX.4.43.0703261225440.18582@hymn04.u.washington.edu>
	<39B6DDB9048D0F4DAD42CB26AAFF0AFA03EB2DA7@usctmx1106.merck.com>
Message-ID: <1174940047.4927.65.camel@Bellerophon>

On Mon, 2007-03-26 at 15:43 -0400, Liaw, Andy wrote:
> From: Thomas Lumley
> > 
> > On Mon, 26 Mar 2007, Marc Schwartz wrote:
> > 
> > > Sergio,
> > >
> > > Please be sure to cc: the list (ie. Reply to All) with follow up 
> > > questions.
> > >
> > > In this case, you would use %in% with a negation:
> > >
> > > NewDF <- subset(DF, (var1 == 0) & (var2 == 0) & (!var3 %in% 2:3))
> > >
> > 
> > Probably a typo: should be !(var3 %in% 2:3) rather than (!var 
> > %in% 2:3)
> 
> I used to think so, but found I didn't need the parens:
> 
> R> a <- 1:3; b <- c(1, 3, 5)
> R> ! a %in% b
> [1] FALSE  TRUE FALSE
> R> ! (a %in% b)
> [1] FALSE  TRUE FALSE
> 
> Andy

Thanks Andy, you beat me to it  :-)

Just for the explicit sake of the variation as I used in my reply above:

> (!a %in% b)
[1] FALSE  TRUE FALSE


I suspect that Thomas may be fearing the following scenario:

> !a
[1] FALSE FALSE FALSE

> (!a) %in% b
[1] FALSE FALSE FALSE


If one looks at ?Syntax, the negation operator '!' is listed 5 rows
after '%any%' relative to precedence of operation, so our examples above
worked as documented.

HTH,

Marc


From bgreen at dyson.brisnet.org.au  Mon Mar 26 22:19:12 2007
From: bgreen at dyson.brisnet.org.au (Bob Green)
Date: Tue, 27 Mar 2007 06:19:12 +1000
Subject: [R] fitted probabilities in multinomial logistic regression are
 identical for each level
In-Reply-To: <mailman.11.1174903204.13861.r-help@stat.math.ethz.ch>
References: <mailman.11.1174903204.13861.r-help@stat.math.ethz.ch>
Message-ID: <20070326201547.3152C595142@borg.st.net.au>


I was hoping for some advice regarding possible explanations for the 
fitted probability values I obtained for a multinomial logistic 
regression. The analysis aims to predict whether Capgras delusions 
(present/absent) are associated with group (ABH, SV, homicide; values 
= 1,2,3,), controlling for previous violence. What has me puzzled is 
that for each combination the fitted probabilities are identical. I 
haven't seen this in the worked examples I have come across and was 
interested to know if this is a problem or what might be the cause 
for this. I ran an analysis with another independent variable and 
obtained a similar pattern.

Any assistance with this is appreciated

Bob Green

 > predictors <- expand.grid(group=1:3, in.acute.danger = c("y","n"), 
violent.convictions = c("y","n"))
 > p.fit <- predict(mod.multacute, predictors, type='probs')
 > p.fit
            1         2         3
1  0.4615070 0.3077061 0.2307869
2  0.4615070 0.3077061 0.2307869
3  0.4615070 0.3077061 0.2307869
4  0.7741997 0.1290310 0.0967693
5  0.7741997 0.1290310 0.0967693
6  0.7741997 0.1290310 0.0967693
7  0.4230927 0.3846055 0.1923017
8  0.4230927 0.3846055 0.1923017
9  0.4230927 0.3846055 0.1923017
10 0.7058783 0.1647063 0.1294153
11 0.7058783 0.1647063 0.1294153
12 0.7058783 0.1647063 0.1294153


 > mod.multacute <- multinom(group ~ in.acute.danger * 
violent.convictions, data = kc,  na.action = na.omit)
# weights:  15 (8 variable)
initial  value 170.284905
iter  10 value 131.016050
final  value 130.993722
converged
 > summary(mod.multacute, cor=F, Wald=T)
Call:
multinom(formula = group ~ in.acute.danger * violent.convictions,
     data = kc, na.action = na.omit)

Coefficients:
   (Intercept) in.acute.dangery violent.convictionsy 
in.acute.dangery:violent.convictionsy
2   -1.455279        1.3599055           -0.3364982 
          0.02651913
3   -1.696416        0.9078901           -0.3830842 
          0.47860722

Std. Errors:
   (Intercept) in.acute.dangery violent.convictionsy 
in.acute.dangery:violent.convictionsy
2   0.2968082        0.5282077            0.6162498 
           0.9936493
3   0.3279838        0.6312569            0.6946869 
           1.1284891

Value/SE (Wald statistics):
   (Intercept) in.acute.dangery violent.convictionsy 
in.acute.dangery:violent.convictionsy
2   -4.903094         2.574566           -0.5460419 
          0.02668862
3   -5.172256         1.438226           -0.5514486 
          0.42411327

Residual Deviance: 261.9874
AIC: 277.9874
 > Anova (mod.multacute)
Anova Table (Type II tests)

Response: group
                                     LR Chisq Df Pr(>Chisq)
in.acute.danger                      10.9335  2   0.004225 **
violent.convictions                   0.5957  2   0.742430
in.acute.danger:violent.convictions   0.1895  2   0.909600
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1


From p.dalgaard at biostat.ku.dk  Mon Mar 26 22:58:03 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 26 Mar 2007 22:58:03 +0200
Subject: [R] Is the random number generator biased?
In-Reply-To: <46081592.7010003@gmx.de>
References: <46081592.7010003@gmx.de>
Message-ID: <460833DB.7000208@biostat.ku.dk>

Oliver Faulhaber wrote:
> Hi all,
>
> in order to verify some results I did the following test in R (2.4.1., 
> windows system):
>
> X <- cumsum(rnorm(1000000))
> for (i in 1:1000) {
>   tmp                   <- seq(1,length(X),by=i)
>   X.coarse              <- X[tmp]
>   X.return              <- diff(X.coarse)
>   X.scale.mean[i]       <- mean(X.return)
> }
> plot(X.scale.mean,type="l")
>
> As X is a random walk with increments following a standard normal 
> distribution, the mean of X should be 0. Further more, each "piece" of 
> the random walk should also have zero mean - independent of its length.
>
> Why is it then, that the plot of X.scale.mean shows a clear linear trend?
>
> Is the generation of the random walk in some way biased or do I just 
> miss some point?
>
>   
The latter. If you think a little closer, you'll realize that 
sum(X.return) is going to be pretty close to X[1000000], except for the 
sum of at most 999 terms (at most 961, actually). You then proceed to 
calculate the mean by dividing by the number of terms which is 
essentially 1/i.

> Thanks for any enlighting
> replies in advance
> Oliver
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jfox at mcmaster.ca  Mon Mar 26 23:30:28 2007
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 26 Mar 2007 17:30:28 -0400
Subject: [R] fitted probabilities in multinomial logistic regression are
 identical for each level
In-Reply-To: <20070326201547.3152C595142@borg.st.net.au>
Message-ID: <web-167662637@cgpsrv2.cis.mcmaster.ca>

Dear Bob,

If I understand correctly what you've done, the "newdata" that you're
using to get predicted values includes the three values of the response
variable, which are irrelevant to the predictions and cause each
prediction to be repeated three times.

I hope that this helps,
 John

On Tue, 27 Mar 2007 06:19:12 +1000
 Bob Green <bgreen at dyson.brisnet.org.au> wrote:
> 
> I was hoping for some advice regarding possible explanations for the 
> fitted probability values I obtained for a multinomial logistic 
> regression. The analysis aims to predict whether Capgras delusions 
> (present/absent) are associated with group (ABH, SV, homicide; values
> 
> = 1,2,3,), controlling for previous violence. What has me puzzled is 
> that for each combination the fitted probabilities are identical. I 
> haven't seen this in the worked examples I have come across and was 
> interested to know if this is a problem or what might be the cause 
> for this. I ran an analysis with another independent variable and 
> obtained a similar pattern.
> 
> Any assistance with this is appreciated
> 
> Bob Green
> 
>  > predictors <- expand.grid(group=1:3, in.acute.danger = c("y","n"),
> 
> violent.convictions = c("y","n"))
>  > p.fit <- predict(mod.multacute, predictors, type='probs')
>  > p.fit
>             1         2         3
> 1  0.4615070 0.3077061 0.2307869
> 2  0.4615070 0.3077061 0.2307869
> 3  0.4615070 0.3077061 0.2307869
> 4  0.7741997 0.1290310 0.0967693
> 5  0.7741997 0.1290310 0.0967693
> 6  0.7741997 0.1290310 0.0967693
> 7  0.4230927 0.3846055 0.1923017
> 8  0.4230927 0.3846055 0.1923017
> 9  0.4230927 0.3846055 0.1923017
> 10 0.7058783 0.1647063 0.1294153
> 11 0.7058783 0.1647063 0.1294153
> 12 0.7058783 0.1647063 0.1294153
> 
> 
>  > mod.multacute <- multinom(group ~ in.acute.danger * 
> violent.convictions, data = kc,  na.action = na.omit)
> # weights:  15 (8 variable)
> initial  value 170.284905
> iter  10 value 131.016050
> final  value 130.993722
> converged
>  > summary(mod.multacute, cor=F, Wald=T)
> Call:
> multinom(formula = group ~ in.acute.danger * violent.convictions,
>      data = kc, na.action = na.omit)
> 
> Coefficients:
>    (Intercept) in.acute.dangery violent.convictionsy 
> in.acute.dangery:violent.convictionsy
> 2   -1.455279        1.3599055           -0.3364982 
>           0.02651913
> 3   -1.696416        0.9078901           -0.3830842 
>           0.47860722
> 
> Std. Errors:
>    (Intercept) in.acute.dangery violent.convictionsy 
> in.acute.dangery:violent.convictionsy
> 2   0.2968082        0.5282077            0.6162498 
>            0.9936493
> 3   0.3279838        0.6312569            0.6946869 
>            1.1284891
> 
> Value/SE (Wald statistics):
>    (Intercept) in.acute.dangery violent.convictionsy 
> in.acute.dangery:violent.convictionsy
> 2   -4.903094         2.574566           -0.5460419 
>           0.02668862
> 3   -5.172256         1.438226           -0.5514486 
>           0.42411327
> 
> Residual Deviance: 261.9874
> AIC: 277.9874
>  > Anova (mod.multacute)
> Anova Table (Type II tests)
> 
> Response: group
>                                      LR Chisq Df Pr(>Chisq)
> in.acute.danger                      10.9335  2   0.004225 **
> violent.convictions                   0.5957  2   0.742430
> in.acute.danger:violent.convictions   0.1895  2   0.909600
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/


From ambertk at ohsu.edu  Mon Mar 26 23:59:12 2007
From: ambertk at ohsu.edu (Kyle.)
Date: Mon, 26 Mar 2007 14:59:12 -0700
Subject: [R] Sphericity and post-hoc analysis in a repeated-measure ANOVA
Message-ID: <AD9186B3-5E4D-4996-BCC1-74933174DB3C@ohsu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070326/25817a9e/attachment.pl 

From molshansky at chimaeracapital.com  Tue Mar 27 01:32:15 2007
From: molshansky at chimaeracapital.com (Moshe Olshansky)
Date: Tue, 27 Mar 2007 09:32:15 +1000
Subject: [R] Updating a worksheet in Excel file using RODBC
Message-ID: <B16F2EFF2E11604E93044813DD4413598A5921@warp8.chimaera.local>

Dear Hans-Peter,

Thank you very much for your note!

I tried your package and it works all right (i.e. it indeed writes data to Excel files), however it creates a new Excel file and this is not what I really need.  I need to update/create one sheet in the existing file.
I am using R do compute some data but then it must be put into an Excel file and an Excel chart must be created.  So I intended to (manually) create one excel file and to write a VBA macro which makes a desirable chart of a certain sheet (let's say Sheet1).  Then I intended to autoomatically make many copies of this file with appropriate file names, write the right data to Sheet1 of each such file so that when it is opened a desired chart is automatically created.  So creting a totally new file does not help me.

Best regards,

Moshe.

-----Original Message-----
From: Hans-Peter [mailto:gchappi at gmail.com]
Sent: Saturday, 24 March 2007 3:44 AM
To: Moshe Olshansky
Cc: R Help
Subject: Re: [R] Updating a worksheet in Excel file using RODBC


Hi,

2007/3/23, Moshe Olshansky <molshansky at chimaeracapital.com>:
> Hello!
>
>I have no problem reading Excel files (each worksheet in the file is
a >"table" which can be read - at least in my case).
>What I would like to do is to read such a table, change it (just the
>contents, not the format) and write it back, and this I can not do.
I am >getting the following error messages (3 slightly different
attempts):

> [snip]

As another option (if you work with Windows) you can check my
"xlsReadWrite" package (-> CRAN).

It should work very well in your case (it's not suited if you want to
use SQL (join) statements, but for plain data reading/writing it is
nice).

For both versions (free/pro) updates are pending. They should be
released by end of next week (but no guarantees).

-- 

Regards,
Hans-Peter


Moshe Olshansky

Chimaera Capital Limited
Level 4 / 349 Collins Street
Melbourne, Victoria 3000
Phone: +613 8614 8400
Fax: +613 8614 8410
Email: molshansky at chimaeracapital.com

Disclaimer: This message is intended only for the personal and confidential use of the designated recipient(s) named above. If you are not the intended recipient of this message you are hereby notified that any review, dissemination, distribution or copying of this message is strictly prohibited. This communication is for information purposes only and should not be regarded as an offer to sell or as a solicitation of an offer to buy any financial product, an official confirmation of any transaction, or as an official statement of Chimaera Capital Limited. E-mail transmissions cannot be guaranteed to be secure or error-free. Therefore, we do not represent that this information is complete or accurate and it should not be relied upon as such. All information is subject to change without notice.


From molshansky at chimaeracapital.com  Tue Mar 27 02:08:26 2007
From: molshansky at chimaeracapital.com (Moshe Olshansky)
Date: Tue, 27 Mar 2007 10:08:26 +1000
Subject: [R] Updating a worksheet in Excel file using RODBC
Message-ID: <B16F2EFF2E11604E93044813DD4413598A5922@warp8.chimaera.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070327/19b56d7c/attachment.pl 

From tkobayas at indiana.edu  Tue Mar 27 04:29:17 2007
From: tkobayas at indiana.edu (tkobayas at indiana.edu)
Date: Mon, 26 Mar 2007 22:29:17 -0400
Subject: [R] Help for looping
Message-ID: <20070326222917.79og8vata840kw84@webmail.iu.edu>

Rusers:

I have tried to minimize computing times by taking advanage of 
lapply(). My data is a 1000*30 matrix and the distance matrix was 
created with dist(). What I am trying to do is to compute the standard 
distances using the frequencies attached to the nearest negibors of n 
reference zones. So I will have 1000 standard distances, and would like 
to see the frequency distribution of the standard distances.

# Convert decimal degrees into UTM miles
x<-(data[,1]-58277.194363)*0.000621
y<-(data[,2]-4414486.03135)*0.000621

# Combine x y for computing distances
coords<-cbind(x,y)
pts<-length(data)

# Subset housing data and employment data
RES<-data[,3:17]
EMP<-data[,378:392]

# Combine all the subdata as D
D<-cbind(coords,RES,EMP)

cases<-ncol(D)-ncol(coords)

# Create a threshold bandwidth for defining the nearest neighbors
thrs<-seq(0,35,by=1)

SDTAZ<-rep(list(matrix(,nrow(D),length(thrs))),cases)


for (j in 1:nrow(D))
for (k in 1:length(thrs))
for (l in 1:cases)
{
{
{

SDTAZ[[l]][j,k]<-
sqrt(
   sum(
	(D[as.vector(which(dis[j,]<=thrs[k])),l+2]-D[j,l+2]-
          min(D[as.vector(which(dis[j,]<=thrs[k])),l+2]-D[j,l+2])+1)*
         (
	   (dis[j,as.vector(which(dis[j,]<=thrs[k]))])^2
         )
      )

	/sum(D[as.vector(which(dis[j,]<=thrs[k])),l+2]-D[j,l+2]-
         min(D[as.vector(which(dis[j,]<=thrs[k])),l+2]-D[j,l+2])+1)
		)
}
}
}

I think that within this nested loop, I should use lapply() but I ended 
up getting different values.... I appreciate if someone could kindly 
help me.

Thank you very much.
------------------------------------
Takatsugu Kobayashi
PhD Candidate
Indiana University, Dept. Geography


From deepayan.sarkar at gmail.com  Tue Mar 27 05:39:32 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 26 Mar 2007 20:39:32 -0700
Subject: [R] using alpha transparency for lines in levelplot
In-Reply-To: <46077D10.6080601@utas.edu.au>
References: <46077D10.6080601@utas.edu.au>
Message-ID: <eb555e660703262039yee24ba0h4daaef277bef07cc@mail.gmail.com>

On 3/26/07, Michael Sumner <mdsumner at utas.edu.au> wrote:
> Hello, I'm having trouble with using the alpha channel for transparency
> with lines with lattice levelplots.
>
> If I use transparency via the alpha argument to rgb to overplot lines on
> levelplot the transparent colour affects all of the region colours in the
> plot.

I don't understand what you are trying to say. Here's a modified
version of your code:


------------------
## panel function to add lines with grey(0.3)
my.panel <- function(...) {
    panel.xyplot(1:60, runif(60, 1, 60), type = "l", lwd = 3,
                 col = grey(0.3))
    panel.contourplot(...);
    panel.xyplot(1:60, runif(60, 1, 60), type = "l", lwd = 3,
                 col = grey(0.3))
}

## panel function to add lines with grey transparency
my.paneltransp <- function(...) {
   panel.xyplot(1:60, runif(60, 1, 60), type = "l", lwd = 3,
                col = rgb(0.3, 0.3, 0.3, 0.5))
   panel.contourplot(...)
   panel.xyplot(1:60, runif(60, 1, 60), type = "l", lwd = 3,
                col = rgb(0.3, 0.3, 0.3, 0.5))
}

pdf("alpha.pdf", version = "1.4")

## this works fine
levelplot(z~x+y, xy, panel = my.panel)

## this doesn't?
levelplot(z~x+y, xy, panel = my.paneltransp)

dev.off()
------------------

and the resulting 2-page PDF file is at

http://dsarkar.fhcrc.org/R/alpha.pdf

I don't see any evidence of transparency in the 'region' colors (the
lines behind the level plot never show up). Are you sure whatever you
are seeing is not an artifact of your display application?

Deepayan


From mdsumner at utas.edu.au  Tue Mar 27 06:01:21 2007
From: mdsumner at utas.edu.au (Michael Sumner)
Date: Tue, 27 Mar 2007 14:01:21 +1000
Subject: [R] using alpha transparency for lines in levelplot
In-Reply-To: <eb555e660703262039yee24ba0h4daaef277bef07cc@mail.gmail.com>
References: <46077D10.6080601@utas.edu.au>
	<eb555e660703262039yee24ba0h4daaef277bef07cc@mail.gmail.com>
Message-ID: <46089711.5090802@utas.edu.au>

Deepayan Sarkar wrote:
>
> I don't understand what you are trying to say. Here's a modified
> version of your code:
>
>

Sorry, it's not that the transparency affects the plot regions but the 
colour of the lines darkens the region
colours. I.e. with grey lines the regions are greyed. In your PDF the 
regions are greyed (darker)
in both plots than the colours I see with the usual lattice colour scheme.
Do you notice any difference between the region colours of the plots if 
you write
them to separate files?

If not it must be my viewer.

Cheers, Mike.

> ------------------
> ## panel function to add lines with grey(0.3)
> my.panel <- function(...) {
>    panel.xyplot(1:60, runif(60, 1, 60), type = "l", lwd = 3,
>                 col = grey(0.3))
>    panel.contourplot(...);
>    panel.xyplot(1:60, runif(60, 1, 60), type = "l", lwd = 3,
>                 col = grey(0.3))
> }
>
> ## panel function to add lines with grey transparency
> my.paneltransp <- function(...) {
>   panel.xyplot(1:60, runif(60, 1, 60), type = "l", lwd = 3,
>                col = rgb(0.3, 0.3, 0.3, 0.5))
>   panel.contourplot(...)
>   panel.xyplot(1:60, runif(60, 1, 60), type = "l", lwd = 3,
>                col = rgb(0.3, 0.3, 0.3, 0.5))
> }
>
> pdf("alpha.pdf", version = "1.4")
>
> ## this works fine
> levelplot(z~x+y, xy, panel = my.panel)
>
> ## this doesn't?
> levelplot(z~x+y, xy, panel = my.paneltransp)
>
> dev.off()
> ------------------
>
> and the resulting 2-page PDF file is at
>
> http://dsarkar.fhcrc.org/R/alpha.pdf
>
> I don't see any evidence of transparency in the 'region' colors (the
> lines behind the level plot never show up). Are you sure whatever you
> are seeing is not an artifact of your display application?
>
> Deepayan
>
>


From mdsumner at utas.edu.au  Tue Mar 27 07:19:54 2007
From: mdsumner at utas.edu.au (Michael Sumner)
Date: Tue, 27 Mar 2007 15:19:54 +1000
Subject: [R] using alpha transparency for lines in levelplot - SUMMARY
In-Reply-To: <eb555e660703262039yee24ba0h4daaef277bef07cc@mail.gmail.com>
References: <46077D10.6080601@utas.edu.au>
	<eb555e660703262039yee24ba0h4daaef277bef07cc@mail.gmail.com>
Message-ID: <4608A97A.6050005@utas.edu.au>

Hello, thanks to Deepayan Sarkar for sorting me out on this one.

The problem with transparent lines affecting region colour in lattice 
plot appears
when using Adobe Reader (v 8 in my case). I've only viewed the file on 
Windows XP.

I've tried using Foxit Reader to view the file and there's no problem.

Cheers, Mike.


From BHeidecker at med.miami.edu  Tue Mar 27 07:41:39 2007
From: BHeidecker at med.miami.edu (Heidecker, Bettina)
Date: Tue, 27 Mar 2007 01:41:39 -0400
Subject: [R] Random divisions
Message-ID: <00AE199C90C1644AB942DEBBD4F9573B0225B080@MEDEX02.ad.med.miami.edu>

I am working with microarray analysis and was using PAM with excel interface. Is there a way to do random divisions for the training set in excel? 
 
I also tried PAM in R with the pamr menu. How can I do the random divisions in R?  
 
 
Then I tried to reproduce "classification with gene expression data", which is chapter 24 from the book "bioinformatics and computational biology solutions using R and Bioconductor". There is also an explanation for random divisions using "eset" as an example. How do I have to load my data into R, to be able to reproduce, what they have done? what kind of file is eset? is it like an excel spreadsheet? I have my microarray data now in a .csv or .txt file with phenotype in the first row, class in the second row and then gene expression data starting from the third row. first column are the affy-IDs.  I have two classes. 
 
I would appreciate it, if someone could help me out with that!!!
Bettina


From Zamikhaya.Mbalu at dhs.vic.gov.au  Tue Mar 27 07:55:41 2007
From: Zamikhaya.Mbalu at dhs.vic.gov.au (Zamikhaya.Mbalu at dhs.vic.gov.au)
Date: Tue, 27 Mar 2007 15:55:41 +1000
Subject: [R] Newbie: Combn and scripting
Message-ID: <OF0E065CE6.E774B44B-ONCA2572AB.00201EA9-CA2572AB.002090BC@n212.csv.au>



Hello All,

I have just installed my R 2.4 (windows) as a test trying to load a data
frame and run combn() for each line into another file. How do I do this?

data.csv:
a,b,c,d
1,2,3.4
g,3,6,t
etc

x=data.csv, m=3

Thank you

Zam



_________________________________________________________________________________

This email contains confidential information intended only for the person named above and may be subject to legal privilege. If you are not the intended recipient, any disclosure, copying or use of this information is prohibited. The Department provides no guarantee that this communication is free of virus or that it has not been intercepted or interfered with. If you have received this email in error or have any other concerns regarding its transmission, please notify Postmaster at dhs.vic.gov.au


From vikasrawal at gmail.com  Tue Mar 27 08:21:46 2007
From: vikasrawal at gmail.com (Vikas Rawal)
Date: Tue, 27 Mar 2007 11:51:46 +0530
Subject: [R] problems with installation of packages
Message-ID: <1900b0fa0703262321l6a942637nf9ccbbb4a3e24be5@mail.gmail.com>

I am having a peculiar problem with installation of packages. I am
trying to install the package "maptools". This is what I get.

install.packages("maptools",depend=T)
Warning in install.packages("maptools", depend = T) :
         argument 'lib' is missing: using /usr/local/lib/R/site-library
Warning: unable to access index for repository
http://cran.wustl.edu/src/contribdependency ''maptools'' is not
available

This is a machine running debian sarge. The http_proxy is correctly
set. I am able to download the package by using wget. I also tried
starting R with (R http_proxy-http://my-proxy/). That does not help
either.

I have R 2.4.1 on my debian system.

Will be grateful  for any help.

Vikas


From ripley at stats.ox.ac.uk  Tue Mar 27 08:44:14 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 27 Mar 2007 07:44:14 +0100 (BST)
Subject: [R] Updating a worksheet in Excel file using RODBC
In-Reply-To: <B16F2EFF2E11604E93044813DD4413598A5922@warp8.chimaera.local>
References: <B16F2EFF2E11604E93044813DD4413598A5922@warp8.chimaera.local>
Message-ID: <Pine.LNX.4.64.0703270740430.9277@gannet.stats.ox.ac.uk>

Yes, sqlDrop does not work correctly for Excel worksheet names (and there 
are other quirks).

As I said in another message, it is on my TODO list to make this work 
better, but in the absence of good documentation of what the Excel ODBC 
driver should do and several with known bugs it is largely a matter of 
trial-and-error.

On Tue, 27 Mar 2007, Moshe Olshansky wrote:

> Dear Prof. Ripley,
>
>> You seem not to have tried the simplest possible option.  The following
>> works for me (beware of wrapped lines from mailers)
>>
>> chan <- odbcDriverConnect("DRIVER=Microsoft Excel Driver (*.xls);DBQ=C:\\bdr\\hills.xls; ReadOnly=False")
>> sqlSave(chan, USArrests, "tests", fast=TRUE) # or FALSE
>
> You are right - I have not.
> It does not work exactly as it should have but this solves my problem.
> I created a very small Excel file odbc1.xls containing 3 sheets (test, Sheet2 and Sheet3).
> Below is a short R session:
>
>> chan <- odbcDriverConnect("DRIVER=Microsoft Excel Driver (*.xls);DBQ=C:\\EFGraphs\\odbc1.xls; ReadOnly=False")
> >x<- c(1:6)
> >x <- matrix(x,nrow=3,ncol=2)
> >x <- data.frame(x)
> x
>  X1 X2
> 1  1  4
> 2  2  5
> 3  3  6
>> sqlSave(chan, x, "test", fast=FALSE)
> Error in sqlSave(chan, x, "test", fast = FALSE) :
>        table 'test' already exists
>> sqlSave(chan, x, "tests", fast=FALSE)
>
> As you see I was unable to overwrite an existing sheet (an attempt to drop this table also fails), but I was able to add a new sheet to an existing Excel file (after this action the file contains 4 sheets - the 3 it contained and the last sheet named tests).
> This allows me to do what I wanted, i.e. manually create an Excel file with a small VBA macro, make many copies of this file (under appropriate names), write an appropriate data to each file and then the macro will work on the right data (different for each file).
>
> Thanky you!
>
> Moshe.
>
>
>
>
>
>
>
>
> Moshe Olshansky
>
> Chimaera Capital Limited
> Level 4 / 349 Collins Street
> Melbourne, Victoria 3000
> Phone: +613 8614 8400
> Fax: +613 8614 8410
> Email: molshansky at chimaeracapital.com
>
> Disclaimer: This message is intended only for the personal and confidential use of the designated recipient(s) named above. If you are not the intended recipient of this message you are hereby notified that any review, dissemination, distribution or copying of this message is strictly prohibited. This communication is for information purposes only and should not be regarded as an offer to sell or as a solicitation of an offer to buy any financial product, an official confirmation of any transaction, or as an official statement of Chimaera Capital Limited. E-mail transmissions cannot be guaranteed to be secure or error-free. Therefore, we do not represent that this information is complete or accurate and it should not be relied upon as such. All information is subject to change without notice.
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From molshansky at chimaeracapital.com  Tue Mar 27 08:53:03 2007
From: molshansky at chimaeracapital.com (Moshe Olshansky)
Date: Tue, 27 Mar 2007 16:53:03 +1000
Subject: [R] Updating a worksheet in Excel file using RODBC
Message-ID: <B16F2EFF2E11604E93044813DD4413598A41D0@warp8.chimaera.local>

OK.

By the way,  I only thought that I could do what I wanted!
It worked once but then it failed.  When I was trying to update an existing sheet I got an error message saying that it existed and when I was trying to make a new sheet (something that worked once) I got a message saying that there was no such table!

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: Tuesday, 27 March 2007 4:44 PM
To: Moshe Olshansky
Cc: r-help at hypatia.math.ethz.ch
Subject: Re: Updating a worksheet in Excel file using RODBC


Yes, sqlDrop does not work correctly for Excel worksheet names (and there 
are other quirks).

As I said in another message, it is on my TODO list to make this work 
better, but in the absence of good documentation of what the Excel ODBC 
driver should do and several with known bugs it is largely a matter of 
trial-and-error.

On Tue, 27 Mar 2007, Moshe Olshansky wrote:

> Dear Prof. Ripley,
>
>> You seem not to have tried the simplest possible option.  The following
>> works for me (beware of wrapped lines from mailers)
>>
>> chan <- odbcDriverConnect("DRIVER=Microsoft Excel Driver (*.xls);DBQ=C:\\bdr\\hills.xls; ReadOnly=False")
>> sqlSave(chan, USArrests, "tests", fast=TRUE) # or FALSE
>
> You are right - I have not.
> It does not work exactly as it should have but this solves my problem.
> I created a very small Excel file odbc1.xls containing 3 sheets (test, Sheet2 and Sheet3).
> Below is a short R session:
>
>> chan <- odbcDriverConnect("DRIVER=Microsoft Excel Driver (*.xls);DBQ=C:\\EFGraphs\\odbc1.xls; ReadOnly=False")
> >x<- c(1:6)
> >x <- matrix(x,nrow=3,ncol=2)
> >x <- data.frame(x)
> x
>  X1 X2
> 1  1  4
> 2  2  5
> 3  3  6
>> sqlSave(chan, x, "test", fast=FALSE)
> Error in sqlSave(chan, x, "test", fast = FALSE) :
>        table 'test' already exists
>> sqlSave(chan, x, "tests", fast=FALSE)
>
> As you see I was unable to overwrite an existing sheet (an attempt to drop this table also fails), but I was able to add a new sheet to an existing Excel file (after this action the file contains 4 sheets - the 3 it contained and the last sheet named tests).
> This allows me to do what I wanted, i.e. manually create an Excel file with a small VBA macro, make many copies of this file (under appropriate names), write an appropriate data to each file and then the macro will work on the right data (different for each file).
>
> Thanky you!
>
> Moshe.
>
>
>
>
>
>
>
>
> Moshe Olshansky
>
> Chimaera Capital Limited
> Level 4 / 349 Collins Street
> Melbourne, Victoria 3000
> Phone: +613 8614 8400
> Fax: +613 8614 8410
> Email: molshansky at chimaeracapital.com
>
> Disclaimer: This message is intended only for the personal and confidential use of the designated recipient(s) named above. If you are not the intended recipient of this message you are hereby notified that any review, dissemination, distribution or copying of this message is strictly prohibited. This communication is for information purposes only and should not be regarded as an offer to sell or as a solicitation of an offer to buy any financial product, an official confirmation of any transaction, or as an official statement of Chimaera Capital Limited. E-mail transmissions cannot be guaranteed to be secure or error-free. Therefore, we do not represent that this information is complete or accurate and it should not be relied upon as such. All information is subject to change without notice.
>
>
>

-- 

Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ggrothendieck at gmail.com  Tue Mar 27 08:59:00 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 27 Mar 2007 02:59:00 -0400
Subject: [R] Updating a worksheet in Excel file using RODBC
In-Reply-To: <B16F2EFF2E11604E93044813DD4413598A41D0@warp8.chimaera.local>
References: <B16F2EFF2E11604E93044813DD4413598A41D0@warp8.chimaera.local>
Message-ID: <971536df0703262359p2f0d7ac1g38492ff925292fde@mail.gmail.com>

You could use RDCOMClient or rcom packages to update an Excel
spreadsheet in place and you would not need any VBA at all.  Search
through the archives for the keyword Excel.Application .

On 3/27/07, Moshe Olshansky <molshansky at chimaeracapital.com> wrote:
> OK.
>
> By the way,  I only thought that I could do what I wanted!
> It worked once but then it failed.  When I was trying to update an existing sheet I got an error message saying that it existed and when I was trying to make a new sheet (something that worked once) I got a message saying that there was no such table!
>
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Tuesday, 27 March 2007 4:44 PM
> To: Moshe Olshansky
> Cc: r-help at hypatia.math.ethz.ch
> Subject: Re: Updating a worksheet in Excel file using RODBC
>
>
> Yes, sqlDrop does not work correctly for Excel worksheet names (and there
> are other quirks).
>
> As I said in another message, it is on my TODO list to make this work
> better, but in the absence of good documentation of what the Excel ODBC
> driver should do and several with known bugs it is largely a matter of
> trial-and-error.
>
> On Tue, 27 Mar 2007, Moshe Olshansky wrote:
>
> > Dear Prof. Ripley,
> >
> >> You seem not to have tried the simplest possible option.  The following
> >> works for me (beware of wrapped lines from mailers)
> >>
> >> chan <- odbcDriverConnect("DRIVER=Microsoft Excel Driver (*.xls);DBQ=C:\\bdr\\hills.xls; ReadOnly=False")
> >> sqlSave(chan, USArrests, "tests", fast=TRUE) # or FALSE
> >
> > You are right - I have not.
> > It does not work exactly as it should have but this solves my problem.
> > I created a very small Excel file odbc1.xls containing 3 sheets (test, Sheet2 and Sheet3).
> > Below is a short R session:
> >
> >> chan <- odbcDriverConnect("DRIVER=Microsoft Excel Driver (*.xls);DBQ=C:\\EFGraphs\\odbc1.xls; ReadOnly=False")
> > >x<- c(1:6)
> > >x <- matrix(x,nrow=3,ncol=2)
> > >x <- data.frame(x)
> > x
> >  X1 X2
> > 1  1  4
> > 2  2  5
> > 3  3  6
> >> sqlSave(chan, x, "test", fast=FALSE)
> > Error in sqlSave(chan, x, "test", fast = FALSE) :
> >        table 'test' already exists
> >> sqlSave(chan, x, "tests", fast=FALSE)
> >
> > As you see I was unable to overwrite an existing sheet (an attempt to drop this table also fails), but I was able to add a new sheet to an existing Excel file (after this action the file contains 4 sheets - the 3 it contained and the last sheet named tests).
> > This allows me to do what I wanted, i.e. manually create an Excel file with a small VBA macro, make many copies of this file (under appropriate names), write an appropriate data to each file and then the macro will work on the right data (different for each file).
> >
> > Thanky you!
> >
> > Moshe.
> >
> >
> >
> >
> >
> >
> >
> >
> > Moshe Olshansky
> >
> > Chimaera Capital Limited
> > Level 4 / 349 Collins Street
> > Melbourne, Victoria 3000
> > Phone: +613 8614 8400
> > Fax: +613 8614 8410
> > Email: molshansky at chimaeracapital.com
> >
> > Disclaimer: This message is intended only for the personal and confidential use of the designated recipient(s) named above. If you are not the intended recipient of this message you are hereby notified that any review, dissemination, distribution or copying of this message is strictly prohibited. This communication is for information purposes only and should not be regarded as an offer to sell or as a solicitation of an offer to buy any financial product, an official confirmation of any transaction, or as an official statement of Chimaera Capital Limited. E-mail transmissions cannot be guaranteed to be secure or error-free. Therefore, we do not represent that this information is complete or accurate and it should not be relied upon as such. All information is subject to change without notice.
> >
> >
> >
>
> --
>
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From vikas at mail.jnu.ac.in  Tue Mar 27 09:30:08 2007
From: vikas at mail.jnu.ac.in (Vikas Rawal)
Date: Tue, 27 Mar 2007 13:00:08 +0530
Subject: [R] problem with installation of packages
Message-ID: <460915580200009E00005E99@mail.jnu.ac.in>

I am having a peculiar problem with installation of packages. I am
trying to install the package "maptools". This is what I get.

install.packages("maptools",depend=T)
Warning in install.packages("maptools", depend = T) :
        argument 'lib' is missing: using /usr/local/lib/R/site-library
Warning: unable to access index for repository
http://cran.wustl.edu/src/contribdependency ''maptools'' is not
available

This is a machine running debian sarge. The http_proxy is correctly
set. I am able to download the package by using wget. I also tried
starting R with (R http_proxy-http://my-proxy/). That does not help
either.

I have R 2.4.1 on my debian system.

Will be grateful  for any help.

Vikas


From Mat.Vanderklift at csiro.au  Tue Mar 27 09:43:35 2007
From: Mat.Vanderklift at csiro.au (Mat.Vanderklift at csiro.au)
Date: Tue, 27 Mar 2007 15:43:35 +0800
Subject: [R] Jackknife estimates of predict.lda success rate
Message-ID: <F83C6ACE124F3E4D83B3A90C9CA0922D26A00E@exwa3-per.nexus.csiro.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070327/e0d1a4fb/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Mar 27 09:56:55 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 27 Mar 2007 08:56:55 +0100 (BST)
Subject: [R] problem with installation of packages
In-Reply-To: <460915580200009E00005E99@mail.jnu.ac.in>
References: <460915580200009E00005E99@mail.jnu.ac.in>
Message-ID: <Pine.LNX.4.64.0703270848390.2215@gannet.stats.ox.ac.uk>

This is an internet access problem (there is missing space in the message 
that has been corrected since 2.4.1)

The issue is

> Warning: unable to access index for repository
> http://cran.wustl.edu/src/contrib

That is an issue specific to your setup, but why is someone mailing from 
an Indian address using a mirror in the USA?  (Your signature is missing: 
please see the R posting guide.)

How do you _know_ 'The http_proxy is correctly set'?: no evidence is 
provided here and proxy problems remain the most likely cause.  You can 
debug your setup via options(internet.info=0), but we cannot do it for 
you.


On Tue, 27 Mar 2007, Vikas Rawal wrote:

> I am having a peculiar problem with installation of packages. I am
> trying to install the package "maptools". This is what I get.
>
> install.packages("maptools",depend=T)
> Warning in install.packages("maptools", depend = T) :
>        argument 'lib' is missing: using /usr/local/lib/R/site-library
> Warning: unable to access index for repository
> http://cran.wustl.edu/src/contribdependency ''maptools'' is not
> available
>
> This is a machine running debian sarge. The http_proxy is correctly
> set. I am able to download the package by using wget. I also tried
> starting R with (R http_proxy-http://my-proxy/). That does not help
> either.

As it is syntactically invalid, it will not.  Perhaps

$ http_proxy="http://my-proxy/" R

might help in bash or similar.

> I have R 2.4.1 on my debian system.
>
> Will be grateful  for any help.
>
> Vikas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From vinodkgul at yahoo.com  Tue Mar 27 10:35:36 2007
From: vinodkgul at yahoo.com (vinod gullu)
Date: Tue, 27 Mar 2007 01:35:36 -0700 (PDT)
Subject: [R] DOE in R
In-Reply-To: <mailman.11.1174903204.13861.r-help@stat.math.ethz.ch>
Message-ID: <20070327083536.65970.qmail@web53801.mail.re2.yahoo.com>

Hello
 I want to know that in R which library/package
supports Design of Experiments(I,D etc optimality or
conventional DOE)
Regards,


 
____________________________________________________________________________________
It's here! Your new message!  
Get new email alerts with the free Yahoo! Toolbar.


From Corinna.Schmitt at igb.fraunhofer.de  Tue Mar 27 10:38:57 2007
From: Corinna.Schmitt at igb.fraunhofer.de (Schmitt, Corinna)
Date: Tue, 27 Mar 2007 10:38:57 +0200
Subject: [R] Listing function
Message-ID: <8B7B0FD99E8AF541A21609104D19615882E94B@izs-xchg01.izs.fraunhofer.de>


Thank you very much, folks.
I had read the paper "Introduction to R" but sometimes I had problems
with the examples. Now I can solve all my problems with the help of your
ideas.

Thanks,
Corinna


From bartjoosen at hotmail.com  Tue Mar 27 10:56:32 2007
From: bartjoosen at hotmail.com (Bartjoosen)
Date: Tue, 27 Mar 2007 01:56:32 -0700 (PDT)
Subject: [R] DOE in R
In-Reply-To: <20070327083536.65970.qmail@web53801.mail.re2.yahoo.com>
References: <20070327083536.65970.qmail@web53801.mail.re2.yahoo.com>
Message-ID: <9688614.post@talk.nabble.com>


Check AlgDesign package from Bob Wheeler.



vinod gullu wrote:
> 
> Hello
>  I want to know that in R which library/package
> supports Design of Experiments(I,D etc optimality or
> conventional DOE)
> Regards,
> 
> 
>  
> ____________________________________________________________________________________
> It's here! Your new message!  
> Get new email alerts with the free Yahoo! Toolbar.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/DOE-in-R-tf3471735.html#a9688614
Sent from the R help mailing list archive at Nabble.com.


From daniel at danielberg.no  Tue Mar 27 10:56:45 2007
From: daniel at danielberg.no (Daniel Berg)
Date: Tue, 27 Mar 2007 10:56:45 +0200
Subject: [R] Use of 'defineVar' and 'install' in .Call
Message-ID: <f13f1c9c0703270156hba5b26fyb541817bed557b0c@mail.gmail.com>

Dear all,

[system and version information below]

I am trying to modify a C function for finding the root of an
expression. The function is to be called from R as .Call with input
parameters:

f: expression for which we will find the root
guesses: interval for the solution
stol: tolerance
rho: environment

The original functions I use are:

SEXP mkans(double x) {
  SEXP ans;
  PROTECT(ans = allocVector(REALSXP, 1));
  REAL(ans)[0] = x;
  UNPROTECT(1);
  return ans;
}
double feval(double x, SEXP f, SEXP rho) {
  defineVar(install("x"), mkans(x), rho);
  return(REAL(eval(f, rho))[0]);
}
SEXP zero(SEXP f, SEXP guesses, SEXP stol, SEXP rho) {
  double x0 = REAL(guesses)[0], x1 = REAL(guesses)[1], tol = REAL(stol)[0];
  double f0, f1, fc, xc;
  if(tol <= 0.0) error("non-positive tol value");
  f0 = feval(x0, f, rho); f1 = feval(x1, f, rho);
  if(f0 == 0.0) return mkans(x0);
  if(f1 == 0.0) return mkans(x1);
  if(f0*f1 > 0.0) error("x[0] and x[1] have the same sign");
  for(;;) {
    xc = 0.5*(x0+x1);
    if(fabs(x0-x1) < tol) return mkans(xc);
    fc = feval(xc, f, rho);
    if(fc == 0) return mkans(xc);
    if(f0*fc > 0.0) {
      x0 = xc; f0 = fc;
    }
    else {
      x1 = xc; f1 = fc;
    }
  }
}


This works great. However, I wish to make it more general, by
modifying 'feval'. Given that my problem involves a data set 'u', with
dimension (i x j), I need to assign values to 'u1', 'u2', ..., 'ui'
via defineVar(install(...)). I tried the following:

double feval(double x, double *u, int d, double v, SEXP f, SEXP rho) {
  int i;
  char *str1="u", str2[1001], *str3;
  defineVar(install("x"), mkans(x), rho);
  defineVar(install("y"), mkans(v), rho);
  for(i=0;i<d;i++) {
    sprintf(str2,"%d",i+1);
    str3 = (char *)malloc((strlen(str1)+strlen(str2)+1)*sizeof(char));
    strcpy(str3,str1);
    strcat(str3,str2);
    defineVar(install(str3), mkans(u[i]), rho);
  }
  free(str3);
  return(REAL(eval(f,rho))[0]);
}

My R-package still compiles without errors but R crashes due to the
defineVar command.

Any suggestions to how I can do the defineVar bit?

Thanks in advance.

Reagards,
Daniel Berg

--------------------------------------------
> R.Version()
$platform
[1] "i486-pc-linux-gnu"
$arch
[1] "i486"
$os
[1] "linux-gnu"
$system
[1] "i486, linux-gnu"
$status
[1] ""
$major
[1] "2"
$minor
[1] "3.1"
$year
[1] "2006"
$month
[1] "06"
$day
[1] "01"
$`svn rev`
[1] "38247"
$language
[1] "R"
$version.string
[1] "Version 2.3.1 (2006-06-01)"


From kubovy at virginia.edu  Tue Mar 27 11:24:06 2007
From: kubovy at virginia.edu (Michael Kubovy)
Date: Tue, 27 Mar 2007 05:24:06 -0400
Subject: [R] Newbie: Combn and scripting
In-Reply-To: <OF0E065CE6.E774B44B-ONCA2572AB.00201EA9-CA2572AB.002090BC@n212.csv.au>
References: <OF0E065CE6.E774B44B-ONCA2572AB.00201EA9-CA2572AB.002090BC@n212.csv.au>
Message-ID: <9CD929C7-02AC-4306-8F60-84E157AC3247@virginia.edu>

x <- matrix(c('a', 'b', 'c', 'd', 1:4, 'g', 3, 6, 't'), nrow = 3,  
byrow = T)
comb <- vector('list', 3)
for(i in 1:3) comb[[i]] <- combn(x[i,], 3)

On Mar 27, 2007, at 1:55 AM, Zamikhaya.Mbalu at dhs.vic.gov.au wrote:

> I have just installed my R 2.4 (windows) as a test trying to load a  
> data
> frame and run combn() for each line into another file. How do I do  
> this?
>
> data.csv:
> a,b,c,d
> 1,2,3.4
> g,3,6,t
> etc
>
> x=data.csv, m=3



_____________________________
Professor Michael Kubovy
University of Virginia
Department of Psychology
USPS:     P.O.Box 400400    Charlottesville, VA 22904-4400
Parcels:    Room 102        Gilmer Hall
         McCormick Road    Charlottesville, VA 22903
Office:    B011    +1-434-982-4729
Lab:        B019    +1-434-982-4751
Fax:        +1-434-982-4766
WWW:    http://www.people.virginia.edu/~mk9y/


From ramasamy at cancer.org.uk  Tue Mar 27 12:39:41 2007
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 27 Mar 2007 11:39:41 +0100
Subject: [R] Help for looping
In-Reply-To: <20070326222917.79og8vata840kw84@webmail.iu.edu>
References: <20070326222917.79og8vata840kw84@webmail.iu.edu>
Message-ID: <4608F46D.4080500@cancer.org.uk>

Please try to give a simple reproducible example and simplify your codes 
a bit if you want to get useful responses.

For example, you say your data is a matrix of 1000*30, where I presume 
the matrix has 1000 rows and 30 columns. If so EMP <- data[,378:392] 
does not make sense.

Perhaps you might be interested in knn() in the class package.

Regards, Adai




tkobayas at indiana.edu wrote:
> Rusers:
> 
> I have tried to minimize computing times by taking advanage of 
> lapply(). My data is a 1000*30 matrix and the distance matrix was 
> created with dist(). What I am trying to do is to compute the standard 
> distances using the frequencies attached to the nearest negibors of n 
> reference zones. So I will have 1000 standard distances, and would like 
> to see the frequency distribution of the standard distances.
> 
> # Convert decimal degrees into UTM miles
> x<-(data[,1]-58277.194363)*0.000621
> y<-(data[,2]-4414486.03135)*0.000621
> 
> # Combine x y for computing distances
> coords<-cbind(x,y)
> pts<-length(data)
> 
> # Subset housing data and employment data
> RES<-data[,3:17]
> EMP<-data[,378:392]
> 
> # Combine all the subdata as D
> D<-cbind(coords,RES,EMP)
> 
> cases<-ncol(D)-ncol(coords)
> 
> # Create a threshold bandwidth for defining the nearest neighbors
> thrs<-seq(0,35,by=1)
> 
> SDTAZ<-rep(list(matrix(,nrow(D),length(thrs))),cases)
> 
> 
> for (j in 1:nrow(D))
> for (k in 1:length(thrs))
> for (l in 1:cases)
> {
> {
> {
> 
> SDTAZ[[l]][j,k]<-
> sqrt(
>    sum(
> 	(D[as.vector(which(dis[j,]<=thrs[k])),l+2]-D[j,l+2]-
>           min(D[as.vector(which(dis[j,]<=thrs[k])),l+2]-D[j,l+2])+1)*
>          (
> 	   (dis[j,as.vector(which(dis[j,]<=thrs[k]))])^2
>          )
>       )
> 
> 	/sum(D[as.vector(which(dis[j,]<=thrs[k])),l+2]-D[j,l+2]-
>          min(D[as.vector(which(dis[j,]<=thrs[k])),l+2]-D[j,l+2])+1)
> 		)
> }
> }
> }
> 
> I think that within this nested loop, I should use lapply() but I ended 
> up getting different values.... I appreciate if someone could kindly 
> help me.
> 
> Thank you very much.
> ------------------------------------
> Takatsugu Kobayashi
> PhD Candidate
> Indiana University, Dept. Geography
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
>


From Luisr at frs.fo  Tue Mar 27 12:54:19 2007
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Tue, 27 Mar 2007 11:54:19 +0100
Subject: [R] gam parameter predictions --Sorry for double posting
Message-ID: <s60905f1.023@ffdata.setur.fo>

R-help,

Sorry for posting again the same question (dated 26-03-2007) but 
all my mails have been sent to the recycle bin without possibility
of recovering and thus I don't know if anyone has answer my query.

Here is the original message:

I'm applying a gam model (package mgcv) to predict
relative abundances of a fish species.

The covariates are year, month, vessel and statistical rectangle.


The model looks like this:

g1 <- gam(log(cpue) ~  s(rekt1) + s(year) + s(mon) + s(reg1), data =
dataTest)


Once the model is fitted to the data I want to get the mean model
estimates by year.

I do the following:

obsPred <- data.frame(year = dataTest$year, pred = predict(g1, type =
"response"))

gamFit <- tapply(obsPred$pred, list(year = obsPred$ar), mean)



Is this correct?



Thanks in advance


> version
               _                           
platform       i386-pc-mingw32             
arch           i386                        
os             mingw32                     
system         i386, mingw32               
status                                     
major          2                           
minor          4.1                         
year           2006                        
month          12                          
day            18                          
svn rev        40228                       
language       R                           
version.string R version 2.4.1 (2006-12-18)


From jrkrideau at yahoo.ca  Tue Mar 27 12:56:26 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 27 Mar 2007 06:56:26 -0400 (EDT)
Subject: [R] Newbie: Combn and scripting
In-Reply-To: <OF0E065CE6.E774B44B-ONCA2572AB.00201EA9-CA2572AB.002090BC@n212.csv.au>
Message-ID: <526521.11890.qm@web32802.mail.mud.yahoo.com>

I think Michael K has covered the comb issue.  
Have a look at 
 ?read.table (you want read.csv for ease of use)
 ?save 

--- Zamikhaya.Mbalu at dhs.vic.gov.au wrote:

> 
> 
> Hello All,
> 
> I have just installed my R 2.4 (windows) as a test
> trying to load a data
> frame and run combn() for each line into another
> file. How do I do this?
> 
> data.csv:
> a,b,c,d
> 1,2,3.4
> g,3,6,t
> etc
> 
> x=data.csv, m=3
> 
> Thank you
> 
> Zam
> 
> 
> 
>
_________________________________________________________________________________
> 
> This email contains confidential information
> intended only for the person named above and may be
> subject to legal privilege. If you are not the
> intended recipient, any disclosure, copying or use
> of this information is prohibited. The Department
> provides no guarantee that this communication is
> free of virus or that it has not been intercepted or
> interfered with. If you have received this email in
> error or have any other concerns regarding its
> transmission, please notify
> Postmaster at dhs.vic.gov.au
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From vikas at mail.jnu.ac.in  Tue Mar 27 13:47:24 2007
From: vikas at mail.jnu.ac.in (Vikas Rawal)
Date: Tue, 27 Mar 2007 17:17:24 +0530
Subject: [R] problem with installation of packages
Message-ID: <460951A40200009E00005EBF@mail.jnu.ac.in>

Dear Professor Ripley,

Your diagnosis was absolutely right and I have been able to solve my
problem. I had looked up the archives and R Faq for similar problems
before posting, but they all pertained to windows. In fact you had
replied to the posts pointing to RW Faq. But these all pertained to
windows and the same issue is not discussed for some reason in Faq for
linux.

> That is an issue specific to your setup, but why is someone mailing
from 
> an Indian address using a mirror in the USA?  (Your signature is
missing: 
> please see the R posting guide.)

I used the USA mirror because the Indian mirrors do not show up in the
list that pops up. I have always wondered why. 

> How do you _know_ 'The http_proxy is correctly set'?: no evidence is 
> provided here and proxy problems remain the most likely cause.  You
can 
> debug your setup via options(internet.info=0), but we cannot do it for

> you.

What led me to believe the proxy was set correctly was that I was able
to download the packages using wget. My instinct was that wget and R
would both pick up the proxy settings from the same system variable.
There are obviously things about this part that I do not understand.

> >This is a machine running debian sarge. The http_proxy is correctly
> >set. I am able to download the package by using wget. I also tried
> >starting R with (R http_proxy-http://my-proxy/). That does not help
> >either.
> 
> As it is syntactically invalid, it will not.  Perhaps
> 
> $ http_proxy="http://my-proxy/" R

This indeed solved the problem. Thanks very much.

Vikas


From s.wood at bath.ac.uk  Tue Mar 27 13:29:46 2007
From: s.wood at bath.ac.uk (Simon Wood)
Date: Tue, 27 Mar 2007 12:29:46 +0100
Subject: [R] gam parameter predictions --Sorry for double posting
In-Reply-To: <s60905f1.023@ffdata.setur.fo>
References: <s60905f1.023@ffdata.setur.fo>
Message-ID: <200703271229.46755.s.wood@bath.ac.uk>

Looks ok to me, provided that you want averages (on log scale) taken over the 
the observed covariate values. If you want variances for the means you could 
always do something like the following...

Xa <- model.matrix(~as.factor(year)-1)
Xa <- t(Xa)/colSums(Xa)  ## Xa%*%fitted(g1) gives required averages

Xp <- predict(g1,type="lpmatrix") ## Xp%*%coef(g1) gives fitted values
Xm <- Xa%*%Xp  ## now Xm%*%coef(g1) gives required averages

yearly.means <- Xm%*%coef(g1)
ym.cov <- Xm%*%vcov(g1,freq=FALSE)%*%t(Xm) ## cov matrix of yearly means

(help file for predict.gam has a bit more information on this sort of thing, 
or section 5.2.6 of my book)

best,
Simon

-- 
> Simon Wood, Mathematical Sciences, University of Bath, Bath, BA2 7AY UK
> +44 1225 386603  www.maths.bath.ac.uk/~sw283 

>
> The covariates are year, month, vessel and statistical rectangle.
>
>
> The model looks like this:
>
> g1 <- gam(log(cpue) ~  s(rekt1) + s(year) + s(mon) + s(reg1), data =
> dataTest)
>
>
> Once the model is fitted to the data I want to get the mean model
> estimates by year.
>
> I do the following:
>
> obsPred <- data.frame(year = dataTest$year, pred = predict(g1, type =
> "response"))
>
> gamFit <- tapply(obsPred$pred, list(year = obsPred$ar), mean)
>
>
>
> Is this correct?
>
>
>
> Thanks in advance
>
> > version
>
>                _
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          4.1
> year           2006
> month          12
> day            18
> svn rev        40228
> language       R
> version.string R version 2.4.1 (2006-12-18)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.


From john_d_mchenry at yahoo.com  Tue Mar 27 13:52:03 2007
From: john_d_mchenry at yahoo.com (John McHenry)
Date: Tue, 27 Mar 2007 04:52:03 -0700 (PDT)
Subject: [R] Source Code for zoo?
Message-ID: <238052.68161.qm@web35412.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070327/74a0165d/attachment.pl 

From berwin at maths.uwa.edu.au  Tue Mar 27 14:07:08 2007
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Tue, 27 Mar 2007 20:07:08 +0800
Subject: [R] Source Code for zoo?
In-Reply-To: <238052.68161.qm@web35412.mail.mud.yahoo.com>
References: <238052.68161.qm@web35412.mail.mud.yahoo.com>
Message-ID: <20070327200708.3a733b15@berwin5>

G'day John,

On Tue, 27 Mar 2007 04:52:03 -0700 (PDT)
John McHenry <john_d_mchenry at yahoo.com> wrote:
 
> I downloaded the source code for the package zoo (zoo_1.2-2.tgz from
> CRAN).

Are you sure?  How did you download it and from where exactly?

As far as I know, and as far as I can tell, CRAN has all source
packages in tar.gz format, not in tgz format.

> When I gunzip and untar the file I can't find the R / C /
> FORTRAN code for the library. 

There is no C or FORTRAN code in that *package*, only R code.  And it
if I download zoo_1.2-2.tar.gz from my preferred mirror, unpack and
untar the file, I can see the R code in the directory R.

> In the R directory under zoo, the listing is for 3 files:
> 
> zoo
> zoo.rdb
> zoo.rdx
> 
> Is the code encoded in one of these files?

Yes, and it looks very much as if you have downloaded a binary version
of that package and not the source version.

> I've had this problem with other packages too, trying to browse the
> source. Am I missing something obvious? 

That you are downloading binary distributions of packages instead of
their source distribution? :-)

Cheers,

	Berwin


From geir.bolstad at bio.ntnu.no  Tue Mar 27 11:35:07 2007
From: geir.bolstad at bio.ntnu.no (Geir H Bolstad)
Date: Tue, 27 Mar 2007 11:35:07 +0200
Subject: [R] Solving a system of nonlinear equations involving weighted
	parameters
Message-ID: <4608E54B.1020000@bio.ntnu.no>

Hi, I'm trying to solve the following system of nonlinear equations

        P1 - F2 = x[1] + (1/2) * x[3] * x[1]^2
        P2 - F2 = x[2] + (1/2) * x [3] * x[2]^2
        F1 - F2 = -(1/2) * x[1] - (1/2) * x[2] + (1/8) * x [3] * (x[1] + 
x[2])^2
        B1 - F2 = (1/4) * x[1] - (1/4) * x[2] + (1/16) * x[3] * (x[1] - 
x[2])^2
        B2 - F2 = (1/4) * x[1] + (1/4) * x[2] + (1/16) * x[3] * (x[1] - 
x[2])^2

where P1, P2, F1, F2, B1, and B2 are weighted parameters. I have tried 
to use

f <- function(x){
        (x[1]+(1/2)*x[3]*x[1]^2-P1+F2)^2
        +(x[2]+(1/2)*x[3]*x[2]^2-P2+F2)^2
        +(-(1/2)*x[1]-(1/2)*x[2]+(1/8)*x[3]*(x[1]+x[2])^2-F1+F2)^2
        +((1/4)*x[1]-(1/4)*x[2]+(1/16)*x[3]*(x[1]-x[2])^2-B1+F2)^2
        +(-(1/4)*x[1]+(1/4)*x[2]+(1/16)*x[3]*(x[1]-x[2])^2-B2+F2)^2
}

 optim(c(P1-F2,P2-F2,0), f)
 nlm(f, c(P1-F2,P2-F2,0))

but neither optim() or nlm() allows for weighted parameters. In addition 
I need standard errors for the estimates of x[1], x[2] and x[3].

Thanks
Geir


From manuel.martin at orleans.inra.fr  Tue Mar 27 14:37:12 2007
From: manuel.martin at orleans.inra.fr (manuel.martin)
Date: Tue, 27 Mar 2007 14:37:12 +0200
Subject: [R] catching a console output
Message-ID: <46090FF8.2010300@orleans.inra.fr>

Hello all,
I cannot figure out how to catch the console output from a function 
which does not return anything but a console output, any hints?

Thank you in advance,   Manuel

-- 
----------------------------------------------------
Manuel Martin - Unit? InfoSol
INRA
INSTITUT NATIONAL RECHERCHE AGRONOMIQUE
2163 AVENUE DE LA POMME DE PIN
BP 20619 ARDON
45166 OLIVET CEDEX
T?l. : 02 38 41 48 21


From frank.preiswerk at stud.unibas.ch  Tue Mar 27 14:43:35 2007
From: frank.preiswerk at stud.unibas.ch (Frank Preiswerk)
Date: Tue, 27 Mar 2007 14:43:35 +0200
Subject: [R] snow parLapply standard output
Message-ID: <63d2cca90703270543q667a9626o7bee23af91d89dfb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070327/36f19c53/attachment.pl 

From john_d_mchenry at yahoo.com  Tue Mar 27 14:46:57 2007
From: john_d_mchenry at yahoo.com (John McHenry)
Date: Tue, 27 Mar 2007 05:46:57 -0700 (PDT)
Subject: [R] Source Code for zoo?
In-Reply-To: <20070327200708.3a733b15@berwin5>
Message-ID: <581395.69732.qm@web35402.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070327/12c094d1/attachment.pl 

From Luisr at frs.fo  Tue Mar 27 15:15:25 2007
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Tue, 27 Mar 2007 14:15:25 +0100
Subject: [R] impose points on lattice plot
Message-ID: <s6092705.029@ffdata.setur.fo>


R-help,

I'm using the lattice package to plot 2 variables (vekt ~ aldur)
conditioned to a third (kyn * 2 categories).

I use the following:

xyplot(vekt ~ aldur|kyn, , data = sexSu)


I want to superimpose the average(vekt) by 'aldur' 
conditioned to kyn by using something like:

xyplot(vekt~aldur|kyn, subset = aldur <= 12
, data = sexSu, panel = function(x, y)
       {
       panel.xyplot(x, y)
       panel.points(x,mean(y),col=2,cex=2 )
       }) 


but th output is just a horozontal line ( the average of 'vekt')
in both panels I guess)

How can be done?


Thanks in advance


> version
               _                           
platform       i386-pc-mingw32             
arch           i386                        
os             mingw32                     
system         i386, mingw32               
status                                     
major          2                           
minor          4.1                         
year           2006                        
month          12                          
day            18                          
svn rev        40228                       
language       R                           
version.string R version 2.4.1 (2006-12-18)
>


From Corinna.Schmitt at igb.fraunhofer.de  Tue Mar 27 15:36:16 2007
From: Corinna.Schmitt at igb.fraunhofer.de (Schmitt, Corinna)
Date: Tue, 27 Mar 2007 15:36:16 +0200
Subject: [R] Import of a workspace into R
Message-ID: <8B7B0FD99E8AF541A21609104D19615882E9D4@izs-xchg01.izs.fraunhofer.de>

Hallo,

can anyone tell me how I can import a stored workspace of another
program into R. I want to use the variables together with their values
in new R programmed functions.

I could not really find a solution in the paper "R Data Export/Import"

Thanks,
Corinna


From daniel at danielberg.no  Tue Mar 27 15:56:42 2007
From: daniel at danielberg.no (Daniel Berg)
Date: Tue, 27 Mar 2007 15:56:42 +0200
Subject: [R] Replacement in an expression - can't use parse()
Message-ID: <f13f1c9c0703270656t56dd736axb46b334575b136f5@mail.gmail.com>

Dear all,

Suppose I have a very long expression e. Lets assume, for simplicity, that it is

e = expression(u1+u2+u3)

Now I wish to replace u2 with x and u3 with 1. I.e. the 'new'
expression, after replacement, should be:

> e
expression(u1+x+1)

My question is how to do the replacement?

I have tried using:

> e = parse(text=gsub("u2","x",e))
> e = parse(text=gsub("u3",1,e))

Even though this works fine in this simple example, the use of parse
when e is very long will fail since parse has a maximum line length
and will cut my expressions. I need to keep mode(e)=expression since I
will use e further in symbolic derivation and division.

Any suggestions are most welcome.

Thank you.

Regards,
Daniel Berg


From wwwhsd at gmail.com  Tue Mar 27 16:06:06 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Tue, 27 Mar 2007 11:06:06 -0300
Subject: [R] Replacement in an expression - can't use parse()
In-Reply-To: <f13f1c9c0703270656t56dd736axb46b334575b136f5@mail.gmail.com>
References: <f13f1c9c0703270656t56dd736axb46b334575b136f5@mail.gmail.com>
Message-ID: <da79af330703270706j33919e4eq762d507534e7f950@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070327/7a0b0939/attachment.pl 

From charles.dupont at vanderbilt.edu  Tue Mar 27 16:09:27 2007
From: charles.dupont at vanderbilt.edu (Charles Dupont)
Date: Tue, 27 Mar 2007 09:09:27 -0500
Subject: [R] Prefered date and date/time classes
Message-ID: <46092597.4060407@vanderbilt.edu>

What are the preferred date, and data/time classes for R?

Thanks

Charles Dupont


-- 
Charles Dupont	Computer System Analyst		School of Medicine
		Department of Biostatistics	Vanderbilt University


From sundar.dorai-raj at pdf.com  Tue Mar 27 16:14:44 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 27 Mar 2007 07:14:44 -0700
Subject: [R] impose points on lattice plot
In-Reply-To: <s6092705.029@ffdata.setur.fo>
References: <s6092705.029@ffdata.setur.fo>
Message-ID: <460926D4.1040901@pdf.com>


Luis Ridao Cruz said the following on 3/27/2007 6:15 AM:
> R-help,
> 
> I'm using the lattice package to plot 2 variables (vekt ~ aldur)
> conditioned to a third (kyn * 2 categories).
> 
> I use the following:
> 
> xyplot(vekt ~ aldur|kyn, , data = sexSu)
> 
> 
> I want to superimpose the average(vekt) by 'aldur' 
> conditioned to kyn by using something like:
> 
> xyplot(vekt~aldur|kyn, subset = aldur <= 12
> , data = sexSu, panel = function(x, y)
>        {
>        panel.xyplot(x, y)
>        panel.points(x,mean(y),col=2,cex=2 )
>        }) 
> 
> 
> but th output is just a horozontal line ( the average of 'vekt')
> in both panels I guess)
> 
> How can be done?
> 
> 


An working example would be nice. But here's one possible solution if I 
understand your question correctly:

xyplot(vekt~aldur|kyn, subset = aldur <= 12
, data = sexSu, panel = function(x, y)
        {
        panel.xyplot(x, y)
        mx <- sort(unique(x))
        my <- tapply(y, x, mean)
        o <- order(mx)
        panel.points(mx[o],my[o],col=2,cex=2 )
        })


but th output

> Thanks in advance
> 
> 
>> version
>                _                           
> platform       i386-pc-mingw32             
> arch           i386                        
> os             mingw32                     
> system         i386, mingw32               
> status                                     
> major          2                           
> minor          4.1                         
> year           2006                        
> month          12                          
> day            18                          
> svn rev        40228                       
> language       R                           
> version.string R version 2.4.1 (2006-12-18)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Tue Mar 27 16:16:25 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 27 Mar 2007 10:16:25 -0400
Subject: [R] Prefered date and date/time classes
In-Reply-To: <46092597.4060407@vanderbilt.edu>
References: <46092597.4060407@vanderbilt.edu>
Message-ID: <971536df0703270716s3a2a6661k3442420238489ae7@mail.gmail.com>

On 3/27/07, Charles Dupont <charles.dupont at vanderbilt.edu> wrote:
> What are the preferred date, and data/time classes for R?

See the help desk article in R News 4/1.


From luke at stat.uiowa.edu  Tue Mar 27 16:18:21 2007
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Tue, 27 Mar 2007 09:18:21 -0500 (CDT)
Subject: [R] snow parLapply standard output
In-Reply-To: <63d2cca90703270543q667a9626o7bee23af91d89dfb@mail.gmail.com>
References: <63d2cca90703270543q667a9626o7bee23af91d89dfb@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0703270910580.27522@nokomis.stat.uiowa.edu>

On Tue, 27 Mar 2007, Frank Preiswerk wrote:

> I am slightly confused by the way the standard output is redirected in a R
> snow cluster environment.
> I am using parLapply from the snow package to execute a function on my
> MPI/LAM cluster. How can I redirect standard output (produced using "cat")
> from this function back to the terminal where I invoked it? I intend to
> transmit some status information in advance to the final result of the
> function. I investigated the chain of functions called by parLapply and it
> seems that snow is designed to just retrieve the final result of the
> computation.

The R level only deals with final values.  There is currently no means
within snow to communicate status information back to the master.

By default output is redirected to /dev/null.  You can provide an
alternate destination with the outfile option and monitor those files
from another process.

Best,

luke

>
> Thanks in advance for any hints,
> Frank Preiswerk
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From ggrothendieck at gmail.com  Tue Mar 27 16:22:26 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 27 Mar 2007 10:22:26 -0400
Subject: [R] Replacement in an expression - can't use parse()
In-Reply-To: <f13f1c9c0703270656t56dd736axb46b334575b136f5@mail.gmail.com>
References: <f13f1c9c0703270656t56dd736axb46b334575b136f5@mail.gmail.com>
Message-ID: <971536df0703270722u46d7f78bg3fbe8b31c8c3e885@mail.gmail.com>

Try substitute:

e <- expression(u1 + u2 + u3)
L <- list(u2 = as.name("x"), u3 = 1)
as.expression(do.call(substitute, list(as.call(e), L))[[1]])


On 3/27/07, Daniel Berg <daniel at danielberg.no> wrote:
> Dear all,
>
> Suppose I have a very long expression e. Lets assume, for simplicity, that it is
>
> e = expression(u1+u2+u3)
>
> Now I wish to replace u2 with x and u3 with 1. I.e. the 'new'
> expression, after replacement, should be:
>
> > e
> expression(u1+x+1)
>
> My question is how to do the replacement?
>
> I have tried using:
>
> > e = parse(text=gsub("u2","x",e))
> > e = parse(text=gsub("u3",1,e))
>
> Even though this works fine in this simple example, the use of parse
> when e is very long will fail since parse has a maximum line length
> and will cut my expressions. I need to keep mode(e)=expression since I
> will use e further in symbolic derivation and division.
>
> Any suggestions are most welcome.
>
> Thank you.
>
> Regards,
> Daniel Berg
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From maitra at iastate.edu  Tue Mar 27 16:22:39 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Tue, 27 Mar 2007 09:22:39 -0500
Subject: [R] catching a console output
In-Reply-To: <46090FF8.2010300@orleans.inra.fr>
References: <46090FF8.2010300@orleans.inra.fr>
Message-ID: <20070327092239.5106ef82@triveni.stat.iastate.edu>

see ?sink or ?capture.output -- they may do your task.

hth,
ranjan

On Tue, 27 Mar 2007 14:37:12 +0200 "manuel.martin" <manuel.martin at orleans.inra.fr> wrote:

> Hello all,
> I cannot figure out how to catch the console output from a function 
> which does not return anything but a console output, any hints?
> 
> Thank you in advance,   Manuel
> 
> -- 
> ----------------------------------------------------
> Manuel Martin - Unit? InfoSol
> INRA
> INSTITUT NATIONAL RECHERCHE AGRONOMIQUE
> 2163 AVENUE DE LA POMME DE PIN
> BP 20619 ARDON
> 45166 OLIVET CEDEX
> T?l. : 02 38 41 48 21
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sundar.dorai-raj at pdf.com  Tue Mar 27 16:24:46 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 27 Mar 2007 07:24:46 -0700
Subject: [R] Replacement in an expression - can't use parse()
In-Reply-To: <f13f1c9c0703270656t56dd736axb46b334575b136f5@mail.gmail.com>
References: <f13f1c9c0703270656t56dd736axb46b334575b136f5@mail.gmail.com>
Message-ID: <4609292E.1080805@pdf.com>



Daniel Berg said the following on 3/27/2007 6:56 AM:
> Dear all,
> 
> Suppose I have a very long expression e. Lets assume, for simplicity, that it is
> 
> e = expression(u1+u2+u3)
> 
> Now I wish to replace u2 with x and u3 with 1. I.e. the 'new'
> expression, after replacement, should be:
> 
>> e
> expression(u1+x+1)
> 
> My question is how to do the replacement?
> 
> I have tried using:
> 
>> e = parse(text=gsub("u2","x",e))
>> e = parse(text=gsub("u3",1,e))
> 
> Even though this works fine in this simple example, the use of parse
> when e is very long will fail since parse has a maximum line length
> and will cut my expressions. I need to keep mode(e)=expression since I
> will use e further in symbolic derivation and division.
> 
> Any suggestions are most welcome.
> 
> Thank you.
> 
> Regards,
> Daniel Berg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


Take a look at ?substitute (or ?bquote)

 > as.expression(substitute(u1+u2+u3, list(u1 = quote(x))))
expression(x + u2 + u3)
 > as.expression(bquote(u1+u2+.(u3), list(u3 = 1)))
expression(u1 + u2 + 1)

HTH,

--sundar


From zhongmiao at gmail.com  Tue Mar 27 16:26:50 2007
From: zhongmiao at gmail.com (zhongmiao wang)
Date: Tue, 27 Mar 2007 08:26:50 -0600
Subject: [R] what is the difference between survival analysis and logistic
	regression with a timing variable?
Message-ID: <1def27350703270726r762b5a8ao8d4e109c800966d5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070327/7fa871a6/attachment.pl 

From petr.pikal at precheza.cz  Tue Mar 27 16:27:05 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 27 Mar 2007 16:27:05 +0200
Subject: [R] Import of a workspace into R
In-Reply-To: <8B7B0FD99E8AF541A21609104D19615882E9D4@izs-xchg01.izs.fraunhofer.de>
Message-ID: <460945D9.29620.1EAC40C@localhost>

Hi

Do you know what is the program from which you have stored workspace? 

If you mean that you have some stored R data you shall probably 
consult save/load help page and/or other possible means of data 
loading like dput/dget, write/read and maybe some other.

Regards
Petr
 

On 27 Mar 2007 at 15:36, Schmitt, Corinna wrote:

Date sent:      	Tue, 27 Mar 2007 15:36:16 +0200
From:           	"Schmitt, Corinna" <Corinna.Schmitt at igb.fraunhofer.de>
To:             	<r-help at stat.math.ethz.ch>
Subject:        	[R] Import of a workspace into R

> Hallo,
> 
> can anyone tell me how I can import a stored workspace of another
> program into R. I want to use the variables together with their values
> in new R programmed functions.
> 
> I could not really find a solution in the paper "R Data Export/Import"
> 
> Thanks,
> Corinna
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From christos at nuverabio.com  Tue Mar 27 16:29:36 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Tue, 27 Mar 2007 10:29:36 -0400
Subject: [R] Replacement in an expression - can't use parse()
In-Reply-To: <f13f1c9c0703270656t56dd736axb46b334575b136f5@mail.gmail.com>
References: <f13f1c9c0703270656t56dd736axb46b334575b136f5@mail.gmail.com>
Message-ID: <001d01c7707c$59e64fb0$0e010a0a@headquarters.silicoinsights>

A way to do this is through substitute:

e1 <- substitute(expression(u1 + u2 + u3), list(u2=quote(x), u3=1))

But I am not sure whether you will run into similar limitations regarding
the length of the expression to be substituted. 

-Christos

Christos Hatzis, Ph.D.
Nuvera Biosciences, Inc.
400 West Cummings Park
Suite 5350
Woburn, MA 01801
Tel: 781-938-3830
www.nuverabio.com
 


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Daniel Berg
> Sent: Tuesday, March 27, 2007 9:57 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Replacement in an expression - can't use parse()
> 
> Dear all,
> 
> Suppose I have a very long expression e. Lets assume, for 
> simplicity, that it is
> 
> e = expression(u1+u2+u3)
> 
> Now I wish to replace u2 with x and u3 with 1. I.e. the 'new'
> expression, after replacement, should be:
> 
> > e
> expression(u1+x+1)
> 
> My question is how to do the replacement?
> 
> I have tried using:
> 
> > e = parse(text=gsub("u2","x",e))
> > e = parse(text=gsub("u3",1,e))
> 
> Even though this works fine in this simple example, the use 
> of parse when e is very long will fail since parse has a 
> maximum line length and will cut my expressions. I need to 
> keep mode(e)=expression since I will use e further in 
> symbolic derivation and division.
> 
> Any suggestions are most welcome.
> 
> Thank you.
> 
> Regards,
> Daniel Berg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From dimitris.rizopoulos at med.kuleuven.be  Tue Mar 27 16:27:49 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 27 Mar 2007 16:27:49 +0200
Subject: [R] Replacement in an expression - can't use parse()
References: <f13f1c9c0703270656t56dd736axb46b334575b136f5@mail.gmail.com>
Message-ID: <010101c7707c$1884cb00$0540210a@www.domain>

you could try something like the following (untested):

new.e <- eval(substitute(expression(u1+u2+u3), list(u2 = x, u3 = 1)))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Daniel Berg" <daniel at danielberg.no>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, March 27, 2007 3:56 PM
Subject: [R] Replacement in an expression - can't use parse()


> Dear all,
>
> Suppose I have a very long expression e. Lets assume, for 
> simplicity, that it is
>
> e = expression(u1+u2+u3)
>
> Now I wish to replace u2 with x and u3 with 1. I.e. the 'new'
> expression, after replacement, should be:
>
>> e
> expression(u1+x+1)
>
> My question is how to do the replacement?
>
> I have tried using:
>
>> e = parse(text=gsub("u2","x",e))
>> e = parse(text=gsub("u3",1,e))
>
> Even though this works fine in this simple example, the use of parse
> when e is very long will fail since parse has a maximum line length
> and will cut my expressions. I need to keep mode(e)=expression since 
> I
> will use e further in symbolic derivation and division.
>
> Any suggestions are most welcome.
>
> Thank you.
>
> Regards,
> Daniel Berg
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From P.Dalgaard at biostat.ku.dk  Tue Mar 27 16:30:11 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 27 Mar 2007 16:30:11 +0200
Subject: [R] Replacement in an expression - can't use parse()
In-Reply-To: <f13f1c9c0703270656t56dd736axb46b334575b136f5@mail.gmail.com>
References: <f13f1c9c0703270656t56dd736axb46b334575b136f5@mail.gmail.com>
Message-ID: <46092A73.6020302@biostat.ku.dk>

Daniel Berg wrote:
> Dear all,
>
> Suppose I have a very long expression e. Lets assume, for simplicity, that it is
>
> e = expression(u1+u2+u3)
>
> Now I wish to replace u2 with x and u3 with 1. I.e. the 'new'
> expression, after replacement, should be:
>
>   
>> e
>>     
> expression(u1+x+1)
>
> My question is how to do the replacement?
>
> I have tried using:
>
>   
>> e = parse(text=gsub("u2","x",e))
>> e = parse(text=gsub("u3",1,e))
>>     
>
> Even though this works fine in this simple example, the use of parse
> when e is very long will fail since parse has a maximum line length
> and will cut my expressions. I need to keep mode(e)=expression since I
> will use e further in symbolic derivation and division.
>
> Any suggestions are most welcome.
>   
The short answer is substitute().

However, this is not entirely trivial to apply if you have your
expression already inside an expression() object.

The easy thing to do is

> substitute(u1+u2+u3, list(u2=quote(x),u3=1))
u1 + x + 1

but notice that this "autoquotes" the first argument, so

> substitute(e, list(u2=quote(x),u3=1))
e

which is pretty much useless.

(Arguably it would have been a better design to avoid this feature and
require substitute(quote(.....)....) for the former case.)

The way around this is to add a further layer of substitute() to insert
the value of e:

>
eval(substitute(substitute(call,list(u2=quote(x),u3=1)),list(call=e[[1]])))
u1 + x + 1

Notice that substitute will not go inside expression objects, so we need
to extract the mode "call" object using e[[1]]. Also, the result is
"call" not "expression". You may need an as.expression construct around
the result to get exactly what you asked.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From petr.pikal at precheza.cz  Tue Mar 27 16:32:09 2007
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 27 Mar 2007 16:32:09 +0200
Subject: [R] Prefered date and date/time classes
In-Reply-To: <46092597.4060407@vanderbilt.edu>
Message-ID: <46094709.32251.1EF6828@localhost>

Hi

On 27 Mar 2007 at 9:09, Charles Dupont wrote:

Date sent:      	Tue, 27 Mar 2007 09:09:27 -0500
From:           	Charles Dupont <charles.dupont at vanderbilt.edu>
Organization:   	Vanderbilt University; Department of Biostatistics 
To:             	r-help at stat.math.ethz.ch
Subject:        	[R] Prefered date and date/time classes
Send reply to:  	charles.dupont at vanderbilt.edu
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>

> What are the preferred date, and data/time classes for R?

It is probably a personal choice. You can use POSIX, chron or other 
options. They are nicely described in RNEWS 4-1 in section Help Desk.

Regards
Petr


> 
> Thanks
> 
> Charles Dupont
> 
> 
> -- 
> Charles Dupont	Computer System Analyst		School of Medicine
>   Department of Biostatistics	Vanderbilt University
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.

Petr Pikal
petr.pikal at precheza.cz


From marc_schwartz at comcast.net  Tue Mar 27 16:32:33 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 27 Mar 2007 09:32:33 -0500
Subject: [R] Replacement in an expression - can't use parse()
In-Reply-To: <f13f1c9c0703270656t56dd736axb46b334575b136f5@mail.gmail.com>
References: <f13f1c9c0703270656t56dd736axb46b334575b136f5@mail.gmail.com>
Message-ID: <1175005953.5153.5.camel@Bellerophon>

On Tue, 2007-03-27 at 15:56 +0200, Daniel Berg wrote:
> Dear all,
> 
> Suppose I have a very long expression e. Lets assume, for simplicity, that it is
> 
> e = expression(u1+u2+u3)
> 
> Now I wish to replace u2 with x and u3 with 1. I.e. the 'new'
> expression, after replacement, should be:
> 
> > e
> expression(u1+x+1)
> 
> My question is how to do the replacement?
> 
> I have tried using:
> 
> > e = parse(text=gsub("u2","x",e))
> > e = parse(text=gsub("u3",1,e))
> 
> Even though this works fine in this simple example, the use of parse
> when e is very long will fail since parse has a maximum line length
> and will cut my expressions. I need to keep mode(e)=expression since I
> will use e further in symbolic derivation and division.
> 
> Any suggestions are most welcome.
> 
> Thank you.
> 
> Regards,
> Daniel Berg

Here is one possibility, depending upon how complicated the
substitutions end up being:

e <- expression(u1+u2+u3)

TMP <- unlist(strsplit(as.character(e), " "))

> TMP
[1] "u1" "+"  "u2" "+"  "u3"

TMP <- gsub("u2", "x", TMP)

TMP <- gsub("u3", "i", TMP)

> TMP
[1] "u1" "+"  "x"  "+"  "i" 

> as.expression(paste(TMP, collapse = ""))
expression("u1+x+i")

HTH,

Marc Schwartz


From marc_schwartz at comcast.net  Tue Mar 27 16:33:41 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 27 Mar 2007 09:33:41 -0500
Subject: [R] catching a console output
In-Reply-To: <46090FF8.2010300@orleans.inra.fr>
References: <46090FF8.2010300@orleans.inra.fr>
Message-ID: <1175006021.5153.6.camel@Bellerophon>

On Tue, 2007-03-27 at 14:37 +0200, manuel.martin wrote:
> Hello all,
> I cannot figure out how to catch the console output from a function 
> which does not return anything but a console output, any hints?
> 
> Thank you in advance,   Manuel


See ?sink and ?capture.output

HTH,

Marc Schwartz


From marc_schwartz at comcast.net  Tue Mar 27 16:36:41 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 27 Mar 2007 09:36:41 -0500
Subject: [R] Import of a workspace into R
In-Reply-To: <8B7B0FD99E8AF541A21609104D19615882E9D4@izs-xchg01.izs.fraunhofer.de>
References: <8B7B0FD99E8AF541A21609104D19615882E9D4@izs-xchg01.izs.fraunhofer.de>
Message-ID: <1175006201.5153.10.camel@Bellerophon>

On Tue, 2007-03-27 at 15:36 +0200, Schmitt, Corinna wrote:
> Hallo,
> 
> can anyone tell me how I can import a stored workspace of another
> program into R. I want to use the variables together with their values
> in new R programmed functions.
> 
> I could not really find a solution in the paper "R Data Export/Import"
> 
> Thanks,
> Corinna

You will need to provide more information regarding the data source.

Marc Schwartz


From ripley at stats.ox.ac.uk  Tue Mar 27 16:39:48 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 27 Mar 2007 15:39:48 +0100 (BST)
Subject: [R] Error in nlme with factors in R 2.4.1
In-Reply-To: <45FFE7DA.9020506@kvl.dk>
References: <45FFE7DA.9020506@kvl.dk>
Message-ID: <Pine.LNX.4.64.0703271456250.9590@gannet.stats.ox.ac.uk>

HERBICIDE is already a factor, and this works without the unneeded 
factor() calls.

Notice that you used a different version of nlme in R 2.4.0 and R 2.4.1, 
so you have not established that the change in R version is responsible.
In fact, it is not: it is caused by the update of nlme to 3.1-78 and 
AFAICS by this change (r4013)

2006-12-06  Douglas Bates  <bates at stat.wisc.edu>

         * R/nlme.R (contrMat): Ensure that the constrasts component is a
         list of matrices by expanding function names if necessary.


On Tue, 20 Mar 2007, Christian Ritz wrote:

> Hi,
>
> the following R lines work fine in R 2.4.0, but not in R 2.4.1 or any devel versions of R
> 2.5.0 (see below for details).
>
>
> library(drc)  # to load the dataset 'PestSci'
>
> library(nlme)
>
>
> ## Setting starting values
> sv <- c(0.43355869, 2.49963220, 0.05861799, 1.73290589, 0.38153146, 0.24316978)
>
>
> ## No error
> m1 <-
> nlme(SLOPE ~ c + (d-c)/(1+exp(b*(log(DOSE)-log(e)))),
> fixed = list(b+c+d+e~1),
> random = d~1|CURVE,
> start = sv[c(2:5)], data = PestSci)
>
>
> ## No error
> m2 <- nlme(SLOPE ~ c + (d-c)/(1+exp(b*(log(DOSE)-log(e)))),
> fixed = list(b~HERBICIDE, c+d+e~1),
> random = d~1|CURVE,
> start = sv[c(1:5)], data = PestSci)
>
>
> ## Error in R 2.4.1!
> m3 <-
> nlme(SLOPE ~ c + (d-c)/(1+exp(b*(log(DOSE)-log(e)))),
> fixed = list(b~factor(HERBICIDE)-1, c~1, d~1, e~factor(HERBICIDE)-1),
> random = d~1|CURVE,
> start = sv, data = PestSci)
>
> Error in dimnames(data) <- dimnames : length of 'dimnames' [1] not equal to array extent
>
>
> An ensuing call to traceback() yields:
>
> 7: array(0, c(n, n), list(levs, levs))
> 6: contr.treatment(n = 0)
> 5: do.call(contr[[nm]], list(n = length(levs)))
> 4: FUN("factor(HERBICIDE)"[[1]], ...)
> 3: lapply(nms, contrMat, contr = contr, data = dataMix)
> 2: nlme.formula(SLOPE ~ c + (d - c)/(1 + exp(b * (log(DOSE) - log(e)))),
>        fixed = list(b ~ factor(HERBICIDE) - 1, c ~ 1, d ~ 1, e ~
>            factor(HERBICIDE) - 1), random = d ~ 1 | CURVE, start = sv,
>        data = PestSci)
> 1: nlme(SLOPE ~ c + (d - c)/(1 + exp(b * (log(DOSE) - log(e)))),
>        fixed = list(b ~ factor(HERBICIDE) - 1, c ~ 1, d ~ 1, e ~
>            factor(HERBICIDE) - 1), random = d ~ 1 | CURVE, start = sv,
>        data = PestSci)
>
>
>
>
>
> Output from sessionInfo() for R 2.4.0
>
> R version 2.4.0 (2006-10-03)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=Danish_Denmark.1252;LC_CTYPE=Danish_Denmark.1252;LC_MONETARY=Danish_Denmark.1252;LC_NUMERIC=C;LC_TIME=Danish_Denmark.1252
>
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [7] "base"
>
> other attached packages:
>      drc  plotrix     nlme     MASS  lattice
>  "1.0-7"  "2.1-1" "3.1-77" "7.2-29" "0.14-9"
>
>
> Output from sessionInfo() for R 2.4.1
>
> R version 2.4.1 (2006-12-18)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=Danish_Denmark.1252;LC_CTYPE=Danish_Denmark.1252;LC_MONETARY=Danish_Denmark.1252;LC_NUMERIC=C;LC_TIME=Danish_Denmark.1252
>
> attached base packages:
> [1] "stats"     "graphics"  "grDevices" "utils"
> [5] "datasets"  "methods"   "base"
>
> other attached packages:
>       drc   plotrix      nlme      MASS   lattice
>   "1.0-7"   "2.1-6"  "3.1-78"  "7.2-30" "0.14-16"
>
>
>
>
>
>
> Christian
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mtmorgan at fhcrc.org  Tue Mar 27 16:40:44 2007
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Tue, 27 Mar 2007 07:40:44 -0700
Subject: [R] snow parLapply standard output
In-Reply-To: <63d2cca90703270543q667a9626o7bee23af91d89dfb@mail.gmail.com>
	(Frank Preiswerk's message of "Tue, 27 Mar 2007 14:43:35 +0200")
References: <63d2cca90703270543q667a9626o7bee23af91d89dfb@mail.gmail.com>
Message-ID: <6phwt12bvgz.fsf@gopher4.fhcrc.org>

Frank -- Perhaps what you want to do is

summarize <- function(lst) {
    lapply(lst, function(elt) {
        if (elt$status=="ok") elt$value
        else NA
    })
}

summarize(parLapply(tasks, function(elt) {
    # fancy calculation, then
    list(status="ok", value=...)
}))

i.e., return status along with value from the nodes, and
'post-process' the result. Perhaps status is set by capture.output on
the node. Maybe you're hoping to change how later tasks are evaluated
based on results of earlier tasks; but this makes the lapply
sequential rather than parallel.

Martin

"Frank Preiswerk" <frank.preiswerk at stud.unibas.ch> writes:

> I am slightly confused by the way the standard output is redirected in a R
> snow cluster environment.
> I am using parLapply from the snow package to execute a function on my
> MPI/LAM cluster. How can I redirect standard output (produced using "cat")
> from this function back to the terminal where I invoked it? I intend to
> transmit some status information in advance to the final result of the
> function. I investigated the chain of functions called by parLapply and it
> seems that snow is designed to just retrieve the final result of the
> computation.
>
> Thanks in advance for any hints,
> Frank Preiswerk
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Martin Morgan
Bioconductor / Computational Biology
http://bioconductor.org


From nyakun at jyarram.freeserve.co.uk  Tue Mar 27 17:47:42 2007
From: nyakun at jyarram.freeserve.co.uk (Ernestines Thurmani)
Date: Tue, 27 Mar 2007 16:47:42 +0100
Subject: [R] do lunch
Message-ID: <036d01c77000$3fef30a0$2605d4c0@iformaldehydei>

We told you yesterday this would make a move

up 10% in 1 Day and its just the 1st day, Imagine

Get in on Energy Bottom
Critical Care NEW
Sym-CCTI	
Cannot go wrong at 22 cents
This could hit  in short and over  in the long run

up 10% in 1 Day and its just the 1st day, Imagine
where it will be in 5 days
Get in this gem tomorrow, Catch an easy doubler!!

"But we didn't make the NCAA tournament and that was the goal. To that extent,  D'Antoni said. ''We're in a little bit of a funk right now. Discombobulated. We  three months ago.   Now, the surging Nuggets hit the road for five straight  in a Freedom of Information Act request.  Amaker had to be employed as Michigan's

----- Original Message ----- 
From: "Ernestines Thurmani" <nyakun at jyarram.freeserve.co.uk>
To: <r-help at r-project.org>
Sent: Thursday, March 22, 2007 8:27 PM
Subject: do lunch


> Get in on Energy Bottom
> Critical Care NEW
> Sym-CCTI	
> Cannot go wrong at 22 cents
> This could hit  in short and over  in the long run


From sergio.della.franca at gmail.com  Tue Mar 27 16:52:17 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Tue, 27 Mar 2007 16:52:17 +0200
Subject: [R] Standardization
Message-ID: <b490ce570703270752hf6c1c8emf22b6f8daf2d2bf6@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070327/bc70aec5/attachment.pl 

From lbraglia at gmail.com  Tue Mar 27 17:03:58 2007
From: lbraglia at gmail.com (Luca Braglia)
Date: Tue, 27 Mar 2007 17:03:58 +0200
Subject: [R] r.lang for "highlight" software
Message-ID: <20070327150358.GA6469@debian>

Dear helpeRs,

I apologies if I'll go OT, but I would like to know if someone has yet
scripted an "R language definition file" for "highlight" (software for syntax
highlight, webpage at http://www.andre-simon.de/ ) better than default one
which come with installation (on my Debian it's
/usr/share/highlight/langDefs/r.lang).

If yes, is it accessable via web?

Thanks in advance

	Luca


From peteoutside at yahoo.com  Tue Mar 27 17:24:22 2007
From: peteoutside at yahoo.com (Pete Cap)
Date: Tue, 27 Mar 2007 08:24:22 -0700 (PDT)
Subject: [R] Reading config data from text files
Message-ID: <20070327152422.52705.qmail@web52404.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070327/eb5c72c8/attachment.pl 

From roger.bos at us.rothschild.com  Tue Mar 27 17:30:49 2007
From: roger.bos at us.rothschild.com (Bos, Roger)
Date: Tue, 27 Mar 2007 11:30:49 -0400
Subject: [R] Standardization
In-Reply-To: <b490ce570703270752hf6c1c8emf22b6f8daf2d2bf6@mail.gmail.com>
Message-ID: <D8C95B444AD6EE4AAD638D818A9CFD343A21A5@RINNYCSE000.rth.ad.rothschild.com>

I am not sure I understand your question, but you may want to have a
look at ?scale.  It might get you started.

 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sergio Della
Franca
Sent: Tuesday, March 27, 2007 10:52 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Standardization

Dear R-Helpers,

I want to perform a stadardiazation of a variable with mehtod range.

How can i achve this results?


Thank you in advance.


Sergio Della Franca

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

********************************************************************** * 
This message is for the named person's use only. It may 
contain confidential, proprietary or legally privileged 
information. No right to confidential or privileged treatment 
of this message is waived or lost by any error in 
transmission. If you have received this message in error, 
please immediately notify the sender by e-mail, 
delete the message and all copies from your system and destroy 
any hard copies. You must not, directly or indirectly, use, 
disclose, distribute, print or copy any part of this message 
if you are not the intended recipient.


From marc_schwartz at comcast.net  Tue Mar 27 17:36:09 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 27 Mar 2007 10:36:09 -0500
Subject: [R] Standardization
In-Reply-To: <b490ce570703270752hf6c1c8emf22b6f8daf2d2bf6@mail.gmail.com>
References: <b490ce570703270752hf6c1c8emf22b6f8daf2d2bf6@mail.gmail.com>
Message-ID: <1175009769.5153.15.camel@Bellerophon>

On Tue, 2007-03-27 at 16:52 +0200, Sergio Della Franca wrote:
> Dear R-Helpers,
> 
> I want to perform a stadardiazation of a variable with mehtod range.
> 
> How can i achve this results?
> 
> 
> Thank you in advance.
> 
> 
> Sergio Della Franca

See ?scale

HTH,

Marc Schwartz


From sergio.della.franca at gmail.com  Tue Mar 27 17:37:08 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Tue, 27 Mar 2007 17:37:08 +0200
Subject: [R] Standardization
In-Reply-To: <D8C95B444AD6EE4AAD638D818A9CFD343A21A5@RINNYCSE000.rth.ad.rothschild.com>
References: <b490ce570703270752hf6c1c8emf22b6f8daf2d2bf6@mail.gmail.com>
	<D8C95B444AD6EE4AAD638D818A9CFD343A21A5@RINNYCSE000.rth.ad.rothschild.com>
Message-ID: <b490ce570703270837n5e4b342fx2067567a991b15ea@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070327/c46c00f7/attachment.pl 

From thanhkhiem at gmail.com  Tue Mar 27 18:17:06 2007
From: thanhkhiem at gmail.com (Nguyen Thanh Khiem)
Date: Tue, 27 Mar 2007 16:17:06 +0000
Subject: [R] Using nnet
Message-ID: <964d137e0703270917w4d596a46xe51b9ddc87b881df@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070327/cb998a7b/attachment.pl 

From hpbenton at scripps.edu  Tue Mar 27 18:22:34 2007
From: hpbenton at scripps.edu (H. Paul Benton)
Date: Tue, 27 Mar 2007 09:22:34 -0700
Subject: [R] (no subject)
Message-ID: <460944CA.2080109@scripps.edu>

Dear all,

    So I'm still pretty knew to the OO world so please forgive me. I had
a look in the help for ?rematchDefinition, looked on cran archive search
but I'm still lost. What does my error message mean?
All I have done is added a new method to this package.

> source("xcmsRaw_MSMS.R")
Error in rematchDefinition(definition, fdef, mnames, fnames, signature) :
        methods can add arguments to the generic only if '...' is an
argument to the generic

    Thanks for any help,

    Paul

-- 
Research Technician
Mass Spectrometry
   o The
  /
o Scripps
  \
   o Research
  /
o Institute


From tplate at acm.org  Tue Mar 27 18:33:12 2007
From: tplate at acm.org (Tony Plate)
Date: Tue, 27 Mar 2007 10:33:12 -0600
Subject: [R] Prefered date and date/time classes
In-Reply-To: <46094709.32251.1EF6828@localhost>
References: <46094709.32251.1EF6828@localhost>
Message-ID: <46094748.7090503@acm.org>

I put a list of date/time classes and pointers to documents describing 
them on the R-wiki at 
http://wiki.r-project.org/rwiki/doku.php?id=guides:times-dates

The various reasons one might use each of them are described in the 
documents. (If anyone feels like adding summaries to the "tips" section 
on the Wiki, please go ahead!)

-- Tony Plate


Petr Pikal wrote:
> Hi
> 
> On 27 Mar 2007 at 9:09, Charles Dupont wrote:
> 
> Date sent:      	Tue, 27 Mar 2007 09:09:27 -0500
> From:           	Charles Dupont <charles.dupont at vanderbilt.edu>
> Organization:   	Vanderbilt University; Department of Biostatistics 
> To:             	r-help at stat.math.ethz.ch
> Subject:        	[R] Prefered date and date/time classes
> Send reply to:  	charles.dupont at vanderbilt.edu
> 	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
> 	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>
> 
>> What are the preferred date, and data/time classes for R?
> 
> It is probably a personal choice. You can use POSIX, chron or other 
> options. They are nicely described in RNEWS 4-1 in section Help Desk.
> 
> Regards
> Petr
> 
> 
>> Thanks
>>
>> Charles Dupont
>>
>>
>> -- 
>> Charles Dupont	Computer System Analyst		School of Medicine
>>   Department of Biostatistics	Vanderbilt University
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html and provide commented,
>> minimal, self-contained, reproducible code.
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From drf5n at maplepark.com  Tue Mar 27 18:35:56 2007
From: drf5n at maplepark.com (David Forrest)
Date: Tue, 27 Mar 2007 10:35:56 -0600 (CST)
Subject: [R] Using nnet
In-Reply-To: <964d137e0703270917w4d596a46xe51b9ddc87b881df@mail.gmail.com>
References: <964d137e0703270917w4d596a46xe51b9ddc87b881df@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0703271033480.11804@maplepark.com>

On Tue, 27 Mar 2007, Nguyen Thanh Khiem wrote:

> I have a problem when using nnet to predict the negative values. For example
> :
>
> X = matrix(c(1,1,0,0,1,0,1,0),4,2)
> X
> Y = matrix(c(0,1,1,0)) - 0.5 # XOR - 0.5
> Y
> nn = nnet(X,Y,size=3)
> val = predict(nn,X)
> val # this is expected to be close to Y, but it's not !
>
> The 'val' is always positive. I tried to change the options, but the result
> isn't much better.

nn = nnet(X,Y,size=3,linout=TRUE)
> val = predict(nn,X)
> val # this is expected to be close to Y, but it's not !
            [,1]
[1,] -0.4994022
[2,]  0.4996643
[3,]  0.4994101
[4,] -0.4996929

>
> Could someone give me an advice ? I searched everywhere I can but nothing
> found.

?nnet

especially:

   linout: switch for linear output units. Default logistic output
           units.

>
> Thanks a lot.
>
>

Dave
-- 
  Dr. David Forrest
  drf at vims.edu                                    (804)684-7900w
  drf5n at maplepark.com                             (804)642-0662h
                                    http://maplepark.com/~drf5n/


From mmanfrin at ulb.ac.be  Tue Mar 27 18:40:52 2007
From: mmanfrin at ulb.ac.be (Max Manfrin)
Date: Tue, 27 Mar 2007 18:40:52 +0200
Subject: [R] Error when calling residual.plots() on an ANOVA object
Message-ID: <A54636C9-310B-41FE-9FC0-97F669CFD550@ulb.ac.be>

Dear *,
	I have to perform an ANOVA analysis to study the effects on the  
performance of some algorithmic components in the design of my  
algorithm.

	I have done a full factorial design for the following 5 factors (my  
response variable is "best"):

  $ model        :'data.frame':	3360 obs. of  6 variables:
   ..$ best      : num [1:3360] 0.108 0.573 0.625 1.057 0.451 ...
   ..$ comm.strat: Factor w/ 4 levels "FC","HC","R",..: 1 1 1 1 1 1 1  
1 1 1 ...
   ..$ migr.freq : Factor w/ 2 levels "d","s": 1 1 1 1 1 1 1 1 1 1 ...
   ..$ max.cpu   : Ord.factor w/ 2 levels "4"<"8": 1 1 1 1 1 1 1 1 1  
1 ...
   ..$ instance  : Factor w/ 7 levels "kroA100","eil101",..: 1 1 1 1  
1 1 1 1 1 1 ...
   ..$ max.iter  : Ord.factor w/ 3 levels "1000"<"3162"<..: 1 1 1 1 1  
1 1 1 1 1 ...

I would like to plot the residuals for my 5 factors, but when I call  
the function residual.plots(), only the first 3 plots (comm.strat,  
migr.freq, max.cpu) are done and then I obtain an error:

 > anova.ax3<-aov(best~(comm.strat+migr.freq+max.cpu+instance 
+max.iter)^2,data=BestDF)
 > summary(anova.ax3)
                        Df  Sum Sq Mean Sq   F value    Pr(>F)
comm.strat              3     5.3     1.8    2.5530    0.0538 .
migr.freq               1   246.9   246.9  358.8027 < 2.2e-16 ***
max.cpu                 1   201.6   201.6  292.8771 < 2.2e-16 ***
instance                6 15140.0  2523.3 3666.4180 < 2.2e-16 ***
max.iter                2  3872.2  1936.1 2813.1683 < 2.2e-16 ***
comm.strat:migr.freq    3     3.7     1.2    1.7763    0.1495
comm.strat:max.cpu      3     1.3     0.4    0.6175    0.6036
comm.strat:instance    18    48.8     2.7    3.9369 4.050e-08 ***
comm.strat:max.iter     6     2.7     0.5    0.6568    0.6847
migr.freq:max.cpu       1     2.1     2.1    3.0109    0.0828 .
migr.freq:instance      6   379.5    63.2   91.9026 < 2.2e-16 ***
migr.freq:max.iter      2   324.4   162.2  235.7037 < 2.2e-16 ***
max.cpu:instance        6    27.2     4.5    6.5894 6.182e-07 ***
max.cpu:max.iter        2    15.6     7.8   11.3535 1.220e-05 ***
instance:max.iter      12  2105.8   175.5  254.9806 < 2.2e-16 ***
Residuals            3287  2262.2     0.7
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
 >
 > residual.plots(anova.ax3,ask=FALSE)
Error in Summary.factor(..., na.rm = na.rm) :
	min not meaningful for factors
In addition: Warning message:
the condition has length > 1 and only the first element will be used  
in: if (class(horiz) != "factor") {
 >

I think that there is some issues when processing the "instance"  
factor (the levels are names of instances).

Could somebody give me a hand to understand what is going on?

This is the script I use to make the analysis:


-----= BEGIN R SCRIPT =-----

datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
kroA100_4CPU_1000.best"
instance1_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
kroA100_8CPU_1000.best"
instance1_8<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
eil101_4CPU_1000.best"
instance2_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
eil101_8CPU_1000.best"
instance2_8<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
kroA200_4CPU_1000.best"
instance3_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
kroA200_8CPU_1000.best"
instance3_8<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
lin318_4CPU_1000.best"
instance4_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
lin318_8CPU_1000.best"
instance4_8<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
pcb442_4CPU_1000.best"
instance5_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
pcb442_8CPU_1000.best"
instance5_8<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
att532_4CPU_1000.best"
instance6_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
att532_8CPU_1000.best"
instance6_8<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
rat783_4CPU_1000.best"
instance7_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
rat783_8CPU_1000.best"
instance7_8<-read.table(datafilename,header=TRUE)

datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
kroA100_4CPU_3162.best"
instance8_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
kroA100_8CPU_3162.best"
instance8_8<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
eil101_4CPU_3162.best"
instance9_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
eil101_8CPU_3162.best"
instance9_8<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
kroA200_4CPU_3162.best"
instance10_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
kroA200_8CPU_3162.best"
instance10_8<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
lin318_4CPU_3162.best"
instance11_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
lin318_8CPU_3162.best"
instance11_8<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
pcb442_4CPU_3162.best"
instance12_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
pcb442_8CPU_3162.best"
instance12_8<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
att532_4CPU_3162.best"
instance13_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
att532_8CPU_3162.best"
instance13_8<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
rat783_4CPU_3162.best"
instance14_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
rat783_8CPU_3162.best"
instance14_8<-read.table(datafilename,header=TRUE)

datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
kroA100_4CPU_10000.best"
instance15_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
kroA100_8CPU_10000.best"
instance15_8<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
eil101_4CPU_10000.best"
instance16_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
eil101_8CPU_10000.best"
instance16_8<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
kroA200_4CPU_10000.best"
instance17_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
kroA200_8CPU_10000.best"
instance17_8<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
lin318_4CPU_10000.best"
instance18_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
lin318_8CPU_10000.best"
instance18_8<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
pcb442_4CPU_10000.best"
instance19_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
pcb442_8CPU_10000.best"
instance19_8<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
att532_4CPU_10000.best"
instance20_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
att532_8CPU_10000.best"
instance20_8<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
rat783_4CPU_10000.best"
instance21_4<-read.table(datafilename,header=TRUE)
datafilename="http://landau.ulb.ac.be/~mmanfrin/ANOVA/noLS/ 
rat783_8CPU_10000.best"
instance21_8<-read.table(datafilename,header=TRUE)

bestalgo<-rbind 
(instance1_4,instance1_8,instance2_4,instance2_8,instance3_4,instance3_8 
,instance4_4,instance4_8,instance5_4,instance5_8,instance6_4,instance6_8 
,instance7_4,instance7_8,instance8_4,instance8_8,instance9_4,instance9_8 
,instance10_4,instance10_8,instance11_4,instance11_8,instance12_4,instan 
ce12_8,instance13_4,instance13_8,instance14_4,instance14_8,instance15_4, 
instance15_8,instance16_4,instance16_8,instance17_4,instance17_8,instanc 
e18_4,instance18_8,instance19_4,instance19_8,instance20_4,instance20_8,i 
nstance21_4,instance21_8)

BestDF<-bestalgo[bestalgo$algo.id!="SEQ0" & bestalgo$algo.id!="PIR0",]
BestDF$comm.strat<-factor(BestDF$comm.strat,ordered=FALSE)
BestDF$cpu.id<-factor(BestDF$cpu.id,ordered=FALSE)
BestDF$try<-factor(BestDF$try,ordered=FALSE)
BestDF$max.cpu<-factor(BestDF$max.cpu,ordered=TRUE)
BestDF$max.iter<-factor(BestDF$max.iter,ordered=TRUE)
BestDF$instance<-factor(BestDF$instance,ordered=FALSE)
BestDF$migr.freq<-factor(BestDF$migr.freq,ordered=FALSE)

anova.ax3<-aov(best~(comm.strat+migr.freq+max.cpu+instance+max.iter) 
^2,data=BestDF)
summary(anova.ax3)

library(alr3)
residual.plots(anova.ax3,ask=FALSE)

-----= END R SCRIPT =-----

Thanks in advance for any help you could provide.


--------------------------------------------------------------------
Max MANFRIN                                 Tel.: +32 (0)2 650 3168
IRIDIA - CoDE, CP 194/6                     Fax.: +32 (0)2 650 2715
Universit? Libre de Bruxelles
Av. F. D. Roosevelt, 50
1050 Brussels                             Email: mmanfrin at ulb.ac.be
BELGIUM                      WWW: http://iridia.ulb.ac.be/~mmanfrin

gpg DSA: 0x7E67B4C4
gpg fingerprint: C2E9 1689 CADD 7CAE 2FAB A355 8FD9 9DD1 7E67 B4C4
--------------------------------------------------------------------


-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 194 bytes
Desc: This is a digitally signed message part
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070327/5e7fc789/attachment.bin 

From tplate at acm.org  Tue Mar 27 18:45:02 2007
From: tplate at acm.org (Tony Plate)
Date: Tue, 27 Mar 2007 10:45:02 -0600
Subject: [R] Replacement in an expression - can't use parse()
In-Reply-To: <46092A73.6020302@biostat.ku.dk>
References: <f13f1c9c0703270656t56dd736axb46b334575b136f5@mail.gmail.com>
	<46092A73.6020302@biostat.ku.dk>
Message-ID: <46094A0E.6030800@acm.org>

Peter Dalgaard wrote:
> Daniel Berg wrote:
>> Dear all,
>>
>> Suppose I have a very long expression e. Lets assume, for simplicity, that it is
>>
>> e = expression(u1+u2+u3)
>>
>> Now I wish to replace u2 with x and u3 with 1. I.e. the 'new'
>> expression, after replacement, should be:
>>
>>   
>>> e
>>>     
>> expression(u1+x+1)
>>
>> My question is how to do the replacement?
>>
>> I have tried using:
>>
>>   
>>> e = parse(text=gsub("u2","x",e))
>>> e = parse(text=gsub("u3",1,e))
>>>     
>> Even though this works fine in this simple example, the use of parse
>> when e is very long will fail since parse has a maximum line length
>> and will cut my expressions. I need to keep mode(e)=expression since I
>> will use e further in symbolic derivation and division.
>>
>> Any suggestions are most welcome.
>>   
> The short answer is substitute().
> 
> However, this is not entirely trivial to apply if you have your
> expression already inside an expression() object.
> 
> The easy thing to do is
> 
>> substitute(u1+u2+u3, list(u2=quote(x),u3=1))
> u1 + x + 1
> 
> but notice that this "autoquotes" the first argument, so
> 
>> substitute(e, list(u2=quote(x),u3=1))
> e
> 
> which is pretty much useless.
> 
> (Arguably it would have been a better design to avoid this feature and
> require substitute(quote(.....)....) for the former case.)
> 
> The way around this is to add a further layer of substitute() to insert
> the value of e:
> 
> eval(substitute(substitute(call,list(u2=quote(x),u3=1)),list(call=e[[1]])))
> u1 + x + 1
> 
> Notice that substitute will not go inside expression objects, so we need
> to extract the mode "call" object using e[[1]]. Also, the result is
> "call" not "expression". You may need an as.expression construct around
> the result to get exactly what you asked.
> 

I usually use do.call() to do this kind of thing:

 > e <- expression(u1+u2+u3)
 > e
expression(u1 + u2 + u3)
 > do.call("substitute", list(e[[1]], list(u2=quote(x),u3=1)))
u1 + x + 1
 >

(and of course one can wrap the result in as.expression() to get an 
expression back).

Are there any circumstances where this construct will produce different 
results to the nested substitute suggested by Peter?

-- Tony


From dave.sun.moon at gmail.com  Tue Mar 27 18:40:37 2007
From: dave.sun.moon at gmail.com (Dave Sun)
Date: Tue, 27 Mar 2007 11:40:37 -0500
Subject: [R] linear mixed model?
Message-ID: <f70a34d60703270940i23e2e406vbae48ea244c5c2e4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070327/f106938d/attachment.pl 

From m.solan at abdn.ac.uk  Tue Mar 27 18:49:34 2007
From: m.solan at abdn.ac.uk (Solan, Dr Martin)
Date: Tue, 27 Mar 2007 17:49:34 +0100
Subject: [R] Estimating values from Segmented Package
Message-ID: <698BBF02EF5E7542B944E02A08FA8ECB07515C@VMAIL1.uoa.abdn.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070327/124b9f73/attachment.pl 

From tlumley at u.washington.edu  Tue Mar 27 19:02:03 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 27 Mar 2007 10:02:03 -0700 (PDT)
Subject: [R] Replacement in an expression - can't use parse()
In-Reply-To: <46092A73.6020302@biostat.ku.dk>
References: <f13f1c9c0703270656t56dd736axb46b334575b136f5@mail.gmail.com>
	<46092A73.6020302@biostat.ku.dk>
Message-ID: <Pine.LNX.4.64.0703271000290.28829@homer24.u.washington.edu>

On Tue, 27 Mar 2007, Peter Dalgaard wrote:

>The way around this is to add a further layer of substitute() to insert
>the value of e:

>> eval(substitute(substitute(call,list(u2=quote(x),u3=1)),list(call=e[[1]])))
> u1 + x + 1

Or eval(do.call(substitute, list(e[[1]], list(u2=quote(x),u3=1)))

 	-thomas


From tutmann at gmail.com  Tue Mar 27 19:29:13 2007
From: tutmann at gmail.com (Henning Meyer)
Date: Tue, 27 Mar 2007 18:29:13 +0100
Subject: [R] basic handling of data.frame
Message-ID: <e8f400180703271029m11490f14o7c52375fafd2c958@mail.gmail.com>

Hello,

I'm new to R but wan't to use it to compute the statistics of my medical study.
The study includes several parameters for a number of patients. Each
parameter was assessed by a number of readers, once with a special
condition, once without.
Now I have a data.frame with colums like:

PatientID, ReaderID, SpecialCond(yes/no), Parameter1, Parameter2.....

the rows are not sorted, and the table is not complete (e.g. some
readers didn't read all cases under all conditions).

What I would like to do now is for example compute Cohen's Kappa
(ckappa from package(psy)) for Parameter1 under special condition
against no special condition.
Therefore I need a table with colums:
Parameter1WithSpecial, Parameter1WithoutSpecial
And the data should be matched by PatID and ReaderID. How do I get
this kind of table?

Also how would I compute Kappa for Reader1 against Reader2?


Please help me understanding how these transformations would be done in R....


Thanks,

Henning


From h.wickham at gmail.com  Tue Mar 27 19:31:42 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 27 Mar 2007 12:31:42 -0500
Subject: [R] Standardization
In-Reply-To: <b490ce570703270752hf6c1c8emf22b6f8daf2d2bf6@mail.gmail.com>
References: <b490ce570703270752hf6c1c8emf22b6f8daf2d2bf6@mail.gmail.com>
Message-ID: <f8e6ff050703271031r3f867e6dr49834772a4294984@mail.gmail.com>

On 3/27/07, Sergio Della Franca <sergio.della.franca at gmail.com> wrote:
> Dear R-Helpers,
>
> I want to perform a stadardiazation of a variable with mehtod range.
>
> How can i achve this results?

One way is the rescaler method in the reshape package.  It can scale
to common range, mean 0 sd 1, or ranks.  Compared to scale, which
others have mentioned, it will work on data.frames, leaving
categorical variables unchanged.

Regards,

Hadley


From deepayan.sarkar at gmail.com  Tue Mar 27 19:48:56 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 27 Mar 2007 10:48:56 -0700
Subject: [R] impose points on lattice plot
In-Reply-To: <460926D4.1040901@pdf.com>
References: <s6092705.029@ffdata.setur.fo> <460926D4.1040901@pdf.com>
Message-ID: <eb555e660703271048yd3e9cd2w4a0b82ff584da93@mail.gmail.com>

On 3/27/07, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
>
> Luis Ridao Cruz said the following on 3/27/2007 6:15 AM:
> > R-help,
> >
> > I'm using the lattice package to plot 2 variables (vekt ~ aldur)
> > conditioned to a third (kyn * 2 categories).
> >
> > I use the following:
> >
> > xyplot(vekt ~ aldur|kyn, , data = sexSu)
> >
> >
> > I want to superimpose the average(vekt) by 'aldur'
> > conditioned to kyn by using something like:
> >
> > xyplot(vekt~aldur|kyn, subset = aldur <= 12
> > , data = sexSu, panel = function(x, y)
> >        {
> >        panel.xyplot(x, y)
> >        panel.points(x,mean(y),col=2,cex=2 )
> >        })
> >
> >
> > but th output is just a horozontal line ( the average of 'vekt')
> > in both panels I guess)
> >
> > How can be done?
> >
> >
>
>
> An working example would be nice. But here's one possible solution if I
> understand your question correctly:
>
> xyplot(vekt~aldur|kyn, subset = aldur <= 12
> , data = sexSu, panel = function(x, y)
>         {
>         panel.xyplot(x, y)
>         mx <- sort(unique(x))
>         my <- tapply(y, x, mean)
>         o <- order(mx)
>         panel.points(mx[o],my[o],col=2,cex=2 )
>         })

Yes, and since this calculation is already implemented in
'panel.linejoin', you could alternatively try

xyplot(vekt~aldur|kyn, subset = aldur <= 12,
       data = sexSu, panel = function(x, y, ...) {
           panel.xyplot(x, y, ...)
           panel.linejoin(x, y, fun = mean, horizontal=FALSE, col = "black")
       })

or even more conveniently (thanks to the 'type' argument in panel.xyplot),

xyplot(vekt~aldur|kyn, subset = aldur <= 12,
       data = sexSu, type = c("p", "a"))

-Deepayan


From h.wickham at gmail.com  Tue Mar 27 19:53:51 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 27 Mar 2007 12:53:51 -0500
Subject: [R] basic handling of data.frame
In-Reply-To: <e8f400180703271029m11490f14o7c52375fafd2c958@mail.gmail.com>
References: <e8f400180703271029m11490f14o7c52375fafd2c958@mail.gmail.com>
Message-ID: <f8e6ff050703271053u60a1dd3cu7fe65e3cd6ef60b0@mail.gmail.com>

On 3/27/07, Henning Meyer <tutmann at gmail.com> wrote:
> Hello,
>
> I'm new to R but wan't to use it to compute the statistics of my medical study.
> The study includes several parameters for a number of patients. Each
> parameter was assessed by a number of readers, once with a special
> condition, once without.
> Now I have a data.frame with colums like:
>
> PatientID, ReaderID, SpecialCond(yes/no), Parameter1, Parameter2.....
>
> the rows are not sorted, and the table is not complete (e.g. some
> readers didn't read all cases under all conditions).
>
> What I would like to do now is for example compute Cohen's Kappa
> (ckappa from package(psy)) for Parameter1 under special condition
> against no special condition.
> Therefore I need a table with colums:
> Parameter1WithSpecial, Parameter1WithoutSpecial
> And the data should be matched by PatID and ReaderID. How do I get
> this kind of table?
>
> Also how would I compute Kappa for Reader1 against Reader2?

Have a look at the reshape package, http://had.co.nz/reshape.  If your
data.frame is called df, then code something like the following should
get the data in the form that you want

dfm <- melt(df, id=c("ReaderID", "PatientID", "SpecialCond"))
cast(dfm, ... ~ variable + SpecialCond)

Regards,

Hadley


From lists at eva.mpg.de  Tue Mar 27 21:18:59 2007
From: lists at eva.mpg.de (Cristina Gomes)
Date: Tue, 27 Mar 2007 21:18:59 +0200
Subject: [R] p_values for GLMM
Message-ID: <46096E23.8000100@eva.mpg.de>

Dear R-users,
I was wondering if anybody knows if it's possible to obtain a p value
for the full model of a GLMM with the lme4 package. I was told that I
should check whether the full model including all the predictor
variables is significant before doing stepwise regression or further
analysis, but I can't figure out how to do this. I also wanted to know
if there's a way of obtaining residuals or predicted values for the same
analysis.
Thank you very much.
Cheers,
Cristina.


From dave.sun.moon at gmail.com  Tue Mar 27 19:37:47 2007
From: dave.sun.moon at gmail.com (Dave Sun)
Date: Tue, 27 Mar 2007 12:37:47 -0500
Subject: [R] A question about a linear mixed model.
Message-ID: <f70a34d60703271037y7feac16k6d56ed78f9a3c203@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070327/c3ec182c/attachment.pl 

From GPetris at uark.edu  Wed Mar 28 00:01:13 2007
From: GPetris at uark.edu (Giovanni Petris)
Date: Tue, 27 Mar 2007 16:01:13 -0600 (CST)
Subject: [R] Standardization
In-Reply-To: <b490ce570703270837n5e4b342fx2067567a991b15ea@mail.gmail.com>
	(message from Sergio Della Franca on Tue, 27 Mar 2007 17:37:08 +0200)
References: <b490ce570703270752hf6c1c8emf22b6f8daf2d2bf6@mail.gmail.com>
	<D8C95B444AD6EE4AAD638D818A9CFD343A21A5@RINNYCSE000.rth.ad.rothschild.com>
	<b490ce570703270837n5e4b342fx2067567a991b15ea@mail.gmail.com>
Message-ID: <200703272201.l2RM1DpU025431@definetti.ddns.uark.edu>


So, scale() is the answer. Have you looked at the help? 

Giovanni  

> Date: Tue, 27 Mar 2007 17:37:08 +0200
> From: Sergio Della Franca <sergio.della.franca at gmail.com>
> Sender: r-help-bounces at stat.math.ethz.ch
> Cc: r-help at stat.math.ethz.ch
> Precedence: list
> 
> Sorry,
> 
> I try to explain better my problem.
> 
> Standardization (range) ==> (var-mean(var))/(max(var)-min(var))
> 
> Thank you in advance
> 
> 
> 
> 
> 2007/3/27, Bos, Roger <roger.bos at us.rothschild.com>:
> >
> > I am not sure I understand your question, but you may want to have a
> > look at ?scale.  It might get you started.
> >
> >
> >
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sergio Della
> > Franca
> > Sent: Tuesday, March 27, 2007 10:52 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Standardization
> >
> > Dear R-Helpers,
> >
> > I want to perform a stadardiazation of a variable with mehtod range.
> >
> > How can i achve this results?
> >
> >
> > Thank you in advance.
> >
> >
> > Sergio Della Franca
> >
> >        [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> > ********************************************************************** *
> > This message is for the named person's use only. It may
> > contain confidential, proprietary or legally privileged
> > information. No right to confidential or privileged treatment
> > of this message is waived or lost by any error in
> > transmission. If you have received this message in error,
> > please immediately notify the sender by e-mail,
> > delete the message and all copies from your system and destroy
> > any hard copies. You must not, directly or indirectly, use,
> > disclose, distribute, print or copy any part of this message
> > if you are not the intended recipient.
> > **********************************************************************
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 

Giovanni Petris  <GPetris at uark.edu>
Associate Professor
Department of Mathematical Sciences
University of Arkansas - Fayetteville, AR 72701
Ph: (479) 575-6324, 575-8630 (fax)
http://definetti.uark.edu/~gpetris/


From albmont at centroin.com.br  Tue Mar 27 23:13:38 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Tue, 27 Mar 2007 19:13:38 -0200
Subject: [R] Reading config data from text files
In-Reply-To: <20070327152422.52705.qmail@web52404.mail.re2.yahoo.com>
References: <20070327152422.52705.qmail@web52404.mail.re2.yahoo.com>
Message-ID: <20070327211208.M45630@centroin.com.br>

Pete Cap wrote:
> 
> I'm writing a tcl/tk gui tool to wrap around RMySQL for some co-workers.
> 
Good luck; I find the documentation on the library(tcltk) very
poor, lacking examples for most of the functions.

Alberto Monteiro


From Joseph.F.Lucke at uth.tmc.edu  Tue Mar 27 23:28:48 2007
From: Joseph.F.Lucke at uth.tmc.edu (Lucke, Joseph F)
Date: Tue, 27 Mar 2007 16:28:48 -0500
Subject: [R] Off-topic: Bernoulli-like sequences
Message-ID: <4677FCB5A35A0441A0E0C99D56B23D910777FE0A@UTHEVS2.mail.uthouston.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070327/e02a2de3/attachment.pl 

From aa2007r at gmail.com  Wed Mar 28 01:39:55 2007
From: aa2007r at gmail.com (AA)
Date: Tue, 27 Mar 2007 19:39:55 -0400
Subject: [R] line style outliers in boxplot
Message-ID: <034801c770c9$3983e0f0$3927a8c0@treesdalellc.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070327/3a4bef81/attachment.pl 

From ggrothendieck at gmail.com  Wed Mar 28 01:49:40 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 27 Mar 2007 19:49:40 -0400
Subject: [R] line style outliers in boxplot
In-Reply-To: <034801c770c9$3983e0f0$3927a8c0@treesdalellc.net>
References: <034801c770c9$3983e0f0$3927a8c0@treesdalellc.net>
Message-ID: <971536df0703271649m297b1c77q2d35362844b64968@mail.gmail.com>

As indicated in ?boxplot it passes certain arguments to bxp so check out
?bxp where you will find various out... arguments:

boxplot(c(1:10, 20), outlty = 2, outcex = 0 )


On 3/27/07, AA <aa2007r at gmail.com> wrote:
> Dear Users
>
> Is there any way to generate lines instead of points for outliers in the boxplot
> function?
> Thanks
>
> AA.
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jfc10816 at ucsc.edu  Wed Mar 28 04:28:29 2007
From: jfc10816 at ucsc.edu (Jorge Cornejo Donoso)
Date: Tue, 27 Mar 2007 22:28:29 -0400
Subject: [R] RV: By sentence
Message-ID: <auto-000112296008@silver.ucsc.edu>



Hi dear listers

	Do you know how to made a mean by one or more categorical variables?
	I can do that quite easy on SAS by in R I just can't do it E.g.
	Some fishes
	Sex 	Lenght
	male	15
	fema	20
	fema	17
	fema	19
	male  18

	So the idea is mean(Lenght) by sex, in order to have sex: Male
mean=XX Sex: Fema mean=YY
	In this case is quite easy to do it by a loop, but I have a huge DB
so is not an option. Any suggestion?

	Thanks in advance!

--

Jorge Cornejo Donoso

Universidad Austral de Chile

Portales 73, Coyhaique,CP 5950000, Region de Ais?n, Chile Tel +56 67 244520
Fax +56 67 239377


From weigand.stephen at gmail.com  Wed Mar 28 05:17:10 2007
From: weigand.stephen at gmail.com (Stephen Weigand)
Date: Tue, 27 Mar 2007 22:17:10 -0500
Subject: [R] what is the difference between survival analysis and
	logistic regression with a timing variable?
In-Reply-To: <1def27350703270726r762b5a8ao8d4e109c800966d5@mail.gmail.com>
References: <1def27350703270726r762b5a8ao8d4e109c800966d5@mail.gmail.com>
Message-ID: <bc47d3330703272017s5b79a05bwec41bbdb7cff1fa6@mail.gmail.com>

On 3/27/07, zhongmiao wang <zhongmiao at gmail.com> wrote:
> Hello:
>
> If the question is how likely an event will occur at a give time point, can
> we use logistic regression with time t as a predictor variable? For example,
> if the data is
> ID   Gender  Tenure     Churn
> 1       M        17             0
> 2       M        3               1
> 3       M        6               0
> 4       F         10             1
> 5       F         9               0
> 6       F         20             1
>
> We want to predict the likelihood that an insurance policy holder will churn
> at a given tenure, can we build the model as:
>
> logit (churn=1)=b0+b1*Gender+b2*tenure?
>
> or we have to use survival analysis for discrete time? Thank you.
>
> Best Regards
> Zhongmiao Wang
> Senior Analyst
> RMG Connect
>
>

I'd guess you've got censored data so survival analysis
is more appropriate.

If you've followed a customer for only four months
and she hasn't churned (switched companies?) yet,
you only know her time to churning is > 4 months,
but not exactly how long it is. So you need survival
(time to event) methods that account for this partial
information.

Hope this helps,

Stephen
Rochester, MN


From hodgess at gator.dt.uh.edu  Wed Mar 28 05:19:36 2007
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Tue, 27 Mar 2007 22:19:36 -0500
Subject: [R]  RV: by sentence
Message-ID: <200703280319.l2S3Jamr006343@gator.dt.uh.edu>

Hello, Jorge!

This might help:

> fish.df
   Sex Length
1 male     15
2 fema     20
3 fema     17
4 fema     19
5 male     18
> str(fish.df)
'data.frame':   5 obs. of  2 variables:
 $ Sex   : Factor w/ 2 levels "fema","male": 2 1 1 1 2
 $ Length: int  15 20 17 19 18
> tapply(fish.df$Length,fish.df$Sex,mean)
    fema     male 
18.66667 16.50000 
>

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu

From: "Jorge Cornejo Donoso" <jfc10816 at ucsc.edu>
Subject: [R] RV: By sentence




Hi dear listers

	Do you know how to made a mean by one or more categorical variables?
	I can do that quite easy on SAS by in R I just can't do it E.g.
	Some fishes
	Sex 	Lenght
	male	15
	fema	20
	fema	17
	fema	19
	male  18

	So the idea is mean(Lenght) by sex, in order to have sex: Male
mean=XX Sex: Fema mean=YY
	In this case is quite easy to do it by a loop, but I have a huge DB
so is not an option. Any suggestion?

	Thanks in advance!

--

Jorge Cornejo Donoso

Universidad Austral de Chile


From Mat.Vanderklift at csiro.au  Wed Mar 28 06:30:10 2007
From: Mat.Vanderklift at csiro.au (Mat.Vanderklift at csiro.au)
Date: Wed, 28 Mar 2007 12:30:10 +0800
Subject: [R] Jackknife estimates of predict.lda success rate
Message-ID: <F83C6ACE124F3E4D83B3A90C9CA0922D26A017@exwa3-per.nexus.csiro.au>

Dear all
I have used the lda and predict functions to classify a set of objects of unknown origin. I would like to use a jackknife reclassification to assess the degree to which the outcomes deviate from that expected by chance. However, I can't find any function that allows me to do this. Any suggestions of how to generate the jackknife reclassification to assess classification accuracy? (BTW, commands I used to generate predictions appended below.)
Many thanks
Mat Vanderklift
> library(MASS)
> Sep04 <- read.table("Sep04 dataframe2.txt", header=TRUE)
> trainSep04 <- Sep04[1:125,]
> Sep04.lda <- lda(Reef ~ ., trainSep04)
> Sep04.lda
> predict.Sep04 <- predict(Sep04.lda, Sep04) $class
?


From klaster at karlin.mff.cuni.cz  Wed Mar 28 06:59:14 2007
From: klaster at karlin.mff.cuni.cz (Petr Klasterecky)
Date: Wed, 28 Mar 2007 06:59:14 +0200
Subject: [R] RV: By sentence
In-Reply-To: <auto-000112296008@silver.ucsc.edu>
References: <auto-000112296008@silver.ucsc.edu>
Message-ID: <4609F622.1040804@karlin.mff.cuni.cz>

Well, you want a mean of length BY sex, so look at by()...
See ?by
by(your.data.frame,sex,mean)

Petr

Jorge Cornejo Donoso napsal(a):
> 
> Hi dear listers
> 
> 	Do you know how to made a mean by one or more categorical variables?
> 	I can do that quite easy on SAS by in R I just can't do it E.g.
> 	Some fishes
> 	Sex 	Lenght
> 	male	15
> 	fema	20
> 	fema	17
> 	fema	19
> 	male  18
> 
> 	So the idea is mean(Lenght) by sex, in order to have sex: Male
> mean=XX Sex: Fema mean=YY
> 	In this case is quite easy to do it by a loop, but I have a huge DB
> so is not an option. Any suggestion?
> 
> 	Thanks in advance!
> 
> --
> 
> Jorge Cornejo Donoso
> 
> Universidad Austral de Chile
> 
> Portales 73, Coyhaique,CP 5950000, Region de Ais?n, Chile Tel +56 67 244520
> Fax +56 67 239377
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
Petr Klasterecky
Dept. of Probability and Statistics
Charles University in Prague
Czech Republic


From ripley at stats.ox.ac.uk  Wed Mar 28 09:08:00 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 28 Mar 2007 08:08:00 +0100 (BST)
Subject: [R] Jackknife estimates of predict.lda success rate
In-Reply-To: <F83C6ACE124F3E4D83B3A90C9CA0922D26A017@exwa3-per.nexus.csiro.au>
References: <F83C6ACE124F3E4D83B3A90C9CA0922D26A017@exwa3-per.nexus.csiro.au>
Message-ID: <Pine.LNX.4.64.0703280804470.2363@gannet.stats.ox.ac.uk>

Reposting the same message is not making it any easier for your audience 
to understand.

You need to tell us what you mean by your subject line and 'jackknife 
reclassification'.  Given these are far from standard terms, either you 
mean something else (and 'jackknife' is very frequently misused for 'leave 
one out cross-validation) or it is likely that you will need to code the 
method yourself.

On Wed, 28 Mar 2007, Mat.Vanderklift at csiro.au wrote:

> Dear all

> I have used the lda and predict functions to classify a set of objects 
> of unknown origin. I would like to use a jackknife reclassification to 
> assess the degree to which the outcomes deviate from that expected by 
> chance. However, I can't find any function that allows me to do this. 
> Any suggestions of how to generate the jackknife reclassification to 
> assess classification accuracy? (BTW, commands I used to generate 
> predictions appended below.)

> Many thanks
> Mat Vanderklift
>> library(MASS)
>> Sep04 <- read.table("Sep04 dataframe2.txt", header=TRUE)
>> trainSep04 <- Sep04[1:125,]
>> Sep04.lda <- lda(Reef ~ ., trainSep04)
>> Sep04.lda
>> predict.Sep04 <- predict(Sep04.lda, Sep04) $class
> ?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From daniel at danielberg.no  Wed Mar 28 10:09:12 2007
From: daniel at danielberg.no (Daniel Berg)
Date: Wed, 28 Mar 2007 10:09:12 +0200
Subject: [R] Replacement in an expression - can't use parse()
In-Reply-To: <Pine.LNX.4.64.0703271000290.28829@homer24.u.washington.edu>
References: <f13f1c9c0703270656t56dd736axb46b334575b136f5@mail.gmail.com>
	<46092A73.6020302@biostat.ku.dk>
	<Pine.LNX.4.64.0703271000290.28829@homer24.u.washington.edu>
Message-ID: <f13f1c9c0703280109o4a833d63pc722c088957b888e@mail.gmail.com>

Thank you very much for your good suggestions.
I have chosen to pursue the suggestion by Peter which worked like a dream :)
However, my problem is slightly more complicated still. I apologize
for not mentioning this in the initial question.

I have to do the evaluation inside a loop, not knowing explicitly
which 'u' to replace. This is given by the loop I'm in, say something
like:

i <- 2; j <- 3
eval(substitute(substitute(call,list(paste("u",i,sep="")=quote(x),paste("u",j,sep="")=1)),list(call=e[[1]])))

But this returns a syntax error.
Any further suggestions?

Regards,
Daniel

On 3/27/07, Thomas Lumley <tlumley at u.washington.edu> wrote:
> On Tue, 27 Mar 2007, Peter Dalgaard wrote:
>
> >The way around this is to add a further layer of substitute() to insert
> >the value of e:
>
> >> eval(substitute(substitute(call,list(u2=quote(x),u3=1)),list(call=e[[1]])))
> > u1 + x + 1
>
> Or eval(do.call(substitute, list(e[[1]], list(u2=quote(x),u3=1)))
>
>         -thomas
>
>


-- 
danielberg.no


From f.tamagni at sssup.it  Wed Mar 28 10:57:49 2007
From: f.tamagni at sssup.it (Federico Tamagni)
Date: Wed, 28 Mar 2007 10:57:49 +0200
Subject: [R] nlsystemfit: Errors with reproducing the manual example
Message-ID: <460A2E0D.2000902@sssup.it>

Hi everybody, I'm a newbye with lots of problems :). I'm trying to use 
nlsystemfit, but I recieve two error messages whose origin that I don't 
understand.

1) When I try to reproduce the example reported in the systemfit package 
manual, that is

library( systemfit )
data( ppine )
 hg.formula <- hg ~ exp( h0 + h1*log(tht) + h2*tht^2 + h3*elev + h4*cr)
dg.formula <- dg ~ exp( d0 + d1*log(dbh) + d2*hg + d3*cr + d4*ba )
labels <- list( "height.growth", "diameter.growth" )
inst <- ~ tht + dbh + elev + cr + ba
start.values <- c(h0=-0.5, h1=0.5, h2=-0.001, h3=0.0001, h4=0.08,
+ d0=-0.5, d1=0.009, d2=0.25, d3=0.005, d4=-0.02 )
model <- list( hg.formula, dg.formula )
model.ols <- nlsystemfit( "OLS", model, start.values, data=ppine, 
eqnlabels=labels )

then I get the following messages:
/
The following object(s) are masked _by_ .GlobalEnv :

         dg hg


        The following object(s) are masked from data ( position 3 ) :

         ba cr dbh dg elev hg smi tht

Error in array(x, c(length(x), 1), if (!is.null(names(x))) list(names(x),  :
        dim<- : invalid first argument
/


2) When I try to do something similar on my own data (called R_manuf and 
containing 3 columns named va98, ts98 and gom98) with the code

library(systemfit)
dati <- read.table("R_manuf",header=T,na.strings="nan")
attach(dati)

y1      <-  va98/ts98 - 1;
y2      <-  gom98 - va98;
eq1.formula  <-  y1.98 ~ a1*ts98^(b1-1) ;
eq2.formula  <-  y2.98 ~ a2*ts98^b2 ;
labels <- list("eq1", "eq2" );
start.values <- c(a1=-0.5,  b1=1.02, a2=-0.8, b2=0.9 );
model <- list( eq1.formula, eq2.formula);
model.ols <- nlsystemfit( "OLS",  model,  start.values,  
eqnlabels=labels  );
print(model.ols);

then I get a different error message: /

Error in eval(expr, envir, enclos) : incorrect number of subscripts on 
matrix/


Thanks for your help,


Federico





-- 
Federico Tamagni, PhD student
LEM - Laboratory of Economics and Management
Scuola Superiore Sant'Anna, Pisa, Italy
Phone: (+39)-050-883341 
Fax: (+39)-050-883344
email: f.tamagni at sssup.it
website: https://mail.sssup.it/~f.tamagni/


From sergio.della.franca at gmail.com  Wed Mar 28 11:00:20 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Wed, 28 Mar 2007 11:00:20 +0200
Subject: [R] Standardization Range
Message-ID: <b490ce570703280200s75ce28b8rf9f465b9feace9ab@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070328/ee4422a2/attachment.pl 

From dray at biomserv.univ-lyon1.fr  Wed Mar 28 11:15:54 2007
From: dray at biomserv.univ-lyon1.fr (=?ISO-8859-1?Q?St=E9phane_Dray?=)
Date: Wed, 28 Mar 2007 11:15:54 +0200
Subject: [R] Standardization Range
In-Reply-To: <b490ce570703280200s75ce28b8rf9f465b9feace9ab@mail.gmail.com>
References: <b490ce570703280200s75ce28b8rf9f465b9feace9ab@mail.gmail.com>
Message-ID: <460A324A.5020306@biomserv.univ-lyon1.fr>

Hi sergio,
Sergio Della Franca wrote:
> Dear R-Helpers,
>
>
> I want to perform a standardization of a variable with range method.
>
> i.e.:
>
>  Standardization (range) ==> (var-min(var))/(max(var)-min(var))
>
>
> Do you konw how can i develop this?
>
>   
As you do ... but don't use var which is the name of the function to 
compute variance.

Try something like :

 > stdrange <- function(x) {(x-min(x))/(max(x)-min(x))}
 > var=1:10 # not a good idea, just for fun
 > stdrange(var)
 [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667
 [8] 0.7777778 0.8888889 1.0000000

> Thank you in advance.
>
>
> Sergio Della Franca
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>   


-- 
St?phane DRAY (dray at biomserv.univ-lyon1.fr )
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - Lyon I
43, Bd du 11 Novembre 1918, 69622 Villeurbanne Cedex, France
Tel: 33 4 72 43 27 57       Fax: 33 4 72 43 13 88
http://biomserv.univ-lyon1.fr/~dray/


From daniel at danielberg.no  Wed Mar 28 11:19:04 2007
From: daniel at danielberg.no (Daniel Berg)
Date: Wed, 28 Mar 2007 11:19:04 +0200
Subject: [R] Use of 'defineVar' and 'install' in .Call
In-Reply-To: <f13f1c9c0703270156hba5b26fyb541817bed557b0c@mail.gmail.com>
References: <f13f1c9c0703270156hba5b26fyb541817bed557b0c@mail.gmail.com>
Message-ID: <f13f1c9c0703280219y46a21533k5a72b0c27dae51b8@mail.gmail.com>

Now it works, I made no changes to the functions in question, must
have been some other mistake in my code and I obviously misinterpreted
the crash report.

defineVar(install(str),mkans(x),rho) works fine. The content of the
variable 'str' is assigned in the given environment in R as it should.

I apologize that I posted this before testing it properly.

Regards,
Daniel

On 3/27/07, Daniel Berg <daniel at danielberg.no> wrote:
> Dear all,
>
> [system and version information below]
>
> I am trying to modify a C function for finding the root of an
> expression. The function is to be called from R as .Call with input
> parameters:
>
> f: expression for which we will find the root
> guesses: interval for the solution
> stol: tolerance
> rho: environment
>
> The original functions I use are:
>
> SEXP mkans(double x) {
>   SEXP ans;
>   PROTECT(ans = allocVector(REALSXP, 1));
>   REAL(ans)[0] = x;
>   UNPROTECT(1);
>   return ans;
> }
> double feval(double x, SEXP f, SEXP rho) {
>   defineVar(install("x"), mkans(x), rho);
>   return(REAL(eval(f, rho))[0]);
> }
> SEXP zero(SEXP f, SEXP guesses, SEXP stol, SEXP rho) {
>   double x0 = REAL(guesses)[0], x1 = REAL(guesses)[1], tol = REAL(stol)[0];
>   double f0, f1, fc, xc;
>   if(tol <= 0.0) error("non-positive tol value");
>   f0 = feval(x0, f, rho); f1 = feval(x1, f, rho);
>   if(f0 == 0.0) return mkans(x0);
>   if(f1 == 0.0) return mkans(x1);
>   if(f0*f1 > 0.0) error("x[0] and x[1] have the same sign");
>   for(;;) {
>     xc = 0.5*(x0+x1);
>     if(fabs(x0-x1) < tol) return mkans(xc);
>     fc = feval(xc, f, rho);
>     if(fc == 0) return mkans(xc);
>     if(f0*fc > 0.0) {
>       x0 = xc; f0 = fc;
>     }
>     else {
>       x1 = xc; f1 = fc;
>     }
>   }
> }
>
>
> This works great. However, I wish to make it more general, by
> modifying 'feval'. Given that my problem involves a data set 'u', with
> dimension (i x j), I need to assign values to 'u1', 'u2', ..., 'ui'
> via defineVar(install(...)). I tried the following:
>
> double feval(double x, double *u, int d, double v, SEXP f, SEXP rho) {
>   int i;
>   char *str1="u", str2[1001], *str3;
>   defineVar(install("x"), mkans(x), rho);
>   defineVar(install("y"), mkans(v), rho);
>   for(i=0;i<d;i++) {
>     sprintf(str2,"%d",i+1);
>     str3 = (char *)malloc((strlen(str1)+strlen(str2)+1)*sizeof(char));
>     strcpy(str3,str1);
>     strcat(str3,str2);
>     defineVar(install(str3), mkans(u[i]), rho);
>   }
>   free(str3);
>   return(REAL(eval(f,rho))[0]);
> }
>
> My R-package still compiles without errors but R crashes due to the
> defineVar command.
>
> Any suggestions to how I can do the defineVar bit?
>
> Thanks in advance.
>
> Reagards,
> Daniel Berg
>
> --------------------------------------------
> > R.Version()
> $platform
> [1] "i486-pc-linux-gnu"
> $arch
> [1] "i486"
> $os
> [1] "linux-gnu"
> $system
> [1] "i486, linux-gnu"
> $status
> [1] ""
> $major
> [1] "2"
> $minor
> [1] "3.1"
> $year
> [1] "2006"
> $month
> [1] "06"
> $day
> [1] "01"
> $`svn rev`
> [1] "38247"
> $language
> [1] "R"
> $version.string
> [1] "Version 2.3.1 (2006-06-01)"
> --------------------------------------------
>


-- 
danielberg.no


From dimitris.rizopoulos at med.kuleuven.be  Wed Mar 28 11:19:14 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 28 Mar 2007 11:19:14 +0200
Subject: [R] Standardization Range
References: <b490ce570703280200s75ce28b8rf9f465b9feace9ab@mail.gmail.com>
Message-ID: <003301c7711a$275d68f0$0540210a@www.domain>

you can still use scale() (as you have been told), look at the help 
page for more info, especially at the Arguments section, e.g.,


mat <- matrix(rnorm(100*10), 100, 10)

rng <- apply(mat, 2, range)
scale(mat, scale = rng[2, ] - rng[1, ])

or you could even use apply() directly, e.g.,

apply(mat, 2, function(x) (x - mean(x)) / diff(range(x)) )


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Sergio Della Franca" <sergio.della.franca at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, March 28, 2007 11:00 AM
Subject: [R] Standardization Range


> Dear R-Helpers,
>
>
> I want to perform a standardization of a variable with range method.
>
> i.e.:
>
> Standardization (range) ==> (var-min(var))/(max(var)-min(var))
>
>
> Do you konw how can i develop this?
>
> Thank you in advance.
>
>
> Sergio Della Franca
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From francogrex at mail.com  Wed Mar 28 11:42:48 2007
From: francogrex at mail.com (francogrex)
Date: Wed, 28 Mar 2007 02:42:48 -0700 (PDT)
Subject: [R] R equivalent of S+SeqTrial?
Message-ID: <9708877.post@talk.nabble.com>


Does anyone know of an R package that is equivalent of S+SeqTrial for
analysis of clinical trials using group sequential methods? Thanks.
-- 
View this message in context: http://www.nabble.com/R-equivalent-of-S%2BSeqTrial--tf3478858.html#a9708877
Sent from the R help mailing list archive at Nabble.com.


From sergio.della.franca at gmail.com  Wed Mar 28 12:39:21 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Wed, 28 Mar 2007 12:39:21 +0200
Subject: [R] Kmeans Ward
Message-ID: <b490ce570703280339s71d9fb0cl94fe7a943f4a1ca1@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070328/daef6446/attachment.pl 

From gyadav at ccilindia.co.in  Wed Mar 28 12:57:56 2007
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Wed, 28 Mar 2007 16:27:56 +0530
Subject: [R] Regarding Vista
Message-ID: <OFBD6509A6.420C69CA-ON652572AC.003BF675-652572AC.003C2DC9@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070328/f27638d0/attachment.pl 

From ligges at statistik.uni-dortmund.de  Wed Mar 28 13:10:11 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 28 Mar 2007 13:10:11 +0200
Subject: [R] Regarding Vista
In-Reply-To: <OFBD6509A6.420C69CA-ON652572AC.003BF675-652572AC.003C2DC9@ccilindia.co.in>
References: <OFBD6509A6.420C69CA-ON652572AC.003BF675-652572AC.003C2DC9@ccilindia.co.in>
Message-ID: <460A4D13.60009@statistik.uni-dortmund.de>



gyadav at ccilindia.co.in wrote:
> Hi All
> 
> Does R supports Vista Business and Vista Home Premium. I am planning to 
> upgrade my system. Before that I just wanted to confirm whether R will 
> work on Vista or not.

Although R Core does not officially support Vista, AFAIK, we have heard 
of working installations on 32-bit versions of Vista.
Check the mailing list archives for hints on installing additional 
packages, if that won't work at once for you.

Uwe Ligges



> Thanks for this help in advance
> -gaurav
> 
> 
> 
> 
> 
> 
>    Sayonara With Smile & With Warm Regards :-)
> 
>   G a u r a v   Y a d a v
>   Assistant Manager,
>   Economic Research & Surveillance Department,
>   Clearing Corporation Of India Limited.
> 
>   Address: 5th, 6th, 7th Floor, Trade Wing 'C',  Kamala City, S.B. Marg, 
> Mumbai - 400 013
>   Telephone(Office): - +91 022 6663 9398 ,  Mobile(Personal) (0)9821286118
>   Email(Office) :- gyadav at ccilindia.co.in ,  Email(Personal) :- 
> emailtogauravyadav at gmail.com
> 
> 
> ============================================================================================
> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and ...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From john.bullock at stanford.edu  Wed Mar 28 13:20:49 2007
From: john.bullock at stanford.edu (John Bullock)
Date: Wed, 28 Mar 2007 04:20:49 -0700
Subject: [R] eliminating panel borders from lattice plots
References: <02ee01c76ecf$773d2510$87ac0c80@WINSTON>
	<971536df0703250607m5b40b4dds806235e5de677585@mail.gmail.com>
Message-ID: <02a101c7712b$249cd9a0$87ac0c80@WINSTON>

> On 3/25/07, John Bullock
> <john.bullock at stanford.edu> wrote:
>>
>> I am trying to eliminate panel borders from my
>> lattice plots.  By default, they always print.
>> For
>> example:
>>
>>    library(lattice)
>>    x <- seq(-3,3,length=1000)
>>    y1 <- dnorm(x)
>>    y2 <- dnorm(x, sd=.5)
>>    data <- data.frame(x=rep(x,2), y=c(y,y2),
>>        panel=rep(c(1,2), each=1000))
>>    dplot <- xyplot(y~x | panel, data=data,
>> strip=F,
>>        scales=list(draw=F))
>>    print(dplot, scales=list(draw=F))
>>
>> prints borders around each panel.  I see no way
>> to get rid of them short of creating a panel
>> function and "painting over" these default
>> borders with grid.rect().  But I suspect that
>> there is an easier way -- is there?
>>
>> I searched the archives but saw nothing on this.
>> I'm running R 2.3.1 with lattice 0.13.
>>
>> Thank you,
>> --John


----- Original Message ----- 
From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
To: "John Bullock" <john.bullock at stanford.edu>
Cc: <r-help at stat.math.ethz.ch>
Sent: Sunday, March 25, 2007 6:07 AM
Subject: Re: [R] eliminating panel borders from
lattice plots


> Try adding this argument to your xyplot call:
>
>   par.settings = list(axis.line = list(col = 0))
>
> The subparameters oif axis.line are:
>
>  trellis.par.get()$axis.line
>
> in case you want to temporarily set others.
>

Thank you.  That worked beautifully.


From Corinna.Schmitt at igb.fraunhofer.de  Wed Mar 28 13:22:10 2007
From: Corinna.Schmitt at igb.fraunhofer.de (Schmitt, Corinna)
Date: Wed, 28 Mar 2007 13:22:10 +0200
Subject: [R] Import of a workspace into R
Message-ID: <8B7B0FD99E8AF541A21609104D19615882EA99@izs-xchg01.izs.fraunhofer.de>

Hallo,

Sorry for more details before but I was in a hurry. 

One workspace comes from Matlab and one should come from Bioconductor.
At the moment I can just work with the Matlab-workspace. I just know
that the data is stored in an Excel-File --> I have a table. 
I know that there exists an RMatlab packet but I could not found any
near information or description of it. 

Can anyone help me or give me additional papers or Links?

Thanks, Corinna


From gyadav at ccilindia.co.in  Wed Mar 28 13:27:58 2007
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Wed, 28 Mar 2007 16:57:58 +0530
Subject: [R] regarding time series object
Message-ID: <OF2AE76332.18BE0248-ON652572AC.003E57BA-652572AC.003EEDC4@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070328/73e9fddc/attachment.pl 

From ggrothendieck at gmail.com  Wed Mar 28 13:37:57 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 28 Mar 2007 07:37:57 -0400
Subject: [R] Replacement in an expression - can't use parse()
In-Reply-To: <f13f1c9c0703280109o4a833d63pc722c088957b888e@mail.gmail.com>
References: <f13f1c9c0703270656t56dd736axb46b334575b136f5@mail.gmail.com>
	<46092A73.6020302@biostat.ku.dk>
	<Pine.LNX.4.64.0703271000290.28829@homer24.u.washington.edu>
	<f13f1c9c0703280109o4a833d63pc722c088957b888e@mail.gmail.com>
Message-ID: <971536df0703280437h757bb5aaw523a0bc0e65e8b0@mail.gmail.com>

The solution in my post has the advantage of not using
eval or character conversions (except in setting up
L where you want such character conversions to
build up the names, as your more general sitution
shows).  The following is the same as in that post except
the line setting L in that post is replaced with the 2 lines
setting i, j and L.  Setting L in this way would also work
in conjunction with some of the other solutions too:

e <- expression(u1 + u2 + u3)
i <- 1; j <- 2; L <- list()
L[[paste("u", i, sep = "")]] <- as.name(paste("u", j, sep = ""))
as.expression(do.call(substitute, list(as.call(e), L))[[1]])

One can streamline it further by making it a function and
using assign:

subu <- function(e, i, j) {
	assign(paste("u", i, sep = ""), as.name(paste("u", j, sep = "")))
	as.expression(do.call(substitute, list(as.call(e)))[[1]])
}

e <- expression(u1 + u2 + u3)
subu(e, 1, 2)

On 3/28/07, Daniel Berg <daniel at danielberg.no> wrote:
> Thank you very much for your good suggestions.
> I have chosen to pursue the suggestion by Peter which worked like a dream :)
> However, my problem is slightly more complicated still. I apologize
> for not mentioning this in the initial question.
>
> I have to do the evaluation inside a loop, not knowing explicitly
> which 'u' to replace. This is given by the loop I'm in, say something
> like:
>
> i <- 2; j <- 3
> eval(substitute(substitute(call,list(paste("u",i,sep="")=quote(x),paste("u",j,sep="")=1)),list(call=e[[1]])))
>
> But this returns a syntax error.
> Any further suggestions?
>
> Regards,
> Daniel
>
> On 3/27/07, Thomas Lumley <tlumley at u.washington.edu> wrote:
> > On Tue, 27 Mar 2007, Peter Dalgaard wrote:
> >
> > >The way around this is to add a further layer of substitute() to insert
> > >the value of e:
> >
> > >> eval(substitute(substitute(call,list(u2=quote(x),u3=1)),list(call=e[[1]])))
> > > u1 + x + 1
> >
> > Or eval(do.call(substitute, list(e[[1]], list(u2=quote(x),u3=1)))
> >
> >         -thomas
> >
> >
>
>
> --
> danielberg.no
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Wed Mar 28 13:59:57 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 28 Mar 2007 07:59:57 -0400
Subject: [R] regarding time series object
In-Reply-To: <OF2AE76332.18BE0248-ON652572AC.003E57BA-652572AC.003EEDC4@ccilindia.co.in>
References: <OF2AE76332.18BE0248-ON652572AC.003E57BA-652572AC.003EEDC4@ccilindia.co.in>
Message-ID: <971536df0703280459w6aa4fee8s89002f401937cc11@mail.gmail.com>

I am not 100% sure of what you want but maybe this will help.  I
have modified your example data to omit one point so that there
is a holiday among the rows:

library(zoo)

# read data
Lines <- "Date    Open    High     Low CloseNifty
2004-01-01 1880.35 1917.05 1880.35    1912.25
2004-01-02 1912.25 1951.70 1911.05    1946.05
# 2004-01-05 1946.30 1969.20 1930.75    1955.00
2004-01-06 1955.10 1979.05 1908.75    1926.70
2004-01-07 1927.95 1930.95 1888.10    1916.75
2004-01-08 1918.10 1973.45 1918.10    1968.55
2004-01-09 1969.00 2014.65 1957.45    1971.90
2004-01-12 1972.00 1980.55 1936.75    1945.60
2004-01-13 1944.70 1967.85 1926.10    1963.60
2004-01-14 1987.40 1995.20 1970.10    1982.15
2004-01-15 1983.20 2000.30 1933.25    1944.45
2004-01-16 1944.15 1953.05 1887.10    1900.65
2004-01-19 1901.90 1943.10 1874.95    1935.35
2004-01-20 1928.80 1957.65 1876.85    1893.25
2004-01-21 1895.45 1899.55 1811.35    1824.60
"
# replace next line with: z <- read.zoo("myfile", header = TRUE)
z <- read.zoo(textConnection(Lines), header = TRUE)

# add NA wherever there is a missing day
z <- as.zoo(as.ts(z))
time(z) <- as.Date(time(z))
z

# remove weekends
z <- z[format(time(z), "%w") %in% 1:5]
z

# interpolate
z <- zoo(na.approx(coredata(z)), time(z))
z



On 3/28/07, gyadav at ccilindia.co.in <gyadav at ccilindia.co.in> wrote:
>
> Hello All
>
> I have time series data like given hereinbelow :-
> The data is having dates. I tried using ts() but it requires frequency. As
> i have holidays, saturdays sunday market closed, and also sometimes the
> data may not be recorded. I want to create a time series object but i want
> that where ever the holiday is the except saturday/sunday then it should
> impute the mean of previous and the next working day. I want to do this so
> that i can perfectly say that 5 obs is my weekly frequency, 25 is my
> monthly etc :- Please help in this regards
>
> > tui[1:15,]
>         Date    Open    High     Low CloseNifty
> 1  2004-01-01 1880.35 1917.05 1880.35    1912.25
> 2  2004-01-02 1912.25 1951.70 1911.05    1946.05
> 3  2004-01-05 1946.30 1969.20 1930.75    1955.00
> 4  2004-01-06 1955.10 1979.05 1908.75    1926.70
> 5  2004-01-07 1927.95 1930.95 1888.10    1916.75
> 6  2004-01-08 1918.10 1973.45 1918.10    1968.55
> 7  2004-01-09 1969.00 2014.65 1957.45    1971.90
> 8  2004-01-12 1972.00 1980.55 1936.75    1945.60
> 9  2004-01-13 1944.70 1967.85 1926.10    1963.60
> 10 2004-01-14 1987.40 1995.20 1970.10    1982.15
> 11 2004-01-15 1983.20 2000.30 1933.25    1944.45
> 12 2004-01-16 1944.15 1953.05 1887.10    1900.65
> 13 2004-01-19 1901.90 1943.10 1874.95    1935.35
> 14 2004-01-20 1928.80 1957.65 1876.85    1893.25
> 15 2004-01-21 1895.45 1899.55 1811.35    1824.60
> >
>
> Thanks a lot
> -gaurav
>
> ============================================================================================
> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and ...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From albmont at centroin.com.br  Wed Mar 28 14:18:01 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Wed, 28 Mar 2007 10:18:01 -0200
Subject: [R] Help with grep (and similar functions)
Message-ID: <20070328121517.M98908@centroin.com.br>

This works:

grep("([A-Za-z]*) ", "Aaaa 3 x 0 Bbbb") # 1

This also works:

grep("([A-Za-z]*) ", "Aaaa 3 x 0 Bbbb", value=T) # Aaaa 3 x 0 Bbbb

However, I want a grep that returns the _matched_ pattern, which, in this 
case, would be Aaaa. How can I do it?

Alberto Monteiro


From cincinattikid at bigpond.com  Wed Mar 28 14:22:06 2007
From: cincinattikid at bigpond.com (Alfonso Sammassimo)
Date: Wed, 28 Mar 2007 22:22:06 +1000
Subject: [R] aggregating data with Zoo
Message-ID: <00c201c77133$b2f68680$0300a8c0@Vaio>

Is there a way of aggregating 'zoo' daily data according to day of week? eg 
all Thursdays

I came across the 'nextfri' function in the documentation but am unsure how 
to change this so any day of week can be aggregated.

I have used POSIX to arrange the data (not as 'zoo' series) according to day 
of week, but am curious if I've missed if a similar option available with 
zoo.

Thank you for any help,

Alf Sammassimo
Melbourne, Australia


From christian.ritter at shell.com  Wed Mar 28 14:26:20 2007
From: christian.ritter at shell.com (christian.ritter at shell.com)
Date: Wed, 28 Mar 2007 14:26:20 +0200
Subject: [R] Regarding Vista
In-Reply-To: <460A4D13.60009@statistik.uni-dortmund.de>
Message-ID: <156CDC8CCFD1894295D2907F16337A4801420ACA@bru-s-006.europe.shell.com>

Hi folks,

The Vista issue is not innocent as it threatens the life of R within large corporations. So any posts on how R runs under Vista and what has to be done to make it work and what cannot be done etc will be very useful. 

Chris

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Uwe Ligges
Sent: Wednesday, 28 March, 2007 1:10 PM
To: gyadav at ccilindia.co.in
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Regarding Vista




gyadav at ccilindia.co.in wrote:
> Hi All
> 
> Does R supports Vista Business and Vista Home Premium. I am planning to 
> upgrade my system. Before that I just wanted to confirm whether R will 
> work on Vista or not.

Although R Core does not officially support Vista, AFAIK, we have heard 
of working installations on 32-bit versions of Vista.
Check the mailing list archives for hints on installing additional 
packages, if that won't work at once for you.

Uwe Ligges



> Thanks for this help in advance
> -gaurav
> 
> 
> 
> 
> 
> 
>    Sayonara With Smile & With Warm Regards :-)
> 
>   G a u r a v   Y a d a v
>   Assistant Manager,
>   Economic Research & Surveillance Department,
>   Clearing Corporation Of India Limited.
> 
>   Address: 5th, 6th, 7th Floor, Trade Wing 'C',  Kamala City, S.B. Marg, 
> Mumbai - 400 013
>   Telephone(Office): - +91 022 6663 9398 ,  Mobile(Personal) (0)9821286118
>   Email(Office) :- gyadav at ccilindia.co.in ,  Email(Personal) :- 
> emailtogauravyadav at gmail.com
> 
> 
> ============================================================================================
> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and ...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From gunther.hoening at ukmainz.de  Wed Mar 28 14:32:30 2007
From: gunther.hoening at ukmainz.de (=?iso-8859-1?Q?H=F6ning=2C_Gunther?=)
Date: Wed, 28 Mar 2007 14:32:30 +0200
Subject: [R] Optimal workstation architecture for running R / Bioconductor
Message-ID: <399CF0758B16ED48B6B3A44E4537D393DA83@c2-s1.it.klinik.uni-mainz.de>

Dear list,

I use a lot of data files (at the moment 35 files by 34 MB each) for analysis. 
My own processing steps  will (hopefully) improve with increasing number of files I have.
So I am looking for an suitable workstation configuration, e.g. 64bit architecture, 16 GB RAM.... etc.

Does anyone have any experience with setting up such a system ?

Gunther


From kaloytyn at cc.jyu.fi  Wed Mar 28 14:46:28 2007
From: kaloytyn at cc.jyu.fi (kaloytyn at cc.jyu.fi)
Date: Wed, 28 Mar 2007 15:46:28 +0300 (EEST)
Subject: [R] (no subject)
Message-ID: <1389.130.234.33.111.1175085988.squirrel@webmail3.cc.jyu.fi>

Hallo,

I'm trying to sample a matrix with simple random sampling without
replacement but seem to have a problem with the matrix length. Both
sample() and srswor() use length() which returns the number of columns in
matrix. This means that to function it seems that the sample size exceeds
the matrix length. I need to sample the whole matrix for there are
auxiliary variables I need for further sample processing. Ay help is much
appreciated.

Regards,
Katja L?ytynoja


From ggrothendieck at gmail.com  Wed Mar 28 14:51:15 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 28 Mar 2007 08:51:15 -0400
Subject: [R] Help with grep (and similar functions)
In-Reply-To: <20070328121517.M98908@centroin.com.br>
References: <20070328121517.M98908@centroin.com.br>
Message-ID: <971536df0703280551n155bc3bfl692fa1037cdbf495@mail.gmail.com>

Try this:

library(gsubfn)
pat <- "([[:upper:]][[:lower:]]*) "
s <- "Aaaa 3 x 0 Bbbb"

strapply(s, pat, backref =-1)[[1]]

or (not quite as general but works in this case):

pat <- "([[:upper:]][[:lower:]]*) "
s <- "Aaaa 3 x 0 Bbbb"

s.idx <- gregexpr(pat, s)[[1]]
substring(s, s.idx, s.idx + attr(s.idx, "match.length") - 2)

On 3/28/07, Alberto Monteiro <albmont at centroin.com.br> wrote:
> This works:
>
> grep("([A-Za-z]*) ", "Aaaa 3 x 0 Bbbb") # 1
>
> This also works:
>
> grep("([A-Za-z]*) ", "Aaaa 3 x 0 Bbbb", value=T) # Aaaa 3 x 0 Bbbb
>
> However, I want a grep that returns the _matched_ pattern, which, in this
> case, would be Aaaa. How can I do it?
>
> Alberto Monteiro
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From marc_schwartz at comcast.net  Wed Mar 28 14:53:14 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 28 Mar 2007 07:53:14 -0500
Subject: [R] R equivalent of S+SeqTrial?
In-Reply-To: <9708877.post@talk.nabble.com>
References: <9708877.post@talk.nabble.com>
Message-ID: <1175086394.14657.7.camel@Bellerophon>

On Wed, 2007-03-28 at 02:42 -0700, francogrex wrote:
> Does anyone know of an R package that is equivalent of S+SeqTrial for
> analysis of clinical trials using group sequential methods? Thanks.

I don't know that there are fully functional equivalents, but you might
want to look at the following:

1. The GroupSeq package on CRAN:

  http://cran.r-project.org/src/contrib/Descriptions/GroupSeq.html


2. The ldBands() function in Frank Harrell's Hmisc package on CRAN:

  http://biostat.mc.vanderbilt.edu/s/Hmisc/html/ldBands.html

This also requires the ld98 executable, which is available from:

  http://www.biostat.wisc.edu/landemets/


HTH,

Marc Schwartz


From ggrothendieck at gmail.com  Wed Mar 28 14:55:30 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 28 Mar 2007 08:55:30 -0400
Subject: [R] aggregating data with Zoo
In-Reply-To: <00c201c77133$b2f68680$0300a8c0@Vaio>
References: <00c201c77133$b2f68680$0300a8c0@Vaio>
Message-ID: <971536df0703280555y248e6ba6rcbcbfd188b308086@mail.gmail.com>

Please read the last line on every post to r-help.

On 3/28/07, Alfonso Sammassimo <cincinattikid at bigpond.com> wrote:
> Is there a way of aggregating 'zoo' daily data according to day of week? eg
> all Thursdays
>
> I came across the 'nextfri' function in the documentation but am unsure how
> to change this so any day of week can be aggregated.
>
> I have used POSIX to arrange the data (not as 'zoo' series) according to day
> of week, but am curious if I've missed if a similar option available with
> zoo.
>
> Thank you for any help,
>
> Alf Sammassimo
> Melbourne, Australia
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From marc_schwartz at comcast.net  Wed Mar 28 14:57:58 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 28 Mar 2007 07:57:58 -0500
Subject: [R] Import of a workspace into R
In-Reply-To: <8B7B0FD99E8AF541A21609104D19615882EA99@izs-xchg01.izs.fraunhofer.de>
References: <8B7B0FD99E8AF541A21609104D19615882EA99@izs-xchg01.izs.fraunhofer.de>
Message-ID: <1175086678.14657.11.camel@Bellerophon>

On Wed, 2007-03-28 at 13:22 +0200, Schmitt, Corinna wrote:
> Hallo,
> 
> Sorry for more details before but I was in a hurry. 
> 
> One workspace comes from Matlab and one should come from Bioconductor.
> At the moment I can just work with the Matlab-workspace. I just know
> that the data is stored in an Excel-File --> I have a table. 
> I know that there exists an RMatlab packet but I could not found any
> near information or description of it. 
> 
> Can anyone help me or give me additional papers or Links?
> 
> Thanks, Corinna

There are various ways to read data stored in an Excel file and these
are detailed in the R Data Import/Export manual.


For the Matlab files, see the R.matlab package on CRAN:

http://cran.r-project.org/src/contrib/Descriptions/R.matlab.html

Using:

  RSiteSearch("matlab")

would have revealed the above.


Since Bioconductor is built upon R, those files should not be an issue.

HTH,

Marc Schwartz


From ripley at stats.ox.ac.uk  Wed Mar 28 15:08:12 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 28 Mar 2007 14:08:12 +0100 (BST)
Subject: [R] Regarding Vista
In-Reply-To: <156CDC8CCFD1894295D2907F16337A4801420ACA@bru-s-006.europe.shell.com>
References: <156CDC8CCFD1894295D2907F16337A4801420ACA@bru-s-006.europe.shell.com>
Message-ID: <Pine.LNX.4.64.0703281348110.12465@gannet.stats.ox.ac.uk>

On Wed, 28 Mar 2007, christian.ritter at shell.com wrote:

> The Vista issue is not innocent as it threatens the life of R within 
> large corporations. So any posts on how R runs under Vista and what has 
> to be done to make it work and what cannot be done etc will be very 
> useful.

See the rw-FAQ in the R-2.5.0 alpha versions and later.  That contains all 
that we have seen reported, wiht documented workarounds.

My understanding is that there are no Vista issues per se with R, but 
there are permission issues just as there are under 2000/XP if set up to 
the same levels of security as are the defaults in Vista.  (Further, 
that agrees with advice from someone within Microsoft who uses R.)

We do not intend to roll out Vista for several months, and it would 
surprise me if 'large corporations' would do so without ensuring that 
there is full software compatibility.

>
> Chris
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Uwe Ligges
> Sent: Wednesday, 28 March, 2007 1:10 PM
> To: gyadav at ccilindia.co.in
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Regarding Vista
>
>
>
>
> gyadav at ccilindia.co.in wrote:
>> Hi All
>>
>> Does R supports Vista Business and Vista Home Premium. I am planning to
>> upgrade my system. Before that I just wanted to confirm whether R will
>> work on Vista or not.
>
> Although R Core does not officially support Vista, AFAIK, we have heard
> of working installations on 32-bit versions of Vista.
> Check the mailing list archives for hints on installing additional
> packages, if that won't work at once for you.
>
> Uwe Ligges
>
>
>
>> Thanks for this help in advance
>> -gaurav
>>
>>
>>
>>
>>
>>
>>    Sayonara With Smile & With Warm Regards :-)
>>
>>   G a u r a v   Y a d a v
>>   Assistant Manager,
>>   Economic Research & Surveillance Department,
>>   Clearing Corporation Of India Limited.
>>
>>   Address: 5th, 6th, 7th Floor, Trade Wing 'C',  Kamala City, S.B. Marg,
>> Mumbai - 400 013
>>   Telephone(Office): - +91 022 6663 9398 ,  Mobile(Personal) (0)9821286118
>>   Email(Office) :- gyadav at ccilindia.co.in ,  Email(Personal) :-
>> emailtogauravyadav at gmail.com
>>
>>
>> ============================================================================================
>> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and ...{{dropped}}
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From aa2007r at gmail.com  Wed Mar 28 15:15:30 2007
From: aa2007r at gmail.com (AA)
Date: Wed, 28 Mar 2007 09:15:30 -0400
Subject: [R] line style outliers in boxplot
References: <034801c770c9$3983e0f0$3927a8c0@treesdalellc.net>
	<971536df0703271649m297b1c77q2d35362844b64968@mail.gmail.com>
Message-ID: <03e001c7713b$2959ab20$3927a8c0@treesdalellc.net>

Thank you Gabor. This works. And you are right, bxp is actually
on the boxplot help page. I am sorry I missed it. Thanks again.
A.

----- Original Message ----- 
From: "Gabor Grothendieck" <ggrothendieck at gmail.com>
To: "AA" <aa2007r at gmail.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Tuesday, March 27, 2007 7:49 PM
Subject: Re: [R] line style outliers in boxplot


> As indicated in ?boxplot it passes certain arguments to bxp so check out
> ?bxp where you will find various out... arguments:
>
> boxplot(c(1:10, 20), outlty = 2, outcex = 0 )
>
>
> On 3/27/07, AA <aa2007r at gmail.com> wrote:
>> Dear Users
>>
>> Is there any way to generate lines instead of points for outliers in the 
>> boxplot
>> function?
>> Thanks
>>
>> AA.
>>        [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


From albmont at centroin.com.br  Wed Mar 28 15:20:04 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Wed, 28 Mar 2007 11:20:04 -0200
Subject: [R] Help with grep (and similar functions)
In-Reply-To: <971536df0703280551n155bc3bfl692fa1037cdbf495@mail.gmail.com>
References: <20070328121517.M98908@centroin.com.br>
	<971536df0703280551n155bc3bfl692fa1037cdbf495@mail.gmail.com>
Message-ID: <20070328130149.M72321@centroin.com.br>

Gabor Grothendieck wrote:
>
> Try this:
> 
> library(gsubfn)
> pat <- "([[:upper:]][[:lower:]]*) "
> s <- "Aaaa 3 x 0 Bbbb"
> 
> strapply(s, pat, backref =-1)[[1]]
> 
Ok, I think I got it.

For example, if I want to retrieve Aaaa and Bbbb (which
could be any strings) or the numbers 3 and 0, I would do
this:

library(gsubfn)
pat <- "^([a-zA-Z ]+) ([1-9]*[0-9]) x ([1-9]*[0-9]) ([a-zA-Z ]+)$"
s <- "My team 3 x 0 the team from Outer Space"
x <- strapply(s, pat, c)[[1]]

Then x[1] is s, x[2] is "My team", x[3] is "3", x[4] is "0"
and x[5] is "the team from Outer Space".

Thanks.

Alberto Monteiro


From christian.ritter at shell.com  Wed Mar 28 15:27:35 2007
From: christian.ritter at shell.com (christian.ritter at shell.com)
Date: Wed, 28 Mar 2007 15:27:35 +0200
Subject: [R] Regarding Vista
In-Reply-To: <Pine.LNX.4.64.0703281348110.12465@gannet.stats.ox.ac.uk>
Message-ID: <156CDC8CCFD1894295D2907F16337A4801420ACB@bru-s-006.europe.shell.com>

Thanks Brian. 

Working for one of these large corporations, I receive a considerable amount of mails requesting me to verify whether software I use (and which was made avalailable via company wide installers on my request) is Vista compatible. It's not that these big companies will roll out Vista right away, but they want to be sure that if they do, things will go fine ... and also to weed out software (all those who cannot prove Vista compatibility go on the black list until they get cleared). With respect to R and Vista, I guess I'll have to get my hand on a machine running Vista on which I have administrator privileges. 

Chris.

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: Wednesday, 28 March, 2007 3:08 PM
To: Ritter, Christian C GSMCIL-GSTMS/2
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Regarding Vista


On Wed, 28 Mar 2007, christian.ritter at shell.com wrote:

> The Vista issue is not innocent as it threatens the life of R within 
> large corporations. So any posts on how R runs under Vista and what has 
> to be done to make it work and what cannot be done etc will be very 
> useful.

See the rw-FAQ in the R-2.5.0 alpha versions and later.  That contains all 
that we have seen reported, wiht documented workarounds.

My understanding is that there are no Vista issues per se with R, but 
there are permission issues just as there are under 2000/XP if set up to 
the same levels of security as are the defaults in Vista.  (Further, 
that agrees with advice from someone within Microsoft who uses R.)

We do not intend to roll out Vista for several months, and it would 
surprise me if 'large corporations' would do so without ensuring that 
there is full software compatibility.

>
> Chris
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Uwe Ligges
> Sent: Wednesday, 28 March, 2007 1:10 PM
> To: gyadav at ccilindia.co.in
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Regarding Vista
>
>
>
>
> gyadav at ccilindia.co.in wrote:
>> Hi All
>>
>> Does R supports Vista Business and Vista Home Premium. I am planning to
>> upgrade my system. Before that I just wanted to confirm whether R will
>> work on Vista or not.
>
> Although R Core does not officially support Vista, AFAIK, we have heard
> of working installations on 32-bit versions of Vista.
> Check the mailing list archives for hints on installing additional
> packages, if that won't work at once for you.
>
> Uwe Ligges
>
>
>
>> Thanks for this help in advance
>> -gaurav
>>
>>
>>
>>
>>
>>
>>    Sayonara With Smile & With Warm Regards :-)
>>
>>   G a u r a v   Y a d a v
>>   Assistant Manager,
>>   Economic Research & Surveillance Department,
>>   Clearing Corporation Of India Limited.
>>
>>   Address: 5th, 6th, 7th Floor, Trade Wing 'C',  Kamala City, S.B. Marg,
>> Mumbai - 400 013
>>   Telephone(Office): - +91 022 6663 9398 ,  Mobile(Personal) (0)9821286118
>>   Email(Office) :- gyadav at ccilindia.co.in ,  Email(Personal) :-
>> emailtogauravyadav at gmail.com
>>
>>
>> ============================================================================================
>> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and ...{{dropped}}
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From marc_schwartz at comcast.net  Wed Mar 28 15:34:43 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 28 Mar 2007 08:34:43 -0500
Subject: [R] Samplng from a matrix (was: no subject)
In-Reply-To: <1389.130.234.33.111.1175085988.squirrel@webmail3.cc.jyu.fi>
References: <1389.130.234.33.111.1175085988.squirrel@webmail3.cc.jyu.fi>
Message-ID: <1175088883.14657.25.camel@Bellerophon>

On Wed, 2007-03-28 at 15:46 +0300, kaloytyn at cc.jyu.fi wrote:
> Hallo,
> 
> I'm trying to sample a matrix with simple random sampling without
> replacement but seem to have a problem with the matrix length. Both
> sample() and srswor() use length() which returns the number of columns in
> matrix. This means that to function it seems that the sample size exceeds
> the matrix length. I need to sample the whole matrix for there are
> auxiliary variables I need for further sample processing. Ay help is much
> appreciated.
> 
> Regards,
> Katja L?ytynoja

First, please use an informative subject line in your posts to aid in
folks searching the list archives.

It is not clear whether you want to randomly sample individual values in
the matrix, random rows or random columns.

Using:

mat <- matrix(1:64, ncol = 8)

> mat
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
[1,]    1    9   17   25   33   41   49   57
[2,]    2   10   18   26   34   42   50   58
[3,]    3   11   19   27   35   43   51   59
[4,]    4   12   20   28   36   44   52   60
[5,]    5   13   21   29   37   45   53   61
[6,]    6   14   22   30   38   46   54   62
[7,]    7   15   23   31   39   47   55   63
[8,]    8   16   24   32   40   48   56   64



1. Sample 10 values:

> sample(mat, 10)
 [1] 28  5 34 43 44 41 11 35 25 30


2. Sample 4 rows out of the 8:

> mat[sample(8, 4), ]
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
[1,]    4   12   20   28   36   44   52   60
[2,]    8   16   24   32   40   48   56   64
[3,]    3   11   19   27   35   43   51   59
[4,]    5   13   21   29   37   45   53   61


3. Sample 4 columns out of the 8:

> mat[, sample(8, 4)]
     [,1] [,2] [,3] [,4]
[1,]   17   57    9   41
[2,]   18   58   10   42
[3,]   19   59   11   43
[4,]   20   60   12   44
[5,]   21   61   13   45
[6,]   22   62   14   46
[7,]   23   63   15   47
[8,]   24   64   16   48


The key is to use sample() to generate random indices into the matrix
and to use matrix indexing appropriately.

HTH,

Marc Schwartz


From jasonshi510 at hotmail.com  Wed Mar 28 15:44:51 2007
From: jasonshi510 at hotmail.com (Xin)
Date: Wed, 28 Mar 2007 14:44:51 +0100
Subject: [R] scale of gamma distribution
Message-ID: <BAY117-DAV15C2236B26CBFD35085E9BF06D0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070328/f73144db/attachment.pl 

From P.Dalgaard at biostat.ku.dk  Wed Mar 28 16:06:03 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 28 Mar 2007 16:06:03 +0200
Subject: [R] Reading config data from text files
In-Reply-To: <20070327211208.M45630@centroin.com.br>
References: <20070327152422.52705.qmail@web52404.mail.re2.yahoo.com>
	<20070327211208.M45630@centroin.com.br>
Message-ID: <460A764B.2030002@biostat.ku.dk>

Alberto Monteiro wrote:
> Pete Cap wrote:
>   
>> I'm writing a tcl/tk gui tool to wrap around RMySQL for some co-workers.
>>
>>     
> Good luck; I find the documentation on the library(tcltk) very
> poor, lacking examples for most of the functions.
>
>   
Well, they are interface functions, and what you really need is
documentation for what they interface to, i.e. generic tcl and tk docs,
plus some general feeling for how the interface glue works in order to
make the appropriate translation.

For examples of typical usage,  look at  James Wettenhall's pages at 
http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/

> Alberto Monteiro
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From Jan.Kleinn at partnerre.com  Wed Mar 28 16:10:49 2007
From: Jan.Kleinn at partnerre.com (Jan.Kleinn at partnerre.com)
Date: Wed, 28 Mar 2007 16:10:49 +0200
Subject: [R] changes in behaviour of Rcmd SHLIB since 2.4.0
Message-ID: <OFF9311C1E.A47B6A90-ONC12572AC.004C8E8E-C12572AC.004E4A18@partnerre.com>


Dear all,

I have some C code using the zlib uncompress. It compiles fine under Win XP
(with SP2) with R up to 2.3.1 and generates four files (Makedeps, *.d,
*.dll, and *.o):

C:\Data\R-packages\cmg>rcmd SHLIB cmg.c -o cmg.dll
latex: not found
making cmg.d from cmg.c
gcc   -IC:/PROGRA~1/R/R-2.3.1/include -Wall -O2   -c cmg.c -o cmg.o
cmg.c: In function `readcmg':
cmg.c:33: warning: assignment from incompatible pointer type
cmg.c:40: warning: assignment from incompatible pointer type
cmg.c:50: warning: assignment from incompatible pointer type
gcc  -shared -s  -o cmg.dll cmg.def cmg.o  -LC:/PROGRA~1/R/R-2.3.1/bin
-lR

With R 2.4.0 and later, the compilation fails at the linking, so the four
files generated are Makedeps, *.d, *.def, and *.o:

C:\Data\R-packages\cmg>rcmd SHLIB cmg.c -o cmg.dll
latex: not found
making cmg.d from cmg.c
gcc   -IC:/PROGRA~1/R/R-2.4.1/include  -Wall -O2 -std=gnu99   -c cmg.c -o
cmg.o
cmg.c: In function `readcmg':
cmg.c:33: warning: assignment from incompatible pointer type
cmg.c:40: warning: assignment from incompatible pointer type
cmg.c:50: warning: assignment from incompatible pointer type
gcc  -shared -s  -o cmg.dll cmg.def cmg.o  -LC:/PROGRA~1/R/R-2.4.1/bin
-lR
cmg.o:cmg.c:(.text+0xc0): undefined reference to `uncompress'
collect2: ld returned 1 exit status
make: *** [cmg.dll] Error 1

I have seen the page by Duncan Murdoch
(http://www.murdoch-sutherland.com/Rtools/), that since 2.4.0 you need
MinGW runtime 3.11 or higher, so I reinstalled the whole MinGW (runtime now
3.11, gcc core and C++ compiler now 3.4.5) and ActivePerl (now 5.8.8. build
820) but the problem persists.

Any ideas or hints on how I could compile my C code with the current
version of R are highly welcome.

Many thanks in advance and best regards, Jan



DISCLAIMER: This e-mail contains information solely intended...{{dropped}}


From sergio.della.franca at gmail.com  Wed Mar 28 16:21:51 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Wed, 28 Mar 2007 16:21:51 +0200
Subject: [R] substitute NA values
Message-ID: <b490ce570703280721t75c65230r898a4322c12ac129@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070328/95a9e643/attachment.pl 

From yseiler at bluewin.ch  Wed Mar 28 16:30:44 2007
From: yseiler at bluewin.ch (yseiler at bluewin.ch)
Date: Wed, 28 Mar 2007 14:30:44 +0000 (GMT)
Subject: [R] fitting data with conditions
Message-ID: <6318918.328591175092244352.JavaMail.webmail@ps8zhh.bluewin.ch>

Mich besch?ftig folgende Fragestellung. Ich kenne die Verteilung 
(lognormal) zus?tzlich weiss ich das 99%, das 90% und das 1% Quantil. 
Gibt es in R eine M?glichkeit die Lognormalverteilung zu finden, das 
heisst den korrespondierenden logmean und logsd?

Vielen Dank f?r ihre Hilfe
Gruss
Yvonne


From Jeff.Lusk at ngpc.ne.gov  Wed Mar 28 16:34:37 2007
From: Jeff.Lusk at ngpc.ne.gov (Jeff Lusk)
Date: Wed, 28 Mar 2007 09:34:37 -0500
Subject: [R] Problem with adding packages to default start-up list
Message-ID: <WorldClient-F200703280934.AA34370459@ngpc.ne.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070328/8fd78354/attachment.pl 

From marc_schwartz at comcast.net  Wed Mar 28 16:36:20 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 28 Mar 2007 09:36:20 -0500
Subject: [R] substitute NA values
In-Reply-To: <b490ce570703280721t75c65230r898a4322c12ac129@mail.gmail.com>
References: <b490ce570703280721t75c65230r898a4322c12ac129@mail.gmail.com>
Message-ID: <1175092580.14657.31.camel@Bellerophon>

On Wed, 2007-03-28 at 16:21 +0200, Sergio Della Franca wrote:
> Dearl R-Helpers,
> 
> 
> I have the following data set:
> 
> YEAR   PRODUCTS
> 1990     2478
> 1995     3192
> 2000     NA
> 2005     1594
> 
> 
> I wanto to replace NA values, in the PRODUCTS column, with 0.
> 
> 
> How can i obtain this?
> 
> 
> Thak you in advance.
> 
> 
> Sergio Della Franca

Several ways:

1. Using replace():

  DF$PRODUCTS <- replace(DF$PRODUCTS, is.na(DF$PRODUCTS), 0)


2. Using regular indexing:

 DF$PRODUCTS[is.na(DF$PRODUCTS)] <- 0


See ?replace and ?is.na

That being said, be very cautious about doing this. Most R functions are
designed to handle NA values in very predictable ways, but not so with 0
values.  See ?NA for more information.

For example:

> DF
  YEAR PRODUCTS
1 1990     2478
2 1995     3192
3 2000       NA
4 2005     1594

> mean(DF$PRODUCTS)
[1] NA

> mean(DF$PRODUCTS, na.rm = TRUE)
[1] 2421.333


Now with:

> DF
  YEAR PRODUCTS
1 1990     2478
2 1995     3192
3 2000        0
4 2005     1594


> mean(DF$PRODUCTS)
[1] 1816


HTH,

Marc Schwartz


From ligges at statistik.uni-dortmund.de  Wed Mar 28 16:36:32 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 28 Mar 2007 16:36:32 +0200
Subject: [R] changes in behaviour of Rcmd SHLIB since 2.4.0
In-Reply-To: <OFF9311C1E.A47B6A90-ONC12572AC.004C8E8E-C12572AC.004E4A18@partnerre.com>
References: <OFF9311C1E.A47B6A90-ONC12572AC.004C8E8E-C12572AC.004E4A18@partnerre.com>
Message-ID: <460A7D70.9040908@statistik.uni-dortmund.de>

Since R-2.4.0 less symbols are exported from R.dll.

Uwe Ligges


Jan.Kleinn at partnerre.com wrote:
> Dear all,
> 
> I have some C code using the zlib uncompress. It compiles fine under Win XP
> (with SP2) with R up to 2.3.1 and generates four files (Makedeps, *.d,
> *.dll, and *.o):
> 
> C:\Data\R-packages\cmg>rcmd SHLIB cmg.c -o cmg.dll
> latex: not found
> making cmg.d from cmg.c
> gcc   -IC:/PROGRA~1/R/R-2.3.1/include -Wall -O2   -c cmg.c -o cmg.o
> cmg.c: In function `readcmg':
> cmg.c:33: warning: assignment from incompatible pointer type
> cmg.c:40: warning: assignment from incompatible pointer type
> cmg.c:50: warning: assignment from incompatible pointer type
> gcc  -shared -s  -o cmg.dll cmg.def cmg.o  -LC:/PROGRA~1/R/R-2.3.1/bin
> -lR
> 
> With R 2.4.0 and later, the compilation fails at the linking, so the four
> files generated are Makedeps, *.d, *.def, and *.o:
> 
> C:\Data\R-packages\cmg>rcmd SHLIB cmg.c -o cmg.dll
> latex: not found
> making cmg.d from cmg.c
> gcc   -IC:/PROGRA~1/R/R-2.4.1/include  -Wall -O2 -std=gnu99   -c cmg.c -o
> cmg.o
> cmg.c: In function `readcmg':
> cmg.c:33: warning: assignment from incompatible pointer type
> cmg.c:40: warning: assignment from incompatible pointer type
> cmg.c:50: warning: assignment from incompatible pointer type
> gcc  -shared -s  -o cmg.dll cmg.def cmg.o  -LC:/PROGRA~1/R/R-2.4.1/bin
> -lR
> cmg.o:cmg.c:(.text+0xc0): undefined reference to `uncompress'
> collect2: ld returned 1 exit status
> make: *** [cmg.dll] Error 1
> 
> I have seen the page by Duncan Murdoch
> (http://www.murdoch-sutherland.com/Rtools/), that since 2.4.0 you need
> MinGW runtime 3.11 or higher, so I reinstalled the whole MinGW (runtime now
> 3.11, gcc core and C++ compiler now 3.4.5) and ActivePerl (now 5.8.8. build
> 820) but the problem persists.
> 
> Any ideas or hints on how I could compile my C code with the current
> version of R are highly welcome.
> 
> Many thanks in advance and best regards, Jan
> 
> 
> 
> DISCLAIMER: This e-mail contains information solely intended...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Wed Mar 28 16:38:24 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 28 Mar 2007 16:38:24 +0200
Subject: [R] substitute NA values
In-Reply-To: <b490ce570703280721t75c65230r898a4322c12ac129@mail.gmail.com>
References: <b490ce570703280721t75c65230r898a4322c12ac129@mail.gmail.com>
Message-ID: <460A7DE0.5010102@statistik.uni-dortmund.de>

See ?is.na and use its result for indexing.

Uwe Ligges

Sergio Della Franca wrote:
> Dearl R-Helpers,
> 
> 
> I have the following data set:
> 
> YEAR   PRODUCTS
> 1990     2478
> 1995     3192
> 2000     NA
> 2005     1594
> 
> 
> I wanto to replace NA values, in the PRODUCTS column, with 0.
> 
> 
> How can i obtain this?
> 
> 
> Thak you in advance.
> 
> 
> Sergio Della Franca
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Wed Mar 28 16:46:06 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 28 Mar 2007 15:46:06 +0100 (BST)
Subject: [R] changes in behaviour of Rcmd SHLIB since 2.4.0
In-Reply-To: <OFF9311C1E.A47B6A90-ONC12572AC.004C8E8E-C12572AC.004E4A18@partnerre.com>
References: <OFF9311C1E.A47B6A90-ONC12572AC.004C8E8E-C12572AC.004E4A18@partnerre.com>
Message-ID: <Pine.LNX.4.64.0703281540450.19241@gannet.stats.ox.ac.uk>

On Wed, 28 Mar 2007, Jan.Kleinn at partnerre.com wrote:

>
> Dear all,
>
> I have some C code using the zlib uncompress. It compiles fine under Win XP
> (with SP2) with R up to 2.3.1 and generates four files (Makedeps, *.d,
> *.dll, and *.o):

No one every said that zlib interface would be exported from R, and as 
from R 2.4.0 it is not on those platforms where exports can be controlled.
`Wrting R Extensions' has always warned you about making use of entry 
points not in the API, so this should have come as no surprise to you, and 
CHANGES for 2.4.0 said

   Entry points which are hidden in libR.so on Unix are now also hidden
   in R.dll.  (Just over half the entry points are hidden, which speeds up
   loading.)

Please use the appropriate list for programming questions (as documented 
in the R posting guide).


> C:\Data\R-packages\cmg>rcmd SHLIB cmg.c -o cmg.dll
> latex: not found
> making cmg.d from cmg.c
> gcc   -IC:/PROGRA~1/R/R-2.3.1/include -Wall -O2   -c cmg.c -o cmg.o
> cmg.c: In function `readcmg':
> cmg.c:33: warning: assignment from incompatible pointer type
> cmg.c:40: warning: assignment from incompatible pointer type
> cmg.c:50: warning: assignment from incompatible pointer type
> gcc  -shared -s  -o cmg.dll cmg.def cmg.o  -LC:/PROGRA~1/R/R-2.3.1/bin
> -lR
>
> With R 2.4.0 and later, the compilation fails at the linking, so the four
> files generated are Makedeps, *.d, *.def, and *.o:
>
> C:\Data\R-packages\cmg>rcmd SHLIB cmg.c -o cmg.dll
> latex: not found
> making cmg.d from cmg.c
> gcc   -IC:/PROGRA~1/R/R-2.4.1/include  -Wall -O2 -std=gnu99   -c cmg.c -o
> cmg.o
> cmg.c: In function `readcmg':
> cmg.c:33: warning: assignment from incompatible pointer type
> cmg.c:40: warning: assignment from incompatible pointer type
> cmg.c:50: warning: assignment from incompatible pointer type
> gcc  -shared -s  -o cmg.dll cmg.def cmg.o  -LC:/PROGRA~1/R/R-2.4.1/bin
> -lR
> cmg.o:cmg.c:(.text+0xc0): undefined reference to `uncompress'
> collect2: ld returned 1 exit status
> make: *** [cmg.dll] Error 1
>
> I have seen the page by Duncan Murdoch
> (http://www.murdoch-sutherland.com/Rtools/), that since 2.4.0 you need
> MinGW runtime 3.11 or higher, so I reinstalled the whole MinGW (runtime now
> 3.11, gcc core and C++ compiler now 3.4.5) and ActivePerl (now 5.8.8. build
> 820) but the problem persists.
>
> Any ideas or hints on how I could compile my C code with the current
> version of R are highly welcome.
>
> Many thanks in advance and best regards, Jan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jorgecornejo at uach.cl  Wed Mar 28 16:47:04 2007
From: jorgecornejo at uach.cl (Jorge Cornejo-Donoso)
Date: Wed, 28 Mar 2007 10:47:04 -0400
Subject: [R] substitute NA values
In-Reply-To: <b490ce570703280721t75c65230r898a4322c12ac129@mail.gmail.com>
Message-ID: <200703281447.l2SElG9w013750@hypatia.math.ethz.ch>

This could work, but not with big matrix!

year <- c(1990,1995,2000,2005)
Prod <- c(2478,3192,NA,1594)

matrix <- data.frame(cbind(year,Prod))
for (i in 1:dim(matrix)[1])
	{
	if (is.na(matrix[i,2])) {matrix[i,2] <- 0}
	}


From ripley at stats.ox.ac.uk  Wed Mar 28 16:49:50 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 28 Mar 2007 15:49:50 +0100 (BST)
Subject: [R] Problem with adding packages to default start-up list
In-Reply-To: <WorldClient-F200703280934.AA34370459@ngpc.ne.gov>
References: <WorldClient-F200703280934.AA34370459@ngpc.ne.gov>
Message-ID: <Pine.LNX.4.64.0703281547060.19241@gannet.stats.ox.ac.uk>

On Wed, 28 Mar 2007, Jeff Lusk wrote:

> After reading the help files (?Startup) and using
> RSiteSearch("defaultPackages"), I have been trying to add several packages
> to my default startup list using the following code:
>
> local({
> old<-getOption("defaultPackages")
> options(defaultPackages=c(old,"lattice","RODBC")
> })

Where did you put this code?
It has a syntax error in the third line, which might be the issue (and 
whether you get an error depends on where you put it).  Once that is 
corrected this works for me in ~/.Rprofile.

>
> Subsequently, when I query the default list using:
>
> getOption("defaultPackages")
>
> the list of packages I have added appears along with the standard defaults.
> However, when I restart R, none of these packages are loaded and they don't
> appear in the default list when it is queried again.  There's probably
> something simple I am missing and I am hoping that someone will (politely)
> point out where I've messed up.
>
> I am running R 2.4.0 on a Windows 2000 PC.  Sys.getenv("R_USER") returns the
> location of my Rconsole in the "...My Documents" file, but my working
> directory is in the "...program files/R/R-2.4.0" directory, if that helps.
>
> Thanks for your assistance.
>
> Jeff--
>
>
> ===================================================
> Dr. Jeffrey J. Lusk
> Upland Game Program Manager
> Nebraska Game & Parks Commission, Wildlife Division
> 2200 N. 33rd Street
> Lincoln, NE 68506
> e-mail: jeff.lusk at ngpc.ne.gov [mailto:jeff.lusk at ngpc.ne.gov]
> Web: http://www.geocities.com/jefflusk2002
> [http://www.geocities.com/jefflusk2002]
> Web: http://www.OutdoorNebraska.org [http://www.outdoornebraska.org/]
>
> Phone: (402) 471-1756
> ===================================================
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ligges at statistik.uni-dortmund.de  Wed Mar 28 16:50:56 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 28 Mar 2007 16:50:56 +0200
Subject: [R] fitting data with conditions
In-Reply-To: <6318918.328591175092244352.JavaMail.webmail@ps8zhh.bluewin.ch>
References: <6318918.328591175092244352.JavaMail.webmail@ps8zhh.bluewin.ch>
Message-ID: <460A80D0.5010709@statistik.uni-dortmund.de>

This mailing list is in english ....

If you know that for the quantiles, say:

  1%: 3
90%: 5.2
99%: 6.1

then you could quickly look for the minimum of the following function

foo <- function(x){
     (qlnorm(0.01, meanlog = x[1], sdlog = x[2]) - 3)^2 +
     (qlnorm(0.9, meanlog = x[1], sdlog = x[2]) - 5.2)^2 +
     (qlnorm(0.99, meanlog = x[1], sdlog = x[2]) - 6.1)^2
}

using optim() as in

optim(c(1,1), foo)

Uwe Ligges



yseiler at bluewin.ch wrote:
> Mich besch?ftig folgende Fragestellung. Ich kenne die Verteilung 
> (lognormal) zus?tzlich weiss ich das 99%, das 90% und das 1% Quantil. 
> Gibt es in R eine M?glichkeit die Lognormalverteilung zu finden, das 
> heisst den korrespondierenden logmean und logsd?
> 
> Vielen Dank f?r ihre Hilfe
> Gruss
> Yvonne
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Eric.Elguero at mpl.ird.fr  Wed Mar 28 15:39:36 2007
From: Eric.Elguero at mpl.ird.fr (Eric Elguero)
Date: Wed, 28 Mar 2007 15:39:36 +0200
Subject: [R] what is the difference between survival analysis and (...)
Message-ID: <001801c7713e$868ee050$b480150a@pcelguero>

Hi everybody,

recently I had to teach a course on Cox model, of which I am
not a specialist, to an audience of medical epidemiologists.
Not a good idea you might say.. anyway, someone in the
audience was very hostile. At some point, he sayed that
Cox model was useless, since all you have to do is count
who dies and who survives, divide by the sample sizes
and compute a relative risk, and if there was significant
censoring, use cumulated follow-up instead of sample
sizes and that's it!
I began arguing that in Cox model you could introduce
several variables, interactions, etc, then I remembered
of logistic models ;-)
The only (and poor) argument I could think of was that
if mr Cox took pains to devise his model, there should
be some reason...

but the story doesn't end here. When I came back to my office,
I tried these two methods on a couple of data sets, and true,
crude RRs are very close to those coming from Cox model.

hence this question: could someone provide me with a
dataset (preferably real) where there is a striking
difference between estimated RRs and/or between
P-values? and of course I am interested in theoretical
arguments and references.

sorry that this question has nothing to do with R
and thank you in advance for your leniency.

Eric Elguero
GEMI-UMR 2724 IRD-CNRS,
?quipe "?volution des Syst?mes Symbiotiques"
911 avenue Agropolis, BP 64501,
34394 Montpellier cedex 5 FRANCE


From Achim.Zeileis at R-project.org  Wed Mar 28 17:16:09 2007
From: Achim.Zeileis at R-project.org (Achim Zeileis)
Date: Wed, 28 Mar 2007 17:16:09 +0200 (CEST)
Subject: [R] aggregating data with Zoo
In-Reply-To: <00c201c77133$b2f68680$0300a8c0@Vaio>
References: <00c201c77133$b2f68680$0300a8c0@Vaio>
Message-ID: <Pine.LNX.4.64.0703281712520.5067@eowyn>

On Wed, 28 Mar 2007, Alfonso Sammassimo wrote:

> Is there a way of aggregating 'zoo' daily data according to day of week? eg
> all Thursdays

Sure, the easiest way will probably differ depending on the time stamp 
class. One example might be this:
   ## small example with Date index
   z <- read.zoo(file.path(.find.package("zoo"), "doc", "demo1.txt"),
                 sep = "|", format = "%d %b %Y")
   ## visualization
   plot(z)
   ## aggregate along week days (via POSIXlt representation)
   aggregate(z, as.POSIXlt(time(z))$wday, mean)

hth,
Z

> I came across the 'nextfri' function in the documentation but am unsure how
> to change this so any day of week can be aggregated.
>
> I have used POSIX to arrange the data (not as 'zoo' series) according to day
> of week, but am curious if I've missed if a similar option available with
> zoo.
>
> Thank you for any help,
>
> Alf Sammassimo
> Melbourne, Australia
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From f.harrell at vanderbilt.edu  Wed Mar 28 17:31:23 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 28 Mar 2007 10:31:23 -0500
Subject: [R] what is the difference between survival analysis and (...)
In-Reply-To: <001801c7713e$868ee050$b480150a@pcelguero>
References: <001801c7713e$868ee050$b480150a@pcelguero>
Message-ID: <460A8A4B.1020900@vanderbilt.edu>

Eric Elguero wrote:
> Hi everybody,
> 
> recently I had to teach a course on Cox model, of which I am
> not a specialist, to an audience of medical epidemiologists.
> Not a good idea you might say.. anyway, someone in the
> audience was very hostile. At some point, he sayed that
> Cox model was useless, since all you have to do is count
> who dies and who survives, divide by the sample sizes
> and compute a relative risk, and if there was significant
> censoring, use cumulated follow-up instead of sample
> sizes and that's it!
> I began arguing that in Cox model you could introduce
> several variables, interactions, etc, then I remembered
> of logistic models ;-)
> The only (and poor) argument I could think of was that
> if mr Cox took pains to devise his model, there should
> be some reason...

That is a very ignorant person, concerning statistical 
efficiency/power/precision and how to handle incomplete follow-up 
(variable follow-up duration).  There are papers in the literature (I 
wish I had them at my fingertips) that go into the efficiency loss of 
just counting events.  If the events are very rare, knowing the time 
doesn't help as much, but the Cox model still can handle censoring 
correctly and that person's approach doesn't.

Frank

> 
> but the story doesn't end here. When I came back to my office,
> I tried these two methods on a couple of data sets, and true,
> crude RRs are very close to those coming from Cox model.
> 
> hence this question: could someone provide me with a
> dataset (preferably real) where there is a striking
> difference between estimated RRs and/or between
> P-values? and of course I am interested in theoretical
> arguments and references.
> 
> sorry that this question has nothing to do with R
> and thank you in advance for your leniency.
> 
> Eric Elguero
> GEMI-UMR 2724 IRD-CNRS,
> ?quipe "?volution des Syst?mes Symbiotiques"
> 911 avenue Agropolis, BP 64501,
> 34394 Montpellier cedex 5 FRANCE
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From sergio.della.franca at gmail.com  Wed Mar 28 17:48:18 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Wed, 28 Mar 2007 17:48:18 +0200
Subject: [R] Kmeans
Message-ID: <b490ce570703280848r639abeafo8f3ba4d01c2127c7@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070328/ef5ea906/attachment.pl 

From bates at stat.wisc.edu  Wed Mar 28 17:49:18 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 28 Mar 2007 10:49:18 -0500
Subject: [R] R equivalent of S+SeqTrial?
In-Reply-To: <1175086394.14657.7.camel@Bellerophon>
References: <9708877.post@talk.nabble.com>
	<1175086394.14657.7.camel@Bellerophon>
Message-ID: <40e66e0b0703280849y3f2691e5n7d48668dafa9dba@mail.gmail.com>

On 3/28/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> On Wed, 2007-03-28 at 02:42 -0700, francogrex wrote:
> > Does anyone know of an R package that is equivalent of S+SeqTrial for
> > analysis of clinical trials using group sequential methods? Thanks.
>
> I don't know that there are fully functional equivalents, but you might
> want to look at the following:
>
> 1. The GroupSeq package on CRAN:
>
>   http://cran.r-project.org/src/contrib/Descriptions/GroupSeq.html
>
>
> 2. The ldBands() function in Frank Harrell's Hmisc package on CRAN:
>
>   http://biostat.mc.vanderbilt.edu/s/Hmisc/html/ldBands.html
>
> This also requires the ld98 executable, which is available from:
>
>   http://www.biostat.wisc.edu/landemets/

There is also the ldbounds package on CRAN for calculating Lan-DeMets
boundaries.  It was prepared by students working with Dave DeMets and
does not require the ld98 executable.


From P.Dalgaard at biostat.ku.dk  Wed Mar 28 17:52:02 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 28 Mar 2007 17:52:02 +0200
Subject: [R] fitting data with conditions
In-Reply-To: <6318918.328591175092244352.JavaMail.webmail@ps8zhh.bluewin.ch>
References: <6318918.328591175092244352.JavaMail.webmail@ps8zhh.bluewin.ch>
Message-ID: <460A8F22.8050906@biostat.ku.dk>

yseiler at bluewin.ch wrote:
> Mich besch?ftig folgende Fragestellung. Ich kenne die Verteilung 
> (lognormal) zus?tzlich weiss ich das 99%, das 90% und das 1% Quantil. 
> Gibt es in R eine M?glichkeit die Lognormalverteilung zu finden, das 
> heisst den korrespondierenden logmean und logsd?
>   
(Nicht auf Deutch, bitte...)

Well, you can do it by hand: Start by taking log of the quantiles. The
interquantile spacings are then proportional to the SD, so you can use
them (in multiple ways!) to determine the SD. Once that is found,
determine the 50%-quantile. Done.
> Vielen Dank f?r ihre Hilfe
> Gruss
> Yvonne
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From marc_schwartz at comcast.net  Wed Mar 28 17:58:09 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 28 Mar 2007 10:58:09 -0500
Subject: [R] R equivalent of S+SeqTrial?
In-Reply-To: <40e66e0b0703280849y3f2691e5n7d48668dafa9dba@mail.gmail.com>
References: <9708877.post@talk.nabble.com>
	<1175086394.14657.7.camel@Bellerophon>
	<40e66e0b0703280849y3f2691e5n7d48668dafa9dba@mail.gmail.com>
Message-ID: <1175097489.14657.45.camel@Bellerophon>

On Wed, 2007-03-28 at 10:49 -0500, Douglas Bates wrote:
> On 3/28/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> > On Wed, 2007-03-28 at 02:42 -0700, francogrex wrote:
> > > Does anyone know of an R package that is equivalent of S+SeqTrial for
> > > analysis of clinical trials using group sequential methods? Thanks.
> >
> > I don't know that there are fully functional equivalents, but you might
> > want to look at the following:
> >
> > 1. The GroupSeq package on CRAN:
> >
> >   http://cran.r-project.org/src/contrib/Descriptions/GroupSeq.html
> >
> >
> > 2. The ldBands() function in Frank Harrell's Hmisc package on CRAN:
> >
> >   http://biostat.mc.vanderbilt.edu/s/Hmisc/html/ldBands.html
> >
> > This also requires the ld98 executable, which is available from:
> >
> >   http://www.biostat.wisc.edu/landemets/
> 
> There is also the ldbounds package on CRAN for calculating Lan-DeMets
> boundaries.  It was prepared by students working with Dave DeMets and
> does not require the ld98 executable.


Thanks Doug, I missed that one.

Regards,

Marc


From maitra at iastate.edu  Wed Mar 28 18:07:05 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Wed, 28 Mar 2007 11:07:05 -0500
Subject: [R] Kmeans
In-Reply-To: <b490ce570703280848r639abeafo8f3ba4d01c2127c7@mail.gmail.com>
References: <b490ce570703280848r639abeafo8f3ba4d01c2127c7@mail.gmail.com>
Message-ID: <20070328110705.0c702aaf@triveni.stat.iastate.edu>

The answer is correct, and is the way it should be. Cluster indicators are only nominal: there is no ordering in the ids, and hence the means are in different order.

Either way, the reason for this (and you could get totally different answers also) is because of how R initializes kmeans (random starts) which is clearly explained in the help on the subject.

Ranjan

On Wed, 28 Mar 2007 17:48:18 +0200 "Sergio Della Franca" <sergio.della.franca at gmail.com> wrote:

> Dear R-Helpers,
> 
> I performed kmeans clustering on the following data set(y):
> 
>  YEAR  PRODUCTS
>     1          10
>     2          42
>     3          25
>     4          42
>     5          40
>     6          45
>     7          44
>     8          47
>     9          42
> 
> 
> with this code:
> 
> cluster<-kmeans(y[,c("YEAR","PRODUCTS")],3).
> 
> Every time i run this code the components of cluster ("mean"  "vector")
> changed value,i.e.
> 
> First run:
> 
> Cluster means
> 
>       YEAR  PRODUCTS
> 1 7.500000 44.50000
> 2 3.666667 41.33333
> 3 2.000000 17.50000
> 
> Clustering vector:
> 1 2 3 4 5 6 7 8 9
> 3 2 3 2 2 1 1 1 1
> 
> Second run:
> Cluster means
>       YEAR  PRODUCTS
> 1 2.000000 17.50000
> 2 3.666667 41.33333
> 3 7.500000 44.50000
> 
> Clustering vector:
> 1 2 3 4 5 6 7 8 9
> 1 2 1 2 2 3 3 3 3
> 
> 
> How can i modify, if it is possible, the code to obtain the same value
> ("mean"  "vector") every time i'll run the code?
> 
> Thank you in advance.
> 
> Sergio Della Franca.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Joseph.F.Lucke at uth.tmc.edu  Wed Mar 28 18:10:16 2007
From: Joseph.F.Lucke at uth.tmc.edu (Lucke, Joseph F)
Date: Wed, 28 Mar 2007 11:10:16 -0500
Subject: [R] what is the difference between survival analysis and (...)
In-Reply-To: <001801c7713e$868ee050$b480150a@pcelguero>
References: <001801c7713e$868ee050$b480150a@pcelguero>
Message-ID: <4677FCB5A35A0441A0E0C99D56B23D910777FE0C@UTHEVS2.mail.uthouston.edu>

You can (and I have) fit survival data with logistic regression. Agresti (1990, pp 189--196) has an introductory discussion. 

The issue is whether the occurrence of the event is of interest or whether the time-to-event is of interest. If the study lasts 180 days (as in my case) logistic regression treats an event at 1 day the same as an event at 179 days. Similarly, non-occurrence censored at 5 days is treated the same as non-occurrence censored at 180 days. These assumptions only make sense if the hazard rate is constant and (therefore) the time-to-failure distribution is exponential.

One can include exposure time as a offset (non-estimated covariate) to handle non-constant hazard rates. One can also model the hazard rate directly as a log-linear model.

Based on what he said (number events/sample size, using cumulative times), the hostile medical epidemiologist was implicitly assuming the survival time followed an exponential distribution. This assumption is often incorrect.   His arrogance was exceeded only by his ignorance.

Joe

@BOOK{Agresti1990,
  author = {Agresti, Alan},
  title = {Categorical data analysis},
  year = {1990},
  publisher = {John Wiley \& Sons},
  address = {New York, NY},
  series = {Wiley Series in Probability and Mathematical Statistics},
  keywords = {loglinear; logistic}
}


 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Eric Elguero
Sent: Wednesday, March 28, 2007 8:40 AM
To: R-help at stat.math.ethz.ch
Subject: Re: [R] what is the difference between survival analysis and (...)

Hi everybody,

recently I had to teach a course on Cox model, of which I am not a specialist, to an audience of medical epidemiologists.
Not a good idea you might say.. anyway, someone in the audience was very hostile. At some point, he sayed that Cox model was useless, since all you have to do is count who dies and who survives, divide by the sample sizes and compute a relative risk, and if there was significant censoring, use cumulated follow-up instead of sample sizes and that's it!
I began arguing that in Cox model you could introduce several variables, interactions, etc, then I remembered of logistic models ;-) The only (and poor) argument I could think of was that if mr Cox took pains to devise his model, there should be some reason...

but the story doesn't end here. When I came back to my office, I tried these two methods on a couple of data sets, and true, crude RRs are very close to those coming from Cox model.

hence this question: could someone provide me with a dataset (preferably real) where there is a striking difference between estimated RRs and/or between P-values? and of course I am interested in theoretical arguments and references.

sorry that this question has nothing to do with R and thank you in advance for your leniency.

Eric Elguero
GEMI-UMR 2724 IRD-CNRS,
?quipe "?volution des Syst?mes Symbiotiques"
911 avenue Agropolis, BP 64501,
34394 Montpellier cedex 5 FRANCE

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From sara.vallecillo at ctfc.es  Wed Mar 28 18:36:12 2007
From: sara.vallecillo at ctfc.es (sara.vallecillo at ctfc.es)
Date: Wed, 28 Mar 2007 18:36:12 +0200 (CEST)
Subject: [R] weight factor in somers2 function
Message-ID: <1447.84.88.104.1.1175099772.squirrel@llca925-a.servidoresdns.net>

Hi!
I?m trying to calculate de C index (concordance probability) through the
somers2 function (library Hmisc). I?m interesting on including the
sampling effort as a weight factor for the evaluation of model predictions
with real data. I?ve some questions about that: first of all I?m not
really sure if I can include sampling effort as a weight factor. Since the
weight factor should be a numeric vector of observation (usually
frequencies), I would expect that sampling effort could be a surrogate of
the frequency count of the number of subjects (i.e. frequency of
observation). However, when I use sampling effort as a weight factor, I
get C index larger than one. I guess/know this is statistically wrong.
Then, if these values were frequency of observation; what is working
incorrectly? What should be the characteristics of the weight vector? Or
what could be exactly included as weight factor?
Thank you very much!


From jrkrideau at yahoo.ca  Wed Mar 28 18:48:23 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 28 Mar 2007 12:48:23 -0400 (EDT)
Subject: [R] multi-level modeling  & R?
Message-ID: <808634.34255.qm@web32814.mail.mud.yahoo.com>

A colleague was asking me if R does multi-level
modelling as opposed to multiple regression.  Since I
have no knowledge of multi-level modelling (except 5
minutes googling )  I thought that I would as here.

Does are offer any multi-level modeling packages?  It
looked like arm might be one but I was not sure.

Thanks


From clists at perrin.socsci.unc.edu  Wed Mar 28 19:01:31 2007
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Wed, 28 Mar 2007 13:01:31 -0400 (EDT)
Subject: [R] multi-level modeling  & R?
In-Reply-To: <808634.34255.qm@web32814.mail.mud.yahoo.com>
References: <808634.34255.qm@web32814.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0703281301170.27967@perrin.socsci.unc.edu>

Yes, typically one uses lme4 or nlme to do multilevel modeling in R.

----------------------------------------------------------------------
Andrew J Perrin - andrew_perrin (at) unc.edu - http://perrin.socsci.unc.edu
Assistant Professor of Sociology; Book Review Editor, _Social Forces_
University of North Carolina - CB#3210, Chapel Hill, NC 27599-3210 USA
New Book: http://www.press.uchicago.edu/cgi-bin/hfs.cgi/00/178592.ctl



On Wed, 28 Mar 2007, John Kane wrote:

> A colleague was asking me if R does multi-level
> modelling as opposed to multiple regression.  Since I
> have no knowledge of multi-level modelling (except 5
> minutes googling )  I thought that I would as here.
>
> Does are offer any multi-level modeling packages?  It
> looked like arm might be one but I was not sure.
>
> Thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From christos at nuverabio.com  Wed Mar 28 19:06:28 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Wed, 28 Mar 2007 13:06:28 -0400
Subject: [R] what is the difference between survival analysis and (...)
In-Reply-To: <4677FCB5A35A0441A0E0C99D56B23D910777FE0C@UTHEVS2.mail.uthouston.edu>
References: <001801c7713e$868ee050$b480150a@pcelguero>
	<4677FCB5A35A0441A0E0C99D56B23D910777FE0C@UTHEVS2.mail.uthouston.edu>
Message-ID: <002601c7715b$6e522bb0$0e010a0a@headquarters.silicoinsights>

On the same point, transforming time-to-event data to binary outcomes so
that contingency-table analysis (odds ratios etc) or logistic regression can
be applied will result in loss of information that could lead to misleading
conclusions. 

For example, assuming that there is a good-prognosis group (low risk) and a
poor-prognosis group (high risk) that need to be compared.  By definition,
patients in the good prognosis group are those that have been followed up
for a longer time in the study, whereas patients with poor prognosis will
tend to die earlier.  Therefore censoring will occur later in the good
prognosis group and thus the two groups will not have a homogeneous
censorship structure. In this case, na?ve analysis could be misleading.

For more details and a simulation example take a look at  

http://jnci.oxfordjournals.org/cgi/data/99/2/147/DC1/3

HTH

-Christos

Christos Hatzis, Ph.D.
Nuvera Biosciences, Inc.
400 West Cummings Park
Suite 5350
Woburn, MA 01801
Tel: 781-938-3830
www.nuverabio.com
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Lucke, Joseph F
> Sent: Wednesday, March 28, 2007 12:10 PM
> To: Eric Elguero; R-help at stat.math.ethz.ch
> Subject: Re: [R] what is the difference between survival 
> analysis and (...)
> 
> You can (and I have) fit survival data with logistic 
> regression. Agresti (1990, pp 189--196) has an introductory 
> discussion. 
> 
> The issue is whether the occurrence of the event is of 
> interest or whether the time-to-event is of interest. If the 
> study lasts 180 days (as in my case) logistic regression 
> treats an event at 1 day the same as an event at 179 days. 
> Similarly, non-occurrence censored at 5 days is treated the 
> same as non-occurrence censored at 180 days. These 
> assumptions only make sense if the hazard rate is constant 
> and (therefore) the time-to-failure distribution is exponential.
> 
> One can include exposure time as a offset (non-estimated 
> covariate) to handle non-constant hazard rates. One can also 
> model the hazard rate directly as a log-linear model.
> 
> Based on what he said (number events/sample size, using 
> cumulative times), the hostile medical epidemiologist was 
> implicitly assuming the survival time followed an exponential 
> distribution. This assumption is often incorrect.   His 
> arrogance was exceeded only by his ignorance.
> 
> Joe
> 
> @BOOK{Agresti1990,
>   author = {Agresti, Alan},
>   title = {Categorical data analysis},
>   year = {1990},
>   publisher = {John Wiley \& Sons},
>   address = {New York, NY},
>   series = {Wiley Series in Probability and Mathematical Statistics},
>   keywords = {loglinear; logistic}
> }
> 
> 
>  
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Eric Elguero
> Sent: Wednesday, March 28, 2007 8:40 AM
> To: R-help at stat.math.ethz.ch
> Subject: Re: [R] what is the difference between survival 
> analysis and (...)
> 
> Hi everybody,
> 
> recently I had to teach a course on Cox model, of which I am 
> not a specialist, to an audience of medical epidemiologists.
> Not a good idea you might say.. anyway, someone in the 
> audience was very hostile. At some point, he sayed that Cox 
> model was useless, since all you have to do is count who dies 
> and who survives, divide by the sample sizes and compute a 
> relative risk, and if there was significant censoring, use 
> cumulated follow-up instead of sample sizes and that's it!
> I began arguing that in Cox model you could introduce several 
> variables, interactions, etc, then I remembered of logistic 
> models ;-) The only (and poor) argument I could think of was 
> that if mr Cox took pains to devise his model, there should 
> be some reason...
> 
> but the story doesn't end here. When I came back to my 
> office, I tried these two methods on a couple of data sets, 
> and true, crude RRs are very close to those coming from Cox model.
> 
> hence this question: could someone provide me with a dataset 
> (preferably real) where there is a striking difference 
> between estimated RRs and/or between P-values? and of course 
> I am interested in theoretical arguments and references.
> 
> sorry that this question has nothing to do with R and thank 
> you in advance for your leniency.
> 
> Eric Elguero
> GEMI-UMR 2724 IRD-CNRS,
> ?quipe "?volution des Syst?mes Symbiotiques"
> 911 avenue Agropolis, BP 64501,
> 34394 Montpellier cedex 5 FRANCE
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From ccleland at optonline.net  Wed Mar 28 19:06:17 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 28 Mar 2007 13:06:17 -0400
Subject: [R] multi-level modeling  & R?
In-Reply-To: <808634.34255.qm@web32814.mail.mud.yahoo.com>
References: <808634.34255.qm@web32814.mail.mud.yahoo.com>
Message-ID: <460AA089.8000506@optonline.net>

John Kane wrote:
> A colleague was asking me if R does multi-level
> modelling as opposed to multiple regression.  Since I
> have no knowledge of multi-level modelling (except 5
> minutes googling )  I thought that I would as here.
> 
> Does are offer any multi-level modeling packages?  It
> looked like arm might be one but I was not sure.

  RSiteSearch("multilevel model") and a search for "multilevel" on CRAN
point to a number of other relevant packages and docs, including:

http://finzi.psych.upenn.edu/R/library/nlme/html/lme.html

http://cran.r-project.org/src/contrib/Descriptions/lme4.html

http://cran.r-project.org/doc/packages/mlmRev.pdf

http://cran.r-project.org/src/contrib/Descriptions/multilevel.html

http://cran.r-project.org/doc/contrib/Bliese_Multilevel.pdf

> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code. 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From mamoralesri at telecom.com.co  Wed Mar 28 19:17:37 2007
From: mamoralesri at telecom.com.co (Mario A. Morales R. )
Date: Wed, 28 Mar 2007 12:17:37 -0500
Subject: [R] differences among time points Using manova
Message-ID: <460AA331.50406@telecom.com.co>

How I test for differences among time points without assuming
parallelism, With a profile analysis model, Using manova R function.

For example, with Potthoff and Roy data (Potthoff and Roy 1964) how
I can test

H: ABC=0,

when

A<-diag(2)

and

C<-rbind(diag(3),rep(-1,3))

(Davis, 2005 p.p 81 ).



I have the SAS code but I want to do it with R, of course.

Thank for your help.


From ezhil02 at yahoo.com  Wed Mar 28 19:27:48 2007
From: ezhil02 at yahoo.com (A Ezhil)
Date: Wed, 28 Mar 2007 10:27:48 -0700 (PDT)
Subject: [R] Large matrix into a vector
Message-ID: <973479.12210.qm@web32406.mail.mud.yahoo.com>

Hi,

I have a matrix HR(9x27). I would like to make a
single vector with elements: t(HR[,1]) followed by
t(HR[,2]) and then t(HR[,3] ... etc. Is there any neat
way of converting this matrix into a vector rather
doing something like c(t(HR[,1]), t(HR[,2]), t(HR[,3])
..)?

Thanks in Advance.
Kind regards,
Ezhil


 
____________________________________________________________________________________
TV dinner still cooling? 
Check out "Tonight's Picks" on Yahoo! TV.


From rvaradhan at jhmi.edu  Wed Mar 28 19:32:40 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Wed, 28 Mar 2007 13:32:40 -0400
Subject: [R] Large matrix into a vector
In-Reply-To: <973479.12210.qm@web32406.mail.mud.yahoo.com>
References: <973479.12210.qm@web32406.mail.mud.yahoo.com>
Message-ID: <000b01c7715f$15b2f530$7c94100a@win.ad.jhu.edu>

c(HR) will do it.

Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of A Ezhil
Sent: Wednesday, March 28, 2007 1:28 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Large matrix into a vector

Hi,

I have a matrix HR(9x27). I would like to make a
single vector with elements: t(HR[,1]) followed by
t(HR[,2]) and then t(HR[,3] ... etc. Is there any neat
way of converting this matrix into a vector rather
doing something like c(t(HR[,1]), t(HR[,2]), t(HR[,3])
..)?

Thanks in Advance.
Kind regards,
Ezhil


 
____________________________________________________________________________
________
TV dinner still cooling? 
Check out "Tonight's Picks" on Yahoo! TV.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From csardi at rmki.kfki.hu  Wed Mar 28 19:33:25 2007
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Wed, 28 Mar 2007 19:33:25 +0200
Subject: [R] Large matrix into a vector
In-Reply-To: <973479.12210.qm@web32406.mail.mud.yahoo.com>
References: <973479.12210.qm@web32406.mail.mud.yahoo.com>
Message-ID: <20070328173325.GC4967@guzu>

A matrix is just a vector with a dim attribute, so if you remove 
the dim attribute it'll be a vector. The elements of a matrix 
are stored columnwise so you'll get just what you want, if i get
your question right.

> g <- matrix(1:6, nc=2, nr=3)
> g
     [,1] [,2]
[1,]    1    4
[2,]    2    5
[3,]    3    6
> dim(g) <- NULL
> g
[1] 1 2 3 4 5 6
> 

Gabor

On Wed, Mar 28, 2007 at 10:27:48AM -0700, A Ezhil wrote:
> Hi,
> 
> I have a matrix HR(9x27). I would like to make a
> single vector with elements: t(HR[,1]) followed by
> t(HR[,2]) and then t(HR[,3] ... etc. Is there any neat
> way of converting this matrix into a vector rather
> doing something like c(t(HR[,1]), t(HR[,2]), t(HR[,3])
> ..)?
> 
> Thanks in Advance.
> Kind regards,
> Ezhil
> 
> 
>  
> ____________________________________________________________________________________
> TV dinner still cooling? 
> Check out "Tonight's Picks" on Yahoo! TV.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From marc_schwartz at comcast.net  Wed Mar 28 19:39:11 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 28 Mar 2007 12:39:11 -0500
Subject: [R] Large matrix into a vector
In-Reply-To: <973479.12210.qm@web32406.mail.mud.yahoo.com>
References: <973479.12210.qm@web32406.mail.mud.yahoo.com>
Message-ID: <1175103551.14657.51.camel@Bellerophon>

On Wed, 2007-03-28 at 10:27 -0700, A Ezhil wrote:
> Hi,
> 
> I have a matrix HR(9x27). I would like to make a
> single vector with elements: t(HR[,1]) followed by
> t(HR[,2]) and then t(HR[,3] ... etc. Is there any neat
> way of converting this matrix into a vector rather
> doing something like c(t(HR[,1]), t(HR[,2]), t(HR[,3])
> ..)?

Keep in mind that a matrix is simply a vector, with a 'dim' attribute.
In addition, the matrix elements are stored in column order, so:

  mat <- matrix(1:(9 * 27), ncol = 27)


> mat
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
 [1,]    1   10   19   28   37   46   55   64   73    82    91   100
 [2,]    2   11   20   29   38   47   56   65   74    83    92   101
 [3,]    3   12   21   30   39   48   57   66   75    84    93   102
 [4,]    4   13   22   31   40   49   58   67   76    85    94   103
 [5,]    5   14   23   32   41   50   59   68   77    86    95   104
 [6,]    6   15   24   33   42   51   60   69   78    87    96   105
 [7,]    7   16   25   34   43   52   61   70   79    88    97   106
 [8,]    8   17   26   35   44   53   62   71   80    89    98   107
 [9,]    9   18   27   36   45   54   63   72   81    90    99   108
      [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23]
 [1,]   109   118   127   136   145   154   163   172   181   190   199
 [2,]   110   119   128   137   146   155   164   173   182   191   200
 [3,]   111   120   129   138   147   156   165   174   183   192   201
 [4,]   112   121   130   139   148   157   166   175   184   193   202
 [5,]   113   122   131   140   149   158   167   176   185   194   203
 [6,]   114   123   132   141   150   159   168   177   186   195   204
 [7,]   115   124   133   142   151   160   169   178   187   196   205
 [8,]   116   125   134   143   152   161   170   179   188   197   206
 [9,]   117   126   135   144   153   162   171   180   189   198   207
      [,24] [,25] [,26] [,27]
 [1,]   208   217   226   235
 [2,]   209   218   227   236
 [3,]   210   219   228   237
 [4,]   211   220   229   238
 [5,]   212   221   230   239
 [6,]   213   222   231   240
 [7,]   214   223   232   241
 [8,]   215   224   233   242
 [9,]   216   225   234   243


> as.vector(mat)
  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16
 [17]  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32
 [33]  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48
 [49]  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64
 [65]  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80
 [81]  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96
 [97]  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112
[113] 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128
[129] 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144
[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160
[161] 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176
[177] 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192
[193] 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208
[209] 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224
[225] 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240
[241] 241 242 243


HTH,

Marc Schwartz


From albmont at centroin.com.br  Wed Mar 28 19:39:50 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Wed, 28 Mar 2007 15:39:50 -0200
Subject: [R] Large matrix into a vector
In-Reply-To: <973479.12210.qm@web32406.mail.mud.yahoo.com>
References: <973479.12210.qm@web32406.mail.mud.yahoo.com>
Message-ID: <20070328173747.M13152@centroin.com.br>

A Ezhil wrote:
> 
> I have a matrix HR(9x27). I would like to make a
> single vector with elements: t(HR[,1]) followed by
> t(HR[,2]) and then t(HR[,3] ... etc. Is there any neat
> way of converting this matrix into a vector rather
> doing something like c(t(HR[,1]), t(HR[,2]), t(HR[,3])
> ..)?
> 
It might be simpler than you thought...

HR <- matrix(1:(9*27), nrow=9) # just to create a 9x27 matrix
c(HR)  # oops, here it is!

Alberto Monteiro


From lmpalaci at ucalgary.ca  Wed Mar 28 19:47:03 2007
From: lmpalaci at ucalgary.ca (lmpalaci at ucalgary.ca)
Date: Wed, 28 Mar 2007 11:47:03 -0600 (MDT)
Subject: [R] warnings on adapt
Message-ID: <3717.205.206.237.25.1175104023.squirrel@205.206.237.25>

Hi all

I was wondering if someone could help me.

I have to estimate some parameters, so I am using the function nlm. Inside
this function I have to integrate, hence
I am using the function adapt.
I don't understand why it is giving the following warnings:

At the beginning:

Warning: a final empty element has been omitted
the part of the args list of 'c' being evaluated was:
   (paste("Ifail=2, lenwrk was too small. -- fix adapt() !\n", "Check the
returned relerr!"), "Ifail=3: ndim > 20 -- rewrite the fortran code ;-)
!", "Ifail=4, minpts > maxpts; should not happen!", "Ifail=5, internal
non-convergence; should not happen!", )

When it finishes:

Warning messages:
1: Ifail=2, lenwrk was too small. -- fix adapt() !
 Check the returned relerr! in: adapt(ndim = 1 + numcycles, lower =
rep(lowlim.int, (numcycles +
2: NA/Inf replaced by maximum positive value

Some people already asked similar questions but couldn't find the answer.

Thanks in advance

Luz

PS: When using the function adapt only (not within nlm) it gives no warnings.


From ripley at stats.ox.ac.uk  Wed Mar 28 19:47:10 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 28 Mar 2007 18:47:10 +0100 (BST)
Subject: [R] what is the difference between survival analysis and (...)
In-Reply-To: <002601c7715b$6e522bb0$0e010a0a@headquarters.silicoinsights>
References: <001801c7713e$868ee050$b480150a@pcelguero>
	<4677FCB5A35A0441A0E0C99D56B23D910777FE0C@UTHEVS2.mail.uthouston.edu>
	<002601c7715b$6e522bb0$0e010a0a@headquarters.silicoinsights>
Message-ID: <Pine.LNX.4.64.0703281834400.24235@gannet.stats.ox.ac.uk>

That is not the only way to apply logistic regression to this problem 
(although it is a common error in the analysis of cancer studies).

One can discretize time and apply logistic regression to survival over 
each short time period (jointly): doing so comes pretty close to what the 
Cox proportional hazard models do but can be harder to interpret.  But 
that is a far more sophisticated analysis than looking at crude relative 
risks for subgroups, and my understanding of Sir David Cox's motivation 
was to be able to do regression modelling of prognostic factors, not just 
comparison of groups.

I would claim that the analysis of the Australian AIDS data in MASS needs 
regression-like methods to extract all the information from what is a 
limited and expensive set of data (and I happen to know Cox agrees).


On Wed, 28 Mar 2007, Christos Hatzis wrote:

> On the same point, transforming time-to-event data to binary outcomes so
> that contingency-table analysis (odds ratios etc) or logistic regression can
> be applied will result in loss of information that could lead to misleading
> conclusions.
>
> For example, assuming that there is a good-prognosis group (low risk) and a
> poor-prognosis group (high risk) that need to be compared.  By definition,
> patients in the good prognosis group are those that have been followed up
> for a longer time in the study, whereas patients with poor prognosis will
> tend to die earlier.  Therefore censoring will occur later in the good
> prognosis group and thus the two groups will not have a homogeneous
> censorship structure. In this case, na?ve analysis could be misleading.
>
> For more details and a simulation example take a look at
>
> http://jnci.oxfordjournals.org/cgi/data/99/2/147/DC1/3
>
> HTH
>
> -Christos
>
> Christos Hatzis, Ph.D.
> Nuvera Biosciences, Inc.
> 400 West Cummings Park
> Suite 5350
> Woburn, MA 01801
> Tel: 781-938-3830
> www.nuverabio.com
>
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Lucke, Joseph F
>> Sent: Wednesday, March 28, 2007 12:10 PM
>> To: Eric Elguero; R-help at stat.math.ethz.ch
>> Subject: Re: [R] what is the difference between survival
>> analysis and (...)
>>
>> You can (and I have) fit survival data with logistic
>> regression. Agresti (1990, pp 189--196) has an introductory
>> discussion.
>>
>> The issue is whether the occurrence of the event is of
>> interest or whether the time-to-event is of interest. If the
>> study lasts 180 days (as in my case) logistic regression
>> treats an event at 1 day the same as an event at 179 days.
>> Similarly, non-occurrence censored at 5 days is treated the
>> same as non-occurrence censored at 180 days. These
>> assumptions only make sense if the hazard rate is constant
>> and (therefore) the time-to-failure distribution is exponential.
>>
>> One can include exposure time as a offset (non-estimated
>> covariate) to handle non-constant hazard rates. One can also
>> model the hazard rate directly as a log-linear model.
>>
>> Based on what he said (number events/sample size, using
>> cumulative times), the hostile medical epidemiologist was
>> implicitly assuming the survival time followed an exponential
>> distribution. This assumption is often incorrect.   His
>> arrogance was exceeded only by his ignorance.
>>
>> Joe
>>
>> @BOOK{Agresti1990,
>>   author = {Agresti, Alan},
>>   title = {Categorical data analysis},
>>   year = {1990},
>>   publisher = {John Wiley \& Sons},
>>   address = {New York, NY},
>>   series = {Wiley Series in Probability and Mathematical Statistics},
>>   keywords = {loglinear; logistic}
>> }
>>
>>
>>
>>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Eric Elguero
>> Sent: Wednesday, March 28, 2007 8:40 AM
>> To: R-help at stat.math.ethz.ch
>> Subject: Re: [R] what is the difference between survival
>> analysis and (...)
>>
>> Hi everybody,
>>
>> recently I had to teach a course on Cox model, of which I am
>> not a specialist, to an audience of medical epidemiologists.
>> Not a good idea you might say.. anyway, someone in the
>> audience was very hostile. At some point, he sayed that Cox
>> model was useless, since all you have to do is count who dies
>> and who survives, divide by the sample sizes and compute a
>> relative risk, and if there was significant censoring, use
>> cumulated follow-up instead of sample sizes and that's it!
>> I began arguing that in Cox model you could introduce several
>> variables, interactions, etc, then I remembered of logistic
>> models ;-) The only (and poor) argument I could think of was
>> that if mr Cox took pains to devise his model, there should
>> be some reason...
>>
>> but the story doesn't end here. When I came back to my
>> office, I tried these two methods on a couple of data sets,
>> and true, crude RRs are very close to those coming from Cox model.
>>
>> hence this question: could someone provide me with a dataset
>> (preferably real) where there is a striking difference
>> between estimated RRs and/or between P-values? and of course
>> I am interested in theoretical arguments and references.
>>
>> sorry that this question has nothing to do with R and thank
>> you in advance for your leniency.
>>
>> Eric Elguero
>> GEMI-UMR 2724 IRD-CNRS,
>> ?quipe "?volution des Syst?mes Symbiotiques"
>> 911 avenue Agropolis, BP 64501,
>> 34394 Montpellier cedex 5 FRANCE
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Wed Mar 28 19:50:42 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 28 Mar 2007 18:50:42 +0100 (BST)
Subject: [R] Large matrix into a vector
In-Reply-To: <1175103551.14657.51.camel@Bellerophon>
References: <973479.12210.qm@web32406.mail.mud.yahoo.com>
	<1175103551.14657.51.camel@Bellerophon>
Message-ID: <Pine.LNX.4.64.0703281847560.24235@gannet.stats.ox.ac.uk>

We have already seen three solutions.

I don't like to see the use of c() for its side effects.  In this case 
Marc's as.vector seems to me to be self-explanatory, and that is a virtue 
in programming that is too often undervalued.

On Wed, 28 Mar 2007, Marc Schwartz wrote:

> On Wed, 2007-03-28 at 10:27 -0700, A Ezhil wrote:
>> Hi,
>>
>> I have a matrix HR(9x27). I would like to make a
>> single vector with elements: t(HR[,1]) followed by
>> t(HR[,2]) and then t(HR[,3] ... etc. Is there any neat
>> way of converting this matrix into a vector rather
>> doing something like c(t(HR[,1]), t(HR[,2]), t(HR[,3])
>> ..)?
>
> Keep in mind that a matrix is simply a vector, with a 'dim' attribute.
> In addition, the matrix elements are stored in column order, so:
>
>  mat <- matrix(1:(9 * 27), ncol = 27)
>
>
>> mat
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
> [1,]    1   10   19   28   37   46   55   64   73    82    91   100
> [2,]    2   11   20   29   38   47   56   65   74    83    92   101
> [3,]    3   12   21   30   39   48   57   66   75    84    93   102
> [4,]    4   13   22   31   40   49   58   67   76    85    94   103
> [5,]    5   14   23   32   41   50   59   68   77    86    95   104
> [6,]    6   15   24   33   42   51   60   69   78    87    96   105
> [7,]    7   16   25   34   43   52   61   70   79    88    97   106
> [8,]    8   17   26   35   44   53   62   71   80    89    98   107
> [9,]    9   18   27   36   45   54   63   72   81    90    99   108
>      [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23]
> [1,]   109   118   127   136   145   154   163   172   181   190   199
> [2,]   110   119   128   137   146   155   164   173   182   191   200
> [3,]   111   120   129   138   147   156   165   174   183   192   201
> [4,]   112   121   130   139   148   157   166   175   184   193   202
> [5,]   113   122   131   140   149   158   167   176   185   194   203
> [6,]   114   123   132   141   150   159   168   177   186   195   204
> [7,]   115   124   133   142   151   160   169   178   187   196   205
> [8,]   116   125   134   143   152   161   170   179   188   197   206
> [9,]   117   126   135   144   153   162   171   180   189   198   207
>      [,24] [,25] [,26] [,27]
> [1,]   208   217   226   235
> [2,]   209   218   227   236
> [3,]   210   219   228   237
> [4,]   211   220   229   238
> [5,]   212   221   230   239
> [6,]   213   222   231   240
> [7,]   214   223   232   241
> [8,]   215   224   233   242
> [9,]   216   225   234   243
>
>
>> as.vector(mat)
>  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16
> [17]  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32
> [33]  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48
> [49]  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64
> [65]  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80
> [81]  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96
> [97]  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112
> [113] 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128
> [129] 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144
> [145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160
> [161] 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176
> [177] 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192
> [193] 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208
> [209] 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224
> [225] 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240
> [241] 241 242 243
>
>
> HTH,
>
> Marc Schwartz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From maitra at iastate.edu  Wed Mar 28 20:07:58 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Wed, 28 Mar 2007 13:07:58 -0500
Subject: [R] warnings on adapt
In-Reply-To: <3717.205.206.237.25.1175104023.squirrel@205.206.237.25>
References: <3717.205.206.237.25.1175104023.squirrel@205.206.237.25>
Message-ID: <20070328130758.5b18392f@subarnarekha.stat.iastate.edu>

PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

No one can help you.

Ranjan


On Wed, 28 Mar 2007 11:47:03 -0600 (MDT) lmpalaci at ucalgary.ca wrote:

> Hi all
> 
> I was wondering if someone could help me.
> 
> I have to estimate some parameters, so I am using the function nlm. Inside
> this function I have to integrate, hence
> I am using the function adapt.
> I don't understand why it is giving the following warnings:
> 
> At the beginning:
> 
> Warning: a final empty element has been omitted
> the part of the args list of 'c' being evaluated was:
>    (paste("Ifail=2, lenwrk was too small. -- fix adapt() !\n", "Check the
> returned relerr!"), "Ifail=3: ndim > 20 -- rewrite the fortran code ;-)
> !", "Ifail=4, minpts > maxpts; should not happen!", "Ifail=5, internal
> non-convergence; should not happen!", )
> 
> When it finishes:
> 
> Warning messages:
> 1: Ifail=2, lenwrk was too small. -- fix adapt() !
>  Check the returned relerr! in: adapt(ndim = 1 + numcycles, lower =
> rep(lowlim.int, (numcycles +
> 2: NA/Inf replaced by maximum positive value
> 
> Some people already asked similar questions but couldn't find the answer.
> 
> Thanks in advance
> 
> Luz
> 
> PS: When using the function adapt only (not within nlm) it gives no warnings.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jrkrideau at yahoo.ca  Wed Mar 28 20:09:58 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 28 Mar 2007 14:09:58 -0400 (EDT)
Subject: [R] Large matrix into a vector
In-Reply-To: <973479.12210.qm@web32406.mail.mud.yahoo.com>
Message-ID: <574943.75897.qm@web32802.mail.mud.yahoo.com>

Is this something like what you want?

ab  <- rep(1,4)
bb <- rep(2,4)
cc <- rep(3,4)
mydata  <- data.frame(ab,bb,cc)
unlist(mydata)
unlist(data.frame(t(mydata)))


--- A Ezhil <ezhil02 at yahoo.com> wrote:

> Hi,
> 
> I have a matrix HR(9x27). I would like to make a
> single vector with elements: t(HR[,1]) followed by
> t(HR[,2]) and then t(HR[,3] ... etc. Is there any
> neat
> way of converting this matrix into a vector rather
> doing something like c(t(HR[,1]), t(HR[,2]),
> t(HR[,3])
> ..)?
> 
> Thanks in Advance.
> Kind regards,
> Ezhil
> 
> 
>  
>
____________________________________________________________________________________
> TV dinner still cooling? 
> Check out "Tonight's Picks" on Yahoo! TV.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From kapatp at gmail.com  Wed Mar 28 20:21:19 2007
From: kapatp at gmail.com (Prasenjit Kapat)
Date: Wed, 28 Mar 2007 14:21:19 -0400
Subject: [R] is this trellis device or standard graphics device?
Message-ID: <de8c7cb40703281121j2922fa95i59ef3ce3d4f38d94@mail.gmail.com>

OK, here is the issue:
I have a graphcis device (say X11) which I can set as active by
dev.set(n). Is there ANY way to identify/infer whether this X11 device
was obtained from a lattice function (like xyplot, levelplot...) or a
standard function (like plot, scatterplot,...).

A related question is whether plot.new (or frame) was called to obtain
this graphics device...

Thanks.
PK


From jrkrideau at yahoo.ca  Wed Mar 28 20:24:43 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 28 Mar 2007 14:24:43 -0400 (EDT)
Subject: [R] multi-level modeling  & R?
In-Reply-To: <460AA089.8000506@optonline.net>
Message-ID: <116313.82125.qm@web32802.mail.mud.yahoo.com>

Thanks to everyone for the information and to Chuck
for the links. I must have misspelt something because
I missed most of them in the RsiteSearch.

Anyway back to selling R :)

--- Chuck Cleland <ccleland at optonline.net> wrote:

> John Kane wrote:
> > A colleague was asking me if R does multi-level
> > modelling as opposed to multiple regression. 
> Since I
> > have no knowledge of multi-level modelling (except
> 5
> > minutes googling )  I thought that I would as
> here.
> > 
> > Does are offer any multi-level modeling packages? 
> It
> > looked like arm might be one but I was not sure.
> 
>   RSiteSearch("multilevel model") and a search for
> "multilevel" on CRAN
> point to a number of other relevant packages and
> docs, including:
> 
>
http://finzi.psych.upenn.edu/R/library/nlme/html/lme.html
> 
>
http://cran.r-project.org/src/contrib/Descriptions/lme4.html
> 
> http://cran.r-project.org/doc/packages/mlmRev.pdf
> 
>
http://cran.r-project.org/src/contrib/Descriptions/multilevel.html
> 
>
http://cran.r-project.org/doc/contrib/Bliese_Multilevel.pdf
> 
> > Thanks
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code. 
> 
> -- 
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 512-0171 (M, W, F)
> fax: (917) 438-0894
>


From christos at nuverabio.com  Wed Mar 28 20:41:15 2007
From: christos at nuverabio.com (Christos Hatzis)
Date: Wed, 28 Mar 2007 14:41:15 -0400
Subject: [R] multi-level modeling  & R?
In-Reply-To: <116313.82125.qm@web32802.mail.mud.yahoo.com>
References: <460AA089.8000506@optonline.net>
	<116313.82125.qm@web32802.mail.mud.yahoo.com>
Message-ID: <003601c77168$ab48a230$0e010a0a@headquarters.silicoinsights>

Also, take a look at Andrew Gelman's recent book with Jennifer Hill on
multilevel modeling.
It is written as a practical how-to text book using R as the primary
platform.  I have found it very easy to read, extremely useful and very good
value for its price (I am not related to the authors or the publisher in any
way).

http://www.stat.columbia.edu/~gelman/arm/   

-Christos

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Kane
> Sent: Wednesday, March 28, 2007 2:25 PM
> To: Chuck Cleland
> Cc: R R-help
> Subject: Re: [R] multi-level modeling & R?
> 
> Thanks to everyone for the information and to Chuck for the 
> links. I must have misspelt something because I missed most 
> of them in the RsiteSearch.
> 
> Anyway back to selling R :)
> 
> --- Chuck Cleland <ccleland at optonline.net> wrote:
> 
> > John Kane wrote:
> > > A colleague was asking me if R does multi-level modelling 
> as opposed 
> > > to multiple regression.
> > Since I
> > > have no knowledge of multi-level modelling (except
> > 5
> > > minutes googling )  I thought that I would as
> > here.
> > > 
> > > Does are offer any multi-level modeling packages? 
> > It
> > > looked like arm might be one but I was not sure.
> > 
> >   RSiteSearch("multilevel model") and a search for "multilevel" on 
> > CRAN point to a number of other relevant packages and docs, 
> including:
> > 
> >
> http://finzi.psych.upenn.edu/R/library/nlme/html/lme.html
> > 
> >
> http://cran.r-project.org/src/contrib/Descriptions/lme4.html
> > 
> > http://cran.r-project.org/doc/packages/mlmRev.pdf
> > 
> >
> http://cran.r-project.org/src/contrib/Descriptions/multilevel.html
> > 
> >
> http://cran.r-project.org/doc/contrib/Bliese_Multilevel.pdf
> > 
> > > Thanks
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained,
> > reproducible code. 
> > 
> > --
> > Chuck Cleland, Ph.D.
> > NDRI, Inc.
> > 71 West 23rd Street, 8th floor
> > New York, NY 10010
> > tel: (212) 845-4495 (Tu, Th)
> > tel: (732) 512-0171 (M, W, F)
> > fax: (917) 438-0894
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
>


From jrkrideau at yahoo.ca  Wed Mar 28 20:51:34 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 28 Mar 2007 14:51:34 -0400 (EDT)
Subject: [R] multi-level modeling  & R?
In-Reply-To: <003601c77168$ab48a230$0e010a0a@headquarters.silicoinsights>
Message-ID: <756137.80918.qm@web32803.mail.mud.yahoo.com>

Thanks Christos
Some low life managed to get it out of the library!
It looks worth reading, I must say.

--- Christos Hatzis <christos at nuverabio.com> wrote:

> Also, take a look at Andrew Gelman's recent book
> with Jennifer Hill on
> multilevel modeling.
> It is written as a practical how-to text book using
> R as the primary
> platform.  I have found it very easy to read,
> extremely useful and very good
> value for its price (I am not related to the authors
> or the publisher in any
> way).
> 
> http://www.stat.columbia.edu/~gelman/arm/   
> 
> -Christos
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On
> Behalf Of John Kane
> > Sent: Wednesday, March 28, 2007 2:25 PM
> > To: Chuck Cleland
> > Cc: R R-help
> > Subject: Re: [R] multi-level modeling & R?
> > 
> > Thanks to everyone for the information and to
> Chuck for the 
> > links. I must have misspelt something because I
> missed most 
> > of them in the RsiteSearch.
> > 
> > Anyway back to selling R :)
> > 
> > --- Chuck Cleland <ccleland at optonline.net> wrote:
> > 
> > > John Kane wrote:
> > > > A colleague was asking me if R does
> multi-level modelling 
> > as opposed 
> > > > to multiple regression.
> > > Since I
> > > > have no knowledge of multi-level modelling
> (except
> > > 5
> > > > minutes googling )  I thought that I would as
> > > here.
> > > > 
> > > > Does are offer any multi-level modeling
> packages? 
> > > It
> > > > looked like arm might be one but I was not
> sure.
> > > 
> > >   RSiteSearch("multilevel model") and a search
> for "multilevel" on 
> > > CRAN point to a number of other relevant
> packages and docs, 
> > including:
> > > 
> > >
> >
>
http://finzi.psych.upenn.edu/R/library/nlme/html/lme.html
> > > 
> > >
> >
>
http://cran.r-project.org/src/contrib/Descriptions/lme4.html
> > > 
> > >
> http://cran.r-project.org/doc/packages/mlmRev.pdf
> > > 
> > >
> >
>
http://cran.r-project.org/src/contrib/Descriptions/multilevel.html
> > > 
> > >
> >
>
http://cran.r-project.org/doc/contrib/Bliese_Multilevel.pdf
> > > 
> > > > Thanks
> > > > 
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list 
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal,
> self-contained,
> > > reproducible code. 
> > > 
> > > --
> > > Chuck Cleland, Ph.D.
> > > NDRI, Inc.
> > > 71 West 23rd Street, 8th floor
> > > New York, NY 10010
> > > tel: (212) 845-4495 (Tu, Th)
> > > tel: (732) 512-0171 (M, W, F)
> > > fax: (917) 438-0894
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> > 
> > 
> 
> 
>


From Mark.Leeds at morganstanley.com  Wed Mar 28 21:11:13 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Wed, 28 Mar 2007 15:11:13 -0400
Subject: [R] .duplicate question
Message-ID: <D3AEEDA31E57474B840BEBC25A8A8344F3987A@NYWEXMB23.msad.ms.com>

I am using the vars package and it calls a function causality() which
then calls something
called .duplicate. I had to modify the causality function slightly for
my purposes and
I called it my.causality()  but now the .duplicate function is no longer
known to the my.causality function.
I'm fairly certain that this is due to my lack of expertise in R but if
someone could tell me how
to make the my.causality function know about .duplicate, it would be
appreciated. Thanks.


	
Mark
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}


From bcarvalh at jhsph.edu  Wed Mar 28 21:14:08 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Wed, 28 Mar 2007 15:14:08 -0400
Subject: [R] .duplicate question
In-Reply-To: <D3AEEDA31E57474B840BEBC25A8A8344F3987A@NYWEXMB23.msad.ms.com>
References: <D3AEEDA31E57474B840BEBC25A8A8344F3987A@NYWEXMB23.msad.ms.com>
Message-ID: <CD503251-D29E-4C81-853D-D70E6902A0AE@jhsph.edu>

have you tried replacing

.duplicate()

in your my.causality() by

vars:::.duplicate()

?

b

On Mar 28, 2007, at 3:11 PM, Leeds, Mark ((IED)) wrote:

> I am using the vars package and it calls a function causality() which
> then calls something
> called .duplicate. I had to modify the causality function slightly for
> my purposes and
> I called it my.causality()  but now the .duplicate function is no  
> longer
> known to the my.causality function.
> I'm fairly certain that this is due to my lack of expertise in R  
> but if
> someone could tell me how
> to make the my.causality function know about .duplicate, it would be
> appreciated. Thanks.
>
>
> 	
> Mark
> --------------------------------------------------------
>
> This is not an offer (or solicitation of an offer) to buy/se... 
> {{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From knussear at usgs.gov  Wed Mar 28 21:15:26 2007
From: knussear at usgs.gov (Ken Nussear)
Date: Wed, 28 Mar 2007 12:15:26 -0700
Subject: [R] geoRglm question with covariates
Message-ID: <0921694A-EE4E-416E-97F0-FD5B503C7E26@usgs.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070328/76724fb7/attachment.pl 

From f.harrell at vanderbilt.edu  Wed Mar 28 21:44:19 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 28 Mar 2007 14:44:19 -0500
Subject: [R] weight factor in somers2 function
In-Reply-To: <1447.84.88.104.1.1175099772.squirrel@llca925-a.servidoresdns.net>
References: <1447.84.88.104.1.1175099772.squirrel@llca925-a.servidoresdns.net>
Message-ID: <460AC593.2060300@vanderbilt.edu>

sara.vallecillo at ctfc.es wrote:
> Hi!
> I?m trying to calculate de C index (concordance probability) through the
> somers2 function (library Hmisc). I?m interesting on including the
> sampling effort as a weight factor for the evaluation of model predictions
> with real data. I?ve some questions about that: first of all I?m not
> really sure if I can include sampling effort as a weight factor. Since the
> weight factor should be a numeric vector of observation (usually
> frequencies), I would expect that sampling effort could be a surrogate of
> the frequency count of the number of subjects (i.e. frequency of
> observation). However, when I use sampling effort as a weight factor, I
> get C index larger than one. I guess/know this is statistically wrong.
> Then, if these values were frequency of observation; what is working
> incorrectly? What should be the characteristics of the weight vector? Or
> what could be exactly included as weight factor?
> Thank you very much!

Send me the smallest artificial example you can construct and I'll work 
on it.

Frank


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From zhongmiao at gmail.com  Wed Mar 28 22:50:02 2007
From: zhongmiao at gmail.com (zhongmiao wang)
Date: Wed, 28 Mar 2007 14:50:02 -0600
Subject: [R] what is the difference between EM clustering and latent class
	analysis
Message-ID: <1def27350703281350n458bf494v5ed98b5c1278f8e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070328/717368b7/attachment.pl 

From cincinattikid at bigpond.com  Wed Mar 28 23:20:23 2007
From: cincinattikid at bigpond.com (Alfonso Sammassimo)
Date: Thu, 29 Mar 2007 07:20:23 +1000
Subject: [R] aggregating data with Zoo
References: <00c201c77133$b2f68680$0300a8c0@Vaio>
	<Pine.LNX.4.64.0703281712520.5067@eowyn>
Message-ID: <00c501c7717e$e58587e0$0300a8c0@Vaio>

Thankyou Achim.

Along these lines, how might I extract subsets from that set by day of week?
eg: create a new series that includes Mondays only.

Thanks,
Alf
----- Original Message ----- 
From: "Achim Zeileis" <Achim.Zeileis at R-project.org>
To: "Alfonso Sammassimo" <cincinattikid at bigpond.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Thursday, March 29, 2007 1:16 AM
Subject: Re: [R] aggregating data with Zoo


> On Wed, 28 Mar 2007, Alfonso Sammassimo wrote:
>
>> Is there a way of aggregating 'zoo' daily data according to day of week? 
>> eg
>> all Thursdays
>
> Sure, the easiest way will probably differ depending on the time stamp 
> class. One example might be this:
>   ## small example with Date index
>   z <- read.zoo(file.path(.find.package("zoo"), "doc", "demo1.txt"),
>                 sep = "|", format = "%d %b %Y")
>   ## visualization
>   plot(z)
>   ## aggregate along week days (via POSIXlt representation)
>   aggregate(z, as.POSIXlt(time(z))$wday, mean)
>
> hth,
> Z
>
>> I came across the 'nextfri' function in the documentation but am unsure 
>> how
>> to change this so any day of week can be aggregated.
>>
>> I have used POSIX to arrange the data (not as 'zoo' series) according to 
>> day
>> of week, but am curious if I've missed if a similar option available with
>> zoo.
>>
>> Thank you for any help,
>>
>> Alf Sammassimo
>> Melbourne, Australia
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>


From hodgess at gator.dt.uh.edu  Wed Mar 28 23:23:00 2007
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Wed, 28 Mar 2007 16:23:00 -0500
Subject: [R]  By sentence
Message-ID: <200703282123.l2SLN0QG006090@gator.dt.uh.edu>

Hi again!

Please try something like this:

> zone1.df
  zone  sex len
1    1 male  15
2    1 fema  20
3    1 fema  17
4    2 fema  19
5    2 male  18
> str(zone1.df)
'data.frame':   5 obs. of  3 variables:
 $ zone: Factor w/ 2 levels "1","2": 1 1 1 2 2
 $ sex : Factor w/ 2 levels "fema","male": 2 1 1 1 2
 $ len : int  15 20 17 19 18
> tapply(zone1.df$len,zone1.df[,-3],mean)
    sex
zone fema male
   1 18.5   15
   2 19.0   18
> 

Hope this helps!

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From: "Jorge Cornejo-Donoso" <jorgecornejo at uach.cl>
To: "'Erin Hodgess'" <hodgess at gator.dt.uh.edu>
Subject: RE: [R]  RV: by sentence




OK.. Thank for the advice.
	By and tapply works with 1 categorical variable, but what can be
used for 2 categorical variables. Example:
Zone   Sex Length
1 	male     15
1	fema     20
1 	fema     17
2	fema     19
2 	male     18

The idea is to get the means by sex in each zone!

Thank in advance!

--

Jorge Cornejo Donoso

Universidad Austral de Chile

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From albmont at centroin.com.br  Wed Mar 28 23:55:41 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Wed, 28 Mar 2007 19:55:41 -0200
Subject: [R] Large matrix into a vector
In-Reply-To: <Pine.LNX.4.64.0703281847560.24235@gannet.stats.ox.ac.uk>
References: <973479.12210.qm@web32406.mail.mud.yahoo.com>
	<1175103551.14657.51.camel@Bellerophon>
	<Pine.LNX.4.64.0703281847560.24235@gannet.stats.ox.ac.uk>
Message-ID: <20070328215502.M35167@centroin.com.br>

Prof Brian Ripley wrote:
>
> We have already seen three solutions.
> 
> I don't like to see the use of c() for its side effects.  In this 
> case Marc's as.vector seems to me to be self-explanatory, and that 
> is a virtue in programming that is too often undervalued.
> 
I agree; but for our enlightnment, what are the side effects of
c()?

Alberto Monteiro


From marc_schwartz at comcast.net  Thu Mar 29 00:11:14 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 28 Mar 2007 17:11:14 -0500
Subject: [R] Large matrix into a vector
In-Reply-To: <20070328215502.M35167@centroin.com.br>
References: <973479.12210.qm@web32406.mail.mud.yahoo.com>
	<1175103551.14657.51.camel@Bellerophon>
	<Pine.LNX.4.64.0703281847560.24235@gannet.stats.ox.ac.uk>
	<20070328215502.M35167@centroin.com.br>
Message-ID: <1175119874.14657.71.camel@localhost.localdomain>

On Wed, 2007-03-28 at 19:55 -0200, Alberto Monteiro wrote:
> Prof Brian Ripley wrote:
> >
> > We have already seen three solutions.
> > 
> > I don't like to see the use of c() for its side effects.  In this 
> > case Marc's as.vector seems to me to be self-explanatory, and that 
> > is a virtue in programming that is too often undervalued.
> > 
> I agree; but for our enlightnment, what are the side effects of
> c()?
> 
> Alberto Monteiro

I believe that Prof. Ripley is referring to the following, from the
Details section in ?c:


"c is sometimes used for its side effect of removing attributes except
names, for example to turn an array into a vector. as.vector is a more
intuitive way to do this, but also drops names."


There are also examples in ?c of this behavior.

HTH,

Marc


From topkatz at msn.com  Thu Mar 29 00:27:52 2007
From: topkatz at msn.com (Talbot Katz)
Date: Wed, 28 Mar 2007 18:27:52 -0400
Subject: [R] Outlier detection with the dprep library
Message-ID: <BAY132-F393DED49321217F208BDAFAA6D0@phx.gbl>

Hi.

The dprep library has at least three different methods for outlier 
detection: baysout, mahaout, robout.

I wanted to test them on a very simple data set:

vrmat<-cbind((1:22),c(8,14,14,17,21,20,27,23,25,33,31,32,30,36,37,40,42,44,52,61,81,265))

As you can see by eyeballing this, the last point is a very good outlier 
candidate, and maybe the second to last point, too.  Anyway, I couldn't get 
a single one of the dprep methods to work.  They all gave me 
incomprehensible error messages (incomprehensible to me, at least).


>robout(vrmat,1)
Error in matrix(0, nrow, rep) : non-numeric matrix extent
>

>mahaout(vrmat,1)
Error in cov.rob(tempo, method = "classical") :
        at least 2 cases are needed
>


>baysout(vrmat)
Error in as.matrix(out.sort[1:n, ]) : subscript out of bounds
>


Can anybody tell me what I'm doing wrong?  I know there are a bunch of 
outlier detectors in R, can anyone recommend one (and perhaps show an 
example, preferably with my vrmat data set)?

Thanks!

--  TMK  --
212-460-5430	home
917-656-5351	cell


From p.dalgaard at biostat.ku.dk  Thu Mar 29 00:29:30 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 29 Mar 2007 00:29:30 +0200
Subject: [R] Large matrix into a vector
In-Reply-To: <1175119874.14657.71.camel@localhost.localdomain> (Marc
	Schwartz's message of "Wed, 28 Mar 2007 17:11:14 -0500")
References: <973479.12210.qm@web32406.mail.mud.yahoo.com>
	<1175103551.14657.51.camel@Bellerophon>
	<Pine.LNX.4.64.0703281847560.24235@gannet.stats.ox.ac.uk>
	<20070328215502.M35167@centroin.com.br>
	<1175119874.14657.71.camel@localhost.localdomain>
Message-ID: <x21wj9q9x1.fsf@viggo.kubism.ku.dk>

Marc Schwartz <marc_schwartz at comcast.net> writes:

> On Wed, 2007-03-28 at 19:55 -0200, Alberto Monteiro wrote:
>> Prof Brian Ripley wrote:
>> >
>> > We have already seen three solutions.
>> > 
>> > I don't like to see the use of c() for its side effects.  In this 
>> > case Marc's as.vector seems to me to be self-explanatory, and that 
>> > is a virtue in programming that is too often undervalued.
>> > 
>> I agree; but for our enlightnment, what are the side effects of
>> c()?
>> 
>> Alberto Monteiro
>
> I believe that Prof. Ripley is referring to the following, from the
> Details section in ?c:
>
>
> "c is sometimes used for its side effect of removing attributes except
> names, for example to turn an array into a vector. as.vector is a more
> intuitive way to do this, but also drops names."
>
>
> There are also examples in ?c of this behavior.

The terminology is a bit unfortunate, though. "Side effect" usually
means an effect that is not reflected in the return value of a
function, like printing, plotting, or assignment.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From Horace.Tso at pgn.com  Thu Mar 29 00:35:46 2007
From: Horace.Tso at pgn.com (Horace Tso)
Date: Wed, 28 Mar 2007 15:35:46 -0700
Subject: [R] Bonferroni p-value greater than 1
In-Reply-To: <756137.80918.qm@web32803.mail.mud.yahoo.com>
References: <003601c77168$ab48a230$0e010a0a@headquarters.silicoinsights>
	<756137.80918.qm@web32803.mail.mud.yahoo.com>
Message-ID: <460A8B52020000650000478C@pgn.com>

Hi folks,

I use the outlier.test in package car to test a lm model and the bonferroni p value returned is shown as NA. When the object is typed it indicates the p value is greater than 1. I'm not sure how to interpret it. 

Thanks in advance.

Horace W. Tso


> outlier.test(mod)$test
max|rstudent|            df  unadjusted p  Bonferroni p 
   2.04106376   18.00000000    0.05618628            NA 

> outlier.test(mod)

max|rstudent| = 2.041064, degrees of freedom = 18,
unadjusted p = 0.05618628, Bonferroni p > 1

Observation: 1 

The lm model looks fine to me,

> summary(mod)

Call:
lm(formula = x ~ ind, na.action = na.fail)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.2082 -0.5200  0.1309  0.5725  0.9593 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 59.84586    0.31900   187.6  < 2e-16 ***
ind         -0.16768    0.02541    -6.6 2.57e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.705 on 19 degrees of freedom
Multiple R-Squared: 0.6963,     Adjusted R-squared: 0.6803 
F-statistic: 43.56 on 1 and 19 DF,  p-value: 2.57


From buddha_314 at yahoo.com  Wed Mar 28 17:38:32 2007
From: buddha_314 at yahoo.com (Brian Dolan)
Date: Wed, 28 Mar 2007 08:38:32 -0700 (PDT)
Subject: [R] string into command
Message-ID: <129811.29458.qm@web36405.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070328/7e7d86ed/attachment.pl 

From Andy.Jacobson at noaa.gov  Thu Mar 29 01:01:19 2007
From: Andy.Jacobson at noaa.gov (Andy Jacobson)
Date: Wed, 28 Mar 2007 17:01:19 -0600
Subject: [R] options(error=recover) in .Rprofile
Message-ID: <53B338CF-3CFB-4CF4-BA49-F200BAE000FC@noaa.gov>

Hi,

I'd like to try using "options(error=recover) in my ~/.Rprofile, but  
it appears that the function "recover" is not defined during R  
startup when the .Rprofile is processed.  "recover" is defined after  
I get an R prompt, however.  Can anyone shed light on why this is,  
and whether a work-around is possible?

I've tested this on a variety of systems including recent linux and  
OSX, using R 2.4.0.  Seems to be pretty generic.

Thanks,

Andy

-- 
Andy Jacobson
andy.jacobson at noaa.gov

NOAA Earth System Research Lab
Global Monitoring Division
325 Broadway
Boulder, Colorado 80305

303/497-4916


From bcarvalh at jhsph.edu  Thu Mar 29 01:18:22 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Wed, 28 Mar 2007 19:18:22 -0400
Subject: [R] string into command
In-Reply-To: <129811.29458.qm@web36405.mail.mud.yahoo.com>
References: <129811.29458.qm@web36405.mail.mud.yahoo.com>
Message-ID: <36FE6246-B5CA-4BBB-839F-9BA69DC8A6B8@jhsph.edu>

cmd = "mylist = list(a = 5, b = 7)"
(eval(parse(text=cmd)))


b

On Mar 28, 2007, at 11:38 AM, Brian Dolan wrote:

> Hello,
>
> I would like to take the string
>
> "mylist = list(a = 5, b = 7)"
>
> and evaluate it as a list.  I have attempted to use parse and  
> several other functions with no success.
>
> Thanks for your time.
>
> -brian dolan
>
> ~~~
>
> may all your sequences converge
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at comcast.net  Thu Mar 29 01:20:43 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 28 Mar 2007 18:20:43 -0500
Subject: [R] string into command
In-Reply-To: <129811.29458.qm@web36405.mail.mud.yahoo.com>
References: <129811.29458.qm@web36405.mail.mud.yahoo.com>
Message-ID: <1175124043.14657.79.camel@localhost.localdomain>

On Wed, 2007-03-28 at 08:38 -0700, Brian Dolan wrote:
> Hello,
> 
> I would like to take the string
> 
> "mylist = list(a = 5, b = 7)"
> 
> and evaluate it as a list.  I have attempted to use parse and several
> other functions with no success.
> 
> Thanks for your time.


Not sure what combinations you may have tried, but here is a solution:

> ls()
character(0)

> eval(parse(text = "mylist <- list(a = 5, b = 7)"))

> ls()
[1] "mylist"

> mylist
$a
[1] 5

$b
[1] 7


HTH,

Marc Schwartz


From p.murrell at auckland.ac.nz  Thu Mar 29 01:32:08 2007
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Thu, 29 Mar 2007 11:32:08 +1200
Subject: [R] is this trellis device or standard graphics device?
In-Reply-To: <de8c7cb40703281121j2922fa95i59ef3ce3d4f38d94@mail.gmail.com>
References: <de8c7cb40703281121j2922fa95i59ef3ce3d4f38d94@mail.gmail.com>
Message-ID: <460AFAF8.3040509@stat.auckland.ac.nz>

Hi


Prasenjit Kapat wrote:
> OK, here is the issue:
> I have a graphcis device (say X11) which I can set as active by
> dev.set(n). Is there ANY way to identify/infer whether this X11 device
> was obtained from a lattice function (like xyplot, levelplot...) or a
> standard function (like plot, scatterplot,...).


You could call the grid function getNames() and if that is zero-length,
then the device is either blank or only contains traditional graphics.


> A related question is whether plot.new (or frame) was called to obtain
> this graphics device...


Try ...

recordPlot()[[1]][[1]]

... which should look something like this after plot.new() ...

[[1]]
.Primitive("plot.new")

[[2]]
NULL

... though you're starting to get on shakier ground here.

Paul



> Thanks.
> PK
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From sundar.dorai-raj at pdf.com  Thu Mar 29 01:46:54 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 28 Mar 2007 16:46:54 -0700
Subject: [R] options(error=recover) in .Rprofile
In-Reply-To: <53B338CF-3CFB-4CF4-BA49-F200BAE000FC@noaa.gov>
References: <53B338CF-3CFB-4CF4-BA49-F200BAE000FC@noaa.gov>
Message-ID: <460AFE6E.9010405@pdf.com>



Andy Jacobson said the following on 3/28/2007 4:01 PM:
> Hi,
> 
> I'd like to try using "options(error=recover) in my ~/.Rprofile, but  
> it appears that the function "recover" is not defined during R  
> startup when the .Rprofile is processed.  "recover" is defined after  
> I get an R prompt, however.  Can anyone shed light on why this is,  
> and whether a work-around is possible?
> 
> I've tested this on a variety of systems including recent linux and  
> OSX, using R 2.4.0.  Seems to be pretty generic.
> 
> Thanks,
> 
> Andy
> 

Perhaps this thread will help:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/84872.html

BTW, I found this using RSiteSearch("recover") in R.

HTH,

--sundar


From aa2007r at gmail.com  Thu Mar 29 01:29:49 2007
From: aa2007r at gmail.com (AA)
Date: Wed, 28 Mar 2007 19:29:49 -0400
Subject: [R] multi-level modeling  & R?
References: <460AA089.8000506@optonline.net><116313.82125.qm@web32802.mail.mud.yahoo.com>
	<003601c77168$ab48a230$0e010a0a@headquarters.silicoinsights>
Message-ID: <049e01c77190$fb173560$3927a8c0@treesdalellc.net>

Thanks Christos for this tip on the book. I needed something like that
get started on the topic.
AA.
----- Original Message ----- 
From: "Christos Hatzis" <christos at nuverabio.com>
To: "'John Kane'" <jrkrideau at yahoo.ca>; "'Chuck Cleland'" 
<ccleland at optonline.net>
Cc: "'R R-help'" <r-help at stat.math.ethz.ch>
Sent: Wednesday, March 28, 2007 2:41 PM
Subject: Re: [R] multi-level modeling & R?


> Also, take a look at Andrew Gelman's recent book with Jennifer Hill on
> multilevel modeling.
> It is written as a practical how-to text book using R as the primary
> platform.  I have found it very easy to read, extremely useful and very 
> good
> value for its price (I am not related to the authors or the publisher in 
> any
> way).
>
> http://www.stat.columbia.edu/~gelman/arm/
>
> -Christos
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Kane
>> Sent: Wednesday, March 28, 2007 2:25 PM
>> To: Chuck Cleland
>> Cc: R R-help
>> Subject: Re: [R] multi-level modeling & R?
>>
>> Thanks to everyone for the information and to Chuck for the
>> links. I must have misspelt something because I missed most
>> of them in the RsiteSearch.
>>
>> Anyway back to selling R :)
>>
>> --- Chuck Cleland <ccleland at optonline.net> wrote:
>>
>> > John Kane wrote:
>> > > A colleague was asking me if R does multi-level modelling
>> as opposed
>> > > to multiple regression.
>> > Since I
>> > > have no knowledge of multi-level modelling (except
>> > 5
>> > > minutes googling )  I thought that I would as
>> > here.
>> > >
>> > > Does are offer any multi-level modeling packages?
>> > It
>> > > looked like arm might be one but I was not sure.
>> >
>> >   RSiteSearch("multilevel model") and a search for "multilevel" on
>> > CRAN point to a number of other relevant packages and docs,
>> including:
>> >
>> >
>> http://finzi.psych.upenn.edu/R/library/nlme/html/lme.html
>> >
>> >
>> http://cran.r-project.org/src/contrib/Descriptions/lme4.html
>> >
>> > http://cran.r-project.org/doc/packages/mlmRev.pdf
>> >
>> >
>> http://cran.r-project.org/src/contrib/Descriptions/multilevel.html
>> >
>> >
>> http://cran.r-project.org/doc/contrib/Bliese_Multilevel.pdf
>> >
>> > > Thanks
>> > >
>> > > ______________________________________________
>> > > R-help at stat.math.ethz.ch mailing list
>> > > https://stat.ethz.ch/mailman/listinfo/r-help
>> > > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > > and provide commented, minimal, self-contained,
>> > reproducible code.
>> >
>> > --
>> > Chuck Cleland, Ph.D.
>> > NDRI, Inc.
>> > 71 West 23rd Street, 8th floor
>> > New York, NY 10010
>> > tel: (212) 845-4495 (Tu, Th)
>> > tel: (732) 512-0171 (M, W, F)
>> > fax: (917) 438-0894
>> >
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bates at stat.wisc.edu  Thu Mar 29 01:21:57 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 28 Mar 2007 18:21:57 -0500
Subject: [R] Use of 'defineVar' and 'install' in .Call
In-Reply-To: <f13f1c9c0703270156hba5b26fyb541817bed557b0c@mail.gmail.com>
References: <f13f1c9c0703270156hba5b26fyb541817bed557b0c@mail.gmail.com>
Message-ID: <40e66e0b0703281621u58200b39oe3695fc4eaa6d948@mail.gmail.com>

I did read your second message about the problem symptoms disappearing
but I thought that I might make a couple of suggestions about your
code anyway.

There are a number of helper functions declared in Rinternals.h such
as ScalarReal, which is equivalent to your mkans.  (Also
ScalarInteger, ScalarLogical, ...) The functions to convert scalars in
the other direction, i.e. taking an SEXP and returning an int or a
double or a ... , are called asInteger, asReal, ...

If you are writing a package you can define an initialization function
called R_init_<pkgname> to perform initialization for the package.  If
you are going to use a symbol like x many times then you can save the
result of install("x") as a global, say, myPkg_xSymbol during
initialization and use the global variable instead of calling install
many times.  The overhead for calling install is small but if you can
avoid it I don't see a reason not to.

Finally, why do you want to use identifiers like u1, u2, ... when you
could pass a vector named "u" and use that.  In other words, wouldn't
it be easier to do the extraction of the components in the R code for
the function rather than generating a whole bunch of different names?

I suggest that this thread be moved to the r-devel list, which I have cc:d.

On 3/27/07, Daniel Berg <daniel at danielberg.no> wrote:
> Dear all,
>
> [system and version information below]
>
> I am trying to modify a C function for finding the root of an
> expression. The function is to be called from R as .Call with input
> parameters:
>
> f: expression for which we will find the root
> guesses: interval for the solution
> stol: tolerance
> rho: environment
>
> The original functions I use are:
>
> SEXP mkans(double x) {
>   SEXP ans;
>   PROTECT(ans = allocVector(REALSXP, 1));
>   REAL(ans)[0] = x;
>   UNPROTECT(1);
>   return ans;
> }
> double feval(double x, SEXP f, SEXP rho) {
>   defineVar(install("x"), mkans(x), rho);
>   return(REAL(eval(f, rho))[0]);
> }
> SEXP zero(SEXP f, SEXP guesses, SEXP stol, SEXP rho) {
>   double x0 = REAL(guesses)[0], x1 = REAL(guesses)[1], tol = REAL(stol)[0];
>   double f0, f1, fc, xc;
>   if(tol <= 0.0) error("non-positive tol value");
>   f0 = feval(x0, f, rho); f1 = feval(x1, f, rho);
>   if(f0 == 0.0) return mkans(x0);
>   if(f1 == 0.0) return mkans(x1);
>   if(f0*f1 > 0.0) error("x[0] and x[1] have the same sign");
>   for(;;) {
>     xc = 0.5*(x0+x1);
>     if(fabs(x0-x1) < tol) return mkans(xc);
>     fc = feval(xc, f, rho);
>     if(fc == 0) return mkans(xc);
>     if(f0*fc > 0.0) {
>       x0 = xc; f0 = fc;
>     }
>     else {
>       x1 = xc; f1 = fc;
>     }
>   }
> }
>
>
> This works great. However, I wish to make it more general, by
> modifying 'feval'. Given that my problem involves a data set 'u', with
> dimension (i x j), I need to assign values to 'u1', 'u2', ..., 'ui'
> via defineVar(install(...)). I tried the following:
>
> double feval(double x, double *u, int d, double v, SEXP f, SEXP rho) {
>   int i;
>   char *str1="u", str2[1001], *str3;
>   defineVar(install("x"), mkans(x), rho);
>   defineVar(install("y"), mkans(v), rho);
>   for(i=0;i<d;i++) {
>     sprintf(str2,"%d",i+1);
>     str3 = (char *)malloc((strlen(str1)+strlen(str2)+1)*sizeof(char));
>     strcpy(str3,str1);
>     strcat(str3,str2);
>     defineVar(install(str3), mkans(u[i]), rho);
>   }
>   free(str3);
>   return(REAL(eval(f,rho))[0]);
> }
>
> My R-package still compiles without errors but R crashes due to the
> defineVar command.
>
> Any suggestions to how I can do the defineVar bit?
>
> Thanks in advance.
>
> Reagards,
> Daniel Berg
>
> --------------------------------------------
> > R.Version()
> $platform
> [1] "i486-pc-linux-gnu"
> $arch
> [1] "i486"
> $os
> [1] "linux-gnu"
> $system
> [1] "i486, linux-gnu"
> $status
> [1] ""
> $major
> [1] "2"
> $minor
> [1] "3.1"
> $year
> [1] "2006"
> $month
> [1] "06"
> $day
> [1] "01"
> $`svn rev`
> [1] "38247"
> $language
> [1] "R"
> $version.string
> [1] "Version 2.3.1 (2006-06-01)"
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tlumley at u.washington.edu  Thu Mar 29 02:28:20 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 28 Mar 2007 17:28:20 -0700 (PDT)
Subject: [R] what is the difference between survival analysis and (...)
In-Reply-To: <460A8A4B.1020900@vanderbilt.edu>
Message-ID: <Pine.LNX.4.43.0703281728200.3629@hymn02.u.washington.edu>

On Wed, 28 Mar 2007, Frank E Harrell Jr wrote:

> Eric Elguero wrote:
>> Hi everybody,
>>
>> recently I had to teach a course on Cox model, of which I am
>> not a specialist, to an audience of medical epidemiologists.
>> Not a good idea you might say.. anyway, someone in the
>> audience was very hostile. At some point, he sayed that
>> Cox model was useless, since all you have to do is count
>> who dies and who survives, divide by the sample sizes
>> and compute a relative risk, and if there was significant
>> censoring, use cumulated follow-up instead of sample
>> sizes and that's it!
>> I began arguing that in Cox model you could introduce
>> several variables, interactions, etc, then I remembered
>> of logistic models ;-)
>> The only (and poor) argument I could think of was that
>> if mr Cox took pains to devise his model, there should
>> be some reason...
>
> That is a very ignorant person, concerning statistical
> efficiency/power/precision and how to handle incomplete follow-up
> (variable follow-up duration).  There are papers in the literature (I
> wish I had them at my fingertips) that go into the efficiency loss of
> just counting events.  If the events are very rare, knowing the time
> doesn't help as much, but the Cox model still can handle censoring
> correctly and that person's approach doesn't.
>

Certainly just counting the events is inefficient -- the simplest example would be studies of some advanced cancers where nearly everyone dies during followup, so that there is little or no censoring but simple counts are completely uninformative.

It's relatively hard to come up with an example where using the total-time-on-test (rather than sample size) as a denominator is much worse than the Cox mode, though. You need the baseline hazard to vary a lot over time and the censoring patterns to be quite different in the groups, but proportional hazards to still hold.

I think the advantages of the Cox model over a reasonably sensible person-time analysis are real, but not dramatic -- it would be hard to find a data set that would convince the sort of person who would make that sort of claim.

I would argue that computational convenience on the one hand, and the ability to exercise lots of nice mathematical tools on the other hand have also contributed to the continuing popularity of the Cox model.


     -thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From jfox at mcmaster.ca  Thu Mar 29 02:37:54 2007
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 28 Mar 2007 20:37:54 -0400
Subject: [R] Bonferroni p-value greater than 1
In-Reply-To: <460A8B52020000650000478C@pgn.com>
Message-ID: <20070329003756.MOIR1624.tomts40-srv.bellnexxia.net@JohnDesktop8300>

Dear Horace,

The Bonferonni p-value is obtained from the "unadjusted" p-value by
multiplying the latter by the number of observations, and provides a
conservative (although usually quite accurate) outlier test. When the
adjusted p-value exceeds 1 you can take that as an indication that there are
no unusually large studentized residuals (and indeed that the largest
studentized residual is smaller than one would expect under the standard
linear-model assumptions). 

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Horace Tso
> Sent: Wednesday, March 28, 2007 6:36 PM
> To: 'R R-help'
> Subject: [R] Bonferroni p-value greater than 1
> 
> Hi folks,
> 
> I use the outlier.test in package car to test a lm model and 
> the bonferroni p value returned is shown as NA. When the 
> object is typed it indicates the p value is greater than 1. 
> I'm not sure how to interpret it. 
> 
> Thanks in advance.
> 
> Horace W. Tso
> 
> 
> > outlier.test(mod)$test
> max|rstudent|            df  unadjusted p  Bonferroni p
>    2.04106376   18.00000000    0.05618628            NA 
> 
> > outlier.test(mod)
> 
> max|rstudent| = 2.041064, degrees of freedom = 18,
> unadjusted p = 0.05618628, Bonferroni p > 1
> 
> Observation: 1 
> 
> The lm model looks fine to me,
> 
> > summary(mod)
> 
> Call:
> lm(formula = x ~ ind, na.action = na.fail)
> 
> Residuals:
>     Min      1Q  Median      3Q     Max 
> -1.2082 -0.5200  0.1309  0.5725  0.9593 
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)    
> (Intercept) 59.84586    0.31900   187.6  < 2e-16 ***
> ind         -0.16768    0.02541    -6.6 2.57e-06 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> Residual standard error: 0.705 on 19 degrees of freedom
> Multiple R-Squared: 0.6963,     Adjusted R-squared: 0.6803 
> F-statistic: 43.56 on 1 and 19 DF,  p-value: 2.57
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From kapatp at gmail.com  Thu Mar 29 05:41:06 2007
From: kapatp at gmail.com (Prasenjit Kapat)
Date: Wed, 28 Mar 2007 23:41:06 -0400
Subject: [R] is this trellis device or standard graphics device?
In-Reply-To: <460AFAF8.3040509@stat.auckland.ac.nz>
References: <de8c7cb40703281121j2922fa95i59ef3ce3d4f38d94@mail.gmail.com>
	<460AFAF8.3040509@stat.auckland.ac.nz>
Message-ID: <200703282341.06422.kapatp@gmail.com>

Hi,

On Wednesday 28 March 2007 07:32:08 pm Paul Murrell wrote:
> You could call the grid function getNames() and if that is zero-length,
> then the device is either blank or only contains traditional graphics.

Thanks Dr. Murrell. I was looking for something simple like this.

>
> > A related question is whether plot.new (or frame) was called to obtain
> > this graphics device...
>
> Try ...
>
> recordPlot()[[1]][[1]]
>
> ... which should look something like this after plot.new() ...
>
> [[1]]
> .Primitive("plot.new")
>
> [[2]]
> NULL
>
> ... though you're starting to get on shakier ground here.
>
> Paul

PK.


From tijahbus at gmail.com  Thu Mar 29 08:25:06 2007
From: tijahbus at gmail.com (subhajit dutta)
Date: Thu, 29 Mar 2007 11:55:06 +0530
Subject: [R] to use EM algorithm for MLE
Message-ID: <cab59a750703282325u66409d62h98a0a2fb7d159d44@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070329/7dd936b6/attachment.pl 

From daniel at danielberg.no  Thu Mar 29 08:39:02 2007
From: daniel at danielberg.no (Daniel Berg)
Date: Thu, 29 Mar 2007 08:39:02 +0200
Subject: [R] Replacement in an expression - can't use parse()
In-Reply-To: <971536df0703280437h757bb5aaw523a0bc0e65e8b0@mail.gmail.com>
References: <f13f1c9c0703270656t56dd736axb46b334575b136f5@mail.gmail.com>
	<46092A73.6020302@biostat.ku.dk>
	<Pine.LNX.4.64.0703271000290.28829@homer24.u.washington.edu>
	<f13f1c9c0703280109o4a833d63pc722c088957b888e@mail.gmail.com>
	<971536df0703280437h757bb5aaw523a0bc0e65e8b0@mail.gmail.com>
Message-ID: <f13f1c9c0703282339p7209c761j8a2eb89078a5b84c@mail.gmail.com>

Thank you Gabor, your solution works great, even for veeery long expressions.

//Daniel

On 3/28/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> The solution in my post has the advantage of not using
> eval or character conversions (except in setting up
> L where you want such character conversions to
> build up the names, as your more general sitution
> shows).  The following is the same as in that post except
> the line setting L in that post is replaced with the 2 lines
> setting i, j and L.  Setting L in this way would also work
> in conjunction with some of the other solutions too:
>
> e <- expression(u1 + u2 + u3)
> i <- 1; j <- 2; L <- list()
> L[[paste("u", i, sep = "")]] <- as.name(paste("u", j, sep = ""))
> as.expression(do.call(substitute, list(as.call(e), L))[[1]])
>
> One can streamline it further by making it a function and
> using assign:
>
> subu <- function(e, i, j) {
>         assign(paste("u", i, sep = ""), as.name(paste("u", j, sep = "")))
>         as.expression(do.call(substitute, list(as.call(e)))[[1]])
> }
>
> e <- expression(u1 + u2 + u3)
> subu(e, 1, 2)
>
> On 3/28/07, Daniel Berg <daniel at danielberg.no> wrote:
> > Thank you very much for your good suggestions.
> > I have chosen to pursue the suggestion by Peter which worked like a dream :)
> > However, my problem is slightly more complicated still. I apologize
> > for not mentioning this in the initial question.
> >
> > I have to do the evaluation inside a loop, not knowing explicitly
> > which 'u' to replace. This is given by the loop I'm in, say something
> > like:
> >
> > i <- 2; j <- 3
> > eval(substitute(substitute(call,list(paste("u",i,sep="")=quote(x),paste("u",j,sep="")=1)),list(call=e[[1]])))
> >
> > But this returns a syntax error.
> > Any further suggestions?
> >
> > Regards,
> > Daniel
> >
> > On 3/27/07, Thomas Lumley <tlumley at u.washington.edu> wrote:
> > > On Tue, 27 Mar 2007, Peter Dalgaard wrote:
> > >
> > > >The way around this is to add a further layer of substitute() to insert
> > > >the value of e:
> > >
> > > >> eval(substitute(substitute(call,list(u2=quote(x),u3=1)),list(call=e[[1]])))
> > > > u1 + x + 1
> > >
> > > Or eval(do.call(substitute, list(e[[1]], list(u2=quote(x),u3=1)))
> > >
> > >         -thomas
> > >
> > >
> >
> >
> > --
> > danielberg.no
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>


-- 
danielberg.no


From p.dalgaard at biostat.ku.dk  Thu Mar 29 08:42:27 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 29 Mar 2007 08:42:27 +0200
Subject: [R] Bonferroni p-value greater than 1
In-Reply-To: <20070329003756.MOIR1624.tomts40-srv.bellnexxia.net@JohnDesktop8300>
References: <20070329003756.MOIR1624.tomts40-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <460B5FD3.1030907@biostat.ku.dk>

John Fox wrote:
> Dear Horace,
>
> The Bonferonni p-value is obtained from the "unadjusted" p-value by
> multiplying the latter by the number of observations, and provides a
> conservative (although usually quite accurate) outlier test. When the
> adjusted p-value exceeds 1 you can take that as an indication that there are
> no unusually large studentized residuals (and indeed that the largest
> studentized residual is smaller than one would expect under the standard
> linear-model assumptions). 
>
>   
Yes, or put differently, the Bonferroni p is an upper bound of the true 
p. It is only accurate at the low end of the p scale. (It works by 
approximating P(A or B) by P(A)+P(B); since P(A and B) gets counted 
twice (make a diagram) that term needs to be small.)

BTW. This was yet another case of someone tacking their mail onto a 
completely different thread (replying to a random mail from r-help). 
Please avoid, since it confuses threading mail programs and the 
archiving system). I was looking for responses, so shifted to threaded 
mode and the post all but disappeared because it got tucked in under 
"multi-level modeling & R?"

> I hope this helps,
>  John
>
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
>
>   
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Horace Tso
>> Sent: Wednesday, March 28, 2007 6:36 PM
>> To: 'R R-help'
>> Subject: [R] Bonferroni p-value greater than 1
>>
>> Hi folks,
>>
>> I use the outlier.test in package car to test a lm model and 
>> the bonferroni p value returned is shown as NA. When the 
>> object is typed it indicates the p value is greater than 1. 
>> I'm not sure how to interpret it. 
>>
>> Thanks in advance.
>>
>> Horace W. Tso
>>
>>
>>     
>>> outlier.test(mod)$test
>>>       
>> max|rstudent|            df  unadjusted p  Bonferroni p
>>    2.04106376   18.00000000    0.05618628            NA 
>>
>>     
>>> outlier.test(mod)
>>>       
>> max|rstudent| = 2.041064, degrees of freedom = 18,
>> unadjusted p = 0.05618628, Bonferroni p > 1
>>
>> Observation: 1 
>>
>> The lm model looks fine to me,
>>
>>     
>>> summary(mod)
>>>       
>> Call:
>> lm(formula = x ~ ind, na.action = na.fail)
>>
>> Residuals:
>>     Min      1Q  Median      3Q     Max 
>> -1.2082 -0.5200  0.1309  0.5725  0.9593 
>>
>> Coefficients:
>>             Estimate Std. Error t value Pr(>|t|)    
>> (Intercept) 59.84586    0.31900   187.6  < 2e-16 ***
>> ind         -0.16768    0.02541    -6.6 2.57e-06 ***
>> ---
>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
>>
>> Residual standard error: 0.705 on 19 degrees of freedom
>> Multiple R-Squared: 0.6963,     Adjusted R-squared: 0.6803 
>> F-statistic: 43.56 on 1 and 19 DF,  p-value: 2.57
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jacob.ricky at gmail.com  Thu Mar 29 09:12:01 2007
From: jacob.ricky at gmail.com (Ricky Jacob)
Date: Thu, 29 Mar 2007 12:42:01 +0530
Subject: [R] Impute Values for Forest Inventory
Message-ID: <5df165dc0703290012w190ab5e9g127c581617c470f7@mail.gmail.com>

Dear All,
I am Ricky Jacob, a project Student from India who is working on
Forest Inventories.
Input data:
Plot(area = .1 ha) data having the following information:
1) Basal Area
2)Tree Density
3)Volume
So I am applying this information to the corresponding pixels in the
satellite imagery of the study area.  I also have given the same
values to a 3x3 window around that pixel.
Inventory is taken compartmentwise and so this information of all the
plots in a compartment is taken and generalised over all the pixels
falling in that compartment.  To make the estimation more realistic, i
decided to use the impute opton in r.
The yai requires me to enter the x and y variable.  I stll haven't
figured out how i should put my datasets to impute the values of
variables like Basal area, volume for only those pixels for which no
data is available.
is msn or randomforest a better option.
I tried to work on the example MoscowMtStJoe dataset.  But I wasn't
able to figure out how I should put in my data.
I have records which have both satellite and attribute data and also
have records with only satellite data.  I need to impute attribute
data into those records alone for which no attribute data is available
without altering values in the records having the attribute
information.

Hope for a response
Regards
Ricky Jacob


From gyadav at ccilindia.co.in  Thu Mar 29 09:35:55 2007
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Thu, 29 Mar 2007 13:05:55 +0530
Subject: [R] C interface
Message-ID: <OF62010BDF.307A44D6-ON652572AD.00299628-652572AD.0029AF45@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070329/ba7ebf19/attachment.pl 

From klausch at gmx.de  Thu Mar 29 09:46:36 2007
From: klausch at gmx.de (Klaus Nordhausen)
Date: Thu, 29 Mar 2007 09:46:36 +0200
Subject: [R] Tail area of sum of  Chi-square variables
Message-ID: <20070329074636.204120@gmx.net>

Dear R experts, 

I was wondering if there are any R functions that give the tail area
of a sum of chisquare distributions of the type:
         a_1 X_1 + a_2 X_2
where a_1 and a_2 are constants and X_1 and X_2 are independent chi-square variables with different degrees of freedom.

Thanks,

Klaus
-- 
"Feel free" - 5 GB Mailbox, 50 FreeSMS/Monat ...


From ajung at gfz-potsdam.de  Thu Mar 29 09:44:08 2007
From: ajung at gfz-potsdam.de (Andre Jung)
Date: Thu, 29 Mar 2007 09:44:08 +0200
Subject: [R] Plot degree symbol by itself
Message-ID: <460B6E48.4040809@gfz-potsdam.de>

Dear all,

I'm trying to plot the degree symbol by itself between two square
brackets. I just want to have "K [?]". So far I got to:

expression(K ~ group("[",1*degree,"]"))
or
expression(K ~ group("[",1^o,"]"))

But it won't work without a number or letter.

Any suggestions?

Thanks,
andre


From Achim.Zeileis at wu-wien.ac.at  Thu Mar 29 10:11:13 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 29 Mar 2007 10:11:13 +0200 (CEST)
Subject: [R] Tail area of sum of  Chi-square variables
In-Reply-To: <20070329074636.204120@gmx.net>
References: <20070329074636.204120@gmx.net>
Message-ID: <Pine.LNX.4.64.0703291008380.5078@eowyn>

Hi Klausch:

> I was wondering if there are any R functions that give the tail area
> of a sum of chisquare distributions of the type:
>         a_1 X_1 + a_2 X_2
> where a_1 and a_2 are constants and X_1 and X_2 are independent 
> chi-square variables with different degrees of freedom.

Christian Kleiber (Cc on this reply) has code for computing the 
distribution function of general linear combinations of chi-squared 
variables. It's not yet on CRAN, but just ask him for a devel-snapshot.

Best wishes,
Z


From ripley at stats.ox.ac.uk  Thu Mar 29 10:10:59 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Mar 2007 09:10:59 +0100 (BST)
Subject: [R] C interface
In-Reply-To: <OF62010BDF.307A44D6-ON652572AD.00299628-652572AD.0029AF45@ccilindia.co.in>
References: <OF62010BDF.307A44D6-ON652572AD.00299628-652572AD.0029AF45@ccilindia.co.in>
Message-ID: <Pine.LNX.4.64.0703290905260.24845@gannet.stats.ox.ac.uk>

We don't know what document that is, and you haven't given us a useful 
pointer, have you?  And the authors (presumably Roger Peng and Jan de 
Leeuw) deserve credit.

The definitive documentation is the 'Writing R Extensions' manual which 
ships with R.  There is also a lot in 'S Programming' (see the books in 
the FAQ), and many examples in the R sources and contributed packages.

On Thu, 29 Mar 2007, gyadav at ccilindia.co.in wrote:

>
> Hi All
> I have read this document - An Introduction to the .C Interface to R,
> which primarily tells how to interface C language with R.
> Is there and more elaborative and online documentation regarding this
> interface.
>
> Any pointers appreciated
> -thanks
> -gaurav
>
> ============================================================================================
> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and ...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From han at dmi.dk  Thu Mar 29 10:28:18 2007
From: han at dmi.dk (Henrik Andersson)
Date: Thu, 29 Mar 2007 10:28:18 +0200
Subject: [R] Plot degree symbol by itself
In-Reply-To: <460B6E48.4040809@gfz-potsdam.de>
References: <460B6E48.4040809@gfz-potsdam.de>
Message-ID: <460B78A2.5060304@dmi.dk>

Andre Jung wrote:
> Dear all,
>
> I'm trying to plot the degree symbol by itself between two square
> brackets. I just want to have "K [?]". So far I got to:
>
> expression(K ~ group("[",1*degree,"]"))
> or
> expression(K ~ group("[",1^o,"]"))
>
> But it won't work without a number or letter.
>
>   
This worked for me:
expression(K ~ group("[",degree,"]"))
> Any suggestions?
>
> Thanks,
> andre
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
Henrik Andersson

Danish Meteorological Institute
Lyngbyvej 100
2100 Copenhagen ?
Denmark
Tel: +45 39157215 
Email: han at dmi.dk


From p.dalgaard at biostat.ku.dk  Thu Mar 29 10:36:54 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 29 Mar 2007 10:36:54 +0200
Subject: [R] Plot degree symbol by itself
In-Reply-To: <460B6E48.4040809@gfz-potsdam.de>
References: <460B6E48.4040809@gfz-potsdam.de>
Message-ID: <460B7AA6.1090706@biostat.ku.dk>

Andre Jung wrote:
> Dear all,
>
> I'm trying to plot the degree symbol by itself between two square
> brackets. I just want to have "K [?]". So far I got to:
>
> expression(K ~ group("[",1*degree,"]"))
> or
> expression(K ~ group("[",1^o,"]"))
>
> But it won't work without a number or letter.
>
> Any suggestions?
>
>   
An empty pair of braces usually works:

plot(0)
text(1,1,expression(K ~ group("[",{}*degree,"]")))

(or {}^degree, but that looks odd -- degree is already a raised symbol)

But whatever was wrong with just
 
text(1.2,1,expression(K ~ group("[",degree,"]")))

??
> Thanks,
> andre
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ezhil02 at yahoo.com  Thu Mar 29 11:12:50 2007
From: ezhil02 at yahoo.com (A Ezhil)
Date: Thu, 29 Mar 2007 02:12:50 -0700 (PDT)
Subject: [R] Large matrix into a vector
In-Reply-To: <x21wj9q9x1.fsf@viggo.kubism.ku.dk>
Message-ID: <725257.45454.qm@web32406.mail.mud.yahoo.com>

Hi All,

Thank you very much for all your suggestions. It's a
great learning for me. All the three suggested
solutions seem working. I don't know what 'side
effects' that you were talking about. To summarize the
responses:

> s <- read.table("sample.txt", sep="\t")
> s1 <- as.matrix(s)
> s1
    V1   V2   V3
1 0.59 0.47 0.44
2 0.85 0.42 0.57
3 0.48 0.57 0.57
4 0.61 0.24 0.24
5 0.38 0.21 0.36
6 0.65 0.42 1.50
7 0.49 0.23 0.42
8 0.60 0.51 0.53
9 0.00 0.00 0.00

> s2 <- as.vector(s1)
> s2
 [1] 0.59 0.85 0.48 0.61 0.38 0.65 0.49 0.60 0.00 0.47
0.42 0.57 0.24 0.21 0.42 0.23 0.51 0.00 0.44 0.57 0.57
0.24 0.36
[24] 1.50 0.42 0.53 0.00
> s3 <- c(s1)
> s3
 [1] 0.59 0.85 0.48 0.61 0.38 0.65 0.49 0.60 0.00 0.47
0.42 0.57 0.24 0.21 0.42 0.23 0.51 0.00 0.44 0.57 0.57
0.24 0.36
[24] 1.50 0.42 0.53 0.00
> s3 <- s1
> dim(s3) <- NULL
> s3
 [1] 0.59 0.85 0.48 0.61 0.38 0.65 0.49 0.60 0.00 0.47
0.42 0.57 0.24 0.21 0.42 0.23 0.51 0.00 0.44 0.57 0.57
0.24 0.36
[24] 1.50 0.42 0.53 0.00

Interestingly, if I apply the same three solutions to
the data.frame 's', the results are different: (1)
as.vector() keeps the data frame as it is. (2) c()
changes into lists, and (3) making dim() <- NULL keeps
as structure. 

Thanks again.
Kind regards,
Ezhil


--- Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:

> Marc Schwartz <marc_schwartz at comcast.net> writes:
> 
> > On Wed, 2007-03-28 at 19:55 -0200, Alberto
> Monteiro wrote:
> >> Prof Brian Ripley wrote:
> >> >
> >> > We have already seen three solutions.
> >> > 
> >> > I don't like to see the use of c() for its side
> effects.  In this 
> >> > case Marc's as.vector seems to me to be
> self-explanatory, and that 
> >> > is a virtue in programming that is too often
> undervalued.
> >> > 
> >> I agree; but for our enlightnment, what are the
> side effects of
> >> c()?
> >> 
> >> Alberto Monteiro
> >
> > I believe that Prof. Ripley is referring to the
> following, from the
> > Details section in ?c:
> >
> >
> > "c is sometimes used for its side effect of
> removing attributes except
> > names, for example to turn an array into a vector.
> as.vector is a more
> > intuitive way to do this, but also drops names."
> >
> >
> > There are also examples in ?c of this behavior.
> 
> The terminology is a bit unfortunate, though. "Side
> effect" usually
> means an effect that is not reflected in the return
> value of a
> function, like printing, plotting, or assignment.
> 
> -- 
>    O__  ---- Peter Dalgaard             ?ster
> Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099,
> 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark     
>     Ph:  (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             
>     FAX: (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
> 



 
____________________________________________________________________________________
Food fight? Enjoy some healthy debate


From ripley at stats.ox.ac.uk  Thu Mar 29 11:22:57 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Mar 2007 10:22:57 +0100 (BST)
Subject: [R] gls bug?
In-Reply-To: <98284.72109.qm@web86908.mail.ukl.yahoo.com>
References: <98284.72109.qm@web86908.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.64.0703291020450.1343@gannet.stats.ox.ac.uk>

On Fri, 16 Mar 2007, simon bond wrote:

> I found that the following code crashes R (version 2.4.0 in windows).
>
>> x=rnorm(10,0.1,1)
>> library(nlme)
>> gls(x~0)
>
> I quickly found a work-around for what I was trying to do, but I thought 
> I should report this.

This will be fixed (to give a sensible non-fatal error message) in nlme 
3.1-80.

If you are still using 2.4.0 you probably have not updated your 
recommended packages either: please use sessionInfo() in bug reports as
requested in the R posting guide.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ronggui.huang at gmail.com  Thu Mar 29 11:29:10 2007
From: ronggui.huang at gmail.com (ronggui)
Date: Thu, 29 Mar 2007 17:29:10 +0800
Subject: [R] C interface
In-Reply-To: <Pine.LNX.4.64.0703290905260.24845@gannet.stats.ox.ac.uk>
References: <OF62010BDF.307A44D6-ON652572AD.00299628-652572AD.0029AF45@ccilindia.co.in>
	<Pine.LNX.4.64.0703290905260.24845@gannet.stats.ox.ac.uk>
Message-ID: <38b9f0350703290229p4bf18080nf5e2829f6b46d0a4@mail.gmail.com>

On 3/29/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> We don't know what document that is, and you haven't given us a useful
> pointer, have you?  And the authors (presumably Roger Peng and Jan de
> Leeuw) deserve credit.
It is a great point. I have googled and confirm that the authors are
Roger Peng and Jan de
Leeuw. Here is the link
http://www.biostat.jhsph.edu/~rpeng/docs/interface.pdf

> The definitive documentation is the 'Writing R Extensions' manual which
> ships with R.  There is also a lot in 'S Programming' (see the books in
> the FAQ), and many examples in the R sources and contributed packages.
>
> On Thu, 29 Mar 2007, gyadav at ccilindia.co.in wrote:
>
> >
> > Hi All
> > I have read this document - An Introduction to the .C Interface to R,
> > which primarily tells how to interface C language with R.
> > Is there and more elaborative and online documentation regarding this
> > interface.
> >
> > Any pointers appreciated
> > -thanks
> > -gaurav
> >
> > ============================================================================================
> > DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and ...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ronggui Huang
Department of Sociology
Fudan University, Shanghai, China


From sergio.della.franca at gmail.com  Thu Mar 29 12:29:12 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Thu, 29 Mar 2007 12:29:12 +0200
Subject: [R] Fanny Clustering
Message-ID: <b490ce570703290329w580f2ad1w34859b61d21cbe28@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070329/5d6b2e57/attachment.pl 

From phgrosjean at sciviews.org  Thu Mar 29 12:37:48 2007
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 29 Mar 2007 12:37:48 +0200
Subject: [R] Fanny Clustering
In-Reply-To: <b490ce570703290329w580f2ad1w34859b61d21cbe28@mail.gmail.com>
References: <b490ce570703290329w580f2ad1w34859b61d21cbe28@mail.gmail.com>
Message-ID: <460B96FC.5070900@sciviews.org>

1) Reduce the size of your sample (random or stratified subsampling),

2) Increase the memory of your computer available to R.

Best,

Philippe Grosjean

..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Belgium
( ( ( ( (
..............................................................

Sergio Della Franca wrote:
> Dear R-Helpers,
> 
> 
> I'd like to develop a fanny clustering on my data set(70.000 rows), but when
> i run the procedure i obtain this error:
> 
> error in vector("double", lenght): too big dimension for
> the selected vector.
> 
> 
> How can i solve this problem?
> 
> 
> Thank you in advance.
> 
> 
> Sergio Della Franca.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murali.menon at uk.abnamro.com  Thu Mar 29 12:40:00 2007
From: murali.menon at uk.abnamro.com (murali.menon at uk.abnamro.com)
Date: Thu, 29 Mar 2007 11:40:00 +0100
Subject: [R] creating conditional list of elements
In-Reply-To: <OF0B44B0EB.48E10BFD-ONC12572AD.00373940-802572AD.003841F1@LocalDomain>
Message-ID: <OFAFBC0563.B4CC92F6-ONC12572AD.0039BCB4-802572AD.003AA727@abnamro.com>

Sorry to plague the list, but I think I got the answer. The following 
would do:

> signalList <- list(tradingRules$Signal[tradingRules$Enabled]) [[1]]
> length(signalList)
[1] 2

Now my problem is shifted: I have the Signal column in the original data 
frame referring to actual 
matrices previously created in R. That is, bar_signal and cif_signal are 
extant matrices. What I 
need is the minimum number of rows in these matrices, so what I plan to do 
is:

> n <- min(sapply(signalList, NROW))

but this doesn't work (it returns 1, but I have 2800 rows in each of 
bar_signal and cif_signal, so I should
get 2800)

Is there a smart way to do this? And is there a way to tell R that the 
entries in the Signal column 
of tradingSignal are the names of objects?

Thanks,
Murali



Murali Menon/GB/ABNAMRO/NL 
29/03/2007 11:13

To
r-help at stat.math.ethz.ch
cc

Subject
creating conditional list of elements





Folks,

I have a matrix as follows (first column is the rownames, first row is the 
columnnames)

Rule    Enabled Signal
Foo     False           foo_signal
Bar     True            bar_signal
Gum     False           gum_signal
Cif     True            cif_signal

I would like to create a list of only those signals whose 'enabled' flag 
is True. So in the above 
case I should end up with

signalList = bar_signal, cif_signal

Likewise, if the enabled flags were all set to False, then signalList 
should be an empty list.

What's a good way to achieve this, please?

Thanks,

Murali



---------------------------------------------------------------------------
This message (including any attachments) is confidential and...{{dropped}}


From sergio.della.franca at gmail.com  Thu Mar 29 12:45:39 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Thu, 29 Mar 2007 12:45:39 +0200
Subject: [R] Fanny Clustering
In-Reply-To: <460B96FC.5070900@sciviews.org>
References: <b490ce570703290329w580f2ad1w34859b61d21cbe28@mail.gmail.com>
	<460B96FC.5070900@sciviews.org>
Message-ID: <b490ce570703290345y156ad31fsac7a41d41ecbe9f4@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070329/e98ef65a/attachment.pl 

From phgrosjean at sciviews.org  Thu Mar 29 13:06:08 2007
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 29 Mar 2007 13:06:08 +0200
Subject: [R] Fanny Clustering
In-Reply-To: <b490ce570703290345y156ad31fsac7a41d41ecbe9f4@mail.gmail.com>
References: <b490ce570703290329w580f2ad1w34859b61d21cbe28@mail.gmail.com>	<460B96FC.5070900@sciviews.org>
	<b490ce570703290345y156ad31fsac7a41d41ecbe9f4@mail.gmail.com>
Message-ID: <460B9DA0.6060600@sciviews.org>



Sergio Della Franca wrote:
> Ok,
> 
> How can i increase the memory of your computer available to R?

Well, if you would like to increase memory of MY computer... you are 
welcome to do so... but I doubt it would be of any use for you ;-)

You don't tell us how much RAM you have currently, which platform you 
use, etc... The general approach is to use a computer with more RAM, up 
to the limit permitted by a 32-bit system for R, and then, to switch to 
a 64-bit version under Linux, if you need even more RAM.

The other proposed solution is not stupid. With 70.000 cases, you have a 
fairly large dataset. You don't tell use how many groups you expect from 
your clustering, but it is often better to use a couple of tens, or 
hundreds of representative cases for each group, no more. In supervised 
classification, it is easier to build such a training set with 
relatively balanced number of items in each group, because targeted 
classification is known a priori from the manual classification provided.

With unsupervised classification, you could either try a pure random 
subsampling, or select your subsample based on similarity according to a 
given distance measurement. I did something like that using a 
Malahanobis distance, MDS, and then, stratified subsampling inside a 
regular grid placed on top of the MDS plot.

Otherwise, I am not a specialist of unsupervised classification, and 
other people here could have better suggestion.

Best,

Philippe Grosjean

> 
> 2007/3/29, Philippe Grosjean <phgrosjean at sciviews.org>:
>> 1) Reduce the size of your sample (random or stratified subsampling),
>>
>> 2) Increase the memory of your computer available to R.
>>
>> Best,
>>
>> Philippe Grosjean
>>
>> ..............................................<?}))><........
>> ) ) ) ) )
>> ( ( ( ( (    Prof. Philippe Grosjean
>> ) ) ) ) )
>> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>> ) ) ) ) )   Mons-Hainaut University, Belgium
>> ( ( ( ( (
>> ..............................................................
>>
>> Sergio Della Franca wrote:
>>> Dear R-Helpers,
>>>
>>>
>>> I'd like to develop a fanny clustering on my data set(70.000 rows), but
>> when
>>> i run the procedure i obtain this error:
>>>
>>> error in vector("double", lenght): too big dimension for
>>> the selected vector.
>>>
>>>
>>> How can i solve this problem?
>>>
>>>
>>> Thank you in advance.
>>>
>>>
>>> Sergio Della Franca.
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From eva.ubl at univie.ac.at  Thu Mar 29 13:10:21 2007
From: eva.ubl at univie.ac.at (Eva Ubl)
Date: Thu, 29 Mar 2007 13:10:21 +0200 (CEST)
Subject: [R] composed matrices
Message-ID: <1821.131.130.42.61.1175166621.squirrel@webmail.univie.ac.at>

I have just started programming in R and so my question might be basic but
I cant find it in the manual
I have four matrices A,B,C and D and want them to compose in a big matrix X
with A in the upper left corner, B in the right upper corner, c below A
and D below B.
Thanks for your help
eva


From dimitris.rizopoulos at med.kuleuven.be  Thu Mar 29 13:16:37 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 29 Mar 2007 13:16:37 +0200
Subject: [R] composed matrices
References: <1821.131.130.42.61.1175166621.squirrel@webmail.univie.ac.at>
Message-ID: <004001c771f3$b7ab0aa0$0540210a@www.domain>

try this:

A1 <- matrix(1:20, 5, 4)
B1 <- matrix(1:15, 5, 3)
A2 <- matrix(1:8, 2, 4)
B2 <- matrix(1:6, 2, 3)
#####################
rbind(cbind(A1, B1), cbind(A2, B2))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Eva Ubl" <eva.ubl at univie.ac.at>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, March 29, 2007 1:10 PM
Subject: [R] composed matrices


>I have just started programming in R and so my question might be 
>basic but
> I cant find it in the manual
> I have four matrices A,B,C and D and want them to compose in a big 
> matrix X
> with A in the upper left corner, B in the right upper corner, c 
> below A
> and D below B.
> Thanks for your help
> eva
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ccleland at optonline.net  Thu Mar 29 13:17:01 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 29 Mar 2007 07:17:01 -0400
Subject: [R] composed matrices
In-Reply-To: <1821.131.130.42.61.1175166621.squirrel@webmail.univie.ac.at>
References: <1821.131.130.42.61.1175166621.squirrel@webmail.univie.ac.at>
Message-ID: <460BA02D.3000301@optonline.net>

Eva Ubl wrote:
> I have just started programming in R and so my question might be basic but
> I cant find it in the manual
> I have four matrices A,B,C and D and want them to compose in a big matrix X
> with A in the upper left corner, B in the right upper corner, c below A
> and D below B.

A <- matrix(1:4, ncol=2)
B <- matrix(5:8, ncol=2)
C <- matrix(9:12, ncol=2)
D <- matrix(13:16, ncol=2)

rbind(cbind(A,B), cbind(C,D))
     [,1] [,2] [,3] [,4]
[1,]    1    3    5    7
[2,]    2    4    6    8
[3,]    9   11   13   15
[4,]   10   12   14   16

?rbind
?cbind

> Thanks for your help
> eva
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From Corinna.Schmitt at igb.fraunhofer.de  Thu Mar 29 12:45:28 2007
From: Corinna.Schmitt at igb.fraunhofer.de (Schmitt, Corinna)
Date: Thu, 29 Mar 2007 12:45:28 +0200
Subject: [R] equivalent datatypes
Message-ID: <8B7B0FD99E8AF541A21609104D19615882EB56@izs-xchg01.izs.fraunhofer.de>

Hallo,

can anyone help me with datatypes? Which datatypes from R are equivalent
to the following ones from C?

C datatypes:

- double array
- struct array
- cell array
- char array
- logical array

Thanks, Corinna


From S.Ellison at lgc.co.uk  Thu Mar 29 14:27:22 2007
From: S.Ellison at lgc.co.uk (S Ellison)
Date: Thu, 29 Mar 2007 13:27:22 +0100
Subject: [R] Tail area of sum of  Chi-square variables
Message-ID: <s60bbed6.091@tedmail2.lgc.co.uk>

> I was wondering if there are any R functions that give the tail area
> of a sum of chisquare distributions of the type:
>         a_1 X_1 + a_2 X_2
> where a_1 and a_2 are constants and X_1 and X_2 are independent 
> chi-square variables with different degrees of freedom.

You might also check out Welch and Satterthwaite's (separate) papers on effective degrees of freedom for compound estimates of variance, which led to a thing called the welch-satterthwaite equation by one (more or less notorious, but widely used) document called the ISO Guide to Expression of Uncertainty in Measurement (ISO, 1995). The original papers are
B. L. Welch, J. Royal Stat. Soc. Suppl.(1936)  3 29-48
B. L. Welch, Biometrika, (1938) 29 350-362
B. L. Welch, Biometrika, (1947) 34 28-35

F. E. Satterthwaite, Psychometrika (1941) 6 309-316
F. E. Satterthwaite, Biometrics Bulletin, (1946) 2 part 6 110-114

The W-S equation - which I believe is a special case of Welch's somewhat more general treatment - says that if you have multiple independent estimated variances v[i] (could be more or less equivalent to your a_i X_i?) with degrees of freedom nu[i], the distribution of their sum is approximately a scaled chi-squared distribution with effective degrees of freedom nu.effective given by

nu.effective =  sum(v[i])^2 / sum(    (v[i]^2)/nu[i]     )

If I recall correctly, with an observed variance s^2 (corresponding to the sum(v[i] above if those are observed varianes), nu*(s^2 /sigma^2) is distributed as chi-squared with degrees of freedom nu, so the scaling factor for quantiles would come out of there (depending whether you're after the tail areas for s^2 given sigma^2 or for a confidence interval for sigma^2 given s^2)

However, I will be most interested to see what a more exact calculation provides!

Steve Ellison


*******************************************************************
This email and any attachments are confidential. Any use, co...{{dropped}}


From murdoch at stats.uwo.ca  Thu Mar 29 14:44:25 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 29 Mar 2007 08:44:25 -0400
Subject: [R] equivalent datatypes
In-Reply-To: <8B7B0FD99E8AF541A21609104D19615882EB56@izs-xchg01.izs.fraunhofer.de>
References: <8B7B0FD99E8AF541A21609104D19615882EB56@izs-xchg01.izs.fraunhofer.de>
Message-ID: <460BB4A9.5040000@stats.uwo.ca>

On 3/29/2007 6:45 AM, Schmitt, Corinna wrote:
> Hallo,
> 
> can anyone help me with datatypes? Which datatypes from R are equivalent
> to the following ones from C?

I assume you mean functionally equivalent; if you mean equivalent in 
storage so you can pass them to C functions, see the Writing R 
Extensions manual.
> 
> C datatypes:
> 
> - double array

A numeric vector.

> - struct array

R has no exact equivalent.  A struct is similar to a list, but there is 
no enforcement of types in a list.  You could also implement a struct as 
an S4 object to get some type safety.  An array of structs would usually 
be done in R as a list of arrays, but could be done as a list of lists.

> - cell array

I don't know what a cell is in C.

> - char array

A single element character vector.

> - logical array

A logical vector.

Duncan Murdoch


From f.harrell at vanderbilt.edu  Thu Mar 29 14:54:04 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 29 Mar 2007 07:54:04 -0500
Subject: [R] what is the difference between survival analysis and (...)
In-Reply-To: <Pine.LNX.4.43.0703281728200.3629@hymn02.u.washington.edu>
References: <Pine.LNX.4.43.0703281728200.3629@hymn02.u.washington.edu>
Message-ID: <460BB6EC.5080603@vanderbilt.edu>

Thomas Lumley wrote:
> On Wed, 28 Mar 2007, Frank E Harrell Jr wrote:
> 
>> Eric Elguero wrote:
>>> Hi everybody,
>>>
>>> recently I had to teach a course on Cox model, of which I am
>>> not a specialist, to an audience of medical epidemiologists.
>>> Not a good idea you might say.. anyway, someone in the
>>> audience was very hostile. At some point, he sayed that
>>> Cox model was useless, since all you have to do is count
>>> who dies and who survives, divide by the sample sizes
>>> and compute a relative risk, and if there was significant
>>> censoring, use cumulated follow-up instead of sample
>>> sizes and that's it!
>>> I began arguing that in Cox model you could introduce
>>> several variables, interactions, etc, then I remembered
>>> of logistic models ;-)
>>> The only (and poor) argument I could think of was that
>>> if mr Cox took pains to devise his model, there should
>>> be some reason...
>>
>> That is a very ignorant person, concerning statistical
>> efficiency/power/precision and how to handle incomplete follow-up
>> (variable follow-up duration).  There are papers in the literature (I
>> wish I had them at my fingertips) that go into the efficiency loss of
>> just counting events.  If the events are very rare, knowing the time
>> doesn't help as much, but the Cox model still can handle censoring
>> correctly and that person's approach doesn't.
>>
> 
> Certainly just counting the events is inefficient -- the simplest 
> example would be studies of some advanced cancers where nearly everyone 
> dies during followup, so that there is little or no censoring but simple 
> counts are completely uninformative.
> 
> It's relatively hard to come up with an example where using the 
> total-time-on-test (rather than sample size) as a denominator is much 
> worse than the Cox mode, though. You need the baseline hazard to vary a 
> lot over time and the censoring patterns to be quite different in the 
> groups, but proportional hazards to still hold.
> 
> I think the advantages of the Cox model over a reasonably sensible 
> person-time analysis are real, but not dramatic -- it would be hard to 
> find a data set that would convince the sort of person who would make 
> that sort of claim.
> 
> I would argue that computational convenience on the one hand, and the 
> ability to exercise lots of nice mathematical tools on the other hand 
> have also contributed to the continuing popularity of the Cox model.
> 
> 
>     -thomas
> 
> Thomas Lumley            Assoc. Professor, Biostatistics
> tlumley at u.washington.edu    University of Washington, Seattle
> 
> 
> 

Nicely put Thomas.  I have seen examples from surgical research where 
the hazard function is bathtub shaped and the epidemiologist's use of 
the exponential distribution is very problematic.  I have also seen 
examples in acute illness and medical treatment where time until death 
is important even with only 30-day follow-up.

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From murali.menon at uk.abnamro.com  Thu Mar 29 14:56:15 2007
From: murali.menon at uk.abnamro.com (murali.menon at uk.abnamro.com)
Date: Thu, 29 Mar 2007 13:56:15 +0100
Subject: [R] creating conditional list of elements
In-Reply-To: <008801c771f7$1f56ee00$0502a8c0@MangoSolutions.local>
Message-ID: <OF123D7860.B25EF8F0-ONC12572AD.0046FBC5-802572AD.00472080@abnamro.com>

John,

Thanks, that works nicely. Didn't know about 'get'. 

Onwards and upwards! :-)
Cheers,
Murali




"John James" <jjames at mango-solutions.com> 
29/03/2007 12:40

To
<murali.menon at uk.abnamro.com>
cc

Subject
RE: [R] creating conditional list of elements






Murali

If sm is your orginal matrix
# such that 
> sm[sm$Enabled,]$Signal
[1] "bar_signal" "cif_signal" 

# and
bar_signal <- matrix(rnorm(100), nrow=5, ncol=20)
cif_signal <- matrix(rnorm(60), nrow=12, ncol=5)

# then...
sapply(sm[sm$Enabled,]$Signal, function(x){NROW(get(x))})
bar_signal cif_signal 
         5         12

Hope this helps

John James
Mango Solutions

-----Original Message-----
From: murali.menon at uk.abnamro.com [mailto:murali.menon at uk.abnamro.com] 
Sent: 29 March 2007 11:40
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] creating conditional list of elements

Sorry to plague the list, but I think I got the answer. The following 
would do:

> signalList <- list(tradingRules$Signal[tradingRules$Enabled]) [[1]]
> length(signalList)
[1] 2

Now my problem is shifted: I have the Signal column in the original data 
frame referring to actual 
matrices previously created in R. That is, bar_signal and cif_signal are 
extant matrices. What I 
need is the minimum number of rows in these matrices, so what I plan to do 

is:

> n <- min(sapply(signalList, NROW))

but this doesn't work (it returns 1, but I have 2800 rows in each of 
bar_signal and cif_signal, so I should
get 2800)

Is there a smart way to do this? And is there a way to tell R that the 
entries in the Signal column 
of tradingSignal are the names of objects?

Thanks,
Murali



Murali Menon/GB/ABNAMRO/NL 
29/03/2007 11:13

To
r-help at stat.math.ethz.ch
cc

Subject
creating conditional list of elements





Folks,

I have a matrix as follows (first column is the rownames, first row is the 

columnnames)

Rule    Enabled Signal
Foo     False           foo_signal
Bar     True            bar_signal
Gum     False           gum_signal
Cif     True            cif_signal

I would like to create a list of only those signals whose 'enabled' flag 
is True. So in the above 
case I should end up with

signalList = bar_signal, cif_signal

Likewise, if the enabled flags were all set to False, then signalList 
should be an empty list.

What's a good way to achieve this, please?

Thanks,

Murali



---------------------------------------------------------------------------
This message (including any attachments) is confidential and...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.





---------------------------------------------------------------------------
This message (including any attachments) is confidential and...{{dropped}}


From sergio.della.franca at gmail.com  Thu Mar 29 15:02:02 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Thu, 29 Mar 2007 15:02:02 +0200
Subject: [R] Kmeans centers
Message-ID: <b490ce570703290602v75412a79gcb0fbcda4f4912ae@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070329/e95a323e/attachment.pl 

From swiss.notification at mail.com  Thu Mar 29 15:12:57 2007
From: swiss.notification at mail.com (NOTIFICATION AWARD)
Date: Thu, 29 Mar 2007 22:12:57 +0900
Subject: [R] CONGRATULATIONS YOU HAVE WON &#12409;
	50,000 POUNDS THESE NEW YEAR
Message-ID: <200703291312.l2TDCvbZ003493@www.kirei-navi.jp>

CONGRATULATIONS YOU HAVE WON &#12409;50,000 POUNDS THESE NEW YEAR

SWISS LOTTO UK.
P O Box 1010
Liverpool, L70 1NL
UNITED KINGDOM
https://www.swisslotto.ch
 
 
CONGRATULATIONS!!!.....YOU HAVE WON &#12409;50,000 Pounds

You have been awarded &#12409;50,000 Pounds(Seven Hundred and Fifty Thousand Pounds) in the SWISS-LOTTO Satellite software email lottery in which e-mail addresses are picked Randomly by software powered by the Internet through the worldwide website.

Your email address was amongst those chosen this year 2006/2007 for the SWISS LOTTO Satellite lottery. And this promo is proudly sponsored by the SWISS-LOTTO UK organization.

You can log on to our website for more information concerning our entire lottery promo https://www.swisslotto.ch
Your email address,attached to Ref number 5, 7, 14, 17, 18, 43 with Serial number 1979-12 drew the lucky Numbers 10, and consequently won the lottery in the "A" Category.

You have therefore been approved for a lump sum pay out of &#12409;50,000 Pounds(Seven Hundred and Fifty Thousand Pounds) Please note that your lucky winning number falls within our European Booklet representative office in Europe as indicated in your play coupon.

In View of this, your &#12409;50,000 Pounds will be released to you by our firm in Europe. 

Our European agent will immediately commence the process to facilitate the release of your funds as soon as you contact them.

All participants Were selected randomly from World Wide Web site through computer draw System and extracted from over 300,000 companies and individual email addresses.

This promotion takes place annually. For security reasons, you are advised to keep your winning information confidential till your claims is processed and your money remitted to you in whatever manner you deem fit to Claim your prize. This is part of our precautionary measure to avoid double claiming and unwarranted abuse of this program by some unscrupulous elements.

To file for your claim, please contact the agent assigned to you.

Name :  Mr Paul Gate B.
Swiss Claims Agent 
Email:  swisslotto1979 at yahoo.co.uk

You are to provide the following informations to our european agent:-
 
1. Full Name:-
2.Contact Address:-
3.Country:-
4.Age:-
5.Telephone and Fax:-
6.Occupation:-
 
This informations fercilitate the due process of the release of winnings avoid unnecessary delays and complications in the processing of your winnings. please alway remember to quote your Reference / serial number in any correspondences with us or our Designated agent. 
 
Congratulations once more from all members and staff of This program and Thank you for being part of our promotional lottery program. 
 
Sincerely, 
Kenneth Lackman
Co-Ordinator Euro Millions Organisation.


From swiss.notification at mail.com  Thu Mar 29 15:12:56 2007
From: swiss.notification at mail.com (NOTIFICATION AWARD)
Date: Thu, 29 Mar 2007 22:12:56 +0900
Subject: [R] CONGRATULATIONS YOU HAVE WON &#12409;
	50,000 POUNDS THESE NEW YEAR
Message-ID: <200703291312.l2TDCuUn003463@www.kirei-navi.jp>

CONGRATULATIONS YOU HAVE WON &#12409;50,000 POUNDS THESE NEW YEAR

SWISS LOTTO UK.
P O Box 1010
Liverpool, L70 1NL
UNITED KINGDOM
https://www.swisslotto.ch
 
 
CONGRATULATIONS!!!.....YOU HAVE WON &#12409;50,000 Pounds

You have been awarded &#12409;50,000 Pounds(Seven Hundred and Fifty Thousand Pounds) in the SWISS-LOTTO Satellite software email lottery in which e-mail addresses are picked Randomly by software powered by the Internet through the worldwide website.

Your email address was amongst those chosen this year 2006/2007 for the SWISS LOTTO Satellite lottery. And this promo is proudly sponsored by the SWISS-LOTTO UK organization.

You can log on to our website for more information concerning our entire lottery promo https://www.swisslotto.ch
Your email address,attached to Ref number 5, 7, 14, 17, 18, 43 with Serial number 1979-12 drew the lucky Numbers 10, and consequently won the lottery in the "A" Category.

You have therefore been approved for a lump sum pay out of &#12409;50,000 Pounds(Seven Hundred and Fifty Thousand Pounds) Please note that your lucky winning number falls within our European Booklet representative office in Europe as indicated in your play coupon.

In View of this, your &#12409;50,000 Pounds will be released to you by our firm in Europe. 

Our European agent will immediately commence the process to facilitate the release of your funds as soon as you contact them.

All participants Were selected randomly from World Wide Web site through computer draw System and extracted from over 300,000 companies and individual email addresses.

This promotion takes place annually. For security reasons, you are advised to keep your winning information confidential till your claims is processed and your money remitted to you in whatever manner you deem fit to Claim your prize. This is part of our precautionary measure to avoid double claiming and unwarranted abuse of this program by some unscrupulous elements.

To file for your claim, please contact the agent assigned to you.

Name :  Mr Paul Gate B.
Swiss Claims Agent 
Email:  swisslotto1979 at yahoo.co.uk

You are to provide the following informations to our european agent:-
 
1. Full Name:-
2.Contact Address:-
3.Country:-
4.Age:-
5.Telephone and Fax:-
6.Occupation:-
 
This informations fercilitate the due process of the release of winnings avoid unnecessary delays and complications in the processing of your winnings. please alway remember to quote your Reference / serial number in any correspondences with us or our Designated agent. 
 
Congratulations once more from all members and staff of This program and Thank you for being part of our promotional lottery program. 
 
Sincerely, 
Kenneth Lackman
Co-Ordinator Euro Millions Organisation.


From marc_schwartz at comcast.net  Thu Mar 29 15:27:35 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 29 Mar 2007 08:27:35 -0500
Subject: [R] Large matrix into a vector
In-Reply-To: <725257.45454.qm@web32406.mail.mud.yahoo.com>
References: <725257.45454.qm@web32406.mail.mud.yahoo.com>
Message-ID: <1175174855.14657.108.camel@localhost.localdomain>

On Thu, 2007-03-29 at 02:12 -0700, A Ezhil wrote:
> Hi All,
> 
> Thank you very much for all your suggestions. It's a
> great learning for me. All the three suggested
> solutions seem working. I don't know what 'side
> effects' that you were talking about. 

As Peter noted, the phrase 'side effect' may be sub-optimal. 

However, I would use the interpretation that in this case, the removal
of dim attributes on a single object as a consequence of the use of c(),
though documented, is not the primary intended purpose of the function.

Stated differently, perhaps in a manner consistent with Prof. Ripley's
reply, the coercion of a matrix to a vector by the use of c(), is
applying a function to an object to gain the benefit of behavior that is
secondary to the intended functionality of combining objects.

> To summarize the
> responses:
> 
> > s <- read.table("sample.txt", sep="\t")
> > s1 <- as.matrix(s)
> > s1
>     V1   V2   V3
> 1 0.59 0.47 0.44
> 2 0.85 0.42 0.57
> 3 0.48 0.57 0.57
> 4 0.61 0.24 0.24
> 5 0.38 0.21 0.36
> 6 0.65 0.42 1.50
> 7 0.49 0.23 0.42
> 8 0.60 0.51 0.53
> 9 0.00 0.00 0.00
> 
> > s2 <- as.vector(s1)
> > s2
>  [1] 0.59 0.85 0.48 0.61 0.38 0.65 0.49 0.60 0.00 0.47
> 0.42 0.57 0.24 0.21 0.42 0.23 0.51 0.00 0.44 0.57 0.57
> 0.24 0.36
> [24] 1.50 0.42 0.53 0.00
> > s3 <- c(s1)
> > s3
>  [1] 0.59 0.85 0.48 0.61 0.38 0.65 0.49 0.60 0.00 0.47
> 0.42 0.57 0.24 0.21 0.42 0.23 0.51 0.00 0.44 0.57 0.57
> 0.24 0.36
> [24] 1.50 0.42 0.53 0.00
> > s3 <- s1
> > dim(s3) <- NULL
> > s3
>  [1] 0.59 0.85 0.48 0.61 0.38 0.65 0.49 0.60 0.00 0.47
> 0.42 0.57 0.24 0.21 0.42 0.23 0.51 0.00 0.44 0.57 0.57
> 0.24 0.36
> [24] 1.50 0.42 0.53 0.00
> 
> Interestingly, if I apply the same three solutions to
> the data.frame 's', the results are different: (1)
> as.vector() keeps the data frame as it is. (2) c()
> changes into lists, and (3) making dim() <- NULL keeps
> as structure. 

That's because the underlying structure of a data frame is a list,
whereas a matrix is a vector.

HTH,

Marc

<snip>


From kw.statr at gmail.com  Thu Mar 29 15:54:11 2007
From: kw.statr at gmail.com (Kevin Wright)
Date: Thu, 29 Mar 2007 08:54:11 -0500
Subject: [R] using alpha transparency for lines in levelplot - SUMMARY
In-Reply-To: <4608A97A.6050005@utas.edu.au>
References: <46077D10.6080601@utas.edu.au>
	<eb555e660703262039yee24ba0h4daaef277bef07cc@mail.gmail.com>
	<4608A97A.6050005@utas.edu.au>
Message-ID: <c968588d0703290654n6bc669daof3cfb8f9ed0158e@mail.gmail.com>

I reported a similar issue with Adobe Reader in a thread starting here:
http://tolstoy.newcastle.edu.au/R/e2/devel/06/10/0706.html

K Wright



On 3/27/07, Michael Sumner <mdsumner at utas.edu.au> wrote:
> Hello, thanks to Deepayan Sarkar for sorting me out on this one.
>
> The problem with transparent lines affecting region colour in lattice
> plot appears
> when using Adobe Reader (v 8 in my case). I've only viewed the file on
> Windows XP.
>
> I've tried using Foxit Reader to view the file and there's no problem.
>
> Cheers, Mike.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tlumley at u.washington.edu  Thu Mar 29 16:31:10 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 29 Mar 2007 07:31:10 -0700 (PDT)
Subject: [R] Tail area of sum of  Chi-square variables
In-Reply-To: <20070329074636.204120@gmx.net>
References: <20070329074636.204120@gmx.net>
Message-ID: <Pine.LNX.4.64.0703290730370.21308@homer22.u.washington.edu>

On Thu, 29 Mar 2007, Klaus Nordhausen wrote:
>
> I was wondering if there are any R functions that give the tail area
> of a sum of chisquare distributions of the type:
>         a_1 X_1 + a_2 X_2
> where a_1 and a_2 are constants and X_1 and X_2 are independent chi-square variables with different degrees of freedom.
>

pchisqsum() in the "survey" package does this.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From tom.soyer at gmail.com  Thu Mar 29 18:26:00 2007
From: tom.soyer at gmail.com (tom soyer)
Date: Thu, 29 Mar 2007 11:26:00 -0500
Subject: [R] two questions about ccf
Message-ID: <65cc7bdf0703290926v458db791h15d46da0a7b85e59@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070329/4b4d3242/attachment.pl 

From gavin.simpson at ucl.ac.uk  Thu Mar 29 18:50:40 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 29 Mar 2007 17:50:40 +0100
Subject: [R] Kmeans centers
In-Reply-To: <b490ce570703290602v75412a79gcb0fbcda4f4912ae@mail.gmail.com>
References: <b490ce570703290602v75412a79gcb0fbcda4f4912ae@mail.gmail.com>
Message-ID: <1175187040.26892.23.camel@gsimpson.geog.ucl.ac.uk>

On Thu, 2007-03-29 at 15:02 +0200, Sergio Della Franca wrote:
> Dear R-Helpers,
> 
> I read in the R documentation, about kmeans:
> 
>   centers
> 
> Either the number of clusters or a set of initial (distinct) cluster
> centres. *If a number*, a random set of (distinct) rows in x is chosen as
> the initial centres.
> My question is: could it be possible that the centers are character and not
> number?

I think you misunderstand - centers is the number of clusters you want
to partition your data into. How else would you specify the number of
clusters other than by a number? So no, it has to be a numeric number.

The alternative use of centers is to provide known starting points for
the algorithm, such as from the results of a hierarchical cluster
analysis, that are the locations of the cluster centroids, for each
cluster, on each of the feature variables.

Also, argument x to kmeans() is specific about requiring a numeric
matrix (or something coercible to one), so characters here are not
allowed either.

But then again, I may not have understood what it is that you are
asking, but that is not surprising given that you have not provided an
example of what you are trying to do, and how you tried to do it but
failed.

> and provide commented, minimal, self-contained, reproducible code.
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From Horace.Tso at pgn.com  Thu Mar 29 18:51:09 2007
From: Horace.Tso at pgn.com (Horace Tso)
Date: Thu, 29 Mar 2007 09:51:09 -0700
Subject: [R] Bonferroni p-value greater than 1
In-Reply-To: <20070329003756.MOIR1624.tomts40-srv.bellnexxia.net@JohnDesktop8300>
References: <460A8B52020000650000478C@pgn.com>
	<20070329003756.MOIR1624.tomts40-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <460B8C0D02000065000047C8@pgn.com>

Thank you John and Peter.

Peter, yes I'm guilty of tacking onto a random mail. I thought you couldn't tell since I got ride of the text from the last mail. Apologize.

H.

>>> "John Fox" <jfox at mcmaster.ca> 3/28/2007 5:37 PM >>>
Dear Horace,

The Bonferonni p-value is obtained from the "unadjusted" p-value by
multiplying the latter by the number of observations, and provides a
conservative (although usually quite accurate) outlier test. When the
adjusted p-value exceeds 1 you can take that as an indication that there are
no unusually large studentized residuals (and indeed that the largest
studentized residual is smaller than one would expect under the standard
linear-model assumptions). 

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Horace Tso
> Sent: Wednesday, March 28, 2007 6:36 PM
> To: 'R R-help'
> Subject: [R] Bonferroni p-value greater than 1
> 
> Hi folks,
> 
> I use the outlier.test in package car to test a lm model and 
> the bonferroni p value returned is shown as NA. When the 
> object is typed it indicates the p value is greater than 1. 
> I'm not sure how to interpret it. 
> 
> Thanks in advance.
> 
> Horace W. Tso
> 
> 
> > outlier.test(mod)$test
> max|rstudent|            df  unadjusted p  Bonferroni p
>    2.04106376   18.00000000    0.05618628            NA 
> 
> > outlier.test(mod)
> 
> max|rstudent| = 2.041064, degrees of freedom = 18,
> unadjusted p = 0.05618628, Bonferroni p > 1
> 
> Observation: 1 
> 
> The lm model looks fine to me,
> 
> > summary(mod)
> 
> Call:
> lm(formula = x ~ ind, na.action = na.fail)
> 
> Residuals:
>     Min      1Q  Median      3Q     Max 
> -1.2082 -0.5200  0.1309  0.5725  0.9593 
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)    
> (Intercept) 59.84586    0.31900   187.6  < 2e-16 ***
> ind         -0.16768    0.02541    -6.6 2.57e-06 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> Residual standard error: 0.705 on 19 degrees of freedom
> Multiple R-Squared: 0.6963,     Adjusted R-squared: 0.6803 
> F-statistic: 43.56 on 1 and 19 DF,  p-value: 2.57
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html 
> and provide commented, minimal, self-contained, reproducible code.
>


From albmont at centroin.com.br  Thu Mar 29 18:57:52 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Thu, 29 Mar 2007 14:57:52 -0200
Subject: [R] tcltk, tclRequire and Tktable help
Message-ID: <20070329165444.M9265@centroin.com.br>

I know almost nothing about the tcltk library, and the
documentation seems very poor. What's the meaning of this
error, and is there any way to fix it? I'm running R 2.4.1
in a Windows XP machine where I have almost no privileges
(but at home I am the Evil Overlord of a Linux machine...)

library(tcltk)
tclRequire("Tktable")
# [1] FALSE
# Warning message:
# Tcl package 'Tktable' not found in: tclRequire("Tktable") 

Alberto Monteiro


From marc_schwartz at comcast.net  Thu Mar 29 19:15:27 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 29 Mar 2007 12:15:27 -0500
Subject: [R] E-Mail/Post Threading (was: Bonferroni p-value greater	than
	1)
In-Reply-To: <460B8C0D02000065000047C8@pgn.com>
References: <460A8B52020000650000478C@pgn.com>
	<20070329003756.MOIR1624.tomts40-srv.bellnexxia.net@JohnDesktop8300>
	<460B8C0D02000065000047C8@pgn.com>
Message-ID: <1175188527.14657.115.camel@localhost.localdomain>

On Thu, 2007-03-29 at 09:51 -0700, Horace Tso wrote:
> Thank you John and Peter.
> 
> Peter, yes I'm guilty of tacking onto a random mail. I thought you
> couldn't tell since I got ride of the text from the last mail.
> Apologize.
> 
> H.

Just a quick heads up here, that deleting the body text of a message or
changing the subject line, does not alter the 'linkage' between posts.

There are standards for how messages are 'threaded' and largely have to
do with the e-mail headers, not the e-mail content.

A couple of quick references that might be helpful:

http://people.dsv.su.se/~jpalme/ietf/message-threading.html

http://www.jwz.org/doc/threading.html


HTH,

Marc Schwartz


From abeaujean at gmail.com  Thu Mar 29 19:39:08 2007
From: abeaujean at gmail.com (A. Beaujean)
Date: Thu, 29 Mar 2007 12:39:08 -0500
Subject: [R] pipe Apple
Message-ID: <c74b04310703291039j4df5185aofc94012de5c644db@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070329/e1f5dd91/attachment.pl 

From albmont at centroin.com.br  Thu Mar 29 19:58:18 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Thu, 29 Mar 2007 15:58:18 -0200
Subject: [R] tcltk crashing R after the (ab)use of tkwait
Message-ID: <20070329175201.M14183@centroin.com.br>

Running this:

library(tcltk)
tt <- tktoplevel()
done <- tclVar(0)
but <- tkbutton(tt, text="OK", command=function() tclvalue(done) <- 1)
tkpack(but)
tkwait.variable(done)

works as fine as long as I click the OK. However, if I close
the window (by clicking in the X), R enters into an infinite loop
and there's no way of returning except by closing the R window.

Why? What am I doing wrong?

Alberto Monteiro


From ted.harding at nessie.mcc.ac.uk  Thu Mar 29 20:38:05 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 29 Mar 2007 19:38:05 +0100 (BST)
Subject: [R] E-Mail/Post Threading (was: Bonferroni p-value greater	t
In-Reply-To: <1175188527.14657.115.camel@localhost.localdomain>
Message-ID: <XFMail.070329193805.ted.harding@nessie.mcc.ac.uk>

On 29-Mar-07 17:15:27, Marc Schwartz wrote:
> [...]
> Just a quick heads up here, that deleting the body text of
> a message or changing the subject line, does not alter the
> 'linkage' between posts.
> 
> There are standards for how messages are 'threaded' and largely
> have to do with the e-mail headers, not the e-mail content.
> 
> A couple of quick references that might be helpful:
> 
> http://people.dsv.su.se/~jpalme/ietf/message-threading.html
> 
> http://www.jwz.org/doc/threading.html

This above, of course, is a good reason for not replying to an
existing message when you want to start a completely new thread.

However, I'm wondering what is the best way to start a new thread
which legitimately branches out from an existing one.

For example, someone posts a message which discusses at length a
method of isotonic binary regression, and in the middle of this
describes a curious approach to obtaining confidence bands for
the regression. I'm intrigued by the confidence band issue, get
some ideas about it, and want to start a new thread to develop
just this aspect.

However, to do so I want in the first place to include several
quotations from the original message. This, of course, is most
easily done by replying to that message -- so that it gets included
in the reply -- and editing this included message, and changing
the subject.

But that stays in the old thread, which I don't want. Now of course
one can copy over the old text into a brand new blank message,
and edit it up into a simulacrum of a "reply" -- all the usual
"On NN March 2007, XXX wrote:" ... as well as the "> " inclusion
markers, etc.. But that could be tedious. Nevertheless, perhaps
it is the right thing to do -- unless there's a work-round using
the "reply" mechanism?

Best wishes to all,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 29-Mar-07                                       Time: 19:37:54
------------------------------ XFMail ------------------------------


From P.Dalgaard at biostat.ku.dk  Thu Mar 29 15:37:46 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 29 Mar 2007 15:37:46 +0200
Subject: [R] Tail area of sum of  Chi-square variables
In-Reply-To: <s60bbed6.091@tedmail2.lgc.co.uk>
References: <s60bbed6.091@tedmail2.lgc.co.uk>
Message-ID: <460BC12A.9020205@biostat.ku.dk>

S Ellison wrote:
>> I was wondering if there are any R functions that give the tail area
>> of a sum of chisquare distributions of the type:
>>         a_1 X_1 + a_2 X_2
>> where a_1 and a_2 are constants and X_1 and X_2 are independent 
>> chi-square variables with different degrees of freedom.
>>     
>
> You might also check out Welch and Satterthwaite's (separate) papers on effective degrees of freedom for compound estimates of variance, which led to a thing called the welch-satterthwaite equation by one (more or less notorious, but widely used) document called the ISO Guide to Expression of Uncertainty in Measurement (ISO, 1995). The original papers are
> B. L. Welch, J. Royal Stat. Soc. Suppl.(1936)  3 29-48
> B. L. Welch, Biometrika, (1938) 29 350-362
> B. L. Welch, Biometrika, (1947) 34 28-35
>
> F. E. Satterthwaite, Psychometrika (1941) 6 309-316
> F. E. Satterthwaite, Biometrics Bulletin, (1946) 2 part 6 110-114
>
> The W-S equation - which I believe is a special case of Welch's somewhat more general treatment - says that if you have multiple independent estimated variances v[i] (could be more or less equivalent to your a_i X_i?) with degrees of freedom nu[i], the distribution of their sum is approximately a scaled chi-squared distribution with effective degrees of freedom nu.effective given by
>
> nu.effective =  sum(v[i])^2 / sum(    (v[i]^2)/nu[i]     )
>
> If I recall correctly, with an observed variance s^2 (corresponding to the sum(v[i] above if those are observed varianes), nu*(s^2 /sigma^2) is distributed as chi-squared with degrees of freedom nu, so the scaling factor for quantiles would come out of there (depending whether you're after the tail areas for s^2 given sigma^2 or for a confidence interval for sigma^2 given s^2)
>
> However, I will be most interested to see what a more exact calculation provides!
>   

I believe this is also in Box, 1954, in the guise of sums of squares
under heteroscedsticity and correlation.

@Article{box54ptI,
  author =       {G. E. P. Box},
  title =        {Some Theorems on Quadratic Forms Applied in the
                  Study of Analysis of Variance Problems, I. Effect of
                  Inequality of Variance in the One-Way
                  Classification},
  journal =      {Annals of Mathematical Statistics},
  year =         1954,
  volume =       25,
  number =       2,
  pages =        {290-302}
}

IIRC (I seem to have temporarily misplaced the actual paper...), there
is a trick by which the characteristic function of a linear combination
of chi-squared variables can be expanded in a  (weighted) _sum_ of
chisquare characteristic functions, which gives you an exact expression
for the density and CDF.   

> Steve Ellison
>
>
> *******************************************************************
> This email and any attachments are confidential. Any use, co...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From tlumley at u.washington.edu  Thu Mar 29 16:45:15 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 29 Mar 2007 07:45:15 -0700 (PDT)
Subject: [R] Tail area of sum of  Chi-square variables
In-Reply-To: <s60bbed6.091@tedmail2.lgc.co.uk>
References: <s60bbed6.091@tedmail2.lgc.co.uk>
Message-ID: <Pine.LNX.4.64.0703290739240.21308@homer22.u.washington.edu>


The Satterthwaite approximation is surprisingly good, especially in the 
most interesting range in the right tail (say 0.9 to 0.999). There is also 
another, better, approximation with a power of a chi-squared distribution 
that has been used in the survey literature.

However, since it is easy to write down the characteristic function and 
perfectly feasible to invert it by numerical integration, we might as well 
use the right answer.

 	-thomas

On Thu, 29 Mar 2007, S Ellison wrote:
>> I was wondering if there are any R functions that give the tail area
>> of a sum of chisquare distributions of the type:
>>         a_1 X_1 + a_2 X_2
>> where a_1 and a_2 are constants and X_1 and X_2 are independent
>> chi-square variables with different degrees of freedom.
>
> You might also check out Welch and Satterthwaite's (separate) papers on effective degrees of freedom for compound estimates of variance, which led to a thing called the welch-satterthwaite equation by one (more or less notorious, but widely used) document called the ISO Guide to Expression of Uncertainty in Measurement (ISO, 1995). The original papers are
> B. L. Welch, J. Royal Stat. Soc. Suppl.(1936)  3 29-48
> B. L. Welch, Biometrika, (1938) 29 350-362
> B. L. Welch, Biometrika, (1947) 34 28-35
>
> F. E. Satterthwaite, Psychometrika (1941) 6 309-316
> F. E. Satterthwaite, Biometrics Bulletin, (1946) 2 part 6 110-114
>
> The W-S equation - which I believe is a special case of Welch's somewhat more general treatment - says that if you have multiple independent estimated variances v[i] (could be more or less equivalent to your a_i X_i?) with degrees of freedom nu[i], the distribution of their sum is approximately a scaled chi-squared distribution with effective degrees of freedom nu.effective given by
>
> nu.effective =  sum(v[i])^2 / sum(    (v[i]^2)/nu[i]     )
>
> If I recall correctly, with an observed variance s^2 (corresponding to the sum(v[i] above if those are observed varianes), nu*(s^2 /sigma^2) is distributed as chi-squared with degrees of freedom nu, so the scaling factor for quantiles would come out of there (depending whether you're after the tail areas for s^2 given sigma^2 or for a confidence interval for sigma^2 given s^2)
>
> However, I will be most interested to see what a more exact calculation provides!
>
> Steve Ellison
>
>
> *******************************************************************
> This email and any attachments are confidential. Any use, co...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From dieter.menne at menne-biomed.de  Thu Mar 29 20:50:37 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 29 Mar 2007 18:50:37 +0000 (UTC)
Subject: [R] tcltk crashing R after the (ab)use of tkwait
References: <20070329175201.M14183@centroin.com.br>
Message-ID: <loom.20070329T204841-549@post.gmane.org>

Alberto Monteiro <albmont <at> centroin.com.br> writes:

> library(tcltk)
> tt <- tktoplevel()
> done <- tclVar(0)
> but <- tkbutton(tt, text="OK", command=function() tclvalue(done) <- 1)
> tkpack(but)
> tkwait.variable(done)
> 
> works as fine as long as I click the OK. However, if I close
> the window (by clicking in the X), R enters into an infinite loop
> and there's no way of returning except by closing the R window.
> 
Works for me with R-2.4.1 on Windows 2000. So better tell us about the details
of your operating system.

Dieter


From rhelpme at gmail.com  Thu Mar 29 20:57:31 2007
From: rhelpme at gmail.com (c n)
Date: Thu, 29 Mar 2007 13:57:31 -0500
Subject: [R] Xemacs, ESS, R config issue
Message-ID: <63706fbc0703291157y14969bf4i5507753ab6786e3c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070329/7d176199/attachment.pl 

From dieter.menne at menne-biomed.de  Thu Mar 29 20:54:52 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 29 Mar 2007 18:54:52 +0000 (UTC)
Subject: [R] equivalent datatypes
References: <8B7B0FD99E8AF541A21609104D19615882EB56@izs-xchg01.izs.fraunhofer.de>
	<460BB4A9.5040000@stats.uwo.ca>
Message-ID: <loom.20070329T205359-303@post.gmane.org>

Duncan Murdoch <murdoch <at> stats.uwo.ca> writes:

> 
> On 3/29/2007 6:45 AM, Schmitt, Corinna wrote:
> > - cell array
> 
> I don't know what a cell is in C.
> 

This is MathLabish.

Dieter


From dieter.menne at menne-biomed.de  Thu Mar 29 21:02:32 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Thu, 29 Mar 2007 19:02:32 +0000 (UTC)
Subject: [R] Regarding Vista
References: <460A4D13.60009@statistik.uni-dortmund.de>
	<156CDC8CCFD1894295D2907F16337A4801420ACA@bru-s-006.europe.shell.com>
Message-ID: <loom.20070329T205737-370@post.gmane.org>

 <christian.ritter <at> shell.com> writes:

> The Vista issue is not innocent as it threatens the life of R within large
corporations. So any posts on how R
> runs under Vista and what has to be done to make it work and what cannot be
done etc will be very useful. 

Main problem seems to be the installation of chm-files when upgrading. I had to
switch off all security mechanisms to get it done. Which is probably not what
you would like to have in a corporate environment.

As a temporary workaround, it would probably be best to optionally disable chm
installation. Or do a poll if is is required, after all it's considered a legacy
format nowadays.

Dieter


From albmont at centroin.com.br  Thu Mar 29 21:15:36 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Thu, 29 Mar 2007 17:15:36 -0200
Subject: [R] tcltk crashing R after the (ab)use of tkwait
In-Reply-To: <loom.20070329T204841-549@post.gmane.org>
References: <20070329175201.M14183@centroin.com.br>
	<loom.20070329T204841-549@post.gmane.org>
Message-ID: <20070329191414.M39492@centroin.com.br>

Dieter Menne wrote:
> 
>> library(tcltk)
>> tt <- tktoplevel()
>> done <- tclVar(0)
>> but <- tkbutton(tt, text="OK", command=function() tclvalue(done) <- 1)
>> tkpack(but)
>> tkwait.variable(done)
>> 
>> works as fine as long as I click the OK. However, if I close
>> the window (by clicking in the X), R enters into an infinite loop
>> and there's no way of returning except by closing the R window.
> 
> Works for me with R-2.4.1 on Windows 2000. So better tell us about 
> the details of your operating system.
>
R 2.4.1 on Windows XP. Should they work differently?

Alberto Monteiro


From marc_schwartz at comcast.net  Thu Mar 29 21:21:12 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 29 Mar 2007 14:21:12 -0500
Subject: [R] E-Mail/Post Threading (was: Bonferroni p-value greater	t
In-Reply-To: <XFMail.070329193805.ted.harding@nessie.mcc.ac.uk>
References: <XFMail.070329193805.ted.harding@nessie.mcc.ac.uk>
Message-ID: <1175196072.14657.134.camel@localhost.localdomain>

On Thu, 2007-03-29 at 19:38 +0100, ted.harding at nessie.mcc.ac.uk wrote:
> On 29-Mar-07 17:15:27, Marc Schwartz wrote:
> > [...]
> > Just a quick heads up here, that deleting the body text of
> > a message or changing the subject line, does not alter the
> > 'linkage' between posts.
> > 
> > There are standards for how messages are 'threaded' and largely
> > have to do with the e-mail headers, not the e-mail content.
> > 
> > A couple of quick references that might be helpful:
> > 
> > http://people.dsv.su.se/~jpalme/ietf/message-threading.html
> > 
> > http://www.jwz.org/doc/threading.html
> 
> This above, of course, is a good reason for not replying to an
> existing message when you want to start a completely new thread.
> 
> However, I'm wondering what is the best way to start a new thread
> which legitimately branches out from an existing one.
> 
> For example, someone posts a message which discusses at length a
> method of isotonic binary regression, and in the middle of this
> describes a curious approach to obtaining confidence bands for
> the regression. I'm intrigued by the confidence band issue, get
> some ideas about it, and want to start a new thread to develop
> just this aspect.
> 
> However, to do so I want in the first place to include several
> quotations from the original message. This, of course, is most
> easily done by replying to that message -- so that it gets included
> in the reply -- and editing this included message, and changing
> the subject.
> 
> But that stays in the old thread, which I don't want. Now of course
> one can copy over the old text into a brand new blank message,
> and edit it up into a simulacrum of a "reply" -- all the usual
> "On NN March 2007, XXX wrote:" ... as well as the "> " inclusion
> markers, etc.. But that could be tedious. Nevertheless, perhaps
> it is the right thing to do -- unless there's a work-round using
> the "reply" mechanism?
> 
> Best wishes to all,
> Ted.

Hi Ted,

The general approach, if "relatedly digressing" (also described in
various 'netiquette' guides) is to do what I did here, which is reply to
the post in question, but change the subject header by using "New
Subject (was: Old Subject)".

This enables you to easily engage in the sort of editing that you
describe, and still links your reply back to the original thread, under
the presumption that your reply is in some way related to the subject
matter of the original thread.

Since most e-mail systems (list managers, MUA's, etc.) thread based upon
the headers and not the subject, as described in the above references,
unless you generate a completely new e-mail, your reply will be linked
to the e-mail and thread to which you are replying.

It's pretty much a dichotomous situation.  Use 'reply' and you get
linked to the old thread. Use a 'new' e-mail and you start a new thread.

If you are truly moving in a new direction, I would be tempted to start
a new thread and perhaps to make it easier for readers, include a
reference/link to the post in question. That way, you keep your new
e-mail in a separate thread, while 'virtually' linking it back to the
original that raised your interest.

HTH,

Marc


From albmont at centroin.com.br  Thu Mar 29 21:32:17 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Thu, 29 Mar 2007 17:32:17 -0200
Subject: [R] Wikibooks
Message-ID: <20070329193059.M91171@centroin.com.br>

As a big fan of Wikipedia, it's frustrating to see how little there is about 
R in the correlated project, the Wikibooks:

http://en.wikibooks.org/wiki/R_Programming

Alberto Monteiro


From marc_schwartz at comcast.net  Thu Mar 29 21:37:29 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 29 Mar 2007 14:37:29 -0500
Subject: [R] Xemacs, ESS, R config issue
In-Reply-To: <63706fbc0703291157y14969bf4i5507753ab6786e3c@mail.gmail.com>
References: <63706fbc0703291157y14969bf4i5507753ab6786e3c@mail.gmail.com>
Message-ID: <1175197049.14657.143.camel@localhost.localdomain>

On Thu, 2007-03-29 at 13:57 -0500, c n wrote:
> I've searched for 45 minutes, apparently in all the wrong places for a
> solution to a configuration issue I'm having.
> 
> When I use Xemacs with ESS running in R-mode, and I type a "-" character, it
> autocompletes it to "<- ".  How do I disable this annoying "feature"?
> 
> Thanks much.

I think you mean an underscore "_" rather than a hyphen "-"?

If you type "__" (a double underscore) you will get an underscore. 

If you want to disable that feature, put the following:

  (ess-toggle-underscore nil)

in your init.el file after the point where ESS is loaded.


BTW, there is a dedicated ESS list:

https://stat.ethz.ch/mailman/listinfo/ess-help


HTH,

Marc Schwartz


From gavin.simpson at ucl.ac.uk  Thu Mar 29 21:55:44 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 29 Mar 2007 20:55:44 +0100
Subject: [R] Xemacs, ESS, R config issue
In-Reply-To: <63706fbc0703291157y14969bf4i5507753ab6786e3c@mail.gmail.com>
References: <63706fbc0703291157y14969bf4i5507753ab6786e3c@mail.gmail.com>
Message-ID: <1175198144.26892.33.camel@gsimpson.geog.ucl.ac.uk>

On Thu, 2007-03-29 at 13:57 -0500, c n wrote:
> I've searched for 45 minutes, apparently in all the wrong places for a
> solution to a configuration issue I'm having.
> 
> When I use Xemacs with ESS running in R-mode, and I type a "-" character, it
> autocompletes it to "<- ".  How do I disable this annoying "feature"?
> 
> Thanks much.

Don't know about disabling it, but to get a "_", just press it twice.
"_" doesn't auto-complete in text strings (quoted).

"_" isn't allowed for assignment anymore, so the only place I can think
of at the mo is that you want to type a "_" in a variable name?

Also, this is best addressed on the ESS mailing list:
ess-helpATstatDOTmathDOTethzDOTch (just remove the AT and DOTs)

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From patrick at pdrechsler.de  Thu Mar 29 22:19:33 2007
From: patrick at pdrechsler.de (Patrick Drechsler)
Date: Thu, 29 Mar 2007 22:19:33 +0200
Subject: [R] Xemacs, ESS, R config issue
References: <63706fbc0703291157y14969bf4i5507753ab6786e3c@mail.gmail.com>
Message-ID: <87mz1verai.fsf@pdrechsler.de>

"c n" <rhelpme at gmail.com> writes:

> When I use Xemacs with ESS running in R-mode, and I type a "-" character, it
> autocompletes it to "<- ".  How do I disable this annoying "feature"?

>From the ESS manual:

,----
|    * ESS[S]: Pressing underscore ("_") once inserts " <- " (as before);
|      pressing underscore twice inserts a literal underscore.  To stop
|      this smart behaviour, add "(ess-toggle-underscore nil)" to your
|      .emacs after ess-site has been loaded;
`----

HTH

Patrick
-- 
Patrick Drechsler         tel: +44 1223 334441
Department of Zoology     fax: +44 1223 336687
University of Cambridge
Downing Street
Cambridge CB2 3EJ, UK


From ted.harding at nessie.mcc.ac.uk  Thu Mar 29 22:39:20 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 29 Mar 2007 21:39:20 +0100 (BST)
Subject: [R] E-Mail/Post Threading (was: Bonferroni p-value greater	t
In-Reply-To: <1175196072.14657.134.camel@localhost.localdomain>
Message-ID: <XFMail.070329213920.ted.harding@nessie.mcc.ac.uk>

On 29-Mar-07 19:21:12, Marc Schwartz wrote:
> On Thu, 2007-03-29 at 19:38 +0100, ted.harding at nessie.mcc.ac.uk wrote:
>> On 29-Mar-07 17:15:27, Marc Schwartz wrote:
>> > [...]
>> > Just a quick heads up here, that deleting the body text of
>> > a message or changing the subject line, does not alter the
>> > 'linkage' between posts.
>> > 
>> > There are standards for how messages are 'threaded' and largely
>> > have to do with the e-mail headers, not the e-mail content.
>> > 
>> > A couple of quick references that might be helpful:
>> > 
>> > http://people.dsv.su.se/~jpalme/ietf/message-threading.html
>> > 
>> > http://www.jwz.org/doc/threading.html
>> 
>> This above, of course, is a good reason for not replying to an
>> existing message when you want to start a completely new thread.
>> 
>> However, I'm wondering what is the best way to start a new thread
>> which legitimately branches out from an existing one.
>> 
>> For example, someone posts a message which discusses at length a
>> method of isotonic binary regression, and in the middle of this
>> describes a curious approach to obtaining confidence bands for
>> the regression. I'm intrigued by the confidence band issue, get
>> some ideas about it, and want to start a new thread to develop
>> just this aspect.
>> 
>> However, to do so I want in the first place to include several
>> quotations from the original message. This, of course, is most
>> easily done by replying to that message -- so that it gets included
>> in the reply -- and editing this included message, and changing
>> the subject.
>> 
>> But that stays in the old thread, which I don't want. Now of course
>> one can copy over the old text into a brand new blank message,
>> and edit it up into a simulacrum of a "reply" -- all the usual
>> "On NN March 2007, XXX wrote:" ... as well as the "> " inclusion
>> markers, etc.. But that could be tedious. Nevertheless, perhaps
>> it is the right thing to do -- unless there's a work-round using
>> the "reply" mechanism?
>> 
>> Best wishes to all,
>> Ted.
> 
> Hi Ted,
> 
> The general approach, if "relatedly digressing" (also described
> in various 'netiquette' guides) is to do what I did here, which
> is reply to the post in question, but change the subject header
> by using "New Subject (was: Old Subject)".
> 
> This enables you to easily engage in the sort of editing that you
> describe, and still links your reply back to the original thread,
> under the presumption that your reply is in some way related to
> the subject matter of the original thread.
> 
> Since most e-mail systems (list managers, MUA's, etc.) thread
> based upon the headers and not the subject, as described in the
> above references, unless you generate a completely new e-mail,
> your reply will be linked to the e-mail and thread to which you
> are replying.
> 
> It's pretty much a dichotomous situation.  Use 'reply' and you get
> linked to the old thread. Use a 'new' e-mail and you start a new
> thread.
> 
> If you are truly moving in a new direction, I would be tempted to
> start a new thread and perhaps to make it easier for readers,
> include a reference/link to the post in question. That way, you
> keep your new e-mail in a separate thread, while 'virtually'
> linking it back to the original that raised your interest.
> 
> HTH,
> 
> Marc

Thanks for the clarification, Marc. Nicely put.

Now that I visit the R-help archives website, I can see that these
"subthreads" get hung off the originals in that display (just as
out present subthread is, as seen there), which is a good way for
it to happen (and ideally suited to the sort of situation which
I described). And the way the subject was changed keeps it nice
and clear as to what happened.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 29-Mar-07                                       Time: 21:39:17
------------------------------ XFMail ------------------------------


From macq at llnl.gov  Thu Mar 29 22:43:28 2007
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 29 Mar 2007 13:43:28 -0700
Subject: [R] pipe Apple
In-Reply-To: <c74b04310703291039j4df5185aofc94012de5c644db@mail.gmail.com>
References: <c74b04310703291039j4df5185aofc94012de5c644db@mail.gmail.com>
Message-ID: <p06230904c231d3d604a0@[128.115.153.6]>

This expression,
    read.table(pipe("pbpaste"))
does work for me on an OS X 10.4.9 system.
And it was working 2 years ago on a 10.3.8 system (an older R, obviously).

I would look to how R was installed, what version of R, that kind of thing.
Use the sessionInfo() command to provide that information.
(the posting guide asks for information about the R version, without 
which it is more difficult to help)

What does the R expression
    capabilities()
return?

Also, if what she's copying and pasting from is Excel, the gdata 
package has a function read.xls() function which can read data 
directly from Excel.

-Don

At 12:39 PM -0500 3/29/07, A. Beaujean wrote:
>Hi,
>
>I have a student trying to run R on an Apple (OS 10.3.9). She tried to
>cut-and-paste the data via the code:
>data<-read.table(pipe("pbpaste"))
>
>But she keeps getting the error message:
>'error in pipe("pbpaste"):  pipe connections are not available on this
>system'
>
>I do not know much about using an Apple. Has anyone run into this before?
>Does anyone have any ideas for how to fix the problem?
>
>Thanks!
>
>Alex
>
>--
>***************
>A. Alexander Beaujean, Ph.D., LSSP
>Licensed Psychologist (Provisional, TX)
>http://myprofile.cos.com/abeaujean
>http://www.baylor.edu/soe/faculty/index.php?id=38476
>
>
>"General impressions are never to be trusted. Unfortunately when they are of
>long standing they become fixed rules of life, and assume a prescriptive
>right not to be questioned. Consequently those who are not accustomed to
>original inquiry entertain a hatred and a horror of statistics. They cannot
>endure the idea of submitting their sacred  impressions to cold-blooded
>verification. But it is the triumph of scientific men to rise superior to
>such superstitions, to devise tests by which the value of beliefs may be
>ascertained, and to feel sufficiently masters of themselves to discard
>contemptuously whatever may be found untrue." --Sir Francis Galton, FRS
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA


From skiadas at hanover.edu  Thu Mar 29 23:02:46 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Thu, 29 Mar 2007 17:02:46 -0400
Subject: [R] E-Mail/Post Threading (was: Bonferroni p-value greater	t
In-Reply-To: <1175196072.14657.134.camel@localhost.localdomain>
References: <XFMail.070329193805.ted.harding@nessie.mcc.ac.uk>
	<1175196072.14657.134.camel@localhost.localdomain>
Message-ID: <39690E71-7C2D-4A79-A362-740E7FCD9849@hanover.edu>

On Mar 29, 2007, at 3:21 PM, Marc Schwartz wrote:

> Since most e-mail systems (list managers, MUA's, etc.) thread based  
> upon
> the headers and not the subject, as described in the above references,
> unless you generate a completely new e-mail, your reply will be linked
> to the e-mail and thread to which you are replying.
>
> It's pretty much a dichotomous situation.  Use 'reply' and you get
> linked to the old thread. Use a 'new' e-mail and you start a new  
> thread.
>
> If you are truly moving in a new direction, I would be tempted to  
> start
> a new thread and perhaps to make it easier for readers, include a
> reference/link to the post in question. That way, you keep your new
> e-mail in a separate thread, while 'virtually' linking it back to the
> original that raised your interest.

Perhaps moving a bit off topic here, but to elaborate a bit more on  
this: Each "thread" is really a "tree", where your message is a child  
of the message you responded to. Since typically each person responds  
to the last message in the thread, this often ends up being linear.  
But if for instance three people respond to the same original  
message, this creates  three children of the root node.

You can see this in action here for instance: http://news.gmane.org/ 
gmane.emacs.ess.general

It then depends on your software, how to show this tree. Most mail  
clients would just flatten it out into a single list, which is what  
we usually refer to as a "thread" I guess. But the richer structure  
is there.

So based on this I would suggest simply responding to the message you  
want to, changing the subject appropriately.

> HTH,
>
> Marc

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From paulaugust2003 at yahoo.com  Thu Mar 29 23:40:42 2007
From: paulaugust2003 at yahoo.com (Paul August)
Date: Thu, 29 Mar 2007 14:40:42 -0700 (PDT)
Subject: [R] Using functions in LAPACK in a C program
Message-ID: <221244.52205.qm@web58404.mail.re3.yahoo.com>

Hi,

I wonder where I can find an example of using a function in LAPACK library in a user's own C code. I wrote a C program which will be compiled and linked to produce a DLL file and then loaded into R. I hope to use a function from LAPACK library, for example, dgesdd, in the program. Following R manual, I call the function by F77_CALL(dgesdd) in the program. The program can be compiled without problems. However, when it is linked to produce a DLL file, I get an error message 

  Test.obj : error LNK2001: unresolved external symbol _dgesdd_
  Test.dll : fatal error LNK1120: 1 unresolved externals

I use VC++6.0 and the command of linking is something like this

  link.exe Rdll.lib /nologo /dll /out:Test.dll /libpath:C:\R\R-2.2.1\src\gnuwin32 Test.obj

Apparently, the linker cannot resolve dgesdd from Rdll.lib. If anyone knows what I missed here or any example that shows how this can be done properly, please let me know. Thanks a lot.

Paul.



 
____________________________________________________________________________________
Never miss an email again!


From bolker at zoo.ufl.edu  Thu Mar 29 23:53:52 2007
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Thu, 29 Mar 2007 21:53:52 +0000 (UTC)
Subject: [R] Wikibooks
References: <20070329193059.M91171@centroin.com.br>
Message-ID: <loom.20070329T234918-210@post.gmane.org>

Alberto Monteiro <albmont <at> centroin.com.br> writes:

> 
> As a big fan of Wikipedia, it's frustrating to see how little there is about 
> R in the correlated project, the Wikibooks:
> 
> http://en.wikibooks.org/wiki/R_Programming
> 
> Alberto Monteiro
> 

  Well, we do have an R wiki -- http://wiki.r-project.org/rwiki/doku.php --
although it is not as active as I'd like.  (We got stuck halfway through
porting Paul Johnson's "R Tips" to it ...)   Please contribute!
  Most of the (considerable) effort people expend in answering
questions about R goes to the mailing lists -- I personally would like it if some
tiny fraction of that energy could be redirected toward the wiki, where
information can be presented in a nicer format and (ideally) polished
over time -- rather than having to dig back through multiple threads on the
mailing lists to get answers.  (After that we have to get people
to look for the answers on the wiki.)

  Just my two cents -- and I've been delinquent in my 
wiki'ing recently too ...

  Ben Bolker


From patrick at pdrechsler.de  Fri Mar 30 00:22:55 2007
From: patrick at pdrechsler.de (Patrick Drechsler)
Date: Fri, 30 Mar 2007 00:22:55 +0200
Subject: [R] [HH] extending ancova function for 2 factors
Message-ID: <878xdfbsg0.fsf@pdrechsler.de>

Hi,

what would be a good way of enhancing the ancova function from the HH
package when using a 2 factor ANCOVA?

The current behaviour for the "ancova" function from package HH is:

----------------------------------------------
|  P1  || P1   || P3   || P4   |   | PS      | <- the lattice strip
----------------------------------------------
|     x||    x ||x     ||x     |   |         |
|  x   ||x     || x    ||  x   |   |superimp.| <- the lattice plot
|x     ||  x   ||   x  ||    x |   |         |
----------------------------------------------

P1:P4 :   Show an xy-plot for each factor level of the covariate P.
PS:       Displays the previous plots superimposed
superimp: P1:P4 superimposed

I would like to use this function for a two factor design.

-------------------------------------------------
|  Q1  || Q1   || Q1   || Q1   |   | PS.Pi      | <- strip1
------------------------------------------------- 
|  P1  || P1   || P3   || P4   |   | PS.Pi      | <- strip2
-------------------------------------------------
|x     ||  x   ||   x  ||    x |   |            |
|x     ||  x   ||   x  ||    x |   |            | <- plot
|x     ||  x   ||   x  ||    x |   |            |
-------------------------------------------------

-------------------------------------------------
|  Q2  || Q2   || Q2   || Q2   |   | PS.Pi      | <- strip1
------------------------------------------------- 
|  P1  || P1   || P3   || P4   |   | PS.Pi      | <- strip2
-------------------------------------------------
|x     ||  x   ||   x  ||    x |   |            |
|x     ||  x   ||   x  ||    x |   |            | <- plot
|x     ||  x   ||   x  ||    x |   |            |
-------------------------------------------------

[...]

-------------------------------------------------
|  Q5  || Q5   || Q5   || Q5   |   | PS.Pi      | <- strip1
------------------------------------------------- 
|  P1  || P1   || P3   || P4   |   | PS.Pi      | <- strip2
-------------------------------------------------
|x     ||  x   ||   x  ||    x |   |            |
|x     ||  x   ||   x  ||    x |   |            | <- plot
|x     ||  x   ||   x  ||    x |   |            |
-------------------------------------------------


Here is an example:

--8<---------------cut here---------------start------------->8---
rm(list = ls(all = TRUE))
rm(list = c(ls()))

library(lattice)
library(HH)

## 1. generate data
random <- rnorm(200)
y <- abs(random)
x1.cont <- abs(random)
x2.fac <- as.factor(rep(1:5, 4))     # 4 groups
x3.fac <- as.factor(rep(1:4, each=5))# 5 groups
A <- data.frame(y, x1.cont, x2.fac, x2.fac)

## 2. plot trellis plot:
foo <- xyplot(log(y) ~ log(x1.cont) | x2.fac * x3.fac,
              data = A,
              type = c("p","r"),
              strip = strip.custom(strip.names=TRUE),
              as.table=TRUE)

plot(foo)

## 3. plot "ancova" plot using HH:
## normal usage:
ancova(y ~ x1.cont * x2.fac)

## 4. Trying to combine "ancova" with trellis:
## does not work:
ancova(y ~ x1.cont * x2.fac * x3.fac)
--8<---------------cut here---------------end--------------->8---

-- 
Patrick Drechsler
Department of Zoology
University of Cambridge
Downing Street
Cambridge CB2 3EJ, UK


From patrick at pdrechsler.de  Fri Mar 30 00:26:50 2007
From: patrick at pdrechsler.de (Patrick Drechsler)
Date: Fri, 30 Mar 2007 00:26:50 +0200
Subject: [R] [HH] extending ancova function for 2 factors
References: <878xdfbsg0.fsf@pdrechsler.de>
Message-ID: <871wj7bs9h.fsf@pdrechsler.de>

sorry, I accidentally hit the "send" button too soon...

Please ignore.

Patrick
-- 
Patrick Drechsler
Department of Zoology
University of Cambridge
Downing Street
Cambridge CB2 3EJ, UK


From f.harrell at vanderbilt.edu  Fri Mar 30 00:31:45 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 29 Mar 2007 17:31:45 -0500
Subject: [R] Wikibooks
In-Reply-To: <loom.20070329T234918-210@post.gmane.org>
References: <20070329193059.M91171@centroin.com.br>
	<loom.20070329T234918-210@post.gmane.org>
Message-ID: <460C3E51.1010008@vanderbilt.edu>

Ben Bolker wrote:
> Alberto Monteiro <albmont <at> centroin.com.br> writes:
> 
>> As a big fan of Wikipedia, it's frustrating to see how little there is about 
>> R in the correlated project, the Wikibooks:
>>
>> http://en.wikibooks.org/wiki/R_Programming
>>
>> Alberto Monteiro
>>
> 
>   Well, we do have an R wiki -- http://wiki.r-project.org/rwiki/doku.php --
> although it is not as active as I'd like.  (We got stuck halfway through
> porting Paul Johnson's "R Tips" to it ...)   Please contribute!
>   Most of the (considerable) effort people expend in answering
> questions about R goes to the mailing lists -- I personally would like it if some
> tiny fraction of that energy could be redirected toward the wiki, where
> information can be presented in a nicer format and (ideally) polished
> over time -- rather than having to dig back through multiple threads on the
> mailing lists to get answers.  (After that we have to get people
> to look for the answers on the wiki.)

I would like to strongly second Ben.  In some ways, R experts are too 
nice.  Continuing to answer the same questions over and over does not 
lead to a better way using R wiki.  I would rather see the work go into 
enhancing the wiki and refactoring information, and responses to many 
r-help please for help be "see wiki topic x".  While doing this let's 
consider putting a little more burden on new users to look for good 
answers already provided.

Frank

> 
>   Just my two cents -- and I've been delinquent in my 
> wiki'ing recently too ...
> 
>   Ben Bolker
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From tom.soyer at gmail.com  Fri Mar 30 01:21:47 2007
From: tom.soyer at gmail.com (tom soyer)
Date: Thu, 29 Mar 2007 18:21:47 -0500
Subject: [R] ccf time units
Message-ID: <65cc7bdf0703291621s7b72db03i15ffa4e550710712@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070329/7aec5732/attachment.pl 

From ggrothendieck at gmail.com  Fri Mar 30 01:40:25 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 29 Mar 2007 19:40:25 -0400
Subject: [R] ccf time units
In-Reply-To: <65cc7bdf0703291621s7b72db03i15ffa4e550710712@mail.gmail.com>
References: <65cc7bdf0703291621s7b72db03i15ffa4e550710712@mail.gmail.com>
Message-ID: <971536df0703291640m5661f0c0g6ce44b163e8123a5@mail.gmail.com>

The unit of time for a "ts" class object is deltat(ldeaths).
See the
   ?deltat
help page.

On 3/29/07, tom soyer <tom.soyer at gmail.com> wrote:
> Hi,
>
> I am using ccf but I could not figure out how to calculate the actual lag in
> number of periods from the returned results. The documentation for ccf
> says:"The lag is returned and plotted in units of time". What does "units of
> time" mean? For example:
>
> > x=ldeaths
> > x1=lag(ldeaths,1)
> > results=ccf(x,x1)
> > results
>
> Autocorrelations of series 'X', by lag
>
> -1.2500 -1.1667 -1.0833 -1.0000 -0.9167 -0.8333 -0.7500 -0.6667 -0.5833 -
> 0.5000
>  -0.297   0.011   0.380   0.651   0.738   0.664   0.392  -0.011  -0.383  -
> 0.618
> -0.4167 -0.3333 -0.2500 -0.1667 -0.0833  0.0000  0.0833  0.1667  0.2500
> 0.3333
>  -0.693  -0.619  -0.362   0.020   0.405   0.770   0.981   0.749   0.376  -
> 0.004
>  0.4167  0.5000  0.5833  0.6667  0.7500  0.8333  0.9167  1.0000  1.0833
> 1.1667
>  -0.366  -0.610  -0.686  -0.602  -0.368  -0.012   0.388   0.650   0.705
> 0.614
>  1.2500
>  0.341
> I see that at time unit 0.0833, the correlation is close to 1. But how do I
> know time unit 0.0833 is actually lag 1? Is there a function to do the
> conversion?
>
> Thanks,
>
> Tom
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From plynchnlm at gmail.com  Fri Mar 30 01:55:40 2007
From: plynchnlm at gmail.com (Paul Lynch)
Date: Thu, 29 Mar 2007 19:55:40 -0400
Subject: [R] Vector indexing question
Message-ID: <50d6c72a0703291655j6bf6e4d3h531bf015c3b577c2@mail.gmail.com>

Suppose you have 4 related vectors:

a.id<-c(1:25, 1:25, 1:25)
a.vals <- c(101:175)        # same length as a.id (the values for those IDs)
a.id.levels <- c(1:25)
a.id.ratings <- rep(letters[1:5], times=5)    # same length as a.id.levels

What I would like to do is specify a rating from a.ratings (e.g. "e"),
get the vector of corresponding IDs from a.id.levels (via
a.id.levels[a.id.ratings=='e']) and then somehow use those IDs in a.id
to get the corresponding values from a.vals.

I think I can probably write a loop to construct of a vector of
ratings of the same length as a.id so that the ratings match the ID,
and then go from there.  Is there a better way?  Perhaps using factors
or levels or something?

Thanks,
      --Paul

-- 
Paul Lynch
Aquilent, Inc.
National Library of Medicine (Contractor)


From ramasamy at cancer.org.uk  Fri Mar 30 01:59:03 2007
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 30 Mar 2007 00:59:03 +0100
Subject: [R] Wikibooks
In-Reply-To: <460C3E51.1010008@vanderbilt.edu>
References: <20070329193059.M91171@centroin.com.br>	<loom.20070329T234918-210@post.gmane.org>
	<460C3E51.1010008@vanderbilt.edu>
Message-ID: <460C52C7.8010105@cancer.org.uk>

I think sometime ago someone suggested that we append a 
comments/discussion/wiki section to the end of every R functions' help 
page that is editable by everyday users.

In other words, every R function help page has a fixed component that 
has met R-core's approval and a clearly marked and more flexible 
components by everyday users.

The comments section on every function could contain suggestions, 
warnings (e.g. the use of "c" versus as.vector thread that was discussed 
today), examples, do's and don'ts, suggestion for clarification in 
documents.

I think starting from function-level is an interesting idea to 
complement Paul Johnson's "R tips".

This comments could perhaps be cleaned up and integrated for future 
releases if the R-core agrees on its usefulness. Think of as a Bayesian 
approach for maintaining information.

Regards, Adai



Frank E Harrell Jr wrote:
> Ben Bolker wrote:
>> Alberto Monteiro <albmont <at> centroin.com.br> writes:
>>
>>> As a big fan of Wikipedia, it's frustrating to see how little there is about 
>>> R in the correlated project, the Wikibooks:
>>>
>>> http://en.wikibooks.org/wiki/R_Programming
>>>
>>> Alberto Monteiro
>>>
>>   Well, we do have an R wiki -- http://wiki.r-project.org/rwiki/doku.php --
>> although it is not as active as I'd like.  (We got stuck halfway through
>> porting Paul Johnson's "R Tips" to it ...)   Please contribute!
>>   Most of the (considerable) effort people expend in answering
>> questions about R goes to the mailing lists -- I personally would like it if some
>> tiny fraction of that energy could be redirected toward the wiki, where
>> information can be presented in a nicer format and (ideally) polished
>> over time -- rather than having to dig back through multiple threads on the
>> mailing lists to get answers.  (After that we have to get people
>> to look for the answers on the wiki.)
> 
> I would like to strongly second Ben.  In some ways, R experts are too 
> nice.  Continuing to answer the same questions over and over does not 
> lead to a better way using R wiki.  I would rather see the work go into 
> enhancing the wiki and refactoring information, and responses to many 
> r-help please for help be "see wiki topic x".  While doing this let's 
> consider putting a little more burden on new users to look for good 
> answers already provided.
> 
> Frank
> 
>>   Just my two cents -- and I've been delinquent in my 
>> wiki'ing recently too ...
>>
>>   Ben Bolker
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
>


From hodgess at gator.dt.uh.edu  Fri Mar 30 02:11:46 2007
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Thu, 29 Mar 2007 19:11:46 -0500
Subject: [R]  Using Java or Tcl/Tk in R
Message-ID: <200703300011.l2U0BkXd013065@gator.dt.uh.edu>

Dear R People:

This is more of an opinion question please:
When putting together GUI type functions, is
it better to use JAVA or Tcl/Tk, please?

Any input is appreciated!

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu


From gunter.berton at gene.com  Fri Mar 30 02:14:25 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Thu, 29 Mar 2007 17:14:25 -0700
Subject: [R] Wikibooks
In-Reply-To: <460C3E51.1010008@vanderbilt.edu>
Message-ID: <00d701c77260$604303e0$4d908980@gne.windows.gene.com>

Question:

Many (perhaps most?) questions on the list are easily answerable simply by
checking existing R Docs (Help file/man pages, Intro to R, etc.). Why would
a Wiki be more effective in deflecting such questions from the mailing list
than them? Why would too helpful R experts be more inclined to refer people
to the Wiki than the existing docs? Bottom line: it's psychology at issue
here, I think, not the form of the docs. 

Disclaimer 1: None of this is meant to reflect one way or ther other on the
usefulness of Wikis as a documentation format -- only their ability to
change the Help list culture.

Disclaimer 2: Others have repeatedly made similar comments (asking us to
refer people to the docs rather than providing explicit answers, I mean).

Cheers,
Bert Gunter
Genentech Nonclinical Statistics
South San Francisco, CA 94404
650-467-7374


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Frank E Harrell Jr
Sent: Thursday, March 29, 2007 3:32 PM
To: Ben Bolker
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Wikibooks

Ben Bolker wrote:
> Alberto Monteiro <albmont <at> centroin.com.br> writes:
> 
>> As a big fan of Wikipedia, it's frustrating to see how little there is
about 
>> R in the correlated project, the Wikibooks:
>>
>> http://en.wikibooks.org/wiki/R_Programming
>>
>> Alberto Monteiro
>>
> 
>   Well, we do have an R wiki -- http://wiki.r-project.org/rwiki/doku.php
--
> although it is not as active as I'd like.  (We got stuck halfway through
> porting Paul Johnson's "R Tips" to it ...)   Please contribute!
>   Most of the (considerable) effort people expend in answering
> questions about R goes to the mailing lists -- I personally would like it
if some
> tiny fraction of that energy could be redirected toward the wiki, where
> information can be presented in a nicer format and (ideally) polished
> over time -- rather than having to dig back through multiple threads on
the
> mailing lists to get answers.  (After that we have to get people
> to look for the answers on the wiki.)

I would like to strongly second Ben.  In some ways, R experts are too 
nice.  Continuing to answer the same questions over and over does not 
lead to a better way using R wiki.  I would rather see the work go into 
enhancing the wiki and refactoring information, and responses to many 
r-help please for help be "see wiki topic x".  While doing this let's 
consider putting a little more burden on new users to look for good 
answers already provided.

Frank

> 
>   Just my two cents -- and I've been delinquent in my 
> wiki'ing recently too ...
> 
>   Ben Bolker
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ramasamy at cancer.org.uk  Fri Mar 30 02:39:51 2007
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 30 Mar 2007 01:39:51 +0100
Subject: [R] Vector indexing question
In-Reply-To: <50d6c72a0703291655j6bf6e4d3h531bf015c3b577c2@mail.gmail.com>
References: <50d6c72a0703291655j6bf6e4d3h531bf015c3b577c2@mail.gmail.com>
Message-ID: <460C5C57.6080307@cancer.org.uk>

Sounds like you have two different tables and are trying to mine one 
based on the other. Try

ref <- data.frame( levels  = 1:25,
                    ratings = rep(letters[1:5], times=5) )

db <- data.frame( vals=101:175, levels=c(1:25, 1:25, 1:25) )

levels.of.interest <- ref$levels[ ref$rating=="a" ]
db$vals[ which(db$levels %in% levels.of.interest) ]

  [1] 101 106 111 116 121 126 131 136 141 146 151 156 161 166 171


OR a much more intuitive way is to merge both tables and proceeding as

out <- merge( db, ref, by="levels", all.x=TRUE )
out <- out[ order(out$val), ] # little cleanup
subset( out, ratings=="a" )   # ignore the rownames

    levels vals ratings
1       1  101       a
16      6  106       a
31     11  111       a
46     16  116       a
61     21  121       a
3       1  126       a
17      6  131       a
32     11  136       a
47     16  141       a
62     21  146       a
2       1  151       a
18      6  156       a
33     11  161       a
48     16  166       a
63     21  171       a

Then you can do cool things using the apply() family like
   tapply( out$vals, out$ratings, mean )
     a   b   c   d   e
   136 137 138 139 140

Check out %in%, merge and apply.

Regards, Adai



Paul Lynch wrote:
> Suppose you have 4 related vectors:
> 
> a.id<-c(1:25, 1:25, 1:25)
> a.vals <- c(101:175)        # same length as a.id (the values for those IDs)
> a.id.levels <- c(1:25)
> a.id.ratings <- rep(letters[1:5], times=5)    # same length as a.id.levels
> 
> What I would like to do is specify a rating from a.ratings (e.g. "e"),
> get the vector of corresponding IDs from a.id.levels (via
> a.id.levels[a.id.ratings=='e']) and then somehow use those IDs in a.id
> to get the corresponding values from a.vals.
> 
> I think I can probably write a loop to construct of a vector of
> ratings of the same length as a.id so that the ratings match the ID,
> and then go from there.  Is there a better way?  Perhaps using factors
> or levels or something?
> 
> Thanks,
>       --Paul
>


From h.wickham at gmail.com  Fri Mar 30 02:47:32 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 29 Mar 2007 19:47:32 -0500
Subject: [R] Wikibooks
In-Reply-To: <00d701c77260$604303e0$4d908980@gne.windows.gene.com>
References: <460C3E51.1010008@vanderbilt.edu>
	<00d701c77260$604303e0$4d908980@gne.windows.gene.com>
Message-ID: <f8e6ff050703291747s1ec0a309r53341186e27e7798@mail.gmail.com>

> Many (perhaps most?) questions on the list are easily answerable simply by
> checking existing R Docs (Help file/man pages, Intro to R, etc.). Why would
> a Wiki be more effective in deflecting such questions from the mailing list
> than them? Why would too helpful R experts be more inclined to refer people
> to the Wiki than the existing docs? Bottom line: it's psychology at issue
> here, I think, not the form of the docs.

I agree - and there's also a problem that until the wiki becomes
useful there's no point referring people to it, and because no one
visits it, it doesn't get better.

http://www.wikipatterns.com provides some good advice for getting a wiki going.

Hadley


From marc_schwartz at comcast.net  Fri Mar 30 02:51:20 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 29 Mar 2007 19:51:20 -0500
Subject: [R] Vector indexing question
In-Reply-To: <50d6c72a0703291655j6bf6e4d3h531bf015c3b577c2@mail.gmail.com>
References: <50d6c72a0703291655j6bf6e4d3h531bf015c3b577c2@mail.gmail.com>
Message-ID: <1175215880.14657.173.camel@localhost.localdomain>

On Thu, 2007-03-29 at 19:55 -0400, Paul Lynch wrote:
> Suppose you have 4 related vectors:
> 
> a.id<-c(1:25, 1:25, 1:25)
> a.vals <- c(101:175)        # same length as a.id (the values for those IDs)
> a.id.levels <- c(1:25)
> a.id.ratings <- rep(letters[1:5], times=5)    # same length as a.id.levels
> 
> What I would like to do is specify a rating from a.ratings (e.g. "e"),
> get the vector of corresponding IDs from a.id.levels (via
> a.id.levels[a.id.ratings=='e']) and then somehow use those IDs in a.id
> to get the corresponding values from a.vals.
> 
> I think I can probably write a loop to construct of a vector of
> ratings of the same length as a.id so that the ratings match the ID,
> and then go from there.  Is there a better way?  Perhaps using factors
> or levels or something?
> 
> Thanks,
>       --Paul

Is this what you want?

DF <- data.frame(a.id, a.vals, a.id.levels, a.id.ratings)

> DF[DF$a.id.ratings == "e", "a.vals"]
 [1] 105 110 115 120 125 130 135 140 145 150 155 160 165 170 175

or

> subset(DF, a.id.ratings == "e", select = a.vals)
   a.vals
5     105
10    110
15    115
20    120
25    125
30    130
35    135
40    140
45    145
50    150
55    155
60    160
65    165
70    170
75    175

See ?subset

HTH,

Marc Schwartz


From jsorkin at grecc.umaryland.edu  Fri Mar 30 02:55:07 2007
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Thu, 29 Mar 2007 20:55:07 -0400
Subject: [R] Wikibooks
In-Reply-To: <f8e6ff050703291747s1ec0a309r53341186e27e7798@mail.gmail.com>
References: <460C3E51.1010008@vanderbilt.edu>
	<00d701c77260$604303e0$4d908980@gne.windows.gene.com>
	<f8e6ff050703291747s1ec0a309r53341186e27e7798@mail.gmail.com>
Message-ID: <460C198C.A712.00CB.0@grecc.umaryland.edu>

I think we occasionally think that it is very easy to get information because we know how to find the information. This does not mean that other people know how to find the answer. It is for this reason that questions appear on the listserver that we might think could be easily found from other sources.
John 

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC,
University of Maryland School of Medicine Claude D. Pepper OAIC,
University of Maryland Clinical Nutrition Research Unit, and
Baltimore VA Center Stroke of Excellence

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)
jsorkin at grecc.umaryland.edu

>>> "hadley wickham" <h.wickham at gmail.com> 3/29/2007 7:47 PM >>>
> Many (perhaps most?) questions on the list are easily answerable simply by
> checking existing R Docs (Help file/man pages, Intro to R, etc.). Why would
> a Wiki be more effective in deflecting such questions from the mailing list
> than them? Why would too helpful R experts be more inclined to refer people
> to the Wiki than the existing docs? Bottom line: it's psychology at issue
> here, I think, not the form of the docs.

I agree - and there's also a problem that until the wiki becomes
useful there's no point referring people to it, and because no one
visits it, it doesn't get better.

http://www.wikipatterns.com provides some good advice for getting a wiki going.

Hadley

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.

Confidentiality Statement:
This email message, including any attachments, is for the so...{{dropped}}


From lawremi at iastate.edu  Fri Mar 30 04:28:12 2007
From: lawremi at iastate.edu (Michael Lawrence)
Date: Thu, 29 Mar 2007 21:28:12 -0500
Subject: [R] Using Java or Tcl/Tk in R
In-Reply-To: <200703300011.l2U0BkXd013065@gator.dt.uh.edu>
References: <200703300011.l2U0BkXd013065@gator.dt.uh.edu>
Message-ID: <509e0620703291928j504c3eccq2f5d9a11032d75f7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070329/a9d11ecc/attachment.pl 

From andydolman at gmail.com  Fri Mar 30 05:16:34 2007
From: andydolman at gmail.com (Andrew Dolman)
Date: Fri, 30 Mar 2007 13:16:34 +1000
Subject: [R] problem using mcmcsamp() with glmer models containing
	interaction terms in fixed effects
Message-ID: <951234ac0703292016t12a0cc9cp27443ebee9b773aa@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070330/3f9abece/attachment.pl 

From cberry at tajo.ucsd.edu  Fri Mar 30 05:55:49 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Thu, 29 Mar 2007 20:55:49 -0700
Subject: [R] Vector indexing question
In-Reply-To: <50d6c72a0703291655j6bf6e4d3h531bf015c3b577c2@mail.gmail.com>
References: <50d6c72a0703291655j6bf6e4d3h531bf015c3b577c2@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0703292024001.11751@tajo.ucsd.edu>

On Thu, 29 Mar 2007, Paul Lynch wrote:

> Suppose you have 4 related vectors:
>
> a.id<-c(1:25, 1:25, 1:25)
> a.vals <- c(101:175)        # same length as a.id (the values for those IDs)
> a.id.levels <- c(1:25)
> a.id.ratings <- rep(letters[1:5], times=5)    # same length as a.id.levels
>
> What I would like to do is specify a rating from a.ratings (e.g. "e"),
> get the vector of corresponding IDs from a.id.levels (via
> a.id.levels[a.id.ratings=='e']) and then somehow use those IDs in a.id
> to get the corresponding values from a.vals.

see

 	?factor
 	?match ( in case a.id.levels does not actually index a.id.ratings)
 	?split

> a.ratings.factor <- factor( a.id.ratings[ match(a.id, a.id.levels) ])
> a.vals[ a.ratings.factor == 'e' ]
  [1] 105 110 115 120 125 130 135 140 145 150 155 160 165 170 175
>
> split( a.vals, a.ratings.factor ) # more generally
$a
  [1] 101 106 111 116 121 126 131 136 141 146 151 156 161 166 171

$b
  [1] 102 107 112 117 122 127 132 137 142 147 152 157 162 167 172
[output truncated]


> lm( a.vals ~ a.ratings.factor - 1 ) # means of a.vals

Call:
lm(formula = a.vals ~ a.ratings.factor - 1)

Coefficients:
a.ratings.factora  a.ratings.factorb  a.ratings.factorc  a.ratings.factord  a.ratings.factore
               136                137                138                139                140

>
> I think I can probably write a loop to construct of a vector of
> ratings of the same length as a.id so that the ratings match the ID,
> and then go from there.  Is there a better way?  Perhaps using factors
> or levels or something?

A warning: using factor() in this way

 	 a.ratings.factor <- factor( a.id, levels=a.id.levels, labels=a.id.ratings )

will work in this case:

 	a.vals[ a.ratings.factor == 'e' ]

but generally will get you into trouble as its creates a factor with 25 
non-unique levels. So,

 	split( a.vals, a.ratings.factor )

ends up giving a list of 25 (non-uniquely labelled) components

HTH,

Chuck

>
> Thanks,
>      --Paul
>
> -- 
> Paul Lynch
> Aquilent, Inc.
> National Library of Medicine (Contractor)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901


From ooziqpub at widexl.com  Fri Mar 30 05:56:37 2007
From: ooziqpub at widexl.com (YMarguerite FFarr)
Date: Fri, 30 Mar 2007 11:56:37 +0800
Subject: [R] on hazardous
Message-ID: <07a701c77309$3ada19d0$8435f4e0@zelitej>



Ground floor to the future
Critical Care New
SYm-C.C.T.I
16 Cents is a STEAL
This could hit  in short and over  in the long run

This one is Guaranteed to double in next 2 days
Get in this gem tomorrow, Catch an easy doubler!!

 their fourth straight win, a 131-107 blowout of the Phoenix Suns on Saturday  have the regroup a little bit.''   Denver has struggled all season to get any  "But we didn't make the NCAA tournament and that was the goal. To that extent,  defense.  ''That team can blow you out and they can score in bunches,'' he said

----- Original Message ----- 
From: "YMarguerite FFarr" <ooziqpub at widexl.com>
To: <r-help at r-project.org>
Sent: Thursday, March 22, 2007 8:27 PM
Subject: on hazardous


> Ground floor to the future
> Critical Care New
> SYm-C.C.T.I
> 16 Cents is a STEAL
> This could hit  in short and over  in the long run


From plynchnlm at gmail.com  Fri Mar 30 06:07:58 2007
From: plynchnlm at gmail.com (Paul Lynch)
Date: Fri, 30 Mar 2007 00:07:58 -0400
Subject: [R] Vector indexing question
In-Reply-To: <460C5C57.6080307@cancer.org.uk>
References: <50d6c72a0703291655j6bf6e4d3h531bf015c3b577c2@mail.gmail.com>
	<460C5C57.6080307@cancer.org.uk>
Message-ID: <50d6c72a0703292107y6d2a7c86l1d6185aad24eb2c0@mail.gmail.com>

Adai-- Thanks a lot!  This is just what I was looking for.  I was
almost sure there had to be a neat of doing this.

Bert--  Thanks for the tip.

Marc-- Not quite, although your solution works fine for the case I
gave.  What I had in mind for a.id was an arbitrary sequence of the
numbers in the range [1,25], of length 75, though I was not savvy
enough with R to express that succinctly.  You spotted a shortcut that
I hadn't reallized I was introducing.

Thanks all for your help!
          --Paul

On 3/29/07, Adaikalavan Ramasamy <ramasamy at cancer.org.uk> wrote:
> Sounds like you have two different tables and are trying to mine one
> based on the other. Try
>
> ref <- data.frame( levels  = 1:25,
>                     ratings = rep(letters[1:5], times=5) )
>
> db <- data.frame( vals=101:175, levels=c(1:25, 1:25, 1:25) )
>
> levels.of.interest <- ref$levels[ ref$rating=="a" ]
> db$vals[ which(db$levels %in% levels.of.interest) ]
>
>   [1] 101 106 111 116 121 126 131 136 141 146 151 156 161 166 171
>
>
> OR a much more intuitive way is to merge both tables and proceeding as
>
> out <- merge( db, ref, by="levels", all.x=TRUE )
> out <- out[ order(out$val), ] # little cleanup
> subset( out, ratings=="a" )   # ignore the rownames
>
>     levels vals ratings
> 1       1  101       a
> 16      6  106       a
> 31     11  111       a
> 46     16  116       a
> 61     21  121       a
> 3       1  126       a
> 17      6  131       a
> 32     11  136       a
> 47     16  141       a
> 62     21  146       a
> 2       1  151       a
> 18      6  156       a
> 33     11  161       a
> 48     16  166       a
> 63     21  171       a
>
> Then you can do cool things using the apply() family like
>    tapply( out$vals, out$ratings, mean )
>      a   b   c   d   e
>    136 137 138 139 140
>
> Check out %in%, merge and apply.
>
> Regards, Adai
>
>
>
> Paul Lynch wrote:
> > Suppose you have 4 related vectors:
> >
> > a.id<-c(1:25, 1:25, 1:25)
> > a.vals <- c(101:175)        # same length as a.id (the values for those IDs)
> > a.id.levels <- c(1:25)
> > a.id.ratings <- rep(letters[1:5], times=5)    # same length as a.id.levels
> >
> > What I would like to do is specify a rating from a.ratings (e.g. "e"),
> > get the vector of corresponding IDs from a.id.levels (via
> > a.id.levels[a.id.ratings=='e']) and then somehow use those IDs in a.id
> > to get the corresponding values from a.vals.
> >
> > I think I can probably write a loop to construct of a vector of
> > ratings of the same length as a.id so that the ratings match the ID,
> > and then go from there.  Is there a better way?  Perhaps using factors
> > or levels or something?
> >
> > Thanks,
> >       --Paul
> >
>
>


-- 
Paul Lynch
Aquilent, Inc.
National Library of Medicine (Contractor)


From gyadav at ccilindia.co.in  Fri Mar 30 07:55:37 2007
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Fri, 30 Mar 2007 11:25:37 +0530
Subject: [R] Regarding Vista
In-Reply-To: <loom.20070329T205737-370@post.gmane.org>
Message-ID: <OF9CFD03FD.FBA42B3B-ON652572AE.0020497B-652572AE.00207F91@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070330/a3725c91/attachment.pl 

From ripley at stats.ox.ac.uk  Fri Mar 30 07:55:33 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 30 Mar 2007 06:55:33 +0100 (BST)
Subject: [R] Using functions in LAPACK in a C program
In-Reply-To: <221244.52205.qm@web58404.mail.re3.yahoo.com>
References: <221244.52205.qm@web58404.mail.re3.yahoo.com>
Message-ID: <Pine.LNX.4.64.0703300644330.30405@gannet.stats.ox.ac.uk>

On Thu, 29 Mar 2007, Paul August wrote:

> Hi,
>
> I wonder where I can find an example of using a function in LAPACK 
> library in a user's own C code.

In about 20 R packages, e.g. the recommended package mgcv.

> I wrote a C program which will be 
> compiled and linked to produce a DLL file and then loaded into R. I hope 
> to use a function from LAPACK library, for example, dgesdd, in the 
> program. Following R manual, I call the function by F77_CALL(dgesdd) in 
> the program. The program can be compiled without problems. However, when 
> it is linked to produce a DLL file, I get an error message
>
>  Test.obj : error LNK2001: unresolved external symbol _dgesdd_
>  Test.dll : fatal error LNK1120: 1 unresolved externals
>
> I use VC++6.0 and the command of linking is something like this
>
>  link.exe Rdll.lib /nologo /dll /out:Test.dll /libpath:C:\R\R-2.2.1\src\gnuwin32 Test.obj
>
> Apparently, the linker cannot resolve dgesdd from Rdll.lib. If anyone 
> knows what I missed here or any example that shows how this can be done 
> properly, please let me know. Thanks a lot.

It is in Rlapack.dll not R.dll.

The linking information is in 'Writing R Extensions' for those using the 
recommended compilation system (search for LAPACK_LIBS).

You will need to build an import library for Rlapack.dll and link against 
that.

And BTW you seem to be using R 2.2.1: please update as we can only offer 
accurate advice on recent systems.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From gyadav at ccilindia.co.in  Fri Mar 30 07:59:05 2007
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Fri, 30 Mar 2007 11:29:05 +0530
Subject: [R] Regarding Vista
In-Reply-To: <loom.20070329T205737-370@post.gmane.org>
Message-ID: <OFDE9BD6D5.43FF988F-ON652572AE.0020BA4F-652572AE.0020D0BC@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070330/dae16f29/attachment.pl 

From ripley at stats.ox.ac.uk  Fri Mar 30 08:58:19 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 30 Mar 2007 07:58:19 +0100 (BST)
Subject: [R] Regarding Vista
In-Reply-To: <OFDE9BD6D5.43FF988F-ON652572AE.0020BA4F-652572AE.0020D0BC@ccilindia.co.in>
References: <OFDE9BD6D5.43FF988F-ON652572AE.0020BA4F-652572AE.0020D0BC@ccilindia.co.in>
Message-ID: <Pine.LNX.4.64.0703300744470.11859@gannet.stats.ox.ac.uk>

Please to both of you: check the advice in the R 2.5.0 alpha version of 
the rw-FAQ.  That has worked for everyone else who has reported a problem 
with installing packages on Vista.

I've placed a copy online at
http://www.stats.ox.ac.uk/pub/R/rw-FAQ.html
specifically
http://www.stats.ox.ac.uk/pub/R/rw-FAQ.html#I-don_0027t-have-permission-to-write-to-the-R_002d2_002e5_002e0alpha_005clibrary-directory

On Fri, 30 Mar 2007, gyadav at ccilindia.co.in wrote:

>
> *Sorry for duplicate post - i forgot to tell the error
>
>
> Hi Dieter,
>
> I am facing the same problem in my case. R 2.4.1 have installed
> successfully, but when i try to install the packages from a local zip
> file. It gives the following error message.
> +++++++++++++++++++++++++++++++++++
>> utils:::menuInstallLocal()
> Error in zip.unpack(pkg, tmpDir) : cannot open file 'C:/Program
> Files/R/R-2.4.1/library/file5d2b5841/aaMI/chtml/aaMI.chm'
>>
> +++++++++++++++++++++++++++++++++++
>
> Please tell me how to install the packages in a corporate environment. I
> mean i could not understand your reply, may you be a bit more elaborate so
> that i can fix up the problem in my corporate laptop.
>
> Thanks in advance
> -gaurav
>
>
>
>
>
>
> Dieter Menne <dieter.menne at menne-biomed.de>
> Sent by: r-help-bounces at stat.math.ethz.ch
> 30-03-07 12:32 AM
>
> To
> r-help at stat.math.ethz.ch
> cc
>
> Subject
> Re: [R] Regarding Vista
>
>
>
>
>
>
> <christian.ritter <at> shell.com> writes:
>
>> The Vista issue is not innocent as it threatens the life of R within
> large
> corporations. So any posts on how R
>> runs under Vista and what has to be done to make it work and what cannot
> be
> done etc will be very useful.
>
> Main problem seems to be the installation of chm-files when upgrading. I
> had to
> switch off all security mechanisms to get it done. Which is probably not
> what
> you would like to have in a corporate environment.
>
> As a temporary workaround, it would probably be best to optionally disable
> chm
> installation. Or do a poll if is is required, after all it's considered a
> legacy
> format nowadays.
>
> Dieter
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
> ============================================================================================
> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and ...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From sergio.della.franca at gmail.com  Fri Mar 30 09:07:33 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Fri, 30 Mar 2007 09:07:33 +0200
Subject: [R] Kmeans centers
In-Reply-To: <1175187040.26892.23.camel@gsimpson.geog.ucl.ac.uk>
References: <b490ce570703290602v75412a79gcb0fbcda4f4912ae@mail.gmail.com>
	<1175187040.26892.23.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <b490ce570703300007i37598886p5e43a440493d63a@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070330/e5a5178f/attachment.pl 

From dieter.menne at menne-biomed.de  Fri Mar 30 09:07:24 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 30 Mar 2007 07:07:24 +0000 (UTC)
Subject: [R] Wikibooks
References: <20070329193059.M91171@centroin.com.br>
	<loom.20070329T234918-210@post.gmane.org>
Message-ID: <loom.20070330T084949-731@post.gmane.org>

Ben Bolker <bolker <at> zoo.ufl.edu> writes:

>   Well, we do have an R wiki -- http://wiki.r-project.org/rwiki/doku.php --
> although it is not as active as I'd like.  (We got stuck halfway through
> porting Paul Johnson's "R Tips" to it ...)   Please contribute!

I once tried:

http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests

but I don't think I will do this again on the existing Wiki. I am a frequent
Wikipedia-Writer, so I know how it works, but this was discouraging.

1) The structure of the Wiki was and is still incomprehensibly to me. I needed
too much time to find out how to put the stuff into it.

2) I decided to use the "large guides" section, because I wanted the thread
transcript to be one one page. If you check the revision history, you will find
that I needed more than three hours to get it working. The main reason is the
sluggish response, and the incomprehensible error messages or the lack of it
when some " was not matched or whatever (Thanks, Ben, for correcting the
remaining errors). This is a problem of the Wiki software used, other Wikis such
as Media(pedia) are much more tolerant or informant.

Then, Philippe Grosjean informed me: "Your page is way too long and is a rather
crude copy and paste from the long thread in the mailing list."

I disagree. Why do you have a "large guides" section? And taking into account
the amount of work I put into reformatting the transcript, I decided it was my
first and last contribution to the Wiki.

Dieter Menne


From dieter.menne at menne-biomed.de  Fri Mar 30 09:15:03 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 30 Mar 2007 07:15:03 +0000 (UTC)
Subject: [R] Regarding Vista
References: <loom.20070329T205737-370@post.gmane.org>
	<OFDE9BD6D5.43FF988F-ON652572AE.0020BA4F-652572AE.0020D0BC@ccilindia.co.in>
Message-ID: <loom.20070330T090954-483@post.gmane.org>

 <gyadav <at> ccilindia.co.in> writes:

> 
> I am facing the same problem in my case. R 2.4.1 have installed 
> successfully, but when i try to install the packages from a local zip 
> file. It gives the following error message. 
> +++++++++++++++++++++++++++++++++++
> > utils:::menuInstallLocal()
> Error in zip.unpack(pkg, tmpDir) : cannot open file 'C:/Program 
> Files/R/R-2.4.1/library/file5d2b5841/aaMI/chtml/aaMI.chm'
> > 
> +++++++++++++++++++++++++++++++++++
> 
> Please tell me how to install the packages in a corporate environment. I 
> mean i could not understand your reply, may you be a bit more elaborate so 
> that i can fix up the problem in my corporate laptop.

The general recommendation in the FAQ Prof. Ripley mentions works for me; so
does switching of User Account Control (english?) totally. This may not be
feasible for you on a corporate laptop. I tried the following method: Download
the updates locally, remove the chm folder, install from local zip file. Works
for some packages, but when a DLL is included, the story starts again, and you
cannot remove these. 

So, sorry, currently no simple solution for corporate laptops where you don't
have full rights.

Dieter


From lauri.nikkinen at iki.fi  Fri Mar 30 09:31:13 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Fri, 30 Mar 2007 10:31:13 +0300
Subject: [R] Hmisc summary.formula.reverse export problem
Message-ID: <ba8c09910703300031w4d054cc5tf4fd1b2e6a4c120a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070330/84f06bf6/attachment.pl 

From ripley at stats.ox.ac.uk  Fri Mar 30 09:47:24 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 30 Mar 2007 08:47:24 +0100 (BST)
Subject: [R] Hmisc summary.formula.reverse export problem
In-Reply-To: <ba8c09910703300031w4d054cc5tf4fd1b2e6a4c120a@mail.gmail.com>
References: <ba8c09910703300031w4d054cc5tf4fd1b2e6a4c120a@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0703300844460.15273@gannet.stats.ox.ac.uk>

Well, write.table works on 'tables', that is matrix-like objects.
An object summary is not matrix-like.

You may be looking for

capture.output(print(taulu3), file="O:/taulu1.txt")


On Fri, 30 Mar 2007, Lauri Nikkinen wrote:

> Dear R-users,
>
> I'm trying to export object taulu3 from R to a text file (separated with
> semicolon). Object taulu3 is made with summary.formula in Hmisc package:
>
> taulu3 <- summary(sp ~ pdg_newtext, data=tr_ekahj, method="reverse",
> overall=TRUE)
> class(taulu3)
> [1] "summary.formula.reverse"
>
> When I try to export the object taulu3, I get the following error message:
>
> write.table(taulu3, "O:/taulu1.txt", sep=";")
>
> Error in as.data.frame.default(x[[i]], optional = TRUE, stringsAsFactors =
> stringsAsFactors) :
>        cannot coerce class "summary.formula.reverse" into a data.frame
>
> How can I get this object exported properly? I'm using Windows XP and I
> don't know latex yet.
>
> Regards,
> Lauri
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From gavin.simpson at ucl.ac.uk  Fri Mar 30 09:53:53 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 30 Mar 2007 08:53:53 +0100
Subject: [R] Kmeans centers
In-Reply-To: <b490ce570703300007i37598886p5e43a440493d63a@mail.gmail.com>
References: <b490ce570703290602v75412a79gcb0fbcda4f4912ae@mail.gmail.com>
	<1175187040.26892.23.camel@gsimpson.geog.ucl.ac.uk>
	<b490ce570703300007i37598886p5e43a440493d63a@mail.gmail.com>
Message-ID: <1175241233.3044.20.camel@dhcppc2.my.nat.localnet>

On Fri, 2007-03-30 at 09:07 +0200, Sergio Della Franca wrote:
> My simple problem is that when i run kmeans this give me different
> results because if centers is a number, a random set of (distinct)
> rows in x is chosen as the initial centres.

You can stop this and make it reproducible by setting the seed for the
random number generator before doing kmeans - this way the same
(pseudo)random set of rows get selected each time:

dat <- data.frame(a = rnorm(100), b = rnorm(100), c = rnorm(100))
set.seed(1234)
km <- kmeans(dat, 2)
set.seed(1234)
km2 <- kmeans(dat, 2)
all.equal(km, km2) ## TRUE

But ask yourself is this is helpful? Are the solutions similar each time
you run the function (without setting the seed) and get different
results? If the runs give very different results then it is likely that
you are finding local minima not an optimal solution - a common problem
with iterative algorithms using random starts.

One solution to this /is/ to use several random starts and see if you
get similar results. Some samples may switch clusters, but if the bulk
of samples assigned to same cluster (i.e. together, not in cluster "1"
as the cluster number is random) then you can be happy with the result.
That some samples switch clusters may just indicate that there isn't a
clearly defined clustering of all your samples - some are intermediate
between clusters.

Another is to use a hierarchical cluster analysis (via hclust()). Cut it
at the number of clusters you want and use the centers (sic) of those
clusters as the starting points for kmeans. This way the hclust()
results get you close to a good solution, which kmeans then updates as
it is not constrained by having a hierarchical structure.

There is an example of this in Modern Applied Statistics with S (2002 -
Venables and Ripley, Springer), but if you don't have this book, you can
see the MASS scripts for Chapter 11 of the book. The MASS scripts should
have been provided with your copy of R, in
RINSTALL/library/MASS/scripts/ where RINSTALL is the where your version
of R is installed. Then you want ch11.R in that directory. Look at
section 11.2 Cluster Analysis in that file

>  
> About me the problem is simple. 
>  
> The question i ask you is if it possible that centers could be
> different from number. 
> i.e. instead of indicate a number of center, could be possible
> indicate different character lable to identify the cluster i want to
> obtain?

No. And this is why, despite how clear and simple the problem is to you,
you need to show us an example of your data! Surly, if you have
information that exactly identifies the clusters you want to find, why
do you need a clustering algorithm to find them for you?

G

>    
> thk you
> 
> 
>  
> 2007/3/29, Gavin Simpson <gavin.simpson at ucl.ac.uk>: 
>         On Thu, 2007-03-29 at 15:02 +0200, Sergio Della Franca wrote:
>         > Dear R-Helpers,
>         >
>         > I read in the R documentation, about kmeans: 
>         >
>         >   centers
>         >
>         > Either the number of clusters or a set of initial (distinct)
>         cluster
>         > centres. *If a number*, a random set of (distinct) rows in x
>         is chosen as
>         > the initial centres. 
>         > My question is: could it be possible that the centers are
>         character and not
>         > number?
>         
>         I think you misunderstand - centers is the number of clusters
>         you want
>         to partition your data into. How else would you specify the
>         number of 
>         clusters other than by a number? So no, it has to be a numeric
>         number.
>         
>         The alternative use of centers is to provide known starting
>         points for
>         the algorithm, such as from the results of a hierarchical
>         cluster 
>         analysis, that are the locations of the cluster centroids, for
>         each
>         cluster, on each of the feature variables.
>         
>         Also, argument x to kmeans() is specific about requiring a
>         numeric
>         matrix (or something coercible to one), so characters here are
>         not 
>         allowed either.
>         
>         But then again, I may not have understood what it is that you
>         are
>         asking, but that is not surprising given that you have not
>         provided an
>         example of what you are trying to do, and how you tried to do
>         it but 
>         failed.
>         
>         > and provide commented, minimal, self-contained, reproducible
>         code.
>         
>         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>         G
>         --
>         %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~
>         %~%~%~% 
>         Gavin Simpson                 [t] +44 (0)20 7679 0522
>         ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
>         Pearson Building,             [e]
>         gavin.simpsonATNOSPAMucl.ac.uk
>         Gower Street, London          [w]
>         http://www.ucl.ac.uk/~ucfagls/
>         UK. WC1E 6BT.                 [w]
>         http://www.freshwaters.org.uk
>         %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~
>         %~%~%~%
>         
> 
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT                          [w] http://www.freshwaters.org.uk/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From sergio.della.franca at gmail.com  Fri Mar 30 10:27:44 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Fri, 30 Mar 2007 10:27:44 +0200
Subject: [R] Kmeans centers
In-Reply-To: <1175241233.3044.20.camel@dhcppc2.my.nat.localnet>
References: <b490ce570703290602v75412a79gcb0fbcda4f4912ae@mail.gmail.com>
	<1175187040.26892.23.camel@gsimpson.geog.ucl.ac.uk>
	<b490ce570703300007i37598886p5e43a440493d63a@mail.gmail.com>
	<1175241233.3044.20.camel@dhcppc2.my.nat.localnet>
Message-ID: <b490ce570703300127w421fef71obcbb2b5fad9c8e3f@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070330/aae6b85a/attachment.pl 

From francogrex at mail.com  Fri Mar 30 10:46:42 2007
From: francogrex at mail.com (francogrex)
Date: Fri, 30 Mar 2007 01:46:42 -0700 (PDT)
Subject: [R] R equivalent of S+SeqTrial?
In-Reply-To: <1175097489.14657.45.camel@Bellerophon>
References: <9708877.post@talk.nabble.com>
	<1175086394.14657.7.camel@Bellerophon>
	<40e66e0b0703280849y3f2691e5n7d48668dafa9dba@mail.gmail.com>
	<1175097489.14657.45.camel@Bellerophon>
Message-ID: <9749485.post@talk.nabble.com>


Thanks again; It's ok I'm using the ldBands function now, the ld98.exe is
just a small file to download into the path. I can calculate the alpha
spending function and determine (given the Z statistics of my observations)
if at the interim analysis the null hypothesis is to be rejected or not.
But Does this function allow me to calculate the conditional power given the
accumulating data? (I know they give the exit probabilities but is it those
that I need?) and in the documentations they make a reference to another
function using a bayesian method (Gaussian Bayesian Posterior and Predictive
Distributions), but i'm not comfortable yet using the bayesian methods.


On Wed, 2007-03-28 at 10:49 -0500, Douglas Bates wrote:
> On 3/28/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> > On Wed, 2007-03-28 at 02:42 -0700, francogrex wrote:
> > > Does anyone know of an R package that is equivalent of S+SeqTrial for
> > > analysis of clinical trials using group sequential methods? Thanks.
> >
> > I don't know that there are fully functional equivalents, but you might
> > want to look at the following:
> >
> > 1. The GroupSeq package on CRAN:
> >
> >   http://cran.r-project.org/src/contrib/Descriptions/GroupSeq.html
> >
> >
> > 2. The ldBands() function in Frank Harrell's Hmisc package on CRAN:
> >
> >   http://biostat.mc.vanderbilt.edu/s/Hmisc/html/ldBands.html
> >
> > This also requires the ld98 executable, which is available from:
> >
> >   http://www.biostat.wisc.edu/landemets/
> 
> There is also the ldbounds package on CRAN for calculating Lan-DeMets
> boundaries.  It was prepared by students working with Dave DeMets and
> does not require the ld98 executable.

-- 
View this message in context: http://www.nabble.com/R-equivalent-of-S%2BSeqTrial--tf3478858.html#a9749485
Sent from the R help mailing list archive at Nabble.com.


From lauri.nikkinen at iki.fi  Fri Mar 30 10:55:42 2007
From: lauri.nikkinen at iki.fi (Lauri Nikkinen)
Date: Fri, 30 Mar 2007 11:55:42 +0300
Subject: [R] Hmisc summary.formula.reverse export problem
In-Reply-To: <Pine.LNX.4.64.0703300844460.15273@gannet.stats.ox.ac.uk>
References: <ba8c09910703300031w4d054cc5tf4fd1b2e6a4c120a@mail.gmail.com>
	<Pine.LNX.4.64.0703300844460.15273@gannet.stats.ox.ac.uk>
Message-ID: <ba8c09910703300155s2b4f2f59ue18f96a20311fe07@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070330/c6fc56ef/attachment.pl 

From phgrosjean at sciviews.org  Fri Mar 30 11:27:46 2007
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 30 Mar 2007 11:27:46 +0200
Subject: [R] Wikibooks
In-Reply-To: <00d701c77260$604303e0$4d908980@gne.windows.gene.com>
References: <00d701c77260$604303e0$4d908980@gne.windows.gene.com>
Message-ID: <460CD812.6070901@sciviews.org>



Bert Gunter wrote:
> Question:
> 
> Many (perhaps most?) questions on the list are easily answerable simply by
> checking existing R Docs (Help file/man pages, Intro to R, etc.). Why would
> a Wiki be more effective in deflecting such questions from the mailing list
> than them? Why would too helpful R experts be more inclined to refer people
> to the Wiki than the existing docs? Bottom line: it's psychology at issue
> here, I think, not the form of the docs. 

Answer:

The online help, vignettes and manuals have a very intimidating (i.e., 
technical) presentation for people that tend to be afraid of such a 
crude presentation. It is apparently not your case, and this is probably 
why you even don't realize this could be a problem for a non negligible 
fraction of R. The Wiki was primarily targeted to them. As you say: it's 
psychology at issue here.

As other have pointed out, the main reason for the lack of success of 
the R Wiki is that the mailing lists, particularly R-Help, are sooo 
successful. However, I continue to consider that the mailing list is 
suboptimal in two cases: (1) when text is not enough to express the 
idea, and (2) for frequent questions that would certainly deserve a good 
compilation on a wiki page and a redirection to it everytime the 
question is asked.

Best,

Philippe Grosjean
-- 
..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Belgium
( ( ( ( (
..............................................................



> Disclaimer 1: None of this is meant to reflect one way or ther other on the
> usefulness of Wikis as a documentation format -- only their ability to
> change the Help list culture.
> 
> Disclaimer 2: Others have repeatedly made similar comments (asking us to
> refer people to the docs rather than providing explicit answers, I mean).
> 
> Cheers,
> Bert Gunter
> Genentech Nonclinical Statistics
> South San Francisco, CA 94404
> 650-467-7374
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Frank E Harrell Jr
> Sent: Thursday, March 29, 2007 3:32 PM
> To: Ben Bolker
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Wikibooks
> 
> Ben Bolker wrote:
>> Alberto Monteiro <albmont <at> centroin.com.br> writes:
>>
>>> As a big fan of Wikipedia, it's frustrating to see how little there is
> about 
>>> R in the correlated project, the Wikibooks:
>>>
>>> http://en.wikibooks.org/wiki/R_Programming
>>>
>>> Alberto Monteiro
>>>
>>   Well, we do have an R wiki -- http://wiki.r-project.org/rwiki/doku.php
> --
>> although it is not as active as I'd like.  (We got stuck halfway through
>> porting Paul Johnson's "R Tips" to it ...)   Please contribute!
>>   Most of the (considerable) effort people expend in answering
>> questions about R goes to the mailing lists -- I personally would like it
> if some
>> tiny fraction of that energy could be redirected toward the wiki, where
>> information can be presented in a nicer format and (ideally) polished
>> over time -- rather than having to dig back through multiple threads on
> the
>> mailing lists to get answers.  (After that we have to get people
>> to look for the answers on the wiki.)
> 
> I would like to strongly second Ben.  In some ways, R experts are too 
> nice.  Continuing to answer the same questions over and over does not 
> lead to a better way using R wiki.  I would rather see the work go into 
> enhancing the wiki and refactoring information, and responses to many 
> r-help please for help be "see wiki topic x".  While doing this let's 
> consider putting a little more burden on new users to look for good 
> answers already provided.
> 
> Frank
> 
>>   Just my two cents -- and I've been delinquent in my 
>> wiki'ing recently too ...
>>
>>   Ben Bolker
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
>


From phgrosjean at sciviews.org  Fri Mar 30 11:45:44 2007
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 30 Mar 2007 11:45:44 +0200
Subject: [R] Wikibooks
In-Reply-To: <loom.20070330T084949-731@post.gmane.org>
References: <20070329193059.M91171@centroin.com.br>	<loom.20070329T234918-210@post.gmane.org>
	<loom.20070330T084949-731@post.gmane.org>
Message-ID: <460CDC48.2060901@sciviews.org>


..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Belgium
( ( ( ( (
..............................................................

Dieter Menne wrote:
> Ben Bolker <bolker <at> zoo.ufl.edu> writes:
> 
>>   Well, we do have an R wiki -- http://wiki.r-project.org/rwiki/doku.php --
>> although it is not as active as I'd like.  (We got stuck halfway through
>> porting Paul Johnson's "R Tips" to it ...)   Please contribute!
> 
> I once tried:
> 
> http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests
> 
> but I don't think I will do this again on the existing Wiki. I am a frequent
> Wikipedia-Writer, so I know how it works, but this was discouraging.
> 
> 1) The structure of the Wiki was and is still incomprehensibly to me. I needed
> too much time to find out how to put the stuff into it.

Really bad. This was the best design we obtained after a hard work of 
several tens of people. Sorry for you. By the way, did you ever noticed 
that Wikipedia basically has NO structure? It is intended to be mostly 
accessed by KEYWORDS. On the main page, you have: "main" (that page), 
then "content" (explanation and general links to the whole content), 
plus a couple of selected content links (featured, recent, random).

So, if you like this structure, that is, basically, no structure and 
access through keywords... why not to do the same with the R Wiki? Just 
type your keyword in the top-right text entry and click "search". Then, 
you don't need to care about that "structure that is still 
incomprehensible to you".

> 2) I decided to use the "large guides" section, because I wanted the thread
> transcript to be one one page. If you check the revision history, you will find
> that I needed more than three hours to get it working. The main reason is the
> sluggish response, and the incomprehensible error messages or the lack of it
> when some " was not matched or whatever (Thanks, Ben, for correcting the
> remaining errors). This is a problem of the Wiki software used, other Wikis such
> as Media(pedia) are much more tolerant or informant.

As I said, sluggish response is probably due to a combination of a slow 
Internet communication from your computer to the server at the time you 
edited your page, the edition of a too large page, and lack of edition 
section per section (you can edit each paragraph separately). I already 
made some corrections on the Wiki when I was in USA (the server is in 
Belgium, Europe), and it was not sluggish at all... On other 
circumstances, I noted a much slower reaction, too. That's Internet!

DokuWiki is NOT slower than Mediawiki, especially with an underused Wiki 
site as R wiki is currently.

> Then, Philippe Grosjean informed me: "Your page is way too long and is a rather
> crude copy and paste from the long thread in the mailing list."

Yes, I still believe so. Wiki pages are more effective when they are 
kept short.

> I disagree. Why do you have a "large guides" section? And taking into account
> the amount of work I put into reformatting the transcript, I decided it was my
> first and last contribution to the Wiki.

The "large guides" section is for ... large guides, of course... but who 
said that they should be all contained in a single page??? Just quoting 
http://wiki.r-project.org/rwiki/doku.php?id=guides:guides: "If it is a 
larger contribution with many pages, create a dedicated subsection in 
tutorials (like ?stats-with-r?, for instance)." The key is there: a 
large guide should better be represented by several wiki pages collected 
together in a dedicated subsection. Is it that hard to understand?

Philippe Grosjean

> Dieter Menne
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sergio.della.franca at gmail.com  Fri Mar 30 12:23:10 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Fri, 30 Mar 2007 12:23:10 +0200
Subject: [R] substitute NA values
Message-ID: <b490ce570703300323s5577885dn9b3afa414afd4f45@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070330/65a99309/attachment.pl 

From murdoch at stats.uwo.ca  Fri Mar 30 12:57:46 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 30 Mar 2007 06:57:46 -0400
Subject: [R] Wikibooks
In-Reply-To: <460CD812.6070901@sciviews.org>
References: <00d701c77260$604303e0$4d908980@gne.windows.gene.com>
	<460CD812.6070901@sciviews.org>
Message-ID: <460CED2A.50708@stats.uwo.ca>

On 3/30/2007 5:27 AM, Philippe Grosjean wrote:
> 
> Bert Gunter wrote:
>> Question:
>>
>> Many (perhaps most?) questions on the list are easily answerable simply by
>> checking existing R Docs (Help file/man pages, Intro to R, etc.). Why would
>> a Wiki be more effective in deflecting such questions from the mailing list
>> than them? Why would too helpful R experts be more inclined to refer people
>> to the Wiki than the existing docs? Bottom line: it's psychology at issue
>> here, I think, not the form of the docs. 
> 
> Answer:
> 
> The online help, vignettes and manuals have a very intimidating (i.e., 
> technical) presentation for people that tend to be afraid of such a 
> crude presentation. It is apparently not your case, and this is probably 
> why you even don't realize this could be a problem for a non negligible 
> fraction of R. The Wiki was primarily targeted to them. As you say: it's 
> psychology at issue here.
> 
> As other have pointed out, the main reason for the lack of success of 
> the R Wiki is that the mailing lists, particularly R-Help, are sooo 
> successful. However, I continue to consider that the mailing list is 
> suboptimal in two cases: (1) when text is not enough to express the 
> idea, and (2) for frequent questions that would certainly deserve a good 
> compilation on a wiki page and a redirection to it everytime the 
> question is asked.

But the wiki doesn't offer a way to ask questions.  I'd be just as happy 
to answer questions there as here, but there are none there to answer 
(and the advice there is to ask questions here).

I don't know how to organize a wiki to make it easy to ask and answer 
questions.  It's a reasonably good way to collect reference information, 
but it's not very well suited to Q&A.

Duncan Murdoch


From albmont at centroin.com.br  Fri Mar 30 13:25:38 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 30 Mar 2007 09:25:38 -0200
Subject: [R] Wikibooks
In-Reply-To: <460CD812.6070901@sciviews.org>
References: <00d701c77260$604303e0$4d908980@gne.windows.gene.com>
	<460CD812.6070901@sciviews.org>
Message-ID: <20070330111600.M1590@centroin.com.br>

Philippe Grosjean wrote:
> 
> As other have pointed out, the main reason for the lack of success 
> of the R Wiki is that the mailing lists, particularly R-Help, are 
> sooo successful. However, I continue to consider that the mailing 
> list is suboptimal in two cases: (1) when text is not enough to 
> express the idea, and (2) for frequent questions that would 
> certainly deserve a good compilation on a wiki page and a 
> redirection to it everytime the question is asked.
> 
I think there's one case where the mailing list is non-optimal:
finding examples. This is where a wiki would be great.

Say I don't know (and I can't understand the help) how to
use the rnorm function. If I do RSiteSearch("rnorm"), I
will get too much useless information. OTOH, an ideal wikipedia
would have a page http://www.r-wiki.org/rnorm, where I could
find examples, learn the theory, browse the source code, and 
have links to similar functions. OK, maybe that's too much, I
would be happy just to have some examples :-)

Also, RSiteSearching is dangerous, because if someone replies
in an ignorant or malicous way (let's be creative: someone asks
"how can I open the file CONFIG.SYS", and an evil person replies 
with file.remove("CONFIG.SYS")), then this wrong answer may
be accessed by newbies. A wikipedia _may_ have wrong answers,
but these are (hopefully) ephemeral.

BTW, is it too hard to include the wiki in RSiteSearch?

Alberto Monteiro


From jrkrideau at yahoo.ca  Fri Mar 30 14:02:33 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 30 Mar 2007 08:02:33 -0400 (EDT)
Subject: [R] Regarding Vista
In-Reply-To: <loom.20070330T090954-483@post.gmane.org>
Message-ID: <495800.16744.qm@web32803.mail.mud.yahoo.com>


--- Dieter Menne <dieter.menne at menne-biomed.de> wrote:

>  <gyadav <at> ccilindia.co.in> writes:
> 
> > 
> > I am facing the same problem in my case. R 2.4.1
> have installed 
> > successfully, but when i try to install the
> packages from a local zip 
> > file. It gives the following error message. 
> > +++++++++++++++++++++++++++++++++++
> > > utils:::menuInstallLocal()
> > Error in zip.unpack(pkg, tmpDir) : cannot open
> file 'C:/Program 
> >
>
Files/R/R-2.4.1/library/file5d2b5841/aaMI/chtml/aaMI.chm'
> > > 
> > +++++++++++++++++++++++++++++++++++
> > 
> > Please tell me how to install the packages in a
> corporate environment. I 
> > mean i could not understand your reply, may you be
> a bit more elaborate so 
> > that i can fix up the problem in my corporate
> laptop.

As a somewhat desperate workaround try installing R on
a USB and see if you can run if from there.  I have
2.4.1 on a USB and it seems to work fine albeit a bit 
more slowly than from the hard drive.  


> The general recommendation in the FAQ Prof. Ripley
> mentions works for me; so
> does switching of User Account Control (english?)
> totally. This may not be
> feasible for you on a corporate laptop. I tried the
> following method: Download
> the updates locally, remove the chm folder, install
> from local zip file. Works
> for some packages, but when a DLL is included, the
> story starts again, and you
> cannot remove these. 
> 
> So, sorry, currently no simple solution for
> corporate laptops where you don't
> have full rights.
> 
> Dieter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From albmont at centroin.com.br  Fri Mar 30 14:10:57 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 30 Mar 2007 10:10:57 -0200
Subject: [R] Regarding Vista
In-Reply-To: <495800.16744.qm@web32803.mail.mud.yahoo.com>
References: <loom.20070330T090954-483@post.gmane.org>
	<495800.16744.qm@web32803.mail.mud.yahoo.com>
Message-ID: <20070330120825.M76673@centroin.com.br>


John Kane wrote:
> 
> As a somewhat desperate workaround try installing R on
> a USB and see if you can run if from there.  I have
> 2.4.1 on a USB and it seems to work fine albeit a bit 
> more slowly than from the hard drive.
> 
I love this quote, in http://zoonek2.free.fr/UNIX/48_R/02.html,
from Barry Rowlingson: 

  At some point the complexity of
  installing things like this for Windows will cross the
  complexity of installing Linux... (PS excepting
  live-Linux installs like Knoppix)

Maybe we have reached this point :-)

Alberto Monteiro


From Joao.Fadista at agrsci.dk  Fri Mar 30 14:27:06 2007
From: Joao.Fadista at agrsci.dk (=?iso-8859-1?Q?Jo=E3o_Fadista?=)
Date: Fri, 30 Mar 2007 14:27:06 +0200
Subject: [R] Model comparison
Message-ID: <EA09C4B2B0F16E44B8F3311629493C0D02ED4EA8@DJFPOST01.djf.agrsci.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070330/ad0e9ad5/attachment.pl 

From murdoch at stats.uwo.ca  Fri Mar 30 14:37:36 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 30 Mar 2007 08:37:36 -0400
Subject: [R] Wikibooks
In-Reply-To: <20070330112855.M54539@centroin.com.br>
References: <00d701c77260$604303e0$4d908980@gne.windows.gene.com>
	<460CD812.6070901@sciviews.org> <460CED2A.50708@stats.uwo.ca>
	<20070330112855.M54539@centroin.com.br>
Message-ID: <460D0490.8020204@stats.uwo.ca>

On 3/30/2007 7:34 AM, Alberto Monteiro wrote:
> Duncan Murdoch wrote:
>> 
>> But the wiki doesn't offer a way to ask questions.  I'd be just as 
>> happy to answer questions there as here, but there are none there to 
>> answer 
>> (and the advice there is to ask questions here).
>> 
>> I don't know how to organize a wiki to make it easy to ask and 
>> answer questions.  It's a reasonably good way to collect reference 
>> information, but it's not very well suited to Q&A.
>> 
> The way to ask questions in the Wiki is to micro-vandalize it :-)))))))
> 
> Since anyone can edit, if I don't know how to use some function,
> I can _create_ this page and fill it with my doubts - in the hope
> that someone will then fix it latter.
> 
> Example:
> 
>   rnorm
> 
>   This is a very weird function, because things like rnorm(0.975) 
>   should return 1.96, but returns numeric(0)
> 
> And then someone would either rename the page to qnorm, or write
> a new rnorm page.

If entering a new page is really the way to ask a question, then you 
should write this on the front page, and as a possible way to contribute 
on the getting-started page.  It would also be a good idea to tell 
people like me how to find those questions. ("Recent Edits" seems a 
little too broad, with too little in the way of subject matter in the 
comments, but maybe that's just because nobody's asking questions yet.)

And it would be a good idea to seed the wiki with a lot of questions, 
just to get some activity going.

And maybe set up a page for pointers to questions that are languishing 
unanswered?  I think the key is to make it easy to ask a question and 
easy to answer one, so don't put too much bureaucracy into the process.


Duncan Murdoch


From milopez at ual.es  Fri Mar 30 13:12:23 2007
From: milopez at ual.es (Inmaculada =?iso-8859-1?Q?L=F3pez?= =?iso-8859-1?Q?_Garc=EDa?=)
Date: Fri, 30 Mar 2007 13:12:23 +0200
Subject: [R] Inquiry
Message-ID: <3.0.1.32.20070330131223.00778018@pop.ual.es>



Good morning,

I have a question about R, I would like to know how it is possible not to
chop the result of an operation. How many decimals it is possible to obtain?

Thank you in advance,
I. L?pez
---------------------------------------------
Inmaculada L?pez Garc?a
Dpto. Estad?stica y Matem?tica Aplicada
Universidad de Almer?a
La Ca?ada de San Urbano, s/n
04120  Almer?a (SPAIN)
Tfno.:	 +34 950 01 57 75
Fax:    +34 950 01 51 67
e-mail: milopez at ual.es


From klausch at gmx.de  Fri Mar 30 11:35:33 2007
From: klausch at gmx.de (Klaus Nordhausen)
Date: Fri, 30 Mar 2007 11:35:33 +0200
Subject: [R] Tail area of sum of  Chi-square variables
In-Reply-To: <Pine.LNX.4.64.0703290739240.21308@homer22.u.washington.edu>
References: <s60bbed6.091@tedmail2.lgc.co.uk>
	<Pine.LNX.4.64.0703290739240.21308@homer22.u.washington.edu>
Message-ID: <20070330093533.14730@gmx.net>

Hi,

thanks everyone! 

pchisqsum() in the "survey" package does exactly what I was looking for!

Best wishes,

Klaus


-------- Original-Nachricht --------
Datum: Thu, 29 Mar 2007 07:45:15 -0700 (PDT)
Von: Thomas Lumley <tlumley at u.washington.edu>
An: S Ellison <S.Ellison at lgc.co.uk>
CC: klausch at gmx.de, Achim.Zeileis at wu-wien.ac.at, r-help at stat.math.ethz.ch, Christian.Kleiber at unibas.ch
Betreff: Re: [R] Tail area of sum of  Chi-square variables

> 
> The Satterthwaite approximation is surprisingly good, especially in the 
> most interesting range in the right tail (say 0.9 to 0.999). There is also
> another, better, approximation with a power of a chi-squared distribution 
> that has been used in the survey literature.
> 
> However, since it is easy to write down the characteristic function and 
> perfectly feasible to invert it by numerical integration, we might as well
> use the right answer.
> 
>  	-thomas
> 
> On Thu, 29 Mar 2007, S Ellison wrote:
> >> I was wondering if there are any R functions that give the tail area
> >> of a sum of chisquare distributions of the type:
> >>         a_1 X_1 + a_2 X_2
> >> where a_1 and a_2 are constants and X_1 and X_2 are independent
> >> chi-square variables with different degrees of freedom.
> >
> > You might also check out Welch and Satterthwaite's (separate) papers on
> effective degrees of freedom for compound estimates of variance, which led
> to a thing called the welch-satterthwaite equation by one (more or less
> notorious, but widely used) document called the ISO Guide to Expression of
> Uncertainty in Measurement (ISO, 1995). The original papers are
> > B. L. Welch, J. Royal Stat. Soc. Suppl.(1936)  3 29-48
> > B. L. Welch, Biometrika, (1938) 29 350-362
> > B. L. Welch, Biometrika, (1947) 34 28-35
> >
> > F. E. Satterthwaite, Psychometrika (1941) 6 309-316
> > F. E. Satterthwaite, Biometrics Bulletin, (1946) 2 part 6 110-114
> >
> > The W-S equation - which I believe is a special case of Welch's somewhat
> more general treatment - says that if you have multiple independent
> estimated variances v[i] (could be more or less equivalent to your a_i X_i?) with
> degrees of freedom nu[i], the distribution of their sum is approximately a
> scaled chi-squared distribution with effective degrees of freedom
> nu.effective given by
> >
> > nu.effective =  sum(v[i])^2 / sum(    (v[i]^2)/nu[i]     )
> >
> > If I recall correctly, with an observed variance s^2 (corresponding to
> the sum(v[i] above if those are observed varianes), nu*(s^2 /sigma^2) is
> distributed as chi-squared with degrees of freedom nu, so the scaling factor
> for quantiles would come out of there (depending whether you're after the
> tail areas for s^2 given sigma^2 or for a confidence interval for sigma^2
> given s^2)
> >
> > However, I will be most interested to see what a more exact calculation
> provides!
> >
> > Steve Ellison
> >
> >
> > *******************************************************************
> > This email and any attachments are confidential. Any use,
> co...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle

-- 
"Feel free" - 10 GB Mailbox, 100 FreeSMS/Monat ...


From albmont at centroin.com.br  Fri Mar 30 13:34:20 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 30 Mar 2007 09:34:20 -0200
Subject: [R] Wikibooks
In-Reply-To: <460CED2A.50708@stats.uwo.ca>
References: <00d701c77260$604303e0$4d908980@gne.windows.gene.com>
	<460CD812.6070901@sciviews.org> <460CED2A.50708@stats.uwo.ca>
Message-ID: <20070330112855.M54539@centroin.com.br>


Duncan Murdoch wrote:
> 
> But the wiki doesn't offer a way to ask questions.  I'd be just as 
> happy to answer questions there as here, but there are none there to 
> answer 
> (and the advice there is to ask questions here).
> 
> I don't know how to organize a wiki to make it easy to ask and 
> answer questions.  It's a reasonably good way to collect reference 
> information, but it's not very well suited to Q&A.
> 
The way to ask questions in the Wiki is to micro-vandalize it :-)))))))

Since anyone can edit, if I don't know how to use some function,
I can _create_ this page and fill it with my doubts - in the hope
that someone will then fix it latter.

Example:

  rnorm

  This is a very weird function, because things like rnorm(0.975) 
  should return 1.96, but returns numeric(0)

And then someone would either rename the page to qnorm, or write
a new rnorm page.

Alberto Monteiro


From h.wickham at gmail.com  Fri Mar 30 14:56:16 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 30 Mar 2007 07:56:16 -0500
Subject: [R] Wikibooks
In-Reply-To: <460CDC48.2060901@sciviews.org>
References: <20070329193059.M91171@centroin.com.br>
	<loom.20070329T234918-210@post.gmane.org>
	<loom.20070330T084949-731@post.gmane.org>
	<460CDC48.2060901@sciviews.org>
Message-ID: <f8e6ff050703300556n7489cd9bx579da284c2c953f3@mail.gmail.com>

> >
> > I once tried:
> >
> > http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests
> >
> > but I don't think I will do this again on the existing Wiki. I am a frequent
> > Wikipedia-Writer, so I know how it works, but this was discouraging.
> >
> > 1) The structure of the Wiki was and is still incomprehensibly to me. I needed
> > too much time to find out how to put the stuff into it.
>
> Really bad. This was the best design we obtained after a hard work of
> several tens of people. Sorry for you. By the way, did you ever noticed
> that Wikipedia basically has NO structure? It is intended to be mostly
> accessed by KEYWORDS. On the main page, you have: "main" (that page),
> then "content" (explanation and general links to the whole content),
> plus a couple of selected content links (featured, recent, random).

Why is how wikipedia structured relevant?  The R wiki is not an
encyclopedia, it has a quite different purpose which would be
facilitiated by better structure.  Obviously at some point the
decision was made to structure the site by type of document (large
guide, short tips, package information etc), but why?  Wouldn't it be
more appropriate to organise it around subjects?  (Of course coming up
with a good subject classification is fiendishly difficult, but
perhaps the R keywords hierarchy would have been a good start).

> So, if you like this structure, that is, basically, no structure and
> access through keywords... why not to do the same with the R Wiki? Just
> type your keyword in the top-right text entry and click "search". Then,
> you don't need to care about that "structure that is still
> incomprehensible to you".

If search is the most important navigational element why is it not
more obvious?  Additionally the recent changes button right next to
the search box makes it harder to distinguish whether the text field
is related to search or recent changes.

> > 2) I decided to use the "large guides" section, because I wanted the thread
> > transcript to be one one page. If you check the revision history, you will find
> > that I needed more than three hours to get it working. The main reason is the
> > sluggish response, and the incomprehensible error messages or the lack of it
> > when some " was not matched or whatever (Thanks, Ben, for correcting the
> > remaining errors). This is a problem of the Wiki software used, other Wikis such
> > as Media(pedia) are much more tolerant or informant.
>
> As I said, sluggish response is probably due to a combination of a slow
> Internet communication from your computer to the server at the time you
> edited your page, the edition of a too large page, and lack of edition
> section per section (you can edit each paragraph separately). I already
> made some corrections on the Wiki when I was in USA (the server is in
> Belgium, Europe), and it was not sluggish at all... On other
> circumstances, I noted a much slower reaction, too. That's Internet!

Regardless of the reasons, the fact remains that at least one person
has found it difficult and slow.  Whenever one person complains you
can be sure that 10 other people have tried and given up without
complaining.

Wiki syntax is difficult and the page explaining it is poorly structured.

> DokuWiki is NOT slower than Mediawiki, especially with an underused Wiki
> site as R wiki is currently.
>
> > Then, Philippe Grosjean informed me: "Your page is way too long and is a rather
> > crude copy and paste from the long thread in the mailing list."
>
> Yes, I still believe so. Wiki pages are more effective when they are
> kept short.

This is not a good way to build up a community around a wiki.  The
evidence regarding whether many small interlinked pages is more
preferrered or more effective than one large page is scanty, and often
it comes down to personal preference.

Hadley


From ggrothendieck at gmail.com  Fri Mar 30 15:00:13 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 30 Mar 2007 09:00:13 -0400
Subject: [R] substitute NA values
In-Reply-To: <b490ce570703300323s5577885dn9b3afa414afd4f45@mail.gmail.com>
References: <b490ce570703300323s5577885dn9b3afa414afd4f45@mail.gmail.com>
Message-ID: <971536df0703300600l35b0337sf11ed14abcce3f06@mail.gmail.com>

I assume you are referring to na.roughfix in randomForest.  I don't think it
works for logical vectors or for factors outside of data frames:

> library(randomForest)
> DF <- data.frame(a = c(T, F, T, NA, T), b = c(1:3, NA, 5))
> na.roughfix(DF)
Error in na.roughfix.data.frame(DF) : na.roughfix only works for
numeric or factor
> DF$a <- factor(DF$a)
> na.roughfix(DF$a)
Error in na.roughfix.default(DF$a) : roughfix can only deal with numeric data.
> na.roughfix(DF)
      a   b
1  TRUE 1.0
2 FALSE 2.0
3  TRUE 3.0
4  TRUE 2.5
5  TRUE 5.0


On 3/30/07, Sergio Della Franca <sergio.della.franca at gmail.com> wrote:
> Dear R-Helpers,
>
>
> I have the following data set(y):
>
>  Test_Result   #_Test
>    t                 10
>    f                 14
>    f                 25
>    f                 NA
>    f                 40
>    t                45
>    t                44
>  <NA>           47
>    t                NA
>
>
> I want to replace the NA values with the following method:
> - for the numeric variable, replace NA with median
> - for character variable , replace NA with the most frequent level
>
> If i use x<-na.roughfix(y) the NA values are correctly replaced.
> But if i x<-na.roughfix(y$Test_Result) i obtain the following error:
>
> roughfix can only deal with numeric data.
>
> How can i solve this proble that i met every time i want to replace only the
> NA values of a column (type character)?
>
> Thank you in advance.
>
>
> Sergio Della Franca
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From albmont at centroin.com.br  Fri Mar 30 15:16:05 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 30 Mar 2007 11:16:05 -0200
Subject: [R] Wikibooks
In-Reply-To: <460D0669.5020308@mango-solutions.com>
References: <00d701c77260$604303e0$4d908980@gne.windows.gene.com>	<460CD812.6070901@sciviews.org>
	<20070330111600.M1590@centroin.com.br>
	<460D0669.5020308@mango-solutions.com>
Message-ID: <20070330130916.M74977@centroin.com.br>

Romain Francois wrote:
>
>> Say I don't know (and I can't understand the help) how to
>> use the rnorm function. If I do RSiteSearch("rnorm"), I
>> will get too much useless information. OTOH, an ideal wikipedia
>> would have a page http://www.r-wiki.org/rnorm, where I could
>> find examples, learn the theory, browse the source code, and 
>> have links to similar functions. OK, maybe that's too much, I
>> would be happy just to have some examples :-)
> 
> Do you mean something like (it fullfills basically all your 
> requirements) :
> 
> R> rnorm     # get the code
> R> ?rnorm   # get the help page
>
This works when there's a decent documentation for the function.
The functions in the tcltk package, for example, are horribly
undocumented, and asking for help only loops to a general
help about all (and none) of the functions.
 
> The wiki already has a similar thing, for example for rnorm, you can 
> go to: http://wiki.r-project.org/rwiki/doku.php?id=rdoc:stats:Normal
> 
I didn't like the way it worked. I searched for rnorm and Norm,
and I got a list of pages. Even for this trivial example, I
have no idea how I could find anything using the Search.

> There has been (recently and less recently) some discussions on the
> r-sig-wiki list about why sometimes you get ~~RDOC~~ instead of the
> documentation page, it is still a work in progress.
> 
> The only tricky bit is how do I know that I have to go to 
> stats:normal, well you can ask that to R, for example using that 
> small function :
> 
> wikiHelp <- function( ... , sarcasm = TRUE ){
>     if(  length(hp <- help(...) ) > 0 ){
>       hp <- tail( strsplit(hp[1], "/")[[1]], 3 )
>       wikiPage <-
> sprintf("http://wiki.r-project.org/rwiki/doku.php?id=rdoc:%s:%s",
> hp[1],  hp[3])
>        cat("the following wiki page will be displayed in your 
> browser:",             wikiPage,             ">>>     Please feel 
> free to add information if you have some, " , sep = "\n")      if( 
> sarcasm) cat( ">>>     except if you are an evil person\n")      
>  browseURL(wikiPage)    } else print( hp )  }
> 
Nice code :-)

R> wikiHelp( rnorm )

~~RDOC~~ # what is this?

R> wikiHelp( tkWidgets )

No documentation for 'tkWidgets' in specified packages and libraries:
you could try 'help.search("tkWidgets")'

R> wikiHelp( seq )

Here it worked as expected.

> 
> The wiki has its own search engine already, so you can go there
> and use it. I guess you can search for "search" there and get
> info on how to search.
>
:-)))))))))))))))))))))

Or I could ask help(help) to learn how help works :-P

Alberto Monteiro


From murdoch at stats.uwo.ca  Fri Mar 30 15:27:43 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 30 Mar 2007 09:27:43 -0400
Subject: [R] Wikibooks
In-Reply-To: <20070330130916.M74977@centroin.com.br>
References: <00d701c77260$604303e0$4d908980@gne.windows.gene.com>	<460CD812.6070901@sciviews.org>	<20070330111600.M1590@centroin.com.br>	<460D0669.5020308@mango-solutions.com>
	<20070330130916.M74977@centroin.com.br>
Message-ID: <460D104F.4080802@stats.uwo.ca>

On 3/30/2007 9:16 AM, Alberto Monteiro wrote:
> Romain Francois wrote:
>>
>>> Say I don't know (and I can't understand the help) how to
>>> use the rnorm function. If I do RSiteSearch("rnorm"), I
>>> will get too much useless information. OTOH, an ideal wikipedia
>>> would have a page http://www.r-wiki.org/rnorm, where I could
>>> find examples, learn the theory, browse the source code, and 
>>> have links to similar functions. OK, maybe that's too much, I
>>> would be happy just to have some examples :-)
>> 
>> Do you mean something like (it fullfills basically all your 
>> requirements) :
>> 
>> R> rnorm     # get the code
>> R> ?rnorm   # get the help page
>>
> This works when there's a decent documentation for the function.
> The functions in the tcltk package, for example, are horribly
> undocumented, and asking for help only loops to a general
> help about all (and none) of the functions.

I don't remember if you've said which platform you're working on, but if 
you're on Windows, the TCL/TK documentation is available to you.  It's 
in RHOME/Tcl/doc.  This is mentioned in the ?tcltk R help topic.

I believe most Unix-like systems with TCL/TK support installed would 
have the same documentation available, but I don't know where.

That documentation assumes you're using a TCL interpreter rather than R, 
so the syntax is all wrong, but there's a mechanical translation from it 
to R syntax which is described in the ?TkCommands R help topic.

So these functions may be horribly documented, but they're not horribly 
undocumented.

Duncan Murdoch

>  
>> The wiki already has a similar thing, for example for rnorm, you can 
>> go to: http://wiki.r-project.org/rwiki/doku.php?id=rdoc:stats:Normal
>> 
> I didn't like the way it worked. I searched for rnorm and Norm,
> and I got a list of pages. Even for this trivial example, I
> have no idea how I could find anything using the Search.
> 
>> There has been (recently and less recently) some discussions on the
>> r-sig-wiki list about why sometimes you get ~~RDOC~~ instead of the
>> documentation page, it is still a work in progress.
>> 
>> The only tricky bit is how do I know that I have to go to 
>> stats:normal, well you can ask that to R, for example using that 
>> small function :
>> 
>> wikiHelp <- function( ... , sarcasm = TRUE ){
>>     if(  length(hp <- help(...) ) > 0 ){
>>       hp <- tail( strsplit(hp[1], "/")[[1]], 3 )
>>       wikiPage <-
>> sprintf("http://wiki.r-project.org/rwiki/doku.php?id=rdoc:%s:%s",
>> hp[1],  hp[3])
>>        cat("the following wiki page will be displayed in your 
>> browser:",             wikiPage,             ">>>     Please feel 
>> free to add information if you have some, " , sep = "\n")      if( 
>> sarcasm) cat( ">>>     except if you are an evil person\n")      
>>  browseURL(wikiPage)    } else print( hp )  }
>> 
> Nice code :-)
> 
> R> wikiHelp( rnorm )
> 
> ~~RDOC~~ # what is this?
> 
> R> wikiHelp( tkWidgets )
> 
> No documentation for 'tkWidgets' in specified packages and libraries:
> you could try 'help.search("tkWidgets")'
> 
> R> wikiHelp( seq )
> 
> Here it worked as expected.
> 
>> 
>> The wiki has its own search engine already, so you can go there
>> and use it. I guess you can search for "search" there and get
>> info on how to search.
>>
> :-)))))))))))))))))))))
> 
> Or I could ask help(help) to learn how help works :-P
> 
> Alberto Monteiro
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Corinna.Schmitt at igb.fraunhofer.de  Fri Mar 30 15:34:27 2007
From: Corinna.Schmitt at igb.fraunhofer.de (Schmitt, Corinna)
Date: Fri, 30 Mar 2007 15:34:27 +0200
Subject: [R] math-operations
Message-ID: <8B7B0FD99E8AF541A21609104D19615882EC89@izs-xchg01.izs.fraunhofer.de>

Hallo R-experts,

for a function I need to work with the commands "div" and "mod" known
from Pascal and Ruby. The only help I know is "ceiling()" and "floor()"
in R. Do "div" and "mod" exist? When yes please send me a little
example.

In Pascal-Syntax I want: 
513 div 100 = 5
513 mod 100 = 13

How can I get this in R-syntax?

Thanks, Corinna


From maechler at stat.math.ethz.ch  Fri Mar 30 15:39:46 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 30 Mar 2007 15:39:46 +0200
Subject: [R] CLUSTER Package
In-Reply-To: <17909.32386.504884.796506@stat.math.ethz.ch>
References: <F649B8DBD3DEAD4BB163AC4D9AC18B5F41BDD9@MD-MAIL-01.ARSNET.ARS.USDA.GOV>
	<17909.32386.504884.796506@stat.math.ethz.ch>
Message-ID: <17933.4898.394060.150570@stat.math.ethz.ch>

It seems nobody else was willing to help here
(when the original poster did not at all follow the posting
guide).

In the mean time, someone else has asked me about part of this,
so let me answer in public :

>>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Mon, 12 Mar 2007 17:23:30 +0100 writes:

    MM> Hi Vallejo, I'm pretty busy currently, and feel your
    MM> question has much more to do with how to use R more
    MM> generally than with using the functions from the cluster
    MM> package.

    MM> So you may get help from other R-help readers, but maybe
    MM> only after you have followed the posting-guide and give
    MM> a reproducible example as you're asked there.

    MM> Regards, Martin Maechler

>>>>> "VallejoR" == Vallejo, Roger <Roger.Vallejo at ARS.USDA.GOV>
>>>>>     on Mon, 12 Mar 2007 10:28:01 -0400 writes:

    VallejoR> Hi Martin, In using the Cluster Package, I have
    VallejoR> results for PAM and DIANA clustering algorithms
    VallejoR> (below "part" and "hier" objects):

                                                                        

    VallejoR> part <- pam(trout, bestk) # PAM results


    VallejoR> hier <- diana(trout) # DIANA results


    VallejoR> GeneNames <- show(RG$genes) # Gene Names are in this object

(RG is what)?


    VallejoR> But I would like also to know what genes (NAMES)
    VallejoR> are included in each cluster. I tried
    VallejoR> unsuccessfully to send these results to output
    VallejoR> files (clusters with gene Names). This must be an
    VallejoR> easy task for a good R programmer. I will
    VallejoR> appreciate very much directions or R code on how
    VallejoR> to send the PAM and DIANA results to output files
    VallejoR> including information on genes (Names) per each
    VallejoR> cluster.

For diana(), a *hierarchical* clustering {as agnes()}, you need
to decide about the number of clusters yourself.
Then, as the example in  help(diana.object) shows,
you can use cutree() to get the grouping vector:

Here's a reproducible example :

library(cluster)
data(votes.repub)
dv <- diana(votes.repub, metric = "manhattan", stand = TRUE)
print(dv)
plot(dv)

## Cut into 2 groups:
dv2 <- cutree(as.hclust(dv), k = 2)
table(dv2) # 8 and 42 group members
rownames(votes.repub)[dv2 == 1]

## For two groups, does the metric matter ?
dv0 <- diana(votes.repub, stand = TRUE) # default: Euclidean
dv.2 <- cutree(as.hclust(dv0), k = 2)
table(dv2 == dv.2)## identical group assignments

----------------

For pam(), it's even simpler :

data(ruspini)
pr <- pam(ruspini, 4)
plot(pr)

# ....Hit <Return> to see next plot: 
str(pr)
## or
summary(pr)
## .. shows you that there's a component 'clustering' :

pr$clustering
## a grouping vector with case-labels {your Gene names}; here "1","2",.."150:

## and to get them ``visually'':
split(rownames(ruspini), pr$clustering)
## $`1`
##  [1] "1"  "2"  "3"  "4"  "5"  "6"  "7"  "8"  "9"  "10" "11" "12" "13" "14" "15"
## [16] "16" "17" "18" "19" "20"

## $`2`
##  [1] "21" "22" "23" "24" "25" "26" "27" "28" "29" "30" "31" "32" "33" "34" "35"
## [16] "36" "37" "38" "39" "40" "41" "42" "43"

## $`3`
##  [1] "44" "45" "46" "47" "48" "49" "50" "51" "52" "53" "54" "55" "56" "57" "58"
## [16] "59" "60"

## $`4`
##  [1] "61" "62" "63" "64" "65" "66" "67" "68" "69" "70" "71" "72" "73" "74" "75"


From dimitris.rizopoulos at med.kuleuven.be  Fri Mar 30 15:40:21 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 30 Mar 2007 15:40:21 +0200
Subject: [R] math-operations
References: <8B7B0FD99E8AF541A21609104D19615882EC89@izs-xchg01.izs.fraunhofer.de>
Message-ID: <003d01c772d0$f681bc50$0540210a@www.domain>

513 %/% 100

513 %% 100

check ?"%/%" for more info.


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Schmitt, Corinna" <Corinna.Schmitt at igb.fraunhofer.de>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, March 30, 2007 3:34 PM
Subject: [R] math-operations


> Hallo R-experts,
>
> for a function I need to work with the commands "div" and "mod" 
> known
> from Pascal and Ruby. The only help I know is "ceiling()" and 
> "floor()"
> in R. Do "div" and "mod" exist? When yes please send me a little
> example.
>
> In Pascal-Syntax I want:
> 513 div 100 = 5
> 513 mod 100 = 13
>
> How can I get this in R-syntax?
>
> Thanks, Corinna
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From Stefano.Guazzetti at ausl.re.it  Fri Mar 30 15:41:13 2007
From: Stefano.Guazzetti at ausl.re.it (Guazzetti Stefano)
Date: Fri, 30 Mar 2007 15:41:13 +0200
Subject: [R] R:  math-operations
Message-ID: <F0E5B4FAD37B7844B6D21998C11E60A6445CF9@RE2-EXC-VBE1B.ausl.org>

I guess you need "%%" and ""%/%"

try  
> 513 %/% 100
[1] 5
> 513 %% 100
[1] 13

?"%%"


Stefano


-----Messaggio originale-----
Da: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]Per conto di Schmitt, Corinna
Inviato: venerd? 30 marzo 2007 15.34
A: r-help at stat.math.ethz.ch
Oggetto: [R] math-operations


Hallo R-experts,

for a function I need to work with the commands "div" and "mod" known
from Pascal and Ruby. The only help I know is "ceiling()" and "floor()"
in R. Do "div" and "mod" exist? When yes please send me a little
example.

In Pascal-Syntax I want: 
513 div 100 = 5
513 mod 100 = 13

How can I get this in R-syntax?

Thanks, Corinna

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Fri Mar 30 15:43:39 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 30 Mar 2007 09:43:39 -0400
Subject: [R] math-operations
In-Reply-To: <8B7B0FD99E8AF541A21609104D19615882EC89@izs-xchg01.izs.fraunhofer.de>
References: <8B7B0FD99E8AF541A21609104D19615882EC89@izs-xchg01.izs.fraunhofer.de>
Message-ID: <460D140B.7020008@stats.uwo.ca>

On 3/30/2007 9:34 AM, Schmitt, Corinna wrote:
> Hallo R-experts,
> 
> for a function I need to work with the commands "div" and "mod" known
> from Pascal and Ruby. The only help I know is "ceiling()" and "floor()"
> in R. Do "div" and "mod" exist? When yes please send me a little
> example.
> 
> In Pascal-Syntax I want: 
> 513 div 100 = 5
> 513 mod 100 = 13
> 
> How can I get this in R-syntax?

div is %/% and mod is %%.  These (and the other arithmetic operators in 
R) are described in the ?Arithmetic help topic.  The full list of all 
operators is in the "R Language Definition" manual, near the start of 
the "Evaluation of expressions" chapter.

Duncan Murdoch


From amurta at ipimar.pt  Fri Mar 30 15:46:48 2007
From: amurta at ipimar.pt (Alberto Murta)
Date: Fri, 30 Mar 2007 14:46:48 +0100
Subject: [R] math-operations
In-Reply-To: <8B7B0FD99E8AF541A21609104D19615882EC89@izs-xchg01.izs.fraunhofer.de>
References: <8B7B0FD99E8AF541A21609104D19615882EC89@izs-xchg01.izs.fraunhofer.de>
Message-ID: <200703301446.48712.amurta@ipimar.pt>

On Friday 30 March 2007 14:34, Schmitt, Corinna wrote:
> Hallo R-experts,
>
> for a function I need to work with the commands "div" and "mod" known
> from Pascal and Ruby. The only help I know is "ceiling()" and "floor()"
> in R. Do "div" and "mod" exist? When yes please send me a little
> example.
>
> In Pascal-Syntax I want:
> 513 div 100 = 5

trunc(513 / 5)

> 513 mod 100 = 13

513 %% 100

>
> How can I get this in R-syntax?
>
> Thanks, Corinna
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.

-- 
   Alberto G. Murta
IPIMAR - Portuguese Institute of Fisheries and Marine Research
Avenida de Brasilia s/n; 1449-006 Lisboa; Portugal
Tel: +351 213027120; Fax: +351 213015948; email: amurta at ipimar.pt


From petr.pikal at precheza.cz  Fri Mar 30 15:50:15 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Fri, 30 Mar 2007 15:50:15 +0200
Subject: [R]  Inquiry
Message-ID: <OF841E9A97.AFE14DFD-ONC12572AE.004BC2B4-C12572AE.004C036D@precheza.cz>

Petr Pikal
petr.pikal at precheza.cz
----- Postoupil do Petr PIKAL/CTCAP dne 30.03.2007 15:47 -----

Petr PIKAL/CTCAP napsal dne 30.03.2007 15:01:58:

> Hi
> 
> Do you mean rounding. If yes you can consult 
> ?round, ?floor, ?ceiling.
> 
> If you want just to print different amount of digits you can look at
> options(digits=n) or ?sprintf or ?format
> 
> Regards
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> r-help-bounces at stat.math.ethz.ch napsal dne 30.03.2007 13:12:23:
> 
> > 
> > 
> > Good morning,
> > 
> > I have a question about R, I would like to know how it is possible not 
to
> > chop the result of an operation. How many decimals it is possible to 
obtain?
> > 
> > Thank you in advance,
> > I. L?pez
> > ---------------------------------------------
> > Inmaculada L?pez Garc?a
> > Dpto. Estad?stica y Matem?tica Aplicada
> > Universidad de Almer?a
> > La Ca?ada de San Urbano, s/n
> > 04120  Almer?a (SPAIN)
> > Tfno.:    +34 950 01 57 75
> > Fax:    +34 950 01 51 67
> > e-mail: milopez at ual.es
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From amurta at ipimar.pt  Fri Mar 30 15:59:22 2007
From: amurta at ipimar.pt (Alberto Murta)
Date: Fri, 30 Mar 2007 14:59:22 +0100
Subject: [R] math-operations
In-Reply-To: <200703301446.48712.amurta@ipimar.pt>
References: <8B7B0FD99E8AF541A21609104D19615882EC89@izs-xchg01.izs.fraunhofer.de>
	<200703301446.48712.amurta@ipimar.pt>
Message-ID: <200703301459.23232.amurta@ipimar.pt>

On Friday 30 March 2007 14:46, Alberto Murta wrote:
> On Friday 30 March 2007 14:34, Schmitt, Corinna wrote:
> > Hallo R-experts,
> >
> > for a function I need to work with the commands "div" and "mod" known
> > from Pascal and Ruby. The only help I know is "ceiling()" and "floor()"
> > in R. Do "div" and "mod" exist? When yes please send me a little
> > example.
> >
> > In Pascal-Syntax I want:
> > 513 div 100 = 5
>
> trunc(513 / 5)

Sorry! it's   trunc(513 / 100)

>
> > 513 mod 100 = 13
>
> 513 %% 100
>
> > How can I get this in R-syntax?
> >
> > Thanks, Corinna
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.

-- 
   Alberto G. Murta
IPIMAR - Portuguese Institute of Fisheries and Marine Research
Avenida de Brasilia s/n; 1449-006 Lisboa; Portugal
Tel: +351 213027120; Fax: +351 213015948; email: amurta at ipimar.pt


From theo_rstt at borm.org  Fri Mar 30 16:10:17 2007
From: theo_rstt at borm.org (Theo Borm)
Date: Fri, 30 Mar 2007 16:10:17 +0200
Subject: [R] faster computation of cumulative multinomial distribution
Message-ID: <460D1A49.9010101@borm.org>

Dear list members,

I have a series of /unequal/ probabilities [p1,p2,...,pk], describing
mutually exclusive events, and a "remainder" class with a probability
p0=1-p1-p2-....-pk, and need to calculate, for a given number of trials
t>=k, the combined probability that each of the classes 1...k contains
at least 1 "event" (the remainder class may be empty).

To me this reaks of a sum of multinomial distributions, and indeed, I
can readily calculate the correct answer for small figures t,k using a
small R program.

However, in my typical experiment, k ranges from ~20-60 and t from
~40-100, and having to calculate these for about 6e9 experiments, a
quick calculation on the back of a napkin shows me that I will not be
able to complete these calculations before the expected end of the universe.

I already figured out that in this particular case I may use reciprocal
probabilities, and if I do this I get an equation with "only" 2^k terms,
which would shorten my computations to a few decades.

Isn't there a faster (numerical approximation?) way to do this?

R has dbinom /and/ pbinom functions, but unfortunately only a dmultinom
and no pmultinom function... perhaps because there is no (known) faster way?

with kind regards,

Theo Borm


From Corinna.Schmitt at igb.fraunhofer.de  Fri Mar 30 16:21:08 2007
From: Corinna.Schmitt at igb.fraunhofer.de (Schmitt, Corinna)
Date: Fri, 30 Mar 2007 16:21:08 +0200
Subject: [R] math-operations
In-Reply-To: <F0E5B4FAD37B7844B6D21998C11E60A6445CF9@RE2-EXC-VBE1B.ausl.org>
References: <F0E5B4FAD37B7844B6D21998C11E60A6445CF9@RE2-EXC-VBE1B.ausl.org>
Message-ID: <8B7B0FD99E8AF541A21609104D19615882EC95@izs-xchg01.izs.fraunhofer.de>

Thanks, it works.

Corinna



-----Urspr?ngliche Nachricht-----
Von: Guazzetti Stefano [mailto:Stefano.Guazzetti at ausl.re.it] 
Gesendet: Freitag, 30. M?rz 2007 15:41
An: Schmitt, Corinna; r-help at stat.math.ethz.ch
Betreff: R: [R] math-operations

I guess you need "%%" and ""%/%"

try  
> 513 %/% 100
[1] 5
> 513 %% 100
[1] 13

?"%%"


Stefano


-----Messaggio originale-----
Da: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]Per conto di Schmitt, Corinna
Inviato: venerd? 30 marzo 2007 15.34
A: r-help at stat.math.ethz.ch
Oggetto: [R] math-operations


Hallo R-experts,

for a function I need to work with the commands "div" and "mod" known
from Pascal and Ruby. The only help I know is "ceiling()" and "floor()"
in R. Do "div" and "mod" exist? When yes please send me a little
example.

In Pascal-Syntax I want: 
513 div 100 = 5
513 mod 100 = 13

How can I get this in R-syntax?

Thanks, Corinna

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From albmont at centroin.com.br  Fri Mar 30 16:20:24 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 30 Mar 2007 12:20:24 -0200
Subject: [R] math-operations and the R-Wiki
In-Reply-To: <003d01c772d0$f681bc50$0540210a@www.domain>
References: <8B7B0FD99E8AF541A21609104D19615882EC89@izs-xchg01.izs.fraunhofer.de>
	<003d01c772d0$f681bc50$0540210a@www.domain>
Message-ID: <20070330141857.M52396@centroin.com.br>

Dimitris Rizopoulos wrote:
>
> 513 %/% 100
> 
> 513 %% 100
>
Now this is a great opportunity to improve the R-Wiki.

What about a "Pascal" page in the R-Wiki, where a list
of Pascal-to-R translations would be available?

Alberto Monteiro


From sergio.della.franca at gmail.com  Fri Mar 30 16:25:58 2007
From: sergio.della.franca at gmail.com (Sergio Della Franca)
Date: Fri, 30 Mar 2007 16:25:58 +0200
Subject: [R] substitute NA values
In-Reply-To: <971536df0703300600l35b0337sf11ed14abcce3f06@mail.gmail.com>
References: <b490ce570703300323s5577885dn9b3afa414afd4f45@mail.gmail.com>
	<971536df0703300600l35b0337sf11ed14abcce3f06@mail.gmail.com>
Message-ID: <b490ce570703300725x269279b3l23561a551eea8c51@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070330/e8476abc/attachment.pl 

From sithoreau at coreypavin.com  Fri Mar 30 17:30:11 2007
From: sithoreau at coreypavin.com (XBradly Hans)
Date: Fri, 30 Mar 2007 16:30:11 +0100
Subject: [R] Or starchy
Message-ID: <026e01c77301$3acb50a0$8963b0d0@hregionsc>



Get in on Energy Bottom
CRITICAL Care N E W
SYm-C.C.T.I
Extremely b ullish at 16 Cents
Watch it like a hawk

This one is Guaranteed to double in next 2 days
Get in this gem tomorrow, Catch an easy doubler!!

D'Antoni said. ''We're in a little bit of a funk right now. Discombobulated. We Martin, a now-deceased former booster, told the federal government he lent  the first round of the playoffs next month.   ''We didn't have any energy,''  Detroit. ... The Nuggets play just one more home game before April 4.  

----- Original Message ----- 
From: "XBradly Hans" <sithoreau at coreypavin.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, March 22, 2007 8:27 PM
Subject: Or starchy


> Get in on Energy Bottom
> CRITICAL Care N E W
> SYm-C.C.T.I
> Extremely b ullish at 16 Cents
> Watch it like a hawk


From albmont at centroin.com.br  Fri Mar 30 16:35:42 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 30 Mar 2007 12:35:42 -0200
Subject: [R] Wikibooks
In-Reply-To: <460D104F.4080802@stats.uwo.ca>
References: <00d701c77260$604303e0$4d908980@gne.windows.gene.com>	<460CD812.6070901@sciviews.org>	<20070330111600.M1590@centroin.com.br>	<460D0669.5020308@mango-solutions.com>
	<20070330130916.M74977@centroin.com.br>
	<460D104F.4080802@stats.uwo.ca>
Message-ID: <20070330143155.M48690@centroin.com.br>


Duncan Murdoch wrote:
>
>> This works when there's a decent documentation for the function.
>> The functions in the tcltk package, for example, are horribly
>> undocumented, and asking for help only loops to a general
>> help about all (and none) of the functions.
> 
> I don't remember if you've said which platform you're working on,
> but if you're on Windows, the TCL/TK documentation is available to 
> you.  It's in RHOME/Tcl/doc.  This is mentioned in the ?tcltk R help 
> topic.
> 
I always find it easier to get the help from the Internet, even
using Google (search for "tcl/tk grid", for example) than with
the internal documentation...

> I believe most Unix-like systems with TCL/TK support installed would 
> have the same documentation available, but I don't know where.
> 
> That documentation assumes you're using a TCL interpreter rather 
> than R, so the syntax is all wrong, but there's a mechanical 
> translation from it to R syntax which is described in the 
> ?TkCommands R help topic.
> 
> So these functions may be horribly documented, but they're not 
> horribly undocumented.
>
:-))))))))))))))))))))))))))

Ok, maybe I should shut up complaining and actually _do_ something
useful, like going into the R-Wiki and _writing_ everything I learned
about R in the past 6 months...

Alberto Monteiro


From jrkrideau at yahoo.ca  Fri Mar 30 16:36:38 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 30 Mar 2007 10:36:38 -0400 (EDT)
Subject: [R] Inquiry
In-Reply-To: <3.0.1.32.20070330131223.00778018@pop.ual.es>
Message-ID: <7384.89517.qm@web32802.mail.mud.yahoo.com>

I am not sure I understand your question but have a
look at ?round and the signif command on that page.  

--- Inmaculada L?pez Garc?a <milopez at ual.es> wrote:

> 
> 
> Good morning,
> 
> I have a question about R, I would like to know how
> it is possible not to
> chop the result of an operation. How many decimals
> it is possible to obtain?
> 
> Thank you in advance,
> I. L?pez
> ---------------------------------------------
> Inmaculada L?pez Garc?a
> Dpto. Estad?stica y Matem?tica Aplicada
> Universidad de Almer?a
> La Ca?ada de San Urbano, s/n
> 04120  Almer?a (SPAIN)
> Tfno.:	 +34 950 01 57 75
> Fax:    +34 950 01 51 67
> e-mail: milopez at ual.es
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From albmont at centroin.com.br  Fri Mar 30 16:37:52 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 30 Mar 2007 12:37:52 -0200
Subject: [R] Wikibooks
In-Reply-To: <460D104F.4080802@stats.uwo.ca>
References: <00d701c77260$604303e0$4d908980@gne.windows.gene.com>	<460CD812.6070901@sciviews.org>	<20070330111600.M1590@centroin.com.br>	<460D0669.5020308@mango-solutions.com>
	<20070330130916.M74977@centroin.com.br>
	<460D104F.4080802@stats.uwo.ca>
Message-ID: <20070330143752.M85@centroin.com.br>


Duncan Murdoch wrote:
>
>> This works when there's a decent documentation for the function.
>> The functions in the tcltk package, for example, are horribly
>> undocumented, and asking for help only loops to a general
>> help about all (and none) of the functions.
> 
> I don't remember if you've said which platform you're working on,
> but if you're on Windows, the TCL/TK documentation is available to 
> you.  It's in RHOME/Tcl/doc.  This is mentioned in the ?tcltk R help 
> topic.
> 
I always find it easier to get the help from the Internet, even
using Google (search for "tcl/tk grid", for example) than with
the internal documentation...

> I believe most Unix-like systems with TCL/TK support installed would 
> have the same documentation available, but I don't know where.
> 
> That documentation assumes you're using a TCL interpreter rather 
> than R, so the syntax is all wrong, but there's a mechanical 
> translation from it to R syntax which is described in the 
> ?TkCommands R help topic.
> 
> So these functions may be horribly documented, but they're not 
> horribly undocumented.
>
:-))))))))))))))))))))))))))

Ok, maybe I should shut up complaining and actually _do_ something
useful, like going into the R-Wiki and _writing_ everything I learned
about R in the past 6 months...

Alberto Monteiro


From mmanfrin at ulb.ac.be  Fri Mar 30 16:37:52 2007
From: mmanfrin at ulb.ac.be (Max Manfrin)
Date: Fri, 30 Mar 2007 16:37:52 +0200
Subject: [R] ANOVA and confidence intervals plot
Message-ID: <1AD1E1A9-0AC1-435B-87CD-EB7706CCAEB8@ulb.ac.be>

Dear *,
	I would like to obtain for each factor of my anova model the  
"response variable vs factor" plot with means and 95% Tukey HSD  
intervals.

	I would appreciate any information on how to do that.

	Cheers
--------------------------------------------------------------------
Max MANFRIN                                 Tel.: +32 (0)2 650 3168
IRIDIA - CoDE, CP 194/6                     Fax.: +32 (0)2 650 2715
Universit? Libre de Bruxelles
Av. F. D. Roosevelt, 50
1050 Brussels                             Email: mmanfrin at ulb.ac.be
BELGIUM                      WWW: http://iridia.ulb.ac.be/~mmanfrin

gpg DSA: 0x7E67B4C4
gpg fingerprint: C2E9 1689 CADD 7CAE 2FAB A355 8FD9 9DD1 7E67 B4C4
--------------------------------------------------------------------


-------------- next part --------------
A non-text attachment was scrubbed...
Name: PGP.sig
Type: application/pgp-signature
Size: 194 bytes
Desc: This is a digitally signed message part
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070330/9197e006/attachment.bin 

From ggrothendieck at gmail.com  Fri Mar 30 16:46:03 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 30 Mar 2007 10:46:03 -0400
Subject: [R] substitute NA values
In-Reply-To: <b490ce570703300725x269279b3l23561a551eea8c51@mail.gmail.com>
References: <b490ce570703300323s5577885dn9b3afa414afd4f45@mail.gmail.com>
	<971536df0703300600l35b0337sf11ed14abcce3f06@mail.gmail.com>
	<b490ce570703300725x269279b3l23561a551eea8c51@mail.gmail.com>
Message-ID: <971536df0703300746k3eff53d9ia7712daaa474d064@mail.gmail.com>

Not as part of na.roughfix.

You could convert your character strings to factors
and back again:

library(randomForest)
DF <- data.frame(a = c(T, F, T, NA, T),
  b = c(1:3, NA, 5),
  c = c("b", "b", NA, "d", "e"),
  d = factor(c("a", "a", NA, "d", "e")),
  stringsAsFactors = FALSE)
DF$a <- factor(DF$a)
DF$c <- factor(DF$c)
DF <- na.roughfix(DF)
DF$c <- as.character(DF$c)
DF




On 3/30/07, Sergio Della Franca <sergio.della.franca at gmail.com> wrote:
> This is that i obtained.
>
> There isn't a method to replace the NA values only for character variable?
>
>
>
>
>
>
> 2007/3/30, Gabor Grothendieck <ggrothendieck at gmail.com>:
> > I assume you are referring to na.roughfix in randomForest.  I don't think
> it
> > works for logical vectors or for factors outside of data frames:
> >
> > > library(randomForest)
> > > DF <- data.frame(a = c(T, F, T, NA, T), b = c(1:3, NA, 5))
> > > na.roughfix(DF)
> > Error in na.roughfix.data.frame(DF) : na.roughfix only works for
> > numeric or factor
> > > DF$a <- factor(DF$a)
> > > na.roughfix(DF$a)
> > Error in na.roughfix.default(DF$a) : roughfix can only deal with numeric
> data.
> > > na.roughfix(DF)
> >      a   b
> > 1  TRUE 1.0
> > 2 FALSE 2.0
> > 3  TRUE 3.0
> > 4  TRUE 2.5
> > 5  TRUE 5.0
> >
> >
> > On 3/30/07, Sergio Della Franca <sergio.della.franca at gmail.com> wrote:
> > > Dear R-Helpers,
> > >
> > >
> > > I have the following data set(y):
> > >
> > >  Test_Result   #_Test
> > >    t                 10
> > >    f                 14
> > >    f                 25
> > >    f                 NA
> > >    f                 40
> > >    t                45
> > >    t                44
> > >  <NA>           47
> > >    t                NA
> > >
> > >
> > > I want to replace the NA values with the following method:
> > > - for the numeric variable, replace NA with median
> > > - for character variable , replace NA with the most frequent level
> > >
> > > If i use x<-na.roughfix(y) the NA values are correctly replaced.
> > > But if i x<-na.roughfix(y$Test_Result) i obtain the following error:
> > >
> > > roughfix can only deal with numeric data.
> > >
> > > How can i solve this proble that i met every time i want to replace only
> the
> > > NA values of a column (type character)?
> > >
> > > Thank you in advance.
> > >
> > >
> > > Sergio Della Franca
> > >
> > >        [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
>
>


From gavin.simpson at ucl.ac.uk  Fri Mar 30 16:56:30 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 30 Mar 2007 15:56:30 +0100
Subject: [R] substitute NA values
In-Reply-To: <b490ce570703300725x269279b3l23561a551eea8c51@mail.gmail.com>
References: <b490ce570703300323s5577885dn9b3afa414afd4f45@mail.gmail.com>
	<971536df0703300600l35b0337sf11ed14abcce3f06@mail.gmail.com>
	<b490ce570703300725x269279b3l23561a551eea8c51@mail.gmail.com>
Message-ID: <1175266590.7115.57.camel@gsimpson.geog.ucl.ac.uk>

On Fri, 2007-03-30 at 16:25 +0200, Sergio Della Franca wrote:
> This is that i obtained.
> 
> There isn't a method to replace the NA values only for character variable?

This is R, there is always a way (paraphrasing an R-Helper the name of
whom I forget just now). If you mean a canned function, not that I'm
aware of.

Here is one way:

## some example data - not exactly like yours
set.seed(1234)
dat <- data.frame(test = sample(c("t","f"), 9, replace = TRUE), 
                  num = c(10,14,25,NA,40,45,44,47,NA))

## add an NA to dat$test to match your example
dat$test[8] <- NA

## print out dat
dat

## count the various options in $test and return the name of
## the most frequent
freq <- names(which.max(table(dat$test)))

## replace NA in $test with most frequent
dat$test[is.na(dat$test)] <- freq

## print out dat again to show this worked
dat

There may be better ways - the names(which.max(table(...))) seems a bit
clunky to me but it is Friday afternoon and it's been a long week...

And, as this /is/ R, you could wrap that into a function for you use on
other data sets, but I'll leave that bit up to you.

HTH

G

> 
> 2007/3/30, Gabor Grothendieck <ggrothendieck at gmail.com>:
> >
> > I assume you are referring to na.roughfix in randomForest.  I don't think
> > it
> > works for logical vectors or for factors outside of data frames:
> >
> > > library(randomForest)
> > > DF <- data.frame(a = c(T, F, T, NA, T), b = c(1:3, NA, 5))
> > > na.roughfix(DF)
> > Error in na.roughfix.data.frame(DF) : na.roughfix only works for
> > numeric or factor
> > > DF$a <- factor(DF$a)
> > > na.roughfix(DF$a)
> > Error in na.roughfix.default(DF$a) : roughfix can only deal with numeric
> > data.
> > > na.roughfix(DF)
> >      a   b
> > 1  TRUE 1.0
> > 2 FALSE 2.0
> > 3  TRUE 3.0
> > 4  TRUE 2.5
> > 5  TRUE 5.0
> >
> >
> > On 3/30/07, Sergio Della Franca <sergio.della.franca at gmail.com> wrote:
> > > Dear R-Helpers,
> > >
> > >
> > > I have the following data set(y):
> > >
> > >  Test_Result   #_Test
> > >    t                 10
> > >    f                 14
> > >    f                 25
> > >    f                 NA
> > >    f                 40
> > >    t                45
> > >    t                44
> > >  <NA>           47
> > >    t                NA
> > >
> > >
> > > I want to replace the NA values with the following method:
> > > - for the numeric variable, replace NA with median
> > > - for character variable , replace NA with the most frequent level
> > >
> > > If i use x<-na.roughfix(y) the NA values are correctly replaced.
> > > But if i x<-na.roughfix(y$Test_Result) i obtain the following error:
> > >
> > > roughfix can only deal with numeric data.
> > >
> > > How can i solve this proble that i met every time i want to replace only
> > the
> > > NA values of a column (type character)?
> > >
> > > Thank you in advance.
> > >
> > >
> > > Sergio Della Franca
> > >
> > >        [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From julien.martin2 at usherbrooke.ca  Fri Mar 30 17:09:34 2007
From: julien.martin2 at usherbrooke.ca (Julien)
Date: Fri, 30 Mar 2007 11:09:34 -0400
Subject: [R] Testing random effect in logistic mixed model
In-Reply-To: <mailman.11.1175248808.8613.r-help@stat.math.ethz.ch>
References: <mailman.11.1175248808.8613.r-help@stat.math.ethz.ch>
Message-ID: <000801c772dd$6dfdb1b0$b51ad284@Calypso>

 Hi
When using linear mixed model, I could test for the effect of the random
part of the model using a likelihood ratio test comparing two model with and
without the random part.

model.lmer=lmer(y~x+(1|r))
model.lm = lm(y~x)
anova(model.lmer,model.lm)

However, this does not work with a mixed model with binomial or poisson
distribution

> model.lmer = lmer(z~x+(1|r),family=binomial)
> model.glm =  glm(z~x,family=binomial)
> anova(model.lmer,model.glm)
 Erreur dans FUN(X[[1]], ...) : aucun slot de nom "call" pour cet objet de
la classe "glm"
	De plus : Warning message:
	arguments suppl?mentaires ignor?s in: logLik.glm(X[[2]], ...)

My question is how cold I test for a random effect in logistic mixed model
because the anova function does not work here?

Thanks

Julien Martin

-- 



13:36


From sergeyg at gmail.com  Fri Mar 30 17:18:40 2007
From: sergeyg at gmail.com (Sergey Goriatchev)
Date: Fri, 30 Mar 2007 17:18:40 +0200
Subject: [R] Using split() several times in a row?
Message-ID: <7cb007bd0703300818h742f7ef3wcee5f23a424b4546@mail.gmail.com>

Hi, fellow R users.

I have a question about sapply and split combination.

I have a big dataframe (40000 observations, 21 variables). First
variable (factor) is "date" and it is in format "8.29.97", that is, I
have monthly data. Second variable (also factor) has levels 1 to 6
(fractiles 1 to 5 and missing value with code 6). The other 19
variables are numeric.
For each month I have several hunder observations of 19 numeric and 1 factor.

I am normalizing the numeric variables by dividing val1 by val2, where:

val1: (for each month, for each numeric variable) difference between
mean of ith numeric variable in fractile 1, and mean of ith numeric
variable in fractile 5.

val2: (for each month, for each numeric variable) standard deviation
for ith numeric variable.

Basically, as far as I understand, I need to use split() function several times.
To calculate val1 I need to use split() twice - first to split by
month and then split by fractile. Is this even possible to do (since
after first application of split() I get a list)??

Is there a smart way to perform this normalization computation?

My knowledge of R is not so advanced, but I need to know an efficient
way to perform calculations of this kind.

Would really appreciate some help from experienced R users!

Regards,
S

-- 
Laziness is nothing more than the habit of resting before you get tired.
- Jules Renard (writer)

Experience is one thing you can't get for nothing.
- Oscar Wilde (writer)

When you are finished changing, you're finished.
- Benjamin Franklin (Diplomat)


From pburns at pburns.seanet.com  Fri Mar 30 17:25:34 2007
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 30 Mar 2007 16:25:34 +0100
Subject: [R] Inquiry
In-Reply-To: <7384.89517.qm@web32802.mail.mud.yahoo.com>
References: <7384.89517.qm@web32802.mail.mud.yahoo.com>
Message-ID: <460D2BEE.2040707@pburns.seanet.com>

My interpretation of the question (which of course may be
wrong) has an answer that is the opposite of 'round':

When a result is printed, it is rounded to a certain number of
digits (controlled by the 'digits' argument of 'print').  Just because
it is printed like that, doesn't mean the actual value is rounded.
Values are kept with as much precision as possible.  For double
precision data, there are roughly 15 decimal places available.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

John Kane wrote:

>I am not sure I understand your question but have a
>look at ?round and the signif command on that page.  
>
>--- Inmaculada L?pez Garc?a <milopez at ual.es> wrote:
>
>  
>
>>Good morning,
>>
>>I have a question about R, I would like to know how
>>it is possible not to
>>chop the result of an operation. How many decimals
>>it is possible to obtain?
>>
>>Thank you in advance,
>>I. L?pez
>>---------------------------------------------
>>Inmaculada L?pez Garc?a
>>Dpto. Estad?stica y Matem?tica Aplicada
>>Universidad de Almer?a
>>La Ca?ada de San Urbano, s/n
>>04120  Almer?a (SPAIN)
>>Tfno.:	 +34 950 01 57 75
>>Fax:    +34 950 01 51 67
>>e-mail: milopez at ual.es
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained,
>>reproducible code.
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>


From fjlozl at unileon.es  Fri Mar 30 17:45:40 2007
From: fjlozl at unileon.es (fjlozl at unileon.es)
Date: Fri, 30 Mar 2007 17:45:40 +0200 (CEST)
Subject: [R] Minimum valid number of observations for rpart
Message-ID: <10174.83.54.177.251.1175269540.squirrel@www.unileon.es>

Hi,
I wonder if anyone knows a study dealing with the minimum valid number of
observations when using CART?.
On top of that, when using RandomForest, is it possible to obtained a
interpretable tree model as the graphical output of the analysis, just
like in "rpart"?

Thanks a lot in advance

Javier Lozano
Universidad de Le?n
Spain


From roebuck at mdanderson.org  Fri Mar 30 17:53:06 2007
From: roebuck at mdanderson.org (Paul Roebuck)
Date: Fri, 30 Mar 2007 10:53:06 -0500 (CDT)
Subject: [R] Makeconf on Windows - where?
Message-ID: <Pine.OSF.4.58.0703301043590.229382@wotan.mdacc.tmc.edu>

Read in the Windows release notes for 2.4.0 that:

  There is a new file etc/Makeconf that provides an
  approximation to the Unix version and may help with
  src/Makefile's.

Yet when I look at the CRAN binary I have for 2.4.1patched,
there are five files in the $R_HOME/etc directory, but no
Makeconf. Is the file only available with custom installs?

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)


From rfrancois at mango-solutions.com  Fri Mar 30 14:45:29 2007
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Fri, 30 Mar 2007 13:45:29 +0100
Subject: [R] Wikibooks
In-Reply-To: <20070330111600.M1590@centroin.com.br>
References: <00d701c77260$604303e0$4d908980@gne.windows.gene.com>	<460CD812.6070901@sciviews.org>
	<20070330111600.M1590@centroin.com.br>
Message-ID: <460D0669.5020308@mango-solutions.com>

Alberto Monteiro wrote:
> Philippe Grosjean wrote:
>   
>> As other have pointed out, the main reason for the lack of success 
>> of the R Wiki is that the mailing lists, particularly R-Help, are 
>> sooo successful. However, I continue to consider that the mailing 
>> list is suboptimal in two cases: (1) when text is not enough to 
>> express the idea, and (2) for frequent questions that would 
>> certainly deserve a good compilation on a wiki page and a 
>> redirection to it everytime the question is asked.
>>
>>     
> I think there's one case where the mailing list is non-optimal:
> finding examples. This is where a wiki would be great.
>
> Say I don't know (and I can't understand the help) how to
> use the rnorm function. If I do RSiteSearch("rnorm"), I
> will get too much useless information. OTOH, an ideal wikipedia
> would have a page http://www.r-wiki.org/rnorm, where I could
> find examples, learn the theory, browse the source code, and 
> have links to similar functions. OK, maybe that's too much, I
> would be happy just to have some examples :-)
>   
Hi,

Do you mean something like (it fullfills basically all your requirements) :

R> rnorm     # get the code
R> ?rnorm   # get the help page

The wiki already has a similar thing, for example for rnorm, you can go to:
http://wiki.r-project.org/rwiki/doku.php?id=rdoc:stats:Normal

There has been (recently and less recently) some discussions on the
r-sig-wiki list about why sometimes you get ~~RDOC~~ instead of the
documentation page, it is still a work in progress.

The only tricky bit is how do I know that I have to go to stats:normal,
well you can ask that to R, for example using that small function :

wikiHelp <- function( ... , sarcasm = TRUE ){
    if(  length(hp <- help(...) ) > 0 ){
      hp <- tail( strsplit(hp[1], "/")[[1]], 3 )
      wikiPage <-
sprintf("http://wiki.r-project.org/rwiki/doku.php?id=rdoc:%s:%s",
hp[1],  hp[3])
       cat("the following wiki page will be displayed in your browser:",
             wikiPage,
             ">>>     Please feel free to add information if you have
some, " , sep = "\n")
      if( sarcasm) cat( ">>>     except if you are an evil person\n")
       browseURL(wikiPage)
    } else print( hp )
  }

R> wikiHelp( rnorm )
R> wikiHelp( tkWidgets )
R> wikiHelp( seq )
R> wikiHelp( fewqfrwasaqwetgqwtr) # no such page exists



> Also, RSiteSearching is dangerous, because if someone replies
> in an ignorant or malicous way (let's be creative: someone asks
> "how can I open the file CONFIG.SYS", and an evil person replies 
> with file.remove("CONFIG.SYS")), then this wrong answer may
> be accessed by newbies. A wikipedia _may_ have wrong answers,
> but these are (hopefully) ephemeral.
>   
Are there many people willing to just blindly copy anything and expect
the good result to be returned ?
I don't think there are many evil person around
> BTW, is it too hard to include the wiki in RSiteSearch?
>   

The wiki has its own search engine already, so you can go there and use
it. I guess you can search for "search" there and get info on how to
search .
If you are using a Gecko based browser (firefox, flock, ...) you might
want to check that extension that would search the wiki pages for you as
well as the results from the R site search:
http://addictedtor.free.fr/rsitesearch/

HTH,

Romain

> Alberto Monteiro
-- 
Mango Solutions
data analysis that delivers

Tel:  +44(0) 1249 467 467
Fax:  +44(0) 1249 467 468
Mob:  +44(0) 7813 526 123


From h.wickham at gmail.com  Fri Mar 30 15:06:04 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 30 Mar 2007 08:06:04 -0500
Subject: [R] Wikibooks
In-Reply-To: <20070330111600.M1590@centroin.com.br>
References: <00d701c77260$604303e0$4d908980@gne.windows.gene.com>
	<460CD812.6070901@sciviews.org> <20070330111600.M1590@centroin.com.br>
Message-ID: <f8e6ff050703300606v1919109fs37439f4e79e53770@mail.gmail.com>

On 3/30/07, Alberto Monteiro <albmont at centroin.com.br> wrote:
> Philippe Grosjean wrote:
> >
> > As other have pointed out, the main reason for the lack of success
> > of the R Wiki is that the mailing lists, particularly R-Help, are
> > sooo successful. However, I continue to consider that the mailing
> > list is suboptimal in two cases: (1) when text is not enough to
> > express the idea, and (2) for frequent questions that would
> > certainly deserve a good compilation on a wiki page and a
> > redirection to it everytime the question is asked.
> >
> I think there's one case where the mailing list is non-optimal:
> finding examples. This is where a wiki would be great.
>
> Say I don't know (and I can't understand the help) how to
> use the rnorm function. If I do RSiteSearch("rnorm"), I
> will get too much useless information. OTOH, an ideal wikipedia
> would have a page http://www.r-wiki.org/rnorm, where I could
> find examples, learn the theory, browse the source code, and
> have links to similar functions. OK, maybe that's too much, I
> would be happy just to have some examples :-)

Good documentation is hard to write, usually much harder than writing
the code it documents.  I think coming up with good documentation for
a package is on the order of difficulty of a large refereed paper (or
a book!), and yet you never get any recognition for it, only
complaints when it is inadequate.  Journals like JSS are an attempt to
allow software to recieve academic credit, but only provide
recognition for a specific form of documentation, the expanded
tutorial.  http://tinyurl.com/l7ufz has a good description of the
multiple types of documentation that are needed.

Many of the functions in R can not be properly used without the
appropriate statistical background and it is impossible to provide
this in the documentation. Many R functions are very well documented,
by experts in the field, in conjunction with a book that provides the
statistical background.  Unfortunately all the best things in life are
NOT free, unless you happen to be attached to a good academic library.

The r wiki is a technical solution to a sociological problem.

Hadley


From h.wickham at gmail.com  Fri Mar 30 15:11:35 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 30 Mar 2007 08:11:35 -0500
Subject: [R] Wikibooks
In-Reply-To: <460D0490.8020204@stats.uwo.ca>
References: <00d701c77260$604303e0$4d908980@gne.windows.gene.com>
	<460CD812.6070901@sciviews.org> <460CED2A.50708@stats.uwo.ca>
	<20070330112855.M54539@centroin.com.br>
	<460D0490.8020204@stats.uwo.ca>
Message-ID: <f8e6ff050703300611r3b20f83eud86e17a13e0855f4@mail.gmail.com>

> If entering a new page is really the way to ask a question, then you
> should write this on the front page, and as a possible way to contribute
> on the getting-started page.  It would also be a good idea to tell
> people like me how to find those questions. ("Recent Edits" seems a
> little too broad, with too little in the way of subject matter in the
> comments, but maybe that's just because nobody's asking questions yet.)

I don't think it's a good idea to use the wiki as a way to ask
questions.  We already have a great forum to ask questions - this
mailing list.  Creating a new place to ask questions potentially
fragments the community of people available to answer questions.

I think the wiki would be more appropriate as a way to record
collective best practices, but this relies on it being easy to find
them again.

> And it would be a good idea to seed the wiki with a lot of questions,
> just to get some activity going.

That's good for people who want to ask questions, but people who want
their questions answered are presented with many blank pages
(http://www.wikipatterns.com/display/wikipatterns/Empty+Pages), and
will be discouraged.

> And maybe set up a page for pointers to questions that are languishing
> unanswered?  I think the key is to make it easy to ask a question and
> easy to answer one, so don't put too much bureaucracy into the process.

I think it's useful to consider more the purpose of the wiki - what
makes it different to the mailing list? to the website? to the
existing documentation?  How can the strengths of the wiki form be
used to our advantage?

Hadley


From murdoch at stats.uwo.ca  Fri Mar 30 18:22:13 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 30 Mar 2007 12:22:13 -0400
Subject: [R] Makeconf on Windows - where?
In-Reply-To: <Pine.OSF.4.58.0703301043590.229382@wotan.mdacc.tmc.edu>
References: <Pine.OSF.4.58.0703301043590.229382@wotan.mdacc.tmc.edu>
Message-ID: <460D3935.40205@stats.uwo.ca>

On 3/30/2007 11:53 AM, Paul Roebuck wrote:
> Read in the Windows release notes for 2.4.0 that:
> 
>   There is a new file etc/Makeconf that provides an
>   approximation to the Unix version and may help with
>   src/Makefile's.
> 
> Yet when I look at the CRAN binary I have for 2.4.1patched,
> there are five files in the $R_HOME/etc directory, but no
> Makeconf. Is the file only available with custom installs?

That may be an oversight that it never got added to the build script for 
the installer.  I'll take a look and see if it would be useful with a 
binary install.

Duncan Murdoch


From pengp at queensu.ca  Fri Mar 30 18:50:06 2007
From: pengp at queensu.ca (pengp at queensu.ca)
Date: Fri, 30 Mar 2007 12:50:06 -0400
Subject: [R] Using functions in LAPACK in a C program
In-Reply-To: <Pine.LNX.4.64.0703300644330.30405@gannet.stats.ox.ac.uk>
References: <221244.52205.qm@web58404.mail.re3.yahoo.com>
	<Pine.LNX.4.64.0703300644330.30405@gannet.stats.ox.ac.uk>
Message-ID: <fc8cfcf84652.460d077e@queensu.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070330/ad053264/attachment.pl 

From ripley at stats.ox.ac.uk  Fri Mar 30 19:02:08 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 30 Mar 2007 18:02:08 +0100 (BST)
Subject: [R] Minimum valid number of observations for rpart
In-Reply-To: <10174.83.54.177.251.1175269540.squirrel@www.unileon.es>
References: <10174.83.54.177.251.1175269540.squirrel@www.unileon.es>
Message-ID: <Pine.LNX.4.64.0703301759110.15695@gannet.stats.ox.ac.uk>

On Fri, 30 Mar 2007, fjlozl at unileon.es wrote:

> Hi,
> I wonder if anyone knows a study dealing with the minimum valid number of
> observations when using CART?.

I have no idea what you mean by 'valid' here.
Could you answer the question for logistic regression to indicate to us 
what form of answer you are expecting?

> On top of that, when using RandomForest, is it possible to obtained a
> interpretable tree model as the graphical output of the analysis, just
> like in "rpart"?

By definition randomForest (sic) does not produce a tree but a forest.

> Thanks a lot in advance
>
> Javier Lozano
> Universidad de Le?n
> Spain


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Fri Mar 30 19:09:23 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 30 Mar 2007 18:09:23 +0100 (BST)
Subject: [R] Using functions in LAPACK in a C program
In-Reply-To: <fc8cfcf84652.460d077e@queensu.ca>
References: <221244.52205.qm@web58404.mail.re3.yahoo.com>
	<Pine.LNX.4.64.0703300644330.30405@gannet.stats.ox.ac.uk>
	<fc8cfcf84652.460d077e@queensu.ca>
Message-ID: <Pine.LNX.4.64.0703301804550.15695@gannet.stats.ox.ac.uk>

I think all these points are answered in the README.packages for R 2.5.0 
alpha (and probably for 2.4.1 too).

You can use pexports to create Rlapack.exp, for example.

On Fri, 30 Mar 2007, pengp at queensu.ca wrote:

> Many thanks to Brian for his very useful information and help. A quick 
> look at mgcv package already gives me some directions to try. I remember 
> that I tried to use Rlapack.dll instead of R.dll without success, 
> perhaps because I don't know how to build an import library for 
> Rlapack.dll and link against it (README.packages shows how to do this 
> for R.dll using R.exp, but I could not find Rlapack.exp). I will try to 
> see if I can figure out this part using VC++6.0.
>
> Yes, I am still using R-2.2.1. I did try to upgrade it to R-2.4.1. 
> However, the newer versions cause some troubles for my C programs. They 
> produce error messages when my DLL are loaded into R. The problem 
> relates to function calls such as isfinite. I haven't figured out why my 
> C programs have this problem with newer versions of R but not with 
> version 2.2.1. I remember there was a post on this issue in this list, 
> but I did not see any solution. I hope to find a clue about it too so 
> that I can keep the pace of R development.
>
>
>
>
>
> Paul.
>
>
>
>
>
>
>
>
> ----- Original Message -----
>
>
> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>
>
> Date: Friday, March 30, 2007 1:56 am
>
>
> Subject: Re: [R] Using functions in LAPACK in a C program
>
>
> To: Paul August <paulaugust2003 at yahoo.com>
>
>
> Cc: r-help at stat.math.ethz.ch
>
>
>
>
>
>> On Thu, 29 Mar 2007, Paul August wrote:
>
>
>>
>
>
>>> Hi,
>
>
>>>
>
>
>>> I wonder where I can find an example of using a function in
>
>
>> LAPACK
>
>
>>> library in a user's own C code.
>
>
>>
>
>
>> In about 20 R packages, e.g. the recommended package mgcv.
>
>
>>
>
>
>>> I wrote a C program which will be
>
>
>>> compiled and linked to produce a DLL file and then loaded into
>
>
>> R. I hope
>
>
>>> to use a function from LAPACK library, for example, dgesdd, in
>
>
>> the
>
>
>>> program. Following R manual, I call the function by
>
>
>> F77_CALL(dgesdd) in
>
>
>>> the program. The program can be compiled without problems.
>
>
>> However, when
>
>
>>> it is linked to produce a DLL file, I get an error message
>
>
>>>
>
>
>>> ? Test.obj : error LNK2001: unresolved external symbol _dgesdd_
>
>
>>> ? Test.dll : fatal error LNK1120: 1 unresolved externals
>
>
>>>
>
>
>>> I use VC++6.0 and the command of linking is something like this
>
>
>>>
>
>
>>> ? link.exe Rdll.lib /nologo /dll /out:Test.dll
>
>
>> /libpath:C:\R\R-2.2.1\src\gnuwin32 Test.obj
>
>
>>>
>
>
>>> Apparently, the linker cannot resolve dgesdd from Rdll.lib. If
>
>
>> anyone
>
>
>>> knows what I missed here or any example that shows how this
>
>
>> can be done
>
>
>>> properly, please let me know. Thanks a lot.
>
>
>>
>
>
>> It is in Rlapack.dll not R.dll.
>
>
>>
>
>
>> The linking information is in 'Writing R Extensions' for those
>
>
>> using the
>
>
>> recommended compilation system (search for LAPACK_LIBS).
>
>
>>
>
>
>> You will need to build an import library for Rlapack.dll and
>
>
>> link against
>
>
>> that.
>
>
>>
>
>
>> And BTW you seem to be using R 2.2.1: please update as we can
>
>
>> only offer
>
>
>> accurate advice on recent systems.
>
>
>>
>
>
>> --
>
>
>> Brian D.
>
>
>> Ripley,????????????????? ripley at stats.ox.ac.uk
>
>
>> Professor of Applied Statistics,?
>
>
>> http://www.stats.ox.ac.uk/~ripley/University of
>
>
>> Oxford,???????????? Tel:? +44 1865 272861 (self)
>
>
>> 1 South Parks
>
>
>> Road,???????????????????? +44 1865 272866 (PA)
>
>
>> Oxford OX1 3TG,
>
>
>> UK??????????????? Fax:? +44 1865 272595
>
>
>>
>
>
>> ______________________________________________
>
>
>> R-help at stat.math.ethz.ch mailing list
>
>
>> https://stat.ethz.ch/mailman/listinfo/r-help
>
>
>> PLEASE do read the posting guide http://www.R-
>
>
>> project.org/posting-guide.html
>
>
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>>
>
> -----------------------------------------------
> Dr. Paul Y. Peng
> Associate Professor of Biostatistics
> Department of Community Health and Epidemiology
>   and Department of Mathematics and Statistics
> Queen's University, Kingston, ON, K7L 3N6
>
> Phone: 613-533-6000 Ext 78525
> Email: pengp at queensu.ca
> Fax: 613-533-6794
> -----------------------------------------------
>
> 	[[alternative HTML version deleted]]
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From cberry at tajo.ucsd.edu  Fri Mar 30 20:06:05 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Fri, 30 Mar 2007 11:06:05 -0700
Subject: [R] Model comparison
In-Reply-To: <EA09C4B2B0F16E44B8F3311629493C0D02ED4EA8@DJFPOST01.djf.agrsci.dk>
References: <EA09C4B2B0F16E44B8F3311629493C0D02ED4EA8@DJFPOST01.djf.agrsci.dk>
Message-ID: <Pine.LNX.4.64.0703301059190.13514@tajo.ucsd.edu>

On Fri, 30 Mar 2007, Jo?o Fadista wrote:

> Dear all,
>
> I would like to know if I can compare by a significance test 2 models 
> with different kind of parameters. Perhaps I am wrong but I think that 
> we can only compare 2 models if one is a sub model of the other.
>
>

The literature you seek is on 'non-nested' models. Here is a start:

Bradley Efron
Comparing non-nested linear models
J. of the Amer. Stat'l. Assn.
79:791-803,1984


Google is your friend. Try 'non-nested test' and 'non-nested model'
and you will get many hits.


[snip]
Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://biostat.ucsd.edu/~cberry/         La Jolla, San Diego 92093-0901


From dfarrar at newrvana.com  Fri Mar 30 20:36:40 2007
From: dfarrar at newrvana.com (David Farrar)
Date: Fri, 30 Mar 2007 11:36:40 -0700 (PDT)
Subject: [R] Minimum valid number of observations for rpart
In-Reply-To: <10174.83.54.177.251.1175269540.squirrel@www.unileon.es>
Message-ID: <20070330183640.55826.qmail@web813.biz.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070330/fe9dca66/attachment.pl 

From gsg134 at psu.edu  Fri Mar 30 20:39:54 2007
From: gsg134 at psu.edu (Gaurav Ghosh)
Date: Fri, 30 Mar 2007 14:39:54 -0400
Subject: [R] Trouble installing package 'sp' in R 2.4.1
Message-ID: <1175279992l.1826998l.0l@psu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070330/508bcf87/attachment.pl 

From murdoch at stats.uwo.ca  Fri Mar 30 20:59:29 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 30 Mar 2007 14:59:29 -0400
Subject: [R] Trouble installing package 'sp' in R 2.4.1
In-Reply-To: <1175279992l.1826998l.0l@psu.edu>
References: <1175279992l.1826998l.0l@psu.edu>
Message-ID: <460D5E11.20309@stats.uwo.ca>

On 3/30/2007 2:39 PM, Gaurav Ghosh wrote:
> Hi all,
> 
> I just installed R 2.4.1 and tried to install the package 'sp' working down from the 'Packages' menu in the R GUI.  The package would not install and I got the following message:
> 
> &quot;trying URL 'http://cran.us.r-project.org/bin/windows/contrib/2.4/sp_0.9-10.zip'
> Error in download.file(url, destfile, method, mode = &quot;wb&quot;) : 
>         cannot open URL 'http://cran.us.r-project.org/bin/windows/contrib/2.4/sp_0.9-10.zip'
> In addition: Warning message:
> cannot open: HTTP status was '404 Not Found' 
> Warning in download.packages(pkgs, destdir = tmpd, available = available,  : 
>          download of package 'sp' failed&quot;
> 
> I tried to find the installation file &quot;sp_0.9-10.zip&quot; through the CRAN website, but was unable to do so.  So I was wondering if there was some other way to get this package I need.  Or perhaps  there might be some problem with the website itself.


You didn't say which mirror you were using, but it looks like a problem 
there.  I just tried from one of the Canadian mirrors and there was no 
problem finding the package.

Duncan Murdoch


From deepayan.sarkar at gmail.com  Fri Mar 30 21:17:57 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 30 Mar 2007 12:17:57 -0700
Subject: [R] Wikibooks
In-Reply-To: <loom.20070330T084949-731@post.gmane.org>
References: <20070329193059.M91171@centroin.com.br>
	<loom.20070329T234918-210@post.gmane.org>
	<loom.20070330T084949-731@post.gmane.org>
Message-ID: <eb555e660703301217x2c5b9b66ha4a471ea32d5d5e@mail.gmail.com>

On 3/30/07, Dieter Menne <dieter.menne at menne-biomed.de> wrote:
> Ben Bolker <bolker <at> zoo.ufl.edu> writes:
>
> >   Well, we do have an R wiki -- http://wiki.r-project.org/rwiki/doku.php --
> > although it is not as active as I'd like.  (We got stuck halfway through
> > porting Paul Johnson's "R Tips" to it ...)   Please contribute!
>
> I once tried:
>
> http://wiki.r-project.org/rwiki/doku.php?id=guides:lmer-tests

I was just looking at this page, and it makes me curious: what gives
anyone the right to take someone else's mailing list post and include
that in a Wiki? I'm not saying that anyone involved would object, but
there is the technicality of licensing: the wiki page claims to be
under a certain creative commons license; was permission obtained from
all the contributors? Does posting to r-help automatically constitute
such permission?  More importantly, since the wiki contents can be
edited, where is the guarantee that some text attributed to someone is
really what someone said?

One solution would be to link to the posts rather than repeating them,
perhaps with a one line summary of what information is contained in
the post. This could then form the basis of more comments and links
over time. In any case, the wiki needs to provide some guidance on
this.

-Deepayan


From albmont at centroin.com.br  Fri Mar 30 21:28:14 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 30 Mar 2007 17:28:14 -0200
Subject: [R] RWiki, tcltk and plot
Message-ID: <20070330191833.M21550@centroin.com.br>

I think I - almost - got the knack for GUI programming using the
tcltk library. Maybe I will update the RWiki with this:

#
#################################################
#

library(tcltk)

#
# Create some matrix - nothing about tcltk here
#
matrix <- cbind(rnorm(100), rpois(100, lambda=10), 
  runif(100), rt(100, df=2), rt(100, df=4))

colnames(matrix) <- c("Normal", "Poisson (lambda=10)", 
  "U(0,1)", "Student t (nu=2)", "Student t (nu=4)")

#
# Now comes the interesting part
#
tt <- tktoplevel()
tkwm.title(tt, "A bunch of distributions")
dist.widget <- NULL
plot.widget <- NULL
for (i in 1:length(colnames(matrix))) {
  dist.widget[[i]] <- tklabel(tt, text=(colnames(matrix))[i])
  plot.widget[[i]] <- local({
    n <- i
    tkbutton(tt, text="PLOT", command=function() plot(matrix[,n]))
  })
  tkgrid(dist.widget[[i]], row=i-1)
  tkgrid(plot.widget[[i]], row=i-1, column=1)
}

#
# Game over - click and watch !!!
#
###############################################
#

My question: is there any way to integrate the plot part into
a tcltk window?

Alberto Monteiro


From albmont at centroin.com.br  Fri Mar 30 21:34:04 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 30 Mar 2007 17:34:04 -0200
Subject: [R] Wikibooks
In-Reply-To: <eb555e660703301217x2c5b9b66ha4a471ea32d5d5e@mail.gmail.com>
References: <20070329193059.M91171@centroin.com.br>
	<loom.20070329T234918-210@post.gmane.org>
	<loom.20070330T084949-731@post.gmane.org>
	<eb555e660703301217x2c5b9b66ha4a471ea32d5d5e@mail.gmail.com>
Message-ID: <20070330193122.M49424@centroin.com.br>

Deepayan Sarkar wrote:
> 
> I was just looking at this page, and it makes me curious: what gives
> anyone the right to take someone else's mailing list post and include
> that in a Wiki? 
>
Thinks there were posted to public mailing lists are freely
copied and distributed. It's a scary thought; I may have posted
things in 10 or 12 years ago that might cause me problems today,
but I was pretty aware that I was posting to the whole world.

Alberto Monteiro


From Roger.Bivand at nhh.no  Fri Mar 30 21:37:55 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 30 Mar 2007 21:37:55 +0200 (CEST)
Subject: [R] Trouble installing package 'sp' in R 2.4.1
In-Reply-To: <460D5E11.20309@stats.uwo.ca>
Message-ID: <Pine.LNX.4.44.0703302131510.1819-100000@reclus.nhh.no>

On Fri, 30 Mar 2007, Duncan Murdoch wrote:

> On 3/30/2007 2:39 PM, Gaurav Ghosh wrote:
> > Hi all,
> > 
> > I just installed R 2.4.1 and tried to install the package 'sp' working down from the 'Packages' menu in the R GUI.  The package would not install and I got the following message:
> > 
> > &quot;trying URL 'http://cran.us.r-project.org/bin/windows/contrib/2.4/sp_0.9-10.zip'
> > Error in download.file(url, destfile, method, mode = &quot;wb&quot;) : 
> >         cannot open URL 'http://cran.us.r-project.org/bin/windows/contrib/2.4/sp_0.9-10.zip'
> > In addition: Warning message:
> > cannot open: HTTP status was '404 Not Found' 
> > Warning in download.packages(pkgs, destdir = tmpd, available = available,  : 
> >          download of package 'sp' failed&quot;
> > 
> > I tried to find the installation file &quot;sp_0.9-10.zip&quot; through the CRAN website, but was unable to do so.  So I was wondering if there was some other way to get this package I need.  Or perhaps  there might be some problem with the website itself.
> 
> 
> You didn't say which mirror you were using, but it looks like a problem 
> there.  I just tried from one of the Canadian mirrors and there was no 
> problem finding the package.

>From HTTP, the mirror mentioned inline http://cran.us.r-project.org/ has 
problems, is well behind on versions in the project pages, and

http://cran.us.r-project.org/bin/windows/contrib/2.4/

has no sp package at all. The last check summary is dated 2007-03-16.

Please try another mirror; if anyone could find out what is the matter 
with the mirror, and get it corrected, that would be welcome.

> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From sarah.goslee at gmail.com  Fri Mar 30 21:49:44 2007
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Fri, 30 Mar 2007 15:49:44 -0400
Subject: [R] Wikibooks
In-Reply-To: <20070330193122.M49424@centroin.com.br>
References: <20070329193059.M91171@centroin.com.br>
	<loom.20070329T234918-210@post.gmane.org>
	<loom.20070330T084949-731@post.gmane.org>
	<eb555e660703301217x2c5b9b66ha4a471ea32d5d5e@mail.gmail.com>
	<20070330193122.M49424@centroin.com.br>
Message-ID: <efb536d50703301249n55b101e2te3de2c902d21f6a7@mail.gmail.com>

On 3/30/07, Alberto Monteiro <albmont at centroin.com.br> wrote:
> Deepayan Sarkar wrote:
> >
> > I was just looking at this page, and it makes me curious: what gives
> > anyone the right to take someone else's mailing list post and include
> > that in a Wiki?
> >
> Thinks there were posted to public mailing lists are freely
> copied and distributed. It's a scary thought; I may have posted
> things in 10 or 12 years ago that might cause me problems today,
> but I was pretty aware that I was posting to the whole world.

It's not that simple. Dealing with international contributors it's even worse.
Under US law (the only one I'm familiar with), the author of a mailing list
post or any other written work _automatically holds copyright_ to that
post (although not to the ideas contained therein, but to that particular
description of the ideas). (Of course, if the ideas are original to the author,
it's good form to acknowledge that regardless of whether the exact words
are used).

So, in the US, nobody has the right to take one person's words and put
them in another form. The mailing list archive is one thing, but putting
material from that archive into a wiki (rather than linking to it) requires
the author's permission, at least technically.

I'm by no means an expert on copyright, but this is something that
comes up periodically on many email lists.

Sarah
-- 
Sarah Goslee
http://www.functionaldiversity.org


From edd at debian.org  Fri Mar 30 21:53:40 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 30 Mar 2007 14:53:40 -0500
Subject: [R] RWiki, tcltk and plot
In-Reply-To: <20070330191833.M21550@centroin.com.br>
References: <20070330191833.M21550@centroin.com.br>
Message-ID: <20070330195340.GA28210@eddelbuettel.com>

On Fri, Mar 30, 2007 at 05:28:14PM -0200, Alberto Monteiro wrote:
> My question: is there any way to integrate the plot part into
> a tcltk window?

Are you aware of the tkrplot package on CRAN ?

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From Greg.Snow at intermountainmail.org  Fri Mar 30 21:54:56 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 30 Mar 2007 13:54:56 -0600
Subject: [R] RWiki, tcltk and plot
In-Reply-To: <20070330191833.M21550@centroin.com.br>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB8F6399@LP-EXCHVS07.CO.IHC.COM>

Look at the tkrplot package.

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Alberto Monteiro
> Sent: Friday, March 30, 2007 1:28 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] RWiki, tcltk and plot
> 
> I think I - almost - got the knack for GUI programming using 
> the tcltk library. Maybe I will update the RWiki with this:
> 
> #
> #################################################
> #
> 
> library(tcltk)
> 
> #
> # Create some matrix - nothing about tcltk here # matrix <- 
> cbind(rnorm(100), rpois(100, lambda=10),
>   runif(100), rt(100, df=2), rt(100, df=4))
> 
> colnames(matrix) <- c("Normal", "Poisson (lambda=10)",
>   "U(0,1)", "Student t (nu=2)", "Student t (nu=4)")
> 
> #
> # Now comes the interesting part
> #
> tt <- tktoplevel()
> tkwm.title(tt, "A bunch of distributions") dist.widget <- 
> NULL plot.widget <- NULL for (i in 1:length(colnames(matrix))) {
>   dist.widget[[i]] <- tklabel(tt, text=(colnames(matrix))[i])
>   plot.widget[[i]] <- local({
>     n <- i
>     tkbutton(tt, text="PLOT", command=function() plot(matrix[,n]))
>   })
>   tkgrid(dist.widget[[i]], row=i-1)
>   tkgrid(plot.widget[[i]], row=i-1, column=1) }
> 
> #
> # Game over - click and watch !!!
> #
> ###############################################
> #
> 
> My question: is there any way to integrate the plot part into 
> a tcltk window?
> 
> Alberto Monteiro
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From h.wickham at gmail.com  Fri Mar 30 21:55:38 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 30 Mar 2007 14:55:38 -0500
Subject: [R] Wikibooks
In-Reply-To: <efb536d50703301249n55b101e2te3de2c902d21f6a7@mail.gmail.com>
References: <20070329193059.M91171@centroin.com.br>
	<loom.20070329T234918-210@post.gmane.org>
	<loom.20070330T084949-731@post.gmane.org>
	<eb555e660703301217x2c5b9b66ha4a471ea32d5d5e@mail.gmail.com>
	<20070330193122.M49424@centroin.com.br>
	<efb536d50703301249n55b101e2te3de2c902d21f6a7@mail.gmail.com>
Message-ID: <f8e6ff050703301255x7c748901o7efab7b9a320f73c@mail.gmail.com>

> Under US law (the only one I'm familiar with), the author of a mailing list
> post or any other written work _automatically holds copyright_ to that
> post (although not to the ideas contained therein, but to that particular
> description of the ideas).

That's true in almost any country - see the Berne convention.

Hadley


From deepayan.sarkar at gmail.com  Fri Mar 30 22:02:19 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 30 Mar 2007 13:02:19 -0700
Subject: [R] Wikibooks
In-Reply-To: <efb536d50703301249n55b101e2te3de2c902d21f6a7@mail.gmail.com>
References: <20070329193059.M91171@centroin.com.br>
	<loom.20070329T234918-210@post.gmane.org>
	<loom.20070330T084949-731@post.gmane.org>
	<eb555e660703301217x2c5b9b66ha4a471ea32d5d5e@mail.gmail.com>
	<20070330193122.M49424@centroin.com.br>
	<efb536d50703301249n55b101e2te3de2c902d21f6a7@mail.gmail.com>
Message-ID: <eb555e660703301302j47a6e51cv5bb11bb2776992a7@mail.gmail.com>

On 3/30/07, Sarah Goslee <sarah.goslee at gmail.com> wrote:
> On 3/30/07, Alberto Monteiro <albmont at centroin.com.br> wrote:
> > Deepayan Sarkar wrote:
> > >
> > > I was just looking at this page, and it makes me curious: what gives
> > > anyone the right to take someone else's mailing list post and include
> > > that in a Wiki?
> > >
> > Thinks there were posted to public mailing lists are freely
> > copied and distributed. It's a scary thought; I may have posted
> > things in 10 or 12 years ago that might cause me problems today,
> > but I was pretty aware that I was posting to the whole world.

There's a difference between public archiving and copying.

> It's not that simple. Dealing with international contributors it's even worse.
> Under US law (the only one I'm familiar with), the author of a mailing list
> post or any other written work _automatically holds copyright_ to that
> post (although not to the ideas contained therein, but to that particular
> description of the ideas). (Of course, if the ideas are original to the author,
> it's good form to acknowledge that regardless of whether the exact words
> are used).

I believe this is true for all countries that are signatory to the
Berne convention (which is pretty much all countries [1]). The US in
fact was one of the later ones to get into it, before which you had to
explicitly copyright things if you wanted copyright.

-Deepayan

[1] http://upload.wikimedia.org/wikipedia/commons/6/6c/Berne_Convention.png


From lind1199 at umn.edu  Fri Mar 30 22:08:10 2007
From: lind1199 at umn.edu (Gregg Lind)
Date: Fri, 30 Mar 2007 15:08:10 -0500
Subject: [R] zip.file.extract on Windows
Message-ID: <460D6E2A.8090007@umn.edu>

Hello R-help!

I just wanted to share a tip might help others of us stuck on R.

The documentation for zip.file.extract is rather scant, and a few 
examples would.  

When using zip.file.extract, the first argument should be the full (or 
relative) path and the second the name of the zip file, it seems. 

While this is counterintuitive to me, that it should get the part from 
the first argument, and apply that path to the second, it seems to how 
this works.

This seems to me to be the wrong approach, since it won't handle folders 
inside the zipfile correctly. 

Am I missing something?  Should I just use "unz" instead?

Thanks,

Gregg L.


From albmont at centroin.com.br  Fri Mar 30 22:10:52 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 30 Mar 2007 18:10:52 -0200
Subject: [R] RWiki, tcltk and plot
In-Reply-To: <20070330195340.GA28210@eddelbuettel.com>
References: <20070330191833.M21550@centroin.com.br>
	<20070330195340.GA28210@eddelbuettel.com>
Message-ID: <20070330201001.M38030@centroin.com.br>

Dirk Eddelbuettel wrote:
>
>> My question: is there any way to integrate the plot part into
>> a tcltk window?
> 
> Are you aware of the tkrplot package on CRAN ?
> 
No.

Alberto Monteiro


From ramasamy at cancer.org.uk  Fri Mar 30 22:33:11 2007
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 30 Mar 2007 21:33:11 +0100
Subject: [R] Wikibooks
In-Reply-To: <eb555e660703301302j47a6e51cv5bb11bb2776992a7@mail.gmail.com>
References: <20070329193059.M91171@centroin.com.br>	<loom.20070329T234918-210@post.gmane.org>	<loom.20070330T084949-731@post.gmane.org>	<eb555e660703301217x2c5b9b66ha4a471ea32d5d5e@mail.gmail.com>	<20070330193122.M49424@centroin.com.br>	<efb536d50703301249n55b101e2te3de2c902d21f6a7@mail.gmail.com>
	<eb555e660703301302j47a6e51cv5bb11bb2776992a7@mail.gmail.com>
Message-ID: <460D7407.1070405@cancer.org.uk>

On a related note, one might be interested in checking out citizendium 
which is spin off wikipedia but 1) has more stringent identity 
verification and 2) uses a two-tier system of editors and authors. See 
http://www.citizendium.org/cfa.html.



Deepayan Sarkar wrote:
> On 3/30/07, Sarah Goslee <sarah.goslee at gmail.com> wrote:
>> On 3/30/07, Alberto Monteiro <albmont at centroin.com.br> wrote:
>>> Deepayan Sarkar wrote:
>>>> I was just looking at this page, and it makes me curious: what gives
>>>> anyone the right to take someone else's mailing list post and include
>>>> that in a Wiki?
>>>>
>>> Thinks there were posted to public mailing lists are freely
>>> copied and distributed. It's a scary thought; I may have posted
>>> things in 10 or 12 years ago that might cause me problems today,
>>> but I was pretty aware that I was posting to the whole world.
> 
> There's a difference between public archiving and copying.
> 
>> It's not that simple. Dealing with international contributors it's even worse.
>> Under US law (the only one I'm familiar with), the author of a mailing list
>> post or any other written work _automatically holds copyright_ to that
>> post (although not to the ideas contained therein, but to that particular
>> description of the ideas). (Of course, if the ideas are original to the author,
>> it's good form to acknowledge that regardless of whether the exact words
>> are used).
> 
> I believe this is true for all countries that are signatory to the
> Berne convention (which is pretty much all countries [1]). The US in
> fact was one of the later ones to get into it, before which you had to
> explicitly copyright things if you wanted copyright.
> 
> -Deepayan
> 
> [1] http://upload.wikimedia.org/wikipedia/commons/6/6c/Berne_Convention.png
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
>


From p.dalgaard at biostat.ku.dk  Fri Mar 30 23:05:13 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 30 Mar 2007 23:05:13 +0200
Subject: [R] Wikibooks
In-Reply-To: <eb555e660703301302j47a6e51cv5bb11bb2776992a7@mail.gmail.com>
References: <20070329193059.M91171@centroin.com.br>	<loom.20070329T234918-210@post.gmane.org>	<loom.20070330T084949-731@post.gmane.org>	<eb555e660703301217x2c5b9b66ha4a471ea32d5d5e@mail.gmail.com>	<20070330193122.M49424@centroin.com.br>	<efb536d50703301249n55b101e2te3de2c902d21f6a7@mail.gmail.com>
	<eb555e660703301302j47a6e51cv5bb11bb2776992a7@mail.gmail.com>
Message-ID: <460D7B89.4050103@biostat.ku.dk>

Deepayan Sarkar wrote:
> On 3/30/07, Sarah Goslee <sarah.goslee at gmail.com> wrote:
>   
>> On 3/30/07, Alberto Monteiro <albmont at centroin.com.br> wrote:
>>     
>>> Deepayan Sarkar wrote:
>>>       
>>>> I was just looking at this page, and it makes me curious: what gives
>>>> anyone the right to take someone else's mailing list post and include
>>>> that in a Wiki?
>>>>
>>>>         
>>> Thinks there were posted to public mailing lists are freely
>>> copied and distributed. It's a scary thought; I may have posted
>>> things in 10 or 12 years ago that might cause me problems today,
>>> but I was pretty aware that I was posting to the whole world.
>>>       
>
> There's a difference between public archiving and copying.
>
>   
>> It's not that simple. Dealing with international contributors it's even worse.
>> Under US law (the only one I'm familiar with), the author of a mailing list
>> post or any other written work _automatically holds copyright_ to that
>> post (although not to the ideas contained therein, but to that particular
>> description of the ideas). (Of course, if the ideas are original to the author,
>> it's good form to acknowledge that regardless of whether the exact words
>> are used).
>>     
>
> I believe this is true for all countries that are signatory to the
> Berne convention (which is pretty much all countries [1]). The US in
> fact was one of the later ones to get into it, before which you had to
> explicitly copyright things if you wanted copyright.
>
> -Deepayan
>
> [1] http://upload.wikimedia.org/wikipedia/commons/6/6c/Berne_Convention.png
>   
Yes. It's pretty obvious that by posting you agree to publication, and 
presumably also to archiving.  Think "Letters to the Editor". However, 
you do not agree to just any republication (in particular not to 
commercial usage -- say someone wants to publish the collected works of 
a particularly prolific correspondent, without  paying and obtaining 
consent). 

Interestingly, BYTE magazine back in the late 80's actually ran a Best 
of BIX column with postings from their bulletin board. I've always 
wondered how (and whether) they handled the copyright issues.

There is a middle ground of "fair use" and the right to citation, 
though. I certainly don't expect to be cited by everyone using code 
snippets from one of my posts.

    -pd


From ripley at stats.ox.ac.uk  Fri Mar 30 23:29:23 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 30 Mar 2007 22:29:23 +0100 (BST)
Subject: [R] Copyright of messages (was Wikibooks)
In-Reply-To: <460D7B89.4050103@biostat.ku.dk>
References: <20070329193059.M91171@centroin.com.br>
	<loom.20070329T234918-210@post.gmane.org>
	<loom.20070330T084949-731@post.gmane.org>
	<eb555e660703301217x2c5b9b66ha4a471ea32d5d5e@mail.gmail.com>
	<20070330193122.M49424@centroin.com.br>
	<efb536d50703301249n55b101e2te3de2c902d21f6a7@mail.gmail.com>
	<eb555e660703301302j47a6e51cv5bb11bb2776992a7@mail.gmail.com>
	<460D7B89.4050103@biostat.ku.dk>
Message-ID: <Pine.LNX.4.64.0703302219410.22919@gannet.stats.ox.ac.uk>

What you agree to _is_ covered in the R posting guide and its references, 
as well as what is expected of posters in respecting the rights of others.

Perhaps time yet again to remind people:

> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html


On Fri, 30 Mar 2007, Peter Dalgaard wrote:

> Deepayan Sarkar wrote:
>> On 3/30/07, Sarah Goslee <sarah.goslee at gmail.com> wrote:
>>
>>> On 3/30/07, Alberto Monteiro <albmont at centroin.com.br> wrote:
>>>
>>>> Deepayan Sarkar wrote:
>>>>
>>>>> I was just looking at this page, and it makes me curious: what gives
>>>>> anyone the right to take someone else's mailing list post and include
>>>>> that in a Wiki?
>>>>>
>>>>>
>>>> Thinks there were posted to public mailing lists are freely
>>>> copied and distributed. It's a scary thought; I may have posted
>>>> things in 10 or 12 years ago that might cause me problems today,
>>>> but I was pretty aware that I was posting to the whole world.
>>>>
>>
>> There's a difference between public archiving and copying.
>>
>>
>>> It's not that simple. Dealing with international contributors it's even worse.
>>> Under US law (the only one I'm familiar with), the author of a mailing list
>>> post or any other written work _automatically holds copyright_ to that
>>> post (although not to the ideas contained therein, but to that particular
>>> description of the ideas). (Of course, if the ideas are original to the author,
>>> it's good form to acknowledge that regardless of whether the exact words
>>> are used).
>>>
>>
>> I believe this is true for all countries that are signatory to the
>> Berne convention (which is pretty much all countries [1]). The US in
>> fact was one of the later ones to get into it, before which you had to
>> explicitly copyright things if you wanted copyright.
>>
>> -Deepayan
>>
>> [1] http://upload.wikimedia.org/wikipedia/commons/6/6c/Berne_Convention.png
>>
> Yes. It's pretty obvious that by posting you agree to publication, and
> presumably also to archiving.  Think "Letters to the Editor". However,
> you do not agree to just any republication (in particular not to
> commercial usage -- say someone wants to publish the collected works of
> a particularly prolific correspondent, without  paying and obtaining
> consent).
>
> Interestingly, BYTE magazine back in the late 80's actually ran a Best
> of BIX column with postings from their bulletin board. I've always
> wondered how (and whether) they handled the copyright issues.
>
> There is a middle ground of "fair use" and the right to citation,
> though. I certainly don't expect to be cited by everyone using code
> snippets from one of my posts.
>
>    -pd
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From clint at ecy.wa.gov  Fri Mar 30 23:32:25 2007
From: clint at ecy.wa.gov (Clint Bowman)
Date: Fri, 30 Mar 2007 14:32:25 -0700 (PDT)
Subject: [R] Wikibooks
In-Reply-To: <460D7B89.4050103@biostat.ku.dk>
References: <20070329193059.M91171@centroin.com.br>
	<loom.20070329T234918-210@post.gmane.org>
	<loom.20070330T084949-731@post.gmane.org>
	<eb555e660703301217x2c5b9b66ha4a471ea32d5d5e@mail.gmail.com>
	<20070330193122.M49424@centroin.com.br>
	<efb536d50703301249n55b101e2te3de2c902d21f6a7@mail.gmail.com>
	<eb555e660703301302j47a6e51cv5bb11bb2776992a7@mail.gmail.com>
	<460D7B89.4050103@biostat.ku.dk>
Message-ID: <Pine.LNX.4.62.0703301426480.22626@aeolus.ecy.wa.gov>

On Fri, 30 Mar 2007, Peter Dalgaard wrote:

> Deepayan Sarkar wrote:
> > On 3/30/07, Sarah Goslee <sarah.goslee at gmail.com> wrote:
> >
> >> On 3/30/07, Alberto Monteiro <albmont at centroin.com.br> wrote:
> >>
> >>> Deepayan Sarkar wrote:
> >>>
> >>>> I was just looking at this page, and it makes me curious: what gives
> >>>> anyone the right to take someone else's mailing list post and include
> >>>> that in a Wiki?
> >>>>
> >>>>
> >>> Thinks there were posted to public mailing lists are freely
> >>> copied and distributed. It's a scary thought; I may have posted
> >>> things in 10 or 12 years ago that might cause me problems today,
> >>> but I was pretty aware that I was posting to the whole world.
> >>>
> >
> > There's a difference between public archiving and copying.
> >
> >
> >> It's not that simple. Dealing with international contributors it's even worse.
> >> Under US law (the only one I'm familiar with), the author of a mailing list
> >> post or any other written work _automatically holds copyright_ to that
> >> post (although not to the ideas contained therein, but to that particular
> >> description of the ideas). (Of course, if the ideas are original to the author,
> >> it's good form to acknowledge that regardless of whether the exact words
> >> are used).
> >>
> >
> > I believe this is true for all countries that are signatory to the
> > Berne convention (which is pretty much all countries [1]). The US in
> > fact was one of the later ones to get into it, before which you had to
> > explicitly copyright things if you wanted copyright.
> >
> > -Deepayan
> >
> > [1] http://upload.wikimedia.org/wikipedia/commons/6/6c/Berne_Convention.png
> >
> Yes. It's pretty obvious that by posting you agree to publication, and
> presumably also to archiving.  Think "Letters to the Editor". However,
> you do not agree to just any republication (in particular not to
> commercial usage -- say someone wants to publish the collected works of
> a particularly prolific correspondent, without  paying and obtaining
> consent).
>
> Interestingly, BYTE magazine back in the late 80's actually ran a Best
> of BIX column with postings from their bulletin board. I've always
> wondered how (and whether) they handled the copyright issues.
>
> There is a middle ground of "fair use" and the right to citation,
> though. I certainly don't expect to be cited by everyone using code
> snippets from one of my posts.
>
>     -pd
>

My wife has edited just such a collection (of Compuserve forum messages)
and is currently engaged in writing another.  And yes, obtaining and
keeping track of a hundred citations through the editing process is quite
the chore--but not so bad that she isn't willing to embark on another
book.  Needless to say, she cringes at the looseness of copyright tracking
that occurs on email lists and wikis.

Clint

Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Dispersion Modeler		INTERNET:	clint at math.utah.edu
Air Quality Program		VOICE:		(360) 407-6815
Department of Ecology		FAX:		(360) 407-7534

	USPS:  		PO Box 47600, Olympia, WA 98504-7600
	Parcels:	300 Desmond Drive, Lacey, WA 98503-1274


From albmont at centroin.com.br  Sat Mar 31 00:00:03 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 30 Mar 2007 20:00:03 -0200
Subject: [R] faster computation of cumulative multinomial distribution
In-Reply-To: <460D1A49.9010101@borm.org>
References: <460D1A49.9010101@borm.org>
Message-ID: <20070330213509.M40662@centroin.com.br>

Theo Borm wrote:
> 
> I have a series of /unequal/ probabilities [p1,p2,...,pk], describing
> mutually exclusive events, and a "remainder" class with a probability
> p0=1-p1-p2-....-pk, and need to calculate, for a given number of trials
> t>=k, the combined probability that each of the classes 1...k 
> contains at least 1 "event" (the remainder class may be empty).
> 
> To me this reaks of a sum of multinomial distributions, and indeed, I
> can readily calculate the correct answer for small figures t,k using 
> a small R program.
> 
The multinomial distribution is a sum of independent cathegorial
distributions, so there's no such thing as a sum of multinomial
distributions - unless the sum of multinomial distributions is
also a multinomial distribution.

Yikes. I think I am saying too much.

Just look here:
http://en.wikipedia.org/wiki/Multinomial_distribution

> However, in my typical experiment, k ranges from ~20-60 and t from
> ~40-100, and having to calculate these for about 6e9 experiments, a
> quick calculation on the back of a napkin shows me that I will not be
> able to complete these calculations before the expected end
> of the universe.
>
But what do you want to calculate? The probabilities?
 
> I already figured out that in this particular case I may use reciprocal
> probabilities, and if I do this I get an equation with "only" 2^k 
> terms, which would shorten my computations to a few decades.
> 
> Isn't there a faster (numerical approximation?) way to do this?
> 
I don't know what you want to do, but maybe this is the case
where a Monte Carlo Simulation would give a fast and accurate
result.

Let's say you want to estimate some function of the outcome 
of this multinomial. Just to fix ideas:

# p - the probabilities
p <- c(1, 2, 3, 2, 4, 4, 2, 1, 2, 3, 1, 2, 3, 4, 3, 2, 1, 2) 
# normalize so that sum(p) = 1
p <- p / sum(p)
# get k
k <- length(p)
# number of trials, say 100
t <- 100
# x is the simulation for each trial. x[1] is the number of
# times we got the thing with probability p[1], etc
#
# x <- rmultinom(1, k, p) # simulates 1 experiment
#
# f is the function that you will use to "evaluate" x
#
f <- function(x) {
  return(sum(x * 1.05^-(0:(length(x)-1))))  # anything can come here
}
#
# A Monte Carlo simulation will generate "a lot" of xs and
# get a distribution of f(x)
#
# Simulate 10000 xs
x <- rmultinom(10000, k, p) # takes almost zero time
# Evaluate 10000 f(x). apply(matrix, 1 (rows) or 2 (coluns), function)
f.dist <- apply(x, 2, f)
# analyse it
hist(f.dist) # etc

> R has dbinom /and/ pbinom functions, but unfortunately only a dmultinom
> and no pmultinom function... perhaps because there is no (known) 
> faster way?
> 
There's a rmultinom

Alberto Monteiro


From skiadas at hanover.edu  Sat Mar 31 01:36:37 2007
From: skiadas at hanover.edu (Charilaos Skiadas)
Date: Fri, 30 Mar 2007 19:36:37 -0400
Subject: [R] substitute NA values
In-Reply-To: <1175266590.7115.57.camel@gsimpson.geog.ucl.ac.uk>
References: <b490ce570703300323s5577885dn9b3afa414afd4f45@mail.gmail.com>
	<971536df0703300600l35b0337sf11ed14abcce3f06@mail.gmail.com>
	<b490ce570703300725x269279b3l23561a551eea8c51@mail.gmail.com>
	<1175266590.7115.57.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <4546E926-8C8F-4CFE-95E8-BCC1216CBE03@hanover.edu>

On Mar 30, 2007, at 10:56 AM, Gavin Simpson wrote:

> This is R, there is always a way (paraphrasing an R-Helper the name of
> whom I forget just now).

Can't resist, it's one of my favorite fortunes ;)

That would be Simon 'Yoda' Blomberg:

library(fortunes)
fortune(109)

Haris Skiadas
Department of Mathematics and Computer Science
Hanover College


From theo.borm at wur.nl  Sat Mar 31 01:59:47 2007
From: theo.borm at wur.nl (theo borm)
Date: Sat, 31 Mar 2007 01:59:47 +0200
Subject: [R] faster computation of cumulative multinomial distribution
In-Reply-To: <20070330213509.M40662@centroin.com.br>
References: <460D1A49.9010101@borm.org> <20070330213509.M40662@centroin.com.br>
Message-ID: <460DA473.7040806@wur.nl>

Alberto Monteiro wrote:
> Theo Borm wrote:
>   
>> I have a series of /unequal/ probabilities [p1,p2,...,pk], describing
>> mutually exclusive events, and a "remainder" class with a probability
>> p0=1-p1-p2-....-pk, and need to calculate, for a given number of trials
>> t>=k, the combined probability that each of the classes 1...k 
>> contains at least 1 "event" (the remainder class may be empty).
>>
>> To me this reaks of a sum of multinomial distributions, and indeed, I
>> can readily calculate the correct answer for small figures t,k using 
>> a small R program.
>>
>>     
> The multinomial distribution is a sum of independent cathegorial
> distributions, so there's no such thing as a sum of multinomial
> distributions - unless the sum of multinomial distributions is
> also a multinomial distribution.
>
> Yikes. I think I am saying too much.
>
> Just look here:
> http://en.wikipedia.org/wiki/Multinomial_distribution
>   
*sorry. that should have read "sum of multinomial probabilities"

The following article (first page freely available):

A Representation for Multinomial Cumulative Distribution Functions*
Bruce Levin
/The Annals of Statistics/, Vol. 9, No. 5 (Sep., 1981), pp. 1123-1126

describes more or less what I mean, only instead of calculating

pN=pN(t,p1,p2,...,pt,a1,a2,...at) = P(n1<=a1, n2<=a2, n3<=a3, .... , nt<=at)

I need to calculate
pN= P(n1>=0, n2>=1, n3>=1, .... , nt>=1)
with n1+n2+n3+...+nt=total number of trials

(Note that for notational purposes I renamed my index [0...k] to Levin's
[1...t], hence category 1 represents the "remainder" class, and my
"number of trials" t becomes Levin's sum of a1+a2+a3+...+at  )

Also note  the change from "<=" to ">="
>
> But what do you want to calculate? The probabilities?
>   
I want to calculate single (compound) probability, given t mutually
exclusive and exhaustive events and k trials that the first "slot"
contains /at least/ zero events AND all the other "slots" contain at
least one event.

Think of a sort of "power roulette", played with 58 balls
simultaneously, with a wheel containing 36 red/black slots of unequal
size, and 1 green slot. I need to calculate the probability that each of
the 36 red/black slots contains at least one ball.
> I don't know what you want to do, but maybe this is the case
> where a Monte Carlo Simulation would give a fast and accurate
> result.
>   
hmmm....

I have a hunch that it won't work as my p vector typically contains
values like:

p<-c(0.99, 0.005, 0.003, 0.001, 0.0005, 0.0003, 0.0001, 0.00005,
0.00003, 0.00002).

keeping in mind that the first "green" slot (0.99) represents my
"remainder" class (which may be empty), and calculating the probability
that with 9 trials I get exactly one in each of the other  slots:
 
p=0.005*0.003*0.001*0.0005*0.0003*0.0001*0.00005*0.00003*0.00002*9!

=~ 2.45*10^-27

so, I'm looking for very rare events.

to calculate this figure even marginally /accurately/ with a monte carlo
simulation would require in well excess of 4*10^26 iterations.....


> Let's say you want to estimate some function of the outcome 
> of this multinomial. Just to fix ideas:
>
> # p - the probabilities
> p <- c(1, 2, 3, 2, 4, 4, 2, 1, 2, 3, 1, 2, 3, 4, 3, 2, 1, 2) 
> # normalize so that sum(p) = 1
> p <- p / sum(p)
> # get k
> k <- length(p)
> # number of trials, say 100
> t <- 100
> # x is the simulation for each trial. x[1] is the number of
> # times we got the thing with probability p[1], etc
> #
> # x <- rmultinom(1, k, p) # simulates 1 experiment
> #
> # f is the function that you will use to "evaluate" x
> #
> f <- function(x) {
>   return(sum(x * 1.05^-(0:(length(x)-1))))  # anything can come here
> }
> #
> # A Monte Carlo simulation will generate "a lot" of xs and
> # get a distribution of f(x)
> #
> # Simulate 10000 xs
> x <- rmultinom(10000, k, p) # takes almost zero time
> # Evaluate 10000 f(x). apply(matrix, 1 (rows) or 2 (coluns), function)
> f.dist <- apply(x, 2, f)
> # analyse it
> hist(f.dist) # etc
>
>   
>> R has dbinom /and/ pbinom functions, but unfortunately only a dmultinom
>> and no pmultinom function... perhaps because there is no (known) 
>> faster way?
>>
>>     
> There's a rmultinom
>   

Thanks,


with kind regards,

Theo Borm.


From Mark.Leeds at morganstanley.com  Sat Mar 31 02:08:36 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Fri, 30 Mar 2007 20:08:36 -0400
Subject: [R] X11 and linux and plotting and the vertical axis
Message-ID: <D3AEEDA31E57474B840BEBC25A8A8344F398E9@NYWEXMB23.msad.ms.com>

I use R on linux and I go through exceed from a windows machine.

Depending on the amount of plots I do on a screen, sometimes the numbers
on the vertical axis
of the plots don't show up. I tried infinite of combinations of mar and
cex but nothing helps.
Yet, if instead of shooting the graphic upto the screen, I send it to a
pdf file or
a postscript file, the numbers on the vertical axis of the plots do show
up.

I've included a test.pdf file that shows the plot but the #'s show up
there and not
if I send the same plot to the screen. 

Below my version info, I've also included the output of doing par() at
the prompt in case that some of my
default settings look weird. I know next to nothing  about R graphics
but the problem 
definitely doesn't have anything to do with mar or cex though because
I've played with them pretty much the 
whole day.

If anyone knows of a thread on this or knos what needs to be changed, it
would
Be much appreciated. Thanks.


version
               _                           
platform       i686-pc-linux-gnu           
arch           i686                        
os             linux-gnu                   
system         i686, linux-gnu             
status                                     
major          2                           
minor          4.0                         
year           2006                        
month          10                          
day            03                          
svn rev        39566                       
language       R                           
version.string R version 2.4.0 (2006-10-03)


 par()
$xlog
[1] FALSE

$ylog
[1] FALSE

$adj
[1] 0.5

$ann
[1] TRUE

$ask
[1] FALSE

$bg
[1] "transparent"

$bty
[1] "o"

$cex
[1] 1

$cex.axis
[1] 1

$cex.lab
[1] 1

$cex.main
[1] 1.2

$cex.sub
[1] 1

$cin
[1] 0.1730130 0.1960814

$col
[1] "black"

$col.axis
[1] "black"

$col.lab
[1] "black"

$col.main
[1] "black"

$col.sub
[1] "black"

$cra
[1] 15 17

$crt
[1] 0

$csi
[1] 0.1960814

$cxy
[1] 0.02996404 0.03781139

$din
[1] 6.989727 6.989727


$err
[1] 0

$family
[1] ""

$fg
[1] "black"

$fig
[1] 0 1 0 1

$fin
[1] 6.989727 6.989727

$font
[1] 1

$font.axis
[1] 1

$font.lab
[1] 1

$font.main
[1] 2

$font.sub
[1] 1

$gamma
[1] 1

$lab
[1] 5 5 7

$las
[1] 0

$lend
[1] "round"

$lheight
[1] 1

$ljoin
[1] "round"

$lmitre
[1] 10

$lty
[1] "solid"

$lwd
[1] 1

$mai
[1] 1.000015 0.803934 0.803934 0.411771

$mar
[1] 5.1 4.1 4.1 2.1



$mex
[1] 1

$mfcol
[1] 1 1

$mfg
[1] 1 1 1 1

$mfrow
[1] 1 1

$mgp
[1] 3 1 0

$mkh
[1] 0.001

$new
[1] FALSE

$oma
[1] 0 0 0 0

$omd
[1] 0 1 0 1

$omi
[1] 0 0 0 0

$pch
[1] 1

$pin
[1] 5.774022 5.185778

$plt
[1] 0.1150165 0.9410891 0.1430693 0.8849835

$ps
[1] 12

$pty
[1] "m"

$smo
[1] 1

$srt
[1] 0

$tck
[1] NA

$tcl
[1] -0.5

$usr
[1] 0 1 0 1

$xaxp
[1] 0 1 5

$xaxs
[1] "r"


$xaxt
[1] "s"

$xpd
[1] FALSE

$yaxp
[1] 0 1 5

$yaxs
[1] "r"

$yaxt
[1] "s"
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/sell the securities/instruments mentioned or an official confirmation.  Morgan Stanley may deal as principal in or own or act as market maker for securities/instruments mentioned or may advise the issuers.  This is not research and is not from MS Research but it may refer to a research analyst/research report.  Unless indicated, these views are the author's and may differ from those of Morgan Stanley research or others in the Firm.  We do not represent this is accurate or complete and we may not update this.  Past performance is not indicative of future returns.  For additional information, research reports and important disclosures, contact me or see https://secure.ms.com/servlet/cls.  You should not use e-mail to request, authorize or effect the purchase or sale of any security or instrument, to send transfer instructions, or to effect any other transactions.  We cannot guarantee that any such requests received via e-mail will be processed in a timely manner.  This communication is solely for the addressee(s) and may contain confidential information.  We do not waive confidentiality by mistransmission.  Contact me if you do not wish to receive these communications.  In the UK, this communication is directed in the UK to those persons who are market counterparties or intermediate customers (as defined in the UK Financial Services Authority's rules).

From murdoch at stats.uwo.ca  Sat Mar 31 02:31:26 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 30 Mar 2007 20:31:26 -0400
Subject: [R] Wikibooks
In-Reply-To: <460D7B89.4050103@biostat.ku.dk>
References: <20070329193059.M91171@centroin.com.br>	<loom.20070329T234918-210@post.gmane.org>	<loom.20070330T084949-731@post.gmane.org>	<eb555e660703301217x2c5b9b66ha4a471ea32d5d5e@mail.gmail.com>	<20070330193122.M49424@centroin.com.br>	<efb536d50703301249n55b101e2te3de2c902d21f6a7@mail.gmail.com>	<eb555e660703301302j47a6e51cv5bb11bb2776992a7@mail.gmail.com>
	<460D7B89.4050103@biostat.ku.dk>
Message-ID: <460DABDE.4070409@stats.uwo.ca>

On 3/30/2007 5:05 PM, Peter Dalgaard wrote:
> Deepayan Sarkar wrote:
>> On 3/30/07, Sarah Goslee <sarah.goslee at gmail.com> wrote:
>>   
>>> On 3/30/07, Alberto Monteiro <albmont at centroin.com.br> wrote:
>>>     
>>>> Deepayan Sarkar wrote:
>>>>       
>>>>> I was just looking at this page, and it makes me curious: what gives
>>>>> anyone the right to take someone else's mailing list post and include
>>>>> that in a Wiki?
>>>>>
>>>>>         
>>>> Thinks there were posted to public mailing lists are freely
>>>> copied and distributed. It's a scary thought; I may have posted
>>>> things in 10 or 12 years ago that might cause me problems today,
>>>> but I was pretty aware that I was posting to the whole world.
>>>>       
>> There's a difference between public archiving and copying.
>>
>>   
>>> It's not that simple. Dealing with international contributors it's even worse.
>>> Under US law (the only one I'm familiar with), the author of a mailing list
>>> post or any other written work _automatically holds copyright_ to that
>>> post (although not to the ideas contained therein, but to that particular
>>> description of the ideas). (Of course, if the ideas are original to the author,
>>> it's good form to acknowledge that regardless of whether the exact words
>>> are used).
>>>     
>> I believe this is true for all countries that are signatory to the
>> Berne convention (which is pretty much all countries [1]). The US in
>> fact was one of the later ones to get into it, before which you had to
>> explicitly copyright things if you wanted copyright.
>>
>> -Deepayan
>>
>> [1] http://upload.wikimedia.org/wikipedia/commons/6/6c/Berne_Convention.png
>>   
> Yes. It's pretty obvious that by posting you agree to publication, and 
> presumably also to archiving.  Think "Letters to the Editor". However, 
> you do not agree to just any republication (in particular not to 
> commercial usage -- say someone wants to publish the collected works of 
> a particularly prolific correspondent, without  paying and obtaining 
> consent). 
> 
> Interestingly, BYTE magazine back in the late 80's actually ran a Best 
> of BIX column with postings from their bulletin board. I've always 
> wondered how (and whether) they handled the copyright issues.
> 
> There is a middle ground of "fair use" and the right to citation, 
> though. I certainly don't expect to be cited by everyone using code 
> snippets from one of my posts.

"Fair use" varies quite a bit from country to country.  I've no idea 
about Denmark's laws, but Canada has no "fair use" doctrine in the US 
sense, just a much more limited "fair dealing" doctrine.  Last time I 
looked Wikipedia had a pretty good description of this.

Duncan Murdoch


From brown_emu at yahoo.com  Sat Mar 31 03:41:39 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Fri, 30 Mar 2007 18:41:39 -0700 (PDT)
Subject: [R] Using split() several times in a row?
In-Reply-To: <7cb007bd0703300818h742f7ef3wcee5f23a424b4546@mail.gmail.com>
Message-ID: <470978.64452.qm@web39708.mail.mud.yahoo.com>

Hi Sergey,

I believe the code below should get you close to want you want.

For dates, I usually store them as "POSIXct" classes in data frames, but
according to Gabor Grothendieck and Thomas Petzoldt's R Help Desk article
<http://cran.r-project.org/doc/Rnews/Rnews_2004-1.pdf>, I should probably be
using "chron" date and times...

Nonetheless, POSIXct casses are what I know so I can show you that to get the
month out of your column (replace "8.29.97" with your variable), you can do
the following:

month = format(strptime("8.29.97",format="%m.%d.%y"),format="%m")

Or,
month = as.data.frame(strsplit("8.29.97","\\."))[1,]

In any case, here is a code, in which I follow a series of function
application and definitions (which effectively includes successive
application of split() and lapply().

Best regards,

ST

# define data (I just made this up)
df <-
data.frame(month=as.character(rep(1:3,each=30)),fac=factor(rep(1:2,each=15)),
            data1=round(runif(90),2),
            data2=round(runif(90),2))

# define functions to split the data and another
# to get statistics
doSplits <- function(df) {
  unlist(lapply(split(df,df$month),function(x)
split(x,x$fac)),recursive=FALSE)
}
getStats <- function(x,f) {
  return(as.data.frame(lapply(x[unlist(lapply(x,mode))=="numeric" &
                                unlist(lapply(x,class))!="factor"],f)))
}
# create a matrix of data, means, and standard deviations
listMatrix <- cbind(Data=doSplits(df),
           Means=lapply(doSplits(df),getStats,mean),
           SDs=lapply(doSplits(df),getStats,sd))

# function to subtract means and divide by standard deviations
transformData <- function(x) {
  newdata <- x$Data
  matchedNames <- match(names(x$Means),names(x$Data))
  newdata[matchedNames] <-
    sweep(sweep(data.matrix(x$Data[matchedNames]),2,unlist(x$Means),"-"),
          2,unlist(x$SDs),"/")
  return(newdata)
}
# apply to data
newDF <- lapply(as.data.frame(t(listMatrix)),transformData)

# Defind Fold function
Fold <- function(f, x, L) for(e in L) x <- f(x, e)
# Apply this to the data
finalData <- Fold(rbind,vector(),newDF)






--- Sergey Goriatchev <sergeyg at gmail.com> wrote:

> Hi, fellow R users.
> 
> I have a question about sapply and split combination.
> 
> I have a big dataframe (40000 observations, 21 variables). First
> variable (factor) is "date" and it is in format "8.29.97", that is, I
> have monthly data. Second variable (also factor) has levels 1 to 6
> (fractiles 1 to 5 and missing value with code 6). The other 19
> variables are numeric.
> For each month I have several hunder observations of 19 numeric and 1
> factor.
> 
> I am normalizing the numeric variables by dividing val1 by val2, where:
> 
> val1: (for each month, for each numeric variable) difference between
> mean of ith numeric variable in fractile 1, and mean of ith numeric
> variable in fractile 5.
> 
> val2: (for each month, for each numeric variable) standard deviation
> for ith numeric variable.
> 
> Basically, as far as I understand, I need to use split() function several
> times.
> To calculate val1 I need to use split() twice - first to split by
> month and then split by fractile. Is this even possible to do (since
> after first application of split() I get a list)??
> 
> Is there a smart way to perform this normalization computation?
> 
> My knowledge of R is not so advanced, but I need to know an efficient
> way to perform calculations of this kind.
> 
> Would really appreciate some help from experienced R users!
> 
> Regards,
> S
> 
> -- 
> Laziness is nothing more than the habit of resting before you get tired.
> - Jules Renard (writer)
> 
> Experience is one thing you can't get for nothing.
> - Oscar Wilde (writer)
> 
> When you are finished changing, you're finished.
> - Benjamin Franklin (Diplomat)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch at stats.uwo.ca  Sat Mar 31 03:42:28 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 30 Mar 2007 21:42:28 -0400
Subject: [R] Trouble installing package 'sp' in R 2.4.1
In-Reply-To: <Pine.LNX.4.44.0703302131510.1819-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0703302131510.1819-100000@reclus.nhh.no>
Message-ID: <460DBC84.1010601@stats.uwo.ca>

On 3/30/2007 3:37 PM, Roger Bivand wrote:
> On Fri, 30 Mar 2007, Duncan Murdoch wrote:
> 
>> On 3/30/2007 2:39 PM, Gaurav Ghosh wrote:
>>> Hi all,
>>>
>>> I just installed R 2.4.1 and tried to install the package 'sp' working down from the 'Packages' menu in the R GUI.  The package would not install and I got the following message:
>>>
>>> &quot;trying URL 'http://cran.us.r-project.org/bin/windows/contrib/2.4/sp_0.9-10.zip'
>>> Error in download.file(url, destfile, method, mode = &quot;wb&quot;) : 
>>>         cannot open URL 'http://cran.us.r-project.org/bin/windows/contrib/2.4/sp_0.9-10.zip'
>>> In addition: Warning message:
>>> cannot open: HTTP status was '404 Not Found' 
>>> Warning in download.packages(pkgs, destdir = tmpd, available = available,  : 
>>>          download of package 'sp' failed&quot;
>>>
>>> I tried to find the installation file &quot;sp_0.9-10.zip&quot; through the CRAN website, but was unable to do so.  So I was wondering if there was some other way to get this package I need.  Or perhaps  there might be some problem with the website itself.
>>
>> You didn't say which mirror you were using, but it looks like a problem 
>> there.  I just tried from one of the Canadian mirrors and there was no 
>> problem finding the package.
> 
>>From HTTP, the mirror mentioned inline http://cran.us.r-project.org/ has 
> problems, is well behind on versions in the project pages, and
> 
> http://cran.us.r-project.org/bin/windows/contrib/2.4/
> 
> has no sp package at all. The last check summary is dated 2007-03-16.
> 
> Please try another mirror; if anyone could find out what is the matter 
> with the mirror, and get it corrected, that would be welcome.

I've let them know they're a couple of weeks behind on updates.

Duncan Murdoch


From albmont at centroin.com.br  Sat Mar 31 03:55:50 2007
From: albmont at centroin.com.br (Alberto Vieira Ferreira Monteiro)
Date: Sat, 31 Mar 2007 01:55:50 +0000
Subject: [R] faster computation of cumulative multinomial distribution
In-Reply-To: <460DA473.7040806@wur.nl>
References: <460D1A49.9010101@borm.org> <20070330213509.M40662@centroin.com.br>
	<460DA473.7040806@wur.nl>
Message-ID: <200703310155.51847.albmont@centroin.com.br>

Theo Borm wrote:
>
> Think of a sort of "power roulette", played with 58 balls
> simultaneously, with a wheel containing 36 red/black slots of unequal
> size, and 1 green slot. I need to calculate the probability that each of
> the 36 red/black slots contains at least one ball.
>
Ah, now we come to a more precise problem :-)

> I have a hunch that it won't work as my p vector typically contains
> values like:
>
> p<-c(0.99, 0.005, 0.003, 0.001, 0.0005, 0.0003, 0.0001, 0.00005,
> 0.00003, 0.00002).
>
So you should expect that only 1/50000 will be in the low-prob
hole in the average?

> keeping in mind that the first "green" slot (0.99) represents my
> "remainder" class (which may be empty), and calculating the probability
> that with 9 trials I get exactly one in each of the other  slots:
>
> p=0.005*0.003*0.001*0.0005*0.0003*0.0001*0.00005*0.00003*0.00002*9!
>
> =~ 2.45*10^-27
>
> so, I'm looking for very rare events.
>
Indeed you are!

> to calculate this figure even marginally /accurately/ with a monte carlo
> simulation would require in well excess of 4*10^26 iterations.....
>
Hmmm.... That's right. No way to do a Monte Carlo here!

Alberto Monteiro


From ajung at gfz-potsdam.de  Sat Mar 31 08:24:18 2007
From: ajung at gfz-potsdam.de (Andre Jung)
Date: Sat, 31 Mar 2007 08:24:18 +0200
Subject: [R] Linear model and time series
Message-ID: <460DFE92.7060805@gfz-potsdam.de>

Dear all,

I have three timeseries Uts, Vts, Wts. The relation between the time
series can be expressed as

Uts = x Vts + y Wts + residuals

How would I feed this to lm() to evaluate the unknowns x and y?

Thanks,
andre


From ripley at stats.ox.ac.uk  Sat Mar 31 10:47:51 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 31 Mar 2007 09:47:51 +0100 (BST)
Subject: [R] zip.file.extract on Windows
In-Reply-To: <460D6E2A.8090007@umn.edu>
References: <460D6E2A.8090007@umn.edu>
Message-ID: <Pine.LNX.4.64.0703302214240.22919@gannet.stats.ox.ac.uk>

The current (you give no version information) help file says:

     file: A file name. (If a path is given, see 'Note'.)

  zipname: The file name (not path) of a 'zip' archive, including the
           '".zip"' extension if required.
...

      This is a helper function for 'help', 'example' and 'data'.  As
      such, it handles file paths in an unusual way.  Any path component
      of 'zipname' is ignored, and the path to 'file' is used only to
      determine the directory within which to find 'zipname'.

You do seem to have missed all of these hints, as well as the posting 
guide (see the comments at the end of this reply).

On Fri, 30 Mar 2007, Gregg Lind wrote:

> Hello R-help!
>
> I just wanted to share a tip might help others of us stuck on R.

> The documentation for zip.file.extract is rather scant, and a few
> examples would.

> When using zip.file.extract, the first argument should be the full (or
> relative) path and the second the name of the zip file, it seems.

> While this is counterintuitive to me, that it should get the part from

what is 'the part' here?

> the first argument, and apply that path to the second, it seems to how
> this works.

> This seems to me to be the wrong approach, since it won't handle folders
> inside the zipfile correctly.

> Am I missing something?  Should I just use "unz" instead?
>
> Thanks,
>
> Gregg L.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

PLEASE do!  It asked you to give 'the output from sessionInfo()' and to 
'try the current R-patched or R-devel version of R'.  As the sources are 
online so you really should have checked 
https://svn.r-project.org/R/trunk/src/library/utils/man/zip.file.extract.Rd

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jim at bitwrit.com.au  Sat Mar 31 12:51:26 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Sat, 31 Mar 2007 20:51:26 +1000
Subject: [R] substitute NA values
In-Reply-To: <b490ce570703300323s5577885dn9b3afa414afd4f45@mail.gmail.com>
References: <b490ce570703300323s5577885dn9b3afa414afd4f45@mail.gmail.com>
Message-ID: <460E3D2E.4070305@bitwrit.com.au>

Sergio Della Franca wrote:
> Dear R-Helpers,
> 
> 
> I have the following data set(y):
> 
>   Test_Result   #_Test
>     t                 10
>     f                 14
>     f                 25
>     f                 NA
>     f                 40
>     t                45
>     t                44
>  <NA>           47
>     t                NA
> 
> 
> I want to replace the NA values with the following method:
> - for the numeric variable, replace NA with median
> - for character variable , replace NA with the most frequent level
> 
> If i use x<-na.roughfix(y) the NA values are correctly replaced.
> But if i x<-na.roughfix(y$Test_Result) i obtain the following error:
> 
> roughfix can only deal with numeric data.
> 
> How can i solve this proble that i met every time i want to replace only the
> NA values of a column (type character)?
> 
Hi Sergio,
In the prettyR package is the Mode function. This returns the mode of a 
vector as a character string. So I think this would do what you want:

library(prettyR)
testvec<-c(sample(LETTERS[1:4],20,TRUE),NA,NA)
testvec[is.na(testvec)]<-Mode(testvec)

You could do the same trick with the median function for the numeric values.

Jim


From patrick at pdrechsler.de  Sat Mar 31 14:26:46 2007
From: patrick at pdrechsler.de (Patrick Drechsler)
Date: Sat, 31 Mar 2007 14:26:46 +0200
Subject: [R] add confidence intervales to xyplot for ANCOVA and extracting
	info
Message-ID: <877isxmwe1.fsf@pdrechsler.de>

Hi,

I would like to add confidence intervales to an ANCOVA with 2
covariates when using xyplot.

What would be a good way of accomplishing this?

--8<---------------cut here---------------start------------->8---
rm(list = ls(all = TRUE))
rm(list = c(ls()))

library(lattice)

## 1. generate data
random <- rnorm(200)
y <- abs(random)
x1.cont <- abs(random)
x2.fac <- as.factor(rep(1:5, 4))     # 4 groups
x3.fac <- as.factor(rep(1:4, each=5))# 5 groups
A <- data.frame(y, x1.cont, x2.fac, x2.fac)

## 2. plot data
foo <- xyplot(log(y) ~ log(x1.cont) | x2.fac * x3.fac,
              data = A,
              type = c("p","r"),
              strip = strip.custom(strip.names=TRUE),
              as.table=TRUE)

plot(foo)
--8<---------------cut here---------------end--------------->8---

The function xyplot calculates many statistics so it can produce
regression lines and other things using "type". How does one access
this data for further analysis? I have looked at str(foo), but I did
not see an easy way of extracting the data.

TIA,

Patrick

,----[ sessionInfo() ]
| R version 2.4.1 (2006-12-18) 
| i486-pc-linux-gnu 
| 
| locale:
| LC_CTYPE=de_DE.UTF-8;LC_NUMERIC=C;LC_TIME=de_DE.UTF-8;LC_COLLATE=de_DE.UTF-8;LC_MONETARY=de_DE.UTF-8;LC_MESSAGES=de_DE.UTF-8;LC_PAPER=de_DE.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=de_DE.UTF-8;LC_IDENTIFICATION=C
| 
| attached base packages:
| [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"  
| [7] "base"     
| 
| other attached packages:
|   lattice 
| "0.14-17" 
`----

-- 
Patrick Drechsler
Department of Zoology
University of Cambridge
Downing Street
Cambridge CB2 3EJ, UK


From valderama at gmail.com  Sat Mar 31 14:40:07 2007
From: valderama at gmail.com (Laurent Valdes)
Date: Sat, 31 Mar 2007 14:40:07 +0200
Subject: [R] adding slashes in sql querys
Message-ID: <3ef00e160703310540me0e2ec6q4358b4b509ac19c9@mail.gmail.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070331/d233c43a/attachment.pl 

From Scott.Williams at petermac.org  Sat Mar 31 15:36:04 2007
From: Scott.Williams at petermac.org (Williams Scott)
Date: Sat, 31 Mar 2007 23:36:04 +1000
Subject: [R] strange fisher.test result
Message-ID: <46B75B4A4A45914ABB0901364EFF4A20031AF1@PMC-EMAIL.petermac.org.au>

A simple question - using the following fishers test it appears that the P value is significant, but the CI includes 1. Is this result correct?

 

> data.50p10min <- matrix(c(16,15, 8, 24),nrow=2)

> fisher.test(data.50p10min)

 

        Fisher's Exact Test for Count Data

 

data:  data.50p10min 

p-value = 0.03941

alternative hypothesis: true odds ratio is not equal to 1 

95 percent confidence interval:

  0.9810441 10.7771597 

sample estimates:

odds ratio 

  3.138456 

 

Thanks for comments

 

Scott 

___________________________

Dr Scott Williams MD

Peter MacCallum Cancer Centre

Melbourne Austrailia


From ted.harding at nessie.mcc.ac.uk  Sat Mar 31 16:05:12 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 31 Mar 2007 15:05:12 +0100 (BST)
Subject: [R] strange fisher.test result
In-Reply-To: <46B75B4A4A45914ABB0901364EFF4A20031AF1@PMC-EMAIL.petermac.org.au>
Message-ID: <XFMail.070331150512.ted.harding@nessie.mcc.ac.uk>

On 31-Mar-07 13:36:04, Williams Scott wrote:
> A simple question - using the following fishers test it appears that
> the P value is significant, but the CI includes 1. Is this result
> correct?
> 
>> data.50p10min <- matrix(c(16,15, 8, 24),nrow=2)
> 
>> fisher.test(data.50p10min)
> 
>         Fisher's Exact Test for Count Data
> data:  data.50p10min 
> 
> p-value = 0.03941
> 
> alternative hypothesis: true odds ratio is not equal to 1 
> 
> 95 percent confidence interval:
>   0.9810441 10.7771597 
> 
> sample estimates:
> odds ratio 
>   3.138456 
> 
> Thanks for comments
> Scott

Remember that the distribution on which the Fisher Exact Test is
based is a discrete distribution, so what you might expect based
on experience with continuous distributions will not always be
the case.

In the above case, I can see a possible explanation. The p-value
is the probability of the set of data (more extreme to either side)
such that the probability of (say) the element in row 1, col 1 of
the 2x2 table exceeding an upper critical value is less than or
equal to 0.25, and the same probabiltiy 0.025 for being less than
a lower critical value.

On the other hand, the lower confidence bound for the OR is
the smallest value of OR that would not be rejected by a 1-sided
test at P-value 0.025, and the upper confidence bound for the OR
is the largest value of OR that would not be rejected by a
corresponding 1-sided test at P-value 0.025, on the given data.

There is no reason, with a discrete distribution, for the confidence
interval results for OR to agree exactly with the significance test
of "OR=1".

More than that I cannot say without further information.

If you would post the 2x2 table of your data, it would be possible
to be more specific.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 31-Mar-07                                       Time: 15:05:09
------------------------------ XFMail ------------------------------


From ripley at stats.ox.ac.uk  Sat Mar 31 16:29:21 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 31 Mar 2007 15:29:21 +0100 (BST)
Subject: [R] strange fisher.test result
In-Reply-To: <46B75B4A4A45914ABB0901364EFF4A20031AF1@PMC-EMAIL.petermac.org.au>
References: <46B75B4A4A45914ABB0901364EFF4A20031AF1@PMC-EMAIL.petermac.org.au>
Message-ID: <Pine.LNX.4.64.0703311510001.4892@auk.stats>

The two-sided test of odds-ratio=1 is not necessarily (nor in 
this case) the same thing as an equal-tailed confidence interval: see the 
help page comment

      Two-sided tests are based on the probabilities of the tables, and
      take as 'more extreme' all tables with probabilities less than or
      equal to that of the observed table, the p-value being the sum of
      such probabilities.

> fisher.test(data.50p10min, alternative="greater", conf.level=0.975)

         Fisher's Exact Test for Count Data

data:  data.50p10min
p-value = 0.02727
alternative hypothesis: true odds ratio is greater than 1
97.5 percent confidence interval:
  0.9810441       Inf
sample estimates:
odds ratio
   3.138456

which is not significant at 2.5%, and the two-tailed p-value is not double 
it.  There are other ways to compute confidence intervals, but R's 
fisher.test() gives the 97.5% lower and upper confidence limits.


On Sat, 31 Mar 2007, Williams Scott wrote:

> A simple question - using the following fishers test it appears that the P value is significant, but the CI includes 1. Is this result correct?
>
>
>
>> data.50p10min <- matrix(c(16,15, 8, 24),nrow=2)
>
>> fisher.test(data.50p10min)
>
>
>
>        Fisher's Exact Test for Count Data
>
>
>
> data:  data.50p10min
>
> p-value = 0.03941
>
> alternative hypothesis: true odds ratio is not equal to 1
>
> 95 percent confidence interval:
>
>  0.9810441 10.7771597
>
> sample estimates:
>
> odds ratio
>
>  3.138456


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From kaloytyn at cc.jyu.fi  Sat Mar 31 09:19:00 2007
From: kaloytyn at cc.jyu.fi (kaloytyn at cc.jyu.fi)
Date: Sat, 31 Mar 2007 10:19:00 +0300 (EEST)
Subject: [R] Wikibooks
In-Reply-To: <460C198C.A712.00CB.0@grecc.umaryland.edu>
References: <460C3E51.1010008@vanderbilt.edu>
	<00d701c77260$604303e0$4d908980@gne.windows.gene.com>
	<f8e6ff050703291747s1ec0a309r53341186e27e7798@mail.gmail.com>
	<460C198C.A712.00CB.0@grecc.umaryland.edu>
Message-ID: <1502.130.234.176.117.1175325540.squirrel@webmail2.cc.jyu.fi>

> I think we occasionally think that it is very easy to get information
> because we know how to find the information. This does not mean that other
> people know how to find the answer. It is for this reason that questions
> appear on the listserver that we might think could be easily found from
> other sources.
> John

As a relative n00b user I agree with this. The R organization pages are
not very readable especially if you aren't even sure what you're looking
for (not know the name of a specific function etc.) and pdf:s, search.help
or ? are not the best ways always either. I did not even now there was R
wiki. I couldn't find a link from the R organization pages to it or am I
just blind?

Katja L?ytynoja FM

>
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC,
> University of Maryland School of Medicine Claude D. Pepper OAIC,
> University of Maryland Clinical Nutrition Research Unit, and
> Baltimore VA Center Stroke of Excellence
>
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
> jsorkin at grecc.umaryland.edu
>
>>>> "hadley wickham" <h.wickham at gmail.com> 3/29/2007 7:47 PM >>>
>> Many (perhaps most?) questions on the list are easily answerable simply
>> by
>> checking existing R Docs (Help file/man pages, Intro to R, etc.). Why
>> would
>> a Wiki be more effective in deflecting such questions from the mailing
>> list
>> than them? Why would too helpful R experts be more inclined to refer
>> people
>> to the Wiki than the existing docs? Bottom line: it's psychology at
>> issue
>> here, I think, not the form of the docs.
>
> I agree - and there's also a problem that until the wiki becomes
> useful there's no point referring people to it, and because no one
> visits it, it doesn't get better.
>
> http://www.wikipatterns.com provides some good advice for getting a wiki
> going.
>
> Hadley
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> Confidentiality Statement:
> This email message, including any attachments, is for the so...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From berwin at maths.uwa.edu.au  Sat Mar 31 17:10:15 2007
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Sat, 31 Mar 2007 23:10:15 +0800
Subject: [R] Wikibooks
In-Reply-To: <1502.130.234.176.117.1175325540.squirrel@webmail2.cc.jyu.fi>
References: <460C3E51.1010008@vanderbilt.edu>
	<00d701c77260$604303e0$4d908980@gne.windows.gene.com>
	<f8e6ff050703291747s1ec0a309r53341186e27e7798@mail.gmail.com>
	<460C198C.A712.00CB.0@grecc.umaryland.edu>
	<1502.130.234.176.117.1175325540.squirrel@webmail2.cc.jyu.fi>
Message-ID: <20070331231015.58339b06@berwin5>

G'day Kaltja,

On Sat, 31 Mar 2007 10:19:00 +0300 (EEST)
kaloytyn at cc.jyu.fi wrote:

> [...] I did not even now there was R wiki. I couldn't find a link
> from the R organization pages to it or am I just blind?

If you talk about CRAN (e.g. http://cran.r-project.org/) then no, but
if you really talk about "The R Project for Statistical Computing"
pages, i.e. http://www.r-project.org/ then yes. :) 

On www.r-project.org you have the following links under documentation:
	Manuals
	FAQs
	Newsletter
	Wiki
	Books
	Other

Note that CRAN (and mirrors) have a link called "R Homepage" under
"About R".

Cheers,
	
	Berwin


From charle at cmu.edu  Sat Mar 31 17:13:26 2007
From: charle at cmu.edu (Christopher A. Harle)
Date: Sat, 31 Mar 2007 11:13:26 -0400 (EDT)
Subject: [R] lda() function and Fisher's LDA
Message-ID: <2311.128.237.250.158.1175354006.squirrel@128.237.250.158>

Hi, 

I am using the function lda() from MASS for finding reduced-dimensional representations of a datset.  In reading various texts to compare LDA with Fisher's  LDA approach (including Ripley's Modern Applied Statistics with S-Plus), it is still unclear to me whether or not they produce the same classification results, how they are related, and which is being performed by function lda().  

One interpretation I have is that Fisher's LDA rule for classification is the same as the probabilistic Bayes approach to LDA (assumes classes are distributed Gaussian with equal covariance) when the prior probabilities are equal.  If this is the case, then function lda() would be the same as Fisher's method when priors are equal.  Is this correct?

Thanks,

Chris Harle

H. John Heinz III School of Public Policy & Management
Carnegie Mellon University


From ggrothendieck at gmail.com  Sat Mar 31 19:17:06 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 31 Mar 2007 13:17:06 -0400
Subject: [R] adding slashes in sql querys
In-Reply-To: <3ef00e160703310540me0e2ec6q4358b4b509ac19c9@mail.gmail.com>
References: <3ef00e160703310540me0e2ec6q4358b4b509ac19c9@mail.gmail.com>
Message-ID: <971536df0703311017j53dc3c69y8bf091480a86eb82@mail.gmail.com>

Try this:

'my dog named "Spot" and my cat named "Kitty" fight like cats and dogs'

On 3/31/07, Laurent Valdes <valderama at gmail.com> wrote:
> Hi everybody,
>
> I'm doing a sqlSave() in R, to insert a big data frame of 10000 rows.
> However, there is problems, since several rows contains quotations marks,
> that can leave inserts buggy.
> I would like to find a way to add slashes in front of these quotation marks.
>
> Best regards,
>
> Laurent
>
>
> --
> We are drowning in information, but starved for knowledge.
> ?Germain? @<http://www.le-valdo.com>
>
>        [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From Achim.Zeileis at wu-wien.ac.at  Sat Mar 31 20:05:35 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Sat, 31 Mar 2007 20:05:35 +0200 (CEST)
Subject: [R] Linear model and time series
In-Reply-To: <460DFE92.7060805@gfz-potsdam.de>
Message-ID: <Pine.LNX.4.44.0703312003380.20217-100000@disco.wu-wien.ac.at>

On Sat, 31 Mar 2007, Andre Jung wrote:

> Dear all,
>
> I have three timeseries Uts, Vts, Wts. The relation between the time
> series can be expressed as
>
> Uts = x Vts + y Wts + residuals
>
> How would I feed this to lm() to evaluate the unknowns x and y?

If the time series are aligned (and univariate) you can just do
  lm(Uts ~ Vts + Wts)
If not, have a look at the "dynlm" and/or "dyn" packages.
Z

> Thanks,
> andre
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From maechler at stat.math.ethz.ch  Sat Mar 31 21:22:05 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 31 Mar 2007 21:22:05 +0200
Subject: [R] Using split() several times in a row?
In-Reply-To: <470978.64452.qm@web39708.mail.mud.yahoo.com>
References: <7cb007bd0703300818h742f7ef3wcee5f23a424b4546@mail.gmail.com>
	<470978.64452.qm@web39708.mail.mud.yahoo.com>
Message-ID: <17934.46301.273569.436369@stat.math.ethz.ch>

>>>>> "SteT" == Stephen Tucker <brown_emu at yahoo.com>
>>>>>     on Fri, 30 Mar 2007 18:41:39 -0700 (PDT) writes:

  [..]

    SteT> For dates, I usually store them as "POSIXct" classes
    SteT> in data frames, but according to Gabor Grothendieck
    SteT> and Thomas Petzoldt's R Help Desk article
    SteT> <http://cran.r-project.org/doc/Rnews/Rnews_2004-1.pdf>,
    SteT> I should probably be using "chron" date and times...

I don't think you should (and I doubt Gabor and Thomas would
recommend this in every case):

POSIXct (and 'POSIXlt', 'POSIXt' & 'Date') are part of standard R,
and whereas they may seem not as convenient in all cases as "chron"
etc, I'd rather recommed to stick to them in such a case.

    SteT> Nonetheless, POSIXct casses are what I know so I can
    SteT> show you that to get the month out of your column
    SteT> (replace "8.29.97" with your variable), you can do the
    SteT> following:

    SteT> month = format(strptime("8.29.97",format="%m.%d.%y"),format="%m")

    SteT> Or,
    SteT> month = as.data.frame(strsplit("8.29.97","\\."))[1,]

  [......etc..very....useful..advice............]


From tn_1342003 at yahoo.com  Sat Mar 31 19:59:37 2007
From: tn_1342003 at yahoo.com (tariq algazzale)
Date: Sat, 31 Mar 2007 10:59:37 -0700 (PDT)
Subject: [R] Bootstrap for Time Series
Message-ID: <14783.20000.qm@web36708.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070331/5f004937/attachment.pl 

From ggrothendieck at gmail.com  Sat Mar 31 23:43:58 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 31 Mar 2007 17:43:58 -0400
Subject: [R] Using split() several times in a row?
In-Reply-To: <17934.46301.273569.436369@stat.math.ethz.ch>
References: <7cb007bd0703300818h742f7ef3wcee5f23a424b4546@mail.gmail.com>
	<470978.64452.qm@web39708.mail.mud.yahoo.com>
	<17934.46301.273569.436369@stat.math.ethz.ch>
Message-ID: <971536df0703311443p4352b6a0m12972fe8c651a7bb@mail.gmail.com>

On 3/31/07, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> >>>>> "SteT" == Stephen Tucker <brown_emu at yahoo.com>
> >>>>>     on Fri, 30 Mar 2007 18:41:39 -0700 (PDT) writes:
>
>  [..]
>
>    SteT> For dates, I usually store them as "POSIXct" classes
>    SteT> in data frames, but according to Gabor Grothendieck
>    SteT> and Thomas Petzoldt's R Help Desk article
>    SteT> <http://cran.r-project.org/doc/Rnews/Rnews_2004-1.pdf>,
>    SteT> I should probably be using "chron" date and times...
>
> I don't think you should (and I doubt Gabor and Thomas would
> recommend this in every case):
>
> POSIXct (and 'POSIXlt', 'POSIXt' & 'Date') are part of standard R,
> and whereas they may seem not as convenient in all cases as "chron"
> etc, I'd rather recommed to stick to them in such a case.

There is one change that has occurred since the article that in my
mind would let you safely use POSIX but its pretty drastic.  At the time
of the article you could not set the time zone to GMT in the R process
on Windows but now you can do this:

Sys.putenv(TZ = "GMT")

and you can also change it back like this:

Sys.putenv(TZ = "")

Since the problem is that you never can be sure which time zone the
time is interpreted in within various function (although you can be pretty
sure its either the local time zone or GMT) by setting the process to
GMT you make the two alternatives the same so it no longer matters.

Short of the above, the recommendations of the article should be followed.
Its not a matter of convenience.  Its a matter of being error prone
and introducing
subtle time-zone related errors into your code which are very hard to track
down or worse, even realize that you have.

Those who claim that its not a problem simply have not used dates and times
enough or they would not say that.  I have seen posters make such comments
on this list only later to run into subtle time zone problems that they never
would have had had they followed the advice in the article.

I've used R and dates a lot and therefore have made a lot of programming errors
and these recommendations come from bitter experience looking back to see
how I could have avoided them.


