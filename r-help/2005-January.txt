From younko at uiuc.edu  Sat Jan  1 01:55:59 2005
From: younko at uiuc.edu (Ko,Younhee)
Date: Fri, 31 Dec 2004 18:55:59 -0600
Subject: [R] Export the R object
Message-ID: <ecae682e.b7935efc.81df900@expms3.cites.uiuc.edu>

Thanks very much. 
It was exactly what I want.

>From next time, I will try to figure out more by myself.:-)
Thanks again.

Younhee. 

---- Original message ----
>Date: Fri, 31 Dec 2004 15:54:11 -0500
>From: "Roger D. Peng" <rpeng at jhsph.edu>  
>Subject: Re: [R] Export the R object  
>To: "Ko,Younhee" <younko at uiuc.edu>
>Cc: R-help at stat.math.ethz.ch
>
>If you want to export data frame, you can use write.table() 
to write out a CSV 
>(comma separated value) file, which you can then read into 
Excel.
>
>-roger
>
>Ko,Younhee wrote:
>> Hi, 
>> 
>> I just have a quick question.
>> If I got some result as the result of R, how can I export 
>> the result object?
>> 
>> I mean, if I want to use the result object in Excel or 
other 
>> program in order to more specific investigation, how can 
I 
>> export it?
>> 
>> If I just list the result and copy, 
>> The result is like this..
>> 
>> 
>> [586] "BB170029A10B06" "BB170029A20E06"
>> 
>> First column, automatically show the number of result and 
>> other result also include the "".
>> 
>> If I want to use this result, I have to manipulate the 
>> result by myself(I mean e.g remove " and remove [586] 
like 
>> this way)?????
>> 
>> Or there is any good way to export this result object to 
>> other program?
>> 
>> Please help me. 
>> 
>> Thanks in advance.
>> ========================
>> Younhee Ko(younko at uiuc.edu)
>> 
>> http://comedu.korea.ac.kr/~unygo
>> contact : 217-417-4868
>> Graduate Student in Dept. of Computer Science
>> University of Illinois at Urbana-Champaign
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-
project.org/posting-guide.html
>> 
========================
Younhee Ko(younko at uiuc.edu)

http://comedu.korea.ac.kr/~unygo
contact : 217-417-4868
Graduate Student in Dept. of Computer Science
University of Illinois at Urbana-Champaign



From jwd at surewest.net  Sat Jan  1 03:04:28 2005
From: jwd at surewest.net (John Dougherty)
Date: Fri, 31 Dec 2004 18:04:28 -0800
Subject: [R] Preferred reference for R and time-series?
Message-ID: <200412311804.28147.jwd@surewest.net>

Does someone have recommendations for their preferred references for 
time-series analysis in R?  I have had interesting basic results but I would 
like to read a systematic discussion of using R for TSA.

Thanks
JWDougherty



From mi2kelgrum at yahoo.com  Sat Jan  1 09:56:06 2005
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Sat, 1 Jan 2005 00:56:06 -0800 (PST)
Subject: [R] Supressing empty sections with Sweave
In-Reply-To: <41D5593E.5020400@vanderbilt.edu>
Message-ID: <20050101085606.60386.qmail@web60204.mail.yahoo.com>

Frank Harrel proposed the following solution:

\ifnum\Sexpr{x}=1
\section{}
. . .
\fi

and it works. Amazingly simple!

cheers,
Mikkel

--- Frank E Harrell Jr <f.harrell at vanderbilt.edu>
wrote:

> Mikkel Grum wrote:
> > Dear useRs,
> > 
> > I'm writing regular survey reports using Sweave.
> Each
> > report has several sections along the lines of:
> > 
> > \section*{Disease X}
> > 
> > <<MapX,fig=TRUE,echo=FALSE>>=
> > image(vectorx,vectory,matrixz)
> > 
> > @
> > 
> > Notes with or without Sexpr{a}.
> > 
> > \vfill
> > 
> > \pagebreak
> > 
> > \section*{Disease Y}
> > <<MapY,fig=TRUE,echo=FALSE>>=
> > ...etc.
> > 
> > 
> > Often one or more of the diseases is not observed
> (all
> > values in matrixz are 0), in which case I would
> prefer
> > not to display the section at all.  Does any one
> no
> > whether it is possible automate this with Sweave?
> > 
> > Mikkel



From ggrothendieck at myway.com  Sat Jan  1 13:49:36 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 1 Jan 2005 12:49:36 +0000 (UTC)
Subject: [R] New Year Wish List
Message-ID: <loom.20050101T134856-402@post.gmane.org>


Congrats to all those who have contributed to R over the
past year.  As with last year, for New Year I would like to
list the top 10 features I would like to see in R.   The first 
three are the most important.

1. Scripting

   With this feature it would be possible to call R like
   this as a filter:

	R -f myprog.R infile1.txt infile2.txt > outfile.txt
   or
	prog1 | R -f myprog.R | prog2

   making it convenient to replace many instances of
   awk/perl/batch with R.  This could also be used by R itself
   to eliminate dependence on perl and UNIX tools in the
   package building scripts.  There are workarounds already but
   we need something that works smoothly.  This also relates to
   the discussions on the mailing lists on streaming.

2. Package Building Tools 

   The future of R will increasingly depend on the breadth and
   quality of the libraries so its important to have good
   package building tools and make them solid on all platforms.
   This breaks down into:

	a. installation.  The package building tools could
	   themselves be a package and installing them should 
	   be as easy as installing any other package.  Dependence 
	   on other software should be minimized.  Rewrite R CMD 
	   scripts and texi2dvi in R.  Eliminate dependence on perl, 
	   UNIX tools and path.  

	b. use.  Fix problem with .Rbuildignore and, on
	   Windows, the generation of vignettes.  Moving this 
	   all to R would likely make it easier to use, too.  
	   Need a brief document that discusses the flow of 
 	   issuing the R CMD commands.

3. Naive datetime class.  

   The chron package already provides a naive (i.e. time zone
   free) date time class but we need something in the standard
   packages.  Fortunately, the addition of the Date class has 
   solved 80% of the problem here but the remainder still needs 
   to be addressed.

4. Sourceforge-like Support for Subprojects.

  This refers to groupware support for subproject development.   
  The Lua and Ruby communities have done this with LuaForge.net and
  RubyForge.org.

5. Change Log

   There needs to be a standardized format to communicate changes
   in packages.  

6. Bug Reporting

   Need to find better web-based bug reporting software.

7. User Requirements

   Some way of tracking user requirements, wishlist, etc. This is
   related to last point.  The key point here is that bug tracking 
   needs to be expanded into issue tracking including design issues, 
   features requests, etc.

8. Improve Visibility of Future Plans

   Despite high volume mailing lists there is very little discussion
   of the future of R.

9. \& Back Reference

   We have \1, \2, ... but no \&.

10. Miscellaneous Bugs.  

   a.  NextMethod.  NextMethod works inconsistently.  I have found 
       each time one uses it one must use trial and error to figure out
       which args will be passed or not. [1] 

   b.  Regexps \b and \B do not always work properly. [2]

References

[1]
https://www.stat.math.ethz.ch/pipermail/r-devel/2004-October/031150.html
https://www.stat.math.ethz.ch/pipermail/r-devel/2004-October/031155.html

[2]
https://www.stat.math.ethz.ch/pipermail/r-devel/2004-December/031580.html
https://www.stat.math.ethz.ch/pipermail/r-devel/2004-December/031610.html



From andy_liaw at merck.com  Sat Jan  1 14:52:35 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 1 Jan 2005 08:52:35 -0500
Subject: [R] install.packages() for local source file
Message-ID: <3A822319EB35174CA3714066D590DCD50994E480@usrymx25.merck.com>

Please read ?install.packages.  It's most definitely not a bug for a
function that works as documented.  If you want, you might take the source
for install.packages and strip it down to do what you want, say something
called installLocalPackages.

Andy 

> From: Paul Roebuck
> 
> Wish to install a local source package on Un*x platform from
> within R. Same thing as I can accomplish from cmdline as
> 
> $ export R_LIBS=~/R/library
> $ cd /path/to/pkg
> $ R CMD INSTALL -l $R_LIBS <pkgname>
> 
> 
> So, how do you go about this anyway?
> And isn't this a bug in 'install.packages'?
> 
> -------
> $ R
> 
> R : Copyright 2004, The R Foundation for Statistical Computing
> Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3
> 
> > file.pkg <- "mypkg_0.1.tar.gz"
> > path.pkg <- file.path(path.expand("~"), "cvknn", file.pkg)
> > file.exists(path.pkg)
> [1] TRUE
> > uri.pkg <- paste("file://", path.pkg, sep = "")
> > install.packages(contriburl = uri.pkg, lib = Sys.getenv("R_LIBS"))
> Error in file.info(x) : Object "tmpd" not found
> > traceback()
> 4: file.info(x)
> 3: dirTest(destdir)
> 2: download.packages(pkgs, destdir = tmpd, available = available,
>        contriburl = contriburl, method = method)
> 1: install.packages(contriburl = uri.pkg, lib = Sys.getenv("R_LIBS"))
> > version
>          _
> platform sparc-sun-solaris2.9
> arch     sparc
> os       solaris2.9
> system   sparc, solaris2.9
> status
> major    1
> minor    9.0
> year     2004
> month    04
> day      12
> language R
> 
> 
> ----------------------------------------------------------
> SIGSIG -- signature too long (core dumped)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jfox at mcmaster.ca  Sat Jan  1 15:12:08 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 1 Jan 2005 09:12:08 -0500
Subject: [R] Use of expand.model.frame()
Message-ID: <20050101141207.VSQQ25979.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear list members,

I've encountered a problem using expand.model.frame() with a model specified
without an explicit data argument. To illustrate (R 2.0.1 under Windows XP):

    > x <- rnorm(10)
    > y <- x + rnorm(10)
    > mod <- lm(y ~ x)
    > z <- 1:10
    > expand.model.frame(mod, "z")
    Error in eval(expr, envir, enclos) : Object "y" not found
    > environment(formula(mod))
    <environment: R_GlobalEnv>
    >

But when data is specified in the call to lm(), I get the expected
behaviour:

    > DF <- data.frame(x, y, z)
    > remove(x, y, z)
    > mod <- lm(y ~ x, data=DF)
    > expand.model.frame(mod, "z")
                y          x  z
    1   1.0321783 -0.8145171  1
    2  -1.7639244 -0.5965655  2
    3  -0.8150426 -1.4833768  3
    4   0.6047758  0.2030853  4
    5  -1.9870830 -1.5559511  5
    6  -0.4807146 -1.0751953  6
    7   1.3683736  0.1610050  7
    8   0.9323799  0.3537983  8
    9  -1.0797075 -0.4218457  9
    10 -3.0528776 -1.9750668 10
    > environment(formula(mod))
    <environment: R_GlobalEnv>
    

>From ?expand.model.frame:

    "Usage
    expand.model.frame(model, extras,
                       envir = environment(formula(model)),
                       na.expand = FALSE)

    Arguments
    model a fitted model
    extras one-sided formula or vector of character strings describing new
variables to be added
    envir an environment to evaluate things in
    na.expand logical; see below"
    
So, if "things" are evaluated by default in environment(formula(model)), why
does the first example fail and the second one work? Obviously, I'm not
understanding something properly here.

Any help would be appreciated -- either to get expand.model.frame() to work
or to suggest an alternative approach.

Happy new year to all.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox



From B.Rowlingson at lancaster.ac.uk  Sat Jan  1 15:21:02 2005
From: B.Rowlingson at lancaster.ac.uk (Baz)
Date: Sat, 01 Jan 2005 14:21:02 +0000
Subject: [R] New Year Wish List
In-Reply-To: <loom.20050101T134856-402@post.gmane.org>
References: <loom.20050101T134856-402@post.gmane.org>
Message-ID: <41D6B1CE.8030808@lancaster.ac.uk>

Gabor Grothendieck wrote:
> Congrats to all those who have contributed to R over the
> past year.  As with last year, for New Year I would like to
> list the top 10 features I would like to see in R.   The first 
> three are the most important.

  My wish for R is some sort of timestamping on objects, so I know when 
they were created or modified.

  This would then enable the creation of a 'make'-like system in R, 
where the 'target' is some output of an analysis (eg the fit of a linear 
model) and the dependencies are the input data.

  For a complex problem, you might have a target that depends on many 
other things, and you dont want to have to redo expensive operations (eg 
MCMC runs, large simulations) if nothing that they depend on has 
changed. The grand idea is to have a single specification of the problem 
(which would be an R object of class 'make' perhaps) and then you can 
just do make(foo) to do all the required code to get the result.

  This might be a very useful method for transferring analyses between 
people.

  I did have a hack at putting timestamps on R objects by mucking with 
the C code that created new objects, and adding an 
attribute('timestamp'). It superficially worked, but R started to fall 
to pieces since objects stopped being identical to copies of themselves. 
Oops. it clearly needs attaching at a lower level, but I'm not sure 
where or how much of a performance hit it might be.

  End of wishlist.

Baz



From ggrothendieck at myway.com  Sat Jan  1 16:26:24 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 1 Jan 2005 15:26:24 +0000 (UTC)
Subject: [R] Use of expand.model.frame()
References: <20050101141207.VSQQ25979.tomts25-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <loom.20050101T161738-66@post.gmane.org>

John Fox <jfox <at> mcmaster.ca> writes:

: 
: Dear list members,
: 
: I've encountered a problem using expand.model.frame() with a model specified
: without an explicit data argument. To illustrate (R 2.0.1 under Windows XP):
: 
:     > x <- rnorm(10)
:     > y <- x + rnorm(10)
:     > mod <- lm(y ~ x)
:     > z <- 1:10
:     > expand.model.frame(mod, "z")
:     Error in eval(expr, envir, enclos) : Object "y" not found
:     > environment(formula(mod))
:     <environment: R_GlobalEnv>
:     >
: 
: But when data is specified in the call to lm(), I get the expected
: behaviour:
: 
:     > DF <- data.frame(x, y, z)
:     > remove(x, y, z)
:     > mod <- lm(y ~ x, data=DF)
:     > expand.model.frame(mod, "z")
:                 y          x  z
:     1   1.0321783 -0.8145171  1
:     2  -1.7639244 -0.5965655  2
:     3  -0.8150426 -1.4833768  3
:     4   0.6047758  0.2030853  4
:     5  -1.9870830 -1.5559511  5
:     6  -0.4807146 -1.0751953  6
:     7   1.3683736  0.1610050  7
:     8   0.9323799  0.3537983  8
:     9  -1.0797075 -0.4218457  9
:     10 -3.0528776 -1.9750668 10
:     > environment(formula(mod))
:     <environment: R_GlobalEnv>
: 
: >From ?expand.model.frame:
: 
:     "Usage
:     expand.model.frame(model, extras,
:                        envir = environment(formula(model)),
:                        na.expand = FALSE)
: 
:     Arguments
:     model a fitted model
:     extras one-sided formula or vector of character strings describing new
: variables to be added
:     envir an environment to evaluate things in
:     na.expand logical; see below"
: 
: So, if "things" are evaluated by default in environment(formula(model)), why
: does the first example fail and the second one work? Obviously, I'm not
: understanding something properly here.
: 
: Any help would be appreciated -- either to get expand.model.frame() to work
: or to suggest an alternative approach.
: 
: Happy new year to all.
: 
: John

Look at the source of expand.model.frame.

The second line of expand.model.frame uses the data component of mod$call
and if its not there expand.model.frame chokes.  As a workaround check
if its missing and supply it yourself:

if (is.null(mod$call$data)) mod$call$data <- environment(formula(mod))
expand.model.frame(mod, "z")

expand.model.frame should probably be doing that itself.



From jfox at mcmaster.ca  Sat Jan  1 17:01:35 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 1 Jan 2005 11:01:35 -0500
Subject: [R] Use of expand.model.frame()
In-Reply-To: <loom.20050101T161738-66@post.gmane.org>
Message-ID: <20050101160133.LIXW1899.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Gabor,

That solves the problem -- thank you very  much for the quick response.

(I did look at the source for expand.model.frame but didn't isolate the
problem, which seems clear now that you've pointed it out.)

Regards,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor 
> Grothendieck
> Sent: Saturday, January 01, 2005 10:26 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Use of expand.model.frame()
> 
> John Fox <jfox <at> mcmaster.ca> writes:
> 
> : 
> : Dear list members,
> : 
> : I've encountered a problem using expand.model.frame() with 
> a model specified
> : without an explicit data argument. To illustrate (R 2.0.1 
> under Windows XP):
> : 
> :     > x <- rnorm(10)
> :     > y <- x + rnorm(10)
> :     > mod <- lm(y ~ x)
> :     > z <- 1:10
> :     > expand.model.frame(mod, "z")
> :     Error in eval(expr, envir, enclos) : Object "y" not found
> :     > environment(formula(mod))
> :     <environment: R_GlobalEnv>
> :     >
> : 
> : But when data is specified in the call to lm(), I get the expected
> : behaviour:
> : 
> :     > DF <- data.frame(x, y, z)
> :     > remove(x, y, z)
> :     > mod <- lm(y ~ x, data=DF)
> :     > expand.model.frame(mod, "z")
> :                 y          x  z
> :     1   1.0321783 -0.8145171  1
> :     2  -1.7639244 -0.5965655  2
> :     3  -0.8150426 -1.4833768  3
> :     4   0.6047758  0.2030853  4
> :     5  -1.9870830 -1.5559511  5
> :     6  -0.4807146 -1.0751953  6
> :     7   1.3683736  0.1610050  7
> :     8   0.9323799  0.3537983  8
> :     9  -1.0797075 -0.4218457  9
> :     10 -3.0528776 -1.9750668 10
> :     > environment(formula(mod))
> :     <environment: R_GlobalEnv>
> : 
> : >From ?expand.model.frame:
> : 
> :     "Usage
> :     expand.model.frame(model, extras,
> :                        envir = environment(formula(model)),
> :                        na.expand = FALSE)
> : 
> :     Arguments
> :     model a fitted model
> :     extras one-sided formula or vector of character strings 
> describing new
> : variables to be added
> :     envir an environment to evaluate things in
> :     na.expand logical; see below"
> : 
> : So, if "things" are evaluated by default in 
> environment(formula(model)), why
> : does the first example fail and the second one work? 
> Obviously, I'm not
> : understanding something properly here.
> : 
> : Any help would be appreciated -- either to get 
> expand.model.frame() to work
> : or to suggest an alternative approach.
> : 
> : Happy new year to all.
> : 
> : John
> 
> Look at the source of expand.model.frame.
> 
> The second line of expand.model.frame uses the data component 
> of mod$call and if its not there expand.model.frame chokes.  
> As a workaround check if its missing and supply it yourself:
> 
> if (is.null(mod$call$data)) mod$call$data <- 
> environment(formula(mod)) expand.model.frame(mod, "z")
> 
> expand.model.frame should probably be doing that itself.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Sat Jan  1 17:23:45 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 1 Jan 2005 16:23:45 +0000 (UTC)
Subject: [R] New Year Wish List
References: <loom.20050101T134856-402@post.gmane.org>
	<41D6B1CE.8030808@lancaster.ac.uk>
Message-ID: <loom.20050101T171958-259@post.gmane.org>

Baz <B.Rowlingson <at> lancaster.ac.uk> writes:

: 
: Gabor Grothendieck wrote:
: > Congrats to all those who have contributed to R over the
: > past year.  As with last year, for New Year I would like to
: > list the top 10 features I would like to see in R.   The first 
: > three are the most important.
: 
:   My wish for R is some sort of timestamping on objects, so I know when 
: they were created or modified.
: 
:   This would then enable the creation of a 'make'-like system in R, 
: where the 'target' is some output of an analysis (eg the fit of a linear 
: model) and the dependencies are the input data.
: 
:   For a complex problem, you might have a target that depends on many 
: other things, and you dont want to have to redo expensive operations (eg 
: MCMC runs, large simulations) if nothing that they depend on has 
: changed. The grand idea is to have a single specification of the problem 
: (which would be an R object of class 'make' perhaps) and then you can 
: just do make(foo) to do all the required code to get the result.
: 
:   This might be a very useful method for transferring analyses between 
: people.
: 
:   I did have a hack at putting timestamps on R objects by mucking with 
: the C code that created new objects, and adding an 
: attribute('timestamp'). It superficially worked, but R started to fall 
: to pieces since objects stopped being identical to copies of themselves. 
: Oops. it clearly needs attaching at a lower level, but I'm not sure 
: where or how much of a performance hit it might be.
: 
:   End of wishlist.


Not sure if this is good enough but there have been some attempts
along this line but without modifying R.  One could manually store the
timestamp in an attribute, in a global list with one component per
variable that you want to track or save each variable
manually in an .rda file using that file's stamp.  Its possible
to hide some of it to avoid cluttering your code.  See 
the following:

https://www.stat.math.ethz.ch/pipermail/r-help/2004-October/056969.html
http://maths.newcastle.edu.au/~rking/R/help/03a/6412.html
http://tolstoy.newcastle.edu.au/R/help/04/05/0242.html



From digitalpenis at bluebottle.com  Sat Jan  1 17:18:56 2005
From: digitalpenis at bluebottle.com (digitalpenis@bluebottle.com)
Date: Sat, 1 Jan 2005 18:18:56 +0200
Subject: [R] find parameters for a gamma distribution
Message-ID: <20050101161856.GA15551@rockhopper.sanae.ant>

hello,

i have just started exploring R as an alternative to matlab for data analysis. so far everything is _very_ promising. i have a question though regarding parameter estimation. i have some data which, from a histogram plot, appears to arise from a gamma distribution. i gather that you can fit the data to the distribution using glm(). i am just not quite sure how this is done in practice... so here is a simple example with artificial data:

d <- rgamma(100000, 20, scale = 2)
h <- hist(d, breaks = c(seq(10, 80, 2), 100))

H <- data.frame(x = h$mids, y = h$density)

g <- glm(y ~ x, data = H, family = Gamma)
summary(g)

Call:
glm(formula = y ~ x, family = Gamma, data = H)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.8654  -2.0887  -0.7685   0.7147   1.4508  

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  30.4758    26.7258   1.140    0.262
x             1.0394     0.6825   1.523    0.137

(Dispersion parameter for Gamma family taken to be 1.343021)

    Null deviance: 119.51  on 35  degrees of freedom
Residual deviance: 116.28  on 34  degrees of freedom
AIC: -260.49

Number of Fisher Scoring iterations: 7

now i suppose that the estimates parameters are:

	shape = 30.4758
	scale = 1.0394

am i interpreting the output correctly? and, if so, why are these estimates so poor? i would, perhaps naively, expected the parameters from an artificial sample like this to be pretty good.

my apologies if i am doing something stupid here but my statistics capabilties are rather limited!

best regards,
andrew collier.



From h.wickham at gmail.com  Sat Jan  1 17:45:19 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Sat, 1 Jan 2005 10:45:19 -0600
Subject: [R] New Year Wish List
In-Reply-To: <loom.20050101T134856-402@post.gmane.org>
References: <loom.20050101T134856-402@post.gmane.org>
Message-ID: <f8e6ff0505010108452ef28e35@mail.gmail.com>

> 4. Sourceforge-like Support for Subprojects.
> 
>   This refers to groupware support for subproject development.
>   The Lua and Ruby communities have done this with LuaForge.net and
>   RubyForge.org.

I think both are powered by http://gforge.org/ - it probably wouldn't
be difficult to set up something similar for R.

> 6. Bug Reporting
> 
>    Need to find better web-based bug reporting software.
> 
> 7. User Requirements
> 
>    Some way of tracking user requirements, wishlist, etc. This is
>    related to last point.  The key point here is that bug tracking
>    needs to be expanded into issue tracking including design issues,
>    features requests, etc.

Since the R sources have moved to svn, an obvious candidate for this
would seem to be trac (http://edgewall.com/trac/), which provides
issue tracking and integrates very nicely with svn.  Its wiki-type
facilities could potentially be used for future planning as well.

Hadley



From ggrothendieck at myway.com  Sat Jan  1 19:05:44 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 1 Jan 2005 18:05:44 +0000 (UTC)
Subject: [R] find parameters for a gamma distribution
References: <20050101161856.GA15551@rockhopper.sanae.ant>
Message-ID: <loom.20050101T190313-858@post.gmane.org>

 <digitalpenis <at> bluebottle.com> writes:

: 
: hello,
: 
: i have just started exploring R as an alternative to matlab for data 
analysis. so far everything is _very_
: promising. i have a question though regarding parameter estimation. i have 
some data which, from a
: histogram plot, appears to arise from a gamma distribution. i gather that 
you can fit the data to the
: distribution using glm(). i am just not quite sure how this is done in 
practice... so here is a simple
: example with artificial data:
: 
: d <- rgamma(100000, 20, scale = 2)
: h <- hist(d, breaks = c(seq(10, 80, 2), 100))
: 
: H <- data.frame(x = h$mids, y = h$density)
: 
: g <- glm(y ~ x, data = H, family = Gamma)
: summary(g)
: 
: Call:
: glm(formula = y ~ x, family = Gamma, data = H)
: 
: Deviance Residuals: 
:     Min       1Q   Median       3Q      Max  
: -3.8654  -2.0887  -0.7685   0.7147   1.4508  
: 
: Coefficients:
:             Estimate Std. Error t value Pr(>|t|)
: (Intercept)  30.4758    26.7258   1.140    0.262
: x             1.0394     0.6825   1.523    0.137
: 
: (Dispersion parameter for Gamma family taken to be 1.343021)
: 
:     Null deviance: 119.51  on 35  degrees of freedom
: Residual deviance: 116.28  on 34  degrees of freedom
: AIC: -260.49
: 
: Number of Fisher Scoring iterations: 7
: 
: now i suppose that the estimates parameters are:
: 
: 	shape = 30.4758
: 	scale = 1.0394
: 
: am i interpreting the output correctly? and, if so, why are these estimates 
so poor? i would, perhaps
: naively, expected the parameters from an artificial sample like this to be 
pretty good.
: 
: my apologies if i am doing something stupid here but my statistics 
capabilties are rather limited!

library(MASS)
?fitdistr
example(fitdistr) # note the gamma example



From emelamud at yahoo.com  Sat Jan  1 19:12:03 2005
From: emelamud at yahoo.com (Eugene)
Date: Sat, 1 Jan 2005 10:12:03 -0800 (PST)
Subject: [R] Export the R object
In-Reply-To: <ecae682e.b7935efc.81df900@expms3.cites.uiuc.edu>
Message-ID: <20050101181203.18749.qmail@web50703.mail.yahoo.com>

If you do a lot of exports from R to excel, you might want to use 
a wrapper function around write.table(). I got tired of using 
write.table(), go to excel, select file and load routine. 

My solution to the problem is to create a simple function that
opens excel spreadsheet automatically from R. The function
exports R data.frame into temp file, and calls excel with temp
filename. It saved me a lot of time. 
 
An example: 
> excel(mymydataframe

Here is the function: ( you might have to change path to the excel 
executable )

excel <- function(x) {
   #create a temp filename
  tmpfilename <-paste(tempfile(c("abs")),".csv",sep="");

   #export table in comma seperated by ,
    write.table(x, file=tmpfilename, sep=",");
	         
   #run excel as a background process
   system( paste('"C:\PROGRAM FILES\MICROSOFT OFFICE\OFFICE\EXCEL.EXE"', tmpfilename ),
wait=FALSE);

   #sleep 5 seconds, this is done to insure that R doesn't delete the file before excel opens it
    Sys.sleep(5);
       unlink(tmpfilename);
}

Hope this helps. 
Eugene MeMelamud

--- "KoKooYounhee<yoyounkoiuiucdeduwrote:

> Thanks very much. 
> It was exactly what I want.
> 
> >From next time, I will try to figure out more by myself.:-)
> Thanks again.
> 
> YoYounhee
> 
> ---- Original message ----
> >Date: Fri, 31 Dec 2004 15:54:11 -0500
> >From: "Roger D. PePeng<rprpenghjhsphdedu 
> >Subject: Re: [R] Export the R object  
> >To: "KoKooYounhee<yoyounkoiuiucdedu
> >Cc: R-help at stat.math.etethzh
> >
> >If you want to export data frame, you can use write.table() 
> to write out a CSCSV
> >(comma separated value) file, which you can then read into 
> Excel.
> >
> >-roger
> >
> >KoKooYounheerote:
> >> Hi, 
> >> 
> >> I just have a quick question.
> >> If I got some result as the result of R, how can I export 
> >> the result object?
> >> 
> >> I mean, if I want to use the result object in Excel or 
> other 
> >> program in order to more specific investigation, how can 
> I 
> >> export it?
> >> 
> >> If I just list the result and copy, 
> >> The result is like this..
> >> 
> >> 
> >> [586] "BB170029A10B06" "BB170029A20E06"
> >> 
> >> First column, automatically show the number of result and 
> >> other result also include the "".
> >> 
> >> If I want to use this result, I have to manipulate the 
> >> result by myself(I mean e.g remove " and remove [586] 
> like 
> >> this way)?????
> >> 
> >> Or there is any good way to export this result object to 
> >> other program?
> >> 
> >> Please help me. 
> >> 
> >> Thanks in advance.
> >> ========================
> >> YoYounheeoKooyounkoiuiucdedu
> >> 
> >> hthttp/cocomeduokoreac.krkrununygo> >> contact : 217-417-4868
> >> Graduate Student in Dept. of Computer Science
> >> University of Illinois at Urbana-Champaign
> >> 
> >> ______________________________________________
> >> R-help at stat.math.etethzh mailing list
> >> hthttps/stat.etethzh/mailman/lilistinfo-help
> >> PLEASE do read the posting guide! hthttp/wwwww-
> project.ororgosting-guide.hthtml> >> 
> ========================
> YoYounheeoKooyounkoiuiucdedu
> 
> hthttp/cocomeduokoreac.krkrununygo> contact : 217-417-4868
> Graduate Student in Dept. of Computer Science
> University of Illinois at Urbana-Champaign
> 
> ______________________________________________
> R-help at stat.math.etethzh mailing list
> hthttps/stat.etethzh/mailman/lilistinfo-help
> PLEASE do read the posting guide! hthttp/wwwww-project.ororgosting-guide.hthtml>



From botti23 at libero.it  Sat Jan  1 20:15:02 2005
From: botti23 at libero.it (franci)
Date: Sat, 1 Jan 2005 20:15:02 +0100
Subject: [R] plot
Message-ID: <200501011915.j01JFRFP015596@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050101/7ccd8064/attachment.ksh

From h.brunschwig at utoronto.ca  Sat Jan  1 20:23:53 2005
From: h.brunschwig at utoronto.ca (h.brunschwig@utoronto.ca)
Date: Sat,  1 Jan 2005 14:23:53 -0500
Subject: [R] lme: Variances
Message-ID: <1104607433.41d6f8c9d950d@webmail.utoronto.ca>


Hi R users!

I will try to state my question again. I have longitudinal data and fitted the
following model with lme:

Y = X*beta + U + W(t) + Z

where 

U ~ N(0, nu*I)       I is the identity matrix, so this is the random intercept

W(t)~ N(0, sigma*H)  and H is a matrix which incorporates a Gaussian serial    
                 
                     correlation (covariance) in the offdiagonal elements

Z ~ N(0, tau*J)      the (measurement) error

So lme must have estimated three variance parameters plus the parameter for the
Gaussian correlations. From the output I get, I dont know which is which. The
output was:

> nepal.lme<-lme(ht~sex+died+alive+mage+lit+bf+age+I(age^2),
data=nepal,random=~1|id,correlation=corGaus(corstruct,form=~age|id)
,na.action=na.exclude)

> summary(nepal.lme)
Linear mixed-effects model fit by REML
 Data: nepal 
       AIC    BIC    logLik
  3363.547 3420.7 -1669.774

Random effects:
 Formula: ~1 | id
        (Intercept) Residual
StdDev:    4.240752 1.240242

Correlation Structure: Gaussian spatial correlation
 Formula: ~age | id 
 Parameter estimate(s):
   range 
4.662006 
Fixed effects: ht ~ sex + died + alive + mage + lit + bf + age + I(age^2) 
               Value Std.Error  DF   t-value p-value
(Intercept) 51.07075 2.0274503 674  25.18964  0.0000
sex         -0.54780 0.6210172 191  -0.88210  0.3788
died        -0.01686 0.3915936 191  -0.04306  0.9657
alive       -0.46979 0.2388267 191  -1.96706  0.0506
mage         0.34289 0.0859849 191   3.98785  0.0001
lit          2.62584 1.5123958 191   1.73621  0.0841
bf           0.31873 0.0870432 674   3.66173  0.0003
age          0.86269 0.0198487 674  43.46318  0.0000
I(age^2)    -0.00334 0.0002347 674 -14.24842  0.0000
 Correlation: 
         (Intr) sex    died   alive  mage   lit    bf     age   
sex      -0.419                                                 
died     -0.063 -0.095                                          
alive     0.475  0.041 -0.484                                   
mage     -0.814 -0.035  0.188 -0.780                            
lit      -0.072 -0.099  0.092  0.003  0.059                     
bf       -0.098  0.000  0.002 -0.001  0.008  0.008              
age      -0.168  0.016  0.003 -0.008 -0.018 -0.008  0.250       
I(age^2)  0.146 -0.010 -0.010  0.009 -0.007  0.005 -0.156 -0.917

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-4.22819684 -0.45649041 -0.02996715  0.43379089  2.70363057

Thanks for any help.

Hadassa



From pallier at cogito.lscp.ehess.fr  Sat Jan  1 20:32:35 2005
From: pallier at cogito.lscp.ehess.fr (Pallier Christophe)
Date: Sat, 1 Jan 2005 20:32:35 +0100 (CET)
Subject: [R] plot
In-Reply-To: <200501011915.j01JFRFP015596@hypatia.math.ethz.ch>
References: <200501011915.j01JFRFP015596@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0501012029590.26499@cogito.lscp.ehess.fr>

On Sat, 1 Jan 2005, franci wrote:

> Is it possible to make multiple plots i.e. to represent more than one
> function on the same figure?

It you want to superimpose the curves, for example:

t=seq(-5,5,.1)
plot(t,exp(-t*t),type='l')
lines(t,sin(t)*sin(t))


If you want to plot on separate plotting areas:

par(mfcol=c(1,2))
plot(t,exp(-t*t),type='l')
plot(t,sin(t)*sin(t),type='l')


Christophe Pallier



From edd at debian.org  Sat Jan  1 20:34:17 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 1 Jan 2005 13:34:17 -0600
Subject: [R] New Year Wish List
In-Reply-To: <f8e6ff0505010108452ef28e35@mail.gmail.com>
References: <loom.20050101T134856-402@post.gmane.org>
	<f8e6ff0505010108452ef28e35@mail.gmail.com>
Message-ID: <20050101193417.GA28294@sonny.eddelbuettel.com>

On Sat, Jan 01, 2005 at 10:45:19AM -0600, hadley wickham wrote:
> > 4. Sourceforge-like Support for Subprojects.
> > 
> >   This refers to groupware support for subproject development.
> >   The Lua and Ruby communities have done this with LuaForge.net and
> >   RubyForge.org.
> 
> I think both are powered by http://gforge.org/ - it probably wouldn't
> be difficult to set up something similar for R.

The shortage is not on the software side:  'apt-get install gforge'.
It is on volunteer time configuring and maintaining gforge, providing
bandwidth, running backups, ...  

It's a pretty tall order, so I wouldn't expect this to happen too soon. 
Of course, I'd be very glad to be proven wrong in this matter.

Dirk

-- 
If you don't go with R now, you will someday.
  -- David Kane on r-sig-finance, 30 Nov 2004



From andrewr at uidaho.edu  Sat Jan  1 21:09:32 2005
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Sun, 2 Jan 2005 07:09:32 +1100
Subject: [R] lme: Variances
In-Reply-To: <1104607433.41d6f8c9d950d@webmail.utoronto.ca>
References: <1104607433.41d6f8c9d950d@webmail.utoronto.ca>
Message-ID: <20050101200932.GA599@uidaho.edu>

Hi Hadassa,

> Random effects:
>  Formula: ~1 | id
>         (Intercept) Residual
> StdDev:    4.240752 1.240242
> 
> Correlation Structure: Gaussian spatial correlation
>  Formula: ~age | id 
>  Parameter estimate(s):
>    range 
> 4.662006 

it looks to me as though you have two variance parameters and the
Gaussian variogram.  I speculate that the model that you've written is
not quite the model that you've fit.  I see no provision for sigma
in your lme statement.  Note p. 230 of Pinheiro and Bates, which
says that "The within-group errors can be standardized to have unit
variance, without changing their correlation structure".  Probably 
sigma is confounded with tau.

If my interpretation is correct, then 

nu = 4.240752 
tau = 1.240242 

and tau also has your sigma in there.  Then the p parameter for your
Gaussian semivariogram is 4.662006.

I hope that this helps,

Andrew

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From roebuck at odin.mdacc.tmc.edu  Sat Jan  1 22:25:17 2005
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Sat, 1 Jan 2005 15:25:17 -0600 (CST)
Subject: [R] install.packages() for local source file
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E480@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E480@usrymx25.merck.com>
Message-ID: <Pine.OSF.4.58.0501011456230.420294@odin.mdacc.tmc.edu>

On Sat, 1 Jan 2005, Liaw, Andy wrote:

> > From: Paul Roebuck
> >
> > Wish to install a local source package on Un*x platform from
> > within R. Same thing as I can accomplish from cmdline as
> >
> > $ export R_LIBS=~/R/library
> > $ cd /path/to/pkg
> > $ R CMD INSTALL -l $R_LIBS <pkgname>
> >
> >
> > So, how do you go about this anyway?
> > And isn't this a bug in 'install.packages'?
> >
> > -------
> > $ R
> >
> > R : Copyright 2004, The R Foundation for Statistical Computing
> > Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3
> >
> > > file.pkg <- "mypkg_0.1.tar.gz"
> > > path.pkg <- file.path(path.expand("~"), "cvknn", file.pkg)
> > > file.exists(path.pkg)
> > [1] TRUE
> > > uri.pkg <- paste("file://", path.pkg, sep = "")
> > > install.packages(contriburl = uri.pkg, lib = Sys.getenv("R_LIBS"))
> > Error in file.info(x) : Object "tmpd" not found
> > > traceback()
> > 4: file.info(x)
> > 3: dirTest(destdir)
> > 2: download.packages(pkgs, destdir = tmpd, available = available,
> >        contriburl = contriburl, method = method)
> > 1: install.packages(contriburl = uri.pkg, lib = Sys.getenv("R_LIBS"))
> > > version
> >          _
> > platform sparc-sun-solaris2.9
> > arch     sparc
> > os       solaris2.9
> > system   sparc, solaris2.9
> > status
> > major    1
> > minor    9.0
> > year     2004
> > month    04
> > day      12
> > language R
>
> Please read ?install.packages.  It's most definitely not a bug for a
> function that works as documented.  If you want, you might take the
> source for install.packages and strip it down to do what you want,
> say something called installLocalPackages.

Well, I called myself having read it but it still wasn't obvious
to me; hence I posted the question here. Exactly which part should
I have read more carefully? I will grant that I left out one thing
when I did my cut'n'paste but it doesn't change the result.
My attempt to install my local package should have read:

> install.packages("mypkg",
                   contriburl = uri.pkg,
                   lib = Sys.getenv("R_LIBS"))

Looking at the source, 'tmpd' is only set if the protocol
is not "file:". Since I use that protocol, it would seem
to me that passing 'NULL' instead of 'tempfile("Rinstdir")'
would constitute a bug.

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From andy_liaw at merck.com  Sat Jan  1 22:36:58 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 1 Jan 2005 16:36:58 -0500
Subject: [R] install.packages() for local source file
Message-ID: <3A822319EB35174CA3714066D590DCD50994E484@usrymx25.merck.com>

The version of ?install.packages (in R-2.0.1) I've read never said it could
install local source packages, so why would it be considered a bug not being
able to do something that is never claimed?  You seem to expect the function
to do something that it is never designed to do.

Andy

> From: Paul Roebuck
> 
> On Sat, 1 Jan 2005, Liaw, Andy wrote:
> 
> > > From: Paul Roebuck
> > >
> > > Wish to install a local source package on Un*x platform from
> > > within R. Same thing as I can accomplish from cmdline as
> > >
> > > $ export R_LIBS=~/R/library
> > > $ cd /path/to/pkg
> > > $ R CMD INSTALL -l $R_LIBS <pkgname>
> > >
> > >
> > > So, how do you go about this anyway?
> > > And isn't this a bug in 'install.packages'?
> > >
> > > -------
> > > $ R
> > >
> > > R : Copyright 2004, The R Foundation for Statistical Computing
> > > Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3
> > >
> > > > file.pkg <- "mypkg_0.1.tar.gz"
> > > > path.pkg <- file.path(path.expand("~"), "cvknn", file.pkg)
> > > > file.exists(path.pkg)
> > > [1] TRUE
> > > > uri.pkg <- paste("file://", path.pkg, sep = "")
> > > > install.packages(contriburl = uri.pkg, lib = 
> Sys.getenv("R_LIBS"))
> > > Error in file.info(x) : Object "tmpd" not found
> > > > traceback()
> > > 4: file.info(x)
> > > 3: dirTest(destdir)
> > > 2: download.packages(pkgs, destdir = tmpd, available = available,
> > >        contriburl = contriburl, method = method)
> > > 1: install.packages(contriburl = uri.pkg, lib = 
> Sys.getenv("R_LIBS"))
> > > > version
> > >          _
> > > platform sparc-sun-solaris2.9
> > > arch     sparc
> > > os       solaris2.9
> > > system   sparc, solaris2.9
> > > status
> > > major    1
> > > minor    9.0
> > > year     2004
> > > month    04
> > > day      12
> > > language R
> >
> > Please read ?install.packages.  It's most definitely not a bug for a
> > function that works as documented.  If you want, you might take the
> > source for install.packages and strip it down to do what you want,
> > say something called installLocalPackages.
> 
> Well, I called myself having read it but it still wasn't obvious
> to me; hence I posted the question here. Exactly which part should
> I have read more carefully? I will grant that I left out one thing
> when I did my cut'n'paste but it doesn't change the result.
> My attempt to install my local package should have read:
> 
> > install.packages("mypkg",
>                    contriburl = uri.pkg,
>                    lib = Sys.getenv("R_LIBS"))
> 
> Looking at the source, 'tmpd' is only set if the protocol
> is not "file:". Since I use that protocol, it would seem
> to me that passing 'NULL' instead of 'tempfile("Rinstdir")'
> would constitute a bug.
> 
> ----------------------------------------------------------
> SIGSIG -- signature too long (core dumped)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From roebuck at odin.mdacc.tmc.edu  Sat Jan  1 23:48:35 2005
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Sat, 1 Jan 2005 16:48:35 -0600 (CST)
Subject: [R] install.packages() for local source file
Message-ID: <Pine.OSF.4.58.0501011647400.420294@odin.mdacc.tmc.edu>

On Sat, 1 Jan 2005, Liaw, Andy wrote:

> > From: Paul Roebuck
> >
> > > Please read ?install.packages.  It's most definitely not a bug for a
> > > function that works as documented.  If you want, you might take the
> > > source for install.packages and strip it down to do what you want,
> > > say something called installLocalPackages.
> >
> > Well, I called myself having read it but it still wasn't obvious
> > to me; hence I posted the question here. Exactly which part should
> > I have read more carefully? I will grant that I left out one thing
> > when I did my cut'n'paste but it doesn't change the result.
> > My attempt to install my local package should have read:
> >
> > > install.packages("mypkg",
> >                    contriburl = uri.pkg,
> >                    lib = Sys.getenv("R_LIBS"))
> >
> > Looking at the source, 'tmpd' is only set if the protocol
> > is not "file:". Since I use that protocol, it would seem
> > to me that passing 'NULL' instead of 'tempfile("Rinstdir")'
> > would constitute a bug.
>
> The version of ?install.packages (in R-2.0.1) I've read never said
> it could install local source packages, so why would it be
> considered a bug not being able to do something that is never
> claimed?  You seem to expect the function to do something that
> it is never designed to do.

The name of the routine made the claim, not I.
One could be forgiven for assuming it could since the
'contriburl' argument implies it could use a CD. I've
used it in the past for local installs by simulating
the CRAN directory structure and overriding the 'CRAN'
argument but that's kind of a hassle for something quick.
Perhaps it should be renamed 'install.packages.from.CRAN'
since the current name implies something different to me.

The programmer in me would still call the current situation
a bug - the if statement around the 'localcran' variable
is missing the else case to handle this scenario. Assuming
so, I really don't see why this [w|c]ouldn't handle local source
installs. And if it can't (and never could be made to do so),
then the code should have a 'stop("local install unimplemented")'
there so no one else ever has to ask again.

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From rpeng at jhsph.edu  Sun Jan  2 00:40:52 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Sat, 01 Jan 2005 18:40:52 -0500
Subject: [R] install.packages() for local source file
In-Reply-To: <Pine.OSF.4.58.0412271538090.98639@odin.mdacc.tmc.edu>
References: <Pine.OSF.4.58.0412271538090.98639@odin.mdacc.tmc.edu>
Message-ID: <41D73504.4060208@jhsph.edu>

By the way, do you get this error in a recent version of R (say >= 1.9.1).  I 
believe install.packages() has changed since 1.9.0.  For example, see the thread 
starting here

https://stat.ethz.ch/pipermail/r-help/2004-July/053001.html

-roger

Paul Roebuck wrote:
> Wish to install a local source package on Un*x platform from
> within R. Same thing as I can accomplish from cmdline as
> 
> $ export R_LIBS=~/R/library
> $ cd /path/to/pkg
> $ R CMD INSTALL -l $R_LIBS <pkgname>
> 
> 
> So, how do you go about this anyway?
> And isn't this a bug in 'install.packages'?
> 
> -------
> $ R
> 
> R : Copyright 2004, The R Foundation for Statistical Computing
> Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3
> 
> 
>>file.pkg <- "mypkg_0.1.tar.gz"
>>path.pkg <- file.path(path.expand("~"), "cvknn", file.pkg)
>>file.exists(path.pkg)
> 
> [1] TRUE
> 
>>uri.pkg <- paste("file://", path.pkg, sep = "")
>>install.packages(contriburl = uri.pkg, lib = Sys.getenv("R_LIBS"))
> 
> Error in file.info(x) : Object "tmpd" not found
> 
>>traceback()
> 
> 4: file.info(x)
> 3: dirTest(destdir)
> 2: download.packages(pkgs, destdir = tmpd, available = available,
>        contriburl = contriburl, method = method)
> 1: install.packages(contriburl = uri.pkg, lib = Sys.getenv("R_LIBS"))
> 
>>version
> 
>          _
> platform sparc-sun-solaris2.9
> arch     sparc
> os       solaris2.9
> system   sparc, solaris2.9
> status
> major    1
> minor    9.0
> year     2004
> month    04
> day      12
> language R
> 
> 
> ----------------------------------------------------------
> SIGSIG -- signature too long (core dumped)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From roebuck at odin.mdacc.tmc.edu  Sun Jan  2 06:38:28 2005
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Sat, 1 Jan 2005 23:38:28 -0600 (CST)
Subject: [R] install.packages() for local source file
In-Reply-To: <41D73504.4060208@jhsph.edu>
References: <Pine.OSF.4.58.0412271538090.98639@odin.mdacc.tmc.edu>
	<41D73504.4060208@jhsph.edu>
Message-ID: <Pine.OSF.4.58.0501012136080.438703@odin.mdacc.tmc.edu>

On Sat, 1 Jan 2005, Roger D. Peng wrote:

> Paul Roebuck wrote:
> > Wish to install a local source package on Un*x platform from
> > within R. Same thing as I can accomplish from cmdline as
> >
> > $ export R_LIBS=~/R/library
> > $ cd /path/to/pkg
> > $ R CMD INSTALL -l $R_LIBS <pkgname>
> >
> >
> > So, how do you go about this anyway?
> > And isn't this a bug in 'install.packages'?
> >
> > -------
> > $ R
> >
> > R : Copyright 2004, The R Foundation for Statistical Computing
> > Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3
> >
> >
> >>file.pkg <- "mypkg_0.1.tar.gz"
> >>path.pkg <- file.path(path.expand("~"), "cvknn", file.pkg)
> >>file.exists(path.pkg)
> >
> > [1] TRUE
> >
> >>uri.pkg <- paste("file://", path.pkg, sep = "")
> >>install.packages(contriburl = uri.pkg, lib = Sys.getenv("R_LIBS"))
> >
> > Error in file.info(x) : Object "tmpd" not found
> >
> >>traceback()
> >
> > 4: file.info(x)
> > 3: dirTest(destdir)
> > 2: download.packages(pkgs, destdir = tmpd, available = available,
> >        contriburl = contriburl, method = method)
> > 1: install.packages(contriburl = uri.pkg, lib = Sys.getenv("R_LIBS"))
> >
> >>version
> >
> >          _
> > platform sparc-sun-solaris2.9
> > arch     sparc
> > os       solaris2.9
> > system   sparc, solaris2.9
> > status
> > major    1
> > minor    9.0
> > year     2004
> > month    04
> > day      12
> > language R
> >
>
> By the way, do you get this error in a recent version of R
> (say >= 1.9.1). I believe install.packages() has changed
> since 1.9.0.  For example, see the thread starting here
>
> https://stat.ethz.ch/pipermail/r-help/2004-July/053001.html
>

Roger,

Thanks for that link which helped me diagnose the problem.
I observed the same error you observed (.../053047.html). Still
think it's kind of hinky to pass an uninitialized variable (tmpd)
to another method and count on it doing something though.

I wondered if something had changed as well, but noticed no change
glancing at the source for install.packages on 2.0.1 (OS X).
But underneath, the behavior was different since I got a
different error message which noted the lack of a PACKAGES
file. That was enough to get the rest to work...

Hopefully this will help my case for updating to the current
version on our shared Un*x workstations since I can now point
to a definitive bug that impacted my work due to using an
older version of this software.

--------
R : Copyright 2004, The R Foundation for Statistical Computing
Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0

> parentdir <- file.path(path.expand("~"), "Projects", "cvknn")
> uri.parentdir <- paste("file://", parentdir, sep = "")
> savewd <- getwd()
> setwd(parentdir)
> rmsymlink <- FALSE
> if (file.exists("PACKAGES") == FALSE) {
>     file.symlink(file.path("mypkg", "DESCRIPTION"), "PACKAGES")
>     rmsymlink <- TRUE
> }
> install.packages("mypkg",
+                  contriburl = uri.parentdir,
+                  lib = Sys.getenv("R_LIBS"))
> if (rmsymlink) file.remove("PACKAGES")
> setwd(savewd)

Perhaps the documentation for the 'contriburl' should
specify that it is expecting 'URL of the directory of
the contrib section of CRAN'.

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From ripley at stats.ox.ac.uk  Sun Jan  2 09:29:55 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 2 Jan 2005 08:29:55 +0000 (GMT)
Subject: [R] install.packages() for local source file
In-Reply-To: <Pine.OSF.4.58.0501012136080.438703@odin.mdacc.tmc.edu>
References: <Pine.OSF.4.58.0412271538090.98639@odin.mdacc.tmc.edu>
	<41D73504.4060208@jhsph.edu>
	<Pine.OSF.4.58.0501012136080.438703@odin.mdacc.tmc.edu>
Message-ID: <Pine.LNX.4.61.0501020826120.17083@gannet.stats>

This is not a `definitive bug' but as you have been told repeatedly a 
failure on your part to read the help adequately:

   contriburl: URL of the contrib section of CRAN.

The good news is that R-devel does support local installation, so R 2.1.0 
will.

On Sat, 1 Jan 2005, Paul Roebuck wrote:

> On Sat, 1 Jan 2005, Roger D. Peng wrote:
>
>> Paul Roebuck wrote:
>>> Wish to install a local source package on Un*x platform from
>>> within R. Same thing as I can accomplish from cmdline as
>>>
>>> $ export R_LIBS=~/R/library
>>> $ cd /path/to/pkg
>>> $ R CMD INSTALL -l $R_LIBS <pkgname>
>>>
>>>
>>> So, how do you go about this anyway?
>>> And isn't this a bug in 'install.packages'?
>>>
>>> -------
>>> $ R
>>>
>>> R : Copyright 2004, The R Foundation for Statistical Computing
>>> Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3
>>>
>>>
>>>> file.pkg <- "mypkg_0.1.tar.gz"
>>>> path.pkg <- file.path(path.expand("~"), "cvknn", file.pkg)
>>>> file.exists(path.pkg)
>>>
>>> [1] TRUE
>>>
>>>> uri.pkg <- paste("file://", path.pkg, sep = "")
>>>> install.packages(contriburl = uri.pkg, lib = Sys.getenv("R_LIBS"))
>>>
>>> Error in file.info(x) : Object "tmpd" not found
>>>
>>>> traceback()
>>>
>>> 4: file.info(x)
>>> 3: dirTest(destdir)
>>> 2: download.packages(pkgs, destdir = tmpd, available = available,
>>>        contriburl = contriburl, method = method)
>>> 1: install.packages(contriburl = uri.pkg, lib = Sys.getenv("R_LIBS"))
>>>
>>>> version
>>>
>>>          _
>>> platform sparc-sun-solaris2.9
>>> arch     sparc
>>> os       solaris2.9
>>> system   sparc, solaris2.9
>>> status
>>> major    1
>>> minor    9.0
>>> year     2004
>>> month    04
>>> day      12
>>> language R
>>>
>>
>> By the way, do you get this error in a recent version of R
>> (say >= 1.9.1). I believe install.packages() has changed
>> since 1.9.0.  For example, see the thread starting here
>>
>> https://stat.ethz.ch/pipermail/r-help/2004-July/053001.html
>>
>
> Roger,
>
> Thanks for that link which helped me diagnose the problem.
> I observed the same error you observed (.../053047.html). Still
> think it's kind of hinky to pass an uninitialized variable (tmpd)
> to another method and count on it doing something though.
>
> I wondered if something had changed as well, but noticed no change
> glancing at the source for install.packages on 2.0.1 (OS X).
> But underneath, the behavior was different since I got a
> different error message which noted the lack of a PACKAGES
> file. That was enough to get the rest to work...
>
> Hopefully this will help my case for updating to the current
> version on our shared Un*x workstations since I can now point
> to a definitive bug that impacted my work due to using an
> older version of this software.
>
> --------
> R : Copyright 2004, The R Foundation for Statistical Computing
> Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0
>
>> parentdir <- file.path(path.expand("~"), "Projects", "cvknn")
>> uri.parentdir <- paste("file://", parentdir, sep = "")
>> savewd <- getwd()
>> setwd(parentdir)
>> rmsymlink <- FALSE
>> if (file.exists("PACKAGES") == FALSE) {
>>     file.symlink(file.path("mypkg", "DESCRIPTION"), "PACKAGES")
>>     rmsymlink <- TRUE
>> }
>> install.packages("mypkg",
> +                  contriburl = uri.parentdir,
> +                  lib = Sys.getenv("R_LIBS"))
>> if (rmsymlink) file.remove("PACKAGES")
>> setwd(savewd)
>
> Perhaps the documentation for the 'contriburl' should
> specify that it is expecting 'URL of the directory of
> the contrib section of CRAN'.
>
> ----------------------------------------------------------
> SIGSIG -- signature too long (core dumped)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andrewr at uidaho.edu  Mon Jan  3 00:26:12 2005
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Mon, 3 Jan 2005 10:26:12 +1100
Subject: [R] How to quieten axis() for Sweave: avoid echoing NULL?
Message-ID: <20050102232612.GH593@uidaho.edu>

Dear R-community,

I'm using Sweave to produce reports.  The reports require the "axis"
command.  When I run axis the program returns NULL as well as creating
the axis.  

> plot(1:4, rnorm(4), axes=FALSE)
> axis(1, 1:4, LETTERS[1:4])
NULL
>

So, my Sweave tex files have 

\begin{Schunk}
\begin{Soutput}
NULL
\end{Soutput}
\end{Schunk}

in front of each graphic that requires axis.  I can easily find them
and remove them, but I was wondering: is it something I can avoid?  Or
am I doing something foolish and obvious?

Thanks for any assistance,

Andrew
-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From umalvarez at fata.unam.mx  Mon Jan  3 01:01:59 2005
From: umalvarez at fata.unam.mx (Ulises M. Alvarez)
Date: Sun, 02 Jan 2005 18:01:59 -0600
Subject: [R] How to quieten axis() for Sweave: avoid echoing NULL?
In-Reply-To: <20050102232612.GH593@uidaho.edu>
References: <20050102232612.GH593@uidaho.edu>
Message-ID: <41D88B77.1010204@fata.unam.mx>

Hi!

The following works for me:

\begin{figure}
\centering
<<fig1.R,fig=true,echo=false>>=
plot(1:4, rnorm(4), axes=FALSE)
axis(1, 1:4, LETTERS[1:4])
@
\caption{Plot test}
\label{fig:1}
\end{figure}

I'm using R 2.0.1 on an i686 with GNU/Linux Ubuntu 4.10.

Andrew Robinson wrote:
> Dear R-community,
> 
> I'm using Sweave to produce reports.  The reports require the "axis"
> command.  When I run axis the program returns NULL as well as creating
> the axis.  
> 
> 
>>plot(1:4, rnorm(4), axes=FALSE)
>>axis(1, 1:4, LETTERS[1:4])
> 
> NULL
> 
> 
> So, my Sweave tex files have 
> 
> \begin{Schunk}
> \begin{Soutput}
> NULL
> \end{Soutput}
> \end{Schunk}
> 
> in front of each graphic that requires axis.  I can easily find them
> and remove them, but I was wondering: is it something I can avoid?  Or
> am I doing something foolish and obvious?
> 
> Thanks for any assistance,
> 
> Andrew
---
Ulises M. Alvarez
<umalvarez at fata.unam.mx>



From p.dalgaard at biostat.ku.dk  Mon Jan  3 01:08:57 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Jan 2005 01:08:57 +0100
Subject: [R] How to quieten axis() for Sweave: avoid echoing NULL?
In-Reply-To: <20050102232612.GH593@uidaho.edu>
References: <20050102232612.GH593@uidaho.edu>
Message-ID: <x2k6qv2x2e.fsf@biostat.ku.dk>

Andrew Robinson <andrewr at uidaho.edu> writes:

> Dear R-community,
> 
> I'm using Sweave to produce reports.  The reports require the "axis"
> command.  When I run axis the program returns NULL as well as creating
> the axis.  
> 
> > plot(1:4, rnorm(4), axes=FALSE)
> > axis(1, 1:4, LETTERS[1:4])
> NULL
> >
> 
> So, my Sweave tex files have 
> 
> \begin{Schunk}
> \begin{Soutput}
> NULL
> \end{Soutput}
> \end{Schunk}
> 
> in front of each graphic that requires axis.  I can easily find them
> and remove them, but I was wondering: is it something I can avoid?  Or
> am I doing something foolish and obvious?

I think this qualifies as a bug along with similar issues with
title(), text(), and mtext(). They all end with .Internal calls
producing NULL. Once upon a time, they did so invisibly, but somehow
this changed.

The simple workaround is to wrap the call in invisible().

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From f.harrell at vanderbilt.edu  Mon Jan  3 02:40:36 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 02 Jan 2005 20:40:36 -0500
Subject: [R] Black and white graphics and transparent strip panels with
 lattice under Sweave
Message-ID: <41D8A294.9090105@vanderbilt.edu>

What is the most elegant way to specify that strip panels are to have 
transparent backgrounds and graphs are to be in black and white when 
lattice is being used with Sweave?  I would prefer a global option that 
stays in effect for multiple plots.

If this is best done with a theme, does anyone have a lattice theme like 
col.whitebg but that is for black and white?

I'm using the following with lattice 0.10-16, grid 2.0.0:

platform i586-mandrake-linux-gnu
arch     i586
os       linux-gnu
system   i586, linux-gnu
status
major    2
minor    0.0
year     2004
month    10
day      04
language R

Thanks,

Frank
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From deepayan at stat.wisc.edu  Mon Jan  3 03:11:54 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sun, 2 Jan 2005 20:11:54 -0600
Subject: [R] Black and white graphics and transparent strip panels with
	lattice under Sweave
In-Reply-To: <41D8A294.9090105@vanderbilt.edu>
References: <41D8A294.9090105@vanderbilt.edu>
Message-ID: <200501022011.54523.deepayan@stat.wisc.edu>

On Sunday 02 January 2005 19:40, Frank E Harrell Jr wrote:
> What is the most elegant way to specify that strip panels are to have
> transparent backgrounds and graphs are to be in black and white when
> lattice is being used with Sweave?  I would prefer a global option
> that stays in effect for multiple plots.
>
> If this is best done with a theme, does anyone have a lattice theme
> like col.whitebg but that is for black and white?

I'd do something like this as part of the initialization:

<<...>>
library(lattice)
ltheme <- canonical.theme(color = FALSE)     ## in-built B&W theme
ltheme$strip.background$col <- "transparent" ## change strip bg
lattice.options(default.theme = ltheme)      ## set as default
@

Deepayan



From f.harrell at vanderbilt.edu  Mon Jan  3 03:23:11 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 02 Jan 2005 21:23:11 -0500
Subject: [R] Black and white graphics and transparent strip panels with
	lattice under Sweave
In-Reply-To: <200501022011.54523.deepayan@stat.wisc.edu>
References: <41D8A294.9090105@vanderbilt.edu>
	<200501022011.54523.deepayan@stat.wisc.edu>
Message-ID: <41D8AC8F.30300@vanderbilt.edu>

Deepayan Sarkar wrote:
> On Sunday 02 January 2005 19:40, Frank E Harrell Jr wrote:
> 
>>What is the most elegant way to specify that strip panels are to have
>>transparent backgrounds and graphs are to be in black and white when
>>lattice is being used with Sweave?  I would prefer a global option
>>that stays in effect for multiple plots.
>>
>>If this is best done with a theme, does anyone have a lattice theme
>>like col.whitebg but that is for black and white?
> 
> 
> I'd do something like this as part of the initialization:
> 
> <<...>>
> library(lattice)
> ltheme <- canonical.theme(color = FALSE)     ## in-built B&W theme
> ltheme$strip.background$col <- "transparent" ## change strip bg
> lattice.options(default.theme = ltheme)      ## set as default
> @
> 
> Deepayan
> 

That worked perfectly.  Thank you very much Deepayan.  -Frank



From pmccask at cyllene.uwa.edu.au  Mon Jan  3 04:54:49 2005
From: pmccask at cyllene.uwa.edu.au (Pamela McCaskie)
Date: Mon, 3 Jan 2005 11:54:49 +0800 (WST)
Subject: [R] subsetting within a function
In-Reply-To: <Pine.A41.4.61b.0412300802280.11816@homer11.u.washington.edu>
References: <Pine.LNX.4.56.0412301821070.5229@cyllene.uwa.edu.au>
	<Pine.A41.4.61b.0412300802280.11816@homer11.u.washington.edu>
Message-ID: <Pine.LNX.4.56.0501031150430.31502@cyllene.uwa.edu.au>

Thankyou for your help with subsetting within a function. I have now tried
to apply the same theory in the framework of an lme as follows:

fit1.lme <- eval(substitute(lme(fixed=fixed, data=dataframe,
random=random, correlation=corCAR1(form= corr), na.action=na.omit,
subset=subset),list(subset=subs)))

but I get the following error:

Error in switch(mode(object), name = , numeric = , call = object,
character = as.name(object),  :
        [[ cannot be of mode logical

I'm not sure why. Can anyone help me with this?
kind regards
Pam



On Thu, 30 Dec 2004, Thomas Lumley wrote:

> On Thu, 30 Dec 2004, Pamela McCaskie wrote:
> > And so my attempt to wrap a simple function around this looks like:
> > test.fun <- function(formula, mydata, sub=NULL){
> >  subs <- with(mydata, eval(sub))
> >  fit.glm <- glm(formula=formula, data=mydata, family=binomial, subset=subs)
> >  return(fit.glm)
> > }
> >
> > But when I tested it out with
> > test <- test.fun(y~x1+x2, mydata=testdata, sub=expression(SEX==0))
> >
> > I get:
> > Error in "[.data.frame"(structure(list(N_ASTHMA = as.integer(c(0, 0, 0,  :
> >        invalid subscript type
>
> I get a different error: it may be that you have an object called `subs`
> in the global environment
>
> > I'm guessing that it's looking in the global environment for
> > subs,
>
> More precisely, it is looking in environment where `formula` was created,
> which happens to be the global environment.
>
> This is the sort of thing that happens with the fitting functions because
> they go to such lengths to break the basic scoping of R.
>
>
> You probably have to substitute() the evaluated subset into the glm call.
>
>    fit.glm <- eval(substitute(glm(formula=formula, data=mydata,
>       family=binomial, subset=subset),list(subset=subs)))
>
>
>  	-thomas
>



From patrick.giraudoux at univ-fcomte.fr  Mon Jan  3 08:40:03 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux H)
Date: Mon, 03 Jan 2005 08:40:03 +0100
Subject: [R] matrix and error in points, plot ?
Message-ID: <5.0.2.1.2.20050103082319.00c43c88@utinam.univ-fcomte.fr>

Hi,

Would a specialist of the "point" or "plot" functions try the following:

mat<-matrix(c(1.836767,4.025989,0.6396777,0.3764444),ncol=2)
plot(mat)
points(mat[1,],col="red")

..A lag appears on x for mat [1,1] between the two displays.

I wonder if this example may be due to a bug or to the mis-use of a matrix 
in the plot() points()  functions. In case of mise-use which kind can it be?

I am working with R 2.0.1 and Windows XP.

Cheers,

Patrick Giraudoux



From ripley at stats.ox.ac.uk  Mon Jan  3 09:20:56 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 3 Jan 2005 08:20:56 +0000 (GMT)
Subject: [R] matrix and error in points, plot ?
In-Reply-To: <5.0.2.1.2.20050103082319.00c43c88@utinam.univ-fcomte.fr>
References: <5.0.2.1.2.20050103082319.00c43c88@utinam.univ-fcomte.fr>
Message-ID: <Pine.LNX.4.61.0501030817320.11995@gannet.stats>

You have forgotten drop=FALSE in mat[1,]: take a look at what it produces.
See ?Extract for more details.

On Mon, 3 Jan 2005, Patrick Giraudoux H wrote:

> Would a specialist of the "point" or "plot" functions try the following:
>
> mat<-matrix(c(1.836767,4.025989,0.6396777,0.3764444),ncol=2)
> plot(mat)
> points(mat[1,],col="red")
>
> ..A lag appears on x for mat [1,1] between the two displays.
>
> I wonder if this example may be due to a bug or to the mis-use of a matrix in 
> the plot() points()  functions. In case of mise-use which kind can it be?

Not using a matrix at all!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Mon Jan  3 09:31:59 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 03 Jan 2005 09:31:59 +0100
Subject: [R] matrix and error in points, plot ?
In-Reply-To: <5.0.2.1.2.20050103082319.00c43c88@utinam.univ-fcomte.fr>
References: <5.0.2.1.2.20050103082319.00c43c88@utinam.univ-fcomte.fr>
Message-ID: <41D902FF.5090805@statistik.uni-dortmund.de>

Patrick Giraudoux H wrote:

> Hi,
> 
> Would a specialist of the "point" or "plot" functions try the following:
> 
> mat<-matrix(c(1.836767,4.025989,0.6396777,0.3764444),ncol=2)
> plot(mat)
> points(mat[1,],col="red")
> 
> ..A lag appears on x for mat [1,1] between the two displays.
> 
> I wonder if this example may be due to a bug or to the mis-use of a 
> matrix in the plot() points()  functions. In case of mise-use which kind 
> can it be?


It's misuse:

For a matrix, the first column is interpreted as the x-coordinates, the 
second one as y-coordinates.

For a vector (mat[1,] is a (1D) vector!) plot/points plots the values 
(as y-coordinates) against their index (1:2 in this case, where 1 is not 
visible in the plot).

You can use
   points(mat[1,,drop=FALSE], col="red")
in order not to drop dimensions, so mat[1,,drop=FASLE] is still a 
2-column matrix.

Uwe Ligges



> I am working with R 2.0.1 and Windows XP.
> 
> Cheers,
> 
> Patrick Giraudoux
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jo_brandt at web.de  Mon Jan  3 10:17:34 2005
From: jo_brandt at web.de (Johanna BRANDT)
Date: Mon, 03 Jan 2005 10:17:34 +0100
Subject: [R] library(gee)
In-Reply-To: <200501011101.j01B0jrA020784@hypatia.math.ethz.ch>
References: <200501011101.j01B0jrA020784@hypatia.math.ethz.ch>
Message-ID: <41D90DAE.20504@web.de>

Hello,

R exits every time I want to fit a GEE with AR-1 correlation 
structure. It just closes itself without any error.
My Data has about 3300 observation (using the big data with 
about 20 000 observation it doesn't work at all). I'm a 
Windows 2000 User of R 2.0.1 but it was the same problem on 
a MacOSX.
Is it my fault?

Thank you for your help!
Johanna Brandt



From fred at pik-potsdam.de  Mon Jan  3 10:50:45 2005
From: fred at pik-potsdam.de (Fred Hattermann)
Date: Mon, 03 Jan 2005 10:50:45 +0100
Subject: [R] graphics
In-Reply-To: <20050101113213.GKEX1783.smta00.mail.ozemail.net@there>
References: <41D3D031.6000408@pik-potsdam.de>
	<20050101113213.GKEX1783.smta00.mail.ozemail.net@there>
Message-ID: <41D91575.6080809@pik-potsdam.de>


Dear Jim,
many thanks for your reply and support.
It seems to be that with your help I could solve my problem with the 
plotting of the data. The only thing that does not work is to see the 
coloured lines, maybe because of the crowd of curves.
With the density of curves I ment to distinguish in the confidence 
interval between areas, where a lot of curves (lines) are located, and 
areas having a lower density of lines. In other words, I would like to 
show the area, where 100 % of the lines are included, then the area, 
where 90 % are included, and so forth with the other quantiles, but with 
a steady change of colours, and not by a stepwise change (if posible).

Many thanks again,
Fred


Jim Lemon wrote:

>Fred Hattermann wrote:
>  
>
>>Dear R-user,
>>I am a R beginner, and therefore my questions are very basic.
>>I have a simple problem: I would like to plot 100 time series each
>>containing 55 steps. The data are stored in a matrix of 100 columns and
>>55 rows. The first problem is to load the data from a file: I tried the
>>read.table(), the scan() and the matrix(scan()) options, but I have
>>problems to allocate the single columns. The list() option could be a
>>solution, but it is very unconvenient: list(0,0,0......).
>>    
>>
>
># generate some random numbers
>testts.df<-data.frame(matrix(rnorm(5500)/5,nrow=55))
># superimpose them on a sine curve
>newts<-sapply(testts.df,function(x) return(x+sin(seq(0,pi*2,length=55))))
># make it a time series
>newts<-as.ts(newts)
># write out the data
>write.table(newts,"newts.dat")
># read it in again
>newts<-as.ts(read.table("newts.dat"))
>
>  
>
>>And how do I plot a single time series, let's say the 50s? And how to
>>plot all of them?
>>
>>    
>>
># plot the first one
>plot(newts[,1],ylim=range(newts))
># add the other 99 lines - probably pretty messy!
>for(i in 2:100) lines(newts[,i])
>
>  
>
>>The last problem is maybe more advanced: I would like to plot all 100
>>time series, but with a confidence interval, where the density of data
>>is indicated by the density of the colour of the confidence interval.
>>    
>>
>
># get the means of the observation points
>newts.means<-apply(as.matrix(newts),1,mean)
># calculate a CI - probably not the one you want
>newts.ci<-1.96*sapply(as.matrix(newts),sd)
># plot the CI
>lines(newts.means+newts.ci,col="red")
>lines(newts.means-newts.ci,col="green")
>
>I'm not sure what you mean by the "density" of the curve, so I can't suggest 
>anything. However, I am adding a function named "color.scale" to the next 
>version of the "plotrix" package, so I'll email you when I put it up on CRAN.
>
>Jim
>
>
>  
>


-- 
 *******************************************************
 * Potsdam Institute for Climate Impact Research (PIK) *
 * Telegrafenberg  C 4   D-14473 Potsdam               *
 * PO box:  60 12 03     D-14412 Potsdam      	       *
 * Tel.:  (0331) 288 - 2649                            *
 * Fax:   (0331) 288 - 2695                            *
 * e-mail: hattermann at PIK-Potsdam.de                   *
 *******************************************************



From maechler at stat.math.ethz.ch  Mon Jan  3 11:31:47 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 3 Jan 2005 11:31:47 +0100
Subject: [R] Error in layout(): Too many rows in layout
In-Reply-To: <41D504EA.7070700@statistik.uni-dortmund.de>
References: <200412302231.iBUMVkXH006422@celerio.ucdavis.edu>
	<41D504EA.7070700@statistik.uni-dortmund.de>
Message-ID: <16857.7955.182146.57073@stat.math.ethz.ch>

>>>>> "UweL" == Uwe Ligges <ligges at statistik.uni-dortmund.de>
>>>>>     on Fri, 31 Dec 2004 08:51:06 +0100 writes:

    UweL> Jeroen Van Houtte wrote:
    >> To display many small graphics one above the another
    >> (like Edward Tufte's sparklines), I'd like to use layout
    >> in the graphics package with more than 15 rows.
    >> 
    >> I found that the limit of 15 is set by #define
    >> MAX_LAYOUT_ROWS 15

    UweL> Well, what about replacing 15 by something like 100?
    UweL> The point is that you should look where
    UweL> MAX_LAYOUT_ROWS is used for calculations and whether
    UweL> there might result any problem after increasing it.

    >> in
    >> https://svn.r-project.org/R/trunk/src/include/Graphics.h
    >> but I 'm not familiar enough with compilers to modify
    >> this.

    UweL> Looking into your mail header [telling us you are
    UweL> using Mozilla/4.0 (compatible; MSIE 6.0; Windows NT
    UweL> 5.1; SV1)], I see you are using Windows and probably
    UweL> never have compiled R yourself (please read the
    UweL> posting guide which tells you to mention relevant
    UweL> information such as the OS in use).

    UweL> Please read the file R/src/gnuwin32/Install.  It tells
    UweL> you how to compile R yourself and which tools are
    UweL> required.

    UweL> Uwe Ligges

Unless Jeroen is really keen on learning how build R from source
on MS Windows, I think Jeroen's time might be better invested in
learning to use the "grid" package instead of "graphics".

I think layout() in "graphics" is really just a poor man's
version of using layout.grid() and viewports as provided by the
"grid" package {and as used extensively by the "lattice" package}.

Martin Maechler



From richard at inferspace.com  Mon Jan  3 11:50:22 2005
From: richard at inferspace.com (Richard Dybowski)
Date: Mon, 03 Jan 2005 10:50:22 +0000
Subject: [R] Inspecting R functions
Message-ID: <5.2.1.1.2.20050103104041.00acabb0@mailhost.zen.co.uk>

In S-Plus, I can look at the structure of a function (for example, hist) 
simply by entering
         hist <RETURN>
however, if I do this in R, I get the response
         function (x, ...)
         UseMethod("hist")
         <environment: namespace:graphics>
How can I inspect the structure of a function in R?

-------------------------------
Richard Dybowski
143 Village Way
Pinner HA5 5AA, UK
Tel: 07976 250092



From patrick at burns-stat.com  Mon Jan  3 11:54:33 2005
From: patrick at burns-stat.com (Patrick Burns)
Date: Mon, 03 Jan 2005 10:54:33 +0000
Subject: [R] spreadsheet addiction
Message-ID: <41D92469.1010608@burns-stat.com>

There's a new page on the Burns Statistics website
http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html
that looks at spreadsheets from a quality assurance perspective. It
presents R as a suitable alternative to spreadsheets.  Also there are
several specific problems with Excel that are highlighted, including
the status of statistical functionality in Excel.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")



From ripley at stats.ox.ac.uk  Mon Jan  3 12:00:55 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 3 Jan 2005 11:00:55 +0000 (GMT)
Subject: [R] library(gee)
In-Reply-To: <41D90DAE.20504@web.de>
References: <200501011101.j01B0jrA020784@hypatia.math.ethz.ch>
	<41D90DAE.20504@web.de>
Message-ID: <Pine.LNX.4.61.0501031059040.12157@gannet.stats>

Package gee does that -- it is rather old and abandoned by its author a 
long time ago.  Perhaps you could try yags or geepack instead?

On Mon, 3 Jan 2005, Johanna BRANDT wrote:

> R exits every time I want to fit a GEE with AR-1 correlation structure. It 
> just closes itself without any error.
> My Data has about 3300 observation (using the big data with about 20 000 
> observation it doesn't work at all). I'm a Windows 2000 User of R 2.0.1 but 
> it was the same problem on a MacOSX.
> Is it my fault?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Mon Jan  3 12:05:10 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 03 Jan 2005 12:05:10 +0100
Subject: [R] Inspecting R functions
In-Reply-To: <5.2.1.1.2.20050103104041.00acabb0@mailhost.zen.co.uk>
References: <5.2.1.1.2.20050103104041.00acabb0@mailhost.zen.co.uk>
Message-ID: <41D926E6.4050705@statistik.uni-dortmund.de>

Richard Dybowski wrote:

> In S-Plus, I can look at the structure of a function (for example, hist) 
> simply by entering
>         hist <RETURN>
> however, if I do this in R, I get the response
>         function (x, ...)
>         UseMethod("hist")
>         <environment: namespace:graphics>
> How can I inspect the structure of a function in R?

Well, if hist() would be a an S3 generic with methods in S-PLUS as well, 
  it would look quite similar like in R...

Type methods(hist) to see which methods do exists, in particular you 
want to get hist.default, I guess.

Uwe Ligges



> -------------------------------
> Richard Dybowski
> 143 Village Way
> Pinner HA5 5AA, UK
> Tel: 07976 250092
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From fredrik.bg.lundgren at bredband.net  Mon Jan  3 12:07:56 2005
From: fredrik.bg.lundgren at bredband.net (Fredrik Lundgren)
Date: Mon, 3 Jan 2005 12:07:56 +0100
Subject: [R] Inspecting R functions
References: <5.2.1.1.2.20050103104041.00acabb0@mailhost.zen.co.uk>
Message-ID: <001b01c4f184$7aa61ba0$5f9d72d5@Larissa>

Richard,

There are usually several methods available - depending on the class of 
the object you apply 'hist' on.
In this case two are 'non-visible'

######
> methods(hist)
[1] hist.Date*   hist.default hist.POSIXt*

    Non-visible functions are asterisked
######

Probably you are interested in hist.default

######
> hist.default
function (x, breaks = "Sturges", freq = NULL, probability = !freq,
    include.lowest = TRUE, right = TRUE, density = NULL, angle = 45,
    col = NULL, border = NULL, main = paste
...
snip
...
    }
    else r
}
<environment: namespace:graphics>

######

Fredrik Lundgren

----- Original Message ----- 
From: "Richard Dybowski" <richard at inferspace.com>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, January 03, 2005 11:50 AM
Subject: [R] Inspecting R functions


> In S-Plus, I can look at the structure of a function (for example, 
> hist) simply by entering
>         hist <RETURN>
> however, if I do this in R, I get the response
>         function (x, ...)
>         UseMethod("hist")
>         <environment: namespace:graphics>
> How can I inspect the structure of a function in R?
>
> -------------------------------
> Richard Dybowski
> 143 Village Way
> Pinner HA5 5AA, UK
> Tel: 07976 250092
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Mon Jan  3 12:17:17 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 3 Jan 2005 11:17:17 +0000 (GMT)
Subject: [R] Inspecting R functions
In-Reply-To: <5.2.1.1.2.20050103104041.00acabb0@mailhost.zen.co.uk>
References: <5.2.1.1.2.20050103104041.00acabb0@mailhost.zen.co.uk>
Message-ID: <Pine.LNX.4.61.0501031114520.12157@gannet.stats>

On Mon, 3 Jan 2005, Richard Dybowski wrote:

> In S-Plus, I can look at the structure of a function (for example, hist) 
> simply by entering
>        hist <RETURN>
> however, if I do this in R, I get the response
>        function (x, ...)
>        UseMethod("hist")
>        <environment: namespace:graphics>
> How can I inspect the structure of a function in R?

I think you mean `read the code', as S-PLUS has inspect() that does 
something different, and you don't want the internal structure, do you?

You have! hist() in R is generic, and it is not in S-PLUS.

See also ?methods and ?getAnywhere,

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From buser at stat.math.ethz.ch  Mon Jan  3 12:18:18 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Mon, 3 Jan 2005 12:18:18 +0100
Subject: [R] different DF in package nlme and lme4
Message-ID: <16857.10746.171989.319959@stat.math.ethz.ch>

Hi all

I tried to reproduce an example with lme and used the Orthodont
dataset.

library(nlme)
fm2a.1 <- lme(distance ~ age + Sex, data = Orthodont, random = ~ 1 | Subject)
anova(fm2a.1)
>             numDF denDF  F-value p-value
> (Intercept)     1    80 4123.156  <.0001
> age             1    80  114.838  <.0001
> Sex             1    25    9.292  0.0054

or alternatively (to get the same result)

fm2a.2 <- lme(distance ~ age + Sex, data = Orthodont, random = list(Subject = ~ 1))
anova(fm2a.2)
>             numDF denDF  F-value p-value
> (Intercept)     1    80 4123.156  <.0001
> age             1    80  114.838  <.0001
> Sex             1    25    9.292  0.0054

---------------------------------------------------------------
Then I restarted!!! R to use the lme4 package instead of nlme.
---------------------------------------------------------------

library(lme4)
fm2b.1 <- lme(distance ~ age + Sex, data = Orthodont, random = ~ 1 | Subject)
anova(fm2b.1)
> Analysis of Variance Table
>     Df  Sum Sq Mean Sq   Denom  F value    Pr(>F)    
> age  1 235.356 235.356 105.000 114.8383 < 2.2e-16 ***
> Sex  1  19.044  19.044 105.000   9.2921  0.002912 ** 

or alternatively (to get the same result)

fm2b.2 <- lme(distance ~ age + Sex, data = Orthodont, random = list(Subject = ~ 1anova(fm2b.2)
> Analysis of Variance Table
>     Df  Sum Sq Mean Sq   Denom  F value    Pr(>F)    
> age  1 235.356 235.356 105.000 114.8383 < 2.2e-16 ***
> Sex  1  19.044  19.044 105.000   9.2921  0.002912 ** 


I got different DF for the denominator. Do I have to use lme in
another way in the package lme4?

I use R 2.0.1 under linux and
Package:       nlme
Version:       3.1-53
Date:          2004-11-03
Package:       lme4
Version:       0.6-11
Date:          2004-12-16

Thanks for help.

Regards, 

Christoph Buser

-- 
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C11
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-1-632-5414		fax: 632-1228
http://stat.ethz.ch/~buser/



From murdoch at stats.uwo.ca  Mon Jan  3 13:01:42 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 03 Jan 2005 07:01:42 -0500
Subject: [R] Inspecting R functions
In-Reply-To: <5.2.1.1.2.20050103104041.00acabb0@mailhost.zen.co.uk>
References: <5.2.1.1.2.20050103104041.00acabb0@mailhost.zen.co.uk>
Message-ID: <8pcit016k4fsv5n3lntnrpt3b6gkd47l96@4ax.com>

On Mon, 03 Jan 2005 10:50:22 +0000, Richard Dybowski
<richard at inferspace.com> wrote:

>In S-Plus, I can look at the structure of a function (for example, hist) 
>simply by entering
>         hist <RETURN>
>however, if I do this in R, I get the response
>         function (x, ...)
>         UseMethod("hist")
>         <environment: namespace:graphics>
>How can I inspect the structure of a function in R?

It's the same as in S-PLUS, what you saw *is* the structure of hist in
R, i.e. it is an S3 generic.

To see the implementation for a particular class, you need to say
which method you want to look at, and then use getS3method to see it.
For example,

getS3method('hist', 'default')

shows the default method.

Duncan



From graham.smith at myotis.co.uk  Mon Jan  3 13:19:39 2005
From: graham.smith at myotis.co.uk (Graham Smith)
Date: Mon, 3 Jan 2005 12:19:39 -0000
Subject: [R] spreadsheet addiction
In-Reply-To: <41D92469.1010608@burns-stat.com>
Message-ID: <20050103121939.5CA7A133F37@mra04.ex.eclipse.net.uk>

Patrick,

A very interesting and useful read, but following up on your final point,
should anyone be interested, Quatro Pro allows for 18,000 columns rather
than the 256 columns of Excel, and has done for as many versions back as I
can remember.

Not being expert enough in R I have found that this ability of Quattro
allows me to prepare a data set for importing into a Stats program that is
far larger than Excel can cope with.

Having said that, like most people, my day to day spreadsheet is Excel.

Graham



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Patrick Burns
> Sent: 03 January 2005 10:55
> To: R Help, Listserve
> Subject: [R] spreadsheet addiction
> 
> There's a new page on the Burns Statistics website 
> http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html
> that looks at spreadsheets from a quality assurance 
> perspective. It presents R as a suitable alternative to 
> spreadsheets.  Also there are several specific problems with 
> Excel that are highlighted, including the status of 
> statistical functionality in Excel.
> 
> Patrick Burns
> 
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> 
> __________ NOD32 1.953 (20041219) Information __________
> 
> This message was checked by NOD32 antivirus system.
> http://www.nod32.com
> 
>



From bates at stat.wisc.edu  Mon Jan  3 16:10:09 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 03 Jan 2005 09:10:09 -0600
Subject: [R] different DF in package nlme and lme4
In-Reply-To: <16857.10746.171989.319959@stat.math.ethz.ch>
References: <16857.10746.171989.319959@stat.math.ethz.ch>
Message-ID: <41D96051.4080907@stat.wisc.edu>

Christoph Buser wrote:
> Hi all
> 
> I tried to reproduce an example with lme and used the Orthodont
> dataset.
> 
> library(nlme)
> fm2a.1 <- lme(distance ~ age + Sex, data = Orthodont, random = ~ 1 | Subject)
> anova(fm2a.1)
> 
>>            numDF denDF  F-value p-value
>>(Intercept)     1    80 4123.156  <.0001
>>age             1    80  114.838  <.0001
>>Sex             1    25    9.292  0.0054
> 
> 
> or alternatively (to get the same result)
> 
> fm2a.2 <- lme(distance ~ age + Sex, data = Orthodont, random = list(Subject = ~ 1))
> anova(fm2a.2)
> 
>>            numDF denDF  F-value p-value
>>(Intercept)     1    80 4123.156  <.0001
>>age             1    80  114.838  <.0001
>>Sex             1    25    9.292  0.0054
> 
> 
> ---------------------------------------------------------------
> Then I restarted!!! R to use the lme4 package instead of nlme.
> ---------------------------------------------------------------
> 
> library(lme4)
> fm2b.1 <- lme(distance ~ age + Sex, data = Orthodont, random = ~ 1 | Subject)
> anova(fm2b.1)
> 
>>Analysis of Variance Table
>>    Df  Sum Sq Mean Sq   Denom  F value    Pr(>F)    
>>age  1 235.356 235.356 105.000 114.8383 < 2.2e-16 ***
>>Sex  1  19.044  19.044 105.000   9.2921  0.002912 ** 
> 
> 
> or alternatively (to get the same result)
> 
> fm2b.2 <- lme(distance ~ age + Sex, data = Orthodont, random = list(Subject = ~ 1anova(fm2b.2)
> 
>>Analysis of Variance Table
>>    Df  Sum Sq Mean Sq   Denom  F value    Pr(>F)    
>>age  1 235.356 235.356 105.000 114.8383 < 2.2e-16 ***
>>Sex  1  19.044  19.044 105.000   9.2921  0.002912 ** 
> 
> 
> 
> I got different DF for the denominator. Do I have to use lme in
> another way in the package lme4?
> 
> I use R 2.0.1 under linux and
> Package:       nlme
> Version:       3.1-53
> Date:          2004-11-03
> Package:       lme4
> Version:       0.6-11
> Date:          2004-12-16
> 
> Thanks for help.
> 
> Regards, 
> 
> Christoph Buser
> 

No.  The calculation of denominator degrees of freedom in lme4 is bogus 
and I believe this is documented.  Note that for all practical purposes 
there is very little difference between 25 and 100 denominator degrees 
of freedom.

lme4 is under development (and has been for a seemingly interminable 
period of time).  Getting the denominator degrees of freedom calculation 
"right" is way down the list of priorities.

Many people express dismay about the calculation of denominator degrees 
of freedom in all versions of lme4.  IIRC Frank Harrell characterizes 
this as one of the foremost deficiencies in R relative to SAS.  I don't 
agree that this is a glaring deficiency.  In fact I believe that there 
is no "correct" answer.  The F statistics in a mixed model do not have 
an F distribution under the null hypothesis.  It's all an approximation, 
which is why I don't stay up nights worrying about the exact details of 
the approximation.

My plan for lme4 is that one slot in the summary object for an lme model 
will be an incidence table of terms in the fixed effects versus grouping 
factors for the random effects.  This table will indicate whether a 
given term varies within groups defined by the grouping factor.  Anyone 
who wants to implement their personal favorite calculation of 
denominator degrees of freedom based on this table will be welcome to do so.

I personally think that tests on the fixed-effects terms will be better 
implemented using the restricted likelihood-ratio tests defined by 
Reinsel and Ahn rather than the Wald tests and the whole issue of 
denominator degrees of freedom may be moot.

My apologies if I seem to be peeved.  I am not upset by your question - 
it is an entirely reasonable question.  It is just that I have discussed 
the issue of denominator degrees of freedom too many times.

To me a more important objective of lme4 is to be able to handle random 
effects associated with crossed or partially crossed grouping factors. 
I believe that in those cases the calculation of denominator degrees of 
freedom will be very complicated and even more of an approximation than 
in the case of nested grouping factors.  This is why I would rather 
finesse the whole issue by using the Reinsel and Ahn approach.



From francoisromain at free.fr  Mon Jan  3 16:27:24 2005
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Mon, 03 Jan 2005 16:27:24 +0100
Subject: [R] speed of the cluster.stats function
Message-ID: <41D9645C.5040400@free.fr>

Hello list (happy new yeaR),

Here's a copy of a message i just send to Christian Hennig (who wrote 
the fpc package).
That may interrest some of you, and maybe someone could have a better 
solution than mine.

Romain.

------------------------------------------------------------------------------------------

Mister Hennig,

[[[ I'm writing in english because i don't know german langage and i 
don't know if you know french ]]]

I'm a student in "Institut de Statistique de l'Universit? de Paris" 
using a lot the library fpc that you built for R, specially the 
cluster.stats function. In that function, the calculation of G2 index 
(Goodman & Kruskal) could be really slow as you warned in the help page 
for that function. That speed problem is due to the double loop for(i in 
1:nwithin) for(j in 1:nbetween).

I came up with a solution (probably not the best, but ....) that is 
really faster than your's (with all due respect). (You can see the speed 
calculation above). What i did was just vectorizing the second loop. See 
the code in the patch above.

Could be a good thing for the next fpc release.

Cordially.

Romain Francois.



---------------------------- Time calculation 
------------------------------------------

> dis <- dist(USArrests) # 50 observations, 4 variables
> hcl <- hclust(dis)
> gro <- cutree(hcl,3)
> system.time(print(cluster.stats(dis,gro,G2=T)$g2))

[1] 0.887726    # the G2 value calculated by your function (just to make 
sure that's the same)
[1] 2.87 0.00 2.89   NA   NA
    ^^^^
Warning message: non-square matrix in: as.dist(separation)

> system.time(print(R.cluster.stats(dis,gro,G2=T)$g2))

[1] 0.887726     # the G2 of my function (same value as your's)
[1] 0.12 0.00 0.12   NA   NA
    ^^^^
Warning message: non-square matrix in: as.dist(separation)
--------------------------------------------------------------------------------------- 


--------------------------- patch 
-----------------------------------------------------
...
   if (G2) {
       splus <- sminus <- 0
       for (i in 1:nwithin) {
          splus  <- splus  + sum(within.dist[i]<between.dist)
          sminus <- sminus + sum(within.dist[i]>between.dist)        }
       g2 <- (splus - sminus)/(splus + sminus)
   }
...
--------------------------------------------------------------------------------------- 


-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From kjetil at acelerate.com  Mon Jan  3 16:15:19 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 03 Jan 2005 11:15:19 -0400
Subject: [R] dataset lutenhorm in package bootstrap
Message-ID: <41D96187.40204@acelerate.com>

The dataset lutenhorm in package bootstrap has four
time series, while the book has only one. The one from the book is
"V4". "V1" is just the time indeces. Anybody knows wht is
"V2", "V3", "V5"
?

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra





-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From a.beckerman at sheffield.ac.uk  Mon Jan  3 16:32:56 2005
From: a.beckerman at sheffield.ac.uk (Andrew Beckerman)
Date: Mon, 3 Jan 2005 15:32:56 +0000
Subject: [R] LME-glmmPQL formulation
Message-ID: <BDA602BD-5D9C-11D9-91BB-000A95CD7F02@sheffield.ac.uk>

Hi all -
R2.0.1 on OSX;MASS library;nlme library

I am trying to emulate the solution to a problem set that has normally  
been run in Genstat, using R.  The problem that I am having at the  
moment is with the following glmm question (using glmmPQL from the MASS  
library):

"We have two different forest habitats (first rotation thicket, and  
high forest) which we want to survey for the presence of our study  
animal. We survey both habitats on each of 10 days, and within each  
habitat we have five transects. The sampling unit is the number of  
animals counted per transect.  We therefore have two sources of random  
variation:
?   counts will vary between days due, say, to variation in weather
?   counts will vary between transects, within sites, for any number of  
(known and unknown) reasons
There is no relationship between transects at the two sites: transect 1  
in site 1 has no link to transect 1 in site 2, etc. The random term for  
transectis therefore nested within site, while the main effect of site,  
which is what we are interested in, is a fixed effect."

 > summary(dat)
     site         day           trans       count
  Here :50   Min.   : 1.0   Min.   :1   Min.   : 48.00
  There:50   1st Qu.: 3.0   1st Qu.:2   1st Qu.: 79.00
             Median : 5.5   Median :3   Median : 95.00
             Mean   : 5.5   Mean   :3   Mean   : 95.85
             3rd Qu.: 8.0   3rd Qu.:4   3rd Qu.:112.25
             Max.   :10.0   Max.   :5   Max.   :165.00

In Genstat, the (supposed) procedure is to fit a model with site as a  
fixed effect and then a random effects model of day+transect.site,  
where the transect.site indicates that there are 5 transects nested  
within each site.

My first thought was the following:

glmmPQL(count~site,data=dat,random=~day|site/transect, family="poisson")

however, the random effects are not separated into day and  
site/transect.  Instead, there is day|site and day|site %in% transect,  
which I realize makes sense in light of the model formulation.

my second guess was

glmmPQL(count~site,random=list(~day|site,~1|trans),family="poisson",data 
=dat2)

which estimates a random effect on ~day|site and on  
~1|trans%in%site..... which seems more appropriate, but does not give  
the same answers as I have for the genstat; nor does it estimate the  
p-value for site.  I guess my question is how to separate the two  
random effects so that there is a estimate for day and for  
transect/site.

I would be happy to provide the data if anyone needs/wants IT.

Cheers
andrew

------------------------------------------------------------------------ 
---------
Dr. Andrew Beckerman
Department of Animal and Plant Sciences, University of Sheffield,
Alfred Denny Building, Western Bank, Sheffield S10 2TN, UK
ph +44 (0)114 222 0026; fx +44 (0)114 222 0002
http://www.shef.ac.uk/beckslab
------------------------------------------------------------------------ 
----------



From HDoran at air.org  Mon Jan  3 17:10:54 2005
From: HDoran at air.org (Doran, Harold)
Date: Mon, 3 Jan 2005 11:10:54 -0500
Subject: [R] Memory Efficient Methods for Building Matrix
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7407166139@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050103/1cd6d815/attachment.ksh

From sivana1 at techunix.technion.ac.il  Mon Jan  3 17:15:44 2005
From: sivana1 at techunix.technion.ac.il (Sivan Aldor)
Date: Mon, 3 Jan 2005 18:15:44 +0200
Subject: [R] building phylogenetic trees
Message-ID: <018401c4f1af$7a568d80$e4a24484@AldorSivan>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050103/22ef9f01/attachment.ksh

From f.harrell at vanderbilt.edu  Mon Jan  3 17:42:30 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 03 Jan 2005 11:42:30 -0500
Subject: [R] different DF in package nlme and lme4
In-Reply-To: <41D96051.4080907@stat.wisc.edu>
References: <16857.10746.171989.319959@stat.math.ethz.ch>
	<41D96051.4080907@stat.wisc.edu>
Message-ID: <41D975F6.7060803@vanderbilt.edu>

Douglas Bates wrote:
> Christoph Buser wrote:
> 
>> Hi all
>>
>> I tried to reproduce an example with lme and used the Orthodont
>> dataset.
>>
>> library(nlme)
>> fm2a.1 <- lme(distance ~ age + Sex, data = Orthodont, random = ~ 1 | 
>> Subject)
>> anova(fm2a.1)
...

>> Regards,
>> Christoph Buser
>>
> 
> No.  The calculation of denominator degrees of freedom in lme4 is bogus 
> and I believe this is documented.  Note that for all practical purposes 
> there is very little difference between 25 and 100 denominator degrees 
> of freedom.
> 
> lme4 is under development (and has been for a seemingly interminable 
> period of time).  Getting the denominator degrees of freedom calculation 
> "right" is way down the list of priorities.
> 
> Many people express dismay about the calculation of denominator degrees 
> of freedom in all versions of lme4.  IIRC Frank Harrell characterizes 
> this as one of the foremost deficiencies in R relative to SAS.  I don't 
> agree that this is a glaring deficiency.  In fact I believe that there 
> is no "correct" answer.  The F statistics in a mixed model do not have 
> an F distribution under the null hypothesis.  It's all an approximation, 
> which is why I don't stay up nights worrying about the exact details of 
> the approximation.

Doug - the main concern is accurate P-values; I don't really care which 
approximations are best, just that the ones used are at least as good as 
those in SAS.  Without being an expert, I have come to believe that at 
the moment SAS is better than R in 2 areas: accurate P-values from mixed 
models and handling massive databases.  On the former point I could 
easily be swayed by some type I error simulations.

> 
> My plan for lme4 is that one slot in the summary object for an lme model 
> will be an incidence table of terms in the fixed effects versus grouping 
> factors for the random effects.  This table will indicate whether a 
> given term varies within groups defined by the grouping factor.  Anyone 
> who wants to implement their personal favorite calculation of 
> denominator degrees of freedom based on this table will be welcome to do 
> so.

I will be interested also to see timings of lme4 (using S4) vs nlme 
(using S3) for the same model.

Cheers,

Frank

> 
> I personally think that tests on the fixed-effects terms will be better 
> implemented using the restricted likelihood-ratio tests defined by 
> Reinsel and Ahn rather than the Wald tests and the whole issue of 
> denominator degrees of freedom may be moot.
> 
> My apologies if I seem to be peeved.  I am not upset by your question - 
> it is an entirely reasonable question.  It is just that I have discussed 
> the issue of denominator degrees of freedom too many times.
> 
> To me a more important objective of lme4 is to be able to handle random 
> effects associated with crossed or partially crossed grouping factors. I 
> believe that in those cases the calculation of denominator degrees of 
> freedom will be very complicated and even more of an approximation than 
> in the case of nested grouping factors.  This is why I would rather 
> finesse the whole issue by using the Reinsel and Ahn approach.
>



From KWollenberg at tufts-nemc.org  Mon Jan  3 18:01:31 2005
From: KWollenberg at tufts-nemc.org (Wollenberg, Kurt R)
Date: Mon, 3 Jan 2005 12:01:31 -0500 
Subject: [R] Calculating symbol (letter) frequencies
Message-ID: <A16C769D4A7E564597075EA02A91C003047E87BE@neexchange01.nemc.org>

Hello:

I am attempting to use R to analyze amino acid frequencies in aligned
protein sequences and need some help. So far, I have imported my sequence
alignment into a data frame (lets call it "alignment") with each site in one
column, so that I have a data frame consisting of columns of letters (the 21
amino acid symbols plus "-") with row names being the corresponding protein
names. >summary(alignment) gives me the counts of symbols in each column.
Now I would like to convert these counts into frequencies and perform
calculations on these frequencies. Do I need to place the individual
elements in summary(alignment) in a separate data frame to perform
calculations on them? What I've been thinking of is doing is creating a data
frame of symbol frequencies with each column corresponding to a column in
the sequence alignment. If it makes sense to do this how do I extract these
data into a data frame so that I can perform further analyses on these
frequencies? I've tried >DF1 <- data.frame(alignment, row.names=AA), where
AA is a character vector of amino acid symbols plus "-", but the error
message tells me that the "row names supplied are of the wrong length". As
not all of the symbols are present in each column of "alignment" this makes
some sense to me, as each summary(alignment[[i]]) varies in length. Also, I
would need to match up the individual symbol entries in each
summary(alignment[[i]]) with the corresponding row in the new data frame
(which I believe can be efficiently done with indexing, but I can't put my
finger on an appropriate example of how to do this). I have looked at the
package Biostrings on the Bioconductor site but it doesn't appear to work
with amino acid sequence alignments. So my questions to the R-help community
are: Can I do various statistical calculations on and using
summary(alignment[[i]]) or do I need a separate data frame? If I should be
using a separate data frame for symbol frequencies how do I extract these
from the data? Should I try to extract this from summary or is there a more
efficient way to calculate symbol frequencies?

Thanks,
Kurt Wollenberg, PhD
Tufts Center for Vision Research 
New England Medical Center
750 Washington St, Box 450 
Boston, MA, USA
kwollenberg at tufts-nemc.org 
617-636-8945 (Fax)
617-636-9028 (Lab)

The most exciting phrase to hear in science, the one that heralds new
discoveries, is not "Eureka!" (I found it!) but  "That's funny ..." 
--Isaac Asimov


********************** 
Confidentiality Notice\ **********************\      The inf...{{dropped}}



From grenyer at virginia.edu  Mon Jan  3 18:10:09 2005
From: grenyer at virginia.edu (Rich Grenyer)
Date: Mon, 03 Jan 2005 12:10:09 -0500
Subject: [R] building phylogenetic trees
In-Reply-To: <018401c4f1af$7a568d80$e4a24484@AldorSivan>
Message-ID: <web-103997716@cgatepro-4.mail.virginia.edu>

Hi Silvan - I'd love to proven wrong, but I think unless 
you're interested in a cluster dendrogram 
(neighbour-joining or UPGMA etc.) that the most standard 
phylogeny-generation algorithms aren't implemented as R 
functions (yet). There are very efficient third party 
programs for tree construction (PAUP* at 
http://paup.csit.fsu.edu is the default, PHYLIP at 
http://evolution.genetics.washington.edu/phylip.html is 
99% as comprehensive, and free) and the ape package 
provides a way of getting your trees and data into R for 
any further analysis you want.

Regards,

Rich


On Mon, 3 Jan 2005 18:15:44 +0200
  "Sivan Aldor" <sivana1 at techunix.technion.ac.il> wrote:
>Hello,
>My name is Sivan and I am a master degree student in 
>statistics,my problem is as follows:
>I have a dataset containing gene sequences and I would 
>like to create a phylogenetic tree from it.
>The problem that I can't seem to find a function to do 
>this kind of operation. I read the ape package manual and 
>I haven't found a command that takes raw data and turns 
>it into a tree.
>does anyone know if such a command exist?
>thank you in advance,
>sivan
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html

-------------------------------
Rich Grenyer, Ph.D.
Postdoctoral Research Associate
Biology Department - University of Virginia
Charlottesville, VA 22904
USA

Tel: +1-434-982-5629 Fax: +1-434-982-5626
email: grenyerDELETE at virginia.edu



From bates at stat.wisc.edu  Mon Jan  3 18:14:02 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 03 Jan 2005 11:14:02 -0600
Subject: [R] different DF in package nlme and lme4
In-Reply-To: <41D975F6.7060803@vanderbilt.edu>
References: <16857.10746.171989.319959@stat.math.ethz.ch>
	<41D96051.4080907@stat.wisc.edu> <41D975F6.7060803@vanderbilt.edu>
Message-ID: <41D97D5A.10402@stat.wisc.edu>

Frank E Harrell Jr wrote:
> Douglas Bates wrote:
> 
>> Christoph Buser wrote:
>>
>>> Hi all
>>>
>>> I tried to reproduce an example with lme and used the Orthodont
>>> dataset.
>>>
>>> library(nlme)
>>> fm2a.1 <- lme(distance ~ age + Sex, data = Orthodont, random = ~ 1 | 
>>> Subject)
>>> anova(fm2a.1)
> 
> ...
> 
>>> Regards,
>>> Christoph Buser
>>>
>>
>> No.  The calculation of denominator degrees of freedom in lme4 is 
>> bogus and I believe this is documented.  Note that for all practical 
>> purposes there is very little difference between 25 and 100 
>> denominator degrees of freedom.
>>
>> lme4 is under development (and has been for a seemingly interminable 
>> period of time).  Getting the denominator degrees of freedom 
>> calculation "right" is way down the list of priorities.
>>
>> Many people express dismay about the calculation of denominator 
>> degrees of freedom in all versions of lme4.  IIRC Frank Harrell 
>> characterizes this as one of the foremost deficiencies in R relative 
>> to SAS.  I don't agree that this is a glaring deficiency.  In fact I 
>> believe that there is no "correct" answer.  The F statistics in a 
>> mixed model do not have an F distribution under the null hypothesis.  
>> It's all an approximation, which is why I don't stay up nights 
>> worrying about the exact details of the approximation.
> 
> 
> Doug - the main concern is accurate P-values; I don't really care which 
> approximations are best, just that the ones used are at least as good as 
> those in SAS.  Without being an expert, I have come to believe that at 
> the moment SAS is better than R in 2 areas: accurate P-values from mixed 
> models and handling massive databases.  On the former point I could 
> easily be swayed by some type I error simulations.
> 
>>
>> My plan for lme4 is that one slot in the summary object for an lme 
>> model will be an incidence table of terms in the fixed effects versus 
>> grouping factors for the random effects.  This table will indicate 
>> whether a given term varies within groups defined by the grouping 
>> factor.  Anyone who wants to implement their personal favorite 
>> calculation of denominator degrees of freedom based on this table will 
>> be welcome to do so.
> 
> 
> I will be interested also to see timings of lme4 (using S4) vs nlme 
> (using S3) for the same model.

Such comparisons would be more heavily influenced by the different 
algorithms used in the two packages than by S3 versus S4.  The lme4 
package is not just a translation of the lme part of nlme into S4 
classes and methods.  It is a complete reimplementation from scratch.
It indeed faster than the code in nlme but more important is the fact 
that it will handle models/data sets that simply could not be fit in nlme.



From tlumley at u.washington.edu  Mon Jan  3 18:54:01 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 3 Jan 2005 09:54:01 -0800 (PST)
Subject: [R] Memory Efficient Methods for Building Matrix
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7407166139@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7407166139@dc1ex2.air.org>
Message-ID: <Pine.A41.4.61b.0501030936240.322794@homer06.u.washington.edu>

On Mon, 3 Jan 2005, Doran, Harold wrote:

> Dear List:
>
> I am having to build a block-diagonal matrix (vl) and am currently using
> the following code.
>
> I<-diag(sample.size)
> vl<-kronecker(I,vl.mat)
>
>
> This code works fine, but for large N, it is a huge memory hog. Is there
> a more efficient method for constructing vl?
>

Obvious alternatives such as

nr<-nrow(v1.mat)
nc<-ncol(v1.mat)
result<-matrix(0,nrow=sample.size*nr,ncol=sample.size*nc)
for(i in 1:sample.size){
     result[ (i-1)*nr+1:nr, (i-1)*nc+1:nc]<-v1.mat
}

or
nr<-nrow(v1.mat)
nc<-ncol(v1.mat)
iy<-as.vector(outer(rep(1:nc,each=nr),(0:(sample.size-1))*nc,"+"))
ix<-as.vector(outer(rep(1:nr, nc),(0:(sample.size-1))*nr,"+"))
result<-matrix(0,nrow=sample.size*nr,ncol=sample.size*nc)
result[cbind(ix,iy)]<-v1.mat

seem to take a little less memory and about the same time,
but constructing a large block-diagonal matrix is intrinsically an
inefficient thing to do.


 	-thomas



From lcruz at mail.grupomm.com.br  Mon Jan  3 18:55:36 2005
From: lcruz at mail.grupomm.com.br (Leila Cruz)
Date: Mon, 3 Jan 2005 15:55:36 -0200
Subject: [R] =?iso-8859-1?q?Resposta_Autom=E1tica?=
Message-ID: <200501031755.j03HtaFS026378@mail.grupomm.com.br>

Estarei de f?rias no per?odo de 20 de dezembro ? 03 de fevereiro de 2005.
Durante minha aus?ncia, favor procurar Josi (mmrio at grupomm.com.br) 3235-5109.
Obrigada.



From gunter.berton at gene.com  Mon Jan  3 19:14:16 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 3 Jan 2005 10:14:16 -0800
Subject: [R] Calculating symbol (letter) frequencies
In-Reply-To: <A16C769D4A7E564597075EA02A91C003047E87BE@neexchange01.nemc.org>
Message-ID: <200501031814.j03IEGvd027479@compton.gene.com>


My best advice:

Yours is a complicated question, but it is probably the wrong question. I
suspect that you are trying to reinvent wheels: I believe there has been a
lot of work in the statistical community on (OK, maybe DNA not amino acid)
sequence alignment. Undoubtedly much more published in the
biological/bioinformatics literature that I'm unaware of. I think you should
collaborate with a statistician at your institution to help you access that
literature and its methods, which are almost certainly implementable and
probably implemented within R. Perhaps even on BioConductor (despite your
lack of success there thus far).

My far poorer advice:

See ?lapply and relatives, as well as ?table and the links therein. Also
?factor may be of interest.

lapply(alignment,function(x)table(x)/length(x)) 

will return a list of length the number of sites (columns), each component
of which is a frequency table of the letters at the respective site. You can
then further process these results as you like. Is this the sort of thing
that you want?

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Wollenberg, Kurt R
> Sent: Monday, January 03, 2005 9:02 AM
> To: 'r-help at stat.math.ethz.ch'
> Subject: [R] Calculating symbol (letter) frequencies
> 
> Hello:
> 
> I am attempting to use R to analyze amino acid frequencies in aligned
> protein sequences and need some help. So far, I have imported 
> my sequence
> alignment into a data frame (lets call it "alignment") with 
> each site in one
> column, so that I have a data frame consisting of columns of 
> letters (the 21
> amino acid symbols plus "-") with row names being the 
> corresponding protein
> names. >summary(alignment) gives me the counts of symbols in 
> each column.
> Now I would like to convert these counts into frequencies and perform
> calculations on these frequencies. Do I need to place the individual
> elements in summary(alignment) in a separate data frame to perform
> calculations on them? What I've been thinking of is doing is 
> creating a data
> frame of symbol frequencies with each column corresponding to 
> a column in
> the sequence alignment. If it makes sense to do this how do I 
> extract these
> data into a data frame so that I can perform further analyses on these
> frequencies? I've tried >DF1 <- data.frame(alignment, 
> row.names=AA), where
> AA is a character vector of amino acid symbols plus "-", but the error
> message tells me that the "row names supplied are of the 
> wrong length". As
> not all of the symbols are present in each column of 
> "alignment" this makes
> some sense to me, as each summary(alignment[[i]]) varies in 
> length. Also, I
> would need to match up the individual symbol entries in each
> summary(alignment[[i]]) with the corresponding row in the new 
> data frame
> (which I believe can be efficiently done with indexing, but I 
> can't put my
> finger on an appropriate example of how to do this). I have 
> looked at the
> package Biostrings on the Bioconductor site but it doesn't 
> appear to work
> with amino acid sequence alignments. So my questions to the 
> R-help community
> are: Can I do various statistical calculations on and using
> summary(alignment[[i]]) or do I need a separate data frame? 
> If I should be
> using a separate data frame for symbol frequencies how do I 
> extract these
> from the data? Should I try to extract this from summary or 
> is there a more
> efficient way to calculate symbol frequencies?
> 
> Thanks,
> Kurt Wollenberg, PhD
> Tufts Center for Vision Research 
> New England Medical Center
> 750 Washington St, Box 450 
> Boston, MA, USA
> kwollenberg at tufts-nemc.org 
> 617-636-8945 (Fax)
> 617-636-9028 (Lab)
> 
> The most exciting phrase to hear in science, the one that heralds new
> discoveries, is not "Eureka!" (I found it!) but  "That's funny ..." 
> --Isaac Asimov
> 
> 
> ********************** 
> Confidentiality Notice\ **********************\      The 
> inf...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ggrothendieck at myway.com  Mon Jan  3 19:14:50 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 3 Jan 2005 18:14:50 +0000 (UTC)
Subject: [R] Memory Efficient Methods for Building Matrix
References: <88EAF3512A55DF46B06B1954AEF73F7407166139@dc1ex2.air.org>
	<Pine.A41.4.61b.0501030936240.322794@homer06.u.washington.edu>
Message-ID: <loom.20050103T191402-857@post.gmane.org>

Thomas Lumley <tlumley <at> u.washington.edu> writes:

: 
: On Mon, 3 Jan 2005, Doran, Harold wrote:
: 
: > Dear List:
: >
: > I am having to build a block-diagonal matrix (vl) and am currently using
: > the following code.
: >
: > I<-diag(sample.size)
: > vl<-kronecker(I,vl.mat)
: >
: >
: > This code works fine, but for large N, it is a huge memory hog. Is there
: > a more efficient method for constructing vl?
: >
: 
: Obvious alternatives such as
: 
: nr<-nrow(v1.mat)
: nc<-ncol(v1.mat)
: result<-matrix(0,nrow=sample.size*nr,ncol=sample.size*nc)
: for(i in 1:sample.size){
:      result[ (i-1)*nr+1:nr, (i-1)*nc+1:nc]<-v1.mat
: }
: 
: or
: nr<-nrow(v1.mat)
: nc<-ncol(v1.mat)
: iy<-as.vector(outer(rep(1:nc,each=nr),(0:(sample.size-1))*nc,"+"))
: ix<-as.vector(outer(rep(1:nr, nc),(0:(sample.size-1))*nr,"+"))
: result<-matrix(0,nrow=sample.size*nr,ncol=sample.size*nc)
: result[cbind(ix,iy)]<-v1.mat
: 
: seem to take a little less memory and about the same time,
: but constructing a large block-diagonal matrix is intrinsically an
: inefficient thing to do.

You could try combining some of those ideas with use of the SparseM
package.



From r.verhaak at erasmusmc.nl  Mon Jan  3 20:13:54 2005
From: r.verhaak at erasmusmc.nl (R.G.W. Verhaak)
Date: Mon, 3 Jan 2005 20:13:54 +0100 (MET)
Subject: [BioC] Re: [R] Configuration of memory usage
In-Reply-To: <x2brcez14n.fsf@biostat.ku.dk>
References: <x2brcez14n.fsf@biostat.ku.dk>
Message-ID: <10671.82.169.216.132.1104779634.squirrel@82.169.216.132>


Hi,

I had the same opportunity and ran into the same problem. The error
occurred when trying to read >25 CEL-files with the ReadAffy-package and
happened in R1.9 as well as R2.0. The problem seemed to be due to an error
in the Tcl/Tk package and might relate to the installation of Tcl/Tk on
our Irix machine. We did not completely tracked it down, as we than
discovered that we were using Bioconductor 1.4 (which actually worked on a
second Irix machine with R1.9).  Upgrading this to version 1.5 solved all
our problems.
Bottomline: make sure you use R2.0, Bioconductor 1.5 and install the most
recent packages available.

Regards,
Roel Verhaak


> Tae-Hoon Chung <thchung at tgen.org> writes:
>
>> Hi, all;
>>
>> I know there has been a lot of discussions on memory usage in R.
>> However, I have some odd situation here. Basically, I have a rare
>> opportunity to run R in a system with 64GB memory without any limit on
>> memory usage for any person or process. However, I encountered the
>> memory
>> problem error message like this:
>>
>> Error: cannot allocate vector of size 594075 Kb
> ....
>> Although I have no idea of memory allocation in R, apparently
>> something's
>> wrong with this. The memory problem must have nothing to do with
>> physical
>> memory. My question is this. Is this memory problem due to some
>> non-optimal
>> configuration of memory usage? If so, then what will be the optimal
>> configuration for this? If not, then there must be problems on actual
>> implementations of functions I used here, right? The reason I am asking
>> this
>> is that, according to the reference manual, the error message I got can
>> be
>> brought up by roughly three reasons. First, when the system is unable to
>> provide the R requested memory. Second, when the requested memory size
>> exceeds the address-space limit for a process. Finally, when the length
>> of a
>> vector is larger than 2^31-1.
>
> Hmm, the length issue should not kick in before the length exceeds 2
> billion or so and you are not beyond 75 or 150 million (counting 8 or
> 4 bytes per elements).
>
>> I wonder the problem has anything to do with
>> the third case. (If so, then I think I am hopeless unless the internal
>> implementations change...)
>
> Well, revolutionaries often find themselves just below the cutting
> edge...
>
> Just a sanity check: this is using a 64-bit compiled R on a 64-bit
> operating system, right?
>
> --
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>
>
>



From tlumley at u.washington.edu  Mon Jan  3 20:36:27 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 3 Jan 2005 11:36:27 -0800 (PST)
Subject: [R] Use of expand.model.frame()
In-Reply-To: <loom.20050101T161738-66@post.gmane.org>
References: <20050101141207.VSQQ25979.tomts25-srv.bellnexxia.net@JohnDesktop8300>
	<loom.20050101T161738-66@post.gmane.org>
Message-ID: <Pine.A41.4.61b.0501031132390.322794@homer06.u.washington.edu>

On Sat, 1 Jan 2005, Gabor Grothendieck wrote:

> Look at the source of expand.model.frame.
>
> The second line of expand.model.frame uses the data component of mod$call
> and if its not there expand.model.frame chokes.  As a workaround check
> if its missing and supply it yourself:
>
> if (is.null(mod$call$data)) mod$call$data <- environment(formula(mod))
> expand.model.frame(mod, "z")
>
> expand.model.frame should probably be doing that itself.
>

No, it shouldn't.

It should be setting data to as.list(NULL) if it is NULL.

I am surprised to find that
>  y<-1:10
> eval(quote(y), NULL, .GlobalEnv)
Error in eval(expr, envir, enclos) : Object "y" not found
> eval(quote(y), as.list(NULL), .GlobalEnv)
  [1]  1  2  3  4  5  6  7  8  9 10
> eval(quote(y), as.data.frame(NULL), .GlobalEnv)
Error in eval(quote(y), as.data.frame(NULL), .GlobalEnv) :
         attempt to set an attribute on NULL

The third of these is definitely a bug, and I thought the first used to 
work (and that I would have noticed when writing expand.model.frame if it 
didn't).

 	-thomas



From michalak at uta.edu  Mon Jan  3 20:40:03 2005
From: michalak at uta.edu (Michalak, Pawel)
Date: Mon, 3 Jan 2005 13:40:03 -0600
Subject: [R] power.t.test plots
Message-ID: <6D8CAC895F95814FA9CD510290ABA61C014B60C8@MAILFS1.uta.edu>

Hi there,
I was wondering if you could help me to write the following procedure. I
need to plot x against y.
The x is read directly from a file. The y is delta from power.t.test:
y <- as.vector(power.t.test(n=4, sd =z, power = 0.8)$delta), where z
values are also read from the file.
Thank you for your help.
Pawel 
 
Pawel Michalak
Assistant Professor
Department of Biology
University of Texas at Arlington



From JAROSLAW.W.TUSZYNSKI at saic.com  Mon Jan  3 20:48:40 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Mon, 3 Jan 2005 14:48:40 -0500 
Subject: [R] Problem with as.array (bug?)
Message-ID: <9CC1B717EF3BD511AD98000103D63FC53FA723@us-arl-asg.mail.saic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050103/1c417f44/attachment.pl

From thchung at tgen.org  Mon Jan  3 22:29:03 2005
From: thchung at tgen.org (Tae-Hoon Chung)
Date: Mon, 03 Jan 2005 14:29:03 -0700
Subject: [R] Memory problem ... Again
Message-ID: <BDFF072F.2264%thchung@tgen.org>

Happy new year to all;

A few days ago, I posted similar problem. At that time, I found out that our
R program had been 32-bit compiled, not 64-bit compiled. So the R program
has been re-installed in 64-bit and run the same job, reading in 150
Affymetrix U133A v2 CEL files and perform dChip processing. However, the
memory problem happened again. Since the amount of physical memory is 64GB,
I think it should not be a problem. Is there anyway we can configure memory
usage so that all physical memory can be utilized?

Our system is like this:
System type: IBM AIX Symmetric Multiprocessing (SMP)
OS version: SuSe 8 SP3a
CPU: 8
Memory: 64GB

The codes are as follows:
> Data <- ReadAffy(filenames = paste(HOME, "CelData/", fname, sep=""))
> eset <- expresso(Data, normalize.method="invariantset", bg.correct=FALSE, pmc\
orrect.method="pmonly", summary.method="liwong")
normalization: invariantset
PM/MM correction : pmonly
expression values: liwong
normalizing...Error: cannot allocate vector of size 594075 Kb
> gc()
           used  (Mb) gc trigger   (Mb)
Ncells   797971  21.4    1710298   45.7
Vcells 76716794 585.4  305954055 2334.3
...
> mem.limits()
nsize vsize
   NA    NA
> object.size(Data)
[1] 608355664
> memory.profile()
    NILSXP     SYMSXP    LISTSXP     CLOSXP     ENVSXP    PROMSXP    LANGSXP
         1      30484     372383       4845        420        180     127274
SPECIALSXP BUILTINSXP    CHARSXP     LGLSXP                           INTSXP
       203       1168     111430       5296          0          0      44650
   REALSXP    CPLXSXP     STRSXP     DOTSXP     ANYSXP     VECSXP    EXPRSXP
     13382          9      60170          0          0      26003          0
  BCODESXP  EXTPTRSXP WEAKREFSXP
         0        106          0



From andy_liaw at merck.com  Mon Jan  3 23:31:09 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 3 Jan 2005 17:31:09 -0500
Subject: [R] Memory problem ... Again
Message-ID: <3A822319EB35174CA3714066D590DCD50994E48D@usrymx25.merck.com>

Have you checked whether there are limits set?  What does `ulimit -a' say?
Do you know how much memory the R process is using when the error occurred?
We've had R jobs using upwards of 13GB on a box with 16GB of RAM (SLES8 on
dual Opterons) and never had problems.

Andy



> From: Tae-Hoon Chung
> 
> Happy new year to all;
> 
> A few days ago, I posted similar problem. At that time, I 
> found out that our
> R program had been 32-bit compiled, not 64-bit compiled. So 
> the R program
> has been re-installed in 64-bit and run the same job, reading in 150
> Affymetrix U133A v2 CEL files and perform dChip processing. 
> However, the
> memory problem happened again. Since the amount of physical 
> memory is 64GB,
> I think it should not be a problem. Is there anyway we can 
> configure memory
> usage so that all physical memory can be utilized?
> 
> Our system is like this:
> System type: IBM AIX Symmetric Multiprocessing (SMP)
> OS version: SuSe 8 SP3a
> CPU: 8
> Memory: 64GB
> 
> The codes are as follows:
> > Data <- ReadAffy(filenames = paste(HOME, "CelData/", fname, sep=""))
> > eset <- expresso(Data, normalize.method="invariantset", 
> bg.correct=FALSE, pmc\
> orrect.method="pmonly", summary.method="liwong")
> normalization: invariantset
> PM/MM correction : pmonly
> expression values: liwong
> normalizing...Error: cannot allocate vector of size 594075 Kb
> > gc()
>            used  (Mb) gc trigger   (Mb)
> Ncells   797971  21.4    1710298   45.7
> Vcells 76716794 585.4  305954055 2334.3
> ...
> > mem.limits()
> nsize vsize
>    NA    NA
> > object.size(Data)
> [1] 608355664
> > memory.profile()
>     NILSXP     SYMSXP    LISTSXP     CLOSXP     ENVSXP    
> PROMSXP    LANGSXP
>          1      30484     372383       4845        420        
> 180     127274
> SPECIALSXP BUILTINSXP    CHARSXP     LGLSXP                   
>         INTSXP
>        203       1168     111430       5296          0        
>   0      44650
>    REALSXP    CPLXSXP     STRSXP     DOTSXP     ANYSXP     
> VECSXP    EXPRSXP
>      13382          9      60170          0          0      
> 26003          0
>   BCODESXP  EXTPTRSXP WEAKREFSXP
>          0        106          0
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From p.dalgaard at biostat.ku.dk  Mon Jan  3 23:39:46 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Jan 2005 23:39:46 +0100
Subject: [R] Memory problem ... Again
In-Reply-To: <BDFF072F.2264%thchung@tgen.org>
References: <BDFF072F.2264%thchung@tgen.org>
Message-ID: <x2y8fauogd.fsf@biostat.ku.dk>

Tae-Hoon Chung <thchung at tgen.org> writes:

> Happy new year to all;
> 
> A few days ago, I posted similar problem. At that time, I found out that our
> R program had been 32-bit compiled, not 64-bit compiled. So the R program
> has been re-installed in 64-bit and run the same job, reading in 150
> Affymetrix U133A v2 CEL files and perform dChip processing. However, the
> memory problem happened again. Since the amount of physical memory is 64GB,
> I think it should not be a problem. Is there anyway we can configure memory
> usage so that all physical memory can be utilized?
> 
> Our system is like this:
> System type: IBM AIX Symmetric Multiprocessing (SMP)
> OS version: SuSe 8 SP3a
> CPU: 8
> Memory: 64GB
.....
> expression values: liwong
> normalizing...Error: cannot allocate vector of size 594075 Kb
> > gc()
>            used  (Mb) gc trigger   (Mb)
> Ncells   797971  21.4    1710298   45.7

As Brian Ripley told you, 64-bit builds of R has 56byte Ncells, so if
yours was one, you should have

> 797971*56/1024/1024
[1] 42.61625

i.e. 42.6Mb used for your Ncells, and it seems that you don't....

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From thchung at tgen.org  Mon Jan  3 23:46:15 2005
From: thchung at tgen.org (Tae-Hoon Chung)
Date: Mon, 03 Jan 2005 15:46:15 -0700
Subject: [R] Memory problem ... Again => False Alarm !!!
In-Reply-To: <x2y8fauogd.fsf@biostat.ku.dk>
Message-ID: <BDFF1947.226C%thchung@tgen.org>

Thanks Peter and Andy;

I just found it was not due to memory problem. It was false alarm ...
64-bit compiled program works fine!


On 1/3/05 3:39 PM, "Peter Dalgaard" <p.dalgaard at biostat.ku.dk> wrote:

> Tae-Hoon Chung <thchung at tgen.org> writes:
> 
>> Happy new year to all;
>> 
>> A few days ago, I posted similar problem. At that time, I found out that our
>> R program had been 32-bit compiled, not 64-bit compiled. So the R program
>> has been re-installed in 64-bit and run the same job, reading in 150
>> Affymetrix U133A v2 CEL files and perform dChip processing. However, the
>> memory problem happened again. Since the amount of physical memory is 64GB,
>> I think it should not be a problem. Is there anyway we can configure memory
>> usage so that all physical memory can be utilized?
>> 
>> Our system is like this:
>> System type: IBM AIX Symmetric Multiprocessing (SMP)
>> OS version: SuSe 8 SP3a
>> CPU: 8
>> Memory: 64GB
> .....
>> expression values: liwong
>> normalizing...Error: cannot allocate vector of size 594075 Kb
>>> gc()
>>            used  (Mb) gc trigger   (Mb)
>> Ncells   797971  21.4    1710298   45.7
> 
> As Brian Ripley told you, 64-bit builds of R has 56byte Ncells, so if
> yours was one, you should have
> 
>> 797971*56/1024/1024
> [1] 42.61625
> 
> i.e. 42.6Mb used for your Ncells, and it seems that you don't....



From afpjt at uaa.alaska.edu  Tue Jan  4 00:38:10 2005
From: afpjt at uaa.alaska.edu (afpjt@uaa.alaska.edu)
Date: Mon, 03 Jan 2005 16:38:10 -0700
Subject: [R] Two brief questions concerning sapply. Can anyone please help?!
Message-ID: <54be3568d9.568d954be3@uaa.alaska.edu>

To anyone who can help:

I have two brief questions concerning sapply.  Following below is the 
code for my example.  The two problems are described at the end of the 
code:


site <- rep(2:6, each = 12) 

tillage <- rep(c(1,-1), each = 6, times = 5) 

carbon <- c(18.23, 16.06, 17.81, 16.07, 17.26, 17.08, 
            14.92, 15.88, 12.11, 14.23, 16.99, 13.57, 
            20.34, 20.3,  18.78, 19.91, 18.43, 20.8,  
            18.98, 19.24, 18.18, 16.69, 19.72, 18.35, 
            36.83, 32.85, 30.21, 31.58, 34.35, 31.4,
            27.72, 29.9,  25.97, 27.63, 28.13, 27.68,
            23.11, 24.32, 21.87, 29.18, 26.52, 23.25,
            24.16, 17.62, 19.62, 20.5,  21.01, 25.62,
            25.73, 23.42, 25.17, 23.76, 23.27, 23.37,
            20.89, 19.93, 19.24, 19.4,  22.09, 18.96)

junk <- interaction(tillage, site)

nr.carbon <- sapply(split(carbon, junk), function(x) mean(x))

> nr.carbon

    -1.2      1.2     -1.3      1.3     -1.4      1.4     -1.5      1.5 
14.61667 17.08500 18.52667 19.76000 27.83833 32.87000 21.42167 24.70833 
    -1.6      1.6 
20.08500 24.12000 

PROBLEM 1 -- How do I sort nr.carbon such that 1.2 comes before -1.2, 
1.3 comes before -1.3, etc.  

PROBLEM 2 -- How do I get rid of the pesky names automatically assigned 
by sapply so that I only have the data in nr.carbon? 

Thanks in advance to those who reply!

--------------------------------------
Philip Turk
Assistant Professor of Applied Statistics
Department of Mathematical Sciences
University of Alaska Anchorage
CAS #154N, 3211 Providence Drive
Anchorage, Alaska  99508
Phone: 907-786-1163  
Fax: 907-786-6162
Email: afpjt at uaa.alaska.edu
Web Page: http://afpjt.uaa.alaska.edu



From andrewr at uidaho.edu  Tue Jan  4 00:58:13 2005
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Tue, 4 Jan 2005 10:58:13 +1100
Subject: [R] Two brief questions concerning sapply. Can anyone please
	help?!
In-Reply-To: <54be3568d9.568d954be3@uaa.alaska.edu>
References: <54be3568d9.568d954be3@uaa.alaska.edu>
Message-ID: <20050103235813.GE595@uidaho.edu>

Philip,

1) right now, tillage is being defined as a numeric vector and then
   being coerced into a factor.  So, try defining tillage in this way:

tillage <- factor(rep(c("1","-1"), each = 6, times = 5), levels=c("1","-1"))

2) Why do you want to do this?  

I hope this helps,

Andrew

On Mon, Jan 03, 2005 at 04:38:10PM -0700, afpjt at uaa.alaska.edu wrote:
> To anyone who can help:
> 
> I have two brief questions concerning sapply.  Following below is the 
> code for my example.  The two problems are described at the end of the 
> code:
> 
> 
> site <- rep(2:6, each = 12) 
> 
> tillage <- rep(c(1,-1), each = 6, times = 5) 
> 
> carbon <- c(18.23, 16.06, 17.81, 16.07, 17.26, 17.08, 
>             14.92, 15.88, 12.11, 14.23, 16.99, 13.57, 
>             20.34, 20.3,  18.78, 19.91, 18.43, 20.8,  
>             18.98, 19.24, 18.18, 16.69, 19.72, 18.35, 
>             36.83, 32.85, 30.21, 31.58, 34.35, 31.4,
>             27.72, 29.9,  25.97, 27.63, 28.13, 27.68,
>             23.11, 24.32, 21.87, 29.18, 26.52, 23.25,
>             24.16, 17.62, 19.62, 20.5,  21.01, 25.62,
>             25.73, 23.42, 25.17, 23.76, 23.27, 23.37,
>             20.89, 19.93, 19.24, 19.4,  22.09, 18.96)
> 
> junk <- interaction(tillage, site)
> 
> nr.carbon <- sapply(split(carbon, junk), function(x) mean(x))
> 
> > nr.carbon
> 
>     -1.2      1.2     -1.3      1.3     -1.4      1.4     -1.5      1.5 
> 14.61667 17.08500 18.52667 19.76000 27.83833 32.87000 21.42167 24.70833 
>     -1.6      1.6 
> 20.08500 24.12000 
> 
> PROBLEM 1 -- How do I sort nr.carbon such that 1.2 comes before -1.2, 
> 1.3 comes before -1.3, etc.  
> 
> PROBLEM 2 -- How do I get rid of the pesky names automatically assigned 
> by sapply so that I only have the data in nr.carbon? 
> 
> Thanks in advance to those who reply!
> 
> --------------------------------------
> Philip Turk
> Assistant Professor of Applied Statistics
> Department of Mathematical Sciences
> University of Alaska Anchorage
> CAS #154N, 3211 Providence Drive
> Anchorage, Alaska  99508
> Phone: 907-786-1163  
> Fax: 907-786-6162
> Email: afpjt at uaa.alaska.edu
> Web Page: http://afpjt.uaa.alaska.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From spencer.graves at pdf.com  Tue Jan  4 01:31:53 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 03 Jan 2005 16:31:53 -0800
Subject: [R] different DF in package nlme and lme4
In-Reply-To: <41D97D5A.10402@stat.wisc.edu>
References: <16857.10746.171989.319959@stat.math.ethz.ch>	<41D96051.4080907@stat.wisc.edu>
	<41D975F6.7060803@vanderbilt.edu> <41D97D5A.10402@stat.wisc.edu>
Message-ID: <41D9E3F9.2020802@pdf.com>

Hi, Christoph: 

      As documented in Pinheiro and Bates (2000) Mixed-Effects Models 
for S and S-Plus (Springer), the nlme package includes a function 
"simulate.lme".  They used that function to study the performance of the 
likelihood ratio statistic under the null hypothesis of no effect.  The 
case they considered has a parameter at a boundary, which violates one 
of the assumptions of the standard 2*log(likelihood ratio) being 
approximately chi-square.  Their results appear in sec. 2.4 of that 
book.  I don't know who were the first to investigate cases like this, 
but Pinheiro and Bates are, as far as I know, among the leaders in this 
area.  I found their discussion understandable, useful, even profound. 

         If your question has not been answered by the discussion so 
far, I suspect that you should be able to conduct an appropriate 
simulation, perhaps using "simulate.lme" or building your own.  From 
Doug's and Frank's comments, it sounds like there are still open 
questions here, and others might be interested in your simulation 
results if you have the need and the time to pursue it. 

      hope this helps.  spencer graves

Douglas Bates wrote:

> Frank E Harrell Jr wrote:
>
>> Douglas Bates wrote:
>>
>>> Christoph Buser wrote:
>>>
>>>> Hi all
>>>>
>>>> I tried to reproduce an example with lme and used the Orthodont
>>>> dataset.
>>>>
>>>> library(nlme)
>>>> fm2a.1 <- lme(distance ~ age + Sex, data = Orthodont, random = ~ 1 
>>>> | Subject)
>>>> anova(fm2a.1)
>>>
>>
>> ...
>>
>>>> Regards,
>>>> Christoph Buser
>>>>
>>>
>>> No.  The calculation of denominator degrees of freedom in lme4 is 
>>> bogus and I believe this is documented.  Note that for all practical 
>>> purposes there is very little difference between 25 and 100 
>>> denominator degrees of freedom.
>>>
>>> lme4 is under development (and has been for a seemingly interminable 
>>> period of time).  Getting the denominator degrees of freedom 
>>> calculation "right" is way down the list of priorities.
>>>
>>> Many people express dismay about the calculation of denominator 
>>> degrees of freedom in all versions of lme4.  IIRC Frank Harrell 
>>> characterizes this as one of the foremost deficiencies in R relative 
>>> to SAS.  I don't agree that this is a glaring deficiency.  In fact I 
>>> believe that there is no "correct" answer.  The F statistics in a 
>>> mixed model do not have an F distribution under the null 
>>> hypothesis.  It's all an approximation, which is why I don't stay up 
>>> nights worrying about the exact details of the approximation.
>>
>>
>>
>> Doug - the main concern is accurate P-values; I don't really care 
>> which approximations are best, just that the ones used are at least 
>> as good as those in SAS.  Without being an expert, I have come to 
>> believe that at the moment SAS is better than R in 2 areas: accurate 
>> P-values from mixed models and handling massive databases.  On the 
>> former point I could easily be swayed by some type I error simulations.
>>
>>>
>>> My plan for lme4 is that one slot in the summary object for an lme 
>>> model will be an incidence table of terms in the fixed effects 
>>> versus grouping factors for the random effects.  This table will 
>>> indicate whether a given term varies within groups defined by the 
>>> grouping factor.  Anyone who wants to implement their personal 
>>> favorite calculation of denominator degrees of freedom based on this 
>>> table will be welcome to do so.
>>
>>
>>
>> I will be interested also to see timings of lme4 (using S4) vs nlme 
>> (using S3) for the same model.
>
>
> Such comparisons would be more heavily influenced by the different 
> algorithms used in the two packages than by S3 versus S4.  The lme4 
> package is not just a translation of the lme part of nlme into S4 
> classes and methods.  It is a complete reimplementation from scratch.
> It indeed faster than the code in nlme but more important is the fact 
> that it will handle models/data sets that simply could not be fit in 
> nlme.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Tue Jan  4 01:42:56 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 3 Jan 2005 19:42:56 -0500
Subject: [R] Two brief questions concerning sapply. Can anyone
	please help?!
Message-ID: <3A822319EB35174CA3714066D590DCD50994E48E@usrymx25.merck.com>

Try something like:

> nr.carbon.2 <- tapply(carbon, list(tillage, site), mean)
> nr.carbon.2
          2        3        4        5      6
-1 14.61667 18.52667 27.83833 21.42167 20.085
1  17.08500 19.76000 32.87000 24.70833 24.120
> as.vector(nr.carbon.2[2:1,])
 [1] 17.08500 14.61667 19.76000 18.52667 32.87000 27.83833 24.70833 21.42167
 [9] 24.12000 20.08500

Andy 

> From: afpjt at uaa.alaska.edu
> 
> To anyone who can help:
> 
> I have two brief questions concerning sapply.  Following below is the 
> code for my example.  The two problems are described at the 
> end of the 
> code:
> 
> 
> site <- rep(2:6, each = 12) 
> 
> tillage <- rep(c(1,-1), each = 6, times = 5) 
> 
> carbon <- c(18.23, 16.06, 17.81, 16.07, 17.26, 17.08, 
>             14.92, 15.88, 12.11, 14.23, 16.99, 13.57, 
>             20.34, 20.3,  18.78, 19.91, 18.43, 20.8,  
>             18.98, 19.24, 18.18, 16.69, 19.72, 18.35, 
>             36.83, 32.85, 30.21, 31.58, 34.35, 31.4,
>             27.72, 29.9,  25.97, 27.63, 28.13, 27.68,
>             23.11, 24.32, 21.87, 29.18, 26.52, 23.25,
>             24.16, 17.62, 19.62, 20.5,  21.01, 25.62,
>             25.73, 23.42, 25.17, 23.76, 23.27, 23.37,
>             20.89, 19.93, 19.24, 19.4,  22.09, 18.96)
> 
> junk <- interaction(tillage, site)
> 
> nr.carbon <- sapply(split(carbon, junk), function(x) mean(x))
> 
> > nr.carbon
> 
>     -1.2      1.2     -1.3      1.3     -1.4      1.4     
> -1.5      1.5 
> 14.61667 17.08500 18.52667 19.76000 27.83833 32.87000 
> 21.42167 24.70833 
>     -1.6      1.6 
> 20.08500 24.12000 
> 
> PROBLEM 1 -- How do I sort nr.carbon such that 1.2 comes before -1.2, 
> 1.3 comes before -1.3, etc.  
> 
> PROBLEM 2 -- How do I get rid of the pesky names 
> automatically assigned 
> by sapply so that I only have the data in nr.carbon? 
> 
> Thanks in advance to those who reply!
> 
> --------------------------------------
> Philip Turk
> Assistant Professor of Applied Statistics
> Department of Mathematical Sciences
> University of Alaska Anchorage
> CAS #154N, 3211 Providence Drive
> Anchorage, Alaska  99508
> Phone: 907-786-1163  
> Fax: 907-786-6162
> Email: afpjt at uaa.alaska.edu
> Web Page: http://afpjt.uaa.alaska.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From w.northcott at unsw.edu.au  Tue Jan  4 03:02:10 2005
From: w.northcott at unsw.edu.au (Bill Northcott)
Date: Tue, 4 Jan 2005 13:02:10 +1100
Subject: [R] ISNAN() broken? in ver 2.x on MacOS X
Message-ID: <A4F69DCB-5DF4-11D9-97FA-000393D3D676@unsw.edu.au>

I have a problem building an extension using ISNAN() on R version 2.0.x.

In R 1.9.1 Arith.h and Rmath.h contained code like

#ifdef IEEE_754
# define ISNAN(x) (isnan(x)!=0)
#else
# define ISNAN(x)      R_IsNaNorNA(x)
#endif
#define R_FINITE(x)    R_finite(x)
int R_IsNaNorNA(double);
int R_finite(double);

which works.

R 2.0.x has
# define ISNAN(x) (isnan(x)!=0)
unconditionally.

This breaks because on MacOS X in /usr/include/architecture/ppc/math.h 
isnan() is itself a macro thus:
#define      isnan( x )         ( ( sizeof ( x ) == sizeof(double) ) ?  
          \
                               __isnand ( x ) :                          
        \
                                 ( sizeof ( x ) == sizeof( float) ) ?    
         \
                               __isnanf ( x ) :                          
        \
                               __isnan  ( x ) )

This macro is not substituted because substitution is not recursive. So 
the build breaks with 'error: `isnan' undeclared'.

How can I fix this?

Bill Northcott



From tlumley at u.washington.edu  Tue Jan  4 03:48:45 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 3 Jan 2005 18:48:45 -0800 (PST)
Subject: [R] ISNAN() broken? in ver 2.x on MacOS X
In-Reply-To: <A4F69DCB-5DF4-11D9-97FA-000393D3D676@unsw.edu.au>
References: <A4F69DCB-5DF4-11D9-97FA-000393D3D676@unsw.edu.au>
Message-ID: <Pine.A41.4.61b.0501031842590.322794@homer06.u.washington.edu>

On Tue, 4 Jan 2005, Bill Northcott wrote:

> I have a problem building an extension using ISNAN() on R version 2.0.x.
>
> In R 1.9.1 Arith.h and Rmath.h contained code like
>
> #ifdef IEEE_754
> # define ISNAN(x) (isnan(x)!=0)
> #else
> # define ISNAN(x)      R_IsNaNorNA(x)
> #endif
> #define R_FINITE(x)    R_finite(x)
> int R_IsNaNorNA(double);
> int R_finite(double);
>
> which works.
>
> R 2.0.x has
> # define ISNAN(x) (isnan(x)!=0)
> unconditionally.
>
> This breaks because on MacOS X in /usr/include/architecture/ppc/math.h 
> isnan() is itself a macro thus:
<snip>
> This macro is not substituted because substitution is not recursive. So the 
> build breaks with 'error: `isnan' undeclared'.
>
> How can I fix this?


Although you have clearly gone to some effort to diagnose this, I think 
your diagnosis is incorrect.

1) In R 1.9.1 IEEE_754 was #defined on OS X, so we would already have had
   #define ISNAN(x) (isnan(x)!=0)

2) The gcc C preprocessor documentation says
   "When the preprocessor expands a macro name, the macro's expansion
   replaces the macro invocation, then the expansion is examined for more
   macros to expand. For example,

      #define TABLESIZE BUFSIZE
      #define BUFSIZE 1024
      TABLESIZE
           ==> BUFSIZE
           ==> 1024

   TABLESIZE is expanded first to produce BUFSIZE, then that macro is
   expanded to produce the final result, 1024."

and while I haven't been able to find anything definitive about the ANSI 
standard, the gcc documentation usually flags extensions fairly well and 
in any case you are presumably using gcc (though you don't say 
explicitly).


A work-around would be to use isnan() rather than ISNAN().


 	-thomas



From ys03165003 at student.ecnu.edu.cn  Tue Jan  4 05:13:34 2005
From: ys03165003 at student.ecnu.edu.cn (ys03165003@student.ecnu.edu.cn)
Date: Tue, 04 Jan 2005 12:13:34 +0800
Subject: [R] (no subject)
Message-ID: <304812014.30430@student.ecnu.edu.cn>

Hi

I'd like to know if the R can do the DCCA? Because I can't find the package about
the DCCA in R. If it can not, please introduce a free software to me, which you
think can easily do the work , thanks!



                                                           jeff



From w.northcott at unsw.edu.au  Tue Jan  4 07:45:59 2005
From: w.northcott at unsw.edu.au (Bill Northcott)
Date: Tue, 4 Jan 2005 17:45:59 +1100
Subject: [R] ISNAN() broken? in ver 2.x on MacOS X
In-Reply-To: <Pine.A41.4.61b.0501031842590.322794@homer06.u.washington.edu>
References: <A4F69DCB-5DF4-11D9-97FA-000393D3D676@unsw.edu.au>
	<Pine.A41.4.61b.0501031842590.322794@homer06.u.washington.edu>
Message-ID: <4B294EEC-5E1C-11D9-97FA-000393D3D676@unsw.edu.au>

I see you are quite correct that IEEE_754 is defined in Rconfig.h on 
MacOS X.  However, I was building against a standalone libRmath v1.9.1. 
  So I was including Rmath.h only and IEEE_754 was not defined.  The 
result was that I got R_IsNaNorNA in the preprocessed source.

I finally found it.  The culprit is
#include <iostream>
which is used in our real code

If I preprocess the following code then the substitution is ISNAN() --> 
(isnan(x)!=0)
#include <iostream>
#include <R.h>
#include <Rmath.h>
ISNAN(x);

Removing the #include <iostream> the substitution becomes
ISNAN(x)   --> (( ( sizeof ( x ) == sizeof(double) ) ? __isnand ( x ) : 
( sizeof ( x ) == sizeof( float) ) ? __isnanf ( x ) : __isnan ( x ) 
)!=0)
which is correct.

This behaviour is the same with both gcc 3.3 and pre-release gcc 4.0 on 
MacOS X 10.3.7.

Is this a bug we should report to someone such the gcc maintainers?

Bill Northcott
On 04/01/2005, at 1:48 PM, Thomas Lumley wrote:
>> In R 1.9.1 Arith.h and Rmath.h contained code like
>>
>> #ifdef IEEE_754
>> # define ISNAN(x) (isnan(x)!=0)
>> #else
>> # define ISNAN(x)      R_IsNaNorNA(x)
>> #endif
>> #define R_FINITE(x)    R_finite(x)
>> int R_IsNaNorNA(double);
> Although you have clearly gone to some effort to diagnose this, I 
> think your diagnosis is incorrect.
>
> 1) In R 1.9.1 IEEE_754 was #defined on OS X, so we would already have 
> had
>   #define ISNAN(x) (isnan(x)!=0)
>
> 2) The gcc C preprocessor documentation says
>   "When the preprocessor expands a macro name, the macro's expansion
>   replaces the macro invocation, then the expansion is examined for 
> more
>   macros to expand. For example,
>
>      #define TABLESIZE BUFSIZE
>      #define BUFSIZE 1024
>      TABLESIZE
>           ==> BUFSIZE
>           ==> 1024
>
>   TABLESIZE is expanded first to produce BUFSIZE, then that macro is
>   expanded to produce the final result, 1024."
>
> and while I haven't been able to find anything definitive about the 
> ANSI standard, the gcc documentation usually flags extensions fairly 
> well and in any case you are presumably using gcc (though you don't 
> say explicitly).
>
>
> A work-around would be to use isnan() rather than ISNAN().
>
>
> 	-thomas
>



From petr.pikal at precheza.cz  Tue Jan  4 08:50:40 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 04 Jan 2005 08:50:40 +0100
Subject: [R] (no subject)
In-Reply-To: <304812014.30430@student.ecnu.edu.cn>
Message-ID: <41DA58E0.683.3D0AF4@localhost>

Hi Jeff

If you tried to search e.g. in Google

detrended canonical correspondence R

would give you some hints

e.g.

decorana {vegan}
                                                     R Documentation
                                                                    
Detrended Correspondence Analysis and 
Basic Reciprocal Averaging

Cheers
Petr


On 4 Jan 2005 at 12:13, ys03165003 at student.ecnu.edu.c wrote:

> Hi
> 
> I'd like to know if the R can do the DCCA? Because I can't find the
> package about the DCCA in R. If it can not, please introduce a free
> software to me, which you think can easily do the work , thanks!
> 
> 
> 
>                                    jeff
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Tue Jan  4 09:09:13 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 4 Jan 2005 08:09:13 +0000 (GMT)
Subject: [R] ISNAN() broken? in ver 2.x on MacOS X
In-Reply-To: <4B294EEC-5E1C-11D9-97FA-000393D3D676@unsw.edu.au>
References: <A4F69DCB-5DF4-11D9-97FA-000393D3D676@unsw.edu.au>
	<Pine.A41.4.61b.0501031842590.322794@homer06.u.washington.edu>
	<4B294EEC-5E1C-11D9-97FA-000393D3D676@unsw.edu.au>
Message-ID: <Pine.LNX.4.61.0501040801141.1630@gannet.stats>

This is a good example of why it is necessary to include all the pertinent 
details (and I know that demands knowing the answer, but using standalone 
Rmath.h is *not* `an extension').  That you were using C++ was also 
pertinent.

I don't think you should be using <R.h> if you are building against
standalone libRmath.

On Tue, 4 Jan 2005, Bill Northcott wrote:

> I see you are quite correct that IEEE_754 is defined in Rconfig.h on MacOS X. 
> However, I was building against a standalone libRmath v1.9.1.  So I was 
> including Rmath.h only and IEEE_754 was not defined.  The result was that I 
> got R_IsNaNorNA in the preprocessed source.
>
> I finally found it.  The culprit is
> #include <iostream>
> which is used in our real code
>
> If I preprocess the following code then the substitution is ISNAN() --> 
> (isnan(x)!=0)
> #include <iostream>
> #include <R.h>
> #include <Rmath.h>
> ISNAN(x);
>
> Removing the #include <iostream> the substitution becomes
> ISNAN(x)   --> (( ( sizeof ( x ) == sizeof(double) ) ? __isnand ( x ) : ( 
> sizeof ( x ) == sizeof( float) ) ? __isnanf ( x ) : __isnan ( x ) )!=0)
> which is correct.
>
> This behaviour is the same with both gcc 3.3 and pre-release gcc 4.0 on MacOS 
> X 10.3.7.
>
> Is this a bug we should report to someone such the gcc maintainers?

You might want to swap the order of inclusion of headers.  Using C headers 
in a C++ program is always slightly dodgy.


> Bill Northcott
> On 04/01/2005, at 1:48 PM, Thomas Lumley wrote:
>>> In R 1.9.1 Arith.h and Rmath.h contained code like
>>> 
>>> #ifdef IEEE_754
>>> # define ISNAN(x) (isnan(x)!=0)
>>> #else
>>> # define ISNAN(x)      R_IsNaNorNA(x)
>>> #endif
>>> #define R_FINITE(x)    R_finite(x)
>>> int R_IsNaNorNA(double);
>> Although you have clearly gone to some effort to diagnose this, I think 
>> your diagnosis is incorrect.
>> 
>> 1) In R 1.9.1 IEEE_754 was #defined on OS X, so we would already have had
>>   #define ISNAN(x) (isnan(x)!=0)
>> 
>> 2) The gcc C preprocessor documentation says
>>   "When the preprocessor expands a macro name, the macro's expansion
>>   replaces the macro invocation, then the expansion is examined for more
>>   macros to expand. For example,
>> 
>>      #define TABLESIZE BUFSIZE
>>      #define BUFSIZE 1024
>>      TABLESIZE
>>           ==> BUFSIZE
>>           ==> 1024
>> 
>>   TABLESIZE is expanded first to produce BUFSIZE, then that macro is
>>   expanded to produce the final result, 1024."
>> 
>> and while I haven't been able to find anything definitive about the ANSI 
>> standard, the gcc documentation usually flags extensions fairly well and in 
>> any case you are presumably using gcc (though you don't say explicitly).
>> 
>> 
>> A work-around would be to use isnan() rather than ISNAN().

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jari.oksanen at oulu.fi  Tue Jan  4 09:26:12 2005
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Tue, 04 Jan 2005 10:26:12 +0200
Subject: [R] (no subject)
In-Reply-To: <304812014.30430@student.ecnu.edu.cn>
References: <304812014.30430@student.ecnu.edu.cn>
Message-ID: <4AC30C36-5E2A-11D9-9F47-000A95C76CA8@oulu.fi>

On 4 Jan 2005, at 6:13, ys03165003 at student.ecnu.edu.cn wrote:

> Hi
>
> I'd like to know if the R can do the DCCA? Because I can't find the 
> package about
> the DCCA in R. If it can not, please introduce a free software to me, 
> which you
> think can easily do the work , thanks!
>
As far as I know, there is no function in R to run detrended 
constrained correspondence analysis (if 'DCCA' is an acronym for that 
method). I don't know of any free software to run DCCA. However, you 
may check Legendre & Casgraine pages at 
http://www.bio.umontreal.ca/legendre/index.html to prove me wrong.

On the other hand, I find it extremely hard to find a reason to find a 
reason to use detrended CCA. It seems to destroy everything...

cheers, jari oksanen
--
Jari Oksanen, Oulu, Finland



From pzanis at geol.uoa.gr  Tue Jan  4 10:09:22 2005
From: pzanis at geol.uoa.gr (Prodromos Zanis)
Date: Tue,  4 Jan 2005 11:09:22 +0200
Subject: [R] about kriging
Message-ID: <1104829762.41da5d42a4832@webmail.uoa.gr>


Dear R-project users 

I want to use kriging method to interpolate irregularly distributed temperature 
data (station data) with latitude, longtitude and altitude in order to produce 
a gridded dataset altidude corrected.
Do you have any idea which package to use? 

I look forward for your hints.

With best regards
Prodromos Zanis



-- 
****************************************************
Dr. Prodromos Zanis
Centre for Atmospheric Physics and Climatology 
Academy of Athens
3rd September, Athens 15784, Greece
Tel. +30 210 8832048
Fax: +30 210 8832048
e-mail: pzanis at geol.uoa.gr
Web address: http://users.auth.gr/~zanis/



From ernesto at ipimar.pt  Tue Jan  4 10:58:05 2005
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Tue, 04 Jan 2005 09:58:05 +0000
Subject: [R] about kriging
In-Reply-To: <1104829762.41da5d42a4832@webmail.uoa.gr>
References: <1104829762.41da5d42a4832@webmail.uoa.gr>
Message-ID: <1104832685.3627.1.camel@mordor.ipimar.pt>

Hi,

Take a look at

http://agec144.agecon.uiuc.edu/csiss/Rgeo/

Regards

EJ

On Tue, 2005-01-04 at 09:09, Prodromos Zanis wrote:
> Dear R-project users 
> 
> I want to use kriging method to interpolate irregularly distributed temperature 
> data (station data) with latitude, longtitude and altitude in order to produce 
> a gridded dataset altidude corrected.
> Do you have any idea which package to use? 
> 
> I look forward for your hints.
> 
> With best regards
> Prodromos Zanis
> 
>



From dmb at mrc-dunn.cam.ac.uk  Tue Jan  4 11:29:44 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 4 Jan 2005 10:29:44 +0000 (GMT)
Subject: [R] Question about creating error bars
In-Reply-To: <Pine.LNX.4.21.0412280126020.10273-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.LNX.4.21.0501041012190.17324-100000@mail.mrc-dunn.cam.ac.uk>

On Tue, 28 Dec 2004, Dan Bolser wrote:

>
>On Thu, 23 Dec 2004, Dan Bolser wrote:
>
>>
>>I have data that looks (very roughly) like this...
>>
>>Declarative: 
>>Several 'groups', each group with a very variable number of
>>data points associated.
>>
>>
>>Procedural:
>>v.1 <- c(rep(50,1), rep(5,5), rep(2,10))  # Set up 
>>v.2 <- c('a','b','c','d','e','f','g','h', # the
>>         'i','j','k','l','m','n','o','p') # groups
>>v.3 <- rep(v.2,v.1)                       # here.
>>v.4 <- rnorm(length(v.3))                 # Simulate data.
>>v.5 <- tapply(v.4,v.3,mean)               # My analysis.
>>
>>plot(v.5)
>>
>>As the number of data points in a group gets smaller, so the variance of
>>the mean value for that group goes up. I would like to bootstrap some
>>error bars to show roughly how variable the value for each group is. Here
>>we have a normal distribution, but mostly my data is binary (i.e. each
>>group has a different number of (nearly) binary observations). The groups
>>are ordered, and I want to see any trend in my data accross the groups.
>>
>>Dan.
>
>Nothing I can do to help pal, sorry.
>

No, me either. Reading the ?boot documentation (in the library(boot)) is
less than useful too. The docs appear to be written for an expert in the
field (and R) rather than someone who would like to pick up the function
and run with it.

Its funny because on contrast to the complexity of the (tecnical)
spefication of the required functions is the conceptual simplicity of what
I want to do ...

Sample from some bins and create an error bar (of some kind) from the
samples.

It seems that without knowing how to do this (or exactly where to
look) their is nothing you can do.

>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From michael.watson at bbsrc.ac.uk  Tue Jan  4 12:31:27 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Tue, 4 Jan 2005 11:31:27 -0000
Subject: [R] Difference between "R CMD build --binary" and "R CMD INSTALL
	--build"
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E89A76@iahce2knas1.iah.bbsrc.reserved>

As the title suggests, when building R packages on Windows, what is the
difference between:

R CMD build --binary mypack

And 

R CMD INSTALL --build mypack

??  The former is suggested by my previous notes and seems to work, and
the latter is suggested by
http://www.biostat.jhsph.edu/~kbroman/Rintro/Rwinpack.html, and also
seems to work.

Thanks in advance

Mick



From michael.watson at bbsrc.ac.uk  Tue Jan  4 12:33:25 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Tue, 4 Jan 2005 11:33:25 -0000
Subject: [R] Where to put example files?
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E89A77@iahce2knas1.iah.bbsrc.reserved>

I'm writing an R package which includes some text file parsing
functions, and I want to include with my package some example files to
be used with those functions.  My question is which directory should I
put these example files in for the user to access?  I have tried demo,
example, doc and sample but none seem to work or "fit".

Thanks in advance
Mick



From w.northcott at unsw.edu.au  Tue Jan  4 12:40:23 2005
From: w.northcott at unsw.edu.au (Bill Northcott)
Date: Tue, 4 Jan 2005 22:40:23 +1100
Subject: [R] ISNAN() broken? in ver 2.x on MacOS X
In-Reply-To: <Pine.LNX.4.61.0501040801141.1630@gannet.stats>
References: <A4F69DCB-5DF4-11D9-97FA-000393D3D676@unsw.edu.au>
	<Pine.A41.4.61b.0501031842590.322794@homer06.u.washington.edu>
	<4B294EEC-5E1C-11D9-97FA-000393D3D676@unsw.edu.au>
	<Pine.LNX.4.61.0501040801141.1630@gannet.stats>
Message-ID: <6B9D52AE-5E45-11D9-97FA-000393D3D676@unsw.edu.au>

I was attempting builds using both the standalone library including 
only Rmath.h and the using the installed R using R.h as well.  
Basically I have the following in relevant programs:
#ifndef MATHLIB_STANDALONE
#include <R.h>
#endif /* MATHLIB_STANDALONE */

As far as I can see, the only difference is the version of R.  ISNAN() 
will always fail on MacOS X with R 2.0.x whereas it might work with a 
standalone v1.9.x libRmath.

I tried reordering the headers but without any success.

The minimal test case to illustrate the problem is:
#include <iostream>
#include <math.h>
isnan(x)

Preprocess this on MacOS X and the isnan(x) will be unchanged which is 
wrong as isnan(x) is a macro in math.h.  Remove the iostream include 
and it will work properly.

In short there is a problem with the IEEE macros and C++ on MacOS X.

Bill Northcott
On 04/01/2005, at 7:09 PM, Prof Brian Ripley wrote:
> This is a good example of why it is necessary to include all the 
> pertinent details (and I know that demands knowing the answer, but 
> using standalone Rmath.h is *not* `an extension').  That you were 
> using C++ was also pertinent.
>
> I don't think you should be using <R.h> if you are building against
> standalone libRmath.
>
>>
>> I finally found it.  The culprit is
>> #include <iostream>
>> which is used in our real code
>>
>> If I preprocess the following code then the substitution is ISNAN() 
>> --> (isnan(x)!=0)
>> #include <iostream>
>> #include <R.h>
>> #include <Rmath.h>
>> ISNAN(x);
>>
>> Removing the #include <iostream> the substitution becomes
>> ISNAN(x)   --> (( ( sizeof ( x ) == sizeof(double) ) ? __isnand ( x ) 
>> : ( sizeof ( x ) == sizeof( float) ) ? __isnanf ( x ) : __isnan ( x ) 
>> )!=0)
>> which is correct.
>>
>> This behaviour is the same with both gcc 3.3 and pre-release gcc 4.0 
>> on MacOS X 10.3.7.
>>
>> Is this a bug we should report to someone such the gcc maintainers?
>
> You might want to swap the order of inclusion of headers.  Using C 
> headers in a C++ program is always slightly dodgy.
>
>
>> Bill Northcott
>> On 04/01/2005, at 1:48 PM, Thomas Lumley wrote:
>>>> In R 1.9.1 Arith.h and Rmath.h contained code like
>>>> #ifdef IEEE_754
>>>> # define ISNAN(x) (isnan(x)!=0)
>>>> #else
>>>> # define ISNAN(x)      R_IsNaNorNA(x)
>>>> #endif
>>>> #define R_FINITE(x)    R_finite(x)
>>>> int R_IsNaNorNA(double);
>>> Although you have clearly gone to some effort to diagnose this, I 
>>> think your diagnosis is incorrect.
>>> 1) In R 1.9.1 IEEE_754 was #defined on OS X, so we would already 
>>> have had
>>>   #define ISNAN(x) (isnan(x)!=0)
>>> 2) The gcc C preprocessor documentation says
>>>   "When the preprocessor expands a macro name, the macro's expansion
>>>   replaces the macro invocation, then the expansion is examined for 
>>> more
>>>   macros to expand. For example,
>>>      #define TABLESIZE BUFSIZE
>>>      #define BUFSIZE 1024
>>>      TABLESIZE
>>>           ==> BUFSIZE
>>>           ==> 1024
>>>   TABLESIZE is expanded first to produce BUFSIZE, then that macro is
>>>   expanded to produce the final result, 1024."
>>> and while I haven't been able to find anything definitive about the 
>>> ANSI standard, the gcc documentation usually flags extensions fairly 
>>> well and in any case you are presumably using gcc (though you don't 
>>> say explicitly).
>>> A work-around would be to use isnan() rather than ISNAN().
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From w.northcott at unsw.edu.au  Tue Jan  4 12:42:20 2005
From: w.northcott at unsw.edu.au (Bill Northcott)
Date: Tue, 4 Jan 2005 22:42:20 +1100
Subject: [R] ISNAN() broken? in ver 2.x on MacOS X
In-Reply-To: <Pine.LNX.4.61.0501040801141.1630@gannet.stats>
References: <A4F69DCB-5DF4-11D9-97FA-000393D3D676@unsw.edu.au>
	<Pine.A41.4.61b.0501031842590.322794@homer06.u.washington.edu>
	<4B294EEC-5E1C-11D9-97FA-000393D3D676@unsw.edu.au>
	<Pine.LNX.4.61.0501040801141.1630@gannet.stats>
Message-ID: <B16B6AC2-5E45-11D9-97FA-000393D3D676@unsw.edu.au>

This sort of confirms that it is a bug.

> From: Andrew Pinski <pinskia at physics.uc.edu>
> Date: 4 January 2005 7:39:38 PM
> To: Bill Northcott <w.northcott at unsw.edu.au>
> Cc: gcc at gcc.gnu.org
> Subject: Re: C++ header file problem - is this a bug?
>
>
> On Jan 4, 2005, at 2:13 AM, Bill Northcott wrote:
>
>> Is this a bug or is it expected behaviour and if so why?
>
> Yes this is a bug but it is already filed see PR 14608.
>
> Thanks,
> Andrew Pinski



From murdoch at stats.uwo.ca  Tue Jan  4 12:53:32 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 04 Jan 2005 06:53:32 -0500
Subject: [R] Difference between "R CMD build --binary" and "R CMD INSTALL
	--build"
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E89A76@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E95E89A76@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <an0lt0tm8qea74873khu9b3rhdjhke2nl7@4ax.com>

On Tue, 4 Jan 2005 11:31:27 -0000, "michael watson \(IAH-C\)"
<michael.watson at bbsrc.ac.uk> wrote:

>As the title suggests, when building R packages on Windows, what is the
>difference between:
>
>R CMD build --binary mypack
>
>And 
>
>R CMD INSTALL --build mypack
>
>??  The former is suggested by my previous notes and seems to work, and
>the latter is suggested by
>http://www.biostat.jhsph.edu/~kbroman/Rintro/Rwinpack.html, and also
>seems to work.

The former builds the package in a temporary directory.

The latter installs the package in the standard library location, then
builds from the installed copy.  This does a better job because it
allows help links to other packages.  

Duncan



From Roger.Bivand at nhh.no  Tue Jan  4 13:09:23 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 4 Jan 2005 13:09:23 +0100 (CET)
Subject: [R] Where to put example files?
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E89A77@iahce2knas1.iah.bbsrc.r
	eserved>
Message-ID: <Pine.LNX.4.44.0501041305390.1229-100000@reclus.nhh.no>

On Tue, 4 Jan 2005, michael watson (IAH-C) wrote:

> I'm writing an R package which includes some text file parsing
> functions, and I want to include with my package some example files to
> be used with those functions.  My question is which directory should I
> put these example files in for the user to access?  I have tried demo,
> example, doc and sample but none seem to work or "fit".
> 

Try using a subdirectory to inst/ in the package source. For example, 
inst/shapes/sids.shp in package "maptools" can be accessed by:

system.file("shapes/sids.shp", package="maptools")[1]

See examples in the pixmap package.

Roger


> Thanks in advance
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From dmb at mrc-dunn.cam.ac.uk  Tue Jan  4 14:41:40 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 4 Jan 2005 13:41:40 +0000 (GMT)
Subject: [R] Adding values to the end of a vector?
Message-ID: <Pine.LNX.4.21.0501041336200.20656-100000@mail.mrc-dunn.cam.ac.uk>


I want to add values onto the end of a vector, for example...

x <- vector

for (i in 1:5){
  add_to_end_of_vector(i,x)
}

I just cant find the answer to this question!


Sorry for such a basic question, I tried...

x <- c()

for (i in 1:5) x[length(x)] <- i

but it didn't work.



From andy_liaw at merck.com  Tue Jan  4 14:51:09 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 4 Jan 2005 08:51:09 -0500
Subject: [R] Adding values to the end of a vector?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E496@usrymx25.merck.com>

Is this what you're looking for?

> x <- numeric(0)
> for (i in 1:5) x <- append(x, i)
> x
[1] 1 2 3 4 5

Andy


> From: Dan Bolser
> 
> I want to add values onto the end of a vector, for example...
> 
> x <- vector
> 
> for (i in 1:5){
>   add_to_end_of_vector(i,x)
> }
> 
> I just cant find the answer to this question!
> 
> 
> Sorry for such a basic question, I tried...
> 
> x <- c()
> 
> for (i in 1:5) x[length(x)] <- i
> 
> but it didn't work.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From i.visser at uva.nl  Tue Jan  4 14:57:14 2005
From: i.visser at uva.nl (Ingmar Visser)
Date: Tue, 04 Jan 2005 14:57:14 +0100
Subject: [R] Adding values to the end of a vector?
In-Reply-To: <Pine.LNX.4.21.0501041336200.20656-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <BE005F4A.7F1%i.visser@uva.nl>

The following works, but there may be more efficient ways to do this ...

> x=numeric(0)
> for(i in 1:5) { 
+ x[length(x)+1]=i
+ }
> x
[1] 1 2 3 4 5
> 
Best, ingmar


On 1/4/05 2:41 PM, "Dan Bolser" <dmb at mrc-dunn.cam.ac.uk> wrote:

> 
> I want to add values onto the end of a vector, for example...
> 
> x <- vector
> 
> for (i in 1:5){
>   add_to_end_of_vector(i,x)
> }
> 
> I just cant find the answer to this question!
> 
> 
> Sorry for such a basic question, I tried...
> 
> x <- c()
> 
> for (i in 1:5) x[length(x)] <- i
> 
> but it didn't work.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Roger.Bivand at nhh.no  Tue Jan  4 15:02:25 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 4 Jan 2005 15:02:25 +0100 (CET)
Subject: [R] Adding values to the end of a vector?
In-Reply-To: <Pine.LNX.4.21.0501041336200.20656-100000@mail.mrc-dunn.cam.ac.
	uk>
Message-ID: <Pine.LNX.4.44.0501041501310.1229-100000@reclus.nhh.no>

On Tue, 4 Jan 2005, Dan Bolser wrote:

> 
> I want to add values onto the end of a vector, for example...
> 
> x <- vector
> 
> for (i in 1:5){
>   add_to_end_of_vector(i,x)
> }
> 

?append


> I just cant find the answer to this question!
> 
> 
> Sorry for such a basic question, I tried...
> 
> x <- c()
> 
> for (i in 1:5) x[length(x)] <- i
> 
> but it didn't work.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From jfox at mcmaster.ca  Tue Jan  4 15:03:04 2005
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 4 Jan 2005 09:03:04 -0500
Subject: [R] Adding values to the end of a vector?
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E496@usrymx25.merck.com>
Message-ID: <20050104140302.KCJR2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Dan,

The following also works:

> x <- numeric(0)
> for (i in 1:5) x[i] <- i
> x
[1] 1 2 3 4 5

It's worth noting, however, that extending a vector in this manner can be
very inefficient for large vectors, since the vector is recopied each time.
If you can anticipate the number of elements (or place an upper bound on
it), then it's better to do something like

> x <- numeric(5)
> for (i in 1:5) x[i] <- i

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> Sent: Tuesday, January 04, 2005 8:51 AM
> To: 'Dan Bolser'; R mailing list
> Subject: RE: [R] Adding values to the end of a vector?
> 
> Is this what you're looking for?
> 
> > x <- numeric(0)
> > for (i in 1:5) x <- append(x, i)
> > x
> [1] 1 2 3 4 5
> 
> Andy
> 
> 
> > From: Dan Bolser
> > 
> > I want to add values onto the end of a vector, for example...
> > 
> > x <- vector
> > 
> > for (i in 1:5){
> >   add_to_end_of_vector(i,x)
> > }
> > 
> > I just cant find the answer to this question!
> > 
> > 
> > Sorry for such a basic question, I tried...
> > 
> > x <- c()
> > 
> > for (i in 1:5) x[length(x)] <- i
> > 
> > but it didn't work.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Tristan.Lefebure at univ-lyon1.fr  Tue Jan  4 15:13:01 2005
From: Tristan.Lefebure at univ-lyon1.fr (Lefebure Tristan)
Date: Tue, 4 Jan 2005 15:13:01 +0100
Subject: [R] Adding values to the end of a vector?
In-Reply-To: <BE005F4A.7F1%i.visser@uva.nl>
References: <BE005F4A.7F1%i.visser@uva.nl>
Message-ID: <200501041513.01992.lefebure@univ-lyon1.fr>

why not :

x<-vector()
for (i in 1:5) { x <-c(x,i) }

x
[1] 1 2 3 4 5

for (i in 1:5) { x <-c(x,i) }

x
[1] 1 2 3 4 5 1 2 3 4 5



On Tuesday 04 January 2005 14:57, Ingmar Visser wrote:
> > x=numeric(0)
> > for(i in 1:5) {
>
> + x[length(x)+1]=i
> + }
>
> > x

-- 
------------------------------------------------------------
Tristan LEFEBURE
Laboratoire d'?cologie des hydrosyst?mes fluviaux (UMR 5023)
Universit? Lyon I - Campus de la Doua
Bat. Darwin C 69622 Villeurbanne - France

Phone: (33) (0)4 26 23 44 02
Fax: (33) (0)4 72 43 15 23



From p.dalgaard at biostat.ku.dk  Tue Jan  4 16:00:08 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Jan 2005 16:00:08 +0100
Subject: [R] Adding values to the end of a vector?
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E496@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E496@usrymx25.merck.com>
Message-ID: <x2pt0lz1c7.fsf@biostat.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> Is this what you're looking for?
> 
> > x <- numeric(0)
> > for (i in 1:5) x <- append(x, i)
> > x
> [1] 1 2 3 4 5
> 
> Andy

Also notice that in R many things are vectorized, so you may prefer

> append(x,1:5)
[1] 1 2 3 4 5

Extending a vector is done by allocating a longer vector and copying
the original. You really don't want to do that for every element if
you at all can avoid it. So vectorize, or at least preallocate the
extra storage, if you can.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dalmiral at umich.edu  Tue Jan  4 16:24:49 2005
From: dalmiral at umich.edu (Daniel Almirall)
Date: Tue, 4 Jan 2005 10:24:49 -0500 (EST)
Subject: [R] Adding values to the end of a vector?
In-Reply-To: <20050104140302.KCJR2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>
References: <20050104140302.KCJR2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <Pine.SOL.4.58.0501041022100.3023@timepilot.gpcc.itd.umich.edu>


I am curious.  How are these suggestions different (better, worse?) from

 x <- NULL
 for (i in 1:5) x <- c(x, i)


Thanks,
Danny



On Tue, 4 Jan 2005, John Fox wrote:

> Dear Dan,
>
> The following also works:
>
> > x <- numeric(0)
> > for (i in 1:5) x[i] <- i
> > x
> [1] 1 2 3 4 5
>
> It's worth noting, however, that extending a vector in this manner can be
> very inefficient for large vectors, since the vector is recopied each time.
> If you can anticipate the number of elements (or place an upper bound on
> it), then it's better to do something like
>
> > x <- numeric(5)
> > for (i in 1:5) x[i] <- i
>
> I hope this helps,
>  John
>
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox
> --------------------------------
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> > Sent: Tuesday, January 04, 2005 8:51 AM
> > To: 'Dan Bolser'; R mailing list
> > Subject: RE: [R] Adding values to the end of a vector?
> >
> > Is this what you're looking for?
> >
> > > x <- numeric(0)
> > > for (i in 1:5) x <- append(x, i)
> > > x
> > [1] 1 2 3 4 5
> >
> > Andy
> >
> >
> > > From: Dan Bolser
> > >
> > > I want to add values onto the end of a vector, for example...
> > >
> > > x <- vector
> > >
> > > for (i in 1:5){
> > >   add_to_end_of_vector(i,x)
> > > }
> > >
> > > I just cant find the answer to this question!
> > >
> > >
> > > Sorry for such a basic question, I tried...
> > >
> > > x <- c()
> > >
> > > for (i in 1:5) x[length(x)] <- i
> > >
> > > but it didn't work.
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>



From roger.bos at gmail.com  Tue Jan  4 16:39:37 2005
From: roger.bos at gmail.com (roger bos)
Date: Tue, 4 Jan 2005 10:39:37 -0500
Subject: [R] How to verify using more than 2GB of memory
Message-ID: <1db7268005010407397c1e8813@mail.gmail.com>

My machine has 4Gb of memory under WinXP Pro.  Re-reading R for
Windows FAQ 2.7 I discovered that I can use editbin to add the
"/LARGEADDRESSAWARE" tag in the header without re-compiling, so that
is what I did.  I also check that it worked using dumpbin.  I then
edited my Boot.ini file to add the /3GB switch.  Then I went to my
R.0.0.1 patched that I modified as above and checked memory limit:

> memory.limit(NA)
[1] 1073741824

This is usual default.

> memory.limit(3*1024)
NULL
> memory.limit(4*1024)
Error in memory.size(size) : cannot decrease memory limit

It seems to work, since I am able to allocate 3Gb of memory, but not
4Gb  (though the error message I get isn't very helpful since I am
clearly not trying to decrease the memory limit).

However, this does not seem a good way to check that my
LARGEADDRESSAWARE version of R is working properly, because when I do
the same memory.limit operations on 2.0.1.0 dev that I downloaded this
morning I get the same results, without having made the program
LARGEADDRESSAWARE

Also, although trying to allocate 4Gb of memory does produce an error,
I can allocate 3.9Gb, which seems hard to beleive because the OS is
supposed to reserve 1Gb.

> memory.limit(3.9*1024)
NULL
> memory.limit(NA)
[1] 4187593113
> 

So, finally, here are my questions, in order of interest:
1)  Once I have made my version of R LARGEADDRESSAWARE, how do I best
verify that all is well?
2)  Why was I able to increase the memory.limit to 3.9Gb on the new
version of R.0.1.0 that I did not make LARGEADDRESSAWARE?
3)  Is there any side-effect to making a program LARGEADDRESSAWARE?  I
don't want to sound lazy, but for the benefit of those who don't have
Visual Studio, why not make the distributed binary LARGEADDRESSAWARE?

Thanks in advance for any insight.

Roger J. Bos



From dmb at mrc-dunn.cam.ac.uk  Tue Jan  4 17:20:19 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 4 Jan 2005 16:20:19 +0000 (GMT)
Subject: [R] boot and variances of the bootstrap replicates of the variable
 of interest?
Message-ID: <Pine.LNX.4.21.0501041603040.23140-100000@mail.mrc-dunn.cam.ac.uk>


I want to use boot.ci to generate confidence intervals over the
bootstrapped mean(s) of a group of observations (i.e. I have 10
observations and I want to know how confident I can be on the value for
the mean).

I don't know (or want to know) the details of bootstrapping - I just have
the simplistic idea of taking samples, measuring a statistic on the
sample, and getting some confidence in the result based on the variance of
the sample statistics.

I am not sure which "type of interval [is] required", so I wanted to try
"studentized intervals", because some of my groups have a small number of
samples (like one or two). However, "variance estimates are required for
studentized intervals".

I can generate the variance for the statistic like this..


# Shame the 'mean' function dosn't work with boot
mean.x <- function(xx,ii) mean(xx[ii])

# Don't know how (or why) to pick a value for this.
nboot <- 1000 

# The 10 or so observations (in this case)
boot.sub <- boot(data=shabby.sub$a, statistic=mean.x, R=nboot)

# Variance of the statistic over the 1000 replicates
my.var.t <- var(boot.sub$t)

# t: A matrix with 'R' rows each of which is a bootstrap replicate
#    of 'statistic'. 

However, I need the variance for each R statistic... I think...

It is a shame that boot and boot.ci cant work more together better,
without me (a patent idiot) having to pass values (correctly) from the
results of boot into boot.ci.


Any help with the above would be appreciated, as I am about to weep
horrible tears (once more) at the alter of R.

(not really :)

Dan.



From ligges at statistik.uni-dortmund.de  Tue Jan  4 17:27:06 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 04 Jan 2005 17:27:06 +0100
Subject: [R] How to verify using more than 2GB of memory
In-Reply-To: <1db7268005010407397c1e8813@mail.gmail.com>
References: <1db7268005010407397c1e8813@mail.gmail.com>
Message-ID: <41DAC3DA.2040900@statistik.uni-dortmund.de>

roger bos wrote:

> My machine has 4Gb of memory under WinXP Pro.  Re-reading R for
> Windows FAQ 2.7 I discovered that I can use editbin to add the
> "/LARGEADDRESSAWARE" tag in the header without re-compiling, so that
> is what I did.  I also check that it worked using dumpbin.  I then
> edited my Boot.ini file to add the /3GB switch.  Then I went to my
> R.0.0.1 patched that I modified as above and checked memory limit:
> 
> 
>>memory.limit(NA)
> 
> [1] 1073741824
> 
> This is usual default.
> 
> 
>>memory.limit(3*1024)
> 
> NULL
> 
>>memory.limit(4*1024)


memory.limit(4*1024-1)

should work (you get an overflow with 4*1024).



> Error in memory.size(size) : cannot decrease memory limit
> 
> It seems to work, since I am able to allocate 3Gb of memory, but not
> 4Gb  (though the error message I get isn't very helpful since I am
> clearly not trying to decrease the memory limit).
> 
> However, this does not seem a good way to check that my
> LARGEADDRESSAWARE version of R is working properly, because when I do
> the same memory.limit operations on 2.0.1.0 dev that I downloaded this
> morning I get the same results, without having made the program
> LARGEADDRESSAWARE
> 
> Also, although trying to allocate 4Gb of memory does produce an error,
> I can allocate 3.9Gb, which seems hard to beleive because the OS is
> supposed to reserve 1Gb.

Yes, but R does have the permission now to request 3.9 Gb, even if it 
does not get it from the OS.


> 
>>memory.limit(3.9*1024)
> 
> NULL
> 
>>memory.limit(NA)
> 
> [1] 4187593113
> 
> 
> So, finally, here are my questions, in order of interest:
> 1)  Once I have made my version of R LARGEADDRESSAWARE, how do I best
> verify that all is well?


If you don't trust the FAQ, you can try it out by assigning some big 
objects and look what happens..... if it crashes, you will see...


> 2)  Why was I able to increase the memory.limit to 3.9Gb on the new
> version of R.0.1.0 that I did not make LARGEADDRESSAWARE?

I think you won't get that many memory from the OS, but R is now allowed 
to ask the OS in order to get it.


> 3)  Is there any side-effect to making a program LARGEADDRESSAWARE?  I
> don't want to sound lazy, but for the benefit of those who don't have
> Visual Studio, why not make the distributed binary LARGEADDRESSAWARE?

I guess because of license issues. Is it permitted to distribute such a 
software (modified by Visual Studio) given both the Visual Studio 
license and the GPL from R???

Uwe Ligges


> Thanks in advance for any insight.
> 
> Roger J. Bos
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Tue Jan  4 18:16:50 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 4 Jan 2005 09:16:50 -0800 (PST)
Subject: [R] Adding values to the end of a vector?
In-Reply-To: <Pine.SOL.4.58.0501041022100.3023@timepilot.gpcc.itd.umich.edu>
References: <20050104140302.KCJR2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>
	<Pine.SOL.4.58.0501041022100.3023@timepilot.gpcc.itd.umich.edu>
Message-ID: <Pine.A41.4.61b.0501040908410.182274@homer03.u.washington.edu>

On Tue, 4 Jan 2005, Daniel Almirall wrote:

>
> I am curious.  How are these suggestions different (better, worse?) from
>
> x <- NULL
> for (i in 1:5) x <- c(x, i)

One imporant difference is between solutions that preallocate storage and 
those that don't.
   x<-numeric(5)
   for(i in 1:5) x[i]<-i
allocates one vector of length 5 and then modifies it, but
   x<-NULL
   for(i in 1:5) x<-c(x,i)
allocates vectors of length 1, 2, 3, 4, 5 in turn.

You can't tell this from anything in the language definition, since 
conceptually x[i]<-i also copies: it does
    x <- "[<-"(x,i,i)
and for more complicated replacement functions it will really copy.  Even 
if the first version really copied there would be some potential for 
having more efficient memory allocation with all the objects being of size 
5 (at least, for very large values of 5).

If you don't know how long the vector needs to be then you can't 
preallocate, but a common programming strategy in other languages is to 
allocate powers of 2 (eg start out with x<-numeric(4) and if that isn't 
big enough do something like x<-c(x,numeric(4)) to double the size). I 
don't know if anyone has looked at whether this is ever useful in R.

 	-thomas

>
> Thanks,
> Danny
>
>
>
> On Tue, 4 Jan 2005, John Fox wrote:
>
>> Dear Dan,
>>
>> The following also works:
>>
>>> x <- numeric(0)
>>> for (i in 1:5) x[i] <- i
>>> x
>> [1] 1 2 3 4 5
>>
>> It's worth noting, however, that extending a vector in this manner can be
>> very inefficient for large vectors, since the vector is recopied each time.
>> If you can anticipate the number of elements (or place an upper bound on
>> it), then it's better to do something like
>>
>>> x <- numeric(5)
>>> for (i in 1:5) x[i] <- i
>>
>> I hope this helps,
>>  John
>>
>> --------------------------------
>> John Fox
>> Department of Sociology
>> McMaster University
>> Hamilton, Ontario
>> Canada L8S 4M4
>> 905-525-9140x23604
>> http://socserv.mcmaster.ca/jfox
>> --------------------------------
>>
>>> -----Original Message-----
>>> From: r-help-bounces at stat.math.ethz.ch
>>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
>>> Sent: Tuesday, January 04, 2005 8:51 AM
>>> To: 'Dan Bolser'; R mailing list
>>> Subject: RE: [R] Adding values to the end of a vector?
>>>
>>> Is this what you're looking for?
>>>
>>>> x <- numeric(0)
>>>> for (i in 1:5) x <- append(x, i)
>>>> x
>>> [1] 1 2 3 4 5
>>>
>>> Andy
>>>
>>>
>>>> From: Dan Bolser
>>>>
>>>> I want to add values onto the end of a vector, for example...
>>>>
>>>> x <- vector
>>>>
>>>> for (i in 1:5){
>>>>   add_to_end_of_vector(i,x)
>>>> }
>>>>
>>>> I just cant find the answer to this question!
>>>>
>>>>
>>>> Sorry for such a basic question, I tried...
>>>>
>>>> x <- c()
>>>>
>>>> for (i in 1:5) x[length(x)] <- i
>>>>
>>>> but it didn't work.
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide!
>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From dmb at mrc-dunn.cam.ac.uk  Tue Jan  4 18:29:02 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 4 Jan 2005 17:29:02 +0000 (GMT)
Subject: [R] R help search and Java(tm)?
Message-ID: <Pine.LNX.4.21.0501041717430.23843-100000@mail.mrc-dunn.cam.ac.uk>


I found this great search 

source("http://www.statslab.cam.ac.uk/~djw1005/Stats/Interests/search.R")

helpHTML()

Has (or will) this become the standard search method?

Is R 'Free Software'? The dependence on Java seems a bit of a pain for
'freeness'. 

Did the above make it into CRAN?

Cheers,
Dan.



From hastie at stanford.edu  Tue Jan  4 18:39:38 2005
From: hastie at stanford.edu (Trevor Hastie)
Date: Tue, 4 Jan 2005 09:39:38 -0800
Subject: [R] Statistical Learning and Data Mining Course
Message-ID: <9B8E9708-5E77-11D9-B6DF-000A95AFA6FC@stanford.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050104/7ad73186/attachment.pl

From dm60062003 at yahoo.com  Tue Jan  4 19:25:36 2005
From: dm60062003 at yahoo.com (Derek Margetts)
Date: Tue, 4 Jan 2005 10:25:36 -0800 (PST)
Subject: [R] rownames and plot lablels
Message-ID: <20050104182536.46705.qmail@web53209.mail.yahoo.com>


I have a question about changing rownames.  In the
following function I am plotting the regression
coeficients with their corresponding mean.  Right now,
the labels on the plot and the rownames in the
dataframe are x1,x2,x3...etc.  Is there a way to make
the row names the same as the variable name entered
into the formula?  I.E. if x1 was a vector called
opinions, it would be labeled opinions on the plot
instead of x1.  I have tried using variations of
rownames(x)<- namevector but have not been successful.
I keep getting a null.  Any suggestions would be
appreciated

Quadplot<-function(y, x1, x2, x3 = NULL, x4  =
NULL,x5=NULL,x6=NULL,x7=NULL,x8=NULL,x9=NULL,x10=NULL,
x11=NULL, x12=NULL, x13=NULL, x14=NULL)

#section of code ommitted

my.formula <- as.formula("y ~ x1 + x2 + x3+ x4+ x5+
x6+ x7+ x8+ x9+x10+x11+x12+x13+x14")
outlm <- lm(my.formula)

#section of code ommitted

meanvec<-c(mean(x1,na.rm=TRUE),mean(x2,na.rm=TRUE),mean(x3,na.rm=TRUE),mean(x4,na.rm=TRUE),mean(x5,na.rm=TRUE),mean(x6,na.rm=TRUE),mean(x7,na.rm=TRUE),mean(x8,na.rm=TRUE),mean(x9,na.rm=TRUE),mean(x10,na.rm=TRUE),mean(x11,na.rm=TRUE)
,mean(x12,na.rm=TRUE) ,mean(x13,na.rm=TRUE)
,mean(x14,na.rm=TRUE)) }
meanper<-meanvec/6
meanper1<-meanper*100

#This is the data frame created
mf <- data.frame(Impact = abs(outlm$coef[-1]),
Performance = meanper1)

meanx<-mean(abs(outlm$coef[-1]))
meany<-mean(meanper1)
n<-length(meanvec)
color<-rep("blue",n)
color[(mf$Impact>meanx &
mf$Performance>meany)]<-"green"
color[(mf$Impact>meanx & mf$Performance<meany)]<-"red"
color[(mf$Impact<meanx &
mf$Performance>meany)]<-"gray"

plot((mf),col=color,main="Perfomance vs.
Impact",xlab="Impact",ylab="Performance", type = "n")
points(mf, pch=21, bg=color, cex=4)
text(x = mf$Impact, y = mf$Performance, labels =
rownames(mf))
abline(h=mean(mf$Performance))
abline(v=mean(mf$Impact))
print(mf)
}
			
Thanks
Derek



From p.dalgaard at biostat.ku.dk  Tue Jan  4 19:25:33 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Jan 2005 19:25:33 +0100
Subject: [R] R help search and Java(tm)?
In-Reply-To: <Pine.LNX.4.21.0501041717430.23843-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0501041717430.23843-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <x28y79yrtu.fsf@biostat.ku.dk>

Dan Bolser <dmb at mrc-dunn.cam.ac.uk> writes:

> I found this great search 
> 
> source("http://www.statslab.cam.ac.uk/~djw1005/Stats/Interests/search.R")
> 
> helpHTML()

> Has (or will) this become the standard search method?

I think it got superseded by Jon Baron's RSiteSearch() function which
is finding its way into r-devel.
 
> Is R 'Free Software'? The dependence on Java seems a bit of a pain for
> 'freeness'. 

A pain yes, but free software can rely on non-free compilers and OS's
if need be. It's just more convenient to build on tools that everyone
can get hold of. 

> Did the above make it into CRAN?

Well, you could check... It doesn't look like there is enough of the
ancillary matter (help pages etc.) for a CRAN package.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Tue Jan  4 19:46:09 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 4 Jan 2005 18:46:09 +0000 (GMT)
Subject: [R] How to verify using more than 2GB of memory
In-Reply-To: <1db7268005010407397c1e8813@mail.gmail.com>
References: <1db7268005010407397c1e8813@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0501041836200.17862@gannet.stats>

You forgot that 32-bit machines do integer operation mod 2^32.
So you actually asked for a memory limit of 0, which was a decrease.

We do recommend that you use the command-line flag, e.g. in the rw-FAQ.

% rterm --max-mem-size=4096M
WARNING: --max-mem-size=4096`M': too large and ignored

would have told you the error.

You don't seem to appreciate the difference between `limit' and 
`allocate': please read the help page for memory.limit with a modicum of 
care.  Once you appreciate that you have seriously misread the page, 
things will become a lot clearer.

On Tue, 4 Jan 2005, roger bos wrote:

> My machine has 4Gb of memory under WinXP Pro.  Re-reading R for
> Windows FAQ 2.7 I discovered that I can use editbin to add the
> "/LARGEADDRESSAWARE" tag in the header without re-compiling, so that
> is what I did.  I also check that it worked using dumpbin.  I then
> edited my Boot.ini file to add the /3GB switch.  Then I went to my
> R.0.0.1 patched that I modified as above and checked memory limit:
>
>> memory.limit(NA)
> [1] 1073741824
>
> This is usual default.
>
>> memory.limit(3*1024)
> NULL
>> memory.limit(4*1024)
> Error in memory.size(size) : cannot decrease memory limit
>
> It seems to work, since I am able to allocate 3Gb of memory, but not
> 4Gb  (though the error message I get isn't very helpful since I am
> clearly not trying to decrease the memory limit).
>
> However, this does not seem a good way to check that my
> LARGEADDRESSAWARE version of R is working properly, because when I do
> the same memory.limit operations on 2.0.1.0 dev that I downloaded this
> morning I get the same results, without having made the program
> LARGEADDRESSAWARE
>
> Also, although trying to allocate 4Gb of memory does produce an error,
> I can allocate 3.9Gb, which seems hard to beleive because the OS is
> supposed to reserve 1Gb.
>
>> memory.limit(3.9*1024)
> NULL
>> memory.limit(NA)
> [1] 4187593113
>>
>
> So, finally, here are my questions, in order of interest:
> 1)  Once I have made my version of R LARGEADDRESSAWARE, how do I best
> verify that all is well?
> 2)  Why was I able to increase the memory.limit to 3.9Gb on the new
> version of R.0.1.0 that I did not make LARGEADDRESSAWARE?
> 3)  Is there any side-effect to making a program LARGEADDRESSAWARE?  I
> don't want to sound lazy, but for the benefit of those who don't have
> Visual Studio, why not make the distributed binary LARGEADDRESSAWARE?

The maintainer does not have it.  Simple!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Tue Jan  4 19:35:26 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 4 Jan 2005 18:35:26 +0000 (UTC)
Subject: [R] Adding values to the end of a vector?
References: <20050104140302.KCJR2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>
	<Pine.SOL.4.58.0501041022100.3023@timepilot.gpcc.itd.umich.edu>
	<Pine.A41.4.61b.0501040908410.182274@homer03.u.washington.edu>
Message-ID: <loom.20050104T192332-83@post.gmane.org>


I thought it would be interesting to actually time this.

1. In the first example we keep appending
the square of an N(0.1) r.v. to v until the total is 20000 or more.

2. In the second example we double v's length each time it needs
to be extended.  Even with only 20,000 elements the doubling
strategy is an order of magnitude faster.  

3. In the third example,
we preallocate 30,000 just to be sure we have enough space.  This 
is only slightly faster than #2 and has the downside that we had to 
decide how much to preallocate in advance.

R> # successively append 
R> set.seed(1); s <- 0; v <- numeric()
R> system.time({
+ while(s<20000) {
+ r <- rnorm(1)
+ v <- append(v, r)
+ s <- s+r*r
+ }
+ }, gc = TRUE)
[1] 10.35  0.03 11.25    NA    NA

R> # double length whenever end is reached
R> set.seed(1); s <- 0; v <- numeric(); vlen <- 0
R> system.time({
+ while(s<20000) {
+ if (vlen == length(v)) length(v) <- 2*length(v)
+ vlen <- vlen+1
+ v[vlen] <- r <- rnorm(1)
+ s <- s+r*r
+ }
+ }, gc = TRUE)
[1] 1.12 0.00 1.21   NA   NA

R> # preallocate some space
R> set.seed(1); s <- 0; v <- numeric(30000); vlen <- 0
R> system.time({
+ while(s<20000) {
+ vlen <- vlen+1
+ v[vlen] <- r <- rnorm(1)
+ s <- s+r*r
+ }
+ }, gc = TRUE)
[1] 0.99 0.00 1.10   NA   NA


Thomas Lumley <tlumley <at> u.washington.edu> writes:

: 
: On Tue, 4 Jan 2005, Daniel Almirall wrote:
: 
: >
: > I am curious.  How are these suggestions different (better, worse?) from
: >
: > x <- NULL
: > for (i in 1:5) x <- c(x, i)
: 
: One imporant difference is between solutions that preallocate storage and 
: those that don't.
:    x<-numeric(5)
:    for(i in 1:5) x[i]<-i
: allocates one vector of length 5 and then modifies it, but
:    x<-NULL
:    for(i in 1:5) x<-c(x,i)
: allocates vectors of length 1, 2, 3, 4, 5 in turn.
: 
: You can't tell this from anything in the language definition, since 
: conceptually x[i]<-i also copies: it does
:     x <- "[<-"(x,i,i)
: and for more complicated replacement functions it will really copy.  Even 
: if the first version really copied there would be some potential for 
: having more efficient memory allocation with all the objects being of size 
: 5 (at least, for very large values of 5).
: 
: If you don't know how long the vector needs to be then you can't 
: preallocate, but a common programming strategy in other languages is to 
: allocate powers of 2 (eg start out with x<-numeric(4) and if that isn't 
: big enough do something like x<-c(x,numeric(4)) to double the size). I 
: don't know if anyone has looked at whether this is ever useful in R.
: 
:  	-thomas
: 
: >
: > Thanks,
: > Danny
: >
: >
: >
: > On Tue, 4 Jan 2005, John Fox wrote:
: >
: >> Dear Dan,
: >>
: >> The following also works:
: >>
: >>> x <- numeric(0)
: >>> for (i in 1:5) x[i] <- i
: >>> x
: >> [1] 1 2 3 4 5
: >>
: >> It's worth noting, however, that extending a vector in this manner can be
: >> very inefficient for large vectors, since the vector is recopied each 
time.
: >> If you can anticipate the number of elements (or place an upper bound on
: >> it), then it's better to do something like
: >>
: >>> x <- numeric(5)
: >>> for (i in 1:5) x[i] <- i
: >>
: >> I hope this helps,
: >>  John
: >>
: >> --------------------------------
: >> John Fox
: >> Department of Sociology
: >> McMaster University
: >> Hamilton, Ontario
: >> Canada L8S 4M4
: >> 905-525-9140x23604
: >> http://socserv.mcmaster.ca/jfox
: >> --------------------------------
: >>
: >>> -----Original Message-----
: >>> From: r-help-bounces <at> stat.math.ethz.ch
: >>> [mailto:r-help-bounces <at> stat.math.ethz.ch] On Behalf Of Liaw, Andy
: >>> Sent: Tuesday, January 04, 2005 8:51 AM
: >>> To: 'Dan Bolser'; R mailing list
: >>> Subject: RE: [R] Adding values to the end of a vector?
: >>>
: >>> Is this what you're looking for?
: >>>
: >>>> x <- numeric(0)
: >>>> for (i in 1:5) x <- append(x, i)
: >>>> x
: >>> [1] 1 2 3 4 5
: >>>
: >>> Andy
: >>>
: >>>
: >>>> From: Dan Bolser
: >>>>
: >>>> I want to add values onto the end of a vector, for example...
: >>>>
: >>>> x <- vector
: >>>>
: >>>> for (i in 1:5){
: >>>>   add_to_end_of_vector(i,x)
: >>>> }
: >>>>
: >>>> I just cant find the answer to this question!
: >>>>
: >>>>
: >>>> Sorry for such a basic question, I tried...
: >>>>
: >>>> x <- c()
: >>>>
: >>>> for (i in 1:5) x[length(x)] <- i
: >>>>
: >>>> but it didn't work.



From peter.schlattmann at t-online.de  Tue Jan  4 22:41:28 2005
From: peter.schlattmann at t-online.de (peter.schlattmann@t-online.de)
Date: Tue,  4 Jan 2005 22:41:28 +0100
Subject: [R] x11 is not available
Message-ID: <1ClwQi-2IYCvo0@cmpmail09.bbul.t-online.de>

Dear list,

I have problems installing R-2.0.1 on SUSE Linux 9.2. I used the
following commands in order install R in 
/usr/local 

./configure --with x11 --with-readline
make
make   install

When starting R and trying to display a plot on the X-window system
output is written to a postscript file. When I try to run x11 with 

>x11()

Error in X11(): X11 is not available.


I do have a running x11 window system with KDE


Any help is appreciated.

Many thanks

Peter Schlattmann



From choudary.jagar at swosu.edu  Tue Jan  4 22:41:49 2005
From: choudary.jagar at swosu.edu (Jagarlamudi, Choudary)
Date: Tue, 4 Jan 2005 15:41:49 -0600
Subject: [R] Non linear Square fit Function
Message-ID: <E03EBB50FF2C024781A6E4460AD58F0607C12E@swosu-mbx01.admin.swosu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050104/3b8a39ea/attachment.pl

From anne.piotet at urbanet.ch  Tue Jan  4 22:45:27 2005
From: anne.piotet at urbanet.ch (Anne)
Date: Tue, 4 Jan 2005 22:45:27 +0100
Subject: [R] scree plot
Message-ID: <000d01c4f2a6$b5b382f0$6c00a8c0@mtd4>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050104/fcaca7d1/attachment.pl

From dmb at mrc-dunn.cam.ac.uk  Tue Jan  4 22:49:17 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 4 Jan 2005 21:49:17 +0000 (GMT)
Subject: [R] bootstrap proportions?
Message-ID: <Pine.LNX.4.21.0501042141540.25982-100000@mail.mrc-dunn.cam.ac.uk>


Hello, 

I have several ordered groups (20), each with several observations. Each
group has fewer observations roughly linearly from 30 to 0. Each
observation is a proportion. As I know the max and min values for a
proporion are 1 and 0, I am adding these values to each group to allow
bootstrap for groups with <= 1 observations. Sometimes the bootstrap
confidence interval is > 1, which makes me think that I am doing something
deeply wrong.

How can I do bootstrap properly with observations which are proportions?

What special precautions should I be taking for very low sample sizes with
bootstrap?

Is this a bad question for R-help?

Thanks,
Dan.



From sdavis2 at mail.nih.gov  Tue Jan  4 23:00:46 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 4 Jan 2005 17:00:46 -0500
Subject: [R] x11 is not available
In-Reply-To: <1ClwQi-2IYCvo0@cmpmail09.bbul.t-online.de>
References: <1ClwQi-2IYCvo0@cmpmail09.bbul.t-online.de>
Message-ID: <166D4A6A-5E9C-11D9-B000-000D933565E8@mail.nih.gov>

If I remember correctly, the correct command line switch is:

-with-x11

Note the dash.

Sean

On Jan 4, 2005, at 4:41 PM, peter.schlattmann at t-online.de wrote:

> Dear list,
>
> I have problems installing R-2.0.1 on SUSE Linux 9.2. I used the
> following commands in order install R in
> /usr/local
>
> ./configure --with x11 --with-readline
> make
> make   install
>
> When starting R and trying to display a plot on the X-window system
> output is written to a postscript file. When I try to run x11 with
>
>> x11()
>
> Error in X11(): X11 is not available.
>
>
> I do have a running x11 window system with KDE
>
>
> Any help is appreciated.
>
> Many thanks
>
> Peter Schlattmann
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From johannesson1 at llnl.gov  Tue Jan  4 23:08:57 2005
From: johannesson1 at llnl.gov (Gardar Johannesson)
Date: Tue, 04 Jan 2005 14:08:57 -0800
Subject: [R] x11 is not available
In-Reply-To: <1ClwQi-2IYCvo0@cmpmail09.bbul.t-online.de>
References: <1ClwQi-2IYCvo0@cmpmail09.bbul.t-online.de>
Message-ID: <1104876537.23483.70.camel@johanssen1rh>

My guess is that you have a 'Desktop' version of SUSE installed which
does not include the needed X11 development environment; from the "R
Installation and Administration" manual:

"Unless you do not want to view graphs on-screen you need X11 installed,
including its headers and client libraries. (On Fedora Core 2 Linux this
means the xorg-x11-devel and xorg-x11-libs RPMs, for example. Older
Linuxen used XFree86-.)"


Good luck,
Gardar

On Tue, 2005-01-04 at 13:41, peter.schlattmann at t-online.de wrote:
> Dear list,
> 
> I have problems installing R-2.0.1 on SUSE Linux 9.2. I used the
> following commands in order install R in 
> /usr/local 
> 
> ./configure --with x11 --with-readline
> make
> make   install
> 
> When starting R and trying to display a plot on the X-window system
> output is written to a postscript file. When I try to run x11 with 
> 
> >x11()
> 
> Error in X11(): X11 is not available.
> 
> 
> I do have a running x11 window system with KDE
> 
> 
> Any help is appreciated.
> 
> Many thanks
> 
> Peter Schlattmann
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Gardar Johannesson                          johannesson1 at llnl.gov
 Applied Statistics & Economics Group       Tel: 925 422-3901
 Systems & Decision Sciences Section        Fax: 925 422-4141
Lawrence Livermore National Laboratory      Loc: B141/R1128
7000 East Ave., L-229
Livermore, CA 94550-9234



From dmb at mrc-dunn.cam.ac.uk  Tue Jan  4 23:11:31 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 4 Jan 2005 22:11:31 +0000 (GMT)
Subject: [R] print(cat(bleh)) shows up strange NULL
Message-ID: <Pine.LNX.4.21.0501042207550.25982-100000@mail.mrc-dunn.cam.ac.uk>


I am trying the following code...

for(i in 1:bins){
  print(cat(c(i, length(var1[var1==i]))))
}

Where var1 is a vector with zero or more values = {1, 2, ..., bins} (or
something like that).

The result I get is...

1 33NULL
2 28NULL
3 39NULL
4 27NULL
5 32NULL
6 30NULL
7 23NULL
8 16NULL
9 10NULL
10 15NULL
11 13NULL
12 7NULL
13 3NULL
14 7NULL
15 2NULL
16 0NULL
17 2NULL
18 0NULL
19 0NULL
20 3NULL

I don't know what I am doing to get those stray NULL characters printed.

I also tried...

for(i in 1:bins){
  print(cat(i, length(var1[var1==i])))
}

Which is essentially the same.



From tom_hoary at web.de  Tue Jan  4 23:34:36 2005
From: tom_hoary at web.de (thomas)
Date: Tue, 04 Jan 2005 23:34:36 +0100
Subject: [R] warnings and errors with R CMD INSTALL
Message-ID: <1104878076.7506.28.camel@localhost.localdomain>

Hello,

unfortunately I had to compile latest version of R-2.0.1 by myself. I'd
rather would prefer vendors binaries but since the current version of
Ubuntu defaults to 1.9.x I had to compile R on my own in order to be up
to date!
So far, everything went fine and R runs smoothly. Unfortunately I am not
able to use R CMD INSTALL command to install add-on packages.

I followed RNews 3/3 and used:

1. R CMD INSTALL package_version.tar.gz 

and

2.options(CRAN="http://umfragen.sowi.uni-mainz.de/CRAN/")
install.packages("pkg1". "pkg2")

Both approaches end up i.e.:

WARNING: invalid package 'pkg1.tar.gz'
ERROR: no packages specified

Well, R administration manual and RNews give some examples which I took
as basis to install further packages.
I, as suggested in RNews and administration manual alike, omitted the -l
option, since I want to stay with default library location!

Obviously I am stuck in a situation not covered by the docs! Well, most
likely I misunderstand something, can anyone help out with this?

greetings

Thomas



system:


platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    2                
minor    0.1              
year     2004             
month    11               
day      15               
language R



From ssim at lic.co.nz  Tue Jan  4 23:36:07 2005
From: ssim at lic.co.nz (ssim@lic.co.nz)
Date: Wed, 5 Jan 2005 11:36:07 +1300
Subject: [R] Re : Frequency count 
Message-ID: <OFE95702A3.F18A7336-ONCC256F7F.007B4A3B-CC256F7F.007C2835@livestock.org.nz>

Dear list,

I have a dataset as follow and I would like to count the frequencies for
the combination of the two variables (f1 and f2)  for each id. I know it is
should be straight forward, but I just don't know how to do it in R. Here
is the SAS code I will use to get the output I want :

proc means nway;
class id f1 f2;
var flag
output out=temp;


Dataset:
id    f1    f2    flag
798   1     2     1
777   0     2     1
798   2     2     1
777   0     2     1
777   1     1     1

Output:
Id=798
1-2   1
2-2   1

Id=777
0-2   2
1-1   1
___________________________________________________________________________
This message, including attachments, is confidential. If you...{{dropped}}



From p.dalgaard at biostat.ku.dk  Tue Jan  4 23:40:20 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Jan 2005 23:40:20 +0100
Subject: [R] print(cat(bleh)) shows up strange NULL
In-Reply-To: <Pine.LNX.4.21.0501042207550.25982-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0501042207550.25982-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <x2vfacyg17.fsf@biostat.ku.dk>

Dan Bolser <dmb at mrc-dunn.cam.ac.uk> writes:

> I am trying the following code...
> 
> for(i in 1:bins){
>   print(cat(c(i, length(var1[var1==i]))))
> }
> 
> Where var1 is a vector with zero or more values = {1, 2, ..., bins} (or
> something like that).
> 
> The result I get is...
> 
> 1 33NULL
> 2 28NULL
............
> 19 0NULL
> 20 3NULL
> 
> I don't know what I am doing to get those stray NULL characters printed.

You're printing them, will you believe it...?
 
Consider this:

> x <- cat("abc")
abc> print(x)
NULL
> x
NULL
> print(cat("abc"))
abcNULL

I.e. cat() always returns NULL, invisibly unless you force it to be
printed. 


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dmb at mrc-dunn.cam.ac.uk  Tue Jan  4 23:44:30 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 4 Jan 2005 22:44:30 +0000 (GMT)
Subject: [R] print(cat(bleh)) shows up strange NULL
In-Reply-To: <x2vfacyg17.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.21.0501042244180.25982-100000@mail.mrc-dunn.cam.ac.uk>

On 4 Jan 2005, Peter Dalgaard wrote:

>Dan Bolser <dmb at mrc-dunn.cam.ac.uk> writes:
>
>> I am trying the following code...
>> 
>> for(i in 1:bins){
>>   print(cat(c(i, length(var1[var1==i]))))
>> }
>> 
>> Where var1 is a vector with zero or more values = {1, 2, ..., bins} (or
>> something like that).
>> 
>> The result I get is...
>> 
>> 1 33NULL
>> 2 28NULL
>............
>> 19 0NULL
>> 20 3NULL
>> 
>> I don't know what I am doing to get those stray NULL characters printed.
>
>You're printing them, will you believe it...?
> 
>Consider this:
>
>> x <- cat("abc")
>abc> print(x)
>NULL
>> x
>NULL
>> print(cat("abc"))
>abcNULL
>
>I.e. cat() always returns NULL, invisibly unless you force it to be
>printed. 

never trust a cat!

>
>
>



From Dax42 at web.de  Tue Jan  4 23:45:49 2005
From: Dax42 at web.de (dax42)
Date: Tue, 4 Jan 2005 23:45:49 +0100
Subject: [R] quantiles for geometric distribution
Message-ID: <618B9869-5EA2-11D9-B442-000393883D7E@web.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050104/bab33792/attachment.pl

From spencer.graves at pdf.com  Tue Jan  4 23:48:14 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 04 Jan 2005 14:48:14 -0800
Subject: [R] print(cat(bleh)) shows up strange NULL
In-Reply-To: <Pine.LNX.4.21.0501042207550.25982-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0501042207550.25982-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <41DB1D2E.3090002@pdf.com>

      Consider the following: 

 > for(i in 1:2)cat(i, "")
1 2 >
 > for(i in 1:2)print(i)
[1] 1
[1] 2
 > for(i in 1:2)print(cat(i))
1NULL
2NULL
   
      hope this helps.  spencer graves

Dan Bolser wrote:

>I am trying the following code...
>
>for(i in 1:bins){
>  print(cat(c(i, length(var1[var1==i]))))
>}
>
>Where var1 is a vector with zero or more values = {1, 2, ..., bins} (or
>something like that).
>
>The result I get is...
>
>1 33NULL
>2 28NULL
>3 39NULL
>4 27NULL
>5 32NULL
>6 30NULL
>7 23NULL
>8 16NULL
>9 10NULL
>10 15NULL
>11 13NULL
>12 7NULL
>13 3NULL
>14 7NULL
>15 2NULL
>16 0NULL
>17 2NULL
>18 0NULL
>19 0NULL
>20 3NULL
>
>I don't know what I am doing to get those stray NULL characters printed.
>
>I also tried...
>
>for(i in 1:bins){
>  print(cat(i, length(var1[var1==i])))
>}
>
>Which is essentially the same.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From dmb at mrc-dunn.cam.ac.uk  Tue Jan  4 23:50:53 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 4 Jan 2005 22:50:53 +0000 (GMT)
Subject: [R] print(cat(bleh)) shows up strange NULL
In-Reply-To: <Pine.LNX.4.21.0501042244180.25982-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.LNX.4.21.0501042246130.25982-100000@mail.mrc-dunn.cam.ac.uk>

On Tue, 4 Jan 2005, Dan Bolser wrote:

>On 4 Jan 2005, Peter Dalgaard wrote:
>
>>Dan Bolser <dmb at mrc-dunn.cam.ac.uk> writes:
>>
>>> I am trying the following code...
>>> 
>>> for(i in 1:bins){
>>>   print(cat(c(i, length(var1[var1==i]))))
>>> }
>>> 
>>> Where var1 is a vector with zero or more values = {1, 2, ..., bins} (or
>>> something like that).
>>> 
>>> The result I get is...
>>> 
>>> 1 33NULL
>>> 2 28NULL
>>............
>>> 19 0NULL
>>> 20 3NULL
>>> 
>>> I don't know what I am doing to get those stray NULL characters printed.
>>
>>You're printing them, will you believe it...?
>> 
>>Consider this:
>>
>>> x <- cat("abc")
>>abc> print(x)
>>NULL
>>> x
>>NULL
>>> print(cat("abc"))
>>abcNULL
>>
>>I.e. cat() always returns NULL, invisibly unless you force it to be
>>printed. 
>
>never trust a cat!

I meant to add that this was causing me problems when trying to create a
file name for use with the png command.

png tries to create a 'NULL' file name, and returns a rather cryptic
message...

Error in X11(paste("png::", filename, sep = ""), width, height, pointsize,: 
	unable to start device PNG
In addition: Warning message: 
could not open PNG file `' 
> 

However, the warning provides the answer to the problem... use paste
instead of cat.

Cheers,
Dan.


>
>>
>>
>>
>
>



From andrewr at uidaho.edu  Tue Jan  4 23:59:46 2005
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Wed, 5 Jan 2005 09:59:46 +1100
Subject: [R] Re : Frequency count
In-Reply-To: <OFE95702A3.F18A7336-ONCC256F7F.007B4A3B-CC256F7F.007C2835@livestock.org.nz>
References: <OFE95702A3.F18A7336-ONCC256F7F.007B4A3B-CC256F7F.007C2835@livestock.org.nz>
Message-ID: <20050104225946.GO591@uidaho.edu>

Try table(), aggregate(), or tapply(), using length() in the last two
functions.

I hope that this helps,

Andrew

On Wed, Jan 05, 2005 at 11:36:07AM +1300, ssim at lic.co.nz wrote:
> Dear list,
> 
> I have a dataset as follow and I would like to count the frequencies for
> the combination of the two variables (f1 and f2)  for each id. I know it is
> should be straight forward, but I just don't know how to do it in R. Here
> is the SAS code I will use to get the output I want :
> 
> proc means nway;
> class id f1 f2;
> var flag
> output out=temp;
> 
> 
> Dataset:
> id    f1    f2    flag
> 798   1     2     1
> 777   0     2     1
> 798   2     2     1
> 777   0     2     1
> 777   1     1     1
> 
> Output:
> Id=798
> 1-2   1
> 2-2   1
> 
> Id=777
> 0-2   2
> 1-1   1
> ___________________________________________________________________________
> This message, including attachments, is confidential. If you...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From tarmo.remmel at utoronto.ca  Wed Jan  5 00:19:26 2005
From: tarmo.remmel at utoronto.ca (Tarmo Remmel)
Date: Tue, 4 Jan 2005 18:19:26 -0500
Subject: [R] Re : Frequency count 
In-Reply-To: <OFE95702A3.F18A7336-ONCC256F7F.007B4A3B-CC256F7F.007C2835@livestock.org.nz>
Message-ID: <BEEKLAMKBJMOPPLNMKNLOEFPCJAA.tarmo.remmel@utoronto.ca>

This might help...

Here KI is a matrix such as your dataset (without the flag)

"dups" <- function (intab=KI) {

  # CREATE A MATRIX WITH ONLY THE UNIQUE ROWS OF intab
  intabunique <- unique(intab)

  # COLLAPSE THE FULL TABLE
  a2 <- apply(intab, 1, paste, collapse=":")

  # COLLAPSE THE UNIQUE TABLE
  b2 <- apply(intabunique, 1, paste, collapse=":")

  # COUNT UNIQUE ROW OCCURRENCES AND WRITE TO .COUNT
  assign(".COUNT",c(table(c(a2,unique(b2)))[b2] - 1) ,pos=1)

  # WRITE THE UNIQUE TABLE TO .KIUNIQUE
  assign(".KIUNIQUE", intabunique, pos=1)
}


> KI
     col1 col2 col3
[1,]    1    1    1
[2,]    2    1    1
[3,]    1    2    1
[4,]    1    1    1
[5,]    1    2    1
[6,]    1    1    1
[7,]    2    2    2

> dups(KI)

> .KIUNIQUE
     col1 col2 col3
[1,]    1    1    1
[2,]    2    1    1
[3,]    1    2    1
[4,]    2    2    2

> .COUNT
1:1:1 2:1:1 1:2:1 2:2:2
    3     1     2     1


______________________________________________
Tarmo Remmel  B.E.S., M.Sc.F., Ph.D. Candidate
GUESS Lab, Department of Geography
University of Toronto, Toronto, ON, Canada
http://eratos.erin.utoronto.ca/remmelt




> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of ssim at lic.co.nz
> Sent: Tuesday, January 04, 2005 5:36 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Re : Frequency count
>
>
> Dear list,
>
> I have a dataset as follow and I would like to count the frequencies for
> the combination of the two variables (f1 and f2)  for each id. I
> know it is
> should be straight forward, but I just don't know how to do it in R. Here
> is the SAS code I will use to get the output I want :
>
> proc means nway;
> class id f1 f2;
> var flag
> output out=temp;
>
>
> Dataset:
> id    f1    f2    flag
> 798   1     2     1
> 777   0     2     1
> 798   2     2     1
> 777   0     2     1
> 777   1     1     1
>
> Output:
> Id=798
> 1-2   1
> 2-2   1
>
> Id=777
> 0-2   2
> 1-1   1
> __________________________________________________________________
> _________
> This message, including attachments, is confidential. If you...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Wed Jan  5 00:17:44 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Jan 2005 00:17:44 +0100
Subject: [R] Re : Frequency count
In-Reply-To: <OFE95702A3.F18A7336-ONCC256F7F.007B4A3B-CC256F7F.007C2835@livestock.org.nz>
References: <OFE95702A3.F18A7336-ONCC256F7F.007B4A3B-CC256F7F.007C2835@livestock.org.nz>
Message-ID: <x27jmsn5rb.fsf@biostat.ku.dk>

ssim at lic.co.nz writes:

> Dear list,
> 
> I have a dataset as follow and I would like to count the frequencies for
> the combination of the two variables (f1 and f2)  for each id. I know it is
> should be straight forward, but I just don't know how to do it in R. Here
> is the SAS code I will use to get the output I want :
> 
> proc means nway;
> class id f1 f2;
> var flag
> output out=temp;
> 
> 
> Dataset:
> id    f1    f2    flag
> 798   1     2     1
> 777   0     2     1
> 798   2     2     1
> 777   0     2     1
> 777   1     1     1
> 
> Output:
> Id=798
> 1-2   1
> 2-2   1
> 
> Id=777
> 0-2   2
> 1-1   1

Here's one way:

> dd <- read.table(stdin(),header=TRUE)
0: id    f1    f2    flag
1: 798   1     2     1
2: 777   0     2     1
3: 798   2     2     1
4: 777   0     2     1
5: 777   1     1     1
6:
> by(dd, dd$id, function(x)table(paste(x$f1,x$f2,sep="-")))
 dd$id: 777

0-2 1-1
  2   1
------------------------------------------------------------
dd$id: 798

1-2 2-2
  1   1


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tlumley at u.washington.edu  Wed Jan  5 00:23:51 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 4 Jan 2005 15:23:51 -0800 (PST)
Subject: [R] Re : Frequency count 
In-Reply-To: <OFE95702A3.F18A7336-ONCC256F7F.007B4A3B-CC256F7F.007C2835@livestock.org.nz>
References: <OFE95702A3.F18A7336-ONCC256F7F.007B4A3B-CC256F7F.007C2835@livestock.org.nz>
Message-ID: <Pine.A41.4.61b.0501041522520.182274@homer03.u.washington.edu>

On Wed, 5 Jan 2005 ssim at lic.co.nz wrote:

> Dear list,
>
> I have a dataset as follow and I would like to count the frequencies for
> the combination of the two variables (f1 and f2)  for each id. I know it is
> should be straight forward, but I just don't know how to do it in R. Here
> is the SAS code I will use to get the output I want :

table() will count frequencies, and interaction() will create a variable 
with all combinations of a set of variables, so something like

table(interaction(f1,f2))

 	-thomas



From dkannan41 at yahoo.com  Wed Jan  5 00:37:16 2005
From: dkannan41 at yahoo.com (duraikannan sundaramoorthi)
Date: Tue, 4 Jan 2005 15:37:16 -0800 (PST)
Subject: [R] Histogram
Message-ID: <20050104233716.29875.qmail@web40901.mail.yahoo.com>

I have data on a single variable LOGT. It has about
300,000 observations. I am trying to make a Histogram
out of this data set. Following is my effort. Could
anyone help me to solve this error.


> hist(x)
Error in hist.default(x) : `x' must be numeric
> class(x)
[1] "data.frame"
> is.object(x)
[1] TRUE
> is.vector(x)
[1] FALSE
> is.numeric(x)
[1] FALSE
> is.factor(x)
[1] FALSE

note:plot(x) works okay

durai



From tom_hoary at web.de  Wed Jan  5 00:59:23 2005
From: tom_hoary at web.de (thomas)
Date: Wed, 05 Jan 2005 00:59:23 +0100
Subject: [R] warnings and errors with R CMD INSTALL
In-Reply-To: <Pine.LNX.4.61.0501042324480.30382@gannet.stats>
References: <1104878076.7506.28.camel@localhost.localdomain>
	<Pine.LNX.4.61.0501042324480.30382@gannet.stats>
Message-ID: <1104883163.7506.41.camel@localhost.localdomain>

Hello,

Am Dienstag, den 04.01.2005, 23:25 +0000 schrieb Prof Brian Ripley:
> On Tue, 4 Jan 2005, thomas wrote:

> >
> > I followed RNews 3/3 and used:
> >
> > 1. R CMD INSTALL package_version.tar.gz
> >
> > and
> >
> > 2.options(CRAN="http://umfragen.sowi.uni-mainz.de/CRAN/")
> > install.packages("pkg1". "pkg2")
> >
> > Both approaches end up i.e.:
> >
> > WARNING: invalid package 'pkg1.tar.gz'
> > ERROR: no packages specified
> 
> But there is no package `pkg1' on CRAN.  Try a real name like
> 
> install.packages("tree")

Well, of cause I didn't want install pkg1 or pkg2,
the precise commandline was:

1. R CMD INSTALL epitools_0.3-3.tar.gz

and

2.  install.packages("accuracy", "zoo", "abind")

Sorry for being so unclear in my first message! I did take the package
names from R-CRAN website, copied and pasted to commandline


Thanks
Thomas



From andrewr at uidaho.edu  Wed Jan  5 01:24:27 2005
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Wed, 5 Jan 2005 11:24:27 +1100
Subject: [R] Histogram
In-Reply-To: <20050104233716.29875.qmail@web40901.mail.yahoo.com>
References: <20050104233716.29875.qmail@web40901.mail.yahoo.com>
Message-ID: <20050105002427.GV591@uidaho.edu>

x is not a numeric vector, so R doesn't know how to take a histogram.
Here are your clues:

> class(x)
> [1] "data.frame"
> is.numeric(x)
> [1] FALSE

so, try

names(x)

I speculate that LOGT is contained in the dataframe x.  If so, try

hist(x$LOGT)

I hope that this helps.

Andrew


On Tue, Jan 04, 2005 at 03:37:16PM -0800, duraikannan sundaramoorthi wrote:
> I have data on a single variable LOGT. It has about
> 300,000 observations. I am trying to make a Histogram
> out of this data set. Following is my effort. Could
> anyone help me to solve this error.
> 
> 
> > hist(x)
> Error in hist.default(x) : `x' must be numeric
> > class(x)
> [1] "data.frame"
> > is.object(x)
> [1] TRUE
> > is.vector(x)
> [1] FALSE
> > is.numeric(x)
> [1] FALSE
> > is.factor(x)
> [1] FALSE
> 
> note:plot(x) works okay
> 
> durai
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From pmccask at cyllene.uwa.edu.au  Wed Jan  5 01:48:45 2005
From: pmccask at cyllene.uwa.edu.au (Pamela McCaskie)
Date: Wed, 5 Jan 2005 08:48:45 +0800 (WST)
Subject: [R] subsetting within a function using lme
Message-ID: <Pine.LNX.4.56.0501050847001.27564@cyllene.uwa.edu.au>

Thankyou for your help with subsetting within a function. I have now tried
to apply the same theory in the framework of an lme as follows:

fit1.lme <- eval(substitute(lme(fixed=fixed, data=dataframe,
random=random, correlation=corCAR1(form= corr), na.action=na.omit,
subset=subset),list(subset=subs)))

but I get the following error:

Error in switch(mode(object), name = , numeric = , call = object,
character = as.name(object),  :
        [[ cannot be of mode logical

I'm not sure why. Can anyone help me with this?
kind regards
Pam



On Thu, 30 Dec 2004, Thomas Lumley wrote:

> On Thu, 30 Dec 2004, Pamela McCaskie wrote:
> > And so my attempt to wrap a simple function around this looks like:
> > test.fun <- function(formula, mydata, sub=NULL){
> >  subs <- with(mydata, eval(sub))
> >  fit.glm <- glm(formula=formula, data=mydata, family=binomial, subset=subs)
> >  return(fit.glm)
> > }
> >
> > But when I tested it out with
> > test <- test.fun(y~x1+x2, mydata=testdata, sub=expression(SEX==0))
> >
> > I get:
> > Error in "[.data.frame"(structure(list(N_ASTHMA = as.integer(c(0, 0, 0,  :
> >        invalid subscript type
>
> I get a different error: it may be that you have an object called `subs`
> in the global environment
>
> > I'm guessing that it's looking in the global environment for
> > subs,
>
> More precisely, it is looking in environment where `formula` was created,
> which happens to be the global environment.
>
> This is the sort of thing that happens with the fitting functions because
> they go to such lengths to break the basic scoping of R.
>
>
> You probably have to substitute() the evaluated subset into the glm call.
>
>    fit.glm <- eval(substitute(glm(formula=formula, data=mydata,
>       family=binomial, subset=subset),list(subset=subs)))
>
>
>  	-thomas
>



From andy_liaw at merck.com  Wed Jan  5 01:55:34 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 4 Jan 2005 19:55:34 -0500
Subject: [R] Non linear Square fit Function
Message-ID: <3A822319EB35174CA3714066D590DCD50994E49C@usrymx25.merck.com>

Have you tried fitdistr in the MASS package (should be installed with R by
default)?

Andy

> From: Jagarlamudi, Choudary
> 
> Hi all,
> I'm trying to use the nls function to do a nls curve fit. I 
> have a histogram for my data. I need to get athe best curve 
> fit for this. i am plotting experimental vs the theoretical 
> values. my experimental values are got from my dnorm 
> function, mean=mean of my mid values(hist) and sd=sd of my 
> mid values. For the theoretical i'm using mean and sd of all 
> 20000 raw values as the start values. I'm getting no sd in my 
> new curve. Can someone gimme some pointers to get started here.
> I need my nls to spit out sd and mean of the new fit which it 
> does'nt. is there a better function or can i somehow get around this.
> Thanks in advance.
>  
> Choudary Jagarlamudi
> Instructor
> Southwestern Oklahoma State University
> STF 254
> 100 campus Drive
> Weatherford OK 73096
> Tel 580-774-7136
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andrewr at uidaho.edu  Wed Jan  5 02:00:13 2005
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Wed, 5 Jan 2005 12:00:13 +1100
Subject: [R] subsetting within a function using lme
In-Reply-To: <Pine.LNX.4.56.0501050847001.27564@cyllene.uwa.edu.au>
References: <Pine.LNX.4.56.0501050847001.27564@cyllene.uwa.edu.au>
Message-ID: <20050105010013.GX591@uidaho.edu>

Hi Pam,

can you provide just enough code for us to reproduce the error on one
of the sample datasets included with lme?  No more and no less than
the minimum necessary, please.

I find it easier to debug when things are concrete :)

Thanks,

Andrew

On Wed, Jan 05, 2005 at 08:48:45AM +0800, Pamela McCaskie wrote:
> Thankyou for your help with subsetting within a function. I have now tried
> to apply the same theory in the framework of an lme as follows:
> 
> fit1.lme <- eval(substitute(lme(fixed=fixed, data=dataframe,
> random=random, correlation=corCAR1(form= corr), na.action=na.omit,
> subset=subset),list(subset=subs)))
> 
> but I get the following error:
> 
> Error in switch(mode(object), name = , numeric = , call = object,
> character = as.name(object),  :
>         [[ cannot be of mode logical
> 
> I'm not sure why. Can anyone help me with this?
> kind regards
> Pam
> 
> 
> 
> On Thu, 30 Dec 2004, Thomas Lumley wrote:
> 
> > On Thu, 30 Dec 2004, Pamela McCaskie wrote:
> > > And so my attempt to wrap a simple function around this looks like:
> > > test.fun <- function(formula, mydata, sub=NULL){
> > >  subs <- with(mydata, eval(sub))
> > >  fit.glm <- glm(formula=formula, data=mydata, family=binomial, subset=subs)
> > >  return(fit.glm)
> > > }
> > >
> > > But when I tested it out with
> > > test <- test.fun(y~x1+x2, mydata=testdata, sub=expression(SEX==0))
> > >
> > > I get:
> > > Error in "[.data.frame"(structure(list(N_ASTHMA = as.integer(c(0, 0, 0,  :
> > >        invalid subscript type
> >
> > I get a different error: it may be that you have an object called `subs`
> > in the global environment
> >
> > > I'm guessing that it's looking in the global environment for
> > > subs,
> >
> > More precisely, it is looking in environment where `formula` was created,
> > which happens to be the global environment.
> >
> > This is the sort of thing that happens with the fitting functions because
> > they go to such lengths to break the basic scoping of R.
> >
> >
> > You probably have to substitute() the evaluated subset into the glm call.
> >
> >    fit.glm <- eval(substitute(glm(formula=formula, data=mydata,
> >       family=binomial, subset=subset),list(subset=subs)))
> >
> >
> >  	-thomas
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From pmccask at cyllene.uwa.edu.au  Wed Jan  5 02:22:39 2005
From: pmccask at cyllene.uwa.edu.au (Pamela McCaskie)
Date: Wed, 5 Jan 2005 09:22:39 +0800 (WST)
Subject: [R] subsetting within a function using lme
In-Reply-To: <20050105010013.GX591@uidaho.edu>
References: <Pine.LNX.4.56.0501050847001.27564@cyllene.uwa.edu.au>
	<20050105010013.GX591@uidaho.edu>
Message-ID: <Pine.LNX.4.56.0501050915180.27564@cyllene.uwa.edu.au>

Here you go
A simplified version of the function looks like this:

test.fun <- function(fixed, random, mydata, sub=NULL){

  library(nlme)
  subs <- with(mydata, eval(sub))
  fit.lme <- eval(substitute(lme(fixed=fixed, data=mydata, random=random,
na.action=na.omit, subset=subset),list(subset=subs)))

  return(fit.lme)
}

So using the Orthodont dataset I tried:

test <- snp.long(fixed=distance~age, random=~1|Subject, mydata=Orthodont,
sub=expression(Sex=="Male"))

and get that error.
Error in switch(mode(object), name = , numeric = , call = object,
character = as.name(object),  :
        [[ cannot be of mode logical

Thanks for your help

> Hi Pam,
>
> can you provide just enough code for us to reproduce the error on one
> of the sample datasets included with lme?  No more and no less than
> the minimum necessary, please.
>
> I find it easier to debug when things are concrete :)
>
> Thanks,
>
> Andrew
>
> On Wed, Jan 05, 2005 at 08:48:45AM +0800, Pamela McCaskie wrote:
> > Thankyou for your help with subsetting within a function. I have now tried
> > to apply the same theory in the framework of an lme as follows:
> >
> > fit1.lme <- eval(substitute(lme(fixed=fixed, data=dataframe,
> > random=random, correlation=corCAR1(form= corr), na.action=na.omit,
> > subset=subset),list(subset=subs)))
> >
> > but I get the following error:
> >
> > Error in switch(mode(object), name = , numeric = , call = object,
> > character = as.name(object),  :
> >         [[ cannot be of mode logical
> >
> > I'm not sure why. Can anyone help me with this?
> > kind regards
> > Pam
> >
> >
> >
> > On Thu, 30 Dec 2004, Thomas Lumley wrote:
> >
> > > On Thu, 30 Dec 2004, Pamela McCaskie wrote:
> > > > And so my attempt to wrap a simple function around this looks like:
> > > > test.fun <- function(formula, mydata, sub=NULL){
> > > >  subs <- with(mydata, eval(sub))
> > > >  fit.glm <- glm(formula=formula, data=mydata, family=binomial, subset=subs)
> > > >  return(fit.glm)
> > > > }
> > > >
> > > > But when I tested it out with
> > > > test <- test.fun(y~x1+x2, mydata=testdata, sub=expression(SEX==0))
> > > >
> > > > I get:
> > > > Error in "[.data.frame"(structure(list(N_ASTHMA = as.integer(c(0, 0, 0,  :
> > > >        invalid subscript type
> > >
> > > I get a different error: it may be that you have an object called `subs`
> > > in the global environment
> > >
> > > > I'm guessing that it's looking in the global environment for
> > > > subs,
> > >
> > > More precisely, it is looking in environment where `formula` was created,
> > > which happens to be the global environment.
> > >
> > > This is the sort of thing that happens with the fitting functions because
> > > they go to such lengths to break the basic scoping of R.
> > >
> > >
> > > You probably have to substitute() the evaluated subset into the glm call.
> > >
> > >    fit.glm <- eval(substitute(glm(formula=formula, data=mydata,
> > >       family=binomial, subset=subset),list(subset=subs)))
> > >
> > >
> > >  	-thomas
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> --
> Andrew Robinson                      Ph: 208 885 7115
> Department of Forest Resources       Fa: 208 885 6226
> University of Idaho                  E : andrewr at uidaho.edu
> PO Box 441133                        W : http://www.uidaho.edu/~andrewr
> Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
> No statement above necessarily represents my employer's opinion.
>



From Tom.Mulholland at dpi.wa.gov.au  Wed Jan  5 03:29:13 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Wed, 5 Jan 2005 10:29:13 +0800
Subject: [R] scree plot
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA58@afhex01.dpi.wa.gov.au>

I thought this would be simple enough and in some ways it is, but I don't have an answer I would call easy

The manual process would be to use barplot instead of screeplot, so that you could return the x values. The other choice would be to hack the screeplot function to return the values. (see the end)

Then you need to get the y values. If you follow the example in screeplot

## The variances of the variables in the
     ## USArrests data vary by orders of magnitude, so scaling is appropriate
     (pc.cr <- princomp(USArrests, cor = TRUE))  # inappropriate
     screeplot(pc.cr)

 then you might use
   pc.cr$sdev^2 * 0.9 

My difficulty came when I looked at pc.cr$loadings. This uses the print.loadings method to calulate the cumulative values

a very quick kludge gives me

cumvar <- function (x, digits = 3, cutoff = 0.1, sort = FALSE, ...) 
{
    Lambda <- unclass(x)
    p <- nrow(Lambda)
    factors <- ncol(Lambda)
    if (sort) {
        mx <- max.col(abs(Lambda))
        ind <- cbind(1:p, mx)
        mx[abs(Lambda[ind]) < 0.5] <- factors + 1
        Lambda <- Lambda[order(mx, 1:p), ]
    }
    cat("\nLoadings:\n")
    fx <- format(round(Lambda, digits))
    names(fx) <- NULL
    nc <- nchar(fx[1])
    fx[abs(Lambda) < cutoff] <- paste(rep(" ", nc), collapse = "")
    vx <- colSums(x^2)
    varex <- rbind("SS loadings" = vx)
    if (is.null(attr(x, "covariance"))) {
        varex <- rbind(varex, "Proportion Var" = vx/p)
        if (factors > 1) 
            varex <- rbind(varex, "Cumulative Var" = cumsum(vx/p))
    }
    invisible(return(cumsum(vx/p)))
}


screeplot <- function (x, npcs = min(10, length(x$sdev)), type = c("barplot",
    "lines"), main = deparse(substitute(x)), ...)
{
    main
    type <- match.arg(type)
    pcs <- x$sdev^2
    xp <- seq(length = npcs)
    if (type == "barplot")
        tt <- barplot(pcs[xp], names = names(pcs[xp]), main = main,
            ylab = "Variances", ...)
    else {
        plot(xp, pcs[xp], type = "b", axes = FALSE, main = main,
            xlab = "", ylab = "Variances", ...)
        axis(2)
        axis(1, at = xp, labels = names(pcs[xp]))
    }
    invisible(return(tt))
    
}

here <- screeplot(pc.cr)
text(here,pc.cr$sdev^2 + 0.1,paste(cumvar(pc.cr$loadings)*100,"%",sep = ""),xpd=T)



> -----Original Message-----
> From: Anne [mailto:anne.piotet at urbanet.ch]
> Sent: Wednesday, 5 January 2005 5:45 AM
> To: R list
> Subject: [R] scree plot
> 
> 
> Hi!
> 
> Is there an easy way to add to the scree-plot labels to each 
> value pertaining to the cumulative proportion of explained variance?
> 
> Thanks and a happy new year
> 
> Anne
> ----------------------------------------------------
> Anne Piotet
> Tel: +41 79 359 83 32 (mobile)
> Email: anne.piotet at m-td.com
> ---------------------------------------------------
> M-TD Modelling and Technology Development
> PSE-C
> CH-1015 Lausanne
> Switzerland
> Tel: +41 21 693 83 98
> Fax: +41 21 646 41 33
> --------------------------------------------------
>  
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ggrothendieck at myway.com  Wed Jan  5 04:18:20 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 5 Jan 2005 03:18:20 +0000 (UTC)
Subject: [R] quantiles for geometric distribution
References: <618B9869-5EA2-11D9-B442-000393883D7E@web.de>
Message-ID: <loom.20050105T040218-187@post.gmane.org>


dax42 <Dax42 <at> web.de> writes:

: 
: Dear list,
: 
: I have got an array with observational values t and I would like to fit 
: a geometric distribution to it.
: As I understand the geometric distribution, there is only one 
: parameter, the probability p. I estimated it by 1/mean(t).

p=1/EX if the geometric distribution starts at 1 but in R the
geometric distribution starts at 0. That is, in R the geometric 
distribution is the number of failures before a success, not the 
number of trials including the success.  

If X is a geometric random variable then EX = 0p + (EX+1)(1-p)
and solving for EX gives 1/p-1.

: 
: Now I plotted the estimated density function by
: plot(ecdf(t),do.points=FALSE,col.h="blue");
: 
: and I would like to add the geometric distribution. This should be 
: possibly with the function pgeom().
: 
: Unfortunately I do not understand what is meant by the argument q, 
: "vector of quantiles representing the number of failures in a sequence 
: of Bernoulli trials before success occurs" according to R help.
: 
: I am familiar with quantiles, but why do I need them here?
: Does anybody know what this means? What am I supposed to do?

The quantiles are just the values of the geometric random
variable.  That is if you have a data vector x in which the
ith element of x is the ith observation (where each
observation is the number of failures before a success, viz.
a non-negative integer) then dgeom(x, .2) would give a vector 
of density values assuming the probability of a success is .2 .



From ripley at stats.ox.ac.uk  Wed Jan  5 07:47:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Jan 2005 06:47:05 +0000 (GMT)
Subject: [R] warnings and errors with R CMD INSTALL
In-Reply-To: <1104883163.7506.41.camel@localhost.localdomain>
References: <1104878076.7506.28.camel@localhost.localdomain>
	<Pine.LNX.4.61.0501042324480.30382@gannet.stats>
	<1104883163.7506.41.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.61.0501050643270.2502@gannet.stats>

On Wed, 5 Jan 2005, thomas wrote:

> Hello,
>
> Am Dienstag, den 04.01.2005, 23:25 +0000 schrieb Prof Brian Ripley:
>> On Tue, 4 Jan 2005, thomas wrote:

>>> I followed RNews 3/3 and used:
>>>
>>> 1. R CMD INSTALL package_version.tar.gz
>>>
>>> and
>>>
>>> 2.options(CRAN="http://umfragen.sowi.uni-mainz.de/CRAN/")
>>> install.packages("pkg1". "pkg2")
>>>
>>> Both approaches end up i.e.:
>>>
>>> WARNING: invalid package 'pkg1.tar.gz'
>>> ERROR: no packages specified
>>
>> But there is no package `pkg1' on CRAN.  Try a real name like
>>
>> install.packages("tree")
>
> Well, of cause I didn't want install pkg1 or pkg2,
> the precise commandline was:
>
> 1. R CMD INSTALL epitools_0.3-3.tar.gz

Have you downloaded the file first to the current directory?

> and
>
> 2.  install.packages("accuracy", "zoo", "abind")

The syntax is install.packages(c("accuracy", "zoo", "abind"))

[I did suggest an example you could have tried, deliberately with one 
package.  I don't get the error message you said you got from your line.]


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Wed Jan  5 09:26:50 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 05 Jan 2005 09:26:50 +0100
Subject: [R] rownames and plot lablels
In-Reply-To: <20050104182536.46705.qmail@web53209.mail.yahoo.com>
Message-ID: <41DBB2DA.12441.5179F8@localhost>



On 4 Jan 2005 at 10:25, Derek Margetts wrote:

> 
> I have a question about changing rownames.  In the
> following function I am plotting the regression
> coeficients with their corresponding mean.  Right now,
> the labels on the plot and the rownames in the
> dataframe are x1,x2,x3...etc.  Is there a way to make
> the row names the same as the variable name entered
> into the formula?  I.E. if x1 was a vector called
> opinions, it would be labeled opinions on the plot
> instead of x1.  I have tried using variations of
> rownames(x)<- namevector but have not been successful.

If your mf  is your data frame 

rownames(mf) <- namevector 

with the same length should work.

BTW, If you have x1-x14 in data frame e.g. yourdf you can use 

meanvec<-colMeans(yourdf, na.rm=TRUE)

Cheers
Petr

> I keep getting a null.  Any suggestions would be
> appreciated
> 
> Quadplot<-function(y, x1, x2, x3 = NULL, x4  =
> NULL,x5=NULL,x6=NULL,x7=NULL,x8=NULL,x9=NULL,x10=NULL,
> x11=NULL, x12=NULL, x13=NULL, x14=NULL)
> 
> #section of code ommitted
> 
> my.formula <- as.formula("y ~ x1 + x2 + x3+ x4+ x5+
> x6+ x7+ x8+ x9+x10+x11+x12+x13+x14")
> outlm <- lm(my.formula)
> 
> #section of code ommitted
> 
> meanvec<-c(mean(x1,na.rm=TRUE),mean(x2,na.rm=TRUE),mean(x3,na.rm=TRUE)
> ,mean(x4,na.rm=TRUE),mean(x5,na.rm=TRUE),mean(x6,na.rm=TRUE),mean(x7,n
> a.rm=TRUE),mean(x8,na.rm=TRUE),mean(x9,na.rm=TRUE),mean(x10,na.rm=TRUE
> ),mean(x11,na.rm=TRUE) ,mean(x12,na.rm=TRUE) ,mean(x13,na.rm=TRUE)
> ,mean(x14,na.rm=TRUE)) } meanper<-meanvec/6 meanper1<-meanper*100
> 
> #This is the data frame created
> mf <- data.frame(Impact = abs(outlm$coef[-1]),
> Performance = meanper1)
> 
> meanx<-mean(abs(outlm$coef[-1]))
> meany<-mean(meanper1)
> n<-length(meanvec)
> color<-rep("blue",n)
> color[(mf$Impact>meanx &
> mf$Performance>meany)]<-"green"
> color[(mf$Impact>meanx & mf$Performance<meany)]<-"red"
> color[(mf$Impact<meanx &
> mf$Performance>meany)]<-"gray"
> 
> plot((mf),col=color,main="Perfomance vs.
> Impact",xlab="Impact",ylab="Performance", type = "n")
> points(mf, pch=21, bg=color, cex=4)
> text(x = mf$Impact, y = mf$Performance, labels =
> rownames(mf))
> abline(h=mean(mf$Performance))
> abline(v=mean(mf$Impact))
> print(mf)
> }
> 
> Thanks
> Derek
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From r.hankin at soc.soton.ac.uk  Wed Jan  5 09:49:00 2005
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Wed, 5 Jan 2005 08:49:00 +0000
Subject: [R] integer factorization
Message-ID: <A4D457E6-5EF6-11D9-946F-000A95D86AA8@soc.soton.ac.uk>

Hi

has anyone coded up integer factorization? I want, for example,

R>  factorize(60)

$primes
[1] 2 3 5

$powers
[1] 2 1 1


(actually, I want the divisor function $\sigma_a(n)$, but coding this 
up requires integer factorization).

--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From ripley at stats.ox.ac.uk  Wed Jan  5 09:50:32 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Jan 2005 08:50:32 +0000 (GMT)
Subject: [R] rownames and plot lablels
In-Reply-To: <41DBB2DA.12441.5179F8@localhost>
References: <41DBB2DA.12441.5179F8@localhost>
Message-ID: <Pine.LNX.4.61.0501050845050.6942@gannet.stats>

It is sometimes important to know that row.names accesses the row names of 
a data frame, and rownames the first dimname of an array (including a 
matrix).  They are almost equivalent, but not quite (e.g. row.names is 
generic and has methods for matrices and arrays).

So the example would optimally be

row.names(mf) <- namevector

This tests for the validity of the row names.

On Wed, 5 Jan 2005, Petr Pikal wrote:

> On 4 Jan 2005 at 10:25, Derek Margetts wrote:
>>
>> I have a question about changing rownames.  In the
>> following function I am plotting the regression
>> coeficients with their corresponding mean.  Right now,
>> the labels on the plot and the rownames in the
>> dataframe are x1,x2,x3...etc.  Is there a way to make
>> the row names the same as the variable name entered
>> into the formula?  I.E. if x1 was a vector called
>> opinions, it would be labeled opinions on the plot
>> instead of x1.  I have tried using variations of
>> rownames(x)<- namevector but have not been successful.
>
> If your mf  is your data frame
>
> rownames(mf) <- namevector
>
> with the same length should work.
>
> BTW, If you have x1-x14 in data frame e.g. yourdf you can use
>
> meanvec<-colMeans(yourdf, na.rm=TRUE)
>
> Cheers
> Petr
>
>> I keep getting a null.  Any suggestions would be
>> appreciated
>>
>> Quadplot<-function(y, x1, x2, x3 = NULL, x4  =
>> NULL,x5=NULL,x6=NULL,x7=NULL,x8=NULL,x9=NULL,x10=NULL,
>> x11=NULL, x12=NULL, x13=NULL, x14=NULL)
>>
>> #section of code ommitted
>>
>> my.formula <- as.formula("y ~ x1 + x2 + x3+ x4+ x5+
>> x6+ x7+ x8+ x9+x10+x11+x12+x13+x14")
>> outlm <- lm(my.formula)
>>
>> #section of code ommitted
>>
>> meanvec<-c(mean(x1,na.rm=TRUE),mean(x2,na.rm=TRUE),mean(x3,na.rm=TRUE)
>> ,mean(x4,na.rm=TRUE),mean(x5,na.rm=TRUE),mean(x6,na.rm=TRUE),mean(x7,n
>> a.rm=TRUE),mean(x8,na.rm=TRUE),mean(x9,na.rm=TRUE),mean(x10,na.rm=TRUE
>> ),mean(x11,na.rm=TRUE) ,mean(x12,na.rm=TRUE) ,mean(x13,na.rm=TRUE)
>> ,mean(x14,na.rm=TRUE)) } meanper<-meanvec/6 meanper1<-meanper*100
>>
>> #This is the data frame created
>> mf <- data.frame(Impact = abs(outlm$coef[-1]),
>> Performance = meanper1)
>>
>> meanx<-mean(abs(outlm$coef[-1]))
>> meany<-mean(meanper1)
>> n<-length(meanvec)
>> color<-rep("blue",n)
>> color[(mf$Impact>meanx &
>> mf$Performance>meany)]<-"green"
>> color[(mf$Impact>meanx & mf$Performance<meany)]<-"red"
>> color[(mf$Impact<meanx &
>> mf$Performance>meany)]<-"gray"
>>
>> plot((mf),col=color,main="Perfomance vs.
>> Impact",xlab="Impact",ylab="Performance", type = "n")
>> points(mf, pch=21, bg=color, cex=4)
>> text(x = mf$Impact, y = mf$Performance, labels =
>> rownames(mf))
>> abline(h=mean(mf$Performance))
>> abline(v=mean(mf$Impact))
>> print(mf)
>> }
>>
>> Thanks
>> Derek
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
> Petr Pikal
> petr.pikal at precheza.cz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From manuel_gutierrez_lopez at yahoo.es  Wed Jan  5 09:59:00 2005
From: manuel_gutierrez_lopez at yahoo.es (Manuel Gutierrez)
Date: Wed, 5 Jan 2005 09:59:00 +0100 (CET)
Subject: [R] unlist kills R
Message-ID: <20050105085900.40778.qmail@web25101.mail.ukl.yahoo.com>

When I try to unlist a very large list, R is killed
without any other warning:
A<-as.list(as.data.frame(matrix(1:21639744,nrow=3578,ncol=6048)))
with 
AA<-unlist(A)
or
AA<-c(A,recursive=TRUE)
I get a 
R terminado (killed)
and the end of the session.
I think I'll need to get more RAM (now 1Gb, any other
solutions welcomed) to be able to do this but,
shouldn't I get a more gentle warning than the kill
message?
Thanks,
Manuel


platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    2                
minor    0.1              
year     2004             
month    11               
day      15               
language R



From gregor.gorjanc at bfro.uni-lj.si  Wed Jan  5 16:17:48 2005
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Wed, 05 Jan 2005 16:17:48 +0100
Subject: [R] Converting integers to chars i.e 1 to "01"
Message-ID: <41DC051C.1060805@bfro.uni-lj.si>

Hello!

I am producing a set of images and I would like them to be sorted by names 
I give. I was able to produce my names and add integer to them. That is 
easy. But my problem lies in sort of file from this process:

figure_10.png
figure_11.png
figure_12.png
...
figure_1.png
figure_20.png
...

So I would like to convert integers to something like 01 if upper limit for
this conert is 10 or 001 for 100. I wrote a simple function (see below), 
but I do not know how this limit stuff can be imporved to work really well 
with default. Any suggestions?

int2char <- function(x, limit = max(x)) {

     # Description:
     # Converts integer to character such that numbers bellow limit get 0 in
     # front
     # Gregor GORJANC, 2005-01-05

     # Arguments:
     # x: vector of numbers
     # limit: limit up to which numbers should get 0 in front, default
     #        max(x)

     # Examples:
     # a <- seq(0, 20, 1)
     # int2char(a) # this does not work OK
     # int2char(a, limit = 10) # this does work OK

     # How to:
     # I would like that default would be more efficient - so it would
     # recognize that let say limit 20 in example above should actually be
     # 10 and so on.

     # Code:
     for (i in 1:length(x)) {
         if (x[i] < limit) {
             n[i] <- paste("0", x[i], sep  = "")
         } else {
             n[i] <- as.character(x[i])
         }
     }
     return(n)
}


-- 
Lep pozdrav / With regards,
     Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia



From tom_hoary at web.de  Wed Jan  5 10:17:45 2005
From: tom_hoary at web.de (thomas)
Date: Wed, 05 Jan 2005 10:17:45 +0100
Subject: [R] warnings and errors with R CMD INSTALL
In-Reply-To: <Pine.LNX.4.61.0501050643270.2502@gannet.stats>
References: <1104878076.7506.28.camel@localhost.localdomain>
	<Pine.LNX.4.61.0501042324480.30382@gannet.stats>
	<1104883163.7506.41.camel@localhost.localdomain>
	<Pine.LNX.4.61.0501050643270.2502@gannet.stats>
Message-ID: <1104916665.6537.18.camel@localhost.localdomain>

Hello,

Am Mittwoch, den 05.01.2005, 06:47 +0000 schrieb Prof Brian Ripley:
> On Wed, 5 Jan 2005, thomas wrote:

> >> But there is no package `pkg1' on CRAN.  Try a real name like
> >>
> >> install.packages("tree")
> >
> > Well, of cause I didn't want install pkg1 or pkg2,
> > the precise commandline was:
> >
> > 1. R CMD INSTALL epitools_0.3-3.tar.gz
> 
> Have you downloaded the file first to the current directory?
Well, that was the crucial point, I simply didn't get aware or simply
over-read this issue (what a shame!)

> 
> > and
> >
> > 2.  install.packages("accuracy", "zoo", "abind")
> 
> The syntax is install.packages(c("accuracy", "zoo", "abind"))

Well, given the examples in administration manual and RNews 3/3, your
example looks a bit different (the c !). I didn't figure out that you'll
have to use this to install packages accurately
> 
> [I did suggest an example you could have tried, deliberately with one 
> package.  I don't get the error message you said you got from your line.]

Well, that's correct, now with your kind advice this isn't a problem
anymore



Finally, it seems to me that the install.package should help to stay
tuned with regards to required dependencies.
Just found out, that some dependency descriptions of packages are
somewhat missing some necessities,

i.e. RODBC package descriptions mentions:
(..)
 R (>= 1.9.0)
(..)

When trying to install RODBC I had to learn that on my system there is:

configure: error: "no ODBC driver manager found"
ERROR: configuration failed for package 'RODBC'

So, R CMD INSTALL seemingly doesn't take care of missing dependencies
except given an error message.

Since I am relatively new to R I am kind of lost to figure out these
dependencies on my own (think its getting better gradually).

Anyway,

thanks for immediate response


Thomas



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Jan  5 10:21:19 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 5 Jan 2005 10:21:19 +0100
Subject: [R] unlist kills R
References: <20050105085900.40778.qmail@web25101.mail.ukl.yahoo.com>
Message-ID: <000401c4f307$eabbbfb0$0540210a@www.domain>

Hi Manuel,

try the following:

A <- data.frame(matrix(1:21639744, nrow=3578, ncol=6048))
AA <- unlist(A, use.names=FALSE)

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Manuel Gutierrez" <manuel_gutierrez_lopez at yahoo.es>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, January 05, 2005 9:59 AM
Subject: [R] unlist kills R


> When I try to unlist a very large list, R is killed
> without any other warning:
> A<-as.list(as.data.frame(matrix(1:21639744,nrow=3578,ncol=6048)))
> with
> AA<-unlist(A)
> or
> AA<-c(A,recursive=TRUE)
> I get a
> R terminado (killed)
> and the end of the session.
> I think I'll need to get more RAM (now 1Gb, any other
> solutions welcomed) to be able to do this but,
> shouldn't I get a more gentle warning than the kill
> message?
> Thanks,
> Manuel
>
>
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    2
> minor    0.1
> year     2004
> month    11
> day      15
> language R
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From halldor at vedur.is  Wed Jan  5 10:34:37 2005
From: halldor at vedur.is (=?ISO-8859-1?Q?Halldor_Bj=F6rnsson?=)
Date: Wed, 05 Jan 2005 09:34:37 +0000
Subject: [R] make R package for windows on Linux
Message-ID: <41DBB4AD.6060103@vedur.is>

Hi,
I made a package on a linux box. All worked fine.
The package contains only R code (no C). I then wanted to make a zip 
file so that I could test the package on a windows machine. I have tried 
all the obvious ways to do this (and even some that are not!), but to no 
avail. The only  instructions I find about building packages for windows 
(e.g. mypkg.zip) seem to imply that the build be done on a windows machine.

Is there a simple way to make mypkg.zip under linux and then install it 
as a zip file on a windows machine?

Sincerely,
Halldor
-- 
------------------------------------------
Halldor Bjornsson   (halldor at vedur.is)
Vedurstofa Islands (Icelandic Met. Office)
Bustadavegur 9, IS-150, Reykjavik, Iceland



From lecoutre at stat.ucl.ac.be  Wed Jan  5 10:29:48 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Wed, 05 Jan 2005 10:29:48 +0100
Subject: [R] Converting integers to chars i.e 1 to "01"
In-Reply-To: <41DC051C.1060805@bfro.uni-lj.si>
References: <41DC051C.1060805@bfro.uni-lj.si>
Message-ID: <6.0.1.1.2.20050105102909.03981ec0@stat4ux.stat.ucl.ac.be>


Hi Gregor,

There still exist simple functions to achive that goal:

Look at:
 > x=1:111
 > formatC(format="d",x,flag="0",width=ceiling(log10(max(x))))

   [1] "001" "002" "003" "004" "005" "006" "007" "008" "009" "010" "011" 
"012" "013" "014" "015" "016" "017" "018" "019" "020"
  [21] "021" "022" "023" "024" "025" "026" "027" "028" "029" "030" "031" 
"032" "033" "034" "035" "036" "037" "038" "039" "040"
  [41] "041" "042" "043" "044" "045" "046" "047" "048" "049" "050" "051" 
"052" "053" "054" "055" "056" "057" "058" "059" "060"
  [61] "061" "062" "063" "064" "065" "066" "067" "068" "069" "070" "071" 
"072" "073" "074" "075" "076" "077" "078" "079" "080"
  [81] "081" "082" "083" "084" "085" "086" "087" "088" "089" "090" "091" 
"092" "093" "094" "095" "096" "097" "098" "099" "100"
[101] "101" "102" "103" "104" "105" "106" "107" "108" "109" "110" "111"


? formatC

HTH,

Eric


At 16:17 5/01/2005, Gregor GORJANC wrote:
>Hello!
>
>I am producing a set of images and I would like them to be sorted by names 
>I give. I was able to produce my names and add integer to them. That is 
>easy. But my problem lies in sort of file from this process:
>
>figure_10.png
>figure_11.png
>figure_12.png
>...
>figure_1.png
>figure_20.png
>...
>
>So I would like to convert integers to something like 01 if upper limit for
>this conert is 10 or 001 for 100. I wrote a simple function (see below), 
>but I do not know how this limit stuff can be imporved to work really well 
>with default. Any suggestions?
>
>int2char <- function(x, limit = max(x)) {
>
>     # Description:
>     # Converts integer to character such that numbers bellow limit get 0 in
>     # front
>     # Gregor GORJANC, 2005-01-05
>
>     # Arguments:
>     # x: vector of numbers
>     # limit: limit up to which numbers should get 0 in front, default
>     #        max(x)
>
>     # Examples:
>     # a <- seq(0, 20, 1)
>     # int2char(a) # this does not work OK
>     # int2char(a, limit = 10) # this does work OK
>
>     # How to:
>     # I would like that default would be more efficient - so it would
>     # recognize that let say limit 20 in example above should actually be
>     # 10 and so on.
>
>     # Code:
>     for (i in 1:length(x)) {
>         if (x[i] < limit) {
>             n[i] <- paste("0", x[i], sep  = "")
>         } else {
>             n[i] <- as.character(x[i])
>         }
>     }
>     return(n)
>}
>
>
>--
>Lep pozdrav / With regards,
>     Gregor GORJANC
>
>---------------------------------------------------------------
>University of Ljubljana
>Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
>Zootechnical Department    mail: gregor.gorjanc <at> bfro.uni-lj.si
>Groblje 3                  tel: +386 (0)1 72 17 861
>SI-1230 Domzale            fax: +386 (0)1 72 17 888
>Slovenia
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From Ted.Harding at nessie.mcc.ac.uk  Wed Jan  5 10:44:08 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 05 Jan 2005 09:44:08 -0000 (GMT)
Subject: [R] Converting integers to chars i.e 1 to "01"
In-Reply-To: <41DC051C.1060805@bfro.uni-lj.si>
Message-ID: <XFMail.050105094408.Ted.Harding@nessie.mcc.ac.uk>

On 05-Jan-05 Gregor GORJANC wrote:
> Hello!
> 
> I am producing a set of images and I would like them to be
> sorted by names I give.
> [...]
> So I would like to convert integers to something like 01 if
> upper limit for this conert is 10 or 001 for 100.
> [...]

Hi Gregor,

'formatC' provides access to C-style formatting. For example:

  > formatC(5,format="d",flag="0",width=3)
  [1] "005"

  > formatC(21,format="d",flag="0",width=3)
  [1] "021"

See "?format".

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 05-Jan-05                                       Time: 09:44:08
------------------------------ XFMail ------------------------------



From dmb at mrc-dunn.cam.ac.uk  Wed Jan  5 11:07:28 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Wed, 5 Jan 2005 10:07:28 +0000 (GMT)
Subject: [R] Converting integers to chars i.e 1 to "01"
In-Reply-To: <6.0.1.1.2.20050105102909.03981ec0@stat4ux.stat.ucl.ac.be>
Message-ID: <Pine.LNX.4.21.0501051004010.29958-100000@mail.mrc-dunn.cam.ac.uk>

>
>? formatC

Else (if you are linux) the following shell command can be useful...

rename file_name_ file_name_0 file_name_[0-9]_bleh_*.png

Where "file_name_[0-9]_bleh_*.png" is supposed to match all those files
with a single digit, and the rename command adds a zero before that digit.

rename can be tricky to use at first, just remember not to use any
wildcards in the 'FROM' and 'TO' string...

rename FROM TO PATTERN

NB the above is unix shell, not R.


>
>HTH,
>
>Eric
>
>
>At 16:17 5/01/2005, Gregor GORJANC wrote:
>>Hello!
>>
>>I am producing a set of images and I would like them to be sorted by names 
>>I give. I was able to produce my names and add integer to them. That is 
>>easy. But my problem lies in sort of file from this process:
>>
>>figure_10.png
>>figure_11.png
>>figure_12.png
>>...
>>figure_1.png
>>figure_20.png
>>...
>>
>>So I would like to convert integers to something like 01 if upper limit for
>>this conert is 10 or 001 for 100. I wrote a simple function (see below), 
>>but I do not know how this limit stuff can be imporved to work really well 
>>with default. Any suggestions?
>>
>>int2char <- function(x, limit = max(x)) {
>>
>>     # Description:
>>     # Converts integer to character such that numbers bellow limit get 0 in
>>     # front
>>     # Gregor GORJANC, 2005-01-05
>>
>>     # Arguments:
>>     # x: vector of numbers
>>     # limit: limit up to which numbers should get 0 in front, default
>>     #        max(x)
>>
>>     # Examples:
>>     # a <- seq(0, 20, 1)
>>     # int2char(a) # this does not work OK
>>     # int2char(a, limit = 10) # this does work OK
>>
>>     # How to:
>>     # I would like that default would be more efficient - so it would
>>     # recognize that let say limit 20 in example above should actually be
>>     # 10 and so on.
>>
>>     # Code:
>>     for (i in 1:length(x)) {
>>         if (x[i] < limit) {
>>             n[i] <- paste("0", x[i], sep  = "")
>>         } else {
>>             n[i] <- as.character(x[i])
>>         }
>>     }
>>     return(n)
>>}
>>
>>
>>--
>>Lep pozdrav / With regards,
>>     Gregor GORJANC
>>
>>---------------------------------------------------------------
>>University of Ljubljana
>>Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
>>Zootechnical Department    mail: gregor.gorjanc <at> bfro.uni-lj.si
>>Groblje 3                  tel: +386 (0)1 72 17 861
>>SI-1230 Domzale            fax: +386 (0)1 72 17 888
>>Slovenia
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>Eric Lecoutre
>UCL /  Institut de Statistique
>Voie du Roman Pays, 20
>1348 Louvain-la-Neuve
>Belgium
>
>tel: (+32)(0)10473050
>lecoutre at stat.ucl.ac.be
>http://www.stat.ucl.ac.be/ISpersonnel/lecoutre
>
>If the statistics are boring, then you've got the wrong numbers. -Edward 
>Tufte
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From thpe at hhbio.wasser.tu-dresden.de  Wed Jan  5 11:11:41 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Wed, 05 Jan 2005 11:11:41 +0100
Subject: [R] lme: error message with random=~1
Message-ID: <41DBBD5D.4030808@hhbio.wasser.tu-dresden.de>

Hello,

I have an unbalanced mixed model design with two fixed effects
"site" (2 levels) and "timeOfDay" (4 levels) and two random effects
"day" (3 consecutive days) and "trap" (6 unique traps, 3 per site).

The dependent variable is the body length ("BL") of insect larvae from 7 
to 29 individuals per trap (104 individuals in total).

To account for pseudo replication I used nlme (or lme4 as suggested for 
crossed random factors). The results indicate, that the random effects 
are very small, so I followed the lme example and tried to fit a model 
with random=~1.

Unfortunately I got the following error message:

Error in getGroups.data.frame(dataMix, groups) :
	Invalid formula for groups

I suppose, that it would be redundant (and confusing to the reader) if 
we leave unnecessary random effects in the model, but due to pseudo 
replication it may be an offense if we simply ignore it and use lm.

Reading the respective chapters in Pinheiro & Bates, Venables & Ripley 
and Crawley several times, I found no example for this situation. Is 
there a common way how to handle this?

Thomas P.

PS: I can provide a full example, if necessary.



From buser at stat.math.ethz.ch  Wed Jan  5 11:13:30 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Wed, 5 Jan 2005 11:13:30 +0100
Subject: [R] integer factorization
In-Reply-To: <A4D457E6-5EF6-11D9-946F-000A95D86AA8@soc.soton.ac.uk>
References: <A4D457E6-5EF6-11D9-946F-000A95D86AA8@soc.soton.ac.uk>
Message-ID: <16859.48586.242334.33187@stat.math.ethz.ch>

Hi Robin

There is a function factorize() in the package conf.design

library(conf.design)
factorize(60)
[1] 2 2 3 5

Hope this helps
Christoph

-- 
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C11
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-1-632-5414		fax: 632-1228
http://stat.ethz.ch/~buser/



From maechler at stat.math.ethz.ch  Wed Jan  5 11:15:58 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 5 Jan 2005 11:15:58 +0100
Subject: [R] Converting integers to chars i.e 1 to "01"
In-Reply-To: <6.0.1.1.2.20050105102909.03981ec0@stat4ux.stat.ucl.ac.be>
References: <41DC051C.1060805@bfro.uni-lj.si>
	<6.0.1.1.2.20050105102909.03981ec0@stat4ux.stat.ucl.ac.be>
Message-ID: <16859.48734.592438.511870@stat.math.ethz.ch>

>>>>> "Eric" == Eric Lecoutre <lecoutre at stat.ucl.ac.be>
>>>>>     on Wed, 05 Jan 2005 10:29:48 +0100 writes:

    Eric> Hi Gregor,

    Eric> There still exist simple functions to achive that goal:

    Eric> Look at:
    >> x=1:111
    >> formatC(format="d",x,flag="0",width=ceiling(log10(max(x))))

    Eric> [1] "001" "002" "003" "004" "005" "006" "007" "008" "009" "010" "011" 
    Eric> "012" "013" "014" "015" "016" "017" "018" "019" "020"
    Eric> [21] "021" "022" "023" "024" "025" "026" "027" "028" "029" "030" "031" 
    Eric> "032" "033" "034" "035" "036" "037" "038" "039" "040"
    Eric> [41] "041" "042" "043" "044" "045" "046" "047" "048" "049" "050" "051" 
    Eric> "052" "053" "054" "055" "056" "057" "058" "059" "060"
    Eric> [61] "061" "062" "063" "064" "065" "066" "067" "068" "069" "070" "071" 
    Eric> "072" "073" "074" "075" "076" "077" "078" "079" "080"
    Eric> [81] "081" "082" "083" "084" "085" "086" "087" "088" "089" "090" "091" 
    Eric> "092" "093" "094" "095" "096" "097" "098" "099" "100"
    Eric> [101] "101" "102" "103" "104" "105" "106" "107" "108" "109" "110" "111"
    Eric> ? formatC

Yes; note also  "sprintf".

The folllowing shows it can even be simplified:

> (nn <- c(1:12, sort(outer(10^(2:4), -1:1, "+"))))
 [1]     1     2     3     4     5     6     7     8     9    10    11    12
[13]    99   100   101   999  1000  1001  9999 10000 10001

> formatC(nn, width=3, flag="0")
 [1] "001"   "002"   "003"   "004"   "005"   "006"   "007"   "008"   "009"  
[10] "010"   "011"   "012"   "099"   "100"   "101"   "999"   "1000"  "1001" 
[19] "9999"  "1e+04" "1e+04"
> formatC(nn, width=3, flag="0", format="fg")
 [1] "001"   "002"   "003"   "004"   "005"   "006"   "007"   "008"   "009"  
[10] "010"   "011"   "012"   "099"   "100"   "101"   "999"   "1000"  "1001" 
[19] "9999"  "10000" "10001"

> sapply(as.integer(nn), function(n) sprintf("%03d", n))
 [1] "001"   "002"   "003"   "004"   "005"   "006"   "007"   "008"   "009"  
[10] "010"   "011"   "012"   "099"   "100"   "101"   "999"   "1000"  "1001" 
[19] "9999"  "10000" "10001"


Martin



From thpe at hhbio.wasser.tu-dresden.de  Wed Jan  5 11:18:25 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Wed, 05 Jan 2005 11:18:25 +0100
Subject: [R] Converting integers to chars i.e 1 to "01"
In-Reply-To: <41DC051C.1060805@bfro.uni-lj.si>
References: <41DC051C.1060805@bfro.uni-lj.si>
Message-ID: <41DBBEF1.60403@hhbio.wasser.tu-dresden.de>

Gregor GORJANC wrote:
> Hello!
> 
> I am producing a set of images and I would like them to be sorted by 
> names I give. I was able to produce my names and add integer to them. 
> That is easy. But my problem lies in sort of file from this process:
> 
> figure_10.png
> figure_11.png
> figure_12.png
> ...
> figure_1.png
> figure_20.png

One simple solution for sortable file names is to add a large integer to 
your index i:

png(filename=paste("figure_", i + 1000, ".png", sep=""))

Thomas P.



From ligges at statistik.uni-dortmund.de  Wed Jan  5 11:37:36 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 05 Jan 2005 11:37:36 +0100
Subject: [R] warnings and errors with R CMD INSTALL
In-Reply-To: <1104916665.6537.18.camel@localhost.localdomain>
References: <1104878076.7506.28.camel@localhost.localdomain>	<Pine.LNX.4.61.0501042324480.30382@gannet.stats>	<1104883163.7506.41.camel@localhost.localdomain>	<Pine.LNX.4.61.0501050643270.2502@gannet.stats>
	<1104916665.6537.18.camel@localhost.localdomain>
Message-ID: <41DBC370.9050009@statistik.uni-dortmund.de>

thomas wrote:

> Hello,
> 
> Am Mittwoch, den 05.01.2005, 06:47 +0000 schrieb Prof Brian Ripley:
> 
>>On Wed, 5 Jan 2005, thomas wrote:
> 
> 
>>>>But there is no package `pkg1' on CRAN.  Try a real name like
>>>>
>>>>install.packages("tree")
>>>
>>>Well, of cause I didn't want install pkg1 or pkg2,
>>>the precise commandline was:
>>>
>>>1. R CMD INSTALL epitools_0.3-3.tar.gz
>>
>>Have you downloaded the file first to the current directory?
> 
> Well, that was the crucial point, I simply didn't get aware or simply
> over-read this issue (what a shame!)
> 
> 
>>>and
>>>
>>>2.  install.packages("accuracy", "zoo", "abind")
>>
>>The syntax is install.packages(c("accuracy", "zoo", "abind"))
> 
> 
> Well, given the examples in administration manual and RNews 3/3, your
> example looks a bit different (the c !). I didn't figure out that you'll
> have to use this to install packages accurately

?install.packages tells us that its first argument "pkgs" has to be a 
"character vector".
The examples in the references mentioned above are using one package, 
hence something like "accuracy" as a one element character vector is 
sufficient, for more, you have to use the c() function in order to 
provide a vector of characters (names of the packages to install).



> 
>>[I did suggest an example you could have tried, deliberately with one 
>>package.  I don't get the error message you said you got from your line.]
> 
> 
> Well, that's correct, now with your kind advice this isn't a problem
> anymore
> 
> 
> 
> Finally, it seems to me that the install.package should help to stay
> tuned with regards to required dependencies.
> Just found out, that some dependency descriptions of packages are
> somewhat missing some necessities,
> 
> i.e. RODBC package descriptions mentions:
> (..)
>  R (>= 1.9.0)
> (..)
> 
> When trying to install RODBC I had to learn that on my system there is:
> 
> configure: error: "no ODBC driver manager found"
> ERROR: configuration failed for package 'RODBC'
> 
> So, R CMD INSTALL seemingly doesn't take care of missing dependencies
> except given an error message.


The R scripts are checking dependencies within R (R version, packages, 
etc.), but not external dependencies (compiler, system libraries, etc.). 
The latter are checked, e.g., in configure scripts within the packages, 
as it was perfectly done in your example.

How should R know a) how to look whether ODBC has been installed 
properly on your machine and b) how to install ODBC for you (in 
particular on the specific platform/OS you are using).
That's why some packages are making use of configure scripts.


Uwe Ligges


> Since I am relatively new to R I am kind of lost to figure out these
> dependencies on my own (think its getting better gradually).
>
> Anyway,
> 
> thanks for immediate response
> 
> 
> Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Jan  5 11:43:47 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 5 Jan 2005 11:43:47 +0100
Subject: [R] lme: error message with random=~1
References: <41DBBD5D.4030808@hhbio.wasser.tu-dresden.de>
Message-ID: <002c01c4f313$6fa55190$0540210a@www.domain>

Hi Thomas,

"random=~1" works if your data frame is in "groupedData" format, check 
this:

# Orthodont is in groupedData format
fm1 <- lme(distance~age+Sex, data=Orthodont, random=~1)
#####
dat <- as.data.frame(Orthodont)
fm2.1 <- lme(distance~age+Sex, data=dat, random=~1)

`dat' is an ordinary data.frame and thus random=~1 doesn't work. But 
this works:

fm2.2 <- lme(distance~age+Sex, data=dat, random=~1|Subject)
# you declare the grouping factor

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Thomas Petzoldt" <thpe at hhbio.wasser.tu-dresden.de>
To: "R-Help" <r-help at stat.math.ethz.ch>
Sent: Wednesday, January 05, 2005 11:11 AM
Subject: [R] lme: error message with random=~1


> Hello,
>
> I have an unbalanced mixed model design with two fixed effects
> "site" (2 levels) and "timeOfDay" (4 levels) and two random effects
> "day" (3 consecutive days) and "trap" (6 unique traps, 3 per site).
>
> The dependent variable is the body length ("BL") of insect larvae 
> from 7 to 29 individuals per trap (104 individuals in total).
>
> To account for pseudo replication I used nlme (or lme4 as suggested 
> for crossed random factors). The results indicate, that the random 
> effects are very small, so I followed the lme example and tried to 
> fit a model with random=~1.
>
> Unfortunately I got the following error message:
>
> Error in getGroups.data.frame(dataMix, groups) :
> Invalid formula for groups
>
> I suppose, that it would be redundant (and confusing to the reader) 
> if we leave unnecessary random effects in the model, but due to 
> pseudo replication it may be an offense if we simply ignore it and 
> use lm.
>
> Reading the respective chapters in Pinheiro & Bates, Venables & 
> Ripley and Crawley several times, I found no example for this 
> situation. Is there a common way how to handle this?
>
> Thomas P.
>
> PS: I can provide a full example, if necessary.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From gregor.gorjanc at bfro.uni-lj.si  Wed Jan  5 17:56:30 2005
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Wed, 05 Jan 2005 17:56:30 +0100
Subject: [R] Converting integers to chars i.e 1 to "01"
In-Reply-To: <41DBBEF1.60403@hhbio.wasser.tu-dresden.de>
References: <41DC051C.1060805@bfro.uni-lj.si>
	<41DBBEF1.60403@hhbio.wasser.tu-dresden.de>
Message-ID: <41DC1C3E.1060404@bfro.uni-lj.si>

Thanks to all for valuable suggestions!

-- 
Lep pozdrav / With regards,
     Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia



From ripley at stats.ox.ac.uk  Wed Jan  5 12:27:56 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Jan 2005 11:27:56 +0000 (GMT)
Subject: [R] make R package for windows on Linux
In-Reply-To: <41DBB4AD.6060103@vedur.is>
References: <41DBB4AD.6060103@vedur.is>
Message-ID: <Pine.LNX.4.61.0501051111510.14093@gannet.stats>

On Wed, 5 Jan 2005, [ISO-8859-1] Halldor Bjrnsson wrote:

> I made a package on a linux box. All worked fine.
> The package contains only R code (no C). I then wanted to make a zip file so 
> that I could test the package on a windows machine.

Why?  R works the same on Windows and Linux.

> I have tried all the obvious ways to do this (and even some that are 
> not!), but to no avail. The only instructions I find about building 
> packages for windows (e.g. mypkg.zip) seem to imply that the build be 
> done on a windows machine.

You've not noticed the section on cross-building: it _is_ documented
in README.packages (in the Windows installation, and in the sources).

> Is there a simple way to make mypkg.zip under linux and then install it as a 
> zip file on a windows machine?

The following *may* work (it _is_ the obvious way so you have probably 
tried it?).

On the Linux (or any other OS) box

cd R_HOME/library
zip -r9X mypkg mypkg

Send mypkg.zip to the Windows box.

On the Windows box
unzip mypkg.zip -d /path/to/R/library

I've just tried 'urn' and it worked for me.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Wed Jan  5 12:37:42 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Jan 2005 11:37:42 +0000 (GMT)
Subject: [R] warnings and errors with R CMD INSTALL
In-Reply-To: <1104916665.6537.18.camel@localhost.localdomain>
References: <1104878076.7506.28.camel@localhost.localdomain>
	<Pine.LNX.4.61.0501042324480.30382@gannet.stats>
	<1104883163.7506.41.camel@localhost.localdomain>
	<Pine.LNX.4.61.0501050643270.2502@gannet.stats>
	<1104916665.6537.18.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.61.0501051129010.14093@gannet.stats>

On Wed, 5 Jan 2005, thomas wrote:

[...]

> Finally, it seems to me that the install.package should help to stay
> tuned with regards to required dependencies.
> Just found out, that some dependency descriptions of packages are
> somewhat missing some necessities,

I don't believe you have read the actual file:

Package: RODBC
Version: 1.1-2
...
SystemRequirements: An ODBC driver manager and drivers. See README.
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

> i.e. RODBC package descriptions mentions:
> (..)
> R (>= 1.9.0)
> (..)
>
> When trying to install RODBC I had to learn that on my system there is:
>
> configure: error: "no ODBC driver manager found"
> ERROR: configuration failed for package 'RODBC'
>
> So, R CMD INSTALL seemingly doesn't take care of missing dependencies
> except given an error message.

It's a _system_ requirement, and R does not know how to install systems on 
your unstated system (and probably does not have permission too, either).

> Since I am relatively new to R I am kind of lost to figure out these
> dependencies on my own (think its getting better gradually).

You are not asked to, but you are asked to read the README.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From r.hankin at soc.soton.ac.uk  Wed Jan  5 13:18:24 2005
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Wed, 5 Jan 2005 12:18:24 +0000
Subject: [R] output from table() in matrix form
Message-ID: <E5CED5BD-5F13-11D9-946F-000A95D86AA8@soc.soton.ac.uk>

Hi

How do I get the output from table() in matrix form?

If I have

R>  table(c(1,1,1,1,2,20))

  1  2 20
  4  1  1

I want

      [,1] [,2] [,3]
[1,]    1    2   20
[2,]    4    1    1


The problem is that names(table) is a vector of characters and I need 
the numeric values.

I am using

R>  rbind(as.integer(names(x)),x)




I thought tabulate() might be better as it takes an integer-valued 
vector, but it isn't
quite right because the default bins are 1:20 and  I don't want the 
zeroes.

The following is a little clunky:

R> x <- rbind(1:20,tabulate(c(1,1,1,1,2,20)))
R> x[,x[2,]>0]
      [,1] [,2] [,3]
[1,]    1    2   20
[2,]    4    1    1


Is there a better way?   It seems inelegant to coerce a character 
vector back to integers,
but OTOH  it's wasteful to have 20 bins when I only need 3.  My real 
application would have
maybe a dozen distinct (prime) integers in the range 2 up to about  1e4.





--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From thpe at hhbio.wasser.tu-dresden.de  Wed Jan  5 13:31:51 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Wed, 05 Jan 2005 13:31:51 +0100
Subject: [R] lme: error message with random=~1
In-Reply-To: <002c01c4f313$6fa55190$0540210a@www.domain>
References: <41DBBD5D.4030808@hhbio.wasser.tu-dresden.de>
	<002c01c4f313$6fa55190$0540210a@www.domain>
Message-ID: <41DBDE37.3020506@hhbio.wasser.tu-dresden.de>

Dimitris Rizopoulos wrote:
> Hi Thomas,
> 
> "random=~1" works if your data frame is in "groupedData" format, check 
> this:
> 
> # Orthodont is in groupedData format
> fm1 <- lme(distance~age+Sex, data=Orthodont, random=~1)
> #####
> dat <- as.data.frame(Orthodont)
> fm2.1 <- lme(distance~age+Sex, data=dat, random=~1)
> 
> `dat' is an ordinary data.frame and thus random=~1 doesn't work. But 
> this works:
> 
> fm2.2 <- lme(distance~age+Sex, data=dat, random=~1|Subject)
> # you declare the grouping factor
> 
> I hope it helps.
> 
> Best,
> Dimitris

Thank you very much, now I see, why the Orthodont example works. There 
is however an important difference. In the example the random effects 
structure is not only ~1 but in reality ~1|Subject, which is inherited 
from the groupedData object. So

ranef(fm1)

yields 27 intercepts, but what I want is only one single intercept.

Thomas



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Jan  5 13:35:15 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 5 Jan 2005 13:35:15 +0100
Subject: [R] output from table() in matrix form
References: <E5CED5BD-5F13-11D9-946F-000A95D86AA8@soc.soton.ac.uk>
Message-ID: <000e01c4f323$02332e10$0540210a@www.domain>

Hi Robin,

does this help:

x <- table(c(1,1,1,1,2,20))
matrix(c(as.numeric(names(x)), x), ncol=length(x), byrow=TRUE, 
dimnames=NULL)

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Robin Hankin" <r.hankin at soc.soton.ac.uk>
To: <R-help at stat.math.ethz.ch>
Sent: Wednesday, January 05, 2005 1:18 PM
Subject: [R] output from table() in matrix form


> Hi
>
> How do I get the output from table() in matrix form?
>
> If I have
>
> R>  table(c(1,1,1,1,2,20))
>
>  1  2 20
>  4  1  1
>
> I want
>
>      [,1] [,2] [,3]
> [1,]    1    2   20
> [2,]    4    1    1
>
>
> The problem is that names(table) is a vector of characters and I 
> need the numeric values.
>
> I am using
>
> R>  rbind(as.integer(names(x)),x)
>
>
>
>
> I thought tabulate() might be better as it takes an integer-valued 
> vector, but it isn't
> quite right because the default bins are 1:20 and  I don't want the 
> zeroes.
>
> The following is a little clunky:
>
> R> x <- rbind(1:20,tabulate(c(1,1,1,1,2,20)))
> R> x[,x[2,]>0]
>      [,1] [,2] [,3]
> [1,]    1    2   20
> [2,]    4    1    1
>
>
> Is there a better way?   It seems inelegant to coerce a character 
> vector back to integers,
> but OTOH  it's wasteful to have 20 bins when I only need 3.  My real 
> application would have
> maybe a dozen distinct (prime) integers in the range 2 up to about 
> 1e4.
>
>
>
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> Southampton Oceanography Centre
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From petr.pikal at precheza.cz  Wed Jan  5 13:39:36 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 05 Jan 2005 13:39:36 +0100
Subject: [R] output from table() in matrix form
In-Reply-To: <E5CED5BD-5F13-11D9-946F-000A95D86AA8@soc.soton.ac.uk>
Message-ID: <41DBEE18.17803.1390D2C@localhost>



From bates at stat.wisc.edu  Wed Jan  5 13:41:07 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 05 Jan 2005 06:41:07 -0600
Subject: [R] lme: error message with random=~1
In-Reply-To: <41DBBD5D.4030808@hhbio.wasser.tu-dresden.de>
References: <41DBBD5D.4030808@hhbio.wasser.tu-dresden.de>
Message-ID: <41DBE063.5080408@stat.wisc.edu>

Thomas Petzoldt wrote:
> Hello,
> 
> I have an unbalanced mixed model design with two fixed effects
> "site" (2 levels) and "timeOfDay" (4 levels) and two random effects
> "day" (3 consecutive days) and "trap" (6 unique traps, 3 per site).
> 
> The dependent variable is the body length ("BL") of insect larvae from 7 
> to 29 individuals per trap (104 individuals in total).
> 
> To account for pseudo replication I used nlme (or lme4 as suggested for 
> crossed random factors). The results indicate, that the random effects 
> are very small, so I followed the lme example and tried to fit a model 
> with random=~1.
> 
> Unfortunately I got the following error message:
> 
> Error in getGroups.data.frame(dataMix, groups) :
>     Invalid formula for groups
> 
> I suppose, that it would be redundant (and confusing to the reader) if 
> we leave unnecessary random effects in the model, but due to pseudo 
> replication it may be an offense if we simply ignore it and use lm.
> 
> Reading the respective chapters in Pinheiro & Bates, Venables & Ripley 
> and Crawley several times, I found no example for this situation. Is 
> there a common way how to handle this?
> 
> Thomas P.
> 
> PS: I can provide a full example, if necessary.

I'm not sure what model you want to fit here.  To specify a random 
effect in lme you need both a grouping factor and a model matrix.  The 
error message indicates that lme is unable to determine a grouping 
factor.  It would be correct syntax if you added a single level factor 
to the data frame and used that but then the model fit would fail 
because you would be trying to estimate a variance in a model where 
there is no variation in the term.

It seems to me that you are trying to estimate parameters in a 
mixed-effects model without any random effects and lme can't do that.



From bxc at steno.dk  Wed Jan  5 13:42:34 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Wed, 5 Jan 2005 13:42:34 +0100
Subject: [R] output from table() in matrix form
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FE342@exdkba022.novo.dk>

You probably want something like:

> t1 <- table( x )
> t1
x
 1  2 20 
 3  2  1 
> t2 <- rbind( as.numeric( names( t1 ) ), t1 )
> t2
   1 2 20
   1 2 20
t1 3 2  1
> dimnames( t2 ) <- NULL
> t2
     [,1] [,2] [,3]
[1,]    1    2   20
[2,]    3    2    1
> 

Bendix 
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robin Hankin
> Sent: Wednesday, January 05, 2005 1:18 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] output from table() in matrix form
> 
> 
> Hi
> 
> How do I get the output from table() in matrix form?
> 
> If I have
> 
> R>  table(c(1,1,1,1,2,20))
> 
>   1  2 20
>   4  1  1
> 
> I want
> 
>       [,1] [,2] [,3]
> [1,]    1    2   20
> [2,]    4    1    1
> 
> 
> The problem is that names(table) is a vector of characters and I need 
> the numeric values.
> 
> I am using
> 
> R>  rbind(as.integer(names(x)),x)
> 
> 
> 
> 
> I thought tabulate() might be better as it takes an integer-valued 
> vector, but it isn't
> quite right because the default bins are 1:20 and  I don't want the 
> zeroes.
> 
> The following is a little clunky:
> 
> R> x <- rbind(1:20,tabulate(c(1,1,1,1,2,20)))
> R> x[,x[2,]>0]
>       [,1] [,2] [,3]
> [1,]    1    2   20
> [2,]    4    1    1
> 
> 
> Is there a better way?   It seems inelegant to coerce a character 
> vector back to integers,
> but OTOH  it's wasteful to have 20 bins when I only need 3.  My real 
> application would have
> maybe a dozen distinct (prime) integers in the range 2 up to 
> about  1e4.
> 
> 
> 
> 
> 
> --
> Robin Hankin
> Uncertainty Analyst
> Southampton Oceanography Centre
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Jan  5 13:43:05 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 5 Jan 2005 13:43:05 +0100
Subject: [R] lme: error message with random=~1
References: <41DBBD5D.4030808@hhbio.wasser.tu-dresden.de>
	<002c01c4f313$6fa55190$0540210a@www.domain>
	<41DBDE37.3020506@hhbio.wasser.tu-dresden.de>
Message-ID: <002301c4f324$1a529b10$0540210a@www.domain>

Hi Thomas,

I don't quite understand what you want to do. If you use "random=~1" 
(or "random=~1|Subject" if you don't have a groupedData), then you 
just fit a random-intercepts model. "ranef(fm1)" gives you the 
Empirical Bayes estimates (i.e., posterior means) for the 
random-effects which you can use for instance, if you'd like 
subject-specific fitted values.

Best,
Dimitris


----- Original Message ----- 
From: "Thomas Petzoldt" <thpe at hhbio.wasser.tu-dresden.de>
To: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.ac.be>
Cc: <petzoldt at rcs.urz.tu-dresden.de>; <r-help at stat.math.ethz.ch>
Sent: Wednesday, January 05, 2005 1:31 PM
Subject: Re: [R] lme: error message with random=~1


> Dimitris Rizopoulos wrote:
>> Hi Thomas,
>>
>> "random=~1" works if your data frame is in "groupedData" format, 
>> check this:
>>
>> # Orthodont is in groupedData format
>> fm1 <- lme(distance~age+Sex, data=Orthodont, random=~1)
>> #####
>> dat <- as.data.frame(Orthodont)
>> fm2.1 <- lme(distance~age+Sex, data=dat, random=~1)
>>
>> `dat' is an ordinary data.frame and thus random=~1 doesn't work. 
>> But this works:
>>
>> fm2.2 <- lme(distance~age+Sex, data=dat, random=~1|Subject)
>> # you declare the grouping factor
>>
>> I hope it helps.
>>
>> Best,
>> Dimitris
>
> Thank you very much, now I see, why the Orthodont example works. 
> There is however an important difference. In the example the random 
> effects structure is not only ~1 but in reality ~1|Subject, which is 
> inherited from the groupedData object. So
>
> ranef(fm1)
>
> yields 27 intercepts, but what I want is only one single intercept.
>
> Thomas
>



From ripley at stats.ox.ac.uk  Wed Jan  5 13:59:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Jan 2005 12:59:03 +0000 (GMT)
Subject: [R] output from table() in matrix form
In-Reply-To: <E5CED5BD-5F13-11D9-946F-000A95D86AA8@soc.soton.ac.uk>
References: <E5CED5BD-5F13-11D9-946F-000A95D86AA8@soc.soton.ac.uk>
Message-ID: <Pine.LNX.4.61.0501051255001.15290@gannet.stats>

table works the way it does because it applies to *factors*, so the names 
are the factor levels of the argument after conversion.  So if anything is 
wasteful, that is.

How about using the guts of factor and table, via

xx <- unique(x)
rbind(vals=xx, cnts=tabulate(match(x, xx)))

?

On Wed, 5 Jan 2005, Robin Hankin wrote:

> Hi
>
> How do I get the output from table() in matrix form?
>
> If I have
>
> R>  table(c(1,1,1,1,2,20))
>
> 1  2 20
> 4  1  1
>
> I want
>
>     [,1] [,2] [,3]
> [1,]    1    2   20
> [2,]    4    1    1
>
>
> The problem is that names(table) is a vector of characters and I need the 
> numeric values.
>
> I am using
>
> R>  rbind(as.integer(names(x)),x)
>
>
>
>
> I thought tabulate() might be better as it takes an integer-valued vector, 
> but it isn't
> quite right because the default bins are 1:20 and  I don't want the zeroes.
>
> The following is a little clunky:
>
> R> x <- rbind(1:20,tabulate(c(1,1,1,1,2,20)))
> R> x[,x[2,]>0]
>     [,1] [,2] [,3]
> [1,]    1    2   20
> [2,]    4    1    1
>
>
> Is there a better way?   It seems inelegant to coerce a character vector back 
> to integers,
> but OTOH  it's wasteful to have 20 bins when I only need 3.  My real 
> application would have
> maybe a dozen distinct (prime) integers in the range 2 up to about  1e4.
>
>
>
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> Southampton Oceanography Centre
> European Way, Southampton SO14 3ZH, UK
> tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From blindglobe at gmail.com  Wed Jan  5 14:10:29 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Wed, 5 Jan 2005 14:10:29 +0100
Subject: [R] lme: error message with random=~1
In-Reply-To: <41DBDE37.3020506@hhbio.wasser.tu-dresden.de>
References: <41DBBD5D.4030808@hhbio.wasser.tu-dresden.de>
	<002c01c4f313$6fa55190$0540210a@www.domain>
	<41DBDE37.3020506@hhbio.wasser.tu-dresden.de>
Message-ID: <1abe3fa90501050510647d1bf4@mail.gmail.com>

On Wed, 05 Jan 2005 13:31:51 +0100, Thomas Petzoldt
<thpe at hhbio.wasser.tu-dresden.de> wrote:
> Dimitris Rizopoulos wrote:
> > Hi Thomas,
> >
> > "random=~1" works if your data frame is in "groupedData" format, check
> > this:
> >
> > # Orthodont is in groupedData format
> > fm1 <- lme(distance~age+Sex, data=Orthodont, random=~1)
> > #####
> > dat <- as.data.frame(Orthodont)
> > fm2.1 <- lme(distance~age+Sex, data=dat, random=~1)
> >
> > `dat' is an ordinary data.frame and thus random=~1 doesn't work. But
> > this works:
> >
> > fm2.2 <- lme(distance~age+Sex, data=dat, random=~1|Subject)
> > # you declare the grouping factor
> >
> > I hope it helps.
> >
> > Best,
> > Dimitris
> 
> Thank you very much, now I see, why the Orthodont example works. There
> is however an important difference. In the example the random effects
> structure is not only ~1 but in reality ~1|Subject, which is inherited
> from the groupedData object. So
> 
> ranef(fm1)
> 
> yields 27 intercepts, but what I want is only one single intercept.

Then you don't want a random intercept, right?    Those are fitted
estimates, if I'm not mistaken.


best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From r.hankin at soc.soton.ac.uk  Wed Jan  5 14:19:13 2005
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Wed, 5 Jan 2005 13:19:13 +0000
Subject: [R] output from table() in matrix form
In-Reply-To: <Pine.LNX.4.61.0501051255001.15290@gannet.stats>
References: <E5CED5BD-5F13-11D9-946F-000A95D86AA8@soc.soton.ac.uk>
	<Pine.LNX.4.61.0501051255001.15290@gannet.stats>
Message-ID: <647DFF3C-5F1C-11D9-946F-000A95D86AA8@soc.soton.ac.uk>


On Jan 5, 2005, at 12:59 pm, Prof Brian Ripley wrote:

> table works the way it does because it applies to *factors*, so the 
> names are the factor levels of the argument after conversion.  So if 
> anything is wasteful, that is.
>
> How about using the guts of factor and table, via
>
> xx <- unique(x)
> rbind(vals=xx, cnts=tabulate(match(x, xx)))
>
> ?
>



yes!  this is just what I needed.   For me, it's good for another 
reason too: this
method does not suffer if x contains a single enormous value (common
in my application).

I found this very instructive.  Could we add this concatenation
of tabulate() with match() to one or both manpages?





> On Wed, 5 Jan 2005, Robin Hankin wrote:
>
>> Hi
>>
>> How do I get the output from table() in matrix form?
>>
>> If I have
>>
>> R>  table(c(1,1,1,1,2,20))
>>
>> 1  2 20
>> 4  1  1
>>
>> I want
>>
>>     [,1] [,2] [,3]
>> [1,]    1    2   20
>> [2,]    4    1    1
--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From janus at ices.dk  Wed Jan  5 14:27:37 2005
From: janus at ices.dk (Janus Larsen)
Date: Wed, 5 Jan 2005 14:27:37 +0100
Subject: [R] hist.POSIXt filled bars - axis color changes
Message-ID: <ADD1C7333B42E4459D6E3363FE04A74B733727@coral.ices.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050105/829af51b/attachment.pl

From MSchwartz at MedAnalytics.com  Wed Jan  5 14:28:35 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 05 Jan 2005 07:28:35 -0600
Subject: [R] make R package for windows on Linux
In-Reply-To: <41DBB4AD.6060103@vedur.is>
References: <41DBB4AD.6060103@vedur.is>
Message-ID: <1104931715.9167.12.camel@horizons.localdomain>

On Wed, 2005-01-05 at 09:34 +0000, Halldor Bj?rnsson wrote:
> Hi,
> I made a package on a linux box. All worked fine.
> The package contains only R code (no C). I then wanted to make a zip 
> file so that I could test the package on a windows machine. I have tried 
> all the obvious ways to do this (and even some that are not!), but to no 
> avail. The only  instructions I find about building packages for windows 
> (e.g. mypkg.zip) seem to imply that the build be done on a windows machine.
> 
> Is there a simple way to make mypkg.zip under linux and then install it 
> as a zip file on a windows machine?
> 
> Sincerely,
> Halldor


I have not used it, but there is an article in the Contributed
Documentation section on the main R web site by Jun Yan and Tony Rossini
called "Building Microsoft Windows Versions of R and R packages under
Intel Linux".

A direct link to the PDF is:

http://cran.r-project.org/doc/contrib/cross-build.pdf

and there is a makefile at:

http://cran.r-project.org/doc/contrib/Makefile-rcb

HTH,

Marc Schwartz



From michael.watson at bbsrc.ac.uk  Wed Jan  5 15:26:05 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 5 Jan 2005 14:26:05 -0000
Subject: [R] Replacing all NA values in a matrix
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E89A9B@iahce2knas1.iah.bbsrc.reserved>

OK, dumb question, and it is probably in the docs somewhere, but after
12 months working with R and quite a while looking at the docs, I still
don't know (or have forgotten) how to replace all NA values in a matrix
at once with some other value.  I can do it column by column using
is.na(), but I can't figure out how to do it for the whole matrix.  My
apologies, I am ashamed ;-)

Michael Watson
Head of Informatics
Institute for Animal Health,
Compton Laboratory,
Compton,
Newbury,
Berkshire RG20 7NN
UK

Phone : +44 (0)1635 578411 ext. 2535
Mobile: +44 (0)7990 827831
E-mail: michael.watson at bbsrc.ac.uk



From paulojus at est.ufpr.br  Wed Jan  5 15:48:19 2005
From: paulojus at est.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Wed, 5 Jan 2005 12:48:19 -0200 (BRST)
Subject: [R] make R package for windows on Linux
In-Reply-To: <41DBB4AD.6060103@vedur.is>
References: <41DBB4AD.6060103@vedur.is>
Message-ID: <Pine.LNX.4.58L0.0501051243170.9846@est.ufpr.br>

Hi

I do it for the geoR package following the excelent and detailed
instructions provided by Yan and Rossini:

"Building Microsoft Windows Versions of R and R packages under Intel
Linux"

available at the Contributed Documentation in the R web-site

Notice that not having C/Fortran/C++ code make the process even simpler.

P.J.

On Wed, 5 Jan 2005, Halldor Bj?rnsson wrote:

> Hi,
> I made a package on a linux box. All worked fine.
> The package contains only R code (no C). I then wanted to make a zip
> file so that I could test the package on a windows machine. I have tried
> all the obvious ways to do this (and even some that are not!), but to no
> avail. The only  instructions I find about building packages for windows
> (e.g. mypkg.zip) seem to imply that the build be done on a windows machine.
>
> Is there a simple way to make mypkg.zip under linux and then install it
> as a zip file on a windows machine?
>
> Sincerely,
> Halldor
> --
> ------------------------------------------
> Halldor Bjornsson   (halldor at vedur.is)
> Vedurstofa Islands (Icelandic Met. Office)
> Bustadavegur 9, IS-150, Reykjavik, Iceland
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

Paulo Justiniano Ribeiro Jr
LEG (Laborat?rio de Estat?stica e Geoinforma??o)
Departamento de Estat?stica
Universidade Federal do Paran?
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 361 3573
Fax: (+55) 41 361 3141
e-mail: paulojus at est.ufpr.br
http://www.est.ufpr.br/~paulojus



From sdavis2 at mail.nih.gov  Wed Jan  5 15:44:43 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 5 Jan 2005 09:44:43 -0500
Subject: [R] Replacing all NA values in a matrix
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E89A9B@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E95E89A9B@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <5613D194-5F28-11D9-AE31-000D933565E8@mail.nih.gov>

You have it already....

 > h <- matrix(rnorm(100),nrow=20)
 > h
             [,1]       [,2]        [,3]         [,4]       [,5]
  [1,]  0.4669801  0.3349176 -1.60686041  0.981491440  0.1627222
  [2,] -0.2580262 -0.2620413  0.53852801  1.294129626 -0.1632906
  [3,]  0.9654591  1.0077212 -0.45603772  1.845272884  0.2910091
  [4,]  1.0710281  1.6234312  0.19610087  0.524864299 -0.1197756
  [5,]  0.3727526 -1.0742054 -0.46485338 -0.128577874  0.5177477
  [6,]  0.7289514  1.9020074  0.34534408 -0.313550835  0.7026291
  [7,] -0.3037257  0.3922162 -1.77990093  0.596858216 -0.4039951
  [8,] -1.7857808  0.4271333 -1.32907071 -0.596656935  0.4008593
  [9,]  1.4643707 -0.4369587  0.93522859 -0.948929936  0.1416290
[10,]  0.1243959 -1.4509269 -0.39656577 -0.550951866  1.2189326
[11,]  0.7210016 -0.2337671  0.70075393 -1.034782089 -1.7652139
[12,]  0.5509319 -0.9731717  0.12392721  0.421338123 -1.3197952
[13,]  1.8718778 -0.2853116  0.69003178  0.939630649  0.7421644
[14,] -0.3897884 -1.4627226 -0.32424877  1.115026790 -0.3912558
[15,] -1.5784201  0.6987771 -0.29907714  0.816135639 -0.8182227
[16,] -0.6140077 -0.1025945  0.04281918  0.006010866  1.0701661
[17,] -1.0290960  0.1830102 -0.51057604 -1.034981163 -1.5717075
[18,] -0.0902371  1.6820050 -0.40896428 -1.560638110  1.1076107
[19,] -1.6744785  2.2675648  0.27397118  1.223144752 -0.9754583
[20,] -0.2265682  0.1512010  0.60412290  0.585478462  0.5247539
 > h[h<0] <- NA
 > h
            [,1]      [,2]       [,3]        [,4]      [,5]
  [1,] 0.4669801 0.3349176         NA 0.981491440 0.1627222
  [2,]        NA        NA 0.53852801 1.294129626        NA
  [3,] 0.9654591 1.0077212         NA 1.845272884 0.2910091
  [4,] 1.0710281 1.6234312 0.19610087 0.524864299        NA
  [5,] 0.3727526        NA         NA          NA 0.5177477
  [6,] 0.7289514 1.9020074 0.34534408          NA 0.7026291
  [7,]        NA 0.3922162         NA 0.596858216        NA
  [8,]        NA 0.4271333         NA          NA 0.4008593
  [9,] 1.4643707        NA 0.93522859          NA 0.1416290
[10,] 0.1243959        NA         NA          NA 1.2189326
[11,] 0.7210016        NA 0.70075393          NA        NA
[12,] 0.5509319        NA 0.12392721 0.421338123        NA
[13,] 1.8718778        NA 0.69003178 0.939630649 0.7421644
[14,]        NA        NA         NA 1.115026790        NA
[15,]        NA 0.6987771         NA 0.816135639        NA
[16,]        NA        NA 0.04281918 0.006010866 1.0701661
[17,]        NA 0.1830102         NA          NA        NA
[18,]        NA 1.6820050         NA          NA 1.1076107
[19,]        NA 2.2675648 0.27397118 1.223144752        NA
[20,]        NA 0.1512010 0.60412290 0.585478462 0.5247539
 > h[is.na(h)] <- 0
 > h
            [,1]      [,2]       [,3]        [,4]      [,5]
  [1,] 0.4669801 0.3349176 0.00000000 0.981491440 0.1627222
  [2,] 0.0000000 0.0000000 0.53852801 1.294129626 0.0000000
  [3,] 0.9654591 1.0077212 0.00000000 1.845272884 0.2910091
  [4,] 1.0710281 1.6234312 0.19610087 0.524864299 0.0000000
  [5,] 0.3727526 0.0000000 0.00000000 0.000000000 0.5177477
  [6,] 0.7289514 1.9020074 0.34534408 0.000000000 0.7026291
  [7,] 0.0000000 0.3922162 0.00000000 0.596858216 0.0000000
  [8,] 0.0000000 0.4271333 0.00000000 0.000000000 0.4008593
  [9,] 1.4643707 0.0000000 0.93522859 0.000000000 0.1416290
[10,] 0.1243959 0.0000000 0.00000000 0.000000000 1.2189326
[11,] 0.7210016 0.0000000 0.70075393 0.000000000 0.0000000
[12,] 0.5509319 0.0000000 0.12392721 0.421338123 0.0000000
[13,] 1.8718778 0.0000000 0.69003178 0.939630649 0.7421644
[14,] 0.0000000 0.0000000 0.00000000 1.115026790 0.0000000
[15,] 0.0000000 0.6987771 0.00000000 0.816135639 0.0000000
[16,] 0.0000000 0.0000000 0.04281918 0.006010866 1.0701661
[17,] 0.0000000 0.1830102 0.00000000 0.000000000 0.0000000
[18,] 0.0000000 1.6820050 0.00000000 0.000000000 1.1076107
[19,] 0.0000000 2.2675648 0.27397118 1.223144752 0.0000000
[20,] 0.0000000 0.1512010 0.60412290 0.585478462 0.5247539
 >

Sean

On Jan 5, 2005, at 9:26 AM, michael watson ((IAH-C)) wrote:

> OK, dumb question, and it is probably in the docs somewhere, but after
> 12 months working with R and quite a while looking at the docs, I still
> don't know (or have forgotten) how to replace all NA values in a matrix
> at once with some other value.  I can do it column by column using
> is.na(), but I can't figure out how to do it for the whole matrix.  My
> apologies, I am ashamed ;-)
>
> Michael Watson
> Head of Informatics
> Institute for Animal Health,
> Compton Laboratory,
> Compton,
> Newbury,
> Berkshire RG20 7NN
> UK
>
> Phone : +44 (0)1635 578411 ext. 2535
> Mobile: +44 (0)7990 827831
> E-mail: michael.watson at bbsrc.ac.uk
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Whit.Armstrong at tudor.com  Wed Jan  5 15:45:41 2005
From: Whit.Armstrong at tudor.com (Whit Armstrong)
Date: Wed, 5 Jan 2005 09:45:41 -0500 
Subject: [R] Replacing all NA values in a matrix
Message-ID: <7669F018DC9DD711AEC500065B3D5ABF042B3EB6@tudor.com>

x <- matrix(rnorm(100),ncol=10)
x[x>0] <- NA
x[is.na(x)] <- 1000

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> michael watson (IAH-C)
> Sent: Wednesday, January 05, 2005 9:26 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Replacing all NA values in a matrix
> 
> 
> OK, dumb question, and it is probably in the docs somewhere, 
> but after 12 months working with R and quite a while looking 
> at the docs, I still don't know (or have forgotten) how to 
> replace all NA values in a matrix at once with some other 
> value.  I can do it column by column using is.na(), but I 
> can't figure out how to do it for the whole matrix.  My 
> apologies, I am ashamed ;-)
> 
> Michael Watson
> Head of Informatics
> Institute for Animal Health,
> Compton Laboratory,
> Compton,
> Newbury,
> Berkshire RG20 7NN
> UK
> 
> Phone : +44 (0)1635 578411 ext. 2535
> Mobile: +44 (0)7990 827831
> E-mail: michael.watson at bbsrc.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From michael.watson at bbsrc.ac.uk  Wed Jan  5 15:44:20 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 5 Jan 2005 14:44:20 -0000
Subject: [R] Replacing all NA values in a matrix
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E89A9C@iahce2knas1.iah.bbsrc.reserved>

Doh, replace() does the job just fine.
Sheesh, I'm not coping well with work post christmas ;-)

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of michael watson
(IAH-C)
Sent: 05 January 2005 14:26
To: R-help at stat.math.ethz.ch
Subject: [R] Replacing all NA values in a matrix


OK, dumb question, and it is probably in the docs somewhere, but after
12 months working with R and quite a while looking at the docs, I still
don't know (or have forgotten) how to replace all NA values in a matrix
at once with some other value.  I can do it column by column using
is.na(), but I can't figure out how to do it for the whole matrix.  My
apologies, I am ashamed ;-)

Michael Watson
Head of Informatics
Institute for Animal Health,
Compton Laboratory,
Compton,
Newbury,
Berkshire RG20 7NN
UK

Phone : +44 (0)1635 578411 ext. 2535
Mobile: +44 (0)7990 827831
E-mail: michael.watson at bbsrc.ac.uk

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Wed Jan  5 15:52:12 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 5 Jan 2005 09:52:12 -0500
Subject: [R] make R package for windows on Linux
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4A0@usrymx25.merck.com>

> From: Marc Schwartz
> 
> On Wed, 2005-01-05 at 09:34 +0000, Halldor Bj?rnsson wrote:
> > Hi,
> > I made a package on a linux box. All worked fine.
> > The package contains only R code (no C). I then wanted to 
> make a zip 
> > file so that I could test the package on a windows machine. 
> I have tried 
> > all the obvious ways to do this (and even some that are 
> not!), but to no 
> > avail. The only  instructions I find about building 
> packages for windows 
> > (e.g. mypkg.zip) seem to imply that the build be done on a 
> windows machine.
> > 
> > Is there a simple way to make mypkg.zip under linux and 
> then install it 
> > as a zip file on a windows machine?
> > 
> > Sincerely,
> > Halldor
> 
> 
> I have not used it, but there is an article in the Contributed
> Documentation section on the main R web site by Jun Yan and 
> Tony Rossini
> called "Building Microsoft Windows Versions of R and R packages under
> Intel Linux".
> 
> A direct link to the PDF is:
> 
> http://cran.r-project.org/doc/contrib/cross-build.pdf
>
> and there is a makefile at:
>
> http://cran.r-project.org/doc/contrib/Makefile-rcb
>
> HTH,
>
> Marc Schwartz

If the package has no compiled code, the cross-compiling setup Marc
mentioned might be an overkill.  I just tried the following with a local
package:

On Linux (SLES8 for x86_64):

  R CMD INSTALL -l localRlib mypkg_x.x-x.tar.gz
  cd localRlib
  zip -r mypkg mypkg

Now ftp the file mypkg.zip to the Windows box.  On the Windows box:

  Start up Rgui
  Use the menu Packages / Install Package(s) from Local zip file(s)
  Select the file mypkg.zip

That seems to work fine for me, with R-2.0.1 on both end.

Best,
Andy



From Matthias.Templ at statistik.gv.at  Wed Jan  5 15:55:27 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Wed, 5 Jan 2005 15:55:27 +0100
Subject: AW: [R] Replacing all NA values in a matrix
Message-ID: <83536658864BC243BE3C06D7E936ABD501BE1B17@xchg1.statistik.local>

Replacing the NA?s with eg. 1 :

> a
   1   2  3   4
1 20  50 10  80
2 NA  19 NA  49
3 NA  32 NA  61
4 45 101 44 190


a[try(is.na(a)) == TRUE] <- 1

> a
   1   2  3   4
1 20  50 10  80
2  1  19  1  49
3  1  32  1  61
4 45 101 44 190

I this can help you,
Matthias


> -----Urspr?ngliche Nachricht-----
> Von: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] Im Auftrag von 
> michael watson (IAH-C)
> Gesendet: Mittwoch, 05. J?nner 2005 15:26
> An: R-help at stat.math.ethz.ch
> Betreff: [R] Replacing all NA values in a matrix
> 
> 
> OK, dumb question, and it is probably in the docs somewhere, 
> but after 12 months working with R and quite a while looking 
> at the docs, I still don't know (or have forgotten) how to 
> replace all NA values in a matrix at once with some other 
> value.  I can do it column by column using is.na(), but I 
> can't figure out how to do it for the whole matrix.  My 
> apologies, I am ashamed ;-)
> 
> Michael Watson
> Head of Informatics
> Institute for Animal Health,
> Compton Laboratory,
> Compton,
> Newbury,
> Berkshire RG20 7NN
> UK
> 
> Phone : +44 (0)1635 578411 ext. 2535
> Mobile: +44 (0)7990 827831
> E-mail: michael.watson at bbsrc.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Wed Jan  5 15:56:53 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 5 Jan 2005 09:56:53 -0500
Subject: [R] Replacing all NA values in a matrix
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4A1@usrymx25.merck.com>

What you should keep in mind is that a matrix in R is nothing more than a
vector (formed by stacking the columns of the matrix) with the dim
attribute.  Thus you can do what you want to do by treating the matrix as a
vector; e.g.,

mymat[is.na(mymat)] <- myFavoriteValue

HTH,
Andy


> From: michael watson (IAH-C)
> 
> OK, dumb question, and it is probably in the docs somewhere, but after
> 12 months working with R and quite a while looking at the 
> docs, I still
> don't know (or have forgotten) how to replace all NA values 
> in a matrix
> at once with some other value.  I can do it column by column using
> is.na(), but I can't figure out how to do it for the whole matrix.  My
> apologies, I am ashamed ;-)
> 
> Michael Watson
> Head of Informatics
> Institute for Animal Health,
> Compton Laboratory,
> Compton,
> Newbury,
> Berkshire RG20 7NN
> UK
> 
> Phone : +44 (0)1635 578411 ext. 2535
> Mobile: +44 (0)7990 827831
> E-mail: michael.watson at bbsrc.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Jan  5 15:58:05 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 5 Jan 2005 15:58:05 +0100
Subject: [R] Replacing all NA values in a matrix
References: <8975119BCD0AC5419D61A9CF1A923E95E89A9B@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <00a201c4f336$f64a3b20$0540210a@www.domain>

Hi Michael,

try this:

mat <- matrix(1:25, 5, 5)
mat[sample(25, 10)] <- NA
mat
#####
mat[is.na(mat)] <- 100
mat

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk>
To: <R-help at stat.math.ethz.ch>
Sent: Wednesday, January 05, 2005 3:26 PM
Subject: [R] Replacing all NA values in a matrix


> OK, dumb question, and it is probably in the docs somewhere, but 
> after
> 12 months working with R and quite a while looking at the docs, I 
> still
> don't know (or have forgotten) how to replace all NA values in a 
> matrix
> at once with some other value.  I can do it column by column using
> is.na(), but I can't figure out how to do it for the whole matrix. 
> My
> apologies, I am ashamed ;-)
>
> Michael Watson
> Head of Informatics
> Institute for Animal Health,
> Compton Laboratory,
> Compton,
> Newbury,
> Berkshire RG20 7NN
> UK
>
> Phone : +44 (0)1635 578411 ext. 2535
> Mobile: +44 (0)7990 827831
> E-mail: michael.watson at bbsrc.ac.uk
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From hb at maths.lth.se  Wed Jan  5 16:00:40 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Wed, 5 Jan 2005 16:00:40 +0100
Subject: [R] integer factorization
In-Reply-To: <16859.48586.242334.33187@stat.math.ethz.ch>
Message-ID: <003201c4f337$5264dff0$e502eb82@hblaptop>

See also primeFactors() and allFactors():

 http://www.maths.lth.se/help/R/.R/library/R.basic/html/primeFactors.html
 http://www.maths.lth.se/help/R/.R/library/R.basic/html/allFactors.html

in the R.basic package part of the R.classes bundle:

 http://www.maths.lth.se/help/R/R.classes/

Cheers

Henrik Bengtsson

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Christoph Buser
> Sent: Wednesday, January 05, 2005 11:14 AM
> To: Robin Hankin
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] integer factorization
> 
> 
> Hi Robin
> 
> There is a function factorize() in the package conf.design
> 
> library(conf.design)
> factorize(60)
> [1] 2 2 3 5
> 
> Hope this helps
> Christoph
> 
> -- 
> Christoph Buser <buser at stat.math.ethz.ch>
> Seminar fuer Statistik, LEO C11
> ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
> phone: x-41-1-632-5414		fax: 632-1228
> http://stat.ethz.ch/~buser/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ligges at statistik.uni-dortmund.de  Wed Jan  5 16:04:56 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 05 Jan 2005 16:04:56 +0100
Subject: [R] Replacing all NA values in a matrix
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E89A9B@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E95E89A9B@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <41DC0218.4060306@statistik.uni-dortmund.de>

michael watson (IAH-C) wrote:

> OK, dumb question, and it is probably in the docs somewhere, but after
> 12 months working with R and quite a while looking at the docs, I still
> don't know (or have forgotten) how to replace all NA values in a matrix
> at once with some other value.  I can do it column by column using
> is.na(), but I can't figure out how to do it for the whole matrix.  My
> apologies, I am ashamed ;-)

Since you can index a matrix like a vector (it's a vector with a dim 
attribute), you can simply say something like:

  X[is.na(X)] <- 99999

Uwe Ligges



> Michael Watson
> Head of Informatics
> Institute for Animal Health,
> Compton Laboratory,
> Compton,
> Newbury,
> Berkshire RG20 7NN
> UK
> 
> Phone : +44 (0)1635 578411 ext. 2535
> Mobile: +44 (0)7990 827831
> E-mail: michael.watson at bbsrc.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Wed Jan  5 16:08:20 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 5 Jan 2005 10:08:20 -0500
Subject: [R] Replacing all NA values in a matrix
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E89A9B@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <20050105150820.EPLR19622.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Michael,

A[is.na(A)] <- value

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> michael watson (IAH-C)
> Sent: Wednesday, January 05, 2005 9:26 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Replacing all NA values in a matrix
> 
> OK, dumb question, and it is probably in the docs somewhere, but after
> 12 months working with R and quite a while looking at the 
> docs, I still don't know (or have forgotten) how to replace 
> all NA values in a matrix at once with some other value.  I 
> can do it column by column using is.na(), but I can't figure 
> out how to do it for the whole matrix.  My apologies, I am ashamed ;-)
> 
> Michael Watson



From ligges at statistik.uni-dortmund.de  Wed Jan  5 16:14:32 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 05 Jan 2005 16:14:32 +0100
Subject: [R] hist.POSIXt filled bars - axis color changes
In-Reply-To: <ADD1C7333B42E4459D6E3363FE04A74B733727@coral.ices.local>
References: <ADD1C7333B42E4459D6E3363FE04A74B733727@coral.ices.local>
Message-ID: <41DC0458.6080103@statistik.uni-dortmund.de>

Janus Larsen wrote:

> Hi all,
>  
> I'm plotting a histogram of dates and would like to shade the bars, e.g.
> hist(.leap.seconds,"years",col='gray',freq=T)
> -but the axis color also changes, how do I prevent that?


Looks like that's not very easy with the current implementation, see the 
code in graphics:::hist.POSIXt
Either write your own function along the code mentioned above (and 
consider to contribute your improvements, or use the following (more or 
less ugly) workaround:

   hist(.leap.seconds, "years", col='gray', freq=TRUE, axes=FALSE)
   hist(.leap.seconds, "years", freq=TRUE, add=TRUE)

Uwe Ligges


> thx in advance
> Janus
>  
>  
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From matthew_wiener at merck.com  Wed Jan  5 16:15:55 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Wed, 5 Jan 2005 10:15:55 -0500
Subject: [R] Replacing all NA values in a matrix
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E04993D19@uswsmx03.merck.com>

Michael --

is.na works on the full matrix.  The commands below construct a matrix,
insert some NA's, and then convert them all to 0. 

> temp1 <- matrix(runif(25), 5, 5)
> temp1[temp1 < 0.1] <- NA
> temp1[is.na(temp1)] <- 0

Hope this helps.

Regards,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of michael watson
(IAH-C)
Sent: Wednesday, January 05, 2005 9:26 AM
To: R-help at stat.math.ethz.ch
Subject: [R] Replacing all NA values in a matrix


OK, dumb question, and it is probably in the docs somewhere, but after
12 months working with R and quite a while looking at the docs, I still
don't know (or have forgotten) how to replace all NA values in a matrix
at once with some other value.  I can do it column by column using
is.na(), but I can't figure out how to do it for the whole matrix.  My
apologies, I am ashamed ;-)

Michael Watson
Head of Informatics
Institute for Animal Health,
Compton Laboratory,
Compton,
Newbury,
Berkshire RG20 7NN
UK

Phone : +44 (0)1635 578411 ext. 2535
Mobile: +44 (0)7990 827831
E-mail: michael.watson at bbsrc.ac.uk

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From thpe at hhbio.wasser.tu-dresden.de  Wed Jan  5 16:29:59 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Wed, 05 Jan 2005 16:29:59 +0100
Subject: [R] lme: error message with random=~1
In-Reply-To: <41DBE063.5080408@stat.wisc.edu>
References: <41DBBD5D.4030808@hhbio.wasser.tu-dresden.de>
	<41DBE063.5080408@stat.wisc.edu>
Message-ID: <41DC07F7.6040900@hhbio.wasser.tu-dresden.de>

Douglas Bates wrote:
> 
> I'm not sure what model you want to fit here.  To specify a random 
> effect in lme you need both a grouping factor and a model matrix.  The 
> error message indicates that lme is unable to determine a grouping 
> factor.  It would be correct syntax if you added a single level factor 
> to the data frame and used that but then the model fit would fail 
> because you would be trying to estimate a variance in a model where 
> there is no variation in the term.

O.k. I see and think I understand it.

> It seems to me that you are trying to estimate parameters in a 
> mixed-effects model without any random effects and lme can't do that.
> 

Yes, what I want is a model without any random effects to be tested 
against a model with random effects. I want to show, that the random 
effects are negligible but that we account for pseudo replicates and 
have tested this explicitely.

I'm not sure what is better: to leave the random effects in the model or 
  simply an LR test against a linear model fitted by lm. I've never seen 
such an example in the books. Or have I missed a global alternative here?

Thomas P.



From fren2 at yahoo.com  Wed Jan  5 17:07:34 2005
From: fren2 at yahoo.com (Frederic renaud)
Date: Wed, 5 Jan 2005 08:07:34 -0800 (PST)
Subject: [R] count element in column
Message-ID: <20050105160734.3842.qmail@web51808.mail.yahoo.com>

Hi,
I 've a matrix n*1 (thus a column) and I would like to
count the number of negative element inside.
Can you help me?
Thanks!

eg:
 res[,1]= 1
          -3
          -1

How obtain the number 2 (number of negative-element)?



From bill.shipley at usherbrooke.ca  Wed Jan  5 17:09:19 2005
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Wed, 5 Jan 2005 11:09:19 -0500
Subject: [R] variance of combinations of means - off topic
Message-ID: <001401c4f340$e9b7cd50$ae1ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050105/58dead33/attachment.pl

From a.beckerman at sheffield.ac.uk  Wed Jan  5 17:33:42 2005
From: a.beckerman at sheffield.ac.uk (Andrew Beckerman)
Date: Wed, 5 Jan 2005 16:33:42 +0000
Subject: [R] lme, glmmPQL, multiple random effects
Message-ID: <8FDF5B2B-5F37-11D9-B488-000A95CD7F02@sheffield.ac.uk>

Hi all -
R2.0.1, OS X

Perhaps while there is some discussion of lme going on.....

I am trying to execute a glmm using glmmPQL from the MASS libray, using  
the example data set from McCullagh and Nelder's (1989, p442) table  
14.4 (it happens to be the glmm example for GENSTAT as well).  The data  
are binary, representing mating success (1,0) for crosses between males  
and females from two populations of salamanders.  The idea is to fit a  
fixed effect of Cross, and estimate random effects for females and  
males. (data available as an *.rda for anyone who wants it...)

Following the advice from various postings from R-help and from Pinhero  
and Bates,  I can successfully (I think) code an lme() model, using  
pdBlocked, pdIdent and a dummy grouping variable in a grouped data  
object.

 > load("~/RbinaryData.rda")
 > dd<-data.frame(RbinaryData,dum=factor(rep(1,120)))
 > names(dd)
[1] "Cross"  "Female" "Male"   "Mate1"  "dum"
 > summary(dd)
  Cross       Female        Male        Mate1        dum
  RR:30   1      : 6   1      : 6   Min.   :0.0000   1:120
  RW:30   2      : 6   2      : 6   1st Qu.:0.0000
  WR:30   3      : 6   3      : 6   Median :1.0000
  WW:30   4      : 6   4      : 6   Mean   :0.5833
          5      : 6   5      : 6   3rd Qu.:1.0000
          6      : 6   6      : 6   Max.   :1.0000
          (Other):84   (Other):84
m1<-lme(Mate1~Cross,data=gd,random=pdBlocked(list(pdIdent(~Female 
-1),pdIdent(~Male-1))))

This model executes and estimates separate intercepts for both females  
and males.  Good.

However, when I try and run this as a glmmPQL, the following error  
ensues (traceback provided below). Am I trying something that is  
impossible at the moment, or just doing something wrong?

Cheers
andrew

 >  
m2<-glmmPQL(Mate1~Cross,data=gd,random=pdBlocked(list(pdIdent(~Female 
-1),pdIdent(~Male-1))),family="binomial")
iteration 1
Error in getGroups.data.frame(dataMix, groups) :
	Invalid formula for groups

 > traceback()
8: stop("Invalid formula for groups")
7: getGroups.data.frame(dataMix, groups)
6: getGroups(dataMix, groups)
5: lme.formula(fixed = zz ~ Cross, random = list(numeric(0),  
numeric(0)),
        data = list(Mate1 = c(1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0), Cross = c(1, 2, 1,
        2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2,
        1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2,
        1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1,
        3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3,
        4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 4, 3, 4, 3, 4, 3, 4, 3,
        4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4,
        3, 4, 3), Female = c(1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,
        3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6,
        6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9,
        9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11,
        12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 14, 14, 14,
        14, 14, 14, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16,
        17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 19, 19, 19,
        19, 19, 19, 20, 20, 20, 20, 20, 20), Male = c(1, 14, 5, 11,
        4, 15, 5, 15, 3, 13, 1, 12, 2, 11, 1, 14, 3, 13, 4, 12, 2,
        15, 5, 14, 3, 13, 4, 12, 2, 11, 19, 9, 20, 7, 16, 8, 18,
        8, 19, 9, 17, 6, 16, 6, 17, 10, 20, 9, 20, 7, 18, 6, 19,
        10, 17, 10, 16, 8, 18, 7, 9, 19, 7, 20, 10, 18, 7, 16, 9,
        17, 6, 20, 8, 17, 6, 19, 7, 16, 10, 20, 8, 18, 9, 19, 6,
        18, 10, 16, 8, 17, 15, 2, 13, 4, 12, 1, 14, 1, 15, 2, 11,
        5, 11, 4, 12, 5, 15, 3, 13, 3, 11, 1, 14, 4, 12, 5, 14, 3,
        13, 2), wts = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), zz = c(2.37523506338983,
        2.19314662972470, 2.37523506338983, -2.30685171777016,  
2.37523506338983,
        2.19314662972470, 2.37523506338983, 2.19314662972470,  
2.37523506338983,
        2.19314662972470, 2.37523506338983, 2.19314662972470,  
2.37523506338983,
        -2.30685171777016, 2.37523506338983, 2.19314662972470,  
2.37523506338983,
        2.19314662972470, 2.37523506338983, 2.19314662972470,  
2.37523506338983,
        -2.30685171777016, 2.37523506338983, 2.19314662972470,  
2.37523506338983,
        2.19314662972470, 2.37523506338983, 2.19314662972470,  
2.37523506338983,
        2.19314662972470, 2.19314662972470, 2.37523506338983,  
2.19314662972470,
        -2.73839300553829, 2.19314662972470, 2.37523506338983,  
-2.30685171777016,
        -2.73839300553829, -2.30685171777016, 2.37523506338983,  
-2.30685171777016,
        -2.73839300553829, -2.30685171777016, 2.37523506338983,  
-2.30685171777016,
        -2.73839300553829, 2.19314662972470, 2.37523506338983,  
-2.30685171777016,
        -2.73839300553829, 2.19314662972470, 2.37523506338983,  
2.19314662972470,
        2.37523506338983, -2.30685171777016, -2.73839300553829,  
2.19314662972470,
        -2.73839300553829, 2.19314662972470, -2.73839300553829,  
-2.49392989051313,
        2.27586790674987, 3.09612363937901, 2.27586790674987,  
-2.49392989051313,
        2.27586790674987, -2.49392989051313, -2.48603224779624,  
-2.49392989051313,
        2.27586790674987, -2.49392989051313, -2.48603224779624,  
-2.49392989051313,
        -2.48603224779624, -2.49392989051313, -2.48603224779624,
        -2.49392989051313, 2.27586790674987, -2.49392989051313,  
2.27586790674987,
        3.09612363937901, 2.27586790674987, -2.49392989051313,  
2.27586790674987,
        -2.49392989051313, 2.27586790674987, -2.49392989051313,  
-2.48603224779624,
        -2.49392989051313, -2.48603224779624, -2.48603224779624,
        -2.49392989051313, 2.27586790674987, -2.49392989051313,  
-2.48603224779624,
        -2.49392989051313, 2.27586790674987, 3.09612363937901,  
2.27586790674987,
        -2.49392989051313, 2.27586790674987, 3.09612363937901,  
2.27586790674987,
        -2.49392989051313, 2.27586790674987, -2.49392989051313,  
2.27586790674987,
        -2.49392989051313, 2.27586790674987, 3.09612363937901,  
2.27586790674987,
        3.09612363937901, 2.27586790674987, -2.49392989051313,  
2.27586790674987,
        -2.49392989051313, -2.48603224779624, 3.09612363937901,  
2.27586790674987,
        -2.49392989051313), invwt = c(5.11362806892812, 4.49999834749486,
        5.11362806892812, 4.49999834749486, 5.11362806892812,  
4.49999834749486,
        5.11362806892812, 4.49999834749486, 5.11362806892812,  
4.49999834749486,
        5.11362806892812, 4.49999834749486, 5.11362806892812,  
4.49999834749486,
        5.11362806892812, 4.49999834749486, 5.11362806892812,  
4.49999834749486,
        5.11362806892812, 4.49999834749486, 5.11362806892812,  
4.49999834749486,
        5.11362806892812, 4.49999834749486, 5.11362806892812,  
4.49999834749486,
        5.11362806892812, 4.49999834749486, 5.11362806892812,  
4.49999834749486,
        4.49999834749486, 5.11362806892812, 4.49999834749486,  
5.11362806892812,
        4.49999834749486, 5.11362806892812, 4.49999834749486,  
5.11362806892812,
        4.49999834749486, 5.11362806892812, 4.49999834749486,  
5.11362806892812,
        4.49999834749486, 5.11362806892812, 4.49999834749486,  
5.11362806892812,
        4.49999834749486, 5.11362806892812, 4.49999834749486,  
5.11362806892812,
        4.49999834749486, 5.11362806892812, 4.49999834749486,  
5.11362806892812,
        4.49999834749486, 5.11362806892812, 4.49999834749486,  
5.11362806892812,
        4.49999834749486, 5.11362806892812, 5.59005352989214,  
4.76190015454611,
        5.59005352989214, 4.76190015454611, 5.59005352989214,  
4.76190015454611,
        5.59005352989214, 4.76190015454611, 5.59005352989214,  
4.76190015454611,
        5.59005352989214, 4.76190015454611, 5.59005352989214,  
4.76190015454611,
        5.59005352989214, 4.76190015454611, 5.59005352989214,  
4.76190015454611,
        5.59005352989214, 4.76190015454611, 5.59005352989214,  
4.76190015454611,
        5.59005352989214, 4.76190015454611, 5.59005352989214,  
4.76190015454611,
        5.59005352989214, 4.76190015454611, 5.59005352989214,  
4.76190015454611,
        4.76190015454611, 5.59005352989214, 4.76190015454611,  
5.59005352989214,
        4.76190015454611, 5.59005352989214, 4.76190015454611,  
5.59005352989214,
        4.76190015454611, 5.59005352989214, 4.76190015454611,  
5.59005352989214,
        4.76190015454611, 5.59005352989214, 4.76190015454611,  
5.59005352989214,
        4.76190015454611, 5.59005352989214, 4.76190015454611,  
5.59005352989214,
        4.76190015454611, 5.59005352989214, 4.76190015454611,  
5.59005352989214,
        4.76190015454611, 5.59005352989214, 4.76190015454611,  
5.59005352989214,
        4.76190015454611, 5.59005352989214)), method = "ML", weights =  
varFixed(~invwt))
4: lme(fixed = zz ~ Cross, random = list(numeric(0), numeric(0)),
        data = list(Mate1 = c(1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0), Cross = c(1, 2, 1,
        2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2,
        1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2,
        1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1,
        3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3,
        4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 4, 3, 4, 3, 4, 3, 4, 3,
        4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4,
        3, 4, 3), Female = c(1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,
        3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6,
        6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9,
        9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11,
        12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 14, 14, 14,
        14, 14, 14, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16,
        17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 19, 19, 19,
        19, 19, 19, 20, 20, 20, 20, 20, 20), Male = c(1, 14, 5, 11,
        4, 15, 5, 15, 3, 13, 1, 12, 2, 11, 1, 14, 3, 13, 4, 12, 2,
        15, 5, 14, 3, 13, 4, 12, 2, 11, 19, 9, 20, 7, 16, 8, 18,
        8, 19, 9, 17, 6, 16, 6, 17, 10, 20, 9, 20, 7, 18, 6, 19,
        10, 17, 10, 16, 8, 18, 7, 9, 19, 7, 20, 10, 18, 7, 16, 9,
        17, 6, 20, 8, 17, 6, 19, 7, 16, 10, 20, 8, 18, 9, 19, 6,
        18, 10, 16, 8, 17, 15, 2, 13, 4, 12, 1, 14, 1, 15, 2, 11,
        5, 11, 4, 12, 5, 15, 3, 13, 3, 11, 1, 14, 4, 12, 5, 14, 3,
        13, 2), wts = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), zz = c(2.37523506338983,
        2.19314662972470, 2.37523506338983, -2.30685171777016,  
2.37523506338983,
        2.19314662972470, 2.37523506338983, 2.19314662972470,  
2.37523506338983,
        2.19314662972470, 2.37523506338983, 2.19314662972470,  
2.37523506338983,
        -2.30685171777016, 2.37523506338983, 2.19314662972470,  
2.37523506338983,
        2.19314662972470, 2.37523506338983, 2.19314662972470,  
2.37523506338983,
        -2.30685171777016, 2.37523506338983, 2.19314662972470,  
2.37523506338983,
        2.19314662972470, 2.37523506338983, 2.19314662972470,  
2.37523506338983,
        2.19314662972470, 2.19314662972470, 2.37523506338983,  
2.19314662972470,
        -2.73839300553829, 2.19314662972470, 2.37523506338983,  
-2.30685171777016,
        -2.73839300553829, -2.30685171777016, 2.37523506338983,  
-2.30685171777016,
        -2.73839300553829, -2.30685171777016, 2.37523506338983,  
-2.30685171777016,
        -2.73839300553829, 2.19314662972470, 2.37523506338983,  
-2.30685171777016,
        -2.73839300553829, 2.19314662972470, 2.37523506338983,  
2.19314662972470,
        2.37523506338983, -2.30685171777016, -2.73839300553829,  
2.19314662972470,
        -2.73839300553829, 2.19314662972470, -2.73839300553829,  
-2.49392989051313,
        2.27586790674987, 3.09612363937901, 2.27586790674987,  
-2.49392989051313,
        2.27586790674987, -2.49392989051313, -2.48603224779624,  
-2.49392989051313,
        2.27586790674987, -2.49392989051313, -2.48603224779624,  
-2.49392989051313,
        -2.48603224779624, -2.49392989051313, -2.48603224779624,
        -2.49392989051313, 2.27586790674987, -2.49392989051313,  
2.27586790674987,
        3.09612363937901, 2.27586790674987, -2.49392989051313,  
2.27586790674987,
        -2.49392989051313, 2.27586790674987, -2.49392989051313,  
-2.48603224779624,
        -2.49392989051313, -2.48603224779624, -2.48603224779624,
        -2.49392989051313, 2.27586790674987, -2.49392989051313,  
-2.48603224779624,
        -2.49392989051313, 2.27586790674987, 3.09612363937901,  
2.27586790674987,
        -2.49392989051313, 2.27586790674987, 3.09612363937901,  
2.27586790674987,
        -2.49392989051313, 2.27586790674987, -2.49392989051313,  
2.27586790674987,
        -2.49392989051313, 2.27586790674987, 3.09612363937901,  
2.27586790674987,
        3.09612363937901, 2.27586790674987, -2.49392989051313,  
2.27586790674987,
        -2.49392989051313, -2.48603224779624, 3.09612363937901,  
2.27586790674987,
        -2.49392989051313), invwt = c(5.11362806892812, 4.49999834749486,
        5.11362806892812, 4.49999834749486, 5.11362806892812,  
4.49999834749486,
        5.11362806892812, 4.49999834749486, 5.11362806892812,  
4.49999834749486,
        5.11362806892812, 4.49999834749486, 5.11362806892812,  
4.49999834749486,
        5.11362806892812, 4.49999834749486, 5.11362806892812,  
4.49999834749486,
        5.11362806892812, 4.49999834749486, 5.11362806892812,  
4.49999834749486,
        5.11362806892812, 4.49999834749486, 5.11362806892812,  
4.49999834749486,
        5.11362806892812, 4.49999834749486, 5.11362806892812,  
4.49999834749486,
        4.49999834749486, 5.11362806892812, 4.49999834749486,  
5.11362806892812,
        4.49999834749486, 5.11362806892812, 4.49999834749486,  
5.11362806892812,
        4.49999834749486, 5.11362806892812, 4.49999834749486,  
5.11362806892812,
        4.49999834749486, 5.11362806892812, 4.49999834749486,  
5.11362806892812,
        4.49999834749486, 5.11362806892812, 4.49999834749486,  
5.11362806892812,
        4.49999834749486, 5.11362806892812, 4.49999834749486,  
5.11362806892812,
        4.49999834749486, 5.11362806892812, 4.49999834749486,  
5.11362806892812,
        4.49999834749486, 5.11362806892812, 5.59005352989214,  
4.76190015454611,
        5.59005352989214, 4.76190015454611, 5.59005352989214,  
4.76190015454611,
        5.59005352989214, 4.76190015454611, 5.59005352989214,  
4.76190015454611,
        5.59005352989214, 4.76190015454611, 5.59005352989214,  
4.76190015454611,
        5.59005352989214, 4.76190015454611, 5.59005352989214,  
4.76190015454611,
        5.59005352989214, 4.76190015454611, 5.59005352989214,  
4.76190015454611,
        5.59005352989214, 4.76190015454611, 5.59005352989214,  
4.76190015454611,
        5.59005352989214, 4.76190015454611, 5.59005352989214,  
4.76190015454611,
        4.76190015454611, 5.59005352989214, 4.76190015454611,  
5.59005352989214,
        4.76190015454611, 5.59005352989214, 4.76190015454611,  
5.59005352989214,
        4.76190015454611, 5.59005352989214, 4.76190015454611,  
5.59005352989214,
        4.76190015454611, 5.59005352989214, 4.76190015454611,  
5.59005352989214,
        4.76190015454611, 5.59005352989214, 4.76190015454611,  
5.59005352989214,
        4.76190015454611, 5.59005352989214, 4.76190015454611,  
5.59005352989214,
        4.76190015454611, 5.59005352989214, 4.76190015454611,  
5.59005352989214,
        4.76190015454611, 5.59005352989214)), method = "ML", weights =  
varFixed(~invwt))
3: eval(expr, envir, enclos)
2: eval(mcall)
1: glmmPQL(Mate1 ~ Cross, data = gd, random =  
pdBlocked(list(pdIdent(~Female -
        1), pdIdent(~Male - 1))), family = "binomial")

------------------------------------------------------------------------ 
---------
Dr. Andrew Beckerman
Department of Animal and Plant Sciences, University of Sheffield,
Alfred Denny Building, Western Bank, Sheffield S10 2TN, UK
ph +44 (0)114 222 0026; fx +44 (0)114 222 0002
http://www.shef.ac.uk/beckslab
------------------------------------------------------------------------ 
----------



From bates at wisc.edu  Wed Jan  5 18:02:45 2005
From: bates at wisc.edu (Douglas Bates)
Date: Wed, 05 Jan 2005 11:02:45 -0600
Subject: [R] lme: error message with random=~1
In-Reply-To: <41DC07F7.6040900@hhbio.wasser.tu-dresden.de>
References: <41DBBD5D.4030808@hhbio.wasser.tu-dresden.de>	<41DBE063.5080408@stat.wisc.edu>
	<41DC07F7.6040900@hhbio.wasser.tu-dresden.de>
Message-ID: <41DC1DB5.7070701@wisc.edu>

Thomas Petzoldt wrote:
> Douglas Bates wrote:
> 
>>
>> I'm not sure what model you want to fit here.  To specify a random 
>> effect in lme you need both a grouping factor and a model matrix.  The 
>> error message indicates that lme is unable to determine a grouping 
>> factor.  It would be correct syntax if you added a single level factor 
>> to the data frame and used that but then the model fit would fail 
>> because you would be trying to estimate a variance in a model where 
>> there is no variation in the term.
> 
> 
> O.k. I see and think I understand it.
> 
>> It seems to me that you are trying to estimate parameters in a 
>> mixed-effects model without any random effects and lme can't do that.
>>
> 
> Yes, what I want is a model without any random effects to be tested 
> against a model with random effects. I want to show, that the random 
> effects are negligible but that we account for pseudo replicates and 
> have tested this explicitely.
> 
> I'm not sure what is better: to leave the random effects in the model or 
>  simply an LR test against a linear model fitted by lm. I've never seen 
> such an example in the books. Or have I missed a global alternative here?
> 
> Thomas P.

I would recommend the likelihood ratio test against a linear model fit 
by lm.  The p-value returned from this test will be conservative because 
  you are testing on the boundary of the parameter space.



From maechler at stat.math.ethz.ch  Wed Jan  5 17:35:28 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 5 Jan 2005 17:35:28 +0100
Subject: [R] integer factorization
In-Reply-To: <16859.48586.242334.33187@stat.math.ethz.ch>
References: <A4D457E6-5EF6-11D9-946F-000A95D86AA8@soc.soton.ac.uk>
	<16859.48586.242334.33187@stat.math.ethz.ch>
Message-ID: <16860.5968.78161.887717@stat.math.ethz.ch>

>>>>> "ChrBu" == Christoph Buser <buser at stat.math.ethz.ch>
>>>>>     on Wed, 5 Jan 2005 11:13:30 +0100 writes:

    ChrBu> Hi Robin
    ChrBu> There is a function factorize() in the package conf.design

    ChrBu> library(conf.design)
    ChrBu> factorize(60)
    ChrBu> [1] 2 2 3 5

yes, as quick search on Jonathan Baron's site
(with the nice new  URL  http://search.R-project.org/ !!)
also shows.

    ChrBu> Hope this helps

yes, merci, Christoph!

Note that conf.design is by Bill Venables himself who has also
been involved in the discussion on S-news, ca. 1996, and 1998
about fast / useful prime number and factorization S functions.

>From that time I still have nice file of function definitions,
of other people's and my own functions.
Last summer I've also found a few tricks to speed up {for R} Bill's
already quite fast  primes() function.

Also, I have had a factorize() function that is considerable
faster than the one in 'conf.design' at least for the examples
I've tried.

For the moment, my code and examples are available from
  ftp://stat.ethz.ch/U/maechler/R/  prime-numbers-fn.R
and                                 prime-numbers.R

E.g.,
 > source("ftp://stat.ethz.ch/U/maechler/R/prime-numbers-fn.R")
 > factorize(20:25)
 $"20"
      p m
 [1,] 2 2
 [2,] 5 1

 $"21"
      p m
 [1,] 3 1
 [2,] 7 1

 $"22"
       p m
 [1,]  2 1
 [2,] 11 1

 $"23"
       p m
 [1,] 23 1

 $"24"
      p m
 [1,] 2 3
 [2,] 3 1

 $"25"
      p m
 [1,] 5 2

which is even a bit closer to what Robin was looking for.

{Note that my factorize() is uses prime.sieve() whereas I show
 in other examples that Bill Venables' primes() is much faster
 and my modification of it is even faster;
 also for the case of factorizing( <long vector> ), it would be
 better to make use of *stored* prime numbers...
}
----

and  "ftp://stat.ethz.ch/U/maechler/R/prime-numbers.R"
contains more examples using (and comparing) the function
definitions in ..../prime-numbers-fn.R

Of course, to do this properly one should really use special
code (in C) and maybe even consider long integers etc.  
It's however quite astonishing how fast the primes() and
factorize() functions run, and I have been wondering more than
once if they shouldn't be added to ``standard R'', i.e., the
"utils" package.

Martin



From qunshi at cs.uchicago.edu  Wed Jan  5 18:17:39 2005
From: qunshi at cs.uchicago.edu (Qun Shi)
Date: Wed, 5 Jan 2005 11:17:39 -0600 (CST)
Subject: [R] (no subject)
Message-ID: <Pine.LNX.4.58.0501051109590.15833@swank.cs.uchicago.edu>

Hi,

I'm trying to use the version of dchip combined with R to analyze my data.
I need R version 1.6 which fits for dchip as dchip manual said. So I
would appreciate a lot if someone could tell me where I could find this
version and download? I'm using Windows 2000.

Thanks, Jean

Qun(Jean) Shi
Functional Genomics Facility
University of Chciago
Tel: 773-8345289



From abunn at whrc.org  Wed Jan  5 18:18:50 2005
From: abunn at whrc.org (Andy Bunn)
Date: Wed, 5 Jan 2005 12:18:50 -0500
Subject: [R] count element in column
In-Reply-To: <20050105160734.3842.qmail@web51808.mail.yahoo.com>
Message-ID: <NEBBIPHDAMMOKDKPOFFICEJFCOAA.abunn@whrc.org>

How about this?
length(res[res < 0, 1])
HTH, Andy

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Frederic renaud
> Sent: Wednesday, January 05, 2005 11:08 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] count element in column
> 
> 
> Hi,
> I 've a matrix n*1 (thus a column) and I would like to
> count the number of negative element inside.
> Can you help me?
> Thanks!
> 
> eg:
>  res[,1]= 1
>           -3
>           -1
> 
> How obtain the number 2 (number of negative-element)?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From adi at roda.ro  Wed Jan  5 18:35:58 2005
From: adi at roda.ro (adi@roda.ro)
Date: Wed,  5 Jan 2005 19:35:58 +0200
Subject: [R] Tuning string matching
Message-ID: <1104946558.41dc257e6fbbd@www.roda.ro>

Dear list,

I spent about two hours searching on the message archive, with no avail.
I have a list of people that have to pass an on-line test, but only a fraction
of them do it. Moreover, as they input their names, the resulting string do not
always match the names I have in my database.

I would like to do two things:

1. Match any strings that are 90% the same
Example:
name1 <- "Harry Harrington"
name2 <- "Harry Harington"
I need a function that would declare those strings as a match (ideally having an
argument that would allow introducing 80% instead of 90%)

2. Arrange a final table that would take me from:

Table1 (the complete list of people from my database)
No Name
1  Byron C. Andrew
2  Friedman Bob
3  Harrington Harry

Table2 (the people having been tested)
No Name               Score
1  Harry Harington    13
2  Byron Andrew       28

to:

No Name1              Name2              Score
1  Byron C. Andrew    Byron Andrew       28
2  Friedman Bob
3  Harrington Harry   Harry Harington    13

Thank you in advance, any help is highly appreciated.
Adrian



From bstabler at ptvamerica.com  Wed Jan  5 18:52:54 2005
From: bstabler at ptvamerica.com (Ben Stabler)
Date: Wed, 5 Jan 2005 09:52:54 -0800
Subject: [R] zlib
Message-ID: <00e601c4f34f$62e1d5f0$fc00a8c0@BenStabler>

I need to decompress an array of bytes that were compressed using zlib.  The
bytes are stored within a binary file.  I can seek to the location, read the
length of the compressed byte array, and read the bytes into R.  What I need
to do then is decompress the byte array to an array of doubles.  

The code looks something like this:

compresslength = readBin(infile, integer(), 1, 4)
data = readBin(infile, integer(), compresslength, 1)
#need something here like..... result = zlibDecompress(data)

I implemented it in Python using the zlib module and the decompress method.
The Python code is:

data = file.read(compresslength) #read x number of bytes
result = struct.unpack("3d", zlib.decompress(data)) #decompress data and
convert bytes to 3 doubles

I also tried:

data = readBin(gzcon(infile), integer(), compresslength, 1) 

But it returns what I asked for....a vector of 1 bytes with a length equal
to compresslength.  It seems that the arguments to readBin require
prespecification of the result, which I can't really provide.  gzfile and
gzcon wrap a connection, but then in order to readBin from the connection, I
need to specify the data type, the number of items, and the number of bytes
per item.  But I don't know this since zlib did the compressing.  The gzip
website says to use the zlib library for in-memory compression, so maybe
what I am trying to do cannot be done with what currently exists in R.  

The other issue in this is the conversion of the bytes to doubles.  Since I
have a lot of compress values - ~ 250000 - I'm guessing that I can't afford
to write a little R function to operate on a vector of 1 byte integers.  So
that is why I was hoping to find a way to do it with existing (compiled C
code) R.

Maybe I am missing something...I don't know :)  Thanks for your help.

Ben Stabler
PTV America, Inc.
1128 NE 2nd St, Suite 204
Corvallis, OR 97330
541-754-6836 x205
541-754-6837 fax
www.ptvamerica.com



From robert.kruus at utoronto.ca  Wed Jan  5 18:55:34 2005
From: robert.kruus at utoronto.ca (Robert Kruus)
Date: Wed, 5 Jan 2005 12:55:34 -0500
Subject: [R] count element in column
In-Reply-To: <20050105160734.3842.qmail@web51808.mail.yahoo.com>
References: <20050105160734.3842.qmail@web51808.mail.yahoo.com>
Message-ID: <20050105125534.1b0010cf@kruuslt.forestry.utoronto.ca>

How about
sum(res<0)?

-- 
robert.kruus at utoronto.ca
The computer should be doing the hard work.  That's what it's paid to
do, after all.
             -- Larry Wall in <199709012312.QAA08121 at wall.org>

--------

It is  rumored that on Wed, 5 Jan 2005 08:07:34 -0800 (PST)
Frederic renaud <fren2 at yahoo.com> wrote:

> Hi,
> I 've a matrix n*1 (thus a column) and I would like to
> count the number of negative element inside.
> Can you help me?
> Thanks!
> 
> eg:
>  res[,1]= 1
>           -3
>           -1
> 
> How obtain the number 2 (number of negative-element)?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 


--------



From spencer.graves at pdf.com  Wed Jan  5 19:00:43 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 05 Jan 2005 10:00:43 -0800
Subject: [R] count element in column
In-Reply-To: <20050105160734.3842.qmail@web51808.mail.yahoo.com>
References: <20050105160734.3842.qmail@web51808.mail.yahoo.com>
Message-ID: <41DC2B4B.1020709@pdf.com>

      Have you considered

 > sum(res[,1]<0)
[1] 2

      hope this helps.  spencer graves

Frederic renaud wrote:

>Hi,
>I 've a matrix n*1 (thus a column) and I would like to
>count the number of negative element inside.
>Can you help me?
>Thanks!
>
>eg:
> res[,1]= 1
>          -3
>          -1
>
>How obtain the number 2 (number of negative-element)?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From andy_liaw at merck.com  Wed Jan  5 19:00:16 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 5 Jan 2005 13:00:16 -0500
Subject: [R] count element in column
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4AA@usrymx25.merck.com>

Something like colSums(res < 0), which works for any number of columns.

Andy

> From: Frederic renaud
> 
> Hi,
> I 've a matrix n*1 (thus a column) and I would like to
> count the number of negative element inside.
> Can you help me?
> Thanks!
> 
> eg:
>  res[,1]= 1
>           -3
>           -1
> 
> How obtain the number 2 (number of negative-element)?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Ted.Harding at nessie.mcc.ac.uk  Wed Jan  5 18:43:03 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 05 Jan 2005 17:43:03 -0000 (GMT)
Subject: [R] count element in column
In-Reply-To: <20050105160734.3842.qmail@web51808.mail.yahoo.com>
Message-ID: <XFMail.050105174303.Ted.Harding@nessie.mcc.ac.uk>

On 05-Jan-05 Frederic renaud wrote:
> Hi,
> I 've a matrix n*1 (thus a column) and I would like to
> count the number of negative element inside.
> Can you help me?
> Thanks!
> 
> eg:
>  res[,1]= 1
>           -3
>           -1
> 
> How obtain the number 2 (number of negative-element)?

If there is only one column, then either

  sum(x<0)

or

  sum(x[,1]<0)

If there is more than one column, then

  sum(x[,1]<0)
  sum(x[,2]<0)
  ...

does it for each column, one at a time, and

  colSums(x<0)

does it for all the columns at once (but separately), while

  sum(x<0)

does it for the whole matrix (without regard for columns).

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 05-Jan-05                                       Time: 17:43:03
------------------------------ XFMail ------------------------------



From andy_liaw at merck.com  Wed Jan  5 19:04:33 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 5 Jan 2005 13:04:33 -0500
Subject: [R] (no subject)
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4AB@usrymx25.merck.com>

Sources for versions of R as far back as 0.65, I believe, are available on
CRAN.  You can try to compile from source.

Andy

> From: Qun Shi
> 
> Hi,
> 
> I'm trying to use the version of dchip combined with R to 
> analyze my data.
> I need R version 1.6 which fits for dchip as dchip manual said. So I
> would appreciate a lot if someone could tell me where I could 
> find this
> version and download? I'm using Windows 2000.
> 
> Thanks, Jean
> 
> Qun(Jean) Shi
> Functional Genomics Facility
> University of Chciago
> Tel: 773-8345289
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From bob at kruus.forestry.utoronto.ca  Wed Jan  5 19:11:27 2005
From: bob at kruus.forestry.utoronto.ca (Robert Kruus)
Date: Wed, 5 Jan 2005 13:11:27 -0500
Subject: [R] count element in column
In-Reply-To: <20050105160734.3842.qmail@web51808.mail.yahoo.com>
References: <20050105160734.3842.qmail@web51808.mail.yahoo.com>
Message-ID: <20050105131127.30e55ce0@kruuslt.forestry.utoronto.ca>

How about
sum(res<0)?

-- 
robert.kruus at utoronto.ca
The computer should be doing the hard work.  That's what it's paid to
do, after all.
             -- Larry Wall in <199709012312.QAA08121 at wall.org>

--------

It is  rumored that on Wed, 5 Jan 2005 08:07:34 -0800 (PST)
Frederic renaud <fren2 at yahoo.com> wrote:

> Hi,
> I 've a matrix n*1 (thus a column) and I would like to
> count the number of negative element inside.
> Can you help me?
> Thanks!
> 
> eg:
>  res[,1]= 1
>           -3
>           -1
> 
> How obtain the number 2 (number of negative-element)?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 


--------



-- 
robert.kruus at utoronto.ca
A beginning is the time for taking the most delicate care that balances
are correct.
		-- Princess Irulan, "Manual of Maud'Dib"



From andy_liaw at merck.com  Wed Jan  5 19:07:14 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 5 Jan 2005 13:07:14 -0500
Subject: [R] variance of combinations of means - off topic
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4AC@usrymx25.merck.com>

You could try googling for "delta method".  I believe MASS even has code for
that...

Andy

> From: Bill Shipley
> 
> Hello, and please excuse this off-topic question, but I have not been
> able to find an answer elsewhere.  Consider a value Z that is 
> calculated
> using the product (or ratio) of two means X_mean and Y_mean:
> Z=X_mean*Y_mean.  More generally, Z=f(X_mean, Y_mean).  The standard
> error of Z will be a function of the standard errors of the means of X
> and Y.  I want to calculate this se of Z.  Can someone direct me to a
> reference (text book or other) that gives the solution to 
> this *general*
> problem?
> 
> Thanks.
> 
>  
> 
> Bill Shipley
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From gunter.berton at gene.com  Wed Jan  5 19:15:56 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 5 Jan 2005 10:15:56 -0800
Subject: [R] count element in column
In-Reply-To: <NEBBIPHDAMMOKDKPOFFICEJFCOAA.abunn@whrc.org>
Message-ID: <200501051815.j05IFuhC008615@volta.gene.com>

....


> How about this?
> length(res[res < 0, 1])
> HTH, Andy
> 
or simply sum(res<0) .

For R beginners: This works because the logical res<0 vector is
automatically coerced to a numeric vector of 0's and 1's by sum(). 


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA



> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of 
> Frederic renaud
> > Sent: Wednesday, January 05, 2005 11:08 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] count element in column
> > 
> > 
> > Hi,
> > I 've a matrix n*1 (thus a column) and I would like to
> > count the number of negative element inside.
> > Can you help me?
> > Thanks!
> > 
> > eg:
> >  res[,1]= 1
> >           -3
> >           -1
> > 
> > How obtain the number 2 (number of negative-element)?
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From bobo at medgen.univr.it  Wed Jan  5 19:16:14 2005
From: bobo at medgen.univr.it (Giovanni Malerba)
Date: Wed, 5 Jan 2005 19:16:14 +0100
Subject: [R] Targeting the elements that satisfy matching rules excluding
	the NA
Message-ID: <200501051916.14410.bobo@medgen.univr.it>

Dear R-List,
here my problem:
> a <- c("gio","gao","geo",NA,"1","alpha")
> b <- 1:6
> data.frame(V1=a,V2=b) -> c
> c
     V1 V2
1   gio  1
2   gao  2
3   geo  3
4  <NA>  4
5     1  5
6 alpha  6

> rownames(c) %in% grep("a",as.character(c$V1))
[1] FALSE  TRUE FALSE FALSE FALSE  TRUE

while I would like to obtain
[1] FALSE  TRUE FALSE <NA> FALSE  TRUE

Is there a simple way to do this without doing things like
> rownames(c) %in% grep("a",as.character(c$V1)) -> tmp
> tmp[is.na(c$V1)]<- NA
?

It would be nice if grep (or another command) produced FALSE or TRUE according 
to the matching rule and NA if the value is NA. I was not able to find such a 
feature (just to do this in one simple command-line :)).

Thank you,
Giovanni Malerba.



From anne.piotet at urbanet.ch  Wed Jan  5 19:17:10 2005
From: anne.piotet at urbanet.ch (Anne)
Date: Wed, 5 Jan 2005 19:17:10 +0100
Subject: [R] count element in column
References: <20050105160734.3842.qmail@web51808.mail.yahoo.com>
Message-ID: <002001c4f352$c74b3880$6c00a8c0@mtd4>

x<-c(-3. -4.7, -.005, 1, 9)
> length(x[x<0])
[1] 3

Anne



----- Original Message ----- 
From: "Frederic renaud" <fren2 at yahoo.com>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, January 05, 2005 5:07 PM
Subject: [R] count element in column


> Hi,
> I 've a matrix n*1 (thus a column) and I would like to
> count the number of negative element inside.
> Can you help me?
> Thanks!
>
> eg:
>  res[,1]= 1
>           -3
>           -1
>
> How obtain the number 2 (number of negative-element)?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Wed Jan  5 19:23:31 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 05 Jan 2005 10:23:31 -0800
Subject: [R] variance of combinations of means - off topic
In-Reply-To: <001401c4f340$e9b7cd50$ae1ad284@BIO041>
References: <001401c4f340$e9b7cd50$ae1ad284@BIO041>
Message-ID: <41DC30A3.3040109@pdf.com>

      I know two standard ways to approach this.  The traditional 
approximation is called the "delta method";  it uses a Taylor series 
approximation, usually of first order but could be higher.  Googling for 
"delta method" produced several useful hits just now.  The second method 
is Monte Carlo. 

      hope this helps.  spencer graves

Bill Shipley wrote:

>Hello, and please excuse this off-topic question, but I have not been
>able to find an answer elsewhere.  Consider a value Z that is calculated
>using the product (or ratio) of two means X_mean and Y_mean:
>Z=X_mean*Y_mean.  More generally, Z=f(X_mean, Y_mean).  The standard
>error of Z will be a function of the standard errors of the means of X
>and Y.  I want to calculate this se of Z.  Can someone direct me to a
>reference (text book or other) that gives the solution to this *general*
>problem?
>
>Thanks.
>
> 
>
>Bill Shipley
>
> 
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From giles.heywood at cantab.net  Wed Jan  5 19:29:41 2005
From: giles.heywood at cantab.net (Giles Heywood)
Date: Wed, 5 Jan 2005 18:29:41 -0000
Subject: [R] matrix no longer "is" array in 2.0.1?
Message-ID: <KLEHJMEACMGEMBOOPFCAKEGPCAAA.giles.heywood@cantab.net>

I have an S4 class which extends array and has other slots, and in upgrading
to 2.0.1 (from 1.9.1) I have encountered a change in behaviour. This causes
me some difficulties if I want to allow 2-dimensional arrays in the slot
(which I do).

The following (in 2.0.1) illustrates the point:

> setClass("foo",representation("array"))
[1] "foo"
> a <- new("foo",array(NA,2:4))
> b <- new("foo",matrix(NA,2,3))
Error in "as<-"(`*tmp*`, Classi, value = c(NA, NA, NA, NA, NA, NA)) :
        No method or default for as() replacement of "foo" with
Class="matrix"

This last error did not occur under 1.9.1.

This becomes a practical consideration in an extract method:

> setMethod("[", c("foo"),  function(x, i, j, ..., drop)
+     {
+     if(missing(i)) {i <- min(1,nrow(x)):nrow(x)}
+     if(missing(j)) {j <- min(1,ncol(x)):ncol(x)}
+     if(missing(drop)) {drop <- TRUE}
+     subx <- x at .Data[i, j, ..., drop = drop]
+     new("foo",subx)
+     })
[1] "["
> a[,,1]
Error in "as<-"(`*tmp*`, Classi, value = c(NA, NA, NA, NA, NA, NA)) :
        No method or default for as() replacement of "foo" with
Class="matrix"

This is changed behaviour from 1.9.1, for the reason above.

I can try to assign the matrix an array class, but the 2-dimensional object
has a special status, viz:

> class(as(1,"array"))
[1] "array"
> dim(as(1,"array"))
[1] 1

but:

> class(as(matrix(1,1,1),"array"))
[1] "matrix"

I have checked using extends(), and matrix and array still extend one
another (hence there is a trivial but unhelpful answer to the title of this
posting).

getClass() shows some differences, but I am not sure whether they are
related to this issue:

1.9.1

>getClass("matrix")

No Slots, prototype of class "matrix"

Extends: "structure", "array"

Known Subclasses:
Class "array", directly, with explicit test and coerce

2.0.1

>getClass("matrix")

No Slots, prototype of class "matrix"

Extends:
Class "structure", directly
Class "array", directly
Class "vector", by class "structure", with explicit coerce
Class "vector", by class "array", with explicit coerce

Known Subclasses:
Class "array", directly, with explicit test and coerce

My question is: how can I get a 2-dimensional object into the slot in my S4
class, or into the class "foo" in the minimal example above?

[OS = Windows XP]



From ruben.merz at epfl.ch  Wed Jan  5 19:55:00 2005
From: ruben.merz at epfl.ch (Ruben Merz)
Date: Wed, 05 Jan 2005 19:55:00 +0100
Subject: [R] shape parameter in Lindsay's gnlr
Message-ID: <41DC3804.6070805@epfl.ch>


Dear all

I'm using Jim Lindsay's glnr	to perform some maximum likelihood fitting. 
What I would like to know is to what does the shape parameter returned 
correspond to when using the Laplace distribution and the Cauchy 
distribution?

In particular, given a Laplace distrib

1/(2*s) exp(abs(x-mu)/s)

How to relate the shape parameter with s...?

Thanks a lot in advance for any help
Best regards

Ruben



From tlumley at u.washington.edu  Wed Jan  5 19:54:39 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 5 Jan 2005 10:54:39 -0800 (PST)
Subject: [R] Tuning string matching
In-Reply-To: <1104946558.41dc257e6fbbd@www.roda.ro>
References: <1104946558.41dc257e6fbbd@www.roda.ro>
Message-ID: <Pine.A41.4.61b.0501051047430.59084@homer03.u.washington.edu>

On Wed, 5 Jan 2005 adi at roda.ro wrote:

> Dear list,
>
> I spent about two hours searching on the message archive, with no avail.
> I have a list of people that have to pass an on-line test, but only a fraction
> of them do it. Moreover, as they input their names, the resulting string do not
> always match the names I have in my database.
>
> I would like to do two things:
>
> 1. Match any strings that are 90% the same
> Example:
> name1 <- "Harry Harrington"
> name2 <- "Harry Harington"
> I need a function that would declare those strings as a match (ideally having an
> argument that would allow introducing 80% instead of 90%)

agrep() does something very similar to this.  It has an edit distance 
rather than a % similarity, but you should be able to tune it to do what 
you want.

> 2. Arrange a final table that would take me from:
>
> Table1 (the complete list of people from my database)
> No Name
> 1  Byron C. Andrew
> 2  Friedman Bob
> 3  Harrington Harry
>
> Table2 (the people having been tested)
> No Name               Score
> 1  Harry Harington    13
> 2  Byron Andrew       28
>
> to:
>
> No Name1              Name2              Score
> 1  Byron C. Andrew    Byron Andrew       28
> 2  Friedman Bob
> 3  Harrington Harry   Harry Harington    13
>

This may not be very well-defined, since 90% agreement is not an 
equivalence relation.

Assuming that sets of matches are either identical or disjoint you could 
construct a numeric variable in table 2 that indicates which row of table 
1 to match, by using agrep() in a loop.


 	-thomas



From spencer.graves at pdf.com  Wed Jan  5 20:21:06 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 05 Jan 2005 11:21:06 -0800
Subject: [R] variance of combinations of means - off topic
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E4AC@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E4AC@usrymx25.merck.com>
Message-ID: <41DC3E22.4030109@pdf.com>

Hi, Andy: 

      MASS4 has section 5.7 "Bootstrap and Permutation Methods".  Is 
this what you are suggesting?  It certainly is relevant to the question 
(but not to the "delta method", except as a means of checking on it). 

      Thanks,
      spencer graves

Liaw, Andy wrote:

>You could try googling for "delta method".  I believe MASS even has code for
>that...
>
>Andy
>
>  
>
>>From: Bill Shipley
>>
>>Hello, and please excuse this off-topic question, but I have not been
>>able to find an answer elsewhere.  Consider a value Z that is 
>>calculated
>>using the product (or ratio) of two means X_mean and Y_mean:
>>Z=X_mean*Y_mean.  More generally, Z=f(X_mean, Y_mean).  The standard
>>error of Z will be a function of the standard errors of the means of X
>>and Y.  I want to calculate this se of Z.  Can someone direct me to a
>>reference (text book or other) that gives the solution to 
>>this *general*
>>problem?
>>
>>Thanks.
>>
>> 
>>
>>Bill Shipley
>>
>> 
>>
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From mail at bymouth.com  Wed Jan  5 20:34:57 2005
From: mail at bymouth.com (Stephen Choularton)
Date: Thu, 6 Jan 2005 06:34:57 +1100
Subject: [R] plotting percent of incidents within different 'bins'
Message-ID: <000001c4f35d$a719a3c0$9701a8c0@Tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050106/077309b3/attachment.pl

From Robert.McGehee at geodecapital.com  Wed Jan  5 20:36:12 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Wed, 5 Jan 2005 14:36:12 -0500
Subject: [R] Tuning string matching
Message-ID: <67DCA285A2D7754280D3B8E88EB5480206741DFC@MSGBOSCLB2WIN.DMN1.FMR.COM>

It sounds like what you want is a rudimentary spell-checker whose "word"
is the input name, and whose "dictionary" is an array of your database
names. Spell checking rules are designed to find missing repeats,
transposed letters, extra letters... precisely the reasons you're not
matching your names to your database.

Anyway, as I don't believe R has something like this, what I would do is
simply rewrite one of the dozens of Perl or C spell checkers to fit your
needs (such as Aspell / Ispell), then invoke a script under R using the
"system" call, passing in the student name and your database of names.
And as R can use Perl-like regular expression (?regexpr), you could (if
you really wanted to!) rewrite this into R after the fact, although this
would likely be a waste of time since expression matching is what Perl
is so good for.

You'll also need to think about what this percentage argument is. It's
not obvious to me what percentage of closeness "Robert" and "Robret" are
vs. "Robert" and "RobQQto".

ex: http://tomacorp.com/perl/lingua/style.html
http://aspell.sourceforge.net/

Robert

-----Original Message-----
From: adi at roda.ro [mailto:adi at roda.ro] 
Sent: Wednesday, January 05, 2005 12:36 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Tuning string matching


Dear list,

I spent about two hours searching on the message archive, with no avail.
I have a list of people that have to pass an on-line test, but only a
fraction
of them do it. Moreover, as they input their names, the resulting string
do not
always match the names I have in my database.

I would like to do two things:

1. Match any strings that are 90% the same
Example:
name1 <- "Harry Harrington"
name2 <- "Harry Harington"
I need a function that would declare those strings as a match (ideally
having an
argument that would allow introducing 80% instead of 90%)

2. Arrange a final table that would take me from:

Table1 (the complete list of people from my database)
No Name
1  Byron C. Andrew
2  Friedman Bob
3  Harrington Harry

Table2 (the people having been tested)
No Name               Score
1  Harry Harington    13
2  Byron Andrew       28

to:

No Name1              Name2              Score
1  Byron C. Andrew    Byron Andrew       28
2  Friedman Bob
3  Harrington Harry   Harry Harington    13

Thank you in advance, any help is highly appreciated.
Adrian

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Wed Jan  5 20:40:22 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 5 Jan 2005 14:40:22 -0500
Subject: [R] variance of combinations of means - off topic
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4B0@usrymx25.merck.com>

What I have in mind is the discussion on pp. 167-172 of `S Programming'.

Cheers,
Andy

> From: Spencer Graves
> 
> Hi, Andy: 
> 
>       MASS4 has section 5.7 "Bootstrap and Permutation Methods".  Is 
> this what you are suggesting?  It certainly is relevant to 
> the question 
> (but not to the "delta method", except as a means of checking on it). 
> 
>       Thanks,
>       spencer graves
> 
> Liaw, Andy wrote:
> 
> >You could try googling for "delta method".  I believe MASS 
> even has code for
> >that...
> >
> >Andy
> >
> >  
> >
> >>From: Bill Shipley
> >>
> >>Hello, and please excuse this off-topic question, but I 
> have not been
> >>able to find an answer elsewhere.  Consider a value Z that is 
> >>calculated
> >>using the product (or ratio) of two means X_mean and Y_mean:
> >>Z=X_mean*Y_mean.  More generally, Z=f(X_mean, Y_mean).  The standard
> >>error of Z will be a function of the standard errors of the 
> means of X
> >>and Y.  I want to calculate this se of Z.  Can someone 
> direct me to a
> >>reference (text book or other) that gives the solution to 
> >>this *general*
> >>problem?
> >>
> >>Thanks.
> >>
> >> 
> >>
> >>Bill Shipley
> >>
> >> 
> >>
> >>
> >>	[[alternative HTML version deleted]]
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> >>
> >>
> >>    
> >>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >  
> >
> 
> 
>



From br44114 at yahoo.com  Wed Jan  5 20:46:11 2005
From: br44114 at yahoo.com (bogdan romocea)
Date: Wed, 5 Jan 2005 11:46:11 -0800 (PST)
Subject: [R] Tuning string matching
Message-ID: <20050105194611.43723.qmail@web50301.mail.yahoo.com>

This is a rather complex problem. I'm not aware of an R function /
package that can do something like this, but in case you need to build
it from scratch read
http://support.sas.com/documentation/periodicals/obs/obswww15/index.html
If you're familiar with SAS you could translate the code to R.

HTH,
b.


-----Original Message-----
From: adi at roda.ro
Sent: Wednesday, January 05, 2005 12:36 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Tuning string matching


Dear list,

I spent about two hours searching on the message archive, with no
avail.
I have a list of people that have to pass an on-line test, but only a
fraction
of them do it. Moreover, as they input their names, the resulting
string do not
always match the names I have in my database.

I would like to do two things:

1. Match any strings that are 90% the same
Example:
name1 <- "Harry Harrington"
name2 <- "Harry Harington"
I need a function that would declare those strings as a match (ideally
having an
argument that would allow introducing 80% instead of 90%)

2. Arrange a final table that would take me from:

Table1 (the complete list of people from my database)
No Name
1  Byron C. Andrew
2  Friedman Bob
3  Harrington Harry

Table2 (the people having been tested)
No Name               Score
1  Harry Harington    13
2  Byron Andrew       28

to:

No Name1              Name2              Score
1  Byron C. Andrew    Byron Andrew       28
2  Friedman Bob
3  Harrington Harry   Harry Harington    13

Thank you in advance, any help is highly appreciated.
Adrian

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From colliera at ukzn.ac.za  Wed Jan  5 20:58:59 2005
From: colliera at ukzn.ac.za (Andrew Collier)
Date: Wed, 5 Jan 2005 21:58:59 +0200
Subject: [R] find parameters for a gamma distribution
Message-ID: <20050105195859.GA25395@adelie>

hello,

i have just started exploring R as an alternative to matlab for data analysis. so
+far everything is _very_ promising. i have a question though regarding parameter
+estimation. i have some data which, from a histogram plot, appears to arise from
+a gamma distribution. i gather that you can fit the data to the distribution
+using glm(). i am just not quite sure how this is done in practice... so here is
+a simple example with artificial data:

d <- rgamma(100000, 20, scale = 2)
h <- hist(d, breaks = c(seq(10, 80, 2), 100))

H <- data.frame(x = h$mids, y = h$density)

g <- glm(y ~ x, data = H, family = Gamma)
summary(g)

Call:
glm(formula = y ~ x, family = Gamma, data = H)

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-3.8654  -2.0887  -0.7685   0.7147   1.4508

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  30.4758    26.7258   1.140    0.262
x             1.0394     0.6825   1.523    0.137

(Dispersion parameter for Gamma family taken to be 1.343021)

    Null deviance: 119.51  on 35  degrees of freedom
Residual deviance: 116.28  on 34  degrees of freedom
AIC: -260.49

Number of Fisher Scoring iterations: 7

now i suppose that the estimates parameters are:

        shape = 30.4758
        scale = 1.0394

am i interpreting the output correctly? and, if so, why are these estimates so
+poor? i would, perhaps naively, expected the parameters from an artificial
+sample like this to be pretty good.

my apologies if i am doing something stupid here but my statistics capabilties
+are rather limited!

best regards,
andrew collier.
-- 
Andrew B. Collier

Antarctic Research Fellow                                   tel: +27 31 2601157
Space Physics Research Institute                            fax: +27 31 2616550
University of KwaZulu-Natal, Durban, 4041, South Africa



From ggrothendieck at myway.com  Wed Jan  5 21:26:46 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 5 Jan 2005 20:26:46 +0000 (UTC)
Subject: [R] Targeting the elements that satisfy matching rules
	=?utf-8?b?ZXhjbHVkaW5nCXRoZQ==?= NA
References: <200501051916.14410.bobo@medgen.univr.it>
Message-ID: <loom.20050105T212435-810@post.gmane.org>

Giovanni Malerba <bobo <at> medgen.univr.it> writes:

: 
: Dear R-List,
: here my problem:
: > a <- c("gio","gao","geo",NA,"1","alpha")
: > b <- 1:6
: > data.frame(V1=a,V2=b) -> c
: > c
:      V1 V2
: 1   gio  1
: 2   gao  2
: 3   geo  3
: 4  <NA>  4
: 5     1  5
: 6 alpha  6
: 
: > rownames(c) %in% grep("a",as.character(c$V1))
: [1] FALSE  TRUE FALSE FALSE FALSE  TRUE
: 
: while I would like to obtain
: [1] FALSE  TRUE FALSE <NA> FALSE  TRUE
: 
: Is there a simple way to do this without doing things like
: > rownames(c) %in% grep("a",as.character(c$V1)) -> tmp
: > tmp[is.na(c$V1)]<- NA
: ?
: 
: It would be nice if grep (or another command) produced FALSE or TRUE 
according 
: to the matching rule and NA if the value is NA. I was not able to find such 
a 
: feature (just to do this in one simple command-line :)).
: 


regexpr("a", as.character(c$V1)) > 0



From lmiceli at telos.org.br  Wed Jan  5 22:21:23 2005
From: lmiceli at telos.org.br (Leonardo L Miceli)
Date: Wed, 5 Jan 2005 18:21:23 -0300
Subject: [R] fSeries library
Message-ID: <OF2A11AF65.83E14F88-ON03256F80.0074DD55-03256F80.0075AFA1@telos.org.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050105/2147b8b7/attachment.pl

From elvis at xlsolutions-corp.com  Wed Jan  5 21:34:40 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Wed,  5 Jan 2005 13:34:40 -0700
Subject: [R] Course***R/S-plus Fundamentals and Programming Techniques @ 4
	locations, January 2005
Message-ID: <20050105203440.22321.qmail@webmail04.mesa1.secureserver.net>

 Happy New Year!

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to
announce  2-day "R/S-plus Fundamentals and Programming
Techniques".

****Chicago, IL ---------------------------- January 13th-14th, 2005
****San Francisco, CA ---------------------- January 13th-14th, 2005

****Washington, DC ------------------------- January 27th-28th, 2005
****Boston, MA   --------------------------- January 27th-28th, 2005


Reserve your seat now at the early bird rates! Payment due AFTER
the class.


Course Description:

This two-day beginner to intermediate R/S-plus course focuses on a
broad spectrum of topics, from reading raw data to a comparison of R
and S. We will learn the essentials of data manipulation, graphical
visualization and R/S-plus programming. We will explore statistical
data analysis tools,including graphics with data sets. How to enhance
your plots. We will perform basic statistics and fit linear regression
models. Participants are encouraged to bring data for interactive
sessions


With the following outline:

- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)


Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
classto take advantage of group discount. Register now to secure your
seat! Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com



From bxc at steno.dk  Wed Jan  5 21:47:29 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Wed, 5 Jan 2005 21:47:29 +0100
Subject: [R] plotting percent of incidents within different 'bins'
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FE360@exdkba022.novo.dk>

You want:

tapply( Outcome, predictor, mean )

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Stephen Choularton
> Sent: Wednesday, January 05, 2005 8:35 PM
> To: R Help
> Subject: [R] plotting percent of incidents within different 'bins'
> 
> 
> Hi
>  
> Say I have some data, two columns in a table being a binary 
> outcome plus a predictor and I want to plot a graph that 
> shows the percentage positives of the binary outcome within 
> bands of the predictor, e.g.
>  
>  
> Outcome           predictor
>  
> 0                      1
> 1                      2
> 1                      2
> 0                      3          
> 0                      3
> 0                      2          
> 1                      3
> 1                      4
> 1                      4
> 0                      4
> 0                      4
> 0                      4
> etc
>  
> In this case there are 4 cases in the band 1 - 2 of the 
> predictor, 2 of them are true so the percent is 50% and there 
> are 7 cases in the band 3
> - 4, 3 of which are true making the percentage 43% .
>  
> Is there some function in R that will sum these outcomes by 
> bands of predictor and produce a one by two  data set with 
> the percentages in one column and the ordered bands in the 
> other, or alternately is there some sort of special plot.???? 
> that does it all for you?
>  
> Thanks
>  
> Stephen
>  
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From baron at psych.upenn.edu  Wed Jan  5 21:49:13 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 5 Jan 2005 15:49:13 -0500
Subject: [R] Tuning string matching
In-Reply-To: <20050105194611.43723.qmail@web50301.mail.yahoo.com>
References: <20050105194611.43723.qmail@web50301.mail.yahoo.com>
Message-ID: <20050105204913.GA29023@psych>

Sorry for joining late, but I wanted to see if my search page
could help.  (I don't know which search archive you looked at.)
I entered
fuzzy string match*
and got a few things that look relevant, including the agrep
function.

As for the second part of the question, that seems to be a coding 
problem that is dependent on the current form of your data.
Write me off the list and I'll send you an R script I use for
similar things (making PayPal payments).

Jon

 Dear list,
 
 I spent about two hours searching on the message archive, with no
 avail.
 I have a list of people that have to pass an on-line test, but only a
 fraction
 of them do it. Moreover, as they input their names, the resulting
 string do not
 always match the names I have in my database.
 
 I would like to do two things:
 
 1. Match any strings that are 90% the same
 Example:
 name1 <- "Harry Harrington"
 name2 <- "Harry Harington"
 I need a function that would declare those strings as a match (ideally
 having an
 argument that would allow introducing 80% instead of 90%)

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From qunshi at cs.uchicago.edu  Wed Jan  5 21:55:58 2005
From: qunshi at cs.uchicago.edu (Qun Shi)
Date: Wed, 5 Jan 2005 14:55:58 -0600 (CST)
Subject: [R] (no subject)
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E4AB@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E4AB@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.58.0501051447400.7081@swank.cs.uchicago.edu>

Hi Andy,

Thanks a lot for your promptly response. I searched the whole web site, I
found the source code for version 1.6.X. Since I'm not a computer person,
I don't how to compile it, but what I want is binary file for Windows 2000
so that I could continue to work on my data. Could you or someone kindly make
it for me. I would appreciate a lot.

Thanks for your time, Jean

On Wed, 5 Jan 2005, Liaw, Andy wrote:

> Date: Wed, 5 Jan 2005 13:04:33 -0500
> From: "Liaw, Andy" <andy_liaw at merck.com>
> To: 'Qun Shi' <qunshi at cs.uchicago.edu>, R-help at stat.math.ethz.ch
> Subject: RE: [R] (no subject)
>
> Sources for versions of R as far back as 0.65, I believe, are available on
> CRAN.  You can try to compile from source.
>
> Andy
>
> > From: Qun Shi
> >
> > Hi,
> >
> > I'm trying to use the version of dchip combined with R to
> > analyze my data.
> > I need R version 1.6 which fits for dchip as dchip manual said. So I
> > would appreciate a lot if someone could tell me where I could
> > find this
> > version and download? I'm using Windows 2000.
> >
> > Thanks, Jean
> >
> >
> >
>
>



From james.holtman at convergys.com  Wed Jan  5 22:04:35 2005
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Wed, 5 Jan 2005 16:04:35 -0500
Subject: [R] plotting percent of incidents within different 'bins'
Message-ID: <OF9078900F.779E5E2F-ON85256F80.0073B4C3@nd.convergys.com>





You can use 'cut' to create the breaks..  Actually there are 8 in the 3-4
range:

   Outcome predictor
1        0         1
2        1         2
3        1         2
4        0         3
5        0         3
6        0         2
7        1         3
8        1         4
9        1         4
10       0         4
11       0         4
12       0         4
> cut(x.1$p, breaks=c(0,2,4))
 [1] (0,2] (0,2] (0,2] (2,4] (2,4] (0,2] (2,4] (2,4] (2,4] (2,4] (2,4]
(2,4]
Levels: (0,2] (2,4]
> x.c <- cut(x.1$p, breaks=c(0,2,4))
> tapply(x.1$O, x.c, function(x)sum(x==1)/length(x))
(0,2] (2,4]
0.500 0.375
>
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      "Stephen Choularton"                                                                                                 
                      <mail at bymouth.com>           To:       "R Help" <r-help at stat.math.ethz.ch>                                           
                      Sent by:                     cc:                                                                                     
                      r-help-bounces at stat.m        Subject:  [R] plotting percent of incidents within different 'bins'                     
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      01/05/2005 14:34                                                                                                     
                                                                                                                                           
                                                                                                                                           




Hi

Say I have some data, two columns in a table being a binary outcome plus
a predictor and I want to plot a graph that shows the percentage
positives of the binary outcome within bands of the predictor, e.g.


Outcome           predictor

0                      1
1                      2
1                      2
0                      3
0                      3
0                      2
1                      3
1                      4
1                      4
0                      4
0                      4
0                      4
etc

In this case there are 4 cases in the band 1 - 2 of the predictor, 2 of
them are true so the percent is 50% and there are 7 cases in the band 3
- 4, 3 of which are true making the percentage 43% .

Is there some function in R that will sum these outcomes by bands of
predictor and produce a one by two  data set with the percentages in one
column and the ordered bands in the other, or alternately is there some
sort of special plot.???? that does it all for you?

Thanks

Stephen



             [[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Dax42 at web.de  Wed Jan  5 22:34:04 2005
From: Dax42 at web.de (dax42)
Date: Wed, 5 Jan 2005 22:34:04 +0100
Subject: [R] strange behaviour of negative binomial
Message-ID: <858845F9-5F61-11D9-B442-000393883D7E@web.de>

Dear list,

I ran into a strange behaviour of the pnbinom function - or maybe I 
just made a stupid mistake.
First thing is that pnbinom seems to be very slow. The other - more 
interesting one - is that I get two different curves when I plot the 
estimated density and the density given by pnbinom. Shouldn't it be the 
same?
This is only the case, I think, if I use the parameter size = 1. I just 
tried it for size = 2 and the result seemed correct..

Anyways, this is what I tried:

#parameters
proba<-0.001039302;
s<-1;

#random numbers with this distribution
dom<-rnbinom(10000,s,proba);

#first plot
plot(ecdf(dom),do.points=FALSE);

#second plot
x<-seq(0,8700,0.5);
lines(x,pnbinom(x,s,proba),col="red");



From bill.shipley at usherbrooke.ca  Wed Jan  5 22:39:42 2005
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Wed, 5 Jan 2005 16:39:42 -0500
Subject: [R] cubic spline smoother with heterogeneous variance.
Message-ID: <004b01c4f36f$1119a7f0$ae1ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050105/90593751/attachment.pl

From p.dalgaard at biostat.ku.dk  Wed Jan  5 22:59:16 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Jan 2005 22:59:16 +0100
Subject: [R] find parameters for a gamma distribution
In-Reply-To: <20050105195859.GA25395@adelie>
References: <20050105195859.GA25395@adelie>
Message-ID: <x2u0pvfsgb.fsf@biostat.ku.dk>

Andrew Collier <colliera at ukzn.ac.za> writes:

> hello,
> 
> i have just started exploring R as an alternative to matlab for data analysis. so
> +far everything is _very_ promising. i have a question though regarding parameter
> +estimation. i have some data which, from a histogram plot, appears to arise from
> +a gamma distribution. i gather that you can fit the data to the distribution
> +using glm(). i am just not quite sure how this is done in practice... so here is
> +a simple example with artificial data:
> 
> d <- rgamma(100000, 20, scale = 2)
> h <- hist(d, breaks = c(seq(10, 80, 2), 100))
> 
> H <- data.frame(x = h$mids, y = h$density)
> 
> g <- glm(y ~ x, data = H, family = Gamma)
> summary(g)
> 
> Call:
> glm(formula = y ~ x, family = Gamma, data = H)
> 
> Deviance Residuals:
>     Min       1Q   Median       3Q      Max
> -3.8654  -2.0887  -0.7685   0.7147   1.4508
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept)  30.4758    26.7258   1.140    0.262
> x             1.0394     0.6825   1.523    0.137
> 
> (Dispersion parameter for Gamma family taken to be 1.343021)
> 
>     Null deviance: 119.51  on 35  degrees of freedom
> Residual deviance: 116.28  on 34  degrees of freedom
> AIC: -260.49
> 
> Number of Fisher Scoring iterations: 7
> 
> now i suppose that the estimates parameters are:
> 
>         shape = 30.4758
>         scale = 1.0394
> 
> am i interpreting the output correctly? and, if so, why are these estimates so
> +poor? i would, perhaps naively, expected the parameters from an artificial
> +sample like this to be pretty good.
> 
> my apologies if i am doing something stupid here but my statistics capabilties
> +are rather limited!

Didn't you get a pointer to fitdistr() the last time you posted this?

To quote an old teacher of mine, what you're doing above is "incorrect
in every detail". glm() just isn't for fitting densities to histograms. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From bamelbourne at ucdavis.edu  Wed Jan  5 23:11:25 2005
From: bamelbourne at ucdavis.edu (Brett Melbourne)
Date: Wed, 5 Jan 2005 14:11:25 -0800
Subject: [R] find parameters for a gamma distribution
References: <20050105195859.GA25395@adelie>
Message-ID: <003701c4f373$82f21bb0$0213eda9@des.ucdavis.edu>

You want:
library(MASS)
?fitdist

cheers
Brett

Brett Melbourne, Postdoctoral Fellow
Biological Invasions IGERT www.cpb.ucdavis.edu/bioinv
Center for Population Biology
University of California Davis CA 95616


----- Original Message ----- 
From: "Andrew Collier" <colliera at ukzn.ac.za>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, January 05, 2005 11:58 AM
Subject: [R] find parameters for a gamma distribution


> hello,
>
> i have just started exploring R as an alternative to matlab for data 
> analysis. so
> +far everything is _very_ promising. i have a question though regarding 
> parameter
> +estimation. i have some data which, from a histogram plot, appears to 
> arise from
> +a gamma distribution. i gather that you can fit the data to the 
> distribution
> +using glm(). i am just not quite sure how this is done in practice... so 
> here is
> +a simple example with artificial data:
>
> d <- rgamma(100000, 20, scale = 2)
> h <- hist(d, breaks = c(seq(10, 80, 2), 100))
>
> H <- data.frame(x = h$mids, y = h$density)
>
> g <- glm(y ~ x, data = H, family = Gamma)
> summary(g)
>
> Call:
> glm(formula = y ~ x, family = Gamma, data = H)
>
> Deviance Residuals:
>    Min       1Q   Median       3Q      Max
> -3.8654  -2.0887  -0.7685   0.7147   1.4508
>
> Coefficients:
>            Estimate Std. Error t value Pr(>|t|)
> (Intercept)  30.4758    26.7258   1.140    0.262
> x             1.0394     0.6825   1.523    0.137
>
> (Dispersion parameter for Gamma family taken to be 1.343021)
>
>    Null deviance: 119.51  on 35  degrees of freedom
> Residual deviance: 116.28  on 34  degrees of freedom
> AIC: -260.49
>
> Number of Fisher Scoring iterations: 7
>
> now i suppose that the estimates parameters are:
>
>        shape = 30.4758
>        scale = 1.0394
>
> am i interpreting the output correctly? and, if so, why are these 
> estimates so
> +poor? i would, perhaps naively, expected the parameters from an 
> artificial
> +sample like this to be pretty good.
>
> my apologies if i am doing something stupid here but my statistics 
> capabilties
> +are rather limited!
>
> best regards,
> andrew collier.
> -- 
> Andrew B. Collier
>
> Antarctic Research Fellow                                   tel: +27 31 
> 2601157
> Space Physics Research Institute                            fax: +27 31 
> 2616550
> University of KwaZulu-Natal, Durban, 4041, South Africa
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Jan  5 23:15:37 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Jan 2005 22:15:37 +0000 (GMT)
Subject: [R] find parameters for a gamma distribution
In-Reply-To: <20050105195859.GA25395@adelie>
References: <20050105195859.GA25395@adelie>
Message-ID: <Pine.LNX.4.61.0501052205540.29969@gannet.stats>

First, you want to fit the data not the histogram counts (binned data).

Second, glm does not do a very principled fit of a gamma (it is a moment 
estimator).  Something like

> d <- rgamma(1000, 20, scale = 2)
> summary(glm(d ~ 1, Gamma))
Coefficients:
              Estimate Std. Error t value Pr(>|t|)
(Intercept) 0.0253072  0.0001822   138.9   <2e-16

(Dispersion parameter for Gamma family taken to be 0.05185392)

where the shape = 1/0.05185392 (no s.e.) and 0.0253072 is the mean
= 1/(scale*shape)

Try instead (on 100000 obs here)

library(MASS)
fitdistr(d, "Gamma")
       shape           rate
   19.812444027    0.495157589
  ( 0.087858938) ( 0.002223778)

which works for me.  You can make it use scale =1/rate by
> fitdistr(d, dgamma, start=list(shape=20,scale=2))
       shape          scale
   19.812702659    2.019419771
  ( 0.087774433) ( 0.009060263)


On Wed, 5 Jan 2005, Andrew Collier wrote:

> hello,
>
> i have just started exploring R as an alternative to matlab for data analysis. so
> +far everything is _very_ promising. i have a question though regarding parameter
> +estimation. i have some data which, from a histogram plot, appears to arise from
> +a gamma distribution. i gather that you can fit the data to the distribution
> +using glm(). i am just not quite sure how this is done in practice... so here is
> +a simple example with artificial data:
>
> d <- rgamma(100000, 20, scale = 2)
> h <- hist(d, breaks = c(seq(10, 80, 2), 100))
>
> H <- data.frame(x = h$mids, y = h$density)
>
> g <- glm(y ~ x, data = H, family = Gamma)
> summary(g)
>
> Call:
> glm(formula = y ~ x, family = Gamma, data = H)
>
> Deviance Residuals:
>    Min       1Q   Median       3Q      Max
> -3.8654  -2.0887  -0.7685   0.7147   1.4508
>
> Coefficients:
>            Estimate Std. Error t value Pr(>|t|)
> (Intercept)  30.4758    26.7258   1.140    0.262
> x             1.0394     0.6825   1.523    0.137
>
> (Dispersion parameter for Gamma family taken to be 1.343021)
>
>    Null deviance: 119.51  on 35  degrees of freedom
> Residual deviance: 116.28  on 34  degrees of freedom
> AIC: -260.49
>
> Number of Fisher Scoring iterations: 7
>
> now i suppose that the estimates parameters are:
>
>        shape = 30.4758
>        scale = 1.0394
>
> am i interpreting the output correctly? and, if so, why are these estimates so
> +poor? i would, perhaps naively, expected the parameters from an artificial
> +sample like this to be pretty good.
>
> my apologies if i am doing something stupid here but my statistics capabilties
> +are rather limited!
>
> best regards,
> andrew collier.
> -- 
> Andrew B. Collier
>
> Antarctic Research Fellow                                   tel: +27 31 2601157
> Space Physics Research Institute                            fax: +27 31 2616550
> University of KwaZulu-Natal, Durban, 4041, South Africa
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bamelbourne at ucdavis.edu  Wed Jan  5 23:16:54 2005
From: bamelbourne at ucdavis.edu (Brett Melbourne)
Date: Wed, 5 Jan 2005 14:16:54 -0800
Subject: [R] find parameters for a gamma distribution
Message-ID: <006001c4f374$4733a7a0$0213eda9@des.ucdavis.edu>

Actually, that should be:
library(MASS)
?fitdistr

> You want:
> library(MASS)
> ?fitdist
>
> cheers
> Brett
>
> Brett Melbourne, Postdoctoral Fellow
> Biological Invasions IGERT www.cpb.ucdavis.edu/bioinv
> Center for Population Biology
> University of California Davis CA 95616
>
>
> ----- Original Message ----- 
> From: "Andrew Collier" <colliera at ukzn.ac.za>
> To: <r-help at stat.math.ethz.ch>
> Sent: Wednesday, January 05, 2005 11:58 AM
> Subject: [R] find parameters for a gamma distribution
>
>
>> hello,
>>
>> i have just started exploring R as an alternative to matlab for data 
>> analysis. so
>> +far everything is _very_ promising. i have a question though regarding 
>> parameter
>> +estimation. i have some data which, from a histogram plot, appears to 
>> arise from
>> +a gamma distribution. i gather that you can fit the data to the 
>> distribution
>> +using glm(). i am just not quite sure how this is done in practice... so 
>> here is
>> +a simple example with artificial data:
>>
>> d <- rgamma(100000, 20, scale = 2)
>> h <- hist(d, breaks = c(seq(10, 80, 2), 100))
>>
>> H <- data.frame(x = h$mids, y = h$density)
>>
>> g <- glm(y ~ x, data = H, family = Gamma)
>> summary(g)
>>
>> Call:
>> glm(formula = y ~ x, family = Gamma, data = H)
>>
>> Deviance Residuals:
>>    Min       1Q   Median       3Q      Max
>> -3.8654  -2.0887  -0.7685   0.7147   1.4508
>>
>> Coefficients:
>>            Estimate Std. Error t value Pr(>|t|)
>> (Intercept)  30.4758    26.7258   1.140    0.262
>> x             1.0394     0.6825   1.523    0.137
>>
>> (Dispersion parameter for Gamma family taken to be 1.343021)
>>
>>    Null deviance: 119.51  on 35  degrees of freedom
>> Residual deviance: 116.28  on 34  degrees of freedom
>> AIC: -260.49
>>
>> Number of Fisher Scoring iterations: 7
>>
>> now i suppose that the estimates parameters are:
>>
>>        shape = 30.4758
>>        scale = 1.0394
>>
>> am i interpreting the output correctly? and, if so, why are these 
>> estimates so
>> +poor? i would, perhaps naively, expected the parameters from an 
>> artificial
>> +sample like this to be pretty good.
>>
>> my apologies if i am doing something stupid here but my statistics 
>> capabilties
>> +are rather limited!
>>
>> best regards,
>> andrew collier.
>> -- 
>> Andrew B. Collier
>>
>> Antarctic Research Fellow                                   tel: +27 31 
>> 2601157
>> Space Physics Research Institute                            fax: +27 31 
>> 2616550
>> University of KwaZulu-Natal, Durban, 4041, South Africa
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Wed Jan  5 23:16:46 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Jan 2005 23:16:46 +0100
Subject: [R] (no subject)
In-Reply-To: <Pine.LNX.4.58.0501051447400.7081@swank.cs.uchicago.edu>
References: <3A822319EB35174CA3714066D590DCD50994E4AB@usrymx25.merck.com>
	<Pine.LNX.4.58.0501051447400.7081@swank.cs.uchicago.edu>
Message-ID: <x2pt0jfrn5.fsf@biostat.ku.dk>

Qun Shi <qunshi at cs.uchicago.edu> writes:

> Hi Andy,
> 
> Thanks a lot for your promptly response. I searched the whole web site, I
> found the source code for version 1.6.X. Since I'm not a computer person,
> I don't how to compile it, but what I want is binary file for Windows 2000
> so that I could continue to work on my data. Could you or someone kindly make
> it for me. I would appreciate a lot.
> 
> Thanks for your time, Jean

As far as I can see, dChip is a non-free program which sponges off R
in quite a dubious way, license-wise. I see no reason why we should
support it by supplying binaries of outdated versions of R. I.e., it
is your problem, or possibly the dChip developers'. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tlumley at u.washington.edu  Wed Jan  5 23:18:55 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 5 Jan 2005 14:18:55 -0800 (PST)
Subject: [R] strange behaviour of negative binomial
In-Reply-To: <858845F9-5F61-11D9-B442-000393883D7E@web.de>
References: <858845F9-5F61-11D9-B442-000393883D7E@web.de>
Message-ID: <Pine.A41.4.61b.0501051413530.59084@homer03.u.washington.edu>

On Wed, 5 Jan 2005, dax42 wrote:

> Dear list,
>
> I ran into a strange behaviour of the pnbinom function - or maybe I just made 
> a stupid mistake.
> First thing is that pnbinom seems to be very slow. The other - more 
> interesting one - is that I get two different curves when I plot the 
> estimated density and the density given by pnbinom. Shouldn't it be the same?

Yes, up to sampling error. That's actually one of the sets of tests that R 
does for the random number generators (though we can't test all possible 
parameters).

> This is only the case, I think, if I use the parameter size = 1. I just tried 
> it for size = 2 and the result seemed correct..
>
> Anyways, this is what I tried:
>
> #parameters
> proba<-0.001039302;
> s<-1;
>
> #random numbers with this distribution
> dom<-rnbinom(10000,s,proba);
>
> #first plot
> plot(ecdf(dom),do.points=FALSE);
>
> #second plot
> x<-seq(0,8700,0.5);
> lines(x,pnbinom(x,s,proba),col="red");
>

I get almost identical curves, on both R 2.0.0 under Linux and R-devel on 
Mac (and it only takes half a second on the local computer and 1.25s 
across a couple of miles of network, so it isn't that slow).

 	-thomas



From ripley at stats.ox.ac.uk  Wed Jan  5 23:25:53 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 5 Jan 2005 22:25:53 +0000 (GMT)
Subject: [R] find parameters for a gamma distribution
In-Reply-To: <Pine.LNX.4.61.0501052205540.29969@gannet.stats>
References: <20050105195859.GA25395@adelie>
	<Pine.LNX.4.61.0501052205540.29969@gannet.stats>
Message-ID: <Pine.LNX.4.61.0501052223170.29969@gannet.stats>

On Wed, 5 Jan 2005, Prof Brian Ripley wrote:

> First, you want to fit the data not the histogram counts (binned data).
>
> Second, glm does not do a very principled fit of a gamma (it is a moment 
> estimator).  Something like
>
>> d <- rgamma(1000, 20, scale = 2)
>> summary(glm(d ~ 1, Gamma))
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept) 0.0253072  0.0001822   138.9   <2e-16
>
> (Dispersion parameter for Gamma family taken to be 0.05185392)
>
> where the shape = 1/0.05185392 (no s.e.) and 0.0253072 is the mean
> = 1/(scale*shape)

[In case it confuses you, that got garbled in a cut-and-paste:

0.0253072 is the rate = 1/mean = 1/(scale*shape) as the default link for 
the Gamma glm is reciprocal.

]

>
> Try instead (on 100000 obs here)
>
> library(MASS)
> fitdistr(d, "Gamma")
>      shape           rate
>  19.812444027    0.495157589
> ( 0.087858938) ( 0.002223778)
>
> which works for me.  You can make it use scale =1/rate by
>> fitdistr(d, dgamma, start=list(shape=20,scale=2))
>      shape          scale
>  19.812702659    2.019419771
> ( 0.087774433) ( 0.009060263)
>
>
> On Wed, 5 Jan 2005, Andrew Collier wrote:
>
>> hello,
>> 
>> i have just started exploring R as an alternative to matlab for data 
>> analysis. so
>> +far everything is _very_ promising. i have a question though regarding 
>> parameter
>> +estimation. i have some data which, from a histogram plot, appears to 
>> arise from
>> +a gamma distribution. i gather that you can fit the data to the 
>> distribution
>> +using glm(). i am just not quite sure how this is done in practice... so 
>> here is
>> +a simple example with artificial data:
>> 
>> d <- rgamma(100000, 20, scale = 2)
>> h <- hist(d, breaks = c(seq(10, 80, 2), 100))
>> 
>> H <- data.frame(x = h$mids, y = h$density)
>> 
>> g <- glm(y ~ x, data = H, family = Gamma)
>> summary(g)
>> 
>> Call:
>> glm(formula = y ~ x, family = Gamma, data = H)
>> 
>> Deviance Residuals:
>>    Min       1Q   Median       3Q      Max
>> -3.8654  -2.0887  -0.7685   0.7147   1.4508
>> 
>> Coefficients:
>>            Estimate Std. Error t value Pr(>|t|)
>> (Intercept)  30.4758    26.7258   1.140    0.262
>> x             1.0394     0.6825   1.523    0.137
>> 
>> (Dispersion parameter for Gamma family taken to be 1.343021)
>> 
>>    Null deviance: 119.51  on 35  degrees of freedom
>> Residual deviance: 116.28  on 34  degrees of freedom
>> AIC: -260.49
>> 
>> Number of Fisher Scoring iterations: 7
>> 
>> now i suppose that the estimates parameters are:
>> 
>>        shape = 30.4758
>>        scale = 1.0394
>> 
>> am i interpreting the output correctly? and, if so, why are these estimates 
>> so
>> +poor? i would, perhaps naively, expected the parameters from an artificial
>> +sample like this to be pretty good.
>> 
>> my apologies if i am doing something stupid here but my statistics 
>> capabilties
>> +are rather limited!
>> 
>> best regards,
>> andrew collier.
>> -- 
>> Andrew B. Collier
>> 
>> Antarctic Research Fellow                                   tel: +27 31 
>> 2601157
>> Space Physics Research Institute                            fax: +27 31 
>> 2616550
>> University of KwaZulu-Natal, Durban, 4041, South Africa
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> 
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Wed Jan  5 23:26:00 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 5 Jan 2005 17:26:00 -0500
Subject: [R] (no subject)
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4B1@usrymx25.merck.com>

Googling for `rw1061.exe' turned up:
http://www.cipic.ucdavis.edu/~dmrocke/Class/EAD289D/R/rw1061.exe

Andy

> From: Qun Shi 
> 
> Hi Andy,
> 
> Thanks a lot for your promptly response. I searched the whole 
> web site, I
> found the source code for version 1.6.X. Since I'm not a 
> computer person,
> I don't how to compile it, but what I want is binary file for 
> Windows 2000
> so that I could continue to work on my data. Could you or 
> someone kindly make
> it for me. I would appreciate a lot.
> 
> Thanks for your time, Jean
> 
> On Wed, 5 Jan 2005, Liaw, Andy wrote:
> 
> > Date: Wed, 5 Jan 2005 13:04:33 -0500
> > From: "Liaw, Andy" <andy_liaw at merck.com>
> > To: 'Qun Shi' <qunshi at cs.uchicago.edu>, R-help at stat.math.ethz.ch
> > Subject: RE: [R] (no subject)
> >
> > Sources for versions of R as far back as 0.65, I believe, 
> are available on
> > CRAN.  You can try to compile from source.
> >
> > Andy
> >
> > > From: Qun Shi
> > >
> > > Hi,
> > >
> > > I'm trying to use the version of dchip combined with R to
> > > analyze my data.
> > > I need R version 1.6 which fits for dchip as dchip manual 
> said. So I
> > > would appreciate a lot if someone could tell me where I could
> > > find this
> > > version and download? I'm using Windows 2000.
> > >
> > > Thanks, Jean
> > >
> > >
> > >
> >
> >
> 
> 
>



From p.dalgaard at biostat.ku.dk  Wed Jan  5 23:32:36 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Jan 2005 23:32:36 +0100
Subject: [R] strange behaviour of negative binomial
In-Reply-To: <858845F9-5F61-11D9-B442-000393883D7E@web.de>
References: <858845F9-5F61-11D9-B442-000393883D7E@web.de>
Message-ID: <x2llb7fqwr.fsf@biostat.ku.dk>

dax42 <Dax42 at web.de> writes:

> Dear list,
> 
> I ran into a strange behaviour of the pnbinom function - or maybe I
> just made a stupid mistake.
> First thing is that pnbinom seems to be very slow. The other - more
> interesting one - is that I get two different curves when I plot the
> estimated density and the density given by pnbinom. Shouldn't it be
> the same?
> This is only the case, I think, if I use the parameter size = 1. I
> just tried it for size = 2 and the result seemed correct..
> 
> Anyways, this is what I tried:
> 
> #parameters
> proba<-0.001039302;
> s<-1;
> 
> #random numbers with this distribution
> dom<-rnbinom(10000,s,proba);
> 
> #first plot
> plot(ecdf(dom),do.points=FALSE);
> 
> #second plot
> x<-seq(0,8700,0.5);
> lines(x,pnbinom(x,s,proba),col="red");

Errrr... What's the problem here? The two curves look quite similar to
me. (And both are of course cumulative distribution functions and not
densities, but it doesn't sound like that is the problem.) 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at myway.com  Wed Jan  5 23:36:00 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 5 Jan 2005 22:36:00 +0000 (UTC)
Subject: [R] strange behaviour of negative binomial
References: <858845F9-5F61-11D9-B442-000393883D7E@web.de>
Message-ID: <loom.20050105T233420-67@post.gmane.org>

dax42 <Dax42 <at> web.de> writes:

: 
: Dear list,
: 
: I ran into a strange behaviour of the pnbinom function - or maybe I 
: just made a stupid mistake.
: First thing is that pnbinom seems to be very slow. The other - more 
: interesting one - is that I get two different curves when I plot the 
: estimated density and the density given by pnbinom. Shouldn't it be the 
: same?
: This is only the case, I think, if I use the parameter size = 1. I just 
: tried it for size = 2 and the result seemed correct..
: 
: Anyways, this is what I tried:
: 
: #parameters
: proba<-0.001039302;
: s<-1;
: 
: #random numbers with this distribution
: dom<-rnbinom(10000,s,proba);
: 
: #first plot
: plot(ecdf(dom),do.points=FALSE);
: 
: #second plot
: x<-seq(0,8700,0.5);
: lines(x,pnbinom(x,s,proba),col="red");

There was a change in ecdf in R 2.1.0 so try the development version
of R and see if there still is a problem with that.

BTW, you can omit the trailing the semicolons.



From Robert.McGehee at geodecapital.com  Wed Jan  5 23:51:47 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Wed, 5 Jan 2005 17:51:47 -0500
Subject: [R] Rcmd check  Error help
Message-ID: <67DCA285A2D7754280D3B8E88EB5480206741DFD@MSGBOSCLB2WIN.DMN1.FMR.COM>

R-help,
I'm the primary developer for an increasingly large R package with over
three thousand lines of code. Unfortunately, do the complexity of the
code, I sometimes am required to change several interoperating parts of
the package before testing for bugs and performance. And sometimes
unnoticed syntax errors slip in that cause Rcmd check / INSTALL to fail
with such messages as:

Error in parse(file, n, text, prompt) : syntax error on line 223
Execution halted

My question: what file should I be checking for line 223? When running
Rcmd INSTALL, the previous version is restored on an install fail, so I
can't check the install directory, the Rcmd check directory only
produces two log files, and my R-package has at least a dozen files to
choose from.

Aside from brute force commenting out file by file until the syntax
error disappears (certainly a wasteful use of time), how can I find out
where my error is?

Thank you,
Robert

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R    
       
Robert McGehee
Geode Capital Management, LLC
53 State Street, 5th Floor | Boston, MA | 02109
Tel: 617/392-8396    Fax:617/476-6389
mailto:robert.mcgehee at geodecapital.com



This e-mail, and any attachments hereto, are intended for use by the
addressee(s) only and may contain information that is (i) confidential
information of Geode Capital Management, LLC and/or its affiliates,
and/or (ii) proprietary information of Geode Capital Management, LLC
and/or its affiliates. If you are not the intended recipient of this
e-mail, or if you have otherwise received this e-mail in error, please
immediately notify me by telephone (you may call collect), or by e-mail,
and please permanently delete the original, any print outs and any
copies of the foregoing. Any dissemination, distribution or copying of
this e-mail is strictly prohibited.



From john.fisler at latticesemi.com  Wed Jan  5 23:51:17 2005
From: john.fisler at latticesemi.com (John Fisler)
Date: Wed, 05 Jan 2005 14:51:17 -0800
Subject: [R] Using the Rprofile file to automatically plot data on Startup of
 R version 2.0.1.
Message-ID: <41DC6F65.9030703@latticesemi.com>

Dear R Help Members,

I have some R functions that plot semiconductor data.  I would like to 
automate these plots for individuals in our group such that they don't 
have to know R.  I have read the R help manuals and postings but have 
not found this problem.

I am using R version 2.0.1 under a Windows 2000 operating system.

The following is a simplified version of what I am tring to do:

If I insert the following R code in the Rprofile file located in 
directory 'C:\Program Files\R\rw2001\etc':

x <- c(1,2,3,4,5)
y <- c(2,5,6,3,10)
plot(x,y)

Using the Rgui.exe or the R.exe terminal version of R the following 
message appears:

Error: couldn't find function "plot"

If I copy and paste the command 'plot(x,y)' into the R Console Window 
the scatter plot appears in the R Graphics Device window as expected.

I have also tried putting the above code in a 'R' script file and using 
the source command but I get the same error message.

Any help would be appreciated.

Thank you.

John Fisler



From Dax42 at web.de  Wed Jan  5 23:53:39 2005
From: Dax42 at web.de (dax42)
Date: Wed, 5 Jan 2005 23:53:39 +0100
Subject: [R] strange behaviour of negative binomial
In-Reply-To: <x2llb7fqwr.fsf@biostat.ku.dk>
References: <858845F9-5F61-11D9-B442-000393883D7E@web.de>
	<x2llb7fqwr.fsf@biostat.ku.dk>
Message-ID: <A3B3C3EC-5F6C-11D9-B442-000393883D7E@web.de>

Hi,

I am sorry for the incorrect terms. Of course I mean the cumulative 
distribution functions.
As I just got two answers telling me my problem does not exist, I tried 
again on a different setup - and indeed it works.

So, my first run was done on a Mac OS X 10.3.7 running R 2.0.0, GUI 
version.
Now I tried it on the same system, only I used the R 2.0.0 on my X11, 
not the GUI version.

First setup produces the error (and is VERY slow, talking about several 
minutes here...)
Second works fine.

Does anybody have a similar setup and could try this out as well?
Thanks!
Dax

Am 05.01.2005 um 23:32 schrieb Peter Dalgaard:

> dax42 <Dax42 at web.de> writes:
>
>> Dear list,
>>
>> I ran into a strange behaviour of the pnbinom function - or maybe I
>> just made a stupid mistake.
>> First thing is that pnbinom seems to be very slow. The other - more
>> interesting one - is that I get two different curves when I plot the
>> estimated density and the density given by pnbinom. Shouldn't it be
>> the same?
>> This is only the case, I think, if I use the parameter size = 1. I
>> just tried it for size = 2 and the result seemed correct..
>>
>> Anyways, this is what I tried:
>>
>> #parameters
>> proba<-0.001039302;
>> s<-1;
>>
>> #random numbers with this distribution
>> dom<-rnbinom(10000,s,proba);
>>
>> #first plot
>> plot(ecdf(dom),do.points=FALSE);
>>
>> #second plot
>> x<-seq(0,8700,0.5);
>> lines(x,pnbinom(x,s,proba),col="red");
>
> Errrr... What's the problem here? The two curves look quite similar to
> me. (And both are of course cumulative distribution functions and not
> densities, but it doesn't sound like that is the problem.)
>
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>



From thpe at hhbio.wasser.tu-dresden.de  Wed Jan  5 23:56:58 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Wed, 05 Jan 2005 23:56:58 +0100
Subject: [R] find parameters for a gamma distribution
In-Reply-To: <20050105195859.GA25395@adelie>
References: <20050105195859.GA25395@adelie>
Message-ID: <41DC70BA.2040200@hhbio.wasser.tu-dresden.de>

Andrew Collier wrote:
> hello,
> 
> i have just started exploring R as an alternative to matlab for data analysis. so
> +far everything is _very_ promising. i have a question though regarding parameter
> +estimation. i have some data which, from a histogram plot, appears to arise from
> +a gamma distribution. i gather that you can fit the data to the distribution
> +using glm(). i am just not quite sure how this is done in practice... so here is
> +a simple example with artificial data:
> 
> d <- rgamma(100000, 20, scale = 2)
> h <- hist(d, breaks = c(seq(10, 80, 2), 100))
> 

[...]

The help page of ?dgamma says:

"The mean and variance are E(X) = a*s and Var(X) = a*s^2."

So, to estimate the parameters in your example, try the following:

 > d <- rgamma(100000, 20, scale = 2)
 > var(d)/mean(d)
[1] 1.992091
 > mean(d)^2/var(d)
[1] 20.09559

... and you may use these as start values, see ?fitdistr, e.g

 > fitdistr(d, dgamma, list(shape = 20.1, scale = 1.99))

or simply

 > fitdistr(d, "gamma")

Hint: scale=1/rate

You mentioned "glm", but this is something completely different. It is 
used to fit generalized linear models, see the extra chapter no. 31 of 
Crawley's book on:

http://www.bio.ic.ac.uk/research/mjcraw/statcomp/chapters.htm

Thomas P.



From thpe at hhbio.wasser.tu-dresden.de  Thu Jan  6 00:01:03 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 06 Jan 2005 00:01:03 +0100
Subject: [R] find parameters for a gamma distribution
In-Reply-To: <20050105195859.GA25395@adelie>
References: <20050105195859.GA25395@adelie>
Message-ID: <41DC71AF.4000801@hhbio.wasser.tu-dresden.de>

Andrew Collier wrote:
> hello,
> 
> i have just started exploring R as an alternative to matlab for data analysis. so
> +far everything is _very_ promising. i have a question though regarding parameter
> +estimation. i have some data which, from a histogram plot, appears to arise from
> +a gamma distribution. i gather that you can fit the data to the distribution
> +using glm(). i am just not quite sure how this is done in practice... so here is
> +a simple example with artificial data:
> 
> d <- rgamma(100000, 20, scale = 2)
> h <- hist(d, breaks = c(seq(10, 80, 2), 100))
> 

[...]

The help page of ?dgamma says:

"The mean and variance are E(X) = a*s and Var(X) = a*s^2."

So, to estimate the parameters in your example, try the following:

> d <- rgamma(100000, 20, scale = 2)
> var(d)/mean(d)
[1] 1.992091
> mean(d)^2/var(d)
[1] 20.09559

... and you may use these as start values, see ?fitdistr, e.g

> fitdistr(d, dgamma, list(shape = 20.1, scale = 1.99))

or simply

> fitdistr(d, "gamma")

Hint: scale=1/rate

You mentioned "glm", but this is something completely different. It is
used to fit generalized linear models, see Venables and Ripley's book or 
the extra chapter no. 31 of  Crawley's book on:

http://www.bio.ic.ac.uk/research/mjcraw/statcomp/chapters.htm

Thomas P.



From thpe at hhbio.wasser.tu-dresden.de  Thu Jan  6 00:18:50 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 06 Jan 2005 00:18:50 +0100
Subject: [R] find parameters for a gamma distribution
In-Reply-To: <20050105195859.GA25395@adelie>
References: <20050105195859.GA25395@adelie>
Message-ID: <41DC75DA.5020201@hhbio.wasser.tu-dresden.de>

The help page of ?dgamma says:

"The mean and variance are E(X) = a*s and Var(X) = a*s^2."

So, to estimate the parameters in your example, try the following:

 > d <- rgamma(100000, 20, scale = 2)
 > var(d)/mean(d)
[1] 1.992091
 > mean(d)^2/var(d)
[1] 20.09559

... you may use these as start values for fitdistr, e.g

 > fitdistr(d, dgamma, list(shape = 20.1, scale = 1.99))

or simply

 > fitdistr(d, "gamma")

Hint: scale=1/rate

You mentioned "glm", but this is something completely different. It is 
used to fit generalized linear models, see the extra chapter no. 31 of 
Crawley's book on:

http://www.bio.ic.ac.uk/research/mjcraw/statcomp/chapters.htm

Thomas P.



From kjetil at acelerate.com  Thu Jan  6 00:18:54 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 05 Jan 2005 19:18:54 -0400
Subject: [R] variance of combinations of means - off topic
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E4AC@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E4AC@usrymx25.merck.com>
Message-ID: <41DC75DE.3020301@acelerate.com>

Liaw, Andy wrote:

>You could try googling for "delta method".  I believe MASS even has code for
>that...
>
>Andy
>
>  
>
If you have the original data you can bootstrap --- else you need the 
standar errors and
correlation between the means, and can use the delta methos as above. 
You could even
use D or deriv or deriv3 to get the derivatives automatically.

We learnt this (the delta method) in physics class in high school, to be 
able to write
the lab reports.

Kjetil

>>From: Bill Shipley
>>
>>Hello, and please excuse this off-topic question, but I have not been
>>able to find an answer elsewhere.  Consider a value Z that is 
>>calculated
>>using the product (or ratio) of two means X_mean and Y_mean:
>>Z=X_mean*Y_mean.  More generally, Z=f(X_mean, Y_mean).  The standard
>>error of Z will be a function of the standard errors of the means of X
>>and Y.  I want to calculate this se of Z.  Can someone direct me to a
>>reference (text book or other) that gives the solution to 
>>this *general*
>>problem?
>>
>>Thanks.
>>
>> 
>>
>>Bill Shipley
>>
>> 
>>
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From aschuh at atmos.colostate.edu  Thu Jan  6 00:34:56 2005
From: aschuh at atmos.colostate.edu (Andrew Schuh)
Date: Wed, 05 Jan 2005 16:34:56 -0700
Subject: [R] RSvgDevice "stroke" question
Message-ID: <41DC79A0.1030905@atmos.colostate.edu>

I had a quick question on the RSvgDevice package if there are any users 
out there.  I perused the archives and the docs and didn't see anything, 
maybe I missed?  I have created simple boxplots via devSVG() in both 
Windoze and Linux and it seems that there is an attribute difference in 
that the Linux generated version has a "stroke:none" instead of  the 
"stoke:#000000" that  the Windoze version is generating.  Therefore the 
lines aren't visible from the Linux generated version. It doesn't seem 
like I had issues like this in the past but I'm not sure.  Does anybody 
have any info on this?  Thanks in advance for any responses.

platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    0.0
year     2004
month    10
day      04
language R

I think I'm using the most current version of RSvgDevice, 0.5.3.

-- 
Andrew Schuh

Graduate Research Assistant    Department of Atmospheric Science
Colorado State University      Fort Collins, CO 80523-1317

aschuh at atmos.colostate.edu



From hodgess at gator.uhd.edu  Thu Jan  6 00:40:04 2005
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Wed, 5 Jan 2005 17:40:04 -0600
Subject: [R] fSeries
Message-ID: <200501052340.j05Ne4f23373@gator.dt.uh.edu>

Hi!

For the person who asked about the armaFit from the fSeries
library, here is an example:

> library(its)
>      x1 <- priceIts(instrument = c("^ftse"), start = "1998-01-01",
+                          quote = "Close")
>  fit <- armaFit(x1 ~ arima(1,1,1))
> fit

Call:
armaFit(formula = x1 ~ arima(1, 1, 1))

Model:
ARIMA(1,1,1) with method: CSS-ML

Coefficient(s):
    ar1      ma1  
-0.2562   0.2779  

> 

Hope this helps!

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From ggrothendieck at myway.com  Thu Jan  6 01:13:14 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 6 Jan 2005 00:13:14 +0000 (UTC)
Subject: [R] Rcmd check  Error help
References: <67DCA285A2D7754280D3B8E88EB5480206741DFD@MSGBOSCLB2WIN.DMN1.FMR.COM>
Message-ID: <loom.20050106T010452-769@post.gmane.org>

McGehee, Robert <Robert.McGehee <at> geodecapital.com> writes:

: 
: R-help,
: I'm the primary developer for an increasingly large R package with over
: three thousand lines of code. Unfortunately, do the complexity of the
: code, I sometimes am required to change several interoperating parts of
: the package before testing for bugs and performance. And sometimes
: unnoticed syntax errors slip in that cause Rcmd check / INSTALL to fail
: with such messages as:
: 
: Error in parse(file, n, text, prompt) : syntax error on line 223
: Execution halted
: 
: My question: what file should I be checking for line 223? When running
: Rcmd INSTALL, the previous version is restored on an install fail, so I
: can't check the install directory, the Rcmd check directory only
: produces two log files, and my R-package has at least a dozen files to
: choose from.
: 
: Aside from brute force commenting out file by file until the syntax
: error disappears (certainly a wasteful use of time), how can I find out
: where my error is?

Try concatenating your .R files into one:

   copy *.R all.R

and check the line number in all.R. If that does not work rerun 
the Rcmd INSTALL with all.R instead of the individual files.



From Kiermeier.Andreas at saugov.sa.gov.au  Thu Jan  6 01:17:33 2005
From: Kiermeier.Andreas at saugov.sa.gov.au (Kiermeier, Andreas (PIRSA - SARDI))
Date: Thu, 6 Jan 2005 10:47:33 +1030 
Subject: [R] Using the Rprofile file to automatically plot data on Sta
	rtup of R version 2.0.1.
Message-ID: <0E6A8458D3014647920DDE4B7287BABA06F766A1@sagemsg0012.sagemsmrd01.sa.gov.au>

Dear John,

I belive your problem has to do with the sequence of startup. I think that
.Rprofile is called before the required libraries are attached.

You might like to try putting your code into a .First() function and run it
that way.

Cheers,

Andreas

Dr Andreas Kiermeier
Statistician
SARDI FOOD SAFETY PROGRAM

33 Flemington Street
Glenside   SA   5065
Phone:  +61 8 8207 7884
Fax:       +61 8 8207 7854
Mobile: 0423 028 565

Email: Kiermeier.Andreas at saugov.sa.gov.au
_____________________________

The information in this e-mail and attachments (if any) may be confidential
and/or legally privileged. If you are not the intended recipient, any
disclosure, copying, distribution or action taken is prohibited. SARDI, The
South Australian Research and Development Institute, is the research
division of Primary Industries and Resources (SA) 



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of John Fisler
Sent: Thursday, 6 January 2005 09:21
To: r-help at lists.R-project.org; John Fisler
Subject: [R] Using the Rprofile file to automatically plot data on
Startup of R version 2.0.1.


Dear R Help Members,

I have some R functions that plot semiconductor data.  I would like to 
automate these plots for individuals in our group such that they don't 
have to know R.  I have read the R help manuals and postings but have 
not found this problem.

I am using R version 2.0.1 under a Windows 2000 operating system.

The following is a simplified version of what I am tring to do:

If I insert the following R code in the Rprofile file located in 
directory 'C:\Program Files\R\rw2001\etc':

x <- c(1,2,3,4,5)
y <- c(2,5,6,3,10)
plot(x,y)

Using the Rgui.exe or the R.exe terminal version of R the following 
message appears:

Error: couldn't find function "plot"

If I copy and paste the command 'plot(x,y)' into the R Console Window 
the scatter plot appears in the R Graphics Device window as expected.

I have also tried putting the above code in a 'R' script file and using 
the source command but I get the same error message.

Any help would be appreciated.

Thank you.

John Fisler

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Thu Jan  6 01:24:46 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 6 Jan 2005 00:24:46 +0000 (UTC)
Subject: [R] strange behaviour of negative binomial
References: <858845F9-5F61-11D9-B442-000393883D7E@web.de>
	<x2llb7fqwr.fsf@biostat.ku.dk>
	<A3B3C3EC-5F6C-11D9-B442-000393883D7E@web.de>
Message-ID: <loom.20050106T012129-817@post.gmane.org>


I tried it on Windows XP under both 2.0.1 and 2.1.0 and it seemed to
work both times.  I issued the command:  

set.seed(1)

before both runs to ensure that I was actually using the same
random numbers.  I suggest you try that in all your runs too just 
in case its a problem with some sets of random numbers but not 
others.


dax42 <Dax42 <at> web.de> writes:

: 
: Hi,
: 
: I am sorry for the incorrect terms. Of course I mean the cumulative 
: distribution functions.
: As I just got two answers telling me my problem does not exist, I tried 
: again on a different setup - and indeed it works.
: 
: So, my first run was done on a Mac OS X 10.3.7 running R 2.0.0, GUI 
: version.
: Now I tried it on the same system, only I used the R 2.0.0 on my X11, 
: not the GUI version.
: 
: First setup produces the error (and is VERY slow, talking about several 
: minutes here...)
: Second works fine.
: 
: Does anybody have a similar setup and could try this out as well?
: Thanks!
: Dax
: 
: Am 05.01.2005 um 23:32 schrieb Peter Dalgaard:
: 
: > dax42 <Dax42 <at> web.de> writes:
: >
: >> Dear list,
: >>
: >> I ran into a strange behaviour of the pnbinom function - or maybe I
: >> just made a stupid mistake.
: >> First thing is that pnbinom seems to be very slow. The other - more
: >> interesting one - is that I get two different curves when I plot the
: >> estimated density and the density given by pnbinom. Shouldn't it be
: >> the same?
: >> This is only the case, I think, if I use the parameter size = 1. I
: >> just tried it for size = 2 and the result seemed correct..
: >>
: >> Anyways, this is what I tried:
: >>
: >> #parameters
: >> proba<-0.001039302;
: >> s<-1;
: >>
: >> #random numbers with this distribution
: >> dom<-rnbinom(10000,s,proba);
: >>
: >> #first plot
: >> plot(ecdf(dom),do.points=FALSE);
: >>
: >> #second plot
: >> x<-seq(0,8700,0.5);
: >> lines(x,pnbinom(x,s,proba),col="red");
: >
: > Errrr... What's the problem here? The two curves look quite similar to
: > me. (And both are of course cumulative distribution functions and not
: > densities, but it doesn't sound like that is the problem.)
: >
: > -- 
: >    O__  ---- Peter Dalgaard             Blegdamsvej 3
: >   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
: >  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
: > ~~~~~~~~~~ - (p.dalgaard <at> biostat.ku.dk)             FAX: (+45) 
35327907
: >
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From helprhelp at yahoo.com  Thu Jan  6 01:29:09 2005
From: helprhelp at yahoo.com (Weiwei Shi)
Date: Wed, 5 Jan 2005 16:29:09 -0800 (PST)
Subject: [R] multiple trees
Message-ID: <20050106002909.31447.qmail@web61304.mail.yahoo.com>

Hi, there:
I made a function to do k-fold cross-validation as
below. Basically whenever I call cv(test) for example,
an error message like:
20Fold  1 
Error in model.frame(formula, rownames, variables,
varnames, extras, extranames,  : 
        variable lengths differ

please help. 

My test dataset has 142 variables, the last one is a
categorical response variable.
also, i am not sure how to save the trees into a list
or something so that I can handle, like pointer array
or something in C.

Thanks.

Weiwei Shi, Ph.D

cv<- function(all.data,n.folds=10,mcp=0.003) {

 n <- nrow(all.data)
 idx <- sample(n,n)
 all.data <- all.data[idx,]

 n.each.part <- as.integer(n/n.folds)
 r.model<- vector()
 r.model.prune<- vector()

 for(i in 1:n.folds) {
 cat('Fold ',i,'\n')
 out.fold <- ((i-1)*n.each.part+1):(i*n.each.part)
 tmp<-all.data[-(out.fold),1:141]
 r.model[i]<- rpart(all.data$V142~., data=tmp,
parms=list(split='gini'), cp=0)
 #r.model.prune[i]<-prune(r.model[i], cp=mcp)

 }
return (r.model)
}



From john.fisler at latticesemi.com  Thu Jan  6 01:33:39 2005
From: john.fisler at latticesemi.com (John Fisler)
Date: Wed, 05 Jan 2005 16:33:39 -0800
Subject: [R] 
 Rprofile file to automatically plot data, tried using the .First
 command. 
References: <0E6A8458D3014647920DDE4B7287BABA06F766A1@sagemsg0012.sagemsmrd01.sa.gov.au>
Message-ID: <41DC8763.5080804@latticesemi.com>

Dear Dr. Andreas Kiermeier and the R help Community,

Thank you for your idea on putting the plot command in a .First statement.

I tried putting the plot comamnd in a '.First' statement like the following:

x <- c(1,2,3,4,5)
y <- c(2,5,6,3,10)

.First <- function() {

plot(x,y)

}

and had the same error message:

Error in .First() : couldn't find function "plot"

Should I be put the plot command in a different 'startup' file?

Thanks,

John

the following commands

Kiermeier, Andreas (PIRSA - SARDI) wrote:
> Dear John,
> 
> I belive your problem has to do with the sequence of startup. I think that
> .Rprofile is called before the required libraries are attached.
> 
> You might like to try putting your code into a .First() function and run it
> that way.
> 
> Cheers,
> 
> Andreas
> 
> Dr Andreas Kiermeier
> Statistician
> SARDI FOOD SAFETY PROGRAM
> 
> 33 Flemington Street
> Glenside   SA   5065
> Phone:  +61 8 8207 7884
> Fax:       +61 8 8207 7854
> Mobile: 0423 028 565
> 
> Email: Kiermeier.Andreas at saugov.sa.gov.au
> _____________________________
> 
> The information in this e-mail and attachments (if any) may be confidential
> and/or legally privileged. If you are not the intended recipient, any
> disclosure, copying, distribution or action taken is prohibited. SARDI, The
> South Australian Research and Development Institute, is the research
> division of Primary Industries and Resources (SA) 
> 
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of John Fisler
> Sent: Thursday, 6 January 2005 09:21
> To: r-help at lists.R-project.org; John Fisler
> Subject: [R] Using the Rprofile file to automatically plot data on
> Startup of R version 2.0.1.
> 
> 
> Dear R Help Members,
> 
> I have some R functions that plot semiconductor data.  I would like to 
> automate these plots for individuals in our group such that they don't 
> have to know R.  I have read the R help manuals and postings but have 
> not found this problem.
> 
> I am using R version 2.0.1 under a Windows 2000 operating system.
> 
> The following is a simplified version of what I am tring to do:
> 
> If I insert the following R code in the Rprofile file located in 
> directory 'C:\Program Files\R\rw2001\etc':
> 
> x <- c(1,2,3,4,5)
> y <- c(2,5,6,3,10)
> plot(x,y)
> 
> Using the Rgui.exe or the R.exe terminal version of R the following 
> message appears:
> 
> Error: couldn't find function "plot"
> 
> If I copy and paste the command 'plot(x,y)' into the R Console Window 
> the scatter plot appears in the R Graphics Device window as expected.
> 
> I have also tried putting the above code in a 'R' script file and using 
> the source command but I get the same error message.
> 
> Any help would be appreciated.
> 
> Thank you.
> 
> John Fisler
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From kjetil at acelerate.com  Thu Jan  6 01:41:06 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 05 Jan 2005 20:41:06 -0400
Subject: [R] variance of combinations of means - off topic
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E4AC@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E4AC@usrymx25.merck.com>
Message-ID: <41DC8922.8070103@acelerate.com>

Liaw, Andy wrote:

>You could try googling for "delta method".  I believe MASS even has code for
>that...
>
>Andy
>
>  
>
also,

help.search("delta")
does give nothing usefull, so if it is in MASS it would be hidden, and 
need a \concept
entry in the .Rd file.

The delta method is really nothimg more (or less) than a linear
Taylor approximation.

The following is a very fast implementation, which surely can be bettered:

se.delta <- function(expr, namevec, theta, sigma) {
    # theta is means, or more generally, estimates,
    # sigma is vector or matrix containing (standard errors)^2 and (if 
matrix)
    # covariances. If vector must be same length as theta, and we assume no
    # correlation between estimates theta.
    # expr must be a formula defining a function
    # taking as many arguments as length of theta.
    # namevec is a character vector containing the symbols used in the 
formula.
    p <- length(theta)
    if (!is.matrix(sigma)) sigma <- diag(sigma)
    if(!(NROW(sigma)==p)) stop("sigma and theta must have appropiate 
dimension")
    fun <- deriv(expr, namevec, TRUE)
    derivs <- do.call("fun", as.list(theta))
    derivs <- as.vector(attr(derivs, "gradient"))
    vartheta <- derivs %*% sigma %*% derivs
    setheta <- sqrt(vartheta)
    setheta
}

example:

se.delta(~ x^2 + y^2, c("x", "y"), c(0.5, 1), c(0.001, 0.001))
           [,1]
[1,] 0.07071068

(there must be possible to calculate the namevec argument from the 
formula argument, but I have no time
for that now)

Kjetil


>>From: Bill Shipley
>>
>>Hello, and please excuse this off-topic question, but I have not been
>>able to find an answer elsewhere.  Consider a value Z that is 
>>calculated
>>using the product (or ratio) of two means X_mean and Y_mean:
>>Z=X_mean*Y_mean.  More generally, Z=f(X_mean, Y_mean).  The standard
>>error of Z will be a function of the standard errors of the means of X
>>and Y.  I want to calculate this se of Z.  Can someone direct me to a
>>reference (text book or other) that gives the solution to 
>>this *general*
>>problem?
>>
>>Thanks.
>>
>> 
>>
>>Bill Shipley
>>
>> 
>>
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From andy_liaw at merck.com  Thu Jan  6 01:59:52 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 5 Jan 2005 19:59:52 -0500
Subject: [R] cubic spline smoother with heterogeneous variance.
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4B5@usrymx25.merck.com>

AFAIK the impact of heteroscedasticity on smoothers in general is that
automatic smoothing parameter selection (e.g., via cv or gcv) could be
suboptimal.  One possibility is to supply weights to smooth.spline().  The
problem is how to estimate the weights?  One possibility is to smooth the
squared residuals of an undersmoothed estimate; i.e., use a smoothing
parameter that you know to be surely too small, and smooth the  squared
residuals from that.

[Note though, that at least for local polynomial/kernel smoothers, weighting
by estimates of local variance might not be the best thing to do.  See the
paper by M. C. Jones (1993) in the Australian Journal of Statistics, pp.
89-92.]

Andy

> From: Bill Shipley
> 
> Hello.  I want to estimate the predicted values and standard errors of
> Y=f(t) and its first derivative at each unique value of t using the
> smooth.spline function.  However, the data (plant growth as a function
> of time) show substantial heterogeneity of variance since the variance
> of plant mass increases over time.  What is the consequence of such
> heterogeneity of variance in terms of bias in the estimate of the
> predicted value of Y and its first derivative?  I could 
> Ln-transform the
> data to achieve homogeneity of variance, but this would give 
> me the mean
> of Ln(Y) at each time (i.e. the mode of Y when 
> back-transformed) and the
> derivative of Ln(Y) with time (i.e. d(Ln(Y))/dt = dY/YDt), not dY/dt.
> 
> Can anyone suggest the best strategy for solving this problem?
> 
>  
> 
> Bill Shipley
> 
> Subject Matter Editor, Ecology
> 
> North American Editor, Annals of Botany
> 
> D?partement de biologie, Universit? de Sherbrooke,
> 
> Sherbrooke (Qu?bec) J1K 2R1 CANADA
> 
> Bill.Shipley at USherbrooke.ca
> 
 <http://callisto.si.usherb.ca:8080/bshipley/>
http://callisto.si.usherb.ca:8080/bshipley/

 


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Thu Jan  6 02:09:14 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 5 Jan 2005 20:09:14 -0500
Subject: [R] Rcmd check  Error help
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4B6@usrymx25.merck.com>

The recommended way, I believe, is to _install_ the package (say in some
test directory) and try it out.  Only when you have no problem loading the
package in R from that installation would you then try R CMD check.

When you install the package (as R CMD check does, in the directory
pkg.Rcheck), it concatenate all the files in R/ into one file, and install
it in the R/ subdirectory.  When the package is loaded, that file is
source()'ed into R.  If there are syntax errors, that's where they are
found.  So it'd be easier to diagnose such problem if you install the
package first and try to load it.

One possibility is to try and source() all the files in the R/ directory
under the source package to see which one fails.  You can do that easily
enough in one simple loop.

HTH,
Andy

> From: McGehee, Robert
> 
> R-help,
> I'm the primary developer for an increasingly large R package 
> with over
> three thousand lines of code. Unfortunately, do the complexity of the
> code, I sometimes am required to change several 
> interoperating parts of
> the package before testing for bugs and performance. And sometimes
> unnoticed syntax errors slip in that cause Rcmd check / 
> INSTALL to fail
> with such messages as:
> 
> Error in parse(file, n, text, prompt) : syntax error on line 223
> Execution halted
> 
> My question: what file should I be checking for line 223? When running
> Rcmd INSTALL, the previous version is restored on an install 
> fail, so I
> can't check the install directory, the Rcmd check directory only
> produces two log files, and my R-package has at least a dozen files to
> choose from.
> 
> Aside from brute force commenting out file by file until the syntax
> error disappears (certainly a wasteful use of time), how can 
> I find out
> where my error is?
> 
> Thank you,
> Robert
> 
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.1            
> year     2004           
> month    11             
> day      15             
> language R    
>        
> Robert McGehee
> Geode Capital Management, LLC
> 53 State Street, 5th Floor | Boston, MA | 02109
> Tel: 617/392-8396    Fax:617/476-6389
> mailto:robert.mcgehee at geodecapital.com
> 
> 
> 
> This e-mail, and any attachments hereto, are intended for use by the
> addressee(s) only and may contain information that is (i) confidential
> information of Geode Capital Management, LLC and/or its affiliates,
> and/or (ii) proprietary information of Geode Capital Management, LLC
> and/or its affiliates. If you are not the intended recipient of this
> e-mail, or if you have otherwise received this e-mail in error, please
> immediately notify me by telephone (you may call collect), or 
> by e-mail,
> and please permanently delete the original, any print outs and any
> copies of the foregoing. Any dissemination, distribution or copying of
> this e-mail is strictly prohibited.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From gunter.berton at gene.com  Thu Jan  6 02:16:01 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 5 Jan 2005 17:16:01 -0800
Subject: [R] Using the Rprofile file to automatically plot data on Startup
	of R version 2.0.1.
In-Reply-To: <41DC6F65.9030703@latticesemi.com>
Message-ID: <200501060116.j061G1sW016001@compton.gene.com>


See ?Startup and the rw FAQ for info on startup procedures. I assume by
"Rprofile file" you mean Rprofile.site. As ?options --> defaultPackages
states, the graphics library is one of those loaded by default but, as
?Startup states,  **not** until after Rprofile.site is executed. Since
plot()is now in the graphics library and not in base, it is not present when
you try to execute your plot commands -- ergo the error. So explicitly load
the library by calling library(graphics) ** before ** you plot, and you
should then be able to do your plotting.


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Fisler
> Sent: Wednesday, January 05, 2005 2:51 PM
> To: r-help at lists.R-project.org; John Fisler
> Subject: [R] Using the Rprofile file to automatically plot 
> data on Startup of R version 2.0.1.
> 
> Dear R Help Members,
> 
> I have some R functions that plot semiconductor data.  I 
> would like to 
> automate these plots for individuals in our group such that 
> they don't 
> have to know R.  I have read the R help manuals and postings but have 
> not found this problem.
> 
> I am using R version 2.0.1 under a Windows 2000 operating system.
> 
> The following is a simplified version of what I am tring to do:
> 
> If I insert the following R code in the Rprofile file located in 
> directory 'C:\Program Files\R\rw2001\etc':
> 
> x <- c(1,2,3,4,5)
> y <- c(2,5,6,3,10)
> plot(x,y)
> 
> Using the Rgui.exe or the R.exe terminal version of R the following 
> message appears:
> 
> Error: couldn't find function "plot"
> 
> If I copy and paste the command 'plot(x,y)' into the R Console Window 
> the scatter plot appears in the R Graphics Device window as expected.
> 
> I have also tried putting the above code in a 'R' script file 
> and using 
> the source command but I get the same error message.
> 
> Any help would be appreciated.
> 
> Thank you.
> 
> John Fisler
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From andrewr at uidaho.edu  Thu Jan  6 02:30:30 2005
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Thu, 6 Jan 2005 12:30:30 +1100
Subject: [R] variance of combinations of means - off topic
Message-ID: <20050106013030.GR599@uidaho.edu>

Bill,

P. 146 of Casella and Berger's "Statistical Inference" 1990 starts a
section on bivariate transformations.

Andrew
-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From john.fisler at latticesemi.com  Thu Jan  6 02:40:11 2005
From: john.fisler at latticesemi.com (John Fisler)
Date: Wed, 05 Jan 2005 17:40:11 -0800
Subject: [R] Using the Rprofile file to automatically plot data on Startup
	of R version 2.0.1.
References: <200501060116.j061G1sW016001@compton.gene.com>
Message-ID: <41DC96FB.3040607@latticesemi.com>

Berton,

Thank you for your response.  Loading the graphics library in the .First 
function worked.

John

Berton Gunter wrote:
> See ?Startup and the rw FAQ for info on startup procedures. I assume by
> "Rprofile file" you mean Rprofile.site. As ?options --> defaultPackages
> states, the graphics library is one of those loaded by default but, as
> ?Startup states,  **not** until after Rprofile.site is executed. Since
> plot()is now in the graphics library and not in base, it is not present when
> you try to execute your plot commands -- ergo the error. So explicitly load
> the library by calling library(graphics) ** before ** you plot, and you
> should then be able to do your plotting.
> 
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> 
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Fisler
>>Sent: Wednesday, January 05, 2005 2:51 PM
>>To: r-help at lists.R-project.org; John Fisler
>>Subject: [R] Using the Rprofile file to automatically plot 
>>data on Startup of R version 2.0.1.
>>
>>Dear R Help Members,
>>
>>I have some R functions that plot semiconductor data.  I 
>>would like to 
>>automate these plots for individuals in our group such that 
>>they don't 
>>have to know R.  I have read the R help manuals and postings but have 
>>not found this problem.
>>
>>I am using R version 2.0.1 under a Windows 2000 operating system.
>>
>>The following is a simplified version of what I am tring to do:
>>
>>If I insert the following R code in the Rprofile file located in 
>>directory 'C:\Program Files\R\rw2001\etc':
>>
>>x <- c(1,2,3,4,5)
>>y <- c(2,5,6,3,10)
>>plot(x,y)
>>
>>Using the Rgui.exe or the R.exe terminal version of R the following 
>>message appears:
>>
>>Error: couldn't find function "plot"
>>
>>If I copy and paste the command 'plot(x,y)' into the R Console Window 
>>the scatter plot appears in the R Graphics Device window as expected.
>>
>>I have also tried putting the above code in a 'R' script file 
>>and using 
>>the source command but I get the same error message.
>>
>>Any help would be appreciated.
>>
>>Thank you.
>>
>>John Fisler
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
>



From dmb at mrc-dunn.cam.ac.uk  Thu Jan  6 02:45:04 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Thu, 6 Jan 2005 01:45:04 +0000 (GMT)
Subject: [R] plotting percent of incidents within different 'bins'
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E9027FE360@exdkba022.novo.dk>
Message-ID: <Pine.LNX.4.21.0501060138030.3870-100000@mail.mrc-dunn.cam.ac.uk>


I was doing something very similar. 

I found it tricky to work out how to find a confidence interval for the
'percentage' of the outcome (I call it proportion).

Some of my bins had all zeroes or all ones, so I couldn't work out how to
make a variance that was sensible. Also some bins had few values.

To each bin I decided to add one 0 and one 1 'outcome'. This tends the
mean outcome to 50% for low counts (this is kinda intuitivly correct), and
it also maximizes the (binomial) variance.

Do you have a similar problem? / Have you decided what to do?


Also I decided to bin my bins... I had about 50 distinct bins over a
'predictor' range of about 800, so I broke that 800 into 20 'ranges' and
binned the bins in each range.

(like your 'predictor' my original bins are ordered integer values).

Now I don't know how best to calculate the variance, as I can do this at
at least two levels in the data. 

I don't know if I should pool the original bins and recalculate the pooled
binomial variance, or calculate (bootstrap) the variance in the
proportions already calculated.

Let me know if you think my code would be usefull (its quite simple).

Dan.



On Wed, 5 Jan 2005, BXC (Bendix Carstensen) wrote:

>You want:
>
>tapply( Outcome, predictor, mean )
>
>Bendix Carstensen
>----------------------
>Bendix Carstensen
>Senior Statistician
>Steno Diabetes Center
>Niels Steensens Vej 2
>DK-2820 Gentofte
>Denmark
>tel: +45 44 43 87 38
>mob: +45 30 75 87 38
>fax: +45 44 43 07 06
>bxc at steno.dk
>www.biostat.ku.dk/~bxc
>----------------------
>
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
>> Stephen Choularton
>> Sent: Wednesday, January 05, 2005 8:35 PM
>> To: R Help
>> Subject: [R] plotting percent of incidents within different 'bins'
>> 
>> 
>> Hi
>>  
>> Say I have some data, two columns in a table being a binary 
>> outcome plus a predictor and I want to plot a graph that 
>> shows the percentage positives of the binary outcome within 
>> bands of the predictor, e.g.
>>  
>>  
>> Outcome           predictor
>>  
>> 0                      1
>> 1                      2
>> 1                      2
>> 0                      3          
>> 0                      3
>> 0                      2          
>> 1                      3
>> 1                      4
>> 1                      4
>> 0                      4
>> 0                      4
>> 0                      4
>> etc
>>  
>> In this case there are 4 cases in the band 1 - 2 of the 
>> predictor, 2 of them are true so the percent is 50% and there 
>> are 7 cases in the band 3
>> - 4, 3 of which are true making the percentage 43% .
>>  
>> Is there some function in R that will sum these outcomes by 
>> bands of predictor and produce a one by two  data set with 
>> the percentages in one column and the ordered bands in the 
>> other, or alternately is there some sort of special plot.???? 
>> that does it all for you?
>>  
>> Thanks
>>  
>> Stephen
>>  
>>  
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read 
>> the posting guide! http://www.R-project.org/posting-guide.html
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Thu Jan  6 02:55:30 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 5 Jan 2005 20:55:30 -0500
Subject: [R] multiple trees
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4B9@usrymx25.merck.com>

> From: Weiwei Shi
> 
> Hi, there:
> I made a function to do k-fold cross-validation as
> below. Basically whenever I call cv(test) for example,
> an error message like:
> 20Fold  1 
> Error in model.frame(formula, rownames, variables,
> varnames, extras, extranames,  : 
>         variable lengths differ
> 
> please help. 
> 
> My test dataset has 142 variables, the last one is a
> categorical response variable.
> also, i am not sure how to save the trees into a list
> or something so that I can handle, like pointer array
> or something in C.
> 
> Thanks.
> 
> Weiwei Shi, Ph.D
> 
> cv<- function(all.data,n.folds=10,mcp=0.003) {
> 
>  n <- nrow(all.data)
>  idx <- sample(n,n)
>  all.data <- all.data[idx,]
> 
>  n.each.part <- as.integer(n/n.folds)
>  r.model<- vector()
>  r.model.prune<- vector()
> 
>  for(i in 1:n.folds) {
>  cat('Fold ',i,'\n')
>  out.fold <- ((i-1)*n.each.part+1):(i*n.each.part)
>  tmp<-all.data[-(out.fold),1:141]
>  r.model[i]<- rpart(all.data$V142~., data=tmp,
                      ^^^^^^^^^^^^^^^^^^^^^^^^^

That ain't gonna work.  You specify the response from all.data, which has
length n, but the rest of the variables (found in tmp) has fewer cases,
hence the error.

I'd recommend that you consult either `Monder Applied Statistics in S', 4th
ed., or `S Programming' (which has a chapter on how to do CV efficiently) if
you really want to learn how to code CV.  Otherwise I'd suggest that you use
the errorest() function in the ipred package on CRAN.

Andy




> parms=list(split='gini'), cp=0)
>  #r.model.prune[i]<-prune(r.model[i], cp=mcp)
> 
>  }
> return (r.model)
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Thu Jan  6 02:58:29 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 5 Jan 2005 20:58:29 -0500
Subject: [R] Rprofile file to automatically plot data, tried using
	the .First command.
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4BA@usrymx25.merck.com>

No.  You put the _whole_ thing into .First(), not just the plot statement.
One possibility is to have the code in a script file (say myscript.R) and
define .First as follows:

.First <- function() source("myscript.R")

Haven't try it myself, though.

Andy

> From: John Fisler
> 
> Dear Dr. Andreas Kiermeier and the R help Community,
> 
> Thank you for your idea on putting the plot command in a 
> .First statement.
> 
> I tried putting the plot comamnd in a '.First' statement like 
> the following:
> 
> x <- c(1,2,3,4,5)
> y <- c(2,5,6,3,10)
> 
> .First <- function() {
> 
> plot(x,y)
> 
> }
> 
> and had the same error message:
> 
> Error in .First() : couldn't find function "plot"
> 
> Should I be put the plot command in a different 'startup' file?
> 
> Thanks,
> 
> John
> 
> the following commands
> 
> Kiermeier, Andreas (PIRSA - SARDI) wrote:
> > Dear John,
> > 
> > I belive your problem has to do with the sequence of 
> startup. I think that
> > .Rprofile is called before the required libraries are attached.
> > 
> > You might like to try putting your code into a .First() 
> function and run it
> > that way.
> > 
> > Cheers,
> > 
> > Andreas
> > 
> > Dr Andreas Kiermeier
> > Statistician
> > SARDI FOOD SAFETY PROGRAM
> > 
> > 33 Flemington Street
> > Glenside   SA   5065
> > Phone:  +61 8 8207 7884
> > Fax:       +61 8 8207 7854
> > Mobile: 0423 028 565
> > 
> > Email: Kiermeier.Andreas at saugov.sa.gov.au
> > _____________________________
> > 
> > The information in this e-mail and attachments (if any) may 
> be confidential
> > and/or legally privileged. If you are not the intended 
> recipient, any
> > disclosure, copying, distribution or action taken is 
> prohibited. SARDI, The
> > South Australian Research and Development Institute, is the research
> > division of Primary Industries and Resources (SA) 
> > 
> > 
> > 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of John Fisler
> > Sent: Thursday, 6 January 2005 09:21
> > To: r-help at lists.R-project.org; John Fisler
> > Subject: [R] Using the Rprofile file to automatically plot data on
> > Startup of R version 2.0.1.
> > 
> > 
> > Dear R Help Members,
> > 
> > I have some R functions that plot semiconductor data.  I 
> would like to 
> > automate these plots for individuals in our group such that 
> they don't 
> > have to know R.  I have read the R help manuals and 
> postings but have 
> > not found this problem.
> > 
> > I am using R version 2.0.1 under a Windows 2000 operating system.
> > 
> > The following is a simplified version of what I am tring to do:
> > 
> > If I insert the following R code in the Rprofile file located in 
> > directory 'C:\Program Files\R\rw2001\etc':
> > 
> > x <- c(1,2,3,4,5)
> > y <- c(2,5,6,3,10)
> > plot(x,y)
> > 
> > Using the Rgui.exe or the R.exe terminal version of R the following 
> > message appears:
> > 
> > Error: couldn't find function "plot"
> > 
> > If I copy and paste the command 'plot(x,y)' into the R 
> Console Window 
> > the scatter plot appears in the R Graphics Device window as 
> expected.
> > 
> > I have also tried putting the above code in a 'R' script 
> file and using 
> > the source command but I get the same error message.
> > 
> > Any help would be appreciated.
> > 
> > Thank you.
> > 
> > John Fisler
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From f.harrell at vanderbilt.edu  Thu Jan  6 04:02:42 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 05 Jan 2005 21:02:42 -0600
Subject: [R] JOBS: Vanderbilt University Department of Biostatistics
Message-ID: <41DCAA52.1080705@vanderbilt.edu>

The well-supported and rapidly-growing Department of Biostatistics in 
the Vanderbilt University School of Medicine, Nashville, Tennessee, has 
multiple openings for persons with an MS or PhD in biostatistics, at all 
levels. We are especially recruiting for the following positions.

    - A senior faculty member to lead a multi-center clinical trial data 

      coordinating center.
      The individual must have extensive experience in NIH-sponsored data
      coordinating centers.
    - A senior faculty member with experience in health services or
      outcomes research or clinical epidemiology to lead a biostatistics
      unit for surgical sciences.
    - Senior MS biostatisticians with 5 or more years of experience.  At
      Vanderbilt, such persons have faculty positions.
    - Associate professors
    - Junior MS biostatisticians

We are also seeking assistant and full professors. The department 
currently has 16 PhD faculty biostatisticians, 6 senior and 5 junior MS 
biostatisticians, and 10 computer systems analysts and plans to grow 
significantly in size over the next three years, continuing to add a 
significant number of faculty and staff biostatisticians after that.

In general we seek individuals with expertise in clinical trials, health 
services and outcomes research, Bayesian methods, multicenter clinical 
trial data management, imaging, and modern statistical computing 
(especially R or S-Plus). See 
http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/JobOpenings for 
more information.

All candidates must enjoy collaborative research and possess excellent 
written and verbal communication skills. Send CV to Search Committee, 
Biostatistics, S-2323 MCN, Vanderbilt University, Nashville, TN 
37232-2158. Vanderbilt University is an equal opportunity/affirmative 
action employer; all qualified persons are encouraged to apply. CVs may 
be sent electronically to both f.harrell at vanderbilt.edu and 
biostat at vanderbilt.edu. Please state in the subject of the E-mail the 
position for which you are applying or inquiring.


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From edd at debian.org  Wed Jan  5 19:58:54 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 5 Jan 2005 18:58:54 +0000 (UTC)
Subject: [R] warnings and errors with R CMD INSTALL
References: <1104878076.7506.28.camel@localhost.localdomain>
Message-ID: <loom.20050105T195429-384@post.gmane.org>

thomas <tom_hoary <at> web.de> writes:
> Hello,
> 
> unfortunately I had to compile latest version of R-2.0.1 by myself. I'd
> rather would prefer vendors binaries but since the current version of
> Ubuntu defaults to 1.9.x I had to compile R on my own in order to be up
> to date!

You could have gotten R 2.0.1 from Debian. Ubuntu is a Debian derivative, and 
that is a feature you can take advantage of:  Debian packages should just work.

> So far, everything went fine and R runs smoothly. Unfortunately I am not
> able to use R CMD INSTALL command to install add-on packages.
> 
> I followed RNews 3/3 and used:
> 
> 1. R CMD INSTALL package_version.tar.gz 

Did you do that as root? 

> and
> 
> 2.options(CRAN="http://umfragen.sowi.uni-mainz.de/CRAN/")
> install.packages("pkg1". "pkg2")

Should be    install.packages(c("pkg1", "pkg2"))    You seem to have 
misunderstood how to create a vector on the fly.

> Both approaches end up i.e.:
> 
> WARNING: invalid package 'pkg1.tar.gz'
> ERROR: no packages specified
> 
> Well, R administration manual and RNews give some examples which I took
> as basis to install further packages.
> I, as suggested in RNews and administration manual alike, omitted the -l
> option, since I want to stay with default library location!

Which requires root rights.

> Obviously I am stuck in a situation not covered by the docs! Well, most

Not sure I agree here.

> likely I misunderstand something, can anyone help out with this?

Hope this helps, Dirk



From ksm32 at student.canterbury.ac.nz  Thu Jan  6 04:55:02 2005
From: ksm32 at student.canterbury.ac.nz (Karla Meurk)
Date: Thu, 06 Jan 2005 16:55:02 +1300
Subject: [R] aggregate()
Message-ID: <41DCB696.1060602@student.canterbury.ac.nz>

Hi, some time ago I asked R-help about aggregating data as a result I 
was able to put together some code which includes the line

rain.ag <- aggregate(newdata, list(hod6=cut(mindata,"6 hours")), mean, 
na.rm=T)

I also want to aggregate daily, and 30 minutely etc.

My question is why is it that I get answers with list(.. "hours") but R 
cannot cope with list(.."6 hours") or any other multiple.  I have tried 
overcoming this using nfrequency= but to no avail

can someone help?

Thanks

Carla



From ggrothendieck at myway.com  Thu Jan  6 05:17:54 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 6 Jan 2005 04:17:54 +0000 (UTC)
Subject: [R] aggregate()
References: <41DCB696.1060602@student.canterbury.ac.nz>
Message-ID: <loom.20050106T050914-954@post.gmane.org>

Karla Meurk <ksm32 <at> student.canterbury.ac.nz> writes:

: 
: Hi, some time ago I asked R-help about aggregating data as a result I 
: was able to put together some code which includes the line
: 
: rain.ag <- aggregate(newdata, list(hod6=cut(mindata,"6 hours")), mean, 
: na.rm=T)
: 
: I also want to aggregate daily, and 30 minutely etc.
: 
: My question is why is it that I get answers with list(.. "hours") but R 
: cannot cope with list(.."6 hours") or any other multiple.  I have tried 
: overcoming this using nfrequency= but to no avail
: 
: can someone help?

You need to provide a short reproducible example to illustrate
your problem with an explanation of what you expect from the
code.  That means that someone can just copy the code 
from your posting and paste it into their session and see the 
exact same incorrect output or error that you got.  If its
not short you need to boil it down to something that is short
before posting it.



From edd at debian.org  Thu Jan  6 05:28:29 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 5 Jan 2005 22:28:29 -0600
Subject: [R] warnings and errors with R CMD INSTALL
In-Reply-To: <1104883163.7506.41.camel@localhost.localdomain>
References: <1104878076.7506.28.camel@localhost.localdomain>
	<Pine.LNX.4.61.0501042324480.30382@gannet.stats>
	<1104883163.7506.41.camel@localhost.localdomain>
Message-ID: <20050106042829.GB17662@sonny.eddelbuettel.com>

On Wed, Jan 05, 2005 at 12:59:23AM +0100, thomas wrote:
> 2.  install.packages("accuracy", "zoo", "abind")

Packages like zoo, abind and RODBC (from your other mail) are actually
available pre-built for Debian.  If you file /etc/apt/sources.list points to
Debian and Ubuntu (and you can have Ubuntu at higher priority; google for
"apt-get pinning") you can say

	$ apt-get install r-cran-zoo r-cran-abind r-cran-rodbc

There are a few dozen more packages.

Lastly, the Quantian (http://dirk.eddelbuettel.com/quantian) distribution
has essentially all CRAN packages pre-installed.

Hth,  Dirk

-- 
If you don't go with R now, you will someday.
  -- David Kane on r-sig-finance, 30 Nov 2004



From ggrothendieck at myway.com  Thu Jan  6 05:59:25 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 6 Jan 2005 04:59:25 +0000 (UTC)
Subject: [R] Using the Rprofile file to automatically plot data on
	=?utf-8?b?U3RhcnR1cAlvZg==?= R version 2.0.1.
References: <200501060116.j061G1sW016001@compton.gene.com>
	<41DC96FB.3040607@latticesemi.com>
Message-ID: <loom.20050106T055452-377@post.gmane.org>


I think you could just use the command:  R CMD BATCH myplot.R
with no .First at all.  myplot.R would hold:

win.metafile(filename = "a.wmf", width = 5.7, height = 8.8, pointsize=12)
plot(1:10)
dev.off()

You may want a different output file format so replace the
win.metafile command with whatever you want, 
e.g. bitmap, postscript, etc.   



John Fisler <john.fisler <at> latticesemi.com> writes:

: 
: Berton,
: 
: Thank you for your response.  Loading the graphics library in the .First 
: function worked.
: 
: John
: 
: Berton Gunter wrote:
: > See ?Startup and the rw FAQ for info on startup procedures. I assume by
: > "Rprofile file" you mean Rprofile.site. As ?options --> defaultPackages
: > states, the graphics library is one of those loaded by default but, as
: > ?Startup states,  **not** until after Rprofile.site is executed. Since
: > plot()is now in the graphics library and not in base, it is not present 
when
: > you try to execute your plot commands -- ergo the error. So explicitly load
: > the library by calling library(graphics) ** before ** you plot, and you
: > should then be able to do your plotting.
: > 
: > 
: > -- Bert Gunter
: > Genentech Non-Clinical Statistics
: > South San Francisco, CA
: >  
: > 
: > 
: > 
: >>-----Original Message-----
: >>From: r-help-bounces <at> stat.math.ethz.ch 
: >>[mailto:r-help-bounces <at> stat.math.ethz.ch] On Behalf Of John Fisler
: >>Sent: Wednesday, January 05, 2005 2:51 PM
: >>To: r-help <at> lists.R-project.org; John Fisler
: >>Subject: [R] Using the Rprofile file to automatically plot 
: >>data on Startup of R version 2.0.1.
: >>
: >>Dear R Help Members,
: >>
: >>I have some R functions that plot semiconductor data.  I 
: >>would like to 
: >>automate these plots for individuals in our group such that 
: >>they don't 
: >>have to know R.  I have read the R help manuals and postings but have 
: >>not found this problem.
: >>
: >>I am using R version 2.0.1 under a Windows 2000 operating system.
: >>
: >>The following is a simplified version of what I am tring to do:
: >>
: >>If I insert the following R code in the Rprofile file located in 
: >>directory 'C:\Program Files\R\rw2001\etc':
: >>
: >>x <- c(1,2,3,4,5)
: >>y <- c(2,5,6,3,10)
: >>plot(x,y)
: >>
: >>Using the Rgui.exe or the R.exe terminal version of R the following 
: >>message appears:
: >>
: >>Error: couldn't find function "plot"
: >>
: >>If I copy and paste the command 'plot(x,y)' into the R Console Window 
: >>the scatter plot appears in the R Graphics Device window as expected.
: >>
: >>I have also tried putting the above code in a 'R' script file 
: >>and using 
: >>the source command but I get the same error message.
: >>
: >>Any help would be appreciated.
: >>
: >>Thank you.
: >>
: >>John Fisler
: >>
: >>______________________________________________
: >>R-help <at> stat.math.ethz.ch mailing list
: >>https://stat.ethz.ch/mailman/listinfo/r-help
: >>PLEASE do read the posting guide! 
: >>http://www.R-project.org/posting-guide.html
: >>
: > 
: > 
: >
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From kalibera at nenya.ms.mff.cuni.cz  Thu Jan  6 08:53:05 2005
From: kalibera at nenya.ms.mff.cuni.cz (Tomas Kalibera)
Date: Thu, 06 Jan 2005 08:53:05 +0100
Subject: [R] Vertical labels on axes overlap
Message-ID: <41DCEE61.50904@nenya.ms.mff.cuni.cz>


Hello,

when using horizontal labels (default) in plots on x-axis, R by default 
selects a subset of labels to plot so that the labels do not overlap. 
However, when using vertical labels, all labels are always drawn, even 
when they overlap. Is it a bug or do I have to adjust some magic parameter ?

the problem can be shown on these 2 tiny examples:

horizontal labels (default) [OK]:

 > plot(1:100,axes=FALSE)
 > axis(1,at=1:100,labels=rep("aaa",100))

(only a subset of labels is drawn)

vertical labels [THE PROBLEM]:

 > plot(1:100,axes=FALSE)
 > axis(1,at=1:100,labels=rep("aaa",100),las=2)

(all labels are drawn - and they do overlap)

Thanks,

Tomas



From ligges at statistik.uni-dortmund.de  Thu Jan  6 10:07:03 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 06 Jan 2005 10:07:03 +0100
Subject: [R] Vertical labels on axes overlap
In-Reply-To: <41DCEE61.50904@nenya.ms.mff.cuni.cz>
References: <41DCEE61.50904@nenya.ms.mff.cuni.cz>
Message-ID: <41DCFFB7.8010602@statistik.uni-dortmund.de>

Tomas Kalibera wrote:
> 
> Hello,
> 
> when using horizontal labels (default) in plots on x-axis, R by default 
> selects a subset of labels to plot so that the labels do not overlap. 
> However, when using vertical labels, all labels are always drawn, even 
> when they overlap. Is it a bug or do I have to adjust some magic 
> parameter ?
> 
> the problem can be shown on these 2 tiny examples:
> 
> horizontal labels (default) [OK]:
> 
>  > plot(1:100,axes=FALSE)
>  > axis(1,at=1:100,labels=rep("aaa",100))
> 
> (only a subset of labels is drawn)
> 
> vertical labels [THE PROBLEM]:
> 
>  > plot(1:100,axes=FALSE)
>  > axis(1,at=1:100,labels=rep("aaa",100),las=2)
> 
> (all labels are drawn - and they do overlap)

I think it does not qualify as a bug, because you might want to allow a 
small amount of overlapping if you are already plotting vertically.
Instead, you have select each second/third element yourself, in this 
case most easily with:

  plot(1:100,axes=FALSE)
  axis(1,at=1:100,labels=rep(c("aaa", ""),50),las=2)


Uwe Ligges



> Thanks,
> 
> Tomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From thchung at tgen.org  Thu Jan  6 10:20:15 2005
From: thchung at tgen.org (Tae-Hoon Chung)
Date: Thu, 06 Jan 2005 02:20:15 -0700
Subject: [R] Segmentation fault while using Mclust function of mclust library
 in R-2.0.1
Message-ID: <BE0250DF.2383%thchung@tgen.org>

Hi, all;

I got an unusual error while using mclust library 2.1-7 on R 2.0.1.
When I tried to run Mclust(rnorm(100)), I got segmentation fault error.
Does anyone know what causes this problem?
I came across the same problem even when I tried to run the example shown in
Mclust help using iris data.

Thanks in advance,
Tae-Hoon Chung
--------------------------------------------------
Tae-Hoon Chung
Post-Doctoral Researcher
Translational Genomics Research Institute (TGen)
445 N. 5th Street (Suite 530)
Phoenix, AZ 85004
1-602-343-8724 (Direct)
1-480-323-9820 (Mobile)
1-602-343-8840 (Fax)



From andikumagenge at businessdecision.com  Thu Jan  6 10:34:17 2005
From: andikumagenge at businessdecision.com (NDIKUMAGENGE Alice)
Date: Thu, 6 Jan 2005 10:34:17 +0100
Subject: [R] Help for detrending
Message-ID: <6CC4DC1EC1F92D4B8A5FF590362E75650217D00A@neptune.betd.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050106/7d09336c/attachment.pl

From adi at roda.ro  Thu Jan  6 10:32:07 2005
From: adi at roda.ro (adi@roda.ro)
Date: Thu,  6 Jan 2005 11:32:07 +0200
Subject: [R] Tuning string matching
In-Reply-To: <20050105204913.GA29023@psych>
References: <20050105194611.43723.qmail@web50301.mail.yahoo.com>
	<20050105204913.GA29023@psych>
Message-ID: <1105003927.41dd0597b1a6d@www.roda.ro>


Thank you all for your replies. Indeed I used:
http://finzi.psych.upenn.edu/nmz.html
as a search site. I used "string match", instead of "fuzzy string match".

Fuzzy matching seems to me a rather complicated matter, whereas my initial idea
about solving this problem was a bit simpler:
- check all characters in both strings (a 2 dimensional matrix of characters)
- if 90% (or any other percent) of the characters in both strings are similar
(in terms of distances between each character from the first string to all
characters from the second string), then the two strings will be declared as a
match

I just found out that this algorithm is called the "Levenshtein distance", and I
know there is a PHP function called "levenshtein" (I thought it already might
have been implemented in R).

For anyone that have a clue on how to read this stuff:
http://ro.php.net/levenshtein

I tried to use agrep:
> agrep("Harry Harrington", "Harry Harrington")
[1] 1
> agrep("Harry Harrington", "Harrington Harry")
numeric(0)

So it seems not to be what I'm looking for (I'll try harder with edit distance,
though)

Best regards,
Adrian


Quoting Jonathan Baron <baron at psych.upenn.edu>:

> Sorry for joining late, but I wanted to see if my search page
> could help.  (I don't know which search archive you looked at.)
> I entered
> fuzzy string match*
> and got a few things that look relevant, including the agrep
> function.
>
> As for the second part of the question, that seems to be a coding
> problem that is dependent on the current form of your data.
> Write me off the list and I'll send you an R script I use for
> similar things (making PayPal payments).
>
> Jon
>
>  Dear list,
>
>  I spent about two hours searching on the message archive, with no
>  avail.
>  I have a list of people that have to pass an on-line test, but only a
>  fraction
>  of them do it. Moreover, as they input their names, the resulting
>  string do not
>  always match the names I have in my database.
>
>  I would like to do two things:
>
>  1. Match any strings that are 90% the same
>  Example:
>  name1 <- "Harry Harrington"
>  name2 <- "Harry Harington"
>  I need a function that would declare those strings as a match (ideally
>  having an
>  argument that would allow introducing 80% instead of 90%)
>
> --
> Jonathan Baron, Professor of Psychology, University of Pennsylvania
> Home page: http://www.sas.upenn.edu/~baron
> R search page: http://finzi.psych.upenn.edu/
>



From ligges at statistik.uni-dortmund.de  Thu Jan  6 10:36:31 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 06 Jan 2005 10:36:31 +0100
Subject: [R] Segmentation fault while using Mclust function of mclust
	library in R-2.0.1
In-Reply-To: <BE0250DF.2383%thchung@tgen.org>
References: <BE0250DF.2383%thchung@tgen.org>
Message-ID: <41DD069F.2070202@statistik.uni-dortmund.de>

Tae-Hoon Chung wrote:

> Hi, all;
> 
> I got an unusual error while using mclust library 2.1-7 on R 2.0.1.
> When I tried to run Mclust(rnorm(100)), I got segmentation fault error.
> Does anyone know what causes this problem?
> I came across the same problem even when I tried to run the example shown in
> Mclust help using iris data.

Which OS? Have you (re)compiled the package yourself under R-2.0.1?
You might want to report a reproducible example (e.g. setting the seed 
in the example above) to the package maintainer, including the relevant 
information (including compiler and their versions).

Uwe Ligges



> Thanks in advance,
> Tae-Hoon Chung
> --------------------------------------------------
> Tae-Hoon Chung
> Post-Doctoral Researcher
> Translational Genomics Research Institute (TGen)
> 445 N. 5th Street (Suite 530)
> Phoenix, AZ 85004
> 1-602-343-8724 (Direct)
> 1-480-323-9820 (Mobile)
> 1-602-343-8840 (Fax)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Arne.Muller at aventis.com  Thu Jan  6 11:28:29 2005
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Thu, 6 Jan 2005 11:28:29 +0100
Subject: [R] casting lm.fit output to an lm object
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF2BD@crbsmxsusr04.pharma.aventis.com>

Hello,

Is it possible to cast the output of lm.fit to an lm object? I've 10,000 linear models for a gene expression experiment, all of which have the same model matrix. Maybe calling lm.fit on a model matrix and a data vector is faster than lm. I'd like to use each fit for an anova as well as comparing different models via anova.

	kind regards,

	Arne



From anne.piotet at urbanet.ch  Thu Jan  6 11:39:56 2005
From: anne.piotet at urbanet.ch (Anne)
Date: Thu, 6 Jan 2005 11:39:56 +0100
Subject: [R] library vcd for R rw2001
Message-ID: <00c001c4f3dc$11f48a10$6c00a8c0@mtd4>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050106/bf695879/attachment.pl

From maechler at stat.math.ethz.ch  Thu Jan  6 11:50:10 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 6 Jan 2005 11:50:10 +0100
Subject: [R] Rcmd check  Error help
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E4B6@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E4B6@usrymx25.merck.com>
Message-ID: <16861.6114.809233.523208@stat.math.ethz.ch>

>>>>> "AndyL" == Liaw, Andy <andy_liaw at merck.com>
>>>>>     on Wed, 5 Jan 2005 20:09:14 -0500 writes:

    AndyL> The recommended way, I believe, is to _install_ the
    AndyL> package (say in some test directory) and try it out.
    AndyL> Only when you have no problem loading the package in
    AndyL> R from that installation would you then try R CMD
    AndyL> check.

yes.

    AndyL> When you install the package (as R CMD check does, in
    AndyL> the directory pkg.Rcheck), it concatenate all the
    AndyL> files in R/ into one file, and install it in the R/
    AndyL> subdirectory.  When the package is loaded, that file
    AndyL> is source()'ed into R.  If there are syntax errors,
    AndyL> that's where they are found.  So it'd be easier to
    AndyL> diagnose such problem if you install the package
    AndyL> first and try to load it.

yes.  One thing that helps much nowadays (newer R versions) is
to make sure that you do *not* lazyload by using

    R CMD INSTALL --no-lazy  <pkg>
		  ^^^^^^^^^ 

(or even adding 'LazyLoad: no' to the package DESCRIPTION file
 during development).
Only in this case, you can easily locate the file you were
asking (in the *installed* directory) as file  <pkgname>/R/<pkgname>

    AndyL> One possibility is to try and source() all the files
    AndyL> in the R/ directory under the source package to see
    AndyL> which one fails.  You can do that easily enough in
    AndyL> one simple loop.

    AndyL> HTH, Andy

    >> From: McGehee, Robert
    >> 
    >> R-help, I'm the primary developer for an increasingly
    >> large R package with over three thousand lines of
    >> code. Unfortunately, do the complexity of the code, I
    >> sometimes am required to change several interoperating
    >> parts of the package before testing for bugs and
    >> performance. And sometimes unnoticed syntax errors slip
    >> in that cause Rcmd check / INSTALL to fail with such
    >> messages as:
    >> 
    >> Error in parse(file, n, text, prompt) : syntax error on
    >> line 223 Execution halted
    >> 
    >> My question: what file should I be checking for line 223?
    >> When running Rcmd INSTALL, the previous version is
    >> restored on an install fail, so I can't check the install
    >> directory, the Rcmd check directory only produces two log
    >> files, and my R-package has at least a dozen files to
    >> choose from.
    >> 
    >> Aside from brute force commenting out file by file until
    >> the syntax error disappears (certainly a wasteful use of
    >> time), how can I find out where my error is?
    >> 
    >> Thank you, Robert
    >> 
    >> > version _ platform i386-pc-mingw32 arch i386 os mingw32
    >> system i386, mingw32 status major 2 minor 0.1 year 2004
    >> month 11 day 15 language R
    >> 
    >> Robert McGehee Geode Capital Management, LLC 53 State
    >> Street, 5th Floor | Boston, MA | 02109 Tel: 617/392-8396
    >> Fax:617/476-6389 mailto:robert.mcgehee at geodecapital.com



From ligges at statistik.uni-dortmund.de  Thu Jan  6 11:55:35 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 06 Jan 2005 11:55:35 +0100
Subject: [R] library vcd for R rw2001
In-Reply-To: <00c001c4f3dc$11f48a10$6c00a8c0@mtd4>
References: <00c001c4f3dc$11f48a10$6c00a8c0@mtd4>
Message-ID: <41DD1927.7000609@statistik.uni-dortmund.de>

Anne wrote:

> Is there an upgrate of the vcd library (visualisation of categorical data) for the latest R version? 

Obviously, the most recent version appeared on CRAN on 14-Nov-2004.

> Trying to download it from CRAN I get 
> 
> URL /data/WWW/ftp/pub/R/bin/windows/contrib/r-release/vcd_0.1-3.4.zip was not found on this server.

The URL in the links got temporarily corrupted on CRAN.

Just type
     install.packages("vcd")
within R, or go "manually" to
     CRAN/bin/windows/contrib/r-release/


> googling it, I found it for instance on 
> http://www.sourcekeg.co.uk/cran/bin/windows/contrib/1.9/

Obviously, this was a binary package compiled fort R-1.9.x.

> but trying to install it gave me the message
> 
>>library(vcd)
> 
> Error in library(vcd) : 'vcd' is not a valid package -- installed < 2.0.0?

Yes, it was compiled under R-1.9.x, hence you cannot use it in R-2.x.y.

Uwe Ligges

> 
>  last modification date was 14-Nov-2004 
> 
> 
> Thanks Anne
> ----------------------------------------------------
> Anne Piotet
> Tel: +41 79 359 83 32 (mobile)
> Email: anne.piotet at m-td.com
> ---------------------------------------------------
> M-TD Modelling and Technology Development
> PSE-C
> CH-1015 Lausanne
> Switzerland
> Tel: +41 21 693 83 98
> Fax: +41 21 646 41 33
> --------------------------------------------------
>  
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Thu Jan  6 12:05:45 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 6 Jan 2005 12:05:45 +0100
Subject: [R] *PACKAGE* vcd for R rw2001
In-Reply-To: <00c001c4f3dc$11f48a10$6c00a8c0@mtd4>
References: <00c001c4f3dc$11f48a10$6c00a8c0@mtd4>
Message-ID: <16861.7049.874006.163350@stat.math.ethz.ch>

>>>>> "Anne" == Anne Piotet <anne.piotet at urbanet.ch>
>>>>>     on Thu, 6 Jan 2005 11:39:56 +0100 writes:

    Anne> Is there an upgrate of the vcd library

it's a *PACKAGE*, not a "library" ...

    Anne>  (visualisation of categorical data) for the latest R version?  

    Anne> Trying to download it from CRAN I get

    Anne> URL
    Anne> /data/WWW/ftp/pub/R/bin/windows/contrib/r-release/vcd_0.1-3.4.zip
    Anne> was not found on this server.

Yes, there's a big "problem" on all CRAN mirrors, that all these
links are broken. Unfortunately today is holiday in the country
the CRAN webmasters live.

    Anne> googling it, I found it for instance on
    Anne> http://www.sourcekeg.co.uk/cran/bin/windows/contrib/1.9/
			
and the "1.9" tells you that this was done for R versions 1.9.x,
not version 2.0.0 and upwards

    Anne> but trying to install it gave me the message
    >> library(vcd)
    Anne> Error in library(vcd) : 'vcd' is not a valid package
						       -------
						       see, a "package"
    Anne> -- installed < 2.0.0?

I agree this is a bit misleading for people who install *binary*
packages (as you do when you install a zip file).

In your case, the error message should have meant

 "package built in R version < 2.0.0 ?"

(which of course it has been).

You need to get a windows-binary version of the package that was
built with an R version >= 2.0.0 
{by waiting a bit, till the CRAN links are fixed up}
or then learn how to build it yourself from the *source* of the
vcd package, which is a file  vcd_<vers>.tar.gz

Martin Maechler, ETH Zurich



From anne.piotet at urbanet.ch  Thu Jan  6 12:11:07 2005
From: anne.piotet at urbanet.ch (Anne)
Date: Thu, 6 Jan 2005 12:11:07 +0100
Subject: [R] library vcd for R rw2001
References: <00c001c4f3dc$11f48a10$6c00a8c0@mtd4>
	<41DD1927.7000609@statistik.uni-dortmund.de>
Message-ID: <00d001c4f3e0$6cd04970$6c00a8c0@mtd4>

Thanks! it seems I had a very cumbersome way of installing libraries...

Anne


----- Original Message ----- 
From: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>
To: "Anne" <anne.piotet at urbanet.ch>
Cc: "R list" <r-help at stat.math.ethz.ch>
Sent: Thursday, January 06, 2005 11:55 AM
Subject: Re: [R] library vcd for R rw2001


> Anne wrote:
>
> > Is there an upgrate of the vcd library (visualisation of categorical
data) for the latest R version?
>
> Obviously, the most recent version appeared on CRAN on 14-Nov-2004.
>
> > Trying to download it from CRAN I get
> >
> > URL /data/WWW/ftp/pub/R/bin/windows/contrib/r-release/vcd_0.1-3.4.zip
was not found on this server.
>
> The URL in the links got temporarily corrupted on CRAN.
>
> Just type
>      install.packages("vcd")
> within R, or go "manually" to
>      CRAN/bin/windows/contrib/r-release/
>
>
> > googling it, I found it for instance on
> > http://www.sourcekeg.co.uk/cran/bin/windows/contrib/1.9/
>
> Obviously, this was a binary package compiled fort R-1.9.x.
>
> > but trying to install it gave me the message
> >
> >>library(vcd)
> >
> > Error in library(vcd) : 'vcd' is not a valid package -- installed <
2.0.0?
>
> Yes, it was compiled under R-1.9.x, hence you cannot use it in R-2.x.y.
>
> Uwe Ligges
>
> >
> >  last modification date was 14-Nov-2004
> >
> >
> > Thanks Anne
> > ----------------------------------------------------
> > Anne Piotet
> > Tel: +41 79 359 83 32 (mobile)
> > Email: anne.piotet at m-td.com
> > ---------------------------------------------------
> > M-TD Modelling and Technology Development
> > PSE-C
> > CH-1015 Lausanne
> > Switzerland
> > Tel: +41 21 693 83 98
> > Fax: +41 21 646 41 33
> > --------------------------------------------------
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>



From gregor.gorjanc at bfro.uni-lj.si  Thu Jan  6 12:38:26 2005
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Thu, 06 Jan 2005 12:38:26 +0100
Subject: [R] How can one get scale for ellipses from plotcorr - like in paper
 of Duncan MURDOCH
Message-ID: <41DD2332.3090107@bfro.uni-lj.si>

Hello!

I used plotcorr for display of correlation matrix. I understand the meaning 
of ellipses but if you show that nice graph to someone who does not it is 
... So I would like to produce the scale of correlations coefficients from 
-1 to 1 by .1 and display corresponding ellipses. Since this can be used 
mainly with plorcorr I thought it would be nice to have additional option 
in plotcorr() to show the legend of ellipses.

I saw that scale in paper from Duncan MURDOCH on ellipses. I don't have 
paper here to cite it.

Any suggestions?

-- 
Lep pozdrav / With regards,
     Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia



From supton at referentia.com  Thu Jan  6 13:01:35 2005
From: supton at referentia.com (Stephen Upton)
Date: Thu, 6 Jan 2005 07:01:35 -0500
Subject: [R] Tuning string matching
In-Reply-To: <1105003927.41dd0597b1a6d@www.roda.ro>
Message-ID: <200501061201.j06C1d573775@mail2.referentia.com>

Adrian,

As an exercise, I took the pseudocode on the wiki pages for the Levenshtein
distance and programmed it in R. The code is below. I tested it for just 2
strings, so I'm not claiming that it *really* works, but it seems to. As you
can see, I didn't add any error checking, and there is likely some cool R
shortcuts that could be added. 

As to your problem, I'd also suggest that you might want to apply the below
function to possible combinations of words rather than attempting to apply
the function to a complete name; that should alleviate the "first.name
last.name", "last.name first.name" problem. 

> levenshtein.distance("Harrington","Harington")
[1] 1

> levenshtein.distance("Harrington Harry","Harry Harington")
[1] 11

HTH
Steve


levenshtein.distance <- function(string.1, string.2, subst.cost=1) {
    c1 <- strsplit(string.1,split="")[[1]]
    c2 <- strsplit(string.2,split="")[[1]]
    n <- length(c1)
    m <- length(c2)

    d <- array(0,dim=c(n+1,m+1))
 
    d[,1] <- 1:(n+1)
    d[1,] <- 1:(m+1)
    d[1,1] <- 0		


    for (i in 2:(n+1)) {

        for (j in 2:(m+1)) {

            if (c1[i-1] == c2[j-1]) cost <- 0 else cost <- subst.cost

            d[i,j] <- min(d[i-1,j] + 1,    # insertion
                          d[i,j-1] + 1,    # deletion
                          d[i-1,j-1] + cost) # substitution
	}
    }

    d[n+1,m+1]

}

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of adi at roda.ro
> Sent: Thursday, January 06, 2005 4:32 AM
> To: Jonathan Baron
> Cc: McGehee, Robert; bogdan romocea; r-help at stat.math.ethz.ch
> Subject: Re: [R] Tuning string matching
> 
> 
> Thank you all for your replies. Indeed I used:
> http://finzi.psych.upenn.edu/nmz.html
> as a search site. I used "string match", instead of "fuzzy string match".
> 
> Fuzzy matching seems to me a rather complicated matter, whereas my initial
> idea
> about solving this problem was a bit simpler:
> - check all characters in both strings (a 2 dimensional matrix of
> characters)
> - if 90% (or any other percent) of the characters in both strings are
> similar
> (in terms of distances between each character from the first string to all
> characters from the second string), then the two strings will be declared
> as a
> match
> 
> I just found out that this algorithm is called the "Levenshtein distance",
> and I
> know there is a PHP function called "levenshtein" (I thought it already
> might
> have been implemented in R).
> 
> For anyone that have a clue on how to read this stuff:
> http://ro.php.net/levenshtein
> 
> I tried to use agrep:
> > agrep("Harry Harrington", "Harry Harrington")
> [1] 1
> > agrep("Harry Harrington", "Harrington Harry")
> numeric(0)
> 
> So it seems not to be what I'm looking for (I'll try harder with edit
> distance,
> though)
> 
> Best regards,
> Adrian
> 
> 
> Quoting Jonathan Baron <baron at psych.upenn.edu>:
> 
> > Sorry for joining late, but I wanted to see if my search page
> > could help.  (I don't know which search archive you looked at.)
> > I entered
> > fuzzy string match*
> > and got a few things that look relevant, including the agrep
> > function.
> >
> > As for the second part of the question, that seems to be a coding
> > problem that is dependent on the current form of your data.
> > Write me off the list and I'll send you an R script I use for
> > similar things (making PayPal payments).
> >
> > Jon
> >
> >  Dear list,
> >
> >  I spent about two hours searching on the message archive, with no
> >  avail.
> >  I have a list of people that have to pass an on-line test, but only a
> >  fraction
> >  of them do it. Moreover, as they input their names, the resulting
> >  string do not
> >  always match the names I have in my database.
> >
> >  I would like to do two things:
> >
> >  1. Match any strings that are 90% the same
> >  Example:
> >  name1 <- "Harry Harrington"
> >  name2 <- "Harry Harington"
> >  I need a function that would declare those strings as a match (ideally
> >  having an
> >  argument that would allow introducing 80% instead of 90%)
> >
> > --
> > Jonathan Baron, Professor of Psychology, University of Pennsylvania
> > Home page: http://www.sas.upenn.edu/~baron
> > R search page: http://finzi.psych.upenn.edu/
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From Xin.Liu at arradx.com  Thu Jan  6 13:09:16 2005
From: Xin.Liu at arradx.com (Liu, Xin)
Date: Thu, 6 Jan 2005 12:09:16 -0000
Subject: [R] leave-one-out cross validation for randomForest
Message-ID: <3C4296559D4D4143A537F3F18AFC1C65190FD2@ni-cr-svc-ex1.pharms-services.com>

 Dear all,

 Can I get the leave-one-out cross validation error of randomForest in 
 R? I only found tune(), which got the 10-fold cross validation error. 
 Thanks for any information.

 Xin LIU

This e-mail is from ArraDx Ltd

The e-mail and any files transmitted with it are confidentia...{{dropped}}



From ligges at statistik.uni-dortmund.de  Thu Jan  6 14:11:26 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 06 Jan 2005 14:11:26 +0100
Subject: [R] leave-one-out cross validation for randomForest
In-Reply-To: <3C4296559D4D4143A537F3F18AFC1C65190FD2@ni-cr-svc-ex1.pharms-services.com>
References: <3C4296559D4D4143A537F3F18AFC1C65190FD2@ni-cr-svc-ex1.pharms-services.com>
Message-ID: <41DD38FE.7010408@statistik.uni-dortmund.de>

Liu, Xin wrote:

>  Dear all,
> 
>  Can I get the leave-one-out cross validation error of randomForest in 
>  R? I only found tune(), which got the 10-fold cross validation error. 
>  Thanks for any information.

For example errorest() in package "ipred" can be used for cross-validation.
I wonder which function tune() you are talking about. There is no such 
function in package "randomForest", AFAIK.

Uwe Ligges


>  Xin LIU
> 
> This e-mail is from ArraDx Ltd
> 
> The e-mail and any files transmitted with it are confidentia...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From petr.pikal at precheza.cz  Thu Jan  6 14:18:40 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 06 Jan 2005 14:18:40 +0100
Subject: [R] Targeting the elements that satisfy matching rules
	excluding	the NA
In-Reply-To: <200501051916.14410.bobo@medgen.univr.it>
Message-ID: <41DD48C0.24295.6735EF@localhost>

Hi Giovanni

I am not sure if it is the correct way but

regexpr("a",as.character(df$V1))>0
[1] FALSE  TRUE FALSE    NA FALSE  TRUE

gives me the logical vector you would like to see.

Cheers
Petr

On 5 Jan 2005 at 19:16, Giovanni Malerba wrote:

> Dear R-List,
> here my problem:
> > a <- c("gio","gao","geo",NA,"1","alpha")
> > b <- 1:6
> > data.frame(V1=a,V2=b) -> c
> > c
>      V1 V2
> 1   gio  1
> 2   gao  2
> 3   geo  3
> 4  <NA>  4
> 5     1  5
> 6 alpha  6
> 
> > rownames(c) %in% grep("a",as.character(c$V1))
> [1] FALSE  TRUE FALSE FALSE FALSE  TRUE
> 
> while I would like to obtain
> [1] FALSE  TRUE FALSE <NA> FALSE  TRUE
> 
> Is there a simple way to do this without doing things like
> > rownames(c) %in% grep("a",as.character(c$V1)) -> tmp
> > tmp[is.na(c$V1)]<- NA
> ?
> 
> It would be nice if grep (or another command) produced FALSE or TRUE
> according to the matching rule and NA if the value is NA. I was not
> able to find such a feature (just to do this in one simple
> command-line :)).
> 
> Thank you,
> Giovanni Malerba.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Thu Jan  6 14:38:55 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 06 Jan 2005 14:38:55 +0100
Subject: [R] aggregate()
In-Reply-To: <41DCB696.1060602@student.canterbury.ac.nz>
Message-ID: <41DD4D7F.12574.79C22A@localhost>



On 6 Jan 2005 at 16:55, Karla Meurk wrote:

> Hi, some time ago I asked R-help about aggregating data as a result I
> was able to put together some code which includes the line
> 
> rain.ag <- aggregate(newdata, list(hod6=cut(mindata,"6 hours")), mean,
> na.rm=T)
> 
> I also want to aggregate daily, and 30 minutely etc.
> 
> My question is why is it that I get answers with list(.. "hours") but
> R cannot cope with list(.."6 hours") or any other multiple.  I have
> tried overcoming this using nfrequency= but to no avail

Hi Karla

> aggregate(rnorm(100), list(weeks5 = cut(as.Date("2001/1/1") + 70*runif(100), "5 weeks")),mean)
      weeks5         x
1 2001-01-01 0.1272008
2 2001-02-05 0.1808671

This works as expected so you have some problems in your data 
and without giving more information what is mindata or what sort 
of answer you did get from above mentioned code nobody can 
help.

Cheers
Petr



> 
> can someone help?
> 
> Thanks
> 
> Carla
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From adi at roda.ro  Thu Jan  6 14:41:47 2005
From: adi at roda.ro (Adrian DUSA)
Date: Thu,  6 Jan 2005 15:41:47 +0200
Subject: [R] Tuning string matching
In-Reply-To: <200501061201.j06C1d573775@mail2.referentia.com>
References: <200501061201.j06C1d573775@mail2.referentia.com>
Message-ID: <1105018907.41dd401b8a96c@www.roda.ro>


Oh this is excellent, Stephen. Now I can create a code to break the initial
strings:
> name1 <- "Harry Harrington"
> name2 <- "Harington Harry"
> str1 <- unlist(strsplit(name1, " "))
> str2 <- unlist(strsplit(name2, " "))
> str1
[1] "Harry"      "Harrington"
> str2
[1] "Harington" "Harry"

and compare the words using your function. Brilliant.
Cheers,
Adrian

Quoting Stephen Upton <supton at referentia.com>:

> Adrian,
>
> As an exercise, I took the pseudocode on the wiki pages for the Levenshtein
> distance and programmed it in R. The code is below. I tested it for just 2
> strings, so I'm not claiming that it *really* works, but it seems to. As you
> can see, I didn't add any error checking, and there is likely some cool R
> shortcuts that could be added.
>
> As to your problem, I'd also suggest that you might want to apply the below
> function to possible combinations of words rather than attempting to apply
> the function to a complete name; that should alleviate the "first.name
> last.name", "last.name first.name" problem.
>
> > levenshtein.distance("Harrington","Harington")
> [1] 1
>
> > levenshtein.distance("Harrington Harry","Harry Harington")
> [1] 11
>
> HTH
> Steve
>
>
> levenshtein.distance <- function(string.1, string.2, subst.cost=1) {
>     c1 <- strsplit(string.1,split="")[[1]]
>     c2 <- strsplit(string.2,split="")[[1]]
>     n <- length(c1)
>     m <- length(c2)
>
>     d <- array(0,dim=c(n+1,m+1))
>
>     d[,1] <- 1:(n+1)
>     d[1,] <- 1:(m+1)
>     d[1,1] <- 0
>
>
>     for (i in 2:(n+1)) {
>
>         for (j in 2:(m+1)) {
>
>             if (c1[i-1] == c2[j-1]) cost <- 0 else cost <- subst.cost
>
>             d[i,j] <- min(d[i-1,j] + 1,    # insertion
>                           d[i,j-1] + 1,    # deletion
>                           d[i-1,j-1] + cost) # substitution
> 	}
>     }
>
>     d[n+1,m+1]
>
> }
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> > bounces at stat.math.ethz.ch] On Behalf Of adi at roda.ro
> > Sent: Thursday, January 06, 2005 4:32 AM
> > To: Jonathan Baron
> > Cc: McGehee, Robert; bogdan romocea; r-help at stat.math.ethz.ch
> > Subject: Re: [R] Tuning string matching
> >
> >
> > Thank you all for your replies. Indeed I used:
> > http://finzi.psych.upenn.edu/nmz.html
> > as a search site. I used "string match", instead of "fuzzy string match".
> >
> > Fuzzy matching seems to me a rather complicated matter, whereas my initial
> > idea
> > about solving this problem was a bit simpler:
> > - check all characters in both strings (a 2 dimensional matrix of
> > characters)
> > - if 90% (or any other percent) of the characters in both strings are
> > similar
> > (in terms of distances between each character from the first string to all
> > characters from the second string), then the two strings will be declared
> > as a
> > match
> >
> > I just found out that this algorithm is called the "Levenshtein distance",
> > and I
> > know there is a PHP function called "levenshtein" (I thought it already
> > might
> > have been implemented in R).
> >
> > For anyone that have a clue on how to read this stuff:
> > http://ro.php.net/levenshtein
> >
> > I tried to use agrep:
> > > agrep("Harry Harrington", "Harry Harrington")
> > [1] 1
> > > agrep("Harry Harrington", "Harrington Harry")
> > numeric(0)
> >
> > So it seems not to be what I'm looking for (I'll try harder with edit
> > distance,
> > though)
> >
> > Best regards,
> > Adrian
> >
> >
> > Quoting Jonathan Baron <baron at psych.upenn.edu>:
> >
> > > Sorry for joining late, but I wanted to see if my search page
> > > could help.  (I don't know which search archive you looked at.)
> > > I entered
> > > fuzzy string match*
> > > and got a few things that look relevant, including the agrep
> > > function.
> > >
> > > As for the second part of the question, that seems to be a coding
> > > problem that is dependent on the current form of your data.
> > > Write me off the list and I'll send you an R script I use for
> > > similar things (making PayPal payments).
> > >
> > > Jon
> > >
> > >  Dear list,
> > >
> > >  I spent about two hours searching on the message archive, with no
> > >  avail.
> > >  I have a list of people that have to pass an on-line test, but only a
> > >  fraction
> > >  of them do it. Moreover, as they input their names, the resulting
> > >  string do not
> > >  always match the names I have in my database.
> > >
> > >  I would like to do two things:
> > >
> > >  1. Match any strings that are 90% the same
> > >  Example:
> > >  name1 <- "Harry Harrington"
> > >  name2 <- "Harry Harington"
> > >  I need a function that would declare those strings as a match (ideally
> > >  having an
> > >  argument that would allow introducing 80% instead of 90%)
> > >
> > > --
> > > Jonathan Baron, Professor of Psychology, University of Pennsylvania
> > > Home page: http://www.sas.upenn.edu/~baron
> > > R search page: http://finzi.psych.upenn.edu/
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-
> > guide.html
>

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Adrian Dusa (adi at roda.ro)
Romanian Social Data Archive (www.roda.ro)
1, Schitu Magureanu Bd.
010181 Bucharest sector 5
Romania
Tel./Fax: +40 (21) 312.66.1



From a.beckerman at sheffield.ac.uk  Thu Jan  6 14:41:23 2005
From: a.beckerman at sheffield.ac.uk (Andrew Beckerman)
Date: Thu, 6 Jan 2005 13:41:23 +0000
Subject: [R] GLMM and crossed effects
Message-ID: <A7BA7C52-5FE8-11D9-B488-000A95CD7F02@sheffield.ac.uk>

Hi again.  Perhaps a simple question this time....

I am analysing data with a dependent variable of insect counts, a fixed  
effect of site and two random effects, day, which is the same set of 10  
days for each site, and then transect, which is nested within site (5  
each).

I am trying to fit the cross classified model using GLMM in lme4.  I  
have, for potential use, created a second coding of transect with  
levels 1-5 for site 1 and 6-10 for site2.  Likewise, if a groupedData  
object is necessary, there are als ts1 and ts2 dummy variables, as was  
necessary in the old lme.....

 > str(dat3)
`data.frame':	100 obs. of  7 variables:
  $ site  : Factor w/ 2 levels "Here","There": 1 1 1 1 1 1 1 1 1 1 ...
  $ day   : Factor w/ 10 levels "1","2","3","4",..: 1 1 1 1 1 2 2 2 2 2  
...
  $ trans : Factor w/ 5 levels "1","2","3","4",..: 1 2 3 4 5 1 2 3 4 5  
...
  $ count : int  77 109 81 124 115 84 90 85 130 106 ...
  $ trans2: Factor w/ 10 levels "1","2","3","4",..: 1 2 3 4 5 1 2 3 4 5  
...
  $ ts1   : Factor w/ 10 levels "Here 1","Here 2",..: 1 2 3 4 5 1 2 3 4  
5 ...
  $ ts2   : Factor w/ 10 levels "Here 1","Here 2",..: 1 2 3 4 5 1 2 3 4  
5 ...

Might someone explain to me how I might reflect the fact that transects  
are different between sites, while days are not?

#this does not work, though I thought it might be the best way to  
specify the model.....
 >  
GLMM(count~site,data=dat3,random=list(day=~1,trans=~1|site,family=poisso 
n)
Error in GLMM(count ~ site, data = dat3, random = list(day = ~1, trans  
= ~1 |  :
	subscript out of bounds
In addition: Warning message:
"|" not meaningful for factors in: Ops.factor(1, site)

#This does... but also note the differences in the summary and VarCorr  
variance components...
 >summary(GLMM(count~site,data=dat3,random=list(day=~1,trans2=~1),family= 
poisson))
Generalized Linear Mixed Model

Family: poisson family with log link
Fixed: count ~ site
Data: dat3
       AIC      BIC   logLik
  103.1494 116.1753 -46.5747

Random effects:
  Groups Name        Variance Std.Dev.
  trans2 (Intercept) 0.073011 0.27020
  day    (Intercept) 0.034373 0.18540
# of obs: 100, groups: trans2, 10; day, 10

Estimated scale (compare to 1)  0.6232135

Fixed effects:
             Estimate Std. Error z value Pr(>|z|)
(Intercept)  4.66280    0.13502  34.534   <2e-16 ***
siteThere   -0.25572    0.17216  -1.485   0.1375
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Correlation of Fixed Effects:
           (Intr)
siteThere -0.636
 >VarCorr(GLMM(count~site,data=dat3,random=list(day=~1,trans2=~1),family= 
poisson))
  Groups   Name        Variance Std.Dev.
  trans2   (Intercept) 0.028936 0.17010
  day      (Intercept) 0.013623 0.11672
  Residual             0.396322 0.62954


Many thanks
andrew



From Xin.Liu at arradx.com  Thu Jan  6 15:01:18 2005
From: Xin.Liu at arradx.com (Liu, Xin)
Date: Thu, 6 Jan 2005 14:01:18 -0000
Subject: [R] different result from the same errorest() in library( ipred)
Message-ID: <3C4296559D4D4143A537F3F18AFC1C65216A42@ni-cr-svc-ex1.pharms-services.com>

Dear all,

Does anybody can explain this: different results got when all the same parameters are used in the errorest() in library ipred, as the following?

errorest(Species ~ ., data=iris, model=randomForest, estimator = "cv", est.para=control.errorest(k=3),  mtry=2)$err 
[1] 0.03333333
> errorest(Species ~ ., data=iris, model=randomForest, estimator = "cv", est.para=control.errorest(k=3),  mtry=2)$err 
[1] 0.04
> errorest(Species ~ ., data=iris, model=randomForest, estimator = "cv", est.para=control.errorest(k=3),  mtry=2)$err 
[1] 0.05333333
> errorest(Species ~ ., data=iris, model=randomForest, estimator = "cv", est.para=control.errorest(k=3),  mtry=2)$err 
[1] 0.05333333
> errorest(Species ~ ., data=iris, model=randomForest, estimator = "cv", est.para=control.errorest(k=3),  mtry=2)$err 
[1] 0.04666667
> errorest(Species ~ ., data=iris, model=randomForest, estimator = "cv", est.para=control.errorest(k=3),  mtry=2)$err 
[1] 0.05333333
> errorest(Species ~ ., data=iris, model=randomForest, estimator = "cv", est.para=control.errorest(k=3),  mtry=2)$err 
[1] 0.04
> 

Xin LIU

This e-mail is from ArraDx Ltd

The e-mail and any files transmitted with it are confidentia...{{dropped}}



From ligges at statistik.uni-dortmund.de  Thu Jan  6 15:26:10 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 06 Jan 2005 15:26:10 +0100
Subject: [R] different result from the same errorest() in library( ipred)
In-Reply-To: <3C4296559D4D4143A537F3F18AFC1C65216A42@ni-cr-svc-ex1.pharms-services.com>
References: <3C4296559D4D4143A537F3F18AFC1C65216A42@ni-cr-svc-ex1.pharms-services.com>
Message-ID: <41DD4A82.4040002@statistik.uni-dortmund.de>

Liu, Xin wrote:

> Dear all,
> 
> Does anybody can explain this: different results got when all the same parameters are used in the errorest() in library ipred, as the following?
> 
> errorest(Species ~ ., data=iris, model=randomForest, estimator = "cv", est.para=control.errorest(k=3),  mtry=2)$err 
> [1] 0.03333333

Well, why do you expect to get the same results when you don't set the seed?

Both randomForest and the 3-fold cross validation make use of random 
numbers!

Uwe Ligges



>>errorest(Species ~ ., data=iris, model=randomForest, estimator = "cv", est.para=control.errorest(k=3),  mtry=2)$err 
> 
> [1] 0.04
> 
>>errorest(Species ~ ., data=iris, model=randomForest, estimator = "cv", est.para=control.errorest(k=3),  mtry=2)$err 
> 
> [1] 0.05333333
> 
>>errorest(Species ~ ., data=iris, model=randomForest, estimator = "cv", est.para=control.errorest(k=3),  mtry=2)$err 
> 
> [1] 0.05333333
> 
>>errorest(Species ~ ., data=iris, model=randomForest, estimator = "cv", est.para=control.errorest(k=3),  mtry=2)$err 
> 
> [1] 0.04666667
> 
>>errorest(Species ~ ., data=iris, model=randomForest, estimator = "cv", est.para=control.errorest(k=3),  mtry=2)$err 
> 
> [1] 0.05333333
> 
>>errorest(Species ~ ., data=iris, model=randomForest, estimator = "cv", est.para=control.errorest(k=3),  mtry=2)$err 
> 
> [1] 0.04
> 
> 
> Xin LIU
> 
> This e-mail is from ArraDx Ltd
> 
> The e-mail and any files transmitted with it are confidentia...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Tristan.Lefebure at univ-lyon1.fr  Thu Jan  6 15:37:09 2005
From: Tristan.Lefebure at univ-lyon1.fr (Lefebure Tristan)
Date: Thu, 6 Jan 2005 15:37:09 +0100
Subject: [R] RSvgDevice incomplete svg output
Message-ID: <200501061537.10335.lefebure@univ-lyon1.fr>

Hi
I use RSvgDevice to output plot, and modify them using svg editor (inkscape or 
sodipodi on Linux). Some month ago, results were perfect. I did exactly the 
same analysis today on the same data, and unfortunatly the results are 
different. While looking to the svg file, it seems that all information 
concerning "fill" and "stroke" of objects are lost. The consequence is that 
all objects become invisible ..........

exemple:
Correct old file:
.............
<polygon points="93.35 , 119.69 109.59 , 119.69 109.59 , 110.94 93.35 , 110.94 
" style="stroke-width:1;stroke:#000000;fill:#7F7F7F" />
...............

Today bad file:
..........................
<polygon points="174.51 , 53.95 190.74 , 53.95 190.74 , 53.95 174.51 , 53.95 " 
style="stroke-width:1;stroke:#FFFFFF;fill:none;stroke-dasharray" />
.................

A problem associated to R2.0 ?
Thanks 


package `RSvgDevice' version 0.5.3

$platform
[1] "i586-mandrake-linux-gnu"

$arch
[1] "i586"

$os
[1] "linux-gnu"

$system
[1] "i586, linux-gnu"

$status
[1] ""

$major
[1] "2"

$minor
[1] "0.0"

$year
[1] "2004"

$month
[1] "10"

$day
[1] "04"

$language
[1] "R"

-- 
------------------------------------------------------------
Tristan LEFEBURE
Laboratoire d'?cologie des hydrosyst?mes fluviaux (UMR 5023)
Universit? Lyon I - Campus de la Doua
Bat. Darwin C 69622 Villeurbanne - France

Phone: (33) (0)4 26 23 44 02
Fax: (33) (0)4 72 43 15 23



From chabotd at globetrotter.net  Thu Jan  6 15:40:07 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Thu, 6 Jan 2005 15:40:07 +0100
Subject: [R] "labels" attached to variable names
Message-ID: <DC203C0B-5FF0-11D9-8EA6-00050279D82B@globetrotter.net>

Hi,

Can we attach a more descriptive "label" (I may use the wrong  
terminology, which would explain why I found nothing on the FAQ) to  
variable names, and later have an easy way to switch to these labels in  
plots? I fear this is not possible and one must enter this by hand as  
ylab and xlab when making plots.

Thanks in advance,

Denis Chabot
------------------------------------------------------------------------ 
---------------------------
Denis Chabot, Ph.D.
Chercheur en bio?nerg?tique           Bioenergetics researcher
Institut Maurice-Lamontagne             Institut Maurice-Lamontagne
P?ches et Oc?ans Canada                Fisheries & Oceans Canada
CP 1000, Mont-Joli, QC                      PB 1000, Mont-Joli, QC
G5H 3Z4                                                G5H 3Z4
Canada                                                  Canada

(418) 775-0624			         (418) 775-0624
http://www.qc.dfo-mpo.gc.ca/iml



From bxc at steno.dk  Thu Jan  6 15:50:12 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Thu, 6 Jan 2005 15:50:12 +0100
Subject: [R] arrays emerging from tapply
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FE39A@exdkba022.novo.dk>

The code below illustrates some points about results from tapply that
I find strange. I wonder if they are intended and if so why it is so.

1) When you make a table the dimnames is a *named* list, tapply
   returns an unnamed list.

2) data.frame behaves differently on an array and a table. Is this
   an intended feature?

3) For tables class(TAB) and attr(TAB,"class") both return "table",
   but class(ARR) returns "array" whereas attr(TAB,"class") returns
   NULL.

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------

Code:
------------------------------------------------------------------------
-
Aye <- sample( c("Yes","Si","Oui"), 177, replace=TRUE )
Bee <- sample( c("Hum","Buzz"), 177, replace=TRUE )
Sea <- sample( c("White","Black","Red","Dead"), 177, replace=TRUE )
pyr <- rnorm( 177, 57, 14 )
TAB <- table( Aye, Bee, Sea )
ARR <- tapply( pyr, list( Aye, Bee, Sea ), sum )
dimnames( TAB )
dimnames( ARR )

data.frame( TAB )
data.frame( ARR )
data.frame( as.table( ARR ) )

class( TAB )
attr( TAB, "class" )
class( ARR )
attr( ARR, "class" )
------------------------------------------------------------------------
---

Result:
------------------------------------------------------------------------
---

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R              
> Aye <- sample( c("Yes","Si","Oui"), 177, replace=TRUE )
> Bee <- sample( c("Hum","Buzz"), 177, replace=TRUE )
> Sea <- sample( c("White","Black","Red","Dead"), 177, replace=TRUE )
> pyr <- rnorm( 177, 57, 14 )
> TAB <- table( Aye, Bee, Sea )
> ARR <- tapply( pyr, list( Aye, Bee, Sea ), sum )
> dimnames( TAB )
$Aye
[1] "Oui" "Si"  "Yes"

$Bee
[1] "Buzz" "Hum" 

$Sea
[1] "Black" "Dead"  "Red"   "White"

> dimnames( ARR )
[[1]]
[1] "Oui" "Si"  "Yes"

[[2]]
[1] "Buzz" "Hum" 

[[3]]
[1] "Black" "Dead"  "Red"   "White"

> 
> data.frame( TAB )
   Aye  Bee   Sea Freq
1  Oui Buzz Black    2
2   Si Buzz Black   11
3  Yes Buzz Black   10
4  Oui  Hum Black    7
5   Si  Hum Black   11
6  Yes  Hum Black    3
7  Oui Buzz  Dead    3
8   Si Buzz  Dead   11
9  Yes Buzz  Dead    5
10 Oui  Hum  Dead   11
11  Si  Hum  Dead    3
12 Yes  Hum  Dead    5
13 Oui Buzz   Red    5
14  Si Buzz   Red    4
15 Yes Buzz   Red    8
16 Oui  Hum   Red   11
17  Si  Hum   Red    9
18 Yes  Hum   Red    8
19 Oui Buzz White   10
20  Si Buzz White    7
21 Yes Buzz White    6
22 Oui  Hum White    8
23  Si  Hum White   11
24 Yes  Hum White    8
> data.frame( ARR )
  Buzz.Black Hum.Black Buzz.Dead Hum.Dead Buzz.Red  Hum.Red Buzz.White
Hum.White
1   130.5520  377.0875  177.9144 590.1498 346.4448 633.0975   492.9589
424.1464
2   607.4388  635.3015  651.3304 162.9175 223.1840 531.5512   347.7185
622.0213
3   555.8886  177.8216  295.0027 297.4550 417.6824 461.2143   359.4606
432.6165
> data.frame( as.table( ARR ) )
   Var1 Var2  Var3     Freq
1   Oui Buzz Black 130.5520
2    Si Buzz Black 607.4388
3   Yes Buzz Black 555.8886
4   Oui  Hum Black 377.0875
5    Si  Hum Black 635.3015
6   Yes  Hum Black 177.8216
7   Oui Buzz  Dead 177.9144
8    Si Buzz  Dead 651.3304
9   Yes Buzz  Dead 295.0027
10  Oui  Hum  Dead 590.1498
11   Si  Hum  Dead 162.9175
12  Yes  Hum  Dead 297.4550
13  Oui Buzz   Red 346.4448
14   Si Buzz   Red 223.1840
15  Yes Buzz   Red 417.6824
16  Oui  Hum   Red 633.0975
17   Si  Hum   Red 531.5512
18  Yes  Hum   Red 461.2143
19  Oui Buzz White 492.9589
20   Si Buzz White 347.7185
21  Yes Buzz White 359.4606
22  Oui  Hum White 424.1464
23   Si  Hum White 622.0213
24  Yes  Hum White 432.6165
> 
> class( TAB )
[1] "table"
> attr( TAB, "class" )
[1] "table"
> class( ARR )
[1] "array"
> attr( ARR, "class" )
NULL



From tlumley at u.washington.edu  Thu Jan  6 16:09:37 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 6 Jan 2005 07:09:37 -0800 (PST)
Subject: [R] variance of combinations of means - off topic
In-Reply-To: <41DC8922.8070103@acelerate.com>
References: <3A822319EB35174CA3714066D590DCD50994E4AC@usrymx25.merck.com>
	<41DC8922.8070103@acelerate.com>
Message-ID: <Pine.A41.4.61b.0501060708360.252478@homer08.u.washington.edu>

On Wed, 5 Jan 2005, Kjetil Brinchmann Halvorsen wrote:

> Liaw, Andy wrote:
>
>> You could try googling for "delta method".  I believe MASS even has code 
>> for
>> that...

I believe you are thinking of an example in S Programming, which does 
automatic differentiation and the delta method.

 	-thomas



From HDoran at air.org  Thu Jan  6 16:10:24 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 6 Jan 2005 10:10:24 -0500
Subject: [R] Generating Data mvrnorm and loops
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7407166D99@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050106/2fd30f4b/attachment.pl

From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Jan  6 16:10:30 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 6 Jan 2005 16:10:30 +0100
Subject: [R] "labels" attached to variable names
References: <DC203C0B-5FF0-11D9-8EA6-00050279D82B@globetrotter.net>
Message-ID: <008e01c4f401$dcaa27e0$0540210a@www.domain>

Hi Denis,

maybe something like this could be helpful:

x <- rnorm(20, 75); attr(x, "label") <- "Weight of Males"
y <- rnorm(20, 60); attr(y, "label") <- "Weight of Females"
plot. <- function(x, y, ...) plot.default(x, y, xlab=attr(x, "label"), 
ylab=attr(y, "label"), ...)
############
plot(x, y)
plot.(x, y)



Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Denis Chabot" <chabotd at globetrotter.net>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, January 06, 2005 3:40 PM
Subject: [R] "labels" attached to variable names


> Hi,
>
> Can we attach a more descriptive "label" (I may use the wrong 
> terminology, which would explain why I found nothing on the FAQ) to 
> variable names, and later have an easy way to switch to these labels 
> in  plots? I fear this is not possible and one must enter this by 
> hand as  ylab and xlab when making plots.
>
> Thanks in advance,
>
> Denis Chabot
> ------------------------------------------------------------------------
> ---------------------------
> Denis Chabot, Ph.D.
> Chercheur en bio?nerg?tique           Bioenergetics researcher
> Institut Maurice-Lamontagne             Institut Maurice-Lamontagne
> P?ches et Oc?ans Canada                Fisheries & Oceans Canada
> CP 1000, Mont-Joli, QC                      PB 1000, Mont-Joli, QC
> G5H 3Z4                                                G5H 3Z4
> Canada                                                  Canada
>
> (418) 775-0624          (418) 775-0624
> http://www.qc.dfo-mpo.gc.ca/iml
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tfliao at uiuc.edu  Thu Jan  6 16:15:21 2005
From: tfliao at uiuc.edu (Tim F Liao)
Date: Thu, 6 Jan 2005 09:15:21 -0600
Subject: [R] Segmentation fault while using Mclust
	function of mclust library in R-2.0.1
Message-ID: <8e234013.ba7581ed.81e8900@expms6.cites.uiuc.edu>

Tae-Hoon,

Your code worked with version 1.90:

> Mclust(rnorm(100))

 best model: univariate normal with 1 groups

 averge/median classification uncertainty: 0 / 0 

Warning message: 
optimal number of clusters occurs at min choice in: Mclust
(rnorm(100)) 

The warning should be there because the data you generated 
have only one cluster.  I'm fairly certain it would work 
with R version 2.0 as well which is what I have on my home 
computer and I used Mclust for one-dimensional data last 
month with that version.  If it's a problem with the latest 
version, you can simply download an earlier version of R 
(yesterday there was a thread about where to find version 
1.91 or something like it) and run Mclust with that version.

Tim Liao
Professor of Sociology & Statistics
University of Illinois

---- Original message ----
>Date: Thu, 06 Jan 2005 02:20:15 -0700
>From: Tae-Hoon Chung <thchung at tgen.org>  
>Subject: [R] Segmentation fault while using Mclust function 
of mclust library in R-2.0.1  
>To: "r-help at stat.math.ethz.ch" <r-help at stat.math.ethz.ch>
>
>Hi, all;
>
>I got an unusual error while using mclust library 2.1-7 on 
R 2.0.1.
>When I tried to run Mclust(rnorm(100)), I got segmentation 
fault error.
>Does anyone know what causes this problem?
>I came across the same problem even when I tried to run the 
example shown in
>Mclust help using iris data.
>
>Thanks in advance,
>Tae-Hoon Chung
>--------------------------------------------------
>Tae-Hoon Chung
>Post-Doctoral Researcher
>Translational Genomics Research Institute (TGen)
>445 N. 5th Street (Suite 530)
>Phoenix, AZ 85004
>1-602-343-8724 (Direct)
>1-480-323-9820 (Mobile)
>1-602-343-8840 (Fax)
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-
project.org/posting-guide.html



From peter.schlattmann at t-online.de  Thu Jan  6 16:27:48 2005
From: peter.schlattmann at t-online.de (peter.schlattmann@t-online.de)
Date: Thu,  6 Jan 2005 16:27:48 +0100
Subject: [R] nls - convergence problem
Message-ID: <1CmZYC-27VmCW0@filter10.bbul.t-online.de>

Dear list,

I do have a problem with nls. I use the following data:

>test
    time  conc dose
   0.50  5.40    1
   0.75 11.10    1
   1.00  8.40    1
   1.25 13.80    1
   1.50 15.50    1
   1.75 18.00    1
   2.00 17.00    1
   2.50 13.90    1
   3.00 11.20    1
  3.50  9.90    1
  4.00  4.70    1
  5.00  5.00    1
  6.00  1.90    1
  7.00  1.90    1
  9.00  1.10    1
12.00  0.95    1
14.00  0.46    1
24.00    NA    1
30.00    NA    1
36.00    NA    1

I use the self-starting function SSfol:

nls(conc~SSfol(dose,time,lKe,lKa,lCl),data=test,trace=T,control=nls.control(maxiter=13,tol=0.001,minFactor=1.E-500),na.action=na.omit)

This gives the following output:

99.15824 :  -1.2061792  0.1296157 -4.3020997 
86.07567 :  -0.7053265 -0.3873204 -4.1278009 
85.19743 :  -0.5548499 -0.5333776 -4.1173627 
85.19246 :  -0.5466376 -0.5415731 -4.1173247 
85.1922 :  -0.5444637 -0.5437461 -4.1173223 
85.1922 :  -0.5442240 -0.5439857 -4.1173223 
85.1922 :  -0.5441337 -0.5440760 -4.1173223 
85.1922 :  -0.5441104 -0.5440993 -4.1173223 
85.1922 :  -0.5440984 -0.5441113 -4.1173223 
85.1922 :  -0.5441089 -0.5441008 -4.1173223 
85.1922 :  -0.5441006 -0.5441091 -4.1173223 
85.1922 :  -0.5441051 -0.5441046 -4.1173223 
85.1922 :  -0.5441051 -0.5441046 -4.1173223 
85.1922 :  -0.5441051 -0.5441046 -4.1173223 

Error in nls(conc ~ SSfol(dose, time, lKe, lKa, lCl), data = test, trace
= T,  : 
        number of iterations exceeded maximum of 13

I do not understand this error message for two reasons:

1. Apparently the algorithm has converged, at the end of the output the
change of parameter estimates seems to be less than 0.001


2. If the maximum number of iterations is reached I would expect a
warning in the case that the algortihm did not converge. However, here
the algorithm apparently has converged. Thus I do not understand what
happened.

What can I do to deal with this problem?

Any help is appreciated!

Many thanks
peter



From chabotd at globetrotter.net  Thu Jan  6 16:29:09 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Thu, 6 Jan 2005 16:29:09 +0100
Subject: [R] position of labels in vis.gam or persp
Message-ID: <B5B93757-5FF7-11D9-8EA6-00050279D82B@globetrotter.net>

Hi,

Is there a way to control the position of labels in 3D plots such as  
that produced by vis.gam (in mgvc), or persp? Some of my plots have  
axis labels (titles) that collide with the tick labels. I have  
increased the margin around the plot so that there is plenty of space  
to move the axis labels further away from their axis, but I found no  
option for this in the documentation of either vis.gam or persp. The  
later states that some par options can work, but the one that seemed  
promising (mgp) does not have any effect in vis.gam.

Thanks for any help,

Denis
------------------------------------------------------------------------ 
---------------------------
Denis Chabot, Ph.D.
Chercheur en bio?nerg?tique           Bioenergetics researcher
Institut Maurice-Lamontagne             Institut Maurice-Lamontagne
P?ches et Oc?ans Canada                Fisheries & Oceans Canada
CP 1000, Mont-Joli, QC                      PB 1000, Mont-Joli, QC
G5H 3Z4                                                G5H 3Z4
Canada                                                  Canada

(418) 775-0624			         (418) 775-0624
http://www.qc.dfo-mpo.gc.ca/iml



From brian.tom at mrc-bsu.cam.ac.uk  Thu Jan  6 16:33:33 2005
From: brian.tom at mrc-bsu.cam.ac.uk (Brian D M Tom)
Date: Thu, 06 Jan 2005 15:33:33 +0000
Subject: [R] Parametric Survival Models with Left Truncation, survreg
Message-ID: <41DD5A4D.7090505@mrc-bsu.cam.ac.uk>

Hi,

I would like to fit parametric survival models to time-to-event data 
that are left truncated. I have checked the help page for survreg and 
looked in the R-help archive, and it appears that the R function survreg 
from the survival library (version 2.16) should allow me to take account 
of left truncation. However, when I try the command

summary(survreg(Surv(t,y,d,type="counting")~x2+x3,data=statadata0,dist="weibull",control=list(maxiter=100),na.action=na.exclude))

or

summary(survreg(Surv(t,y,d,type="counting")~x2+x3,data=statadata0,dist="lognormal",control=list(maxiter=100),na.action=na.exclude))

I get the following error message:

Error in survreg(Surv(t, y, d, type = "counting") ~ x2 + x3, data = 
statadata0,  :
         Invalid survival type


However, when I instead try fitting a Cox model (taking into account the 
left truncation) using coxph, i.e,

summary(coxph(Surv(t,y,d,type="counting")~x2+x3,data=statadata0,na.action=na.exclude))

it works:

 >
Call:
coxph(formula = Surv(t, y, d, type = "counting") ~ x2 + x3, data = 
statadata0,
     na.action = na.exclude)

   n= 235
      coef exp(coef) se(coef)    z       p
x2 1.2957      3.65   0.2771 4.68 2.9e-06
x3 0.0936      1.10   0.0128 7.30 2.9e-13

    exp(coef) exp(-coef) lower .95 upper .95
x2      3.65      0.274      2.12      6.29
x3      1.10      0.911      1.07      1.13

Rsquare= 0.322   (max possible= 0.961 )
Likelihood ratio test= 91.2  on 2 df,   p=0
Wald test            = 85.3  on 2 df,   p=0
Score (logrank) test = 106  on 2 df,   p=0

Am I doing something wrong or is there something wrong with survreg?

By the way, I have found a way to fit a Weibull model to left truncated 
data using weibreg from G?ran Brostr?m's library(eha):

 >library(eha) 
 >summary(weibreg(Surv(t,y,d,type="counting")~x2+x3,data=statadata0,na.action=na.exclude))

fit$fail =  0
Call:
weibreg(formula = Surv(t, y, d, type = "counting") ~ x2 + x3,
     data = statadata0, na.action = na.exclude)

Covariate           Mean       Coef  Rel.Risk      L-R p   Wald p
x2                  0.386     1.326     3.765               0.000
x3                 21.076     0.095     1.100               0.000

log(scale)          0.000     3.216    24.919               0.000
log(shape)          0.000     1.134     3.109               0.000

Events                    78
Total time at risk        2096.1
Max. log. likelihood      -262.02
LR test statistic         93.5
Degrees of freedom        4
Overall p-value           0

but I am then only limited to fitting Weibull models. Also, what I 
really am interested in, at the moment, is fitting Weibull "frailty" 
models, which I can't do with weibreg, but would be able to do (I 
think?) with survreg if it did allow left truncation. (I know that Cox 
frailty models can be fitted. For example,

 >summary(coxph(Surv(t,y,d,type="counting")~x2+x3+frailty(id),data=statadata0,na.action=na.exclude))
Call:
coxph(formula = Surv(t, y, d, type = "counting") ~ x2 + x3 +
     frailty(id), data = statadata0, na.action = na.exclude)

   n= 235
             coef  se(coef) se2    Chisq DF   p
x2          1.710 0.3395   0.2890 25.4   1.0 4.7e-07
x3          0.125 0.0191   0.0152 43.0   1.0 5.4e-11
frailty(id)                       88.5  65.1 2.9e-02

    exp(coef) exp(-coef) lower .95 upper .95
x2      5.53      0.181      2.84     10.76
x3      1.13      0.882      1.09      1.18

Iterations: 6 outer, 31 Newton-Raphson
      Variance of random effect= 1.23   I-likelihood = -328.2
Degrees of freedom for terms=  0.7  0.6 65.1
Rsquare= 0.625   (max possible= 0.961 )
Likelihood ratio test= 231  on 66.5 df,   p=0
Wald test            = 71.6  on 66.5 df,   p=0.311

).

If anyone knows of a way to fix survreg so that it allows left 
truncation, I would be most appreciated to hear from you.

All the best,

Brian



From canty at math.mcmaster.ca  Thu Jan  6 16:34:41 2005
From: canty at math.mcmaster.ca (Angelo Canty)
Date: Thu, 6 Jan 2005 10:34:41 -0500 (EST)
Subject: [R] boot package
In-Reply-To: <42129374-465E-11D9-9661-000393B3E9D0@utah.edu>
Message-ID: <Pine.LNX.4.44.0501061028100.5482-100000@mathserv>

Hi Nathan,

sorry about the delay in responding.  

The test of whether all of the bootstrap values are equal is in the wrong 
place.  I will correct it in the next release.  Meanwhile, I think the 
simplest solution is to fix(boot.ci) and move the line

    if (const(t, min(1e-08, mean(t)/1e+06))) {
        print(paste("All values of t are equal to ", mean(t), 
            "\n Cannot calculate confidence intervals"))
        return(NULL)
    }

from its current position to directly after the line

    t <- t[fins]

boot.ci should then work properly for missing statistic values.

Angelo

On Sat, 4 Dec 2004, Nathan Leon Pace, MD, MStat wrote:

> Hi,
> 
> I using the boot package 1.2-20 on R 2.0.1.
> 
> My statistics function estimates 6 parameters.
> 
> In a small percentage of resampled data sets my statistics function 
> doesn't produce an estimate for one parameter and the boot function 
> stops with an error.
> 
> I can write an ifelse(exists('parameter.estimate'), parameter.estimate, 
> NA) statement within the statistic function to substitute an NA for the 
> missing estimate value.
> 
> However, the boot.ci function to generate CIs from the boot object 
> won't accept NAs.
> 
> My problem is writing code to impute a numeric value for the missing 
> estimate. ifelse won't generate a numeric value if the test is mode 
> logical.
> 
> Any suggestions?
> 
> Nathan
> 
> 
> Nathan Leon Pace, MD, MStat	Work:n.l.pace at utah.edu
> Department of Anesthesiology	Home:nlpaces at comcast.net
> University of Utah			Work:801.581.6393
> Salt Lake City, Utah			    Home:801.467.2925
> 					Fax:801.581.4367										Cell:801.558.3987
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
------------------------------------------------------------------
|   Angelo J. Canty                Email: cantya at mcmaster.ca     |
|   Mathematics and Statistics     Phone: (905) 525-9140 x 27079 |
|   McMaster University            Fax  : (905) 522-0935         |
|   1280 Main St. W.                                             |
|   Hamilton ON L8S 4K1                                          |



From qunshi at cs.uchicago.edu  Thu Jan  6 16:39:10 2005
From: qunshi at cs.uchicago.edu (Qun Shi)
Date: Thu, 6 Jan 2005 09:39:10 -0600 (CST)
Subject: [R] (no subject)
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E4B1@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E4B1@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.58.0501060938340.7081@swank.cs.uchicago.edu>


Thank you very much for your help. I'm gonna try right now.-Jean

On Wed, 5 Jan 2005, Liaw, Andy wrote:

> Date: Wed, 5 Jan 2005 17:26:00 -0500
> From: "Liaw, Andy" <andy_liaw at merck.com>
> To: 'Qun Shi' <qunshi at cs.uchicago.edu>, "Liaw, Andy" <andy_liaw at merck.com>
> Cc: R-help at stat.math.ethz.ch
> Subject: RE: [R] (no subject)
>
> Googling for `rw1061.exe' turned up:
> http://www.cipic.ucdavis.edu/~dmrocke/Class/EAD289D/R/rw1061.exe
>
> Andy
>
> > From: Qun Shi
> >
> > Hi Andy,
> >
> > Thanks a lot for your promptly response. I searched the whole
> > web site, I
> > found the source code for version 1.6.X. Since I'm not a
> > computer person,
> > I don't how to compile it, but what I want is binary file for
> > Windows 2000
> > so that I could continue to work on my data. Could you or
> > someone kindly make
> > it for me. I would appreciate a lot.
> >
> > Thanks for your time, Jean
> >
> > On Wed, 5 Jan 2005, Liaw, Andy wrote:
> >
> > > Date: Wed, 5 Jan 2005 13:04:33 -0500
> > > From: "Liaw, Andy" <andy_liaw at merck.com>
> > > To: 'Qun Shi' <qunshi at cs.uchicago.edu>, R-help at stat.math.ethz.ch
> > > Subject: RE: [R] (no subject)
> > >
> > > Sources for versions of R as far back as 0.65, I believe,
> > are available on
> > > CRAN.  You can try to compile from source.
> > >
> > > Andy
> > >
> > > > From: Qun Shi
> > > >
> > > > Hi,
> > > >
> > > > I'm trying to use the version of dchip combined with R to
> > > > analyze my data.
> > > > I need R version 1.6 which fits for dchip as dchip manual
> > said. So I
> > > > would appreciate a lot if someone could tell me where I could
> > > > find this
> > > > version and download? I'm using Windows 2000.
> > > >
> > > > Thanks, Jean
> > > >
> > > >
> > > >
> > >
> > >
> >
> >
> >
>
>
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From anne.piotet at urbanet.ch  Thu Jan  6 16:46:10 2005
From: anne.piotet at urbanet.ch (Anne)
Date: Thu, 6 Jan 2005 16:46:10 +0100
Subject: [R] "labels" attached to variable names
References: <DC203C0B-5FF0-11D9-8EA6-00050279D82B@globetrotter.net>
Message-ID: <016201c4f406$d967b4d0$6c00a8c0@mtd4>

I use Frank Harrell's  library Hmisc to do this, it is very convenient!
>library(Hmisc)
> label(myVar)<-"my variable"

Anne


----- Original Message ----- 
From: "Denis Chabot" <chabotd at globetrotter.net>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, January 06, 2005 3:40 PM
Subject: [R] "labels" attached to variable names


> Hi,
>
> Can we attach a more descriptive "label" (I may use the wrong
> terminology, which would explain why I found nothing on the FAQ) to
> variable names, and later have an easy way to switch to these labels in
> plots? I fear this is not possible and one must enter this by hand as
> ylab and xlab when making plots.
>
> Thanks in advance,
>
> Denis Chabot
> ------------------------------------------------------------------------ 
> ---------------------------
> Denis Chabot, Ph.D.
> Chercheur en bio?nerg?tique           Bioenergetics researcher
> Institut Maurice-Lamontagne             Institut Maurice-Lamontagne
> P?ches et Oc?ans Canada                Fisheries & Oceans Canada
> CP 1000, Mont-Joli, QC                      PB 1000, Mont-Joli, QC
> G5H 3Z4                                                G5H 3Z4
> Canada                                                  Canada
>
> (418) 775-0624          (418) 775-0624
> http://www.qc.dfo-mpo.gc.ca/iml
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Thu Jan  6 16:50:41 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 6 Jan 2005 15:50:41 +0000 (UTC)
Subject: [R] Help for detrending
References: <6CC4DC1EC1F92D4B8A5FF590362E75650217D00A@neptune.betd.fr>
Message-ID: <loom.20050106T163750-299@post.gmane.org>

NDIKUMAGENGE Alice <andikumagenge <at> businessdecision.com> writes:

> I am a new user of R and I am a little bit "lost".
> I would like to know how tu use R for detrending, which kind
> of algorithms  I have to use.

The following will remove a trend from vector y:

	resid(lm(y ~ seq(along = y)))

Also check out ?stl and ?filter .



From johan.lindback at ucr.uu.se  Thu Jan  6 16:59:03 2005
From: johan.lindback at ucr.uu.se (=?ISO-8859-1?Q?Johan_Lindb=E4ck?=)
Date: Thu, 06 Jan 2005 16:59:03 +0100
Subject: [R] "labels" attached to variable names
In-Reply-To: <DC203C0B-5FF0-11D9-8EA6-00050279D82B@globetrotter.net>
References: <DC203C0B-5FF0-11D9-8EA6-00050279D82B@globetrotter.net>
Message-ID: <41DD6047.3060503@ucr.uu.se>

Denis Chabot wrote:
> Hi,
> 
> Can we attach a more descriptive "label" (I may use the wrong  
> terminology, which would explain why I found nothing on the FAQ) to  
> variable names, and later have an easy way to switch to these labels in  
> plots? I fear this is not possible and one must enter this by hand as  
> ylab and xlab when making plots.
> 
> Thanks in advance,
> 
> Denis Chabot


Try

library(Hmisc)
?label

/Johan


-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From ggrothendieck at myway.com  Thu Jan  6 16:54:13 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 6 Jan 2005 15:54:13 +0000 (UTC)
Subject: [R] "labels" attached to variable names
References: <DC203C0B-5FF0-11D9-8EA6-00050279D82B@globetrotter.net>
Message-ID: <loom.20050106T165113-496@post.gmane.org>

Denis Chabot <chabotd <at> globetrotter.net> writes:

: 
: Hi,
: 
: Can we attach a more descriptive "label" (I may use the wrong  
: terminology, which would explain why I found nothing on the FAQ) to  
: variable names, and later have an easy way to switch to these labels in  
: plots? I fear this is not possible and one must enter this by hand as  
: ylab and xlab when making plots.

See ?comment . e.g.

x <- 1:10
comment(x) <- "My X"
y <- x*x
comment(y) <- "My Y"

plot(x, y, xlab = comment(x), ylab = comment(y))


Another possibility is 'label' in package Hmisc.



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Jan  6 17:06:52 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 6 Jan 2005 17:06:52 +0100
Subject: [R] Generating Data mvrnorm and loops
References: <88EAF3512A55DF46B06B1954AEF73F7407166D99@dc1ex2.air.org>
Message-ID: <00cf01c4f409$bc156be0$0540210a@www.domain>

Hi Harold,

check the following:

library(MASS)
Sigma <- 
matrix(c(400,80,80,80,80,400,80,80,80,80,400,80,80,80,80,400), 4, 4)
mu <- c(100,150,200,250)
############

# generating N datasets and store in a list
lis <- lapply(1:10, mvrnorm, n=10, mu=mu, Sigma=Sigma)

# `lis1' contains the extra column whic is the sum of the first two
lis1 <- lapply(lis, function(x) cbind(x, x[,1]+x[,2]))

# `lis2' is `lis1' in a long format with the extra id indicator
lis2 <- lapply(lis, function(x) data.frame(id=rep(1:nrow(x), 
each=ncol(x)), score=c(t(x))))

#######
lis[[1]]; lis2[[1]]

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Doran, Harold" <HDoran at air.org>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, January 06, 2005 4:10 PM
Subject: [R] Generating Data mvrnorm and loops


> Dear List:
>
> I am generating N datasets using the following
>
> Sigma<-matrix(c(400,80,80,80,80,400,80,80,80,80,400,80,80,80,80,400),4,4
> )
> mu<-c(100,150,200,250)
>
> N=100
> for(i in 1:N)
> {
> assign(paste("Data.", i, sep=''),
> as.data.frame(cbind(seq(1:1000),(mvrnorm(n=1000, mu, Sigma)))))
> }
>
> With these datasets, I need to work on some of the variables and 
> then
> run each dataset through a linear model. I am having some trouble
> working with variables within the loop and wonder if anyone can 
> offer
> any pointers.
>
> The first thing I am trying to do is add 2 variables together that 
> are
> in each dataset. I am sure this is extremely trivial, but I can't 
> seem
> to get that to work.
>
> I have tried:
>
> for (i in 1:5){
> assign(paste("x",i,sep=""),(get(paste("Data.",i,sep=""))[["V2"]])+(get(p
> aste("Data.",i,sep=""))[["V2"]]))
> }
>
> Now, this code works, but I want for this vector to be a variable 
> within
> each dataframe. Outside the loop, the equivalent code would be
>
> attach(Data.1)
> Data.1$V6<-V1+V2
> Detach(Data.1)
>
> Another task I would like to perform is to reshape each dataframe 
> for
> longitudinal analysis. I have tried the following:
>
> for (i in 1:5){
> assign(paste("long",i,sep=""),reshape(paste("Data.",i,sep=""),idvar=get(
> paste("Data.",i,sep="")[["V1"]]),
> varying=list(names(get(paste("Data",i,sep="")[["V2"]]):get(paste("Data",
> i,sep="")[["V5"]])),v.names="score",direction="long")
> }
>
> This isn't working and I'm not sure if the code is even close.
>
> In general, performing all of these operations outside a loop for a
> single dataframe is simple. My trouble is performing equivalent
> operations within a loop.
>
> Thanks for any help offered.
>
> Harold
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jonsen at mathstat.dal.ca  Thu Jan  6 17:08:09 2005
From: jonsen at mathstat.dal.ca (Ian Jonsen)
Date: Thu, 06 Jan 2005 12:08:09 -0400
Subject: [R] rbugs in linux
Message-ID: <41DD6269.70900@mathstat.dal.ca>

Hi,

I am trying to run the "schools" example in the rbugs library (running 
winbugs in linux via wine-20041201) but keep getting the following 
error. Note I have tested wine and winbugs to confirm that both are 
operational.

Error in runBugs(bugs, script.file, n.chains, workingDir, useWine, wine,  :
        BUGS stopped before getting to coda.


The command I'm using is:

schools.sim <- rbugs(data = schools.data, inits, parameters, 
schools.bug, n.chains = 2, n.iter = 1000, workingDir = 
'/mnt/win_d/Rwork/daynightR', bugsWorkingDir = 
'/home/jonsen/.wine/drive_c', useWine = TRUE, wine = 
'/usr/local/bin/wine', debug = TRUE, verbose = TRUE)

Any suggestions are greatly appreciated.

Thanks,

-- 

Ian Jonsen, Postdoctoral Fellow

Biology Dept., Dalhousie University
801 LSC, 1355 Oxford St., Halifax, NS B3H 4J1 Canada
voice: 1-902-494-3910, fax: 1-902-494-3736
jonsen at mathstat.dal.ca



From ligges at statistik.uni-dortmund.de  Thu Jan  6 17:14:15 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 06 Jan 2005 17:14:15 +0100
Subject: [R] Generating Data mvrnorm and loops
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7407166D99@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7407166D99@dc1ex2.air.org>
Message-ID: <41DD63D7.1090700@statistik.uni-dortmund.de>

Doran, Harold wrote:

> Dear List:
> 
> I am generating N datasets using the following
> 
> Sigma<-matrix(c(400,80,80,80,80,400,80,80,80,80,400,80,80,80,80,400),4,4
> )
> mu<-c(100,150,200,250)
> 
> N=100
> for(i in 1:N) 
> { 
> assign(paste("Data.", i, sep=''),
> as.data.frame(cbind(seq(1:1000),(mvrnorm(n=1000, mu, Sigma)))))
> } 
> 
> With these datasets, I need to work on some of the variables and then
> run each dataset through a linear model. I am having some trouble
> working with variables within the loop and wonder if anyone can offer
> any pointers. 
> 
> The first thing I am trying to do is add 2 variables together that are
> in each dataset. I am sure this is extremely trivial, but I can't seem
> to get that to work.
> 
> I have tried:
> 
> for (i in 1:5){
> assign(paste("x",i,sep=""),(get(paste("Data.",i,sep=""))[["V2"]])+(get(p
> aste("Data.",i,sep=""))[["V2"]]))
> }

It's time to forget those 100 separate R objects, but try to work with a 
list, which makes life worth living again. ;-)

I'm reusing most of your code:

   Sigma <- matrix(
     c(400,80,80,80,80,400,80,80,80,80,400,80,80,80,80,400),
     4, 4)
   mu <- c(100,150,200,250)
   N <- 100

   Data <- lapply(seq(N), function(x)
     as.data.frame(cbind(1:1000, mvrnorm(n=1000, mu, Sigma))))

   # so we have a list Data with 100 elements,
   # each containing a data.frame.

   for(i in seq(along=Data))
     Data[[i]]$V6 <- Data[[i]]$V1 + Data[[i]]$V2


Uwe Ligges



> Now, this code works, but I want for this vector to be a variable within
> each dataframe. Outside the loop, the equivalent code would be
> 
> attach(Data.1)
> Data.1$V6<-V1+V2
> Detach(Data.1)
> 
> Another task I would like to perform is to reshape each dataframe for
> longitudinal analysis. I have tried the following:
> 
> for (i in 1:5){
> assign(paste("long",i,sep=""),reshape(paste("Data.",i,sep=""),idvar=get(
> paste("Data.",i,sep="")[["V1"]]),
> varying=list(names(get(paste("Data",i,sep="")[["V2"]]):get(paste("Data",
> i,sep="")[["V5"]])),v.names="score",direction="long")
> } 
>
> This isn't working and I'm not sure if the code is even close. 
> 
> In general, performing all of these operations outside a loop for a
> single dataframe is simple. My trouble is performing equivalent
> operations within a loop. 
> 
> Thanks for any help offered.
> 
> Harold
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From kent at darwin.eeb.uconn.edu  Thu Jan  6 17:13:50 2005
From: kent at darwin.eeb.uconn.edu (Kent Holsinger)
Date: Thu, 06 Jan 2005 11:13:50 -0500
Subject: [R] variance of combinations of means - off topic
In-Reply-To: <001401c4f340$e9b7cd50$ae1ad284@BIO041>
References: <001401c4f340$e9b7cd50$ae1ad284@BIO041>
Message-ID: <41DD63BE.7000409@darwin.eeb.uconn.edu>

Bill Shipley wrote:
> Hello, and please excuse this off-topic question, but I have not been
> able to find an answer elsewhere.  Consider a value Z that is calculated
> using the product (or ratio) of two means X_mean and Y_mean:
> Z=X_mean*Y_mean.  More generally, Z=f(X_mean, Y_mean).  The standard
> error of Z will be a function of the standard errors of the means of X
> and Y.  I want to calculate this se of Z.  Can someone direct me to a
> reference (text book or other) that gives the solution to this *general*
> problem?
> 

Kendall's Advanced Theory of statistics (sect. 10.6, p. 324 in the 5th 
edition) provides an approximate expression for the variance of a ratio 
(based on the delta method already mentioned).  Sect. 11.9 and following 
provides a general discussion of the distribution of a ratio, and some 
special cases (F-ratio, ratio of standard normals).

Kent

-- 
Kent E. Holsinger                kent at darwin.eeb.uconn.edu
                                  http://darwin.eeb.uconn.edu
-- Department of Ecology & Evolutionary Biology
-- University of Connecticut, U-3043
-- Storrs, CT   06269-3043



From f.harrell at vanderbilt.edu  Thu Jan  6 17:19:21 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 06 Jan 2005 10:19:21 -0600
Subject: [R] "labels" attached to variable names
In-Reply-To: <DC203C0B-5FF0-11D9-8EA6-00050279D82B@globetrotter.net>
References: <DC203C0B-5FF0-11D9-8EA6-00050279D82B@globetrotter.net>
Message-ID: <41DD6509.8040904@vanderbilt.edu>

Denis Chabot wrote:
> Hi,
> 
> Can we attach a more descriptive "label" (I may use the wrong  
> terminology, which would explain why I found nothing on the FAQ) to  
> variable names, and later have an easy way to switch to these labels in  
> plots? I fear this is not possible and one must enter this by hand as  
> ylab and xlab when making plots.
> 
> Thanks in advance,
> 
> Denis Chabot

The Hmisc package supports this:

label(x) <- 'Some Label'
describe(x)            # uses variable name and label
plot(x, y, xlab=label(x))
Better: xYplot(x, y)   # label used automatically

And if you do units(x) <- 'whatever units of measurement'  then xYplot, 
describe, and other Hmisc functions will include the units (in a 
different font on graphs or when using latex()).

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From p.dalgaard at biostat.ku.dk  Thu Jan  6 17:32:30 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Jan 2005 17:32:30 +0100
Subject: [R] arrays emerging from tapply
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E9027FE39A@exdkba022.novo.dk>
References: <0ABD88905D18E347874E0FB71C0B29E9027FE39A@exdkba022.novo.dk>
Message-ID: <x2652a8qn5.fsf@biostat.ku.dk>

"BXC (Bendix Carstensen)" <bxc at steno.dk> writes:

> The code below illustrates some points about results from tapply that
> I find strange. I wonder if they are intended and if so why it is so.
> 
> 1) When you make a table the dimnames is a *named* list, tapply
>    returns an unnamed list.

That depends on how you call it:

> tapply(airquality$Ozone,list(m=airquality$Month), mean,na.rm=T)
m
       5        6        7        8        9
23.61538 29.44444 59.11538 59.96154 31.44828

The table() function does a bit of extra user-friendliness using the
code controlled by deparse.level, that's all. If we had a named.list()
(or so) function that extended list() with similar functionality,
tapply calls could be made almost as user friendly.

 
> 2) data.frame behaves differently on an array and a table. Is this
>    an intended feature?

Yes (of course it is. What did you expect? Someone to have coded this
in his sleep?). You do want to be able to convert a matrix to a data
frame in the obvious way (d[i,j] <- m[i,j]), but for a table it is
more likely that you want to convert to the format suitable for a
Poisson glm analysis.
 
> 3) For tables class(TAB) and attr(TAB,"class") both return "table",
>    but class(ARR) returns "array" whereas attr(TAB,"class") returns
>    NULL.

Yes, that's a design quirk. In S3, objects only had a class if
explicitly specified via a class attribute, but nowadays all objects
have a class, so that methods can be dispatched on "numeric", etc.

The remaining issue (even if you don't mention it) is why tapply does
not return a table. This could arguably be a better choice, but I
suspect that some code depends on using tapply in ways that would be
incompatible. The change to returning a 1D array instead of a vector
certainly caused some trouble. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From KWollenberg at tufts-nemc.org  Thu Jan  6 17:35:37 2005
From: KWollenberg at tufts-nemc.org (Wollenberg, Kurt R)
Date: Thu, 6 Jan 2005 11:35:37 -0500 
Subject: [R] Calculating a table of symbol frequencies
Message-ID: <A16C769D4A7E564597075EA02A91C003047E87C2@neexchange01.nemc.org>

Hello all:

I have a protein sequence alignment in a data frame (align1, 72 x 236),
where each row is a protein and each column a site in the alignment. AA is
vector of amino acid symbols plus "-" (gap). I can calculate amino acid
frequencies at each site by:

>align1.F <- matrix(0,nrow=22,ncol=236,dimnames=list(AA,seq(1:236)))
>for(i in 1:236)
> align1.F[names(summary(align1[[i]])),i] <-
(summary(align1[[i]])/length(align1[[i]]))

Is there a more efficient (i.e., without a loop) way to do this? Is there
some way to use table or ftable to create an 22 x 236 table of amino acid
frequencies from align1 and AA in one fell swoop?

Thanks,
Kurt Wollenberg, PhD
Tufts Center for Vision Research 
New England Medical Center
750 Washington St, Box 450 
Boston, MA, USA
kwollenberg at tufts-nemc.org 
617-636-8945 (Fax)
617-636-9028 (Lab)

The most exciting phrase to hear in science, the one that heralds new
discoveries, is not "Eureka!" (I found it!) but  "That's funny ..." 
--Isaac Asimov


********************** 
Confidentiality Notice\ **********************\      The inf...{{dropped}}



From spencer.graves at pdf.com  Thu Jan  6 17:41:54 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 06 Jan 2005 08:41:54 -0800
Subject: [R] variance of combinations of means - off topic
In-Reply-To: <Pine.A41.4.61b.0501060708360.252478@homer08.u.washington.edu>
References: <3A822319EB35174CA3714066D590DCD50994E4AC@usrymx25.merck.com>	<41DC8922.8070103@acelerate.com>
	<Pine.A41.4.61b.0501060708360.252478@homer08.u.washington.edu>
Message-ID: <41DD6A52.3050409@pdf.com>

      Of course, the delta method is terrible when the first derivative 
is small relative to the curvature.  In that case, you either need to 
consider bootstrap, Monte Carlo, permutation testing, as suggested by 
Venables and Ripley in MASS and S Programming, or possibly using a 
higher order Taylor series expansion. 

      spencer graves

Thomas Lumley wrote:

> On Wed, 5 Jan 2005, Kjetil Brinchmann Halvorsen wrote:
>
>> Liaw, Andy wrote:
>>
>>> You could try googling for "delta method".  I believe MASS even has 
>>> code for
>>> that...
>>
>
> I believe you are thinking of an example in S Programming, which does 
> automatic differentiation and the delta method.
>
>     -thomas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From friendly at yorku.ca  Thu Jan  6 17:50:00 2005
From: friendly at yorku.ca (Michael Friendly)
Date: Thu, 06 Jan 2005 11:50:00 -0500
Subject: [R] patterns of missing data: determining monotonicity
Message-ID: <41DD6C38.4080002@yorku.ca>

Here is a problem that perhaps someone out here has an idea about.  It 
vaguely reminds me of something
I've seen before, but can't place.  Can anyone help?

For multiple imputation, there are simpler methods available if  the 
patterns of missing data are 'monotone' ---
if Vj is missing then all variables Vk, k>j are also missing, vs. more 
complex methods required when the patterns are not monotone.  The 
problem is to determine if, for a collection of variables, there is an 
ordering of them with a monotone
missing data pattern, or, if not, what the longest monotone sequence is.

Here is an example, where in a dataset of 65 observations, there are 8 
different patterns of missingness,
with X and . representing observed and missing:

Group   V2   V3   V4   V5   V6   V7   V8   V9   V10   V11   nmiss
  1     X    X    X    X    X    X    X    X     X     X      0  
  2     X    X    X    X    X    X    .    X     X     X      1  
  3     X    X    X    X    X    .    X    X     X     X      1  
  4     X    X    X    X    X    .    .    X     X     X      2  
  5     X    X    .    X    .    X    X    X     X     X      2  
  6     X    X    .    .    X    X    X    X     X     X      2  
  7     X    X    .    .    X    .    X    X     X     X      3  
  8     X    X    .    .    .    X    X    X     X     X      3  

Treated as a binary matrix, one can sort the columns by the number
of non-missing for each variable, and monotone means that there
are at most 2 runs -- a string of 0s followed by all 1s for *all*
patterns. But how
to determine an ordering (or orderings) of variables of maximal length?

Group   V2   V3   V9   V10   V11   V6   V8   V5   V7   V4   nmiss
  1      0    0    0    0     0     0    0    0    0    0     0  
  2      0    0    0    0     0     0    1    0    0    0     1  
  3      0    0    0    0     0     0    0    0    1    0     1  
  4      0    0    0    0     0     0    1    0    1    0     2  
  5      0    0    0    0     0     1    0    0    0    1     2  
  6      0    0    0    0     0     0    0    1    0    1     2  
  7      0    0    0    0     0     0    0    1    1    1     3  
  8      0    0    0    0     0     1    0    1    0    1     3  
        ==   ==   ==   ===   ===   ==   ==   ==   ==   ==
         0    0    0    0     0     2    2    3    3    4        



-- 
Michael Friendly     Email: friendly at yorku.ca 
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From anne.piotet at urbanet.ch  Thu Jan  6 17:55:36 2005
From: anne.piotet at urbanet.ch (Anne)
Date: Thu, 6 Jan 2005 17:55:36 +0100
Subject: [R] package Zelig problem with setx
Message-ID: <018701c4f410$8ce37ef0$6c00a8c0@mtd4>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050106/34faba12/attachment.pl

From susana.barbosa at fc.up.pt  Thu Jan  6 17:57:52 2005
From: susana.barbosa at fc.up.pt (susana barbosa)
Date: Thu, 6 Jan 2005 16:57:52 +0000
Subject: [R] Help for detrending
In-Reply-To: <6CC4DC1EC1F92D4B8A5FF590362E75650217D00A@neptune.betd.fr>
References: <6CC4DC1EC1F92D4B8A5FF590362E75650217D00A@neptune.betd.fr>
Message-ID: <200501061657.52972.susana.barbosa@fc.up.pt>


Dear Alice,

you may try stl decomposition

?stl


Best,
Susana

-- 
Susana Barbosa
http://www.fc.up.pt/pessoas/susana.barbosa
Departamento de Matematica Aplicada
Faculdade de Cincias, Universidade Porto
Rua do Campo Alegre, 687, 4169-007, Porto
Tel: 220 100 840
Fax: 220 100 809



From andikumagenge at businessdecision.com  Thu Jan  6 18:08:04 2005
From: andikumagenge at businessdecision.com (NDIKUMAGENGE Alice)
Date: Thu, 6 Jan 2005 18:08:04 +0100
Subject: =?utf-8?Q?RE=C2=A0=3A_=5BR=5D_Help_for_detrending?=
Message-ID: <6CC4DC1EC1F92D4B8A5FF590362E75650217D00F@neptune.betd.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050106/d5552c15/attachment.pl

From I.Wilson at maths.abdn.ac.uk  Thu Jan  6 19:47:49 2005
From: I.Wilson at maths.abdn.ac.uk (Ian Wilson)
Date: Thu, 06 Jan 2005 18:47:49 +0000
Subject: [R] rbugs in linux
In-Reply-To: <41DD6269.70900@mathstat.dal.ca>
References: <41DD6269.70900@mathstat.dal.ca>
Message-ID: <41DD87D5.5000309@maths.abdn.ac.uk>

On my machine  (SuSE 9.0) the following works.

schools.sim <- rbugs(data=schools.data, inits, parameters,
                     schools.bug, n.chains=3, n.iter=1000,
                     workingDir="/home/ijw/.wine/fake_windows/temp",
                     bugs="/windows/c/Program 
Files/WinBUGS14/WinBUGS14.exe",
                     bugsWorkingDir="c:/temp",
                     useWine=TRUE,
                     debug=TRUE)


I found that you have to put the *nix path to the winBUGS binary
rather than the windows path (which is suggested by the rbugs
article in RNEWS).

If you use

denug(runBugs) you can examine the call to winbugs that is made (and
try it from a command line).

Ian Wilson


Ian Jonsen wrote:

> Hi,
>
> I am trying to run the "schools" example in the rbugs library (running 
> winbugs in linux via wine-20041201) but keep getting the following 
> error. Note I have tested wine and winbugs to confirm that both are 
> operational.
>
> Error in runBugs(bugs, script.file, n.chains, workingDir, useWine, 
> wine,  :
>        BUGS stopped before getting to coda.
>
>
> The command I'm using is:
>
> schools.sim <- rbugs(data = schools.data, inits, parameters, 
> schools.bug, n.chains = 2, n.iter = 1000, workingDir = 
> '/mnt/win_d/Rwork/daynightR', bugsWorkingDir = 
> '/home/jonsen/.wine/drive_c', useWine = TRUE, wine = 
> '/usr/local/bin/wine', debug = TRUE, verbose = TRUE)
>
> Any suggestions are greatly appreciated.
>
> Thanks,
>



From matthew_wiener at merck.com  Thu Jan  6 20:39:18 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Thu, 6 Jan 2005 14:39:18 -0500
Subject: [R] Calculating a table of symbol frequencies
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E04993D46@uswsmx03.merck.com>

Kurt -- 

If you create a vector  of alignment positions, you should be able to do 

alignment.pos <- rep(1:236, each = 72)
table(data.frame(as.vector(align1), alignment.pos))

You may want to coerce align1 to a factor with appropriate levels, in case
you are missing some amino acids.  Otherwise there's an automatic coercion,
I believe, and you will use only the levels actually present in your data.

Here's an example using the first 10 letters of the alphabet instead of the
amino acid set:

> align1 <- matrix(sample(LETTERS[1:10], 200, replace = TRUE), nr = 5, nc =
40) 
> alignment.pos <- rep(1:40, each = 5)
> table(data.frame(as.vector(align1), alignment.pos))

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Wollenberg, Kurt R
Sent: Thursday, January 06, 2005 11:36 AM
To: 'r-help at stat.math.ethz.ch'
Subject: [R] Calculating a table of symbol frequencies


Hello all:

I have a protein sequence alignment in a data frame (align1, 72 x 236),
where each row is a protein and each column a site in the alignment. AA is
vector of amino acid symbols plus "-" (gap). I can calculate amino acid
frequencies at each site by:

>align1.F <- matrix(0,nrow=22,ncol=236,dimnames=list(AA,seq(1:236)))
>for(i in 1:236)
> align1.F[names(summary(align1[[i]])),i] <-
(summary(align1[[i]])/length(align1[[i]]))

Is there a more efficient (i.e., without a loop) way to do this? Is there
some way to use table or ftable to create an 22 x 236 table of amino acid
frequencies from align1 and AA in one fell swoop?

Thanks,
Kurt Wollenberg, PhD
Tufts Center for Vision Research 
New England Medical Center
750 Washington St, Box 450 
Boston, MA, USA
kwollenberg at tufts-nemc.org 
617-636-8945 (Fax)
617-636-9028 (Lab)

The most exciting phrase to hear in science, the one that heralds new
discoveries, is not "Eureka!" (I found it!) but  "That's funny ..." 
--Isaac Asimov


********************** 
Confidentiality Notice\ **********************\      The inf...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From cgk at stat.cmu.edu  Thu Jan  6 22:21:36 2005
From: cgk at stat.cmu.edu (Cari G Kaufman)
Date: Thu, 6 Jan 2005 16:21:36 -0500 (EST)
Subject: [R] animation without intermediate files?
Message-ID: <Pine.LNX.4.44.0501061548270.27024-100000@panic.stat.cmu.edu>


Hello, 

Does anyone know how to make "movies" in R by making a sequence of plots?  
I'd like to animate a long trajectory for exploratory purposes only,
without creating a bunch of image files and then using another program to
string them together.  In Splus I would do this using double.buffer()  to
eliminate the flickering caused by replotting. For instance, with a 2-D
trajectory in vectors x and y I would use the following:

motif()
double.buffer("back")
for (i in 1:length(x)) {
  plot(x[i], y[i], xlim=range(x), ylim=range(y))
  double.buffer("copy")
}
double.buffer("front")

I haven't found an equivalent function to double.buffer in R.  I tried
playing around with dev.set() and dev.copy() but so far with no success
(still flickers).

Thanks,
Cari



From choudary.jagar at swosu.edu  Thu Jan  6 23:35:25 2005
From: choudary.jagar at swosu.edu (Jagarlamudi, Choudary)
Date: Thu, 6 Jan 2005 16:35:25 -0600
Subject: [R] Help in Customising NLS function to spit out Mean and SD of the
	new fit!!!
Message-ID: <E03EBB50FF2C024781A6E4460AD58F0607C138@swosu-mbx01.admin.swosu.edu>

Hi all,
 I'm trying to customize the nls function code to spit out the sd and mean of the new fit. All it gives me are the fitted values. THis function seems to work fine for my project otherwise. All i need is the SD and MEAN. Can someone help me to customise. I frankly did not understand the nls code at all.
 
I have 10 x and 10 y values from a histogram. I need to use nls to do the best fit and give me the mean and sd of this new fit.
 
Thanks in advance
 
Choudary Jagarlamudi
Instructor
Southwestern Oklahoma State University
STF 254
100 campus Drive
Weatherford OK 73096
Tel 580-774-7136

From patrick.drechsler at gmx.net  Thu Jan  6 23:56:31 2005
From: patrick.drechsler at gmx.net (Patrick Drechsler)
Date: Thu, 06 Jan 2005 23:56:31 +0100
Subject: [R] autoscaling plot font size in Sweave output possible?
Message-ID: <m37jmqupy8.fsf@pdrechsler.fqdn.th-h.de>

Hi,

I was wondering if it's possible to have fonts in plots to be
autoscaled to the same font size used by LaTeX in a surrounding
Sweave document.

Here's a short example in which the fonts of the first plot are
barely readable:

--8<------------------------schnipp------------------------->8---
\documentclass{article}
\usepackage{graphicx}

\newcommand{\mytext}{Some normal sized text. Some normal sized
  text. Some normal sized text. Some normal sized text.}

\begin{document}
\SweaveOpts{prefix.string=pics/miniexample}

\mytext\par

%% small plot:
\setkeys{Gin}{width=.45\linewidth}
@ 
<<test0,echo=F,fig=T>>=
x <- 1:10
y <- sin(x)
plot(x,y,
     xlab="some x label",
     ylab="some y label"
     )
@ %def 

\par\mytext \mytext 

%% normal sized plot:
\setkeys{Gin}{width=.9\linewidth}
@ 
<<test1,echo=F,fig=T>>=
plot(x,y,
     xlab="some x label",
     ylab="some y label"
     )
@ %def 

\par\mytext

\end{document}
--8<------------------------schnapp------------------------->8---

Do I have to tweak the font size manually (as dicussed f.ex. in
[1] a while back) or is there a more general way to avoid this
problem?

Thankful for any pointers,

Patrick

,----[ R version: ]
| platform i686-pc-linux-gnu
| arch     i686             
| os       linux-gnu        
| system   i686, linux-gnu  
| status                    
| major    2                
| minor    0.0              
| year     2004             
| month    10               
| day      04               
| language R    
`----


Footnotes: 
[1] <URL:http://thread.gmane.org/gmane.comp.lang.r.general/20304>

-- 
Millions long for immortality who do not know what to do with
themselves on a rainy Sunday afternoon.
		-- Susan Ertz



From iquidim at hotmail.com  Fri Jan  7 00:40:03 2005
From: iquidim at hotmail.com (IGOR QUIDIM)
Date: Thu, 06 Jan 2005 21:40:03 -0200
Subject: [R] Survival Analysis with Long Time Survivors
Message-ID: <BAY12-F2268032F97A5D097A85C46AB930@phx.gbl>

Hi,

I was searching for the Survival Analysis with Long Time Survivors on 
Internet and I found the package gfcure for SPlus and R. Anyone uses this 
package? I have some problems to understand how it works...

Thanks a lot,

Igor



From spencer.graves at pdf.com  Fri Jan  7 00:46:13 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 06 Jan 2005 15:46:13 -0800
Subject: [R] Help in Customising NLS function to spit out Mean and SD
	of the	new fit!!!
In-Reply-To: <E03EBB50FF2C024781A6E4460AD58F0607C138@swosu-mbx01.admin.swosu.edu>
References: <E03EBB50FF2C024781A6E4460AD58F0607C138@swosu-mbx01.admin.swosu.edu>
Message-ID: <41DDCDC5.9060305@pdf.com>

      I'm very sorry, but I don't understand what you did and what you 
don't understand.  Have you worked through the examples in the "nls" 
documentation?  If you have and still have questions, "PLEASE do read 
the posting guide! http://www.R-project.org/posting-guide.html".  In 
particular, please provide a very brief and simple, self contained 
example, so someone can copy a few lines from your email and paste it 
into R to replicate what you got.  With that, you will more likely get a 
useful answer.  

      hope this helps. 
      spencer graves

Jagarlamudi, Choudary wrote:

>Hi all,
> I'm trying to customize the nls function code to spit out the sd and mean of the new fit. All it gives me are the fitted values. THis function seems to work fine for my project otherwise. All i need is the SD and MEAN. Can someone help me to customise. I frankly did not understand the nls code at all.
> 
>I have 10 x and 10 y values from a histogram. I need to use nls to do the best fit and give me the mean and sd of this new fit.
> 
>Thanks in advance
> 
>Choudary Jagarlamudi
>Instructor
>Southwestern Oklahoma State University
>STF 254
>100 campus Drive
>Weatherford OK 73096
>Tel 580-774-7136
>  
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ulask at bu.edu  Fri Jan  7 00:57:03 2005
From: ulask at bu.edu (Ulas Karaoz)
Date: Thu, 06 Jan 2005 18:57:03 -0500
Subject: [R] How to avoid rounding of matrix elements?
Message-ID: <41DDD04F.6040509@bu.edu>

Hi all R-users,
If I have a matrix with numeric elements as follows, the values are 
rounded when I try to refer to a specifici element using [], the value 
is rounded.
The same thing happens if the matrix is read from a file, the values are 
stored to the correct precision but then when I try to refer to a 
specific element (such as using [], it is rounded.

How do I avoid this rounding?
 >mdat<-matrix(c(0.0187972950,0.4446208550,1.0000000000,0.0003204380,0.0105002420,1.1087556380,0.0742164230,0.0362898240), 
nrow = 2, ncol=4)
 > mdat
           [,1]        [,2]       [,3]       [,4]
[1,] 0.01879729 1.000000000 0.01050024 0.07421642
[2,] 0.44462085 0.000320438 1.10875564 0.03628982

Thanks.



From tfliao at uiuc.edu  Fri Jan  7 01:44:37 2005
From: tfliao at uiuc.edu (Tim F Liao)
Date: Thu, 6 Jan 2005 18:44:37 -0600
Subject: [R] How to avoid rounding of matrix elements?
Message-ID: <81ea69d4.baa99f99.820ce00@expms6.cites.uiuc.edu>

The signif() function doesn't increase the number 
significant digits beyond the default.  However, if your 
purpose is simply to see more digits (or what was entered), 
a paste() function will do the trick though it will trim the 
trailing zeros.  Other ways?

Tim

---- Original message ----
>Date: Thu, 06 Jan 2005 18:57:03 -0500
>From: Ulas Karaoz <ulask at bu.edu>  
>Subject: [R] How to avoid rounding of matrix elements?  
>To: R-help at stat.math.ethz.ch
>
>Hi all R-users,
>If I have a matrix with numeric elements as follows, the 
values are 
>rounded when I try to refer to a specifici element using 
[], the value 
>is rounded.
>The same thing happens if the matrix is read from a file, 
the values are 
>stored to the correct precision but then when I try to 
refer to a 
>specific element (such as using [], it is rounded.
>
>How do I avoid this rounding?
> >mdat<-matrix(c
(0.0187972950,0.4446208550,1.0000000000,0.0003204380,0.010500
2420,1.1087556380,0.0742164230,0.0362898240), 
>nrow = 2, ncol=4)
> > mdat
>           [,1]        [,2]       [,3]       [,4]
>[1,] 0.01879729 1.000000000 0.01050024 0.07421642
>[2,] 0.44462085 0.000320438 1.10875564 0.03628982
>
>Thanks.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-
project.org/posting-guide.html



From macq at llnl.gov  Fri Jan  7 02:36:17 2005
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 6 Jan 2005 17:36:17 -0800
Subject: [R] How to avoid rounding of matrix elements?
In-Reply-To: <41DDD04F.6040509@bu.edu>
References: <41DDD04F.6040509@bu.edu>
Message-ID: <p06110413be03973de45a@[128.115.153.6]>

It's not being rounded, as this example will show:

>  x <- 0.1234567898765
>  x
[1] 0.1234568
>  print(x,digits=15)
[1] 0.1234567898765

It is being printed, or displayed, with fewer digits. But the 
underlying value has not changed.

-Don

At 6:57 PM -0500 1/6/05, Ulas Karaoz wrote:
>Hi all R-users,
>If I have a matrix with numeric elements as follows, the values are 
>rounded when I try to refer to a specifici element using [], the 
>value is rounded.
>The same thing happens if the matrix is read from a file, the values 
>are stored to the correct precision but then when I try to refer to 
>a specific element (such as using [], it is rounded.
>
>How do I avoid this rounding?
>  >mdat<-matrix(c(0.0187972950,0.4446208550,1.0000000000,0.0003204380,0.0105002420,1.1087556380,0.0742164230,0.0362898240), 
>nrow = 2, ncol=4)
>>  mdat
>           [,1]        [,2]       [,3]       [,4]
>[1,] 0.01879729 1.000000000 0.01050024 0.07421642
>[2,] 0.44462085 0.000320438 1.10875564 0.03628982
>
>Thanks.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From andy_liaw at merck.com  Fri Jan  7 02:38:06 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 6 Jan 2005 20:38:06 -0500
Subject: [R] How to avoid rounding of matrix elements?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4CB@usrymx25.merck.com>

Use the digits option to print():

> print(mdat, digits=20)
                      [,1]        [,2]               [,3]        [,4]
[1,] 0.0187972949999999990 1.000000000 0.0105002420000000 0.074216423
[2,] 0.4446208549999999800 0.000320438 1.1087556380000001 0.036289824

The output may look rounded, but they're not.

Andy

> From: Ulas Karaoz
> 
> Hi all R-users,
> If I have a matrix with numeric elements as follows, the values are 
> rounded when I try to refer to a specifici element using [], 
> the value 
> is rounded.
> The same thing happens if the matrix is read from a file, the 
> values are 
> stored to the correct precision but then when I try to refer to a 
> specific element (such as using [], it is rounded.
> 
> How do I avoid this rounding?
>  
> >mdat<-matrix(c(0.0187972950,0.4446208550,1.0000000000,0.00032
> 04380,0.0105002420,1.1087556380,0.0742164230,0.0362898240), 
> nrow = 2, ncol=4)
>  > mdat
>            [,1]        [,2]       [,3]       [,4]
> [1,] 0.01879729 1.000000000 0.01050024 0.07421642
> [2,] 0.44462085 0.000320438 1.10875564 0.03628982
> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From MSchwartz at MedAnalytics.com  Fri Jan  7 02:52:11 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 06 Jan 2005 19:52:11 -0600
Subject: [R] How to avoid rounding of matrix elements?
In-Reply-To: <41DDD04F.6040509@bu.edu>
References: <41DDD04F.6040509@bu.edu>
Message-ID: <1105062732.553.25.camel@horizons.localdomain>

On Thu, 2005-01-06 at 18:57 -0500, Ulas Karaoz wrote:
> Hi all R-users,
> If I have a matrix with numeric elements as follows, the values are 
> rounded when I try to refer to a specifici element using [], the
> value 
> is rounded.
> The same thing happens if the matrix is read from a file, the values
> are 
> stored to the correct precision but then when I try to refer to a 
> specific element (such as using [], it is rounded.
> 
> How do I avoid this rounding?
>  >mdat<-matrix(c
> (0.0187972950,0.4446208550,1.0000000000,0.0003204380,0.0105002420,1.1087556380,0.0742164230,0.0362898240), 
> nrow = 2, ncol=4)
>  > mdat
>            [,1]        [,2]       [,3]       [,4]
> [1,] 0.01879729 1.000000000 0.01050024 0.07421642
> [2,] 0.44462085 0.000320438 1.10875564 0.03628982
> 
> Thanks.

When you display the matrix, you are using a print method to do so. In
this case print.matrix() is being used. If you review the help for the
function (or ?print.default), you will see that there is an argument
called 'digits' which, if not explicitly defined, is set to 
options("digits"), which by default is 7.

Note importantly, that this has _nothing_ to do with the fashion in
which the data is being stored internally, which is by default a
'double' precision float. This only affects how the data is displayed.

The digits option, when printing a multi-element structure, is the
_minimum_ number of significant digits that will be printed.

If you want additional control over the number of decimal places that is
printed, you can do this in several ways:

# Explicitly use the print function
> print(mdat, digits = 9)
            [,1]        [,2]        [,3]        [,4]
[1,] 0.018797295 1.000000000 0.010500242 0.074216423
[2,] 0.444620855 0.000320438 1.108755638 0.036289824

# Increase 'digits' globally
> options(digits = 9)
> mdat
            [,1]        [,2]        [,3]        [,4]
[1,] 0.018797295 1.000000000 0.010500242 0.074216423
[2,] 0.444620855 0.000320438 1.108755638 0.036289824


# Use a format family function
> formatC(mdat, format = "f", digits = 12)
     [,1]             [,2]             [,3]            
[1,] "0.018797295000" "1.000000000000" "0.010500242000"
[2,] "0.444620855000" "0.000320438000" "1.108755638000"
     [,4]            
[1,] "0.074216423000"
[2,] "0.036289824000"


See:

?print.default
?options
?formatC
?format
?sprintf

for more information.

HTH,

Marc Schwartz



From jfox at mcmaster.ca  Fri Jan  7 03:27:16 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 6 Jan 2005 21:27:16 -0500
Subject: [R] How to avoid rounding of matrix elements?
In-Reply-To: <41DDD04F.6040509@bu.edu>
Message-ID: <20050107022715.XRIY19622.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Ulas,

A number of people have suggested changing the print precision via the
digits argument to print(), and have explained that what you see when a
number is printed doesn't necessarily reflect the precision of the stored
number. You can also reset the digits option, which defaults to 7. (You'll
still lose trailing zeroes, however.) For you example:

> options(digits=10)
> mdat
            [,1]        [,2]        [,3]        [,4]
[1,] 0.018797295 1.000000000 0.010500242 0.074216423
[2,] 0.444620855 0.000320438 1.108755638 0.036289824

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ulas Karaoz
> Sent: Thursday, January 06, 2005 6:57 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] How to avoid rounding of matrix elements?
> 
> Hi all R-users,
> If I have a matrix with numeric elements as follows, the 
> values are rounded when I try to refer to a specifici element 
> using [], the value is rounded.
> The same thing happens if the matrix is read from a file, the 
> values are stored to the correct precision but then when I 
> try to refer to a specific element (such as using [], it is rounded.
> 
> How do I avoid this rounding?
>  
> >mdat<-matrix(c(0.0187972950,0.4446208550,1.0000000000,0.00032
> 04380,0.0105002420,1.1087556380,0.0742164230,0.0362898240),
> nrow = 2, ncol=4)
>  > mdat
>            [,1]        [,2]       [,3]       [,4]
> [1,] 0.01879729 1.000000000 0.01050024 0.07421642 [2,] 
> 0.44462085 0.000320438 1.10875564 0.03628982
> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Andrew.Ward at qsa.qld.edu.au  Fri Jan  7 03:37:50 2005
From: Andrew.Ward at qsa.qld.edu.au (Andrew Ward)
Date: Fri, 7 Jan 2005 12:37:50 +1000
Subject: [R] Windows package development: bad html links to functions in
	non-standard packages
Message-ID: <86EED55AE819274B8308B62A225457D8016B868D@exch1.qsa.local>


I am using R 2.0.1 Patched on Windows 2000.

I have created a binary package for Windows that builds, checks,
installs and works without errors. In some of my .Rd files, I 
have links to functions in the standard packages supplied with R 
as well as links to others in add-on packages that I have installed.
For instance, one .Rd file has the following snippet:

\seealso{ \code{\link[lattice]{xyplot}},
          \code{\link[MASS]{rlm}},
          \code{\link[RODBC]{odbcConnect}}}

I have installed RODBC in a separate library, pointed to by
R_LIBS. The HTML links to xyplot and rlm work fine, but those to
packages not in the standard library can't be found (such as to
odbcConnect above). The link points to a non-existent file in the 
main library (file:///c:/r/library/RODBC/html/odbcConnect.html) 
rather than to the existing file in R_LIBS 
(z:/r/library/RODBC/html/odbcConnect.html).

I would be very grateful if someone could point out what I have 
carelessly overlooked in endeavouring to link to HTML files in both
the main and in non-standard packages libraries.

Thank you very much for your advice.

Regards,

Andrew C. Ward,                
Senior Analyst (Quantitative), Tel: +61 7 3864 0439
Queensland Studies Authority,  Fax: +61 7 3229 3318
295 Ann Street,
Brisbane Qld 4000, Australia 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
This email (including any attached files) is for the intended 
recipient(s) only. If you received this email by mistake, please, 
as a courtesy, tell the sender, then delete this email.

The views and opinions are the originator's and do not necessarily 
reflect those of the Queensland Studies Authority. All reasonable 
precautions have been taken to ensure that this email contained no 
viruses at the time it was sent.



From mikewlcheung at hku.hk  Fri Jan  7 06:45:02 2005
From: mikewlcheung at hku.hk (Mike Cheung)
Date: Fri, 07 Jan 2005 13:45:02 +0800
Subject: [R] Getting empirical percentiles for data
Message-ID: <41DE21DE.50301@hku.hk>

Dear List,

I have some discrete data and want to calculate the percentiles and the 
percentile ranks for each of the unique scores. I can calculate the 
percentiles with quantile().

I know that "ecdf" can be used to calculate the empirical cumulative 
distribution. However, I don't know how to exact the cumulative 
probabilities for each unique element. The requirement is similar to the 
"FREQUENCIES" in SPSS. Could someone help me in exacting the cumulative 
probabilities from the ecdf object? Thanks in advance!

# Generating artificial data
x <- round( rnorm(50, mean=50, sd=10) )
probs <- seq(0.1, 0.9, by=0.1)
# Calculating percentiles
x.percentiles <- quantile(x, probs)
# Calculating percentile ranks
x.ranks <- ecdf(x)

Best,
Mike



From Tom.Mulholland at dpi.wa.gov.au  Fri Jan  7 07:59:42 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Fri, 7 Jan 2005 14:59:42 +0800
Subject: [R] Basic Linear Algebra
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA5C@afhex01.dpi.wa.gov.au>

I don't normally have to go anywhere near this stuff , but it seems to me that this should be a straight-forward process in R.

For the purposes of this enquiry I thought I would use something I can work out on my own. 

So I have my matrix and the right hand results from that matrix

tdata <- matrix(c(0,1,0,-1,-1,2,0,0,-5,-6,0,0,3,-5,-6,1,-1,-1,0,0),byrow = T,ncol = 5)
sumtd <- c(0,0,0,-2)

> tdata
     [,1] [,2] [,3] [,4] [,5]
[1,]    0    1    0   -1   -1
[2,]    2    0    0   -5   -6
[3,]    0    0    3   -5   -6
[4,]    1   -1   -1    0    0
> sumtd
[1]  0  0  0 -2
> 

which I can calculate to give me 3x+30, x+12, 2x+20,  12x, x

Would someone be kind enough to point me in the right direction as to which tools I should be using and any sage words of advice for the barely informed.

Tom Mulholland

         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.0            
year     2004           
month    10             
day      04             
language R



From ripley at stats.ox.ac.uk  Fri Jan  7 08:54:18 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 7 Jan 2005 07:54:18 +0000 (GMT)
Subject: [R] Windows package development: bad html links to functions in
	non-standard packages
In-Reply-To: <86EED55AE819274B8308B62A225457D8016B868D@exch1.qsa.local>
References: <86EED55AE819274B8308B62A225457D8016B868D@exch1.qsa.local>
Message-ID: <Pine.LNX.4.61.0501070746150.15932@gannet.stats>

This is discussed in the documentation.  E.g. rw-FAQ says

3.7 Hyperlinks in Compiled HTML sometimes do not work.
======================================================

They may well not work between packages installed in different
libraries.  This is solved under Unix using symbolic links which Windows
does not implement.

and the same is true (but less so) for HTML.

And ?link.html.help says

      Cross-library links do not work on this platform.
      'fixup.package.URLs' attempts to correct links in the named
      package to the 'doc' directory (usually to icons) and to the
      'base', 'utils', 'graphics' and 'stats' packages, and then stamps
      a file 'fixedHTMLlinks' in the package directory.


On Fri, 7 Jan 2005, Andrew Ward wrote:

> I am using R 2.0.1 Patched on Windows 2000.
>
> I have created a binary package for Windows that builds, checks,
> installs and works without errors. In some of my .Rd files, I
> have links to functions in the standard packages supplied with R
> as well as links to others in add-on packages that I have installed.
> For instance, one .Rd file has the following snippet:
>
> \seealso{ \code{\link[lattice]{xyplot}},
>          \code{\link[MASS]{rlm}},
>          \code{\link[RODBC]{odbcConnect}}}
>
> I have installed RODBC in a separate library, pointed to by
> R_LIBS. The HTML links to xyplot and rlm work fine, but those to
> packages not in the standard library can't be found (such as to
> odbcConnect above). The link points to a non-existent file in the
> main library (file:///c:/r/library/RODBC/html/odbcConnect.html)
> rather than to the existing file in R_LIBS
> (z:/r/library/RODBC/html/odbcConnect.html).

Actually not: it points to a relative link, something like 
../../../RODBC/html/odbcConnect.html.

> I would be very grateful if someone could point out what I have
> carelessly overlooked in endeavouring to link to HTML files in both
> the main and in non-standard packages libraries.

Only the statement that it does not work.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From buser at stat.math.ethz.ch  Fri Jan  7 09:08:59 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Fri, 7 Jan 2005 09:08:59 +0100
Subject: [R] Getting empirical percentiles for data
In-Reply-To: <41DE21DE.50301@hku.hk>
References: <41DE21DE.50301@hku.hk>
Message-ID: <16862.17307.159577.446340@stat.math.ethz.ch>

Dear Mike

Mike Cheung writes:
 > Dear List,
 > 
 > I have some discrete data and want to calculate the percentiles and the 
 > percentile ranks for each of the unique scores. I can calculate the 
 > percentiles with quantile().
 > 
 > I know that "ecdf" can be used to calculate the empirical cumulative 
 > distribution. However, I don't know how to exact the cumulative 
 > probabilities for each unique element. The requirement is similar to the 
 > "FREQUENCIES" in SPSS. Could someone help me in exacting the cumulative 
 > probabilities from the ecdf object? Thanks in advance!

You can use the following function:

f.freq <- function(x)
{
  tab <- data.frame(table(x))
  tab$Percent <- tab$Freq*100/length(x)
  tab$Cum.Percent[1] <- tab$Percent[1]
  for(i in 2:length(tab[,1]))
    tab$Cum.Percent[i] <- tab$Cum.Percent[i-1] + tab$Percent[i]
  tab
}

x <- round( rnorm(50, mean=50, sd=10) )
f.freq(x)

This should give you a table analog to frequencies in SPSS.


 > 
 > # Generating artificial data
 > x <- round( rnorm(50, mean=50, sd=10) )
 > probs <- seq(0.1, 0.9, by=0.1)
 > # Calculating percentiles
 > x.percentiles <- quantile(x, probs)
 > # Calculating percentile ranks
 > x.ranks <- ecdf(x)
 > 
 > Best,
 > Mike
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


Best,
Christoph

-- 
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C11
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-1-632-5414		fax: 632-1228
http://stat.ethz.ch/~buser/



From ligges at statistik.uni-dortmund.de  Fri Jan  7 09:16:09 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 07 Jan 2005 09:16:09 +0100
Subject: [R] autoscaling plot font size in Sweave output possible?
In-Reply-To: <m37jmqupy8.fsf@pdrechsler.fqdn.th-h.de>
References: <m37jmqupy8.fsf@pdrechsler.fqdn.th-h.de>
Message-ID: <41DE4549.90509@statistik.uni-dortmund.de>

Patrick Drechsler wrote:

> Hi,
> 
> I was wondering if it's possible to have fonts in plots to be
> autoscaled to the same font size used by LaTeX in a surrounding
> Sweave document.


See ?strwidth which might help for the required calculations.

Uwe Ligges


> Here's a short example in which the fonts of the first plot are
> barely readable:
> 
> --8<------------------------schnipp------------------------->8---
> \documentclass{article}
> \usepackage{graphicx}
> 
> \newcommand{\mytext}{Some normal sized text. Some normal sized
>   text. Some normal sized text. Some normal sized text.}
> 
> \begin{document}
> \SweaveOpts{prefix.string=pics/miniexample}
> 
> \mytext\par
> 
> %% small plot:
> \setkeys{Gin}{width=.45\linewidth}
> @ 
> <<test0,echo=F,fig=T>>=
> x <- 1:10
> y <- sin(x)
> plot(x,y,
>      xlab="some x label",
>      ylab="some y label"
>      )
> @ %def 
> 
> \par\mytext \mytext 
> 
> %% normal sized plot:
> \setkeys{Gin}{width=.9\linewidth}
> @ 
> <<test1,echo=F,fig=T>>=
> plot(x,y,
>      xlab="some x label",
>      ylab="some y label"
>      )
> @ %def 
> 
> \par\mytext
> 
> \end{document}
> --8<------------------------schnapp------------------------->8---
> 
> Do I have to tweak the font size manually (as dicussed f.ex. in
> [1] a while back) or is there a more general way to avoid this
> problem?
> 
> Thankful for any pointers,
> 
> Patrick
> 
> ,----[ R version: ]
> | platform i686-pc-linux-gnu
> | arch     i686             
> | os       linux-gnu        
> | system   i686, linux-gnu  
> | status                    
> | major    2                
> | minor    0.0              
> | year     2004             
> | month    10               
> | day      04               
> | language R    
> `----
> 
> 
> Footnotes: 
> [1] <URL:http://thread.gmane.org/gmane.comp.lang.r.general/20304>
>



From p.dalgaard at biostat.ku.dk  Fri Jan  7 09:24:06 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Jan 2005 09:24:06 +0100
Subject: [R] Getting empirical percentiles for data
In-Reply-To: <41DE21DE.50301@hku.hk>
References: <41DE21DE.50301@hku.hk>
Message-ID: <x21xcx3avt.fsf@biostat.ku.dk>

Mike Cheung <mikewlcheung at hku.hk> writes:

> I know that "ecdf" can be used to calculate the empirical cumulative
> distribution. However, I don't know how to exact the cumulative
> probabilities for each unique element. The requirement is similar to
> the "FREQUENCIES" in SPSS. Could someone help me in exacting the
> cumulative probabilities from the ecdf object? Thanks in advance!
> 
> # Generating artificial data
> x <- round( rnorm(50, mean=50, sd=10) )
> probs <- seq(0.1, 0.9, by=0.1)
> # Calculating percentiles
> x.percentiles <- quantile(x, probs)
> # Calculating percentile ranks
> x.ranks <- ecdf(x)

I don't quite see why you call it "x.ranks", but it's just a function
that you could evaluate at the step points, so continuing your code:

val <- sort(unique(x))
cbind(val,cum.prop=x.ranks(val))

I'd go for a more direct approach though:

cumsum(prop.table(table(x)))


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Fri Jan  7 09:46:07 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Jan 2005 09:46:07 +0100
Subject: [R] Basic Linear Algebra
In-Reply-To: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA5C@afhex01.dpi.wa.gov.au>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA5C@afhex01.dpi.wa.gov.au>
Message-ID: <x2wtup1vao.fsf@biostat.ku.dk>

"Mulholland, Tom" <Tom.Mulholland at dpi.wa.gov.au> writes:

> > tdata
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    0    1    0   -1   -1
> [2,]    2    0    0   -5   -6
> [3,]    0    0    3   -5   -6
> [4,]    1   -1   -1    0    0
> > sumtd
> [1]  0  0  0 -2
> > 
> 
> which I can calculate to give me 3x+30, x+12, 2x+20,  12x, x

[So presumably you want the complete solution of tdata %*% y == sumtd]
I can't. 4th element should be just 12.
 
> Would someone be kind enough to point me in the right direction as
> to which tools I should be using and any sage words of advice for
> the barely informed.

Just move x*tdata[,5] (where x is y[5]) to the other side of the
equation and solve for each column:

> solve(tdata[,-5],sumtd)
[1] 30 12 20 12
> zapsmall(solve(tdata[,-5],-tdata[,5]))
[1] 3 1 2 0

(A general solution requires a bit more thought. E.g. you run into a
singularity if you try the same idea with the 4th column instead of
the 5th.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From andikumagenge at businessdecision.com  Fri Jan  7 10:24:44 2005
From: andikumagenge at businessdecision.com (NDIKUMAGENGE Alice)
Date: Fri, 7 Jan 2005 10:24:44 +0100
Subject: [R] Help for "calibration"
Message-ID: <6CC4DC1EC1F92D4B8A5FF590362E756502FA41A3@neptune.betd.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050107/a67b0ac0/attachment.pl

From Friedrich.Leisch at tuwien.ac.at  Fri Jan  7 09:49:04 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Fri, 7 Jan 2005 09:49:04 +0100
Subject: [R] autoscaling plot font size in Sweave output possible?
In-Reply-To: <m37jmqupy8.fsf@pdrechsler.fqdn.th-h.de>
References: <m37jmqupy8.fsf@pdrechsler.fqdn.th-h.de>
Message-ID: <16862.19712.400719.16915@celebrian.ci.tuwien.ac.at>

>>>>> On Thu, 06 Jan 2005 23:56:31 +0100,
>>>>> Patrick Drechsler (PD) wrote:

  > Hi,
  > I was wondering if it's possible to have fonts in plots to be
  > autoscaled to the same font size used by LaTeX in a surrounding
  > Sweave document.

Not using the standard mechanism, because there figures are rescaled
*after* they are created, and the font is rescaled together with the rest.

What yoy have to do is:

1) Create your own Sweave.sty file, say MySweave.sty which is a copy
   of the ofiginal one *without* the \setkeys{Gin} line and insert a

   \usepackage{MySweave}

   in the .Rnw document.


2) Explicitly set height and width of each figure chunk to what it
   should be in the final document. Unfortunately you cannot use any
   fractions of \textwidth or the like because Sweawe has no means to
   know what that might be.

3) Set the font size to what you use in the tex document using
   ps.options().


It is because of 2) that I didn't follow this route for the current
defaults, and I haven't found a really convincing alternative which
works in "most" situations ...

Hth,
Fritz

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f?r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit?t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra?e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From dmb at mrc-dunn.cam.ac.uk  Fri Jan  7 11:20:44 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Fri, 7 Jan 2005 10:20:44 +0000 (GMT)
Subject: [R] hist function to give each cell equal area
Message-ID: <Pine.LNX.4.21.0501071004140.25850-100000@mail.mrc-dunn.cam.ac.uk>


Hi,

I want to use hist with non-equi-spaced breaks, picked such that the
fraction of the data points falling in the cells (defined by 'breaks') is 
roughly equal accross all cells.

Is there such a function that will automatically try to determine the
breaks to fullfill this requirement?

Something like..

hist( x, br = magic_function_to_pick_breaks())


For example, if x looked like this...

x <- c( 1:5, 10+1:5, 100+1:5 )

the breaks would define cells like this 

hist(x,breaks=c(0,5,15,105))

Is there such a function?



From r.hankin at soc.soton.ac.uk  Fri Jan  7 11:26:20 2005
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Fri, 7 Jan 2005 10:26:20 +0000
Subject: [R] Visualizing complex analytic functions using domain coloring
Message-ID: <92C4B90A-6096-11D9-85C9-000A95D86AA8@soc.soton.ac.uk>

Hi

has anyone coded up domain colouring for visualizing complex analytic 
functions
(such as elliptic functions)?

[
the idea is to depict a complex function f(z) using a filled.contour() 
variant
in which the hue is given by Arg(f(z)), and the saturation by Mod(f(z)).
]


--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From bxc at steno.dk  Fri Jan  7 11:31:49 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Fri, 7 Jan 2005 11:31:49 +0100
Subject: [R] hist function to give each cell equal area
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FE3C5@exdkba022.novo.dk>

how about:

x <- rnorm(400)
nbin<-7
hist(x,breaks=quantile(x,prob=seq(0,1,length=nbin+1)))

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dan Bolser
> Sent: Friday, January 07, 2005 11:21 AM
> To: R mailing list
> Subject: [R] hist function to give each cell equal area
> 
> 
> 
> Hi,
> 
> I want to use hist with non-equi-spaced breaks, picked such 
> that the fraction of the data points falling in the cells 
> (defined by 'breaks') is 
> roughly equal accross all cells.
> 
> Is there such a function that will automatically try to 
> determine the breaks to fullfill this requirement?
> 
> Something like..
> 
> hist( x, br = magic_function_to_pick_breaks())
> 
> 
> For example, if x looked like this...
> 
> x <- c( 1:5, 10+1:5, 100+1:5 )
> 
> the breaks would define cells like this 
> 
> hist(x,breaks=c(0,5,15,105))
> 
> Is there such a function?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From Achim.Zeileis at wu-wien.ac.at  Fri Jan  7 11:38:48 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 7 Jan 2005 11:38:48 +0100 (CET)
Subject: [R] hist function to give each cell equal area
In-Reply-To: <Pine.LNX.4.21.0501071004140.25850-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0501071004140.25850-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.LNX.4.58.0501071134570.24091@thorin.ci.tuwien.ac.at>

On Fri, 7 Jan 2005, Dan Bolser wrote:

>
> Hi,
>
> I want to use hist with non-equi-spaced breaks, picked such that the
> fraction of the data points falling in the cells (defined by 'breaks') is
> roughly equal accross all cells.
>
> Is there such a function that will automatically try to determine the
> breaks to fullfill this requirement?
>
> Something like..
>
> hist( x, br = magic_function_to_pick_breaks())

It's not that magic, but such points are usually called quantiles in
statistics...

> For example, if x looked like this...
>
> x <- c( 1:5, 10+1:5, 100+1:5 )
>
> the breaks would define cells like this
>
> hist(x,breaks=c(0,5,15,105))

quantile(x, 0:3/3, type = 1)

hth,
Z

> Is there such a function?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Fri Jan  7 11:47:27 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Jan 2005 11:47:27 +0100
Subject: [R] hist function to give each cell equal area
In-Reply-To: <Pine.LNX.4.21.0501071004140.25850-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0501071004140.25850-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <x2acrlcy80.fsf@biostat.ku.dk>

Dan Bolser <dmb at mrc-dunn.cam.ac.uk> writes:

> Hi,
> 
> I want to use hist with non-equi-spaced breaks, picked such that the
> fraction of the data points falling in the cells (defined by 'breaks') is 
> roughly equal accross all cells.
> 
> Is there such a function that will automatically try to determine the
> breaks to fullfill this requirement?
> 
> Something like..
> 
> hist( x, br = magic_function_to_pick_breaks())
> 
> 
> For example, if x looked like this...
> 
> x <- c( 1:5, 10+1:5, 100+1:5 )
> 
> the breaks would define cells like this 
> 
> hist(x,breaks=c(0,5,15,105))
> 
> Is there such a function?

Probably not giving "pretty" numbers, but quantile() gets you most of
the way

 quantile(x,seq(0,1,1/3))
 hist(x,breaks=quantile(x,seq(0,1,1/3)))
 table(cut(x,breaks=quantile(x,seq(0,1,1/3))))

obviously, with that distribution you have to be a little careful:

 hist(x,breaks=quantile(x)) does look somewhat different...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Roger.Bivand at nhh.no  Fri Jan  7 12:01:36 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 7 Jan 2005 12:01:36 +0100 (CET)
Subject: [R] hist function to give each cell equal area
In-Reply-To: <Pine.LNX.4.21.0501071004140.25850-100000@mail.mrc-dunn.cam.ac.
	uk>
Message-ID: <Pine.LNX.4.44.0501071159220.3304-100000@reclus.nhh.no>

On Fri, 7 Jan 2005, Dan Bolser wrote:

> 
> Hi,
> 
> I want to use hist with non-equi-spaced breaks, picked such that the
> fraction of the data points falling in the cells (defined by 'breaks') is 
> roughly equal accross all cells.

?quantile

> 
> Is there such a function that will automatically try to determine the
> breaks to fullfill this requirement?
> 
> Something like..
> 
> hist( x, br = magic_function_to_pick_breaks())
> 
> 
> For example, if x looked like this...
> 
> x <- c( 1:5, 10+1:5, 100+1:5 )
> 
> the breaks would define cells like this 
> 
> hist(x,breaks=c(0,5,15,105))

hist(x, quantile(x, seq(0,1,1/3)))

isn't the same, but your breaks are arbitrary and based on knowing where 
the gaps are.

> 
> Is there such a function?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From fren2 at yahoo.com  Fri Jan  7 12:11:40 2005
From: fren2 at yahoo.com (Frederic renaud)
Date: Fri, 7 Jan 2005 03:11:40 -0800 (PST)
Subject: [R] lognorm
Message-ID: <20050107111140.11394.qmail@web51806.mail.yahoo.com>

Hi!
I 've a problem to have a lognorm distribution with
mean=1 and var (or sigma)=1.

rlnorm(1000,0,0)
rlnorm(1000,1,1)
rlnorm(1000,0,1)
....                     ?

Can you help me?



From dmb at mrc-dunn.cam.ac.uk  Fri Jan  7 12:14:22 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Fri, 7 Jan 2005 11:14:22 +0000 (GMT)
Subject: [R] hist function to give each cell equal area
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E9027FE3C5@exdkba022.novo.dk>
Message-ID: <Pine.LNX.4.21.0501071109340.26888-100000@mail.mrc-dunn.cam.ac.uk>

On Fri, 7 Jan 2005, BXC (Bendix Carstensen) wrote:

>how about:
>
>x <- rnorm(400)
>nbin<-7
>hist(x,breaks=quantile(x,prob=seq(0,1,length=nbin+1)))

Thanks for the replies! Quantiles are my new friend. Its true the example
I gave was a bit tricky, I just wanted to get the idea over.

One thing that is slightly confusing (probably a slight syntax thing),
combining the above with the reply from Peter Dalgaard...

nbin<-10
x <- rnorm(400)
hist(x,breaks=quantile(x,prob=seq(0,1,length=nbin+1)))

y <- table(cut(x,breaks=quantile(x,seq(0,1,length=nbin+1))))

> y
[1] 399

I only get 399 counts out, and I put 400 counts in?

Cheers,
Dan.

>
>Bendix Carstensen
>----------------------
>Bendix Carstensen
>Senior Statistician
>Steno Diabetes Center
>Niels Steensens Vej 2
>DK-2820 Gentofte
>Denmark
>tel: +45 44 43 87 38
>mob: +45 30 75 87 38
>fax: +45 44 43 07 06
>bxc at steno.dk
>www.biostat.ku.dk/~bxc
>----------------------
>
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dan Bolser
>> Sent: Friday, January 07, 2005 11:21 AM
>> To: R mailing list
>> Subject: [R] hist function to give each cell equal area
>> 
>> 
>> 
>> Hi,
>> 
>> I want to use hist with non-equi-spaced breaks, picked such 
>> that the fraction of the data points falling in the cells 
>> (defined by 'breaks') is 
>> roughly equal accross all cells.
>> 
>> Is there such a function that will automatically try to 
>> determine the breaks to fullfill this requirement?
>> 
>> Something like..
>> 
>> hist( x, br = magic_function_to_pick_breaks())
>> 
>> 
>> For example, if x looked like this...
>> 
>> x <- c( 1:5, 10+1:5, 100+1:5 )
>> 
>> the breaks would define cells like this 
>> 
>> hist(x,breaks=c(0,5,15,105))
>> 
>> Is there such a function?
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read 
>> the posting guide! http://www.R-project.org/posting-guide.html
>> 
>



From Ted.Harding at nessie.mcc.ac.uk  Fri Jan  7 12:21:46 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 07 Jan 2005 11:21:46 -0000 (GMT)
Subject: [R] Help for "calibration"
In-Reply-To: <6CC4DC1EC1F92D4B8A5FF590362E756502FA41A3@neptune.betd.fr>
Message-ID: <XFMail.050107112146.Ted.Harding@nessie.mcc.ac.uk>

On 07-Jan-05 NDIKUMAGENGE Alice wrote:
> Hello,
>  
> I've tried stl decomposition but I made a mistake 
> I want to make a "calibration on a sample", in french we called it
> "Redressement d'un ??chantillon"
>  
> Thank u

Are you referring to the sort of thing decsribed, for example,  in

  http://statbel.fgov.be/studies/cal2002d.pdf

and

  http://statbel.fgov.be/studies/cal_en.asp

?

If so, then you may find useful things in Thomas Lumley's
"survey" package, though I'm not expert enough to judge how
extensively it covers this area.

The PDF reference above gives a general mathemtical formulation
which looks as though a solution could be implemented in R as
an optimisation problem.

In any case, it would help if you would describe your problem
in more specific terms!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 07-Jan-05                                       Time: 11:21:46
------------------------------ XFMail ------------------------------



From ernesto at ipimar.pt  Fri Jan  7 12:48:11 2005
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Fri, 07 Jan 2005 11:48:11 +0000
Subject: [R] lognorm
In-Reply-To: <20050107111140.11394.qmail@web51806.mail.yahoo.com>
References: <20050107111140.11394.qmail@web51806.mail.yahoo.com>
Message-ID: <41DE76FB.7010106@ipimar.pt>

 > args(rlnorm)
function (n, meanlog = 0, sdlog = 1)
NULL

Frederic renaud wrote:
> Hi!
> I 've a problem to have a lognorm distribution with
> mean=1 and var (or sigma)=1.
> 
> rlnorm(1000,0,0)
> rlnorm(1000,1,1)
> rlnorm(1000,0,1)
> ....                     ?
> 
> Can you help me?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Roger.Bivand at nhh.no  Fri Jan  7 13:17:31 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 7 Jan 2005 13:17:31 +0100 (CET)
Subject: [R] hist function to give each cell equal area
In-Reply-To: <Pine.LNX.4.21.0501071109340.26888-100000@mail.mrc-dunn.cam.ac.
	uk>
Message-ID: <Pine.LNX.4.44.0501071316140.3327-100000@reclus.nhh.no>

On Fri, 7 Jan 2005, Dan Bolser wrote:

> On Fri, 7 Jan 2005, BXC (Bendix Carstensen) wrote:
> 
> >how about:
> >
> >x <- rnorm(400)
> >nbin<-7
> >hist(x,breaks=quantile(x,prob=seq(0,1,length=nbin+1)))
> 
> Thanks for the replies! Quantiles are my new friend. Its true the example
> I gave was a bit tricky, I just wanted to get the idea over.
> 
> One thing that is slightly confusing (probably a slight syntax thing),
> combining the above with the reply from Peter Dalgaard...
> 
> nbin<-10
> x <- rnorm(400)
> hist(x,breaks=quantile(x,prob=seq(0,1,length=nbin+1)))
> 
> y <- table(cut(x,breaks=quantile(x,seq(0,1,length=nbin+1))))
> 
> > y
> [1] 399
> 
> I only get 399 counts out, and I put 400 counts in?

> y <- table(cut(x,breaks=quantile(x,seq(0,1,length=nbin+1)), 
+    include.lowest = TRUE))

It's the include.lowest= argument to cut()

> 
> Cheers,
> Dan.
> 
> >
> >Bendix Carstensen
> >----------------------
> >Bendix Carstensen
> >Senior Statistician
> >Steno Diabetes Center
> >Niels Steensens Vej 2
> >DK-2820 Gentofte
> >Denmark
> >tel: +45 44 43 87 38
> >mob: +45 30 75 87 38
> >fax: +45 44 43 07 06
> >bxc at steno.dk
> >www.biostat.ku.dk/~bxc
> >----------------------
> >
> >
> >
> >> -----Original Message-----
> >> From: r-help-bounces at stat.math.ethz.ch 
> >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dan Bolser
> >> Sent: Friday, January 07, 2005 11:21 AM
> >> To: R mailing list
> >> Subject: [R] hist function to give each cell equal area
> >> 
> >> 
> >> 
> >> Hi,
> >> 
> >> I want to use hist with non-equi-spaced breaks, picked such 
> >> that the fraction of the data points falling in the cells 
> >> (defined by 'breaks') is 
> >> roughly equal accross all cells.
> >> 
> >> Is there such a function that will automatically try to 
> >> determine the breaks to fullfill this requirement?
> >> 
> >> Something like..
> >> 
> >> hist( x, br = magic_function_to_pick_breaks())
> >> 
> >> 
> >> For example, if x looked like this...
> >> 
> >> x <- c( 1:5, 10+1:5, 100+1:5 )
> >> 
> >> the breaks would define cells like this 
> >> 
> >> hist(x,breaks=c(0,5,15,105))
> >> 
> >> Is there such a function?
> >> 
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list 
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read 
> >> the posting guide! http://www.R-project.org/posting-guide.html
> >> 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From p.dalgaard at biostat.ku.dk  Fri Jan  7 13:22:06 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Jan 2005 13:22:06 +0100
Subject: [R] hist function to give each cell equal area
In-Reply-To: <Pine.LNX.4.21.0501071109340.26888-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0501071109340.26888-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <x2sm5dbf9t.fsf@biostat.ku.dk>

Dan Bolser <dmb at mrc-dunn.cam.ac.uk> writes:

> y <- table(cut(x,breaks=quantile(x,seq(0,1,length=nbin+1))))
> 
> > y
> [1] 399
> 
> I only get 399 counts out, and I put 400 counts in?

Look at the include.lowest argument. For some reason this is TRUE in
hist() but FALSE in cut().

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From giles.heywood at btinternet.com  Fri Jan  7 13:28:22 2005
From: giles.heywood at btinternet.com (Giles Heywood)
Date: Fri, 7 Jan 2005 12:28:22 -0000
Subject: [R] S4 class no longer accepts matrix in array slot under 2.0.1
Message-ID: <KLEHJMEACMGEMBOOPFCAGEIKCAAA.giles.heywood@btinternet.com>

I have an S4 class with a slot of class "array", and in upgrading to 2.0.1
(from 1.9.1) I have encountered a change in behaviour. This causes me some
difficulties if I want to allow 2-dimensional arrays in the slot.

The following (in 2.0.1) illustrates the point:

> setClass("foo",representation("array"))
[1] "foo"
> a <- new("foo",array(NA,2:4))
> b <- new("foo",matrix(NA,2,3))
Error in "as<-"(`*tmp*`, Classi, value = c(NA, NA, NA, NA, NA, NA)) :
        No method or default for as() replacement of "foo" with
Class="matrix"

This last error did not occur under 1.9.1.

I conclude that in this context the methods package does not recognise
"matrix" as a subclass of "array". However if I use getClass(), I see that R
recognises "matrix" as a subclass of "array" (and vice-versa).  So is this
new behaviour correct?

[this is a simplified (and final) reposting of an earlier question entitled
"matrix no longer "is" array in 2.0.1?"]



From talitaperciano at hotmail.com  Fri Jan  7 13:28:04 2005
From: talitaperciano at hotmail.com (Talita Leite)
Date: Fri, 07 Jan 2005 12:28:04 +0000
Subject: [R] Asymmetry and kurtosis coefficients
Message-ID: <BAY14-F8FCCFC9A8843FC8596506C7940@phx.gbl>

Hello everybody,

I'm studying descriptive statistics with R and I want to know how to 
calculate the asymmetry and kurtosis coefficients of a sample using R. I'll 
appreciate some help.

Thanx,


Talita Perciano Costa Leite
Graduanda em Ci?ncia da Computa??o
Universidade Federal de Alagoas - UFAL
Departamento de Tecnologia da Informa??o - TCI
Constru??o de Conhecimento por Agrupamento de Dados - CoCADa



From talitaperciano at hotmail.com  Fri Jan  7 13:28:43 2005
From: talitaperciano at hotmail.com (Talita Leite)
Date: Fri, 07 Jan 2005 12:28:43 +0000
Subject: [R] Asymmetry and kurtosis coefficients
Message-ID: <BAY14-F39EA3F8829AF4F5C4C0053C7940@phx.gbl>

Hello everybody,

I'm studying descriptive statistics with R and I want to know how to 
calculate the asymmetry and kurtosis coefficients of a sample using R. I 
would appreciate some help.

Thanx,


Talita Perciano Costa Leite
Graduanda em Ci?ncia da Computa??o
Universidade Federal de Alagoas - UFAL
Departamento de Tecnologia da Informa??o - TCI
Constru??o de Conhecimento por Agrupamento de Dados - CoCADa



From rdiaz at cnio.es  Fri Jan  7 13:32:17 2005
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Fri, 7 Jan 2005 13:32:17 +0100
Subject: =?UTF-8?Q?Re=3A_=5BR=5D_lme=3A_error_message_with_random=3D=7E1?=
In-Reply-To: <41DC07F7.6040900@hhbio.wasser.tu-dresden.de>
References: <41DBBD5D.4030808@hhbio.wasser.tu-dresden.de>
	<41DBE063.5080408@stat.wisc.edu>
	<41DC07F7.6040900@hhbio.wasser.tu-dresden.de>
Message-ID: <200501071332.17806.rdiaz@cnio.es>

On Wednesday 05 January 2005 16:29, Thomas Petzoldt wrote:
> Douglas Bates wrote:
> > I'm not sure what model you want to fit here.  To specify a random
> > effect in lme you need both a grouping factor and a model matrix.  The
> > error message indicates that lme is unable to determine a grouping
> > factor.  It would be correct syntax if you added a single level factor
> > to the data frame and used that but then the model fit would fail
> > because you would be trying to estimate a variance in a model where
> > there is no variation in the term.
>
> O.k. I see and think I understand it.
>
> > It seems to me that you are trying to estimate parameters in a
> > mixed-effects model without any random effects and lme can't do that.
>
> Yes, what I want is a model without any random effects to be tested
> against a model with random effects. I want to show, that the random
> effects are negligible but that we account for pseudo replicates and
> have tested this explicitely.

Dear Tomas,

What about fitting the models with and without random effects using the gls 
function (instead of lme ---you'll need to change a bit the syntax in the 
model with random effects in lm), and using a LR test? 

R.
>
> I'm not sure what is better: to leave the random effects in the model or
>   simply an LR test against a linear model fitted by lm. I've never seen
> such an example in the books. Or have I missed a global alternative here?
>
> Thomas P.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Ram?n D?az-Uriarte
Bioinformatics Unit
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)


Este correo electronico y, en su caso, cualquier fichero anexo al mismo, contiene informacion exclusivamente dirigida a su destinatario o destinatarios. Si Vd. ha recibido este mensaje por error, se ruega notificar esta circunstancia al remitente. Las ideas y opiniones manifestadas en este mensaje corresponden unicamente a su autor y no representan necesariamente a las del Centro Nacional de Investigaciones Oncologicas (CNIO).


The information contained in this message is intended for the addressee only. If you have received this message in error or there are any problems please notify the originator. Please note that the Spanish National Cancer Centre (CNIO), does not accept liability for any statements or opinions made which are clearly the sender's own and not expressly made on behalf of the Centre.



From Ted.Harding at nessie.mcc.ac.uk  Fri Jan  7 13:44:40 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 07 Jan 2005 12:44:40 -0000 (GMT)
Subject: [R] lognorm
In-Reply-To: <20050107111140.11394.qmail@web51806.mail.yahoo.com>
Message-ID: <XFMail.050107124440.Ted.Harding@nessie.mcc.ac.uk>

On 07-Jan-05 Frederic renaud wrote:
> Hi!
> I 've a problem to have a lognorm distribution with
> mean=1 and var (or sigma)=1.
> 
> rlnorm(1000,0,0)
> rlnorm(1000,1,1)
> rlnorm(1000,0,1)
> ....                     ?
> 
> Can you help me?

Not sure what your problem is.

For rlnorm(1000,0,0), you will get 100 values equal to
exp(X) where X has been sampled from N(0,0), i.e. since
the variance is 0 all X are equal to 0 and exp(X) = 1.
This is what rlnorm(1000,0,0) yields.

In the other two cases, I don't see anything wrong with
the results.

So what is the problem?

Note that rlnorm(N, meanlog=mu, sdlog=s) gives you (as
described above and in "?rlnorm") N random values exp(X)
where X is sampled from N(mu,s^2). mu and s are not the
mean and s.d. of the resulting log-normal variate exp(X).

In terms of the mean mu and s.d. s of the underlying Normal
distribution, the mean and variance of the log-normal
distribution of exp(X) are

  MU = exp(mu + (s^2)/2)

  V = S^2 = (MU^2)*(exp(s^2)-1)^2

so, if you really mean that your problem is how to generate
a log-normal sample with given mean MU and variance V = S^2,
then you have to solve these equations for mu and s.

In particular if (as in your second case) you want MU=1
and S=1, then exp(s^2) - 1 = 1 so

  s = sqrt(log(2)) = 0.8325546

and mu + (s^2)/2 = log(1) = 0 so

  mu = -(s^2)/2 = -0.3465736

With these values of mu and s,

  X <- rlnorm(1000,mu,s)

  mean(X)
  ## [1] 1.054104

  sd(X)
  ## [1] 0.9936088

(for this sample).

However, you're not going to be able to achieve your third
case (MU = 0, V = 1) since this would require mu = -infinity!
You can't make a log-normal random variable with mean 0.
(And in any case it would necessarily have V = 0, not V = 1,
since a log-normal variable cannot be negative, so zero mean
would imply no positive values and hence all values = 0,
hence zero variance).

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 07-Jan-05                                       Time: 12:44:40
------------------------------ XFMail ------------------------------



From friendly at yorku.ca  Fri Jan  7 14:13:28 2005
From: friendly at yorku.ca (Michael Friendly)
Date: Fri, 07 Jan 2005 08:13:28 -0500
Subject: [R] re: patterns of missing data: determining monotonicity
Message-ID: <41DE8AF8.8050801@yorku.ca>

[Sorry for the re-post; my examples got garbled in the original cut/paste.]

Here is a problem that perhaps someone out here has an idea about.  It 
vaguely reminds me of something
I've seen before, but can't place.  Can anyone help?

For multiple imputation, there are simpler methods available if  the 
patterns of missing data are 'monotone' ---
if Vj is missing then all variables Vk, k>j are also missing, vs. more 
complex methods required when the patterns are not monotone.  The 
problem is to determine if, for a collection of variables, there is an 
ordering of them with a monotone
missing data pattern, or, if not, what the longest monotone sequence is.

Here is an example, where in a dataset of 65 observations, there are 8 
different patterns of missingness,
with X and . representing observed and missing:

Group  V2  V3  V4  V5  V6  V7  V8  V9  V10  V11   nmiss

  1    x   x   x   x   x   x   x   x    x    x      0  
  2    x   x   x   x   x   x   .   x    x    x      1  
  3    x   x   x   x   x   .   x   x    x    x      1  
  4    x   x   x   x   x   .   .   x    x    x      2  
  5    x   x   .   x   .   x   x   x    x    x      2  
  6    x   x   .   .   x   x   x   x    x    x      2  
  7    x   x   .   .   x   .   x   x    x    x      3  
  8    x   x   .   .   .   x   x   x    x    x      3  

Treated as a binary matrix, one can sort the columns by the number
of non-missing for each variable, and monotone means that there
are at most 2 runs -- a string of 0s followed by all 1s for *all*
patterns. But how
to determine an ordering (or orderings) of variables of maximal length?

Group   V2  V3  V9 V10  V11  V6  V8  V5  V7  V4   nmiss

  1     0   0   0   0    0    0   0   0   0   0    0  
  2     0   0   0   0    0    0   1   0   0   0    1  
  3     0   0   0   0    0    0   0   0   1   0    1  
  4     0   0   0   0    0    0   1   0   1   0    2  
  5     0   0   0   0    0    1   0   0   0   1    2  
  6     0   0   0   0    0    0   0   1   0   1    2  
  7     0   0   0   0    0    0   0   1   1   1    3  
  8     0   0   0   0    0    1   0   1   0   1    3  
       ==  ==  ==  ===  ===  ==  ==  ==  ==  ==
        0   0   0   0    0    2   2   3   3   4        


-- 
Michael Friendly     Email: friendly at yorku.ca 
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From tobias.verbeke at telenet.be  Fri Jan  7 14:15:07 2005
From: tobias.verbeke at telenet.be (Tobias Verbeke)
Date: Fri, 7 Jan 2005 13:15:07 +0000
Subject: [R] Asymmetry and kurtosis coefficients
In-Reply-To: <BAY14-F8FCCFC9A8843FC8596506C7940@phx.gbl>
References: <BAY14-F8FCCFC9A8843FC8596506C7940@phx.gbl>
Message-ID: <20050107131507.7e6bb22f.tobias.verbeke@telenet.be>

On Fri, 07 Jan 2005 12:28:04 +0000
"Talita Leite" <talitaperciano at hotmail.com> wrote:

> Hello everybody,
> 
> I'm studying descriptive statistics with R and I want to know how to 
> calculate the asymmetry and kurtosis coefficients of a sample using R. I'll 
> appreciate some help.

install.packages("e1071")
library(e1071)

?kurtosis
?skewness

HTH,
Tobias
 
> Thanx,
> 
> 
> Talita Perciano Costa Leite
> Graduanda em Ci?ncia da Computa??o
> Universidade Federal de Alagoas - UFAL
> Departamento de Tecnologia da Informa??o - TCI
> Constru??o de Conhecimento por Agrupamento de Dados - CoCADa
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ramagopa at etek.chalmers.se  Fri Jan  7 14:23:42 2005
From: ramagopa at etek.chalmers.se (Sivakumar Ramagopal)
Date: Fri, 7 Jan 2005 14:23:42 +0100 (CET)
Subject: [R] configure error for R-2.0.1
Message-ID: <1836.129.16.30.223.1105104222.squirrel@webmail.chalmers.se>

Hi List,

I'm trying to build R-2.0.1 on and AMD64/Linux (Suse 9.1) and I get the
following error while running the configure script.

...
checking for dummy main to link with Fortran libraries... none
checking for Fortran name-mangling scheme... lower case, underscore, extra
underscore
checking whether g77 appens underscores to external names... yes
checking whether mixed C/Fortran code can be run... configure: WARNING:
cannot run mixed C/Fortran code
configure: error: Maybe check LDFLAGS for paths to Fortran libraries?

I have gcc-3.3.3.

Regards,
Shiva



From sam.kemp2 at ntlworld.com  Fri Jan  7 14:45:23 2005
From: sam.kemp2 at ntlworld.com (Samuel Kemp)
Date: Fri, 07 Jan 2005 13:45:23 +0000
Subject: [R] R packages on Mac
Message-ID: <41DE9273.5030904@ntlworld.com>

Hi,

I am considering whether or not to buy an apple mac. I have noticed on 
one of the R FAQs for Mac OS X that you cannot install packages from 
other OS's if C++ code is contained. My question is: Is it possible to 
build the package sources containing C++ code on the Mac and then 
install them?

Thanks in advance,

Sam.



From wolski at molgen.mpg.de  Fri Jan  7 14:46:49 2005
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Fri, 07 Jan 2005 13:46:49 +0000
Subject: [R] S4 class no longer accepts matrix in array slot under 2.0.1
In-Reply-To: <KLEHJMEACMGEMBOOPFCAGEIKCAAA.giles.heywood@btinternet.com>
References: <KLEHJMEACMGEMBOOPFCAGEIKCAAA.giles.heywood@btinternet.com>
Message-ID: <41DE92C9.6010001@molgen.mpg.de>

Giles Heywood wrote:

>I have an S4 class with a slot of class "array", and in upgrading to 2.0.1
>(from 1.9.1) I have encountered a change in behaviour. This causes me some
>difficulties if I want to allow 2-dimensional arrays in the slot.
>
>The following (in 2.0.1) illustrates the point:
>
>  
>
>>setClass("foo",representation("array"))
>>    
>>
>[1] "foo"
>  
>
>>a <- new("foo",array(NA,2:4))
>>b <- new("foo",matrix(NA,2,3))
>>    
>>
>Error in "as<-"(`*tmp*`, Classi, value = c(NA, NA, NA, NA, NA, NA)) :
>        No method or default for as() replacement of "foo" with
>Class="matrix"
>
>This last error did not occur under 1.9.1.
>
>I conclude that in this context the methods package does not recognise
>"matrix" as a subclass of "array". However if I use getClass(), I see that R
>recognises "matrix" as a subclass of "array" (and vice-versa).  So is this
>new behaviour correct?
>
>[this is a simplified (and final) reposting of an earlier question entitled
>"matrix no longer "is" array in 2.0.1?"]
>
>  
>
Hi,

 > setClass("foo",representation("matrix"))
[1] "foo"
 > a <- new("foo",array(NA,2:4))
 > b <- new("foo",matrix(NA,2,3))
 >
Will work.
As you say array has a subclass matrix.

getClass("array")
....
Known Subclasses: "matrix"

and

getClass("matrix")
Known Subclasses:
Class "array", directly, with explicit test and coerce

and matrix has a subclass array.

What probably is necessary to use polymorphism -> assigning a subclass 
to the parent class is the "explicit test and coearce".
It seems that matrix is the superclass of array.

/E

>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>




-- 
Dipl. bio-chem. Witold Eryk Wolski
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin
tel: 0049-30-83875219                 __("<    _
http://www.molgen.mpg.de/~wolski      \__/    'v'
http://r4proteomics.sourceforge.net    ||    /   \
mail: witek96 at users.sourceforge.net    ^^     m m
      wolski at molgen.mpg.de



From anne.piotet at urbanet.ch  Fri Jan  7 14:53:05 2005
From: anne.piotet at urbanet.ch (Anne)
Date: Fri, 7 Jan 2005 14:53:05 +0100
Subject: [R] help with polytomous logistic regression
Message-ID: <009001c4f4c0$37e9bda0$6c00a8c0@mtd4>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050107/ba817db0/attachment.pl

From HDoran at air.org  Fri Jan  7 15:05:21 2005
From: HDoran at air.org (Doran, Harold)
Date: Fri, 7 Jan 2005 09:05:21 -0500
Subject: [R] lapply and gls
Message-ID: <88EAF3512A55DF46B06B1954AEF73F74072E9DBA@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050107/ed793970/attachment.pl

From Lorenz.Gygax at fat.admin.ch  Fri Jan  7 15:05:58 2005
From: Lorenz.Gygax at fat.admin.ch (Lorenz.Gygax@fat.admin.ch)
Date: Fri, 7 Jan 2005 15:05:58 +0100 
Subject: [R] GLMM and crossed effects
Message-ID: <BF74FADD4B44554CA7E53D0B5242CD6A01FC63E2@evd-s7014.evd.admin.ch>

> I am analysing data with a dependent variable of insect 
> counts, a fixed effect of site and two random effects, day,
> which is the same set of 10 days for each site, and then
> transect, which is nested within site (5 each).

And what exactly are you interested in? Just the differences between the two
sites? Then you would have sampled on several days just to better estimate
what is going on at those sites? Or are you also interested in a time
course?

> I am trying to fit the cross classified model using GLMM in lme4.

I have yet to get around to working with lme4 (by the way is there some
documentation that describes the changes and advantages of lme4 in
comparison to nlme?), but assuming that the syntax is the same as in lme ()
or glmmPQL ():

Why don't you use:

GLMM (count ~ site, data= dat3,
      random= ~ 1 | site/trans, family= poisson)

This compares your counts between sites considering that transects are
nested in sites and that there are several measurements on each transect
(the days). They are the repeated measurements on the lowest level (within
trans), so you do not need to specify them explicitly.

Acutally, you might not even need the site/ in your random term as this
variable ist constant for each transect and thus the degrees of freedom are
adjusted for the fixed effect of site (but then each of the transect need
its own code).

Well on further thought, you might necessarily need to leave this part of
the random term away if you want to conduct any statistical test, because
otherwise you only have an N= 1 for each site ...

thus, I guess you need to assume that your transects are independent
measures of your two sites (which means that you can conduct some
statistical analysis, but you results hold only for these two specific sites
and can not necessarily be generalised to other sites). Thus the model would
be:

GLMM (count ~ site, data= dat3,
      random= ~ 1 | trans2, family= poisson)

or if you are interested in a time course you might try (and this
explicitely models that these are the same days):

GLMM (count ~ site*day, data= dat3,
      random= ~ 1 | trans2, family= poisson)

I would argue, that you are either not interested in the days, then these
are just your repeated measurements and it does not matter that they were
exactly on the same days for the different transects (and then they are just
implicitly nested in trans2) OR you are interested in them and then I would
include them as a fixed effect, which is crossed with transect, i.e. all
transects were sampled on all days.

By the way, it is not clear to me from your description how trans2, ts and
ts2 differ logically.

> there are als ts1 and ts2 dummy variables, as was necessary
> in the old lme.....

what are these necessary for? (But I have to admit that I usually feed a
standard data.frame to lme and glmmPQL)

> GLMM(count~site,data=dat3,random=list(day=~1,trans=~1|site,
> family=poisson)

I do not know whether you can write such a thing at all. If this has not
changed a lot from nlme to lme4 you would need to write:

random= list (~1 | day, ~ 1 | site/trans)

but that you would implicitly define that site is nested in day, i.e. it
would be the same as writing

random= ~ 1 | day/site/trans

which is not what you want.

> #This does... but also note the differences in the summary 
> and VarCorr variance components...

Here, you loose me completely. It is not clear to me what you compare and
where you perceive a problem.

Cheers, Lorenz
- 
Lorenz Gygax, Dr. sc. nat.
Centre for proper housing of ruminants and pigs
Swiss Federal Veterinary Office
agroscope FAT T?nikon, CH-8356 Ettenhausen / Switzerland



From i.visser at uva.nl  Fri Jan  7 15:11:24 2005
From: i.visser at uva.nl (Ingmar Visser)
Date: Fri, 07 Jan 2005 15:11:24 +0100
Subject: [R] R packages on Mac
In-Reply-To: <41DE9273.5030904@ntlworld.com>
Message-ID: <BE04571C.D359%i.visser@uva.nl>

hi Sam,

Building source packages with C++ source code is no problem at all on
Mac OS X if you simply follow the instructions in the FAQ.

best and good luck with your new Mac,
from a content MAC user, best, ingmar

On 1/7/05 2:45 PM, "Samuel Kemp" <sam.kemp2 at ntlworld.com> wrote:

> Hi,
> 
> I am considering whether or not to buy an apple mac. I have noticed on
> one of the R FAQs for Mac OS X that you cannot install packages from
> other OS's if C++ code is contained. My question is: Is it possible to
> build the package sources containing C++ code on the Mac and then
> install them?
> 
> Thanks in advance,
> 
> Sam.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From anne.piotet at urbanet.ch  Fri Jan  7 15:16:50 2005
From: anne.piotet at urbanet.ch (Anne)
Date: Fri, 7 Jan 2005 15:16:50 +0100
Subject: [R] Visualizing complex analytic functions using domain coloring
References: <92C4B90A-6096-11D9-85C9-000A95D86AA8@soc.soton.ac.uk>
	<005c01c4f4a8$f4e5a3f0$6c00a8c0@mtd4>
	<54764D0A-60AA-11D9-85C9-000A95D86AA8@soc.soton.ac.uk>
Message-ID: <00ba01c4f4c3$8923c780$6c00a8c0@mtd4>

Hi Robin


here's the C++ code (scale already coded)

!  it was done under C++ builder and I seem to remember that RGB is in fact
coded as BGR (I've done that a year ago and already forgotten all about it,
roll the years!)

the scale was generated from another C++ program (which I do not find for
the moment...sorry! when I find it again I'll send it to you) and seemed to
work to the satisfaction of the clients

Anne




----- Original Message ----- 
From: "Robin Hankin" <r.hankin at soc.soton.ac.uk>
To: "Anne" <anne.piotet at urbanet.ch>
Cc: "Robin Hankin" <r.hankin at soc.soton.ac.uk>
Sent: Friday, January 07, 2005 1:47 PM
Subject: Re: [R] Visualizing complex analytic functions using domain
coloring


> Hello Anne
>
> yes, I'd be interested to see your code!
>
> best wishes
>
> Robin
>
> On Jan 7, 2005, at 11:06 am, Anne wrote:
>
> > Hi!
> > do you want to use a color scale such as in FE modelling?
> > I did once code such a scale (rgb) (hues from blue to red, throuh green
> > yellow and orange)
> > This is of course not a R code but in C++  ; however if you are
> > interested I
> > can send the code to you
> >
> > Anne
> >
> >
> > ----- Original Message -----
> > From: "Robin Hankin" <r.hankin at soc.soton.ac.uk>
> > To: <R-help at stat.math.ethz.ch>
> > Sent: Friday, January 07, 2005 11:26 AM
> > Subject: [R] Visualizing complex analytic functions using domain
> > coloring
> >
> >
> >> Hi
> >>
> >> has anyone coded up domain colouring for visualizing complex analytic
> >> functions
> >> (such as elliptic functions)?
> >>
> >> [
> >> the idea is to depict a complex function f(z) using a filled.contour()
> >> variant
> >> in which the hue is given by Arg(f(z)), and the saturation by
> >> Mod(f(z)).
> >> ]
> >>
> >>
> >> --
> >> Robin Hankin
> >> Uncertainty Analyst
> >> Southampton Oceanography Centre
> >> European Way, Southampton SO14 3ZH, UK
> >>   tel  023-8059-7743
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >>
> >
> >
> --
> Robin Hankin
> Uncertainty Analyst
> Southampton Oceanography Centre
> European Way, Southampton SO14 3ZH, UK
>   tel  023-8059-7743
>

From ripley at stats.ox.ac.uk  Fri Jan  7 15:18:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 7 Jan 2005 14:18:05 +0000 (GMT)
Subject: [R] configure error for R-2.0.1
In-Reply-To: <1836.129.16.30.223.1105104222.squirrel@webmail.chalmers.se>
References: <1836.129.16.30.223.1105104222.squirrel@webmail.chalmers.se>
Message-ID: <Pine.LNX.4.61.0501071410470.21676@gannet.stats>

On Fri, 7 Jan 2005, Sivakumar Ramagopal wrote:

> I'm trying to build R-2.0.1 on and AMD64/Linux (Suse 9.1) and I get the
> following error while running the configure script.
>
> ...
> checking for dummy main to link with Fortran libraries... none
> checking for Fortran name-mangling scheme... lower case, underscore, extra
> underscore
> checking whether g77 appens underscores to external names... yes
> checking whether mixed C/Fortran code can be run... configure: WARNING:
> cannot run mixed C/Fortran code
> configure: error: Maybe check LDFLAGS for paths to Fortran libraries?
>
> I have gcc-3.3.3.

Have you done as it suggests?  Please see the R posting guide and make 
sure you give us enough information to help you.  One place to look is in 
config.log.

R 2.0.1 does build on AMD64 with gcc 3.3.3, but SuSE does seem to use 
non-standard paths (that is, not those from the gcc sources).  In 
particular, is this a 32- or 64-bit build and if the latter is /lib64 in 
the library path?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Fri Jan  7 15:13:20 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 07 Jan 2005 14:13:20 -0000 (GMT)
Subject: [R] patterns of missing data: determining monotonicity
In-Reply-To: <41DD6C38.4080002@yorku.ca>
Message-ID: <XFMail.050107141320.Ted.Harding@nessie.mcc.ac.uk>

On 06-Jan-05 Michael Friendly wrote:
> Here is a problem that perhaps someone out here has an idea
> about. It vaguely reminds me of something I've seen before,
> but can't place.  Can anyone help?
> 
> For multiple imputation, there are simpler methods available
> if  the patterns of missing data are 'monotone' --- if Vj is
> missing then all variables Vk, k>j are also missing, vs. more 
> complex methods required when the patterns are not monotone.
> The problem is to determine if, for a collection of variables,
> there is an ordering of them with a monotone missing data pattern,
> or, if not, what the longest monotone sequence is.
> 
> Here is an example, where in a dataset of 65 observations, there
> are 8 different patterns of missingness, with X and . representing
> observed and missing:
> 
> Group   V2   V3   V4   V5   V6   V7   V8   V9   V10   V11   nmiss
>   1     X    X    X    X    X    X    X    X     X     X      0  
>   2     X    X    X    X    X    X    .    X     X     X      1  
>   3     X    X    X    X    X    .    X    X     X     X      1  
>   4     X    X    X    X    X    .    .    X     X     X      2  
>   5     X    X    .    X    .    X    X    X     X     X      2  
>   6     X    X    .    .    X    X    X    X     X     X      2  
>   7     X    X    .    .    X    .    X    X     X     X      3  
>   8     X    X    .    .    .    X    X    X     X     X      3  
> 
> Treated as a binary matrix, one can sort the columns by the number
> of non-missing for each variable, and monotone means that there
> are at most 2 runs -- a string of 0s followed by all 1s for *all*
> patterns. But how
> to determine an ordering (or orderings) of variables of maximal length?
> 
> Group   V2   V3   V9   V10   V11   V6   V8   V5   V7   V4   nmiss
>   1      0    0    0    0     0     0    0    0    0    0     0  
>   2      0    0    0    0     0     0    1    0    0    0     1  
>   3      0    0    0    0     0     0    0    0    1    0     1  
>   4      0    0    0    0     0     0    1    0    1    0     2  
>   5      0    0    0    0     0     1    0    0    0    1     2  
>   6      0    0    0    0     0     0    0    1    0    1     2  
>   7      0    0    0    0     0     0    0    1    1    1     3  
>   8      0    0    0    0     0     1    0    1    0    1     3  
>         ==   ==   ==   ===   ===   ==   ==   ==   ==   ==
>          0    0    0    0     0     2    2    3    3    4        

Hi Michael,

Consider the following approach. It's not a full solution to
the specific problem you have posed above, but it contains
pathways to solutions.

If you're doing multiple imputation anyway, you should install
the packages "cat" (for categorical data), "norm" (for continuous
data, assumed Normal) and "mix" (for data mixing both kinds),
and also "pan" for MI on "panel" data, which might also be useful
to you.

I'll discuss the situation using "cat" as an example, though
"norm" works the same way as far as this question is concerned.

First make sure your data are arranged as a matrix X (say)
with rows representing "cases" and columns variables. If the
variables are categorical, make sure that their values are
represented as integers 1, 2, 3, ... (don't start with "0"),
and represent missing values as NA.

Example of data matrix X:

  X
        [,1] [,2] [,3]
   [1,]    3    1    2
   [2,]    2    1    3
   [3,]    2    1   NA
   [4,]    2    3   NA
   [5,]    1    3   NA
   [6,]    2   NA   NA
   [7,]    2   NA   NA
   [8,]    3   NA   NA
   [9,]   NA   NA   NA
  [10,]   NA   NA   NA

(constructed to have monotone pattern). Now shuffle it:

  X<-X[,sample(1:3)]
  X<-X[sample(1:10),]

  X
        [,1] [,2] [,3]
   [1,]    1    2    3
   [2,]    3   NA    2
   [3,]    1   NA    2
   [4,]    3   NA    1
   [5,]    1    3    2
   [6,]   NA   NA   NA
   [7,]   NA   NA    2
   [8,]   NA   NA    3
   [9,]   NA   NA    2
  [10,]   NA   NA   NA

Consider this as a real data matrix where now it is not
obvious that it has monotone missingness pattern. Then:

  library(cat)
  s <- prelim.cat(X)

Now read *very*carefully"

  ?prelim.cat

and in particular what is said about its value (the value of s).
Note also what is *not* said about it!

Now look at "s" by printing it to the console. Amongst its 17
components the following are of particular interest.

  s$x
        [,1] [,2] [,3]
   [1,]    1    3    2
   [2,]    1    2    3
   [3,]    3   NA    1
   [4,]    1   NA    2
   [5,]    3   NA    2
   [6,]   NA   NA    2
   [7,]   NA   NA    2
   [8,]   NA   NA    3
   [9,]   NA   NA   NA
  [10,]   NA   NA   NA

You can see that this is the same as X except that rows have
been permuted to push the NAs downwards. The component

   s$ro
   [1]  2  5  4  3  1  9  6  8  7 10

shows the permutation: the original Row 1 of X is Row 2 of s$x,
the original row 2 of X is Row 5 of s$x, and so on.

Now look at the component s$nmis of s:

   s$nmis
   [1] 5 8 2

This gives the numbers of missing values in the different
columns of X (and of s$x since the order of columns has not
been changed).

Now you can sort s$nmis into decreasing order using the
"index.return=TRUE" option of 'sort' so as to get the
column permutation:

  sort(s$nmis,index.return=TRUE)
  $x
  [1] 2 5 8

  $ix
  [1] 3 1 2

You can check directly that s$x[,c(3,1,2)] is in monotone
pattern; more directly, you can get X re-structured into
monotone pattern as

  s$x[,sort(s$nmis,index.return=TRUE)$ix]
        [,1] [,2] [,3]
   [1,]    2    1    3
   [2,]    3    1    2
   [3,]    1    3   NA
   [4,]    2    1   NA
   [5,]    2    3   NA
   [6,]    2   NA   NA
   [7,]    2   NA   NA
   [8,]    3   NA   NA
   [9,]   NA   NA   NA
  [10,]   NA   NA   NA

I hope this is some help. At least it shows you places where
you can start digging. If the original X is incompatible with
monotone pattern, then the above should give you something
which is close to monotone, though I'm not sure whether it
will get you "as close as possible"; and you may need to
do some more work to uncover how to determine your "longest
monotone sequence".

In any case, since these MI packages (all based on Shafer's
original S code) work internally with monotonicity in mind,
for reasons of efficiency and fast convergence, you may find
that your imputation needs are met by them.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 07-Jan-05                                       Time: 14:13:20
------------------------------ XFMail ------------------------------



From andikumagenge at businessdecision.com  Fri Jan  7 15:25:49 2005
From: andikumagenge at businessdecision.com (NDIKUMAGENGE Alice)
Date: Fri, 7 Jan 2005 15:25:49 +0100
Subject: =?utf-8?Q?RE=C2=A0=3A_=5BR=5D_Help_for_=22calibration=22?=
Message-ID: <6CC4DC1EC1F92D4B8A5FF590362E756502FA41A8@neptune.betd.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050107/cd7bc992/attachment.pl

From Tristan.Lefebure at univ-lyon1.fr  Fri Jan  7 15:23:12 2005
From: Tristan.Lefebure at univ-lyon1.fr (Lefebure Tristan)
Date: Fri, 7 Jan 2005 15:23:12 +0100
Subject: [R] RSvgDevice incomplete svg output
In-Reply-To: <200501061537.10335.lefebure@univ-lyon1.fr>
References: <200501061537.10335.lefebure@univ-lyon1.fr>
Message-ID: <200501071523.12434.lefebure@univ-lyon1.fr>

Hi
The problem is clearly associated to R.2.0:
Same command on same data with same package (RSvgDevice 0.5.3) but with R1.9.1 
or R2.0.0 gave me different results: svg files are correct with R1.9.1, not 
with R.2.0.0.
(email forward to the package developper)

On Thursday 06 January 2005 15:37, Lefebure Tristan wrote:
> Hi
> I use RSvgDevice to output plot, and modify them using svg editor (inkscape
> or sodipodi on Linux). Some month ago, results were perfect. I did exactly
> the same analysis today on the same data, and unfortunatly the results are
> different. While looking to the svg file, it seems that all information
> concerning "fill" and "stroke" of objects are lost. The consequence is that
> all objects become invisible ..........
>
> exemple:
> Correct old file:
> .............
> <polygon points="93.35 , 119.69 109.59 , 119.69 109.59 , 110.94 93.35 ,
> 110.94 " style="stroke-width:1;stroke:#000000;fill:#7F7F7F" />
> ...............
>
> Today bad file:
> ..........................
> <polygon points="174.51 , 53.95 190.74 , 53.95 190.74 , 53.95 174.51 ,
> 53.95 " style="stroke-width:1;stroke:#FFFFFF;fill:none;stroke-dasharray" />
> .................
>
> A problem associated to R2.0 ?
> Thanks
>
>
> package `RSvgDevice' version 0.5.3
>
> $platform
> [1] "i586-mandrake-linux-gnu"
>
> $arch
> [1] "i586"
>
> $os
> [1] "linux-gnu"
>
> $system
> [1] "i586, linux-gnu"
>
> $status
> [1] ""
>
> $major
> [1] "2"
>
> $minor
> [1] "0.0"
>
> $year
> [1] "2004"
>
> $month
> [1] "10"
>
> $day
> [1] "04"
>
> $language
> [1] "R"

-- 
------------------------------------------------------------
Tristan LEFEBURE
Laboratoire d'?cologie des hydrosyst?mes fluviaux (UMR 5023)
Universit? Lyon I - Campus de la Doua
Bat. Darwin C 69622 Villeurbanne - France

Phone: (33) (0)4 26 23 44 02
Fax: (33) (0)4 72 43 15 23



From jfox at mcmaster.ca  Fri Jan  7 15:38:30 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 7 Jan 2005 09:38:30 -0500
Subject: [R] Basic Linear Algebra
In-Reply-To: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA5C@afhex01.dpi.wa.gov.au>
Message-ID: <20050107143829.BICI19622.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Tom,

I'm not sure that I follow the question correctly, but I think that you want
to solve the over determined system tdata %*% x == sumtd.

Some time ago, I believe that I posted to the list a function for Gaussian
elimination that I use in teaching basic linear algebra. Here's an updated
version of that function, and its application to your problem:

GaussianElimination <- function(A, b, tol=sqrt(.Machine$double.eps),
verbose=FALSE, fractions=FALSE){
    # A: coefficient matrix
    # b: right-hand size vector
    # tol: tolerance for checking for 0 pivot
    # verbose: if TRUE, print intermediate steps
    # fractions: try to express nonintegers as rational numbers
    # If b is absent returns the reduced row-echelon form of A.
    # If b is present, reduces A to RREF carrying b along.
    if (fractions) {
        mass <- require(MASS)
        if (!mass) stop("fractions=TRUE needs MASS package")
        }
    if ((!is.matrix(A)) || (!is.numeric(A))) stop("argument must be a
numeric matrix")
    if ((!missing(b)) && ((!(length(b) == nrow(A))) || (!is.numeric(b))))
        stop("argument must be a numeric vector of length equal to the
number of columns in A")
    n <- nrow(A)
    m <- ncol(A)
    if (!missing(b)) A <- cbind(A, b)
    for (i in 1:min(c(m, n))){
        currentColumn <- A[,i]
        currentColumn[1:n < i] <- 0
        which <- which.max(abs(currentColumn))  # find maximum pivot in
current column at or below current row
        pivot <- A[which, i]
        if (abs(pivot) <= tol) next     # check for 0 pivot
        if (which > i) A[c(i, which),] <- A[c(which, i),]  # exchange rows
        A[i,] <- A[i,]/pivot            # pivot
        row <- A[i,]                    
        A <- A - outer(A[,i], row)      # sweep
        A[i,] <- row                    # restore current row
        if (verbose) if (fractions) print(fractions(A)) else print(round(A,
round(abs(log(tol,10)))))
        }
    for (i in 1:n) if (max(abs(A[i,1:m])) <= tol) A[c(i,n),] <- A[c(n,i),]
# 0 rows to bottom
    if (fractions) fractions (A) else round(A, round(abs(log(tol,10))))
    }
        
> GaussianElimination(tdata, sumtd)
                 b
[1,] 1 0 0 0 -3 30
[2,] 0 1 0 0 -1 12
[3,] 0 0 1 0 -2 20
[4,] 0 0 0 1  0 12

A caveat, however: Unlike some other members of this list, I'm not an expert
in numerical linear algebra and therefore can't vouch for the quality of the
algorithm -- it's meant basically for demonstration.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mulholland, Tom
> Sent: Friday, January 07, 2005 2:00 AM
> To: R-Help (E-mail)
> Subject: [R] Basic Linear Algebra
> 
> I don't normally have to go anywhere near this stuff , but it 
> seems to me that this should be a straight-forward process in R.
> 
> For the purposes of this enquiry I thought I would use 
> something I can work out on my own. 
> 
> So I have my matrix and the right hand results from that matrix
> 
> tdata <- 
> matrix(c(0,1,0,-1,-1,2,0,0,-5,-6,0,0,3,-5,-6,1,-1,-1,0,0),byro
> w = T,ncol = 5) sumtd <- c(0,0,0,-2)
> 
> > tdata
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    0    1    0   -1   -1
> [2,]    2    0    0   -5   -6
> [3,]    0    0    3   -5   -6
> [4,]    1   -1   -1    0    0
> > sumtd
> [1]  0  0  0 -2
> > 
> 
> which I can calculate to give me 3x+30, x+12, 2x+20,  12x, x
> 
> Would someone be kind enough to point me in the right 
> direction as to which tools I should be using and any sage 
> words of advice for the barely informed.
> 
> Tom Mulholland
> 
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.0            
> year     2004           
> month    10             
> day      04             
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From gudrunj at math.su.se  Fri Jan  7 15:40:00 2005
From: gudrunj at math.su.se (Gudrun Jonasdottir)
Date: Fri, 7 Jan 2005 15:40:00 +0100 (CET)
Subject: [R] glm fit with no intercept
Message-ID: <1196.193.10.23.31.1105108800.squirrel@webmail.math.su.se>

Dear R-help list members,

I am currently trying to fit a generalized linear model using a binomial
with the canonical link. The usual solution is to use the R function glm()
in the package "stats". However, I run into problem when I want to fit a
glm without an intercept. It is indicated that the solution is in changing
the function glm.fit (also in "stats"), by specifying intercept=FALSE. I
have not been successful in getting any output though.

Any suggestion on how to fit a glm with no intercept?

Regards,
Gudrun

-- 
Gudrun Jonasdottir, M.Sc.
Matematiska institutionen
Stockholms Universitet
SE- 106 91 Stockholm

Work: +46 (0)8 16 45 56
Mobile: +46 (0)709 779 800
--------------------------------------------
"N?r inget annat hj?lper, l?s instruktionsboken."
- Canns axiom



From dlvanbrunt at gmail.com  Fri Jan  7 15:46:33 2005
From: dlvanbrunt at gmail.com (David L. Van Brunt, Ph.D.)
Date: Fri, 7 Jan 2005 09:46:33 -0500
Subject: [R] R packages on Mac
In-Reply-To: <BE04571C.D359%i.visser@uva.nl>
References: <41DE9273.5030904@ntlworld.com> <BE04571C.D359%i.visser@uva.nl>
Message-ID: <d332d3e1050107064630bc79a6@mail.gmail.com>

I use R on Mac OS X and always install packages from Source. Never had
a problem with that. Not sure if it matters, but I also have Fink
installed as well as the Developers package that comes free with OS X,
so all the various compilers are there.

It's not diffiicult at all, and the OS X interface for R is really
very, very nice... much like in SAS or SPSS, you can now select a
piece of code in the editor, and send it right into R for execution.


On Fri, 07 Jan 2005 15:11:24 +0100, Ingmar Visser <i.visser at uva.nl> wrote:
> hi Sam,
> 
> Building source packages with C++ source code is no problem at all on
> Mac OS X if you simply follow the instructions in the FAQ.
> 
> best and good luck with your new Mac,
> from a content MAC user, best, ingmar
> 
> On 1/7/05 2:45 PM, "Samuel Kemp" <sam.kemp2 at ntlworld.com> wrote:
> 
> > Hi,
> >
> > I am considering whether or not to buy an apple mac. I have noticed on
> > one of the R FAQs for Mac OS X that you cannot install packages from
> > other OS's if C++ code is contained. My question is: Is it possible to
> > build the package sources containing C++ code on the Mac and then
> > install them?
> >
> > Thanks in advance,
> >
> > Sam.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
---------------------------------------
David L. Van Brunt, Ph.D.
mailto:dlvanbrunt at gmail.com



From Christoph.Scherber at uni-jena.de  Fri Jan  7 15:47:04 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Fri, 07 Jan 2005 15:47:04 +0100
Subject: [R] coercing columns
Message-ID: <41DEA0E8.1000202@uni-jena.de>

Dear all,

I have a data frame that looks like this:

c1   c2   c3
A    B    C
B    C    A
A    A    B

and so on;

I?d like to produce one single vector consisting of the columns c1,c2, 
c3, such that

vector=("A","B","A","B","C","A","C","A","B")

I guess it?s easy to do but I don?t know how...Can anyone help me?

Thanks a lot!
Christoph



From jfox at mcmaster.ca  Fri Jan  7 15:56:55 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 7 Jan 2005 09:56:55 -0500
Subject: [R] help with polytomous logistic regression
In-Reply-To: <009001c4f4c0$37e9bda0$6c00a8c0@mtd4>
Message-ID: <20050107145654.BNAI19622.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Anne,

There's an anova() method for multinom objects that will allow you to get
likelihood-ratio tests. Likewise, the Anova() function in the car package
has a method for multinom objects.

As to plotting, I have a paper, coauthored with Bob Andersen, on producing
"effect" plots (partial plots) for polytomous logistic regression, including
multinomial and proportional-odds logistic regression. The paper, at
<http://socserv.socsci.mcmaster.ca/jfox/logit-effect-displays.pdf>, has an R
function for computing such effects, including standard errors. There are
examples in the paper as well. Eventually, I intend to incorporate this into
the effects package to make the graphs more or less automatic.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anne
> Sent: Friday, January 07, 2005 8:53 AM
> To: R list
> Subject: [R] help with polytomous logistic regression
> 
> Hi!
> 
> I'm trying to do some ploytomous logistic regression using 
> multinom() in the nnet package, but am a bit confused about 
> interpretation of the results Is it possible to get the 
> following quantities: 
> 
> I:  maximum likelihood estimates to test for fit of model and 
> significance of each predictor
> 
> (I would like to produce a table of the following type)
> Analysis of Variance: MLE    (values are non sensical, I know!)
>                df      chi2           p
> intercept   2       57            .003
> v1            4       89            .876
> v2            2         7            .05
> LR        110     450            0.93
> 
> II:  chi square values and corresponding p-values for each 
> level of the predictor variates (I'm OK for the estimates and 
> se from summary,)
> 
>  
>                level estimate     se        chi2         p   
> intercept   1        5              .9         32          .002
>                 2       6.3            .8         31          .03
> v1             3       89            .876       43          .001
>                 4       65            .05         67          
> 0.001      
>  v2...
> 
> 
> Is there a convenient way to plot the results? (I'd like to 
> display visually the effects of the predictors: any sugestion?)
> 
> OK, I probably miss something here (no experience with (non 
> ordinal) polytomous logistic regression yet!)
> 
> Thank  very much to all
> 
> Anne
> 
> ----------------------------------------------------
> Anne Piotet
> Tel: +41 79 359 83 32 (mobile)
> Email: anne.piotet at m-td.com
> ---------------------------------------------------
> M-TD Modelling and Technology Development PSE-C
> CH-1015 Lausanne
> Switzerland
> Tel: +41 21 693 83 98
> Fax: +41 21 646 41 33
> --------------------------------------------------
>  
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Fri Jan  7 15:58:27 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Jan 2005 15:58:27 +0100
Subject: [R] re: patterns of missing data: determining monotonicity
In-Reply-To: <41DE8AF8.8050801@yorku.ca>
References: <41DE8AF8.8050801@yorku.ca>
Message-ID: <x27jmpb818.fsf@biostat.ku.dk>

Michael Friendly <friendly at yorku.ca> writes:

> [Sorry for the re-post; my examples got garbled in the original cut/paste.]

I don't know about then but they sure are now!
 
> Here is a problem that perhaps someone out here has an idea about.  It
> vaguely reminds me of something
> I've seen before, but can't place.  Can anyone help?
> 
> For multiple imputation, there are simpler methods available if  the
> patterns of missing data are 'monotone' ---
> if Vj is missing then all variables Vk, k>j are also missing, vs. more
> complex methods required when the patterns are not monotone.  The
> problem is to determine if, for a collection of variables, there is an
> ordering of them with a monotone
> missing data pattern, or, if not, what the longest monotone sequence
> is.

Here's my take - no idea about implementation though.

You need to draw a directed graph.  Nodes are 1...n and the rule is
that if Vk is missing in a pattern, you draw an arrow from each j for
which Vj is nonmissing, to k. If this graph has no cycles, the
collection of patterns is monotone and there is a straightforward
method of putting them in order (pick a node with no ancestors,
remove it from the graph, repeat). A longest monotone sequence is
obtained by finding a maximal cycle-free subgraph. So it all reduces
to graph theory. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Fri Jan  7 16:09:04 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Jan 2005 16:09:04 +0100
Subject: [R] configure error for R-2.0.1
In-Reply-To: <1836.129.16.30.223.1105104222.squirrel@webmail.chalmers.se>
References: <1836.129.16.30.223.1105104222.squirrel@webmail.chalmers.se>
Message-ID: <x23bxdb7jj.fsf@biostat.ku.dk>

"Sivakumar Ramagopal" <ramagopa at etek.chalmers.se> writes:

> Hi List,
> 
> I'm trying to build R-2.0.1 on and AMD64/Linux (Suse 9.1) and I get the
> following error while running the configure script.
> 
> ...
> checking for dummy main to link with Fortran libraries... none
> checking for Fortran name-mangling scheme... lower case, underscore, extra
> underscore
> checking whether g77 appens underscores to external names... yes
> checking whether mixed C/Fortran code can be run... configure: WARNING:
> cannot run  mixed C/Fortran code
> configure: error: Maybe check LDFLAGS for paths to Fortran libraries?
> 
> I have gcc-3.3.3.

Things do work on that platform, so you're probably missing an RPM or
two. My first guess would be

> rpm -qf `which g77`
gcc-g77-3.3.3-33

and you likely need a bunch of -devel packages too (readline-devel,
XFree86-devel, etc.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From depire at inrets.fr  Fri Jan  7 16:12:56 2005
From: depire at inrets.fr (Depire Alexandre)
Date: Fri, 7 Jan 2005 16:12:56 +0100
Subject: [R] Compilation of R code
Message-ID: <200501071612.57037.depire@inrets.fr>

Hello,
I'm a newbie on this list.
I have a R code but its execution take a very long time.
Is it possible to compile it (in C for example) to decrease the execution 
time ?

-- 
----------------
Alexandre DEPIRE
INRETS / GARIG



From anne.piotet at urbanet.ch  Fri Jan  7 16:19:51 2005
From: anne.piotet at urbanet.ch (Anne)
Date: Fri, 7 Jan 2005 16:19:51 +0100
Subject: [R] help with polytomous logistic regression
References: <20050107145654.BNAI19622.tomts10-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <002301c4f4cc$56e01bd0$6c00a8c0@mtd4>

Thank you, John! it is exactly what I need...looking forward to apply it

Anne

what would I become without this list?


----- Original Message ----- 
From: "John Fox" <jfox at mcmaster.ca>
To: "'Anne'" <anne.piotet at urbanet.ch>
Cc: "'R list'" <r-help at stat.math.ethz.ch>
Sent: Friday, January 07, 2005 3:56 PM
Subject: RE: [R] help with polytomous logistic regression


> Dear Anne,
>
> There's an anova() method for multinom objects that will allow you to get
> likelihood-ratio tests. Likewise, the Anova() function in the car package
> has a method for multinom objects.
>
> As to plotting, I have a paper, coauthored with Bob Andersen, on producing
> "effect" plots (partial plots) for polytomous logistic regression,
including
> multinomial and proportional-odds logistic regression. The paper, at
> <http://socserv.socsci.mcmaster.ca/jfox/logit-effect-displays.pdf>, has an
R
> function for computing such effects, including standard errors. There are
> examples in the paper as well. Eventually, I intend to incorporate this
into
> the effects package to make the graphs more or less automatic.
>
> Regards,
>  John
>
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox
> -------------------------------- 
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anne
> > Sent: Friday, January 07, 2005 8:53 AM
> > To: R list
> > Subject: [R] help with polytomous logistic regression
> >
> > Hi!
> >
> > I'm trying to do some ploytomous logistic regression using
> > multinom() in the nnet package, but am a bit confused about
> > interpretation of the results Is it possible to get the
> > following quantities:
> >
> > I:  maximum likelihood estimates to test for fit of model and
> > significance of each predictor
> >
> > (I would like to produce a table of the following type)
> > Analysis of Variance: MLE    (values are non sensical, I know!)
> >                df      chi2           p
> > intercept   2       57            .003
> > v1            4       89            .876
> > v2            2         7            .05
> > LR        110     450            0.93
> >
> > II:  chi square values and corresponding p-values for each
> > level of the predictor variates (I'm OK for the estimates and
> > se from summary,)
> >
> >
> >                level estimate     se        chi2         p
> > intercept   1        5              .9         32          .002
> >                 2       6.3            .8         31          .03
> > v1             3       89            .876       43          .001
> >                 4       65            .05         67
> > 0.001
> >  v2...
> >
> >
> > Is there a convenient way to plot the results? (I'd like to
> > display visually the effects of the predictors: any sugestion?)
> >
> > OK, I probably miss something here (no experience with (non
> > ordinal) polytomous logistic regression yet!)
> >
> > Thank  very much to all
> >
> > Anne
> >
> > ----------------------------------------------------
> > Anne Piotet
> > Tel: +41 79 359 83 32 (mobile)
> > Email: anne.piotet at m-td.com
> > ---------------------------------------------------
> > M-TD Modelling and Technology Development PSE-C
> > CH-1015 Lausanne
> > Switzerland
> > Tel: +41 21 693 83 98
> > Fax: +41 21 646 41 33
> > --------------------------------------------------
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>
>



From macq at llnl.gov  Fri Jan  7 16:34:31 2005
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 7 Jan 2005 07:34:31 -0800
Subject: [R] R packages on Mac
In-Reply-To: <41DE9273.5030904@ntlworld.com>
References: <41DE9273.5030904@ntlworld.com>
Message-ID: <p06110400be04593e5d39@[128.115.153.6]>

Are you referring to this statement from the R FAQs for Mac OS X?

"In same cases you can still build packages without all the tools 
installed, but surely not for all the packages containing C/C++ or 
Fortran source code."

It is saying that if you do *not* have "all the tools" installed on 
your Mac, then you may not be able to install packages containing C++ 
code. But if you do take the steps necessary for building R from 
sources (i.e., install all the tools), then you will also be able to 
build such packages from sources. The FAQs also describe what those 
tools are, that you will have to install.

I install R from source code, and I have yet to encounter a package I 
couldn't install from source. Many of them have C and/or Fortran. 
Whether any are C++ I don't know.

Do you have any specific packages in mind?

-Don

At 1:45 PM +0000 1/7/05, Samuel Kemp wrote:
>Hi,
>
>I am considering whether or not to buy an apple mac. I have noticed 
>on one of the R FAQs for Mac OS X that you cannot install packages 
>from other OS's if C++ code is contained. My question is: Is it 
>possible to build the package sources containing C++ code on the Mac 
>and then install them?
>
>Thanks in advance,
>
>Sam.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From tlumley at u.washington.edu  Fri Jan  7 16:34:59 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 7 Jan 2005 07:34:59 -0800 (PST)
Subject: [R] Help for "calibration"
In-Reply-To: <XFMail.050107112146.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050107112146.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.A41.4.61b.0501070731470.89432@homer03.u.washington.edu>

On Fri, 7 Jan 2005 Ted.Harding at nessie.mcc.ac.uk wrote:

> On 07-Jan-05 NDIKUMAGENGE Alice wrote:
>> Hello,
>>
>> I've tried stl decomposition but I made a mistake
>> I want to make a "calibration on a sample", in french we called it
>> "Redressement d'un ??chantillon"
>>
>> Thank u
>
> Are you referring to the sort of thing decsribed, for example,  in
>
>  http://statbel.fgov.be/studies/cal2002d.pdf
>
> and
>
>  http://statbel.fgov.be/studies/cal_en.asp
>
> ?
>
> If so, then you may find useful things in Thomas Lumley's
> "survey" package, though I'm not expert enough to judge how
> extensively it covers this area.
>

Not very extensively.  The procedure described on those websites is a 
extension and generalisation of what I would call raking to create 
poststratification weights.  The survey package will do raking by 
iterative proportional fitting, which means using a main-effects loglinear 
model instead of the more general formulation.

If you just want to get good poststratification weights you could try 
rake() and postStratify() in the survey package, but if you actually want 
to generalised calibration you are out of luck.


 	-thomas

From ernesto at ipimar.pt  Fri Jan  7 16:22:45 2005
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Fri, 07 Jan 2005 15:22:45 +0000
Subject: [R] lognorm
In-Reply-To: <20050107153132.78236.qmail@web51810.mail.yahoo.com>
References: <20050107153132.78236.qmail@web51810.mail.yahoo.com>
Message-ID: <41DEA945.20503@ipimar.pt>

Fred,

If you want to produce log normal values you can do it by using rlnorm, 
and you have to provide the mean and variance of the process in log 
scale, wich means after log transform. Or you can produce them with 
rnorm and exponentiate.

Check:

 > set.seed(1)
 > rlnorm(100, 0, 1)->vec
 > set.seed(1)
 > rnorm(100, 0, 1)->vec2
 > all.equal(exp(vec2),vec)
[1] TRUE

In both cases I'm using mean=0 and var=1.

Regards

EJ

Frederic renaud wrote:
> This values come from R. I 've read the doc but...
> If I've understood, I must write
> rlnorm(10000,0,0)) to have a mean = 1 and Var =1
> but that doesn't work (log 1 =0) 
> 
> --- Ernesto Jardim <ernesto at ipimar.pt> wrote:
> 
> 
>>Where did you get those values ?
>>
>> > mean(rlnorm(10000,0,1))
>>[1] 1.667496
>> > var(rlnorm(10000,0,1))
>>[1] 4.666731
>>
>>Look at previous emails, they are more explicit. The
>>basis is that you 
>>must give the mean and variance in log scale.
>>
>>EJ
>>
>>
>>Frederic renaud wrote:
>>
>>>yes but
>>>mean(rlnorm(10000,0,1)) ne 0
>>>and var(rlnorm(10000,0,1)) ne 1
>>>
>>>???
>>>
>>>
>>>
>>>--- Ernesto Jardim <ernesto at ipimar.pt> wrote:
>>>
>>>
>>>
>>>>>args(rlnorm)
>>>>
>>>>function (n, meanlog = 0, sdlog = 1)
>>>>NULL
>>>>
>>>>Frederic renaud wrote:
>>>>
>>>>
>>>>>Hi!
>>>>>I 've a problem to have a lognorm distribution
>>>>
>>>>with
>>>>
>>>>
>>>>>mean=1 and var (or sigma)=1.
>>>>>
>>>>>rlnorm(1000,0,0)
>>>>>rlnorm(1000,1,1)
>>>>>rlnorm(1000,0,1)
>>>>>....                     ?
>>>>>
>>>>>Can you help me?
>>>>>
>>>>>______________________________________________
>>>>>R-help at stat.math.ethz.ch mailing list
>>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>PLEASE do read the posting guide!
>>>>
>>>>http://www.R-project.org/posting-guide.html
>>>>
>>>
>>>
>>>
>>>__________________________________________________
>>
> 
> 
> 
> 		
> __________________________________ 



From cipprovince at menara.ma  Fri Jan  7 16:58:48 2005
From: cipprovince at menara.ma (PROVINCE DE KHEMISSET)
Date: Fri, 7 Jan 2005 15:58:48 -0000
Subject: [R] Latent trait models
Message-ID: <20050107155848.4B88620011@IMSS2.menara.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050107/5f8c7f1f/attachment.pl

From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Jan  7 17:05:26 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 7 Jan 2005 17:05:26 +0100
Subject: [R] glm fit with no intercept
References: <1196.193.10.23.31.1105108800.squirrel@webmail.math.su.se>
Message-ID: <00e901c4f4d2$b3c22090$0540210a@www.domain>

Hi Gudrun,

try the following:

y <- rbinom(100, 1, 0.5)
x <- rnorm(100)
##############
glm(y~x, family=binomial)
glm(y~x-1, family=binomial)

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Gudrun Jonasdottir" <gudrunj at math.su.se>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, January 07, 2005 3:40 PM
Subject: [R] glm fit with no intercept


> Dear R-help list members,
>
> I am currently trying to fit a generalized linear model using a 
> binomial
> with the canonical link. The usual solution is to use the R function 
> glm()
> in the package "stats". However, I run into problem when I want to 
> fit a
> glm without an intercept. It is indicated that the solution is in 
> changing
> the function glm.fit (also in "stats"), by specifying 
> intercept=FALSE. I
> have not been successful in getting any output though.
>
> Any suggestion on how to fit a glm with no intercept?
>
> Regards,
> Gudrun
>
> -- 
> Gudrun Jonasdottir, M.Sc.
> Matematiska institutionen
> Stockholms Universitet
> SE- 106 91 Stockholm
>
> Work: +46 (0)8 16 45 56
> Mobile: +46 (0)709 779 800
> --------------------------------------------
> "N?r inget annat hj?lper, l?s instruktionsboken."
> - Canns axiom
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Fri Jan  7 17:14:29 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Jan 2005 17:14:29 +0100
Subject: [R] glm fit with no intercept
In-Reply-To: <1196.193.10.23.31.1105108800.squirrel@webmail.math.su.se>
References: <1196.193.10.23.31.1105108800.squirrel@webmail.math.su.se>
Message-ID: <x2fz1dfc7u.fsf@biostat.ku.dk>

"Gudrun Jonasdottir" <gudrunj at math.su.se> writes:

> Dear R-help list members,
> 
> I am currently trying to fit a generalized linear model using a binomial
> with the canonical link. The usual solution is to use the R function glm()
> in the package "stats". However, I run into problem when I want to fit a
> glm without an intercept. It is indicated that the solution is in changing
> the function glm.fit (also in "stats"), by specifying intercept=FALSE. I
> have not been successful in getting any output though.
> 
> Any suggestion on how to fit a glm with no intercept?

Use -1 in the model formula.

  x <- sample(0:1,10,rep=T)
  d <- runif(10)
  summary(glm(x~d-1,family=binomial))

> "N?r inget annat hj?lper, l?s instruktionsboken."
> - Canns axiom

.. i n?rvarande fall: "An Introduction to R", Ch.11.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jfox at mcmaster.ca  Fri Jan  7 17:19:21 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 7 Jan 2005 11:19:21 -0500
Subject: [R] coercing columns
In-Reply-To: <41DEA0E8.1000202@uni-jena.de>
Message-ID: <20050107161920.IWUW1919.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Christoph,

> as.vector(as.matrix(DF))
[1] "A" "B" "A" "B" "C" "A" "C" "A" "B"

does the trick.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Christoph Scherber
> Sent: Friday, January 07, 2005 9:47 AM
> To: 'r-help at stat.math.ethz.ch'
> Subject: [R] coercing columns
> 
> Dear all,
> 
> I have a data frame that looks like this:
> 
> c1   c2   c3
> A    B    C
> B    C    A
> A    A    B
> 
> and so on;
> 
> I?d like to produce one single vector consisting of the 
> columns c1,c2, c3, such that
> 
> vector=("A","B","A","B","C","A","C","A","B")
> 
> I guess it?s easy to do but I don?t know how...Can anyone help me?
> 
> Thanks a lot!
> Christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From bates at wisc.edu  Fri Jan  7 17:22:36 2005
From: bates at wisc.edu (Douglas Bates)
Date: Fri, 07 Jan 2005 10:22:36 -0600
Subject: [R] nls - convergence problem
In-Reply-To: <1CmZYC-27VmCW0@filter10.bbul.t-online.de>
References: <1CmZYC-27VmCW0@filter10.bbul.t-online.de>
Message-ID: <41DEB74C.1030806@wisc.edu>

peter.schlattmann at t-online.de wrote:
> Dear list,
> 
> I do have a problem with nls. I use the following data:
> 
> 
>>test
> 
>     time  conc dose
>    0.50  5.40    1
>    0.75 11.10    1
>    1.00  8.40    1
>    1.25 13.80    1
>    1.50 15.50    1
>    1.75 18.00    1
>    2.00 17.00    1
>    2.50 13.90    1
>    3.00 11.20    1
>   3.50  9.90    1
>   4.00  4.70    1
>   5.00  5.00    1
>   6.00  1.90    1
>   7.00  1.90    1
>   9.00  1.10    1
> 12.00  0.95    1
> 14.00  0.46    1
> 24.00    NA    1
> 30.00    NA    1
> 36.00    NA    1
> 
> I use the self-starting function SSfol:
> 
> nls(conc~SSfol(dose,time,lKe,lKa,lCl),data=test,trace=T,control=nls.control(maxiter=13,tol=0.001,minFactor=1.E-500),na.action=na.omit)
> 
> This gives the following output:
> 
> 99.15824 :  -1.2061792  0.1296157 -4.3020997 
> 86.07567 :  -0.7053265 -0.3873204 -4.1278009 
> 85.19743 :  -0.5548499 -0.5333776 -4.1173627 
> 85.19246 :  -0.5466376 -0.5415731 -4.1173247 
> 85.1922 :  -0.5444637 -0.5437461 -4.1173223 
> 85.1922 :  -0.5442240 -0.5439857 -4.1173223 
> 85.1922 :  -0.5441337 -0.5440760 -4.1173223 
> 85.1922 :  -0.5441104 -0.5440993 -4.1173223 
> 85.1922 :  -0.5440984 -0.5441113 -4.1173223 
> 85.1922 :  -0.5441089 -0.5441008 -4.1173223 
> 85.1922 :  -0.5441006 -0.5441091 -4.1173223 
> 85.1922 :  -0.5441051 -0.5441046 -4.1173223 
> 85.1922 :  -0.5441051 -0.5441046 -4.1173223 
> 85.1922 :  -0.5441051 -0.5441046 -4.1173223 
> 
> Error in nls(conc ~ SSfol(dose, time, lKe, lKa, lCl), data = test, trace
> = T,  : 
>         number of iterations exceeded maximum of 13
> 
> I do not understand this error message for two reasons:
> 
> 1. Apparently the algorithm has converged, at the end of the output the
> change of parameter estimates seems to be less than 0.001
> 
> 
> 2. If the maximum number of iterations is reached I would expect a
> warning in the case that the algortihm did not converge. However, here
> the algorithm apparently has converged. Thus I do not understand what
> happened.

But it hasn't converged.  The nls function uses a relative offset 
convergence criterion that is not being satisfied here.  By setting the 
minimum step factor to a very small number you are more small steps than 
you would otherwise but you are not converging.  It appears that the 
parameter estimates are not defined for this model/data set combination.



From rolf at math.unb.ca  Fri Jan  7 17:30:07 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Fri, 7 Jan 2005 12:30:07 -0400 (AST)
Subject: [R] glm fit with no intercept
Message-ID: <200501071630.j07GU7j2012188@erdos.math.unb.ca>


As in any modelling situation in R or S, to suppress the
intercept you simply put a ``-1'' in the formula.  E.g.:

	fit <- glm(y~x-1,family=binomial)

			cheers,

				Rolf Turner
				rolf at math.unb.ca



From MSchwartz at MedAnalytics.com  Fri Jan  7 17:32:42 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 07 Jan 2005 10:32:42 -0600
Subject: [R] coercing columns
In-Reply-To: <41DEA0E8.1000202@uni-jena.de>
References: <41DEA0E8.1000202@uni-jena.de>
Message-ID: <1105115562.6081.8.camel@horizons.localdomain>

On Fri, 2005-01-07 at 15:47 +0100, Christoph Scherber wrote:
> Dear all,
> 
> I have a data frame that looks like this:
> 
> c1   c2   c3
> A    B    C
> B    C    A
> A    A    B
> 
> and so on;
> 
> I?d like to produce one single vector consisting of the columns c1,c2, 
> c3, such that
> 
> vector=("A","B","A","B","C","A","C","A","B")
> 
> I guess it?s easy to do but I don?t know how...Can anyone help me?
> 
> Thanks a lot!
> Christoph


> as.vector(as.matrix(df))
[1] "A" "B" "A" "B" "C" "A" "C" "A" "B"

If you review the help for as.matrix, it indicates:

 'as.matrix' is a generic function. The method for data frames will
  convert any non-numeric/complex column into a character vector
  using 'format' and so return a character matrix, except that
  all-logical data frames will be coerced to a logical matrix.

So using as.matrix() converts the 'df' columns into a character matrix,
which is then converted into a vector using as.vector().

HTH,

Marc Schwartz



From sundar.dorai-raj at pdf.com  Fri Jan  7 17:38:42 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 07 Jan 2005 10:38:42 -0600
Subject: [R] coercing columns
In-Reply-To: <41DEA0E8.1000202@uni-jena.de>
References: <41DEA0E8.1000202@uni-jena.de>
Message-ID: <41DEBB12.7070907@pdf.com>



Christoph Scherber wrote:

> Dear all,
> 
> I have a data frame that looks like this:
> 
> c1   c2   c3
> A    B    C
> B    C    A
> A    A    B
> 
> and so on;
> 
> I?d like to produce one single vector consisting of the columns c1,c2, 
> c3, such that
> 
> vector=("A","B","A","B","C","A","C","A","B")
> 
> I guess it?s easy to do but I don?t know how...Can anyone help me?
> 

Christoph,

Assuming c1, c2, and c3 are stored as factor and `x' is your data.frame, 
then you can do:

unlist(lapply(x, as.character))

Otherwise if c1, c2, and c3 are already character then you can skip the 
lapply part and just do unlist.

--sundar



From tarmo.remmel at utoronto.ca  Fri Jan  7 17:38:28 2005
From: tarmo.remmel at utoronto.ca (Tarmo Remmel)
Date: Fri,  7 Jan 2005 11:38:28 -0500
Subject: [R] coercing columns
In-Reply-To: <41DEA0E8.1000202@uni-jena.de>
References: <41DEA0E8.1000202@uni-jena.de>
Message-ID: <1105115908.41debb04cf96c@webmail.utoronto.ca>

if your dataframe is called foo,

as.vector(foo)






Quoting Christoph Scherber <Christoph.Scherber at uni-jena.de>:

> Dear all,
> 
> I have a data frame that looks like this:
> 
> c1   c2   c3
> A    B    C
> B    C    A
> A    A    B
> 
> and so on;
> 
> I?d like to produce one single vector consisting of the columns c1,c2, 
> c3, such that
> 
> vector=("A","B","A","B","C","A","C","A","B")
> 
> I guess it?s easy to do but I don?t know how...Can anyone help me?
> 
> Thanks a lot!
> Christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 


_________________________________________________
Tarmo Remmel  B.E.S. M.Sc.F. (Ph.D. Candidate)
GUESS Research Group - Department of Geography
University of Toronto, Toronto, Ontario, CANADA
Phone: 416.946.3058
http://eratos.erin.utoronto.ca/remmelt



From ligges at statistik.uni-dortmund.de  Fri Jan  7 17:55:08 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 07 Jan 2005 17:55:08 +0100
Subject: [R] Compilation of R code
In-Reply-To: <200501071612.57037.depire@inrets.fr>
References: <200501071612.57037.depire@inrets.fr>
Message-ID: <41DEBEEC.6070205@statistik.uni-dortmund.de>

Depire Alexandre wrote:

> Hello,
> I'm a newbie on this list.
> I have a R code but its execution take a very long time.
> Is it possible to compile it (in C for example) to decrease the execution 
> time ?

a) R is not C.

b) No, we don't have an R compiler (except - in development - a byte 
compiler written by Luke Tierney, but that won't save magnitudes).

You might want either to optimize the code, given it not very well 
written, or reqrite parts of the code in C. See the "Writing R 
Extensions" manual.

Uwe Ligges



From bob at kruus.forestry.utoronto.ca  Fri Jan  7 18:08:34 2005
From: bob at kruus.forestry.utoronto.ca (Robert Kruus)
Date: Fri, 7 Jan 2005 12:08:34 -0500
Subject: [R] coercing columns
In-Reply-To: <41DEA0E8.1000202@uni-jena.de>
References: <41DEA0E8.1000202@uni-jena.de>
Message-ID: <20050107120834.3761e964@kruuslt.forestry.utoronto.ca>

How about 

as.vector(x)
where x is the data?
BTW a matrix in R is stored as a vector anyways (IIRC).

-- 
robert.kruus at utoronto.ca
You will feel hungry again in another hour.

--------

It is  rumored that on Fri, 07 Jan 2005 15:47:04 +0100
Christoph Scherber <Christoph.Scherber at uni-jena.de> wrote:

> Dear all,
> 
> I have a data frame that looks like this:
> 
> c1   c2   c3
> A    B    C
> B    C    A
> A    A    B
> 
> and so on;
> 
> I?d like to produce one single vector consisting of the columns c1,c2,
> c3, such that
> 
> vector=("A","B","A","B","C","A","C","A","B")
> 
> I guess it?s easy to do but I don?t know how...Can anyone help me?
> 
> Thanks a lot!
> Christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 


--------



From ripley at stats.ox.ac.uk  Fri Jan  7 18:26:02 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 7 Jan 2005 17:26:02 +0000 (GMT)
Subject: [R] glm fit with no intercept
In-Reply-To: <1196.193.10.23.31.1105108800.squirrel@webmail.math.su.se>
References: <1196.193.10.23.31.1105108800.squirrel@webmail.math.su.se>
Message-ID: <Pine.LNX.4.61.0501071719160.12015@gannet.stats>

On Fri, 7 Jan 2005, Gudrun Jonasdottir wrote:

> I am currently trying to fit a generalized linear model using a binomial
> with the canonical link. The usual solution is to use the R function glm()
> in the package "stats". However, I run into problem when I want to fit a
> glm without an intercept. It is indicated that the solution is in changing

Where is it so indicated?

> the function glm.fit (also in "stats"), by specifying intercept=FALSE. I
> have not been successful in getting any output though.
>
> Any suggestion on how to fit a glm with no intercept?

>From MASS4:

ldose <- rep(0:5, 2)
numdead <- c(1, 4, 9, 13, 18, 20, 0, 2, 6, 10, 12, 16)
sex <- factor(rep(c("M", "F"), c(6, 6)))
SF <- cbind(numdead, numalive = 20 - numdead)
glm(SF ~ sex+ ldose, family = binomial)
glm(SF ~ sex + ldose - 1, family = binomial) # no intercept
glm(SF ~ ldose - 1, family = binomial) # and not even a factor

Please tell us more of what you have tried.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Fri Jan  7 18:31:03 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 7 Jan 2005 12:31:03 -0500
Subject: [R] coercing columns
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4CE@usrymx25.merck.com>

Coercion to matrix then to vector should work:

> dat
  V1 V2 V3
1  A  B  C
2  B  C  A
3  A  A  B
> as.vector(as.matrix(dat))
[1] "A" "B" "A" "B" "C" "A" "C" "A" "B"

Andy

> From: Christoph Scherber
> 
> Dear all,
> 
> I have a data frame that looks like this:
> 
> c1   c2   c3
> A    B    C
> B    C    A
> A    A    B
> 
> and so on;
> 
> I?d like to produce one single vector consisting of the 
> columns c1,c2, 
> c3, such that
> 
> vector=("A","B","A","B","C","A","C","A","B")
> 
> I guess it?s easy to do but I don?t know how...Can anyone help me?
> 
> Thanks a lot!
> Christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From spencer.graves at pdf.com  Fri Jan  7 18:48:59 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 07 Jan 2005 09:48:59 -0800
Subject: [R] Compilation of R code
In-Reply-To: <200501071612.57037.depire@inrets.fr>
References: <200501071612.57037.depire@inrets.fr>
Message-ID: <41DECB8B.10008@pdf.com>

      R is basically an interpreted language.  To my knowledge, the 
standard approach to improving speed starts by finding which steps take 
the most time.  This can be done using "proc.time" and / or 
"system.time" (preceded by "gc" to clean things up so the timing is more 
stable). 

      Once you've determined which loop or function call consumes the 
most time, you then need to decide what to do about it.  R is 
vectorized, so if you use loops like in Fortran or C, you might be able 
to replace complicated pieces of code with one or only a very few 
commands.  This list has considered many questions about speed, so once 
you know which steps are taking the most time, you can search the 
archives as described in the posting guide, 
"http://www.R-project.org/posting-guide.html".  If that does not solve 
the problem, you can translate a function into a compiled language like 
C.  See help.start() -> "Writing R Extensions" -> "System and foreign 
language interfaces". 

      hope this helps. 
      spencer graves

Depire Alexandre wrote:

>Hello,
>I'm a newbie on this list.
>I have a R code but its execution take a very long time.
>Is it possible to compile it (in C for example) to decrease the execution 
>time ?
>
>  
>



From lyon at fnal.gov  Fri Jan  7 19:35:35 2005
From: lyon at fnal.gov (Adam Lyon)
Date: Fri, 07 Jan 2005 12:35:35 -0600
Subject: [R] Destructor for S4 objects?
Message-ID: <BE043297.EE43%lyon@fnal.gov>

Hi,

To write a "constructor" for an S4 object, you make an initialize method
which will be called by new. But how would I make a "destructor" method to
be called when the S4 object is garbage collected? I'm looking at
reg.finalizer, but I'm not sure how to make that work for an S4 object.

I want to write a destructor because my S4 object's initialize method
allocates some resources. I would like the destructor to free them.

I guess I could make one of the slots in my object an environment that holds
the handles for those resources and register that environment with the
finalizer. Then the finalizer function would release those resources. Is
that the way to do it or is there another simpler way?

Thanks for any help and Happy New Year!

Adam Lyon



From choudary.jagar at swosu.edu  Fri Jan  7 19:50:23 2005
From: choudary.jagar at swosu.edu (Jagarlamudi, Choudary)
Date: Fri, 7 Jan 2005 12:50:23 -0600
Subject: [R] Help in customising the NLS function to spit out mean and SD of
	new fit!!
Message-ID: <E03EBB50FF2C024781A6E4460AD58F0607C13C@swosu-mbx01.admin.swosu.edu>

 i'm coding in  R(first time) for a paper my colleague is publishing.
 i plotted a histogram for 6000 values. Taking the mean and sd of the  midpoints i did a dnorm and got the densities.
 pl<-dnorm(trimmedvals,mean=midsmean,sd=midsSD)
 (now in a loop of 5 times)
 i plotted these experimental values against theoretical values using
 the nls function the following way.
 nlsresult<-nlsModel(pl ~ 1/sqrt(2 *pi)*std)*2.71828^2/(2*std4^2)),data=answer,start=list(std4=std4,sm=sm))
 in the formula mean was mid point of 1st frequency bar and sd was sd  of 6000 values.I do this 5 times each time changing the mean in the formula to be the mid point of the next frequency bar.
 now i plotted plot(fittedvals ~trimmedvals)
 
I am told to plot experimental vs theoretical vlaues from the  histogram and get a non linear least square curve fit.
 I need the mean and sd of this new fit to proceed to my next  module.I'm not sure if i'm on track. Excuse me if this sounds too  naive.If nls is not what i should be using can you please give me some  pinters to solve this.

 Thanks in advance.

Choudary Jagarlamudi
Instructor
Southwestern Oklahoma State University
STF 254
100 campus Drive
Weatherford OK 73096
Tel 580-774-7136
 

From spencer.graves at pdf.com  Fri Jan  7 20:17:32 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 07 Jan 2005 11:17:32 -0800
Subject: [R] nls - convergence problem
In-Reply-To: <41DEB74C.1030806@wisc.edu>
References: <1CmZYC-27VmCW0@filter10.bbul.t-online.de>
	<41DEB74C.1030806@wisc.edu>
Message-ID: <41DEE04C.8060208@pdf.com>

Hi, Doug: 

      How would you diagnose something like this?  For example, might 
the following (from ?nlsModel) help: 

     DNase1 <- DNase[ DNase$Run == 1, ]
     mod <-
      nlsModel(density ~ SSlogis( log(conc), Asym, xmid, scal ),
               DNase1, start=list( Asym = 3, xmid = 0, scal = 1 ))
     mod$Rmat()        # R matrix from the QR decomposition of the gradient

      Wouldn't the first zero or nearly diagonal element of Rmat 
identify a variable to which the model was insensitive and could 
therefore be fixed or removed from the model? 
      Thanks,
      Spencer Graves    

Douglas Bates wrote:

> peter.schlattmann at t-online.de wrote:
>
>> Dear list,
>>
>> I do have a problem with nls. I use the following data:
>>
>>
>>> test
>>
>>
>>     time  conc dose
>>    0.50  5.40    1
>>    0.75 11.10    1
>>    1.00  8.40    1
>>    1.25 13.80    1
>>    1.50 15.50    1
>>    1.75 18.00    1
>>    2.00 17.00    1
>>    2.50 13.90    1
>>    3.00 11.20    1
>>   3.50  9.90    1
>>   4.00  4.70    1
>>   5.00  5.00    1
>>   6.00  1.90    1
>>   7.00  1.90    1
>>   9.00  1.10    1
>> 12.00  0.95    1
>> 14.00  0.46    1
>> 24.00    NA    1
>> 30.00    NA    1
>> 36.00    NA    1
>>
>> I use the self-starting function SSfol:
>>
>> nls(conc~SSfol(dose,time,lKe,lKa,lCl),data=test,trace=T,control=nls.control(maxiter=13,tol=0.001,minFactor=1.E-500),na.action=na.omit) 
>>
>>
>> This gives the following output:
>>
>> 99.15824 :  -1.2061792  0.1296157 -4.3020997 86.07567 :  -0.7053265 
>> -0.3873204 -4.1278009 85.19743 :  -0.5548499 -0.5333776 -4.1173627 
>> 85.19246 :  -0.5466376 -0.5415731 -4.1173247 85.1922 :  -0.5444637 
>> -0.5437461 -4.1173223 85.1922 :  -0.5442240 -0.5439857 -4.1173223 
>> 85.1922 :  -0.5441337 -0.5440760 -4.1173223 85.1922 :  -0.5441104 
>> -0.5440993 -4.1173223 85.1922 :  -0.5440984 -0.5441113 -4.1173223 
>> 85.1922 :  -0.5441089 -0.5441008 -4.1173223 85.1922 :  -0.5441006 
>> -0.5441091 -4.1173223 85.1922 :  -0.5441051 -0.5441046 -4.1173223 
>> 85.1922 :  -0.5441051 -0.5441046 -4.1173223 85.1922 :  -0.5441051 
>> -0.5441046 -4.1173223
>> Error in nls(conc ~ SSfol(dose, time, lKe, lKa, lCl), data = test, trace
>> = T,  :         number of iterations exceeded maximum of 13
>>
>> I do not understand this error message for two reasons:
>>
>> 1. Apparently the algorithm has converged, at the end of the output the
>> change of parameter estimates seems to be less than 0.001
>>
>>
>> 2. If the maximum number of iterations is reached I would expect a
>> warning in the case that the algortihm did not converge. However, here
>> the algorithm apparently has converged. Thus I do not understand what
>> happened.
>
>
> But it hasn't converged.  The nls function uses a relative offset 
> convergence criterion that is not being satisfied here.  By setting 
> the minimum step factor to a very small number you are more small 
> steps than you would otherwise but you are not converging.  It appears 
> that the parameter estimates are not defined for this model/data set 
> combination.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From gunter.berton at gene.com  Fri Jan  7 21:04:28 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 7 Jan 2005 12:04:28 -0800
Subject: [R] Help in customising the NLS function to spit out mean and SD
	ofnew fit!!
In-Reply-To: <E03EBB50FF2C024781A6E4460AD58F0607C13C@swosu-mbx01.admin.swosu.edu>
Message-ID: <200501072004.j07K4S52012415@volta.gene.com>

It **sounds ** like you are trying to fit a nonparametric density to 6000
values...  If so, please stop what you are doing and see ?density. You could
also search on "fit density" or something similar on the R site search, as
there are other R functions in R packages that do density fitting (ash is
one, but other recommendations anyone?).

If this is not what you are trying to do, I think you are still likely going
about it in the wrong way -- histograms lose information and are notoriously
dependent on the choice of cutpoints (which is part of the motivation for
Scott's ash package). You might wish to consult a local statistician to get
some better approaches to whatever it is that you're trying to do.

Cheers,
Bert Gunter


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jagarlamudi, Choudary
Sent: Friday, January 07, 2005 10:50 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Help in customising the NLS function to spit out mean and SD
ofnew fit!!

 i'm coding in  R(first time) for a paper my colleague is publishing.
 i plotted a histogram for 6000 values. Taking the mean and sd of the
midpoints i did a dnorm and got the densities.
 pl<-dnorm(trimmedvals,mean=midsmean,sd=midsSD)
 (now in a loop of 5 times)
 i plotted these experimental values against theoretical values using
 the nls function the following way.
 nlsresult<-nlsModel(pl ~ 1/sqrt(2
*pi)*std)*2.71828^2/(2*std4^2)),data=answer,start=list(std4=std4,sm=sm))
 in the formula mean was mid point of 1st frequency bar and sd was sd  of
6000 values.I do this 5 times each time changing the mean in the formula to
be the mid point of the next frequency bar.
 now i plotted plot(fittedvals ~trimmedvals)
 
I am told to plot experimental vs theoretical vlaues from the  histogram and
get a non linear least square curve fit.
 I need the mean and sd of this new fit to proceed to my next  module.I'm
not sure if i'm on track. Excuse me if this sounds too  naive.If nls is not
what i should be using can you please give me some  pinters to solve this.

 Thanks in advance.

Choudary Jagarlamudi
Instructor
Southwestern Oklahoma State University
STF 254
100 campus Drive
Weatherford OK 73096
Tel 580-774-7136



From andy_liaw at merck.com  Fri Jan  7 21:18:49 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 7 Jan 2005 15:18:49 -0500
Subject: [R] Compilation of R code
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4D3@usrymx25.merck.com>

For figuring out where in the code the bottleneck is, a better tool is
probably the profiler.  See:

http://cran.r-project.org/doc/manuals/R-exts.html#Profiling%20R%20code

HTH,
Andy

> From: Spencer Graves
> 
>       R is basically an interpreted language.  To my knowledge, the 
> standard approach to improving speed starts by finding which 
> steps take 
> the most time.  This can be done using "proc.time" and / or 
> "system.time" (preceded by "gc" to clean things up so the 
> timing is more 
> stable). 
> 
>       Once you've determined which loop or function call consumes the 
> most time, you then need to decide what to do about it.  R is 
> vectorized, so if you use loops like in Fortran or C, you 
> might be able 
> to replace complicated pieces of code with one or only a very few 
> commands.  This list has considered many questions about 
> speed, so once 
> you know which steps are taking the most time, you can search the 
> archives as described in the posting guide, 
> "http://www.R-project.org/posting-guide.html".  If that does 
> not solve 
> the problem, you can translate a function into a compiled 
> language like 
> C.  See help.start() -> "Writing R Extensions" -> "System and foreign 
> language interfaces". 
> 
>       hope this helps. 
>       spencer graves
> 
> Depire Alexandre wrote:
> 
> >Hello,
> >I'm a newbie on this list.
> >I have a R code but its execution take a very long time.
> >Is it possible to compile it (in C for example) to decrease 
> the execution 
> >time ?
> >
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From blindglobe at gmail.com  Fri Jan  7 21:35:15 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Fri, 7 Jan 2005 21:35:15 +0100
Subject: [R] Compilation of R code
In-Reply-To: <200501071612.57037.depire@inrets.fr>
References: <200501071612.57037.depire@inrets.fr>
Message-ID: <1abe3fa905010712351c74033@mail.gmail.com>

There was a real compiler which was under development as a MS thesis
at Rice U last year, but I'm not sure if it was finished or ever will
be released.  I actually saw it in action on a small example...

best,
-tony



On Fri, 7 Jan 2005 16:12:56 +0100, Depire Alexandre <depire at inrets.fr> wrote:
> Hello,
> I'm a newbie on this list.
> I have a R code but its execution take a very long time.
> Is it possible to compile it (in C for example) to decrease the execution
> time ?
> 
> --
> ----------------
> Alexandre DEPIRE
> INRETS / GARIG
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From andy_liaw at merck.com  Fri Jan  7 21:56:29 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 7 Jan 2005 15:56:29 -0500
Subject: [R] Compilation of R code
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4D5@usrymx25.merck.com>

> From: A.J. Rossini
> 
> There was a real compiler which was under development as a MS thesis
> at Rice U last year, but I'm not sure if it was finished or ever will
> be released.  I actually saw it in action on a small example...
> 
> best,
> -tony

Yes: http://hipersoft.cs.rice.edu/rcc/

(I believe Luke had gotten involved to some degree.)

However, I'd still suggest trying to optimize the R code before trying other
more drastic measures...

Andy
 
> On Fri, 7 Jan 2005 16:12:56 +0100, Depire Alexandre 
> <depire at inrets.fr> wrote:
> > Hello,
> > I'm a newbie on this list.
> > I have a R code but its execution take a very long time.
> > Is it possible to compile it (in C for example) to decrease 
> the execution
> > time ?
> > 
> > --
> > ----------------
> > Alexandre DEPIRE
> > INRETS / GARIG
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > 
> 
> 
> -- 
> best,
> 
> -tony
> 
> "Commit early,commit often, and commit in a repository from 
> which we can easily
> roll-back your mistakes" (AJR, 4Jan05).
> 
> A.J. Rossini
> blindglobe at gmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Fri Jan  7 22:03:21 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 7 Jan 2005 16:03:21 -0500
Subject: [R] Help in customising the NLS function to spit out mean
	and SD ofnew fit!!
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4D6@usrymx25.merck.com>

Doesn't look like nonparametric fit to me, since nls() is used to fit to a
gaussian density, so the result is a gaussian density (with estimated
parameter).

What I do not understand is why people would do this.  This is not the first
time I've seen people doing this, on both R-help and S-news (if my memory is
still any good).  If the objective is to fit a Gaussian distribution to the
data, the `best' Gaussian curve is the one where mu = mean(data) and sigma^2
= var(data).  Why go through all that trouble?  What am I missing?

Andy

> From: Berton Gunter
> 
> It **sounds ** like you are trying to fit a nonparametric 
> density to 6000
> values...  If so, please stop what you are doing and see 
> ?density. You could
> also search on "fit density" or something similar on the R 
> site search, as
> there are other R functions in R packages that do density 
> fitting (ash is
> one, but other recommendations anyone?).
> 
> If this is not what you are trying to do, I think you are 
> still likely going
> about it in the wrong way -- histograms lose information and 
> are notoriously
> dependent on the choice of cutpoints (which is part of the 
> motivation for
> Scott's ash package). You might wish to consult a local 
> statistician to get
> some better approaches to whatever it is that you're trying to do.
> 
> Cheers,
> Bert Gunter
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Jagarlamudi, Choudary
> Sent: Friday, January 07, 2005 10:50 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Help in customising the NLS function to spit out 
> mean and SD
> ofnew fit!!
> 
>  i'm coding in  R(first time) for a paper my colleague is publishing.
>  i plotted a histogram for 6000 values. Taking the mean and sd of the
> midpoints i did a dnorm and got the densities.
>  pl<-dnorm(trimmedvals,mean=midsmean,sd=midsSD)
>  (now in a loop of 5 times)
>  i plotted these experimental values against theoretical values using
>  the nls function the following way.
>  nlsresult<-nlsModel(pl ~ 1/sqrt(2
> *pi)*std)*2.71828^2/(2*std4^2)),data=answer,start=list(std4=st
> d4,sm=sm))
>  in the formula mean was mid point of 1st frequency bar and 
> sd was sd  of
> 6000 values.I do this 5 times each time changing the mean in 
> the formula to
> be the mid point of the next frequency bar.
>  now i plotted plot(fittedvals ~trimmedvals)
>  
> I am told to plot experimental vs theoretical vlaues from the 
>  histogram and
> get a non linear least square curve fit.
>  I need the mean and sd of this new fit to proceed to my next 
>  module.I'm
> not sure if i'm on track. Excuse me if this sounds too  
> naive.If nls is not
> what i should be using can you please give me some  pinters 
> to solve this.
> 
>  Thanks in advance.
> 
> Choudary Jagarlamudi
> Instructor
> Southwestern Oklahoma State University
> STF 254
> 100 campus Drive
> Weatherford OK 73096
> Tel 580-774-7136
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Robert.McGehee at geodecapital.com  Fri Jan  7 22:25:51 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Fri, 7 Jan 2005 16:25:51 -0500
Subject: [R] Creating unary operators
Message-ID: <67DCA285A2D7754280D3B8E88EB5480206741DFF@MSGBOSCLB2WIN.DMN1.FMR.COM>

Is it correct (by its lack of mention in the R-Language Definition
Manual) that it is impossible to create a user-defined unary operator?

Ex: (This doesn't work, but it's an example of what I'm looking for)

> "%PLUSONE%" <- function(x) x + 1
> %PLUSONE% 2
[1] 3

And if the above is impossible, am I limited to only the + - ~ ! unary
operators for overloading?

On the same vein, is it correct that "[" and "[[" are the only possible
indexing generics? (Assigning "[" to "[[[" doesn't seem to work, and
nothing else is mentioned in the manual)

My goal is to parse well-defined character strings into R-readable code,
while preserving the structure of this text-language as much as
possible, so I'd rather compute on the R language rather than on this
text language.

Best,
Robert

Robert McGehee
Geode Capital Management, LLC
53 State Street, 5th Floor | Boston, MA | 02109
Tel: 617/392-8396    Fax:617/476-6389
mailto:robert.mcgehee at geodecapital.com



This e-mail, and any attachments hereto, are intended for use by the
addressee(s) only and may contain information that is (i) confidential
information of Geode Capital Management, LLC and/or its affiliates,
and/or (ii) proprietary information of Geode Capital Management, LLC
and/or its affiliates. If you are not the intended recipient of this
e-mail, or if you have otherwise received this e-mail in error, please
immediately notify me by telephone (you may call collect), or by e-mail,
and please permanently delete the original, any print outs and any
copies of the foregoing. Any dissemination, distribution or copying of
this e-mail is strictly prohibited.



From yc176 at yahoo.com  Fri Jan  7 23:05:36 2005
From: yc176 at yahoo.com (Yong Chao)
Date: Fri, 7 Jan 2005 14:05:36 -0800 (PST)
Subject: [R] Wilcoxon rank sum test
Message-ID: <20050107220536.7853.qmail@web51303.mail.yahoo.com>

Hi, 

This might be a general question: I am comparing two
groups using Wilcoxon rank sum test. However, the size
of the two groups differ a lot, e.g., one group has 20
and the other has 10000. Is the test still appropriate
given this huge disparity in size? If yes, what is the
alternative?

Thanks!

Yong



From gunter.berton at gene.com  Fri Jan  7 23:39:17 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 7 Jan 2005 14:39:17 -0800
Subject: [R] Creating unary operators
In-Reply-To: <67DCA285A2D7754280D3B8E88EB5480206741DFF@MSGBOSCLB2WIN.DMN1.FMR.COM>
Message-ID: <200501072239.j07MdH6V004824@faraday.gene.com>

Robert:

V&R's S-PROGRAMMING is an indispensable reference for sophisticated R
programming issues like those you bring up. Also the John Chambers Green
book (Programming with Data ...)might be useful .

I would await definitive confirmation from the experts, but AFAIK you're
correct -- you can only overload the existing unary operators, not create
new ones as you can binary operators via "%whatever%".

Of course, PLUSONE(2) is not all that different from %PLUSONE%2; or you
could even "fake" a unary operator by
 
"%PLUSONE%"<-function(x,y)y+1

and then have your parser turn your unary operator into 1%PLUSONE%y whenever
the increment expression appeared in your original language.  So in terms of
"code readability," I'm not clear why you really need this feature.

Same for the indexing generics, although here something like

"%[]%"<-function(x,y)x[y]

Seems almost exactly like what you want to do. But "readability", like
beauty, is in the eyes of the beholder, I guess.

HTH

Cheers,

Bert Gunter

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of McGehee, Robert
Sent: Friday, January 07, 2005 1:26 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Creating unary operators

Is it correct (by its lack of mention in the R-Language Definition
Manual) that it is impossible to create a user-defined unary operator?

Ex: (This doesn't work, but it's an example of what I'm looking for)

> "%PLUSONE%" <- function(x) x + 1
> %PLUSONE% 2
[1] 3

And if the above is impossible, am I limited to only the + - ~ ! unary
operators for overloading?

On the same vein, is it correct that "[" and "[[" are the only possible
indexing generics? (Assigning "[" to "[[[" doesn't seem to work, and
nothing else is mentioned in the manual)

My goal is to parse well-defined character strings into R-readable code,
while preserving the structure of this text-language as much as
possible, so I'd rather compute on the R language rather than on this
text language.

Best,
Robert

Robert McGehee
Geode Capital Management, LLC
53 State Street, 5th Floor | Boston, MA | 02109
Tel: 617/392-8396    Fax:617/476-6389
mailto:robert.mcgehee at geodecapital.com



This e-mail, and any attachments hereto, are intended for use by the
addressee(s) only and may contain information that is (i) confidential
information of Geode Capital Management, LLC and/or its affiliates,
and/or (ii) proprietary information of Geode Capital Management, LLC
and/or its affiliates. If you are not the intended recipient of this
e-mail, or if you have otherwise received this e-mail in error, please
immediately notify me by telephone (you may call collect), or by e-mail,
and please permanently delete the original, any print outs and any
copies of the foregoing. Any dissemination, distribution or copying of
this e-mail is strictly prohibited.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From gb at tal.stat.umu.se  Fri Jan  7 23:47:16 2005
From: gb at tal.stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Fri, 7 Jan 2005 23:47:16 +0100
Subject: [R] glm fit with no intercept
In-Reply-To: <x2fz1dfc7u.fsf@biostat.ku.dk>
References: <1196.193.10.23.31.1105108800.squirrel@webmail.math.su.se>
	<x2fz1dfc7u.fsf@biostat.ku.dk>
Message-ID: <20050107224716.GA14055@tal.stat.umu.se>

On Fri, Jan 07, 2005 at 05:14:29PM +0100, Peter Dalgaard wrote:
> "Gudrun Jonasdottir" <gudrunj at math.su.se> writes:
> 
> > Dear R-help list members,
> > 
> > I am currently trying to fit a generalized linear model using a binomial
> > with the canonical link. The usual solution is to use the R function glm()
> > in the package "stats". However, I run into problem when I want to fit a
> > glm without an intercept. It is indicated that the solution is in changing
> > the function glm.fit (also in "stats"), by specifying intercept=FALSE. I
> > have not been successful in getting any output though.
> > 
> > Any suggestion on how to fit a glm with no intercept?
> 
> Use -1 in the model formula.
> 
>   x <- sample(0:1,10,rep=T)
>   d <- runif(10)
>   summary(glm(x~d-1,family=binomial))
> 
> > "N?r inget annat hj?lper, l?s instruktionsboken."
> > - Canns axiom
> 
> .. i n?rvarande fall: "An Introduction to R", Ch.11.

.. i f?religgande fall: (or, .. i detta fall:) is better Swedish.
But maybe it was Danish:-). Anyway, I agree. 

HTH, G?ran
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se



From samuel_mwalili at yahoo.com  Sat Jan  8 01:00:23 2005
From: samuel_mwalili at yahoo.com (Sameul M Mwalili)
Date: Fri, 7 Jan 2005 16:00:23 -0800 (PST)
Subject: [R] Compilation of R code
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E4D5@usrymx25.merck.com>
Message-ID: <20050108000023.46622.qmail@web53407.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050107/e1491f1d/attachment.pl

From MSchwartz at MedAnalytics.com  Sat Jan  8 01:47:12 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 07 Jan 2005 18:47:12 -0600
Subject: [R] Compilation of R code
In-Reply-To: <20050108000023.46622.qmail@web53407.mail.yahoo.com>
References: <20050108000023.46622.qmail@web53407.mail.yahoo.com>
Message-ID: <1105145233.6298.7.camel@horizons.localdomain>

On Fri, 2005-01-07 at 16:00 -0800, Sameul M Mwalili wrote:
> Dear ALL,
> In order to install the Rice R to C compiler (RCC) you need to patch
> the R source code. However, the patch command at DOS prompt returns an
> error:  patch is not recognized as internal or external command. How
> do you patch in DOS (or Windows)?
> 
> Regards,
> 
> Samuel.

The GNU patch program is available as source code via:

http://www.fsf.org/software/patch/patch.html

I also located a Sourceforge project that has pre-built native Win32
binaries, which requires msvcrt.dll:

http://unxutils.sourceforge.net/

There may be other resources to reference.

HTH,

Marc Schwartz



From patrick.drechsler at gmx.net  Sat Jan  8 02:33:27 2005
From: patrick.drechsler at gmx.net (Patrick Drechsler)
Date: Sat, 08 Jan 2005 02:33:27 +0100
Subject: [R] autoscaling plot font size in Sweave output possible?
References: <m37jmqupy8.fsf@pdrechsler.fqdn.th-h.de>
	<41DE4549.90509@statistik.uni-dortmund.de>
Message-ID: <m3pt0g4sd4.fsf@pdrechsler.fqdn.th-h.de>


Uwe Ligges wrote on 07 Jan 2005 09:16:09 MET:

> Patrick Drechsler wrote:
>
>> I was wondering if it's possible to have fonts in plots to be
>> autoscaled to the same font size used by LaTeX in a
>> surrounding Sweave document.
>
> See ?strwidth which might help for the required calculations.

Thanks for the feedback, Uwe!

Not yet being familiar with low level plotting in R I will keep
this help page in mind.

Can you or somebody else be so kind as to show me an example how
to use `strwidth' in the example of my OP?

Cheers

Patrick
-- 
Millions long for immortality who do not know what to do with
themselves on a rainy Sunday afternoon.
		-- Susan Ertz



From patrick.drechsler at gmx.net  Sat Jan  8 03:59:14 2005
From: patrick.drechsler at gmx.net (Patrick Drechsler)
Date: Sat, 08 Jan 2005 03:59:14 +0100
Subject: [R] autoscaling plot font size in Sweave output possible?
References: <m37jmqupy8.fsf@pdrechsler.fqdn.th-h.de>
	<16862.19712.400719.16915@celebrian.ci.tuwien.ac.at>
Message-ID: <m3llb44oe5.fsf@pdrechsler.fqdn.th-h.de>

Hi Friedrich,

Friedrich Leisch wrote on 07 Jan 2005 09:49:04 MET:

thanks for taking the time!

>>>>>> On Thu, 06 Jan 2005 23:56:31 +0100,
>>>>>> Patrick Drechsler (PD) wrote:

>> I was wondering if it's possible to have fonts in plots to be
>> autoscaled to the same font size used by LaTeX in a
>> surrounding Sweave document.
>
> Not using the standard mechanism, because there figures are
> rescaled *after* they are created, and the font is rescaled
> together with the rest.

..I was afraid of that...

> What yoy have to do is:
>
> 1) Create your own Sweave.sty file
[...]

Ok...

> 2) Explicitly set height and width of each figure chunk to what
> it should be in the final document. Unfortunately you cannot
> use any fractions of \textwidth or the like because Sweawe has
> no means to know what that might be.

see below...

> 3) Set the font size to what you use in the tex document using
> ps.options().

see below...

Could you or somebody else enhance my OP in this sence to
demonstrate the effect intended? I've fiddled around with
`postscrict' and `ps.options' all day long but haven't reached a
working file. The problem is most likely sitting in front of the
keyboard.

> It is because of 2) that I didn't follow this route for the
> current defaults, and I haven't found a really convincing
> alternative which works in "most" situations ...

Not knowing if it's actually going to help me: Can you point me
to the file which does the conversion to the *.tex file? I've
tried `locate sweave' on my linux box and there wasn't anything
in the results concerning the conversion to TeX. 

I'm one of those guys that wants to have nice looking results at
the end with all plots having the same layout. I'm biased by the
following tools at the moment:

1. PSTricks (within LaTeX; low level; good for publications)

2. export from Matlab7.0.1 -> (e)ps for LaTeX (via PSFrag
replacement of fonts / publishable) [1] 

3a. normal export from R -> (e)ps for LaTeX via library Hmisc
(PSFrag: font replacement untested / publishable?)

3b. export from R "sweaved" into LaTeX -> Sweave and problem
above. I use this option at least once a month to feedback-myself
of current results and very much enjoy the feature of not having
to fiddle-fart around with layouts. And having the `\Sexpr'
available.... indespensable. I can look at my results months
later and know what I've done.


1==3a : fine for high-end publishing.

2 and 3b : is a good routine.

Combining 3b with 1 would be nice.


Ideas?

Patrick

Footnotes: 
[1] <URL:http://www.mathworks.com/matlabcentral/fileexchange/loadFile.do?objectId=4638&objectType=file>

-- 
For animals, the entire universe has been neatly divided into things
to (a) mate with, (b) eat, (c) run away from, and (d) rocks.
        -- (Terry Pratchett, Equal Rites)



From mittime at gmail.com  Sat Jan  8 05:56:46 2005
From: mittime at gmail.com (a j)
Date: Fri, 7 Jan 2005 23:56:46 -0500
Subject: [R] problem with up arrow
Message-ID: <7ee550d60501072056a45cad9@mail.gmail.com>

Hi,

I just compiled R on solaris.

Everything seems to run fine.  

Except, the up and down arrows don't take me
through the history.  See:

=======================================================
~/R-2.0.1/bin: R

R : Copyright 2004, The R Foundation for Statistical Computing
Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for a HTML browser interface to help.
Type 'q()' to quit R.

> x=c(2,3,4)
> ^[[A
Error: syntax error
> ^[[B
Error: syntax error
> 
=======================================================

The ^[[A is my up arrow keypress
and the ^[[B is my down arrow keypress.


How can I get these arrows to take me through history?



From edd at debian.org  Sat Jan  8 06:08:23 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 7 Jan 2005 23:08:23 -0600
Subject: [R] problem with up arrow
In-Reply-To: <7ee550d60501072056a45cad9@mail.gmail.com>
References: <7ee550d60501072056a45cad9@mail.gmail.com>
Message-ID: <20050108050822.GA2895@sonny.eddelbuettel.com>

On Fri, Jan 07, 2005 at 11:56:46PM -0500, a j wrote:
> I just compiled R on solaris.
> 
> Everything seems to run fine.  
> 
> Except, the up and down arrows don't take me
> through the history.  See:
[...]

> How can I get these arrows to take me through history?

It's a FAQ -- you need readline support. Check your build logs, you should
see something like this (which is from the Debian builds I maintain /
coordinate, you'd obviously get something other than i386, though):

R is now configured for i386-pc-linux-gnu

  Source directory:          .
  Installation directory:    /usr
    
  C compiler:                gcc  -g -O2
  C++ compiler:              g++  -g -O2
  Fortran compiler:          g77  -g -O2
	  
  Interfaces supported:      X11, gnome, tcltk
  External libraries:        readline, BLAS(generic)
  Additional capabilities:   PNG, JPEG
  Options enabled:           shared library, R profiling
		  
  Recommended packages:      no
		    

Note the 'readline' after external libraries. You need it for the "up-arrow".

Dirk

-- 
If you don't go with R now, you will someday.
  -- David Kane on r-sig-finance, 30 Nov 2004



From ripley at stats.ox.ac.uk  Sat Jan  8 08:33:13 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 8 Jan 2005 07:33:13 +0000 (GMT)
Subject: [R] Compilation of R code
In-Reply-To: <1105145233.6298.7.camel@horizons.localdomain>
References: <20050108000023.46622.qmail@web53407.mail.yahoo.com>
	<1105145233.6298.7.camel@horizons.localdomain>
Message-ID: <Pine.LNX.4.61.0501080727330.28066@gannet.stats>

On Fri, 7 Jan 2005, Marc Schwartz wrote:

> On Fri, 2005-01-07 at 16:00 -0800, Sameul M Mwalili wrote:
>> Dear ALL,
>> In order to install the Rice R to C compiler (RCC) you need to patch
>> the R source code. However, the patch command at DOS prompt returns an
>> error:  patch is not recognized as internal or external command. How
>> do you patch in DOS (or Windows)?
>>
>> Regards,
>>
>> Samuel.
>
> The GNU patch program is available as source code via:
>
> http://www.fsf.org/software/patch/patch.html
>
> I also located a Sourceforge project that has pre-built native Win32
> binaries, which requires msvcrt.dll:
>
> http://unxutils.sourceforge.net/
>
> There may be other resources to reference.

Most of us use Cygwin (http://www.cygwin.com/).  Gnuwin32 is also a good 
source (a bigger and more active project than unxutils):

http://gnuwin32.sourceforge.net/

and their latest announcement is patch.

Warning: the RCC patches are R-version specific, and the last available 
appears to be 1.9.0.

Warning2: Some Windows versions of patch require CRLF files, and the R 
sources are LF files when unpacked by tar.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Jan  8 08:41:08 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 8 Jan 2005 07:41:08 +0000 (GMT)
Subject: [R] problem with up arrow
In-Reply-To: <20050108050822.GA2895@sonny.eddelbuettel.com>
References: <7ee550d60501072056a45cad9@mail.gmail.com>
	<20050108050822.GA2895@sonny.eddelbuettel.com>
Message-ID: <Pine.LNX.4.61.0501080734050.28066@gannet.stats>

On Fri, 7 Jan 2005, Dirk Eddelbuettel wrote:

> On Fri, Jan 07, 2005 at 11:56:46PM -0500, a j wrote:
>> I just compiled R on solaris.
>>
>> Everything seems to run fine.
>>
>> Except, the up and down arrows don't take me
>> through the history.  See:
> [...]
>
>> How can I get these arrows to take me through history?
>
> It's a FAQ -- you need readline support. Check your build logs, you should
> see something like this (which is from the Debian builds I maintain /
> coordinate, you'd obviously get something other than i386, though):
>
> R is now configured for i386-pc-linux-gnu
>
>  Source directory:          .
>  Installation directory:    /usr
>
>  C compiler:                gcc  -g -O2
>  C++ compiler:              g++  -g -O2
>  Fortran compiler:          g77  -g -O2
>
>  Interfaces supported:      X11, gnome, tcltk
>  External libraries:        readline, BLAS(generic)
>  Additional capabilities:   PNG, JPEG
>  Options enabled:           shared library, R profiling
>
>  Recommended packages:      no
>
>
> Note the 'readline' after external libraries. You need it for the "up-arrow".

Two notes:

1) As from 2.1.0, you will need to explicitly deselect readline to allow 
configure to work without giving you readline.  (Same for X.)  Hopefully 
that will reduce the `F' of this FAQ.

2) Strictly, you need the GNU History library and not just readline, which 
nowadays incorporates it.  So, to quote the R-admin manual, `you will need 
a fairly recent version'.  At least as of R 2.0.1 you do need GNU readline 
and not, say, NetBSD readline (which is what is expected to come with 
MacOS 10.4 aka Tiger).

Finally, a plug: we do have an `Installation and Administration' manual, 
and a file INSTALL which asks you to read it.  Please review it if you 
have a problem with installation.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Friedrich.Leisch at tuwien.ac.at  Sat Jan  8 11:52:41 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Sat, 8 Jan 2005 11:52:41 +0100
Subject: [R] autoscaling plot font size in Sweave output possible?
In-Reply-To: <m3llb44oe5.fsf@pdrechsler.fqdn.th-h.de>
References: <m37jmqupy8.fsf@pdrechsler.fqdn.th-h.de>
	<16862.19712.400719.16915@celebrian.ci.tuwien.ac.at>
	<m3llb44oe5.fsf@pdrechsler.fqdn.th-h.de>
Message-ID: <16863.47993.282910.806214@celebrian.ci.tuwien.ac.at>

>>>>> On Sat, 08 Jan 2005 03:59:14 +0100,
>>>>> Patrick Drechsler (PD) wrote:

  > Hi Friedrich,
  > Friedrich Leisch wrote on 07 Jan 2005 09:49:04 MET:

  > thanks for taking the time!

  >>>>>>> On Thu, 06 Jan 2005 23:56:31 +0100,
  >>>>>>> Patrick Drechsler (PD) wrote:

  >>> I was wondering if it's possible to have fonts in plots to be
  >>> autoscaled to the same font size used by LaTeX in a
  >>> surrounding Sweave document.
  >> 
  >> Not using the standard mechanism, because there figures are
  >> rescaled *after* they are created, and the font is rescaled
  >> together with the rest.

  > ..I was afraid of that...

  >> What yoy have to do is:
  >> 
  >> 1) Create your own Sweave.sty file
  > [...]

  > Ok...

  >> 2) Explicitly set height and width of each figure chunk to what
  >> it should be in the final document. Unfortunately you cannot
  >> use any fractions of \textwidth or the like because Sweawe has
  >> no means to know what that might be.

  > see below...

  >> 3) Set the font size to what you use in the tex document using
  >> ps.options().

  > see below...

  > Could you or somebody else enhance my OP in this sence to
  > demonstrate the effect intended? I've fiddled around with
  > `postscrict' and `ps.options' all day long but haven't reached a
  > working file. The problem is most likely sitting in front of the
  > keyboard.

  >> It is because of 2) that I didn't follow this route for the
  >> current defaults, and I haven't found a really convincing
  >> alternative which works in "most" situations ...

  > Not knowing if it's actually going to help me: Can you point me
  > to the file which does the conversion to the *.tex file? I've
  > tried `locate sweave' on my linux box and there wasn't anything
  > in the results concerning the conversion to TeX.

???

Sweave is part of package utils (which the help page clearly
indicates), the particular driver for Latex is called RweaveLatex and
referenced in the Sweave help page. If you want to have a look at the
source code the easiest thing is to get the R sources and browse in
the sources of package utils ... I'll leave it to you to figure out
which of the files contains the Sweave R code.


  > I'm one of those guys that wants to have nice looking results at
  > the end with all plots having the same layout.

To me it sounds much more like you're one of those guys who like it
when others solve their problems for free. I gave you an IMO perfectly
working cookbook recipe and you didn't even bother to tell me what did
not work out for you, only a "see below" to completely different
things (export from MATLAB isn't exactly helping me in guessing what
went wrong for you).

As christmas present (mostly to other who bother to follow the thread)
attached a solution that has exactly the same font in plots and normal
text (and was obtained by simply following the recipe I gave in my
earlier email step by step).

Best,
Fritz


* file.Rnw *********************************************************

\documentclass{article}

\newcommand{\mytext}{some x label --- some y label --- some x label
  --- some y label}

\usepackage{MySweave}
\usepackage{times}

\begin{document}

\mytext

<<results=hide,echo=false>>=
ps.options(pointsize=10,family="Times")

<<test0,echo=F,fig=T,width=2,height=2>>=
x <- 1:10
y <- sin(x)
plot(x,y,
     xlab="some x label",
     ylab="some y label"
     )
@

\mytext


<<test1,echo=F,fig=T,width=4,height=4>>=
plot(x,y,
     xlab="some x label",
     ylab="some y label"
     )
@ %def 

\mytext

\end{document}


* MySweave.sty *******************************************

\RequirePackage[T1]{fontenc}
\RequirePackage{graphicx,ae,fancyvrb}
\IfFileExists{upquote.sty}{\RequirePackage{upquote}}{}

\DefineVerbatimEnvironment{Sinput}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{}
\DefineVerbatimEnvironment{Scode}{Verbatim}{fontshape=sl}

\newenvironment{Schunk}{}{}



From anne.piotet at urbanet.ch  Sat Jan  8 14:04:54 2005
From: anne.piotet at urbanet.ch (Anne)
Date: Sat, 8 Jan 2005 14:04:54 +0100
Subject: [R] help with polytomous logistic regression
References: <LKEBJIDNILAPLELOPBEEOEDLCOAA.andersr@mcmaster.ca>
Message-ID: <002801c4f582$a7259c30$6c00a8c0@mtd4>

Thank you all for your help!
I tried successfully the Anova() procedure in car packagea and now am woking
on John Fox solution to display effects...

 Anne


----- Original Message ----- 
From: "Robert Andersen" <andersr at mcmaster.ca>
To: "Anne" <anne.piotet at urbanet.ch>
Sent: Friday, January 07, 2005 6:43 PM
Subject: RE: [R] help with polytomous logistic regression


> Dear Anne,
> I'm not sure, but I think the "anova" (which only allows you to compare
> multinomial models that were fit separately) and "Anova" (you only need to
> fit one model to get type II tests) functions John refers to below are not
> functional until R version 2.0 and later.
> Bob
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Anne
> > Sent: Friday, January 07, 2005 10:20 AM
> > To: John Fox
> > Cc: 'R list'
> > Subject: Re: [R] help with polytomous logistic regression
> >
> >
> > Thank you, John! it is exactly what I need...looking forward to apply it
> >
> > Anne
> >
> > what would I become without this list?
> >
> >
> > ----- Original Message -----
> > From: "John Fox" <jfox at mcmaster.ca>
> > To: "'Anne'" <anne.piotet at urbanet.ch>
> > Cc: "'R list'" <r-help at stat.math.ethz.ch>
> > Sent: Friday, January 07, 2005 3:56 PM
> > Subject: RE: [R] help with polytomous logistic regression
> >
> >
> > > Dear Anne,
> > >
> > > There's an anova() method for multinom objects that will allow
> > you to get
> > > likelihood-ratio tests. Likewise, the Anova() function in the
> > car package
> > > has a method for multinom objects.
> > >
> > > As to plotting, I have a paper, coauthored with Bob Andersen,
> > on producing
> > > "effect" plots (partial plots) for polytomous logistic regression,
> > including
> > > multinomial and proportional-odds logistic regression. The paper, at
> > >
> > <http://socserv.socsci.mcmaster.ca/jfox/logit-effect-displays.pdf>, has
an
> > R
> > > function for computing such effects, including standard errors.
> > There are
> > > examples in the paper as well. Eventually, I intend to incorporate
this
> > into
> > > the effects package to make the graphs more or less automatic.
> > >
> > > Regards,
> > >  John
> > >
> > > --------------------------------
> > > John Fox
> > > Department of Sociology
> > > McMaster University
> > > Hamilton, Ontario
> > > Canada L8S 4M4
> > > 905-525-9140x23604
> > > http://socserv.mcmaster.ca/jfox
> > > --------------------------------
> > >
> > > > -----Original Message-----
> > > > From: r-help-bounces at stat.math.ethz.ch
> > > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anne
> > > > Sent: Friday, January 07, 2005 8:53 AM
> > > > To: R list
> > > > Subject: [R] help with polytomous logistic regression
> > > >
> > > > Hi!
> > > >
> > > > I'm trying to do some ploytomous logistic regression using
> > > > multinom() in the nnet package, but am a bit confused about
> > > > interpretation of the results Is it possible to get the
> > > > following quantities:
> > > >
> > > > I:  maximum likelihood estimates to test for fit of model and
> > > > significance of each predictor
> > > >
> > > > (I would like to produce a table of the following type)
> > > > Analysis of Variance: MLE    (values are non sensical, I know!)
> > > >                df      chi2           p
> > > > intercept   2       57            .003
> > > > v1            4       89            .876
> > > > v2            2         7            .05
> > > > LR        110     450            0.93
> > > >
> > > > II:  chi square values and corresponding p-values for each
> > > > level of the predictor variates (I'm OK for the estimates and
> > > > se from summary,)
> > > >
> > > >
> > > >                level estimate     se        chi2         p
> > > > intercept   1        5              .9         32          .002
> > > >                 2       6.3            .8         31          .03
> > > > v1             3       89            .876       43          .001
> > > >                 4       65            .05         67
> > > > 0.001
> > > >  v2...
> > > >
> > > >
> > > > Is there a convenient way to plot the results? (I'd like to
> > > > display visually the effects of the predictors: any sugestion?)
> > > >
> > > > OK, I probably miss something here (no experience with (non
> > > > ordinal) polytomous logistic regression yet!)
> > > >
> > > > Thank  very much to all
> > > >
> > > > Anne
> > > >
> > > > ----------------------------------------------------
> > > > Anne Piotet
> > > > Tel: +41 79 359 83 32 (mobile)
> > > > Email: anne.piotet at m-td.com
> > > > ---------------------------------------------------
> > > > M-TD Modelling and Technology Development PSE-C
> > > > CH-1015 Lausanne
> > > > Switzerland
> > > > Tel: +41 21 693 83 98
> > > > Fax: +41 21 646 41 33
> > > > --------------------------------------------------
> > > >
> > > > [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > > > http://www.R-project.org/posting-guide.html
> > >
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
>



From bates at wisc.edu  Sat Jan  8 15:14:08 2005
From: bates at wisc.edu (Douglas Bates)
Date: Sat, 08 Jan 2005 08:14:08 -0600
Subject: [R] Help in customising the NLS function to spit out mean and
	SD	ofnew fit!!
In-Reply-To: <200501072004.j07K4S52012415@volta.gene.com>
References: <200501072004.j07K4S52012415@volta.gene.com>
Message-ID: <41DFEAB0.5020405@wisc.edu>

Berton Gunter wrote:
> It **sounds ** like you are trying to fit a nonparametric density to 6000
> values...  If so, please stop what you are doing and see ?density. You could
> also search on "fit density" or something similar on the R site search, as
> there are other R functions in R packages that do density fitting (ash is
> one, but other recommendations anyone?).
> 
> If this is not what you are trying to do, I think you are still likely going
> about it in the wrong way -- histograms lose information and are notoriously
> dependent on the choice of cutpoints (which is part of the motivation for
> Scott's ash package). You might wish to consult a local statistician to get
> some better approaches to whatever it is that you're trying to do.
> 
> Cheers,
> Bert Gunter
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jagarlamudi, Choudary
> Sent: Friday, January 07, 2005 10:50 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Help in customising the NLS function to spit out mean and SD
> ofnew fit!!
> 
>  i'm coding in  R(first time) for a paper my colleague is publishing.
>  i plotted a histogram for 6000 values. Taking the mean and sd of the
> midpoints i did a dnorm and got the densities.
>  pl<-dnorm(trimmedvals,mean=midsmean,sd=midsSD)
>  (now in a loop of 5 times)
>  i plotted these experimental values against theoretical values using
>  the nls function the following way.
>  nlsresult<-nlsModel(pl ~ 1/sqrt(2
> *pi)*std)*2.71828^2/(2*std4^2)),data=answer,start=list(std4=std4,sm=sm))
>  in the formula mean was mid point of 1st frequency bar and sd was sd  of
> 6000 values.I do this 5 times each time changing the mean in the formula to
> be the mid point of the next frequency bar.
>  now i plotted plot(fittedvals ~trimmedvals)
>  
> I am told to plot experimental vs theoretical vlaues from the  histogram and
> get a non linear least square curve fit.
>  I need the mean and sd of this new fit to proceed to my next  module.I'm
> not sure if i'm on track. Excuse me if this sounds too  naive.If nls is not
> what i should be using can you please give me some  pinters to solve this.

Also, it is very unusual to call nlsModel directly.  I think that if you 
did want to use nonlinear least squares to fit these data then you 
should call nls, not nlsModel.



From ckjmaner at carolina.rr.com  Sat Jan  8 17:53:57 2005
From: ckjmaner at carolina.rr.com (Charles and Kimberly Maner)
Date: Sat, 8 Jan 2005 11:53:57 -0500
Subject: [R] R packages on Mac
Message-ID: <200501081652.j08GqiCi006390@ms-smtp-04-eri0.southeast.rr.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050108/168c0958/attachment.pl

From mark0060 at tc.umn.edu  Sat Jan  8 19:28:00 2005
From: mark0060 at tc.umn.edu (Kristian Eric Markon)
Date: Sat, 08 Jan 2005 12:28:00 -0600
Subject: [R] suse 9.1 x86_64 rpms?
Message-ID: <41E02630.5000908@tc.umn.edu>

I noticed on the mailing list archives and through Google searches that 
a couple of months ago there was discussion of maintaining rpms for suse 
9.1 on the x86_64 architecture.

Are these rpms still planning on being released? It would help me a 
great deal.

I've tried compiling from source using the R-base spec files provided on 
CRAN. Using that, I am able to produce a running version of R, but 
readline doesn't seem to be working, nor does x11(). Moreover, I'm not 
sure that the compiled version is actually running in 64-bit mode. I've 
double-checked all the required libraries, and I seem to have them all 
to the correct version number (including readline and xfree).

The 32-bit rpms run on a 32-bit install of suse 9.1 on a different 
machine, but not on the 64-bit install of suse 9.1 on a x86_64 machine. 
I suspect some of my problems have to do with libraries being in 
different places in the 64-bit version of suse 9.1.

I could spend more time trying to compile it on my machine--and probably 
will, but I thought I would ask about any possible official distribution 
of rpms for suse 9.1 x86_64 first.

Thanks.



From MSchwartz at MedAnalytics.com  Sat Jan  8 20:19:29 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sat, 08 Jan 2005 13:19:29 -0600
Subject: [R] suse 9.1 x86_64 rpms?
In-Reply-To: <41E02630.5000908@tc.umn.edu>
References: <41E02630.5000908@tc.umn.edu>
Message-ID: <1105211970.21405.21.camel@horizons.localdomain>

On Sat, 2005-01-08 at 12:28 -0600, Kristian Eric Markon wrote:
> I noticed on the mailing list archives and through Google searches that 
> a couple of months ago there was discussion of maintaining rpms for suse 
> 9.1 on the x86_64 architecture.
> 
> Are these rpms still planning on being released? It would help me a 
> great deal.
> 
> I've tried compiling from source using the R-base spec files provided on 
> CRAN. 

You do not need the spec files if you are compiling from source, only if
you want to build an RPM.

In the latter case, from a search of the archives, there are comments
regarding some fine tuning of the default spec files needed for 64-bit.

> Using that, I am able to produce a running version of R, but 
> readline doesn't seem to be working, nor does x11(). Moreover, I'm not 
> sure that the compiled version is actually running in 64-bit mode. I've 
> double-checked all the required libraries, and I seem to have them all 
> to the correct version number (including readline and xfree).

More than likely, depending upon what you have actually done with
respect to compiling, you are missing the 'devel' versions of readline
and XFree86.

You might want to check to see if you have readline-devel and XFree86-
devel installed. These will be required to compile from source:

rpm -q readline-devel
rpm -q XFree86-devel

If nothing is returned from the above in a console, you need to install
them.

> The 32-bit rpms run on a 32-bit install of suse 9.1 on a different 
> machine, but not on the 64-bit install of suse 9.1 on a x86_64 machine. 
> I suspect some of my problems have to do with libraries being in 
> different places in the 64-bit version of suse 9.1.

Possible, but not likely, as I suspect others running 64 bit SuSE would
have reported issues and posts in the archive suggest that the
normal ./configure, make, make install sequence works.

The key is to make sure that the devel versions of the required
libraries are installed and that the full tool chain is 64 bit.

> I could spend more time trying to compile it on my machine--and probably 
> will, but I thought I would ask about any possible official distribution 
> of rpms for suse 9.1 x86_64 first.
> 
> Thanks.

Try the above first. When you run ./configure, pay attention to the
output, which should look something like the following:

R is now configured for <x86_64 version of linux>

  Source directory:          .
  Installation directory:    /usr/local

  C compiler:                gcc  -g -O2
  C++ compiler:              g++  -g -O2
  Fortran compiler:          g77  -g -O2

  Interfaces supported:      X11, tcltk
  External libraries:        readline, BLAS(generic)
  Additional capabilities:   PNG, JPEG
  Options enabled:           R profiling

  Recommended packages:      yes

Note the lines labeled "Interfaces supported" and "External libraries"
indicating that both X11 and readline are present. If these are missing,
it reinforces the lack of the devel packages.

BTW, greetings from Eden Prairie...  :-)

HTH,

Marc Schwartz



From p.dalgaard at biostat.ku.dk  Sat Jan  8 20:22:14 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Jan 2005 20:22:14 +0100
Subject: [R] suse 9.1 x86_64 rpms?
In-Reply-To: <41E02630.5000908@tc.umn.edu>
References: <41E02630.5000908@tc.umn.edu>
Message-ID: <x2d5wfpvyx.fsf@biostat.ku.dk>

Kristian Eric Markon <mark0060 at tc.umn.edu> writes:

> I noticed on the mailing list archives and through Google searches
> that a couple of months ago there was discussion of maintaining rpms
> for suse 9.1 on the x86_64 architecture.
> 
> Are these rpms still planning on being released? It would help me a
> great deal.

Detlef Steuer is the only one to know...
 
> I've tried compiling from source using the R-base spec files provided
> on CRAN. Using that, I am able to produce a running version of R, but
> readline doesn't seem to be working, nor does x11(). Moreover, I'm not
> sure that the compiled version is actually running in 64-bit mode.
> I've double-checked all the required libraries, and I seem to have
> them all to the correct version number (including readline and xfree).

It'll be 64-bit alright unless you take explicit steps to the contrary.
If you want to make d*mn sure, calculate the size of an Ncell from the
gc() output (56 bytes on 64bit 28 bytes otherwise).

Compiling on SuSE 9.1 *does* work, but you seem to be missing a couple
of RPMs in your installation. Usual suspects are the ones that end
with -devel, such as

alsa-devel-1.0.3-36
db1-devel-1.85-78
esound-devel-0.2.33-30
fontconfig-devel-2.2.92.20040221-24
freetype2-devel-2.1.7-46
glibc-devel-2.3.3-63
glib-devel-1.2.10-337
gnome-libs-devel-1.4.1.7-614
gtk-devel-1.2.10-488
ImageMagick-devel-5.5.7-225.9
imlib-devel-1.9.14-180.11
libglade2-devel-2.0.1-437
libglade-devel-0.17-190
libpng-devel-1.2.5-182.10
libstdc++-devel-3.3.3-33
libxml2-devel-2.6.7-28.7
libxml-devel-1.8.17-366.4
mozilla-devel-1.6-53
ncurses-devel-5.4-61.3
orbit-devel-0.5.17-330
pstoedit-devel-3.33-161
readline-devel-4.3-301
tcl-devel-8.4.6-23
tk-devel-8.4.6-28
xforms-devel-1.0-259
XFree86-devel-32bit-9.1-200404070910
XFree86-devel-4.3.99.902-30
XFree86-Mesa-devel-4.3.99.902-30
zlib-devel-1.2.1-70.6

Those are what I have, some are irrelevant, but at least the readline,
ncurses, XFree86, and tcl/tk are crucial for compiling a "normal" R,
and several of the others are needed for Gnome support, the Mesa stuff
for RGL, etc. (Hmm... looks like I'm actually missing the bzip2
headers myself).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From andy_liaw at merck.com  Sat Jan  8 20:33:41 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 8 Jan 2005 14:33:41 -0500
Subject: [R] suse 9.1 x86_64 rpms?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4D9@usrymx25.merck.com>

I just compiled R from source on SLES9-amd64.  I had to install the
following, which are not installed by default:

`devel' part of readline and xfree86
libpng

(I also installed Goto's BLAS, which, obviously, has no rpm.)

After that, you just need to make sure of adding -m64 (and maybe -march=k8)
to CFLAGS/FFLAGS/CXXFLAGS.  Then R will compile as 64-bit with all the
things you want.

I believe an RPM would only help installing R itself.  You still need most
of those rpm's if you are going to install add-on packages from source
(which you almost certainly need to, as pre-built binary packages are
available only for Windows/Mac/Debian).

Andy

> From: Kristian Eric Markon
> 
> I noticed on the mailing list archives and through Google 
> searches that 
> a couple of months ago there was discussion of maintaining 
> rpms for suse 
> 9.1 on the x86_64 architecture.
> 
> Are these rpms still planning on being released? It would help me a 
> great deal.
> 
> I've tried compiling from source using the R-base spec files 
> provided on 
> CRAN. Using that, I am able to produce a running version of R, but 
> readline doesn't seem to be working, nor does x11(). 
> Moreover, I'm not 
> sure that the compiled version is actually running in 64-bit 
> mode. I've 
> double-checked all the required libraries, and I seem to have 
> them all 
> to the correct version number (including readline and xfree).
> 
> The 32-bit rpms run on a 32-bit install of suse 9.1 on a different 
> machine, but not on the 64-bit install of suse 9.1 on a 
> x86_64 machine. 
> I suspect some of my problems have to do with libraries being in 
> different places in the 64-bit version of suse 9.1.
> 
> I could spend more time trying to compile it on my 
> machine--and probably 
> will, but I thought I would ask about any possible official 
> distribution 
> of rpms for suse 9.1 x86_64 first.
> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Sat Jan  8 20:52:10 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 8 Jan 2005 14:52:10 -0500
Subject: [R] suse 9.1 x86_64 rpms?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4DB@usrymx25.merck.com>

> From: Peter Dalgaard
> 
> Kristian Eric Markon <mark0060 at tc.umn.edu> writes:
[snip] 
> Moreover, I'm not
> > sure that the compiled version is actually running in 64-bit mode.
> > I've double-checked all the required libraries, and I seem to have
> > them all to the correct version number (including readline 
> and xfree).
> 
> It'll be 64-bit alright unless you take explicit steps to the 
> contrary.
> If you want to make d*mn sure, calculate the size of an Ncell from the
> gc() output (56 bytes on 64bit 28 bytes otherwise).

This might be a bit more straightforward?

~/R-2.0.1> file bin/exec/R
bin/exec/R: ELF 64-bit LSB executable, AMD x86-64, version 1 (SYSV), for
GNU/Linux 2.4.1, dynamically linked (uses shared libs), not stripped

Andy



From fredrik.bg.lundgren at bredband.net  Sat Jan  8 20:57:34 2005
From: fredrik.bg.lundgren at bredband.net (Fredrik Lundgren)
Date: Sat, 8 Jan 2005 20:57:34 +0100
Subject: [R] translate nroff .d or .sgml files to .rd files
Message-ID: <000701c4f5bc$4be0d8a0$629d72d5@Larissa>

Dear list,

I have some help file for S-Plus 2000 in .d and .sgml format. Is there a 
way to translate these files to .Rd files to use in an R package?

Thanks Fredrik Lundgren



From tlumley at u.washington.edu  Sat Jan  8 21:10:54 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sat, 8 Jan 2005 12:10:54 -0800 (PST)
Subject: [R] suse 9.1 x86_64 rpms?
In-Reply-To: <x2d5wfpvyx.fsf@biostat.ku.dk>
References: <41E02630.5000908@tc.umn.edu> <x2d5wfpvyx.fsf@biostat.ku.dk>
Message-ID: <Pine.A41.4.61b.0501081209500.95586@homer12.u.washington.edu>

On Sat, 8 Jan 2005, Peter Dalgaard wrote:

> It'll be 64-bit alright unless you take explicit steps to the contrary.
> If you want to make d*mn sure, calculate the size of an Ncell from the
> gc() output (56 bytes on 64bit 28 bytes otherwise).

Or
   .Machine$sizeof.pointer
gives the size of SEXP

 	-thomas



From andy_liaw at merck.com  Sat Jan  8 21:23:26 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 8 Jan 2005 15:23:26 -0500
Subject: [R] translate nroff .d or .sgml files to .rd files
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4DD@usrymx25.merck.com>

On *nix:

~> R CMD Sd2Rd --help
Usage: R CMD Sd2Rd [options] FILE

Convert S documentation in FILE to R documentation format.

Options:
  -h, --help            print short help message and exit
  -v, --version         print Sd2Rd version info and exit
  -n                    render examples non-executable by wrapping them
                        into a \dontrun{} environment
  -x                    (S3 docs) interpret all single-quoted names
                        as code names

Email bug reports to <r-bugs at r-project.org>.

Don't know how well it might work for your situation, but probably worth a
try.

HTH,
Andy

> From: Fredrik Lundgren
> 
> Dear list,
> 
> I have some help file for S-Plus 2000 in .d and .sgml format. 
> Is there a 
> way to translate these files to .Rd files to use in an R package?
> 
> Thanks Fredrik Lundgren
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From p.dalgaard at biostat.ku.dk  Sat Jan  8 21:22:21 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Jan 2005 21:22:21 +0100
Subject: [R] translate nroff .d or .sgml files to .rd files
In-Reply-To: <000701c4f5bc$4be0d8a0$629d72d5@Larissa>
References: <000701c4f5bc$4be0d8a0$629d72d5@Larissa>
Message-ID: <x28y73pt6q.fsf@biostat.ku.dk>

"Fredrik Lundgren" <fredrik.bg.lundgren at bredband.net> writes:

> Dear list,
> 
> I have some help file for S-Plus 2000 in .d and .sgml format. Is there
> a way to translate these files to .Rd files to use in an R package?
> 
> Thanks Fredrik Lundgren

That's what

  R CMD Sd2Rd

is for. Not sure how/if it works on Windows, though. It's a Perl
script, so presumably you need Perl...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From fredrik.bg.lundgren at bredband.net  Sat Jan  8 22:12:33 2005
From: fredrik.bg.lundgren at bredband.net (Fredrik Lundgren)
Date: Sat, 8 Jan 2005 22:12:33 +0100
Subject: [R] translate nroff .d or .sgml files to .rd files
References: <000701c4f5bc$4be0d8a0$629d72d5@Larissa>
	<x28y73pt6q.fsf@biostat.ku.dk>
Message-ID: <001d01c4f5c6$c5917330$629d72d5@Larissa>

Thanks,
works OK for Win XP and R 2.0.1
with
R CMD Sd2Rd *.d > *.Rd

Fredrik
----- Original Message ----- 
From: "Peter Dalgaard" <p.dalgaard at biostat.ku.dk>
To: "Fredrik Lundgren" <fredrik.bg.lundgren at bredband.net>
Cc: "R-help" <r-help at stat.math.ethz.ch>
Sent: Saturday, January 08, 2005 9:22 PM
Subject: Re: [R] translate nroff .d or .sgml files to .rd files


> "Fredrik Lundgren" <fredrik.bg.lundgren at bredband.net> writes:
>
>> Dear list,
>>
>> I have some help file for S-Plus 2000 in .d and .sgml format. Is 
>> there
>> a way to translate these files to .Rd files to use in an R package?
>>
>> Thanks Fredrik Lundgren
>
> That's what
>
>  R CMD Sd2Rd
>
> is for. Not sure how/if it works on Windows, though. It's a Perl
> script, so presumably you need Perl...
>
> -- 
>   O__  ---- Peter Dalgaard             Blegdamsvej 3
>  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
> (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 
> 35327907
>



From choudary.jagar at swosu.edu  Sat Jan  8 22:24:11 2005
From: choudary.jagar at swosu.edu (Jagarlamudi, Choudary)
Date: Sat, 8 Jan 2005 15:24:11 -0600
Subject: [R] Least square minimization (non-linear)
Message-ID: <E03EBB50FF2C024781A6E4460AD58F0607C140@swosu-mbx01.admin.swosu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050108/a46228f6/attachment.pl

From Robert.McGehee at geodecapital.com  Sat Jan  8 22:24:49 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Sat, 8 Jan 2005 16:24:49 -0500
Subject: [R] Destructor for S4 objects?
Message-ID: <67DCA285A2D7754280D3B8E88EB5480206741E01@MSGBOSCLB2WIN.DMN1.FMR.COM>

Adam, I'm about to encounter a very similar problem, so would be curious
if you find a good solution, although your thoughts with reg.finalizer
look promising. The thoughts I had sketched out for tackling this
problem:

1) Have all ancillary resources freed when the package is detached. The
new() function would create the object, allocate additional resources,
and store some kind of handle for these resources in the package
environment as a cleanup object. The cleanup object is then referenced
when .Last.lib() is run, and all ancillary resources are deallocated.

2) Write methods for rm() / remove() that cleanup the resources at the
same time as removing the object from the environment. A handle to these
resources would have to be stored either with the object like you
suggested, or in a cleanup object (like #1) that stores the handles of
every object needing cleanup. 

Conceptually, #2 seems very similar to your suggestion of using
reg.finalizer. However, it had seemed to make more sense (to me) to
deallocate resources at rm() time rather than gc() time, as I had
thought that garbage collection happened at R's convenience, rather than
when the user explicitly removes the object from the environment (which
is presumably garbage collected sometime later). However, any
information about the mechanics of removal and garbage collection, and
the correct way to cleanup objects is greatly appreciated, as I had
never quite understood when reg.finalizer should be used.

As a corollary, I have some concern that if R quits, then the garbage
collector never runs (and rm() and .Last.lib() as well). The resources
in my case are database objects that may need to be altered or deleted
after the corresponding R object is removed (or R quits), and I have yet
to find any solution that runs at quit time (although maybe rewriting
the q() function might not be a bad idea, although I'm hesitant to
rewrite base code). Let me know if you come up with anything more
elegant.

Best,
Robert

-----Original Message-----
From: Adam Lyon [mailto:lyon at fnal.gov] 
Sent: Friday, January 07, 2005 1:36 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Destructor for S4 objects?


Hi,

To write a "constructor" for an S4 object, you make an initialize method
which will be called by new. But how would I make a "destructor" method
to
be called when the S4 object is garbage collected? I'm looking at
reg.finalizer, but I'm not sure how to make that work for an S4 object.

I want to write a destructor because my S4 object's initialize method
allocates some resources. I would like the destructor to free them.

I guess I could make one of the slots in my object an environment that
holds
the handles for those resources and register that environment with the
finalizer. Then the finalizer function would release those resources. Is
that the way to do it or is there another simpler way?

Thanks for any help and Happy New Year!

Adam Lyon

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From HDoran at air.org  Sat Jan  8 22:38:31 2005
From: HDoran at air.org (Doran, Harold)
Date: Sat, 8 Jan 2005 16:38:31 -0500
Subject: [R] Does R accumulate memory
Message-ID: <88EAF3512A55DF46B06B1954AEF73F74072EA20F@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050108/b0223ac0/attachment.pl

From solares at unsl.edu.ar  Sat Jan  8 22:54:14 2005
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Sat, 8 Jan 2005 18:54:14 -0300 (ARGSL-ST)
Subject: [R] coordinates of mouse position
Message-ID: <20611.201.254.45.229.1105221254.squirrel@inter17.unsl.edu.ar>

HI, I need known the position of mouse, but i not know how obtain the
parameter %X used in tcl. the script in Tcl is:

set w .probe
catch {destroy $w}
toplevel $w
proc Captura_Datos { ancho alto } {
puts "El ancho es $ancho"
puts "El alto es $alto"
} bind $w <Configure> "Captura_Datos %X %Y"

In R
w<-tktoplevel()
f<-function(a,b)^`
print(a)
print(b)

tkbind(w,"<Configure>",f(%X,%Y))
Not works. How to do this? thanks Ruben



From murdoch at stats.uwo.ca  Sat Jan  8 23:36:47 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 08 Jan 2005 17:36:47 -0500
Subject: [R] Does R accumulate memory
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F74072EA20F@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F74072EA20F@dc1ex2.air.org>
Message-ID: <vdn0u09rnua2thsk84j16j0lr3lph20bo2@4ax.com>

On Sat, 8 Jan 2005 16:38:31 -0500, "Doran, Harold" <HDoran at air.org>
wrote:

>Dear List:
>
>I am running into a memory issue that I haven't noticed before. I am
>running a simulation with all of the code used below. I have increased
>my memory to 712mb and have a total of 1 gb on my machine.
>
>What appears to be happening is I run a simulation where I create 1,000
>datasets with a sample size of 100. I then run each dataset through a
>gls and obtain some estimates.
>
>This works fine. But, when I view how much memory is being used in
>Windows, I see that it does not reduce once the analysis is complete. As
>a result, I must quit R and then perform another analysis. 

If you ask Windows how much memory is being used, you'll likely get an
incorrect answer.  R may not release memory back to the OS, but it may
be available for re-use within R.

Call gc() to see how much memory R thinks is in use.

>So for example, before starting the 1st simulation, my windows task
>manager tells me I am using 200mb of memory. After running the first
>simulation it may go up to 500mb. I then try and run another simulation
>with a larger sample size, but I quickly run out of memory because it
>starts at 500 and increases from there and the simulation halts.

The difficulty you're running into may be memory fragmentation.  When
you run with a larger sample size, R will try to allocate larger
chunks than it did originally.  If the "holes" created when the
original simulation is deleted are too small, R will need to ask
Windows for new memory to store things in.

You could try deleting everything in your workspace before running the
2nd simulation; this should reduce the fragmentation.  Or you could
run the big simulation first, then the smaller one will fit in the
holes left from it.


Duncan Murdoch



From ripley at stats.ox.ac.uk  Sat Jan  8 23:37:38 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 8 Jan 2005 22:37:38 +0000 (GMT)
Subject: [R] Destructor for S4 objects?
In-Reply-To: <67DCA285A2D7754280D3B8E88EB5480206741E01@MSGBOSCLB2WIN.DMN1.FMR.COM>
References: <67DCA285A2D7754280D3B8E88EB5480206741E01@MSGBOSCLB2WIN.DMN1.FMR.COM>
Message-ID: <Pine.LNX.4.61.0501082232540.3540@gannet.stats>

On Sat, 8 Jan 2005, McGehee, Robert wrote:

> Adam, I'm about to encounter a very similar problem, so would be curious
> if you find a good solution, although your thoughts with reg.finalizer
> look promising. The thoughts I had sketched out for tackling this
> problem:
>
> 1) Have all ancillary resources freed when the package is detached. The
> new() function would create the object, allocate additional resources,
> and store some kind of handle for these resources in the package
> environment as a cleanup object. The cleanup object is then referenced
> when .Last.lib() is run, and all ancillary resources are deallocated.
>
> 2) Write methods for rm() / remove() that cleanup the resources at the
> same time as removing the object from the environment. A handle to these
> resources would have to be stored either with the object like you
> suggested, or in a cleanup object (like #1) that stores the handles of
> every object needing cleanup.
>
> Conceptually, #2 seems very similar to your suggestion of using
> reg.finalizer. However, it had seemed to make more sense (to me) to
> deallocate resources at rm() time rather than gc() time, as I had
> thought that garbage collection happened at R's convenience, rather than
> when the user explicitly removes the object from the environment (which
> is presumably garbage collected sometime later).

rm() does not remove the object, just mark it as no longer in use by 
removing the symbol.  If you want to release the storage (to R, at least), 
you need to call gc().

> However, any information about the mechanics of removal and garbage 
> collection, and the correct way to cleanup objects is greatly 
> appreciated, as I had never quite understood when reg.finalizer should 
> be used.
>
> As a corollary, I have some concern that if R quits, then the garbage
> collector never runs (and rm() and .Last.lib() as well). The resources

There are ways around that, as finalizers can be told to run then.
See the examples in the RODBC package.

OTOH, .Last.lib is run only when a package is explicitly detached, and not 
at the end of the session.

> in my case are database objects that may need to be altered or deleted
> after the corresponding R object is removed (or R quits), and I have yet
> to find any solution that runs at quit time (although maybe rewriting
> the q() function might not be a bad idea, although I'm hesitant to
> rewrite base code). Let me know if you come up with anything more
> elegant.

Quitting does not necssarily happen through q() (EOF is another way): but 
you can use .Last.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sat Jan  8 23:37:18 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Jan 2005 23:37:18 +0100
Subject: [R] coordinates of mouse position
In-Reply-To: <20611.201.254.45.229.1105221254.squirrel@inter17.unsl.edu.ar>
References: <20611.201.254.45.229.1105221254.squirrel@inter17.unsl.edu.ar>
Message-ID: <x2zmzjo8dd.fsf@biostat.ku.dk>

solares at unsl.edu.ar writes:

> HI, I need known the position of mouse, but i not know how obtain the
> parameter %X used in tcl. the script in Tcl is:
> 
> set w .probe
> catch {destroy $w}
> toplevel $w
> proc Captura_Datos { ancho alto } {
> puts "El ancho es $ancho"
> puts "El alto es $alto"
> } bind $w <Configure> "Captura_Datos %X %Y"
> 
> In R
> w<-tktoplevel()
> f<-function(a,b)^`
> print(a)
> print(b)
> 
> tkbind(w,"<Configure>",f(%X,%Y))
> Not works. How to do this? thanks Ruben

You need to pass a function which has arguments named X and Y. Check
the demos (notably tkcanvas). 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Sat Jan  8 23:45:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 8 Jan 2005 22:45:03 +0000 (GMT)
Subject: [R] Does R accumulate memory
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F74072EA20F@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F74072EA20F@dc1ex2.air.org>
Message-ID: <Pine.LNX.4.61.0501082239450.3540@gannet.stats>

One hint: R rarely releases memory to the OS, especially under Windows.
So do not expect to see the usage reported by Windows going down.

One possibility is that you are storing lots of results and not removing 
them.  You don't need to store all the gls fits, just the parts you need.

You can use gc(), memory.profile() and object.size() to see where memory 
is being used.

On Sat, 8 Jan 2005, Doran, Harold wrote:

> Dear List:
>
> I am running into a memory issue that I haven't noticed before. I am
> running a simulation with all of the code used below. I have increased
> my memory to 712mb and have a total of 1 gb on my machine.
>
> What appears to be happening is I run a simulation where I create 1,000
> datasets with a sample size of 100. I then run each dataset through a
> gls and obtain some estimates.
>
> This works fine. But, when I view how much memory is being used in
> Windows, I see that it does not reduce once the analysis is complete. As
> a result, I must quit R and then perform another analysis.
>
> So for example, before starting the 1st simulation, my windows task
> manager tells me I am using 200mb of memory. After running the first
> simulation it may go up to 500mb. I then try and run another simulation
> with a larger sample size, but I quickly run out of memory because it
> starts at 500 and increases from there and the simulation halts.
>
> So, it appears that R does not release memory after intense analyses,
> but is accumulated. Is this correct? If so, could this be due to
> inefficient code? Or, is this an issue specific to Windows? I didn't see
> this in the FAQ section on memory or in my searches on the web. I'm not
> sure how I can work more efficiently here.
>
> Thanks
> Harold
> R 2.0
> Windows XP
>
>
> #Housekeeping
> library(MASS)
> library(nlme)
> mu<-c(100,150,200,250)
> Sigma<-matrix(c(400,80,80,80,80,400,80,80,80,80,400,80,80,80,80,400),4,4
> )
> mu2<-c(0,0,0)
> Sigma2<-diag(16,3)
> sample.size<-100
> N<-1000 #Number of datasets
> #Take a draw from VL distribution
> vl.error<-mvrnorm(n=N, mu2, Sigma2)
>
> #Step 1 Create Data
> Data <- lapply(seq(N), function(x)
> as.data.frame(cbind(1:10,mvrnorm(n=sample.size, mu, Sigma))))
>
> #Step 2 Add Vertical Linking Error
> for(i in seq(along=Data)){
> Data[[i]]$V6 <- Data[[i]]$V2
> Data[[i]]$V7 <- Data[[i]]$V3 + vl.error[i,1]
> Data[[i]]$V8 <- Data[[i]]$V4 + vl.error[i,2]
> Data[[i]]$V9 <- Data[[i]]$V5 + vl.error[i,3]
> }
>
> #Step 3 Restructure for Longitudinal Analysis
> long <- lapply(Data, function(x) reshape(x, idvar="Data[[i]]$V1",
> varying=list(c(names(Data[[i]])[2:5]),c(names(Data[[i]])[6:9])),
> v.names=c("score.1","score.2"), direction="long"))
>
> # Step 4 Run GLS
>
> glsrun1 <- lapply(long, function(x) gls(score.1~I(time-1), data=x,
> correlation=corAR1(form=~1|V1), method='ML'))
>
> glsrun2 <- lapply(long, function(x) gls(score.2~I(time-1), data=x,
> correlation=corAR1(form=~1|V1), method='ML'))
>
> # Step 5 Extract Intercepts and slopes
> int1 <- lapply(glsrun1, function(x) x$coefficient[1])
> slo1 <- lapply(glsrun1, function(x) x$coefficient[2])
> int2 <- lapply(glsrun2, function(x) x$coefficient[1])
> slo2 <- lapply(glsrun2, function(x) x$coefficient[2])
>
> # Step 6 Compute SD of intercepts and slopes
>
> int.sd1 <- sapply(glsrun1, function(x) x$coefficient[1])
> slo.sd1 <- sapply(glsrun1, function(x) x$coefficient[2])
> int.sd2 <- sapply(glsrun2, function(x) x$coefficient[1])
> slo.sd2 <- sapply(glsrun2, function(x) x$coefficient[2])
>
> cat("Original Standard Errors","\n", "Intercept","\t",
> sd(int.sd1),"\n","Slope","\t","\t", sd(slo.sd1),"\n")
>
> cat("Modified Standard Errors","\n", "Intercept","\t",
> sd(int.sd2),"\n","Slope","\t","\t", sd(slo.sd2),"\n")
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Sun Jan  9 02:38:35 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 08 Jan 2005 17:38:35 -0800
Subject: [R] Least square minimization (non-linear)
In-Reply-To: <E03EBB50FF2C024781A6E4460AD58F0607C140@swosu-mbx01.admin.swosu.edu>
References: <E03EBB50FF2C024781A6E4460AD58F0607C140@swosu-mbx01.admin.swosu.edu>
Message-ID: <41E08B1B.9040408@pdf.com>

      What are you trying to accomplish? 

      If you want to model the distribution of the data, and if the 
numbers are plausibly normally distributed, I'd start with "qqnorm".  If 
you have some distribution in mind, I'd try "fitdistr" in 
library(MASS).  If it's neither of these, please make another attempt to 
"read the posting guide! http://www.R-project.org/posting-guide.html".  
Your question may seem perfectly clear to you, but it seems to me to be 
too general to answer;  carefully following the instructions in the 
posting guide can increase the likelihood that someone will understand 
what you are trying to do enough to actually help you. 

      hope this helps.
      spencer graves

Jagarlamudi, Choudary wrote:

>Hi all,
>I think the last time i posted this topic i started on the wrong foot. Thnaks alot to everyone who responded.
>i'm coding in  R(first time) for a paper my colleague is publishing.  i plotted a histogram for 6000 values.  
>I am told to plot experimental vs theoretical vlaues from the  histogram and  do a non linear least square curve minimization and compute the mean and sd of the new  x values.Excuse me if this sounds too  naive.Can you help me in getting a start on this one.
>
>Choudary Jagarlamudi
>Instructor
>Southwestern Oklahoma State University
>STF 254
>100 campus Drive
>Weatherford OK 73096
>Tel 580-774-7136
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From andy_liaw at merck.com  Sun Jan  9 04:25:34 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 8 Jan 2005 22:25:34 -0500
Subject: [R] Least square minimization (non-linear)
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4DF@usrymx25.merck.com>

I think the question is fairly clear (to me, at least).  My problem is
`Why?'

If I'm not mistaken, what Choudary is asked to do is fit a gaussian density
to the data, by fitting the gaussian pdf to the (x, y) data where x are the
midpoints of the bins and y are the heights of the histogram, via nonlinear
least squares.  The fitted distribution is, of course, guaranteed to be a
real density, as it's a gaussian pdf with parameters estimated from NLS.

What Choudary (and his colleague) may not realize is that that's about as
convoluted a way of estimating the  parameters as one can imagine (or
perhaps beyond imagination?).  I do not see any advantage of doing things
this way over just estimating the parameters by the sample mean and variance
(or perhaps the MLE).  At least the statistical properties are well known
(and optimal in certain sense).  

If one is going to fit a gaussian distribution, just do it directly.
There's no need to go half way around the world to do that.  If you are
going to use the histogram, how do you decide on how many bins to use, and
where the boundaries of the bins should be?  Even with a fix number of bins
and bin width, there's not a unique histogram for a set of data.  Which one
should you use?  How do you justify these choices?

If the goal is _not_ to fit a gaussian distribution to the data, then please
do explain what it is.  If by `plotting experimental values vs. theoretical
values' you are trying to assess the normality of the data, then the Q-Q
plot (qqnorm() as Spencer suggested) is a far better choice.

Andy

> From: Spencer Graves
> 
>       What are you trying to accomplish? 
> 
>       If you want to model the distribution of the data, and if the 
> numbers are plausibly normally distributed, I'd start with 
> "qqnorm".  If 
> you have some distribution in mind, I'd try "fitdistr" in 
> library(MASS).  If it's neither of these, please make another 
> attempt to 
> "read the posting guide! 
> http://www.R-project.org/posting-guide.html".  
> Your question 
> may seem perfectly clear to you, but it seems to me to be 
> too general to answer;  carefully following the instructions in the 
> posting guide can increase the likelihood that someone will 
> understand 
> what you are trying to do enough to actually help you. 
> 
>       hope this helps.
>       spencer graves
> 
> Jagarlamudi, Choudary wrote:
> 
> >Hi all,
> >I think the last time i posted this topic i started on the 
> wrong foot. Thnaks alot to everyone who responded.
> >i'm coding in  R(first time) for a paper my colleague is 
> publishing.  i plotted a histogram for 6000 values.  
> >I am told to plot experimental vs theoretical vlaues from 
> the  histogram and  do a non linear least square curve 
> minimization and compute the mean and sd of the new  x 
> values.Excuse me if this sounds too  naive.Can you help me in 
> getting a start on this one.
> >
> >Choudary Jagarlamudi
> >Instructor
> >Southwestern Oklahoma State University
> >STF 254
> >100 campus Drive
> >Weatherford OK 73096
> >Tel 580-774-7136
> >
> >	[[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >  
> >
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From xiaoniny at yahoo.com  Sun Jan  9 09:21:47 2005
From: xiaoniny at yahoo.com (Ni Xiao)
Date: Sun, 9 Jan 2005 00:21:47 -0800 (PST)
Subject: [R] How can I simulate Pareto distribution in R?
Message-ID: <20050109082148.10660.qmail@web52807.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050109/314a4bf2/attachment.pl

From ripley at stats.ox.ac.uk  Sun Jan  9 09:51:45 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 9 Jan 2005 08:51:45 +0000 (GMT)
Subject: [R] Does R accumulate memory
In-Reply-To: <vdn0u09rnua2thsk84j16j0lr3lph20bo2@4ax.com>
References: <88EAF3512A55DF46B06B1954AEF73F74072EA20F@dc1ex2.air.org>
	<vdn0u09rnua2thsk84j16j0lr3lph20bo2@4ax.com>
Message-ID: <Pine.LNX.4.61.0501090844010.2574@gannet.stats>

On Sat, 8 Jan 2005, Duncan Murdoch wrote:

> On Sat, 8 Jan 2005 16:38:31 -0500, "Doran, Harold" <HDoran at air.org>
> wrote:
>
>> I am running into a memory issue that I haven't noticed before. I am
>> running a simulation with all of the code used below. I have increased
>> my memory to 712mb and have a total of 1 gb on my machine.
>>
>> What appears to be happening is I run a simulation where I create 1,000
>> datasets with a sample size of 100. I then run each dataset through a
>> gls and obtain some estimates.
>>
>> This works fine. But, when I view how much memory is being used in
>> Windows, I see that it does not reduce once the analysis is complete. As
>> a result, I must quit R and then perform another analysis.
>
> If you ask Windows how much memory is being used, you'll likely get an
> incorrect answer.  R may not release memory back to the OS, but it may
> be available for re-use within R.
>
> Call gc() to see how much memory R thinks is in use.

Calling memory.size() on Windows (this was Windows, I think) is also 
informative, and if the figures differ much. memory is being used by 
C/Fortran code called from R.

>> So for example, before starting the 1st simulation, my windows task
>> manager tells me I am using 200mb of memory. After running the first
>> simulation it may go up to 500mb. I then try and run another simulation
>> with a larger sample size, but I quickly run out of memory because it
>> starts at 500 and increases from there and the simulation halts.
>
> The difficulty you're running into may be memory fragmentation.  When
> you run with a larger sample size, R will try to allocate larger
> chunks than it did originally.  If the "holes" created when the
> original simulation is deleted are too small, R will need to ask
> Windows for new memory to store things in.
>
> You could try deleting everything in your workspace before running the
> 2nd simulation; this should reduce the fragmentation.

You need to delete _and_ run gc() to reduce fragmentation, or the internal 
gc will happen after objects get recreated.

> Or you could run the big simulation first, then the smaller one will fit 
> in the holes left from it.

I would still suggest running them in separate sessions.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From remigijus.lapinskas at mif.vu.lt  Sun Jan  9 10:50:28 2005
From: remigijus.lapinskas at mif.vu.lt (Remigijus Lapinskas)
Date: Sun, 9 Jan 2005 11:50:28 +0200
Subject: [R] How can I simulate Pareto distribution in R?
In-Reply-To: <20050109082148.10660.qmail@web52807.mail.yahoo.com>
References: <20050109082148.10660.qmail@web52807.mail.yahoo.com>
Message-ID: <4510796390.20050109115028@mif.vu.lt>


If you define Pareto density as p(x)=c*x^(-(c+1)) for x>1, then

dpareto <- function(x,c){
if(c<=0)stop("c must be positive") # Diagnostic step
ifelse(x<1,0,c/x^(c+1))}

ppareto <- function(q,c){
if(c<=0)stop("c must be positive > 0")
ifelse(q<1,0,1-1/q^c)}

qpareto <- function(p,c){
if(c<=0) stop("c must be positive > 0")
if(any(p<0)|any(p>1)) # Symbol | denotes logical OR
stop("p must be between 0 and 1")
q <- (1-p)^(-1/c)
q}

rpareto <- function(n,c){
if(c<=0) stop("c must be positive")
rp <- runif(n)^(-1/c)
rp}

Good luck,
Rem

Sunday, January 9, 2005, 10:21:47 AM, you wrote:

NX> Hi, guys,
NX>    I need to simulate Pareto distribution. But I found
NX> 'rpareto' didn't exist in R. And it seems that Pareto distribution
NX> don't have mathematical relationships with other distributions.
NX> What can I do?
 
NX> Thanks a lot.
 
NX> Ni



From gb at tal.stat.umu.se  Sun Jan  9 11:21:03 2005
From: gb at tal.stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Sun, 9 Jan 2005 11:21:03 +0100
Subject: [R] How can I simulate Pareto distribution in R?
Message-ID: <20050109102103.GB19104@tal.stat.umu.se>

On Sun, Jan 09, 2005 at 12:21:47AM -0800, Ni Xiao wrote:
> Hi, guys,
>    I need to simulate Pareto distribution. But I found 'rpareto' didn't exist in R. And it seems that Pareto distribution don't have mathematical relationships with other distributions. What can I do?

Well, it has a relation to the uniform distribution thru 
F(X) = U ~ uniform(0, 1), 
where X is  Pareto with cdf F. Solve for X and you have just reinvented the
"inverse method" of generating random numbers from a cdf F.

Remigijus made that explicit for you.

-- 
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se



From chabotd at globetrotter.net  Sun Jan  9 12:19:34 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Sun, 9 Jan 2005 12:19:34 +0100
Subject: [R] Re: "labels" attached to variable names
In-Reply-To: <200501071111.j07BB6gZ022977@hypatia.math.ethz.ch>
Message-ID: <576B6CF0-6230-11D9-ADB0-00050279D82B@globetrotter.net>

Thank you to the many helpful list members who answered.

I am grateful to have been told about attr and about Hmisc. Actually 
Hmisc also brings me solutions to many things on my wish list!

Cheers,

Denis
Le Vendredi, 7 janv 2005, ? 12:11 Europe/Paris, 
r-help-request at stat.math.ethz.ch a ?crit :

> De: Denis Chabot <chabotd at globetrotter.net>
> Date: Jeud 6 janv 2005  15:40:07 Europe/Paris
> ?: r-help at stat.math.ethz.ch
> Objet: [R] "labels" attached to variable names
>
>
> Hi,
>
> Can we attach a more descriptive "label" (I may use the wrong 
> terminology, which would explain why I found nothing on the FAQ) to 
> variable names, and later have an easy way to switch to these labels 
> in plots? I fear this is not possible and one must enter this by hand 
> as ylab and xlab when making plots.
>
> Thanks in advance,
>
> Denis Chabot



From anne.piotet at urbanet.ch  Sun Jan  9 13:20:04 2005
From: anne.piotet at urbanet.ch (Anne)
Date: Sun, 9 Jan 2005 13:20:04 +0100
Subject: [R] R-etiquette
Message-ID: <003801c4f645$8e3dd610$6c00a8c0@mtd4>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050109/31a31c77/attachment.pl

From f.harrell at vanderbilt.edu  Sun Jan  9 15:17:10 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 09 Jan 2005 09:17:10 -0500
Subject: [R] R-etiquette
In-Reply-To: <003801c4f645$8e3dd610$6c00a8c0@mtd4>
References: <003801c4f645$8e3dd610$6c00a8c0@mtd4>
Message-ID: <41E13CE6.8000400@vanderbilt.edu>

Anne wrote:
> I'm about to present a report (for internal use of governmental agency). I used extensively R , contibuted packages, as well as communications on the R-list
> 
> As well as citing R, I would like to know how to cite the contributed packages (it is not so easy, as some have been used exensively, other marginally, some are called from another package and some were not used as softwares but gave me insight of how to proceed), as well as thank the persons who generously responded to my demands on the list...
> 
> Is there an established way of doing that?
> 
> Thanks
> 
> I take the occasion of thanking all of you! The R project is really a wonderful idea and realisation
> 
> Anne

Good question Anne.  I don't know of an established way but here are 
examples based on the way I prefer people to site my packages.  For 
Hmisc, one of the two following ways works (BibTeX format):

@MISC{Hmisc,
   author = {Harrell, Frank E.},
   year = 2005,
   title = {{\tt Hmisc}: {A} library of miscellaneous \textsc{S} functions},
   howpublished = {Available from
                  {\tt biostat.\-mc.\-vanderbilt.\-edu/\-s/Hmisc}}
}
or
@MISC{alzsplus,
   author = {Alzola, Carlos F. and Harrell, Frank E.},
   year = 2004,
   title = {An {Introduction} to {S} and the {Hmisc} and {Design}
           {Libraries}. {Available} from
           {\tt 
http://biostat.mc.vanderbilt.edu/\-twiki/pub/Main/\-RS/sintro.pdf}.},
   note = {Electronic book, 308 pages}
}

For Design one of the following 2:

@book{rms,
   author = {Harrell, Frank E.},
   year = 2001,
   title = {Regression Modeling Strategies, with Applications to Linear
   Models, Survival Analysis and Logistic Regression},
   publisher = {Springer},
   address = {New York}
}

or

@MISC{Design,
   author = {Harrell, Frank E.},
   year = 2005,
   title = {{\tt Design}: {S} functions for biostatistical/epidemiologic
           modeling, testing, estimation, validation, graphics, 
prediction, and
           typesetting by storing enhanced model design attributes in 
the fit.
           Available from {\tt
   biostat.\-mc.\-vanderbilt.\-edu/s/\-{Design}}}
}

I am never clear on the proper year to use for referencing the packages 
by themselves, as the packages are constantly being improved.

Frank

> 
> ----------------------------------------------------
> Anne Piotet
> Tel: +41 79 359 83 32 (mobile)
> Email: anne.piotet at m-td.com
> ---------------------------------------------------
> M-TD Modelling and Technology Development
> PSE-C
> CH-1015 Lausanne
> Switzerland
> Tel: +41 21 693 83 98
> Fax: +41 21 646 41 33
> --------------------------------------------------
>  
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ripley at stats.ox.ac.uk  Sun Jan  9 16:15:01 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 9 Jan 2005 15:15:01 +0000 (GMT)
Subject: [R] R-etiquette
In-Reply-To: <003801c4f645$8e3dd610$6c00a8c0@mtd4>
References: <003801c4f645$8e3dd610$6c00a8c0@mtd4>
Message-ID: <Pine.LNX.4.61.0501091513110.26511@gannet.stats>

On Sun, 9 Jan 2005, Anne wrote:

> I'm about to present a report (for internal use of governmental agency). 
> I used extensively R , contibuted packages, as well as communications on 
> the R-list
>
> As well as citing R, I would like to know how to cite the contributed 
> packages (it is not so easy, as some have been used exensively, other 
> marginally, some are called from another package and some were not used 
> as softwares but gave me insight of how to proceed), as well as thank 
> the persons who generously responded to my demands on the list...
>
> Is there an established way of doing that?

See the citation() function, which works for packages too and a few 
authors have taken advantage of the means of customizing it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From f.harrell at vanderbilt.edu  Sun Jan  9 16:29:49 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 09 Jan 2005 10:29:49 -0500
Subject: [R] R-etiquette
In-Reply-To: <Pine.LNX.4.61.0501091513110.26511@gannet.stats>
References: <003801c4f645$8e3dd610$6c00a8c0@mtd4>
	<Pine.LNX.4.61.0501091513110.26511@gannet.stats>
Message-ID: <41E14DED.2080100@vanderbilt.edu>

Prof Brian Ripley wrote:
> On Sun, 9 Jan 2005, Anne wrote:
> 
>> I'm about to present a report (for internal use of governmental 
>> agency). I used extensively R , contibuted packages, as well as 
>> communications on the R-list
>>
>> As well as citing R, I would like to know how to cite the contributed 
>> packages (it is not so easy, as some have been used exensively, other 
>> marginally, some are called from another package and some were not 
>> used as softwares but gave me insight of how to proceed), as well as 
>> thank the persons who generously responded to my demands on the list...
>>
>> Is there an established way of doing that?
> 
> 
> See the citation() function, which works for packages too and a few 
> authors have taken advantage of the means of customizing it.
> 

I keep learning - thanks Brian.  I will add CITATION files in the parent 
directory of my packages.  The defaults generated from 
citation('packagename') for them are not bad.  -Frank



From patrick.drechsler at gmx.net  Sun Jan  9 16:52:53 2005
From: patrick.drechsler at gmx.net (Patrick Drechsler)
Date: Sun, 09 Jan 2005 16:52:53 +0100
Subject: [R] autoscaling plot font size in Sweave output possible?
References: <m37jmqupy8.fsf@pdrechsler.fqdn.th-h.de>
	<16862.19712.400719.16915@celebrian.ci.tuwien.ac.at>
	<m3llb44oe5.fsf@pdrechsler.fqdn.th-h.de>
	<16863.47993.282910.806214@celebrian.ci.tuwien.ac.at>
Message-ID: <m3ekguha5m.fsf@pdrechsler.fqdn.th-h.de>


Friedrich Leisch wrote on 08 Jan 2005 11:52:41 MET:

[...]

> To me it sounds much more like you're one of those guys who
> like it when others solve their problems for free. I gave you
> an IMO perfectly working cookbook recipe and you didn't even
> bother to tell me what did not work out for you, only a "see
> below" to completely different things

I am sorry if I made the impression of wanting to be spoon
fed. Next time I will try to explain myself better.

> As christmas present

[...example file...]

Thank you very much!

Patrick
-- 
The only difference between the saint and the sinner is that every saint
has a past and every sinner has a future.
		-- Oscar Wilde



From ripley at stats.ox.ac.uk  Sun Jan  9 17:23:40 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 9 Jan 2005 16:23:40 +0000 (GMT)
Subject: [R] R-etiquette
In-Reply-To: <41E14DED.2080100@vanderbilt.edu>
References: <003801c4f645$8e3dd610$6c00a8c0@mtd4>
	<Pine.LNX.4.61.0501091513110.26511@gannet.stats>
	<41E14DED.2080100@vanderbilt.edu>
Message-ID: <Pine.LNX.4.61.0501091622550.8240@gannet.stats>

On Sun, 9 Jan 2005, Frank E Harrell Jr wrote:

> Prof Brian Ripley wrote:
>> On Sun, 9 Jan 2005, Anne wrote:
>> 
>>> I'm about to present a report (for internal use of governmental agency). I 
>>> used extensively R , contibuted packages, as well as communications on the 
>>> R-list
>>> 
>>> As well as citing R, I would like to know how to cite the contributed 
>>> packages (it is not so easy, as some have been used exensively, other 
>>> marginally, some are called from another package and some were not used as 
>>> softwares but gave me insight of how to proceed), as well as thank the 
>>> persons who generously responded to my demands on the list...
>>> 
>>> Is there an established way of doing that?
>> 
>> 
>> See the citation() function, which works for packages too and a few authors 
>> have taken advantage of the means of customizing it.
>> 
>
> I keep learning - thanks Brian.  I will add CITATION files in the parent 
> directory of my packages.  The defaults generated from 
> citation('packagename') for them are not bad.  -Frank

You need to add them to the inst directory of source packages: see ONEWS.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Sun Jan  9 17:26:05 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 9 Jan 2005 16:26:05 +0000 (UTC)
Subject: [R] R-etiquette
References: <003801c4f645$8e3dd610$6c00a8c0@mtd4>
	<Pine.LNX.4.61.0501091513110.26511@gannet.stats>
Message-ID: <loom.20050109T172502-479@post.gmane.org>

Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:

: 
: On Sun, 9 Jan 2005, Anne wrote:
: 
: > I'm about to present a report (for internal use of governmental agency). 
: > I used extensively R , contibuted packages, as well as communications on 
: > the R-list
: >
: > As well as citing R, I would like to know how to cite the contributed 
: > packages (it is not so easy, as some have been used exensively, other 
: > marginally, some are called from another package and some were not used 
: > as softwares but gave me insight of how to proceed), as well as thank 
: > the persons who generously responded to my demands on the list...
: >
: > Is there an established way of doing that?
: 
: See the citation() function, which works for packages too and a few 
: authors have taken advantage of the means of customizing it.
: 


Could someone point out a few packages that have a CITATION file
to use as examples.  In a search I did I only found one.



From francoisromain at free.fr  Sun Jan  9 18:07:57 2005
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Sun, 09 Jan 2005 18:07:57 +0100
Subject: [R] R-etiquette
In-Reply-To: <loom.20050109T172502-479@post.gmane.org>
References: <003801c4f645$8e3dd610$6c00a8c0@mtd4>	<Pine.LNX.4.61.0501091513110.26511@gannet.stats>
	<loom.20050109T172502-479@post.gmane.org>
Message-ID: <41E164ED.8010005@free.fr>

hello,

package MASS has one that looks just like that :

citHeader("To cite the VR bundle (MASS, class, nnet, spatial) in 
publications use:")

citEntry(entry="Book",
         title = "Modern Applied Statistics with S",
         author = personList(as.person("W. N. Venables"),
                             as.person("B. D. Ripley")),
	 publisher = "Springer",
         edition = "Fourth",
         address      = "New York",
         year         = 2002,
         note         = "ISBN 0-387-95457-0",
         url          = "http://www.stats.ox.ac.uk/pub/MASS4",

         textVersion =
         paste("Venables, W. N. & Ripley, B. D. (2002)",
               "Modern Applied Statistics with S.",
               "Fourth Edition. Springer, New York. ISBN 0-387-95457-0")
)

or package base :

citHeader("To cite R in publications use:")

citEntry(entry="Manual",
         title = "R: A language and environment for statistical computing",
         author = person(last="R Development Core Team"),
         organization = "R Foundation for Statistical Computing",
         address      = "Vienna, Austria",
         year         = version$year,
         note         = "{ISBN} 3-900051-07-0",
         url          = "http://www.R-project.org",

         textVersion =
         paste("R Development Core Team (", version$year, "). ",
               "R: A language and environment for statistical computing. ",
               "R Foundation for Statistical Computing, Vienna, Austria. ",
               "ISBN 3-900051-07-0, URL http://www.R-project.org.",
               sep="")
         )

citFooter("We have invested a lot of time and effort in creating R,",
          "please cite it when using it for data analysis.",
	  "See also", sQuote("citation(\"pkgname\")"),
	  "for citing R packages.")


That's all I found. Hope this helps.

Romain.


Gabor Grothendieck a ?crit :

>Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:
>
>: 
>: On Sun, 9 Jan 2005, Anne wrote:
>: 
>: > I'm about to present a report (for internal use of governmental agency). 
>: > I used extensively R , contibuted packages, as well as communications on 
>: > the R-list
>: >
>: > As well as citing R, I would like to know how to cite the contributed 
>: > packages (it is not so easy, as some have been used exensively, other 
>: > marginally, some are called from another package and some were not used 
>: > as softwares but gave me insight of how to proceed), as well as thank 
>: > the persons who generously responded to my demands on the list...
>: >
>: > Is there an established way of doing that?
>: 
>: See the citation() function, which works for packages too and a few 
>: authors have taken advantage of the means of customizing it.
>: 
>
>
>Could someone point out a few packages that have a CITATION file
>to use as examples.  In a search I did I only found one.
>  
>
-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From dmb at mrc-dunn.cam.ac.uk  Sun Jan  9 19:17:06 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sun, 9 Jan 2005 18:17:06 +0000 (GMT)
Subject: [R] plot.default and open ended limits
Message-ID: <Pine.LNX.4.21.0501091812580.1530-100000@mail.mrc-dunn.cam.ac.uk>


Hi, I would like to bound the lower limit of my y scale to zero, and let R
chose an upper limit.

Something like

plot(x,ylim=c(0,))

or 

plot(x,ylim=c(0,na))

but nither of these do the job. I searched the docs, but I can't see a way
to do this.


Naturally its nothing I can't do 'by hand', I would just like to know if a
correct syntax exists.



From ligges at statistik.uni-dortmund.de  Sun Jan  9 19:33:01 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 09 Jan 2005 19:33:01 +0100
Subject: [R] plot.default and open ended limits
In-Reply-To: <Pine.LNX.4.21.0501091812580.1530-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0501091812580.1530-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <41E178DD.9070806@statistik.uni-dortmund.de>

Dan Bolser wrote:

> Hi, I would like to bound the lower limit of my y scale to zero, and let R
> chose an upper limit.
> 
> Something like
> 
> plot(x,ylim=c(0,))
> 
> or 
> 
> plot(x,ylim=c(0,na))
> 
> but nither of these do the job. I searched the docs, but I can't see a way
> to do this.
> 
> 
> Naturally its nothing I can't do 'by hand', I would just like to know if a
> correct syntax exists.

No, but
   plot(x, ylim = c(0, max(x)))
should do the trick.

Uwe Ligges


> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From francoisromain at free.fr  Sun Jan  9 19:34:08 2005
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Sun, 09 Jan 2005 19:34:08 +0100
Subject: [R] plot.default and open ended limits
In-Reply-To: <Pine.LNX.4.21.0501091812580.1530-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0501091812580.1530-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <41E17920.5090507@free.fr>

Hello Dan,

Look at the code of the plot.default function, you'll see that's not 
possible to specify one limit, nevertheless, you can do :

plot(x,y,ylim=c(0,max(y)))


Does it work for you ?

Cordialement. Romain.

Dan Bolser a ?crit :

>Hi, I would like to bound the lower limit of my y scale to zero, and let R
>chose an upper limit.
>
>Something like
>
>plot(x,ylim=c(0,))
>
>or 
>
>plot(x,ylim=c(0,na))
>
>but nither of these do the job. I searched the docs, but I can't see a way
>to do this.
>
>
>Naturally its nothing I can't do 'by hand', I would just like to know if a
>correct syntax exists.
>
>  
>
-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From ripley at stats.ox.ac.uk  Sun Jan  9 19:44:58 2005
From: ripley at stats.ox.ac.uk (Brian D Ripley)
Date: Sun, 9 Jan 2005 18:44:58 +0000 (GMT)
Subject: [R] R-etiquette
In-Reply-To: <loom.20050109T172502-479@post.gmane.org>
Message-ID: <Pine.GSO.4.31.0501091841420.16935-100000@markov.stats>

On Sun, 9 Jan 2005, Gabor Grothendieck wrote:

> Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:
>
> :
> : On Sun, 9 Jan 2005, Anne wrote:
> :
> : > I'm about to present a report (for internal use of governmental agency).
> : > I used extensively R , contibuted packages, as well as communications on
> : > the R-list
> : >
> : > As well as citing R, I would like to know how to cite the contributed
> : > packages (it is not so easy, as some have been used exensively, other
> : > marginally, some are called from another package and some were not used
> : > as softwares but gave me insight of how to proceed), as well as thank
> : > the persons who generously responded to my demands on the list...
> : >
> : > Is there an established way of doing that?
> :
> : See the citation() function, which works for packages too and a few
> : authors have taken advantage of the means of customizing it.
>
> Could someone point out a few packages that have a CITATION file
> to use as examples.  In a search I did I only found one.

It seems you need to learn how to search, as there are *4* in a basic
installation of R.

gannet% find . -name CITATION
./lmtest/inst/CITATION
./geoR/inst/CITATION
./geoRglm/inst/CITATION
./flexmix/inst/CITATION
./strucchange/inst/CITATION
./dse/dse1/inst/CITATION
./VR/class/inst/CITATION
./VR/MASS/inst/CITATION
./VR/nnet/inst/CITATION
./VR/spatial/inst/CITATION
./limma/inst/CITATION

Note too that citation() works without a CITATION file and provides a
basis for customization.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From edd at debian.org  Sun Jan  9 19:50:09 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 9 Jan 2005 12:50:09 -0600
Subject: [R] R-etiquette
In-Reply-To: <loom.20050109T172502-479@post.gmane.org>
References: <003801c4f645$8e3dd610$6c00a8c0@mtd4>
	<Pine.LNX.4.61.0501091513110.26511@gannet.stats>
	<loom.20050109T172502-479@post.gmane.org>
Message-ID: <20050109185008.GA15492@sonny.eddelbuettel.com>

On Sun, Jan 09, 2005 at 04:26:05PM +0000, Gabor Grothendieck wrote:
> Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:
> : See the citation() function, which works for packages too and a few 
> : authors have taken advantage of the means of customizing it.
[...] 
> Could someone point out a few packages that have a CITATION file
> to use as examples.  In a search I did I only found one.

Looking at the (small) subset of CRAN I maintain as Debian packages:

edd at chibud:~/src/debian/CRAN> ls */inst/CITATION
lmtest-0.9.9/inst/CITATION  strucchange-1.2.7/inst/CITATION

Looking at the (much larger and almost complete) set going into Quantian:

edd at basebud:~/src/quantian/source> ls usr/local/lib/R/site-library/*/CITATION
usr/local/lib/R/site-library/arrayMagic/CITATION
usr/local/lib/R/site-library/flexmix/CITATION
usr/local/lib/R/site-library/geoR/CITATION
usr/local/lib/R/site-library/globaltest/CITATION
usr/local/lib/R/site-library/limma/CITATION
edd at basebud:~/src/quantian/source> ls usr/lib/R/site-library/*/CITATION
usr/lib/R/site-library/lmtest/CITATION
usr/lib/R/site-library/strucchange/CITATION
edd at basebud:~/src/quantian/source> ls usr/lib/R/library/*/CITATION
usr/lib/R/library/MASS/CITATION   usr/lib/R/library/nnet/CITATION
usr/lib/R/library/base/CITATION   usr/lib/R/library/spatial/CITATION
usr/lib/R/library/class/CITATION
edd at basebud:~/src/quantian/source>                                         

Hth, Dirk

-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From rdobrin at molbio.princeton.edu  Sun Jan  9 20:11:22 2005
From: rdobrin at molbio.princeton.edu (Radu Dobrin)
Date: Sun, 09 Jan 2005 14:11:22 -0500
Subject: [R] dist{amap} error??
Message-ID: <41E181DA.8030206@molbio.princeton.edu>

Dear all,

I have come across a very confusing matter regarding dist() supplied by 
the amap package:

--- m is just a test matrix

 > library(amap)
Loading required package: mva
Warning message:
package 'mva' has been merged into 'stats'
 > m           a   b   c
aa 0.1 0.2 0.3
bb 2.0 3.0 4.0
cc 2.0 4.0 6.0
dd 0.3 0.2 0.1
 > ds<-dist(m,method="pearson")
 > ds
            aa          bb        cc
bb 7.416666e-03                     cc 1.110223e-16 0.007416666         
dd 2.857143e-01 0.205933333 0.2857143
 > ds<-dist(m,method="correlation")
 > ds
             aa bb cc
bb -4.440892e-16     cc  6.661338e-16  0  dd  2.000000e+00  2  2
 >

Why using method "correlation" I don't get 1-person also person corr 
coef is between (-1,1). Also in help it is said

 >>'pearson': Also named "not centered Pearson" sum(x_i y_i) /[sum(x_i2) 
sum(y_i2)].
 >>'correlation': Also named "Centered Pearson" 1 - corr(x,y).

Maybe corr() is not Pearson correlation ?? Or maybe the data has to be 
scaled? Very confusing. Maybe somebody can help me with this. Also there 
is a sqrt missing in the person corr coef definition.

Best,
Radu

-- 
Dr. Radu Dobrin
Department of Molecular Biology
Princeton University
Washington Road
Princeton, NJ 08544-1014
Phone: 609-258-5657
E-mail: rdobrin at molbio.princeton.edu



From dmb at mrc-dunn.cam.ac.uk  Sun Jan  9 20:53:14 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sun, 9 Jan 2005 19:53:14 +0000 (GMT)
Subject: [R] plot.default and open ended limits
In-Reply-To: <41E17920.5090507@free.fr>
Message-ID: <Pine.LNX.4.21.0501091951460.1530-100000@mail.mrc-dunn.cam.ac.uk>

On Sun, 9 Jan 2005, [ISO-8859-1] Romain Franois wrote:

>Hello Dan,
>
>Look at the code of the plot.default function, you'll see that's not 
>possible to specify one limit, nevertheless, you can do :

Suppose I wanted to contribute a 'fix' to the code to allow one of my
suggested syntax solutions below, how would I go about it?

Cheers,


>
>plot(x,y,ylim=c(0,max(y)))
>
>
>Does it work for you ?
>
>Cordialement. Romain.
>
>Dan Bolser a crit :
>
>>Hi, I would like to bound the lower limit of my y scale to zero, and let R
>>chose an upper limit.
>>
>>Something like
>>
>>plot(x,ylim=c(0,))
>>
>>or 
>>
>>plot(x,ylim=c(0,na))
>>
>>but nither of these do the job. I searched the docs, but I can't see a way
>>to do this.
>>
>>
>>Naturally its nothing I can't do 'by hand', I would just like to know if a
>>correct syntax exists.
>>
>>  
>>
>



From francoisromain at free.fr  Sun Jan  9 21:32:12 2005
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Sun, 09 Jan 2005 21:32:12 +0100
Subject: [R] plot.default and open ended limits
In-Reply-To: <Pine.LNX.4.21.0501091951460.1530-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0501091951460.1530-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <41E194CC.9070100@free.fr>


Dan Bolser a ?crit :

>>Hello Dan,
>>
>>Look at the code of the plot.default function, you'll see that's not 
>>possible to specify one limit, nevertheless, you can do :
>>    
>>
>
>Suppose I wanted to contribute a 'fix' to the code to allow one of my
>suggested syntax solutions below, how would I go about it?
>
>Cheers,
>
>  
>
I suppose you can overwrite the plot.default function and replace that 
part :

    ylim <- if (is.null(ylim)) range(xy$y[is.finite(xy$y)])
    else ylim


by, (( for example )) that :

    ylim <- if (is.null(ylim)) range(xy$y[is.finite(xy$y)])
            else ylim
    if(is.na(ylim[1])) ylim[1] <- min(xy$y[is.finite(xy$y)])
    if(is.na(ylim[2])) ylim[2] <- max(xy$y[is.finite(xy$y)])
    

I haven't tested it but I think that would do the trick if you specify 
the ylim argument that way :

plot(x,ylim=c(0,NA))  # note that the NA is in upper case


but I don't know if it's really usefull since ou can use that syntax 
below with no problem

plot(x, ylim = c(0, max(x)))


Romain.

>>plot(x,y,ylim=c(0,max(y)))
>>
>>
>>Does it work for you ?
>>
>>Cordialement. Romain.
>>
>>Dan Bolser a ?crit :
>>
>>    
>>
>>>Hi, I would like to bound the lower limit of my y scale to zero, and let R
>>>chose an upper limit.
>>>
>>>Something like
>>>
>>>plot(x,ylim=c(0,))
>>>
>>>or 
>>>
>>>plot(x,ylim=c(0,na))
>>>
>>>but nither of these do the job. I searched the docs, but I can't see a way
>>>to do this.
>>>
>>>
>>>Naturally its nothing I can't do 'by hand', I would just like to know if a
>>>correct syntax exists.
>>>
>>>      
>>>
-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From tlumley at u.washington.edu  Sun Jan  9 22:04:41 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 9 Jan 2005 13:04:41 -0800 (PST)
Subject: [R] plot.default and open ended limits
In-Reply-To: <Pine.LNX.4.21.0501091951460.1530-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0501091951460.1530-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.A41.4.61b.0501091254130.221654@homer05.u.washington.edu>

On Sun, 9 Jan 2005, Dan Bolser wrote:

> On Sun, 9 Jan 2005, [ISO-8859-1] Romain Franois wrote:
>
>> Hello Dan,
>>
>> Look at the code of the plot.default function, you'll see that's not
>> possible to specify one limit, nevertheless, you can do :
>
> Suppose I wanted to contribute a 'fix' to the code to allow one of my
> suggested syntax solutions below, how would I go about it?

The preferred way is to send a patch against the current (R-devel) 
sources, but if you are modifying a short function it's probably ok to 
send just the modified version (being careful not to change indenting and 
spacing  in lines you don't modify).

However, while we never want to discourage people from contributing fixes, 
I will note that users will expect ylim=c(0,NA) to work for all plot() 
methods that accept ylim, if it works for plot.default.  Some of these 
pass their xlim and ylim to  plot.default, so it will happen 
automatically, but some probably don't.


 	-thomas

>
> Cheers,
>
>
>>
>> plot(x,y,ylim=c(0,max(y)))
>>
>>
>> Does it work for you ?
>>
>> Cordialement. Romain.
>>
>> Dan Bolser a crit :
>>
>>> Hi, I would like to bound the lower limit of my y scale to zero, and let R
>>> chose an upper limit.
>>>
>>> Something like
>>>
>>> plot(x,ylim=c(0,))
>>>
>>> or
>>>
>>> plot(x,ylim=c(0,na))
>>>
>>> but nither of these do the job. I searched the docs, but I can't see a way
>>> to do this.
>>>
>>>
>>> Naturally its nothing I can't do 'by hand', I would just like to know if a
>>> correct syntax exists.
>>>
>>>
>>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle

From chabotd at globetrotter.net  Sun Jan  9 23:06:19 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Sun, 9 Jan 2005 23:06:19 +0100
Subject: [R] how to do by-processing using weighted.mean 
Message-ID: <B093AF9E-628A-11D9-8C35-000393707CF2@globetrotter.net>

Hi,

I'd like to compute the weighted mean of some variables, all using the 
same weight variable, for each combination of 3 factor variables.

I found how I could use "summarize" (from Hmisc) to do normal means for 
combinations of 3 factors, but I cannot find a way of doing weighted 
means. Is it possible in R?

Thanks in advance,

Denis Chabot



From dmb at mrc-dunn.cam.ac.uk  Sun Jan  9 23:12:24 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sun, 9 Jan 2005 22:12:24 +0000 (GMT)
Subject: [R] plot.default and open ended limits
In-Reply-To: <Pine.A41.4.61b.0501091254130.221654@homer05.u.washington.edu>
Message-ID: <Pine.LNX.4.21.0501092208200.1530-100000@mail.mrc-dunn.cam.ac.uk>

On Sun, 9 Jan 2005, Thomas Lumley wrote:

>On Sun, 9 Jan 2005, Dan Bolser wrote:
>
>> On Sun, 9 Jan 2005, [ISO-8859-1] Romain Franois wrote:
>>
>>> Hello Dan,
>>>
>>> Look at the code of the plot.default function, you'll see that's not
>>> possible to specify one limit, nevertheless, you can do :
>>
>> Suppose I wanted to contribute a 'fix' to the code to allow one of my
>> suggested syntax solutions below, how would I go about it?
>
>The preferred way is to send a patch against the current (R-devel) 
>sources, but if you are modifying a short function it's probably ok to 
>send just the modified version (being careful not to change indenting and 
>spacing  in lines you don't modify).
>
>However, while we never want to discourage people from contributing fixes, 
>I will note that users will expect ylim=c(0,NA) to work for all plot() 
>methods that accept ylim, if it works for plot.default.  Some of these 
>pass their xlim and ylim to  plot.default, so it will happen 
>automatically, but some probably don't.

Thats a good point. help.search("xlim") / ?xlim don't turn up any pages,
and only plot.default seems to document [xy]lim. Aside from greping the
source / docs how can I find out which plot functions use / abuse the
[xy}lim parameter?


>
>
> 	-thomas
>
>>
>> Cheers,
>>
>>
>>>
>>> plot(x,y,ylim=c(0,max(y)))
>>>
>>>
>>> Does it work for you ?
>>>
>>> Cordialement. Romain.
>>>
>>> Dan Bolser a crit :
>>>
>>>> Hi, I would like to bound the lower limit of my y scale to zero, and let R
>>>> chose an upper limit.
>>>>
>>>> Something like
>>>>
>>>> plot(x,ylim=c(0,))
>>>>
>>>> or
>>>>
>>>> plot(x,ylim=c(0,na))
>>>>
>>>> but nither of these do the job. I searched the docs, but I can't see a way
>>>> to do this.
>>>>
>>>>
>>>> Naturally its nothing I can't do 'by hand', I would just like to know if a
>>>> correct syntax exists.
>>>>
>>>>
>>>>
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>
>Thomas Lumley			Assoc. Professor, Biostatistics
>tlumley at u.washington.edu	University of Washington, Seattle



From f.harrell at vanderbilt.edu  Sun Jan  9 23:14:16 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 09 Jan 2005 17:14:16 -0500
Subject: [R] how to do by-processing using weighted.mean
In-Reply-To: <B093AF9E-628A-11D9-8C35-000393707CF2@globetrotter.net>
References: <B093AF9E-628A-11D9-8C35-000393707CF2@globetrotter.net>
Message-ID: <41E1ACB8.6010106@vanderbilt.edu>

Denis Chabot wrote:
> Hi,
> 
> I'd like to compute the weighted mean of some variables, all using the 
> same weight variable, for each combination of 3 factor variables.
> 
> I found how I could use "summarize" (from Hmisc) to do normal means for 
> combinations of 3 factors, but I cannot find a way of doing weighted 
> means. Is it possible in R?
> 
> Thanks in advance,
> 
> Denis Chabot
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

It's in one of the summarize examples!

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From Tom.Mulholland at dpi.wa.gov.au  Mon Jan 10 02:11:45 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Mon, 10 Jan 2005 09:11:45 +0800
Subject: [R] Basic Linear Algebra
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA5E@afhex01.dpi.wa.gov.au>

Thanks. You've confirmed that I really don't understand the language of mathematics, even if I understand how numbers work. I will keep plodding with my books until I can understand the implications for singularity in what I am trying to achieve.

Tom

> -----Original Message-----
> From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk]
> Sent: Friday, 7 January 2005 4:46 PM
> To: Mulholland, Tom
> Cc: R-Help (E-mail)
> Subject: Re: [R] Basic Linear Algebra
> 
...

> I can't. 4th element should be just 12.

I did indeed transcribe my solution incorrectly
>  
...
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
>



From may_acct at yahoo.com  Mon Jan 10 08:41:22 2005
From: may_acct at yahoo.com (may sawasdee)
Date: Sun, 9 Jan 2005 23:41:22 -0800 (PST)
Subject: [R] I have some problem about GLM function.
Message-ID: <20050110074122.7796.qmail@web54502.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050109/68bdfcd7/attachment.pl

From anne.piotet at urbanet.ch  Mon Jan 10 10:03:33 2005
From: anne.piotet at urbanet.ch (Anne)
Date: Mon, 10 Jan 2005 10:03:33 +0100
Subject: [R] R-etiquette
References: <003801c4f645$8e3dd610$6c00a8c0@mtd4>	<Pine.LNX.4.61.0501091513110.26511@gannet.stats><loom.20050109T172502-479@post.gmane.org>
	<41E164ED.8010005@free.fr>
Message-ID: <002301c4f6f3$44abc490$6c00a8c0@mtd4>

well, all  these responses address part of my question!
- what about citing people who helped with comments/ suggestions even code
on the list? I do think they deserve aknowledgement!


Thanks

Anne



----- Original Message ----- 
From: "Romain Fran?ois" <francoisromain at free.fr>
To: "Gabor Grothendieck" <ggrothendieck at myway.com>; "RHELP"
<R-help at stat.math.ethz.ch>
Sent: Sunday, January 09, 2005 6:07 PM
Subject: Re: [R] R-etiquette


> hello,
>
> package MASS has one that looks just like that :
>
> citHeader("To cite the VR bundle (MASS, class, nnet, spatial) in
> publications use:")
>
> citEntry(entry="Book",
>          title = "Modern Applied Statistics with S",
>          author = personList(as.person("W. N. Venables"),
>                              as.person("B. D. Ripley")),
> publisher = "Springer",
>          edition = "Fourth",
>          address      = "New York",
>          year         = 2002,
>          note         = "ISBN 0-387-95457-0",
>          url          = "http://www.stats.ox.ac.uk/pub/MASS4",
>
>          textVersion =
>          paste("Venables, W. N. & Ripley, B. D. (2002)",
>                "Modern Applied Statistics with S.",
>                "Fourth Edition. Springer, New York. ISBN 0-387-95457-0")
> )
>
> or package base :
>
> citHeader("To cite R in publications use:")
>
> citEntry(entry="Manual",
>          title = "R: A language and environment for statistical
computing",
>          author = person(last="R Development Core Team"),
>          organization = "R Foundation for Statistical Computing",
>          address      = "Vienna, Austria",
>          year         = version$year,
>          note         = "{ISBN} 3-900051-07-0",
>          url          = "http://www.R-project.org",
>
>          textVersion =
>          paste("R Development Core Team (", version$year, "). ",
>                "R: A language and environment for statistical computing.
",
>                "R Foundation for Statistical Computing, Vienna, Austria.
",
>                "ISBN 3-900051-07-0, URL http://www.R-project.org.",
>                sep="")
>          )
>
> citFooter("We have invested a lot of time and effort in creating R,",
>           "please cite it when using it for data analysis.",
>   "See also", sQuote("citation(\"pkgname\")"),
>   "for citing R packages.")
>
>
> That's all I found. Hope this helps.
>
> Romain.
>
>
> Gabor Grothendieck a ?crit :
>
> >Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:
> >
> >:
> >: On Sun, 9 Jan 2005, Anne wrote:
> >:
> >: > I'm about to present a report (for internal use of governmental
agency).
> >: > I used extensively R , contibuted packages, as well as communications
on
> >: > the R-list
> >: >
> >: > As well as citing R, I would like to know how to cite the contributed
> >: > packages (it is not so easy, as some have been used exensively, other
> >: > marginally, some are called from another package and some were not
used
> >: > as softwares but gave me insight of how to proceed), as well as thank
> >: > the persons who generously responded to my demands on the list...
> >: >
> >: > Is there an established way of doing that?
> >:
> >: See the citation() function, which works for packages too and a few
> >: authors have taken advantage of the means of customizing it.
> >:
> >
> >
> >Could someone point out a few packages that have a CITATION file
> >to use as examples.  In a search I did I only found one.
> >
> >
> -- 
> Romain FRANCOIS : francoisromain at free.fr
> page web : http://addictedtor.free.fr/  (en construction)
> 06 18 39 14 69 / 01 46 80 65 60
> _______________________________________________________
> Etudiant en 3eme ann?e
> Institut de Statistique de l'Universit? de Paris (ISUP)
> Fili?re Industrie et Services
> http://www.isup.cicrp.jussieu.fr/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From suhlig at fz-borstel.de  Mon Jan 10 10:17:05 2005
From: suhlig at fz-borstel.de (Stefan Uhlig)
Date: Mon, 10 Jan 2005 10:17:05 +0100
Subject: [R] Multiple comparisons following nlme
Message-ID: <200501100914.j0A9EWd14674@fz-borstel.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050110/a09f32c2/attachment.pl

From dmb at mrc-dunn.cam.ac.uk  Mon Jan 10 12:05:16 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Mon, 10 Jan 2005 11:05:16 +0000 (GMT)
Subject: [R] Graphical table in R
Message-ID: <Pine.LNX.4.21.0501101043210.7001-100000@mail.mrc-dunn.cam.ac.uk>


I would like R to produce some tabulated data in a graphical output. When
I say tabulated data, what I mean is a table with rows and columns. This
would be useful when reading in a big file, performing some analysis on
it, and then wanting to display the results as a table.

Something like 

plot(x,...)

where x is a matrix

For example, the result could look (approximatly) like...

+-------+-----------+
|     A |         B |
+-------+-----------+
|     1 |     33278 |
|     2 |      6790 |
...
|    10 |         1 |
|    12 |         6 |
+-------+-----------+


I havent (and cant) see any way to do this. Is there currently any way to
do this? I imagine it could be put together with various other plotting
'fundamentals', but the syntax of the layout format could be a pain to get
right. 

OK, I just found xtable (Export tables to LaTeX or HTML). Any plans to
make a print.xtable(x,type="X",...) or print.xtable(x,type="png",...) function?

Any easy way to convert a latex table to an image without actually doing

latex latex.table.file.tex

dvi2png latex.table.file.dvi

?

Cheers,
Dan.



From Hummel at mpimp-golm.mpg.de  Mon Jan 10 12:17:41 2005
From: Hummel at mpimp-golm.mpg.de (Jan Hummel)
Date: Mon, 10 Jan 2005 12:17:41 +0100
Subject: [R] Invisible plot using RSvgDevice
Message-ID: <F76D638DE300734B9DBA6D42A185241E0A3AD2@EMAIL.mpimp-golm.mpg.de>

Dear list members,

I have a probably simple question concerning the RSvgDevice. After upgrading from R 1.9.0 to R 2.0.1 the computet svg files looking empty. 
Each time the RSvgDevice 0.5.3 were used. 

Scales and headers are printed but the plots are missing:
<rect x="433.10" y="246.13" width="0.93" height="29.74" style="stroke-width:1;stroke:none;fill:none"/>
<rect x="434.02" y="201.52" width="0.93" height="74.35" style="stroke-width:1;stroke:none;fill:none"/>
<rect x="434.95" y="201.52" width="0.93" height="74.35" style="stroke-width:1;stroke:none;fill:none"/>

before updating the svg looked like that:
<rect x="431.61" y="251.09" width="1.12" height="24.78" style="stroke-width:1;stroke:#000000;fill:#32CD32" />
<rect x="432.72" y="261.00" width="1.12" height="14.87" style="stroke-width:1;stroke:#000000;fill:#32CD32" />
<rect x="433.84" y="256.04" width="1.12" height="19.83" style="stroke-width:1;stroke:#000000;fill:#32CD32" />

So it seems to be the "stroke:none;fill:none" text which will let the plot be "invisible"?

I use following code:
devSVG( file=filename, height = plot_height, width = plot_width, xmlHeader=TRUE, onefile=TRUE)
plot1<-barplot(MI, col=c('limegreen'), beside=TRUE, width = c(1,1), space = c(0,1), ylab='mutual inf.', ylim=c(0,1), main=headertext)
...
dev.off()

I definitly changed no code, but the plot is invisible.

Any help would be appreciated!

Thanks for any help.

	Jan



From r.knell at qmul.ac.uk  Mon Jan 10 12:57:24 2005
From: r.knell at qmul.ac.uk (Rob Knell)
Date: Mon, 10 Jan 2005 11:57:24 +0000
Subject: [R] Partial wireframe plots
In-Reply-To: <200501101121.j0AB75ID016107@hypatia.math.ethz.ch>
References: <200501101121.j0AB75ID016107@hypatia.math.ethz.ch>
Message-ID: <CA6F43AA-62FE-11D9-8FEF-0003936DB7E2@qmul.ac.uk>

Dear R-helpers

Can anyone direct me to a method for plotting what you might call a 
partial wireframe plot? I have two explanatory variables in a dataset 
which give me a significant interaction term when I fit a model. The 
two variables are correlated with each other to a moderate degree, and 
if I plot the predicted values from the model as a surface in a 3D 
wireframe plot there are some quite large parts of the surface that 
don't actually represent reasonable values for both variables. I'd like 
to only plot the parts of the surface that correspond to values of X 
and Y that are present in my dataset, which would give a surface with 
some corners missing. I can't see any way of specifying parts of the 
grid that should not be drawn: can anyone enlighten me? Alternatively, 
is it possible to shade specified squares of the plot to indicate the 
important parts of it?

Thanks for any help

Rob Knell

School of Biological Sciences
Queen Mary, University of London



From andy_liaw at merck.com  Mon Jan 10 13:06:00 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 10 Jan 2005 07:06:00 -0500
Subject: [R] R-etiquette
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4E4@usrymx25.merck.com>

I'd thank them in the acknowledgement section.  I think some (most?) journal
will allow such a section, and most people use that to thank their research
funding sources and/or collaborators who had not made the list of authors.

Andy

> From: Anne
> 
> 
> well, all  these responses address part of my question!
> - what about citing people who helped with comments/ 
> suggestions even code
> on the list? I do think they deserve acknowledgement!
> 
> 
> Thanks
> 
> Anne
> 
> 
> 
> ----- Original Message ----- 
> From: "Romain Fran?ois" <francoisromain at free.fr>
> To: "Gabor Grothendieck" <ggrothendieck at myway.com>; "RHELP"
> <R-help at stat.math.ethz.ch>
> Sent: Sunday, January 09, 2005 6:07 PM
> Subject: Re: [R] R-etiquette
> 
> 
> > hello,
> >
> > package MASS has one that looks just like that :
> >
> > citHeader("To cite the VR bundle (MASS, class, nnet, spatial) in
> > publications use:")
> >
> > citEntry(entry="Book",
> >          title = "Modern Applied Statistics with S",
> >          author = personList(as.person("W. N. Venables"),
> >                              as.person("B. D. Ripley")),
> > publisher = "Springer",
> >          edition = "Fourth",
> >          address      = "New York",
> >          year         = 2002,
> >          note         = "ISBN 0-387-95457-0",
> >          url          = "http://www.stats.ox.ac.uk/pub/MASS4",
> >
> >          textVersion =
> >          paste("Venables, W. N. & Ripley, B. D. (2002)",
> >                "Modern Applied Statistics with S.",
> >                "Fourth Edition. Springer, New York. ISBN 
> 0-387-95457-0")
> > )
> >
> > or package base :
> >
> > citHeader("To cite R in publications use:")
> >
> > citEntry(entry="Manual",
> >          title = "R: A language and environment for statistical
> computing",
> >          author = person(last="R Development Core Team"),
> >          organization = "R Foundation for Statistical Computing",
> >          address      = "Vienna, Austria",
> >          year         = version$year,
> >          note         = "{ISBN} 3-900051-07-0",
> >          url          = "http://www.R-project.org",
> >
> >          textVersion =
> >          paste("R Development Core Team (", version$year, "). ",
> >                "R: A language and environment for 
> statistical computing.
> ",
> >                "R Foundation for Statistical Computing, 
> Vienna, Austria.
> ",
> >                "ISBN 3-900051-07-0, URL http://www.R-project.org.",
> >                sep="")
> >          )
> >
> > citFooter("We have invested a lot of time and effort in 
> creating R,",
> >           "please cite it when using it for data analysis.",
> >   "See also", sQuote("citation(\"pkgname\")"),
> >   "for citing R packages.")
> >
> >
> > That's all I found. Hope this helps.
> >
> > Romain.
> >
> >
> > Gabor Grothendieck a ?crit :
> >
> > >Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:
> > >
> > >:
> > >: On Sun, 9 Jan 2005, Anne wrote:
> > >:
> > >: > I'm about to present a report (for internal use of governmental
> agency).
> > >: > I used extensively R , contibuted packages, as well as 
> communications
> on
> > >: > the R-list
> > >: >
> > >: > As well as citing R, I would like to know how to cite 
> the contributed
> > >: > packages (it is not so easy, as some have been used 
> exensively, other
> > >: > marginally, some are called from another package and 
> some were not
> used
> > >: > as softwares but gave me insight of how to proceed), 
> as well as thank
> > >: > the persons who generously responded to my demands on 
> the list...
> > >: >
> > >: > Is there an established way of doing that?
> > >:
> > >: See the citation() function, which works for packages 
> too and a few
> > >: authors have taken advantage of the means of customizing it.
> > >:
> > >
> > >
> > >Could someone point out a few packages that have a CITATION file
> > >to use as examples.  In a search I did I only found one.
> > >
> > >
> > -- 
> > Romain FRANCOIS : francoisromain at free.fr
> > page web : http://addictedtor.free.fr/  (en construction)
> > 06 18 39 14 69 / 01 46 80 65 60
> > _______________________________________________________
> > Etudiant en 3eme ann?e
> > Institut de Statistique de l'Universit? de Paris (ISUP)
> > Fili?re Industrie et Services
> > http://www.isup.cicrp.jussieu.fr/
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Mon Jan 10 13:08:58 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 10 Jan 2005 12:08:58 +0000 (GMT)
Subject: [R] R-etiquette
In-Reply-To: <002301c4f6f3$44abc490$6c00a8c0@mtd4>
References: <003801c4f645$8e3dd610$6c00a8c0@mtd4>
	<Pine.LNX.4.61.0501091513110.26511@gannet.stats><loom.20050109T172502-479@post.gmane.org>
	<41E164ED.8010005@free.fr> <002301c4f6f3$44abc490$6c00a8c0@mtd4>
Message-ID: <Pine.LNX.4.61.0501101206340.419@gannet.stats>

On Mon, 10 Jan 2005, Anne wrote:

> well, all  these responses address part of my question!
> - what about citing people who helped with comments/ suggestions even code
> on the list? I do think they deserve aknowledgement!

That depends on the journal and its conventions.  Most commonly they are 
acknowledged in acknowledgements, but some journals do allow

Joe Bloggs (2005) Personal communication.

or a URL.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jan 10 13:13:44 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 10 Jan 2005 12:13:44 +0000 (GMT)
Subject: [R] Invisible plot using RSvgDevice
In-Reply-To: <F76D638DE300734B9DBA6D42A185241E0A3AD2@EMAIL.mpimp-golm.mpg.de>
References: <F76D638DE300734B9DBA6D42A185241E0A3AD2@EMAIL.mpimp-golm.mpg.de>
Message-ID: <Pine.LNX.4.61.0501101209290.419@gannet.stats>

Have you contacted the author?

The conventions for R graphics devices changed in R 2.0.0, in particular 
the way the alpha channel is represented.  I doubt if the same version of 
a graphics device works in both R < 2.0.0 and R >= 2.0.0, and this one 
appears to predate R 2.0.0.

On Mon, 10 Jan 2005, Jan Hummel wrote:

> Dear list members,
>
> I have a probably simple question concerning the RSvgDevice. After upgrading from R 1.9.0 to R 2.0.1 the computet svg files looking empty.
> Each time the RSvgDevice 0.5.3 were used.
>
> Scales and headers are printed but the plots are missing:
> <rect x="433.10" y="246.13" width="0.93" height="29.74" style="stroke-width:1;stroke:none;fill:none"/>
> <rect x="434.02" y="201.52" width="0.93" height="74.35" style="stroke-width:1;stroke:none;fill:none"/>
> <rect x="434.95" y="201.52" width="0.93" height="74.35" style="stroke-width:1;stroke:none;fill:none"/>
>
> before updating the svg looked like that:
> <rect x="431.61" y="251.09" width="1.12" height="24.78" style="stroke-width:1;stroke:#000000;fill:#32CD32" />
> <rect x="432.72" y="261.00" width="1.12" height="14.87" style="stroke-width:1;stroke:#000000;fill:#32CD32" />
> <rect x="433.84" y="256.04" width="1.12" height="19.83" style="stroke-width:1;stroke:#000000;fill:#32CD32" />
>
> So it seems to be the "stroke:none;fill:none" text which will let the plot be "invisible"?
>
> I use following code:
> devSVG( file=filename, height = plot_height, width = plot_width, xmlHeader=TRUE, onefile=TRUE)
> plot1<-barplot(MI, col=c('limegreen'), beside=TRUE, width = c(1,1), space = c(0,1), ylab='mutual inf.', ylim=c(0,1), main=headertext)
> ...
> dev.off()
>
> I definitly changed no code, but the plot is invisible.
>
> Any help would be appreciated!
>
> Thanks for any help.
>
> 	Jan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From f.harrell at vanderbilt.edu  Mon Jan 10 13:20:51 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 10 Jan 2005 06:20:51 -0600
Subject: [R] R-etiquette
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E4E4@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E4E4@usrymx25.merck.com>
Message-ID: <41E27323.7000505@vanderbilt.edu>

Liaw, Andy wrote:
> I'd thank them in the acknowledgement section.  I think some (most?) journal
> will allow such a section, and most people use that to thank their research
> funding sources and/or collaborators who had not made the list of authors.
> 
> Andy

You do need to get permission from people to include their names in an 
Ack. section, usually.  Referencing as a personal communication is 
perhaps better.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ripley at stats.ox.ac.uk  Mon Jan 10 13:19:49 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 10 Jan 2005 12:19:49 +0000 (GMT)
Subject: [R] I have some problem about GLM function.
In-Reply-To: <20050110074122.7796.qmail@web54502.mail.yahoo.com>
References: <20050110074122.7796.qmail@web54502.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0501101214060.419@gannet.stats>

On Sun, 9 Jan 2005, may sawasdee wrote:

> Dear R-Help
>
> I 'm using GLM function to Modelling. But when I used Gamma Family in GLM, then I can't run.
> It was error
>
>> glm(DamageRatio~MinTEMP+MaxTEMP+DayRain+Group1+Group2+Group3+Year,family=Gamma())
> Error in eval(expr, envir, enclos) : Non-positive values not allowed for the gamma family
>
> Can Gamma Distribution use data begin 0 ?

No,  That is an impossible value from a gamma distribution: see ?dgamma 
and note what it says about x.

> and then when I used GLM in S-Plus Program then it done, but it didn't have AIC value.
>
> Last Question why are not the AIC in R and S-plus equal.

You have just said S-Plus does not have an AIC value.! However, since a 
gamma GLM is not fitted by ML, you cannot get AIC from a glm fit unless 
you specify the dispersion (unlikely).  Did you notice what ?glm says 
about the aic component?  See also ?logLik.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jan 10 13:23:26 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 10 Jan 2005 12:23:26 +0000 (GMT)
Subject: [R] Partial wireframe plots
In-Reply-To: <CA6F43AA-62FE-11D9-8FEF-0003936DB7E2@qmul.ac.uk>
References: <200501101121.j0AB75ID016107@hypatia.math.ethz.ch>
	<CA6F43AA-62FE-11D9-8FEF-0003936DB7E2@qmul.ac.uk>
Message-ID: <Pine.LNX.4.61.0501101220350.419@gannet.stats>

On Mon, 10 Jan 2005, Rob Knell wrote:

> Can anyone direct me to a method for plotting what you might call a partial 
> wireframe plot? I have two explanatory variables in a dataset which give me a 
> significant interaction term when I fit a model. The two variables are 
> correlated with each other to a moderate degree, and if I plot the predicted 
> values from the model as a surface in a 3D wireframe plot there are some 
> quite large parts of the surface that don't actually represent reasonable 
> values for both variables. I'd like to only plot the parts of the surface 
> that correspond to values of X and Y that are present in my dataset, which 
> would give a surface with some corners missing. I can't see any way of 
> specifying parts of the grid that should not be drawn: can anyone enlighten 
> me? Alternatively, is it possible to shade specified squares of the plot to 
> indicate the important parts of it?

Note what ?persp says about NA values: I believe wireframe (lattice) does 
the same thing, that is only plot the grid over the region of finite 
(non-NA, non-Inf) values, at least provided it is a sensible shape.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From murdoch at stats.uwo.ca  Mon Jan 10 13:34:27 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 10 Jan 2005 07:34:27 -0500
Subject: [R] R-etiquette
In-Reply-To: <002301c4f6f3$44abc490$6c00a8c0@mtd4>
References: <003801c4f645$8e3dd610$6c00a8c0@mtd4>	<Pine.LNX.4.61.0501091513110.26511@gannet.stats><loom.20050109T172502-479@post.gmane.org>
	<41E164ED.8010005@free.fr> <002301c4f6f3$44abc490$6c00a8c0@mtd4>
Message-ID: <8bt4u01qn3cp6o4jkgjlem8gv92clun3u0@4ax.com>

On Mon, 10 Jan 2005 10:03:33 +0100, "Anne" <anne.piotet at urbanet.ch>
wrote :

>well, all  these responses address part of my question!
>- what about citing people who helped with comments/ suggestions even code
>on the list? I do think they deserve aknowledgement!

Mention them in an acknowledgments section at the end of the paper.
You can cite them as "personal communication" or something similar if
you want to credit them with being the originators of some important
idea, but usually citations are supposed to be an aid to the reader to
find support for claims you make in your paper, or other related
information.

Duncan Murdoch



From Joris.DeWolf at cropdesign.com  Mon Jan 10 13:41:52 2005
From: Joris.DeWolf at cropdesign.com (Joris DeWolf)
Date: Mon, 10 Jan 2005 13:41:52 +0100
Subject: [R] contrasts involving random terms in lme
Message-ID: <41E27810.3050908@cropdesign.com>

Hello,

Has somebody an idea of how to fit contrast involving random terms and 
obtain their standard errors with lme?

I am referring to

Welham, S.; Cullis, B.; Gogel, B.; Gilmour, A. and Thompson, R. (2004).
Prediction in linear mixed models.
Australian and New Zealand Journal of Statistics 46, 325-347

who develop the ideas of

McLean, R.A.; Sanders,W.L.; and Stroup,W.W. (1991).
A unified approach to linear mixed models.
American Statistician 45, 54-64.

Any hint would be welcome.

Joris

-- 

======================================================================
Joris De Wolf
CropDesign N.V.
Plant Evaluation Group
Technologiepark 3
B-9052 Zwijnaarde
Belgium
Tel. : +32 9 242 91 55
Fax  : +32 9 241 91 73
======================================================================



confidentiality notice:
The information contained in this e-mail is confidential and...{{dropped}}



From cs_matyi at freemail.hu  Mon Jan 10 13:55:28 2005
From: cs_matyi at freemail.hu (=?ISO-8859-2?Q?Cserh=E1ti_M=E1ty=E1s?=)
Date: Mon, 10 Jan 2005 13:55:28 +0100 (CET)
Subject: [R] doing many commands within R
Message-ID: <freemail.20050010135528.53839@fm15.freemail.hu>



Dear all,

I'm new to this list, so let me greet everyone.

My problem is that I have several thousand data files which I want to 
perform a lot of R commands on, which are found in a seperate .R script.

Now, what I did was within the R prompt, I used to read in a list of the 
data files. 

e.g. 

namelist <- readLines("list_of_names",n=-1)
for (i in 1:100) {
k <- function(namelist[i])
write(k,file="outputfile",append=TRUE)
}

Next, I tried automating the R commands by making a loop. Within the 
loop I called the R-script. Within ecery single loop R called the R-script 
and performs the commands therein (or is supposed to perform it) on 
the actual data file.

After a loop finished, I tried appending the output to an output file.

The problem is that R gives an error message saying that it cannot 
open the input files.

What can I do?

Thanks, Matthew



From ligges at statistik.uni-dortmund.de  Mon Jan 10 14:24:04 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 10 Jan 2005 14:24:04 +0100
Subject: [R] doing many commands within R
In-Reply-To: <freemail.20050010135528.53839@fm15.freemail.hu>
References: <freemail.20050010135528.53839@fm15.freemail.hu>
Message-ID: <41E281F4.4020805@statistik.uni-dortmund.de>

Cserh?ti M?ty?s wrote:

> 
> Dear all,
> 
> I'm new to this list, so let me greet everyone.
> 
> My problem is that I have several thousand data files which I want to 
> perform a lot of R commands on, which are found in a seperate .R script.
> 
> Now, what I did was within the R prompt, I used to read in a list of the 
> data files. 
> 
> e.g. 
> 
> namelist <- readLines("list_of_names",n=-1)
> for (i in 1:100) {
> k <- function(namelist[i])
> write(k,file="outputfile",append=TRUE)
> }
> 
> Next, I tried automating the R commands by making a loop. Within the 
> loop I called the R-script. Within ecery single loop R called the R-script 
> and performs the commands therein (or is supposed to perform it) on 
> the actual data file.
> 
> After a loop finished, I tried appending the output to an output file.
> 
> The problem is that R gives an error message saying that it cannot 
> open the input files.
> 
> What can I do?

Debug! R has many tools for convenient debugging. See the manuals, R 
News, or some good book on R.

What can we do? We don't know anything about a) the files, b) the code 
and c) your intention!

Uwe Ligges

> Thanks, Matthew
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From solares at unsl.edu.ar  Mon Jan 10 15:02:13 2005
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Mon, 10 Jan 2005 11:02:13 -0300 (ARGSL-ST)
Subject: [R] mouse position with package tcltk
Message-ID: <29495.201.254.80.237.1105365733.squirrel@inter17.unsl.edu.ar>

HI, i need a know what is wrong with this code. If you run then not known,
X0, W or XPlace. This code not accept parameter but in tcl yes.
This code make a window panned. Thanks Ruben
In R:
library(tcltk)
XPlace<-function(fract){
         tkcmd("place",nb1,"relwidth"=fract)
         tkcmd("place",nb3,"relx"=fract)
         #tkcmd("place",nb2,"relwidth"=[expr {1.0 - fract}])
         tkcmd("place",nb2,"relwidth"=1.0 - fract)
}

tt<-tktoplevel("background"="black")
w<-.Tk.ID(tt)
nb1<-paste(w,".left",sep="",collapse=".")
nb2<-paste(w,".right",sep="",collapse=".")
nb3<-paste(w,".lrhandle",sep="",collapse=".")
f1<-tkcmd("frame",nb1,"background"="purple","width"=20)
f2<-tkcmd("frame",nb2,"background"="red","width"=20)
f3<-tkcmd("frame",nb3,"borderwidth"=2,"relief"="raised","background"="orange","cursor"="sb_v_double_arrow")

tkcmd("place",nb1,"relheight"=1,"anchor"="nw")
tkcmd("place",nb3,"rely"=.9,"anchor"="e","width"=10,"height"=10)
tkcmd("place",nb2,"relheight"=1,"relx"=1,"anchor"="ne")

tkcmd("bind",nb3,"<Configure>","set W [winfo width .]")
tkcmd("bind",nb3,"<Configure>","set X0 [winfo rootx .]")
tkcmd("bind",nb3,"<B1-Motion>","XPlace [expr {(\"%X\"- $X0)/double($W)}]")

XPlace(.5)

In Tcl/Tk:
proc XPlace {fract} {
place .left -relwidth $fract
place .lrhandle -relx $fract
place .right -relwidth [expr {1.0 - $fract}]
}

frame .left -bg purple -width 20
frame .right -bg red -width 20
frame .lrhandle -borderwidth 2 -relief raised -bg orange -cursor
sb_v_double_arrow

. configure -bg black

place .left  -relheight 1 -anchor nw
place .lrhandle -rely .9 -anchor e -width 10 -height 10
place .right -relheight 1 -relx 1 -anchor ne


bind .lrhandle <Configure> {set W [winfo width .]; set X0 [winfo rootx .]}
bind .lrhandle <B1-Motion> {XPlace [expr {(%X-$X0)/double($W)}]}

XPlace .5

pack [ label .left.label -text "Left side" ]
pack [ label .right.label -text "Right side" ]
Why i can pass parameter in Tcl but with function in R not.?



From MSchwartz at MedAnalytics.com  Mon Jan 10 15:09:34 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 10 Jan 2005 08:09:34 -0600
Subject: [R] Graphical table in R
In-Reply-To: <Pine.LNX.4.21.0501101043210.7001-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0501101043210.7001-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <1105366175.8807.4.camel@horizons.localdomain>

On Mon, 2005-01-10 at 11:05 +0000, Dan Bolser wrote:
> I would like R to produce some tabulated data in a graphical output. When
> I say tabulated data, what I mean is a table with rows and columns. This
> would be useful when reading in a big file, performing some analysis on
> it, and then wanting to display the results as a table.
> 
> Something like 
> 
> plot(x,...)
> 
> where x is a matrix
> 
> For example, the result could look (approximatly) like...
> 
> +-------+-----------+
> |     A |         B |
> +-------+-----------+
> |     1 |     33278 |
> |     2 |      6790 |
> ...
> |    10 |         1 |
> |    12 |         6 |
> +-------+-----------+
> 
> 
> I havent (and cant) see any way to do this. Is there currently any way to
> do this? I imagine it could be put together with various other plotting
> 'fundamentals', but the syntax of the layout format could be a pain to get
> right. 
> 
> OK, I just found xtable (Export tables to LaTeX or HTML). Any plans to
> make a print.xtable(x,type="X",...) or print.xtable(x,type="png",...) function?
> 
> Any easy way to convert a latex table to an image without actually doing
> 
> latex latex.table.file.tex
> 
> dvi2png latex.table.file.dvi
> 
> ?

It is not entirely clear what you want to do with the table once you
have created it. That piece of information might be helpful in
attempting to offer some ideas.

In lieu of that, here are some additional pointers:

1. Review the output and code for:

demo(plotmath)


2. Review the latex() and html() functions in Frank Harrell's Hmisc
package, which provide additional functionality along with xtable().


3. Review these two posts regarding a general approach to plotting
tables:

http://tolstoy.newcastle.edu.au/R/help/02b/0345.html
http://tolstoy.newcastle.edu.au/R/help/02b/0342.html


HTH,

Marc Schwartz



From blindglobe at gmail.com  Mon Jan 10 15:19:28 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Mon, 10 Jan 2005 15:19:28 +0100
Subject: [R] R-etiquette
In-Reply-To: <002301c4f6f3$44abc490$6c00a8c0@mtd4>
References: <003801c4f645$8e3dd610$6c00a8c0@mtd4>
	<Pine.LNX.4.61.0501091513110.26511@gannet.stats>
	<loom.20050109T172502-479@post.gmane.org> <41E164ED.8010005@free.fr>
	<002301c4f6f3$44abc490$6c00a8c0@mtd4>
Message-ID: <1abe3fa9050110061947c776df@mail.gmail.com>

On Mon, 10 Jan 2005 10:03:33 +0100, Anne <anne.piotet at urbanet.ch> wrote:
> well, all  these responses address part of my question!
> - what about citing people who helped with comments/ suggestions even code
> on the list? I do think they deserve aknowledgement!

They probably do, BUT...

one of my previous academic departments had a general policy that we
should refuse acknowledgements when possible, as they are in a sense
the worst of all worlds -- no credit where it counts, and potentially
a huge amount of blame if, for example, you managed to get the advice
wrong at the end.

Not that I'm saying you shouldn't feel like you do, nor that the
people that helped you shouldn't get credit somehow, but that it
should be done carefully.  (of course, a report might be handled
differently than a presentation, and a journal article still
differently from those two).


best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From michael.watson at bbsrc.ac.uk  Mon Jan 10 15:41:16 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Mon, 10 Jan 2005 14:41:16 -0000
Subject: [R] Installation of XML library can't find libxml2.dll
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E89AE1@iahce2knas1.iah.bbsrc.reserved>

Sorry to ask a (probably) dumb question, but I am trying to install XML
package on Windows XP, R 2.0.1, and I get the error: 

"This application has failed to start because libxml2.dll was not found.
Re-installing the application may fix this problem"

> library(XML)
Error in dyn.load(x, as.logical(local), as.logical(now)) : 
        unable to load shared library
"C:/PROGRA~1/R/rw2001/library/XML/libs/XML.dll":
  LoadLibrary failure:  The specified module could not be found.
Error in library(XML) : package/namespace load failed for 'XML'

Now, having read the website at http://www.omegahat.org/RSXML/, I find
that the package:

"uses libxml2, by default and only libxml(version 1) if libxml2 is not
present "

And here's my dumb question - what is libxml2 in a windows context and
where do I get it from?

Thanks
Mick



From michael.watson at bbsrc.ac.uk  Mon Jan 10 15:50:55 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Mon, 10 Jan 2005 14:50:55 -0000
Subject: [R] Installing RCurl on Linux
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E89AE2@iahce2knas1.iah.bbsrc.reserved>

Hi

I'm not having much luck today!  Running Suse Linux 8.2, R 2.0.0, trying
to install RCurl_0.5-1.tar.gz.  

Perhaps there's an issue with my C compiler, but I got some errors:

IAHC-LINUX03:/usr/users/mwatson # R CMD INSTALL RCurl_0.5-1.tar.gz
* Installing *source* package 'RCurl' ...
checking for curl-config... /usr/bin/curl-config
checking for gcc... gcc
checking for C compiler default output... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
No libidn field in version structure.
configure: creating ./config.status
config.status: creating src/Makevars
** libs
gcc -I/usr/lib/R/include -Wall -I/usr/include  -I/usr/local/include
-I/opt/gnome/include -D__NO_MATH_INLINES -mieee-fp  -fPIC   -c curl.c -o
curl.o
curl.c: In function `R_curl_easy_init':
curl.c:43: error: `CURLOPT_HTTPAUTH' undeclared (first use in this
function)
curl.c:43: error: (Each undeclared identifier is reported only once
curl.c:43: error: for each function it appears in.)
curl.c:43: error: `CURLAUTH_ANY' undeclared (first use in this function)
curl.c: In function `R_curl_easy_setopt':
curl.c:153: warning: implicit declaration of function
`curl_easy_strerror'
curl.c:154: warning: format argument is not a pointer (arg 7)
curl.c: In function `RCurlVersionInfoToR':
curl.c:683: error: structure has no member named `ares'
curl.c:683: error: structure has no member named `ares'
curl.c:684: error: structure has no member named `ares_num'
make: *** [curl.o] Error 1
ERROR: compilation failed for package 'RCurl'

Any ideas?  Thanks

Mick



From dmb at mrc-dunn.cam.ac.uk  Mon Jan 10 15:52:25 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Mon, 10 Jan 2005 14:52:25 +0000 (GMT)
Subject: [R] Graphical table in R
In-Reply-To: <1105366175.8807.4.camel@horizons.localdomain>
Message-ID: <Pine.LNX.4.21.0501101451440.11113-100000@mail.mrc-dunn.cam.ac.uk>

On Mon, 10 Jan 2005, Marc Schwartz wrote:

>On Mon, 2005-01-10 at 11:05 +0000, Dan Bolser wrote:
>> I would like R to produce some tabulated data in a graphical output. When
>> I say tabulated data, what I mean is a table with rows and columns. This
>> would be useful when reading in a big file, performing some analysis on
>> it, and then wanting to display the results as a table.
>> 
>> Something like 
>> 
>> plot(x,...)
>> 
>> where x is a matrix
>> 
>> For example, the result could look (approximatly) like...
>> 
>> +-------+-----------+
>> |     A |         B |
>> +-------+-----------+
>> |     1 |     33278 |
>> |     2 |      6790 |
>> ...
>> |    10 |         1 |
>> |    12 |         6 |
>> +-------+-----------+
>> 
>> 
>> I havent (and cant) see any way to do this. Is there currently any way to
>> do this? I imagine it could be put together with various other plotting
>> 'fundamentals', but the syntax of the layout format could be a pain to get
>> right. 
>> 
>> OK, I just found xtable (Export tables to LaTeX or HTML). Any plans to
>> make a print.xtable(x,type="X",...) or print.xtable(x,type="png",...) function?
>> 
>> Any easy way to convert a latex table to an image without actually doing
>> 
>> latex latex.table.file.tex
>> 
>> dvi2png latex.table.file.dvi
>> 
>> ?
>
>It is not entirely clear what you want to do with the table once you
>have created it. That piece of information might be helpful in
>attempting to offer some ideas.
>
>In lieu of that, here are some additional pointers:
>
>1. Review the output and code for:
>
>demo(plotmath)
>
>
>2. Review the latex() and html() functions in Frank Harrell's Hmisc
>package, which provide additional functionality along with xtable().
>
>
>3. Review these two posts regarding a general approach to plotting
>tables:
>
>http://tolstoy.newcastle.edu.au/R/help/02b/0345.html
>http://tolstoy.newcastle.edu.au/R/help/02b/0342.html

Thanks very much.

I want either an image of a table, or to include a table in an existing
plot (i.e. to supplement a plot).




>
>
>HTH,
>
>Marc Schwartz
>
>
>



From andy_liaw at merck.com  Mon Jan 10 15:57:43 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 10 Jan 2005 09:57:43 -0500
Subject: [R] help diagnosing ftp problem
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4EA@usrymx25.merck.com>

Dear R-help,

I've been compiling R from source on our Linux boxes for quite a while.  One
thing that bugs me is that I always get an error when make check-all where
it choked up running tests/internet.R.  That wasn't a big deal, as I can run
install.packages()/update.packages() fine, and that's all I use the 'net
connection for.  The problem is that it takes a _long_ time to fail
internet.R.

I dug in just a bit more this time (that I was sufficiently annoyed).  The
problem is:

> read.table("ftp://ftp.stats.ox.ac.uk/pub/datasets/csb/ch11b.dat")
Error in file(file, "r") : unable to open connection

I tried running the line in an interactive R session, and sure enough, it
takes more than 10 minutes before I see the error.  I do have both HTTP and
FTP proxy set (to the same URL), and ftp (e.g., to CRAN) works outside of R.
Can anyone provide some hints as to what the problem might be, or where to
look?

Best,
Andy

Andy Liaw, PhD
Biometrics Research      PO Box 2000, RY33-300     
Merck Research Labs           Rahway, NJ 07065
andy_liaw <at> merck.com          732-594-0820



From tlumley at u.washington.edu  Mon Jan 10 16:01:21 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 10 Jan 2005 07:01:21 -0800 (PST)
Subject: [R] how to do by-processing using weighted.mean 
In-Reply-To: <B093AF9E-628A-11D9-8C35-000393707CF2@globetrotter.net>
References: <B093AF9E-628A-11D9-8C35-000393707CF2@globetrotter.net>
Message-ID: <Pine.A41.4.61b.0501100701100.333248@homer11.u.washington.edu>

On Sun, 9 Jan 2005, Denis Chabot wrote:

> Hi,
>
> I'd like to compute the weighted mean of some variables, all using the same 
> weight variable, for each combination of 3 factor variables.
>
> I found how I could use "summarize" (from Hmisc) to do normal means for 
> combinations of 3 factors, but I cannot find a way of doing weighted means. 
> Is it possible in R?

?weighted.mean

 	-thomas



From tlumley at u.washington.edu  Mon Jan 10 16:11:15 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 10 Jan 2005 07:11:15 -0800 (PST)
Subject: [R] plot.default and open ended limits
In-Reply-To: <Pine.LNX.4.21.0501092208200.1530-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0501092208200.1530-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.A41.4.61b.0501100707000.333248@homer11.u.washington.edu>

On Sun, 9 Jan 2005, Dan Bolser wrote:
>
> Thats a good point. help.search("xlim") / ?xlim don't turn up any pages,
> and only plot.default seems to document [xy]lim. Aside from greping the
> source / docs how can I find out which plot functions use / abuse the
> [xy}lim parameter?
>

That's actually good news.  If a plot method doesn't document xlim/ylim 
then it's a reasonable bet that it either doesn't accept them or accepts 
them in ... and passes them to plot.default.

 	-thomas



From tfliao at uiuc.edu  Mon Jan 10 16:16:37 2005
From: tfliao at uiuc.edu (Tim F Liao)
Date: Mon, 10 Jan 2005 09:16:37 -0600
Subject: [R] doing many commands within R
Message-ID: <9737757.bc84f3cc.8199600@expms6.cites.uiuc.edu>

Sounds like you've got a file referencing problem.  You may
want to try and input the file into R using a simple
read.table command or something similar without yet running
your long script to see if your file can be called up.

Tim

---- Original message ----
>Date: Mon, 10 Jan 2005 13:55:28 +0100 (CET)
>From: Cserh?ti M?ty?s <cs_matyi at freemail.hu>  
>Subject: [R] doing many commands within R  
>To: r-help at stat.math.ethz.ch
>
>
>
>Dear all,
>
>I'm new to this list, so let me greet everyone.
>
>My problem is that I have several thousand data files which I
want to 
>perform a lot of R commands on, which are found in a seperate
.R script.
>
>Now, what I did was within the R prompt, I used to read in a
list of the 
>data files. 
>
>e.g. 
>
>namelist <- readLines("list_of_names",n=-1)
>for (i in 1:100) {
>k <- function(namelist[i])
>write(k,file="outputfile",append=TRUE)
>}
>
>Next, I tried automating the R commands by making a loop.
Within the 
>loop I called the R-script. Within ecery single loop R called
the R-script 
>and performs the commands therein (or is supposed to perform
it) on 
>the actual data file.
>
>After a loop finished, I tried appending the output to an
output file.
>
>The problem is that R gives an error message saying that it
cannot 
>open the input files.
>
>What can I do?
>
>Thanks, Matthew
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From brostaux.y at fsagx.ac.be  Mon Jan 10 16:38:09 2005
From: brostaux.y at fsagx.ac.be (Yves Brostaux)
Date: Mon, 10 Jan 2005 16:38:09 +0100
Subject: [R] Mixing portrait/landscape in a postscript file
Message-ID: <41E2A161.4050207@fsagx.ac.be>

Dear list,

I'm stuck with a little graphical problem. I'm generating several 
lattice plots which are printed in a single postcript device opened by

 > trellis.device(postscript, theme=canonical.theme("postscript", 
color=F), file="an_phase2_graph.ps", paper="a4", pointsize = 10, 
onefile=TRUE, horizontal=TRUE)

Everything works fine,but some of these plots shoud be printed in a 
portrait orientation to look better. I tried to include [...] 
par.settings = list(horizontal=FALSE) [...] in the lattice plots 
arguments, but with no success. Is there a way to mix different page 
orientation in a single postscript device that I missed ?

-- 
Ir. Yves BROSTAUX
Unit? de Statistique et Informatique
Facult? universitaire des Sciences agronomiques de Gembloux (FUSAGx)
8, avenue de la Facult?
B-5030 Gembloux
Belgique
T?l: +32 81 62 24 69
Email: brostaux.y at fsagx.ac.be



From andy_liaw at merck.com  Mon Jan 10 16:45:07 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 10 Jan 2005 10:45:07 -0500
Subject: [R] Graphical table in R
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4EE@usrymx25.merck.com>

I recall there's a function in the gregmisc bundle that can write text to an
R plot (was it textplot?).  That might be of use here, I guess.

Andy

> From: Dan Bolser
> 
> On Mon, 10 Jan 2005, Marc Schwartz wrote:
> 
> >On Mon, 2005-01-10 at 11:05 +0000, Dan Bolser wrote:
> >> I would like R to produce some tabulated data in a 
> graphical output. When
> >> I say tabulated data, what I mean is a table with rows and 
> columns. This
> >> would be useful when reading in a big file, performing 
> some analysis on
> >> it, and then wanting to display the results as a table.
> >> 
> >> Something like 
> >> 
> >> plot(x,...)
> >> 
> >> where x is a matrix
> >> 
> >> For example, the result could look (approximatly) like...
> >> 
> >> +-------+-----------+
> >> |     A |         B |
> >> +-------+-----------+
> >> |     1 |     33278 |
> >> |     2 |      6790 |
> >> ...
> >> |    10 |         1 |
> >> |    12 |         6 |
> >> +-------+-----------+
> >> 
> >> 
> >> I havent (and cant) see any way to do this. Is there 
> currently any way to
> >> do this? I imagine it could be put together with various 
> other plotting
> >> 'fundamentals', but the syntax of the layout format could 
> be a pain to get
> >> right. 
> >> 
> >> OK, I just found xtable (Export tables to LaTeX or HTML). 
> Any plans to
> >> make a print.xtable(x,type="X",...) or 
> print.xtable(x,type="png",...) function?
> >> 
> >> Any easy way to convert a latex table to an image without 
> actually doing
> >> 
> >> latex latex.table.file.tex
> >> 
> >> dvi2png latex.table.file.dvi
> >> 
> >> ?
> >
> >It is not entirely clear what you want to do with the table once you
> >have created it. That piece of information might be helpful in
> >attempting to offer some ideas.
> >
> >In lieu of that, here are some additional pointers:
> >
> >1. Review the output and code for:
> >
> >demo(plotmath)
> >
> >
> >2. Review the latex() and html() functions in Frank Harrell's Hmisc
> >package, which provide additional functionality along with xtable().
> >
> >
> >3. Review these two posts regarding a general approach to plotting
> >tables:
> >
> >http://tolstoy.newcastle.edu.au/R/help/02b/0345.html
> >http://tolstoy.newcastle.edu.au/R/help/02b/0342.html
> 
> Thanks very much.
> 
> I want either an image of a table, or to include a table in 
> an existing
> plot (i.e. to supplement a plot).
> 
> 
> 
> 
> >
> >
> >HTH,
> >
> >Marc Schwartz
> >
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From anne.piotet at urbanet.ch  Mon Jan 10 17:09:11 2005
From: anne.piotet at urbanet.ch (Anne)
Date: Mon, 10 Jan 2005 17:09:11 +0100
Subject: [R] R-etiquette
References: <3A822319EB35174CA3714066D590DCD50994E4E4@usrymx25.merck.com>
	<41E27323.7000505@vanderbilt.edu>
Message-ID: <00be01c4f72e$ba40ed30$6c00a8c0@mtd4>

In this case, it is not a publication for a journal (where I would follow
the prescriptions of my academic institution, ie ETH) but an internal report
for a local gvt agency.

Looking at some such reports I was horrified to find out that credits were
not made (in a case I do know that R was used, butis nowhere mentionned), no
mention were made of the statistical methods used (only global results were
cited as god divine truths ),  not to mention authors/contributors of
packages methods etc...

So I do fully intend to be as honest as possible....

Cheers

Anne


----- Original Message ----- 
From: "Frank E Harrell Jr" <f.harrell at vanderbilt.edu>
To: "Liaw, Andy" <andy_liaw at merck.com>
Cc: "'Anne'" <anne.piotet at urbanet.ch>; "R list" <r-help at stat.math.ethz.ch>
Sent: Monday, January 10, 2005 1:20 PM
Subject: Re: [R] R-etiquette


> Liaw, Andy wrote:
> > I'd thank them in the acknowledgement section.  I think some (most?)
journal
> > will allow such a section, and most people use that to thank their
research
> > funding sources and/or collaborators who had not made the list of
authors.
> >
> > Andy
>
> You do need to get permission from people to include their names in an
> Ack. section, usually.  Referencing as a personal communication is
> perhaps better.
>
> -- 
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                       Department of Biostatistics   Vanderbilt University



From MSchwartz at MedAnalytics.com  Mon Jan 10 17:10:04 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 10 Jan 2005 10:10:04 -0600
Subject: [R] Graphical table in R
In-Reply-To: <Pine.LNX.4.21.0501101451440.11113-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0501101451440.11113-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <1105373405.8807.55.camel@horizons.localdomain>

On Mon, 2005-01-10 at 14:52 +0000, Dan Bolser wrote:
> On Mon, 10 Jan 2005, Marc Schwartz wrote:
> 
> >On Mon, 2005-01-10 at 11:05 +0000, Dan Bolser wrote:
> >> I would like R to produce some tabulated data in a graphical output. When
> >> I say tabulated data, what I mean is a table with rows and columns. This
> >> would be useful when reading in a big file, performing some analysis on
> >> it, and then wanting to display the results as a table.
> >> 
> >> Something like 
> >> 
> >> plot(x,...)
> >> 
> >> where x is a matrix
> >> 
> >> For example, the result could look (approximatly) like...
> >> 
> >> +-------+-----------+
> >> |     A |         B |
> >> +-------+-----------+
> >> |     1 |     33278 |
> >> |     2 |      6790 |
> >> ...
> >> |    10 |         1 |
> >> |    12 |         6 |
> >> +-------+-----------+
> >> 
> >> 
> >> I havent (and cant) see any way to do this. Is there currently any way to
> >> do this? I imagine it could be put together with various other plotting
> >> 'fundamentals', but the syntax of the layout format could be a pain to get
> >> right. 
> >> 
> >> OK, I just found xtable (Export tables to LaTeX or HTML). Any plans to
> >> make a print.xtable(x,type="X",...) or print.xtable(x,type="png",...) function?
> >> 
> >> Any easy way to convert a latex table to an image without actually doing
> >> 
> >> latex latex.table.file.tex
> >> 
> >> dvi2png latex.table.file.dvi
> >> 
> >> ?
> >
> >It is not entirely clear what you want to do with the table once you
> >have created it. That piece of information might be helpful in
> >attempting to offer some ideas.
> >
> >In lieu of that, here are some additional pointers:
> >
> >1. Review the output and code for:
> >
> >demo(plotmath)
> >
> >
> >2. Review the latex() and html() functions in Frank Harrell's Hmisc
> >package, which provide additional functionality along with xtable().
> >
> >
> >3. Review these two posts regarding a general approach to plotting
> >tables:
> >
> >http://tolstoy.newcastle.edu.au/R/help/02b/0345.html
> >http://tolstoy.newcastle.edu.au/R/help/02b/0342.html
> 
> Thanks very much.
> 
> I want either an image of a table, or to include a table in an existing
> plot (i.e. to supplement a plot).


OK. Then points 1 and 3 might be most helpful.

One of the other things to consider, is that the legend() function can
also be "tweaked" to create a multi-column table as an inset into an
existing plot. 

As a very simplistic example:

# Create a basic blank plot
plot(1:60, type = "n")

# Construct text for the table
legend.txt <- as.matrix(as.data.frame(UCBAdmissions))

# Format the final column to have 2 decimal places
# and have it be right justified
legend.txt[, 4] <- format(as.numeric(legend.txt[, 4]), nsmall = 2)

# Set a monospace font for table alignment
par(family = "mono")

# Plot the legend in 4 columns
legend(1, 60, legend = legend.txt, ncol = 4)


Take a look at ?legend and the various formatting functions.

And....as Andy just mentioned in his reply, the textplot() function in
the gplots package in the gregmisc bundle, which I always forget
about...  :-)

HTH,

Marc



From ligges at statistik.uni-dortmund.de  Mon Jan 10 17:25:00 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 10 Jan 2005 17:25:00 +0100
Subject: [R] help diagnosing ftp problem
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E4EA@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E4EA@usrymx25.merck.com>
Message-ID: <41E2AC5C.6020208@statistik.uni-dortmund.de>

Liaw, Andy wrote:

> Dear R-help,
> 
> I've been compiling R from source on our Linux boxes for quite a while.  One
> thing that bugs me is that I always get an error when make check-all where
> it choked up running tests/internet.R.  That wasn't a big deal, as I can run
> install.packages()/update.packages() fine, and that's all I use the 'net
> connection for.  The problem is that it takes a _long_ time to fail
> internet.R.
> 
> I dug in just a bit more this time (that I was sufficiently annoyed).  The
> problem is:
> 
> 
>>read.table("ftp://ftp.stats.ox.ac.uk/pub/datasets/csb/ch11b.dat")
> 
> Error in file(file, "r") : unable to open connection
> 
> I tried running the line in an interactive R session, and sure enough, it
> takes more than 10 minutes before I see the error.  I do have both HTTP and
> FTP proxy set (to the same URL), and ftp (e.g., to CRAN) works outside of R.
> Can anyone provide some hints as to what the problem might be, or where to
> look?

Some firewall?
Can you open a url() connection?

Works for me within a second on both Windows and Linux.

Uwe


> Best,
> Andy
> 
> Andy Liaw, PhD
> Biometrics Research      PO Box 2000, RY33-300     
> Merck Research Labs           Rahway, NJ 07065
> andy_liaw <at> merck.com          732-594-0820
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Jan 10 17:28:33 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 10 Jan 2005 16:28:33 +0000 (GMT)
Subject: [R] Installation of XML library can't find libxml2.dll
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E89AE1@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E95E89AE1@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <Pine.LNX.4.61.0501101625160.18574@gannet.stats>

Save yourself the problem and get a precompile package from

http://www.stats.ox.ac.uk/pub/RWin

On Mon, 10 Jan 2005, michael watson (IAH-C) wrote:

> Sorry to ask a (probably) dumb question, but I am trying to install XML
> package on Windows XP, R 2.0.1, and I get the error:
>
> "This application has failed to start because libxml2.dll was not found.
> Re-installing the application may fix this problem"
>
>> library(XML)
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>        unable to load shared library
> "C:/PROGRA~1/R/rw2001/library/XML/libs/XML.dll":
>  LoadLibrary failure:  The specified module could not be found.
> Error in library(XML) : package/namespace load failed for 'XML'
>
> Now, having read the website at http://www.omegahat.org/RSXML/, I find
> that the package:
>
> "uses libxml2, by default and only libxml(version 1) if libxml2 is not
> present "
>
> And here's my dumb question - what is libxml2 in a windows context and
> where do I get it from?

See http://www.xmlsoft.org/downloads.html

Note: this is not the list for Omegahat questions.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jan 10 17:34:18 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 10 Jan 2005 16:34:18 +0000 (GMT)
Subject: [R] Mixing portrait/landscape in a postscript file
In-Reply-To: <41E2A161.4050207@fsagx.ac.be>
References: <41E2A161.4050207@fsagx.ac.be>
Message-ID: <Pine.LNX.4.61.0501101630570.18574@gannet.stats>

On Mon, 10 Jan 2005, Yves Brostaux wrote:

> Dear list,
>
> I'm stuck with a little graphical problem. I'm generating several lattice 
> plots which are printed in a single postcript device opened by
>
>> trellis.device(postscript, theme=canonical.theme("postscript", color=F), 
> file="an_phase2_graph.ps", paper="a4", pointsize = 10, onefile=TRUE, 
> horizontal=TRUE)
>
> Everything works fine,but some of these plots shoud be printed in a portrait 
> orientation to look better. I tried to include [...] par.settings = 
> list(horizontal=FALSE) [...] in the lattice plots arguments, but with no 
> success. Is there a way to mix different page orientation in a single 
> postscript device that I missed ?

No.  It is hard-coded in the header of the file.

It is technically impossible in postscript, but what one could do is to 
rotate the coord system on individual pages so all the pages were portrait 
or landscape.  I believe grid viewports can do that for you.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From davidr at rhotrading.com  Mon Jan 10 17:48:26 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Mon, 10 Jan 2005 10:48:26 -0600
Subject: [R] dyn.load Excel add-in on Windows XP and XLCall32.dll
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A422B35@rhosvr02.rhotrading.com>

I am on Win XP SP2 using R 2.0.1 patched.
I could not find anything apropos in the archives or with search engines.
(But maybe I need to be taught how to search better.)

I am trying to dyn.load some commercial DLLs (Excel add-ins),
but I get a message box saying:

"This application has failed to start because XLCall32.dll was not found."

The message in R is:

Error in dyn.load(x, as.logical(local), as.logical(now)) : 
        unable to load shared library "<correct path>":
  LoadLibrary failure:  The specified module could not be found.

The DLLs work in Excel without my system having XLCall32.dll anywhere;
(I believe it was removed from 2000 and XP for security reasons.)
So possibly there is no real dependency there? (Cygwin strings command
does find that word in the DLLs, though.)

Is there a workaround for this, do I need to write an R-compatible wrapper,
or is it just impossible?

(I was able to load other DLLs (non-Excel add-ins), just as a test.)

Thanks for any pointers!

David L. Reiner
Rho Trading
Chicago  IL  60605



From michael.watson at bbsrc.ac.uk  Mon Jan 10 17:50:08 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Mon, 10 Jan 2005 16:50:08 -0000
Subject: [R] SSOAP and Rcurl with proxy server
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E89AF5@iahce2knas1.iah.bbsrc.reserved>

Hi

I'm trying to use a bioconductor package (KEGGSOAP) which relies on
Rcurl and SSOAP.

As an example, a function exists:

> list.organisms
function () 
{
    orgs <- matrix(unlist(.SOAP(KEGGserver, "list_organisms", 
        "", action = KEGGaction, xmlns = KEGGxmlns), use.names = FALSE),

        ncol = 2, byrow = TRUE)
    temp <- orgs[, 2]
    names(temp) <- orgs[, 1]
    return(temp)
}

As can be seen, this function uses the .SOAP method from SSOAP.

The function does not work as the .SOAP function times out.  This is
undoubtedly because it is not aware of my proxy server (functions such
as download.file() and getURL() (from RCurl) work fine, although the
latter requires me to specifically pass the proxy argument as
tag=value).  Unfortunately, the SSOAP package seems to have an alarming
lack of documentation on how to make it work with proxy servers.  The
.SOAP function even includes:

"...: name=value arguments to pass to the "

So a bit of a typo there (no problem we all do it).  Anyway, even if I
pass proxy="blah" to .SOAP I still get a time out.

I would really appreciate hearing from someone who has SSOAP, RCurl and
even KEGGSOAP working from behind a firewall and through a proxy server.

Cheers
Mick



From anne.piotet at urbanet.ch  Mon Jan 10 18:43:33 2005
From: anne.piotet at urbanet.ch (Anne)
Date: Mon, 10 Jan 2005 18:43:33 +0100
Subject: [R] Graphical table in R
References: <Pine.LNX.4.21.0501101043210.7001-100000@mail.mrc-dunn.cam.ac.uk>
	<1105366175.8807.4.camel@horizons.localdomain>
	<00c601c4f72f$68e072c0$6c00a8c0@mtd4>
	<1105376090.8807.67.camel@horizons.localdomain>
Message-ID: <00dc01c4f73b$e970ee40$6c00a8c0@mtd4>

Well I saw the title "graphical table..." and had just a cursory glance
However I do use this function which is good if not too many categories are
represented.

Anne


----- Original Message ----- 
From: "Marc Schwartz" <MSchwartz at MedAnalytics.com>
To: "Anne" <anne.piotet at urbanet.ch>
Sent: Monday, January 10, 2005 5:54 PM
Subject: Re: [R] Graphical table in R


> On Mon, 2005-01-10 at 17:14 +0100, Anne wrote:
> > symbol.freq in Hmisc
> >
> > Anne
>
> Hi Anne,
>
> Thanks for pointing that out, though that function is really a graphical
> representation of a contingency table. It does not print out summary
> details, etc. which is what I believe that Dan is looking for.
>
> Thanks,
>
> Marc
>
>



From dr.mike at ntlworld.com  Mon Jan 10 18:46:41 2005
From: dr.mike at ntlworld.com (dr mike)
Date: Mon, 10 Jan 2005 17:46:41 -0000
Subject: [R] Installation of XML library can't find libxml2.dll
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E89AE1@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <20050110174706.FUDD8064.aamta07-winn.mailhost.ntl.com@c400>

My reading of this is that you may have installed the 'Windows binary'
package from the Omegahat site - if so, it would appear that this is for
versions of R < 2.0 . Further to the pointer to the reply by Prof Ripley,
hisa link provides the correct Windows binary for R >= 2.0.

Regards

Mike

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of michael watson
(IAH-C)
Sent: 10 January 2005 14:41
To: R list
Subject: [R] Installation of XML library can't find libxml2.dll

Sorry to ask a (probably) dumb question, but I am trying to install XML
package on Windows XP, R 2.0.1, and I get the error: 

"This application has failed to start because libxml2.dll was not found.
Re-installing the application may fix this problem"

> library(XML)
Error in dyn.load(x, as.logical(local), as.logical(now)) : 
        unable to load shared library
"C:/PROGRA~1/R/rw2001/library/XML/libs/XML.dll":
  LoadLibrary failure:  The specified module could not be found.
Error in library(XML) : package/namespace load failed for 'XML'

Now, having read the website at http://www.omegahat.org/RSXML/, I find that
the package:

"uses libxml2, by default and only libxml(version 1) if libxml2 is not
present "

And here's my dumb question - what is libxml2 in a windows context and where
do I get it from?

Thanks
Mick

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From david.crabb at ntu.ac.uk  Mon Jan 10 19:03:51 2005
From: david.crabb at ntu.ac.uk (Crabb, David)
Date: Mon, 10 Jan 2005 18:03:51 -0000
Subject: [R] Query: simple autocorrelation ina time series
Message-ID: <958023F70A782142AFA14D3E82F79418091AE6@cherry.ads.ntu.ac.uk>

I hope you can help with what might be an easy query.

I am doing some simple simulations where I am generating series of data
from a normal distribution using rnorm. I am then treating these as a
time series. I want to know how I can incorporate correlation in the
series (autocorrelation) i.e. make the observations non-independent,
with a known (but simple) correlation structure. In other words I want
to simulate correlated data.

Many thanks for your help.




DISCLAIMER:\ This email is intended solely for the addressee...{{dropped}}



From csardi at rmki.kfki.hu  Mon Jan 10 19:49:57 2005
From: csardi at rmki.kfki.hu (Csardi Gabor)
Date: Mon, 10 Jan 2005 19:49:57 +0100
Subject: [R] Invisible plot using RSvgDevice
In-Reply-To: <Pine.LNX.4.61.0501101209290.419@gannet.stats>
References: <F76D638DE300734B9DBA6D42A185241E0A3AD2@EMAIL.mpimp-golm.mpg.de>
	<Pine.LNX.4.61.0501101209290.419@gannet.stats>
Message-ID: <20050110184957.GA12099@bifur.rmki.kfki.hu>

The new 0.6 package is submitted to CRAN. You can grab it from
the author's web site: http://darkridge.com/~jake/RSvg if you
want it now.

It works with R >=2.0.0 correctly.

Cheers,
Gabor

On Mon, Jan 10, 2005 at 12:13:44PM +0000, Prof Brian Ripley
wrote:
> Have you contacted the author?
>
> The conventions for R graphics devices changed in R 2.0.0, in
particular
> the way the alpha channel is represented.  I doubt if the
same version of
> a graphics device works in both R < 2.0.0 and R >= 2.0.0, and
this one
> appears to predate R 2.0.0.
>
> On Mon, 10 Jan 2005, Jan Hummel wrote:
>
> >Dear list members,
> >
> >I have a probably simple question concerning the RSvgDevice.
After
> >upgrading from R 1.9.0 to R 2.0.1 the computet svg files
looking empty.
> >Each time the RSvgDevice 0.5.3 were used.
> >
[...]

--
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK



From HDoran at air.org  Mon Jan 10 20:27:38 2005
From: HDoran at air.org (Doran, Harold)
Date: Mon, 10 Jan 2005 14:27:38 -0500
Subject: [R] Query: simple autocorrelation ina time series
Message-ID: <88EAF3512A55DF46B06B1954AEF73F74072EA462@dc1ex2.air.org>

You need mvrnorm() to do this. 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Crabb, David
Sent: Monday, January 10, 2005 1:04 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Query: simple autocorrelation ina time series

I hope you can help with what might be an easy query.

I am doing some simple simulations where I am generating series of data
from a normal distribution using rnorm. I am then treating these as a
time series. I want to know how I can incorporate correlation in the
series (autocorrelation) i.e. make the observations non-independent,
with a known (but simple) correlation structure. In other words I want
to simulate correlated data.

Many thanks for your help.




DISCLAIMER:\ This email is intended solely for the\ addresse...{{dropped}}



From ripley at stats.ox.ac.uk  Mon Jan 10 20:45:22 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 10 Jan 2005 19:45:22 +0000 (GMT)
Subject: [R] dyn.load Excel add-in on Windows XP and XLCall32.dll
In-Reply-To: <12AE52872B5C5348BE5CF47C707FF53A422B35@rhosvr02.rhotrading.com>
References: <12AE52872B5C5348BE5CF47C707FF53A422B35@rhosvr02.rhotrading.com>
Message-ID: <Pine.LNX.4.61.0501101943170.15431@gannet.stats>

Use pedump -i to find the depedencies (see README.packages).

This is a Windows issue, and you will need to seek advice on a Windows 
forum.  That message is coming (as it says) from the Windows routine
LoadLibrary.


On Mon, 10 Jan 2005 davidr at rhotrading.com wrote:

> I am on Win XP SP2 using R 2.0.1 patched.
> I could not find anything apropos in the archives or with search engines.
> (But maybe I need to be taught how to search better.)
>
> I am trying to dyn.load some commercial DLLs (Excel add-ins),
> but I get a message box saying:
>
> "This application has failed to start because XLCall32.dll was not found."
>
> The message in R is:
>
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>        unable to load shared library "<correct path>":
>  LoadLibrary failure:  The specified module could not be found.
>
> The DLLs work in Excel without my system having XLCall32.dll anywhere;
> (I believe it was removed from 2000 and XP for security reasons.)
> So possibly there is no real dependency there? (Cygwin strings command
> does find that word in the DLLs, though.)
>
> Is there a workaround for this, do I need to write an R-compatible wrapper,
> or is it just impossible?
>
> (I was able to load other DLLs (non-Excel add-ins), just as a test.)
>
> Thanks for any pointers!
>
> David L. Reiner
> Rho Trading
> Chicago? IL? 60605
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From dmb at mrc-dunn.cam.ac.uk  Mon Jan 10 21:00:33 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Mon, 10 Jan 2005 20:00:33 +0000 (GMT)
Subject: [R] Graphical table in R
In-Reply-To: <00dc01c4f73b$e970ee40$6c00a8c0@mtd4>
Message-ID: <Pine.LNX.4.21.0501101957560.15099-100000@mail.mrc-dunn.cam.ac.uk>


Cheers. This is really me just being lazy (as usual). The latex function
in Hmisc allows me to make a .ps file then grab a screen shot of that ps
and make a .png file.

I would just like to use plot so I can wrap it in a png command and not
have to use the 'screen shot' in between.

I can use convert bleah.ps blah.png, but I would like the image to be only
exacly as big as it needs to be. I havent tried auto crop yet...

Thanks all for replies,

All the best,
Dan.

On Mon, 10 Jan 2005, Anne wrote:

>Well I saw the title "graphical table..." and had just a cursory glance
>However I do use this function which is good if not too many categories are
>represented.
>
>Anne
>
>
>----- Original Message ----- 
>From: "Marc Schwartz" <MSchwartz at MedAnalytics.com>
>To: "Anne" <anne.piotet at urbanet.ch>
>Sent: Monday, January 10, 2005 5:54 PM
>Subject: Re: [R] Graphical table in R
>
>
>> On Mon, 2005-01-10 at 17:14 +0100, Anne wrote:
>> > symbol.freq in Hmisc
>> >
>> > Anne
>>
>> Hi Anne,
>>
>> Thanks for pointing that out, though that function is really a graphical
>> representation of a contingency table. It does not print out summary
>> details, etc. which is what I believe that Dan is looking for.
>>
>> Thanks,
>>
>> Marc
>>
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rolf at math.unb.ca  Mon Jan 10 21:11:18 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 10 Jan 2005 16:11:18 -0400 (AST)
Subject: [R] Query: simple autocorrelation ina time series
Message-ID: <200501102011.j0AKBIO7002100@erdos.math.unb.ca>


?arima.sim



From Ted.Harding at nessie.mcc.ac.uk  Mon Jan 10 21:07:55 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 10 Jan 2005 20:07:55 -0000 (GMT)
Subject: [R] Mixing portrait/landscape in a postscript file
In-Reply-To: <Pine.LNX.4.61.0501101630570.18574@gannet.stats>
Message-ID: <XFMail.050110200755.Ted.Harding@nessie.mcc.ac.uk>

On 10-Jan-05 Prof Brian Ripley wrote:
> On Mon, 10 Jan 2005, Yves Brostaux wrote:
>>> [...]
>>> trellis.device(postscript, theme=canonical.theme("postscript",
>>> color=F), 
>> file="an_phase2_graph.ps", paper="a4", pointsize = 10, onefile=TRUE, 
>> horizontal=TRUE)
>>
>> Everything works fine,but some of these plots shoud be printed
>> in a portrait orientation to look better. I tried to include
>> [...] par.settings = list(horizontal=FALSE) [...] in the lattice
>> plots arguments, but with no success. Is there a way to mix
>> different page orientation in a single postscript device that
>> I missed ?
> 
> No.  It is hard-coded in the header of the file.
> 
> It is technically impossible in postscript, but what one could do
> is to rotate the coord system on individual pages so all the pages
> were portrait or landscape.  I believe grid viewports can do that
> for you.

Do you mean "tecnically impossible to achieve it directly using
an R postscript device"? Because it is certainly possible to have
a PostScript document in which some pages are portrait and some
are landscape. And -- with appropriate DSC comments at the right
places in the file -- these can be viewed correctly (i.e. the
text always horizontal) using standard PS viewers; and they can
be correctly converted to PDF to be viewed likewise.

I could follow this up privately if anyone is interested: the
details would not be appropriate for posting to the list!

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 10-Jan-05                                       Time: 20:07:55
------------------------------ XFMail ------------------------------



From p.dalgaard at biostat.ku.dk  Mon Jan 10 22:01:08 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Jan 2005 22:01:08 +0100
Subject: [R] Graphical table in R
In-Reply-To: <Pine.LNX.4.21.0501101957560.15099-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0501101957560.15099-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <x26525yp63.fsf@biostat.ku.dk>

Dan Bolser <dmb at mrc-dunn.cam.ac.uk> writes:

> Cheers. This is really me just being lazy (as usual). The latex function
> in Hmisc allows me to make a .ps file then grab a screen shot of that ps
> and make a .png file.
> 
> I would just like to use plot so I can wrap it in a png command and not
> have to use the 'screen shot' in between.

A screen shot of a ps file? That sounds ... weird. If you can view it,
presumably you have Ghostscript and that can do png files.

However, will textual output do?

  plot(0,type="n",axes=FALSE, xlab="", ylab="")
  con <- textConnection("txt","w")
  sink(con); ftable(UCBAdmissions); sink()
  close(con)
  par(family="mono")
  text(1,0,paste(txt,collapse="\n"))


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From bolker at zoo.ufl.edu  Mon Jan 10 23:52:43 2005
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Mon, 10 Jan 2005 17:52:43 -0500 (EST)
Subject: [R] mle() and with()
Message-ID: <Pine.LNX.4.61.0501101745430.5948@bolker.zoo.ufl.edu>


   I'm trying to figure out the best way of fitting the same negative 
log-likelihood function to more than one set of data, using mle() from the 
stats4 package.

Here's what I would have thought would work:

--------------
library(stats4)

## simulate values
r = rnorm(1000,mean=2)

## very basic neg. log likelihood function
mll <- function(mu,logsigma) {
   -sum(dnorm(r,mean=mu,sd=exp(logsigma),log=TRUE))
}


mle(minuslogl=mll,start=list(mu=1,logsigma=0))

r2 = rnorm(1000,mean=3) ## second "data set"
with(list(r=r2),
      mle(minuslogl=mll,start=list(mu=1,logsigma=0))
    )
-------------

but this doesn't work -- it fits to the original data set, not the new one 
--- presumably because mll() picks up its definition of r when it is 
*defined* -- so using with() at this point doesn't help.

   If I rm(r) then I get an 'Object "r" not found' error.


I can do something like the following, defining the negative 
log-likelihood function within the mle() call ...

lf = function(data) {
   mle(minuslogl=function(mu,logsigma) {
     -sum(dnorm(data,mean=mu,sd=exp(logsigma),log=TRUE))
   },start=list(mu=1,logsigma=0))
}

lf(r)
lf(r2)

-------

  ... and in this case there's no point using with().
  can someone help me understand this behavior and to find a clean way to 
use mle() on a predefined likelihood function that allows substitution of 
an arbitrary data set?

   R 2.0.0 on Gentoo (trying to stick with the package management system so 
haven't installed 2.0.1 yet)


  thanks,
    Ben Bolker

-- 
620B Bartram Hall                            bolker at zoo.ufl.edu
Zoology Department, University of Florida    http://www.zoo.ufl.edu/bolker
Box 118525                                   (ph)  352-392-5697
Gainesville, FL 32611-8525                   (fax) 352-392-3704



From tlumley at u.washington.edu  Mon Jan 10 22:28:46 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 10 Jan 2005 13:28:46 -0800 (PST)
Subject: [R] Graphical table in R
In-Reply-To: <x26525yp63.fsf@biostat.ku.dk>
References: <Pine.LNX.4.21.0501101957560.15099-100000@mail.mrc-dunn.cam.ac.uk>
	<x26525yp63.fsf@biostat.ku.dk>
Message-ID: <Pine.A41.4.61b.0501101327370.272442@homer11.u.washington.edu>

On Mon, 10 Jan 2005, Peter Dalgaard wrote:

> A screen shot of a ps file? That sounds ... weird. If you can view it,
> presumably you have Ghostscript and that can do png files.
>
> However, will textual output do?
>
>  plot(0,type="n",axes=FALSE, xlab="", ylab="")
>  con <- textConnection("txt","w")
>  sink(con); ftable(UCBAdmissions); sink()
>  close(con)
>  par(family="mono")
>  text(1,0,paste(txt,collapse="\n"))
>

and the middle three lines could also be done with

    txt<-capture.output(ftable(UCBAdmissions))


 	-thomas



From ripley at stats.ox.ac.uk  Mon Jan 10 22:34:52 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 10 Jan 2005 21:34:52 +0000 (GMT)
Subject: [R] Mixing portrait/landscape in a postscript file
In-Reply-To: <XFMail.050110200755.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050110200755.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.61.0501102129360.16751@gannet.stats>

On Mon, 10 Jan 2005 Ted.Harding at nessie.mcc.ac.uk wrote:

> On 10-Jan-05 Prof Brian Ripley wrote:
>> On Mon, 10 Jan 2005, Yves Brostaux wrote:
>>>> [...]
>>>> trellis.device(postscript, theme=canonical.theme("postscript",
>>>> color=F),
>>> file="an_phase2_graph.ps", paper="a4", pointsize = 10, onefile=TRUE,
>>> horizontal=TRUE)
>>>
>>> Everything works fine,but some of these plots shoud be printed
>>> in a portrait orientation to look better. I tried to include
>>> [...] par.settings = list(horizontal=FALSE) [...] in the lattice
>>> plots arguments, but with no success. Is there a way to mix
>>> different page orientation in a single postscript device that
>>> I missed ?
>>
>> No.  It is hard-coded in the header of the file.
>>
>> It is technically impossible in postscript, but what one could do
>> is to rotate the coord system on individual pages so all the pages
>> were portrait or landscape.  I believe grid viewports can do that
>> for you.
>
> Do you mean "tecnically impossible to achieve it directly using
> an R postscript device"? Because it is certainly possible to have
> a PostScript document in which some pages are portrait and some
> are landscape. And -- with appropriate DSC comments at the right
> places in the file -- these can be viewed correctly (i.e. the
> text always horizontal) using standard PS viewers; and they can
> be correctly converted to PDF to be viewed likewise.

Yes (I used the name of the device, not the capitalized name of the 
language), because the orientation is set for the device, not the page.

> I could follow this up privately if anyone is interested: the
> details would not be appropriate for posting to the list!

You could always work it out and supply a patch against the R-devel 
sources.  There would need to be a way to specify the orientation of the 
current page, or probably the next page.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Mon Jan 10 22:41:39 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Jan 2005 22:41:39 +0100
Subject: [R] mle() and with()
In-Reply-To: <Pine.LNX.4.61.0501101745430.5948@bolker.zoo.ufl.edu>
References: <Pine.LNX.4.61.0501101745430.5948@bolker.zoo.ufl.edu>
Message-ID: <x21xctynak.fsf@biostat.ku.dk>

Ben Bolker <bolker at zoo.ufl.edu> writes:

>    I'm trying to figure out the best way of fitting the same negative
> log-likelihood function to more than one set of data, using mle() from
> the stats4 package.

It's not the same likelihood function if the data differ, since
likelihood functions are  functions of the parameters only. The design
of mle() & friends is so as to reinforce that idea.
 
> Here's what I would have thought would work:
> 
> --------------
> library(stats4)
> 
> ## simulate values
> r = rnorm(1000,mean=2)
> 
> ## very basic neg. log likelihood function
> mll <- function(mu,logsigma) {
>    -sum(dnorm(r,mean=mu,sd=exp(logsigma),log=TRUE))
> }
> 
> 
> mle(minuslogl=mll,start=list(mu=1,logsigma=0))
> 
> r2 = rnorm(1000,mean=3) ## second "data set"
> with(list(r=r2),
>       mle(minuslogl=mll,start=list(mu=1,logsigma=0))
>     )
> -------------
> 
> but this doesn't work -- it fits to the original data set, not the new
> one --- presumably because mll() picks up its definition of r when it
> is *defined* -- so using with() at this point doesn't help.
> 
>    If I rm(r) then I get an 'Object "r" not found' error.
> 
> 
> I can do something like the following, defining the negative
> log-likelihood function within the mle() call ...
> 
> lf = function(data) {
>    mle(minuslogl=function(mu,logsigma) {
>      -sum(dnorm(data,mean=mu,sd=exp(logsigma),log=TRUE))
>    },start=list(mu=1,logsigma=0))
> }
> 
> lf(r)
> lf(r2)
> 
> -------
> 
>   ... and in this case there's no point using with().
>   can someone help me understand this behavior and to find a clean way
> to use mle() on a predefined likelihood function that allows
> substitution of an arbitrary data set?

I'd do 

mll <- function(data) function(mu,logsigma) 
    -sum(dnorm(data,mean=mu,sd=exp(logsigma),log=TRUE))

fit <- mle(minuslogl = mll(r), start = list(mu=1, logsigma=0))
fit2 <- mle(minuslogl = mll(r2), start = list(mu=1, logsigma=0))

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From apjaworski at mmm.com  Mon Jan 10 23:09:51 2005
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Mon, 10 Jan 2005 16:09:51 -0600
Subject: [R] Graphical table in R
In-Reply-To: <Pine.A41.4.61b.0501101327370.272442@homer11.u.washington.edu>
Message-ID: <OF79D55A18.9D2ECF2A-ON86256F85.00795E58-86256F85.0079C091@mmm.com>






The outer two lines could also be done more easily (?) by:

library(gplots)  #part of gregmisc bundle
textplot(txt)

Check ?textplot for additional options.

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


                                                                           
             Thomas Lumley                                                 
             <tlumley at u.washin                                             
             gton.edu>                                                  To 
             Sent by:                  Peter Dalgaard                      
             r-help-bounces at st         <p.dalgaard at biostat.ku.dk>          
             at.math.ethz.ch                                            cc 
                                       MSchwartz at medanalytics.com, R list  
                                       <r-help at stat.math.ethz.ch>          
             01/10/2005 03:28                                      Subject 
             PM                        Re: [R] Graphical table in R        
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




On Mon, 10 Jan 2005, Peter Dalgaard wrote:

> A screen shot of a ps file? That sounds ... weird. If you can view it,
> presumably you have Ghostscript and that can do png files.
>
> However, will textual output do?
>
>  plot(0,type="n",axes=FALSE, xlab="", ylab="")
>  con <- textConnection("txt","w")
>  sink(con); ftable(UCBAdmissions); sink()
>  close(con)
>  par(family="mono")
>  text(1,0,paste(txt,collapse="\n"))
>

and the middle three lines could also be done with

    txt<-capture.output(ftable(UCBAdmissions))


             -thomas

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tring at gvdnet.dk  Mon Jan 10 23:30:44 2005
From: tring at gvdnet.dk (Troels Ring)
Date: Mon, 10 Jan 2005 23:30:44 +0100
Subject: [R] defining lower part of distribution with covariate
Message-ID: <5.2.0.9.0.20050110230110.037441c0@home.gvdnet.dk>

Dear friends, forgive me a simple question, possibly related to quantreg 
but I failed to get it done and hope for basic instruction.

I have two sets of observed Glasgow coma scores at admission to ICU after 
operation, and accompanying time of anesthesia (in hours).
Thio is cheap and perhaps old fashioned, and ultiva expensive and rapidly 
terminated. The problem is to estimate the probability of GCS 12 or lower 
on the two treatments after taking time of anesthesia into account (antime) 
which is longer for thio. How would I do that in the best way ?

Best wishes
Troels Ring, MD
Aalborg, Denmark


thio
       GCS antime
  [1,]  14    4.5
  [2,]  15    7.5
  [3,]  11    7.5
  [4,]  15    4.5
  [5,]  14    4.5
  [6,]  15    3.5
  [7,]  15    5.5
  [8,]  14    5.5
  [9,]  15    3.5
[10,]  14    8.5
[11,]  13    4.5
[12,]  12    5.5
[13,]  15    3.5
[14,]  13    6.5
[15,]   9    8.5
[16,]  15    6.5
 > ultiva
       GCS antime
  [1,]  15    4.5
  [2,]  15    4.5
  [3,]  15    2.5
  [4,]  15    3.5
  [5,]  15    3.5
  [6,]  12    5.5
  [7,]  15    4.5
  [8,]  15    3.5
  [9,]  15    8.5
[10,]  13    4.5
[11,]  14    3.5
[12,]  14    4.5
[13,]  15    4.5
[14,]  14    2.5
[15,]  15    4.5
[16,]  15    3.5
[17,]  15    3.5
[18,]  14    4.5
[19,]  14    4.5
[20,]  15    4.5



From abeuchat at storebyte.com  Mon Jan 10 23:56:19 2005
From: abeuchat at storebyte.com (Antoine Beuchat)
Date: Mon, 10 Jan 2005 23:56:19 +0100
Subject: [R] Stadard errors and boxplots with 632plus error estimator,
	"errorest"
Message-ID: <200501102256.j0AMuZl4025968@hypatia.math.ethz.ch>

Dear R-users, 
 
I'd like to estimate standard errors (for lda) and make a boxplot with the
"632plus" and "boot" error estimators included in package ipred (method:
errorest). The "boot" estimator returns only a standard deviation but not
the whole error data.

Thank you in advance,
regards, 
Antoine



From talitaperciano at hotmail.com  Tue Jan 11 00:08:37 2005
From: talitaperciano at hotmail.com (Talita Leite)
Date: Mon, 10 Jan 2005 23:08:37 +0000
Subject: [R] Data Set
Message-ID: <BAY14-F72F45D50905092BE69EC8C7970@phx.gbl>

Hi everybody,

I'm studying descriptive statistics using R and I have to make an important 
work about that. I need some help to choose a good data set to apply those 
statistics. Does anybody know a good data set I could work with?

Thanx,


Talita Perciano Costa Leite
Graduanda em Ci?ncia da Computa??o
Universidade Federal de Alagoas - UFAL
Departamento de Tecnologia da Informa??o - TCI
Constru??o de Conhecimento por Agrupamento de Dados - CoCADa



From murthys at us.ibm.com  Tue Jan 11 00:15:46 2005
From: murthys at us.ibm.com (Sridhar Murthy)
Date: Mon, 10 Jan 2005 17:15:46 -0600
Subject: [R] Help requested .....
Message-ID: <OFA403662D.D792B600-ON86256F85.007BF16B-86256F85.007FC81F@us.ibm.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050110/378a9f00/attachment.pl

From choudary.jagar at swosu.edu  Tue Jan 11 00:16:27 2005
From: choudary.jagar at swosu.edu (Jagarlamudi, Choudary)
Date: Mon, 10 Jan 2005 17:16:27 -0600
Subject: [R] How to obtain nls parameter estimates
Message-ID: <E03EBB50FF2C024781A6E4460AD58F0607C14A@swosu-mbx01.admin.swosu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050110/06f75133/attachment.pl

From spencer.graves at pdf.com  Tue Jan 11 00:35:03 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 10 Jan 2005 15:35:03 -0800
Subject: [R] Data Set
In-Reply-To: <BAY14-F72F45D50905092BE69EC8C7970@phx.gbl>
References: <BAY14-F72F45D50905092BE69EC8C7970@phx.gbl>
Message-ID: <41E31127.8070206@pdf.com>

      Much of the documentation including manuals, help files and other 
contributed descriptions include same data sets. 

      What kinds of applications and techniques most interest you?  That 
with the posting guide (http://www.R-project.org/posting-guide.html, 
especially the search at "www.r-project.org") might lead you to suitable 
examples. 

      hope this helps. 
      spencer graves

Talita Leite wrote:

> Hi everybody,
>
> I'm studying descriptive statistics using R and I have to make an 
> important work about that. I need some help to choose a good data set 
> to apply those statistics. Does anybody know a good data set I could 
> work with?
>
> Thanx,
>
>
> Talita Perciano Costa Leite
> Graduanda em Ci?ncia da Computa??o
> Universidade Federal de Alagoas - UFAL
> Departamento de Tecnologia da Informa??o - TCI
> Constru??o de Conhecimento por Agrupamento de Dados - CoCADa
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Tue Jan 11 00:36:38 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 10 Jan 2005 18:36:38 -0500
Subject: [R] How to obtain nls parameter estimates
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4FC@usrymx25.merck.com>

Use coefficients(nlsobject), or if you want standard errors, t-statistics,
and p-values, summary(nlsobject)$paramters.

Andy

> From: Jagarlamudi, Choudary
> 
> How can I receive parameter estimates for a given curve that 
> has been fit with the NLS function.  
>  
> The problem is that I have a "n parameter" curve and I want 
> the optimal fit.  The NLS procedure gives me final function 
> values and not the individual parameter estimates that were 
> used to define this "best" fit.  What function can I use to 
> get these parameters?
>  
> Thanks  
>  
> Choudary Jagarlamudi
> Instructor
> Southwestern Oklahoma State University
> STF 254
> 100 campus Drive
> Weatherford OK 73096
> Tel 580-774-7136
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From talitaperciano at hotmail.com  Tue Jan 11 00:45:12 2005
From: talitaperciano at hotmail.com (Talita Leite)
Date: Mon, 10 Jan 2005 23:45:12 +0000
Subject: [R] Data Set
Message-ID: <BAY14-F25461AB7B83A1CD7497BA6C7970@phx.gbl>

Hi,

I'll try to be more specific asking my question. I want to apply some 
functions like mean(), median(), var(), sd(), mad(), quantile(), kurtosis(), 
skewness() and make some graphics like boxplot, barplot, histogram, stars... 
In order to do that I need a simple data set, simple but interesting.

Thanx,


Talita Perciano Costa Leite
Graduanda em Ci?ncia da Computa??o
Universidade Federal de Alagoas - UFAL
Departamento de Tecnologia da Informa??o - TCI
Constru??o de Conhecimento por Agrupamento de Dados - CoCADa



From andy_liaw at merck.com  Tue Jan 11 01:57:06 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 10 Jan 2005 19:57:06 -0500
Subject: [R] Data Set
Message-ID: <3A822319EB35174CA3714066D590DCD50994E4FD@usrymx25.merck.com>

There are a few packages on CRAN that are collections of data sets, some
from intro textbooks.  You might find some of them suitable.

There are also datasets that come with R.  Type data() at the R prompt to
see a list.

Andy

> From: Talita Leite
> 
> Hi,
> 
> I'll try to be more specific asking my question. I want to apply some 
> functions like mean(), median(), var(), sd(), mad(), 
> quantile(), kurtosis(), 
> skewness() and make some graphics like boxplot, barplot, 
> histogram, stars... 
> In order to do that I need a simple data set, simple but interesting.
> 
> Thanx,
> 
> 
> Talita Perciano Costa Leite
> Graduanda em Ci?ncia da Computa??o
> Universidade Federal de Alagoas - UFAL
> Departamento de Tecnologia da Informa??o - TCI
> Constru??o de Conhecimento por Agrupamento de Dados - CoCADa
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Andrew.Ward at qsa.qld.edu.au  Tue Jan 11 02:18:22 2005
From: Andrew.Ward at qsa.qld.edu.au (Andrew Ward)
Date: Tue, 11 Jan 2005 11:18:22 +1000
Subject: [R] Windows package development: bad html links to functions in
	non-standard packages
Message-ID: <86EED55AE819274B8308B62A225457D8016B8696@exch1.qsa.local>

Thank you to Professor Ripley for clearing this up for me.
Embarrassingly, I read this FAQ several times, completely
missing the point each time.

Regards,

Andrew C. Ward,                
Senior Analyst (Quantitative), Tel: +61 7 3864 0439
Queensland Studies Authority,  Fax: +61 7 3229 3318
295 Ann Street,
Brisbane Qld 4000, Australia

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: Friday, 7 January 2005 5:54 PM
To: Andrew Ward
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Windows package development: bad html links to
functions in non-standard packages


This is discussed in the documentation.  E.g. rw-FAQ says

3.7 Hyperlinks in Compiled HTML sometimes do not work.
======================================================

They may well not work between packages installed in different
libraries.  This is solved under Unix using symbolic links which Windows
does not implement.

and the same is true (but less so) for HTML.

And ?link.html.help says

      Cross-library links do not work on this platform.
      'fixup.package.URLs' attempts to correct links in the named
      package to the 'doc' directory (usually to icons) and to the
      'base', 'utils', 'graphics' and 'stats' packages, and then stamps
      a file 'fixedHTMLlinks' in the package directory.


On Fri, 7 Jan 2005, Andrew Ward wrote:

> I am using R 2.0.1 Patched on Windows 2000.
>
> I have created a binary package for Windows that builds, checks,
> installs and works without errors. In some of my .Rd files, I
> have links to functions in the standard packages supplied with R
> as well as links to others in add-on packages that I have installed.
> For instance, one .Rd file has the following snippet:
>
> \seealso{ \code{\link[lattice]{xyplot}},
>          \code{\link[MASS]{rlm}},
>          \code{\link[RODBC]{odbcConnect}}}
>
> I have installed RODBC in a separate library, pointed to by
> R_LIBS. The HTML links to xyplot and rlm work fine, but those to
> packages not in the standard library can't be found (such as to
> odbcConnect above). The link points to a non-existent file in the
> main library (file:///c:/r/library/RODBC/html/odbcConnect.html)
> rather than to the existing file in R_LIBS
> (z:/r/library/RODBC/html/odbcConnect.html).

Actually not: it points to a relative link, something like 
../../../RODBC/html/odbcConnect.html.

> I would be very grateful if someone could point out what I have
> carelessly overlooked in endeavouring to link to HTML files in both
> the main and in non-standard packages libraries.

Only the statement that it does not work.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
This email (including any attached files) is for the intende...{{dropped}}



From thopper106035 at comcast.net  Tue Jan 11 04:45:56 2005
From: thopper106035 at comcast.net (Thomas Hopper)
Date: Mon, 10 Jan 2005 22:45:56 -0500
Subject: [R] Calculate Mean of Column Vectors?
Message-ID: <4C910DFA-6383-11D9-83F5-000D933268AE@comcast.net>

Hello,

I've got an array defined as y <- rnorm(3000), dim(y) <- c(3, 1000).

I'd like to produce a 1000-element vector z that is the mean of the 
corresponding elements of y (like z[1,1] <- mean(y[1,1], y[2,1], 
y[3,1])), but being new to R, I'm not sure how to do this for all 
elements at once (or, at least, simply). Any help is appreciated.

Thanks,

Tom



From MSchwartz at MedAnalytics.com  Tue Jan 11 05:03:36 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 10 Jan 2005 22:03:36 -0600
Subject: [R] Calculate Mean of Column Vectors?
In-Reply-To: <4C910DFA-6383-11D9-83F5-000D933268AE@comcast.net>
References: <4C910DFA-6383-11D9-83F5-000D933268AE@comcast.net>
Message-ID: <1105416216.9251.21.camel@horizons.localdomain>

On Mon, 2005-01-10 at 22:45 -0500, Thomas Hopper wrote:
> Hello,
> 
> I've got an array defined as y <- rnorm(3000), dim(y) <- c(3, 1000).
> 
> I'd like to produce a 1000-element vector z that is the mean of the 
> corresponding elements of y (like z[1,1] <- mean(y[1,1], y[2,1], 
> y[3,1])), but being new to R, I'm not sure how to do this for all 
> elements at once (or, at least, simply). Any help is appreciated.
> 
> Thanks,
> 
> Tom

# You can create 'y' in one step here
y <- matrix(rnorm(3000), ncol = 1000)

# get the column means
z <- colMeans(y)

> str(z)
 num [1:1000] -0.5664  0.8232 -0.0138 -0.5511  1.0224 ...


See ?colMeans for more info.


HTH,

Marc Schwartz



From tfliao at uiuc.edu  Tue Jan 11 05:08:53 2005
From: tfliao at uiuc.edu (Tim F Liao)
Date: Mon, 10 Jan 2005 22:08:53 -0600
Subject: [R] Calculate Mean of Column Vectors?
Message-ID: <d33aabde.bccba763.8199800@expms6.cites.uiuc.edu>

Tom,

I often use a loop, such as:

>m <- rep(0,1000)
> for (i in 1:1000)
+ m[i]<-mean(y[,i])
> m[1:20]
 [1] -0.04914724 -0.28253861 -0.31112690 -0.18034371 
0.18839167  0.66448244
 [7]  0.19769017 -1.28363405 -0.05167451 -0.95492534
-1.23285174  0.10288562
[13] -0.73792584 -0.19297468 -0.59059036 -0.11870173 
0.38285449  1.19154411
[19]  0.34663980  0.21322554

But there may be more efficient ways to accomplish this.

Tim


---- Original message ----
>Date: Mon, 10 Jan 2005 22:45:56 -0500
>From: Thomas Hopper <thopper106035 at comcast.net>  
>Subject: [R] Calculate Mean of Column Vectors?  
>To: r-help at stat.math.ethz.ch
>
>Hello,
>
>I've got an array defined as y <- rnorm(3000), dim(y) <- c(3,
1000).
>
>I'd like to produce a 1000-element vector z that is the mean
of the 
>corresponding elements of y (like z[1,1] <- mean(y[1,1], y[2,1], 
>y[3,1])), but being new to R, I'm not sure how to do this for
all 
>elements at once (or, at least, simply). Any help is appreciated.
>
>Thanks,
>
>Tom
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Tom.Mulholland at dpi.wa.gov.au  Tue Jan 11 05:10:09 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 11 Jan 2005 12:10:09 +0800
Subject: [R] Calculate Mean of Column Vectors?
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3C953@afhex01.dpi.wa.gov.au>

apply(y,2,mean)

Is this what you are after. If it is I would suggest that you look at the examples not just for this but for what I call the apply family sapply, tapply, mapply. Once you get the hang of these they are really helpful.

Tom.

> -----Original Message-----
> From: Thomas Hopper [mailto:thopper106035 at comcast.net]
> Sent: Tuesday, 11 January 2005 11:46 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Calculate Mean of Column Vectors?
> 
> 
> Hello,
> 
> I've got an array defined as y <- rnorm(3000), dim(y) <- c(3, 1000).
> 
> I'd like to produce a 1000-element vector z that is the mean of the 
> corresponding elements of y (like z[1,1] <- mean(y[1,1], y[2,1], 
> y[3,1])), but being new to R, I'm not sure how to do this for all 
> elements at once (or, at least, simply). Any help is appreciated.
> 
> Thanks,
> 
> Tom
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Simon.Blomberg at anu.edu.au  Tue Jan 11 05:10:29 2005
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Tue, 11 Jan 2005 15:10:29 +1100
Subject: [R] Calculate Mean of Column Vectors?
In-Reply-To: <4C910DFA-6383-11D9-83F5-000D933268AE@comcast.net>
References: <4C910DFA-6383-11D9-83F5-000D933268AE@comcast.net>
Message-ID: <a06110400be0901ad7d2f@[150.203.51.113]>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050111/80b9b81a/attachment.pl

From spencer.graves at pdf.com  Tue Jan 11 05:14:18 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 10 Jan 2005 20:14:18 -0800
Subject: [R] Calculate Mean of Column Vectors?
In-Reply-To: <4C910DFA-6383-11D9-83F5-000D933268AE@comcast.net>
References: <4C910DFA-6383-11D9-83F5-000D933268AE@comcast.net>
Message-ID: <41E3529A.7040807@pdf.com>

      Have you considered the following: 

    apply(y, 1, mean)

      Alternatively: 
          
y.means <- rep(NA, 3)
for(i in 1:3)
  y.means[i] <- mean(y[i,])

      hope this helps.  spencer graves

Thomas Hopper wrote:

> Hello,
>
> I've got an array defined as y <- rnorm(3000), dim(y) <- c(3, 1000).
>
> I'd like to produce a 1000-element vector z that is the mean of the 
> corresponding elements of y (like z[1,1] <- mean(y[1,1], y[2,1], 
> y[3,1])), but being new to R, I'm not sure how to do this for all 
> elements at once (or, at least, simply). Any help is appreciated.
>
> Thanks,
>
> Tom
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From hb at maths.lth.se  Tue Jan 11 05:17:05 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 11 Jan 2005 05:17:05 +0100
Subject: [R] Calculate Mean of Column Vectors?
In-Reply-To: <4C910DFA-6383-11D9-83F5-000D933268AE@comcast.net>
Message-ID: <001401c4f794$6995c520$4c0040d5@hblaptop>

See ?apply. Example

y <- rnorm(3000), dim(y) <- c(3, 1000)
my <- apply(y, MARGIN=2, FUN=mean, na.rm=TRUE) 

to calculate the mean over the 2nd index, that is, for each column, and pass
argument na.rm=TRUE to each call to mean(). This is in practice the same as

my <- rep(NA, ncol(y))
for (kk in 1:ncol(y)) my[kk] <- mean(y[,kk], na.rm=TRUE)

but apply() is to prefer when you get used to it.

Best wishes

Henrik Bengtsson

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas Hopper
> Sent: Tuesday, January 11, 2005 4:46 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Calculate Mean of Column Vectors?
> 
> 
> Hello,
> 
> I've got an array defined as y <- rnorm(3000), dim(y) <- c(3, 1000).
> 
> I'd like to produce a 1000-element vector z that is the mean of the 
> corresponding elements of y (like z[1,1] <- mean(y[1,1], y[2,1], 
> y[3,1])), but being new to R, I'm not sure how to do this for all 
> elements at once (or, at least, simply). Any help is appreciated.
> 
> Thanks,
> 
> Tom
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From flavio.zanini at epfl.ch  Tue Jan 11 06:49:56 2005
From: flavio.zanini at epfl.ch (flavio.zanini@epfl.ch)
Date: Tue, 11 Jan 2005 06:49:56 +0100
Subject: [R] problem with fitted probability in glm
Message-ID: <1105422596.41e369044e3f8@imapwww.epfl.ch>

Good morning,

Sorry to disturb you.

I have a set of variable (not correlated) and with a stepwise selection
procedure (step) on my multiple glm I obtain a result but with a warning message
saying:

fitted probabilities numerically 0 or 1 occurred in: glm.fit(x[, jj, drop =
FALSE], y, wt, offset = object$offset,

I try to understand what it means without success. Could you help me?

Thanks

Flavio Zanini



From Tom.Mulholland at dpi.wa.gov.au  Tue Jan 11 07:23:44 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 11 Jan 2005 14:23:44 +0800
Subject: [R] problem with fitted probability in glm
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA61@afhex01.dpi.wa.gov.au>


http://finzi.psych.upenn.edu/R/Rhelp02a/archive/29712.html
This was the first message that I came across when searching the archives. From my limited experience I would think that you need to understand what you are doing rather than what R is doing. As the message explains R is flagging issues with your data.

Tom

> -----Original Message-----
> From: flavio.zanini at epfl.ch [mailto:flavio.zanini at epfl.ch]
> Sent: Tuesday, 11 January 2005 1:50 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] problem with fitted probability in glm
> 
> 
> Good morning,
> 
> Sorry to disturb you.
> 
> I have a set of variable (not correlated) and with a stepwise 
> selection
> procedure (step) on my multiple glm I obtain a result but 
> with a warning message
> saying:
> 
> fitted probabilities numerically 0 or 1 occurred in: 
> glm.fit(x[, jj, drop =
> FALSE], y, wt, offset = object$offset,
> 
> I try to understand what it means without success. Could you help me?
> 
> Thanks
> 
> Flavio Zanini
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From mark0060 at tc.umn.edu  Tue Jan 11 07:43:20 2005
From: mark0060 at tc.umn.edu (Kristian Eric Markon)
Date: Tue, 11 Jan 2005 00:43:20 -0600
Subject: [R] suse 9.1 x86_64 rpms?
In-Reply-To: <x2d5wfpvyx.fsf@biostat.ku.dk>
References: <41E02630.5000908@tc.umn.edu> <x2d5wfpvyx.fsf@biostat.ku.dk>
Message-ID: <41E37588.5010100@tc.umn.edu>

Thanks for the help.

You were all right--I was missing the -devel versions of a few of the 
packages.

Everything seems to be working well in 64-bit so far.



Peter Dalgaard wrote:

>Kristian Eric Markon <mark0060 at tc.umn.edu> writes:
>
>  
>
>>I noticed on the mailing list archives and through Google searches
>>that a couple of months ago there was discussion of maintaining rpms
>>for suse 9.1 on the x86_64 architecture.
>>
>>Are these rpms still planning on being released? It would help me a
>>great deal.
>>    
>>
>
>Detlef Steuer is the only one to know...
> 
>  
>
>>I've tried compiling from source using the R-base spec files provided
>>on CRAN. Using that, I am able to produce a running version of R, but
>>readline doesn't seem to be working, nor does x11(). Moreover, I'm not
>>sure that the compiled version is actually running in 64-bit mode.
>>I've double-checked all the required libraries, and I seem to have
>>them all to the correct version number (including readline and xfree).
>>    
>>
>
>It'll be 64-bit alright unless you take explicit steps to the contrary.
>If you want to make d*mn sure, calculate the size of an Ncell from the
>gc() output (56 bytes on 64bit 28 bytes otherwise).
>
>Compiling on SuSE 9.1 *does* work, but you seem to be missing a couple
>of RPMs in your installation. Usual suspects are the ones that end
>with -devel, such as
>
>alsa-devel-1.0.3-36
>db1-devel-1.85-78
>esound-devel-0.2.33-30
>fontconfig-devel-2.2.92.20040221-24
>freetype2-devel-2.1.7-46
>glibc-devel-2.3.3-63
>glib-devel-1.2.10-337
>gnome-libs-devel-1.4.1.7-614
>gtk-devel-1.2.10-488
>ImageMagick-devel-5.5.7-225.9
>imlib-devel-1.9.14-180.11
>libglade2-devel-2.0.1-437
>libglade-devel-0.17-190
>libpng-devel-1.2.5-182.10
>libstdc++-devel-3.3.3-33
>libxml2-devel-2.6.7-28.7
>libxml-devel-1.8.17-366.4
>mozilla-devel-1.6-53
>ncurses-devel-5.4-61.3
>orbit-devel-0.5.17-330
>pstoedit-devel-3.33-161
>readline-devel-4.3-301
>tcl-devel-8.4.6-23
>tk-devel-8.4.6-28
>xforms-devel-1.0-259
>XFree86-devel-32bit-9.1-200404070910
>XFree86-devel-4.3.99.902-30
>XFree86-Mesa-devel-4.3.99.902-30
>zlib-devel-1.2.1-70.6
>
>Those are what I have, some are irrelevant, but at least the readline,
>ncurses, XFree86, and tcl/tk are crucial for compiling a "normal" R,
>and several of the others are needed for Gnome support, the Mesa stuff
>for RGL, etc. (Hmm... looks like I'm actually missing the bzip2
>headers myself).
>
>  
>



From ligges at statistik.uni-dortmund.de  Tue Jan 11 08:56:55 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 11 Jan 2005 08:56:55 +0100
Subject: Please use colMeans()! was: Re: [R] Calculate Mean of Column Vectors?
In-Reply-To: <001401c4f794$6995c520$4c0040d5@hblaptop>
References: <001401c4f794$6995c520$4c0040d5@hblaptop>
Message-ID: <41E386C7.6020100@statistik.uni-dortmund.de>

Folks, please see ?colMeans (as already pointed out in at least one 
message) and friends.
These are faster than those apply(X, 2, mean) calls
and please do use them when publishing code (both in packages in on this 
list).

Uwe Ligges


>>Hello,
>>
>>I've got an array defined as y <- rnorm(3000), dim(y) <- c(3, 1000).
>>
>>I'd like to produce a 1000-element vector z that is the mean of the 
>>corresponding elements of y (like z[1,1] <- mean(y[1,1], y[2,1], 
>>y[3,1])), but being new to R, I'm not sure how to do this for all 
>>elements at once (or, at least, simply). Any help is appreciated.
>>
>>Thanks,
>>
>>Tom



From r.hankin at soc.soton.ac.uk  Tue Jan 11 09:13:24 2005
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Tue, 11 Jan 2005 08:13:24 +0000
Subject: [R] integrate()
Message-ID: <AA62097C-63A8-11D9-85C9-000A95D86AA8@soc.soton.ac.uk>

Hi

I found the following unexpected:

 > integrate(function(x){0*x+1+1i},0,1)
1 with absolute error < 1.1e-14
 >

One can write a little wrapper, but it's messy.  Would it be hard to
accommodate such functions?

The manpage for integrate() does not mention imaginary numbers.


--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From anne.piotet at urbanet.ch  Tue Jan 11 09:13:08 2005
From: anne.piotet at urbanet.ch (Anne)
Date: Tue, 11 Jan 2005 09:13:08 +0100
Subject: [R] Data Set
References: <BAY14-F72F45D50905092BE69EC8C7970@phx.gbl>
Message-ID: <002101c4f7b5$63d91990$6c00a8c0@mtd4>

You could be really classical and use the iris data!


have a look at:

http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/DataSets

the titanic dataset is a real classic one!

However it depends very much what you want to study:

Anne


----- Original Message ----- 
From: "Talita Leite" <talitaperciano at hotmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 11, 2005 12:08 AM
Subject: [R] Data Set


> Hi everybody,
>
> I'm studying descriptive statistics using R and I have to make an
important
> work about that. I need some help to choose a good data set to apply those
> statistics. Does anybody know a good data set I could work with?
>
> Thanx,
>
>
> Talita Perciano Costa Leite
> Graduanda em Ci?ncia da Computa??o
> Universidade Federal de Alagoas - UFAL
> Departamento de Tecnologia da Informa??o - TCI
> Constru??o de Conhecimento por Agrupamento de Dados - CoCADa
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From Tom.Mulholland at dpi.wa.gov.au  Tue Jan 11 09:51:46 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 11 Jan 2005 16:51:46 +0800
Subject: Please use colMeans()! was: Re: [R] Calculate Mean of Column
	Vectors?
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA62@afhex01.dpi.wa.gov.au>

There are indeed speed advantages in using colSums etc. However the disadvantage is that the newbie doesn't always find the power inherent in the apply, sapply, tapply and mapply. For many things that I do, the speed is the least of my worries; although I take the point that using apply for means or sums in packages that are distibuted to others is not the way to go. 

As many of us have found out (and I think it was in S Poetry) the statement was made that vectorisation befuddles some beginners. So learning how to use this command, on a nice easy topic such as summing or averaging does have some merit. I have to admit using colSums a lot, but I don't think I have ever thought to use colMeans. 

As an aside; does anyone have a list of optimised functions. That is functions like this one whose main benefit is speed (I think) I guess many of us feel that we are constantly using hammers to crack the nuts, but we still don't really know what's in the toolbox.

Tom

> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
> Sent: Tuesday, 11 January 2005 3:57 PM
> To: r-help at stat.math.ethz.ch
> Cc: 'Thomas Hopper'
> Subject: Please use colMeans()! was: Re: [R] Calculate Mean of Column
> Vectors?
> 
> 
> Folks, please see ?colMeans (as already pointed out in at least one 
> message) and friends.
> These are faster than those apply(X, 2, mean) calls
> and please do use them when publishing code (both in packages 
> in on this 
> list).
> 
> Uwe Ligges
> 
> 
> >>Hello,
> >>
> >>I've got an array defined as y <- rnorm(3000), dim(y) <- c(3, 1000).
> >>
> >>I'd like to produce a 1000-element vector z that is the mean of the 
> >>corresponding elements of y (like z[1,1] <- mean(y[1,1], y[2,1], 
> >>y[3,1])), but being new to R, I'm not sure how to do this for all 
> >>elements at once (or, at least, simply). Any help is appreciated.
> >>
> >>Thanks,
> >>
> >>Tom
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jason at flyonthewall.com  Tue Jan 11 10:02:31 2005
From: jason at flyonthewall.com (Jason Gleave)
Date: Tue, 11 Jan 2005 10:02:31 +0100
Subject: [R] automated response
Message-ID: <10501111002.AA6967565@flyonthewall.de>

Thank you for contacting me - I am travelling and away from 
the office between 7th-13th of January although I will be 
accessing email occasionally.

Please contact me on my mobile if the matter is urgent +44 
(0)7880 743 962  or in my absence please contact one of the 
following members team who will be hapy to assist:

Client enquiries and financial:
Peter Gleave 
Company Secretary and Finance Manager
peter at flyonthewall.com

Any technical matters:
Patrick Asch
CTO
patrick at flyonthewall.com

Any new business enquiries:
Brad Pianta-McGill
Head of Business Development
brad at flyonthewall.com

Thank you 

Best wishes and a Happy New Year

Jason Gleave
Chief Executive
FlyOnTheWall - the Internet broadcasting company
http://www.flyonthewall.com 
Tel: +44 (0)20 7381 5500
Fax: +44 (0)7092 381 401
Mob: +44 (0)7880 743 962



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Jan 11 10:13:57 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 11 Jan 2005 10:13:57 +0100
Subject: Please use colMeans()! was: Re: [R] Calculate Mean of
	ColumnVectors?
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA62@afhex01.dpi.wa.gov.au>
Message-ID: <010601c4f7bd$e199f720$0540210a@www.domain>

Hi Tom,

There is the "R Reference Card" by Tom Short and Rpad, which is 
extremely useful for a quick browsing of basic R functions. Check:

http://tolstoy.newcastle.edu.au/R/help/04/12/9637.html


For instance, colSums() and friends are in the "Matrices" section.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Mulholland, Tom" <Tom.Mulholland at dpi.wa.gov.au>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 11, 2005 9:51 AM
Subject: RE: Please use colMeans()! was: Re: [R] Calculate Mean of 
ColumnVectors?


> There are indeed speed advantages in using colSums etc. However the 
> disadvantage is that the newbie doesn't always find the power 
> inherent in the apply, sapply, tapply and mapply. For many things 
> that I do, the speed is the least of my worries; although I take the 
> point that using apply for means or sums in packages that are 
> distibuted to others is not the way to go.
>
> As many of us have found out (and I think it was in S Poetry) the 
> statement was made that vectorisation befuddles some beginners. So 
> learning how to use this command, on a nice easy topic such as 
> summing or averaging does have some merit. I have to admit using 
> colSums a lot, but I don't think I have ever thought to use 
> colMeans.
>
> As an aside; does anyone have a list of optimised functions. That is 
> functions like this one whose main benefit is speed (I think) I 
> guess many of us feel that we are constantly using hammers to crack 
> the nuts, but we still don't really know what's in the toolbox.
>
> Tom
>
>> -----Original Message-----
>> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
>> Sent: Tuesday, 11 January 2005 3:57 PM
>> To: r-help at stat.math.ethz.ch
>> Cc: 'Thomas Hopper'
>> Subject: Please use colMeans()! was: Re: [R] Calculate Mean of 
>> Column
>> Vectors?
>>
>>
>> Folks, please see ?colMeans (as already pointed out in at least one
>> message) and friends.
>> These are faster than those apply(X, 2, mean) calls
>> and please do use them when publishing code (both in packages
>> in on this
>> list).
>>
>> Uwe Ligges
>>
>>
>> >>Hello,
>> >>
>> >>I've got an array defined as y <- rnorm(3000), dim(y) <- c(3, 
>> >>1000).
>> >>
>> >>I'd like to produce a 1000-element vector z that is the mean of 
>> >>the
>> >>corresponding elements of y (like z[1,1] <- mean(y[1,1], y[2,1],
>> >>y[3,1])), but being new to R, I'm not sure how to do this for all
>> >>elements at once (or, at least, simply). Any help is appreciated.
>> >>
>> >>Thanks,
>> >>
>> >>Tom
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From michael.watson at bbsrc.ac.uk  Tue Jan 11 10:20:11 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Tue, 11 Jan 2005 09:20:11 -0000
Subject: [R] Installation of XML library can't find libxml2.dll
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E89AFD@iahce2knas1.iah.bbsrc.reserved>

Thank you for all the responses;  in fact, it was all solved by using
Bioconductor

library(reposTools)
install.packages2("XML")

However, I am still having heaps of trouble getting SSOAP, Rcurl and
KEGGSOAP to use my proxy server....

Mick

-----Original Message-----
From: dr mike [mailto:dr.mike at ntlworld.com] 
Sent: 10 January 2005 17:47
To: michael watson (IAH-C)
Cc: R-help at stat.math.ethz.ch
Subject: RE: [R] Installation of XML library can't find libxml2.dll


My reading of this is that you may have installed the 'Windows binary'
package from the Omegahat site - if so, it would appear that this is for
versions of R < 2.0 . Further to the pointer to the reply by Prof
Ripley, hisa link provides the correct Windows binary for R >= 2.0.

Regards

Mike

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of michael watson
(IAH-C)
Sent: 10 January 2005 14:41
To: R list
Subject: [R] Installation of XML library can't find libxml2.dll

Sorry to ask a (probably) dumb question, but I am trying to install XML
package on Windows XP, R 2.0.1, and I get the error: 

"This application has failed to start because libxml2.dll was not found.
Re-installing the application may fix this problem"

> library(XML)
Error in dyn.load(x, as.logical(local), as.logical(now)) : 
        unable to load shared library
"C:/PROGRA~1/R/rw2001/library/XML/libs/XML.dll":
  LoadLibrary failure:  The specified module could not be found. Error
in library(XML) : package/namespace load failed for 'XML'

Now, having read the website at http://www.omegahat.org/RSXML/, I find
that the package:

"uses libxml2, by default and only libxml(version 1) if libxml2 is not
present "

And here's my dumb question - what is libxml2 in a windows context and
where do I get it from?

Thanks
Mick

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From robin_gruna at hotmail.com  Tue Jan 11 11:15:41 2005
From: robin_gruna at hotmail.com (Robin Gruna)
Date: Tue, 11 Jan 2005 11:15:41 +0100
Subject: [R] LDA discriminant coefficients
Message-ID: <BAY103-DAV5A7D899417268677C6E1887880@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050111/48f61a80/attachment.pl

From Lorenz.Gygax at fat.admin.ch  Tue Jan 11 11:11:56 2005
From: Lorenz.Gygax at fat.admin.ch (Lorenz.Gygax@fat.admin.ch)
Date: Tue, 11 Jan 2005 11:11:56 +0100
Subject: [R] Multiple comparisons following nlme
Message-ID: <BF74FADD4B44554CA7E53D0B5242CD6A01FC63EC@evd-s7014.evd.admin.ch>

> I need to do multiple comparisons following nlme analysis (Compare
> the effects of different treatments on a response measured 
> repeatedly over time;
> fixed = response ~ treat*time).

If you have an interaction it does not really make sense to conduct a
multiple comparison because the difference in treatment depends on time,
i.e. this presumed post-hoc test could only give you a correct result for
one of your points in time. Why not conduct this analysis and then interpret
the pattern based on the estimates of your parameters and/or on a graphical
display of your data?

If your interaction is non-significant and you drop it, you still have a
mulitvariate problem and I have never come across a multiple comparison test
for such multivariate problems (but perhaps someone else has a pointer). In
your case it might be enough to carefully decide on how the set contrasts.

Then, an additional issue would be what kind of multiple comparisons to
conduct (for univarite anova's there are at least a dozen methods). You can
always conduct several to see which of the comparisons are highly
significant and which ones might not be so strong. But usually you do not
learn more than what you get when you interpret your parameters and/or
graphs of your data.

... and by the way, I guess your model is using lme (linear mixed effects
model) in package nlme and not actually an nlme (non-linear mixed effects
model) itself.

Regards, Lorenz
- 
Lorenz Gygax, Dr. sc. nat.
Centre for proper housing of ruminants and pigs
Swiss Federal Veterinary Office, agroscope FAT T?nikon



From Christoph.Scherber at uni-jena.de  Tue Jan 11 11:20:33 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Tue, 11 Jan 2005 11:20:33 +0100
Subject: [R] Calculate Mean of Column Vectors?
In-Reply-To: <4C910DFA-6383-11D9-83F5-000D933268AE@comcast.net>
References: <4C910DFA-6383-11D9-83F5-000D933268AE@comcast.net>
Message-ID: <41E3A871.5060609@uni-jena.de>

Dear Thomas,

My suggestion would be as follows:

y <- rnorm(3000)
dim(y) <- c(3, 1000).

#then create three column vectors out of y using cbind():

w<-cbind(y[1,],y[2,],y[3,])

# and calculate the row means

for (i in 1:1000) z[i]<-mean(w[i,1:3])

z

Hope I got it right!

Regards,
Christoph




Thomas Hopper wrote:

> Hello,
>
> I've got an array defined as y <- rnorm(3000), dim(y) <- c(3, 1000).
>
> I'd like to produce a 1000-element vector z that is the mean of the 
> corresponding elements of y (like z[1,1] <- mean(y[1,1], y[2,1], 
> y[3,1])), but being new to R, I'm not sure how to do this for all 
> elements at once (or, at least, simply). Any help is appreciated.
>
> Thanks,
>
> Tom
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From a.beckerman at sheffield.ac.uk  Tue Jan 11 11:24:55 2005
From: a.beckerman at sheffield.ac.uk (Andrew Beckerman)
Date: Tue, 11 Jan 2005 10:24:55 +0000
Subject: [R] lme4 print and summary errror
Message-ID: <09E214C1-63BB-11D9-9451-000A95CD7F02@sheffield.ac.uk>

Hi all - (this is posted to r-help and R-SIG-MAC)

OSX 10.3.7, R 2.0.1, lme4/Matrix/latticeExtra latest, fresh install of 
R. MASS loaded (or not).

I am getting an error message for the print() and summary() commands 
with all lme models I try and run in lme4 (GLMM's work fine).  Using 
the example from the lme help, summary and print produce the following 
errors, despite the model being fit, as indicated by VarCorr() and 
examination of str(fm).  Any ideas?

(I can't reproduce this on a windows95 install of 2.0.1, so I am 
guessing it may be a mac thing at the moment?  This happens with the 
binary or the source installation of lme4.)

Cheers
andrew

 > data(bdf)
 > fm <- lme(langPOST ~ IQ.ver.cen + avg.IQ.ver.cen, data = bdf,
+           random = ~ IQ.ver.cen | schoolNR)
 > summary(fm)
Error in verbose || attr(x, "verbose") : invalid `y' type in `x || y'
 > fm
Linear mixed-effects model fit by   Data: NULL
   Log-likelihood: NULL
   Fixed: list()
NULL

Length  Class   Mode
      0   NULL   NULL
Number of Observations:
Number of Groups: Error in 1:dd$Q : NA/NaN argument
 > VarCorr(fm)
  Groups   Name        Variance Std.Dev. Corr
  schoolNR (Intercept)  8.07702 2.84201
           IQ.ver.cen   0.20806 0.45614  -0.642
  Residual             41.34942 6.43035



From thopper106035 at comcast.net  Tue Jan 11 12:08:27 2005
From: thopper106035 at comcast.net (Thomas Hopper)
Date: Tue, 11 Jan 2005 06:08:27 -0500
Subject: [R] Calculate Mean of Column Vectors?
In-Reply-To: <1105416216.9251.21.camel@horizons.localdomain>
References: <4C910DFA-6383-11D9-83F5-000D933268AE@comcast.net>
	<1105416216.9251.21.camel@horizons.localdomain>
Message-ID: <1E7FA238-63C1-11D9-83F5-000D933268AE@comcast.net>

Many helpful replies; my thanks to all of you!

colMeans() or the apply() function were what I was looking for (though 
the looping functions will surely come in handy elsewhere).

On Jan 10, 2005, at 11:03 PM, Marc Schwartz wrote:
> # get the column means
> z <- colMeans(y)
>
Best regards,

Tom



From ales.ziberna at guest.arnes.si  Tue Jan 11 12:27:22 2005
From: ales.ziberna at guest.arnes.si (=?windows-1250?Q?Ale=9A_=8Eiberna?=)
Date: Tue, 11 Jan 2005 12:27:22 +0100
Subject: [R] Changes in expression in R 2.0.1
Message-ID: <009401c4f7d0$beb3d010$1409f9c2@ales>

Hello!



plot(1:10,main=expression(1 <= "m" <= 5))

The above command works perfectly in R 1.9.1 but returns an error in R 2.0.1



This is documented in NEWS for R 2.0.0 (or ONEWS in R 2.0.1) and it is said:

o   R no longer accepts associative use of relational operators.
     That is, 3 < 2 < 1 (which used to evalute as TRUE!) now causes
     a syntax error.  If this breaks existing code, just add
     parentheses -- or braces in the case of plotmath.



It is true that the following command does not give en error, however, I do 
not what the extra brackets in the plot:

plot(1:10,main=expression(1 <= ("m" <= 5)))



Is it possible to have this thing plotted with the same look as in R 1.9.1?
I apologize if the instruction are in NEWS, since I am not sure how to 
interpret exactly this section (especially : "just add parentheses -- or 
braces in the case of plotmath".



Thank you in advance for any replies!

Ales Ziberna









I am running binary version of  R 2.0.1 (and R 1.9.1) on Windows XP

Here are the R 2.0.1 version details:

$platform
[1] "i386-pc-mingw32"



$arch
[1] "i386"



$os
[1] "mingw32"



$system
[1] "i386, mingw32"



$status
[1] ""



$major
[1] "2"



$minor
[1] "0.1"



$year
[1] "2004"



$month
[1] "11"



$day
[1] "15"



$language
[1] "R"



> .Platform
$OS.type
[1] "windows"



$file.sep
[1] "/"



$dynlib.ext
[1] ".dll"



$GUI
[1] "Rgui"



$endian
[1] "little"



From sekemp at glam.ac.uk  Tue Jan 11 12:37:03 2005
From: sekemp at glam.ac.uk (Samuel Kemp (Comp))
Date: Tue, 11 Jan 2005 11:37:03 +0000
Subject: [R] transfer function models
Message-ID: <41E3BA5F.8010707@glam.ac.uk>

Hi,

Does anyone know of a function in R that can estimate the parameters of 
a transfer function model with added noise like in SAS?

Thanks in advance,

Sam.



From SAULEAUEA at ch-mulhouse.fr  Tue Jan 11 12:36:17 2005
From: SAULEAUEA at ch-mulhouse.fr (=?iso-8859-1?Q?SAULEAU_Erik-Andr=E9?=)
Date: Tue, 11 Jan 2005 12:36:17 +0100
Subject: [R] StructTS
Message-ID: <A91EF0B9121F834EA6484582DFE1CF4436F617@messagerie.chm.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050111/22bbdbba/attachment.pl

From bates at wisc.edu  Tue Jan 11 12:37:04 2005
From: bates at wisc.edu (Douglas Bates)
Date: Tue, 11 Jan 2005 05:37:04 -0600
Subject: [R] Re: [R-SIG-Mac] lme4 print and summary errror
In-Reply-To: <09E214C1-63BB-11D9-9451-000A95CD7F02@sheffield.ac.uk>
References: <09E214C1-63BB-11D9-9451-000A95CD7F02@sheffield.ac.uk>
Message-ID: <41E3BA60.2080507@wisc.edu>

Andrew Beckerman wrote:
> Hi all - (this is posted to r-help and R-SIG-MAC)
> 
> OSX 10.3.7, R 2.0.1, lme4/Matrix/latticeExtra latest, fresh install of 
> R. MASS loaded (or not).
> 
> I am getting an error message for the print() and summary() commands 
> with all lme models I try and run in lme4 (GLMM's work fine).  Using the 
> example from the lme help, summary and print produce the following 
> errors, despite the model being fit, as indicated by VarCorr() and 
> examination of str(fm).  Any ideas?
> 
> (I can't reproduce this on a windows95 install of 2.0.1, so I am 
> guessing it may be a mac thing at the moment?  This happens with the 
> binary or the source installation of lme4.)
> 
> Cheers
> andrew
> 
>  > data(bdf)
>  > fm <- lme(langPOST ~ IQ.ver.cen + avg.IQ.ver.cen, data = bdf,
> +           random = ~ IQ.ver.cen | schoolNR)
>  > summary(fm)
> Error in verbose || attr(x, "verbose") : invalid `y' type in `x || y'
>  > fm
> Linear mixed-effects model fit by   Data: NULL
>   Log-likelihood: NULL
>   Fixed: list()
> NULL
> 
> Length  Class   Mode
>      0   NULL   NULL
> Number of Observations:
> Number of Groups: Error in 1:dd$Q : NA/NaN argument
>  > VarCorr(fm)
>  Groups   Name        Variance Std.Dev. Corr
>  schoolNR (Intercept)  8.07702 2.84201
>           IQ.ver.cen   0.20806 0.45614  -0.642
>  Residual             41.34942 6.43035
> 
> _______________________________________________
> R-SIG-Mac mailing list
> R-SIG-Mac at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-mac

Check what packages you have loaded.  It looks as if you have loaded 
lme4 and nlme at the same time and you their methods will conflict.



From ligges at statistik.uni-dortmund.de  Tue Jan 11 12:47:02 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 11 Jan 2005 12:47:02 +0100
Subject: [R] Changes in expression in R 2.0.1
In-Reply-To: <009401c4f7d0$beb3d010$1409f9c2@ales>
References: <009401c4f7d0$beb3d010$1409f9c2@ales>
Message-ID: <41E3BCB6.2000307@statistik.uni-dortmund.de>

Ale. .iberna wrote:
> Hello!
> 
> 
> 
> plot(1:10,main=expression(1 <= "m" <= 5))
> 
> The above command works perfectly in R 1.9.1 but returns an error in R 2.0.1
> 
> 
> 
> This is documented in NEWS for R 2.0.0 (or ONEWS in R 2.0.1) and it is said:
> 
> o   R no longer accepts associative use of relational operators.
>      That is, 3 < 2 < 1 (which used to evalute as TRUE!) now causes
>      a syntax error.  If this breaks existing code, just add
>      parentheses -- or braces in the case of plotmath.
> 
> 
> 
> It is true that the following command does not give en error, however, I do 
> not what the extra brackets in the plot:
> 
> plot(1:10,main=expression(1 <= ("m" <= 5)))
> 
> 
> 
> Is it possible to have this thing plotted with the same look as in R 1.9.1?
> I apologize if the instruction are in NEWS, since I am not sure how to 
> interpret exactly this section (especially : "just add parentheses -- or 
> braces in the case of plotmath".

A good dictionary should tell you that {} rather than () is meant with 
"braces":

   plot(1:10,main=expression(1 <= {"m" <= 5}))

You have to group it syntactically, that's all.

Uwe Ligges


> 
> 
> Thank you in advance for any replies!
> 
> Ales Ziberna
>



From r.hankin at soc.soton.ac.uk  Tue Jan 11 12:50:11 2005
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Tue, 11 Jan 2005 11:50:11 +0000
Subject: [R] integrate() and complex values
Message-ID: <F341077A-63C6-11D9-85C9-000A95D86AA8@soc.soton.ac.uk>

[apologies for possible multiple post]


Hi

The manpage for integrate() does not mention imaginary numbers.
I found the following unexpected:

 > integrate(function(x){0*x+1+1i},0,1)
1 with absolute error < 1.1e-14
 >

(I would expect 1+1i here)

One can write a little wrapper, but it's not straightforward.
Would it be hard to accommodate such functions in integrate()?

Some other functions returning complex values cause integrate() to trip 
up:

 > integrate(function(x){exp(1i*x)},0,1)
Error in integrate(function(x) { : maximum number of subdivisions 
reached


--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From kadusei at uni-goettingen.de  Tue Jan 11 13:01:19 2005
From: kadusei at uni-goettingen.de (Kwabena Adusei-Poku)
Date: Tue, 11 Jan 2005 13:01:19 +0100
Subject: [R] Kolmogorov-Smirnof test for lognormal distribution with
	estimated parameters 
Message-ID: <000001c4f7d5$43051dc0$9bad4c86@wso1809r>

Hello all,

Would somebody be kind enough to show me how to do a KS test in R for a
lognormal distribution with ESTIMATED parameters. The R function
ks.test()says "the parameters specified must be prespecified and not
estimated from the data" Is there a way to correct this when one uses
estimated data?

Regards,

Kwabena.

--------------------------------------------
Kwabena Adusei-Poku
University of Goettingen
Institute of Statistics and Econometrics
Platz der Goettingen Sieben 5
37073 Goettingen
Germany
Tel: +49-(0)551-394794



From paolacerchiello at libero.it  Tue Jan 11 13:10:16 2005
From: paolacerchiello at libero.it (paolacerchiello@libero.it)
Date: Tue, 11 Jan 2005 13:10:16 +0100
Subject: [R] Re:Chi-square distance
Message-ID: <IA5J54$D2BF829549B8067BF0A331205ADB2983@libero.it>


> Hi
> I'm Ph.D student and I need an R code to compute the chi square diistance between n profile rows in a matrix.
> 
> could you help me please?
> Thanks
> 
> Paola
> 




____________________________________________________________



From ripley at stats.ox.ac.uk  Tue Jan 11 13:21:56 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 11 Jan 2005 12:21:56 +0000 (GMT)
Subject: [R] integrate() and complex values
In-Reply-To: <F341077A-63C6-11D9-85C9-000A95D86AA8@soc.soton.ac.uk>
Message-ID: <Pine.GSO.4.31.0501111219040.21904-100000@toucan.stats>

On Tue, 11 Jan 2005, Robin Hankin wrote:

> The manpage for integrate() does not mention imaginary numbers.

The help page does say

       f: an R function taking a numeric first argument and returning a
          numeric vector of the same length.

and in R complex numbers are not numeric (see ?numeric).  So you are
misusing it,

> I found the following unexpected:
>
>  > integrate(function(x){0*x+1+1i},0,1)
> 1 with absolute error < 1.1e-14
>  >
>
> (I would expect 1+1i here)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sdavis2 at mail.nih.gov  Tue Jan 11 13:35:40 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 11 Jan 2005 07:35:40 -0500
Subject: [R] Matrix to "indexed" vector
Message-ID: <4D77837B-63CD-11D9-A2FE-000D933565E8@mail.nih.gov>

I have a matrix that I want to turn into a transformed matrix that 
includes the indices from the original matrix and the value.  The 
matrix is simply real-valued and is square (and large (8k x 8k)).  I 
want something that looks like (for the 3x3 case):

i	j	value
1	1	1.0
1	2	0.783432
1	3	-0.123482
2	1	0.783432
2	2	1.0
2	3	0.928374

and so on....

I can do this with for loops, but there is likely to be a better way 
and for my own edification, I would like to see what others would do.  
I am sinking the results to a file for loading into SQL database.

Thanks,
Sean



From petr.pikal at precheza.cz  Tue Jan 11 13:37:19 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 11 Jan 2005 13:37:19 +0100
Subject: [R] Changes in expression in R 2.0.1
In-Reply-To: <009401c4f7d0$beb3d010$1409f9c2@ales>
Message-ID: <41E3D68F.11612.19C3573@localhost>



From tobias.verbeke at telenet.be  Tue Jan 11 13:52:19 2005
From: tobias.verbeke at telenet.be (Tobias Verbeke)
Date: Tue, 11 Jan 2005 12:52:19 +0000
Subject: [R] Re:Chi-square distance
Message-ID: <W4720312870158451105447939@asteria.telenet-ops.be>

>----- Oorspronkelijk bericht -----
>Van: paolacerchiello at libero.it [mailto:paolacerchiello at libero.it]
>Verzonden: dinsdag, januari 11, 2005 12:10 PM


>> I'm Ph.D student and I need an R code to compute the chi square diistance between n profile rows in a matrix.

There is a ready-made function dudi.dist in package ade4.
This function needs an object of class "dudi", so (in your
case) you need to do a correspondence analysis on your matrix.
This will define the metrics on the row and column 
spaces the way you want. The function dudi.coa only accepts
data frames as its first argument.

install.packages(ade4)
library(ade4)
mydf <- as.data.frame(mymat)
mydf.coa <- dudi.coa(mydf, scannf = FALSE, nf = 2)
dudi.dist(mydf.coa, amongrow = TRUE)

should give you the chi square distances between
row profiles.

HTH,
Tobias

>> 
>> could you help me please?
>> Thanks
>> 
>> Paola
>> 
>
>
>
>
>____________________________________________________________
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Jan 11 13:55:48 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 11 Jan 2005 13:55:48 +0100
Subject: [R] Matrix to "indexed" vector
References: <4D77837B-63CD-11D9-A2FE-000D933565E8@mail.nih.gov>
Message-ID: <003801c4f7dc$df281bb0$0540210a@www.domain>

maybe something like:

mat <- rnorm(9); dim(mat) <- c(3,3)
mat
###########
cbind(i=rep(1:nrow(mat), each=ncol(mat)),
                j=rep(1:ncol(mat), nrow(mat)), value=c(t(mat)))

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Sean Davis" <sdavis2 at mail.nih.gov>
To: "r-help" <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 11, 2005 1:35 PM
Subject: [R] Matrix to "indexed" vector


>I have a matrix that I want to turn into a transformed matrix that 
>includes the indices from the original matrix and the value.  The 
>matrix is simply real-valued and is square (and large (8k x 8k)).  I 
>want something that looks like (for the 3x3 case):
>
> i j value
> 1 1 1.0
> 1 2 0.783432
> 1 3 -0.123482
> 2 1 0.783432
> 2 2 1.0
> 2 3 0.928374
>
> and so on....
>
> I can do this with for loops, but there is likely to be a better way 
> and for my own edification, I would like to see what others would 
> do.  I am sinking the results to a file for loading into SQL 
> database.
>
> Thanks,
> Sean
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ales.ziberna at guest.arnes.si  Tue Jan 11 13:56:46 2005
From: ales.ziberna at guest.arnes.si (=?windows-1250?Q?Ale=9A_=8Eiberna?=)
Date: Tue, 11 Jan 2005 13:56:46 +0100
Subject: [R] Changes in expression in R 2.0.1
References: <009401c4f7d0$beb3d010$1409f9c2@ales>
	<41E3BCB6.2000307@statistik.uni-dortmund.de>
Message-ID: <013c01c4f7dd$23d2ff00$1409f9c2@ales>

Thank  you very  much!

I guess I need a new dictionary!

Ales Ziberna


----- Original Message ----- 
From: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>
To: "Ale? Ziberna" <ales.ziberna at guest.arnes.si>
Cc: "R-help" <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 11, 2005 12:47 PM
Subject: Re: [R] Changes in expression in R 2.0.1


Ale? Ziberna wrote:
> Hello!
>
>
>
> plot(1:10,main=expression(1 <= "m" <= 5))
>
> The above command works perfectly in R 1.9.1 but returns an error in R 
> 2.0.1
>
>
>
> This is documented in NEWS for R 2.0.0 (or ONEWS in R 2.0.1) and it is 
> said:
>
> o   R no longer accepts associative use of relational operators.
>      That is, 3 < 2 < 1 (which used to evalute as TRUE!) now causes
>      a syntax error.  If this breaks existing code, just add
>      parentheses -- or braces in the case of plotmath.
>
>
>
> It is true that the following command does not give en error, however, I 
> do
> not what the extra brackets in the plot:
>
> plot(1:10,main=expression(1 <= ("m" <= 5)))
>
>
>
> Is it possible to have this thing plotted with the same look as in R 
> 1.9.1?
> I apologize if the instruction are in NEWS, since I am not sure how to
> interpret exactly this section (especially : "just add parentheses -- or
> braces in the case of plotmath".

A good dictionary should tell you that {} rather than () is meant with
"braces":

   plot(1:10,main=expression(1 <= {"m" <= 5}))

You have to group it syntactically, that's all.

Uwe Ligges


>
>
> Thank you in advance for any replies!
>
> Ales Ziberna
>



From sdavis2 at mail.nih.gov  Tue Jan 11 14:00:25 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 11 Jan 2005 08:00:25 -0500
Subject: [R] Matrix to "indexed" vector
In-Reply-To: <003801c4f7dc$df281bb0$0540210a@www.domain>
References: <4D77837B-63CD-11D9-A2FE-000D933565E8@mail.nih.gov>
	<003801c4f7dc$df281bb0$0540210a@www.domain>
Message-ID: <C29959D8-63D0-11D9-A2FE-000D933565E8@mail.nih.gov>


On Jan 11, 2005, at 7:55 AM, Dimitris Rizopoulos wrote:

> mat <- rnorm(9); dim(mat) <- c(3,3)
> mat
> ###########
> cbind(i=rep(1:nrow(mat), each=ncol(mat)),
>                j=rep(1:ncol(mat), nrow(mat)), value=c(t(mat)))
>
That will do it.  Thanks!

Sean



From sundar.dorai-raj at pdf.com  Tue Jan 11 14:01:23 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 11 Jan 2005 07:01:23 -0600
Subject: [R] Matrix to "indexed" vector
In-Reply-To: <4D77837B-63CD-11D9-A2FE-000D933565E8@mail.nih.gov>
References: <4D77837B-63CD-11D9-A2FE-000D933565E8@mail.nih.gov>
Message-ID: <41E3CE23.9090308@pdf.com>



Sean Davis wrote:

> I have a matrix that I want to turn into a transformed matrix that 
> includes the indices from the original matrix and the value.  The matrix 
> is simply real-valued and is square (and large (8k x 8k)).  I want 
> something that looks like (for the 3x3 case):
> 
> i    j    value
> 1    1    1.0
> 1    2    0.783432
> 1    3    -0.123482
> 2    1    0.783432
> 2    2    1.0
> 2    3    0.928374
> 
> and so on....
> 
> I can do this with for loops, but there is likely to be a better way and 
> for my own edification, I would like to see what others would do.  I am 
> sinking the results to a file for loading into SQL database.
> 
> Thanks,
> Sean
> 

How about:

x <- c(1.0, 0.783432, -0.123482, 0.783432, 1.0, 0.928374)
x <- matrix(x, 2, 3, TRUE)
y <- cbind(expand.grid(i = seq(nrow(x)), j = seq(ncol(x))), c(x))
y[order(y[, 1], y[, 2]), ]


   i j      c(x)
1 1 1  1.000000
3 1 2  0.783432
5 1 3 -0.123482
2 2 1  0.783432
4 2 2  1.000000
6 2 3  0.928374


HTH,

--sundar



From r.ramyar at gmail.com  Tue Jan 11 14:10:51 2005
From: r.ramyar at gmail.com (Rick Ram)
Date: Tue, 11 Jan 2005 13:10:51 +0000
Subject: [R] CUSUM SQUARED structural breaks approach?
Message-ID: <5f4f8279050111051031974bc4@mail.gmail.com>

Dear all,

Does anyone know where there is R or S code for the CUSUM SQUARED
structural breaks approach?   (Brown, Durban and Evans, 1975 - used in
Pesaran and Timmerman, 2002)

The problem is that the breaks package only appears to offer the
standard 'unsquared' CUSUM, even though it appears most think it is
inferior to the squared version.  It might appear to be a relatively
simple problem - just use the recursive residuals function, square
them and then take the cumulative sum.  But the problem is that the
normalisation and calculation of confidence levels is completely
different.

Any help or pointers about where to look would be more than
appreciated!  Hopefully I have just missed obvious something in the
package...

Many thanks,

Rick R.



From lecoutre at stat.ucl.ac.be  Tue Jan 11 14:08:17 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Tue, 11 Jan 2005 14:08:17 +0100
Subject: [R] Matrix to "indexed" vector
In-Reply-To: <4D77837B-63CD-11D9-A2FE-000D933565E8@mail.nih.gov>
References: <4D77837B-63CD-11D9-A2FE-000D933565E8@mail.nih.gov>
Message-ID: <6.0.1.1.2.20050111140324.03780908@stat4ux.stat.ucl.ac.be>


Hi Sean,

Here is a way to do that:

idmatrix <- function(m){
   if (!is(m,"matrix")) stop("Please provide a matrix as argument")
   ind <- cbind(as.vector(row(m)),as.vector(col(m)))
   out <- cbind(ind, m[ind])
   return(out)
}


There are others way to provide indexes, such as
 > expand.grid(1:nrow(mm),1:ncol(mm))

Not sure which is the most efficient.

HTH,

Eric


 > data(iris)
 > mm=as.matrix(iris[1:5,1:4])
 > idmatrix(mm)
       [,1] [,2] [,3]
  [1,]    1    1  5.1
  [2,]    2    1  4.9
  [3,]    3    1  4.7
  [4,]    4    1  4.6
  [5,]    5    1  5.0
  [6,]    1    2  3.5
  [7,]    2    2  3.0
  [8,]    3    2  3.2
  [9,]    4    2  3.1
[10,]    5    2  3.6
[11,]    1    3  1.4
[12,]    2    3  1.4
[13,]    3    3  1.3
[14,]    4    3  1.5
[15,]    5    3  1.4
[16,]    1    4  0.2
[17,]    2    4  0.2
[18,]    3    4  0.2
[19,]    4    4  0.2
[20,]    5    4  0.2

At 13:35 11/01/2005, Sean Davis wrote:
>I have a matrix that I want to turn into a transformed matrix that 
>includes the indices from the original matrix and the value.  The matrix 
>is simply real-valued and is square (and large (8k x 8k)).  I want 
>something that looks like (for the 3x3 case):
>
>i       j       value
>1       1       1.0
>1       2       0.783432
>1       3       -0.123482
>2       1       0.783432
>2       2       1.0
>2       3       0.928374
>
>and so on....
>
>I can do this with for loops, but there is likely to be a better way and 
>for my own edification, I would like to see what others would do.
>I am sinking the results to a file for loading into SQL database.
>
>Thanks,
>Sean
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From K.E.Vorloou at durham.ac.uk  Tue Jan 11 14:28:38 2005
From: K.E.Vorloou at durham.ac.uk (Costas Vorlow)
Date: Tue, 11 Jan 2005 13:28:38 +0000
Subject: [R] Changing the ranges for the axis in image()
Message-ID: <41E3D486.2010900@durham.ac.uk>

Dear all,

I can not find/understand the solution to this from the help pages:

Say we have the following script:

 x<-matrix(c(1,1,0,1,0,1,0,1,1),3,3)
image(x)

How can I change the ranges on the vertical and horizontal axis to a 
range of my own or at least place a box frame around the image if I 
choose to use "axes=FALSE"?

Apologies for such a bsic question and thanks beforehand for your answers.



From bates at wisc.edu  Tue Jan 11 14:34:52 2005
From: bates at wisc.edu (Douglas Bates)
Date: Tue, 11 Jan 2005 07:34:52 -0600
Subject: [R] Matrix to "indexed" vector
In-Reply-To: <41E3CE23.9090308@pdf.com>
References: <4D77837B-63CD-11D9-A2FE-000D933565E8@mail.nih.gov>
	<41E3CE23.9090308@pdf.com>
Message-ID: <41E3D5FC.80300@wisc.edu>

Sundar Dorai-Raj wrote:
> 
> 
> Sean Davis wrote:
> 
>> I have a matrix that I want to turn into a transformed matrix that 
>> includes the indices from the original matrix and the value.  The 
>> matrix is simply real-valued and is square (and large (8k x 8k)).  I 
>> want something that looks like (for the 3x3 case):
>>
>> i    j    value
>> 1    1    1.0
>> 1    2    0.783432
>> 1    3    -0.123482
>> 2    1    0.783432
>> 2    2    1.0
>> 2    3    0.928374
>>
>> and so on....
>>
>> I can do this with for loops, but there is likely to be a better way 
>> and for my own edification, I would like to see what others would do.  
>> I am sinking the results to a file for loading into SQL database.
>>
>> Thanks,
>> Sean
>>
> 
> How about:
> 
> x <- c(1.0, 0.783432, -0.123482, 0.783432, 1.0, 0.928374)
> x <- matrix(x, 2, 3, TRUE)
> y <- cbind(expand.grid(i = seq(nrow(x)), j = seq(ncol(x))), c(x))
> y[order(y[, 1], y[, 2]), ]
> 
> 
>   i j      c(x)
> 1 1 1  1.000000
> 3 1 2  0.783432
> 5 1 3 -0.123482
> 2 2 1  0.783432
> 4 2 2  1.000000
> 6 2 3  0.928374
> 
> 
> HTH,
> 
> --sundar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

This form is called the 'triplet' form of a sparse matrix and is used in 
the 'tripletMatrix' class from the Matrix package.  You could do the 
transformation as

 > library(Matrix)
 > mm <- matrix(rnorm(12), nc = 4)
 > mm
            [,1]       [,2]       [,3]        [,4]
[1,]  0.3270100  2.3091426 -0.3843992  1.13819897
[2,] -0.1161578 -0.4877587 -0.0709391 -0.91308167
[3,]  0.9652397 -0.3230332 -1.3011591 -0.03514905

 > as(as(mm, 'cscMatrix'), 'tripletMatrix')
An object of class "tripletMatrix"
Slot "i":
  [1] 0 1 2 0 1 2 0 1 2 0 1 2

Slot "j":
  [1] 0 0 0 1 1 1 2 2 2 3 3 3

Slot "x":
  [1]  0.32700997 -0.11615783  0.96523972  2.30914265 -0.48775866 
-0.32303318
  [7] -0.38439915 -0.07093910 -1.30115907  1.13819897 -0.91308167 
-0.03514905

Slot "Dim":
[1] 3 4

(There should be a direct coersion method from the "matrix" class to the 
"tripletMatrix" class and there will be in the next release of the 
Matrix package.  For the time being you must go through the "cscMatrix" 
class as an intermediate.)

Note that the index vectors for the tripletMatrix class are 0-based 
indices, not 1-based.  If you want to make them 1-based then just add 1 
to the 'i' and 'j' slots.



From p.dalgaard at biostat.ku.dk  Tue Jan 11 14:59:46 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Jan 2005 14:59:46 +0100
Subject: [R] Matrix to "indexed" vector
In-Reply-To: <C29959D8-63D0-11D9-A2FE-000D933565E8@mail.nih.gov>
References: <4D77837B-63CD-11D9-A2FE-000D933565E8@mail.nih.gov>
	<003801c4f7dc$df281bb0$0540210a@www.domain>
	<C29959D8-63D0-11D9-A2FE-000D933565E8@mail.nih.gov>
Message-ID: <x2hdloawx9.fsf@biostat.ku.dk>

Sean Davis <sdavis2 at mail.nih.gov> writes:

> On Jan 11, 2005, at 7:55 AM, Dimitris Rizopoulos wrote:
> 
> > mat <- rnorm(9); dim(mat) <- c(3,3)
> > mat
> > ###########
> > cbind(i=rep(1:nrow(mat), each=ncol(mat)),
> >                j=rep(1:ncol(mat), nrow(mat)), value=c(t(mat)))
> >
> That will do it.  Thanks!

Also, almost the same (you might want to change the column names and
the levels):

> as.data.frame(as.table(mat))
   Var1 Var2        Freq
1     A    A  1.30228891
2     B    A  0.03432453
3     C    A  0.84913086
4     D    A -0.72574037
5     A    B  0.57836049
6     B    B -1.53737385
7     C    B -0.73390887
8     D    B  1.01061902
9     A    C  0.27826502
10    B    C -2.50497059
11    C    C -0.57292384
12    D    C  2.13511711

and

> cbind(i=c(row(mat)),j=c(col(mat)),value=c(mat))
      i j       value
 [1,] 1 1  1.30228891
 [2,] 2 1  0.03432453
 [3,] 3 1  0.84913086
 [4,] 4 1 -0.72574037
 [5,] 1 2  0.57836049
 [6,] 2 2 -1.53737385
 [7,] 3 2 -0.73390887
 [8,] 4 2  1.01061902
 [9,] 1 3  0.27826502
[10,] 2 3 -2.50497059
[11,] 3 3 -0.57292384
[12,] 4 3  2.13511711

(In both cases you need a transpose or reordering if you want j to
vary the fastest.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Achim.Zeileis at wu-wien.ac.at  Tue Jan 11 15:10:32 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 11 Jan 2005 15:10:32 +0100
Subject: [R] CUSUM SQUARED structural breaks approach?
In-Reply-To: <5f4f8279050111051031974bc4@mail.gmail.com>
References: <5f4f8279050111051031974bc4@mail.gmail.com>
Message-ID: <20050111151032.07d05ab4.Achim.Zeileis@wu-wien.ac.at>

On Tue, 11 Jan 2005 13:10:51 +0000 Rick Ram wrote:

> Dear all,
> 
> Does anyone know where there is R or S code for the CUSUM SQUARED
> structural breaks approach?   (Brown, Durban and Evans, 1975 - used in
> Pesaran and Timmerman, 2002)

Not to my knowledge.
 
> The problem is that the breaks package

I guess you're referring to the strucchange package. There is no package
breaks, at least not on CRAN.

> only appears to offer the
> standard 'unsquared' CUSUM, even though it appears most think it is
> inferior to the squared version. 

Yes, it's part of the folklore that one should use the CUSUM of squares
rather than the CUSUM test. However, in many scenarios one wouldn't want
to use either test. The BDE CUSUM test is good when you want to know
when 1-step ahead predictions start to break down (and that works only
if the break occurs early in the sample period). The CUSUM of squares
test is more appropriate if you want to detect changes in the variance
(rather than the conditional mean).
For both problems (breaks in the mean / breaks in the variance) there
are other tests which are usually more appropriate than the BDE CUSUM or
CUSUM of squares test which is why I haven't implemented the latter.
(But, of course, no test can be uniformly superior due to the vast
alternative.)

> It might appear to be a relatively
> simple problem - just use the recursive residuals function, square
> them and then take the cumulative sum.  But the problem is that the
> normalisation and calculation of confidence levels is completely
> different.

Yes. Computing the CUSUM of square process is straightforward using the
recresid() methods. To obtain critical values, you will either have to
simulate them or use the tabulated values referenced in BDE from Durbin
(1969).

But depending on the model and hypothesis you want to test, another
technique than CUSUM of squares might be more appropriate and also 
available in strucchange.

hth,
Z

> Any help or pointers about where to look would be more than
> appreciated!  Hopefully I have just missed obvious something in the
> package...
> 
> Many thanks,
> 
> Rick R.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From br44114 at yahoo.com  Tue Jan 11 15:26:28 2005
From: br44114 at yahoo.com (bogdan romocea)
Date: Tue, 11 Jan 2005 06:26:28 -0800 (PST)
Subject: [R] global objects not overwritten within function
Message-ID: <20050111142628.48038.qmail@web50303.mail.yahoo.com>

Dear useRs,

I have a function that creates several global objects with
assign("obj",obj,.GlobalEnv), and which I need to run iteratively in
another function. The code is similar to

f <- function(...) {
assign("obj",obj,.GlobalEnv)
}
fct <- function(...) {
for (i in 1:1000)
	{
	...
	f(...)	
	...obj...
	rm(obj)	#code fails without this line
	}
}

I don't understand why f(), when run in a for() loop inside fct(), does
not overwrite the global object 'obj'. If I don't delete 'obj' after I
use it, the code fails - the same objects created by the first
iteration are used by subsequent iterations. 

I checked ?assign and the Evaluation chapter in 'R Language Definition'
but still don't understand why the above happens. Can someone briefly
explain or suggest something I should read? By the way, I don't want to
use 'better' techniques (lists, functions that return values instead of
creating global objects etc) - I want to create global objects with f()
and overwrite them again and again within fct().

Thank you,
b.



From dmb at mrc-dunn.cam.ac.uk  Tue Jan 11 15:59:12 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 11 Jan 2005 14:59:12 +0000 (GMT)
Subject: [R] Graphical table in R
In-Reply-To: <x26525yp63.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.21.0501111454180.25669-100000@mail.mrc-dunn.cam.ac.uk>

On 10 Jan 2005, Peter Dalgaard wrote:

>Dan Bolser <dmb at mrc-dunn.cam.ac.uk> writes:
>
>> Cheers. This is really me just being lazy (as usual). The latex function
>> in Hmisc allows me to make a .ps file then grab a screen shot of that ps
>> and make a .png file.
>> 
>> I would just like to use plot so I can wrap it in a png command and not
>> have to use the 'screen shot' in between.
>
>A screen shot of a ps file? That sounds ... weird. If you can view it,
>presumably you have Ghostscript and that can do png files.

The thing is the ps file has teh wrong size, so I end up with a small
table in the corner of a big white page (using imageMagick convert
function).

I havent tried ghostscript (don't know the cmd).

I could set the paper size correctly if I knew the size of my table, but I
don't know how to calculate that before hand and feed it into the latex
commands (Hmisc).

Seems like I should roll my own table with the plot command and
'primatives' (like the demo(mathplot)) - I just hoped that someone had
already done the hard work for me and I could type something like...

plot.xtable(x)

x = any R object that makes sense to have a tabular output.

Seems like such a function done correctly could be usefull for helping
people write up (hem) analysis.

Thanks again for the help everyone.

Dan.

>
>However, will textual output do?
>
>  plot(0,type="n",axes=FALSE, xlab="", ylab="")
>  con <- textConnection("txt","w")
>  sink(con); ftable(UCBAdmissions); sink()
>  close(con)
>  par(family="mono")
>  text(1,0,paste(txt,collapse="\n"))
>
>
>



From cs_matyi at freemail.hu  Tue Jan 11 15:59:58 2005
From: cs_matyi at freemail.hu (=?ISO-8859-2?Q?Cserh=E1ti_M=E1ty=E1s?=)
Date: Tue, 11 Jan 2005 15:59:58 +0100 (CET)
Subject: [R] thanks
Message-ID: <freemail.20050011155958.44093@fm13.freemail.hu>



Dear all,

Thanks to those 3 people who sent me answers to my question. Got 
the problem solved. Great!

Now, another question of mine is:

I would like to run an R script from the Linux prompt. Is there any way 
possible to do this? The reason is, the calculation that I'm doing takes a 
few hours, and I would like to automatize it.

Or does it mean that I have to run source within the R prompt?

Or is there a way to do the automatization within the R prompt?

Thanks, Matthew

u.i. K?szi, Zoli!



From sundar.dorai-raj at pdf.com  Tue Jan 11 16:01:31 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 11 Jan 2005 09:01:31 -0600
Subject: [R] Changing the ranges for the axis in image()
In-Reply-To: <41E3D486.2010900@durham.ac.uk>
References: <41E3D486.2010900@durham.ac.uk>
Message-ID: <41E3EA4B.2030502@pdf.com>



Costas Vorlow wrote:

> Dear all,
> 
> I can not find/understand the solution to this from the help pages:
> 
> Say we have the following script:
> 
> x<-matrix(c(1,1,0,1,0,1,0,1,1),3,3)
> image(x)
> 
> How can I change the ranges on the vertical and horizontal axis to a 
> range of my own or at least place a box frame around the image if I 
> choose to use "axes=FALSE"?
> 
> Apologies for such a bsic question and thanks beforehand for your answers.
> 

 From ?image:

  ## Default S3 method:
      image(x, y, z, zlim, xlim, ylim, col = heat.colors(12),
            add = FALSE, xaxs = "i", yaxs = "i", xlab, ylab,
            breaks, oldstyle = FALSE, ...)


Unless I misunderstood your question, the zlim, xlim, and ylim arguments 
should give you what you want.

--sundar



From James.A.Rogers at pfizer.com  Tue Jan 11 16:08:12 2005
From: James.A.Rogers at pfizer.com (Rogers, James A [PGRD Groton])
Date: Tue, 11 Jan 2005 10:08:12 -0500
Subject: [R] Multiple comparisons following nlme
Message-ID: <2A8DE2E40F52E049B8FBB4634D32AFE64B604F@groamrexm01.amer.pfizer.com>


> I need to do multiple comparisons following nlme analysis (Compare
> the effects of different treatments on a response measured 
> repeatedly over time;
> fixed = response ~ treat*time).

I don't believe there exists a nice interface for doing this, but you can do
it "by hand". You need to obtain the point estimates of contrasts of
interest, and the estimated covariance matrix for those estimates. You can
then use these as arguments to csimint() in the multcomp package. 

--Jim

James A. Rogers 
Manager, Nonclinical Statistics
PGR&D Groton Labs
Eastern Point Road (MS 8260-1331)
Groton, CT 06340
office: (860) 686-0786
fax: (860) 715-5445
 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From qu at oakland.edu  Tue Jan 11 16:15:59 2005
From: qu at oakland.edu (Harvey Qu)
Date: Tue, 11 Jan 2005 10:15:59 -0500
Subject: [R] R program for optimal balanced Arrays!
Message-ID: <41E3EDAF.5812AFBB@oakland.edu>

Does anybody know whether there is a program in R (or related) generating optimal balanced arrays? I would appreciate if
you could share it with me!
Thanks a lot!
Harvey.



From qu at oakland.edu  Tue Jan 11 16:15:59 2005
From: qu at oakland.edu (Harvey Qu)
Date: Tue, 11 Jan 2005 10:15:59 -0500
Subject: [R] R program for optimal balanced Arrays!
Message-ID: <41E3EDAF.5812AFBB@oakland.edu>

Does anybody know whether there is a program in R (or related) generating optimal balanced arrays? I would appreciate if
you could share it with me!
Thanks a lot!
Harvey.



From vito_ricci at yahoo.com  Tue Jan 11 16:39:00 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Tue, 11 Jan 2005 16:39:00 +0100 (CET)
Subject: [R] Re: Kolmogorov-Smirnof test for lognormal distribution with
	estimated parameters
Message-ID: <20050111153900.37196.qmail@web41210.mail.yahoo.com>

Hi,

I believe that to performe KS test parameters must not
be estimated by sample data.

Despite some advantages, the KS test has several
important limitations:

   1. It only applies to continuous distributions.
   2. It tends to be more sensitive near the center of
the distribution than at the tails.
-->3. Perhaps the most serious limitation is that the
distribution must be fully specified. That is, if
location, scale, and shape parameters are estimated
from the data, the critical region of the K-S test is
no longer valid. It typically must be determined by
simulation. <--

Due to limitations 2 and 3 above, many analysts prefer
to use the Anderson-Darling goodness-of-fit test.
However, the Anderson-Darling test is only available
for a few specific distributions. 

See:

http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm

for KS test

http://www.itl.nist.gov/div898/handbook/eda/section3/eda35e.htm
for Anderson-Darling test

http://www.itl.nist.gov/div898/handbook/eda/section3/eda35f.htm
X2 test

I suggest you to use th chisquare test.

Hoping to help you.
Best,
Vito

You wrote:

Hello all,

Would somebody be kind enough to show me how to do a
KS test in R for a lognormal distribution with
ESTIMATED parameters. The R function ks.test()says
"the parameters specified must be prespecified and not
estimated from the data" Is there a way to correct
this when one uses
estimated data?

Regards,

Kwabena.

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From apjaworski at mmm.com  Tue Jan 11 20:27:26 2005
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Tue, 11 Jan 2005 13:27:26 -0600
Subject: [R] A question about r-help-bounce
In-Reply-To: <W4720312870158451105447939@asteria.telenet-ops.be>
Message-ID: <OFF350CAE8.BCCA5B0C-ON86256F86.00695D71-86256F86.006AE1D7@mmm.com>






Could somebody please tell me what does the r-help-bounce address do?

When I try to respond to an r-help post, my mailer (Lotus Notes) generates
the r-help bounce return address automatically in addition to the original
sender address and the r-help address.  I responded to an r-help message
last night and I never saw my response posted.  Could this be due to the
r-help-bounce being there?

Thanks in advance,

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122



From Tristan.Lefebure at univ-lyon1.fr  Tue Jan 11 16:24:11 2005
From: Tristan.Lefebure at univ-lyon1.fr (Lefebure Tristan)
Date: Tue, 11 Jan 2005 16:24:11 +0100
Subject: [R] thanks
In-Reply-To: <freemail.20050011155958.44093@fm13.freemail.hu>
References: <freemail.20050011155958.44093@fm13.freemail.hu>
Message-ID: <200501111624.12101.lefebure@univ-lyon1.fr>

see man R

example from a shell:

echo -e "pdf(file=\"test.pdf\")\nplot(1:10,11:20)\ndev.off(dev.cur())\n">cmd.R
R -s <cmd.R

(write a file of command for R, and than feed R with it)

On Tuesday 11 January 2005 15:59, Cserh?ti M?ty?s wrote:
> Dear all,
>
> Thanks to those 3 people who sent me answers to my question. Got
> the problem solved. Great!
>
> Now, another question of mine is:
>
> I would like to run an R script from the Linux prompt. Is there any way
> possible to do this? The reason is, the calculation that I'm doing takes a
> few hours, and I would like to automatize it.
>
> Or does it mean that I have to run source within the R prompt?
>
> Or is there a way to do the automatization within the R prompt?
>
> Thanks, Matthew
>
> u.i. K?szi, Zoli!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
------------------------------------------------------------
Tristan LEFEBURE
Laboratoire d'?cologie des hydrosyst?mes fluviaux (UMR 5023)
Universit? Lyon I - Campus de la Doua
Bat. Darwin C 69622 Villeurbanne - France

Phone: (33) (0)4 26 23 44 02
Fax: (33) (0)4 72 43 15 23



From bigby at woosh.co.nz  Wed Jan 12 06:54:27 2005
From: bigby at woosh.co.nz (Bigby)
Date: Wed, 12 Jan 2005 18:54:27 +1300
Subject: [R] (no subject)
Message-ID: <APEDJPMADIEPCPHPOGHIGEPBCEAA.bigby@woosh.co.nz>

Hello,

My name is Graham, I am an engineering student and my lecturer wishes us to
get a numerical summary of some data. He said use the command
numerical.summary(Data), which didnt work, he suggested we try library(s20x)
first, which came up with an error on my console. I have version 2.0.1 of R
and i dont understand what to do. As this is part of an assignment I would
really apreciate some advice.

Regards

-  Graham



From Robert.McGehee at geodecapital.com  Tue Jan 11 16:49:51 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Tue, 11 Jan 2005 10:49:51 -0500
Subject: [R] global objects not overwritten within function
Message-ID: <67DCA285A2D7754280D3B8E88EB5480206741E06@MSGBOSCLB2WIN.DMN1.FMR.COM>

I would suggest reading the posting guide,
(http://www.r-project.org/posting-guide.html) and give a reproducible
example, with the error message that you received. As is, I have no idea
what you are doing here, and certainly cannot run this code. You use
"..." as an argument to your functions (why I have no idea), but then
use "..." within your function seemingly to mean code was omitted rather
than using your function argument "...". What "...obj..." means I have
no idea.

One point though. If the f() function does not take any arguments, then
why are you using the special R object  "..." as an argument?
Furthermore, why not just do the assignment inside of fct() instead of
calling another function that just runs code? Please reference the
_rest_ of the R Language guide for information on correct usage of
"...", especially the chapter on functions, and include a script that
can be run from start to finish by anyone at an R prompt without having
to decipher what the missing code does, or what the ellipsis is doing in
your context.

My guess to what I think is going on here is that you are trying to use
dynamic scoping, when R uses lexical scoping. If you are an S user, this
will be a change. The f() function is stored in .GlobalEnv, so is not
aware of any objects stored in the fct() environment. When you run f(),
it's looking for an "obj" object in its environment, probably can't find
one, and then looking for the "obj" object in the global environment. If
it finds it there, it assigns it to itself, basically doing nothing.
Once again, the R language guide will explain this. You could solve this
by either imbedding the f() function inside of fct(), passing in the obj
object, instead of relying on dynamic scoping (which R doesn't use), or
probably preferably, not have an f() function at all, as all it does is
call another function.

Also, I'd reference ?"<<-" for perhaps a cleaner way of doing global
assignments. Using this alone may solve your problems, as it may force
you to scope your code correctly.


-----Original Message-----
From: bogdan romocea [mailto:br44114 at yahoo.com] 
Sent: Tuesday, January 11, 2005 9:26 AM
To: r-help at stat.math.ethz.ch
Subject: [R] global objects not overwritten within function


Dear useRs,

I have a function that creates several global objects with
assign("obj",obj,.GlobalEnv), and which I need to run iteratively in
another function. The code is similar to

f <- function(...) {
assign("obj",obj,.GlobalEnv)
}
fct <- function(...) {
for (i in 1:1000)
	{
	...
	f(...)	
	...obj...
	rm(obj)	#code fails without this line
	}
}

I don't understand why f(), when run in a for() loop inside fct(), does
not overwrite the global object 'obj'. If I don't delete 'obj' after I
use it, the code fails - the same objects created by the first
iteration are used by subsequent iterations. 

I checked ?assign and the Evaluation chapter in 'R Language Definition'
but still don't understand why the above happens. Can someone briefly
explain or suggest something I should read? By the way, I don't want to
use 'better' techniques (lists, functions that return values instead of
creating global objects etc) - I want to create global objects with f()
and overwrite them again and again within fct().

Thank you,
b.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From MSchwartz at MedAnalytics.com  Tue Jan 11 17:39:47 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 11 Jan 2005 10:39:47 -0600
Subject: [R] Graphical table in R
In-Reply-To: <Pine.LNX.4.21.0501111454180.25669-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0501111454180.25669-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <1105461587.19635.60.camel@horizons.localdomain>

On Tue, 2005-01-11 at 14:59 +0000, Dan Bolser wrote:
> On 10 Jan 2005, Peter Dalgaard wrote:
> 
> >Dan Bolser <dmb at mrc-dunn.cam.ac.uk> writes:
> >
> >> Cheers. This is really me just being lazy (as usual). The latex
> function
> >> in Hmisc allows me to make a .ps file then grab a screen shot of
> that ps
> >> and make a .png file.
> >> 
> >> I would just like to use plot so I can wrap it in a png command and
> not
> >> have to use the 'screen shot' in between.
> >
> >A screen shot of a ps file? That sounds ... weird. If you can view
> it,
> >presumably you have Ghostscript and that can do png files.
> 
> The thing is the ps file has teh wrong size, so I end up with a small
> table in the corner of a big white page (using imageMagick convert
> function).
> 
> I havent tried ghostscript (don't know the cmd).
> 
> I could set the paper size correctly if I knew the size of my table,
> but I
> don't know how to calculate that before hand and feed it into the
> latex
> commands (Hmisc).
> 
> Seems like I should roll my own table with the plot command and
> 'primatives' (like the demo(mathplot)) - I just hoped that someone had
> already done the hard work for me and I could type something like...
> 
> plot.xtable(x)
> 
> x = any R object that makes sense to have a tabular output.
> 
> Seems like such a function done correctly could be usefull for helping
> people write up (hem) analysis.
> 
> Thanks again for the help everyone.
> 
> Dan.

Dan,

I think that taking Peter's/Thomas' solution provides a substantial
level of flexibility in formatting. I wish that I had thought of that
approach... :-)

For example:

  plot(1:10, type="n")

  txt <- capture.output(ftable(UCBAdmissions))

  par(family = "mono")

  text(4, 8, paste(txt,collapse="\n"))

  text(4, 6, paste(txt,collapse="\n"), cex = 0.75)

  text(4, 4, paste(txt,collapse="\n"), cex = 0.5)


Using par(cex) in the call to text() and modifying the x,y coordinates
will enable you to place the table anywhere within the plot region and
also adjust the overall size of the table by modifying the font size.

You can also use the 'adj' and 'pos' arguments in the call to text() to
adjust the placement of the table, so rather than being centered on x,y
(the default) it could be moved accordingly. See ?text for more
information.

Finally, you can even put a frame around the table by crudely using
strwidth() and strheight(). Some additional hints on this would be
available by reviewing the code for legend()...

# Do this for the first table (assumes 'cex = 1'):

# Get table width and add 10%
table.w <- max(strwidth(txt)) * 1.1

# Get table height (not including space between rows)
table.h <- sum(strheight(txt))

rect(4 - (table.w / 2), 8 - (table.h), 
     4 + (table.w / 2), 8 + (table.h))


It would take some work to combine all of this into a single function,
providing for additional flexibility in positioning, frame line
types/color/width, adjusting for 'cex' and so on. It could be done
though...

This is, in effect, taking an entire R character object and plotting it.

Does that help?

Marc



From Luisr at frs.fo  Tue Jan 11 16:34:20 2005
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Tue, 11 Jan 2005 15:34:20 +0000
Subject: [R] RODBC package -- sqlQuery(channel,.....,nullstring=0) still
	gives NA's
Message-ID: <s1e3f201.023@ffdata.setur.fo>

R-help,

I'm using the RODBC package to retrieve data froma ODBC database which
contain NA's.

By using the argument nullstring = "0"  in sqlQuery() I expect to
coerce them to numeric but still get NA's in my select.

I'm running on Windows XP

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R 


Thank you



From colin.a.cunningham at intel.com  Tue Jan 11 18:57:50 2005
From: colin.a.cunningham at intel.com (Cunningham, Colin A)
Date: Tue, 11 Jan 2005 09:57:50 -0800
Subject: [R] Meeker's SPLIDA Reliability in R
Message-ID: <1AC79F16F5C5284499BB9591B33D6F00033FB12F@orsmsx408>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050111/003470f8/attachment.pl

From jtk at cmp.uea.ac.uk  Tue Jan 11 17:38:59 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Tue, 11 Jan 2005 16:38:59 +0000
Subject: [R] thanks
In-Reply-To: <freemail.20050011155958.44093@fm13.freemail.hu>
References: <freemail.20050011155958.44093@fm13.freemail.hu>
Message-ID: <20050111163859.GD11799@jtkpc.cmp.uea.ac.uk>

On Tue, Jan 11, 2005 at 03:59:58PM +0100, Cserh?ti M?ty?s wrote:

> I would like to run an R script from the Linux prompt. Is there any way 
> possible to do this? The reason is, the calculation that I'm doing takes a 
> few hours, and I would like to automatize it.
> 
> Or does it mean that I have to run source within the R prompt?
> 
> Or is there a way to do the automatization within the R prompt?

The standard way (well, my usual way, anyway) is to just use I/O
redirection:

    linux> R --vanilla < stuff.r

is, for the most part (see below), equivalent to

    linux> R
    > source("stuff.r");

The --vanilla option is necessary to suppress any interactive questions
concerning workspace saving (i.e. the "Save workspace image? [y/n/c]"
thing); differences between the automated and the interactive form may
be due to your script depending on some saved environment, or some
stuff in your init files.

I'd like to encourage you to automate your calculations, as this enhances
not only convenience but also reproducibility of your results.

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From ripley at stats.ox.ac.uk  Tue Jan 11 18:01:21 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 11 Jan 2005 17:01:21 +0000 (GMT)
Subject: [R] global objects not overwritten within function
In-Reply-To: <20050111142628.48038.qmail@web50303.mail.yahoo.com>
References: <20050111142628.48038.qmail@web50303.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0501111655510.7548@gannet.stats>

On Tue, 11 Jan 2005, bogdan romocea wrote:

> Dear useRs,
>
> I have a function that creates several global objects with
> assign("obj",obj,.GlobalEnv), and which I need to run iteratively in
> another function. The code is similar to
>
> f <- function(...) {
> assign("obj",obj,.GlobalEnv)
> }
> fct <- function(...) {
> for (i in 1:1000)
> 	{
> 	...
> 	f(...)
> 	...obj...
> 	rm(obj)	#code fails without this line
> 	}
> }
>
> I don't understand why f(), when run in a for() loop inside fct(), does
> not overwrite the global object 'obj'. If I don't delete 'obj' after I
> use it, the code fails - the same objects created by the first
> iteration are used by subsequent iterations.
>
> I checked ?assign and the Evaluation chapter in 'R Language Definition'
> but still don't understand why the above happens. Can someone briefly
> explain or suggest something I should read? By the way, I don't want to
> use 'better' techniques (lists, functions that return values instead of
> creating global objects etc) - I want to create global objects with f()
> and overwrite them again and again within fct().

Since you are not using ... in the sense it is used in R, we have little 
idea of what your real code looks like and so what it does.

Can you please give a small real example that fails.  Here is one that 
works, yet has all the features I can deduce from your non-code:

f <- function(x) assign("obj", x, pos=.GlobalEnv)
fct <- function()
{
    for(i in 1:2) {
      x <- i+3
      f(x)
      print(obj)
    }
}
> fct()
[1] 4
[1] 5
> obj
[1] 5

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fxx103 at hotmail.com  Tue Jan 11 19:45:21 2005
From: fxx103 at hotmail.com (Francisca xuan)
Date: Tue, 11 Jan 2005 13:45:21 -0500
Subject: [R] help on integrate function
Message-ID: <BAY102-F30D2949669020F06A95FDA9B880@phx.gbl>

here is a function I wrote

cdfest=function(t,lambda,delta,x,y){
    a1=mean(x< t)
    a2=mean(x< t-delta)
    a3=mean(y1< t)
   s=((1-lambda)*a1+lambda*a2-a3)^2
   s
}

when I try to integrate over t, I got this message:

>integrate(cdfest,0,4,lambda=0.3,delta=1,x=x,y=y1)
Error in integrate(cdfest, 0, 4, lambda = 0.3, delta = 1, x = x, y = y1) :
        evaluation of function gave a result of wrong length

but the function is definitely in one dimension. what is wrong?

any suggestions are welcome. thanks



From Rau at demogr.mpg.de  Tue Jan 11 17:39:56 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Tue, 11 Jan 2005 17:39:56 +0100
Subject: [R] useR 2005 ?
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E61505EB@HERMES.demogr.mpg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050111/cb05cc89/attachment.pl

From ligges at statistik.uni-dortmund.de  Tue Jan 11 22:35:40 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 11 Jan 2005 22:35:40 +0100
Subject: [R] thanks
In-Reply-To: <freemail.20050011155958.44093@fm13.freemail.hu>
References: <freemail.20050011155958.44093@fm13.freemail.hu>
Message-ID: <41E446AC.6030704@statistik.uni-dortmund.de>

Cserh?ti M?ty?s wrote:

> 
> Dear all,
> 
> Thanks to those 3 people who sent me answers to my question. Got 
> the problem solved. Great!
> 
> Now, another question of mine is:
> 
> I would like to run an R script from the Linux prompt. Is there any way 
> possible to do this? The reason is, the calculation that I'm doing takes a 
> few hours, and I would like to automatize it.
> 
> Or does it mean that I have to run source within the R prompt?
> 
> Or is there a way to do the automatization within the R prompt?
> 
> Thanks, Matthew
> 
> u.i. K?szi, Zoli!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

See
a) the manual "An Introduction to R", Appendix B
b) "inside" R type:  ?BATCH
c) "outside" R type: R CMD BATCH --help

Uwe Ligges



From dalmiral at umich.edu  Tue Jan 11 21:55:27 2005
From: dalmiral at umich.edu (Daniel Almirall)
Date: Tue, 11 Jan 2005 15:55:27 -0500 (EST)
Subject: [R] getting variable names from formula
Message-ID: <Pine.SOL.4.58.0501111522400.22803@mspacman.gpcc.itd.umich.edu>

R-list,

1.  Given a formula (f) w variables referencing some data set (dat), is
there any easier/faster way than this to get the names (in character form)
of the variables on the RHS of '~' ?

 dat <- data.frame(x1 = x1 <- rnorm(100,0,1), x2 = x2 <- rnorm(100,0,1), y = x1 + x2 + rnorm(100,0,1))

 f <- y ~ x1 + x2

 mf <- model.frame(f, data=dat)

 mt <- attr(mf, "terms")

 predvarnames <- attr(mt, "term.labels")

> predvarnames
[1] "x1" "x2"

-----

2.  Also, is there an easy/fast way to do it, without having the data set
(dat) available?  That is, not using 'model.frame' which requires 'data'?
I understand that one approach for this is to use the way formulas are
stored as 'list's.  For example, this works

 predvarnames <- character()

 for (i in 2:length(f[[3]]) ){

 predvarnames <- c(predvarnames, as.character(f[[3]][[i]]))

 }

> predvarnames
[1] "x1" "x2"

but is there a better way?

Thanks,
Danny



From dmb at mrc-dunn.cam.ac.uk  Tue Jan 11 18:16:52 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 11 Jan 2005 17:16:52 +0000 (GMT)
Subject: [R] Graphical table in R
In-Reply-To: <1105461587.19635.60.camel@horizons.localdomain>
Message-ID: <Pine.LNX.4.21.0501111713280.25669-100000@mail.mrc-dunn.cam.ac.uk>

On Tue, 11 Jan 2005, Marc Schwartz wrote:

>On Tue, 2005-01-11 at 14:59 +0000, Dan Bolser wrote:
>> On 10 Jan 2005, Peter Dalgaard wrote:
>> 
>> >Dan Bolser <dmb at mrc-dunn.cam.ac.uk> writes:
>> >
>> >> Cheers. This is really me just being lazy (as usual). The latex
>> function
>> >> in Hmisc allows me to make a .ps file then grab a screen shot of
>> that ps
>> >> and make a .png file.
>> >> 
>> >> I would just like to use plot so I can wrap it in a png command and
>> not
>> >> have to use the 'screen shot' in between.
>> >
>> >A screen shot of a ps file? That sounds ... weird. If you can view
>> it,
>> >presumably you have Ghostscript and that can do png files.
>> 
>> The thing is the ps file has teh wrong size, so I end up with a small
>> table in the corner of a big white page (using imageMagick convert
>> function).
>> 
>> I havent tried ghostscript (don't know the cmd).
>> 
>> I could set the paper size correctly if I knew the size of my table,
>> but I
>> don't know how to calculate that before hand and feed it into the
>> latex
>> commands (Hmisc).
>> 
>> Seems like I should roll my own table with the plot command and
>> 'primatives' (like the demo(mathplot)) - I just hoped that someone had
>> already done the hard work for me and I could type something like...
>> 
>> plot.xtable(x)
>> 
>> x = any R object that makes sense to have a tabular output.
>> 
>> Seems like such a function done correctly could be usefull for helping
>> people write up (hem) analysis.
>> 
>> Thanks again for the help everyone.
>> 
>> Dan.
>
>Dan,
>
>I think that taking Peter's/Thomas' solution provides a substantial
>level of flexibility in formatting. I wish that I had thought of that
>approach... :-)
>
>For example:
>
>  plot(1:10, type="n")
>
>  txt <- capture.output(ftable(UCBAdmissions))
>
>  par(family = "mono")
>
>  text(4, 8, paste(txt,collapse="\n"))
>
>  text(4, 6, paste(txt,collapse="\n"), cex = 0.75)
>
>  text(4, 4, paste(txt,collapse="\n"), cex = 0.5)
>
>
>Using par(cex) in the call to text() and modifying the x,y coordinates
>will enable you to place the table anywhere within the plot region and
>also adjust the overall size of the table by modifying the font size.
>
>You can also use the 'adj' and 'pos' arguments in the call to text() to
>adjust the placement of the table, so rather than being centered on x,y
>(the default) it could be moved accordingly. See ?text for more
>information.
>
>Finally, you can even put a frame around the table by crudely using
>strwidth() and strheight(). Some additional hints on this would be
>available by reviewing the code for legend()...
>
># Do this for the first table (assumes 'cex = 1'):
>
># Get table width and add 10%
>table.w <- max(strwidth(txt)) * 1.1
>
># Get table height (not including space between rows)
>table.h <- sum(strheight(txt))
>
>rect(4 - (table.w / 2), 8 - (table.h), 
>     4 + (table.w / 2), 8 + (table.h))
>
>
>It would take some work to combine all of this into a single function,
>providing for additional flexibility in positioning, frame line
>types/color/width, adjusting for 'cex' and so on. It could be done
>though...
>
>This is, in effect, taking an entire R character object and plotting it.
>
>Does that help?

It certainly fits the bill. I will give it a go, but I may stick with the
latex() functions in Hmisc.

Thanks for all the help, it is a really elegant solution in the end :)

Dan.


>
>Marc
>
>



From dmb at mrc-dunn.cam.ac.uk  Tue Jan 11 17:50:58 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 11 Jan 2005 16:50:58 +0000 (GMT)
Subject: [R] Standard error for the area under a smoothed ROC curve?
Message-ID: <Pine.LNX.4.21.0501111628290.25669-100000@mail.mrc-dunn.cam.ac.uk>


Hello, 

I am making some use of ROC curve analysis. 

I find much help on the mailing list, and I have used the Area Under the
Curve (AUC) functions from the ROC function in the bioconductor project...

http://www.bioconductor.org/repository/release1.5/package/Source/
ROC_1.0.13.tar.gz 

However, I read here...

http://www.medcalc.be/manual/mpage06-13b.php

"The 95% confidence interval for the area can be used to test the
hypothesis that the theoretical area is 0.5. If the confidence interval
does not include the 0.5 value, then there is evidence that the laboratory
test does have an ability to distinguish between the two groups (Hanley &
McNeil, 1982; Zweig & Campbell, 1993)."

But aside from early on the above article is short on details. Can anyone
tell me how to calculate the CI of the AUC calculation?


I read this...

http://www.bioconductor.org/repository/devel/vignette/ROCnotes.pdf

Which talks about resampling (by showing R code), but I can't understand
what is going on, or what is calculated (the example given is specific to
microarray analysis I think).

I think a general AUC CI function would be a good addition to the ROC
package.




One more thing, in calculating the AUC I see the splines function is
recomended over the approx function. Here...

http://tolstoy.newcastle.edu.au/R/help/04/10/6138.html

How would I rewrite the following AUC functions (adapted from bioconductor
source) to use splines (or approxfun or splinefun) ...

> spe # Specificity
 [1] 0.02173913 0.13043478 0.21739130 0.32608696 0.43478261 0.54347826
 [7] 0.65217391 0.76086957 0.89130435 1.00000000 1.00000000 1.00000000
[13] 1.00000000

> sen # Sensitivity
 [1] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.9302326 0.8139535
 [8] 0.6976744 0.5581395 0.4418605 0.3488372 0.2325581 0.1162791

trapezint(1-spe,sen)
my.integrate(1-spe,sen)

## Functions
## Nicked (and modified) from the ROC function in bioconductor.
"trapezint" <-
function (x, y, a = 0, b = 1)
{
    if (x[1] > x[length(x)]) {
      x <- rev(x)
      y <- rev(y)
    }
    y <- y[x >= a & x <= b]
    x <- x[x >= a & x <= b]
    if (length(unique(x)) < 2)
        return(NA)
    ya <- approx(x, y, a, ties = max, rule = 2)$y
    yb <- approx(x, y, b, ties = max, rule = 2)$y
    x <- c(a, x, b)
    y <- c(ya, y, yb)
    h <- diff(x)
    lx <- length(x)
    0.5 * sum(h * (y[-1] + y[-lx]))
}

"my.integrate" <-
function (x, y, t0 = 1)
{
    f <- function(j) approx(x,y,j,rule=2,ties=max)$y
    integrate(f, 0, t0)$value
}





Thanks for any pointers,
Dan.



From tom_hoary at web.de  Tue Jan 11 12:49:04 2005
From: tom_hoary at web.de (Thomas =?ISO-8859-1?Q?Sch=F6nhoff?=)
Date: Tue, 11 Jan 2005 12:49:04 +0100
Subject: [R] Ess packages for Suse-9.2 available ?
Message-ID: <1105444144.16642.8.camel@linux.tlink.de>

Hi,

I've just managed to install R on Suse-9.2. Unfortunately there seem to
be no Ess packages in Suse distri available. Doesn't seem like that Suse
is ever to integrate these packages (since 2000)!?  
Are there any precompiled binaries around? So far I haven't been lucky
to find any of them on rpmfind and similar sites.
Maybe someone has already Ess binary for Suse-9.2 and is willing to
share this one!? Otherwise I had to go through the hassle of building an
rpm packages, which would be a real challenge to do for me.

sincerely

Thomas



From reid_huntsinger at merck.com  Tue Jan 11 16:33:30 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 11 Jan 2005 10:33:30 -0500
Subject: [R] global objects not overwritten within function
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9249@uswpmx00.merck.com>

Do you do anything with "obj" in the elided code? When you say the code
fails, do you mean that somewhere it accesses "obj" and doesn't get the
version in .GlobalEnv? The line "rm(obj)" removes the copy in the function's
environment, so I guess there is a local copy of "obj", otherwise the line
would have no effect. 

If you want the "obj" in .GlobalEnv, you have to ask for it, with "get",
unless there are no local copies.

More code, or better, a distilled working example, the R version, and
platform would be helpful.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
Sent: Tuesday, January 11, 2005 9:26 AM
To: r-help at stat.math.ethz.ch
Subject: [R] global objects not overwritten within function


Dear useRs,

I have a function that creates several global objects with
assign("obj",obj,.GlobalEnv), and which I need to run iteratively in
another function. The code is similar to

f <- function(...) {
assign("obj",obj,.GlobalEnv)
}
fct <- function(...) {
for (i in 1:1000)
	{
	...
	f(...)	
	...obj...
	rm(obj)	#code fails without this line
	}
}

I don't understand why f(), when run in a for() loop inside fct(), does
not overwrite the global object 'obj'. If I don't delete 'obj' after I
use it, the code fails - the same objects created by the first
iteration are used by subsequent iterations. 

I checked ?assign and the Evaluation chapter in 'R Language Definition'
but still don't understand why the above happens. Can someone briefly
explain or suggest something I should read? By the way, I don't want to
use 'better' techniques (lists, functions that return values instead of
creating global objects etc) - I want to create global objects with f()
and overwrite them again and again within fct().

Thank you,
b.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From murthys at us.ibm.com  Wed Jan 12 07:06:57 2005
From: murthys at us.ibm.com (Sridhar Murthy)
Date: Wed, 12 Jan 2005 00:06:57 -0600
Subject: [R] Help requested 
Message-ID: <OFD59D0A7B.F09A8063-ON86256F87.001FA49D-86256F87.0021970E@us.ibm.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050112/68b3f166/attachment.pl

From mikewlcheung at hku.hk  Wed Jan 12 08:54:38 2005
From: mikewlcheung at hku.hk (Mike Cheung)
Date: Wed, 12 Jan 2005 15:54:38 +0800
Subject: [R] insert missing values in a sequence
Message-ID: <41E4D7BE.70207@hku.hk>

Hello.

I have a data frame with two variables where x are incomplete sequences. 
I would like to see how I could complete the sequences in x and insert 
the missing values in y.

# Example data missing 5 and 7
x <- c(1:5, 8:10)
y <- rnorm(8)
data <- data.frame(x,y)
data
# Output
#   x          y
#1  1 -1.0693219
#2  2  0.9134950
#3  3  1.3231290
#4  4 -1.1376213
#5  5  0.5367879
#6  8  0.9595294
#7  9 -0.3254167
#8 10  0.1358020

# Expected output after some manipulations
    x          y
1  1 -1.0693219
2  2  0.9134950
3  3  1.3231290
4  4 -1.1376213
5  5  0.5367879
6  6         NA
7  7         NA
8  8  0.9595294
9  9 -0.3254167
10 10  0.1358020

Thanks a lot in advance!

Best,
Mike



From p.dalgaard at biostat.ku.dk  Tue Jan 11 16:40:17 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Jan 2005 16:40:17 +0100
Subject: [R] thanks
In-Reply-To: <freemail.20050011155958.44093@fm13.freemail.hu>
References: <freemail.20050011155958.44093@fm13.freemail.hu>
Message-ID: <x2d5wcas9q.fsf@biostat.ku.dk>

Cserh?ti M?ty?s <cs_matyi at freemail.hu> writes:

> Dear all,
> 
> Thanks to those 3 people who sent me answers to my question. Got 
> the problem solved. Great!
> 
> Now, another question of mine is:
> 
> I would like to run an R script from the Linux prompt. Is there any way 
> possible to do this? The reason is, the calculation that I'm doing takes a 
> few hours, and I would like to automatize it.
> 
> Or does it mean that I have to run source within the R prompt?
> 
> Or is there a way to do the automatization within the R prompt?

This could be what you are looking for:

R CMD BATCH myfile.R

or maybe

R --vanilla < myfile.R

both possibly followed by "&" to execute them in the background. 


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From vito_ricci at yahoo.com  Tue Jan 11 17:06:41 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Tue, 11 Jan 2005 17:06:41 +0100 (CET)
Subject: [R] Kolmogorov-Smirnof test for lognormal distribution with
	estimated parameters
Message-ID: <20050111160641.49643.qmail@web41212.mail.yahoo.com>

Hi Kwabena,

in addition to my preview reply:

you can use some normality test included in nortest
package transfroming log-normal data in normal data
using log-transformation.

See:
http://cran.r-mirror.de/src/contrib/Descriptions/nortest.html

ad.test  Anderson-Darling test for normality
cvm.test Cramer-von Mises test for normality
lillie.test Lilliefors (Kolmogorov-Smirnov) test for  
                     normality 
Pearson.test Pearson chi-square test for normality
sf.test    Shapiro-Francia test for normality

Best Regards,

Vito


you wrote:

Hello all,

Would somebody be kind enough to show me how to do a
KS test in R for a
lognormal distribution with ESTIMATED parameters. The
R function
ks.test()says "the parameters specified must be
prespecified and not
estimated from the data" Is there a way to correct
this when one uses
estimated data?

Regards,

Kwabena.

--------------------------------------------
Kwabena Adusei-Poku
University of Goettingen
Institute of Statistics and Econometrics
Platz der Goettingen Sieben 5
37073 Goettingen
Germany
Tel: +49-(0)551-394794

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From pmccask at cyllene.uwa.edu.au  Wed Jan 12 09:07:42 2005
From: pmccask at cyllene.uwa.edu.au (Pamela McCaskie)
Date: Wed, 12 Jan 2005 16:07:42 +0800 (WST)
Subject: [R] Backsolve error
In-Reply-To: <20050111153900.37196.qmail@web41210.mail.yahoo.com>
References: <20050111153900.37196.qmail@web41210.mail.yahoo.com>
Message-ID: <Pine.LNX.4.56.0501121602430.15939@cyllene.uwa.edu.au>

Dear all
I am receiving the following error, but I don't understand what it means.
Can someone help me?

Error in MEEM(object, conLin, control$niterEM) :
        Singularity in backsolve at level 0, block 1

cheers
--
Pamela A McCaskie



From murthys at us.ibm.com  Wed Jan 12 07:05:57 2005
From: murthys at us.ibm.com (Sridhar Murthy)
Date: Tue, 11 Jan 2005 23:05:57 -0700
Subject: [R] Help requested 
Message-ID: <OFD59D0A7B.F09A8063-ON86256F87.001FA49D-86256F87.00217FA3@us.ibm.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050111/659508f0/attachment.pl

From jeff at kanecap.com  Wed Jan 12 00:18:55 2005
From: jeff at kanecap.com (Jeff Enos)
Date: Tue, 11 Jan 2005 18:18:55 -0500
Subject: [R] Nested ifelse - is there a better way?
Message-ID: <16868.24287.140185.569661@gargle.gargle.HOWL>

Dear r-help,

I'm interested in finding a better way to add a column to a data frame
when calculating the new column requires more than one conditional.

For example, if I wanted to associate a character string in
{"Pos","Neg","Zero"} with each number in the following data frame:

> d <- data.frame(num = -2:2)
> d
  num
1  -2
2  -1
3   0
4   1
5   2

I could use a nested ifelse:

> d$p1 <- ifelse(d$num < 0, "Neg", ifelse(d$num == 0, "Zero", "Pos"))
> d
  num   p1
1  -2  Neg
2  -1  Neg
3   0 Zero
4   1  Pos
5   2  Pos

which I believe becomes cumbersome if more conditionals are required.
I've also considered using sapply:

> d$p2 <- sapply(d$num, function(num) { if (num < 0) { "Neg" } else if (num == 0) { "Zero" } else { "Pos" } })
> d
  num   p1   p2
1  -2  Neg  Neg
2  -1  Neg  Neg
3   0 Zero Zero
4   1  Pos  Pos
5   2  Pos  Pos

Is there a more elegant approach to situations like this?

Thanks in advance,

Jeff



From br44114 at yahoo.com  Wed Jan 12 02:31:42 2005
From: br44114 at yahoo.com (bogdan romocea)
Date: Tue, 11 Jan 2005 17:31:42 -0800 (PST)
Subject: [R] global objects not overwritten within function
In-Reply-To: <Pine.LNX.4.61.0501111655510.7548@gannet.stats>
Message-ID: <20050112013142.3620.qmail@web50303.mail.yahoo.com>

Thank you to everyone who replied. I had no idea that ... means
something in R, I only wanted to make the code look simpler. I'm
pasting below the functional equivalent of what took me yesterday a
couple of hours to debug. Function f() takes several arguments (that's
why I want to have the code as a function) and creates several objects.
I then need to use those objects in another function fct(), and I want
to overwrite them to save memory (they're pretty large).

It appears that Robert's guess (dynamic/lexical scoping) explains
what's going on. I've noticed though another strange (to me) issue:
without indexing (such as obj1 <- obj1[obj1 > 0] - which I need to use
though), fct() prints the expected values even without removing the
objects after each iteration. However, after indexing is introduced,
rm() must be used to make fct() return the intended output. How would
that be explained?

Kind regards,
b.

f <- function(read,position){
obj1 <- 5 * read[position]:(read[position]+5)
obj2 <- 7 * read[position]:(read[position]+5)
assign("obj1",obj1,.GlobalEnv)
assign("obj2",obj2,.GlobalEnv)
}
fct <- function(input){
for (i in 1:5)
	{
	f(input,i)
	obj1 <- obj1[obj1 > 0]
	obj2 <- obj2[obj2 > 0]
	print(obj1)
	print(obj2)
#	rm(obj1,obj2)	#get intended results with this line
	}
}
a <- 1:10
fct(a)







--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> On Tue, 11 Jan 2005, bogdan romocea wrote:
> 
> > Dear useRs,
> >
> > I have a function that creates several global objects with
> > assign("obj",obj,.GlobalEnv), and which I need to run iteratively
> in
> > another function. The code is similar to
> >
> > f <- function(...) {
> > assign("obj",obj,.GlobalEnv)
> > }
> > fct <- function(...) {
> > for (i in 1:1000)
> > 	{
> > 	...
> > 	f(...)
> > 	...obj...
> > 	rm(obj)	#code fails without this line
> > 	}
> > }
> >
> > I don't understand why f(), when run in a for() loop inside fct(),
> does
> > not overwrite the global object 'obj'. If I don't delete 'obj'
> after I
> > use it, the code fails - the same objects created by the first
> > iteration are used by subsequent iterations.
> >
> > I checked ?assign and the Evaluation chapter in 'R Language
> Definition'
> > but still don't understand why the above happens. Can someone
> briefly
> > explain or suggest something I should read? By the way, I don't
> want to
> > use 'better' techniques (lists, functions that return values
> instead of
> > creating global objects etc) - I want to create global objects with
> f()
> > and overwrite them again and again within fct().
> 
> Since you are not using ... in the sense it is used in R, we have
> little 
> idea of what your real code looks like and so what it does.
> 
> Can you please give a small real example that fails.  Here is one
> that 
> works, yet has all the features I can deduce from your non-code:
> 
> f <- function(x) assign("obj", x, pos=.GlobalEnv)
> fct <- function()
> {
>     for(i in 1:2) {
>       x <- i+3
>       f(x)
>       print(obj)
>     }
> }
> > fct()
> [1] 4
> [1] 5
> > obj
> [1] 5
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From liyin.lee at gmail.com  Wed Jan 12 04:18:38 2005
From: liyin.lee at gmail.com (Liyin Lee)
Date: Wed, 12 Jan 2005 11:18:38 +0800
Subject: [R] a question about Boxplot
Message-ID: <5ac4fe9005011119181ea15c5e@mail.gmail.com>

Dear all,
I have a question about boxplot function in R. 

Is there a way to represent four different groups in the boxplot in
black and white patterns, like adding different shadding patterns for
each box?
The Journal requires figures in black and white patterens rather than
grayscales so that I can not represent groups using different colors.

Thank you very much.

LY



From helprhelp at yahoo.com  Tue Jan 11 20:22:38 2005
From: helprhelp at yahoo.com (Weiwei Shi)
Date: Tue, 11 Jan 2005 11:22:38 -0800 (PST)
Subject: [R] question about scalability of R
Message-ID: <20050111192238.12633.qmail@web61304.mail.yahoo.com>

Hi, there:
I am a newbie of R but I am very interested in
applying R in my research. I got some help before from
this mailist and I really appreciate it.

Here is my another question. I think it might be a
common problem for some R's packages.

Recently I used rpart  to do some research and my
dataset has around 200 vars and over 50,000
observations. I loaded half of the data into R (I
cannot load the whole dataset into it, even) and it is
really slow.

I think the implementation of rpart is kinda
memory-resident. I am wondering if there is another
implementation or some way to optimize it to solve the
problem.



Thanks in advance,

Weiwei



From r.ramyar at gmail.com  Tue Jan 11 20:33:41 2005
From: r.ramyar at gmail.com (Rick Ram)
Date: Tue, 11 Jan 2005 19:33:41 +0000
Subject: [R] CUSUM SQUARED structural breaks approach?
In-Reply-To: <20050111151032.07d05ab4.Achim.Zeileis@wu-wien.ac.at>
References: <5f4f8279050111051031974bc4@mail.gmail.com>
	<20050111151032.07d05ab4.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <5f4f82790501111133193a21fb@mail.gmail.com>

On Tue, 11 Jan 2005 15:10:32 +0100, Achim Zeileis
<Achim.Zeileis at wu-wien.ac.at> wrote:
> On Tue, 11 Jan 2005 13:10:51 +0000 Rick Ram wrote:
> 
> > Dear all,
> >
> > Does anyone know where there is R or S code for the CUSUM SQUARED
> > structural breaks approach?   (Brown, Durban and Evans, 1975 - used in
> > Pesaran and Timmerman, 2002)
> 
> Not to my knowledge.
> 
> > The problem is that the breaks package
> 
> I guess you're referring to the strucchange package. There is no package
> breaks, at least not on CRAN.

Yes, sorry... a miscitation :)

> 
> > only appears to offer the
> > standard 'unsquared' CUSUM, even though it appears most think it is
> > inferior to the squared version.
> 
> Yes, it's part of the folklore that one should use the CUSUM of squares
> rather than the CUSUM test. However, in many scenarios one wouldn't want
> to use either test. The BDE CUSUM test is good when you want to know
> when 1-step ahead predictions start to break down (and that works only
> if the break occurs early in the sample period). The CUSUM of squares
> test is more appropriate if you want to detect changes in the variance
> (rather than the conditional mean).
> For both problems (breaks in the mean / breaks in the variance) there
> are other tests which are usually more appropriate than the BDE CUSUM or
> CUSUM of squares test which is why I haven't implemented the latter.
> (But, of course, no test can be uniformly superior due to the vast
> alternative.)

Groundwork for the choice of break method in my specific application
has already been done - otherwise I would need to rework the wheel
(make a horribly detailed comparison of performance of break
approaches in context of modelling post break)

If it interests you, Pesaran & Timmerman 2002 compared CUSUM Squared,
BaiPerron and a time varying approach to detect singular previous
breaks in reverse ordered financial time series so as to update a
forecasting model.  I am building on the modelling aspect of it,
rather than breaks side, and have to assume their break model
conclusions are sound for econometric forecasting.

> 
> > It might appear to be a relatively
> > simple problem - just use the recursive residuals function, square
> > them and then take the cumulative sum.  But the problem is that the
> > normalisation and calculation of confidence levels is completely
> > different.
> 
> Yes. Computing the CUSUM of square process is straightforward using the
> recresid() methods. To obtain critical values, you will either have to
> simulate them or use the tabulated values referenced in BDE from Durbin
> (1969).

I had managed a basic cumulative sum of squared residuals as follows,
very simple:

rr <- (recresid(US.PT2002.lm, data=US[end:start,]))
rr <- rr^2
cumrr <- cumsum(rr)
plot(cumrr, type = "l")

This works "fine" i.e. the plot looks correct.  The problem is how to
appropriately normalise these to rescale them to what the CUSUM
squared procedure expects (this looks to be a different and more
complicated procedure than the normalisation used for the basic
CUSUM).  I am from an IT background and am slightly illiterate in
terms of math notation... guidance from anyone would be appreciated

Does anyone know if this represents some commonly performed type of
normalisation than exists in another function??

I will hunt out the 1969 paper for the critical values but prior to
doing this I am a bit confused as to how they will
implemented/interpreted... the CUSUM squared plot does/should run
diagonally up from left to right and there are two straight lines that
one would put around this from the critical values.  Hence, a
different interpretation/implementation of confidence levels than in
other contexts.  I realise this is not just a R thing but a problem
with my theoretical background.


Thanks for detailed reply!

Rick.


> 
> But depending on the model and hypothesis you want to test, another
> technique than CUSUM of squares might be more appropriate and also
> available in strucchange.

> 
> hth,
> Z
> 
> > Any help or pointers about where to look would be more than
> > appreciated!  Hopefully I have just missed obvious something in the
> > package...
> >
> > Many thanks,
> >
> > Rick R.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>



From p.marshall at slingshot.co.nz  Wed Jan 12 06:08:42 2005
From: p.marshall at slingshot.co.nz (Peter)
Date: Wed, 12 Jan 2005 18:08:42 +1300
Subject: [R] R for Mac OS 9.2.2?
Message-ID: <BE0B182A.57E%p.marshall@slingshot.co.nz>

Hi,

Just wondering if R works properly on Mac OS 9.2.2? I downloaded it and
everything seems to be fine. However, I attempted to drag a library copied
from a PC into my library folder and run it but got the error message:

> library(s20x)
Error in testRversion(descfields) : This package has not been installed
properly
 See the Note in ?library
In addition: Warning message:
the condition has length > 1 and only the first element will be used in: if
(!package.dependencies(fields, check = TRUE)) {

Would you be able to tell me how I can utilise this library on my Mac.

Many thanks,

Peter



From maechler at stat.math.ethz.ch  Wed Jan 12 10:11:39 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 12 Jan 2005 10:11:39 +0100
Subject: [R] R list problems ; "r-help-bounces" etc
In-Reply-To: <OFF350CAE8.BCCA5B0C-ON86256F86.00695D71-86256F86.006AE1D7@mmm.com>
References: <W4720312870158451105447939@asteria.telenet-ops.be>
	<OFF350CAE8.BCCA5B0C-ON86256F86.00695D71-86256F86.006AE1D7@mmm.com>
Message-ID: <16868.59851.926199.770645@stat.math.ethz.ch>

First things first :

  Our mail server had experienced a bad fit for a bit more than 16 hours, 
  from about 2005-01-11 15:24 GMT aka UTC
	  to 2005-01-12 07:37 GMT
  Of course, a timeout of such an extended period of time should
  have never happened, and we are very sorry about it.


>>>>> "AndyJ" == Andy Jaworski <apjaworski at mmm.com>
>>>>>     on Tue, 11 Jan 2005 13:27:26 -0600 writes:

    AndyJ> Could somebody please tell me what does the
    AndyJ> r-help-bounce address do?

r-help-bounces (final "s") that is.

All our mailman mailing list e-mails are sent with a header
"Sender: <mailinglist>-bounces at ...."
[The following is not 100% precise, since e-mail protocols are
 even a bit more complicated]
This is the address that your mail server (not your "mailer" aka
your mail client!) should use when it finds that your
own address is not valid and it cannot deliver mail to you.
The mail server should return the mail to the
"Sender" and tell the reason why the mail couldn't be delivered.

[BTW, also ``decently configured'' vacation programs should reply to
 the "Sender:" address when that differs from the "From:" one ..]

On our side, the mailing list software "mailman" receives all
the mails to *-bounces at ... and tries to deal with it properly.
In most cases mailman, registers that an address has had
problems, and if an address produces too many such bounces, it
will be automatically be "disabled".
mailman will not trying to forward mailing list mails to that
address, but still send a daily reminder for a while
before its completely unsubscribed.

Here is a version of the "R-help bounce processing" page that
usually only I as list administrator see :

  http://stat.ethz.ch/~maechler/r-help-bounce-processing.html

You can see the current setting of R-help's bounce processing
parameters.


    AndyJ> When I try to respond to an r-help post, my mailer
    AndyJ> (Lotus Notes) generates the r-help bounce return
    AndyJ> address automatically in addition to the original
    AndyJ> sender address and the r-help address.  

this is a clear mis-feature (aka "bug") in your mailer.
Without knowning the exact protocols, I know that a user's reply
shouldn't go to "Sender:" if there was a "From:"

    AndyJ> I responded to an r-help message last night and I
    AndyJ> never saw my response posted.  Could this be due to
    AndyJ> the r-help-bounce being there?

no, see above.

    AndyJ> Thanks in advance,

you're welcome.

Martin Maechler, ETH Zurich



From ripley at stats.ox.ac.uk  Wed Jan 12 10:14:22 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 12 Jan 2005 09:14:22 +0000 (GMT)
Subject: [R] RODBC package -- sqlQuery(channel,.....,nullstring=0) still
	gives NA's
In-Reply-To: <s1e3f201.023@ffdata.setur.fo>
References: <s1e3f201.023@ffdata.setur.fo>
Message-ID: <Pine.LNX.4.61.0501120908320.10520@gannet.stats>

On Tue, 11 Jan 2005, Luis Rideau Cruz wrote:

> R-help,
>
> I'm using the RODBC package to retrieve data froma ODBC database which
> contain NA's.
>
> By using the argument nullstring = "0"  in sqlQuery() I expect to
> coerce them to numeric but still get NA's in my select.

You need to read the help page (as the posting guide asks): it says

na.strings: character string(s) to be mapped to 'NA' when reading
           character data.

which is the opposite of what you are saying you want to do.

An ODBC database cannot contain NA's.  It may contain NULLs, and it may 
contain "NA", so we have no idea what you mean.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jan 12 10:17:02 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 12 Jan 2005 09:17:02 +0000 (GMT)
Subject: [R] help on integrate function
In-Reply-To: <BAY102-F30D2949669020F06A95FDA9B880@phx.gbl>
References: <BAY102-F30D2949669020F06A95FDA9B880@phx.gbl>
Message-ID: <Pine.LNX.4.61.0501120915030.10520@gannet.stats>

On Tue, 11 Jan 2005, Francisca xuan wrote:

> here is a function I wrote
>
> cdfest=function(t,lambda,delta,x,y){
>   a1=mean(x< t)
>   a2=mean(x< t-delta)
>   a3=mean(y1< t)
>  s=((1-lambda)*a1+lambda*a2-a3)^2
>  s
> }
>
> when I try to integrate over t, I got this message:
>
>> integrate(cdfest,0,4,lambda=0.3,delta=1,x=x,y=y1)
> Error in integrate(cdfest, 0, 4, lambda = 0.3, delta = 1, x = x, y = y1) :
>       evaluation of function gave a result of wrong length
>
> but the function is definitely in one dimension. what is wrong?

Please read the help page:

        f: an R function taking a numeric first argument and returning a
           numeric vector of the same length.  Returning a non-finite
           element will generate an error.

Your function does not do that: it returns a scalar for a vector input of 
length > 1, as the message clearly says.

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From PalosJ at bsci.com  Tue Jan 11 16:45:48 2005
From: PalosJ at bsci.com (Palos, Judit)
Date: Tue, 11 Jan 2005 10:45:48 -0500
Subject: [R] Breslow Day Test
Message-ID: <9166919A78EAD0119E3800805FA6D749031E53CB@ma2pr1.bsc2exc1.bsci.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050111/72116cf4/attachment.pl

From bates at wisc.edu  Wed Jan 12 10:32:59 2005
From: bates at wisc.edu (Douglas Bates)
Date: Wed, 12 Jan 2005 03:32:59 -0600
Subject: [R] getting variable names from formula
In-Reply-To: <Pine.SOL.4.58.0501111522400.22803@mspacman.gpcc.itd.umich.edu>
References: <Pine.SOL.4.58.0501111522400.22803@mspacman.gpcc.itd.umich.edu>
Message-ID: <41E4EECB.6050808@wisc.edu>

Daniel Almirall wrote:
> R-list,
> 
> 1.  Given a formula (f) w variables referencing some data set (dat), is
> there any easier/faster way than this to get the names (in character form)
> of the variables on the RHS of '~' ?
> 
>  dat <- data.frame(x1 = x1 <- rnorm(100,0,1), x2 = x2 <- rnorm(100,0,1), y = x1 + x2 + rnorm(100,0,1))
> 
>  f <- y ~ x1 + x2
> 
>  mf <- model.frame(f, data=dat)
> 
>  mt <- attr(mf, "terms")
> 
>  predvarnames <- attr(mt, "term.labels")
> 
> 
>>predvarnames
> 
> [1] "x1" "x2"
> 
> -----
> 
> 2.  Also, is there an easy/fast way to do it, without having the data set
> (dat) available?  That is, not using 'model.frame' which requires 'data'?
> I understand that one approach for this is to use the way formulas are
> stored as 'list's.  For example, this works
> 
>  predvarnames <- character()
> 
>  for (i in 2:length(f[[3]]) ){
> 
>  predvarnames <- c(predvarnames, as.character(f[[3]][[i]]))
> 
>  }
> 
> 
>>predvarnames
> 
> [1] "x1" "x2"
> 
> but is there a better way?
> 
> Thanks,
> Danny
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

That's exactly what the all.vars function does.  If you apply it to the 
formula you get all the names of variables referenced in the formula. 
If you only want the right hand side then apply it to the third 
component of the formula

 > f <- y ~ x1 + x2
 > all.vars(f)
[1] "y"  "x1" "x2"
 > all.vars(f[[3]])
[1] "x1" "x2"



From avneetaquarian at yahoo.com  Tue Jan 11 21:22:52 2005
From: avneetaquarian at yahoo.com (avneet singh)
Date: Tue, 11 Jan 2005 12:22:52 -0800 (PST)
Subject: [R] transcan() from Hmisc package for imputing data
Message-ID: <20050111202252.11509.qmail@web14928.mail.yahoo.com>

Hello:

I have been trying to impute missing values of a data
frame which has both numerical and categorical values
using the function transcan() with little luck.

Would you be able to give me a simple example where a
data frame is fed to transcan and it spits out a new
data frame with the NA values filled up?

Or is there any other function that i could use?

Thank you
avneet

=====
I believe in equality for everyone, except reporters and photographers.
~Mahatma Gandhi



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Jan 12 10:56:43 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 12 Jan 2005 10:56:43 +0100
Subject: [R] getting variable names from formula
References: <Pine.SOL.4.58.0501111522400.22803@mspacman.gpcc.itd.umich.edu>
Message-ID: <002c01c4f88d$04f77560$0540210a@www.domain>

maybe something like:

f <- y ~ x1 + x2
attr(terms(f), "term.labels")

but this wan't work if you have a more complex formula (e.g., f <- y ~ 
x1*x2 + I(x1^2)) and you want only c("x1", "x2").

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Daniel Almirall" <dalmiral at umich.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 11, 2005 9:55 PM
Subject: [R] getting variable names from formula


> R-list,
>
> 1.  Given a formula (f) w variables referencing some data set (dat), 
> is
> there any easier/faster way than this to get the names (in character 
> form)
> of the variables on the RHS of '~' ?
>
> dat <- data.frame(x1 = x1 <- rnorm(100,0,1), x2 = x2 <- 
> rnorm(100,0,1), y = x1 + x2 + rnorm(100,0,1))
>
> f <- y ~ x1 + x2
>
> mf <- model.frame(f, data=dat)
>
> mt <- attr(mf, "terms")
>
> predvarnames <- attr(mt, "term.labels")
>
>> predvarnames
> [1] "x1" "x2"
>
> -----
>
> 2.  Also, is there an easy/fast way to do it, without having the 
> data set
> (dat) available?  That is, not using 'model.frame' which requires 
> 'data'?
> I understand that one approach for this is to use the way formulas 
> are
> stored as 'list's.  For example, this works
>
> predvarnames <- character()
>
> for (i in 2:length(f[[3]]) ){
>
> predvarnames <- c(predvarnames, as.character(f[[3]][[i]]))
>
> }
>
>> predvarnames
> [1] "x1" "x2"
>
> but is there a better way?
>
> Thanks,
> Danny
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Kevin.Wang at maths.anu.edu.au  Wed Jan 12 11:01:09 2005
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Wed, 12 Jan 2005 21:01:09 +1100 (EST)
Subject: [R] (no subject)
In-Reply-To: <APEDJPMADIEPCPHPOGHIGEPBCEAA.bigby@woosh.co.nz>
References: <APEDJPMADIEPCPHPOGHIGEPBCEAA.bigby@woosh.co.nz>
Message-ID: <Pine.GSO.4.58.0501122059140.17135@yin>

Hi,

On Wed, 12 Jan 2005, Bigby wrote:

> Hello,
>
> numerical.summary(Data), which didnt work, he suggested we try library(s20x)
> first, which came up with an error on my console. I have version 2.0.1 of R

library(s20x) is a package written by the Department of Statistics at the
University of Auckland.  It is used for their STATS 201/208 courses.  It
is not on CRAN.  You may need to contact them for it.

But you can get most of it using other commands.  From memory it simply
combines several other R functions, such as summary(), quantile()...etc.

HTH,

Kevin

--------------------------------
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From lyon at fnal.gov  Wed Jan 12 01:10:31 2005
From: lyon at fnal.gov (Adam Lyon)
Date: Tue, 11 Jan 2005 18:10:31 -0600
Subject: [R] Destructor for S4 objects?
In-Reply-To: <Pine.LNX.4.61.0501082232540.3540@gannet.stats>
Message-ID: <BE09C717.F35B%lyon@fnal.gov>

Hi Robert,

It looks like there is no way to explicitly make an S4 object call a
function when it is garbage collected unless you resort to tricks with
reg.finalizer.

It turns out that Prof. Ripley's reply (thanks!!) had enough hints in it
that I was able to get the effect I wanted by using R's external pointer
facility. In fact it works quite nicely.

In a nutshell, I create a C++ object (with new) and then wrap its pointer
with an R external pointer using
SEXP rExtPtr = R_MakeExternalPtr( cPtr, aTag, R_NilValue);

Where cPtr is the C++/C pointer to the object and aTag is an R symbol
describing the pointer type [e.g. SEXP aTag =
install("this_is_a_tag_for_a_pointer_to_my_object")]. The final argument is
"a value to protect". I don't know what this means, but all of the examples
I saw use R_NilValue.

If you want a C++ function to be called when R loses the reference to the
external pointer (actually when R garbage collects it, or when R quits), do
R_RegisterCFinalizerEx( rExtPtr, (R_CFinalizer_t)functionToBeCalled, TRUE );

The TRUE means that R will call the "functionToBeCalled" if the pointer is
still around when R quits. I guess if you set it to FALSE, then you are
assuming that your shell can delete memory and/or release resources when R
quits. 

So return this external pointer to R (the function that new'ed it was called
by .Call or something similar) and stick it in a slot of your object. Then
when your object is garbage collected, "functionToBeCalled" will be called.
The slot would have the type "externalptr".

The functionToBeCalled contains the code to delete the C++ pointer or
release resources, for example...

SEXP functionToBeCalled( SEXP rExtPtr ) {
  // Get the C++ pointer
  MyThing* ptr = R_ExternalPtrAddr(rExtPtr);

  // Delete it
  delete ptr;

  // Clear the external pointer
  R_ClearExternalPtr(rExtPtr);

  return R_NilValue;
}

And there you have it.

There doesn't seem to be any official documentation on this stuff (at least
none that I could find). The best references I found are on the R developers
web page. See the links within  "some notes on _references, external
objects, or mutable state_ for R and a _simple implementation_ of external
references and finalization". Note that the documents are slightly out of
date (the function names have apparently been changed somewhat). The latter
one has some examples that are very helpful. And as Prof. Ripley pointed
out, RODBC uses this facility too, so look at that code.

Hope this was useful. Good luck.

--- Adam

Adam Lyon (lyon-at-fnal.gov)
Fermi National Accelerator Laboratory
Computing Division / D0 Experiment



From blindglobe at gmail.com  Wed Jan 12 11:52:39 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Wed, 12 Jan 2005 11:52:39 +0100
Subject: [R] Ess packages for Suse-9.2 available ?
In-Reply-To: <1105444144.16642.8.camel@linux.tlink.de>
References: <1105444144.16642.8.camel@linux.tlink.de>
Message-ID: <1abe3fa9050112025220d2fca8@mail.gmail.com>

Does SuSE have anything like Debian's "alien" for importing foreign packages?  

If so, then you might use the Debian packages.  

Otherwise, it isn't too hard to unpack in your home directory, and add
the appropriate  " (load-file ...)" statement to your .emacs /
.xemacs/init.el

best,
-tony



On Tue, 11 Jan 2005 12:49:04 +0100, Thomas Sch?nhoff <tom_hoary at web.de> wrote:
> Hi,
> 
> I've just managed to install R on Suse-9.2. Unfortunately there seem to
> be no Ess packages in Suse distri available. Doesn't seem like that Suse
> is ever to integrate these packages (since 2000)!?
> Are there any precompiled binaries around? So far I haven't been lucky
> to find any of them on rpmfind and similar sites.
> Maybe someone has already Ess binary for Suse-9.2 and is willing to
> share this one!? Otherwise I had to go through the hassle of building an
> rpm packages, which would be a real challenge to do for me.
> 
> sincerely
> 
> Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From Luisr at frs.fo  Wed Jan 12 11:58:23 2005
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Wed, 12 Jan 2005 10:58:23 +0000
Subject: [R] RODBC package -- sqlQuery(channel,.....,nullstring=0)
	stillgives NA's
Message-ID: <s1e502d7.028@ffdata.setur.fo>

There is something strange in R behaviour (perhaps).

I have run the same select in Oracle SQL*Plus (version 10.1.0.2.0) and
the output comes out with NULLs  (which is what it ougth to be).

But in R I still get the same result with NAs (no matter I use
na.strings or nullstring arguments)
An output example follows below:

Using na.string="0"  and nullstring="0" (sorry by the indents):

   Length 2003 2002 2001 2000 1999 1998 1997 1996 1995
1      32   NA    1   NA   NA   NA   NA   NA    2   NA
2      34    3   NA   NA   NA   NA   NA   NA    6   NA
3      35   NA   NA   NA   NA    2   NA   NA   NA   NA
4      36   NA   12   NA   NA   10   NA   NA    1   NA
5      37    3    3   NA   NA    4   NA   NA   31   NA
6      38    2    4    1    1   12    6   NA   11   NA
7      39    4   13    5    5   34    8   NA   58   13

  Length 2003 2002 2001 2000 1999 1998 1997 1996 1995
       32                     1                                        
                        2
       34          3                                                   
                        6
       35                                                      2
       36                    12                               10       
                        1
       37          3          3                                4       
                       31
       38          2          4          1          1         12       
  6                    11
       39          4         13          5          5         34       
  8                    58         13


Best,
Luis


>>> Prof Brian Ripley <ripley at stats.ox.ac.uk> 12/01/2005 09:14:22 >>>
On Tue, 11 Jan 2005, Luis Rideau Cruz wrote:

> R-help,
>
> I'm using the RODBC package to retrieve data froma ODBC database
which
> contain NA's.
>
> By using the argument nullstring = "0"  in sqlQuery() I expect to
> coerce them to numeric but still get NA's in my select.

You need to read the help page (as the posting guide asks): it says

na.strings: character string(s) to be mapped to 'NA' when reading
           character data.

which is the opposite of what you are saying you want to do.

An ODBC database cannot contain NA's.  It may contain NULLs, and it may

contain "NA", so we have no idea what you mean.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk 
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/ 
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Jan 12 12:09:26 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Jan 2005 12:09:26 +0100
Subject: [R] A question about r-help-bounce
In-Reply-To: <OFF350CAE8.BCCA5B0C-ON86256F86.00695D71-86256F86.006AE1D7@mmm.com>
References: <OFF350CAE8.BCCA5B0C-ON86256F86.00695D71-86256F86.006AE1D7@mmm.com>
Message-ID: <x2fz16hpjt.fsf@biostat.ku.dk>

apjaworski at mmm.com writes:

> Could somebody please tell me what does the r-help-bounce address do?
> 
> When I try to respond to an r-help post, my mailer (Lotus Notes) generates
> the r-help bounce return address automatically in addition to the original
> sender address and the r-help address.  I responded to an r-help message
> last night and I never saw my response posted.  Could this be due to the
> r-help-bounce being there?
> 
> Thanks in advance,
> 
> Andy

As far as I know (Martin may know better), the situation is this:
r-help-bounce appears as the Sender: and Errors-to: fields on messages
from r-help. Non-defective software replies to the content of the
From: field, and wide replies should go to the union of the "From:",
"To:", and "Cc:" fields (unless Reply-To: is set, but it generally is
not). If your mailer tries to reply to the Sender: field, it is
defective and/or misconfigured (OK, that's a bit strong. RFC2822 says
that you SHOULD use the From: field for replies and that the Sender:
field is used when, for instance a secretary sends a letter on
someone's behalf.)

The bounce address is used to catch error messages and suchlike. These
are only scanned by the list manager and never sent on to r-help, so as
not to inform everyone on a 2000+ subscriber mailing list that one of
the 2000+ computers involved is malfunctioning. However, if r-help is
among the recipients, things should get through (unless caught by the
spam filter).

By the way, your mailer (or yourself) also attached your mail to a
completely unrelated thread. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Wed Jan 12 12:21:33 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 12 Jan 2005 11:21:33 +0000 (GMT)
Subject: [R] RODBC package -- sqlQuery(channel,.....,nullstring=0)
	stillgives NA's
In-Reply-To: <s1e502d7.029@ffdata.setur.fo>
References: <s1e502d7.029@ffdata.setur.fo>
Message-ID: <Pine.LNX.4.61.0501121114380.20297@gannet.stats>

PLEASE do read the help page, which says

nullstring: character string to be used when reading 'SQL_NULL_DATA'
           character items from the database.
           ^^^^^^^^^^^^^^^
so this does not apply to numeric items.

You can of course easily change numeric NAs to 0s, if you want to.

On Wed, 12 Jan 2005, Luis Rideau Cruz wrote:

> There is something strange in R behaviour (perhaps).

Your negative remarks are not appreciated.

> I have run the same select in Oracle SQL*Plus (version 10.1.0.2.0) and
> the output comes out with NULLs  (which is what it ougth to be).
>
> But in R I still get the same result with NAs (no matter I use
> na.strings or nullstring arguments)
> An output example follows below:
>
> Using na.string="0"  and nullstring="0" (sorry by the indents):
>
>   Length 2003 2002 2001 2000 1999 1998 1997 1996 1995
> 1      32   NA    1   NA   NA   NA   NA   NA    2   NA
> 2      34    3   NA   NA   NA   NA   NA   NA    6   NA
> 3      35   NA   NA   NA   NA    2   NA   NA   NA   NA
> 4      36   NA   12   NA   NA   10   NA   NA    1   NA
> 5      37    3    3   NA   NA    4   NA   NA   31   NA
> 6      38    2    4    1    1   12    6   NA   11   NA
> 7      39    4   13    5    5   34    8   NA   58   13
>
>  Length 2003 2002 2001 2000 1999 1998 1997 1996 1995
>       32                     1
>                        2
>       34          3
>                        6
>       35                                                      2
>       36                    12                               10
>                        1
>       37          3          3                                4
>                       31
>       38          2          4          1          1         12
>  6                    11
>       39          4         13          5          5         34
>  8                    58         13
>
>
> Best,
> Luis
>
>
>>>> Prof Brian Ripley <ripley at stats.ox.ac.uk> 12/01/2005 09:14:22 >>>
> On Tue, 11 Jan 2005, Luis Rideau Cruz wrote:
>
>> R-help,
>>
>> I'm using the RODBC package to retrieve data froma ODBC database
> which
>> contain NA's.
>>
>> By using the argument nullstring = "0"  in sqlQuery() I expect to
>> coerce them to numeric but still get NA's in my select.
>
> You need to read the help page (as the posting guide asks): it says
>
> na.strings: character string(s) to be mapped to 'NA' when reading
>           character data.
>
> which is the opposite of what you are saying you want to do.
>
> An ODBC database cannot contain NA's.  It may contain NULLs, and it may
>
> contain "NA", so we have no idea what you mean.
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Luisr at frs.fo  Wed Jan 12 12:50:33 2005
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Wed, 12 Jan 2005 11:50:33 +0000
Subject: [R] RODBC package --
	sqlQuery(channel,.....,nullstring=0)stillgives NA's
Message-ID: <s1e50f12.066@ffdata.setur.fo>


(1) I do read the posting guide (the fact that I missread o
missunderstood something does not imply not reading)

(2) I could change NAs to 0 (I know) but I have previously (older
versions of R and SQL*Plus) used the same select with the "right" output
(namely with 0s).

(3) AFAIK "strange" is not a negative remark and does not seem to me at
the very least but that is always a matter of taste.

(4) Thank you for your replies but the door is still open so as to know
a solution to the select without coercing NAs to 0s after retrieving the
data

Best,
Luis 

>>> Prof Brian Ripley <ripley at stats.ox.ac.uk> 12/01/2005 11:21:33 >>>
PLEASE do read the help page, which says

nullstring: character string to be used when reading 'SQL_NULL_DATA'
           character items from the database.
           ^^^^^^^^^^^^^^^
so this does not apply to numeric items.

You can of course easily change numeric NAs to 0s, if you want to.

On Wed, 12 Jan 2005, Luis Rideau Cruz wrote:

> There is something strange in R behaviour (perhaps).

Your negative remarks are not appreciated.

> I have run the same select in Oracle SQL*Plus (version 10.1.0.2.0)
and
> the output comes out with NULLs  (which is what it ougth to be).
>
> But in R I still get the same result with NAs (no matter I use
> na.strings or nullstring arguments)
> An output example follows below:
>
> Using na.string="0"  and nullstring="0" (sorry by the indents):
>
>   Length 2003 2002 2001 2000 1999 1998 1997 1996 1995
> 1      32   NA    1   NA   NA   NA   NA   NA    2   NA
> 2      34    3   NA   NA   NA   NA   NA   NA    6   NA
> 3      35   NA   NA   NA   NA    2   NA   NA   NA   NA
> 4      36   NA   12   NA   NA   10   NA   NA    1   NA
> 5      37    3    3   NA   NA    4   NA   NA   31   NA
> 6      38    2    4    1    1   12    6   NA   11   NA
> 7      39    4   13    5    5   34    8   NA   58   13
>
>  Length 2003 2002 2001 2000 1999 1998 1997 1996 1995
>       32                     1
>                        2
>       34          3
>                        6
>       35                                                      2
>       36                    12                               10
>                        1
>       37          3          3                                4
>                       31
>       38          2          4          1          1         12
>  6                    11
>       39          4         13          5          5         34
>  8                    58         13
>
>
> Best,
> Luis
>
>
>>>> Prof Brian Ripley <ripley at stats.ox.ac.uk> 12/01/2005 09:14:22 >>>
> On Tue, 11 Jan 2005, Luis Rideau Cruz wrote:
>
>> R-help,
>>
>> I'm using the RODBC package to retrieve data froma ODBC database
> which
>> contain NA's.
>>
>> By using the argument nullstring = "0"  in sqlQuery() I expect to
>> coerce them to numeric but still get NA's in my select.
>
> You need to read the help page (as the posting guide asks): it says
>
> na.strings: character string(s) to be mapped to 'NA' when reading
>           character data.
>
> which is the opposite of what you are saying you want to do.
>
> An ODBC database cannot contain NA's.  It may contain NULLs, and it
may
>
> contain "NA", so we have no idea what you mean.
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk 
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/

> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk 
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/ 
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tobias.verbeke at telenet.be  Wed Jan 12 14:12:01 2005
From: tobias.verbeke at telenet.be (Tobias Verbeke)
Date: Wed, 12 Jan 2005 13:12:01 +0000
Subject: [R] Breslow Day Test
In-Reply-To: <9166919A78EAD0119E3800805FA6D749031E53CB@ma2pr1.bsc2exc1.bsci.com>
References: <9166919A78EAD0119E3800805FA6D749031E53CB@ma2pr1.bsc2exc1.bsci.com>
Message-ID: <20050112131201.273cab82.tobias.verbeke@telenet.be>

On Tue, 11 Jan 2005 10:45:48 -0500
"Palos, Judit" <PalosJ at bsci.com> wrote:

> Breslow-Day test 
> A statistical test for the homogeneity of odds ratios.
>  
[..some definitions..]

Your message was not particularly clear, but if
you were looking for R code to do a Breslow-Day test,
Google found this for you:

http://www.math.montana.edu/~jimrc/classes/stat524/Rcode/breslowday.test.r

HTH,
Tobias

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From f.harrell at vanderbilt.edu  Wed Jan 12 14:18:39 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 12 Jan 2005 07:18:39 -0600
Subject: [R] Standard error for the area under a smoothed ROC curve?
In-Reply-To: <Pine.LNX.4.21.0501111628290.25669-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0501111628290.25669-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <41E523AF.5030303@vanderbilt.edu>

Dan Bolser wrote:
> Hello, 
> 
> I am making some use of ROC curve analysis. 
> 
> I find much help on the mailing list, and I have used the Area Under the
> Curve (AUC) functions from the ROC function in the bioconductor project...
> 
> http://www.bioconductor.org/repository/release1.5/package/Source/
> ROC_1.0.13.tar.gz 
> 
> However, I read here...
> 
> http://www.medcalc.be/manual/mpage06-13b.php
> 
> "The 95% confidence interval for the area can be used to test the
> hypothesis that the theoretical area is 0.5. If the confidence interval
> does not include the 0.5 value, then there is evidence that the laboratory
> test does have an ability to distinguish between the two groups (Hanley &
> McNeil, 1982; Zweig & Campbell, 1993)."
> 
> But aside from early on the above article is short on details. Can anyone
> tell me how to calculate the CI of the AUC calculation?
> 
> 
> I read this...
> 
> http://www.bioconductor.org/repository/devel/vignette/ROCnotes.pdf
> 
> Which talks about resampling (by showing R code), but I can't understand
> what is going on, or what is calculated (the example given is specific to
> microarray analysis I think).
> 
> I think a general AUC CI function would be a good addition to the ROC
> package.
> 
> 
> 
> 
> One more thing, in calculating the AUC I see the splines function is
> recomended over the approx function. Here...
> 
> http://tolstoy.newcastle.edu.au/R/help/04/10/6138.html
> 
> How would I rewrite the following AUC functions (adapted from bioconductor
> source) to use splines (or approxfun or splinefun) ...
> 
> 
>>spe # Specificity
> 
>  [1] 0.02173913 0.13043478 0.21739130 0.32608696 0.43478261 0.54347826
>  [7] 0.65217391 0.76086957 0.89130435 1.00000000 1.00000000 1.00000000
> [13] 1.00000000
> 
> 
>>sen # Sensitivity
> 
>  [1] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.9302326 0.8139535
>  [8] 0.6976744 0.5581395 0.4418605 0.3488372 0.2325581 0.1162791
> 
> trapezint(1-spe,sen)
> my.integrate(1-spe,sen)
> 
> ## Functions
> ## Nicked (and modified) from the ROC function in bioconductor.
> "trapezint" <-
> function (x, y, a = 0, b = 1)
> {
>     if (x[1] > x[length(x)]) {
>       x <- rev(x)
>       y <- rev(y)
>     }
>     y <- y[x >= a & x <= b]
>     x <- x[x >= a & x <= b]
>     if (length(unique(x)) < 2)
>         return(NA)
>     ya <- approx(x, y, a, ties = max, rule = 2)$y
>     yb <- approx(x, y, b, ties = max, rule = 2)$y
>     x <- c(a, x, b)
>     y <- c(ya, y, yb)
>     h <- diff(x)
>     lx <- length(x)
>     0.5 * sum(h * (y[-1] + y[-lx]))
> }
> 
> "my.integrate" <-
> function (x, y, t0 = 1)
> {
>     f <- function(j) approx(x,y,j,rule=2,ties=max)$y
>     integrate(f, 0, t0)$value
> }
> 
> 
> 
> 
> 
> Thanks for any pointers,
> Dan.

I don't see why the above formulas are being used.  The 
Bamber-Hanley-McNeil-Wilcoxon-Mann-Whitney nonparametric method works 
great.  Just get the U statistic (concordance probability) used in 
Wilcoxon.  As Somers' Dxy rank correlation coefficient is 2*(1-C) where 
C is the concordance or ROC area, the Hmisc package function rcorr.cens 
uses U statistic methods to get the standard error of Dxy.  You can 
easily translate this to a standard error of C.

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From f.harrell at vanderbilt.edu  Wed Jan 12 14:20:51 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 12 Jan 2005 07:20:51 -0600
Subject: [R] transcan() from Hmisc package for imputing data
In-Reply-To: <20050111202252.11509.qmail@web14928.mail.yahoo.com>
References: <20050111202252.11509.qmail@web14928.mail.yahoo.com>
Message-ID: <41E52433.6070400@vanderbilt.edu>

avneet singh wrote:
> Hello:
> 
> I have been trying to impute missing values of a data
> frame which has both numerical and categorical values
> using the function transcan() with little luck.
> 
> Would you be able to give me a simple example where a
> data frame is fed to transcan and it spits out a new
> data frame with the NA values filled up?
> 
> Or is there any other function that i could use?
> 
> Thank you
> avneet

It's in the help file for transcan.  But multiple imputation is much 
better, and transcan does not do multiple imputation as well as the 
newer Hmisc function aregImpute.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From lecoutre at stat.ucl.ac.be  Wed Jan 12 14:16:57 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Wed, 12 Jan 2005 14:16:57 +0100
Subject: [R] [survey] R for Reporting - the R Output MAnager (ROMA) project
Message-ID: <6.0.1.1.2.20050112141558.0312eee8@stat4ux.stat.ucl.ac.be>


Hi R UseRs,

I am interested in providing Reporting abilities to R and have initiated a 
project called R Output MAnager (ROMA).
My starting point was my R2HTML package which provides (rough) HTML 
exportations. I began with trying to mimic it for LaTeX but fastly did 
realize that it was a bad idea.
Thus, I started again from scratch a new package and did spend a lot of 
time reading about this topic, looking at what other software do (SAS ODS, 
SPlus SPXML,...) and studying technologies and formats: XML+XLST, LyX, 
DocBook, RTF,...

What follows is a description of my plans. This email is targetted to 
interested useRs, in order to have a return on that.
It comes with a little survey at the end that will be useful to me to 
target my package.
If you are also interested in Reporting (Output, Formats, Exchange,...), 
please read the following and answer the survey.
If not, you can skip this message - apologies for sending it to R-help, I 
hope you don't mind.


---



As a matter of fact, I have implemented something that shows promises 
(according to me). Currently, from the following output description:


***
  data(iris)
  mm=as.matrix(iris[1:5,1:4])

  out = emptyContent()
  out = out + Section("A title here")
  out = out + diag(2)
  out = out + Comment("comment: yes!")
  out = out + list(un=1,pi)
  out = out + "Then a boolean:" + TRUE
  out = out + Section("Default matrix",level=2)
  out = out + mm
  out = out + Section("Custom matrix" + Footnote("It works!"),level=2)
  out = out + 
ROMA(mm,style="custommatrix",rowstyle=paste("color",row(mm)[,1]%%2,sep=""),align="left")
  out = out + Section("An other title")
  out = out + ROMAgenerated()    # ROMAgenerated is a predefined function
***

You can generate a proper HTML file by the following command:

 > Export(out)

(see result: http://www.stat.ucl.ac.be/ROMA/sample.htm)

The same "output object" could be exported to (tex+dvi+ps+pdf) with:

 >  Export(out,driver="latex")

(see result: http://www.stat.ucl.ac.be/ROMA/sample.pdf / Change extension 
for other formats: tex and ps)


--- Survey ---

IMPORTANT: ONLY DO REPLY TO ME, NOT TO R-HELP MAILING LIST

Simply fill in the questions you want to asnwer to:

1. I am interesting in Reporting abilities for R
     [ ] Definitively
     [ ] Rather Yes
     [ ] Rather No
     [ ] Not at all
	
2. I have some knowledge about those different formats / specifications
     [ ] rtf      [ ] LaTeX  [ ] LyX
     [ ] html     [ ] css    [ ] xHTML
     [ ] XML      [ ] XLST   [ ] DocBook
	
3. I have some knowledge about those tools
     [ ] SAS ODS
     [ ] SPlus SPXML library
     [ ] XLST + XLST-FO chain

4. I would be specially interested in the following formats (multiple 
choices possible)
     [ ] rtf
     [ ] tex
     [ ] lyx
     [ ] XML, with a DTD specific to R
     [ ] XML, with the DTD from SPlus (compatible with SPXML library)
     [ ] XML, DocBook flavor
     [ ] HTML + css (good xHTML)
     [ ] Word (doc)
     [ ] OpenOffice (oo)
     [ ] Plain text
     [ ] Other:

4bis: If several formats, the best (according to me and my needs) one would 
be: ____

5. The approach is to fully separate content from formating. So, XML would 
be an ideal output format. Nevertheless, few people who use R may also 
mater XLST to produce nice formatted output. Thus, a way to handle styles 
(bold, colors, fonts, etc.) from R would also be great. It may not be a 
priority. Statistical output do have some specific issues: mathematics, 
complicated tables, graphs, and so on. For each of the following items, 
please tell me how important the issue is for you:

	0: I don't need that (and think I will never need it)
	1: Not really important
	...
	5: Crucial - I can't leave without that point anymore
	
  5.1 - Beeing able to read the document in any OS:
  Importance: __

  5.2 - Having an object that describes the output within R (as in the 
example), so that I could add/remove things, reexport it
  Importance: __	

  5.3 - Beeing able to define basics formatting also within R (bold, 
colors, fonts, and so on)
  Importance: __	

  5.4 - Beeing able to include mathematics, as (La)TeX codes or MathML
  Importance: __	

  5.5 - Beeing able to build complicated tables, with merged cells, 
embedding lists, eventually sub-tables
  Importance: __	


6. Here are some conceptual objects that a report may contain. Are there 
any more you can think to which may be important?
	Tables (containing Rows and Cells), Lists, Titles, Footnotes, Comment, 
Abbreviations / Acronyms, Code, Links, Graphs, Layout  (to have 2 or 3 
columns), Mathematics (equations), Table of Contents, Index
	
	Other that could be added:
	

7. Two different tools allow to create "dynamic" or alike documents: Sweave 
(for LaTeX and HTML) and Rpad (HTML, with a server). I would be interested 
in beeing able to describe the structure of a document that would be 
exportable to:

  7.1 -  Sweave   [ ] Yes    [ ] No
  7.2 -  Rpad     [ ] Yes    [ ] No


If you are interested in contributing to the project, please let me know 
also. If many people do need XML representations, I think it would be great 
to have a guru in XLST abilities in the development team.

Thanks for attention,

Happy R,

Eric
	

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From jtk at cmp.uea.ac.uk  Wed Jan 12 15:39:44 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Wed, 12 Jan 2005 14:39:44 +0000
Subject: [R] thanks
In-Reply-To: <200501111624.12101.lefebure@univ-lyon1.fr>
References: <freemail.20050011155958.44093@fm13.freemail.hu>
	<200501111624.12101.lefebure@univ-lyon1.fr>
Message-ID: <20050112143944.GG11799@jtkpc.cmp.uea.ac.uk>

On Tue, Jan 11, 2005 at 04:24:11PM +0100, Lefebure Tristan wrote:

> example from a shell:
> 
> echo -e "pdf(file=\"test.pdf\")\nplot(1:10,11:20)\ndev.off(dev.cur())\n">cmd.R
> R -s <cmd.R
> 
> (write a file of command for R, and than feed R with it)

This may be on the verge of becoming offtopic, but let me remark
that the technique proposed here should be used for illustrative
purposes only. For real life, use pipes:

    echo 'print(mean(rnorm(10)));' | R --vanilla

This is equivalent to

    echo ''print(mean(rnorm(10)));' > cmd.R
    R --vanilla < cmd.R

*as long as only one shell is executing this sequence at any given time*.

The reason I mention this here is that I've seen it happen a few times
that this "temporary command file" approach has made it from examples
into shell scripts of which then, later on, multiple instances were
run at a time, resulting in very rare, very irreproducible, and most
inexplicable erroneous results.

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From Achim.Zeileis at wu-wien.ac.at  Wed Jan 12 14:45:53 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 12 Jan 2005 14:45:53 +0100
Subject: [R] CUSUM SQUARED structural breaks approach?
In-Reply-To: <5f4f82790501111133193a21fb@mail.gmail.com>
References: <5f4f8279050111051031974bc4@mail.gmail.com>
	<20050111151032.07d05ab4.Achim.Zeileis@wu-wien.ac.at>
	<5f4f82790501111133193a21fb@mail.gmail.com>
Message-ID: <20050112144553.4faf3cc8.Achim.Zeileis@wu-wien.ac.at>

On Tue, 11 Jan 2005 19:33:41 +0000 Rick Ram wrote:

> Groundwork for the choice of break method in my specific application
> has already been done - otherwise I would need to rework the wheel
> (make a horribly detailed comparison of performance of break
> approaches in context of modelling post break)
> 
> If it interests you, Pesaran & Timmerman 2002 compared CUSUM Squared,
> BaiPerron and a time varying approach to detect singular previous
> breaks in reverse ordered financial time series so as to update a
> forecasting model. 

Yes, I know that paper. And if I recall correctly they are mainly
interested in modelling the time period after the last break. For this,
the reverse ordered recursive CUSUM approach works because they
essentially look back in time to see when their predictions break down.
And for their application looking for variance changes also makes sense.
The approach is surely valid and sound in this context...but it might be
possible to do something better (but I would have to look much closer at
the particular application to have an idea what might be a way to go).

> This works "fine" i.e. the plot looks correct.  The problem is how to
> appropriately normalise these to rescale them to what the CUSUM
> squared procedure expects (this looks to be a different and more
> complicated procedure than the normalisation used for the basic
> CUSUM).  I am from an IT background and am slightly illiterate in
> terms of math notation... guidance from anyone would be appreciated

I just had a brief glance at BDE75, page 154, Section 2.4. If I
haven't missed anything important on reading it very quickly, you just
need to do something like the following (a reproducible example, based
on data from strucchange, using a notation similar to BDE's):

## load GermanM1 data and model
library(strucchange)
data(GermanM1)
M1.model <- dm ~ dy2 + dR + dR1 + dp + ecm.res + season

## compute squared recursive residuals
w2 <- recresid(M1.model, data = GermanM1)^2
## compute CUSUM of squares process
sr <- ts(cumsum(c(0, w2))/sum(w2), end = end(GermanM1$dm), freq = 12)
## the border (r-k)/(T-k)
border <- ts(seq(0, 1, length = length(sr)),
             start = start(sr), freq = 12)

## nice plot
plot(sr, xaxs = "i", yaxs = "i", main = "CUSUM of Squares")
lines(border, col = grey(0.5))
lines(0.4 + border, col = grey(0.5))
lines(- 0.4 + border, col = grey(0.5))

Instead of 0.4 you would have to use the appropriate critical values
from Durbin (1969) if my reading of the paper is correct.
 
hth,
Z

> Does anyone know if this represents some commonly performed type of
> normalisation than exists in another function??
> 
> I will hunt out the 1969 paper for the critical values but prior to
> doing this I am a bit confused as to how they will
> implemented/interpreted... the CUSUM squared plot does/should run
> diagonally up from left to right and there are two straight lines that
> one would put around this from the critical values.  Hence, a
> different interpretation/implementation of confidence levels than in
> other contexts.  I realise this is not just a R thing but a problem
> with my theoretical background.
> 
> 
> Thanks for detailed reply!
> 
> Rick.
> 
> 
> > 
> > But depending on the model and hypothesis you want to test, another
> > technique than CUSUM of squares might be more appropriate and also
> > available in strucchange.
> 
> > 
> > hth,
> > Z
> > 
> > > Any help or pointers about where to look would be more than
> > > appreciated!  Hopefully I have just missed obvious something in
> > > the package...
> > >
> > > Many thanks,
> > >
> > > Rick R.
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> >
>



From blindglobe at gmail.com  Wed Jan 12 14:46:41 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Wed, 12 Jan 2005 14:46:41 +0100
Subject: [R] [survey] R for Reporting - the R Output MAnager (ROMA) project
In-Reply-To: <6.0.1.1.2.20050112141558.0312eee8@stat4ux.stat.ucl.ac.be>
References: <6.0.1.1.2.20050112141558.0312eee8@stat4ux.stat.ucl.ac.be>
Message-ID: <1abe3fa9050112054634d5f01@mail.gmail.com>

Your example is sequential, ignoring the tree-like structure of most
documents. Why not via a DOM or similar "XML-ish" structure?

While I'd never advocate general purpose XML as a user format, as you
note, that is what XSLT is for, and using XML as an electronic
internal document representation would provide a potentially more
scalable system.

(i.e. use XML and the DOM internally, but provide a simple API to it).

The other advantage would be that you could stick a dependency DAG
(ADG) via a second set of marked edges of the document "graph/tree" to
allow for selective regeneration of results.

But then, this project isn't on my to-do list this year :-).

best,
-tony



On Wed, 12 Jan 2005 14:16:57 +0100, Eric Lecoutre
<lecoutre at stat.ucl.ac.be> wrote:
> 
> Hi R UseRs,
> 
> I am interested in providing Reporting abilities to R and have initiated a
> project called R Output MAnager (ROMA).
> My starting point was my R2HTML package which provides (rough) HTML
> exportations. I began with trying to mimic it for LaTeX but fastly did
> realize that it was a bad idea.
> Thus, I started again from scratch a new package and did spend a lot of
> time reading about this topic, looking at what other software do (SAS ODS,
> SPlus SPXML,...) and studying technologies and formats: XML+XLST, LyX,
> DocBook, RTF,...
> 
> What follows is a description of my plans. This email is targetted to
> interested useRs, in order to have a return on that.
> It comes with a little survey at the end that will be useful to me to
> target my package.
> If you are also interested in Reporting (Output, Formats, Exchange,...),
> please read the following and answer the survey.
> If not, you can skip this message - apologies for sending it to R-help, I
> hope you don't mind.
> 
> ---
> 
> As a matter of fact, I have implemented something that shows promises
> (according to me). Currently, from the following output description:
> 
> ***
>  data(iris)
>  mm=as.matrix(iris[1:5,1:4])
> 
>  out = emptyContent()
>  out = out + Section("A title here")
>  out = out + diag(2)
>  out = out + Comment("comment: yes!")
>  out = out + list(un=1,pi)
>  out = out + "Then a boolean:" + TRUE
>  out = out + Section("Default matrix",level=2)
>  out = out + mm
>  out = out + Section("Custom matrix" + Footnote("It works!"),level=2)
>  out = out +
> ROMA(mm,style="custommatrix",rowstyle=paste("color",row(mm)[,1]%%2,sep=""),align="left")
>  out = out + Section("An other title")
>  out = out + ROMAgenerated()    # ROMAgenerated is a predefined function
> ***
> 
> You can generate a proper HTML file by the following command:
> 
> > Export(out)
> 
> (see result: http://www.stat.ucl.ac.be/ROMA/sample.htm)
> 
> The same "output object" could be exported to (tex+dvi+ps+pdf) with:
> 
> >  Export(out,driver="latex")
> 
> (see result: http://www.stat.ucl.ac.be/ROMA/sample.pdf / Change extension
> for other formats: tex and ps)
> 
> --- Survey ---
> 
> IMPORTANT: ONLY DO REPLY TO ME, NOT TO R-HELP MAILING LIST
> 
> Simply fill in the questions you want to asnwer to:
> 
> 1. I am interesting in Reporting abilities for R
>     [ ] Definitively
>     [ ] Rather Yes
>     [ ] Rather No
>     [ ] Not at all
> 
> 2. I have some knowledge about those different formats / specifications
>     [ ] rtf      [ ] LaTeX  [ ] LyX
>     [ ] html     [ ] css    [ ] xHTML
>     [ ] XML      [ ] XLST   [ ] DocBook
> 
> 3. I have some knowledge about those tools
>     [ ] SAS ODS
>     [ ] SPlus SPXML library
>     [ ] XLST + XLST-FO chain
> 
> 4. I would be specially interested in the following formats (multiple
> choices possible)
>     [ ] rtf
>     [ ] tex
>     [ ] lyx
>     [ ] XML, with a DTD specific to R
>     [ ] XML, with the DTD from SPlus (compatible with SPXML library)
>     [ ] XML, DocBook flavor
>     [ ] HTML + css (good xHTML)
>     [ ] Word (doc)
>     [ ] OpenOffice (oo)
>     [ ] Plain text
>     [ ] Other:
> 
> 4bis: If several formats, the best (according to me and my needs) one would
> be: ____
> 
> 5. The approach is to fully separate content from formating. So, XML would
> be an ideal output format. Nevertheless, few people who use R may also
> mater XLST to produce nice formatted output. Thus, a way to handle styles
> (bold, colors, fonts, etc.) from R would also be great. It may not be a
> priority. Statistical output do have some specific issues: mathematics,
> complicated tables, graphs, and so on. For each of the following items,
> please tell me how important the issue is for you:
> 
>        0: I don't need that (and think I will never need it)
>        1: Not really important
>        ...
>        5: Crucial - I can't leave without that point anymore
> 
>  5.1 - Beeing able to read the document in any OS:
>  Importance: __
> 
>  5.2 - Having an object that describes the output within R (as in the
> example), so that I could add/remove things, reexport it
>  Importance: __
> 
>  5.3 - Beeing able to define basics formatting also within R (bold,
> colors, fonts, and so on)
>  Importance: __
> 
>  5.4 - Beeing able to include mathematics, as (La)TeX codes or MathML
>  Importance: __
> 
>  5.5 - Beeing able to build complicated tables, with merged cells,
> embedding lists, eventually sub-tables
>  Importance: __
> 
> 6. Here are some conceptual objects that a report may contain. Are there
> any more you can think to which may be important?
>        Tables (containing Rows and Cells), Lists, Titles, Footnotes, Comment,
> Abbreviations / Acronyms, Code, Links, Graphs, Layout  (to have 2 or 3
> columns), Mathematics (equations), Table of Contents, Index
> 
>        Other that could be added:
> 
> 7. Two different tools allow to create "dynamic" or alike documents: Sweave
> (for LaTeX and HTML) and Rpad (HTML, with a server). I would be interested
> in beeing able to describe the structure of a document that would be
> exportable to:
> 
>  7.1 -  Sweave   [ ] Yes    [ ] No
>  7.2 -  Rpad     [ ] Yes    [ ] No
> 
> If you are interested in contributing to the project, please let me know
> also. If many people do need XML representations, I think it would be great
> to have a guru in XLST abilities in the development team.
> 
> Thanks for attention,
> 
> Happy R,
> 
> Eric
> 
> Eric Lecoutre
> UCL /  Institut de Statistique
> Voie du Roman Pays, 20
> 1348 Louvain-la-Neuve
> Belgium
> 
> tel: (+32)(0)10473050
> lecoutre at stat.ucl.ac.be
> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre
> 
> If the statistics are boring, then you've got the wrong numbers. -Edward
> Tufte
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From MSchwartz at MedAnalytics.com  Wed Jan 12 15:06:20 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 12 Jan 2005 08:06:20 -0600
Subject: [R] useR 2005 ?
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E61505EB@HERMES.demogr.mpg.de>
References: <8B08A3A1EA7AAC41BE24C750338754E61505EB@HERMES.demogr.mpg.de>
Message-ID: <1105538780.14753.8.camel@horizons.localdomain>

On Tue, 2005-01-11 at 17:39 +0100, Rau, Roland wrote:
> Dear R-Help-List,
> 
> are there any plans to organize a "useR" conference in 2005?
> 
> Best,
> Roland


As I understand it, no. The next one will be in 2006, so it will be
every other year, interleaved with the DSC meeting the odd years. 

Information on past DSC meetings is here:

http://www.ci.tuwien.ac.at/Conferences/DSC.html

I have not seen anything posted yet for DSC 2005, unless I missed it
someplace.

HTH,

Marc Schwartz



From nicolas.deig at epfl.ch  Wed Jan 12 15:17:26 2005
From: nicolas.deig at epfl.ch (nicolas.deig@epfl.ch)
Date: Wed, 12 Jan 2005 15:17:26 +0100 (MET)
Subject: [R] (no subject)
Message-ID: <1105539446.41e531763d829@imapwww.epfl.ch>


hi,

I am trying to grow a classification tree on some data, but I have a little
problem. In order to do so I have to use a function like "tree" in R and on the
internet help(tree) I get the following: 

"The left-hand-side (response) should be either a numerical vector when a
regression tree will be fitted or a factor, when a classification tree is produced"

I would like to know what is a "factor" in R, is it numerical value with no
formula or just a word??

Thanks in advance
Nicolas



From petr.pikal at precheza.cz  Wed Jan 12 15:33:17 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 12 Jan 2005 15:33:17 +0100
Subject: [R] (no subject)
In-Reply-To: <1105539446.41e531763d829@imapwww.epfl.ch>
Message-ID: <41E5433D.27531.1FC21B9@localhost>



On 12 Jan 2005 at 15:17, nicolas.deig at epfl.ch wrote:

> 
> hi,
> 
> I am trying to grow a classification tree on some data, but I have a
> little problem. In order to do so I have to use a function like "tree"
> in R and on the internet help(tree) I get the following: 
> 
> "The left-hand-side (response) should be either a numerical vector
> when a regression tree will be fitted or a factor, when a
> classification tree is produced"
> 
> I would like to know what is a "factor" in R, is it numerical value
> with no formula or just a word??

Hi Nicolas

?factor will show you what it is.


Description:

     The function 'factor' is used to encode a vector as a factor (the
     terms 'category' and 'enumerated type' are also used for 
factors).
      If 'ordered' is 'TRUE', the factor levels are assumed to be
     ordered. For compatibility with S there is also a function
     'ordered'.

Cheers
Petr


> 
> Thanks in advance
> Nicolas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From MSchwartz at MedAnalytics.com  Wed Jan 12 15:35:49 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 12 Jan 2005 08:35:49 -0600
Subject: [R] What is a factor? [was :(no subject)]
In-Reply-To: <1105539446.41e531763d829@imapwww.epfl.ch>
References: <1105539446.41e531763d829@imapwww.epfl.ch>
Message-ID: <1105540549.14753.20.camel@horizons.localdomain>

On Wed, 2005-01-12 at 15:17 +0100, nicolas.deig at epfl.ch wrote:
> hi,
> 
> I am trying to grow a classification tree on some data, but I have a little
> problem. In order to do so I have to use a function like "tree" in R and on the
> internet help(tree) I get the following: 
> 
> "The left-hand-side (response) should be either a numerical vector when a
> regression tree will be fitted or a factor, when a classification tree is produced"
> 
> I would like to know what is a "factor" in R, is it numerical value with no
> formula or just a word??
> 
> Thanks in advance
> Nicolas

See ?factor and/or Chapter 4 "Ordered and Unordered Factors" in "An
Introduction to R".

Also, you might want to look into the 'rpart' package for an alternative
to 'tree'. rpart is included in the base R distribution:

library(rpart)
?rpart

HTH,

Marc Schwartz



From saverio.vicario at yale.edu  Wed Jan 12 16:29:09 2005
From: saverio.vicario at yale.edu (saverio vicario)
Date: Wed, 12 Jan 2005 10:29:09 -0500
Subject: [R] (no subject)
Message-ID: <p06010203be0aeebf0e8f@[130.132.249.15]>

Dear help desk and R community,
I have a problem on how R2.0 handle the RAM, maybe a bug
In fact I used R1.9 since  january 2004 with large data set using a 
macosx G5 with 1G of ram without problem .  Then I passed to 2.0 and 
I found myself short in  ram using virtual memory.  I tried to use 
the program from terminal windows to avoid the GUI but it was the 
same.  The annoying part  is that even if I cancel big object from 
the workspace the RAM consumption do not decrease (looking at the 
percentage of usage in ps or the actual value in "activity monitor"). 
Only after a long time (1/2 hour ) the consumption of RAM decreased 
somewhat. When I use the workspace browser on the GUI and I use 
"refresh" the consumption of RAM fluctuate each time  both decreasing 
and incresing even if the workspace do not change. For example I can 
pass from using 270mb of memory to 430mb (or the contrary) simply 
pushing several time refresh.  The value is stable once I do not push 
refresh anymore
thanks
saverio



From tlumley at u.washington.edu  Wed Jan 12 16:30:16 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 12 Jan 2005 07:30:16 -0800 (PST)
Subject: [R] Breslow Day Test
In-Reply-To: <20050112131201.273cab82.tobias.verbeke@telenet.be>
References: <9166919A78EAD0119E3800805FA6D749031E53CB@ma2pr1.bsc2exc1.bsci.com>
	<20050112131201.273cab82.tobias.verbeke@telenet.be>
Message-ID: <Pine.A41.4.61b.0501120728560.45670@homer05.u.washington.edu>

On Wed, 12 Jan 2005, Tobias Verbeke wrote:

> On Tue, 11 Jan 2005 10:45:48 -0500
> "Palos, Judit" <PalosJ at bsci.com> wrote:
>
>> Breslow-Day test
>> A statistical test for the homogeneity of odds ratios.
>>
> [..some definitions..]
>
> Your message was not particularly clear, but if
> you were looking for R code to do a Breslow-Day test,
> Google found this for you:
>

There is code for meta-analyses, including a test of homogeneity that I 
think is the same as the Breslow-Day one, in the rmeta package. The 
package does forest plots, too.

 	-thomas



From anne.piotet at urbanet.ch  Wed Jan 12 17:16:40 2005
From: anne.piotet at urbanet.ch (Anne)
Date: Wed, 12 Jan 2005 17:16:40 +0100
Subject: [R] (no subject)
References: <1105539446.41e531763d829@imapwww.epfl.ch>
Message-ID: <001e01c4f8c2$1b07dae0$6c00a8c0@mtd4>

I think you will find all the doc in the help files
> ?factor()
  gets

   The function 'factor' is used to encode a vector as a factor (the
     terms 'category' and 'enumerated type' are also used for factors).
      If 'ordered' is 'TRUE', the factor levels are assumed to be
     ordered. For compatibility with S there is also a function
     'ordered'.

     'is.factor', 'is.ordered', 'as.factor' and 'as.ordered' are the
     membership and coercion functions for these classes.

Usage:

     factor(x, levels = sort(unique.default(x), na.last = TRUE),
            labels = levels, exclude = NA, ordered = is.ordered(x))
     ordered(x, ...)
etc...

c'est une  variable de type cat?gorique! whose levels (values) are strings

To get help:
type ?functionname
or if you are under Windows see the menu "Help\Html help" and look under
Packages. What you will want first are the "Base" and "Statistics" packages

Anne



----- Original Message ----- 
From: <nicolas.deig at epfl.ch>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, January 12, 2005 3:17 PM
Subject: [R] (no subject)


>
> hi,
>
> I am trying to grow a classification tree on some data, but I have a
little
> problem. In order to do so I have to use a function like "tree" in R and
on the
> internet help(tree) I get the following:
>
> "The left-hand-side (response) should be either a numerical vector when a
> regression tree will be fitted or a factor, when a classification tree is
produced"
>
> I would like to know what is a "factor" in R, is it numerical value with
no
> formula or just a word??
>
> Thanks in advance
> Nicolas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From kurt.sys at pandora.be  Wed Jan 12 17:38:04 2005
From: kurt.sys at pandora.be (Kurt Sys)
Date: Wed, 12 Jan 2005 17:38:04 +0100
Subject: [R] changing langage 
In-Reply-To: <1105539446.41e531763d829@imapwww.epfl.ch>
References: <1105539446.41e531763d829@imapwww.epfl.ch>
Message-ID: <41E5526C.4060608@pandora.be>

Hi all,

I've got a small, practical question, which untill now I couldn't solve 
(otherwhise I wouldn't mail it, right?) First of all, I'm talking about 
R 2.0.1 on a winxp system (using the default graphical interface being 
'Rgui').
When I make plots, using dates on the x-axis, it puts the labels in 
Dutch, which is nice (since it's my mother tongue) unless I want them in 
English... Is there a way to change this behaviour?  (Can I change the 
labels etc to English?)

tnx,
Kurt Sys



From tring at gvdnet.dk  Wed Jan 12 18:11:19 2005
From: tring at gvdnet.dk (Troels Ring)
Date: Wed, 12 Jan 2005 18:11:19 +0100
Subject: [R] defining lower part of distribution with covariate
Message-ID: <5.2.0.9.0.20050112180549.0378f9b0@home.gvdnet.dk>

I try again - perhaps it is analysis of covariance with treatment 
(thio,ultiva) as two categories and antime as covariate. On the basis of 
such a model, is then the probability of GCS <= 12 larger with thio treatment ?


Dear friends, forgive me a simple question, possibly related to quantreg 
but I failed to get it done and hope for basic instruction.

I have two sets of observed Glasgow coma scores at admission to ICU after 
operation, and accompanying time of anesthesia (in hours).
Thio is cheap and perhaps old fashioned, and ultiva expensive and rapidly 
terminated. The problem is to estimate the probability of GCS 12 or lower 
on the two treatments after taking time of anesthesia into account (antime) 
which is longer for thio. How would I do that in the best way ?

Best wishes
Troels Ring, MD
Aalborg, Denmark


thio
       GCS antime
  [1,]  14    4.5
  [2,]  15    7.5
  [3,]  11    7.5
  [4,]  15    4.5
  [5,]  14    4.5
  [6,]  15    3.5
  [7,]  15    5.5
  [8,]  14    5.5
  [9,]  15    3.5
[10,]  14    8.5
[11,]  13    4.5
[12,]  12    5.5
[13,]  15    3.5
[14,]  13    6.5
[15,]   9    8.5
[16,]  15    6.5
 > ultiva
       GCS antime
  [1,]  15    4.5
  [2,]  15    4.5
  [3,]  15    2.5
  [4,]  15    3.5
  [5,]  15    3.5
  [6,]  12    5.5
  [7,]  15    4.5
  [8,]  15    3.5
  [9,]  15    8.5
[10,]  13    4.5
[11,]  14    3.5
[12,]  14    4.5
[13,]  15    4.5
[14,]  14    2.5
[15,]  15    4.5
[16,]  15    3.5
[17,]  15    3.5
[18,]  14    4.5
[19,]  14    4.5
[20,]  15    4.5

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From buser at stat.math.ethz.ch  Wed Jan 12 18:12:38 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Wed, 12 Jan 2005 18:12:38 +0100
Subject: [R] Kolmogorov-Smirnof test for lognormal distribution with
	estimated parameters 
In-Reply-To: <000001c4f7d5$43051dc0$9bad4c86@wso1809r>
References: <000001c4f7d5$43051dc0$9bad4c86@wso1809r>
Message-ID: <16869.23174.212434.122453@stat.math.ethz.ch>

Hi Kwabena

I did once a simulation, generating normal distributed values
(500 values) and calculating a KS test with estimated
parameters. For 10000 times repeating this test I got about
1 significant tests (on a level alpha=0.05 I'm expecting about 500 
significant tests by chance)
So I think if you estiamte the parameters from the data, you fit
to good and the used distribution of the test statistic is not
adequate as it is indicated in the help page you cited. There
(in the help page) is some literature, but it is no easy stuff
to read.
Furthermore I know no implementation of an KS test which
accounts for this estimation of the parameter.

I recommend a graphical tool instead of a test:

x <- rlnorm(100)
qqnorm(log(x))

See also ?qqnorm and ?qqplot.

If you insist on testing a theoretical distribution be aware
that a non significant test does not mean that your data has the
tested distribution (especially if you have few data, there is
no power in the test to detect deviations from the theoretical
distribution and the conclusion that the data fits well is
trappy)

If there are enough data I'd prefer a chi square test to the KS
test (but even there I use graphical tools instead). 

See ?chisq

For this test you have to specify classes and this is 
subjective (you can't avoid this).

You can reduce the DF of the expected chi square distribution
(under H_0) by the number of estimated parameters from the data
and will get better results. 

DF = number of classes - 1 - estimated parameters

I think this test is more powerful than the KS test,
particularly if you must estimate the parameters from data.

Regards,

Christoph

-- 
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C11
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-1-632-5414		fax: 632-1228
http://stat.ethz.ch/~buser/



Kwabena Adusei-Poku writes:
 > Hello all,
 > 
 > Would somebody be kind enough to show me how to do a KS test in R for a
 > lognormal distribution with ESTIMATED parameters. The R function
 > ks.test()says "the parameters specified must be prespecified and not
 > estimated from the data" Is there a way to correct this when one uses
 > estimated data?
 > 
 > Regards,
 > 
 > Kwabena.
 > 
 > --------------------------------------------
 > Kwabena Adusei-Poku
 > University of Goettingen
 > Institute of Statistics and Econometrics
 > Platz der Goettingen Sieben 5
 > 37073 Goettingen
 > Germany
 > Tel: +49-(0)551-394794
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jbang at uiuc.edu  Wed Jan 12 18:13:44 2005
From: jbang at uiuc.edu (Bang)
Date: Wed, 12 Jan 2005 11:13:44 -0600
Subject: [R] "model.response" error
Message-ID: <200501121713.j0CHDjD5023414@expredir2.cites.uiuc.edu>

When I installed R 2.0.1 (replacing 1.9.0) for Windows, a code using
"model.response" began acting up.  Here are the first several lines of a
code I had been tweaking for a spatial model (the code is mostly that of
Roger Bivand--I am adapting it to a slightly different data structure and
the problem I'm sure is with my changes, not his code).

<command name> <- function (formula, data = list(), weights, na.action =
na.fail, type = "lag", quiet = TRUE, zero.policy = FALSE, tol.solve = 1e-07,
tol.opt = .Machine$double.eps^0.5, sparsedebug = FALSE)
{
    mt <- terms(formula, data = data)
    mf <- lm(formula, data, na.action = na.action, method = "model.frame")
    na.act <- attr(mf, "na.action")
    if (!is.matrix.csr(weights))
        cat("\nWarning: weights matrix not in sparse form\n")
    switch(type, lag = if (!quiet)
        cat("\nSpatial lag model\n"), mixed = if (!quiet)
        cat("\nSpatial mixed autoregressive model\n"), stop("\nUnknown model
type\n"))
    if (!quiet)
        cat("Jacobian calculated using weights matrix eigenvalues\n")
    y <- model.response(mf, "numeric")
    if (any(is.na(y))) stop("NAs in dependent variable")
    x <- model.matrix(mt, mf)
    if (any(is.na(x))) stop("NAs in independent variable")
    if (nrow(x) != nrow(weights))
        stop("Input data and weights have different dimensions")
    n <- nrow(x)
    m <- ncol(x)

When it reads the "Y" variable in the command:

"y <- model.response(mf, "numeric")"

The error it gives is:

"Error in model.response(mf, "numeric") : No direct or inherited method
for function "model.response" for this call"

The problem is puzzling me because it is not something I encountered when I
was running the same code in 1.9.0, but is causing problems in 2.0.1

Thanks, and any comments on debugging the error are welcome. 

Jim

Well I AM missing the back of my head.you COULD cut me a little slack!
-Homer Simpson



From f.harrell at vanderbilt.edu  Wed Jan 12 18:19:17 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 12 Jan 2005 11:19:17 -0600
Subject: [R] Kolmogorov-Smirnof test for lognormal distribution
	with	estimated parameters
In-Reply-To: <16869.23174.212434.122453@stat.math.ethz.ch>
References: <000001c4f7d5$43051dc0$9bad4c86@wso1809r>
	<16869.23174.212434.122453@stat.math.ethz.ch>
Message-ID: <41E55C15.4060203@vanderbilt.edu>

Christoph Buser wrote:
> Hi Kwabena
> 
> I did once a simulation, generating normal distributed values
> (500 values) and calculating a KS test with estimated
> parameters. For 10000 times repeating this test I got about
> 1 significant tests (on a level alpha=0.05 I'm expecting about 500 
> significant tests by chance)
> So I think if you estiamte the parameters from the data, you fit
> to good and the used distribution of the test statistic is not
> adequate as it is indicated in the help page you cited. There
> (in the help page) is some literature, but it is no easy stuff
> to read.
> Furthermore I know no implementation of an KS test which
> accounts for this estimation of the parameter.
> 
> I recommend a graphical tool instead of a test:
> 
> x <- rlnorm(100)
> qqnorm(log(x))
> 
> See also ?qqnorm and ?qqplot.
> 
> If you insist on testing a theoretical distribution be aware
> that a non significant test does not mean that your data has the
> tested distribution (especially if you have few data, there is
> no power in the test to detect deviations from the theoretical
> distribution and the conclusion that the data fits well is
> trappy)
> 
> If there are enough data I'd prefer a chi square test to the KS
> test (but even there I use graphical tools instead). 
> 
> See ?chisq
> 
> For this test you have to specify classes and this is 
> subjective (you can't avoid this).
> 
> You can reduce the DF of the expected chi square distribution
> (under H_0) by the number of estimated parameters from the data
> and will get better results. 
> 
> DF = number of classes - 1 - estimated parameters
> 
> I think this test is more powerful than the KS test,
> particularly if you must estimate the parameters from data.
> 
> Regards,
> 
> Christoph
> 

It is also a good idea to ask why one compares against a known 
distribution form.  If you use the empirical CDF to select a parametric 
distribution, the final estimate of the distribution will inherit the 
variance of the ECDF.  The main reason statisticians think that 
parametric curve fits are far more efficient than nonparametric ones is 
that they don't account for model uncertainty in their final confidence 
intervals.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From p.dalgaard at biostat.ku.dk  Wed Jan 12 18:16:56 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Jan 2005 18:16:56 +0100
Subject: [R] changing langage
In-Reply-To: <41E5526C.4060608@pandora.be>
References: <1105539446.41e531763d829@imapwww.epfl.ch>
	<41E5526C.4060608@pandora.be>
Message-ID: <x2llayftyv.fsf@biostat.ku.dk>

Kurt Sys <kurt.sys at pandora.be> writes:

> Hi all,
> 
> I've got a small, practical question, which untill now I couldn't
> solve (otherwhise I wouldn't mail it, right?) First of all, I'm
> talking about R 2.0.1 on a winxp system (using the default graphical
> interface being 'Rgui').
> When I make plots, using dates on the x-axis, it puts the labels in
> Dutch, which is nice (since it's my mother tongue) unless I want them
> in English... Is there a way to change this behaviour?  (Can I change
> the labels etc to English?)

This type of stuff works on Linux at least:

Sys.setlocale("LC_ALL","da_DK") # or en_GB, or....
plot(date,....)


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From fm3a004 at math.uni-hamburg.de  Wed Jan 12 18:25:02 2005
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Wed, 12 Jan 2005 18:25:02 +0100 (MET)
Subject: [R] Kolmogorov-Smirnof test for lognormal distribution with
	estimated parameters 
In-Reply-To: <16869.23174.212434.122453@stat.math.ethz.ch>
Message-ID: <Pine.GSO.3.95q.1050112182308.5401D-100000@sun12.math.uni-hamburg.de>

For the KS-test of normality with estimated parameters see

?lillie.test in package nortest.

Best,
Christian

On Wed, 12 Jan 2005, Christoph Buser wrote:

> Hi Kwabena
> 
> I did once a simulation, generating normal distributed values
> (500 values) and calculating a KS test with estimated
> parameters. For 10000 times repeating this test I got about
> 1 significant tests (on a level alpha=0.05 I'm expecting about 500 
> significant tests by chance)
> So I think if you estiamte the parameters from the data, you fit
> to good and the used distribution of the test statistic is not
> adequate as it is indicated in the help page you cited. There
> (in the help page) is some literature, but it is no easy stuff
> to read.
> Furthermore I know no implementation of an KS test which
> accounts for this estimation of the parameter.
> 
> I recommend a graphical tool instead of a test:
> 
> x <- rlnorm(100)
> qqnorm(log(x))
> 
> See also ?qqnorm and ?qqplot.
> 
> If you insist on testing a theoretical distribution be aware
> that a non significant test does not mean that your data has the
> tested distribution (especially if you have few data, there is
> no power in the test to detect deviations from the theoretical
> distribution and the conclusion that the data fits well is
> trappy)
> 
> If there are enough data I'd prefer a chi square test to the KS
> test (but even there I use graphical tools instead). 
> 
> See ?chisq
> 
> For this test you have to specify classes and this is 
> subjective (you can't avoid this).
> 
> You can reduce the DF of the expected chi square distribution
> (under H_0) by the number of estimated parameters from the data
> and will get better results. 
> 
> DF = number of classes - 1 - estimated parameters
> 
> I think this test is more powerful than the KS test,
> particularly if you must estimate the parameters from data.
> 
> Regards,
> 
> Christoph
> 
> -- 
> Christoph Buser <buser at stat.math.ethz.ch>
> Seminar fuer Statistik, LEO C11
> ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
> phone: x-41-1-632-5414		fax: 632-1228
> http://stat.ethz.ch/~buser/
> 
> 
> 
> Kwabena Adusei-Poku writes:
>  > Hello all,
>  > 
>  > Would somebody be kind enough to show me how to do a KS test in R for a
>  > lognormal distribution with ESTIMATED parameters. The R function
>  > ks.test()says "the parameters specified must be prespecified and not
>  > estimated from the data" Is there a way to correct this when one uses
>  > estimated data?
>  > 
>  > Regards,
>  > 
>  > Kwabena.
>  > 
>  > --------------------------------------------
>  > Kwabena Adusei-Poku
>  > University of Goettingen
>  > Institute of Statistics and Econometrics
>  > Platz der Goettingen Sieben 5
>  > 37073 Goettingen
>  > Germany
>  > Tel: +49-(0)551-394794
>  > 
>  > ______________________________________________
>  > R-help at stat.math.ethz.ch mailing list
>  > https://stat.ethz.ch/mailman/listinfo/r-help
>  > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From ggrothendieck at myway.com  Wed Jan 12 18:26:10 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 12 Jan 2005 17:26:10 +0000 (UTC)
Subject: [R] changing langage
References: <1105539446.41e531763d829@imapwww.epfl.ch>
	<41E5526C.4060608@pandora.be>
Message-ID: <loom.20050112T181854-50@post.gmane.org>

Kurt Sys <kurt.sys <at> pandora.be> writes:

: 
: Hi all,
: 
: I've got a small, practical question, which untill now I couldn't solve 
: (otherwhise I wouldn't mail it, right?) First of all, I'm talking about 
: R 2.0.1 on a winxp system (using the default graphical interface being 
: 'Rgui').
: When I make plots, using dates on the x-axis, it puts the labels in 
: Dutch, which is nice (since it's my mother tongue) unless I want them in 
: English... Is there a way to change this behaviour?  (Can I change the 
: labels etc to English?)


Here is an example:

R> Sys.setlocale("LC_TIME", "en-us")
[1] "English_United States.1252"
R> format(ISOdate(2004,1:12,1),"%B")
 [1] "January"   "February"  "March"     "April"     "May"       "June"     
 [7] "July"      "August"    "September" "October"   "November"  "December" 
R> Sys.setlocale("LC_TIME", "du-be")
[1] "Dutch_Netherlands.1252"
R> format(ISOdate(2004,1:12,1),"%B")
 [1] "januari"   "februari"  "maart"     "april"     "mei"       "juni"     
 [7] "juli"      "augustus"  "september" "oktober"   "november"  "december"> 
R> R.version.string # XP
[1] "R version 2.1.0, 2005-01-02" 

For more codes, google for:

   Microsoft language codes 

and look at the first result that is on a Microsoft site.

This may or may not change your labels depending on precisely
what you are doing.



From ripley at stats.ox.ac.uk  Wed Jan 12 18:27:10 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 12 Jan 2005 17:27:10 +0000 (GMT)
Subject: [R] changing langage 
In-Reply-To: <41E5526C.4060608@pandora.be>
References: <1105539446.41e531763d829@imapwww.epfl.ch>
	<41E5526C.4060608@pandora.be>
Message-ID: <Pine.LNX.4.61.0501121724001.28564@gannet.stats>

It uses the language set by LC_TIME: see ?Sys.setlocale and ?format.Date
which references it.

On Wed, 12 Jan 2005, Kurt Sys wrote:

> Hi all,
>
> I've got a small, practical question, which untill now I couldn't solve 
> (otherwhise I wouldn't mail it, right?) First of all, I'm talking about R 
> 2.0.1 on a winxp system (using the default graphical interface being 'Rgui').
> When I make plots, using dates on the x-axis, it puts the labels in Dutch, 
> which is nice (since it's my mother tongue) unless I want them in English... 
> Is there a way to change this behaviour?  (Can I change the labels etc to 
> English?)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jan 12 18:34:58 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 12 Jan 2005 17:34:58 +0000 (GMT)
Subject: [R] "model.response" error
In-Reply-To: <200501121713.j0CHDjD5023414@expredir2.cites.uiuc.edu>
References: <200501121713.j0CHDjD5023414@expredir2.cites.uiuc.edu>
Message-ID: <Pine.LNX.4.61.0501121728070.28564@gannet.stats>

You have some package in use which defines an S4 method for 
model.response, most likely SparseM.  You didn't say that, but therein 
lies the problem.  It's hard for us to guess why it is failing to  find 
its default method.

Incidentally,

mf <- lm(formula, data, na.action = na.action, method = "model.frame")

is a roundabout way to do

mf <- model.frame(formula, data, na.action = na.action)


On Wed, 12 Jan 2005, Bang wrote:

> When I installed R 2.0.1 (replacing 1.9.0) for Windows, a code using
> "model.response" began acting up.  Here are the first several lines of a
> code I had been tweaking for a spatial model (the code is mostly that of
> Roger Bivand--I am adapting it to a slightly different data structure and
> the problem I'm sure is with my changes, not his code).
>
> <command name> <- function (formula, data = list(), weights, na.action =
> na.fail, type = "lag", quiet = TRUE, zero.policy = FALSE, tol.solve = 1e-07,
> tol.opt = .Machine$double.eps^0.5, sparsedebug = FALSE)
> {
>    mt <- terms(formula, data = data)
>    mf <- lm(formula, data, na.action = na.action, method = "model.frame")
>    na.act <- attr(mf, "na.action")
>    if (!is.matrix.csr(weights))
>        cat("\nWarning: weights matrix not in sparse form\n")
>    switch(type, lag = if (!quiet)
>        cat("\nSpatial lag model\n"), mixed = if (!quiet)
>        cat("\nSpatial mixed autoregressive model\n"), stop("\nUnknown model
> type\n"))
>    if (!quiet)
>        cat("Jacobian calculated using weights matrix eigenvalues\n")
>    y <- model.response(mf, "numeric")
>    if (any(is.na(y))) stop("NAs in dependent variable")
>    x <- model.matrix(mt, mf)
>    if (any(is.na(x))) stop("NAs in independent variable")
>    if (nrow(x) != nrow(weights))
>        stop("Input data and weights have different dimensions")
>    n <- nrow(x)
>    m <- ncol(x)
>
> When it reads the "Y" variable in the command:
>
> "y <- model.response(mf, "numeric")"
>
> The error it gives is:
>
> "Error in model.response(mf, "numeric") : No direct or inherited method
> for function "model.response" for this call"
>
> The problem is puzzling me because it is not something I encountered when I
> was running the same code in 1.9.0, but is causing problems in 2.0.1
>
> Thanks, and any comments on debugging the error are welcome.
>
> Jim
>
> Well I AM missing the back of my head.you COULD cut me a little slack!
> -Homer Simpson

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Roger.Bivand at nhh.no  Wed Jan 12 18:41:06 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 12 Jan 2005 18:41:06 +0100 (CET)
Subject: [R] "model.response" error
In-Reply-To: <200501121713.j0CHDjD5023414@expredir2.cites.uiuc.edu>
Message-ID: <Pine.LNX.4.44.0501121833580.18381-100000@reclus.nhh.no>

On Wed, 12 Jan 2005, Bang wrote:

> When I installed R 2.0.1 (replacing 1.9.0) for Windows, a code using
> "model.response" began acting up.  Here are the first several lines of a
> code I had been tweaking for a spatial model (the code is mostly that of
> Roger Bivand--I am adapting it to a slightly different data structure and
> the problem I'm sure is with my changes, not his code).

I don't think it's the R versions, rather the SparseM versions. I think
what is happening is the SparseM generic for "model.response" is being
picked up. In the current NAMESPACE file in the spdep package I now have:

importFrom(stats, model.matrix, model.response)

but I'm not sure that your function is in a package. You will probably 
need to say that both model.response and model.matrix are from stats, at 
least this should give you a lead.

Best wishes,

Roger

> 
> <command name> <- function (formula, data = list(), weights, na.action =
> na.fail, type = "lag", quiet = TRUE, zero.policy = FALSE, tol.solve = 1e-07,
> tol.opt = .Machine$double.eps^0.5, sparsedebug = FALSE)
> {
>     mt <- terms(formula, data = data)
>     mf <- lm(formula, data, na.action = na.action, method = "model.frame")
>     na.act <- attr(mf, "na.action")
>     if (!is.matrix.csr(weights))
>         cat("\nWarning: weights matrix not in sparse form\n")
>     switch(type, lag = if (!quiet)
>         cat("\nSpatial lag model\n"), mixed = if (!quiet)
>         cat("\nSpatial mixed autoregressive model\n"), stop("\nUnknown model
> type\n"))
>     if (!quiet)
>         cat("Jacobian calculated using weights matrix eigenvalues\n")
>     y <- model.response(mf, "numeric")
>     if (any(is.na(y))) stop("NAs in dependent variable")
>     x <- model.matrix(mt, mf)
>     if (any(is.na(x))) stop("NAs in independent variable")
>     if (nrow(x) != nrow(weights))
>         stop("Input data and weights have different dimensions")
>     n <- nrow(x)
>     m <- ncol(x)
> 
> When it reads the "Y" variable in the command:
> 
> "y <- model.response(mf, "numeric")"
> 
> The error it gives is:
> 
> "Error in model.response(mf, "numeric") : No direct or inherited method
> for function "model.response" for this call"
> 
> The problem is puzzling me because it is not something I encountered when I
> was running the same code in 1.9.0, but is causing problems in 2.0.1
> 
> Thanks, and any comments on debugging the error are welcome. 
> 
> Jim
> 
> Well I AM missing the back of my head.you COULD cut me a little slack!
> -Homer Simpson
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From gunter.berton at gene.com  Wed Jan 12 18:45:22 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 12 Jan 2005 09:45:22 -0800
Subject: [R] defining lower part of distribution with covariate
In-Reply-To: <5.2.0.9.0.20050112180549.0378f9b0@home.gvdnet.dk>
Message-ID: <200501121745.j0CHjMgq001149@volta.gene.com>

Troels:

It would be best if you discussed this with a local statistician to make
sure that the data and analysis are properly addressing the scientific
issues. Perhaps that is why no one replied to your previous post. Also, this
is primarily a **statistical** issue, not really an ** R-issue **.

Having said that, I'll take a stab at it ...

Probably the most important thing to say is that there is probably not much
that these data can tell you as you only have 36 cases in all and only 3 are
<= 12. While this probably represented a **lot** of work for you, the simple
fact is that when trying to understand what influences dichotomous
probabilities, you generally need lots of data (hundreds of cases,
typically). Note: This remark may be subject to correction by wiser
statisticians.

Next, the nature of your response, GCS. It appears to be a subjective rating
score that is probably best modeled as an ordered categorical response,
which in R is called an ordered factor. Dichotomizing it to <=12/>12 loses
information. Treating it as a continuous response (quantreg/ancova) seems
inappropriate for your data.

Finally, the model. Considering GCS to be an ordered category, a reasonable
modeling strategy seems to be "proportional odds logistic regression," which
models the GCS response as a linear function of the anstimes and anstypes
(which encompasses your ancova ideas). The results from this model would
then allow you to calculate the <=12 probability if you chose to do so. This
model can be fit using the polr() function in the MASS package.

However, I again urge you to discuss this with a local statistically
knowledgeable resource -- and not to expect too much from such rather meager
data. 

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Troels 
> Ring (by way of Troels Ring <tring at gvdnet.dk>)
> Sent: Wednesday, January 12, 2005 9:11 AM
> To: R-help
> Subject: [R] defining lower part of distribution with covariate
> 
> I try again - perhaps it is analysis of covariance with treatment 
> (thio,ultiva) as two categories and antime as covariate. On 
> the basis of 
> such a model, is then the probability of GCS <= 12 larger 
> with thio treatment ?
> 
> 
> Dear friends, forgive me a simple question, possibly related 
> to quantreg 
> but I failed to get it done and hope for basic instruction.
> 
> I have two sets of observed Glasgow coma scores at admission 
> to ICU after 
> operation, and accompanying time of anesthesia (in hours).
> Thio is cheap and perhaps old fashioned, and ultiva expensive 
> and rapidly 
> terminated. The problem is to estimate the probability of GCS 
> 12 or lower 
> on the two treatments after taking time of anesthesia into 
> account (antime) 
> which is longer for thio. How would I do that in the best way ?
> 
> Best wishes
> Troels Ring, MD
> Aalborg, Denmark
> 
> 
> thio
>        GCS antime
>   [1,]  14    4.5
>   [2,]  15    7.5
>   [3,]  11    7.5
>   [4,]  15    4.5
>   [5,]  14    4.5
>   [6,]  15    3.5
>   [7,]  15    5.5
>   [8,]  14    5.5
>   [9,]  15    3.5
> [10,]  14    8.5
> [11,]  13    4.5
> [12,]  12    5.5
> [13,]  15    3.5
> [14,]  13    6.5
> [15,]   9    8.5
> [16,]  15    6.5
>  > ultiva
>        GCS antime
>   [1,]  15    4.5
>   [2,]  15    4.5
>   [3,]  15    2.5
>   [4,]  15    3.5
>   [5,]  15    3.5
>   [6,]  12    5.5
>   [7,]  15    4.5
>   [8,]  15    3.5
>   [9,]  15    8.5
> [10,]  13    4.5
> [11,]  14    3.5
> [12,]  14    4.5
> [13,]  15    4.5
> [14,]  14    2.5
> [15,]  15    4.5
> [16,]  15    3.5
> [17,]  15    3.5
> [18,]  14    4.5
> [19,]  14    4.5
> [20,]  15    4.5
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From pgilbert at bank-banque-canada.ca  Wed Jan 12 19:00:26 2005
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Wed, 12 Jan 2005 13:00:26 -0500
Subject: [R] transfer function models
In-Reply-To: <41E3BA5F.8010707@glam.ac.uk>
References: <41E3BA5F.8010707@glam.ac.uk>
Message-ID: <41E565BA.2030306@bank-banque-canada.ca>

I don't know what SAS does, but transfer functions are essentially MA/AR 
from an ARMA model, so you should be able to get what you want from the 
various ARMA estimation tools in R.

Paul Gilbert

Samuel Kemp (Comp) wrote:
> Hi,
> 
> Does anyone know of a function in R that can estimate the parameters of 
> a transfer function model with added noise like in SAS?
> 
> Thanks in advance,
> 
> Sam.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From dmb at mrc-dunn.cam.ac.uk  Wed Jan 12 19:13:18 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Wed, 12 Jan 2005 18:13:18 +0000 (GMT)
Subject: [R] Standard error for the area under a smoothed ROC curve?
In-Reply-To: <41E523AF.5030303@vanderbilt.edu>
Message-ID: <Pine.LNX.4.21.0501121803370.8740-100000@mail.mrc-dunn.cam.ac.uk>

On Wed, 12 Jan 2005, Frank E Harrell Jr wrote:

>Dan Bolser wrote:
>> Hello, 
>> 
>> I am making some use of ROC curve analysis. 
>> 
>> I find much help on the mailing list, and I have used the Area Under the
>> Curve (AUC) functions from the ROC function in the bioconductor project...
>> 
>> http://www.bioconductor.org/repository/release1.5/package/Source/
>> ROC_1.0.13.tar.gz 
>> 
>> However, I read here...
>> 
>> http://www.medcalc.be/manual/mpage06-13b.php
>> 
>> "The 95% confidence interval for the area can be used to test the
>> hypothesis that the theoretical area is 0.5. If the confidence interval
>> does not include the 0.5 value, then there is evidence that the laboratory
>> test does have an ability to distinguish between the two groups (Hanley &
>> McNeil, 1982; Zweig & Campbell, 1993)."
>> 
>> But aside from early on the above article is short on details. Can anyone
>> tell me how to calculate the CI of the AUC calculation?
>> 
>> 
>> I read this...
>> 
>> http://www.bioconductor.org/repository/devel/vignette/ROCnotes.pdf
>> 
>> Which talks about resampling (by showing R code), but I can't understand
>> what is going on, or what is calculated (the example given is specific to
>> microarray analysis I think).
>> 
>> I think a general AUC CI function would be a good addition to the ROC
>> package.
>> 
>> 
>> 
>> 
>> One more thing, in calculating the AUC I see the splines function is
>> recomended over the approx function. Here...
>> 
>> http://tolstoy.newcastle.edu.au/R/help/04/10/6138.html
>> 
>> How would I rewrite the following AUC functions (adapted from bioconductor
>> source) to use splines (or approxfun or splinefun) ...
>> 
>> 
>>>spe # Specificity
>> 
>>  [1] 0.02173913 0.13043478 0.21739130 0.32608696 0.43478261 0.54347826
>>  [7] 0.65217391 0.76086957 0.89130435 1.00000000 1.00000000 1.00000000
>> [13] 1.00000000
>> 
>> 
>>>sen # Sensitivity
>> 
>>  [1] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.9302326 0.8139535
>>  [8] 0.6976744 0.5581395 0.4418605 0.3488372 0.2325581 0.1162791
>> 
>> trapezint(1-spe,sen)
>> my.integrate(1-spe,sen)
>> 
>> ## Functions
>> ## Nicked (and modified) from the ROC function in bioconductor.
>> "trapezint" <-
>> function (x, y, a = 0, b = 1)
>> {
>>     if (x[1] > x[length(x)]) {
>>       x <- rev(x)
>>       y <- rev(y)
>>     }
>>     y <- y[x >= a & x <= b]
>>     x <- x[x >= a & x <= b]
>>     if (length(unique(x)) < 2)
>>         return(NA)
>>     ya <- approx(x, y, a, ties = max, rule = 2)$y
>>     yb <- approx(x, y, b, ties = max, rule = 2)$y
>>     x <- c(a, x, b)
>>     y <- c(ya, y, yb)
>>     h <- diff(x)
>>     lx <- length(x)
>>     0.5 * sum(h * (y[-1] + y[-lx]))
>> }
>> 
>> "my.integrate" <-
>> function (x, y, t0 = 1)
>> {
>>     f <- function(j) approx(x,y,j,rule=2,ties=max)$y
>>     integrate(f, 0, t0)$value
>> }
>> 
>> 
>> 
>> 
>> 
>> Thanks for any pointers,
>> Dan.
>
>I don't see why the above formulas are being used.  The 
>Bamber-Hanley-McNeil-Wilcoxon-Mann-Whitney nonparametric method works 
>great.  Just get the U statistic (concordance probability) used in 
>Wilcoxon.  As Somers' Dxy rank correlation coefficient is 2*(1-C) where 
>C is the concordance or ROC area, the Hmisc package function rcorr.cens 
>uses U statistic methods to get the standard error of Dxy.  You can 
>easily translate this to a standard error of C.


I am sure I could do this easily, except I can't. 

The good thing about ROC is that I understand it (I can see it). I know
why the area means what it means, and I could even imagine how sampling
the data could give a CI on the area. 

However, I don't know why "the area under the ROC curve is well known to
be equivalent to the numerator of the Mann-Whitney U statistic" - from

http://www.bioconductor.org/repository/devel/vignette/ROCnotes.pdf


Nor do I know how to calculate "the numerator of the Mann-Whitney U
statistic".

Can you point me at some ? pages or tutorials or even give an example of
what you suggested so I can try to follow it through?

I tried the following...

x <- rnorm(100,5,1)    # REAL NEGATIVE
#
y <- rnorm(100,8,1)    # REAL POSITIVE

t <- wilcox.test(x,y,paired=FALSE,conf.int=0.95)

> t

	Wilcoxon rank sum test with continuity correction

data:  x and y 
W = 132, p-value < 2.2e-16
alternative hypothesis: true mu is not equal to 0 
95 percent confidence interval:
 -3.232207 -2.664620 
sample estimates:
difference in location 
             -2.957496 


And from ?wilcox.test ...

"if both x and y are given and paired is FALSE, a Wilcoxon rank sum test
(equivalent to the Mann-Whitney test) is carried out."


But I don't know what to do next. Sorry for all the questions, but I am a
dumb biologist.

Thanks for the help, Dan.



>
>Frank
>
>



From sundar.dorai-raj at pdf.com  Wed Jan 12 19:22:47 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 12 Jan 2005 12:22:47 -0600
Subject: [R] "model.response" error
In-Reply-To: <200501121713.j0CHDjD5023414@expredir2.cites.uiuc.edu>
References: <200501121713.j0CHDjD5023414@expredir2.cites.uiuc.edu>
Message-ID: <41E56AF7.4060904@pdf.com>



Bang wrote:

> When I installed R 2.0.1 (replacing 1.9.0) for Windows, a code using
> "model.response" began acting up.  Here are the first several lines of a
> code I had been tweaking for a spatial model (the code is mostly that of
> Roger Bivand--I am adapting it to a slightly different data structure and
> the problem I'm sure is with my changes, not his code).
> 
> <command name> <- function (formula, data = list(), weights, na.action =
> na.fail, type = "lag", quiet = TRUE, zero.policy = FALSE, tol.solve = 1e-07,
> tol.opt = .Machine$double.eps^0.5, sparsedebug = FALSE)
> {
>     mt <- terms(formula, data = data)
>     mf <- lm(formula, data, na.action = na.action, method = "model.frame")
>     na.act <- attr(mf, "na.action")
>     if (!is.matrix.csr(weights))
>         cat("\nWarning: weights matrix not in sparse form\n")
>     switch(type, lag = if (!quiet)
>         cat("\nSpatial lag model\n"), mixed = if (!quiet)
>         cat("\nSpatial mixed autoregressive model\n"), stop("\nUnknown model
> type\n"))
>     if (!quiet)
>         cat("Jacobian calculated using weights matrix eigenvalues\n")
>     y <- model.response(mf, "numeric")
>     if (any(is.na(y))) stop("NAs in dependent variable")
>     x <- model.matrix(mt, mf)
>     if (any(is.na(x))) stop("NAs in independent variable")
>     if (nrow(x) != nrow(weights))
>         stop("Input data and weights have different dimensions")
>     n <- nrow(x)
>     m <- ncol(x)
> 
> When it reads the "Y" variable in the command:
> 
> "y <- model.response(mf, "numeric")"
> 
> The error it gives is:
> 
> "Error in model.response(mf, "numeric") : No direct or inherited method
> for function "model.response" for this call"
> 
> The problem is puzzling me because it is not something I encountered when I
> was running the same code in 1.9.0, but is causing problems in 2.0.1
> 
> Thanks, and any comments on debugging the error are welcome. 
> 
> Jim
> 
> Well I AM missing the back of my head.you COULD cut me a little slack!
> -Homer Simpson
> 

Jim,

The following works for me on R-2.0.1 on Win2000:

# example from ?lm
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2,10,20, labels=c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group, method = "model.frame")
y <- model.response(lm.D9, "numeric")

HTH,

--sundar



From kebasita40 at yahoo.com  Wed Jan 12 19:24:48 2005
From: kebasita40 at yahoo.com (Kevin Ita)
Date: Wed, 12 Jan 2005 10:24:48 -0800 (PST)
Subject: [R] Please unsubscribe me from you list
In-Reply-To: <200501121713.j0CHDjD5023414@expredir2.cites.uiuc.edu>
Message-ID: <20050112182448.25997.qmail@web52108.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050112/26c7b0a6/attachment.pl

From f.harrell at vanderbilt.edu  Wed Jan 12 20:09:33 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 12 Jan 2005 13:09:33 -0600
Subject: [R] Standard error for the area under a smoothed ROC curve?
In-Reply-To: <Pine.LNX.4.21.0501121803370.8740-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0501121803370.8740-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <41E575ED.8010202@vanderbilt.edu>

Dan Bolser wrote:
> On Wed, 12 Jan 2005, Frank E Harrell Jr wrote:
> 
> 
>>Dan Bolser wrote:
>>
>>>Hello, 
>>>
>>>I am making some use of ROC curve analysis. 
>>>
>>>I find much help on the mailing list, and I have used the Area Under the
>>>Curve (AUC) functions from the ROC function in the bioconductor project...
>>>
>>>http://www.bioconductor.org/repository/release1.5/package/Source/
>>>ROC_1.0.13.tar.gz 
>>>
>>>However, I read here...
>>>
>>>http://www.medcalc.be/manual/mpage06-13b.php
>>>
>>>"The 95% confidence interval for the area can be used to test the
>>>hypothesis that the theoretical area is 0.5. If the confidence interval
>>>does not include the 0.5 value, then there is evidence that the laboratory
>>>test does have an ability to distinguish between the two groups (Hanley &
>>>McNeil, 1982; Zweig & Campbell, 1993)."
>>>
>>>But aside from early on the above article is short on details. Can anyone
>>>tell me how to calculate the CI of the AUC calculation?
>>>
>>>
>>>I read this...
>>>
>>>http://www.bioconductor.org/repository/devel/vignette/ROCnotes.pdf
>>>
>>>Which talks about resampling (by showing R code), but I can't understand
>>>what is going on, or what is calculated (the example given is specific to
>>>microarray analysis I think).
>>>
>>>I think a general AUC CI function would be a good addition to the ROC
>>>package.
>>>
>>>
>>>
>>>
>>>One more thing, in calculating the AUC I see the splines function is
>>>recomended over the approx function. Here...
>>>
>>>http://tolstoy.newcastle.edu.au/R/help/04/10/6138.html
>>>
>>>How would I rewrite the following AUC functions (adapted from bioconductor
>>>source) to use splines (or approxfun or splinefun) ...
>>>
>>>
>>>
>>>>spe # Specificity
>>>
>>> [1] 0.02173913 0.13043478 0.21739130 0.32608696 0.43478261 0.54347826
>>> [7] 0.65217391 0.76086957 0.89130435 1.00000000 1.00000000 1.00000000
>>>[13] 1.00000000
>>>
>>>
>>>
>>>>sen # Sensitivity
>>>
>>> [1] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.9302326 0.8139535
>>> [8] 0.6976744 0.5581395 0.4418605 0.3488372 0.2325581 0.1162791
>>>
>>>trapezint(1-spe,sen)
>>>my.integrate(1-spe,sen)
>>>
>>>## Functions
>>>## Nicked (and modified) from the ROC function in bioconductor.
>>>"trapezint" <-
>>>function (x, y, a = 0, b = 1)
>>>{
>>>    if (x[1] > x[length(x)]) {
>>>      x <- rev(x)
>>>      y <- rev(y)
>>>    }
>>>    y <- y[x >= a & x <= b]
>>>    x <- x[x >= a & x <= b]
>>>    if (length(unique(x)) < 2)
>>>        return(NA)
>>>    ya <- approx(x, y, a, ties = max, rule = 2)$y
>>>    yb <- approx(x, y, b, ties = max, rule = 2)$y
>>>    x <- c(a, x, b)
>>>    y <- c(ya, y, yb)
>>>    h <- diff(x)
>>>    lx <- length(x)
>>>    0.5 * sum(h * (y[-1] + y[-lx]))
>>>}
>>>
>>>"my.integrate" <-
>>>function (x, y, t0 = 1)
>>>{
>>>    f <- function(j) approx(x,y,j,rule=2,ties=max)$y
>>>    integrate(f, 0, t0)$value
>>>}
>>>
>>>
>>>
>>>
>>>
>>>Thanks for any pointers,
>>>Dan.
>>
>>I don't see why the above formulas are being used.  The 
>>Bamber-Hanley-McNeil-Wilcoxon-Mann-Whitney nonparametric method works 
>>great.  Just get the U statistic (concordance probability) used in 
>>Wilcoxon.  As Somers' Dxy rank correlation coefficient is 2*(1-C) where 
>>C is the concordance or ROC area, the Hmisc package function rcorr.cens 
>>uses U statistic methods to get the standard error of Dxy.  You can 
>>easily translate this to a standard error of C.
> 
> 
> 
> I am sure I could do this easily, except I can't. 
> 
> The good thing about ROC is that I understand it (I can see it). I know
> why the area means what it means, and I could even imagine how sampling
> the data could give a CI on the area. 
> 
> However, I don't know why "the area under the ROC curve is well known to
> be equivalent to the numerator of the Mann-Whitney U statistic" - from
> 
> http://www.bioconductor.org/repository/devel/vignette/ROCnotes.pdf
> 
> 
> Nor do I know how to calculate "the numerator of the Mann-Whitney U
> statistic".

This is clear in the original Bamber or Hanley-McNeil articles.  The ROC 
area is a linear translation of the mean rank of predicted values in one 
of the two outcome groups.  The little somers2 function in Hmisc shows this:

##S function somers2
##
##    Calculates concordance probability and Somers'  Dxy  rank  correlation
##    between  a  variable  X  (for  which  ties are counted) and a binary
##    variable Y (having values 0 and 1, for which ties are not  counted).
##    Uses short cut method based on average ranks in two groups.
##
##    Usage:
##
##         somers2(X,Y)
##
##    Returns vector whose elements are C Index, Dxy, n and missing, where
##    C Index is the concordance probability and Dxy=2(C Index-.5).
##
##    F. Harrell 28 Nov 90     6 Apr 98: added weights

somers2 <- function(x, y, weights=NULL, normwt=FALSE, na.rm=TRUE) {
   if(length(y)!=length(x))stop("y must have same length as x")
   y <- as.integer(y)
   wtpres <- length(weights)
   if(wtpres && (wtpres != length(x)))
         stop('weights must have same length as x')
   if(na.rm) {
         miss <- if(wtpres) is.na(x + y + weights) else is.na(x + y)
         nmiss <- sum(miss)
         if(nmiss>0)     {
           miss <- !miss
           x <- x[miss]
           y <- y[miss]
           if(wtpres) weights <- weights[miss]
         }
   } else nmiss <- 0
    u <- sort(unique(y))
   if(any(y %nin% 0:1)) stop('y must be binary')  ## 7dec02
   if(wtpres) {
         if(normwt) weights <- length(x)*weights/sum(weights)
         n <- sum(weights)
   } else n <- length(x)

   if(n<2)stop("must have >=2 non-missing observations")

   n1 <- if(wtpres)sum(weights[y==1]) else sum(y==1)
   if(n1==0 || n1==n) return(c(C=NA,Dxy=NA,n=n,Missing=nmiss))  ## 7dec02
   ## added weights > 0 30Mar00
   mean.rank <- if(wtpres) mean(wtd.rank(x, weights, na.rm=FALSE)[weights >
                                                       0 & y==1]) else
                  mean(rank(x)[y==1])
   c.index <- (mean.rank - (n1+1)/2)/(n-n1)
   dxy <- 2*(c.index-.5)
   r <- c(c.index, dxy, n, nmiss)
   names(r) <- c("C","Dxy","n","Missing")
   r
}

Just ignore all the stuff with weights.  The ROC area is given by 
c.index in the 5th line from the end.  But somers2 does not compute its 
standard error.

A standard text on nonparametric tests shows how to compute the 
Wilcoxon-M-W test stat based on mean(rank(x in y==1 group)).

Frank

> 
> Can you point me at some ? pages or tutorials or even give an example of
> what you suggested so I can try to follow it through?
> 
> I tried the following...
> 
> x <- rnorm(100,5,1)    # REAL NEGATIVE
> #
> y <- rnorm(100,8,1)    # REAL POSITIVE
> 
> t <- wilcox.test(x,y,paired=FALSE,conf.int=0.95)
> 
> 
>>t
> 
> 
> 	Wilcoxon rank sum test with continuity correction
> 
> data:  x and y 
> W = 132, p-value < 2.2e-16
> alternative hypothesis: true mu is not equal to 0 
> 95 percent confidence interval:
>  -3.232207 -2.664620 
> sample estimates:
> difference in location 
>              -2.957496 
> 
> 
> And from ?wilcox.test ...
> 
> "if both x and y are given and paired is FALSE, a Wilcoxon rank sum test
> (equivalent to the Mann-Whitney test) is carried out."
> 
> 
> But I don't know what to do next. Sorry for all the questions, but I am a
> dumb biologist.
> 
> Thanks for the help, Dan.
> 
> 
> 
> 
>>Frank
>>
>>
> 
> 
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From br44114 at yahoo.com  Wed Jan 12 21:04:37 2005
From: br44114 at yahoo.com (bogdan romocea)
Date: Wed, 12 Jan 2005 12:04:37 -0800 (PST)
Subject: [R] global objects not overwritten within function
Message-ID: <20050112200437.13542.qmail@web50306.mail.yahoo.com>

Apparently the message below wasn't posted on R-help, so I'm sending it
again. Sorry if you received it twice.

--- bogdan romocea <br44114 at yahoo.com> wrote:

> Date: Tue, 11 Jan 2005 17:31:42 -0800 (PST)
> From: bogdan romocea <br44114 at yahoo.com>
> Subject: Re: [R] global objects not overwritten within function

Thank you to everyone who replied. I had no idea that ... means
something in R, I only wanted to make the code look simpler. I'm
pasting below the functional equivalent of what took me yesterday a
couple of hours to debug. Function f() takes several arguments (that's
why I want to have the code as a function) and creates several objects.
I then need to use those objects in another function fct(), and I want
to overwrite them to save memory (they're pretty large).

It appears that Robert's guess (dynamic/lexical scoping) explains
what's going on. I've noticed though another strange (to me) issue:
without indexing (such as obj1 <- obj1[obj1 > 0] - which I need to use
though), fct() prints the expected values even without removing the
objects after each iteration. However, after indexing is introduced,
rm() must be used to make fct() return the intended output. How would
that be explained?

Kind regards,
b.

f <- function(read,position){
obj1 <- 5 * read[position]:(read[position]+5)
obj2 <- 7 * read[position]:(read[position]+5)
assign("obj1",obj1,.GlobalEnv)
assign("obj2",obj2,.GlobalEnv)
}
fct <- function(input){
for (i in 1:5)
	{
	f(input,i)
	obj1 <- obj1[obj1 > 0]
	obj2 <- obj2[obj2 > 0]
	print(obj1)
	print(obj2)
#	rm(obj1,obj2)	#get intended results with this line
	}
}
a <- 1:10
fct(a)



From kurt.sys at telenet.be  Wed Jan 12 21:24:13 2005
From: kurt.sys at telenet.be (Kurt Sys)
Date: Wed, 12 Jan 2005 20:24:13 +0000
Subject: [R] changing langage [SOLVED]
Message-ID: <W6126728566130051105561453@asteria.telenet-ops.be>

To all that replied, thanks... I have a clue where I can change the settings.

tnx,
Kurt Sys


>----- Oorspronkelijk bericht -----
>Van
: Gabor Grothendieck [mailto:ggrothendieck at myway.com]
>Verzonden
: woensdag
, januari
 12, 2005 05:26 PM
>Aan
: r-help at stat.math.ethz.ch
>Onderwerp
: Re: [R] changing langage
>
>Kurt Sys <kurt.sys <at> pandora.be> writes:
>
>: 
>: Hi all,
>: 
>: I've got a small, practical question, which untill now I couldn't solve 
>: (otherwhise I wouldn't mail it, right?) First of all, I'm talking about 
>: R 2.0.1 on a winxp system (using the default graphical interface being 
>: 'Rgui').
>: When I make plots, using dates on the x-axis, it puts the labels in 
>: Dutch, which is nice (since it's my mother tongue) unless I want them in 
>: English... Is there a way to change this behaviour?  (Can I change the 
>: labels etc to English?)
>
>
>Here is an example:
>
>R> Sys.setlocale("LC_TIME", "en-us")
>[1] "English_United States.1252"
>R> format(ISOdate(2004,1:12,1),"%B")
> [1] "January"   "February"  "March"     "April"     "May"       "June"     
> [7] "July"      "August"    "September" "October"   "November"  "December" 
>R> Sys.setlocale("LC_TIME", "du-be")
>[1] "Dutch_Netherlands.1252"
>R> format(ISOdate(2004,1:12,1),"%B")
> [1] "januari"   "februari"  "maart"     "april"     "mei"       "juni"     
> [7] "juli"      "augustus"  "september" "oktober"   "november"  "december"> 
>R> R.version.string # XP
>[1] "R version 2.1.0, 2005-01-02" 
>
>For more codes, google for:
>
>   Microsoft language codes 
>
>and look at the first result that is on a Microsoft site.
>
>This may or may not change your labels depending on precisely
>what you are doing.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>



From helprhelp at yahoo.com  Wed Jan 12 22:40:17 2005
From: helprhelp at yahoo.com (Weiwei Shi)
Date: Wed, 12 Jan 2005 13:40:17 -0800 (PST)
Subject: [R] gbm
Message-ID: <20050112214019.77642.qmail@web61305.mail.yahoo.com>

Hi, there:
I am wondering if I can find some detailed explanation
on gbm or explanation on examples of gbm.

thanks,

Ed



From drcarbon at gmail.com  Wed Jan 12 23:18:38 2005
From: drcarbon at gmail.com (Dr Carbon)
Date: Wed, 12 Jan 2005 17:18:38 -0500
Subject: [R] Finding seasonal peaks in a time series....
Message-ID: <e89bb7ac05011214182ea16125@mail.gmail.com>

 I have a seasonal time series. I want to calculate the annual mean
value of the time series at its peak

 (say the mean of the three values before the peak, the peak, and the
three values after the peak).

 The peak of the time series might change cycle slightly from year to year.

# E.g.,
nPts <- 254
foo <- sin((2 * pi * 1/24) * 1:nPts)
foo <- foo + rnorm(nPts, 0, 0.05)
bar <- ts(foo, start = c(1980,3), frequency = 24)
plot(bar)
start(bar)
end(bar)

# I want to find the peak value from each year, and then get the mean
of the values on either side.
# So, if the peak value in the year 1981 is
max.in.1981 <- max(window(bar, start = c(1981,1), end = c(1981,24)))
# e.g, cycle 7 or 8
window(bar, start = c(1981,1), end = c(1981,24)) == max.in.1981
# E.g. if the highest value in 1981 is in cycle 8 I want
mean.in.1981 <- mean(window(bar, start = c(1981,5), end = c(1981,11)))
plot(bar)
points(ts(mean.in.1981, start = c(1981,8), frequency = 24), col =
"red", pch = "+")


 Is there a way to "automate" this for each year.

 How can I return the cycle of the max value by year?

 Thanks in advance. -DC



From spencer.graves at pdf.com  Wed Jan 12 23:26:34 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 12 Jan 2005 14:26:34 -0800
Subject: [R] gbm
In-Reply-To: <20050112214019.77642.qmail@web61305.mail.yahoo.com>
References: <20050112214019.77642.qmail@web61305.mail.yahoo.com>
Message-ID: <41E5A41A.1080807@pdf.com>

      I just got 25 hits from "www.r-project.org" -> search -> "R site 
search".  Might one or more of these help you?  If they don't solve your 
problem, I suggest you try "the posting guide! 
http://www.R-project.org/posting-guide.html".  If that still doesn't 
solve your problem, it should help you phrase your question to increase 
the chances of getting a helpful reply. 

      hope this helps.  spencer graves

Weiwei Shi wrote:

>Hi, there:
>I am wondering if I can find some detailed explanation
>on gbm or explanation on examples of gbm.
>
>thanks,
>
>Ed
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From p.dalgaard at biostat.ku.dk  Wed Jan 12 23:24:15 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Jan 2005 23:24:15 +0100
Subject: [R] gbm
In-Reply-To: <20050112214019.77642.qmail@web61305.mail.yahoo.com>
References: <20050112214019.77642.qmail@web61305.mail.yahoo.com>
Message-ID: <x2hdlme168.fsf@biostat.ku.dk>

Weiwei Shi <helprhelp at yahoo.com> writes:

> Hi, there:
> I am wondering if I can find some detailed explanation
> on gbm or explanation on examples of gbm.

What is gbm?

Green Belt Movement?
Georgie Boy Manufacturing?

I'm serious! Well, only sort of, but try Google on "gbm" and you'll
find those two expansions and several others like them.

I suppose you mean Gradient Boosting Machine, or Generalized Boosted
regression Models. Have you followed up on the references and examples
on its help page?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dalmiral at umich.edu  Thu Jan 13 00:12:30 2005
From: dalmiral at umich.edu (Daniel Almirall)
Date: Wed, 12 Jan 2005 18:12:30 -0500 (EST)
Subject: [R] gbm
In-Reply-To: <x2hdlme168.fsf@biostat.ku.dk>
References: <20050112214019.77642.qmail@web61305.mail.yahoo.com>
	<x2hdlme168.fsf@biostat.ku.dk>
Message-ID: <Pine.SOL.4.58.0501121810400.23711@timepilot.gpcc.itd.umich.edu>


You can also check out:

http://www.i-pensieri.com/gregr/gbm.shtml

There are reference papers on there, too.

HTH,
Danny


On Wed, 12 Jan 2005, Peter Dalgaard wrote:

> Weiwei Shi <helprhelp at yahoo.com> writes:
>
> > Hi, there:
> > I am wondering if I can find some detailed explanation
> > on gbm or explanation on examples of gbm.
>
> What is gbm?
>
> Green Belt Movement?
> Georgie Boy Manufacturing?
>
> I'm serious! Well, only sort of, but try Google on "gbm" and you'll
> find those two expansions and several others like them.
>
> I suppose you mean Gradient Boosting Machine, or Generalized Boosted
> regression Models. Have you followed up on the references and examples
> on its help page?
>
> --
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>



From gunter.berton at gene.com  Thu Jan 13 00:29:01 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 12 Jan 2005 15:29:01 -0800
Subject: [R] Off Topic: Statistical "philosophy"  rant
Message-ID: <200501122329.j0CNT1E8001364@volta.gene.com>

R-Listers.

The following is a rant originally sent privately to Frank Harrell in
response to remarks he made on this list. The ideas are not new or original,
but he suggested I share it with the list, as he felt that it might be of
wider interest, nonetheless. I have real doubts about this, and I apologize
in advance to those who agree that I should have kept my remarks private.
In view of this, if you wish to criticize my remarks on list, that's fine,
but I won't respond (I've said enough already!). I would be happy to discuss
issues (a little) further off list with anyone who wishes to bother, but not
on list. 

Also, Frank sent me a relevant reference for those who might wish to read a
more thoughtful consideration of the issues:

@ARTICLE{far92cos,
   author = {Faraway, J. J.},
   year = 1992,
   title = {The cost of data analysis},
   journal = J Comp Graphical Stat,
   volume = 1,
   pages = {213-229},
   annote = {bootstrap; validation; predictive accuracy; modeling strategy;
            regression diagnostics;model uncertainty}
}

I welcome further relevant references, pro or con!

Finally, I need to emphasize that these are clearly my very personal views
and do not reflect those of my company or colleagues. 

Cheers to all ...
-----------

The relevant portion of Frank's original comment was in a thread about K-S
tests for the goodness of fit of a parametric distribution:

...
> If you use the empirical CDF to select a parametric 
> distribution, the final estimate of the distribution will inherit the 
> variance of the ECDF.
> The main reason statisticians think that 
> parametric curve fits are far more efficient than 
> nonparametric ones is 
> that they don't account for model uncertainty in their final 
> confidence 
> intervals.
> 
> -- Frank Harrell

My reply:

That's a perceptive remark, but I would go further... You mentioned
**model** uncertainty. In fact, in any data analysis in which we explore the
data first to choose a model, fit the model (parametric or non..), and then
use whatever (pivots from parametric analysis; bootstrapping;...) to say
something about "model uncertainty," we're always kidding ourselves and our
colleagues because we fail to take into account the considerable variability
introduced by our initial subjective exploration and subsequent choice of
modeling strategy. One can only say (at best) that the stated model
uncertainty is an underestimate of the true uncertainty. And very likely a
considerable underestimate because of the model choice subjectivity.

Now I in no way wish to discourage or abridge data exploration; only to
point out that we statisticians have promulgated a self-serving and
unrealistic view of the value of formal inference in quantifying true
scientific uncertainty when we do such exploration -- and that there is
therefore something fundamentally contradictory in our own rhetoric and
methods. Taking a larger view, I think this remark is part of the deeper
epistemological issue of characterizing what can be scientifically "known"
or, indeed, defining the difference between science and art, say. My own
view is that scientific certainty is a fruitless concept: we build models
that we benchmark against our subjective measurements (as the measurements
themselves depend on earlier scientific models) of "reality." Insofar as
data can limit or support our flights of modeling fancy, they do; but in the
end, it is neither an objective process nor one whose "uncertainty" can be
strictly quantified. In creating the illusion that "statistical methods" can
overcome these limitations, I think we have both done science a disservice
and relegated ourselves to an isolated, fringe role in scientific inquiry.

Needless to say, opposing viewpoints to such iconclastic remarks are
cheerfully welcomed.

Best regards,

Bert Gunter



From helprhelp at yahoo.com  Thu Jan 13 01:11:58 2005
From: helprhelp at yahoo.com (Weiwei Shi)
Date: Wed, 12 Jan 2005 16:11:58 -0800 (PST)
Subject: [R] gbm
Message-ID: <20050113001159.28336.qmail@web61310.mail.yahoo.com>

Hi, there:
Thanks a lot for all people' prompt replies.

In detail, I am facing a huge amount of data: over
10,000 and 400 vars. This project is very challenging
and interesting to me. I tried rpart which gives me
some promising results but not good enough. So I am
trying randomForest and gbm now. 

My plan of using gbm is like this:
rt<-rpart(...)
gbm(formula(rt)...)

Does this work? (My first question)

My another CONCERN FOR GBM is the scalability since I
realize R seems to load all the data into memory. (My
second question)

But I believe the idea above will run very slowly. (I
think I might try TreeNet, though I don't like it
since it is commercial.). BTW, sampling might be a
good idea, but it does not seem a good idea for my
project from previous experiments.

I read some reference mentioned earlier by helpers
before I sent my first email. But I still appreciate
any helps. You guys are so nice!

BTW, gbm means gradient boosting modeling :)

Ed



From dmb at mrc-dunn.cam.ac.uk  Thu Jan 13 01:38:10 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Thu, 13 Jan 2005 00:38:10 +0000 (GMT)
Subject: [R] Off Topic: Statistical "philosophy"  rant
In-Reply-To: <200501122329.j0CNT1E8001364@volta.gene.com>
Message-ID: <Pine.LNX.4.21.0501130019240.10954-100000@mail.mrc-dunn.cam.ac.uk>

On Wed, 12 Jan 2005, Berton Gunter wrote:

>R-Listers.
>
>The following is a rant originally sent privately to Frank Harrell in
>response to remarks he made on this list. The ideas are not new or original,
>but he suggested I share it with the list, as he felt that it might be of
>wider interest, nonetheless. I have real doubts about this, and I apologize
>in advance to those who agree that I should have kept my remarks private.
>In view of this, if you wish to criticize my remarks on list, that's fine,
>but I won't respond (I've said enough already!). I would be happy to discuss
>issues (a little) further off list with anyone who wishes to bother, but not
>on list. 
>
>Also, Frank sent me a relevant reference for those who might wish to read a
>more thoughtful consideration of the issues:
>
>@ARTICLE{far92cos,
>   author = {Faraway, J. J.},
>   year = 1992,
>   title = {The cost of data analysis},
>   journal = J Comp Graphical Stat,
>   volume = 1,
>   pages = {213-229},
>   annote = {bootstrap; validation; predictive accuracy; modeling strategy;
>            regression diagnostics;model uncertainty}
>}
>
>I welcome further relevant references, pro or con!
>
>Finally, I need to emphasize that these are clearly my very personal views
>and do not reflect those of my company or colleagues. 
>
>Cheers to all ...
>-----------
>
>The relevant portion of Frank's original comment was in a thread about K-S
>tests for the goodness of fit of a parametric distribution:
>
>...
>> If you use the empirical CDF to select a parametric 
>> distribution, the final estimate of the distribution will inherit the 
>> variance of the ECDF.
>> The main reason statisticians think that 
>> parametric curve fits are far more efficient than 
>> nonparametric ones is 
>> that they don't account for model uncertainty in their final 
>> confidence 
>> intervals.
>> 
>> -- Frank Harrell
>
>My reply:
>
>That's a perceptive remark, but I would go further... You mentioned
>**model** uncertainty. In fact, in any data analysis in which we explore the
>data first to choose a model, fit the model (parametric or non..), and then
>use whatever (pivots from parametric analysis; bootstrapping;...) to say
>something about "model uncertainty," we're always kidding ourselves and our
>colleagues because we fail to take into account the considerable variability
>introduced by our initial subjective exploration and subsequent choice of
>modeling strategy. One can only say (at best) that the stated model
>uncertainty is an underestimate of the true uncertainty. And very likely a
>considerable underestimate because of the model choice subjectivity.
>
>Now I in no way wish to discourage or abridge data exploration; only to
>point out that we statisticians have promulgated a self-serving and
>unrealistic view of the value of formal inference in quantifying true
>scientific uncertainty when we do such exploration -- and that there is
>therefore something fundamentally contradictory in our own rhetoric and
>methods. Taking a larger view, I think this remark is part of the deeper
>epistemological issue of characterizing what can be scientifically "known"
>or, indeed, defining the difference between science and art, say. My own
>view is that scientific certainty is a fruitless concept: we build models
>that we benchmark against our subjective measurements (as the measurements
>themselves depend on earlier scientific models) of "reality." Insofar as
>data can limit or support our flights of modeling fancy, they do; but in the
>end, it is neither an objective process nor one whose "uncertainty" can be
>strictly quantified. 

I totally agree with the above and I am totally unqualified to comment on
the below.


You (and others) might find these papers interesting...

http://www.santafe.edu/~chaos/chaos/pubs.htm


Specifically papers like...

Synchronizing to the Environment: Information Theoretic Constraints on
Agent Learning.
http://www.santafe.edu/~cmg/papers/stte.pdf

Is Anything Ever New? Considering Emergence.
http://www.santafe.edu/~cmg/papers/EverNew.pdf


Observing Complexity and The Complexity of Observation
http://www.santafe.edu/~cmg/papers/OCACO.pdf


What Lies Between Order and Chaos?
http://www.santafe.edu/~cmg/papers/wlboac.pdf



And probably many more.


>In creating the illusion that "statistical methods" can
>overcome these limitations, I think we have both done science a disservice
>and relegated ourselves to an isolated, fringe role in scientific inquiry.
>
>Needless to say, opposing viewpoints to such iconclastic remarks are
>cheerfully welcomed.

Does it make any difference to the mass of Saturn?

Dan.

>
>Best regards,
>
>Bert Gunter
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Thu Jan 13 01:53:52 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 12 Jan 2005 19:53:52 -0500
Subject: [R] gbm
Message-ID: <3A822319EB35174CA3714066D590DCD50994E51C@usrymx25.merck.com>

> From: Weiwei Shi
> 
> Hi, there:
> Thanks a lot for all people' prompt replies.
> 
> In detail, I am facing a huge amount of data: over
> 10,000 and 400 vars. This project is very challenging
> and interesting to me. I tried rpart which gives me
> some promising results but not good enough. So I am
> trying randomForest and gbm now. 
> 
> My plan of using gbm is like this:
> rt<-rpart(...)
> gbm(formula(rt)...)
> 
> Does this work? (My first question)

Given a machine with sufficient memory and CPU speed, yes.
 
> My another CONCERN FOR GBM is the scalability since I
> realize R seems to load all the data into memory. (My
> second question)

We have dealt with data larger than what you described.  One thing to avoid
is the use of the formula interface if you have _lots_ (like, hundreds) of
variables.  gbm.fit(), I believe, was created for that reason.
 
> But I believe the idea above will run very slowly. (I
> think I might try TreeNet, though I don't like it
> since it is commercial.). BTW, sampling might be a
> good idea, but it does not seem a good idea for my
> project from previous experiments.

To me being commercial is not a crime.  I judge software on quality, ease of
use, access to source (if I need it), etc.  To me, TreeNet failed on several
of those criteria, but it works just fine for some people.
 
> I read some reference mentioned earlier by helpers
> before I sent my first email. But I still appreciate
> any helps. You guys are so nice!

That's no excuse for not following the posting guide, right?
 
> BTW, gbm means gradient boosting modeling :)

No.  I believe Greg calls it `generalized boosting models'.

Andy

 
> Ed
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Tom.Mulholland at dpi.wa.gov.au  Thu Jan 13 03:06:40 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 13 Jan 2005 10:06:40 +0800
Subject: [R] Changing the ranges for the axis in image()
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA64@afhex01.dpi.wa.gov.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050113/59a91704/attachment.pl

From yuleih at umich.edu  Thu Jan 13 03:42:58 2005
From: yuleih at umich.edu (Yulei He)
Date: Wed, 12 Jan 2005 21:42:58 -0500 (EST)
Subject: [R] multivariate diagnostics
Message-ID: <Pine.SOL.4.58.0501122139570.810@tetris.gpcc.itd.umich.edu>

Hi, there.

I have two questions about the diagnostics in multivarite statistics.

1. Is there any diagnostics tool to check if a multivariate sample is from
multivariate normal distribution? If there is one, is there any function
doing it in R?

2. Is there any function of testing if two multivariate distribution are
same, i.e. the multivariate extension of Kolomogrov-Smirnov test?

Thanks for your help.

Yulei


$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
Yulei He
1586 Murfin Ave. Apt 37
Ann Arbor, MI 48105-3135
yuleih at umich.edu
734-647-0305(H)
734-763-0421(O)
734-763-0427(O)
734-764-8263(fax)
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$



From Tom.Mulholland at dpi.wa.gov.au  Thu Jan 13 03:45:42 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 13 Jan 2005 10:45:42 +0800
Subject: [R] Off Topic: Statistical "philosophy"  rant
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA65@afhex01.dpi.wa.gov.au>

I have often noted that "statistics can't prove a damn thing, but they can be really useful in disproving something." Having spent most of 80s and half of the 90s with the Australian Bureau of Statistics to find out how you collect these numbers, I am disconcerted at the apparent disregard for measurement issues such as bias, input error, questionnaire design etc etc. ... Science wars ... the real world ... and the not so real world. Having only recently discovered what our esteemed J Baron does I should say that a lot of his work requires us to ask how we use (abuse?) the tools we have.

Having said that some of my most influential work has come from data exploration within fields where I would describe myself as a complete novice. Using ony the phrase "the data seems to indicate" realtionship x with y or some variant and asking if this is an accepted norm has produced some unexpected paradigm shifts.

Someone on the list has a footline of something along the lines of "All models are wrong, but some of them are useful." I think this is attributed to Box. As most of us know some of the advice on this list has more sage than others.

That all concludes to say the manner in which we deal with non-model uncertainty, impacts upon the degree to which we perform a disservice to science/ourselves. I think you are being unduly pessimistic, but then again I might just be a cynic masquerading as a realist.

Tom

> -----Original Message-----
...
> That's a perceptive remark, but I would go further... You mentioned
> **model** uncertainty. In fact, in any data analysis in which 
> we explore the
> data first to choose a model, fit the model (parametric or 
> non..), and then
> use whatever (pivots from parametric analysis; 
> bootstrapping;...) to say
> something about "model uncertainty," we're always kidding 
> ourselves and our
> colleagues because we fail to take into account the 
> considerable variability
> introduced by our initial subjective exploration and 
> subsequent choice of
> modelling strategy. One can only say (at best) that the stated model
> uncertainty is an underestimate of the true uncertainty. And 
> very likely a
> considerable underestimate because of the model choice subjectivity.
> 
> Now I in no way wish to discourage or abridge data 
> exploration; only to
> point out that we statisticians have promulgated a self-serving and
> unrealistic view of the value of formal inference in quantifying true
> scientific uncertainty when we do such exploration -- and 
> that there is
> therefore something fundamentally contradictory in our own 
> rhetoric and
> methods. Taking a larger view, I think this remark is part of 
> the deeper
> epistemological issue of characterizing what can be 
> scientifically "known"
> or, indeed, defining the difference between science and art, 
> say. My own
> view is that scientific certainty is a fruitless concept: we 
> build models
> that we benchmark against our subjective measurements (as the 
> measurements
> themselves depend on earlier scientific models) of "reality." 
> Insofar as
> data can limit or support our flights of modeling fancy, they 
> do; but in the
> end, it is neither an objective process nor one whose 
> "uncertainty" can be
> strictly quantified. In creating the illusion that 
> "statistical methods" can
> overcome these limitations, I think we have both done science 
> a disservice
> and relegated ourselves to an isolated, fringe role in 
> scientific inquiry.
> 
> Needless to say, opposing viewpoints to such iconclastic remarks are
> cheerfully welcomed.
> 
> Best regards,
> 
> Bert Gunter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Tom.Mulholland at dpi.wa.gov.au  Thu Jan 13 03:51:41 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 13 Jan 2005 10:51:41 +0800
Subject: [R] Finding seasonal peaks in a time series....
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3C959@afhex01.dpi.wa.gov.au>

You might find breakpoints in strucchange helpful

Tom

> -----Original Message-----
> From: Dr Carbon [mailto:drcarbon at gmail.com]
> Sent: Thursday, 13 January 2005 6:19 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Finding seasonal peaks in a time series....
> 
> 
>  I have a seasonal time series. I want to calculate the annual mean
> value of the time series at its peak
> 
>  (say the mean of the three values before the peak, the peak, and the
> three values after the peak).
> 
>  The peak of the time series might change cycle slightly from 
> year to year.
> 
> # E.g.,
> nPts <- 254
> foo <- sin((2 * pi * 1/24) * 1:nPts)
> foo <- foo + rnorm(nPts, 0, 0.05)
> bar <- ts(foo, start = c(1980,3), frequency = 24)
> plot(bar)
> start(bar)
> end(bar)
> 
> # I want to find the peak value from each year, and then get the mean
> of the values on either side.
> # So, if the peak value in the year 1981 is
> max.in.1981 <- max(window(bar, start = c(1981,1), end = c(1981,24)))
> # e.g, cycle 7 or 8
> window(bar, start = c(1981,1), end = c(1981,24)) == max.in.1981
> # E.g. if the highest value in 1981 is in cycle 8 I want
> mean.in.1981 <- mean(window(bar, start = c(1981,5), end = c(1981,11)))
> plot(bar)
> points(ts(mean.in.1981, start = c(1981,8), frequency = 24), col =
> "red", pch = "+")
> 
> 
>  Is there a way to "automate" this for each year.
> 
>  How can I return the cycle of the max value by year?
> 
>  Thanks in advance. -DC
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Tom.Mulholland at dpi.wa.gov.au  Thu Jan 13 03:54:32 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 13 Jan 2005 10:54:32 +0800
Subject: [R] Please unsubscribe me from you list
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3C95A@afhex01.dpi.wa.gov.au>

You stand more chance if you do it yourself
https://stat.ethz.ch/mailman/listinfo/r-help

> -----Original Message-----
> From: Kevin Ita [mailto:kebasita40 at yahoo.com]
> Sent: Thursday, 13 January 2005 2:25 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Please unsubscribe me from you list
> 
> 
> Please unsubscribe me from your list.
>  
> Thank you.
>  
> Kevin
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Tom.Mulholland at dpi.wa.gov.au  Thu Jan 13 04:20:18 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 13 Jan 2005 11:20:18 +0800
Subject: [R] Finding seasonal peaks in a time series....
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3C95B@afhex01.dpi.wa.gov.au>

Sorry I didn't read the question properly. Please disregard, my mind was elsewhere.

Tom

> -----Original Message-----
> From: Mulholland, Tom 
> Sent: Thursday, 13 January 2005 10:52 AM
> To: Dr Carbon; r-help at stat.math.ethz.ch
> Subject: RE: [R] Finding seasonal peaks in a time series....
> 
> 
> You might find breakpoints in strucchange helpful
> 
> Tom
> 
> > -----Original Message-----
> > From: Dr Carbon [mailto:drcarbon at gmail.com]
> > Sent: Thursday, 13 January 2005 6:19 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Finding seasonal peaks in a time series....
> > 
> > 
> >  I have a seasonal time series. I want to calculate the annual mean
> > value of the time series at its peak
> > 
> >  (say the mean of the three values before the peak, the 
> peak, and the
> > three values after the peak).
> > 
> >  The peak of the time series might change cycle slightly from 
> > year to year.
> > 
> > # E.g.,
> > nPts <- 254
> > foo <- sin((2 * pi * 1/24) * 1:nPts)
> > foo <- foo + rnorm(nPts, 0, 0.05)
> > bar <- ts(foo, start = c(1980,3), frequency = 24)
> > plot(bar)
> > start(bar)
> > end(bar)
> > 
> > # I want to find the peak value from each year, and then 
> get the mean
> > of the values on either side.
> > # So, if the peak value in the year 1981 is
> > max.in.1981 <- max(window(bar, start = c(1981,1), end = c(1981,24)))
> > # e.g, cycle 7 or 8
> > window(bar, start = c(1981,1), end = c(1981,24)) == max.in.1981
> > # E.g. if the highest value in 1981 is in cycle 8 I want
> > mean.in.1981 <- mean(window(bar, start = c(1981,5), end = 
> c(1981,11)))
> > plot(bar)
> > points(ts(mean.in.1981, start = c(1981,8), frequency = 24), col =
> > "red", pch = "+")
> > 
> > 
> >  Is there a way to "automate" this for each year.
> > 
> >  How can I return the cycle of the max value by year?
> > 
> >  Thanks in advance. -DC
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From stuart at stat.harvard.edu  Tue Jan 11 19:17:29 2005
From: stuart at stat.harvard.edu (Elizabeth Stuart)
Date: Tue, 11 Jan 2005 13:17:29 -0500 (EST)
Subject: [R] [R-pkgs] New package: MatchIt
Message-ID: <Pine.LNX.4.58.0501111309520.4433@stat.harvard.edu>

We would like to announce the release of our software MatchIt, now 
available on CRAN.   MatchIt implements a variety of matching methods for 
causal inference.  

Abstract:
MatchIt implements the suggestions of Ho, Imai, King, and Stuart (2004) 
for improving parametric statistical models by preprocessing data with 
nonparametric matching methods. MatchIt implements a wide range of 
sophisticated matching methods, making it possible to greatly reduce the 
dependence of causal inferences on hard-to-justify, but commonly made, 
statistical modeling assumptions. The software also easily fits into 
existing research practices since, after preprocessing data with MatchIt, 
researchers can use whatever parametric model they would have used without 
MatchIt, but produce inferences with substantially more robustness and 
less sensitivity to modeling assumptions. MatchIt is an R program, and 
also works seamlessly with Zelig.

For more information, please see http://gking.harvard.edu/matchit/.  
Comments and suggestions are welcome.  

Sincerely,
Daniel Ho, Kosuke Imai, Gary King, and Elizabeth Stuart

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From vito_ricci at yahoo.com  Thu Jan 13 09:07:07 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 13 Jan 2005 09:07:07 +0100 (CET)
Subject: [R] Re: multivariate diagnostics
Message-ID: <20050113080707.65234.qmail@web41206.mail.yahoo.com>

Hi,

give a look to the package:

mvnormtest	
Normality test for multivariate variables 

http://www.biometrics.mtu.edu/CRAN/src/contrib/Descriptions/mvnormtest.html

it includes a multivariate extention of Shapiro test.

I believe in statistical literature, I read somewhere,
 exists a multivariate extension of KS test. It
assumes that the sum of squared single normal
variables  is distributed accordig a chi square with a
certain number of df.  KS test is used to test if this
is true or not.

Regards
Vito


Yulei He wrote:

Hi, there.

I have two questions about the diagnostics in
multivarite statistics.

1. Is there any diagnostics tool to check if a
multivariate sample is from
multivariate normal distribution? If there is one, is
there any function
doing it in R?

2. Is there any function of testing if two
multivariate distribution are same, i.e. the
multivariate extension of Kolomogrov-Smirnov test?

Thanks for your help.

Yulei

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From Reinhold.Hafner at risklab.de  Thu Jan 13 09:48:45 2005
From: Reinhold.Hafner at risklab.de (Hafner, Reinhold (Risklab))
Date: Thu, 13 Jan 2005 09:48:45 +0100
Subject: [R] Naming Convention
Message-ID: <34309ADC21099A4A92CE4F8A8123391A02542D9D@afwpm006.intradit.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050113/f3128ec8/attachment.pl

From h.andersson at nioo.knaw.nl  Thu Jan 13 10:12:26 2005
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Thu, 13 Jan 2005 10:12:26 +0100
Subject: [R] Setting the width and height of a Sweave figure
Message-ID: <cs5e5m$c2l$1@sea.gmane.org>

Hello R-people,

I have a function which plots two figures next to each other
using  if(dev.cur()==1) x11(width=14,height=7) to create a new window 
and set the size of the window automatically if no device is open.

This works fine in interactive mode but I don't know how Sweave gets the 
parameters for the figures, and right now the default behavior causes 
this figure to be square and the subfigures becomes not no square, 
making them look silly.

I want a general solution so my function can be used in both interactive 
mode and in a sweave file.

Thanks, Henrik Andersson

---------------------------------------------------------
<<>>=
myfun <- function(){
if(dev.cur()==1) x11(width=14,height=7)
par(mfrow=c(1,2))
plot(1:5)
plot(1:5)
}

<<fig=TRUE>>=
myfun()


---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From Christoph.Scherber at uni-jena.de  Thu Jan 13 10:32:14 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Thu, 13 Jan 2005 10:32:14 +0100
Subject: [R] Naming Convention
In-Reply-To: <34309ADC21099A4A92CE4F8A8123391A02542D9D@afwpm006.intradit.net>
References: <34309ADC21099A4A92CE4F8A8123391A02542D9D@afwpm006.intradit.net>
Message-ID: <1105608733.41e6401e02a44@webmail.uni-jena.de>

Dear Reinhold,

All entries are allowed except "price swap" or "price_swap"

Of course it?s most convenient to use short names and small letters for quicker
typing.

Regards
Christoph



Quoting "Hafner, Reinhold (Risklab)" <Reinhold.Hafner at risklab.de>:

> I was wondering whether there exists a naming convention for row and column
> names in R data frames and matrices.
> E.g: PriceSwap or PRICESWAP or PRICE.SWAP
>
> Many thanks.
> Reinhold
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>




----------------------------------------------------------------
This mail was sent through http://webmail.uni-jena.de



From francoisromain at free.fr  Thu Jan 13 10:47:34 2005
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Thu, 13 Jan 2005 10:47:34 +0100
Subject: [R] Naming Convention
In-Reply-To: <1105608733.41e6401e02a44@webmail.uni-jena.de>
References: <34309ADC21099A4A92CE4F8A8123391A02542D9D@afwpm006.intradit.net>
	<1105608733.41e6401e02a44@webmail.uni-jena.de>
Message-ID: <41E643B6.7080001@free.fr>


Christoph Scherber a ?crit :

>Dear Reinhold,
>
>All entries are allowed except "price swap" or "price_swap"
>
>  
>
Hi,

As a matter of fact, the syntax price_swap is now allowed (since R 1.9.1 
I think). The rest is a choice issue ... deprends on you.

Romain.

>Of course it?s most convenient to use short names and small letters for quicker
>typing.
>
>Regards
>Christoph
>
>
>
>Quoting "Hafner, Reinhold (Risklab)" <Reinhold.Hafner at risklab.de>:
>
>  
>
>>I was wondering whether there exists a naming convention for row and column
>>names in R data frames and matrices.
>>E.g: PriceSwap or PRICESWAP or PRICE.SWAP
>>
>>Many thanks.
>>Reinhold
>>
>>    
>>

-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From lecoutre at stat.ucl.ac.be  Thu Jan 13 10:50:18 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Thu, 13 Jan 2005 10:50:18 +0100
Subject: [R] Naming Convention
In-Reply-To: <1105608733.41e6401e02a44@webmail.uni-jena.de>
References: <34309ADC21099A4A92CE4F8A8123391A02542D9D@afwpm006.intradit.net>
	<1105608733.41e6401e02a44@webmail.uni-jena.de>
Message-ID: <6.0.1.1.2.20050113104733.02039708@stat4ux.stat.ucl.ac.be>


Hi,

Not right: any entry is allowed as long as it is a character. Though, some 
are not recommanded...
See:

 > M=diag(2)
 > rownames(M)=c("with a space", "without")
 > colnames(M) <- c("%*%","~")
 > M
              %*% ~
with a space   1 0
without        0 1

Problem araised for data.frame if you want to access to column by names:
 > M <- as.data.frame(M)
 > M$~
Error: syntax error
 > M$"~"
[1] 0 1

(same problem if name contains any other operator or a space)

Eric





At 10:32 13/01/2005, Christoph Scherber wrote:
>Dear Reinhold,
>
>All entries are allowed except "price swap" or "price_swap"
>
>Of course it?s most convenient to use short names and small letters for 
>quicker
>typing.
>
>Regards
>Christoph
>
>
>
>Quoting "Hafner, Reinhold (Risklab)" <Reinhold.Hafner at risklab.de>:
>
> > I was wondering whether there exists a naming convention for row and column
> > names in R data frames and matrices.
> > E.g: PriceSwap or PRICESWAP or PRICE.SWAP
> >
> > Many thanks.
> > Reinhold
> >
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
>
>
>
>
>----------------------------------------------------------------
>This mail was sent through http://webmail.uni-jena.de
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From ripley at stats.ox.ac.uk  Thu Jan 13 11:07:50 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 13 Jan 2005 10:07:50 +0000 (GMT)
Subject: [R] Naming Convention
In-Reply-To: <41E643B6.7080001@free.fr>
References: <34309ADC21099A4A92CE4F8A8123391A02542D9D@afwpm006.intradit.net>
	<1105608733.41e6401e02a44@webmail.uni-jena.de>
	<41E643B6.7080001@free.fr>
Message-ID: <Pine.LNX.4.61.0501131005070.28662@gannet.stats>

On Thu, 13 Jan 2005, [ISO-8859-1] Romain Fran?ois wrote:

>
> Christoph Scherber a ?crit :
>
>> Dear Reinhold,
>> 
>> All entries are allowed except "price swap" or "price_swap"
>
> As a matter of fact, the syntax price_swap is now allowed (since R 1.9.1 I 
> think). The rest is a choice issue ... deprends on you.

Even "price swap" is allowed, but you will have to work a bit harder, e.g. 
call data.frame() with check.names=FALSE and quote the name (preferably 
with backticks) when you want to use it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From chabotd at globetrotter.net  Thu Jan 13 11:52:04 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Thu, 13 Jan 2005 11:52:04 +0100
Subject: [R] subsetting like in SAS
Message-ID: <29A95B6F-6551-11D9-94AF-000393707CF2@globetrotter.net>

Hi,

Being in the process of translating some of my SAS programs to R, I 
encountered one difficulty. I have a solution, but it is not elegant 
(and not pleasant to implement).

I have a large dataset with many variables needed to identify the 
origin of a sample, many to describe sample characteristics, others to 
describe site characteristics.

I want only a (shorter) list of sites and their characteristics.

If "origin", "ship_cat", "ship_nb", "trip" and "set" are needed to 
identify a site, in SAS you'd sort on those variables, then read the 
data with:

data sites;
	set alldata;
	by origin ship_cat ship_nb trip set;
	if first.set;
	keep list-of-variables-detailing-sites;
run;

In R I did this with the Lag function of Hmisc, and the original data 
set also needs to be sorted first:

oL <- Lag(origin)
scL <- Lag(ship_cat)
snL <- Lag(ship_nb)
tL <- Lag(trip)
sL <- Lag(set)
same <- origin==oL & ship_cat==scL & ship_nb==snL & trip==tL & set==sL
sites <- subset(alldata, !same, 
select=c(list-of-variables-detailing-sites)

Could I do better than this?

Thanks in advance,

Denis Chabot



From K.E.Vorloou at durham.ac.uk  Thu Jan 13 12:09:42 2005
From: K.E.Vorloou at durham.ac.uk (Costas Vorlow)
Date: Thu, 13 Jan 2005 11:09:42 +0000
Subject: [R] Changing the ranges for the axis in image()
In-Reply-To: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA64@afhex01.dpi.wa.gov.au>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA64@afhex01.dpi.wa.gov.au>
Message-ID: <41E656F6.30005@durham.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050113/da034892/attachment.pl

From vegard.andersen at ism.uit.no  Thu Jan 13 12:51:44 2005
From: vegard.andersen at ism.uit.no (Vegard Andersen)
Date: Thu, 13 Jan 2005 12:51:44 +0100
Subject: [R] coxph() and intervening events
Message-ID: <opskjhwiwzk0a1dr@petter-smart>

Hello!

I am using the coxph() function for counting process data. I want to  
include an intervening event as one of my covariates. In order to do this  
I have split the relevant observations in my data at the time of  
intervention. But I have not found any way to "inform" coxph() of the id  
of these observations. The result of this is that coxph() interprets the  
split data as extra observations, which is wrong.
Do anyone have a suggestion on how to specify the ID of subjects  
experiencing intervening events?
Or maybe other solutions on how to perform a cox regression including an  
intervening event covariate?

Thanks in advance!

-- 
Best regards,
Vegard Andersen
Institute of Community Medicine
University of Tromso
Tromso, Norway

vegard.andersen at ism.uit.no



From bxc at steno.dk  Thu Jan 13 13:07:39 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Thu, 13 Jan 2005 13:07:39 +0100
Subject: [R] coxph() and intervening events
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FE4B3@exdkba022.novo.dk>

Unless you have repeated events per person, you do not need to
keep tack of which follow-up belongs to whom.

The likelihood contribution from the two parts is a product.

See any textbook on survival anlysis or p. 50 ff. of:
http://staff.pubhealth.ku.dk/~bxc/Melbourne/Staff/foils.pdf
or another of the zillion of survival/epidemiology lecture 
notes on the net.

Best,
Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vegard Andersen
> Sent: Thursday, January 13, 2005 12:52 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] coxph() and intervening events
> 
> 
> Hello!
> 
> I am using the coxph() function for counting process data. I want to  
> include an intervening event as one of my covariates. In 
> order to do this  
> I have split the relevant observations in my data at the time of  
> intervention. But I have not found any way to "inform" 
> coxph() of the id  
> of these observations. The result of this is that coxph() 
> interprets the  
> split data as extra observations, which is wrong.
> Do anyone have a suggestion on how to specify the ID of subjects  
> experiencing intervening events?
> Or maybe other solutions on how to perform a cox regression 
> including an  
> intervening event covariate?
> 
> Thanks in advance!
> 
> -- 
> Best regards,
> Vegard Andersen
> Institute of Community Medicine
> University of Tromso
> Tromso, Norway
> 
> vegard.andersen at ism.uit.no
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From v.demartino2 at virgilio.it  Thu Jan 13 14:14:15 2005
From: v.demartino2 at virgilio.it (Vittorio)
Date: Thu, 13 Jan 2005 13:14:15 +0000
Subject: [R] Time-Series
Message-ID: <200501131314.15697.v.demartino2@virgilio.it>

In a dataframe you call one of the variable (or the column) connecting the 
name of the column to the dataframe name by means of the $ sign:
so z$energy is the column named energy in the dataframe z.

I don't know  how to do the same with a multi-variable time-series. I tried 
both z.energy, z$energy to no avail.

What's the right synthax?

Ciao
Vittorio



From nicolas.deig at epfl.ch  Thu Jan 13 13:50:38 2005
From: nicolas.deig at epfl.ch (nicolas.deig@epfl.ch)
Date: Thu, 13 Jan 2005 13:50:38 +0100 (MET)
Subject: [R] random samples
Message-ID: <1105620638.41e66e9e9458b@imapwww.epfl.ch>

hi,
I am encoutering a very little problem that seemed to be so easy to solve....
I need to divide the array

> A<-c(1:200)

 into two subsets at random. Therefore I use the function "sample" in R:

> S<-sample(A,100)

 for a random sample of size 100. Then I need the values in A that are not
selected in S to be put in another array, there is my problem!
Is there anyway to do this with a function of R or should I do one by myself?

Thanks in advance
Nicolas



From vito_ricci at yahoo.com  Thu Jan 13 13:54:43 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 13 Jan 2005 13:54:43 +0100 (CET)
Subject: [R] Re:Time-Series
Message-ID: <20050113125443.4322.qmail@web41208.mail.yahoo.com>

Hi,

you can address to a single ts in a multivariate ts
object by namets[,index]. See this example:

> dati
    X   Y
1 100 200
2 150 210
3 180 220
4 200 230
5 220 250
> serie<-ts(dati,start=1999)
> serie
Time Series:
Start = 1999 
End = 2003 
Frequency = 1 
       X   Y
1999 100 200
2000 150 210
2001 180 220
2002 200 230
2003 220 250

> serie[,1] ## first ts
Time Series:
Start = 1999 
End = 2003 
Frequency = 1 
[1] 100 150 180 200 220
> serie[,2] ## second ts
Time Series:
Start = 1999 
End = 2003 
Frequency = 1 
[1] 200 210 220 230 250

Regards
Vito


you wrote:

In a dataframe you call one of the variable (or the
column) connecting the 
name of the column to the dataframe name by means of
the $ sign:
so z$energy is the column named energy in the
dataframe z.

I don't know  how to do the same with a multi-variable
time-series. I tried 
both z.energy, z$energy to no avail.

What's the right synthax?

Ciao
Vittorio

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Jan 13 14:04:06 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 13 Jan 2005 14:04:06 +0100
Subject: [R] random samples
References: <1105620638.41e66e9e9458b@imapwww.epfl.ch>
Message-ID: <001001c4f970$5cf1a450$0540210a@www.domain>

try this:

S. <- A[!A%in%S]

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: <nicolas.deig at epfl.ch>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, January 13, 2005 1:50 PM
Subject: [R] random samples


> hi,
> I am encoutering a very little problem that seemed to be so easy to 
> solve....
> I need to divide the array
>
>> A<-c(1:200)
>
> into two subsets at random. Therefore I use the function "sample" in 
> R:
>
>> S<-sample(A,100)
>
> for a random sample of size 100. Then I need the values in A that 
> are not
> selected in S to be put in another array, there is my problem!
> Is there anyway to do this with a function of R or should I do one 
> by myself?
>
> Thanks in advance
> Nicolas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Matthias.Templ at statistik.gv.at  Thu Jan 13 14:08:06 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Thu, 13 Jan 2005 14:08:06 +0100
Subject: [R] random samples
Message-ID: <83536658864BC243BE3C06D7E936ABD501BE1B3C@xchg1.statistik.local>

which(A%in%S==FALSE)

Best
Matthias

> -----Urspr?ngliche Nachricht-----
> Von: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] Im Auftrag von 
> nicolas.deig at epfl.ch
> Gesendet: Donnerstag, 13. J?nner 2005 13:51
> An: r-help at stat.math.ethz.ch
> Betreff: [R] random samples
> 
> 
> hi,
> I am encoutering a very little problem that seemed to be so 
> easy to solve.... I need to divide the array
> 
> > A<-c(1:200)
> 
>  into two subsets at random. Therefore I use the function 
> "sample" in R:
> 
> > S<-sample(A,100)
> 
>  for a random sample of size 100. Then I need the values in A 
> that are not selected in S to be put in another array, there 
> is my problem! Is there anyway to do this with a function of 
> R or should I do one by myself?
> 
> Thanks in advance
> Nicolas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From Kevin.Wang at maths.anu.edu.au  Thu Jan 13 14:09:47 2005
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Fri, 14 Jan 2005 00:09:47 +1100 (EST)
Subject: [R] random samples
In-Reply-To: <1105620638.41e66e9e9458b@imapwww.epfl.ch>
References: <1105620638.41e66e9e9458b@imapwww.epfl.ch>
Message-ID: <Pine.GSO.4.58.0501140009130.25642@yin>

Hi,

On Thu, 13 Jan 2005 nicolas.deig at epfl.ch wrote:

>
> > A<-c(1:200)
> > S<-sample(A,100)
> selected in S to be put in another array, there is my problem!
> Is there anyway to do this with a function of R or should I do one by myself?

Something like:
 A[-S]
should do, I think.

Kev

--------------------------------
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From andy_liaw at merck.com  Thu Jan 13 14:10:29 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 13 Jan 2005 08:10:29 -0500
Subject: [R] random samples
Message-ID: <3A822319EB35174CA3714066D590DCD50994E51D@usrymx25.merck.com>

See ?setdiff.

Andy

> From: nicolas.deig at epfl.ch
> 
> hi,
> I am encoutering a very little problem that seemed to be so 
> easy to solve....
> I need to divide the array
> 
> > A<-c(1:200)
> 
>  into two subsets at random. Therefore I use the function 
> "sample" in R:
> 
> > S<-sample(A,100)
> 
>  for a random sample of size 100. Then I need the values in A 
> that are not
> selected in S to be put in another array, there is my problem!
> Is there anyway to do this with a function of R or should I 
> do one by myself?
> 
> Thanks in advance
> Nicolas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From petr.pikal at precheza.cz  Thu Jan 13 14:23:38 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 13 Jan 2005 14:23:38 +0100
Subject: [R] subsetting like in SAS
In-Reply-To: <29A95B6F-6551-11D9-94AF-000393707CF2@globetrotter.net>
Message-ID: <41E6846A.9797.B8885B@localhost>

Hi Denis

maybe unique() can choose unique entries from your data set 
without need for sorting.

Cheers
Petr

On 13 Jan 2005 at 11:52, Denis Chabot wrote:

> Hi,
> 
> Being in the process of translating some of my SAS programs to R, I
> encountered one difficulty. I have a solution, but it is not elegant
> (and not pleasant to implement).
> 
> I have a large dataset with many variables needed to identify the
> origin of a sample, many to describe sample characteristics, others to
> describe site characteristics.
> 
> I want only a (shorter) list of sites and their characteristics.
> 
> If "origin", "ship_cat", "ship_nb", "trip" and "set" are needed to
> identify a site, in SAS you'd sort on those variables, then read the
> data with:
> 
> data sites;
>  set alldata;
>  by origin ship_cat ship_nb trip set;
>  if first.set;
>  keep list-of-variables-detailing-sites;
> run;
> 
> In R I did this with the Lag function of Hmisc, and the original data
> set also needs to be sorted first:
> 
> oL <- Lag(origin)
> scL <- Lag(ship_cat)
> snL <- Lag(ship_nb)
> tL <- Lag(trip)
> sL <- Lag(set)
> same <- origin==oL & ship_cat==scL & ship_nb==snL & trip==tL & set==sL
> sites <- subset(alldata, !same,
> select=c(list-of-variables-detailing-sites)
> 
> Could I do better than this?
> 
> Thanks in advance,
> 
> Denis Chabot
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From Achim.Zeileis at wu-wien.ac.at  Thu Jan 13 14:30:47 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 13 Jan 2005 14:30:47 +0100
Subject: [R] Re:Time-Series
In-Reply-To: <20050113125443.4322.qmail@web41208.mail.yahoo.com>
References: <20050113125443.4322.qmail@web41208.mail.yahoo.com>
Message-ID: <20050113143047.79900c1a.Achim.Zeileis@wu-wien.ac.at>

On Thu, 13 Jan 2005 13:54:43 +0100 (CET) Vito Ricci wrote:

> Hi,
> 
> you can address to a single ts in a multivariate ts
> object by namets[,index]. See this example:

Note, that index can also be a string and not only the index number. You
wanted to have the analogue of z$energy which could also be
  z[, "energy"]

hth,
Z

> > dati
>     X   Y
> 1 100 200
> 2 150 210
> 3 180 220
> 4 200 230
> 5 220 250
> > serie<-ts(dati,start=1999)
> > serie
> Time Series:
> Start = 1999 
> End = 2003 
> Frequency = 1 
>        X   Y
> 1999 100 200
> 2000 150 210
> 2001 180 220
> 2002 200 230
> 2003 220 250
> 
> > serie[,1] ## first ts
> Time Series:
> Start = 1999 
> End = 2003 
> Frequency = 1 
> [1] 100 150 180 200 220
> > serie[,2] ## second ts
> Time Series:
> Start = 1999 
> End = 2003 
> Frequency = 1 
> [1] 200 210 220 230 250
> 
> Regards
> Vito
> 
> 
> you wrote:
> 
> In a dataframe you call one of the variable (or the
> column) connecting the 
> name of the column to the dataframe name by means of
> the $ sign:
> so z$energy is the column named energy in the
> dataframe z.
> 
> I don't know  how to do the same with a multi-variable
> time-series. I tried 
> both z.energy, z$energy to no avail.
> 
> What's the right synthax?
> 
> Ciao
> Vittorio
> 
> =====
> Diventare costruttori di soluzioni
> Became solutions' constructors
> 
> "The business of the statistician is to catalyze 
> the scientific learning process."  
> George E. P. Box
> 
> Top 10 reasons to become a Statistician
> 
>      1. Deviation is considered normal
>      2. We feel complete and sufficient
>      3. We are 'mean' lovers
>      4. Statisticians do it discretely and continuously
>      5. We are right 95% of the time
>      6. We can legally comment on someone's posterior distribution
>      7. We may not be normal, but we are transformable
>      8. We never have to say we are certain
>      9. We are honestly significantly different
>     10. No one wants our jobs
> 
> 
> Visitate il portale http://www.modugno.it/
> e in particolare la sezione su Palese 
> http://www.modugno.it/archivio/palese/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Thu Jan 13 14:54:52 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 13 Jan 2005 13:54:52 +0000 (GMT)
Subject: [R] Re:Time-Series
In-Reply-To: <20050113143047.79900c1a.Achim.Zeileis@wu-wien.ac.at>
References: <20050113125443.4322.qmail@web41208.mail.yahoo.com>
	<20050113143047.79900c1a.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <Pine.LNX.4.61.0501131352160.14748@gannet.stats>

On Thu, 13 Jan 2005, Achim Zeileis wrote:

> On Thu, 13 Jan 2005 13:54:43 +0100 (CET) Vito Ricci wrote:
>
>> you can address to a single ts in a multivariate ts
>> object by namets[,index]. See this example:
>
> Note, that index can also be a string and not only the index number. You
> wanted to have the analogue of z$energy which could also be
>  z[, "energy"]

The point is that an object of class "mts" is normally a matrix and 
certainly matrix-like (ts.union can make a data frame of that class), so 
matrix indexing works: see `An Introduction to R' for the details.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jfox at mcmaster.ca  Thu Jan 13 15:02:40 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 13 Jan 2005 09:02:40 -0500
Subject: [R] random samples
In-Reply-To: <1105620638.41e66e9e9458b@imapwww.epfl.ch>
Message-ID: <20050113140240.GJT1899.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Nicolas,

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> nicolas.deig at epfl.ch
> Sent: Thursday, January 13, 2005 7:51 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] random samples
> 
> hi,
> I am encoutering a very little problem that seemed to be so 
> easy to solve....
> I need to divide the array
> 
> > A<-c(1:200)

Note that A is a vector, not an array, and that you don't need c().

> 
>  into two subsets at random. Therefore I use the function 
> "sample" in R:
> 
> > S<-sample(A,100)
> 
>  for a random sample of size 100. Then I need the values in A 
> that are not selected in S to be put in another array, there 
> is my problem!
> Is there anyway to do this with a function of R or should I 
> do one by myself?
> 

If in your application, as in your example, the elements of A are all
distinct, then setdiff(A, S) will give you what you want. If the elements of
A are not distinct, then you could sample the indices of the elements and
proceed as above, indexing A by the two vectors of indices.

I hope this helps.
 John

> Thanks in advance
> Nicolas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From p.pagel at gsf.de  Thu Jan 13 15:05:05 2005
From: p.pagel at gsf.de (Philipp Pagel)
Date: Thu, 13 Jan 2005 15:05:05 +0100
Subject: [R] random samples
In-Reply-To: <1105620638.41e66e9e9458b@imapwww.epfl.ch>
References: <1105620638.41e66e9e9458b@imapwww.epfl.ch>
Message-ID: <20050113140505.GA6165@localhost>

On Thu, Jan 13, 2005 at 01:50:38PM +0100, nicolas.deig at epfl.ch wrote:

> I need to divide the array
> 
> > A<-c(1:200)
> 
>  into two subsets at random. Therefore I use the function "sample" in R:

I suggest a slightly different approach which is probably faster than
using setdif() or %in% and friends:

a <- 1:200
i <- sample(1:200, 100)
s1 <- a[i]
s2 <- a[-i]

cu
	Philipp
	
-- 
Dr. Philipp Pagel                            Tel.  +49-89-3187-3675
Institute for Bioinformatics / MIPS          Fax.  +49-89-3187-3585
GSF - National Research Center for Environment and Health
Ingolstaedter Landstrasse 1
85764 Neuherberg, Germany
http://mips.gsf.de/staff/pagel



From v.demartino2 at virgilio.it  Thu Jan 13 15:07:03 2005
From: v.demartino2 at virgilio.it (v.demartino2@virgilio.it)
Date: Thu, 13 Jan 2005 15:07:03 +0100
Subject: [R] R, postgresql, windows & bsd
Message-ID: <41536B850012C130@ims3e.cp.tin.it>

I usually work with R on a windows querying data through RODBC from a postgresql
db on a freebsd machine on my offcie lan. 
Now I have the chance to use R also on a linux gentoo client box and to
connect to the same db.
I know that I can install   the unixodbc stuff and stick to RODBC, BUT my
question is:

Is there any more 'linuxish' way of querying a postgresql server from a
client? 

Ciao
Vittorio



From angelare at to.infn.it  Thu Jan 13 16:07:14 2005
From: angelare at to.infn.it (Angela Re)
Date: Thu, 13 Jan 2005 16:07:14 +0100
Subject: [R] (no subject)
Message-ID: <41E68EA2.9050404@to.infn.it>

Good morning,
I wrote a little code in R which has to show two graphs  but  I can get 
only one. How can I  adress  the graphs in two files?

Second, I'd like, always in the same code, to add a legend  to a graph.  
Better,  I'd like to  put in  such a legend a  new item whose color 
could remind the colour ol the columns it  refers to in the plot. I wrote:

leg.txt<-c("control people", "radiated ill people", "radiated healthy 
people",
"pesticide exposed people")

leg.col<-c("lightblue", "gray", "lightcyan","lavender")

grA<-barplot(seqA, type = "h", col = c(colors),legend.text = 
c(leg.txt),main = " Number of breaks occured on cluster A bands on 
patients' sample", xlab = "patient ID", ylab = "breaks number")
but I don't know how to assign the right colors to legend's items.

Thanks  of helping me, Angela



-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: rotture_campione_cluster.R
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050113/e8c76c33/rotture_campione_cluster.pl
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: ROTTURE_PER_SOGGETTO_RAD_NO_MALATI_CLUSTER_A.dat
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050113/e8c76c33/ROTTURE_PER_SOGGETTO_RAD_NO_MALATI_CLUSTER_A.pl

From tlumley at u.washington.edu  Thu Jan 13 16:07:55 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 13 Jan 2005 07:07:55 -0800 (PST)
Subject: [R] coxph() and intervening events
In-Reply-To: <opskjhwiwzk0a1dr@petter-smart>
References: <opskjhwiwzk0a1dr@petter-smart>
Message-ID: <Pine.A41.4.61b.0501130702500.271994@homer08.u.washington.edu>

On Thu, 13 Jan 2005, Vegard Andersen wrote:

> Hello!
>
> I am using the coxph() function for counting process data. I want to include 
> an intervening event as one of my covariates. In order to do this I have 
> split the relevant observations in my data at the time of intervention. But I 
> have not found any way to "inform" coxph() of the id of these observations. 
> The result of this is that coxph() interprets the split data as extra 
> observations, which is wrong.

No, it is not wrong.

> Do anyone have a suggestion on how to specify the ID of subjects experiencing 
> intervening events?

You can specify the ID by including cluster(id) in your formula. This 
gives you a model-robust sandwich variance.  You don't need to do this and 
it is not standard (though IMO it's a good idea).


 	-thomas



From angelare at to.infn.it  Thu Jan 13 16:10:52 2005
From: angelare at to.infn.it (Angela Re)
Date: Thu, 13 Jan 2005 16:10:52 +0100
Subject: [R] (no subject)
Message-ID: <41E68F7C.30508@to.infn.it>

Good morning,
I wrote a little code in R which has to show two graphs  but  I can get 
only one. How can I  adress  the graphs in two files?

Second, I'd like, always in the same code, to add a legend  to a graph.  
Better,  I'd like to  put in  such a legend a  new item whose color 
could remind the colour ol the columns it  refers to in the plot. I wrote:

leg.txt<-c("control people", "radiated ill people", "radiated healthy 
people",
"pesticide exposed people")

leg.col<-c("lightblue", "gray", "lightcyan","lavender")

grA<-barplot(seqA, type = "h", col = c(colors),legend.text = 
c(leg.txt),main = " Number of breaks occured on cluster A bands on 
patients' sample", xlab = "patient ID", ylab = "breaks number")
but I don't know how to assign the right colors to legend's items.

Thanks  of helping me, Angela


-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: rotture_campione_cluster.R
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050113/56dc2f05/rotture_campione_cluster.pl
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: ROTTURE_PER_SOGGETTO_RAD_NO_MALATI_CLUSTER_A.dat
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050113/56dc2f05/ROTTURE_PER_SOGGETTO_RAD_NO_MALATI_CLUSTER_A.pl

From spencer.graves at pdf.com  Thu Jan 13 17:28:41 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 13 Jan 2005 08:28:41 -0800
Subject: [R] Naming Convention
In-Reply-To: <Pine.LNX.4.61.0501131005070.28662@gannet.stats>
References: <34309ADC21099A4A92CE4F8A8123391A02542D9D@afwpm006.intradit.net>	<1105608733.41e6401e02a44@webmail.uni-jena.de>	<41E643B6.7080001@free.fr>
	<Pine.LNX.4.61.0501131005070.28662@gannet.stats>
Message-ID: <41E6A1B9.6060006@pdf.com>

      Also, while any character string is allowed, the use of strings 
that are not legal names limits their utility, especially for the 
columns of a data.frame, which could normally be used in a formula for 
something like "lm" or even more general using "with" or "attach". 

      hope this helps.  spencer graves

Prof Brian Ripley wrote:

> On Thu, 13 Jan 2005, [ISO-8859-1] Romain Fran?ois wrote:
>
>>
>> Christoph Scherber a ?crit :
>>
>>> Dear Reinhold,
>>>
>>> All entries are allowed except "price swap" or "price_swap"
>>
>>
>> As a matter of fact, the syntax price_swap is now allowed (since R 
>> 1.9.1 I think). The rest is a choice issue ... deprends on you.
>
>
> Even "price swap" is allowed, but you will have to work a bit harder, 
> e.g. call data.frame() with check.names=FALSE and quote the name 
> (preferably with backticks) when you want to use it.
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From michael.watson at bbsrc.ac.uk  Thu Jan 13 17:43:34 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 13 Jan 2005 16:43:34 -0000
Subject: [R] Space between bars in barplot
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E89B3B@iahce2knas1.iah.bbsrc.reserved>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050113/9a2ab472/attachment.pl

From ripley at stats.ox.ac.uk  Thu Jan 13 18:04:37 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 13 Jan 2005 17:04:37 +0000 (GMT)
Subject: [R] Naming Convention
In-Reply-To: <41E6A1B9.6060006@pdf.com>
References: <34309ADC21099A4A92CE4F8A8123391A02542D9D@afwpm006.intradit.net>
	<1105608733.41e6401e02a44@webmail.uni-jena.de>
	<41E643B6.7080001@free.fr>
	<Pine.LNX.4.61.0501131005070.28662@gannet.stats>
	<41E6A1B9.6060006@pdf.com>
Message-ID: <Pine.LNX.4.61.0501131700470.16878@gannet.stats>

On Thu, 13 Jan 2005, Spencer Graves wrote:

>     Also, while any character string is allowed, the use of strings that 
> are not legal names limits their utility, especially for the columns of 
> a data.frame, which could normally be used in a formula for something 
> like "lm" or even more general using "with" or "attach".

That's the advice I was correcting!  Any name can be used in (at least 
simpler versions of) those contexts by use of backquoting.

I think your advice was sound before the introduction of backquoting, but 
is no longer (and that was a while back).

>     hope this helps.  spencer graves

I don't believe it does.


> Prof Brian Ripley wrote:
>
>> On Thu, 13 Jan 2005, [ISO-8859-1] Romain Fran?ois wrote:
>> 
>>> 
>>> Christoph Scherber a ?crit :
>>> 
>>>> Dear Reinhold,
>>>> 
>>>> All entries are allowed except "price swap" or "price_swap"
>>> 
>>> 
>>> As a matter of fact, the syntax price_swap is now allowed (since R 1.9.1 I 
>>> think). The rest is a choice issue ... deprends on you.
>> 
>> 
>> Even "price swap" is allowed, but you will have to work a bit harder, e.g. 
>> call data.frame() with check.names=FALSE and quote the name (preferably 
>> with backticks) when you want to use it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From reid_huntsinger at merck.com  Thu Jan 13 18:19:57 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Thu, 13 Jan 2005 12:19:57 -0500
Subject: [R] global objects not overwritten within function
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9254@uswpmx00.merck.com>

This got rejected by SpamCop but I didn't see another reply so am trying
again.

-----Original Message-----
From: Huntsinger, Reid 
Sent: Wednesday, January 12, 2005 4:29 PM
To: 'bogdan romocea'; r-help at stat.math.ethz.ch
Subject: RE: [R] global objects not overwritten within function


Assigning via <- to "obj1" and "obj2" in fct() creates local copies. In the
next iteration "obj1[obj1 > 0]" and "obj2[obj2 > 0]" refer to these local
copies, unless you remove them, not the ones in .GlobalEnv, as you intend. 

You can also use "get" to specify which environment to look in.

Reid Huntsinger
 -----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
Sent: Wednesday, January 12, 2005 3:05 PM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] global objects not overwritten within function


Apparently the message below wasn't posted on R-help, so I'm sending it
again. Sorry if you received it twice.

--- bogdan romocea <br44114 at yahoo.com> wrote:

> Date: Tue, 11 Jan 2005 17:31:42 -0800 (PST)
> From: bogdan romocea <br44114 at yahoo.com>
> Subject: Re: [R] global objects not overwritten within function

Thank you to everyone who replied. I had no idea that ... means
something in R, I only wanted to make the code look simpler. I'm
pasting below the functional equivalent of what took me yesterday a
couple of hours to debug. Function f() takes several arguments (that's
why I want to have the code as a function) and creates several objects.
I then need to use those objects in another function fct(), and I want
to overwrite them to save memory (they're pretty large).

It appears that Robert's guess (dynamic/lexical scoping) explains
what's going on. I've noticed though another strange (to me) issue:
without indexing (such as obj1 <- obj1[obj1 > 0] - which I need to use
though), fct() prints the expected values even without removing the
objects after each iteration. However, after indexing is introduced,
rm() must be used to make fct() return the intended output. How would
that be explained?

Kind regards,
b.

f <- function(read,position){
obj1 <- 5 * read[position]:(read[position]+5)
obj2 <- 7 * read[position]:(read[position]+5)
assign("obj1",obj1,.GlobalEnv)
assign("obj2",obj2,.GlobalEnv)
}
fct <- function(input){
for (i in 1:5)
	{
	f(input,i)
	obj1 <- obj1[obj1 > 0]
	obj2 <- obj2[obj2 > 0]
	print(obj1)
	print(obj2)
#	rm(obj1,obj2)	#get intended results with this line
	}
}
a <- 1:10
fct(a)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From vito_ricci at yahoo.com  Thu Jan 13 18:23:37 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 13 Jan 2005 18:23:37 +0100 (CET)
Subject: [R] chisq.test() as a goodness of fit test
Message-ID: <20050113172337.35780.qmail@web41215.mail.yahoo.com>

Dear R-Users,

How can I use chisq.test() as a goodness of fit test?
Reading man-page I?ve some doubts that kind of test is
available with this statement. Am I wrong?


X2=sum((O-E)^2)/E)

O=empirical frequencies
E=expected freq. calculated with the model (such as
normal distribution)

See:
http://www.itl.nist.gov/div898/handbook/eda/section3/eda35f.htm
for X2 used as a goodness of fit test.

Any help will be appreciated.
Thank a lot. Bye.
Vito


=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From abunn at whrc.org  Thu Jan 13 18:26:21 2005
From: abunn at whrc.org (Andy Bunn)
Date: Thu, 13 Jan 2005 12:26:21 -0500
Subject: [R] Finding seasonal peaks in a time series....
In-Reply-To: <e89bb7ac05011214182ea16125@mail.gmail.com>
Message-ID: <NEBBIPHDAMMOKDKPOFFIKEDCCPAA.abunn@whrc.org>

This is inelegant, but works:

# (following the example)
nPts <- 254
foo <- sin((2 * pi * 1/24) * 1:nPts)
foo <- foo + rnorm(nPts, 0, 0.05)
bar <- ts(foo, start = c(1980,3), frequency = 24)
mean.in.i <- numeric(length(start(bar)[1]:end(bar)[1]))
peak.ts   <- ts(rep(NA, length(foo)), start = c(1980,3), frequency = 24)
count <- 1
for(i in start(bar)[1]:end(bar)[1]){
      bar.win <- window(bar, start = c(i,1), end = c(i,24))
      max.in.i <- max(bar.win)
      max.cycle.in.i <- cycle(bar.win)[bar.win == max.in.i]
      mean.in.i[count] <- mean(window(bar, start = c(i,max.cycle.in.i - 3),
end = c(i,max.cycle.in.i + 3)))
      window(peak.ts, start = c(i, max.cycle.in.i), end = c(i,
max.cycle.in.i)) <- mean.in.i[count]
      count <- count+1
}
plot(bar)
points(peak.ts, col = "red", pch = "+")
mean.in.i



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Dr Carbon
> Sent: Wednesday, January 12, 2005 5:19 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Finding seasonal peaks in a time series....
>
>
>  I have a seasonal time series. I want to calculate the annual mean
> value of the time series at its peak
>
>  (say the mean of the three values before the peak, the peak, and the
> three values after the peak).
>
>  The peak of the time series might change cycle slightly from
> year to year.
>
> # E.g.,
> nPts <- 254
> foo <- sin((2 * pi * 1/24) * 1:nPts)
> foo <- foo + rnorm(nPts, 0, 0.05)
> bar <- ts(foo, start = c(1980,3), frequency = 24)
> plot(bar)
> start(bar)
> end(bar)
>
> # I want to find the peak value from each year, and then get the mean
> of the values on either side.
> # So, if the peak value in the year 1981 is
> max.in.1981 <- max(window(bar, start = c(1981,1), end = c(1981,24)))
> # e.g, cycle 7 or 8
> window(bar, start = c(1981,1), end = c(1981,24)) == max.in.1981
> # E.g. if the highest value in 1981 is in cycle 8 I want
> mean.in.1981 <- mean(window(bar, start = c(1981,5), end = c(1981,11)))
> plot(bar)
> points(ts(mean.in.1981, start = c(1981,8), frequency = 24), col =
> "red", pch = "+")
>
>
>  Is there a way to "automate" this for each year.
>
>  How can I return the cycle of the max value by year?
>
>  Thanks in advance. -DC
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From f.gherardini at pigrecodata.net  Thu Jan 13 19:45:17 2005
From: f.gherardini at pigrecodata.net (Federico Gherardini)
Date: Thu, 13 Jan 2005 19:45:17 +0100
Subject: [R] Exact poisson confidence intervals
Message-ID: <41E6C1BD.8070302@pigrecodata.net>

Hi all,
Is there any R function to compute exact confidence limits for a Poisson 
distribution with a given Lambda?

Thanks in advance
Federico



From jimmcloughlin at earthlink.net  Thu Jan 13 18:48:02 2005
From: jimmcloughlin at earthlink.net (Jim McLoughlin)
Date: Thu, 13 Jan 2005 09:48:02 -0800
Subject: [R] R, postgresql, windows & bsd
In-Reply-To: <41536B850012C130@ims3e.cp.tin.it>
References: <41536B850012C130@ims3e.cp.tin.it>
Message-ID: <458AC1A6-658B-11D9-8BA1-000393B2DF14@earthlink.net>

On Jan 13, 2005, at 6:07 AM, v.demartino2 at virgilio.it wrote:

> I usually work with R on a windows querying data through RODBC from a 
> postgresql
> db on a freebsd machine on my offcie lan.
> Now I have the chance to use R also on a linux gentoo client box and to
> connect to the same db.
> I know that I can install   the unixodbc stuff and stick to RODBC, BUT 
> my
> question is:
>
> Is there any more 'linuxish' way of querying a postgresql server from a
> client?

Use the Rdbi and RdbiPgSQL packages from www.bioconductor.org.  You 
will need to have the postgresql client libraries installed beforehand 
- the installation of these packages will ask you for the Postgresql 
library and include paths.

Once installed, queries are easy.  For example:

         library(Rdbi)
         pcon <- dbConnect(PgSQL(),host="myIP", 
dbname="myDBName",user="me",password="mypasswd")
         myquery <- paste("select * from mytable")
         resultDataframe <- dbGetQuery(pcon,myquery)
         dbDisconnect(pcon)



From dr.mike at ntlworld.com  Thu Jan 13 19:09:36 2005
From: dr.mike at ntlworld.com (dr mike)
Date: Thu, 13 Jan 2005 18:09:36 -0000
Subject: [R] R, postgresql, windows & bsd
In-Reply-To: <41536B850012C130@ims3e.cp.tin.it>
Message-ID: <20050113180950.BLLP13480.aamta04-winn.mailhost.ntl.com@c400>

 You can do this in Perl (see e.g. Interchange, using Perl-DBI and TCP-IP
instead of sockets). If you're familiar with Perl, that is
......................

Mike

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
v.demartino2 at virgilio.it
Sent: 13 January 2005 14:07
To: r-help
Subject: [R] R, postgresql, windows & bsd

I usually work with R on a windows querying data through RODBC from a
postgresql db on a freebsd machine on my offcie lan. 
Now I have the chance to use R also on a linux gentoo client box and to
connect to the same db.
I know that I can install   the unixodbc stuff and stick to RODBC, BUT my
question is:

Is there any more 'linuxish' way of querying a postgresql server from a
client? 

Ciao
Vittorio

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From rizzo at frognet.net  Thu Jan 13 18:08:09 2005
From: rizzo at frognet.net (Maria Rizzo)
Date: Thu, 13 Jan 2005 12:08:09 -0500
Subject: [R] Re: multivariate diagnostics
Message-ID: <6.1.2.0.2.20050113111547.01b816e0@math.ohiou.edu>

Yulei,

There are tests for multivariate normality and for equal multivariate 
distributions in the energy package. The reference mentioned in the help 
file for the multivariate normality test is in JMVA March 2005 Vol. 93(1), 
58-80.

Some other tests for multivariate normality include Mardia's multivariate 
skewness and kurtosis tests, and the BHEP tests based on the empirical 
characteristic function. I have R functions (not in an R package) for these 
tests if you are interested.

Maria Rizzo



>Message: 47
>Date: Wed, 12 Jan 2005 21:42:58 -0500 (EST)
>From: Yulei He <yuleih at umich.edu>
>Subject: [R] multivariate diagnostics
>To: r-help at stat.math.ethz.ch
>Message-ID:
>         <Pine.SOL.4.58.0501122139570.810 at tetris.gpcc.itd.umich.edu>
>Content-Type: TEXT/PLAIN; charset=US-ASCII
>
>Hi, there.
>
>I have two questions about the diagnostics in multivarite statistics.
>
>1. Is there any diagnostics tool to check if a multivariate sample is from
>multivariate normal distribution? If there is one, is there any function
>doing it in R?
>
>2. Is there any function of testing if two multivariate distribution are
>same, i.e. the multivariate extension of Kolomogrov-Smirnov test?
>
>Thanks for your help.
>
>Yulei
>
>
>$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
>Yulei He
>1586 Murfin Ave. Apt 37
>Ann Arbor, MI 48105-3135
>yuleih at umich.edu
>734-647-0305(H)
>734-763-0421(O)
>734-763-0427(O)
>734-764-8263(fax)
>$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$



From MSchwartz at MedAnalytics.com  Thu Jan 13 19:01:45 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 13 Jan 2005 12:01:45 -0600
Subject: [R] Space between bars in barplot
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E89B3B@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E95E89B3B@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <1105639305.9401.48.camel@horizons.localdomain>

On Thu, 2005-01-13 at 16:43 +0000, michael watson (IAH-C) wrote:
> Hi
> 
> I am trying to understand the "space" argument to barplot() and I think
> it is not working as stated.  The docs say:
> 
>   space: the amount of space (as a fraction of the average bar width)
>           left before each bar.
> 
> Which means that I can pass a vector, the same length as the no. of
> bars, and the nth element of that vector will be the space left before
> the nth bar.  This is always true except for the FIRST bar.  Eg:
> 
> barplot(c(1,2,3,4),space=c(1,1,1,1))  # equally spaced bars as expected
> 
> barplot(c(1,2,3,4),space=c(1,20,1,1))  # massive gap before the 2nd bar
> 
> barplot(c(1,2,3,4),space=c(20,1,1,1))  # the same as the first plot
> 
> I'm guessing this is going to be something to do with par().... any idea
> how I can adjust the space between the x-axis and the first bar?


Upon a quick review, it looks like the calculation in question here is
at line 62 in barplot.R:

   w.l <- w.m - delta

In this case, 'w.m' contains the calculated bar midpoints, adjusted for
the 'space' and 'width' arguments (using your third example):

   # width is 1 by default

   delta <- width / 2                # equals 0.5 0.5 0.5 0.5
   w.r <- cumsum(space + width)      # equals 21 23 25 27
   w.m <- w.r - delta                # equals 20.5 22.5 24.5 26.5
   w.l <- w.m - delta                # equals 20 22 24 26

The 'xlim' for the plot is then set to:

  if (missing(xlim)) xlim <- c(min(w.l), max(w.r))

In the case of your third example, xlim is set to:

   20 27

and the bar midpoints are in wm, which means that bar 1 (mp = 20.5) runs
from 20 - 21. Since the left side of the x axis is set to 20, it does
not leave the defined space (20) to the left of the bar. Some space is
there as a result of the adjustment to the axis ranges by default (see
the help for par("xaxs").

If the calculation in line 62 were set to:

  w.l <- w.m - delta - space[1]

that would do the trick here, but I have not considered the impact of
that change elsewhere in the code, before considering recommending that
it get modified by R Core.

The alternative, would be to simply change the language in the help for
barplot() to explicitly indicate that the 'space' does not apply to the
first bar:

  space: the amount of space (as a fraction of the average bar width)
         left before each bar, with the exception of the first bar.
         Thus, space[1] has no effect if given as a vector.


As a temporary fix, you could explicitly specify the xlim argument:

  barplot(1:4, space=c(20, 1, 1, 1), xlim = c(0, 27))

Now, of course, I calculated the min,max values from the above code. You
could replicate the same calculations prior to calling barplot() and
then adjust xlim as you require to use with your actual data.

HTH,

Marc Schwartz



From Achim.Zeileis at wu-wien.ac.at  Thu Jan 13 19:39:08 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 13 Jan 2005 19:39:08 +0100
Subject: [R] chisq.test() as a goodness of fit test
In-Reply-To: <20050113172337.35780.qmail@web41215.mail.yahoo.com>
References: <20050113172337.35780.qmail@web41215.mail.yahoo.com>
Message-ID: <20050113193908.0be3594b.Achim.Zeileis@wu-wien.ac.at>

On Thu, 13 Jan 2005 18:23:37 +0100 (CET) Vito Ricci wrote:

> Dear R-Users,
> 
> How can I use chisq.test() as a goodness of fit test?
> Reading man-page I?ve some doubts that kind of test is
> available with this statement. Am I wrong?
> 
> X2=sum((O-E)^2)/E)
> 
> O=empirical frequencies
> E=expected freq.

You can do
  chisq.test(O, p = E/sum(E))
but note that this assumes that the expected frequencies/probabilities
are known (and not estimated).

> calculated with the model (such as normal distribution)

"Normal distribution" is not a fully specified model! If you estimate
the parameters by ML, the inference will typically not be valid. Another
approach would be to estimate the parameters by grouped ML or minimum
Chi-squared instead. See also ?pearson.test from package nortest and the
references therein.
For discrete distributions, this Chi-squared statistic is more natural
(though not always without problems): see ?goodfit in package vcd.
Z


> See:
> http://www.itl.nist.gov/div898/handbook/eda/section3/eda35f.htm
> for X2 used as a goodness of fit test.
> 
> Any help will be appreciated.
> Thank a lot. Bye.
> Vito
> 
> 
> =====
> Diventare costruttori di soluzioni
> Became solutions' constructors
> 
> "The business of the statistician is to catalyze 
> the scientific learning process."  
> George E. P. Box
> 
> Top 10 reasons to become a Statistician
> 
>      1. Deviation is considered normal
>      2. We feel complete and sufficient
>      3. We are 'mean' lovers
>      4. Statisticians do it discretely and continuously
>      5. We are right 95% of the time
>      6. We can legally comment on someone's posterior distribution
>      7. We may not be normal, but we are transformable
>      8. We never have to say we are certain
>      9. We are honestly significantly different
>     10. No one wants our jobs
> 
> 
> Visitate il portale http://www.modugno.it/
> e in particolare la sezione su Palese 
> http://www.modugno.it/archivio/palese/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From roger.bos at gmail.com  Thu Jan 13 19:44:58 2005
From: roger.bos at gmail.com (roger bos)
Date: Thu, 13 Jan 2005 13:44:58 -0500
Subject: [R] how to use solve.QP
Message-ID: <1db7268005011310442d6f4572@mail.gmail.com>

At the risk of ridicule for my deficient linear algebra skills, I ask
for help using the solve.QP function to do portfolio optimization.  I
am trying to following a textbook example and need help converting the
problem into the format required by solve.QP.  Below is my sample code
if anyone is willing to go through it.  This problem will not solve
because it is not set up properly.  I hope I included enough details
for someone to deciper it.  Or does anyone have a good example they
can send me?

Thanks so much for any hints and suggestions, Roger.



library(quadprog, lib.loc="C:\\Program Files\\R\\tools")
library(MASS, lib.loc="C:\\Program Files\\R\\tools")
n<-100
m<-200
rho<-0.7
sigma<-0.2
mu<-0.1
Cov <- matrix(rho*sigma*sigma, ncol=n, nrow=n)
diag(Cov) <- rep(sigma*sigma, n)
S <- 1+matrix(mvrnorm(m, rep(mu, n), Sigma=Cov), ncol=n)

#The problem is formulated as minimize t(b) Cov b
#subject to cLo <= A <= cUp
#and bLo=0 <= w <= 1=bUp

Cov <- var(S)
mu <- apply(S, 2, mean)
mu.target <- 0.1
#subject to cLo <= A <= cUp and bLo=0 <= b <= 1=bUp
A <- rbind(1,mu)
cLo <- c(1, mu.target)
cUp <- c(1, Inf)
bLo <- rep(0, n)
bUp <- rep(1, n)

#I convert [cLo <= A <= cUp] to Amat >= bvec and [bLo=0 <= w #<=1=bUp] to
Amat <- rbind(-1, 1, -mu, mu)
dim(bLo) <- c(n,1)
dim(bUp) <- c(n,1)
bvec <- rbind(-1, 1, mu.target, Inf, bLo, -bUp)
zMat <- matrix(rep(0,2*n*n),ncol=n, nrow=n*2)
zMat[,1] <- c(rep(1,n), rep(-1,n))
Amat <- t(rbind(Amat, zMat))

#So I set Dmat=Cov and set dvec=0
Dmat=Cov
dvec=rep(0, nrow(Amat))

#The first two rows of Amat should be equality constraints (so weights sum to 1)
meq <- 2
					
sol <- solve.QP(Dmat=Dmat, dvec=dvec, Amat=Amat, bvec=bvec, meq)
sol



From Ted.Harding at nessie.mcc.ac.uk  Thu Jan 13 19:30:58 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 13 Jan 2005 18:30:58 -0000 (GMT)
Subject: [R] chisq.test() as a goodness of fit test
In-Reply-To: <20050113172337.35780.qmail@web41215.mail.yahoo.com>
Message-ID: <XFMail.050113183058.Ted.Harding@nessie.mcc.ac.uk>

On 13-Jan-05 Vito Ricci wrote:
> Dear R-Users,
> 
> How can I use chisq.test() as a goodness of fit test?
> Reading man-page I've some doubts that kind of test is
> available with this statement. Am I wrong?
> 
> 
> X2=sum((O-E)^2)/E)
> 
> O=empirical frequencies
> E=expected freq. calculated with the model (such as
> normal distribution)
> 
> See:
> http://www.itl.nist.gov/div898/handbook/eda/section3/eda35f.htm
> for X2 used as a goodness of fit test.

It is not conspicuous in "?chisqu.test", though in fact it is
the case, that chisq.test() could perform the sort of test you
are looking for. No doubt this is a result of so much space
devoted to the contingency table case.

However, if you use it in the form

  chisq.test(x,p)

where x is a vector of counts in "bins" and p is a vector,
of the same length as x, of the probabilities that a random
observation will fall in the various bins, then it is that
sort of test.

So, for example, if you dissect the range of X into k intervals
[,X1], (X1,X2], ... , (X[k-2],X[k-1]], (X[k-1],],
let N1, N2, ... , Nk be the numbers of observations in these
intervals,
let

  x = c(N1,...,Nk)

  p = c(pnorm(X1),
        pnorm(c(X2,...,X[k-1])-pnorm(c(X1,...,X[k-2]),
        1-pnorm(X[k-1]) )

then

  chisq.test(x,p)

will test the goodness of fit of the normal distribution.
(Note that the above is schematic pseudo-R code, not real
R code!)

However, this use of chisq.test(x,p) is limited (as far
as I can see) to the case where no parameters have been
estimated in choosing the distribution from which p is
calculated, and so will be based on the wrong number
of degrees of freedom if the distribution is estimated
from the data. I cannot see any provision for specifying
either the degrees of freedom, or the number of parameters
estimated for p, in the documentation for chisq.test().

So in the latter case you are better off doing it directly.
This is not more difficult, since the hard work is in
calculating the elements of p. After that, with E=N*p,

  X2 <- sum(((O-E)^2)/E)

has the chi-squared distribution with df=(k-r) d.f. where
k is the number of "bins" and r is the number of parameters
that have been estimated. So get 1-pchisq(X2,df).

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 13-Jan-05                                       Time: 18:30:58
------------------------------ XFMail ------------------------------



From Achim.Zeileis at wu-wien.ac.at  Thu Jan 13 19:58:33 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 13 Jan 2005 19:58:33 +0100
Subject: [R] how to use solve.QP
In-Reply-To: <1db7268005011310442d6f4572@mail.gmail.com>
References: <1db7268005011310442d6f4572@mail.gmail.com>
Message-ID: <20050113195833.6d8e3be7.Achim.Zeileis@wu-wien.ac.at>

On Thu, 13 Jan 2005 13:44:58 -0500 roger bos wrote:

> At the risk of ridicule for my deficient linear algebra skills, I ask
> for help using the solve.QP function to do portfolio optimization.  I
> am trying to following a textbook example and need help converting the
> problem into the format required by solve.QP.  Below is my sample code
> if anyone is willing to go through it.  This problem will not solve
> because it is not set up properly.  I hope I included enough details
> for someone to deciper it.  Or does anyone have a good example they
> can send me?

You can look at the man page, code and example of the function
portfolio.optim() in package tseries which does portfolio optimization
based on solve.QP from quadprog.

hth,
Z

> Thanks so much for any hints and suggestions, Roger.
> 
> 
> 
> library(quadprog, lib.loc="C:\\Program Files\\R\\tools")
> library(MASS, lib.loc="C:\\Program Files\\R\\tools")
> n<-100
> m<-200
> rho<-0.7
> sigma<-0.2
> mu<-0.1
> Cov <- matrix(rho*sigma*sigma, ncol=n, nrow=n)
> diag(Cov) <- rep(sigma*sigma, n)
> S <- 1+matrix(mvrnorm(m, rep(mu, n), Sigma=Cov), ncol=n)
> 
> #The problem is formulated as minimize t(b) Cov b
> #subject to cLo <= A <= cUp
> #and bLo=0 <= w <= 1=bUp
> 
> Cov <- var(S)
> mu <- apply(S, 2, mean)
> mu.target <- 0.1
> #subject to cLo <= A <= cUp and bLo=0 <= b <= 1=bUp
> A <- rbind(1,mu)
> cLo <- c(1, mu.target)
> cUp <- c(1, Inf)
> bLo <- rep(0, n)
> bUp <- rep(1, n)
> 
> #I convert [cLo <= A <= cUp] to Amat >= bvec and [bLo=0 <= w #<=1=bUp]
> #to
> Amat <- rbind(-1, 1, -mu, mu)
> dim(bLo) <- c(n,1)
> dim(bUp) <- c(n,1)
> bvec <- rbind(-1, 1, mu.target, Inf, bLo, -bUp)
> zMat <- matrix(rep(0,2*n*n),ncol=n, nrow=n*2)
> zMat[,1] <- c(rep(1,n), rep(-1,n))
> Amat <- t(rbind(Amat, zMat))
> 
> #So I set Dmat=Cov and set dvec=0
> Dmat=Cov
> dvec=rep(0, nrow(Amat))
> 
> #The first two rows of Amat should be equality constraints (so weights
> #sum to 1)
> meq <- 2
> 					
> sol <- solve.QP(Dmat=Dmat, dvec=dvec, Amat=Amat, bvec=bvec, meq)
> sol
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From helprhelp at yahoo.com  Thu Jan 13 20:00:21 2005
From: helprhelp at yahoo.com (Weiwei Shi)
Date: Thu, 13 Jan 2005 11:00:21 -0800 (PST)
Subject: [R] load object
Message-ID: <20050113190021.3312.qmail@web61304.mail.yahoo.com>

Hi,
I happen to re-write my codes to save memory and my
approach is write my obj into file first and later I
load it.

However, it seems like:
load(filename) can load the object but the function
returns the name of the object instead of the
reference to it. For example, I have an object called
r0.prune, which is saved by
save(r0.prune, file='r0.prune')

and later, I want to load it by using:
load('r0.prune')
but I need to put the reference to the object r0.prune
into a var or a list. I tried:
t<-load('r0.prune'),
and class(t) gave me a char, which means t stores the
name of obj instead of the obj itself.

Sorry for the dumb question but please help...

Weiwei



From roger.bos at gmail.com  Thu Jan 13 20:07:29 2005
From: roger.bos at gmail.com (roger bos)
Date: Thu, 13 Jan 2005 14:07:29 -0500
Subject: [R] how to use solve.QP
In-Reply-To: <20050113195833.6d8e3be7.Achim.Zeileis@wu-wien.ac.at>
References: <1db7268005011310442d6f4572@mail.gmail.com>
	<20050113195833.6d8e3be7.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <1db7268005011311075f31396c@mail.gmail.com>

Zeileis,

Thanks, I didn't know about "portfolio.optim".  I wanted to see how it
works, but when I try showMethods, it doesn't show it to me.  Does
that mean I am not allowed to see the inner workings?

Thanks,

Roger

> showMethods("portfolio.optim")

Function "portfolio.optim":
<not a generic function>
> 


On Thu, 13 Jan 2005 19:58:33 +0100, Achim Zeileis
<Achim.Zeileis at wu-wien.ac.at> wrote:
> On Thu, 13 Jan 2005 13:44:58 -0500 roger bos wrote:
> 
> > At the risk of ridicule for my deficient linear algebra skills, I ask
> > for help using the solve.QP function to do portfolio optimization.  I
> > am trying to following a textbook example and need help converting the
> > problem into the format required by solve.QP.  Below is my sample code
> > if anyone is willing to go through it.  This problem will not solve
> > because it is not set up properly.  I hope I included enough details
> > for someone to deciper it.  Or does anyone have a good example they
> > can send me?
> 
> You can look at the man page, code and example of the function
> portfolio.optim() in package tseries which does portfolio optimization
> based on solve.QP from quadprog.
> 
> hth,
> Z
> 
> > Thanks so much for any hints and suggestions, Roger.
> >
> >
> >
> > library(quadprog, lib.loc="C:\\Program Files\\R\\tools")
> > library(MASS, lib.loc="C:\\Program Files\\R\\tools")
> > n<-100
> > m<-200
> > rho<-0.7
> > sigma<-0.2
> > mu<-0.1
> > Cov <- matrix(rho*sigma*sigma, ncol=n, nrow=n)
> > diag(Cov) <- rep(sigma*sigma, n)
> > S <- 1+matrix(mvrnorm(m, rep(mu, n), Sigma=Cov), ncol=n)
> >
> > #The problem is formulated as minimize t(b) Cov b
> > #subject to cLo <= A <= cUp
> > #and bLo=0 <= w <= 1=bUp
> >
> > Cov <- var(S)
> > mu <- apply(S, 2, mean)
> > mu.target <- 0.1
> > #subject to cLo <= A <= cUp and bLo=0 <= b <= 1=bUp
> > A <- rbind(1,mu)
> > cLo <- c(1, mu.target)
> > cUp <- c(1, Inf)
> > bLo <- rep(0, n)
> > bUp <- rep(1, n)
> >
> > #I convert [cLo <= A <= cUp] to Amat >= bvec and [bLo=0 <= w #<=1=bUp]
> > #to
> > Amat <- rbind(-1, 1, -mu, mu)
> > dim(bLo) <- c(n,1)
> > dim(bUp) <- c(n,1)
> > bvec <- rbind(-1, 1, mu.target, Inf, bLo, -bUp)
> > zMat <- matrix(rep(0,2*n*n),ncol=n, nrow=n*2)
> > zMat[,1] <- c(rep(1,n), rep(-1,n))
> > Amat <- t(rbind(Amat, zMat))
> >
> > #So I set Dmat=Cov and set dvec=0
> > Dmat=Cov
> > dvec=rep(0, nrow(Amat))
> >
> > #The first two rows of Amat should be equality constraints (so weights
> > #sum to 1)
> > meq <- 2
> >
> > sol <- solve.QP(Dmat=Dmat, dvec=dvec, Amat=Amat, bvec=bvec, meq)
> > sol
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>



From scottb at uci.edu  Thu Jan 13 20:14:46 2005
From: scottb at uci.edu (Scott Brown)
Date: Thu, 13 Jan 2005 11:14:46 -0800
Subject: [R] Optim simplex start size problem
Message-ID: <41E6C8A6.2080004@uci.edu>

Hi R Users,

I've struck a problem with 'optim', which has previously worked very 
well for me.  I cannot find a way to set the size of starting simplex 
when using the Nelder Mead method.  The "parscale" argument will control 
the relative sizes of sides, but not the global magnitude.

A short example:

f<-function(x){print(x);sum(sin(x))}  # A minimum at (0,0).
optim(par=c(1,1),fn=f,method="Nel",control=list(maxit=10,parscale=c(1,1))

That makes the initial simplex look like this:
1,   1
1.1, 1
1,   1.1

Now say I want to use a larger intial simplex, mabye:
1,   1
2,   1
1,   2

The documentation suggests that parscale=c(10,10) should do it.  But if 
I do that, I get the same simplex as before.

Only the *relative* sizes of parscale seem to matter (I can make one 
side longer than the others).

Does anyone have any advice for me?

Scott Brown.



From pbruce at statistics.com  Thu Jan 13 20:15:52 2005
From: pbruce at statistics.com (Peter C. Bruce)
Date: Thu, 13 Jan 2005 14:15:52 -0500
Subject: [R] Online course: DNA Microarray Data Analysis starts Jan. 28
Message-ID: <6.1.0.6.2.20050113141514.053ea250@mail.statistics.com>


Prof. Javier Cabrera will be giving the online course "DNA Microarray Data 
Analysis" from Jan. 28 - Feb. 25 at statistics.com.  Dr. Cabrera is 
co-author of "Exploration and Analysis of DNA Microarray and Protein Array 
Data" (the course text; Wiley) and has published a number of articles on 
gene expression data analysis, data mining and multivariate methods in 
leading statistics journals. This online course covers data preprocessing, 
replication, data reduction, distance measures, clustering, supervised and 
unsupervised classification of genes, k-nearest neighbors, neural nets, 
software, more.  R is used.  Participants will have access to a private 
discussion board with the instructor, weekly lessons, exercises and 
solutions.  There are no set hours - participate on a schedule that is 
convenient for you.  Estimated time required is 10+ hours per 
week.  Details and registration at 
http://www.statistics.com/content/courses/microarray/index.html

Peter Bruce
statistics.com



From Achim.Zeileis at wu-wien.ac.at  Thu Jan 13 20:19:35 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 13 Jan 2005 20:19:35 +0100
Subject: [R] how to use solve.QP
In-Reply-To: <1db7268005011311075f31396c@mail.gmail.com>
References: <1db7268005011310442d6f4572@mail.gmail.com>
	<20050113195833.6d8e3be7.Achim.Zeileis@wu-wien.ac.at>
	<1db7268005011311075f31396c@mail.gmail.com>
Message-ID: <20050113201935.7f028e7e.Achim.Zeileis@wu-wien.ac.at>

On Thu, 13 Jan 2005 14:07:29 -0500 roger bos wrote:

> Zeileis,
> 
> Thanks, I didn't know about "portfolio.optim".  I wanted to see how it
> works, but when I try showMethods, it doesn't show it to me.  Does
> that mean I am not allowed to see the inner workings?

1. All of this is open source, so you are *always* allowed to look
   at the sources. (If you haven't got a version of the source package,
   then you can always get it from CRAN.)
2. showMethods() is for S4 generics, portfolio.optim is an S3 generic.
3. Simply typing portfolio.optim.default at the prompt should print the
   function.
Z

> Thanks,
> 
> Roger
> 
> > showMethods("portfolio.optim")
> 
> Function "portfolio.optim":
> <not a generic function>
> > 
> 
> 
> On Thu, 13 Jan 2005 19:58:33 +0100, Achim Zeileis
> <Achim.Zeileis at wu-wien.ac.at> wrote:
> > On Thu, 13 Jan 2005 13:44:58 -0500 roger bos wrote:
> > 
> > > At the risk of ridicule for my deficient linear algebra skills, I
> > > ask for help using the solve.QP function to do portfolio
> > > optimization.  I am trying to following a textbook example and
> > > need help converting the problem into the format required by
> > > solve.QP.  Below is my sample code if anyone is willing to go
> > > through it.  This problem will not solve because it is not set up
> > > properly.  I hope I included enough details for someone to deciper
> > > it.  Or does anyone have a good example they can send me?
> > 
> > You can look at the man page, code and example of the function
> > portfolio.optim() in package tseries which does portfolio
> > optimization based on solve.QP from quadprog.
> > 
> > hth,
> > Z
> > 
> > > Thanks so much for any hints and suggestions, Roger.
> > >
> > >
> > >
> > > library(quadprog, lib.loc="C:\\Program Files\\R\\tools")
> > > library(MASS, lib.loc="C:\\Program Files\\R\\tools")
> > > n<-100
> > > m<-200
> > > rho<-0.7
> > > sigma<-0.2
> > > mu<-0.1
> > > Cov <- matrix(rho*sigma*sigma, ncol=n, nrow=n)
> > > diag(Cov) <- rep(sigma*sigma, n)
> > > S <- 1+matrix(mvrnorm(m, rep(mu, n), Sigma=Cov), ncol=n)
> > >
> > > #The problem is formulated as minimize t(b) Cov b
> > > #subject to cLo <= A <= cUp
> > > #and bLo=0 <= w <= 1=bUp
> > >
> > > Cov <- var(S)
> > > mu <- apply(S, 2, mean)
> > > mu.target <- 0.1
> > > #subject to cLo <= A <= cUp and bLo=0 <= b <= 1=bUp
> > > A <- rbind(1,mu)
> > > cLo <- c(1, mu.target)
> > > cUp <- c(1, Inf)
> > > bLo <- rep(0, n)
> > > bUp <- rep(1, n)
> > >
> > > #I convert [cLo <= A <= cUp] to Amat >= bvec and [bLo=0 <= w
> > > ##<=1=bUp] to
> > > Amat <- rbind(-1, 1, -mu, mu)
> > > dim(bLo) <- c(n,1)
> > > dim(bUp) <- c(n,1)
> > > bvec <- rbind(-1, 1, mu.target, Inf, bLo, -bUp)
> > > zMat <- matrix(rep(0,2*n*n),ncol=n, nrow=n*2)
> > > zMat[,1] <- c(rep(1,n), rep(-1,n))
> > > Amat <- t(rbind(Amat, zMat))
> > >
> > > #So I set Dmat=Cov and set dvec=0
> > > Dmat=Cov
> > > dvec=rep(0, nrow(Amat))
> > >
> > > #The first two rows of Amat should be equality constraints (so
> > > #weights sum to 1)
> > > meq <- 2
> > >
> > > sol <- solve.QP(Dmat=Dmat, dvec=dvec, Amat=Amat, bvec=bvec, meq)
> > > sol
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> >
>



From bates at wisc.edu  Thu Jan 13 20:27:18 2005
From: bates at wisc.edu (Douglas Bates)
Date: Thu, 13 Jan 2005 13:27:18 -0600
Subject: [R] load object
In-Reply-To: <20050113190021.3312.qmail@web61304.mail.yahoo.com>
References: <20050113190021.3312.qmail@web61304.mail.yahoo.com>
Message-ID: <41E6CB96.4090306@wisc.edu>

Weiwei Shi wrote:
> Hi,
> I happen to re-write my codes to save memory and my
> approach is write my obj into file first and later I
> load it.
> 
> However, it seems like:
> load(filename) can load the object but the function
> returns the name of the object instead of the
> reference to it. For example, I have an object called
> r0.prune, which is saved by
> save(r0.prune, file='r0.prune')
> 
> and later, I want to load it by using:
> load('r0.prune')
> but I need to put the reference to the object r0.prune
> into a var or a list. I tried:
> t<-load('r0.prune'),
> and class(t) gave me a char, which means t stores the
> name of obj instead of the obj itself.
> 
> Sorry for the dumb question but please help...
> 
> Weiwei

I was going to suggest that you read the help page for load but when I 
looked at it myself I found that it was not obvious what the effect of 
calling load at the prompt is.  The help page is accurate but it is a 
bit confusing if you don't know what the default environment is.

Anyway, when called from the prompt, load has the side effect of loading 
the object into the global environment.  Try

save(r0.prune, file='r0.prune')
rm(r0.prune)
find(r0.prune)  # should produce an error message
load('r0.prune')
find(r0.prune)  # should show the object in the global environment
str(r0.prune)   # etc.



From qu at oakland.edu  Thu Jan 13 20:31:01 2005
From: qu at oakland.edu (qu@oakland.edu)
Date: Thu, 13 Jan 2005 14:31:01 -0500
Subject: [R] Balanced Arrays!
Message-ID: <e2c4e8b5.be27647a.8252d00@mcfeely.acs.oakland.edu>

Hi,
Does anyone know whether there is an R program constructing 
optimal balanced arrays in experimental design? 
Thanks  a lot!
Harvey.
Xianggui (Harvey) Qu
Assistant Professor in Statistics
Department of Mathematics & Statistics
Oakland University, Rochester, MI 48309
Tel. 248-370-4029 (O)



From MSchwartz at MedAnalytics.com  Thu Jan 13 20:34:45 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 13 Jan 2005 13:34:45 -0600
Subject: [R] load object
In-Reply-To: <20050113190021.3312.qmail@web61304.mail.yahoo.com>
References: <20050113190021.3312.qmail@web61304.mail.yahoo.com>
Message-ID: <1105644885.3527.20.camel@horizons.localdomain>

On Thu, 2005-01-13 at 11:00 -0800, Weiwei Shi wrote:
> Hi,
> I happen to re-write my codes to save memory and my
> approach is write my obj into file first and later I
> load it.
> 
> However, it seems like:
> load(filename) can load the object but the function
> returns the name of the object instead of the
> reference to it. For example, I have an object called
> r0.prune, which is saved by
> save(r0.prune, file='r0.prune')
> 
> and later, I want to load it by using:
> load('r0.prune')
> but I need to put the reference to the object r0.prune
> into a var or a list. I tried:
> t<-load('r0.prune'),
> and class(t) gave me a char, which means t stores the
> name of obj instead of the obj itself.
> 
> Sorry for the dumb question but please help...


Does the following help?


# create the object
> r0.prune <- 1:10

# display the object
> r0.prune
 [1]  1  2  3  4  5  6  7  8  9 10

# save the object
> save(r0.prune, file='r0.prune')

# show the object is present
> ls()
[1] "r0.prune"

# remove the object
> rm(r0.prune)

# show that the object is gone
> ls()
character(0)

# reload the object into the current workspace
> load('r0.prune')

# show the object is back
> ls()
[1] "r0.prune"

# display the object
> r0.prune
 [1]  1  2  3  4  5  6  7  8  9 10


# now remove the object again
> rm(r0.prune)

# It's gone
> ls()
character(0)

# now use:
> t <- load('r0.prune')

# See what is now present
> ls()
[1] "r0.prune" "t"

> t
[1] "r0.prune"

> r0.prune
 [1]  1  2  3  4  5  6  7  8  9 10


't' returns the _name(s)_ of the loaded object(s) as a character vector,
just as documented.

The object itself is available in the workspace. You can use 'r0.prune'
just as per normal routine:

> mean(r0.prune)
[1] 5.5

> MyList <- list(r0.prune)

> MyList
[[1]]
 [1]  1  2  3  4  5  6  7  8  9 10


HTH,

Marc Schwartz



From spencer.graves at pdf.com  Thu Jan 13 20:43:46 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 13 Jan 2005 11:43:46 -0800
Subject: [R] how to use solve.QP
In-Reply-To: <20050113201935.7f028e7e.Achim.Zeileis@wu-wien.ac.at>
References: <1db7268005011310442d6f4572@mail.gmail.com>	<20050113195833.6d8e3be7.Achim.Zeileis@wu-wien.ac.at>	<1db7268005011311075f31396c@mail.gmail.com>
	<20050113201935.7f028e7e.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <41E6CF72.9060804@pdf.com>

      Also, ' methods("portfolio.optim")' revealed 2 functions for this 
generic: 

[1] portfolio.optim.default portfolio.optim.ts    

      Typing "portfolio.optim.ts" exposes the code for the second one.  
If the class of the first argument "x" is "ts", R dispatches 
"portfiolio.optim(x, ...)" to "portfolio.optim.ts(x, ...)".  Otherwise, 
it is dispatched to "portfolio.optim.default(x,...)". 

      hope this helps. 
      spencer graves

Achim Zeileis wrote:

>On Thu, 13 Jan 2005 14:07:29 -0500 roger bos wrote:
>
>  
>
>>Zeileis,
>>
>>Thanks, I didn't know about "portfolio.optim".  I wanted to see how it
>>works, but when I try showMethods, it doesn't show it to me.  Does
>>that mean I am not allowed to see the inner workings?
>>    
>>
>
>1. All of this is open source, so you are *always* allowed to look
>   at the sources. (If you haven't got a version of the source package,
>   then you can always get it from CRAN.)
>2. showMethods() is for S4 generics, portfolio.optim is an S3 generic.
>3. Simply typing portfolio.optim.default at the prompt should print the
>   function.
>Z
>
>  
>
>>Thanks,
>>
>>Roger
>>
>>    
>>
>>>showMethods("portfolio.optim")
>>>      
>>>
>>Function "portfolio.optim":
>><not a generic function>
>>    
>>
>>On Thu, 13 Jan 2005 19:58:33 +0100, Achim Zeileis
>><Achim.Zeileis at wu-wien.ac.at> wrote:
>>    
>>
>>>On Thu, 13 Jan 2005 13:44:58 -0500 roger bos wrote:
>>>
>>>      
>>>
>>>>At the risk of ridicule for my deficient linear algebra skills, I
>>>>ask for help using the solve.QP function to do portfolio
>>>>optimization.  I am trying to following a textbook example and
>>>>need help converting the problem into the format required by
>>>>solve.QP.  Below is my sample code if anyone is willing to go
>>>>through it.  This problem will not solve because it is not set up
>>>>properly.  I hope I included enough details for someone to deciper
>>>>it.  Or does anyone have a good example they can send me?
>>>>        
>>>>
>>>You can look at the man page, code and example of the function
>>>portfolio.optim() in package tseries which does portfolio
>>>optimization based on solve.QP from quadprog.
>>>
>>>hth,
>>>Z
>>>
>>>      
>>>
>>>>Thanks so much for any hints and suggestions, Roger.
>>>>
>>>>
>>>>
>>>>library(quadprog, lib.loc="C:\\Program Files\\R\\tools")
>>>>library(MASS, lib.loc="C:\\Program Files\\R\\tools")
>>>>n<-100
>>>>m<-200
>>>>rho<-0.7
>>>>sigma<-0.2
>>>>mu<-0.1
>>>>Cov <- matrix(rho*sigma*sigma, ncol=n, nrow=n)
>>>>diag(Cov) <- rep(sigma*sigma, n)
>>>>S <- 1+matrix(mvrnorm(m, rep(mu, n), Sigma=Cov), ncol=n)
>>>>
>>>>#The problem is formulated as minimize t(b) Cov b
>>>>#subject to cLo <= A <= cUp
>>>>#and bLo=0 <= w <= 1=bUp
>>>>
>>>>Cov <- var(S)
>>>>mu <- apply(S, 2, mean)
>>>>mu.target <- 0.1
>>>>#subject to cLo <= A <= cUp and bLo=0 <= b <= 1=bUp
>>>>A <- rbind(1,mu)
>>>>cLo <- c(1, mu.target)
>>>>cUp <- c(1, Inf)
>>>>bLo <- rep(0, n)
>>>>bUp <- rep(1, n)
>>>>
>>>>#I convert [cLo <= A <= cUp] to Amat >= bvec and [bLo=0 <= w
>>>>##<=1=bUp] to
>>>>Amat <- rbind(-1, 1, -mu, mu)
>>>>dim(bLo) <- c(n,1)
>>>>dim(bUp) <- c(n,1)
>>>>bvec <- rbind(-1, 1, mu.target, Inf, bLo, -bUp)
>>>>zMat <- matrix(rep(0,2*n*n),ncol=n, nrow=n*2)
>>>>zMat[,1] <- c(rep(1,n), rep(-1,n))
>>>>Amat <- t(rbind(Amat, zMat))
>>>>
>>>>#So I set Dmat=Cov and set dvec=0
>>>>Dmat=Cov
>>>>dvec=rep(0, nrow(Amat))
>>>>
>>>>#The first two rows of Amat should be equality constraints (so
>>>>#weights sum to 1)
>>>>meq <- 2
>>>>
>>>>sol <- solve.QP(Dmat=Dmat, dvec=dvec, Amat=Amat, bvec=bvec, meq)
>>>>sol
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide!
>>>>http://www.R-project.org/posting-guide.html
>>>>
>>>>        
>>>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From ray at mcs.vuw.ac.nz  Thu Jan 13 20:57:13 2005
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Fri, 14 Jan 2005 08:57:13 +1300 (NZDT)
Subject: [R] load object
Message-ID: <200501131957.j0DJvD2B006240@tahi.mcs.vuw.ac.nz>

> From: Douglas Bates <bates at wisc.edu> Fri Jan 14 08:35:33 2005
> 
> Weiwei Shi wrote:
> > Hi,
> > I happen to re-write my codes to save memory and my
> > approach is write my obj into file first and later I
> > load it.
> > 
> > However, it seems like:
> > load(filename) can load the object but the function
> > returns the name of the object instead of the
> > reference to it. For example, I have an object called
> > r0.prune, which is saved by
> > save(r0.prune, file='r0.prune')
> > 
> > and later, I want to load it by using:
> > load('r0.prune')
> > but I need to put the reference to the object r0.prune
> > into a var or a list. I tried:
> > t<-load('r0.prune'),
> > and class(t) gave me a char, which means t stores the
> > name of obj instead of the obj itself.
> > 
> > Sorry for the dumb question but please help...
> > 
> > Weiwei
> 
> I was going to suggest that you read the help page for load but when I 
> looked at it myself I found that it was not obvious what the effect of 
> calling load at the prompt is.  The help page is accurate but it is a 
> bit confusing if you don't know what the default environment is.
> 
> Anyway, when called from the prompt, load has the side effect of loading 
> the object into the global environment.  Try
> 
Well ?load does say:
Value:

     A character vector of the names of objects created, invisibly.

Note the plurals.  The point is that the file being loaded may contain
the definition of more than one R object.  You can say:
tt <- get(load('r0.prune'))	# t is not a good example name to use
which will do what you want, but:
1) load() has a side-effect of also creating r0.prune (in your case)
2) if the file 'r0.prune' contains more than one object, only the first
is assigned to tt (but all of them are loaded into memory with their
original names).

Hope this helps,
Ray Brownrigg



From helprhelp at yahoo.com  Thu Jan 13 21:09:55 2005
From: helprhelp at yahoo.com (Weiwei Shi)
Date: Thu, 13 Jan 2005 12:09:55 -0800 (PST)
Subject: [R] load object
In-Reply-To: <200501131957.j0DJvD2B006240@tahi.mcs.vuw.ac.nz>
Message-ID: <20050113200955.76516.qmail@web61301.mail.yahoo.com>

Hi,
thanks for everyone.
I think function "get" is what I wanted.
?get returns like:
Description:
Search for an R object with a given name and return
it.

Actually I need such reference to handle this obj.
Sorry if there is a misleading description in my
previous email.

weiwei

--- Ray Brownrigg <ray at mcs.vuw.ac.nz> wrote:

> > From: Douglas Bates <bates at wisc.edu> Fri Jan 14
> 08:35:33 2005
> > 
> > Weiwei Shi wrote:
> > > Hi,
> > > I happen to re-write my codes to save memory and
> my
> > > approach is write my obj into file first and
> later I
> > > load it.
> > > 
> > > However, it seems like:
> > > load(filename) can load the object but the
> function
> > > returns the name of the object instead of the
> > > reference to it. For example, I have an object
> called
> > > r0.prune, which is saved by
> > > save(r0.prune, file='r0.prune')
> > > 
> > > and later, I want to load it by using:
> > > load('r0.prune')
> > > but I need to put the reference to the object
> r0.prune
> > > into a var or a list. I tried:
> > > t<-load('r0.prune'),
> > > and class(t) gave me a char, which means t
> stores the
> > > name of obj instead of the obj itself.
> > > 
> > > Sorry for the dumb question but please help...
> > > 
> > > Weiwei
> > 
> > I was going to suggest that you read the help page
> for load but when I 
> > looked at it myself I found that it was not
> obvious what the effect of 
> > calling load at the prompt is.  The help page is
> accurate but it is a 
> > bit confusing if you don't know what the default
> environment is.
> > 
> > Anyway, when called from the prompt, load has the
> side effect of loading 
> > the object into the global environment.  Try
> > 
> Well ?load does say:
> Value:
> 
>      A character vector of the names of objects
> created, invisibly.
> 
> Note the plurals.  The point is that the file being
> loaded may contain
> the definition of more than one R object.  You can
> say:
> tt <- get(load('r0.prune'))	# t is not a good
> example name to use
> which will do what you want, but:
> 1) load() has a side-effect of also creating
> r0.prune (in your case)
> 2) if the file 'r0.prune' contains more than one
> object, only the first
> is assigned to tt (but all of them are loaded into
> memory with their
> original names).
> 
> Hope this helps,
> Ray Brownrigg
>



From p.dalgaard at biostat.ku.dk  Thu Jan 13 21:12:36 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Jan 2005 21:12:36 +0100
Subject: [R] chisq.test() as a goodness of fit test
In-Reply-To: <XFMail.050113183058.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050113183058.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x2llaxozpn.fsf@biostat.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> This is not more difficult, since the hard work is in
> calculating the elements of p. After that, with E=N*p,
> 
>   X2 <- sum(((O-E)^2)/E)
> 
> has the chi-squared distribution with df=(k-r) d.f. where
> k is the number of "bins" and r is the number of parameters
> that have been estimated. So get 1-pchisq(X2,df).

As Achim indicated, this only works if you estimate the parameters
from the binned data (and I suspect that you in principle need to have
decided the bins in advance too.) My old Stat-1 notes had a claim that
if you used the mean and variance of unbinned data to estimate the
normal distribution, then the X2 would be between chi-squares with
k-2 and k-1 d.f.

Incidentally, my .02 DKK is that you're more likely to want a test
against smoother alternative than the omnibus alternative implied by
the chi-square. For instance, if you have digit-preference effects in the
distribution (some weight measurements rounded to nearest half kg,
e.g.), it can throw a highly significant X2, but the deviation is of a
character that has little importance for the validity of subsequent
analyses. I haven't ever seen any of those for the case of estimated
parameters, though...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From martin.julien.2 at courrier.uqam.ca  Thu Jan 13 21:24:14 2005
From: martin.julien.2 at courrier.uqam.ca (Martin Julien)
Date: Thu, 13 Jan 2005 15:24:14 -0500
Subject: [R] Lme and centered variables
Message-ID: <200501132020.j0DKKDJq011713@intrant.uqam.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050113/a1931aaa/attachment.pl

From cberry at tajo.ucsd.edu  Thu Jan 13 21:35:57 2005
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Thu, 13 Jan 2005 12:35:57 -0800
Subject: [R] Nested ifelse - is there a better way?
Message-ID: <Pine.LNX.4.61.0501131200340.31375@tajo.ucsd.edu>


Jeff> I'm interested in finding a better way to add a column to a data 
Jeff> frame when calculating the new column requires more than one
Jeff> conditional.

[description of doing this with lots of ifelse's deleted]

Jeff

If all you are doing is slicing one quantitative variable into categories, 
then see

 	?cut

and

 	?findInterval

---------

If you need to use multiple variable to define the splits, you might 
construct an index using a 'mixed radix positional numbering system' (I 
suggest you google that term if it isn't familiar)

As an example if you have two variables (x and y,say) coded as (0,1,2) and 
(0,1,2,3,4), then

 	z <- x * ( max(y) + 1 ) + y

gives you z with categories (0,...,14). Extended to three or more 
variables, you need to do some nested multiplications.

Alternatively, you can use tapply to get this type of result

 	z <- tapply( x, list( x, y ) )

but here the categories will be numbered from 1. Some care may be needed 
to be sure that the category numbers are what you wanted.

----------

Subscripting can be used to combine categories

Here I combine 6 and 7, 8 and 9, and 14 and 15:

 	z.recode <- c(1,2,3,4,5,6,6,7,7,8,9,10,11,12,12)

 	z2 <- z.recode[ z ]

and do not miss out on

 	?levels

as "levels(x) <- value" will do the same when x is a factor.

HTH,

Chuck

Charles C. Berry                        (858) 534-2098
                                          Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	         UC San Diego
http://hacuna.ucsd.edu/members/ccb.html  La Jolla, San Diego 92093-0717



From White.Denis at epamail.epa.gov  Thu Jan 13 21:42:59 2005
From: White.Denis at epamail.epa.gov (White.Denis@epamail.epa.gov)
Date: Thu, 13 Jan 2005 12:42:59 -0800
Subject: [R] zero index and lazy evaluation in ifelse()
Message-ID: <OF86ADC055.B9ED24F8-ON88256F88.007183D7-88256F88.0071CCB7@epamail.epa.gov>

I don't understand this behavior:

> a <- c(0, 1, 2, 3)
> b <- c(1, 2, 3, 4)
> ifelse (a == 0, 0, b[a])
[1] 0 2 3 1

rather than the desired 0 1 2 3.  Thanks for any explanation.



From ray at mcs.vuw.ac.nz  Thu Jan 13 21:55:39 2005
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Fri, 14 Jan 2005 09:55:39 +1300 (NZDT)
Subject: [R] zero index and lazy evaluation in ifelse()
Message-ID: <200501132055.j0DKtdHS006534@tahi.mcs.vuw.ac.nz>

> Date: Thu, 13 Jan 2005 12:42:59 -0800
> From: White.Denis at epamail.epa.gov
> 
> I don't understand this behavior:
> 
> > a <- c(0, 1, 2, 3)
> > b <- c(1, 2, 3, 4)
> > ifelse (a == 0, 0, b[a])
> [1] 0 2 3 1
> 
> rather than the desired 0 1 2 3.  Thanks for any explanation.
> 
Look at b[a].

Ray



From das at cshl.edu  Thu Jan 13 21:59:46 2005
From: das at cshl.edu (Das, Rajdeep)
Date: Thu, 13 Jan 2005 15:59:46 -0500
Subject: [R] using created R objects in a program
Message-ID: <C8696843AE995F4EA4CDC3E2B83482A90A1B7D@mailbox02.cshl.edu>

Hi,

I am running a program in --vanilla mode. I would like to know how to load
saved functions in the program that it calls. EVerytime I call those objects
it returns error and complains that it cannot find the function object I am
calling. Please let me know. Thanks in advance.

Regards,

Raj



From jfox at mcmaster.ca  Thu Jan 13 21:59:57 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 13 Jan 2005 15:59:57 -0500
Subject: [R] zero index and lazy evaluation in ifelse()
In-Reply-To: <OF86ADC055.B9ED24F8-ON88256F88.007183D7-88256F88.0071CCB7@epamail.epa.gov>
Message-ID: <20050113210001.EJQD1899.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Denis,

b[a] gives you c(1, 2, 3) (try it), which is recycled as c(1, 2, 3, 1); the
elements in positions 2, 3, and 4 of this vector (i.e., where a != 0) are 2,
3, 1.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> White.Denis at epamail.epa.gov
> Sent: Thursday, January 13, 2005 3:43 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] zero index and lazy evaluation in ifelse()
> 
> I don't understand this behavior:
> 
> > a <- c(0, 1, 2, 3)
> > b <- c(1, 2, 3, 4)
> > ifelse (a == 0, 0, b[a])
> [1] 0 2 3 1
> 
> rather than the desired 0 1 2 3.  Thanks for any explanation.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From THOMAS.VOLSCHO at huskymail.uconn.edu  Thu Jan 13 22:15:49 2005
From: THOMAS.VOLSCHO at huskymail.uconn.edu (Thomas W Volscho)
Date: Thu, 13 Jan 2005 16:15:49 -0500
Subject: [R] Installing R on Xandros 3.0
Message-ID: <552b0d54ec84.54ec84552b0d@huskymail.uconn.edu>

Dear List,
After obtaining a second-hand PC and because XP costs too much, I installed Xandros 3.0 (based on Debian) but pretty easy to use if migrating from WinXP.

Does anyone know how to install R on this OS?

Thank you for your time,
Tom Volscho

************************************        
Thomas W. Volscho
Graduate Student
Dept. of Sociology U-2068
University of Connecticut
Storrs, CT 06269
Phone: (860) 486-3882
http://vm.uconn.edu/~twv00001



From p.dalgaard at biostat.ku.dk  Thu Jan 13 22:24:31 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Jan 2005 22:24:31 +0100
Subject: [R] zero index and lazy evaluation in ifelse()
In-Reply-To: <OF86ADC055.B9ED24F8-ON88256F88.007183D7-88256F88.0071CCB7@epamail.epa.gov>
References: <OF86ADC055.B9ED24F8-ON88256F88.007183D7-88256F88.0071CCB7@epamail.epa.gov>
Message-ID: <x2hdllowds.fsf@biostat.ku.dk>

White.Denis at epamail.epa.gov writes:

> I don't understand this behavior:
> 
> > a <- c(0, 1, 2, 3)
> > b <- c(1, 2, 3, 4)
> > ifelse (a == 0, 0, b[a])
> [1] 0 2 3 1
> 
> rather than the desired 0 1 2 3.  Thanks for any explanation.

b[a] is c(1,2,3), recycling to length 4 gives c(1,2,3,1), get it?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From rpeng at jhsph.edu  Thu Jan 13 22:27:14 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 13 Jan 2005 16:27:14 -0500
Subject: [R] Installing R on Xandros 3.0
In-Reply-To: <552b0d54ec84.54ec84552b0d@huskymail.uconn.edu>
References: <552b0d54ec84.54ec84552b0d@huskymail.uconn.edu>
Message-ID: <41E6E7B2.1030709@jhsph.edu>

I'm guessing this is a Linux based system.  Does

./configure
make
make install

work?

-roger

Thomas W Volscho wrote:
> Dear List,
> After obtaining a second-hand PC and because XP costs too much, I installed Xandros 3.0 (based on Debian) but pretty easy to use if migrating from WinXP.
> 
> Does anyone know how to install R on this OS?
> 
> Thank you for your time,
> Tom Volscho
> 
> ************************************        
> Thomas W. Volscho
> Graduate Student
> Dept. of Sociology U-2068
> University of Connecticut
> Storrs, CT 06269
> Phone: (860) 486-3882
> http://vm.uconn.edu/~twv00001
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From pbruce at statistics.com  Thu Jan 13 20:25:24 2005
From: pbruce at statistics.com (Peter C. Bruce)
Date: Thu, 13 Jan 2005 14:25:24 -0500
Subject: [R] Online course: DNA Microarray Data Analysis starts Jan. 28
Message-ID: <6.1.0.6.2.20050113141514.053ea250@mail.statistics.com>


Prof. Javier Cabrera will be giving the online course "DNA Microarray Data 
Analysis" from Jan. 28 - Feb. 25 at statistics.com.  Dr. Cabrera is 
co-author of "Exploration and Analysis of DNA Microarray and Protein Array 
Data" (the course text; Wiley) and has published a number of articles on 
gene expression data analysis, data mining and multivariate methods in 
leading statistics journals. This online course covers data preprocessing, 
replication, data reduction, distance measures, clustering, supervised and 
unsupervised classification of genes, k-nearest neighbors, neural nets, 
software, more.  R is used.  Participants will have access to a private 
discussion board with the instructor, weekly lessons, exercises and 
solutions.  There are no set hours - participate on a schedule that is 
convenient for you.  Estimated time required is 10+ hours per 
week.  Details and registration at 
http://www.statistics.com/content/courses/microarray/index.html

Peter Bruce
statistics.com



From MSchwartz at MedAnalytics.com  Thu Jan 13 22:36:37 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 13 Jan 2005 15:36:37 -0600
Subject: [R] Installing R on Xandros 3.0
In-Reply-To: <552b0d54ec84.54ec84552b0d@huskymail.uconn.edu>
References: <552b0d54ec84.54ec84552b0d@huskymail.uconn.edu>
Message-ID: <1105652197.3495.26.camel@horizons.localdomain>

On Thu, 2005-01-13 at 16:15 -0500, Thomas W Volscho wrote:
> Dear List,
> After obtaining a second-hand PC and because XP costs too much, I installed Xandros 3.0 (based on Debian) but pretty easy to use if migrating from WinXP.
> 
> Does anyone know how to install R on this OS?
> 
> Thank you for your time,
> Tom Volscho

If it is Debian based, it seems that you should be able to use apt-get
to install base R and many of the CRAN packages that Dirk has packaged
up.

Alternatively, you can download the source tarball and compile using the
typical:

./configure
make
make install (as root)

I would of course defer to Dirk and other Debian experts here.

You might also want to look at Dirk's Quantian distribution, which is
based upon Knoppix and includes R:

http://dirk.eddelbuettel.com/quantian.html

HTH,

Marc Schwartz



From White.Denis at epamail.epa.gov  Thu Jan 13 22:39:08 2005
From: White.Denis at epamail.epa.gov (White.Denis@epamail.epa.gov)
Date: Thu, 13 Jan 2005 13:39:08 -0800
Subject: [R] zero index and lazy evaluation in ifelse()
In-Reply-To: <x2hdllowds.fsf@biostat.ku.dk>
Message-ID: <OFC0F792E0.2C39FD58-ON88256F88.0076D520-88256F88.0076F0AF@epamail.epa.gov>

This seems to contradict the help file.

"... 'yes' will be evaluated if and only if any element of 'test' is
true,
and analogously for 'no'..."


Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote on 2005-01-13 13:24:31:

> White.Denis at epamail.epa.gov writes:
>
> > I don't understand this behavior:
> >
> > > a <- c(0, 1, 2, 3)
> > > b <- c(1, 2, 3, 4)
> > > ifelse (a == 0, 0, b[a])
> > [1] 0 2 3 1
> >
> > rather than the desired 0 1 2 3.  Thanks for any explanation.
>
> b[a] is c(1,2,3), recycling to length 4 gives c(1,2,3,1), get it?
>
> --
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45)
35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45)
35327907



From ligges at statistik.uni-dortmund.de  Thu Jan 13 22:47:18 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 13 Jan 2005 22:47:18 +0100
Subject: [R] Installing R on Xandros 3.0
In-Reply-To: <552b0d54ec84.54ec84552b0d@huskymail.uconn.edu>
References: <552b0d54ec84.54ec84552b0d@huskymail.uconn.edu>
Message-ID: <41E6EC66.20102@statistik.uni-dortmund.de>

Thomas W Volscho wrote:

> Dear List,
> After obtaining a second-hand PC and because XP costs too much, I installed Xandros 3.0 (based on Debian) but pretty easy to use if migrating from WinXP.
> 
> Does anyone know how to install R on this OS?


No, but I'd try to install like on Debian, either using apt-get, or 
compile from sources yourself as described in the documentation (please 
read it!).

Uwe Ligges


> Thank you for your time,
> Tom Volscho
> 
> ************************************        
> Thomas W. Volscho
> Graduate Student
> Dept. of Sociology U-2068
> University of Connecticut
> Storrs, CT 06269
> Phone: (860) 486-3882
> http://vm.uconn.edu/~twv00001
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From roger.bos at gmail.com  Thu Jan 13 22:44:40 2005
From: roger.bos at gmail.com (roger bos)
Date: Thu, 13 Jan 2005 16:44:40 -0500
Subject: [R] how to use solve.QP [SOLVED - ANSWER AVAIL]
In-Reply-To: <41E6CF72.9060804@pdf.com>
References: <1db7268005011310442d6f4572@mail.gmail.com>
	<20050113195833.6d8e3be7.Achim.Zeileis@wu-wien.ac.at>
	<1db7268005011311075f31396c@mail.gmail.com>
	<20050113201935.7f028e7e.Achim.Zeileis@wu-wien.ac.at>
	<41E6CF72.9060804@pdf.com>
Message-ID: <1db7268005011313444f2310da@mail.gmail.com>

Thanks to Zeileis and Spencer for commenting.  I case anyone wants to
see an example of portfolio optimization using solve.QP directly and
verify that the answer matches that of portfolio.optim, here is the
code:

library(quadprog, lib.loc="C:\\Program Files\\R\\tools")
library(MASS, lib.loc="C:\\Program Files\\R\\tools")
rm(sol)
n<-100	 # number of assets
m<-200			 # number of states of the world
rho<-0.7
sigma<-0.2
mu<-.1
Cov <- matrix(rho*sigma*sigma, ncol=n, nrow=n)
diag(Cov) <- rep(sigma*sigma, n)
S <- 1+matrix(mvrnorm(m, rep(mu, n), Sigma=Cov), ncol=n)


Cov <- var(S) #gives same answer as cov(S)
mu <- apply(S, 2, mean)
mu.target <- mean(mu)
bLo <- rep(0, n)

Amat <- rbind(1, mu)
dim(bLo) <- c(n,1)
bvec <- t(rbind(1, mu.target, bLo))
zMat <- diag(n)
Amat <- t(rbind(Amat, zMat))

Dmat=Cov
dvec=rep(0, nrow(Amat))

#The first two rows of Amat should be equality constraints (so weights sum to 1)
meq <- 2

sol <- solve.QP(Dmat=Dmat, dvec=dvec, Amat=Amat, bvec=bvec, meq)
sol
w.asset <- zapsmall(sol$solution)
par(mfrow=c(1,2))
plot(w.asset, type="n")
lines(w.asset, type="h")
hist(w.asset)

#test to see that we get the same result with portfolio.optim
res<-portfolio.optim(S)
wgt <- zapsmall(res$pw)
cbind(w.asset, wgt) #shows both methods are equal






On Thu, 13 Jan 2005 11:43:46 -0800, Spencer Graves
<spencer.graves at pdf.com> wrote:
>      Also, ' methods("portfolio.optim")' revealed 2 functions for this
> generic:
> 
> [1] portfolio.optim.default portfolio.optim.ts
> 
>      Typing "portfolio.optim.ts" exposes the code for the second one.
> If the class of the first argument "x" is "ts", R dispatches
> "portfiolio.optim(x, ...)" to "portfolio.optim.ts(x, ...)".  Otherwise,
> it is dispatched to "portfolio.optim.default(x,...)".
> 
>      hope this helps.
>      spencer graves
> 
> Achim Zeileis wrote:
> 
> >On Thu, 13 Jan 2005 14:07:29 -0500 roger bos wrote:
> >
> >
> >
> >>Zeileis,
> >>
> >>Thanks, I didn't know about "portfolio.optim".  I wanted to see how it
> >>works, but when I try showMethods, it doesn't show it to me.  Does
> >>that mean I am not allowed to see the inner workings?
> >>
> >>
> >
> >1. All of this is open source, so you are *always* allowed to look
> >   at the sources. (If you haven't got a version of the source package,
> >   then you can always get it from CRAN.)
> >2. showMethods() is for S4 generics, portfolio.optim is an S3 generic.
> >3. Simply typing portfolio.optim.default at the prompt should print the
> >   function.
> >Z
> >
> >
> >
> >>Thanks,
> >>
> >>Roger
> >>
> >>
> >>
> >>>showMethods("portfolio.optim")
> >>>
> >>>
> >>Function "portfolio.optim":
> >><not a generic function>
> >>
> >>
> >>On Thu, 13 Jan 2005 19:58:33 +0100, Achim Zeileis
> >><Achim.Zeileis at wu-wien.ac.at> wrote:
> >>
> >>
> >>>On Thu, 13 Jan 2005 13:44:58 -0500 roger bos wrote:
> >>>
> >>>
> >>>
> >>>>At the risk of ridicule for my deficient linear algebra skills, I
> >>>>ask for help using the solve.QP function to do portfolio
> >>>>optimization.  I am trying to following a textbook example and
> >>>>need help converting the problem into the format required by
> >>>>solve.QP.  Below is my sample code if anyone is willing to go
> >>>>through it.  This problem will not solve because it is not set up
> >>>>properly.  I hope I included enough details for someone to deciper
> >>>>it.  Or does anyone have a good example they can send me?
> >>>>
> >>>>
> >>>You can look at the man page, code and example of the function
> >>>portfolio.optim() in package tseries which does portfolio
> >>>optimization based on solve.QP from quadprog.
> >>>
> >>>hth,
> >>>Z
> >>>
> >>>
> >>>
> >>>>Thanks so much for any hints and suggestions, Roger.
> >>>>
> >>>>
> >>>>
> >>>>library(quadprog, lib.loc="C:\\Program Files\\R\\tools")
> >>>>library(MASS, lib.loc="C:\\Program Files\\R\\tools")
> >>>>n<-100
> >>>>m<-200
> >>>>rho<-0.7
> >>>>sigma<-0.2
> >>>>mu<-0.1
> >>>>Cov <- matrix(rho*sigma*sigma, ncol=n, nrow=n)
> >>>>diag(Cov) <- rep(sigma*sigma, n)
> >>>>S <- 1+matrix(mvrnorm(m, rep(mu, n), Sigma=Cov), ncol=n)
> >>>>
> >>>>#The problem is formulated as minimize t(b) Cov b
> >>>>#subject to cLo <= A <= cUp
> >>>>#and bLo=0 <= w <= 1=bUp
> >>>>
> >>>>Cov <- var(S)
> >>>>mu <- apply(S, 2, mean)
> >>>>mu.target <- 0.1
> >>>>#subject to cLo <= A <= cUp and bLo=0 <= b <= 1=bUp
> >>>>A <- rbind(1,mu)
> >>>>cLo <- c(1, mu.target)
> >>>>cUp <- c(1, Inf)
> >>>>bLo <- rep(0, n)
> >>>>bUp <- rep(1, n)
> >>>>
> >>>>#I convert [cLo <= A <= cUp] to Amat >= bvec and [bLo=0 <= w
> >>>>##<=1=bUp] to
> >>>>Amat <- rbind(-1, 1, -mu, mu)
> >>>>dim(bLo) <- c(n,1)
> >>>>dim(bUp) <- c(n,1)
> >>>>bvec <- rbind(-1, 1, mu.target, Inf, bLo, -bUp)
> >>>>zMat <- matrix(rep(0,2*n*n),ncol=n, nrow=n*2)
> >>>>zMat[,1] <- c(rep(1,n), rep(-1,n))
> >>>>Amat <- t(rbind(Amat, zMat))
> >>>>
> >>>>#So I set Dmat=Cov and set dvec=0
> >>>>Dmat=Cov
> >>>>dvec=rep(0, nrow(Amat))
> >>>>
> >>>>#The first two rows of Amat should be equality constraints (so
> >>>>#weights sum to 1)
> >>>>meq <- 2
> >>>>
> >>>>sol <- solve.QP(Dmat=Dmat, dvec=dvec, Amat=Amat, bvec=bvec, meq)
> >>>>sol
> >>>>
> >>>>______________________________________________
> >>>>R-help at stat.math.ethz.ch mailing list
> >>>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>PLEASE do read the posting guide!
> >>>>http://www.R-project.org/posting-guide.html
> >>>>
> >>>>
> >>>>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
> 
>



From Robert.McGehee at geodecapital.com  Thu Jan 13 22:47:12 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Thu, 13 Jan 2005 16:47:12 -0500
Subject: [R] as.character methods
Message-ID: <67DCA285A2D7754280D3B8E88EB5480206741E14@MSGBOSCLB2WIN.DMN1.FMR.COM>

Hello,
?as.character says that the as.character function is a generic with
usage: as.character(x, ...). So, I want to create an S4 object with an
as.character method following the above usage, but I get the below error
telling me that ... isn't in the generic for as.character.

> setClass("tmp", "numeric")
> setMethod("as.character", "tmp", function(x, ...) paste(x, c(...)))
Error in rematchDefinition(definition, fdef, mnames, fnames, signature)
: 
	Methods can add arguments to the generic only if "..." is an
argument to the generic

Am I reading the documentation incorrectly? How do I correctly pass the
... object to the method for this "tmp" object?

However I note that looking at the generic function, I see no mention of
... (despite the documentation).
> getGeneric("as.character")
standardGeneric for "as.character" defined from package "base"

function (x) 
standardGeneric("as.character", .Primitive("as.character"))
<environment: 0145EDC4>
Methods may be defined for arguments: x 


So, briefly, is the documentation wrong? Am I doing something wrong? Can
I create an as.character method and pass additional arguments to it as I
think I should be able to?

Thanks,
Robert

Robert McGehee
Geode Capital Management, LLC
53 State Street, 5th Floor | Boston, MA | 02109
Tel: 617/392-8396    Fax:617/476-6389
mailto:robert.mcgehee at geodecapital.com



This e-mail, and any attachments hereto, are intended for use by the
addressee(s) only and may contain information that is (i) confidential
information of Geode Capital Management, LLC and/or its affiliates,
and/or (ii) proprietary information of Geode Capital Management, LLC
and/or its affiliates. If you are not the intended recipient of this
e-mail, or if you have otherwise received this e-mail in error, please
immediately notify me by telephone (you may call collect), or by e-mail,
and please permanently delete the original, any print outs and any
copies of the foregoing. Any dissemination, distribution or copying of
this e-mail is strictly prohibited.



From ligges at statistik.uni-dortmund.de  Thu Jan 13 22:53:03 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 13 Jan 2005 22:53:03 +0100
Subject: [R] using created R objects in a program
In-Reply-To: <C8696843AE995F4EA4CDC3E2B83482A90A1B7D@mailbox02.cshl.edu>
References: <C8696843AE995F4EA4CDC3E2B83482A90A1B7D@mailbox02.cshl.edu>
Message-ID: <41E6EDBF.8080106@statistik.uni-dortmund.de>

Das, Rajdeep wrote:

> Hi,
> 
> I am running a program in --vanilla mode. 

What does "program" refer to?
Is the "program" called "R", or do you mean some R code you are trying 
to run within R?


> I would like to know how to load
> saved functions in the program that it calls. EVerytime I call those objects
> it returns error and complains that it cannot find the function object I am
> calling. Please let me know. Thanks in advance.

save()d objects can be loaded with load(), ASCII code defining a 
function can be source()d. That's all described in the documentation.
Please be much more specific in your questions, the terminology you are 
using is quite imprecise.

Please read at least:
  - the posting guide,
  - An Introduction to R,
  - R Data Import/Export Manual, and
  - the R FAQ

Uwe Ligges


> Regards,
> 
> Raj
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Thu Jan 13 23:06:46 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 13 Jan 2005 17:06:46 -0500
Subject: [R] Installing R on Xandros 3.0
Message-ID: <3A822319EB35174CA3714066D590DCD50994E524@usrymx25.merck.com>

If I have to guess, `desktop Linux' such as Xandros do not come with
compilers and other development tools, so compiling from source will be a
bit of a challenge.  I recall having played with Corel Linux during its
rather short lifetime (also Debian based), and was horrified to find it
lacking GCC.  Apparently people who make up such distros have the same view
as Mr. Gates that compilers need not be part of a desktop OS.

(If I'm not mistaken, Unbuntu has the same problem.  However, apt-get is
_really_ easy...)

Cheers,
Andy 

> From: Marc Schwartz
> 
> On Thu, 2005-01-13 at 16:15 -0500, Thomas W Volscho wrote:
> > Dear List,
> > After obtaining a second-hand PC and because XP costs too 
> much, I installed Xandros 3.0 (based on Debian) but pretty 
> easy to use if migrating from WinXP.
> > 
> > Does anyone know how to install R on this OS?
> > 
> > Thank you for your time,
> > Tom Volscho
> 
> If it is Debian based, it seems that you should be able to use apt-get
> to install base R and many of the CRAN packages that Dirk has packaged
> up.
> 
> Alternatively, you can download the source tarball and 
> compile using the
> typical:
> 
> ./configure
> make
> make install (as root)
> 
> I would of course defer to Dirk and other Debian experts here.
> 
> You might also want to look at Dirk's Quantian distribution, which is
> based upon Knoppix and includes R:
> 
http://dirk.eddelbuettel.com/quantian.html

HTH,

Marc Schwartz

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Jan 13 23:11:48 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 13 Jan 2005 23:11:48 +0100
Subject: [R] zero index and lazy evaluation in ifelse()
In-Reply-To: <OFC0F792E0.2C39FD58-ON88256F88.0076D520-88256F88.0076F0AF@epamail.epa.gov>
References: <OFC0F792E0.2C39FD58-ON88256F88.0076D520-88256F88.0076F0AF@epamail.epa.gov>
Message-ID: <41E6F224.6080100@statistik.uni-dortmund.de>

White.Denis at epamail.epa.gov wrote:

> This seems to contradict the help file.
> 
> "... 'yes' will be evaluated if and only if any element of 'test' is
> true,
> and analogously for 'no'..."


Note: "Evaluated", not "returned"!
So both "0" and "b[a]" are evaluated, because a==0 is true for a[1], and 
false for a[2], a[3], a[4].

a[1] == 0 is true, hence you get "0",
a[2] == 0 is false, hence you get b[a][2]=2
a[3] == 0 is false, hence you get b[a][3]=3
a[4] == 0 is false, hence you get b[a][4], which is recycled to b[a][1]=1

Uwe Ligges


> 
> Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote on 2005-01-13 13:24:31:
> 
> 
>>White.Denis at epamail.epa.gov writes:
>>
>>
>>>I don't understand this behavior:
>>>
>>>
>>>>a <- c(0, 1, 2, 3)
>>>>b <- c(1, 2, 3, 4)
>>>>ifelse (a == 0, 0, b[a])
>>>
>>>[1] 0 2 3 1
>>>
>>>rather than the desired 0 1 2 3.  Thanks for any explanation.
>>
>>b[a] is c(1,2,3), recycling to length 4 gives c(1,2,3,1), get it?
>>
>>--
>>   O__  ---- Peter Dalgaard             Blegdamsvej 3
>>  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>> (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45)
> 
> 35327918
> 
>>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45)
> 
> 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Jan 13 23:21:15 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 13 Jan 2005 23:21:15 +0100
Subject: [R] Exact poisson confidence intervals
In-Reply-To: <41E6C1BD.8070302@pigrecodata.net>
References: <41E6C1BD.8070302@pigrecodata.net>
Message-ID: <41E6F45B.8080304@statistik.uni-dortmund.de>

Federico Gherardini wrote:

> Hi all,
> Is there any R function to compute exact confidence limits for a Poisson 
> distribution with a given Lambda?

For sure you are looking for certain quantiles of the poisson 
distribution? See ?Poisson.

Uwe Ligges


> Thanks in advance
> Federico
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From MSchwartz at MedAnalytics.com  Thu Jan 13 23:25:04 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 13 Jan 2005 16:25:04 -0600
Subject: [R] Installing R on Xandros 3.0
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E524@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E524@usrymx25.merck.com>
Message-ID: <1105655104.3495.35.camel@horizons.localdomain>

On Thu, 2005-01-13 at 17:06 -0500, Liaw, Andy wrote:
> If I have to guess, `desktop Linux' such as Xandros do not come with
> compilers and other development tools, so compiling from source will be a
> bit of a challenge.  I recall having played with Corel Linux during its
> rather short lifetime (also Debian based), and was horrified to find it
> lacking GCC.  Apparently people who make up such distros have the same view
> as Mr. Gates that compilers need not be part of a desktop OS.
> 
> (If I'm not mistaken, Unbuntu has the same problem.  However, apt-get is
> _really_ easy...)

[snip]

Ubuntu does include devel tools on the main CD:

http://www.ubuntulinux.org/support/documentation/faq/development-tools

While the install CD is of course limited due to space, there are a lot
of packages that are available via repositories.

My general sense of Ubuntu is that it is a something of an attempt to
parallel the aggressive release schedule of Fedora utilizing the Debian
'sid' framework.

A general overview of the relationship between Debian and Ubuntu is
here:

http://www.ubuntulinux.org/ubuntu/relationship/document_view

HTH,

Marc



From anne.piotet at urbanet.ch  Thu Jan 13 23:34:19 2005
From: anne.piotet at urbanet.ch (Anne)
Date: Thu, 13 Jan 2005 23:34:19 +0100
Subject: [R] (no subject)
References: <41E68EA2.9050404@to.infn.it>
Message-ID: <008a01c4f9c0$0749ca50$6c00a8c0@mtd4>

Hi Angela
I'm not sure what you want to do with these 2 plots:
1)   getting them on the same screen ( >par(mfrow=c(1,2))
2)   if you need them on separate files try the appropiate Hmisc functions
(for instance psslide(), whih are in the words of the author "Postscript and
Adobe PDF Setup for 35mm Slides and Other Formats")


for legend try:
legend(locator(1),legend=leg.text,lty=1, col=leg.col)

Anne

Ciao! Vorrei essere nelle vacanze...



----- Original Message ----- 
From: "Angela Re" <angelare at to.infn.it>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, January 13, 2005 4:07 PM
Subject: [R] (no subject)


> Good morning,
> I wrote a little code in R which has to show two graphs  but  I can get
> only one. How can I  adress  the graphs in two files?
>
> Second, I'd like, always in the same code, to add a legend  to a graph.
> Better,  I'd like to  put in  such a legend a  new item whose color
> could remind the colour ol the columns it  refers to in the plot. I wrote:
>
> leg.txt<-c("control people", "radiated ill people", "radiated healthy
> people",
> "pesticide exposed people")
>
> leg.col<-c("lightblue", "gray", "lightcyan","lavender")
>
> grA<-barplot(seqA, type = "h", col = c(colors),legend.text =
> c(leg.txt),main = " Number of breaks occured on cluster A bands on
> patients' sample", xlab = "patient ID", ylab = "breaks number")
> but I don't know how to assign the right colors to legend's items.
>
> Thanks  of helping me, Angela
>
>
>
>


----------------------------------------------------------------------------
----


> #inserisci in inp1/2/3/4 il numero di rotture per ciascun soggetto delle 4
> #classi per le bande dei cluster A e B.
>
>
>
>
> #CLUSTER A
>
> inp1A<-scan("ROTTURE_PER_SOGGETTO_CONTROLLO_CLUSTER_A.dat")
>
> inp2A<-scan("ROTTURE_PER_SOGGETTO_RAD_MALATI_CLUSTER_A.dat")
>
> inp3A<-scan("ROTTURE_PER_SOGGETTO_RAD_NO_MALATI_CLUSTER_A.dat")
>
> inp4A<-scan("ROTTURE_PER_SOGGETTO_PEST_CLUSTER_A.dat")
>
> TOT_ROTTURE_CLUSTER_A<-2153
>
> seqA<-rep(0, times = 60)
>
> k<-0
>
> for( i in 1:length(inp1A) ) {
>
> k<-k+1
>
> seqA[k]<-(inp1A[i]/(2*TOT_ROTTURE_CLUSTER_A))
>
> }
> perc_1A<-100*sum(inp1A)/TOT_ROTTURE_CLUSTER_A
>
>
>
>
> for( i in 1:length(inp2A) ) {
>
> k<-k+1
>
> seqA[k]<-inp2A[i]/TOT_ROTTURE_CLUSTER_A
>
> }
> perc_2A<-100*sum(inp2A)/TOT_ROTTURE_CLUSTER_A
>
>
>
>
>
> for( i in 1:length(inp3A) ) {
>
> k<-k+1
>
> seqA[k]<-inp3A[i]/TOT_ROTTURE_CLUSTER_A
>
> }
> perc_3A<-100*sum(inp3A)/TOT_ROTTURE_CLUSTER_A
>
>
>
>
>
> for( i in 1:length(inp4A) ) {
>
> k<-k+1
>
> seqA[k]<-(inp4A[i]/(2*TOT_ROTTURE_CLUSTER_A))
>
> }
> perc_4A<-100*sum(inp4A)/TOT_ROTTURE_CLUSTER_A
>
>
> #oltre a disegnare quantifichiamo un po'
> media_inp1A<-mean(inp1A)
> media_inp2A<-mean(inp2A)
> media_inp3A<-mean(inp3A)
> media_inp4A<-mean(inp4A)
>
> media_fra_inp1A<-mean(inp1A/TOT_ROTTURE_CLUSTER_A)
> media_fra_inp2A<-mean(inp2A/TOT_ROTTURE_CLUSTER_A)
> media_fra_inp3A<-mean(inp3A/TOT_ROTTURE_CLUSTER_A)
> media_fra_inp4A<-mean(inp4A/TOT_ROTTURE_CLUSTER_A)
>
>
>
>
>
>
>
>
> #CLUSTER B
>
> inp1B<-scan("ROTTURE_PER_SOGGETTO_CONTROLLO_CLUSTER_B.dat")
>
> inp2B<-scan("ROTTURE_PER_SOGGETTO_RAD_MALATI_CLUSTER_B.dat")
>
> inp3B<-scan("ROTTURE_PER_SOGGETTO_RAD_NO_MALATI_CLUSTER_B.dat")
>
> inp4B<-scan("ROTTURE_PER_SOGGETTO_PEST_CLUSTER_B.dat")
>
> TOT_ROTTURE_CLUSTER_B<-3553
>
> seqB<-rep(0, times = 60)
>
> z<-0
>
> for( i in 1:length(inp1B) ) {
>
> z<-z+1
>
> seqB[z]<-(inp1B[i]/(2*TOT_ROTTURE_CLUSTER_B))
>
> }
> perc_1B<-100*sum(inp1B)/TOT_ROTTURE_CLUSTER_B
>
>
>
>
>
> for( i in 1:length(inp2B) ) {
>
> z<-z+1
>
> seqB[z]<-inp2B[i]/TOT_ROTTURE_CLUSTER_B
>
> }
> perc_2B<-100*sum(inp2B)/TOT_ROTTURE_CLUSTER_B
>
>
>
>
>
> for( i in 1:length(inp3B) ) {
>
> z<-z+1
>
> seqB[z]<-inp3B[i]/TOT_ROTTURE_CLUSTER_B
>
> }
> perc_3B<-100*sum(inp3B)/TOT_ROTTURE_CLUSTER_B
>
>
>
>
>
> for( i in 1:length(inp4B) ) {
>
> z<-z+1
>
> seqB[z]<-(inp4B[i]/(2*TOT_ROTTURE_CLUSTER_B))
>
> }
> perc_4B<-100*sum(inp4B)/TOT_ROTTURE_CLUSTER_B
>
>
> #oltre a disegnare quantifichiamo un po'
> media_inp1B<-mean(inp1B)
> media_inp2B<-mean(inp2B)
> media_inp3B<-mean(inp3B)
> media_inp4B<-mean(inp4B)
>
> media_fra_inp1B<-mean(inp1B/TOT_ROTTURE_CLUSTER_B)
> media_fra_inp2B<-mean(inp2B/TOT_ROTTURE_CLUSTER_B)
> media_fra_inp3B<-mean(inp3B/TOT_ROTTURE_CLUSTER_B)
> media_fra_inp4B<-mean(inp4B/TOT_ROTTURE_CLUSTER_B)
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> #costruisco col
> colors<-NULL
>
> q<-0
>
> for( i in 1:length(inp1B) ) {
>
> q<-q+1
>
> colors[q]<-"lightblue"
>
> }
>
>
>
>
>
>
> for( i in 1:length(inp2B) ) {
>
> q<-q+1
>
> colors[q]<-"gray"
>
> }
>
>
>
>
>
>
> for( i in 1:length(inp3B) ) {
>
> q<-q+1
>
> colors[q]<-"lightcyan"
>
> }
>
>
>
>
>
>
> for( i in 1:length(inp4B) ) {
>
> q<-q+1
>
> colors[q]<-"lavender"
>
> }
>
>
> #costruisco legenda
>
>
> leg.txt<-c("control people", "radiated ill people", "radiated healthy
people",
> "pesticide exposed people")
> leg.col<-c("lightblue", "gray", "lightcyan","lavender")
>
>
>
> #grafico del numero di rotture dei soggetti del campione ( preventivamente
ordinati per classi )in riferimento alle bande del cluster A o B
> #grA<-barplot(seqA, type = "h", col = c(colors),legend.text =
c(leg.txt),main = " Number of breaks occured on cluster A bands on patients'
sample", xlab = "patient ID", ylab = "breaks number")
> #grB<-barplot(seqB, type = "h",col = c(colors),legend.text
=c(leg.txt),main = " Number of breaks occured on cluster B bands on
patients' sample", xlab = "patient ID", ylab = "breaks number")
>
> #grafico la frazione del numero di rotture totale del cluster per soggetto
> grA<-barplot(seqA, type = "h", col = c(colors),legend.text =
c(leg.txt),main = "Number of breaks occured on cluster A bands on patients's
ample", xlab ="patient ID", ylab = "breaks number/cluster breaks number")
> #grB<-barplot(seqB, type = "h",col = c(colors),legend.text
=c(leg.txt),main = " Number of breaks occured on cluster B bands
onpatients'sample", xlab = "patient ID", ylab = "breaks number/cluster
breaks number")
>
>


----------------------------------------------------------------------------
----


> 79
> 18
> 32
> 35
> 62
> 60
> 32
> 39
> 38
> 34
> 30
>


----------------------------------------------------------------------------
----


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jjvanhoutte at ucdavis.edu  Thu Jan 13 23:37:43 2005
From: jjvanhoutte at ucdavis.edu (Jeroen Van Houtte UCD)
Date: Thu, 13 Jan 2005 14:37:43 -0800
Subject: [R] grid graphics
In-Reply-To: <16857.7955.182146.57073@stat.math.ethz.ch>
Message-ID: <ABELICKKKMGIDJALKLAOGEOGCCAA.jjvanhoutte@ucdavis.edu>

Thank you, Uwe,
I probably phrased my question wrong, but I'm not into compiling my own
software.

Thank you Martin,

It was definitely worth the few weeks spent learning to use the grid
package.
I want to share with others making that same step some of the functions I
wrote as I tried to convert from the graphics mindset to the grid.graphics
mindset.

gtext<-function(x=0.5,y=0.5,label="gtext()",adj=0.5){
	if(adj==1) {
	grid.text(x=x,y=y,default.units="native", label, just="right")
	} else {
	if(adj==0) {grid.text(x=x,y=y,default.units="native", label,just="left")
	} else {
	grid.text(x=x,y=y,default.units="native", label,just="center")
	}}}
glines<-function(x,y) grid.lines(x,y,default.units="native")
gpol<-function(x,y,fill=grey(0.5)) grid.polygon(c(x,x[1]),c(y,y[length(y)]),
	gp=gpar(fill=fill),default.units="native")
garrows<-function(){
	grid.arrows(x=c(0,105),y=0,length=unit(2,"mm"),angle=10,default.units="nati
ve")
	grid.arrows(x=0,y=c(0,105),length=unit(2,"mm"),angle=10,default.units="nati
ve")
	}
grect<-function(x,y,x2,y2) {
	grid.rect(x,y,x2-x,y2-y,
		default.units="native", just=c("left","bottom"))
	}

gbalk<-function(lijn=1,x1=0,x2=1,label=label){
	vp<-viewport(x = x1, y = lijn-1, w = x2-x1, h = 1,
		default.units="native", just = c("left", "bottom"))
	pushViewport(vp)
	grid.rect()
	grid.text(label) #(,x=0,just="right")
	popViewport()
	}

To see how I used these, look at
http://www.geocities.com/jjeroen1/cescompexample.R.txt


-----Original Message-----
From: Martin Maechler [mailto:maechler at stat.math.ethz.ch]
Sent: Monday, January 03, 2005 2:32 AM
To: Uwe Ligges
Cc: Jeroen Van Houtte; r-help at stat.math.ethz.ch
Subject: Re: [R] Error in layout(): Too many rows in layout

   >> I found that the limit of 15 is set by #define
    >> MAX_LAYOUT_ROWS 15

    UweL> Well, what about replacing 15 by something like 100?
    UweL> The point is that you should look where
    UweL> MAX_LAYOUT_ROWS is used for calculations and whether
    UweL> there might result any problem after increasing it.

    >> in
    >> https://svn.r-project.org/R/trunk/src/include/Graphics.h
    >> but I 'm not familiar enough with compilers to modify
    >> this.


Unless Jeroen is really keen on learning how build R from source
on MS Windows, I think Jeroen's time might be better invested in
learning to use the "grid" package instead of "graphics".

I think layout() in "graphics" is really just a poor man's
version of using layout.grid() and viewports as provided by the
"grid" package {and as used extensively by the "lattice" package}.

Martin Maechler



From White.Denis at epamail.epa.gov  Thu Jan 13 23:44:24 2005
From: White.Denis at epamail.epa.gov (White.Denis@epamail.epa.gov)
Date: Thu, 13 Jan 2005 14:44:24 -0800
Subject: [R] zero index and lazy evaluation in ifelse()
In-Reply-To: <x28y6xotol.fsf@biostat.ku.dk>
Message-ID: <OF6A24E995.10BAA039-ON88256F88.007C8178-88256F88.007CEA18@epamail.epa.gov>

Thanks to all for clarifying that it isn't an element by element test
and evaluation, rather an "any" test as the code says.


Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote on 2005-01-13 14:22:50:

> White.Denis at epamail.epa.gov writes:
>
> > This seems to contradict the help file.
> >
> > "... 'yes' will be evaluated if and only if any element of 'test' is
> > true,
> > and analogously for 'no'..."
> >
>
> It doesn't.  Read closer.  Both 'yes' and 'no' is evaluated. The
> former is c(0, 0, 0, 0) the latter is c(1, 2, 3, 1).
>
>
> > Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote on 2005-01-13
13:24:31:
> >
> > > White.Denis at epamail.epa.gov writes:
> > >
> > > > I don't understand this behavior:
> > > >
> > > > > a <- c(0, 1, 2, 3)
> > > > > b <- c(1, 2, 3, 4)
> > > > > ifelse (a == 0, 0, b[a])
> > > > [1] 0 2 3 1
> > > >
> > > > rather than the desired 0 1 2 3.  Thanks for any explanation.
> > >
> > > b[a] is c(1,2,3), recycling to length 4 gives c(1,2,3,1), get it?
> > >



From ripley at stats.ox.ac.uk  Thu Jan 13 23:58:18 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 13 Jan 2005 22:58:18 +0000 (GMT)
Subject: [R] as.character methods
In-Reply-To: <67DCA285A2D7754280D3B8E88EB5480206741E14@MSGBOSCLB2WIN.DMN1.FMR.COM>
References: <67DCA285A2D7754280D3B8E88EB5480206741E14@MSGBOSCLB2WIN.DMN1.FMR.COM>
Message-ID: <Pine.LNX.4.61.0501132248290.12867@gannet.stats>

You seem to be confusing `generic' with `S4 generic', and `method' with 
`S4 method'.

Note that all references to `generic' and `method' outside the `methods' 
package are not to S4 concepts unless explicitly stated.

On Thu, 13 Jan 2005, McGehee, Robert wrote:

> Hello,
> ?as.character says that the as.character function is a generic with
> usage: as.character(x, ...). So, I want to create an S4 object with an
> as.character method following the above usage, but I get the below error
> telling me that ... isn't in the generic for as.character.
>
>> setClass("tmp", "numeric")
>> setMethod("as.character", "tmp", function(x, ...) paste(x, c(...)))
> Error in rematchDefinition(definition, fdef, mnames, fnames, signature)
> :
> 	Methods can add arguments to the generic only if "..." is an
> argument to the generic
>
> Am I reading the documentation incorrectly? How do I correctly pass the
> ... object to the method for this "tmp" object?
>
> However I note that looking at the generic function, I see no mention of
> ... (despite the documentation).
>> getGeneric("as.character")
> standardGeneric for "as.character" defined from package "base"
>
> function (x)
> standardGeneric("as.character", .Primitive("as.character"))
> <environment: 0145EDC4>
> Methods may be defined for arguments: x
>
>
> So, briefly, is the documentation wrong? Am I doing something wrong? Can
> I create an as.character method and pass additional arguments to it as I
> think I should be able to?

Briefly, No, yes, yes.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From statistician at jindan.homedns.org  Fri Jan 14 00:11:00 2005
From: statistician at jindan.homedns.org (Jindan Zhou)
Date: Thu, 13 Jan 2005 17:11:00 -0600
Subject: [R] Graph format: quality vs. file size
In-Reply-To: <ABELICKKKMGIDJALKLAOGEOGCCAA.jjvanhoutte@ucdavis.edu>
References: <ABELICKKKMGIDJALKLAOGEOGCCAA.jjvanhoutte@ucdavis.edu>
Message-ID: <Pine.CYG.4.58.0501131659380.3404@shenzhen>

Hello R-List,

The question is related to R, but not strictly:

I have generated a Q-Q plot with some 15,000 data points, when saved in
postscript format, the file became really large, which is not good to be
included in a LaTex file, as the output pdf file will be too big in file
size, too. If I save the graph in jpeg format, the quality is simply
unacceptable.

Do you have a workaround for this problem? What is the best graph format
in R that preserves the quality while minimizes the file size?

Thanks for input!

Jindan



From jorelien at scimetrika.com  Fri Jan 14 00:22:29 2005
From: jorelien at scimetrika.com (Jean G. Orelien)
Date: Thu, 13 Jan 2005 18:22:29 -0500
Subject: [R] GAM: Remedial measures
Message-ID: <NC-TDA03mQwKLAffPc900000748@mail.nctda.org>

I fitted a GAM model with Poisson distribution to a data with about 200
observations.  I noticed that the plot of the residuals versus fitted values
show a trend.  Residuals tend to be lower for higher fitted values. Because,
I'm dealing with count data, I'm thinking that this might be due to
overdispersion.  Is there a way to account for overdispersion in any of the
packages MGCV or GAM?  

 

I welcome any suggestions that one may have on this topic.

 

Jean

 


From MSchwartz at MedAnalytics.com  Fri Jan 14 00:25:26 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 13 Jan 2005 17:25:26 -0600
Subject: [R] Graph format: quality vs. file size
In-Reply-To: <Pine.CYG.4.58.0501131659380.3404@shenzhen>
References: <ABELICKKKMGIDJALKLAOGEOGCCAA.jjvanhoutte@ucdavis.edu>
	<Pine.CYG.4.58.0501131659380.3404@shenzhen>
Message-ID: <1105658726.29053.1.camel@horizons.localdomain>

On Thu, 2005-01-13 at 17:11 -0600, Jindan Zhou wrote:
> Hello R-List,
> 
> The question is related to R, but not strictly:
> 
> I have generated a Q-Q plot with some 15,000 data points, when saved in
> postscript format, the file became really large, which is not good to be
> included in a LaTex file, as the output pdf file will be too big in file
> size, too. If I save the graph in jpeg format, the quality is simply
> unacceptable.
> 
> Do you have a workaround for this problem? What is the best graph format
> in R that preserves the quality while minimizes the file size?
> 
> Thanks for input!
> 
> Jindan


You might want to scan through this recent thread:

http://tolstoy.newcastle.edu.au/R/help/04/11/7858.html

HTH,

Marc Schwartz



From gunter.berton at gene.com  Fri Jan 14 00:52:47 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 13 Jan 2005 15:52:47 -0800
Subject: [R] Graph format: quality vs. file size
In-Reply-To: <Pine.CYG.4.58.0501131659380.3404@shenzhen>
Message-ID: <200501132352.j0DNqku3003706@hertz.gene.com>

Given that you are doing a Q-Q plot, I strongly suspect that other then in
the extreme tails, there will be no loss of visible information if you plot
only 1 out of every 10 of the ordered values instead of all of them (as the
ordered values are highly correlated). This makes the file size manageable.
If you are interested in the extreme tails (the highest and lowest 50 or so
points), these probably should be examined separately. I would guess that
they are not part of the rest of the distribution, anyway (whatever that
means).

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jindan Zhou
> Sent: Thursday, January 13, 2005 3:11 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Graph format: quality vs. file size
> 
> Hello R-List,
> 
> The question is related to R, but not strictly:
> 
> I have generated a Q-Q plot with some 15,000 data points, 
> when saved in
> postscript format, the file became really large, which is not 
> good to be
> included in a LaTex file, as the output pdf file will be too 
> big in file
> size, too. If I save the graph in jpeg format, the quality is simply
> unacceptable.
> 
> Do you have a workaround for this problem? What is the best 
> graph format
> in R that preserves the quality while minimizes the file size?
> 
> Thanks for input!
> 
> Jindan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From saverio.vicario at yale.edu  Fri Jan 14 01:01:20 2005
From: saverio.vicario at yale.edu (saverio vicario)
Date: Thu, 13 Jan 2005 19:01:20 -0500
Subject: [R] problem on how R2.0 handle the RAM, maybe a bug
Message-ID: <p06010212be0cbb933c68@[130.132.249.15]>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050113/68aea1f2/attachment.pl

From spencer.graves at pdf.com  Fri Jan 14 01:07:45 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 13 Jan 2005 16:07:45 -0800
Subject: [R] Exact poisson confidence intervals
In-Reply-To: <41E6F45B.8080304@statistik.uni-dortmund.de>
References: <41E6C1BD.8070302@pigrecodata.net>
	<41E6F45B.8080304@statistik.uni-dortmund.de>
Message-ID: <41E70D51.3020000@pdf.com>

      "Exact confidence limits" are highly conservative.  I have not 
studied this for the Poisson distribution, but for the binomial 
distribution, Brown, Cai and DasGupta (2001, 2002) showed that the exact 
coverage probabilities exhibit increasingly wild oscillations as the 
binomial probability goes to either 0 or 1.  The interval width for 
"exact" 95% confidence intervals is increased to compensate for these 
oscillations so the minimum coverage is 95%.  In practice, this means 
that the actually coverage may be much higher, possible as much as 99% 
or more in most applications.  Moreover, unless the binomial / Poisson 
parameter is exactly constant, any minor variations in the parameter 
would move the peaks to fill the valleys, making the "exact" intervals 
highly conservative. 

      As part of this work, Brown, Cai and DasGupta also showed that the 
actual coverage probabilities of the standard approximate confidence 
limits [p.bar +/-2*sqrt(p.bar*(1-p.bar)/n)] are highly biased.  They 
described several other alternatives.  It turns out that the standard 
asymptotic normal approximation to the logit actually performs fairly 
close to the best. 

      By extension, I would expect that the standard asymptotic normal 
approximation for the log(PoissonRate) might perform better than other 
confidence intervals for the Poisson, though of course, this should be 
verified.  At the risk of making a fool of myself, I'll continue with 
this exercise:  If I haven't made a mistake, the Fisher information for 
g = log(PoissonRate) is the PoissonRate, so the approximate standard 
deviation for g-hat is 1/sqrt(PoissonRate).  But the maximum likelihood 
estimate for the PoissonRate is x.bar = mean of the Poisson 
observations.  This would suggest x.bar*exp(+/-2/sqrt(x.bar)) as an 
approximate 95% confidence interval for a Poisson.  If someone does any 
checks on this, I would like to hear the results. 

      hope this helps. 
      spencer graves
###########################
Brown, Cai and DasGupta (2001) Statistical Science, 16:  101-133 and 
(2002) Annals of Statistics, 30:  160-2001 
###########################

Uwe Ligges wrote:

> Federico Gherardini wrote:
>
>> Hi all,
>> Is there any R function to compute exact confidence limits for a 
>> Poisson distribution with a given Lambda?
>
>
> For sure you are looking for certain quantiles of the poisson 
> distribution? See ?Poisson.
>
> Uwe Ligges
>
>
>> Thanks in advance
>> Federico
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Fri Jan 14 01:26:26 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 13 Jan 2005 19:26:26 -0500
Subject: [R] Lme and centered variables
Message-ID: <3A822319EB35174CA3714066D590DCD50994E526@usrymx25.merck.com>

I'm by no means expert in such matter, but where did you get the idea that
such manipulation is needed?  If you center the response by its mean, that's
the same as taking out the intercept.  There's no reason to do that
beforehand.  As to centering by median, that's a first for me, and quite
frankly I don't see what it buys you.

Andy

> From: Martin Julien
> 
> I want to fit a linear mixed-model with my data but I want to 
> know if I have
> to center all my responses variables on their median and why 
> I have to do
> it?
> 
> Thank
> 
> Julien Martin
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Charles.Annis at StatisticalEngineering.com  Fri Jan 14 01:30:03 2005
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Thu, 13 Jan 2005 19:30:03 -0500
Subject: [R] Exact poisson confidence intervals
In-Reply-To: <41E70D51.3020000@pdf.com>
Message-ID: <200501140030.j0E0U7Ur007033@hypatia.math.ethz.ch>

Federico:

You might also look at Professor Agresti's observations on exact vs
approximate, which appeared in the American Statistician a few years ago.
(I believe it was the AS; my memory isn't what it once was.)

Google produced this
http://www.amstat.org/publications/tas/index.cfm?fuseaction=agresti1998 

when searching for "approximate is better than exact"



Charles Annis, P.E.
 
Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
Sent: Thursday, January 13, 2005 7:08 PM
To: Uwe Ligges
Cc: R-help mailing list
Subject: Re: [R] Exact poisson confidence intervals

      "Exact confidence limits" are highly conservative.  I have not 
studied this for the Poisson distribution, but for the binomial 
distribution, Brown, Cai and DasGupta (2001, 2002) showed that the exact 
coverage probabilities exhibit increasingly wild oscillations as the 
binomial probability goes to either 0 or 1.  The interval width for 
"exact" 95% confidence intervals is increased to compensate for these 
oscillations so the minimum coverage is 95%.  In practice, this means 
that the actually coverage may be much higher, possible as much as 99% 
or more in most applications.  Moreover, unless the binomial / Poisson 
parameter is exactly constant, any minor variations in the parameter 
would move the peaks to fill the valleys, making the "exact" intervals 
highly conservative. 

      As part of this work, Brown, Cai and DasGupta also showed that the 
actual coverage probabilities of the standard approximate confidence 
limits [p.bar +/-2*sqrt(p.bar*(1-p.bar)/n)] are highly biased.  They 
described several other alternatives.  It turns out that the standard 
asymptotic normal approximation to the logit actually performs fairly 
close to the best. 

      By extension, I would expect that the standard asymptotic normal 
approximation for the log(PoissonRate) might perform better than other 
confidence intervals for the Poisson, though of course, this should be 
verified.  At the risk of making a fool of myself, I'll continue with 
this exercise:  If I haven't made a mistake, the Fisher information for 
g = log(PoissonRate) is the PoissonRate, so the approximate standard 
deviation for g-hat is 1/sqrt(PoissonRate).  But the maximum likelihood 
estimate for the PoissonRate is x.bar = mean of the Poisson 
observations.  This would suggest x.bar*exp(+/-2/sqrt(x.bar)) as an 
approximate 95% confidence interval for a Poisson.  If someone does any 
checks on this, I would like to hear the results. 

      hope this helps. 
      spencer graves
###########################
Brown, Cai and DasGupta (2001) Statistical Science, 16:  101-133 and 
(2002) Annals of Statistics, 30:  160-2001 
###########################

Uwe Ligges wrote:

> Federico Gherardini wrote:
>
>> Hi all,
>> Is there any R function to compute exact confidence limits for a 
>> Poisson distribution with a given Lambda?
>
>
> For sure you are looking for certain quantiles of the poisson 
> distribution? See ?Poisson.
>
> Uwe Ligges
>
>
>> Thanks in advance
>> Federico
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Robert.McGehee at geodecapital.com  Fri Jan 14 02:06:34 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Thu, 13 Jan 2005 20:06:34 -0500
Subject: [R] as.character methods
Message-ID: <67DCA285A2D7754280D3B8E88EB5480208CA11D2@MSGBOSCLB2WIN.DMN1.FMR.COM>

Professor Ripley, 
You are quite correct, I did have some S3/S4 confusion. Thank you. By
using an S3 method with the documented usage, I had no trouble writing
an appropriate as.character method.

I had also not absorbed the fact that S3 and S4 generics might (and do!)
have different argument lists, as I see that the S3 as.character generic
takes (x, ...), and the S4 generic takes just (x).

I'd also like to ask a follow-up question to clarify this difference.
The documentation for ?.BasicFunsList (first page of methods package
documentation) reads:

"Functions in R that are defined as '.Primitive(<name>)' are not
suitable for formal methods, because they lack the basic reflectance
property."

As "as.character"() calls .Primitive("as.character"), I read the above
sentence to say that S4 methods are not appropriate for this function.
But certainly S4 methods _can_ be defined for .Primitives, and I can
seemingly get the argument list for such a primitive with the
getGeneric() function. Is the point then that S4 cannot pass on extended
(...) argument list for some primitives (such as the message below), and
thus informal S3 methods are required in some cases? Or perhaps I'm
missing the point entirely. Any clarification is greatly appreciated.

Thanks,
Robert

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Thursday, January 13, 2005 5:58 PM
To: McGehee, Robert
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] as.character methods


You seem to be confusing `generic' with `S4 generic', and `method' with 
`S4 method'.

Note that all references to `generic' and `method' outside the `methods'

package are not to S4 concepts unless explicitly stated.

On Thu, 13 Jan 2005, McGehee, Robert wrote:

> Hello,
> ?as.character says that the as.character function is a generic with
> usage: as.character(x, ...). So, I want to create an S4 object with an
> as.character method following the above usage, but I get the below
error
> telling me that ... isn't in the generic for as.character.
>
>> setClass("tmp", "numeric")
>> setMethod("as.character", "tmp", function(x, ...) paste(x, c(...)))
> Error in rematchDefinition(definition, fdef, mnames, fnames,
signature)
> :
> 	Methods can add arguments to the generic only if "..." is an
> argument to the generic
>
> Am I reading the documentation incorrectly? How do I correctly pass
the
> ... object to the method for this "tmp" object?
>
> However I note that looking at the generic function, I see no mention
of
> ... (despite the documentation).
>> getGeneric("as.character")
> standardGeneric for "as.character" defined from package "base"
>
> function (x)
> standardGeneric("as.character", .Primitive("as.character"))
> <environment: 0145EDC4>
> Methods may be defined for arguments: x
>
>
> So, briefly, is the documentation wrong? Am I doing something wrong?
Can
> I create an as.character method and pass additional arguments to it as
I
> think I should be able to?

Briefly, No, yes, yes.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From umalvarez at fata.unam.mx  Fri Jan 14 03:00:12 2005
From: umalvarez at fata.unam.mx (Ulises M. Alvarez)
Date: Thu, 13 Jan 2005 20:00:12 -0600
Subject: [R] Installing R on Xandros 3.0
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E524@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E524@usrymx25.merck.com>
Message-ID: <41E727AC.7070709@fata.unam.mx>

Just for the sake of clarity:

Ubuntu does include developer tools ('apt-get install build-essential').

Liaw, Andy wrote:
> If I have to guess, `desktop Linux' such as Xandros do not come with
> compilers and other development tools, so compiling from source will be a
> bit of a challenge.  I recall having played with Corel Linux during its
> rather short lifetime (also Debian based), and was horrified to find it
> lacking GCC.  Apparently people who make up such distros have the same view
> as Mr. Gates that compilers need not be part of a desktop OS.
> 
> (If I'm not mistaken, Unbuntu has the same problem.  However, apt-get is
> _really_ easy...)
> 
> Cheers,
> Andy 
> 
> 
>>From: Marc Schwartz
>>
>>On Thu, 2005-01-13 at 16:15 -0500, Thomas W Volscho wrote:
>>
>>>Dear List,
>>>After obtaining a second-hand PC and because XP costs too 
>>
>>much, I installed Xandros 3.0 (based on Debian) but pretty 
>>easy to use if migrating from WinXP.
>>
>>>Does anyone know how to install R on this OS?
>>>
>>>Thank you for your time,
>>>Tom Volscho
>>
>>If it is Debian based, it seems that you should be able to use apt-get
>>to install base R and many of the CRAN packages that Dirk has packaged
>>up.
>>
>>Alternatively, you can download the source tarball and 
>>compile using the
>>typical:
>>
>>./configure
>>make
>>make install (as root)
>>
>>I would of course defer to Dirk and other Debian experts here.
>>
>>You might also want to look at Dirk's Quantian distribution, which is
>>based upon Knoppix and includes R:
>>
> 
> http://dirk.eddelbuettel.com/quantian.html
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From xianggui01 at yahoo.com  Fri Jan 14 03:35:35 2005
From: xianggui01 at yahoo.com (Xianggui QU)
Date: Thu, 13 Jan 2005 18:35:35 -0800 (PST)
Subject: [R] Questions on Inserting R graphs in latex!
Message-ID: <20050114023535.95652.qmail@web51803.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050113/d8886841/attachment.pl

From andy_liaw at merck.com  Fri Jan 14 03:48:27 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 13 Jan 2005 21:48:27 -0500
Subject: [R] Questions on Inserting R graphs in latex!
Message-ID: <3A822319EB35174CA3714066D590DCD50994E529@usrymx25.merck.com>

This is not an R problem, but LaTeX.  

Seems like the `slides' document class you're using doesn't recognize the
`figure' environment.  You should check that.

I seem to recall that you need to specify a driver option for graphicx, such
as `epsfig' for some such.  You should check the documentation for that
package.

Andy

> From: Xianggui QU
> 
> Hi,
> I try to insert R graphs into a latex file and I am using a 
> Texniccenter/MikTex combination in my Windows XP. Here are 
> the errors I got. Could someone give me some clues?
>  
> In R, I did
> > plot(sin(1:10), pch="+") 
> > dev.print(device=postscript, 
> "C:\\myFigure.eps",onefile=FALSE, horizontal=FALSE, paper="special") 
>  
> In latex, I did (myFigure.eps is in the same folder as my latex file)
> 
> \documentclass{slides} 
> \usepackage{graphicx} 
> \begin{document} 
> It can be seen in Figure \ref{fig:myFigure}.
> \begin{figure} 
>         \begin{center} 
>                 \includegraphics[scale=0.9]{myFigure} 
>         \end{center} 
>   \caption{test statement} 
>         \label{fig:myFigure} 
> \end{figure} 
> \end{document} 
>  
> I got th following errors while I used Ctrl+F7 to edit.
>  
> ! LaTeX Error: Environment figure undefined.
> See the LaTeX manual or LaTeX Companion for explanation.
> Type  H <return>  for immediate help.
>  ...                                              
>                                                   
> l.5 \begin{figure}
>                   
> Your command was ignored.
> Type  I <command> <return>  to replace it with another command,
> or  <return>  to continue without it.
> 
> ! LaTeX Error: File `myFigure' not found.
> See the LaTeX manual or LaTeX Companion for explanation.
> Type  H <return>  for immediate help.
>  ...                                              
>                                                   
> l.7 ...      \includegraphics[scale=0.9]{myFigure}
>                                                   
> I could not locate the file with any of these extensions:
> .png,.pdf,.jpg,.mps,.tif
> Try typing  <return>  to proceed.
> If that doesn't work, type  X <return>  to quit.
> 
> ! LaTeX Error: \caption outside float.
> See the LaTeX manual or LaTeX Companion for explanation.
> Type  H <return>  for immediate help.
>  ...                                              
>                                                   
> l.9   \caption
>               {test statement}
> You're in trouble here.  Try typing  <return>  to proceed.
> If that doesn't work, type  X <return>  to quit.
> 
> ! LaTeX Error: \begin{document} ended by \end{figure}.
> See the LaTeX manual or LaTeX Companion for explanation.
> Type  H <return>  for immediate help.
>  ...                                              
>                                                   
> l.11 \end{figure}
>                  
> Your command was ignored.
> Type  I <command> <return>  to replace it with another command,
> or  <return>  to continue without it.
>  
>  
> Thank you very much!
> Xianggui.
>  
>  
>  
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ggrothendieck at myway.com  Fri Jan 14 03:49:34 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 14 Jan 2005 02:49:34 +0000 (UTC)
Subject: [R] Finding seasonal peaks in a time series....
References: <e89bb7ac05011214182ea16125@mail.gmail.com>
Message-ID: <loom.20050114T034335-633@post.gmane.org>

Dr Carbon <drcarbon <at> gmail.com> writes:

: 
: I have a seasonal time series. I want to calculate the annual mean
: value of the time series at its peak
: 
:  (say the mean of the three values before the peak, the peak, and the
: three values after the peak).
: 
:  The peak of the time series might change cycle slightly from year to year.
: 
: # E.g.,
: nPts <- 254
: foo <- sin((2 * pi * 1/24) * 1:nPts)
: foo <- foo + rnorm(nPts, 0, 0.05)
: bar <- ts(foo, start = c(1980,3), frequency = 24)
: plot(bar)
: start(bar)
: end(bar)
: 
: # I want to find the peak value from each year, and then get the mean
: of the values on either side.
: # So, if the peak value in the year 1981 is
: max.in.1981 <- max(window(bar, start = c(1981,1), end = c(1981,24)))
: # e.g, cycle 7 or 8
: window(bar, start = c(1981,1), end = c(1981,24)) == max.in.1981
: # E.g. if the highest value in 1981 is in cycle 8 I want
: mean.in.1981 <- mean(window(bar, start = c(1981,5), end = c(1981,11)))
: plot(bar)
: points(ts(mean.in.1981, start = c(1981,8), frequency = 24), col =
: "red", pch = "+")
: 
:  Is there a way to "automate" this for each year.


Calculate the moving average of bar, which we call barma, and define a
function f which takes a two column structure, locates
the largest entry in column 1 and returns the corresponding
entry in column 2.  Use 'by' to apply f to the two columns, bar and barma
for each year.  Finally convert result back to a ts object.

barma <- filter(bar, rep(1,7)/7)
f <- function(bar) bar[which.max(bar[,1]),2]
barpeakavg <- by(cbind(bar, barma), floor(time(bar)+.0001), f)
barpeakavg.ts <- ts(barpeakavg, start = start(time(aggregate(bar))))

: 
:  How can I return the cycle of the max value by year?
: 

aggregate(bar, FUN = which.max)



From ggrothendieck at myway.com  Fri Jan 14 04:02:44 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 14 Jan 2005 03:02:44 +0000 (UTC)
Subject: [R] Finding seasonal peaks in a time series....
References: <e89bb7ac05011214182ea16125@mail.gmail.com>
	<loom.20050114T034335-633@post.gmane.org>
Message-ID: <loom.20050114T035829-935@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Dr Carbon <drcarbon <at> gmail.com> writes:
: 
: : 
: : I have a seasonal time series. I want to calculate the annual mean
: : value of the time series at its peak
: : 
: :  (say the mean of the three values before the peak, the peak, and the
: : three values after the peak).
: : 
: :  The peak of the time series might change cycle slightly from year to year.
: : 
: : # E.g.,
: : nPts <- 254
: : foo <- sin((2 * pi * 1/24) * 1:nPts)
: : foo <- foo + rnorm(nPts, 0, 0.05)
: : bar <- ts(foo, start = c(1980,3), frequency = 24)
: : plot(bar)
: : start(bar)
: : end(bar)
: : 
: : # I want to find the peak value from each year, and then get the mean
: : of the values on either side.
: : # So, if the peak value in the year 1981 is
: : max.in.1981 <- max(window(bar, start = c(1981,1), end = c(1981,24)))
: : # e.g, cycle 7 or 8
: : window(bar, start = c(1981,1), end = c(1981,24)) == max.in.1981
: : # E.g. if the highest value in 1981 is in cycle 8 I want
: : mean.in.1981 <- mean(window(bar, start = c(1981,5), end = c(1981,11)))
: : plot(bar)
: : points(ts(mean.in.1981, start = c(1981,8), frequency = 24), col =
: : "red", pch = "+")
: : 
: :  Is there a way to "automate" this for each year.
: 
: Calculate the moving average of bar, which we call barma, and define a
: function f which takes a two column structure, locates
: the largest entry in column 1 and returns the corresponding
: entry in column 2.  Use 'by' to apply f to the two columns, bar and barma
: for each year.  Finally convert result back to a ts object.
: 
: barma <- filter(bar, rep(1,7)/7)
: f <- function(bar) bar[which.max(bar[,1]),2]
: barpeakavg <- by(cbind(bar, barma), floor(time(bar)+.0001), f)
: barpeakavg.ts <- ts(barpeakavg, start = start(time(aggregate(bar))))
: 
: : 
: :  How can I return the cycle of the max value by year?
: : 
: 
: aggregate(bar, FUN = which.max)

Note that this will be off for the first year if that year does
not begin at the beginning of the cycle.

If thats a problem then use the previous solution but replace
barma with barcycle where barcycle <- cycle(bar)



From tfliao at uiuc.edu  Fri Jan 14 04:36:09 2005
From: tfliao at uiuc.edu (Tim F Liao)
Date: Thu, 13 Jan 2005 21:36:09 -0600
Subject: [R] GAM: Remedial measures
Message-ID: <b74c3e45.be5427a2.81ab400@expms6.cites.uiuc.edu>

Jean,

The standard treatment of overdispersed data when using the
Poisson distribution to model count data is to switch to the
negative binomial distribution.  Hope this helps,

Tim Liao

---- Original message ----
>Date: Thu, 13 Jan 2005 18:22:29 -0500
>From: "Jean G. Orelien" <jorelien at scimetrika.com>  
>Subject: [R] GAM: Remedial measures  
>To: <r-help at stat.math.ethz.ch>
>
>I fitted a GAM model with Poisson distribution to a data with
about 200
>observations.  I noticed that the plot of the residuals
versus fitted values
>show a trend.  Residuals tend to be lower for higher fitted
values. Because,
>I'm dealing with count data, I'm thinking that this might be
due to
>overdispersion.  Is there a way to account for overdispersion
in any of the
>packages MGCV or GAM?  
>
> 
>
>I welcome any suggestions that one may have on this topic.
>
> 
>
>Jean
>
> 
>
>________________
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From m.abdolell at utoronto.ca  Fri Jan 14 05:25:24 2005
From: m.abdolell at utoronto.ca (Mohamed Abdolell)
Date: Thu, 13 Jan 2005 23:25:24 -0500
Subject: [R] Installing R on Xandros 3.0
In-Reply-To: <552b0d54ec84.54ec84552b0d@huskymail.uconn.edu>
References: <552b0d54ec84.54ec84552b0d@huskymail.uconn.edu>
Message-ID: <1105676724.41e749b40eaea@webmail.utoronto.ca>

[1] go to http://packages.debian.org/testing/math/ and download the essential
r-base  packages to your hard drive
[2] install downloaded files using xandros networks
       File --> Install DEB file ...
[3] once it's working install remaining packages from within R using 
       install.packages(pkgs, lib, CRAN = getOption("CRAN"),
               contriburl = contrib.url(CRAN),
               method, available = NULL, destdir = NULL,
               installWithVers = FALSE, dependencies = FALSE)
     you'll need to read the help file on this ... but you'll have to specify

----

Quoting Thomas W Volscho <THOMAS.VOLSCHO at huskymail.uconn.edu>:

> Dear List,
> After obtaining a second-hand PC and because XP costs too much, I installed
> Xandros 3.0 (based on Debian) but pretty easy to use if migrating from
> WinXP.
> 
> Does anyone know how to install R on this OS?
> 
> Thank you for your time,
> Tom Volscho
> 
> ************************************        
> Thomas W. Volscho
> Graduate Student
> Dept. of Sociology U-2068
> University of Connecticut
> Storrs, CT 06269
> Phone: (860) 486-3882
> http://vm.uconn.edu/~twv00001
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From m.abdolell at utoronto.ca  Fri Jan 14 05:33:33 2005
From: m.abdolell at utoronto.ca (Mohamed Abdolell)
Date: Thu, 13 Jan 2005 23:33:33 -0500
Subject: [R] Installing R on Xandros 3.0
In-Reply-To: <552b0d54ec84.54ec84552b0d@huskymail.uconn.edu>
References: <552b0d54ec84.54ec84552b0d@huskymail.uconn.edu>
Message-ID: <1105677213.41e74b9d298b2@webmail.utoronto.ca>

Hi Tom,

[1] go to http://packages.debian.org/testing/math/ and download r-base and other
essential packages to your computer
[2] install packages using Xandros Networks; from the pull-down menu select
File-->Install DEB file...
[3] once you have a functional R 2.0.1 working install the rest of the packages
you want directly from within R using install.packages (see help for this); this
is best since all the dependencies are sorted out automatically

HTH.

- Mohamed


---------------------------------------


Quoting Thomas W Volscho <THOMAS.VOLSCHO at huskymail.uconn.edu>:

> Dear List,
> After obtaining a second-hand PC and because XP costs too much, I installed
> Xandros 3.0 (based on Debian) but pretty easy to use if migrating from
> WinXP.
> 
> Does anyone know how to install R on this OS?
> 
> Thank you for your time,
> Tom Volscho
> 
> ************************************        
> Thomas W. Volscho
> Graduate Student
> Dept. of Sociology U-2068
> University of Connecticut
> Storrs, CT 06269
> Phone: (860) 486-3882
> http://vm.uconn.edu/~twv00001
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From e.catchpole at adfa.edu.au  Fri Jan 14 06:06:01 2005
From: e.catchpole at adfa.edu.au (ecatchpole)
Date: Fri, 14 Jan 2005 16:06:01 +1100
Subject: [R] glmm multinomial?
Message-ID: <41E75339.1020901@adfa.edu.au>

I'm looking for something like Brian Ripley's glmmPQL that will handle 
multinomial data. Does anyone know of anything?

Thanks,  Ted.

-- 
Dr E.A. Catchpole  
Visiting Fellow
Univ of New South Wales at ADFA, Canberra, Australia
and University of Kent, Canterbury, England
- e.catchpole at adfa.edu.au
- www.ma.adfa.edu.au/~eac          
- fax: +61 2 6268 8687		   
- ph:  +61 2 6268 8895



From MSchwartz at MedAnalytics.com  Fri Jan 14 06:16:40 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 13 Jan 2005 23:16:40 -0600
Subject: [R] Questions on Inserting R graphs in latex!
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E529@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E529@usrymx25.merck.com>
Message-ID: <1105679800.14610.22.camel@horizons.localdomain>

On Thu, 2005-01-13 at 21:48 -0500, Liaw, Andy wrote:
> This is not an R problem, but LaTeX.  
> 
> Seems like the `slides' document class you're using doesn't recognize the
> `figure' environment.  You should check that.
> 
> I seem to recall that you need to specify a driver option for graphicx, such
> as `epsfig' for some such.  You should check the documentation for that
> package.
> 
> Andy


I believe that Andy is correct here. The slides and letter classes do
not recognize the figure environment.

In checking some of the other popular slide classes (ie. Seminar,
Prosper and Beamer), I don't think that they support the figure
environment either.

The figure environment is better for longer documents, where it can
become difficult to keep track of figure numbering and referencing.

For shorter slide based presentations, it is easier to 'hard code' the
numbers and captions, etc. where these are appropriate to use.

More information on inserting EPS graphics into LaTeX documents is
available in:

http://www.ctan.org/tex-archive/info/epslatex.pdf

Additional information on the other slide classes are at:

http://tug.org/applications/Seminar/
http://prosper.sourceforge.net/
http://latex-beamer.sourceforge.net/

HTH,

Marc Schwartz



From edd at debian.org  Fri Jan 14 06:24:23 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 13 Jan 2005 23:24:23 -0600
Subject: [R] Installing R on Xandros 3.0
In-Reply-To: <1105677213.41e74b9d298b2@webmail.utoronto.ca>
References: <552b0d54ec84.54ec84552b0d@huskymail.uconn.edu>
	<1105677213.41e74b9d298b2@webmail.utoronto.ca>
Message-ID: <20050114052423.GA18254@sonny.eddelbuettel.com>

On Thu, Jan 13, 2005 at 11:33:33PM -0500, Mohamed Abdolell wrote:
> Hi Tom,
> 
> [1] go to http://packages.debian.org/testing/math/ and download r-base and other
> essential packages to your computer
> [2] install packages using Xandros Networks; from the pull-down menu select
> File-->Install DEB file...

That's is one way, but I'd prefer another one. The main alternative would be
to point apt-get to the proper Debian sources, then use 'apt-get update' and
'apt-get install' as it opens the door to the other 40 or 50 CRAN packages a
few of use have packaged.

I can't help with the Xandros specifics; under Debian you'd edit
/etc/apt/sources.list -- but you need to know what Debian flavour
corresponds to your Xandros version. If it is Debian testing, you're set.

Maybe a local Linux user group at U Conn can help you with these steps.

> [3] once you have a functional R 2.0.1 working install the rest of the packages
> you want directly from within R using install.packages (see help for this); this
> is best since all the dependencies are sorted out automatically

But it doesn't deal with Build-Depends; installing binary Debian packages
where available is a lot easier.

Hope this helps, and good luck, Dirk

-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From p.dalgaard at biostat.ku.dk  Fri Jan 14 09:31:42 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Jan 2005 09:31:42 +0100
Subject: [R] Questions on Inserting R graphs in latex!
In-Reply-To: <1105679800.14610.22.camel@horizons.localdomain>
References: <3A822319EB35174CA3714066D590DCD50994E529@usrymx25.merck.com>
	<1105679800.14610.22.camel@horizons.localdomain>
Message-ID: <x2acrc1kep.fsf@biostat.ku.dk>

Marc Schwartz <MSchwartz at medanalytics.com> writes:

> In checking some of the other popular slide classes (ie. Seminar,
> Prosper and Beamer), I don't think that they support the figure
> environment either.
> 
> The figure environment is better for longer documents, where it can
> become difficult to keep track of figure numbering and referencing.
> 
> For shorter slide based presentations, it is easier to 'hard code' the
> numbers and captions, etc. where these are appropriate to use.

Not so much that, but if you need a reference, it is usually easier just
to use the slide number, rather than have multiple numbering systems.

And of course the whole idea of floating figures, that appear on a page
"not too far" from their insertion point, doesn't carry over to slides
very well.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From mmetz at astro.uni-bonn.de  Fri Jan 14 09:37:01 2005
From: mmetz at astro.uni-bonn.de (Manuel Metz)
Date: Fri, 14 Jan 2005 09:37:01 +0100
Subject: [R] kde2d and borders
Message-ID: <41E784AD.1020703@astro.uni-bonn.de>

Hallo,
I want to use kde2d to visualize data on a sphere given in spherical 
coordinates. Now the problem is, that "phi == 2*pi = 0", so in principal 
I have to connect (in a graphical view) the left and right border of my 
plot (and the bottom and top). Has anyone any idea how to do that ?

Thanks,
Manuel

-- 
-------------------------------------
  Manuel Metz
  Sternwarte der Universitaet Bonn
  Auf dem Huegel 71 (room 3.06)
  D - 53121 Bonn

  E-Mail: mmetz at astro.uni-bonn.de
  Phone:  (+49) 228 / 73-3660
  Fax:    (+49) 228 / 73-3672



From ripley at stats.ox.ac.uk  Fri Jan 14 09:38:06 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 14 Jan 2005 08:38:06 +0000 (GMT)
Subject: [R] as.character methods
In-Reply-To: <67DCA285A2D7754280D3B8E88EB5480208CA11D2@MSGBOSCLB2WIN.DMN1.FMR.COM>
References: <67DCA285A2D7754280D3B8E88EB5480208CA11D2@MSGBOSCLB2WIN.DMN1.FMR.COM>
Message-ID: <Pine.LNX.4.61.0501140837260.18523@gannet.stats>

I am moving this thread to R-devel, which is probably where it should have 
started and it is getting even more technical.

On Thu, 13 Jan 2005, McGehee, Robert wrote:

> Professor Ripley,
> You are quite correct, I did have some S3/S4 confusion. Thank you. By
> using an S3 method with the documented usage, I had no trouble writing
> an appropriate as.character method.
>
> I had also not absorbed the fact that S3 and S4 generics might (and do!)
> have different argument lists, as I see that the S3 as.character generic
> takes (x, ...), and the S4 generic takes just (x).
>
> I'd also like to ask a follow-up question to clarify this difference.
> The documentation for ?.BasicFunsList (first page of methods package
> documentation) reads:
>
> "Functions in R that are defined as '.Primitive(<name>)' are not
> suitable for formal methods, because they lack the basic reflectance
> property."
>
> As "as.character"() calls .Primitive("as.character"), I read the above
> sentence to say that S4 methods are not appropriate for this function.
> But certainly S4 methods _can_ be defined for .Primitives, and I can
> seemingly get the argument list for such a primitive with the
> getGeneric() function. Is the point then that S4 cannot pass on extended
> (...) argument list for some primitives (such as the message below), and
> thus informal S3 methods are required in some cases? Or perhaps I'm
> missing the point entirely. Any clarification is greatly appreciated.
>
> Thanks,
> Robert
>
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Thursday, January 13, 2005 5:58 PM
> To: McGehee, Robert
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] as.character methods
>
>
> You seem to be confusing `generic' with `S4 generic', and `method' with
> `S4 method'.
>
> Note that all references to `generic' and `method' outside the `methods'
>
> package are not to S4 concepts unless explicitly stated.
>
> On Thu, 13 Jan 2005, McGehee, Robert wrote:
>
>> Hello,
>> ?as.character says that the as.character function is a generic with
>> usage: as.character(x, ...). So, I want to create an S4 object with an
>> as.character method following the above usage, but I get the below
> error
>> telling me that ... isn't in the generic for as.character.
>>
>>> setClass("tmp", "numeric")
>>> setMethod("as.character", "tmp", function(x, ...) paste(x, c(...)))
>> Error in rematchDefinition(definition, fdef, mnames, fnames,
> signature)
>> :
>> 	Methods can add arguments to the generic only if "..." is an
>> argument to the generic
>>
>> Am I reading the documentation incorrectly? How do I correctly pass
> the
>> ... object to the method for this "tmp" object?
>>
>> However I note that looking at the generic function, I see no mention
> of
>> ... (despite the documentation).
>>> getGeneric("as.character")
>> standardGeneric for "as.character" defined from package "base"
>>
>> function (x)
>> standardGeneric("as.character", .Primitive("as.character"))
>> <environment: 0145EDC4>
>> Methods may be defined for arguments: x
>>
>>
>> So, briefly, is the documentation wrong? Am I doing something wrong?
> Can
>> I create an as.character method and pass additional arguments to it as
> I
>> think I should be able to?
>
> Briefly, No, yes, yes.
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jan 14 09:48:16 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 14 Jan 2005 08:48:16 +0000 (GMT)
Subject: [R] glmm multinomial?
In-Reply-To: <41E75339.1020901@adfa.edu.au>
References: <41E75339.1020901@adfa.edu.au>
Message-ID: <Pine.LNX.4.61.0501140845140.18523@gannet.stats>

On Fri, 14 Jan 2005, ecatchpole wrote:

> I'm looking for something like Brian Ripley's glmmPQL that will handle 
> multinomial data. Does anyone know of anything?

It's a lot more complicated conceptually.  A multinomial model has K-1 
linear predictors which should probably have a correlated joint 
distribution.  We don't even have software for multivariate linear mixed 
models, AFAIK.

Would a surrogate Poisson model be good enough? glmmPQL can fit those.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jan 14 09:58:01 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 14 Jan 2005 08:58:01 +0000 (GMT)
Subject: [R] kde2d and borders
In-Reply-To: <41E784AD.1020703@astro.uni-bonn.de>
References: <41E784AD.1020703@astro.uni-bonn.de>
Message-ID: <Pine.LNX.4.61.0501140853300.18523@gannet.stats>

On Fri, 14 Jan 2005, Manuel Metz wrote:

> I want to use kde2d to visualize data on a sphere given in spherical 
> coordinates. Now the problem is, that "phi == 2*pi = 0", so in principal I 
> have to connect (in a graphical view) the left and right border of my plot 
> (and the bottom and top). Has anyone any idea how to do that ?

This is discussed on the same page (130) as kde2d in the book it 
supports.  The posting guide does say

     * If the function is from a package accompanying a book, e.g., the
       MASS package, consult the book before posting

so how on earth did you miss that?

There are other ways to do this: see e.g. package sm.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From s.o.nyangoma at rug.nl  Fri Jan 14 10:18:26 2005
From: s.o.nyangoma at rug.nl (Stephen Nyangoma)
Date: Fri, 14 Jan 2005 10:18:26 +0100
Subject: [R] summing subsets of rows matrices
Message-ID: <003401c4fa1a$02d56650$9f8f7d81@NIJENBOR13TN3U>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050114/c476a38b/attachment.pl

From h.andersson at nioo.knaw.nl  Fri Jan 14 10:18:54 2005
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Fri, 14 Jan 2005 10:18:54 +0100
Subject: [R] Questions on Inserting R graphs in latex!
In-Reply-To: <20050114023535.95652.qmail@web51803.mail.yahoo.com>
References: <20050114023535.95652.qmail@web51803.mail.yahoo.com>
Message-ID: <cs82pq$ofe$1@sea.gmane.org>

An additional problem separate from it not recognizing the figure 
environment is that I guess you ran pdflatex (to get a .pdf file and not 
a .dvi) and this indeed accepts only files in the formats below.

Try:

pdf(file="c:/myfigure.pdf")
plot(sin(1:10), pch="+")
dev.off()

Xianggui QU wrote:
> Hi,
....
>                                                   
> l.7 ...      \includegraphics[scale=0.9]{myFigure}
>                                                   
> I could not locate the file with any of these extensions:
> .png,.pdf,.jpg,.mps,.tif



From bxc at steno.dk  Fri Jan 14 10:45:36 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Fri, 14 Jan 2005 10:45:36 +0100
Subject: [R] summing subsets of rows matrices
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FE507@exdkba022.novo.dk>

What you need is matrix multiplication:

rbind( c(1,1,0,0,0,0),
       c(0,0,1,1,0,0),
       c(0,0,0,0,1,1) ) %*% M

Bendix
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Stephen Nyangoma
> Sent: Friday, January 14, 2005 10:18 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] summing subsets of rows matrices
> 
> 
> I have a large data matrix (4460X3500) and I want to sum row 
> subsets in groups of 10 (bin). Is there an efficient way to 
> do this in R without using loops. Looping takes forever to 
> perform this task!
> 
> For example suppose we have the matrix
> > matrix(1:12,6,2)
>      [,1] [,2]
> [1,]    1    7
> [2,]    2    8
> [3,]    3    9
> [4,]    4   10
> [5,]    5   11
> [6,]    6   12
> 
> my problem is to sdum for example:
> 1. the first and second row 
> 2. the third and fourth
> and
> 3. the fifth and the sixth.
> 
> To obtain
> 
>     [,1] [,2]
> [1,]    3   15
> [2,]    7   19
> [3,]   11   23
> 
> 
> 
> Thank you.
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From commercial at s-boehringer.de  Fri Jan 14 10:51:19 2005
From: commercial at s-boehringer.de (commercial@s-boehringer.de)
Date: Fri, 14 Jan 2005 10:51:19 +0100 (CET)
Subject: [R] Porting from Linux to Windows
Message-ID: <53960.132.252.149.100.1105696279.squirrel@webmail.loomes.de>

I intend to port an R project from Linux to Windows.
It involves C code that is loaded via dyn.load().
I could manage to produce a 'dll' File using cygwin which seems to be
o.k.

Now, using dyn.load("pcr.dll") i get:

Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library
"c:/cygwin/home/pingu/rt-pcr/pcr.dll":
  LoadLibrary failure:  Invalid access to memory location.

Besides modifying the makefile no changes have been made to the source
code. This is the Makefile (the relevant target is winlib):
---
RHOME="C:/Program Files/R/rw1090"

SOURCE=lambert.c
OBJ=lambert.o brent.o nrutil.o pcr.o
#CFLAGS=-I/usr/lib/R/include -g -O2
CFLAGS+=-I${RHOME}/src/include -mno-cygwin
CFLAGS+=-I/usr/lib/R/include -g -O2
OBJT=${OBJ} test.o
OBJR=${OBJ} pcr-R.o

${OBJ} : ${SOURCE}
#	cc -I/usr/lib/R/include -o $@.o $@.c

test : test.o ${OBJ}
	cc -g -o test ${OBJT} -lc -lm

lib : ${OBJR}
	cc -o pcr.so ${OBJR} -lc -lm -shared

# we may not use ld because the -mno-cygwin cannot be passed
# which is required
winlib : ${OBJR}
	cc -mno-cygwin  -o pcr.dll ${OBJR} ${RHOME}/bin/R.dll -lc -lm -shared
---

Does anybody have some clue? I am using gcc v3.3.3 and R1.9.0 on
Windows. If there is a recipe somewhere that I have missed I would be
most grateful.

Thank you very much,

	Stefan



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Jan 14 11:01:56 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 14 Jan 2005 11:01:56 +0100
Subject: [R] summing subsets of rows matrices
References: <003401c4fa1a$02d56650$9f8f7d81@NIJENBOR13TN3U>
Message-ID: <012801c4fa20$149dcd40$0540210a@www.domain>

Hi Stephen,

you could try something like this:

mat <- matrix(rnorm(4460*3500), 4460, 3500)
###############
d <- dim(mat)
bin <- 10
system.time(out <- lapply(split(mat,rep(seq(1, d[1]/bin), each=bin)), 
function(x){dim(x) <- c(bin, d[2]); colSums(x)}), gcFirst=TRUE)
out <- matrix(unlist(out, use.names=FALSE), ncol=d[2], byrow=TRUE)

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Stephen Nyangoma" <s.o.nyangoma at rug.nl>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, January 14, 2005 10:18 AM
Subject: [R] summing subsets of rows matrices


>I have a large data matrix (4460X3500) and I want to sum row subsets 
>in groups of 10 (bin). Is there an efficient way to do this in R 
>without using loops. Looping takes forever to perform this task!
>
> For example suppose we have the matrix
>> matrix(1:12,6,2)
>     [,1] [,2]
> [1,]    1    7
> [2,]    2    8
> [3,]    3    9
> [4,]    4   10
> [5,]    5   11
> [6,]    6   12
>
> my problem is to sdum for example:
> 1. the first and second row
> 2. the third and fourth
> and
> 3. the fifth and the sixth.
>
> To obtain
>
>    [,1] [,2]
> [1,]    3   15
> [2,]    7   19
> [3,]   11   23
>
>
>
> Thank you.
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Fri Jan 14 11:03:45 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 14 Jan 2005 10:03:45 +0000 (GMT)
Subject: [R] summing subsets of rows matrices
In-Reply-To: <003401c4fa1a$02d56650$9f8f7d81@NIJENBOR13TN3U>
References: <003401c4fa1a$02d56650$9f8f7d81@NIJENBOR13TN3U>
Message-ID: <Pine.LNX.4.61.0501140958390.23855@gannet.stats>

Here's a simple and efficient way

A <- matrix(1:12,6,2)
nbin <- 2

dim(A) <- c(nbin, nrow(A)/nbin, 2)
B <- colSums(A)
dim(B) <- dim(A)[-1]
B
      [,1] [,2]
[1,]    3   15
[2,]    7   19
[3,]   11   23


On Fri, 14 Jan 2005, Stephen Nyangoma wrote:

> I have a large data matrix (4460X3500) and I want to sum row subsets in 
> groups of 10 (bin). Is there an efficient way to do this in R without 
> using loops. Looping takes forever to perform this task!
>
> For example suppose we have the matrix
>> matrix(1:12,6,2)
>     [,1] [,2]
> [1,]    1    7
> [2,]    2    8
> [3,]    3    9
> [4,]    4   10
> [5,]    5   11
> [6,]    6   12
>
> my problem is to sdum for example:
> 1. the first and second row
> 2. the third and fourth
> and
> 3. the fifth and the sixth.
>
> To obtain
>
>    [,1] [,2]
> [1,]    3   15
> [2,]    7   19
> [3,]   11   23

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From simon at stats.gla.ac.uk  Fri Jan 14 11:16:05 2005
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Fri, 14 Jan 2005 10:16:05 +0000 (GMT)
Subject: [R] GAM: Remedial measures
In-Reply-To: <NC-TDA03mQwKLAffPc900000748@mail.nctda.org>
References: <NC-TDA03mQwKLAffPc900000748@mail.nctda.org>
Message-ID: <Pine.LNX.4.58.0501141011420.5441@moon.stats.gla.ac.uk>

> I fitted a GAM model with Poisson distribution to a data with about 200
> observations.  I noticed that the plot of the residuals versus fitted values
> show a trend.  Residuals tend to be lower for higher fitted values. Because,
> I'm dealing with count data, I'm thinking that this might be due to
> overdispersion.  Is there a way to account for overdispersion in any of the
> packages MGCV or GAM?  

You can `allow for' overdispersion in mgcv::gam by using the quasipoisson 
family, or setting scale to -1 in the gam call. In a straight GLM this 
would make no difference to the residual plots, since the scale parameter 
does not change the coefficient estimates. However, things are different 
for a GAM with automatic smoothness estimations, since the scale parameter 
does influence the smoothing parameter estimation criterion. Another 
possibility is to use the negative binomial family from the MASS library, 
and a third is to use the quasi family.

Simon
_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From maechler at stat.math.ethz.ch  Fri Jan 14 11:18:57 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 14 Jan 2005 11:18:57 +0100
Subject: [R] scan; 1 items
In-Reply-To: <p06100500bdd10116e3ab@[83.132.29.55]>
References: <p06100500bdd10116e3ab@[83.132.29.55]>
Message-ID: <16871.40081.919361.93045@stat.math.ethz.ch>

>>>>> "Tiago" == Tiago R Magalhaes <tiago17 at socrates.Berkeley.EDU>
>>>>>     on Mon, 29 Nov 2004 16:41:30 +0000 writes:

    Tiago> HI
    Tiago> the scan function when only one item is read says:

    Tiago> scan()
    Tiago> 1: 3.3
    Tiago> 2:
    Tiago> Read 1 items
    Tiago> [1] 3.3

    Tiago> I hope my english is not playing a trick on me, but "1 items" sounds 
    Tiago> very strange
    Tiago> this makes me feel very anal, and it's really not important and I 
    Tiago> apologize for it but here it goes anyway

and since you *have* brought it up, and I've stumbled about that
e-mail again, I've corrected it now {for R-devel only}.

Martin Maechler



From p.dalgaard at biostat.ku.dk  Fri Jan 14 11:21:12 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Jan 2005 11:21:12 +0100
Subject: [R] Porting from Linux to Windows
In-Reply-To: <53960.132.252.149.100.1105696279.squirrel@webmail.loomes.de>
References: <53960.132.252.149.100.1105696279.squirrel@webmail.loomes.de>
Message-ID: <x2ekgowbtz.fsf@biostat.ku.dk>

commercial at s-boehringer.de writes:

> I intend to port an R project from Linux to Windows.
> It involves C code that is loaded via dyn.load().
> I could manage to produce a 'dll' File using cygwin which seems to be
> o.k.
> 
> Now, using dyn.load("pcr.dll") i get:
> 
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library
> "c:/cygwin/home/pingu/rt-pcr/pcr.dll":
>   LoadLibrary failure:  Invalid access to memory location.
> 
> Besides modifying the makefile no changes have been made to the source
> code. This is the Makefile (the relevant target is winlib):
> ---
> RHOME="C:/Program Files/R/rw1090"
> 
> SOURCE=lambert.c
> OBJ=lambert.o brent.o nrutil.o pcr.o
> #CFLAGS=-I/usr/lib/R/include -g -O2
> CFLAGS+=-I${RHOME}/src/include -mno-cygwin
> CFLAGS+=-I/usr/lib/R/include -g -O2
> OBJT=${OBJ} test.o
> OBJR=${OBJ} pcr-R.o
> 
> ${OBJ} : ${SOURCE}
> #	cc -I/usr/lib/R/include -o $@.o $@.c
> 
> test : test.o ${OBJ}
> 	cc -g -o test ${OBJT} -lc -lm
> 
> lib : ${OBJR}
> 	cc -o pcr.so ${OBJR} -lc -lm -shared
> 
> # we may not use ld because the -mno-cygwin cannot be passed
> # which is required
> winlib : ${OBJR}
> 	cc -mno-cygwin  -o pcr.dll ${OBJR} ${RHOME}/bin/R.dll -lc -lm -shared
> ---
> 
> Does anybody have some clue? I am using gcc v3.3.3 and R1.9.0 on
> Windows. If there is a recipe somewhere that I have missed I would be
> most grateful.

You're definitely missing R CMD SHLIB for a start. DLLs are more
complicated to make than .so files because they require explicit
export files and such. 

This has all been packaged up in a user friendly (as far as Windows
allows) set of development tools and whatnot. The place to start
reading is readme.packages in the windows distribution, also available
from e.g.

https://svn.r-project.org/R/trunk/src/gnuwin32/README.packages 

As the comment at the start of the document indicates, you should take
care to read the whole text and follow the instructions very
precisely. People often get in trouble by reading what they think
it says and not what it actually says...

If your original project is in the form of a valid CRAN package, and
all the build tools are in place on windows, the porting effort can be
as little as "R CMD build --binary mysrc.tar.gz".

You probably also want to read the "Writing R Extensions" document,
which also ships with R and is on CRAN via

http://cran.r-project.org/manuals.html

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From cg.pettersson at evp.slu.se  Fri Jan 14 11:26:24 2005
From: cg.pettersson at evp.slu.se (CG Pettersson)
Date: Fri, 14 Jan 2005 11:26:24 +0100
Subject: [R] Fine tuning scatterplot() (car package)
Message-ID: <200501141026.j0EAQOAn029542@mail1.slu.se>

Hello all!

System: R2.0.1, W2k. 
All packages apdated with update.packages().
I use the default graphic device for my system (no active choice).

I?m trying to fine-tune scatterplot() graphs (package car), for
publication.
I want to control plotting symbols and colours to match plots from
other, less flexible, systems. I also need to make all text and
symbols bigger, to enable printing of the plots in smaller scale than
the original outputs.

My strategy has been to start in Rcmdr, using the command line
produced there and rerun variants of the scatterplot() call directly 
in the console.  

After consulting MASS and CAR, the colour and characters were easy to
fix by overrunning the defaults with col=c() and pch=c(). 

But the size of letters and figures beats me so far. I?ve tried cex
and csi 
in the same positiones as col and pch on the line. "cex" doesn?t do
anything as far as I can see, "csi" produces warning messages (which
is some sort of effect!) but does nothing on the size.

Trying to tune with these commands inside xlab and ylab calls (based
on the defaults from the documentation) results in unspecified syntax
error messages.

My code in the console is this for the moment:

> scatterplot(Kernel.protein~TC.OS | Year, reg.line=FALSE, 
        smooth=FALSE, labels=FALSE, boxplots=FALSE, span=0.5, 
        by.groups=TRUE, col=c("","red","blue","black"), pch=c(15,4,5),
 
        data=ecpa.f)

Which produces nice plots, but with too small symbols, letters an
numbers.

Thanks!
/CG

CG Pettersson, MSci, PhD Stud.
Swedish University of Agricultural Sciences
Dep. of Ecology and Crop Production. Box 7043
SE-750 07 Uppsala



From michael.watson at bbsrc.ac.uk  Fri Jan 14 12:20:01 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 14 Jan 2005 11:20:01 -0000
Subject: [R] Replacing NAs in a data frame using is.na() fails if there are
	no NAs
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B934@iahce2knas1.iah.bbsrc.reserved>

Hi

This is a difference between the way matrices and data frames work I
guess.  I want to replace the NA values in a data frame by 0, and the
code works as long as the data frame in question actually includes an NA
value.  If it doesn't, there is an error:

df <- data.frame(c1=c(1,1,1),c2=c(2,2,NA))
df[is.na(df)] <- 0
df

df <- data.frame(c1=c(1,1,1),c2=c(2,2,2))
df[is.na(df)] <- 0
Df

Any help would be appreciated.  I could just convert the data frame to a
matrix, execute the code, then convert it back to a data frame, but that
appears long winded.

Thanks
Mick



From h.andersson at nioo.knaw.nl  Fri Jan 14 12:37:43 2005
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Fri, 14 Jan 2005 12:37:43 +0100
Subject: [R] spreadsheet addiction
In-Reply-To: <41D92469.1010608@burns-stat.com>
References: <41D92469.1010608@burns-stat.com>
Message-ID: <cs8au2$edk$1@sea.gmane.org>

I am moving to using R more and more as a computational platform and
I use Microsoft Excel at the moment in exactly the way you describe in 
WRITING ASCII FILES as a staging area for data, to gather data and make 
simple calculations. I've have experienced problems with saving to .csv 
files, and not only using scientific notation. The problem occurs also 
in normal notation if you choose to show only some decimals.

That is a serious problem, what is the remedy?

To always avoid showing only some decimals and avoid scientific notation 
or does someone have a  better solution, like a global option, Save with 
full represention to .csv.

I'm open for all sorts of suggestions, including ditching Excel...and 
use ???

Cheers, Henrik

Patrick Burns wrote:
> There's a new page on the Burns Statistics website
> http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html
> that looks at spreadsheets from a quality assurance perspective. It
> presents R as a suitable alternative to spreadsheets.  Also there are
> several specific problems with Excel that are highlighted, including
> the status of statistical functionality in Excel.
> 
> Patrick Burns
> 
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From blindglobe at gmail.com  Fri Jan 14 12:39:44 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Fri, 14 Jan 2005 12:39:44 +0100
Subject: [R] Porting from Linux to Windows
In-Reply-To: <53960.132.252.149.100.1105696279.squirrel@webmail.loomes.de>
References: <53960.132.252.149.100.1105696279.squirrel@webmail.loomes.de>
Message-ID: <1abe3fa90501140339445530a8@mail.gmail.com>

Don't mix R and Cygwin, at least not this year.

Peter's advice is spot on, for the rest.

best,
-tony



On Fri, 14 Jan 2005 10:51:19 +0100 (CET), commercial at s-boehringer.de
<commercial at s-boehringer.de> wrote:
> I intend to port an R project from Linux to Windows.
> It involves C code that is loaded via dyn.load().
> I could manage to produce a 'dll' File using cygwin which seems to be
> o.k.
> 
> Now, using dyn.load("pcr.dll") i get:
> 
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>        unable to load shared library
> "c:/cygwin/home/pingu/rt-pcr/pcr.dll":
>  LoadLibrary failure:  Invalid access to memory location.
> 
> Besides modifying the makefile no changes have been made to the source
> code. This is the Makefile (the relevant target is winlib):
> ---
> RHOME="C:/Program Files/R/rw1090"
> 
> SOURCE=lambert.c
> OBJ=lambert.o brent.o nrutil.o pcr.o
> #CFLAGS=-I/usr/lib/R/include -g -O2
> CFLAGS+=-I${RHOME}/src/include -mno-cygwin
> CFLAGS+=-I/usr/lib/R/include -g -O2
> OBJT=${OBJ} test.o
> OBJR=${OBJ} pcr-R.o
> 
> ${OBJ} : ${SOURCE}
> #       cc -I/usr/lib/R/include -o $@.o $@.c
> 
> test : test.o ${OBJ}
>        cc -g -o test ${OBJT} -lc -lm
> 
> lib : ${OBJR}
>        cc -o pcr.so ${OBJR} -lc -lm -shared
> 
> # we may not use ld because the -mno-cygwin cannot be passed
> # which is required
> winlib : ${OBJR}
>        cc -mno-cygwin  -o pcr.dll ${OBJR} ${RHOME}/bin/R.dll -lc -lm -shared
> ---
> 
> Does anybody have some clue? I am using gcc v3.3.3 and R1.9.0 on
> Windows. If there is a recipe somewhere that I have missed I would be
> most grateful.
> 
> Thank you very much,
> 
>        Stefan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From benner at dkfz-heidelberg.de  Fri Jan 14 12:47:24 2005
From: benner at dkfz-heidelberg.de (Axel Benner)
Date: Fri, 14 Jan 2005 12:47:24 +0100
Subject: [R] 2nd Workshop "Ensemble Methods", Tuebingen (Germany)
Message-ID: <41E7B14C.3030006@dkfz.de>

2nd Workshop "Ensemble Methods"
Max Planck Institute Tuebingen, Germany
March 4-5, 2005

The second workshop on ensemble methods will take place at the Max
Planck Institute Tuebingen (Germany) on March 4-5, 2005. This workshop
is jointly organised by the German working groups "Computational
Statistics" (IBS-DR) and "Statistical Computing" (GMDS) as well as the
Austrian Association for Statistical Computing.

The workshop aims at discussing further theoretical and practical
developments for prediction models such as boosting, random forests or
support vector machines. The workshop tries to bring together the
machine learning view with the statistical view on ensemble methods.

By now, the following talks have been scheduled:
* Gilles Blanchard: Consistency results for Boosting
* Peter Buehlmann: tba
* Gunnar Raetsch: Boosting SVMs: a new way for multiple kernel learning
* Koji Tsuda: Matrix exponentiated Gradients

Participants can attend the workshop free of charge, the number of
participants is restricted to 30 persons.

For registration and submission of abstracts, please send an email
message to Gunnar.Raetsch at tuebingen.mpg.de

For further information, please visit the webpage at
http://www.imbe.med.uni-erlangen.de/~hothorn/ensembleWS2005.html

---

Axel Benner, DKFZ Heidelberg
Torsten Hothorn, Universitaet Erlangen-Nuernberg
Gunnar Raetsch, MPI Tuebingen



From B.Rowlingson at lancaster.ac.uk  Fri Jan 14 12:49:47 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 14 Jan 2005 11:49:47 +0000
Subject: [R] Replacing NAs in a data frame using is.na() fails if there
	are	no NAs
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121B934@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121B934@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <41E7B1DB.3090103@lancaster.ac.uk>

michael watson (IAH-C) wrote:

> Any help would be appreciated.  I could just convert the data frame to a
> matrix, execute the code, then convert it back to a data frame, but that
> appears long winded.

  Slightly less long-winded (but probably a worse solution than some 
R-guru is about to give you) would be to stick a row of NAs at the end 
of the dataframe, do your replacement, then remove the last row. This 
slightly reminds me of the old joke about algorithms for hunting 
elephants in Africa, which involve placing a known elephant at the Cape 
Of Good Hope so that the algorithm is guaranteed to terminate, like all 
good algorithms should...

  But probably better to test for NAs in the dataframe beforehand:

    if(any(is.na(f)))f[is.na(f)]=0


Baz



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Jan 14 12:50:44 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 14 Jan 2005 12:50:44 +0100
Subject: [R] Replacing NAs in a data frame using is.na() fails if there
	areno NAs
References: <8975119BCD0AC5419D61A9CF1A923E950121B934@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <016701c4fa2f$47ba86f0$0540210a@www.domain>

Hi Mick,

try the following:

dat[] <- lapply(dat, function(x) ifelse(is.na(x), 0, x))
dat

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, January 14, 2005 12:20 PM
Subject: [R] Replacing NAs in a data frame using is.na() fails if 
there areno NAs


> Hi
>
> This is a difference between the way matrices and data frames work I
> guess.  I want to replace the NA values in a data frame by 0, and 
> the
> code works as long as the data frame in question actually includes 
> an NA
> value.  If it doesn't, there is an error:
>
> df <- data.frame(c1=c(1,1,1),c2=c(2,2,NA))
> df[is.na(df)] <- 0
> df
>
> df <- data.frame(c1=c(1,1,1),c2=c(2,2,2))
> df[is.na(df)] <- 0
> Df
>
> Any help would be appreciated.  I could just convert the data frame 
> to a
> matrix, execute the code, then convert it back to a data frame, but 
> that
> appears long winded.
>
> Thanks
> Mick
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Fri Jan 14 12:56:45 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 14 Jan 2005 11:56:45 +0000 (GMT)
Subject: [R] Replacing NAs in a data frame using is.na() fails if there
	are no NAs
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121B934@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121B934@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <Pine.LNX.4.61.0501141154240.25187@gannet.stats>

On Fri, 14 Jan 2005, michael watson (IAH-C) wrote:

> Hi
>
> This is a difference between the way matrices and data frames work I
> guess.  I want to replace the NA values in a data frame by 0, and the
> code works as long as the data frame in question actually includes an NA
> value.  If it doesn't, there is an error:
>
> df <- data.frame(c1=c(1,1,1),c2=c(2,2,NA))
> df[is.na(df)] <- 0
> df
>
> df <- data.frame(c1=c(1,1,1),c2=c(2,2,2))
> df[is.na(df)] <- 0
> Df
>
> Any help would be appreciated.  I could just convert the data frame to a
> matrix, execute the code, then convert it back to a data frame, but that
> appears long winded.

As always, look at the objects:

> is.na(df)
      c1    c2
1 FALSE FALSE
2 FALSE FALSE
3 FALSE FALSE

so there is nothing to replace by 0.

What you should have is

ind <- is.na(df)
df[ind] <- rep(0, sum(ind))

to give the right number of replacements.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From michael.watson at bbsrc.ac.uk  Fri Jan 14 13:03:11 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 14 Jan 2005 12:03:11 -0000
Subject: [R] Replacing NAs in a data frame using is.na() fails if there
	are no NAs
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E89B53@iahce2knas1.iah.bbsrc.reserved>

Thank you for the answers.  Checking for NAs using any() will help.
I just thought it was worth mentioning because the behaviour of data
frames (which throw an error) is different to the behaviour of matrices
(which don't), and that might not be expected.  

mat <- matrix(c(1,1,1,1),nrow=2,ncol=2)
mat[is.na(mat)] <- 0

mat <- matrix(c(1,1,1,NA),nrow=2,ncol=2)
mat[is.na(mat)] <- 0

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: 14 January 2005 11:57
To: michael watson (IAH-C)
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Replacing NAs in a data frame using is.na() fails if
there are no NAs


On Fri, 14 Jan 2005, michael watson (IAH-C) wrote:

> Hi
>
> This is a difference between the way matrices and data frames work I 
> guess.  I want to replace the NA values in a data frame by 0, and the 
> code works as long as the data frame in question actually includes an 
> NA value.  If it doesn't, there is an error:
>
> df <- data.frame(c1=c(1,1,1),c2=c(2,2,NA))
> df[is.na(df)] <- 0
> df
>
> df <- data.frame(c1=c(1,1,1),c2=c(2,2,2))
> df[is.na(df)] <- 0
> Df
>
> Any help would be appreciated.  I could just convert the data frame to

> a matrix, execute the code, then convert it back to a data frame, but 
> that appears long winded.

As always, look at the objects:

> is.na(df)
      c1    c2
1 FALSE FALSE
2 FALSE FALSE
3 FALSE FALSE

so there is nothing to replace by 0.

What you should have is

ind <- is.na(df)
df[ind] <- rep(0, sum(ind))

to give the right number of replacements.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sdavis2 at mail.nih.gov  Fri Jan 14 13:05:01 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 14 Jan 2005 07:05:01 -0500
Subject: [R] Replacing NAs in a data frame using is.na() fails if there
	are no NAs
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121B934@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121B934@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <846B6714-6624-11D9-A6EF-000D933565E8@mail.nih.gov>

Mick,

The actual error is telling:

 > df <- data.frame(c1=c(1,1,1),c2=c(2,2,2))
 > df[is.na(df)] <- 0
Error in "[<-.data.frame"(`*tmp*`, is.na(df), value = 0) :
	rhs is the wrong length for indexing by a logical matrix

If you look at is.na(df), you will see that it is all FALSE, of course. 
  The right-hand-side (rhs) can't be assigned to a vector of length=0 
(the length of df[is.na(df)] if there are no NAs), hence the error.  An 
easy work-around is to check if there are NAs first.

tmp <- is.na(df); #get total number of NAs
if (sum(tmp)) {   #only execute if there is at least one NA
	df[tmp] <- 0
}

Sean


On Jan 14, 2005, at 6:20 AM, michael watson ((IAH-C)) wrote:

> Hi
>
> This is a difference between the way matrices and data frames work I
> guess.  I want to replace the NA values in a data frame by 0, and the
> code works as long as the data frame in question actually includes an 
> NA
> value.  If it doesn't, there is an error:
>
> df <- data.frame(c1=c(1,1,1),c2=c(2,2,NA))
> df[is.na(df)] <- 0
> df
>
> df <- data.frame(c1=c(1,1,1),c2=c(2,2,2))
> df[is.na(df)] <- 0
> Df
>
> Any help would be appreciated.  I could just convert the data frame to 
> a
> matrix, execute the code, then convert it back to a data frame, but 
> that
> appears long winded.
>
> Thanks
> Mick
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From f.gherardini at pigrecodata.net  Fri Jan 14 14:43:54 2005
From: f.gherardini at pigrecodata.net (Federico Gherardini)
Date: Fri, 14 Jan 2005 14:43:54 +0100
Subject: [R] Exact poisson confidence intervals
In-Reply-To: <20050114003011.74B6A47E7B@mail.pigrecodata.net>
References: <20050114003011.74B6A47E7B@mail.pigrecodata.net>
Message-ID: <41E7CC9A.7050007@pigrecodata.net>

Thanks everybody for their answers!

Cheers,
fede



From buczkowski at delta.sggw.waw.pl  Fri Jan 14 13:55:25 2005
From: buczkowski at delta.sggw.waw.pl (buczkowski@delta.sggw.waw.pl)
Date: Fri, 14 Jan 2005 13:55:25 +0100
Subject: [R] Colors and legend on a scatterplot?
Message-ID: <1105707325.41e7c13d22a2b@poczta.wl.sggw.waw.pl>

Hello

I am new to R and I cannot overcome the 2 problems

1.Defining my own color scale for a value on a scatterplot
2.Adding legend showing values for colours

I have a set of points in area of my research with coordinates X and Y and
a value which I would like to show with colors.
 
#values for points are residuals from regression

stepmod = step(wmodzad, direction="both")

#I am adding residuals to original data to get coordinates for residuals

expzad = cbind(zadrz, residuals(stepmod))

#I want to have colors for classes defined as standard deviation from mean


std = sd(residuals(stepmod))
mn = mean(residuals(stepmod))
sdclas = seq(-3*std+mn, 3*std+mn, by=std)

#I would like to have colors from green to red, but I can't do it so I am
doing only

palette( rainbow(6) )

#I am dividing residuals into earlier defined classes by

residsd = cut(residuals(stepmod), sdclas, labels=FALSE) 

#or by (THEN POINTS DON'T APPEAR ON PLOT IN NEXT STEP)

residsd = cut(residuals(stepmod), sdclas)

# I am creating a plot

plot(expzad$X,zadrz$Y,col=residsd, pch=15 )

# I would like to add legend to plot showing values for colours from plot
but I can't do this. Simply help for 'legend' on my level of knowledge on
R is not helpfull.

loc = locator(1)
legend(loc$x, loc$y, legend = levels(residsd) )

Thanks in advance for your help

Rafal Buczkowski



From andy_liaw at merck.com  Fri Jan 14 14:19:46 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 14 Jan 2005 08:19:46 -0500
Subject: [R] Installing R on Xandros 3.0
Message-ID: <3A822319EB35174CA3714066D590DCD50994E52D@usrymx25.merck.com>

I think Dirk's recommendation is better.  If the system lacks tools to
compile R from source, install.packages() will not work for many packages,
as that compile packages from source.

Andy

ps:  Thanks to those who pointed out about availability of dev tools in
Xandros & Unbuntu.  I still can't get comfortable with a *nix system that
doesn't have compilers installed by default...


> From: Mohamed Abdolell
> 
> Hi Tom,
> 
> [1] go to http://packages.debian.org/testing/math/ and 
> download r-base and other
> essential packages to your computer
> [2] install packages using Xandros Networks; from the 
> pull-down menu select
> File-->Install DEB file...
> [3] once you have a functional R 2.0.1 working install the 
> rest of the packages
> you want directly from within R using install.packages (see 
> help for this); this
> is best since all the dependencies are sorted out automatically
> 
> HTH.
> 
> - Mohamed
> 
> 
> ---------------------------------------
> 
> 
> Quoting Thomas W Volscho <THOMAS.VOLSCHO at huskymail.uconn.edu>:
> 
> > Dear List,
> > After obtaining a second-hand PC and because XP costs too 
> much, I installed
> > Xandros 3.0 (based on Debian) but pretty easy to use if 
> migrating from
> > WinXP.
> > 
> > Does anyone know how to install R on this OS?
> > 
> > Thank you for your time,
> > Tom Volscho
> > 
> > ************************************        
> > Thomas W. Volscho
> > Graduate Student
> > Dept. of Sociology U-2068
> > University of Connecticut
> > Storrs, CT 06269
> > Phone: (860) 486-3882
> > http://vm.uconn.edu/~twv00001
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From m.abdolell at utoronto.ca  Fri Jan 14 14:48:43 2005
From: m.abdolell at utoronto.ca (Mohamed Abdolell)
Date: Fri, 14 Jan 2005 08:48:43 -0500
Subject: [R] Installing R on Xandros 3.0
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E52D@usrymx25.merck.com>
Message-ID: <ALEJLOAANHOJCNBLGOFBEEAOCBAA.m.abdolell@utoronto.ca>

Hi Andy,

I certainly deffer to both you and Dirk on this ... my suggestions were only
ment to get an installation of R up and running on Xandros.  I'm by no means
an expert, as you can see by my missuse of the term 'dependencies'.  I'd had
trouble using apt-get so I fiddled with Xandros Networks until I got
something that worked.

Perhaps, either you or Dirk could comment on the following for me ... when I
tried the installation on Xandros using the method I indicated, everything
seemed to install.  But, when I tried running some R code I'd written and
run on WinXP it actually ran slower than on either WinXP or on Quantian ...
would this have something to do with the way I installed R?

Thanks.

- Mohamed


-----Original Message-----
From: Liaw, Andy [mailto:andy_liaw at merck.com]
Sent: 14 January 2005 08:20
To: 'Mohamed Abdolell'; Thomas.Volscho at uconn.edu; Thomas W Volscho
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] Installing R on Xandros 3.0


I think Dirk's recommendation is better.  If the system lacks tools to
compile R from source, install.packages() will not work for many packages,
as that compile packages from source.

Andy

ps:  Thanks to those who pointed out about availability of dev tools in
Xandros & Unbuntu.  I still can't get comfortable with a *nix system that
doesn't have compilers installed by default...


> From: Mohamed Abdolell
>
> Hi Tom,
>
> [1] go to http://packages.debian.org/testing/math/ and
> download r-base and other
> essential packages to your computer
> [2] install packages using Xandros Networks; from the
> pull-down menu select
> File-->Install DEB file...
> [3] once you have a functional R 2.0.1 working install the
> rest of the packages
> you want directly from within R using install.packages (see
> help for this); this
> is best since all the dependencies are sorted out automatically
>
> HTH.
>
> - Mohamed
>
>
> ---------------------------------------
>
>
> Quoting Thomas W Volscho <THOMAS.VOLSCHO at huskymail.uconn.edu>:
>
> > Dear List,
> > After obtaining a second-hand PC and because XP costs too
> much, I installed
> > Xandros 3.0 (based on Debian) but pretty easy to use if
> migrating from
> > WinXP.
> >
> > Does anyone know how to install R on this OS?
> >
> > Thank you for your time,
> > Tom Volscho
> >
> > ************************************
> > Thomas W. Volscho
> > Graduate Student
> > Dept. of Sociology U-2068
> > University of Connecticut
> > Storrs, CT 06269
> > Phone: (860) 486-3882
> > http://vm.uconn.edu/~twv00001
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>


----------------------------------------------------------------------------
--
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From andy_liaw at merck.com  Fri Jan 14 15:17:10 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 14 Jan 2005 09:17:10 -0500
Subject: [R] Installing R on Xandros 3.0
Message-ID: <3A822319EB35174CA3714066D590DCD50994E531@usrymx25.merck.com>

Hi Mohamed,

> I certainly deffer to both you and Dirk on this ... my 
> suggestions were only
> ment to get an installation of R up and running on Xandros.  

If you are using Xandros, then your information is better than my
speculation.  If you were able to use install.packages(), you must have the
devel tools installed.

> Perhaps, either you or Dirk could comment on the following 
> for me ... when I
> tried the installation on Xandros using the method I 
> indicated, everything
> seemed to install.  But, when I tried running some R code I'd 
> written and
> run on WinXP it actually ran slower than on either WinXP or 
> on Quantian ...
> would this have something to do with the way I installed R?

Is the difference observed on the same machine?  If so, that sound very
peculiar, as the R installation on Quantian should be the same as the .deb
file you install from on Xandros...  If you can, please show us example code
and timings.

Andy



From ggrothendieck at myway.com  Fri Jan 14 15:28:03 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 14 Jan 2005 14:28:03 +0000 (UTC)
Subject: [R] Replacing NAs in a data frame using is.na() fails if
	=?utf-8?b?dGhlcmUJYXJlbm8=?= NAs
References: <8975119BCD0AC5419D61A9CF1A923E950121B934@iahce2knas1.iah.bbsrc.reserved>
	<016701c4fa2f$47ba86f0$0540210a@www.domain>
Message-ID: <loom.20050114T151749-975@post.gmane.org>


Your solution suggests that it works as expected on single columns
so in the special case that one is really only interested in NAs in one
particular column then the original can be made to work, e.g. for 
column c2:

DF$c2[ is.na(DF$c2) ] <- 0


Dimitris Rizopoulos <dimitris.rizopoulos <at> med.kuleuven.ac.be> writes:

: 
: Hi Mick,
: 
: try the following:
: 
: dat[] <- lapply(dat, function(x) ifelse(is.na(x), 0, x))
: dat
: 
: I hope it helps.
: 
: Best,
: Dimitris
: 
: ----
: Dimitris Rizopoulos
: Ph.D. Student
: Biostatistical Centre
: School of Public Health
: Catholic University of Leuven
: 
: Address: Kapucijnenvoer 35, Leuven, Belgium
: Tel: +32/16/336899
: Fax: +32/16/337015
: Web: http://www.med.kuleuven.ac.be/biostat
:      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
: 
: ----- Original Message ----- 
: From: "michael watson (IAH-C)" <michael.watson <at> bbsrc.ac.uk>
: To: <r-help <at> stat.math.ethz.ch>
: Sent: Friday, January 14, 2005 12:20 PM
: Subject: [R] Replacing NAs in a data frame using is.na() fails if 
: there areno NAs
: 
: > Hi
: >
: > This is a difference between the way matrices and data frames work I
: > guess.  I want to replace the NA values in a data frame by 0, and 
: > the
: > code works as long as the data frame in question actually includes 
: > an NA
: > value.  If it doesn't, there is an error:
: >
: > df <- data.frame(c1=c(1,1,1),c2=c(2,2,NA))
: > df[is.na(df)] <- 0
: > df
: >
: > df <- data.frame(c1=c(1,1,1),c2=c(2,2,2))
: > df[is.na(df)] <- 0
: > Df
: >
: > Any help would be appreciated.  I could just convert the data frame 
: > to a
: > matrix, execute the code, then convert it back to a data frame, but 
: > that
: > appears long winded.
: >
: > Thanks
: > Mick
: >
: > ______________________________________________
: > R-help <at> stat.math.ethz.ch mailing list
: > https://stat.ethz.ch/mailman/listinfo/r-help
: > PLEASE do read the posting guide! 
: > http://www.R-project.org/posting-guide.html
: >
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From ozric at web.de  Fri Jan 14 15:42:41 2005
From: ozric at web.de (Christian Schulz)
Date: Fri, 14 Jan 2005 15:42:41 +0100
Subject: [R] spreadsheet addiction
In-Reply-To: <cs8au2$edk$1@sea.gmane.org>
References: <41D92469.1010608@burns-stat.com> <cs8au2$edk$1@sea.gmane.org>
Message-ID: <41E7DA61.7030603@web.de>

Henrik Andersson wrote:

> I am moving to using R more and more as a computational platform and
> I use Microsoft Excel at the moment in exactly the way you describe in 
> WRITING ASCII FILES as a staging area for data, to gather data and 
> make simple calculations. I've have experienced problems with saving 
> to .csv files, and not only using scientific notation. The problem 
> occurs also in normal notation if you choose to show only some decimals.
>
> That is a serious problem, what is the remedy?
>
> To always avoid showing only some decimals and avoid scientific 
> notation or does someone have a  better solution, like a global 
> option, Save with full represention to .csv.
>
> I'm open for all sorts of suggestions, including ditching Excel...and 
> use ???
>
Perhaps MySQL &  RMySQL , and one of the many existing gui-tools like:
Query-Browser, Mysql-Front,SqlYog, ToadForMysql, Tora etc..
Since mysql have the possibility  to create stored procedures and 
cursors it's  my ideal "spreadsheet", too.

One of a big reason to avoid working with excel is the limited row-size 
of  ~ 65.0000 .

regards,
Christian


 


.



From davidr at rhotrading.com  Fri Jan 14 16:00:24 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Fri, 14 Jan 2005 09:00:24 -0600
Subject: [R] spreadsheet addiction
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A422CAC@rhosvr02.rhotrading.com>

I get around the dependence on how things are displayed in Excel by
using 
VBA to write my data files. Then you get full precision and you can use 
white space as a separator if you like.

Sub WriteRangeToFile(rData as Excel.Range, fName as String)
  Application.ScreenUpdating = False
  Application.Calculation = xlCalculationManual
  Application.StatusBar = "Writing Data File " & fName

  Dim ofs As Object, ots As Object
  Dim vData As Variant
  Dim r As Long, rCount As Long
  Dim c As Long, cCount As Long

  Set ofs = CreateObject("Scripting.FileSystemObject")
  vData = rData.Value
  Set ots = ofs.CreateTextFile(fName, True)
  cCount = UBound(vData, 2)
  rCount = UBound(vData, 1)
  For r = 1 To rCount
    str = ""
    For c = 1 To cCount - 1
      str = str & vData(r, c) & " "
    Next c
    str = str & vData(r, cCount)
    ots.WriteLine (str)
  Next r
  ots.Close
  Set ots = Nothing

  Application.ScreenUpdating = True
  Application.Calculation = xlCalculationAutomatic
  Application.StatusBar = False
End Sub

You can call this from another macro to set the range to a selection,
e.g., and the filename to the contents of some cell.

HTH,
David Reiner

-----Original Message-----
From: Henrik Andersson [mailto:h.andersson at nioo.knaw.nl] 
Sent: Friday, January 14, 2005 5:38 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] spreadsheet addiction

I am moving to using R more and more as a computational platform and
I use Microsoft Excel at the moment in exactly the way you describe in 
WRITING ASCII FILES as a staging area for data, to gather data and make 
simple calculations. I've have experienced problems with saving to .csv 
files, and not only using scientific notation. The problem occurs also 
in normal notation if you choose to show only some decimals.

That is a serious problem, what is the remedy?

To always avoid showing only some decimals and avoid scientific notation

or does someone have a  better solution, like a global option, Save with

full represention to .csv.

I'm open for all sorts of suggestions, including ditching Excel...and 
use ???

Cheers, Henrik

Patrick Burns wrote:
> There's a new page on the Burns Statistics website
> http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html
> that looks at spreadsheets from a quality assurance perspective. It
> presents R as a suitable alternative to spreadsheets.  Also there are
> several specific problems with Excel that are highlighted, including
> the status of statistical functionality in Excel.
> 
> Patrick Burns
> 
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From MSchwartz at MedAnalytics.com  Fri Jan 14 16:08:44 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 14 Jan 2005 09:08:44 -0600
Subject: [R] spreadsheet addiction
In-Reply-To: <cs8au2$edk$1@sea.gmane.org>
References: <41D92469.1010608@burns-stat.com>  <cs8au2$edk$1@sea.gmane.org>
Message-ID: <1105715324.9188.46.camel@horizons.localdomain>

On Fri, 2005-01-14 at 12:37 +0100, Henrik Andersson wrote:
> I am moving to using R more and more as a computational platform and
> I use Microsoft Excel at the moment in exactly the way you describe in 
> WRITING ASCII FILES as a staging area for data, to gather data and make 
> simple calculations. I've have experienced problems with saving to .csv 
> files, and not only using scientific notation. The problem occurs also 
> in normal notation if you choose to show only some decimals.
> 
> That is a serious problem, what is the remedy?
> 
> To always avoid showing only some decimals and avoid scientific notation 
> or does someone have a  better solution, like a global option, Save with 
> full represention to .csv.
> 
> I'm open for all sorts of suggestions, including ditching Excel...and 
> use ???
> 
> Cheers, Henrik

If you need to use a spreadsheet, for whatever reason, under Windows, I
would suggest going to OpenOffice.org:

http://www.openoffice.org/

When I was still running under Windows, I had notable problems saving
sheets to CSV files using Excel. There would not only be issues with
numeric formats, but also in dealing with blank cells. The latter
resulting in an incorrect number of fields being created in the CSV
file. This caused problems upon importing to R (ie. using read.csv())
whereby the number of fields in the header row did not match the number
of fields in all of the data rows.

OO.org's calc is _not_ better then Excel for actual analyses, since they
are focused on matching Excel functionality, but for this one feature,
it is far better IMHO.

You might also want to keep an eye on Gnumeric, which is in the process
of developing a Windows version:

http://www.gnome.org/projects/gnumeric/

HTH,

Marc Schwartz



From dgrueter at uwinst.unizh.ch  Fri Jan 14 16:11:24 2005
From: dgrueter at uwinst.unizh.ch (=?ISO-8859-1?Q?Dominique_Gr=FCter?=)
Date: Fri, 14 Jan 2005 16:11:24 +0100
Subject: [R] meaning of "iner" etc. relating to cca(ade4)
Message-ID: <8E44B938-663E-11D9-A438-0030656E6642@unizh.ch>

Does someone know the meaning of the following values relating to the 
"cca"-command?

  iner, inercum, inerC, inercumC

Thanks!
DOMI



From danbebber at yahoo.co.uk  Fri Jan 14 16:39:51 2005
From: danbebber at yahoo.co.uk (Dan Bebber)
Date: Fri, 14 Jan 2005 15:39:51 +0000 (GMT)
Subject: [R] R package classification
Message-ID: <20050114153951.22145.qmail@web26309.mail.ukl.yahoo.com>

Dear list,

there are now >400 packages available on CRAN.
Would it be useful to classify these packages
according to what they do (e.g. classification,
graphics, spatial statistics), to assist the user in
finding the appropriate package for their problem? Or
perhaps the search facility is enough.
I would attempt such a classification, but my
knowledge of statistical methods isn't good enough.

Dan Bebber

Department of Plant Sciences
University of Oxford
UK



From jfox at mcmaster.ca  Fri Jan 14 16:41:12 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 14 Jan 2005 10:41:12 -0500
Subject: [R] Fine tuning scatterplot() (car package)
In-Reply-To: <200501141026.j0EAQOAn029542@mail1.slu.se>
Message-ID: <20050114154111.PLXD2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear CG,

When I wrote the scatterplot() function I expected it to be used in data
exploration and didn't anticipate its use for publication graphics. 

That said, it wouldn't be hard to add the full set of cex* arguments and to
pass these through to plot() and points(). I'll do this for the next release
of the car package, but in the meantime, you could do it for yourself.

Thank you for the suggestion.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of CG Pettersson
> Sent: Friday, January 14, 2005 5:26 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Fine tuning scatterplot() (car package)
> 
> Hello all!
> 
> System: R2.0.1, W2k. 
> All packages apdated with update.packages().
> I use the default graphic device for my system (no active choice).
> 
> I?m trying to fine-tune scatterplot() graphs (package car), 
> for publication.
> I want to control plotting symbols and colours to match plots 
> from other, less flexible, systems. I also need to make all 
> text and symbols bigger, to enable printing of the plots in 
> smaller scale than the original outputs.
> 
> My strategy has been to start in Rcmdr, using the command 
> line produced there and rerun variants of the scatterplot() 
> call directly in the console.  
> 
> After consulting MASS and CAR, the colour and characters were 
> easy to fix by overrunning the defaults with col=c() and pch=c(). 
> 
> But the size of letters and figures beats me so far. I?ve 
> tried cex and csi in the same positiones as col and pch on 
> the line. "cex" doesn?t do anything as far as I can see, 
> "csi" produces warning messages (which is some sort of 
> effect!) but does nothing on the size.
> 
> Trying to tune with these commands inside xlab and ylab calls 
> (based on the defaults from the documentation) results in 
> unspecified syntax error messages.
> 
> My code in the console is this for the moment:
> 
> > scatterplot(Kernel.protein~TC.OS | Year, reg.line=FALSE,
>         smooth=FALSE, labels=FALSE, boxplots=FALSE, span=0.5, 
>         by.groups=TRUE, col=c("","red","blue","black"), pch=c(15,4,5),
>  
>         data=ecpa.f)
> 
> Which produces nice plots, but with too small symbols, 
> letters an numbers.
> 
> Thanks!
> /CG
> 
> CG Pettersson, MSci, PhD Stud.
> Swedish University of Agricultural Sciences Dep. of Ecology 
> and Crop Production. Box 7043 SE-750 07 Uppsala
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Robert at sanctumfi.com  Fri Jan 14 17:25:16 2005
From: Robert at sanctumfi.com (Robert Sams)
Date: Fri, 14 Jan 2005 16:25:16 -0000
Subject: [R] R package classification
Message-ID: <E585EABA11227445B918BFB74C1A4D36015985@sanctum01.sanctumfi.com>

dan,

good idea. i propose an additional field called 'keywords' to the package source file 'description'. this would be better than a single taxonomy of packages.

Robert Sams

SANCTUM FI LLP
email: robert at sanctumfi.com

Authorised and Regulated by the FSA. 

Sending encrypted mail:

See http://pgp.mit.edu (search string 'sanctumfi') for updates.
-----BEGIN PGP PUBLIC KEY BLOCK-----
Version: GnuPG v1.0.6 (GNU/Linux)
Comment: For info see http://www.gnupg.org

mQGiBEHVOaIRBAC3kUplVB1o00GQXU9TkKmMWz/LiknbhLgDPBZ+1nw1ffb2FpXW
mYet2yja8YIG3H2LnVla1WGVyfAyZ+QRmnUgm2hOnP7LNZIoHctnGqLmtQvOXvHz
xoDonFAl/ztBFE9aNPbiL4nlgdz9GS6LN6hbw9GTdYGip71QhaeVIRR3HwCgySSl
rUtSV8C7nbHR9C+zwku3EpMD/0yoq1R4lQgJGNbyjzzCRWRblK5cKLh1XUYG3JSc
ltLhoCZMvR+359FUSz+jEbbJaLRpquX48Wjv+7WW5KNVm2nPHSZrZ6tr+nm/ov1Y
DQxbIvBHxLXxltDWJlL/gg3bPaSIRlj/KVw0fB0NU1b25eGFRTtlHEvkAROj92sv
MiaSA/9jjEi2H0+yWujcBxJ6syUjWLlgRxvm85sJBihyan4ufAMFRria+QKx7ZMf
A7MNJ+r7ULdqCPsPdZP3kC7GNfLXBzizy3d2HSB/0oB7AjIoXrYyQvXQjFNQQX+u
XjeRGhALxou3H1NEPiCiHJiiaPU6Uh+KYhBCDOYZpc82FgKwWLQzUm9iZXJ0IFNh
bXMgKFNhbmN0dW0gRkkgTExQKSA8cm9iZXJ0QHNhbmN0dW1maS5jb20+iF0EExEC
AB0FAkHVOaIFCQHhM4AFCwcKAwQDFQMCAxYCAQIXgAAKCRB4Q4+orHX8Yk0hAJ9+
odzCRiih6wZz4NOOSVboJP+lngCeNvFGVxVQW35/qpTaF6wsym9jehi5AQ0EQdU5
oxAEALZEnBUQqKiF/gUqK7zyLJarsVxGsmuj0pkV5gFwCbChA4RA7QgHjknJT3Qd
jLUJOa+rW49WtbDCOBv+VOVp//gLROByZpizW4BYaOw01kI9emMuoc6el6nYXarJ
6aZcA84IFBifdi2a8lB3ofhQuWc/YmxLjcOKbkaIC9lUYHrzAAMGA/0RRhkXCHCL
zRSQj+7nSBE4MTeMJycdytl1wnpWkRUa8MoUYBF6/3oiyCnO9bHbOAkQrULSWRLA
YsUJv0c1b6Dht5LVChGikJqKgCzWVEVUI7ob0F2LctvDxhZLlCctHapFGZn9+6pi
rZW+2XkBmbqhJ8ybKsRAIJNy7OV3sIHoVYhMBBgRAgAMBQJB1TmjBQkB4TOAAAoJ
EHhDj6isdfxizmIAn3I/mZyfAuBNZl0lG+9XpAhR80ThAKDAJEnXrH8dX30rRwDz
1mgpwRYCiw==
=59j7
-----END PGP PUBLIC KEY BLOCK-----



> -----Original Message-----
> From: Dan Bebber [mailto:danbebber at yahoo.co.uk]
> Sent: Friday, January 14, 2005 3:40 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] R package classification
> 
> 
> Dear list,
> 
> there are now >400 packages available on CRAN.
> Would it be useful to classify these packages
> according to what they do (e.g. classification,
> graphics, spatial statistics), to assist the user in
> finding the appropriate package for their problem? Or
> perhaps the search facility is enough.
> I would attempt such a classification, but my
> knowledge of statistical methods isn't good enough.
> 
> Dan Bebber
> 
> Department of Plant Sciences
> University of Oxford
> UK
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From choudary.jagar at swosu.edu  Fri Jan 14 17:20:32 2005
From: choudary.jagar at swosu.edu (Jagarlamudi, Choudary)
Date: Fri, 14 Jan 2005 10:20:32 -0600
Subject: [R] Help in Overlaying of 2 Plots on the same Device.
Message-ID: <E03EBB50FF2C024781A6E4460AD58F0607C150@swosu-mbx01.admin.swosu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050114/27efb884/attachment.pl

From nicolas.deig at epfl.ch  Fri Jan 14 17:23:33 2005
From: nicolas.deig at epfl.ch (nicolas.deig@epfl.ch)
Date: Fri, 14 Jan 2005 17:23:33 +0100 (MET)
Subject: [R] subsampling
Message-ID: <1105719813.41e7f205d5319@imapwww.epfl.ch>

hi,

I would like to subsample the array c(1:200) at random into ten subsamples 
v1,v2,...,v10.

I tried with to go progressively like this:


> x<-c(1:200)
> v1<-sample(x,20)
> y<-x[-v1]
> v2<-sample(y,20)

and then I want to do:

>x<-y[-v2]
Error: subscript out of bounds.



From Scott.Waichler at pnl.gov  Fri Jan 14 17:25:10 2005
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Fri, 14 Jan 2005 08:25:10 -0800
Subject: [R] Code contributed for a gant (Gantt) chart
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01343EFA@pnlmse35.pnl.gov>


I searched CRAN for material on gant (or Gantt) charts, those schedule
plots that look like horizontal barplots where the x-axis is time.
Finding none, I wrote the following and am submitting it to the archive
so that it may help someone in the future.

plotfile <- "gant.eps"

regular.color <- "lightgray"
important.color <- "yellow"

tasks <- list()  # Define tasks.  Start and end refer to times.
tasks[[length(tasks) + 1]] <-
  list(name = "Name of first task", start = 0, end = 3, color =
regular.color)
tasks[[length(tasks) + 1]] <-
  list(name = "Name of second task", start = 3, end = 7, color =
regular.color)
tasks[[length(tasks) + 1]] <-
  list(name = "Name of third task", start = 3, end = 7, color =
regular.color)
tasks[[length(tasks) + 1]] <-
  list(name = "Name of fourth task", start = 5, end = 9, color =
important.color)
tasks[[length(tasks) + 1]] <-
  list(name = "Name of fifth task", start = 3, end = 8, color =
regular.color)
tasks[[length(tasks) + 1]] <-
  list(name = "Name of sixth task", start = 3, end = 11, color =
important.color)

num.tasks <- length(tasks)
task.labels <- rep(NA, num.tasks)
task.times <- NULL
for(i in 1:num.tasks) {
  task.labels[i] <- paste(i, ". ", tasks[[i]]$name)
  task.times <- c(task.times, tasks[[i]]$start, tasks[[i]]$end)
}

# Make the plot
ps.options(onefile=T, print.it=F, paper="special", width=8.5, height=4,
          horizontal=F, pointsize=12, family="Helvetica")
postscript(file=plotfile)

par(omi=c(0.05, 2, 0.5, 0.1))
par(mar=c(0, 2, 0.60, 0.05))

plot(1:3, 1:3,
     ylim=c(1, num.tasks) , xlim=range(task.times),
     main="", xlab = "",
     xaxs="i", xaxt="n", yaxt="n", ylab="", 
     type="n")
par(mgp = c(3, 0.5, 0))
axis(3, at=min(task.times):max(task.times),
labels=min(task.times):max(task.times),
     cex.axis=1.0, las=1, tcl=-0.3)
reverse.task.numbers <- rev(1:num.tasks)
axis(2, at=reverse.task.numbers, labels=task.labels, cex.axis=1.0,
las=1, tcl=-0.2)
abline(v=c(min(task.times):max(task.times)), col="darkgray", lty=3)

half.height <- 0.25
for(i in 1:num.tasks) {
  rect(tasks[[i]]$start, reverse.task.numbers[i] - half.height,
tasks[[i]]$end,
       reverse.task.numbers[i] + half.height, col = tasks[[i]]$color,
border = F)
}
box()

mtext("Month", side=3, outer=F, line=2, cex=1.1) # x-axis label

dev.off()

Scott

Scott Waichler, Senior Research Scientist
Pacific Northwest National Laboratory
MSIN K9-36
P.O. Box 999
Richland, WA   99352    USA
509-372-4423 (voice)
509-372-6089 (fax)
scott.waichler at pnl.gov
http://hydrology.pnl.gov



From ggrothendieck at myway.com  Fri Jan 14 17:28:53 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 14 Jan 2005 16:28:53 +0000 (UTC)
Subject: [R] spreadsheet addiction
References: <41D92469.1010608@burns-stat.com> <cs8au2$edk$1@sea.gmane.org>
Message-ID: <loom.20050114T172018-726@post.gmane.org>

Henrik Andersson <h.andersson <at> nioo.knaw.nl> writes:

: 
: I am moving to using R more and more as a computational platform and
: I use Microsoft Excel at the moment in exactly the way you describe in 
: WRITING ASCII FILES as a staging area for data, to gather data and make 
: simple calculations. I've have experienced problems with saving to .csv 
: files, and not only using scientific notation. The problem occurs also 
: in normal notation if you choose to show only some decimals.
: 
: That is a serious problem, what is the remedy?
: 
: To always avoid showing only some decimals and avoid scientific notation 
: or does someone have a  better solution, like a global option, Save with 
: full represention to .csv.
: 
: I'm open for all sorts of suggestions, including ditching Excel...and 
: use ???


You could try getting it directly from Excel using R, rather than using
CSV as an intermediary.  Check out:

1. rcom package (http://sunsite.univie.ac.at/rcom/download/)
rcom has a mailing list at http://mailman.csd.univie.ac.at/pipermail/rcom-l/
although it seems to be down at the moment.   This allows one to use
Microsoft COM objects to interface to Excel.  

2. RDCOMClient package.  Another Microsoft COM client. 

3. RODBC package.  For using the ODBC interface to Excel.

The last two are on CRAN.  

The first two also have a number of associated packages that might
be useful to you too.



From das at cshl.edu  Fri Jan 14 17:37:01 2005
From: das at cshl.edu (Das, Rajdeep)
Date: Fri, 14 Jan 2005 11:37:01 -0500
Subject: [R] probabilty calculation in SVM
Message-ID: <C8696843AE995F4EA4CDC3E2B83482A90A1B7E@mailbox02.cshl.edu>

Hi All,

In package e1071 for SVM based classification, one can get a probability
measure for each prediction. I like to know what is method that is used for
calculating this probability. Is it calculated using logistic link function?
Thanks for your help.

Regards,

Raj



From ligges at statistik.uni-dortmund.de  Fri Jan 14 17:49:16 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 14 Jan 2005 17:49:16 +0100
Subject: [R] subsampling
In-Reply-To: <1105719813.41e7f205d5319@imapwww.epfl.ch>
References: <1105719813.41e7f205d5319@imapwww.epfl.ch>
Message-ID: <41E7F80C.8080300@statistik.uni-dortmund.de>

nicolas.deig at epfl.ch wrote:

> hi,
> 
> I would like to subsample the array c(1:200) at random into ten subsamples 
> v1,v2,...,v10.
> 
> I tried with to go progressively like this:
> 
> 
> 
>>x<-c(1:200)
>>v1<-sample(x,20)
>>y<-x[-v1]
>>v2<-sample(y,20)
> 
> 
> and then I want to do:
> 
> 
>>x<-y[-v2]
> 
> Error: subscript out of bounds.


Let's do a simple example:

x <- 1:3
v1 <- sample(x, 1)
# assume v1 = 2:
y <- x[-v1]
# now: y = c(1, 3)
v2 <- sample(y,1)
# assume v2 = 3, what happens now?
x <- y[-v2]


Uwe Ligges


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Jan 14 17:51:39 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 14 Jan 2005 17:51:39 +0100
Subject: [R] Help in Overlaying of 2 Plots on the same Device.
In-Reply-To: <E03EBB50FF2C024781A6E4460AD58F0607C150@swosu-mbx01.admin.swosu.edu>
References: <E03EBB50FF2C024781A6E4460AD58F0607C150@swosu-mbx01.admin.swosu.edu>
Message-ID: <41E7F89B.4090106@statistik.uni-dortmund.de>

Jagarlamudi, Choudary wrote:

> I'm trying to overlay a density plot on a previously plotted histogram. However, i need to use the same axis as of the 1st(histogram) plot to plot the second. My second plot is creating its own axis and causing my plot to extend the entire histogram instead of getting a subplot on a portion of the histogram. I tried 'fig' and 'new' parameters with no luck.

You could add the density using lines(), after setting the argument ylim 
for the histogram.

Uwe Ligges

> Thanks in advance.
>  
> Choudary Jagarlamudi
> Instructor
> Southwestern Oklahoma State University
> STF 254
> 100 campus Drive
> Weatherford OK 73096
> Tel 580-774-7136
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Fri Jan 14 17:50:29 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 14 Jan 2005 11:50:29 -0500
Subject: [R] subsampling
Message-ID: <3A822319EB35174CA3714066D590DCD50994E535@usrymx25.merck.com>

> From: nicolas.deig at epfl.ch
> 
> hi,
> 
> I would like to subsample the array c(1:200) at random into 
> ten subsamples 
> v1,v2,...,v10.
> 
> I tried with to go progressively like this:
> 
> 
> > x<-c(1:200)
> > v1<-sample(x,20)
> > y<-x[-v1]
> > v2<-sample(y,20)

This only worked because your original data happens to be the same as the
indices (1:200).  The next round failed because that's not true any more.

> and then I want to do:
> 
> >x<-y[-v2]
> Error: subscript out of bounds.

Can you explain more explicitly what you mean by subsamples?  Here you're
trying to overwrite the original data by sampling from the subsample, which
doesn't seem like what you said you want.

If you simply want to randomly divide 1:200 into 10 sets, try something
like:

x.sample <- matrix(sample(x), ncol=10)

where x.sample is a matrix with 10 columns, each column containing a random
(but disjoint) part of x.

Andy



From francoisromain at free.fr  Fri Jan 14 17:52:23 2005
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Fri, 14 Jan 2005 17:52:23 +0100
Subject: [R] subsampling
In-Reply-To: <1105719813.41e7f205d5319@imapwww.epfl.ch>
References: <1105719813.41e7f205d5319@imapwww.epfl.ch>
Message-ID: <41E7F8C7.2070703@free.fr>


nicolas.deig at epfl.ch a ?crit :

>hi,
>
>I would like to subsample the array c(1:200) at random into ten subsamples 
>v1,v2,...,v10.
>
>I tried with to go progressively like this:
>
>
>  
>
>>x<-c(1:200)
>>v1<-sample(x,20)
>>y<-x[-v1]
>>v2<-sample(y,20)
>>    
>>
>
>and then I want to do:
>
>  
>
>>x<-y[-v2]
>>    
>>
>Error: subscript out of bounds.
>
>__
>
You should try :

x[-c(v2,v1)]

But more accurately, try permutting the all x and choose the 20 firsts 
entries, the 20 after, ...
I think that will do the job

x  <- 1:200 
y  <- x[sample(200)]
v1 <- y[ 1:20]
v2 <- y[21:40]
...

Regards.

Romain.

-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From p.dalgaard at biostat.ku.dk  Fri Jan 14 17:50:26 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Jan 2005 17:50:26 +0100
Subject: [R] subsampling
In-Reply-To: <1105719813.41e7f205d5319@imapwww.epfl.ch>
References: <1105719813.41e7f205d5319@imapwww.epfl.ch>
Message-ID: <x2mzvcuf8t.fsf@biostat.ku.dk>

nicolas.deig at epfl.ch writes:

> hi,
> 
> I would like to subsample the array c(1:200) at random into ten subsamples 
> v1,v2,...,v10.

(Why are you c()'ing a single vector?)

> I tried with to go progressively like this:
> 
> 
> > x<-c(1:200)
> > v1<-sample(x,20)
> > y<-x[-v1]
> > v2<-sample(y,20)
> 
> and then I want to do:
> 
> >x<-y[-v2]
> Error: subscript out of bounds.

That's not going to work (hint, the values contained in y do not
correspond to their indices).

I'd just do 

split(sample(1:200),rep(1:10,each=20))

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From MSchwartz at MedAnalytics.com  Fri Jan 14 17:54:22 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 14 Jan 2005 10:54:22 -0600
Subject: [R] spreadsheet addiction
In-Reply-To: <loom.20050114T172018-726@post.gmane.org>
References: <41D92469.1010608@burns-stat.com> <cs8au2$edk$1@sea.gmane.org>
	<loom.20050114T172018-726@post.gmane.org>
Message-ID: <1105721662.14779.8.camel@horizons.localdomain>

On Fri, 2005-01-14 at 16:28 +0000, Gabor Grothendieck wrote:
> Henrik Andersson <h.andersson <at> nioo.knaw.nl> writes:
> 
> : 
> : I am moving to using R more and more as a computational platform and
> : I use Microsoft Excel at the moment in exactly the way you describe in 
> : WRITING ASCII FILES as a staging area for data, to gather data and make 
> : simple calculations. I've have experienced problems with saving to .csv 
> : files, and not only using scientific notation. The problem occurs also 
> : in normal notation if you choose to show only some decimals.
> : 
> : That is a serious problem, what is the remedy?
> : 
> : To always avoid showing only some decimals and avoid scientific notation 
> : or does someone have a  better solution, like a global option, Save with 
> : full represention to .csv.
> : 
> : I'm open for all sorts of suggestions, including ditching Excel...and 
> : use ???
> 
> 
> You could try getting it directly from Excel using R, rather than using
> CSV as an intermediary.  Check out:
> 
> 1. rcom package (http://sunsite.univie.ac.at/rcom/download/)
> rcom has a mailing list at http://mailman.csd.univie.ac.at/pipermail/rcom-l/
> although it seems to be down at the moment.   This allows one to use
> Microsoft COM objects to interface to Excel.  
> 
> 2. RDCOMClient package.  Another Microsoft COM client. 
> 
> 3. RODBC package.  For using the ODBC interface to Excel.
> 
> The last two are on CRAN.  
> 
> The first two also have a number of associated packages that might
> be useful to you too.


Along that same line is the read.xls() function, which is in the 'gdata'
package, which in turn is part of the 'gregmisc' bundle on CRAN.

You would need to install Perl in order for this to work. More
information is here:

http://www.activestate.com/Products/ActivePerl/

HTH,

Marc Schwartz



From ligges at statistik.uni-dortmund.de  Fri Jan 14 18:02:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 14 Jan 2005 18:02:44 +0100
Subject: [R] Colors and legend on a scatterplot?
In-Reply-To: <1105707325.41e7c13d22a2b@poczta.wl.sggw.waw.pl>
References: <1105707325.41e7c13d22a2b@poczta.wl.sggw.waw.pl>
Message-ID: <41E7FB34.50600@statistik.uni-dortmund.de>

buczkowski at delta.sggw.waw.pl wrote:

> Hello
> 
> I am new to R and I cannot overcome the 2 problems
> 
> 1.Defining my own color scale for a value on a scatterplot
> 2.Adding legend showing values for colours
> 
> I have a set of points in area of my research with coordinates X and Y and
> a value which I would like to show with colors.
>  
> #values for points are residuals from regression
> 
> stepmod = step(wmodzad, direction="both")
> 
> #I am adding residuals to original data to get coordinates for residuals
> 
> expzad = cbind(zadrz, residuals(stepmod))
> 
> #I want to have colors for classes defined as standard deviation from mean
> 
> 
> std = sd(residuals(stepmod))
> mn = mean(residuals(stepmod))
> sdclas = seq(-3*std+mn, 3*std+mn, by=std)
> 
> #I would like to have colors from green to red, but I can't do it so I am
> doing only
> 
> palette( rainbow(6) )

You could either use something like the RColorBrewer package, or specify 
the colors yourself using, e.g., rgb().



> #I am dividing residuals into earlier defined classes by
> 
> residsd = cut(residuals(stepmod), sdclas, labels=FALSE) 
> 
> #or by (THEN POINTS DON'T APPEAR ON PLOT IN NEXT STEP)
> 
> residsd = cut(residuals(stepmod), sdclas)
> 
> # I am creating a plot
> 
> plot(expzad$X,zadrz$Y,col=residsd, pch=15 )
> 
> # I would like to add legend to plot showing values for colours from plot
> but I can't do this. Simply help for 'legend' on my level of knowledge on
> R is not helpfull.
> 
> loc = locator(1)
> legend(loc$x, loc$y, legend = levels(residsd) )

I guess you want something like

legend(loc$x, loc$y, col = residsd, legend=levels(residsd), pch=15)

Uwe Ligges

> Thanks in advance for your help
> 
> Rafal Buczkowski
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From james.holtman at convergys.com  Fri Jan 14 18:06:17 2005
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Fri, 14 Jan 2005 12:06:17 -0500
Subject: [R] subsampling
Message-ID: <OF8470C484.610B04CB-ON85256F89.005DE493@nd.convergys.com>





Consider using a list.  This will create a list with 10 entries of your 20
samples:

x <- 1:200
myList <- split(x, cut(sample(x,200),breaks=10))

__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      nicolas.deig at epfl.ch                                                                                                 
                      Sent by:                     To:       r-help at stat.math.ethz.ch                                                      
                      r-help-bounces at stat.m        cc:                                                                                     
                      ath.ethz.ch                  Subject:  [R] subsampling                                                               
                                                                                                                                           
                                                                                                                                           
                      01/14/2005 11:23                                                                                                     
                                                                                                                                           
                                                                                                                                           




hi,

I would like to subsample the array c(1:200) at random into ten subsamples
v1,v2,...,v10.

I tried with to go progressively like this:


> x<-c(1:200)
> v1<-sample(x,20)
> y<-x[-v1]
> v2<-sample(y,20)

and then I want to do:

>x<-y[-v2]
Error: subscript out of bounds.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ktiwari at bgc-jena.mpg.de  Fri Jan 14 18:08:06 2005
From: ktiwari at bgc-jena.mpg.de (Yogesh K. Tiwari)
Date: Fri, 14 Jan 2005 18:08:06 +0100
Subject: [R] how to produce 2-d color plots in R
Message-ID: <41E7FC76.7000500@bgc-jena.mpg.de>

Hello 'R' Users,

I am very new on 'R', so excuse me if I ask something wrong.

I have ASCII data and the colums of the data are looks like :-


!-------------------------
time,yr,mo,dy,hr,min,sec,lat,lon,ht,co2obs,sigma,co2model
--
-
--

!----------------------------

Each column has data value. Now I want to produce 2-d color maps,
for example the plot should look like :-

on x-axis =lon    ,   on y-axis=lat

and on these two axis I want to shade  or fill co2model values.

Also, after ploting how I can draw the continent line, for
example the data belongs North America, so how the plot
recognise the continent line.

Can this type of color plot is possible in R.

Pls help !!!


Many thanks in advance,

Regarda,
Yogesh

-

===========================================
Yogesh K. Tiwari,
Max-Planck Institute for Biogeochemistry,
Beutenberg Campus, Hans-Knoell-Strasse 10,
D-07745 Jena,
Germany

Office   : 0049 3641 576376
Home     : 0049 3641 223163
Fax      : 0049 3641 577300
Handy    : 0049 173 698 8789
e-mail   : yogesh.tiwari at bgc-jena.mpg.de



From Achim.Zeileis at wu-wien.ac.at  Fri Jan 14 18:08:10 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 14 Jan 2005 18:08:10 +0100 (CET)
Subject: [R] R package classification
In-Reply-To: <20050114153951.22145.qmail@web26309.mail.ukl.yahoo.com>
References: <20050114153951.22145.qmail@web26309.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.58.0501141800060.13138@thorin.ci.tuwien.ac.at>

On Fri, 14 Jan 2005, Dan Bebber wrote:

> Dear list,
>
> there are now >400 packages available on CRAN.
> Would it be useful to classify these packages
> according to what they do (e.g. classification,
> graphics, spatial statistics), to assist the user in
> finding the appropriate package for their problem? Or
> perhaps the search facility is enough.
> I would attempt such a classification, but my
> knowledge of statistical methods isn't good enough.

We are currently playing around with a concept of "task views" for CRAN.
The idea is that there are maintained "views" on CRAN that highlight which
CRAN packages are useful for, say, biostats, econometrics, machine
learning, etc. So there should be a web page giving a summary and a
package list that could also be queried for automatic package
installation.
Although there is some prototype code, this still needs more refinement
and discussions etc. But I hope there will be something useful in the not
so distant future.
Z

> Dan Bebber
>
> Department of Plant Sciences
> University of Oxford
> UK
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From nali at umn.edu  Fri Jan 14 17:41:44 2005
From: nali at umn.edu (Na Li)
Date: Fri, 14 Jan 2005 10:41:44 -0600
Subject: [R] empirical (sandwich) SE estimate in lme ()?
Message-ID: <1vacrc3quv.fsf@bass.biostat.umn.edu>


Is it possible to get the empirical (sandwich) S.E. estimates for the
fixed effects in lme () (thus allowing possibly correlated errors within
the group)? In SAS you can get it by the 'empirical' option to PROC MIXED.

Cheers,

Michael

-- 
Na (Michael) Li, Ph.D.               
Division of Biostatistics          A443 Mayo Building, MMC 303   
School of Public Health            420 Delaware St SE            
University of Minnesota            Minneapolis, MN 55455         
Phone: (612) 626-4765              Email: nali at umn.edu                  
Fax:   (612) 626-0660              http://www.biostat.umn.edu/~nali



From baron at psych.upenn.edu  Fri Jan 14 18:58:03 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Fri, 14 Jan 2005 12:58:03 -0500
Subject: [R] R package classification
In-Reply-To: <Pine.LNX.4.58.0501141800060.13138@thorin.ci.tuwien.ac.at>
References: <20050114153951.22145.qmail@web26309.mail.ukl.yahoo.com>
	<Pine.LNX.4.58.0501141800060.13138@thorin.ci.tuwien.ac.at>
Message-ID: <20050114175803.GA25585@psych>

I worry about this proposal because of the following example.
One of the packages I use a lot (because it solves a problem I've 
been mulling for 10 years about how to test individual subjects)
is multtest.  That is part of Bioconductor, and the original
purpose of it was (apparently) for DNA stuff.  I doubt I ever
would have thought to look there.

That said, I actually found it by using my own search engine and
looking for "multiple tests" or something like that.  So I guess
this proposal isn't so bad, so long as people know about search.

Jon

On 01/14/05 18:08, Achim Zeileis wrote:
 We are currently playing around with a concept of "task views" for CRAN.
 The idea is that there are maintained "views" on CRAN that highlight which
 CRAN packages are useful for, say, biostats, econometrics, machine
 learning, etc. So there should be a web page giving a summary and a
 package list that could also be queried for automatic package
 installation.

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From andy_liaw at merck.com  Fri Jan 14 19:00:03 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 14 Jan 2005 13:00:03 -0500
Subject: [R] how to produce 2-d color plots in R
Message-ID: <3A822319EB35174CA3714066D590DCD50994E537@usrymx25.merck.com>

Cut and paste the following function (to be in next release of R, I believe)
and type:

RSiteSearch("election maps")

This, of course, assumes that you have Internet connection.

Andy

RSiteSearch <- function(string,
                        restrict=c("Rhelp02a","Rhelp01","functions","docs"),
                        format="normal", sortby="score", matchesPerPage=20)
{
    string <-
paste("http://finzi.psych.upenn.edu/cgi-bin/namazu.cgi?query=",
                    gsub(" ", "+", string), sep="")
    mpp <- paste("max=", matchesPerPage, sep="")

    format <- charmatch(format, c("normal", "short"))
    if (format == 0) stop("format must be either normal or short")
    format <- paste("result=", switch(format, "normal", "short"), sep="")

    sortby <- charmatch(sortby, c("score", "date:late", "date:early",
                                  "field:subject:ascending",
"field:subject:decending",
                                  "field:from:ascending",
"field:from:decending",
                                  "field:size:ascending",
"field:size:decending",
                                  "field:uri:ascending",
"field:uri:decending",))
    if (sortby == 0) stop("wrong sortby specified")
    sortby <- paste("sort=",
                    switch(sortby, "score", "date:late", "date:early",
                                  "field:subject:ascending",
"field:subject:decending",
                                  "field:from:ascending",
"field:from:decending",
                                  "field:size:ascending",
"field:size:decending",
                                  "field:uri:ascending",
"field:uri:decending"),
                    sep="")

    res <- ""
    if ("Rhelp02a" %in% restrict) res <- "idxname=Rhelp02a"
    if ("Rhelp01" %in% restrict) res <- paste(res,"idxname=Rhelp01",sep="&")
    if ("docs" %in% restrict) res <- paste(res,"idxname=docs",sep="&")
    if ("functions" %in% restrict) res <-
paste(res,"idxname=functions",sep="&")
    if (res=="") {print("Using defaults: Rhelp 2002-; functions; docs.")
      res <-
paste("idxname=Rhelp02a&idxname=functions&idxname=docs",sep="")}
    res <- sub("^&+","",res)

    qstring <- paste(string, mpp, format, sortby, res, sep="&")
    browseURL(qstring)
    invisible(qstring)
}





> From: Yogesh K. Tiwari
> 
> Hello 'R' Users,
> 
> I am very new on 'R', so excuse me if I ask something wrong.
> 
> I have ASCII data and the colums of the data are looks like :-
> 
> 
> !-------------------------
> time,yr,mo,dy,hr,min,sec,lat,lon,ht,co2obs,sigma,co2model
> --
> -
> --
> 
> !----------------------------
> 
> Each column has data value. Now I want to produce 2-d color maps,
> for example the plot should look like :-
> 
> on x-axis =lon    ,   on y-axis=lat
> 
> and on these two axis I want to shade  or fill co2model values.
> 
> Also, after ploting how I can draw the continent line, for
> example the data belongs North America, so how the plot
> recognise the continent line.
> 
> Can this type of color plot is possible in R.
> 
> Pls help !!!
> 
> 
> Many thanks in advance,
> 
> Regarda,
> Yogesh
> 
> -
> 
> ===========================================
> Yogesh K. Tiwari,
> Max-Planck Institute for Biogeochemistry,
> Beutenberg Campus, Hans-Knoell-Strasse 10,
> D-07745 Jena,
> Germany
> 
> Office   : 0049 3641 576376
> Home     : 0049 3641 223163
> Fax      : 0049 3641 577300
> Handy    : 0049 173 698 8789
> e-mail   : yogesh.tiwari at bgc-jena.mpg.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Achim.Zeileis at wu-wien.ac.at  Fri Jan 14 19:11:35 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 14 Jan 2005 19:11:35 +0100 (CET)
Subject: [R] R package classification
In-Reply-To: <20050114175803.GA25585@psych>
References: <20050114153951.22145.qmail@web26309.mail.ukl.yahoo.com>
	<Pine.LNX.4.58.0501141800060.13138@thorin.ci.tuwien.ac.at>
	<20050114175803.GA25585@psych>
Message-ID: <Pine.LNX.4.58.0501141904150.13138@thorin.ci.tuwien.ac.at>

On Fri, 14 Jan 2005, Jonathan Baron wrote:

> I worry about this proposal because of the following example.

I don't think you will get something that satisfies everyone in all
cases...and not doing it for the reason will result in still having >400
unsorted packages on CRAN.

> One of the packages I use a lot (because it solves a problem I've
> been mulling for 10 years about how to test individual subjects)
> is multtest.  That is part of Bioconductor, and the original
> purpose of it was (apparently) for DNA stuff.  I doubt I ever
> would have thought to look there.

I don't really see the point here: if there were, say, a "DNA analysis"
view and multtest were a part of it, how could that be a problem? If there
were another view "social sciences" or "psychology", say, then multtest
could, of course, also be part of it.
The point of task views would be to have maintained lists of packages that
are useful for a certain taks. And there will obviously be packages that
are suitable for several views and there might be some that are suitable
for not a single one.

> That said, I actually found it by using my own search engine and
> looking for "multiple tests" or something like that.  So I guess
> this proposal isn't so bad, so long as people know about search.

It is not meant to replace any search facilities! It should help users
who come to CRAN and say: I want to do biostatistics with R, which of the
>400 packages do I need to look at?
Z

> Jon
>
> On 01/14/05 18:08, Achim Zeileis wrote:
>  We are currently playing around with a concept of "task views" for CRAN.
>  The idea is that there are maintained "views" on CRAN that highlight which
>  CRAN packages are useful for, say, biostats, econometrics, machine
>  learning, etc. So there should be a web page giving a summary and a
>  package list that could also be queried for automatic package
>  installation.
>
> --
> Jonathan Baron, Professor of Psychology, University of Pennsylvania
> Home page: http://www.sas.upenn.edu/~baron
> R search page: http://finzi.psych.upenn.edu/
>



From feferraz at ime.usp.br  Fri Jan 14 19:16:52 2005
From: feferraz at ime.usp.br (Fernando Henrique Ferraz P. da Rosa)
Date: Fri, 14 Jan 2005 16:16:52 -0200
Subject: [R] load object
In-Reply-To: <20050113190021.3312.qmail@web61304.mail.yahoo.com>
References: <20050113190021.3312.qmail@web61304.mail.yahoo.com>
Message-ID: <20050114181652.GA3767@ime.usp.br>

Weiwei Shi writes:
> Hi,
> I happen to re-write my codes to save memory and my
> approach is write my obj into file first and later I
> load it.
> 
> However, it seems like:
> load(filename) can load the object but the function
> returns the name of the object instead of the
> reference to it. For example, I have an object called
> r0.prune, which is saved by
> save(r0.prune, file='r0.prune')
> 
> and later, I want to load it by using:
> load('r0.prune')
> but I need to put the reference to the object r0.prune
> into a var or a list. I tried:
> t<-load('r0.prune'),
> and class(t) gave me a char, which means t stores the
> name of obj instead of the obj itself.

        As load() when used interactively will load it into the Global
workspace by default, you can just use get() on it.

        r0.prune <- 1:10
        save(r0.prune,file='dados.dat')
        rm(r0.prune)
        t <- get(load('dados.dat'))
        t

--
Fernando Henrique Ferraz P. da Rosa
http://www.ime.usp.br/~feferraz



From gunter.berton at gene.com  Fri Jan 14 20:16:10 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 14 Jan 2005 11:16:10 -0800
Subject: [R] empirical (sandwich) SE estimate in lme ()?
In-Reply-To: <1vacrc3quv.fsf@bass.biostat.umn.edu>
Message-ID: <200501141916.j0EJGAZH006681@volta.gene.com>

???
correlated within group errors are explicitly modeled by corStruct classes.
See ?lme and Chapter 5.3 in Bates and Pinheiro.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Na Li
> Sent: Friday, January 14, 2005 8:42 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] empirical (sandwich) SE estimate in lme ()?
> 
> 
> Is it possible to get the empirical (sandwich) S.E. estimates for the
> fixed effects in lme () (thus allowing possibly correlated 
> errors within
> the group)? In SAS you can get it by the 'empirical' option 
> to PROC MIXED.
> 
> Cheers,
> 
> Michael
> 
> -- 
> Na (Michael) Li, Ph.D.               
> Division of Biostatistics          A443 Mayo Building, MMC 303   
> School of Public Health            420 Delaware St SE            
> University of Minnesota            Minneapolis, MN 55455         
> Phone: (612) 626-4765              Email: nali at umn.edu        
>           
> Fax:   (612) 626-0660              http://www.biostat.umn.edu/~nali
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From choudary.jagar at swosu.edu  Fri Jan 14 20:22:19 2005
From: choudary.jagar at swosu.edu (Jagarlamudi, Choudary)
Date: Fri, 14 Jan 2005 13:22:19 -0600
Subject: [R] How to overlay 2 plots on the same device using Axis of Plot1.
Message-ID: <E03EBB50FF2C024781A6E4460AD58F0607C152@swosu-mbx01.admin.swosu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050114/1d24d4b9/attachment.pl

From blindglobe at gmail.com  Fri Jan 14 20:22:36 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Fri, 14 Jan 2005 20:22:36 +0100
Subject: [R] empirical (sandwich) SE estimate in lme ()?
In-Reply-To: <200501141916.j0EJGAZH006681@volta.gene.com>
References: <1vacrc3quv.fsf@bass.biostat.umn.edu>
	<200501141916.j0EJGAZH006681@volta.gene.com>
Message-ID: <1abe3fa905011411224748b23@mail.gmail.com>

Right, but you still can sandwich them if you want.  

(I recently did that in Proc MIXED, but Michael, I'm not sure how to
do it using lme).

best,
-tony


On Fri, 14 Jan 2005 11:16:10 -0800, Berton Gunter
<gunter.berton at gene.com> wrote:
> ???
> correlated within group errors are explicitly modeled by corStruct classes.
> See ?lme and Chapter 5.3 in Bates and Pinheiro.
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
> 
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
> 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Na Li
> > Sent: Friday, January 14, 2005 8:42 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] empirical (sandwich) SE estimate in lme ()?
> >
> >
> > Is it possible to get the empirical (sandwich) S.E. estimates for the
> > fixed effects in lme () (thus allowing possibly correlated
> > errors within
> > the group)? In SAS you can get it by the 'empirical' option
> > to PROC MIXED.
> >
> > Cheers,
> >
> > Michael
> >
> > --
> > Na (Michael) Li, Ph.D.
> > Division of Biostatistics          A443 Mayo Building, MMC 303
> > School of Public Health            420 Delaware St SE
> > University of Minnesota            Minneapolis, MN 55455
> > Phone: (612) 626-4765              Email: nali at umn.edu
> >
> > Fax:   (612) 626-0660              http://www.biostat.umn.edu/~nali
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From aragon at berkeley.edu  Fri Jan 14 20:24:49 2005
From: aragon at berkeley.edu (Tomas Aragon)
Date: Fri, 14 Jan 2005 11:24:49 -0800 (PST)
Subject: [R] Exact poisson confidence intervals
In-Reply-To: <41E6F45B.8080304@statistik.uni-dortmund.de>
Message-ID: <20050114192449.11464.qmail@web80106.mail.yahoo.com>

> Federico Gherardini wrote:
> 
> > Hi all,
> > Is there any R function to compute exact confidence limits for a
> Poisson 
> > distribution with a given Lambda?

Try the 'pois.exact' function in the 'epitools' package.

Tomas



=====
Tomas Aragon, MD, DrPH, Director
Center for Infectious Disease Preparedness
UC Berkeley School of Public Health
1918 University Avenue, 4th Floor, MC-7350
Berkeley, CA 94720-7350
http://www.idready.org



From f.harrell at vanderbilt.edu  Fri Jan 14 20:45:12 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 14 Jan 2005 13:45:12 -0600
Subject: [R] empirical (sandwich) SE estimate in lme ()?
In-Reply-To: <1abe3fa905011411224748b23@mail.gmail.com>
References: <1vacrc3quv.fsf@bass.biostat.umn.edu>	<200501141916.j0EJGAZH006681@volta.gene.com>
	<1abe3fa905011411224748b23@mail.gmail.com>
Message-ID: <41E82148.7010403@vanderbilt.edu>

A.J. Rossini wrote:
> Right, but you still can sandwich them if you want.  
> 
> (I recently did that in Proc MIXED, but Michael, I'm not sure how to
> do it using lme).
> 
> best,
> -tony

The sandwich estimator can help if the model is misspecified but at a 
cost of worse precision in estimating variances.

Frank

> 
> 
> On Fri, 14 Jan 2005 11:16:10 -0800, Berton Gunter
> <gunter.berton at gene.com> wrote:
> 
>>???
>>correlated within group errors are explicitly modeled by corStruct classes.
>>See ?lme and Chapter 5.3 in Bates and Pinheiro.
>>
>>-- Bert Gunter
>>Genentech Non-Clinical Statistics
>>South San Francisco, CA
>>
>>"The business of the statistician is to catalyze the scientific learning
>>process."  - George E. P. Box
>>
>>
>>
>>>-----Original Message-----
>>>From: r-help-bounces at stat.math.ethz.ch
>>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Na Li
>>>Sent: Friday, January 14, 2005 8:42 AM
>>>To: r-help at stat.math.ethz.ch
>>>Subject: [R] empirical (sandwich) SE estimate in lme ()?
>>>
>>>
>>>Is it possible to get the empirical (sandwich) S.E. estimates for the
>>>fixed effects in lme () (thus allowing possibly correlated
>>>errors within
>>>the group)? In SAS you can get it by the 'empirical' option
>>>to PROC MIXED.
>>>
>>>Cheers,
>>>
>>>Michael
>>>
>>>--
>>>Na (Michael) Li, Ph.D.
>>>Division of Biostatistics          A443 Mayo Building, MMC 303
>>>School of Public Health            420 Delaware St SE
>>>University of Minnesota            Minneapolis, MN 55455
>>>Phone: (612) 626-4765              Email: nali at umn.edu
>>>
>>>Fax:   (612) 626-0660              http://www.biostat.umn.edu/~nali
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From blindglobe at gmail.com  Fri Jan 14 21:12:20 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Fri, 14 Jan 2005 21:12:20 +0100
Subject: [R] R package classification
In-Reply-To: <Pine.LNX.4.58.0501141904150.13138@thorin.ci.tuwien.ac.at>
References: <20050114153951.22145.qmail@web26309.mail.ukl.yahoo.com>
	<Pine.LNX.4.58.0501141800060.13138@thorin.ci.tuwien.ac.at>
	<20050114175803.GA25585@psych>
	<Pine.LNX.4.58.0501141904150.13138@thorin.ci.tuwien.ac.at>
Message-ID: <1abe3fa905011412125394b01b@mail.gmail.com>

But similar to Jon's issue, one might imagine the following future
scenario, in 10 years, after R dominates statistics:

    "but it's not in R's biostat task list, so I don't have to know about it!"

(I am NOT being serious, but I've heard similar justifications in the
past, i.e. "it's not part of the curriculum, it wasn't explicit in the
syllabus",etc, etc....)

best,
-tony



On Fri, 14 Jan 2005 19:11:35 +0100 (CET), Achim Zeileis
<Achim.Zeileis at wu-wien.ac.at> wrote:
> On Fri, 14 Jan 2005, Jonathan Baron wrote:
> 
> > I worry about this proposal because of the following example.
> 
> I don't think you will get something that satisfies everyone in all
> cases...and not doing it for the reason will result in still having >400
> unsorted packages on CRAN.
> 
> > One of the packages I use a lot (because it solves a problem I've
> > been mulling for 10 years about how to test individual subjects)
> > is multtest.  That is part of Bioconductor, and the original
> > purpose of it was (apparently) for DNA stuff.  I doubt I ever
> > would have thought to look there.
> 
> I don't really see the point here: if there were, say, a "DNA analysis"
> view and multtest were a part of it, how could that be a problem? If there
> were another view "social sciences" or "psychology", say, then multtest
> could, of course, also be part of it.
> The point of task views would be to have maintained lists of packages that
> are useful for a certain taks. And there will obviously be packages that
> are suitable for several views and there might be some that are suitable
> for not a single one.
> 
> > That said, I actually found it by using my own search engine and
> > looking for "multiple tests" or something like that.  So I guess
> > this proposal isn't so bad, so long as people know about search.
> 
> It is not meant to replace any search facilities! It should help users
> who come to CRAN and say: I want to do biostatistics with R, which of the
> >400 packages do I need to look at?
> Z
> 
> > Jon
> >
> > On 01/14/05 18:08, Achim Zeileis wrote:
> >  We are currently playing around with a concept of "task views" for CRAN.
> >  The idea is that there are maintained "views" on CRAN that highlight which
> >  CRAN packages are useful for, say, biostats, econometrics, machine
> >  learning, etc. So there should be a web page giving a summary and a
> >  package list that could also be queried for automatic package
> >  installation.
> >
> > --
> > Jonathan Baron, Professor of Psychology, University of Pennsylvania
> > Home page: http://www.sas.upenn.edu/~baron
> > R search page: http://finzi.psych.upenn.edu/
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From dray at biomserv.univ-lyon1.fr  Fri Jan 14 22:04:12 2005
From: dray at biomserv.univ-lyon1.fr (Stephane Dray)
Date: Fri, 14 Jan 2005 16:04:12 -0500
Subject: [R] meaning of "iner" etc. relating to cca(ade4)
In-Reply-To: <8E44B938-663E-11D9-A438-0030656E6642@unizh.ch>
Message-ID: <5.2.1.1.0.20050114154745.02032cc0@biomserv.univ-lyon1.fr>

Hi Dominique,
please, if you have some questions about a particular package, do not send 
to R-Help. There is a list about ade4, see details at:

http://pbil.univ-lyon1.fr/ADE-4/adelist.html

cca is a correspondence analysis with constraint imposed by external 
information. Redundancy analysis (available in ade4 by the function pcaiv) 
is PCA with constraint.

Here is an example using pcaiv:

data(rhone)
pca1 <- dudi.pca(rhone$tab, scan = FALSE, nf = 3)
iv1 <- pcaiv(pca1, rhone$disch, scan = FALSE)
iv1$param

  iner inercum inerC inercumC ratio R2    lambda
  6.27 6.27    5.52  5.52     0.879 0.671 3.7
  4.14 10.4    4.74  10.3     0.984 0.747 3.54

iner represent eigenvalues of the unconstrained analysis:
 > pca1$eig[1:2]
[1] 6.274265 4.140947

inercum is equivalent to
cumsum(pca1$eig)

prediction of sites scores by environmental variables produce a new set 
score (often named LC scores), inerC represent variance of this scores:

diag(t(iv1$ls)%*%diag(iv1$lw)%*%as.matrix(iv1$ls))
    Axis1    Axis2
5.515241 4.736362

inercumC is the cumsum.

ratio is simply inercum/inerCcum

Finally note that CCA or RDA find a site score that maximizes the variance 
explained by environmental variables. This is a product of variance by a R2:
  0.671*5.52  # lambda = R2*inerC
[1] 3.70392


At 10:11 14/01/2005, Dominique Gr?ter wrote:
>Does someone know the meaning of the following values relating to the 
>"cca"-command?
>
>  iner, inercum, inerC, inercumC
>
>Thanks!
>DOMI
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

St?phane DRAY
-------------------------------------------------------------------------------------------------- 

D?partement des Sciences Biologiques
Universit? de Montr?al, C.P. 6128, succursale centre-ville
Montr?al, Qu?bec H3C 3J7, Canada

Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From mzp3769 at yahoo.com  Fri Jan 14 22:34:23 2005
From: mzp3769 at yahoo.com (m p)
Date: Fri, 14 Jan 2005 13:34:23 -0800 (PST)
Subject: [R] contour and filled contour plots
Message-ID: <20050114213423.69590.qmail@web51010.mail.yahoo.com>

Hello,
I'd like to remove color bar from "filled.contour"
plot.
Is it possible and how?
I also want to overlay "contour" plot on
filled.contour
but due to scaling of the "filled.contour" to
account for the color bar, aspect ratio for "contour"
is different from "filled.contour". Can this problem
be solved?
Thanks,
Mark



From sway at tanox.com  Fri Jan 14 22:52:30 2005
From: sway at tanox.com (Shawn Way)
Date: Fri, 14 Jan 2005 15:52:30 -0600
Subject: [R] XML
Message-ID: <2DBF8A8E1A1AEE4AB3618AC4D6BF30888D0F1B@houston.tanox.net>

 
 I thought I would take a look at the StatDataML package for some work
I'm doing, but I receive the following error when starting:

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R              
> library(StatDataML)
Loading required package: XML 
Error: package 'XML' could not be loaded
In addition: Warning message: 
There is no package called 'XML' in: library(pkg, character.only = TRUE,
logical = TRUE, lib.loc = lib.loc) 
> 


Realizing that I don't have the Package XML, I've tried to download it,
and I cannot find the file.  Can anyone point me in the right direction,
or at lease upload it to CRAN?

Thanks...

________________________________

"Policies are many, Principles are few, Policies will change, Principles
never do." 
-John C. Maxwell

________________________________

Shawn Way, PE	 Tanox, Inc.	
Engineering Manager	 10301 Stella Link	
Sway at tanox.com 	 Houston, TX 77025	
________________________________

Note: Any use, dissemination, forwarding, printing or copying of this
e-mail without consent of Tanox, Inc. is not authorized.  Further, this
communication may contain confidential information intended only for the
person to whom it is addressed, and any use, dissemination, forwarding,
printing or copying of such confidential information without the express
consent of Tanox or in violation of any agreements to which the
recipient is subject is prohibited.  If you have received this e-mail in
error, please immediately notify the sender and delete the original and
all copies.  Any views or opinions expressed may be solely those of the
author and do not necessarily represent the views or opinions of Tanox,
Inc.



From martin.julien.2 at courrier.uqam.ca  Fri Jan 14 22:44:21 2005
From: martin.julien.2 at courrier.uqam.ca (Martin Julien)
Date: Fri, 14 Jan 2005 16:44:21 -0500
Subject: [R] Centered variables and mixed-model
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E526@usrymx25.merck.com>
Message-ID: <200501142140.j0ELeJBQ028837@intrant.uqam.ca>

I work in biology and I use mixed-model for my data analysis 
In a scientific paper, the author wrote: 
"All continuous exploratory variables were centred on their median value
prior to inclusion in the analysis (Pinheiro & Bates, 2000)."
They refer to the book "Mixed-effects model in S and S-Plus" by Pinheiro et
Bates in 2000.
I feel a bit strange with that paper because I can't find in the book why
they centred the variables on their median.
So I have two question:
First, is it correct to centred the variables on their median in a
mixed-model?
Second, why they do that?
Thank
Julien 

-----Message d'origine-----
De : Liaw, Andy [mailto:andy_liaw at merck.com] 
Envoy? : 13 janvier 2005 19:26
? : 'Martin Julien'; r-help at stat.math.ethz.ch
Objet : RE: [R] Lme and centered variables

I'm by no means expert in such matter, but where did you get the idea that
such manipulation is needed?  If you center the response by its mean, that's
the same as taking out the intercept.  There's no reason to do that
beforehand.  As to centering by median, that's a first for me, and quite
frankly I don't see what it buys you.

Andy

> From: Martin Julien
> 
> I want to fit a linear mixed-model with my data but I want to 
> know if I have
> to center all my responses variables on their median and why 
> I have to do
> it?
> 
> Thank
> 
> Julien Martin
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


----------------------------------------------------------------------------
--
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From francoisromain at free.fr  Fri Jan 14 23:02:17 2005
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Fri, 14 Jan 2005 23:02:17 +0100
Subject: [R] contour and filled contour plots
In-Reply-To: <20050114213423.69590.qmail@web51010.mail.yahoo.com>
References: <20050114213423.69590.qmail@web51010.mail.yahoo.com>
Message-ID: <41E84169.3080003@free.fr>

You might want to try ?image like in :

require(MASS)
x <- rnorm(50)
y <- rnorm(50)
d <- kde2d(x,y)
image(d,col=terrain.colors(50))
contour(d,add=T)


Hope this is what you are looking for.
Romain.


m p a ?crit :

>Hello,
>I'd like to remove color bar from "filled.contour"
>plot.
>Is it possible and how?
>I also want to overlay "contour" plot on
>filled.contour
>but due to scaling of the "filled.contour" to
>account for the color bar, aspect ratio for "contour"
>is different from "filled.contour". Can this problem
>be solved?
>Thanks,
>Mark
>
>  
>
-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From MSchwartz at MedAnalytics.com  Fri Jan 14 23:12:36 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 14 Jan 2005 16:12:36 -0600
Subject: [R] XML
In-Reply-To: <2DBF8A8E1A1AEE4AB3618AC4D6BF30888D0F1B@houston.tanox.net>
References: <2DBF8A8E1A1AEE4AB3618AC4D6BF30888D0F1B@houston.tanox.net>
Message-ID: <1105740756.6934.28.camel@horizons.localdomain>

On Fri, 2005-01-14 at 15:52 -0600, Shawn Way wrote:
>   I thought I would take a look at the StatDataML package for some work
> I'm doing, but I receive the following error when starting:
> 
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.1            
> year     2004           
> month    11             
> day      15             
> language R              
> > library(StatDataML)
> Loading required package: XML 
> Error: package 'XML' could not be loaded
> In addition: Warning message: 
> There is no package called 'XML' in: library(pkg, character.only = TRUE,
> logical = TRUE, lib.loc = lib.loc) 
> > 
> 
> 
> Realizing that I don't have the Package XML, I've tried to download it,
> and I cannot find the file.  Can anyone point me in the right direction,
> or at lease upload it to CRAN?
> 
> Thanks...

The Windows ZIP file for package XML is available here:

http://www.stats.ox.ac.uk/pub/RWin/2.0.0/

thanks to Prof. Ripley.

The current build report for the Windows packages indicates that this is
one of the packages that will not build "out of the box".

The Windows build report is here:

http://cran.us.r-project.org/bin/windows/contrib/checkSummaryWin.html

and the README file that covers this issue is here:

http://cran.us.r-project.org/bin/windows/contrib/2.0/ReadMe

HTH,

Marc Schwartz



From mzp3769 at yahoo.com  Fri Jan 14 23:20:51 2005
From: mzp3769 at yahoo.com (m p)
Date: Fri, 14 Jan 2005 14:20:51 -0800 (PST)
Subject: [R] contour and filled contour plots
In-Reply-To: <41E84169.3080003@free.fr>
Message-ID: <20050114222051.16953.qmail@web51002.mail.yahoo.com>

Thanks, but this does not work correctly.
Contour lines with "contour"  do not coincide
with those produced by "image". Also "image" plots 
contour lines with right angles while "contour" plots
contour lines taking the diagonal between neighboring
points. All looks quite ugly.
If there are options to make them coincide I would
like to hear about them since I can't find them.
BTW: is it possible to change labeling og color
bar for filled.contour?
Mark   

--- Romain Franois <francoisromain at free.fr> wrote:

> You might want to try ?image like in :
> 
> require(MASS)
> x <- rnorm(50)
> y <- rnorm(50)
> d <- kde2d(x,y)
> image(d,col=terrain.colors(50))
> contour(d,add=T)
> 
> 
> Hope this is what you are looking for.
> Romain.
> 
> 
> m p a crit :
> 
> >Hello,
> >I'd like to remove color bar from "filled.contour"
> >plot.
> >Is it possible and how?
> >I also want to overlay "contour" plot on
> >filled.contour
> >but due to scaling of the "filled.contour" to
> >account for the color bar, aspect ratio for
> "contour"
> >is different from "filled.contour". Can this
> problem
> >be solved?
> >Thanks,
> >Mark
> >
> >  
> >
> -- 
> Romain FRANCOIS : francoisromain at free.fr
> page web : http://addictedtor.free.fr/  (en
> construction)
> 06 18 39 14 69 / 01 46 80 65 60
>
_______________________________________________________
> Etudiant en 3eme anne
> Institut de Statistique de l'Universit de Paris
> (ISUP)
> Filire Industrie et Services
> http://www.isup.cicrp.jussieu.fr/
>
_______________________________________________________
> 
> 
>



From gunter.berton at gene.com  Fri Jan 14 23:26:46 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 14 Jan 2005 14:26:46 -0800
Subject: [R] Centered variables and mixed-model
In-Reply-To: <200501142140.j0ELeJBQ028837@intrant.uqam.ca>
Message-ID: <200501142226.j0EMQkMa000815@hertz.gene.com>

(Posted because private reply to sender was kicked back).

Note that you said "centered **response** variables in your original
original post, not centered **explanatory** variables, which is what (your
misquote of) Bates and Pinheiro said. ** Big Difference!** If you don't know
what the difference is, I strongly suggest you consult a statistician.

Centering of explanatory variables can sometimes help numerical accuracy,
but otherwise produces an exactly equivalent linear model with a different
intercept. Obviously, it changes NONlinear models.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Martin Julien
> Sent: Friday, January 14, 2005 1:44 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Centered variables and mixed-model
> 
> I work in biology and I use mixed-model for my data analysis 
> In a scientific paper, the author wrote: 
> "All continuous exploratory variables were centred on their 
> median value
> prior to inclusion in the analysis (Pinheiro & Bates, 2000)."
> They refer to the book "Mixed-effects model in S and S-Plus" 
> by Pinheiro et
> Bates in 2000.
> I feel a bit strange with that paper because I can't find in 
> the book why
> they centred the variables on their median.
> So I have two question:
> First, is it correct to centred the variables on their median in a
> mixed-model?
> Second, why they do that?
> Thank
> Julien 
> 
> -----Message d'origine-----
> De : Liaw, Andy [mailto:andy_liaw at merck.com] 
> Envoy? : 13 janvier 2005 19:26
> ? : 'Martin Julien'; r-help at stat.math.ethz.ch
> Objet : RE: [R] Lme and centered variables
> 
> I'm by no means expert in such matter, but where did you get 
> the idea that
> such manipulation is needed?  If you center the response by 
> its mean, that's
> the same as taking out the intercept.  There's no reason to do that
> beforehand.  As to centering by median, that's a first for 
> me, and quite
> frankly I don't see what it buys you.
> 
> Andy
> 
> > From: Martin Julien
> > 
> > I want to fit a linear mixed-model with my data but I want to 
> > know if I have
> > to center all my responses variables on their median and why 
> > I have to do
> > it?
> > 
> > Thank
> > 
> > Julien Martin
> > 
> >  
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
> --------------------------------------------------------------
> --------------
> --
> Notice:  This e-mail message, together with any 
> attachments,...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From francoisromain at free.fr  Fri Jan 14 23:42:09 2005
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Fri, 14 Jan 2005 23:42:09 +0100
Subject: [R] contour and filled contour plots
In-Reply-To: <20050114222051.16953.qmail@web51002.mail.yahoo.com>
References: <20050114222051.16953.qmail@web51002.mail.yahoo.com>
Message-ID: <41E84AC1.7060601@free.fr>

Hello again,

look at the data provided by d, it's a list with:
- x  : the 1st coordinate
- y : the 2nde coordinate
- z : a matrix of dimension 25 x 25 (the rectangles shown by image are
the representation of those 625 points, the contour
function uses interpolation.

See the n parameter in kde2d function, if you specify n=100, you won't
be able to see the rectangles, and that will be as pretty as you want.

Hope that clarify a bit.

Romain

m p a ?crit :

>Thanks, but this does not work correctly.
>Contour lines with "contour"  do not coincide
>with those produced by "image". Also "image" plots 
>contour lines with right angles while "contour" plots
>contour lines taking the diagonal between neighboring
>points. All looks quite ugly.
>If there are options to make them coincide I would
>like to hear about them since I can't find them.
>BTW: is it possible to change labeling og color
>bar for filled.contour?
>Mark   
>
>--- Romain Fran?ois <francoisromain at free.fr> wrote:
>
>  
>
>>You might want to try ?image like in :
>>
>>require(MASS)
>>x <- rnorm(50)
>>y <- rnorm(50)
>>d <- kde2d(x,y)
>>image(d,col=terrain.colors(50))
>>contour(d,add=T)
>>
>>
>>Hope this is what you are looking for.
>>Romain.
>>
>>
>>m p a ?crit :
>>
>>    
>>
>>>Hello,
>>>I'd like to remove color bar from "filled.contour"
>>>plot.
>>>Is it possible and how?
>>>I also want to overlay "contour" plot on
>>>filled.contour
>>>but due to scaling of the "filled.contour" to
>>>account for the color bar, aspect ratio for
>>>      
>>>
>>"contour"
>>    
>>
>>>is different from "filled.contour". Can this
>>>      
>>>
>>problem
>>    
>>
>>>be solved?
>>>Thanks,
>>>Mark
>>>
>>> 
>>>
>>>      
>>>

-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From francoisromain at free.fr  Fri Jan 14 23:43:02 2005
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Fri, 14 Jan 2005 23:43:02 +0100
Subject: [R] Help in Overlaying of 2 Plots on the same Device.
In-Reply-To: <E03EBB50FF2C024781A6E4460AD58F0607C151@swosu-mbx01.admin.swosu.edu>
References: <E03EBB50FF2C024781A6E4460AD58F0607C151@swosu-mbx01.admin.swosu.edu>
Message-ID: <41E84AF6.6010208@free.fr>

Hello, it would be nice (as it is asked in the posting guide) to post 
something i can actually run on my machine, since i have no idea what 
are xvals, fr, .. i can only assume what you want.

1) if you want the second plot to overlay the first one (generated by 
hist) you might want to use ?points instead of plot.

2) Second, please look at the dimension of ds$y (usually 512) and the 
dimension of xvals[1:fr] (once more i can only assume), that's not the same.

3) It's not because you demand the density with values that lies in the 
interval [a,b] that the support of the density is [a,b], so it's not a 
problem to have that line. you may try

points(ds$x[someSubset],ds$y[someSubset],type="l")


Good luck with your problem (which is not perfectly clear for me, i'm 
not sure i got you clear)

Romain.

Jagarlamudi, Choudary a ?crit :

> Thank you very much for your time and response.
> Idid the following:
> x1<-range(xvals)
> hist(xvals,xlim=x1,ylim=NULL)
> #now i'd like to plot a density plot only for the 1st 2 bars of the 
> histogram by passing xvals in that range.
> ds<-density(xvals[1:fr],bw=30,n=fr)
> plot(ds$y ~ xvals[1:fr])
>  
> My second plot creates its own axis and gives a line going across the 
> histogram(~10bars), when it should
> only extend upto the second bar coz i passed only values upto bar 2 
> range. I need to use the histogram axis for my second plot as well. 
> Please let me know if i can provide more detail.
> Thanks in advance.
>  
>  
> *Choudary Jagarlamudi*
> *Instructor*
> *Southwestern Oklahoma State University*
> *STF 254*
> *100 campus Drive*
> *Weatherford OK 73096*
> *Tel 580-774-7136*
>
> ------------------------------------------------------------------------
> *From:* Romain Fran?ois [mailto:francoisromain at free.fr]
> *Sent:* Fri 1/14/2005 10:39 AM
> *To:* Jagarlamudi, Choudary
> *Subject:* Re: [R] Help in Overlaying of 2 Plots on the same Device.
>
> Is that what you are looking for ?
>
> x <- rnorm(50)
>
> hist(x,freq=F)
>
> points(density(x),type="l")
>
>
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
> It will tell you to post some code of what you tried.
>
> Regards.
>
> Romain.
>
>
> Jagarlamudi, Choudary a ?crit :
>
> >I'm trying to overlay a density plot on a previously plotted 
> histogram. However, i need to use the same axis as of the 
> 1st(histogram) plot to plot the second. My second plot is creating its 
> own axis and causing my plot to extend the entire histogram instead of 
> getting a subplot on a portion of the histogram. I tried 'fig' and 
> 'new' parameters with no luck.
> >Thanks in advance.
> >
> >Choudary Jagarlamudi
> >Instructor
> >Southwestern Oklahoma State University
> >STF 254
> >100 campus Drive
> >Weatherford OK 73096
> >Tel 580-774-7136
> > 
> >
>
> --
> Romain FRANCOIS : francoisromain at free.fr
> page web : http://addictedtor.free.fr/  (en construction)
> 06 18 39 14 69 / 01 46 80 65 60
> _______________________________________________________
> Etudiant en 3eme ann?e
> Institut de Statistique de l'Universit? de Paris (ISUP)
> Fili?re Industrie et Services
> http://www.isup.cicrp.jussieu.fr/
> _______________________________________________________
>
>

-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From tlumley at u.washington.edu  Fri Jan 14 23:45:10 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 14 Jan 2005 14:45:10 -0800 (PST)
Subject: [R] contour and filled contour plots
In-Reply-To: <20050114213423.69590.qmail@web51010.mail.yahoo.com>
References: <20050114213423.69590.qmail@web51010.mail.yahoo.com>
Message-ID: <Pine.A41.4.61b.0501141442540.64538@homer09.u.washington.edu>

On Fri, 14 Jan 2005, m p wrote:

> Hello,
> I'd like to remove color bar from "filled.contour"
> plot.
> Is it possible and how?
> I also want to overlay "contour" plot on
> filled.contour

You can do the overlay by putting the call to contour() in the plot.axes= 
argument of filled.contour.

 	-thomas



From mzp3769 at yahoo.com  Fri Jan 14 23:59:52 2005
From: mzp3769 at yahoo.com (m p)
Date: Fri, 14 Jan 2005 14:59:52 -0800 (PST)
Subject: [R] contour and filled contour plots
In-Reply-To: <Pine.A41.4.61b.0501141442540.64538@homer09.u.washington.edu>
Message-ID: <20050114225952.97254.qmail@web51010.mail.yahoo.com>

Yes, this works perfectly. I still need to
change the labeling on the label bar since it 
labels reals (1, 1.5, 2., 2.5 etc) which does not
make sense for this plot - it's a number of iterations
which needs to integer. Anything can be done?
Thanks,
Mark

--- Thomas Lumley <tlumley at u.washington.edu> wrote:

> On Fri, 14 Jan 2005, m p wrote:
> 
> > Hello,
> > I'd like to remove color bar from "filled.contour"
> > plot.
> > Is it possible and how?
> > I also want to overlay "contour" plot on
> > filled.contour
> 
> You can do the overlay by putting the call to
> contour() in the plot.axes= 
> argument of filled.contour.
> 
>  	-thomas
> 
>



From stonehusky at gmail.com  Sat Jan 15 01:34:25 2005
From: stonehusky at gmail.com (Donna-n-Doug Finner)
Date: Fri, 14 Jan 2005 19:34:25 -0500
Subject: [R] Seeking pointers to help
Message-ID: <529e16e0050114163416cba8d2@mail.gmail.com>

Noob alert....
I've been through most of the available documentation at a fairly high
level - if I've missed the obvious, just point me in the right
direction.

What I'd like to build:
Windows XP system (actually, I'd prever a *nix but must use Win).
Backend data stored in MySQL db
Front end user interface built using PHP/HTML running on Apache.

User builds a select via the web front end.
User selects a canned analysis or graph for the data set.
System grabs the data, passes the data to R, magic happens, graphs displayed.
User isn't exposed to the R-GUI.

Is this kind of design possible?
Without going into detail, how would this kind of system be
structured?  What keywords can I look up that fill in the blanks?  Are
there any samples of this type of system I can see and/or use?
Is there another way I should be thinking about this kind of design?

Additional wrinkle - we may need to substitute Excel as the raw data
source.  I've been playing with converting XL files to tab sep var
files and had some level of luck.  I haven't given the ODBC method a
try yet since that was a 'not' recommended method.

I'm sure I'll have more to follow.  This tool looks pretty neat. 
Thanks in advance for any tips or pointers.

Doug



From m.abdolell at utoronto.ca  Sat Jan 15 01:52:06 2005
From: m.abdolell at utoronto.ca (Mohamed Abdolell)
Date: Fri, 14 Jan 2005 19:52:06 -0500
Subject: [R] Installing R on Xandros 3.0
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E531@usrymx25.merck.com>
Message-ID: <ALEJLOAANHOJCNBLGOFBGEAPCBAA.m.abdolell@utoronto.ca>

Hi Andy,

Your comment that the difference sounded peculiar got me checking my code
and notes again, especially since all OS installations were in fact on the
same machine.

I'm embarrassed to say that discovered I had added an extra few loops to my
algorithm when I ran it on Xandros ... explaining the descrepancy of about 1
minute between Xandros and Quantian/WinXP.

Thanks.

- Mohamed


-----Original Message-----
From: Liaw, Andy [mailto:andy_liaw at merck.com]
Sent: 14 January 2005 09:17
To: 'Mohamed Abdolell'
Cc: r-help at r-project.org
Subject: RE: [R] Installing R on Xandros 3.0


Hi Mohamed,

> I certainly deffer to both you and Dirk on this ... my
> suggestions were only
> ment to get an installation of R up and running on Xandros.

If you are using Xandros, then your information is better than my
speculation.  If you were able to use install.packages(), you must have the
devel tools installed.

> Perhaps, either you or Dirk could comment on the following
> for me ... when I
> tried the installation on Xandros using the method I
> indicated, everything
> seemed to install.  But, when I tried running some R code I'd
> written and
> run on WinXP it actually ran slower than on either WinXP or
> on Quantian ...
> would this have something to do with the way I installed R?

Is the difference observed on the same machine?  If so, that sound very
peculiar, as the R installation on Quantian should be the same as the .deb
file you install from on Xandros...  If you can, please show us example code
and timings.

Andy


----------------------------------------------------------------------------
--
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From tlumley at u.washington.edu  Sat Jan 15 02:03:05 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 14 Jan 2005 17:03:05 -0800 (PST)
Subject: [R] contour and filled contour plots
In-Reply-To: <20050114225952.97254.qmail@web51010.mail.yahoo.com>
References: <20050114225952.97254.qmail@web51010.mail.yahoo.com>
Message-ID: <Pine.A41.4.61b.0501141701380.64538@homer09.u.washington.edu>

On Fri, 14 Jan 2005, m p wrote:

> Yes, this works perfectly. I still need to
> change the labeling on the label bar since it
> labels reals (1, 1.5, 2., 2.5 etc) which does not
> make sense for this plot - it's a number of iterations
> which needs to integer. Anything can be done?

You can specify the levels, and also the key.axes= argument controls the 
axes on the key.

 	-thomas



From sdavis2 at mail.nih.gov  Sat Jan 15 02:32:37 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 14 Jan 2005 20:32:37 -0500
Subject: [R] Seeking pointers to help
In-Reply-To: <529e16e0050114163416cba8d2@mail.gmail.com>
Message-ID: <BE0DDCE5.3541%sdavis2@mail.nih.gov>

On 1/14/05 19:34, "Donna-n-Doug Finner" <stonehusky at gmail.com> wrote:

> Noob alert....
> I've been through most of the available documentation at a fairly high
> level - if I've missed the obvious, just point me in the right
> direction.
> 
> What I'd like to build:
> Windows XP system (actually, I'd prever a *nix but must use Win).
> Backend data stored in MySQL db
> Front end user interface built using PHP/HTML running on Apache.
> 
> User builds a select via the web front end.
> User selects a canned analysis or graph for the data set.
> System grabs the data, passes the data to R, magic happens, graphs displayed.
> User isn't exposed to the R-GUI.
> 
> Is this kind of design possible?
> Without going into detail, how would this kind of system be
> structured?  What keywords can I look up that fill in the blanks?  Are
> there any samples of this type of system I can see and/or use?
> Is there another way I should be thinking about this kind of design?
> 
> Additional wrinkle - we may need to substitute Excel as the raw data
> source.  I've been playing with converting XL files to tab sep var
> files and had some level of luck.  I haven't given the ODBC method a
> try yet since that was a 'not' recommended method.
> 
> I'm sure I'll have more to follow.  This tool looks pretty neat.
> Thanks in advance for any tips or pointers.
> 
> Doug
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
It sounds like you are early in the design process?  Have you looked at
PostgreSQL and Pl/R?

You can simply pipe the data to R in batch mode, write the graphs generated
by R to temporary files, and then use those in your generated HTML.  There
are numerous posts about running R in batch mode.  As for using R with PHP,
someone else will have to comment about specifics.

Sean



From aar420 at hotmail.com  Sat Jan 15 02:42:57 2005
From: aar420 at hotmail.com (aar paar)
Date: Sat, 15 Jan 2005 01:42:57 +0000
Subject: [R] UNIROOT
Message-ID: <BAY21-F293338189AF3D268C244E68B8C0@phx.gbl>


f(x) = .01*(1.2^y) integrated from 0 to x

Find x such that

f(x) = some number, say 4

How do we do it in R?



From lists at norvelle.org  Sat Jan 15 05:39:00 2005
From: lists at norvelle.org (List account)
Date: Sat, 15 Jan 2005 05:39:00 +0100
Subject: [R] Newbie question regarding graphing of Princomp object
Message-ID: <608865D8-66AF-11D9-BD23-000A9583BF06@norvelle.org>

Greetings,

I am working on a stylometric analysis of some latin texts; one of the  
latest stylometric techniques involves using principal components  
analysis.  Not being a statistician, I can't really fully rely on PCA  
as my primary tool, since I don't really understand the statistics  
behind the PCA technique.  Nevertheless, the ability to use PCA and  
graph the results has been marvelously helpful as a preliminary  
technique to determine what kinds of stylometric variables are worth  
pursuing as indicators of authorship.

For instance, I'm doing the following...  I have a set of data for  
approximately 120 different latin works, about half of which are by St.  
Thomas Aquinas, and the other half are by various other authors in the  
Thomistic tradition, some known and some anonymous.  My data for  
frequencies of prepositions looks like the following:

A,AD,CIRCA,CUM,DE, .... (total of 10 variables)
1,0.00967667222531036,0.0208124884194923,0.00142671854734112,0.004863813 
22957198,0.00758291643505651 ...
2,0.00874917700292081,0.0217315416668508,0.00133005165549453,0.004379007 
27772451,0.00537323193714733 ....
3,0.0064258378627327,0.0280901956627422,0.00178739176045295,0.0043058230 
9573329,0.00821688482105979 ....
4,0.00706850368364528,0.027446604903448,0.000821141574836712,0.004617615 
47172807,0.00812783899774761 ....
5,0.010214039424891,0.015409971157808,0.000745993537614122,0.00584650749 
246416,0.00475787738815518 ....
6,0.00952534711010655,0.0180981595092025,0.00125928317726832,0.005150145 
30190507,0.00447206974491443 ...
.... (and so on for the rest of the 120 works)

The works are numbered such that works 100 and below are by St. Thomas,  
those from 101 to 117 are of dubious authenticity, and those from 118  
to 179 are by other authors.

When I perform a biplot, on the results of the princomp() function, I  
get a nice graph that plots the 120 works on the two principal  
component axes (I've figured out how to get rid of the red arrows  
already).  Given that the data points tend to jumble together, I'd like  
some way to color the different categories of works in the biplot, so  
that data points for works 1-100 are red, those from 101-117 are blue,  
and those from 118 to 179 are green (for instance).

I've included a sample of the output that I'm currently getting, in  
case it's helpful to anybody.  BTW, I am running RAqua (for the Mac),  
version 1.8.1.

Thanks in advance for any help!

-Erik Norvelle
erik (at) norvelle (dot) org
Facultad de Filosof?a y Letras
Universidad de Navarra
Pamplona, Navarra, Espa?a

-------------- next part --------------
A non-text attachment was scrubbed...
Name: prepositions.pdf
Type: application/pdf
Size: 12639 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20050115/3611db92/prepositions.pdf
-------------- next part --------------
  

From ggrothendieck at myway.com  Sat Jan 15 08:16:49 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 15 Jan 2005 07:16:49 +0000 (UTC)
Subject: [R] UNIROOT
References: <BAY21-F293338189AF3D268C244E68B8C0@phx.gbl>
Message-ID: <loom.20050115T080452-787@post.gmane.org>

aar paar <aar420 <at> hotmail.com> writes:

: 
: f(x) = .01*(1.2^y) integrated from 0 to x
: 
: Find x such that
: 
: f(x) = some number, say 4
: 
: How do we do it in R?

You don't need R for this.

Your integral can be solved analytically, check any 
table of integrals, and the rest can be solved using
elementary algebra so R is not needed here.  If you 
really do want to use R anyways:

g <- function(y) .01 * 1.2^y
f <- function(x) integrate(g, 0, x)$value - 4
uniroot(f, low = -100, up = 100)



From jwd at surewest.net  Sat Jan 15 08:29:15 2005
From: jwd at surewest.net (John Dougherty)
Date: Fri, 14 Jan 2005 23:29:15 -0800
Subject: [R] pair() and X11 fonts error
Message-ID: <200501142329.15173.jwd@surewest.net>

I am encountering an font error when using either plot() or pairs() for a 
scatter plot matrice under some circumstances.  For instances pairs(hills) 
using the "hills" data set in MASS results in the following error:

"Error in text.default(x, y, txt, cex = cex, font = font) :
        X11 font at size 16 could not be loaded"

However pairs(iris) works fine.  Since the iris data set contains considerably 
more variables than hills, I am at sea regarding the correction.

JWDougherty



From dr.mike at ntlworld.com  Sat Jan 15 08:22:51 2005
From: dr.mike at ntlworld.com (dr mike)
Date: Sat, 15 Jan 2005 07:22:51 -0000
Subject: [R] XML
In-Reply-To: <1105740756.6934.28.camel@horizons.localdomain>
Message-ID: <20050115072309.SEYH8064.aamta07-winn.mailhost.ntl.com@c400>

This cropped up in a previous posting here (Monday this week, in fact),
responded to by Prof. Ripley. It also cropped up on the Bioconductor mailing
list before Christmas, in relation to the AnnBuilder package. For those who
have reposTools installed, the solution provided by John Zhang was, from the
command line enter:

>library(reposTools)
>install.pacakges2("XML")

Regards,

Mike

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc Schwartz
Sent: 14 January 2005 22:13
To: Shawn Way
Cc: R-Help
Subject: Re: [R] XML

On Fri, 2005-01-14 at 15:52 -0600, Shawn Way wrote:
>   I thought I would take a look at the StatDataML package for some 
> work I'm doing, but I receive the following error when starting:
> 
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.1            
> year     2004           
> month    11             
> day      15             
> language R              
> > library(StatDataML)
> Loading required package: XML
> Error: package 'XML' could not be loaded In addition: Warning message:
> There is no package called 'XML' in: library(pkg, character.only = 
> TRUE, logical = TRUE, lib.loc = lib.loc)
> > 
> 
> 
> Realizing that I don't have the Package XML, I've tried to download 
> it, and I cannot find the file.  Can anyone point me in the right 
> direction, or at lease upload it to CRAN?
> 
> Thanks...

The Windows ZIP file for package XML is available here:

http://www.stats.ox.ac.uk/pub/RWin/2.0.0/

thanks to Prof. Ripley.

The current build report for the Windows packages indicates that this is one
of the packages that will not build "out of the box".

The Windows build report is here:

http://cran.us.r-project.org/bin/windows/contrib/checkSummaryWin.html

and the README file that covers this issue is here:

http://cran.us.r-project.org/bin/windows/contrib/2.0/ReadMe

HTH,

Marc Schwartz

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jwd at surewest.net  Sat Jan 15 08:41:27 2005
From: jwd at surewest.net (John Dougherty)
Date: Fri, 14 Jan 2005 23:41:27 -0800
Subject: [R] pair() and X11 fonts error
In-Reply-To: <200501142329.15173.jwd@surewest.net>
References: <200501142329.15173.jwd@surewest.net>
Message-ID: <200501142341.27174.jwd@surewest.net>

On Friday 14 January 2005 23:29, John Dougherty wrote:
> I am encountering an font error when using either plot() or pairs() for a
> scatter plot matrice under some circumstances.  For instances pairs(hills)
> using the "hills" data set in MASS results in the following error:
>
> "Error in text.default(x, y, txt, cex = cex, font = font) :
>         X11 font at size 16 could not be loaded"
>
> However pairs(iris) works fine.  Since the iris data set contains
> considerably more variables than hills, I am at sea regarding the
> correction.
>
Managed to forget to list the OS.  I'm running SuSE 9.2.

John



From ripley at stats.ox.ac.uk  Sat Jan 15 09:23:02 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 15 Jan 2005 08:23:02 +0000 (GMT)
Subject: [R] Seeking pointers to help
In-Reply-To: <BE0DDCE5.3541%sdavis2@mail.nih.gov>
References: <BE0DDCE5.3541%sdavis2@mail.nih.gov>
Message-ID: <Pine.LNX.4.61.0501150809230.1888@gannet.stats>

On Fri, 14 Jan 2005, Sean Davis wrote:

> On 1/14/05 19:34, "Donna-n-Doug Finner" <stonehusky at gmail.com> wrote:
>
>> Noob alert....
>> I've been through most of the available documentation at a fairly high
>> level - if I've missed the obvious, just point me in the right
>> direction.

Can I suggest technical questions like this are better sent to R-devel?
Few R users will know what

>> Backend data stored in MySQL db
>> Front end user interface built using PHP/HTML running on Apache.

means.

>> What I'd like to build:
>> Windows XP system (actually, I'd prever a *nix but must use Win).
>> Backend data stored in MySQL db
>> Front end user interface built using PHP/HTML running on Apache.
>>
>> User builds a select via the web front end.
>> User selects a canned analysis or graph for the data set.
>> System grabs the data, passes the data to R, magic happens, graphs displayed.
>> User isn't exposed to the R-GUI.
>>
>> Is this kind of design possible?

Yes.  There are a number of existing R-cgi interfaces, although they seem 
primarily designed for Unix.  See section 4 of the FAQ.

>> Without going into detail, how would this kind of system be
>> structured?  What keywords can I look up that fill in the blanks?  Are
>> there any samples of this type of system I can see and/or use?
>> Is there another way I should be thinking about this kind of design?
>>
>> Additional wrinkle - we may need to substitute Excel as the raw data
>> source.  I've been playing with converting XL files to tab sep var
>> files and had some level of luck.  I haven't given the ODBC method a
>> try yet since that was a 'not' recommended method.

Not recommended by whom?  It's the obvious choice to me.

>> I'm sure I'll have more to follow.  This tool looks pretty neat.
>> Thanks in advance for any tips or pointers.

> It sounds like you are early in the design process?  Have you looked at
> PostgreSQL and Pl/R?

Would that be PL/R from http://www.joeconway.com/plr/?
AFAIK it does not work on Windows since it depends on a platform-specific 
way to embed R.

> You can simply pipe the data to R in batch mode, write the graphs generated
> by R to temporary files, and then use those in your generated HTML.  There
> are numerous posts about running R in batch mode.  As for using R with PHP,
> someone else will have to comment about specifics.

That's going to be really kludgy for a Web interface, especially under 
Windows.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tobias.verbeke at telenet.be  Sat Jan 15 09:47:06 2005
From: tobias.verbeke at telenet.be (Tobias Verbeke)
Date: Sat, 15 Jan 2005 08:47:06 +0000
Subject: [R] Newbie question regarding graphing of Princomp object
In-Reply-To: <608865D8-66AF-11D9-BD23-000A9583BF06@norvelle.org>
References: <608865D8-66AF-11D9-BD23-000A9583BF06@norvelle.org>
Message-ID: <20050115084706.658a4721.tobias.verbeke@telenet.be>

On Sat, 15 Jan 2005 05:39:00 +0100
List account <lists at norvelle.org> wrote:

> Greetings,
> 
> I am working on a stylometric analysis of some latin texts; one of the  
> latest stylometric techniques involves using principal components  
> analysis.  Not being a statistician, I can't really fully rely on PCA  
> as my primary tool, since I don't really understand the statistics  
> behind the PCA technique.  Nevertheless, the ability to use PCA and  
> graph the results has been marvelously helpful as a preliminary  
> technique to determine what kinds of stylometric variables are worth  
> pursuing as indicators of authorship.
> 
> For instance, I'm doing the following...  I have a set of data for  
> approximately 120 different latin works, about half of which are by St.  
> Thomas Aquinas, and the other half are by various other authors in the  
> Thomistic tradition, some known and some anonymous.  My data for  
> frequencies of prepositions looks like the following:
> 
> A,AD,CIRCA,CUM,DE, .... (total of 10 variables)
> 1,0.00967667222531036,0.0208124884194923,0.00142671854734112,0.004863813 
> 22957198,0.00758291643505651 ...
> 2,0.00874917700292081,0.0217315416668508,0.00133005165549453,0.004379007 
> 27772451,0.00537323193714733 ....
> 3,0.0064258378627327,0.0280901956627422,0.00178739176045295,0.0043058230 
> 9573329,0.00821688482105979 ....
> 4,0.00706850368364528,0.027446604903448,0.000821141574836712,0.004617615 
> 47172807,0.00812783899774761 ....
> 5,0.010214039424891,0.015409971157808,0.000745993537614122,0.00584650749 
> 246416,0.00475787738815518 ....
> 6,0.00952534711010655,0.0180981595092025,0.00125928317726832,0.005150145 
> 30190507,0.00447206974491443 ...
> .... (and so on for the rest of the 120 works)
> 
> The works are numbered such that works 100 and below are by St. Thomas,  
> those from 101 to 117 are of dubious authenticity, and those from 118  
> to 179 are by other authors.
> 
> When I perform a biplot, on the results of the princomp() function, I  
> get a nice graph that plots the 120 works on the two principal  
> component axes (I've figured out how to get rid of the red arrows  
> already).  Given that the data points tend to jumble together, I'd like  
> some way to color the different categories of works in the biplot, so  
> that data points for works 1-100 are red, those from 101-117 are blue,  
> and those from 118 to 179 are green (for instance).

You can use the `col' argument in the biplot call. In this case, I
would do something like 

biplot(mydata, col = c(rep("red", 100), rep("blue", 17), rep("green", 62)))

For a list of built-in color names, you can type colors() at the R prompt.
For more information on biplot, type ?biplot

VaRiis modis bene fit.

HTH,
Tobias

> I've included a sample of the output that I'm currently getting, in  
> case it's helpful to anybody.  BTW, I am running RAqua (for the Mac),  
> version 1.8.1.
> 
> Thanks in advance for any help!
> 
> -Erik Norvelle
> erik (at) norvelle (dot) org
> Facultad de Filosof?a y Letras
> Universidad de Navarra
> Pamplona, Navarra, Espa?a
> 
>



From p.dalgaard at biostat.ku.dk  Sat Jan 15 11:59:02 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Jan 2005 11:59:02 +0100
Subject: [R] pair() and X11 fonts error
In-Reply-To: <200501142341.27174.jwd@surewest.net>
References: <200501142329.15173.jwd@surewest.net>
	<200501142341.27174.jwd@surewest.net>
Message-ID: <x2acrbm009.fsf@biostat.ku.dk>

John Dougherty <jwd at surewest.net> writes:

> On Friday 14 January 2005 23:29, John Dougherty wrote:
> > I am encountering an font error when using either plot() or pairs() for a
> > scatter plot matrice under some circumstances.  For instances pairs(hills)
> > using the "hills" data set in MASS results in the following error:
> >
> > "Error in text.default(x, y, txt, cex = cex, font = font) :
> >         X11 font at size 16 could not be loaded"
> >
> > However pairs(iris) works fine.  Since the iris data set contains
> > considerably more variables than hills, I am at sea regarding the
> > correction.
> >
> Managed to forget to list the OS.  I'm running SuSE 9.2.

This is a fairly well-known result of SuSE installing only unscalable
Adobe fonts and only in one of 100 and 75 dpi resolutions. R expects
that all the possible fonts are there. This is arguably a bug in R,
but the easy way out is to install the missing fonts. In 9.1, I have

XFree86-fonts-75dpi-4.3.99.902-30
XFree86-fonts-100dpi-4.3.99.902-30

but 9.2 seems to have shifted away from XFree and to x.org, so you'd
have

xorg-x11-fonts-100dpi-6.8.1-15.4.i586.rpm 
xorg-x11-fonts-75dpi-6.8.1-15.4.i586.rpm 

And probably only one of them got installed for you.

[As you may have gathered, the issue is that there is only a small
number of sizes of the Adobe fonts (Helvetica, etc.), and they have
been carefully designed to look good. Attempts to rescale them to
other sizes come out absolutely horrid. The pixel sizes of the fonts
are 8,10,11,12,14,17,18,20,24,25,34. If you play around with xfontsel,
you'll see that some of those are available in both 100 and 75 dpi
variants, but others only in one of them. The logic in the X11 driver
is to select the nearest "adobe-sized" font, but it isn't careful
enough about the case where you have only one resolution installed. So
you go in with size 16, the logic tells you to use size 17 because it
looks better, but you really only had sizes 18 and 14...]

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From stonehusky at gmail.com  Sat Jan 15 13:06:07 2005
From: stonehusky at gmail.com (Donna-n-Doug Finner)
Date: Sat, 15 Jan 2005 07:06:07 -0500
Subject: [R] Seeking pointers to help
In-Reply-To: <BE0DDCE5.3541%sdavis2@mail.nih.gov>
References: <529e16e0050114163416cba8d2@mail.gmail.com>
	<BE0DDCE5.3541%sdavis2@mail.nih.gov>
Message-ID: <529e16e005011504067c9a103d@mail.gmail.com>

Sean,

I'll check out the PostgreSQL Pl/R.

I've poked through the postings on batch processing and don't get it
yet.  I'm new enough to each of these tools that I know I'm missing a
couple of key bits that pull it all together.  I need to play so I can
construct a specific intellegent question.

Thank you for your reply.

Doug


On Fri, 14 Jan 2005 20:32:37 -0500, Sean Davis <sdavis2 at mail.nih.gov> wrote:
> On 1/14/05 19:34, "Donna-n-Doug Finner" <stonehusky at gmail.com> wrote:
>
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> It sounds like you are early in the design process?  Have you looked at
> PostgreSQL and Pl/R?
> 
> You can simply pipe the data to R in batch mode, write the graphs generated
> by R to temporary files, and then use those in your generated HTML.  There
> are numerous posts about running R in batch mode.  As for using R with PHP,
> someone else will have to comment about specifics.
> 
> Sean
> 
>



From tobias.verbeke at telenet.be  Sat Jan 15 16:30:15 2005
From: tobias.verbeke at telenet.be (Tobias Verbeke)
Date: Sat, 15 Jan 2005 15:30:15 +0000
Subject: [R] graphing of Princomp object
In-Reply-To: <312A38E6-6705-11D9-BD23-000A9583BF06@norvelle.org>
References: <608865D8-66AF-11D9-BD23-000A9583BF06@norvelle.org>
	<20050115084706.658a4721.tobias.verbeke@telenet.be>
	<312A38E6-6705-11D9-BD23-000A9583BF06@norvelle.org>
Message-ID: <20050115153015.186bd638.tobias.verbeke@telenet.be>

On Sat, 15 Jan 2005 15:53:18 +0100
List account <lists at norvelle.org> wrote:

> Thanks, Tobias for the response.
> 
> I tried the suggestion you gave, and apparently (at least according to 
> the biplot manpage, only the first two members of the col vector are 
> used, the first to plot the first set of values, i.e. the scores, and 
> the second color is used for the loadings (I think I have that right).  
> At any rate, if I add the clause 'col = c(rep("red", 100), rep("blue", 
> 17), rep("green", 62))' I just get a bunch of red points! :(

You're right. I'm sorry I did not read ?biplot, but only checked it had
a col argument (Semel in anno licet insanire..). 
Anyway, with PCA it is not a good idea to plot both variables and
cases on one single plot, because the temptation is too great to interpret
proximities between variables and cases. You'd better plot two different
graphs, one for the cases and one for the `circle of correlations'.

For plotting the cases, you could make up your own plot using
something similar to this:

library(MASS) # for eqscplot
F1 <- yourpca$score[,1]
F2 <- yourpca$score[,2]
eqscplot(F1, F2, pch = 20)
text(F1, F2, labels = names(F1), 
     col = c(rep("red", 100), 
             rep("blue", 17), 
             rep("green", 62)),
     pos = 3)


Tobias

> Si vales, valeo...
> 
> -Erik



From Gregor.Gorjanc at bfro.uni-lj.si  Sat Jan 15 22:41:11 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Sat, 15 Jan 2005 22:41:11 +0100
Subject: [R] Centered variables and mixed-model
Message-ID: <7FFEE688B57D7346BC6241C55900E7300FD0B9@pollux.bfro.uni-lj.si>

Martin,

You got some fine response already from other r-users. Indeed,
centering explanatory variables is quite common and it really
can have benefits in numerical accuracy. 

However one might also use centering for response variable and
even standardization i.e. x - mean(x) / sd(x). This
might increase the robustness of multivariate analysis, where 
variables have very different scales. For example in biology -
agriculture: amount of milk of cows [ from few kg up to 50 kg or
even more ] and fat percentage in milk [ around 4% ]. Although,
centering of response variables is really not common.

--
Lep pozdrav / With regards,
    Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia
---------------------------------------------------------------

Message: 52
Date: Fri, 14 Jan 2005 16:44:21 -0500
From: "Martin Julien" <martin.julien.2 at courrier.uqam.ca>
Subject: [R] Centered variables and mixed-model
To: <r-help at stat.math.ethz.ch>
Message-ID: <200501142140.j0ELeJBQ028837 at intrant.uqam.ca>
Content-Type: text/plain;       charset="iso-8859-1"

I work in biology and I use mixed-model for my data analysis
In a scientific paper, the author wrote:
"All continuous exploratory variables were centred on their median value
prior to inclusion in the analysis (Pinheiro & Bates, 2000)."
They refer to the book "Mixed-effects model in S and S-Plus" by Pinheiro et
Bates in 2000.
I feel a bit strange with that paper because I can't find in the book why
they centred the variables on their median.
So I have two question:
First, is it correct to centred the variables on their median in a
mixed-model?
Second, why they do that?
Thank
Julien



From robin_gruna at hotmail.com  Sun Jan 16 00:02:25 2005
From: robin_gruna at hotmail.com (Robin Gruna)
Date: Sat, 15 Jan 2005 23:02:25 +0000
Subject: [R] Overlapping grid in plot
Message-ID: <BAY103-F36BF0182CC15DDAAABE551878C0@phx.gbl>

Hi,
I want to create a barchart plot with a grid. The grid is overlapping my
plot and legend, looking not very nice. How can I put the grid into the
background of my plot?
Thank you, Robin



From jeanhee at post.harvard.edu  Sun Jan 16 00:22:16 2005
From: jeanhee at post.harvard.edu (J Chung)
Date: Sat, 15 Jan 2005 18:22:16 -0500
Subject: [R] RMySQL vs. Rdbi
Message-ID: <5.2.1.1.2.20050114130706.00a79db0@pop.rcn.com>

Hello,

I know that the topics of using large datasets in R vs. SAS, using 
PostGreSQL vs MySQL, and using databases with R have been discussed 
extensively on this list and elsewhere. However I hope that I have a 
slightly new combination of the questions here.

I am doing my PhD research on a large dataset and trying to decide whether 
to use PostgreSQL or MySQL with R, or simply use SAS, to which I also have 
access. My databasing experience is currently limited to reading a few 
chapters of a textbook, but I have some general programming experience in 
C, C++, and Perl. I am leaning towards PGSQL at the moment because it seems 
to have more core functions (i.e. less writing for me to do) but  on the 
other hand, our sysadmin already has MySQL installed, and I hear that it's 
faster.

Of PostGreSQL vs. MySQL, which has the more mature interface with R?   Are 
there any issues with RMySQL or Rdbi.PGSQL (or .MySQL) that I should be 
aware of and should they influence my decision for MySQL vs. PGSQL vs. the 
SAS integrated database?

My dataset is about  26G, currently split up into files of 260 MB... about 
540,000 records with 40 "explanatory" variables, many of which are probably 
redundant but I just don't know at the moment. It was way too slow to 
work  with in R using Red Hat Linux machines with 500MB-1G RAM, especially 
when producing plots. Preprocessing using Perl scripts every time I wanted 
to look at a different subset of the data became too tedious. I hope to 
create exploratory graphics such as sunflower plots, and also try some 
lattices to help me get a feel for the data. Then I'm interested in trying 
some stepwise ANOVA,  and finally searching for patterns using discriminant 
analysis, and/or classification trees.

I would greatly appreciate any advice you might have on choosing a 
databasing software environment.

Thank you,
Jean Chung



From murdoch at stats.uwo.ca  Sun Jan 16 00:33:00 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 15 Jan 2005 18:33:00 -0500
Subject: [R] Overlapping grid in plot
In-Reply-To: <BAY103-F36BF0182CC15DDAAABE551878C0@phx.gbl>
References: <BAY103-F36BF0182CC15DDAAABE551878C0@phx.gbl>
Message-ID: <9s9ju0td0505atjkkptqquskjnk2cqb52s@4ax.com>

On Sat, 15 Jan 2005 23:02:25 +0000, "Robin Gruna"
<robin_gruna at hotmail.com> wrote :

>Hi,
>I want to create a barchart plot with a grid. The grid is overlapping my
>plot and legend, looking not very nice. How can I put the grid into the
>background of my plot?

You should show some sample code to illustrate the problem.  There are
several ways to draw a bar chart; how to fix yours depends on which
one you used.

Duncan Murdoch



From cybermax at gmail.com  Sun Jan 16 03:32:33 2005
From: cybermax at gmail.com (Wei-Hao Lin)
Date: Sat, 15 Jan 2005 21:32:33 -0500
Subject: [R] probabilty calculation in SVM
In-Reply-To: <C8696843AE995F4EA4CDC3E2B83482A90A1B7E@mailbox02.cshl.edu>
	(Rajdeep Das's message of "Fri, 14 Jan 2005 11:37:01 -0500")
References: <C8696843AE995F4EA4CDC3E2B83482A90A1B7E@mailbox02.cshl.edu>
Message-ID: <uzmzaqf26.fsf@gmail.com>

"Das, Rajdeep" <das at cshl.edu> writes:

> measure for each prediction. I like to know what is method that is used for
> calculating this probability. Is it calculated using logistic link function?

You can find more details in the section 8 of 

http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf



From robin_gruna at hotmail.com  Sun Jan 16 10:05:01 2005
From: robin_gruna at hotmail.com (Robin Gruna)
Date: Sun, 16 Jan 2005 10:05:01 +0100
Subject: [R] Overlapping grid in plot
References: <BAY103-F36BF0182CC15DDAAABE551878C0@phx.gbl>
	<9s9ju0td0505atjkkptqquskjnk2cqb52s@4ax.com>
Message-ID: <BAY103-DAV14DBBCA4ACC6DF7AACF30A878D0@phx.gbl>

Ok, here is some sample code to my problem

> barplot(c(1,2,4,3,2), legend.text = "Legend")
> grid()

..the lines are crossing my barchart :-(...


----- Original Message ----- 
From: "Duncan Murdoch" <murdoch at stats.uwo.ca>
To: <r-help at stat.math.ethz.ch>
Sent: Sunday, January 16, 2005 12:33 AM
Subject: Re: [R] Overlapping grid in plot


> On Sat, 15 Jan 2005 23:02:25 +0000, "Robin Gruna"
> <robin_gruna at hotmail.com> wrote :
>
> >Hi,
> >I want to create a barchart plot with a grid. The grid is overlapping my
> >plot and legend, looking not very nice. How can I put the grid into the
> >background of my plot?
>
> You should show some sample code to illustrate the problem.  There are
> several ways to draw a bar chart; how to fix yours depends on which
> one you used.
>
> Duncan Murdoch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From bates at wisc.edu  Sun Jan 16 10:25:18 2005
From: bates at wisc.edu (Douglas Bates)
Date: Sun, 16 Jan 2005 03:25:18 -0600
Subject: [R] Centered variables and mixed-model
In-Reply-To: <200501142140.j0ELeJBQ028837@intrant.uqam.ca>
References: <200501142140.j0ELeJBQ028837@intrant.uqam.ca>
Message-ID: <41EA32FE.1070505@wisc.edu>

Martin Julien wrote:
> I work in biology and I use mixed-model for my data analysis 
> In a scientific paper, the author wrote: 
> "All continuous exploratory variables were centred on their median value
> prior to inclusion in the analysis (Pinheiro & Bates, 2000)."
> They refer to the book "Mixed-effects model in S and S-Plus" by Pinheiro et
> Bates in 2000.
> I feel a bit strange with that paper because I can't find in the book why
> they centred the variables on their median.
> So I have two question:
> First, is it correct to centred the variables on their median in a
> mixed-model?
> Second, why they do that?

Well I don't recall Pinheiro and Bates saying that variables needed to 
be centered on their median.  It is often the case that the conditioning 
of the numerical optimization for obtaining parameter estimates is 
improved if explanatory variables are centered in some way but I don't 
know of a particular reason for centering on the median.  Also, as Bert 
Gunter pointed out, this statement is about the explanatory variables 
and not a response variable.

With any statistical model, centering of variables results in a 
reparameterization of the model and one must keep this in mind when 
performing statistical tests.



From murdoch at stats.uwo.ca  Sun Jan 16 11:00:36 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 16 Jan 2005 05:00:36 -0500
Subject: [R] Overlapping grid in plot
In-Reply-To: <BAY103-DAV14DBBCA4ACC6DF7AACF30A878D0@phx.gbl>
References: <BAY103-F36BF0182CC15DDAAABE551878C0@phx.gbl>
	<9s9ju0td0505atjkkptqquskjnk2cqb52s@4ax.com>
	<BAY103-DAV14DBBCA4ACC6DF7AACF30A878D0@phx.gbl>
Message-ID: <5udku0t31keruab31pmfa3uggguee91vii@4ax.com>

On Sun, 16 Jan 2005 10:05:01 +0100, "Robin Gruna"
<robin_gruna at hotmail.com> wrote :

>Ok, here is some sample code to my problem
>
>> barplot(c(1,2,4,3,2), legend.text = "Legend")
>> grid()
>
>..the lines are crossing my barchart :-(...

The reason for this is the way R thinks of graphics, essentially as
ways to put ink on paper.  You draw the grid on top of the existing
plot.

Getting it to look the way you want is a little tricky:  you want to
draw the grid first so the bars appear on top, but R won't know how to
draw the grid until it has drawn the plot.  So the solution is to draw
the plot twice, e.g.

barplot(c(1,2,4,3,2), legend.text = "Legend")
grid(col='black', lty='solid')

oldpar <- par(bg='white') 
# this says to use a solid white background 
# instead of the default one, which is usually transparent.  The
# old colour is saved in oldpar

par(new=T) 
# this says to overwrite the plot

barplot(c(1,2,4,3,2), legend.text = "Legend")
par(oldpar) # restore the old colour	

Duncan Murdoch



From erik at norvelle.org  Sun Jan 16 10:11:56 2005
From: erik at norvelle.org (Erik Norvelle)
Date: Sun, 16 Jan 2005 10:11:56 +0100
Subject: [R] graphing of Princomp object
In-Reply-To: <20050115153015.186bd638.tobias.verbeke@telenet.be>
References: <608865D8-66AF-11D9-BD23-000A9583BF06@norvelle.org>
	<20050115084706.658a4721.tobias.verbeke@telenet.be>
	<312A38E6-6705-11D9-BD23-000A9583BF06@norvelle.org>
	<20050115153015.186bd638.tobias.verbeke@telenet.be>
Message-ID: <ABD6D9FA-679E-11D9-BD23-000A9583BF06@norvelle.org>

Tobias,

Great, works like a charm!  I'm already seeing all kinds of patterns 
that were invisible before.  I appreciate your help.

-Erik

On 15/01/2005, at 16:30, Tobias Verbeke wrote:

> On Sat, 15 Jan 2005 15:53:18 +0100
> List account <lists at norvelle.org> wrote:
>
>> Thanks, Tobias for the response.
>>
>> I tried the suggestion you gave, and apparently (at least according to
>> the biplot manpage, only the first two members of the col vector are
>> used, the first to plot the first set of values, i.e. the scores, and
>> the second color is used for the loadings (I think I have that right).
>> At any rate, if I add the clause 'col = c(rep("red", 100), rep("blue",
>> 17), rep("green", 62))' I just get a bunch of red points! :(
>
> You're right. I'm sorry I did not read ?biplot, but only checked it had
> a col argument (Semel in anno licet insanire..).
> Anyway, with PCA it is not a good idea to plot both variables and
> cases on one single plot, because the temptation is too great to 
> interpret
> proximities between variables and cases. You'd better plot two 
> different
> graphs, one for the cases and one for the `circle of correlations'.
>
> For plotting the cases, you could make up your own plot using
> something similar to this:
>
> library(MASS) # for eqscplot
> F1 <- yourpca$score[,1]
> F2 <- yourpca$score[,2]
> eqscplot(F1, F2, pch = 20)
> text(F1, F2, labels = names(F1),
>      col = c(rep("red", 100),
>              rep("blue", 17),
>              rep("green", 62)),
>      pos = 3)
>
>
> Tobias
>
>> Si vales, valeo...
>>
>> -Erik
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From robertweenink at planet.nl  Sun Jan 16 11:32:58 2005
From: robertweenink at planet.nl (Robert Weenink)
Date: Sun, 16 Jan 2005 11:32:58 +0100
Subject: [R] Multiple plots in one screen
Message-ID: <001801c4fbb6$bf62cf40$0300000a@ROBERT>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050116/e44f9716/attachment.pl

From p.dalgaard at biostat.ku.dk  Sun Jan 16 12:15:55 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Jan 2005 12:15:55 +0100
Subject: [R] Multiple plots in one screen
In-Reply-To: <001801c4fbb6$bf62cf40$0300000a@ROBERT>
References: <001801c4fbb6$bf62cf40$0300000a@ROBERT>
Message-ID: <x28y6tbp5g.fsf@biostat.ku.dk>

Robert Weenink <robertweenink at planet.nl> writes:

> I'm a fairly new user of R and I'm confronted with a problem to which I can't find the solution in any R manual or FAQ.
> 
> I'd like to plot multiple 'graphs' in one plot screen. For example, my data frame would be:
> 
> x     a      b      c
> 1    11    15    18
> 2    26    12    19
> 3    22    17    14
> 
> And I'd like to plot a, b and c against x at the same time in one plot screen.
> 
> How is this done?

With matplot(). There are various possibilities, e.g.

> dd <- read.table(stdin(),header=TRUE)
0: x     a      b      c
1: 1    11    15    18
2: 2    26    12    19
3: 3    22    17    14
4: >
> matplot(dd$x,dd[-1],type="b")

(and you don't actually need the dd$x bit because it will plot against
the index if no x is given).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From david.meyer at wu-wien.ac.at  Sun Jan 16 13:21:34 2005
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Sun, 16 Jan 2005 12:21:34 -0000
Subject: [R] probabilty calculation in SVM
Message-ID: <20040116132126.34edbce0.david.meyer@wu-wien.ac.at>

Raj:

The references given on the help page will tell you.

Best,
David

---------

Hi All,

In package e1071 for SVM based classification, one can get a probability
measure for each prediction. I like to know what is method that is used
for
calculating this probability. Is it calculated using logistic link
function?
Thanks for your help.

Regards,

Raj



From MSchwartz at MedAnalytics.com  Sun Jan 16 17:22:21 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sun, 16 Jan 2005 10:22:21 -0600
Subject: [R] Overlapping grid in plot
In-Reply-To: <5udku0t31keruab31pmfa3uggguee91vii@4ax.com>
References: <BAY103-F36BF0182CC15DDAAABE551878C0@phx.gbl>
	<9s9ju0td0505atjkkptqquskjnk2cqb52s@4ax.com>
	<BAY103-DAV14DBBCA4ACC6DF7AACF30A878D0@phx.gbl>
	<5udku0t31keruab31pmfa3uggguee91vii@4ax.com>
Message-ID: <1105892541.20801.16.camel@horizons.localdomain>

On Sun, 2005-01-16 at 05:00 -0500, Duncan Murdoch wrote:
> On Sun, 16 Jan 2005 10:05:01 +0100, "Robin Gruna"
> <robin_gruna at hotmail.com> wrote :
> 
> >Ok, here is some sample code to my problem
> >
> >> barplot(c(1,2,4,3,2), legend.text = "Legend")
> >> grid()
> >
> >..the lines are crossing my barchart :-(...
> 
> The reason for this is the way R thinks of graphics, essentially as
> ways to put ink on paper.  You draw the grid on top of the existing
> plot.
> 
> Getting it to look the way you want is a little tricky:  you want to
> draw the grid first so the bars appear on top, but R won't know how to
> draw the grid until it has drawn the plot.  So the solution is to draw
> the plot twice, e.g.
> 
> barplot(c(1,2,4,3,2), legend.text = "Legend")
> grid(col='black', lty='solid')
> 
> oldpar <- par(bg='white') 
> # this says to use a solid white background 
> # instead of the default one, which is usually transparent.  The
> # old colour is saved in oldpar
> 
> par(new=T) 
> # this says to overwrite the plot
> 
> barplot(c(1,2,4,3,2), legend.text = "Legend")
> par(oldpar) # restore the old colour	


You can also look at the barplot2() function, which is in the 'gregmisc'
bundle on CRAN and take note of the 'plot.grid' argument, which will
enable a grid to be drawn behind the bars. As an example:

[after installing gregmisc]

library(gplots)
barplot2(c(1, 2, 4, 3, 2), legend.text = "Legend",
         plot.grid = TRUE, grid.lty = "solid")

This will result in horizontal lines behind the bars at the same points
as the y axis default tick marks.

HTH,

Marc Schwartz



From arshia22 at yahoo.com  Sun Jan 16 21:22:04 2005
From: arshia22 at yahoo.com (ebashi)
Date: Sun, 16 Jan 2005 12:22:04 -0800 (PST)
Subject: [R] CGIwithR
Message-ID: <20050116202204.41470.qmail@web81001.mail.yahoo.com>

Dear R users;
I'm trying to use CGIwithR on a linux machine, I have
followed the instructions on the package manual but
still it does not run,
the message that I get is as follows: 
    The requested URL was not found on this server
I used the example trivial, I put trivial.html under
Web directory and trivial.R in cgi-bin directory,
which itself is a subdirectory of Web directory, ( I
have changed the modes of R.cgi and .Rprofile
according to what package says) but i still get
the same message, do you have any tips for me? my
question is that where should for example myscript.R
that is mentioned in the manual, be
placed? (under Web directory or   
under cgi-bin).
besides the path to R and GS, should anything else in
the R.cgi be changed?

many tanx in advance
Sean



From d.firth at warwick.ac.uk  Sun Jan 16 21:50:20 2005
From: d.firth at warwick.ac.uk (David Firth)
Date: Sun, 16 Jan 2005 20:50:20 +0000
Subject: [R] CGIwithR
In-Reply-To: <20050116202204.41470.qmail@web81001.mail.yahoo.com>
References: <20050116202204.41470.qmail@web81001.mail.yahoo.com>
Message-ID: <3C859C66-6800-11D9-BF05-000D93C7F2C4@warwick.ac.uk>

See below for the reply I sent you when you asked me this earlier today.

David

On 16 Jan 2005, at 20:22, ebashi wrote:

> Dear R users;
> I'm trying to use CGIwithR on a linux machine, I have
> followed the instructions on the package manual but
> still it does not run,
> the message that I get is as follows:
>     The requested URL was not found on this server
> I used the example trivial, I put trivial.html under
> Web directory and trivial.R in cgi-bin directory,
> which itself is a subdirectory of Web directory, ( I
> have changed the modes of R.cgi and .Rprofile
> according to what package says) but i still get
> the same message, do you have any tips for me? my
> question is that where should for example myscript.R
> that is mentioned in the manual, be
> placed? (under Web directory or
> under cgi-bin).
> besides the path to R and GS, should anything else in
> the R.cgi be changed?
>
> many tanx in advance
> Sean
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
Dear Sean

Thanks for your email.  It is hard to diagnose your problem at a 
distance, but here is some general stuff:

-- the trivial.html file needs to be in a place that is served by your 
http server (is that the URL that is not found?  ie can you see the 
form in your browser?  it wasn't clear to me from your email.)

-- your web server needs to be configured to allow CGI scripts, and 
probably to specify one or more directories where such scripts are 
allowed to live.  This is often a directory called cgi-bin; but note 
that it is not enough just to create a directory with that name, you 
must tell your webserver through its configuration file(s) that it 
exists and that CGI scripts placed there can be run.  How this is done 
depends on which HTTP server you are running, and I cannot help you 
with that I'm afraid.

-- the trivial.R, R.cgi and .Rprofile files all need to be in one of 
the places where CGI scripts are allowed (by your HTTP server) to live.

I hope that helps.  Good luck.

best wishes,
David



From murray at math.umass.edu  Sun Jan 16 22:09:21 2005
From: murray at math.umass.edu (Murray Eisenberg)
Date: Sun, 16 Jan 2005 16:09:21 -0500
Subject: [R] RWinEdt install problem
Message-ID: <41EAD801.7000003@math.umass.edu>

I cannot get the "R" button to appear in RWinEdt.

I'm running R 2.0.1 under Windows XP.  I did a clean install of the 
latest WinEdt.  Previously (for an earlier installation of WinEdt) I had 
RWinEdt running OK.  Now, even though I have the distributed 
RWinEdt_1.6-2.zip extracted to the right place in the R directory, and 
when I execute

   library(RWinEdt)

in the R Console, when RWinEdt opens I do not have the R button on the 
toolbar.

How do I fix this?

In case it matters, note that I'm using a WinEdt User configuration 
(i.e., a directory that WinEdt refers to as %b) in addition to the main 
WinEdt directory (the one that WinEdt refers to as %B).

-- 
Murray Eisenberg                     murray at math.umass.edu
Mathematics & Statistics Dept.
Lederle Graduate Research Tower      phone 413 549-1020 (H)
University of Massachusetts                413 545-2859 (W)
710 North Pleasant Street            fax   413 545-1801
Amherst, MA 01003-9305



From machud at rbg.informatik.tu-darmstadt.de  Sun Jan 16 21:22:33 2005
From: machud at rbg.informatik.tu-darmstadt.de (Marco Chiarandini)
Date: Sun, 16 Jan 2005 21:22:33 +0100
Subject: [R] Empirical cumulative distribution with censored data
In-Reply-To: <200501141124.j0EB5miU005832@hypatia.math.ethz.ch>
References: <200501141124.j0EB5miU005832@hypatia.math.ethz.ch>
Message-ID: <41EACD09.4040305@rbg.informatik.tu-darmstadt.de>

Dear list,

I would like to plot the empirical cumulative distribution of the time 
needed by a treatment to attain a certain goal. A number of 
experiments is run with a strict time limit. In some experiments the 
goal is attained before the time limit, in other experiments time 
expires before the goal is attained. The situation is very similar to 
survivial analysis with censored data. I tryed the function:

plot(survfit(Surv(time),data=mydata,conf.int=F))

from the package "survival". Nevertheless, what i would like to see is 
an increase of probability as time increases, and not a decrease of 
survival probabilty. I tried to play with ecdf(), but dealing with the 
censored data is quite hard-working in this case.

Is there anything for censored data in ecdf like-functions or a way to 
adapt plot.survfit to my case?

Thank you for consideration,

Ragards,

-------------------------------------------------------------------
Marco Chiarandini,
Fachgebiet Intellektik, Fachbereich Informatik,
Technische Universit?t Darmstadt



From p.dalgaard at biostat.ku.dk  Sun Jan 16 22:24:16 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Jan 2005 22:24:16 +0100
Subject: [R] Empirical cumulative distribution with censored data
In-Reply-To: <41EACD09.4040305@rbg.informatik.tu-darmstadt.de>
References: <200501141124.j0EB5miU005832@hypatia.math.ethz.ch>
	<41EACD09.4040305@rbg.informatik.tu-darmstadt.de>
Message-ID: <x2r7klt6db.fsf@biostat.ku.dk>

Marco Chiarandini <machud at rbg.informatik.tu-darmstadt.de> writes:

> Dear list,
> 
> I would like to plot the empirical cumulative distribution of the time
> needed by a treatment to attain a certain goal. A number of
> experiments is run with a strict time limit. In some experiments the
> goal is attained before the time limit, in other experiments time
> expires before the goal is attained. The situation is very similar to
> survivial analysis with censored data. I tryed the function:
> 
> plot(survfit(Surv(time),data=mydata,conf.int=F))
> 
> from the package "survival". Nevertheless, what i would like to see is
> an increase of probability as time increases, and not a decrease of
> survival probabilty. I tried to play with ecdf(), but dealing with the
> censored data is quite hard-working in this case.
> 
> Is there anything for censored data in ecdf like-functions or a way to
> adapt plot.survfit to my case?

Did you try the fun="event" argument?


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From machud at rbg.informatik.tu-darmstadt.de  Sun Jan 16 21:46:38 2005
From: machud at rbg.informatik.tu-darmstadt.de (Marco Chiarandini)
Date: Sun, 16 Jan 2005 21:46:38 +0100
Subject: [R] Empirical cumulative distribution with censored data
In-Reply-To: <x2r7klt6db.fsf@biostat.ku.dk>
References: <200501141124.j0EB5miU005832@hypatia.math.ethz.ch>	<41EACD09.4040305@rbg.informatik.tu-darmstadt.de>
	<x2r7klt6db.fsf@biostat.ku.dk>
Message-ID: <41EAD2AE.2040106@rbg.informatik.tu-darmstadt.de>

Peter Dalgaard wrote:
> Marco Chiarandini <machud at rbg.informatik.tu-darmstadt.de> writes:
> 
> 
>>Dear list,
>>
>>I would like to plot the empirical cumulative distribution of the time
>>needed by a treatment to attain a certain goal. A number of
>>experiments is run with a strict time limit. In some experiments the
>>goal is attained before the time limit, in other experiments time
>>expires before the goal is attained. The situation is very similar to
>>survivial analysis with censored data. I tryed the function:
>>
>>plot(survfit(Surv(time),data=mydata,conf.int=F))
>>
>>from the package "survival". Nevertheless, what i would like to see is
>>an increase of probability as time increases, and not a decrease of
>>survival probabilty. I tried to play with ecdf(), but dealing with the
>>censored data is quite hard-working in this case.
>>
>>Is there anything for censored data in ecdf like-functions or a way to
>>adapt plot.survfit to my case?
> 
> 
> Did you try the fun="event" argument?


Now yes.

It does what I want indeed.

Thank you a lot!

Marco



From ligges at statistik.uni-dortmund.de  Mon Jan 17 08:37:02 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 17 Jan 2005 08:37:02 +0100
Subject: [R] RWinEdt install problem
In-Reply-To: <41EAD801.7000003@math.umass.edu>
References: <41EAD801.7000003@math.umass.edu>
Message-ID: <41EB6B1E.1090809@statistik.uni-dortmund.de>

Murray Eisenberg wrote:
> I cannot get the "R" button to appear in RWinEdt.
> 
> I'm running R 2.0.1 under Windows XP.  I did a clean install of the 
> latest WinEdt.  Previously (for an earlier installation of WinEdt) I had 
> RWinEdt running OK.  Now, even though I have the distributed 
> RWinEdt_1.6-2.zip extracted to the right place in the R directory, and 
> when I execute
> 
>   library(RWinEdt)
> 
> in the R Console, when RWinEdt opens I do not have the R button on the 
> toolbar.
> 
> How do I fix this?
>
> In case it matters, note that I'm using a WinEdt User configuration 
> (i.e., a directory that WinEdt refers to as %b) in addition to the main 
> WinEdt directory (the one that WinEdt refers to as %B).

If you don't have write access to the WinEdt directory, you have to copy 
the R.ini file to, e.g., your local directory (%b), use the *manual* 
installation procedure as described in the ReadMe, but replace the 
command line args "-e=R.ini" by the full path specification, e.g.: 
"-E=/path/to/%b/R.ini"

In a second (apparently private mail), you told that WinEdt "... 5.4 
builds ... including the most recent one of 14 January 2005..." do not 
work with the recent version of R-WinEdt, because "1. The first time, 
immediately I get a message that WinEdt.ini is corrupted and that a 
default copy (from the RWinEdt distribution I presume) is being used."

Is this version also configured to use a User configuration?

I don't have a WinEdt 5.4 version installed (still working with 5.2), 
but I will certainly take a look during the next one or two weeks.

Uwe Ligges



From paul.hewson at plymouth.ac.uk  Mon Jan 17 10:56:20 2005
From: paul.hewson at plymouth.ac.uk (Paul Hewson)
Date: Mon, 17 Jan 2005 09:56:20 -0000
Subject: [R] pairs: altering pch options on upper and lower panel of
	pairwise scatter plots
Message-ID: <52A8091888A23F47A013223014B6E9FE64BEA8@03-CSEXCH.uopnet.plymouth.ac.uk>

Hello,

I can't figure out how to use the upper.panel and lower.panel options in
pairs to alter the label options for either panel independently of the
other.

I would like to be able to show the pairwise scatter plots for the data
as they are (a vanilla pairs plot?) but separately to be able to label
the points according to a factor level.   It is easy enough to do this
independently, but I can't figure out how to combine the two settings on
the same plot.   I have enclosed a toy example:   

Some made up data, e.g.

## make up some multivariate normal data using mvrnorm from MASS
##
require(MASS)
mu <- c(0,0,0)
digma <- matrix(c(0.9,0.5,0.5, 0.5,0.9,0.5, 0.5,0.5,0.9)
dummy.data <- mvrnorm(100,mu,sigma)
##
## add a factor
##
gender <- as.factor(c(rep("Male",50),rep("Female",50)))




I can create two separate pairs plots (upper and lower panel) as
follows:

## pairs plot just showing the data

pairs(dummy.data, lower.panel = NULL)

## pairs plot where the points are labelled according to a factor

pairs(dummy.data, pch = levels(gender), upper.panel = NULL)


But I would like to be able to combine the two panels on the same plot.
I would be grateful for any advice.   I've tried passing several
combinations of functions to upper.panel and lower.panel but either
they're the wrong function or I'm barking up the wrong tree entirely.

Thanks

Paul Hewson

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
Lecturer in Statistics
University of Plymouth
Drake Circus 
Plymouth PL4 8AA

Tel ++ 44 1752 232778
Email paul.hewson at plymouth.ac.uk



From R-user at zutt.org  Mon Jan 17 11:37:01 2005
From: R-user at zutt.org (R user)
Date: Mon, 17 Jan 2005 11:37:01 +0100
Subject: [R] 3d bar plot
Message-ID: <1105958221.29121.16.camel@dutiih.st.ewi.tudelft.nl>

This graph -> http://www.math.hope.edu/~tanis/dallas/images/disth36.gif
is an example I found at
http://www.math.hope.edu/~tanis/dallas/disth1.html
created by Maple.

Does anybody know how to create something similar in R?

I have a feeling it could be possible using scatterplot3d
(perhaps with type=h, the fourth example in help('scatterplot3d')?),
but I cannot figure it out.

Thanks in advance,
Jonne.



From ccleland at optonline.net  Mon Jan 17 12:10:22 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 17 Jan 2005 06:10:22 -0500
Subject: [R] RWinEdt install problem
In-Reply-To: <41EB6B1E.1090809@statistik.uni-dortmund.de>
References: <41EAD801.7000003@math.umass.edu>
	<41EB6B1E.1090809@statistik.uni-dortmund.de>
Message-ID: <41EB9D1E.9090202@optonline.net>

   I am using WinEdt 5.4 (Build: 20040513 (v. 5.4), on XP service pack 
2), and the following approach works for me:

1) Install RWinEdt_1.6-2.zip using RGui menus.

2) Include the following line in the Rprofile file.

library(RWinEdt)

   This will launch WinEdt with R customizations whenever RGui is launched.

hope this helps,

Chuck Cleland

Uwe Ligges wrote:
> Murray Eisenberg wrote:
> 
>> I cannot get the "R" button to appear in RWinEdt.
>>
>> I'm running R 2.0.1 under Windows XP.  I did a clean install of the 
>> latest WinEdt.  Previously (for an earlier installation of WinEdt) I 
>> had RWinEdt running OK.  Now, even though I have the distributed 
>> RWinEdt_1.6-2.zip extracted to the right place in the R directory, and 
>> when I execute
>>
>>   library(RWinEdt)
>>
>> in the R Console, when RWinEdt opens I do not have the R button on the 
>> toolbar.
>>
>> How do I fix this?
>>
>> In case it matters, note that I'm using a WinEdt User configuration 
>> (i.e., a directory that WinEdt refers to as %b) in addition to the 
>> main WinEdt directory (the one that WinEdt refers to as %B).
> 
> 
> If you don't have write access to the WinEdt directory, you have to copy 
> the R.ini file to, e.g., your local directory (%b), use the *manual* 
> installation procedure as described in the ReadMe, but replace the 
> command line args "-e=R.ini" by the full path specification, e.g.: 
> "-E=/path/to/%b/R.ini"
> 
> In a second (apparently private mail), you told that WinEdt "... 5.4 
> builds ... including the most recent one of 14 January 2005..." do not 
> work with the recent version of R-WinEdt, because "1. The first time, 
> immediately I get a message that WinEdt.ini is corrupted and that a 
> default copy (from the RWinEdt distribution I presume) is being used."
> 
> Is this version also configured to use a User configuration?
> 
> I don't have a WinEdt 5.4 version installed (still working with 5.2), 
> but I will certainly take a look during the next one or two weeks.
> 
> Uwe Ligges
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From sam.parvaneh at sh.se  Mon Jan 17 12:11:24 2005
From: sam.parvaneh at sh.se (Sam Parvaneh)
Date: Mon, 17 Jan 2005 12:11:24 +0100
Subject: [R] help wanted using R in a classroom
Message-ID: <OFD10A3DBB.4AA85192-ONC1256F8C.003BD083-C1256F8C.003D7B27@sh.se>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050117/bd45929a/attachment.pl

From marwan.khawaja at aub.edu.lb  Mon Jan 17 01:03:51 2005
From: marwan.khawaja at aub.edu.lb (Marwan Khawaja)
Date: Mon, 17 Jan 2005 02:03:51 +0200
Subject: [R] 3d bar plot
In-Reply-To: <1105958221.29121.16.camel@dutiih.st.ewi.tudelft.nl>
Message-ID: <E1CqUZ9-0004Yn-00@spool.aub.edu.lb>

You can check these packages,
?scatterplot3d
?scatter3d

Best Marwan 

 
-------------------------------------------------------------------
Marwan Khawaja         http://staff.aub.edu.lb/~mk36/
-------------------------------------------------------------------

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of R user
> Sent: Monday, January 17, 2005 12:37 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] 3d bar plot
> 
> This graph -> 
> http://www.math.hope.edu/~tanis/dallas/images/disth36.gif
> is an example I found at
> http://www.math.hope.edu/~tanis/dallas/disth1.html
> created by Maple.
> 
> Does anybody know how to create something similar in R?
> 
> I have a feeling it could be possible using scatterplot3d 
> (perhaps with type=h, the fourth example in 
> help('scatterplot3d')?), but I cannot figure it out.
> 
> Thanks in advance,
> Jonne.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From hagric at sbox.TUGraz.at  Mon Jan 17 13:31:02 2005
From: hagric at sbox.TUGraz.at (hagric)
Date: Mon, 17 Jan 2005 13:31:02 +0100
Subject: [R] bold face labelling/expression
Message-ID: <41EBB006.4040708@sbox.TUGraz.at>

Dear colleagues,

I have a great problem in using "expression" for axes labels. I want the 
labels in bold face (i.e.: par(font.lab=2)). When typing

boxplot(y ~ groups, names = "" , xlab = "", ylab = "", axes = F)
axis(side=1, at=c(1,2), xlab=c(expression(H[2]*O),"others")),

I do not get a bold face labelling. Does anyone know, how I could proceed?
Thank you very much.

Best regards
H.



From vera.hofer at uni-graz.at  Mon Jan 17 13:34:38 2005
From: vera.hofer at uni-graz.at (Vera Hofer)
Date: Mon, 17 Jan 2005 13:34:38 +0100
Subject: [R] bold face labelling / expression
Message-ID: <41EBB0DE.7060603@uni-graz.at>

Dear colleagues,

I have a great problem in using "expression" for axes labels. I want the 
labels in bold face (i.e.: par(font.lab=2)). When typing

boxplot(y ~ groups, names = "" , xlab = "", ylab = "", axes = F)
axis(side=1, at=c(1,2), xlab=c(expression(H[2]*O),"others")),

I do not get a bold face labelling. Does anyone know, how I could proceed?
Thank you very much.

Best regards
V.H.



From ligges at statistik.uni-dortmund.de  Mon Jan 17 14:06:35 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 17 Jan 2005 14:06:35 +0100
Subject: [R] bold face labelling / expression
In-Reply-To: <41EBB0DE.7060603@uni-graz.at>
References: <41EBB0DE.7060603@uni-graz.at>
Message-ID: <41EBB85B.6000001@statistik.uni-dortmund.de>

Vera Hofer wrote:

> Dear colleagues,
> 
> I have a great problem in using "expression" for axes labels. I want the 
> labels in bold face (i.e.: par(font.lab=2)). When typing
> 
> boxplot(y ~ groups, names = "" , xlab = "", ylab = "", axes = F)
> axis(side=1, at=c(1,2), xlab=c(expression(H[2]*O),"others")),

1. If you don't mean "labels" instead of "xlab" in the axis() call, use 
xlab in the boxplot() call or in a call to title() (but then, you cannot 
use c(expression(), ...).

2. See ?plotmath and use, e.g., bold(H[2]*O) ...

Uwe Ligges


> I do not get a bold face labelling. Does anyone know, how I could proceed?
> Thank you very much.
> 
> Best regards
> V.H.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From nielssteenkrogh at hotmail.com  Mon Jan 17 14:22:56 2005
From: nielssteenkrogh at hotmail.com (Niels Steen Krogh)
Date: Mon, 17 Jan 2005 14:22:56 +0100
Subject: [R] Multiple plots in one screen
Message-ID: <BAY24-F2366F0668D859127A62977A28E0@phx.gbl>

see ?par
look for par(new=T)

(if you want plot-overlay)

or just
?lines
(if you want several lines in one plot)



From luk111111 at yahoo.com  Mon Jan 17 14:26:37 2005
From: luk111111 at yahoo.com (luk)
Date: Mon, 17 Jan 2005 05:26:37 -0800 (PST)
Subject: [R] randomForest: too many element specified?
In-Reply-To: <41EBB85B.6000001@statistik.uni-dortmund.de>
Message-ID: <20050117132637.53208.qmail@web30903.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050117/e673a8fc/attachment.pl

From slist at oomvanlieshout.net  Mon Jan 17 14:30:05 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Mon, 17 Jan 2005 15:30:05 +0200
Subject: [R] Time line plot in R?
Message-ID: <41EBBDDD.3040207@oomvanlieshout.net>

Dear R users,

In order to illustrate the possible effects of events on variables 
plotted against time, I would like plot a time line of events along side 
the plot of the variables.

The x-axis should be some time unit; the y-axis should be the variable 
of interest; the time line should be plotted below the graph along the 
same x-axis scale.

As I have many variables and different events lists, I would like to 
write a script to read the events from a file and plot them together 
with the other plot.

The time line should look something like this:
http://www.oslis.k12.or.us/secondary/howto/organize/images/timeline.gif

The rest of the graph can be generated as in the example below.

Any suggestions on how to plot the events along a time line?

Thanks in advance,

Sander Oom.

PS: Would it be an idea to start a R gallery of graphs with sample code? 
I came across the 'R graph gallery' compilation by Lepoutre, but I'm 
sure the R community can come up with many more graph examples.



******************************
library(chron)

date<-c("2002/08/01","2002/08/02","2002/08/03","2002/08/04","2002/08/05","2002/08/06","2002/08/07",
"2002/08/08","2002/08/09","2002/08/10")
time<-c("6:15:00","7:15:00","8:15:00","9:15:00","10:15:00","11:15:00","12:15:00","13:15:00",
"14:15:00","15:15:00")
temp<-rnorm(10)+seq(1,1.9,by=0.1)
co2<-rnorm(10)+seq(1,1.9,by=0.1)
df <-data.frame(date, time, temp, co2)

dateEvents<-c("2002/08/01","2002/08/02","2002/08/03","2002/08/07","2002/08/08","2002/08/09")
descEvents<-c("disaster1", "change1", "disaster2", "disaster3", 
"change2", "change3")
dfEvents<-data.frame(dateEvents, descEvents)
dfEvents

df$datetime <- chron(dates=as.character(df$date), 
times=df$time,format=c('y/m/d', 'h:m:s'))

plot(temp ~ datetime, data=df, type="l")



From andy_liaw at merck.com  Mon Jan 17 14:28:47 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 17 Jan 2005 08:28:47 -0500
Subject: [R] bold face labelling/expression
Message-ID: <3A822319EB35174CA3714066D590DCD50994E53B@usrymx25.merck.com>



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of hagric
> Sent: Monday, January 17, 2005 7:31 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] bold face labelling/expression
> 
> 
> Dear colleagues,
> 
> I have a great problem in using "expression" for axes labels. 
> I want the 
> labels in bold face (i.e.: par(font.lab=2)). When typing
> 
> boxplot(y ~ groups, names = "" , xlab = "", ylab = "", axes = F)
> axis(side=1, at=c(1,2), xlab=c(expression(H[2]*O),"others")),
> 
> I do not get a bold face labelling. Does anyone know, how I 
> could proceed?
> Thank you very much.

My guess is you meant font.axis, rather than font.lab.  However, it seems
like the group labels are not drawn by axis(), so that won't work either.
Uwe's suggestion should work:

> y <- rnorm(40, mean=10)
> groups <- factor(rep(c("H20", "Others"), each=20))
> boxplot(y ~ groups, names=c(expression(bold(H[2]*O)),
expression(bold("Others"))))

Andy 

 
> Best regards
> H.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From fisher at plessthan.com  Mon Jan 17 14:37:59 2005
From: fisher at plessthan.com (Dennis Fisher)
Date: Mon, 17 Jan 2005 05:37:59 -0800
Subject: [R] Question about time series
Message-ID: <00E46D62-688D-11D9-9805-0011242E1C5E@plessthan.com>

I have data in the following format:

 > DATE
[1] "01/13/2004"

In order to find the difference between two data points, I presently 
use brute force to calculate the day of the year:

 > strptime(DATE, format="%m/%d/%Y")$yday
[1] 12

Although this works, it may not be robust over different years.  I 
assume that R is sufficiently clever that a much simpler approach 
exists, e.g., can I calculate the number of days since some fixed time? 
  Unfortunately, the man pages and FAQ did not lead me to an obvious 
solution.

Any help appreciated.

Dennis

Dennis Fisher MD
P < (The "P Less Than" Company)
Phone: 1-866-PLessThan (1-866-753-7784)
Fax: 1-415-564-2220
www.PLessThan.com



From andy_liaw at merck.com  Mon Jan 17 14:48:01 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 17 Jan 2005 08:48:01 -0500
Subject: [R] randomForest: too many element specified?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E53C@usrymx25.merck.com>

> From: luk
> 
> When I run randonForest with a 169453x5 matrix, I got the 
> following message. 
> 
> Error in matrix(0, n, n) : matrix: too many elements specified
> 
> Can you please advise me how to solve this problem?
>  
> Thanks,
>  
> Lu

1.  When asking new questions, please don't reply to other posts.

2.  When asking questions like these, please do show the commands you used.

My guess is that you asked for the proximity matrix, or is running
unsupervised randomForest (by not providing a response vector).  This will
requires a couple of n by n matrices to be created (on top of other things),
n being 169453 in this case.  To store a 169453 x 169453 matrix in double
precision, you need 169453^2 * 8 bytes, or or nearly 214 GB of memory.  Even
if you have that kind of hardware, I doubt you'll be able to make much sense
out of the result.

Andy

 
> Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Vera Hofer wrote:
> 
> > Dear colleagues,
> > 
> > I have a great problem in using "expression" for axes 
> labels. I want the 
> > labels in bold face (i.e.: par(font.lab=2)). When typing
> > 
> > boxplot(y ~ groups, names = "" , xlab = "", ylab = "", axes = F)
> > axis(side=1, at=c(1,2), xlab=c(expression(H[2]*O),"others")),
> 
> 1. If you don't mean "labels" instead of "xlab" in the axis() 
> call, use 
> xlab in the boxplot() call or in a call to title() (but then, 
> you cannot 
> use c(expression(), ...).
> 
> 2. See ?plotmath and use, e.g., bold(H[2]*O) ...
> 
> Uwe Ligges
> 
> 
> > I do not get a bold face labelling. Does anyone know, how I 
> could proceed?
> > Thank you very much.
> > 
> > Best regards
> > V.H.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 		
> 
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jfox at mcmaster.ca  Mon Jan 17 14:49:06 2005
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 17 Jan 2005 08:49:06 -0500
Subject: [R] 3d bar plot
In-Reply-To: <E1CqUZ9-0004Yn-00@spool.aub.edu.lb>
Message-ID: <20050117134904.VYNK25979.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Marwan and Jonne,

I don't think that there's a scatter3d package, so perhaps Marwan is
referring to the scatter3d() function in the Rcmdr package. If so, that
function won't make the kind of 3D graph that Jonne wants -- though the rgl
package, on which scatter3d() is based, should be able to create the graph. 

I don't believe that the scatterplot3d() function in the scatterplot3d
package can make the plot either, but I may be wrong.

I hope this helps.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marwan Khawaja
> Sent: Sunday, January 16, 2005 7:04 PM
> To: 'R user'; r-help at stat.math.ethz.ch
> Subject: RE: [R] 3d bar plot
> 
> You can check these packages,
> ?scatterplot3d
> ?scatter3d
> 
> Best Marwan 
> 
>  
> -------------------------------------------------------------------
> Marwan Khawaja         http://staff.aub.edu.lb/~mk36/
> -------------------------------------------------------------------
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of R user
> > Sent: Monday, January 17, 2005 12:37 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] 3d bar plot
> > 
> > This graph ->
> > http://www.math.hope.edu/~tanis/dallas/images/disth36.gif
> > is an example I found at
> > http://www.math.hope.edu/~tanis/dallas/disth1.html
> > created by Maple.
> > 
> > Does anybody know how to create something similar in R?
> > 
> > I have a feeling it could be possible using scatterplot3d (perhaps 
> > with type=h, the fourth example in help('scatterplot3d')?), but I 
> > cannot figure it out.
> > 
> > Thanks in advance,
> > Jonne.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From r.ghezzo at staff.mcgill.ca  Mon Jan 17 15:03:15 2005
From: r.ghezzo at staff.mcgill.ca (r.ghezzo@staff.mcgill.ca)
Date: Mon, 17 Jan 2005 09:03:15 -0500
Subject: [R] find source code
Message-ID: <1105970595.41ebc5a36a070@webmail.mcgill.ca>

I am using R 2.0.2 on a WinXP
I am trying to get the code of the Kruskal-Wallis test but
> kruskal.test
function (x, ...)
UseMethod("kruskal.test")
<environment: namespace:stats>

> ls(3)
  [1] "acf"                  "acf2AR"               "add.scope"
..............
[181] "kruskal.test"         "ks.test"              "ksmooth"
.......................
[475] "window<-"             "write.ftable"         "xtabs"

> class(kruskal.test)
[1] "function"

> getS3method("kruskal.test","function")
Error in getS3method("kruskal.test", "function") :
        S3 method kruskal.test.function not found

> getS3method("stats::kruskal.test","function")
Error in getS3method("stats::kruskal.test", "function") :
        no function 'stats::kruskal.test' could be found
>
I searched the archives and the answer was ' use getS3method ' . The help for
getS3method is getS3method(f,class,optional=FALSE) so I am lost
Can somebody tell me how to get the source listing of kruskal.test or of any
other hidden function?
Thanks
Heberto Ghezzo
Meakins-Christie Labs
Canada



From p.dalgaard at biostat.ku.dk  Mon Jan 17 15:06:37 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Jan 2005 15:06:37 +0100
Subject: [R] Question about time series
In-Reply-To: <00E46D62-688D-11D9-9805-0011242E1C5E@plessthan.com>
References: <00E46D62-688D-11D9-9805-0011242E1C5E@plessthan.com>
Message-ID: <x2hdlgqhea.fsf@biostat.ku.dk>

Dennis Fisher <fisher at plessthan.com> writes:

> I have data in the following format:
> 
>  > DATE
> [1] "01/13/2004"
> 
> In order to find the difference between two data points, I presently
> use brute force to calculate the day of the year:
> 
>  > strptime(DATE, format="%m/%d/%Y")$yday
> [1] 12
> 
> Although this works, it may not be robust over different years.  I
> assume that R is sufficiently clever that a much simpler approach
> exists, e.g., can I calculate the number of days since some fixed
> time? Unfortunately, the man pages and FAQ did not lead me to an
> obvious solution.
> 
> Any help appreciated.


> str(as.Date("2004-01-13"))
Class 'Date'  num 12430
> as.Date("2004-01-13") - as.Date("1970-01-01")
Time difference of 12430 days

(Using ISO standard format, others require an explicit format= argument)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From simon at stats.gla.ac.uk  Mon Jan 17 15:11:08 2005
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Mon, 17 Jan 2005 14:11:08 +0000 (GMT)
Subject: [R] find source code
In-Reply-To: <1105970595.41ebc5a36a070@webmail.mcgill.ca>
References: <1105970595.41ebc5a36a070@webmail.mcgill.ca>
Message-ID: <Pine.LNX.4.58.0501171410560.5547@moon.stats.gla.ac.uk>

stats:::kruskal.test.default

On Mon, 17 Jan 2005 r.ghezzo at staff.mcgill.ca wrote:

> I am using R 2.0.2 on a WinXP
> I am trying to get the code of the Kruskal-Wallis test but
> > kruskal.test
> function (x, ...)
> UseMethod("kruskal.test")
> <environment: namespace:stats>
> 
> > ls(3)
>   [1] "acf"                  "acf2AR"               "add.scope"
> ..............
> [181] "kruskal.test"         "ks.test"              "ksmooth"
> .......................
> [475] "window<-"             "write.ftable"         "xtabs"
> 
> > class(kruskal.test)
> [1] "function"
> 
> > getS3method("kruskal.test","function")
> Error in getS3method("kruskal.test", "function") :
>         S3 method kruskal.test.function not found
> 
> > getS3method("stats::kruskal.test","function")
> Error in getS3method("stats::kruskal.test", "function") :
>         no function 'stats::kruskal.test' could be found
> >
> I searched the archives and the answer was ' use getS3method ' . The help for
> getS3method is getS3method(f,class,optional=FALSE) so I am lost
> Can somebody tell me how to get the source listing of kruskal.test or of any
> other hidden function?
> Thanks
> Heberto Ghezzo
> Meakins-Christie Labs
> Canada
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Mon Jan 17 15:15:08 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 17 Jan 2005 09:15:08 -0500
Subject: [R] find source code
Message-ID: <3A822319EB35174CA3714066D590DCD50994E53D@usrymx25.merck.com>

> From: r.ghezzo at staff.mcgill.ca
> 
> I am using R 2.0.2 on a WinXP

Does that version of R exist?

> I am trying to get the code of the Kruskal-Wallis test but
> > kruskal.test
> function (x, ...)
> UseMethod("kruskal.test")
> <environment: namespace:stats>
> 
> > ls(3)
>   [1] "acf"                  "acf2AR"               "add.scope"
> ..............
> [181] "kruskal.test"         "ks.test"              "ksmooth"
> .......................
> [475] "window<-"             "write.ftable"         "xtabs"
> 
> > class(kruskal.test)
> [1] "function"
> 
> > getS3method("kruskal.test","function")
> Error in getS3method("kruskal.test", "function") :
>         S3 method kruskal.test.function not found
> 
> > getS3method("stats::kruskal.test","function")
> Error in getS3method("stats::kruskal.test", "function") :
>         no function 'stats::kruskal.test' could be found
> >
> I searched the archives and the answer was ' use getS3method 
> ' . The help for
> getS3method is getS3method(f,class,optional=FALSE) so I am lost
> Can somebody tell me how to get the source listing of 
> kruskal.test or of any
> other hidden function?

Before you use getS3method(), you need to know which S3 method you want to
get.  To see what S3 methods are available, use:

> methods("kruskal.test")
[1] kruskal.test.default* kruskal.test.formula*

    Non-visible functions are asterisked

Andy

> Thanks
> Heberto Ghezzo
> Meakins-Christie Labs
> Canada
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From abitbol at sent.com  Mon Jan 17 15:20:22 2005
From: abitbol at sent.com (Jean-Louis Abitbol)
Date: Mon, 17 Jan 2005 15:20:22 +0100
Subject: [R] [basic ?] Merging result of by processing with a data frame
Message-ID: <1105971622.17536.212913612@webmail.messagingengine.com>

Dear All,

I would like to merge a data frame such as:

> basetab
   subject dose  cmax
1     1031   50  21.8
....

with the result of a by processing such as:

> tmax<-by(pkga,subject,f.tmax)
> tmax
subject: 1031
[1] 6
--------------------------------------------------

f.tmax being a function written by H. Nguyen (in applied statistics in
the pharmaceutical industry). pkga is a data frame with subject conc
time variates.

The aim is to have a single data frame with subject dose cmax tmax etc
(AUCs) ... so that I can do summary stats and plots by dose.


I have tried unlist without success.

Thanks for any help.

Kind regards, Jean-Louis



From ozric at web.de  Mon Jan 17 15:30:35 2005
From: ozric at web.de (Christian Schulz)
Date: Mon, 17 Jan 2005 15:30:35 +0100
Subject: [R] Question about time series
In-Reply-To: <00E46D62-688D-11D9-9805-0011242E1C5E@plessthan.com>
References: <00E46D62-688D-11D9-9805-0011242E1C5E@plessthan.com>
Message-ID: <41EBCC0B.3040509@web.de>

Hi

 >>TODAY           <- as.POSIXlt('2004-01-17')
 >>LYTODAY  <- as.POSIXlt('2003-01-17')
 >>DAYS <- TODAY-LYTODAY
 >>DAYS
Time difference of 365 days
 >>DAYS[[1]]
[1] 365

perhaps it helps,
Christian


Dennis Fisher wrote:

> I have data in the following format:
>
> > DATE
> [1] "01/13/2004"
>
> In order to find the difference between two data points, I presently 
> use brute force to calculate the day of the year:
>
> > strptime(DATE, format="%m/%d/%Y")$yday
> [1] 12
>
> Although this works, it may not be robust over different years.  I 
> assume that R is sufficiently clever that a much simpler approach 
> exists, e.g., can I calculate the number of days since some fixed 
> time?  Unfortunately, the man pages and FAQ did not lead me to an 
> obvious solution.
>
> Any help appreciated.
>
> Dennis
>
> Dennis Fisher MD
> P < (The "P Less Than" Company)
> Phone: 1-866-PLessThan (1-866-753-7784)
> Fax: 1-415-564-2220
> www.PLessThan.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Mon Jan 17 15:32:40 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 17 Jan 2005 15:32:40 +0100
Subject: [R] find source code
In-Reply-To: <Pine.LNX.4.58.0501171410560.5547@moon.stats.gla.ac.uk>
References: <1105970595.41ebc5a36a070@webmail.mcgill.ca>
	<Pine.LNX.4.58.0501171410560.5547@moon.stats.gla.ac.uk>
Message-ID: <41EBCC88.50203@statistik.uni-dortmund.de>

Simon Wood wrote:

> stats:::kruskal.test.default


and how to get there:

methods(kruskal.test) # note, you probably want the "default" method!
getS3method("kruskal.test", "default")


Uwe


> On Mon, 17 Jan 2005 r.ghezzo at staff.mcgill.ca wrote:
> 
> 
>>I am using R 2.0.2 on a WinXP
>>I am trying to get the code of the Kruskal-Wallis test but
>>
>>>kruskal.test
>>
>>function (x, ...)
>>UseMethod("kruskal.test")
>><environment: namespace:stats>
>>
>>>ls(3)
>>
>>  [1] "acf"                  "acf2AR"               "add.scope"
>>..............
>>[181] "kruskal.test"         "ks.test"              "ksmooth"
>>.......................
>>[475] "window<-"             "write.ftable"         "xtabs"
>>
>>
>>>class(kruskal.test)
>>
>>[1] "function"
>>
>>
>>>getS3method("kruskal.test","function")
>>
>>Error in getS3method("kruskal.test", "function") :
>>        S3 method kruskal.test.function not found
>>
>>
>>>getS3method("stats::kruskal.test","function")
>>
>>Error in getS3method("stats::kruskal.test", "function") :
>>        no function 'stats::kruskal.test' could be found
>>
>>I searched the archives and the answer was ' use getS3method ' . The help for
>>getS3method is getS3method(f,class,optional=FALSE) so I am lost
>>Can somebody tell me how to get the source listing of kruskal.test or of any
>>other hidden function?
>>Thanks
>>Heberto Ghezzo
>>Meakins-Christie Labs
>>Canada
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Mon Jan 17 15:34:56 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 17 Jan 2005 15:34:56 +0100
Subject: [R] 3d bar plot
In-Reply-To: <20050117134904.VYNK25979.tomts25-srv.bellnexxia.net@JohnDesktop8300>
References: <20050117134904.VYNK25979.tomts25-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <41EBCD10.7040502@statistik.uni-dortmund.de>

John Fox wrote:

> Dear Marwan and Jonne,
> 
> I don't think that there's a scatter3d package, so perhaps Marwan is
> referring to the scatter3d() function in the Rcmdr package. If so, that
> function won't make the kind of 3D graph that Jonne wants -- though the rgl
> package, on which scatter3d() is based, should be able to create the graph. 
> 
> I don't believe that the scatterplot3d() function in the scatterplot3d
> package can make the plot either, but I may be wrong.

John, you are right. scatterplot3d is not designed to do stuff like 
that. You can do some workarounds, but it will look rather ugly.

Uwe


> I hope this helps.
> 
> John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marwan Khawaja
>>Sent: Sunday, January 16, 2005 7:04 PM
>>To: 'R user'; r-help at stat.math.ethz.ch
>>Subject: RE: [R] 3d bar plot
>>
>>You can check these packages,
>>?scatterplot3d
>>?scatter3d
>>
>>Best Marwan 
>>
>> 
>>-------------------------------------------------------------------
>>Marwan Khawaja         http://staff.aub.edu.lb/~mk36/
>>-------------------------------------------------------------------
>>
>>
>>>-----Original Message-----
>>>From: r-help-bounces at stat.math.ethz.ch 
>>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of R user
>>>Sent: Monday, January 17, 2005 12:37 PM
>>>To: r-help at stat.math.ethz.ch
>>>Subject: [R] 3d bar plot
>>>
>>>This graph ->
>>>http://www.math.hope.edu/~tanis/dallas/images/disth36.gif
>>>is an example I found at
>>>http://www.math.hope.edu/~tanis/dallas/disth1.html
>>>created by Maple.
>>>
>>>Does anybody know how to create something similar in R?
>>>
>>>I have a feeling it could be possible using scatterplot3d (perhaps 
>>>with type=h, the fourth example in help('scatterplot3d')?), but I 
>>>cannot figure it out.
>>>
>>>Thanks in advance,
>>>Jonne.
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mhyrien at geosys.com  Mon Jan 17 15:33:48 2005
From: mhyrien at geosys.com (HYRIEN Matthieu)
Date: Mon, 17 Jan 2005 15:33:48 +0100
Subject: [R] kmeans with weighted variables
Message-ID: <200501171436.j0HEaat9004939@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050117/e0ad660c/attachment.pl

From bhx2 at mevik.net  Mon Jan 17 15:40:50 2005
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Mon, 17 Jan 2005 15:40:50 +0100
Subject: [R] Setting the width and height of a Sweave figure
In-Reply-To: <cs5e5m$c2l$1@sea.gmane.org> (Henrik Andersson's message of
	"Thu, 13 Jan 2005 10:12:26 +0100")
References: <cs5e5m$c2l$1@sea.gmane.org>
Message-ID: <m0wtuc9kzx.fsf@bar.nemo-project.org>

I haven't found any other solution than using

<<fig=TRUE,height=7,width=14>>
theCode()
@

(but of course that doesn't have any effect when theCode() is used
interactively).

-- 
Bj?rn-Helge Mevik



From andy_liaw at merck.com  Mon Jan 17 15:46:15 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 17 Jan 2005 09:46:15 -0500
Subject: [R] [basic ?] Merging result of by processing with a data fra me
Message-ID: <3A822319EB35174CA3714066D590DCD50994E540@usrymx25.merck.com>

> From: Jean-Louis Abitbol
> 
> Dear All,
> 
> I would like to merge a data frame such as:
> 
> > basetab
>    subject dose  cmax
> 1     1031   50  21.8
> ....
> 
> with the result of a by processing such as:
> 
> > tmax<-by(pkga,subject,f.tmax)
> > tmax
> subject: 1031
> [1] 6
> --------------------------------------------------
> 
> f.tmax being a function written by H. Nguyen (in applied statistics in
> the pharmaceutical industry). pkga is a data frame with subject conc
> time variates.
> 
> The aim is to have a single data frame with subject dose cmax tmax etc
> (AUCs) ... so that I can do summary stats and plots by dose.

I would use tapply() instead of by().  Something like the following might
work (assuming `subject' is factor, as I believe it should be):

tmax <- tapply(pkga, subject, f.tmax)
tmax <- data.frame(subject=factor(names(tmax), tmax=tmax)

Then use merge() to merge basetab with tmax, or if all subjects are present
and in the same order, just cbind() them.

Andy
 
> I have tried unlist without success.
> 
> Thanks for any help.
> 
> Kind regards, Jean-Louis
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From t.muhlhofer at lse.ac.uk  Mon Jan 17 15:57:33 2005
From: t.muhlhofer at lse.ac.uk (Tobias Muhlhofer)
Date: Mon, 17 Jan 2005 14:57:33 +0000
Subject: [R] Omitting constant in ols() from Design
Message-ID: <41EBD25D.90207@lse.ac.uk>

Hi!

I need to run ols regressions with Huber-White sandwich estimators and 
the correponding standard errors, without an intercept. What I'm trying 
to do is create an ols object and then use the robcov() function, on the 
order of:

f <- ols(depvar ~ ind1 + ind2, x=TRUE)
robcov(f)

However, when I go

f <- ols(depvar ~ ind1 + ind2 -1, x=TRUE)

I get the following error:

Error in ols(nareit ~ SnP500 + d3yrtr - 1) :
         length of dimnames [2] not equal to array extent

same with +0 instead of -1.

Is there a different way to create an ols object without a constant? I 
can't use lm(), because robcov() needs an object from the Design() series.

Or is there a different way to go about this?

Tobias Muhlhofer



From Matthias.Templ at statistik.gv.at  Mon Jan 17 15:59:01 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Mon, 17 Jan 2005 15:59:01 +0100
Subject: [R] find source code
Message-ID: <83536658864BC243BE3C06D7E936ABD501BE1B4D@xchg1.statistik.local>

But how can I see the *documented* source code in Windows? I think i.e.
in the /flexmix/library/man/all.rda file is the documented(?) code for
all functions of Package flexmix, but the standard Windows XP cannot
open .rda files correct.

Is there a way to read the documented source code of a function under
Windows and R2.0.1?

Thanks in advance,
Matthias


> Betreff: Re: [R] find source code
> 
> 
> stats:::kruskal.test.default
> 
> On Mon, 17 Jan 2005 r.ghezzo at staff.mcgill.ca wrote:
> 
> > I am using R 2.0.2 on a WinXP
> > I am trying to get the code of the Kruskal-Wallis test but
> > > kruskal.test
> > function (x, ...)
> > UseMethod("kruskal.test")
> > <environment: namespace:stats>
> > 
> > > ls(3)
> >   [1] "acf"                  "acf2AR"               "add.scope"
> > ..............
> > [181] "kruskal.test"         "ks.test"              "ksmooth"
> > .......................
> > [475] "window<-"             "write.ftable"         "xtabs"
> > 
> > > class(kruskal.test)
> > [1] "function"
> > 
> > > getS3method("kruskal.test","function")
> > Error in getS3method("kruskal.test", "function") :
> >         S3 method kruskal.test.function not found
> > 
> > > getS3method("stats::kruskal.test","function")
> > Error in getS3method("stats::kruskal.test", "function") :
> >         no function 'stats::kruskal.test' could be found
> > >
> > I searched the archives and the answer was ' use 
> getS3method ' . The 
> > help for getS3method is getS3method(f,class,optional=FALSE) so I am 
> > lost Can somebody tell me how to get the source listing of 
> > kruskal.test or of any other hidden function? Thanks
> > Heberto Ghezzo
> > Meakins-Christie Labs
> > Canada
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Mon Jan 17 16:32:50 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 17 Jan 2005 10:32:50 -0500
Subject: [R] find source code
Message-ID: <3A822319EB35174CA3714066D590DCD50994E541@usrymx25.merck.com>

> From: TEMPL Matthias
> 
> But how can I see the *documented* source code in Windows? I 
> think i.e.
> in the /flexmix/library/man/all.rda file is the documented(?) code for
> all functions of Package flexmix, but the standard Windows XP cannot
> open .rda files correct.
> 
> Is there a way to read the documented source code of a function under
> Windows and R2.0.1?

Do you mean `commented' rather than `documented' (i.e., having a help page)?
If so, I believe the only guaranteed way of doing that is by getting the
package _source_ (i.e., the .tar.gz file) from CRAN and read the file in the
R/ subdirectory after unpacking.  This has nothing to do with
hardware/software platform.

.rda files are really only readable by R, regardless of platform, and R on
Windows doesn't behave differently than on other OS in this respect.

Andy

 
> Thanks in advance,
> Matthias
> 
> 
> > Betreff: Re: [R] find source code
> > 
> > 
> > stats:::kruskal.test.default
> > 
> > On Mon, 17 Jan 2005 r.ghezzo at staff.mcgill.ca wrote:
> > 
> > > I am using R 2.0.2 on a WinXP
> > > I am trying to get the code of the Kruskal-Wallis test but
> > > > kruskal.test
> > > function (x, ...)
> > > UseMethod("kruskal.test")
> > > <environment: namespace:stats>
> > > 
> > > > ls(3)
> > >   [1] "acf"                  "acf2AR"               "add.scope"
> > > ..............
> > > [181] "kruskal.test"         "ks.test"              "ksmooth"
> > > .......................
> > > [475] "window<-"             "write.ftable"         "xtabs"
> > > 
> > > > class(kruskal.test)
> > > [1] "function"
> > > 
> > > > getS3method("kruskal.test","function")
> > > Error in getS3method("kruskal.test", "function") :
> > >         S3 method kruskal.test.function not found
> > > 
> > > > getS3method("stats::kruskal.test","function")
> > > Error in getS3method("stats::kruskal.test", "function") :
> > >         no function 'stats::kruskal.test' could be found
> > > >
> > > I searched the archives and the answer was ' use 
> > getS3method ' . The 
> > > help for getS3method is 
> getS3method(f,class,optional=FALSE) so I am 
> > > lost Can somebody tell me how to get the source listing of 
> > > kruskal.test or of any other hidden function? Thanks
> > > Heberto Ghezzo
> > > Meakins-Christie Labs
> > > Canada
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read 
> > the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From p.dalgaard at biostat.ku.dk  Mon Jan 17 16:32:58 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Jan 2005 16:32:58 +0100
Subject: [R] find source code
In-Reply-To: <83536658864BC243BE3C06D7E936ABD501BE1B4D@xchg1.statistik.local>
References: <83536658864BC243BE3C06D7E936ABD501BE1B4D@xchg1.statistik.local>
Message-ID: <x27jmcqded.fsf@biostat.ku.dk>

"TEMPL Matthias" <Matthias.Templ at statistik.gv.at> writes:

> But how can I see the *documented* source code in Windows? I think i.e.
> in the /flexmix/library/man/all.rda file is the documented(?) code for
> all functions of Package flexmix, but the standard Windows XP cannot
> open .rda files correct.

No that's not it. The .rda files are like R workspaces.
 
> Is there a way to read the documented source code of a function under
> Windows and R2.0.1?

Yes. Fetch the source code for the package or R itself, and read the
relevant source files. These are available from CRAN.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From fm3a004 at math.uni-hamburg.de  Mon Jan 17 16:49:35 2005
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Mon, 17 Jan 2005 16:49:35 +0100 (MET)
Subject: [R] How do I format something as "0.000"?
Message-ID: <Pine.GSO.3.95q.1050117164410.2245C-100000@sun11.math.uni-hamburg.de>

Hi,

I would like to use the format function to get numbers all with three 
digits to the right of the decimal point, even in cases where there is no
significant digit left. For example, I would like to get
c(0.3456789,0.0000053) as "0.346" "0.000".
It seems that it is not possible to force format to print a "0.000", i.e.
without any significant decimal places.
Is it possible to do this somehow in R?

Thanks,
Christian


***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From Achim.Zeileis at wu-wien.ac.at  Mon Jan 17 16:57:33 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 17 Jan 2005 16:57:33 +0100
Subject: [R] Omitting constant in ols() from Design
In-Reply-To: <41EBD25D.90207@lse.ac.uk>
References: <41EBD25D.90207@lse.ac.uk>
Message-ID: <20050117165733.4c807387.Achim.Zeileis@wu-wien.ac.at>

On Mon, 17 Jan 2005 14:57:33 +0000 Tobias Muhlhofer wrote:

> Hi!
> 
> I need to run ols regressions with Huber-White sandwich estimators and
> the correponding standard errors, without an intercept. What I'm
> trying to do is create an ols object and then use the robcov()
> function, on the order of:
> 
> f <- ols(depvar ~ ind1 + ind2, x=TRUE)
> robcov(f)
> 
> However, when I go
> 
> f <- ols(depvar ~ ind1 + ind2 -1, x=TRUE)
> 
> I get the following error:
> 
> Error in ols(nareit ~ SnP500 + d3yrtr - 1) :
>          length of dimnames [2] not equal to array extent
> 
> same with +0 instead of -1.
> 
> Is there a different way to create an ols object without a constant? I
> can't use lm(), because robcov() needs an object from the Design()
> series.
> 
> Or is there a different way to go about this?

I am not sure if the problem can also be avoided or worked around in
Design, but the functions hccm() from car or vcovHC() from sandwich can
also compute the Huber-White (and other) HC covariance matrix estimates
for fitted "lm" objects.
For computing the corresponding t-tests or z-tests, you can use the
function coeftest() in lmtest. See the corresponding man page for
examples.

Best,
Z

> Tobias Muhlhofer
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Achim.Zeileis at wu-wien.ac.at  Mon Jan 17 17:03:36 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 17 Jan 2005 17:03:36 +0100
Subject: [R] How do I format something as "0.000"?
In-Reply-To: <Pine.GSO.3.95q.1050117164410.2245C-100000@sun11.math.uni-hamburg.de>
References: <Pine.GSO.3.95q.1050117164410.2245C-100000@sun11.math.uni-hamburg.de>
Message-ID: <20050117170336.305f62bb.Achim.Zeileis@wu-wien.ac.at>

On Mon, 17 Jan 2005 16:49:35 +0100 (MET) Christian Hennig wrote:

> Hi,
> 
> I would like to use the format function to get numbers all with three 
> digits to the right of the decimal point, even in cases where there is
> no significant digit left. For example, I would like to get
> c(0.3456789,0.0000053) as "0.346" "0.000".
> It seems that it is not possible to force format to print a "0.000",
> i.e. without any significant decimal places.
> Is it possible to do this somehow in R?

You can round() before:

R> x <- c(0.3456789,0.0000053)
R> round(x, digits = 3)
[1] 0.346 0.000
R> format(round(x, digits = 3))
[1] "0.346" "0.000"

hth,
Z

> Thanks,
> Christian
> 
> 
> *********************************************************************
> ** Christian Hennig
> Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
> hennig at math.uni-hamburg.de,
> http://www.math.uni-hamburg.de/home/hennig/
> #####################################################################
> ###
> ich empfehle www.boag-online.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From f.harrell at vanderbilt.edu  Mon Jan 17 17:04:10 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 17 Jan 2005 11:04:10 -0500
Subject: [R] Omitting constant in ols() from Design
In-Reply-To: <41EBD25D.90207@lse.ac.uk>
References: <41EBD25D.90207@lse.ac.uk>
Message-ID: <41EBE1FA.7090005@vanderbilt.edu>

Tobias Muhlhofer wrote:
> Hi!
> 
> I need to run ols regressions with Huber-White sandwich estimators and 
> the correponding standard errors, without an intercept. What I'm trying 
> to do is create an ols object and then use the robcov() function, on the 
> order of:
> 
> f <- ols(depvar ~ ind1 + ind2, x=TRUE)
> robcov(f)
> 
> However, when I go
> 
> f <- ols(depvar ~ ind1 + ind2 -1, x=TRUE)
> 
> I get the following error:
> 
> Error in ols(nareit ~ SnP500 + d3yrtr - 1) :
>         length of dimnames [2] not equal to array extent
> 
> same with +0 instead of -1.
> 
> Is there a different way to create an ols object without a constant? I 
> can't use lm(), because robcov() needs an object from the Design() series.
> 
> Or is there a different way to go about this?
> 
> Tobias Muhlhofer

ols does not support this.  Sorry.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From avneetaquarian at yahoo.com  Mon Jan 17 17:14:55 2005
From: avneetaquarian at yahoo.com (avneet singh)
Date: Mon, 17 Jan 2005 08:14:55 -0800 (PST)
Subject: [R] merge data frames taking mean/mode of multiple macthes
Message-ID: <20050117161455.75234.qmail@web14926.mail.yahoo.com>

Hello :)

I have two data frames, one has properties taken on a
piece by piece basis and the other has performance on
a lot by lot basis. I wish to combine these two data
frames but the problem is that each lot has multiple
pieces and hence i need to take a mean of the
properties of multiple pieces and match it to the row
having data about the lot.

I was wondering if there is a simple commmand, an
extension of "merge", or an option of merge i do not
know which could easily do this work.

Thank you  :)

=====
I believe in equality for everyone, except reporters and photographers.
~Mahatma Gandhi



From murray at math.umass.edu  Mon Jan 17 17:41:44 2005
From: murray at math.umass.edu (Murray Eisenberg)
Date: Mon, 17 Jan 2005 11:41:44 -0500
Subject: [R] RWinEdt install problem
In-Reply-To: <41EB6B1E.1090809@statistik.uni-dortmund.de>
References: <41EAD801.7000003@math.umass.edu>
	<41EB6B1E.1090809@statistik.uni-dortmund.de>
Message-ID: <41EBEAC8.2030908@math.umass.edu>

The problem with RWinEdt can be resolved by NOT using User Profiles in 
WinEdt:

I did a clean re-install of WinEdt 5.4, build 20050114.  Then, executing 
"library(RWinEdt)" in the RGui did start up a properly configured 
R-WinEdt session of WinEdt.  No error messages; the expected R buttons 
were now on the R-WinEdt menu bar.

However, it would desirable if RWinEdt were revised so as to be 
compatible with WinEdt User Profiles.  The reason is that WinEdt 
encourages use of User Profiles so that, when one upgrades WinEdt 
itself, the settings are not lost.  (And, in general, the idea of 
separating a program such as WinEdt from user settings for it is a good 
one.)

Uwe Ligges wrote:
> Murray Eisenberg wrote:
> 
>> I cannot get the "R" button to appear in RWinEdt.
>>
>> I'm running R 2.0.1 under Windows XP.  I did a clean install of the 
>> latest WinEdt.  Previously (for an earlier installation of WinEdt) I 
>> had RWinEdt running OK.  Now, even though I have the distributed 
>> RWinEdt_1.6-2.zip extracted to the right place in the R directory, and 
>> when I execute
>>
>>   library(RWinEdt)
>>
>> in the R Console, when RWinEdt opens I do not have the R button on the 
>> toolbar.
>>
>> How do I fix this?
>>
>> In case it matters, note that I'm using a WinEdt User configuration 
>> (i.e., a directory that WinEdt refers to as %b) in addition to the 
>> main WinEdt directory (the one that WinEdt refers to as %B).
> 
> 
> If you don't have write access to the WinEdt directory, you have to copy 
> the R.ini file to, e.g., your local directory (%b), use the *manual* 
> installation procedure as described in the ReadMe, but replace the 
> command line args "-e=R.ini" by the full path specification, e.g.: 
> "-E=/path/to/%b/R.ini"
> 
> In a second (apparently private mail), you told that WinEdt "... 5.4 
> builds ... including the most recent one of 14 January 2005..." do not 
> work with the recent version of R-WinEdt, because "1. The first time, 
> immediately I get a message that WinEdt.ini is corrupted and that a 
> default copy (from the RWinEdt distribution I presume) is being used."
> 
> Is this version also configured to use a User configuration?
> 
> I don't have a WinEdt 5.4 version installed (still working with 5.2), 
> but I will certainly take a look during the next one or two weeks.
> 
> Uwe Ligges
> 

-- 
Murray Eisenberg                     murray at math.umass.edu
Mathematics & Statistics Dept.
Lederle Graduate Research Tower      phone 413 549-1020 (H)
University of Massachusetts                413 545-2859 (W)
710 North Pleasant Street            fax   413 545-1801
Amherst, MA 01003-9305



From Matthias.Templ at statistik.gv.at  Mon Jan 17 17:47:57 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Mon, 17 Jan 2005 17:47:57 +0100
Subject: [R] find source code
Message-ID: <83536658864BC243BE3C06D7E936ABD501BE1B51@xchg1.statistik.local>


> -----Urspr?ngliche Nachricht-----
> Von: Liaw, Andy [mailto:andy_liaw at merck.com] 
> Gesendet: Montag, 17. J?nner 2005 16:33
> An: TEMPL Matthias; r-help at stat.math.ethz.ch
> Betreff: RE: [R] find source code
> 
> 
> > From: TEMPL Matthias
> > 
> > But how can I see the *documented* source code in Windows? I
> > think i.e.
> > in the /flexmix/library/man/all.rda file is the 
> documented(?) code for
> > all functions of Package flexmix, but the standard Windows XP cannot
> > open .rda files correct.
> > 
> > Is there a way to read the documented source code of a 
> function under 
> > Windows and R2.0.1?
> 
> Do you mean `commented' rather than `documented' (i.e., 
> having a help page)? If so, I believe the only guaranteed way 
> of doing that is by getting the package _source_ (i.e., the 
> .tar.gz file) from CRAN and read the file in the R/ 
> subdirectory after unpacking.  This has nothing to do with 
> hardware/software platform.
> 

Sorry for my poor english and thank you for your quick replies! 
I have meant the `commented' source code.
Now I know that the commented source code is findable in the (unpacked) ..tar.gz file and not in the .zip file.

Thank you very much,
Matthias

>
> .rda files are really only readable by R, regardless of 
> platform, and R on Windows doesn't behave differently than on 
> other OS in this respect.
> 
> Andy
> 
>  
> > Thanks in advance,
> > Matthias
> > 
> > 
> > > Betreff: Re: [R] find source code
> > > 
> > > 
> > > stats:::kruskal.test.default
> > > 
> > > On Mon, 17 Jan 2005 r.ghezzo at staff.mcgill.ca wrote:
> > > 
> > > > I am using R 2.0.2 on a WinXP
> > > > I am trying to get the code of the Kruskal-Wallis test but
> > > > > kruskal.test
> > > > function (x, ...)
> > > > UseMethod("kruskal.test")
> > > > <environment: namespace:stats>
> > > > 
> > > > > ls(3)
> > > >   [1] "acf"                  "acf2AR"               "add.scope"
> > > > ..............
> > > > [181] "kruskal.test"         "ks.test"              "ksmooth"
> > > > .......................
> > > > [475] "window<-"             "write.ftable"         "xtabs"
> > > > 
> > > > > class(kruskal.test)
> > > > [1] "function"
> > > > 
> > > > > getS3method("kruskal.test","function")
> > > > Error in getS3method("kruskal.test", "function") :
> > > >         S3 method kruskal.test.function not found
> > > > 
> > > > > getS3method("stats::kruskal.test","function")
> > > > Error in getS3method("stats::kruskal.test", "function") :
> > > >         no function 'stats::kruskal.test' could be found
> > > > >
> > > > I searched the archives and the answer was ' use
> > > getS3method ' . The
> > > > help for getS3method is
> > getS3method(f,class,optional=FALSE) so I am
> > > > lost Can somebody tell me how to get the source listing of
> > > > kruskal.test or of any other hidden function? Thanks
> > > > Heberto Ghezzo
> > > > Meakins-Christie Labs
> > > > Canada
> > > > 
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide! 
> > > > http://www.R-project.org/posting-guide.html
> > > >
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read 
> > > the posting guide! http://www.R-project.org/posting-guide.html
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any attachments, 
> contains information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New Jersey, USA 08889), and/or its 
> affiliates (which may be known outside the United States as 
> Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
> Banyu) that may be confidential, proprietary copyrighted 
> and/or legally privileged. It is intended solely for the use 
> of the individual or entity named on this message.  If you 
> are not the intended recipient, and have received this 
> message in error, please notify us immediately by reply 
> e-mail and then delete it from your system.
> --------------------------------------------------------------
> ----------------
>



From buser at stat.math.ethz.ch  Mon Jan 17 18:19:41 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Mon, 17 Jan 2005 18:19:41 +0100
Subject: [R] merge data frames taking mean/mode of multiple macthes
In-Reply-To: <20050117161455.75234.qmail@web14926.mail.yahoo.com>
References: <20050117161455.75234.qmail@web14926.mail.yahoo.com>
Message-ID: <16875.62381.819984.48422@stat.math.ethz.ch>

Dear Avneet

the function aggregate (see also ?aggregate) could be useful for
your problem. See the short example I've written below.

dat1 <- data.frame(lot = factor(1:10),y1 = rnorm(10))
str(dat1)
dat2 <- data.frame(nr = 1:100, lot = factor(rep(1:10, each = 10)),y2 = rnorm(100))
str(dat2)

dat2.agr <- aggregate(dat2$y, by = list(lot = dat2$lot), FUN = mean)
names(dat2.agr)[2] <- "y2"

dat.mer <- merge(dat1, dat2.agr)
str(dat.mer)

Be careful about merging dataframes. There should always be a
control that the right cases are merged together.

Regards,

Christoph Buser

-- 
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C11
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-1-632-5414		fax: 632-1228
http://stat.ethz.ch/~buser/


avneet singh writes:
 > Hello :)
 > 
 > I have two data frames, one has properties taken on a
 > piece by piece basis and the other has performance on
 > a lot by lot basis. I wish to combine these two data
 > frames but the problem is that each lot has multiple
 > pieces and hence i need to take a mean of the
 > properties of multiple pieces and match it to the row
 > having data about the lot.
 > 
 > I was wondering if there is a simple commmand, an
 > extension of "merge", or an option of merge i do not
 > know which could easily do this work.
 > 
 > Thank you  :)
 > 
 > =====
 > I believe in equality for everyone, except reporters and photographers.
 > ~Mahatma Gandhi
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C11
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-1-632-5414		fax: 632-1228
http://stat.ethz.ch/~buser/



From murdoch at stats.uwo.ca  Mon Jan 17 19:05:09 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 17 Jan 2005 18:05:09 +0000
Subject: [R] 3d bar plot
In-Reply-To: <20050117134904.VYNK25979.tomts25-srv.bellnexxia.net@JohnDesktop8300>
References: <E1CqUZ9-0004Yn-00@spool.aub.edu.lb>
	<20050117134904.VYNK25979.tomts25-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <3fvnu0lik01fjhpn0hc3l0pptssjlsl84d@4ax.com>

On Mon, 17 Jan 2005 08:49:06 -0500, "John Fox" <jfox at mcmaster.ca>
wrote :

>Dear Marwan and Jonne,
>
>I don't think that there's a scatter3d package, so perhaps Marwan is
>referring to the scatter3d() function in the Rcmdr package. If so, that
>function won't make the kind of 3D graph that Jonne wants -- though the rgl
>package, on which scatter3d() is based, should be able to create the graph. 
>
>I don't believe that the scatterplot3d() function in the scatterplot3d
>package can make the plot either, but I may be wrong.

I had a function in my (Windows-only) djmrgl package to do plots like
that; one of the things on my to-do list is to port it to rgl.  This
may happen in about a month.

Duncan Murdoch



From friendly at yorku.ca  Mon Jan 17 19:39:23 2005
From: friendly at yorku.ca (Michael Friendly)
Date: Mon, 17 Jan 2005 13:39:23 -0500
Subject: [R] debian /etc/apt/sources.list for CRAN?
Message-ID: <41EC065B.8000303@yorku.ca>

When I set up my debian linux (unstable) system, the only R debian 
packages were on CRAN, so I
followed directions and added the last line to my /etc/apt/sources.list

euclid: # m /etc/apt/sources.list

deb http://debian.yorku.ca/debian/ unstable main non-free contrib
deb http://debian.yorku.ca/debian/non-US unstable/non-US main contrib 
non-free
deb http://hexamon.ccs.yorku.ca/ unixteam-debs/
deb http://cran.r-project.org/bin/linux/debian woody main

But now, when I try to update the local package cache I get errors (below).
Should I just delete the cran.r-project.org line
above, or are there debian R packages elsewhere on CRAN that I should 
include instead?

 # apt-get update
Hit http://debian.yorku.ca unstable/main Packages
Hit http://debian.yorku.ca unstable/main Release
Hit http://debian.yorku.ca unstable/non-free Packages
Hit http://debian.yorku.ca unstable/non-free Release
Hit http://debian.yorku.ca unstable/contrib Packages
Hit http://debian.yorku.ca unstable/contrib Release
Hit http://debian.yorku.ca unstable/non-US/main Packages
Hit http://debian.yorku.ca unstable/non-US/main Release
Hit http://debian.yorku.ca unstable/non-US/contrib Packages
Hit http://debian.yorku.ca unstable/non-US/contrib Release
Hit http://debian.yorku.ca unstable/non-US/non-free Packages
Hit http://debian.yorku.ca unstable/non-US/non-free Release
Hit http://hexamon.ccs.yorku.ca unixteam-debs/ Packages
Ign http://hexamon.ccs.yorku.ca unixteam-debs/ Release
Err http://cran.r-project.org woody/main Packages
  404 Not Found
Ign http://cran.r-project.org woody/main Release
Failed to fetch 
http://cran.r-project.org/bin/linux/debian/dists/woody/main/binary-i386/Packages.gz  
404 Not Found
Reading Package Lists... Done
E: Some index files failed to download, they have been ignored, or old 
ones used instead.

-- 
Michael Friendly     Email: friendly at yorku.ca 
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From fm3a004 at math.uni-hamburg.de  Mon Jan 17 19:59:32 2005
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Mon, 17 Jan 2005 19:59:32 +0100 (MET)
Subject: [R] Skewness test
Message-ID: <Pine.GSO.3.95q.1050117195751.2245E-100000@sun11.math.uni-hamburg.de>

Hi,

is there a test for the H0 skewness=0 (or with skewness as test
statistic and normality as H0) implemented in R?

Thank you,
Christian


***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From r.ghezzo at staff.mcgill.ca  Mon Jan 17 20:18:20 2005
From: r.ghezzo at staff.mcgill.ca (r.ghezzo@staff.mcgill.ca)
Date: Mon, 17 Jan 2005 14:18:20 -0500
Subject: [R] find source code
In-Reply-To: <41EBCC88.50203@statistik.uni-dortmund.de>
References: <1105970595.41ebc5a36a070@webmail.mcgill.ca>
	<Pine.LNX.4.58.0501171410560.5547@moon.stats.gla.ac.uk>
	<41EBCC88.50203@statistik.uni-dortmund.de>
Message-ID: <1105989500.41ec0f7cbf283@webmail.mcgill.ca>

Thanks to all who answered my query, I forgot completely to call methods() first
to check the true whole name of the function.
Heberto Ghezzo

Quoting Uwe Ligges <ligges at statistik.uni-dortmund.de>:

> Simon Wood wrote:
>
> > stats:::kruskal.test.default
>
>
> and how to get there:
>
> methods(kruskal.test) # note, you probably want the "default" method!
> getS3method("kruskal.test", "default")
>
>
> Uwe
>
>
> > On Mon, 17 Jan 2005 r.ghezzo at staff.mcgill.ca wrote:
> >
> >
> >>I am using R 2.0.2 on a WinXP
> >>I am trying to get the code of the Kruskal-Wallis test but
> >>
> >>>kruskal.test
> >>
> >>function (x, ...)
> >>UseMethod("kruskal.test")
> >><environment: namespace:stats>
> >>
> >>>ls(3)
> >>
> >>  [1] "acf"                  "acf2AR"               "add.scope"
> >>..............
> >>[181] "kruskal.test"         "ks.test"              "ksmooth"
> >>.......................
> >>[475] "window<-"             "write.ftable"         "xtabs"
> >>
> >>
> >>>class(kruskal.test)
> >>
> >>[1] "function"
> >>
> >>
> >>>getS3method("kruskal.test","function")
> >>
> >>Error in getS3method("kruskal.test", "function") :
> >>        S3 method kruskal.test.function not found
> >>
> >>
> >>>getS3method("stats::kruskal.test","function")
> >>
> >>Error in getS3method("stats::kruskal.test", "function") :
> >>        no function 'stats::kruskal.test' could be found
> >>
> >>I searched the archives and the answer was ' use getS3method ' . The help
> for
> >>getS3method is getS3method(f,class,optional=FALSE) so I am lost
> >>Can somebody tell me how to get the source listing of kruskal.test or of
> any
> >>other hidden function?
> >>Thanks
> >>Heberto Ghezzo
> >>Meakins-Christie Labs
> >>Canada
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >>
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jeaneid at chass.utoronto.ca  Mon Jan 17 20:40:40 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Mon, 17 Jan 2005 14:40:40 -0500
Subject: [R] C-c C-c  and tcltk cause R to get a segmentation fault
Message-ID: <Pine.SGI.4.40.0501171346140.6034669-100000@origin.chass.utoronto.ca>


when tcltk is called and a C-c C-c is issued a message of "Illegal
instruction" is issued and the R process is killed.  To reproduce please
do the following

require(tcltk)
C-c C-c

It will say "Illegal instruction"
I use a debian 2.4 kernel with R 2.0.1 tcltck version 2.0.1.
I use tcltk to have gui for setwd, any other methods to graphically do
this is also appreciated.

Thanks so much for any solutions.


Jean



From helprhelp at yahoo.com  Mon Jan 17 20:35:09 2005
From: helprhelp at yahoo.com (Weiwei Shi)
Date: Mon, 17 Jan 2005 11:35:09 -0800 (PST)
Subject: [R] discretization
Message-ID: <20050117193509.26039.qmail@web61306.mail.yahoo.com>

Hi, there:
I have a variable whose distribution is far from
normal and its qqnorm is S-shape, like a logisitic
plot. My purpose is to discretize it into 2 or 3
classes. (basically, a transformation from quantative
to discrete). I am wondering if there is a good way to
do that.

thanks,

Ed



From edd at debian.org  Mon Jan 17 21:14:04 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 17 Jan 2005 14:14:04 -0600
Subject: [R] debian /etc/apt/sources.list for CRAN?
In-Reply-To: <41EC065B.8000303@yorku.ca>
Message-ID: <20050117201403.GA13018@sonny.eddelbuettel.com>

On Mon, Jan 17, 2005 at 01:39:23PM -0500, Michael Friendly wrote:
> Should I just delete the cran.r-project.org line
> above, 

I think so.  Plus, if you already point to Debian unstable, CRAN has nothing
you wouldn't have otherwise.

> or are there debian R packages elsewhere on CRAN that I should 
> include instead?

Not that I know of. 

In the past, CRAN had current Debian packages of R itself for Debian
flavours other than unstable which typically has the most current R version
right at release time. However, migration to testing can take time so the we
sometime kept the same . And for stable we used to have volunteer-donated
rebuilds of the current R. This has gotten progressively more difficult as
Debian stable got older and older, and process of creating the R packages
started to take advantage of facilities to available to the versions of the
tools in stable.

We have talked at times about providing more CRAN packages as Debian
packages, and possibly via CRAN itself rather than full Debian uploads. But
I have not had time to work on that for a while, and that is unfortunately
rather unlikely to change soon.  Others have expressed an interest in
helping as well but haven't gotten very far, unfortunately.

Fellow Debian maintainer Matt Hope, has started a project on Debian's alioth
host (that is open to non-Debianers who want to contribute) to coordinate
debianisation of BioConductor. As this would use the same infrastructures,
it could get used for CRAN too. But this project is also slow in progressing
-- we're all overworked volunteers in this.  

Right now I'd say this is waiting for someone with a real itch to scratch.

Regards,  Dirk

-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From edd at debian.org  Mon Jan 17 21:19:29 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 17 Jan 2005 14:19:29 -0600
Subject: [R] How do I format something as "0.000"?
In-Reply-To: <Pine.GSO.3.95q.1050117164410.2245C-100000@sun11.math.uni-hamburg.de>
References: <Pine.GSO.3.95q.1050117164410.2245C-100000@sun11.math.uni-hamburg.de>
Message-ID: <20050117201929.GB13018@sonny.eddelbuettel.com>

On Mon, Jan 17, 2005 at 04:49:35PM +0100, Christian Hennig wrote:
> Hi,
> 
> I would like to use the format function to get numbers all with three 
> digits to the right of the decimal point, even in cases where there is no
> significant digit left. For example, I would like to get
> c(0.3456789,0.0000053) as "0.346" "0.000".
> It seems that it is not possible to force format to print a "0.000", i.e.
> without any significant decimal places.
> Is it possible to do this somehow in R?

sprintf can do that

	> sprintf("%3.3f %3.3f", 0.3456789, 0.00000053)
	[1] "0.346 0.000"

You'd have to loop over your vector to do it one by one, I think.

Hth, Dirk

-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From edd at debian.org  Mon Jan 17 21:24:11 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 17 Jan 2005 14:24:11 -0600
Subject: [R] Skewness test
In-Reply-To: <Pine.GSO.3.95q.1050117195751.2245E-100000@sun11.math.uni-hamburg.de>
References: <Pine.GSO.3.95q.1050117195751.2245E-100000@sun11.math.uni-hamburg.de>
Message-ID: <20050117202411.GC13018@sonny.eddelbuettel.com>

On Mon, Jan 17, 2005 at 07:59:32PM +0100, Christian Hennig wrote:
> Hi,
> 
> is there a test for the H0 skewness=0 (or with skewness as test
> statistic and normality as H0) implemented in R?

Not that I know of, but tseries has the standard Jarque-Bera test that
combines 3rd and 4th moment tests into a chisq(2) as on omnibus test for
normality.  Cutting and pasting from the code in tseries/R/test.R, one could
probably take that apart and just use 

    m1 <- sum(x)/n
    m2 <- sum((x-m1)^2)/n
    m3 <- sum((x-m1)^3)/n
    m4 <- sum((x-m1)^4)/n
    b1 <- (m3/m2^(3/2))^2

    STATISTICS <-  n * 1/6 * b1
    
as a chisq(1).  But I have no idea what the power of that test would be.  

Hth, Dirk

-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From sundar.dorai-raj at pdf.com  Mon Jan 17 21:51:10 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 17 Jan 2005 12:51:10 -0800
Subject: [R] bwplot: how not to draw outliers
In-Reply-To: <20050117211322.701d59b3@portia.local>
References: <20050117211322.701d59b3@portia.local>
Message-ID: <41EC253E.8020100@pdf.com>



RenE J.V. Bertin wrote:

> Hello, and (somewhat belated) best wishes for 2005.
> 
> Can one order not to draw outliers in bwplot, or at least exclude them from the vertical axis scaling? If so, how (or what doc do I need to consult)?
> The options that have this effect in boxplot() do not appear to have any effect with bwplot (although outline=FALSE in boxplot does *not* change the scaling).
> 
> Thanks,
> RenE Bertin
> 


RenE,

There may be other solutions but you can do this using the prepanel 
option to set the ylim:

library(lattice)
set.seed(1)
z <- data.frame(x = rt(100, 1), g = rep(letters[1:4], each = 25))
bwplot(x ~ g, z,
        prepanel = function(x, y) {
          bp <- boxplot(split(y, x), plot = FALSE)
          ylim <- range(bp$stats)
          list(ylim = ylim)
        })

If you actually want to exclude the points (rather than just prevent 
outliers from affecting the scale), you will have to modify the 
panel.bwplot function in addition to using the above.

--sundar



From chabotd at globetrotter.net  Mon Jan 17 22:01:22 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Mon, 17 Jan 2005 22:01:22 +0100
Subject: [R] subsetting like in SAS
Message-ID: <F10CFE48-68CA-11D9-A0EC-000393707CF2@globetrotter.net>

I want to thank Petr Pikal, Robert Balshaw and Na Li for suggesting the 
use of "unique" or "!duplicated" on a subset of my data where unwanted 
variables have been removed. This worked perfectly.

Denis Chabot
On 13 Jan 2005 at 11:52, Denis Chabot wrote:

> Hi,
>
> Being in the process of translating some of my SAS programs to R, I
> encountered one difficulty. I have a solution, but it is not elegant
> (and not pleasant to implement).
>
> I have a large dataset with many variables needed to identify the
> origin of a sample, many to describe sample characteristics, others to
> describe site characteristics.
>
> I want only a (shorter) list of sites and their characteristics.
>
> If "origin", "ship_cat", "ship_nb", "trip" and "set" are needed to
> identify a site, in SAS you'd sort on those variables, then read the
> data with:
>
> data sites;
>  set alldata;
>  by origin ship_cat ship_nb trip set;
>  if first.set;
>  keep list-of-variables-detailing-sites;
> run;
>
> In R I did this with the Lag function of Hmisc, and the original data
> set also needs to be sorted first:
>
> oL <- Lag(origin)
> scL <- Lag(ship_cat)
> snL <- Lag(ship_nb)
> tL <- Lag(trip)
> sL <- Lag(set)
> same <- origin==oL & ship_cat==scL & ship_nb==snL & trip==tL & set==sL
> sites <- subset(alldata, !same,
> select=c(list-of-variables-detailing-sites)
>
> Could I do better than this?



From Robert at sanctumfi.com  Mon Jan 17 22:17:18 2005
From: Robert at sanctumfi.com (Robert Sams)
Date: Mon, 17 Jan 2005 21:17:18 -0000
Subject: [R] problem installing RSPython 
Message-ID: <E585EABA11227445B918BFB74C1A4D36015986@sanctum01.sanctumfi.com>

Hi,

I'm trying to install RSPython v0.5-4 on a debian machine (woody,
testing) but am having the following problem.

$R CMD INSTALL -c --library=/usr/lib/R/library RSPython_0.5-4.tar.gz 2>err

But then...
$python
>> import RS
Error in .PythonInit() : Error in Python call: values
Error in library("RSPython") : .First.lib failed for 'RSPython'
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
  File "/usr/lib/R/library/RSPython/Python/RS.py", line 69, in ?
    library("RSPython")
  File "/usr/lib/R/library/RSPython/Python/RS.py", line 58, in library
    return(call("library", name));
  File "/usr/lib/R/library/RSPython/Python/RS.py", line 21, in call
    return RSInternal.call(name, args, other, convert, ref)
RuntimeError: error in calling R: Error in library("RSPython") : .First.lib failed for 'RSPython'

Hmm... But the installation is ok, except for these warnings:
$more err
PythonCall.c: In function `RPy_get':
PythonCall.c:305: warning: passing arg 1 of `PyImport_ImportModule' discards qualifiers from pointer target type
RCall.c: In function `RPy_callPython':
RCall.c:62: warning: passing arg 1 of `PyType_GenericNew' from incompatible pointer type


I'm running R v2.0.1, Python v2.3.4 and have the r-base-dev and
python2.3-dev packages installed. Relevant environment variable settings:
PYTHONPATH=/usr/lib/python2.3:/usr/lib/python2.3/site-packages
R_HOME=/usr/lib/R
PYTHONPATH=${PYTHONPATH}:${R_HOME}/library/RSPython/Python
PYTHONPATH=${PYTHONPATH}:${R_HOME}/library/RSPython/libs
LD_LIBRARY_PATH=${R_HOME}/lib

Any suggestions? I'm probably overlooking something obvious. Thanks.

Robert


Robert Sams

SANCTUM FI LLP
Authorised and Regulated by the FSA. 

Sending encrypted mail:

See http://pgp.mit.edu (search string 'sanctumfi') for updates.
-----BEGIN PGP PUBLIC KEY BLOCK-----
Version: GnuPG v1.0.6 (GNU/Linux)
Comment: For info see http://www.gnupg.org

mQGiBEHVOaIRBAC3kUplVB1o00GQXU9TkKmMWz/LiknbhLgDPBZ+1nw1ffb2FpXW
mYet2yja8YIG3H2LnVla1WGVyfAyZ+QRmnUgm2hOnP7LNZIoHctnGqLmtQvOXvHz
xoDonFAl/ztBFE9aNPbiL4nlgdz9GS6LN6hbw9GTdYGip71QhaeVIRR3HwCgySSl
rUtSV8C7nbHR9C+zwku3EpMD/0yoq1R4lQgJGNbyjzzCRWRblK5cKLh1XUYG3JSc
ltLhoCZMvR+359FUSz+jEbbJaLRpquX48Wjv+7WW5KNVm2nPHSZrZ6tr+nm/ov1Y
DQxbIvBHxLXxltDWJlL/gg3bPaSIRlj/KVw0fB0NU1b25eGFRTtlHEvkAROj92sv
MiaSA/9jjEi2H0+yWujcBxJ6syUjWLlgRxvm85sJBihyan4ufAMFRria+QKx7ZMf
A7MNJ+r7ULdqCPsPdZP3kC7GNfLXBzizy3d2HSB/0oB7AjIoXrYyQvXQjFNQQX+u
XjeRGhALxou3H1NEPiCiHJiiaPU6Uh+KYhBCDOYZpc82FgKwWLQzUm9iZXJ0IFNh
bXMgKFNhbmN0dW0gRkkgTExQKSA8cm9iZXJ0QHNhbmN0dW1maS5jb20+iF0EExEC
AB0FAkHVOaIFCQHhM4AFCwcKAwQDFQMCAxYCAQIXgAAKCRB4Q4+orHX8Yk0hAJ9+
odzCRiih6wZz4NOOSVboJP+lngCeNvFGVxVQW35/qpTaF6wsym9jehi5AQ0EQdU5
oxAEALZEnBUQqKiF/gUqK7zyLJarsVxGsmuj0pkV5gFwCbChA4RA7QgHjknJT3Qd
jLUJOa+rW49WtbDCOBv+VOVp//gLROByZpizW4BYaOw01kI9emMuoc6el6nYXarJ
6aZcA84IFBifdi2a8lB3ofhQuWc/YmxLjcOKbkaIC9lUYHrzAAMGA/0RRhkXCHCL
zRSQj+7nSBE4MTeMJycdytl1wnpWkRUa8MoUYBF6/3oiyCnO9bHbOAkQrULSWRLA
YsUJv0c1b6Dht5LVChGikJqKgCzWVEVUI7ob0F2LctvDxhZLlCctHapFGZn9+6pi
rZW+2XkBmbqhJ8ybKsRAIJNy7OV3sIHoVYhMBBgRAgAMBQJB1TmjBQkB4TOAAAoJ
EHhDj6isdfxizmIAn3I/mZyfAuBNZl0lG+9XpAhR80ThAKDAJEnXrH8dX30rRwDz
1mgpwRYCiw==
=59j7
-----END PGP PUBLIC KEY BLOCK-----



From deepayan at stat.wisc.edu  Mon Jan 17 22:12:36 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 17 Jan 2005 15:12:36 -0600
Subject: [R] bwplot: how not to draw outliers
In-Reply-To: <41EC253E.8020100@pdf.com>
References: <20050117211322.701d59b3@portia.local> <41EC253E.8020100@pdf.com>
Message-ID: <200501171512.36764.deepayan@stat.wisc.edu>

On Monday 17 January 2005 14:51, Sundar Dorai-Raj wrote:
> RenE J.V. Bertin wrote:
> > Hello, and (somewhat belated) best wishes for 2005.
> >
> > Can one order not to draw outliers in bwplot, or at least exclude
> > them from the vertical axis scaling? If so, how (or what doc do I
> > need to consult)? The options that have this effect in boxplot() do
> > not appear to have any effect with bwplot (although outline=FALSE
> > in boxplot does *not* change the scaling).
> >
> > Thanks,
> > RenE Bertin
>
> RenE,
>
> There may be other solutions but you can do this using the prepanel
> option to set the ylim:
>
> library(lattice)
> set.seed(1)
> z <- data.frame(x = rt(100, 1), g = rep(letters[1:4], each = 25))
> bwplot(x ~ g, z,
>         prepanel = function(x, y) {
>           bp <- boxplot(split(y, x), plot = FALSE)
>           ylim <- range(bp$stats)
>           list(ylim = ylim)
>         })
>
> If you actually want to exclude the points (rather than just prevent
> outliers from affecting the scale), you will have to modify the
> panel.bwplot function in addition to using the above.

Right. panel.bwplot calls 

       stats <- boxplot.stats(y[x == xval], coef = coef)

I guess it should have a 'do.out' argument as well. A workaround 
(without changing the panel function) might be to use something like

bwplot(voice.part ~ height, data = singer, 
       par.settings = list(plot.symbol = list(col = "transparent")))

(neither of these would change the limits, of course)

-Deepayan



From Mike.Prager at noaa.gov  Mon Jan 17 22:13:43 2005
From: Mike.Prager at noaa.gov (Michael Prager)
Date: Mon, 17 Jan 2005 16:13:43 -0500
Subject: [R] How do I format something as "0.000"?
In-Reply-To: <Pine.GSO.3.95q.1050117164410.2245C-100000@sun11.math.uni-hamburg.de>
References: <Pine.GSO.3.95q.1050117164410.2245C-100000@sun11.math.uni-hamburg.de>
Message-ID: <41EC2A87.1000007@noaa.gov>

Christian Hennig wrote:

>I would like to use the format function to get numbers all with three 
>digits to the right of the decimal point, even in cases where there is no
>significant digit left. For example, I would like to get
>c(0.3456789,0.0000053) as "0.346" "0.000".

I posted a fairly complete example of doing this some time ago.  If you 
search the list archives under my name, you should be able to find it. 
It's not at hand right now, or I would post it again.  The key was using 
formatC(), but my example had other little issues worked out as well.


MHP

-- 
Michael H. Prager, Ph.D.
Population Dynamics Team
NOAA Center for Coastal Habitat and Fisheries Research
NMFS Southeast Fisheries Science Center
Beaufort, North Carolina  28516  USA
http://shrimp.ccfhrb.noaa.gov/~mprager/



From f.harrell at vanderbilt.edu  Mon Jan 17 22:14:06 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 17 Jan 2005 16:14:06 -0500
Subject: [R] bwplot: how not to draw outliers
In-Reply-To: <41EC253E.8020100@pdf.com>
References: <20050117211322.701d59b3@portia.local> <41EC253E.8020100@pdf.com>
Message-ID: <41EC2A9E.8060705@vanderbilt.edu>

Sundar Dorai-Raj wrote:
> 
> 
> RenE J.V. Bertin wrote:
> 
>> Hello, and (somewhat belated) best wishes for 2005.
>>
>> Can one order not to draw outliers in bwplot, or at least exclude them 
>> from the vertical axis scaling? If so, how (or what doc do I need to 
>> consult)?
>> The options that have this effect in boxplot() do not appear to have 
>> any effect with bwplot (although outline=FALSE in boxplot does *not* 
>> change the scaling).
>>
>> Thanks,
>> RenE Bertin
>>
> 
> 
> RenE,
> 
> There may be other solutions but you can do this using the prepanel 
> option to set the ylim:
> 
> library(lattice)
> set.seed(1)
> z <- data.frame(x = rt(100, 1), g = rep(letters[1:4], each = 25))
> bwplot(x ~ g, z,
>        prepanel = function(x, y) {
>          bp <- boxplot(split(y, x), plot = FALSE)
>          ylim <- range(bp$stats)
>          list(ylim = ylim)
>        })
> 
> If you actually want to exclude the points (rather than just prevent 
> outliers from affecting the scale), you will have to modify the 
> panel.bwplot function in addition to using the above.
> 
> --sundar

You may also want to try

library(Hmisc)
library(lattice)
bwplot(..., panel=panel.bpplot)
?panel.bpplot


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From MSchwartz at MedAnalytics.com  Mon Jan 17 22:33:50 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 17 Jan 2005 15:33:50 -0600
Subject: [R] How do I format something as "0.000"?
In-Reply-To: <20050117201929.GB13018@sonny.eddelbuettel.com>
References: <Pine.GSO.3.95q.1050117164410.2245C-100000@sun11.math.uni-hamburg.de>
	<20050117201929.GB13018@sonny.eddelbuettel.com>
Message-ID: <1105997630.20801.40.camel@horizons.localdomain>

On Mon, 2005-01-17 at 14:19 -0600, Dirk Eddelbuettel wrote:
> On Mon, Jan 17, 2005 at 04:49:35PM +0100, Christian Hennig wrote:
> > Hi,
> > 
> > I would like to use the format function to get numbers all with three 
> > digits to the right of the decimal point, even in cases where there is no
> > significant digit left. For example, I would like to get
> > c(0.3456789,0.0000053) as "0.346" "0.000".
> > It seems that it is not possible to force format to print a "0.000", i.e.
> > without any significant decimal places.
> > Is it possible to do this somehow in R?
> 
> sprintf can do that
> 
> 	> sprintf("%3.3f %3.3f", 0.3456789, 0.00000053)
> 	[1] "0.346 0.000"
> 
> You'd have to loop over your vector to do it one by one, I think.

To do this in vectorized fashion, you can use formatC():

x <- c(0.3456789, 0.00000053)

> formatC(x, format = "f", digits = 3)
[1] "0.346" "0.000"

See ?formatC for more help.

HTH,

Marc Schwartz



From chang10000.tw at yahoo.com.tw  Mon Jan 17 22:39:01 2005
From: chang10000.tw at yahoo.com.tw (chang10000.tw)
Date: Tue, 18 Jan 2005 05:39:01 +0800 (CST)
Subject: [R] a question of mixed effect in R
Message-ID: <20050117213901.56869.qmail@web18004.mail.tpe.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050118/f552ce05/attachment.pl

From cts at debian.org  Mon Jan 17 23:37:31 2005
From: cts at debian.org (Christian T. Steigies)
Date: Mon, 17 Jan 2005 23:37:31 +0100
Subject: [R] debian /etc/apt/sources.list for CRAN?
In-Reply-To: <20050117201403.GA13018@sonny.eddelbuettel.com>
References: <41EC065B.8000303@yorku.ca>
	<20050117201403.GA13018@sonny.eddelbuettel.com>
Message-ID: <20050117223731.GA5432@skeeve>

On Mon, Jan 17, 2005 at 02:14:04PM -0600, Dirk Eddelbuettel wrote:
> On Mon, Jan 17, 2005 at 01:39:23PM -0500, Michael Friendly wrote:
> > Should I just delete the cran.r-project.org line
> > above, 
> 
> I think so.  Plus, if you already point to Debian unstable, CRAN has nothing
> you wouldn't have otherwise.
> 
> > or are there debian R packages elsewhere on CRAN that I should 
> > include instead?
> 
> Not that I know of. 
> 
> In the past, CRAN had current Debian packages of R itself for Debian
> flavours other than unstable which typically has the most current R version
> right at release time. However, migration to testing can take time so the we
> sometime kept the same . And for stable we used to have volunteer-donated
> rebuilds of the current R. This has gotten progressively more difficult as
> Debian stable got older and older, and process of creating the R packages
> started to take advantage of facilities to available to the versions of the
> tools in stable.

Not sure if this is still relevant, but the backport I made has been
apt-getable from this address for a while:

deb http://people.debian.org/~cts/debian stable/

I'd prefer if this where integrated in CRAN, but I don't think I can make
that directory available via rsync (unless you are a debian developer).
Probably the debian server can handle the extra load for a few people still
running Debian/stable.
I should be able to keep the packages current, but not necessarily within a
day, like Dirk manages to do.

Christian



From helprhelp at yahoo.com  Tue Jan 18 00:27:31 2005
From: helprhelp at yahoo.com (Weiwei Shi)
Date: Mon, 17 Jan 2005 15:27:31 -0800 (PST)
Subject: [R] rpart
Message-ID: <20050117232731.43963.qmail@web61305.mail.yahoo.com>

Hi, there:
I am working on a classification problem by using
rpart. when my response variable y is binary, the
trees grow very fast, but if I add one more case to y,
that is making y has 3 cases, the tree growing cannot
be finished.
the command looks like:
x<-rpart(r0$V142~.,data=r0[,1:141],
parms=list(split='gini'), cp=0.01)

changing cp or removing parms does not help. 

summary($V142) gives like:
> summary(r0$V142)
  0   1   2 
370  14  16 

I am not sure if rpart can do this or there is
something wrong with my approach.

Please be advised.

Ed



From julien.damon at free.fr  Mon Jan 17 21:00:53 2005
From: julien.damon at free.fr (Julien Damon)
Date: Mon, 17 Jan 2005 21:00:53 +0100
Subject: [R] debian /etc/apt/sources.list for CRAN?
In-Reply-To: <41EC065B.8000303@yorku.ca>
References: <41EC065B.8000303@yorku.ca>
Message-ID: <41EC1975.4030700@free.fr>

I think the debian packages are now maintened in the official debian 
repository.

http://packages.debian.org/cgi-bin/search_packages.pl?keywords=r-cran&searchon=names&subword=1&version=unstable&release=all
http://packages.debian.org/cgi-bin/search_packages.pl?keywords=r-base&searchon=names&subword=1&version=unstable&release=all

So you don't need a specific entry in your source.list anymore.
-- 
   Julien Damon <julien.damon at free.fr>
   http://julien.damon.free.fr



From edd at debian.org  Tue Jan 18 01:12:52 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 17 Jan 2005 18:12:52 -0600
Subject: [R] debian /etc/apt/sources.list for CRAN?
In-Reply-To: <41EC1975.4030700@free.fr>
References: <41EC065B.8000303@yorku.ca> <41EC1975.4030700@free.fr>
Message-ID: <20050118001252.GA1937@sonny.eddelbuettel.com>

On Mon, Jan 17, 2005 at 09:00:53PM +0100, Julien Damon wrote:
> I think the debian packages are now maintened in the official debian 
> repository.

Yes, as they have been since December 1997.

Dirk

-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From hsu at charter.net  Tue Jan 18 01:29:45 2005
From: hsu at charter.net (Ming Hsu)
Date: Mon, 17 Jan 2005 16:29:45 -0800
Subject: [R] Weighted least squares
Message-ID: <BE119879.176D7%hsu@charter.net>

Hi,

I would like to run a weighted least squares with the the weighting matrix
W.  I ran the following two regressions,

(W^-1)Y = Xb + e
Y = WXb+ We

In both cases, E[bhat] = b.

I used the following commands in R

lm1 <- lm(Y/W ~ X)
lm2 <- lm(Y ~ W:X, weights = W)

where 

Y <- rnorm(10,1)
X <- Y + rnorm(10,1)
W <- 1:10

In lm2, I believe W is applied to the error term, resulting in WLS.  However
the estimated coefficients in lm1 and lm2 are really different.  I tried glm
as well, but the result was the same as lm.

Any advice would be appreciated,

Hsu Ming



From hsu at charter.net  Tue Jan 18 01:30:00 2005
From: hsu at charter.net (Ming Hsu)
Date: Mon, 17 Jan 2005 16:30:00 -0800
Subject: [R] lme confusion
Message-ID: <BE119888.176D7%hsu@charter.net>

Hi, this is my first time using the nlme package, and I ran into the
following puzzling problem.

I estimated a mixed effects model using lme, once using groupedData, once
explicitly stating the equations.  I had the following outputs.  All the
coefficients were similar, but they're always slightly different, making me
think that it's not due to numerical error.

Also, what is the "Corr" field in the Random Effects output?  Is it the
correlation between the various regressors?

Here are the outputs.

1. Linear mixed-effects model fit by REML
 Data: groupedData(dPx ~ EMX + EMY | Session, data = X.cen)
       AIC     BIC    logLik
  834.1692 862.532 -407.0846

Random effects:
 Formula: ~EMX + EMY | Session
 Structure: General positive-definite
            StdDev    Corr
(Intercept) 1.0205525 (Intr) EMX
EMX         0.2708627  1
EMY         0.2795289 -1     -1
Residual    5.5076376

Fixed effects: dPx ~ EMX + EMY
                 Value Std.Error  DF   t-value p-value
(Intercept)  1.3011219 0.6807083 121  1.911423  0.0583
EMX          0.7878296 0.2539316 121  3.102526  0.0024
EMY         -0.1566070 0.1534066 121 -1.020862  0.3094
 Correlation: 
    (Intr) EMX   
EMX  0.151       
EMY -0.573 -0.092

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max
-3.00618687 -0.23680151 -0.03431868  0.15386198  6.27114243

Number of Observations: 129
Number of Groups: 6

===============================

2. Linear mixed-effects model fit by REML
 Data: X.cen 
      AIC      BIC    logLik
  834.457 862.8199 -407.2285

Random effects:
 Formula: ~EMX + EMY | Session
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev    Corr
(Intercept) 1.0101137 (Intr) EMX
EMX         0.2108649  0.857
EMY         0.2995491 -0.944 -0.882
Residual    5.5104113

Fixed effects: dPx ~ EMX + EMY
                 Value Std.Error  DF   t-value p-value
(Intercept)  1.3062194 0.6823464 121  1.914305  0.0579
EMX          0.7612238 0.2440504 121  3.119125  0.0023
EMY         -0.1677985 0.1618076 121 -1.037025  0.3018
 Correlation: 
    (Intr) EMX   
EMX  0.059       
EMY -0.552 -0.002

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max
-3.00604994 -0.24210830 -0.01660797  0.14846499  6.27931955

Number of Observations: 129
Number of Groups: 6



From jwd at surewest.net  Tue Jan 18 02:19:00 2005
From: jwd at surewest.net (John Dougherty)
Date: Mon, 17 Jan 2005 17:19:00 -0800
Subject: [R] Skewness test
In-Reply-To: <Pine.GSO.3.95q.1050117195751.2245E-100000@sun11.math.uni-hamburg.de>
References: <Pine.GSO.3.95q.1050117195751.2245E-100000@sun11.math.uni-hamburg.de>
Message-ID: <200501171719.00820.jwd@surewest.net>

On Monday 17 January 2005 10:59, Christian Hennig wrote:
> Hi,
>
> is there a test for the H0 skewness=0 (or with skewness as test
> statistic and normality as H0) implemented in R?
>
> Thank you,
> Christian
>
The e1071 package contains skewness()and kurtosis()commands.  That may be a 
place to start at least.

John



From may_acct at yahoo.com  Tue Jan 18 03:43:42 2005
From: may_acct at yahoo.com (may sawasdee)
Date: Mon, 17 Jan 2005 18:43:42 -0800 (PST)
Subject: [R] Why  can't run Gamma Distribution by GLM?
Message-ID: <20050118024342.84463.qmail@web54501.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050117/8263c03c/attachment.pl

From robert.nelson at case.edu  Tue Jan 18 05:12:32 2005
From: robert.nelson at case.edu (Robert Nelson)
Date: Mon, 17 Jan 2005 23:12:32 -0500
Subject: [R] X11 installation problem Linux X86_64
Message-ID: <4.3.2.7.2.20050117230653.00ddbe38@hal.cwru.edu>

Hello all,

This is my first time posting, so forgive my ignorance.  I've searched high 
and low but haven't come across the specific answer to my questions.

configure runs fine, then make gives this error:

...
gcc -shared -L/usr/local/lib -o R_X11.so  dataentry.lo devX11.lo rotated.lo 
rbitmap.lo   -L/usr/X11R6/lib -lX11  -ljpeg -lpng -lz  -lreadline -ldl 
-lncurses -lm
/usr/bin/ld: skipping incompatible /usr/X11R6/lib/libX11.so when searching 
for -lX11
/usr/bin/ld: skipping incompatible /usr/X11R6/lib/libX11.a when searching 
for -lX11
/usr/bin/ld: cannot find -lX11
collect2: ld returned 1 exit status
make[4]: *** [R_X11.so] Error 1
make[4]: Leaving directory `/root/R-2.0.1/src/modules/X11'
...

This is a 64-bit machine, so I'm wondering if the problem stems from make 
not looking in the /usr/X11R6/lib64 directory, as opposed to 
/usr/X11R6/lib64.  Both dirs are in /etc/ld.so.conf.  I also tried 
configure --x-libraries=/usr/X11R6/lib64, but that produced the same 
error.  I believe that I have all the requisite packages installed.  Could 
someone please point me in the right direction?  Thanks in advance

-Robert



From Lorenz.Gygax at fat.admin.ch  Tue Jan 18 07:50:41 2005
From: Lorenz.Gygax at fat.admin.ch (Lorenz.Gygax@fat.admin.ch)
Date: Tue, 18 Jan 2005 07:50:41 +0100
Subject: [R] 3d bar plot
Message-ID: <BF74FADD4B44554CA7E53D0B5242CD6A01FC6428@evd-s7014.evd.admin.ch>

> This graph -> 
> http://www.math.hope.edu/~tanis/dallas/images/disth36.gif
> is an example I found at
> http://www.math.hope.edu/~tanis/dallas/disth1.html
> created by Maple.
> 
> Does anybody know how to create something similar in R?
> 
> I have a feeling it could be possible using scatterplot3d
> (perhaps with type=h, the fourth example in help('scatterplot3d')?),
> but I cannot figure it out.

Sorry to butt in with a more fundamental question. Is this really the kind
of graph we want to cultivate and support? In my oppinion, it is hardly ever
necessary to have a graph in 3D or even in higher dimensions (one certain
exception is if one tries to spin a higly dimensional dataset in search of
patterns as you can do in ggobi and there might certainly be others).

At least the graph presented in the example does - in my eyes - not warrant
a 3D plot. Why not just draw curves for each of the n's in a plot of 'A'
against 'row'? This would enable a reader to make straightforward
comparisons of the curves and allow to estimate the height of the 'columns'
along the 'A'-axis much more easily.

Only because we can easily create 3D graphs, I do not believe that we should
use them often. Only if a careful evaluation of alternatives was not
promising success I would resign myself to using 3D graphs.

Lorenz
- 
Lorenz Gygax, Dr. sc. nat.
Centre for proper housing of ruminants and pigs
Swiss Federal Veterinary Office
agroscope FAT T?nikon, CH-8356 Ettenhausen / Switzerland



From marwan.khawaja at aub.edu.lb  Mon Jan 17 19:43:21 2005
From: marwan.khawaja at aub.edu.lb (Marwan Khawaja)
Date: Mon, 17 Jan 2005 20:43:21 +0200
Subject: [R] 3d bar plot
In-Reply-To: <20050117134904.VYNK25979.tomts25-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <E1Cqm2w-00074o-00@cool.aub.edu.lb>

Dear John,
Yes, I meant the scatter3d function in the Rcmdr package -- I will 'behave' next
time :-)
Jonne asked for 'something similar in R' -- hence the suggestion to also use the
package scatterplot3d.

Best Marwan

 
-------------------------------------------------------------------
Marwan Khawaja         http://staff.aub.edu.lb/~mk36/
-------------------------------------------------------------------

> -----Original Message-----
> From: John Fox [mailto:jfox at mcmaster.ca] 
> Sent: Monday, January 17, 2005 3:49 PM
> To: 'Marwan Khawaja'; 'R user'
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] 3d bar plot
> 
> Dear Marwan and Jonne,
> 
> I don't think that there's a scatter3d package, so perhaps 
> Marwan is referring to the scatter3d() function in the Rcmdr 
> package. If so, that function won't make the kind of 3D graph 
> that Jonne wants -- though the rgl package, on which 
> scatter3d() is based, should be able to create the graph. 
> 
> I don't believe that the scatterplot3d() function in the 
> scatterplot3d package can make the plot either, but I may be wrong.
> 
> I hope this helps.
> 
> John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox
> -------------------------------- 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Marwan Khawaja
> > Sent: Sunday, January 16, 2005 7:04 PM
> > To: 'R user'; r-help at stat.math.ethz.ch
> > Subject: RE: [R] 3d bar plot
> > 
> > You can check these packages,
> > ?scatterplot3d
> > ?scatter3d
> > 
> > Best Marwan
> > 
> >  
> > -------------------------------------------------------------------
> > Marwan Khawaja         http://staff.aub.edu.lb/~mk36/
> > -------------------------------------------------------------------
> > 
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch 
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of R user
> > > Sent: Monday, January 17, 2005 12:37 PM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] 3d bar plot
> > > 
> > > This graph ->
> > > http://www.math.hope.edu/~tanis/dallas/images/disth36.gif
> > > is an example I found at
> > > http://www.math.hope.edu/~tanis/dallas/disth1.html
> > > created by Maple.
> > > 
> > > Does anybody know how to create something similar in R?
> > > 
> > > I have a feeling it could be possible using scatterplot3d 
> (perhaps 
> > > with type=h, the fourth example in help('scatterplot3d')?), but I 
> > > cannot figure it out.
> > > 
> > > Thanks in advance,
> > > Jonne.
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Tue Jan 18 08:27:08 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 18 Jan 2005 07:27:08 +0000 (GMT)
Subject: [R] rpart
In-Reply-To: <20050117232731.43963.qmail@web61305.mail.yahoo.com>
References: <20050117232731.43963.qmail@web61305.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0501180723100.1587@gannet.stats>

On Mon, 17 Jan 2005, Weiwei Shi wrote:

> I am working on a classification problem by using
> rpart. when my response variable y is binary, the
> trees grow very fast, but if I add one more case to y,
> that is making y has 3 cases,

Do you mean 3 classes?: you have many more than 3 cases below.

> the tree growing cannot be finished.

Whatever does that mean?  Please see the posting guide and supply the 
information it asks for, a reproducible example and what happens when you 
run it and why you think it is wrong.

> the command looks like:
> x<-rpart(r0$V142~.,data=r0[,1:141],
> parms=list(split='gini'), cp=0.01)
>
> changing cp or removing parms does not help.
>
> summary($V142) gives like:
>> summary(r0$V142)
>  0   1   2
> 370  14  16
>
> I am not sure if rpart can do this or there is something wrong with my 
> approach.

What is `this' you want to do?  Rpart works well with multiple classes: 
see for example MASS4.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jan 18 08:37:22 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 18 Jan 2005 07:37:22 +0000 (GMT)
Subject: [R] Weighted least squares
In-Reply-To: <BE119879.176D7%hsu@charter.net>
References: <BE119879.176D7%hsu@charter.net>
Message-ID: <Pine.LNX.4.61.0501180727270.1587@gannet.stats>

On Mon, 17 Jan 2005, Ming Hsu wrote:

> I would like to run a weighted least squares with the the weighting 
> matrix W.

This is generalized not weighted least squares if W really is a matrix and 
not a vector of case-by-case weights.

> I ran the following two regressions,
>
> (W^-1)Y = Xb + e
> Y = WXb+ We

If W is a diagonal matrix, the weights for the second are W^(-2) and you 
used W below.

> In both cases, E[bhat] = b.
>
> I used the following commands in R
>
> lm1 <- lm(Y/W ~ X)
> lm2 <- lm(Y ~ W:X, weights = W)
>
> where
>
> Y <- rnorm(10,1)
> X <- Y + rnorm(10,1)
> W <- 1:10

That W is not a matrix!

> In lm2, I believe W is applied to the error term, resulting in WLS.  However
> the estimated coefficients in lm1 and lm2 are really different.  I tried glm
> as well, but the result was the same as lm.
>
> Any advice would be appreciated,

Please do check that you supply an example that agrees with your words.

Use lm.gls in MASS or gls in nlme for generalized least squares, if that 
is what you meant.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jan 18 08:55:56 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 18 Jan 2005 07:55:56 +0000 (GMT)
Subject: [R] X11 installation problem Linux X86_64
In-Reply-To: <4.3.2.7.2.20050117230653.00ddbe38@hal.cwru.edu>
References: <4.3.2.7.2.20050117230653.00ddbe38@hal.cwru.edu>
Message-ID: <Pine.LNX.4.61.0501180741140.1587@gannet.stats>

On Mon, 17 Jan 2005, Robert Nelson wrote:

> Hello all,
>
> This is my first time posting, so forgive my ignorance.  I've searched high 
> and low but haven't come across the specific answer to my questions.

I don't think we have seen such a misconfigured system before.  What is 
`Linux' here?

> configure runs fine,

I doubt it, unless erroneously is `fine'.

> then make gives this error:
>
> ...
> gcc -shared -L/usr/local/lib -o R_X11.so  dataentry.lo devX11.lo rotated.lo 
> rbitmap.lo   -L/usr/X11R6/lib -lX11  -ljpeg -lpng -lz  -lreadline -ldl 
> -lncurses -lm
> /usr/bin/ld: skipping incompatible /usr/X11R6/lib/libX11.so when searching 
> for -lX11
> /usr/bin/ld: skipping incompatible /usr/X11R6/lib/libX11.a when searching for 
> -lX11
> /usr/bin/ld: cannot find -lX11
> collect2: ld returned 1 exit status
> make[4]: *** [R_X11.so] Error 1
> make[4]: Leaving directory `/root/R-2.0.1/src/modules/X11'
> ...
>
> This is a 64-bit machine, so I'm wondering if the problem stems from make not 
> looking in the /usr/X11R6/lib64 directory, as opposed to /usr/X11R6/lib64.

R works well on several Linux x86_64, e.g. FC3.  I get

gcc -shared -L/usr/local/lib -o R_X11.so  dataentry.lo devX11.lo 
rotated.lo rbitmap.lo  -lSM -lICE -L/usr/X11R6/lib64 -lX11  -ljpeg -lpng -lz

(on R-devel, where the unnecessary -lncurses -lm are skipped).

> Both dirs are in /etc/ld.so.conf.  I also tried configure 
> --x-libraries=/usr/X11R6/lib64, but that produced the same error.  I believe 
> that I have all the requisite packages installed.  Could someone please point 
> me in the right direction?  Thanks in advance

Where did /usr/X11R6/lib come from?   Probably by configure calling xmkmf, 
and likely that is broken on your `Linux'.   Please take a close look at 
the configure output.  I see in config.log:

configure:34402: checking for X
configure:34640: result: libraries /usr/X11R6/lib64, headers /usr/X11R6/include


Setting --x-libraries should work, but you do need to delete the cache 
first.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Tue Jan 18 08:57:53 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 18 Jan 2005 08:57:53 +0100
Subject: [R] discretization
In-Reply-To: <20050117193509.26039.qmail@web61306.mail.yahoo.com>
References: <20050117193509.26039.qmail@web61306.mail.yahoo.com>
Message-ID: <41ECC181.2040900@statistik.uni-dortmund.de>

Weiwei Shi wrote:
> Hi, there:
> I have a variable whose distribution is far from
> normal and its qqnorm is S-shape, like a logisitic
> plot. My purpose is to discretize it into 2 or 3
> classes. (basically, a transformation from quantative
> to discrete). I am wondering if there is a good way to
> do that.

If this question is meant technical: ?cut
If you want to know how to choose the breaks, I'd suggest to think about 
the problem that you are going to analyze and to look at the data.

Uwe Ligges

> thanks,
> 
> Ed
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Jan 18 09:02:24 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 18 Jan 2005 09:02:24 +0100
Subject: [R] pairs: altering pch options on upper and lower panel
	of	pairwise scatter plots
In-Reply-To: <52A8091888A23F47A013223014B6E9FE64BEA8@03-CSEXCH.uopnet.plymouth.ac.uk>
References: <52A8091888A23F47A013223014B6E9FE64BEA8@03-CSEXCH.uopnet.plymouth.ac.uk>
Message-ID: <41ECC290.9080607@statistik.uni-dortmund.de>

Paul Hewson wrote:

> Hello,
> 
> I can't figure out how to use the upper.panel and lower.panel options in
> pairs to alter the label options for either panel independently of the
> other.
> 
> I would like to be able to show the pairwise scatter plots for the data
> as they are (a vanilla pairs plot?) but separately to be able to label
> the points according to a factor level.   It is easy enough to do this
> independently, but I can't figure out how to combine the two settings on
> the same plot.   I have enclosed a toy example:   
> 
> Some made up data, e.g.
> 
> ## make up some multivariate normal data using mvrnorm from MASS
> ##
> require(MASS)
> mu <- c(0,0,0)
> digma <- matrix(c(0.9,0.5,0.5, 0.5,0.9,0.5, 0.5,0.5,0.9)
> dummy.data <- mvrnorm(100,mu,sigma)
> ##
> ## add a factor
> ##
> gender <- as.factor(c(rep("Male",50),rep("Female",50)))
> 
> 
> 
> 
> I can create two separate pairs plots (upper and lower panel) as
> follows:
> 
> ## pairs plot just showing the data
> 
> pairs(dummy.data, lower.panel = NULL)
> 
> ## pairs plot where the points are labelled according to a factor
> 
> pairs(dummy.data, pch = levels(gender), upper.panel = NULL)
> 
> 
> But I would like to be able to combine the two panels on the same plot.
> I would be grateful for any advice.   I've tried passing several
> combinations of functions to upper.panel and lower.panel but either
> they're the wrong function or I'm barking up the wrong tree entirely.


   pairs(dummy.data,
     lower.panel = function(x, y) points(x, y, pch = levels(gender)))

Uwe Ligges



> Thanks
> 
> Paul Hewson
> 
> -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
> Lecturer in Statistics
> University of Plymouth
> Drake Circus 
> Plymouth PL4 8AA
> 
> Tel ++ 44 1752 232778
> Email paul.hewson at plymouth.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Jan 18 09:07:10 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 18 Jan 2005 08:07:10 +0000 (GMT)
Subject: [R] Weighted least squares
In-Reply-To: <Pine.LNX.4.61.0501180727270.1587@gannet.stats>
References: <BE119879.176D7%hsu@charter.net>
	<Pine.LNX.4.61.0501180727270.1587@gannet.stats>
Message-ID: <Pine.LNX.4.61.0501180805570.2225@gannet.stats>

On Tue, 18 Jan 2005, Prof Brian Ripley wrote:

> On Mon, 17 Jan 2005, Ming Hsu wrote:
>
>> I would like to run a weighted least squares with the the weighting matrix 
>> W.
>
> This is generalized not weighted least squares if W really is a matrix and 
> not a vector of case-by-case weights.
>
>> I ran the following two regressions,
>> 
>> (W^-1)Y = Xb + e
>> Y = WXb+ We
>
> If W is a diagonal matrix, the weights for the second are W^(-2) and you used 
> W below.
>
>> In both cases, E[bhat] = b.
>> 
>> I used the following commands in R
>> 
>> lm1 <- lm(Y/W ~ X)
>> lm2 <- lm(Y ~ W:X, weights = W)
>> 
>> where
>> 
>> Y <- rnorm(10,1)
>> X <- Y + rnorm(10,1)
>> W <- 1:10
>
> That W is not a matrix!
>
>> In lm2, I believe W is applied to the error term, resulting in WLS. 
>> However
>> the estimated coefficients in lm1 and lm2 are really different.  I tried 
>> glm
>> as well, but the result was the same as lm.
>> 
>> Any advice would be appreciated,
>
> Please do check that you supply an example that agrees with your words.

Another difference is that your equations have no intercept, but the lm 
calls do, so you need to add +0 to their rhs.

> Use lm.gls in MASS or gls in nlme for generalized least squares, if that is 
> what you meant.
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Tue Jan 18 09:09:22 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 18 Jan 2005 09:09:22 +0100
Subject: [R] how to produce 2-d color plots in R
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E537@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E537@usrymx25.merck.com>
Message-ID: <16876.50226.878824.270742@stat.math.ethz.ch>

>>>>> "AndyL" == Liaw, Andy <andy_liaw at merck.com>
>>>>>     on Fri, 14 Jan 2005 13:00:03 -0500 writes:

    AndyL> Cut and paste the following function (to be in next
    AndyL> release of R, I believe) 

that's correct; it's in current R-devel (for which snapshots
are available from CRAN).

    AndyL> and type:

    AndyL> RSiteSearch("election maps")

    AndyL> This, of course, assumes that you have Internet
    AndyL> connection.

    AndyL> Andy

    AndyL> RSiteSearch <- function(string,
    AndyL> 	restrict=c("Rhelp02a","Rhelp01","functions","docs"),
    AndyL> 	format="normal", sortby="score", matchesPerPage=20) {

    AndyL> string <-
    AndyL> paste("http://finzi.psych.upenn.edu/cgi-bin/namazu.cgi?query=",
    AndyL>    ..............
    AndyL>    ..............

one of the differences of the R-devel version of RSiteSearch()
and yours is that we now have the nice `generic' URL
    http://search.R-project.org/  {which resolves to Jonathan Baron's site}

Martin



From vito_ricci at yahoo.com  Tue Jan 18 09:49:46 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Tue, 18 Jan 2005 09:49:46 +0100 (CET)
Subject: [R] Re: Skewness test
Message-ID: <20050118084946.87355.qmail@web41208.mail.yahoo.com>

Hi Christian,

see this links for skewness tests:

http://www.xycoon.com/skewness_test_1.htm
http://www.xycoon.com/skewness_test_2.htm

http://www.xycoon.com/skewness_small_sample_test_1.htm
http://www.xycoon.com/skewness_small_sample_test_2.htm

there you can find some help to solve your problem. I
believe it's simple to write R code!

Regards,
Vito


You wrote:

Hi,

is there a test for the H0 skewness=0 (or with
skewness as test
statistic and normality as H0) implemented in R?

Thank you,
Christian

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From r.alberts at rug.nl  Tue Jan 18 19:27:32 2005
From: r.alberts at rug.nl (Rudi Alberts)
Date: 18 Jan 2005 10:27:32 -0800
Subject: [R] embedding fonts in eps files
Message-ID: <1106072852.1829.6.camel@gbic04>

Hi,

I have to make eps files with fonts embedded. 
I use the following postscript command:


postscript("fig3a.eps", width = 5.2756, height = 7.27, pointsize =
7,horizontal = FALSE, onefile = FALSE, paper = "special",family =
"Times")

plot(...)

dev.off()


Are fonts automatically embedded in this way?
How can I see that?
If not, how to do it?


regards, Rudi.



From agl22 at hermes.cam.ac.uk  Tue Jan 18 11:10:47 2005
From: agl22 at hermes.cam.ac.uk (A.G. Lynch)
Date: Tue, 18 Jan 2005 10:10:47 +0000 (GMT)
Subject: [R] Possible problem with pbirthday
Message-ID: <Pine.LNX.4.60.0501180938480.26831@hermes-1.csi.cam.ac.uk>


Dear all

I have come across the following problem with pbirthday:
(I have observed this in R 1.9.1 and R 2.0.1 on the Windows OS)

As I understand it, pbirthday(n,c,k) gives the approximate probability 
that we see a class with k coicident people in it when n people are sorted into 
c classes.

so the command

> pbirthday(4,classes=3,coincident=4)

should give the approximate probability that when four people fall into 
three classes, all four end up in the same class. A probability 
that is clearly lower than the presently returned value of 1.


It seems to me that the line in the function

if (n > classes) return(1)

is only relevant to the default case of coincident = 2. (Naturally, if 
there are more people than classes, then at least one class must contain 
2 people).


Since the pbirthday function has been generalized to accept 
other values for coincidence

if (n > (classes * (coincident-1))) return(1)

would seem to me to be (one version of) the necessary line.


Alternatively I could be getting entirely the wrong end of the stick.

If you could either confirm or disabuse then I would be grateful

Yours

Andy Lynch

Centre for Applied Medical Statistics
Department of Public Health and Primary Care
University of Cambridge
Institute of Public Health



From ripley at stats.ox.ac.uk  Tue Jan 18 11:59:08 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 18 Jan 2005 10:59:08 +0000 (GMT)
Subject: [R] embedding fonts in eps files
In-Reply-To: <1106072852.1829.6.camel@gbic04>
References: <1106072852.1829.6.camel@gbic04>
Message-ID: <Pine.LNX.4.61.0501181051220.29697@gannet.stats>

On Tue, 18 Jan 2005, Rudi Alberts wrote:

> I have to make eps files with fonts embedded.
> I use the following postscript command:
>
> postscript("fig3a.eps", width = 5.2756, height = 7.27, pointsize =
> 7,horizontal = FALSE, onefile = FALSE, paper = "special",family =
> "Times")
>
> plot(...)
>
> dev.off()
>
> Are fonts automatically embedded in this way?
>
> How can I see that?
> If not, how to do it?

Let me read to you the help page:

      The software including the PostScript plot file should either embed
      the font outlines (usually from '.pfb' or '.pfa' files) or use DSC
      comments to instruct the print spooler to do so.

In the case of the Times family, all PostScript interpreters should have 
access to the fonts, but the DSC comments will list them to ensure this.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From iparmet at bgumail.bgu.ac.il  Tue Jan 18 12:06:11 2005
From: iparmet at bgumail.bgu.ac.il (Yisrael Parmet)
Date: Tue, 18 Jan 2005 13:06:11 +0200
Subject: [R] How can i fit mixed effect model in a logistic regression?
Message-ID: <000501c4fd4d$b83f1eb0$40644884@ie.ad.bgu.ac.il>



From rado.bonk at jrc.it  Tue Jan 18 12:55:48 2005
From: rado.bonk at jrc.it (Rado Bonk)
Date: Tue, 18 Jan 2005 12:55:48 +0100
Subject: [R] standardised residuals using standard deviation
Message-ID: <E4AC2239-6947-11D9-BA8F-000D9332569C@jrc.it>

Dear R-users,

I need to standardize residuals using standard deviation. Is the 
'stdres' the proper function? Beside other methods (for standardization 
and normalization), are there some approaches how to standardize using 
standard deviation?

Here is the sample of my residuals:

   Zres040    Zres0820     Sres040    Sres0820     PCres040
  -2.101740  -2.0682900   1.6328500   0.5046730 -0.005208170
   1.203350   1.2594100  -1.6836200  -1.1871700  0.001087080
  -0.724331  -0.8699000  -1.2246600  -0.2533400  0.002608070
  -0.333226  -0.2219900  -1.5083400  -1.6578600  0.001130550
  -1.850980  -1.8498800   6.7426900   7.6073600 -0.006078260
   2.676660   2.5251400   3.4960500   1.9224600  0.005720740
  -0.628793  -0.7680440  -0.7999610   0.7899290  0.007231940

Thanks in advance

Rado



--
Radoslav Bonk
European Commission - DG  Joint Research Centre (JRC)
Institute of Environment and Sustainability (IES)
LM Unit - Natural Hazards
Via E. Fermi, TP 261, 210 20 Ispra (VA), ITALY
tel: 0039 0332 78 6013
fax: 0039 0332 78 6653
http://natural-hazards.jrc.it/floods



From Ted.Harding at nessie.mcc.ac.uk  Tue Jan 18 12:48:58 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 18 Jan 2005 11:48:58 -0000 (GMT)
Subject: [R] embedding fonts in eps files
In-Reply-To: <1106072852.1829.6.camel@gbic04>
Message-ID: <XFMail.050118114858.Ted.Harding@nessie.mcc.ac.uk>

On 18-Jan-05 Rudi Alberts wrote:
> Hi,
> 
> I have to make eps files with fonts embedded. 
> I use the following postscript command:
> 
> 
> postscript("fig3a.eps", width = 5.2756, height = 7.27, pointsize =
> 7,horizontal = FALSE, onefile = FALSE, paper = "special",family =
> "Times")
> 
> plot(...)
> 
> dev.off()
> 
> 
> Are fonts automatically embedded in this way?
> How can I see that?
> If not, how to do it?

Well, it seems to have set Times as the working font family
when I used your postscript(...) command above; but see further
down. I viewed the resulting .eps file (using 'less' in Linux
but Windows users should also have some way of looking into a
text file).

The first few lines of the file are:

%!PS-Adobe-3.0 EPSF-3.0
%%DocumentNeededResources: font Times-Roman
%%+ font Times-Bold
%%+ font Times-Italic
%%+ font Times-BoldItalic
%%+ font Symbol
%%Title: R Graphics Output
...

These are so-called DSC ("Document Structuring Conventions")
comments and are not directly executed by whatever renders
the PostScript code. They do, however, provided useful
information for programs which have to handle the PS file.

>From the above, it can be seen that R's postscript() function
has taken note of the 'family="Times"' option.

Further down the .eps file are the lines

%%IncludeResource: font Times-Roman
/Times-Roman findfont
dup length dict begin
  {1 index /FID ne {def} {pop pop} ifelse} forall
  /Encoding ISOLatin1Encoding def
  currentdict
  end
/Font1 exch definefont pop
%%IncludeResource: font Times-Bold
/Times-Bold findfont
dup length dict begin
  {1 index /FID ne {def} {pop pop} ifelse} forall
  /Encoding ISOLatin1Encoding def
  currentdict
  end
/Font2 exch definefont pop
%%IncludeResource: font Times-Italic
/Times-Italic findfont
dup length dict begin
  {1 index /FID ne {def} {pop pop} ifelse} forall
  /Encoding ISOLatin1Encoding def
  currentdict
  end
/Font3 exch definefont pop
%%IncludeResource: font Times-BoldItalic
/Times-BoldItalic findfont
dup length dict begin
  {1 index /FID ne {def} {pop pop} ifelse} forall
  /Encoding ISOLatin1Encoding def
  currentdict
  end
/Font4 exch definefont pop
%%IncludeResource: font Symbol
/Symbol findfont
dup length dict begin
  {1 index /FID ne {def} {pop pop} ifelse} forall
  currentdict
  end
/Font5 exch definefont pop

Apart from the "%%" DSC comments, this is executable PS
code which calls on the interpreter to set up the Times
fonts Times-Roman as Font1, Times-Bold as Font2,
Times-Italic as Font3, Times-BoldItalic as Font4,
and Symbol (not a Times font) as Font5.

If, instead of 'family="Times"', you had used the option
'family="Helvetica"', you would have got (try it and see)
exactly the same with "Helvetica" substituted for "Times"
throughout.

So far so good. Now comes the crunch.

The above (and this is the only part of the .eps file
which has anything to do with setting up fonts) assumes
that the PS interpreter (i.e. the program, including
printer firmware, which renders the PS visible) already
has access to the PostScript definitions of these fonts.

There is a default assumption (not just in R but in
practically any software which outputs PostScript) that
the rendering device will have built-in access to the
"Standard Adobe Font Set" -- a set of 13 fonts comprising
the Times, Helvetica and Courier families, and the Symbol
font, together with the encoding vectors StandardEncoding
and ISOLatin1Encoding; most software also assumes the
presence of further families (typically Bookman, Palatino,
AvantGarde, HelveticaNarrow, ZapfChanceryMediumItalic,
ZapfDingbats). None of these are strictly required by
the specification of the PostScript language, but they
have been a de facto standard for decades and it is most
unusual to find PostScript-generating software which does
not take them for granted (at least the 13 "Standard Adobe"
fonts).

Now "built-in access" means that the rendering device
is already equipped with the PostScript definitions of
how to draw ("render") the characters ("glyphs") in these
various fonts -- e.g. a PostScript printer will have the
definitions internally stored in a ROM chip. Therefore
when the PS definitions of "Font1" etc. as in the above
file are encountered, the device simply hooks its own PS
definitions into the working stack. It is not necessary
to include the definitions in the file which is being
interpreted.

However, to "embed" a font (which is what you refer to
in your query) means to include the PS font definition
in the file itself. This is certainly necessary if you
want to use a "non-standard" font, which might just be
an arty-farty font for English characters (e.g. a fancy
cursive script for greetings cards), or something more
exotic like Cyrillic characters or the International
Phonetic Alphabet. Since PS definitions of such fonts
cannot be expected to be present in a standard PS
rendering device, software which creates files to be
displayed with such fonts must itself have the necessary
resources.

You say you "have to make eps files with fonts embedded."
It would be most unusual to need to embed Times fonts.
Are you using a "non-standard" font family?

As far as I know, R has no provision to embed PS font
definitions in an EPS file. Others may be able to correct
this statement ...

If you do need to embed a PS font, it is not exactly
straightforward to do it "by hand". There is a lot of
stuff that needs to be set up, and one would need to
know more about your environment in order to give any
specific advice.

You ask how you can find out if a font has been embedded.
You need to look through the PS file for stuff like the
following (exemplified for the non-standard font
AGaramond-Regular, the Roman style of Garamond):

%%BeginResource: font AGaramond-Regular
%%CreationDate: Thu Jan 16 17:32:29 1992
%%VMusage: 41576 52468
11 dict begin
/FontInfo 10 dict dup begin
...
/FullName (Adobe Garamond Regular) readonly def
/FamilyName (Adobe Garamond) readonly def
/Weight (Regular) readonly def
/isFixedPitch false def
/ItalicAngle 0 def
/UnderlinePosition -100 def
/UnderlineThickness 50 def
end readonly def
/FontName /AGaramond-Regular def
/Encoding StandardEncoding def
/PaintType 0 def
/FontType 1 def
/FontMatrix [0.001 0 0 0.001 0 0] readonly def
/UniqueID 37598 def
/FontBBox{-183 -269 1099 851}readonly def
currentdict end
currentfile eexec
63c3dc0161e235a2106828042ed9adba0cb00296e1d7da605eb328f0654cec2c
...
[1471 lines of hex-encoded stuff: the PS font definition]
...
0000000000000000000000000000000000000000000000000000000000000000
cleartomark
%%EndResource

The "eexec" instructs the rendering device to decode and
execute the hex-encoded block, after which it then has the
font definition stored in its working memory and available
to be used via "findfont" etc.

Hoping this helps: please supply more specific information
of what you need to do, if you want to follow this up.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 18-Jan-05                                       Time: 11:48:58
------------------------------ XFMail ------------------------------



From bitwrit at ozemail.com.au  Wed Jan 19 23:15:50 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Thu, 20 Jan 2005 09:15:50 +1100
Subject: [R] Time line plot in R?
In-Reply-To: <41EBBDDD.3040207@oomvanlieshout.net>
References: <41EBBDDD.3040207@oomvanlieshout.net>
Message-ID: <20050118121510.MYPS18660.smta08.mail.ozemail.net@there>

Sander Oom wrote:
> Dear R users,
>
> In order to illustrate the possible effects of events on variables
> plotted against time, I would like plot a time line of events along side
> the plot of the variables.
>
> The x-axis should be some time unit; the y-axis should be the variable
> of interest; the time line should be plotted below the graph along the
> same x-axis scale.
>
> As I have many variables and different events lists, I would like to
> write a script to read the events from a file and plot them together
> with the other plot.
>
> The time line should look something like this:
> http://www.oslis.k12.or.us/secondary/howto/organize/images/timeline.gif
>
Here's one way:

# some fake data please, maestro
fakedata<-sample(1:10,10)
# leave a bit of extra space beneath and to the left of the plot
par(mar=6,6,4,2)
# this function will probably end up in the "plotrix" package
time.line<-function(x,y,at=NULL,labels=TRUE,tlticks=NULL,...) {
 if(missing(x) && missing (y))
  stop("Usage: time.line(x,y,at=NULL,labels=TRUE)")
 plotrange<-par("usr")
 # get the graphical parameters
 oldpar<-par(no.readonly=TRUE)
 # turn off clipping
 par(xpd=TRUE)
 if(missing(x)) {
  # it's a horizontal line
  segments(plotrange[1],y,plotrange[2],y,...)
  ticklen<-(plotrange[4]-plotrange[3])*0.02
  if(!is.null(tlticks))
   segments(tlticks,y+ticklen,tlticks,y-ticklen,...)
  mwidth<-strwidth("M")
  # blank out the line where labels will appear
  rect(at-mwidth,y-ticklen,at+mwidth,y+ticklen,col="white",border=FALSE)
 # rotate the text
  par(srt=90)
  # draw the labels
  text(at,y,labels,...)
 }
 if(missing(y)) {
  # it's a vertical line
  # draw the line
  segments(x,plotrange[3],x,plotrange[4],...)
  ticklen<-(plotrange[2]-plotrange[1])*0.02
  if(!is.null(tlticks))
   segments(x+ticklen,tlticks,x-ticklen,tlticks,...)
  mheight<-strheight("M")
  # blank out the line where labels will appear
  rect(x-ticklen,at-mheight,x+ticklen,at+mheight,col="white",border=FALSE)
  # draw the labels
  text(x,at,labels,...)
 }
 # restore the parameters
 par(oldpar)
}
# create a file with the positions and labels you want like this:
# 2.5,first
# 4,second
# 7,third
# 8.5,fourth
# call it "labels.txt" and read it in
tl.labels<-read.table("labels.txt",sep=",")
plot(fakedata,xlab="")
# display a horizontal time line
time.line(x=-1,at=tl.labels[[1]],labels=tl.labels[[2]],col="black")
# now a vertical one
time.line(y=-1,at=tl.labels[[1]],labels=tl.labels[[2]],col="black")



From luk111111 at yahoo.com  Tue Jan 18 13:28:57 2005
From: luk111111 at yahoo.com (luk)
Date: Tue, 18 Jan 2005 04:28:57 -0800 (PST)
Subject: [R] Interpretation of randomForest results
In-Reply-To: <Pine.A41.4.61b.0501100701100.333248@homer11.u.washington.edu>
Message-ID: <20050118122857.54014.qmail@web30902.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050118/5a9550b8/attachment.pl

From simon at stats.gla.ac.uk  Tue Jan 18 13:38:36 2005
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Tue, 18 Jan 2005 12:38:36 +0000 (GMT)
Subject: [R] embedding fonts in eps files
In-Reply-To: <1106072852.1829.6.camel@gbic04>
References: <1106072852.1829.6.camel@gbic04>
Message-ID: <Pine.LNX.4.58.0501181230360.7310@moon.stats.gla.ac.uk>

Do you ultimately need the eps files themselves with embedded fonts, or 
is that you need to include them in a document that that has all fonts 
embedded? 

If it's the latter then you may find:

http://mpa.itc.it/markus/highres_pdf.html

useful. The trick seems to be to substitute the standard Fonts with their 
URW equivalents: the URW fonts are freely available and can therefor be 
embedded. It simplifies matters if you choose URW `families' when calling
`postscript()'.

> 
> I have to make eps files with fonts embedded. 
> I use the following postscript command:
> 
> 
> postscript("fig3a.eps", width = 5.2756, height = 7.27, pointsize =
> 7,horizontal = FALSE, onefile = FALSE, paper = "special",family =
> "Times")
> 
> plot(...)
> 
> dev.off()
> 
> 
> Are fonts automatically embedded in this way?
> How can I see that?
> If not, how to do it?
> 
> 
> regards, Rudi.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Tue Jan 18 14:12:23 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 18 Jan 2005 08:12:23 -0500
Subject: [R] Interpretation of randomForest results
Message-ID: <3A822319EB35174CA3714066D590DCD50994E546@usrymx25.merck.com>

> From: luk
> 
> I got the following results when I run radomForest with below 
> commands:
>  
> qair <- read.table("train10.dat", header = T)
> oz.rf <- randomForest(LESION ~ ., data = qair, ntree = 220,  
> importance = TRUE)
> print(oz.rf)
> 
> Call:
>  randomForest.formula(x = LESION ~ ., data = qair, ntree = 
> 220,      importance = TRUE) 
>                Type of random forest: classification
>                      Number of trees: 220
> No. of variables tried at each split: 2
>         OOB estimate of  error rate: 15.86%
          ^^^

Note what that says, which applies to the confusion matrix below as well.

> Confusion matrix:
>        lesion noninf class.error
> lesion   3949    525   0.1173447
> noninf    894   3580   0.1998212
> 
> What did this mean? Is 11.7% the classification error for 
> 'lesion' class, and 19.98% the classification error for 
> 'noninf' class in the training set?

The results you showed above are out-of-bag (OOB) results.  If you don't
know what that means, you should read the documentation, and perhaps the
references.
 
> But when I run below command to test the performance of 
> classification in the same training set.
> 
> ntrain <- read.table("train10.dat", header = T)
> ntrain.pred <- predict(oz.rf, ntrain)
> table(observed = ntrain[, "LESION"], predicted = ntrain.pred)
> 
> I got the following results. It seemed that the 
> classification rates for 'lesion' and 'noninf' classes are 0. 
> Any suggestion will be very appreciated.

randomForest is rather good at overfitting _training_ data, but that's
(usually) not a problem in classification.  What one usually cares about is
the _test set_ performance.  There, randomForest performance does not
degrade as the number of trees increases, and that's what Breiman meant by
`random forests do not overfit'.

Andy

 
> 
>         predicted
> observed lesion noninf
>   lesion 4474      0  
>   noninf    0   4474  
> 
> 
>  
> 
> 
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From slist at oomvanlieshout.net  Tue Jan 18 14:22:19 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Tue, 18 Jan 2005 15:22:19 +0200
Subject: [R] Time line plot in R?
In-Reply-To: <20050118121510.MYPS18660.smta08.mail.ozemail.net@there>
References: <41EBBDDD.3040207@oomvanlieshout.net>
	<20050118121510.MYPS18660.smta08.mail.ozemail.net@there>
Message-ID: <41ED0D8B.3080800@oomvanlieshout.net>

Jim,

Brilliant! Thought someone might have figured it out already. Now we 
just need a gallery to show off this graph!

One little thing: the par(mar=6,6,4,2) gives an error:

'Error in par(args) : parameter "mar" has the wrong length'

Any suggestions?

Code below includes fake labels for testing.

Cheers,

Sander.


# some fake data please, maestro
fakedata<-sample(1:10,10)
# leave a bit of extra space beneath and to the left of the plot
par(mar=6,6,4,2)
# this function will probably end up in the "plotrix" package
time.line<-function(x,y,at=NULL,labels=TRUE,tlticks=NULL,...) {
  if(missing(x) && missing (y))
   stop("Usage: time.line(x,y,at=NULL,labels=TRUE)")
  plotrange<-par("usr")
  # get the graphical parameters
  oldpar<-par(no.readonly=TRUE)
  # turn off clipping
  par(xpd=TRUE)
  if(missing(x)) {
   # it's a horizontal line
   segments(plotrange[1],y,plotrange[2],y,...)
   ticklen<-(plotrange[4]-plotrange[3])*0.02
   if(!is.null(tlticks))
    segments(tlticks,y+ticklen,tlticks,y-ticklen,...)
   mwidth<-strwidth("M")
   # blank out the line where labels will appear
   rect(at-mwidth,y-ticklen,at+mwidth,y+ticklen,col="white",border=FALSE)
  # rotate the text
   par(srt=270)
   # draw the labels
   text(at,y,labels,...)
  }
  if(missing(y)) {
   # it's a vertical line
   # draw the line
   segments(x,plotrange[3],x,plotrange[4],...)
   ticklen<-(plotrange[2]-plotrange[1])*0.02
   if(!is.null(tlticks))
    segments(x+ticklen,tlticks,x-ticklen,tlticks,...)
   mheight<-strheight("M")
   # blank out the line where labels will appear
   rect(x-ticklen,at-mheight,x+ticklen,at+mheight,col="white",border=FALSE)
   # draw the labels
   text(x,at,labels,...)
  }
  # restore the parameters
  par(oldpar)
}

# some fake labels
eventT<-c(2.5, 4, 7, 8.5)
eventD<-c("first event","second event","third event","fourth event")
tl.labels<-data.frame(eventT,eventD)

plot(fakedata,xlab="")
# display a horizontal time line
time.line(x=-1,at=tl.labels[[1]],labels=tl.labels[[2]],col="black")
# now a vertical one
time.line(y=-1,at=tl.labels[[1]],labels=tl.labels[[2]],col="black")


 > version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    0.1
year     2004
month    11
day      15
language R
 >

Jim Lemon wrote:
> Sander Oom wrote:
> 
>>Dear R users,
>>
>>In order to illustrate the possible effects of events on variables
>>plotted against time, I would like plot a time line of events along side
>>the plot of the variables.
>>
>>The x-axis should be some time unit; the y-axis should be the variable
>>of interest; the time line should be plotted below the graph along the
>>same x-axis scale.
>>
>>As I have many variables and different events lists, I would like to
>>write a script to read the events from a file and plot them together
>>with the other plot.
>>
>>The time line should look something like this:
>>http://www.oslis.k12.or.us/secondary/howto/organize/images/timeline.gif
>>
> 
> Here's one way:
> 
> # some fake data please, maestro
> fakedata<-sample(1:10,10)
> # leave a bit of extra space beneath and to the left of the plot
> par(mar=6,6,4,2)
> # this function will probably end up in the "plotrix" package
> time.line<-function(x,y,at=NULL,labels=TRUE,tlticks=NULL,...) {
>  if(missing(x) && missing (y))
>   stop("Usage: time.line(x,y,at=NULL,labels=TRUE)")
>  plotrange<-par("usr")
>  # get the graphical parameters
>  oldpar<-par(no.readonly=TRUE)
>  # turn off clipping
>  par(xpd=TRUE)
>  if(missing(x)) {
>   # it's a horizontal line
>   segments(plotrange[1],y,plotrange[2],y,...)
>   ticklen<-(plotrange[4]-plotrange[3])*0.02
>   if(!is.null(tlticks))
>    segments(tlticks,y+ticklen,tlticks,y-ticklen,...)
>   mwidth<-strwidth("M")
>   # blank out the line where labels will appear
>   rect(at-mwidth,y-ticklen,at+mwidth,y+ticklen,col="white",border=FALSE)
>  # rotate the text
>   par(srt=90)
>   # draw the labels
>   text(at,y,labels,...)
>  }
>  if(missing(y)) {
>   # it's a vertical line
>   # draw the line
>   segments(x,plotrange[3],x,plotrange[4],...)
>   ticklen<-(plotrange[2]-plotrange[1])*0.02
>   if(!is.null(tlticks))
>    segments(x+ticklen,tlticks,x-ticklen,tlticks,...)
>   mheight<-strheight("M")
>   # blank out the line where labels will appear
>   rect(x-ticklen,at-mheight,x+ticklen,at+mheight,col="white",border=FALSE)
>   # draw the labels
>   text(x,at,labels,...)
>  }
>  # restore the parameters
>  par(oldpar)
> }
> # create a file with the positions and labels you want like this:
> # 2.5,first
> # 4,second
> # 7,third
> # 8.5,fourth
> # call it "labels.txt" and read it in
> tl.labels<-read.table("labels.txt",sep=",")
> plot(fakedata,xlab="")
> # display a horizontal time line
> time.line(x=-1,at=tl.labels[[1]],labels=tl.labels[[2]],col="black")
> # now a vertical one
> time.line(y=-1,at=tl.labels[[1]],labels=tl.labels[[2]],col="black")
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From ramagopa at etek.chalmers.se  Tue Jan 18 14:38:45 2005
From: ramagopa at etek.chalmers.se (Sivakumar Ramagopal)
Date: Tue, 18 Jan 2005 14:38:45 +0100 (CET)
Subject: [R] Build problem with R-1.4.1 on AM64/Linux
Message-ID: <34689.129.16.20.46.1106055525.squirrel@webmail.chalmers.se>

Hi,

I get the following errors while building R-1.4.1 on AMD64/Linux (SuSE
9.1). R-2.0.1 compiles fine though. I'm building R-1.4.1 because I need to
install SNet (available at
http://cm.bell-labs.com/cm/ms/departments/sia/InternetTraffic/S-Net/SNet_1.0.zip).

The errors I get are:

In file included from dataentry.c:34:
/usr/X11R6/include/X11/Xlib.h:1400: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:1488: error: parse error before "char"

This happens a lot of times in various places in file dataentry.c


dataentry.c: In function `GetKey':
dataentry.c:1249: warning: passing arg 1 of `XLookupString' from
incompatible pointer type
dataentry.c:1249: warning: passing arg 4 of `XLookupString' from
incompatible pointer type
dataentry.c: In function `GetCharP':
dataentry.c:1258: warning: passing arg 1 of `XLookupString' from
incompatible pointer type
dataentry.c:1258: warning: passing arg 4 of `XLookupString' from
incompatible pointer type
dataentry.c: In function `doControl':
dataentry.c:1279: warning: passing arg 1 of `XLookupString' from
incompatible pointer type
dataentry.c:1279: warning: passing arg 4 of `XLookupString' from
incompatible pointer type
dataentry.c: In function `RefreshKeyboardMapping':
dataentry.c:1308: warning: passing arg 1 of `XRefreshKeyboardMapping' from
incompatible pointer type
make[4]: *** [dataentry.lo] Error 1

My guess is I'm missing a few packages but I don't know which ones. How
should I go about from here?

Thanks,
Shiva



From Robert at sanctumfi.com  Tue Jan 18 15:14:58 2005
From: Robert at sanctumfi.com (Robert Sams)
Date: Tue, 18 Jan 2005 14:14:58 -0000
Subject: [R] problem installing RSPython
Message-ID: <E585EABA11227445B918BFB74C1A4D3621487E@sanctum01.sanctumfi.com>

Thank you Duncan. Still no luck, though.

> There are scripts in the installed version
> of RSPython 
> that sets the value of LD_LIBRARY_PATH,
> PYTHONPATH, etc.  appropriately.
> Your LD_LIBRARY_PATH seems to be incomplete for
> RSPython and you report three different settings
> for PYTHONPATH, but of course only one of them is
> the actual value that is set when python is run.
> 
> Use the appropriate script in
> 
>  RSPython/scripts/
> 
> by source'ing them into your shell,
> e.g. 
> 
>   . RSPython/scripts/RPython.bsh

I did...
$. /usr/lib/R/library/RSPython/scripts/RPython.bsh

now i have..
LD_LIBRARY_PATH=/usr/lib/R/lib
PYTHONPATH=/usr/lib/R/library/RSPython/libs:/usr/lib/R/library/RSPython/
Python
R_HOME=/usr/lib/R

but...
$python
import RS

returns the same error. 

And in R...
$R
library(RSPython)
Initialized R-Python interface package.
Warning message: 
the Python callback manager has been registered implicitly using the
defaults (referenceManager())! 

Any other hints? I'm having trouble problem solving this one.
Thanks.
Robert

> Robert Sams wrote:
> > Hi,
> > 
> > I'm trying to install RSPython v0.5-4 on a debian machine (woody,
> > testing) but am having the following problem.
> > 
> > $R CMD INSTALL -c --library=/usr/lib/R/library 
> RSPython_0.5-4.tar.gz 2>err
> > 
> > But then...
> > $python
> > >> import RS
> > Error in .PythonInit() : Error in Python call: values
> > Error in library("RSPython") : .First.lib failed for 'RSPython'
> > Traceback (most recent call last):
> >   File "<stdin>", line 1, in ?
> >   File "/usr/lib/R/library/RSPython/Python/RS.py", line 69, in ?
> >     library("RSPython")
> >   File "/usr/lib/R/library/RSPython/Python/RS.py", line 58, 
> in library
> >     return(call("library", name));
> >   File "/usr/lib/R/library/RSPython/Python/RS.py", line 21, in call
> >     return RSInternal.call(name, args, other, convert, ref)
> > RuntimeError: error in calling R: Error in 
> library("RSPython") : .First.lib failed for 'RSPython'
> > 
> > Hmm... But the installation is ok, except for these warnings:
> > $more err
> > PythonCall.c: In function `RPy_get':
> > PythonCall.c:305: warning: passing arg 1 of 
> `PyImport_ImportModule' discards qualifiers from pointer target type
> > RCall.c: In function `RPy_callPython':
> > RCall.c:62: warning: passing arg 1 of `PyType_GenericNew' 
> from incompatible pointer type
> > 
> > 
> > I'm running R v2.0.1, Python v2.3.4 and have the r-base-dev and
> > python2.3-dev packages installed. Relevant environment 
> variable settings:
> > PYTHONPATH=/usr/lib/python2.3:/usr/lib/python2.3/site-packages
> > R_HOME=/usr/lib/R
> > PYTHONPATH=${PYTHONPATH}:${R_HOME}/library/RSPython/Python
> > PYTHONPATH=${PYTHONPATH}:${R_HOME}/library/RSPython/libs
> > LD_LIBRARY_PATH=${R_HOME}/lib
> > 
> > Any suggestions? I'm probably overlooking something obvious. Thanks.
> > 
> > Robert
> > 
> > 
> > Robert Sams
> > 
> > SANCTUM FI LLP
> > Authorised and Regulated by the FSA. 
> > 
> > Sending encrypted mail:
> > 
> > See http://pgp.mit.edu (search string 'sanctumfi') for updates.
> > -----BEGIN PGP PUBLIC KEY BLOCK-----
> > Version: GnuPG v1.0.6 (GNU/Linux)
> > Comment: For info see http://www.gnupg.org
> > 
> > mQGiBEHVOaIRBAC3kUplVB1o00GQXU9TkKmMWz/LiknbhLgDPBZ+1nw1ffb2FpXW
> > mYet2yja8YIG3H2LnVla1WGVyfAyZ+QRmnUgm2hOnP7LNZIoHctnGqLmtQvOXvHz
> > xoDonFAl/ztBFE9aNPbiL4nlgdz9GS6LN6hbw9GTdYGip71QhaeVIRR3HwCgySSl
> > rUtSV8C7nbHR9C+zwku3EpMD/0yoq1R4lQgJGNbyjzzCRWRblK5cKLh1XUYG3JSc
> > ltLhoCZMvR+359FUSz+jEbbJaLRpquX48Wjv+7WW5KNVm2nPHSZrZ6tr+nm/ov1Y
> > DQxbIvBHxLXxltDWJlL/gg3bPaSIRlj/KVw0fB0NU1b25eGFRTtlHEvkAROj92sv
> > MiaSA/9jjEi2H0+yWujcBxJ6syUjWLlgRxvm85sJBihyan4ufAMFRria+QKx7ZMf
> > A7MNJ+r7ULdqCPsPdZP3kC7GNfLXBzizy3d2HSB/0oB7AjIoXrYyQvXQjFNQQX+u
> > XjeRGhALxou3H1NEPiCiHJiiaPU6Uh+KYhBCDOYZpc82FgKwWLQzUm9iZXJ0IFNh
> > bXMgKFNhbmN0dW0gRkkgTExQKSA8cm9iZXJ0QHNhbmN0dW1maS5jb20+iF0EExEC
> > AB0FAkHVOaIFCQHhM4AFCwcKAwQDFQMCAxYCAQIXgAAKCRB4Q4+orHX8Yk0hAJ9+
> > odzCRiih6wZz4NOOSVboJP+lngCeNvFGVxVQW35/qpTaF6wsym9jehi5AQ0EQdU5
> > oxAEALZEnBUQqKiF/gUqK7zyLJarsVxGsmuj0pkV5gFwCbChA4RA7QgHjknJT3Qd
> > jLUJOa+rW49WtbDCOBv+VOVp//gLROByZpizW4BYaOw01kI9emMuoc6el6nYXarJ
> > 6aZcA84IFBifdi2a8lB3ofhQuWc/YmxLjcOKbkaIC9lUYHrzAAMGA/0RRhkXCHCL
> > zRSQj+7nSBE4MTeMJycdytl1wnpWkRUa8MoUYBF6/3oiyCnO9bHbOAkQrULSWRLA
> > YsUJv0c1b6Dht5LVChGikJqKgCzWVEVUI7ob0F2LctvDxhZLlCctHapFGZn9+6pi
> > rZW+2XkBmbqhJ8ybKsRAIJNy7OV3sIHoVYhMBBgRAgAMBQJB1TmjBQkB4TOAAAoJ
> > EHhDj6isdfxizmIAn3I/mZyfAuBNZl0lG+9XpAhR80ThAKDAJEnXrH8dX30rRwDz
> > 1mgpwRYCiw==
> > =59j7
> > -----END PGP PUBLIC KEY BLOCK-----
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> -- 
> Duncan Temple Lang                duncan at wald.ucdavis.edu
> Department of Statistics          work:  (530) 752-4782
> 371 Kerr Hall                     fax:   (530) 752-7099
> One Shields Ave.
> University of California at Davis
> Davis, CA 95616, USA
> 
> 
>



From andy_liaw at merck.com  Tue Jan 18 15:10:14 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 18 Jan 2005 09:10:14 -0500
Subject: [R] Build problem with R-1.4.1 on AM64/Linux
Message-ID: <3A822319EB35174CA3714066D590DCD50994E547@usrymx25.merck.com>

When R-1.4.1 was released, AMD64 did not exist yet.  You are likely to do
better by trying to update the package to the current version of R.  The
first step of that is to change all the `_' for assignment to `<-' in the R
code.  Some functions might be deprecated, and if so need to be replaced.
It is possible that you may run into more problems, but those problems, I'd
guess, are easier to solve than trying to get R-1.4.1 to compile correctly
on a current AMD64 Linux installation.

Just my 0.01532326 Euro (as of 2005-01-18)...

Andy

> From: Sivakumar Ramagopal
> 
> Hi,
> 
> I get the following errors while building R-1.4.1 on AMD64/Linux (SuSE
> 9.1). R-2.0.1 compiles fine though. I'm building R-1.4.1 
> because I need to
> install SNet (available at
> http://cm.bell-labs.com/cm/ms/departments/sia/InternetTraffic/
> S-Net/SNet_1.0.zip).
> 
> The errors I get are:
> 
> In file included from dataentry.c:34:
> /usr/X11R6/include/X11/Xlib.h:1400: error: parse error before 
> "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:1488: error: parse error before "char"
> 
> This happens a lot of times in various places in file dataentry.c
> 
> 
> dataentry.c: In function `GetKey':
> dataentry.c:1249: warning: passing arg 1 of `XLookupString' from
> incompatible pointer type
> dataentry.c:1249: warning: passing arg 4 of `XLookupString' from
> incompatible pointer type
> dataentry.c: In function `GetCharP':
> dataentry.c:1258: warning: passing arg 1 of `XLookupString' from
> incompatible pointer type
> dataentry.c:1258: warning: passing arg 4 of `XLookupString' from
> incompatible pointer type
> dataentry.c: In function `doControl':
> dataentry.c:1279: warning: passing arg 1 of `XLookupString' from
> incompatible pointer type
> dataentry.c:1279: warning: passing arg 4 of `XLookupString' from
> incompatible pointer type
> dataentry.c: In function `RefreshKeyboardMapping':
> dataentry.c:1308: warning: passing arg 1 of 
> `XRefreshKeyboardMapping' from
> incompatible pointer type
> make[4]: *** [dataentry.lo] Error 1
> 
> My guess is I'm missing a few packages but I don't know which 
> ones. How
> should I go about from here?
> 
> Thanks,
> Shiva
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ligges at statistik.uni-dortmund.de  Tue Jan 18 15:25:17 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 18 Jan 2005 15:25:17 +0100
Subject: [R] Time line plot in R?
In-Reply-To: <41ED0D8B.3080800@oomvanlieshout.net>
References: <41EBBDDD.3040207@oomvanlieshout.net>	<20050118121510.MYPS18660.smta08.mail.ozemail.net@there>
	<41ED0D8B.3080800@oomvanlieshout.net>
Message-ID: <41ED1C4D.3070803@statistik.uni-dortmund.de>

Sander Oom wrote:

> Jim,
> 
> Brilliant! Thought someone might have figured it out already. Now we 
> just need a gallery to show off this graph!
> 
> One little thing: the par(mar=6,6,4,2) gives an error:


par(mar=c(6,6,4,2))

Uwe Ligges

> 'Error in par(args) : parameter "mar" has the wrong length'
> 
> Any suggestions?
> 
> Code below includes fake labels for testing.
> 
> Cheers,
> 
> Sander.
> 
> 
> # some fake data please, maestro
> fakedata<-sample(1:10,10)
> # leave a bit of extra space beneath and to the left of the plot
> par(mar=6,6,4,2)
> # this function will probably end up in the "plotrix" package
> time.line<-function(x,y,at=NULL,labels=TRUE,tlticks=NULL,...) {
>  if(missing(x) && missing (y))
>   stop("Usage: time.line(x,y,at=NULL,labels=TRUE)")
>  plotrange<-par("usr")
>  # get the graphical parameters
>  oldpar<-par(no.readonly=TRUE)
>  # turn off clipping
>  par(xpd=TRUE)
>  if(missing(x)) {
>   # it's a horizontal line
>   segments(plotrange[1],y,plotrange[2],y,...)
>   ticklen<-(plotrange[4]-plotrange[3])*0.02
>   if(!is.null(tlticks))
>    segments(tlticks,y+ticklen,tlticks,y-ticklen,...)
>   mwidth<-strwidth("M")
>   # blank out the line where labels will appear
>   rect(at-mwidth,y-ticklen,at+mwidth,y+ticklen,col="white",border=FALSE)
>  # rotate the text
>   par(srt=270)
>   # draw the labels
>   text(at,y,labels,...)
>  }
>  if(missing(y)) {
>   # it's a vertical line
>   # draw the line
>   segments(x,plotrange[3],x,plotrange[4],...)
>   ticklen<-(plotrange[2]-plotrange[1])*0.02
>   if(!is.null(tlticks))
>    segments(x+ticklen,tlticks,x-ticklen,tlticks,...)
>   mheight<-strheight("M")
>   # blank out the line where labels will appear
>   rect(x-ticklen,at-mheight,x+ticklen,at+mheight,col="white",border=FALSE)
>   # draw the labels
>   text(x,at,labels,...)
>  }
>  # restore the parameters
>  par(oldpar)
> }
> 
> # some fake labels
> eventT<-c(2.5, 4, 7, 8.5)
> eventD<-c("first event","second event","third event","fourth event")
> tl.labels<-data.frame(eventT,eventD)
> 
> plot(fakedata,xlab="")
> # display a horizontal time line
> time.line(x=-1,at=tl.labels[[1]],labels=tl.labels[[2]],col="black")
> # now a vertical one
> time.line(y=-1,at=tl.labels[[1]],labels=tl.labels[[2]],col="black")
> 
> 
>  > version
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    0.1
> year     2004
> month    11
> day      15
> language R
>  >
> 
> Jim Lemon wrote:
> 
>> Sander Oom wrote:
>>
>>> Dear R users,
>>>
>>> In order to illustrate the possible effects of events on variables
>>> plotted against time, I would like plot a time line of events along side
>>> the plot of the variables.
>>>
>>> The x-axis should be some time unit; the y-axis should be the variable
>>> of interest; the time line should be plotted below the graph along the
>>> same x-axis scale.
>>>
>>> As I have many variables and different events lists, I would like to
>>> write a script to read the events from a file and plot them together
>>> with the other plot.
>>>
>>> The time line should look something like this:
>>> http://www.oslis.k12.or.us/secondary/howto/organize/images/timeline.gif
>>>
>>
>> Here's one way:
>>
>> # some fake data please, maestro
>> fakedata<-sample(1:10,10)
>> # leave a bit of extra space beneath and to the left of the plot
>> par(mar=6,6,4,2)
>> # this function will probably end up in the "plotrix" package
>> time.line<-function(x,y,at=NULL,labels=TRUE,tlticks=NULL,...) {
>>  if(missing(x) && missing (y))
>>   stop("Usage: time.line(x,y,at=NULL,labels=TRUE)")
>>  plotrange<-par("usr")
>>  # get the graphical parameters
>>  oldpar<-par(no.readonly=TRUE)
>>  # turn off clipping
>>  par(xpd=TRUE)
>>  if(missing(x)) {
>>   # it's a horizontal line
>>   segments(plotrange[1],y,plotrange[2],y,...)
>>   ticklen<-(plotrange[4]-plotrange[3])*0.02
>>   if(!is.null(tlticks))
>>    segments(tlticks,y+ticklen,tlticks,y-ticklen,...)
>>   mwidth<-strwidth("M")
>>   # blank out the line where labels will appear
>>   rect(at-mwidth,y-ticklen,at+mwidth,y+ticklen,col="white",border=FALSE)
>>  # rotate the text
>>   par(srt=90)
>>   # draw the labels
>>   text(at,y,labels,...)
>>  }
>>  if(missing(y)) {
>>   # it's a vertical line
>>   # draw the line
>>   segments(x,plotrange[3],x,plotrange[4],...)
>>   ticklen<-(plotrange[2]-plotrange[1])*0.02
>>   if(!is.null(tlticks))
>>    segments(x+ticklen,tlticks,x-ticklen,tlticks,...)
>>   mheight<-strheight("M")
>>   # blank out the line where labels will appear
>>   
>> rect(x-ticklen,at-mheight,x+ticklen,at+mheight,col="white",border=FALSE)
>>   # draw the labels
>>   text(x,at,labels,...)
>>  }
>>  # restore the parameters
>>  par(oldpar)
>> }
>> # create a file with the positions and labels you want like this:
>> # 2.5,first
>> # 4,second
>> # 7,third
>> # 8.5,fourth
>> # call it "labels.txt" and read it in
>> tl.labels<-read.table("labels.txt",sep=",")
>> plot(fakedata,xlab="")
>> # display a horizontal time line
>> time.line(x=-1,at=tl.labels[[1]],labels=tl.labels[[2]],col="black")
>> # now a vertical one
>> time.line(y=-1,at=tl.labels[[1]],labels=tl.labels[[2]],col="black")
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From bates at wisc.edu  Tue Jan 18 15:31:07 2005
From: bates at wisc.edu (Douglas Bates)
Date: Tue, 18 Jan 2005 08:31:07 -0600
Subject: [R] lme confusion
In-Reply-To: <BE119888.176D7%hsu@charter.net>
References: <BE119888.176D7%hsu@charter.net>
Message-ID: <41ED1DAB.7070007@wisc.edu>

Ming Hsu wrote:
> Hi, this is my first time using the nlme package, and I ran into the
> following puzzling problem.
> 
> I estimated a mixed effects model using lme, once using groupedData, once
> explicitly stating the equations.  I had the following outputs.  All the
> coefficients were similar, but they're always slightly different, making me
> think that it's not due to numerical error.
> 
> Also, what is the "Corr" field in the Random Effects output?  Is it the
> correlation between the various regressors?
> 
> Here are the outputs.
> 
> 1. Linear mixed-effects model fit by REML
>  Data: groupedData(dPx ~ EMX + EMY | Session, data = X.cen)
>        AIC     BIC    logLik
>   834.1692 862.532 -407.0846
> 
> Random effects:
>  Formula: ~EMX + EMY | Session
>  Structure: General positive-definite
>             StdDev    Corr
> (Intercept) 1.0205525 (Intr) EMX
> EMX         0.2708627  1
> EMY         0.2795289 -1     -1
> Residual    5.5076376
> 
> Fixed effects: dPx ~ EMX + EMY
>                  Value Std.Error  DF   t-value p-value
> (Intercept)  1.3011219 0.6807083 121  1.911423  0.0583
> EMX          0.7878296 0.2539316 121  3.102526  0.0024
> EMY         -0.1566070 0.1534066 121 -1.020862  0.3094
>  Correlation: 
>     (Intr) EMX   
> EMX  0.151       
> EMY -0.573 -0.092
> 
> Standardized Within-Group Residuals:
>         Min          Q1         Med          Q3         Max
> -3.00618687 -0.23680151 -0.03431868  0.15386198  6.27114243
> 
> Number of Observations: 129
> Number of Groups: 6
> 
> ===============================
> 
> 2. Linear mixed-effects model fit by REML
>  Data: X.cen 
>       AIC      BIC    logLik
>   834.457 862.8199 -407.2285
> 
> Random effects:
>  Formula: ~EMX + EMY | Session
>  Structure: General positive-definite, Log-Cholesky parametrization
>             StdDev    Corr
> (Intercept) 1.0101137 (Intr) EMX
> EMX         0.2108649  0.857
> EMY         0.2995491 -0.944 -0.882
> Residual    5.5104113
> 
> Fixed effects: dPx ~ EMX + EMY
>                  Value Std.Error  DF   t-value p-value
> (Intercept)  1.3062194 0.6823464 121  1.914305  0.0579
> EMX          0.7612238 0.2440504 121  3.119125  0.0023
> EMY         -0.1677985 0.1618076 121 -1.037025  0.3018
>  Correlation: 
>     (Intr) EMX   
> EMX  0.059       
> EMY -0.552 -0.002
> 
> Standardized Within-Group Residuals:
>         Min          Q1         Med          Q3         Max
> -3.00604994 -0.24210830 -0.01660797  0.14846499  6.27931955
> 
> Number of Observations: 129
> Number of Groups: 6

Could you please include the calls to lme so we can see exactly what is 
being fit?

You asked about the Corr columns, those are the correlation form of the 
estimated variance-covariance matrix of the random effects.  Notice that 
you are trying to estmate 6 variance-covariance parameters (3 variances 
and 3 covariances) from information on 6 groups.  In the first output 
the estimated variance-covariance matrix is singlular (correlations of 
-1 and +1).  With so many parameters to estimate from so few groups it 
is not surprising that there is difficulty.



From p.dalgaard at biostat.ku.dk  Tue Jan 18 15:29:24 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Jan 2005 15:29:24 +0100
Subject: [R] Build problem with R-1.4.1 on AM64/Linux
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E547@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E547@usrymx25.merck.com>
Message-ID: <x2sm4y4xq3.fsf@biostat.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> When R-1.4.1 was released, AMD64 did not exist yet.  You are likely to do
> better by trying to update the package to the current version of R.  The
> first step of that is to change all the `_' for assignment to `<-' in the R
> code.  Some functions might be deprecated, and if so need to be replaced.
> It is possible that you may run into more problems, but those problems, I'd
> guess, are easier to solve than trying to get R-1.4.1 to compile correctly
> on a current AMD64 Linux installation.

My thoughts too. Also, there seems to be an issue with lazy loading,
which is a newer feature, but can be turned off with a line in the
DESCRIPTION file
 
> Just my 0.01532326 Euro (as of 2005-01-18)...

Wrong currency:

0.138560 SEK

or maybe (guessing from the name)

0.873306 INR

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From etptupaf at bs.ehu.es  Tue Jan 18 15:37:51 2005
From: etptupaf at bs.ehu.es (F.Tusell)
Date: Tue, 18 Jan 2005 15:37:51 +0100
Subject: [R] "Attach" for S4 objects?
Message-ID: <1106059071.7979.14.camel@agesi.bs.ehu.es>

When passing a list as an argument to a function, I find it convenient
to attach it in the first line of th function code, then refer to the
components as A, B, etc. rather than as list$A, list$B, etc.

If I pass a S4 class object, is there a way to "attach" it, or do I have
to refer to the slots as object at A, objetc at B, etc.? 

I could always make copies,  

     A <- object at A 
     B <- object at B
     ...

but "object" has many slots, some fairly large, so this seems wasteful.

Thank you.
-- 
Fernando TUSELL                                e-mail:
Departamento de Econometr?a y Estad?stica           etptupaf at bs.ehu.es 
Facultad de CC.EE. y Empresariales             Tel:   (+34)94.601.3733
Avenida Lendakari Aguirre, 83                  Fax:   (+34)94.601.3754
E-48015 BILBAO  (Spain)                        Secr:  (+34)94.601.3740



From peterm at andrew.cmu.edu  Tue Jan 18 16:18:22 2005
From: peterm at andrew.cmu.edu (Peter Muhlberger)
Date: Tue, 18 Jan 2005 10:18:22 -0500
Subject: [R] Function to modify existing data.frame
Message-ID: <BE1292EE.D0AC%peterm@andrew.cmu.edu>

I'm used to statistical languages, such as Stata, in which it's trivial to
pass a list of variables to a function & have that function modify those
variables in the existing dataset rather than create copies of the variables
or having to replace the entire dataset to change a few variables.  In R, I
suppose I could paste together the right instructions in a function and then
execute it, but is there any more straightforward way of doing this I'm
missing?

Thx,

Peter



From ligges at statistik.uni-dortmund.de  Tue Jan 18 16:23:50 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 18 Jan 2005 16:23:50 +0100
Subject: [R] embedding fonts in eps files
In-Reply-To: <XFMail.050118114858.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050118114858.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <41ED2A06.9070102@statistik.uni-dortmund.de>

To clarify why people might want to embed fonts, I'd like to give an 
example:

After I submitted my book the first time, the printing house asked me 
explicitly to embed *all* fonts in graphics, or they would not be able 
to print from a pdf file (generated from PostScript by certain styles 
for Adobe Distiller). And sometimes discussions are simply hopeless.....

Uwe Ligges



(Ted Harding) wrote:

> On 18-Jan-05 Rudi Alberts wrote:
> 
>>Hi,
>>
>>I have to make eps files with fonts embedded. 
>>I use the following postscript command:
>>
>>
>>postscript("fig3a.eps", width = 5.2756, height = 7.27, pointsize =
>>7,horizontal = FALSE, onefile = FALSE, paper = "special",family =
>>"Times")
>>
>>plot(...)
>>
>>dev.off()
>>
>>
>>Are fonts automatically embedded in this way?
>>How can I see that?
>>If not, how to do it?
> 
> 
> Well, it seems to have set Times as the working font family
> when I used your postscript(...) command above; but see further
> down. I viewed the resulting .eps file (using 'less' in Linux
> but Windows users should also have some way of looking into a
> text file).
> 
> The first few lines of the file are:
> 
> %!PS-Adobe-3.0 EPSF-3.0
> %%DocumentNeededResources: font Times-Roman
> %%+ font Times-Bold
> %%+ font Times-Italic
> %%+ font Times-BoldItalic
> %%+ font Symbol
> %%Title: R Graphics Output
> ...
> 
> These are so-called DSC ("Document Structuring Conventions")
> comments and are not directly executed by whatever renders
> the PostScript code. They do, however, provided useful
> information for programs which have to handle the PS file.
> 
>>From the above, it can be seen that R's postscript() function
> has taken note of the 'family="Times"' option.
> 
> Further down the .eps file are the lines
> 
> %%IncludeResource: font Times-Roman
> /Times-Roman findfont
> dup length dict begin
>   {1 index /FID ne {def} {pop pop} ifelse} forall
>   /Encoding ISOLatin1Encoding def
>   currentdict
>   end
> /Font1 exch definefont pop
> %%IncludeResource: font Times-Bold
> /Times-Bold findfont
> dup length dict begin
>   {1 index /FID ne {def} {pop pop} ifelse} forall
>   /Encoding ISOLatin1Encoding def
>   currentdict
>   end
> /Font2 exch definefont pop
> %%IncludeResource: font Times-Italic
> /Times-Italic findfont
> dup length dict begin
>   {1 index /FID ne {def} {pop pop} ifelse} forall
>   /Encoding ISOLatin1Encoding def
>   currentdict
>   end
> /Font3 exch definefont pop
> %%IncludeResource: font Times-BoldItalic
> /Times-BoldItalic findfont
> dup length dict begin
>   {1 index /FID ne {def} {pop pop} ifelse} forall
>   /Encoding ISOLatin1Encoding def
>   currentdict
>   end
> /Font4 exch definefont pop
> %%IncludeResource: font Symbol
> /Symbol findfont
> dup length dict begin
>   {1 index /FID ne {def} {pop pop} ifelse} forall
>   currentdict
>   end
> /Font5 exch definefont pop
> 
> Apart from the "%%" DSC comments, this is executable PS
> code which calls on the interpreter to set up the Times
> fonts Times-Roman as Font1, Times-Bold as Font2,
> Times-Italic as Font3, Times-BoldItalic as Font4,
> and Symbol (not a Times font) as Font5.
> 
> If, instead of 'family="Times"', you had used the option
> 'family="Helvetica"', you would have got (try it and see)
> exactly the same with "Helvetica" substituted for "Times"
> throughout.
> 
> So far so good. Now comes the crunch.
> 
> The above (and this is the only part of the .eps file
> which has anything to do with setting up fonts) assumes
> that the PS interpreter (i.e. the program, including
> printer firmware, which renders the PS visible) already
> has access to the PostScript definitions of these fonts.
> 
> There is a default assumption (not just in R but in
> practically any software which outputs PostScript) that
> the rendering device will have built-in access to the
> "Standard Adobe Font Set" -- a set of 13 fonts comprising
> the Times, Helvetica and Courier families, and the Symbol
> font, together with the encoding vectors StandardEncoding
> and ISOLatin1Encoding; most software also assumes the
> presence of further families (typically Bookman, Palatino,
> AvantGarde, HelveticaNarrow, ZapfChanceryMediumItalic,
> ZapfDingbats). None of these are strictly required by
> the specification of the PostScript language, but they
> have been a de facto standard for decades and it is most
> unusual to find PostScript-generating software which does
> not take them for granted (at least the 13 "Standard Adobe"
> fonts).
> 
> Now "built-in access" means that the rendering device
> is already equipped with the PostScript definitions of
> how to draw ("render") the characters ("glyphs") in these
> various fonts -- e.g. a PostScript printer will have the
> definitions internally stored in a ROM chip. Therefore
> when the PS definitions of "Font1" etc. as in the above
> file are encountered, the device simply hooks its own PS
> definitions into the working stack. It is not necessary
> to include the definitions in the file which is being
> interpreted.
> 
> However, to "embed" a font (which is what you refer to
> in your query) means to include the PS font definition
> in the file itself. This is certainly necessary if you
> want to use a "non-standard" font, which might just be
> an arty-farty font for English characters (e.g. a fancy
> cursive script for greetings cards), or something more
> exotic like Cyrillic characters or the International
> Phonetic Alphabet. Since PS definitions of such fonts
> cannot be expected to be present in a standard PS
> rendering device, software which creates files to be
> displayed with such fonts must itself have the necessary
> resources.
> 
> You say you "have to make eps files with fonts embedded."
> It would be most unusual to need to embed Times fonts.
> Are you using a "non-standard" font family?
> 
> As far as I know, R has no provision to embed PS font
> definitions in an EPS file. Others may be able to correct
> this statement ...
> 
> If you do need to embed a PS font, it is not exactly
> straightforward to do it "by hand". There is a lot of
> stuff that needs to be set up, and one would need to
> know more about your environment in order to give any
> specific advice.
> 
> You ask how you can find out if a font has been embedded.
> You need to look through the PS file for stuff like the
> following (exemplified for the non-standard font
> AGaramond-Regular, the Roman style of Garamond):
> 
> %%BeginResource: font AGaramond-Regular
> %%CreationDate: Thu Jan 16 17:32:29 1992
> %%VMusage: 41576 52468
> 11 dict begin
> /FontInfo 10 dict dup begin
> ...
> /FullName (Adobe Garamond Regular) readonly def
> /FamilyName (Adobe Garamond) readonly def
> /Weight (Regular) readonly def
> /isFixedPitch false def
> /ItalicAngle 0 def
> /UnderlinePosition -100 def
> /UnderlineThickness 50 def
> end readonly def
> /FontName /AGaramond-Regular def
> /Encoding StandardEncoding def
> /PaintType 0 def
> /FontType 1 def
> /FontMatrix [0.001 0 0 0.001 0 0] readonly def
> /UniqueID 37598 def
> /FontBBox{-183 -269 1099 851}readonly def
> currentdict end
> currentfile eexec
> 63c3dc0161e235a2106828042ed9adba0cb00296e1d7da605eb328f0654cec2c
> ...
> [1471 lines of hex-encoded stuff: the PS font definition]
> ...
> 0000000000000000000000000000000000000000000000000000000000000000
> cleartomark
> %%EndResource
> 
> The "eexec" instructs the rendering device to decode and
> execute the hex-encoded block, after which it then has the
> font definition stored in its working memory and available
> to be used via "findfont" etc.
> 
> Hoping this helps: please supply more specific information
> of what you need to do, if you want to follow this up.
> 
> Best wishes,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> Date: 18-Jan-05                                       Time: 11:48:58
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From MSchwartz at MedAnalytics.com  Tue Jan 18 16:31:33 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 18 Jan 2005 09:31:33 -0600
Subject: [R] Function to modify existing data.frame
In-Reply-To: <BE1292EE.D0AC%peterm@andrew.cmu.edu>
References: <BE1292EE.D0AC%peterm@andrew.cmu.edu>
Message-ID: <1106062293.7835.5.camel@horizons.localdomain>

On Tue, 2005-01-18 at 10:18 -0500, Peter Muhlberger wrote:
> I'm used to statistical languages, such as Stata, in which it's trivial to
> pass a list of variables to a function & have that function modify those
> variables in the existing dataset rather than create copies of the variables
> or having to replace the entire dataset to change a few variables.  In R, I
> suppose I could paste together the right instructions in a function and then
> execute it, but is there any more straightforward way of doing this I'm
> missing?
> 
> Thx,
> 
> Peter


What type of modifications?

Take a look at:

?replace
?transform

HTH,

Marc Schwartz



From j.van_den_hoff at fz-rossendorf.de  Tue Jan 18 17:14:51 2005
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Tue, 18 Jan 2005 17:14:51 +0100
Subject: [R] background color for plotting symbols in 'matplot'
In-Reply-To: <x2sm4y4xq3.fsf@biostat.ku.dk>
References: <3A822319EB35174CA3714066D590DCD50994E547@usrymx25.merck.com>
	<x2sm4y4xq3.fsf@biostat.ku.dk>
Message-ID: <41ED35FB.4090905@fz-rossendorf.de>

something like


matplot2(matrix(1:6,3,2),matrix(7:12,3,2),pch=21,bg=c(2,3),type='b')


does not yield the expected (at least by me) result: only the points on 
the first line get (successively) background colors for the plotting 
symbols, the second line gets no background color at all for its 
plotting symbols.

I think the natural behaviour should be two curves which (for the 
example given above) symbol-background colors 2 and 3, respectively (as 
would be obtained by a suitable manual combination of 'plot' and 
'lines'). the modification of the matplot code to achieve this 
behaviour is obvious as far as I can see (adding 'bg' to the explicit 
arguments of matplot and handling similar to 'lty', 'cex' and the like 
inside the function including transfer to 'plot' and 'lines' argument list).


is the present behaviour a bug of 'matplot' or is it for some reason 
intended behaviour?


regards,

joerg



From cc2240 at columbia.edu  Tue Jan 18 17:25:34 2005
From: cc2240 at columbia.edu (Chung Chang)
Date: Tue, 18 Jan 2005 11:25:34 -0500
Subject: [R] a question about linear mixed model in R 
Message-ID: <1106065534.41ed387e3e601@cubmail.cc.columbia.edu>

Dear all,

I have a somewhat unusual linear mixed model that I can't seem
to code in lme.  It's only unusual in that one random effect is
applied only to some of the observations (I have an indicator
variable
that specifies which observations have this random effect).

The model is:

X_hijk = alpha_h + h * b_i + r_(ij) + e_hijk , where

  h = 0 or 1 (indicator)
  i = 1, ..., N
  j = 1, ..., n_i
  k = 1, ..., K
alpha is fixed, and the rest are random.
I'm willing to assume b, r, and e are mutually independent
and normal with var(b) = sigma^2_b, var(r) = sigma^2_r, and
var(e) = sigma^2.

Any help in writing this model in lme() would be greatly
appreciated.

Thanks,

Chung Cheng



From jtk at cmp.uea.ac.uk  Tue Jan 18 18:49:42 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Tue, 18 Jan 2005 17:49:42 +0000
Subject: [R] Function to modify existing data.frame
In-Reply-To: <1106062293.7835.5.camel@horizons.localdomain>
References: <BE1292EE.D0AC%peterm@andrew.cmu.edu>
	<1106062293.7835.5.camel@horizons.localdomain>
Message-ID: <20050118174942.GD8758@jtkpc.cmp.uea.ac.uk>

On Tue, Jan 18, 2005 at 09:31:33AM -0600, Marc Schwartz wrote:
> On Tue, 2005-01-18 at 10:18 -0500, Peter Muhlberger wrote:
> > I'm used to statistical languages, such as Stata, in which it's trivial to
> > pass a list of variables to a function & have that function modify those
> > variables in the existing dataset rather than create copies of the variables
> > or having to replace the entire dataset to change a few variables.  In R, I
> > suppose I could paste together the right instructions in a function and then
> > execute it, but is there any more straightforward way of doing this I'm
> > missing?

> What type of modifications?
> 
> Take a look at:
> 
> ?replace
> ?transform

I know neither the original poster nor Stata, but as I understand the
question, this answer may not be exactly what Peter has been looking
for. As the docs on replace state:

    x is unchanged: remember to assign the result.

So, something like

    x <- replace(x, 4:6, 3:5 * 3)

does not eliminate creation of a copy of x. A function that modifies
its argument would require a call by reference mechanism, which isn't
provided by R.

Some time ago, I looked quite extensively for a call by reference facility
in R, but found that there is none. In the archives of this mailing list
and elsewhere, various alternatives to call by reference approaches are
suggested. As far as I can see, the issues of performance hits incurred
by unnecessary copying of large objects, and of implementing true state
changes in object oriented programming cannot fully be resolved, however.

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From t.muhlhofer at lse.ac.uk  Tue Jan 18 18:06:37 2005
From: t.muhlhofer at lse.ac.uk (Tobias Muhlhofer)
Date: Tue, 18 Jan 2005 17:06:37 +0000
Subject: [R] Panel methods, implied var/cov structure
Message-ID: <41ED421D.8010107@lse.ac.uk>

Hi!

I have a (fairly narrow and long) panel dataset of returns across three 
portfolios over 100-odd time-series observations. I have reason to 
believe that there is heteroskedasticity in the error terms, but that 
this heteroskedasticity is only through time, i.e. that the three 
portfolios have the same underlying covariance structure over time, 
which of course is unknown to me and about which I do not want to make 
assumptions as to functional form.

Greene says there are GLS methods to handle this type of situation. What 
R functions am I looking for?

Is there another way of estimating this? Perhaps SUR, or something like 
that?

Thanks,
	Tobias



From dmb at mrc-dunn.cam.ac.uk  Tue Jan 18 18:16:46 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 18 Jan 2005 17:16:46 +0000 (GMT)
Subject: [R] chi-square and error bars?
Message-ID: <Pine.LNX.4.21.0501181706450.16945-100000@mail.mrc-dunn.cam.ac.uk>


This may sound crazy but...

I have data like this...

> results.matrix
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
[1,]  949   93    2   11   26   20    7    6   10     5     0     3
[2,] 1233  124   24   35   58   57   17   21   31    19    11    21


Which is the result of binning (summing) the response variables of an
underlying (nearly) continious range of predictor variable into 12 bins.

I can see that each row is not drawn from the same underlying population
using chisq.test.

What I would like to do is plot each proportion from each row with a
confidence interval which represents the results of the chisq.test,
i.e. put confidence intervals on each proportion.

I thought about doing the arcsine transformation on the proportions and
then I can use the binomial distribution to get the variance and hence 95%
confidence interval (then back transformation).

Clearly confidence in latter points is weaker (fewer observations), but I
can't seem to find a plot to capture this fact.



From Robert.McGehee at geodecapital.com  Tue Jan 18 18:19:22 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Tue, 18 Jan 2005 12:19:22 -0500
Subject: [R] "Attach" for S4 objects?
Message-ID: <67DCA285A2D7754280D3B8E88EB5480206741E1D@MSGBOSCLB2WIN.DMN1.FMR.COM>

Certainly this _is_ possible. However, there are no built-in functions to do just this (that I know of). Here's a function that attaches like you say (but still makes a local copy)

attachslot <- function(x) {
    xname <- substitute(x)
    sl <- names(getSlots(class(x)))
    slotnames <- paste(sl, " <<- ", xname, "@", sl, sep = "")

    for (i in slotnames) {
        eval(parse(text = substitute(slotnames, list(slotnames = slotnames))))
    }
}

If you wanted to do this without making a local copy, then the strategy might be either to make a promise object out of the "slotname" (which stores only the unevaluated expression until you need the data), see ?delay, or a bit more complicated, store the "name" of the object as a pointer to the underlying slot. Then, you would be referencing the pointer to the object rather than the copy. Don't want to spend the time to figure this out, but I'm pretty sure it _can_ be done.

All this said, there are good reasons not to attach like this as it is certainly a bad coding habit. If your code ever references two objects of the same class, there might be confusion as to which one is attached as slots are not unique. Also, this relies on the class design and not on the underlying data. I would certainly (and do!) want to switch around my slots or classes without substantively rewriting my code. And lastly, if you ever had an object named x and a slot of an attached object named x then either the former would be rewritten, or you'd have to spend a great deal of time figuring out ways to properly scope your attached slots.

In short, don't do it.

Best,
Robert


-----Original Message-----
From: F.Tusell [mailto:etptupaf at bs.ehu.es] 
Sent: Tuesday, January 18, 2005 9:38 AM
To: R-help at stat.math.ethz.ch
Subject: [R] "Attach" for S4 objects?


When passing a list as an argument to a function, I find it convenient
to attach it in the first line of th function code, then refer to the
components as A, B, etc. rather than as list$A, list$B, etc.

If I pass a S4 class object, is there a way to "attach" it, or do I have
to refer to the slots as object at A, objetc at B, etc.? 

I could always make copies,  

     A <- object at A 
     B <- object at B
     ...

but "object" has many slots, some fairly large, so this seems wasteful.

Thank you.
-- 
Fernando TUSELL                                e-mail:
Departamento de Econometr?a y Estad?stica           etptupaf at bs.ehu.es 
Facultad de CC.EE. y Empresariales             Tel:   (+34)94.601.3733
Avenida Lendakari Aguirre, 83                  Fax:   (+34)94.601.3754
E-48015 BILBAO  (Spain)                        Secr:  (+34)94.601.3740

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Achim.Zeileis at wu-wien.ac.at  Tue Jan 18 18:31:58 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 18 Jan 2005 18:31:58 +0100
Subject: [R] chi-square and error bars?
In-Reply-To: <Pine.LNX.4.21.0501181706450.16945-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0501181706450.16945-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <20050118183158.43da1047.Achim.Zeileis@wu-wien.ac.at>

On Tue, 18 Jan 2005 17:16:46 +0000 (GMT) Dan Bolser wrote:

> 
> This may sound crazy but...
> 
> I have data like this...
> 
> > results.matrix
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
> [1,]  949   93    2   11   26   20    7    6   10     5     0     3
> [2,] 1233  124   24   35   58   57   17   21   31    19    11    21
> 
> 
> Which is the result of binning (summing) the response variables of an
> underlying (nearly) continious range of predictor variable into 12
> bins.
> 
> I can see that each row is not drawn from the same underlying
> population using chisq.test.
> 
> What I would like to do is plot each proportion from each row with a
> confidence interval which represents the results of the chisq.test,
> i.e. put confidence intervals on each proportion.
> 
> I thought about doing the arcsine transformation on the proportions
> and then I can use the binomial distribution to get the variance and
> hence 95% confidence interval (then back transformation).

You could also sample from the indepence table using r2dtable() and
derive simultanous confidence intervals from that.

Other visualizations of departures from indepence include mosaicplots
with shading or association plots. See mosaicplot() and assocplot() and
the package vcd.
Z
 
> Clearly confidence in latter points is weaker (fewer observations),
> but I can't seem to find a plot to capture this fact.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Tue Jan 18 18:35:26 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 18 Jan 2005 09:35:26 -0800 (PST)
Subject: [R] Function to modify existing data.frame
In-Reply-To: <BE1292EE.D0AC%peterm@andrew.cmu.edu>
References: <BE1292EE.D0AC%peterm@andrew.cmu.edu>
Message-ID: <Pine.A41.4.61b.0501180924500.212568@homer10.u.washington.edu>

On Tue, 18 Jan 2005, Peter Muhlberger wrote:

> I'm used to statistical languages, such as Stata, in which it's trivial to
> pass a list of variables to a function & have that function modify those
> variables in the existing dataset rather than create copies of the variables
> or having to replace the entire dataset to change a few variables.  In R, I
> suppose I could paste together the right instructions in a function and then
> execute it, but is there any more straightforward way of doing this I'm
> missing?
>

Yes and no.

This isn't so much a question of pass-by-reference, as one reply 
suggested, but of macros vs functions.

Stata is (largely) a macro language: it does operations on command strings 
and then evaluates them.  It's not that Stata programs work with 
references, it's that all objects (except local macros) are global.

  R is based on functions: it evaluates arguments and then operates on 
them.  When you have functions, with local variables, it then becomes 
relevant to ask whether the arguments to the function are just copies or 
are references to the real thing. In R they are just copies (from a 
language point of view) but are often references from an efficiency point 
of view.


There are two ways to get the effect you are looking for. I don't 
recommend either, though.


1) Store your variables in an environment, rather than a data frame. 
Environments are passed by reference.

2) The right combinations of eval() and substitute() and lazy evaluation 
make it possible to write macros in R.  There's an R Newsletter article 
about this.

 	-thomas



From stephane.dray at umontreal.ca  Tue Jan 18 18:37:54 2005
From: stephane.dray at umontreal.ca (Stephane Dray)
Date: Tue, 18 Jan 2005 12:37:54 -0500
Subject: [R] Off topic - Combine results of two testing procedures
Message-ID: <5.2.1.1.0.20050118121653.03a78dd8@magellan.umontreal.ca>

Hello list,
I have a question not directly linked to R, I have googling during one day 
but did not find any satisfying answer. Perhaps some people of the list 
could give me some advices.

Here is my problem, I compute a statistic which measure the link between 
three tables X (n,p), Y(p,m) and Z(n,q). I would like to conduct a test the 
hypothesis  that X,Y and Z are linked.

I have implemented two testing procedures (by monte-carlo permutation) 
which test:

Ho: X,Y and Z are not linked vs. H1: at least X and Z are linked
Ho: X,Y and Z are not linked vs. H2: at least X and Y are linked


I would like to combine probabilites from these two tests in order to test
Ho: X,Y and Z are not linked vs. H3: X ,Y and Z are linked

I can not implement a direct permutation procedure to test Ho vs H3, and 
wonder if there is one way to deduce it from the two first testing procedures.

Help and reference are welcome,

Thanks in advance,
Sincerely.


St?phane DRAY
-------------------------------------------------------------------------------------------------- 

D?partement des Sciences Biologiques
Universit? de Montr?al, C.P. 6128, succursale centre-ville
Montr?al, Qu?bec H3C 3J7, Canada

Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From slist at oomvanlieshout.net  Tue Jan 18 18:51:11 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Tue, 18 Jan 2005 19:51:11 +0200
Subject: [R] embedding fonts in eps files
In-Reply-To: <1106072852.1829.6.camel@gbic04>
References: <1106072852.1829.6.camel@gbic04>
Message-ID: <41ED4C8F.4060000@oomvanlieshout.net>

Rudi,

If it turns out that fonts can not be embedded with R, then one option 
is to import/export the file through CorelDraw (or other vector drawing 
software equivalent). The 'export to eps' function in CorelDraw provides 
an option to embed all the fonts!

It requires manual labour, but it will work. You will be able to embed 
any font available in Corel Draw: fonts galore!!

Sander.

Rudi Alberts wrote:
> Hi,
> 
> I have to make eps files with fonts embedded. 
> I use the following postscript command:
> 
> 
> postscript("fig3a.eps", width = 5.2756, height = 7.27, pointsize =
> 7,horizontal = FALSE, onefile = FALSE, paper = "special",family =
> "Times")
> 
> plot(...)
> 
> dev.off()
> 
> 
> Are fonts automatically embedded in this way?
> How can I see that?
> If not, how to do it?
> 
> 
> regards, Rudi.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From slist at oomvanlieshout.net  Tue Jan 18 19:40:49 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Tue, 18 Jan 2005 20:40:49 +0200
Subject: [R] Time line plot in R? -- one more problem
In-Reply-To: <20050118121510.MYPS18660.smta08.mail.ozemail.net@there>
References: <41EBBDDD.3040207@oomvanlieshout.net>
	<20050118121510.MYPS18660.smta08.mail.ozemail.net@there>
Message-ID: <41ED5831.9000900@oomvanlieshout.net>

Jim,

Inspired by the question about font embedding, I plotted the time line 
script to a postscript file. To my disappointment, I can not make the 
time line appear properly on the postscript graph. It seems that the 
device does not know I have plotted something new below the original graph!?

Any suggesting how to resize the graph to plot the time line correctly 
in postscript?

Thanks,

Sander.

Jim Lemon wrote:
> Sander Oom wrote:
> 
>>Dear R users,
>>
>>In order to illustrate the possible effects of events on variables
>>plotted against time, I would like plot a time line of events along side
>>the plot of the variables.
>>
>>The x-axis should be some time unit; the y-axis should be the variable
>>of interest; the time line should be plotted below the graph along the
>>same x-axis scale.
>>
>>As I have many variables and different events lists, I would like to
>>write a script to read the events from a file and plot them together
>>with the other plot.
>>
>>The time line should look something like this:
>>http://www.oslis.k12.or.us/secondary/howto/organize/images/timeline.gif
>>
> 
> Here's one way:
> 
> # some fake data please, maestro
> fakedata<-sample(1:10,10)
> # leave a bit of extra space beneath and to the left of the plot
> par(mar=c(6,6,4,2))
> # this function will probably end up in the "plotrix" package
> time.line<-function(x,y,at=NULL,labels=TRUE,tlticks=NULL,...) {
>  if(missing(x) && missing (y))
>   stop("Usage: time.line(x,y,at=NULL,labels=TRUE)")
>  plotrange<-par("usr")
>  # get the graphical parameters
>  oldpar<-par(no.readonly=TRUE)
>  # turn off clipping
>  par(xpd=TRUE)
>  if(missing(x)) {
>   # it's a horizontal line
>   segments(plotrange[1],y,plotrange[2],y,...)
>   ticklen<-(plotrange[4]-plotrange[3])*0.02
>   if(!is.null(tlticks))
>    segments(tlticks,y+ticklen,tlticks,y-ticklen,...)
>   mwidth<-strwidth("M")
>   # blank out the line where labels will appear
>   rect(at-mwidth,y-ticklen,at+mwidth,y+ticklen,col="white",border=FALSE)
>  # rotate the text
>   par(srt=90)
>   # draw the labels
>   text(at,y,labels,...)
>  }
>  if(missing(y)) {
>   # it's a vertical line
>   # draw the line
>   segments(x,plotrange[3],x,plotrange[4],...)
>   ticklen<-(plotrange[2]-plotrange[1])*0.02
>   if(!is.null(tlticks))
>    segments(x+ticklen,tlticks,x-ticklen,tlticks,...)
>   mheight<-strheight("M")
>   # blank out the line where labels will appear
>   rect(x-ticklen,at-mheight,x+ticklen,at+mheight,col="white",border=FALSE)
>   # draw the labels
>   text(x,at,labels,...)
>  }
>  # restore the parameters
>  par(oldpar)
> }
> # create a file with the positions and labels you want like this:
> # 2.5,first
> # 4,second
> # 7,third
> # 8.5,fourth
> # call it "labels.txt" and read it in
> tl.labels<-read.table("labels.txt",sep=",")
> plot(fakedata,xlab="")
> # display a horizontal time line
> time.line(x=-1,at=tl.labels[[1]],labels=tl.labels[[2]],col="black")
> # now a vertical one
> time.line(y=-1,at=tl.labels[[1]],labels=tl.labels[[2]],col="black")
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Tue Jan 18 20:32:46 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 18 Jan 2005 19:32:46 +0000 (GMT)
Subject: [R] embedding fonts in eps files
In-Reply-To: <41ED2A06.9070102@statistik.uni-dortmund.de>
References: <XFMail.050118114858.Ted.Harding@nessie.mcc.ac.uk>
	<41ED2A06.9070102@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.61.0501181928340.6952@gannet.stats>

On Tue, 18 Jan 2005, Uwe Ligges wrote:

> To clarify why people might want to embed fonts, I'd like to give an example:
>
> After I submitted my book the first time, the printing house asked me 
> explicitly to embed *all* fonts in graphics, or they would not be able to 
> print from a pdf file (generated from PostScript by certain styles for Adobe 
> Distiller). And sometimes discussions are simply hopeless.....

Agreed.  So I got Distiller to embed them.  For that you need a valid 
licence for the font you use for the application you have (in Uwe's and my 
case, printing a book on someone else's printer).

Let me refer people again to the help file, which does contain all the 
information required, evne if it has not been read.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From br44114 at yahoo.com  Tue Jan 18 20:57:48 2005
From: br44114 at yahoo.com (bogdan romocea)
Date: Tue, 18 Jan 2005 11:57:48 -0800 (PST)
Subject: [R] help wanted using R in a classroom
Message-ID: <20050118195748.76244.qmail@web50308.mail.yahoo.com>

It appears you wouldn't get much improvement at all even if the 2nd CPU
were used at 100%. Five R sessions can easily overwhelm one CPU. I
think you need (a lot) more CPUs than 2 to solve your problem.

Possible solutions:
1. Install R on each eMac. Since you have 40 of them, you might want to
put together a script to do this.
2. Get some boxes that can run Windows. On Windows, you can run R from
a CD/zip drive/USB drive. (So you could burn 40 CDs and have everyone
run their R session on their box.) As far as I know the same is not
true for GNU/Linux and Mac OS.

HTH,
b.


-----Original Message-----
From: Sam Parvaneh
Sent: Monday, January 17, 2005 6:11 AM
To: r-help at stat.math.ethz.ch
Subject: [R] help wanted using R in a classroom


Hi everyone!

I'm using R 2.0.1 for Mac OS X in a classroom with 40 eMacs running Mac
OS 
X version 10.3.6.
These Macs are network based, meaning that the students log in to an 
XServe G4 where their user accounts and home directories are stored.

The problem that I'm having each time a group of students (usually 7 to

10) use R is that the whole system get incredibly slow.
The response time for opening an application  while the students are 
running R is around 5 minutes.
If a student wants to log into the system while others are running R,
it 
can take up to 10 minutes for the student to get logged in.
Everything gets very slow that it's almost impossible to work.
When I look at the server Graphs, the CPU usage of the first CPU is
always 
100% when these students are using R. The second CPU is left at 15%. 

When these students quit R, then everything's is back to normal again.
The 
usage of both CPUs go back down to between 5-10%.
Is there anyone out there using R in a university like this?
Does anyone have an idea what this might depend one or maybe a
solution?
I can provide some more information if anyone wants, if you think you
can 
help me.

Thanks in advance
/Sam


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Tue Jan 18 21:13:36 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 18 Jan 2005 15:13:36 -0500
Subject: [R] help wanted using R in a classroom
Message-ID: <3A822319EB35174CA3714066D590DCD50994E54D@usrymx25.merck.com>

> From: bogdan romocea
> 
> It appears you wouldn't get much improvement at all even if 
> the 2nd CPU
> were used at 100%. Five R sessions can easily overwhelm one CPU. I
> think you need (a lot) more CPUs than 2 to solve your problem.
> 
> Possible solutions:
> 1. Install R on each eMac. Since you have 40 of them, you 
> might want to
> put together a script to do this.
> 2. Get some boxes that can run Windows. On Windows, you can run R from
> a CD/zip drive/USB drive. (So you could burn 40 CDs and have everyone
> run their R session on their box.) As far as I know the same is not
> true for GNU/Linux and Mac OS.

If one can run Linux off a (live) CD, why wouldn't it be possible to do the
same for R?  (Burn a Quantian DVD and you can boot a x86 box from it to run
R.)

If I'm not mistaken, it shouldn't be a problem running R off a USB pen drive
or CD under Linux.  I can give it a shot tonight (if I can find the time).

Andy

 
> HTH,
> b.
> 
> 
> -----Original Message-----
> From: Sam Parvaneh
> Sent: Monday, January 17, 2005 6:11 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] help wanted using R in a classroom
> 
> 
> Hi everyone!
> 
> I'm using R 2.0.1 for Mac OS X in a classroom with 40 eMacs 
> running Mac
> OS 
> X version 10.3.6.
> These Macs are network based, meaning that the students log in to an 
> XServe G4 where their user accounts and home directories are stored.
> 
> The problem that I'm having each time a group of students 
> (usually 7 to
> 
> 10) use R is that the whole system get incredibly slow.
> The response time for opening an application  while the students are 
> running R is around 5 minutes.
> If a student wants to log into the system while others are running R,
> it 
> can take up to 10 minutes for the student to get logged in.
> Everything gets very slow that it's almost impossible to work.
> When I look at the server Graphs, the CPU usage of the first CPU is
> always 
> 100% when these students are using R. The second CPU is left at 15%. 
> 
> When these students quit R, then everything's is back to normal again.
> The 
> usage of both CPUs go back down to between 5-10%.
> Is there anyone out there using R in a university like this?
> Does anyone have an idea what this might depend one or maybe a
> solution?
> I can provide some more information if anyone wants, if you think you
> can 
> help me.
> 
> Thanks in advance
> /Sam
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jtk at cmp.uea.ac.uk  Tue Jan 18 22:20:27 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Tue, 18 Jan 2005 21:20:27 +0000
Subject: [R] Function to modify existing data.frame
In-Reply-To: <Pine.A41.4.61b.0501180924500.212568@homer10.u.washington.edu>
References: <BE1292EE.D0AC%peterm@andrew.cmu.edu>
	<Pine.A41.4.61b.0501180924500.212568@homer10.u.washington.edu>
Message-ID: <20050118212027.GF8758@jtkpc.cmp.uea.ac.uk>

On Tue, Jan 18, 2005 at 09:35:26AM -0800, Thomas Lumley wrote:
> On Tue, 18 Jan 2005, Peter Muhlberger wrote:
> 
> >I'm used to statistical languages, such as Stata, in which it's trivial to
> >pass a list of variables to a function & have that function modify those
> >variables in the existing dataset rather than create copies of the 
> >variables
> >or having to replace the entire dataset to change a few variables.  In R, I
> >suppose I could paste together the right instructions in a function and 
> >then
> >execute it, but is there any more straightforward way of doing this I'm
> >missing?

> There are two ways to get the effect you are looking for. I don't 
> recommend either, though.

This leaves me wondering which way you actually would recommend? After
all, one cannot entirely recommend against the concept of a function
(sensu lato, perhaps) which modifies a few members of a (possibly
very large) object in place, without copying the entire object in the
process.

> 1) Store your variables in an environment, rather than a data frame. 
> Environments are passed by reference.

This is the approach used in the OOP package -- so, while implementing
the data set as an object in this way is probably substantially "more"
than what Peter asked for, wouldn't this be an interesting option to
at least look into?

> 2) The right combinations of eval() and substitute() and lazy evaluation 
> make it possible to write macros in R.  There's an R Newsletter article 
> about this.

Ok, this refers to the "defmacro" technique, if I dereference this call
by reference correctly  ;-)

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From tghoward at gw.dec.state.ny.us  Tue Jan 18 21:36:49 2005
From: tghoward at gw.dec.state.ny.us (Tim Howard)
Date: Tue, 18 Jan 2005 15:36:49 -0500
Subject: [R] Dropping a digit with scan() on a connection
Message-ID: <s1ed2d23.027@gwsmtp.DEC.STATE.NY.US>

R gurus,

My use of scan() seems to be dropping the first digit of sequential
scans on a connection. It looks like it happens only within a line:

> cat("TITLE extra line", "235 335 535 735", "115 135 175",
file="ex.data", sep="\n")
> cn.x <- file("ex.data", open="r")
> a <- scan(cn.x, skip=1, n=2)
Read 2 items
> a
[1] 235 335
> b <- scan(cn.x, n=2)
Read 2 items
> b
[1]  35 735
> c <- scan(cn.x, n=2)
Read 2 items
> c
[1] 115 135
> d <- scan(cn.x, n=1)
Read 1 items
> d
[1] 75
> 

Note in b, I should get 535, not 35 as the first value. In d, I should
get 175.  Does anyone know how to get these digits?

The reason I'm not scanning the entire file at once is that my real
dataset is much larger than a Gig and I'll need to pull only portions of
the file in at once. I got readLines to work, but then I have to figure
out how to convert each entire line into a data.frame. Scan seems a lot
cleaner, with the exception of the funny character dropping issue.

Thanks so much!
Tim Howard



From martin.julien.2 at courrier.uqam.ca  Tue Jan 18 21:42:04 2005
From: martin.julien.2 at courrier.uqam.ca (Martin Julien)
Date: Tue, 18 Jan 2005 15:42:04 -0500
Subject: [R] Randoms interactions in lme
Message-ID: <200501182037.j0IKbuhH009267@intrant.uqam.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050118/a0eb7d1d/attachment.pl

From spamsa at cnlab.ch  Tue Jan 18 22:46:08 2005
From: spamsa at cnlab.ch (spamsa@cnlab.ch)
Date: Tue, 18 Jan 2005 22:46:08 +0100
Subject: [R] address statistics
Message-ID: <ASTERIX711HZv7FDwfC00000c5b@mail.cnlab.ch>

Guten Tag

Im Rahmen einer Studienarbeit an der Hochschule fuer Technik in Rapperswil (HSR), zum Thema SPAM, haben wir 
Ihre E-Mail Adresse auf einer CD-ROM gefunden welche bei EBay gehandelt wird (Adresshandel).
Wir haben insbesondere alle E-Mail Adressen von Schweizer Unis und ETHs herausgesucht.

Nun ist es fuer unsere Statistik wichtig, dass wir wissen welche der gefundenen Adressen noch aktiv sind.
Aus diesem Grund senden wir Ihnen diese E-Mail.

Wichtig: Sie sollen darauf nicht Antworten, es geht nur darum herauszufinden ob die E-Mail Adresse noch existiert,
also diese E-Mail nicht rejected wird.


Fuer weitere Infos:
http://stud.ita.hsr.ch/sw04/sw0403/



Mit freundlichen Gruessen

Ch. Hoehn und A. Ruoss


<<-----english------->>


Hello


Within the scope of a studying work at the "Hochschule fuer Technik in Rapperswil" (HSR), with the subject SPAM,
have we found your E-Mail Address on a CD-ROM which is sold at EBay.
We searched on this CD-ROM for E-Mail addresses from Universities and ETHs in switzerland.

It's important for our statistics, that we know which of the addresses we found, is still active.
This is the reason for this E-Mail.

Important: You shouldn't give an answer to this message. If this message isn't rejected, we will think it's an
active address.


For more infos:
http://stud.ita.hsr.ch/sw04/sw0403/


Sincerely yours

Ch. Hoehn and A. Ruoss



From gunter.berton at gene.com  Tue Jan 18 23:16:36 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 18 Jan 2005 14:16:36 -0800
Subject: [R] Randoms interactions in lme
In-Reply-To: <200501182037.j0IKbuhH009267@intrant.uqam.ca>
Message-ID: <200501182216.j0IMGaF2012324@compton.gene.com>

Julien:

Big difference! You need to spend time with Bates and Pinheiro to understand
the concepts, but in brief:

~1|x/y  means that x and y are grouping variables with y nested within x and
a different random offset for each x and y within x.

~y|x means that x is a grouping variable and y is a linear covariate with a
different random (intercept and) slope for each x group.


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Martin Julien
> Sent: Tuesday, January 18, 2005 12:42 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Randoms interactions in lme
> 
> In lme, what's the difference between "random = ~ 1 | x / y" 
> and "random = ~
> y | x"     ?
> 
>  
> 
> Thanks
> 
> Julien
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jdressel at surromed.com  Tue Jan 18 23:55:21 2005
From: jdressel at surromed.com (Jon Dressel)
Date: Tue, 18 Jan 2005 14:55:21 -0800
Subject: [R] R: error while loading shared libraries: libg2c.so.o
Message-ID: <9D33C6169B1FDB419767356B8A8FEDB5010872CF@lynx.corp.surromed.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050118/dcb7ab05/attachment.pl

From HDoran at air.org  Wed Jan 19 00:28:53 2005
From: HDoran at air.org (Doran, Harold)
Date: Tue, 18 Jan 2005 18:28:53 -0500
Subject: [R] Data Simulation in R
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7407449EF3@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050118/8d4ebb40/attachment.pl

From peterm at andrew.cmu.edu  Wed Jan 19 00:55:17 2005
From: peterm at andrew.cmu.edu (Peter Muhlberger)
Date: Tue, 18 Jan 2005 18:55:17 -0500
Subject: [R] a question about linear mixed model in R 
Message-ID: <BE130C15.D145%peterm@andrew.cmu.edu>

Hi Chung Cheng:  This seems related to a problem I'm having in some data of
mine as well.  I'm new to R (played w/ it some a year ago) & to lme
modeling, so take this w/ a grain of salt, but here are some thoughts:

In my problem, D would be an indicator of whether a subject was in the
control condition or not.  In the control condition, all people participated
individually, in the experimental condition there was small-group based
discussion.  r(ij) would be some variable that affects the outcome, but
whose effect may be moderated by the group the discussion was in.

The model assumes that the non-control condition values will have a
distribution of coefficients for r(ij).  The coefficient for r(ij) in the
controls need not have the same central value as for the non-controls
(though it would be nice to be able constrain it so it would be).  So, it
might make some sense to split the variable into two variables, one with
zeros for the controls & one w/ zeros for the experimental groups and
estimate the former w/ random effects & the other not.

I'm not 100% sure that's what you're asking, but it seems related.

Peter

>Dear all,
>
>I have a somewhat unusual linear mixed model that I can't seem
>to code in lme.  It's only unusual in that one random effect is
>applied only to some of the observations (I have an indicator
>variable
>that specifies which observations have this random effect).
>
>The model is:
>
>X_hijk = alpha_h + h * b_i + r_(ij) + e_hijk , where
>
>  h = 0 or 1 (indicator)
>  i = 1, ..., N
>  j = 1, ..., n_i
>  k = 1, ..., K
>alpha is fixed, and the rest are random.
>I'm willing to assume b, r, and e are mutually independent
>and normal with var(b) = sigma^2_b, var(r) = sigma^2_r, and
>var(e) = sigma^2.
>
>Any help in writing this model in lme() would be greatly
>appreciated.
>
>Thanks,
>
>Chung Cheng



From ockham at gmx.net  Wed Jan 19 01:49:42 2005
From: ockham at gmx.net (ockham@gmx.net)
Date: Wed, 19 Jan 2005 01:49:42 +0100
Subject: [R] signif() generic
Message-ID: <1106095782.29183.26.camel@localhost>

Dear list, 

I'm trying to write a class for Gaussian error propagation of measured
values and their (estimated) errors,

> setClass("sec", representation(val="numeric", err="numeric"))

I've already successfully implemented basic arithmetics using mostly the
"Arith" group generics. But I'm running into trouble when trying to get
signif() to work for my purposes, i.e. with a default argument
(digits=2): When trying 

> seMethod("signif", signature(x="sec", digits="numeric")), 
	function(x, digits=2){
	# ...do something...
	}
)

and 

> signif(sec1)

I get

> Error in signif(x, digits) : Non-numeric argument to mathematical
function

Putting a second argument (like digits=2) into the call makes it work,
but I want some default behavior specified for missing digits argument
so it works in an analogous fashion as signif for numeric values. 
I also tried inserting 

> setGeneric("signif", function(x, digits=6) standardGeneric("signif")) 

before the setMethod block, but that wouldn't help either. I'm pretty
clueless after having studied most of the material in the documentation
of the methods package and single functions concerning generic behavior
-- somehow I don't see what I'm doing wrong. 
Any help would be greatly appreciated,

Greetings, 
Ockham



From Robert.McGehee at geodecapital.com  Wed Jan 19 02:13:37 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Tue, 18 Jan 2005 20:13:37 -0500
Subject: [R] signif() generic
Message-ID: <67DCA285A2D7754280D3B8E88EB5480206741E22@MSGBOSCLB2WIN.DMN1.FMR.COM>

The method below looks for arguments of "sec" and "numeric". Sounds like
you want to pass in "sec" and "missing" (the class of missing data) and
get a default behavior. So in addition to the method you already wrote,
try this one as well:

setMethod("signif", signature(x = "sec", digits = "missing"),
	function(x, digits) {
		digits <- 2
		## .. do something
		print(digits)
		}
)

Now we get the default behavior:
> signif(new("sec"))
[1] 2

Robert


-----Original Message-----
From: ockham at gmx.net [mailto:ockham at gmx.net] 
Sent: Tuesday, January 18, 2005 7:50 PM
To: r-help at stat.math.ethz.ch
Subject: [R] signif() generic


Dear list, 

I'm trying to write a class for Gaussian error propagation of measured
values and their (estimated) errors,

> setClass("sec", representation(val="numeric", err="numeric"))

I've already successfully implemented basic arithmetics using mostly the
"Arith" group generics. But I'm running into trouble when trying to get
signif() to work for my purposes, i.e. with a default argument
(digits=2): When trying 

> seMethod("signif", signature(x="sec", digits="numeric")), 
	function(x, digits=2){
	# ...do something...
	}
)

and 

> signif(sec1)

I get

> Error in signif(x, digits) : Non-numeric argument to mathematical
function

Putting a second argument (like digits=2) into the call makes it work,
but I want some default behavior specified for missing digits argument
so it works in an analogous fashion as signif for numeric values. 
I also tried inserting 

> setGeneric("signif", function(x, digits=6) standardGeneric("signif")) 

before the setMethod block, but that wouldn't help either. I'm pretty
clueless after having studied most of the material in the documentation
of the methods package and single functions concerning generic behavior
-- somehow I don't see what I'm doing wrong. 
Any help would be greatly appreciated,

Greetings, 
Ockham

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From mcclatchie.sam at saugov.sa.gov.au  Wed Jan 19 02:52:41 2005
From: mcclatchie.sam at saugov.sa.gov.au (McClatchie, Sam (PIRSA-SARDI))
Date: Wed, 19 Jan 2005 12:22:41 +1030
Subject: [R] looking for a basic spatial diff function
Message-ID: <032A8573186A2B4EBBAEFA5784D0523502BCB2D4@sagemsg0007.sagemsmrd01.sa.gov.au>

Background:
OS: Linux Mandrake 10.1
release: R 2.0.0
editor: GNU Emacs 21.3.2
front-end: ESS 5.2.3
---------------------------------

Colleagues

Is there a function to calculate distances between adjacent latitude/
longitude pairs in a matrix? It is basically a spatial diff() function that
I have in mind.

Data:

long1, lat1
long2, lat2
long3, lat3

looking for: diff(data$long, data$lat)

function Result

long1, lat1
long2, lat2, distance = long/lat2 - long/lat1
long3, lat3, distance = long/lat3 - long/lat2

I've probably missed it but a search of the maillist archives and quick scan
of the packages (map*) did not solve my query. For this simple task I don't
really want to get into GRASS.

Best fishes

Sam
----
Sam McClatchie,
Sub-program leader, Pelagic Fisheries
South Australian Aquatic Sciences Centre
PO Box 120, Henley Beach 5022
Adelaide, South Australia
email <mcclatchie.sam at saugov.sa.gov.au>
Telephone: (61-8) 8207 5448
FAX: (61-8) 8200 2481
Research home page <http://www.members.iinet.net.au/~s.mcclatchie/>
  
                   /\
      ...>><xX(?> 
                //// \\\\
                   <?)Xx><<
              /////  \\\\\\
                        ><(((?> 
  >><(((?>   ...>><xX(?>O<?)Xx><<



From YiYao_Jiang at smics.com  Wed Jan 19 03:12:07 2005
From: YiYao_Jiang at smics.com (YiYao_Jiang)
Date: Wed, 19 Jan 2005 10:12:07 +0800
Subject: [R] how to call R in delphi? 
Message-ID: <E0B8EBA2EBD83347A588FC4840F8C9E90666BC@ex115.smic-sh.com>

Dear All:

Now I am writing  program in delphi , found it is very convenience to do anova, T-test, F-Test, etc in R , how to call R in delphi? Thsnks.




Best Regards

YiYao Jiang 

Product Division/ product Testing Department
Semiconductor Manufacturing International Corporation
ZhangJiang Road, PuDong New Area, Shanghai  ZIP: 201203
Tel:86-21-5080-2000 Ext. 15173



From Tom.Mulholland at dpi.wa.gov.au  Wed Jan 19 04:31:41 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Wed, 19 Jan 2005 11:31:41 +0800
Subject: [R] how to call R in delphi? 
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3C968@afhex01.dpi.wa.gov.au>

Try searching this list for delphi. This was the first post I found. You may find it helpful.

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/19154.html

Tom

> -----Original Message-----
> From: YiYao_Jiang [mailto:YiYao_Jiang at smics.com]
> Sent: Wednesday, 19 January 2005 10:12 AM
> To: r-help at stat.math.ethz.ch
> Cc: Jack_Kang; Ivy_Li
> Subject: [R] how to call R in delphi? 
> 
> 
> Dear All:
> 
> Now I am writing  program in delphi , found it is very 
> convenience to do anova, T-test, F-Test, etc in R , how to 
> call R in delphi? Thsnks.
> 
> 
> 
> 
> Best Regards
> 
> YiYao Jiang 
> 
> Product Division/ product Testing Department
> Semiconductor Manufacturing International Corporation
> ZhangJiang Road, PuDong New Area, Shanghai  ZIP: 201203
> Tel:86-21-5080-2000 Ext. 15173
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tiago17 at socrates.Berkeley.EDU  Wed Jan 19 05:25:00 2005
From: tiago17 at socrates.Berkeley.EDU (Tiago R Magalhaes)
Date: Wed, 19 Jan 2005 04:25:00 +0000
Subject: [R] importing files, columns "invade next column"
Message-ID: <p06100500be136a7f5ed6@[83.132.29.8]>

Dear R-listers:

I want to import a reasonably big file into a table. (15797 x 257 
columns). The file is tab delimited with NA in every empty space. I 
have reproduced what I have used as my read.table instruction. I have 
read the R-dataImportExport FAQ and still couldn't solve my problem. 
(I might have missed it, of course). I'm using R.2.01 in a Mac G4, 
10.3.7.

I can import the file, but one of the columns "invades the other", 
meaning that the if there is an empty space marked as NA on the first 
column, it gets the value of the second column. I tried to import 
four different files (details below) and I think the problem is with 
the number of columns (with less columns it works)

workarounds:
a) I can separate my file into several files, import them and then 
make one file in R
b) try to learn basic commands in awk? perl?
any advice on this?

another question (much less important) I have a binnary file in Splus 
for this object. I exported the object in Splus as it says in the FAQ 
(dump.data). But data.restore doesn't exist as a function. Is it 
because I'm using a Mac?

details of what I did:
##
a) importing a shorter version of my file (58 columns); I get the 
"invading" behaviour and a column of row.names that I don't 
understand where it comes from. (UNIQID should be empty and 1006 
should be in All.FB.Id

>  AllFBImpFields <- read.table('AllFBAllFieldsNAShorter.txt', fill=T, header=T,
+                              row.names=paste('a',1:15797, sep=''),
+                              as.is=T, nrows=15797)
>  AllFBImpFields[1:2,1:5]
    row.names UNIQID All.FB.Id All.FB.5 All.FB.4
a1      <NA>  10006      <NA>     <NA>     <NA>
a2      <NA>  10007      <NA>     <NA>     <NA>

##
b) Importing only 5 cols of the previous file. It works. there is no 
"invasion" and the col row.names is not inserted

>  AllFB5Cols <- read.table('AllFB5Cols.txt', fill=T, header=T,
+                          row.names=paste('a',1:15797, sep=''),
+                          as.is=T, nrows=15797)
>  AllFB5Cols[1:2,1:5]
    UNIQID All.FB.Id Symbol       FB.gn CG.name
a1   <NA>     10006    p53 FBgn0039044 CG10873
a2   <NA>     10007  Gr94a FBgn0041225 CG31280

##
c) importing file with 4 rows, 58 columns; invasion behaviour and a 
warning that I don't get in a) although the file is the same for the 
first 4 rows

>  x4rowsAllCol <- read.table('AllFB4rowsAllCols.txt', fill=T, header=T,
+                            row.names=paste('a',1:4, sep=''),
+                            as.is=T, nrows=4)
Warning message:
incomplete final line found by readTableHeader on `AllFB4rowsAllCols.txt'
>  x4rowsAllCol[1:2,1:5]
    row.names UNIQID All.FB.Id All.FB.5 All.FB.4
a1        NA  10006        NA       NA       NA
a2        NA  10007        NA       NA       NA

##
d) importing file with 4 rows and 4 cols, result is like b) but gives 
the same warning as c!)
>  x4rows5cols <- read.table('AllFB4rows5cols.txt', fill=T, header=T,
+                      row.names=paste('a',1:4, sep=''),
+                      as.is=T, nrows=4)
Warning message:
incomplete final line found by readTableHeader on `AllFB4rows5cols.txt'
>  x4rows5cols[1:2,1:5]
    UNIQID All.FB.Id All.FB.5 All.FB.4 All.FB.3
a1     NA     10006       NA       NA       NA
a2     NA     10007       NA       NA       NA



From vograno at evafunds.com  Wed Jan 19 05:56:01 2005
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Tue, 18 Jan 2005 20:56:01 -0800
Subject: [R] recursive penalized regression
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A57D901C@phost015.EVAFUNDS.intermedia.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050118/b1cc7859/attachment.pl

From MSchwartz at MedAnalytics.com  Wed Jan 19 06:42:23 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 18 Jan 2005 23:42:23 -0600
Subject: [R] importing files, columns "invade next column"
In-Reply-To: <p06100500be136a7f5ed6@[83.132.29.8]>
References: <p06100500be136a7f5ed6@[83.132.29.8]>
Message-ID: <1106113343.28502.13.camel@horizons.localdomain>

On Wed, 2005-01-19 at 04:25 +0000, Tiago R Magalhaes wrote:
> Dear R-listers:
> 
> I want to import a reasonably big file into a table. (15797 x 257 
> columns). The file is tab delimited with NA in every empty space. 

Tiago,

Have you tried to use read.table() explicitly defining the field
delimiting character as a tab to see if that changes anything?

Try the following:

AllFBImpFields <- read.table('AllFBAllFieldsNAShorter.txt',
                              header = TRUE,
                              row.names=paste('a',1:15797, sep=''),
                              as.is = TRUE,
                              sep = "\t")

I added the 'sep = "\t"' argument at the end.

Also, leave out the 'fill = TRUE', which can cause problems. You do not
need this unless your source file has a varying number of fields per
line.

Note that you do not need to specify the 'nrows' argument unless you
generally want something less than all of the rows. Using the
combination of 'skip' and 'nrows', you can read a subset of rows from
the middle of the input file.

See if that helps. Usually when there are column alignment problems, it
is because the rows are not being consistently parsed into fields, which
is frequently the result of not having the proper delimiting character
specified.

The last thought is to be sure that a '#' is not in your data file. This
is interpreted as a comment character by default, which means that
anything after it on a row will be ignored.

HTH,

Marc Schwartz



From ripley at stats.ox.ac.uk  Wed Jan 19 08:53:57 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 19 Jan 2005 07:53:57 +0000 (GMT)
Subject: [R] help wanted using R in a classroom
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E54D@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E54D@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.61.0501190737090.18036@gannet.stats>

On Tue, 18 Jan 2005, Liaw, Andy wrote:

>> From: bogdan romocea
>>
>> It appears you wouldn't get much improvement at all even if
>> the 2nd CPU
>> were used at 100%. Five R sessions can easily overwhelm one CPU. I
>> think you need (a lot) more CPUs than 2 to solve your problem.
>>
>> Possible solutions:
>> 1. Install R on each eMac. Since you have 40 of them, you
>> might want to
>> put together a script to do this.
>> 2. Get some boxes that can run Windows. On Windows, you can run R from
>> a CD/zip drive/USB drive. (So you could burn 40 CDs and have everyone
>> run their R session on their box.) As far as I know the same is not
>> true for GNU/Linux and Mac OS.
>
> If one can run Linux off a (live) CD, why wouldn't it be possible to do the
> same for R?  (Burn a Quantian DVD and you can boot a x86 box from it to run
> R.)
>
> If I'm not mistaken, it shouldn't be a problem running R off a USB pen drive
> or CD under Linux.  I can give it a shot tonight (if I can find the time).

There is.  The script `R' used to run R contains the absolute path to the 
R_HOME_DIR, e.g.

R_HOME_DIR=/usr/local/lib/R
if test -n "${R_HOME}" && \
    test "${R_HOME}" != "${R_HOME_DIR}"; then
   echo "WARNING: ignoring environment value of R_HOME"
fi
R_HOME="${R_HOME_DIR}"
export R_HOME

and so you would have to mount a removable drive on the exact place it was 
written for.  I am not sure that is possible with multiple USB ports.

I believe R's MacOS X port is written to be always in one place in the 
file system, and that is encoded by e.g.

     RLAPACK_LDFLAGS="-install_name \$(Rexeclibdir)/libRlapack.dylib"


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jan 19 09:18:43 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 19 Jan 2005 08:18:43 +0000 (GMT)
Subject: [R] R: error while loading shared libraries: libg2c.so.o
In-Reply-To: <9D33C6169B1FDB419767356B8A8FEDB5010872CF@lynx.corp.surromed.com>
References: <9D33C6169B1FDB419767356B8A8FEDB5010872CF@lynx.corp.surromed.com>
Message-ID: <Pine.LNX.4.61.0501190757180.18036@gannet.stats>

On Tue, 18 Jan 2005, Jon Dressel wrote:

> I have just installed the version of R for Linux Enterprise Server AS.

Is the `RedHat  Linux Enterprise Server AS'?  AFAIK, there is no version 
of R for any specific RHEL, but there are versions like 2.0.1.
If you installed an RPM, please tell us so, and which one from where.

> When R is launched, I receive the following error message:

> R: error while loading shared libraries: libg2c.so.o cannot open shared 
> object file: no such file or directory

I very much suspect it is libg2c.so.0.

> I have added the path to the library

It should be in /usr/lib if you have the prerequisites installed.
It may be that you have libg2c.so.0 there, linked to a non-existent 
file.

> above to the /usr/etc/ld.so.conf file and then run ldconfig -v. The 
> listing then shows that the libg2c.so.o is included in the path. 
> Please advise.

You need to know where R's home is.  Normally R RHOME will tell you, but 
that might give the same error.  So use

grep '^R_HOME_DIR' `which R`

and then substitute it in

R CMD ldd R_HOME_DIR/bin/exec/R

My FC3 system with gcc-3.4.3 and readline-5.0 installed from source gives

gannet% R CMD ldd /usr/local/lib/R/bin/exec/R
         libblas.so.3 => /usr/lib/libblas.so.3 (0x0094e000)
         libg2c.so.0 => /usr/local/lib/libg2c.so.0 (0xb7fc0000)
         libm.so.6 => /lib/tls/libm.so.6 (0x00b4f000)
         libgcc_s.so.1 => /usr/local/lib/libgcc_s.so.1 (0xb7fb7000)
         libreadline.so.5 => /usr/local/lib/libreadline.so.5 (0xb7f8d000)
         libdl.so.2 => /lib/libdl.so.2 (0x00948000)
         libncurses.so.5 => /usr/lib/libncurses.so.5 (0x04c8b000)
         libc.so.6 => /lib/tls/libc.so.6 (0x0081c000)
         /lib/ld-linux.so.2 (0x00803000)

and then see if the place it resolves libg2c.so.0 to is an actual file.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From buser at stat.math.ethz.ch  Wed Jan 19 09:20:32 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Wed, 19 Jan 2005 09:20:32 +0100
Subject: [R] Dropping a digit with scan() on a connection
In-Reply-To: <s1ed2d23.027@gwsmtp.DEC.STATE.NY.US>
References: <s1ed2d23.027@gwsmtp.DEC.STATE.NY.US>
Message-ID: <16878.6224.312005.303195@stat.math.ethz.ch>

Dear Tim

You can use

cat("TITLE extra line", "235 335 535 735", "115 135 175", file="ex.data", sep="\n")
cn.x <- file("ex.data", open="r")

a <- scan(cn.x, skip=1, n=2, sep = " ")
> Read 2 items
a
> [1] 235 335
b <- scan(cn.x, n=2, sep = " ")
> Read 2 items
b
> [1] 535 735
c <- scan(cn.x, n=2, sep = " ")
> Read 2 items
c
> [1] 115 135
d <- scan(cn.x, n=1, sep = " ")
> Read 1 items
d
> [1] 175

Regards,

Christoph Buser

-- 
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C11
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-1-632-5414		fax: 632-1228
http://stat.ethz.ch/~buser/


Tim Howard writes:
 > R gurus,
 > 
 > My use of scan() seems to be dropping the first digit of sequential
 > scans on a connection. It looks like it happens only within a line:
 > 
 > > cat("TITLE extra line", "235 335 535 735", "115 135 175",
 > file="ex.data", sep="\n")
 > > cn.x <- file("ex.data", open="r")
 > > a <- scan(cn.x, skip=1, n=2)
 > Read 2 items
 > > a
 > [1] 235 335
 > > b <- scan(cn.x, n=2)
 > Read 2 items
 > > b
 > [1]  35 735
 > > c <- scan(cn.x, n=2)
 > Read 2 items
 > > c
 > [1] 115 135
 > > d <- scan(cn.x, n=1)
 > Read 1 items
 > > d
 > [1] 75
 > > 
 > 
 > Note in b, I should get 535, not 35 as the first value. In d, I should
 > get 175.  Does anyone know how to get these digits?
 > 
 > The reason I'm not scanning the entire file at once is that my real
 > dataset is much larger than a Gig and I'll need to pull only portions of
 > the file in at once. I got readLines to work, but then I have to figure
 > out how to convert each entire line into a data.frame. Scan seems a lot
 > cleaner, with the exception of the funny character dropping issue.
 > 
 > Thanks so much!
 > Tim Howard
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Jan 19 09:25:02 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 19 Jan 2005 08:25:02 +0000 (GMT)
Subject: [R] importing files, columns "invade next column"
In-Reply-To: <p06100500be136a7f5ed6@[83.132.29.8]>
References: <p06100500be136a7f5ed6@[83.132.29.8]>
Message-ID: <Pine.LNX.4.61.0501190821120.18036@gannet.stats>

On Wed, 19 Jan 2005, Tiago R Magalhaes wrote:

> another question (much less important) I have a binnary file in Splus for 
> this object. I exported the object in Splus as it says in the FAQ 
> (dump.data).

Whose FAQ?  data.dump is not mentioned in the R FAQ.

> But data.restore doesn't exist as a function. Is it because I'm using a 
> Mac?

It is in package foreign: please consult the `R Data Import/Export 
Manual'.  There are details you need to follow, including loading package 
foreign.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jan 19 09:42:57 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 19 Jan 2005 08:42:57 +0000 (GMT)
Subject: [R] Dropping a digit with scan() on a connection
In-Reply-To: <s1ed2d23.027@gwsmtp.DEC.STATE.NY.US>
References: <s1ed2d23.027@gwsmtp.DEC.STATE.NY.US>
Message-ID: <Pine.LNX.4.61.0501190839420.19161@gannet.stats>

This is because scan() has a private pushback.
Either:

1) Read the file a whole line at a time: I cannot see why you need to do 
so here nor in your sketched application.

or

2) Use an explicit separator, e.g. " " in your example.

scan() is not designed to read parts of lines of a file,


On Tue, 18 Jan 2005, Tim Howard wrote:

> R gurus,
>
> My use of scan() seems to be dropping the first digit of sequential
> scans on a connection. It looks like it happens only within a line:
>
>> cat("TITLE extra line", "235 335 535 735", "115 135 175",
> file="ex.data", sep="\n")
>> cn.x <- file("ex.data", open="r")
>> a <- scan(cn.x, skip=1, n=2)
> Read 2 items
>> a
> [1] 235 335
>> b <- scan(cn.x, n=2)
> Read 2 items
>> b
> [1]  35 735
>> c <- scan(cn.x, n=2)
> Read 2 items
>> c
> [1] 115 135
>> d <- scan(cn.x, n=1)
> Read 1 items
>> d
> [1] 75
>>
>
> Note in b, I should get 535, not 35 as the first value. In d, I should
> get 175.  Does anyone know how to get these digits?
>
> The reason I'm not scanning the entire file at once is that my real
> dataset is much larger than a Gig and I'll need to pull only portions of
> the file in at once. I got readLines to work, but then I have to figure
> out how to convert each entire line into a data.frame. Scan seems a lot
> cleaner, with the exception of the funny character dropping issue.
>
> Thanks so much!
> Tim Howard
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Roger.Bivand at nhh.no  Wed Jan 19 10:05:56 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 19 Jan 2005 10:05:56 +0100 (CET)
Subject: [R] looking for a basic spatial diff function
In-Reply-To: <032A8573186A2B4EBBAEFA5784D0523502BCB2D4@sagemsg0007.sagemsmrd
	01.sa.gov.au>
Message-ID: <Pine.LNX.4.44.0501190954270.2507-100000@reclus.nhh.no>

On Wed, 19 Jan 2005, McClatchie, Sam (PIRSA-SARDI) wrote:

> Background:
> OS: Linux Mandrake 10.1
> release: R 2.0.0
> editor: GNU Emacs 21.3.2
> front-end: ESS 5.2.3
> ---------------------------------
> 
> Colleagues
> 
> Is there a function to calculate distances between adjacent latitude/
> longitude pairs in a matrix? It is basically a spatial diff() function that
> I have in mind.
> 
> Data:
> 
> long1, lat1
> long2, lat2
> long3, lat3
> 
> looking for: diff(data$long, data$lat)
> 
> function Result
> 
> long1, lat1
> long2, lat2, distance = long/lat2 - long/lat1
> long3, lat3, distance = long/lat3 - long/lat2

A distance matrix can be got from the rdist.earth() function in the fields 
package:

> lon <- seq(-10,10,5)
> lat <- seq(-10,10,5)
> rdist.earth(cbind(lon, lat), miles=FALSE)
      
               [,1]      [,2]      [,3]      [,4]         [,5]
  [1,] 1.344145e-04  783.6868 1570.3395 2357.0091 3.140679e+03
  [2,] 7.836868e+02    0.0000  786.6784 1573.3568 2.357009e+03
  [3,] 1.570339e+03  786.6784    0.0000  786.6784 1.570339e+03
  [4,] 2.357009e+03 1573.3568  786.6784    0.0000 7.836868e+02
  [5,] 3.140679e+03 2357.0091 1570.3395  783.6868 1.344145e-04

from which you need to get the values below the diagonal.


> 
> I've probably missed it but a search of the maillist archives and quick scan
> of the packages (map*) did not solve my query. For this simple task I don't
> really want to get into GRASS.
> 
> Best fishes
> 
> Sam
> ----
> Sam McClatchie,
> Sub-program leader, Pelagic Fisheries
> South Australian Aquatic Sciences Centre
> PO Box 120, Henley Beach 5022
> Adelaide, South Australia
> email <mcclatchie.sam at saugov.sa.gov.au>
> Telephone: (61-8) 8207 5448
> FAX: (61-8) 8200 2481
> Research home page <http://www.members.iinet.net.au/~s.mcclatchie/>
>   
>                    /\
>       ...>><xX(?> 
>                 //// \\\\
>                    <?)Xx><<
>               /////  \\\\\\
>                         ><(((?> 
>   >><(((?>   ...>><xX(?>O<?)Xx><<
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From murdoch at stats.uwo.ca  Wed Jan 19 10:10:18 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 19 Jan 2005 09:10:18 +0000
Subject: [R] how to call R in delphi? 
In-Reply-To: <E0B8EBA2EBD83347A588FC4840F8C9E90666BC@ex115.smic-sh.com>
References: <E0B8EBA2EBD83347A588FC4840F8C9E90666BC@ex115.smic-sh.com>
Message-ID: <en8su09gr858sffdu1n87ldfik9dekm0ku@4ax.com>

On Wed, 19 Jan 2005 10:12:07 +0800, "YiYao_Jiang"
<YiYao_Jiang at smics.com> wrote :

>Dear All:
>
>Now I am writing  program in delphi , found it is very convenience to do anova, T-test, F-Test, etc in R , how to call R in delphi? Thsnks.

Tom referred you to a post about the COM interface.  

You can also do some of what you want with direct calls to R.dll.  See
the "R API: entry points for C code" section of the Writing R
Extensions manual.  If you're unfamiliar with calling C entries from
Delphi, my web page
<http://www.stats.uwo.ca/faculty/murdoch/software/compilingDLLs/>
might help, though it is aimed at writing DLLs, rather than using
R.dll.  

But that's another option:  let R be in charge, and just add some
functions in a DLL written in Delphi.

Duncan Murdoch



From etptupaf at bs.ehu.es  Wed Jan 19 10:35:05 2005
From: etptupaf at bs.ehu.es (F.Tusell)
Date: Wed, 19 Jan 2005 09:35:05 +0000
Subject: [R] recursive penalized regression
Message-ID: <1106127305.3722.4.camel@agesi.bs.ehu.es>

Seems to me that your requirements 1 and 2 could be met by casting the
autoregressive model in state-space form and using the Kalman filter.
I am not sure about the kind of regularization that you want to  apply
to model coefficients, but it could also likely be accommodated: see
Durbin and Koopman "Time Series Analysis by State Space Methods", sec.
6.5.

Best, ft.
-- 
Fernando TUSELL                                e-mail:
Departamento de Econometr?a y Estad?stica           etptupaf at bs.ehu.es 
Facultad de CC.EE. y Empresariales             Tel:   (+34)94.601.3733
Avenida Lendakari Aguirre, 83                  Fax:   (+34)94.601.3754
E-48015 BILBAO  (Spain)                        Secr:  (+34)94.601.3740



From ramesh_k77 at yahoo.com  Wed Jan 19 11:06:44 2005
From: ramesh_k77 at yahoo.com (kolluru ramesh)
Date: Wed, 19 Jan 2005 02:06:44 -0800 (PST)
Subject: [R] Imputation missing observations
Message-ID: <20050119100644.59112.qmail@web41407.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050119/cf16858f/attachment.pl

From Philippe.Hupe at curie.fr  Wed Jan 19 11:12:34 2005
From: Philippe.Hupe at curie.fr (=?ISO-8859-1?Q?Philippe_Hup=E9?=)
Date: Wed, 19 Jan 2005 11:12:34 +0100
Subject: [R] * creating vignettes ... ERROR
Message-ID: <41EE3292.9030006@curie.fr>

Hi,

I have the following error when building a package

* creating vignettes ... ERROR

Error:  chunk 3
Error in pmatch(x, table, duplicates.ok) :
        argument is not of mode character
Error in buildVignettes(dir = ".") : Error:  chunk 3
Error in pmatch(x, table, duplicates.ok) :
        argument is not of mode character
Execution halted



I use R-2.0.1 under debian linux 2.6.9

Thanks for any idea

Philippe

-- 
Philippe Hup?
UMR 144 - Service Bioinformatique
Institut Curie
Laboratoire de Transfert (4?me ?tage)
26 rue d'Ulm
75005 Paris - France
 	
Email :  Philippe.Hupe at curie.fr
T?l :	 +33 (0)1 44 32 42 75
Fax :  	 +33 (0)1 42 34 65 28



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Jan 19 11:22:58 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 19 Jan 2005 11:22:58 +0100
Subject: [R] Imputation missing observations
References: <20050119100644.59112.qmail@web41407.mail.yahoo.com>
Message-ID: <002e01c4fe10$d8d16570$0540210a@www.domain>

from your mail is not clear if you have loaded the Hmisc package, try

library(Hmisc)
?aregImpute
f <- aregImpute(~y + x1 + x2 + x3, n.impute=100, data=d)

If this doesn't work, try to download a fresh copy of the package from 
CRAN.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "kolluru ramesh" <ramesh_k77 at yahoo.com>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, January 19, 2005 11:06 AM
Subject: [R] Imputation missing observations


>
>>From Internet I downloaded the file Hmisc.zip and used it for R 
>>package updation. and R gave the message 'Hmisc' successfull 
>>unpacked.
>
> But when I use the functions like aregImpute the package is 
> displaying coundn't find the function
>
> Where as in help.search it is giving that use of the function
>
>> install.packages(choose.files('',filters=Filters[c('zip','All'),]), 
>> .libPaths()[1], CRAN = NULL)
>
> package 'Hmisc' successfully unpacked and MD5 sums checked
>
> updating HTML package descriptions
>
>> ?aregImpute
>
> Error in help("aregImpute") : No documentation for `aregImpute' in 
> specified packages and libraries:
>
>  you could try `help.search("aregImpute")'
>
>> f <- aregImpute(~y + x1 + x2 + x3, n.impute=100, data=d)
>
> Error: couldn't find function "aregImpute"
>
>
>
>
> ---------------------------------
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Wed Jan 19 11:47:04 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 19 Jan 2005 11:47:04 +0100
Subject: [R] * creating vignettes ... ERROR
In-Reply-To: <41EE3292.9030006@curie.fr>
References: <41EE3292.9030006@curie.fr>
Message-ID: <41EE3AA8.6070104@statistik.uni-dortmund.de>

Philippe Hup? wrote:

> Hi,
> 
> I have the following error when building a package
> 
> * creating vignettes ... ERROR
> 
> Error:  chunk 3
> Error in pmatch(x, table, duplicates.ok) :
>        argument is not of mode character
> Error in buildVignettes(dir = ".") : Error:  chunk 3
> Error in pmatch(x, table, duplicates.ok) :
>        argument is not of mode character
> Execution halted
> 
> 
> 
> I use R-2.0.1 under debian linux 2.6.9
> 
> Thanks for any idea
> 
> Philippe
> 

So what about looking for the mode of x and table, and after that fixing 
your code?
We do not know anything about your code, your objects and your aims.

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Wed Jan 19 11:51:27 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 19 Jan 2005 11:51:27 +0100
Subject: [R] Data Simulation in R
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7407449EF3@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7407449EF3@dc1ex2.air.org>
Message-ID: <41EE3BAF.8040501@statistik.uni-dortmund.de>

Doran, Harold wrote:

> Dear List:
> 
> A few weeks ago I posted some questions regarding data simulation and
> received some very helpful comments, thank you. I have modified my code
> accordingly and have made some progress. 
> 
> However, I now am facing a new challenge along similar lines. I am
> attempting to simulate 250 datasets and then run the data through a
> linear model. I use rm() and gc() as I move along to clean up the
> workspace and preserve memory. However, my aim is to use sample sizes of
> 5,000 and 10,000. By any measure this is a huge task.
> 
> My machine has 2GB RAM and a Pentium 4 2.8 GHz machine with Windows XP.
> I have the following in the "target" section of the Windows shortcut
> --max-mem-size=1812M
> 
> With such large samples, R is unable to perform the analysis, at least
> with the code I have developed. It halts when it runs out of memory. A
> collegue subsequently constructed the simulation using another software
> program with a similar computer and, while it took over night (and then
> some), the program produced the results desired.
> 
> I am curious if it is the case that such large simulations are out of
> the grasp of R or if my code is not adequately organized to perform the
> simulation. 
> 
> I would appreciate any thoughts or advice.


Don't hold all datasets (and results, if they are big) in the memory at 
the same time!!!

Either generate them when you use them and delete them afterwards,
or save them to disc an only load one by one for further analyses.
Also, you might want to call gc() after you removed large objects...

Uwe Ligges



> Harold
> 
> 
> 
> library(MASS)
> library(nlme)
> mu<-c(100,150,200,250)
> Sigma<-matrix(c(400,80,80,80,80,400,80,80,80,80,400,80,80,80,80,400),4,4
> )
> mu2<-c(0,0,0)
> Sigma2<-diag(64,3)
> sample.size<-5000
> N<-250 #Number of datasets
> #Take a single draw from VL distribution
> vl.error<-mvrnorm(n=N, mu2, Sigma2)
> 
> #Step 1 Create Data
> Data <- lapply(seq(N), function(x)
> as.data.frame(cbind(1:10,mvrnorm(n=sample.size, mu, Sigma))))
> 
> #Step 2 Add Vertical Linking Error
> for(i in seq(along=Data)){
> Data[[i]]$V6 <- Data[[i]]$V2
> Data[[i]]$V7 <- Data[[i]]$V3 + vl.error[i,1] 
> Data[[i]]$V8 <- Data[[i]]$V4 + vl.error[i,2]
> Data[[i]]$V9 <- Data[[i]]$V5 + vl.error[i,3] 
> }
> 
> #Step 3 Restructure for Longitudinal Analysis
> long <- lapply(Data, function(x) reshape(x, idvar="Data[[i]]$V1",
> varying=list(c(names(Data[[i]])[2:5]),c(names(Data[[i]])[6:9])),
> v.names=c("score.1","score.2"), direction="long"))
> 
> #####################
> #Clean up Workspace
> rm(Data,vl.error) 
> gc()
> #####################
> 
> # Step 4 Run GLS
> 
> glsrun1 <- lapply(long, function(x) gls(score.1~I(time-1), data=x, 
> correlation=corAR1(form=~1|V1), method='ML'))
> 
> # Extract intercepts and slopes 
> int1 <- sapply(glsrun1, function(x) x$coefficient[1])
> slo1 <- sapply(glsrun1, function(x) x$coefficient[2])
> 
> ################
> #Clean up workspace
> rm(glsrun1)
> gc()
> 
> glsrun2 <- lapply(long, function(x) gls(score.2~I(time-1), data=x, 
> correlation=corAR1(form=~1|V1), method='ML')) 
> 
> # Extract intercepts and slopes 
> int2 <- sapply(glsrun2, function(x) x$coefficient[1])
> slo2 <- sapply(glsrun2, function(x) x$coefficient[2])
> 
>  
> #Clean up workspace
> rm(glsrun2)
> gc()
> 
> 
> 
> # Print Results
> 
> cat("Original Standard Errors","\n", "Intercept","\t",
> sd(int1),"\n","Slope","\t","\t", sd(slo1),"\n")
> 
> cat("Modified Standard Errors","\n", "Intercept","\t",
> sd(int2),"\n","Slope","\t","\t", sd(slo2),"\n")
> 
> rm(list=ls())
> gc() 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From news-and-lists at the-me.de  Wed Jan 19 12:03:01 2005
From: news-and-lists at the-me.de (Axel Bock)
Date: Wed, 19 Jan 2005 12:03:01 +0100
Subject: [R] help with hist-plots?
Message-ID: <41EE3E65.9020409@the-me.de>

Hi, I am working my way through the examples given in the R manual, and I am 
now at the hist-plot of the eruptions data set.

The question I have is as follows: I truly like the
	hist(thing, seq(a,b,c), prob=TRUE)
thingy, but ... I would rather plot that with gnuplot than with R. (First, it 
looks nicer :-), second, all my other graphs are in gnuplot, third, all my 
scripts work with gnuplot).

Now can I somehow get the data pairs created by this command into a file? That 
would be really nice ... !


Thanks for help & greetings,

Axel.



From ligges at statistik.uni-dortmund.de  Wed Jan 19 12:06:24 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 19 Jan 2005 12:06:24 +0100
Subject: [R] background color for plotting symbols in 'matplot'
In-Reply-To: <41ED35FB.4090905@fz-rossendorf.de>
References: <3A822319EB35174CA3714066D590DCD50994E547@usrymx25.merck.com>	<x2sm4y4xq3.fsf@biostat.ku.dk>
	<41ED35FB.4090905@fz-rossendorf.de>
Message-ID: <41EE3F30.9070602@statistik.uni-dortmund.de>

joerg van den hoff wrote:

> something like
> 
> 
> matplot2(matrix(1:6,3,2),matrix(7:12,3,2),pch=21,bg=c(2,3),type='b')

Where can we find "matplot2"?


> does not yield the expected (at least by me) result: only the points on 
> the first line get (successively) background colors for the plotting 
> symbols, the second line gets no background color at all for its 
> plotting symbols.
> 
> I think the natural behaviour should be two curves which (for the 
> example given above) symbol-background colors 2 and 3, respectively (as 
> would be obtained by a suitable manual combination of 'plot' and 
> 'lines'). the modification of the matplot code to achieve this behaviour 
> is obvious as far as I can see (adding 'bg' to the explicit arguments of 
> matplot and handling similar to 'lty', 'cex' and the like inside the 
> function including transfer to 'plot' and 'lines' argument list).
> 
> 
> is the present behaviour a bug of 'matplot' or is it for some reason 
> intended behaviour?

The real point is that you might want to mark by rows *or* by columns, 
so it's not that easy to specify a sensible default behaviour, at least 
one has to think about it.

If you want to implement it for all possible arguments, the well known 
problem of huge number of arguments springs to mind as well.

Since you say the "modification [...] is obvious": I think R-core 
welcomes your contribution.

Uwe Ligges



> 
> regards,
> 
> joerg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From dieter.menne at menne-biomed.de  Wed Jan 19 12:18:38 2005
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Wed, 19 Jan 2005 11:18:38 +0000 (UTC)
Subject: [R] how to call R in delphi?
References: <E0B8EBA2EBD83347A588FC4840F8C9E90666BC@ex115.smic-sh.com>
Message-ID: <loom.20050119T121646-759@post.gmane.org>

To call R from Delphi, you may try 
http://www.menne-biomed.de/download/RDComDelphi.zip.

  Example program showing use of R from Delphi.
  Connecting to R via COM using Neuwirth's StatConnectorSrvLib 
  Uses RCom.pas, which is a simple Delphi wrapper for passing
  commands, integer and double arrays.
  See http://cran.r-project.org/contrib/extra/dcom
  By:  dieter.menne at menne-biomed.de



From murdoch at stats.uwo.ca  Wed Jan 19 12:43:11 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 19 Jan 2005 11:43:11 +0000
Subject: [R] help with hist-plots?
In-Reply-To: <41EE3E65.9020409@the-me.de>
References: <41EE3E65.9020409@the-me.de>
Message-ID: <uohsu09e85s6t5g38fa2sr0ciossch7glj@4ax.com>

On Wed, 19 Jan 2005 12:03:01 +0100, Axel Bock
<news-and-lists at the-me.de> wrote :

>Hi, I am working my way through the examples given in the R manual, and I am 
>now at the hist-plot of the eruptions data set.
>
>The question I have is as follows: I truly like the
>	hist(thing, seq(a,b,c), prob=TRUE)
>thingy, but ... I would rather plot that with gnuplot than with R. (First, it 
>looks nicer :-), second, all my other graphs are in gnuplot, third, all my 
>scripts work with gnuplot).
>
>Now can I somehow get the data pairs created by this command into a file? That 
>would be really nice ...

When you call hist, it returns a list of things that are necessary to
reproduce the plot.  Try printing it using print(hist( ... )), and you
should be able to see which component contains the numbers you want to
send to gnuplot.

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Wed Jan 19 13:02:31 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 19 Jan 2005 13:02:31 +0100
Subject: [R] help with hist-plots?
In-Reply-To: <41EE3E65.9020409@the-me.de>
References: <41EE3E65.9020409@the-me.de>
Message-ID: <41EE4C57.3060306@statistik.uni-dortmund.de>

Axel Bock wrote:

> Hi, I am working my way through the examples given in the R manual, and 
> I am now at the hist-plot of the eruptions data set.
> 
> The question I have is as follows: I truly like the
>     hist(thing, seq(a,b,c), prob=TRUE)
> thingy, but ... I would rather plot that with gnuplot than with R. 
> (First, it looks nicer :-), second, all my other graphs are in gnuplot, 
> third, all my scripts work with gnuplot).
> 
> Now can I somehow get the data pairs created by this command into a 
> file? That would be really nice ... !

Example:

  histogramObject <- hist(1:10, plot=FALSE)
  histogramObject


Uwe Ligges



> 
> Thanks for help & greetings,
> 
> Axel.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From j.van_den_hoff at fz-rossendorf.de  Wed Jan 19 13:03:40 2005
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Wed, 19 Jan 2005 13:03:40 +0100
Subject: [R] background color for plotting symbols in 'matplot'
In-Reply-To: <41EE3F30.9070602@statistik.uni-dortmund.de>
References: <3A822319EB35174CA3714066D590DCD50994E547@usrymx25.merck.com>	<x2sm4y4xq3.fsf@biostat.ku.dk>
	<41ED35FB.4090905@fz-rossendorf.de>
	<41EE3F30.9070602@statistik.uni-dortmund.de>
Message-ID: <41EE4C9C.70700@fz-rossendorf.de>

thanks for the response.


Uwe Ligges wrote:
> joerg van den hoff wrote:
> 
>> something like
>>
>>
>> matplot2(matrix(1:6,3,2),matrix(7:12,3,2),pch=21,bg=c(2,3),type='b')
> 
> 
> Where can we find "matplot2"?
oops. that should have been 'matplot' (not 'matplot2'), of course.
> 
> 
>> does not yield the expected (at least by me) result: only the points 
>> on the first line get (successively) background colors for the 
>> plotting symbols, the second line gets no background color at all for 
>> its plotting symbols.
>>
>> I think the natural behaviour should be two curves which (for the 
>> example given above) symbol-background colors 2 and 3, respectively 
>> (as would be obtained by a suitable manual combination of 'plot' and 
>> 'lines'). the modification of the matplot code to achieve this 
>> behaviour is obvious as far as I can see (adding 'bg' to the explicit 
>> arguments of matplot and handling similar to 'lty', 'cex' and the like 
>> inside the function including transfer to 'plot' and 'lines' argument 
>> list).
>>
>>
>> is the present behaviour a bug of 'matplot' or is it for some reason 
>> intended behaviour?
> 
> 
> The real point is that you might want to mark by rows *or* by columns, 
> so it's not that easy to specify a sensible default behaviour, at least 
> one has to think about it.
I'm aware of this: any specific behaviour could be the 'best' default 
for someone. in terms of consistency, I would argue that matplot plots 
"columns of x against columns of y", so these columns should be 
addressed. that is how 'lty' and 'pch' and 'cex' do it. the present 
behaviour of 'bg' ('bg' interpreted only for "column 1 of x against 
column 1 of y") is not sensible.

> 
> If you want to implement it for all possible arguments, the well known 
> problem of huge number of arguments springs to mind as well.
that is indeed a problem, but I think mainly when reading the help 
pages, which then are cluttered with many not often used graphic parameters.
> 
> Since you say the "modification [...] is obvious": I think R-core 
> welcomes your contribution.
well, I'm not a fluent R programmer. I'm not sure if the simple minded 
modification of 'matplot' would be welcome by R-core. rather, I attach 
here the modified code 'matplot2' (sic!), if someone wants to use it. a 
'diff' vs. the original versions shows easily the few modified lines.

joerg

> 
> Uwe Ligges
> 
> 
> 
>>
>> regards,
>>
>> joerg
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> 
> 




-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: matplot2.R
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050119/02cbf05b/matplot2.pl

From HDoran at air.org  Wed Jan 19 13:36:33 2005
From: HDoran at air.org (Doran, Harold)
Date: Wed, 19 Jan 2005 07:36:33 -0500
Subject: [R] Data Simulation in R
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7407449F0B@dc1ex2.air.org>

Thanks. But, I think I am doing that. I use rm() and gc() as the code
moves along. The datasets are stored as a list. Is there a way that I
can save the list object and call each dataset within a list one at a
time, or must the entire list be in memory at once?

Harold

-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
Sent: Wednesday, January 19, 2005 5:51 AM
To: Doran, Harold
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Data Simulation in R

Doran, Harold wrote:

> Dear List:
> 
> A few weeks ago I posted some questions regarding data simulation and 
> received some very helpful comments, thank you. I have modified my 
> code accordingly and have made some progress.
> 
> However, I now am facing a new challenge along similar lines. I am 
> attempting to simulate 250 datasets and then run the data through a 
> linear model. I use rm() and gc() as I move along to clean up the 
> workspace and preserve memory. However, my aim is to use sample sizes 
> of 5,000 and 10,000. By any measure this is a huge task.
> 
> My machine has 2GB RAM and a Pentium 4 2.8 GHz machine with Windows
XP.
> I have the following in the "target" section of the Windows shortcut 
> --max-mem-size=1812M
> 
> With such large samples, R is unable to perform the analysis, at least

> with the code I have developed. It halts when it runs out of memory. A

> collegue subsequently constructed the simulation using another 
> software program with a similar computer and, while it took over night

> (and then some), the program produced the results desired.
> 
> I am curious if it is the case that such large simulations are out of 
> the grasp of R or if my code is not adequately organized to perform 
> the simulation.
> 
> I would appreciate any thoughts or advice.


Don't hold all datasets (and results, if they are big) in the memory at
the same time!!!

Either generate them when you use them and delete them afterwards, or
save them to disc an only load one by one for further analyses.
Also, you might want to call gc() after you removed large objects...

Uwe Ligges



> Harold
> 
> 
> 
> library(MASS)
> library(nlme)
> mu<-c(100,150,200,250)
> Sigma<-matrix(c(400,80,80,80,80,400,80,80,80,80,400,80,80,80,80,400),4
> ,4
> )
> mu2<-c(0,0,0)
> Sigma2<-diag(64,3)
> sample.size<-5000
> N<-250 #Number of datasets
> #Take a single draw from VL distribution vl.error<-mvrnorm(n=N, mu2, 
> Sigma2)
> 
> #Step 1 Create Data
> Data <- lapply(seq(N), function(x)
> as.data.frame(cbind(1:10,mvrnorm(n=sample.size, mu, Sigma))))
> 
> #Step 2 Add Vertical Linking Error
> for(i in seq(along=Data)){
> Data[[i]]$V6 <- Data[[i]]$V2
> Data[[i]]$V7 <- Data[[i]]$V3 + vl.error[i,1]
> Data[[i]]$V8 <- Data[[i]]$V4 + vl.error[i,2]
> Data[[i]]$V9 <- Data[[i]]$V5 + vl.error[i,3] }
> 
> #Step 3 Restructure for Longitudinal Analysis long <- lapply(Data, 
> function(x) reshape(x, idvar="Data[[i]]$V1", 
> varying=list(c(names(Data[[i]])[2:5]),c(names(Data[[i]])[6:9])),
> v.names=c("score.1","score.2"), direction="long"))
> 
> #####################
> #Clean up Workspace
> rm(Data,vl.error)
> gc()
> #####################
> 
> # Step 4 Run GLS
> 
> glsrun1 <- lapply(long, function(x) gls(score.1~I(time-1), data=x, 
> correlation=corAR1(form=~1|V1), method='ML'))
> 
> # Extract intercepts and slopes
> int1 <- sapply(glsrun1, function(x) x$coefficient[1])
> slo1 <- sapply(glsrun1, function(x) x$coefficient[2])
> 
> ################
> #Clean up workspace
> rm(glsrun1)
> gc()
> 
> glsrun2 <- lapply(long, function(x) gls(score.2~I(time-1), data=x, 
> correlation=corAR1(form=~1|V1), method='ML'))
> 
> # Extract intercepts and slopes
> int2 <- sapply(glsrun2, function(x) x$coefficient[1])
> slo2 <- sapply(glsrun2, function(x) x$coefficient[2])
> 
>  
> #Clean up workspace
> rm(glsrun2)
> gc()
> 
> 
> 
> # Print Results
> 
> cat("Original Standard Errors","\n", "Intercept","\t", 
> sd(int1),"\n","Slope","\t","\t", sd(slo1),"\n")
> 
> cat("Modified Standard Errors","\n", "Intercept","\t", 
> sd(int2),"\n","Slope","\t","\t", sd(slo2),"\n")
> 
> rm(list=ls())
> gc()
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From tghoward at gw.dec.state.ny.us  Wed Jan 19 13:42:24 2005
From: tghoward at gw.dec.state.ny.us (Tim Howard)
Date: Wed, 19 Jan 2005 07:42:24 -0500
Subject: [R] Dropping a digit with scan() on a connection
Message-ID: <s1ee0f6d.087@gwsmtp.DEC.STATE.NY.US>

Thank you Dr. Ripley and Christoph Buser for your explanations and
help.

Using sep = " "  within scan worked within lines of my file, but then I
gained an NA record when wrapping from one line to the next (because the
linebreak character is no longer recognized as a sep?).  So, I'll
continue by ensuring each group I read ends at the end of a line (as
scan was designed), and by using scan without the sep option.

FYI, Here's how the NA showed up, each line is 800 numbers long:

>test4 <- scan(cn.test, n=1600, sep = " ")
>test5 <- scan(cn.test, n=1600)
>test4[797:803]
[1]  81.00000  81.08746  81.89484  82.00000        NA 580.09030
576.90300
> test5[797:803]
[1]  81.01944  81.62060  81.96495  82.00000  82.00000 567.91840
563.10470

Thanks again.
Tim


>>> Prof Brian Ripley <ripley at stats.ox.ac.uk> 01/19/05 03:42AM >>>
This is because scan() has a private pushback.
Either:

1) Read the file a whole line at a time: I cannot see why you need to
do 
so here nor in your sketched application.

or

2) Use an explicit separator, e.g. " " in your example.

scan() is not designed to read parts of lines of a file,


On Tue, 18 Jan 2005, Tim Howard wrote:

> R gurus,
>
> My use of scan() seems to be dropping the first digit of sequential
> scans on a connection. It looks like it happens only within a line:
>
>> cat("TITLE extra line", "235 335 535 735", "115 135 175",
> file="ex.data", sep="\n")
>> cn.x <- file("ex.data", open="r")
>> a <- scan(cn.x, skip=1, n=2)
> Read 2 items
>> a
> [1] 235 335
>> b <- scan(cn.x, n=2)
> Read 2 items
>> b
> [1]  35 735
>> c <- scan(cn.x, n=2)
> Read 2 items
>> c
> [1] 115 135
>> d <- scan(cn.x, n=1)
> Read 1 items
>> d
> [1] 75
>>
>
> Note in b, I should get 535, not 35 as the first value. In d, I
should
> get 175.  Does anyone know how to get these digits?
>
> The reason I'm not scanning the entire file at once is that my real
> dataset is much larger than a Gig and I'll need to pull only portions
of
> the file in at once. I got readLines to work, but then I have to
figure
> out how to convert each entire line into a data.frame. Scan seems a
lot
> cleaner, with the exception of the funny character dropping issue.
>
> Thanks so much!
> Tim Howard
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html 
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk 
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/ 
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Wed Jan 19 13:49:26 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 19 Jan 2005 13:49:26 +0100
Subject: [R] Data Simulation in R
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7407449F0B@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7407449F0B@dc1ex2.air.org>
Message-ID: <41EE5756.202@statistik.uni-dortmund.de>

Doran, Harold wrote:

> Thanks. But, I think I am doing that. I use rm() and gc() as the code
> moves along. The datasets are stored as a list. Is there a way that I
> can save the list object and call each dataset within a list one at a
> time, or must the entire list be in memory at once?

The list is in memory - and must be to access its elements.
Either save the list elements to separate files, or even better make use 
of a database.

Uwe Ligges




> Harold
> 
> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
> Sent: Wednesday, January 19, 2005 5:51 AM
> To: Doran, Harold
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Data Simulation in R
> 
> Doran, Harold wrote:
> 
> 
>>Dear List:
>>
>>A few weeks ago I posted some questions regarding data simulation and 
>>received some very helpful comments, thank you. I have modified my 
>>code accordingly and have made some progress.
>>
>>However, I now am facing a new challenge along similar lines. I am 
>>attempting to simulate 250 datasets and then run the data through a 
>>linear model. I use rm() and gc() as I move along to clean up the 
>>workspace and preserve memory. However, my aim is to use sample sizes 
>>of 5,000 and 10,000. By any measure this is a huge task.
>>
>>My machine has 2GB RAM and a Pentium 4 2.8 GHz machine with Windows
> 
> XP.
> 
>>I have the following in the "target" section of the Windows shortcut 
>>--max-mem-size=1812M
>>
>>With such large samples, R is unable to perform the analysis, at least
> 
> 
>>with the code I have developed. It halts when it runs out of memory. A
> 
> 
>>collegue subsequently constructed the simulation using another 
>>software program with a similar computer and, while it took over night
> 
> 
>>(and then some), the program produced the results desired.
>>
>>I am curious if it is the case that such large simulations are out of 
>>the grasp of R or if my code is not adequately organized to perform 
>>the simulation.
>>
>>I would appreciate any thoughts or advice.
> 
> 
> 
> Don't hold all datasets (and results, if they are big) in the memory at
> the same time!!!
> 
> Either generate them when you use them and delete them afterwards, or
> save them to disc an only load one by one for further analyses.
> Also, you might want to call gc() after you removed large objects...
> 
> Uwe Ligges
> 
> 
> 
> 
>>Harold
>>
>>
>>
>>library(MASS)
>>library(nlme)
>>mu<-c(100,150,200,250)
>>Sigma<-matrix(c(400,80,80,80,80,400,80,80,80,80,400,80,80,80,80,400),4
>>,4
>>)
>>mu2<-c(0,0,0)
>>Sigma2<-diag(64,3)
>>sample.size<-5000
>>N<-250 #Number of datasets
>>#Take a single draw from VL distribution vl.error<-mvrnorm(n=N, mu2, 
>>Sigma2)
>>
>>#Step 1 Create Data
>>Data <- lapply(seq(N), function(x)
>>as.data.frame(cbind(1:10,mvrnorm(n=sample.size, mu, Sigma))))
>>
>>#Step 2 Add Vertical Linking Error
>>for(i in seq(along=Data)){
>>Data[[i]]$V6 <- Data[[i]]$V2
>>Data[[i]]$V7 <- Data[[i]]$V3 + vl.error[i,1]
>>Data[[i]]$V8 <- Data[[i]]$V4 + vl.error[i,2]
>>Data[[i]]$V9 <- Data[[i]]$V5 + vl.error[i,3] }
>>
>>#Step 3 Restructure for Longitudinal Analysis long <- lapply(Data, 
>>function(x) reshape(x, idvar="Data[[i]]$V1", 
>>varying=list(c(names(Data[[i]])[2:5]),c(names(Data[[i]])[6:9])),
>>v.names=c("score.1","score.2"), direction="long"))
>>
>>#####################
>>#Clean up Workspace
>>rm(Data,vl.error)
>>gc()
>>#####################
>>
>># Step 4 Run GLS
>>
>>glsrun1 <- lapply(long, function(x) gls(score.1~I(time-1), data=x, 
>>correlation=corAR1(form=~1|V1), method='ML'))
>>
>># Extract intercepts and slopes
>>int1 <- sapply(glsrun1, function(x) x$coefficient[1])
>>slo1 <- sapply(glsrun1, function(x) x$coefficient[2])
>>
>>################
>>#Clean up workspace
>>rm(glsrun1)
>>gc()
>>
>>glsrun2 <- lapply(long, function(x) gls(score.2~I(time-1), data=x, 
>>correlation=corAR1(form=~1|V1), method='ML'))
>>
>># Extract intercepts and slopes
>>int2 <- sapply(glsrun2, function(x) x$coefficient[1])
>>slo2 <- sapply(glsrun2, function(x) x$coefficient[2])
>>
>> 
>>#Clean up workspace
>>rm(glsrun2)
>>gc()
>>
>>
>>
>># Print Results
>>
>>cat("Original Standard Errors","\n", "Intercept","\t", 
>>sd(int1),"\n","Slope","\t","\t", sd(slo1),"\n")
>>
>>cat("Modified Standard Errors","\n", "Intercept","\t", 
>>sd(int2),"\n","Slope","\t","\t", sd(slo2),"\n")
>>
>>rm(list=ls())
>>gc()
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
> 
>



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Jan 19 13:56:37 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 19 Jan 2005 13:56:37 +0100
Subject: [R] Data Simulation in R
References: <88EAF3512A55DF46B06B1954AEF73F7407449F0B@dc1ex2.air.org>
Message-ID: <002e01c4fe26$50758d30$0540210a@www.domain>

Hi Harold,

maybe you could try something like this:

int1 <- slo1 <- int2 <- slo2 <- numeric(N)
for(i in seq(N)){
    Data <- as.data.frame(cbind(1:10, mvrnorm(n=sample.size, mu, 
Sigma)))
    Data$V6 <- Data$V2
    Data$V7 <- Data$V3 + vl.error[i,1]
    Data$V8 <- Data$V4 + vl.error[i,2]
    Data$V9 <- Data$V5 + vl.error[i,3]
    long <- reshape(Data, idvar="Data$V1", 
varying=list(c(names(Data)[2:5]), c(names(Data)[6:9])),
                    v.names=c("score.1", "score.2"), direction="long")
    rm(Data); gc()
    ##
    fm1 <- gls(score.1~time-1, data=long, 
correlation=corAR1(form=~1|V1), method='ML')
    int1[i] <- fm1$coefficient[1]; slo1[i] <- fm1$coefficient[2]
    rm(fm1); gc()
    ##
    fm2 <- gls(score.2~time-1, data=long, 
correlation=corAR1(form=~1|V1), method='ML')
    int2[i] <- fm2$coefficient[1]; slo2[i] <- fm2$coefficient[2]
    rm(fm2, long); gc()
}

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Doran, Harold" <HDoran at air.org>
To: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>
Cc: <r-help at stat.math.ethz.ch>
Sent: Wednesday, January 19, 2005 1:36 PM
Subject: RE: [R] Data Simulation in R


> Thanks. But, I think I am doing that. I use rm() and gc() as the 
> code
> moves along. The datasets are stored as a list. Is there a way that 
> I
> can save the list object and call each dataset within a list one at 
> a
> time, or must the entire list be in memory at once?
>
> Harold
>
> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
> Sent: Wednesday, January 19, 2005 5:51 AM
> To: Doran, Harold
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Data Simulation in R
>
> Doran, Harold wrote:
>
>> Dear List:
>>
>> A few weeks ago I posted some questions regarding data simulation 
>> and
>> received some very helpful comments, thank you. I have modified my
>> code accordingly and have made some progress.
>>
>> However, I now am facing a new challenge along similar lines. I am
>> attempting to simulate 250 datasets and then run the data through a
>> linear model. I use rm() and gc() as I move along to clean up the
>> workspace and preserve memory. However, my aim is to use sample 
>> sizes
>> of 5,000 and 10,000. By any measure this is a huge task.
>>
>> My machine has 2GB RAM and a Pentium 4 2.8 GHz machine with Windows
> XP.
>> I have the following in the "target" section of the Windows 
>> shortcut
>> --max-mem-size=1812M
>>
>> With such large samples, R is unable to perform the analysis, at 
>> least
>
>> with the code I have developed. It halts when it runs out of 
>> memory. A
>
>> collegue subsequently constructed the simulation using another
>> software program with a similar computer and, while it took over 
>> night
>
>> (and then some), the program produced the results desired.
>>
>> I am curious if it is the case that such large simulations are out 
>> of
>> the grasp of R or if my code is not adequately organized to perform
>> the simulation.
>>
>> I would appreciate any thoughts or advice.
>
>
> Don't hold all datasets (and results, if they are big) in the memory 
> at
> the same time!!!
>
> Either generate them when you use them and delete them afterwards, 
> or
> save them to disc an only load one by one for further analyses.
> Also, you might want to call gc() after you removed large objects...
>
> Uwe Ligges
>
>
>
>> Harold
>>
>>
>>
>> library(MASS)
>> library(nlme)
>> mu<-c(100,150,200,250)
>> Sigma<-matrix(c(400,80,80,80,80,400,80,80,80,80,400,80,80,80,80,400),4
>> ,4
>> )
>> mu2<-c(0,0,0)
>> Sigma2<-diag(64,3)
>> sample.size<-5000
>> N<-250 #Number of datasets
>> #Take a single draw from VL distribution vl.error<-mvrnorm(n=N, 
>> mu2,
>> Sigma2)
>>
>> #Step 1 Create Data
>> Data <- lapply(seq(N), function(x)
>> as.data.frame(cbind(1:10,mvrnorm(n=sample.size, mu, Sigma))))
>>
>> #Step 2 Add Vertical Linking Error
>> for(i in seq(along=Data)){
>> Data[[i]]$V6 <- Data[[i]]$V2
>> Data[[i]]$V7 <- Data[[i]]$V3 + vl.error[i,1]
>> Data[[i]]$V8 <- Data[[i]]$V4 + vl.error[i,2]
>> Data[[i]]$V9 <- Data[[i]]$V5 + vl.error[i,3] }
>>
>> #Step 3 Restructure for Longitudinal Analysis long <- lapply(Data,
>> function(x) reshape(x, idvar="Data[[i]]$V1",
>> varying=list(c(names(Data[[i]])[2:5]),c(names(Data[[i]])[6:9])),
>> v.names=c("score.1","score.2"), direction="long"))
>>
>> #####################
>> #Clean up Workspace
>> rm(Data,vl.error)
>> gc()
>> #####################
>>
>> # Step 4 Run GLS
>>
>> glsrun1 <- lapply(long, function(x) gls(score.1~I(time-1), data=x,
>> correlation=corAR1(form=~1|V1), method='ML'))
>>
>> # Extract intercepts and slopes
>> int1 <- sapply(glsrun1, function(x) x$coefficient[1])
>> slo1 <- sapply(glsrun1, function(x) x$coefficient[2])
>>
>> ################
>> #Clean up workspace
>> rm(glsrun1)
>> gc()
>>
>> glsrun2 <- lapply(long, function(x) gls(score.2~I(time-1), data=x,
>> correlation=corAR1(form=~1|V1), method='ML'))
>>
>> # Extract intercepts and slopes
>> int2 <- sapply(glsrun2, function(x) x$coefficient[1])
>> slo2 <- sapply(glsrun2, function(x) x$coefficient[2])
>>
>>
>> #Clean up workspace
>> rm(glsrun2)
>> gc()
>>
>>
>>
>> # Print Results
>>
>> cat("Original Standard Errors","\n", "Intercept","\t",
>> sd(int1),"\n","Slope","\t","\t", sd(slo1),"\n")
>>
>> cat("Modified Standard Errors","\n", "Intercept","\t",
>> sd(int2),"\n","Slope","\t","\t", sd(slo2),"\n")
>>
>> rm(list=ls())
>> gc()
>>
>> [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Joerg.Klausen at empa.ch  Wed Jan 19 14:50:41 2005
From: Joerg.Klausen at empa.ch (Joerg Klausen)
Date: Wed, 19 Jan 2005 14:50:41 +0100
Subject: [R] How to replace slashes with back slashes
Message-ID: <s1ee73f6.096@du-gwpo.emp-eaw.ch>

Dear R-helpers

I am running R2.0.0 under Windows 2000. I am compiling a number of file paths into a simple text file that will be read by some other software we use. Unfortunately, it  can only handle file paths with back slashes (MS Windows convention), and from R, I get file paths with forward slashes. The following didn't work. 

> gsub('/', '\\', 'c:/dir1/dir2/file.ext')
[1] "c:dir1dir2file.ext"
> gsub('/', '\\\\', 'c:/dir1/dir2/file.ext')
[1] "c:\\dir1\\dir2\\file.ext"

I have tried to find an answer on R-help, but didn't ;-(

Thanks for helping me,
Kind regards,
J?rg

Dr. J?rg Klausen                       phone : +41 (0)44 823 41 27
EMPA (134)/GAW/QA-SAC     fax   : +41 (0)44 821 62 44
?berlandstrasse 129                 mailto: joerg.klausen at empa.ch 
CH-8600 D?bendorf                  http://www.empa.ch/gaw 
Switzerland                                http://www.empa.ch/gaw/gawsis



From chabotd at globetrotter.net  Wed Jan 19 14:56:57 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Wed, 19 Jan 2005 14:56:57 +0100
Subject: [R] recoding large number of categories (select in SAS)
Message-ID: <FB8AE69F-6A21-11D9-B555-00050279D82B@globetrotter.net>

Hi,

I have data on stomach contents. Possible prey species are in the 
hundreds, so a list of prey codes has been in used in many labs doing 
this kind of work.

When comes time to do analyses on these data one often wants to regroup 
prey in broader categories, especially for rare prey.

In SAS you can nest a large number of "if-else", or do this more 
cleanly with "select" like this:
select;
   when (149 <= prey <=150)   preyGr= 150;
   when (186 <= prey <= 187)  preyGr= 187;
   when (prey= 438)                 preyGr= 438;
   when (prey= 430)                 preyGr= 430;
   when (prey= 436)                 preyGr= 436;
   when (prey= 431)                 preyGr= 431;
   when (prey= 451)                 preyGr= 451;
   when (prey= 461)                 preyGr= 461;
   when (prey= 478)                 preyGr= 478;
   when (prey= 572)                 preyGr= 572;
   when (692 <= prey <=  695 )                                       
preyGr= 692;
   when (808 <= prey <=  826, 830 <= prey <= 832 )  	preyGr= 808;
   when (997 <= prey <= 998, 792 <= prey <= 796)  	preyGr= 792;
   when (882 <= prey <= 909)  						preyGr= 882;
   when (prey in (999, 125, 994))  					       preyGr= 9994;
   otherwise                             preyGr= 1;
end; *select;

The number of transformations is usually much larger than this short 
example.

What is the best way of doing this in R?

Sincerely,

Denis Chabot



From jeaneid at chass.utoronto.ca  Wed Jan 19 15:16:26 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Wed, 19 Jan 2005 09:16:26 -0500
Subject: [R] How to replace slashes with back slashes
In-Reply-To: <s1ee73f6.096@du-gwpo.emp-eaw.ch>
Message-ID: <Pine.SGI.4.40.0501190914130.6308410-100000@origin.chass.utoronto.ca>

try

sink("filename")
cat(paste(gsub('/', '\\\\', 'c:/dir1/dir2/file.ext'),"\n",gsub('/',
'\\\\', 'c:/dir1/dir2/file.ext'),"\n" ))

sink()
the idea is to use cat,

instead of paste you can do a for loop where you cat the string into the
sinked file

On Wed, 19 Jan 2005, Joerg Klausen wrote:

> Dear R-helpers
>
> I am running R2.0.0 under Windows 2000. I am compiling a number of file paths into a simple text file that will be read by some other software we use. Unfortunately, it  can only handle file paths with back slashes (MS Windows convention), and from R, I get file paths with forward slashes. The following didn't work.
>
> > gsub('/', '\\', 'c:/dir1/dir2/file.ext')
> [1] "c:dir1dir2file.ext"
> > gsub('/', '\\\\', 'c:/dir1/dir2/file.ext')
> [1] "c:\\dir1\\dir2\\file.ext"
>
> I have tried to find an answer on R-help, but didn't ;-(
>
> Thanks for helping me,
> Kind regards,
> Jrg
>
> Dr. Jrg Klausen                       phone : +41 (0)44 823 41 27
> EMPA (134)/GAW/QA-SAC     fax   : +41 (0)44 821 62 44
> berlandstrasse 129                 mailto: joerg.klausen at empa.ch
> CH-8600 Dbendorf                  http://www.empa.ch/gaw
> Switzerland                                http://www.empa.ch/gaw/gawsis
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From phgrosjean at sciviews.org  Wed Jan 19 15:14:56 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 19 Jan 2005 15:14:56 +0100
Subject: [R] recoding large number of categories (select in SAS)
In-Reply-To: <FB8AE69F-6A21-11D9-B555-00050279D82B@globetrotter.net>
References: <FB8AE69F-6A21-11D9-B555-00050279D82B@globetrotter.net>
Message-ID: <41EE6B60.3010109@sciviews.org>

Does

 > ?cut

answers to your question?

Best,

Philippe Grosjean

..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................

Denis Chabot wrote:
> Hi,
> 
> I have data on stomach contents. Possible prey species are in the 
> hundreds, so a list of prey codes has been in used in many labs doing 
> this kind of work.
> 
> When comes time to do analyses on these data one often wants to regroup 
> prey in broader categories, especially for rare prey.
> 
> In SAS you can nest a large number of "if-else", or do this more cleanly 
> with "select" like this:
> select;
>   when (149 <= prey <=150)   preyGr= 150;
>   when (186 <= prey <= 187)  preyGr= 187;
>   when (prey= 438)                 preyGr= 438;
>   when (prey= 430)                 preyGr= 430;
>   when (prey= 436)                 preyGr= 436;
>   when (prey= 431)                 preyGr= 431;
>   when (prey= 451)                 preyGr= 451;
>   when (prey= 461)                 preyGr= 461;
>   when (prey= 478)                 preyGr= 478;
>   when (prey= 572)                 preyGr= 572;
>   when (692 <= prey <=  695 )                                       
> preyGr= 692;
>   when (808 <= prey <=  826, 830 <= prey <= 832 )      preyGr= 808;
>   when (997 <= prey <= 998, 792 <= prey <= 796)      preyGr= 792;
>   when (882 <= prey <= 909)                          preyGr= 882;
>   when (prey in (999, 125, 994))                             preyGr= 9994;
>   otherwise                             preyGr= 1;
> end; *select;
> 
> The number of transformations is usually much larger than this short 
> example.
> 
> What is the best way of doing this in R?
> 
> Sincerely,
> 
> Denis Chabot
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jtk at cmp.uea.ac.uk  Wed Jan 19 16:15:12 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Wed, 19 Jan 2005 15:15:12 +0000
Subject: [R] How to replace slashes with back slashes
In-Reply-To: <s1ee73f6.096@du-gwpo.emp-eaw.ch>
References: <s1ee73f6.096@du-gwpo.emp-eaw.ch>
Message-ID: <20050119151512.GC18873@jtkpc.cmp.uea.ac.uk>

On Wed, Jan 19, 2005 at 02:50:41PM +0100, Joerg Klausen wrote:

> I am running R2.0.0 under Windows 2000. I am compiling a number of file paths into a simple text file that will be read by some other software we use. Unfortunately, it  can only handle file paths with back slashes (MS Windows convention), and from R, I get file paths with forward slashes. The following didn't work. 
> 
> > gsub('/', '\\', 'c:/dir1/dir2/file.ext')
> [1] "c:dir1dir2file.ext"
> > gsub('/', '\\\\', 'c:/dir1/dir2/file.ext')
> [1] "c:\\dir1\\dir2\\file.ext"
> 
> I have tried to find an answer on R-help, but didn't ;-(

Sometimes, you don't find an(other) answer because you already have
one...  ;-)

In this case,

    gsub('/', '\\\\', 'c:/dir1/dir2/file.ext')

actually does what you want. Notice that print doesn't just dump the
string, it renders it in a representation suitable for the righthand
side in an R assignment. You could convince yourself with

    cat(gsub('/', '\\\\', 'c:/dir1/dir2/file.ext'))

which actually does dump the string.

Greetinx, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From ligges at statistik.uni-dortmund.de  Wed Jan 19 15:18:40 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 19 Jan 2005 15:18:40 +0100
Subject: [R] How to replace slashes with back slashes
In-Reply-To: <s1ee73f6.096@du-gwpo.emp-eaw.ch>
References: <s1ee73f6.096@du-gwpo.emp-eaw.ch>
Message-ID: <41EE6C40.1010002@statistik.uni-dortmund.de>

Joerg Klausen wrote:

> Dear R-helpers
> 
> I am running R2.0.0 under Windows 2000. I am compiling a number of file paths into a simple text file that will be read by some other software we use. Unfortunately, it  can only handle file paths with back slashes (MS Windows convention), and from R, I get file paths with forward slashes. The following didn't work. 
> 
> 
>>gsub('/', '\\', 'c:/dir1/dir2/file.ext')
> 
> [1] "c:dir1dir2file.ext"
> 
>>gsub('/', '\\\\', 'c:/dir1/dir2/file.ext')
> 
> [1] "c:\\dir1\\dir2\\file.ext"

The latter worked very well, it's just a matter of print()ing:

  cat(gsub('/', '\\\\', 'c:/dir1/dir2/file.ext'))

Uwe Ligges




> I have tried to find an answer on R-help, but didn't ;-(
> 
> Thanks for helping me,
> Kind regards,
> J?rg
> 
> Dr. J?rg Klausen                       phone : +41 (0)44 823 41 27
> EMPA (134)/GAW/QA-SAC     fax   : +41 (0)44 821 62 44
> ?berlandstrasse 129                 mailto: joerg.klausen at empa.ch 
> CH-8600 D?bendorf                  http://www.empa.ch/gaw 
> Switzerland                                http://www.empa.ch/gaw/gawsis
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From phgrosjean at sciviews.org  Wed Jan 19 15:21:06 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 19 Jan 2005 15:21:06 +0100
Subject: [R] How to replace slashes with back slashes
In-Reply-To: <s1ee73f6.096@du-gwpo.emp-eaw.ch>
References: <s1ee73f6.096@du-gwpo.emp-eaw.ch>
Message-ID: <41EE6CD2.2090100@sciviews.org>

The second syntax is correct:

 > gsub('/', '\\\\', 'c:/dir1/dir2/file.ext')
[1] "c:\\dir1\\dir2\\file.ext"

Indeed, R escapes backslashes. That is why they are doubled in R 
strings. However, they appear as single backslashes in oyur text file:

 > res <- gsub('/', '\\\\', 'c:/dir1/dir2/file.ext')
 > cat(res, file = "test.txt")
 > file.show("test.txt", delete.file = TRUE)

You got this in your file:

c:\dir1\dir2\file.ext

thus pretty readable by a Windows app.
Best,

Philippe Grosjean

..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................

Joerg Klausen wrote:
> Dear R-helpers
> 
> I am running R2.0.0 under Windows 2000. I am compiling a number of file paths into a simple text file that will be read by some other software we use. Unfortunately, it  can only handle file paths with back slashes (MS Windows convention), and from R, I get file paths with forward slashes. The following didn't work. 
> 
> 
>>gsub('/', '\\', 'c:/dir1/dir2/file.ext')
> 
> [1] "c:dir1dir2file.ext"
> 
>>gsub('/', '\\\\', 'c:/dir1/dir2/file.ext')
> 
> [1] "c:\\dir1\\dir2\\file.ext"
> 
> I have tried to find an answer on R-help, but didn't ;-(
> 
> Thanks for helping me,
> Kind regards,
> J?rg
> 
> Dr. J?rg Klausen                       phone : +41 (0)44 823 41 27
> EMPA (134)/GAW/QA-SAC     fax   : +41 (0)44 821 62 44
> ?berlandstrasse 129                 mailto: joerg.klausen at empa.ch 
> CH-8600 D?bendorf                  http://www.empa.ch/gaw 
> Switzerland                                http://www.empa.ch/gaw/gawsis
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From james.holtman at convergys.com  Wed Jan 19 15:30:57 2005
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Wed, 19 Jan 2005 09:30:57 -0500
Subject: [R] recoding large number of categories (select in SAS)
Message-ID: <OFEF2C9DA8.0631A866-ON85256F8E.004F7E7E@nd.convergys.com>





Here is a way of doing it by setting up a matrix of values to test against.
Easier than writing all the 'select' statements.

> x.trans <- matrix(c(  # translation matrix; first column is min, second
is max,
+     149, 150, 150,      # and third is the value to be returned
+     186, 187, 187,
+     438, 438, 438,
+     430, 430, 430,
+     808, 826, 808,
+     830, 832, 808,
+     997, 998, 792,
+     792, 796, 792), ncol=3, byrow=T)
> colnames(x.trans) <- c('min', 'max', 'value')
>
> x.default <- 9999   # default/nomatch value
>
> x.test <- c(150, 149, 148, 438, 997, 791, 795, 810, 820, 834)   # test
data
> #
> # this function will test each value and if between the min/max, return 3
column
> #
> newValues <- sapply(x.test, function(x){
+     .value <- x.trans[(x >= x.trans[,'min']) & (x <=
x.trans[,'max']),'value']
+     if (length(.value) == 0) .value <- x.default    # on no match, take
default
+     .value[1]   # return first value if multiple matches
+ })
> newValues
 [1]  150  150 9999  438  792 9999  792  808  808 9999
>
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      Denis Chabot                                                                                                         
                      <chabotd at globetrotter        To:       r-help at stat.math.ethz.ch                                                      
                      .net>                        cc:                                                                                     
                      Sent by:                     Subject:  [R] recoding large number of categories (select in SAS)                       
                      r-help-bounces at stat.m                                                                                                
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      01/19/2005 08:56 AM                                                                                                  
                                                                                                                                           
                                                                                                                                           




Hi,

I have data on stomach contents. Possible prey species are in the
hundreds, so a list of prey codes has been in used in many labs doing
this kind of work.

When comes time to do analyses on these data one often wants to regroup
prey in broader categories, especially for rare prey.

In SAS you can nest a large number of "if-else", or do this more
cleanly with "select" like this:
select;
   when (149 <= prey <=150)   preyGr= 150;
   when (186 <= prey <= 187)  preyGr= 187;
   when (prey= 438)                 preyGr= 438;
   when (prey= 430)                 preyGr= 430;
   when (prey= 436)                 preyGr= 436;
   when (prey= 431)                 preyGr= 431;
   when (prey= 451)                 preyGr= 451;
   when (prey= 461)                 preyGr= 461;
   when (prey= 478)                 preyGr= 478;
   when (prey= 572)                 preyGr= 572;
   when (692 <= prey <=  695 )
preyGr= 692;
   when (808 <= prey <=  826, 830 <= prey <= 832 )           preyGr= 808;
   when (997 <= prey <= 998, 792 <= prey <= 796)             preyGr= 792;
   when (882 <= prey <= 909)
                         preyGr= 882;
   when (prey in (999, 125, 994))
                    preyGr= 9994;
   otherwise                             preyGr= 1;
end; *select;

The number of transformations is usually much larger than this short
example.

What is the best way of doing this in R?

Sincerely,

Denis Chabot

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From roebuck at odin.mdacc.tmc.edu  Wed Jan 19 16:00:18 2005
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Wed, 19 Jan 2005 09:00:18 -0600 (CST)
Subject: [R] help wanted using R in a classroom
In-Reply-To: <20050118195748.76244.qmail@web50308.mail.yahoo.com>
References: <20050118195748.76244.qmail@web50308.mail.yahoo.com>
Message-ID: <Pine.OSF.4.58.0501190840530.102517@odin.mdacc.tmc.edu>

On Tue, 18 Jan 2005, bogdan romocea wrote:

> It appears you wouldn't get much improvement at all even if the 2nd CPU
> were used at 100%. Five R sessions can easily overwhelm one CPU. I
> think you need (a lot) more CPUs than 2 to solve your problem.
>
> Possible solutions:
> 1. Install R on each eMac. Since you have 40 of them, you might want to
> put together a script to do this.
> 2. Get some boxes that can run Windows. On Windows, you can run R from
> a CD/zip drive/USB drive. (So you could burn 40 CDs and have everyone
> run their R session on their box.) As far as I know the same is not
> true for GNU/Linux and Mac OS.
>

Mac OS X is certainly capable of running programs on CD/Zip/USB drive.

> -----Original Message-----
> From: Sam Parvaneh
> Sent: Monday, January 17, 2005 6:11 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] help wanted using R in a classroom
>
> I'm using R 2.0.1 for Mac OS X in a classroom with 40 eMacs running
> Mac OS X version 10.3.6.
> These Macs are network based, meaning that the students log in to an
> XServe G4 where their user accounts and home directories are stored.
>
> The problem that I'm having each time a group of students (usually 7
> to 10) use R is that the whole system get incredibly slow.
> The response time for opening an application while the students are
> running R is around 5 minutes.
> If a student wants to log into the system while others are running R,
> it can take up to 10 minutes for the student to get logged in.
> Everything gets very slow that it's almost impossible to work.
> When I look at the server Graphs, the CPU usage of the first CPU is
> always 100% when these students are using R. The second CPU is left
> at 15%.
>
> When these students quit R, then everything's back to normal again.
> The usage of both CPUs go back down to between 5-10%.
> Is there anyone out there using R in a university like this?
> Does anyone have an idea what this might depend one or maybe a
> solution?

You didn't mention where R is currently installed. What is
the path to the R framework and is that located on the XServe?
If only the student's personal R library directory is involved,
that shouldn't cause these problems.

Did you try creating a disk image (dot-dmg) containing the
R application and have it mount locally on each eMac?

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From macq at llnl.gov  Wed Jan 19 16:23:11 2005
From: macq at llnl.gov (Don MacQueen)
Date: Wed, 19 Jan 2005 07:23:11 -0800
Subject: [R] help wanted using R in a classroom
In-Reply-To: <OFD10A3DBB.4AA85192-ONC1256F8C.003BD083-C1256F8C.003D7B27@sh.se>
References: <OFD10A3DBB.4AA85192-ONC1256F8C.003BD083-C1256F8C.003D7B27@sh.se>
Message-ID: <p06110405be142719d3d8@[128.115.153.6]>

Re-reading your email carefully, I'm not absolutely sure whether the 
multiple R processes are all running on the server, as I first 
assumed. If they are, then it seems to me that the solution is to get 
the R processes running on the clients.

On my OS X box, "R" is a link to the R script located on an NFS 
mounted file system:

[166]% ls -l /usr/local/bin/R
lrwxr-xr-x  1 root  wheel  42  9 Dec 12:25 /usr/local/bin/R@ -> 
/statsrvr/apps/R/R-2.0.1/darwin/bin/R

[167]% df -k | grep statsrvr
statbox:/APPS1               9830344  9161202    659312    93% 
/private/automount/statsrvr/apps

statbox is a Solaris machine.

So, R is "installed", by which I mean all of its files are located 
on, a server, but when I run it, it runs locally.

So I think should be able to arrange things so that each student's R 
session runs on the eMac in front of him, instead of on the server.

-Don

At 12:11 PM +0100 1/17/05, Sam Parvaneh wrote:
>Hi everyone!
>
>I'm using R 2.0.1 for Mac OS X in a classroom with 40 eMacs running Mac OS
>X version 10.3.6.
>These Macs are network based, meaning that the students log in to an
>XServe G4 where their user accounts and home directories are stored.

Do they actually log in to the XServe, or do they login to the eMac 
and have their home directories mounted on the eMac?

>
>The problem that I'm having each time a group of students (usually 7 to
>10) use R is that the whole system get incredibly slow.
>The response time for opening an application  while the students are
>running R is around 5 minutes.
>If a student wants to log into the system while others are running R, it
>can take up to 10 minutes for the student to get logged in.
>Everything gets very slow that it's almost impossible to work.
>When I look at the server Graphs, the CPU usage of the first CPU is always
>100% when these students are using R. The second CPU is left at 15%.
>
>When these students quit R, then everything's is back to normal again. The
>usage of both CPUs go back down to between 5-10%.
>Is there anyone out there using R in a university like this?
>Does anyone have an idea what this might depend one or maybe a solution?
>I can provide some more information if anyone wants, if you think you can
>help me.
>
>Thanks in advance
>/Sam
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From luk111111 at yahoo.com  Wed Jan 19 16:27:27 2005
From: luk111111 at yahoo.com (luk)
Date: Wed, 19 Jan 2005 07:27:27 -0800 (PST)
Subject: [R] extract rules from  randomForest?
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E546@usrymx25.merck.com>
Message-ID: <20050119152727.29939.qmail@web30906.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050119/b7cb52cd/attachment.pl

From cc2240 at columbia.edu  Wed Jan 19 16:31:27 2005
From: cc2240 at columbia.edu (Chung Chang)
Date: Wed, 19 Jan 2005 10:31:27 -0500
Subject: [R] a question about linear mixed model in R 
In-Reply-To: <BE130C15.D145%peterm@andrew.cmu.edu>
References: <BE130C15.D145%peterm@andrew.cmu.edu>
Message-ID: <1106148687.41ee7d4f682e6@cubmail.cc.columbia.edu>

Thanks for your post.
Yes, your example is indeed similar to my question.
If i means group, j means individual(subject)
h:indicator(0:control;1:experiment) k:repeat(if no repeat then k=1)
the the model is also X_hijk = alpha_h + h * b_i + r_(ij) + e_hijk.

After I posted this question, I found out how to do it in R. 
So I would like to share with you guys and hear the comments from
you. 
X:response,b_i subject effect, r(ij) nested effect within subject

lme(X~alpha_h,data=dataset,random=list(subject=~h-1,r=~1),method="ML
",na.action="na.omit")
the fixed effect part is alpha_h
the random effect is subject effect, the corresponding coefficient
is h and -1 means no random intercept of subject.  
and random effect of r(nested effect within subject)
Thanks for your help



 Peter Muhlberger <peterm at andrew.cmu.edu>:

> Hi Chung Cheng:  This seems related to a problem I'm having in
> some data of
> mine as well.  I'm new to R (played w/ it some a year ago) & to
> lme
> modeling, so take this w/ a grain of salt, but here are some
> thoughts:
> 
> In my problem, D would be an indicator of whether a subject was
> in the
> control condition or not.  In the control condition, all people
> participated
> individually, in the experimental condition there was small-group
> based
> discussion.  r(ij) would be some variable that affects the
> outcome, but
> whose effect may be moderated by the group the discussion was in.
> 
> The model assumes that the non-control condition values will have
> a
> distribution of coefficients for r(ij).  The coefficient for
> r(ij) in the
> controls need not have the same central value as for the
> non-controls
> (though it would be nice to be able constrain it so it would be).
>  So, it
> might make some sense to split the variable into two variables,
> one with
> zeros for the controls & one w/ zeros for the experimental groups
> and
> estimate the former w/ random effects & the other not.
> 
> I'm not 100% sure that's what you're asking, but it seems
> related.
> 
> Peter
> 
> >Dear all,
> >
> >I have a somewhat unusual linear mixed model that I can't seem
> >to code in lme.  It's only unusual in that one random effect is
> >applied only to some of the observations (I have an indicator
> >variable
> >that specifies which observations have this random effect).
> >
> >The model is:
> >
> >X_hijk = alpha_h + h * b_i + r_(ij) + e_hijk , where
> >
> >  h = 0 or 1 (indicator)
> >  i = 1, ..., N
> >  j = 1, ..., n_i
> >  k = 1, ..., K
> >alpha is fixed, and the rest are random.
> >I'm willing to assume b, r, and e are mutually independent
> >and normal with var(b) = sigma^2_b, var(r) = sigma^2_r, and
> >var(e) = sigma^2.
> >
> >Any help in writing this model in lme() would be greatly
> >appreciated.
> >
> >Thanks,
> >
> >Chung Cheng
>



From roebuck at odin.mdacc.tmc.edu  Wed Jan 19 16:38:00 2005
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Wed, 19 Jan 2005 09:38:00 -0600 (CST)
Subject: [R] signif() generic
In-Reply-To: <1106095782.29183.26.camel@localhost>
References: <1106095782.29183.26.camel@localhost>
Message-ID: <Pine.OSF.4.58.0501190921480.102517@odin.mdacc.tmc.edu>

On Wed, 19 Jan 2005 ockham at gmx.net wrote:

> I'm trying to write a class for Gaussian error propagation of measured
> values and their (estimated) errors,
>
> > setClass("sec", representation(val="numeric", err="numeric"))
>
> I've already successfully implemented basic arithmetics using mostly the
> "Arith" group generics. But I'm running into trouble when trying to get
> signif() to work for my purposes, i.e. with a default argument
> (digits=2): When trying
>
> > seMethod("signif", signature(x="sec", digits="numeric")),
> 	function(x, digits=2){
> 	# ...do something...
> 	}
> )
>
> and
>
> > signif(sec1)
>
> I get
>
> > Error in signif(x, digits) : Non-numeric argument to mathematical
> function
>
> Putting a second argument (like digits=2) into the call makes it work,
> but I want some default behavior specified for missing digits argument
> so it works in an analogous fashion as signif for numeric values.
> I also tried inserting
>
> > setGeneric("signif", function(x, digits=6) standardGeneric("signif"))
>
> before the setMethod block, but that wouldn't help either.


setGeneric("signif", function(x, digits=6) standardGeneric("signif"))
setMethod("signif", signature(x="sec", digits="numeric"),
    function(x, digits) {
        # ...do something...
        print(x)
        print(digits)
    }
)

setMethod("signif", signature(x="sec", digits="missing"),
    function(x, digits) {
        callGeneric(x, digits)
    }
)

The missing data method will cause the default value (6) to
be passed to the numeric data method.

> signif(new("sec"))
An object of class "sec"
   ...
[1] 6
>

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From p.dalgaard at biostat.ku.dk  Wed Jan 19 16:51:52 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Jan 2005 16:51:52 +0100
Subject: [R] recoding large number of categories (select in SAS)
In-Reply-To: <41EE6B60.3010109@sciviews.org>
References: <FB8AE69F-6A21-11D9-B555-00050279D82B@globetrotter.net>
	<41EE6B60.3010109@sciviews.org>
Message-ID: <x2is5te7s7.fsf@biostat.ku.dk>

Philippe Grosjean <phgrosjean at sciviews.org> writes:

> Does
> 
>  > ?cut
> 
> answers to your question?

That's one way, but it tends to get messy to get the names right.

You might consider using the rather little-known variant of levels
assignment:

preyGR <- prey # or factor(prey) if it wasn't one already
levels(preyGR) <- list("150"=149:150,
                       "187"=187:188,
                       "438"=438, 

                        [...]
                    
                       "9994"=c(999,125,994), "1"=NA) 

preyGR[is.na(preyGR) & !is.na(prey)] <- "1"

This would be roughly as clean as the SAS way, only the "otherwise"
case got a bit tricky.

> Best,
> 
> Philippe Grosjean
> 
> ..............................................<?}))><........
>   ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
>   ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>   ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
> ( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
>   ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
> ( ( ( ( (
>   ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
> ( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
>   ) ) ) ) )
> ( ( ( ( (    web:   http://www.umh.ac.be/~econum
>   ) ) ) ) )          http://www.sciviews.org
> ( ( ( ( (
> ..............................................................
> 
> Denis Chabot wrote:
> > Hi,
> > I have data on stomach contents. Possible prey species are in the
> > hundreds, so a list of prey codes has been in used in many labs
> > doing this kind of work.
> > When comes time to do analyses on these data one often wants to
> > regroup prey in broader categories, especially for rare prey.
> > In SAS you can nest a large number of "if-else", or do this more
> > cleanly with "select" like this:
> > select;
> >   when (149 <= prey <=150)   preyGr= 150;
> >   when (186 <= prey <= 187)  preyGr= 187;
> >   when (prey= 438)                 preyGr= 438;
> >   when (prey= 430)                 preyGr= 430;
> >   when (prey= 436)                 preyGr= 436;
> >   when (prey= 431)                 preyGr= 431;
> >   when (prey= 451)                 preyGr= 451;
> >   when (prey= 461)                 preyGr= 461;
> >   when (prey= 478)                 preyGr= 478;
> >   when (prey= 572)                 preyGr= 572;
> >   when (692 <= prey <=  695 )
> > preyGr= 692;
> >   when (808 <= prey <=  826, 830 <= prey <= 832 )      preyGr= 808;
> >   when (997 <= prey <= 998, 792 <= prey <= 796)      preyGr= 792;
> >   when (882 <= prey <= 909)                          preyGr= 882;
> >   when (prey in (999, 125, 994))                             preyGr= 9994;
> >   otherwise                             preyGr= 1;
> > end; *select;
> > The number of transformations is usually much larger than this short
> > example.
> > What is the best way of doing this in R?
> > Sincerely,
> > Denis Chabot
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jfox at mcmaster.ca  Wed Jan 19 17:09:51 2005
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 19 Jan 2005 11:09:51 -0500
Subject: [R] recoding large number of categories (select in SAS)
In-Reply-To: <x2is5te7s7.fsf@biostat.ku.dk>
Message-ID: <20050119160950.XJPQ25979.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Peter et al.,

The recode() function in the car package will also do this kind of thing,
will work even when the ranges include non-integers, and supports an else=
construction.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Dalgaard
> Sent: Wednesday, January 19, 2005 10:52 AM
> To: Philippe Grosjean
> Cc: r-help at stat.math.ethz.ch; Denis Chabot
> Subject: Re: [R] recoding large number of categories (select in SAS)
> 
> Philippe Grosjean <phgrosjean at sciviews.org> writes:
> 
> > Does
> > 
> >  > ?cut
> > 
> > answers to your question?
> 
> That's one way, but it tends to get messy to get the names right.
> 
> You might consider using the rather little-known variant of levels
> assignment:
> 
> preyGR <- prey # or factor(prey) if it wasn't one already
> levels(preyGR) <- list("150"=149:150,
>                        "187"=187:188,
>                        "438"=438, 
> 
>                         [...]
>                     
>                        "9994"=c(999,125,994), "1"=NA) 
> 
> preyGR[is.na(preyGR) & !is.na(prey)] <- "1"
> 
> This would be roughly as clean as the SAS way, only the "otherwise"
> case got a bit tricky.
> 
> > Best,
> > 
> > Philippe Grosjean
> > 
> > ..............................................<?}))><........
> >   ) ) ) ) )
> > ( ( ( ( (    Prof. Philippe Grosjean
> >   ) ) ) ) )
> > ( ( ( ( (    Numerical Ecology of Aquatic Systems
> >   ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
> > ( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
> >   ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
> > ( ( ( ( (
> >   ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
> > ( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
> >   ) ) ) ) )
> > ( ( ( ( (    web:   http://www.umh.ac.be/~econum
> >   ) ) ) ) )          http://www.sciviews.org
> > ( ( ( ( (
> > ..............................................................
> > 
> > Denis Chabot wrote:
> > > Hi,
> > > I have data on stomach contents. Possible prey species are in the 
> > > hundreds, so a list of prey codes has been in used in many labs 
> > > doing this kind of work.
> > > When comes time to do analyses on these data one often wants to 
> > > regroup prey in broader categories, especially for rare prey.
> > > In SAS you can nest a large number of "if-else", or do this more 
> > > cleanly with "select" like this:
> > > select;
> > >   when (149 <= prey <=150)   preyGr= 150;
> > >   when (186 <= prey <= 187)  preyGr= 187;
> > >   when (prey= 438)                 preyGr= 438;
> > >   when (prey= 430)                 preyGr= 430;
> > >   when (prey= 436)                 preyGr= 436;
> > >   when (prey= 431)                 preyGr= 431;
> > >   when (prey= 451)                 preyGr= 451;
> > >   when (prey= 461)                 preyGr= 461;
> > >   when (prey= 478)                 preyGr= 478;
> > >   when (prey= 572)                 preyGr= 572;
> > >   when (692 <= prey <=  695 )
> > > preyGr= 692;
> > >   when (808 <= prey <=  826, 830 <= prey <= 832 )      
> preyGr= 808;
> > >   when (997 <= prey <= 998, 792 <= prey <= 796)      preyGr= 792;
> > >   when (882 <= prey <= 909)                          preyGr= 882;
> > >   when (prey in (999, 125, 994))                          
>    preyGr= 9994;
> > >   otherwise                             preyGr= 1;
> > > end; *select;
> > > The number of transformations is usually much larger than 
> this short 
> > > example.
> > > What is the best way of doing this in R?
> > > Sincerely,
> > > Denis Chabot
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Markku.Kiiskinen at laerdal.no  Wed Jan 19 17:44:38 2005
From: Markku.Kiiskinen at laerdal.no (Kiiskinen, Markku)
Date: Wed, 19 Jan 2005 17:44:38 +0100
Subject: [R] =?iso-8859-1?q?Automatisk_svar_ved_frav=E6r=3A_Notify?=
Message-ID: <D31520723288584FB72869E577C83604011EDAC1@nosrv003.laerdal.global>

Olen  talvilomalla ja seuraavan kerran tavattavissa  31.01.2005
Pyyd?n yst?v?llisesti ottamaan yhteytt? kiireellisiss? asioissa 
asiakaspalveluumme www.laerdal.fi tai 09-6129980

terv. Markku Kiiskinen

Laerdal Oy



From rolf at math.unb.ca  Wed Jan 19 17:51:17 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Wed, 19 Jan 2005 12:51:17 -0400 (AST)
Subject: [R] Off topic -- when is max min = min max?
Message-ID: <200501191651.j0JGpHdC023206@erdos.math.unb.ca>


This question has little or nothing to do with R; as usual I'm simply
hoping to take advantage of the great depth of knowledge and
expertise in the R community.

Anyone who is interested in replying should send email directly to me
(rolf at math.unb.ca) and not to this list.

To get to my question:  In a two person zero-sum game, the
value of the game to the row player is

	v_r = max min x'Ay
               x   y

where A is the m x n reward matrix and x and y are vectors of
probabilities summing to 1 (constituting the row player's and
and column player's ``mixed'' strategies, respectively).  Note
that x' means ``x transpose''.

The value of the game to the column player is (the negative of)

	v_c = min max x'Ay
               y   x

These two values, i.e. v_r and v_c, are equal --- as one might hope.
(One man's ceiling is another man's floor, as Paul Simon puts it.)

***Proving*** that they are equal is done (in game theory books) by
setting the problem up as a linear programming problem and invoking
the Duality Theorem.  (The column player's LP turns out to be the
dual of the row player's LP.)

Initially I thought that there must/should be an easier way to
prove that the two values are equal.  But I can't see one.  So
I would like to ask:

	o ***Is*** there a ``simple'' direct proof that v_r = v_c?

	o How crucial is it that we are maximizing and minimizing
	  over the simplices of probability vectors (summing to 1)
	  in m-dimensional and n-dimensional space respectively?

	  Could we optimize over some more general compact (convex?)
	  set in R^{m+n} and preserve the equality?

	o Are there known ``general'' sufficient conditions (on the
	  function phi(.,.) and the domain of optimization) so that

		max min phi(x,y) = min max phi(x,y)       ???
                 x   y              y   x

	o Has anyone any idea where I might look to find answers
	  to such questions?

Thanks for any insights.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From peterm at andrew.cmu.edu  Wed Jan 19 18:27:19 2005
From: peterm at andrew.cmu.edu (Peter Muhlberger)
Date: Wed, 19 Jan 2005 12:27:19 -0500
Subject: [R] Function to modify existing data.frame--Improving R Language
In-Reply-To: <200501191116.j0JBEokq015082@hypatia.math.ethz.ch>
Message-ID: <BE1402A7.D0AC%peterm@andrew.cmu.edu>

Thanks to everyone who replied to my question!

A comment regarding Thomas's point below:  Java & other object-oriented
languages that are function rather than macro based usually provide methods
& other capabilities that allow pass by reference in certain cases and these
methods would solve the problem I posed regarding how to change values in an
existing data.frame from within a function.  In Java, for example, objects
pass by reference, so I could easily transfer a dataset to a function & have
it efficiently changed.  In Visual Basic, one could indicate whether a
variable was being passed by reference or value.  By not allowing any
straightforward passing by reference, R strikes me as a lot less flexible &
useful than it might be.  A basic operation in other stats languages is to
update a dataset using a program.  This proves very helpful for managing
data and setting up analyses.  But, this seems to be quite inelegant to do
in R.

Peter

On 1/19/05 6:16 AM, "r-help-request at stat.math.ethz.ch" From: Thomas Lumley
<tlumley at u.washington.edu>
<r-help-request at stat.math.ethz.ch> wrote:

> Yes and no.
> 
> This isn't so much a question of pass-by-reference, as one reply
> suggested, but of macros vs functions.
> 
> Stata is (largely) a macro language: it does operations on command strings
> and then evaluates them.  It's not that Stata programs work with
> references, it's that all objects (except local macros) are global.
> 
> R is based on functions: it evaluates arguments and then operates on
> them.  When you have functions, with local variables, it then becomes
> relevant to ask whether the arguments to the function are just copies or
> are references to the real thing. In R they are just copies (from a
> language point of view) but are often references from an efficiency point
> of view.



From HDoran at air.org  Wed Jan 19 18:46:53 2005
From: HDoran at air.org (Doran, Harold)
Date: Wed, 19 Jan 2005 12:46:53 -0500
Subject: [R] Referencing objects within a loop
Message-ID: <88EAF3512A55DF46B06B1954AEF73F74075D02C1@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050119/5a5f7058/attachment.pl

From bates at stat.wisc.edu  Wed Jan 19 19:28:44 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 19 Jan 2005 12:28:44 -0600
Subject: [R] Referencing objects within a loop
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F74075D02C1@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F74075D02C1@dc1ex2.air.org>
Message-ID: <41EEA6DC.2010606@stat.wisc.edu>

Doran, Harold wrote:
> Dear List:
> 
> It appears that simulating data where all dataframes are stored as a
> list will only work for relatively small analyses. Instead, it appears
> that creating N individual dataframes, saving them, and loading them
> when needed is the best way to save memory and make this a feasible
> task.
> 
> As such, I now have a new(er) question with respect to dealing with
> individual files within a loop. To begin, I construct 250 individual
> data files as follows:
> 
> library(MASS)
> Sigma<-matrix(c(400,80,80,80,80,400,80,80,80,80,400,80,80,80,80,400),4,4
> ) 
> mu<-c(100,150,200,250) 
> sample.size<-5000
> N=250 #Number of dataframes 
> 
> # Step 1 Create dataframes
> for(i in 1:N) 
> { 
> assign(paste("Data.", i, sep=''), 
> as.data.frame(cbind(seq(1:sample.size),(mvrnorm(n=sample.size, mu,
> Sigma))))) 
> } 
> 
> 
> My goal is to now save each file, remove them from memory, and load each
> file individually, run it through a linear model, and save the output.
> 
> To attempt to save each file I try
> 
> for (i in 1:N) {
> save(paste("Data.", i, sep=""), file=paste("Data.",i, ".Rdata", sep=""))
> 
> }
> 
> Which gives me an error message. It essentially states that Object
> "paste("Data.", i, sep = "")" not found
> 
> I'm not quite sure what I am doing wrong here. 

You need to use 'get' to convert a character string into an object.  An 
alternative is

for (nm in paste("Data.", 1:N, sep = '')) {
   save(list = nm, file = paste(nm, ".Rdata", sep = ''))
}

> The other issue I am encountering is how to best work with an object in
> a loop. I get the following to work, but I'm not sure if this is the
> best method for doing so. 
> 
> for(i in 1:N){
> assign(paste("out.",i,sep=""),
> lm((get(paste("Data.",i,sep=""))[["V3"]])~(get(paste("Data.",i,sep=""))[
> ["V2"]])))
> }

As shown above, looping in R does not need to be over a numeric index 
vector.  You can loop over any vector object, including the vector of 
names.  Another way you can simplify this is to use the formula/data 
specification for lm.  If your model formula is always going to be V3 ~ 
V2 then just use that as in

for (nm in paste("Data.", 1:N, sep = '')) {
   assign(paste(nm, '.out', sep=''), lm(V3 ~ V2, data = get(nm))
}

There are probably better approaches but I think you would prefer that I 
work on the lme4 code instead of spending more time answering these 
questions.  :-)

(BTW, I believe I have a very general formulation for fitting models 
with non-nested grouping factors, such as the student-teacher-school 
data from DC.  Still a few bugs though.)



From tlumley at u.washington.edu  Wed Jan 19 19:31:13 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 19 Jan 2005 10:31:13 -0800 (PST)
Subject: [R] Function to modify existing data.frame--Improving R Language
In-Reply-To: <BE1402A7.D0AC%peterm@andrew.cmu.edu>
References: <BE1402A7.D0AC%peterm@andrew.cmu.edu>
Message-ID: <Pine.A41.4.61b.0501191019110.332684@homer11.u.washington.edu>

On Wed, 19 Jan 2005, Peter Muhlberger wrote:

> 						 By not allowing any
> straightforward passing by reference, R strikes me as a lot less flexible &
> useful than it might be.  A basic operation in other stats languages is to
> update a dataset using a program.  This proves very helpful for managing
> data and setting up analyses.  But, this seems to be quite inelegant to do
> in R.

I don't see why
     mydata <- some.program(mydata)
is much less elegant than
     mydata.someProgram()
as a way of updating a data set. It may use more memory, but that wasn't 
the point at issue.

Of course there are advantages to the ability to pass by reference, and 
disadvantages -- the most obvious disadvantage is that it is not easy to 
tell which variables are modified by a given piece of code.

It probably wouldn't be that hard to produce something that looked like a 
data frame but was passed by reference, by wrapping it in a environment.

 	-thomas




>
> Peter
>
> On 1/19/05 6:16 AM, "r-help-request at stat.math.ethz.ch" From: Thomas Lumley
> <tlumley at u.washington.edu>
> <r-help-request at stat.math.ethz.ch> wrote:
>
>> Yes and no.
>>
>> This isn't so much a question of pass-by-reference, as one reply
>> suggested, but of macros vs functions.
>>
>> Stata is (largely) a macro language: it does operations on command strings
>> and then evaluates them.  It's not that Stata programs work with
>> references, it's that all objects (except local macros) are global.
>>
>> R is based on functions: it evaluates arguments and then operates on
>> them.  When you have functions, with local variables, it then becomes
>> relevant to ask whether the arguments to the function are just copies or
>> are references to the real thing. In R they are just copies (from a
>> language point of view) but are often references from an efficiency point
>> of view.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From ripley at stats.ox.ac.uk  Wed Jan 19 19:39:13 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 19 Jan 2005 18:39:13 +0000 (GMT)
Subject: [R] Referencing objects within a loop
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F74075D02C1@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F74075D02C1@dc1ex2.air.org>
Message-ID: <Pine.LNX.4.61.0501191836310.14555@gannet.stats>

On Wed, 19 Jan 2005, Doran, Harold wrote:

[...]

> Which gives me an error message. It essentially states that Object
> "paste("Data.", i, sep = "")" not found
>
> I'm not quite sure what I am doing wrong here.

Please read the help page: save works on objects, not names of objects
unless you use the list= argument.

You may in any case find it easier to use .save/readRDS to save and 
retrieve anonymous objects.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From peterm at andrew.cmu.edu  Wed Jan 19 19:41:20 2005
From: peterm at andrew.cmu.edu (Peter Muhlberger)
Date: Wed, 19 Jan 2005 13:41:20 -0500
Subject: [R] a question about linear mixed model in R 
In-Reply-To: <1106148687.41ee7d4f682e6@cubmail.cc.columbia.edu>
Message-ID: <BE141400.D0C0%peterm@andrew.cmu.edu>

On 1/19/05 10:31 AM, "Chung Chang" <cc2240 at columbia.edu> wrote:

> Thanks for your post.
> Yes, your example is indeed similar to my question.
> If i means group, j means individual(subject)

Isn't 'i' individual & j group?

> h:indicator(0:control;1:experiment) k:repeat(if no repeat then k=1)
> the the model is also X_hijk = alpha_h + h * b_i + r_(ij) + e_hijk.
> 
> After I posted this question, I found out how to do it in R.
> So I would like to share with you guys and hear the comments from
> you. 
> X:response,b_i subject effect, r(ij) nested effect within subject
> 
> lme(X~alpha_h,data=dataset,random=list(subject=~h-1,r=~1),method="ML
> ",na.action="na.omit")
> the fixed effect part is alpha_h
> the random effect is subject effect, the corresponding coefficient
> is h and -1 means no random intercept of subject.
> and random effect of r(nested effect within subject)
> Thanks for your help

Hi Chung Chang:  I gather that subject & r above are the grouping variables.
subject would indicate each individual participant, which probably means you
must have multiple observations per individual.  I'm not clear why you would
nest within subject for h but within r for the model constant, but then I
don't know details about your experiment.

Let me see, though, whether I can apply this to my own experiment
(simplified):

Participants engaged in a 2X2 experiment:

c1 (condition 1):  0, 1 indicator.  1=person participated in group-based
political discussion.  0=no group-based discussion (individual sit & think).

c2:  1=person received a citizenship prime.  0=no citizenship prime

c1 & c2 are crossed to yield 4 cells.

grp=a 1:n variable indicating which discussion group a person was in, and
n+1 for those in no discussion (c1==0)

V=some continuous covariate of Y, but one whose coefficient I suspect may be
affected through discussion groups, for those people who were in discussion.

a=constant

lm Model (expanded for clarity):

Y ~ a + c1 + c2 + c1:c2 + V

If I understood correctly, you are suggesting the following lme Model:

Fixed Model for lme:

Y ~ a + c1 + c2 + c1:c2 + c1:V + V

list(grp = ~ c1 + c1:c2 + c1:V)

Is this what you had in mind?

Thanks,

Peter



From hodgess at gator.uhd.edu  Wed Jan 19 19:50:58 2005
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Wed, 19 Jan 2005 12:50:58 -0600
Subject: [R] way off topic
Message-ID: <200501191850.j0JIowI04459@gator.dt.uh.edu>

Dear R People:

Here is another off topic question, please:

Does anyone know where to find some archaelogical data (carbon
dating), please?


When I googled, I got reseach papers but no data.


Thanks,

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From gregory.r.warnes at pfizer.com  Wed Jan 19 20:06:12 2005
From: gregory.r.warnes at pfizer.com (Warnes, Gregory R)
Date: Wed, 19 Jan 2005 14:06:12 -0500
Subject: [R] how to call R in delphi? 
Message-ID: <915D2D65A9986440A277AC5C98AA466F9788CA@groamrexm02.amer.pfizer.com>

You can also use SOAP to communicate with R via RSOAP (http://rsoap.sf.net).

-G

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Duncan Murdoch
> Sent: Wednesday, January 19, 2005 4:10 AM
> To: YiYao_Jiang
> Cc: r-help at stat.math.ethz.ch; Jack_Kang; Ivy_Li
> Subject: Re: [R] how to call R in delphi? 
> 
> 
> On Wed, 19 Jan 2005 10:12:07 +0800, "YiYao_Jiang"
> <YiYao_Jiang at smics.com> wrote :
> 
> >Dear All:
> >
> >Now I am writing  program in delphi , found it is very 
> convenience to do anova, T-test, F-Test, etc in R , how to 
> call R in delphi? Thsnks.
> 
> Tom referred you to a post about the COM interface.  
> 
> You can also do some of what you want with direct calls to R.dll.  See
> the "R API: entry points for C code" section of the Writing R
> Extensions manual.  If you're unfamiliar with calling C entries from
> Delphi, my web page
> <http://www.stats.uwo.ca/faculty/murdoch/software/compilingDLLs/>
> might help, though it is aimed at writing DLLs, rather than using
> R.dll.  
> 
> But that's another option:  let R be in charge, and just add some
> functions in a DLL written in Delphi.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From tiago17 at socrates.Berkeley.EDU  Wed Jan 19 20:28:38 2005
From: tiago17 at socrates.Berkeley.EDU (Tiago R Magalhaes)
Date: Wed, 19 Jan 2005 19:28:38 +0000
Subject: [R] importing files, columns "invade next column"
Message-ID: <p06100500be1465501d1b@[83.132.28.242]>

Thanks very much Mark and Prof Ripley

a) using sep='\t' when using read.table() helps somewhat

there is still a problem:I cannot get all the lines:
df <- read.table('file.txt', fill=T, header=T, sep='\t')
dim(df)
  9543  195

while with the shorter file (11 cols) I get all the rows
dim(df)
  15797    11

I have looked at row 9544 where the file seems to stop reading, but I 
cannot see in any of the cols an obvious reason for this to happen. 
Any ideas why? Maybe there is one col that is stopping the reading 
process and that column is not one of the 11 that are present in the 
smaller file.

b) fill=T is necessary
without fill=T, I get an error:
"line 1892 did not have 195 elements"

c) help page for read.table
I reread the help file for read.table and I would suggest to change 
it. From what I think I am reading, the '\t' would not be needed to 
work in my file, but it actually is:from the help page:

  If 'sep = ""' (the default for 'read.table') the separator is "white 
space", that is one or more spaces, tabs or newlines.

d) I incorrectly mentioned the FAQ in relation with data.restore. 
Where I actually saw data.restore mentioned was in the `R Data 
Import/Export Manual', which I read (even more than once...) failing 
to read the first paragraph of section where it's stated that the 
foreign package is used.

it works! (with source):
in Splus 6.1, windows 2000:
dump('file')
in R2.01, Mac 10.3.7:
source('file')

I get a list, where the first element is the data.frame I want
the column names have value added to them



>On Wed, 2005-01-19 at 04:25 +0000, Tiago R Magalhaes wrote:
>  > Dear R-listers:
>>
>>  I want to import a reasonably big file into a table. (15797 x 257
>>  columns). The file is tab delimited with NA in every empty space.
>
>Tiago,
>
>Have you tried to use read.table() explicitly defining the field
>delimiting character as a tab to see if that changes anything?
>
>Try the following:
>
>AllFBImpFields <- read.table('AllFBAllFieldsNAShorter.txt',
>                               header = TRUE,
>                               row.names=paste('a',1:15797, sep=''),
>                               as.is = TRUE,
>                               sep = "\t")
>
>I added the 'sep = "\t"' argument at the end.
>
>Also, leave out the 'fill = TRUE', which can cause problems. You do not
>need this unless your source file has a varying number of fields per
>line.
>
>Note that you do not need to specify the 'nrows' argument unless you
>generally want something less than all of the rows. Using the
>combination of 'skip' and 'nrows', you can read a subset of rows from
>the middle of the input file.
>
>See if that helps. Usually when there are column alignment problems, it
>is because the rows are not being consistently parsed into fields, which
>is frequently the result of not having the proper delimiting character
>specified.
>
>The last thought is to be sure that a '#' is not in your data file. This
>is interpreted as a comment character by default, which means that
>anything after it on a row will be ignored.
>
>HTH,
>
>Marc Schwartz



From ismdiego at est-econ.uc3m.es  Wed Jan 19 20:47:03 2005
From: ismdiego at est-econ.uc3m.es (=?ISO-8859-1?Q?Isaac_Mart=EDn?=)
Date: Wed, 19 Jan 2005 20:47:03 +0100
Subject: [R] Quadratic constraints
Message-ID: <41EEB937.4090307@est-econ.uc3m.es>

Hello,

I have a linear problem with quadratic constraints but i have no idea 
about what package to use... any help?

Thanks in advance



From MSchwartz at MedAnalytics.com  Wed Jan 19 20:55:35 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 19 Jan 2005 13:55:35 -0600
Subject: [R] importing files, columns "invade next column"
In-Reply-To: <p06100500be1465501d1b@[83.132.28.242]>
References: <p06100500be1465501d1b@[83.132.28.242]>
Message-ID: <1106164536.7114.14.camel@horizons.localdomain>

On Wed, 2005-01-19 at 19:28 +0000, Tiago R Magalhaes wrote:
> Thanks very much Mark and Prof Ripley
> 
> a) using sep='\t' when using read.table() helps somewhat
> 
> there is still a problem:I cannot get all the lines:
> df <- read.table('file.txt', fill=T, header=T, sep='\t')
> dim(df)
>   9543  195
> 
> while with the shorter file (11 cols) I get all the rows
> dim(df)
>   15797    11
> 
> I have looked at row 9544 where the file seems to stop reading, but I 
> cannot see in any of the cols an obvious reason for this to happen. 
> Any ideas why? Maybe there is one col that is stopping the reading 
> process and that column is not one of the 11 that are present in the 
> smaller file.
> 
> b) fill=T is necessary
> without fill=T, I get an error:
> "line 1892 did not have 195 elements"

Tiago,

How was this data file generated? Is it a raw file created by some other
application or was it an ASCII export, perhaps from a spreadsheet or
database program?

It seems that there is something inconsistent in the large data file,
which is either by design or perhaps the result of being corrupted by a
poor export.

It may be helpful to know how the file was generated in the effort to
assist you.

> c) help page for read.table
> I reread the help file for read.table and I would suggest to change 
> it. From what I think I am reading, the '\t' would not be needed to 
> work in my file, but it actually is:from the help page:
> 
>   If 'sep = ""' (the default for 'read.table') the separator is "white 
> space", that is one or more spaces, tabs or newlines.

Under normal circumstances, this should not be a problem, but given the
unknowns about your file, it leaves an open question as to the etiology
of the incorrect import.

Marc



From helprhelp at yahoo.com  Wed Jan 19 21:10:04 2005
From: helprhelp at yahoo.com (Weiwei Shi)
Date: Wed, 19 Jan 2005 12:10:04 -0800 (PST)
Subject: [R] suggestion on data mining book using R
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7407449F0B@dc1ex2.air.org>
Message-ID: <20050119201004.3742.qmail@web61308.mail.yahoo.com>

Hi, there:
I think I need a book on data mining book using R. I
knew 
Modern Applied Statistics with S-plus (2nd Ed)
or
Modern Applied Statistics with S (4th Ed)
might be a good choice.

But not sure if there is other better suggestion and
which one between the two is better.

thanks,

Ed



From epurdom at stanford.edu  Wed Jan 19 21:29:29 2005
From: epurdom at stanford.edu (Elizabeth Purdom)
Date: Wed, 19 Jan 2005 12:29:29 -0800
Subject: [R] name space of a package
Message-ID: <6.1.2.0.2.20050119121304.020d9df0@epurdom.pobox.stanford.edu>

I am trying to copy a R function (specifically "boot" of the boot package, 
but I don't think my problem is related to the specific function/package 
I'm using) and modify it slightly for my own use and/or understanding. For 
example, I want to understand why I'm getting an error from "func", so I 
want to create a temporary function "myfunc" that will print out 
intermediate steps so I understand why my input creates an error. But when 
the function uses programs internal to the name space, my new function 
"myfunc" cannot find the function. I've tried attachNamespace, but it's 
already attached from my library() call. I know the function is around, 
because I can type in, for example, "boot:::isMatrix" and get the function. 
Other than inserting "boot:::" in front of all the problematic functions, 
how can I get "myfunc" to run?

Note:
If I do:

myboot<-boot
edit(myboot)

this seems to work, so I do that now. But sometimes I just want to work 
with it as a separate file in my editor just like any other function I 
would write (and I would like to understand better how the name spaces work).



From wib2004 at med.cornell.edu  Wed Jan 19 21:43:16 2005
From: wib2004 at med.cornell.edu (William Briggs)
Date: Wed, 19 Jan 2005 15:43:16 -0500
Subject: [R] forcing all tick labels to plot
Message-ID: <41EEC664.2000906@med.cornell.edu>

I'm trying to find a way to force all the x-axis tick labels to plot, 
regardless whether or not they overlap or look pretty.

V is a factor with, say, 4 levels.  A call to plot(V) gives a 
histogram-like plot, one bar for each level in V.  The problem is that 
all the label names may not be plotted because some of the names are 
lengthy and would tend to overlap if plotted.

I don't care if they do, I want to see all the labels, overplotted or not.

I tried:

     plot(V,axes=F)
     d<-levels(V)
     axis(1,at=1:4,label=d)
     axis(2)

But this does nothing more than the default, that is, only some of the 
labels print.  The other problem is the "at=1:4", which is my guessing 
where the tickmarks go.  The function "axTicks(1)" may help, but it 
frequently has more tick marks then levels in "V", so again I have to 
guess which goes where.

I see this:

     c<-par()
     c$xaxp = 0.5 3.5 4

or something similar.  Is there another par() in which the tick marks 
that are calculated are given explicitly?

I have tried:

     par(cex=.7)

and that works for some V, but not for all, and it isn't a very general 
solution.

Is there some flag that I can set to force all the tick labels to plot?

Matt Briggs



From elvis at xlsolutions-corp.com  Wed Jan 19 22:02:31 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Wed, 19 Jan 2005 14:02:31 -0700
Subject: [R] Course***R/S-plus Fundamentals and Programming Techniques In
	San Diego, February 2005
Message-ID: <20050119210231.28944.qmail@webmail08.mesa1.secureserver.net>

Happy New Year!

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to 
announce  2-day "R/S-plus Fundamentals and Programming 
Techniques" in San Diego: www.xlsolutions-corp.com/training.htm 

****San Diego, CA ---------------------------- February 21st-22nd, 2005

Reserve your seat now at the early bird rates! Payment due AFTER 
the class.


Course Description:

This two-day beginner to intermediate R/S-plus course focuses on a 
broad spectrum of topics, from reading raw data to a comparison of R 
and S. We will learn the essentials of data manipulation, graphical 
visualization and R/S-plus programming. We will explore statistical 
data analysis tools,including graphics with data sets. How to enhance 
your plots. We will perform basic statistics and fit linear regression
models. Participants are encouraged to bring data for interactive 
sessions


With the following outline:

- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)


Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com 
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm 
Please let us know if you and your colleagues are interested in this 
classto take advantage of group discount. Register now to secure your 
seat! Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com 
elvis at xlsolutions-corp.com



From MSchwartz at MedAnalytics.com  Wed Jan 19 22:12:13 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 19 Jan 2005 15:12:13 -0600
Subject: [R] forcing all tick labels to plot
In-Reply-To: <41EEC664.2000906@med.cornell.edu>
References: <41EEC664.2000906@med.cornell.edu>
Message-ID: <1106169134.7114.38.camel@horizons.localdomain>

On Wed, 2005-01-19 at 15:43 -0500, William Briggs wrote:
> I'm trying to find a way to force all the x-axis tick labels to plot, 
> regardless whether or not they overlap or look pretty.
> 
> V is a factor with, say, 4 levels.  A call to plot(V) gives a 
> histogram-like plot, one bar for each level in V.  The problem is that 
> all the label names may not be plotted because some of the names are 
> lengthy and would tend to overlap if plotted.
> 
> I don't care if they do, I want to see all the labels, overplotted or not.
> 
> I tried:
> 
>      plot(V,axes=F)
>      d<-levels(V)
>      axis(1,at=1:4,label=d)
>      axis(2)
> 
> But this does nothing more than the default, that is, only some of the 
> labels print.  The other problem is the "at=1:4", which is my guessing 
> where the tickmarks go.  The function "axTicks(1)" may help, but it 
> frequently has more tick marks then levels in "V", so again I have to 
> guess which goes where.
> 
> I see this:
> 
>      c<-par()
>      c$xaxp = 0.5 3.5 4
> 
> or something similar.  Is there another par() in which the tick marks 
> that are calculated are given explicitly?
> 
> I have tried:
> 
>      par(cex=.7)
> 
> and that works for some V, but not for all, and it isn't a very general 
> solution.
> 
> Is there some flag that I can set to force all the tick labels to plot?
> 
> Matt Briggs

plot.factor() is the plot method used in this case. Since it uses
barplot() internally, it will return the bar midpoints invisibly (which
I noted is not defined as a return value in the help for plot.factor)

You can then use mtext() to force the drawing of the labels:

# Draw the plot, but not the x axis
# This example presumes V is a four level factor
mp <- plot(V, xaxt = "n")

# Create some long labels
labels <- c(paste("Very Very Very Long Label Here", 1:4, sep = ""))

# Now plot the labels below the bar midpoint
mtext(side = 1, labels, at= mp, line = 1)

That should do it.

See ?mtext and ?barplot for more information.

HTH,

Marc Schwartz



From peterm at andrew.cmu.edu  Wed Jan 19 22:36:00 2005
From: peterm at andrew.cmu.edu (Peter Muhlberger)
Date: Wed, 19 Jan 2005 16:36:00 -0500
Subject: [R] Function to modify existing data.frame--Improving R Language
In-Reply-To: <Pine.A41.4.61b.0501191019110.332684@homer11.u.washington.edu>
Message-ID: <BE143CF0.D164%peterm@andrew.cmu.edu>

Thomas & Jeff:  Thanks again for your thoughts.  The program Thomas suggests
below is elegant, but I was avoiding that because I assumed the memory
requirements and amount of time required for a large dataset would be
substantial.  Of course, it depends on what's happening 'under the hood.'
Perhaps mydata doesn't get copied and then replaced w/ a modified copy of
itself, as it seems.  R might simply have one copy & a list of updates in
memory.  I tried something like the program below w/ my data & it only takes
a couple seconds, so this looks like the elegant solution to my problem.
Thomas is right that there is a programming advantage to pass by value,
though I wonder whether for complex programming it would be enough to allow
a function to modify only one workspace object at a time.  I guess I'll see.
R is very slick.

Peter

On 1/19/05 1:31 PM, "Thomas Lumley" <tlumley at u.washington.edu> wrote:

> I don't see why
>    mydata <- some.program(mydata)
> is much less elegant than
>    mydata.someProgram()
> as a way of updating a data set. It may use more memory, but that wasn't
> the point at issue.
> 
> Of course there are advantages to the ability to pass by reference, and
> disadvantages -- the most obvious disadvantage is that it is not easy to
> tell which variables are modified by a given piece of code.
> 
> It probably wouldn't be that hard to produce something that looked like a
> data frame but was passed by reference, by wrapping it in a environment.



From jdressel at surromed.com  Wed Jan 19 22:37:09 2005
From: jdressel at surromed.com (Jon Dressel)
Date: Wed, 19 Jan 2005 13:37:09 -0800
Subject: [R] R: error while loading shared libraries: libg2c.so.o
Message-ID: <9D33C6169B1FDB419767356B8A8FEDB5010872D1@lynx.corp.surromed.com>

> If you installed an RPM, please tell us so, and which one from where

The RPM I installed is R-2.0.1-0.RH3AS.i386.rpm from Index of /bin/linux/redhat/el3/i386 located at http://cran.stat.ucla.edu/ .

> I very much suspect it is libg2c.so.0.
> It should be in /usr/lib if you have the prerequisites installed.

It is currently installed in /usr/lib64

> You need to know where R's home is.  Normally R RHOME will tell you, but 
> that might give the same error.  So use

> grep '^R_HOME_DIR' `which R`

> and then substitute it in

> R CMD ldd R_HOME_DIR/bin/exec/R

I did the above and determined that R_HOME_DIR is /usr/lib/R . I plugged in above and did not get any of the description as in your example and received the same error message:

R: error while loading shared libraries: libg2c.so.o cannot open shared 
object file: no such file or directory

Please advise.

Jon


-----Original Message-----
From:	Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent:	Wed 1/19/2005 12:18 AM
To:	Jon Dressel
Cc:	R-help at stat.math.ethz.ch
Subject:	Re: [R] R: error while loading shared libraries: libg2c.so.o
On Tue, 18 Jan 2005, Jon Dressel wrote:

> I have just installed the version of R for Linux Enterprise Server AS.

Is the `RedHat  Linux Enterprise Server AS'?  AFAIK, there is no version 
of R for any specific RHEL, but there are versions like 2.0.1.
If you installed an RPM, please tell us so, and which one from where.

> When R is launched, I receive the following error message:

> R: error while loading shared libraries: libg2c.so.o cannot open shared 
> object file: no such file or directory

I very much suspect it is libg2c.so.0.

> I have added the path to the library

It should be in /usr/lib if you have the prerequisites installed.
It may be that you have libg2c.so.0 there, linked to a non-existent 
file.

> above to the /usr/etc/ld.so.conf file and then run ldconfig -v. The 
> listing then shows that the libg2c.so.o is included in the path. 
> Please advise.

You need to know where R's home is.  Normally R RHOME will tell you, but 
that might give the same error.  So use

grep '^R_HOME_DIR' `which R`

and then substitute it in

R CMD ldd R_HOME_DIR/bin/exec/R

My FC3 system with gcc-3.4.3 and readline-5.0 installed from source gives

gannet% R CMD ldd /usr/local/lib/R/bin/exec/R
         libblas.so.3 => /usr/lib/libblas.so.3 (0x0094e000)
         libg2c.so.0 => /usr/local/lib/libg2c.so.0 (0xb7fc0000)
         libm.so.6 => /lib/tls/libm.so.6 (0x00b4f000)
         libgcc_s.so.1 => /usr/local/lib/libgcc_s.so.1 (0xb7fb7000)
         libreadline.so.5 => /usr/local/lib/libreadline.so.5 (0xb7f8d000)
         libdl.so.2 => /lib/libdl.so.2 (0x00948000)
         libncurses.so.5 => /usr/lib/libncurses.so.5 (0x04c8b000)
         libc.so.6 => /lib/tls/libc.so.6 (0x0081c000)
         /lib/ld-linux.so.2 (0x00803000)

and then see if the place it resolves libg2c.so.0 to is an actual file.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Paul.Sorenson at vision-bio.com  Wed Jan 19 23:18:34 2005
From: Paul.Sorenson at vision-bio.com (Paul Sorenson)
Date: Thu, 20 Jan 2005 09:18:34 +1100
Subject: [R] easing out of Excel
Message-ID: <5E06BFED29594F4C9C5EBE230DE320C6068027B3@ewok.vsl.com.au>

I know enough about R to be dangerous and our marketing people have asked me to "automate" some reporting.  Data comes from an SQL source and graphs and various summaries are currently created manually in Excel.  The raw information is invoicing records and the reporting is basically summaries by customer, region, product line etc.

With function such as aggregate(), hist() and pareto() (which someone on this list kindly pointed me at) I can produce something roughly equivalent to the current reports.

My question is, are there any neat R "lock out" features people here like to use on this kind of info, particularly when the output is very visual (report is intended for marketing people).

Another way of looking at this is, What kind of "hidden" information can I extract with R that the Excel solution hasn't touched?

For example, even the pareto plot mentioned earlier is something the Excel guys haven't thought of or can't easily produce.

regards

BTW the tool chain I am using goes something like:
	Production (run daily):
		DB -> SQL/python -> CSV -> R/python -> images -> network
	Presentation:
		network -> CGI/python -> browser



From kjetil at acelerate.com  Wed Jan 19 16:07:11 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 19 Jan 2005 11:07:11 -0400
Subject: [R] Data Simulation in R
In-Reply-To: <41EE5756.202@statistik.uni-dortmund.de>
References: <88EAF3512A55DF46B06B1954AEF73F7407449F0B@dc1ex2.air.org>
	<41EE5756.202@statistik.uni-dortmund.de>
Message-ID: <41EE779F.2010406@acelerate.com>

Uwe Ligges wrote:

> Doran, Harold wrote:
>
>> Thanks. But, I think I am doing that. I use rm() and gc() as the code
>> moves along. The datasets are stored as a list. Is there a way that I
>> can save the list object and call each dataset within a list one at a
>> time, or must the entire list be in memory at once?
>
>
> The list is in memory - and must be to access its elements.
> Either save the list elements to separate files, or even better make 
> use of a database.
>
> Uwe Ligges

Or, when the dat is simulated, why can't you just (re)-simulate the 
dataset just before using
it, then delete, but saving the random seed, so you can re-simulate if 
needed?

Kjetil

>
>
>
>
>> Harold
>>
>> -----Original Message-----
>> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] Sent: 
>> Wednesday, January 19, 2005 5:51 AM
>> To: Doran, Harold
>> Cc: r-help at stat.math.ethz.ch
>> Subject: Re: [R] Data Simulation in R
>>
>> Doran, Harold wrote:
>>
>>
>>> Dear List:
>>>
>>> A few weeks ago I posted some questions regarding data simulation 
>>> and received some very helpful comments, thank you. I have modified 
>>> my code accordingly and have made some progress.
>>>
>>> However, I now am facing a new challenge along similar lines. I am 
>>> attempting to simulate 250 datasets and then run the data through a 
>>> linear model. I use rm() and gc() as I move along to clean up the 
>>> workspace and preserve memory. However, my aim is to use sample 
>>> sizes of 5,000 and 10,000. By any measure this is a huge task.
>>>
>>> My machine has 2GB RAM and a Pentium 4 2.8 GHz machine with Windows
>>
>>
>> XP.
>>
>>> I have the following in the "target" section of the Windows shortcut 
>>> --max-mem-size=1812M
>>>
>>> With such large samples, R is unable to perform the analysis, at least
>>
>>
>>
>>> with the code I have developed. It halts when it runs out of memory. A
>>
>>
>>
>>> collegue subsequently constructed the simulation using another 
>>> software program with a similar computer and, while it took over night
>>
>>
>>
>>> (and then some), the program produced the results desired.
>>>
>>> I am curious if it is the case that such large simulations are out 
>>> of the grasp of R or if my code is not adequately organized to 
>>> perform the simulation.
>>>
>>> I would appreciate any thoughts or advice.
>>
>>
>>
>>
>> Don't hold all datasets (and results, if they are big) in the memory at
>> the same time!!!
>>
>> Either generate them when you use them and delete them afterwards, or
>> save them to disc an only load one by one for further analyses.
>> Also, you might want to call gc() after you removed large objects...
>>
>> Uwe Ligges
>>
>>
>>
>>
>>> Harold
>>>
>>>
>>>
>>> library(MASS)
>>> library(nlme)
>>> mu<-c(100,150,200,250)
>>> Sigma<-matrix(c(400,80,80,80,80,400,80,80,80,80,400,80,80,80,80,400),4
>>> ,4
>>> )
>>> mu2<-c(0,0,0)
>>> Sigma2<-diag(64,3)
>>> sample.size<-5000
>>> N<-250 #Number of datasets
>>> #Take a single draw from VL distribution vl.error<-mvrnorm(n=N, mu2, 
>>> Sigma2)
>>>
>>> #Step 1 Create Data
>>> Data <- lapply(seq(N), function(x)
>>> as.data.frame(cbind(1:10,mvrnorm(n=sample.size, mu, Sigma))))
>>>
>>> #Step 2 Add Vertical Linking Error
>>> for(i in seq(along=Data)){
>>> Data[[i]]$V6 <- Data[[i]]$V2
>>> Data[[i]]$V7 <- Data[[i]]$V3 + vl.error[i,1]
>>> Data[[i]]$V8 <- Data[[i]]$V4 + vl.error[i,2]
>>> Data[[i]]$V9 <- Data[[i]]$V5 + vl.error[i,3] }
>>>
>>> #Step 3 Restructure for Longitudinal Analysis long <- lapply(Data, 
>>> function(x) reshape(x, idvar="Data[[i]]$V1", 
>>> varying=list(c(names(Data[[i]])[2:5]),c(names(Data[[i]])[6:9])),
>>> v.names=c("score.1","score.2"), direction="long"))
>>>
>>> #####################
>>> #Clean up Workspace
>>> rm(Data,vl.error)
>>> gc()
>>> #####################
>>>
>>> # Step 4 Run GLS
>>>
>>> glsrun1 <- lapply(long, function(x) gls(score.1~I(time-1), data=x, 
>>> correlation=corAR1(form=~1|V1), method='ML'))
>>>
>>> # Extract intercepts and slopes
>>> int1 <- sapply(glsrun1, function(x) x$coefficient[1])
>>> slo1 <- sapply(glsrun1, function(x) x$coefficient[2])
>>>
>>> ################
>>> #Clean up workspace
>>> rm(glsrun1)
>>> gc()
>>>
>>> glsrun2 <- lapply(long, function(x) gls(score.2~I(time-1), data=x, 
>>> correlation=corAR1(form=~1|V1), method='ML'))
>>>
>>> # Extract intercepts and slopes
>>> int2 <- sapply(glsrun2, function(x) x$coefficient[1])
>>> slo2 <- sapply(glsrun2, function(x) x$coefficient[2])
>>>
>>>
>>> #Clean up workspace
>>> rm(glsrun2)
>>> gc()
>>>
>>>
>>>
>>> # Print Results
>>>
>>> cat("Original Standard Errors","\n", "Intercept","\t", 
>>> sd(int1),"\n","Slope","\t","\t", sd(slo1),"\n")
>>>
>>> cat("Modified Standard Errors","\n", "Intercept","\t", 
>>> sd(int2),"\n","Slope","\t","\t", sd(slo2),"\n")
>>>
>>> rm(list=ls())
>>> gc()
>>>
>>>     [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra





-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From p.dalgaard at biostat.ku.dk  Wed Jan 19 23:22:19 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Jan 2005 23:22:19 +0100
Subject: [R] R: error while loading shared libraries: libg2c.so.o
In-Reply-To: <9D33C6169B1FDB419767356B8A8FEDB5010872D1@lynx.corp.surromed.com>
References: <9D33C6169B1FDB419767356B8A8FEDB5010872D1@lynx.corp.surromed.com>
Message-ID: <x2ekgh12lg.fsf@biostat.ku.dk>

"Jon Dressel" <jdressel at surromed.com> writes:

> > If you installed an RPM, please tell us so, and which one from where
> 
> The RPM I installed is R-2.0.1-0.RH3AS.i386.rpm from Index of /bin/linux/redhat/el3/i386 located at http://cran.stat.ucla.edu/ .
> 
> > I very much suspect it is libg2c.so.0.
> > It should be in /usr/lib if you have the prerequisites installed.
> 
> It is currently installed in /usr/lib64

So it's an x86_64 system? If so, what do you want an i386 RPM of R
for? You could well be better off compiling from source. If you
insist, then you might get it to work by installing the 32bit version
of the library in /usr/lib, but there's no guarantee, and you probably
get into further trouble the first time you try installing a CRAN
package.
 
> > You need to know where R's home is.  Normally R RHOME will tell you, but 
> > that might give the same error.  So use
> 
> > grep '^R_HOME_DIR' `which R`
> 
> > and then substitute it in
> 
> > R CMD ldd R_HOME_DIR/bin/exec/R


> I did the above and determined that R_HOME_DIR is /usr/lib/R . I
> plugged in above and did not get any of the description as in your
> example and received the same error message:

I think that was a typo; just "ldd /usr/lib/R/bin/exec/R".


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Wed Jan 19 23:34:23 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 19 Jan 2005 22:34:23 +0000 (GMT)
Subject: [R] R: error while loading shared libraries: libg2c.so.o
In-Reply-To: <x2ekgh12lg.fsf@biostat.ku.dk>
References: <9D33C6169B1FDB419767356B8A8FEDB5010872D1@lynx.corp.surromed.com>
	<x2ekgh12lg.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.61.0501192230510.22189@gannet.stats>

On Wed, 19 Jan 2005, Peter Dalgaard wrote:

> "Jon Dressel" <jdressel at surromed.com> writes:
>
>>> If you installed an RPM, please tell us so, and which one from where
>>
>> The RPM I installed is R-2.0.1-0.RH3AS.i386.rpm from Index of 
>> /bin/linux/redhat/el3/i386 located at http://cran.stat.ucla.edu/ .
>>
>>> I very much suspect it is libg2c.so.0.
>>> It should be in /usr/lib if you have the prerequisites installed.
>>
>> It is currently installed in /usr/lib64
>
> So it's an x86_64 system? If so, what do you want an i386 RPM of R
> for? You could well be better off compiling from source. If you
> insist, then you might get it to work by installing the 32bit version
> of the library in /usr/lib, but there's no guarantee, and you probably
> get into further trouble the first time you try installing a CRAN
> package.
>
>>> You need to know where R's home is.  Normally R RHOME will tell you, but
>>> that might give the same error.  So use
>>
>>> grep '^R_HOME_DIR' `which R`
>>
>>> and then substitute it in
>>
>>> R CMD ldd R_HOME_DIR/bin/exec/R
>
>
>> I did the above and determined that R_HOME_DIR is /usr/lib/R . I
>> plugged in above and did not get any of the description as in your
>> example and received the same error message:
>
> I think that was a typo; just "ldd /usr/lib/R/bin/exec/R".

It was deliberate and correct, as R CMD sets the LD_LIBRARY_PATH.
ldd /usr/lib/R/bin/exec/R gives unsatisfied references on many systems.

I just do not believe references to libg2c.so.o, though.
                                               ^
If that is really what is being seen, that RPM is corrupt.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dmb at mrc-dunn.cam.ac.uk  Wed Jan 19 23:42:30 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Wed, 19 Jan 2005 22:42:30 +0000 (GMT)
Subject: [R] way off topic
In-Reply-To: <200501191850.j0JIowI04459@gator.dt.uh.edu>
Message-ID: <Pine.LNX.4.21.0501192242010.3262-100000@mail.mrc-dunn.cam.ac.uk>


Try one of the sci.* news groups (google groups) you should be able to
find a specific group and ask this question there.

On Wed, 19 Jan 2005, Erin Hodgess wrote:

>Dear R People:
>
>Here is another off topic question, please:
>
>Does anyone know where to find some archaelogical data (carbon
>dating), please?
>
>
>When I googled, I got reseach papers but no data.
>
>
>Thanks,
>
>Sincerely,
>Erin Hodgess
>Associate Professor
>Department of Computer and Mathematical Sciences
>University of Houston - Downtown
>mailto: hodgess at gator.uhd.edu
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From kafernan at uwaterloo.ca  Thu Jan 20 00:14:57 2005
From: kafernan at uwaterloo.ca (K Fernandes)
Date: Wed, 19 Jan 2005 18:14:57 -0500
Subject: [R] How to filter information from a table into a new table
Message-ID: <200501192315.j0JNFs012866@services04.student.cs.uwaterloo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050119/4f0c150d/attachment.pl

From kjetil at acelerate.com  Thu Jan 20 00:43:29 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 19 Jan 2005 19:43:29 -0400
Subject: [R] way off topic
In-Reply-To: <200501191850.j0JIowI04459@gator.dt.uh.edu>
References: <200501191850.j0JIowI04459@gator.dt.uh.edu>
Message-ID: <41EEF0A1.4060501@acelerate.com>

Erin Hodgess wrote:

>Dear R People:
>
>Here is another off topic question, please:
>
>Does anyone know where to find some archaelogical data (carbon
>dating), please?
>  
>
http://lib.stat.cmu.edu/DASL/

has archeology as a topic.

Kjetil

>
>When I googled, I got reseach papers but no data.
>
>
>Thanks,
>
>Sincerely,
>Erin Hodgess
>Associate Professor
>Department of Computer and Mathematical Sciences
>University of Houston - Downtown
>mailto: hodgess at gator.uhd.edu
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From parkhurs at ariel.ucs.indiana.edu  Thu Jan 20 01:55:04 2005
From: parkhurs at ariel.ucs.indiana.edu (David Parkhurst)
Date: Wed, 19 Jan 2005 19:55:04 -0500
Subject: [R] font size in console
Message-ID: <41EF0168.6060505@ariel.ucs.indiana.edu>

I'm using R in a statistics class, and when I project the console, the 
font is smaller than ideal.  I've checked the faq's, the manual, and the 
  help system as best I can, and I don't see how to change the font size.

Can it be changed from within a session, or will I have to ask the folks 
who installed the program on the server I use in classes to set it 
(assuming that can be done)?

Thanks for any help.

David Parkhurst



From p.connolly at hortresearch.co.nz  Thu Jan 20 02:00:53 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Thu, 20 Jan 2005 14:00:53 +1300
Subject: [R] 3d bar plot
In-Reply-To: <BF74FADD4B44554CA7E53D0B5242CD6A01FC6428@evd-s7014.evd.admin.c h>
References: <BF74FADD4B44554CA7E53D0B5242CD6A01FC6428@evd-s7014.evd.admin.ch >
Message-ID: <20050120010053.GI22446@hortresearch.co.nz>

On Tue, 18-Jan-2005 at 07:50AM +0100, Lorenz.Gygax at fat.admin.ch wrote:

|> > This graph -> 
|> > http://www.math.hope.edu/~tanis/dallas/images/disth36.gif
|> > is an example I found at
|> > http://www.math.hope.edu/~tanis/dallas/disth1.html
|> > created by Maple.
|> > 
|> > Does anybody know how to create something similar in R?
|> > 
|> > I have a feeling it could be possible using scatterplot3d
|> > (perhaps with type=h, the fourth example in help('scatterplot3d')?),
|> > but I cannot figure it out.
|> 
|> Sorry to butt in with a more fundamental question. Is this really the kind
|> of graph we want to cultivate and support? In my oppinion, it is hardly ever
|> necessary to have a graph in 3D or even in higher dimensions (one certain
|> exception is if one tries to spin a higly dimensional dataset in search of
|> patterns as you can do in ggobi and there might certainly be others).
|> 
|> At least the graph presented in the example does - in my eyes - not warrant
|> a 3D plot. Why not just draw curves for each of the n's in a plot of 'A'
|> against 'row'? This would enable a reader to make straightforward
|> comparisons of the curves and allow to estimate the height of the 'columns'
|> along the 'A'-axis much more easily.

I'd agree that 7 curves of different line type and/or colour would
make the whole story clearer.  More informative labels would help
also.  What is a "row" and a "column"?  The rainbow eye candy adds no
information and consequently does nothing to improve the information
to ink ratio.  Of course, that's not important in a world where form
takes precedence over substance which I think is what is called "the
real world".

Dejectedly yours,

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From Tom.Mulholland at dpi.wa.gov.au  Thu Jan 20 02:06:58 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 20 Jan 2005 09:06:58 +0800
Subject: [R] How to filter information from a table into a new table
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA6B@afhex01.dpi.wa.gov.au>

temp <- table(rpois(100,5),rpois(100,1))
temp
temp[temp[,1] > 1,]
temp[temp[,1] == 1,]
temp[temp[,1] == 2,]
temp[temp[,1] == 3,]

Of course you have not made it obvious if you are using the term table to mean any output that looks like a table, but in general terms you will find information in the documentation that comes with R, such as "An Introduction to R."

In your case 

anything.you.like <- T[T[,1] == 1,]

Tom

> -----Original Message-----
> From: K Fernandes [mailto:kafernan at uwaterloo.ca]
> Sent: Thursday, 20 January 2005 7:15 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How to filter information from a table into a new table
> 
> 
> Hello,
> 
>  
> 
> Say I have a table T as follows
> 
>  
> 
> A b
> 
> 1 2
> 
> 1 3
> 
> 2 2
> 
> 2 3
> 
> ...etc.
> 
>  
> 
> And I would like create a table C from this existing table T, but only
> including rows where A=1.
> 
>  
> 
> How might I do this?
> 
>  
> 
> Thanks,
> 
>  
> 
> K - please reply to kafernan at uwaterloo.ca as well as I am on 
> the digest
> version :)
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Thu Jan 20 02:06:54 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 19 Jan 2005 20:06:54 -0500
Subject: [R] font size in console
Message-ID: <3A822319EB35174CA3714066D590DCD50994E558@usrymx25.merck.com>

It makes a very real difference which platform you're on.  Please don't
forget to mention that.

If you're using Rgui on Windows, use the menu Edit / GUI preferences... to
set the font.  You'll have to save the setting, and then re-start R for it
to take effect.

If running ESS under (X?)Emacs, I believe you can change the font
dynamically, but I don't remember how that's done...

Andy

> From: David Parkhurst
> 
> I'm using R in a statistics class, and when I project the 
> console, the 
> font is smaller than ideal.  I've checked the faq's, the 
> manual, and the 
>   help system as best I can, and I don't see how to change 
> the font size.
> 
> Can it be changed from within a session, or will I have to 
> ask the folks 
> who installed the program on the server I use in classes to set it 
> (assuming that can be done)?
> 
> Thanks for any help.
> 
> David Parkhurst
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From tiago17 at socrates.Berkeley.EDU  Thu Jan 20 02:12:31 2005
From: tiago17 at socrates.Berkeley.EDU (Tiago R Magalhaes)
Date: Thu, 20 Jan 2005 01:12:31 +0000
Subject: [R] importing files, columns "invade next column"
In-Reply-To: <1106164536.7114.14.camel@horizons.localdomain>
References: <p06100500be1465501d1b@[83.132.28.242]>
	<1106164536.7114.14.camel@horizons.localdomain>
Message-ID: <p06100502be146dcf1ad1@[83.132.28.242]>

Thanks again Marc for your help.

At this point I already have the whole file as a data.frame in R (via 
Splus dump and then R source) so my problem for this specific problem 
is solved.

I had changed my file in Excel and thought everything was fine but 
apparently it wasn't. What program is used to display a tab file 
separated in columns that doesn't corrupt the data?

I tried again from the initial file and a very simple:
x <- read.table('file.txt', header=T, sep='\t') works fine. The 
sep='\t' is very important, otherwise the columns are imported in the 
wrong places when there are empty spaces next to them
I would suggest again advising people to use sep='\t' for tab 
delimited files in the help page for read.data.

##

If anyone is interested in a detailed history of the problem:

I had gotten my initial by exporting from Splus6.1, windows 2000 as a 
tab delimited file.

I tried to open the file in R, it didn't work and I opened the file 
in EXCEL and substituted the empty cells with NA. I saved the file as 
txt file - tab delimited. This was the file that I could not read 
only 9543 lines instead of the 15797 that the file is. The file is 
probably corrupted through the use of Excel, so I guess the lesson is 
don't do this in Excel.

I went back to Splus, exported a new tab delimited file and tried again:

x <- read.table('file.txt', header=T, sep='\t') #works fine

x <- read.table('file.txt', header=T) #gives an error
Error in scan(file = file, what = what, sep = sep, quote = quote, dec = dec,  :
	line 1 did not have 194 elements

x <- read.table('file.txt', header=T, fill=T) #wrong columns take 
empty (NA) space

x <- read.table('file.txt', header=T, fill=T, sep='\t') #works fine


>On Wed, 2005-01-19 at 19:28 +0000, Tiago R Magalhaes wrote:
>>  Thanks very much Mark and Prof Ripley
>>
>>  a) using sep='\t' when using read.table() helps somewhat
>>
>>  there is still a problem:I cannot get all the lines:
>>  df <- read.table('file.txt', fill=T, header=T, sep='\t')
>>  dim(df)
>>    9543  195
>>
>>  while with the shorter file (11 cols) I get all the rows
>>  dim(df)
>>    15797    11
>>
>>  I have looked at row 9544 where the file seems to stop reading, but I
>>  cannot see in any of the cols an obvious reason for this to happen.
>>  Any ideas why? Maybe there is one col that is stopping the reading
>>  process and that column is not one of the 11 that are present in the
>>  smaller file.
>>
>>  b) fill=T is necessary
>>  without fill=T, I get an error:
>>  "line 1892 did not have 195 elements"
>
>Tiago,
>
>How was this data file generated? Is it a raw file created by some other
>application or was it an ASCII export, perhaps from a spreadsheet or
>database program?
>
>It seems that there is something inconsistent in the large data file,
>which is either by design or perhaps the result of being corrupted by a
>poor export.
>
>It may be helpful to know how the file was generated in the effort to
>assist you.
>
>>  c) help page for read.table
>>  I reread the help file for read.table and I would suggest to change
>>  it. From what I think I am reading, the '\t' would not be needed to
>>  work in my file, but it actually is:from the help page:
>>
>>    If 'sep = ""' (the default for 'read.table') the separator is "white
>>  space", that is one or more spaces, tabs or newlines.
>
>Under normal circumstances, this should not be a problem, but given the
>unknowns about your file, it leaves an open question as to the etiology
>of the incorrect import.
>
>Marc



From Tom.Mulholland at dpi.wa.gov.au  Thu Jan 20 02:23:13 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 20 Jan 2005 09:23:13 +0800
Subject: [R] font size in console
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA6C@afhex01.dpi.wa.gov.au>

I use windows, so I can't state that the same is true of other operating systems.

There is a file which is in the etc folder of the R directory called Rconsole. 

part of mine looks like

## Font.
# Please use only fixed width font.
# If font=FixedFont the system fixed font is used; in this case
# points and style are ignored. If font begins with "TT ", only
# True Type fonts are searched for.
font = TT Courier New
points = 10
style = normal # Style can be normal, bold, italic


Just change it.

In the windows GUI, under Edit you can make these changes and save the resulting Rconsole file. There may be some difficulties if you do not have write access to this directory. 

There are a number of places where Rconsole can live depending upon whether or not you wish to have a local or global effect. ?Rconsole gives you more information

Tom 

> -----Original Message-----
> From: David Parkhurst [mailto:parkhurs at ariel.ucs.indiana.edu]
> Sent: Thursday, 20 January 2005 8:55 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] font size in console
> 
> 
> I'm using R in a statistics class, and when I project the 
> console, the 
> font is smaller than ideal.  I've checked the faq's, the 
> manual, and the 
>   help system as best I can, and I don't see how to change 
> the font size.
> 
> Can it be changed from within a session, or will I have to 
> ask the folks 
> who installed the program on the server I use in classes to set it 
> (assuming that can be done)?
> 
> Thanks for any help.
> 
> David Parkhurst
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From MSchwartz at MedAnalytics.com  Thu Jan 20 02:54:05 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 19 Jan 2005 19:54:05 -0600
Subject: [R] importing files, columns "invade next column"
In-Reply-To: <p06100502be146dcf1ad1@[83.132.28.242]>
References: <p06100500be1465501d1b@[83.132.28.242]>
	<1106164536.7114.14.camel@horizons.localdomain>
	<p06100502be146dcf1ad1@[83.132.28.242]>
Message-ID: <1106186045.7114.76.camel@horizons.localdomain>

On Thu, 2005-01-20 at 01:12 +0000, Tiago R Magalhaes wrote:
> Thanks again Marc for your help.

Happy to help.

> At this point I already have the whole file as a data.frame in R (via 
> Splus dump and then R source) so my problem for this specific problem 
> is solved.
> 
> I had changed my file in Excel and thought everything was fine but 
> apparently it wasn't.

I think that this is the source of the problem. Excel has known problems
when generating an ASCII delimited export file, especially when there
are blank cells. It can result in an inconsistent number of columns
being exported, which is in turn consistent with what you are
experiencing here. 

>  What program is used to display a tab file 
> separated in columns that doesn't corrupt the data?

There are a variety of them, from data editors to simple file viewing
programs to data format conversion programs. 

Some good (free) options are referenced in Chapter 8 of the R Data
Import/Export manual. These generally include avoiding the export step
and reading the data in the Excel file directly or for example, using
OO.org's Calc spreadsheet application, which does not have the same
problem with the export process.

HTH,

Marc



From kafernan at uwaterloo.ca  Thu Jan 20 03:20:22 2005
From: kafernan at uwaterloo.ca (K Fernandes)
Date: Wed, 19 Jan 2005 21:20:22 -0500
Subject: [R] Using lm with quadratic term
In-Reply-To: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA6B@afhex01.dpi.wa.gov.au>
Message-ID: <200501200221.j0K2LH013733@services04.student.cs.uwaterloo.ca>

Hello,
I would like to use lm to model the equation y=x^2.

However, when I use

z<-lm(formula=y~x^2)
summary(z)

I obtain results that are equivalent to when I use 

z<-lm(formula=y~x)
summary(z)

That is, using x instead of x^2.

However, I do get different results when I use

z<-lm(formula=y~log(x))
summary(z)

Does anyone know why this might be the case?  Any ideas are appreciated.

Thank you,
K



From Achim.Zeileis at wu-wien.ac.at  Thu Jan 20 03:33:36 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 20 Jan 2005 03:33:36 +0100
Subject: [R] Using lm with quadratic term
In-Reply-To: <200501200221.j0K2LH013733@services04.student.cs.uwaterloo.ca>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA6B@afhex01.dpi.wa.gov.au>
	<200501200221.j0K2LH013733@services04.student.cs.uwaterloo.ca>
Message-ID: <20050120033336.4e7bbce3.Achim.Zeileis@wu-wien.ac.at>

On Wed, 19 Jan 2005 21:20:22 -0500 K Fernandes wrote:

> Hello,
> I would like to use lm to model the equation y=x^2.
> 
> However, when I use
> 
> z<-lm(formula=y~x^2)
> summary(z)
> 
> I obtain results that are equivalent to when I use 
> 
> z<-lm(formula=y~x)
> summary(z)

In formulas "^" specifies interactions. "^2" selects all second order
interactions from the preceeding term.

If you want the arithmetic function "^" you have to insulate the term in
I(), i.e., try to fit
  lm(y ~ I(x^2))

See ?formula for more details.
Z

> That is, using x instead of x^2.
> 
> However, I do get different results when I use
> 
> z<-lm(formula=y~log(x))
> summary(z)
> 
> Does anyone know why this might be the case?  Any ideas are
> appreciated.
> 
> Thank you,
> K
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Thu Jan 20 03:42:46 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 19 Jan 2005 21:42:46 -0500
Subject: [R] Using lm with quadratic term
Message-ID: <3A822319EB35174CA3714066D590DCD50994E559@usrymx25.merck.com>

I'd suggest that you read
http://cran.r-project.org/doc/manuals/R-intro.html#Formulae-for-statistical-
models  before proceeding.  It will help a lot, really.

Andy

> From: K Fernandes
> 
> Hello,
> I would like to use lm to model the equation y=x^2.
> 
> However, when I use
> 
> z<-lm(formula=y~x^2)
> summary(z)
> 
> I obtain results that are equivalent to when I use 
> 
> z<-lm(formula=y~x)
> summary(z)
> 
> That is, using x instead of x^2.
> 
> However, I do get different results when I use
> 
> z<-lm(formula=y~log(x))
> summary(z)
> 
> Does anyone know why this might be the case?  Any ideas are 
> appreciated.
> 
> Thank you,
> K
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ligges at statistik.uni-dortmund.de  Thu Jan 20 08:32:53 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 20 Jan 2005 08:32:53 +0100
Subject: [R] suggestion on data mining book using R
In-Reply-To: <20050119201004.3742.qmail@web61308.mail.yahoo.com>
References: <20050119201004.3742.qmail@web61308.mail.yahoo.com>
Message-ID: <41EF5EA5.8020706@statistik.uni-dortmund.de>

Weiwei Shi wrote:
> Hi, there:
> I think I need a book on data mining book using R. I
> knew 
> Modern Applied Statistics with S-plus (2nd Ed)
> or
> Modern Applied Statistics with S (4th Ed)
> might be a good choice.
> 
> But not sure if there is other better suggestion and
> which one between the two is better.


Well, the authors seldom debase later editions ...

Uwe



> thanks,
> 
> Ed
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From vito_ricci at yahoo.com  Thu Jan 20 09:01:22 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 20 Jan 2005 09:01:22 +0100 (CET)
Subject: [R] Re: suggestion on data mining book using R
Message-ID: <20050120080122.38369.qmail@web41210.mail.yahoo.com>

Hi,

see these links:
http://www.liacc.up.pt/~ltorgo/DataMiningWithR/
http://sawww.epfl.ch/SIC/SA/publications/FI01/fi-sp-1/sp-1-page45.html

Brian D. Ripley, Datamining: Large Databases and
Methods, in Proceedings  of "useR! 2004 - The R User
Conference", may 2004
http://www.ci.tuwien.ac.at/Conferences/useR-2004/Keynotes/Ripley.pdf

looking for a book I suggest:

Trevor Hastie , Robert Tibshirani,  Jerome Friedman, 
The Elements of Statistical Learning: Data Mining,
Inference, and Prediction, 2001, Springer-Verlag.
http://www-stat.stanford.edu/~tibs/ElemStatLearn/

B.D. Ripley, Pattern Recognition and Neural Networks
http://www.stats.ox.ac.uk/~ripley/PRbook/

Hoping I helped you.
Best
Vito


you wrote:
Hi, there:
I think I need a book on data mining book using R. I
knew 
Modern Applied Statistics with S-plus (2nd Ed)
or
Modern Applied Statistics with S (4th Ed)
might be a good choice.

But not sure if there is other better suggestion and
which one between the two is better.

thanks,

Ed

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From ales.ziberna at guest.arnes.si  Thu Jan 20 09:38:59 2005
From: ales.ziberna at guest.arnes.si (=?windows-1250?Q?Ale=9A_=8Eiberna?=)
Date: Thu, 20 Jan 2005 09:38:59 +0100
Subject: [R] Interpreting Rprof output
Message-ID: <009701c4fecb$9912aa50$0109f9c2@ales>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050120/e850f820/attachment.pl

From kalibera at nenya.ms.mff.cuni.cz  Thu Jan 20 09:59:20 2005
From: kalibera at nenya.ms.mff.cuni.cz (Tomas Kalibera)
Date: Thu, 20 Jan 2005 09:59:20 +0100
Subject: [R] Creating a custom connection to read from multiple files
Message-ID: <41EF72E8.9030903@nenya.ms.mff.cuni.cz>

Hello,

is it possible to create my own connection which I could use with
read.table or scan ? I would like to create a connection that would read
from multiple files in sequence (like if they were concatenated),
possibly with an option to skip first n lines of each file. I would like
to avoid using platform specific scripts for that... (currently I invoke
"/bin/cat" from R to create a concatenation of all those files).

Thanks,

Tomas



From ripley at stats.ox.ac.uk  Thu Jan 20 10:11:06 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 20 Jan 2005 09:11:06 +0000 (GMT)
Subject: [R] Creating a custom connection to read from multiple files
In-Reply-To: <41EF72E8.9030903@nenya.ms.mff.cuni.cz>
References: <41EF72E8.9030903@nenya.ms.mff.cuni.cz>
Message-ID: <Pine.LNX.4.61.0501200908090.4145@gannet.stats>

On Thu, 20 Jan 2005, Tomas Kalibera wrote:

> is it possible to create my own connection which I could use with

Yes.  In a sense, all the connections are custom connections written by 
someone.

> read.table or scan ? I would like to create a connection that would read
> from multiple files in sequence (like if they were concatenated),
> possibly with an option to skip first n lines of each file. I would like
> to avoid using platform specific scripts for that... (currently I invoke
> "/bin/cat" from R to create a concatenation of all those files).

I would use pipes, but a pure R solution is to process the files to an 
anonymous file() connection and then read that.

However, what is wrong with reading a file at a time and combining the 
results in R using rbind?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kjetilv at mi.uib.no  Thu Jan 20 11:27:15 2005
From: kjetilv at mi.uib.no (Kjetil Vestfossen)
Date: Thu, 20 Jan 2005 11:27:15 +0100
Subject: [R] confidence intervals in Manova and Mancova in Splus
Message-ID: <200501201127.15963.kjetilv@mi.uib.no>

Anyone

I'm wondering how to make confidence intervals (bonferroni or simultaneous) 
when using Manova and Mancova in Splus. I 'm doing manova with four variables 
on length and four variables on weight (of salmon). The measuring is done on 
different time points. I'm working on my master in the field between 
biostatistics and fishery biology. If anyone knows a good book on mancova and 
manova in Splus, I would be grateful for any advice.




-- 
Best regards
Kjetil Vestfossen



From kalibera at nenya.ms.mff.cuni.cz  Thu Jan 20 11:32:23 2005
From: kalibera at nenya.ms.mff.cuni.cz (Tomas Kalibera)
Date: Thu, 20 Jan 2005 11:32:23 +0100
Subject: [R] Creating a custom connection to read from multiple files
In-Reply-To: <Pine.LNX.4.61.0501200908090.4145@gannet.stats>
References: <41EF72E8.9030903@nenya.ms.mff.cuni.cz>
	<Pine.LNX.4.61.0501200908090.4145@gannet.stats>
Message-ID: <41EF88B7.20502@nenya.ms.mff.cuni.cz>


Dear Prof Ripley,

thanks for your suggestions, it's very nice one can create custom 
connections directly in R and I think it is what I need just now.

> However, what is wrong with reading a file at a time and combining the 
> results in R using rbind?
>
Well, the problem is performance. If I concatenate all those files, they 
have around 8MB, can grow to tens of MBs in near future.

Both concatenating and reading from a single file by scan takes 5 
seconds (which is almost OK).

However, reading individual files by read.table and rbinding one by one 
( samples=rbind(samples, newSamples ) takes minutes. The same is when I 
concatenate lists manually. Scan does not help significantly. I guess 
there is some overhead in detecting dimensions of objects in rbind (?) 
or re-allocation or copying data ?

Best regards,

Tomas Kalibera



From ripley at stats.ox.ac.uk  Thu Jan 20 11:58:11 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 20 Jan 2005 10:58:11 +0000 (GMT)
Subject: [R] Creating a custom connection to read from multiple files
In-Reply-To: <41EF88B7.20502@nenya.ms.mff.cuni.cz>
References: <41EF72E8.9030903@nenya.ms.mff.cuni.cz>
	<Pine.LNX.4.61.0501200908090.4145@gannet.stats>
	<41EF88B7.20502@nenya.ms.mff.cuni.cz>
Message-ID: <Pine.LNX.4.61.0501201056330.24223@gannet.stats>

On Thu, 20 Jan 2005, Tomas Kalibera wrote:

>
> Dear Prof Ripley,
>
> thanks for your suggestions, it's very nice one can create custom connections 
> directly in R and I think it is what I need just now.
>
>> However, what is wrong with reading a file at a time and combining the 
>> results in R using rbind?
>> 
> Well, the problem is performance. If I concatenate all those files, they have 
> around 8MB, can grow to tens of MBs in near future.
>
> Both concatenating and reading from a single file by scan takes 5 seconds 
> (which is almost OK).
>
> However, reading individual files by read.table and rbinding one by one ( 
> samples=rbind(samples, newSamples ) takes minutes. The same is when I 
> concatenate lists manually. Scan does not help significantly. I guess there 
> is some overhead in detecting dimensions of objects in rbind (?) or 
> re-allocation or copying data ?

rbind is vectorized so you are using it (way) suboptimally.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From marco.sandri at economia.univr.it  Thu Jan 20 12:10:03 2005
From: marco.sandri at economia.univr.it (Marco Sandri)
Date: Thu, 20 Jan 2005 12:10:03 +0100
Subject: [R] Problem loading a library
Message-ID: <008601c4fee0$989864b0$f88f7450@msandri>

Hi.
I have R (Ver 2.0) correctly running on a Suse 9.0
Linux machine.

I correclty installed the "Logic Regression" LogicReg library
(by the command: R CMD INSTALL LogicReg)
developed by Ingo Ruczinski and Charles Kooperberg :
http://bear.fhcrc.org/~ingor/logic/html/program.html

When I try to load the library in R by the command:
library(LogicReg)
I get the following error:
Error in dyn.load(x, as.logical(local), as.logical(now)) :
unable to load shared library
"/usr/lib/R/library/LogicReg/libs/LogicReg.so":
/usr/lib/R/library/LogicReg/libs/LogicReg.so: cannot map zero-fill pages:
Cannot allocate memory
Error in library(LogicReg) : .First.lib failed

How could I solve the problem?
Thanks in advance for your kind help.
Marco



From BPikouni at CNTUS.JNJ.COM  Thu Jan 20 13:21:07 2005
From: BPikouni at CNTUS.JNJ.COM (Pikounis, Bill [CNTUS])
Date: Thu, 20 Jan 2005 07:21:07 -0500
Subject: [R] easing out of Excel
Message-ID: <E5382FD31214D6118FF40002A541DECE1185E98A@CNTUSMAEXS4.na.jnj.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050120/facf31da/attachment.pl

From kjetil at acelerate.com  Thu Jan 20 13:38:08 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Thu, 20 Jan 2005 08:38:08 -0400
Subject: [R] confidence intervals in Manova and Mancova in Splus
In-Reply-To: <200501201127.15963.kjetilv@mi.uib.no>
References: <200501201127.15963.kjetilv@mi.uib.no>
Message-ID: <41EFA630.5000605@acelerate.com>

Kjetil Vestfossen wrote:

>Anyone
>
>I'm wondering how to make confidence intervals (bonferroni or simultaneous) 
>when using Manova and Mancova in Splus. I 'm doing manova with four variables 
>on length and four variables on weight (of salmon). The measuring is done on 
>different time points. I'm working on my master in the field between 
>biostatistics and fishery biology. If anyone knows a good book on mancova and 
>manova in Splus, I would be grateful for any advice.
>
>
>
>
>  
>
I beleave there is a mailing list for S-Plus which is more appropriate 
for this Question!

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From andy_liaw at merck.com  Thu Jan 20 13:51:29 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 20 Jan 2005 07:51:29 -0500
Subject: [R] Creating a custom connection to read from multiple
 files
Message-ID: <3A822319EB35174CA3714066D590DCD50994E55A@usrymx25.merck.com>

> From: Prof Brian Ripley
> 
> On Thu, 20 Jan 2005, Tomas Kalibera wrote:
> 
> > Dear Prof Ripley,
> >
> > thanks for your suggestions, it's very nice one can create 
> custom connections 
> > directly in R and I think it is what I need just now.
> >
> >> However, what is wrong with reading a file at a time and 
> combining the 
> >> results in R using rbind?
> >> 
> > Well, the problem is performance. If I concatenate all 
> those files, they have 
> > around 8MB, can grow to tens of MBs in near future.
> >
> > Both concatenating and reading from a single file by scan 
> takes 5 seconds 
> > (which is almost OK).
> >
> > However, reading individual files by read.table and 
> rbinding one by one ( 
> > samples=rbind(samples, newSamples ) takes minutes. The same 
> is when I 
> > concatenate lists manually. Scan does not help 
> significantly. I guess there 
> > is some overhead in detecting dimensions of objects in rbind (?) or 
> > re-allocation or copying data ?
> 
> rbind is vectorized so you are using it (way) suboptimally.

Here's an example:

>  ## Create a 500 x 100 data matrix.
>  x <- matrix(rnorm(5e4), 500, 100)
>  ## Generate 50 filenames.
>  fname <- paste("f", formatC(1:50, width=2, flag="0"), ".txt", sep="")
>  ## Write the data to files 50 times.
>  for (f in fname) write(t(x), file=f, ncol=ncol(x))
>  
>  ## Read the files into a list of data frames.
>  system.time(datList <- lapply(fname, read.table, header=FALSE),
gcFirst=TRUE)
[1] 11.91  0.05 12.33    NA    NA
>  ## Specify colClasses to speed up.
>  system.time(datList <- lapply(fname, read.table,
colClasses=rep("numeric", 100)),
+              gcFirst=TRUE)
[1] 10.69  0.07 10.79    NA    NA
>  ## Stack them together.
>  system.time(dat <- do.call("rbind", datList), gcFirst=TRUE)
[1] 5.34 0.09 5.45   NA   NA
>  
>  ## Use matrices instead of data frames.
>  system.time(datList <- lapply(fname, 
+      function(f) matrix(scan(f), ncol=100, byrow=TRUE)), gcFirst=TRUE)
Read 50000 items
...
Read 50000 items
[1]  9.49  0.08 15.06    NA    NA
>  system.time(dat <- do.call("rbind", datList), gcFirst=TRUE)
[1] 0.09 0.03 0.12   NA   NA
>  ## Clean up the files.
>  unlink(fname)

A couple of points:

- Usually specifying colClasses will make read.table() quite a bit 
  faster, even though it's only marginally faster here.  Look back
  in the list archive to see examples.

- If your data files are all numerics (as in this example), 
  storing them in matrices will be much more efficient.  Note
  the difference in rbind()ing the 50 data frames and 50 
  matrices (5.34 seconds vs. 0.09!).  rbind.data.frame()
  needs to ensure that the resulting data frame has unique
  rownames (a requirement for a legit data frame), and
  that's probably taking a big chunk of the time.

Andy

 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From HDoran at air.org  Thu Jan 20 14:06:27 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 20 Jan 2005 08:06:27 -0500
Subject: [R] Constructing Matrices
Message-ID: <88EAF3512A55DF46B06B1954AEF73F74075D0527@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050120/b2f67dfd/attachment.pl

From ripley at stats.ox.ac.uk  Thu Jan 20 14:23:55 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 20 Jan 2005 13:23:55 +0000 (GMT)
Subject: [R] Problem loading a library
In-Reply-To: <008601c4fee0$989864b0$f88f7450@msandri>
References: <008601c4fee0$989864b0$f88f7450@msandri>
Message-ID: <Pine.LNX.4.61.0501201316230.6270@gannet.stats>

On Thu, 20 Jan 2005, Marco Sandri wrote:

> Hi.
> I have R (Ver 2.0) correctly running on a Suse 9.0
> Linux machine.

32- or 64-bit?

> I correclty installed the "Logic Regression" LogicReg library
> (by the command: R CMD INSTALL LogicReg)
> developed by Ingo Ruczinski and Charles Kooperberg :
> http://bear.fhcrc.org/~ingor/logic/html/program.html
>
> When I try to load the library in R by the command:
> library(LogicReg)
> I get the following error:
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
> unable to load shared library
> "/usr/lib/R/library/LogicReg/libs/LogicReg.so":
> /usr/lib/R/library/LogicReg/libs/LogicReg.so: cannot map zero-fill pages:
> Cannot allocate memory
> Error in library(LogicReg) : .First.lib failed
>
> How could I solve the problem?

Use a different machine?  That package works on all of mine, 32- and 
64-bit.

BTW, the posting guide does suggest you contact the package authors first, 
so what do they say?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Thu Jan 20 14:40:52 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 20 Jan 2005 08:40:52 -0500
Subject: [R] Constructing Matrices
Message-ID: <3A822319EB35174CA3714066D590DCD50994E55B@usrymx25.merck.com>

I'm still not clear on exactly what your question is.  If you can plug in
the numbers you want in, say, the lower triangular portion, you can copy
those to the upper triangular part easily; something like:

m[upper.tri(m)] <- m[lower.tri(m)]

Is that what you're looking for?

Andy

> From: Doran, Harold
> 
> Dear List:
> 
> I am working to construct a matrix of a particular form. For the most
> part, developing the matrix is simple and is built as follows:
> 
> vl.mat<-matrix(c(0,0,0,0,0,64,0,0,0,0,64,0,0,0,0,64),nc=4)
> 
> Now to expand this matrix to be block-diagonal, I do the following:
> 
> sample.size <- 100 # number of individual students
> I<- diag(sample.size)
> bd.mat<-kronecker(I,vl.mat)
> 
> This creates a block-diagonal matrix with variances along the diagonal
> and covariances within-student to be zero (I am working with
> longitudinal student achievement data). However, across 
> student, I want
> to have the correlation equal to 1 for each variance term. To
> illustrate, here is a matrix for 2 students. The goal is for the
> correlation between the second variance term for student 1 to be
> perfectly correlated with the variance term for student 2. In other
> words, I need to plug in 64 at position (6,2) and (2,6), another 64 at
> position (7,3) and (3,7) and another 64 at positions (8,4) and (4,8).
> I'm having some difficulty conceptualizing how to construct 
> this part of
> the matrix and would appreciate any thoughts.
> 
> Thank you,
> Harold
> 
> 
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
> [1,]    0    0    0    0    0    0    0    0
> [2,]    0   64    0    0    0    0    0    0
> [3,]    0    0   64    0    0    0    0    0
> [4,]    0    0    0   64    0    0    0    0
> [5,]    0    0    0    0    0    0    0    0
> [6,]    0    0    0    0    0   64    0    0
> [7,]    0    0    0    0    0    0   64    0
> [8,]    0    0    0    0    0    0    0   64
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ramesh_k77 at yahoo.com  Thu Jan 20 14:54:29 2005
From: ramesh_k77 at yahoo.com (kolluru ramesh)
Date: Thu, 20 Jan 2005 05:54:29 -0800 (PST)
Subject: [R] References
Message-ID: <20050120135430.20348.qmail@web41415.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050120/3aa7df1e/attachment.pl

From michael.watson at bbsrc.ac.uk  Thu Jan 20 14:57:55 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 20 Jan 2005 13:57:55 -0000
Subject: [R] Subsetting a data frame by a factor,
	using the level that occurs the most times
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E89C18@iahce2knas1.iah.bbsrc.reserved>

I think that title makes sense... I hope it does...

I have a data frame, one of the columns of which is a factor.  I want
the rows of data that correspond to the level in that factor which
occurs the most times.  

I can get a list by doing:

by(data,data$pattern,subset)

And go through each element of the list counting the rows, to find the
maximum....

BUT I can't help thinking there's a more elegant way of doing this....

The second part is figuring out the rows which have the maximum number
of consecutive patterns which are the same... Now that I would love some
help with... :-)

Thanks
Mick



From ramesh_k77 at yahoo.com  Thu Jan 20 14:58:57 2005
From: ramesh_k77 at yahoo.com (kolluru ramesh)
Date: Thu, 20 Jan 2005 05:58:57 -0800 (PST)
Subject: [R] Reference Material for Multiple Imputation
Message-ID: <20050120135857.3988.qmail@web41407.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050120/823990e3/attachment.pl

From ccleland at optonline.net  Thu Jan 20 15:08:10 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 20 Jan 2005 09:08:10 -0500
Subject: [R] Subsetting a data frame by a factor,	using the level that
	occurs the most times
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E89C18@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E95E89C18@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <41EFBB4A.5070604@optonline.net>

newdata <- subset(mydata, mydata$myfact == 
names(which.max(table(mydata$myfact))))

michael watson (IAH-C) wrote:
> I think that title makes sense... I hope it does...
> 
> I have a data frame, one of the columns of which is a factor.  I want
> the rows of data that correspond to the level in that factor which
> occurs the most times.  
> 
> I can get a list by doing:
> 
> by(data,data$pattern,subset)
> 
> And go through each element of the list counting the rows, to find the
> maximum....
> 
> BUT I can't help thinking there's a more elegant way of doing this....
> 
> The second part is figuring out the rows which have the maximum number
> of consecutive patterns which are the same... Now that I would love some
> help with... :-)
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From sdavis2 at mail.nih.gov  Thu Jan 20 15:16:12 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 20 Jan 2005 09:16:12 -0500
Subject: [R] Subsetting a data frame by a factor,
	using the level that occurs the most times
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E89C18@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E95E89C18@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <D669F8D3-6AED-11D9-9D53-000D933565E8@mail.nih.gov>


On Jan 20, 2005, at 8:57 AM, michael watson ((IAH-C)) wrote:

> I think that title makes sense... I hope it does...
>
> I have a data frame, one of the columns of which is a factor.  I want
> the rows of data that correspond to the level in that factor which
> occurs the most times.
>
> I can get a list by doing:
>
> by(data,data$pattern,subset)
>

see ?split.

data.split <- split(data,data$pattern)


> And go through each element of the list counting the rows, to find the
> maximum....
>

sort(sapply(data.split,nrow))

> BUT I can't help thinking there's a more elegant way of doing this....

We'll see what the other responses are....

Sean



From HDoran at air.org  Thu Jan 20 15:17:11 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 20 Jan 2005 09:17:11 -0500
Subject: [R] Constructing Matrices
Message-ID: <88EAF3512A55DF46B06B1954AEF73F74075D0551@dc1ex2.air.org>

I should probably have explained my data and model a little better.
Assume I have student achievement scores across four time points. I
estimate the model using gls() as follows

fm1 <- gls(score ~ time, long, correlation=corAR1(form=~1|stuid),
method='ML')

I can now extract the variance-covariance matrix for this model as
follows:

var.mat<-getVarCov(fm1)

Assume for sake of argument I have a sample size of 100 students. I can
expand this to the full matrix as follows
I<-diag(100)
V<-kronecker(I,var.mat)

For my particular model, the scores within each student are assumed
correlated (AR1), but across student are uncorrelated. Now, for a
particular problem I am dealing with I need to make some adjustments to
this matrix, V,  and reestimate the gls(). The adjustments I need to
make cannot be done using any of the existing varFunc classes, so I am
having to do this manually.

What I need to do is create a new matrix manually, add it to V, then
reestimate the gls. Creating this new matrix is the challenge I
currently face, let's call it v.prime. 

The issue at hand is creating v.prime to have non-zero covariance terms
across students in very specific places. The matrix I used below is only
for two students. But assume I am doing this for thousands of students.
My goal is to create a full block-diagonal covariance matrix where the
correlation across students at time two is always perfectly correlated
and the correlation at time three is always perfectly correlated across
students. So, within each block of v.prime, the variances are
uncorrelated, but across each block the variances are correlated. 

So, I need to construct v.prime such that it is one the same order of V
to make them conformable for addition. More importantly, I need the
off-diagonal elements across students to represent a perfect correlation
in very specific places. In the example below, if there was a 64 at
position (2,6) this would represent a perfect correlation between
student 1 and 2 at this point in time since the variance along the
diagonal at time 2 is 64. Since I am doing this for many students, there
would need to be a 64 between student 1 and all other students (not just
student 2) and so on.

>From here I can use R's matrix facilities to reestimate the gls.

I hope this clarifies a bit.

Harold

-----Original Message-----
From: Liaw, Andy [mailto:andy_liaw at merck.com] 
Sent: Thursday, January 20, 2005 8:41 AM
To: Doran, Harold; r-help at stat.math.ethz.ch
Subject: RE: [R] Constructing Matrices

I'm still not clear on exactly what your question is.  If you can plug
in the numbers you want in, say, the lower triangular portion, you can
copy those to the upper triangular part easily; something like:

m[upper.tri(m)] <- m[lower.tri(m)]

Is that what you're looking for?

Andy

> From: Doran, Harold
> 
> Dear List:
> 
> I am working to construct a matrix of a particular form. For the most 
> part, developing the matrix is simple and is built as follows:
> 
> vl.mat<-matrix(c(0,0,0,0,0,64,0,0,0,0,64,0,0,0,0,64),nc=4)
> 
> Now to expand this matrix to be block-diagonal, I do the following:
> 
> sample.size <- 100 # number of individual students
> I<- diag(sample.size)
> bd.mat<-kronecker(I,vl.mat)
> 
> This creates a block-diagonal matrix with variances along the diagonal

> and covariances within-student to be zero (I am working with 
> longitudinal student achievement data). However, across student, I 
> want to have the correlation equal to 1 for each variance term. To 
> illustrate, here is a matrix for 2 students. The goal is for the 
> correlation between the second variance term for student 1 to be 
> perfectly correlated with the variance term for student 2. In other 
> words, I need to plug in 64 at position (6,2) and (2,6), another 64 at

> position (7,3) and (3,7) and another 64 at positions (8,4) and (4,8).
> I'm having some difficulty conceptualizing how to construct this part 
> of the matrix and would appreciate any thoughts.
> 
> Thank you,
> Harold
> 
> 
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
> [1,]    0    0    0    0    0    0    0    0
> [2,]    0   64    0    0    0    0    0    0
> [3,]    0    0   64    0    0    0    0    0
> [4,]    0    0    0   64    0    0    0    0
> [5,]    0    0    0    0    0    0    0    0
> [6,]    0    0    0    0    0   64    0    0
> [7,]    0    0    0    0    0    0   64    0
> [8,]    0    0    0    0    0    0    0   64
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------
------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From r.hankin at soc.soton.ac.uk  Thu Jan 20 15:14:27 2005
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Thu, 20 Jan 2005 14:14:27 +0000
Subject: [R] Cauchy's theorem
Message-ID: <97E2FCA0-6AED-11D9-8EC8-000A95D86AA8@soc.soton.ac.uk>

In complex analysis, Cauchy's integral theorem states (loosely 
speaking) that the path integral
of any entire differentiable function, around any closed curve, is zero.

I would like to see this numerically, using R (and indeed I would like 
to use the
residue theorem as well).

Has anyone coded up path integration?




--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From Virginie.Rondeau at isped.u-bordeaux2.fr  Thu Jan 20 15:18:53 2005
From: Virginie.Rondeau at isped.u-bordeaux2.fr (Virginie Rondeau)
Date: Thu, 20 Jan 2005 15:18:53 +0100
Subject: [R] (no subject)
Message-ID: <41EFBDCD.30308@isped.u-bordeaux2.fr>

Hello
I would like to compare the results obtained with a classical non 
parametric proportionnal hazard model with a parametric proportionnal 
hazard model using a Weibull.

How can we obtain the equivalence of the parameters using coxph(non 
parametric model) and survreg(parametric model) ?

Thanks
Virginie



From bates at stat.wisc.edu  Thu Jan 20 15:32:29 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 20 Jan 2005 08:32:29 -0600
Subject: [R] Subsetting a data frame by a factor,	using the level that
	occurs the most times
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E89C18@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E95E89C18@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <41EFC0FD.2070800@stat.wisc.edu>

michael watson (IAH-C) wrote:
> I think that title makes sense... I hope it does...
> 
> I have a data frame, one of the columns of which is a factor.  I want
> the rows of data that correspond to the level in that factor which
> occurs the most times.  

So first you want to determine the mode (in the sense of the most 
frequently occuring value) of the factor.   One way to do this is

names(which.max(table(fac)))

Use this comparison for the subset as

subset(data, pattern == names(which.max(table(pattern))))



From gb at tal.stat.umu.se  Thu Jan 20 16:06:56 2005
From: gb at tal.stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Thu, 20 Jan 2005 16:06:56 +0100
Subject: [R] (no subject)
In-Reply-To: <41EFBDCD.30308@isped.u-bordeaux2.fr>
References: <41EFBDCD.30308@isped.u-bordeaux2.fr>
Message-ID: <20050120150656.GC6014@tal.stat.umu.se>

On Thu, Jan 20, 2005 at 03:18:53PM +0100, Virginie Rondeau wrote:
> Hello
> I would like to compare the results obtained with a classical non 
> parametric proportionnal hazard model with a parametric proportionnal 
> hazard model using a Weibull.
> 
> How can we obtain the equivalence of the parameters using coxph(non 
> parametric model) and survreg(parametric model) ?

One way of avoiding this problem is to fit the Weibull model with 'weibreg'
in the package eha.


-- 
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se



From carladefranceschi at libero.it  Thu Jan 20 16:16:29 2005
From: carladefranceschi at libero.it (carladefranceschi@libero.it)
Date: Thu, 20 Jan 2005 16:16:29 +0100
Subject: [R] Johnson transformation
Message-ID: <IAMFRH$558B58651DDA5F903BE029FE9192D6EE@libero.it>

Hello,
I'm Carla, an italian student, I'm looking for a package to transform non normal data to normality. I tried to use Box Cox, but it's not ok. There is a package to use Johnson families' transormation? Can you give me any suggestions to find free software as R that use this trasform?
Thank yuo very much
Carla




____________________________________________________________
6X velocizzare la tua navigazione a 56k? 6X Web Accelerator di Libero!
Scaricalo su INTERNET GRATIS 6X http://www.libero.it



From f.harrell at vanderbilt.edu  Thu Jan 20 16:28:02 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 20 Jan 2005 09:28:02 -0600
Subject: [R] (no subject)
In-Reply-To: <41EFBDCD.30308@isped.u-bordeaux2.fr>
References: <41EFBDCD.30308@isped.u-bordeaux2.fr>
Message-ID: <41EFCE02.3070809@vanderbilt.edu>

Virginie Rondeau wrote:
> Hello
> I would like to compare the results obtained with a classical non 
> parametric proportionnal hazard model with a parametric proportionnal 
> hazard model using a Weibull.
> 
> How can we obtain the equivalence of the parameters using coxph(non 
> parametric model) and survreg(parametric model) ?
> 
> Thanks
> Virginie

In the Design package look at the pphsm function that converts a survreg 
Weibull fit (fitted by the psm function which is an adaptation of 
survreg) to PH form.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From Charles.Annis at StatisticalEngineering.com  Thu Jan 20 16:37:35 2005
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Thu, 20 Jan 2005 10:37:35 -0500
Subject: [R] Johnson transformation
In-Reply-To: <IAMFRH$558B58651DDA5F903BE029FE9192D6EE@libero.it>
Message-ID: <200501201537.j0KFbdr5028944@hypatia.math.ethz.ch>

Greetings, Carla:

While it is possible to map any proper density into a normal through their
CDFs, that may not be useful in your case.

I suggest that you first plot your data.
?qqnorm

(Type ?qqnorm on the R command line and hit Enter.)

Are your data continuous, or do they occur in groups?  Do the data curve?
Do they look like two (or more) distinct lines?

If your data have only one mode and if they are smooth then the Box-Cox
transform should provide a symmetrical result.  Not all symmetrical
densities are normal, of course.  And if your data are discrete then using a
continuous density like the normal (or Johnson family) is inappropriate.

The purpose of "fitting" a distribution to data is usually to permit some
probability statement, like Prob(x < X) = alpha.  Why do you want to use the
Johnson family?  I am not aware of convenient methods for making such
probability statements for them.

Best wishes.


Charles Annis, P.E.
 
Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
carladefranceschi at libero.it
Sent: Thursday, January 20, 2005 10:16 AM
To: r-help
Subject: [R] Johnson transformation

Hello,
I'm Carla, an italian student, I'm looking for a package to transform non
normal data to normality. I tried to use Box Cox, but it's not ok. There is
a package to use Johnson families' transormation? Can you give me any
suggestions to find free software as R that use this trasform?
Thank yuo very much
Carla




____________________________________________________________
6X velocizzare la tua navigazione a 56k? 6X Web Accelerator di Libero!
Scaricalo su INTERNET GRATIS 6X http://www.libero.it

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bwheeler at echip.com  Thu Jan 20 16:53:33 2005
From: bwheeler at echip.com (Bob Wheeler)
Date: Thu, 20 Jan 2005 10:53:33 -0500
Subject: [R] Johnson transformation
In-Reply-To: <IAMFRH$558B58651DDA5F903BE029FE9192D6EE@libero.it>
References: <IAMFRH$558B58651DDA5F903BE029FE9192D6EE@libero.it>
Message-ID: <41EFD3FD.1010204@echip.com>

carladefranceschi at libero.it wrote:

> Hello,
> I'm Carla, an italian student, I'm looking for a package to transform non normal data to normality. I tried to use Box Cox, but it's not ok. There is a package to use Johnson families' transormation? Can you give me any suggestions to find free software as R that use this trasform?
> Thank yuo very much
> Carla
> 
> 
> 
> 
> ____________________________________________________________
> 6X velocizzare la tua navigazione a 56k? 6X Web Accelerator di Libero!
> Scaricalo su INTERNET GRATIS 6X http://www.libero.it
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

The Johnson system is in the SuppDists package.

-- 
Bob Wheeler --- http://www.bobwheeler.com/
         ECHIP, Inc. ---
Randomness comes in bunches.



From p.campbell at econ.bbk.ac.uk  Thu Jan 20 16:53:58 2005
From: p.campbell at econ.bbk.ac.uk (Campbell)
Date: Thu, 20 Jan 2005 15:53:58 +0000
Subject: [R] Compile R-2.0.1 on SPARC Solaris 5.9
Message-ID: <s1efd43b.037@markets.econ.bbk.ac.uk>

I'm using GCC 3.4.2 to build R-2.0.1 from sources downloaded from the
University of Bristol mirror. 

bash-2.05$./configure
<SNIP>
R is now configured for sparc-sun-solaris2.9

  Source directory:          .
  Installation directory:    /usr/local

  C compiler:                gcc  -g -O2
  C++ compiler:              g++  -g -O2
  Fortran compiler:          g77  -g -O2

  Interfaces supported:      X11
  External libraries:        readline
  Additional capabilities:   
  Options enabled:           R profiling

  Recommended packages:      yes

configure: WARNING: you cannot build DVI versions of the R manuals
configure: WARNING: you cannot build info or html versions of the R
manuals
configure: WARNING: you cannot build PDF versions of the R manuals
configure: WARNING: I could not determine a browser

bash-2.0.5$make
<SNIP>

bash-20.5$ make check
make[1]: Entering directory `/home/phineas/R_HOME/R-2.0.1/tests'
make[2]: Entering directory `/home/phineas/R_HOME/R-2.0.1/tests'
make[3]: Entering directory
`/home/phineas/R_HOME/R-2.0.1/tests/Examples'
make[4]: Entering directory
`/home/phineas/R_HOME/R-2.0.1/tests/Examples'
make[4]: Leaving directory `/home/phineas/R_HOME/R-2.0.1/tests/Examples'
make[4]: Entering directory
`/home/phineas/R_HOME/R-2.0.1/tests/Examples'
make[4]: *** No rule to make target `../../bin/exec/R', needed by
`base-Ex.Rout'.  Stop.
make[4]: Leaving directory `/home/phineas/R_HOME/R-2.0.1/tests/Examples'
make[3]: *** [test-Examples-Base] Error 2
make[3]: Leaving directory `/home/phineas/R_HOME/R-2.0.1/tests/Examples'
make[2]: *** [test-Examples] Error 2
make[2]: Leaving directory `/home/phineas/R_HOME/R-2.0.1/tests'
make[1]: *** [test-all-basics] Error 1
make[1]: Leaving directory `/home/phineas/R_HOME/R-2.0.1/tests'
make: *** [check] Error 2


/home/phineas/R_HOME/R-2.0.1/bin/exec is empty when presumably it should
have the executable installed.  This may be a permissions issue
as I installed a previous version of R as root.  However this required
some gymnastics as $PATH cannot be altered for root.  Do I have a
correct version of GCC for my version of Solaris?  Any help gratefully
apreciated.

Phineas Campbell



From dr.mike at ntlworld.com  Thu Jan 20 16:57:53 2005
From: dr.mike at ntlworld.com (dr mike)
Date: Thu, 20 Jan 2005 15:57:53 -0000
Subject: [R] Cauchy's theorem
In-Reply-To: <97E2FCA0-6AED-11D9-8EC8-000A95D86AA8@soc.soton.ac.uk>
Message-ID: <20050120155803.RVIN3971.aamta07-winn.mailhost.ntl.com@c400>

 I don't know about the 'in R' bit, but ISTR that Monte-Carlo (or pseudo
Monte-Carlo) Integration is a way of doing this 'numerically'. I know that
Mathematica implements the (pseudo Monte-Carlo)
Halton-Hammersley-Wozniakowski algorithm as Nintegrate. Perhaps something
equivalent has been coded by someone for WINBUGS (OPENBUGS) (accessible from
R via the BRUGS package).

HTH

Mike

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robin Hankin
Sent: 20 January 2005 14:14
To: R-help at stat.math.ethz.ch
Subject: [R] Cauchy's theorem

In complex analysis, Cauchy's integral theorem states (loosely
speaking) that the path integral
of any entire differentiable function, around any closed curve, is zero.

I would like to see this numerically, using R (and indeed I would like to
use the residue theorem as well).

Has anyone coded up path integration?




--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From christian.kamenik at ips.unibe.ch  Thu Jan 20 17:02:35 2005
From: christian.kamenik at ips.unibe.ch (Christian Kamenik)
Date: Thu, 20 Jan 2005 16:02:35 -0000
Subject: [R] glm and percentage data with many zero values
In-Reply-To: <200501201136.j0KBH1wd031095@hypatia.math.ethz.ch>
References: <200501201136.j0KBH1wd031095@hypatia.math.ethz.ch>
Message-ID: <42666EF1.7040901@ips.unibe.ch>

Dear all,

I am interested in correctly testing effects of continuous environmental 
variables and ordered factors on bacterial abundance. Bacterial 
abundance is derived from counts and expressed as percentage. My problem 
is that the abundance data contain many zero values:
Bacteria <- 
c(2.23,0,0.03,0.71,2.34,0,0.2,0.2,0.02,2.07,0.85,0.12,0,0.59,0.02,2.3,0.29,0.39,1.32,0.07,0.52,1.2,0,0.85,1.09,0,0.5,1.4,0.08,0.11,0.05,0.17,0.31,0,0.12,0,0.99,1.11,1.78,0,0,0,2.33,0.07,0.66,1.03,0.15,0.15,0.59,0,0.03,0.16,2.86,0.2,1.66,0.12,0.09,0.01,0,0.82,0.31,0.2,0.48,0.15)

First I tried transforming the data (e.g., logit) but because of the 
zeros I was not satisfied. Next I converted the percentages into integer 
values by round(Bacteria*10) or ceiling(Bacteria*10) and calculated a 
glm with a Poisson error structure; however, I am not very happy with 
this approach because it changes the original percentage data 
substantially (e.g., 0.03 becomes either 0 or 1). The same is true for 
converting the percentages into factors and calculating a multinomial or 
proportional-odds model (anyway, I do not know if this would be a 
meaningful approach).
I was searching the web and the best answer I could get was 
http://www.biostat.wustl.edu/archives/html/s-news/1998-12/msg00010.html 
in which several persons suggested quasi-likelihood. Would it be 
reasonable to use a glm with quasipoisson? If yes, how I can I find the 
appropriate variance function? Any other suggestions?

Many thanks in advance, Christian


================================


Christian Kamenik
Institute of Plant Sciences
University of Bern
Altenbergrain 21
3013 Bern
Switzerland



From andy_liaw at merck.com  Thu Jan 20 17:12:29 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 20 Jan 2005 11:12:29 -0500
Subject: [R] Subsetting a data frame by a factor, using the level
	that occurs the most times
Message-ID: <3A822319EB35174CA3714066D590DCD50994E55E@usrymx25.merck.com>

> From: Douglas Bates
> 
> michael watson (IAH-C) wrote:
> > I think that title makes sense... I hope it does...
> > 
> > I have a data frame, one of the columns of which is a 
> factor.  I want
> > the rows of data that correspond to the level in that factor which
> > occurs the most times.  
> 
> So first you want to determine the mode (in the sense of the most 
> frequently occuring value) of the factor.   One way to do this is
> 
> names(which.max(table(fac)))
> 
> Use this comparison for the subset as
> 
> subset(data, pattern == names(which.max(table(pattern))))

Just be careful that if there are ties (i.e., more than one level having the
max) which.max() will randomly pick one of them.  That may or may not be
what's desired.  If that is a possibility, Mick will need to think what he
wants in such cases.

Andy

 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From sabrina.carpentier at curie.fr  Thu Jan 20 17:44:19 2005
From: sabrina.carpentier at curie.fr (Sabrina Carpentier)
Date: Thu, 20 Jan 2005 17:44:19 +0100
Subject: [R] ROracle error
Message-ID: <035201c4ff0f$49e03800$020a140a@Gaspesie>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050120/30d6deda/attachment.pl

From greg.snow at ihc.com  Thu Jan 20 17:52:07 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Thu, 20 Jan 2005 09:52:07 -0700
Subject: [R] easing out of Excel
Message-ID: <s1ef7f4c.013@lp-msg1.co.ihc.com>

>>> "Paul Sorenson" <Paul.Sorenson at vision-bio.com> 01/19/05 03:18PM >>>
>> I know enough about R to be dangerous and our marketing people have
>> asked me to "automate" some reporting.  Data comes from an SQL
source
>> and graphs and various summaries are currently created manually in
>> Excel.  The raw information is invoicing records and the reporting
is
>> basically summaries by customer, region, product line etc.
>> 
>> With function such as aggregate(), hist() and pareto() (which
someone
>> on this list kindly pointed me at) I can produce something roughly
>> equivalent to the current reports.
>> 
>> My question is, are there any neat R "lock out" features people
here
>> like to use on this kind of info, particularly when the output is
very
>> visual (report is intended for marketing people).
>> 
>> Another way of looking at this is, What kind of "hidden"
information
>> can I extract with R that the Excel solution hasn't touched?

Since you are looking for summaries within groups, you should look at
the
lattice package and some of the plots that you can produce with it
(maybe
for each product line you can produce a lattice/trellis graph with each
panel
representing a region and different colors symbols within panels to
represent
different customers).

If we had more of an idea of what you are looking for, we could give
better
suggestions.


>> For example, even the pareto plot mentioned earlier is something
the
>> Excel guys haven't thought of or can't easily produce.
>> 
>> regards
>> 
>> BTW the tool chain I am using goes something like:
>> 	Production (run daily):
>> 		DB -> SQL/python -> CSV -> R/python -> images ->
network
>> 	Presentation:
>> 		network -> CGI/python -> browser

It looks like you want the reports fully automated and the final result
as HTML
(to be viewed with a browser), I suggest you look at the R2HTML package
and
the sweave function (this lets you write a report in HTML with r-code
in place of 
graphs and output, then a quick run through sweave and you have a final
report
in HTML ready to be viewed).

There are also several tools available for running R through CGI, go
to: 
http://www-r.project.org/ and click on "R web-servers" under the
"Related Projects"
heading in the left column to get details.

Hope this helps,


Greg Snow, Ph.D.
Statistical Data Center
greg.snow at ihc.com
(801) 408-8111



From ripley at stats.ox.ac.uk  Thu Jan 20 17:58:53 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 20 Jan 2005 16:58:53 +0000 (GMT)
Subject: [R] ROracle error
In-Reply-To: <035201c4ff0f$49e03800$020a140a@Gaspesie>
References: <035201c4ff0f$49e03800$020a140a@Gaspesie>
Message-ID: <Pine.LNX.4.61.0501201657020.17616@gannet.stats>

On Thu, 20 Jan 2005, Sabrina Carpentier wrote:

> I am running R 2.0.0 on a SunOs 5.9 machine and using Oracle 8i.1.7.0.0 (enterprise edition)
>
> and when I  try to load ROracle I receive the following error:
>
> "require(ROracle)
> Loading required package: ROracle
> Loading required package: DBI
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>        unable to load shared library "/bioinfo/local/R/lib/R/library/ROracle/libs/ROracle.so":
>  ld.so.1: /bioinfo/local/R/lib/R/bin/exec/R: fatal: relocation error: file /bioinfo/local/R/lib/R/library/ROracle/libs/ROracle.so: symbol ncrov: referenced symbol not found
> [1] FALSE"

It's not an R issue, so please ask your sysadmins for help.  But

ldd /bioinfo/local/R/lib/R/library/ROracle/libs/ROracle.so

would be a good start as I suspect your Oracle client libraries are not 
being found.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From greg.snow at ihc.com  Thu Jan 20 18:08:54 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Thu, 20 Jan 2005 10:08:54 -0700
Subject: [R] Constructing Matrices
Message-ID: <s1ef834c.084@lp-msg1.co.ihc.com>

Does the following do what you want (or at least get you closer)?

> tmp <- matrix(0,16,16)
> tmp[col(tmp)%%4 == row(tmp)%%4] <- 64
> tmp
...




Greg Snow, Ph.D.
Statistical Data Center
greg.snow at ihc.com
(801) 408-8111

>>> "Doran, Harold" <HDoran at air.org> 01/20/05 07:17AM >>>
I should probably have explained my data and model a little better.
Assume I have student achievement scores across four time points. I
estimate the model using gls() as follows

fm1 <- gls(score ~ time, long, correlation=corAR1(form=~1|stuid),
method='ML')

I can now extract the variance-covariance matrix for this model as
follows:

var.mat<-getVarCov(fm1)

Assume for sake of argument I have a sample size of 100 students. I
can
expand this to the full matrix as follows
I<-diag(100)
V<-kronecker(I,var.mat)

For my particular model, the scores within each student are assumed
correlated (AR1), but across student are uncorrelated. Now, for a
particular problem I am dealing with I need to make some adjustments
to
this matrix, V,  and reestimate the gls(). The adjustments I need to
make cannot be done using any of the existing varFunc classes, so I am
having to do this manually.

What I need to do is create a new matrix manually, add it to V, then
reestimate the gls. Creating this new matrix is the challenge I
currently face, let's call it v.prime. 

The issue at hand is creating v.prime to have non-zero covariance
terms
across students in very specific places. The matrix I used below is
only
for two students. But assume I am doing this for thousands of
students.
My goal is to create a full block-diagonal covariance matrix where the
correlation across students at time two is always perfectly correlated
and the correlation at time three is always perfectly correlated
across
students. So, within each block of v.prime, the variances are
uncorrelated, but across each block the variances are correlated. 

So, I need to construct v.prime such that it is one the same order of
V
to make them conformable for addition. More importantly, I need the
off-diagonal elements across students to represent a perfect
correlation
in very specific places. In the example below, if there was a 64 at
position (2,6) this would represent a perfect correlation between
student 1 and 2 at this point in time since the variance along the
diagonal at time 2 is 64. Since I am doing this for many students,
there
would need to be a 64 between student 1 and all other students (not
just
student 2) and so on.

>From here I can use R's matrix facilities to reestimate the gls.

I hope this clarifies a bit.

Harold

-----Original Message-----
From: Liaw, Andy [mailto:andy_liaw at merck.com] 
Sent: Thursday, January 20, 2005 8:41 AM
To: Doran, Harold; r-help at stat.math.ethz.ch 
Subject: RE: [R] Constructing Matrices

I'm still not clear on exactly what your question is.  If you can plug
in the numbers you want in, say, the lower triangular portion, you can
copy those to the upper triangular part easily; something like:

m[upper.tri(m)] <- m[lower.tri(m)]

Is that what you're looking for?

Andy

> From: Doran, Harold
> 
> Dear List:
> 
> I am working to construct a matrix of a particular form. For the most

> part, developing the matrix is simple and is built as follows:
> 
> vl.mat<-matrix(c(0,0,0,0,0,64,0,0,0,0,64,0,0,0,0,64),nc=4)
> 
> Now to expand this matrix to be block-diagonal, I do the following:
> 
> sample.size <- 100 # number of individual students
> I<- diag(sample.size)
> bd.mat<-kronecker(I,vl.mat)
> 
> This creates a block-diagonal matrix with variances along the
diagonal

> and covariances within-student to be zero (I am working with 
> longitudinal student achievement data). However, across student, I 
> want to have the correlation equal to 1 for each variance term. To 
> illustrate, here is a matrix for 2 students. The goal is for the 
> correlation between the second variance term for student 1 to be 
> perfectly correlated with the variance term for student 2. In other 
> words, I need to plug in 64 at position (6,2) and (2,6), another 64
at

> position (7,3) and (3,7) and another 64 at positions (8,4) and
(4,8).
> I'm having some difficulty conceptualizing how to construct this part

> of the matrix and would appreciate any thoughts.
> 
> Thank you,
> Harold
> 
> 
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
> [1,]    0    0    0    0    0    0    0    0
> [2,]    0   64    0    0    0    0    0    0
> [3,]    0    0   64    0    0    0    0    0
> [4,]    0    0    0   64    0    0    0    0
> [5,]    0    0    0    0    0    0    0    0
> [6,]    0    0    0    0    0   64    0    0
> [7,]    0    0    0    0    0    0   64    0
> [8,]    0    0    0    0    0    0    0   64
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html 
> 
> 


------------------------------------------------------------------------
------
Notice:  This e-mail message, together with any
attachments,...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bates at stat.wisc.edu  Thu Jan 20 18:16:50 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 20 Jan 2005 11:16:50 -0600
Subject: [R] Subsetting a data frame by a factor, using the level	that
	occurs the most times
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E55E@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E55E@usrymx25.merck.com>
Message-ID: <41EFE782.60708@stat.wisc.edu>

Liaw, Andy wrote:
>>From: Douglas Bates
>>
>>michael watson (IAH-C) wrote:
>>
>>>I think that title makes sense... I hope it does...
>>>
>>>I have a data frame, one of the columns of which is a 
>>
>>factor.  I want
>>
>>>the rows of data that correspond to the level in that factor which
>>>occurs the most times.  
>>
>>So first you want to determine the mode (in the sense of the most 
>>frequently occuring value) of the factor.   One way to do this is
>>
>>names(which.max(table(fac)))
>>
>>Use this comparison for the subset as
>>
>>subset(data, pattern == names(which.max(table(pattern))))
> 
> 
> Just be careful that if there are ties (i.e., more than one level having the
> max) which.max() will randomly pick one of them.  That may or may not be
> what's desired.  If that is a possibility, Mick will need to think what he
> wants in such cases.

According to the documentation it picks the first one.  Also, that's 
what Martin Maechler told me and he wrote the code so I trust him on 
that.  I figure that if you have to trust someone to be meticulous and 
precise then a German-speaking Swiss is a good choice.



From andy_liaw at merck.com  Thu Jan 20 18:40:29 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 20 Jan 2005 12:40:29 -0500
Subject: [R] Subsetting a data frame by a factor, using the
	level	that occurs the most times
Message-ID: <3A822319EB35174CA3714066D590DCD50994E561@usrymx25.merck.com>

> From: Douglas Bates
> 
> Liaw, Andy wrote:
> >>From: Douglas Bates
> >>
> >>michael watson (IAH-C) wrote:
> >>
> >>>I think that title makes sense... I hope it does...
> >>>
> >>>I have a data frame, one of the columns of which is a 
> >>
> >>factor.  I want
> >>
> >>>the rows of data that correspond to the level in that factor which
> >>>occurs the most times.  
> >>
> >>So first you want to determine the mode (in the sense of the most 
> >>frequently occuring value) of the factor.   One way to do this is
> >>
> >>names(which.max(table(fac)))
> >>
> >>Use this comparison for the subset as
> >>
> >>subset(data, pattern == names(which.max(table(pattern))))
> > 
> > 
> > Just be careful that if there are ties (i.e., more than one 
> level having the
> > max) which.max() will randomly pick one of them.  That may 
> or may not be
> > what's desired.  If that is a possibility, Mick will need 
> to think what he
> > wants in such cases.
> 
> According to the documentation it picks the first one.  Also, that's 
> what Martin Maechler told me and he wrote the code so I trust him on 
> that.  I figure that if you have to trust someone to be 
> meticulous and 
> precise then a German-speaking Swiss is a good choice.

My apologies!  I got it mixed up with max.col, which does the tie-breaking. 

Andy



From HDoran at air.org  Thu Jan 20 19:16:13 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 20 Jan 2005 13:16:13 -0500
Subject: [R] Windows Front end-crash error
Message-ID: <88EAF3512A55DF46B06B1954AEF73F74075D0615@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050120/0cca37b5/attachment.pl

From tvandaelen at scitegic.com  Thu Jan 20 20:46:03 2005
From: tvandaelen at scitegic.com (Ton van Daelen)
Date: Thu, 20 Jan 2005 11:46:03 -0800
Subject: [R] Cross-validation accuracy in SVM
Message-ID: <830D8D4719112B418ABBC3A0EBA95812C67C4B@webmail.scitegic.com>

Hi all -

I am trying to tune an SVM model by optimizing the cross-validation
accuracy. Maximizing this value doesn't necessarily seem to minimize the
number of misclassifications. Can anyone tell me how the
cross-validation accuracy is defined? In the output below, for example,
cross-validation accuracy is 92.2%, while the number of correctly
classified samples is (1476+170)/(1476+170+4) = 99.7% !?

Thanks for any help.

Regards - Ton

---
Parameters:
   SVM-Type:  C-classification 
 SVM-Kernel:  radial 
       cost:  8 
      gamma:  0.007 

Number of Support Vectors:  1015

 ( 148 867 )

Number of Classes:  2 

Levels: 
 false true

5-fold cross-validation on training data:

Total Accuracy: 92.24242 
Single Accuracies:
 90 93.33333 94.84848 92.72727 90.30303 

Contingency Table
           predclasses
origclasses false true
      false 1476     0
      true     4   170



From andy_liaw at merck.com  Thu Jan 20 20:59:30 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 20 Jan 2005 14:59:30 -0500
Subject: [R] Cross-validation accuracy in SVM
Message-ID: <3A822319EB35174CA3714066D590DCD50994E563@usrymx25.merck.com>

The 99.7% accuracy you quoted, I take it, is the accuracy on the training
set.  If so, that number hardly means anything (other than, perhaps,
self-fulfilling prophecy).  Usually what one would want is for the model to
be able to predict data that weren't used to train the model with high
accuracy.  That's what cross-validation tries to emulate.  It gives you an
estimate of how well you can expect your model to do on data that the model
has not seen.

Andy

> From: Ton van Daelen
> 
> Hi all -
> 
> I am trying to tune an SVM model by optimizing the cross-validation
> accuracy. Maximizing this value doesn't necessarily seem to 
> minimize the
> number of misclassifications. Can anyone tell me how the
> cross-validation accuracy is defined? In the output below, 
> for example,
> cross-validation accuracy is 92.2%, while the number of correctly
> classified samples is (1476+170)/(1476+170+4) = 99.7% !?
> 
> Thanks for any help.
> 
> Regards - Ton
> 
> ---
> Parameters:
>    SVM-Type:  C-classification 
>  SVM-Kernel:  radial 
>        cost:  8 
>       gamma:  0.007 
> 
> Number of Support Vectors:  1015
> 
>  ( 148 867 )
> 
> Number of Classes:  2 
> 
> Levels: 
>  false true
> 
> 5-fold cross-validation on training data:
> 
> Total Accuracy: 92.24242 
> Single Accuracies:
>  90 93.33333 94.84848 92.72727 90.30303 
> 
> Contingency Table
>            predclasses
> origclasses false true
>       false 1476     0
>       true     4   170
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From cpan at utk.edu  Thu Jan 20 21:46:46 2005
From: cpan at utk.edu (Pan, Chongle)
Date: Thu, 20 Jan 2005 15:46:46 -0500
Subject: [R] Straight-line fitting with errors in both coordinates
Message-ID: <420B1870@webmail.utk.edu>

Hi All,
I want to fit a straight line into a group of two-dimensional data points with 
errors in both x and y coordinates. I found there is an algorithm provided in 
"NUMERICAL RECIPES IN C" http://www.library.cornell.edu/nr/bookcpdf/c15-3.pdf

I'm wondering if there is a similar function for this implemented in R. And 
how can I change the objective function, from example, from sum of squared 
error to sum of absolute error?

Regards,
Chongle



From sway at tanox.com  Thu Jan 20 21:41:40 2005
From: sway at tanox.com (Shawn Way)
Date: Thu, 20 Jan 2005 14:41:40 -0600
Subject: [R] easing out of Excel
Message-ID: <2DBF8A8E1A1AEE4AB3618AC4D6BF3088072095@houston.tanox.net>

Definitely check out the lattice package.

One other option is to use sweave/latex mixed with RODBC.  This can be
used to produce PDF's for easy distribution as well.  I would also
consider operating this in a batch mode, the R/sweave/latex works very
well this way.


Shawn Way, PE
Engineering Manager

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Greg Snow
Sent: Thursday, January 20, 2005 10:52 AM
To: r-help at stat.math.ethz.ch; Paul.Sorenson at vision-bio.com
Subject: Re: [R] easing out of Excel

>>> "Paul Sorenson" <Paul.Sorenson at vision-bio.com> 01/19/05 03:18PM >>>
>> I know enough about R to be dangerous and our marketing people have
>> asked me to "automate" some reporting.  Data comes from an SQL
source
>> and graphs and various summaries are currently created manually in
>> Excel.  The raw information is invoicing records and the reporting
is
>> basically summaries by customer, region, product line etc.
>> 
>> With function such as aggregate(), hist() and pareto() (which
someone
>> on this list kindly pointed me at) I can produce something roughly
>> equivalent to the current reports.
>> 
>> My question is, are there any neat R "lock out" features people
here
>> like to use on this kind of info, particularly when the output is
very
>> visual (report is intended for marketing people).
>> 
>> Another way of looking at this is, What kind of "hidden"
information
>> can I extract with R that the Excel solution hasn't touched?

Since you are looking for summaries within groups, you should look at
the
lattice package and some of the plots that you can produce with it
(maybe
for each product line you can produce a lattice/trellis graph with each
panel
representing a region and different colors symbols within panels to
represent
different customers).

If we had more of an idea of what you are looking for, we could give
better
suggestions.


>> For example, even the pareto plot mentioned earlier is something
the
>> Excel guys haven't thought of or can't easily produce.
>> 
>> regards
>> 
>> BTW the tool chain I am using goes something like:
>> 	Production (run daily):
>> 		DB -> SQL/python -> CSV -> R/python -> images ->
network
>> 	Presentation:
>> 		network -> CGI/python -> browser

It looks like you want the reports fully automated and the final result
as HTML
(to be viewed with a browser), I suggest you look at the R2HTML package
and
the sweave function (this lets you write a report in HTML with r-code
in place of 
graphs and output, then a quick run through sweave and you have a final
report
in HTML ready to be viewed).

There are also several tools available for running R through CGI, go
to: 
http://www-r.project.org/ and click on "R web-servers" under the
"Related Projects"
heading in the left column to get details.

Hope this helps,


Greg Snow, Ph.D.
Statistical Data Center
greg.snow at ihc.com
(801) 408-8111

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Thu Jan 20 22:22:37 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 20 Jan 2005 15:22:37 -0600
Subject: [R] Cross-validation accuracy in SVM
In-Reply-To: <830D8D4719112B418ABBC3A0EBA95812C67C4B@webmail.scitegic.com>
References: <830D8D4719112B418ABBC3A0EBA95812C67C4B@webmail.scitegic.com>
Message-ID: <41F0211D.9020406@vanderbilt.edu>

Ton van Daelen wrote:
> Hi all -
> 
> I am trying to tune an SVM model by optimizing the cross-validation
> accuracy. Maximizing this value doesn't necessarily seem to minimize the
> number of misclassifications. Can anyone tell me how the
> cross-validation accuracy is defined? In the output below, for example,
> cross-validation accuracy is 92.2%, while the number of correctly
> classified samples is (1476+170)/(1476+170+4) = 99.7% !?
> 
> Thanks for any help.
> 
> Regards - Ton

Percent correctly classified is an improper scoring rule.  The percent 
is maximized when the predicted values are bogus.  In addition, one can 
add a very important predictor and have the % actually decrease.

Frank Harrell

> 
> ---
> Parameters:
>    SVM-Type:  C-classification 
>  SVM-Kernel:  radial 
>        cost:  8 
>       gamma:  0.007 
> 
> Number of Support Vectors:  1015
> 
>  ( 148 867 )
> 
> Number of Classes:  2 
> 
> Levels: 
>  false true
> 
> 5-fold cross-validation on training data:
> 
> Total Accuracy: 92.24242 
> Single Accuracies:
>  90 93.33333 94.84848 92.72727 90.30303 
> 
> Contingency Table
>            predclasses
> origclasses false true
>       false 1476     0
>       true     4   170
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From minhan.science at gmail.com  Thu Jan 20 22:40:58 2005
From: minhan.science at gmail.com (Min-Han Tan)
Date: Thu, 20 Jan 2005 16:40:58 -0500
Subject: [R] Successful installation of R 2.0.1 on SUSE 9.1
Message-ID: <7902152a05012013401d466030@mail.gmail.com>

Hi,

We managed to compile R 2.0.1 on 64-bit SUSE Linux 9.1 on a HP
Proliant setup fairly uneventfully by following instructions on the R
installation guide. We did encounter a minor hiccup in setting up x11,
a problem which we note has been raised 4 or 5 times previously, but
this was overcome thanks to this recent post by Peter Dalgaard on SUSE
9.1 and R.https://stat.ethz.ch/pipermail/r-help/2005-January/062397.html,
as well as previous comments on the mailing list.

One clarification on that post may be helpful: there are only 3
additional developmental packages required for successful X11
installation.

XFree86-devel-4.3.99.902-30
fontconfig-devel
freetype2-devel

These were not available in YAST (9.1 SUSE), but were located in

http://ftp.suse.com/pub/suse/ 

Once again, thanks to the assorted R gurus and wizards for making this
mailing list such a great resource.

Regards,
Min-Han Tan



From efg at stowers-institute.org  Thu Jan 20 22:49:32 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Thu, 20 Jan 2005 15:49:32 -0600
Subject: [R] how to call R in delphi?
References: <E0B8EBA2EBD83347A588FC4840F8C9E90666BC@ex115.smic-sh.com>
	<loom.20050119T121646-759@post.gmane.org>
Message-ID: <csp91f$16p$1@sea.gmane.org>

"Dieter Menne" <dieter.menne at menne-biomed.de> wrote in message
news:loom.20050119T121646-759 at post.gmane.org...
> To call R from Delphi, you may try
> http://www.menne-biomed.de/download/RDComDelphi.zip.

I downloaded this file and tried to compile the RDCom project using Delphi 5
and Delphi 7 but I get this message from both  compilers:

[Fatal Error] STATCONNECTORCLNTLib_TLB.pas(406): Could not create output
file 'c:\program
files\borland\delphi7\Twain\d5\dcu\STATCONNECTORCLNTLib_TLB.dcu'



The "\db\dcu" in the path in this error message was a bit curious so I
looked at

Project | Options | Directories/Conditionals



Unit output directory:

$(DELPHI)\Twain\d5\dcu



Search path:

$(DELPHI)\Compon;C:\D2Pr\CascCont\COMPON;$(DELPHI)\Source\Toolsapi



On my "vanilla" Delphi 5 and Delphi 7 installations all of the directories
for the Unit output directory and Search path are invalid for the RDCom.dpr
project.

If I delete the Unit output directory, I then get 17 compilation errors, all
like this:
[Error] RCom.pas(115): Undeclared identifier: 'VarType'
[Error] RCom.pas(141): Undeclared identifier: 'VarArrayDimCount'
[Error] RCom.pas(123): Undeclared identifier: 'VarArrayHighBound'
. . .

All of the above seems to happen whether or not I install
STATCONNECTORCLNTLib_TLB.pas and STATCONNECTORSRVLib_TLP.pas as components
(i.e., Component | Install Component | <browse to .pas file> | Open | OK |
Compile).  Am I supposed to do this at some point?

Can you give me any clues how to make this work?  Something seems to be
missing.

>   Example program showing use of R from Delphi.
>   Connecting to R via COM using Neuwirth's StatConnectorSrvLib
>   Uses RCom.pas, which is a simple Delphi wrapper for passing
>   commands, integer and double arrays.
>   See http://cran.r-project.org/contrib/extra/dcom
>   By:  dieter.menne at menne-biomed.de

I'm not sure I understand this either.  I went to
http://cran.r-project.org/contrib/extra/dcom

I read this documentation:
http://cran.r-project.org/contrib/extra/dcom/RSrv135.html

I downloaded and installed the R(COM) server (and rebooted)
http://cran.r-project.org/contrib/extra/dcom/RSrv135.exe

So, how I can I call "R" from Delphi using R(COM)?  Something seems to be
missing.

Duncan Murdoch's suggestion about direct calls to R.dll looks interesting,
but a complete working example would be nice.

Thanks for any help with this.

efg
Earl F. Glynn
Scientific Programmer
Stowers Institute for Medical Research



From robin_gruna at hotmail.com  Thu Jan 20 23:53:27 2005
From: robin_gruna at hotmail.com (Robin Gruna)
Date: Thu, 20 Jan 2005 23:53:27 +0100
Subject: [R] Barplot at the axes of another plot
Message-ID: <BAY103-DAV6A9D2811BA2DEDDCF414A87810@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050120/fd73b0a6/attachment.pl

From MSchwartz at MedAnalytics.com  Fri Jan 21 00:10:54 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 20 Jan 2005 17:10:54 -0600
Subject: [R] Barplot at the axes of another plot
In-Reply-To: <BAY103-DAV6A9D2811BA2DEDDCF414A87810@phx.gbl>
References: <BAY103-DAV6A9D2811BA2DEDDCF414A87810@phx.gbl>
Message-ID: <1106262654.25337.16.camel@horizons.localdomain>

On Thu, 2005-01-20 at 23:53 +0100, Robin Gruna wrote:
> Hi,
> I want to draw a barplot at the axes of another plot. I saw that with
> two histogramms and a scatterplot in a R graphics tutorial somewhere
> on the net, seemed to be a 2d histogramm. Can someone figure out what
> I mean and give me a hint to create such a graphic? Thank you very
> much, 
> Robin  


See the examples in ?layout, which has the scatterplot with the marginal
histograms.

HTH,

Marc Schwartz



From murdoch at stats.uwo.ca  Fri Jan 21 01:08:38 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 21 Jan 2005 00:08:38 +0000
Subject: [R] Windows Front end-crash error
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F74075D0615@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F74075D0615@dc1ex2.air.org>
Message-ID: <c4f0v0lmb1r2q6oj56q48cibmnq68dkml3@4ax.com>

On Thu, 20 Jan 2005 13:16:13 -0500, "Doran, Harold" <HDoran at air.org>
wrote :

>Dear List:
>
>First, many thanks to those who offered assistance while I constructed
>code for the simulation. I think I now have code that resolves most of
>the issues I encountered with memory.
>
>While the code works perfectly for smallish datasets with small sample
>sizes, it arouses a windows-based error with samples of 5,000 and 250
>datasets. The error is a dialogue box with the following:
>
>"R for Windows terminal front-end has encountered a problem and needs to
>close.  We are sorry for the inconvenience. If you were in the middle of
>something, the information you were working on might be lost."
>
>The new code is below. Can anyone suggest whether this error is derived
>from inefficient code, or is it derived based on a windows specific
>issue that can somehow be resolved and if so, how.

It looks to me like an nlme bug.  I get the error in R-patched (built
Jan 15).  DrMingw shows this at the time of the crash:

Rgui.exe caused an Access Violation at location 01c8ae4b in module
nlme.dll Reading from location 7f1e8f18.

Registers:
eax=7f210020 ebx=00000000 ecx=01368c50 edx=ffffb1df esi=00004e20
edi=01108918
eip=01c8ae4b esp=0022d1d0 ebp=0022d208 iopl=0         nv up ei ng nz
ac po nc
cs=001b  ss=0023  ds=0023  es=0023  fs=003b  gs=0000
efl=00000296

Call stack:
01C8AE4B  nlme.dll:01C8AE4B  gls_loglik
004E5E77  R.dll:004E5E77  do_dotCode
...

I changed the loop to print some status lines, and it failed after the
first time it printed gls2...

library(MASS)
library(nlme)

set.seed(123)

mu<-c(100,150,200,250)
Sigma<-matrix(c(400,80,80,80,80,400,80,80,80,80,400,80,80,80,80,400),4,4
)
mu2<-c(0,0,0)
LE<-8^2 #Linking Error
Sigma2<-diag(LE,3)
sample.size<-5000
N<-100 #Number of datasets
#Take a single draw from VL distribution
vl.error<-mvrnorm(n=N, mu2, Sigma2)

intercept1 <- 0
slope1     <- 0
intercept2 <- 0
slope2     <- 0

for(i in 1:N){
print(i)
flush.console()
temp <- data.frame(ID=seq(1:sample.size),mvrnorm(n=sample.size,
mu,Sigma))    

temp$X5 <- temp$X1
temp$X6 <- temp$X2 + vl.error[i,1] 
temp$X7 <- temp$X3 + vl.error[i,2]
temp$X8 <- temp$X4 + vl.error[i,3] 

print("reshape...")
flush.console()

long<-reshape(temp, idvar="ID",
varying=list(c("X1","X2","X3","X4"),c("X5","X6","X7","X8")), 
v.names=c("score.1","score.2"),direction='long')

print("gls1...")
flush.console()

glsrun1 <- gls(score.1~I(time-1), data=long, 
correlation=corAR1(form=~1|ID), method='ML')

print("gls2...")
flush.console()

glsrun2 <- gls(score.2~I(time-1), data=long, 
correlation=corAR1(form=~1|ID), method='ML')

intercept1[[i]] <- glsrun1$coefficient[1]
slope1[[i]]     <- glsrun1$coefficient[2]
intercept2[[i]] <- glsrun2$coefficient[1]
slope2[[i]]     <- glsrun2$coefficient[2]
}

Hopefully this will let someone more familiar with nlme track it down.

Duncan Murdoch



From robin_gruna at hotmail.com  Fri Jan 21 01:48:26 2005
From: robin_gruna at hotmail.com (Robin Gruna)
Date: Fri, 21 Jan 2005 01:48:26 +0100
Subject: [R] Plots with same x-axes
Message-ID: <BAY103-DAV6A46AAAA16BD92D4509E687820@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050121/d79d5853/attachment.pl

From kafernan at uwaterloo.ca  Fri Jan 21 02:51:04 2005
From: kafernan at uwaterloo.ca (K Fernandes)
Date: Thu, 20 Jan 2005 20:51:04 -0500
Subject: [R] Plotting points from two vectors onto the same graph
In-Reply-To: <20050120033336.4e7bbce3.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <200501210152.j0L1q1314815@services04.student.cs.uwaterloo.ca>

Hello,

I have three vectors defined as follows:

> x<-c(10,20,30,40,50)
> y1<-c(154,143,147,140,148)
> y2<-c(178,178,171,188,180)

I would like to plot y1 vs x and y2 vs x on the same graph.  How might I do
this?  I have looked through a help file on plots but could not find the
answer to plotting multiple plots on the same graph.

Thank you for your help,
K



From judielife at yahoo.com  Fri Jan 21 03:19:19 2005
From: judielife at yahoo.com (Judie Z)
Date: Thu, 20 Jan 2005 18:19:19 -0800 (PST)
Subject: [R] Need help to transform data into co-occurence matix
Message-ID: <20050121021919.48500.qmail@web30809.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050120/3bbf9c44/attachment.pl

From Tom.Mulholland at dpi.wa.gov.au  Fri Jan 21 03:31:48 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Fri, 21 Jan 2005 10:31:48 +0800
Subject: [R] Plotting points from two vectors onto the same graph
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA6E@afhex01.dpi.wa.gov.au>

?points has this example

plot(-4:4, -4:4, type = "n")# setting up coord. system
points(rnorm(200), rnorm(200), col = "red")
points(rnorm(100)/2, rnorm(100)/2, col = "blue", cex = 1.5)

In general you might want to check out the keyword section of the help, in particular the Graphics section which has an entry called aplot for ways to add to existing plots.

Tom

> -----Original Message-----
> From: K Fernandes [mailto:kafernan at uwaterloo.ca]
> Sent: Friday, 21 January 2005 9:51 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Plotting points from two vectors onto the same graph
> 
> 
> Hello,
> 
> I have three vectors defined as follows:
> 
> > x<-c(10,20,30,40,50)
> > y1<-c(154,143,147,140,148)
> > y2<-c(178,178,171,188,180)
> 
> I would like to plot y1 vs x and y2 vs x on the same graph.  
> How might I do
> this?  I have looked through a help file on plots but could 
> not find the
> answer to plotting multiple plots on the same graph.
> 
> Thank you for your help,
> K
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From MSchwartz at MedAnalytics.com  Fri Jan 21 03:47:45 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 20 Jan 2005 20:47:45 -0600
Subject: [R] Plotting points from two vectors onto the same graph
Message-ID: <1106275665.25337.52.camel@horizons.localdomain>

On Thu, 2005-01-20 at 20:51 -0500, K Fernandes wrote: 
> Hello,
> 
> I have three vectors defined as follows:
> 
> > x<-c(10,20,30,40,50)
> > y1<-c(154,143,147,140,148)
> > y2<-c(178,178,171,188,180)
> 
> I would like to plot y1 vs x and y2 vs x on the same graph.  How might I do
> this?  I have looked through a help file on plots but could not find the
> answer to plotting multiple plots on the same graph.
> 
> Thank you for your help,
> K

First, when posting a new query, please do not do so by replying to an
existing post. Your post is now listed in the archive linked to an
entirely different thread.

The easiest way to do this is to use the matplot() function:

x <- c(10,20,30,40,50)
y1 <- c(154,143,147,140,148)
y2 <- c(178,178,171,188,180)

# now do the plot. cbind() the two sets of y values
# and the x values with be cycled for each
matplot(x, cbind(y1, y2), col = c("red", "blue"))

See ?matplot for more information.

HTH,

Marc Schwartz



From MSchwartz at MedAnalytics.com  Fri Jan 21 04:30:53 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 20 Jan 2005 21:30:53 -0600
Subject: [R] Plots with same x-axes
In-Reply-To: <BAY103-DAV6A46AAAA16BD92D4509E687820@phx.gbl>
References: <BAY103-DAV6A46AAAA16BD92D4509E687820@phx.gbl>
Message-ID: <1106278254.3497.18.camel@horizons.localdomain>

On Fri, 2005-01-21 at 01:48 +0100, Robin Gruna wrote:
> Hi,
> I want to plot two graphics on top of each other with layout(), a
> scatterplot and a barplot. The problems are the different x-axes
> ratios of the plots. How can I align the two x-axes?   Thank you very
> much,
> Robin


Robin,

Here is an example:

# Set the layout, smaller plot on top for the 
# barplot region
nf <- layout(c(2, 1), heights = c(1, 3))
layout.show(nf)

# Create the data
x <- rnorm(50)
y <- rnorm(50)

# Set the margins for the scatterplot so that they will match with the
# barplot settings
par(mar = c(3, 3, 0, 3))

# now do the scatterplot
plot(x, y)

# Get the hist data for x
xhist <- hist(x, plot = FALSE)

# Set the margins for the barplot to use more of the plot
# region
par(mar = c(0, 3, 1, 3))

# now plot that barplot on top
# Set the 'space' argument to 0 so that the bars are
# next to each other
barplot(xhist$counts, axes = FALSE, space = 0)

HTH,

Marc Schwartz



From Paul.Sorenson at vision-bio.com  Fri Jan 21 04:33:16 2005
From: Paul.Sorenson at vision-bio.com (Paul Sorenson)
Date: Fri, 21 Jan 2005 14:33:16 +1100
Subject: [R] easing out of Excel
Message-ID: <5E06BFED29594F4C9C5EBE230DE320C6068027B5@ewok.vsl.com.au>

Thanks for the responses to this question, I fully realise it is a rather open question and the "open" pointers are the kind of thing I am looking for.

I will look into the lattice package and layout.

Regarding the HTML output, the current "tool chain" assets that I have have been refactored over time and are almost totally driven by config files so they suit my purposes very well.  I will look into other possibilities at a later date.

For those looking for a more rigorous specification of the problem, you are well justified in this.  I was deliberately fuzzy since managers just want "stuff" and I thought casting a wide net would pay off.  The problem is to summarise information which is nothing more than sales data.  The kinds of columns I am dealing with look like:

date, customer, invoice_no, product, amount, sales_region, etc etc.

Managers want to know things like:
	- which products are doing well
	- which regions are doing well
	- who are good customers
	- etc

To me these are simple aggregates and sorts, with visual presentations to match.

I figure a bit of effort, R can extract considerably more useful information from the data.

To be honest I am just evolving it as I go, using an existing spreadsheet as a basis.  I try something and if it is useful then great, if not, put it down to learning.

cheers



From andy_liaw at merck.com  Fri Jan 21 04:41:24 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 20 Jan 2005 22:41:24 -0500
Subject: [R] Need help to transform data into co-occurence matix
Message-ID: <3A822319EB35174CA3714066D590DCD50994E56E@usrymx25.merck.com>

> From: Judie Z
> 
> Dear R experts,
> I have the data in the following fomat(from some kind of card 
> sorting process)
>  
> ID  Category   Card numbers
> 1   1               1,2,5
> 1   2               3,4
> 2   1               1,2
> 2   2               3
> 2   3               4,5
>  
> I want to transform this data into two co-occurence matrix 
> (one for each ID)
> -- For ID 1
>    1 2 3 4 5
> 1 1 1 0 0 1
> 2 1 1 0 0 1
> 3 0 0 1 1 0
> 4 0 0 1 1 0
> 5 1 1 0 0 1
>  
> -- For ID 2
>    1 2 3 4 5
> 1  1 1 0 0 0
> 2  1 1 0 0 0
> 3  0 0 1 0 0 
> 4  0 0 0 1 1
> 5  0 0 0 1 1
>  
> The columns and rows are representing the card numbers. All 
> "0"s mean the card numbers are not in the same category, vice versa.
>  
> Is there any way I can to this in R?
> I would really appreciate your help. 

It depends on how the data are structured in R.  Here's an example (I'm sure
others can come up with more clever/efficient ways):

> cardlist <- list(c(1,2,5), c(3,4))
> indicator <- function(i, n=max(i)) { x <- rep(0, n); x[i] <- 1; x}
> matrix(rowSums(sapply(cardlist, function(i) crossprod(t(indicator(i,
5))))), nrow=5)
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    1    0    0    1
[2,]    1    1    0    0    1
[3,]    0    0    1    1    0
[4,]    0    0    1    1    0
[5,]    1    1    0    0    1

which is the matrix for ID 1 in your example.

HTH,
Andy
  
> Judie, Tie
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From tfliao at uiuc.edu  Fri Jan 21 04:49:52 2005
From: tfliao at uiuc.edu (Tim F Liao)
Date: Thu, 20 Jan 2005 21:49:52 -0600
Subject: [R] Need help to transform data into
 co-occurence matix
Message-ID: <b840f613.c1f03ca5.81fff00@expms6.cites.uiuc.edu>

Judie,

You may want to see if the MedlineR library, which has a
program for constructing co-occurrence matrices, will work for
you.  The program can be found at:

http://dbsr.duke.edu/pub/MedlineR/

Have fun with it,

Tim Liao
Professor of Sociology & Statistics
University of Illinois
Urbana, IL 61801

---- Original message ----
>Date: Thu, 20 Jan 2005 18:19:19 -0800 (PST)
>From: Judie Z <judielife at yahoo.com>  
>Subject: [R] Need help to transform data into co-occurence
matix  
>To: r-help at stat.math.ethz.ch
>
>Dear R experts,
>I have the data in the following fomat(from some kind of card
sorting process)
> 
>ID  Category   Card numbers
>1   1               1,2,5
>1   2               3,4
>2   1               1,2
>2   2               3
>2   3               4,5
> 
>I want to transform this data into two co-occurence matrix
(one for each ID)
>-- For ID 1
>   1 2 3 4 5
>1 1 1 0 0 1
>2 1 1 0 0 1
>3 0 0 1 1 0
>4 0 0 1 1 0
>5 1 1 0 0 1
> 
>-- For ID 2
>   1 2 3 4 5
>1  1 1 0 0 0
>2  1 1 0 0 0
>3  0 0 1 0 0 
>4  0 0 0 1 1
>5  0 0 0 1 1
> 
>The columns and rows are representing the card numbers. All
"0"s mean the card numbers are not in the same category, vice
versa.
> 
>Is there any way I can to this in R?
>I would really appreciate your help. 
> 
>Judie, Tie
> 
> 
>
>		
>---------------------------------
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Tom.Mulholland at dpi.wa.gov.au  Fri Jan 21 05:04:59 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Fri, 21 Jan 2005 12:04:59 +0800
Subject: [R] easing out of Excel
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA6F@afhex01.dpi.wa.gov.au>

I hesitate to add this comment since it either completely confuses people or they take to it very quickly.

The data that you are using is mostly categorical. I expect that tables will have been used in the past and that to acertain extent the graphics are suppossed to help with getting a quick understanding of the data.

There is a package called vcd (Visualizing Categorical Data) which is useful for analysing this type of data. I like the use of the mosaicplot and in particular the shade parameter (which is based on standardized residuals). If set up properly it can be used to very quickly identify sales regions that are doing significantly better than they were last year, customers who have significantly reduced purchases. Basically if you can produce a table that would give this information then a shaded mosaicplot can efficiently highlight the  significant parts of the table.

They take a little bit of getting used to at first, but if you need to analyse this type of data they take a lot of the guess work out of making commentary on the data. How useful they are depends upon the users, who as I have said seem to be polarised in their reactions to the output.

Tom

> -----Original Message-----
> From: Paul Sorenson [mailto:Paul.Sorenson at vision-bio.com]
> Sent: Friday, 21 January 2005 11:33 AM
> To: r-help at stat.math.ethz.ch
> Subject: RE: [R] easing out of Excel
> 
> 
> Thanks for the responses to this question, I fully realise it 
> is a rather open question and the "open" pointers are the 
> kind of thing I am looking for.
> 
> I will look into the lattice package and layout.
> 
> Regarding the HTML output, the current "tool chain" assets 
> that I have have been refactored over time and are almost 
> totally driven by config files so they suit my purposes very 
> well.  I will look into other possibilities at a later date.
> 
> For those looking for a more rigorous specification of the 
> problem, you are well justified in this.  I was deliberately 
> fuzzy since managers just want "stuff" and I thought casting 
> a wide net would pay off.  The problem is to summarise 
> information which is nothing more than sales data.  The kinds 
> of columns I am dealing with look like:
> 
> date, customer, invoice_no, product, amount, sales_region, etc etc.
> 
> Managers want to know things like:
> 	- which products are doing well
> 	- which regions are doing well
> 	- who are good customers
> 	- etc
> 
> To me these are simple aggregates and sorts, with visual 
> presentations to match.
> 
> I figure a bit of effort, R can extract considerably more 
> useful information from the data.
> 
> To be honest I am just evolving it as I go, using an existing 
> spreadsheet as a basis.  I try something and if it is useful 
> then great, if not, put it down to learning.
> 
> cheers
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From olau at fas.harvard.edu  Fri Jan 21 06:06:02 2005
From: olau at fas.harvard.edu (Olivia Lau)
Date: Fri, 21 Jan 2005 00:06:02 -0500
Subject: [R] dim vs length for vectors
Message-ID: <000301c4ff76$e7951ef0$7700a8c0@olau>

Hi all,

I'm not sure if this is a feature or a bug (and I did read the 
FAQ and the posting guide, but am still not sure).  Some of my 
students have been complaining and I thought I just might ask: 
Let K be a vector of length k.  If one types dim(K), you get 
NULL rather than [1] k.  Is this logical?

Here's the way I explain it (and maybe someone can provide a 
more accurate explanation of what's going on):  R has several 
types of scalar (atomic) values, the most common of which are 
numeric, integer, logical, and character values.  Arrays are 
data structures which hold only one type of atomic value. 
Arrays can be one-dimensional (vectors), two-dimensional 
(matrices), or n-dimensional.

(We generally use arrays of n-1 dimensions to populate 
n-dimensional arrays -- thus, we generally use vectors to 
populate matrices, and matrices to populate 3-dimensional 
arrays, but could use any array of dimension < n-1 to populate 
an n-dimensional array.)

It logically follows that when one does dim() on a vector, one 
should *not* get NULL, but should get the length of the vector 
(which one *could* obtain by doing length(), but I think this is 
less logical).  I think that R should save length() for lists 
that have objects of different dimension and type.

Does this make sense?  Or is there a better explanation?

Thanks in advance!  Yours,

Olivia Lau



From ggrothendieck at myway.com  Fri Jan 21 06:35:11 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 21 Jan 2005 05:35:11 +0000 (UTC)
Subject: [R] dim vs length for vectors
References: <000301c4ff76$e7951ef0$7700a8c0@olau>
Message-ID: <loom.20050121T063201-271@post.gmane.org>

Olivia Lau <olau <at> fas.harvard.edu> writes:

: 
: Hi all,
: 
: I'm not sure if this is a feature or a bug (and I did read the 
: FAQ and the posting guide, but am still not sure).  Some of my 
: students have been complaining and I thought I just might ask: 
: Let K be a vector of length k.  If one types dim(K), you get 
: NULL rather than [1] k.  Is this logical?
: 
: Here's the way I explain it (and maybe someone can provide a 
: more accurate explanation of what's going on):  R has several 
: types of scalar (atomic) values, the most common of which are 
: numeric, integer, logical, and character values.  Arrays are 
: data structures which hold only one type of atomic value. 
: Arrays can be one-dimensional (vectors), two-dimensional 
: (matrices), or n-dimensional.
: 
: (We generally use arrays of n-1 dimensions to populate 
: n-dimensional arrays -- thus, we generally use vectors to 
: populate matrices, and matrices to populate 3-dimensional 
: arrays, but could use any array of dimension < n-1 to populate 
: an n-dimensional array.)
: 
: It logically follows that when one does dim() on a vector, one 
: should *not* get NULL, but should get the length of the vector 
: (which one *could* obtain by doing length(), but I think this is 
: less logical).  I think that R should save length() for lists 
: that have objects of different dimension and type.
: 

In R, vectors are not arrays:

R> v <- 1:4
R> dim(v)
NULL
R> is.array(v)
[1] FALSE

R> a <- array(1:4)
R> dim(a)
[1] 4
R> is.array(a)
[1] TRUE



From jjonphl at gmail.com  Fri Jan 21 06:53:18 2005
From: jjonphl at gmail.com (miguel manese)
Date: Fri, 21 Jan 2005 13:53:18 +0800
Subject: [R] dim vs length for vectors
In-Reply-To: <loom.20050121T063201-271@post.gmane.org>
References: <000301c4ff76$e7951ef0$7700a8c0@olau>
	<loom.20050121T063201-271@post.gmane.org>
Message-ID: <d35b9da605012021532edf4895@mail.gmail.com>

I think the more intuitive way to think of it is that dim works only
for matrices (an array being a 1 column matrix). and vectors are not
matrices.

> x <- 1:5
> class(x)  # numeric
>  dim(x) <- 5
> class(x) #  array
> dim(x) <- c(5,1)
> class(x) # matrix
> dim(x) <- c(1,5)
> class(x) # matrix


On Fri, 21 Jan 2005 05:35:11 +0000 (UTC), Gabor Grothendieck
<ggrothendieck at myway.com> wrote:
> Olivia Lau <olau <at> fas.harvard.edu > writes:
> 
> :
> : Hi all,
> :
> : I'm not sure if this is a feature or a bug (and I did read the
> : FAQ and the posting guide, but am still not sure).  Some of my
> : students have been complaining and I thought I just might ask:
> : Let K be a vector of length k.  If one types dim(K), you get
> : NULL rather than [1] k.  Is this logical?
> :
> : Here's the way I explain it (and maybe someone can provide a
> : more accurate explanation of what's going on):  R has several
> : types of scalar (atomic) values, the most common of which are
> : numeric, integer, logical, and character values.  Arrays are
> : data structures which hold only one type of atomic value.
> : Arrays can be one-dimensional (vectors), two-dimensional
> : (matrices), or n-dimensional.
> :
> : (We generally use arrays of n-1 dimensions to populate
> : n-dimensional arrays -- thus, we generally use vectors to
> : populate matrices, and matrices to populate 3-dimensional
> : arrays, but could use any array of dimension < n-1 to populate
> : an n-dimensional array.)
> :
> : It logically follows that when one does dim() on a vector, one
> : should *not* get NULL, but should get the length of the vector
> : (which one *could* obtain by doing length(), but I think this is
> : less logical).  I think that R should save length() for lists
> : that have objects of different dimension and type.
> :
> 
> In R, vectors are not arrays:
> 
> R> v <- 1:4
> R> dim(v)
> NULL
> R> is.array(v)
> [1] FALSE
> 
> R> a <- array(1:4)
> R> dim(a)
> [1] 4
> R> is.array(a)
> [1] TRUE
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html 
>



From ramesh_k77 at yahoo.com  Fri Jan 21 08:10:24 2005
From: ramesh_k77 at yahoo.com (kolluru ramesh)
Date: Thu, 20 Jan 2005 23:10:24 -0800 (PST)
Subject: [R] Cholesky Decomposition 
Message-ID: <20050121071024.17221.qmail@web41413.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050120/0f4d9637/attachment.pl

From ggrothendieck at myway.com  Fri Jan 21 08:29:35 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 21 Jan 2005 07:29:35 +0000 (UTC)
Subject: [R] dim vs length for vectors
References: <000301c4ff76$e7951ef0$7700a8c0@olau>
	<loom.20050121T063201-271@post.gmane.org>
	<d35b9da605012021532edf4895@mail.gmail.com>
Message-ID: <loom.20050121T074034-86@post.gmane.org>


More generally, anything that has a dim attribute is an array 
including 1d, 2d, 3d structures with dim attributes.
Matrices have a dim attribute so matrices are arrays and
is.array(m) will be TRUE if m is a matrix.  

miguel manese <jjonphl <at> gmail.com> writes:

: 
: I think the more intuitive way to think of it is that dim works only
: for matrices (an array being a 1 column matrix). and vectors are not
: matrices.
: 
: > x <- 1:5
: > class(x)  # numeric
: >  dim(x) <- 5
: > class(x) #  array
: > dim(x) <- c(5,1)
: > class(x) # matrix
: > dim(x) <- c(1,5)
: > class(x) # matrix
: 
: On Fri, 21 Jan 2005 05:35:11 +0000 (UTC), Gabor Grothendieck
: <ggrothendieck <at> myway.com> wrote:
: > Olivia Lau <olau <at> fas.harvard.edu > writes:
: > 
: > :
: > : Hi all,
: > :
: > : I'm not sure if this is a feature or a bug (and I did read the
: > : FAQ and the posting guide, but am still not sure).  Some of my
: > : students have been complaining and I thought I just might ask:
: > : Let K be a vector of length k.  If one types dim(K), you get
: > : NULL rather than [1] k.  Is this logical?
: > :
: > : Here's the way I explain it (and maybe someone can provide a
: > : more accurate explanation of what's going on):  R has several
: > : types of scalar (atomic) values, the most common of which are
: > : numeric, integer, logical, and character values.  Arrays are
: > : data structures which hold only one type of atomic value.
: > : Arrays can be one-dimensional (vectors), two-dimensional
: > : (matrices), or n-dimensional.
: > :
: > : (We generally use arrays of n-1 dimensions to populate
: > : n-dimensional arrays -- thus, we generally use vectors to
: > : populate matrices, and matrices to populate 3-dimensional
: > : arrays, but could use any array of dimension < n-1 to populate
: > : an n-dimensional array.)
: > :
: > : It logically follows that when one does dim() on a vector, one
: > : should *not* get NULL, but should get the length of the vector
: > : (which one *could* obtain by doing length(), but I think this is
: > : less logical).  I think that R should save length() for lists
: > : that have objects of different dimension and type.
: > :
: > 
: > In R, vectors are not arrays:
: > 
: > R> v <- 1:4
: > R> dim(v)
: > NULL
: > R> is.array(v)
: > [1] FALSE
: > 
: > R> a <- array(1:4)
: > R> dim(a)
: > [1] 4
: > R> is.array(a)
: > [1] TRUE
: > 
: > ______________________________________________
: > R-help <at> stat.math.ethz.ch mailing list
: > https://stat.ethz.ch/mailman/listinfo/r-help 
: > PLEASE do read the posting guide! http://www.R-project.org/posting-
guide.html 
: >
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From buser at stat.math.ethz.ch  Fri Jan 21 08:57:34 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Fri, 21 Jan 2005 08:57:34 +0100
Subject: [R] Cholesky Decomposition 
In-Reply-To: <20050121071024.17221.qmail@web41413.mail.yahoo.com>
References: <20050121071024.17221.qmail@web41413.mail.yahoo.com>
Message-ID: <16880.46574.382057.437154@stat.math.ethz.ch>

Dear Kolluru

For a real symmetric positive-definite square matrix you can use
chol (see ?chol) in the base package.

help.search("cholesky") gives some more alternatives:

chol.new(assist)        A Modified Cholesky Decomposition
chol.reduce(kernlab)    Incomplete Cholesky decomposition
gchol(kinship)          Generalized Cholesky decompostion
solve.bdsmatrix(kinship)
                        Solve a matrix equation using the generalized
                        Cholesky decompostion
solve.gchol(kinship)    Solve a matrix equation using the generalized
                        Cholesky decompostion
Cholesky-class(Matrix)
                        Cholesky decompositions
sscChol-class(Matrix)   Cholesky decompositions of sscMatrix objects
chol(base)              The Choleski Decomposition
chol2inv(base)          Inverse from Choleski Decomposition

Hope there is something for you.

Christoph

-- 
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C11
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-1-632-5414		fax: 632-1228
http://stat.ethz.ch/~buser/


kolluru ramesh writes:
 > Can we do Cholesky Decompositon in R for any matrix
 > 
 > 		
 > ---------------------------------
 > 
 > 
 > 	[[alternative HTML version deleted]]
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From manuel_gutierrez_lopez at yahoo.es  Fri Jan 21 10:03:08 2005
From: manuel_gutierrez_lopez at yahoo.es (Manuel Gutierrez)
Date: Fri, 21 Jan 2005 10:03:08 +0100 (CET)
Subject: [R] memory and swap space in ncdf
Message-ID: <20050121090308.95677.qmail@web25106.mail.ukl.yahoo.com>

I've a linux system with 2Gb of memory which  is not
enough for reading a 446Mb netcdf file using ncdf:
library(ncdf)
ncold <- open.ncdf("gridone.grd")
Error: cannot allocate vector of size 1822753 Kb

When I look at the free memory in my system I can see
that none of the Swap space is being used by R.
I am a newbie in linux and R, I've read the Memory
help pages but still have some questions:
can I use the swap space in R to solve my problem of
lack of memory?
if not, are there any ways to read the data apart from
buying more RAM?
Thanks,
M



From brostaux.y at fsagx.ac.be  Fri Jan 21 10:13:17 2005
From: brostaux.y at fsagx.ac.be (Yves Brostaux)
Date: Fri, 21 Jan 2005 10:13:17 +0100
Subject: [R] Selecting a subplot of pairs
Message-ID: <41F0C7AD.20307@fsagx.ac.be>

Hello,

I'm trying to plot a set of 3 dependant variables (y) against 4 
predictors (x) in a matrix-like plot, sharing x- an y-axis for all the 
plot on the same column/line :

y1/x1   y1/x2   y1/x3   y1/x4
y2/x1   y2/x2   y2/x3   y2/x4
y3/x1   y3/x2   y3/x3   y3/x4

In fact, this plot is a rectangular selection of the result of pairs(), 
limited to the relations between x's and y's and excluding those within 
x's and y's. I managed to recreate such a plot using a script with 
layout(), axis() and so on, but I was wondering if there already exists 
a clean function for such a task, in case I would encounter this problem 
again ?

-- 
Ir. Yves BROSTAUX
Unit? de Statistique et Informatique
Facult? universitaire des Sciences agronomiques de Gembloux (FUSAGx)
8, avenue de la Facult?
B-5030 Gembloux
Belgique
T?l: +32 81 62 24 69
Email: brostaux.y at fsagx.ac.be



From michael.beer at unifr.ch  Fri Jan 21 10:35:00 2005
From: michael.beer at unifr.ch (BEER Michael)
Date: Fri, 21 Jan 2005 10:35:00 +0100
Subject: [R] Parallel computations using snow: how to combine boot objects?
Message-ID: <24D0F1947691984E89F3151A7DC314DDAC1FCB@EXCHANGE2.unifr.ch>

Hello,

I'm trying to do some bootstrapping in a parallel environment (Linux
cluster) in order to estimate confidence intervals for a certain
parameter. Following the example in the documentation of the "snow"
package (http://www.stat.uiowa.edu/~luke/R/cluster/cluster.html), I
launch my computations by something like

> cl.nuke.boot <-
+             clusterCall(cl,boot,nuke.data, nuke.fun, R=500, m=1,
+                         fit.pred=new.fit, x.pred=new.data)

which gives me a list of n boot objects (where n is the number of nodes
in my cluster). So far, so good.

However, if I now want to go further, I need to combine all these boot
objects to a single one which I can pass to boot.ci for example. Is
there a recommended way to do this?

Thanks,
Michael



From ozric at web.de  Fri Jan 21 10:53:42 2005
From: ozric at web.de (Christian Schulz)
Date: Fri, 21 Jan 2005 10:53:42 +0100
Subject: [R] gsub pattern?
Message-ID: <41F0D126.2010803@web.de>

Hi,

search in web for regular expressions i get the information that
the line  below replace all  AUTO string's like AUTOBAHN
,AUTORENNEN  with 1 but nothing happend.
Using the [] in the pattern it works like i'm expected, but i didn't
want single character replacment. Where is my mistake?

bcode <- gsub("/^AUTO.*/","1",MyStringVector,ignore.case=T,extended=T)

many thanks & regards,
christian



From C.Combe at napier.ac.uk  Fri Jan 21 10:58:31 2005
From: C.Combe at napier.ac.uk (Combe, Colin)
Date: Fri, 21 Jan 2005 09:58:31 -0000
Subject: [R] functions not found after installing DBI/RDBI packages
Message-ID: <735F04A99D358E468A16EDB64FC04555232B61@EVS1.napier-mail.napier.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050121/e286149c/attachment.pl

From fredrik.bg.lundgren at bredband.net  Fri Jan 21 10:59:39 2005
From: fredrik.bg.lundgren at bredband.net (Fredrik Lundgren)
Date: Fri, 21 Jan 2005 10:59:39 +0100
Subject: [R] Slow R Graphics: Device 2 within Xemacs/Ess
Message-ID: <000901c4ff9f$ec3f9790$4c9d72d5@Larissa>

Dear R-list,

I use Xemacs 21.4.13 with Ess, R 2.0.1, and Win XP.
When i plot something it shows up OK in the graphics window but when I 
want to change between windows in the foreground (between Xemacs and R 
Graphics: Device) the graphics window is awfully slow to redraw
when forced to the foreground. When I use Rgui this is not any problem 
at all so it appears to be something wrong between Xemacs and R. (Ess 
and R work very well in Xemacs from other aspects) Any hints how to 
tweak the programs to each other?

Best wishes
Fredrik Lundgren



From B.Rowlingson at lancaster.ac.uk  Fri Jan 21 11:03:28 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 21 Jan 2005 10:03:28 +0000
Subject: [R] gsub pattern?
In-Reply-To: <41F0D126.2010803@web.de>
References: <41F0D126.2010803@web.de>
Message-ID: <41F0D370.70500@lancaster.ac.uk>

Christian Schulz wrote:
>  Where is my mistake?
> 
> bcode <- gsub("/^AUTO.*/","1",MyStringVector,ignore.case=T,extended=T)
> 

  You dont need the slashes! You've been looking at documentation for 
Perl regular expression replacements, I guess.

  help(gsub) may have showed you the way. Here's how to do it:

  > MyStringVector=c("AUTOBAHN","NAUTON","FOO","AUTOGRAPH")

# wrong way:

  > gsub("/^AUTO.*/","1",MyStringVector,ignore.case=T,extended=T)
  [1] "AUTOBAHN"  "NAUTON"    "FOO"       "AUTOGRAPH"

# dont slash all over the regexp:

  > gsub("^AUTO.*","1",MyStringVector,ignore.case=T,extended=T)
  [1] "1"      "NAUTON" "FOO"    "1"

  Is that what you're after?

Baz



From p.dalgaard at biostat.ku.dk  Fri Jan 21 11:05:59 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Jan 2005 11:05:59 +0100
Subject: [R] gsub pattern?
In-Reply-To: <41F0D126.2010803@web.de>
References: <41F0D126.2010803@web.de>
Message-ID: <x2y8enjdvc.fsf@biostat.ku.dk>

Christian Schulz <ozric at web.de> writes:

> Hi,
> 
> search in web for regular expressions i get the information that
> the line  below replace all  AUTO string's like AUTOBAHN
> ,AUTORENNEN  with 1 but nothing happend.
> Using the [] in the pattern it works like i'm expected, but i didn't
> want single character replacment. Where is my mistake?
> 
> bcode <- gsub("/^AUTO.*/","1",MyStringVector,ignore.case=T,extended=T)

What are the "/"-es for?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ksiva at prytania.co.uk  Fri Jan 21 11:16:39 2005
From: ksiva at prytania.co.uk (Kavithan Siva)
Date: Fri, 21 Jan 2005 10:16:39 -0000
Subject: [R] axis placement with stacked barplots and the asp=1 parameter
Message-ID: <C675D2B0F853F54389286F460C712C065C64DD@hyperion.sol.prytaniagroup.com>

Hi,

I'm currently using barplot to generate vertically stacked bar charts.
I'd like to be able to use the "asp=1" parameter with barplot(), but
doing this causes the y axis to be placed on the far left as shown in
the attachment demo.pdf.

I could get around this by using the negative values for the line
parameter of the axis() function. I'd rather not do this and imagine I'm
doing something wrong. Any help would be much appreciated.

Thanks
Kav
 
 ---
This e-mail may contain confidential and/or privileged information. If you are not the 
intended recipient (or have received this e-mail in error) please notify the sender 
immediately and destroy this e-mail. Any unauthorised copying, disclosure or distribution 
of the material in this e-mail is strictly forbidden. The Prytania Group has taken every 
reasonable precaution to ensure that any attachment to this e-mail has been swept for 
viruses. However, we cannot accept liability for any damage sustained as a result of 
software viruses and would advise that you carry out your own virus checks before 
opening any attachment.

From ozric at web.de  Fri Jan 21 11:14:45 2005
From: ozric at web.de (Christian Schulz)
Date: Fri, 21 Jan 2005 11:14:45 +0100
Subject: [R] gsub pattern?
In-Reply-To: <x2y8enjdvc.fsf@biostat.ku.dk>
References: <41F0D126.2010803@web.de> <x2y8enjdvc.fsf@biostat.ku.dk>
Message-ID: <41F0D615.6060609@web.de>

many thanks
..the different styles from linux to r-project a little confusing for me :-(

christian

Peter Dalgaard wrote:

>Christian Schulz <ozric at web.de> writes:
>
>  
>
>>Hi,
>>
>>search in web for regular expressions i get the information that
>>the line  below replace all  AUTO string's like AUTOBAHN
>>,AUTORENNEN  with 1 but nothing happend.
>>Using the [] in the pattern it works like i'm expected, but i didn't
>>want single character replacment. Where is my mistake?
>>
>>bcode <- gsub("/^AUTO.*/","1",MyStringVector,ignore.case=T,extended=T)
>>    
>>
>
>What are the "/"-es for?
>
>  
>



From ramesh_k77 at yahoo.com  Fri Jan 21 11:19:49 2005
From: ramesh_k77 at yahoo.com (kolluru ramesh)
Date: Fri, 21 Jan 2005 02:19:49 -0800 (PST)
Subject: [R] cross validation
Message-ID: <20050121101949.67540.qmail@web41404.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050121/4ab67ef3/attachment.pl

From jacques.veslot at cirad.fr  Fri Jan 21 11:35:53 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Fri, 21 Jan 2005 14:35:53 +0400
Subject: [R] an R script editor for Mac
In-Reply-To: <735F04A99D358E468A16EDB64FC04555232B61@EVS1.napier-mail.napier.ac.uk>
Message-ID: <HHEDKBCGCMDOHEDELFBCGEMACHAA.jacques.veslot@cirad.fr>

Dear all,

Could someone please make me know if there is a nice script editor available
under Mac, similar to Crimson, that offers R syntax highlighting (and pairs
of parentheses underlining) ?

Thanks in advance,

Jacques VESLOT
Cirad



From numero.primo at tele2.it  Fri Jan 21 11:43:17 2005
From: numero.primo at tele2.it (Landini Massimiliano)
Date: Fri, 21 Jan 2005 11:43:17 +0100
Subject: [R] Average disjunction
Message-ID: <sum1v0pbeu0vhg5ri1rskpctpt21pdc0up@4ax.com>

Dear All
after performing a test like TukeyHSD is there a simple/complicated way to
perform average disjunction with letter? example

10	aA
6	bA
1.5	cB
2	cB
1	cB

Thanks



-------------------------------------------------------------------------------------------------------------------------
Landini dr. Massimiliano
Tel. mob. (+39) 347 140 11 94
Tel./Fax. (+39) 051 762 196
e-mail: numero (dot) primo (at) tele2 (dot) it
-------------------------------------------------------------------------------------------------------------------------
Legge di Hanggi: Pi? stupida ? la tua ricerca, pi? verr? letta e approvata.
Corollario alla Legge di Hanggi: Pi? importante ? la tua ricerca, meno verr?
capita.



From ahenningsen at email.uni-kiel.de  Fri Jan 21 12:01:59 2005
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Fri, 21 Jan 2005 12:01:59 +0100
Subject: [R] dim vs length for vectors
In-Reply-To: <loom.20050121T063201-271@post.gmane.org>
References: <000301c4ff76$e7951ef0$7700a8c0@olau>
	<loom.20050121T063201-271@post.gmane.org>
Message-ID: <200501211201.59297.ahenningsen@email.uni-kiel.de>

On Friday 21 January 2005 06:35, Gabor Grothendieck wrote:
> In R, vectors are not arrays:
>
> R> v <- 1:4
> R> dim(v)
> NULL
> R> is.array(v)
> [1] FALSE
>
> R> a <- array(1:4)
> R> dim(a)
> [1] 4
> R> is.array(a)
> [1] TRUE

Is this a feature which is useful in some applications?
IMHO the difference between vectors and 1-dimensional arrays is really 
annoying and I had already several bugs in my code, because I mixed these up.
Is it possible in future versions of R that R does not differentiate between 
vectors and 1-dimensional arrays (e.g. by treating all vectors as 
1-dimensional arrays)?

Arne



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Jan 21 12:05:47 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 21 Jan 2005 12:05:47 +0100
Subject: [R] cross validation
References: <20050121101949.67540.qmail@web41404.mail.yahoo.com>
Message-ID: <010801c4ffa9$29368010$0540210a@www.domain>

you could something like this (based on V&R's S Programming, pp. 175):

dat <- data.frame(matrix(rnorm(100*6), 100, 6))
#####
n <- nrow(dat)
V <- 10 # number of folds
samps <- sample(rep(1:V, length=n), n, replace=FALSE)
#####
# Using the first fold:
train <- dat[samps!=1,] # fit the model
test <- dat[samps==1,] # predict


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "kolluru ramesh" <ramesh_k77 at yahoo.com>
To: "Rpackage help" <r-help at stat.math.ethz.ch>
Sent: Friday, January 21, 2005 11:19 AM
Subject: [R] cross validation


> How to select training data set and test data set from the original 
> data for performing cross-validation
>
>
> ---------------------------------
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From phgrosjean at sciviews.org  Fri Jan 21 12:23:32 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 21 Jan 2005 12:23:32 +0100
Subject: [R] an R script editor for Mac
In-Reply-To: <HHEDKBCGCMDOHEDELFBCGEMACHAA.jacques.veslot@cirad.fr>
References: <HHEDKBCGCMDOHEDELFBCGEMACHAA.jacques.veslot@cirad.fr>
Message-ID: <41F0E634.2040007@sciviews.org>

You could look at http://www.r-project.org/GUI/projects/Editors.html for 
a list of text editors offering, at least, syntax highlighting of R 
code. For Mac (OS X), it seems you have the choice between SubEthaEdit, 
BlueFish and Jedit,... plus the code editor included in JGR (see: 
http://stats.math.uni-augsburg.de/JGR/).

Best,

Philippe Grosjean

..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................

Jacques VESLOT wrote:
> Dear all,
> 
> Could someone please make me know if there is a nice script editor available
> under Mac, similar to Crimson, that offers R syntax highlighting (and pairs
> of parentheses underlining) ?
> 
> Thanks in advance,
> 
> Jacques VESLOT
> Cirad
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From pbalapra at ulb.ac.be  Fri Jan 21 12:27:15 2005
From: pbalapra at ulb.ac.be (Prasanna Balaprakash)
Date: Fri, 21 Jan 2005 12:27:15 +0100 (MET)
Subject: [R] chi-Squared distribution
Message-ID: <200501211127.MAA00206@web1.ulb.ac.be>

Dear Rs:


outer(1:3, 1:3, function(df1, df2) qf(0.95, df1, df2))  

I compare this F distribution results with the table, the answers were perfect. But I need to see for chi-sqaured distribution. When I employed the similar formula

outer(1:3, 1:3, function(df1, df2) qchisq(0.95, df1, df2)) , I am getting unexpected results. I need to see the following values:

     p=0.750  .....
1     1.323    

2     2.773   

3     4.108   


Thanking you
Prasanna






Prasanna Balaprakash,
Universit? Libre de Bruxelles, 
50, Av. F. Roosevelt, CP 194/6, 
B-1050 Brussels, 
Belgium.



From andy_liaw at merck.com  Fri Jan 21 12:29:03 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 21 Jan 2005 06:29:03 -0500
Subject: [R] cross validation
Message-ID: <3A822319EB35174CA3714066D590DCD50994E56F@usrymx25.merck.com>

One way is to create an indicator vector that indicate which `fold' a case
should belong to.  Something like:

fold <- 10
idx <- sample(fold, n, replace=TRUE)
for (I in 1:fold) {
   train.dat <- dat[idx != i,]
   test.dat <- dat[idx == i,]
   ...
}

Also see the errorest() function in the ipred package.  It is more careful
in making sure the folds are as close in size as possible, and can do
stratified splits.

Andy


> From: kolluru ramesh
> 
> How to select training data set and test data set from the 
> original data for performing cross-validation
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Christoph.Scherber at uni-jena.de  Fri Jan 21 12:30:07 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Fri, 21 Jan 2005 12:30:07 +0100
Subject: [R] mixed effects model:how to include  initial conditions 
Message-ID: <41F0E7BF.6050503@uni-jena.de>

Dear R users,

I am analyzing a dataset on growth of plants in response to several 
factors. I am using a mixed-effects model of the following structure:

model<-lme(growth~block*treatment*factor1*factor2,
random=~1|plot/treatment/initialsize)

I have measured the initial size of the plants (in 2003) and thought it 
might be sensible to include this (random) variation into the random 
effects term of the model.

Is that correct? Or should "initialsize" rather be included as a 
covariate into the fixed effects term, as in:

alternative<-lme(growth~block*initialsize*treatment*factor1*factor2,
random=~1|plot/treatment)

I would very much appreciate any suggestions on how to analyze these 
data correctly.

Best regards
Chris.



From sdavis2 at mail.nih.gov  Fri Jan 21 12:39:43 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 21 Jan 2005 06:39:43 -0500
Subject: [R] an R script editor for Mac
In-Reply-To: <HHEDKBCGCMDOHEDELFBCGEMACHAA.jacques.veslot@cirad.fr>
References: <HHEDKBCGCMDOHEDELFBCGEMACHAA.jacques.veslot@cirad.fr>
Message-ID: <24A23A3C-6BA1-11D9-91D9-000D933565E8@mail.nih.gov>

Consider using ESS and xemacs or emacs.  You get syntax highlighting, 
auto-indent, command auto-complete, transcripts for your session, 
integrated help, and the tools of one of the most powerful text editors 
on the planet.  The learning curve is a bit steep, but if you use R 
much, it is worth it.

Sean

On Jan 21, 2005, at 5:35 AM, Jacques VESLOT wrote:

> Dear all,
>
> Could someone please make me know if there is a nice script editor 
> available
> under Mac, similar to Crimson, that offers R syntax highlighting (and 
> pairs
> of parentheses underlining) ?
>
> Thanks in advance,
>
> Jacques VESLOT
> Cirad
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From vito_ricci at yahoo.com  Fri Jan 21 12:44:52 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Fri, 21 Jan 2005 12:44:52 +0100 (CET)
Subject: [R] R: chi-Squared distribution
Message-ID: <20050121114452.33367.qmail@web41204.mail.yahoo.com>

Hi,
Attention chi-squared distribution, unlike F
distribution, has only df1 as parameter, not df1 and
df2. So correct into:

outer(1:3, 1:3, function(df1, df2) qchisq(0.95, df1,
df2))

outer(1:3, 1:3, function(df1, df2) qchisq(0.95, df1))
                         ^^^^^^^^^^^^^^^^^^^^

Regards,
Vito


you wrote:

Dear Rs:


outer(1:3, 1:3, function(df1, df2) qf(0.95, df1, df2))
 

I compare this F distribution results with the table,
the answers were perfect. But I need to see for
chi-sqaured distribution. When I employed the similar
formula

outer(1:3, 1:3, function(df1, df2) qchisq(0.95, df1,
df2)) , I am getting unexpected results. I need to see
the following values:

     p=0.750  .....
1     1.323    

2     2.773   

3     4.108   


Thanking you
Prasanna

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From sdavis2 at mail.nih.gov  Fri Jan 21 12:45:50 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 21 Jan 2005 06:45:50 -0500
Subject: [R] functions not found after installing DBI/RDBI packages
In-Reply-To: <735F04A99D358E468A16EDB64FC04555232B61@EVS1.napier-mail.napier.ac.uk>
References: <735F04A99D358E468A16EDB64FC04555232B61@EVS1.napier-mail.napier.ac.uk>
Message-ID: <FF4BBAB3-6BA1-11D9-91D9-000D933565E8@mail.nih.gov>

Colin,

Rdbi provides generic methods for working with databases.  It requires 
a driver for a specific database, as well.  Therefore, you will need to 
download and install RdbiPgSQL, also.

Sean

On Jan 21, 2005, at 4:58 AM, Combe, Colin wrote:

> Hi,
> I would like to be able to use R with a connection to a PostgreSQL 
> database. I am using R 2.0.1 on windows XP. I have tried installing 
> both the DBI and rDBI packages (which is better?) but in either case I 
> run into the same problem - when i try to use either the dbDriver or 
> dbConnect functions i'm told the function couldn't be found. Seems 
> like the packages aren't installing properly. I installed DBI from a 
> zip file and rDBI from bioconductor.
>
> Any clues as to where i'm going wrong?
>
> thanks,
> colin
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Jan 21 12:49:56 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 21 Jan 2005 12:49:56 +0100
Subject: [R] cross validation
In-Reply-To: <010801c4ffa9$29368010$0540210a@www.domain>
References: <20050121101949.67540.qmail@web41404.mail.yahoo.com>
	<010801c4ffa9$29368010$0540210a@www.domain>
Message-ID: <41F0EC64.5000409@statistik.uni-dortmund.de>

Dimitris Rizopoulos wrote:

> you could something like this (based on V&R's S Programming, pp. 175):
> 
> dat <- data.frame(matrix(rnorm(100*6), 100, 6))
> #####
> n <- nrow(dat)
> V <- 10 # number of folds
> samps <- sample(rep(1:V, length=n), n, replace=FALSE)
> #####
> # Using the first fold:
> train <- dat[samps!=1,] # fit the model
> test <- dat[samps==1,] # predict


Or see ?errorest in the "ipred" package.

Uwe Ligges


> 
> I hope it helps.
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/336899
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat
>     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> 
> 
> ----- Original Message ----- From: "kolluru ramesh" <ramesh_k77 at yahoo.com>
> To: "Rpackage help" <r-help at stat.math.ethz.ch>
> Sent: Friday, January 21, 2005 11:19 AM
> Subject: [R] cross validation
> 
> 
>> How to select training data set and test data set from the original 
>> data for performing cross-validation
>>
>>
>> ---------------------------------
>>
>>
>> [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Jan 21 12:52:21 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 21 Jan 2005 12:52:21 +0100
Subject: [R] axis placement with stacked barplots and the asp=1 parameter
In-Reply-To: <C675D2B0F853F54389286F460C712C065C64DD@hyperion.sol.prytaniagroup.com>
References: <C675D2B0F853F54389286F460C712C065C64DD@hyperion.sol.prytaniagroup.com>
Message-ID: <41F0ECF5.4040108@statistik.uni-dortmund.de>

Kavithan Siva wrote:

> Hi,
> 
> I'm currently using barplot to generate vertically stacked bar charts.
> I'd like to be able to use the "asp=1" parameter with barplot(), but
> doing this causes the y axis to be placed on the far left as shown in
> the attachment demo.pdf.

See the posting guide, some file types (including pdf) will be omitted 
from your messages to R-help. You might want to specify a short but 
easily reproducible examples in 2-5 lines of code.

Why do you want to use "asp" with a barplot???

Uwe Ligges


> I could get around this by using the negative values for the line
> parameter of the axis() function. I'd rather not do this and imagine I'm
> doing something wrong. Any help would be much appreciated.
> 
> Thanks
> Kav
>  
>  ---
> This e-mail may contain confidential and/or privileged inf...{{dropped}}



From ligges at statistik.uni-dortmund.de  Fri Jan 21 12:54:06 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 21 Jan 2005 12:54:06 +0100
Subject: [R] Selecting a subplot of pairs
In-Reply-To: <41F0C7AD.20307@fsagx.ac.be>
References: <41F0C7AD.20307@fsagx.ac.be>
Message-ID: <41F0ED5E.2090905@statistik.uni-dortmund.de>

Yves Brostaux wrote:

> Hello,
> 
> I'm trying to plot a set of 3 dependant variables (y) against 4 
> predictors (x) in a matrix-like plot, sharing x- an y-axis for all the 
> plot on the same column/line :
> 
> y1/x1   y1/x2   y1/x3   y1/x4
> y2/x1   y2/x2   y2/x3   y2/x4
> y3/x1   y3/x2   y3/x3   y3/x4
> 
> In fact, this plot is a rectangular selection of the result of pairs(), 
> limited to the relations between x's and y's and excluding those within 
> x's and y's. I managed to recreate such a plot using a script with 
> layout(), axis() and so on, but I was wondering if there already exists 
> a clean function for such a task, in case I would encounter this problem 
> again ?
> 

The functionality in packages grid and lattice is your friend (if you 
dislike it, you can twiddle with par()'s argument "mfrow").

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Fri Jan 21 12:55:40 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 21 Jan 2005 12:55:40 +0100
Subject: [R] dim vs length for vectors
In-Reply-To: <200501211201.59297.ahenningsen@email.uni-kiel.de>
References: <000301c4ff76$e7951ef0$7700a8c0@olau>	<loom.20050121T063201-271@post.gmane.org>
	<200501211201.59297.ahenningsen@email.uni-kiel.de>
Message-ID: <41F0EDBC.4080709@statistik.uni-dortmund.de>

Arne Henningsen wrote:

> On Friday 21 January 2005 06:35, Gabor Grothendieck wrote:
> 
>>In R, vectors are not arrays:
>>
>>R> v <- 1:4
>>R> dim(v)
>>NULL
>>R> is.array(v)
>>[1] FALSE
>>
>>R> a <- array(1:4)
>>R> dim(a)
>>[1] 4
>>R> is.array(a)
>>[1] TRUE
> 
> 
> Is this a feature which is useful in some applications?
> IMHO the difference between vectors and 1-dimensional arrays is really 
> annoying and I had already several bugs in my code, because I mixed these up.
> Is it possible in future versions of R that R does not differentiate between 
> vectors and 1-dimensional arrays (e.g. by treating all vectors as 
> 1-dimensional arrays)?

No, that would break huge amounts of code!

See ?"[" and learn how to use its argument "drop".

Uwe Ligges


> Arne
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sekemp at glam.ac.uk  Fri Jan 21 12:56:43 2005
From: sekemp at glam.ac.uk (Kemp S E (Comp))
Date: Fri, 21 Jan 2005 11:56:43 -0000
Subject: [R] transfer function estimation
Message-ID: <0BA7EE4D4646E0409D458D347C508B783C61F6@MAILSERV1.uni.glam.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050121/40ec9d06/attachment.pl

From gregor.gorjanc at bfro.uni-lj.si  Fri Jan 21 13:05:21 2005
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Fri, 21 Jan 2005 13:05:21 +0100
Subject: [R] glm and percentage data with many zero values
Message-ID: <41F0F001.8010604@bfro.uni-lj.si>

A hint.

You might try with ZIP i.e. zero inflated poisson model. I did not used it, 
but I have such data to work on. So if there is anyone hwo can point how to 
do this in R - please. There is also a classs of ZINB or something like 
that for zero inflated negative binomial models.

Actually I just went on web and found a book from Simonoff "Analyzing 
Categorical Data" and there are some examples in it for ZIP et al. Look 
examples for sections 4.5 and 5.4

http://www.stern.nyu.edu/~jsimonof/AnalCatData/Splus/analcatdata.s
http://www.stern.nyu.edu/~jsimonof/AnalCatData/Splus/functions.s

-- 
Lep pozdrav / With regards,
     Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Jan 21 13:05:29 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 21 Jan 2005 13:05:29 +0100
Subject: [R] chi-Squared distribution
References: <200501211127.MAA00206@web1.ulb.ac.be>
Message-ID: <001601c4ffb1$80394e30$0540210a@www.domain>

if you check ?qchisq, you'll see that the second argument of the 
function denotes the non-centrality parameter! Try

qchisq(0.75, 1:3)

to get your answer.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Prasanna Balaprakash" <pbalapra at ulb.ac.be>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, January 21, 2005 12:27 PM
Subject: [R] chi-Squared distribution


> Dear Rs:
>
>
> outer(1:3, 1:3, function(df1, df2) qf(0.95, df1, df2))
>
> I compare this F distribution results with the table, the answers 
> were perfect. But I need to see for chi-sqaured distribution. When I 
> employed the similar formula
>
> outer(1:3, 1:3, function(df1, df2) qchisq(0.95, df1, df2)) , I am 
> getting unexpected results. I need to see the following values:
>
>     p=0.750  .....
> 1     1.323
>
> 2     2.773
>
> 3     4.108
>
>
> Thanking you
> Prasanna
>
>
>
>
>
>
> Prasanna Balaprakash,
> Universit? Libre de Bruxelles,
> 50, Av. F. Roosevelt, CP 194/6,
> B-1050 Brussels,
> Belgium.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ksiva at prytania.co.uk  Fri Jan 21 13:09:47 2005
From: ksiva at prytania.co.uk (Kavithan Siva)
Date: Fri, 21 Jan 2005 12:09:47 -0000
Subject: [R] axis placement with stacked barplots and the asp=1 parameter
Message-ID: <C675D2B0F853F54389286F460C712C065C64DE@hyperion.sol.prytaniagroup.com>

Hi

Here's some example code which illustrates the issue:

dataSeries<-array(0, c(5, 2))

dataSeries[1, 1] <- 5
dataSeries[2, 1] <- 5
dataSeries[3, 1] <- 0
dataSeries[4, 1] <- 2
dataSeries[5, 1] <- 0

dataSeries[1, 2] <- 7
dataSeries[2, 2] <- 0
dataSeries[3, 2] <- 0
dataSeries[4, 2] <- 0
dataSeries[5, 2] <- 1

barplot(dataSeries, asp=1)

Removing the asp=1 parameter produces a correct looking plot. I need to
use the asp=1 parameter because I was re-writing the barplot function to
create stacked charts with rounded corners (instead of rectangles). For
the arc at the corners of the bars to look correct I need asp=1. I've
attached a .ps file to show you what I was trying to achieve.

Thanks
Kav


Kavithan Siva
Prytania Investment Advisors LLP
105 Ladbroke Grove, London, W11 1PG
Tel: +44 20 7616 8475   Fax: +44 20 7616 8472
Prytania Investment Advisors LLP is regulated by the FSA

-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
Sent: Friday, January 21, 2005 11:52 AM
To: Kavithan Siva
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] axis placement with stacked barplots and the asp=1
parameter

Kavithan Siva wrote:

> Hi,
> 
> I'm currently using barplot to generate vertically stacked bar charts.
> I'd like to be able to use the "asp=1" parameter with barplot(), but
> doing this causes the y axis to be placed on the far left as shown in
> the attachment demo.pdf.

See the posting guide, some file types (including pdf) will be omitted 
from your messages to R-help. You might want to specify a short but 
easily reproducible examples in 2-5 lines of code.

Why do you want to use "asp" with a barplot???

Uwe Ligges


> I could get around this by using the negative values for the line
> parameter of the axis() function. I'd rather not do this and imagine
I'm
> doing something wrong. Any help would be much appreciated.
> 
> Thanks
> Kav
>  
>  ---
> This e-mail may contain confidential and/or privileged information. If
you are not the 
> intended recipient (or have received this e-mail in error) please
notify the sender 
> immediately and destroy this e-mail. Any unauthorised copying,
disclosure or distribution 
> of the material in this e-mail is strictly forbidden. The Prytania
Group has taken every 
> reasonable precaution to ensure that any attachment to this e-mail has
been swept for 
> viruses. However, we cannot accept liability for any damage sustained
as a result of 
> software viruses and would advise that you carry out your own virus
checks before 
> opening any attachment.
> 
> 
>
------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
 
 ---
This e-mail may contain confidential and/or privileged information. If you are not the 
intended recipient (or have received this e-mail in error) please notify the sender 
immediately and destroy this e-mail. Any unauthorised copying, disclosure or distribution 
of the material in this e-mail is strictly forbidden. The Prytania Group has taken every 
reasonable precaution to ensure that any attachment to this e-mail has been swept for 
viruses. However, we cannot accept liability for any damage sustained as a result of 
software viruses and would advise that you carry out your own virus checks before 
opening any attachment.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: demo.ps
Type: application/postscript
Size: 48749 bytes
Desc: demo.ps
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20050121/0a2d4393/demo.ps

From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Jan 21 13:07:51 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 21 Jan 2005 13:07:51 +0100
Subject: [R] chi-Squared distribution
Message-ID: <001f01c4ffb1$d4d32c40$0540210a@www.domain>


----- Original Message ----- 
From: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.ac.be>
To: "Prasanna Balaprakash" <pbalapra at ulb.ac.be>
Cc: <r-help at stat.math.ethz.ch>
Sent: Friday, January 21, 2005 1:05 PM
Subject: Re: [R] chi-Squared distribution


> if you check ?qchisq, you'll see that the second

My mistake, the third argument

> argument of the function denotes the non-centrality parameter! Try
>
> qchisq(0.75, 1:3)
>
> to get your answer.
>
> Best,
> Dimitris
>
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
>
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/336899
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat
>     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>
>
> ----- Original Message ----- 
> From: "Prasanna Balaprakash" <pbalapra at ulb.ac.be>
> To: <r-help at stat.math.ethz.ch>
> Sent: Friday, January 21, 2005 12:27 PM
> Subject: [R] chi-Squared distribution
>
>
>> Dear Rs:
>>
>>
>> outer(1:3, 1:3, function(df1, df2) qf(0.95, df1, df2))
>>
>> I compare this F distribution results with the table, the answers 
>> were perfect. But I need to see for chi-sqaured distribution. When 
>> I employed the similar formula
>>
>> outer(1:3, 1:3, function(df1, df2) qchisq(0.95, df1, df2)) , I am 
>> getting unexpected results. I need to see the following values:
>>
>>     p=0.750  .....
>> 1     1.323
>>
>> 2     2.773
>>
>> 3     4.108
>>
>>
>> Thanking you
>> Prasanna
>>
>>
>>
>>
>>
>>
>> Prasanna Balaprakash,
>> Universit? Libre de Bruxelles,
>> 50, Av. F. Roosevelt, CP 194/6,
>> B-1050 Brussels,
>> Belgium.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>



From ramesh_k77 at yahoo.com  Fri Jan 21 13:11:19 2005
From: ramesh_k77 at yahoo.com (kolluru ramesh)
Date: Fri, 21 Jan 2005 04:11:19 -0800 (PST)
Subject: Fwd: RE: [R] cross validation
Message-ID: <20050121121119.37599.qmail@web41401.mail.yahoo.com>



Note: forwarded message attached.

		
---------------------------------

-------------- next part --------------
An embedded message was scrubbed...
From: unknown sender
Subject: no subject
Date: no date
Size: 2269
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050121/5c358904/attachment.mht

From bates at stat.wisc.edu  Fri Jan 21 13:28:29 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 21 Jan 2005 06:28:29 -0600
Subject: [R] mixed effects model:how to include  initial conditions
In-Reply-To: <41F0E7BF.6050503@uni-jena.de>
References: <41F0E7BF.6050503@uni-jena.de>
Message-ID: <41F0F56D.7050208@stat.wisc.edu>

Christoph Scherber wrote:
> Dear R users,
> 
> I am analyzing a dataset on growth of plants in response to several 
> factors. I am using a mixed-effects model of the following structure:
> 
> model<-lme(growth~block*treatment*factor1*factor2,
> random=~1|plot/treatment/initialsize)
> 
> I have measured the initial size of the plants (in 2003) and thought it 
> might be sensible to include this (random) variation into the random 
> effects term of the model.
> 
> Is that correct? Or should "initialsize" rather be included as a 
> covariate into the fixed effects term, as in:
> 
> alternative<-lme(growth~block*initialsize*treatment*factor1*factor2,
> random=~1|plot/treatment)
> 
> I would very much appreciate any suggestions on how to analyze these 
> data correctly.
> 
> Best regards
> Chris.

I think you should include it as a covariate but not in the way you have 
written it.  I would include it as a separate term, not in an interaction

alternative<-lme(growth~initialsize+block*treatment*factor1*factor2,
  random=~1|plot/treatment)

I recommend that you look carefully at the number of coefficients that 
you need to estimate in the model as you have specified it and perhaps 
change to an initial model that had more additive effects and fewer 
interactions.



From Rau at demogr.mpg.de  Fri Jan 21 13:31:52 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Fri, 21 Jan 2005 13:31:52 +0100
Subject: [R] Avoiding a Loop?
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E6150667@HERMES.demogr.mpg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050121/3abb53e1/attachment.pl

From arv at ono.com  Fri Jan 21 14:02:13 2005
From: arv at ono.com (antonio rodriguez)
Date: Fri, 21 Jan 2005 14:02:13 +0100
Subject: [R] memory and swap space in ncdf
In-Reply-To: <20050121090308.95677.qmail@web25106.mail.ukl.yahoo.com>
Message-ID: <IPEFKICOHOECENGJBAGLIEJODMAA.arv@ono.com>

Hi Manuel,

Look at your memory.limit() and run R with '--mem-max=' option

Try also to use the simpler netCDF library


Antonio

> -----Mensaje original-----
> De: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]En nombre de Manuel Gutierrez
> Enviado el: viernes, 21 de enero de 2005 10:03
> Para: r-help at stat.math.ethz.ch
> Asunto: [R] memory and swap space in ncdf
> 
> 
> I've a linux system with 2Gb of memory which  is not
> enough for reading a 446Mb netcdf file using ncdf:
> library(ncdf)
> ncold <- open.ncdf("gridone.grd")
> Error: cannot allocate vector of size 1822753 Kb
> 
> When I look at the free memory in my system I can see
> that none of the Swap space is being used by R.
> I am a newbie in linux and R, I've read the Memory
> help pages but still have some questions:
> can I use the swap space in R to solve my problem of
> lack of memory?
> if not, are there any ways to read the data apart from
> buying more RAM?
> Thanks,
> M
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html
-- 
No virus found in this incoming message.
Checked by AVG Anti-Virus.


-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From kalibera at nenya.ms.mff.cuni.cz  Fri Jan 21 14:30:21 2005
From: kalibera at nenya.ms.mff.cuni.cz (Tomas Kalibera)
Date: Fri, 21 Jan 2005 14:30:21 +0100
Subject: [R] Creating a custom connection to read from multiple files
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E55A@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E55A@usrymx25.merck.com>
Message-ID: <41F103ED.4040701@nenya.ms.mff.cuni.cz>

Hello Andy,

thanks for your examples, I rewrote everything to matrices & 
lapply/sapply, rbind  calls instead of for-cycles & appends, it really 
helped. Reading files one by one and concatenating is now even faster 
than concatenating on disk, that 8MB table is read in 3.5 seconds.

Tomas

>>rbind is vectorized so you are using it (way) suboptimally.
>>    
>>
>
>Here's an example:
>
>  
>
>> ## Create a 500 x 100 data matrix.
>> x <- matrix(rnorm(5e4), 500, 100)
>> ## Generate 50 filenames.
>> fname <- paste("f", formatC(1:50, width=2, flag="0"), ".txt", sep="")
>> ## Write the data to files 50 times.
>> for (f in fname) write(t(x), file=f, ncol=ncol(x))
>> 
>> ## Read the files into a list of data frames.
>> system.time(datList <- lapply(fname, read.table, header=FALSE),
>>    
>>
>gcFirst=TRUE)
>[1] 11.91  0.05 12.33    NA    NA
>  
>
>> ## Specify colClasses to speed up.
>> system.time(datList <- lapply(fname, read.table,
>>    
>>
>colClasses=rep("numeric", 100)),
>+              gcFirst=TRUE)
>[1] 10.69  0.07 10.79    NA    NA
>  
>
>> ## Stack them together.
>> system.time(dat <- do.call("rbind", datList), gcFirst=TRUE)
>>    
>>
>[1] 5.34 0.09 5.45   NA   NA
>  
>
>> 
>> ## Use matrices instead of data frames.
>> system.time(datList <- lapply(fname, 
>>    
>>
>+      function(f) matrix(scan(f), ncol=100, byrow=TRUE)), gcFirst=TRUE)
>Read 50000 items
>...
>Read 50000 items
>[1]  9.49  0.08 15.06    NA    NA
>  
>
>> system.time(dat <- do.call("rbind", datList), gcFirst=TRUE)
>>    
>>
>[1] 0.09 0.03 0.12   NA   NA
>  
>
>> ## Clean up the files.
>> unlink(fname)
>>    
>>
>
>A couple of points:
>
>- Usually specifying colClasses will make read.table() quite a bit 
>  faster, even though it's only marginally faster here.  Look back
>  in the list archive to see examples.
>
>- If your data files are all numerics (as in this example), 
>  storing them in matrices will be much more efficient.  Note
>  the difference in rbind()ing the 50 data frames and 50 
>  matrices (5.34 seconds vs. 0.09!).  rbind.data.frame()
>  needs to ensure that the resulting data frame has unique
>  rownames (a requirement for a legit data frame), and
>  that's probably taking a big chunk of the time.
>
>Andy
>
> 
>  
>



From pbalapra at ulb.ac.be  Fri Jan 21 14:38:27 2005
From: pbalapra at ulb.ac.be (Prasanna Balaprakash)
Date: Fri, 21 Jan 2005 14:38:27 +0100 (MET)
Subject: [R] chi-Squared distribution in Friedman test
Message-ID: <200501211338.OAA23054@web1.ulb.ac.be>

Dear R helpers:


Thanks for the previous reply. I am using Friedman racing test. According the the book "Pratical Nonprametric Statistic" by WJ Conover, after computing the statistics, he suggested to use chi-squared or F distribution to accept or reject null hypothesis. After looking into the source code, I found that R uses chi-sqaured distribution as below:

PVAL <- pchisq(STATISTIC, PARAMETER, lower = FALSE)

but still I cant figure out why they are using this pschisq insted of dchisq. Sorry I am wrong!!


Thanking you
truly
Prasanna









>
>----- Original Message ----- 
>From: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.ac.be>
>To: "Prasanna Balaprakash" <pbalapra at ulb.ac.be>
>Cc: <r-help at stat.math.ethz.ch>
>Sent: Friday, January 21, 2005 1:05 PM
>Subject: Re: [R] chi-Squared distribution
>
>
>> if you check ?qchisq, you'll see that the second
>
>My mistake, the third argument
>
>> argument of the function denotes the non-centrality parameter! Try
>>
>> qchisq(0.75, 1:3)
>>
>> to get your answer.
>>
>> Best,
>> Dimitris
>>
>> ----
>> Dimitris Rizopoulos
>> Ph.D. Student
>> Biostatistical Centre
>> School of Public Health
>> Catholic University of Leuven
>>
>> Address: Kapucijnenvoer 35, Leuven, Belgium
>> Tel: +32/16/336899
>> Fax: +32/16/337015
>> Web: http://www.med.kuleuven.ac.be/biostat
>>     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>>
>>
>> ----- Original Message ----- 
>> From: "Prasanna Balaprakash" <pbalapra at ulb.ac.be>
>> To: <r-help at stat.math.ethz.ch>
>> Sent: Friday, January 21, 2005 12:27 PM
>> Subject: [R] chi-Squared distribution
>>
>>
>>> Dear Rs:
>>>
>>>
>>> outer(1:3, 1:3, function(df1, df2) qf(0.95, df1, df2))
>>>
>>> I compare this F distribution results with the table, the answers 
>>> were perfect. But I need to see for chi-sqaured distribution. When 
>>> I employed the similar formula
>>>
>>> outer(1:3, 1:3, function(df1, df2) qchisq(0.95, df1, df2)) , I am 
>>> getting unexpected results. I need to see the following values:
>>>
>>>     p=0.750  .....
>>> 1     1.323
>>>
>>> 2     2.773
>>>
>>> 3     4.108
>>>
>>>
>>> Thanking you
>>> Prasanna
>>>
>>>
>>>
>>>
>>>
>>>
>>> Prasanna Balaprakash,
>>> Universit? Libre de Bruxelles,
>>> 50, Av. F. Roosevelt, CP 194/6,
>>> B-1050 Brussels,
>>> Belgium.
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html



From bxc at steno.dk  Fri Jan 21 14:46:13 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Fri, 21 Jan 2005 14:46:13 +0100
Subject: [R] glm and percentage data with many zero values
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FE65E@exdkba022.novo.dk>

The ZIP model can be fitted with Jim Lindsey's function fmr 
from his gnlm library, see:

http://popgen0146uns50.unimaas.nl/~jlindsey/rcode.html

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gregor GORJANC
> Sent: Friday, January 21, 2005 1:05 PM
> To: christian.kamenik at ips.unibe.ch; r-help at stat.math.ethz.ch
> Subject: RE: [R] glm and percentage data with many zero values
> 
> 
> A hint.
> 
> You might try with ZIP i.e. zero inflated poisson model. I 
> did not used it, 
> but I have such data to work on. So if there is anyone hwo 
> can point how to 
> do this in R - please. There is also a classs of ZINB or 
> something like 
> that for zero inflated negative binomial models.
> 
> Actually I just went on web and found a book from Simonoff "Analyzing 
> Categorical Data" and there are some examples in it for ZIP 
> et al. Look 
> examples for sections 4.5 and 5.4
> 
http://www.stern.nyu.edu/~jsimonof/AnalCatData/Splus/analcatdata.s
http://www.stern.nyu.edu/~jsimonof/AnalCatData/Splus/functions.s

-- 
Lep pozdrav / With regards,
     Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 17 888
Slovenia

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From vito_ricci at yahoo.com  Fri Jan 21 14:53:08 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Fri, 21 Jan 2005 14:53:08 +0100 (CET)
Subject: [R] R: chi-Squared distribution in Friedman test
Message-ID: <20050121135308.3475.qmail@web41207.mail.yahoo.com>

Hi,

pchisq -> distribution function
dchisq -> density function

pval is the area under the curve, to calculte it you
use distribution function which is the integral of
density function. See:

http://www.itl.nist.gov/div898/handbook/eda/section3/eda362.htm
http://mathworld.wolfram.com/DistributionFunction.html


f(x) density function
F(x) distribution function =Pr(X<x)= integral(f(x))

Hoping I helped you!
Regards
Vito

you wrote:

Dear R helpers:

Thanks for the previous reply. I am using Friedman
racing test. According the the book "Pratical
Nonprametric Statistic" by WJ Conover, after computing
the statistics, he suggested to use chi-squared or F
distribution to accept or reject null hypothesis.
After looking into the source code, I found that R
uses chi-sqaured distribution as below:

PVAL <- pchisq(STATISTIC, PARAMETER, lower = FALSE)

but still I cant figure out why they are using this
pschisq insted of dchisq. Sorry I am wrong!!


Thanking you
truly
Prasanna











=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From andy_liaw at merck.com  Fri Jan 21 15:20:05 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 21 Jan 2005 09:20:05 -0500
Subject: [R] an R script editor for Mac
Message-ID: <3A822319EB35174CA3714066D590DCD50994E571@usrymx25.merck.com>

JGR has an fairly intelligent editor, and it works on Mac.

Andy

> From: Sean Davis
> 
> Consider using ESS and xemacs or emacs.  You get syntax highlighting, 
> auto-indent, command auto-complete, transcripts for your session, 
> integrated help, and the tools of one of the most powerful 
> text editors 
> on the planet.  The learning curve is a bit steep, but if you use R 
> much, it is worth it.
> 
> Sean
> 
> On Jan 21, 2005, at 5:35 AM, Jacques VESLOT wrote:
> 
> > Dear all,
> >
> > Could someone please make me know if there is a nice script editor 
> > available
> > under Mac, similar to Crimson, that offers R syntax 
> highlighting (and 
> > pairs
> > of parentheses underlining) ?
> >
> > Thanks in advance,
> >
> > Jacques VESLOT
> > Cirad
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Fri Jan 21 15:32:42 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 21 Jan 2005 09:32:42 -0500
Subject: [R] Avoiding a Loop?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E572@usrymx25.merck.com>

If `v' the the vector in the first column and `k' is the constant, then the
matrix has k^(j-1) * v in the jth column, right?

> k <- 27.5^seq(0, length=4)
> k
[1]     1.00    27.50   756.25 20796.88
> outer(1:5, k, "*")
     [,1]  [,2]    [,3]      [,4]
[1,]    1  27.5  756.25  20796.88
[2,]    2  55.0 1512.50  41593.75
[3,]    3  82.5 2268.75  62390.63
[4,]    4 110.0 3025.00  83187.50
[5,]    5 137.5 3781.25 103984.38

Andy

> From: Rau, Roland
> 
> Dear R-Helpers,
> 
> I have a matrix where the first column is known. The second column is
> the result of multiplying this first column with a constant 
> "const". The
> third column is the result of multiplying the second column with
> "const".....
> So far, I did it like this (as a simplified example):
> 
> nr.of.columns <- 4
> 
> myconstant <- 27.5
> 
> mymatrix <- matrix(numeric(0), nrow=5, ncol=nr.of.columns)
> 
> mymatrix[,1] <- 1:5
> 
> for (i in 2:nr.of.columns) {
> 	mymatrix[,i] <- myconstant * mymatrix[,i-1]
> }
> 
> 
> Can anyone give me some advice whether it is possible to 
> avoid this loop
> (and if yes: how)?
> 
> Any suggestions are welcome!
> 
> Thanks,
> Roland
> 
> 
> 
> 
> +++++
> This mail has been sent through the MPI for Demographic 
> Rese...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From MSchwartz at MedAnalytics.com  Fri Jan 21 15:37:09 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 21 Jan 2005 08:37:09 -0600
Subject: [R] axis placement with stacked barplots and the asp=1 parameter
In-Reply-To: <C675D2B0F853F54389286F460C712C065C64DE@hyperion.sol.prytaniagroup.com>
References: <C675D2B0F853F54389286F460C712C065C64DE@hyperion.sol.prytaniagroup.com>
Message-ID: <1106318229.3497.39.camel@horizons.localdomain>

On Fri, 2005-01-21 at 12:09 +0000, Kavithan Siva wrote:
> Hi
> 
> Here's some example code which illustrates the issue:
> 
> dataSeries<-array(0, c(5, 2))
> 
> dataSeries[1, 1] <- 5
> dataSeries[2, 1] <- 5
> dataSeries[3, 1] <- 0
> dataSeries[4, 1] <- 2
> dataSeries[5, 1] <- 0
> 
> dataSeries[1, 2] <- 7
> dataSeries[2, 2] <- 0
> dataSeries[3, 2] <- 0
> dataSeries[4, 2] <- 0
> dataSeries[5, 2] <- 1
> 
> barplot(dataSeries, asp=1)
> 
> Removing the asp=1 parameter produces a correct looking plot. I need to
> use the asp=1 parameter because I was re-writing the barplot function to
> create stacked charts with rounded corners (instead of rectangles). For
> the arc at the corners of the bars to look correct I need asp=1. I've
> attached a .ps file to show you what I was trying to achieve.
> 
> Thanks
> Kav

There are a couple of options to consider here. First, note that using
the 'asp' argument forces the x and y axes to be scaled such that one
unit of measurement in each direction is the same. As you are seeing,
this impacts the look of curves and of course would be relevant to
certain plots where the horizontal and vertical measures need to be
"square".

One option is to leave the plot as is, but "move" the y axis closer to
the bars. Using your same data above:

# Draw the plot, but no axes
barplot(dataSeries, asp=1, axes= FALSE)

# Now using the 'line' argument, move the axis closer
# to the bars. Negative values move the axis to the right.
axis(2, line = -14)


Another option, which results in something closer to the barplot when
not using the 'asp' argument, is to adjust the 'width' argument to
increase the horizontal size of the bars:

# Draw the barplot and increase the width of the bars
# Do not draw the axes
barplot(dataSeries, asp=1, width = 8, axes = FALSE)

# Now draw the y axis
axis(2, at = seq(0, 12, 2))

Note however, that in this example, the vertical dimension is
"shortened", since the 'asp' argument is still trying to properly set
the aspect ratio and the larger bar width increases the range of the x
axis.


Finally(?), another option to consider is to set par(pty = "s") as an
alternative to using 'asp = 1'. This results in a square plot region,
rather than a maximal (typically rectangular) one. See ?par for more
info here. In this case:

par(pty = "s")
barplot(dataSeries)

This might result in better looking arcs for your corners.

Without having your modified barplot() function, it is hard to know
which might work best here, but hopefully this might provide some
possibilities.

HTH,

Marc Schwartz



From davidr at rhotrading.com  Fri Jan 21 15:42:43 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Fri, 21 Jan 2005 08:42:43 -0600
Subject: [R] Straight-line fitting with errors in both coordinates
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A422DE6@rhosvr02.rhotrading.com>

You want to look in the archives or elsewhere online for "errors in
variables" or "total least squares". There are many resources. Note that
the method in NR assumes both variables have equal-sized observational
errors. If that is not the case for your data, then the method is
inappropriate.

HTH,
David Reiner

-----Original Message-----
From: Pan, Chongle [mailto:cpan at utk.edu] 
Sent: Thursday, January 20, 2005 2:47 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Straight-line fitting with errors in both coordinates

Hi All,
I want to fit a straight line into a group of two-dimensional data
points with 
errors in both x and y coordinates. I found there is an algorithm
provided in 
"NUMERICAL RECIPES IN C"
http://www.library.cornell.edu/nr/bookcpdf/c15-3.pdf

I'm wondering if there is a similar function for this implemented in R.
And 
how can I change the objective function, from example, from sum of
squared 
error to sum of absolute error?

Regards,
Chongle

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From sfalcon at fhcrc.org  Fri Jan 21 15:44:27 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Fri, 21 Jan 2005 06:44:27 -0800
Subject: [R] Parallel computations using snow: how to combine boot objects?
In-Reply-To: <24D0F1947691984E89F3151A7DC314DDAC1FCB@EXCHANGE2.unifr.ch>
References: <24D0F1947691984E89F3151A7DC314DDAC1FCB@EXCHANGE2.unifr.ch>
Message-ID: <F3837051-6BBA-11D9-8FDA-000D933A3A9E@fhcrc.org>

Take a look at the utility function in snow called something like 
clusterSplit.



From christian_mora at vtr.net  Fri Jan 21 15:44:52 2005
From: christian_mora at vtr.net (christian_mora@vtr.net)
Date: Fri, 21 Jan 2005 10:44:52 -0400
Subject: [R] mixed effects model:how to include  initial conditions 
In-Reply-To: <41F0E7BF.6050503@uni-jena.de>
Message-ID: <41E28889000295C6@hudson.vtr.net>

Christoph,

If you take a look at journal articles related to this topic published elsewhere,
you will find that the most common analysis is:

model<-lme(growth~initialsize+block+treatment+....)

maybe it would be important to check if your model really needs the interactions
you pointed out. I would suggest to try simpler models first....which generally
work OK in analysis of growth of plants, trees, etc

CM

>-- Mensaje Original --
>Date: Fri, 21 Jan 2005 12:30:07 +0100
>From: Christoph Scherber <Christoph.Scherber at uni-jena.de>
>To: r-help at stat.math.ethz.ch
>Subject: [R] mixed effects model:how to include  initial conditions 
>
>
>Dear R users,
>
>I am analyzing a dataset on growth of plants in response to several 
>factors. I am using a mixed-effects model of the following structure:
>
>model<-lme(growth~block*treatment*factor1*factor2,
>random=~1|plot/treatment/initialsize)
>
>I have measured the initial size of the plants (in 2003) and thought it

>might be sensible to include this (random) variation into the random 
>effects term of the model.
>
>Is that correct? Or should "initialsize" rather be included as a 
>covariate into the fixed effects term, as in:
>
>alternative<-lme(growth~block*initialsize*treatment*factor1*factor2,
>random=~1|plot/treatment)
>
>I would very much appreciate any suggestions on how to analyze these 
>data correctly.
>
>Best regards
>Chris.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Jan 21 15:47:16 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 21 Jan 2005 15:47:16 +0100
Subject: [R] axis placement with stacked barplots and the asp=1 parameter
In-Reply-To: <C675D2B0F853F54389286F460C712C065C64DE@hyperion.sol.prytaniagroup.com>
References: <C675D2B0F853F54389286F460C712C065C64DE@hyperion.sol.prytaniagroup.com>
Message-ID: <41F115F4.9080109@statistik.uni-dortmund.de>

Kavithan Siva wrote:

> Hi
> 
> Here's some example code which illustrates the issue:
> 
> dataSeries<-array(0, c(5, 2))
> 
> dataSeries[1, 1] <- 5
> dataSeries[2, 1] <- 5
> dataSeries[3, 1] <- 0
> dataSeries[4, 1] <- 2
> dataSeries[5, 1] <- 0
> 
> dataSeries[1, 2] <- 7
> dataSeries[2, 2] <- 0
> dataSeries[3, 2] <- 0
> dataSeries[4, 2] <- 0
> dataSeries[5, 2] <- 1
> 
> barplot(dataSeries, asp=1)
> 
> Removing the asp=1 parameter produces a correct looking plot. I need to
> use the asp=1 parameter because I was re-writing the barplot function to
> create stacked charts with rounded corners (instead of rectangles). For
> the arc at the corners of the bars to look correct I need asp=1. I've
> attached a .ps file to show you what I was trying to achieve.

Marc Schwartz already made some good points.
I'd like to suggest to build your new function up on grid using 
viewports in order to finally get a clean implementation.

Uwe Ligges



> Thanks
> Kav
> 
> 
> Kavithan Siva
> Prytania Investment Advisors LLP
> 105 Ladbroke Grove, London, W11 1PG
> Tel: +44 20 7616 8475   Fax: +44 20 7616 8472
> Prytania Investment Advisors LLP is regulated by the FSA
> 
> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
> Sent: Friday, January 21, 2005 11:52 AM
> To: Kavithan Siva
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] axis placement with stacked barplots and the asp=1
> parameter
> 
> Kavithan Siva wrote:
> 
> 
>>Hi,
>>
>>I'm currently using barplot to generate vertically stacked bar charts.
>>I'd like to be able to use the "asp=1" parameter with barplot(), but
>>doing this causes the y axis to be placed on the far left as shown in
>>the attachment demo.pdf.
> 
> 
> See the posting guide, some file types (including pdf) will be omitted 
> from your messages to R-help. You might want to specify a short but 
> easily reproducible examples in 2-5 lines of code.
> 
> Why do you want to use "asp" with a barplot???
> 
> Uwe Ligges
> 
> 
> 
>>I could get around this by using the negative values for the line
>>parameter of the axis() function. I'd rather not do this and imagine
> 
> I'm
> 
>>doing something wrong. Any help would be much appreciated.
>>
>>Thanks
>>Kav
>> 
>> ---
>>This e-mail may contain confidential and/or privileged information. If
> 
> you are not the 
> 
>>intended recipient (or have received this e-mail in error) please
> 
> notify the sender 
> 
>>immediately and destroy this e-mail. Any unauthorised copying,
> 
> disclosure or distribution 
> 
>>of the material in this e-mail is strictly forbidden. The Prytania
> 
> Group has taken every 
> 
>>reasonable precaution to ensure that any attachment to this e-mail has
> 
> been swept for 
> 
>>viruses. However, we cannot accept liability for any damage sustained
> 
> as a result of 
> 
>>software viruses and would advise that you carry out your own virus
> 
> checks before 
> 
>>opening any attachment.
>>
>>
>>
> 
> ------------------------------------------------------------------------
> 
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
> 
> http://www.R-project.org/posting-guide.html
>  
>  ---
> This e-mail may contain confidential and/or privileged inf...{{dropped}}



From james.holtman at convergys.com  Fri Jan 21 15:56:53 2005
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Fri, 21 Jan 2005 09:56:53 -0500
Subject: [R] Avoiding a Loop?
Message-ID: <OF6A331C4C.086484C2-ON85256F90.0052095F@nd.convergys.com>





Does this do what you want?

nr.of.columns <- 4

myconstant <- 27.5

mymatrix <- matrix(myconstant, nrow=5, ncol=nr.of.columns)

mymatrix[,1] <- 1:5

t(apply(mymatrix, 1, function(x) cumprod(x)))


__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      "Rau, Roland"                                                                                                        
                      <Rau at demogr.mpg.de>          To:       <r-help at stat.math.ethz.ch>                                                    
                      Sent by:                     cc:                                                                                     
                      r-help-bounces at stat.m        Subject:  [R] Avoiding a Loop?                                                          
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      01/21/2005 07:31                                                                                                     
                                                                                                                                           
                                                                                                                                           




Dear R-Helpers,

I have a matrix where the first column is known. The second column is
the result of multiplying this first column with a constant "const". The
third column is the result of multiplying the second column with
"const".....
So far, I did it like this (as a simplified example):

nr.of.columns <- 4

myconstant <- 27.5

mymatrix <- matrix(numeric(0), nrow=5, ncol=nr.of.columns)

mymatrix[,1] <- 1:5

for (i in 2:nr.of.columns) {
             mymatrix[,i] <- myconstant * mymatrix[,i-1]
}


Can anyone give me some advice whether it is possible to avoid this loop
(and if yes: how)?

Any suggestions are welcome!

Thanks,
Roland




+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From olau at fas.harvard.edu  Fri Jan 21 16:00:01 2005
From: olau at fas.harvard.edu (Olivia Lau)
Date: Fri, 21 Jan 2005 10:00:01 -0500
Subject: [R] dim vs length for vectors
References: <000301c4ff76$e7951ef0$7700a8c0@olau>	<loom.20050121T063201-271@post.gmane.org>
	<200501211201.59297.ahenningsen@email.uni-kiel.de>
	<41F0EDBC.4080709@statistik.uni-dortmund.de>
Message-ID: <002f01c4ffc9$e1d8aec0$6501a8c0@olau>

>> IMHO the difference between vectors and 1-dimensional arrays 
>> is really annoying and I had already several bugs in my code, 
>> because I mixed these up.
>> Is it possible in future versions of R that R does not 
>> differentiate between vectors and 1-dimensional arrays (e.g. 
>> by treating all vectors as 1-dimensional arrays)?
>
> No, that would break huge amounts of code!
>
> See ?"[" and learn how to use its argument "drop".

What I was proposing doesn't require a lot of programming.  Just 
whenever you call dim(), it does length() if the object is a 
vector and returns it in the format:

R> a <- 1:12
R> dim(a)
  [1]  12

This means that you can provide a unified introduction to data 
structures that take only one type of atomic value (and 
generally call these structures "arrays").  What I call a "one 
dimensional array" only has one dim attribute and hence would 
use a[4] for extraction (for example), which is consistent with 
the usage for "[" (to my understanding).  That way, when you 
introduce "[" and arrays at the same time, you can tell 
beginners:
  1) Run dim()
  2) If there's one dim, use [  ]; if there are 2 dims, use [ 
,  ]; if there are 3 dims, you use [ , , ].
This is conceptually easy for a beginner and avoids a bit of 
frustration.

If I need to initiate an empty vector for example I use

R>  a <- array()

It just looks like the difference between a 1-d array (which 
doesn't exist as far as I can tell) and a vector is semantic, 
and I think that R would be more logical if they were treated as 
the same thing.  This doesn't mean changing the way the 
is.array(), is.vector(), <-, etc functions work, but just 
changing dim().

Yours,

Olivia Lau



From deepayan at stat.wisc.edu  Fri Jan 21 16:08:32 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 21 Jan 2005 09:08:32 -0600
Subject: [R] Selecting a subplot of pairs
In-Reply-To: <41F0C7AD.20307@fsagx.ac.be>
References: <41F0C7AD.20307@fsagx.ac.be>
Message-ID: <200501210908.32315.deepayan@stat.wisc.edu>

On Friday 21 January 2005 03:13, Yves Brostaux wrote:
> Hello,
>
> I'm trying to plot a set of 3 dependant variables (y) against 4
> predictors (x) in a matrix-like plot, sharing x- an y-axis for all
> the plot on the same column/line :
>
> y1/x1   y1/x2   y1/x3   y1/x4
> y2/x1   y2/x2   y2/x3   y2/x4
> y3/x1   y3/x2   y3/x3   y3/x4
>
> In fact, this plot is a rectangular selection of the result of
> pairs(), limited to the relations between x's and y's and excluding
> those within x's and y's. I managed to recreate such a plot using a
> script with layout(), axis() and so on, but I was wondering if there
> already exists a clean function for such a task, in case I would
> encounter this problem again ?

You could use xyplot from lattice as:

xyplot(y1 + y2 + y3 ~ x1 + x2 + x3 + x4, data=, outer=TRUE)

You can add relation="free" to get column and row-specific axis limits, 
but they will be shown for every panel.

Hope that helps,

Deepayan



From Rau at demogr.mpg.de  Fri Jan 21 16:07:26 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Fri, 21 Jan 2005 16:07:26 +0100
Subject: [R] Avoiding a Loop?
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E6150671@HERMES.demogr.mpg.de>

Dear R-List,

thank you very much for the fast answers.
Andy Liaw and James Holtman both gave me some example code which does
exactly what I want.

I will now try to check which one is better suited for my actual matrix
(in terms of speed).

Thanks,
Roland


-----Original Message-----
From: james.holtman at convergys.com [mailto:james.holtman at convergys.com] 
Sent: Friday, January 21, 2005 3:57 PM
To: Rau, Roland
Cc: r-help at stat.math.ethz.ch; r-help-bounces at stat.math.ethz.ch
Subject: Re: [R] Avoiding a Loop?





Does this do what you want?

nr.of.columns <- 4

myconstant <- 27.5

mymatrix <- matrix(myconstant, nrow=5, ncol=nr.of.columns)

mymatrix[,1] <- 1:5

t(apply(mymatrix, 1, function(x) cumprod(x)))


__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


 

                      "Rau, Roland"

                      <Rau at demogr.mpg.de>          To:
<r-help at stat.math.ethz.ch>

                      Sent by:                     cc:

                      r-help-bounces at stat.m        Subject:  [R]
Avoiding a Loop?

                      ath.ethz.ch

 

 

                      01/21/2005 07:31

 

 





Dear R-Helpers,

I have a matrix where the first column is known. The second column is
the result of multiplying this first column with a constant "const". The
third column is the result of multiplying the second column with
"const".....
So far, I did it like this (as a simplified example):

nr.of.columns <- 4

myconstant <- 27.5

mymatrix <- matrix(numeric(0), nrow=5, ncol=nr.of.columns)

mymatrix[,1] <- 1:5

for (i in 2:nr.of.columns) {
             mymatrix[,i] <- myconstant * mymatrix[,i-1]
}


Can anyone give me some advice whether it is possible to avoid this loop
(and if yes: how)?

Any suggestions are welcome!

Thanks,
Roland




+++++
This mail has been sent through the MPI for Demographic
Rese...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html





+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From david.shin at pearson.com  Fri Jan 21 16:08:21 2005
From: david.shin at pearson.com (Shin, David)
Date: Fri, 21 Jan 2005 09:08:21 -0600
Subject: [R] output data in a fixed format
Message-ID: <6F3CF8F7047E374CAF4DCED3ED14576E09621042@iowacexch4.ic.ncs.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050121/d9620fd7/attachment.pl

From ligges at statistik.uni-dortmund.de  Fri Jan 21 16:21:47 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 21 Jan 2005 16:21:47 +0100
Subject: [R] dim vs length for vectors
In-Reply-To: <002f01c4ffc9$e1d8aec0$6501a8c0@olau>
References: <000301c4ff76$e7951ef0$7700a8c0@olau>	<loom.20050121T063201-271@post.gmane.org>
	<200501211201.59297.ahenningsen@email.uni-kiel.de>
	<41F0EDBC.4080709@statistik.uni-dortmund.de>
	<002f01c4ffc9$e1d8aec0$6501a8c0@olau>
Message-ID: <41F11E0B.6020405@statistik.uni-dortmund.de>

Olivia Lau wrote:
>>> IMHO the difference between vectors and 1-dimensional arrays is 
>>> really annoying and I had already several bugs in my code, because I 
>>> mixed these up.
>>> Is it possible in future versions of R that R does not differentiate 
>>> between vectors and 1-dimensional arrays (e.g. by treating all 
>>> vectors as 1-dimensional arrays)?
>>
>>
>> No, that would break huge amounts of code!
>>
>> See ?"[" and learn how to use its argument "drop".
> 
> 
> What I was proposing doesn't require a lot of programming.  Just 
> whenever you call dim(), it does length() if the object is a vector and 
> returns it in the format:
> 
> R> a <- 1:12
> R> dim(a)
>  [1]  12

Dim reports the dimesnion attribute, so why do you want to make it 
inconsistant????



> This means that you can provide a unified introduction to data 
> structures that take only one type of atomic value (and generally call 
> these structures "arrays").  What I call a "one dimensional array" only 
> has one dim attribute and hence would use a[4] for extraction (for 
> example), which is consistent with the usage for "[" (to my 
> understanding).  That way, when you introduce "[" and arrays at the same 
> time, you can tell beginners:
>  1) Run dim()
>  2) If there's one dim, use [  ]; if there are 2 dims, use [ ,  ]; if 
> there are 3 dims, you use [ , , ].
> This is conceptually easy for a beginner and avoids a bit of frustration.


Well, you forgot that even a matrix is represented as a vector, but with 
dimension attributes!
And indexing a matrix vector-like has some benefits in some cases.


> If I need to initiate an empty vector for example I use
> 
> R>  a <- array()
> 
> It just looks like the difference between a 1-d array (which doesn't 
> exist as far as I can tell) and a vector is semantic, and I think that R 


Note the difference:

   x <- 1:5
   dim(x) # NULL
   str(x) # int [1:5] 1 2 3 4 5

   x <- array(1:5, dim=5)
   dim(x) # [1] 5
   str(x) # int [, 1:5] 1 2 3 4 5



> would be more logical if they were treated as the same thing.  This 
> doesn't mean changing the way the is.array(), is.vector(), <-, etc 
> functions work, but just changing dim().

But then, you won't see the difference beween an 1D array (vector with 
dim attribute and a vector without dim attribute any more!

You might want to read the R Language Definition manual.

Uwe Ligges

> Yours,
> 
> Olivia Lau



From ligges at statistik.uni-dortmund.de  Fri Jan 21 16:25:22 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 21 Jan 2005 16:25:22 +0100
Subject: [R] chi-Squared distribution in Friedman test
In-Reply-To: <200501211338.OAA23054@web1.ulb.ac.be>
References: <200501211338.OAA23054@web1.ulb.ac.be>
Message-ID: <41F11EE2.1090009@statistik.uni-dortmund.de>

Prasanna Balaprakash wrote:

> Dear R helpers:
> 
> 
> Thanks for the previous reply. I am using Friedman racing test. According the the book "Pratical Nonprametric Statistic" by WJ Conover, after computing the statistics, he suggested to use chi-squared or F distribution to accept or reject null hypothesis. After looking into the source code, I found that R uses chi-sqaured distribution as below:
> 
> PVAL <- pchisq(STATISTIC, PARAMETER, lower = FALSE)
> 
> but still I cant figure out why they are using this pschisq insted of dchisq. Sorry I am wrong!!

You want to get the distribution function rather than the density.

Uwe Ligges


> 
> Thanking you
> truly
> Prasanna
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
>>----- Original Message ----- 
>>From: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.ac.be>
>>To: "Prasanna Balaprakash" <pbalapra at ulb.ac.be>
>>Cc: <r-help at stat.math.ethz.ch>
>>Sent: Friday, January 21, 2005 1:05 PM
>>Subject: Re: [R] chi-Squared distribution
>>
>>
>>
>>>if you check ?qchisq, you'll see that the second
>>
>>My mistake, the third argument
>>
>>
>>>argument of the function denotes the non-centrality parameter! Try
>>>
>>>qchisq(0.75, 1:3)
>>>
>>>to get your answer.
>>>
>>>Best,
>>>Dimitris
>>>
>>>----
>>>Dimitris Rizopoulos
>>>Ph.D. Student
>>>Biostatistical Centre
>>>School of Public Health
>>>Catholic University of Leuven
>>>
>>>Address: Kapucijnenvoer 35, Leuven, Belgium
>>>Tel: +32/16/336899
>>>Fax: +32/16/337015
>>>Web: http://www.med.kuleuven.ac.be/biostat
>>>    http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>>>
>>>
>>>----- Original Message ----- 
>>>From: "Prasanna Balaprakash" <pbalapra at ulb.ac.be>
>>>To: <r-help at stat.math.ethz.ch>
>>>Sent: Friday, January 21, 2005 12:27 PM
>>>Subject: [R] chi-Squared distribution
>>>
>>>
>>>
>>>>Dear Rs:
>>>>
>>>>
>>>>outer(1:3, 1:3, function(df1, df2) qf(0.95, df1, df2))
>>>>
>>>>I compare this F distribution results with the table, the answers 
>>>>were perfect. But I need to see for chi-sqaured distribution. When 
>>>>I employed the similar formula
>>>>
>>>>outer(1:3, 1:3, function(df1, df2) qchisq(0.95, df1, df2)) , I am 
>>>>getting unexpected results. I need to see the following values:
>>>>
>>>>    p=0.750  .....
>>>>1     1.323
>>>>
>>>>2     2.773
>>>>
>>>>3     4.108
>>>>
>>>>
>>>>Thanking you
>>>>Prasanna
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>Prasanna Balaprakash,
>>>>Universit? Libre de Bruxelles,
>>>>50, Av. F. Roosevelt, CP 194/6,
>>>>B-1050 Brussels,
>>>>Belgium.
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! 
>>>>http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From zbarta at delfin.unideb.hu  Fri Jan 21 16:27:14 2005
From: zbarta at delfin.unideb.hu (Zoltan Barta)
Date: Fri, 21 Jan 2005 16:27:14 +0100
Subject: [R] Avoiding a Loop?
References: <8B08A3A1EA7AAC41BE24C750338754E6150667@HERMES.demogr.mpg.de>
Message-ID: <87k6q6n6p9.fsf@delfin.unideb.hu>

"Rau, Roland" <Rau at demogr.mpg.de> writes:

> Dear R-Helpers,
>
> I have a matrix where the first column is known. The second column is
> the result of multiplying this first column with a constant "const". The
> third column is the result of multiplying the second column with
> "const".....
> So far, I did it like this (as a simplified example):
>
> nr.of.columns <- 4
>
> myconstant <- 27.5
>
> mymatrix <- matrix(numeric(0), nrow=5, ncol=nr.of.columns)
>
> mymatrix[,1] <- 1:5
>
> for (i in 2:nr.of.columns) {
> 	mymatrix[,i] <- myconstant * mymatrix[,i-1]
> }
>
>
> Can anyone give me some advice whether it is possible to avoid this loop
> (and if yes: how)?
>

How about:

> myconstant <- 27.5
> a <- 1:5
> myconstant <- myconstant^((1:nr.of.columns)-1)
> myconstant
[1]     1.00    27.50   756.25 20796.88
> mymatrix <- outer(a,myconstant)
> mymatrix
     [,1]  [,2]    [,3]      [,4]
[1,]    1  27.5  756.25  20796.88
[2,]    2  55.0 1512.50  41593.75
[3,]    3  82.5 2268.75  62390.62
[4,]    4 110.0 3025.00  83187.50
[5,]    5 137.5 3781.25 103984.38
> 

HTH

Zoltan

> Any suggestions are welcome!
>
> Thanks,
> Roland
>
>
>
>
> +++++
> This mail has been sent through the MPI for Demographic Rese...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 


Z. Barta 
Dept. of Evol. Zool., Univ. of Debrecen, Debrecen, H-4010, Hungary
Phone: 36 52 316 666 ext. 2334, Fax: 36 52 533 677
E-mail: zbarta at delfin.unideb.hu, http://puma.unideb.hu/~zbarta/



From ligges at statistik.uni-dortmund.de  Fri Jan 21 16:30:27 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 21 Jan 2005 16:30:27 +0100
Subject: [R] Avoiding a Loop?
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E6150667@HERMES.demogr.mpg.de>
References: <8B08A3A1EA7AAC41BE24C750338754E6150667@HERMES.demogr.mpg.de>
Message-ID: <41F12013.60507@statistik.uni-dortmund.de>

Rau, Roland wrote:

> Dear R-Helpers,
> 
> I have a matrix where the first column is known. The second column is
> the result of multiplying this first column with a constant "const". The
> third column is the result of multiplying the second column with
> "const".....
> So far, I did it like this (as a simplified example):
> 
> nr.of.columns <- 4
> 
> myconstant <- 27.5
> 
> mymatrix <- matrix(numeric(0), nrow=5, ncol=nr.of.columns)
> 
> mymatrix[,1] <- 1:5
> 
> for (i in 2:nr.of.columns) {
> 	mymatrix[,i] <- myconstant * mymatrix[,i-1]
> }

   nr.of.columns <- 4
   myconstant <- 27.5
   mycolumn <- 1:5
   outer(mycolumn, myconstant^(0:(nr.of.columns-1)))

Uwe Ligges



> 
> Can anyone give me some advice whether it is possible to avoid this loop
> (and if yes: how)?
> 
> Any suggestions are welcome!
> 
> Thanks,
> Roland
> 
> 
> 
> 
> +++++
> This mail has been sent through the MPI for Demographic Rese...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Fri Jan 21 17:25:59 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 21 Jan 2005 08:25:59 -0800
Subject: [R] chi-Squared distribution in Friedman test
In-Reply-To: <200501211338.OAA23054@web1.ulb.ac.be>
References: <200501211338.OAA23054@web1.ulb.ac.be>
Message-ID: <41F12D17.8030704@pdf.com>

Easy:  First letter

      "d" = density,
      "p" = probability = cumulative distribution function (cdf),
      "q" = quantile = inverse cdf,
      "r" = pseudo-random number generation. 

      This is in, for example, "An Introduction to R".  Are you familiar 
with "help.start()"?  From an R command prompt (at least with recent 
versions under Windows), this opens a web browser with a page offering 
"manuals", "reference", and "Miscellaneous Material".  "An Intro to R" 
is in the upper left.  "Probability Distributions" is section 8. 

      hope this helps.  spencer graves

Prasanna Balaprakash wrote:

>Dear R helpers:
>
>
>Thanks for the previous reply. I am using Friedman racing test. According the the book "Pratical Nonprametric Statistic" by WJ Conover, after computing the statistics, he suggested to use chi-squared or F distribution to accept or reject null hypothesis. After looking into the source code, I found that R uses chi-sqaured distribution as below:
>
>PVAL <- pchisq(STATISTIC, PARAMETER, lower = FALSE)
>
>but still I cant figure out why they are using this pschisq insted of dchisq. Sorry I am wrong!!
>
>
>Thanking you
>truly
>Prasanna
>
>
>
>
>
>
>
>
>
>  
>
>>----- Original Message ----- 
>>From: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.ac.be>
>>To: "Prasanna Balaprakash" <pbalapra at ulb.ac.be>
>>Cc: <r-help at stat.math.ethz.ch>
>>Sent: Friday, January 21, 2005 1:05 PM
>>Subject: Re: [R] chi-Squared distribution
>>
>>
>>    
>>
>>>if you check ?qchisq, you'll see that the second
>>>      
>>>
>>My mistake, the third argument
>>
>>    
>>
>>>argument of the function denotes the non-centrality parameter! Try
>>>
>>>qchisq(0.75, 1:3)
>>>
>>>to get your answer.
>>>
>>>Best,
>>>Dimitris
>>>
>>>----
>>>Dimitris Rizopoulos
>>>Ph.D. Student
>>>Biostatistical Centre
>>>School of Public Health
>>>Catholic University of Leuven
>>>
>>>Address: Kapucijnenvoer 35, Leuven, Belgium
>>>Tel: +32/16/336899
>>>Fax: +32/16/337015
>>>Web: http://www.med.kuleuven.ac.be/biostat
>>>    http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>>>
>>>
>>>----- Original Message ----- 
>>>From: "Prasanna Balaprakash" <pbalapra at ulb.ac.be>
>>>To: <r-help at stat.math.ethz.ch>
>>>Sent: Friday, January 21, 2005 12:27 PM
>>>Subject: [R] chi-Squared distribution
>>>
>>>
>>>      
>>>
>>>>Dear Rs:
>>>>
>>>>
>>>>outer(1:3, 1:3, function(df1, df2) qf(0.95, df1, df2))
>>>>
>>>>I compare this F distribution results with the table, the answers 
>>>>were perfect. But I need to see for chi-sqaured distribution. When 
>>>>I employed the similar formula
>>>>
>>>>outer(1:3, 1:3, function(df1, df2) qchisq(0.95, df1, df2)) , I am 
>>>>getting unexpected results. I need to see the following values:
>>>>
>>>>    p=0.750  .....
>>>>1     1.323
>>>>
>>>>2     2.773
>>>>
>>>>3     4.108
>>>>
>>>>
>>>>Thanking you
>>>>Prasanna
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>Prasanna Balaprakash,
>>>>Universit? Libre de Bruxelles,
>>>>50, Av. F. Roosevelt, CP 194/6,
>>>>B-1050 Brussels,
>>>>Belgium.
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! 
>>>>http://www.R-project.org/posting-guide.html
>>>>        
>>>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From lamkelj at yahoo.com  Fri Jan 21 17:31:31 2005
From: lamkelj at yahoo.com (Kel Lam)
Date: Fri, 21 Jan 2005 08:31:31 -0800 (PST)
Subject: [R] gamm with correlation structure question
Message-ID: <20050121163131.86500.qmail@web52702.mail.yahoo.com>

Dear group,

I am trying to use gamm() in mgcv.  Here's the
scenario.  The data frame has approx. 110K
observations with information on paediatric
readmission binary outcome (Y/N) and total volume of
their most responsible physician as the covariate. 
Since any physician can have multiple patients, the
data contains clustering structure which I am trying
to account for.  My original formula is

a <- gamm(readmission~s(volume,fx=F),
correlation=corCompSymm(0.1,form=~1|physician),
family=binomial)

During the first iteration, I received the warning "
Incompatible formulas for groups in "random" and
"correlation"".  Frankly I don't understand what it
means.  So I looked up the archive and saw one posted
by Prof. Thomas Lumley on lme as follows:

http://tolstoy.newcastle.edu.au/R/help/00a/0913.html

So I changed my formula as this:

a <- gamm(readmission~s(volume,fx=F),
random=list(physician=~1),family=binomial)

However I still get an error message saying "Lapack
routine dgesv : system is exactly singular".  

Can anyone suggest a better way to deal with my
problem.  Please let me know if you need further
details.  Thank you so much.

Regards,
Kel



From halldor at vedur.is  Fri Jan 21 17:49:06 2005
From: halldor at vedur.is (=?ISO-8859-1?Q?Halldor_Bj=F6rnsson?=)
Date: Fri, 21 Jan 2005 16:49:06 +0000
Subject: [R] Windows plots & fontsize 
Message-ID: <41F13282.1060005@vedur.is>

I recently wrote a package that runs on both Linux and Windows.
The functions fetch data to an SQL database and make diverse plots, 
usually with many labels and annotations.

I recently noticed that on SOME windows computers the fonts
in the plots were too large, which caused labels to overlap.

On other windows computers this does not happen.
However on both computers the command par("ps") gives 12

Now, windows is not my forte, but it seems likely that this has to do 
with some user/system settings. If so, my users would probably object to 
me tweaking their windows setup since it might have an effect on other 
programs.

One simple solution is to use par(ps=10), but I cannot find out how to
set this globally.


a) Does anyone know how I can set par(ps=10) by default on all figure?

b) Is there a windows specific way to set default fontsizes in figures?

Thanks,
Halld?r

-- 
------------------------------------------
Halldor Bjornsson   (halldor at vedur.is)
Vedurstofa Islands (Icelandic Met. Office)
Bustadavegur 9, IS-150, Reykjavik, Iceland



From gunter.berton at gene.com  Fri Jan 21 17:53:01 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 21 Jan 2005 08:53:01 -0800
Subject: [R] R Reference Card (especially useful for Newbies)
Message-ID: <200501211653.j0LGr2em014122@compton.gene.com>

[This hopefully helpful note is posted about once a month]

Newbies (and others!) may find the R Reference Card made available by Tom
Short and Rpad at http://www.rpad.org/Rpad/Rpad-refcard.pdf  useful. It
categorizes and organizes a bunch of R's (S's) basic, most used functions so
that they can be easily found. For example, paste() is under the "Strings"
heading and expand.grid() is under "Data Creation." For newbies struggling
to find the right R function as well as veterans who can't quite remember
the function name, it's very handy.


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA



From roger.bos at gmail.com  Fri Jan 21 18:07:19 2005
From: roger.bos at gmail.com (roger bos)
Date: Fri, 21 Jan 2005 12:07:19 -0500
Subject: [R] how to use do.call("rbind", get(list(mlist)))
Message-ID: <1db7268005012109071f3454df@mail.gmail.com>

I have around 200 data frames I want to rbind in a vectorized way. 
The object names are:

m302
m303
...
m500

So I tried:

mlist <- paste("m",302:500,sep="")
dat <- do.call("rbind", get(list(mlist)))

and I get "Error in get(x, envir, mode, inherits) : invalid first argument"

I know "rbind" is valid because 

dat <- rbind(m302, m303, m304, m305)

works, I am just too lazy to type it out to m500.

I also tried it without the get() portion, but then dat ends up being
a column with just the names of the objects, not the objects
themselves.

Thanks in advance for showing me the errors in my attempts.

Roger



From aldi at wubios.wustl.edu  Fri Jan 21 18:25:43 2005
From: aldi at wubios.wustl.edu (Aldi Kraja)
Date: Fri, 21 Jan 2005 11:25:43 -0600
Subject: [R] text miner:
Message-ID: <41F13B17.8020207@wubios.wustl.edu>

Hi,
Does a text miner exist in R-language similar to Splus miner or SAS text 
miner?
I would appreciate any information.
TIA,
Aldi

--



From ggrothendieck at myway.com  Fri Jan 21 18:21:39 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 21 Jan 2005 12:21:39 -0500 (EST)
Subject: [R] dim vs length for vectors
Message-ID: <20050121172139.7AC5E3A06@mprdmxin.myway.com>



Perhaps you could just write dim(array(x)) when you want
that effect.  That has the advantage of not requiring any
change to R and preserving the meaning of dim(x) as retrieving
the dim attribute.


Date:   Fri, 21 Jan 2005 10:00:01 -0500 
From:   Olivia Lau <olau at fas.harvard.edu>
To:   Uwe Ligges <ligges at statistik.uni-dortmund.de>,Arne Henningsen <ahenningsen at email.uni-kiel.de> 
Cc:   <r-help at stat.math.ethz.ch> 
Subject:   Re: [R] dim vs length for vectors 


What I was proposing doesn't require a lot of programming. Just 
whenever you call dim(), it does length() if the object is a 
vector and returns it in the format:

R> a <- 1:12
R> dim(a)
[1] 12

This means that you can provide a unified introduction to data 
structures that take only one type of atomic value (and 
generally call these structures "arrays"). What I call a "one 
dimensional array" only has one dim attribute and hence would 
use a[4] for extraction (for example), which is consistent with 
the usage for "[" (to my understanding). That way, when you 
introduce "[" and arrays at the same time, you can tell 
beginners:
1) Run dim()
2) If there's one dim, use [ ]; if there are 2 dims, use [ 
, ]; if there are 3 dims, you use [ , , ].
This is conceptually easy for a beginner and avoids a bit of 
frustration.

If I need to initiate an empty vector for example I use

R> a <- array()

It just looks like the difference between a 1-d array (which 
doesn't exist as far as I can tell) and a vector is semantic, 
and I think that R would be more logical if they were treated as 
the same thing. This doesn't mean changing the way the 
is.array(), is.vector(), <-, etc functions work, but just 
changing dim().



From reid_huntsinger at merck.com  Fri Jan 21 18:42:55 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Fri, 21 Jan 2005 12:42:55 -0500
Subject: [R] dim vs length for vectors
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9271@uswpmx00.merck.com>

I think you can assign a dim attribute to a vector to make a 1-d array.

> v <- 1:10
> dim(v)
NULL
>dim(v) <- 10
>dim(v)
[1] 10

Also,

> u <- array(1:10,dim=10)
> u
 [1]  1  2  3  4  5  6  7  8  9 10
> dim(u)
[1] 10

Perhaps the philosophy is that vectors are the basic, underlying type, and
arrays (including 1-d arrays) carry additional structure information. So
arrays are vectors, if you forget the "dim" attribute, but vectors are not
any particular type of array.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Olivia Lau
Sent: Friday, January 21, 2005 10:00 AM
To: Uwe Ligges; Arne Henningsen
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] dim vs length for vectors


>> IMHO the difference between vectors and 1-dimensional arrays 
>> is really annoying and I had already several bugs in my code, 
>> because I mixed these up.
>> Is it possible in future versions of R that R does not 
>> differentiate between vectors and 1-dimensional arrays (e.g. 
>> by treating all vectors as 1-dimensional arrays)?
>
> No, that would break huge amounts of code!
>
> See ?"[" and learn how to use its argument "drop".

What I was proposing doesn't require a lot of programming.  Just 
whenever you call dim(), it does length() if the object is a 
vector and returns it in the format:

R> a <- 1:12
R> dim(a)
  [1]  12

This means that you can provide a unified introduction to data 
structures that take only one type of atomic value (and 
generally call these structures "arrays").  What I call a "one 
dimensional array" only has one dim attribute and hence would 
use a[4] for extraction (for example), which is consistent with 
the usage for "[" (to my understanding).  That way, when you 
introduce "[" and arrays at the same time, you can tell 
beginners:
  1) Run dim()
  2) If there's one dim, use [  ]; if there are 2 dims, use [ 
,  ]; if there are 3 dims, you use [ , , ].
This is conceptually easy for a beginner and avoids a bit of 
frustration.

If I need to initiate an empty vector for example I use

R>  a <- array()

It just looks like the difference between a 1-d array (which 
doesn't exist as far as I can tell) and a vector is semantic, 
and I think that R would be more logical if they were treated as 
the same thing.  This doesn't mean changing the way the 
is.array(), is.vector(), <-, etc functions work, but just 
changing dim().

Yours,

Olivia Lau

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Fri Jan 21 18:43:08 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Jan 2005 18:43:08 +0100
Subject: [R] transfer function estimation
In-Reply-To: <0BA7EE4D4646E0409D458D347C508B783C61F6@MAILSERV1.uni.glam.ac.uk>
References: <0BA7EE4D4646E0409D458D347C508B783C61F6@MAILSERV1.uni.glam.ac.uk>
Message-ID: <x2r7kehe4z.fsf@biostat.ku.dk>

"Kemp S E (Comp)" <sekemp at glam.ac.uk> writes:

> I have got as far as being able to compute the residual noise, a_t.
> However, I am slightly confused about what to do next. Reading
> Box-Jenkins, 1976 (pp. 391) they state the following
> 
> "....However, it seems simplest to work with a standard nonlinear
> least squares computer program in which the derivatives are
> determined numerically and an option is available of 'constrained
> iteration' to prevent instability. It is then only necessary to
> program the computation of a_t itself......."
> 
> I know that there is a 'nls' function in R but I really do not have
> a clue about how to use it in this situation. Perhaps Box-Jenkins
> are confusing me with their last sentance with regard to the a_t's -
> is this really possible?

I'm not sure I'd trust any computer recommendation from 1976, no
matter how famous the authors are. However, the hint would lead me to
consider the optim() function.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gunter.berton at gene.com  Fri Jan 21 18:47:01 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 21 Jan 2005 09:47:01 -0800
Subject: [R] Avoiding a Loop?
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E6150667@HERMES.demogr.mpg.de>
Message-ID: <200501211747.j0LHl1Tc026998@hertz.gene.com>

Roland:

Andy Liaw and others have already given perfectly good answers to this (but
note: Using apply() type functions does **not** avoid loops; apply's **are**
loops). However, mostly as an illustration to reinforce Uwe Ligges's
comments (in the "dim vs length" thread) about the usefulness of sometimes
treating arrays as vectors, I offer the following:

nc<-ncol(yourmatrix)
matrix(yourmatrix[,1]*rep(k^seq(0,length=nc),e=nrow(yourmatrix)),ncol=nc)

Alteratively, one could use matrix multiplication:

yourmatrix[,1]%*%matrix(k^seq(0,length=nc),nrow=1)

Both of these will be very fast (although unlikely to make a noticeable
difference without a lot of rows or columns) and I think use less memory
than outer() (again, unlikely to make a noticeable difference).


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rau, Roland
> Sent: Friday, January 21, 2005 4:32 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Avoiding a Loop?
> 
> Dear R-Helpers,
> 
> I have a matrix where the first column is known. The second column is
> the result of multiplying this first column with a constant 
> "const". The
> third column is the result of multiplying the second column with
> "const".....
> So far, I did it like this (as a simplified example):
> 
> nr.of.columns <- 4
> 
> myconstant <- 27.5
> 
> mymatrix <- matrix(numeric(0), nrow=5, ncol=nr.of.columns)
> 
> mymatrix[,1] <- 1:5
> 
> for (i in 2:nr.of.columns) {
> 	mymatrix[,i] <- myconstant * mymatrix[,i-1]
> }
> 
> 
> Can anyone give me some advice whether it is possible to 
> avoid this loop
> (and if yes: how)?
> 
> Any suggestions are welcome!
> 
> Thanks,
> Roland
> 
> 
> 
> 
> +++++
> This mail has been sent through the MPI for Demographic 
> Rese...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Fri Jan 21 19:08:53 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 21 Jan 2005 19:08:53 +0100
Subject: [R] Windows plots & fontsize
In-Reply-To: <41F13282.1060005@vedur.is>
References: <41F13282.1060005@vedur.is>
Message-ID: <41F14535.2040703@statistik.uni-dortmund.de>

Halldor Bj?rnsson wrote:

> I recently wrote a package that runs on both Linux and Windows.
> The functions fetch data to an SQL database and make diverse plots, 
> usually with many labels and annotations.
> 
> I recently noticed that on SOME windows computers the fonts
> in the plots were too large, which caused labels to overlap.
> 
> On other windows computers this does not happen.
> However on both computers the command par("ps") gives 12
> 
> Now, windows is not my forte, but it seems likely that this has to do 
> with some user/system settings. If so, my users would probably object to 
> me tweaking their windows setup since it might have an effect on other 
> programs.
> 
> One simple solution is to use par(ps=10), but I cannot find out how to
> set this globally.
> 

All this depends on the device. Which device are we talking about?


> a) Does anyone know how I can set par(ps=10) by default on all figure?

For the postscript device see ?ps.options.


> b) Is there a windows specific way to set default fontsizes in figures?

No. But you might want to read the second paragraph in ?windows if we 
are talking about this device.

Uwe Ligges


> Thanks,
> Halld?r
>



From andy_liaw at merck.com  Fri Jan 21 19:10:36 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 21 Jan 2005 13:10:36 -0500
Subject: [R] how to use do.call("rbind", get(list(mlist)))
Message-ID: <3A822319EB35174CA3714066D590DCD50994E57B@usrymx25.merck.com>

Here's one way (should work for data frames, too):

> mlist <- paste("m", 1:5, sep="")
> for (i in 1:5) assign(mlist[i], matrix(runif(4), 2, 2))
> do.call("rbind", mget(mlist, .GlobalEnv))
            [,1]       [,2]
 [1,] 0.33846095 0.09494054
 [2,] 0.65229451 0.26009002
 [3,] 0.53969914 0.66481544
 [4,] 0.59316498 0.36125843
 [5,] 0.24005411 0.01010594
 [6,] 0.06878842 0.93929806
 [7,] 0.02618064 0.06243190
 [8,] 0.08144251 0.04331587
 [9,] 0.95216684 0.61066435
[10,] 0.03712678 0.95809995

get() takes a character vector as the first argument, and you passed it a
list with one component (a character vector).  Even if you pass mlist to
get(), it will only get the first one.

Andy


> From: roger bos
> 
> I have around 200 data frames I want to rbind in a vectorized way. 
> The object names are:
> 
> m302
> m303
> ...
> m500
> 
> So I tried:
> 
> mlist <- paste("m",302:500,sep="")
> dat <- do.call("rbind", get(list(mlist)))
> 
> and I get "Error in get(x, envir, mode, inherits) : invalid 
> first argument"
> 
> I know "rbind" is valid because 
> 
> dat <- rbind(m302, m303, m304, m305)
> 
> works, I am just too lazy to type it out to m500.
> 
> I also tried it without the get() portion, but then dat ends up being
> a column with just the names of the objects, not the objects
> themselves.
> 
> Thanks in advance for showing me the errors in my attempts.
> 
> Roger
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From gunter.berton at gene.com  Fri Jan 21 19:11:14 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 21 Jan 2005 10:11:14 -0800
Subject: [R] how to use do.call("rbind", get(list(mlist)))
In-Reply-To: <1db7268005012109071f3454df@mail.gmail.com>
Message-ID: <200501211811.j0LIBFHI022941@ohm.gene.com>

Roger:

Please re-read ?get, where it says that the x argument must be a single
character string name, not a list.You must first create the list using get
and then call do.call on that list:

do.call("rbind",lapply(mlist,get))

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of roger bos
> Sent: Friday, January 21, 2005 9:07 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] how to use do.call("rbind", get(list(mlist)))
> 
> I have around 200 data frames I want to rbind in a vectorized way. 
> The object names are:
> 
> m302
> m303
> ...
> m500
> 
> So I tried:
> 
> mlist <- paste("m",302:500,sep="")
> dat <- do.call("rbind", get(list(mlist)))
> 
> and I get "Error in get(x, envir, mode, inherits) : invalid 
> first argument"
> 
> I know "rbind" is valid because 
> 
> dat <- rbind(m302, m303, m304, m305)
> 
> works, I am just too lazy to type it out to m500.
> 
> I also tried it without the get() portion, but then dat ends up being
> a column with just the names of the objects, not the objects
> themselves.
> 
> Thanks in advance for showing me the errors in my attempts.
> 
> Roger
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ggrothendieck at myway.com  Fri Jan 21 19:12:43 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 21 Jan 2005 13:12:43 -0500 (EST)
Subject: [R] dim vs length for vectors
Message-ID: <20050121181243.469FE3A00@mprdmxin.myway.com>



Sorry, I meant as.array, not array.


Date:   Fri, 21 Jan 2005 12:21:39 -0500 (EST) 
From:   Gabor Grothendieck <ggrothendieck at myway.com>
To:   <olau at fas.harvard.edu>, <ligges at statistik.uni-dortmund.de>, <ahenningsen at email.uni-kiel.de> 
Cc:   <r-help at stat.math.ethz.ch> 
Subject:   Re: [R] dim vs length for vectors 

 


Perhaps you could just write dim(array(x)) when you want
that effect. That has the advantage of not requiring any
change to R and preserving the meaning of dim(x) as retrieving
the dim attribute.


Date: Fri, 21 Jan 2005 10:00:01 -0500 
From: Olivia Lau <olau at fas.harvard.edu>
To: Uwe Ligges <ligges at statistik.uni-dortmund.de>,Arne Henningsen <ahenningsen at email.uni-kiel.de> 
Cc: <r-help at stat.math.ethz.ch> 
Subject: Re: [R] dim vs length for vectors 


What I was proposing doesn't require a lot of programming. Just 
whenever you call dim(), it does length() if the object is a 
vector and returns it in the format:

R> a <- 1:12
R> dim(a)
[1] 12

This means that you can provide a unified introduction to data 
structures that take only one type of atomic value (and 
generally call these structures "arrays"). What I call a "one 
dimensional array" only has one dim attribute and hence would 
use a[4] for extraction (for example), which is consistent with 
the usage for "[" (to my understanding). That way, when you 
introduce "[" and arrays at the same time, you can tell 
beginners:
1) Run dim()
2) If there's one dim, use [ ]; if there are 2 dims, use [ 
, ]; if there are 3 dims, you use [ , , ].
This is conceptually easy for a beginner and avoids a bit of 
frustration.

If I need to initiate an empty vector for example I use

R> a <- array()

It just looks like the difference between a 1-d array (which 
doesn't exist as far as I can tell) and a vector is semantic, 
and I think that R would be more logical if they were treated as 
the same thing. This doesn't mean changing the way the 
is.array(), is.vector(), <-, etc functions work, but just 
changing dim().



From ligges at statistik.uni-dortmund.de  Fri Jan 21 19:13:25 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 21 Jan 2005 19:13:25 +0100
Subject: [R] how to use do.call("rbind", get(list(mlist)))
In-Reply-To: <1db7268005012109071f3454df@mail.gmail.com>
References: <1db7268005012109071f3454df@mail.gmail.com>
Message-ID: <41F14645.3000803@statistik.uni-dortmund.de>

roger bos wrote:

> I have around 200 data frames I want to rbind in a vectorized way. 
> The object names are:
> 
> m302
> m303
> ...
> m500
> 
> So I tried:
> 
> mlist <- paste("m",302:500,sep="")
> dat <- do.call("rbind", get(list(mlist)))


   do.call("rbind", lapply(mlist, get))


Uwe Ligges


> and I get "Error in get(x, envir, mode, inherits) : invalid first argument"
> 
> I know "rbind" is valid because 
> 
> dat <- rbind(m302, m303, m304, m305)
> 
> works, I am just too lazy to type it out to m500.
> 
> I also tried it without the get() portion, but then dat ends up being
> a column with just the names of the objects, not the objects
> themselves.
> 
> Thanks in advance for showing me the errors in my attempts.
> 
> Roger
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Jan 21 19:14:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 21 Jan 2005 19:14:44 +0100
Subject: [R] output data in a fixed format
In-Reply-To: <6F3CF8F7047E374CAF4DCED3ED14576E09621042@iowacexch4.ic.ncs.com>
References: <6F3CF8F7047E374CAF4DCED3ED14576E09621042@iowacexch4.ic.ncs.com>
Message-ID: <41F14694.9090906@statistik.uni-dortmund.de>

Shin, David wrote:

> Dear List,
> 
>  
> 
> I would like to write my output data to a file with the fixed format like
> what you can do in SAS.


See ?write.matrix in package "MASS".

Uwe Ligges




> For example: 
> 
>  
> 
> put 
> 
>  
> 
> @1    ( variable1 )   (10.5)
> 
> @11  ( variable 2)   ($5.)
> 
> :
> 
> :
> 
> etc.
> 
>  
> 
> Is there a way in "R" that would allow me to do the same thing?
> 
> Thank you for your answer in advance.
> 
>  
> 
> David
> 
>  
> 
>  
> 
> 
> **************************************************************************** 
> This email may contain confidential material. 
> If you were not an intended recipient, 
> Please notify the sender and delete all copies. 
> We may monitor email to and from our network. 
> ****************************************************************************
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Robert.McGehee at geodecapital.com  Fri Jan 21 19:13:40 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Fri, 21 Jan 2005 13:13:40 -0500
Subject: [R] how to use do.call("rbind", get(list(mlist)))
Message-ID: <67DCA285A2D7754280D3B8E88EB5480206741E2D@MSGBOSCLB2WIN.DMN1.FMR.COM>

get() doesn't take a list. You need to wrap it in an lapply so that it
grabs the dataframes individually and then wraps it into a list for
do.call().

The easy way, however, that doesn't involve do.call or get is to simply
construct the proper text rbind query and then run it with eval.

eval(parse(text = paste("rbind(", paste("m", 302:500, sep = "", collapse
= ", "), ")")))

To use the do.call() / get() construct, I think this is what you were
thinking of:

mlist <- paste("m",302:500,sep="")
do.call("rbind", lapply(mlist, get))

Best,
Robert

-----Original Message-----
From: roger bos [mailto:roger.bos at gmail.com] 
Sent: Friday, January 21, 2005 12:07 PM
To: r-help at stat.math.ethz.ch
Subject: [R] how to use do.call("rbind", get(list(mlist)))


I have around 200 data frames I want to rbind in a vectorized way. 
The object names are:

m302
m303
...
m500

So I tried:

mlist <- paste("m",302:500,sep="")
dat <- do.call("rbind", get(list(mlist)))

and I get "Error in get(x, envir, mode, inherits) : invalid first
argument"

I know "rbind" is valid because 

dat <- rbind(m302, m303, m304, m305)

works, I am just too lazy to type it out to m500.

I also tried it without the get() portion, but then dat ends up being
a column with just the names of the objects, not the objects
themselves.

Thanks in advance for showing me the errors in my attempts.

Roger

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From matthew_wiener at merck.com  Fri Jan 21 19:17:20 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Fri, 21 Jan 2005 13:17:20 -0500
Subject: [R] how to use do.call("rbind", get(list(mlist)))
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E04993E0F@uswsmx03.merck.com>

Roger --

First, if you have the data frames in a list already, you can just use
those; no need to bother with the names.  If the data frames are all
separate, but you have the names, you can first create a list of the data
frames themselves:

do.call("rbind", lapply(mlist, get))  # assumes they're in the workspace

An example:
> temp1 <- matrix(runif(10), 5, 2)
> temp2 <- matrix(-runif(10), 5, 2)   (for easy identifiability)
> temp3 <- do.call("rbind", lapply(c("temp1", "temp2"), get))
> temp3 <- do.call("rbind", list(temp1, temp2))     (same as above)

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of roger bos
Sent: Friday, January 21, 2005 12:07 PM
To: r-help at stat.math.ethz.ch
Subject: [R] how to use do.call("rbind", get(list(mlist)))


I have around 200 data frames I want to rbind in a vectorized way. 
The object names are:

m302
m303
...
m500

So I tried:

mlist <- paste("m",302:500,sep="")
dat <- do.call("rbind", get(list(mlist)))

and I get "Error in get(x, envir, mode, inherits) : invalid first argument"

I know "rbind" is valid because 

dat <- rbind(m302, m303, m304, m305)

works, I am just too lazy to type it out to m500.

I also tried it without the get() portion, but then dat ends up being
a column with just the names of the objects, not the objects
themselves.

Thanks in advance for showing me the errors in my attempts.

Roger

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From b-li1 at northwestern.edu  Fri Jan 21 19:18:49 2005
From: b-li1 at northwestern.edu (Bobai Li)
Date: Fri, 21 Jan 2005 12:18:49 -0600
Subject: [R] some questions about font
Message-ID: <41F14789.7020808@northwestern.edu>


Hi,

I have been using R to create some mathematical and statistical graphs 
for a book manuscript, but I got some problems:

1)  Some web positngs said that default typeface for math expressions is 
italic, but in my system (R 2.01 on WinXP), the default is regular font.
How can I change the default to ilatic?

2)  When use ComputerModern font,  (i.e., 
family=c("CM_regular_10.afm","CM_boldx_10.afm","cmti10.afm","cmbxti10.afm","CM_symbol_10.afm") 
), some accented symbols are not available. For example, 
"expression(hat(beta))" will produce warning message like "font metrics 
unknown for character 94."

Thanks.

Bobai Li

----------------------------------
Assistant Professor
Department of Sociology
Northwestern University
1810 Chicao Ave.,
Evanston, IL 60201



From tlumley at u.washington.edu  Fri Jan 21 19:24:49 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 21 Jan 2005 10:24:49 -0800 (PST)
Subject: [R] dim vs length for vectors
In-Reply-To: <002f01c4ffc9$e1d8aec0$6501a8c0@olau>
References: <000301c4ff76$e7951ef0$7700a8c0@olau>
	<loom.20050121T063201-271@post.gmane.org>
	<200501211201.59297.ahenningsen@email.uni-kiel.de>
	<41F0EDBC.4080709@statistik.uni-dortmund.de>
	<002f01c4ffc9$e1d8aec0$6501a8c0@olau>
Message-ID: <Pine.A41.4.61b.0501211022360.293758@homer05.u.washington.edu>

On Fri, 21 Jan 2005, Olivia Lau wrote:
> What I was proposing doesn't require a lot of programming.  Just whenever you 
> call dim(), it does length() if the object is a vector and returns it in the 
> format:
>
> R> a <- 1:12
> R> dim(a)
> [1]  12
>

One problem is that there is code using is.null(dim(x)) to distinguish 
vectors from matrices.  This code would break.

 	-thomas



From ripley at stats.ox.ac.uk  Fri Jan 21 19:29:57 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 21 Jan 2005 18:29:57 +0000 (GMT)
Subject: [R] Windows plots & fontsize 
In-Reply-To: <41F13282.1060005@vedur.is>
References: <41F13282.1060005@vedur.is>
Message-ID: <Pine.LNX.4.61.0501211820580.14100@gannet.stats>

On Fri, 21 Jan 2005, [ISO-8859-1] Halldor Bj?rnsson wrote:

> I recently wrote a package that runs on both Linux and Windows.
> The functions fetch data to an SQL database and make diverse plots, usually 
> with many labels and annotations.
>
> I recently noticed that on SOME windows computers the fonts
> in the plots were too large, which caused labels to overlap.

What graphics device are you talking about for your plots?

> On other windows computers this does not happen.
> However on both computers the command par("ps") gives 12

As one would expect.  But are the windows the same size?

> Now, windows is not my forte, but it seems likely that this has to do with 
> some user/system settings. If so, my users would probably object to me 
> tweaking their windows setup since it might have an effect on other programs.

Windows will be asked for the same fonts, but the window size may differ: 
see the help page.  And users can apply magnification factors.

> One simple solution is to use par(ps=10), but I cannot find out how to
> set this globally.
>
> a) Does anyone know how I can set par(ps=10) by default on all figure?

I think you want to start the device with pointsize=10.

?windows lists the options you have, if it is the device.

> b) Is there a windows specific way to set default fontsizes in figures?

It is device-specific, usually parameter `pointsize'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From tlumley at u.washington.edu  Fri Jan 21 19:31:53 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 21 Jan 2005 10:31:53 -0800 (PST)
Subject: [R] how to use do.call("rbind", get(list(mlist)))
In-Reply-To: <1db7268005012109071f3454df@mail.gmail.com>
References: <1db7268005012109071f3454df@mail.gmail.com>
Message-ID: <Pine.A41.4.61b.0501211027340.293758@homer05.u.washington.edu>

On Fri, 21 Jan 2005, roger bos wrote:

> I have around 200 data frames I want to rbind in a vectorized way.
> The object names are:
>
> m302
> m303
> ...
> m500
>
> So I tried:
>
> mlist <- paste("m",302:500,sep="")
> dat <- do.call("rbind", get(list(mlist)))
>
> and I get "Error in get(x, envir, mode, inherits) : invalid first argument"

Yep. get() takes a single variable name as its argument.

If you tried   get(list("m302","m303")) you would get the same error.

> I know "rbind" is valid because
>
> dat <- rbind(m302, m303, m304, m305)
>
> works, I am just too lazy to type it out to m500.
>
> I also tried it without the get() portion, but then dat ends up being
> a column with just the names of the objects, not the objects
> themselves.

Indeed.  This translates to rbind("m302","m303"....

You could use mget() instead of get().

 	-thomas



From charlie at stat.umn.edu  Fri Jan 21 20:21:45 2005
From: charlie at stat.umn.edu (Charles Geyer)
Date: Fri, 21 Jan 2005 13:21:45 -0600
Subject: [R] niceness
Message-ID: <20050121192145.GA19554@stat.umn.edu>

Can anyone tell me if the following C code (which proved very useful when
using the snow package -- use it to nice slaves) compiles and dyn.loads
under Windoze or Mac?  It is (apparently) POSIX, so I suppose it is fine
in OS X, but does Windoze's advertized POSIX compliance mean anything here?

If it doesn't work on some platform, how do I deal with that?  I want
to submit as contributed package to CRAN.  The whole package can be found at

    http://www.stat.umn.edu/geyer/nice_0.1.tar.gz

----- begin C code file -----

#include <sys/resource.h>
#include <errno.h>
#include <string.h>
#include <R.h>

void
get_my_priority(int *result)
{
    errno = 0;
    result[0] = getpriority(PRIO_PROCESS, 0);
    if (errno != 0)
        warning(strerror(errno));
}

void
set_my_priority(int *priority)
{
    if (setpriority(PRIO_PROCESS, 0, priority[0]) != 0)
        warning(strerror(errno));
}

------ end C code file ------

-- 
Charles Geyer
Professor, School of Statistics
University of Minnesota
charlie at stat.umn.edu



From jdressel at surromed.com  Fri Jan 21 20:51:50 2005
From: jdressel at surromed.com (Jon Dressel)
Date: Fri, 21 Jan 2005 11:51:50 -0800
Subject: [R] Hardware Suggestions
Message-ID: <9D33C6169B1FDB419767356B8A8FEDB5011475E0@lynx.corp.surromed.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050121/701dbb88/attachment.pl

From stephen.mcintyre at utoronto.ca  Fri Jan 21 22:42:41 2005
From: stephen.mcintyre at utoronto.ca (Steve McIntyre)
Date: Fri, 21 Jan 2005 16:42:41 -0500
Subject: [R] R Citation
Message-ID: <007501c50002$22a747d0$6402a8c0@herbert>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050121/7bb2956e/attachment.pl

From Achim.Zeileis at wu-wien.ac.at  Fri Jan 21 23:06:07 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 21 Jan 2005 23:06:07 +0100
Subject: [R] R Citation
In-Reply-To: <007501c50002$22a747d0$6402a8c0@herbert>
References: <007501c50002$22a747d0$6402a8c0@herbert>
Message-ID: <20050121230607.78dac655.Achim.Zeileis@wu-wien.ac.at>

On Fri, 21 Jan 2005 16:42:41 -0500 Steve McIntyre wrote:

> I've cited R in an article accepted by a journal using the following
> reference:
> 
> >R Development Core Team (2004), R: A language and environment for 
>  >statistical computing. R Foundation for Statistical Computing,
>  >Vienna, Austria. ISBN 3-900051-00-3, <http://www.Rproject.org>.
                                  ^^^^^             ^^^^

The ISBN is 3-900051-07-0 and the URL http://www.R-project.org/.

> I received the following query about my citation. 
>   
>  > 7. For R Development Core Team [2004], you provided both an ISBN
>  > and a
> > Web site address.  If this is a book, please provide the name and
> > location of the publisher.  If you do not have print publication
> > information, the Web site will be cited in text only, and the
> > reference will be removed from the reference list.
> 
> I'd appreciate advice on this. I had a similar query about a similar
> citation of an R package.

Well, both R and the package are software and no books, so print
publication information cannot be available. You could either argue that
you want to cite software and that the information is complete, or (if
the publisher of the journal would not accept that) argue that both are
manuals which you want to cite (for which the publisher should have a
citation style).
One would hope that the latter is not necessary as a publisher should
always allow a possibility to properly cite software used in a
scientific article...

hth,
Z
 
> Thanks, Steve McIntyre
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From tom_hoary at web.de  Fri Jan 21 23:21:30 2005
From: tom_hoary at web.de (Thomas =?ISO-8859-1?Q?Sch=F6nhoff?=)
Date: Fri, 21 Jan 2005 23:21:30 +0100
Subject: [R] an R script editor for Mac
In-Reply-To: <24A23A3C-6BA1-11D9-91D9-000D933565E8@mail.nih.gov>
References: <HHEDKBCGCMDOHEDELFBCGEMACHAA.jacques.veslot@cirad.fr>
	<24A23A3C-6BA1-11D9-91D9-000D933565E8@mail.nih.gov>
Message-ID: <1106346091.5363.16.camel@linux.tlink.de>

Hello,

Am Freitag, den 21.01.2005, 06:39 -0500 schrieb Sean Davis:
> Consider using ESS and xemacs or emacs.  You get syntax highlighting, 
> auto-indent, command auto-complete, transcripts for your session, 
> integrated help, and the tools of one of the most powerful text editors 
> on the planet.  The learning curve is a bit steep, but if you use R 
> much, it is worth it.

Yes, definitely. I'd like to add that there is an exellent ref-card
regarding the Emacs/ESS duo that may smoothing the learning curve a bit!
Don't remember the right place to fetch from but I could give you a copy
of mine if desired. Drop me a line.

Thomas



From jun at cc.gatech.edu  Fri Jan 21 23:26:41 2005
From: jun at cc.gatech.edu (Seung Jun)
Date: Fri, 21 Jan 2005 17:26:41 -0500
Subject: [R] which.pmin?
Message-ID: <41F181A1.3020204@cc.gatech.edu>

I have two vectors (k.floor and k.ceiling) of integers of the same
length, and a function (fpr).

    b <- 10:40
    k.floor <- floor(log(2) * b)
    k.ceiling <- ceiling(log(2) * b)
    fpr.floor <- fpr(b, k.floor)
    fpr.ceiling <- fpr(b, k.ceiling)

If R had a element-wise ternary function, I'd like to do something like 
this:

    (fpr.floor < fpr.ceiling) ? k.floor : k.ceiling

That is, I'd like to go through the two vectors in parallel, picking
the one that returns the lower value of fpr. Failing to find such a
function, I wrote the following two lines:

    ind <- sapply(data.frame(rbind(fpr.floor,fpr.ceiling)), which.min)
    opt.k <- cbind(k.floor,k.ceiling)[1:length(ind)+length(ind)*(ind-1)]

opt.k is the vector I want, but I guess I abuse some functions here.
I'd like to ask the experts, What is the proper R-way to do this?

The API should be like "which.pmin(FUN, X, Y, ...)" that returns a
vector of the same length as X (and Y), provided that X, Y, ... have
the same length. Please fill the function body.

Seung



From tlumley at u.washington.edu  Fri Jan 21 23:36:56 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 21 Jan 2005 14:36:56 -0800 (PST)
Subject: [R] which.pmin?
In-Reply-To: <41F181A1.3020204@cc.gatech.edu>
References: <41F181A1.3020204@cc.gatech.edu>
Message-ID: <Pine.A41.4.61b.0501211436310.88574@homer07.u.washington.edu>

On Fri, 21 Jan 2005, Seung Jun wrote:

> If R had a element-wise ternary function, I'd like to do something like this:
>
>   (fpr.floor < fpr.ceiling) ? k.floor : k.ceiling
>

R does:  ifelse()

 	-thomas



From MSchwartz at MedAnalytics.com  Fri Jan 21 23:38:01 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 21 Jan 2005 16:38:01 -0600
Subject: [R] which.pmin?
In-Reply-To: <41F181A1.3020204@cc.gatech.edu>
References: <41F181A1.3020204@cc.gatech.edu>
Message-ID: <1106347081.27510.7.camel@horizons.localdomain>

On Fri, 2005-01-21 at 17:26 -0500, Seung Jun wrote:
> I have two vectors (k.floor and k.ceiling) of integers of the same
> length, and a function (fpr).
> 
>     b <- 10:40
>     k.floor <- floor(log(2) * b)
>     k.ceiling <- ceiling(log(2) * b)
>     fpr.floor <- fpr(b, k.floor)
>     fpr.ceiling <- fpr(b, k.ceiling)
> 
> If R had a element-wise ternary function, I'd like to do something like 
> this:
> 
>     (fpr.floor < fpr.ceiling) ? k.floor : k.ceiling
> 
> That is, I'd like to go through the two vectors in parallel, picking
> the one that returns the lower value of fpr. Failing to find such a
> function, I wrote the following two lines:
> 
>     ind <- sapply(data.frame(rbind(fpr.floor,fpr.ceiling)), which.min)
>     opt.k <- cbind(k.floor,k.ceiling)[1:length(ind)+length(ind)*(ind-1)]
> 
> opt.k is the vector I want, but I guess I abuse some functions here.
> I'd like to ask the experts, What is the proper R-way to do this?
> 
> The API should be like "which.pmin(FUN, X, Y, ...)" that returns a
> vector of the same length as X (and Y), provided that X, Y, ... have
> the same length. Please fill the function body.

I believe that ifelse() is what you seek. Try:

  ifelse(fpr.floor < fpr.ceiling, k.floor, k.ceiling)

See ?ifelse for more information.

HTH,

Marc Schwartz



From rolf at math.unb.ca  Fri Jan 21 23:45:02 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Fri, 21 Jan 2005 18:45:02 -0400 (AST)
Subject: [R] which.pmin?
Message-ID: <200501212245.j0LMj2Bu020966@erdos.math.unb.ca>


If I understand you correctly you could just do

	ifelse(fpr.floor < fpr.ceiling, k.floor, k.ceiling)

				cheers,

					Rolf Turner
					rolf at math.unb.ca

===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===

Seung Jun wrote:

> I have two vectors (k.floor and k.ceiling) of integers of the same
> length, and a function (fpr).
> 
>     b <- 10:40
>     k.floor <- floor(log(2) * b)
>     k.ceiling <- ceiling(log(2) * b)
>     fpr.floor <- fpr(b, k.floor)
>     fpr.ceiling <- fpr(b, k.ceiling)
> 
> If R had a element-wise ternary function, I'd like to do something like 
> this:
> 
>     (fpr.floor < fpr.ceiling) ? k.floor : k.ceiling
> 
> That is, I'd like to go through the two vectors in parallel, picking
> the one that returns the lower value of fpr. Failing to find such a
> function, I wrote the following two lines:
> 
>     ind <- sapply(data.frame(rbind(fpr.floor,fpr.ceiling)), which.min)
>     opt.k <- cbind(k.floor,k.ceiling)[1:length(ind)+length(ind)*(ind-1)]
> 
> opt.k is the vector I want, but I guess I abuse some functions here.
> I'd like to ask the experts, What is the proper R-way to do this?
> 
> The API should be like "which.pmin(FUN, X, Y, ...)" that returns a
> vector of the same length as X (and Y), provided that X, Y, ... have
> the same length. Please fill the function body.



From stephen.mcintyre at utoronto.ca  Fri Jan 21 23:48:39 2005
From: stephen.mcintyre at utoronto.ca (Steve McIntyre)
Date: Fri, 21 Jan 2005 17:48:39 -0500
Subject: Fw: [R] R Citation
Message-ID: <008c01c5000b$5a323fd0$6402a8c0@herbert>

Dear Achim, Thanks for the comment. Here is the publisher's style guideline
(AGU)
"Because the Internet is dynamic environment and sites may change or move,
treat World Wide Web, ftp files, and electronically archived data stored at
data centers other than World or National Data Centers as unpublished, i.e.,
in text only." http://www.agu.org/pubs/AuthorRefSheet.pdf

So by this policy, R and its packages cannot be included in the list of
references and has the same citation as a pers.comm. The problem for my
paper can be resolved by removing the citation from references to text, but
this seems unfair to R and the package authors for AGU publications. AGU
does recognize some permanent data archives listed here
http://www.agu.org/pubs/datacent.html - maybe it would make sense to have a
mirror at one of these permanent archives for packages and versions.

Regards, Steve McIntyre

----- Original Message ----- 

From: "Achim Zeileis" <Achim.Zeileis at wu-wien.ac.at>
To: "Steve McIntyre" <stephen.mcintyre at utoronto.ca>
Cc: <R-help at r-project.org>
Sent: Friday, January 21, 2005 5:06 PM
Subject: Re: [R] R Citation


> On Fri, 21 Jan 2005 16:42:41 -0500 Steve McIntyre wrote:
>
> > I've cited R in an article accepted by a journal using the following
> > reference:
> >
> > >R Development Core Team (2004), R: A language and environment for
> >  >statistical computing. R Foundation for Statistical Computing,
> >  >Vienna, Austria. ISBN 3-900051-00-3, <http://www.Rproject.org>.
>                                   ^^^^^             ^^^^
>
> The ISBN is 3-900051-07-0 and the URL http://www.R-project.org/.
>
> > I received the following query about my citation.
> >
> >  > 7. For R Development Core Team [2004], you provided both an ISBN
> >  > and a
> > > Web site address.  If this is a book, please provide the name and
> > > location of the publisher.  If you do not have print publication
> > > information, the Web site will be cited in text only, and the
> > > reference will be removed from the reference list.
> >
> > I'd appreciate advice on this. I had a similar query about a similar
> > citation of an R package.
>
> Well, both R and the package are software and no books, so print
> publication information cannot be available. You could either argue that
> you want to cite software and that the information is complete, or (if
> the publisher of the journal would not accept that) argue that both are
> manuals which you want to cite (for which the publisher should have a
> citation style).
> One would hope that the latter is not necessary as a publisher should
> always allow a possibility to properly cite software used in a
> scientific article...
>
> hth,
> Z
>
> > Thanks, Steve McIntyre
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >



From p.dalgaard at biostat.ku.dk  Fri Jan 21 23:55:37 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Jan 2005 23:55:37 +0100
Subject: [R] Hardware Suggestions
In-Reply-To: <9D33C6169B1FDB419767356B8A8FEDB5011475E0@lynx.corp.surromed.com>
References: <9D33C6169B1FDB419767356B8A8FEDB5011475E0@lynx.corp.surromed.com>
Message-ID: <x2is5qz91y.fsf@biostat.ku.dk>

"Jon Dressel" <jdressel at surromed.com> writes:

> We are currently running R under Windows 2000 on a server box
> running with 2 1.2 GHZ Intel Pentium III Processors. We would like
> to run this on a new computer running Linux and receive a
> significant speed increase over our current implementation. Could
> anyone provide some suggestions for a fast 64 BIT Intel based
> processor computer with a recommendation for memory and
> speed/type/number of processors. Also which version of R would
> install "out-of-the-box" easily on this computer and what version of
> Linux should be used? Thanks in advance for any help.

(I assume "Intel" also means AMD?)

People seem quite happy with dual and quad Opterons (and there are
dual-core chips coming up soon, I hear), but you do need to do your
homework, since there have been trouble with some chipsets/BIOSes in
large-memory configurations, and there are not all that many people
using the high-end stuff. Check out the archives of the x86_64 mailing
lists for the popular Linux distributions.

Distribution-wise Fedora Core and SuSE both work nicely and R has been
tested on both with no issues that I can think of. There's an RPM up
for FC3, but it's not a big hassle to build from source and you need
most of the build tools in place to install CRAN packages anyway.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Achim.Zeileis at wu-wien.ac.at  Sat Jan 22 00:00:42 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Sat, 22 Jan 2005 00:00:42 +0100
Subject: Fw: [R] R Citation
In-Reply-To: <008c01c5000b$5a323fd0$6402a8c0@herbert>
References: <008c01c5000b$5a323fd0$6402a8c0@herbert>
Message-ID: <20050122000042.04bf8b92.Achim.Zeileis@wu-wien.ac.at>

On Fri, 21 Jan 2005 17:48:39 -0500 Steve McIntyre wrote:

> Dear Achim, Thanks for the comment. Here is the publisher's style
> guideline(AGU)
> "Because the Internet is dynamic environment and sites may change or
> move, treat World Wide Web, ftp files, and electronically archived
> data stored at data centers other than World or National Data Centers
> as unpublished, i.e., in text only."
> http://www.agu.org/pubs/AuthorRefSheet.pdf

My understanding of this is that you cannot refer to files (especially
documents and data files) in the references. A software package is
neither. So maybe you should just remove the URL?
For proprietary software packages, you don't have a URL from which you
can download it, but nevertheless you should cite the software you used
in an article.

In short: The text above does not provide a guideline on how to cite
software (afaics), and thus it does not apply to citing R or R packages.

my EUR 0.02
Z

> So by this policy, R and its packages cannot be included in the list
> of references and has the same citation as a pers.comm. The problem
> for my paper can be resolved by removing the citation from references
> to text, but this seems unfair to R and the package authors for AGU
> publications. AGU does recognize some permanent data archives listed
> here http://www.agu.org/pubs/datacent.html - maybe it would make sense
> to have a mirror at one of these permanent archives for packages and
> versions.
> 
> Regards, Steve McIntyre
> 
> ----- Original Message ----- 
> 
> From: "Achim Zeileis" <Achim.Zeileis at wu-wien.ac.at>
> To: "Steve McIntyre" <stephen.mcintyre at utoronto.ca>
> Cc: <R-help at r-project.org>
> Sent: Friday, January 21, 2005 5:06 PM
> Subject: Re: [R] R Citation
> 
> 
> > On Fri, 21 Jan 2005 16:42:41 -0500 Steve McIntyre wrote:
> >
> > > I've cited R in an article accepted by a journal using the
> > > following reference:
> > >
> > > >R Development Core Team (2004), R: A language and environment for
> > >  >statistical computing. R Foundation for Statistical Computing,
> > >  >Vienna, Austria. ISBN 3-900051-00-3, <http://www.Rproject.org>.
> >                                   ^^^^^             ^^^^
> >
> > The ISBN is 3-900051-07-0 and the URL http://www.R-project.org/.
> >
> > > I received the following query about my citation.
> > >
> > >  > 7. For R Development Core Team [2004], you provided both an
> > >  > ISBN and a
> > > > Web site address.  If this is a book, please provide the name
> > > > and location of the publisher.  If you do not have print
> > > > publication information, the Web site will be cited in text
> > > > only, and the reference will be removed from the reference list.
> > >
> > > I'd appreciate advice on this. I had a similar query about a
> > > similar citation of an R package.
> >
> > Well, both R and the package are software and no books, so print
> > publication information cannot be available. You could either argue
> > that you want to cite software and that the information is complete,
> > or (if the publisher of the journal would not accept that) argue
> > that both are manuals which you want to cite (for which the
> > publisher should have a citation style).
> > One would hope that the latter is not necessary as a publisher
> > should always allow a possibility to properly cite software used in
> > a scientific article...
> >
> > hth,
> > Z
> >
> > > Thanks, Steve McIntyre
> > > [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> 
>



From murdoch at stats.uwo.ca  Sat Jan 22 01:02:22 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 22 Jan 2005 00:02:22 +0000
Subject: [R] niceness
In-Reply-To: <20050121192145.GA19554@stat.umn.edu>
References: <20050121192145.GA19554@stat.umn.edu>
Message-ID: <k443v0h1v2l6sacml8ph18otoaphtubo83@4ax.com>

On Fri, 21 Jan 2005 13:21:45 -0600, Charles Geyer
<charlie at stat.umn.edu> wrote :

>Can anyone tell me if the following C code (which proved very useful when
>using the snow package -- use it to nice slaves) compiles and dyn.loads
>under Windoze or Mac?  It is (apparently) POSIX, so I suppose it is fine
>in OS X, but does Windoze's advertized POSIX compliance mean anything here?

It doesn't compile with the standard MinGW installation, because the
sys/resource.h header file isn't found.

Duncan Murdoch



From Mike.Prager at noaa.gov  Sat Jan 22 02:23:16 2005
From: Mike.Prager at noaa.gov (Michael Prager)
Date: Fri, 21 Jan 2005 20:23:16 -0500
Subject: Fw: [R] R Citation
In-Reply-To: <008c01c5000b$5a323fd0$6402a8c0@herbert>
References: <008c01c5000b$5a323fd0$6402a8c0@herbert>
Message-ID: <41F1AB04.8090900@noaa.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050121/e2124549/attachment.pl

From lbarrera at bioinf.ucsd.edu  Sat Jan 22 03:06:45 2005
From: lbarrera at bioinf.ucsd.edu (Leah Barrera)
Date: Fri, 21 Jan 2005 18:06:45 -0800 (PST)
Subject: [R] Plotting with Statistics::R, Perl/R
Message-ID: <Pine.GSO.4.58.0501211757210.3973@bioinf.ucsd.edu>

I am trying to plot in R from a perl script using the Statistics::R
package as my bridge.  The following are the conditions:

0. I am running from a Linux server.

1. Even without xwindows the following saves test.png correctly from an
interactive session with R:
> xy<-cbind(1,1)
> png("test.png")
> plot(xy)
> dev.off()

2. However, when called from the perl script I get the ff. warning:
--
X11 module is not available under this GUI
--
and nothing is saved

3. I tried to hijack the script by modifying
lib/Statistics/R/Bridge/Linux.pm to remove the
"gui=none" setting
--
$this->{START_CMD} = "$this->{R_BIN} --slave --vanilla --gui=none" ;
--
changed to
--
$this->{START_CMD} = "$this->{R_BIN} --slave --vanilla" ;
--

4.  When plotting in R from the perl script again, I no longer get the
warning and  I get test.png saved, but it is empty.

Please advise.



From edd at debian.org  Sat Jan 22 03:19:45 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 21 Jan 2005 20:19:45 -0600
Subject: [R] Plotting with Statistics::R, Perl/R
In-Reply-To: <Pine.GSO.4.58.0501211757210.3973@bioinf.ucsd.edu>
References: <Pine.GSO.4.58.0501211757210.3973@bioinf.ucsd.edu>
Message-ID: <20050122021945.GA633@sonny.eddelbuettel.com>

On Fri, Jan 21, 2005 at 06:06:45PM -0800, Leah Barrera wrote:
> I am trying to plot in R from a perl script using the Statistics::R
> package as my bridge.  The following are the conditions:
> 
> 0. I am running from a Linux server.
> 
> 1. Even without xwindows the following saves test.png correctly from an
> interactive session with R:
> > xy<-cbind(1,1)
> > png("test.png")
> > plot(xy)
> > dev.off()
> 
> 2. However, when called from the perl script I get the ff. warning:
> --
> X11 module is not available under this GUI
> --
> and nothing is saved
> 
> 3. I tried to hijack the script by modifying
> lib/Statistics/R/Bridge/Linux.pm to remove the
> "gui=none" setting
> --
> $this->{START_CMD} = "$this->{R_BIN} --slave --vanilla --gui=none" ;
> --
> changed to
> --
> $this->{START_CMD} = "$this->{R_BIN} --slave --vanilla" ;
> --
> 
> 4.  When plotting in R from the perl script again, I no longer get the
> warning and  I get test.png saved, but it is empty.
> 
> Please advise.

It's a FAQ.  

Plotting certain formats requires the X11 server to be present as the font
metrics for those formats can be supplied only the X11 server. Other drivers
don;t the font metrics from X11 -- I think pdf is a good counterexample.
When you run in 'batch' via a Perl script, you don't have the X11 server --
even though it may be on the machine and running, it is not associated with
the particular session running your Perl job.  There are two common fixes:

a) if you must have png() as a format, you can start a virtual X11 server
   with the xvfb server -- this is a bit involved, but doable;
   
b) if you can do other formats instead of png(), this can work -- I have
   forgotten the details but the story has been discussed many times over
   here as web servers use the same "session-less" setup.  In essence,
   using ghostscript for the bitmap conversion works so it may be that
   the bitmap() device works.  If I recall pdf(), works.  
   
c) With that, you could try creating a pdf() first, and then use Perl to
   call ghostscript to convert the pdf for you.  R can do that too for
   you, but only for certain devices.  
   
Hth,  Dirk   

-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From tchur at optushome.com.au  Sat Jan 22 04:52:40 2005
From: tchur at optushome.com.au (Tim Churches)
Date: Sat, 22 Jan 2005 14:52:40 +1100
Subject: [R] Plotting with Statistics::R, Perl/R
In-Reply-To: <20050122021945.GA633@sonny.eddelbuettel.com>
References: <Pine.GSO.4.58.0501211757210.3973@bioinf.ucsd.edu>
	<20050122021945.GA633@sonny.eddelbuettel.com>
Message-ID: <41F1CE08.9040906@optushome.com.au>

Dirk Eddelbuettel wrote:

>Plotting certain formats requires the X11 server to be present as the font
>metrics for those formats can be supplied only the X11 server. Other drivers
>don;t the font metrics from X11 -- I think pdf is a good counterexample.
>When you run in 'batch' via a Perl script, you don't have the X11 server --
>even though it may be on the machine and running, it is not associated with
>the particular session running your Perl job.  There are two common fixes:
>
>a) if you must have png() as a format, you can start a virtual X11 server
>   with the xvfb server -- this is a bit involved, but doable;
>  
>
An example of a Python programme which manages the starting of an Xvfb 
server when one is required can be found in the xvfb_spawn.py file 
/SOOMv0 directory of the tarball for NetEpi Analysis, which can be 
downloaded by following the links at http://www.netepi.org

xvfb_spawn.py was written for use with RPy, which is a Python-to-R 
bridge, when used in a Web server setting (hence no X11 display server 
available). It should be possible to translate the programme to Perl, or 
to write somethig similar in Perl. Comments in the code note some 
potential security traps for the unwary.

Hopefully one day the dependency of the R raster graphics devices on an 
X11 server will be removed. R on Win32 doesn't have that dependency (but 
then, Windows machines, even servers, have displays running all the time 
as part of their kernel, and who would wish that on other operating 
system?). However, there are several graphics back-ends which produce 
very high quality raster graphics on POSIX platforms without the need 
for an X11 device to be present - Agg ("Anti-grain geometry", see 
http://www.antigrain.com/) and Cairo (see http://cairographics.org/) 
spring to mind (usually disclaimers about the foregoing comments not 
meaning to seem like ingratitude to the R development team etc apply).

Tim C



From stephen.mcintyre at utoronto.ca  Sat Jan 22 05:01:53 2005
From: stephen.mcintyre at utoronto.ca (Steve McIntyre)
Date: Fri, 21 Jan 2005 23:01:53 -0500
Subject: Fw: [R] R Citation
References: <008c01c5000b$5a323fd0$6402a8c0@herbert>
	<20050122000042.04bf8b92.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <009b01c50037$1cbd81b0$6402a8c0@herbert>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050121/b115ccc1/attachment.pl

From jwd at surewest.net  Sat Jan 22 05:37:27 2005
From: jwd at surewest.net (John Dougherty)
Date: Fri, 21 Jan 2005 20:37:27 -0800
Subject: Fw: [R] R Citation
In-Reply-To: <008c01c5000b$5a323fd0$6402a8c0@herbert>
References: <008c01c5000b$5a323fd0$6402a8c0@herbert>
Message-ID: <200501212037.27221.jwd@surewest.net>

On Friday 21 January 2005 14:48, Steve McIntyre wrote:
> Dear Achim, Thanks for the comment. Here is the publisher's style guideline
> (AGU)
> "Because the Internet is dynamic environment and sites may change or move,
> treat World Wide Web, ftp files, and electronically archived data stored at
> data centers other than World or National Data Centers as unpublished,
> i.e., in text only." http://www.agu.org/pubs/AuthorRefSheet.pdf
>
> So by this policy, R and its packages cannot be included in the list of
> references and has the same citation as a pers.comm. The problem for my
> paper can be resolved by removing the citation from references to text, but
> this seems unfair to R and the package authors for AGU publications. AGU
> does recognize some permanent data archives listed here
> http://www.agu.org/pubs/datacent.html - maybe it would make sense to have a
> mirror at one of these permanent archives for packages and versions.
>
> Regards, Steve McIntyre
>

Steve,

Depending upon the actual format of your paper, you might either add a  
reference to R in your "Methods" or in an acknowledgements section if the 
format permits such.  Looking at the AGU publishing guide, their obvious 
concern is to insure that the data used in an article are sourced properly.

Statistical software, like ground-penetrating radar or a magnetometer for 
instance, is a tool.  Thus it really falls within the scope of methods.  It 
is reasonable in that case to cite the URL in the text.  You can also 
elaborate somewhat and explain sonething of the history and origins of R,  
citing the The R Reference Manual by the R Development Core Team, which is 
available through Amazon Books (volume 1 for mere $57.56 plus shipping at 
present).  You ought to be able with a bit of creativity to drag in mention 
of both the URL and the manuals.

Good luck,

John



From mail at joeconway.com  Sat Jan 22 05:55:22 2005
From: mail at joeconway.com (Joe Conway)
Date: Fri, 21 Jan 2005 20:55:22 -0800
Subject: [R] Plotting with Statistics::R, Perl/R
In-Reply-To: <20050122021945.GA633@sonny.eddelbuettel.com>
References: <Pine.GSO.4.58.0501211757210.3973@bioinf.ucsd.edu>
	<20050122021945.GA633@sonny.eddelbuettel.com>
Message-ID: <41F1DCBA.8020706@joeconway.com>

Dirk Eddelbuettel wrote:
> On Fri, Jan 21, 2005 at 06:06:45PM -0800, Leah Barrera wrote:
>> I am trying to plot in R from a perl script using the Statistics::R
>>  package as my bridge.  The following are the conditions:
>> 
>> 0. I am running from a Linux server.
>> 
> Plotting certain formats requires the X11 server to be present as the
> font metrics for those formats can be supplied only the X11 server.
> Other drivers don;t the font metrics from X11 -- I think pdf is a
> good counterexample. When you run in 'batch' via a Perl script, you
> don't have the X11 server -- even though it may be on the machine and
> running, it is not associated with the particular session running
> your Perl job.  There are two common fixes:
> 
> a) if you must have png() as a format, you can start a virtual X11
> server with the xvfb server -- this is a bit involved, but doable;

Attached is an init script I use to start up xvfb on Linux.

HTH,

Joe
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Xvfb
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050121/d7e7cded/Xvfb.pl

From tchur at optushome.com.au  Sat Jan 22 06:33:39 2005
From: tchur at optushome.com.au (Tim Churches)
Date: Sat, 22 Jan 2005 16:33:39 +1100
Subject: [R] Plotting with Statistics::R, Perl/R
In-Reply-To: <41F1DCBA.8020706@joeconway.com>
References: <Pine.GSO.4.58.0501211757210.3973@bioinf.ucsd.edu>	<20050122021945.GA633@sonny.eddelbuettel.com>
	<41F1DCBA.8020706@joeconway.com>
Message-ID: <41F1E5B3.9020704@optushome.com.au>

Joe Conway wrote:

> Dirk Eddelbuettel wrote:
>
>> On Fri, Jan 21, 2005 at 06:06:45PM -0800, Leah Barrera wrote:
>>
>>> I am trying to plot in R from a perl script using the Statistics::R
>>>  package as my bridge.  The following are the conditions:
>>>
>>> 0. I am running from a Linux server.
>>>
>> Plotting certain formats requires the X11 server to be present as the
>> font metrics for those formats can be supplied only the X11 server.
>> Other drivers don;t the font metrics from X11 -- I think pdf is a
>> good counterexample. When you run in 'batch' via a Perl script, you
>> don't have the X11 server -- even though it may be on the machine and
>> running, it is not associated with the particular session running
>> your Perl job.  There are two common fixes:
>>
>> a) if you must have png() as a format, you can start a virtual X11
>> server with the xvfb server -- this is a bit involved, but doable;
>
>
> Attached is an init script I use to start up xvfb on Linux.
>
> HTH,
>
> Joe
>
>------------------------------------------------------------------------
>
>#!/bin/bash
>#
># syslog        Starts Xvfb.
>#
>#
># chkconfig: 2345 12 88
># description: Xvfb is a facility that applications requiring an X frame buffer \
># can use in place of actually running X on the server
>
># Source function library.
>. /etc/init.d/functions
>
>[ -f /usr/X11R6/bin/Xvfb ] || exit 0
>
>XVFB="/usr/X11R6/bin/Xvfb :5 -screen 0 1024x768x16"
>
>RETVAL=0
>
>umask 077
>
>start() {
>        echo -n $"Starting Xvfb: "
>        $XVFB&
>        RETVAL=$?
>        echo_success
>        echo
>        [ $RETVAL = 0 ] && touch /var/lock/subsys/Xvfb
>        return $RETVAL
>}
>stop() {
>        echo -n $"Shutting down Xvfb: "
>        killproc Xvfb
>        RETVAL=$?
>        echo
>        [ $RETVAL = 0 ] && rm -f /var/lock/subsys/Xvfb
>        return $RETVAL
>}
>restart() {
>        stop
>        start
>}
>
>case "$1" in
>  start)
>        start
>        ;;
>  stop)
>        stop
>        ;;
>  restart|reload)
>        restart
>        ;;
>  condrestart)
>        [ -f /var/lock/subsys/Xvfb ] && restart || :
>        ;;
>  *)
>        echo $"Usage: $0 {start|stop|restart|condrestart}"
>        exit 1
>esac
>
>exit $RETVAL
>  
>
Hmm, the only problem with that is that, if I am not mistaken, you are 
starting Xvfb without any authentication, and I am told by people who 
know about such things that in the context of an Internet-accessible Web 
server, having an X server accepting unauthenticated connections is not 
a good idea. In other, less hostile environments, it might be OK. Maybe 
such concerns are unreasonable paranoia, but my motto is better safe 
than sorry when it comes to Internet-facing servers. I think there are 
also other switches you can pass to Xvfb to stop it listening on various 
TCP/IP ports etc.

Tim C



From ripley at stats.ox.ac.uk  Sat Jan 22 09:18:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 22 Jan 2005 08:18:03 +0000 (GMT)
Subject: [R] Hardware Suggestions
In-Reply-To: <x2is5qz91y.fsf@biostat.ku.dk>
References: <9D33C6169B1FDB419767356B8A8FEDB5011475E0@lynx.corp.surromed.com>
	<x2is5qz91y.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.61.0501220810530.15330@gannet.stats>

On Fri, 21 Jan 2005, Peter Dalgaard wrote:

> "Jon Dressel" <jdressel at surromed.com> writes:
>
>> We are currently running R under Windows 2000 on a server box
>> running with 2 1.2 GHZ Intel Pentium III Processors. We would like
>> to run this on a new computer running Linux and receive a
>> significant speed increase over our current implementation. Could
>> anyone provide some suggestions for a fast 64 BIT Intel based
>> processor computer with a recommendation for memory and
>> speed/type/number of processors. Also which version of R would
>> install "out-of-the-box" easily on this computer and what version of
>> Linux should be used? Thanks in advance for any help.
>
> (I assume "Intel" also means AMD?)
>
> People seem quite happy with dual and quad Opterons (and there are
> dual-core chips coming up soon, I hear), but you do need to do your
> homework, since there have been trouble with some chipsets/BIOSes in
> large-memory configurations, and there are not all that many people
> using the high-end stuff. Check out the archives of the x86_64 mailing
> lists for the popular Linux distributions.

Just find a good box-builder and let them take care of such details.
We have several dual Opterons as well as a 100+ processor cluster.

> Distribution-wise Fedora Core and SuSE both work nicely and R has been
> tested on both with no issues that I can think of. There's an RPM up
> for FC3, but it's not a big hassle to build from source and you need
> most of the build tools in place to install CRAN packages anyway.

Agreed. You do need recent versions of the OS (FC3 works better than FC2, 
and a lot better than the version of RHEL3 we returned for a refund: we 
also run SuSe 9.x).

People have been running 64-bit R for a long time on other hardware and I 
have run systematic tests across CRAN on one of our x86_64s.  All but a 
handful of maintainers have responded to my change suggestions and so 
almost all packages have passes their tests.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sat Jan 22 11:59:08 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Jan 2005 11:59:08 +0100
Subject: [R] Plotting with Statistics::R, Perl/R
In-Reply-To: <20050122021945.GA633@sonny.eddelbuettel.com>
References: <Pine.GSO.4.58.0501211757210.3973@bioinf.ucsd.edu>
	<20050122021945.GA633@sonny.eddelbuettel.com>
Message-ID: <x23bwtafwj.fsf@biostat.ku.dk>

Dirk Eddelbuettel <edd at debian.org> writes:

> a) if you must have png() as a format, you can start a virtual X11 server
>    with the xvfb server -- this is a bit involved, but doable;
>    
> b) if you can do other formats instead of png(), this can work -- I have
>    forgotten the details but the story has been discussed many times over
>    here as web servers use the same "session-less" setup.  In essence,
>    using ghostscript for the bitmap conversion works so it may be that
>    the bitmap() device works.  If I recall pdf(), works.  
>    
> c) With that, you could try creating a pdf() first, and then use Perl to
>    call ghostscript to convert the pdf for you.  R can do that too for
>    you, but only for certain devices.  

d) Use bitmap(). It requires a working Ghostscript install, but is
otherwise much more convenient. Newer versions of Ghostscript have
some quite decent antialiasing built into some of the png devices.
Currently you need a small hack to pass the extra options to
Ghostscript -- we should probably add a gsOptions argument in due
course. This works for me on FC3 (Ghostscript 7.07):

mybitmap(file="foo.png", type="png16m", gsOptions=" -dTextAlphaBits=4
-dGraphicsAlphaBits=4 ")

where mybitmap() is a modified bitmap() that just sticks the options
into the command line. There are definitely better ways...

[The antialiasing is not quite perfect. In particular, the axes stand
out from the box around plots, presumably because an additive model is
used (so that if you draw a line on top of itself, the result becomes
darker). Also, text gets a little muddy at the default 9pt @ 72dpi, so
you probably want to increase the pointsize or the resolution.]

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ckjmaner at carolina.rr.com  Sat Jan 22 16:56:16 2005
From: ckjmaner at carolina.rr.com (Charles and Kimberly Maner)
Date: Sat, 22 Jan 2005 10:56:16 -0500
Subject: [R] an R script editor for Mac
Message-ID: <200501221557.j0MFv6Ci029544@ms-smtp-04-eri0.southeast.rr.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050122/a9fd839c/attachment.pl

From 0034058 at fudan.edu.cn  Sat Jan 22 17:03:31 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Sun, 23 Jan 2005 00:03:31 +0800
Subject: [R] how to analysis the factor score?
Message-ID: <0IAQ001S070O1T@mail.fudan.edu.cn>

i use factanal() to do factor analysis and get the factor score.
and i want to do further analysis with the score and some varibles in the original data frame.
but i don't know how to do it.i want to merge the 2 file into one.bu fail.
maybe due to some missing value.the original data has 865 obs,but the score has 827obs.so i don't know how to do the further job.

i want to scan the book MASS,but don't find the answer.



From shuangge at uwalumni.com  Sat Jan 22 17:36:36 2005
From: shuangge at uwalumni.com (shuangge@uwalumni.com)
Date: Sat, 22 Jan 2005 10:36:36 -0600
Subject: [R] questions with library lars()
Message-ID: <20050122103636.hqo8w4sckkg8g0s8@secure.uwalumni.com>

hello,
I have the following questions with the R package lars(). I would really
appreciate som help here.

1. do I have to standardize the predictors and the response?

2. the function cv.lars(): how can I specify it is for lasso not stagewise or
lars?

your help will be really appreciated. thanks,



From paulo.barbosa at oninetspeed.pt  Sat Jan 22 18:34:48 2005
From: paulo.barbosa at oninetspeed.pt (Paulo Barbosa)
Date: Sat, 22 Jan 2005 17:34:48 -0000
Subject: [R] evir package as.double problem
Message-ID: <BF5E81AE19A10C4595C2037C75B8A705216355@BELAVSNN.at.isp>

Dear Sirs,

I am working with a time series of financial data compiled in a csv file.

When i tried to apply the findthresh funtion of the evir package to that data I got the following error messages (the second after transforming the list in a vector):

"Error in as.double.default(x) : (list) object cannot be coerced to double"

"Error in as.double.default(as.vector(port)) :
        (list) object cannot be coerced to double"


I hope ypu can help me with this problem.

Thanks,

Paulo Barbosa



From machud at intellektik.informatik.tu-darmstadt.de  Sat Jan 22 17:38:45 2005
From: machud at intellektik.informatik.tu-darmstadt.de (Marco Chiarandini)
Date: Sat, 22 Jan 2005 17:38:45 +0100 (CET)
Subject: [R] Wilcoxon test for mixed design (between-within subjects)
In-Reply-To: <200501211125.j0LBBNYs009020@hypatia.math.ethz.ch>
References: <200501211125.j0LBBNYs009020@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0501221719020.15446@kika.intellektik.informatik.tu-darmstadt.de>

Hallo,

is there any extension of the pairwise Wilcoxon test to a dependent
samples layout with replicates (or, in other terms, a one-way layout
with blocking and replicates)?

The Wilcoxon method with matched pairs works for the case of dependent
samples with one observation per block, while the Mann-Whitney test
works for independent samples, thus one single block and replicated
observations. Is there a method which mixes this two cases considering a
depedent sample design in which each block has more than one
observation? I know it exists a Friedman test for this case but in the
Friedman test ranks are constructed considering all subjects jointly,
while in Wilcoxon only the pair of subject currently considered are
ranked, thus resulting in a more powerful test.

If no such method exists in R, I am anyway intersted in possible
references.

Thank you for the consideration,

Regards,

Marco



From numero.primo at tele2.it  Sat Jan 22 22:49:53 2005
From: numero.primo at tele2.it (Landini Massimiliano)
Date: Sat, 22 Jan 2005 22:49:53 +0100
Subject: [R] statistical test improvement of readability (was average
	disjunction)
Message-ID: <uc75v012u679kp07igrf54pssor2q1ad7t@4ax.com>

Dear all ReadeRs
I was finding a quick method to improve test readability adding or constructing
(with your help....) one or more function that allow what follow.

Please consider

Trt<-c(1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5)
Block<-c(1,2,3,4,1,2,3,4,1,2,3,4,1,2,3,4,1,2,3,4)
AD2DAT<-c(1.44,2.32,1.68,1.28,0.12,0.08,0.24,0.52,0.12,0.08,0.16,0.28,0.16,0.08,0.12,0.12,0.16,0.08,0.32,0.76)
Tab<-cbind(Trt,Block,AD2DAT);Tab
AD2DAT.aov.lm<-aov(lm(AD2DAT~as.factor(Block)+as.factor(Trt),method="qr"))
TukeyHSD(AD2DAT.aov.lm,"as.factor(Trt)",ordered=TRUE,conf.level=.95)
TukeyHSD(AD2DAT.aov.lm,"as.factor(Trt)",ordered=TRUE,conf.level=.99)
TabMEANS<-aggregate(Tab,list(Trt),FUN=mean);TabMEANS

Group.1	Trt	Block	AD2DAT
1	1	 2.5	 1.68
2	 2	 2.5	 0.24
3	3	2.5	0.16
4	4	2.5	0.12
5	5	2.5	0.33

So I'm thinking that was more readable something like one function that permit
me to juxtapose tiny letters when we are talking about .95 confidence level and
capital letters when we are talking about .99 confidence level so results will
be approx this

Group.1	Trt	Block	AD2DAT		.95	.99
1	1	 2.5	 1.68		a	A
2	 2	 2.5	 0.24		b	B
3	3	2.5	0.16		b	B
4	4	2.5	0.12		b	B
5	5	2.5	0.33		b	B

Does anyone have suggestion???
Thanks in advance!!




-------------------------------------------------------------------------------------------------------------------------
Landini dr. Massimiliano
Tel. mob. (+39) 347 140 11 94
Tel./Fax. (+39) 051 762 196
e-mail: numero (dot) primo (at) tele2 (dot) it
-------------------------------------------------------------------------------------------------------------------------
Legge di Hanggi: Pi? stupida ? la tua ricerca, pi? verr? letta e approvata.
Corollario alla Legge di Hanggi: Pi? importante ? la tua ricerca, meno verr?
capita.



From tchur at optushome.com.au  Sat Jan 22 23:29:02 2005
From: tchur at optushome.com.au (Tim Churches)
Date: Sun, 23 Jan 2005 09:29:02 +1100
Subject: [R] Plotting with Statistics::R, Perl/R
In-Reply-To: <x23bwtafwj.fsf@biostat.ku.dk>
References: <Pine.GSO.4.58.0501211757210.3973@bioinf.ucsd.edu>	<20050122021945.GA633@sonny.eddelbuettel.com>
	<x23bwtafwj.fsf@biostat.ku.dk>
Message-ID: <41F2D3AE.3040507@optushome.com.au>

Peter Dalgaard wrote:

>d) Use bitmap(). It requires a working Ghostscript install, but is
>otherwise much more convenient. Newer versions of Ghostscript have
>some quite decent antialiasing built into some of the png devices.
>Currently you need a small hack to pass the extra options to
>Ghostscript -- we should probably add a gsOptions argument in due
>course. This works for me on FC3 (Ghostscript 7.07):
>
>mybitmap(file="foo.png", type="png16m", gsOptions=" -dTextAlphaBits=4
>-dGraphicsAlphaBits=4 ")
>
>where mybitmap() is a modified bitmap() that just sticks the options
>into the command line. There are definitely better ways...
>
>[The antialiasing is not quite perfect. In particular, the axes stand
>out from the box around plots, presumably because an additive model is
>used (so that if you draw a line on top of itself, the result becomes
>darker). Also, text gets a little muddy at the default 9pt @ 72dpi, so
>you probably want to increase the pointsize or the resolution.]
>  
>
Apart from the significant quality issues which you mention, the other 
problem with using bitmap() in a Web server environment is the speed 
issue - it takes much longer to produce the output. Whether it takes too 
long depends on the users of your Web application, and how many 
simultaneous users there are. However, most users are more worried by 
the poor quality of the fonts in output produced by bitmap().

Tim C



From je_lemaitre at hotmail.com  Sat Jan 22 23:31:25 2005
From: je_lemaitre at hotmail.com (=?iso-8859-1?B?Suly9G1lIExlbWHudHJl?=)
Date: Sat, 22 Jan 2005 17:31:25 -0500
Subject: [R] printing PCA scores
Message-ID: <BAY103-DAV1758A435D8552B5E8FDEF990830@phx.gbl>

Hey folks,

I have an environmental dataset on which I conducted a PCA (prcomp) and I
need the scores of this PCA for each site (=each row) to conduct further
analyses.

Can you please help me with that?

Thanks a lot


J?r?me Lema?tre



From dassybr at gmail.com  Sun Jan 23 01:41:46 2005
From: dassybr at gmail.com (Hadassa Brunschwig)
Date: Sat, 22 Jan 2005 16:41:46 -0800
Subject: [R] survreg: fitting different location parameters
Message-ID: <83cfc0bd05012216415be16cc@mail.gmail.com>

Hi R-Help!

My question: I have lifetime/failure data of machines with different
stress levels and i think an weibull/extreme value distribution would
fit this data. So I did:

model1      <- survreg(Surv(lfailure)~stress,data=steel,dist="extreme")

(where lfailure=log(failure))

Now I would like to do a likelihood ratio test to test the hypothesis 

H0: location parameters of the extreme value distribution are equal
for each stress level.

So in order to perform the likelihood ratio test I need the likelood
of the model under

HA: location parameters are not equal (i.e. each stress level has its
slope and intercept).

How can I do this? 

Thanks for any help!

Hadassa



From omfant at sbcglobal.net  Sun Jan 23 05:55:12 2005
From: omfant at sbcglobal.net (Olivia Fant)
Date: Sat, 22 Jan 2005 20:55:12 -0800 (PST)
Subject: [R] scan command
Message-ID: <20050123045512.94050.qmail@web81510.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050122/f7ca3b3f/attachment.pl

From Christoph.Scherber at uni-jena.de  Sun Jan 23 10:34:58 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Sun, 23 Jan 2005 10:34:58 +0100
Subject: [R] comparing glmmPQL models
Message-ID: <1106472898.41f36fc2708b4@webmail.uni-jena.de>

Dear R users,

Is there a way to compare glmmPQL models differing in their fixed-effects
structure (similar to the ANOVA approach in lme) ?

Thank you very much for your help!
Chris.


----------------------------------------------------------------
This mail was sent through http://webmail.uni-jena.de



From h.brunschwig at utoronto.ca  Sun Jan 23 14:33:37 2005
From: h.brunschwig at utoronto.ca (h.brunschwig@utoronto.ca)
Date: Sun, 23 Jan 2005 08:33:37 -0500
Subject: [R] survreg: fitting different location parameters
Message-ID: <1106487217.41f3a7b1c8395@webmail.utoronto.ca>

Hi R-Help!

My question: I have lifetime/failure data of machines with different
stress levels and i think an weibull/extreme value distribution would
fit this data. So I did:

model1      <- survreg(Surv(lfailure)~stress,data=steel,dist="extreme")

(where lfailure=log(failure))

Now I would like to do a likelihood ratio test to test the hypothesis

H0: location parameters of the extreme value distribution are equal
for each stress level.

So in order to perform the likelihood ratio test I need the likelood
of the model under

HA: location parameters are not equal (i.e. each stress level has its
slope and intercept).

How can I do this?

Thanks for any help!

Hadassa



From rpeng at jhsph.edu  Sun Jan 23 17:03:30 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Sun, 23 Jan 2005 11:03:30 -0500
Subject: [R] Hardware Suggestions
In-Reply-To: <9D33C6169B1FDB419767356B8A8FEDB5011475E0@lynx.corp.surromed.com>
References: <9D33C6169B1FDB419767356B8A8FEDB5011475E0@lynx.corp.surromed.com>
Message-ID: <41F3CAD2.90408@jhsph.edu>

We've been running servers with AMD Opterons (both duals and 4-ways) using SuSE
SLES 8 and Fedora Core 2 and 3 (64 bit).  All work well and R can access up to
~15GB of RAM on our 4-way machine (which has 16GB installed).  One of the Red
Hat Enterprise Linux versions comes with a buggy compiler but I can't remember
which version (there is some discussion on the R-devel mailing list archives).
Watch out for that.  I think using gcc 3.4.x solves that problem.

R pretty much compiles out of the box on AMD Opteron/Fedora Core 3 (64-bit) 
Linux.  We've been very happy with our R experience so far.

-roger

Jon Dressel wrote:
> We are currently running R under Windows 2000 on a server box running with 2
> 1.2 GHZ Intel Pentium III Processors.  We would like to run this on a new
> computer running Linux and receive a significant speed increase over our
> current implementation.   Could anyone provide some suggestions for a fast 64
> BIT Intel based processor computer with a  recommendation  for memory and
> speed/type/number of processors.  Also which version of R would install
> "out-of-the-box" easily on this computer and what version of Linux should be
> used?  Thanks in advance for any help.
> 
> Jon Dressel, MCSE MCSA MCP A+<?xml:namespace prefix = o ns =
> "urn:schemas-microsoft-com:office:office" />
> 
> Applications Supervisor
> 
> SurroMed, Inc.
> 
> 
> 1430 O'Brien Drive
> 
> 
> Menlo Park, CA 94025
> 
> Phone: 650.470.2322
> 
> Fax:     650.470.2400
> 
> email:   jdressel at surromed.com
> 
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________ R-help at stat.math.ethz.ch
> mailing list https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide! http://www.R-project.org/posting-guide.html
>



From patrick.giraudoux at univ-fcomte.fr  Sun Jan 23 18:51:54 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux H)
Date: Sun, 23 Jan 2005 18:51:54 +0100
Subject: [R] error preparing a package for lazy loading with R CMD
Message-ID: <5.0.2.1.2.20050123182045.02bc6f48@utinam.univ-fcomte.fr>

Dear Lister,

I  work with R 2.0.1 and Windows XP, and meet a strange trouble trying to 
make a R package with a make-package.bat file John Fox has kindly provided 
(see detailed script below). I am working with it since some months with 
excellent results (I do'nt use compiled C code so far). Just adding a new 
function in the R directory today, when running make-package and thus 
excuting the command ..\..\bin\R CMD build --force --binary --auto-zip 
%1,  I got the following message after the "compile" stage, preparing the 
package for lazy loading :

preparing package pgirmess for lazy loading
Error in "names<-.default"(`*tmp*`, value =c("R", "Platform", "Date", :
         names attribute[4] must be the same length as the vector [1]
Execution halted
make: ***[lazyload] Error 1
*** Installation of pgirmess failed ***

(pgirmess is the package name)

... and the zip file is not generated.

I have checked and rechecked everything during this (long) afternoon... and 
get nothing except that if I drag out any of the *.r file of the R folder, 
everything comes to be OK (except the function that has been dragged out is 
missing...). It looks like if having added one extra function in the R 
folder on the top of the earlier 32 (+ 2 data frames) makes problem.

I have then consulted John Fox offlist and he seems quite perplexed "I'm 
not sure why you're experiencing this problem". On his advise I have 
included "LazyLoad: no" in the package description file. In this case 
everything goes smoothly then (except LazyLoad will not be activated), the 
zip file is generated and the package can be installed from R.

Has anybody an idea about why a problem occurs when preparing the package 
for lazy loading? Any remedy?

Kind regards,

Patrick

Make-Package script:

cd c:\R\rw2001\src\library
del %1\INDEX
del %1\data\00Index
del %1\chm\*.* /Q
..\..\bin\R CMD build --force --binary --auto-zip %1
..\..\bin\R CMD build --force %1
..\..\bin\R CMD check %1
cd %1.Rcheck
dvipdfm %1-manual
notepad 00check.log
cd ..
cd ..


>From: "John Fox" <jfox at mcmaster.ca>
>To: "'Patrick Giraudoux H'" <patrick.giraudoux at univ-fcomte.fr>
>Subject: RE: [R] writing a simple package in R 2.0 under Windows XP
>Date: Sun, 23 Jan 2005 11:41:25 -0500
>X-Mailer: Microsoft Office Outlook, Build 11.0.6353
>X-MIME-Autoconverted: from quoted-printable to 8bit by 
>utinam.univ-fcomte.fr id j0NGfJoD011345
>
>Dear Patrick,
>
>I'm not sure why you're experiencing this problem.
>
>Two suggestions: (1) Since the problem appears to be with preparing the
>package for lazy loading, try adding the directive "LazyLoad: no" (without
>the quotes) to the package's DESCRIPTION file. (2) Rather than using my
>batch file, run the commands one at a time to see exactly where the problem
>is produced; then you could send a message to r-help.
>
>I hope this helps,
>  John



From cuiczhao at yahoo.com  Mon Jan 24 00:18:09 2005
From: cuiczhao at yahoo.com (Cuichang Zhao)
Date: Sun, 23 Jan 2005 15:18:09 -0800 (PST)
Subject: [R] read data from a file and vector expansion
Message-ID: <20050123231810.32645.qmail@web30704.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050123/5fdbf0d5/attachment.pl

From parkhurs at ariel.ucs.indiana.edu  Mon Jan 24 00:35:00 2005
From: parkhurs at ariel.ucs.indiana.edu (David Parkhurst)
Date: Sun, 23 Jan 2005 18:35:00 -0500
Subject: [R] How to use "identify"
Message-ID: <41F434A4.8090809@ariel.ucs.indiana.edu>

I can't get identify to work, using R 2.0.1 under windows xp pro, 
service pack 2.  Here's what I enter, and the result:

 > plot((our.frame2$c1),(our.frame2$c9))  # Produces desired plot
 > identify(our.frame2$c1) # Plot comes to forefront, so I select a point
warning: no point with 0.25 inches
numeric(0)

Is my call to identify correct?  The help page for indentify (from 
?identify) doesn't give any examples, so I don't know what the "x" 
should be.  I right click right on top of one of the points, then left 
click and select stop.

Thanks for any help.

Dave Parkhurst



From h.brunschwig at utoronto.ca  Sun Jan 23 21:10:18 2005
From: h.brunschwig at utoronto.ca (h.brunschwig@utoronto.ca)
Date: Sun, 23 Jan 2005 15:10:18 -0500
Subject: [R] survreg: fitting different location parameters
Message-ID: <1106511018.41f404aab2411@webmail.utoronto.ca>

Hi R-Help! 

My question: I have lifetime/failure data of machines with different 
stress levels and i think an weibull/extreme value distribution would 
fit this data. So I did: 

model1      <- survreg(Surv(lfailure)~stress,data=steel,dist="extreme") 

(where lfailure=log(failure)) 

Now I would like to do a likelihood ratio test to test the hypothesis 

H0: location parameters of the extreme value distribution are equal 
for each stress level. 

So in order to perform the likelihood ratio test I need the likelood 
of the model under 

HA: location parameters are not equal (i.e. each stress level has its 
slope and intercept). 

How can I do this? 

Thanks for any help! 

Hadassa



From duncan.mackay at flinders.edu.au  Mon Jan 24 08:39:13 2005
From: duncan.mackay at flinders.edu.au (duncan.mackay@flinders.edu.au)
Date: Mon, 24 Jan 2005 18:09:13 +1030
Subject: [R] package dependency error on loading lme4
Message-ID: <1106552353.41f4a621b0e5e@imap.flinders.edu.au>



Hi all,
I recently (today) updated the Matrix package and installed the latticeExtra
package, but then when I tried to load the lme4 package, I got the following
error message:-

> library(lme4)
Loading required package: Matrix 
Loading required package: latticeExtra 
Error in importIntoEnv(impenv, impnames, ns, impvars) : 
        object(s) '.__C__lmeRep' are not exported by 'namespace:Matrix' Error in
library(lme4) : package/namespace load failed for 'lme4'
> 

???????????????????????????????????

I'm running R 2.0.1 under WinXP as follows
> R.Version()
$platform
[1] "i386-pc-mingw32"

$arch
[1] "i386"

$os
[1] "mingw32"

$system
[1] "i386, mingw32"

$status
[1] ""

$major
[1] "2"

$minor
[1] "0.1"

$year
[1] "2004"

$month
[1] "11"

$day
[1] "15"

$language
[1] "R"

Cheers, Duncan


*****************************************
Dr. Duncan Mackay
School of Biological Sciences
Flinders University
GPO Box 2100
Adelaide
S.A.    5001
AUSTRALIA

Ph (08) 8201 2627    FAX (08) 8201 3015

http://www.scieng.flinders.edu.au/biology/people/mackay_d/index.html



From jarioksa at sun3.oulu.fi  Mon Jan 24 10:17:47 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Mon, 24 Jan 2005 11:17:47 +0200
Subject: [R] printing PCA scores
In-Reply-To: <000001c500d2$1b70b2f0$5006cb84@yourd93e6doqk7>
References: <000001c500d2$1b70b2f0$5006cb84@yourd93e6doqk7>
Message-ID: <1106558267.5364.37.camel@biol102145.oulu.fi>

On Sat, 2005-01-22 at 17:31 -0500, J?r?me Lema?tre wrote:
> Hey folks,
> 
> I have an environmental dataset on which I conducted a PCA (prcomp) and I
> need the scores of this PCA for each site (=each row) to conduct further
> analyses.
> 
> Can you please help me with that?
> 
Did you try help(prcomp) ?

It says that prcomp (may) return an item called 'x':

x: if 'retx' is true the value of the rotated data (the centred
          (and scaled if requested) data multiplied by the 'rotation'
          matrix) is returned.

[non-matching parentheses in the original help file]

So this is what you asked for.

cheers, jari oksanen

-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From petr.pikal at precheza.cz  Mon Jan 24 10:28:54 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 24 Jan 2005 10:28:54 +0100
Subject: [R] scan command
In-Reply-To: <20050123045512.94050.qmail@web81510.mail.yahoo.com>
Message-ID: <41F4CDE6.22620.9D1F25@localhost>

Hi Olivia


On 22 Jan 2005 at 20:55, Olivia Fant wrote:

> I am using the scan command to read a file I called data1.  The
> program returns
> 
>                Error in file(file,"r") : unable to open connection In
>                addition: Warning message: cannot open file 'data1'
> 

I am not exactly sure but 

either your file is locked by some program

or your data1 has an extension which your OS (probably 
Windows) hides and which you need to use

or data1 is in different directory

Do you see it through choose.files() ?

Cheers
Petr

> I already checked to see if the starting directory in R was correct
> and it is. Also, the file opens ok from its directory so it isn't
> corrupted. Does someone have an idea?
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From paul.bliese at us.army.mil  Mon Jan 24 10:23:47 2005
From: paul.bliese at us.army.mil (Bliese, Paul D LTC USAMH)
Date: Mon, 24 Jan 2005 10:23:47 +0100
Subject: [R] mcnemar.test odds ratios, CI, etc.
Message-ID: <FADCFAA8BA80C748890C1D3893C198D91579DF@amedmlmhah01.eur.amed.ds.army.mil>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050124/7a607846/attachment.pl

From petr.pikal at precheza.cz  Mon Jan 24 11:40:52 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 24 Jan 2005 11:40:52 +0100
Subject: [R] read data from a file and vector expansion
In-Reply-To: <20050123231810.32645.qmail@web30704.mail.mud.yahoo.com>
Message-ID: <41F4DEC4.15739.DF2955@localhost>



From petr.pikal at precheza.cz  Mon Jan 24 11:43:54 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 24 Jan 2005 11:43:54 +0100
Subject: [R] How to use "identify"
In-Reply-To: <41F434A4.8090809@ariel.ucs.indiana.edu>
Message-ID: <41F4DF7A.29771.E1F16E@localhost>



From tmulholland at bigpond.com  Mon Jan 24 11:50:33 2005
From: tmulholland at bigpond.com (Tom Mulholland)
Date: Mon, 24 Jan 2005 18:50:33 +0800
Subject: [R] How to use "identify"
In-Reply-To: <41F434A4.8090809@ariel.ucs.indiana.edu>
References: <41F434A4.8090809@ariel.ucs.indiana.edu>
Message-ID: <41F4D2F9.6060100@bigpond.com>

Does this work for you?

x <- runif(30)
y <- runif(30)
z <- 1:30
plot(x,y)
identify(x,y,z)

That is you need to tell it everything it is looking for. Sometimes x is 
boy x and y because they use xy.coords

Tom

David Parkhurst wrote:
> I can't get identify to work, using R 2.0.1 under windows xp pro, 
> service pack 2.  Here's what I enter, and the result:
> 
>  > plot((our.frame2$c1),(our.frame2$c9))  # Produces desired plot
>  > identify(our.frame2$c1) # Plot comes to forefront, so I select a point
> warning: no point with 0.25 inches
> numeric(0)
> 
> Is my call to identify correct?  The help page for indentify (from 
> ?identify) doesn't give any examples, so I don't know what the "x" 
> should be.  I right click right on top of one of the points, then left 
> click and select stop.
> 
> Thanks for any help.
> 
> Dave Parkhurst
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From gavin.simpson at ucl.ac.uk  Mon Jan 24 11:52:43 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 24 Jan 2005 10:52:43 +0000
Subject: [R] How to use "identify"
In-Reply-To: <41F434A4.8090809@ariel.ucs.indiana.edu>
References: <41F434A4.8090809@ariel.ucs.indiana.edu>
Message-ID: <41F4D37B.4020706@ucl.ac.uk>

David Parkhurst wrote:
> I can't get identify to work, using R 2.0.1 under windows xp pro, 
> service pack 2.  Here's what I enter, and the result:
> 
>  > plot((our.frame2$c1),(our.frame2$c9))  # Produces desired plot
>  > identify(our.frame2$c1) # Plot comes to forefront, so I select a point
> warning: no point with 0.25 inches
> numeric(0)
> 
> Is my call to identify correct?  The help page for indentify (from 
> ?identify) doesn't give any examples, so I don't know what the "x" 
> should be.  I right click right on top of one of the points, then left 
> click and select stop.
> 
> Thanks for any help.
> 
> Dave Parkhurst

You forgot to pass in your y coordinates in your call to identify(). You 
can only leave off the y argument in certain circumstances (see ?identify)

Here's an example:

xdat <- rnorm(10)
ydat <- rnorm(10)
plot(xdat, ydat)
identify(xdat, ydat)

So you need:

plot(our.frame2$c1, our.frame2$c9) # you don't need the extra brackets
identify(our.frame2$c1, our.frame2$c9)

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From ligges at statistik.uni-dortmund.de  Mon Jan 24 12:05:14 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 24 Jan 2005 12:05:14 +0100
Subject: [R] package dependency error on loading lme4
In-Reply-To: <1106552353.41f4a621b0e5e@imap.flinders.edu.au>
References: <1106552353.41f4a621b0e5e@imap.flinders.edu.au>
Message-ID: <41F4D66A.1060309@statistik.uni-dortmund.de>

duncan.mackay at flinders.edu.au wrote:

> 
> Hi all,
> I recently (today) updated the Matrix package and installed the latticeExtra
> package, but then when I tried to load the lme4 package, I got the following
> error message:-
> 
> 
>>library(lme4)
> 
> Loading required package: Matrix 
> Loading required package: latticeExtra 
> Error in importIntoEnv(impenv, impnames, ns, impvars) : 
>         object(s) '.__C__lmeRep' are not exported by 'namespace:Matrix' Error in
> library(lme4) : package/namespace load failed for 'lme4'

Confirmed. There are new versions of Matrix and lme4 in the sources area 
of CRAN. They will probably appear in the Windows repository within 24 
hours. Please check again with the new versions tomorrow.

Uwe Ligges


> 
> ???????????????????????????????????
> 
> I'm running R 2.0.1 under WinXP as follows
> 
>>R.Version()
> 
> $platform
> [1] "i386-pc-mingw32"
> 
> $arch
> [1] "i386"
> 
> $os
> [1] "mingw32"
> 
> $system
> [1] "i386, mingw32"
> 
> $status
> [1] ""
> 
> $major
> [1] "2"
> 
> $minor
> [1] "0.1"
> 
> $year
> [1] "2004"
> 
> $month
> [1] "11"
> 
> $day
> [1] "15"
> 
> $language
> [1] "R"
> 
> Cheers, Duncan
> 
> 
> *****************************************
> Dr. Duncan Mackay
> School of Biological Sciences
> Flinders University
> GPO Box 2100
> Adelaide
> S.A.    5001
> AUSTRALIA
> 
> Ph (08) 8201 2627    FAX (08) 8201 3015
> 
> http://www.scieng.flinders.edu.au/biology/people/mackay_d/index.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From buser at stat.math.ethz.ch  Mon Jan 24 12:10:54 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Mon, 24 Jan 2005 12:10:54 +0100
Subject: [R] Wilcoxon test for mixed design (between-within subjects)
In-Reply-To: <Pine.LNX.4.58.0501221719020.15446@kika.intellektik.informatik.tu-darmstadt.de>
References: <200501211125.j0LBBNYs009020@hypatia.math.ethz.ch>
	<Pine.LNX.4.58.0501221719020.15446@kika.intellektik.informatik.tu-darmstadt.de>
Message-ID: <16884.55230.81332.411060@stat.math.ethz.ch>

Hallo Marco

Marco Chiarandini writes:
 > Hallo,
 > 
 > is there any extension of the pairwise Wilcoxon test to a dependent
 > samples layout with replicates (or, in other terms, a one-way layout
 > with blocking and replicates)?
 > 

There is always the possibility to summarize the replicates and
then calculate a common pairwise Wilcoxon test.


 > The Wilcoxon method with matched pairs works for the case of dependent
 > samples with one observation per block, while the Mann-Whitney test
 > works for independent samples, thus one single block and replicated
 > observations. Is there a method which mixes this two cases considering a
 > depedent sample design in which each block has more than one
 > observation? I know it exists a Friedman test for this case but in the
 > Friedman test ranks are constructed considering all subjects jointly,
 > while in Wilcoxon only the pair of subject currently considered are
 > ranked, thus resulting in a more powerful test.
 > 

The Friedman test calculates the ranks inside of the blocks, for
each block separately and is not considering all subjects
jointly.
In R implemented is the case with unreplicated blocked data, so
this doesn't help you. 

 > If no such method exists in R, I am anyway intersted in possible
 > references.

Look at:

     Myles Hollander & Douglas A. Wolfe (1999), _Nonparametric
     statistical methods_. New York: John Wiley & Sons. 

There you find the generalization of the replicated case, but
you have to implement it.

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C11
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-1-632-5414		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------



 > Thank you for the consideration,
 > 
 > Regards,
 > 
 > Marco
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From C.Combe at napier.ac.uk  Mon Jan 24 12:13:41 2005
From: C.Combe at napier.ac.uk (Combe, Colin)
Date: Mon, 24 Jan 2005 11:13:41 -0000
Subject: [R] functions not found after installing DBI/RDBI packages
Message-ID: <735F04A99D358E468A16EDB64FC04555232B65@EVS1.napier-mail.napier.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050124/f79a890d/attachment.pl

From machud at intellektik.informatik.tu-darmstadt.de  Mon Jan 24 11:26:23 2005
From: machud at intellektik.informatik.tu-darmstadt.de (Marco Chiarandini)
Date: Mon, 24 Jan 2005 11:26:23 +0100 (CET)
Subject: [R] Wilcoxon test for mixed design (between-within subjects)
In-Reply-To: <16884.55230.81332.411060@stat.math.ethz.ch>
References: <200501211125.j0LBBNYs009020@hypatia.math.ethz.ch>
	<Pine.LNX.4.58.0501221719020.15446@kika.intellektik.informatik.tu-darmstadt.de>
	<16884.55230.81332.411060@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0501241118140.20245@kika.intellektik.informatik.tu-darmstadt.de>

Hallo Christoph,


> There is always the possibility to summarize the replicates and
> then calculate a common pairwise Wilcoxon test.


mmmh, in this case I prefer the Friedman test, I would like not to loose
any data.


> The Friedman test calculates the ranks inside of the blocks, for
> each block separately and is not considering all subjects
> jointly.


true, but it considers all the treatments together, while Wilcoxon works
per pairs of treatments within the blocks.


> In R implemented is the case with unreplicated blocked data, so
> this doesn't help you.
>
>  > If no such method exists in R, I am anyway intersted in possible
>  > references.
>
> Look at:
>
>      Myles Hollander & Douglas A. Wolfe (1999), _Nonparametric
>      statistical methods_. New York: John Wiley & Sons.
>
> There you find the generalization of the replicated case, but
> you have to implement it.


I already implemented the Friedman test for the replicated case although
taken from Conover 1999 _Practical non parametric statistics_. I hope it
is the same. My interest in a Wilcoxon procedure is due to the fact that
according to Hsu 1996 _Multiple comparisons_ the Friedman test is not
recommended for multiple comparisons becasue it has been shown that it
is not a confident method (the actual alpha value is greater).


Regards,


Marco



-------------------------------------------------------------------
Marco Chiarandini, Fachgebiet Intellektik, Fachbereich Informatik,
Technische Universit?t Darmstadt, Hochschulstra?e 10,
D-64289 Darmstadt - Germany, Office: S2/02 Raum E312
Tel: +49.(0)6151.166802 Fax: +49.(0)6151.165326
email: machud at intellektik.informatik.tu-darmstadt.de
web page: http://www.intellektik.informatik.tu-darmstadt.de/~machud



From murdoch at stats.uwo.ca  Mon Jan 24 12:45:14 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 24 Jan 2005 11:45:14 +0000
Subject: [R] How to use "identify"
In-Reply-To: <41F434A4.8090809@ariel.ucs.indiana.edu>
References: <41F434A4.8090809@ariel.ucs.indiana.edu>
Message-ID: <gqn9v0d4dp7uu8u9jbqavr0ir73epfran2@4ax.com>

On Sun, 23 Jan 2005 18:35:00 -0500, David Parkhurst
<parkhurs at ariel.ucs.indiana.edu> wrote :

>I can't get identify to work, using R 2.0.1 under windows xp pro, 
>service pack 2.  Here's what I enter, and the result:
>
> > plot((our.frame2$c1),(our.frame2$c9))  # Produces desired plot
> > identify(our.frame2$c1) # Plot comes to forefront, so I select a point
>warning: no point with 0.25 inches
>numeric(0)
>
>Is my call to identify correct?  The help page for indentify (from 
>?identify) doesn't give any examples, so I don't know what the "x" 
>should be.  I right click right on top of one of the points, then left 
>click and select stop.

You want to give both x and y coordinates to identify(), matching the
points you plotted, e.g.. change your example to

 plot((our.frame2$c1),(our.frame2$c9))  # Produces desired plot
 identify(our.frame2$c1, our.frame2$c9) # Give the same coords

Duncan Murdoch



From p.dalgaard at biostat.ku.dk  Mon Jan 24 13:12:34 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Jan 2005 13:12:34 +0100
Subject: [R] mcnemar.test odds ratios, CI, etc.
In-Reply-To: <FADCFAA8BA80C748890C1D3893C198D91579DF@amedmlmhah01.eur.amed.ds.army.mil>
References: <FADCFAA8BA80C748890C1D3893C198D91579DF@amedmlmhah01.eur.amed.ds.army.mil>
Message-ID: <x2651n10wd.fsf@biostat.ku.dk>

"Bliese, Paul D LTC USAMH" <paul.bliese at us.army.mil> writes:

> Does anyone know of another version of the Mcnemar test that provides:
> 
>  
> 
> 1.  Odds Ratios
> 
> 2.  95% Confidence intervals of the Odds Ratios
> 
> 3.  Sample probability 
> 
> 4.  95% Confidence intervals of the sample probability
> 
>  
> 
> Obviously the Odds Ratios and Sample probabilities are easy to calculate
> from the contingency table, but I would appreciate any help on how to
> calculate the confidence intervals.
> 
>  
> 
> Below is a simple example of the test, and the corresponding output with
> the function mcnemar.test.
> 
>  
> 
> > xtabs(~PLC50.T1+PLC50.T2,data=LANCET.DAT)
> 
>         PLC50.T2
> 
> PLC50.T1 0   1  
> 
>        0 464  22
> 
>        1   6   1
> 
> > mcnemar.test(xtabs(~PLC50.T1+PLC50.T2,data=LANCET.DAT))
> 
>  
> 
>         McNemar's Chi-squared test with continuity correction
> 
>  
> 
> data:  xtabs(~PLC50.T1 + PLC50.T2, data = LANCET.DAT) 
> 
> McNemar's chi-squared = 8.0357, df = 1, p-value = 0.004586


What is the "sample probability" in this context? The odds ratio is a
simple functional of the off-diagonal elements, and the conditional
distribution of those given their sum is just a binomial, so you can
use prop.test or binom.test to get estimate and confidence intervals
for the probability parameter and convert that to odds.

E.g.

> prop.test(6,28)

        1-sample proportions test with continuity correction

data:  6 out of 28, null probability 0.5
X-squared = 8.0357, df = 1, p-value = 0.004586
alternative hypothesis: true p is not equal to 0.5
95 percent confidence interval:
 0.09027927 0.41462210
sample estimates:
        p
0.2142857


> ci.p <- prop.test(22,28)$conf
> ci.odds <- ci.p/(1-ci.p)
> ci.odds
[1]  1.411835 10.076740
attr(,"conf.level")
[1] 0.95
> ci.p <- binom.test(22,28)$conf
> ci.odds <- ci.p/(1-ci.p)
> ci.odds
[1]  1.441817 11.053913
attr(,"conf.level")
[1] 0.95



-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gb at tal.stat.umu.se  Mon Jan 24 13:49:09 2005
From: gb at tal.stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Mon, 24 Jan 2005 13:49:09 +0100
Subject: [R] survreg: fitting different location parameters
In-Reply-To: <1106511018.41f404aab2411@webmail.utoronto.ca>
References: <1106511018.41f404aab2411@webmail.utoronto.ca>
Message-ID: <20050124124909.GA12334@tal.stat.umu.se>

On Sun, Jan 23, 2005 at 03:10:18PM -0500, h.brunschwig at utoronto.ca wrote:
> Hi R-Help! 
> 
> My question: I have lifetime/failure data of machines with different 
> stress levels and i think an weibull/extreme value distribution would 
> fit this data. So I did: 
> 
> model1      <- survreg(Surv(lfailure)~stress,data=steel,dist="extreme") 
> 
> (where lfailure=log(failure)) 
> 
> Now I would like to do a likelihood ratio test to test the hypothesis 
> 
> H0: location parameters of the extreme value distribution are equal 
> for each stress level. 
> 
> So in order to perform the likelihood ratio test I need the likelood 
> of the model under 
> 
> HA: location parameters are not equal (i.e. each stress level has its 
> slope and intercept). 
> 
> How can I do this? 

Stratify on stress level. 
-- 
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se



From josephine.brennan at ucd.ie  Mon Jan 24 13:55:29 2005
From: josephine.brennan at ucd.ie (Josephine)
Date: Mon, 24 Jan 2005 12:55:29 +0000
Subject: [R] limma "ref"
Message-ID: <001501c50213$fbda7780$d7782b89@YOUR533DA7520E>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050124/7683efa4/attachment.pl

From ripley at stats.ox.ac.uk  Mon Jan 24 14:14:46 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 24 Jan 2005 13:14:46 +0000 (GMT)
Subject: [R] printing PCA scores
In-Reply-To: <1106558267.5364.37.camel@biol102145.oulu.fi>
References: <000001c500d2$1b70b2f0$5006cb84@yourd93e6doqk7>
	<1106558267.5364.37.camel@biol102145.oulu.fi>
Message-ID: <Pine.LNX.4.61.0501241313170.21912@gannet.stats>

On Mon, 24 Jan 2005, Jari Oksanen wrote:

> On Sat, 2005-01-22 at 17:31 -0500, J?r?me Lema?tre wrote:
>> Hey folks,
>>
>> I have an environmental dataset on which I conducted a PCA (prcomp) and I
>> need the scores of this PCA for each site (=each row) to conduct further
>> analyses.
>>
>> Can you please help me with that?
>>
> Did you try help(prcomp) ?
>
> It says that prcomp (may) return an item called 'x':
>
> x: if 'retx' is true the value of the rotated data (the centred
>          (and scaled if requested) data multiplied by the 'rotation'
>          matrix) is returned.
>
> [non-matching parentheses in the original help file]

What `non-matching parentheses'?  Try counting them: two ( and two )!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From rkoenker at uiuc.edu  Mon Jan 24 14:25:07 2005
From: rkoenker at uiuc.edu (roger koenker)
Date: Mon, 24 Jan 2005 07:25:07 -0600
Subject: [R] CIS inquiries
Message-ID: <5D77D996-6E0B-11D9-883E-000A95A7E3AA@uiuc.edu>

Does anyone have an automated way to make Current Index to Statistics
inquiries from R, or from the Unix command line?  I thought it might be
convenient to have something like this for occasions in which I'm in a
foreign domain and would like to make inquires on my office machine
without firing up a full fledged browser.  Lynx is ok for this purpose, 
but it
might be nice to have something more specifically designed for CIS.


url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820



From Christoph.Scherber at uni-jena.de  Mon Jan 24 14:28:44 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Mon, 24 Jan 2005 14:28:44 +0100
Subject: [R] lme and varFunc()
Message-ID: <41F4F80C.5060105@uni-jena.de>

Dear R users,

I am currently analyzing a dataset using lme(). The model I use has the 
following structure:

model<-lme(response~Covariate+TreatmentA+TreatmentB,random=~1|Block/Plot,method="ML")

When I plot the residuals against the fitted values, I see a clear 
positive trend (meaning that the variance increases with the mean).

I tried to solve this issue using weights=varPower(), but it doesn?t 
change the residual plot at all.

How would you implement such a positive trend in the variance? I?ve 
tried glmmPQL (which works great with poisson errors), but using glmmPQL 
I can?t do model simplification.

Many thanks for your help!

Regards
Chris.



From christoph.lehmann at gmx.ch  Mon Jan 24 14:44:34 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Mon, 24 Jan 2005 14:44:34 +0100
Subject: [R] several boxplots or bwplots into one graphic
Message-ID: <41F4FBC2.80303@gmx.ch>

Hi
I have 10 variables and 2 groups. I know how to plot a bwplot for ONE 
var. e.g.

var.a var.b var.c .. GROUP
0.2   0.5   0.2   .. 0
0.3   0.2   0.2   .. 0
..
0.1   0.8   0.7   .. 1
0.5   0.5   0.1   .. 1
..


bwplot(var.a ~ GROUP, data = my.data)

How can I plot 10 bwplots (or boxplots) automatically into one graphic? 
is there any function from lattice which can do this?

thanks for a short hint

christoph



From andy_liaw at merck.com  Mon Jan 24 14:45:54 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 24 Jan 2005 08:45:54 -0500
Subject: [R] error preparing a package for lazy loading with R CMD
Message-ID: <3A822319EB35174CA3714066D590DCD50994E592@usrymx25.merck.com>

Not sure if this will help, but have you tried loading the package after
install with no lazyload?   I've found that if there are syntax errors in
the R source, that can give the problem you described.  Just a guess.

Andy

> From: Patrick Giraudoux H
> 
> Dear Lister,
> 
> I  work with R 2.0.1 and Windows XP, and meet a strange 
> trouble trying to 
> make a R package with a make-package.bat file John Fox has 
> kindly provided 
> (see detailed script below). I am working with it since some 
> months with 
> excellent results (I do'nt use compiled C code so far). Just 
> adding a new 
> function in the R directory today, when running make-package and thus 
> excuting the command ..\..\bin\R CMD build --force --binary 
> --auto-zip 
> %1,  I got the following message after the "compile" stage, 
> preparing the 
> package for lazy loading :
> 
> preparing package pgirmess for lazy loading
> Error in "names<-.default"(`*tmp*`, value =c("R", "Platform", 
> "Date", :
>          names attribute[4] must be the same length as the vector [1]
> Execution halted
> make: ***[lazyload] Error 1
> *** Installation of pgirmess failed ***
> 
> (pgirmess is the package name)
> 
> ... and the zip file is not generated.
> 
> I have checked and rechecked everything during this (long) 
> afternoon... and 
> get nothing except that if I drag out any of the *.r file of 
> the R folder, 
> everything comes to be OK (except the function that has been 
> dragged out is 
> missing...). It looks like if having added one extra function 
> in the R 
> folder on the top of the earlier 32 (+ 2 data frames) makes problem.
> 
> I have then consulted John Fox offlist and he seems quite 
> perplexed "I'm 
> not sure why you're experiencing this problem". On his advise I have 
> included "LazyLoad: no" in the package description file. In this case 
> everything goes smoothly then (except LazyLoad will not be 
> activated), the 
> zip file is generated and the package can be installed from R.
> 
> Has anybody an idea about why a problem occurs when preparing 
> the package 
> for lazy loading? Any remedy?
> 
> Kind regards,
> 
> Patrick
> 
> Make-Package script:
> 
> cd c:\R\rw2001\src\library
> del %1\INDEX
> del %1\data\00Index
> del %1\chm\*.* /Q
> ..\..\bin\R CMD build --force --binary --auto-zip %1
> ..\..\bin\R CMD build --force %1
> ..\..\bin\R CMD check %1
> cd %1.Rcheck
> dvipdfm %1-manual
> notepad 00check.log
> cd ..
> cd ..
> 
> 
> >From: "John Fox" <jfox at mcmaster.ca>
> >To: "'Patrick Giraudoux H'" <patrick.giraudoux at univ-fcomte.fr>
> >Subject: RE: [R] writing a simple package in R 2.0 under Windows XP
> >Date: Sun, 23 Jan 2005 11:41:25 -0500
> >X-Mailer: Microsoft Office Outlook, Build 11.0.6353
> >X-MIME-Autoconverted: from quoted-printable to 8bit by 
> >utinam.univ-fcomte.fr id j0NGfJoD011345
> >
> >Dear Patrick,
> >
> >I'm not sure why you're experiencing this problem.
> >
> >Two suggestions: (1) Since the problem appears to be with 
> preparing the
> >package for lazy loading, try adding the directive 
> "LazyLoad: no" (without
> >the quotes) to the package's DESCRIPTION file. (2) Rather 
> than using my
> >batch file, run the commands one at a time to see exactly 
> where the problem
> >is produced; then you could send a message to r-help.
> >
> >I hope this helps,
> >  John
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From sdavis2 at mail.nih.gov  Mon Jan 24 15:04:13 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Mon, 24 Jan 2005 09:04:13 -0500
Subject: [R] functions not found after installing DBI/RDBI packages
References: <735F04A99D358E468A16EDB64FC04555232B65@EVS1.napier-mail.napier.ac.uk>
Message-ID: <003701c5021d$95c5c300$7d75f345@WATSON>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050124/1918bf61/attachment.pl

From maechler at stat.math.ethz.ch  Mon Jan 24 15:32:33 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 24 Jan 2005 15:32:33 +0100
Subject: [R] an R script editor for Mac
In-Reply-To: <1106346091.5363.16.camel@linux.tlink.de>
References: <HHEDKBCGCMDOHEDELFBCGEMACHAA.jacques.veslot@cirad.fr>
	<24A23A3C-6BA1-11D9-91D9-000D933565E8@mail.nih.gov>
	<1106346091.5363.16.camel@linux.tlink.de>
Message-ID: <16885.1793.289621.552913@stat.math.ethz.ch>

>>>>> "TomH" == Thomas Sch?nhoff <tom_hoary at web.de>
>>>>>     on Fri, 21 Jan 2005 23:21:30 +0100 writes:

    TomH> Hello,
    TomH> Am Freitag, den 21.01.2005, 06:39 -0500 schrieb Sean Davis:
    >> Consider using ESS and xemacs or emacs.  You get syntax highlighting, 
    >> auto-indent, command auto-complete, transcripts for your session, 
    >> integrated help, and the tools of one of the most powerful text editors 
    >> on the planet.  The learning curve is a bit steep, but if you use R 
    >> much, it is worth it.

    TomH> Yes, definitely. I'd like to add that there is an exellent ref-card
    TomH> regarding the Emacs/ESS duo that may smoothing the learning curve a bit!
    TomH> Don't remember the right place to fetch from

>From the ESS page, http://ESS.r-project.org/ 
last line in the list.



From ccleland at optonline.net  Mon Jan 24 15:41:28 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 24 Jan 2005 09:41:28 -0500
Subject: [R] several boxplots or bwplots into one graphic
In-Reply-To: <41F4FBC2.80303@gmx.ch>
References: <41F4FBC2.80303@gmx.ch>
Message-ID: <41F50918.9000606@optonline.net>

   Your variables (var.*) seem to be on the same scale.  How about 
reshaping the data into a univariate layout and then using bwplot as 
follows:

mydata <- data.frame(ID = 1:20, A = runif(20), B = runif(20),
                      C = runif(20), GROUP = rep(c(0,1), c(10,10)))

mydata.uni <- reshape(mydata, varying = list(c("A", "B", "C")),
                       v.names = "Y", timevar = "VAR", times = c("A",
                       "B", "C"), direction = "long")

library(lattice)

bwplot(Y ~ as.factor(GROUP) | VAR, data = mydata.uni, layout=c(3,1,1),
        xlab="Group")

hope this helps,

Chuck Cleland

Christoph Lehmann wrote:
> Hi
> I have 10 variables and 2 groups. I know how to plot a bwplot for ONE 
> var. e.g.
> 
> var.a var.b var.c .. GROUP
> 0.2   0.5   0.2   .. 0
> 0.3   0.2   0.2   .. 0
> ..
> 0.1   0.8   0.7   .. 1
> 0.5   0.5   0.1   .. 1
> ..
> 
> 
> bwplot(var.a ~ GROUP, data = my.data)
> 
> How can I plot 10 bwplots (or boxplots) automatically into one graphic? 
> is there any function from lattice which can do this?
> 
> thanks for a short hint
> 
> christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From R-user at zutt.org  Mon Jan 24 15:50:27 2005
From: R-user at zutt.org (R user)
Date: Mon, 24 Jan 2005 15:50:27 +0100
Subject: [R] parameter couldn't be set in high-level plot() function
Message-ID: <1106578227.29697.51.camel@dutiih.st.ewi.tudelft.nl>



Dear R users,

I am using function bandplot from the gplots package.
To my understanding (viewing the source of bandplot) it calls
function plot (add = FALSE) with the same parameters (except for a few
removed).

I would like to give extra parameters 'xlab' and 'ylab' to function
bandplot, but, as can be seen below, that raises warnings (and the
labels do not show up at the end).

It does work to call title(... xlab="blah", ylab="foo") after bandplot
(), but then I have two labels on top of each other, which is even more
ugly.

Can anyone explain me why this goes wrong?

Thanks in advance,
Jonne.


> x11() ; bandplot(x=xdata, y=zdata)

[works fine]

> x11() ; bandplot(x=xdata, y=zdata, xlab="blah", ylab="foo")
There were 22 warnings (use warnings() to see them)
> warnings()
Warning messages:
1: parameter "xlab" couldn't be set in high-level plot() function
2: parameter "ylab" couldn't be set in high-level plot() function
3: parameter "xlab" couldn't be set in high-level plot() function
4: parameter "ylab" couldn't be set in high-level plot() function
5: parameter "xlab" couldn't be set in high-level plot() function
6: parameter "ylab" couldn't be set in high-level plot() function
7: parameter "xlab" couldn't be set in high-level plot() function
8: parameter "ylab" couldn't be set in high-level plot() function
9: parameter "xlab" couldn't be set in high-level plot() function
10: parameter "ylab" couldn't be set in high-level plot() function
11: parameter "xlab" couldn't be set in high-level plot() function
12: parameter "ylab" couldn't be set in high-level plot() function
13: parameter "xlab" couldn't be set in high-level plot() function
14: parameter "ylab" couldn't be set in high-level plot() function
15: parameter "xlab" couldn't be set in high-level plot() function
16: parameter "ylab" couldn't be set in high-level plot() function
17: parameter "xlab" couldn't be set in high-level plot() function
18: parameter "ylab" couldn't be set in high-level plot() function
19: parameter "xlab" couldn't be set in high-level plot() function
20: parameter "ylab" couldn't be set in high-level plot() function
21: parameter "xlab" couldn't be set in high-level plot() function
22: parameter "ylab" couldn't be set in high-level plot() function
There were 22 warnings (use warnings() to see them)



From parkhurs at ariel.ucs.indiana.edu  Mon Jan 24 15:54:11 2005
From: parkhurs at ariel.ucs.indiana.edu (David Parkhurst)
Date: Mon, 24 Jan 2005 09:54:11 -0500
Subject: [R] How to use "identify" --- Summary
In-Reply-To: <gqn9v0d4dp7uu8u9jbqavr0ir73epfran2@4ax.com>
References: <41F434A4.8090809@ariel.ucs.indiana.edu>
	<gqn9v0d4dp7uu8u9jbqavr0ir73epfran2@4ax.com>
Message-ID: <41F50C13.7090406@ariel.ucs.indiana.edu>

I asked why my call to identify wasn?t working.  Thanks to Petr Pikal, 
Tom Mulholland, Gavin Simpson, and Duncan Murdoch for explaining that I 
had misinterpreted the ?identify help page, and that I needed to feed 
both the x and y vectors in the plot to identify().  It?s working fine 
for me now.

Dave Parkhurst



From vito_ricci at yahoo.com  Mon Jan 24 16:02:28 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Mon, 24 Jan 2005 16:02:28 +0100 (CET)
Subject: [R] R: text miner:
Message-ID: <20050124150228.55832.qmail@web41213.mail.yahoo.com>

See:

http://wwwpeople.unil.ch/jean-pierre.mueller/

ttda - tools for textual data analysis

Regards
Vito

you wrote:

Hi,
Does a text miner exist in R-language similar to Splus
miner or SAS text 
miner?
I would appreciate any information.
TIA,
Aldi

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From piet.vanremortel at gmail.com  Mon Jan 24 16:15:40 2005
From: piet.vanremortel at gmail.com (Piet van Remortel)
Date: Mon, 24 Jan 2005 16:15:40 +0100
Subject: [R] proj() function for lm objects
Message-ID: <CF2BF321-6E1A-11D9-BAA8-000D933A1C82@ua.ac.be>

Dear all,

I'm trying to find a clear explanation of what the 'proj(lm)' function 
produces after having fit a linear model using 'lm'.   I find the help 
page on the proj() function highly unclear (surely part to my limited 
knowledge of statistics).  Can anybody provide a pointer to a clearer 
explanation, preferable containing some examples of the calculations 
involved ?  More particularly, I am interested in the relation between 
the coefficients produced by the 'coef()' function on an 'lm' object, 
and the corresponding projections.

regards,

Piet

--
Piet van Remortel
Intelligent Systems Lab
University of Antwerp
Belgium
http://www.islab.ua.ac.be



From Heather.Turner at warwick.ac.uk  Mon Jan 24 17:38:21 2005
From: Heather.Turner at warwick.ac.uk (Heather Turner)
Date: Mon, 24 Jan 2005 16:38:21 +0000
Subject: [R] using eval() with pre-built expression inside function
Message-ID: <s1f52483.004@liberator.csv.warwick.ac.uk>

I'm trying to evaluate a pre-built expression using eval(), e.g.

dataset <- data.frame(y = runif(30, 50,100), x = gl(5, 6))

# one like this
mf <- expression(model.frame(y~x))
eval(mf, dataset, parent.frame())

# rather than this
eval(expression(model.frame(y~x)), dataset, parent.frame())

In the example above there is no problem, the problem comes when I try to do a similar thing within a function, e.g.

f1 <- function(formula, data) {
    mt <- terms(formula)
    mf <- as.expression(as.call(c(as.name("model.frame"), formula = mt)))
    eval(mf, data, parent.frame())
}

> f1(formula = y ~ x, data = dataset)
Error in eval(expr, envir, enclos) : Object "y" not found

I can get round this by building a call to eval using paste, e.g.

f2 <- function(formula, data) {
    mt <- terms(formula)
    mf <- as.expression(as.call(c(as.name("model.frame"), formula = mt)))
    direct <- parse(text = paste("eval(expression(", mf,
               "), data, parent.frame())"))
    print(direct)
    eval(direct)
}

> f2(formula = y ~ x, data = dataset)
expression(eval(expression(model.frame(formula = y ~ x)), data, 
    parent.frame()))
          y x
1  92.23087 1
2  63.43658 1
3  55.24448 1
4  72.75650 1
5  67.58781 1
...

but this seems rather convoluted. Can anyone explain why f1 doesn't work (when f2 does) and/or suggest a neater way of dealing with this?

Thanks

Heather

Mrs H Turner
Research Assistant
Dept. of Statistics
University of Warwick



From roebuck at odin.mdacc.tmc.edu  Mon Jan 24 17:41:44 2005
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Mon, 24 Jan 2005 10:41:44 -0600 (CST)
Subject: [R] an R script editor for Mac
In-Reply-To: <41F0E634.2040007@sciviews.org>
References: <HHEDKBCGCMDOHEDELFBCGEMACHAA.jacques.veslot@cirad.fr>
	<41F0E634.2040007@sciviews.org>
Message-ID: <Pine.OSF.4.58.0501241031410.451542@odin.mdacc.tmc.edu>

Jacques VESLOT wrote:

> Could someone please make me know if there is a nice script editor available
> under Mac, similar to Crimson, that offers R syntax highlighting (and pairs
> of parentheses underlining) ?

Did you look into BareBone's TextWrangler, 'BBEdit Lite' replacement?
It's now available as free (as in beer) download for 10.3.5+.
<http://www.barebones.com/products/textwrangler/download.shtml>

I created a quick version of the Codeless Language Module for R
but it's lacking at the moment.

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From rpeng at jhsph.edu  Mon Jan 24 17:48:08 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 24 Jan 2005 11:48:08 -0500
Subject: [R] using eval() with pre-built expression inside function
In-Reply-To: <s1f52483.004@liberator.csv.warwick.ac.uk>
References: <s1f52483.004@liberator.csv.warwick.ac.uk>
Message-ID: <41F526C8.3020108@jhsph.edu>

If you look at the beginning of lm(), you'll see that match.call() is 
used and name of the function (in this case "f1") is replaced with 
"model.frame".  Does something like this work?

f1 <- function(formula, data) {
	mf <- match.call(expand.dots = FALSE)
	mf[[1]] <- as.name("model.frame")
	eval(mf, parent.frame())
}

-roger

Heather Turner wrote:
> I'm trying to evaluate a pre-built expression using eval(), e.g.
> 
> dataset <- data.frame(y = runif(30, 50,100), x = gl(5, 6))
> 
> # one like this
> mf <- expression(model.frame(y~x))
> eval(mf, dataset, parent.frame())
> 
> # rather than this
> eval(expression(model.frame(y~x)), dataset, parent.frame())
> 
> In the example above there is no problem, the problem comes when I try to do a similar thing within a function, e.g.
> 
> f1 <- function(formula, data) {
>     mt <- terms(formula)
>     mf <- as.expression(as.call(c(as.name("model.frame"), formula = mt)))
>     eval(mf, data, parent.frame())
> }
> 
> 
>>f1(formula = y ~ x, data = dataset)
> 
> Error in eval(expr, envir, enclos) : Object "y" not found
> 
> I can get round this by building a call to eval using paste, e.g.
> 
> f2 <- function(formula, data) {
>     mt <- terms(formula)
>     mf <- as.expression(as.call(c(as.name("model.frame"), formula = mt)))
>     direct <- parse(text = paste("eval(expression(", mf,
>                "), data, parent.frame())"))
>     print(direct)
>     eval(direct)
> }
> 
> 
>>f2(formula = y ~ x, data = dataset)
> 
> expression(eval(expression(model.frame(formula = y ~ x)), data, 
>     parent.frame()))
>           y x
> 1  92.23087 1
> 2  63.43658 1
> 3  55.24448 1
> 4  72.75650 1
> 5  67.58781 1
> ...
> 
> but this seems rather convoluted. Can anyone explain why f1 doesn't work (when f2 does) and/or suggest a neater way of dealing with this?
> 
> Thanks
> 
> Heather
> 
> Mrs H Turner
> Research Assistant
> Dept. of Statistics
> University of Warwick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From jfox at mcmaster.ca  Mon Jan 24 18:33:08 2005
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 24 Jan 2005 12:33:08 -0500
Subject: [R] proj() function for lm objects
In-Reply-To: <CF2BF321-6E1A-11D9-BAA8-000D933A1C82@ua.ac.be>
Message-ID: <20050124173305.XIAK1567.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Piet,

You'll find more of an explanation in Sec. 5.3.3 of Chambers and Hastie,
eds., Statistical Models in S.

Briefly, proj() computes the orthogonal projections of the response (y)
sequentially onto the subspaces spanned by columns of the model matrix (X)
corresponding to terms in the model. Consequently, in a model in which the
first term is the constant, the first such projection is just the mean of
the response. Suppose the second term has just one degree of freedom; then
the second projection is of y minus its mean onto the second column of X
minus its mean. Each such projection is essentially residualized (both y and
X) for the preceding ones, and therefore the coefficients are for each term
in a sequential set of regressions, not the coefficients of the model as
whole (or of any particular such regression). The last projection is onto
the orthogonal complement of X and thus gives residuals for the model as a
whole. The sum of these projections is y.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Piet 
> van Remortel
> Sent: Monday, January 24, 2005 10:16 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] proj() function for lm objects
> 
> Dear all,
> 
> I'm trying to find a clear explanation of what the 'proj(lm)' 
> function 
> produces after having fit a linear model using 'lm'.   I find 
> the help 
> page on the proj() function highly unclear (surely part to my 
> limited knowledge of statistics).  Can anybody provide a 
> pointer to a clearer explanation, preferable containing some 
> examples of the calculations involved ?  More particularly, I 
> am interested in the relation between the coefficients 
> produced by the 'coef()' function on an 'lm' object, and the 
> corresponding projections.
> 
> regards,
> 
> Piet
> 
> --
> Piet van Remortel
> Intelligent Systems Lab
> University of Antwerp
> Belgium
> http://www.islab.ua.ac.be
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From faheem at email.unc.edu  Mon Jan 24 18:58:15 2005
From: faheem at email.unc.edu (Faheem Mitha)
Date: Mon, 24 Jan 2005 12:58:15 -0500 (EST)
Subject: [R] converting R objects to C types in .Call
Message-ID: <Pine.LNX.4.61.0501241028370.4188@Chrestomanci>


Dear People,

I'm trying to write an R wrapper for a C++ library, using .Call. I've 
never used .Call before. I'm currently having some difficulties converting 
a R character string to a C one.

Here is a little test program.

#include <R.h>
#include <Rinternals.h>
#include <stdio.h>

SEXP testfn(SEXP chstr)
{
   char * charptr = CHAR(chstr);
   printf("%s", charptr);
}

This compiles without problems, but when I try to run this as

> .Call("testfn", "foo")

I get a segmentation fault.

I am sure I am making an obvious mistake, but can someone help me to sort 
it out? Thanks in advance. Please cc me, I'm not subscribed.

                                                              Faheem.



From B.Rowlingson at lancaster.ac.uk  Mon Jan 24 19:20:52 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 24 Jan 2005 18:20:52 +0000
Subject: [R] converting R objects to C types in .Call
In-Reply-To: <Pine.LNX.4.61.0501241028370.4188@Chrestomanci>
References: <Pine.LNX.4.61.0501241028370.4188@Chrestomanci>
Message-ID: <41F53C84.6030705@lancaster.ac.uk>

Faheem Mitha wrote:

> SEXP testfn(SEXP chstr)
> {
>   char * charptr = CHAR(chstr);
>   printf("%s", charptr);
> }
> 

> I am sure I am making an obvious mistake, but can someone help me to 
> sort it out? Thanks in advance. Please cc me, I'm not subscribed.
> 

Firstly, R is expecting an SEXP as a return value! And secondly, your 
SEXP chstr is still a vector - so you need to get an element from it. If 
there's only one element, then that'll be the zeroth element. Here's my 
code:

#include <R.h>
#include <Rinternals.h>
#include <stdio.h>

SEXP testfn(SEXP chstr)
{
   char * charptr = CHAR(VECTOR_ELT(chstr,0));
   printf("%s", charptr);
   return(chstr);
}

  - I'm just returning the same object back in the return() statement, 
and using VECTOR_ELT(chstr,0) to get to the 0'th element.

  SO in R:

  > foo = .Call("testfn","fnord")

should print 'fnord' and return foo as "fnord".

  > foo = .Call("testfn",c("bar","baz"))

will print 'bar' and return foo as c("bar","baz")

Baz



From ripley at stats.ox.ac.uk  Mon Jan 24 19:28:47 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 24 Jan 2005 18:28:47 +0000 (GMT)
Subject: [R] converting R objects to C types in .Call
In-Reply-To: <Pine.LNX.4.61.0501241028370.4188@Chrestomanci>
References: <Pine.LNX.4.61.0501241028370.4188@Chrestomanci>
Message-ID: <Pine.LNX.4.61.0501241825560.5042@gannet.stats>

On Mon, 24 Jan 2005, Faheem Mitha wrote:

> I'm trying to write an R wrapper for a C++ library, using .Call. I've never 
> used .Call before. I'm currently having some difficulties converting a R 
> character string to a C one.

> Here is a little test program.
>
> #include <R.h>
> #include <Rinternals.h>
> #include <stdio.h>
>
> SEXP testfn(SEXP chstr)
> {
>  char * charptr = CHAR(chstr);

CHAR(STRING_ELT(chstr, 0));

First you select the first element of the character vector, then select 
its contents as a C-level character array.

>  printf("%s", charptr);
> }
>
> This compiles without problems, but when I try to run this as
>
>> .Call("testfn", "foo")
>
> I get a segmentation fault.
>
> I am sure I am making an obvious mistake, but can someone help me to sort it 
> out? Thanks in advance. Please cc me, I'm not subscribed.

Please read `Writing R Extensions' for more examples.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From reid_huntsinger at merck.com  Mon Jan 24 19:44:02 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Mon, 24 Jan 2005 13:44:02 -0500
Subject: [R] which.pmin?
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9276@uswpmx00.merck.com>

That's what "ifelse" is for:

ifelse(fpr(b, k.floor) < fpr(b, k.ceiling), k.floor, k.ceiling)

Reid Huntsinger



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Seung Jun
Sent: Friday, January 21, 2005 5:27 PM
To: R-help at stat.math.ethz.ch
Subject: [R] which.pmin?


I have two vectors (k.floor and k.ceiling) of integers of the same
length, and a function (fpr).

    b <- 10:40
    k.floor <- floor(log(2) * b)
    k.ceiling <- ceiling(log(2) * b)
    fpr.floor <- fpr(b, k.floor)
    fpr.ceiling <- fpr(b, k.ceiling)

If R had a element-wise ternary function, I'd like to do something like 
this:

    (fpr.floor < fpr.ceiling) ? k.floor : k.ceiling

That is, I'd like to go through the two vectors in parallel, picking
the one that returns the lower value of fpr. Failing to find such a
function, I wrote the following two lines:

    ind <- sapply(data.frame(rbind(fpr.floor,fpr.ceiling)), which.min)
    opt.k <- cbind(k.floor,k.ceiling)[1:length(ind)+length(ind)*(ind-1)]

opt.k is the vector I want, but I guess I abuse some functions here.
I'd like to ask the experts, What is the proper R-way to do this?

The API should be like "which.pmin(FUN, X, Y, ...)" that returns a
vector of the same length as X (and Y), provided that X, Y, ... have
the same length. Please fill the function body.

Seung

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From faheem at email.unc.edu  Mon Jan 24 20:13:49 2005
From: faheem at email.unc.edu (Faheem Mitha)
Date: Mon, 24 Jan 2005 14:13:49 -0500 (EST)
Subject: [R] converting R objects to C types in .Call
In-Reply-To: <41F53C84.6030705@lancaster.ac.uk>
References: <Pine.LNX.4.61.0501241028370.4188@Chrestomanci>
	<41F53C84.6030705@lancaster.ac.uk>
Message-ID: <Pine.LNX.4.61.0501241344460.29450@Chrestomanci>


Hi Barry,

Thanks for your reply.

On Mon, 24 Jan 2005, Barry Rowlingson wrote:

> Firstly, R is expecting an SEXP as a return value!

Ouch...

I haven't found anything that says this explicitly, but it looks like 
.Call expects a SEXP to be returned to R. At any rate, trying to use void 
instead gives a segfault.

> And secondly, your SEXP chstr is still a vector - so you need to get an 
> element from it. If there's only one element, then that'll be the zeroth 
> element. Here's my code:

> #include <R.h>
> #include <Rinternals.h>
> #include <stdio.h>
>
> SEXP testfn(SEXP chstr)
> {
>  char * charptr = CHAR(VECTOR_ELT(chstr,0));
>  printf("%s", charptr);
>  return(chstr);
> }

> - I'm just returning the same object back in the return() statement, and 
> using VECTOR_ELT(chstr,0) to get to the 0'th element.

> SO in R:
>
> > foo = .Call("testfn","fnord")
>
> should print 'fnord' and return foo as "fnord".
>
> > foo = .Call("testfn",c("bar","baz"))
>
> will print 'bar' and return foo as c("bar","baz")

Thanks for your help. That works fine.                   Faheem.



From andrewr at uidaho.edu  Mon Jan 24 20:24:18 2005
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Tue, 25 Jan 2005 06:24:18 +1100
Subject: [R] lme and varFunc()
In-Reply-To: <41F4F80C.5060105@uni-jena.de>
References: <41F4F80C.5060105@uni-jena.de>
Message-ID: <20050124192418.GE590@uidaho.edu>

Dear Christoph,

what command are you using to plot the residuals?  If you use the
default residuals it will not reflect the variance model.  If you use
the argument

type="p"

then you get the Pearson residuals, which will reflect the weights
model.  Try something like this:

plot(model, resid(., type = "p") ~ fitted(.), abline = 0)

I hope that this helps,

Andrew

On Mon, Jan 24, 2005 at 02:28:44PM +0100, Christoph Scherber wrote:
> Dear R users,
> 
> I am currently analyzing a dataset using lme(). The model I use has the 
> following structure:
> 
> model<-lme(response~Covariate+TreatmentA+TreatmentB,random=~1|Block/Plot,method="ML")
> 
> When I plot the residuals against the fitted values, I see a clear 
> positive trend (meaning that the variance increases with the mean).
> 
> I tried to solve this issue using weights=varPower(), but it doesn?t 
> change the residual plot at all.
> 
> How would you implement such a positive trend in the variance? I?ve 
> tried glmmPQL (which works great with poisson errors), but using glmmPQL 
> I can?t do model simplification.
> 
> Many thanks for your help!
> 
> Regards
> Chris.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From bates at stat.wisc.edu  Mon Jan 24 20:35:26 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 24 Jan 2005 13:35:26 -0600
Subject: [R] Follow-up on nls convergence failure with SSfol
Message-ID: <41F54DFE.9000602@stat.wisc.edu>

A couple of weeks ago there was a question regarding apparent 
convergence in nls when using the SSfol selfStart model for fitting a 
first-order pharmacokinetic model.  I can't manage to find the original 
message either in my archive or in the list archives but the data were

   time  conc dose
   0.50  5.40    1
   0.75 11.10    1
   1.00  8.40    1
   1.25 13.80    1
   1.50 15.50    1
   1.75 18.00    1
   2.00 17.00    1
   2.50 13.90    1
   3.00 11.20    1
   3.50  9.90    1
   4.00  4.70    1
   5.00  5.00    1
   6.00  1.90    1
   7.00  1.90    1
   9.00  1.10    1
  12.00  0.95    1
  14.00  0.46    1

and the attempted fit looked like

 > nls(conc ~ SSfol(dose, time, lKe, lKa, lCl), testdat, trace = 1)
nls(conc ~ SSfol(dose, time, lKe, lKa, lCl), testdat, trace = 1)
99.15824 :  -1.2061792  0.1296156 -4.3020997
86.07567 :  -0.7053265 -0.3873204 -4.1278009
85.19743 :  -0.5548499 -0.5333776 -4.1173627
Error in nls(conc ~ SSfol(dose, time, lKe, lKa, lCl), testdat, trace = 1) :
	step factor 0.000488281 reduced below `minFactor' of 0.000976562

If one allows much smaller step factors you can get more iterations but 
nls still doesn't converge.

It took me a long time to find out why.  The lack of convergence is 
related to the form of the SSfol model.  When the first two parameters 
(lKe and lKa) are equal the model degenerates to a different analytic 
form.  If you were to express the corresponding system of ordinary 
differential equations in the matrix form (as described in Appendix 5 of 
Bates and Watts (1988)), it would be a Schur triangular block and the 
special techniques for nondiagonalizable matrices, described in section 
A5.2 of that appendix, must be used.

The clue here is that the first two parameter values are getting very 
close to each other and this shows up as a singularity.  The fitted 
model is determined by two parameters, not three.



From jcb-bartier at wanadoo.fr  Mon Jan 24 20:52:42 2005
From: jcb-bartier at wanadoo.fr (jcb)
Date: Mon, 24 Jan 2005 20:52:42 +0100
Subject: [R] hist() and database
Message-ID: <41F5520A.9060506@wanadoo.fr>

Hello,

I'm a new R user and I'm having a little trouble getting started. I'm 
hoping
someone can help me out. I read numbers (integer) from a SQL database. 
Some calculations as mean()are possible but hist() give an error. Here 
are the commands:

library(RMySQL)
Loading required package: DBI
 > con <- dbConnect(dbDriver("MySQL"), dbname = "test")
 > dbListTables(con)
[1] "individu"
res <- dbSendQuery(con,"select taille from individu")
 > data <- fetch(res,-1)
 > data
   taille
1     184
2     175
3     189
4     173
5     169
6     175
7     164
8     190
9     174
10    184
11    168
12    179
13    174
14    163
15    175
16    176
17    190
18    172
19    178
20    173
 > mean(data)
taille
176.25
 > hist(data)
Error in hist.default(data) : `x' must be numeric

What can I do to draw the histogram ?

Thanks for your time

JC Bartier



From ligges at statistik.uni-dortmund.de  Mon Jan 24 20:59:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 24 Jan 2005 20:59:45 +0100
Subject: [R] hist() and database
In-Reply-To: <41F5520A.9060506@wanadoo.fr>
References: <41F5520A.9060506@wanadoo.fr>
Message-ID: <41F553B1.7090108@statistik.uni-dortmund.de>

jcb wrote:

> Hello,
> 
> I'm a new R user and I'm having a little trouble getting started. I'm 
> hoping
> someone can help me out. I read numbers (integer) from a SQL database. 
> Some calculations as mean()are possible but hist() give an error. Here 
> are the commands:
> 
> library(RMySQL)
> Loading required package: DBI
>  > con <- dbConnect(dbDriver("MySQL"), dbname = "test")
>  > dbListTables(con)
> [1] "individu"
> res <- dbSendQuery(con,"select taille from individu")
>  > data <- fetch(res,-1)
>  > data
>   taille
> 1     184
> 2     175
> 3     189
> 4     173
> 5     169
> 6     175
> 7     164
> 8     190
> 9     174
> 10    184
> 11    168
> 12    179
> 13    174
> 14    163
> 15    175
> 16    176
> 17    190
> 18    172
> 19    178
> 20    173
>  > mean(data)
> taille
> 176.25
>  > hist(data)
> Error in hist.default(data) : `x' must be numeric
> 
> What can I do to draw the histogram ?

data is a data.frame, instead use:
   hist(data$taille)

Uwe Ligges


> Thanks for your time
> 
> JC Bartier
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From drew.balazs at gmail.com  Mon Jan 24 21:02:50 2005
From: drew.balazs at gmail.com (Drew Balazs)
Date: Mon, 24 Jan 2005 14:02:50 -0600
Subject: [R] R 2.0.1 and Rggobi install issues on windows XP
Message-ID: <6a2c704c05012412023c62cdba@mail.gmail.com>

Has anyone else ran into problems installing Rggobi with R 2.0.1 on a
windows platform? I've followed all the instructions available and I
still can not get R to recognize Rggobi as a library (package). I

I've already emailed Duncan Temple Lange at ggobi.org and bell labs
and both emails have bounced back.

Any help would be greatly appreciated.


-Drew Balazs



From goedman at mac.com  Mon Jan 24 21:43:06 2005
From: goedman at mac.com (Rob J Goedman)
Date: Mon, 24 Jan 2005 12:43:06 -0800
Subject: [R] an R script editor for Mac
In-Reply-To: <Pine.OSF.4.58.0501241031410.451542@odin.mdacc.tmc.edu>
References: <HHEDKBCGCMDOHEDELFBCGEMACHAA.jacques.veslot@cirad.fr>
	<41F0E634.2040007@sciviews.org>
	<Pine.OSF.4.58.0501241031410.451542@odin.mdacc.tmc.edu>
Message-ID: <8D2EF608-6E48-11D9-BF71-000A95C478FE@mac.com>

Hi,

There is also SubEthaEdit, quite an elegant editor with R/S syntax 
highlighting
as 1 of the many available modes.

With AppleScript its easy to sent the edit window, a file, a selection 
etc. to R for
execution.

http://www.codingmonkeys.de/subethaedit/

Rob

On Jan 24, 2005, at 8:41 AM, Paul Roebuck wrote:

> Jacques VESLOT wrote:
>
>> Could someone please make me know if there is a nice script editor 
>> available
>> under Mac, similar to Crimson, that offers R syntax highlighting (and 
>> pairs
>> of parentheses underlining) ?
>
> Did you look into BareBone's TextWrangler, 'BBEdit Lite' replacement?
> It's now available as free (as in beer) download for 10.3.5+.
> <http://www.barebones.com/products/textwrangler/download.shtml>
>
> I created a quick version of the Codeless Language Module for R
> but it's lacking at the moment.
>
> ----------------------------------------------------------
> SIGSIG -- signature too long (core dumped)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jerk_alert at hotmail.com  Mon Jan 24 21:51:34 2005
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Mon, 24 Jan 2005 20:51:34 +0000
Subject: [R] Deleted objects keep coming back
Message-ID: <BAY101-F308DA907659FDB2988FE3DE8850@phx.gbl>

Having a very strange and frustrating problem with v2.0.1 under Mac OSX 
10.3. I have several closely related workspaces in the same directory that I 
need to keep separate from one another (there are a few objects common to 
all workspaces, and then there are other objects that store the results of 
different analyses on the common objects, which is why I need separate 
workspaces).

When I attempted to delete objects with rm() and then re-save the workspace, 
after quitting and restarting R, reloading the workspace, the objects were 
still present in the workspace (even though I'd deleted them).

So I deleted the objects again with rm() and also called gc(), and saved the 
workspace with a new name. I then quit R and reloaded the workspaces, and 
that seemed to work. The deleted objects stayed deleted.

But then an hour later, I came back and reloaded these workspaces, and the 
old objects which I'd deleted had come back again. This is really annoying 
and I feel like I'm losing my mind! I'm concerned that R is not accessing 
the directory properly, and may be reading one workspace file in place of 
another. This would completely jeopardize my work.

Please help!
Thanks in advance,
Ken



From andy_liaw at merck.com  Mon Jan 24 22:04:41 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 24 Jan 2005 16:04:41 -0500
Subject: [R] Deleted objects keep coming back
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5A1@usrymx25.merck.com>

> From: Ken Termiso
> 
> Having a very strange and frustrating problem with v2.0.1 
> under Mac OSX 
> 10.3. I have several closely related workspaces in the same 
> directory that I 
> need to keep separate from one another (there are a few 
> objects common to 
> all workspaces, and then there are other objects that store 
> the results of 
> different analyses on the common objects, which is why I need 
> separate 
> workspaces).
> 
> When I attempted to delete objects with rm() and then re-save 
> the workspace, 
> after quitting and restarting R, reloading the workspace, the 
> objects were 
> still present in the workspace (even though I'd deleted them).
> 
> So I deleted the objects again with rm() and also called 
> gc(), and saved the 
> workspace with a new name. I then quit R and reloaded the 
> workspaces, and 
> that seemed to work. The deleted objects stayed deleted.
> 
> But then an hour later, I came back and reloaded these 
> workspaces, and the 
> old objects which I'd deleted had come back again. This is 
> really annoying 
> and I feel like I'm losing my mind! I'm concerned that R is 
> not accessing 
> the directory properly, and may be reading one workspace file 
> in place of 
> another. This would completely jeopardize my work.
> 
> Please help!
> Thanks in advance,
> Ken

Are you sure the objects that won't go away are actually in the workspace
you think they're in?  What are the exact commands you used to check?  How
did you load the workspaces (the commands, please)?  Please show the exact
sequence of what you did and what you saw.

Andy



From cliff at ms.washington.edu  Mon Jan 24 22:23:24 2005
From: cliff at ms.washington.edu (Cliff Lunneborg)
Date: Mon, 24 Jan 2005 13:23:24 -0800
Subject: [R] Re:[ R] Wilcoxon test for mixed deisgn
Message-ID: <005001c5025a$f17472e0$6401a8c0@C56909A>

Marco Chirandini writes:

"is there any extension of the pairwise Wilcoxon test to a dependent
samples layout with replicates (or, in other terms, a one-way layout
with blocking and replicates)?

The Wilcoxon method with matched pairs works for the case of dependent
samples with one observation per block, while the Mann-Whitney test
works for independent samples, thus one single block and replicated
observations. Is there a method which mixes this two cases considering a
depedent sample design in which each block has more than one
observation? I know it exists a Friedman test for this case but in the
Friedman test ranks are constructed considering all subjects jointly,
while in Wilcoxon only the pair of subject currently considered are
ranked, thus resulting in a more powerful test."

It is my understanding that the Friedman test ranks responses within
each block, independently from block to block.

friedman.test() is an R function.

**********************************************************
Cliff Lunneborg, Professor Emeritus, Statistics &
Psychology, University of Washington, Seattle
cliff at ms.washington.edu



From dalexander at ccesearch.com  Mon Jan 24 23:22:55 2005
From: dalexander at ccesearch.com (Don Alexander)
Date: Mon, 24 Jan 2005 17:22:55 -0500
Subject: [R] Job Opportunity: Senior Statistician CC 083
Message-ID: <012301c50263$40dc3b80$4a01a8c0@D3F82361>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050124/cf20d42c/attachment.pl

From Paul.Sorenson at vision-bio.com  Mon Jan 24 23:34:59 2005
From: Paul.Sorenson at vision-bio.com (Paul Sorenson)
Date: Tue, 25 Jan 2005 09:34:59 +1100
Subject: [R] lookups and joins
Message-ID: <5E06BFED29594F4C9C5EBE230DE320C6068027BB@ewok.vsl.com.au>

I have some data coming from SQL sources that I wish to relate in various ways.  For reasons only known to our IT people, this can't be done in SQL at present.

I am looking for an R'ish technique for looking up new columns on a data frame.  As a simple, hardwired example I have tried the following:

# This gives me two columns, one the lookup value and the second one
# the result column, ie my lookup table.
stcl = read.csv("stockclass.csv")
stockclass = as.vector(stcl$stock_class)
# This gives me what appears to be a dictionary or map
names(stockclass) = as.vector(stcl$stock_group)

getstockclass = function(stock_group) {
	try(stockclass[[stock_group]], TRUE)
}
csg$stk_class=factor(sapply(csg$stock_group, getstockclass))

I need the try since if there is a missing value I get an exception.

I also tried something along the lines of (from memory):
getstockclass = function(stock_group) {
	stcl[which(stcl$stock_group == stock_group),]$stock_class
}

These work but I just wanted to check if there was an inbuilt way to do this kind of thing in R?  I searched on "join" without much luck.

Really what I would like is a generic function that:
	- Takes 2 data frames,
	- Some kind of specification on which column(s) to join
	- Outputs the joined frames, or perhaps a vector which is an index vector that I can use on the second data frame.

I don't really want to reinvent SQL and my data sets are not huge.

cheers



From michael.watson at bbsrc.ac.uk  Mon Jan 24 23:41:54 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Mon, 24 Jan 2005 22:41:54 -0000
Subject: [R] lookups and joins
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B977@iahce2knas1.iah.bbsrc.reserved>

All together now:

?merge

:-)


-----Original Message-----
From:	r-help-bounces at stat.math.ethz.ch on behalf of Paul Sorenson
Sent:	Mon 1/24/2005 10:34 PM
To:	r-help at stat.math.ethz.ch
Cc:	
Subject:	[R] lookups and joins
I have some data coming from SQL sources that I wish to relate in various ways.  For reasons only known to our IT people, this can't be done in SQL at present.

I am looking for an R'ish technique for looking up new columns on a data frame.  As a simple, hardwired example I have tried the following:

# This gives me two columns, one the lookup value and the second one
# the result column, ie my lookup table.
stcl = read.csv("stockclass.csv")
stockclass = as.vector(stcl$stock_class)
# This gives me what appears to be a dictionary or map
names(stockclass) = as.vector(stcl$stock_group)

getstockclass = function(stock_group) {
	try(stockclass[[stock_group]], TRUE)
}
csg$stk_class=factor(sapply(csg$stock_group, getstockclass))

I need the try since if there is a missing value I get an exception.

I also tried something along the lines of (from memory):
getstockclass = function(stock_group) {
	stcl[which(stcl$stock_group == stock_group),]$stock_class
}

These work but I just wanted to check if there was an inbuilt way to do this kind of thing in R?  I searched on "join" without much luck.

Really what I would like is a generic function that:
	- Takes 2 data frames,
	- Some kind of specification on which column(s) to join
	- Outputs the joined frames, or perhaps a vector which is an index vector that I can use on the second data frame.

I don't really want to reinvent SQL and my data sets are not huge.

cheers

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From r.muttiah at tcu.edu  Mon Jan 24 23:57:16 2005
From: r.muttiah at tcu.edu (Ranjan S. Muttiah)
Date: Mon, 24 Jan 2005 16:57:16 -0600
Subject: [R] Weighted.mean(x,wt) vs. t(x) %*% wt
Message-ID: <ICEJLDHMPIJKCOGJOCDAEEDICJAA.r.muttiah@tcu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050124/ddbe1fd8/attachment.pl

From p.murrell at auckland.ac.nz  Tue Jan 25 01:40:15 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 25 Jan 2005 13:40:15 +1300
Subject: [R] animation without intermediate files?
References: <Pine.LNX.4.44.0501061548270.27024-100000@panic.stat.cmu.edu>
Message-ID: <41F5956F.8010101@stat.auckland.ac.nz>

Hi


Cari G Kaufman wrote:
> Hello, 
> 
> Does anyone know how to make "movies" in R by making a sequence of plots?  
> I'd like to animate a long trajectory for exploratory purposes only,
> without creating a bunch of image files and then using another program to
> string them together.  In Splus I would do this using double.buffer()  to
> eliminate the flickering caused by replotting. For instance, with a 2-D
> trajectory in vectors x and y I would use the following:
> 
> motif()
> double.buffer("back")
> for (i in 1:length(x)) {
>   plot(x[i], y[i], xlim=range(x), ylim=range(y))
>   double.buffer("copy")
> }
> double.buffer("front")
> 
> I haven't found an equivalent function to double.buffer in R.  I tried
> playing around with dev.set() and dev.copy() but so far with no success
> (still flickers).


Double buffering is only currently an option on the Windows graphics 
device (and there it is "on" by default).  So something like ...

x <- rnorm(100)
for (i in 1:100)
   plot(1:i, x[1:i], xlim=c(0, 100), ylim=c(-4, 4), pch=16, cex=2)

is already "smooth"

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From p.murrell at auckland.ac.nz  Tue Jan 25 01:44:37 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 25 Jan 2005 13:44:37 +1300
Subject: [R] some questions about font
References: <41F14789.7020808@northwestern.edu>
Message-ID: <41F59675.40205@stat.auckland.ac.nz>

Hi


Bobai Li wrote:
> 
> Hi,
> 
> I have been using R to create some mathematical and statistical graphs 
> for a book manuscript, but I got some problems:
> 
> 1)  Some web positngs said that default typeface for math expressions is 
> italic, but in my system (R 2.01 on WinXP), the default is regular font.
> How can I change the default to ilatic?


expression(italic(whatever))


> 2)  When use ComputerModern font,  (i.e., 
> family=c("CM_regular_10.afm","CM_boldx_10.afm","cmti10.afm","cmbxti10.afm","CM_symbol_10.afm") 
> ), some accented symbols are not available. For example, 
> "expression(hat(beta))" will produce warning message like "font metrics 
> unknown for character 94."


That appears to be a bug (that requires changes to the PostScript device 
driver).  A nastyish workaround for hat(beta) is widehat(beta), but I 
suspect there are other problems that this will not solve.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From h.brunschwig at utoronto.ca  Tue Jan 25 01:46:03 2005
From: h.brunschwig at utoronto.ca (h.brunschwig@utoronto.ca)
Date: Mon, 24 Jan 2005 19:46:03 -0500
Subject: [R] survreg: fitting different location parameters
In-Reply-To: <20050124124909.GA12334@tal.stat.umu.se>
References: <1106511018.41f404aab2411@webmail.utoronto.ca>
	<20050124124909.GA12334@tal.stat.umu.se>
Message-ID: <1106613963.41f596cb647f7@webmail.utoronto.ca>

I am still trying to find a common intercept but a different slopes for each 
group within my lifetime data. By stratifying the variable stress (the groups) 
I get different scale parameters which is not my goal.
So I did this:

survreg(Surv(lfailure)~as.factor(stress),data=steel,dist="extreme")

and I did get different slopes but common intercepts and scales.
Is this correct? I am a bit unsure as I did it with a different software and 
got different estimates...

Thanks for any help

Hadassa





Quoting G?ran Brostr?m <gb at tal.stat.umu.se>:

> On Sun, Jan 23, 2005 at 03:10:18PM -0500, h.brunschwig at utoronto.ca wrote:
> > Hi R-Help! 
> > 
> > My question: I have lifetime/failure data of machines with different 
> > stress levels and i think an weibull/extreme value distribution would 
> > fit this data. So I did: 
> > 
> > model1      <- survreg(Surv(lfailure)~stress,data=steel,dist="extreme") 
> > 
> > (where lfailure=log(failure)) 
> > 
> > Now I would like to do a likelihood ratio test to test the hypothesis 
> > 
> > H0: location parameters of the extreme value distribution are equal 
> > for each stress level. 
> > 
> > So in order to perform the likelihood ratio test I need the likelood 
> > of the model under 
> > 
> > HA: location parameters are not equal (i.e. each stress level has its 
> > slope and intercept). 
> > 
> > How can I do this? 
> 
> Stratify on stress level. 
> -- 
>  G?ran Brostr?m                    tel: +46 90 786 5223
>  Department of Statistics          fax: +46 90 786 6614
>  Ume? University                   http://www.stat.umu.se/egna/gb/
>  SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se
>



From andy_liaw at merck.com  Tue Jan 25 02:40:58 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 24 Jan 2005 20:40:58 -0500
Subject: [R] Weighted.mean(x,wt) vs. t(x) %*% wt
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5A3@usrymx25.merck.com>

Just look at the code:

> weighted.mean
function (x, w, na.rm = FALSE) 
{
    if (missing(w)) 
        w <- rep.int(1, length(x))
    if (is.integer(w)) 
        w <- as.numeric(w)
    if (na.rm) {
        w <- w[i <- !is.na(x)]
        x <- x[i]
    }
    sum(x * w)/sum(w)
}
<environment: namespace:stats>

So the differences are:

- missing values handling
- weight normalization 
- the difference between t(x) %*% w and sum(x * w) (I'd say the latter is
more efficient)

Here's an example:

> x <- rnorm(5e6)
> w <- runif(x)
> w <- w / sum(w)
> system.time(sum(x * w), gcFirst=T)
[1] 0.17 0.03 0.20   NA   NA
> system.time(s1 <- sum(x * w), gcFirst=T)
[1] 0.19 0.01 0.20   NA   NA
> system.time(s2 <- t(x) %*% w, gcFirst=T)
[1] 0.30 0.01 0.33   NA   NA
> system.time(s3 <- crossprod(x, w), gcFirst=T)
[1] 0.04 0.00 0.04   NA   NA
> c(s1, s2, s3)
[1] -0.0008922782 -0.0008922782 -0.0008922782

[This is w/o using an optimized BLAS.  With an optimized BLAS, the two
latter ones might be significantly faster than what's seen here.]

Andy


> From: Ranjan S. Muttiah
> 
> What is the difference between the above two operations ?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From msck9 at mizzou.edu  Tue Jan 25 03:02:51 2005
From: msck9 at mizzou.edu (msck9@mizzou.edu)
Date: Mon, 24 Jan 2005 20:02:51 -0600
Subject: [R] Related with "kmeans"
Message-ID: <20050125020251.GA8311@localhost>

Hi, 
 I am new here. 
 I used y<-kmeans(x, 2)(x is a vector) to do the cluster analysis. I get 
 y has $cluster, $center etc. I need to pull out the data belong to
 cluster 1 and cluster 2. How can I do that? 

 Thanks,
 Ming



From andy_liaw at merck.com  Tue Jan 25 03:14:18 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 24 Jan 2005 21:14:18 -0500
Subject: [R] Related with "kmeans"
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5A5@usrymx25.merck.com>

> From: msck9 at mizzou.edu
> 
> Hi, 
>  I am new here. 
>  I used y<-kmeans(x, 2)(x is a vector) to do the cluster 
> analysis. I get 
>  y has $cluster, $center etc. I need to pull out the data belong to
>  cluster 1 and cluster 2. How can I do that? 

Here's an example:

> cl <- round(runif(20))
> x <- rnorm(20, mean=cl)
> km <- kmeans(x, 2)
> x[km$cluster == 1]
 [1] -1.739624361 -0.046792299 -0.309680321  0.001928340 -1.036573237
-1.875550649
 [7] -2.276369358 -0.453359967 -1.500753548 -1.864152516
> x[km$cluster == 2]
 [1] 0.6467709 1.9746519 1.9463621 2.9971197 2.8290800 0.8533460 0.6175901
2.2296458
 [9] 0.6616395 1.5822679

Andy
 
>  Thanks,
>  Ming
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From gkoru2000 at yahoo.com  Tue Jan 25 03:22:03 2005
From: gkoru2000 at yahoo.com (A. Gunes Koru)
Date: Mon, 24 Jan 2005 18:22:03 -0800 (PST)
Subject: [R] Estimating error rate for a classification tree
Message-ID: <20050125022203.25280.qmail@web53408.mail.yahoo.com>

Hi,

I created an rpart object and pruned the tree using
1-SE rule. I used 10-fold cross validation while
creating the tree. Then, I extracted the
cross-validated predictions for my data points using
xpred.rpart and obtained some statistics like
precision, recall, overall error rate, etc.

However, these values change each time I run
xpred.rpart because of the random shuffling going on
before cross validation (I think so). What should I do
in this case? I am inclined to treat them as random
variables with normal distribution. So, when I have,
say 100 runs, i can say something about the mean with
some confidence interval.

However, I also doubt that these subsequent runs may
not be independent from each other. I would highly
appreciate if someone could make a suggestion.

Best regards



From bostonknot at yahoo.com  Tue Jan 25 05:50:53 2005
From: bostonknot at yahoo.com (boston knot)
Date: Mon, 24 Jan 2005 20:50:53 -0800 (PST)
Subject: [R] Recursive default argument reference
Message-ID: <20050125045053.85928.qmail@web42004.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050124/2d77569f/attachment.pl

From wgshi2001 at yahoo.ca  Tue Jan 25 06:11:26 2005
From: wgshi2001 at yahoo.ca (Weiguang Shi)
Date: Tue, 25 Jan 2005 00:11:26 -0500 (EST)
Subject: [R] agglomerative coefficient in agnes (cluster)
Message-ID: <20050125051127.10888.qmail@web30002.mail.mud.yahoo.com>

I haven't read the book, but could anyone explain more
about this parameter? 

help(agnes) says that ac measures the amount of 
clustering structure found. From the definition given
in help(agnes.object), however, it seems that as long
as 
the dissimilarity of the merger in the final step of
the
algorithm is large enough, the ac value will be close
to 
1. So what does ac really mean?

Thank you,
Weiguang



From wgshi2001 at yahoo.ca  Tue Jan 25 06:15:18 2005
From: wgshi2001 at yahoo.ca (Weiguang Shi)
Date: Tue, 25 Jan 2005 00:15:18 -0500 (EST)
Subject: [R] Zipf random number generation
Message-ID: <20050125051518.69271.qmail@web30003.mail.mud.yahoo.com>

Hi,

Is there a Zipf-like distribution RNG in R?

Thanks,
Weiguang



From patrick.giraudoux at univ-fcomte.fr  Tue Jan 25 06:24:43 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux H)
Date: Tue, 25 Jan 2005 06:24:43 +0100
Subject: [R] error preparing a package for lazy loading with R CMD
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E592@usrymx25.merck.co
 m>
Message-ID: <5.0.2.1.2.20050125062113.02c11ec0@utinam.univ-fcomte.fr>

That's the way John Fox advised to turn the problem. Indeed it works but 
doe snot explain this bug in lazy loading. I don't think that it may come 
for a syntax error somewhere. All the functions have been checked and the 
problem does not occur if any of the 35 functions is removed. Something 
happens when I add another function to the 34 already present. Strange.


A 08:45 24/01/2005 -0500, vous avez ?crit :
>Not sure if this will help, but have you tried loading the package after
>install with no lazyload?   I've found that if there are syntax errors in
>the R source, that can give the problem you described.  Just a guess.
>
>Andy
>
> > From: Patrick Giraudoux H
> >
> > Dear Lister,
> >
> > I  work with R 2.0.1 and Windows XP, and meet a strange
> > trouble trying to
> > make a R package with a make-package.bat file John Fox has
> > kindly provided
> > (see detailed script below). I am working with it since some
> > months with
> > excellent results (I do'nt use compiled C code so far). Just
> > adding a new
> > function in the R directory today, when running make-package and thus
> > excuting the command ..\..\bin\R CMD build --force --binary
> > --auto-zip
> > %1,  I got the following message after the "compile" stage,
> > preparing the
> > package for lazy loading :
> >
> > preparing package pgirmess for lazy loading
> > Error in "names<-.default"(`*tmp*`, value =c("R", "Platform",
> > "Date", :
> >          names attribute[4] must be the same length as the vector [1]
> > Execution halted
> > make: ***[lazyload] Error 1
> > *** Installation of pgirmess failed ***
> >
> > (pgirmess is the package name)
> >
> > ... and the zip file is not generated.
> >
> > I have checked and rechecked everything during this (long)
> > afternoon... and
> > get nothing except that if I drag out any of the *.r file of
> > the R folder,
> > everything comes to be OK (except the function that has been
> > dragged out is
> > missing...). It looks like if having added one extra function
> > in the R
> > folder on the top of the earlier 32 (+ 2 data frames) makes problem.
> >
> > I have then consulted John Fox offlist and he seems quite
> > perplexed "I'm
> > not sure why you're experiencing this problem". On his advise I have
> > included "LazyLoad: no" in the package description file. In this case
> > everything goes smoothly then (except LazyLoad will not be
> > activated), the
> > zip file is generated and the package can be installed from R.
> >
> > Has anybody an idea about why a problem occurs when preparing
> > the package
> > for lazy loading? Any remedy?
> >
> > Kind regards,
> >
> > Patrick
> >
> > Make-Package script:
> >
> > cd c:\R\rw2001\src\library
> > del %1\INDEX
> > del %1\data\00Index
> > del %1\chm\*.* /Q
> > ..\..\bin\R CMD build --force --binary --auto-zip %1
> > ..\..\bin\R CMD build --force %1
> > ..\..\bin\R CMD check %1
> > cd %1.Rcheck
> > dvipdfm %1-manual
> > notepad 00check.log
> > cd ..
> > cd ..
> >
> >
> > >From: "John Fox" <jfox at mcmaster.ca>
> > >To: "'Patrick Giraudoux H'" <patrick.giraudoux at univ-fcomte.fr>
> > >Subject: RE: [R] writing a simple package in R 2.0 under Windows XP
> > >Date: Sun, 23 Jan 2005 11:41:25 -0500
> > >X-Mailer: Microsoft Office Outlook, Build 11.0.6353
> > >X-MIME-Autoconverted: from quoted-printable to 8bit by
> > >utinam.univ-fcomte.fr id j0NGfJoD011345
> > >
> > >Dear Patrick,
> > >
> > >I'm not sure why you're experiencing this problem.
> > >
> > >Two suggestions: (1) Since the problem appears to be with
> > preparing the
> > >package for lazy loading, try adding the directive
> > "LazyLoad: no" (without
> > >the quotes) to the package's DESCRIPTION file. (2) Rather
> > than using my
> > >batch file, run the commands one at a time to see exactly
> > where the problem
> > >is produced; then you could send a message to r-help.
> > >
> > >I hope this helps,
> > >  John
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>
>
>------------------------------------------------------------------------------
>Notice:  This e-mail message, together with any attachments, contains 
>information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New 
>Jersey, USA 08889), and/or its affiliates (which may be known outside the 
>United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
>Banyu) that may be confidential, proprietary copyrighted and/or legally 
>privileged. It is intended solely for the use of the individual or entity 
>named on this message.  If you are not the intended recipient, and have 
>received this message in error, please notify us immediately by reply 
>e-mail and then delete it from your system.
>------------------------------------------------------------------------------ 
>



From petr.pikal at precheza.cz  Tue Jan 25 07:47:38 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 25 Jan 2005 07:47:38 +0100
Subject: [R] several boxplots or bwplots into one graphic
In-Reply-To: <41F4FBC2.80303@gmx.ch>
Message-ID: <41F5F99A.4365.B04AD@localhost>



On 24 Jan 2005 at 14:44, Christoph Lehmann wrote:

> Hi
> I have 10 variables and 2 groups. I know how to plot a bwplot for ONE
> var. e.g.
> 
> var.a var.b var.c .. GROUP
> 0.2   0.5   0.2   .. 0
> 0.3   0.2   0.2   .. 0
> ..
> 0.1   0.8   0.7   .. 1
> 0.5   0.5   0.1   .. 1
> ..
> 
> 
> bwplot(var.a ~ GROUP, data = my.data)
> 
> How can I plot 10 bwplots (or boxplots) automatically into one
> graphic? is there any function from lattice which can do this?

Hallo

It probably depends on how you want to organise your boxplots. You 
can reformat your table to long format to get second grouping 
(a,b,...) by reshape (or reShape from Hmisc) and than use interaction 
and boxplot or bwplot(Group~var|second.grouping)

Cheers
Petr


> 
> thanks for a short hint
> 
> christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From Lorenz.Gygax at fat.admin.ch  Tue Jan 25 08:01:41 2005
From: Lorenz.Gygax at fat.admin.ch (Lorenz.Gygax@fat.admin.ch)
Date: Tue, 25 Jan 2005 08:01:41 +0100
Subject: [R] lme and varFunc()
Message-ID: <BF74FADD4B44554CA7E53D0B5242CD6A01FC645A@evd-s7014.evd.admin.ch>

> I am currently analyzing a dataset using lme(). The model I 
> use has the following structure:
> 
> model<-lme(response~Covariate+TreatmentA+TreatmentB,
>            random=~1|Block/Plot,method="ML")
> 
> When I plot the residuals against the fitted values, I see a clear 
> positive trend (meaning that the variance increases with the mean).
> 
> I tried to solve this issue using weights=varPower(), but it
> doesn?t change the residual plot at all.

You are aware that you need to use something like 

weigths= varPower (form= fitted (.))

and the plot residuals using e.g.

scatter.smooth (fitted (model), resid (model, type= 'n'))

Maybe the latter should also be ok with the default pearson residuals, but I
am not sure.

Possibly a look into the following would help?

@Book{Pin:00a,
  author = 	 {Pinheiro, Jose C and Bates, Douglas M},
  title = 	 {Mixed-Effects Models in {S} and {S}-{P}{L}{U}{S}},
  publisher = 	 {Springer},
  year = 	 {2000},
  address = 	 {New York}
}

> How would you implement such a positive trend in the variance? I?ve 
> tried glmmPQL (which works great with poisson errors), but 
> using glmmPQL I can?t do model simplification.

Well, what error distribution do you have / do you expect?

Regards, Lorenz
- 
Lorenz Gygax, Dr. sc. nat.
Centre for proper housing of ruminants and pigs
Swiss Federal Veterinary Office



From cuiczhao at yahoo.com  Tue Jan 25 08:25:27 2005
From: cuiczhao at yahoo.com (Cuichang Zhao)
Date: Mon, 24 Jan 2005 23:25:27 -0800 (PST)
Subject: [R] more question
Message-ID: <20050125072527.23695.qmail@web30707.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050124/bd543f2e/attachment.pl

From ligges at statistik.uni-dortmund.de  Tue Jan 25 09:00:19 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 25 Jan 2005 09:00:19 +0100
Subject: [R] more question
In-Reply-To: <20050125072527.23695.qmail@web30707.mail.mud.yahoo.com>
References: <20050125072527.23695.qmail@web30707.mail.mud.yahoo.com>
Message-ID: <41F5FC93.1030309@statistik.uni-dortmund.de>

Cuichang Zhao wrote:
> Hello, 
> thank you very much for your help in last email. it is very helpful. right now, i have more questions about my project,
>  
> 1. solve can i remove the NA from a vectors:
> for exmample, if my vector is:
> v <- (NA, 1, 2, 3, 4, 5)
> how can I remove the NA from vector v
>  
> 2. how I can get input from the user?
>  
> 3. can I read more than one data files in one program? and how i can write something to a file
>  
> 4. how i can display/output something on the screen?
>  
> thank you so much
>  
> Best wish
>  
> C-Ming
>  
> Jan 24, 2005
>  
>  
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Please do a suggested above: Read the posting guide and read a minimal 
amount of documentation, as well as the R FAQ and learn to search the 
mailing list archives.

Uwe Ligges



From bxc at steno.dk  Tue Jan 25 09:01:46 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Tue, 25 Jan 2005 09:01:46 +0100
Subject: [R] COURSE: Statistical practice in Epidemiology with R
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FE6C2@exdkba022.novo.dk>

Course in
STATISTICAL PRACTICE IN EPIDEMIOLOGY USING R
====================================================
Tartu, Estonia, 26 - 31 May 2005

The course is aimed at epidemiologists and statisticians who wish to
use R for statistical modelling and analysis of epidemiological data.
The course requires basic knowledge of epidemiological concepts and
study types. These will only be briefly reviewed, whereas the more
advanced epidemiological and statistical concepts will be treated in
depth.

Contents:
---------
History of R. Language. Objects. Functions. 
Interface to other dataformats. Dataframes. 
Classical methods: Mantel-Haenszel etc.
Tabulation of data. 
Logistic regression for case-control-studies. 
Poisson regression for follow-up studies. 
Parametrization of models. 
Graphics in R. 
Graphical reporting of results. 
Time-splitting & SMR. 
Survival analysis in continuous time. 
Parametric survival models. 
Interval censoring. 
Nested and matched case-control studies. 
Case-cohort studies. 
Competing risk models. 
Multistage models. 
Bootstrap and simulation.
Causal inference. 

The methods will be illustrated using R in practical exercises.  

The Epi package which is under development for epidemiological
analysis in R will be introduced.

Participants are required to have a fairly good understanding of
statistical principles and some familiarity with epidemiological
concepts. The course will be mainly practically oriented with more
than half the time at the computer.

Price: 500 EUR. (250 EUR for cuntries outside EU-2003 and the like).

Application deadline: 1 April 2005.

Further information at: www.pubhealth.ku.dk/~bxc/SPE

------------------------------------------------------
Krista Fischer, University of Tartu, Estonia
Esa L??r?, University of Oulu, Finland
Bendix Carstensen, Steno Diabetes Center, Denmark 

Organizers



From hb at maths.lth.se  Tue Jan 25 09:05:02 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 25 Jan 2005 09:05:02 +0100
Subject: [R] error preparing a package for lazy loading with R CMD
In-Reply-To: <5.0.2.1.2.20050125062113.02c11ec0@utinam.univ-fcomte.fr>
Message-ID: <002f01c502b4$93854240$e502eb82@hblaptop>

A wild guess: Do you have one file one function? Could it be that the last
line in one of the files does not end with a newline and this is not taken
care of by the build with lazy loading? Try to add a newline at the end of
each of your files.

Henrik Bengtsson

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Patrick Giraudoux H
> Sent: Tuesday, January 25, 2005 6:25 AM
> To: r-help
> Cc: Liaw, Andy
> Subject: RE: [R] error preparing a package for lazy loading with R CMD
> 
> 
> That's the way John Fox advised to turn the problem. Indeed 
> it works but 
> doe snot explain this bug in lazy loading. I don't think that 
> it may come 
> for a syntax error somewhere. All the functions have been 
> checked and the 
> problem does not occur if any of the 35 functions is removed. 
> Something 
> happens when I add another function to the 34 already 
> present. Strange.
> 
> 
> A 08:45 24/01/2005 -0500, vous avez ?crit :
> >Not sure if this will help, but have you tried loading the 
> package after
> >install with no lazyload?   I've found that if there are 
> syntax errors in
> >the R source, that can give the problem you described.  Just a guess.
> >
> >Andy
> >
> > > From: Patrick Giraudoux H
> > >
> > > Dear Lister,
> > >
> > > I  work with R 2.0.1 and Windows XP, and meet a strange trouble 
> > > trying to make a R package with a make-package.bat file 
> John Fox has
> > > kindly provided
> > > (see detailed script below). I am working with it since some
> > > months with
> > > excellent results (I do'nt use compiled C code so far). Just
> > > adding a new
> > > function in the R directory today, when running 
> make-package and thus
> > > excuting the command ..\..\bin\R CMD build --force --binary
> > > --auto-zip
> > > %1,  I got the following message after the "compile" stage,
> > > preparing the
> > > package for lazy loading :
> > >
> > > preparing package pgirmess for lazy loading
> > > Error in "names<-.default"(`*tmp*`, value =c("R", "Platform", 
> > > "Date", :
> > >          names attribute[4] must be the same length as the vector 
> > > [1] Execution halted
> > > make: ***[lazyload] Error 1
> > > *** Installation of pgirmess failed ***
> > >
> > > (pgirmess is the package name)
> > >
> > > ... and the zip file is not generated.
> > >
> > > I have checked and rechecked everything during this (long) 
> > > afternoon... and get nothing except that if I drag out any of the 
> > > *.r file of the R folder,
> > > everything comes to be OK (except the function that has been
> > > dragged out is
> > > missing...). It looks like if having added one extra function
> > > in the R
> > > folder on the top of the earlier 32 (+ 2 data frames) 
> makes problem.
> > >
> > > I have then consulted John Fox offlist and he seems quite 
> perplexed 
> > > "I'm not sure why you're experiencing this problem". On 
> his advise I 
> > > have included "LazyLoad: no" in the package description file. In 
> > > this case everything goes smoothly then (except LazyLoad 
> will not be
> > > activated), the
> > > zip file is generated and the package can be installed from R.
> > >
> > > Has anybody an idea about why a problem occurs when preparing the 
> > > package for lazy loading? Any remedy?
> > >
> > > Kind regards,
> > >
> > > Patrick
> > >
> > > Make-Package script:
> > >
> > > cd c:\R\rw2001\src\library
> > > del %1\INDEX
> > > del %1\data\00Index
> > > del %1\chm\*.* /Q
> > > ..\..\bin\R CMD build --force --binary --auto-zip %1 
> ..\..\bin\R CMD 
> > > build --force %1 ..\..\bin\R CMD check %1
> > > cd %1.Rcheck
> > > dvipdfm %1-manual
> > > notepad 00check.log
> > > cd ..
> > > cd ..
> > >
> > >
> > > >From: "John Fox" <jfox at mcmaster.ca>
> > > >To: "'Patrick Giraudoux H'" <patrick.giraudoux at univ-fcomte.fr>
> > > >Subject: RE: [R] writing a simple package in R 2.0 under 
> Windows XP
> > > >Date: Sun, 23 Jan 2005 11:41:25 -0500
> > > >X-Mailer: Microsoft Office Outlook, Build 11.0.6353
> > > >X-MIME-Autoconverted: from quoted-printable to 8bit by 
> > > >utinam.univ-fcomte.fr id j0NGfJoD011345
> > > >
> > > >Dear Patrick,
> > > >
> > > >I'm not sure why you're experiencing this problem.
> > > >
> > > >Two suggestions: (1) Since the problem appears to be with
> > > preparing the
> > > >package for lazy loading, try adding the directive
> > > "LazyLoad: no" (without
> > > >the quotes) to the package's DESCRIPTION file. (2) Rather
> > > than using my
> > > >batch file, run the commands one at a time to see exactly
> > > where the problem
> > > >is produced; then you could send a message to r-help.
> > > >
> > > >I hope this helps,
> > > >  John
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > >
> > >
> >
> >
> >-------------------------------------------------------------
> ----------
> >-------
> >Notice:  This e-mail message, together with any attachments, 
> contains 
> >information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New 
> >Jersey, USA 08889), and/or its affiliates (which may be 
> known outside the 
> >United States as Merck Frosst, Merck Sharp & Dohme or MSD 
> and in Japan, as 
> >Banyu) that may be confidential, proprietary copyrighted 
> and/or legally 
> >privileged. It is intended solely for the use of the 
> individual or entity 
> >named on this message.  If you are not the intended 
> recipient, and have 
> >received this message in error, please notify us immediately 
> by reply 
> >e-mail and then delete it from your system.
> >-------------------------------------------------------------
> ----------------- 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Tom.Mulholland at dpi.wa.gov.au  Tue Jan 25 09:10:22 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 25 Jan 2005 16:10:22 +0800
Subject: [R] more question
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA73@afhex01.dpi.wa.gov.au>

The answers to your questions are in the text below. However these questions are generally answered in the base documentation. You might try going through the Keywords by Topic which on the windows system at least) is accessed by going through the html help entry on the help menu. Once your browser is open select "Search Engine & Keywords." Depending on how your browser is set up you may have to scroll down to the keywords. I found it helpful to browse through the categories to get a feel for what R can do.



> -----Original Message-----
> From: Cuichang Zhao [mailto:cuiczhao at yahoo.com]
> Sent: Tuesday, 25 January 2005 3:25 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] more question
> 
> 
> Hello, 
> thank you very much for your help in last email. it is very 
> helpful. right now, i have more questions about my project,
>  
> 1. solve can i remove the NA from a vectors:
> for exmample, if my vector is:
> v <- (NA, 1, 2, 3, 4, 5)
> how can I remove the NA from vector v

Assuming you meant v <- c(NA, 1, 2, 3, 4, 5)

then

v[!is.na(v)]

An introduction to R, Page 10 gives an example 
>  
> 2. how I can get input from the user?

Page 279 of the Full reference Manual
?readline

fun <- function() {
       ANSWER <- readline("Are you a satisfied R user? ")
       if (substr(ANSWER, 1, 1) == "n")
         cat("This is impossible.  YOU LIED!\n")
       else
         cat("I knew it.\n")
     }
fun()

>  
> 3. can I read more than one data files in one program? and 
> how i can write something to a file

Yes, The whole document R Data Import/Export is devoted to this topic. There are too many ways to answer this question. 

>  
> 4. how i can display/output something on the screen?


cat("something")
print("something")
plot(5)
plot(v)

Try searching for device in the Full reference manual, if you are interested in the various ways output mechanisms.

I think you need to be more precise in your question.
>  
> thank you so much
>  
> Best wish
>  
> C-Ming
>  
> Jan 24, 2005
>  
>  
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Tue Jan 25 09:13:13 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 25 Jan 2005 09:13:13 +0100
Subject: [R] R 2.0.1 and Rggobi install issues on windows XP
In-Reply-To: <6a2c704c05012412023c62cdba@mail.gmail.com>
References: <6a2c704c05012412023c62cdba@mail.gmail.com>
Message-ID: <41F5FF99.9070806@statistik.uni-dortmund.de>

Drew Balazs wrote:

> Has anyone else ran into problems installing Rggobi with R 2.0.1 on a
> windows platform? I've followed all the instructions available and I
> still can not get R to recognize Rggobi as a library (package). I

I think it won't be that easy to get it installed.
By any chance, have you tried R CMD INSTALL on the package sources? Or 
did you use a binary package (I don't know any available)?
If you try to install from sources, you have to follow the instructions 
in the F for Windows FAQ (and the documents cited therein). You will 
certainly need to apply some changes and adaptions for Windows in the 
Rggobi package.

Uwe Ligges


> I've already emailed Duncan Temple Lange at ggobi.org and bell labs
> and both emails have bounced back.
> 
> Any help would be greatly appreciated.
> 
> 
> -Drew Balazs
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From petr.pikal at precheza.cz  Tue Jan 25 09:25:06 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 25 Jan 2005 09:25:06 +0100
Subject: [R] more question
In-Reply-To: <41F5FC93.1030309@statistik.uni-dortmund.de>
References: <20050125072527.23695.qmail@web30707.mail.mud.yahoo.com>
Message-ID: <41F61072.24923.6440FA@localhost>

Hi

On 25 Jan 2005 at 9:00, Uwe Ligges wrote:

> Cuichang Zhao wrote:
> > Hello, 
> > thank you very much for your help in last email. it is very helpful.
> > right now, i have more questions about my project,
> >  
> > 1. solve can i remove the NA from a vectors:
> > for exmample, if my vector is:
> > v <- (NA, 1, 2, 3, 4, 5)

v[!is.na(v)]

> > how can I remove the NA from vector v
> >  
> > 2. how I can get input from the user?

scan()  probably?

> >  
> > 3. can I read more than one data files in one program? and how i can
> > write something to a file

what program?

sink, write, ...

> >  
> > 4. how i can display/output something on the screen?

print,

And most effective, what Uwe did recommend, read available 
documentation (R-intro, help pages).

> >  
> > thank you so much
> >  
> > Best wish
> >  
> > C-Ming
> >  
> > Jan 24, 2005
> >  
> >  
> > 
> > 		
> > ---------------------------------
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> Please do a suggested above: Read the posting guide and read a minimal
> amount of documentation, as well as the R FAQ and learn to search the
> mailing list archives.
> 
> Uwe Ligges
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From j.van_den_hoff at fz-rossendorf.de  Tue Jan 25 09:53:37 2005
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Tue, 25 Jan 2005 09:53:37 +0100
Subject: [R] more question
In-Reply-To: <20050125072527.23695.qmail@web30707.mail.mud.yahoo.com>
References: <20050125072527.23695.qmail@web30707.mail.mud.yahoo.com>
Message-ID: <41F60911.4040702@fz-rossendorf.de>

Cuichang Zhao wrote:
> Hello, 
> thank you very much for your help in last email. it is very helpful. right now, i have more questions about my project,
>  
> 1. solve can i remove the NA from a vectors:
> for exmample, if my vector is:
> v <- (NA, 1, 2, 3, 4, 5)

which should read v <- c(NA, 1, 2, 3, 4, 5):



> how can I remove the NA from vector v
>  
v <- v[!is.na(v)]

> 2. how I can get input from the user?

try "?readline"

>  
> 3. can I read more than one data files in one program? and how i can write something to a file

?read.table, ?write.table, ?scan, ?readLines, ?write
>  
> 4. how i can display/output something on the screen?

?print, ?x11, ?plot, ?Devices
>  
> thank you so much
>  
> Best wish
>  
> C-Ming
>  
> Jan 24, 2005
one remark: I personally think it's a bad idea to refer people too 
quickly (with implied reproach) to the manuals (if the manuals are good, 
_anything_ is written down _somewhere_ but it might take substantial 
time to find it).
but at the very least your questions 2.-4. (and the solution to no. 1 is 
not so hard to find either) are so elementary, that you probably have 
not spent a single minute reading the documentation (online help or e.g. 
Introduction to R). and that is in my view not a healthy ratio of "time 
I invested myself to solve the problem" to "time others invest to solve 
my problem".

no offense meant, but really ...

joerg
>  
>  
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Christoph.Scherber at uni-jena.de  Tue Jan 25 09:57:36 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Tue, 25 Jan 2005 09:57:36 +0100
Subject: [R] lme and varFunc()
In-Reply-To: <20050124192418.GE590@uidaho.edu>
References: <41F4F80C.5060105@uni-jena.de> <20050124192418.GE590@uidaho.edu>
Message-ID: <41F60A00.5060006@uni-jena.de>

Dear all,

Regarding the lme with varFunc() question I posted a few days ago: I 
have used the following two approaches:

model1<-lme(response~Covariate+Block+TreatmentA+TreatmentB,random=~1|Plot/Subplot,method="ML")
model2a<-update(model1,weights=varPower(form=~ fitted(.)))
model2b<-update(model1,weights=varPower(form=~block))

While model2a produces an error
"Problem in .C("mixed_loglik",: subroutine mixed_loglik: Missing values in argument 1 
Use traceback() to see the call stack"

Model 2b seems to work fine, now.

I?m not sure why model2a doesn?t work, but using an important explanatory variable (block) as a variance covariate seems to do a better job (although I don?t really understand why)

Does anyone have an explanation for this?

Regards,
Chris.





Andrew Robinson wrote:

>Dear Christoph,
>
>what command are you using to plot the residuals?  If you use the
>default residuals it will not reflect the variance model.  If you use
>the argument
>
>type="p"
>
>then you get the Pearson residuals, which will reflect the weights
>model.  Try something like this:
>
>plot(model, resid(., type = "p") ~ fitted(.), abline = 0)
>
>I hope that this helps,
>
>Andrew
>
>On Mon, Jan 24, 2005 at 02:28:44PM +0100, Christoph Scherber wrote:
>  
>
>>Dear R users,
>>
>>I am currently analyzing a dataset using lme(). The model I use has the 
>>following structure:
>>
>>model<-lme(response~Covariate+TreatmentA+TreatmentB,random=~1|Block/Plot,method="ML")
>>
>>When I plot the residuals against the fitted values, I see a clear 
>>positive trend (meaning that the variance increases with the mean).
>>
>>I tried to solve this issue using weights=varPower(), but it doesn?t 
>>change the residual plot at all.
>>
>>How would you implement such a positive trend in the variance? I?ve 
>>tried glmmPQL (which works great with poisson errors), but using glmmPQL 
>>I can?t do model simplification.
>>
>>Many thanks for your help!
>>
>>Regards
>>Chris.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>    
>>
>
>  
>



From gc4 at duke.edu  Tue Jan 25 09:58:11 2005
From: gc4 at duke.edu (gc4@duke.edu)
Date: Tue, 25 Jan 2005 03:58:11 -0500 (EST)
Subject: [R] CODA vs. BOA discrepancy
In-Reply-To: <200501241218.j0OCIJTO026983@hypatia.math.ethz.ch>
References: <200501241218.j0OCIJTO026983@hypatia.math.ethz.ch>
Message-ID: <Pine.GSO.4.58.0501250342460.22916@godzilla.acpub.duke.edu>

Dear List:

the CODA and BOA packages for the analysis of MCMC output yield different
results on two dignostic test of convergence: 1) Geweke's convergence
diagnostic; 2) Heidelberger and Welch's convergence diagnostic. Does that
imply that the CODA and BOA packages implement different ``flavors'' of
the same test?

I paste below an example.

Geweke's test
cbind(coda.gwk,boa.gwk)

      z-score p-value Z-Score p-value
b[1]    1.143   0.253   0.000   1.000
b[2]    0.470   0.638   0.056   0.956
b[3]   -1.037   0.300  -0.388   0.698
b[4]   -0.618   0.536  -0.085   0.933
tau    -0.206   0.837  -0.008   0.994
sigma   0.716   0.474   0.437   0.662

CODA -- Heidelberger and Welch

      Stationarity start     p-value
      test         iteration
b[1]  passed       1         0.2649
b[2]  passed       1         0.6709
b[3]  passed       1         0.6376
b[4]  passed       1         0.3673
tau   passed       1         0.1944
sigma passed       1         0.0725

      Halfwidth Mean    Halfwidth
      test
b[1]  passed    -39.800 0.303994
b[2]  passed      0.714 0.003505
b[3]  passed      1.297 0.010317
b[4]  passed     -0.153 0.004025
tau   passed      0.106 0.000918
sigma passed      3.193 0.014986

BOA -- Heidelberger and Welch

      Stationarity Test Keep Discard      C-von-M
b[1]             passed 5000       0 2.841419e-13
b[2]             passed 5000       0 1.512525e-03
b[3]             passed 5000       0 1.162634e-02
b[4]             passed 5000       0 2.589256e-03
tau              passed 5000       0 2.742828e-04
sigma            passed 5000       0 1.086017e-01

       Halfwidth Test        Mean    Halfwidth
b[1]           failed -39.8000008 2.569228e+04
b[2]           passed   0.7137679 2.777682e-02
b[3]           passed   1.2968816 2.812990e-02
b[4]           failed  -0.1527912 2.778146e-02
tau            failed   0.1063554 2.772179e-02
sigma          passed   3.1930083 2.872075e-02

Thank you very much!
giacomo



From maechler at stat.math.ethz.ch  Tue Jan 25 09:59:03 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 25 Jan 2005 09:59:03 +0100
Subject: [R] animation without intermediate files?
In-Reply-To: <41F5956F.8010101@stat.auckland.ac.nz>
References: <Pine.LNX.4.44.0501061548270.27024-100000@panic.stat.cmu.edu>
	<41F5956F.8010101@stat.auckland.ac.nz>
Message-ID: <16886.2647.973007.550441@stat.math.ethz.ch>

>>>>> "Paul" == Paul Murrell <p.murrell at auckland.ac.nz>
>>>>>     on Tue, 25 Jan 2005 13:40:15 +1300 writes:

    Paul> Hi
    Paul> Cari G Kaufman wrote:
    >> Hello, 
    >> 
    >> Does anyone know how to make "movies" in R by making a sequence of plots?  
    >> I'd like to animate a long trajectory for exploratory purposes only,
    >> without creating a bunch of image files and then using another program to
    >> string them together.  In Splus I would do this using double.buffer()  to
    >> eliminate the flickering caused by replotting. For instance, with a 2-D
    >> trajectory in vectors x and y I would use the following:
    >> 
    >> motif()
    >> double.buffer("back")
    >> for (i in 1:length(x)) {
    >>   plot(x[i], y[i], xlim=range(x), ylim=range(y))
    >>   double.buffer("copy")
    >> }
    >> double.buffer("front")
    >> 
    >> I haven't found an equivalent function to double.buffer in R.  I tried
    >> playing around with dev.set() and dev.copy() but so far with no success
    >> (still flickers).

    Paul> Double buffering is only currently an option on the Windows graphics 
    Paul> device (and there it is "on" by default).  So something like ...

    Paul> x <- rnorm(100)
    Paul> for (i in 1:100)
    Paul>    plot(1:i, x[1:i], xlim=c(0, 100), ylim=c(-4, 4), pch=16, cex=2)

    Paul> is already "smooth"

well, sorry Paul, but not for my definition of "smooth"!

Instead, 

  n <- 100
  plot(1,1, xlim=c(0,n), ylim=c(-4,4), type="n")
  x <- rnorm(n)
  for (i in 1:n) { points(i, x[i], pch=16, cex=2); Sys.sleep(0.02) }

comes much closer to my version of "smooth"  ;-)

Martin



From ray at mcs.vuw.ac.nz  Tue Jan 25 10:11:48 2005
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Tue, 25 Jan 2005 22:11:48 +1300 (NZDT)
Subject: [R] Recursive default argument reference
Message-ID: <200501250911.j0P9Bmvd026641@tahi.mcs.vuw.ac.nz>

> From: boston knot <bostonknot at yahoo.com>
> Date: Mon, 24 Jan 2005 20:50:53 -0800 (PST)

> "Recursive default argument reference" keeps appearing when I try to run a haplo.score function in R for Windows. I'm new to using this program. Does anyone know what this means? 
>  
I don't know exactly what it means, but I have seen it quite some time
ago when I was creating a package using delay().  It is easily
reproduced by:
> x <- delay({runtimeerror; 10})
> x
Error: Object "runtimeerror" not found
> x
Error: recursive default argument reference
> 

HTH,
Ray Brownrigg



From Heather.Turner at warwick.ac.uk  Tue Jan 25 10:23:39 2005
From: Heather.Turner at warwick.ac.uk (Heather Turner)
Date: Tue, 25 Jan 2005 09:23:39 +0000
Subject: [R] using eval() with pre-built expression inside function
Message-ID: <s1f61026.066@liberator.csv.warwick.ac.uk>

Sorry, I forgot to add to my original post that I want to use the "envir" argument of eval() so that I can specify a secondary data source as follows:

eval(expression(model.frame(formula, data = lookHereFirst), envir = lookHereSecond, enclos = lookHereThird)

Your version of f1 does produce the desired model frame, but does this by passing all the data to model.frame() directly, i.e. it evaluates

model.frame(formula = y~x, data = dataset)

However, if I adapt your f1 as follows:

 f1 <- function(formula, data) {
     mf <- match.call(expand.dots = FALSE)
     mf[[1]] <- as.name("model.frame")
     print(mf)
     mf
 }
 
it still works if some of the data is provided through the "data" argument of model.frame() and the rest of the data is provided through the "envir" argument of eval():
> test1 <- f1(formula = y ~ x, data = dataset[,1, drop = FALSE])
model.frame(formula = y ~ x, data = dataset[, 1, drop = FALSE])
> eval(test1, dataset[,2,drop = FALSE], parent.frame())
          y x
1  66.71259 1
2  83.21937 1
3  85.24412 1
4  88.10189 1
5  75.71918 1
....

So you have provided a solution, but I still don't understand why I can't construct the call as I was trying to before:

f2 <- function(formula, data) {
    mf <- as.call(c(as.name("model.frame"), formula = formula, 
        data = substitute(data)))
    print(mf)
    mf
}

This doesn't work:

> test2 <- f2(formula = y ~ x, data = dataset[,1, drop = FALSE])
model.frame(formula = y ~ x, data = dataset[, 1, drop = FALSE])
> eval(test2, dataset[,2,drop = FALSE], parent.frame())
Error in eval(expr, envir, enclos) : Object "x" not found

Apparently test1 and test2 are not identical, but I can't see why, which may be the root of my problem here. Can anyone explain why they are different?

Thanks

Heather

>>> "Roger D. Peng" <rpeng at jhsph.edu> 01/24/05 04:48pm >>>
If you look at the beginning of lm(), you'll see that match.call() is 
used and name of the function (in this case "f1") is replaced with 
"model.frame".  Does something like this work?

f1 <- function(formula, data) {
	mf <- match.call(expand.dots = FALSE)
	mf[[1]] <- as.name("model.frame")
	eval(mf, parent.frame())
}

-roger

Heather Turner wrote:
> I'm trying to evaluate a pre-built expression using eval(), e.g.
> 
> dataset <- data.frame(y = runif(30, 50,100), x = gl(5, 6))
> 
> # one like this
> mf <- expression(model.frame(y~x))
> eval(mf, dataset, parent.frame())
> 
> # rather than this
> eval(expression(model.frame(y~x)), dataset, parent.frame())
> 
> In the example above there is no problem, the problem comes when I try to do a similar thing within a function, e.g.
> 
> f1 <- function(formula, data) {
>     mt <- terms(formula)
>     mf <- as.expression(as.call(c(as.name("model.frame"), formula = mt)))
>     eval(mf, data, parent.frame())
> }
> 
> 
>>f1(formula = y ~ x, data = dataset)
> 
> Error in eval(expr, envir, enclos) : Object "y" not found
> 
> I can get round this by building a call to eval using paste, e.g.
> 
> f2 <- function(formula, data) {
>     mt <- terms(formula)
>     mf <- as.expression(as.call(c(as.name("model.frame"), formula = mt)))
>     direct <- parse(text = paste("eval(expression(", mf,
>                "), data, parent.frame())"))
>     print(direct)
>     eval(direct)
> }
> 
> 
>>f2(formula = y ~ x, data = dataset)
> 
> expression(eval(expression(model.frame(formula = y ~ x)), data, 
>     parent.frame()))
>           y x
> 1  92.23087 1
> 2  63.43658 1
> 3  55.24448 1
> 4  72.75650 1
> 5  67.58781 1
> ...
> 
> but this seems rather convoluted. Can anyone explain why f1 doesn't work (when f2 does) and/or suggest a neater way of dealing with this?
> 
> Thanks
> 
> Heather
> 
> Mrs H Turner
> Research Assistant
> Dept. of Statistics
> University of Warwick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html 
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From vito_ricci at yahoo.com  Tue Jan 25 10:31:06 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Tue, 25 Jan 2005 10:31:06 +0100 (CET)
Subject: [R] Fitting distribution with R: a contribute
Message-ID: <20050125093106.81240.qmail@web41210.mail.yahoo.com>

Dear R-useRs,

I've written a contribute (in Italian language)
concering fitting distribution with R. I believe it
could be usefull for someones. It's available on CRAN
web-site:

http://cran.r-project.org/doc/contrib/Ricci-distribuzioni.pdf

Here's the abstract: 
This paper deals with distribution fitting using R
environment for statistical computing. It treats
briefly some theoretical issues and it points out
especially practical ones proposing some examples of R
statements for data graphical exploration and
presentation, parameters? estimates of patterns and
tests for goodness of fit.

TOC

1.0	Introduction
2.0	Graphics
3.0	Model/function choice
4.0	Parameters estimate
5.0	Measure of goodness of fit
6.0	Goodness of fit tests
6.1     Normality tests
7.0     Conclusions

Appendix
References

Any comments and suggestion will be appreciated.
Best regards
Vito 

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Jan 25 10:35:13 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 25 Jan 2005 10:35:13 +0100
Subject: [R] lme and varFunc()
References: <41F4F80C.5060105@uni-jena.de> <20050124192418.GE590@uidaho.edu>
	<41F60A00.5060006@uni-jena.de>
Message-ID: <002601c502c1$2c16c2c0$0540210a@www.domain>

Hi Chris,

You could perform a graphical check before deciding which variance 
function is reasonable to use. For example, in your case maybe 
something like:

plot(model1, resid(., type="p")~Block)

would have shown that the variability depends on `Block' (note: 
`Block' sounds like a categorical variable, if so probably you could 
also consider `varIdent()')

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Christoph Scherber" <Christoph.Scherber at uni-jena.de>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 25, 2005 9:57 AM
Subject: Re: [R] lme and varFunc()


> Dear all,
>
> Regarding the lme with varFunc() question I posted a few days ago: I 
> have used the following two approaches:
>
> model1<-lme(response~Covariate+Block+TreatmentA+TreatmentB,random=~1|Plot/Subplot,method="ML")
> model2a<-update(model1,weights=varPower(form=~ fitted(.)))
> model2b<-update(model1,weights=varPower(form=~block))
>
> While model2a produces an error
> "Problem in .C("mixed_loglik",: subroutine mixed_loglik: Missing 
> values in argument 1 Use traceback() to see the call stack"
>
> Model 2b seems to work fine, now.
>
> I?m not sure why model2a doesn?t work, but using an important 
> explanatory variable (block) as a variance covariate seems to do a 
> better job (although I don?t really understand why)
>
> Does anyone have an explanation for this?
>
> Regards,
> Chris.
>
>
>
>
>
> Andrew Robinson wrote:
>
>>Dear Christoph,
>>
>>what command are you using to plot the residuals?  If you use the
>>default residuals it will not reflect the variance model.  If you 
>>use
>>the argument
>>
>>type="p"
>>
>>then you get the Pearson residuals, which will reflect the weights
>>model.  Try something like this:
>>
>>plot(model, resid(., type = "p") ~ fitted(.), abline = 0)
>>
>>I hope that this helps,
>>
>>Andrew
>>
>>On Mon, Jan 24, 2005 at 02:28:44PM +0100, Christoph Scherber wrote:
>>
>>>Dear R users,
>>>
>>>I am currently analyzing a dataset using lme(). The model I use has 
>>>the following structure:
>>>
>>>model<-lme(response~Covariate+TreatmentA+TreatmentB,random=~1|Block/Plot,method="ML")
>>>
>>>When I plot the residuals against the fitted values, I see a clear 
>>>positive trend (meaning that the variance increases with the mean).
>>>
>>>I tried to solve this issue using weights=varPower(), but it 
>>>doesn?t change the residual plot at all.
>>>
>>>How would you implement such a positive trend in the variance? I?ve 
>>>tried glmmPQL (which works great with poisson errors), but using 
>>>glmmPQL I can?t do model simplification.
>>>
>>>Many thanks for your help!
>>>
>>>Regards
>>>Chris.
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>>
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ACACIAS1/PROSEGUR at es.prosegur.com  Tue Jan 25 10:33:03 2005
From: ACACIAS1/PROSEGUR at es.prosegur.com (ACACIAS1/PROSEGUR@es.prosegur.com)
Date: Tue, 25 Jan 2005 10:33:03 +0100
Subject: [R] Informar al remitente
Message-ID: <OF86E7C9D8.20635540-ONC1256F94.00347724@prosegur.es>

Informaci?n de incidente:-

Base de datos:       g:/notes/data/mail1.box
Originador:          r-help at stat.math.ethz.ch@PROSEGUR
Destinatarios:       carlos.perea at es.prosegur.com
Asunto:              Mail Delivery (failure carlos.perea at es.prosegur.com)
Fecha/Hora:          25/01/2005 10.32.53

El mensaje enviado a carlos.perea at es.prosegur.com fue puesto en cuarentena
porque inclu?a un contenido prohibido.
El mensaje enviado a carlos.perea at es.prosegur.com fue puesto en cuarentena
porque inclu?a un contenido prohibido.
El mensaje enviado a carlos.perea at es.prosegur.com fue puesto en cuarentena
porque inclu?a un contenido prohibido.



From christoph.lehmann at gmx.ch  Tue Jan 25 10:42:40 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Tue, 25 Jan 2005 10:42:40 +0100
Subject: [R] several boxplots or bwplots into one graphic
In-Reply-To: <41F50918.9000606@optonline.net>
References: <41F4FBC2.80303@gmx.ch> <41F50918.9000606@optonline.net>
Message-ID: <41F61490.1030103@gmx.ch>

many thanks for your great tip. I didn't know reshape.

Unfortunately in my real data, the values of my variables are ont all 
within the same range. Therefore, what shall I change in the code to get 
for each plot a scale, which is adjusted to the range of my variable?

thanks a lot

christoph

Chuck Cleland wrote:
>   Your variables (var.*) seem to be on the same scale.  How about 
> reshaping the data into a univariate layout and then using bwplot as 
> follows:
> 
> mydata <- data.frame(ID = 1:20, A = runif(20), B = runif(20),
>                      C = runif(20), GROUP = rep(c(0,1), c(10,10)))
> 
> mydata.uni <- reshape(mydata, varying = list(c("A", "B", "C")),
>                       v.names = "Y", timevar = "VAR", times = c("A",
>                       "B", "C"), direction = "long")
> 
> library(lattice)
> 
> bwplot(Y ~ as.factor(GROUP) | VAR, data = mydata.uni, layout=c(3,1,1),
>        xlab="Group")
> 
> hope this helps,
> 
> Chuck Cleland
> 
> Christoph Lehmann wrote:
>> Hi
>> I have 10 variables and 2 groups. I know how to plot a bwplot for ONE 
>> var. e.g.
>>
>> var.a var.b var.c .. GROUP
>> 0.2   0.5   0.2   .. 0
>> 0.3   0.2   0.2   .. 0
>> ..
>> 0.1   0.8   0.7   .. 1
>> 0.5   0.5   0.1   .. 1
>> ..
>>
>>
>> bwplot(var.a ~ GROUP, data = my.data)
>>
>> How can I plot 10 bwplots (or boxplots) automatically into one 
>> graphic? is there any function from lattice which can do this?
>>
>> thanks for a short hint
>>
>> christoph
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>



From ccleland at optonline.net  Tue Jan 25 11:23:31 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 25 Jan 2005 05:23:31 -0500
Subject: [R] several boxplots or bwplots into one graphic
In-Reply-To: <41F61490.1030103@gmx.ch>
References: <41F4FBC2.80303@gmx.ch> <41F50918.9000606@optonline.net>
	<41F61490.1030103@gmx.ch>
Message-ID: <41F61E23.5050609@optonline.net>

How about something like this?

mydata <- matrix(runif(180), ncol=9)
mydata <- as.data.frame(mydata)
mydata$V10 <- rnorm(20, 50, 10)
mydata$GROUP <- as.factor(rep(c("G1","G2"), c(10,10)))

par(mfrow=c(2,5))

for(i in 1:10){
boxplot(mydata[,i] ~ mydata$GROUP, main=names(mydata)[i])
}

hope it helps,

Chuck Cleland

Christoph Lehmann wrote:
> many thanks for your great tip. I didn't know reshape.
> 
> Unfortunately in my real data, the values of my variables are ont all 
> within the same range. Therefore, what shall I change in the code to get 
> for each plot a scale, which is adjusted to the range of my variable?
> 
> thanks a lot
> 
> christoph
> 
> Chuck Cleland wrote:
> 
>>   Your variables (var.*) seem to be on the same scale.  How about 
>> reshaping the data into a univariate layout and then using bwplot as 
>> follows:
>>
>> mydata <- data.frame(ID = 1:20, A = runif(20), B = runif(20),
>>                      C = runif(20), GROUP = rep(c(0,1), c(10,10)))
>>
>> mydata.uni <- reshape(mydata, varying = list(c("A", "B", "C")),
>>                       v.names = "Y", timevar = "VAR", times = c("A",
>>                       "B", "C"), direction = "long")
>>
>> library(lattice)
>>
>> bwplot(Y ~ as.factor(GROUP) | VAR, data = mydata.uni, layout=c(3,1,1),
>>        xlab="Group")
>>
>> hope this helps,
>>
>> Chuck Cleland
>>
>> Christoph Lehmann wrote:
>>
>>> Hi
>>> I have 10 variables and 2 groups. I know how to plot a bwplot for ONE 
>>> var. e.g.
>>>
>>> var.a var.b var.c .. GROUP
>>> 0.2   0.5   0.2   .. 0
>>> 0.3   0.2   0.2   .. 0
>>> ..
>>> 0.1   0.8   0.7   .. 1
>>> 0.5   0.5   0.1   .. 1
>>> ..
>>>
>>>
>>> bwplot(var.a ~ GROUP, data = my.data)
>>>
>>> How can I plot 10 bwplots (or boxplots) automatically into one 
>>> graphic? is there any function from lattice which can do this?
>>>
>>> thanks for a short hint
>>>
>>> christoph
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From Ted.Harding at nessie.mcc.ac.uk  Tue Jan 25 11:20:57 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 25 Jan 2005 10:20:57 -0000 (GMT)
Subject: [R] Zipf random number generation
In-Reply-To: <20050125051518.69271.qmail@web30003.mail.mud.yahoo.com>
Message-ID: <XFMail.050125102057.Ted.Harding@nessie.mcc.ac.uk>

On 25-Jan-05 Weiguang Shi wrote:
> Hi,
> 
> Is there a Zipf-like distribution RNG in R?
> 
> Thanks,
> Weiguang

"Zipf's Law" (as originally formulated in studies of the
frequencies of words in texts) is to the effect that the
relative frequencies with which words occur once, twice,
3 times, ... are in proportion to 1/1, 1/2, 1/3, ... ,
1/n, ... with no limit on n (i.e. the number of different
words each represented n times is proportional to 1/n).

This is "improper" since sum(1/n) = infinity, so does not
define a probability distribution. A respectable analogue
is Fisher's logarithmic distribution p(x) where

  p(x) = ((t^x)/x)/(-log(1-t)), x = 1, 2, 3, ...

where t in (0,1) is the parameter of the distribution.

As t -> 1, p(x+1)/p(x) -> x/(x+1) as in Zipf's Law.

However, I've searched the R site and have found only
one instance of a function directly related to this
distribution, namely logff() in package VGAM, which
is for estimating the parameter t.

So it looks as though there is no direct implementation
of something like "rlogdist".

However, the logarithmic distribution is a limiting
form of the negative binomial distribution (conditional
on a positive value), and there are functions in R for
random sampling from this distribution.

>From my reading of "?rnbinom" in the base package, this
function does not have the flexibitility you would need
for this. But in the MASS package there is the function
rnegbin() which does.

You would need to invoke

  rnegbin(N, mu=..., theta=... )

where the value ... of mu is large and the value ... of
theta is small, and reject cases with value zero. This
of course makes it awkward to generate a sample of given
size.

Alternatively, you can try along the lines of

> x0<-1:10000; t<-0.999; p<-((t^x0)/x0)/(-log(1-t))
> Y<-sample(x0,5000,replace=TRUE,prob=p)
> hist(Y,breaks=100)

While this gives the logarithmic distribution over the
range of x in x0, it is inexact in that it does not
permit values greater than max(x0) to be sampled.

No doubt others can suggest something better than this!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 25-Jan-05                                       Time: 10:20:57
------------------------------ XFMail ------------------------------



From robin_gruna at hotmail.com  Tue Jan 25 11:29:28 2005
From: robin_gruna at hotmail.com (Robin Gruna)
Date: Tue, 25 Jan 2005 11:29:28 +0100
Subject: [R] LDA: variables seems to be constant
Message-ID: <BAY103-DAV55705FC727E5B6446B7D487860@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050125/6dd0e36a/attachment.pl

From e.pebesma at geog.uu.nl  Tue Jan 25 11:34:22 2005
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Tue, 25 Jan 2005 11:34:22 +0100
Subject: [R] converting R objects to C types in .Call
Message-ID: <41F620AE.2020103@geog.uu.nl>

Is there a specific reason why, instead of

    CHAR(STRING_ELT(chstr, 0));

The S-Plus compatible

   CHARACTER_POINTER(chstr)[0]

does not work in R?
--
Edzer



From piet.vanremortel at gmail.com  Tue Jan 25 11:41:23 2005
From: piet.vanremortel at gmail.com (Piet van Remortel)
Date: Tue, 25 Jan 2005 11:41:23 +0100
Subject: [R] "disregarded projections" warning when fitting lm model
Message-ID: <A8630EEC-6EBD-11D9-8FE5-000D933A1C82@ua.ac.be>

Hi all,

I'm fitting a linear model (using lm) to some 2500 data points.  The 
model consists of 4 single terms and two combined terms.  I get the 
following warning message:

"Extra arguments projections are just disregarded. in: lm.fit(x, y, 
offset = offset, singular.ok = singular.ok, ...) "

Can anybody clarify this ?  I don't seem to find any pointer to what 
this might mean.  Too many/little data points ?  too many terms in the 
model ?

thanks

Piet


--
Piet van Remortel
Intelligent Systems Lab
University of Antwerp
Belgium
http://www.islab.ua.ac.be



From casile at tuebingen.mpg.de  Tue Jan 25 12:14:31 2005
From: casile at tuebingen.mpg.de (Antonino Casile)
Date: Tue, 25 Jan 2005 12:14:31 +0100
Subject: [R] spearman rank test correlation
In-Reply-To: <200501241225.j0OCIJTw026983@hypatia.math.ethz.ch>
References: <200501241225.j0OCIJTw026983@hypatia.math.ethz.ch>
Message-ID: <41F62A17.8030207@tuebingen.mpg.de>

Hallo,
does anybody know if there is an implementation of the Spearman rank 
correlation in R that gives a correct (or at least 'safe') p-value in 
the case of ties??
I have browsed the R-help archives but I found nothing.
Thanks a lot in advance for any help,
				Antonino Casile



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Jan 25 12:42:05 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 25 Jan 2005 12:42:05 +0100
Subject: [R] "disregarded projections" warning when fitting lm model
References: <A8630EEC-6EBD-11D9-8FE5-000D933A1C82@ua.ac.be>
Message-ID: <002e01c502d2$e4c683d0$0540210a@www.domain>

Hi Piet,

did you put an argument named `projections' in you lm call? If you 
look in lm.fit, then you'll see when this warning message appears. 
E.g.,:

x <- rnorm(100)
y <- rnorm(100)
lm(y~x, projections=5)

Call:
lm(formula = y ~ x, projections = 5)

Coefficients:
(Intercept)            x
   -0.09678      0.16589

Warning message:
Extra arguments projections are just disregarded. in: lm.fit(x, y, 
offset = offset, singular.ok = singular.ok, ...)

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Piet van Remortel" <piet.vanremortel at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 25, 2005 11:41 AM
Subject: [R] "disregarded projections" warning when fitting lm model


> Hi all,
>
> I'm fitting a linear model (using lm) to some 2500 data points.  The
> model consists of 4 single terms and two combined terms.  I get the
> following warning message:
>
> "Extra arguments projections are just disregarded. in: lm.fit(x, y,
> offset = offset, singular.ok = singular.ok, ...) "
>
> Can anybody clarify this ?  I don't seem to find any pointer to what
> this might mean.  Too many/little data points ?  too many terms in 
> the
> model ?
>
> thanks
>
> Piet
>
>
> --
> Piet van Remortel
> Intelligent Systems Lab
> University of Antwerp
> Belgium
> http://www.islab.ua.ac.be
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From vito_ricci at yahoo.com  Tue Jan 25 12:56:54 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Tue, 25 Jan 2005 12:56:54 +0100 (CET)
Subject: [R] Re: spearman rank test correlation
Message-ID: <20050125115654.1281.qmail@web41213.mail.yahoo.com>

Hi,

see: http://finzi.psych.upenn.edu/search.html and
search for "Spearman test" there are many results;

for Spearman rank test see:
cor.test(...., method="spearman") 
? cor.test
? rcorr {Hmisc}
? spearman2 (Hmisc)
http://finzi.psych.upenn.edu/R/library/Hmisc/html/rcorr.html

Regards,
Vito


you wrote:

Hallo,
does anybody know if there is an implementation of the
Spearman rank 
correlation in R that gives a correct (or at least
'safe') p-value in 
the case of ties??
I have browsed the R-help archives but I found
nothing.
Thanks a lot in advance for any help,
Antonino Casile


=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From ripley at stats.ox.ac.uk  Tue Jan 25 13:42:07 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 25 Jan 2005 12:42:07 +0000 (GMT)
Subject: [R] converting R objects to C types in .Call
In-Reply-To: <41F620AE.2020103@geog.uu.nl>
References: <41F620AE.2020103@geog.uu.nl>
Message-ID: <Pine.LNX.4.61.0501251239260.7734@gannet.stats>

On Tue, 25 Jan 2005, Edzer J. Pebesma wrote:

> Is there a specific reason why, instead of
>
>   CHAR(STRING_ELT(chstr, 0));
>
> The S-Plus compatible
>
>  CHARACTER_POINTER(chstr)[0]
>
> does not work in R?

Yes.

This is really a question for R-devel, but briefly, R and S-PLUS store 
character vectors in very different ways.  This worked prior to 1.2.0, but 
was changed as part of the price of the new (then) garbage collector.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jan 25 13:45:10 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 25 Jan 2005 12:45:10 +0000 (GMT)
Subject: [R] "disregarded projections" warning when fitting lm model
In-Reply-To: <A8630EEC-6EBD-11D9-8FE5-000D933A1C82@ua.ac.be>
References: <A8630EEC-6EBD-11D9-8FE5-000D933A1C82@ua.ac.be>
Message-ID: <Pine.LNX.4.61.0501251242530.7734@gannet.stats>

On Tue, 25 Jan 2005, Piet van Remortel wrote:

> I'm fitting a linear model (using lm) to some 2500 data points.  The
> model consists of 4 single terms and two combined terms.  I get the
> following warning message:
>
> "Extra arguments projections are just disregarded. in: lm.fit(x, y,
> offset = offset, singular.ok = singular.ok, ...) "
>
> Can anybody clarify this ?  I don't seem to find any pointer to what
> this might mean.  Too many/little data points ?  too many terms in the
> model ?

It means you called lm.fit with an argument `projections' it does not 
support.  Presumably you called lm with such an argument, even though none 
is documented.  Please read the help page for lm for more details.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From T.A.Wassenaar at rug.nl  Tue Jan 25 13:53:12 2005
From: T.A.Wassenaar at rug.nl (T.A.Wassenaar)
Date: Tue, 25 Jan 2005 13:53:12 +0100
Subject: [R] Manova and contrasts
Message-ID: <web-785953@mail3.rug.nl>


Hi,

In the archive I found a function for testing contrasts 
after manova, based on solving 'LBM=K'. There are a few 
questions on my mind:

1. A contrast between levels A and B of a certain factor 
with levels ABC is given as (0 -1 0) because the 
parameters for the intercept are fixed to zero in R. A 
contrast between B and C would then be (0 1 -1) right?

2. How would I go about if I want to determine which 
variate is causing rejection of the null hypothesis?

3. Is it also possible to construct confidence intervals 
for these contrasts?

Alternatively,

Does anyone by chance know whether it is possible to do 
multiple comparisons on multivariate data with the 
multcomp package? There seems to be an indication that it 
is possible, but I haven't been able to get it going.

Many thanks!

Tsjerk



From andy_liaw at merck.com  Tue Jan 25 14:54:04 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 25 Jan 2005 08:54:04 -0500
Subject: [R] agglomerative coefficient in agnes (cluster)
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5AA@usrymx25.merck.com>

Google really can be a very useful thing, in case you haven't found that.
This is the first hit I got with `agglomerative coefficient':

http://www.unesco.org/webworld/idams/advguide/Chapt7_1_4.htm

Andy

> From: Weiguang Shi
> 
> I haven't read the book, but could anyone explain more
> about this parameter? 
> 
> help(agnes) says that ac measures the amount of 
> clustering structure found. From the definition given
> in help(agnes.object), however, it seems that as long
> as 
> the dissimilarity of the merger in the final step of
> the
> algorithm is large enough, the ac value will be close
> to 
> 1. So what does ac really mean?
> 
> Thank you,
> Weiguang
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From hove at ift.uib.no  Tue Jan 25 14:57:18 2005
From: hove at ift.uib.no (Joakim Hove)
Date: Tue, 25 Jan 2005 14:57:18 +0100
Subject: [R] Plotting only masked part of data
Message-ID: <bjjy8ehtxvl.fsf@termo1.fi.uib.no>


Hello,

I have x and y data to plot (synthetic example):

  x <- seq(0,4*pi,by=0.1)
  y <- sin(x)

I then want to plot (x,y) in those points where abs(y) is smaller than
0.5. As a first approximation

  plot(x[abs(y) < 0.5],y[abs(y) < 0.5])

is quite close - however I want to plot with lines, i.e. type="l", and
then I get solid lines connecting the endpoint of one "active" region
to the start of the next active region, I would prefer to get rid of
those.

Lineart:
-------
 
   _____               ____
  / (1) \             / (3)\
 /       \           /      \
/         \         /        \
           \       /          \
            \_____/
              (2)


The problem is the horisontal lines shown as (1), (2) and (3) in the
schematic figure above, I would like to get rid of those,
i.e. retaining only four disconnected segments. I have started on a
programming based solution, plotting one piece at a time, but it is
*very* ugly!

Is there a simple general solution to my problem?



Regards

Joakim Hove

-- 
Joakim Hove
hove AT ift uib no
Tlf: +47 (55 5)8 27 90 
Fax: +47 (55 5)8 94 40
http://www.ift.uib.no/~hove/



From roger.bos at gmail.com  Tue Jan 25 14:57:52 2005
From: roger.bos at gmail.com (roger bos)
Date: Tue, 25 Jan 2005 08:57:52 -0500
Subject: [R] lookups and joins
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121B977@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121B977@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <1db7268005012505577b6cc1cd@mail.gmail.com>

Paul,

You don't want to write you own function.  merge() will do that for
you very quickly and efficiently.  Just to elaborate on Mike's reply,
here is an example of how to use merge:

> test <- merge(out, trt1m, by="gvkey")
> names(out)
[1] "gvkey"    "datadate" "Price"    "FV"       "ER"       "Rank"    
> names(trt1m)
[1] "gvkey" "trt1m"
> names(test)
[1] "gvkey"    "datadate" "Price"    "FV"       "ER"       "Rank"     "trt1m"   
> 
"gvkey" is the column that is joined on and the new data frame (test)
has all the columns of both data frames.  Now you can subscript as
usual to get just the data you need.  Read ?merge to see what
all.x=TRUE does in case you need to use that.

Thanks,

Roger

On Mon, 24 Jan 2005 22:41:54 -0000, michael watson (IAH-C)
<michael.watson at bbsrc.ac.uk> wrote:
> All together now:
> 
> ?merge
> 
> :-)
> 
> 
> -----Original Message-----
> From:   r-help-bounces at stat.math.ethz.ch on behalf of Paul Sorenson
> Sent:   Mon 1/24/2005 10:34 PM
> To:     r-help at stat.math.ethz.ch
> Cc:
> Subject:        [R] lookups and joins
> I have some data coming from SQL sources that I wish to relate in various ways.  For reasons only known to our IT people, this can't be done in SQL at present.
> 
> I am looking for an R'ish technique for looking up new columns on a data frame.  As a simple, hardwired example I have tried the following:
> 
> # This gives me two columns, one the lookup value and the second one
> # the result column, ie my lookup table.
> stcl = read.csv("stockclass.csv")
> stockclass = as.vector(stcl$stock_class)
> # This gives me what appears to be a dictionary or map
> names(stockclass) = as.vector(stcl$stock_group)
> 
> getstockclass = function(stock_group) {
>        try(stockclass[[stock_group]], TRUE)
> }
> csg$stk_class=factor(sapply(csg$stock_group, getstockclass))
> 
> I need the try since if there is a missing value I get an exception.
> 
> I also tried something along the lines of (from memory):
> getstockclass = function(stock_group) {
>        stcl[which(stcl$stock_group == stock_group),]$stock_class
> }
> 
> These work but I just wanted to check if there was an inbuilt way to do this kind of thing in R?  I searched on "join" without much luck.
> 
> Really what I would like is a generic function that:
>        - Takes 2 data frames,
>        - Some kind of specification on which column(s) to join
>        - Outputs the joined frames, or perhaps a vector which is an index vector that I can use on the second data frame.
> 
> I don't really want to reinvent SQL and my data sets are not huge.
> 
> cheers
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From B.Rowlingson at lancaster.ac.uk  Tue Jan 25 15:16:15 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 25 Jan 2005 14:16:15 +0000
Subject: [R] Plotting only masked part of data
In-Reply-To: <bjjy8ehtxvl.fsf@termo1.fi.uib.no>
References: <bjjy8ehtxvl.fsf@termo1.fi.uib.no>
Message-ID: <41F654AF.9010006@lancaster.ac.uk>


> is quite close - however I want to plot with lines, i.e. type="l", and
> then I get solid lines connecting the endpoint of one "active" region
> to the start of the next active region, I would prefer to get rid of
> those.

> Is there a simple general solution to my problem?

  Plot will draw disconnected lines if there are NA values in the vector 
of X or Y values, so if you do:

  > x <- seq(0,4*pi,by=0.1)
  > y <- sin(x)
  > x[abs(y)>=0.5]=NA
  > y[abs(y)>=0.5]=NA
  > plot(x,y,type='l')

You may get what you want.

Baz



From Achim.Zeileis at wu-wien.ac.at  Tue Jan 25 15:20:06 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 25 Jan 2005 15:20:06 +0100
Subject: [R] Plotting only masked part of data
In-Reply-To: <bjjy8ehtxvl.fsf@termo1.fi.uib.no>
References: <bjjy8ehtxvl.fsf@termo1.fi.uib.no>
Message-ID: <20050125152006.59c55694.Achim.Zeileis@wu-wien.ac.at>

On Tue, 25 Jan 2005 14:57:18 +0100 Joakim Hove wrote:

> 
> Hello,
> 
> I have x and y data to plot (synthetic example):
> 
>   x <- seq(0,4*pi,by=0.1)
>   y <- sin(x)
> 
> I then want to plot (x,y) in those points where abs(y) is smaller than
> 0.5. As a first approximation
> 
>   plot(x[abs(y) < 0.5],y[abs(y) < 0.5])
> 
> is quite close - however I want to plot with lines, i.e. type="l", and
> then I get solid lines connecting the endpoint of one "active" region
> to the start of the next active region, I would prefer to get rid of
> those.
> 
> Lineart:
> -------
>  
>    _____               ____
>   / (1) \             / (3)\
>  /       \           /      \
> /         \         /        \
>            \       /          \
>             \_____/
>               (2)
> 
> 
> The problem is the horisontal lines shown as (1), (2) and (3) in the
> schematic figure above, I would like to get rid of those,
> i.e. retaining only four disconnected segments. I have started on a
> programming based solution, plotting one piece at a time, but it is
> *very* ugly!
> 
> Is there a simple general solution to my problem?

That depends what general means. For this particular problem, the
following works:
  y2 <- y
  is.na(y2) <- abs(y) >= 0.5
  plot(x, y2, type = "l")

hth,
Z

> 
> 
> Regards
> 
> Joakim Hove
> 
> -- 
> Joakim Hove
> hove AT ift uib no
> Tlf: +47 (55 5)8 27 90 
> Fax: +47 (55 5)8 94 40
> http://www.ift.uib.no/~hove/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From hove at ift.uib.no  Tue Jan 25 15:21:03 2005
From: hove at ift.uib.no (Joakim Hove)
Date: Tue, 25 Jan 2005 15:21:03 +0100
Subject: [R] Plotting only masked part of data
In-Reply-To: <41F654AF.9010006@lancaster.ac.uk> (Barry Rowlingson's message
	of "Tue, 25 Jan 2005 14:16:15 +0000")
References: <bjjy8ehtxvl.fsf@termo1.fi.uib.no>
	<41F654AF.9010006@lancaster.ac.uk>
Message-ID: <bjjr7k9tws0.fsf@termo1.fi.uib.no>


Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:


>> Is there a simple general solution to my problem?
>
>   Plot will draw disconnected lines if there are NA values in the
> vector of X or Y values, so if you do:
>
>   > x <- seq(0,4*pi,by=0.1)
>   > y <- sin(x)
>   > x[abs(y)>=0.5]=NA
>   > y[abs(y)>=0.5]=NA
>   > plot(x,y,type='l')


That solved it immediately - thanks a lot.

Joakim


-- 
Joakim Hove
hove AT ift uib no
Tlf: +47 (55 5)8 27 90 
Fax: +47 (55 5)8 94 40
http://www.ift.uib.no/~hove/



From angiopteris at yahoo.com  Tue Jan 25 15:22:26 2005
From: angiopteris at yahoo.com (Florian Menzel)
Date: Tue, 25 Jan 2005 06:22:26 -0800 (PST)
Subject: [R] GLM function with poisson distribution
Message-ID: <20050125142227.95623.qmail@web40629.mail.yahoo.com>

Hello all, 
I found a weird result of the GLM function that seems
to be a bug.
The code:
  a=c(rep(1,8),rep(2,8))
  b=c(rep(0,8),rep(3,8))
  cbind(a,b)
  model=glm(b~a, family=poisson)
  summary(model)
generates a dataset with two groups. One group
consists entirely of zeros, the other of 3's (as
happened in a dataset Im analyzing right now). Since
they are count data, one should apply a poisson
distribution. A GLM with poisson distribution delivers
a p value > 0.99, thus, completely fails to detect the
difference between the two groups. Why not and what
should I do to avoid this error? A quasipoisson
distribution detects the difference but Im not sure
whether its appropriate to use it.
Thanks a lot to everybody who answers!
		 Florian

Version information:
version 1.9.0 (2004-4-12)
os mingw32
arch i386



From angiopteris at yahoo.com  Tue Jan 25 15:22:26 2005
From: angiopteris at yahoo.com (Florian Menzel)
Date: Tue, 25 Jan 2005 06:22:26 -0800 (PST)
Subject: [R] GLM function with poisson distribution
Message-ID: <20050125142227.95623.qmail@web40629.mail.yahoo.com>

Hello all, 
I found a weird result of the GLM function that seems
to be a bug.
The code:
  a=c(rep(1,8),rep(2,8))
  b=c(rep(0,8),rep(3,8))
  cbind(a,b)
  model=glm(b~a, family=poisson)
  summary(model)
generates a dataset with two groups. One group
consists entirely of zeros, the other of 3's (as
happened in a dataset Im analyzing right now). Since
they are count data, one should apply a poisson
distribution. A GLM with poisson distribution delivers
a p value > 0.99, thus, completely fails to detect the
difference between the two groups. Why not and what
should I do to avoid this error? A quasipoisson
distribution detects the difference but Im not sure
whether its appropriate to use it.
Thanks a lot to everybody who answers!
		 Florian

Version information:
version 1.9.0 (2004-4-12)
os mingw32
arch i386



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Jan 25 15:26:03 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 25 Jan 2005 15:26:03 +0100
Subject: [R] Plotting only masked part of data
References: <bjjy8ehtxvl.fsf@termo1.fi.uib.no>
Message-ID: <006301c502e9$cd03eff0$0540210a@www.domain>

a simple approach is to use NAs, i.e.,

x <- seq(0,4*pi,by=0.1)
y <- sin(x)
##########
x. <- x; x.[!abs(y) < 0.5] <- NA
y. <- y; y.[!abs(y) < 0.5] <- NA

plot(x., y., type="l")


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Joakim Hove" <hove at ift.uib.no>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 25, 2005 2:57 PM
Subject: [R] Plotting only masked part of data


>
> Hello,
>
> I have x and y data to plot (synthetic example):
>
>  x <- seq(0,4*pi,by=0.1)
>  y <- sin(x)
>
> I then want to plot (x,y) in those points where abs(y) is smaller 
> than
> 0.5. As a first approximation
>
>  plot(x[abs(y) < 0.5],y[abs(y) < 0.5])
>
> is quite close - however I want to plot with lines, i.e. type="l", 
> and
> then I get solid lines connecting the endpoint of one "active" 
> region
> to the start of the next active region, I would prefer to get rid of
> those.
>
> Lineart:
> -------
>
>   _____               ____
>  / (1) \             / (3)\
> /       \           /      \
> /         \         /        \
>           \       /          \
>            \_____/
>              (2)
>
>
> The problem is the horisontal lines shown as (1), (2) and (3) in the
> schematic figure above, I would like to get rid of those,
> i.e. retaining only four disconnected segments. I have started on a
> programming based solution, plotting one piece at a time, but it is
> *very* ugly!
>
> Is there a simple general solution to my problem?
>
>
>
> Regards
>
> Joakim Hove
>
> -- 
> Joakim Hove
> hove AT ift uib no
> Tlf: +47 (55 5)8 27 90
> Fax: +47 (55 5)8 94 40
> http://www.ift.uib.no/~hove/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Christoph.Scherber at uni-jena.de  Tue Jan 25 15:42:45 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Tue, 25 Jan 2005 15:42:45 +0100
Subject: [R] Box-Cox / data transformation question
Message-ID: <41F65AE5.4020601@uni-jena.de>

Dear R users,

Is it reasonable to transform data (measurements of plant height) to the 
power of 1/4? I?ve used boxcox(response~A*B) and lambda was close to 0.25.

Regards,
Christoph



From gb at tal.stat.umu.se  Tue Jan 25 15:55:00 2005
From: gb at tal.stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Tue, 25 Jan 2005 15:55:00 +0100
Subject: [R] tapply and names
Message-ID: <20050125145500.GB3223@tal.stat.umu.se>

I have a data frame containing children, with variables 'year' = birth
year, and 'm.id' = mother's id number. Let's assume that all the births of
each mother is represented in the data frame. 

Now I want to create a subset of this data frame containing all children,
whose mother's first birth was in the year 1816 or later. This seems to
work: 

    mid <- tapply(dat$year, dat$m.id, min)
    mid <- as.numeric(names(mid)[mid >= 1816])
    dat <- dat[dat$m.id %in% mid, ]

but I'm worried about the second line, because the output from 'tapply'
isn't documented to have a 'dimnames' attribute (although it has one, at
least in R-2.1.0, 2005-01-19). Another aspect is that this code relies on
m.id being numeric; I would have to change it if the type of m.id changes
to, eg, character.

So, question: Is there a better way of doing this?
-- 
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se



From gb at tal.stat.umu.se  Tue Jan 25 16:03:57 2005
From: gb at tal.stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Tue, 25 Jan 2005 16:03:57 +0100
Subject: [R] GLM function with poisson distribution
In-Reply-To: <20050125142227.95623.qmail@web40629.mail.yahoo.com>
References: <20050125142227.95623.qmail@web40629.mail.yahoo.com>
Message-ID: <20050125150357.GC3223@tal.stat.umu.se>

On Tue, Jan 25, 2005 at 06:22:26AM -0800, Florian Menzel wrote:
> Hello all, 
> I found a weird result of the GLM function that seems
> to be a bug.
> The code:
>   a=c(rep(1,8),rep(2,8))
>   b=c(rep(0,8),rep(3,8))
>   cbind(a,b)
>   model=glm(b~a, family=poisson)
>   summary(model)
> generates a dataset with two groups. One group
> consists entirely of zeros, the other of 3?s (as
> happened in a dataset I?m analyzing right now). Since
> they are count data, one should apply a poisson
> distribution. A GLM with poisson distribution delivers
> a p value > 0.99, thus, completely fails to detect the
> difference between the two groups. Why not and what
> should I do to avoid this error? A quasipoisson
> distribution detects the difference but I?m not sure
> whether it?s appropriate to use it.
> Thanks a lot to everybody who answers!


This seems to be a good example of the Hauk-Donner effect; the likelihood
ratio test gives a p-value of   8.017e-09, while the Wald p-value is 1 !
-- 
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se



From michael.watson at bbsrc.ac.uk  Tue Jan 25 16:04:25 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Tue, 25 Jan 2005 15:04:25 -0000
Subject: [R] Rd problems when converting DVI version
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950172CC16@iahce2knas1.iah.bbsrc.reserved>

Hi

Running R v2.0 on SuSe linux 8.2.

I'm trying to build a package (which built perfectly on Windows...) on
Linux, and I ran:

R CMD check mypackage

I got:

* checking mypackage-maual.tex ... ERROR
LaTeX errors when creating DVI version
This typically indicates Rd problems

OK, there are no problems with my Rd - I got 3 warnings but they were
all expected, and the whole package builds find on windows.  There is no
indication which of my Rd files is upsetting LaTeX and no information
about what it is about the Rd that is the problem.

I have in my notes that I used to use:

R CMD R2dvi

But that now returns an error saying R2dvi is not found... Did this
change when upgrading to v2?  Or are my notes wrong?

So any help getting to the root of this error would be very much
appreciated.

Cheers
Mick

Michael Watson
Head of Informatics
Institute for Animal Health,
Compton Laboratory,
Compton,
Newbury,
Berkshire RG20 7NN
UK

Phone : +44 (0)1635 578411 ext. 2535
Mobile: +44 (0)7990 827831
E-mail: michael.watson at bbsrc.ac.uk

"To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to
say what the experiment died of." R.A. Fisher, 1938.



From deepayan at stat.wisc.edu  Tue Jan 25 16:14:37 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 25 Jan 2005 09:14:37 -0600
Subject: [R] several boxplots or bwplots into one graphic
In-Reply-To: <41F61490.1030103@gmx.ch>
References: <41F4FBC2.80303@gmx.ch> <41F50918.9000606@optonline.net>
	<41F61490.1030103@gmx.ch>
Message-ID: <200501250914.37640.deepayan@stat.wisc.edu>


On Tuesday 25 January 2005 03:42, Christoph Lehmann wrote:
> many thanks for your great tip. I didn't know reshape.
>
> Unfortunately in my real data, the values of my variables are ont all
> within the same range. Therefore, what shall I change in the code to
> get for each plot a scale, which is adjusted to the range of my
> variable?

With bwplot, something like

bwplot(factor(GROUP) ~ var.a + var.b + var.c, mydata, 
       outer = TRUE, scales = list(x = "free"))

or

bwplot(var.a + var.b + var.c ~ factor(GROUP), mydata, 
       outer = TRUE, scales = list(y = "free"))

may get you what you want. This does the 'reshape'-ing 
automagically ;-), and the same idea (scales="free") can be used with 
reshaped data as well.

Deepayan



From bxc at steno.dk  Tue Jan 25 16:16:06 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Tue, 25 Jan 2005 16:16:06 +0100
Subject: [R] GLM function with poisson distribution
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FE6F5@exdkba022.novo.dk>


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Florian Menzel
> Sent: Tuesday, January 25, 2005 3:22 PM
> To: r-help at stat.math.ethz.ch; r-help at stat.math.ethz.ch
> Subject: [R] GLM function with poisson distribution
> 
> 
> Hello all, 
> I found a weird result of the GLM function that seems
> to be a bug.
> The code:
>   a=c(rep(1,8),rep(2,8))
>   b=c(rep(0,8),rep(3,8))
>   cbind(a,b)
>   model=glm(b~a, family=poisson)
>   summary(model)

It' because one of the values of b is 0, hence the linear predictor for
the corresponding level of a is -Inf, viz. the value -49 for the
intercept,
and the huge standard errors. Usual theory breaks down.

Replace the 0s with 1s and you get something which is closer
to what is covered by standard theory.

Bendix Carstensen

> generates a dataset with two groups. One group
> consists entirely of zeros, the other of 3's (as
> happened in a dataset I'm analyzing right now). Since
> they are count data, one should apply a poisson
> distribution. A GLM with poisson distribution delivers
> a p value > 0.99, thus, completely fails to detect the 
> difference between the two groups. Why not and what should I 
> do to avoid this error? A quasipoisson distribution detects 
> the difference but I'm not sure whether it's appropriate to 
> use it. Thanks a lot to everybody who answers!
> 		 Florian
> 
> Version information:
> version 1.9.0 (2004-4-12)
> os mingw32
> arch i386
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Tue Jan 25 16:43:24 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 25 Jan 2005 10:43:24 -0500
Subject: [R] tapply and names
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5AD@usrymx25.merck.com>

> From: G?ran Brostr?m
> 
> I have a data frame containing children, with variables 'year' = birth
> year, and 'm.id' = mother's id number. Let's assume that all 
> the births of
> each mother is represented in the data frame. 
> 
> Now I want to create a subset of this data frame containing 
> all children,
> whose mother's first birth was in the year 1816 or later. 
> This seems to
> work: 
> 
>     mid <- tapply(dat$year, dat$m.id, min)
>     mid <- as.numeric(names(mid)[mid >= 1816])
>     dat <- dat[dat$m.id %in% mid, ]
> 
> but I'm worried about the second line, because the output 
> from 'tapply'
> isn't documented to have a 'dimnames' attribute (although it 
> has one, at
> least in R-2.1.0, 2005-01-19). Another aspect is that this 
> code relies on
> m.id being numeric; I would have to change it if the type of 
> m.id changes
> to, eg, character.
> 
> So, question: Is there a better way of doing this?

Would this work?

  dat <- dat[ave(dat$year, dat$m.id, min) >= 1816, ]

Andy

> -- 
>  G?ran Brostr?m                    tel: +46 90 786 5223
>  Department of Statistics          fax: +46 90 786 6614
>  Ume? University                   http://www.stat.umu.se/egna/gb/
>  SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Tue Jan 25 16:48:58 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 25 Jan 2005 10:48:58 -0500
Subject: [R] Rd problems when converting DVI version
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5AF@usrymx25.merck.com>

> From: michael watson (IAH-C)
[snip]
> I have in my notes that I used to use:
> 
> R CMD R2dvi
> 
> But that now returns an error saying R2dvi is not found... Did this
> change when upgrading to v2?  Or are my notes wrong?

Would that be `Rd2dvi' by any chance?  If so, it's still there...

Andy
 
> So any help getting to the root of this error would be very much
> appreciated.
> 
> Cheers
> Mick
> 
> Michael Watson
> Head of Informatics
> Institute for Animal Health,
> Compton Laboratory,
> Compton,
> Newbury,
> Berkshire RG20 7NN
> UK
> 
> Phone : +44 (0)1635 578411 ext. 2535
> Mobile: +44 (0)7990 827831
> E-mail: michael.watson at bbsrc.ac.uk
> 
> "To call in the statistician after the experiment is done may 
> be no more
> than asking him to perform a post-mortem examination: he may 
> be able to
> say what the experiment died of." R.A. Fisher, 1938.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Jan 25 16:59:43 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 25 Jan 2005 16:59:43 +0100
Subject: [R] tapply and names
References: <20050125145500.GB3223@tal.stat.umu.se>
Message-ID: <00bf01c502f6$e299d570$0540210a@www.domain>

your approach, after omitting the "as.numeric()" in the second line, 
seems to work even for `m.id' being factor, i.e.,

dat <- data.frame(m.id=rep(letters[1:10], 10), year=sample(1805:1950, 
100, TRUE))
###########
mid <- tapply(dat$year, dat$m.id, min)
mid <- names(mid)[mid >= 1816]
dat. <- dat[dat$m.id %in% mid, ]
dat; dat.

but maybe there is something better.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "G?ran Brostr?m" <gb at tal.stat.umu.se>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, January 25, 2005 3:55 PM
Subject: [R] tapply and names


>I have a data frame containing children, with variables 'year' = 
>birth
> year, and 'm.id' = mother's id number. Let's assume that all the 
> births of
> each mother is represented in the data frame.
>
> Now I want to create a subset of this data frame containing all 
> children,
> whose mother's first birth was in the year 1816 or later. This seems 
> to
> work:
>
>    mid <- tapply(dat$year, dat$m.id, min)
>    mid <- as.numeric(names(mid)[mid >= 1816])
>    dat <- dat[dat$m.id %in% mid, ]
>
> but I'm worried about the second line, because the output from 
> 'tapply'
> isn't documented to have a 'dimnames' attribute (although it has 
> one, at
> least in R-2.1.0, 2005-01-19). Another aspect is that this code 
> relies on
> m.id being numeric; I would have to change it if the type of m.id 
> changes
> to, eg, character.
>
> So, question: Is there a better way of doing this?
> -- 
> G?ran Brostr?m                    tel: +46 90 786 5223
> Department of Statistics          fax: +46 90 786 6614
> Ume? University                   http://www.stat.umu.se/egna/gb/
> SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Tue Jan 25 17:19:18 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 25 Jan 2005 17:19:18 +0100
Subject: [R] Rd problems when converting DVI version
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950172CC16@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950172CC16@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <41F67186.3060208@statistik.uni-dortmund.de>

michael watson (IAH-C) wrote:

> Hi
> 
> Running R v2.0 on SuSe linux 8.2.
> 
> I'm trying to build a package (which built perfectly on Windows...) on
> Linux, and I ran:
> 
> R CMD check mypackage
> 
> I got:
> 
> * checking mypackage-maual.tex ... ERROR
> LaTeX errors when creating DVI version
> This typically indicates Rd problems


You can look in the log file of the LaTeX run found in the 
packagename.check directory what went wrong.

> OK, there are no problems with my Rd - I got 3 warnings but they were
> all expected, and the whole package builds find on windows.  There is no
> indication which of my Rd files is upsetting LaTeX and no information
> about what it is about the Rd that is the problem.
> 
> I have in my notes that I used to use:
> 
> R CMD R2dvi

*Rd2dvi* still exists...


Uwe Ligges


> But that now returns an error saying R2dvi is not found... Did this
> change when upgrading to v2?  Or are my notes wrong?
> 
> So any help getting to the root of this error would be very much
> appreciated.
> 
> Cheers
> Mick
> 
> Michael Watson
> Head of Informatics
> Institute for Animal Health,
> Compton Laboratory,
> Compton,
> Newbury,
> Berkshire RG20 7NN
> UK
> 
> Phone : +44 (0)1635 578411 ext. 2535
> Mobile: +44 (0)7990 827831
> E-mail: michael.watson at bbsrc.ac.uk
> 
> "To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to
> say what the experiment died of." R.A. Fisher, 1938.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Tue Jan 25 17:42:00 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 25 Jan 2005 08:42:00 -0800 (PST)
Subject: [R] GLM function with poisson distribution
In-Reply-To: <20050125142227.95623.qmail@web40629.mail.yahoo.com>
References: <20050125142227.95623.qmail@web40629.mail.yahoo.com>
Message-ID: <Pine.A41.4.61b.0501250838510.329090@homer06.u.washington.edu>

On Tue, 25 Jan 2005, Florian Menzel wrote:

> Hello all,
> I found a weird result of the GLM function that seems
> to be a bug.

No, the problem is that you are using the Wald test when the mle is 
infinite, which is always going to be unreliable.  It's even worse because 
you are using data that couldn't really have come from a Poisson 
distribution (because for a=1 you have mean 3 and variance 0).

If you used anova(model) to get a likelihood ratio test the p-value would 
be 4e-10.

 	-thomas

> The code:
>  a=c(rep(1,8),rep(2,8))
>  b=c(rep(0,8),rep(3,8))
>  cbind(a,b)
>  model=glm(b~a, family=poisson)
>  summary(model)
> generates a dataset with two groups. One group
> consists entirely of zeros, the other of 3s (as
> happened in a dataset Im analyzing right now). Since
> they are count data, one should apply a poisson
> distribution. A GLM with poisson distribution delivers
> a p value > 0.99, thus, completely fails to detect the
> difference between the two groups. Why not and what
> should I do to avoid this error? A quasipoisson
> distribution detects the difference but Im not sure
> whether its appropriate to use it.
> Thanks a lot to everybody who answers!
> 		 Florian
>
> Version information:
> version 1.9.0 (2004-4-12)
> os mingw32
> arch i386
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle

From jerk_alert at hotmail.com  Tue Jan 25 17:50:47 2005
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Tue, 25 Jan 2005 16:50:47 +0000
Subject: [R] Deleted objects keep coming back
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E5A1@usrymx25.merck.com>
Message-ID: <BAY101-F7772CCB5772DC91F2728AE8860@phx.gbl>

Ah! I think this is what happened -- whenever I restarted R, it would 
automatically load the previous workspace and its objects (call this 
workspace 1). Then, when I would attempt to load another workspace (call 
this workspace 2), it would retain the objects from workspace 1 in ADDITION 
to the objects loaded from workspace 2.

If I load up R, it will say previous workspace restored. If I then type 
 >rm(list=ls()) to delete all objects, and then load another workspace, it 
will only have the objects from the workspace I loaded...

Am I correct here?

Thanks, Andy.
-Ken

>From: "Liaw, Andy" <andy_liaw at merck.com>
>To: "'Ken Termiso'" <jerk_alert at hotmail.com>,r-help at stat.math.ethz.ch
>Subject: RE: [R] Deleted objects keep coming back
>Date: Mon, 24 Jan 2005 16:04:41 -0500
>
> > From: Ken Termiso
> >
> > Having a very strange and frustrating problem with v2.0.1
> > under Mac OSX
> > 10.3. I have several closely related workspaces in the same
> > directory that I
> > need to keep separate from one another (there are a few
> > objects common to
> > all workspaces, and then there are other objects that store
> > the results of
> > different analyses on the common objects, which is why I need
> > separate
> > workspaces).
> >
> > When I attempted to delete objects with rm() and then re-save
> > the workspace,
> > after quitting and restarting R, reloading the workspace, the
> > objects were
> > still present in the workspace (even though I'd deleted them).
> >
> > So I deleted the objects again with rm() and also called
> > gc(), and saved the
> > workspace with a new name. I then quit R and reloaded the
> > workspaces, and
> > that seemed to work. The deleted objects stayed deleted.
> >
> > But then an hour later, I came back and reloaded these
> > workspaces, and the
> > old objects which I'd deleted had come back again. This is
> > really annoying
> > and I feel like I'm losing my mind! I'm concerned that R is
> > not accessing
> > the directory properly, and may be reading one workspace file
> > in place of
> > another. This would completely jeopardize my work.
> >
> > Please help!
> > Thanks in advance,
> > Ken
>
>Are you sure the objects that won't go away are actually in the workspace
>you think they're in?  What are the exact commands you used to check?  How
>did you load the workspaces (the commands, please)?  Please show the exact
>sequence of what you did and what you saw.
>
>Andy
>
>
>------------------------------------------------------------------------------
>Notice:  This e-mail message, together with any attachments, contains 
>information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New 
>Jersey, USA 08889), and/or its affiliates (which may be known outside the 
>United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
>Banyu) that may be confidential, proprietary copyrighted and/or legally 
>privileged. It is intended solely for the use of the individual or entity 
>named on this message.  If you are not the intended recipient, and have 
>received this message in error, please notify us immediately by reply 
>e-mail and then delete it from your system.
>------------------------------------------------------------------------------



From wgshi2001 at yahoo.ca  Tue Jan 25 17:59:47 2005
From: wgshi2001 at yahoo.ca (Weiguang Shi)
Date: Tue, 25 Jan 2005 11:59:47 -0500 (EST)
Subject: [R] agglomerative coefficient in agnes (cluster)
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E5AA@usrymx25.merck.com>
Message-ID: <20050125165947.34983.qmail@web30005.mail.mud.yahoo.com>

Thanks Andy. Google is really useful.

But that page doesn't answer my question, does it?

I repeat: AC highly depends on the value of the
dissimilarity of the last merge.
My question: what is the use of AC?

Weiguang
    

 --- "Liaw, Andy" <andy_liaw at merck.com> wrote: 
> Google really can be a very useful thing, in case
> you haven't found that.
> This is the first hit I got with `agglomerative
> coefficient':
> 
>
http://www.unesco.org/webworld/idams/advguide/Chapt7_1_4.htm
> 
> Andy
> 
> > From: Weiguang Shi
> > 
> > I haven't read the book, but could anyone explain
> more
> > about this parameter? 
> > 
> > help(agnes) says that ac measures the amount of 
> > clustering structure found. From the definition
> given
> > in help(agnes.object), however, it seems that as
> long
> > as 
> > the dissimilarity of the merger in the final step
> of
> > the
> > algorithm is large enough, the ac value will be
> close
> > to 
> > 1. So what does ac really mean?
> > 
> > Thank you,
> > Weiguang
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
>
------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any
> attachments, contains information of Merck & Co.,
> Inc. (One Merck Drive, Whitehouse Station, New
> Jersey, USA 08889), and/or its affiliates (which may
> be known outside the United States as Merck Frosst,
> Merck Sharp & Dohme or MSD and in Japan, as Banyu)
> that may be confidential, proprietary copyrighted
> and/or legally privileged. It is intended solely for
> the use of the individual or entity named on this
> message.  If you are not the intended recipient, and
> have received this message in error, please notify
> us immediately by reply e-mail and then delete it
> from your system.
>
------------------------------------------------------------------------------
>



From pkhomski at wiwi.uni-bielefeld.de  Tue Jan 25 18:15:46 2005
From: pkhomski at wiwi.uni-bielefeld.de (Pavel Khomski)
Date: Tue, 25 Jan 2005 18:15:46 +0100
Subject: [R] lme (and glmmPQL)
Message-ID: <41F67EC2.9040208@wiwi.uni-bielefeld.de>

Hello!

hier one question about the limits in the number of random effects 
specified by the random part in lme-Call.

could the Call to lme(...) realy fail only becouse of too large 
random-structure part in lme(..., random=....) ?
I really would need many random parameters in the model, to be specified 
for example by the following:


lme(...,
random=list(my.Subject=pdBlocked(list(pdIdent(value=diag(vector_1),form=,nam=),  

                                                                              
pdIdent(value=diag(vector_2),form=,nam=),
                                                                        
      pdIdent(value=diag(vector_3),form=,nam=),
                                                                              
--------------------------------------------------,
                                                                              
pdIdent(value=diag(vector_30),form=,nam=)
                         ))),

...)

Has anybody got such a problem with lme(...) or glmmPQL(...) or  
relative problems?


Thanks a lot in advance.



From tobias.verbeke at telenet.be  Tue Jan 25 18:23:00 2005
From: tobias.verbeke at telenet.be (Tobias Verbeke)
Date: Tue, 25 Jan 2005 18:23:00 +0100
Subject: [R] GLM function with poisson distribution
In-Reply-To: <20050125150357.GC3223@tal.stat.umu.se>
References: <20050125142227.95623.qmail@web40629.mail.yahoo.com>
	<20050125150357.GC3223@tal.stat.umu.se>
Message-ID: <20050125182300.1cd61fea.tobias.verbeke@telenet.be>

On Tue, 25 Jan 2005 16:03:57 +0100
G?ran Brostr?m <gb at tal.stat.umu.se> wrote:

> On Tue, Jan 25, 2005 at 06:22:26AM -0800, Florian Menzel wrote:
> > Hello all, 
> > I found a weird result of the GLM function that seems
> > to be a bug.
> > The code:
> >   a=c(rep(1,8),rep(2,8))
> >   b=c(rep(0,8),rep(3,8))
> >   cbind(a,b)
> >   model=glm(b~a, family=poisson)
> >   summary(model)
> > generates a dataset with two groups. One group
> > consists entirely of zeros, the other of 3?s (as
> > happened in a dataset I?m analyzing right now). Since
> > they are count data, one should apply a poisson
> > distribution. A GLM with poisson distribution delivers
> > a p value > 0.99, thus, completely fails to detect the
> > difference between the two groups. Why not and what
> > should I do to avoid this error? A quasipoisson
> > distribution detects the difference but I?m not sure
> > whether it?s appropriate to use it.
> > Thanks a lot to everybody who answers!
> 
> 
> This seems to be a good example of the Hauk-Donner effect; the likelihood
> ratio test gives a p-value of   8.017e-09, while the Wald p-value is 1 !

For the record: Hauck [sic], W.W. and A. Donner, Wald's test 
                as applied to hypotheses in logit analysis,
                JASA, 72(1977), p. 851-853.
                

Tobias
> -- 
>  G?ran Brostr?m                    tel: +46 90 786 5223
>  Department of Statistics          fax: +46 90 786 6614
>  Ume? University                   http://www.stat.umu.se/egna/gb/
>  SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Avneet.Singh at graftech.com  Tue Jan 25 18:50:09 2005
From: Avneet.Singh at graftech.com (Singh, Avneet)
Date: Tue, 25 Jan 2005 12:50:09 -0500
Subject: [R] r square values for independent variables in multiple linear
	regr ession model -- newbie
Message-ID: <A8722C0C0FB4D3118A13009027C3C82E042A80F8@U742EXC1>

Hello

Could you please suggest a way to find out the r square values for each
independent variable while using "lm" for developing a multiple linear
regression model.

Thank you
avneet

"I have no data yet. It is a capital mistake to theorize before one has
data. Insensibly one begins to twist facts to suit theories instead of
theories to suit facts."
~ Sir Arthur Conan Doyle (1859-1930), Sherlock Holmes



From kottha at zhr.tu-dresden.de  Tue Jan 25 19:06:17 2005
From: kottha at zhr.tu-dresden.de (kottha@zhr.tu-dresden.de)
Date: Tue, 25 Jan 2005 19:06:17 +0100
Subject: [R] Plotting hclust with lot of objects
Message-ID: <200501251906.17978.kottha@zhr.tu-dresden.de>

Hi!

I am newbee to R and I am facing the problem in plotting 
the dedrogram with lot of objects. The lines and labels are overlapped very 
badly, and writing the graphic to postscript and zooming there is not helping 
either. I tried cut.dendrogram method, but getting the error that it doesn't 
exist even though I get the man pages for it. 

I would not find any solution in web as well, and I would be grateful to you 
if you would share your thoughts or solutions with me. 

Here is my code:
A <- read.table("temp.csv", sep =",", header =TRUE, row.names = 1)
hc <- hclust(dist(A),method='single')
plot(as.dendrogram(hc), horiz=TRUE, cex=0.2)
memb <- cut.dendrogram(as.dendrogram(hc), 4)
Error: couldn't find function "cut.dendrogram"

Thanks allot,
Samatha

_______________________________________________________________
Samatha Kottha
Center for High Performance computing		TeL: (+49) 351 4633 1945
Technical University Dresden
Willers-bau A106 
Zellescher Weg 12
D-01069 Dresden
Germany



From kottha at zhr.tu-dresden.de  Tue Jan 25 19:04:47 2005
From: kottha at zhr.tu-dresden.de (kottha@zhr.tu-dresden.de)
Date: Tue, 25 Jan 2005 19:04:47 +0100
Subject: [R] Plotting hclust with lot of objects
Message-ID: <200501251904.47060.kottha@zhr.tu-dresden.de>

Hi!

I am newbee to R and I am facing the problem in plotting 
the dedrogram with lot of objects. The lines and labels are overlapped very 
badly, and writing the graphic to postscript and zooming there is not helping 
either. I tried cut.dendrogram method, but getting the error that it doesn't 
exist even though I get the man pages for it. 

I would not find any solution in web as well, and I would be grateful to you 
if you would share your thoughts or solutions with me. 

Here is my code:
A <- read.table("temp.csv", sep =",", header =TRUE, row.names = 1)
hc <- hclust(dist(A),method='single')
plot(as.dendrogram(hc), horiz=TRUE, cex=0.2)
memb <- cut.dendrogram(as.dendrogram(hc), 4)
Error: couldn't find function "cut.dendrogram"

Thanks allot,
Samatha



From R-user at zutt.org  Tue Jan 25 19:01:44 2005
From: R-user at zutt.org (R user)
Date: Tue, 25 Jan 2005 19:01:44 +0100
Subject: [R] parameter couldn't be set in high-level plot() function
In-Reply-To: <1106578227.29697.51.camel@dutiih.st.ewi.tudelft.nl>
References: <1106578227.29697.51.camel@dutiih.st.ewi.tudelft.nl>
Message-ID: <1106676104.11550.3.camel@dutiih.st.ewi.tudelft.nl>

Think the problem I had with the bandplot (gplots) function is solved by
changing the expand.dots = FALSE to expand.dots = TRUE.
Don't understand actually why it says FALSE here, because that means it
does *not* pass extra arguments to plot.
If I change it to TRUE, my main/xlab/ylab arguments are passed just like
I wanted.

fragment of bandplot[gplots]

    if (!add) {
        m <- match.call(expand.dots = FALSE)
        m$width <- m$add <- m$sd <- m$sd.col <- NULL
        m$method <- m$n <- NULL
        m[[1]] <- as.name("plot")
        mf <- eval(m, parent.frame())
    }

Jonne.


On Mon, 2005-01-24 at 15:50 +0100, R user wrote:
> 
> Dear R users,
> 
> I am using function bandplot from the gplots package.
> To my understanding (viewing the source of bandplot) it calls
> function plot (add = FALSE) with the same parameters (except for a few
> removed).
> 
> I would like to give extra parameters 'xlab' and 'ylab' to function
> bandplot, but, as can be seen below, that raises warnings (and the
> labels do not show up at the end).
> 
> It does work to call title(... xlab="blah", ylab="foo") after bandplot
> (), but then I have two labels on top of each other, which is even more
> ugly.
> 
> Can anyone explain me why this goes wrong?
> 
> Thanks in advance,
> Jonne.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
R user <R-user at zutt.org>



From sdavis2 at mail.nih.gov  Tue Jan 25 19:18:59 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 25 Jan 2005 13:18:59 -0500
Subject: [R] Plotting hclust with lot of objects
In-Reply-To: <200501251906.17978.kottha@zhr.tu-dresden.de>
References: <200501251906.17978.kottha@zhr.tu-dresden.de>
Message-ID: <959114A0-6EFD-11D9-91EA-000D933565E8@mail.nih.gov>

Samatha,

Look at ?cutree.

Sean

On Jan 25, 2005, at 1:06 PM, kottha at zhr.tu-dresden.de wrote:

> Hi!
>
> I am newbee to R and I am facing the problem in plotting
> the dedrogram with lot of objects. The lines and labels are overlapped 
> very
> badly, and writing the graphic to postscript and zooming there is not 
> helping
> either. I tried cut.dendrogram method, but getting the error that it 
> doesn't
> exist even though I get the man pages for it.
>
> I would not find any solution in web as well, and I would be grateful 
> to you
> if you would share your thoughts or solutions with me.
>
> Here is my code:
> A <- read.table("temp.csv", sep =",", header =TRUE, row.names = 1)
> hc <- hclust(dist(A),method='single')
> plot(as.dendrogram(hc), horiz=TRUE, cex=0.2)
> memb <- cut.dendrogram(as.dendrogram(hc), 4)
> Error: couldn't find function "cut.dendrogram"
>
> Thanks allot,
> Samatha
>
> _______________________________________________________________
> Samatha Kottha
> Center for High Performance computing		TeL: (+49) 351 4633 1945
> Technical University Dresden
> Willers-bau A106
> Zellescher Weg 12
> D-01069 Dresden
> Germany
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From michael.watson at bbsrc.ac.uk  Tue Jan 25 19:20:28 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Tue, 25 Jan 2005 18:20:28 -0000
Subject: [R] Plotting hclust with lot of objects
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B983@iahce2knas1.iah.bbsrc.reserved>

I think you need to use just:

cut

You may also want to look at:

cutree

-----Original Message-----
From:	r-help-bounces at stat.math.ethz.ch on behalf of kottha at zhr.tu-dresden.de
Sent:	Tue 1/25/2005 6:06 PM
To:	r-help at stat.math.ethz.ch
Cc:	
Subject:	[R] Plotting hclust with lot of objects
Hi!

I am newbee to R and I am facing the problem in plotting 
the dedrogram with lot of objects. The lines and labels are overlapped very 
badly, and writing the graphic to postscript and zooming there is not helping 
either. I tried cut.dendrogram method, but getting the error that it doesn't 
exist even though I get the man pages for it. 

I would not find any solution in web as well, and I would be grateful to you 
if you would share your thoughts or solutions with me. 

Here is my code:
A <- read.table("temp.csv", sep =",", header =TRUE, row.names = 1)
hc <- hclust(dist(A),method='single')
plot(as.dendrogram(hc), horiz=TRUE, cex=0.2)
memb <- cut.dendrogram(as.dendrogram(hc), 4)
Error: couldn't find function "cut.dendrogram"

Thanks allot,
Samatha

_______________________________________________________________
Samatha Kottha
Center for High Performance computing		TeL: (+49) 351 4633 1945
Technical University Dresden
Willers-bau A106 
Zellescher Weg 12
D-01069 Dresden
Germany

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gb at tal.stat.umu.se  Tue Jan 25 19:43:16 2005
From: gb at tal.stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Tue, 25 Jan 2005 19:43:16 +0100
Subject: [R] tapply and names
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E5AD@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E5AD@usrymx25.merck.com>
Message-ID: <20050125184316.GA21390@tal.stat.umu.se>

On Tue, Jan 25, 2005 at 10:43:24AM -0500, Liaw, Andy wrote:
> > From: G?ran Brostr?m
> > 
> > I have a data frame containing children, with variables 'year' = birth
> > year, and 'm.id' = mother's id number. Let's assume that all 
> > the births of
> > each mother is represented in the data frame. 
> > 
> > Now I want to create a subset of this data frame containing 
> > all children,
> > whose mother's first birth was in the year 1816 or later. 
> > This seems to
> > work: 
> > 
> >     mid <- tapply(dat$year, dat$m.id, min)
> >     mid <- as.numeric(names(mid)[mid >= 1816])
> >     dat <- dat[dat$m.id %in% mid, ]
> > 
> > but I'm worried about the second line, because the output 
> > from 'tapply'
> > isn't documented to have a 'dimnames' attribute (although it 
> > has one, at
> > least in R-2.1.0, 2005-01-19). Another aspect is that this 
> > code relies on
> > m.id being numeric; I would have to change it if the type of 
> > m.id changes
> > to, eg, character.
> > 
> > So, question: Is there a better way of doing this?
> 
> Would this work?
> 
>   dat <- dat[ave(dat$year, dat$m.id, min) >= 1816, ]

Yes, but you (or I) need

> dat <- dat[ave(dat$year, dat$m.id, FUN = min) >= 1816, ]
                                     ^^^^^
(took me some time to figure out), because

?ave

Usage:

     ave(x, ..., FUN = mean)

Thanks Andy for giving me 'ave'! And thanks to Dimitris for his suggestion. 

G?ran



From helprhelp at gmail.com  Tue Jan 25 20:38:12 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Tue, 25 Jan 2005 14:38:12 -0500
Subject: [R] multi-class classification using rpart
Message-ID: <cdf8178305012511384091ce72@mail.gmail.com>

Hi,
I am trying to make a multi-class classification tree by using rpart.
I used MASS package'd data: fgl to test and it works well.

However, when I used my small-sampled data as below, the program seems
to take forever. I am not sure if it is due to slowness or there is
something wrong with my codes or data manipulation.

Please be advised !

The data is described as the output from str() function. The call to
rpart is like:

library(rpart)
test_tree<-rpart(x$V142 ~ ., data=x, parms=list(split='gini'), cp =0.01)

the response variable is $V142, with 3 levels.

Thanks for your suggestions!

Ed.

> str(x)
`data.frame':   500 obs. of  142 variables
 $ V1  : int  4 4 4 4 4 4 4 4 4 4 ...
 $ V2  : Factor w/ 8 levels "1","2","3","4",..: 4 4 4 5 5 7 6 4 5 4 ...
 $ V3  : num  0.00803 0.00111 0.00995 0.01032 0.01295 ...
 $ V4  : num  -0.011034 -0.003711  0.003436  0.000968 -0.006914 ...
 $ V5  : num  0.00524 0.00563 0.00973 0.01285 0.02148 ...
 $ V6  : num  0.00633 0.00831 0.02750 0.03375 0.01254 ...
 $ V7  : num  -0.00422 -0.00151  0.00214 -0.00101  0.00299 ...
 $ V8  : num  0.02224 0.01761 0.01359 0.01045 0.00592 ...
 $ V9  : num  -0.00301 -0.00260  0.01338  0.01129  0.00604 ...
 $ V10 : num  0.00224 0.00303 0.02312 0.02414 0.02752 ...
 $ V11 : num  0.00857 0.01134 0.05062 0.05789 0.04007 ...
 $ V12 : num  0.00435 0.00983 0.05276 0.05688 0.04305 ...
 $ V13 : num  0.025627 0.000429 0.055087 0.088517 0.068946 ...
 $ V14 : num  0.21 0.15 0.34 0.31 0.36 ...
 $ V15 : int  157 81 39 40 40 40 38 72 105 31 ...
 $ V16 : int  21 238 236 259 253 253 258 258 259 246 ...
 $ V17 : int  82 81 39 40 40 40 38 72 105 31 ...
 $ V18 : int  21 15 7 129 14 129 129 6 9 110 ...
 $ V19 : int  60 45 39 40 40 40 38 59 63 31 ...
 $ V20 : int  21 15 7 14 14 12 53 6 9 62 ...
 $ V21 : num  0.953 0.893 0.913 0.843 0.872 ...
 $ V22 : num  1.19 1.08 1.03 1.04 1.11 ...
 $ V23 : num  0.953 0.893 0.913 0.898 0.872 ...
 $ V24 : num  0.955 0.893 0.913 0.898 0.872 ...
 $ V25 : num  1.013 0.979 0.985 0.998 0.994 ...
 $ V26 : num  0.972 0.940 0.913 0.909 0.895 ...
 $ V27 : num  0.999 0.979 0.985 0.998 0.994 ...
 $ V28 : num  0.979 0.959 0.928 0.940 0.926 ...
 $ V29 : num  0.999 0.979 0.985 0.998 0.994 ...
 $ V30 : num  0.989 0.969 0.962 0.976 0.951 ...
 $ V31 : num  0.992 0.973 0.971 0.980 0.973 ...
 $ V32 : num  0.992 0.975 0.977 0.989 0.980 ...
 $ V33 : num  0.999 0.979 0.985 0.998 0.994 ...
 $ V34 : num  0.585 0.633 0.878 1.355 0.880 ...
 $ V35 : num  1.40 1.18 1.55 1.99 1.62 ...
 $ V36 : num  0.906 1.156 1.661 1.981 1.372 ...
 $ V37 : num  0.375 0.881 1.445 1.603 0.605 ...
 $ V38 : num  0.462 1.016 1.448 1.766 0.718 ...
 $ V39 : num  0.424 0.509 1.322 1.754 0.566 ...
 $ V40 : num  0.341 0.514 1.393 1.786 0.546 ...
 $ V41 : num  -0.0681  0.8196  1.2009  1.6561  2.7571 ...
 $ V42 : num  -4.354 -1.388  0.761 -0.145 -3.107 ...
 $ V43 : num  0.478 0.341 0.501 0.983 0.511 ...
 $ V44 : num  0.341 0.274 0.504 1.108 0.447 ...
 $ V45 : num  0.440 0.196 0.631 1.076 0.535 ...
 $ V46 : num  0.873 0.326 0.933 1.354 0.977 ...
 $ V47 : num  -0.383 -0.170  0.686  0.843  0.328 ...
 $ V48 : num  0.138 0.384 1.332 1.352 0.217 ...
 $ V49 : num  -0.105  0.311  0.984  1.201 -0.196 ...
 $ V50 : num  -0.118  0.215  0.942  1.173 -0.233 ...
 $ V51 : num  -0.245  0.165  0.890  1.057 -0.354 ...
 $ V52 : num  -1.568 -0.577 -0.399 -0.748 -1.883 ...
 $ V53 : num  -1.530 -0.420 -0.264 -0.522 -1.430 ...
 $ V54 : num  0.331 0.264 0.324 0.574 0.308 ...
 $ V55 : num  0.426 0.497 1.209 1.296 0.901 ...
 $ V56 : num  0.149 0.282 1.028 0.888 0.277 ...
 $ V57 : num  0.384 0.430 1.039 1.387 0.541 ...
 $ V58 : num  0.334 0.420 1.033 1.348 0.524 ...
 $ V59 : num  0.780 0.866 0.792 1.296 2.664 ...
 $ V60 : num  -8.25 -3.22  6.06  3.82 -2.95 ...
 $ V61 : Factor w/ 20 levels "1","2","3","4",..: 3 14 8 3 14 10 9 16 17 14 ...
 $ V62 : num  0.589 1.062 1.083 0.721 2.764 ...
 $ V63 : num  0.830 0.878 1.030 1.218 3.371 ...
 $ V64 : num  0.0477 0.0183 0.0195 0.0535 0.0230 ...
 $ V65 : num  1.01 1.04 1.04 1.05 1.07 ...
 $ V66 : num  1.00 1.00 1.01 1.01 1.01 ...
 $ V67 : num  1.01 1.02 1.04 1.05 1.06 ...
 $ V68 : num  0.865 1.181 0.797 0.863 1.584 ...
 $ V69 : num  0.955 1.105 0.876 0.953 1.434 ...
 $ V70 : num  0.769 0.974 1.036 0.935 1.809 ...
 $ V71 : num  0.665 1.150 0.826 0.807 2.866 ...
 $ V72 : num  1.001 0.999 0.997 0.999    NA ...
 $ V73 : num  0.998 0.000    NA 0.992    NA ...
 $ V74 : num  1462 1462 1462 1462 1462 ...
 $ V75 : num  1 1 1 1 1 ...
 $ V76 : num  1.00 1.00 1.00 1.00 1.00 ...
 $ V77 : num  1.00 1.00 1.00 1.00 1.00 ...
 $ V78 : num  1.00 1.00 1.00 1.00 1.00 ...
 $ V79 : num  1.17 4.25 0.37 0.60   NA ...
 $ V80 : num  0.4375 0.6296 0.0855 0.0411 4.3333 ...
 $ V81 : num  0.500 0.160    NA 0.167    NA ...
 $ V82 : num  0.0625 0.0000 0.4444 0.2740 1.0000 ...
 $ V83 : num  0.0714 0.0000     NA 1.1111     NA ...
 $ V84 : num   3  1  3 NA NA ...
 $ V85 : num  0.600 0.667 0.750 0.667 1.000 ...
 $ V86 : num  0.200 0.667 0.500 0.667 3.000 ...
 $ V87 : num  4.16 4.16 4.16 4.16 4.16 ...
 $ V88 : int  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...
 $ V89 : num  1 1 1 1 1 ...
 $ V90 : num  1.01 1.01 1.01 1.01 1.01 ...
 $ V91 : num  1.02 1.02 1.02 1.02 1.02 ...
 $ V92 : num  1.00 1.00 1.00 1.00 1.00 ...
 $ V93 : num  0.998 0.998 0.998 0.998 0.998 ...
 $ V94 : num   0.28851 -0.00130  0.16621  0.19513  0.15963 ...
 $ V95 : num   0.2804 -0.1910  0.1693  0.2661  0.0609 ...
 $ V96 : num   0.290 -0.238  0.233  0.287  0.147 ...
 $ V97 : num   0.4559 -0.4030  0.0401  0.0264 -0.0420 ...
 $ V98 : num  -1.64 -1.58 -1.15 -1.90 -1.48 ...
 $ V99 : num  -1.47 -1.47 -1.19 -2.30 -1.85 ...
 $ V100: num  -1.350 -1.517 -0.362 -2.072 -1.323 ...
 $ V101: num  -1.070 -0.450 -1.064 -1.175 -0.453 ...
 $ V102: num  -1.038 -0.183 -0.948 -1.094 -0.355 ...
 $ V103: num  -1.093 -0.215 -1.019 -1.205 -0.399 ...
 $ V104: num  -0.9914  0.0897 -0.1980 -0.1433 -0.2038 ...
 $ V105: num  -2.168 -0.535 -0.850 -1.161 -2.329 ...
 $ V106: num  1.00 1.00 1.00 1.00 1.00 ...
 $ V107: num  0.261 0.119 0.199 0.248 0.214 ...
 $ V108: num   0.2236 -0.0521  0.1689  0.2283  0.1619 ...
 $ V109: num   0.247 -0.127  0.128  0.198  0.159 ...
 $ V110: num   0.22516 -0.25404 -0.10006 -0.00692  0.00511 ...
 $ V111: num  -0.994 -0.720 -0.796 -1.127 -0.684 ...
 $ V112: num  -0.707 -0.188 -0.492 -1.176 -0.445 ...
 $ V113: num  -0.535  0.150 -0.566 -0.864 -0.183 ...
 $ V114: num  -0.636 -0.296 -0.812 -1.070 -0.365 ...
 $ V115: num  -0.596 -0.263 -0.755 -0.964 -0.318 ...
 $ V116: num  -0.6744 -0.0153 -0.0433 -0.0560 -0.2140 ...
 $ V117: num  -1.195 -0.316 -0.298 -0.489 -1.175 ...
 $ V118: num  0.974 0.974 0.974 0.974 0.974 ...
 $ V119: num  0.98 0.98 0.98 0.98 0.98 ...
 $ V120: num  1.01 1.01 1.01 1.01 1.01 ...
 $ V121: num   0.00896  0.02651 -0.04939  0.00899 -0.08663 ...
 $ V122: num   0.0738 -0.1606 -0.1370 -0.1215 -0.2073 ...
 $ V123: num   0.0198 -0.3605 -0.2717 -0.1872 -0.3372 ...
 $ V124: num   0.16734 -0.41323 -0.10217 -0.05534  0.00103 ...
 $ V125: num  -0.601 -0.845 -0.541 -0.669 -0.304 ...
 $ V126: num  -0.803 -1.478 -1.113 -1.634 -1.208 ...
 $ V127: num  -0.201 -1.387 -1.049 -1.007 -0.366 ...
 $ V128: num  -0.0721 -0.3654 -1.3118 -1.6504  0.2691 ...
 $ V129: num  -0.0444 -0.3489 -1.2789 -1.5970  0.2938 ...
 $ V130: num  2.52 1.28 1.79 2.53 4.80 ...
 $ V131: num  3.572 0.769 2.283 2.694 4.399 ...
 $ V132: num  0.379 0.295 0.352 0.541 0.373 ...
 $ V133: num  0.401 0.264 0.488 0.728 0.554 ...
 $ V134: num  0.859 0.214 0.572 0.801 0.683 ...
 $ V135: num  0.367 0.149 1.021 1.161 0.480 ...
 $ V136: num  0.2451 0.0938 0.7866 1.1074 0.2471 ...
 $ V137: num  0.290 0.357 0.933 1.231 0.353 ...
 $ V138: num  0.238 0.343 0.922 1.188 0.320 ...
 $ V139: num  -0.00656  1.05492  0.84693  1.45898  2.93747 ...
 $ V140: num  -9.64 -2.58 -2.16 -3.73 -9.33 ...
 $ V141: Factor w/ 88 levels "1001","1002",..: 59 59 59 59 59 59 55 78 7 73 ...
 $ V142: Factor w/ 3 levels "1","2","3": 3 3 3 3 3 3 3 3 3 3 ...



From andy_liaw at merck.com  Tue Jan 25 20:58:04 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 25 Jan 2005 14:58:04 -0500
Subject: [R] multi-class classification using rpart
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5B1@usrymx25.merck.com>

> From: WeiWei Shi
> 
> Hi,
> I am trying to make a multi-class classification tree by using rpart.
> I used MASS package'd data: fgl to test and it works well.
> 
> However, when I used my small-sampled data as below, the program seems
> to take forever. I am not sure if it is due to slowness or there is
> something wrong with my codes or data manipulation.
> 
> Please be advised !
> 
> The data is described as the output from str() function. The call to
> rpart is like:
> 
> library(rpart)
> test_tree<-rpart(x$V142 ~ ., data=x, 
> parms=list(split='gini'), cp =0.01)
> 
> the response variable is $V142, with 3 levels.
> 
> Thanks for your suggestions!
> 
> Ed.

[snip]

>  $ V141: Factor w/ 88 levels "1001","1002",..: 59 59 59 59 59 
> 59 55 78 7 73 ...

I'd bet this is the problem.  There are 2^(88-1) - 1 possible ways to split
a factor with 88 levels.  It will work on those splits til the cows come
home...

I'd suggest getting rid of that variable, or collapse the levels to
something more reasonable.  The CART book describes some heuristic shortcuts
for testing only n-1 splits for factors with n levels, but I believe that
only works for 2-class problems, if I'm not mistaken.

Andy



From luke at stat.uiowa.edu  Tue Jan 25 21:06:35 2005
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Tue, 25 Jan 2005 14:06:35 -0600 (CST)
Subject: [R] Parallel computations using snow: how to combine boot objects?
In-Reply-To: <24D0F1947691984E89F3151A7DC314DDAC1FCB@EXCHANGE2.unifr.ch>
References: <24D0F1947691984E89F3151A7DC314DDAC1FCB@EXCHANGE2.unifr.ch>
Message-ID: <Pine.LNX.4.58.0501251406210.13053@nokomis.stat.uiowa.edu>

The example ducks that issue. Someone needs to write a function for
merging these results.  Probably just involves making a suitable call
to boot.return, which is what happens at the end of boot(), but I
don't know if anyone has actually done this yet.

luke

On Fri, 21 Jan 2005, BEER Michael wrote:

> Hello,
> 
> I'm trying to do some bootstrapping in a parallel environment (Linux
> cluster) in order to estimate confidence intervals for a certain
> parameter. Following the example in the documentation of the "snow"
> package (http://www.stat.uiowa.edu/~luke/R/cluster/cluster.html), I
> launch my computations by something like
> 
> > cl.nuke.boot <-
> +             clusterCall(cl,boot,nuke.data, nuke.fun, R=500, m=1,
> +                         fit.pred=new.fit, x.pred=new.data)
> 
> which gives me a list of n boot objects (where n is the number of nodes
> in my cluster). So far, so good.
> 
> However, if I now want to go further, I need to combine all these boot
> objects to a single one which I can pass to boot.ci for example. Is
> there a recommended way to do this?
> 
> Thanks,
> Michael
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From helprhelp at gmail.com  Tue Jan 25 21:14:37 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Tue, 25 Jan 2005 15:14:37 -0500
Subject: [R] multi-class classification using rpart
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E5B1@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E5B1@usrymx25.merck.com>
Message-ID: <cdf8178305012512142f4ed7ff@mail.gmail.com>

Hi, Andy:
Thanks. It works after I removed the variable. I think I got a similar
problem when I used randomForest. And I am not sure if they were due
to the same reason.

Practically and Unfortunately, that variable is very important to the
accuracy. I am wondering if there is another way besides collapsing
it. BTW, I remember you mentioned some alternative implementation to
randomForest (the author provided) to avoid the upper limit (32, if I
am correct) for the level of factor which can be used in the R
version's randomForest.

Thanks for further assistance!

Ed

On Tue, 25 Jan 2005 14:58:04 -0500, Liaw, Andy <andy_liaw at merck.com> wrote:
> > From: WeiWei Shi
> >
> > Hi,
> > I am trying to make a multi-class classification tree by using rpart.
> > I used MASS package'd data: fgl to test and it works well.
> >
> > However, when I used my small-sampled data as below, the program seems
> > to take forever. I am not sure if it is due to slowness or there is
> > something wrong with my codes or data manipulation.
> >
> > Please be advised !
> >
> > The data is described as the output from str() function. The call to
> > rpart is like:
> >
> > library(rpart)
> > test_tree<-rpart(x$V142 ~ ., data=x,
> > parms=list(split='gini'), cp =0.01)
> >
> > the response variable is $V142, with 3 levels.
> >
> > Thanks for your suggestions!
> >
> > Ed.
> 
> [snip]
> 
> >  $ V141: Factor w/ 88 levels "1001","1002",..: 59 59 59 59 59
> > 59 55 78 7 73 ...
> 
> I'd bet this is the problem.  There are 2^(88-1) - 1 possible ways to split
> a factor with 88 levels.  It will work on those splits til the cows come
> home...
> 
> I'd suggest getting rid of that variable, or collapse the levels to
> something more reasonable.  The CART book describes some heuristic shortcuts
> for testing only n-1 splits for factors with n levels, but I believe that
> only works for 2-class problems, if I'm not mistaken.
> 
> Andy
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From falimadhi at latte.harvard.edu  Tue Jan 25 21:23:22 2005
From: falimadhi at latte.harvard.edu (Ferdinand Alimadhi)
Date: Tue, 25 Jan 2005 15:23:22 -0500
Subject: [R] Systematic and stochastic components SUR, 2SLS, W2SLS, 3SLS
Message-ID: <41F6AABA.4070400@latte.harvard.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050125/3902d920/attachment.pl

From arshia22 at yahoo.com  Tue Jan 25 21:58:07 2005
From: arshia22 at yahoo.com (ebashi)
Date: Tue, 25 Jan 2005 12:58:07 -0800 (PST)
Subject: [R] How to make R faster?
Message-ID: <20050125205807.67486.qmail@web81004.mail.yahoo.com>

Dear R users;
I am using R for a project. I have some PHP forms that
pass parameters to R for calculations, and publish the
result in HTML format by CGIwithR. I'm using a Linux
machine and every things work perfectly. However, it
is  too slow, it takes 5 to 10 seconds to run, and
even  if I start R from the Shell it takes the same
amount of time, which is probably due to installing
packages. My first question is that how can i make R
run faster? and second if I am supposed to reduce the
packages which are being loaded at initiation of R,
how can I limit it to only the packages that i want?
and third how can i make R not to get open each time,
and let it sit on the server so that, when i pass
something to it , i get result faster?

Sincerely,
Sean



From helprhelp at gmail.com  Tue Jan 25 21:59:29 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Tue, 25 Jan 2005 15:59:29 -0500
Subject: Collapsing solution to the question discussed above: Re: [R]
	multi-class classification using rpart
In-Reply-To: <41F6B0D6.50506@statistik.uni-dortmund.de>
References: <3A822319EB35174CA3714066D590DCD50994E5B1@usrymx25.merck.com>
	<cdf8178305012512142f4ed7ff@mail.gmail.com>
	<41F6B0D6.50506@statistik.uni-dortmund.de>
Message-ID: <cdf8178305012512593a1f45a0@mail.gmail.com>

Hi, All:
The variable is used to encode industries: like computer science,
electronics and so on. Therefore, there is no order in them.

My previous effforts indicate that grouping  them according to some
domain knowledge decreases the accuracy. However, using some
"distance" or "entropy" is my current thought to collapse them since
it is a classification problem. I am searching for some papers which
discussed on this topic.

Anyone has more ideas or info like paper?

Thanks.

Ed


On Tue, 25 Jan 2005 21:49:26 +0100, Uwe Ligges
<ligges at statistik.uni-dortmund.de> wrote:
> WeiWei Shi wrote:
> 
> > Hi, Andy:
> > Thanks. It works after I removed the variable. I think I got a similar
> > problem when I used randomForest. And I am not sure if they were due
> > to the same reason.
> >
> > Practically and Unfortunately, that variable is very important to the
> > accuracy. I am wondering if there is another way besides collapsing
> > it. BTW, I remember you mentioned some alternative implementation to
> > randomForest (the author provided) to avoid the upper limit (32, if I
> > am correct) for the level of factor which can be used in the R
> > version's randomForest.
> >
> > Thanks for further assistance!
> 
> 
> So you *really* want it to be factor?! Thought it was a mistake not to
> have it numerical....
> Amazing! Maybe computers are sometimes even too fast these days.
> 
> Uwe
> 
> 
> > Ed
> >
> > On Tue, 25 Jan 2005 14:58:04 -0500, Liaw, Andy <andy_liaw at merck.com> wrote:
> >
> >>>From: WeiWei Shi
> >>>
> >>>Hi,
> >>>I am trying to make a multi-class classification tree by using rpart.
> >>>I used MASS package'd data: fgl to test and it works well.
> >>>
> >>>However, when I used my small-sampled data as below, the program seems
> >>>to take forever. I am not sure if it is due to slowness or there is
> >>>something wrong with my codes or data manipulation.
> >>>
> >>>Please be advised !
> >>>
> >>>The data is described as the output from str() function. The call to
> >>>rpart is like:
> >>>
> >>>library(rpart)
> >>>test_tree<-rpart(x$V142 ~ ., data=x,
> >>>parms=list(split='gini'), cp =0.01)
> >>>
> >>>the response variable is $V142, with 3 levels.
> >>>
> >>>Thanks for your suggestions!
> >>>
> >>>Ed.
> >>
> >>[snip]
> >>
> >>
> >>> $ V141: Factor w/ 88 levels "1001","1002",..: 59 59 59 59 59
> >>>59 55 78 7 73 ...
> >>
> >>I'd bet this is the problem.  There are 2^(88-1) - 1 possible ways to split
> >>a factor with 88 levels.  It will work on those splits til the cows come
> >>home...
> >>
> >>I'd suggest getting rid of that variable, or collapse the levels to
> >>something more reasonable.  The CART book describes some heuristic shortcuts
> >>for testing only n-1 splits for factors with n levels, but I believe that
> >>only works for 2-class problems, if I'm not mistaken.
> >>
> >>Andy
> >>
> >>------------------------------------------------------------------------------
> >>Notice:  This e-mail message, together with any attachment...{{dropped}}
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From spencer.graves at pdf.com  Tue Jan 25 22:26:15 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 25 Jan 2005 13:26:15 -0800
Subject: [R] How to make R faster?
In-Reply-To: <20050125205807.67486.qmail@web81004.mail.yahoo.com>
References: <20050125205807.67486.qmail@web81004.mail.yahoo.com>
Message-ID: <41F6B977.3080500@pdf.com>

      My standard algorithm for improving speed is as follows: 

      1.  Identify what takes the most time. 

      2.  Try to find ways in R to speed it up, e.g., converting loops 
to vector operations.  Many tasks in R can be performed in a variety of 
different ways to get the same result but with different time. 

      3.  If that fails, convert the most time consuming part into 
compiled code and link to it. 

      Use "system.time" to time a single expression and "proc.time" to 
store the time at multiple places in your code;  precede any test with 
garbage collection (gc) to reduce variations in the answers. 

      An "R site search" (www.r-project.org -> Search -> "R site 
search") for "timing R" produced 126 hits, the second of which gave 
"system.time";  you might skim the rest for other ideas.  A similar 
search for "compute speed" produced 64 hits, some of which will 
doubtless interest you if you haven't already read them. 

      For more help on this, please be more specific, e.g., following 
the posting guide! "http://www.R-project.org/posting-guide.html". 

      hope this helps.  spencer graves

ebashi wrote:

>Dear R users;
>I am using R for a project. I have some PHP forms that
>pass parameters to R for calculations, and publish the
>result in HTML format by CGIwithR. I'm using a Linux
>machine and every things work perfectly. However, it
>is  too slow, it takes 5 to 10 seconds to run, and
>even  if I start R from the Shell it takes the same
>amount of time, which is probably due to installing
>packages. My first question is that how can i make R
>run faster? and second if I am supposed to reduce the
>packages which are being loaded at initiation of R,
>how can I limit it to only the packages that i want?
>and third how can i make R not to get open each time,
>and let it sit on the server so that, when i pass
>something to it , i get result faster?
>
>Sincerely,
>Sean
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From reid_huntsinger at merck.com  Tue Jan 25 22:45:34 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 25 Jan 2005 16:45:34 -0500
Subject: Collapsing solution to the question discussed above: Re:
	[R] multi-class classification using rpart
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9283@uswpmx00.merck.com>

You could break your 3 class problem into several (2 or 3) 2 class problems,
and then use Andy's suggestion (see the CART book). There are several ways
to break the problem into 2 class problems, and several ways to combine the
resulting classifiers. Tom Dietterich, Jerry Friedman, Trevor Hastie and Rob
Tibshirani, among others, have articles on the question, in places like
Annals of Statistics, Machine Learning from the mid-to-late 90s. 

Alternatively, or in addition, you could look at the simulated annealing
approach to searching for a good split for a categorical variable in
Quinlan's C4.5 book and implement that in R. 

There are also many ways to create "indices" to use in place of the
categorical variable. These often depend on some hierarchical structure,
like with SIC codes.

Reid Huntsinger



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of WeiWei Shi
Sent: Tuesday, January 25, 2005 3:59 PM
To: Uwe Ligges
Cc: R-help at stat.math.ethz.ch; Liaw, Andy
Subject: Collapsing solution to the question discussed above: Re: [R]
multi-class classification using rpart


Hi, All:
The variable is used to encode industries: like computer science,
electronics and so on. Therefore, there is no order in them.

My previous effforts indicate that grouping  them according to some
domain knowledge decreases the accuracy. However, using some
"distance" or "entropy" is my current thought to collapse them since
it is a classification problem. I am searching for some papers which
discussed on this topic.

Anyone has more ideas or info like paper?

Thanks.

Ed


On Tue, 25 Jan 2005 21:49:26 +0100, Uwe Ligges
<ligges at statistik.uni-dortmund.de> wrote:
> WeiWei Shi wrote:
> 
> > Hi, Andy:
> > Thanks. It works after I removed the variable. I think I got a similar
> > problem when I used randomForest. And I am not sure if they were due
> > to the same reason.
> >
> > Practically and Unfortunately, that variable is very important to the
> > accuracy. I am wondering if there is another way besides collapsing
> > it. BTW, I remember you mentioned some alternative implementation to
> > randomForest (the author provided) to avoid the upper limit (32, if I
> > am correct) for the level of factor which can be used in the R
> > version's randomForest.
> >
> > Thanks for further assistance!
> 
> 
> So you *really* want it to be factor?! Thought it was a mistake not to
> have it numerical....
> Amazing! Maybe computers are sometimes even too fast these days.
> 
> Uwe
> 
> 
> > Ed
> >
> > On Tue, 25 Jan 2005 14:58:04 -0500, Liaw, Andy <andy_liaw at merck.com>
wrote:
> >
> >>>From: WeiWei Shi
> >>>
> >>>Hi,
> >>>I am trying to make a multi-class classification tree by using rpart.
> >>>I used MASS package'd data: fgl to test and it works well.
> >>>
> >>>However, when I used my small-sampled data as below, the program seems
> >>>to take forever. I am not sure if it is due to slowness or there is
> >>>something wrong with my codes or data manipulation.
> >>>
> >>>Please be advised !
> >>>
> >>>The data is described as the output from str() function. The call to
> >>>rpart is like:
> >>>
> >>>library(rpart)
> >>>test_tree<-rpart(x$V142 ~ ., data=x,
> >>>parms=list(split='gini'), cp =0.01)
> >>>
> >>>the response variable is $V142, with 3 levels.
> >>>
> >>>Thanks for your suggestions!
> >>>
> >>>Ed.
> >>
> >>[snip]
> >>
> >>
> >>> $ V141: Factor w/ 88 levels "1001","1002",..: 59 59 59 59 59
> >>>59 55 78 7 73 ...
> >>
> >>I'd bet this is the problem.  There are 2^(88-1) - 1 possible ways to
split
> >>a factor with 88 levels.  It will work on those splits til the cows come
> >>home...
> >>
> >>I'd suggest getting rid of that variable, or collapse the levels to
> >>something more reasonable.  The CART book describes some heuristic
shortcuts
> >>for testing only n-1 splits for factors with n levels, but I believe
that
> >>only works for 2-class problems, if I'm not mistaken.
> >>
> >>Andy
> >>
>
>>--------------------------------------------------------------------------
----
> >>Notice:  This e-mail message, together with any attachment...{{dropped}}
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
> 
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From wgshi2001 at yahoo.ca  Tue Jan 25 22:53:47 2005
From: wgshi2001 at yahoo.ca (Weiguang Shi)
Date: Tue, 25 Jan 2005 16:53:47 -0500 (EST)
Subject: [R] Zipf random number generation
In-Reply-To: <XFMail.050125102057.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20050125215347.86818.qmail@web30005.mail.mud.yahoo.com>

Thanks very much Ted for the detailed explanation.

It might be flawed but some "common practices" in my
area use the following approach to generate a random
rank for some Zipf-like distribution with the
parameter alpha.

The key is to introduce the limit of ranks, N. With N
and alpha, therefore, the probability of rank i is
determined by 
    c/pow((double) i, alpha)
where
    c = sum(1.0 / pow((double) i, alpha))

Any comment on that?

Thanks,
Weiguang

 --- Ted.Harding at nessie.mcc.ac.uk wrote: 
> On 25-Jan-05 Weiguang Shi wrote:
> > Hi,
> > 
> > Is there a Zipf-like distribution RNG in R?
> > 
> > Thanks,
> > Weiguang
> 
> "Zipf's Law" (as originally formulated in studies of
> the
> frequencies of words in texts) is to the effect that
> the
> relative frequencies with which words occur once,
> twice,
> 3 times, ... are in proportion to 1/1, 1/2, 1/3, ...
> ,
> 1/n, ... with no limit on n (i.e. the number of
> different
> words each represented n times is proportional to
> 1/n).
> 
> This is "improper" since sum(1/n) = infinity, so
> does not
> define a probability distribution. A respectable
> analogue
> is Fisher's logarithmic distribution p(x) where
> 
>   p(x) = ((t^x)/x)/(-log(1-t)), x = 1, 2, 3, ...
> 
> where t in (0,1) is the parameter of the
> distribution.
> 
> As t -> 1, p(x+1)/p(x) -> x/(x+1) as in Zipf's Law.
> 
> However, I've searched the R site and have found
> only
> one instance of a function directly related to this
> distribution, namely logff() in package VGAM, which
> is for estimating the parameter t.
> 
> So it looks as though there is no direct
> implementation
> of something like "rlogdist".
> 
> However, the logarithmic distribution is a limiting
> form of the negative binomial distribution
> (conditional
> on a positive value), and there are functions in R
> for
> random sampling from this distribution.
> 
> From my reading of "?rnbinom" in the base package,
> this
> function does not have the flexibitility you would
> need
> for this. But in the MASS package there is the
> function
> rnegbin() which does.
> 
> You would need to invoke
> 
>   rnegbin(N, mu=..., theta=... )
> 
> where the value ... of mu is large and the value ...
> of
> theta is small, and reject cases with value zero.
> This
> of course makes it awkward to generate a sample of
> given
> size.
> 
> Alternatively, you can try along the lines of
> 
> > x0<-1:10000; t<-0.999; p<-((t^x0)/x0)/(-log(1-t))
> > Y<-sample(x0,5000,replace=TRUE,prob=p)
> > hist(Y,breaks=100)
> 
> While this gives the logarithmic distribution over
> the
> range of x in x0, it is inexact in that it does not
> permit values greater than max(x0) to be sampled.
> 
> No doubt others can suggest something better than
> this!
> 
> Best wishes,
> Ted.
> 
> 
>
--------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> Date: 25-Jan-05                                     
>  Time: 10:20:57
> ------------------------------ XFMail
> ------------------------------
>



From Benjamin.Osborne at uvm.edu  Tue Jan 25 23:31:33 2005
From: Benjamin.Osborne at uvm.edu (Benjamin M. Osborne)
Date: Tue, 25 Jan 2005 17:31:33 -0500
Subject: [R] chron: parsing dates into a data frame using a forloop
Message-ID: <1106692293.41f6c8c59b4a8@webmail.uvm.edu>

I have one data frame with a column of dates and I want to fill another data
frame with one column of dates, one of years, one of months, one of a unique
combination of year and month, and one of days, but R seems to have some
problems with this.  My initial data frame looks like this (ignore the NAs in
the other fields):

> mans[1:10,]
       date loc snow.new prcp tmin snow.dep tmax
1  11/01/54   2       NA   NA   NA       NA   NA
2  11/02/54   2       NA   NA   NA       NA   NA
3  11/03/54   2       NA   NA   NA       NA   NA
4  11/04/54   2       NA   NA   NA       NA   NA
5  11/05/54   2       NA   NA   NA       NA   NA
6  11/06/54   2       NA   NA   NA       NA   NA
7  11/07/54   2       NA   NA   NA       NA   NA
8  11/08/54   2       NA   NA   NA       NA   NA
9  11/09/54   2       NA   NA   NA       NA   NA
10 11/10/54   2       NA   NA   NA       NA   NA
>

The code and resultant data frame look like this:

> for(i in 1:10){
+ mans.met$date[i]<-mans$date[i]
+ mans.met$year[i]<-years(mans.met$date[i])
+ mans.met$month[i]<-months(mans.met$date[i])
+ mans.met$yearmo[i]<-cut(mans.met$date[i], "months")
+ mans.met$day[i]<-days(mans.met$date[i])
+ }
> mans.met[1:10,]
       date year month yearmo day snow.new snow.dep prcp tmin tmax tmean
1  11/01/54    1    11      1   1       NA       NA   NA   NA   NA    NA
2  11/02/54    1    11      1   2       NA       NA   NA   NA   NA    NA
3  11/03/54    1    11      1   3       NA       NA   NA   NA   NA    NA
4  11/04/54    1    11      1   4       NA       NA   NA   NA   NA    NA
5  11/05/54    1    11      1   5       NA       NA   NA   NA   NA    NA
6  11/06/54    1    11      1   6       NA       NA   NA   NA   NA    NA
7  11/07/54    1    11      1   7       NA       NA   NA   NA   NA    NA
8  11/08/54    1    11      1   8       NA       NA   NA   NA   NA    NA
9  11/09/54    1    11      1   9       NA       NA   NA   NA   NA    NA
10 11/10/54    1    11      1  10       NA       NA   NA   NA   NA    NA
>

The problem seems to be with assigning within the forloop, or making the
assignment into a data frame, since:

> years(mans.met$date[5])
[1] 1954
Levels: 1954
> test<-years(mans.met$date[5])
> test
[1] 1954
Levels: 1954
>
> months(mans.met$date[5])
[1] Nov
12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec
> test<-months(mans.met$date[5])
> test
[1] Nov
12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec
>
> cut(mans.met$date[3], "months")
[1] Nov 54
Levels: Nov 54
> test<-cut(mans.met$date[3], "months")
> test
[1] Nov 54
Levels: Nov 54
>
> days(mans.met$date[4])
[1] 4
31 Levels: 1 < 2 < 3 < 4 < 5 < 6 < 7 < 8 < 9 < 10 < 11 < 12 < 13 < ... < 31
> test<-days(mans.met$date[4])
> test
[1] 4
31 Levels: 1 < 2 < 3 < 4 < 5 < 6 < 7 < 8 < 9 < 10 < 11 < 12 < 13 < ... < 31
>

Any suggestions will be appreciated.
-Ben Osborne
-- 
Botany Department
University of Vermont
109 Carrigan Drive
Burlington, VT 05405

benjamin.osborne at uvm.edu
phone: 802-656-0297
fax: 802-656-0440



From wgshi2001 at yahoo.ca  Tue Jan 25 23:46:38 2005
From: wgshi2001 at yahoo.ca (Weiguang Shi)
Date: Tue, 25 Jan 2005 17:46:38 -0500 (EST)
Subject: [R] Zipf random number generation
In-Reply-To: <20050125215347.86818.qmail@web30005.mail.mud.yahoo.com>
Message-ID: <20050125224638.92054.qmail@web30001.mail.mud.yahoo.com>

Sorry. 

c = sum(1.0 / pow((double) j, alpha)), j=1,2,...,N

 --- Weiguang Shi <wgshi2001 at yahoo.ca> wrote: 
> Thanks very much Ted for the detailed explanation.
> 
> It might be flawed but some "common practices" in my
> area use the following approach to generate a random
> rank for some Zipf-like distribution with the
> parameter alpha.
> 
> The key is to introduce the limit of ranks, N. With
> N
> and alpha, therefore, the probability of rank i is
> determined by 
>     c/pow((double) i, alpha)
> where
>     c = sum(1.0 / pow((double) i, alpha))
> 
> Any comment on that?
> 
> Thanks,
> Weiguang
> 
>  --- Ted.Harding at nessie.mcc.ac.uk wrote: 
> > On 25-Jan-05 Weiguang Shi wrote:
> > > Hi,
> > > 
> > > Is there a Zipf-like distribution RNG in R?
> > > 
> > > Thanks,
> > > Weiguang
> > 
> > "Zipf's Law" (as originally formulated in studies
> of
> > the
> > frequencies of words in texts) is to the effect
> that
> > the
> > relative frequencies with which words occur once,
> > twice,
> > 3 times, ... are in proportion to 1/1, 1/2, 1/3,
> ...
> > ,
> > 1/n, ... with no limit on n (i.e. the number of
> > different
> > words each represented n times is proportional to
> > 1/n).
> > 
> > This is "improper" since sum(1/n) = infinity, so
> > does not
> > define a probability distribution. A respectable
> > analogue
> > is Fisher's logarithmic distribution p(x) where
> > 
> >   p(x) = ((t^x)/x)/(-log(1-t)), x = 1, 2, 3, ...
> > 
> > where t in (0,1) is the parameter of the
> > distribution.
> > 
> > As t -> 1, p(x+1)/p(x) -> x/(x+1) as in Zipf's
> Law.
> > 
> > However, I've searched the R site and have found
> > only
> > one instance of a function directly related to
> this
> > distribution, namely logff() in package VGAM,
> which
> > is for estimating the parameter t.
> > 
> > So it looks as though there is no direct
> > implementation
> > of something like "rlogdist".
> > 
> > However, the logarithmic distribution is a
> limiting
> > form of the negative binomial distribution
> > (conditional
> > on a positive value), and there are functions in R
> > for
> > random sampling from this distribution.
> > 
> > From my reading of "?rnbinom" in the base package,
> > this
> > function does not have the flexibitility you would
> > need
> > for this. But in the MASS package there is the
> > function
> > rnegbin() which does.
> > 
> > You would need to invoke
> > 
> >   rnegbin(N, mu=..., theta=... )
> > 
> > where the value ... of mu is large and the value
> ...
> > of
> > theta is small, and reject cases with value zero.
> > This
> > of course makes it awkward to generate a sample of
> > given
> > size.
> > 
> > Alternatively, you can try along the lines of
> > 
> > > x0<-1:10000; t<-0.999;
> p<-((t^x0)/x0)/(-log(1-t))
> > > Y<-sample(x0,5000,replace=TRUE,prob=p)
> > > hist(Y,breaks=100)
> > 
> > While this gives the logarithmic distribution over
> > the
> > range of x in x0, it is inexact in that it does
> not
> > permit values greater than max(x0) to be sampled.
> > 
> > No doubt others can suggest something better than
> > this!
> > 
> > Best wishes,
> > Ted.
> > 
> > 
> >
>
--------------------------------------------------------------------
> > E-Mail: (Ted Harding)
> <Ted.Harding at nessie.mcc.ac.uk>
> > Fax-to-email: +44 (0)870 094 0861  [NB: New
> number!]
> > Date: 25-Jan-05                                   
>  
> >  Time: 10:20:57
> > ------------------------------ XFMail
> > ------------------------------
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Tue Jan 25 23:55:09 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 25 Jan 2005 17:55:09 -0500
Subject: [R] multi-class classification using rpart
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5B2@usrymx25.merck.com>

> From: Uwe Ligges 
> 
> WeiWei Shi wrote:
> 
> > Hi, Andy:
> > Thanks. It works after I removed the variable. I think I 
> got a similar
> > problem when I used randomForest. And I am not sure if they were due
> > to the same reason.
> > 
> > Practically and Unfortunately, that variable is very 
> important to the
> > accuracy. I am wondering if there is another way besides collapsing
> > it. BTW, I remember you mentioned some alternative implementation to
> > randomForest (the author provided) to avoid the upper limit 
> (32, if I
> > am correct) for the level of factor which can be used in the R
> > version's randomForest.
> > 
> > Thanks for further assistance!
> 
> 
> So you *really* want it to be factor?! Thought it was a 
> mistake not to 
> have it numerical....
> Amazing! Maybe computers are sometimes even too fast these days.
> 
> Uwe

[Uwe: Not sure if you meant to keep this off-list.  If so, my most sincere
apologies.]

Er... not really.  Currently (classification) randomForest encode splits on
categorical variables by binary expansion of levels that go to the left.
Such split is stored in (4-byte) integers, thus the 32-level restriction.
In newer version of Breiman & Cutler's Fortran code, that restriction is
removed by storing the entire indicator matrix (# of nodes by max. number of
levels, then by number of trees in the forest).  For the stand-alone
Fortran, each tree is written to file as soon as it's grown, so it doesn't
need to store the entire forest in memory.  The R version has no such luxury
(if you can call it that).

The way the new RF Fortran code deals with categorical variables with more
than 10 categories is by randomly sampling some number (say 512) of random
splits and pick the best among them.  That's probably a good strategy for
random forests, but may not be what one would do to grow a single tree.

When growing a single tree with data containing categorical variables with
large number of categories, one should also be mindful of the problem that,
because of the greedy nature of the algorithm, it will tend to split on
variables with larger numbers of possible splits, even if those variables
are less `informative'.

Andy
 
> > Ed
> > 
> > On Tue, 25 Jan 2005 14:58:04 -0500, Liaw, Andy 
> <andy_liaw at merck.com> wrote:
> > 
> >>>From: WeiWei Shi
> >>>
> >>>Hi,
> >>>I am trying to make a multi-class classification tree by 
> using rpart.
> >>>I used MASS package'd data: fgl to test and it works well.
> >>>
> >>>However, when I used my small-sampled data as below, the 
> program seems
> >>>to take forever. I am not sure if it is due to slowness or there is
> >>>something wrong with my codes or data manipulation.
> >>>
> >>>Please be advised !
> >>>
> >>>The data is described as the output from str() function. 
> The call to
> >>>rpart is like:
> >>>
> >>>library(rpart)
> >>>test_tree<-rpart(x$V142 ~ ., data=x,
> >>>parms=list(split='gini'), cp =0.01)
> >>>
> >>>the response variable is $V142, with 3 levels.
> >>>
> >>>Thanks for your suggestions!
> >>>
> >>>Ed.
> >>
> >>[snip]
> >>
> >>
> >>> $ V141: Factor w/ 88 levels "1001","1002",..: 59 59 59 59 59
> >>>59 55 78 7 73 ...
> >>
> >>I'd bet this is the problem.  There are 2^(88-1) - 1 
> possible ways to split
> >>a factor with 88 levels.  It will work on those splits til 
> the cows come
> >>home...
> >>
> >>I'd suggest getting rid of that variable, or collapse the levels to
> >>something more reasonable.  The CART book describes some 
> heuristic shortcuts
> >>for testing only n-1 splits for factors with n levels, but 
> I believe that
> >>only works for 2-class problems, if I'm not mistaken.
> >>
> >>Andy
> >>
> >>------------------------------------------------------------
> ------------------
> >>Notice:  This e-mail message, together with any 
> attachment...{{dropped}}
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From emcintire at forestry.umt.edu  Wed Jan 26 00:01:58 2005
From: emcintire at forestry.umt.edu (Eliot McIntire)
Date: Tue, 25 Jan 2005 16:01:58 -0700
Subject: [R] Threshhold Models in gnlm
Message-ID: <opsk6kxktxw3ajb1@smtp.umt.edu>

Hello,

I am interested in fitting a generalized nonlinear regression (gnlr) model  
with negative binomial errors.

I have found Jim Lindsay's package that will do gnlr, but I have having  
trouble with the particular model I am interested in fitting.

It is a threshhold model, where below a certain value of one of the  
parameters being fitted, the model changes.

Here is a sample:

Cones: is counts of number of cones on each tree
dt: is the residual energy of that tree (basically how many stored  
reserves that tree has)
The model is this: if dt is above some threshhold (c), then the number of  
cones will be a function of b and a and c, if dt is below that threshhold  
(c), then the number of cones will simply be b

# Does not work
   gnlr.T1 = gnlr(Cones, distribution="negative binomial",
         mu= ~ b + (dt >=c) * a * ( dt - c ),
         pmu= list(b=6.86, a=1.64, c=-3), pshape=0)  #, a = 1.5, c = -3)

I fit the same model using nls (obviously without the negative binomial  
errors), and it works fine.
# Does work
   nls.T1 = nls(Cones ~ b + (dt >=c) * a * ( dt - c ),
            start=list(b=20, c=3.1, a=2),
            control = nls.control(tol=0.001, minFactor=1e-020))

Does anybody know how to do this?  Perhaps using if statements in a  
function statement?

The data are below.

Thank you,
Eliot

#data
Cones = c(0,8,50,6,1,0,160,6,44,33,21,0,0,85,0,25,105,0,
56,2,72,13,3,14,0,1,0,51,2,41,37,7,0,0,32,0,
15,47,0,31,0,34,10,9,6,0,0,0,37,2,15,9,5,0,
2,17,0,0,14,0,61,0,27,31,23,29,0,0,0,27,3,119,
8,2,0,0,86,0,3,18,0,50,0,62,15,5,23,0,4,0,
29,1,16,7,2,0,1,28,0,0,5,0,9,0,27,26,11,44,
2,5,0,75,4,33,14,7,0,2,49,0,11,35,0,17,1,26,
9,3,54,0,0,0,44,2,24,31,7,0,0,29,0,0,73,0,
51,0,31,15,2,35,0,11,0,22,0,39,3,4,0,0,41,0,
11,23,0,105,0,3,16,9,22,1,12,0,53,1,5,3,3,0,
0,40,0,4,23,0,0,0,65,5,2,6,0,0,0,55,3,26,
13,2,0,0,32,0,1,18,0,41,0)

dt = c(-50.237,-17.241,7.756,-9.247,17.749,49.746,
82.743,-44.261,-17.264,-28.268,-28.271,-16.274,16.722,
49.719,-2.285,30.712,38.709,-33.295,-0.298,-23.302,
7.695,-5.828,-9.101,-1.418,-3.834,8.596,20.816,35.112,
0.01,15.011,-8.43,-27.856,-17.75,-1.026,15.497,-0.029,
16.546,18.361,-11.723,5.228,-8.76,8.34,-1.193,-3.456,
-4.63,-2.736,5.205,13.172,21.257,-7.357,-0.832,-7.344,
-8.068,-5.238,1.948,7.07,-2.198,6.817,17.736,15.932,29.048,
-17.867,-2.762,9.729,-5.244,-11.358,-22.671,-4.238,14.883,
34.932,29.377,48.799,-47.257,-32.246,-11.604,10.52,32.168,
-32.614,-11.782,5.707,7.855,27.657,-2.883,16.239,1.918,
-2.359,3.036,-9.903,-0.183,5.193,14.244,-5.988,1.469,
-6.42,-5.687,-0.391,6.351,11.647,-10.362,-4.535,1.261,
1.92,7.376,3.634,8.698,-1.171,-4.799,6.073,-16.571,
2.254,17.531,37.315,-18.277,-3.332,-17.946,-14.212,
-4.237,11.923,25.387,-8.708,5.778,8.971,-12.236,1.094,
-3.02,8.44,-3.727,3.031,15.64,-22.902,-7.596,7.558,22.454,
-7.072,5.143,-4.727,-21.505,-14.087,0.39,15.316,2.162,
19.401,38.466,-14.101,7.45,-20.829,3.111,2.435,-0.731,
9.002,-14.355,-2.79,-2.291,8.986,-2.214,8.369,-19.982,
-11.986,-4.465,7.368,20.032,-6.837,9.398,17.321,15.297,
37.977,-42.582,-16.325,-0.741,-2.164,2.845,-5.706,6.191,
6.545,18.313,-23.571,-14.022,-8.946,-2.247,4.089,12.807,
21.207,-10.322,-1.388,4.372,-8.341,2.285,13.265,24.619,
-9.316,-6.736,-0.483,2.383,11.803,21.715,32.755,-9.863,
0.17,-12.836,-13.572,-4.425,5.919,15.819,-6.363,3.732,
13.005,5.14,15.17,-15.871)



-- 
Eliot McIntire
NSERC Post Doctoral Fellow
Department of Ecosystems and Conservation Science
College of Forestry and Conservation
University of Montana, Missoula, MT 59812
406-243-5239
fax: 406-243-4557
emcintire at forestry.umt.edu



From tchur at optushome.com.au  Wed Jan 26 00:07:37 2005
From: tchur at optushome.com.au (Tim Churches)
Date: Wed, 26 Jan 2005 10:07:37 +1100
Subject: [R] How to make R faster?
In-Reply-To: <20050125205807.67486.qmail@web81004.mail.yahoo.com>
References: <20050125205807.67486.qmail@web81004.mail.yahoo.com>
Message-ID: <41F6D139.2010201@optushome.com.au>

ebashi wrote:

>Dear R users;
>I am using R for a project. I have some PHP forms that
>pass parameters to R for calculations, and publish the
>result in HTML format by CGIwithR. I'm using a Linux
>machine and every things work perfectly. However, it
>is  too slow, it takes 5 to 10 seconds to run, and
>even  if I start R from the Shell it takes the same
>amount of time, which is probably due to installing
>packages. My first question is that how can i make R
>run faster? and second if I am supposed to reduce the
>packages which are being loaded at initiation of R,
>how can I limit it to only the packages that i want?
>and third how can i make R not to get open each time,
>and let it sit on the server so that, when i pass
>something to it , i get result faster?
>  
>
Have a look at RSOAP, which does exactly what you suggest and allows you 
to commuicate with the R session via SOAP. I'm sure there are SOAP 
libraries available for PHP. See 
http://research.warnes.net/projects/rzope/rsoap/

Tim C



From andy_liaw at merck.com  Wed Jan 26 00:06:58 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 25 Jan 2005 18:06:58 -0500
Subject: [R] How to make R faster?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5B3@usrymx25.merck.com>

A few things to add to what Spencer said:

- Please give more info about your setup, as the posting guide asks you to
(e.g., R version, OS [which Linux distro and which release?], hardware).
These things matter!  As an example, the time it takes to start an R process
was dramatically reduced since R-2.0.0.  If you are using older version, you
need to upgrade.

- See the example section of ?Startup, which has an example of how to
specify packages to be loaded at startup.

- There's a section on profiling R code in the `Writing R Extensions'
manual, that may help you pinpoint the bottleneck.

- Sometimes re-thinking the organization of the computation or algorithm can
make a huge difference.  I remember reading in an old computing book the
staggering difference between FFT and doing the computation the na?ve way.

Andy

> From: Spencer Graves
> 
>       My standard algorithm for improving speed is as follows: 
> 
>       1.  Identify what takes the most time. 
> 
>       2.  Try to find ways in R to speed it up, e.g., 
> converting loops 
> to vector operations.  Many tasks in R can be performed in a 
> variety of 
> different ways to get the same result but with different time. 
> 
>       3.  If that fails, convert the most time consuming part into 
> compiled code and link to it. 
> 
>       Use "system.time" to time a single expression and 
> "proc.time" to 
> store the time at multiple places in your code;  precede any 
> test with 
> garbage collection (gc) to reduce variations in the answers. 
> 
>       An "R site search" (www.r-project.org -> Search -> "R site 
> search") for "timing R" produced 126 hits, the second of which gave 
> "system.time";  you might skim the rest for other ideas.  A similar 
> search for "compute speed" produced 64 hits, some of which will 
> doubtless interest you if you haven't already read them. 
> 
>       For more help on this, please be more specific, e.g., following 
> the posting guide! "http://www.R-project.org/posting-guide.html". 
> 
>       hope this helps.  spencer graves
> 
> ebashi wrote:
> 
> >Dear R users;
> >I am using R for a project. I have some PHP forms that
> >pass parameters to R for calculations, and publish the
> >result in HTML format by CGIwithR. I'm using a Linux
> >machine and every things work perfectly. However, it
> >is  too slow, it takes 5 to 10 seconds to run, and
> >even  if I start R from the Shell it takes the same
> >amount of time, which is probably due to installing
> >packages. My first question is that how can i make R
> >run faster? and second if I am supposed to reduce the
> >packages which are being loaded at initiation of R,
> >how can I limit it to only the packages that i want?
> >and third how can i make R not to get open each time,
> >and let it sit on the server so that, when i pass
> >something to it , i get result faster?
> >
> >Sincerely,
> >Sean
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >  
> >
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From p.murrell at auckland.ac.nz  Wed Jan 26 00:16:50 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 26 Jan 2005 12:16:50 +1300
Subject: [R] some questions about font
References: <41F14789.7020808@northwestern.edu>
	<41F59675.40205@stat.auckland.ac.nz>
Message-ID: <41F6D362.1020608@stat.auckland.ac.nz>

Hi


Paul Murrell wrote:
> Hi
> 
> 
> Bobai Li wrote:
> 
>>
>> Hi,
>>
>> I have been using R to create some mathematical and statistical graphs 
>> for a book manuscript, but I got some problems:
>>
>> 1)  Some web positngs said that default typeface for math expressions 
>> is italic, but in my system (R 2.01 on WinXP), the default is regular 
>> font.
>> How can I change the default to ilatic?
> 
> 
> 
> expression(italic(whatever))
> 
> 
>> 2)  When use ComputerModern font,  (i.e., 
>> family=c("CM_regular_10.afm","CM_boldx_10.afm","cmti10.afm","cmbxti10.afm","CM_symbol_10.afm") 
>> ), some accented symbols are not available. For example, 
>> "expression(hat(beta))" will produce warning message like "font 
>> metrics unknown for character 94."
> 
> 
> 
> That appears to be a bug (that requires changes to the PostScript device 
> driver).  A nastyish workaround for hat(beta) is widehat(beta), but I 
> suspect there are other problems that this will not solve.


Some good news and some bad news.

The good news is that this can be worked around fairly simply (i.e., 
without installing a new version of R) as follows:
(i) make a copy of $R_HOME/library/grDevices/afm/ISOLatin1.enc (or 
possibly $R_HOME/afm/ISOLatin1.enc, depending on your R version) and 
call the copy CMISOLatin1.enc.  Modify CMISOLatin1.enc so that the 
second line starts with CMISOLatin1Encoding (rather than 
ISOLatin1Encoding) and (further down the file) change \asciicircum to 
\circumflex and \asciitilde to \tilde.
(ii) when opening your PostScript device, as well as specifying the 
family argument as you do above, specify encoding="CMISOLatin1".

The bad news is that R does not position the accents as well as LaTeX 
does it (e.g., a hat is not located above a beta in quite the same place).

Do you know about PSfrag ...?

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From cuiczhao at yahoo.com  Wed Jan 26 01:16:59 2005
From: cuiczhao at yahoo.com (Cuichang Zhao)
Date: Tue, 25 Jan 2005 16:16:59 -0800 (PST)
Subject: [R] modular in R
Message-ID: <20050126001659.22305.qmail@web30708.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050125/1d8c3f78/attachment.pl

From andy_liaw at merck.com  Wed Jan 26 01:31:07 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 25 Jan 2005 19:31:07 -0500
Subject: [R] modular in R
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5B4@usrymx25.merck.com>

> From: Cuichang Zhao
> 
> hello,
> i wonder what command should i used in R to do the modular. 
> right now i have a vector v <- c(3, 4, 5, 6), but i have 
> tried v%2 or v mod 2 or mod(v, 2) or modular(v, 2), and none 
> of these works.

It's `%%', as in v %% 2.  
  
> Also, how can i find more function command online, i have to 
> search in the mailing list questions, but it is not enough, 
> is that a better website i can search more about R online.

help.search("modulo") points you to the basic arithmetic operators in base,
where %% is documented.  You might also want to use
http://search.r-project.org/.  In the next version of R you can use
RSiteSearch().

Andy
  
> Thank you so much
>  
> Best Wishes
>  
>  
> C-Ming
>  
> Jan 25, 2005
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From p.dalgaard at biostat.ku.dk  Wed Jan 26 01:39:36 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Jan 2005 01:39:36 +0100
Subject: [R] GLM function with poisson distribution
In-Reply-To: <Pine.A41.4.61b.0501250838510.329090@homer06.u.washington.edu>
References: <20050125142227.95623.qmail@web40629.mail.yahoo.com>
	<Pine.A41.4.61b.0501250838510.329090@homer06.u.washington.edu>
Message-ID: <x2hdl53tx3.fsf@biostat.ku.dk>

Thomas Lumley <tlumley at u.washington.edu> writes:

> On Tue, 25 Jan 2005, Florian Menzel wrote:
> 
> > Hello all,
> > I found a weird result of the GLM function that seems
> > to be a bug.
> 
> No, the problem is that you are using the Wald test when the mle is
> infinite, which is always going to be unreliable.  It's even worse
> because you are using data that couldn't really have come from a
> Poisson distribution (because for a=1 you have mean 3 and variance 0).

Actually, that's rather immaterial. Try it with

b=c(rep(0,8),rmultinom(1,24,rep(1/8,8))) 

or even

b=c(rep(0,8),rpois(8,3))

(in the former case, notice that the LRT is constant).

The zero variance does add to the confusion if you begin to consider
underdispersed models, though.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Wed Jan 26 01:47:29 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Jan 2005 01:47:29 +0100
Subject: [R] Deleted objects keep coming back
In-Reply-To: <BAY101-F7772CCB5772DC91F2728AE8860@phx.gbl>
References: <BAY101-F7772CCB5772DC91F2728AE8860@phx.gbl>
Message-ID: <x2d5vt3tjy.fsf@biostat.ku.dk>

"Ken Termiso" <jerk_alert at hotmail.com> writes:

> Ah! I think this is what happened -- whenever I restarted R, it would
> automatically load the previous workspace and its objects (call this
> workspace 1). Then, when I would attempt to load another workspace
> (call this workspace 2), it would retain the objects from workspace 1
> in ADDITION to the objects loaded from workspace 2.
> 
> If I load up R, it will say previous workspace restored. If I then
> type >rm(list=ls()) to delete all objects, and then load another
> workspace, it will only have the objects from the workspace I loaded...
> 
> Am I correct here?

Yes, but I think you'd be better off to learn about R's command line
argument in particular --no-restore and --vanilla. If you're not
starting R from the command line, I'm sure there's a way to pass
arguments anyway...
 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gerifalte28 at hotmail.com  Wed Jan 26 02:00:08 2005
From: gerifalte28 at hotmail.com (F Z)
Date: Wed, 26 Jan 2005 01:00:08 +0000
Subject: [R] Fitting distribution with R: a contribute
In-Reply-To: <20050125093106.81240.qmail@web41210.mail.yahoo.com>
Message-ID: <BAY103-F1591377E2B886D624505DCA6870@phx.gbl>

Hi Vito

The document seems a systematic and organized primer on distribution fitting 
in R. Thanks for sharing it with the R community!.  Are you planning to make 
it available in English?  I would offer you help with the translation but I 
don't know enough italian to be useful for this task!

Regards

Francisco

>From: Vito Ricci <vito_ricci at yahoo.com>
>To: r-help at stat.math.ethz.ch
>Subject: [R] Fitting distribution with R: a contribute
>Date: Tue, 25 Jan 2005 10:31:06 +0100 (CET)
>
>Dear R-useRs,
>
>I've written a contribute (in Italian language)
>concering fitting distribution with R. I believe it
>could be usefull for someones. It's available on CRAN
>web-site:
>
>http://cran.r-project.org/doc/contrib/Ricci-distribuzioni.pdf
>
>Here's the abstract:
>This paper deals with distribution fitting using R
>environment for statistical computing. It treats
>briefly some theoretical issues and it points out
>especially practical ones proposing some examples of R
>statements for data graphical exploration and
>presentation, parameters estimates of patterns and
>tests for goodness of fit.
>
>TOC
>
>1.0	Introduction
>2.0	Graphics
>3.0	Model/function choice
>4.0	Parameters estimate
>5.0	Measure of goodness of fit
>6.0	Goodness of fit tests
>6.1     Normality tests
>7.0     Conclusions
>
>Appendix
>References
>
>Any comments and suggestion will be appreciated.
>Best regards
>Vito
>
>=====
>Diventare costruttori di soluzioni
>Became solutions' constructors
>
>"The business of the statistician is to catalyze
>the scientific learning process."
>George E. P. Box
>
>Top 10 reasons to become a Statistician
>
>      1. Deviation is considered normal
>      2. We feel complete and sufficient
>      3. We are 'mean' lovers
>      4. Statisticians do it discretely and continuously
>      5. We are right 95% of the time
>      6. We can legally comment on someone's posterior distribution
>      7. We may not be normal, but we are transformable
>      8. We never have to say we are certain
>      9. We are honestly significantly different
>     10. No one wants our jobs
>
>
>Visitate il portale http://www.modugno.it/
>e in particolare la sezione su Palese  
>http://www.modugno.it/archivio/palese/
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Wed Jan 26 03:02:47 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 25 Jan 2005 21:02:47 -0500
Subject: [R] transfer function estimation
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5B5@usrymx25.merck.com>

> From: Peter Dalgaard
> 
> "Kemp S E (Comp)" <sekemp at glam.ac.uk> writes:
> 
> > I have got as far as being able to compute the residual noise, a_t.
> > However, I am slightly confused about what to do next. Reading
> > Box-Jenkins, 1976 (pp. 391) they state the following
> > 
> > "....However, it seems simplest to work with a standard nonlinear
> > least squares computer program in which the derivatives are
> > determined numerically and an option is available of 'constrained
> > iteration' to prevent instability. It is then only necessary to
> > program the computation of a_t itself......."
> > 
> > I know that there is a 'nls' function in R but I really do not have
> > a clue about how to use it in this situation. Perhaps Box-Jenkins
> > are confusing me with their last sentance with regard to the a_t's -
> > is this really possible?
> 
> I'm not sure I'd trust any computer recommendation from 1976, no
> matter how famous the authors are. However, the hint would lead me to
> consider the optim() function.

My copy of Box/Jenkins/Reinsel (1994, 3rd ed.) has exactly the same passage
on page 430 (except an reference to chapter 7)...

Andy
 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jeaneid at chass.utoronto.ca  Wed Jan 26 03:36:05 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Tue, 25 Jan 2005 21:36:05 -0500
Subject: [R] multi line comment
Message-ID: <Pine.SGI.4.40.0501252127330.7450565-100000@origin.chass.utoronto.ca>

Hi,

Are there any plans to do multi line commenting? like /*...*/


Jean



From ripley at stats.ox.ac.uk  Wed Jan 26 05:26:14 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 26 Jan 2005 04:26:14 +0000 (GMT)
Subject: [R] multi-class classification using rpart
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E5B1@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E5B1@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.61.0501260425020.11679@gannet.stats>

On Tue, 25 Jan 2005, Liaw, Andy wrote:

>> From: WeiWei Shi
>>
>> Hi,
>> I am trying to make a multi-class classification tree by using rpart.
>> I used MASS package'd data: fgl to test and it works well.
>>
>> However, when I used my small-sampled data as below, the program seems
>> to take forever. I am not sure if it is due to slowness or there is
>> something wrong with my codes or data manipulation.
>>
>> Please be advised !
>>
>> The data is described as the output from str() function. The call to
>> rpart is like:
>>
>> library(rpart)
>> test_tree<-rpart(x$V142 ~ ., data=x,
>> parms=list(split='gini'), cp =0.01)
>>
>> the response variable is $V142, with 3 levels.
>>
>> Thanks for your suggestions!
>>
>> Ed.
>
> [snip]
>
>>  $ V141: Factor w/ 88 levels "1001","1002",..: 59 59 59 59 59
>> 59 55 78 7 73 ...
>
> I'd bet this is the problem.  There are 2^(88-1) - 1 possible ways to split
> a factor with 88 levels.  It will work on those splits til the cows come
> home...
>
> I'd suggest getting rid of that variable, or collapse the levels to
> something more reasonable.  The CART book describes some heuristic shortcuts
> for testing only n-1 splits for factors with n levels, but I believe that
> only works for 2-class problems, if I'm not mistaken.

You don't need heuristics: there is a fast algorithm (proved in my PRNN 
book) for two classes only.  I believe rpart implements it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Wed Jan 26 05:28:51 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 26 Jan 2005 04:28:51 +0000 (UTC)
Subject: [R] chron: parsing dates into a data frame using a forloop
References: <1106692293.41f6c8c59b4a8@webmail.uvm.edu>
Message-ID: <loom.20050126T052153-333@post.gmane.org>

Benjamin M. Osborne <Benjamin.Osborne <at> uvm.edu> writes:

: 
: I have one data frame with a column of dates and I want to fill another data
: frame with one column of dates, one of years, one of months, one of a unique
: combination of year and month, and one of days, but R seems to have some
: problems with this.  My initial data frame looks like this (ignore the NAs in
: the other fields):
: 
: > mans[1:10,]
:        date loc snow.new prcp tmin snow.dep tmax
: 1  11/01/54   2       NA   NA   NA       NA   NA
: 2  11/02/54   2       NA   NA   NA       NA   NA
: 3  11/03/54   2       NA   NA   NA       NA   NA
: 4  11/04/54   2       NA   NA   NA       NA   NA
: 5  11/05/54   2       NA   NA   NA       NA   NA
: 6  11/06/54   2       NA   NA   NA       NA   NA
: 7  11/07/54   2       NA   NA   NA       NA   NA
: 8  11/08/54   2       NA   NA   NA       NA   NA
: 9  11/09/54   2       NA   NA   NA       NA   NA
: 10 11/10/54   2       NA   NA   NA       NA   NA
: >
: 
: The code and resultant data frame look like this:
: 
: > for(i in 1:10){
: + mans.met$date[i]<-mans$date[i]
: + mans.met$year[i]<-years(mans.met$date[i])
: + mans.met$month[i]<-months(mans.met$date[i])
: + mans.met$yearmo[i]<-cut(mans.met$date[i], "months")
: + mans.met$day[i]<-days(mans.met$date[i])
: + }
: > mans.met[1:10,]
:        date year month yearmo day snow.new snow.dep prcp tmin tmax tmean
: 1  11/01/54    1    11      1   1       NA       NA   NA   NA   NA    NA
: 2  11/02/54    1    11      1   2       NA       NA   NA   NA   NA    NA
: 3  11/03/54    1    11      1   3       NA       NA   NA   NA   NA    NA
: 4  11/04/54    1    11      1   4       NA       NA   NA   NA   NA    NA
: 5  11/05/54    1    11      1   5       NA       NA   NA   NA   NA    NA
: 6  11/06/54    1    11      1   6       NA       NA   NA   NA   NA    NA
: 7  11/07/54    1    11      1   7       NA       NA   NA   NA   NA    NA
: 8  11/08/54    1    11      1   8       NA       NA   NA   NA   NA    NA
: 9  11/09/54    1    11      1   9       NA       NA   NA   NA   NA    NA
: 10 11/10/54    1    11      1  10       NA       NA   NA   NA   NA    NA
: >
: 
: The problem seems to be with assigning within the forloop, or making the
: assignment into a data frame, since:
: 
: > years(mans.met$date[5])
: [1] 1954
: Levels: 1954
: > test<-years(mans.met$date[5])
: > test
: [1] 1954
: Levels: 1954
: >
: > months(mans.met$date[5])
: [1] Nov
: 12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec
: > test<-months(mans.met$date[5])
: > test
: [1] Nov
: 12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec
: >
: > cut(mans.met$date[3], "months")
: [1] Nov 54
: Levels: Nov 54
: > test<-cut(mans.met$date[3], "months")
: > test
: [1] Nov 54
: Levels: Nov 54
: >
: > days(mans.met$date[4])
: [1] 4
: 31 Levels: 1 < 2 < 3 < 4 < 5 < 6 < 7 < 8 < 9 < 10 < 11 < 12 < 13 < ... < 31
: > test<-days(mans.met$date[4])
: > test
: [1] 4
: 31 Levels: 1 < 2 < 3 < 4 < 5 < 6 < 7 < 8 < 9 < 10 < 11 < 12 < 13 < ... < 31
: >
: 
: Any suggestions will be appreciated.
: -Ben Osborne

I guess you set up mans.met as numeric columns and when you
assign your factors to numeric variables you get
the underlying codes.  Note that if f is a factor then as.numeric(f)
gives the codes underlying the factor whereas as.character(f) gives
the labels.

It would be better not to use a loop at all.  I don't know whether you
want or not want factors but at any rate here is something you could
try.  It creates data frame df2 without a loop.

df2 <- data.frame(date = mans$date, yearmo = as.character(cut(mans$date, "m")))
df2 <- cbind(df2, month.day.year(mans$date))

Finally, do you really want this redundant representation?  I would tend to
go with just storing the dates and computing any of the other quantities
on-the-fly as needed.



From ripley at stats.ox.ac.uk  Wed Jan 26 05:36:21 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 26 Jan 2005 04:36:21 +0000 (GMT)
Subject: [R] multi line comment
In-Reply-To: <Pine.SGI.4.40.0501252127330.7450565-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0501252127330.7450565-100000@origin.chass.utoronto.ca>
Message-ID: <Pine.LNX.4.61.0501260430310.11679@gannet.stats>

On Tue, 25 Jan 2005, Jean Eid wrote:

> Are there any plans to do multi line commenting? like /*...*/

This has been discussed on R-devel (the appropriate place!) in 2005.
Please see the archives at
https://stat.ethz.ch/pipermail/r-devel/2005-January/031833.html

Short answer: No.
Slightly longer answer: you don't need it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From cuiczhao at yahoo.com  Wed Jan 26 06:34:03 2005
From: cuiczhao at yahoo.com (Cuichang Zhao)
Date: Tue, 25 Jan 2005 21:34:03 -0800 (PST)
Subject: [R] plot function
Message-ID: <20050126053404.40116.qmail@web30704.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050125/60225436/attachment.pl

From ligges at statistik.uni-dortmund.de  Wed Jan 26 08:42:10 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 26 Jan 2005 08:42:10 +0100
Subject: [R] multi-class classification using rpart
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E5B2@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E5B2@usrymx25.merck.com>
Message-ID: <41F749D2.8000709@statistik.uni-dortmund.de>

Liaw, Andy wrote:

>>From: Uwe Ligges 
>>
>>WeiWei Shi wrote:
>>
>>
>>>Hi, Andy:
>>>Thanks. It works after I removed the variable. I think I 
>>
>>got a similar
>>
>>>problem when I used randomForest. And I am not sure if they were due
>>>to the same reason.
>>>
>>>Practically and Unfortunately, that variable is very 
>>
>>important to the
>>
>>>accuracy. I am wondering if there is another way besides collapsing
>>>it. BTW, I remember you mentioned some alternative implementation to
>>>randomForest (the author provided) to avoid the upper limit 
>>
>>(32, if I
>>
>>>am correct) for the level of factor which can be used in the R
>>>version's randomForest.
>>>
>>>Thanks for further assistance!
>>
>>
>>So you *really* want it to be factor?! Thought it was a 
>>mistake not to 
>>have it numerical....
>>Amazing! Maybe computers are sometimes even too fast these days.
>>
>>Uwe
> 
> 
> [Uwe: Not sure if you meant to keep this off-list.  If so, my most sincere
> apologies.]

Andy, *you* do not need to apologize (yes, I meant to keep it off list, 
but WeiWei Shi posted it anyway).


> Er... not really.  Currently (classification) randomForest encode splits on
> categorical variables by binary expansion of levels that go to the left.
> Such split is stored in (4-byte) integers, thus the 32-level restriction.
> In newer version of Breiman & Cutler's Fortran code, that restriction is
> removed by storing the entire indicator matrix (# of nodes by max. number of
> levels, then by number of trees in the forest).  For the stand-alone
> Fortran, each tree is written to file as soon as it's grown, so it doesn't
> need to store the entire forest in memory.  The R version has no such luxury
> (if you can call it that).
> 
> The way the new RF Fortran code deals with categorical variables with more
> than 10 categories is by randomly sampling some number (say 512) of random
> splits and pick the best among them.  That's probably a good strategy for
> random forests, but may not be what one would do to grow a single tree.
> 
> When growing a single tree with data containing categorical variables with
> large number of categories, one should also be mindful of the problem that,
> because of the greedy nature of the algorithm, it will tend to split on
> variables with larger numbers of possible splits, even if those variables
> are less `informative'.
> 
> Andy

Certainly you are right - I don't know all those details about 
RandomForests, but the point I tried to make is different:
Be aware not to be called a professional overfitter: Variable name 
"V141" and at least in one of those variables a factor with 88 levels...!!!


Uwe



From jacques.veslot at cirad.fr  Wed Jan 26 08:40:19 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Wed, 26 Jan 2005 11:40:19 +0400
Subject: [R] Still avoiding loops
Message-ID: <HHEDKBCGCMDOHEDELFBCEEPPCHAA.jacques.veslot@cirad.fr>

Dear all,

I have a matrix X with 47 lines and say 500 columns - values are in {0,1}.
I'd like to compare lines.

For that, I first did:

for (i in 1:(dim(X)[1]-1))
for (j in (i+1):dim(X)[1]) {
	Y <- X[i,]+Y[j,]
	etc.

but, since it takes a long time, I would prefer avoding loops;
for that, my first idea was to add this matrix:

X1=X[,rep(1:46,46:1)]

to this one:

res=NULL
for (i in (2:47)) res=c(res,i:47)

X2=X[,res]

(Is it a nice alternative way ?)
Is there a way to create the second matrix X2 without a loop, such as for X1
?

Thanks in advance,

Jacques VESLOT



From vito_ricci at yahoo.com  Wed Jan 26 08:44:50 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Wed, 26 Jan 2005 08:44:50 +0100 (CET)
Subject: [R] Fitting distribution with R: a contribute
In-Reply-To: <BAY103-F1591377E2B886D624505DCA6870@phx.gbl>
Message-ID: <20050126074450.13332.qmail@web41209.mail.yahoo.com>

Dear Francisco,
Many thanks for your attention to my contribute. As
regard its translation into English -also other people
asked me-, maybe in next months it could take place.
When it will be available I inform the R-help mailing
list.
Best regards,
Vito


 --- F Z <gerifalte28 at hotmail.com> ha scritto: 
> Hi Vito
> 
> The document seems a systematic and organized primer
> on distribution fitting 
> in R. Thanks for sharing it with the R community!. 
> Are you planning to make 
> it available in English?  I would offer you help
> with the translation but I 
> don't know enough italian to be useful for this
> task!
> 
> Regards
> 
> Francisco
> 
> >From: Vito Ricci <vito_ricci at yahoo.com>
> >To: r-help at stat.math.ethz.ch
> >Subject: [R] Fitting distribution with R: a
> contribute
> >Date: Tue, 25 Jan 2005 10:31:06 +0100 (CET)
> >
> >Dear R-useRs,
> >
> >I've written a contribute (in Italian language)
> >concering fitting distribution with R. I believe it
> >could be usefull for someones. It's available on
> CRAN
> >web-site:
> >
>
>http://cran.r-project.org/doc/contrib/Ricci-distribuzioni.pdf
> >
> >Here's the abstract:
> >This paper deals with distribution fitting using R
> >environment for statistical computing. It treats
> >briefly some theoretical issues and it points out
> >especially practical ones proposing some examples
> of R
> >statements for data graphical exploration and
> >presentation, parameters? estimates of patterns and
> >tests for goodness of fit.
> >
> >TOC
> >
> >1.0	Introduction
> >2.0	Graphics
> >3.0	Model/function choice
> >4.0	Parameters estimate
> >5.0	Measure of goodness of fit
> >6.0	Goodness of fit tests
> >6.1     Normality tests
> >7.0     Conclusions
> >
> >Appendix
> >References
> >
> >Any comments and suggestion will be appreciated.
> >Best regards
> >Vito
> >
> >=====
> >Diventare costruttori di soluzioni
> >Became solutions' constructors
> >
> >"The business of the statistician is to catalyze
> >the scientific learning process."
> >George E. P. Box
> >
> >Top 10 reasons to become a Statistician
> >
> >      1. Deviation is considered normal
> >      2. We feel complete and sufficient
> >      3. We are 'mean' lovers
> >      4. Statisticians do it discretely and
> continuously
> >      5. We are right 95% of the time
> >      6. We can legally comment on someone's
> posterior distribution
> >      7. We may not be normal, but we are
> transformable
> >      8. We never have to say we are certain
> >      9. We are honestly significantly different
> >     10. No one wants our jobs
> >
> >
> >Visitate il portale http://www.modugno.it/
> >e in particolare la sezione su Palese  
> >http://www.modugno.it/archivio/palese/
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> >http://www.R-project.org/posting-guide.html
> 
> 
>  

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/


		
___________________________________

e posta indesiderata



From j.van_den_hoff at fz-rossendorf.de  Wed Jan 26 09:03:30 2005
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Wed, 26 Jan 2005 09:03:30 +0100
Subject: [R] plot function
In-Reply-To: <20050126053404.40116.qmail@web30704.mail.mud.yahoo.com>
References: <20050126053404.40116.qmail@web30704.mail.mud.yahoo.com>
Message-ID: <41F74ED2.1010500@fz-rossendorf.de>

Cuichang Zhao wrote:
> Hello, 
> how can use change the plot function to change the range of axises. I want my graph from a certain range [a, b], instead of  from the min to max of of datas?
> if i want draw a line instead of dots, should i use both plot and lines function.
> for example:
> plot(x, y);
> lines(x, y);
>  
> things seem not working if i only use lines(x, y).
>  
> Thank you so much.
>  
> Cuichang Zhao
>  
> Jan 25, 2005
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
the first "see also" entry in the help text for "plot" is 
"plot.default". well, do that: try "?plot.default". time estimate for 
finding it yourself: one minute. you really should think about using the 
online help. if everybody would use your approach, the list would 
probably get a thousand 'hits' per hour of the quality: "is there a way 
to speed up matrix addition? the for loops run too slow."



From h.andersson at nioo.knaw.nl  Wed Jan 26 09:18:21 2005
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Wed, 26 Jan 2005 09:18:21 +0100
Subject: [R] plot function
In-Reply-To: <20050126053404.40116.qmail@web30704.mail.mud.yahoo.com>
References: <20050126053404.40116.qmail@web30704.mail.mud.yahoo.com>
Message-ID: <ct7jne$obt$1@sea.gmane.org>

Try this,

plot(x,y,xlim=c(a,b),ylim=c(c,d))

lines(z,q) will only add lines onto an existing plot, not create a new plot!

Have a look at:
http://cran.r-project.org/doc/manuals/R-intro.html#Graphics

Cheers, Henrik

Cuichang Zhao wrote:
> Hello, 
> how can use change the plot function to change the range of axises. I want my graph from a certain range [a, b], instead of  from the min to max of of datas?
> if i want draw a line instead of dots, should i use both plot and lines function.
> for example:
> plot(x, y);
> lines(x, y);
>  
> things seem not working if i only use lines(x, y).
>  
> Thank you so much.
>  
> Cuichang Zhao
>  
> Jan 25, 2005
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From Christoph.Scherber at uni-jena.de  Wed Jan 26 09:57:57 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Wed, 26 Jan 2005 09:57:57 +0100
Subject: [R] lme and varFunc()
In-Reply-To: <BF74FADD4B44554CA7E53D0B5242CD6A01FC645A@evd-s7014.evd.admin.ch>
References: <BF74FADD4B44554CA7E53D0B5242CD6A01FC645A@evd-s7014.evd.admin.ch>
Message-ID: <41F75B95.80901@uni-jena.de>

Dear all,

I am expecting a Poisson error distribution in my lme with 
weights=varFunc().

The "weigths= varPower (form= fitted (.))" doesn?t work due to missing 
values in the response:

Problem in lme.formula(fixed = sqrt(nrmainaxes + 0...: Maximum number of iterations reached without convergence. 
Use traceback() to see the call stack

That?s why I?ve used one of my most important explanatory variables as a variance covariate:

weigths= varPower (form=~explanatory)

With that, it worked out properly so far.

What would your suggestion in such a case be?

Regards
Christoph



Lorenz.Gygax at fat.admin.ch wrote:

>>I am currently analyzing a dataset using lme(). The model I 
>>use has the following structure:
>>
>>model<-lme(response~Covariate+TreatmentA+TreatmentB,
>>           random=~1|Block/Plot,method="ML")
>>
>>When I plot the residuals against the fitted values, I see a clear 
>>positive trend (meaning that the variance increases with the mean).
>>
>>I tried to solve this issue using weights=varPower(), but it
>>doesn?t change the residual plot at all.
>>    
>>
>
>You are aware that you need to use something like 
>
>weigths= varPower (form= fitted (.))
>
>and the plot residuals using e.g.
>
>scatter.smooth (fitted (model), resid (model, type= 'n'))
>
>Maybe the latter should also be ok with the default pearson residuals, but I
>am not sure.
>
>Possibly a look into the following would help?
>
>@Book{Pin:00a,
>  author = 	 {Pinheiro, Jose C and Bates, Douglas M},
>  title = 	 {Mixed-Effects Models in {S} and {S}-{P}{L}{U}{S}},
>  publisher = 	 {Springer},
>  year = 	 {2000},
>  address = 	 {New York}
>}
>
>  
>
>>How would you implement such a positive trend in the variance? I?ve 
>>tried glmmPQL (which works great with poisson errors), but 
>>using glmmPQL I can?t do model simplification.
>>    
>>
>
>Well, what error distribution do you have / do you expect?
>
>Regards, Lorenz
>- 
>Lorenz Gygax, Dr. sc. nat.
>Centre for proper housing of ruminants and pigs
>Swiss Federal Veterinary Office
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From buser at stat.math.ethz.ch  Wed Jan 26 10:18:30 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Wed, 26 Jan 2005 10:18:30 +0100
Subject: [R] r square values for independent variables in multiple linear
	regr ession model -- newbie
In-Reply-To: <A8722C0C0FB4D3118A13009027C3C82E042A80F8@U742EXC1>
References: <A8722C0C0FB4D3118A13009027C3C82E042A80F8@U742EXC1>
Message-ID: <16887.24678.407931.22741@stat.math.ethz.ch>

Dear Avneet

If you fit a multiple linear regression model your independent
variables will not be orthogonal and therefore it is difficult
to divide the r square on the different variables with a
meaningful interpretation.
In a balanced analysis of variance design the situation is
somehow different. As result of the balanced design the
explanatory variables are orthogonal and you can decompose the
sum of squares on the different variables.
But if you have covariables you must be careful. I wouldn't try
to assign r square values for each Variable.

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C11
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-1-632-5414		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


Singh, Avneet writes:
 > Hello
 > 
 > Could you please suggest a way to find out the r square values for each
 > independent variable while using "lm" for developing a multiple linear
 > regression model.
 > 
 > Thank you
 > avneet
 > 
 > "I have no data yet. It is a capital mistake to theorize before one has
 > data. Insensibly one begins to twist facts to suit theories instead of
 > theories to suit facts."
 > ~ Sir Arthur Conan Doyle (1859-1930), Sherlock Holmes
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gb at tal.stat.umu.se  Wed Jan 26 10:20:47 2005
From: gb at tal.stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Wed, 26 Jan 2005 10:20:47 +0100
Subject: [R] GLM function with poisson distribution
In-Reply-To: <x2hdl53tx3.fsf@biostat.ku.dk>
References: <20050125142227.95623.qmail@web40629.mail.yahoo.com>
	<Pine.A41.4.61b.0501250838510.329090@homer06.u.washington.edu>
	<x2hdl53tx3.fsf@biostat.ku.dk>
Message-ID: <20050126092047.GA2094@tal.stat.umu.se>

On Wed, Jan 26, 2005 at 01:39:36AM +0100, Peter Dalgaard wrote:
> Thomas Lumley <tlumley at u.washington.edu> writes:
> 
> > On Tue, 25 Jan 2005, Florian Menzel wrote:
> > 
> > > Hello all,
> > > I found a weird result of the GLM function that seems
> > > to be a bug.
> > 
> > No, the problem is that you are using the Wald test when the mle is
> > infinite, which is always going to be unreliable.  It's even worse
> > because you are using data that couldn't really have come from a
> > Poisson distribution (because for a=1 you have mean 3 and variance 0).
> 
> Actually, that's rather immaterial. 

Right. For the Hauck[sic]-Donner effect. This two-sample problem admits a
two-dimensional sufficient statistic, the totals in the two groups. In this
example Xtot = 0, Ytot = 24. So the problem is reduced to: Given two
independent Poisson observations X = 0 and Y = 24, can they possibly come
from the same distribution?

Thomas' observation is relevant from a practical point of view; by
reducing data to sufficient statistics, we lose the possibility to check
model assumptions, here the iid (within groups) Poisson distribution.

> Try it with
> 
> b=c(rep(0,8),rmultinom(1,24,rep(1/8,8))) 

This is the conditional distribution, given the sufficient statistic.
 
> or even
> 
> b=c(rep(0,8),rpois(8,3))

This is the unconditional distribution, given that the parameter is
equal to the mle.

> (in the former case, notice that the LRT is constant).

because the LRT statistic is a function of data only through the sufficient 
statistic.

What is the "correct" p-value in this case? The exact p-value for the LRT
is hard to calculate (the distribution of the test statistic under the null
depends on thrue, common, unknown  parameter), but
the conditional test, given X+Y, (has certain optimality properties) is
very easy to perform. The p-value is 2 * dbinom(0, 24, 0.5) = 1.192093e-07,
compared to the LRT  8.017e-09. 

Who cares? But suppose the observed value of sufficient statistic was 
(0, 5). Then the two p-values are 0.0625 and 0.00847, respectively, and one
should care. 

Besides the binomial test, you can also use a bootstrap approach, by
simulating iid poisson pairs with mean (X+Y)/2 and calculate the LRT
statistic. For (X, Y) = (0, 5), this results in a p-value of about 0.018
(one million replicates). So, it seems as if the conditional test is too
conservative, and the asymptotics are far too optimistic. I _think_ this is
"common knowledge".  

-- 
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se



From p.dalgaard at biostat.ku.dk  Wed Jan 26 11:01:03 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Jan 2005 11:01:03 +0100
Subject: [R] GLM function with poisson distribution
In-Reply-To: <20050126092047.GA2094@tal.stat.umu.se>
References: <20050125142227.95623.qmail@web40629.mail.yahoo.com>
	<Pine.A41.4.61b.0501250838510.329090@homer06.u.washington.edu>
	<x2hdl53tx3.fsf@biostat.ku.dk> <20050126092047.GA2094@tal.stat.umu.se>
Message-ID: <x2hdl4ldb4.fsf@biostat.ku.dk>

G?ran Brostr?m <gb at tal.stat.umu.se> writes:

> > Try it with
> > 
> > b=c(rep(0,8),rmultinom(1,24,rep(1/8,8))) 
> 
> This is the conditional distribution, given the sufficient statistic.

Yes (and that was of course the point). I was surprised that there is
a difference at all (between this and rep(3,8) and also between
different calls to rmultinom). Theoretically, the MLE should be the
same, but of course the "saturated models" have a different
likelihoods, so the deviances vary and there may be variations in the
number of iterations taken in the (divergent) fitting process.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From asanchez at ub.edu  Wed Jan 26 10:54:32 2005
From: asanchez at ub.edu (Alex Sanchez)
Date: Wed, 26 Jan 2005 10:54:32 +0100
Subject: [R] string evaluation
Message-ID: <000001c50390$d72139a0$0d5274a1@stat.ub.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050126/669e9bd0/attachment.pl

From ligges at statistik.uni-dortmund.de  Wed Jan 26 11:36:02 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 26 Jan 2005 11:36:02 +0100
Subject: [R] string evaluation
In-Reply-To: <000001c50390$d72139a0$0d5274a1@stat.ub.es>
References: <000001c50390$d72139a0$0d5274a1@stat.ub.es>
Message-ID: <41F77292.7080909@statistik.uni-dortmund.de>

Alex Sanchez wrote:

> Hello,
> 
> I would like to be able to compose an expression using some of the arguments I give to a function, that is assuming I have a list such as:
> my.list<-list(A=1:3,B=c("Brave", "new", "world"))
> and a function
> printComponent<-function(component){
>     if (component=="A"||component=="B"){
>         my.expression<-paste("my.list",component,sep="$")
>         print(my.expression)}
>     else{cat("Wrong argument, must be A or B")}
> }
> 
> I would like that a call "printComponent("A")" to be equivalent to calling
> 
>>my.list$A

It's much simpler! Use the [[ as in:

printComponent <- function(component){
     my.list[[component]]
}

Uwe Ligges




> [1] 1 2 3
> but I only obtain 
> 
>>printComponent("A")
> 
> [1] "my.list$A"
> 
> I presume I must evaluate it somehow but I really can't find the instruction in the manuals (I would also appreciate to know where should I have looked at)
> 
> Thanks for the help
> 
> ----------------------------------------------------------
> Dr. Alex S?nchez
> Departament d'Estad?stica U.B.
> Telf: 34 934021590
> Fax: 34 93 4111733
> e-mail : asanchez at ub.edu
> -----------------------------------------------------------
>         
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Jan 26 11:41:51 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 26 Jan 2005 11:41:51 +0100
Subject: [R] string evaluation
References: <000001c50390$d72139a0$0d5274a1@stat.ub.es>
Message-ID: <002401c50393$a4e89930$0540210a@www.domain>

Hi Alex,


you could use something like this:

printComponent <- function(component, lis){
    if(component %in% names(lis)) return(lis[component])
    else cat(component, "in not an element of", substitute(lis), "\n")
}
##############
my.list <- list(A=1:3, B=c("Brave", "new", "world"))
printComponent("A", my.list)
printComponent("C", my.list)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Alex Sanchez" <asanchez at ub.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, January 26, 2005 10:54 AM
Subject: [R] string evaluation


Hello,

I would like to be able to compose an expression using some of the 
arguments I give to a function, that is assuming I have a list such 
as:
my.list<-list(A=1:3,B=c("Brave", "new", "world"))
and a function
printComponent<-function(component){
    if (component=="A"||component=="B"){
        my.expression<-paste("my.list",component,sep="$")
        print(my.expression)}
    else{cat("Wrong argument, must be A or B")}
}

I would like that a call "printComponent("A")" to be equivalent to 
calling
> my.list$A
[1] 1 2 3
but I only obtain
> printComponent("A")
[1] "my.list$A"

I presume I must evaluate it somehow but I really can't find the 
instruction in the manuals (I would also appreciate to know where 
should I have looked at)

Thanks for the help

----------------------------------------------------------
Dr. Alex S?nchez
Departament d'Estad?stica U.B.
Telf: 34 934021590
Fax: 34 93 4111733
e-mail : asanchez at ub.edu
-----------------------------------------------------------

[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From trautmann at statistik.uni-dortmund.de  Wed Jan 26 11:49:50 2005
From: trautmann at statistik.uni-dortmund.de (Heike Trautmann)
Date: Wed, 26 Jan 2005 11:49:50 +0100
Subject: [R] NSGA-II implementation?
Message-ID: <41F775CE.4090406@statistik.uni-dortmund.de>

Hello,

has anyone of you ever implemented (or integrated a C-SourceCode of)  a 
Multiobjective Evolutionary Algorithm in R, especially the NSGA-II 
developed by Kalyanmoy Deb?

Thank you very much for your help,

Heike Trautmann

------------------------------------------------
Dr. Heike Trautmann
Lehrstuhl f?r Computergest?tzte Statistik
Fachbereich Statistik
Universit?t Dortmund
Tel.: 0049 231 755-7222
e-mail: trautmann at statistik.uni-dortmund.de



From Ted.Harding at nessie.mcc.ac.uk  Wed Jan 26 11:46:08 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 26 Jan 2005 10:46:08 -0000 (GMT)
Subject: [R] Zipf random number generation
In-Reply-To: <20050125224638.92054.qmail@web30001.mail.mud.yahoo.com>
Message-ID: <XFMail.050126104608.Ted.Harding@nessie.mcc.ac.uk>

On 25-Jan-05 Weiguang Shi wrote:
> Sorry. 
> 
> c = sum(1.0 / pow((double) j, alpha)), j=1,2,...,N

Understood! Though I think you mean

  1/c = sum(1.0 / pow((double) j, alpha)), j=1,2,...,N

Anyway, provided alpha > 1 the series converges for the
infinite sum, but if you limit the range to 1...N then
you could have any value of alpha (even 0 or negative).

Be that as it may, I don't know of any R function for
sampling such a distribution directly, but you could
certainly try the suggestion I gave at the end of my
previous response:

  N <- 10000 ##For instance. You need N large enough
  i0 <- (1:N) 
  p <- 1/(i0^alpha) ; p <- p/sum(p)
  X <- sample (i0, n, replace=TRUE, prob=p)

Then X will be a random sample of size n from the
distribution you describe on the range 1:N.

When I tried the above with alpha=1.1, N=10000, n=1000
the largest value of X was 9928, i.e. very nearly
equal to N, so this suggested that N=10000 may not be
large enough! So I then tried it with N=50000 and got
max(X) = 48303; then with N=100000 and max(X) = 94914.
And so on (at this point I was hitting swap-space on
this little machine). In short: with alpha so near 1,
the sample will tend to extend over the whole range
you give it when you specify N.

This is an indication that the simple-minded method
of my suggestion may fail to provide sufficiently large
sampled values when alpha is near 1.

However, since the "common practices" in your area,
as you describe them below, do set a value of N, then
presumably there is also a "common view" of what limit
for N is acceptable -- i.e. in your area it does not
matter if you fail to sample values larger than N.

In that case the above method should be satisfactory.

If not, then some other approach would be needed.
You certainly don't want to use the above method with N
approaching the largest machine-representable integer!

One possibility might be to split the sampling.
Let alpha > 1. Choose N fairly large. You would need
a method for evaluating sum(i^alpha) from 1 to infinity.
Let P = sum[1:N](i^alpha)/sum[1:inf](i^alpha).

With probability P, sample on (1:N) with the above method.
With probability (1-P), use a continuous approximation
on [(N+1):inf] with density proportional to 1/(x^alpha).
For the latter, the CDF is easily evaluated analytically;
likewise its inverse function. Hence by sampling from a
uniform distribution you can sample X from the distribution
with density proportional to 1/(x^alpha) on [(N+1):inf].
Then get the sampled value of i as floor(X).

This is in interesting question! I hope other readers can
suggest good ideas.

Hoping this helps,
Ted.

>  --- Weiguang Shi <wgshi2001 at yahoo.ca> wrote: 
>> Thanks very much Ted for the detailed explanation.
>> 
>> It might be flawed but some "common practices" in my
>> area use the following approach to generate a random
>> rank for some Zipf-like distribution with the
>> parameter alpha.
>> 
>> The key is to introduce the limit of ranks, N. With
>> N
>> and alpha, therefore, the probability of rank i is
>> determined by 
>>     c/pow((double) i, alpha)
>> where
>>     c = sum(1.0 / pow((double) i, alpha))
>> 
>> Any comment on that?
>> 
>> Thanks,
>> Weiguang
>> 
>>  --- Ted.Harding at nessie.mcc.ac.uk wrote: 
>> > On 25-Jan-05 Weiguang Shi wrote:
>> > > Hi,
>> > > 
>> > > Is there a Zipf-like distribution RNG in R?
>> > > 
>> > > Thanks,
>> > > Weiguang
>> > 
>> > "Zipf's Law" (as originally formulated in studies
>> of
>> > the
>> > frequencies of words in texts) is to the effect
>> that
>> > the
>> > relative frequencies with which words occur once,
>> > twice,
>> > 3 times, ... are in proportion to 1/1, 1/2, 1/3,
>> ...
>> > ,
>> > 1/n, ... with no limit on n (i.e. the number of
>> > different
>> > words each represented n times is proportional to
>> > 1/n).
>> > 
>> > This is "improper" since sum(1/n) = infinity, so
>> > does not
>> > define a probability distribution. A respectable
>> > analogue
>> > is Fisher's logarithmic distribution p(x) where
>> > 
>> >   p(x) = ((t^x)/x)/(-log(1-t)), x = 1, 2, 3, ...
>> > 
>> > where t in (0,1) is the parameter of the
>> > distribution.
>> > 
>> > As t -> 1, p(x+1)/p(x) -> x/(x+1) as in Zipf's
>> Law.
>> > 
>> > However, I've searched the R site and have found
>> > only
>> > one instance of a function directly related to
>> this
>> > distribution, namely logff() in package VGAM,
>> which
>> > is for estimating the parameter t.
>> > 
>> > So it looks as though there is no direct
>> > implementation
>> > of something like "rlogdist".
>> > 
>> > However, the logarithmic distribution is a
>> limiting
>> > form of the negative binomial distribution
>> > (conditional
>> > on a positive value), and there are functions in R
>> > for
>> > random sampling from this distribution.
>> > 
>> > From my reading of "?rnbinom" in the base package,
>> > this
>> > function does not have the flexibitility you would
>> > need
>> > for this. But in the MASS package there is the
>> > function
>> > rnegbin() which does.
>> > 
>> > You would need to invoke
>> > 
>> >   rnegbin(N, mu=..., theta=... )
>> > 
>> > where the value ... of mu is large and the value
>> ...
>> > of
>> > theta is small, and reject cases with value zero.
>> > This
>> > of course makes it awkward to generate a sample of
>> > given
>> > size.
>> > 
>> > Alternatively, you can try along the lines of
>> > 
>> > > x0<-1:10000; t<-0.999;
>> p<-((t^x0)/x0)/(-log(1-t))
>> > > Y<-sample(x0,5000,replace=TRUE,prob=p)
>> > > hist(Y,breaks=100)
>> > 
>> > While this gives the logarithmic distribution over
>> > the
>> > range of x in x0, it is inexact in that it does
>> not
>> > permit values greater than max(x0) to be sampled.
>> > 
>> > No doubt others can suggest something better than
>> > this!
>> > 
>> > Best wishes,
>> > Ted.
>> > 
>> > 
>> >
>>
> --------------------------------------------------------------------
>> > E-Mail: (Ted Harding)
>> <Ted.Harding at nessie.mcc.ac.uk>
>> > Fax-to-email: +44 (0)870 094 0861  [NB: New
>> number!]
>> > Date: 25-Jan-05                                   
>>  
>> >  Time: 10:20:57
>> > ------------------------------ XFMail
>> > ------------------------------
>> >
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 26-Jan-05                                       Time: 10:46:08
------------------------------ XFMail ------------------------------



From petr.pikal at precheza.cz  Wed Jan 26 11:55:34 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 26 Jan 2005 11:55:34 +0100
Subject: [R] modular in R
In-Reply-To: <20050126001659.22305.qmail@web30708.mail.mud.yahoo.com>
Message-ID: <41F78536.18090.E50E1F@localhost>

Hi
If you go to

http://www.r-project.org/

there is search section, where you can search not only mailing lists 
but also R site.

My favorite site is Paul Johnson's Rtips (try to search it by Google)

Cheers
Petr

On 25 Jan 2005 at 16:16, Cuichang Zhao wrote:

> hello,
> i wonder what command should i used in R to do the modular. 
> right now i have a vector v <- c(3, 4, 5, 6), but i have tried v%2 or
> v mod 2 or mod(v, 2) or modular(v, 2), and none of these works.
> 
> Also, how can i find more function command online, i have to search in
> the mailing list questions, but it is not enough, is that a better
> website i can search more about R online.
> 
> Thank you so much
> 
> Best Wishes
> 
> 
> C-Ming
> 
> Jan 25, 2005
> 
> 
> ---------------------------------
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Wed Jan 26 12:03:42 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 26 Jan 2005 12:03:42 +0100
Subject: [R] plot function
In-Reply-To: <20050126053404.40116.qmail@web30704.mail.mud.yahoo.com>
Message-ID: <41F7871E.22225.EC805F@localhost>

Hi

first of all do

?plot
?lines


and read the help page (especially about parameters xlim and ylim) 
and/or try to use examples provided.

plot(1:100, rnorm(100), xlim=c(1,50), type="l")

Cheers
Petr


On 25 Jan 2005 at 21:34, Cuichang Zhao wrote:

> Hello, 
> how can use change the plot function to change the range of axises. I
> want my graph from a certain range [a, b], instead of  from the min to
> max of of datas? if i want draw a line instead of dots, should i use
> both plot and lines function. for example: plot(x, y); lines(x, y);
> 
> things seem not working if i only use lines(x, y).
> 
> Thank you so much.
> 
> Cuichang Zhao
> 
> Jan 25, 2005
> 
> 
> ---------------------------------
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ramasamy at cancer.org.uk  Wed Jan 26 12:15:49 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 26 Jan 2005 11:15:49 +0000
Subject: [R] Deleted objects keep coming back
In-Reply-To: <x2d5vt3tjy.fsf@biostat.ku.dk>
References: <BAY101-F7772CCB5772DC91F2728AE8860@phx.gbl>
	<x2d5vt3tjy.fsf@biostat.ku.dk>
Message-ID: <1106738150.5996.10.camel@ndmpc126.orc.ox.ac.uk>

When you start you R session, see if it says

  [Previously saved workspace restored]

immediately before the first command prompt.

I think the only way a workspace can be restored automatically is if you
used save.image() at some point, in which case it would save all objects
as a hidden file called ".RData".

Another way a different workspace would have been saved if it was
specified in .Rprofile, Rprofile.site. See help(Startup) for more
information about the order of files that R reads in.


On Wed, 2005-01-26 at 01:47 +0100, Peter Dalgaard wrote:
> "Ken Termiso" <jerk_alert at hotmail.com> writes:
> 
> > Ah! I think this is what happened -- whenever I restarted R, it would
> > automatically load the previous workspace and its objects (call this
> > workspace 1). Then, when I would attempt to load another workspace
> > (call this workspace 2), it would retain the objects from workspace 1
> > in ADDITION to the objects loaded from workspace 2.
> > 
> > If I load up R, it will say previous workspace restored. If I then
> > type >rm(list=ls()) to delete all objects, and then load another
> > workspace, it will only have the objects from the workspace I loaded...
> > 
> > Am I correct here?
> 
> Yes, but I think you'd be better off to learn about R's command line
> argument in particular --no-restore and --vanilla. If you're not
> starting R from the command line, I'm sure there's a way to pass
> arguments anyway...
>  
>



From maechler at stat.math.ethz.ch  Wed Jan 26 12:32:17 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 26 Jan 2005 12:32:17 +0100
Subject: [R] animation without intermediate files?
In-Reply-To: <16886.2647.973007.550441@stat.math.ethz.ch>
References: <Pine.LNX.4.44.0501061548270.27024-100000@panic.stat.cmu.edu>
	<41F5956F.8010101@stat.auckland.ac.nz>
	<16886.2647.973007.550441@stat.math.ethz.ch>
Message-ID: <16887.32705.419707.350064@stat.math.ethz.ch>

>>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Tue, 25 Jan 2005 09:59:03 +0100 writes:

>>>>> "Paul" == Paul Murrell <p.murrell at auckland.ac.nz>
>>>>>     on Tue, 25 Jan 2005 13:40:15 +1300 writes:

    Paul> Hi
    Paul> Cari G Kaufman wrote:
    >>> Hello, 
    >>> 
    >>> Does anyone know how to make "movies" in R by making a
    >>> sequence of plots?  I'd like to animate a long
    >>> trajectory for exploratory purposes only, without
    >>> creating a bunch of image files and then using another
    >>> program to string them together.  In Splus I would do
    >>> this using double.buffer() to eliminate the flickering
    >>> caused by replotting. For instance, with a 2-D
    >>> trajectory in vectors x and y I would use the following:
    >>> 
    >>> motif()
    >>> double.buffer("back")
    >>> for (i in 1:length(x)) {
    >>>   plot(x[i], y[i], xlim=range(x), ylim=range(y))
    >>>   double.buffer("copy")
    >>> }
    >>> double.buffer("front")
    >>> 
    >>> I haven't found an equivalent function to double.buffer in R.  I tried
    >>> playing around with dev.set() and dev.copy() but so far with no success
    >>> (still flickers).

     Paul> Double buffering is only currently an option on the Windows graphics 
     Paul> device (and there it is "on" by default).  So something like ...

     Paul> x <- rnorm(100)
     Paul> for (i in 1:100)
     Paul>     plot(1:i, x[1:i], xlim=c(0, 100), ylim=c(-4, 4), pch=16, cex=2)

     Paul> is already "smooth"

    MM> well, sorry Paul, but not for my definition of "smooth"!

    MM> Instead, 

    MM> n <- 100
    MM> plot(1,1, xlim=c(0,n), ylim=c(-4,4), type="n")
    MM> x <- rnorm(n)
    MM> for (i in 1:n) { points(i, x[i], pch=16, cex=2); Sys.sleep(0.02) }

    MM> comes much closer to my version of "smooth"  ;-)

I apologize to Paul, since  what I said seems to be quite
platform dependent.  Here's my current "knowledge" on the matter:

o  Paul's  " for(..) plot(..) "
    - flickers quite a bit for me {on Linux X11 with no
      particularly fast graphics card}.
    - seems quite smooth for at least two Windows users who have
      relatively fast graphics cards.

o  My solution of
	   " for(..) { points(..) ; Sys.sleep(..) } "
   doesn't redraw the coordinate system and so doesn't "flicker" 
   (afaik, independently of platform)

   HOWEVER on windows; the graphics are somehow buffered and
   points are not drawn one by one, but rather in batches --> "not smooth"


Martin



From ramasamy at cancer.org.uk  Wed Jan 26 12:56:50 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 26 Jan 2005 11:56:50 +0000
Subject: [R] Still avoiding loops
In-Reply-To: <HHEDKBCGCMDOHEDELFBCEEPPCHAA.jacques.veslot@cirad.fr>
References: <HHEDKBCGCMDOHEDELFBCEEPPCHAA.jacques.veslot@cirad.fr>
Message-ID: <1106740611.5996.16.camel@ndmpc126.orc.ox.ac.uk>

Please give a simple example of the input data and output that you
desire. It is difficult to understand from you partial codes what you
mean. For example what is Y ?

Are you trying to find add values from pairs of rows ? If so, please see
my posting "pairwise difference operator" where I wanted to find the
differences between pairs of columns. 
http://tolstoy.newcastle.edu.au/R/help/04/07/1633.html

Otherwise, please send a sample input and output. Thank you.

Regards, Adai


On Wed, 2005-01-26 at 11:40 +0400, Jacques VESLOT wrote:
> Dear all,
> 
> I have a matrix X with 47 lines and say 500 columns - values are in {0,1}.
> I'd like to compare lines.
> 
> For that, I first did:
> 
> for (i in 1:(dim(X)[1]-1))
> for (j in (i+1):dim(X)[1]) {
> 	Y <- X[i,]+Y[j,]
> 	etc.
> 
> but, since it takes a long time, I would prefer avoding loops;
> for that, my first idea was to add this matrix:
> 
> X1=X[,rep(1:46,46:1)]
> 
> to this one:
> 
> res=NULL
> for (i in (2:47)) res=c(res,i:47)
> 
> X2=X[,res]
> 
> (Is it a nice alternative way ?)
> Is there a way to create the second matrix X2 without a loop, such as for X1
> ?
> 
> Thanks in advance,
> 
> Jacques VESLOT
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From i.visser at uva.nl  Wed Jan 26 13:03:01 2005
From: i.visser at uva.nl (Ingmar Visser)
Date: Wed, 26 Jan 2005 13:03:01 +0100
Subject: [R] error in building help files 
Message-ID: <BE1D4585.E41C%i.visser@uva.nl>

Hi All,

I was updating my help pages for a package when all of a sudden I got the
following error when using R CMD INSTALL:

 >>> Building/Updating help pages for package 'depmix'
     Formats: text html latex example
Error in load(zfile, envir = envir) : error reading from connection

Removing all the help files makes the error disappear, but only removing the
help file that I was working on does not ...

I'm rather puzzled, which file is R CMD INSTALL looking for here?
Has anyone come across this error before?

best, ingmar visser

ps: this is on OS X 10.3.7 and R 2.0.1 and R CMD INSTALL  invoked from the
terminal.



From gb at tal.stat.umu.se  Wed Jan 26 13:35:50 2005
From: gb at tal.stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Wed, 26 Jan 2005 13:35:50 +0100
Subject: [R] GLM function with poisson distribution
In-Reply-To: <x2hdl4ldb4.fsf@biostat.ku.dk>
References: <20050125142227.95623.qmail@web40629.mail.yahoo.com>
	<Pine.A41.4.61b.0501250838510.329090@homer06.u.washington.edu>
	<x2hdl53tx3.fsf@biostat.ku.dk>
	<20050126092047.GA2094@tal.stat.umu.se>
	<x2hdl4ldb4.fsf@biostat.ku.dk>
Message-ID: <20050126123550.GB12594@tal.stat.umu.se>

On Wed, Jan 26, 2005 at 11:01:03AM +0100, Peter Dalgaard wrote:
> G?ran Brostr?m <gb at tal.stat.umu.se> writes:
> 
> > > Try it with
> > > 
> > > b=c(rep(0,8),rmultinom(1,24,rep(1/8,8))) 
> > 
> > This is the conditional distribution, given the sufficient statistic.
> 
> Yes (and that was of course the point). I was surprised that there is
> a difference at all (between this and rep(3,8) and also between
> different calls to rmultinom). 

That surprises me :-) Really, if the observed values are exactly equal to
the predicted values or not must make a difference?

> Theoretically, the MLE should be the same, 

They are exactly the same, being functions of (the same) sufficient
statistic.  

> but of course the "saturated models" have a different
> likelihoods, so the deviances vary and there may be variations in the
> number of iterations taken in the (divergent) fitting process.

The important thing is that the difference between the null and residual
deviances is constant. The 'divergence' is due to the parametrization in
glm only. The estimation problem has a simple closed-form solution. 

-- 
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se



From i.visser at uva.nl  Wed Jan 26 13:58:59 2005
From: i.visser at uva.nl (Ingmar Visser)
Date: Wed, 26 Jan 2005 13:58:59 +0100
Subject: [R] error in building help files
Message-ID: <BE1D52A3.E425%i.visser@uva.nl>

Hi All,

I was updating my help pages for a package when all of a sudden I got the
following error when using R CMD INSTALL:

 >>> Building/Updating help pages for package 'depmix'
     Formats: text html latex example
Error in load(zfile, envir = envir) : error reading from connection

Removing all the help files makes the error disappear, but only removing the
help file that I was working on does not ...

I'm rather puzzled, which file is R CMD INSTALL looking for here?
Has anyone come across this error before?

best, ingmar visser

ps: this is on OS X 10.3.7 and R 2.0.1 and R CMD INSTALL  invoked from the
terminal.



From p.dalgaard at biostat.ku.dk  Wed Jan 26 14:03:25 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Jan 2005 14:03:25 +0100
Subject: [R] GLM function with poisson distribution
In-Reply-To: <20050126123550.GB12594@tal.stat.umu.se>
References: <20050125142227.95623.qmail@web40629.mail.yahoo.com>
	<Pine.A41.4.61b.0501250838510.329090@homer06.u.washington.edu>
	<x2hdl53tx3.fsf@biostat.ku.dk> <20050126092047.GA2094@tal.stat.umu.se>
	<x2hdl4ldb4.fsf@biostat.ku.dk>
	<20050126123550.GB12594@tal.stat.umu.se>
Message-ID: <x23bwol4v6.fsf@biostat.ku.dk>

G?ran Brostr?m <gb at tal.stat.umu.se> writes:

> On Wed, Jan 26, 2005 at 11:01:03AM +0100, Peter Dalgaard wrote:
> > G?ran Brostr?m <gb at tal.stat.umu.se> writes:
> > 
> > > > Try it with
> > > > 
> > > > b=c(rep(0,8),rmultinom(1,24,rep(1/8,8))) 
> > > 
> > > This is the conditional distribution, given the sufficient statistic.
> > 
> > Yes (and that was of course the point). I was surprised that there is
> > a difference at all (between this and rep(3,8) and also between
> > different calls to rmultinom). 
> 
> That surprises me :-) Really, if the observed values are exactly equal to
> the predicted values or not must make a difference?

To the deviance, yes. It is less obvious why the table of coefficients
comes out different, given that everything inside of it is a function
of the same sufficient statistics. The reason is the termination
criterion in glm.fit, which is on the relative change in deviance,

(I'm not too happy about that in principle. It is probably not too
important with our current epsilon of 1e-8 but I was burnt once by old
Genstat where the epsilon was something like 1e-4, deviances were
on the order of 1600, and absolute changes were used for tests.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jacques.veslot at cirad.fr  Wed Jan 26 14:10:56 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Wed, 26 Jan 2005 17:10:56 +0400
Subject: [R] Still avoiding loops
In-Reply-To: <1106740611.5996.16.camel@ndmpc126.orc.ox.ac.uk>
Message-ID: <HHEDKBCGCMDOHEDELFBCIEAICIAA.jacques.veslot@cirad.fr>


It is part of a function to determine Dice's index in the framewok of AFLP
analysis.

X is a binary matrix which value for each strain (lines) and each base pair
(columns) is 1 where there is a peak and 0 where there is no peak as
biologists explained to me.

The first step is to compare each strain with one another by counting the
number of 0, 1 and 2, respectively where there is no peak, one peak or 2
peaks for each base pair.

In that respect, I want to add together each pair of X's lines.

For the moment, there is a double loop calculating, at each step, the sum of
two lines as a vector Y and counting the number of 0, 1 and 2 in it for
inclusion in other operations.

I read your posting...

Thanks for helping,

Jacques VESLOT


-----Message d'origine-----
De : Adaikalavan Ramasamy [mailto:ramasamy at cancer.org.uk]
Envoy? : mercredi 26 janvier 2005 15:57
? : jacques.veslot at cirad.fr
Cc : R-help
Objet : Re: [R] Still avoiding loops


Please give a simple example of the input data and output that you
desire. It is difficult to understand from you partial codes what you
mean. For example what is Y ?

Are you trying to find add values from pairs of rows ? If so, please see
my posting "pairwise difference operator" where I wanted to find the
differences between pairs of columns.
http://tolstoy.newcastle.edu.au/R/help/04/07/1633.html

Otherwise, please send a sample input and output. Thank you.

Regards, Adai


On Wed, 2005-01-26 at 11:40 +0400, Jacques VESLOT wrote:
> Dear all,
>
> I have a matrix X with 47 lines and say 500 columns - values are in {0,1}.
> I'd like to compare lines.
>
> For that, I first did:
>
> for (i in 1:(dim(X)[1]-1))
> for (j in (i+1):dim(X)[1]) {
> 	Y <- X[i,]+Y[j,]
> 	etc.
>
> but, since it takes a long time, I would prefer avoding loops;
> for that, my first idea was to add this matrix:
>
> X1=X[,rep(1:46,46:1)]
>
> to this one:
>
> res=NULL
> for (i in (2:47)) res=c(res,i:47)
>
> X2=X[,res]
>
> (Is it a nice alternative way ?)
> Is there a way to create the second matrix X2 without a loop, such as for
X1
> ?
>
> Thanks in advance,
>
> Jacques VESLOT
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From gb at tal.stat.umu.se  Wed Jan 26 14:23:28 2005
From: gb at tal.stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Wed, 26 Jan 2005 14:23:28 +0100
Subject: [R] GLM function with poisson distribution
In-Reply-To: <x23bwol4v6.fsf@biostat.ku.dk>
References: <20050125142227.95623.qmail@web40629.mail.yahoo.com>
	<Pine.A41.4.61b.0501250838510.329090@homer06.u.washington.edu>
	<x2hdl53tx3.fsf@biostat.ku.dk>
	<20050126092047.GA2094@tal.stat.umu.se>
	<x2hdl4ldb4.fsf@biostat.ku.dk>
	<20050126123550.GB12594@tal.stat.umu.se>
	<x23bwol4v6.fsf@biostat.ku.dk>
Message-ID: <20050126132328.GA26464@tal.stat.umu.se>

On Wed, Jan 26, 2005 at 02:03:25PM +0100, Peter Dalgaard wrote:
> G?ran Brostr?m <gb at tal.stat.umu.se> writes:
> 
> > On Wed, Jan 26, 2005 at 11:01:03AM +0100, Peter Dalgaard wrote:
> > > G?ran Brostr?m <gb at tal.stat.umu.se> writes:
> > > 
> > > > > Try it with
> > > > > 
> > > > > b=c(rep(0,8),rmultinom(1,24,rep(1/8,8))) 
> > > > 
> > > > This is the conditional distribution, given the sufficient statistic.
> > > 
> > > Yes (and that was of course the point). I was surprised that there is
> > > a difference at all (between this and rep(3,8) and also between
> > > different calls to rmultinom). 
> > 
> > That surprises me :-) Really, if the observed values are exactly equal to
> > the predicted values or not must make a difference?
> 
> To the deviance, yes. It is less obvious why the table of coefficients
> comes out different, given that everything inside of it is a function
> of the same sufficient statistics. The reason is the termination
> criterion in glm.fit, which is on the relative change in deviance,
> 
> (I'm not too happy about that in principle. 

I had the same thought. Would it be possible to automatically reduce to
sufficient statistics inside glm, and would it be desirable? Should I, as a
user, reduce to sufficient statistics before running glm? It shouldn't
really matter, but I get different deviances and AIC, and that can be
confusing. Note that this is the case also in situations where the fitting 
algorithm converges nicely.

-- 
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se



From hellik at web.de  Wed Jan 26 14:39:41 2005
From: hellik at web.de (Helmut Kudrnovsky)
Date: Wed, 26 Jan 2005 14:39:41 +0100
Subject: [R] save contigency table
Message-ID: <1677295678@web.de>

Content-Type: text/plain; charset="iso-8859-1"
Received-SPF: none (hypatia: domain of hellik at web.de does not designate permitted sender hosts)
X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
Content-Transfer-Encoding: 8bit
X-MIME-Autoconverted: from quoted-printable to 8bit by hypatia.math.ethz.ch id j0QDdhgh026614
X-Spam-Checker-Version: SpamAssassin 3.0.2 (2004-11-16) on hypatia.math.ethz.ch
X-Spam-Level: 
X-Spam-Status: No, score=0.0 required=5.0 tests=BAYES_50 autolearn=no version=3.0.2

hi R-friends,

i build a contigency table with the function table()  which looks like:

                 LAGECODE
BID                            1 2 10 11 12 13 14 15 19 21 22 23 24 31 32 46 47 54 56 57 62 67 70 71 80 81 82 430 460 ........
  200310413290143  0 0 0  0  0  1  0  0  0  0  0  0  0  0  0  1  0  0  0  0  1  1  0  0  0  0  1  0   0  ...
  200310413290144  0 0 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0   0 ...   
  200310413290145  0 0 0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0   0 .... 
  200310413290146  0 0 0  0  0  1  0  0  1 .....
.
.
.


i want to save this contigency table in a text-file. i tried the function write.matrix(tabelle, file="data.txt", sep=";") with following result in the text-file:

1;2;10;11;12;13;14;15;19;21;22;23;24;31;32;46;47;54;56;57;62;67;70;71;80;81;82;430;460
0;0;0;0;0;1;0;0;0;0;0;0;0;0;0;1;0;0;0;0;1;1;0;0;0;0;1;0;0
0;0;0;0;0;1;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;1;0;0;0;0;0;0;0
0;0;0;0;0;0;0;0;1;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;1;0;0;0

is it possible the get an text-file including BID in the first column?  ive studied the R-help, but i couldnt find any information about this.

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.0            
year     2004           
month    10             
day      04             
language R              

with thanks in advance
greetings
helli



From andy_liaw at merck.com  Wed Jan 26 15:24:44 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 26 Jan 2005 09:24:44 -0500
Subject: [R] Still avoiding loops
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5BC@usrymx25.merck.com>

See if this does what you want:

> m <- matrix(round(runif(24)), 4, 6)  # simulate some data
> m
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    0    1    0    1    0    1
[2,]    0    1    1    1    0    0
[3,]    1    1    1    0    0    0
[4,]    1    0    0    0    1    0
> library(gtools)  # Install the gregmisc package if you don't have it.
> idx <- combinations(nrow(m), 2)
> res <- m[idx[,1],] + m[idx[,2]]
> rownames(res) <- paste(idx[,1], idx[,2], sep="+")
> res
    [,1] [,2] [,3] [,4] [,5] [,6]
1+2    0    1    0    1    0    1
1+3    1    2    1    2    1    2
1+4    1    2    1    2    1    2
2+3    1    2    2    2    1    1
2+4    1    2    2    2    1    1
3+4    2    2    2    1    1    1

Andy
 

> From: Jacques VESLOT
> 
> It is part of a function to determine Dice's index in the 
> framewok of AFLP
> analysis.
> 
> X is a binary matrix which value for each strain (lines) and 
> each base pair
> (columns) is 1 where there is a peak and 0 where there is no peak as
> biologists explained to me.
> 
> The first step is to compare each strain with one another by 
> counting the
> number of 0, 1 and 2, respectively where there is no peak, 
> one peak or 2
> peaks for each base pair.
> 
> In that respect, I want to add together each pair of X's lines.
> 
> For the moment, there is a double loop calculating, at each 
> step, the sum of
> two lines as a vector Y and counting the number of 0, 1 and 2 
> in it for
> inclusion in other operations.
> 
> I read your posting...
> 
> Thanks for helping,
> 
> Jacques VESLOT
> 
> 
> -----Message d'origine-----
> De : Adaikalavan Ramasamy [mailto:ramasamy at cancer.org.uk]
> Envoy? : mercredi 26 janvier 2005 15:57
> ? : jacques.veslot at cirad.fr
> Cc : R-help
> Objet : Re: [R] Still avoiding loops
> 
> 
> Please give a simple example of the input data and output that you
> desire. It is difficult to understand from you partial codes what you
> mean. For example what is Y ?
> 
> Are you trying to find add values from pairs of rows ? If so, 
> please see
> my posting "pairwise difference operator" where I wanted to find the
> differences between pairs of columns.
> http://tolstoy.newcastle.edu.au/R/help/04/07/1633.html
> 
> Otherwise, please send a sample input and output. Thank you.
> 
> Regards, Adai
> 
> 
> On Wed, 2005-01-26 at 11:40 +0400, Jacques VESLOT wrote:
> > Dear all,
> >
> > I have a matrix X with 47 lines and say 500 columns - 
> values are in {0,1}.
> > I'd like to compare lines.
> >
> > For that, I first did:
> >
> > for (i in 1:(dim(X)[1]-1))
> > for (j in (i+1):dim(X)[1]) {
> > 	Y <- X[i,]+Y[j,]
> > 	etc.
> >
> > but, since it takes a long time, I would prefer avoding loops;
> > for that, my first idea was to add this matrix:
> >
> > X1=X[,rep(1:46,46:1)]
> >
> > to this one:
> >
> > res=NULL
> > for (i in (2:47)) res=c(res,i:47)
> >
> > X2=X[,res]
> >
> > (Is it a nice alternative way ?)
> > Is there a way to create the second matrix X2 without a 
> loop, such as for
> X1
> > ?
> >
> > Thanks in advance,
> >
> > Jacques VESLOT
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Wed Jan 26 15:31:29 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 26 Jan 2005 09:31:29 -0500
Subject: [R] save contigency table
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5BD@usrymx25.merck.com>

Try something like:

  write.table(unclass(tabelle), "data.txt", row=T, col=T, quote=F, sep=";")

HTH,
Andy

> From: Helmut Kudrnovsky
> 
> Content-Type: text/plain; charset="iso-8859-1"
> Received-SPF: none (hypatia: domain of hellik at web.de does not 
> designate permitted sender hosts)
> X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
> Content-Transfer-Encoding: 8bit
> X-MIME-Autoconverted: from quoted-printable to 8bit by 
> hypatia.math.ethz.ch id j0QDdhgh026614
> X-Spam-Checker-Version: SpamAssassin 3.0.2 (2004-11-16) on 
> hypatia.math.ethz.ch
> X-Spam-Level: 
> X-Spam-Status: No, score=0.0 required=5.0 tests=BAYES_50 
> autolearn=no version=3.0.2
> 
> hi R-friends,
> 
> i build a contigency table with the function table()  which 
> looks like:
> 
>                  LAGECODE
> BID                            1 2 10 11 12 13 14 15 19 21 22 
> 23 24 31 32 46 47 54 56 57 62 67 70 71 80 81 82 430 460 ........
>   200310413290143  0 0 0  0  0  1  0  0  0  0  0  0  0  0  0  
> 1  0  0  0  0  1  1  0  0  0  0  1  0   0  ...
>   200310413290144  0 0 0  0  0  1  0  0  0  0  0  0  0  0  0  
> 0  0  0  0  0  0  1  0  0  0  0  0  0   0 ...   
>   200310413290145  0 0 0  0  0  0  0  0  1  0  0  0  0  0  0  
> 0  0  0  0  0  0  0  0  0  0  1  0  0   0 .... 
>   200310413290146  0 0 0  0  0  1  0  0  1 .....
> .
> .
> .
> 
> 
> i want to save this contigency table in a text-file. i tried 
> the function write.matrix(tabelle, file="data.txt", sep=";") 
> with following result in the text-file:
> 
> 1;2;10;11;12;13;14;15;19;21;22;23;24;31;32;46;47;54;56;57;62;6
> 7;70;71;80;81;82;430;460
> 0;0;0;0;0;1;0;0;0;0;0;0;0;0;0;1;0;0;0;0;1;1;0;0;0;0;1;0;0
> 0;0;0;0;0;1;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;1;0;0;0;0;0;0;0
> 0;0;0;0;0;0;0;0;1;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;1;0;0;0
> 
> is it possible the get an text-file including BID in the 
> first column?  i?ve studied the R-help, but i couldn?t find 
> any information about this.
> 
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.0            
> year     2004           
> month    10             
> day      04             
> language R              
> 
> with thanks in advance
> greetings
> helli
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From angelare at to.infn.it  Wed Jan 26 15:35:41 2005
From: angelare at to.infn.it (Angela Re)
Date: Wed, 26 Jan 2005 15:35:41 +0100
Subject: [R] networks in R
Message-ID: <41F7AABD.5030203@to.infn.it>

Good afternoon,
do you know if R provides a special package to make networks ?
Thank you,
Angela



From reid_huntsinger at merck.com  Wed Jan 26 17:12:51 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 26 Jan 2005 11:12:51 -0500
Subject: [R] networks in R
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9288@uswpmx00.merck.com>

What kind of networks did you have in mind, and how did you want to make
them?

R has a few packages dealing with so-called Bayesian networks; at least one
social network analysis package; also some packages for graphs (as in graph
theory),...

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Angela Re
Sent: Wednesday, January 26, 2005 9:36 AM
To: r-help at stat.math.ethz.ch
Subject: [R] networks in R


Good afternoon,
do you know if R provides a special package to make networks ?
Thank you,
Angela

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From plummer at iarc.fr  Wed Jan 26 17:10:59 2005
From: plummer at iarc.fr (Martyn Plummer)
Date: Wed, 26 Jan 2005 17:10:59 +0100
Subject: [R] CODA vs. BOA discrepancy
In-Reply-To: <Pine.GSO.4.58.0501250342460.22916@godzilla.acpub.duke.edu>
References: <200501241218.j0OCIJTO026983@hypatia.math.ethz.ch>
	<Pine.GSO.4.58.0501250342460.22916@godzilla.acpub.duke.edu>
Message-ID: <1106755860.3421.126.camel@seurat>

On Tue, 2005-01-25 at 03:58 -0500, gc4 at duke.edu wrote:
> Dear List:
> 
> the CODA and BOA packages for the analysis of MCMC output yield different
> results on two dignostic test of convergence: 1) Geweke's convergence
> diagnostic; 2) Heidelberger and Welch's convergence diagnostic. Does that
> imply that the CODA and BOA packages implement different ``flavors'' of
> the same test?
> 
> I paste below an example.
> 
> Geweke's test
> cbind(coda.gwk,boa.gwk)
> 
>       z-score p-value Z-Score p-value
> b[1]    1.143   0.253   0.000   1.000
> b[2]    0.470   0.638   0.056   0.956
> b[3]   -1.037   0.300  -0.388   0.698
> b[4]   -0.618   0.536  -0.085   0.933
> tau    -0.206   0.837  -0.008   0.994
> sigma   0.716   0.474   0.437   0.662
> 
> CODA -- Heidelberger and Welch
> 
>       Stationarity start     p-value
>       test         iteration
> b[1]  passed       1         0.2649
> b[2]  passed       1         0.6709
> b[3]  passed       1         0.6376
> b[4]  passed       1         0.3673
> tau   passed       1         0.1944
> sigma passed       1         0.0725
> 
>       Halfwidth Mean    Halfwidth
>       test
> b[1]  passed    -39.800 0.303994
> b[2]  passed      0.714 0.003505
> b[3]  passed      1.297 0.010317
> b[4]  passed     -0.153 0.004025
> tau   passed      0.106 0.000918
> sigma passed      3.193 0.014986
> 
> BOA -- Heidelberger and Welch
> 
>       Stationarity Test Keep Discard      C-von-M
> b[1]             passed 5000       0 2.841419e-13
> b[2]             passed 5000       0 1.512525e-03
> b[3]             passed 5000       0 1.162634e-02
> b[4]             passed 5000       0 2.589256e-03
> tau              passed 5000       0 2.742828e-04
> sigma            passed 5000       0 1.086017e-01
> 
>        Halfwidth Test        Mean    Halfwidth
> b[1]           failed -39.8000008 2.569228e+04
> b[2]           passed   0.7137679 2.777682e-02
> b[3]           passed   1.2968816 2.812990e-02
> b[4]           failed  -0.1527912 2.778146e-02
> tau            failed   0.1063554 2.772179e-02
> sigma          passed   3.1930083 2.872075e-02
> 
> Thank you very much!
> giacomo
> 

I have a long list of bug reports for CODA, with patches, from Russell
Almond. When I have folded these in I suggest you try again, and if the
problem persists, send me your data so I can look into it.

Both of these diagnostics require an estimate of the spectral density at
zero.  I am using a modification of the method suggested by Heidelburger
and Welch (1981) - using GLMs insted of smoothing and transforming the
data to fit a linear regression model - but I haven't validated it, so
there might be circumstances where it performs badly.  In particular, if
you have a short run with high autocorrelation, there is no reliable
method of estimating the spectral density at zero, so if BOA is using
another method, such as batch means, there may be big differences.

Have a look at the help page for the function spectrum0().

Martyn



From bob at kruus.forestry.utoronto.ca  Wed Jan 26 17:31:26 2005
From: bob at kruus.forestry.utoronto.ca (Robert Kruus)
Date: Wed, 26 Jan 2005 11:31:26 -0500
Subject: [R] Still avoiding loops
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E5BC@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E5BC@usrymx25.merck.com>
Message-ID: <20050126113126.60b7f4df@kruuslt.forestry.utoronto.ca>

Slight edit?

--------

It is  rumored that on Wed, 26 Jan 2005 09:24:44 -0500
"Liaw, Andy" <andy_liaw at merck.com> wrote:

> See if this does what you want:
> 
> > m <- matrix(round(runif(24)), 4, 6)  # simulate some data
> > m
>      [,1] [,2] [,3] [,4] [,5] [,6]
> [1,]    0    1    0    1    0    1
> [2,]    0    1    1    1    0    0
> [3,]    1    1    1    0    0    0
> [4,]    1    0    0    0    1    0
> > library(gtools)  # Install the gregmisc package if you don't have
> > it. idx <- combinations(nrow(m), 2)
> > res <- m[idx[,1],] + m[idx[,2]]

I think you missed a "," (if you want pairwise row sums)
res <- m[idx[,1],] + m[idx[,2],]

> > rownames(res) <- paste(idx[,1], idx[,2], sep="+")
> > res
>     [,1] [,2] [,3] [,4] [,5] [,6]
> 1+2    0    1    0    1    0    1
> 1+3    1    2    1    2    1    2
> 1+4    1    2    1    2    1    2
> 2+3    1    2    2    2    1    1
> 2+4    1    2    2    2    1    1
> 3+4    2    2    2    1    1    1
> 
> Andy
>  
> 
> > From: Jacques VESLOT
> > 
> > It is part of a function to determine Dice's index in the 
> > framewok of AFLP
> > analysis.
> > 
> > X is a binary matrix which value for each strain (lines) and 
> > each base pair
> > (columns) is 1 where there is a peak and 0 where there is no peak as
> > biologists explained to me.
> > 
> > The first step is to compare each strain with one another by 
> > counting the
> > number of 0, 1 and 2, respectively where there is no peak, 
> > one peak or 2
> > peaks for each base pair.
> > 
> > In that respect, I want to add together each pair of X's lines.
> > 
> > For the moment, there is a double loop calculating, at each 
> > step, the sum of
> > two lines as a vector Y and counting the number of 0, 1 and 2 
> > in it for
> > inclusion in other operations.
> > 
> > I read your posting...
> > 
> > Thanks for helping,
> > 
> > Jacques VESLOT
> > 
> > 
> > -----Message d'origine-----
> > De : Adaikalavan Ramasamy [mailto:ramasamy at cancer.org.uk]
> > Envoy? : mercredi 26 janvier 2005 15:57
> > ? : jacques.veslot at cirad.fr
> > Cc : R-help
> > Objet : Re: [R] Still avoiding loops
> > 
> > 
> > Please give a simple example of the input data and output that you
> > desire. It is difficult to understand from you partial codes what
> > you mean. For example what is Y ?
> > 
> > Are you trying to find add values from pairs of rows ? If so, 
> > please see
> > my posting "pairwise difference operator" where I wanted to find the
> > differences between pairs of columns.
> > http://tolstoy.newcastle.edu.au/R/help/04/07/1633.html
> > 
> > Otherwise, please send a sample input and output. Thank you.
> > 
> > Regards, Adai
> > 
> > 
> > On Wed, 2005-01-26 at 11:40 +0400, Jacques VESLOT wrote:
> > > Dear all,
> > >
> > > I have a matrix X with 47 lines and say 500 columns - 
> > values are in {0,1}.
> > > I'd like to compare lines.
> > >
> > > For that, I first did:
> > >
> > > for (i in 1:(dim(X)[1]-1))
> > > for (j in (i+1):dim(X)[1]) {
> > > 	Y <- X[i,]+Y[j,]
> > > 	etc.
> > >
> > > but, since it takes a long time, I would prefer avoding loops;
> > > for that, my first idea was to add this matrix:
> > >
> > > X1=X[,rep(1:46,46:1)]
> > >
> > > to this one:
> > >
> > > res=NULL
> > > for (i in (2:47)) res=c(res,i:47)
> > >
> > > X2=X[,res]
> > >
> > > (Is it a nice alternative way ?)
> > > Is there a way to create the second matrix X2 without a 
> > loop, such as for
> > X1
> > > ?
> > >
> > > Thanks in advance,
> > >
> > > Jacques VESLOT
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 


--------

-- 
robert.kruus at utoronto.ca
"There are ten church members by inheritance for every one by
conviction."                           [Anonymous]
t



From helprhelp at gmail.com  Wed Jan 26 17:33:58 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Wed, 26 Jan 2005 11:33:58 -0500
Subject: [R] how to evaluate the significance of attributes in tree growing
Message-ID: <cdf8178305012608335cfb6de2@mail.gmail.com>

Hi, there:

I am wondering if there is a package in R (doing decison trees) which
can provide some methods to evaluate the significance of attributes. I
remembered randomForest gives some output like that. Unfortunately my
current computing env. cannot handle my datasets if I use
randomForest. So, I am thinking if other packages can do this job or
not.


Thanks,

Ed



From yshao at wadsworth.org  Wed Jan 26 17:33:24 2005
From: yshao at wadsworth.org (Yu Shao)
Date: Wed, 26 Jan 2005 11:33:24 -0500
Subject: [R] Source code for "extractAIC"?
Message-ID: <41F7C654.7030702@wadsworth.org>

Dear R users:

I am looking for the source code for the R function extractAIC. Type the 
function name doesn't help:

 > extractAIC
function (fit, scale, k = 2, ...)
UseMethod("extractAIC")
<environment: namespace:stats>

And when I search it in the R source code, the best I can find is in (R 
source root)/library/stats/R/add.R:

extractAIC <- function(fit, scale, k = 2, ...) UseMethod("extractAIC")


Could anyone point out to me where I can its source code? And my R 
version is:

 > version
          _
platform sparc-sun-solaris2.9
arch     sparc
os       solaris2.9
system   sparc, solaris2.9
status
major    1
minor    9.0
year     2004
month    04
day      12
language R


Thanks,

-- 
Yu Shao
Bioinformatics Group, Wadsworth Research Center
New York State Department of Health



From spencer.graves at pdf.com  Wed Jan 26 17:36:05 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 26 Jan 2005 08:36:05 -0800
Subject: [R] networks in R
In-Reply-To: <D9A95B4B7B20354992E165EEADA31999056A9288@uswpmx00.merck.com>
References: <D9A95B4B7B20354992E165EEADA31999056A9288@uswpmx00.merck.com>
Message-ID: <41F7C6F5.2000007@pdf.com>

Angela: 

      Also, have you done a search at "www.r-project.org" -> search -> 
"R site search"?  A search for "networks" there just now produced 218 
hits. 

      Have you reviewed the list of officially contributed packages is 
available at "www.r-project.org" -> CRAN -> (select a local mirror like 
"http://microarrays.unife.it/CRAN/") -> Software:  Packages.  Since the 
"search" in the web browser didn't work, I copied the page into MS Word 
(paste special, text only), and found one reference to networks:  "deal 
Learning Bayesian Networks with Mixed Variables". 

      Finally, have you "read the posting guide! 
http://www.R-project.org/posting-guide.html"?  This contains many 
suggestions for how to get information yourself and how to formulate 
questions so they are more likely to elicit useful answers.  

      hope this helps. 
      spencer graves

Huntsinger, Reid wrote:

>What kind of networks did you have in mind, and how did you want to make
>them?
>
>R has a few packages dealing with so-called Bayesian networks; at least one
>social network analysis package; also some packages for graphs (as in graph
>theory),...
>
>Reid Huntsinger
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Angela Re
>Sent: Wednesday, January 26, 2005 9:36 AM
>To: r-help at stat.math.ethz.ch
>Subject: [R] networks in R
>
>
>Good afternoon,
>do you know if R provides a special package to make networks ?
>Thank you,
>Angela
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From asanchez at ub.edu  Wed Jan 26 17:49:58 2005
From: asanchez at ub.edu (Alexandre Sanchez Pla)
Date: Wed, 26 Jan 2005 17:49:58 +0100
Subject: [R] apply for nested lists
Message-ID: <1106758198.41f7ca36d969b@webmail3.ub.edu>

Hi,

I am working with lists whose terms are lists whose terms are lists. Although 
the real ones contain locuslink identifiers and GO annotations (I work with the 
Bioconductor GO) package, I have prepared an simplified example of what I have 
and what I would like to do with it:

Imagine I have a list such as:

tst.list<-list("1"=list("1A"=list(ID="1A",VAL=172),"1B"=list
(ID="1B",VAL=134),"1C"=list(ID="1C",VAL=0)),"2"=list("2A"=NA),"3"=list("3A"=list
(ID="3A",VAL=33),"3B"=list(ID="3B",VAL=2)))

I would like, for instance, to be able to extract some values such as the 
content of the "VAL" field, which may sometimes not be available.
I may do it using a nested for such as:

x<-character(0)
for (i in 1:length(tst.list)){
    if (!is.na(tst.list[[i]][[1]][[1]])){
        for (j in 1:length(tst.list[[i]]))
            {x<-c(x,tst.list[[i]][[j]]$VAL)}}
    else    
            {x<-c(x, NA)}}

which gives me what I need

> x
[1] "172" "134" "0"   NA  "33"  "2"  

According to most R documents this may be done more efficiently using apply 
instructions, but I have failed in my temptatives to obtain the same

Thanks for any help.

Alex


-- 
Dr.Alex S?nchez
Departament d'Estad?stica.
Universitat de Barcelona
asanchez at ub.edu



From rgentlem at fhcrc.org  Wed Jan 26 17:52:03 2005
From: rgentlem at fhcrc.org (Robert Gentleman)
Date: Wed, 26 Jan 2005 08:52:03 -0800
Subject: [R] networks in R
In-Reply-To: <41F7AABD.5030203@to.infn.it>
References: <41F7AABD.5030203@to.infn.it>
Message-ID: <9A896F8C-6FBA-11D9-B479-000D933DC9FE@fhcrc.org>

Sort of, there are three packages in Bioconductor (graph, RBGL and  
Rgraphviz), the second two require that you have installed additional  
libraries (and only one of those two, namely RBGL, works on windows;  
but we are working on Rgraphviz and hope to have something soon). There  
are also some packages on CRAN (sna and dynamicGraph)


    Robert

On Jan 26, 2005, at 6:35 AM, Angela Re wrote:

> Good afternoon,
> do you know if R provides a special package to make networks ?
> Thank you,
> Angela
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!  
> http://www.R-project.org/posting-guide.html
>
>
+----------------------------------------------------------------------- 
----------------+
| Robert Gentleman              phone: (206) 667-7700                    
          |
| Head, Program in Computational Biology   fax:  (206) 667-1319   |
| Division of Public Health Sciences       office: M2-B865               
       |
| Fred Hutchinson Cancer Research Center                                 
          |
| email: rgentlem at fhcrc.org                                              
                          |
+----------------------------------------------------------------------- 
----------------+



From andy_liaw at merck.com  Wed Jan 26 18:07:27 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 26 Jan 2005 12:07:27 -0500
Subject: [R] Still avoiding loops
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5BE@usrymx25.merck.com>

> From: Robert Kruus
> 
> Slight edit?

Yes.  Thank you. Jacques caught that as well...

Andy
 
> --------
> 
> It is  rumored that on Wed, 26 Jan 2005 09:24:44 -0500
> "Liaw, Andy" <andy_liaw at merck.com> wrote:
> 
> > See if this does what you want:
> > 
> > > m <- matrix(round(runif(24)), 4, 6)  # simulate some data
> > > m
> >      [,1] [,2] [,3] [,4] [,5] [,6]
> > [1,]    0    1    0    1    0    1
> > [2,]    0    1    1    1    0    0
> > [3,]    1    1    1    0    0    0
> > [4,]    1    0    0    0    1    0
> > > library(gtools)  # Install the gregmisc package if you don't have
> > > it. idx <- combinations(nrow(m), 2)
> > > res <- m[idx[,1],] + m[idx[,2]]
> 
> I think you missed a "," (if you want pairwise row sums)
> res <- m[idx[,1],] + m[idx[,2],]
> 
> > > rownames(res) <- paste(idx[,1], idx[,2], sep="+")
> > > res
> >     [,1] [,2] [,3] [,4] [,5] [,6]
> > 1+2    0    1    0    1    0    1
> > 1+3    1    2    1    2    1    2
> > 1+4    1    2    1    2    1    2
> > 2+3    1    2    2    2    1    1
> > 2+4    1    2    2    2    1    1
> > 3+4    2    2    2    1    1    1
> > 
> > Andy
> >  
> > 
> > > From: Jacques VESLOT
> > > 
> > > It is part of a function to determine Dice's index in the 
> > > framewok of AFLP
> > > analysis.
> > > 
> > > X is a binary matrix which value for each strain (lines) and 
> > > each base pair
> > > (columns) is 1 where there is a peak and 0 where there is 
> no peak as
> > > biologists explained to me.
> > > 
> > > The first step is to compare each strain with one another by 
> > > counting the
> > > number of 0, 1 and 2, respectively where there is no peak, 
> > > one peak or 2
> > > peaks for each base pair.
> > > 
> > > In that respect, I want to add together each pair of X's lines.
> > > 
> > > For the moment, there is a double loop calculating, at each 
> > > step, the sum of
> > > two lines as a vector Y and counting the number of 0, 1 and 2 
> > > in it for
> > > inclusion in other operations.
> > > 
> > > I read your posting...
> > > 
> > > Thanks for helping,
> > > 
> > > Jacques VESLOT
> > > 
> > > 
> > > -----Message d'origine-----
> > > De : Adaikalavan Ramasamy [mailto:ramasamy at cancer.org.uk]
> > > Envoy? : mercredi 26 janvier 2005 15:57
> > > ? : jacques.veslot at cirad.fr
> > > Cc : R-help
> > > Objet : Re: [R] Still avoiding loops
> > > 
> > > 
> > > Please give a simple example of the input data and output that you
> > > desire. It is difficult to understand from you partial codes what
> > > you mean. For example what is Y ?
> > > 
> > > Are you trying to find add values from pairs of rows ? If so, 
> > > please see
> > > my posting "pairwise difference operator" where I wanted 
> to find the
> > > differences between pairs of columns.
> > > http://tolstoy.newcastle.edu.au/R/help/04/07/1633.html
> > > 
> > > Otherwise, please send a sample input and output. Thank you.
> > > 
> > > Regards, Adai
> > > 
> > > 
> > > On Wed, 2005-01-26 at 11:40 +0400, Jacques VESLOT wrote:
> > > > Dear all,
> > > >
> > > > I have a matrix X with 47 lines and say 500 columns - 
> > > values are in {0,1}.
> > > > I'd like to compare lines.
> > > >
> > > > For that, I first did:
> > > >
> > > > for (i in 1:(dim(X)[1]-1))
> > > > for (j in (i+1):dim(X)[1]) {
> > > > 	Y <- X[i,]+Y[j,]
> > > > 	etc.
> > > >
> > > > but, since it takes a long time, I would prefer avoding loops;
> > > > for that, my first idea was to add this matrix:
> > > >
> > > > X1=X[,rep(1:46,46:1)]
> > > >
> > > > to this one:
> > > >
> > > > res=NULL
> > > > for (i in (2:47)) res=c(res,i:47)
> > > >
> > > > X2=X[,res]
> > > >
> > > > (Is it a nice alternative way ?)
> > > > Is there a way to create the second matrix X2 without a 
> > > loop, such as for
> > > X1
> > > > ?
> > > >
> > > > Thanks in advance,
> > > >
> > > > Jacques VESLOT
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > > >
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > > 
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> 
> 
> --------
> 
> -- 
> robert.kruus at utoronto.ca
> "There are ten church members by inheritance for every one by
> conviction."                           [Anonymous]
> t
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From spencer.graves at pdf.com  Wed Jan 26 18:14:01 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 26 Jan 2005 09:14:01 -0800
Subject: [R] Source code for "extractAIC"?
In-Reply-To: <41F7C654.7030702@wadsworth.org>
References: <41F7C654.7030702@wadsworth.org>
Message-ID: <41F7CFD9.60804@pdf.com>

      In R 2.0.1 under Windows 2000: 

 > methods("extractAIC")
[1] extractAIC.aov*     extractAIC.coxph*   extractAIC.glm*   
[4] extractAIC.lm*      extractAIC.negbin*  extractAIC.survreg*

    Non-visible functions are asterisked
 > getAnywhere("extractAIC.lm")
A single object matching 'extractAIC.lm' was found
It was found in the following places
  registered S3 method for extractAIC from namespace stats
  namespace:stats
with value

function (fit, scale = 0, k = 2, ...)
{
    n <- length(fit$residuals)
    edf <- n - fit$df.residual
    RSS <- deviance.lm(fit)
    dev <- if (scale > 0)
        RSS/scale - n
    else n * log(RSS/n)
    c(edf, dev + k * edf)
}
<environment: namespace:stats>
 >
      hope this helps.  spencer graves

Yu Shao wrote:

> Dear R users:
>
> I am looking for the source code for the R function extractAIC. Type 
> the function name doesn't help:
>
> > extractAIC
> function (fit, scale, k = 2, ...)
> UseMethod("extractAIC")
> <environment: namespace:stats>
>
> And when I search it in the R source code, the best I can find is in 
> (R source root)/library/stats/R/add.R:
>
> extractAIC <- function(fit, scale, k = 2, ...) UseMethod("extractAIC")
>
>
> Could anyone point out to me where I can its source code? And my R 
> version is:
>
> > version
>          _
> platform sparc-sun-solaris2.9
> arch     sparc
> os       solaris2.9
> system   sparc, solaris2.9
> status
> major    1
> minor    9.0
> year     2004
> month    04
> day      12
> language R
>
>
> Thanks,
>



From andy_liaw at merck.com  Wed Jan 26 18:20:07 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 26 Jan 2005 12:20:07 -0500
Subject: [R] Source code for "extractAIC"?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5BF@usrymx25.merck.com>

Isn't this in the FAQ?

Try:


> methods("extractAIC")
[1] extractAIC.aov*     extractAIC.coxph*   extractAIC.glm*
extractAIC.lm*     
[5] extractAIC.negbin*  extractAIC.survreg*

    Non-visible functions are asterisked

Then use getAnywhere() or getS3method() to get the particular one you're
interested in.

Andy

> From: Yu Shao
> 
> Dear R users:
> 
> I am looking for the source code for the R function 
> extractAIC. Type the 
> function name doesn't help:
> 
>  > extractAIC
> function (fit, scale, k = 2, ...)
> UseMethod("extractAIC")
> <environment: namespace:stats>
> 
> And when I search it in the R source code, the best I can 
> find is in (R 
> source root)/library/stats/R/add.R:
> 
> extractAIC <- function(fit, scale, k = 2, ...) UseMethod("extractAIC")
> 
> 
> Could anyone point out to me where I can its source code? And my R 
> version is:
> 
>  > version
>           _
> platform sparc-sun-solaris2.9
> arch     sparc
> os       solaris2.9
> system   sparc, solaris2.9
> status
> major    1
> minor    9.0
> year     2004
> month    04
> day      12
> language R
> 
> 
> Thanks,
> 
> -- 
> Yu Shao
> Bioinformatics Group, Wadsworth Research Center
> New York State Department of Health
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Wed Jan 26 18:26:39 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 26 Jan 2005 12:26:39 -0500
Subject: [R] apply for nested lists
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5C0@usrymx25.merck.com>

Here's one (not so general) way:

> x <- unlist(tst.list)
> x[grep("VAL", names(x))]
1.1A.VAL 1.1B.VAL 1.1C.VAL 3.3A.VAL 3.3B.VAL 
   "172"    "134"      "0"     "33"      "2" 

Do you need the NA?

Andy

> From: Alexandre Sanchez Pla
> 
> Hi,
> 
> I am working with lists whose terms are lists whose terms are 
> lists. Although 
> the real ones contain locuslink identifiers and GO 
> annotations (I work with the 
> Bioconductor GO) package, I have prepared an simplified 
> example of what I have 
> and what I would like to do with it:
> 
> Imagine I have a list such as:
> 
> tst.list<-list("1"=list("1A"=list(ID="1A",VAL=172),"1B"=list
> (ID="1B",VAL=134),"1C"=list(ID="1C",VAL=0)),"2"=list("2A"=NA),
> "3"=list("3A"=list
> (ID="3A",VAL=33),"3B"=list(ID="3B",VAL=2)))
> 
> I would like, for instance, to be able to extract some values 
> such as the 
> content of the "VAL" field, which may sometimes not be available.
> I may do it using a nested for such as:
> 
> x<-character(0)
> for (i in 1:length(tst.list)){
>     if (!is.na(tst.list[[i]][[1]][[1]])){
>         for (j in 1:length(tst.list[[i]]))
>             {x<-c(x,tst.list[[i]][[j]]$VAL)}}
>     else    
>             {x<-c(x, NA)}}
> 
> which gives me what I need
> 
> > x
> [1] "172" "134" "0"   NA  "33"  "2"  
> 
> According to most R documents this may be done more 
> efficiently using apply 
> instructions, but I have failed in my temptatives to obtain the same
> 
> Thanks for any help.
> 
> Alex
> 
> 
> -- 
> Dr.Alex S?nchez
> Departament d'Estad?stica.
> Universitat de Barcelona
> asanchez at ub.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From b-li1 at northwestern.edu  Wed Jan 26 18:54:15 2005
From: b-li1 at northwestern.edu (Bobai Li)
Date: Wed, 26 Jan 2005 11:54:15 -0600
Subject: [R] some questions about font
In-Reply-To: <41F6D362.1020608@stat.auckland.ac.nz>
References: <41F14789.7020808@northwestern.edu>
	<41F59675.40205@stat.auckland.ac.nz>
	<41F6D362.1020608@stat.auckland.ac.nz>
Message-ID: <41F7D947.8070802@northwestern.edu>


Dear Paul,

Thank you for your help!

1. I know how to use "expression(italic(whatever))", but I don't know 
how to make "expression(whatever)" to produce italic font.

2. With regard to the CM font, I guess psfrag may be to best way to get 
around the problem. Indeed, I have been using psfrag since I am also 
using LaTeX. My only complaint is that I have yet to find out an easy 
way to export  the modified  graph  back to  the EPS format (to be used 
in other documents).  I know how to do it the hard way, but here I am 
talking about handling some 100 graphic files (It would be more 
efficient to have a R file to create all graphic files with psfrag tags 
and a latex file to replace the tags and export individual graphs to 
EPS; and it would be ideal if the latex file can also be generated 
automatically from R)

3. If you don't mind, I would like ask another question: how to write a 
function f(x) to define---      x <<- "x"

Many thanks for your help!

Bobai




Paul Murrell wrote:

> Hi
>
>
> Paul Murrell wrote:
>
>> Hi
>>
>>
>> Bobai Li wrote:
>>
>>>
>>> Hi,
>>>
>>> I have been using R to create some mathematical and statistical 
>>> graphs for a book manuscript, but I got some problems:
>>>
>>> 1)  Some web positngs said that default typeface for math 
>>> expressions is italic, but in my system (R 2.01 on WinXP), the 
>>> default is regular font.
>>> How can I change the default to ilatic?
>>
>>
>>
>>
>> expression(italic(whatever))
>>
>>
>>> 2)  When use ComputerModern font,  (i.e., 
>>> family=c("CM_regular_10.afm","CM_boldx_10.afm","cmti10.afm","cmbxti10.afm","CM_symbol_10.afm") 
>>> ), some accented symbols are not available. For example, 
>>> "expression(hat(beta))" will produce warning message like "font 
>>> metrics unknown for character 94."
>>
>>
>>
>>
>> That appears to be a bug (that requires changes to the PostScript 
>> device driver).  A nastyish workaround for hat(beta) is 
>> widehat(beta), but I suspect there are other problems that this will 
>> not solve.
>
>
>
> Some good news and some bad news.
>
> The good news is that this can be worked around fairly simply (i.e., 
> without installing a new version of R) as follows:
> (i) make a copy of $R_HOME/library/grDevices/afm/ISOLatin1.enc (or 
> possibly $R_HOME/afm/ISOLatin1.enc, depending on your R version) and 
> call the copy CMISOLatin1.enc.  Modify CMISOLatin1.enc so that the 
> second line starts with CMISOLatin1Encoding (rather than 
> ISOLatin1Encoding) and (further down the file) change \asciicircum to 
> \circumflex and \asciitilde to \tilde.
> (ii) when opening your PostScript device, as well as specifying the 
> family argument as you do above, specify encoding="CMISOLatin1".
>
> The bad news is that R does not position the accents as well as LaTeX 
> does it (e.g., a hat is not located above a beta in quite the same 
> place).
>
> Do you know about PSfrag ...?
>
> Paul



From andrewr at uidaho.edu  Wed Jan 26 19:06:04 2005
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Thu, 27 Jan 2005 05:06:04 +1100
Subject: [R] lme and varFunc()
Message-ID: <3c489e3c5e01.3c5e013c489e@uidaho.edu>

Christoph,

----- Original Message -----
From: Christoph Scherber <Christoph.Scherber at uni-jena.de>
Date: Wednesday, January 26, 2005 7:57 pm
Subject: Re: [R] lme and varFunc()

> Dear all,
> 
> I am expecting a Poisson error distribution in my lme with 
> weights=varFunc().
> 
> The "weigths= varPower (form= fitted (.))" doesn?t work due to 
> missing 
> values in the response:
> 
> Problem in lme.formula(fixed = sqrt(nrmainaxes + 0...: Maximum 
> number of iterations reached without convergence. 
> Use traceback() to see the call stack

Are you certain that this is caused by missing values in the response?  It looks like something quite different to me.  How did the response come to have missing values?  

> That?s why I?ve used one of my most important explanatory 
> variables as a variance covariate:
> 
> weigths= varPower (form=~explanatory)
> 
> With that, it worked out properly so far.
> 
> What would your suggestion in such a case be?

Does it satisfy your needs with regards to the model diagnostics?  If so, then ask yourself: am I fitting a Poisson because it is an important and intrinsic part of the model that I need to construct, or am I fitting it in order to satisfy the model assumptions?  If the latter then it seems that the Poisson might be unnecessary.

I hope that this helps,

Andrew



From gunter.berton at gene.com  Wed Jan 26 19:07:37 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 26 Jan 2005 10:07:37 -0800
Subject: [R] apply for nested lists
In-Reply-To: <1106758198.41f7ca36d969b@webmail3.ub.edu>
Message-ID: <200501261807.j0QI7bvr009107@hertz.gene.com>

apply() statements **are** disguised loops and therefore are **not**
necessarily more efficient than explicit looping. Their principal advantage
is usually code readability. 

As another readability issue, note that x[[i]][[j]][[k]] can be abbreviated
to x[[c(i,j,k]].

I leave to others the pleasure of deciphering and improving your code,
however. 

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Alexandre Sanchez Pla
> Sent: Wednesday, January 26, 2005 8:50 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] apply for nested lists
> 
> Hi,
> 
> I am working with lists whose terms are lists whose terms are 
> lists. Although 
> the real ones contain locuslink identifiers and GO 
> annotations (I work with the 
> Bioconductor GO) package, I have prepared an simplified 
> example of what I have 
> and what I would like to do with it:
> 
> Imagine I have a list such as:
> 
> tst.list<-list("1"=list("1A"=list(ID="1A",VAL=172),"1B"=list
> (ID="1B",VAL=134),"1C"=list(ID="1C",VAL=0)),"2"=list("2A"=NA),
> "3"=list("3A"=list
> (ID="3A",VAL=33),"3B"=list(ID="3B",VAL=2)))
> 
> I would like, for instance, to be able to extract some values 
> such as the 
> content of the "VAL" field, which may sometimes not be available.
> I may do it using a nested for such as:
> 
> x<-character(0)
> for (i in 1:length(tst.list)){
>     if (!is.na(tst.list[[i]][[1]][[1]])){
>         for (j in 1:length(tst.list[[i]]))
>             {x<-c(x,tst.list[[i]][[j]]$VAL)}}
>     else    
>             {x<-c(x, NA)}}
> 
> which gives me what I need
> 
> > x
> [1] "172" "134" "0"   NA  "33"  "2"  
> 
> According to most R documents this may be done more 
> efficiently using apply 
> instructions, but I have failed in my temptatives to obtain the same
> 
> Thanks for any help.
> 
> Alex
> 
> 
> -- 
> Dr.Alex S?nchez
> Departament d'Estad?stica.
> Universitat de Barcelona
> asanchez at ub.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From parkhurs at ariel.ucs.indiana.edu  Wed Jan 26 19:06:40 2005
From: parkhurs at ariel.ucs.indiana.edu (David Parkhurst)
Date: Wed, 26 Jan 2005 13:06:40 -0500
Subject: [R] Converting yr mo da to dates
Message-ID: <001001c503d2$080d6c00$7c7e4f81@ads.iu.edu>

I'm using R 2.0.1 in windows XP (and am not currently subscribed to this 
mailing list).

I have a USGS dataset, a text file with fixed width fields, that includes 
dates as 6-digit integers in the form yrmoda.  I could either read them that 
way, or with yr, mo, and da as separate integers.  In either case, I'd like 
to convert them to a form will allow plotting other "y" variables against 
the dates (with correct spacing) on the horizontal axis.

I've looked in all the manuals, but didn't find a way to do this.  I can 
copy the data to a spreadsheet, make the conversion there, and then move the 
data to R, but that's a nuisance.

I'd appreciate learning whether there is a way to do this all within R. 
Thanks.

Dave Parkhurst



From ggrothendieck at myway.com  Wed Jan 26 19:09:13 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 26 Jan 2005 18:09:13 +0000 (UTC)
Subject: [R] apply for nested lists
References: <1106758198.41f7ca36d969b@webmail3.ub.edu>
Message-ID: <loom.20050126T190711-870@post.gmane.org>

Alexandre Sanchez Pla <asanchez <at> ub.edu> writes:

: 
: Hi,
: 
: I am working with lists whose terms are lists whose terms are lists. 
Although 
: the real ones contain locuslink identifiers and GO annotations (I work with 
the 
: Bioconductor GO) package, I have prepared an simplified example of what I 
have 
: and what I would like to do with it:
: 
: Imagine I have a list such as:
: 
: tst.list<-list("1"=list("1A"=list(ID="1A",VAL=172),"1B"=list
: (ID="1B",VAL=134),"1C"=list(ID="1C",VAL=0)),"2"=list("2A"=NA),"3"=list
("3A"=list
: (ID="3A",VAL=33),"3B"=list(ID="3B",VAL=2)))
: 
: I would like, for instance, to be able to extract some values such as the 
: content of the "VAL" field, which may sometimes not be available.
: I may do it using a nested for such as:
: 
: x<-character(0)
: for (i in 1:length(tst.list)){
:     if (!is.na(tst.list[[i]][[1]][[1]])){
:         for (j in 1:length(tst.list[[i]]))
:             {x<-c(x,tst.list[[i]][[j]]$VAL)}}
:     else    
:             {x<-c(x, NA)}}
: 
: which gives me what I need
: 
: > x
: [1] "172" "134" "0"   NA  "33"  "2"  
: 
: According to most R documents this may be done more efficiently using apply 
: instructions, but I have failed in my temptatives to obtain the same
: 
: Thanks for any help.
: 

Try this:

f <- function(x) if (is.null(x$VAL)) NA else x$VAL
unlist(lapply(tst.list, lapply, f))



From kbartz at loyaltymatrix.com  Wed Jan 26 19:28:32 2005
From: kbartz at loyaltymatrix.com (Kevin Bartz)
Date: Wed, 26 Jan 2005 10:28:32 -0800
Subject: [R] apply for nested lists
In-Reply-To: <1106758198.41f7ca36d969b@webmail3.ub.edu>
References: <1106758198.41f7ca36d969b@webmail3.ub.edu>
Message-ID: <41F7E150.6070100@loyaltymatrix.com>

Actually, what you want is sapply.

sapply(tst.list, "[[", "VAL")

Kevin

Alexandre Sanchez Pla wrote:
> Hi,
> 
> I am working with lists whose terms are lists whose terms are lists. Although 
> the real ones contain locuslink identifiers and GO annotations (I work with the 
> Bioconductor GO) package, I have prepared an simplified example of what I have 
> and what I would like to do with it:
> 
> Imagine I have a list such as:
> 
> tst.list<-list("1"=list("1A"=list(ID="1A",VAL=172),"1B"=list
> (ID="1B",VAL=134),"1C"=list(ID="1C",VAL=0)),"2"=list("2A"=NA),"3"=list("3A"=list
> (ID="3A",VAL=33),"3B"=list(ID="3B",VAL=2)))
> 
> I would like, for instance, to be able to extract some values such as the 
> content of the "VAL" field, which may sometimes not be available.
> I may do it using a nested for such as:
> 
> x<-character(0)
> for (i in 1:length(tst.list)){
>     if (!is.na(tst.list[[i]][[1]][[1]])){
>         for (j in 1:length(tst.list[[i]]))
>             {x<-c(x,tst.list[[i]][[j]]$VAL)}}
>     else    
>             {x<-c(x, NA)}}
> 
> which gives me what I need
> 
> 
>>x
> 
> [1] "172" "134" "0"   NA  "33"  "2"  
> 
> According to most R documents this may be done more efficiently using apply 
> instructions, but I have failed in my temptatives to obtain the same
> 
> Thanks for any help.
> 
> Alex
> 
>



From ywang at fhcrc.org  Wed Jan 26 19:40:59 2005
From: ywang at fhcrc.org (Wang, Yan)
Date: Wed, 26 Jan 2005 10:40:59 -0800
Subject: [R] postscript() and levelplot() in a for loop
Message-ID: <9667A0D2033CD51195F90002B330A3BF0CCFB200@MOE.FHCRC.ORG>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050126/e88c9cba/attachment.pl

From Benjamin.Osborne at uvm.edu  Wed Jan 26 19:46:56 2005
From: Benjamin.Osborne at uvm.edu (Benjamin M. Osborne)
Date: Wed, 26 Jan 2005 13:46:56 -0500
Subject: [R] summarizing daily time-series date by month
Message-ID: <1106765216.41f7e5a0a9951@webmail.uvm.edu>

Message: 63
Date: Wed, 26 Jan 2005 04:28:51 +0000 (UTC)
From: Gabor Grothendieck <ggrothendieck at myway.com>
Subject: Re: [R] chron: parsing dates into a data frame using a
        forloop
To: r-help at stat.math.ethz.ch
Message-ID: <loom.20050126T052153-333 at post.gmane.org>
Content-Type: text/plain; charset=us-ascii

Benjamin M. Osborne <Benjamin.Osborne <at> uvm.edu> writes:

:
: I have one data frame with a column of dates and I want to fill another data
: frame with one column of dates, one of years, one of months, one of a unique
: combination of year and month, and one of days, but R seems to have some
: problems with this.  My initial data frame looks like this (ignore the NAs in
: the other fields):
:
: > mans[1:10,]
:        date loc snow.new prcp tmin snow.dep tmax
: 1  11/01/54   2       NA   NA   NA       NA   NA
: 2  11/02/54   2       NA   NA   NA       NA   NA
: 3  11/03/54   2       NA   NA   NA       NA   NA
: 4  11/04/54   2       NA   NA   NA       NA   NA
: 5  11/05/54   2       NA   NA   NA       NA   NA
: 6  11/06/54   2       NA   NA   NA       NA   NA
: 7  11/07/54   2       NA   NA   NA       NA   NA
: 8  11/08/54   2       NA   NA   NA       NA   NA
: 9  11/09/54   2       NA   NA   NA       NA   NA
: 10 11/10/54   2       NA   NA   NA       NA   NA
: >
:
: The code and resultant data frame look like this:
:
: > for(i in 1:10){
: + mans.met$date[i]<-mans$date[i]
: + mans.met$year[i]<-years(mans.met$date[i])
: + mans.met$month[i]<-months(mans.met$date[i])
: + mans.met$yearmo[i]<-cut(mans.met$date[i], "months")
: + mans.met$day[i]<-days(mans.met$date[i])
: + }
: > mans.met[1:10,]
:        date year month yearmo day snow.new snow.dep prcp tmin tmax tmean
: 1  11/01/54    1    11      1   1       NA       NA   NA   NA   NA    NA
: 2  11/02/54    1    11      1   2       NA       NA   NA   NA   NA    NA
: 3  11/03/54    1    11      1   3       NA       NA   NA   NA   NA    NA
: 4  11/04/54    1    11      1   4       NA       NA   NA   NA   NA    NA
: 5  11/05/54    1    11      1   5       NA       NA   NA   NA   NA    NA
: 6  11/06/54    1    11      1   6       NA       NA   NA   NA   NA    NA
: 7  11/07/54    1    11      1   7       NA       NA   NA   NA   NA    NA
: 8  11/08/54    1    11      1   8       NA       NA   NA   NA   NA    NA
: 9  11/09/54    1    11      1   9       NA       NA   NA   NA   NA    NA
: 10 11/10/54    1    11      1  10       NA       NA   NA   NA   NA    NA
: >
:
: The problem seems to be with assigning within the forloop, or making the
: assignment into a data frame, since:
:
: > years(mans.met$date[5])
: [1] 1954
: Levels: 1954
: > test<-years(mans.met$date[5])
: > test
: [1] 1954
: Levels: 1954
: >
: > months(mans.met$date[5])
: [1] Nov
: 12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec
: > test<-months(mans.met$date[5])
: > test
: [1] Nov
: 12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec
: >
: > cut(mans.met$date[3], "months")
: [1] Nov 54
: Levels: Nov 54
: > test<-cut(mans.met$date[3], "months")
: > test
: [1] Nov 54
: Levels: Nov 54
: >
: > days(mans.met$date[4])
: [1] 4
: 31 Levels: 1 < 2 < 3 < 4 < 5 < 6 < 7 < 8 < 9 < 10 < 11 < 12 < 13 < ... < 31
: > test<-days(mans.met$date[4])
: > test
: [1] 4
: 31 Levels: 1 < 2 < 3 < 4 < 5 < 6 < 7 < 8 < 9 < 10 < 11 < 12 < 13 < ... < 31
: >
:
: Any suggestions will be appreciated.
: -Ben Osborne

I guess you set up mans.met as numeric columns and when you
assign your factors to numeric variables you get
the underlying codes.  Note that if f is a factor then as.numeric(f)
gives the codes underlying the factor whereas as.character(f) gives
the labels.

It would be better not to use a loop at all.  I don't know whether you
want or not want factors but at any rate here is something you could
try.  It creates data frame df2 without a loop.

df2 <- data.frame(date = mans$date, yearmo = as.character(cut(mans$date, "m")))
df2 <- cbind(df2, month.day.year(mans$date))

Finally, do you really want this redundant representation?  I would tend to
go with just storing the dates and computing any of the other quantities
on-the-fly as needed.

##########
The reason for the redundancy is that I will want to summarize these 50 years of
daily time series data by month, so that records that share each unique year
and month in the mans.met$yearmo column will be summed or averaged, etc. into a
new row in another data frame(mans.monthly, having
nrow=length(unique(mans.met$yearmo))).  The way I would do this is again using
a forloop, but the loop won't recognize :
     for (i in 1:(length(unique(mans.met$yearmo[i])))){

What I really need to know is why I can call any ith of
     unique(mans.met$yearmo[i])
by itself, but not in a loop.

Or, perhaps there is an even easier way to extract the year and month from the
date
column on the fly to compute these summaries?

Thanks,
Ben Osborne

-- 
Botany Department
University of Vermont
109 Carrigan Drive
Burlington, VT 05405

benjamin.osborne at uvm.edu
phone: 802-656-0297
fax: 802-656-0440



From chris at subtlety.com  Wed Jan 26 19:54:11 2005
From: chris at subtlety.com (Chris Bergstresser)
Date: Wed, 26 Jan 2005 12:54:11 -0600
Subject: [R] Linear Trend Analysis?
Message-ID: <200501261854.j0QIsWxn011885@hypatia.math.ethz.ch>

Hi all --

   I'm trying to use R for my "Analysis of Categorical Data" class, but I
can't figure out how to do a weighted linear trend analysis.  I have a table
of categorical data, to which I've assigned weights to the rows and columns.
I need to calculate r and M^2, which is apparently done in SAS using "PROC
FREQ" and in STATA using "correlate var1 var2 fw=count".  What's the command
for R?

-- Chris



From tlumley at u.washington.edu  Wed Jan 26 20:00:58 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 26 Jan 2005 11:00:58 -0800 (PST)
Subject: [R] postscript() and levelplot() in a for loop
In-Reply-To: <9667A0D2033CD51195F90002B330A3BF0CCFB200@MOE.FHCRC.ORG>
References: <9667A0D2033CD51195F90002B330A3BF0CCFB200@MOE.FHCRC.ORG>
Message-ID: <Pine.A41.4.61b.0501261100040.294700@homer09.u.washington.edu>

On Wed, 26 Jan 2005, Wang, Yan wrote:

> Hi,
>
>
>
> I like to produce a series of levelplot graphs in postscript file, so I put
> the trunk of codes including postscript() and levelplot() in a for loop. The
> codes work fine outside the loop, but only produce empty .ps file when being
> put within the loop. Is it a problem associated with postscript() or
> levelplot()? How to get around the problem? Many thanks!
>

It's hard to be sure without seeing any code, but it sounds like you 
should look at
FAQ 7.22 Why do lattice/trellis graphics not work?

 	-thomas



From parkhurs at ariel.ucs.indiana.edu  Wed Jan 26 20:07:45 2005
From: parkhurs at ariel.ucs.indiana.edu (David Parkhurst)
Date: Wed, 26 Jan 2005 14:07:45 -0500
Subject: [R] graphsheet substitute in R?
Message-ID: <002a01c503da$5379c410$7c7e4f81@ads.iu.edu>

I'm using R 2.0.1 in a class I teach, with most students working under 
windows XP.  We have a data frame with the first column containing the 
factor "site," and five water-quality variables at each site.  As one part 
of exploring these data, I'd like the students to run
by(ourdata,site,pairs).
When I try that, however, the pairs plots for the first 10 sites scroll 
past, and I'm left to look at the eleventh only.

 I think that if I did this same thing in s-plus with a graph sheet, each of 
the graphs would be saved to a separate page, and then they could be viewed 
one at a time.  Is there any way to do the equivalent in R?  (Using splom 
from lattice makes the graphs too small.)

 Thanks for any help.  (I am not subscribed to the news mailing list, so I 
would appreciate direct replies.)

 David Parkhurst



From ggrothendieck at myway.com  Wed Jan 26 20:15:51 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 26 Jan 2005 19:15:51 +0000 (UTC)
Subject: [R] summarizing daily time-series date by month
References: <1106765216.41f7e5a0a9951@webmail.uvm.edu>
Message-ID: <loom.20050126T200716-823@post.gmane.org>

Benjamin M. Osborne <Benjamin.Osborne <at> uvm.edu> writes:

: 
: Message: 63
: Date: Wed, 26 Jan 2005 04:28:51 +0000 (UTC)
: From: Gabor Grothendieck <ggrothendieck <at> myway.com>
: Subject: Re: [R] chron: parsing dates into a data frame using a
:         forloop
: To: r-help <at> stat.math.ethz.ch
: Message-ID: <loom.20050126T052153-333 <at> post.gmane.org>
: Content-Type: text/plain; charset=us-ascii
: 
: Benjamin M. Osborne <Benjamin.Osborne <at> uvm.edu> writes:
: 
: :
: : I have one data frame with a column of dates and I want to fill another 
data
: : frame with one column of dates, one of years, one of months, one of a 
unique
: : combination of year and month, and one of days, but R seems to have some
: : problems with this.  My initial data frame looks like this (ignore the NAs 
in
: : the other fields):
: :
: : > mans[1:10,]
: :        date loc snow.new prcp tmin snow.dep tmax
: : 1  11/01/54   2       NA   NA   NA       NA   NA
: : 2  11/02/54   2       NA   NA   NA       NA   NA
: : 3  11/03/54   2       NA   NA   NA       NA   NA
: : 4  11/04/54   2       NA   NA   NA       NA   NA
: : 5  11/05/54   2       NA   NA   NA       NA   NA
: : 6  11/06/54   2       NA   NA   NA       NA   NA
: : 7  11/07/54   2       NA   NA   NA       NA   NA
: : 8  11/08/54   2       NA   NA   NA       NA   NA
: : 9  11/09/54   2       NA   NA   NA       NA   NA
: : 10 11/10/54   2       NA   NA   NA       NA   NA
: : >
: :
: : The code and resultant data frame look like this:
: :
: : > for(i in 1:10){
: : + mans.met$date[i]<-mans$date[i]
: : + mans.met$year[i]<-years(mans.met$date[i])
: : + mans.met$month[i]<-months(mans.met$date[i])
: : + mans.met$yearmo[i]<-cut(mans.met$date[i], "months")
: : + mans.met$day[i]<-days(mans.met$date[i])
: : + }
: : > mans.met[1:10,]
: :        date year month yearmo day snow.new snow.dep prcp tmin tmax tmean
: : 1  11/01/54    1    11      1   1       NA       NA   NA   NA   NA    NA
: : 2  11/02/54    1    11      1   2       NA       NA   NA   NA   NA    NA
: : 3  11/03/54    1    11      1   3       NA       NA   NA   NA   NA    NA
: : 4  11/04/54    1    11      1   4       NA       NA   NA   NA   NA    NA
: : 5  11/05/54    1    11      1   5       NA       NA   NA   NA   NA    NA
: : 6  11/06/54    1    11      1   6       NA       NA   NA   NA   NA    NA
: : 7  11/07/54    1    11      1   7       NA       NA   NA   NA   NA    NA
: : 8  11/08/54    1    11      1   8       NA       NA   NA   NA   NA    NA
: : 9  11/09/54    1    11      1   9       NA       NA   NA   NA   NA    NA
: : 10 11/10/54    1    11      1  10       NA       NA   NA   NA   NA    NA
: : >
: :
: : The problem seems to be with assigning within the forloop, or making the
: : assignment into a data frame, since:
: :
: : > years(mans.met$date[5])
: : [1] 1954
: : Levels: 1954
: : > test<-years(mans.met$date[5])
: : > test
: : [1] 1954
: : Levels: 1954
: : >
: : > months(mans.met$date[5])
: : [1] Nov
: : 12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec
: : > test<-months(mans.met$date[5])
: : > test
: : [1] Nov
: : 12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec
: : >
: : > cut(mans.met$date[3], "months")
: : [1] Nov 54
: : Levels: Nov 54
: : > test<-cut(mans.met$date[3], "months")
: : > test
: : [1] Nov 54
: : Levels: Nov 54
: : >
: : > days(mans.met$date[4])
: : [1] 4
: : 31 Levels: 1 < 2 < 3 < 4 < 5 < 6 < 7 < 8 < 9 < 10 < 11 < 12 < 13 < ... < 31
: : > test<-days(mans.met$date[4])
: : > test
: : [1] 4
: : 31 Levels: 1 < 2 < 3 < 4 < 5 < 6 < 7 < 8 < 9 < 10 < 11 < 12 < 13 < ... < 31
: : >
: :
: : Any suggestions will be appreciated.
: : -Ben Osborne
: 
: I guess you set up mans.met as numeric columns and when you
: assign your factors to numeric variables you get
: the underlying codes.  Note that if f is a factor then as.numeric(f)
: gives the codes underlying the factor whereas as.character(f) gives
: the labels.
: 
: It would be better not to use a loop at all.  I don't know whether you
: want or not want factors but at any rate here is something you could
: try.  It creates data frame df2 without a loop.
: 
: df2 <- data.frame(date = mans$date, yearmo = as.character(cut
(mans$date, "m")))
: df2 <- cbind(df2, month.day.year(mans$date))
: 
: Finally, do you really want this redundant representation?  I would tend to
: go with just storing the dates and computing any of the other quantities
: on-the-fly as needed.
: 
: ##########
: The reason for the redundancy is that I will want to summarize these 50 
years of
: daily time series data by month, so that records that share each unique year
: and month in the mans.met$yearmo column will be summed or averaged, etc. 
into a
: new row in another data frame(mans.monthly, having
: nrow=length(unique(mans.met$yearmo))).  The way I would do this is again 
using
: a forloop, but the loop won't recognize :
:      for (i in 1:(length(unique(mans.met$yearmo[i])))){

This seems circular.  You are defining i in terms of i.

: 
: What I really need to know is why I can call any ith of
:      unique(mans.met$yearmo[i])
: by itself, but not in a loop.
: 
: Or, perhaps there is an even easier way to extract the year and month from 
the
: date
: column on the fly to compute these summaries?

Look at ?aggregate, ?by and ?tapply.  e.g.

   aggregate(mans[,-1], list(cut(mans$date, "m")), mean)



From ywang at fhcrc.org  Wed Jan 26 20:23:17 2005
From: ywang at fhcrc.org (Wang, Yan)
Date: Wed, 26 Jan 2005 11:23:17 -0800
Subject: [R] postscript() and levelplot() in a for loop
Message-ID: <9667A0D2033CD51195F90002B330A3BF0CCFB233@MOE.FHCRC.ORG>

Dear Thomas,

It is very interesting to read FAQ 7.22. I don't see any example of
lattice/trellis graphics linked with print() statement. How should I specify
print()? Here are the actual codes I used to create the .ps file:

x=y=myaa
grid=expand.grid(x=x,y=y)
postscript(paste(./Graphs/",file[k],".ps",sep=""))
grid$z=as.vector(mymatrix)
levelplot(z~x*y,grid,at=seq(0,1,by=0.01),scales=list(cex=2),
          colorkey=list(labels=list(cex=2,at=seq(0,1,by=0.2))),
          main="mytitle")
dev.off()

Thank you for any help.

Yan

-----Original Message-----
From: Thomas Lumley [mailto:tlumley at u.washington.edu] 
Sent: Wednesday, January 26, 2005 11:01 AM
To: Wang, Yan
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] postscript() and levelplot() in a for loop

On Wed, 26 Jan 2005, Wang, Yan wrote:

> Hi,
>
>
>
> I like to produce a series of levelplot graphs in postscript file, so I
put
> the trunk of codes including postscript() and levelplot() in a for loop.
The
> codes work fine outside the loop, but only produce empty .ps file when
being
> put within the loop. Is it a problem associated with postscript() or
> levelplot()? How to get around the problem? Many thanks!
>

It's hard to be sure without seeing any code, but it sounds like you 
should look at
FAQ 7.22 Why do lattice/trellis graphics not work?

 	-thomas



From tlumley at u.washington.edu  Wed Jan 26 20:37:29 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 26 Jan 2005 11:37:29 -0800 (PST)
Subject: [R] postscript() and levelplot() in a for loop
In-Reply-To: <9667A0D2033CD51195F90002B330A3BF0CCFB233@MOE.FHCRC.ORG>
References: <9667A0D2033CD51195F90002B330A3BF0CCFB233@MOE.FHCRC.ORG>
Message-ID: <Pine.A41.4.61b.0501261137110.294700@homer09.u.washington.edu>

On Wed, 26 Jan 2005, Wang, Yan wrote:

> Dear Thomas,
>
> It is very interesting to read FAQ 7.22. I don't see any example of
> lattice/trellis graphics linked with print() statement. How should I specify
> print()? Here are the actual codes I used to create the .ps file:
>
> x=y=myaa
> grid=expand.grid(x=x,y=y)
> postscript(paste(./Graphs/",file[k],".ps",sep=""))
> grid$z=as.vector(mymatrix)

print(
> levelplot(z~x*y,grid,at=seq(0,1,by=0.01),scales=list(cex=2),
>          colorkey=list(labels=list(cex=2,at=seq(0,1,by=0.2))),
>          main="mytitle")
)

 	-thomas

> dev.off()
>
> Thank you for any help.
>
> Yan
>
> -----Original Message-----
> From: Thomas Lumley [mailto:tlumley at u.washington.edu]
> Sent: Wednesday, January 26, 2005 11:01 AM
> To: Wang, Yan
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] postscript() and levelplot() in a for loop
>
> On Wed, 26 Jan 2005, Wang, Yan wrote:
>
>> Hi,
>>
>>
>>
>> I like to produce a series of levelplot graphs in postscript file, so I
> put
>> the trunk of codes including postscript() and levelplot() in a for loop.
> The
>> codes work fine outside the loop, but only produce empty .ps file when
> being
>> put within the loop. Is it a problem associated with postscript() or
>> levelplot()? How to get around the problem? Many thanks!
>>
>
> It's hard to be sure without seeing any code, but it sounds like you
> should look at
> FAQ 7.22 Why do lattice/trellis graphics not work?
>
> 	-thomas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From jeaneid at chass.utoronto.ca  Wed Jan 26 20:54:35 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Wed, 26 Jan 2005 14:54:35 -0500
Subject: [R] reshape (a better way)
Message-ID: <Pine.SGI.4.40.0501261448340.7788473-100000@origin.chass.utoronto.ca>


Hi,

I am using the NLSY79 data (longitudinal data from the Bureau of labour
stats in the US). The extractor exctracts this data in a "wide" format and
I need to reshape it into a long format.

What I am doing right now is to do it in chuncks for each and evry
variable that is varying and then I merge the data together. This is
taking a long time. my question is:

How do I specify that there are multiple variables that are varying in
reshape. Is there a way to do this?

The idea is to have something like varying1, varying2,etc,... and each are
assosiated with their own times1, times2.

something like
reshape(mydata, direction="long", varying1=varying1,varying2=varying2,
varying3=varying3, split=list(regexp="[a-z][0-9]", include=TRUE),
idvar="ID", times1=times1, times2=times2, times3=times3)



Jean



From p.murrell at auckland.ac.nz  Wed Jan 26 20:46:09 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Thu, 27 Jan 2005 08:46:09 +1300
Subject: [R] animation without intermediate files?
References: <Pine.LNX.4.44.0501061548270.27024-100000@panic.stat.cmu.edu>	<41F5956F.8010101@stat.auckland.ac.nz>	<16886.2647.973007.550441@stat.math.ethz.ch>
	<16887.32705.419707.350064@stat.math.ethz.ch>
Message-ID: <41F7F381.6090601@stat.auckland.ac.nz>

Hi


Martin Maechler wrote:
>>>>>>"MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>>    on Tue, 25 Jan 2005 09:59:03 +0100 writes:
>>>>>
> 
>>>>>>"Paul" == Paul Murrell <p.murrell at auckland.ac.nz>
>>>>>>    on Tue, 25 Jan 2005 13:40:15 +1300 writes:
>>>>>
> 
>     Paul> Hi
>     Paul> Cari G Kaufman wrote:
>     >>> Hello, 
>     >>> 
>     >>> Does anyone know how to make "movies" in R by making a
>     >>> sequence of plots?  I'd like to animate a long
>     >>> trajectory for exploratory purposes only, without
>     >>> creating a bunch of image files and then using another
>     >>> program to string them together.  In Splus I would do
>     >>> this using double.buffer() to eliminate the flickering
>     >>> caused by replotting. For instance, with a 2-D
>     >>> trajectory in vectors x and y I would use the following:
>     >>> 
>     >>> motif()
>     >>> double.buffer("back")
>     >>> for (i in 1:length(x)) {
>     >>>   plot(x[i], y[i], xlim=range(x), ylim=range(y))
>     >>>   double.buffer("copy")
>     >>> }
>     >>> double.buffer("front")
>     >>> 
>     >>> I haven't found an equivalent function to double.buffer in R.  I tried
>     >>> playing around with dev.set() and dev.copy() but so far with no success
>     >>> (still flickers).
> 
>      Paul> Double buffering is only currently an option on the Windows graphics 
>      Paul> device (and there it is "on" by default).  So something like ...
> 
>      Paul> x <- rnorm(100)
>      Paul> for (i in 1:100)
>      Paul>     plot(1:i, x[1:i], xlim=c(0, 100), ylim=c(-4, 4), pch=16, cex=2)
> 
>      Paul> is already "smooth"
> 
>     MM> well, sorry Paul, but not for my definition of "smooth"!
> 
>     MM> Instead, 
> 
>     MM> n <- 100
>     MM> plot(1,1, xlim=c(0,n), ylim=c(-4,4), type="n")
>     MM> x <- rnorm(n)
>     MM> for (i in 1:n) { points(i, x[i], pch=16, cex=2); Sys.sleep(0.02) }
> 
>     MM> comes much closer to my version of "smooth"  ;-)
> 
> I apologize to Paul, since  what I said seems to be quite
> platform dependent.  Here's my current "knowledge" on the matter:
> 
> o  Paul's  " for(..) plot(..) "
>     - flickers quite a bit for me {on Linux X11 with no
>       particularly fast graphics card}.
>     - seems quite smooth for at least two Windows users who have
>       relatively fast graphics cards.
> 
> o  My solution of
> 	   " for(..) { points(..) ; Sys.sleep(..) } "
>    doesn't redraw the coordinate system and so doesn't "flicker" 
>    (afaik, independently of platform)
> 
>    HOWEVER on windows; the graphics are somehow buffered and
>    points are not drawn one by one, but rather in batches --> "not smooth"


Thanks Martin; I wasn't very clear on my original message.  Double 
buffering has only been implemented on the Windows graphics device at 
this stage (thanks to Brian) and this implementation basically always 
writes to a buffer and updates the screen at fixed time intervals 
(quoting the source: "100ms after last plotting call or 500ms after last 
update") so there is no user control of when the off-screen buffer is 
swapped to the screen.

For "animating" a plot where only new output is added (i.e., no existing 
output is modified or removed), your suggestion should produce the 
smoothest result.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From ccleland at optonline.net  Wed Jan 26 20:58:49 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 26 Jan 2005 14:58:49 -0500
Subject: [R] reshape (a better way)
In-Reply-To: <Pine.SGI.4.40.0501261448340.7788473-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0501261448340.7788473-100000@origin.chass.utoronto.ca>
Message-ID: <41F7F679.5070405@optonline.net>

It's in the documentation for reshape - make the varying argument a 
list.  Here is an example:

famf.uni <- reshape(famf,
  varying = list(c("DUSI2BL",   "DUSI3M",    "DUSI6M",    "DUSI1Y"   ),
                 c("DNKTOTBL",  "DNKTOT3M",  "DNKTOT6M",  "DNKTOT1Y" ),
                 c("DAYDNKBL",  "DAYDNK3M",  "DAYDNK6M",  "DAYDNK1Y" ),
                 c("HVYALCBL",  "HVYALC3M",  "HVYALC6M",  "HVYALC1Y" ),
                 c("BALCSV15",  "DALCSV15",  "EALCSV15",  "FALCSV15" ),
                 c("BALCSV27",  "DALCSV27",  "EALCSV27",  "FALCSV27" ),
                 c("PCSBLT",    "PCS3MT",    "PCS6MT",    "PCS1YT"   ),
                 c("MCSBLT",    "MCS3MT",    "MCS6MT",    "MCS1YT"   ),
                 c("BSIANXBL",  "BSIANX3M",  "BSIANX6M",  "BSIANX1Y" ),
                 c("BSIPSYBL",  "BSIPSY3M",  "BSIPSY6M",  "BSIPSY1Y" ),
                 c("CESDBL",    "CESD3M",    "CESD6M",    "CESD1Y"   )),
  v.names = c("DUSI",   "DNKTOT",  "DAYDNK",
               "HVYALC", "ALCSV15", "ALCSV27", "PCST",
               "MCST",   "BSIANX",  "BSIPSY",
               "CESD"),
  timevar = "TIME", times = c("BL", "3M", "6M", "1Y"), direction = "long")

hope this helps,

Chuck Cleland

Jean Eid wrote:
> I am using the NLSY79 data (longitudinal data from the Bureau of labour
> stats in the US). The extractor exctracts this data in a "wide" format and
> I need to reshape it into a long format.
> 
> What I am doing right now is to do it in chuncks for each and evry
> variable that is varying and then I merge the data together. This is
> taking a long time. my question is:
> 
> How do I specify that there are multiple variables that are varying in
> reshape. Is there a way to do this?
> 
> The idea is to have something like varying1, varying2,etc,... and each are
> assosiated with their own times1, times2.
> 
> something like
> reshape(mydata, direction="long", varying1=varying1,varying2=varying2,
> varying3=varying3, split=list(regexp="[a-z][0-9]", include=TRUE),
> idvar="ID", times1=times1, times2=times2, times3=times3)

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From tlumley at u.washington.edu  Wed Jan 26 21:08:46 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 26 Jan 2005 12:08:46 -0800 (PST)
Subject: [R] reshape (a better way)
In-Reply-To: <Pine.SGI.4.40.0501261448340.7788473-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0501261448340.7788473-100000@origin.chass.utoronto.ca>
Message-ID: <Pine.A41.4.61b.0501261157020.294700@homer09.u.washington.edu>

On Wed, 26 Jan 2005, Jean Eid wrote:

>
> Hi,
>
> I am using the NLSY79 data (longitudinal data from the Bureau of labour
> stats in the US). The extractor exctracts this data in a "wide" format and
> I need to reshape it into a long format.
>
> What I am doing right now is to do it in chuncks for each and evry
> variable that is varying and then I merge the data together. This is
> taking a long time. my question is:
>
> How do I specify that there are multiple variables that are varying in
> reshape. Is there a way to do this?

Yes. The help page says

varying: names of sets of variables in the wide format that correspond
           to single variables in long format ('time-varying').  A list
           of vectors (or optionally a matrix for 'direction="wide"').
           See below for more details and options.

That is, you can use something like

varying=list(c("foo1","foo2","foo3"),
          c("bar1","bar2","bar3"),
          c("baz1","baz2","baz3")),
v.names=c("foo","bar","baz")


 	-thomas



From marie-pierre.sylvestre at mail.mcgill.ca  Wed Jan 26 21:23:43 2005
From: marie-pierre.sylvestre at mail.mcgill.ca (Marie-Pierre Sylvestre)
Date: Wed, 26 Jan 2005 15:23:43 -0500
Subject: [R] Changing axis labels in plots of zph objects (survival analysis)
Message-ID: <006801c503e4$ee949970$1392a8c0@epimgh.mcgill.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050126/9ea3972e/attachment.pl

From br44114 at yahoo.com  Wed Jan 26 21:36:30 2005
From: br44114 at yahoo.com (bogdan romocea)
Date: Wed, 26 Jan 2005 12:36:30 -0800 (PST)
Subject: [R] graphsheet substitute in R?
Message-ID: <20050126203630.16120.qmail@web50310.mail.yahoo.com>

Here are 3 solutions I'm aware of:
1. Export the graphs to PDF:
pdf("file.pdf", height=9, width=12)
[...your code...]
dev.off()
(If you export to PNG you end up with a bunch of files - which is OK if
you have an image browser.)
2. par(ask=T)
3. Sys.sleep(), in case you produce plots in a loop.

HTH,
b.


-----Original Message-----
From: David Parkhurst 
Sent: Wednesday, January 26, 2005 2:08 PM
To: r-help at stat.math.ethz.ch
Subject: [R] graphsheet substitute in R?


I'm using R 2.0.1 in a class I teach, with most students working under 
windows XP.  We have a data frame with the first column containing the 
factor "site," and five water-quality variables at each site.  As one
part 
of exploring these data, I'd like the students to run
by(ourdata,site,pairs).
When I try that, however, the pairs plots for the first 10 sites scroll

past, and I'm left to look at the eleventh only.

 I think that if I did this same thing in s-plus with a graph sheet,
each of 
the graphs would be saved to a separate page, and then they could be
viewed 
one at a time.  Is there any way to do the equivalent in R?  (Using
splom 
from lattice makes the graphs too small.)

 Thanks for any help.  (I am not subscribed to the news mailing list,
so I 
would appreciate direct replies.)

 David Parkhurst

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From br44114 at yahoo.com  Wed Jan 26 21:59:26 2005
From: br44114 at yahoo.com (bogdan romocea)
Date: Wed, 26 Jan 2005 12:59:26 -0800 (PST)
Subject: [R] animation without intermediate files?
Message-ID: <20050126205926.26539.qmail@web50310.mail.yahoo.com>

Here's a different suggestion. Create a bunch of image files, and then
use an image browser (GQview is one of the best; if you're on Win look
at ACDSee) to view them as a slide show. Good image browsers read
images in advance and should not produce flickering. I haven't
experimented though with delays under 5 seconds.

HTH,
b.


-----Original Message-----
From: Paul Murrell
Sent: Wednesday, January 26, 2005 2:46 PM
To: Martin Maechler
Cc: Cari G Kaufman; r-help at stat.math.ethz.ch
Subject: Re: [R] animation without intermediate files?


Hi


Martin Maechler wrote:
>>>>>>"MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>>    on Tue, 25 Jan 2005 09:59:03 +0100 writes:
>>>>>
> 
>>>>>>"Paul" == Paul Murrell <p.murrell at auckland.ac.nz>
>>>>>>    on Tue, 25 Jan 2005 13:40:15 +1300 writes:
>>>>>
> 
>     Paul> Hi
>     Paul> Cari G Kaufman wrote:
>     >>> Hello, 
>     >>> 
>     >>> Does anyone know how to make "movies" in R by making a
>     >>> sequence of plots?  I'd like to animate a long
>     >>> trajectory for exploratory purposes only, without
>     >>> creating a bunch of image files and then using another
>     >>> program to string them together.  In Splus I would do
>     >>> this using double.buffer() to eliminate the flickering
>     >>> caused by replotting. For instance, with a 2-D
>     >>> trajectory in vectors x and y I would use the following:
>     >>> 
>     >>> motif()
>     >>> double.buffer("back")
>     >>> for (i in 1:length(x)) {
>     >>>   plot(x[i], y[i], xlim=range(x), ylim=range(y))
>     >>>   double.buffer("copy")
>     >>> }
>     >>> double.buffer("front")
>     >>> 
>     >>> I haven't found an equivalent function to double.buffer in R.
 I tried
>     >>> playing around with dev.set() and dev.copy() but so far with
no success
>     >>> (still flickers).
> 
>      Paul> Double buffering is only currently an option on the
Windows graphics 
>      Paul> device (and there it is "on" by default).  So something
like ...
> 
>      Paul> x <- rnorm(100)
>      Paul> for (i in 1:100)
>      Paul>     plot(1:i, x[1:i], xlim=c(0, 100), ylim=c(-4, 4),
pch=16, cex=2)
> 
>      Paul> is already "smooth"
> 
>     MM> well, sorry Paul, but not for my definition of "smooth"!
> 
>     MM> Instead, 
> 
>     MM> n <- 100
>     MM> plot(1,1, xlim=c(0,n), ylim=c(-4,4), type="n")
>     MM> x <- rnorm(n)
>     MM> for (i in 1:n) { points(i, x[i], pch=16, cex=2);
Sys.sleep(0.02) }
> 
>     MM> comes much closer to my version of "smooth"  ;-)
> 
> I apologize to Paul, since  what I said seems to be quite
> platform dependent.  Here's my current "knowledge" on the matter:
> 
> o  Paul's  " for(..) plot(..) "
>     - flickers quite a bit for me {on Linux X11 with no
>       particularly fast graphics card}.
>     - seems quite smooth for at least two Windows users who have
>       relatively fast graphics cards.
> 
> o  My solution of
> 	   " for(..) { points(..) ; Sys.sleep(..) } "
>    doesn't redraw the coordinate system and so doesn't "flicker" 
>    (afaik, independently of platform)
> 
>    HOWEVER on windows; the graphics are somehow buffered and
>    points are not drawn one by one, but rather in batches --> "not
smooth"


Thanks Martin; I wasn't very clear on my original message.  Double 
buffering has only been implemented on the Windows graphics device at 
this stage (thanks to Brian) and this implementation basically always 
writes to a buffer and updates the screen at fixed time intervals 
(quoting the source: "100ms after last plotting call or 500ms after
last 
update") so there is no user control of when the off-screen buffer is 
swapped to the screen.

For "animating" a plot where only new output is added (i.e., no
existing 
output is modified or removed), your suggestion should produce the 
smoothest result.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Wed Jan 26 22:01:53 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 26 Jan 2005 16:01:53 -0500
Subject: [R] graphsheet substitute in R?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5C1@usrymx25.merck.com>

You can turn on plot history from the menu in the windows() device.  Then
whatever plots you make, you can page-up/page-down to navigate through them.
(This is specific to Windows.)

Andy

> From: David Parkhurst
> 
> I'm using R 2.0.1 in a class I teach, with most students 
> working under 
> windows XP.  We have a data frame with the first column 
> containing the 
> factor "site," and five water-quality variables at each site. 
>  As one part 
> of exploring these data, I'd like the students to run
> by(ourdata,site,pairs).
> When I try that, however, the pairs plots for the first 10 
> sites scroll 
> past, and I'm left to look at the eleventh only.
> 
>  I think that if I did this same thing in s-plus with a graph 
> sheet, each of 
> the graphs would be saved to a separate page, and then they 
> could be viewed 
> one at a time.  Is there any way to do the equivalent in R?  
> (Using splom 
> from lattice makes the graphs too small.)
> 
>  Thanks for any help.  (I am not subscribed to the news 
> mailing list, so I 
> would appreciate direct replies.)
> 
>  David Parkhurst
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jeaneid at chass.utoronto.ca  Wed Jan 26 22:17:52 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Wed, 26 Jan 2005 16:17:52 -0500
Subject: [R] reshape (a better way)
In-Reply-To: <Pine.A41.4.61b.0501261157020.294700@homer09.u.washington.edu>
Message-ID: <Pine.SGI.4.40.0501261606200.7781783-100000@origin.chass.utoronto.ca>

thank you Thomas and Chuck for the helpfull codes. My problem was with the
times variable adn chuck's example made that really clear. I just have one
more question regarding reshape. when varying are not the same length
reshape complains, so I generated the name of the variables that are
missing and it complained about "undefined columns selected". So the way
around it that I did is that i generated an NA vector in the wide format
for every one of these variables that are missing and it did it
successfully.

Is there a way to do this without generating these vectors. e.g. Here's an
example

XX <- data.frame(one1=rnorm(10), one2=rnorm(10), one3=rnorm(10),
two1=rnorm(10), two2=rnorm(10))

 reshape(XX, direction="long", varying=list(c("one1", "one2", "one3"),
c("two1", "two2")), v.names=c("one", "two"), times=c("1", "2", "3"))

## gives Error in reshapeLong(data, idvar = idvar, timevar = timevar,
## varying = varying,  :
##	'varying' arguments must be the same length

 reshape(XX, direction="long", varying=list(c("one1", "one2", "one3"),
c("two1", "two2", "two3")), v.names=c("one", "two"), times=c("1", "2", "3"))


## Error in "[.data.frame"(data, , varying[[j]][i]) :
##	undefined columns selected


#and finally
XX$two3<-NA
reshape(XX, direction="long", varying=list(c("one1", "one2", "one3"),
c("two1", "two2", "two3")), v.names=c("one", "two"), times=c("1", "2",
"3"))


# I get the correct output.


I there a way not to generate the XX$two3 above



Thank you


Jean







On Wed, 26 Jan 2005, Thomas Lumley wrote:

> On Wed, 26 Jan 2005, Jean Eid wrote:
>
> >
> > Hi,
> >
> > I am using the NLSY79 data (longitudinal data from the Bureau of labour
> > stats in the US). The extractor exctracts this data in a "wide" format and
> > I need to reshape it into a long format.
> >
> > What I am doing right now is to do it in chuncks for each and evry
> > variable that is varying and then I merge the data together. This is
> > taking a long time. my question is:
> >
> > How do I specify that there are multiple variables that are varying in
> > reshape. Is there a way to do this?
>
> Yes. The help page says
>
> varying: names of sets of variables in the wide format that correspond
>            to single variables in long format ('time-varying').  A list
>            of vectors (or optionally a matrix for 'direction="wide"').
>            See below for more details and options.
>
> That is, you can use something like
>
> varying=list(c("foo1","foo2","foo3"),
>           c("bar1","bar2","bar3"),
>           c("baz1","baz2","baz3")),
> v.names=c("foo","bar","baz")
>
>
>  	-thomas
>
>



From wgshi2001 at yahoo.ca  Wed Jan 26 22:24:13 2005
From: wgshi2001 at yahoo.ca (Weiguang Shi)
Date: Wed, 26 Jan 2005 16:24:13 -0500 (EST)
Subject: [R] Zipf random number generation
In-Reply-To: <XFMail.050126104608.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20050126212413.28025.qmail@web30003.mail.mud.yahoo.com>

Thank you very much again. Your code is really helpful
and it demontrates the power of R to a new-comer like
me.

 --- Ted.Harding at nessie.mcc.ac.uk wrote: 
> Understood! Though I think you mean
> 
>   1/c = sum(1.0 / pow((double) j, alpha)),
> j=1,2,...,N

You are right.

Weiguang



From tfliao at uiuc.edu  Wed Jan 26 22:39:04 2005
From: tfliao at uiuc.edu (Tim F Liao)
Date: Wed, 26 Jan 2005 15:39:04 -0600
Subject: [R] Linear Trend Analysis?
Message-ID: <40cfc3e0.c4e56593.840a000@expms6.cites.uiuc.edu>

Chris,

While there might be certain functions in R that I'm not aware
of for such purposes, anything (well, almost) can be done in R
if you're willing to write a line or two of code, and that's
the beauty of R.  In a course I recently taught, I used the
following code to generate individual data from grouped data,
which would give the same results as using fweight=count in Stata.

Ind.Data<-data.frame(cbind(rep(Y,freq),rep(X1,freq),rep(X2,freq)))

where Y is the dependent variables and X1 and X2 are two
explanatory variables and freq is your count variable.

Hope this helps,

Tim F Liao
Professor of Sociology & Statistics
University of Illinois
Urbana, IL 61801

---- Original message ----
>Date: Wed, 26 Jan 2005 12:54:11 -0600
>From: "Chris Bergstresser" <chris at subtlety.com>  
>Subject: [R] Linear Trend Analysis?  
>To: <R-help at stat.math.ethz.ch>
>
>Hi all --
>
>   I'm trying to use R for my "Analysis of Categorical Data"
class, but I
>can't figure out how to do a weighted linear trend analysis.
 I have a table
>of categorical data, to which I've assigned weights to the
rows and columns.
>I need to calculate r and M^2, which is apparently done in
SAS using "PROC
>FREQ" and in STATA using "correlate var1 var2 fw=count". 
What's the command
>for R?
>
>-- Chris
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From nw.galwey at ukonline.co.uk  Wed Jan 26 22:45:08 2005
From: nw.galwey at ukonline.co.uk (Nicholas Galwey)
Date: Wed, 26 Jan 2005 21:45:08 -0000
Subject: [R] Specification of factorial random-effects model
Message-ID: <E1CtuyM-0003h9-00@smarthost4.mail.uk.easynet.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050126/ec220e48/attachment.pl

From gunter.berton at gene.com  Wed Jan 26 23:43:10 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 26 Jan 2005 14:43:10 -0800
Subject: [R] Specification of factorial random-effects model
In-Reply-To: <E1CtuyM-0003h9-00@smarthost4.mail.uk.easynet.net>
Message-ID: <200501262243.j0QMhFKI006967@volta.gene.com>

If you read the Help file for lme (!), you'll see that ~1|a*b is certainly
incorrect.

Briefly, the issue has been discussed before on this list: the current
version of lme() follows the original Laird/Ware formulation for **nested**
random effects. Specifying **crossed** random effects is possible but
difficult, and the fitting algorithm is not optimized for this. See p. 163
in Bates and Pinheiro for an example.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nicholas Galwey
> Sent: Wednesday, January 26, 2005 1:45 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Specification of factorial random-effects model
> 
> I want to specify two factors and their interaction as random 
> effects using
> the function lme().   This works okay when I specify these 
> terms using the
> function Error() within the function aov(), but I can't get 
> the same model
> fitted using lme().   The code below illustrates the problem.
> 
>  
> 
> a <- factor(rep(c(1:3), each = 27))
> 
> b <- factor(rep(rep(c(1:3), each = 9), times = 3))
> 
> c <- factor(rep(rep(c(1:3), each = 3), times = 9))
> 
> y <- c(74.59,75.63,76.7,63.48,63.17,65.99,64,66.35,64.5,
> 
>    46.57,44.16,47.96,35.09,36.14,35.16,36.4,34.72,34.58,
> 
>    41.82,47.35,45.74,33.33,36.8,33.38,34.13,34.39,34.48,
> 
>    89.73,85.24,90.86,82.5,79.44,81.65,77.74,77.02,81.62,
> 
>    59.32,62.29,60.7,55.42,55.5,51.17,50.54,53.54,51.85,
> 
>    64.5,63.6,65.19,55.07,50.26,53.73,54.57,47.8,48.8,91.56,
> 
>    94.49,92.17,82.14,83.16,81.31,83.58,78.63,77.08,60.53,
> 
>    60.79,58.57,51.28,52.9,51.54,49.15,48.97,51.61,59.44,
> 
>    60.07,60.07,51.94,52.2,50.2,49.45,50.75,49.56)
> 
> anovamodel <- aov(y ~ c + Error(a*b))
> 
> summary(anovamodel)
> 
> lmemodel <- lme(y ~ c, random = ~ 1|a*b)
> 
> anova(lmemodel)
> 
>  
> 
> N.B. You have to load the package 'nlme' before lme() will run at all.
> When it does run, it gives the error
> 
>  
> 
> Error in getGroups.data.frame(dataMix, groups) : 
> 
>         Invalid formula for groups
> 
>  
> 
> I think there is something I haven't understood about the 
> syntax of the
> argument 'random' in lme() .   Can anyone explain it to me?
> 
>  
> 
> Nick Galwey
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From m_osm at gmx.net  Thu Jan 27 00:25:16 2005
From: m_osm at gmx.net (Mahdi Osman)
Date: Thu, 27 Jan 2005 00:25:16 +0100 (MET)
Subject: [R] sw
Message-ID: <32727.1106781916@www58.gmx.net>

Hi list,

I am just a new user of R.

How can I run stepwise regression in R?
Is there a graphic user interphase for any of the spatial packages inculded
in R, such as gstat, geoR and someothers. I am mainly interested interactive
variogram modelling and mapping.

Thanks
Mahdi

-- 
-----------------------------------
Mahdi Osman (PhD)
E-mail: m_osm at gmx.net
-----------------------------------

10 GB Mailbox, 100 FreeSMS http://www.gmx.net/de/go/topmail



From wgshi2001 at yahoo.ca  Thu Jan 27 00:28:19 2005
From: wgshi2001 at yahoo.ca (Weiguang Shi)
Date: Wed, 26 Jan 2005 18:28:19 -0500 (EST)
Subject: [R] agglomerative coefficient in agnes (cluster)
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E5BB@usrymx25.merck.com>
Message-ID: <20050126232819.8917.qmail@web30005.mail.mud.yahoo.com>

Thanks again Andy.

The definition of AC is understood, yet I have trouble
picturing the amount of "clear clustering structure"
it measures. To put things into perspective, for two
series 
   1,2,1000,1001
and 
   1,2,3,1000
agnes(x, method="single") generates ac values of 
0.998998 and 0.0.7492477 respectively, yet it seems to
me that both have fairly clear clustering structures.

 --- "Liaw, Andy" <andy_liaw at merck.com> wrote: 
> BTW, I checked the book.  You're not going find much
> more than that.
> 
Thanks for checking.

Weiguang



From chris at subtlety.com  Thu Jan 27 02:08:34 2005
From: chris at subtlety.com (Chris Bergstresser)
Date: Wed, 26 Jan 2005 19:08:34 -0600
Subject: [R] Linear Trend Analysis?
In-Reply-To: <40cfc3e0.c4e56593.840a000@expms6.cites.uiuc.edu>
Message-ID: <200501270108.j0R18wDQ031062@hypatia.math.ethz.ch>

> -----Original Message-----
> From: Tim F Liao [mailto:tfliao at uiuc.edu]
>
>
> In a course I recently taught, I used the
> following code to generate individual data from grouped data,
> which would give the same results as using fweight=count in Stata.
> 
> Ind.Data<-data.frame(cbind(rep(Y,freq),rep(X1,freq),rep(X2,freq)))
> 
> where Y is the dependent variables and X1 and X2 are two
> explanatory variables and freq is your count variable.

   That had occurred to me, but some of the frequencies are around 17,000,
so it seemed there had to be a more elegant way.

-- Chris



From andy_liaw at merck.com  Thu Jan 27 02:42:51 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 26 Jan 2005 20:42:51 -0500
Subject: [R] how to evaluate the significance of attributes in tree
	gr owing
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5C6@usrymx25.merck.com>

FWIW, I wrote a little function to extract variable importance as defined in
the CART book a while ago.  It's rather limited:  Only works for regression
problem, and you need to set maxsurrogate=0 and maxcompete=0.  It may (or
may not) help you:

varimp.rpart <- function(x) {
    dev <- x$frame[, c("var", "dev")]
    dev <- dev[dev$var != "<leaf>", ]
    improve <- x$split[, "improve"]
    imp <- tapply(dev[, 2] * improve, dev$var, sum)[-1]
    if (any(is.na(imp))) 
        imp[is.na(imp)] <- 0
    imp
}

Here's an example using the Boston housing data:

> library(rpart)
> data(Boston, package="MASS")
> boston.rp <- rpart(medv ~ ., Boston, control=rpart.control(maxsurrogate=0,
maxcompete=0))
> varimp.rpart(boston.rp)
     crim        zn     indus      chas       nox        rm       age
dis 
 1136.809     0.000     0.000     0.000     0.000 23825.922     0.000
1544.804 
      rad       tax   ptratio     black     lstat 
    0.000     0.000     0.000     0.000  7988.955 

Both gbm and randomForest has analogous measures.

Andy


> From: WeiWei Shi
> 
> Hi, there:
> 
> I am wondering if there is a package in R (doing decison trees) which
> can provide some methods to evaluate the significance of attributes. I
> remembered randomForest gives some output like that. Unfortunately my
> current computing env. cannot handle my datasets if I use
> randomForest. So, I am thinking if other packages can do this job or
> not.
> 
> 
> Thanks,
> 
> Ed
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From arthur_edward_andersen at yahoo.com  Thu Jan 27 03:05:10 2005
From: arthur_edward_andersen at yahoo.com (Arthur Andersen)
Date: Wed, 26 Jan 2005 18:05:10 -0800 (PST)
Subject: [R] (no subject)
Message-ID: <20050127020510.54770.qmail@web51103.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050126/2b5b125b/attachment.pl

From msck9 at mizzou.edu  Thu Jan 27 03:38:29 2005
From: msck9 at mizzou.edu (msck9@mizzou.edu)
Date: Wed, 26 Jan 2005 20:38:29 -0600
Subject: [R] Cluster analysis using EM algorithm
Message-ID: <20050127023829.GA23735@localhost>

Hi, 
 I am looking for a package to do the clustering analysis using the
 expectation maximization algorithm. 

 Thanks in advance.

 Ming



From vrobles at fi.upm.es  Thu Jan 27 04:09:42 2005
From: vrobles at fi.upm.es (Victor Robles)
Date: Wed, 26 Jan 2005 22:09:42 -0500
Subject: [R] Relative importance of inputs in nnet
Message-ID: <000801c5041d$aff1a8a0$1035ae86@kakuru>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050126/911ed739/attachment.pl

From andy_liaw at merck.com  Thu Jan 27 04:43:52 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 26 Jan 2005 22:43:52 -0500
Subject: [R] agglomerative coefficient in agnes (cluster)
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5C7@usrymx25.merck.com>



> -----Original Message-----
> From: Weiguang Shi
> 
> Thanks again Andy.
> 
> The definition of AC is understood, yet I have trouble
> picturing the amount of "clear clustering structure"
> it measures. To put things into perspective, for two
> series 
>    1,2,1000,1001
> and 
>    1,2,3,1000
> agnes(x, method="single") generates ac values of 
> 0.998998 and 0.0.7492477 respectively, yet it seems to
> me that both have fairly clear clustering structures.

It has to do with sample sizes.  Consider the following:

testAC <- function(prop1=0.5, x=rnorm(50), center=c(0, 100), ...) {
    stopifnot(require(cluster))
    n <- length(x)
    n1 <- ceiling(n * prop1)
    n2 <- n - n1
    agnes(x + rep(center, c(n1, n2)), ...)$ac
}

Now some tests:

> sapply(c(.25, .5), testAC, x=x[1:4], method="single")
[1] 0.7427591 0.9862944
> sapply(1:5 / 10, testAC, x=x[1:10], method="single")
[1] 0.8977139 0.9974224 0.9950061 0.9946366 0.9946366
> sapply(1:5 / 10, testAC, x=x, method="single")
[1] 0.9982955 0.9969757 0.9971114 0.9971127 0.9975111

So it seems like AC does not consider isolated singletons as cluster
structures.  This is only discernable in small sample size, though.

Andy


 
>  --- "Liaw, Andy" <andy_liaw at merck.com> wrote: 
> > BTW, I checked the book.  You're not going find much
> > more than that.
> > 
> Thanks for checking.
> 
> Weiguang
> 
> ______________________________________________________________
> ________ 
> Post your free ad now! http://personals.yahoo.ca
> 
>



From Tom.Mulholland at dpi.wa.gov.au  Thu Jan 27 04:48:47 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 27 Jan 2005 11:48:47 +0800
Subject: [R] agglomerative coefficient in agnes (cluster)
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA74@afhex01.dpi.wa.gov.au>

Well I am not sure that can call a single figure a cluster. Sure it's not near the others but how can you conceptually measure it's cluster properties. It seems reasonable that there has to be some form of doubt about it.

Back to that Google search hit number 3 www.stat.ncu.edu.tw/teacher/ hungy/mva/notes/lecture-cluster-example.pdf  gives examples which are not close to 1. 

It is said that "The quality of an agglomerative clustering of the data can be measured by the agglomerative coefficient" this is ascribed to Kaufman L. and Rousseeuw P. (1990), "Finding Groups in Data, an Introduction to Cluster Analysis", Wiley, New York. After I had read some of the recent work on clustering I realised that clustering is as much art as it is anything else. There is a wealth of papers with arguments about which methods should be used to assess the effectiveness of the clustering process. I don't think it matters which type of evaluation method you use they are not absolute numbers, they need to be seen as relative. They also need to be seen as an attempt at modelling a method of quality assessment for which there is no clear winner. So the bottom line is that if for your purposes a single number on it's own should be classified as a group, you may well have to define your own method of evaluation.

Tom

> -----Original Message-----
> From: Weiguang Shi [mailto:wgshi2001 at yahoo.ca]
> Sent: Thursday, 27 January 2005 7:28 AM
> To: Liaw, Andy
> Cc: rhelp
> Subject: RE: [R] agglomerative coefficient in agnes (cluster)
> 
> 
> Thanks again Andy.
> 
> The definition of AC is understood, yet I have trouble
> picturing the amount of "clear clustering structure"
> it measures. To put things into perspective, for two
> series 
>    1,2,1000,1001
> and 
>    1,2,3,1000
> agnes(x, method="single") generates ac values of 
> 0.998998 and 0.0.7492477 respectively, yet it seems to
> me that both have fairly clear clustering structures.
> 
>  --- "Liaw, Andy" <andy_liaw at merck.com> wrote: 
> > BTW, I checked the book.  You're not going find much
> > more than that.
> > 
> Thanks for checking.
> 
> Weiguang
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From adrian at maths.uwa.edu.au  Thu Jan 27 05:03:47 2005
From: adrian at maths.uwa.edu.au (Adrian Baddeley)
Date: Thu, 27 Jan 2005 12:03:47 +0800
Subject: [R] getting package version inside .First.lib
Message-ID: <16888.26659.789957.825999@maths.uwa.edu.au>

Greetings - 

Is it possible, inside .First.lib,
to find out the version number of the package that is being loaded?

If only one version of the package has been installed,
we could scan the DESCRIPTION file, something like

.First.lib <- function(lib, pkg) {
    library.dynam("spatstat", pkg, lib)
    dfile <- system.file("DESCRIPTION", package="spatstat")
    ttt <- scan(dfile, what="", sep="^M", quiet=TRUE)[2]
    vvv <- strsplit(ttt," ")[[1]][2]
    cat("spatstat version number",vvv,"\n")
}

but even this does not seem very safe (it makes assumptions about the
format of the DESCRIPTION file).

Is there a better way?

thanks
Adrian Baddeley



From adrian at maths.uwa.edu.au  Thu Jan 27 06:02:26 2005
From: adrian at maths.uwa.edu.au (Adrian Baddeley)
Date: Thu, 27 Jan 2005 13:02:26 +0800
Subject: [R] getting package version inside .First.lib
Message-ID: <16888.30178.78542.527213@maths.uwa.edu.au>


    > Is it possible, inside .First.lib, to find out the
    > version number of the package that is being loaded?

Berwin Turlach kindly informs me that installed.packages() will extract 
details of all installed packages.

So if only one version of package "spatstat" has been installed,
we can determine the version number by
      installed.packages()["spatstat", "Version"]

But: if several versions of the same package are installed,
the question remains how to determine which version is being loaded...

A



From msck9 at mizzou.edu  Thu Jan 27 06:09:51 2005
From: msck9 at mizzou.edu (msck9@mizzou.edu)
Date: Wed, 26 Jan 2005 23:09:51 -0600
Subject: [R] A "rude" question
Message-ID: <20050127050951.GA26565@localhost>

Dear all, 
 I am beginner using R. I have a question about it. When you use it,
 since it is written by so many authors, how do you know that the
 results are trustable?(I don't want to affend anyone, also I trust
 people). But I think this should be a question.

 Thanks,
 Ming



From renaud.lancelot at cirad.fr  Thu Jan 27 06:29:54 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Thu, 27 Jan 2005 08:29:54 +0300
Subject: [R] Specification of factorial random-effects model
In-Reply-To: <200501262243.j0QMhFKI006967@volta.gene.com>
References: <200501262243.j0QMhFKI006967@volta.gene.com>
Message-ID: <41F87C52.10100@cirad.fr>

Berton Gunter a ?crit :
> If you read the Help file for lme (!), you'll see that ~1|a*b is certainly
> incorrect.
> 
> Briefly, the issue has been discussed before on this list: the current
> version of lme() follows the original Laird/Ware formulation for **nested**
> random effects. Specifying **crossed** random effects is possible but
> difficult, and the fitting algorithm is not optimized for this. See p. 163
> in Bates and Pinheiro for an example.
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
>  
>  
[snip]

You can use the package lme4 to fit models with crossed random effects. 
However, it looks like you have to explicitly create the interaction term:

 > library(lme4)
 >
 > a <- factor(rep(c(1:3), each = 27))
 > b <- factor(rep(rep(c(1:3), each = 9), times = 3))
 > c <- factor(rep(rep(c(1:3), each = 3), times = 9))
 > y <- c(74.59,75.63,76.7,63.48,63.17,65.99,64,66.35,64.5,
+    46.57,44.16,47.96,35.09,36.14,35.16,36.4,34.72,34.58,
+    41.82,47.35,45.74,33.33,36.8,33.38,34.13,34.39,34.48,
+    89.73,85.24,90.86,82.5,79.44,81.65,77.74,77.02,81.62,
+    59.32,62.29,60.7,55.42,55.5,51.17,50.54,53.54,51.85,
+    64.5,63.6,65.19,55.07,50.26,53.73,54.57,47.8,48.8,91.56,
+    94.49,92.17,82.14,83.16,81.31,83.58,78.63,77.08,60.53,
+    60.79,58.57,51.28,52.9,51.54,49.15,48.97,51.61,59.44,
+    60.07,60.07,51.94,52.2,50.2,49.45,50.75,49.56)
 > Data <- data.frame(a=a, b=b, ab=paste(a, b, sep = ""), c=c, y=y)
 > rm(a, b, c, y)
 > fm1 <- lme(y ~ c, random = ~ 1 | a * b, data = Data)
 > fm1
Linear mixed-effects model
Fixed: y ~ c
  Data: Data
  log-restricted-likelihood:  -183.6605
Random effects:
  Groups   Name        Variance Std.Dev.
  b        (Intercept) 286.5391 16.9275
  a        (Intercept)  86.3823  9.2942
  Residual               4.0039  2.0010
# of obs: 81, groups: b, 3; a, 3

Fixed effects:
             Estimate Std. Error DF  t value  Pr(>|t|)
(Intercept)  65.9126    11.1560 78   5.9083  8.58e-08
c2           -9.4700     0.5446 78 -17.3890 < 2.2e-16
c3          -10.8826     0.5446 78 -19.9829 < 2.2e-16
 >
 > fm2 <- lme(y ~ c, random = ~ 1 | a + b + ab, data = Data)
 > fm2
Linear mixed-effects model
Fixed: y ~ c
  Data: Data
  log-restricted-likelihood:  -181.0074
Random effects:
  Groups   Name        Variance Std.Dev.
  ab       (Intercept)   1.1118  1.0544
  b        (Intercept) 286.8433 16.9364
  a        (Intercept)  86.2138  9.2851
  Residual               3.4626  1.8608
# of obs: 81, groups: ab, 9; b, 3; a, 3

Fixed effects:
              Estimate Std. Error DF  t value  Pr(>|t|)
(Intercept)  65.91259   11.16262 78   5.9048 8.707e-08
c2           -9.47000    0.50645 78 -18.6989 < 2.2e-16
c3          -10.88259    0.50645 78 -21.4881 < 2.2e-16

#######

Beware: if you loaded nlme before, you have to start a new session to 
use lme4 which conflicts with nlme.

Best,

Renaud


-- 
Dr Renaud Lancelot, v?t?rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From renaud.lancelot at cirad.fr  Thu Jan 27 06:31:40 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Thu, 27 Jan 2005 08:31:40 +0300
Subject: [R] Specification of factorial random-effects model
In-Reply-To: <200501262243.j0QMhFKI006967@volta.gene.com>
References: <200501262243.j0QMhFKI006967@volta.gene.com>
Message-ID: <41F87CBC.7070203@cirad.fr>

Berton Gunter a ?crit :
> If you read the Help file for lme (!), you'll see that ~1|a*b is certainly
> incorrect.
> 
> Briefly, the issue has been discussed before on this list: the current
> version of lme() follows the original Laird/Ware formulation for **nested**
> random effects. Specifying **crossed** random effects is possible but
> difficult, and the fitting algorithm is not optimized for this. See p. 163
> in Bates and Pinheiro for an example.
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
>  
>  
[snip]

You can use the package lme4 to fit models with crossed random effects. 
However, it looks like you have to explicitly create the interaction term:

 > library(lme4)
Loading required package: Matrix
Loading required package: latticeExtra
 >
 > a <- factor(rep(c(1:3), each = 27))
 > b <- factor(rep(rep(c(1:3), each = 9), times = 3))
 > c <- factor(rep(rep(c(1:3), each = 3), times = 9))
 > y <- c(74.59,75.63,76.7,63.48,63.17,65.99,64,66.35,64.5,
+    46.57,44.16,47.96,35.09,36.14,35.16,36.4,34.72,34.58,
+    41.82,47.35,45.74,33.33,36.8,33.38,34.13,34.39,34.48,
+    89.73,85.24,90.86,82.5,79.44,81.65,77.74,77.02,81.62,
+    59.32,62.29,60.7,55.42,55.5,51.17,50.54,53.54,51.85,
+    64.5,63.6,65.19,55.07,50.26,53.73,54.57,47.8,48.8,91.56,
+    94.49,92.17,82.14,83.16,81.31,83.58,78.63,77.08,60.53,
+    60.79,58.57,51.28,52.9,51.54,49.15,48.97,51.61,59.44,
+    60.07,60.07,51.94,52.2,50.2,49.45,50.75,49.56)
 > Data <- data.frame(a=a, b=b, ab=paste(a, b, sep = ""), c=c, y=y)
 > rm(a, b, c, y)
 > fm1 <- lme(y ~ c, random = ~ 1 | a * b, data = Data)
 > fm1
Linear mixed-effects model
Fixed: y ~ c
  Data: Data
  log-restricted-likelihood:  -183.6605
Random effects:
  Groups   Name        Variance Std.Dev.
  b        (Intercept) 286.5391 16.9275
  a        (Intercept)  86.3823  9.2942
  Residual               4.0039  2.0010
# of obs: 81, groups: b, 3; a, 3

Fixed effects:
             Estimate Std. Error DF  t value  Pr(>|t|)
(Intercept)  65.9126    11.1560 78   5.9083  8.58e-08
c2           -9.4700     0.5446 78 -17.3890 < 2.2e-16
c3          -10.8826     0.5446 78 -19.9829 < 2.2e-16
 >
 > fm2 <- lme(y ~ c, random = ~ 1 | a + b + ab, data = Data)
 > fm2
Linear mixed-effects model
Fixed: y ~ c
  Data: Data
  log-restricted-likelihood:  -181.0074
Random effects:
  Groups   Name        Variance Std.Dev.
  ab       (Intercept)   1.1118  1.0544
  b        (Intercept) 286.8433 16.9364
  a        (Intercept)  86.2138  9.2851
  Residual               3.4626  1.8608
# of obs: 81, groups: ab, 9; b, 3; a, 3

Fixed effects:
              Estimate Std. Error DF  t value  Pr(>|t|)
(Intercept)  65.91259   11.16262 78   5.9048 8.707e-08
c2           -9.47000    0.50645 78 -18.6989 < 2.2e-16
c3          -10.88259    0.50645 78 -21.4881 < 2.2e-16

#######

Beware: if you loaded nlme before, you have to start a new session to 
use lme4 which conflicts with nlme.

Best,

Renaud


-- 
Dr Renaud Lancelot, v?t?rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From blindglobe at gmail.com  Thu Jan 27 06:32:13 2005
From: blindglobe at gmail.com (A.J. Rossini)
Date: Thu, 27 Jan 2005 06:32:13 +0100
Subject: [R] A "rude" question
In-Reply-To: <20050127050951.GA26565@localhost>
References: <20050127050951.GA26565@localhost>
Message-ID: <1abe3fa905012621325e7a222@mail.gmail.com>

How do you know that any results from any software package are
trustable?  I'm not sure that the number of authors has anything to do
with it.

If you are extremely paranoid, you can reprogram everything you do a
few times in a large number of completely different languages written
by different people, and top it off with hand calculations.   Then you
should do this across 4-5 operating systems with different core
libraries.

I'm somewhat joking in the second paragraph, but very serious in the
first.  How and why do YOU trust software?  What criteria fit?

Perhaps a better question would be to ask by what criteria people use
to "trust" software, using R as an illustration.

best,
-tony

p.s. R does satisfy a good part of the second paragraph, at least for
a critical subset of the language.

On Wed, 26 Jan 2005 23:09:51 -0600, msck9 at mizzou.edu <msck9 at mizzou.edu> wrote:
> Dear all,
>  I am beginner using R. I have a question about it. When you use it,
>  since it is written by so many authors, how do you know that the
>  results are trustable?(I don't want to affend anyone, also I trust
>  people). But I think this should be a question.
> 
>  Thanks,
>  Ming
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
best,
-tony

"Commit early,commit often, and commit in a repository from which we can easily
roll-back your mistakes" (AJR, 4Jan05).

A.J. Rossini
blindglobe at gmail.com



From Tom.Mulholland at dpi.wa.gov.au  Thu Jan 27 07:15:31 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 27 Jan 2005 14:15:31 +0800
Subject: [R] A "rude" question
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA76@afhex01.dpi.wa.gov.au>

What makes you trust any software? 

There are some obvious points. First of all the code is open so if you know enough you can actually read the code and make sure it does what you want. Secondly you can replicate a process using two pieces of software and compare the results. You can check the archives and you will find a number of posts that talk about the results produced by R and how they compare with other software. Typically R versus Excel or R versus SPSS / SAS. Just be careful as different answers does not automatically mean one is wrong, and it certainly doesn't mean R is wrong.

Excel computes =ROUND(2.5,0) to be 3
R computes round(2.5) to be 2

As I understand it both are right, they are just using different standards. I however have always used the latter and rounded to the even number where the figure to be rounded lies exactly at the halfway mark.

Hang around this list for a short time and it will become evident that if this software didn't work; the people using it would have stopped using it long ago.

Forget the commercial versus open software arguments that raise their head from time to time. The question is how well a piece of software is written / maintained & supported and not issues of payment or the greater good. There is some woeful  freeware, just as there is some woeful commercial products.

The pedigree of the contributors to the base package is hard to beat. I wouldn't know the pedigree of those who write the other stats programmes, but I assume that R contributors are right in there, with the best.

As to packages. They must vary with quality, and people do make mistakes. If you have something that in modern parlance is "mission critical" it wouldn't matter which product you had, you would test it to see that it fitted your requirements.

You have raised a question that is often ignored or assumed. But to really know the answer for yourself you need to test it yourself or rely upon others that you trust. Whenever I start using a package I make sure it does not just what it states it can do, but also that it does what I want it to do.

Tom


> -----Original Message-----
> From: msck9 at mizzou.edu [mailto:msck9 at mizzou.edu]
> Sent: Thursday, 27 January 2005 1:10 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] A "rude" question
> 
> 
> Dear all, 
>  I am beginner using R. I have a question about it. When you use it,
>  since it is written by so many authors, how do you know that the
>  results are trustable?(I don't want to affend anyone, also I trust
>  people). But I think this should be a question.
> 
>  Thanks,
>  Ming
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jwd at surewest.net  Thu Jan 27 08:56:41 2005
From: jwd at surewest.net (John Dougherty)
Date: Wed, 26 Jan 2005 23:56:41 -0800
Subject: [R] A "rude" question
In-Reply-To: <20050127050951.GA26565@localhost>
References: <20050127050951.GA26565@localhost>
Message-ID: <200501262356.41964.jwd@surewest.net>

On Wednesday 26 January 2005 21:09, msck9 at mizzou.edu wrote:
> Dear all,
>  I am beginner using R. I have a question about it. When you use it,
>  since it is written by so many authors, how do you know that the
>  results are trustable?(I don't want to affend anyone, also I trust
>  people). But I think this should be a question.
>
Almost all software - generally all "important" software - is has numerous 
authors.  Windows has hundreds, perhaps thousand of coders.  So too does 
Unix.  The big difference between open source and closed source is not in the 
number of authors.  Rather it is in the open availability of the code.  
Arguably, if there is sufficient interest in an open source project, studies 
have indicated that the code is likely to be superior to that of a comparable 
closed source program.  This a probability though, not a natural law.

If you are concerned about the trustworthiness of R, then perhaps the best 
gauge is that some of our favorite if occasionally curmudgeonly authors on 
this list are also experts in S and S-Plus, the proprietary, closed source 
language of which R is also a dialect.  They evidently know what they're 
doing and work comfortably in both domains.

If you compare statistical results using R and Excel, there is no question 
that R is superior, but that will also be true if you tested Excel against 
S-Plus, or SAS, or NCSS - all proprietary programs, or any number of other 
closed and open source programs designed to do statistical analyses.  At the 
same time just about any spreadsheet, open or closed source will also suffer 
in a similar comparison.

If you want a more information about the safety of Excel I would suggest this 
site:

http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html

Read the various links. Beyond this there is a broad literature available on 
the risks and benefits of open and close source programs.  Read it.

JWDougherty



From ripley at stats.ox.ac.uk  Thu Jan 27 09:07:13 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 27 Jan 2005 08:07:13 +0000 (GMT)
Subject: [R] getting package version inside .First.lib
In-Reply-To: <16888.26659.789957.825999@maths.uwa.edu.au>
References: <16888.26659.789957.825999@maths.uwa.edu.au>
Message-ID: <Pine.LNX.4.61.0501270803240.9283@gannet.stats>

On Thu, 27 Jan 2005, Adrian Baddeley wrote:

> Greetings -
>
> Is it possible, inside .First.lib,
> to find out the version number of the package that is being loaded?
>
> If only one version of the package has been installed,
> we could scan the DESCRIPTION file, something like
>
> .First.lib <- function(lib, pkg) {
>    library.dynam("spatstat", pkg, lib)
>    dfile <- system.file("DESCRIPTION", package="spatstat")
>    ttt <- scan(dfile, what="", sep="^M", quiet=TRUE)[2]

"\n" not "^M", please, and readLines is better than scan here.

>    vvv <- strsplit(ttt," ")[[1]][2]
>    cat("spatstat version number",vvv,"\n")
> }
>
> but even this does not seem very safe (it makes assumptions about the
> format of the DESCRIPTION file).

It is better to use read.dcf or the installed description information in 
package.rds. Take a look at how library() does this.

Post R-2.0.0 you can assume the format is as library uses.

BTW: all installed.packages does is to read the descriptions of all the 
packages it finds, and in .First.lib you know the path to your package.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Bill.Venables at csiro.au  Thu Jan 27 09:15:24 2005
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Thu, 27 Jan 2005 19:15:24 +1100
Subject: [R] A "rude" question
Message-ID: <B998A44C8986644EA8029CFE6396A9241B28E4@exqld2-bne.qld.csiro.au>

When Haydn was asked about his 100+ symphonies he is reputed to have
replied "sunt mala bona mixta" which is kind of dog latin for "There are
good ones and bad ones all mixed together".  It's certainly the same
with R packages so to continue the latin motif: "caveat emptor"

The R engine, on the other hand, is pretty well uniformly excellent code
but you have to take my word for that.  Actually, you don't.  The whole
engine is open source so, if you wish, you can check every line of it.
If people were out to push dodgy software, this is not the way they'd go
about it.

Bill Venables.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of msck9 at mizzou.edu
Sent: Thursday, 27 January 2005 3:10 PM
To: r-help at stat.math.ethz.ch
Subject: [R] A "rude" question


Dear all, 
 I am beginner using R. I have a question about it. When you use it,
since it is written by so many authors, how do you know that the
results are trustable?(I don't want to affend anyone, also I trust
people). But I think this should be a question.

 Thanks,
 Ming

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From michela.marignani at uniroma1.it  Thu Jan 27 09:51:54 2005
From: michela.marignani at uniroma1.it (Michela Marignani)
Date: Thu, 27 Jan 2005 09:51:54 +0100
Subject: [R] Request for help
Message-ID: <001f01c5044d$73a42fa0$db4810ac@MAPPING>

My name is Michela Marignani and I'm an ecologist trying to solve a problem 
linked to knight' s tour algorithm.
I need a program to create random matrices with presence/absence (i.e. 1,0 
values), with defined colums and rows sums, to create null models for 
statistical comparison of species distribution phenomena.
I've seen on the web many solutions of the problem, but none provides the 
freedom to easily change row+colums constraint and none of them produce 
matrices  with 1 and 0. Also, I've tryied to use R, but it is too 
complicated for a not-statistician as I am....can you help me?

Thank you for your attention,
so long

Michela Marignani
University of Siena
Environmental Science Dept.
Siena, Italy
michela.marignani at uniroma1.it



From bates at stat.wisc.edu  Thu Jan 27 10:20:19 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 27 Jan 2005 03:20:19 -0600
Subject: [R] Specification of factorial random-effects model
In-Reply-To: <200501262243.j0QMhFKI006967@volta.gene.com>
References: <200501262243.j0QMhFKI006967@volta.gene.com>
Message-ID: <41F8B253.6060406@stat.wisc.edu>

Berton Gunter wrote:
> If you read the Help file for lme (!), you'll see that ~1|a*b is certainly
> incorrect.
> 
> Briefly, the issue has been discussed before on this list: the current
> version of lme() follows the original Laird/Ware formulation for **nested**
> random effects. Specifying **crossed** random effects is possible but
> difficult, and the fitting algorithm is not optimized for this. See p. 163
> in Bates and Pinheiro for an example.

The development package lme4 has a version of a linear mixed model 
function that does handle crossed random effects.  In lme4_0.8-1 and 
later the new version of lme, called lmer (which could mean either "lme 
revised" or "lme for R"), has a different syntax for specifying mixed 
models.  A random effects specification is indicated by a `|' character 
which  separates a linear model expression on the left side from the 
grouping factor on the right side.  Because the | operator has very low 
precedence, such terms usually must be enclosed in parentheses.

The same type of specification is used for nested or crossed or 
partially crossed grouping factors.  The only restriction is that the 
grouping factor must have a unique level for each group, which is to say 
that you must explicitly create nested factors - you cannot specify them 
implicitly.

This example could be fit as

 > (fm1 <- lmer(y ~ c + (1|a) +(1|b) + (1|a:b)))
Linear mixed-effects model fit by REML
Formula: y ~ c + (1 | a) + (1 | b) + (1 | a:b)
       AIC      BIC    logLik MLdeviance REMLdeviance
  376.0148 392.7759 -181.0074   369.2869     362.0148
Random effects:
  Groups   Name        Variance Std.Dev.
  a:b      (Intercept)   1.1118  1.0544
  b        (Intercept) 286.8433 16.9364
  a        (Intercept)  86.2138  9.2851
  Residual               3.4626  1.8608
# of obs: 81, groups: a:b, 9; b, 3; a, 3

Fixed effects:
              Estimate Std. Error DF  t value  Pr(>|t|)
(Intercept)  65.91259   11.16262 78   5.9048 8.707e-08
c2           -9.47000    0.50645 78 -18.6989 < 2.2e-16
c3          -10.88259    0.50645 78 -21.4881 < 2.2e-16

For the random effects the Variance column is the estimate of the 
variance component.  The Std.Dev. column is simply the square root of 
the estimated variance.  I find it easier to think in terms of standard 
deviations rather than variances because I can compare the standard 
deviations to the scale of the data.  Note that this column is *not* a 
standard error of the estimated variance component (and purposely so 
because I feel that such quantities are often nonsensical).

A test of, say, whether the variance component for the interaction could 
be zero is performed by fitting the reduced model and using the anova 
function to compare the fitted models.  The p-value quoted for this test 
is conservative because the null hypothesis is on the boundary of the 
parameter space.


 > (fm2 <- lmer(y ~ c + (1|a) +(1|b)))
Linear mixed-effects model fit by REML
Formula: y ~ c + (1 | a) + (1 | b)
       AIC      BIC    logLik MLdeviance REMLdeviance
  379.3209 393.6876 -183.6605   374.8822     367.3209
Random effects:
  Groups   Name        Variance Std.Dev.
  a        (Intercept)  86.3823  9.2942
  b        (Intercept) 286.5391 16.9275
  Residual               4.0039  2.0010
# of obs: 81, groups: a, 3; b, 3

Fixed effects:
             Estimate Std. Error DF  t value  Pr(>|t|)
(Intercept)  65.9126    11.1560 78   5.9083  8.58e-08
c2           -9.4700     0.5446 78 -17.3890 < 2.2e-16
c3          -10.8826     0.5446 78 -19.9829 < 2.2e-16
Warning message:
optim returned message ERROR: ABNORMAL_TERMINATION_IN_LNSRCH
  in: "LMEoptimize<-"(`*tmp*`, value = list(maxIter = 50, msMaxIter = 50,
 > anova(fm1, fm2)
Data:

Models:
fm2: y ~ c + (1 | a) + (1 | b)
fm1: y ~ c + (1 | a) + (1 | b) + (1 | a:b)

     Df     AIC     BIC  logLik  Chisq Chi Df Pr(>Chisq)
fm2  6  386.88  401.25 -187.44
fm1  7  383.29  400.05 -184.64 5.5953      1    0.01801

>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nicholas Galwey
>>Sent: Wednesday, January 26, 2005 1:45 PM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] Specification of factorial random-effects model
>>
>>I want to specify two factors and their interaction as random 
>>effects using
>>the function lme().   This works okay when I specify these 
>>terms using the
>>function Error() within the function aov(), but I can't get 
>>the same model
>>fitted using lme().   The code below illustrates the problem.
>>
>> 
>>
>>a <- factor(rep(c(1:3), each = 27))
>>
>>b <- factor(rep(rep(c(1:3), each = 9), times = 3))
>>
>>c <- factor(rep(rep(c(1:3), each = 3), times = 9))
>>
>>y <- c(74.59,75.63,76.7,63.48,63.17,65.99,64,66.35,64.5,
>>
>>   46.57,44.16,47.96,35.09,36.14,35.16,36.4,34.72,34.58,
>>
>>   41.82,47.35,45.74,33.33,36.8,33.38,34.13,34.39,34.48,
>>
>>   89.73,85.24,90.86,82.5,79.44,81.65,77.74,77.02,81.62,
>>
>>   59.32,62.29,60.7,55.42,55.5,51.17,50.54,53.54,51.85,
>>
>>   64.5,63.6,65.19,55.07,50.26,53.73,54.57,47.8,48.8,91.56,
>>
>>   94.49,92.17,82.14,83.16,81.31,83.58,78.63,77.08,60.53,
>>
>>   60.79,58.57,51.28,52.9,51.54,49.15,48.97,51.61,59.44,
>>
>>   60.07,60.07,51.94,52.2,50.2,49.45,50.75,49.56)
>>
>>anovamodel <- aov(y ~ c + Error(a*b))
>>
>>summary(anovamodel)
>>
>>lmemodel <- lme(y ~ c, random = ~ 1|a*b)
>>
>>anova(lmemodel)



From michael.watson at bbsrc.ac.uk  Thu Jan 27 10:23:44 2005
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 27 Jan 2005 09:23:44 -0000
Subject: [R] A "rude" question
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B999@iahce2knas1.iah.bbsrc.reserved>

Hi

I don't know if you are asking the question for the same reasons I did,
but recently (and ongoing) we have been required to adopt an
internationally recognised standard.  Being in the bioinformatics field,
where open-source software is the beating heart of cutting edge
research, we have obviously had to ask ourselves that exact question -
"How can we be sure the software we use works?".  

In science, this doesn't just apply to software though.  When someone
publishes a paper, how can any of us be sure they did what they said
they did?  Or that their methods are the correct ones to use?  Luckily,
there is a two word answer that we hope will satisfy our auditors, and
that is "Peer Review".  In the context of R, I would say that you could
put a confidence measure on any package based on the number of people
who use it; the more people who use a package, the more likely they are
to find and remove bugs.  

I won't get into the "open source" vs "commercial" argument, but put
simply, all software has bugs at some stage, no matter who has written
it.  Given that fact, I prefer the code to be open so I can see them,
not closed so that I can't.  The fact that we can see all code relating
to R is surely the biggest quality measure of all?

Cheers
Mick

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of msck9 at mizzou.edu
Sent: 27 January 2005 05:10
To: r-help at stat.math.ethz.ch
Subject: [R] A "rude" question


Dear all, 
 I am beginner using R. I have a question about it. When you use it,
since it is written by so many authors, how do you know that the
results are trustable?(I don't want to affend anyone, also I trust
people). But I think this should be a question.

 Thanks,
 Ming

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From rainer.grohmann at gmx.net  Thu Jan 27 10:27:43 2005
From: rainer.grohmann at gmx.net (rainer grohmann)
Date: Thu, 27 Jan 2005 10:27:43 +0100 (MET)
Subject: [R] Results of MCD estimators in MASS and rrcov
Message-ID: <31046.1106818063@www69.gmx.net>

Hi!

I tested two different implementations of the robust MCD estimator:
cov.mcd from the MASS package and
covMcd from the rrcov package.
Tests were done on the hbk dataset included in the rrcov package. 

Unfortunately I get quite differing results -- so the question is whether
this differences are justified or an error on my side or a bug?

Here is, what I did:

> require(MASS)
> require(rrcov)
> data(hbk)

> mass.mcd<-cov.mcd(hbk,quantile.used=57)
> rrcov.covMcd<-covMcd(hbk,alpha=0.75)

> #output from cov.mcd (MASS)
> mass.mcd$center
         X1          X2          X3           Y
 1.55833333  1.80333333  1.66000000 -0.08666667
> mass.mcd$cov
           X1         X2         X3           Y
X1 1.12484463 0.02217514  0.1537288  0.07615819
X2 0.02217514 1.13897175  0.1814915  0.02029379
X3 0.15372881 0.18149153  1.0434576 -0.12877966
Y  0.07615819 0.02029379 -0.1287797  0.31236158

> #output from covMcd (rrcov)
> rrcov.covMcd$center
         X1          X2          X3           Y
 1.53770492  1.78032787  1.68688525 -0.07377049
> rrcov.covMcd$cov

             X1          X2         X3            Y
  X1 1.61921813 0.072595397  0.1678300  0.083905209
  X2 0.07259540 1.648137481  0.2013022  0.002657454
  X3 0.16782996 0.201302158  1.5306858 -0.150876964
  Y  0.08390521 0.002657454 -0.1508770  0.453846286

As you can see, the results are quite different. 

I tried to start both calls with 75% (=57 of 75) good data-points.

I crosschecked the results with the MCD implementation in MATLAB by Verboven
and Hubert. This functions give the same results as cov.mcd (MASS).

If somebody knows, why the results do not match, although both functions are
implementation referring to the same estimator, please tell me.

Thanks,
   Rainer

-- 
10 GB Mailbox, 100 FreeSMS http://www.gmx.net/de/go/topmail



From Roger.Bivand at nhh.no  Thu Jan 27 10:46:41 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 27 Jan 2005 10:46:41 +0100 (CET)
Subject: [R] getting package version inside .First.lib
In-Reply-To: <Pine.LNX.4.61.0501270803240.9283@gannet.stats>
Message-ID: <Pine.LNX.4.44.0501271042100.18370-100000@reclus.nhh.no>

On Thu, 27 Jan 2005, Prof Brian Ripley wrote:

> On Thu, 27 Jan 2005, Adrian Baddeley wrote:
> 
> > Greetings -
> >
> > Is it possible, inside .First.lib,
> > to find out the version number of the package that is being loaded?
> >
> > If only one version of the package has been installed,
> > we could scan the DESCRIPTION file, something like
> >
> > .First.lib <- function(lib, pkg) {
> >    library.dynam("spatstat", pkg, lib)
> >    dfile <- system.file("DESCRIPTION", package="spatstat")
> >    ttt <- scan(dfile, what="", sep="^M", quiet=TRUE)[2]
> 
> "\n" not "^M", please, and readLines is better than scan here.
> 
> >    vvv <- strsplit(ttt," ")[[1]][2]
> >    cat("spatstat version number",vvv,"\n")
> > }
> >
> > but even this does not seem very safe (it makes assumptions about the
> > format of the DESCRIPTION file).
> 
> It is better to use read.dcf or the installed description information in 
> package.rds. Take a look at how library() does this.

Or even packageDescription() in utils, which uses read.dcf() and should be
a way of making sure you get the version even if the underlying formatting
changes.

Roger

> 
> Post R-2.0.0 you can assume the format is as library uses.
> 
> BTW: all installed.packages does is to read the descriptions of all the 
> packages it finds, and in .First.lib you know the path to your package.
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From fm3a004 at math.uni-hamburg.de  Thu Jan 27 11:02:48 2005
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Thu, 27 Jan 2005 11:02:48 +0100 (MET)
Subject: [R] Cluster analysis using EM algorithm
In-Reply-To: <20050127023829.GA23735@localhost>
Message-ID: <Pine.GSO.3.95q.1050127110049.7563B-100000@sun12.math.uni-hamburg.de>

Hi!

Take a look at the packages mclust and flexmix!
They use the EM algorithm for mixture modelling, sometimes called "model
based cluster analysis".

Best,
Christian

On Wed, 26 Jan 2005 msck9 at mizzou.edu wrote:

> Hi, 
>  I am looking for a package to do the clustering analysis using the
>  expectation maximization algorithm. 
> 
>  Thanks in advance.
> 
>  Ming
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From coutand at clermont.inra.fr  Thu Jan 27 12:55:49 2005
From: coutand at clermont.inra.fr (coutand)
Date: Thu, 27 Jan 2005 11:55:49 +0000
Subject: [R] computing roots of bessel function
Message-ID: <4.3.2.7.1.20050127115151.019d4340@valmont.clermont.inra.fr>

I am not yet a R user but I will be soon.
I am looking for the R command and syntax to compute the roots of Bessel 
function i.e. computing the z values that lead to Jnu(z)=0 where J is a 
Bessel function or order nu.
May You help me ?
thanks in advance.

Dr Catherine COUTAND
Institut National de la Recherche Agronomique (INRA)
umr Physiologie Int?grative de l'Arbre Fruitier et Forestier (PIAF)
234 av. du Br?zet
63039 Clermont-Ferrand cedex 02
France

tel : 00-33-(0)4-73-62-46-73
fax : 00-33-(0)4-73-62-44-54
email : coutand at clermont.inra.fr
http://www.clermont.inra.fr/piaf



From ligges at statistik.uni-dortmund.de  Thu Jan 27 11:59:57 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 27 Jan 2005 11:59:57 +0100
Subject: [R] Converting yr mo da to dates
In-Reply-To: <001001c503d2$080d6c00$7c7e4f81@ads.iu.edu>
References: <001001c503d2$080d6c00$7c7e4f81@ads.iu.edu>
Message-ID: <41F8C9AD.6030102@statistik.uni-dortmund.de>

David Parkhurst wrote:

> I'm using R 2.0.1 in windows XP (and am not currently subscribed to this 
> mailing list).
> 
> I have a USGS dataset, a text file with fixed width fields, that 
> includes dates as 6-digit integers in the form yrmoda.  I could either 
> read them that way, or with yr, mo, and da as separate integers.  In 
> either case, I'd like to convert them to a form will allow plotting 
> other "y" variables against the dates (with correct spacing) on the 
> horizontal axis.
> 
> I've looked in all the manuals, but didn't find a way to do this.  I can 
> copy the data to a spreadsheet, make the conversion there, and then move 
> the data to R, but that's a nuisance.
> 
> I'd appreciate learning whether there is a way to do this all within R. 
> Thanks.
> 
> Dave Parkhurst
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

See ?strptime as in:

strptime(c("050127", "050128"), "%y%m%d")


Uwe Ligges



From Christoph.Scherber at uni-jena.de  Thu Jan 27 12:20:25 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Thu, 27 Jan 2005 12:20:25 +0100
Subject: [R] self-written function
Message-ID: <41F8CE79.3090505@uni-jena.de>

Dear all,

I?ve got a simple self-written function to calculate the mean + s.e. 
from arcsine-transformed data:

backsin<-function(x,y,...){
 backtransf<-list()
 backtransf$back<-((sin(x[x!="NA"]))^2)*100
 backtransf$mback<-tapply(backtransf$back,y[x!="NA"],mean)
 backtransf$sdback<-tapply(backtransf$back,y[x!="NA"],stdev)/sqrt(length(y[x!="NA"]))
 backtransf
}

I would like to apply this function to whole datasets, such as

tapply(variable,list(A,B,C,D),backsin)

Of course, this doesn?t work with the way in which the backsin() 
function is specified.

Does anyone have suggestions on how I could improve my function?

Regards,
Christoph



From andy_liaw at merck.com  Thu Jan 27 12:24:45 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 27 Jan 2005 06:24:45 -0500
Subject: [R] getting package version inside .First.lib
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5C8@usrymx25.merck.com>

> From: Roger Bivand
> 
> On Thu, 27 Jan 2005, Prof Brian Ripley wrote:
> 
> > On Thu, 27 Jan 2005, Adrian Baddeley wrote:
> > 
> > > Greetings -
> > >
> > > Is it possible, inside .First.lib,
> > > to find out the version number of the package that is 
> being loaded?
> > >
> > > If only one version of the package has been installed,
> > > we could scan the DESCRIPTION file, something like
> > >
> > > .First.lib <- function(lib, pkg) {
> > >    library.dynam("spatstat", pkg, lib)
> > >    dfile <- system.file("DESCRIPTION", package="spatstat")
> > >    ttt <- scan(dfile, what="", sep="^M", quiet=TRUE)[2]
> > 
> > "\n" not "^M", please, and readLines is better than scan here.
> > 
> > >    vvv <- strsplit(ttt," ")[[1]][2]
> > >    cat("spatstat version number",vvv,"\n")
> > > }
> > >
> > > but even this does not seem very safe (it makes 
> assumptions about the
> > > format of the DESCRIPTION file).
> > 
> > It is better to use read.dcf or the installed description 
> information in 
> > package.rds. Take a look at how library() does this.
> 
> Or even packageDescription() in utils, which uses read.dcf() 
> and should be
> a way of making sure you get the version even if the 
> underlying formatting
> changes.

This is how I do it in randomForest (using .onAttach instead of .First.Lib):

.onAttach <- function(libname, pkgname) {
    RFver <- if (as.numeric(R.version$major) < 2 && 
                 as.numeric(R.version$minor) < 9.0)
      package.description("randomForest")["Version"] else
    packageDescription("randomForest")$Version
    cat(paste("randomForest", RFver), "\n")
    cat("Type rfNews() to see new features/changes/bug fixes.\n")
}

HTH,
Andy
 
> Roger
> 
> > 
> > Post R-2.0.0 you can assume the format is as library uses.
> > 
> > BTW: all installed.packages does is to read the 
> descriptions of all the 
> > packages it finds, and in .First.lib you know the path to 
> your package.
> > 
> > 
> 
> -- 
> Roger Bivand
> Economic Geography Section, Department of Economics, 
> Norwegian School of
> Economics and Business Administration, Breiviksveien 40, 
> N-5045 Bergen,
> Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
> e-mail: Roger.Bivand at nhh.no
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Thu Jan 27 12:31:01 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 27 Jan 2005 11:31:01 +0000 (GMT)
Subject: [R] getting package version inside .First.lib
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E5C8@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E5C8@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.61.0501271126310.4424@gannet.stats>

On Thu, 27 Jan 2005, Liaw, Andy wrote:

>> From: Roger Bivand
>>
>> On Thu, 27 Jan 2005, Prof Brian Ripley wrote:
>>
>>> On Thu, 27 Jan 2005, Adrian Baddeley wrote:
>>>
>>>> Greetings -
>>>>
>>>> Is it possible, inside .First.lib,
>>>> to find out the version number of the package that is
>> being loaded?
>>>>
>>>> If only one version of the package has been installed,
>>>> we could scan the DESCRIPTION file, something like
>>>>
>>>> .First.lib <- function(lib, pkg) {
>>>>    library.dynam("spatstat", pkg, lib)
>>>>    dfile <- system.file("DESCRIPTION", package="spatstat")
>>>>    ttt <- scan(dfile, what="", sep="^M", quiet=TRUE)[2]
>>>
>>> "\n" not "^M", please, and readLines is better than scan here.
>>>
>>>>    vvv <- strsplit(ttt," ")[[1]][2]
>>>>    cat("spatstat version number",vvv,"\n")
>>>> }
>>>>
>>>> but even this does not seem very safe (it makes
>> assumptions about the
>>>> format of the DESCRIPTION file).
>>>
>>> It is better to use read.dcf or the installed description
>> information in
>>> package.rds. Take a look at how library() does this.
>>
>> Or even packageDescription() in utils, which uses read.dcf() and should 
>> be a way of making sure you get the version even if the underlying 
>> formatting changes.
>
> This is how I do it in randomForest (using .onAttach instead of .First.Lib):
>
> .onAttach <- function(libname, pkgname) {
>    RFver <- if (as.numeric(R.version$major) < 2 &&
>                 as.numeric(R.version$minor) < 9.0)
>      package.description("randomForest")["Version"] else
>    packageDescription("randomForest")$Version
>    cat(paste("randomForest", RFver), "\n")
>    cat("Type rfNews() to see new features/changes/bug fixes.\n")
> }

Please don't use functions from utils in such places without explicitly 
loading them from utils unless your package has an explicit dependence on 
utils (and randomForest does not).

There was a good reason why I suggested what I did: you don't need the 
utils namespace for this.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From adrian at maths.uwa.edu.au  Thu Jan 27 12:31:18 2005
From: adrian at maths.uwa.edu.au (Adrian Baddeley)
Date: Thu, 27 Jan 2005 19:31:18 +0800
Subject: [R] getting package version inside .First.lib
Message-ID: <16888.53510.945011.275908@maths.uwa.edu.au>

Thanks, Brian. 

So, to print the version number when 'mypackage' is loaded,

.First.lib <- function(lib, pkg) {
    library.dynam("mypackage", pkg, lib)
    vvv <- read.dcf(file=system.file("DESCRIPTION", package="mypackage"), 
                    fields="Version")
    cat(paste("mypackage", vvv, "\n"))
}



From r.hankin at soc.soton.ac.uk  Thu Jan 27 12:28:10 2005
From: r.hankin at soc.soton.ac.uk (Robin Hankin)
Date: Thu, 27 Jan 2005 11:28:10 +0000
Subject: [R] computing roots of bessel function
In-Reply-To: <4.3.2.7.1.20050127115151.019d4340@valmont.clermont.inra.fr>
References: <4.3.2.7.1.20050127115151.019d4340@valmont.clermont.inra.fr>
Message-ID: <fb78404614c8b7b9d65c6f4d629d7998@soc.soton.ac.uk>

hi

package(gsl)

calculates zeroes of regular Bessel functions of integral order.

You need function bessel_zero_Jnu()


best wishes

Robn

On Jan 27, 2005, at 11:55 am, coutand wrote:

> I am not yet a R user but I will be soon.
> I am looking for the R command and syntax to compute the roots of 
> Bessel function i.e. computing the z values that lead to Jnu(z)=0 
> where J is a Bessel function or order nu.
> May You help me ?
> thanks in advance.
>
> Dr Catherine COUTAND
> Institut National de la Recherche Agronomique (INRA)
> umr Physiologie Int?grative de l'Arbre Fruitier et Forestier (PIAF)
> 234 av. du Br?zet
> 63039 Clermont-Ferrand cedex 02
> France
>
> tel : 00-33-(0)4-73-62-46-73
> fax : 00-33-(0)4-73-62-44-54
> email : coutand at clermont.inra.fr
> http://www.clermont.inra.fr/piaf
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
--
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From andy_liaw at merck.com  Thu Jan 27 12:43:05 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 27 Jan 2005 06:43:05 -0500
Subject: [R] getting package version inside .First.lib
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5C9@usrymx25.merck.com>

> From: Prof Brian Ripley 
> 
> On Thu, 27 Jan 2005, Liaw, Andy wrote:
> 
> >> From: Roger Bivand
> >>
> >> On Thu, 27 Jan 2005, Prof Brian Ripley wrote:
> >>
> >>> On Thu, 27 Jan 2005, Adrian Baddeley wrote:
> >>>
> >>>> Greetings -
> >>>>
> >>>> Is it possible, inside .First.lib,
> >>>> to find out the version number of the package that is
> >> being loaded?
> >>>>
> >>>> If only one version of the package has been installed,
> >>>> we could scan the DESCRIPTION file, something like
> >>>>
> >>>> .First.lib <- function(lib, pkg) {
> >>>>    library.dynam("spatstat", pkg, lib)
> >>>>    dfile <- system.file("DESCRIPTION", package="spatstat")
> >>>>    ttt <- scan(dfile, what="", sep="^M", quiet=TRUE)[2]
> >>>
> >>> "\n" not "^M", please, and readLines is better than scan here.
> >>>
> >>>>    vvv <- strsplit(ttt," ")[[1]][2]
> >>>>    cat("spatstat version number",vvv,"\n")
> >>>> }
> >>>>
> >>>> but even this does not seem very safe (it makes
> >> assumptions about the
> >>>> format of the DESCRIPTION file).
> >>>
> >>> It is better to use read.dcf or the installed description
> >> information in
> >>> package.rds. Take a look at how library() does this.
> >>
> >> Or even packageDescription() in utils, which uses 
> read.dcf() and should 
> >> be a way of making sure you get the version even if the underlying 
> >> formatting changes.
> >
> > This is how I do it in randomForest (using .onAttach 
> instead of .First.Lib):
> >
> > .onAttach <- function(libname, pkgname) {
> >    RFver <- if (as.numeric(R.version$major) < 2 &&
> >                 as.numeric(R.version$minor) < 9.0)
> >      package.description("randomForest")["Version"] else
> >    packageDescription("randomForest")$Version
> >    cat(paste("randomForest", RFver), "\n")
> >    cat("Type rfNews() to see new features/changes/bug fixes.\n")
> > }
> 
> Please don't use functions from utils in such places without 
> explicitly 
> loading them from utils unless your package has an explicit 
> dependence on 
> utils (and randomForest does not).
> 
> There was a good reason why I suggested what I did: you don't 
> need the 
> utils namespace for this.

Thanks for the tip!  Will remediate...

Andy
 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
>



From pburns at pburns.seanet.com  Thu Jan 27 13:19:17 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Thu, 27 Jan 2005 12:19:17 +0000
Subject: [R] Request for help
In-Reply-To: <001f01c5044d$73a42fa0$db4810ac@MAPPING>
References: <001f01c5044d$73a42fa0$db4810ac@MAPPING>
Message-ID: <41F8DC45.9070209@pburns.seanet.com>

If I understand your problem properly, then your matrices have a
known number of zeros and ones in them.  So you can create a
matrix with just this constraint binding via:

mat <- matrix(sample(rep(0:1, c(nzeros, nones))), nr, nc)

That command first generates the appropriate number of zeros and ones
(via 'rep'), then does a random permutation of them (with 'sample') and
finally turns it into a matrix.

You could then test for the row and column constraints, and permute
the sub-matrix of rows and columns that do not obey their constraints.
It could look something like:

mat[bad.rows, bad.cols] <- sample(mat[bad.rows, bad.cols])

where 'bad.rows' and 'bad.cols' are logical vectors stating if the 
constraints
are satisfied or not.

You do not need to be a statistician to use R -- far from it.  The 
'Guide for the
Unwilling' gives you a  brief introduction.  There is also a lot of 
introductory
material in the contributed documentation section of  the R Project website.

It would be good to use a more descriptive subject for messages to R-help.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Michela Marignani wrote:

> My name is Michela Marignani and I'm an ecologist trying to solve a 
> problem linked to knight' s tour algorithm.
> I need a program to create random matrices with presence/absence (i.e. 
> 1,0 values), with defined colums and rows sums, to create null models 
> for statistical comparison of species distribution phenomena.
> I've seen on the web many solutions of the problem, but none provides 
> the freedom to easily change row+colums constraint and none of them 
> produce matrices  with 1 and 0. Also, I've tryied to use R, but it is 
> too complicated for a not-statistician as I am....can you help me?
>
> Thank you for your attention,
> so long
>
> Michela Marignani
> University of Siena
> Environmental Science Dept.
> Siena, Italy
> michela.marignani at uniroma1.it
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
>



From BPikouni at CNTUS.JNJ.COM  Thu Jan 27 13:23:12 2005
From: BPikouni at CNTUS.JNJ.COM (Pikounis, Bill [CNTUS])
Date: Thu, 27 Jan 2005 07:23:12 -0500
Subject: [R] A "rude" question
Message-ID: <E5382FD31214D6118FF40002A541DECE1185E9B7@CNTUSMAEXS4.na.jnj.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050127/4f2e2fa9/attachment.pl

From gfraser1 at staffmail.ed.ac.uk  Thu Jan 27 13:29:43 2005
From: gfraser1 at staffmail.ed.ac.uk (George Fraser)
Date: Thu, 27 Jan 2005 12:29:43 +0000
Subject: [R] Output predictions based on a Cox model
Message-ID: <1106828983.5683.11.camel@localhost.localdomain>

Hi,

I've generated a cox model, but I'm struggling to work out how to output
predctions based on the model I've made.

my.model<-coxph(Surv(duration,status) ~ gender + var1 + var2,
data=mydata)

My test data set looks something like this:

id,actualduration,gender,var1,var2
a,65,m,1,3
b,34,f,1,5
...

What i need to do is for each id, output a predicted duration based on
my cox model so that I can compare it with other models.

I've looked in the survival package, and the Design package, but I can
only see how to output survival probabilities.  I'm probably missing
something obvious, but trawling the mail archives has been fruitless,
any suggestions?

Cheers,
George



From mwgrant2001 at yahoo.com  Thu Jan 27 14:21:38 2005
From: mwgrant2001 at yahoo.com (Michael Grant)
Date: Thu, 27 Jan 2005 05:21:38 -0800 (PST)
Subject: [R] A "rude" question
In-Reply-To: <20050127050951.GA26565@localhost>
Message-ID: <20050127132139.22253.qmail@web52002.mail.yahoo.com>

Ming,

You have received a number of excellent replies to
your question and should really consider them. Here is
another point--really extending a Bill Venables
comment:

"If people were out to push dodgy software, this is
not the way they'd go about it."

Definitely! Look at the requirements for submitting a
package to R. While the required documentation and
uniform approach mandated do not automatically equate
to V&V'ed code it is a strong indication of commitment
of the R core and contributing communities. The
imposition of these standards by the core team and the
time committed to the project vis-a-vis development,
the help list, etc. speaks volumes about the quality
of  R. Rest assured such commitment is not the norm.

That being said, I do respectfully disagree with Dr.
Rossini in one minor detail ;O). It is not 'extremely
paranoid' to  re-code in another language and
definitely not so to do hand calculations! Murphy's
Law is relentless in all matters! If you are like most
of us (all of us?) you will find errors in your own
coding and maybe rarely an R bug. 

BTW, since you are starting out in R...voraciously
read the documentation, helplist, newletter, and other
free and commercial material on R, work thru the
examples relevant to you area of endeavor, read more,
code more, read more, code more, read more, code
more.... The facility with R that you gain as a result
will reward you multifold down the road.

Best regards,
Michael Grant

P.S. Whenever you upgrade R, read the CHANGES, NEWS
files, etc. R does evolve--even the core--although it
is very controlled and managed. (You will learn of
bugfixes there too.)


--- msck9 at mizzou.edu wrote:

> Dear all, 
>  I am beginner using R. I have a question about it.
> When you use it,
>  since it is written by so many authors, how do you
> know that the
>  results are trustable?(I don't want to affend
> anyone, also I trust
>  people). But I think this should be a question.
> 
>  Thanks,
>  Ming
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From buser at stat.math.ethz.ch  Thu Jan 27 14:31:16 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Thu, 27 Jan 2005 14:31:16 +0100
Subject: [R] sw
In-Reply-To: <32727.1106781916@www58.gmx.net>
References: <32727.1106781916@www58.gmx.net>
Message-ID: <16888.60708.364548.615599@stat.math.ethz.ch>

Dear Mahdi

Mahdi Osman writes:
 > Hi list,
 > 
 > I am just a new user of R.
 > 
 > How can I run stepwise regression in R?

If you look for a stepwise procedure for Linear Regression
Models or Generalized Linear Models, you can use step() 
(see ?step) 

Regards,

Christoph

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C11
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-1-632-5414		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------



 > Is there a graphic user interphase for any of the spatial packages inculded
 > in R, such as gstat, geoR and someothers. I am mainly interested interactive
 > variogram modelling and mapping.
 > 
 > Thanks
 > Mahdi
 > 
 > -- 
 > -----------------------------------
 > Mahdi Osman (PhD)
 > E-mail: m_osm at gmx.net
 > -----------------------------------
 > 
 > 10 GB Mailbox, 100 FreeSMS http://www.gmx.net/de/go/topmail
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From morten.mattingsdal at student.uib.no  Thu Jan 27 15:06:32 2005
From: morten.mattingsdal at student.uib.no (Morten Mattingsdal)
Date: Thu, 27 Jan 2005 15:06:32 +0100
Subject: [R] cluster, mona error
Message-ID: <41F8F568.8010908@student.uib.no>

Hi

I have a problem using the package cluster on my binary data. I want to 
try mona at first. But i get the an error.

hc<-read.table("all.txt", header=TRUE, sep="\t", row.names=1)
srt(hc)
`data.frame':   51 obs. of  59 variables:
 $ G1p : int  2 1 1 1 1 1 1 1 1 1 ...
 $ G1q : int  1 1 1 1 1 1 1 1 1 1 ...
 $ G2p : int  1 1 1 1 1 1 1 1 1 1 ...
 $ G2q : int  1 1 1 1 1 1 1 1 1 1 ...
 $ G3p : int  1 1 1 1 1 1 1 1 1 1 ...

m<-mona(hc)
Error in mona(hc) : All variables must be binary (factor with 2 levels).

I find this strange when the cluster dataset "animals" have the same 
structure as my data.

srt(animals)
`data.frame':   20 obs. of  6 variables:
 $ war: int  1 1 2 1 2 2 2 2 2 1 ...
 $ fly: int  1 2 1 1 1 1 2 2 1 2 ...
 $ ver: int  1 1 2 1 2 2 2 2 2 1 ...
 $ end: int  1 1 1 1 2 1 1 2 2 1 ...
 $ gro: int  2 2 1 1 2 2 2 1 2 1 ...
 $ hai: int  1 2 2 2 2 2 1 1 1 1 ...

m<-mona(animals) #works fine

what is this error trying to tell me?
mvh
morten



From rpeng at jhsph.edu  Thu Jan 27 15:08:15 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 27 Jan 2005 09:08:15 -0500
Subject: [R] getting package version inside .First.lib
In-Reply-To: <16888.26659.789957.825999@maths.uwa.edu.au>
References: <16888.26659.789957.825999@maths.uwa.edu.au>
Message-ID: <41F8F5CF.3030701@jhsph.edu>

This is what I use for all my packages, which I believe handles multiple 
versions of the same package being installed:

.First.lib <- function(lib, pkg) {
     ver <- read.dcf(file.path(lib, pkg, "DESCRIPTION"), "Version")
     ver <- as.character(ver)
     ...
}

-roger

Adrian Baddeley wrote:
> Greetings - 
> 
> Is it possible, inside .First.lib,
> to find out the version number of the package that is being loaded?
> 
> If only one version of the package has been installed,
> we could scan the DESCRIPTION file, something like
> 
> .First.lib <- function(lib, pkg) {
>     library.dynam("spatstat", pkg, lib)
>     dfile <- system.file("DESCRIPTION", package="spatstat")
>     ttt <- scan(dfile, what="", sep="^M", quiet=TRUE)[2]
>     vvv <- strsplit(ttt," ")[[1]][2]
>     cat("spatstat version number",vvv,"\n")
> }
> 
> but even this does not seem very safe (it makes assumptions about the
> format of the DESCRIPTION file).
> 
> Is there a better way?
> 
> thanks
> Adrian Baddeley
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From sdavis2 at mail.nih.gov  Thu Jan 27 15:16:11 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 27 Jan 2005 09:16:11 -0500
Subject: [R] cluster, mona error
In-Reply-To: <41F8F568.8010908@student.uib.no>
References: <41F8F568.8010908@student.uib.no>
Message-ID: <FEA43547-706D-11D9-8FBF-000D933565E8@mail.nih.gov>


On Jan 27, 2005, at 9:06 AM, Morten Mattingsdal wrote:

> Hi
>
> I have a problem using the package cluster on my binary data. I want 
> to try mona at first. But i get the an error.
>
> hc<-read.table("all.txt", header=TRUE, sep="\t", row.names=1)
> srt(hc)
> `data.frame':   51 obs. of  59 variables:
> $ G1p : int  2 1 1 1 1 1 1 1 1 1 ...
> $ G1q : int  1 1 1 1 1 1 1 1 1 1 ...
> $ G2p : int  1 1 1 1 1 1 1 1 1 1 ...
> $ G2q : int  1 1 1 1 1 1 1 1 1 1 ...
> $ G3p : int  1 1 1 1 1 1 1 1 1 1 ...
>
> m<-mona(hc)
> Error in mona(hc) : All variables must be binary (factor with 2 
> levels).
>

You have to be careful that the data are indeed each factors with 2 
levels (numeric variables with values 1 and 2 will not do).  A summary 
of the data will tell you that.

Sean

> I find this strange when the cluster dataset "animals" have the same 
> structure as my data.
>
> srt(animals)
> `data.frame':   20 obs. of  6 variables:
> $ war: int  1 1 2 1 2 2 2 2 2 1 ...
> $ fly: int  1 2 1 1 1 1 2 2 1 2 ...
> $ ver: int  1 1 2 1 2 2 2 2 2 1 ...
> $ end: int  1 1 1 1 2 1 1 2 2 1 ...
> $ gro: int  2 2 1 1 2 2 2 1 2 1 ...
> $ hai: int  1 2 2 2 2 2 1 1 1 1 ...
>
> m<-mona(animals) #works fine
>
> what is this error trying to tell me?
> mvh
> morten
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From morten.mattingsdal at student.uib.no  Thu Jan 27 15:30:34 2005
From: morten.mattingsdal at student.uib.no (Morten Mattingsdal)
Date: Thu, 27 Jan 2005 15:30:34 +0100
Subject: [R] cluster, mona error solved
In-Reply-To: <FEA43547-706D-11D9-8FBF-000D933565E8@mail.nih.gov>
References: <41F8F568.8010908@student.uib.no>
	<FEA43547-706D-11D9-8FBF-000D933565E8@mail.nih.gov>
Message-ID: <41F8FB0A.8080207@student.uib.no>

Sean Davis wrote:

>
> On Jan 27, 2005, at 9:06 AM, Morten Mattingsdal wrote:
>
>> Hi
>>
>> I have a problem using the package cluster on my binary data. I want 
>> to try mona at first. But i get the an error.
>>
>> hc<-read.table("all.txt", header=TRUE, sep="\t", row.names=1)
>> srt(hc)
>> `data.frame':   51 obs. of  59 variables:
>> $ G1p : int  2 1 1 1 1 1 1 1 1 1 ...
>> $ G1q : int  1 1 1 1 1 1 1 1 1 1 ...
>> $ G2p : int  1 1 1 1 1 1 1 1 1 1 ...
>> $ G2q : int  1 1 1 1 1 1 1 1 1 1 ...
>> $ G3p : int  1 1 1 1 1 1 1 1 1 1 ...
>>
>> m<-mona(hc)
>> Error in mona(hc) : All variables must be binary (factor with 2 levels).
>>
>
> You have to be careful that the data are indeed each factors with 2 
> levels (numeric variables with values 1 and 2 will not do).  A summary 
> of the data will tell you that.
>
> Sean
>

Yes. Now I understand. There was one single variable among my 59, which 
did only have 1 level: I used summary(mydata) as you said:
and found

L16p
Min.   :1
1st Qu.:1
Median :1
Mean   :1
3rd Qu.:1
Max.   :1

I removed this and now it workes fine.... thanks alot for your quick reply
regards
greatful morten



From fm3a004 at math.uni-hamburg.de  Thu Jan 27 15:31:45 2005
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Thu, 27 Jan 2005 15:31:45 +0100 (MET)
Subject: [R] cluster, mona error
In-Reply-To: <41F8F568.8010908@student.uib.no>
Message-ID: <Pine.GSO.3.95q.1050127152949.7563D-100000@sun12.math.uni-hamburg.de>

Morten,

just a try: is there a constant variable (only 1) in the first dataset?

Christian

On Thu, 27 Jan 2005, Morten Mattingsdal wrote:

> Hi
> 
> I have a problem using the package cluster on my binary data. I want to 
> try mona at first. But i get the an error.
> 
> hc<-read.table("all.txt", header=TRUE, sep="\t", row.names=1)
> srt(hc)
> `data.frame':   51 obs. of  59 variables:
>  $ G1p : int  2 1 1 1 1 1 1 1 1 1 ...
>  $ G1q : int  1 1 1 1 1 1 1 1 1 1 ...
>  $ G2p : int  1 1 1 1 1 1 1 1 1 1 ...
>  $ G2q : int  1 1 1 1 1 1 1 1 1 1 ...
>  $ G3p : int  1 1 1 1 1 1 1 1 1 1 ...
> 
> m<-mona(hc)
> Error in mona(hc) : All variables must be binary (factor with 2 levels).
> 
> I find this strange when the cluster dataset "animals" have the same 
> structure as my data.
> 
> srt(animals)
> `data.frame':   20 obs. of  6 variables:
>  $ war: int  1 1 2 1 2 2 2 2 2 1 ...
>  $ fly: int  1 2 1 1 1 1 2 2 1 2 ...
>  $ ver: int  1 1 2 1 2 2 2 2 2 1 ...
>  $ end: int  1 1 1 1 2 1 1 2 2 1 ...
>  $ gro: int  2 2 1 1 2 2 2 1 2 1 ...
>  $ hai: int  1 2 2 2 2 2 1 1 1 1 ...
> 
> m<-mona(animals) #works fine
> 
> what is this error trying to tell me?
> mvh
> morten
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From Robert.McGehee at geodecapital.com  Thu Jan 27 15:34:09 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Thu, 27 Jan 2005 09:34:09 -0500
Subject: [R] Indexing Lists and Partial Matching
Message-ID: <67DCA285A2D7754280D3B8E88EB5480206741E40@MSGBOSCLB2WIN.DMN1.FMR.COM>

I was unaware until recently that partial matching was used to index
data frames and lists. This is now causing a great deal of problems in
my code as I sometimes index a list without knowing what elements it
contains, expecting a NULL if the column does not exist. However, if
partial matching is used, sometimes R will return an object I do not
want. My question, is there an easy way of getting around this?

For example:
> a <- NULL
> a$abc <- 5
> a$a
[1] 5
> a$a <- a$a
> a
$abc
[1] 5
$a
[1] 5

Certainly from a coding prospective, one might expect assigning a$a to
itself wouldn't do anything since either 1) a$a doesn't exist, so
nothing happens, or 2) a$a does exist and so it just assigns its value
to itself. However, in the above case, it creates a new column entirely
because I happen to have another column called a$abc. I do not want this
behavior.

The solution I came up with was to create another indexing function that
uses the subset() (which doesn't partial match), then check for an
error, and if there is an error substitute NULL (to mimic the "["
behavior). However, I don't really want to start using another indexing
function altogether just to get around this behavior. Is there a better
way? Can I turn off partial matching?

Thanks,
Robert


Robert McGehee
Geode Capital Management, LLC
53 State Street, 5th Floor | Boston, MA | 02109
Tel: 617/392-8396    Fax:617/476-6389
mailto:robert.mcgehee at geodecapital.com



This e-mail, and any attachments hereto, are intended for us...{{dropped}}



From info at rhkoning.com  Thu Jan 27 15:47:24 2005
From: info at rhkoning.com (Ruud H. Koning)
Date: Thu, 27 Jan 2005 15:47:24 +0100
Subject: [R] partial ranking models
Message-ID: <41F8FEFC.3030608@rhkoning.com>

Dear R-users, is a library available to estimate partial ranking models? 
Best, Ruud



From r.g.brown at cefas.co.uk  Thu Jan 27 15:55:10 2005
From: r.g.brown at cefas.co.uk (Robert Brown FM CEFAS)
Date: Thu, 27 Jan 2005 14:55:10 -0000
Subject: [R] weighting in nls
Message-ID: <3589BC4D64C84341AE0C258244F977A2B60B8F@expressa.corp.cefas.co.uk>

I'm fitting nonlinear functions to some growth data but  I'm getting radically different results in R to another program (Prism). Furthermore the values from the other program give a better fit and seem more realistic.  I think there is a problem with the results from the r nls function. The differences only occur with weighted data so I think I'm making a mistake in the weighting. I'm following the procedure outlined on p 244 of MASS (or at least I'm trying to).

Thus, I'm using mean data with heteroscedasticity so I'm weighting by n/ variance, where the variance is well known from a large data set. This weighting factor is available as the variable 'novervar'.

The function is a von Bertalanffy curve of the form weight~(a*(1-exp(-b*(age-c))))^3.  Thus I'm entering the command in the form:

solb1wvb<-nls(~sqrt(novervar)*(weight-(a*(1-exp(-b*(age-c))))^3),data=solb1.na.rm,start=list(a=0.85,b=0.45,c=0.48))

Can anyone suggest what I'm doing wrong?  I seem to be folowing the instructions in MASS. I tried following the similar instructions on page 450 of the white book but these were a bit cryptic.

I'm using R 2.0.0 on a Windows 2000 machine

Regards,

Robert Brown


***********************************************************************************
This email and any attachments are intended for the named re...{{dropped}}



From andy_liaw at merck.com  Thu Jan 27 16:13:11 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 27 Jan 2005 10:13:11 -0500
Subject: [R] Indexing Lists and Partial Matching
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5CA@usrymx25.merck.com>

This has been discussed a few times on this list before, so you might want
to dig into the archive...

You might want to check existence of name instead of checking whether the
component is NULL:

> x <- list(bc="bc", ab="ab")
> is.null(x$b)
[1] FALSE
> "b" %in% names(x)
[1] FALSE

Andy


> From: McGehee, Robert
> 
> I was unaware until recently that partial matching was used to index
> data frames and lists. This is now causing a great deal of problems in
> my code as I sometimes index a list without knowing what elements it
> contains, expecting a NULL if the column does not exist. However, if
> partial matching is used, sometimes R will return an object I do not
> want. My question, is there an easy way of getting around this?
> 
> For example:
> > a <- NULL
> > a$abc <- 5
> > a$a
> [1] 5
> > a$a <- a$a
> > a
> $abc
> [1] 5
> $a
> [1] 5
> 
> Certainly from a coding prospective, one might expect assigning a$a to
> itself wouldn't do anything since either 1) a$a doesn't exist, so
> nothing happens, or 2) a$a does exist and so it just assigns its value
> to itself. However, in the above case, it creates a new 
> column entirely
> because I happen to have another column called a$abc. I do 
> not want this
> behavior.
> 
> The solution I came up with was to create another indexing 
> function that
> uses the subset() (which doesn't partial match), then check for an
> error, and if there is an error substitute NULL (to mimic the "["
> behavior). However, I don't really want to start using 
> another indexing
> function altogether just to get around this behavior. Is 
> there a better
> way? Can I turn off partial matching?
> 
> Thanks,
> Robert
> 
> 
> Robert McGehee
> Geode Capital Management, LLC
> 53 State Street, 5th Floor | Boston, MA | 02109
> Tel: 617/392-8396    Fax:617/476-6389
> mailto:robert.mcgehee at geodecapital.com
> 
> 
> 
> This e-mail, and any attachments hereto, are intended for 
> us...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From reid_huntsinger at merck.com  Thu Jan 27 16:14:31 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Thu, 27 Jan 2005 10:14:31 -0500
Subject: [R] Indexing Lists and Partial Matching
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A928D@uswpmx00.merck.com>

This came up a few months ago. Check the thread on hashing and partial
matching around Nov 18. The short answer is no, you can't turn it off
because lots of code relies on that behavior. 

Reid Huntsinger 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of McGehee, Robert
Sent: Thursday, January 27, 2005 9:34 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Indexing Lists and Partial Matching


I was unaware until recently that partial matching was used to index
data frames and lists. This is now causing a great deal of problems in
my code as I sometimes index a list without knowing what elements it
contains, expecting a NULL if the column does not exist. However, if
partial matching is used, sometimes R will return an object I do not
want. My question, is there an easy way of getting around this?

For example:
> a <- NULL
> a$abc <- 5
> a$a
[1] 5
> a$a <- a$a
> a
$abc
[1] 5
$a
[1] 5

Certainly from a coding prospective, one might expect assigning a$a to
itself wouldn't do anything since either 1) a$a doesn't exist, so
nothing happens, or 2) a$a does exist and so it just assigns its value
to itself. However, in the above case, it creates a new column entirely
because I happen to have another column called a$abc. I do not want this
behavior.

The solution I came up with was to create another indexing function that
uses the subset() (which doesn't partial match), then check for an
error, and if there is an error substitute NULL (to mimic the "["
behavior). However, I don't really want to start using another indexing
function altogether just to get around this behavior. Is there a better
way? Can I turn off partial matching?

Thanks,
Robert


Robert McGehee
Geode Capital Management, LLC
53 State Street, 5th Floor | Boston, MA | 02109
Tel: 617/392-8396    Fax:617/476-6389
mailto:robert.mcgehee at geodecapital.com



This e-mail, and any attachments hereto, are intended for us...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tfliao at uiuc.edu  Thu Jan 27 16:21:12 2005
From: tfliao at uiuc.edu (Tim F Liao)
Date: Thu, 27 Jan 2005 09:21:12 -0600
Subject: [R] A "rude" question
Message-ID: <173924d3.c546a2d8.8210600@expms6.cites.uiuc.edu>

I'm in agreement with Tom with respect to all the points he
made but two in particular:

Open code: very useful and much easier (than other software)
to make sure the trustworthiness of the function/library.  I
often do go into the code and make sure this is what I want
and it is a good way to find out the "meaning" of certain
parts of the output and to learn others' programming tricks. 
And that's the power of R.

Pedigree of the contributors: top-notch.  I remember finding a
"bug" (having to do with detecting heteroscedasticity) in SAS
back in the early 90s and communicated to a SAS tech.  SAS was
considered the industry's standard back then, but contributed
mostly by professonal programmers.  In comparison, R's
libraries are contributed by statisticians who are at the
forefront of statistical methods research.

Tim

---- Original message ----
>Date: Thu, 27 Jan 2005 14:15:31 +0800
>From: "Mulholland, Tom" <Tom.Mulholland at dpi.wa.gov.au>  
>Subject: RE: [R] A "rude" question  
>To: <msck9 at mizzou.edu>, <r-help at stat.math.ethz.ch>
>
>What makes you trust any software? 
>
>There are some obvious points. First of all the code is open
so if you know enough you can actually read the code and make
sure it does what you want. Secondly you can replicate a
process using two pieces of software and compare the results.
You can check the archives and you will find a number of posts
that talk about the results produced by R and how they compare
with other software. Typically R versus Excel or R versus SPSS
/ SAS. Just be careful as different answers does not
automatically mean one is wrong, and it certainly doesn't mean
R is wrong.
>
>Excel computes =ROUND(2.5,0) to be 3
>R computes round(2.5) to be 2
>
>As I understand it both are right, they are just using
different standards. I however have always used the latter and
rounded to the even number where the figure to be rounded lies
exactly at the halfway mark.
>
>Hang around this list for a short time and it will become
evident that if this software didn't work; the people using it
would have stopped using it long ago.
>
>Forget the commercial versus open software arguments that
raise their head from time to time. The question is how well a
piece of software is written / maintained & supported and not
issues of payment or the greater good. There is some woeful 
freeware, just as there is some woeful commercial products.
>
>The pedigree of the contributors to the base package is hard
to beat. I wouldn't know the pedigree of those who write the
other stats programmes, but I assume that R contributors are
right in there, with the best.
>
>As to packages. They must vary with quality, and people do
make mistakes. If you have something that in modern parlance
is "mission critical" it wouldn't matter which product you
had, you would test it to see that it fitted your requirements.
>
>You have raised a question that is often ignored or assumed.
But to really know the answer for yourself you need to test it
yourself or rely upon others that you trust. Whenever I start
using a package I make sure it does not just what it states it
can do, but also that it does what I want it to do.
>
>Tom
>
>
>> -----Original Message-----
>> From: msck9 at mizzou.edu [mailto:msck9 at mizzou.edu]
>> Sent: Thursday, 27 January 2005 1:10 PM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] A "rude" question
>> 
>> 
>> Dear all, 
>>  I am beginner using R. I have a question about it. When
you use it,
>>  since it is written by so many authors, how do you know
that the
>>  results are trustable?(I don't want to affend anyone, also
I trust
>>  people). But I think this should be a question.
>> 
>>  Thanks,
>>  Ming
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Thu Jan 27 16:25:06 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 27 Jan 2005 10:25:06 -0500
Subject: [R] weighting in nls
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5CB@usrymx25.merck.com>

Can you show us the difference; i.e., what are the parameter estimates and
associated SEs from the two programs?  Even better, can you supply an
example data set?

[With is `trick' for weighted nls, you need to be careful with the output of
predict().]

Andy

> From: Robert Brown FM CEFAS
> 
> I'm fitting nonlinear functions to some growth data but  I'm 
> getting radically different results in R to another program 
> (Prism). Furthermore the values from the other program give a 
> better fit and seem more realistic.  I think there is a 
> problem with the results from the r nls function. The 
> differences only occur with weighted data so I think I'm 
> making a mistake in the weighting. I'm following the 
> procedure outlined on p 244 of MASS (or at least I'm trying to).
> 
> Thus, I'm using mean data with heteroscedasticity so I'm 
> weighting by n/ variance, where the variance is well known 
> from a large data set. This weighting factor is available as 
> the variable 'novervar'.
> 
> The function is a von Bertalanffy curve of the form 
> weight~(a*(1-exp(-b*(age-c))))^3.  Thus I'm entering the 
> command in the form:
> 
> solb1wvb<-nls(~sqrt(novervar)*(weight-(a*(1-exp(-b*(age-c))))^
> 3),data=solb1.na.rm,start=list(a=0.85,b=0.45,c=0.48))
> 
> Can anyone suggest what I'm doing wrong?  I seem to be 
> folowing the instructions in MASS. I tried following the 
> similar instructions on page 450 of the white book but these 
> were a bit cryptic.
> 
> I'm using R 2.0.0 on a Windows 2000 machine
> 
> Regards,
> 
> Robert Brown
> 
> 
> **************************************************************
> *********************
> This email and any attachments are intended for the named 
> re...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From r.g.brown at cefas.co.uk  Thu Jan 27 16:35:07 2005
From: r.g.brown at cefas.co.uk (Robert Brown FM CEFAS)
Date: Thu, 27 Jan 2005 15:35:07 -0000
Subject: [R] weighting in nls
Message-ID: <3589BC4D64C84341AE0C258244F977A2B4D1D3@expressa.corp.cefas.co.uk>


Hi there,

this is the output from R

> solb2wvb<-nls(~sqrt(novervar)*(weight-(a*(1-exp(-b*(age-c))))^3),data=solb2.na.rm,start=list(a=0.85,b=0.45,c=0.48))
> summary(solb2wvb)

Formula:  ~ sqrt(novervar) * (weight - (a * (1 - exp( - b * (age - c))))^3)

Parameters:
      Value Std. Error  t value 
a  1.087370 0.01193090  91.1392
b  0.151838 0.00714963  21.2372
c -1.809770 0.13186000 -13.7250

Residual standard error: 4.41368 on 109 degrees of freedom

The output from Prism is:

von Bertalanffy	
Best-fit values	
     A	0.8957
     B	0.2381
     C	-1.358
Std. Error	
     A	0.002280
     B	0.002568
     C	0.02919
95% Confidence Intervals	
     A	0.8912 to 0.9001
     B	0.2331 to 0.2431
     C	-1.415 to -1.300

The latter has much better visual fit and reasonable residuals. Furthermore theory and practice both lead to the expectation that this model should fit the data.

Incidentally, I was under the impression that with a weighted nls in R the SE values were not accurate.

Finally I've attached the dataset




-----Original Message-----
From: Liaw, Andy [mailto:andy_liaw at merck.com]
Sent: 27 January 2005 15:25
To: Robert Brown FM CEFAS; r-help at stat.math.ethz.ch
Subject: RE: [R] weighting in nls


Can you show us the difference; i.e., what are the parameter estimates and
associated SEs from the two programs?  Even better, can you supply an
example data set?

[With is `trick' for weighted nls, you need to be careful with the output of
predict().]

Andy

> From: Robert Brown FM CEFAS
> 
> I'm fitting nonlinear functions to some growth data but  I'm 
> getting radically different results in R to another program 
> (Prism). Furthermore the values from the other program give a 
> better fit and seem more realistic.  I think there is a 
> problem with the results from the r nls function. The 
> differences only occur with weighted data so I think I'm 
> making a mistake in the weighting. I'm following the 
> procedure outlined on p 244 of MASS (or at least I'm trying to).
> 
> Thus, I'm using mean data with heteroscedasticity so I'm 
> weighting by n/ variance, where the variance is well known 
> from a large data set. This weighting factor is available as 
> the variable 'novervar'.
> 
> The function is a von Bertalanffy curve of the form 
> weight~(a*(1-exp(-b*(age-c))))^3.  Thus I'm entering the 
> command in the form:
> 
> solb1wvb<-nls(~sqrt(novervar)*(weight-(a*(1-exp(-b*(age-c))))^
> 3),data=solb1.na.rm,start=list(a=0.85,b=0.45,c=0.48))
> 
> Can anyone suggest what I'm doing wrong?  I seem to be 
> folowing the instructions in MASS. I tried following the 
> similar instructions on page 450 of the white book but these 
> were a bit cryptic.
> 
> I'm using R 2.0.0 on a Windows 2000 machine
> 
> Regards,
> 
> Robert Brown
> 
> 
> **************************************************************
> *********************
> This email and any attachments are intended for the named 
> re...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
------------------------------------------------------------------------------


***********************************************************************************
This email and any attachments are intended for the named recipient only.  Its unauthorised use, distribution, disclosure, storage or copying is not permitted.  If you have received it in error, please destroy all copies and notify the sender.  In messages of a non-business nature, the views and opinions expressed are the author's own and do not necessarily reflect those of the organisation from which it is sent.  All emails may be subject to monitoring.
***********************************************************************************


From reid_huntsinger at merck.com  Thu Jan 27 16:49:31 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Thu, 27 Jan 2005 10:49:31 -0500
Subject: [R] Request for help
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A928E@uswpmx00.merck.com>

Persi Diaconis and Bernd Sturmfels have an article on generating random
contingency tables uniformly distributed subject to having fixed marginals
for the same purpose (null distribution of conditional test) and they used
Markov Chain Monte Carlo to sample. That could perhaps be adapted here. The
article is in Annals of Statistics from several years ago, and if you google
for "algebraic statistics" you'll probably find several recent expositions
of the ideas, possibly even code.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Michela Marignani
Sent: Thursday, January 27, 2005 3:52 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Request for help


My name is Michela Marignani and I'm an ecologist trying to solve a problem 
linked to knight' s tour algorithm.
I need a program to create random matrices with presence/absence (i.e. 1,0 
values), with defined colums and rows sums, to create null models for 
statistical comparison of species distribution phenomena.
I've seen on the web many solutions of the problem, but none provides the 
freedom to easily change row+colums constraint and none of them produce 
matrices  with 1 and 0. Also, I've tryied to use R, but it is too 
complicated for a not-statistician as I am....can you help me?

Thank you for your attention,
so long

Michela Marignani
University of Siena
Environmental Science Dept.
Siena, Italy
michela.marignani at uniroma1.it

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Robert.McGehee at geodecapital.com  Thu Jan 27 17:29:54 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Thu, 27 Jan 2005 11:29:54 -0500
Subject: [R] Indexing Lists and Partial Matching
Message-ID: <67DCA285A2D7754280D3B8E88EB5480206741E41@MSGBOSCLB2WIN.DMN1.FMR.COM>

Thank you both for your reference. I had missed the previous discussions
before posting.

I am surprised to hear that there is code that relies on this indexing
behavior, especially if it is in the base package. I'm not sure how a
function could even make use of this feature without first asking R what
the names of the list or data frame are, and then intentionally
shortening them to something else. It even seems reasonable that if code
_does_ rely on this behavior, then it may be subject to other problems
anyway, such as if the wrong data is unintentionally returned (when NULL
or error should be returned instead). (Although I freely acknowledge my
ignorance of the uses of this feature as I only recently discovered it.)

>From the previous posts, it seems the only way in R to code around this
is to _always_ check the names of a list before indexing, as anything
else could lead to very subtle errors in complex code, unless one can a
priori guarantee that the list names are always distinguishable. 

Perhaps one easy way to optionally remove this feature without breaking
anything would be to have an option/flag in the description or namespace
of a package indicating that list-indexing partial-matching should not
be used for any function within that package. But that might be a bit
hackish.

However, for my personal code, the a[[match("abc", names(a))]] construct
(from one of the Nov 18th posts) is easy enough to use, so no intention
to rehash an already well-discussed topic.

Thanks,
Robert

PS. None of this applies to partial matching of function arguments, as
this is certainly widely used.

-----Original Message-----
From: Huntsinger, Reid [mailto:reid_huntsinger at merck.com] 
Sent: Thursday, January 27, 2005 10:15 AM
To: 'McGehee, Robert'; r-help at stat.math.ethz.ch
Subject: RE: [R] Indexing Lists and Partial Matching


This came up a few months ago. Check the thread on hashing and partial
matching around Nov 18. The short answer is no, you can't turn it off
because lots of code relies on that behavior. 

Reid Huntsinger 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of McGehee, Robert
Sent: Thursday, January 27, 2005 9:34 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Indexing Lists and Partial Matching


I was unaware until recently that partial matching was used to index
data frames and lists. This is now causing a great deal of problems in
my code as I sometimes index a list without knowing what elements it
contains, expecting a NULL if the column does not exist. However, if
partial matching is used, sometimes R will return an object I do not
want. My question, is there an easy way of getting around this?

For example:
> a <- NULL
> a$abc <- 5
> a$a
[1] 5
> a$a <- a$a
> a
$abc
[1] 5
$a
[1] 5

Certainly from a coding prospective, one might expect assigning a$a to
itself wouldn't do anything since either 1) a$a doesn't exist, so
nothing happens, or 2) a$a does exist and so it just assigns its value
to itself. However, in the above case, it creates a new column entirely
because I happen to have another column called a$abc. I do not want this
behavior.

The solution I came up with was to create another indexing function that
uses the subset() (which doesn't partial match), then check for an
error, and if there is an error substitute NULL (to mimic the "["
behavior). However, I don't really want to start using another indexing
function altogether just to get around this behavior. Is there a better
way? Can I turn off partial matching?

Thanks,
Robert


Robert McGehee
Geode Capital Management, LLC
53 State Street, 5th Floor | Boston, MA | 02109
Tel: 617/392-8396    Fax:617/476-6389
mailto:robert.mcgehee at geodecapital.com



This e-mail, and any attachments hereto, are intended for
us...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



------------------------------------------------------------------------
------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From vashok at gmail.com  Thu Jan 27 17:48:08 2005
From: vashok at gmail.com (Ashok Veeraraghavan)
Date: Thu, 27 Jan 2005 11:48:08 -0500
Subject: [R] Installing Problems
Message-ID: <ac0fd70d05012708486c7e52b8@mail.gmail.com>

Hi,

I tried installing R on my MAC OS 10.3. After R installation I tried
installing BioConductor which requires R. I ran into some problems
with Bioconductor. Right now I want to remove (uninstall) all R and
Bioconductor components from my machine and start afresh. Can somebody
tell me how i can remove(uninstall) all R and Bioconductor components.

Thanks
Regards

Ashok



From spencer.graves at pdf.com  Thu Jan 27 18:03:19 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 27 Jan 2005 09:03:19 -0800
Subject: [R] sw
In-Reply-To: <16888.60708.364548.615599@stat.math.ethz.ch>
References: <32727.1106781916@www58.gmx.net>
	<16888.60708.364548.615599@stat.math.ethz.ch>
Message-ID: <41F91ED7.2030901@pdf.com>

also stepAIC in library(MASS).  hope this helps.  spencer graves

Christoph Buser wrote:

>Dear Mahdi
>
>Mahdi Osman writes:
> > Hi list,
> > 
> > I am just a new user of R.
> > 
> > How can I run stepwise regression in R?
>
>If you look for a stepwise procedure for Linear Regression
>Models or Generalized Linear Models, you can use step() 
>(see ?step) 
>
>Regards,
>
>Christoph
>
>--------------------------------------------------------------
>Christoph Buser <buser at stat.math.ethz.ch>
>Seminar fuer Statistik, LEO C11
>ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
>phone: x-41-1-632-5414		fax: 632-1228
>http://stat.ethz.ch/~buser/
>--------------------------------------------------------------
>
>
>
> > Is there a graphic user interphase for any of the spatial packages inculded
> > in R, such as gstat, geoR and someothers. I am mainly interested interactive
> > variogram modelling and mapping.
> > 
> > Thanks
> > Mahdi
> > 
> > -- 
> > -----------------------------------
> > Mahdi Osman (PhD)
> > E-mail: m_osm at gmx.net
> > -----------------------------------
> > 
> > 10 GB Mailbox, 100 FreeSMS http://www.gmx.net/de/go/topmail
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From thewavyx at gmail.com  Thu Jan 27 18:03:27 2005
From: thewavyx at gmail.com (Eric Rodriguez)
Date: Thu, 27 Jan 2005 18:03:27 +0100
Subject: [R] How to generate labels or names?
Message-ID: <47779aab0501270903ed12d56@mail.gmail.com>

Hi,

I'm new to R and I would like to generate labels like data.frame does
: "V1 V2 V3...".
I'm trying to generate a N vector with label such as "Lab1 Lab2 ... LabN".

I guess this is pretty easy when you know R ;)

Thanks for help

Eric



From tongtong at ucla.edu  Thu Jan 27 18:25:22 2005
From: tongtong at ucla.edu (WU,TONGTONG)
Date: Thu, 27 Jan 2005 09:25:22 -0800
Subject: [R] svd error
Message-ID: <1106846722.41f924028bb77@mail.ucla.edu>

Hi,

  I met a probem recently and need your help.  I would really appreciate
it.

  I kept receiving the following error message when running a program:

'Error in svd(X) : infinite or missing values in x'.

However, I did not use any svd function in this program though I did
include the function pseudoinverse.  Is the problem caused by doing
pseudoinverse?

Best regards,
Tongtong



From spencer.graves at pdf.com  Thu Jan 27 18:26:09 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 27 Jan 2005 09:26:09 -0800
Subject: [R] How to generate labels or names?
In-Reply-To: <47779aab0501270903ed12d56@mail.gmail.com>
References: <47779aab0501270903ed12d56@mail.gmail.com>
Message-ID: <41F92431.9090005@pdf.com>

?paste

One of its examples is
 paste("A", 1:6, sep = "")
[1] "A1" "A2" "A3" "A4" "A5" "A6"

spencer graves

Eric Rodriguez wrote:

>Hi,
>
>I'm new to R and I would like to generate labels like data.frame does
>: "V1 V2 V3...".
>I'm trying to generate a N vector with label such as "Lab1 Lab2 ... LabN".
>
>I guess this is pretty easy when you know R ;)
>
>Thanks for help
>
>Eric
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From andy_liaw at merck.com  Thu Jan 27 18:25:50 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 27 Jan 2005 12:25:50 -0500
Subject: [R] weighting in nls
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5CC@usrymx25.merck.com>

There seems to be some peculiarity with the weights.  If you try the
unweighted fit, it comes much closer to the answer from Prism...

Andy

> From: Robert Brown FM CEFAS 
> 
> Hi there,
> 
> this is the output from R
> 
> > 
> solb2wvb<-nls(~sqrt(novervar)*(weight-(a*(1-exp(-b*(age-c))))^
> 3),data=solb2.na.rm,start=list(a=0.85,b=0.45,c=0.48))
> > summary(solb2wvb)
> 
> Formula:  ~ sqrt(novervar) * (weight - (a * (1 - exp( - b * 
> (age - c))))^3)
> 
> Parameters:
>       Value Std. Error  t value 
> a  1.087370 0.01193090  91.1392
> b  0.151838 0.00714963  21.2372
> c -1.809770 0.13186000 -13.7250
> 
> Residual standard error: 4.41368 on 109 degrees of freedom
> 
> The output from Prism is:
> 
> von Bertalanffy	
> Best-fit values	
>      A	0.8957
>      B	0.2381
>      C	-1.358
> Std. Error	
>      A	0.002280
>      B	0.002568
>      C	0.02919
> 95% Confidence Intervals	
>      A	0.8912 to 0.9001
>      B	0.2331 to 0.2431
>      C	-1.415 to -1.300
> 
> The latter has much better visual fit and reasonable 
> residuals. Furthermore theory and practice both lead to the 
> expectation that this model should fit the data.
> 
> Incidentally, I was under the impression that with a weighted 
> nls in R the SE values were not accurate.
> 
> Finally I've attached the dataset
> 
> 
> 
> 
> -----Original Message-----
> From: Liaw, Andy [mailto:andy_liaw at merck.com]
> Sent: 27 January 2005 15:25
> To: Robert Brown FM CEFAS; r-help at stat.math.ethz.ch
> Subject: RE: [R] weighting in nls
> 
> 
> Can you show us the difference; i.e., what are the parameter 
> estimates and
> associated SEs from the two programs?  Even better, can you supply an
> example data set?
> 
> [With is `trick' for weighted nls, you need to be careful 
> with the output of
> predict().]
> 
> Andy
> 
> > From: Robert Brown FM CEFAS
> > 
> > I'm fitting nonlinear functions to some growth data but  I'm 
> > getting radically different results in R to another program 
> > (Prism). Furthermore the values from the other program give a 
> > better fit and seem more realistic.  I think there is a 
> > problem with the results from the r nls function. The 
> > differences only occur with weighted data so I think I'm 
> > making a mistake in the weighting. I'm following the 
> > procedure outlined on p 244 of MASS (or at least I'm trying to).
> > 
> > Thus, I'm using mean data with heteroscedasticity so I'm 
> > weighting by n/ variance, where the variance is well known 
> > from a large data set. This weighting factor is available as 
> > the variable 'novervar'.
> > 
> > The function is a von Bertalanffy curve of the form 
> > weight~(a*(1-exp(-b*(age-c))))^3.  Thus I'm entering the 
> > command in the form:
> > 
> > solb1wvb<-nls(~sqrt(novervar)*(weight-(a*(1-exp(-b*(age-c))))^
> > 3),data=solb1.na.rm,start=list(a=0.85,b=0.45,c=0.48))
> > 
> > Can anyone suggest what I'm doing wrong?  I seem to be 
> > folowing the instructions in MASS. I tried following the 
> > similar instructions on page 450 of the white book but these 
> > were a bit cryptic.
> > 
> > I'm using R 2.0.0 on a Windows 2000 machine
> > 
> > Regards,
> > 
> > Robert Brown
> > 
> > 
> > **************************************************************
> > *********************
> > This email and any attachments are intended for the named 
> > re...{{dropped}}
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any attachments, 
> contains information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New Jersey, USA 08889), and/or its 
> affiliates (which may be known outside the United States as 
> Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
> Banyu) that may be confidential, proprietary copyrighted 
> and/or legally privileged. It is intended solely for the use 
> of the individual or entity named on this message.  If you 
> are not the intended recipient, and have received this 
> message in error, please notify us immediately by reply 
> e-mail and then delete it from your system.
> --------------------------------------------------------------
> ----------------
> 
> 
> **************************************************************
> *********************
> This email and any attachments are intended for the named 
> recipient only.  Its unauthorised use, distribution, 
> disclosure, storage or copying is not permitted.  If you have 
> received it in error, please destroy all copies and notify 
> the sender.  In messages of a non-business nature, the views 
> and opinions expressed are the author's own and do not 
> necessarily reflect those of the organisation from which it 
> is sent.  All emails may be subject to monitoring.
> **************************************************************
> *********************
> 
>



From reid_huntsinger at merck.com  Thu Jan 27 18:28:40 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Thu, 27 Jan 2005 12:28:40 -0500
Subject: [R] Request for help (reference details)
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9290@uswpmx00.merck.com>

I referred in my reply to a paper by Diaconis and Sturmfels. The exact
reference is:

Diaconis and Sturmfels, Algebraic algorithms for sampling from conditional
distributions, Ann. Stat 26 (1998) 363-397. 

They cite the following:

Besag and Clifford, Generalized Monte Carlo significance tests, Biometrika
76 (1989) 633-42.

which actually contains your problem (section 3, Testing the Rasch model)
and gives a very simple Markov chain for sampling from the uniform
distribution on these matrices. If you need other than the uniform
distribution, see the modifications Diaconis and Sturmfels make (the
"Metropolis" step).

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Huntsinger, Reid
Sent: Thursday, January 27, 2005 10:50 AM
To: 'Michela Marignani'; r-help at stat.math.ethz.ch
Subject: RE: [R] Request for help


Persi Diaconis and Bernd Sturmfels have an article on generating random
contingency tables uniformly distributed subject to having fixed marginals
for the same purpose (null distribution of conditional test) and they used
Markov Chain Monte Carlo to sample. That could perhaps be adapted here. The
article is in Annals of Statistics from several years ago, and if you google
for "algebraic statistics" you'll probably find several recent expositions
of the ideas, possibly even code.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Michela Marignani
Sent: Thursday, January 27, 2005 3:52 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Request for help


My name is Michela Marignani and I'm an ecologist trying to solve a problem 
linked to knight' s tour algorithm.
I need a program to create random matrices with presence/absence (i.e. 1,0 
values), with defined colums and rows sums, to create null models for 
statistical comparison of species distribution phenomena.
I've seen on the web many solutions of the problem, but none provides the 
freedom to easily change row+colums constraint and none of them produce 
matrices  with 1 and 0. Also, I've tryied to use R, but it is too 
complicated for a not-statistician as I am....can you help me?

Thank you for your attention,
so long

Michela Marignani
University of Siena
Environmental Science Dept.
Siena, Italy
michela.marignani at uniroma1.it

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

----------------------------------------------------------------------------
--
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From buser at stat.math.ethz.ch  Thu Jan 27 18:32:18 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Thu, 27 Jan 2005 18:32:18 +0100
Subject: [R] How to generate labels or names?
In-Reply-To: <47779aab0501270903ed12d56@mail.gmail.com>
References: <47779aab0501270903ed12d56@mail.gmail.com>
Message-ID: <16889.9634.96755.67432@stat.math.ethz.ch>

Hi Eric

If you want produce a vector with names, you can use

v <- rnorm(20)
names(v) <- paste("Lab",1:20, sep="")

Regards,

Christoph

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C11
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-1-632-5414		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


Eric Rodriguez writes:
 > Hi,
 > 
 > I'm new to R and I would like to generate labels like data.frame does
 > : "V1 V2 V3...".
 > I'm trying to generate a N vector with label such as "Lab1 Lab2 ... LabN".
 > 
 > I guess this is pretty easy when you know R ;)
 > 
 > Thanks for help
 > 
 > Eric
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Jan 27 18:46:13 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 27 Jan 2005 17:46:13 +0000 (GMT)
Subject: [R] svd error
In-Reply-To: <1106846722.41f924028bb77@mail.ucla.edu>
References: <1106846722.41f924028bb77@mail.ucla.edu>
Message-ID: <Pine.LNX.4.61.0501271740460.6660@gannet.stats>

On Thu, 27 Jan 2005, WU,TONGTONG wrote:

> Hi,
>
>  I met a probem recently and need your help.  I would really appreciate
> it.
>
>  I kept receiving the following error message when running a program:
>
> 'Error in svd(X) : infinite or missing values in x'.
>
> However, I did not use any svd function in this program though I did
> include the function pseudoinverse.  Is the problem caused by doing
> pseudoinverse?

Where did you find that function?  It is not part of R as it ships, and it 
*may* be part of GeneTS, where it calls svd after squaring the matrix. 
But there are simpler pseudoinverse functions (e.g. ginv in MASS) that 
will not introduce that error.

The tool you needed was traceback(): try it to see what it tells you here.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Thu Jan 27 18:47:40 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 27 Jan 2005 09:47:40 -0800
Subject: [R] svd error
In-Reply-To: <1106846722.41f924028bb77@mail.ucla.edu>
References: <1106846722.41f924028bb77@mail.ucla.edu>
Message-ID: <41F9293C.5070309@pdf.com>

      You haven't told us what you used to compute the pseudoinverse, 
but I can get that error message using ginv in library(MASS).  When I 
then typed "ginv" (without the parentheses, it listed the code, and I 
quickly saw "Xsvd <- svd(X)" [using R 2.0.1 under Windows 2000]. 

      hope this helps. 
      spencer graves
p.s.  The posting guide (www.R-project.org/posting-guide.html) can help 
you find answers to many questions like this yourself, in addition to 
improving your facility with language AND improving, I believe, your 
chances of getting a reply that actually answers your question.  In this 
case, if you are not using "ginv" in library(MASS) and the discussion 
above doesn't help you solve the problem otherwise, following the 
posting guide would have made it much easier for someone like me to 
provide a more useful answer. 

WU,TONGTONG wrote:

>Hi,
>
>  I met a probem recently and need your help.  I would really appreciate
>it.
>
>  I kept receiving the following error message when running a program:
>
>'Error in svd(X) : infinite or missing values in x'.
>
>However, I did not use any svd function in this program though I did
>include the function pseudoinverse.  Is the problem caused by doing
>pseudoinverse?
>
>Best regards,
>Tongtong
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From gunter.berton at gene.com  Thu Jan 27 18:52:30 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 27 Jan 2005 09:52:30 -0800
Subject: [R] weighting in nls
In-Reply-To: <3589BC4D64C84341AE0C258244F977A2B4D1D3@expressa.corp.cefas.co.uk>
Message-ID: <200501271752.j0RHqUQL011722@ohm.gene.com>

Robert:

So far as I can see:

1. Your specification is correct, (note,however, that weights of the form
sqrt(n) are all that you need, as a constant multiplier makes no
difference). However, as you have not specified a gradient attribute,
numerical derivatives are being used by nls() while PRISM may be using
analytical derivatives if your model is one of those on its standard list,
which I suspect is the case. As a result, the particular data and weights
that you have may be causing the nls algorithm to terminate too early, as
numerical derivatives (differences) often require more iterations. So I
suggest you try increasing the maxiter parameter of the control list, say by
control = nls.control(maxiter=100). 

2. Also note that for a fixed b and c, the model is linear in a^3. So this
would allow use of the "plinear" algorithm, which might considerably improve
convergence behavior (as there are then only 2, not 3, nonlinear
parameters). V&R's MASS section on nls() again contains a good explanation
of how to do this.

That the algorithm worked better with unweighted than weighted data means
little, as you are at the mercy of the particulars of the interaction
between optimization algorithm and likelihood surface ... that is, the
particular data including weighting. Also, you are correct afaik: the
standard errors and confidence intervals from nls are incorrect for weighted
fits, though of course they usually aren't that meaningful for unweighted
fits either (I don't think they are even necessarily asymptotically correct,
as they are merely based on the local likehood surface at the converged
value, which does not in any sense take into account the uncertainty of the
fitting algorithm -- however, others may (eagerly) point out the errors of
my ways, here).

As Always, standard discalimers apply, especially the one about my not being
an expert on this... so caveat emptor!

Cheers,

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robert 
> Brown FM CEFAS
> Sent: Thursday, January 27, 2005 7:35 AM
> To: Liaw, Andy
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] weighting in nls
> 
> 
> Hi there,
> 
> this is the output from R
> 
> > 
> solb2wvb<-nls(~sqrt(novervar)*(weight-(a*(1-exp(-b*(age-c))))^
> 3),data=solb2.na.rm,start=list(a=0.85,b=0.45,c=0.48))
> > summary(solb2wvb)
> 
> Formula:  ~ sqrt(novervar) * (weight - (a * (1 - exp( - b * 
> (age - c))))^3)
> 
> Parameters:
>       Value Std. Error  t value 
> a  1.087370 0.01193090  91.1392
> b  0.151838 0.00714963  21.2372
> c -1.809770 0.13186000 -13.7250
> 
> Residual standard error: 4.41368 on 109 degrees of freedom
> 
> The output from Prism is:
> 
> von Bertalanffy	
> Best-fit values	
>      A	0.8957
>      B	0.2381
>      C	-1.358
> Std. Error	
>      A	0.002280
>      B	0.002568
>      C	0.02919
> 95% Confidence Intervals	
>      A	0.8912 to 0.9001
>      B	0.2331 to 0.2431
>      C	-1.415 to -1.300
> 
> The latter has much better visual fit and reasonable 
> residuals. Furthermore theory and practice both lead to the 
> expectation that this model should fit the data.
> 
> Incidentally, I was under the impression that with a weighted 
> nls in R the SE values were not accurate.
> 
> Finally I've attached the dataset
> 
> 
> 
> 
> -----Original Message-----
> From: Liaw, Andy [mailto:andy_liaw at merck.com]
> Sent: 27 January 2005 15:25
> To: Robert Brown FM CEFAS; r-help at stat.math.ethz.ch
> Subject: RE: [R] weighting in nls
> 
> 
> Can you show us the difference; i.e., what are the parameter 
> estimates and
> associated SEs from the two programs?  Even better, can you supply an
> example data set?
> 
> [With is `trick' for weighted nls, you need to be careful 
> with the output of
> predict().]
> 
> Andy
> 
> > From: Robert Brown FM CEFAS
> > 
> > I'm fitting nonlinear functions to some growth data but  I'm 
> > getting radically different results in R to another program 
> > (Prism). Furthermore the values from the other program give a 
> > better fit and seem more realistic.  I think there is a 
> > problem with the results from the r nls function. The 
> > differences only occur with weighted data so I think I'm 
> > making a mistake in the weighting. I'm following the 
> > procedure outlined on p 244 of MASS (or at least I'm trying to).
> > 
> > Thus, I'm using mean data with heteroscedasticity so I'm 
> > weighting by n/ variance, where the variance is well known 
> > from a large data set. This weighting factor is available as 
> > the variable 'novervar'.
> > 
> > The function is a von Bertalanffy curve of the form 
> > weight~(a*(1-exp(-b*(age-c))))^3.  Thus I'm entering the 
> > command in the form:
> > 
> > solb1wvb<-nls(~sqrt(novervar)*(weight-(a*(1-exp(-b*(age-c))))^
> > 3),data=solb1.na.rm,start=list(a=0.85,b=0.45,c=0.48))
> > 
> > Can anyone suggest what I'm doing wrong?  I seem to be 
> > folowing the instructions in MASS. I tried following the 
> > similar instructions on page 450 of the white book but these 
> > were a bit cryptic.
> > 
> > I'm using R 2.0.0 on a Windows 2000 machine
> > 
> > Regards,
> > 
> > Robert Brown
> > 
> > 
> > **************************************************************
> > *********************
> > This email and any attachments are intended for the named 
> > re...{{dropped}}
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any attachments, 
> contains information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New Jersey, USA 08889), and/or its 
> affiliates (which may be known outside the United States as 
> Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
> Banyu) that may be confidential, proprietary copyrighted 
> and/or legally privileged. It is intended solely for the use 
> of the individual or entity named on this message.  If you 
> are not the intended recipient, and have received this 
> message in error, please notify us immediately by reply 
> e-mail and then delete it from your system.
> --------------------------------------------------------------
> ----------------
> 
> 
> **************************************************************
> *********************
> This email and any attachments are intended for the named 
> recipient only.  Its unauthorised use, distribution, 
> disclosure, storage or copying is not permitted.  If you have 
> received it in error, please destroy all copies and notify 
> the sender.  In messages of a non-business nature, the views 
> and opinions expressed are the author's own and do not 
> necessarily reflect those of the organisation from which it 
> is sent.  All emails may be subject to monitoring.
> **************************************************************
> *********************
> 
>



From sdavis2 at mail.nih.gov  Thu Jan 27 18:57:43 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 27 Jan 2005 12:57:43 -0500
Subject: [R] How to generate labels or names?
In-Reply-To: <41F92431.9090005@pdf.com>
References: <47779aab0501270903ed12d56@mail.gmail.com>
	<41F92431.9090005@pdf.com>
Message-ID: <F1615EE6-708C-11D9-8FBF-000D933565E8@mail.nih.gov>

Note that sometimes it makes more sense to use a list than a labeled 
vector.

Sean

On Jan 27, 2005, at 12:26 PM, Spencer Graves wrote:

> ?paste
>
> One of its examples is
> paste("A", 1:6, sep = "")
> [1] "A1" "A2" "A3" "A4" "A5" "A6"
>
> spencer graves
>
> Eric Rodriguez wrote:
>
>> Hi,
>>
>> I'm new to R and I would like to generate labels like data.frame does
>> : "V1 V2 V3...".
>> I'm trying to generate a N vector with label such as "Lab1 Lab2 ... 
>> LabN".
>>
>> I guess this is pretty easy when you know R ;)
>>
>> Thanks for help
>>
>> Eric
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Thu Jan 27 19:23:30 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 27 Jan 2005 10:23:30 -0800
Subject: [R] svd error
In-Reply-To: <Pine.LNX.4.61.0501271740460.6660@gannet.stats>
References: <1106846722.41f924028bb77@mail.ucla.edu>
	<Pine.LNX.4.61.0501271740460.6660@gannet.stats>
Message-ID: <41F931A2.3010301@pdf.com>

Dear Prof. Ripley: 

      With library(MASS), I got the following in R 2.0.1 under Windows 
2000: 

 > X
     [,1] [,2]
[1,]    1    3
[2,]    2   NA
 > ginv(X)
Error in svd(X) : infinite or missing values in x

      This may not relate to Tongtong Wu's problem, but it used "ginv" 
in library(MASS) as you suggested and did produce the cited error message. 

      spencer graves

Prof Brian Ripley wrote:

> On Thu, 27 Jan 2005, WU,TONGTONG wrote:
>
>> Hi,
>>
>>  I met a probem recently and need your help.  I would really appreciate
>> it.
>>
>>  I kept receiving the following error message when running a program:
>>
>> 'Error in svd(X) : infinite or missing values in x'.
>>
>> However, I did not use any svd function in this program though I did
>> include the function pseudoinverse.  Is the problem caused by doing
>> pseudoinverse?
>
>
> Where did you find that function?  It is not part of R as it ships, 
> and it *may* be part of GeneTS, where it calls svd after squaring the 
> matrix. But there are simpler pseudoinverse functions (e.g. ginv in 
> MASS) that will not introduce that error.
>
> The tool you needed was traceback(): try it to see what it tells you 
> here.
>



From vashok at gmail.com  Thu Jan 27 19:30:59 2005
From: vashok at gmail.com (Ashok Veeraraghavan)
Date: Thu, 27 Jan 2005 13:30:59 -0500
Subject: [R] Help with R and Bioconductor
Message-ID: <ac0fd70d05012710304fa8ee02@mail.gmail.com>

Hi,

I am new to using R and Bioconductor. My first attempt at installing R
seemed successful. Then while attempting to getBioC() I had to force
quit the R application since I had to attend to something else
urgently. When i returned and tried to getBioC, I am getting errors
indicating that there is a lock on some files. So i would like to
uninstall/remove all R components and Bioconductor components and then
start afresh again. Can somebody tell me how i can remove/uninstall
all R and Bioconductor components?

Thanks for the time and effort involved in answering my question.

Regards
Ashok



From ripley at stats.ox.ac.uk  Thu Jan 27 19:39:11 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 27 Jan 2005 18:39:11 +0000 (GMT)
Subject: [R] svd error
In-Reply-To: <41F931A2.3010301@pdf.com>
References: <1106846722.41f924028bb77@mail.ucla.edu>
	<Pine.LNX.4.61.0501271740460.6660@gannet.stats>
	<41F931A2.3010301@pdf.com>
Message-ID: <Pine.LNX.4.61.0501271836550.4049@gannet.stats>

On Thu, 27 Jan 2005, Spencer Graves wrote:

> Dear Prof. Ripley: 
>     With library(MASS), I got the following in R 2.0.1 under Windows 2000: 
>> X
>    [,1] [,2]
> [1,]    1    3
> [2,]    2   NA
>> ginv(X)
> Error in svd(X) : infinite or missing values in x
>
>     This may not relate to Tongtong Wu's problem, but it used "ginv" in 
> library(MASS) as you suggested and did produce the cited error message.

I said `introduce'.  The cause of the error is in X, not introduced by 
ginv. pseudoinverse can introduce NaNs/infinities.

Please do remember the care I take when writing things.

BDR


>     spencer graves
>
> Prof Brian Ripley wrote:
>
>> On Thu, 27 Jan 2005, WU,TONGTONG wrote:
>> 
>>> Hi,
>>> 
>>>  I met a probem recently and need your help.  I would really appreciate
>>> it.
>>> 
>>>  I kept receiving the following error message when running a program:
>>> 
>>> 'Error in svd(X) : infinite or missing values in x'.
>>> 
>>> However, I did not use any svd function in this program though I did
>>> include the function pseudoinverse.  Is the problem caused by doing
>>> pseudoinverse?
>> 
>> 
>> Where did you find that function?  It is not part of R as it ships, and it 
>> *may* be part of GeneTS, where it calls svd after squaring the matrix. But 
>> there are simpler pseudoinverse functions (e.g. ginv in MASS) that will not 
>> introduce that error.
>> 
>> The tool you needed was traceback(): try it to see what it tells you here.
>> 
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jgentry at jimmy.harvard.edu  Thu Jan 27 19:41:45 2005
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Thu, 27 Jan 2005 13:41:45 -0500 (EST)
Subject: [R] Help with R and Bioconductor
In-Reply-To: <ac0fd70d05012710304fa8ee02@mail.gmail.com>
Message-ID: <Pine.SOL.4.20.0501271340360.16228-100000@santiam.dfci.harvard.edu>

> seemed successful. Then while attempting to getBioC() I had to force
> quit the R application since I had to attend to something else
> urgently. When i returned and tried to getBioC, I am getting errors

Why not just let it run?

> indicating that there is a lock on some files. So i would like to

The directory will likely be <path-to-R>/library/00LOCK (I say likely
because the '<path-to-R>/library' part could be something else if you
specified an alternate installation directory or your default .libPaths is
different then standard), and removing that directory will solve your
issues.



From cdsmith at ksu.edu  Thu Jan 27 20:30:36 2005
From: cdsmith at ksu.edu (cdsmith@ksu.edu)
Date: Thu, 27 Jan 2005 13:30:36 -0600
Subject: [R] Array Manipulation
Message-ID: <1106854236.41f9415c6cd9b@webmail.ksu.edu>

I have a data set that looks like the following:
ID  Responce
1   57
1   63
1   49
2   31
2   45
2   67
2   91
3   56
3   43
4   23
4   51
4   61
4   76
4   68
5   34
5   35
5   45
I used sample(unique(ID)) to select a sample if ID's, say, (1,4,5).  Now
I want to pull out the rows with ID's 1, 4, and 5.  I've tried forceing
the matrix into a vector but it does not create and appropriate vector.
 I've also tried the if statment but it didn't work right either.  Any
suggestions?



From andy_liaw at merck.com  Thu Jan 27 20:37:02 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 27 Jan 2005 14:37:02 -0500
Subject: [R] Array Manipulation
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5D2@usrymx25.merck.com>

Something like:

dat[dat$ID %in% sample(unique(dat$ID), 3), ]

Andy

> From: cdsmith at ksu.edu
> 
> I have a data set that looks like the following:
> ID  Responce
> 1   57
> 1   63
> 1   49
> 2   31
> 2   45
> 2   67
> 2   91
> 3   56
> 3   43
> 4   23
> 4   51
> 4   61
> 4   76
> 4   68
> 5   34
> 5   35
> 5   45
> I used sample(unique(ID)) to select a sample if ID's, say, 
> (1,4,5).  Now
> I want to pull out the rows with ID's 1, 4, and 5.  I've 
> tried forceing
> the matrix into a vector but it does not create and 
> appropriate vector.
>  I've also tried the if statment but it didn't work right either.  Any
> suggestions?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From rwilson+ at pitt.edu  Thu Jan 27 20:54:06 2005
From: rwilson+ at pitt.edu (roy wilson)
Date: Thu, 27 Jan 2005 14:54:06 -0500
Subject: [R] Is glm weird or am I?
Message-ID: <41F946DE.8060809@pitt.edu>

Hi,

I've written a script that checks all bivariate correlations for 
variables in a matrix. I'm now trying to run a logistic regression on 
each pair (x,y) where y is a factor with 2 levels. I don't know how (or 
whether I want) to try to fathom what's up with glm.

What I wrote is attached. Here's what I get.

*****************************************************

source("lrtest.R")
building model: Wgend ~ WAY
construct_and_run_model:
class of x: integer nlevels(x): 0
class of y: factor nlevels(y): 2
  model built
  model ran
-1.070886   0.01171153
building model: Wgend ~ WBWS
construct_and_run_model:
class of x: integer nlevels(x): 0
class of y: factor nlevels(y): 2
  model built
  model ran
0.0837854   0.01898052
building model: Wgend ~ Wcond
construct_and_run_model:
class of x: factor nlevels(x): 2
class of y: factor nlevels(y): 2
Error in "contrasts<-"(`*tmp*`, value = "contr.treatment") :
        contrasts can be applied only to factors with 2 or more levels

*********************************************************************

Both Wcond and Wgend take values in {1,2}. My understanding is that, 
when family is bonomial, GLM recodes these to {0, 1}. That's consistent 
with what I've seen previously.

Excuse the possible stupidity :-).

Roy
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: lrtest.R
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050127/8a551f7e/lrtest.pl

From bates at stat.wisc.edu  Thu Jan 27 21:18:15 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 27 Jan 2005 14:18:15 -0600
Subject: [R] Array Manipulation
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E5D2@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E5D2@usrymx25.merck.com>
Message-ID: <41F94C87.7060405@stat.wisc.edu>

Liaw, Andy wrote:
> Something like:
> 
> dat[dat$ID %in% sample(unique(dat$ID), 3), ]
> 

or

subset(dat, ID %in% sample(unique(ID), 3))

which I find to be more readable.



From p.dalgaard at biostat.ku.dk  Thu Jan 27 21:16:08 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Jan 2005 21:16:08 +0100
Subject: [R] Is glm weird or am I?
In-Reply-To: <41F946DE.8060809@pitt.edu>
References: <41F946DE.8060809@pitt.edu>
Message-ID: <x27jlyvd9z.fsf@biostat.ku.dk>

roy wilson <rwilson+ at pitt.edu> writes:

> Hi,
> 
> I've written a script that checks all bivariate correlations for
> variables in a matrix. I'm now trying to run a logistic regression on
> each pair (x,y) where y is a factor with 2 levels. I don't know how
> (or whether I want) to try to fathom what's up with glm.
> 
> What I wrote is attached. Here's what I get.

[If you want people to debug your code, you might supply the data as
well. People might be more helpful if they can actually run your code.
Remember who is asking who for a favour...]
 
> *****************************************************
> 
> source("lrtest.R")
> building model: Wgend ~ WAY
> construct_and_run_model:
> class of x: integer nlevels(x): 0
> class of y: factor nlevels(y): 2
>   model built
>   model ran
> -1.070886   0.01171153
> building model: Wgend ~ WBWS
> construct_and_run_model:
> class of x: integer nlevels(x): 0
> class of y: factor nlevels(y): 2
>   model built
>   model ran
> 0.0837854   0.01898052
> building model: Wgend ~ Wcond
> construct_and_run_model:
> class of x: factor nlevels(x): 2
> class of y: factor nlevels(y): 2
> Error in "contrasts<-"(`*tmp*`, value = "contr.treatment") :
>         contrasts can be applied only to factors with 2 or more levels
> 
> *********************************************************************
> 
> Both Wcond and Wgend take values in {1,2}. My understanding is that,
> when family is bonomial, GLM recodes these to {0, 1}. That's
> consistent with what I've seen previously.
> 
> Excuse the possible stupidity :-).

What you're seeing is similar to this:

> x <- factor(rep(0,20),levels=0:1)
> y <- rbinom(20,1,.5)
> glm(y~x,binomial)
Error in "contrasts<-"(`*tmp*`, value = "contr.treatment") :
        contrasts can be applied only to factors with 2 or more levels

I.e. x is a two-level factor, but only one level is actually present
in data.

You have

> attach(newDataSet)
> for (cond in 1:2) {
>     # Select rows for each condition
>     t <- newDataSet[Wcond == cond,] 
....

and then you proceed to use Wcond as a regressor within the data frame
t. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From HDoran at air.org  Thu Jan 27 21:38:59 2005
From: HDoran at air.org (Doran, Harold)
Date: Thu, 27 Jan 2005 15:38:59 -0500
Subject: [R] Where is MASS
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7407739620@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050127/6d29e2b5/attachment.pl

From helprhelp at gmail.com  Thu Jan 27 21:44:42 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Thu, 27 Jan 2005 15:44:42 -0500
Subject: [R] clustering
Message-ID: <cdf817830501271244fcd7338@mail.gmail.com>

Hi,
I just get a question (sorry if it is a dumb one) and I "phase" my
question in the following R codes:

group1<-rnorm(n=50, mean=0, sd=1)
group2<-rnorm(n=20, mean=1, sd=1.5)
group3<-c(group1,group2)


Now, if I am given a dataset from group3, what method (discriminant
analysis, clustering, maybe) is the best to cluster them by using R.
The known info includes: 2 clusters, normal distribution (but the
parameters are unknown).

Thanks,

Ed



From andy_liaw at merck.com  Thu Jan 27 21:51:32 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 27 Jan 2005 15:51:32 -0500
Subject: [R] Where is MASS
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5D9@usrymx25.merck.com>

It's part of the VR bundle (for a _long_ time...)

Andy

> From: Doran, Harold
> 
> Dear List:
> 
> I have been using the MASS package until 5 minutes ago. I just updated
> some packages from CRAN, something happened and R crashed. I then
> started R again and tried to source in some code that calls MASS, but
> received an error that there is not a package called MASS.
> 
> I then went to install packages from CRAN and MASS was not 
> visible as an
> option and I then went to the CRAN website and did not see MASS as one
> of the contributed packages available. I looked at the changes in the
> last two editions of R-News and didn't see anything related to MASS. I
> might be missing something obvious. 
> 
> Has something happened to this package?
> 
> Thanks,
> 
> Harold
> 
> Windows XP
> 2.0.1
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From rolf at math.unb.ca  Thu Jan 27 22:01:08 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 27 Jan 2005 17:01:08 -0400 (AST)
Subject: [R] Where is MASS
Message-ID: <200501272101.j0RL18ru018152@erdos.math.unb.ca>

You need to get the VR bundle from CRAN.  MASS has been part of the
VR bundle depuis longtemps.  Like forever.  But the packages in
the VR bundle ship with R by default, so if you've installed R
you should have those packages there automatically.

So it would seem that something got damaged in the R crash that
you spoke of.  Perhaps you should re-install R.

				cheers,

					Rolf Turner
					rolf at math.unb.ca

===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===

Harold Doran wrote:

> I have been using the MASS package until 5 minutes ago. I just updated
> some packages from CRAN, something happened and R crashed. I then
> started R again and tried to source in some code that calls MASS, but
> received an error that there is not a package called MASS.
> 
> I then went to install packages from CRAN and MASS was not visible as an
> option and I then went to the CRAN website and did not see MASS as one
> of the contributed packages available. I looked at the changes in the
> last two editions of R-News and didn't see anything related to MASS. I
> might be missing something obvious. 
> 
> Has something happened to this package?



From valentin.todorov at chello.at  Thu Jan 27 22:08:09 2005
From: valentin.todorov at chello.at (Valentin Todorov)
Date: Thu, 27 Jan 2005 22:08:09 +0100
Subject: [R] Results of MCD estimators in MASS and rrcov
Message-ID: <001601c504b4$4dbddcc0$0101a8c0@KILER>

The two implementations use different consistency factors as well as
different small sample correction factors.

1. The search parts of both implementations produce the same result -
compare rrcov.mcd$best and mass.mcd$best.

2. The raw MCD covariance matrix is corrected as follows:

MASS:
 - Rousseeuw and Leroy (1987), p.259 (eq. 1.26)
 - Marazzi (1993) (or may be Rousseeuw and van Zomeren (1900) p.638 (eq A.9)

rrcov:
 - Croux and Haesbroeck (1999), Pison et.al. p. 337
 - Pison et.al. (2002), p.338

3. The reweighted (final) covariance matrix is corrected as follows:

MASS: no correction
rrcov: Pison et.al. (2002) p. 339

This explains the different covariance matrices.
As far as the location is concerned, in this particular case the raw MCD
estimates in MASS identify one additional outlier - observation 53, which is
discarded from the computation of the reweighted estimates.
Look at the following plots and judge yourself if this is an outlier or not:

  covPlot(hbk, mcd=rrcov.mcd, which="distance", id.n=15)
  covPlot(hbk, mcd=mass.mcd, which="distance", id.n=15)

valentin



From BPikouni at CNTUS.JNJ.COM  Thu Jan 27 22:20:13 2005
From: BPikouni at CNTUS.JNJ.COM (Pikounis, Bill [CNTUS])
Date: Thu, 27 Jan 2005 16:20:13 -0500
Subject: [R] Where is MASS
Message-ID: <E5382FD31214D6118FF40002A541DECE1185E9CB@CNTUSMAEXS4.na.jnj.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050127/91b49c5a/attachment.pl

From msck9 at mizzou.edu  Thu Jan 27 22:37:16 2005
From: msck9 at mizzou.edu (msck9@mizzou.edu)
Date: Thu, 27 Jan 2005 15:37:16 -0600
Subject: [R] clustering
In-Reply-To: <cdf817830501271244fcd7338@mail.gmail.com>
References: <cdf817830501271244fcd7338@mail.gmail.com>
Message-ID: <20050127213716.GA8526@localhost>

The cluster analysis should be able to handle that. I think if you 
know how many clusters you have, "kmeans" is ok, or the EM algorithm 
can also do that. 
On Thu, Jan 27, 2005 at 03:44:42PM -0500, WeiWei Shi wrote:
> Hi,
> I just get a question (sorry if it is a dumb one) and I "phase" my
> question in the following R codes:
> 
> group1<-rnorm(n=50, mean=0, sd=1)
> group2<-rnorm(n=20, mean=1, sd=1.5)
> group3<-c(group1,group2)
> 
> 
> Now, if I am given a dataset from group3, what method (discriminant
> analysis, clustering, maybe) is the best to cluster them by using R.
> The known info includes: 2 clusters, normal distribution (but the
> parameters are unknown).
> 
> Thanks,
> 
> Ed
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sdavis2 at mail.nih.gov  Thu Jan 27 23:13:56 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 27 Jan 2005 17:13:56 -0500
Subject: [R] Finding "runs" of TRUE in binary vector
Message-ID: <BC8DDFAA-70B0-11D9-A33E-000D933565E8@mail.nih.gov>

I have a binary vector and I want to find all "regions" of that vector 
that are runs of TRUE (or FALSE).

 > a <- rnorm(10)
 > b <- a<0.5
 > b
  [1]  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE

My function would return something like a list:
region[[1]] 1,3
region[[2]] 5,5
region[[3]] 7,10

Any ideas besides looping and setting start and ends directly?

Thanks,
Sean



From helprhelp at gmail.com  Thu Jan 27 23:20:14 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Thu, 27 Jan 2005 17:20:14 -0500
Subject: [R] clustering
In-Reply-To: <20050127213716.GA8526@localhost>
References: <cdf817830501271244fcd7338@mail.gmail.com>
	<20050127213716.GA8526@localhost>
Message-ID: <cdf8178305012714206985ca31@mail.gmail.com>

Hi,
thanks for reply. In fact, I tried both of them and I also tried the
other method and I found all of them gave me different boundaries (to
my real datasets). I am thinking about k-median but hoping to get more
suggestions from all of you in this forum.

Cheers,

Ed


On Thu, 27 Jan 2005 15:37:16 -0600, msck9 at mizzou.edu <msck9 at mizzou.edu> wrote:
> The cluster analysis should be able to handle that. I think if you
> know how many clusters you have, "kmeans" is ok, or the EM algorithm
> can also do that.
> On Thu, Jan 27, 2005 at 03:44:42PM -0500, WeiWei Shi wrote:
> > Hi,
> > I just get a question (sorry if it is a dumb one) and I "phase" my
> > question in the following R codes:
> >
> > group1<-rnorm(n=50, mean=0, sd=1)
> > group2<-rnorm(n=20, mean=1, sd=1.5)
> > group3<-c(group1,group2)
> >
> >
> > Now, if I am given a dataset from group3, what method (discriminant
> > analysis, clustering, maybe) is the best to cluster them by using R.
> > The known info includes: 2 clusters, normal distribution (but the
> > parameters are unknown).
> >
> > Thanks,
> >
> > Ed
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From vograno at evafunds.com  Thu Jan 27 23:29:34 2005
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Thu, 27 Jan 2005 14:29:34 -0800
Subject: [R] Finding "runs" of TRUE in binary vector
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A584DEC6@phost015.EVAFUNDS.intermedia.net>

Untested:

c(TRUE, b[-1] != b[-length(b)]) gives you the (logical) indexes of the
beginnings of the runs
c(b[-1] != b[-length(b)], TRUE) gives you the (logical) indexes of the
ends of the runs

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sean Davis
> Sent: Thursday, January 27, 2005 2:14 PM
> To: r-help
> Subject: [R] Finding "runs" of TRUE in binary vector
> 
> I have a binary vector and I want to find all "regions" of 
> that vector that are runs of TRUE (or FALSE).
> 
>  > a <- rnorm(10)
>  > b <- a<0.5
>  > b
>   [1]  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE
> 
> My function would return something like a list:
> region[[1]] 1,3
> region[[2]] 5,5
> region[[3]] 7,10
> 
> Any ideas besides looping and setting start and ends directly?
> 
> Thanks,
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From sdavis2 at mail.nih.gov  Thu Jan 27 23:37:28 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 27 Jan 2005 17:37:28 -0500
Subject: [R] Finding "runs" of TRUE in binary vector
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A584DEC6@phost015.EVAFUNDS.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A584DEC6@phost015.EVAFUNDS.intermedia.net>
Message-ID: <0643489A-70B4-11D9-AA8B-000D933565E8@mail.nih.gov>

Thanks Patrick, Albyn, and Vadim.

rle() does what I want and, Vadim, your method gives the same results 
in a different form.  I appreciate the help!

Sean

On Jan 27, 2005, at 5:29 PM, Vadim Ogranovich wrote:

> Untested:
>
> c(TRUE, b[-1] != b[-length(b)]) gives you the (logical) indexes of the
> beginnings of the runs
> c(b[-1] != b[-length(b)], TRUE) gives you the (logical) indexes of the
> ends of the runs
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sean Davis
>> Sent: Thursday, January 27, 2005 2:14 PM
>> To: r-help
>> Subject: [R] Finding "runs" of TRUE in binary vector
>>
>> I have a binary vector and I want to find all "regions" of
>> that vector that are runs of TRUE (or FALSE).
>>
>>> a <- rnorm(10)
>>> b <- a<0.5
>>> b
>>   [1]  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE
>>
>> My function would return something like a list:
>> region[[1]] 1,3
>> region[[2]] 5,5
>> region[[3]] 7,10
>>
>> Any ideas besides looping and setting start and ends directly?
>>
>> Thanks,
>> Sean
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>



From roger.dungan at canterbury.ac.nz  Thu Jan 27 23:40:59 2005
From: roger.dungan at canterbury.ac.nz (Roger Dungan)
Date: Fri, 28 Jan 2005 11:40:59 +1300
Subject: [R] Survreg with gamma distribution
Message-ID: <7C773F4D89C83147A023704E9E78103701E352F0@cantwe.giga.canterbury.ac.nz>

Dear r-help subscribers,

I am working on some survival analysis of some interval censored failure
time data in R. I have done similar analysis before using PROC LIFEREG
in SAS. In that instance, a gamma survival function was the optimum
parametric model for describing the survival and hazard functions. I
would like to be able to use a gamma function in R, but apparently the
survival package does not support this distribution. I have been
googling around for some help, and have found some threads to a similar
question posted to the R-Help list in October last year. Because I am a
bit of a survival analysis and R newbie, I didn't really understand the
discussion thread. 

I've been working with a Weibull distribution, thus:

>leafsurv.weibull<-survreg(Surv(minage, maxage, censorcode, type =
"interval")~1, dist = "weib")

And I guess I'd like to be able to do something that's the equivalent of

>leafsurv.gamma<-survreg(Surv(minage, maxage, censorcode, type =
"interval")~1, dist = "gamma")

At least one of the R-help listserver comments mentioned using
survreg.distributions to customise a gamma distribution, but I can't
figure out how to make this work with the resources (intellectual and
bibliographical!) that I have available.

With thanks in advance for your help,

Dr Roger Dungan
School of Biological Sciences
University of Cantebury
Christchurch, New Zealand
ph +64 3 366 7001 ext. 4848
fax +64 3 354 2590



From espindle at gmail.com  Thu Jan 27 23:42:00 2005
From: espindle at gmail.com (Leo Espindle)
Date: Thu, 27 Jan 2005 17:42:00 -0500
Subject: [R] Multiple colors in a plot title
Message-ID: <438abbc70501271442565b5a60@mail.gmail.com>

R-Help:

Is there a way to use multiple colors in the title of a plot?  For
instance, to have certain words be red, and certain words be blue?

thanks in advance,
Leo

-- 
1718 Commonwealth Avenue
Apt 2
Brighton, MA 02135
Cell:  617-599-0037



From thewavyx at gmail.com  Thu Jan 27 23:46:41 2005
From: thewavyx at gmail.com (Eric Rodriguez)
Date: Thu, 27 Jan 2005 23:46:41 +0100
Subject: [R] Array Manipulation
In-Reply-To: <41F94C87.7060405@stat.wisc.edu>
References: <3A822319EB35174CA3714066D590DCD50994E5D2@usrymx25.merck.com>
	<41F94C87.7060405@stat.wisc.edu>
Message-ID: <47779aab05012714464b706d1c@mail.gmail.com>

and something like that:

dat[dat$ID == sample(unique(dat$ID), 3), 2]   ?

I'm not sure about the ",2" maybe you need the full matrix ?


ps: first time, i forgot the list



From p.dalgaard at biostat.ku.dk  Thu Jan 27 23:49:27 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Jan 2005 23:49:27 +0100
Subject: [R] Finding "runs" of TRUE in binary vector
In-Reply-To: <BC8DDFAA-70B0-11D9-A33E-000D933565E8@mail.nih.gov>
References: <BC8DDFAA-70B0-11D9-A33E-000D933565E8@mail.nih.gov>
Message-ID: <x2wttytrm0.fsf@biostat.ku.dk>

Sean Davis <sdavis2 at mail.nih.gov> writes:

> I have a binary vector and I want to find all "regions" of that vector
> that are runs of TRUE (or FALSE).
> 
>  > a <- rnorm(10)
>  > b <- a<0.5
>  > b
>   [1]  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE
> 
> My function would return something like a list:
> region[[1]] 1,3
> region[[2]] 5,5
> region[[3]] 7,10
> 
> Any ideas besides looping and setting start and ends directly?

You could base it on

> rle(b)
Run Length Encoding
  lengths: int [1:5] 1 1 2 4 2
  values : logi [1:5]  TRUE FALSE  TRUE FALSE  TRUE
> b
 [1]  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE

(Notice that my b differs from yours)

then you might proceed with 

> end <- cumsum(rle(b)$lengths)
> start <- rev(length(b) + 1  - cumsum(rev(rle(b)$lengths)))
> # or:   start <- c(1, end[-length(end)] + 1)
> cbind(start,end)[rle(b)$values,]
     start end
[1,]     1   1
[2,]     3   4
[3,]     9  10


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From deanm at maildrop.srv.ualberta.ca  Fri Jan 28 00:25:21 2005
From: deanm at maildrop.srv.ualberta.ca (deanm)
Date: Thu, 27 Jan 2005 16:25:21 -0700
Subject: [R] binomia data and mixed model
Message-ID: <41FEEDF5@webmail.ualberta.ca>

Hi,
I am a first user of R.

I was hoping I could get some help on some data I need to analyze.

The experimental design is a complete randomized design with 2 factors (Source 
material and Depth). The experimental design was suppose to consist of 4 
treatments replicated 3 time, Source 1 and applied at 10 cm and source 2 
applied at 20 cm. During the construction of the treatmetns the depths vary 
considerably so i can't test all my samples based on 10 and 20 cm any more the 
depths are now considered random and not fixed. Each treatment was sampled for 
depth and total density of plants with 3 transects with 28 quadrats per 
transect. The data is very non-normal (lots of zeros) therefore the only way 
to analyze it is to convert to binomial data. Does any one know what type of 
analysis I should use? I was told that a NLmixed model would work but also a 
GLIM mixed model was appropriate. Is there any info using these in R.

Dean D. MacKenzie
Master's of Science Candidate. A.Ag
Department of Renewable Resources
Rm 723 GSB
University of Alberta
Edmonton, AB
T6G 2H1
Office tel: (780) 492-4135
Home tel:   (780) 437-9563



From bolker at ufl.edu  Fri Jan 28 00:11:29 2005
From: bolker at ufl.edu (Ben Bolker)
Date: Thu, 27 Jan 2005 23:11:29 +0000 (UTC)
Subject: [R] Threshhold Models in gnlm
References: <opsk6kxktxw3ajb1@smtp.umt.edu>
Message-ID: <loom.20050128T000714-635@post.gmane.org>


  

Eliot McIntire <emcintire <at> forestry.umt.edu> writes:

> Hello,
> 
> I am interested in fitting a generalized nonlinear regression (gnlr) model  
> with negative binomial errors.
> 
> I have found Jim Lindsay's package that will do gnlr, but I have having  
> trouble with the particular model I am interested in fitting.
> 
> It is a threshhold model, where below a certain value of one of the  
> parameters being fitted, the model changes.
> 
>   [BIG SNIP]

Threshold models (also known as "piecewise linear", or more recently,
"hockey stick" models) are actually surprisingly challenging to fit
numerically.  There are papers on least-squares fitting going back
to Bacon and Watts (1971, Biometrika) and before to Quandt, and more
recent (2000) posts on the S-PLUS lists from Bill Venables, Mary Lindstrom,
and Nicholas Barrowman (who has a paper with Ram Myers on hockey stick
models in fisheries).  The basic trick is that, unless you do some kind
of numerical smoothing, it's very easy to get stuck in local minima.
I got a little carried away with the problem and am sending you some
code off-list ...

  cheers
    Ben Bolker



From vashok at gmail.com  Fri Jan 28 02:19:27 2005
From: vashok at gmail.com (Ashok Veeraraghavan)
Date: Thu, 27 Jan 2005 20:19:27 -0500
Subject: [R] Help with R and Bioconductor
In-Reply-To: <Pine.SOL.4.20.0501271340360.16228-100000@santiam.dfci.harvard.edu>
References: <ac0fd70d05012710304fa8ee02@mail.gmail.com>
	<Pine.SOL.4.20.0501271340360.16228-100000@santiam.dfci.harvard.edu>
Message-ID: <ac0fd70d05012717195fd269c5@mail.gmail.com>

Hi Jeff,

First of all thanks for the response. But i still am encountering some
problems with installing bioconductor.

I did remove the directory 00Lock.
I ran getBioC("affy")

During the installation i get a warning several times.
chmod:/Library/Frameworks/R.framework/Version/2.0.1/Resources/library/R.css:Operation
Not Permitted
Is this warning message critical to the installation?

Moreover at the end of the Installation i also got this message. I
have appended that message to the end of this email.

Question 1.
Package Annotate was not updated. Why wasnt it updated. How can i update it?

Question 2.
Should i be worried abt the other warning messages?

Thanks
Regards
Ashok




Packages that were not updated:
        annotate

Warning messages: 
1: 
 Package annotate version 1.5.1 suggests zebrafish
Package annotate version 1.5.1 suggests xenopuslaevis 
 in: resolve.depends(pkgInfo, repEntry, force, lib = lib,
searchOptions = searchOptions,
2: Installation of package annaffy had non-zero exit status in:
installPkg(fileName, pkg, pkgVer, type, lib, repEntry, versForce)
3: 
 Package annotate version 1.5.1 suggests zebrafish
Package annotate version 1.5.1 suggests xenopuslaevis 
 in: resolve.depends(pkgInfo, repEntry, force, lib = lib,
searchOptions = searchOptions,
4: Installation of package Rgraphviz had non-zero exit status in:
installPkg(fileName, pkg, pkgVer, type, lib, repEntry, versForce)
5: Installation of package geneplotter had non-zero exit status in:
installPkg(fileName, pkg, pkgVer, type, lib, repEntry, versForce)
6: 
 Package annotate version 1.5.1 suggests zebrafish
Package annotate version 1.5.1 suggests xenopuslaevis 
 in: resolve.depends(pkgInfo, repEntry, force, lib = lib,
searchOptions = searchOptions,




On Thu, 27 Jan 2005 13:41:45 -0500 (EST), Jeff Gentry
<jgentry at jimmy.harvard.edu> wrote:
> > seemed successful. Then while attempting to getBioC() I had to force
> > quit the R application since I had to attend to something else
> > urgently. When i returned and tried to getBioC, I am getting errors
> 
> Why not just let it run?
> 
> > indicating that there is a lock on some files. So i would like to
> 
> The directory will likely be <path-to-R>/library/00LOCK (I say likely
> because the '<path-to-R>/library' part could be something else if you
> specified an alternate installation directory or your default .libPaths is
> different then standard), and removing that directory will solve your
> issues.
> 
>



From james.holtman at convergys.com  Fri Jan 28 03:28:32 2005
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Thu, 27 Jan 2005 21:28:32 -0500
Subject: [R] Finding "runs" of TRUE in binary vector
Message-ID: <OFD2701250.2D1521BC-ON85256F97.000D8E10@nd.convergys.com>





use 'rle';

> a <- rnorm(20)
> b <- a < .5
> b
 [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE
TRUE
[13] FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE
> rle(b)
Run Length Encoding
  lengths: int [1:9] 1 7 2 2 2 3 1 1 1
  values : logi [1:9] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE
>
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      Sean Davis                                                                                                           
                      <sdavis2 at mail.nih.gov        To:       r-help <r-help at stat.math.ethz.ch>                                             
                      >                            cc:                                                                                     
                      Sent by:                     Subject:  [R] Finding "runs" of TRUE in binary vector                                   
                      r-help-bounces at stat.m                                                                                                
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      01/27/2005 17:13                                                                                                     
                                                                                                                                           
                                                                                                                                           




I have a binary vector and I want to find all "regions" of that vector
that are runs of TRUE (or FALSE).

 > a <- rnorm(10)
 > b <- a<0.5
 > b
  [1]  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE

My function would return something like a list:
region[[1]] 1,3
region[[2]] 5,5
region[[3]] 7,10

Any ideas besides looping and setting start and ends directly?

Thanks,
Sean

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From wgshi2001 at yahoo.ca  Fri Jan 28 05:23:33 2005
From: wgshi2001 at yahoo.ca (Weiguang Shi)
Date: Thu, 27 Jan 2005 23:23:33 -0500 (EST)
Subject: [R] agglomerative coefficient in agnes (cluster)
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E5C7@usrymx25.merck.com>
Message-ID: <20050128042333.95233.qmail@web30008.mail.mud.yahoo.com>

Thanks very much Andy for the code and the
explanation.
The meaning of AC is much more clear now.

I did notice, when I tried the code, the results were
not exactly the same as yours.
  > sapply(c(.25,.5), testAC, x=x[1:4],
method="single")
  Loading required package: cluster 
  Error in FUN(X[[1]], ...) : Object "x" not found
  > x=rnorm(50)
  > sapply(c(.25,.5), testAC, x=x[1:4],
method="single")
  [1] 0.7450599 0.9926918

  > version
         _                
  platform i686-pc-linux-gnu
  arch     i686             
  os       linux-gnu        
  system   i686, linux-gnu  
  status                    
  major    2                
  minor    0.1              
  year     2004             
  month    11               
  day      15                 
  language R                

Regards,
Weiguang

 --- "Liaw, Andy" <andy_liaw at merck.com> wrote: 
> It has to do with sample sizes.  Consider the
> following:
> 
> testAC <- function(prop1=0.5, x=rnorm(50),
> center=c(0, 100), ...) {
>     stopifnot(require(cluster))
>     n <- length(x)
>     n1 <- ceiling(n * prop1)
>     n2 <- n - n1
>     agnes(x + rep(center, c(n1, n2)), ...)$ac
> }
> 
> Now some tests:
> 
> > sapply(c(.25, .5), testAC, x=x[1:4],
> method="single")
> [1] 0.7427591 0.9862944
> > sapply(1:5 / 10, testAC, x=x[1:10],
> method="single")
> [1] 0.8977139 0.9974224 0.9950061 0.9946366
> 0.9946366
> > sapply(1:5 / 10, testAC, x=x, method="single")
> [1] 0.9982955 0.9969757 0.9971114 0.9971127
> 0.9975111
> 
> So it seems like AC does not consider isolated
> singletons as cluster
> structures.  This is only discernable in small
> sample size, though.
> 
> Andy
> 
> 
>  
> >  --- "Liaw, Andy" <andy_liaw at merck.com> wrote: 
> > > BTW, I checked the book.  You're not going find
> much
> > > more than that.
> > > 
> > Thanks for checking.
> > 
> > Weiguang
> > 
> >
>
______________________________________________________________
> > ________ 
> > Post your free ad now! http://personals.yahoo.ca
> > 
> > 
> 
> 
>
------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any
> attachments, contains information of Merck & Co.,
> Inc. (One Merck Drive, Whitehouse Station, New
> Jersey, USA 08889), and/or its affiliates (which may
> be known outside the United States as Merck Frosst,
> Merck Sharp & Dohme or MSD and in Japan, as Banyu)
> that may be confidential, proprietary copyrighted
> and/or legally privileged. It is intended solely for
> the use of the individual or entity named on this
> message.  If you are not the intended recipient, and
> have received this message in error, please notify
> us immediately by reply e-mail and then delete it
> from your system.
>
------------------------------------------------------------------------------
>



From andy_liaw at merck.com  Fri Jan 28 00:58:28 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 27 Jan 2005 18:58:28 -0500
Subject: [R] clustering
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5DB@usrymx25.merck.com>

It depends a lot on what you know or don't know about the data, and what
problem you're trying to solve.  

If you know for sure it's a mixture of gaussians, likelihood based
approaches might be better.  MASS (the book) has an example of fitting
univariate mixture of gaussians using various optimizers.  The code is even
in $R_HOME/library/MASS/scripts/ch16.R.

Andy

> From: WeiWei Shi
> 
> Hi,
> thanks for reply. In fact, I tried both of them and I also tried the
> other method and I found all of them gave me different boundaries (to
> my real datasets). I am thinking about k-median but hoping to get more
> suggestions from all of you in this forum.
> 
> Cheers,
> 
> Ed
> 
> 
> On Thu, 27 Jan 2005 15:37:16 -0600, msck9 at mizzou.edu 
> <msck9 at mizzou.edu> wrote:
> > The cluster analysis should be able to handle that. I think if you
> > know how many clusters you have, "kmeans" is ok, or the EM algorithm
> > can also do that.
> > On Thu, Jan 27, 2005 at 03:44:42PM -0500, WeiWei Shi wrote:
> > > Hi,
> > > I just get a question (sorry if it is a dumb one) and I "phase" my
> > > question in the following R codes:
> > >
> > > group1<-rnorm(n=50, mean=0, sd=1)
> > > group2<-rnorm(n=20, mean=1, sd=1.5)
> > > group3<-c(group1,group2)
> > >
> > >
> > > Now, if I am given a dataset from group3, what method 
> (discriminant
> > > analysis, clustering, maybe) is the best to cluster them 
> by using R.
> > > The known info includes: 2 clusters, normal distribution (but the
> > > parameters are unknown).
> > >
> > > Thanks,
> > >
> > > Ed
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From helprhelp at gmail.com  Fri Jan 28 06:19:35 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Fri, 28 Jan 2005 00:19:35 -0500
Subject: [R] clustering
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E5DB@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E5DB@usrymx25.merck.com>
Message-ID: <cdf8178305012721196838aab5@mail.gmail.com>

Actually the problem I am trying to solve is to discretize a
continuous variable (which is my response variable (dependent
variable) in my project so that I can make a regression problem into a
classification one. (There are many reasons for doing this.)

Since there is no class label for this variable (because this variable
is my class variable :), the unsupervised approach can be applied
here. However, checking the related papers shows there is little
research (in my knowledge, and I haven't checked the MCC yet) in this
field. Using qqnorm to check the normality and histogram indicates
there might be two normal distributions.

My approach is splitting the values for this variable into 2 or 3
intervals and check each interval's normality again. If some approach
like clustering or the one Andy suggests works well, then I should get
much better normality. I will try that tomorrow.

I am not sure if my idea works or not here, please be advised !

Thanks,

Ed


On Thu, 27 Jan 2005 18:58:28 -0500, Liaw, Andy <andy_liaw at merck.com> wrote:
> It depends a lot on what you know or don't know about the data, and what
> problem you're trying to solve.
> 
> If you know for sure it's a mixture of gaussians, likelihood based
> approaches might be better.  MASS (the book) has an example of fitting
> univariate mixture of gaussians using various optimizers.  The code is even
> in $R_HOME/library/MASS/scripts/ch16.R.
> 
> Andy
> 
> > From: WeiWei Shi
> >
> > Hi,
> > thanks for reply. In fact, I tried both of them and I also tried the
> > other method and I found all of them gave me different boundaries (to
> > my real datasets). I am thinking about k-median but hoping to get more
> > suggestions from all of you in this forum.
> >
> > Cheers,
> >
> > Ed
> >
> >
> > On Thu, 27 Jan 2005 15:37:16 -0600, msck9 at mizzou.edu
> > <msck9 at mizzou.edu> wrote:
> > > The cluster analysis should be able to handle that. I think if you
> > > know how many clusters you have, "kmeans" is ok, or the EM algorithm
> > > can also do that.
> > > On Thu, Jan 27, 2005 at 03:44:42PM -0500, WeiWei Shi wrote:
> > > > Hi,
> > > > I just get a question (sorry if it is a dumb one) and I "phase" my
> > > > question in the following R codes:
> > > >
> > > > group1<-rnorm(n=50, mean=0, sd=1)
> > > > group2<-rnorm(n=20, mean=1, sd=1.5)
> > > > group3<-c(group1,group2)
> > > >
> > > >
> > > > Now, if I am given a dataset from group3, what method
> > (discriminant
> > > > analysis, clustering, maybe) is the best to cluster them
> > by using R.
> > > > The known info includes: 2 clusters, normal distribution (but the
> > > > parameters are unknown).
> > > >
> > > > Thanks,
> > > >
> > > > Ed
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > >
> >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From vikas at mail.jnu.ac.in  Fri Jan 28 06:56:02 2005
From: vikas at mail.jnu.ac.in (Vikas Rawal)
Date: Fri, 28 Jan 2005 11:26:02 +0530
Subject: [R] Matrix multiplication in R is inaccurate!!
Message-ID: <1106891762.914ccf80vikas@mail.jnu.ac.in>


If you multiply a matrix by its inverse you should get an identity matrix. In R, you get an answer that is accurate up to about 16 decimal points? Why can't one get a perfect answer?

See for example: 

c(5,3)->x1
c(3,2)->x2
cbind(x1,x2)->x
solve(x)->y
x%*%y

Vikas Rawal

==============================================

 This Mail was Scanned for Virus and found Virus free



From james.muller at internode.on.net  Fri Jan 28 07:12:24 2005
From: james.muller at internode.on.net (James Muller)
Date: Fri, 28 Jan 2005 17:12:24 +1100
Subject: [R] Error: cannot allocate vector of size... but with a twist
Message-ID: <41F9D7C8.2010606@internode.on.net>

Hi,

I have a memory problem, one which I've seen pop up in the list a few 
times, but which seems to be a little different. It is the Error: cannot 
allocate vector of size x problem. I'm running R2.0 on RH9.

My R program is joining big datasets together, so there are lots of 
duplicate cases of data in memory. This (and other tasks) prompted me 
to... expand... my swap partition to 16Gb. I have 0.5Gb of regular, fast 
DDR. The OS seems to be fine accepting the large amount of memory, and 
I'm not restricting memory use or vector size in any way.

R chews up memory up until the 3.5Gb area, then halts. Here's the last 
bit of output:

 > # join the data together
 > cdata01.data <- 
cbind(c.1,c.2,c.3,c.4,c.5,c.6,c.7,c.8,c.9,c.10,c.11,c.12,c.13,c.14,c.15,c.16,c.17,c.18,c.19,c.20,c.21,c.22,c.23,c.24,c.25,c.26,c.27,c.28,c.29,c.30,c.31,c.32,c.33)
Error: cannot allocate vector of size 145 Kb
Execution halted

145--Kb---?? This has me rather lost. Maybe on overflow of some sort?? 
Maybe on OS problem of some sort? I'm scratching here.

Before you question it, there is a legitimate reason for sticking all 
these components in the one data.frame.

One of the problems here is that tinkering is not really feasible. This 
cbind took 1.5 hrs to finally halt.

Any help greatly appreciated,

James



From ripley at stats.ox.ac.uk  Fri Jan 28 07:59:13 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 28 Jan 2005 06:59:13 +0000 (GMT)
Subject: [R] Error: cannot allocate vector of size... but with a twist
In-Reply-To: <41F9D7C8.2010606@internode.on.net>
References: <41F9D7C8.2010606@internode.on.net>
Message-ID: <Pine.LNX.4.61.0501280653190.18916@gannet.stats>

On Fri, 28 Jan 2005, James Muller wrote:

> Hi,
>
> I have a memory problem, one which I've seen pop up in the list a few times, 
> but which seems to be a little different. It is the Error: cannot allocate 
> vector of size x problem. I'm running R2.0 on RH9.
>
> My R program is joining big datasets together, so there are lots of duplicate 
> cases of data in memory. This (and other tasks) prompted me to... expand... 
> my swap partition to 16Gb. I have 0.5Gb of regular, fast DDR. The OS seems to 
> be fine accepting the large amount of memory, and I'm not restricting memory 
> use or vector size in any way.
>
> R chews up memory up until the 3.5Gb area, then halts. Here's the last bit of 
> output:

You have, presumably, a 32-bit computer with a 4GB-per-process memory 
limit.  You have hit it (you get less than 4Gb as the OS services need 
some and there is some fragmentation).  The last failed allocation may be 
small, as you see, if you are allocating lots of smallish pieces.

The only way to overcome that is to use a 64-bit OS and version of R.

What was the `twist' mentioned in the title?  You will find a similar 
overall limit mentioned about weekly on this list if you look in the 
archives.

>
>> # join the data together
>> cdata01.data <- 
> cbind(c.1,c.2,c.3,c.4,c.5,c.6,c.7,c.8,c.9,c.10,c.11,c.12,c.13,c.14,c.15,c.16,c.17,c.18,c.19,c.20,c.21,c.22,c.23,c.24,c.25,c.26,c.27,c.28,c.29,c.30,c.31,c.32,c.33)
> Error: cannot allocate vector of size 145 Kb
> Execution halted
>
> 145--Kb---?? This has me rather lost. Maybe on overflow of some sort?? Maybe 
> on OS problem of some sort? I'm scratching here.
>
> Before you question it, there is a legitimate reason for sticking all these 
> components in the one data.frame.
>
> One of the problems here is that tinkering is not really feasible. This cbind 
> took 1.5 hrs to finally halt.
>
> Any help greatly appreciated,
>
> James
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From roebuck at odin.mdacc.tmc.edu  Fri Jan 28 08:05:09 2005
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Fri, 28 Jan 2005 01:05:09 -0600 (CST)
Subject: [R] Error: cannot allocate vector of size... but with a twist
In-Reply-To: <41F9D7C8.2010606@internode.on.net>
References: <41F9D7C8.2010606@internode.on.net>
Message-ID: <Pine.OSF.4.58.0501280101290.208141@odin.mdacc.tmc.edu>

On Fri, 28 Jan 2005, James Muller wrote:

> I have a memory problem, one which I've seen pop up in the list a few
> times, but which seems to be a little different. It is the Error: cannot
> allocate vector of size x problem. I'm running R2.0 on RH9.
>
> [SNIP]
>
> R chews up memory up until the 3.5Gb area, then halts. Here's the last
> bit of output:
>

32-bit addressing goes to ~4Gb.

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From ligges at statistik.uni-dortmund.de  Fri Jan 28 09:10:46 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 28 Jan 2005 09:10:46 +0100
Subject: [R] Multiple colors in a plot title
In-Reply-To: <438abbc70501271442565b5a60@mail.gmail.com>
References: <438abbc70501271442565b5a60@mail.gmail.com>
Message-ID: <41F9F386.3060109@statistik.uni-dortmund.de>

Leo Espindle wrote:
> R-Help:
> 
> Is there a way to use multiple colors in the title of a plot?  For
> instance, to have certain words be red, and certain words be blue?

No, you have top typset them word by word.

Uwe Ligges


> thanks in advance,
> Leo
>



From ligges at statistik.uni-dortmund.de  Fri Jan 28 09:12:22 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 28 Jan 2005 09:12:22 +0100
Subject: [R] Matrix multiplication in R is inaccurate!!
In-Reply-To: <1106891762.914ccf80vikas@mail.jnu.ac.in>
References: <1106891762.914ccf80vikas@mail.jnu.ac.in>
Message-ID: <41F9F3E6.6000307@statistik.uni-dortmund.de>

Vikas Rawal wrote:
> If you multiply a matrix by its inverse you should get an identity matrix. In R, you get an answer that is accurate up to about 16 decimal points? Why can't one get a perfect answer?

Because R makes use of a digital computer which cannot work exactly 
using floating-point arithmetics ...
Please read some textbook about a numerics and computers.

Uwe Ligges


> See for example: 
> 
> c(5,3)->x1
> c(3,2)->x2
> cbind(x1,x2)->x
> solve(x)->y
> x%*%y
> 
> Vikas Rawal
> 
> ==============================================
> 
>  This Mail was Scanned for Virus and found Virus free
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Christoph.Scherber at uni-jena.de  Fri Jan 28 09:28:26 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Fri, 28 Jan 2005 09:28:26 +0100
Subject: [R] self-written function
In-Reply-To: <200501272055.j0RKtn123512@gator.dt.uh.edu>
References: <200501272055.j0RKtn123512@gator.dt.uh.edu>
Message-ID: <41F9F7AA.2090405@uni-jena.de>

Hi!

I?m using the function for a data frame, but up to now it only works 
with single vectors, such as

backsin(variable,grouping.factor)

Actually, you?re right, the "..." in the function definition is not 
needed...:-)

Regards
Christoph


Erin Hodgess wrote:

>Hello Christoph!
>
>I have a question about your question, please:
>
>In your first line of code, 
>backsin <- function(x,y,...){
>
>why do you have the three dots at the end?
>
>Also, what kind of data sets are you applying this to, please?
>
>Data frames or Matrixes?
>
>Thanks,
>Erin
>mailto: hodgess at gator.uhd.edu
>  
>
Dear all,

I?ve got a simple self-written function to calculate the mean + s.e. 
from arcsine-transformed data:

backsin<-function(x,y,...){
backtransf<-list()
backtransf$back<-((sin(x[x!="NA"]))^2)*100
backtransf$mback<-tapply(backtransf$back,y[x!="NA"],mean)
backtransf$sdback<-tapply(backtransf$back,y[x!="NA"],stdev)/sqrt(length(y[x!="NA"])) 

backtransf
}

I would like to apply this function to whole datasets, such as

tapply(variable,list(A,B,C,D),backsin)

Of course, this doesn?t work with the way in which the backsin() 
function is specified.

Does anyone have suggestions on how I could improve my function?

Regards,
Christoph



From maechler at stat.math.ethz.ch  Fri Jan 28 09:34:31 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 28 Jan 2005 09:34:31 +0100
Subject: [R] clustering
In-Reply-To: <cdf8178305012714206985ca31@mail.gmail.com>
References: <cdf817830501271244fcd7338@mail.gmail.com>
	<20050127213716.GA8526@localhost>
	<cdf8178305012714206985ca31@mail.gmail.com>
Message-ID: <16889.63767.140612.463108@stat.math.ethz.ch>

Did you try  pam()  from package "cluster"?

I'd recommend it as an improvement compared to kmeans().

Martin Maechler, ETH Zurich



From jhong at jhsph.edu  Fri Jan 28 09:48:40 2005
From: jhong at jhsph.edu (Jeanhee Hong)
Date: Fri, 28 Jan 2005 03:48:40 -0500
Subject: [R] R-Help : running MIX package
Message-ID: <78D5BFE5CDEC5A4CA1101197911F1EEA01A0E788@XCH-VN01.sph.ad.jhsph.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050128/a8bb4bce/attachment.pl

From uth at zhwin.ch  Fri Jan 28 10:35:36 2005
From: uth at zhwin.ch (Thomas Unternaehrer)
Date: Fri, 28 Jan 2005 10:35:36 +0100
Subject: [R] Multiple colors in a plot title
In-Reply-To: <438abbc70501271442565b5a60@mail.gmail.com>
References: <438abbc70501271442565b5a60@mail.gmail.com>
Message-ID: <41FA0768.1000605@zhwin.ch>

Try something like this:

plot(1:10)
mtext(c("title1", "title2"), col = c(2:3), at = c(4, 6))


Leo Espindle wrote:

>R-Help:
>
>Is there a way to use multiple colors in the title of a plot?  For
>instance, to have certain words be red, and certain words be blue?
>
>thanks in advance,
>Leo
>
>  
>



From Ted.Harding at nessie.mcc.ac.uk  Fri Jan 28 10:52:02 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 28 Jan 2005 09:52:02 -0000 (GMT)
Subject: [R] R-Help : running MIX package
In-Reply-To: <78D5BFE5CDEC5A4CA1101197911F1EEA01A0E788@XCH-VN01.sph.ad.jhsph.edu>
Message-ID: <XFMail.050128095202.Ted.Harding@nessie.mcc.ac.uk>

On 28-Jan-05 Jeanhee Hong wrote:
> Hello all. I am inexperienced with  R and am clumsily trying to work
> through it for specific multiple imputations Id like to run for my
> thesis.In running the MIX package, I keep getting an error message
> regarding the use of the prelim.mix command. 
>  
> Error in as.integer.default(list(alcohol = c(1, 1, 1, 1, 1, 1, 1, 1, 1,
>: 
>         (list) object cannot be coerced to integer
>  
> I cannot find the source of this error. I am assuming its somewhere in
> the format of my data but cant quite seem to figure it out. I have
> followed the procedures in the manual in terms of listing categorical
> variables first and making sure they take positive integer values...Has
> anyone encountered this same error? Any help would be appreciated.Thank
> you.

I have not seen such an error message myself, but the occurrence
of "list" in it is ominous! According to the R code of prelim.mix,
this should not be introduced by prelim.mix itself and so is likely
to be somehow present in the data you submit to the function (which
cannot be checked without looking at how you define your data).

Please try the following. The function call prelim.mix(x,p)
assumes that x is a *matrix* whose first p columns are the values
of the p categorical variables (coded, as you note, as positive
integers).

So make sure that x is indeed a *matrix* and not some other R
structure (which may, e.g. a data-frame, look like a matrix but
isn't one). 'matrix' is a very specific data structure in R,
and is not to be confused with other structures which may look
like matrices.

One way to ensure this could be to use

  x<-as.matrix(your.data.frame)

(assuming that "your.data.frame" already has its columns satisfying
the requisite conditions). Another (which is what I've mainly
used) is to construct x by using 'cbind': I've tended to find
that for the sort of data one often gets, some preliminary
manipulation of the categorical variables (at least) is required
so as to get them into the required form, and this is best done
separately. So, once you have established your p categorical
variables cat1,...,catp and your k continuous variables
cont1,...,contk, something like

  x <- cbind(cat1,cat2,...,catp,cont1,cont2,...,contk)

would create a matrix called "x".

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 28-Jan-05                                       Time: 09:52:02
------------------------------ XFMail ------------------------------



From Christoph.Scherber at uni-jena.de  Fri Jan 28 11:04:43 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Fri, 28 Jan 2005 11:04:43 +0100
Subject: [R] self-written function
In-Reply-To: <41FA150F.1352.A26250@localhost>
References: <41FA150F.1352.A26250@localhost>
Message-ID: <41FA0E3B.8010600@uni-jena.de>

Hi!

OK, here are some more details on the function: My dataframe consists of 
several columns of categorical variables (let?s call them A,B,C) plus a 
column with a response variable y (which is arcsine-square root 
transformed proportions)

I am now trying to write a function that automatically gives me the 
back-transformed mean values + standard errors for y for each column in 
the dataframe. Ideally, this would be something like

tapply(y,list(A,B,C,D),backtransformed.mean)

Here is the correct version of the function:

backsin<-function(x,y){
backtransf<-list()

back<-((sin(x[x!="NA"]))^2)*100
backtransf$mback<-tapply(back,y[x!="NA"],mean)
backtransf$sdback<-tapply(back,y[x!="NA"],std)/sqrt(length(y[x!="NA"]))
backtransf
}


Regards,
Christoph


Petr Pikal wrote:

>Hi
>
>I am not sure if anybody gave you a reply, but do you think you gave 
>enough detail about your data, what you expect, what you did, what 
>was the response and how it differ from expected output and best of 
>all ***working*** example?
>
>BTW, what is stdev?
>
>If you wanted to compute standard deviation sd is enough.
>
>Cheers
>Petr
>
>On 27 Jan 2005 at 12:20, Christoph Scherber wrote:
>
>  
>
>>Dear all,
>>
>>I?ve got a simple self-written function to calculate the mean + s.e.
>>from arcsine-transformed data:
>>
>>backsin<-function(x,y,...){
>> backtransf<-list()
>> backtransf$back<-((sin(x[x!="NA"]))^2)*100
>> backtransf$mback<-tapply(backtransf$back,y[x!="NA"],mean)
>> backtransf$sdback<-tapply(backtransf$back,y[x!="NA"],std)/sqrt(length(y[x!="NA"])) 
>>

>>backtransf
>>    
>>
>>}
>>
>>I would like to apply this function to whole datasets, such as
>>
>>tapply(variable,list(A,B,C,D),backsin)
>>
>>Of course, this doesn?t work with the way in which the backsin()
>>function is specified.
>>
>>Does anyone have suggestions on how I could improve my function?
>>
>>Regards,
>>Christoph
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>    
>>
>
>Petr Pikal
>petr.pikal at precheza.cz
>
>
>
>  
>



From vito_ricci at yahoo.com  Fri Jan 28 11:14:12 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Fri, 28 Jan 2005 11:14:12 +0100 (CET)
Subject: [R] GLM fitting
Message-ID: <20050128101412.5842.qmail@web41214.mail.yahoo.com>

DeaR R-useRs,

I'm trying to fit a logist model with these data:

> dati
   y  x
1  1 37
2  1 35
3  1 33
4  1 40
5  1 45
6  1 41
7  1 42
8  0 20
9  0 21
10 0 25
11 0 27
12 0 29
13 0 18

I use glm(), having this output:

> g<-glm(y~x,family=binomial,data=dati)
Warning messages: 
1: Algorithm did not converge in: glm.fit(x = X, y =
Y, weights = weights, start = start, etastart =
etastart,  
2: fitted probabilities numerically 0 or 1 occurred
in: glm.fit(x = X, y = Y, weights = weights, start =
start, etastart = etastart,  
> g

Call:  glm(formula = y ~ x, family = binomial, data =
dati) 

Coefficients:
(Intercept)            x  
    -348.23        11.23  

Degrees of Freedom: 12 Total (i.e. Null);  11 Residual
Null Deviance:      17.94 
Residual Deviance: 7.011e-10    AIC: 4 

I don't understand the meaning of warning. Can anyone
help me? Many thanks.
Cheers,
Vito


=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From cg.pettersson at evp.slu.se  Fri Jan 28 11:15:08 2005
From: cg.pettersson at evp.slu.se (CG Pettersson)
Date: Fri, 28 Jan 2005 11:15:08 +0100
Subject: [R] Conflicts using Rcmdr, nlme and lme4
Message-ID: <200501281015.j0SAF8Id009565@mail1.slu.se>

Hello all!

R2.0.1, W2k. All packages updated.

I?m heavily dependant on using mixed models. Up til?now I have used
lme() from nlme as I have been told to. Together with estimable() from
gmodels it works smooth. I also often run Rcmdr, mostly for quick
graphics.

After using Rcmdr, on reopening the R workspace all help libraries for
Rcmdr (22 !) loads, among them nlme, but not Rcmdr itself. Why? 

Now I saw on the list yesterday, that lmer() from lme4 offers better
coding possibilities for crossed random factors, it feels natural to
switch from lme() to lmer() if estimable() still works on the objects.

So I installed lme4, but got a conflict with the (auto)loaded nlme:

> library(lme4)
Loading required package: Matrix 
Loading required package: latticeExtra 
Error in fun(...) : Package lme4 conflicts with package nlme.
 To attach lme4 you must restart R without package nlme.
Error: .onLoad failed in loadNamespace for 'lme4'
Error in library(lme4) : package/namespace load failed for 'lme4'

Several questions here:

Why do all these packages autoload, and could I avoid this in some
way?
Why a conflict? Isn?t it possible to mask the conflicting parts of
nlme when loading lme4?
Why are the mixed models tools from the same author(s?) splitted
between nlme and lme4? Are there any technical reasons not to include
lmer() in nlme?

Cheers
/CG

CG Pettersson, MSci, PhD Stud.
Swedish University of Agricultural Sciences
Dep. of Ecology and Crop Production. Box 7043
SE-750 07 Uppsala



From nicolas.deig at epfl.ch  Fri Jan 28 11:22:47 2005
From: nicolas.deig at epfl.ch (NICOLAS DEIG)
Date: Fri, 28 Jan 2005 11:22:47 +0100
Subject: [R] plot
Message-ID: <174fc165fd.165fd174fc@imap.epfl.ch>

hello,

I wonder whether there is a way to plot in R the following:

I have two arrays of dimension (200*1) say x1 and x2.
I would like to plot on the same graph x1[1:50,]*x2[1:50,] with pch=1  
                        

                                       x1[51:100,]*x2[51:100,] with pch=2
                                     x1[101:150,]*x2[101:150,] with pch=3
                                      x1[151:200,]*x2[151:200,] with pch=4
is it possible with the "plot" function? Or do I have to plot
4differents graphs?
THanks in advance.
nicolas



From kjetil at acelerate.com  Fri Jan 28 11:44:11 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 28 Jan 2005 06:44:11 -0400
Subject: [R] Finding "runs" of TRUE in binary vector
In-Reply-To: <BC8DDFAA-70B0-11D9-A33E-000D933565E8@mail.nih.gov>
References: <BC8DDFAA-70B0-11D9-A33E-000D933565E8@mail.nih.gov>
Message-ID: <41FA177B.5090803@acelerate.com>

Sean Davis wrote:

> I have a binary vector and I want to find all "regions" of that vector 
> that are runs of TRUE (or FALSE).
>
> > a <- rnorm(10)
> > b <- a<0.5
> > b
>  [1]  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE
>
> My function would return something like a list:
> region[[1]] 1,3
> region[[2]] 5,5
> region[[3]] 7,10
>
> Any ideas besides looping and setting start and ends directly?
>
?rle


> Thanks,
> Sean
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From bxc at steno.dk  Fri Jan 28 11:45:14 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Fri, 28 Jan 2005 11:45:14 +0100
Subject: [R] GLM fitting
Message-ID: <0ABD88905D18E347874E0FB71C0B29E9027FE7A6@exdkba022.novo.dk>

You have a perfect separtaion of y by x, i.e.

y == (x>30)

is true for all units.

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vito Ricci
> Sent: Friday, January 28, 2005 11:14 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] GLM fitting
> 
> 
> DeaR R-useRs,
> 
> I'm trying to fit a logist model with these data:
> 
> > dati
>    y  x
> 1  1 37
> 2  1 35
> 3  1 33
> 4  1 40
> 5  1 45
> 6  1 41
> 7  1 42
> 8  0 20
> 9  0 21
> 10 0 25
> 11 0 27
> 12 0 29
> 13 0 18
> 
> I use glm(), having this output:
> 
> > g<-glm(y~x,family=binomial,data=dati)
> Warning messages: 
> 1: Algorithm did not converge in: glm.fit(x = X, y =
> Y, weights = weights, start = start, etastart =
> etastart,  
> 2: fitted probabilities numerically 0 or 1 occurred
> in: glm.fit(x = X, y = Y, weights = weights, start =
> start, etastart = etastart,  
> > g
> 
> Call:  glm(formula = y ~ x, family = binomial, data =
> dati) 
> 
> Coefficients:
> (Intercept)            x  
>     -348.23        11.23  
> 
> Degrees of Freedom: 12 Total (i.e. Null);  11 Residual
> Null Deviance:      17.94 
> Residual Deviance: 7.011e-10    AIC: 4 
> 
> I don't understand the meaning of warning. Can anyone
> help me? Many thanks.
> Cheers,
> Vito
> 
> 
> =====
> Diventare costruttori di soluzioni
> Became solutions' constructors
> 
> "The business of the statistician is to catalyze 
> the scientific learning process."  
> George E. P. Box
> 
> Top 10 reasons to become a Statistician
> 
>      1. Deviation is considered normal
>      2. We feel complete and sufficient
>      3. We are 'mean' lovers
>      4. Statisticians do it discretely and continuously
>      5. We are right 95% of the time
>      6. We can legally comment on someone's posterior distribution
>      7. We may not be normal, but we are transformable
>      8. We never have to say we are certain
>      9. We are honestly significantly different
>     10. No one wants our jobs
> 
> 
> Visitate il portale http://www.modugno.it/
> e in particolare la sezione su Palese  
> http://www.modugno.it/archivio/palese/
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From junjunx at ualberta.ca  Fri Jan 28 11:50:04 2005
From: junjunx at ualberta.ca (Junjun Xie)
Date: Fri, 28 Jan 2005 03:50:04 -0700
Subject: [R] latent class model in R
Message-ID: <200501281050.j0SAo5mr011183@pilsener.srv.ualberta.ca>

Hi  Everyone,
I am facing the problem of running the latent class model in R. By "mmlcr", the program always shows as following,

Error in model.frame(formula, rownames, variables, varnames, extras, extranames,  :      invalid variable type

Does anyone happen to know why this happened? I am stopped at this step.  Thank you very much for the answer.


Best regards, 

Junjun Xie



From fm3a004 at math.uni-hamburg.de  Fri Jan 28 11:51:44 2005
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Fri, 28 Jan 2005 11:51:44 +0100 (MET)
Subject: [R] clustering
In-Reply-To: <cdf8178305012721196838aab5@mail.gmail.com>
Message-ID: <Pine.GSO.3.95q.1050128114547.7798A-100000@sun11.math.uni-hamburg.de>

Hi,

EMclust in package mclust fits normal mixtures.
Note that if you split your data values into intervals, the resulting
distributions conditional on the intervals are not normals, but truncated
normals!
This is important if you try to check within group normality, unless you
have strongly separated clusters (which does not seem to be the case).

Christian


On Fri, 28 Jan 2005, WeiWei Shi wrote:

> Actually the problem I am trying to solve is to discretize a
> continuous variable (which is my response variable (dependent
> variable) in my project so that I can make a regression problem into a
> classification one. (There are many reasons for doing this.)
> 
> Since there is no class label for this variable (because this variable
> is my class variable :), the unsupervised approach can be applied
> here. However, checking the related papers shows there is little
> research (in my knowledge, and I haven't checked the MCC yet) in this
> field. Using qqnorm to check the normality and histogram indicates
> there might be two normal distributions.
> 
> My approach is splitting the values for this variable into 2 or 3
> intervals and check each interval's normality again. If some approach
> like clustering or the one Andy suggests works well, then I should get
> much better normality. I will try that tomorrow.
> 
> I am not sure if my idea works or not here, please be advised !
> 
> Thanks,
> 
> Ed
> 
> 
> On Thu, 27 Jan 2005 18:58:28 -0500, Liaw, Andy <andy_liaw at merck.com> wrote:
> > It depends a lot on what you know or don't know about the data, and what
> > problem you're trying to solve.
> > 
> > If you know for sure it's a mixture of gaussians, likelihood based
> > approaches might be better.  MASS (the book) has an example of fitting
> > univariate mixture of gaussians using various optimizers.  The code is even
> > in $R_HOME/library/MASS/scripts/ch16.R.
> > 
> > Andy
> > 
> > > From: WeiWei Shi
> > >
> > > Hi,
> > > thanks for reply. In fact, I tried both of them and I also tried the
> > > other method and I found all of them gave me different boundaries (to
> > > my real datasets). I am thinking about k-median but hoping to get more
> > > suggestions from all of you in this forum.
> > >
> > > Cheers,
> > >
> > > Ed
> > >
> > >
> > > On Thu, 27 Jan 2005 15:37:16 -0600, msck9 at mizzou.edu
> > > <msck9 at mizzou.edu> wrote:
> > > > The cluster analysis should be able to handle that. I think if you
> > > > know how many clusters you have, "kmeans" is ok, or the EM algorithm
> > > > can also do that.
> > > > On Thu, Jan 27, 2005 at 03:44:42PM -0500, WeiWei Shi wrote:
> > > > > Hi,
> > > > > I just get a question (sorry if it is a dumb one) and I "phase" my
> > > > > question in the following R codes:
> > > > >
> > > > > group1<-rnorm(n=50, mean=0, sd=1)
> > > > > group2<-rnorm(n=20, mean=1, sd=1.5)
> > > > > group3<-c(group1,group2)
> > > > >
> > > > >
> > > > > Now, if I am given a dataset from group3, what method
> > > (discriminant
> > > > > analysis, clustering, maybe) is the best to cluster them
> > > by using R.
> > > > > The known info includes: 2 clusters, normal distribution (but the
> > > > > parameters are unknown).
> > > > >
> > > > > Thanks,
> > > > >
> > > > > Ed
> > > > >
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > > >
> > >
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > >
> > 
> > 
> > ------------------------------------------------------------------------------
> > Notice:  This e-mail message, together with any attachment...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From Christoph.Scherber at uni-jena.de  Fri Jan 28 11:53:06 2005
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Fri, 28 Jan 2005 11:53:06 +0100
Subject: [R] self-written function
In-Reply-To: <1106908856.8102.1.camel@biol102145.oulu.fi>
References: <41FA150F.1352.A26250@localhost> <41FA0E3B.8010600@uni-jena.de>
	<1106908856.8102.1.camel@biol102145.oulu.fi>
Message-ID: <41FA1992.3040901@uni-jena.de>

Dear Jari,

I had included the test for NA?s just to be sure the function can handle 
datasets with missing values.

Here?s an updated version without this specification:

backsin<-function(x,y){
backtransf<-list()
back<-((sin(x))^2)*100
backtransf$mback<-tapply(back,y,mean)
backtransf$sdback<-tapply(back,y,sd)/sqrt(length(y))
backtransf
}



Jari Oksanen wrote:

>Christoph,
>
>I guess your "NA" refers to Not Available (missing, NA). In that case,
>you should check it with is.na() instead of != "NA". See this:
>
>  
>
>>x <- c(3,4,NA,5)
>>x != "NA"
>>    
>>
>[1] TRUE TRUE   NA TRUE
>  
>
>>!is.na(x)
>>    
>>
>[1]  TRUE  TRUE FALSE  TRUE
>
>No idea if this helps, but it was a problem with the code anyway.
>
>cheers, jari oksanen
>On Fri, 2005-01-28 at 11:04 +0100, Christoph Scherber wrote:
>  
>
>>Hi!
>>
>>OK, here are some more details on the function: My dataframe consists of 
>>several columns of categorical variables (let?s call them A,B,C) plus a 
>>column with a response variable y (which is arcsine-square root 
>>transformed proportions)
>>
>>I am now trying to write a function that automatically gives me the 
>>back-transformed mean values + standard errors for y for each column in 
>>the dataframe. Ideally, this would be something like
>>
>>tapply(y,list(A,B,C,D),backtransformed.mean)
>>
>>Here is the correct version of the function:
>>
>>backsin<-function(x,y){
>>backtransf<-list()
>>
>>back<-((sin(x[x!="NA"]))^2)*100
>>backtransf$mback<-tapply(back,y[x!="NA"],mean)
>>backtransf$sdback<-tapply(back,y[x!="NA"],std)/sqrt(length(y[x!="NA"]))
>>backtransf
>>}
>>
>>
>>Regards,
>>Christoph
>>
>>
>>Petr Pikal wrote:
>>
>>    
>>
>>>Hi
>>>
>>>I am not sure if anybody gave you a reply, but do you think you gave 
>>>enough detail about your data, what you expect, what you did, what 
>>>was the response and how it differ from expected output and best of 
>>>all ***working*** example?
>>>
>>>BTW, what is stdev?
>>>
>>>If you wanted to compute standard deviation sd is enough.
>>>
>>>Cheers
>>>Petr
>>>
>>>On 27 Jan 2005 at 12:20, Christoph Scherber wrote:
>>>
>>> 
>>>
>>>      
>>>
>>>>Dear all,
>>>>
>>>>I?ve got a simple self-written function to calculate the mean + s.e.
>>>>        
>>>>
>>>>from arcsine-transformed data:
>>>      
>>>
>>>>backsin<-function(x,y,...){
>>>>backtransf<-list()
>>>>backtransf$back<-((sin(x[x!="NA"]))^2)*100
>>>>backtransf$mback<-tapply(backtransf$back,y[x!="NA"],mean)
>>>>backtransf$sdback<-tapply(backtransf$back,y[x!="NA"],std)/sqrt(length(y[x!="NA"])) 
>>>>
>>>>        
>>>>
>>>>backtransf
>>>>   
>>>>
>>>>}
>>>>
>>>>I would like to apply this function to whole datasets, such as
>>>>
>>>>tapply(variable,list(A,B,C,D),backsin)
>>>>
>>>>Of course, this doesn?t work with the way in which the backsin()
>>>>function is specified.
>>>>
>>>>Does anyone have suggestions on how I could improve my function?
>>>>
>>>>Regards,
>>>>Christoph
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide!
>>>>http://www.R-project.org/posting-guide.html
>>>>   
>>>>
>>>>        
>>>>
>>>Petr Pikal
>>>petr.pikal at precheza.cz
>>>
>>>
>>>
>>> 
>>>
>>>      
>>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>    
>>
>
>
>  
>



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Jan 28 11:56:29 2005
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 28 Jan 2005 11:56:29 +0100
Subject: [R] plot
References: <174fc165fd.165fd174fc@imap.epfl.ch>
Message-ID: <012701c50528$05341fe0$0540210a@www.domain>

try this:

x1 <- as.matrix(rnorm(200))
x2 <- as.matrix(rnorm(200))
###############
dim(x1) <- dim(x2) <- c(50,4)
matplot(x1,x2, pch=1:4, col=1)



Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "NICOLAS DEIG" <nicolas.deig at epfl.ch>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, January 28, 2005 11:22 AM
Subject: [R] plot


> hello,
>
> I wonder whether there is a way to plot in R the following:
>
> I have two arrays of dimension (200*1) say x1 and x2.
> I would like to plot on the same graph x1[1:50,]*x2[1:50,] with 
> pch=1
>
>
>                                       x1[51:100,]*x2[51:100,] with 
> pch=2
>                                     x1[101:150,]*x2[101:150,] with 
> pch=3
>                                      x1[151:200,]*x2[151:200,] with 
> pch=4
> is it possible with the "plot" function? Or do I have to plot
> 4differents graphs?
> THanks in advance.
> nicolas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From bates at stat.wisc.edu  Fri Jan 28 12:02:24 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 28 Jan 2005 05:02:24 -0600
Subject: [R] Conflicts using Rcmdr, nlme and lme4
In-Reply-To: <200501281015.j0SAF8Id009565@mail1.slu.se>
References: <200501281015.j0SAF8Id009565@mail1.slu.se>
Message-ID: <41FA1BC0.5040601@stat.wisc.edu>

CG Pettersson wrote:
> Hello all!
> 
> R2.0.1, W2k. All packages updated.
> 
> I?m heavily dependant on using mixed models. Up til?now I have used
> lme() from nlme as I have been told to. Together with estimable() from
> gmodels it works smooth. I also often run Rcmdr, mostly for quick
> graphics.
> 
> After using Rcmdr, on reopening the R workspace all help libraries for
> Rcmdr (22 !) loads, among them nlme, but not Rcmdr itself. Why? 
> 
> Now I saw on the list yesterday, that lmer() from lme4 offers better
> coding possibilities for crossed random factors, it feels natural to
> switch from lme() to lmer() if estimable() still works on the objects.
> 
> So I installed lme4, but got a conflict with the (auto)loaded nlme:
> 
> 
>>library(lme4)
> 
> Loading required package: Matrix 
> Loading required package: latticeExtra 
> Error in fun(...) : Package lme4 conflicts with package nlme.
>  To attach lme4 you must restart R without package nlme.
> Error: .onLoad failed in loadNamespace for 'lme4'
> Error in library(lme4) : package/namespace load failed for 'lme4'
> 
> Several questions here:
> 
> Why do all these packages autoload, and could I avoid this in some
> way?
> Why a conflict? Isn?t it possible to mask the conflicting parts of
> nlme when loading lme4?

One reason for my recently introducing the name lmer for the revised lme 
function was eventually to remove the conflict of the lme4 package with 
the nlme package.  In current versions of lme4 I still have an lme 
function that generates an S4 object of the S4 class "lme".    Because 
of the conflict of functions of the same name producing different 
classes  of objects I must have the nlme and lme4 packages conflict. 
Right now there are some models that can be fit by the S4-based lme and 
not be lmer.  There are some "infelicities" in the underlying C code for 
lmer when dealing with non-nested vector-valued random effects.  Once I 
track down and fix these infelicies I can remove the lme function from 
the lme4 package and remove the conflict with the nlme package.

> Why are the mixed models tools from the same author(s?) splitted
> between nlme and lme4? Are there any technical reasons not to include
> lmer() in nlme?

Once you have published a book you have committed yourself to a 
particular API for as long as the book remains in print.  I can't easily 
change the way that the lme and related functions in the nlme package 
work because they are documented in our book.  The functions in lme4 
operate in a fundamentally different way, precisely because they are 
designed to handle crossed random effects in addition to nested random 
effects.

The lme4 package is a work in progress.  I am following Eric Raymond's 
Open Source dictum of 'release early and release often'.  There will 
inevitably be inconveniences associated with using developmental code.

I can guarantee that this is the last time that I will fundamentally 
redesign the computational methods and data structures used in fitting 
mixed-effects models.  This is the fifth time I have done it from 
scratch and after this one I am quitting the business.



From ligges at statistik.uni-dortmund.de  Fri Jan 28 12:06:28 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 28 Jan 2005 12:06:28 +0100
Subject: [R] plot
In-Reply-To: <174fc165fd.165fd174fc@imap.epfl.ch>
References: <174fc165fd.165fd174fc@imap.epfl.ch>
Message-ID: <41FA1CB4.3090405@statistik.uni-dortmund.de>

NICOLAS DEIG wrote:

> hello,
> 
> I wonder whether there is a way to plot in R the following:
> 
> I have two arrays of dimension (200*1) say x1 and x2.
> I would like to plot on the same graph x1[1:50,]*x2[1:50,] with pch=1  
>                         
> 
>                                        x1[51:100,]*x2[51:100,] with pch=2
>                                      x1[101:150,]*x2[101:150,] with pch=3
>                                       x1[151:200,]*x2[151:200,] with pch=4

You can specify pch in form of a vector (as long as x1) as in:

plot(1:10, pch=rep(1:2, each=5))

Uwe Ligges


> is it possible with the "plot" function? Or do I have to plot
> 4differents graphs?
> THanks in advance.
> nicolas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jacques.veslot at cirad.fr  Fri Jan 28 12:22:49 2005
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Fri, 28 Jan 2005 15:22:49 +0400
Subject: [R] plot
In-Reply-To: <174fc165fd.165fd174fc@imap.epfl.ch>
Message-ID: <HHEDKBCGCMDOHEDELFBCAEDKCIAA.jacques.veslot@cirad.fr>


if * in x1[1:50,]*x2[1:50,] means "times", try :
matplot(matrix(x1,ncol=4)*matrix(x2,ncol=4))

if * means "vs", try :
matplot(matrix(x1,ncol=4),matrix(x2,ncol=4))


-----Message d'origine-----
De : r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]De la part de NICOLAS DEIG
Envoye : vendredi 28 janvier 2005 14:23
A : r-help at stat.math.ethz.ch
Objet : [R] plot


hello,

I wonder whether there is a way to plot in R the following:

I have two arrays of dimension (200*1) say x1 and x2.
I would like to plot on the same graph x1[1:50,]*x2[1:50,] with pch=1


                                       x1[51:100,]*x2[51:100,] with pch=2
                                     x1[101:150,]*x2[101:150,] with pch=3
                                      x1[151:200,]*x2[151:200,] with pch=4
is it possible with the "plot" function? Or do I have to plot
4differents graphs?
THanks in advance.
nicolas

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From scallopie at gmx.net  Fri Jan 28 12:42:05 2005
From: scallopie at gmx.net (Elaine Han)
Date: Fri, 28 Jan 2005 12:42:05 +0100 (MET)
Subject: [R] Windows Batch File
Message-ID: <20742.1106912525@www6.gmx.net>

Hiya,

I have read the FAQ for windows and read point 2.10. I setup a dos batch
file which contains the following line:
C:\progra~1\R\rw2001\bin\Rterm.exe --vanilla

I then tried:
R CMD BATCH --argument myscript.R result.out

then it started R. I thought by running R CMD BATCH executes R
non-interactively. I am working on a Windows XP. I tried it on a Mac OSX and
that worked fine. So I'm not sure what I have done wrong here and have been
searching the web for solution but to no avail. Please consult.

I thank you in advance.

Regards,
Elaine

-- 
Sparen beginnt mit GMX DSL: http://www.gmx.net/de/go/dsl



From francoisromain at free.fr  Fri Jan 28 12:45:45 2005
From: francoisromain at free.fr (Romain Francois)
Date: Fri, 28 Jan 2005 12:45:45 +0100
Subject: [R] plot
In-Reply-To: <174fc165fd.165fd174fc@imap.epfl.ch>
References: <174fc165fd.165fd174fc@imap.epfl.ch>
Message-ID: <41FA25E9.4010601@free.fr>

Hello,

You search that command :

plot(x1,x2,pch=rep(1:4,50))


I think you should read some basic manuals about R, the FAQ, etc ...



NICOLAS DEIG wrote:

>hello,
>
>I wonder whether there is a way to plot in R the following:
>
>I have two arrays of dimension (200*1) say x1 and x2.
>I would like to plot on the same graph x1[1:50,]*x2[1:50,] with pch=1  
>                        
>
>                                       x1[51:100,]*x2[51:100,] with pch=2
>                                     x1[101:150,]*x2[101:150,] with pch=3
>                                      x1[151:200,]*x2[151:200,] with pch=4
>is it possible with the "plot" function? Or do I have to plot
>4differents graphs?
>THanks in advance.
>nicolas
>  
>

-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From francoisromain at free.fr  Fri Jan 28 12:50:11 2005
From: francoisromain at free.fr (Romain Francois)
Date: Fri, 28 Jan 2005 12:50:11 +0100
Subject: [R] plot
In-Reply-To: <41FA25E9.4010601@free.fr>
References: <174fc165fd.165fd174fc@imap.epfl.ch> <41FA25E9.4010601@free.fr>
Message-ID: <41FA26F3.2050904@free.fr>


> Hello,
>
> You search that command :
>
> plot(x1,x2,pch=rep(1:4,50))

Oups, sorry, I meant that :

plot(x1,x2,pch=rep(1:4,each=50))
                       ^^^^


I need more cafeine on morning !

>
>
> I think you should read some basic manuals about R, the FAQ, etc ...
>
>
>
> NICOLAS DEIG wrote:
>
>> hello,
>>
>> I wonder whether there is a way to plot in R the following:
>>
>> I have two arrays of dimension (200*1) say x1 and x2.
>> I would like to plot on the same graph x1[1:50,]*x2[1:50,] with 
>> pch=1                        
>>                                       x1[51:100,]*x2[51:100,] with pch=2
>>                                     x1[101:150,]*x2[101:150,] with pch=3
>>                                      x1[151:200,]*x2[151:200,] with 
>> pch=4
>> is it possible with the "plot" function? Or do I have to plot
>> 4differents graphs?
>> THanks in advance.
>> nicolas
>>  
>>
>


-- 
Romain FRANCOIS : francoisromain at free.fr
page web : http://addictedtor.free.fr/  (en construction)
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann?e
Institut de Statistique de l'Universit? de Paris (ISUP)
Fili?re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From petr.pikal at precheza.cz  Fri Jan 28 13:16:15 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 28 Jan 2005 13:16:15 +0100
Subject: [R] self-written function
In-Reply-To: <41FA0E3B.8010600@uni-jena.de>
References: <41FA150F.1352.A26250@localhost>
Message-ID: <41FA3B1F.3708.1373724@localhost>

Hi Christoph

On 28 Jan 2005 at 11:04, Christoph Scherber wrote:

> Hi!
> 
> OK, here are some more details on the function: My dataframe consists
> of several columns of categorical variables (let?s call them A,B,C)
> plus a column with a response variable y (which is arcsine-square root
> transformed proportions)
> 
> I am now trying to write a function that automatically gives me the
> back-transformed mean values + standard errors for y for each column
> in the dataframe. Ideally, this would be something like
> 

why not do it in several steps:

omit <- !is.na(yourdf$y)
transfy <- ((sin(yourdf$y[omit]))^2)*100
transm <- aggregatey(list(m=transfy), list(A,B,C,D), mean)
transs <- aggregate(list(s=transfy), list(A,B,C,D), function(x) 
(sd(x)/sqrt(length(x))) # I hope I have corect no of ()

cbind(transm,transs$s)

and if you feel like you can pack it to some function.

Cheers
Petr


> tapply(y,list(A,B,C,D),backtransformed.mean)
> 
> Here is the correct version of the function:
> 
> backsin<-function(x,y){
> backtransf<-list()
> 
> back<-((sin(x[x!="NA"]))^2)*100
> backtransf$mback<-tapply(back,y[x!="NA"],mean)
> backtransf$sdback<-tapply(back,y[x!="NA"],std)/sqrt(length(y[x!="NA"])
> ) backtransf }
> 
> 
> Regards,
> Christoph
> 
> 
> Petr Pikal wrote:
> 
> >Hi
> >
> >I am not sure if anybody gave you a reply, but do you think you gave
> >enough detail about your data, what you expect, what you did, what
> >was the response and how it differ from expected output and best of
> >all ***working*** example?
> >
> >BTW, what is stdev?
> >
> >If you wanted to compute standard deviation sd is enough.
> >
> >Cheers
> >Petr
> >
> >On 27 Jan 2005 at 12:20, Christoph Scherber wrote:
> >
> >  
> >
> >>Dear all,
> >>
> >>I?ve got a simple self-written function to calculate the mean + s.e.
> >>from arcsine-transformed data:
> >>
> >>backsin<-function(x,y,...){
> >> backtransf<-list()
> >> backtransf$back<-((sin(x[x!="NA"]))^2)*100
> >> backtransf$mback<-tapply(backtransf$back,y[x!="NA"],mean)
> >> backtransf$sdback<-tapply(backtransf$back,y[x!="NA"],std)/sqrt(leng
> >> th(y[x!="NA"])) 
> >>
> 
> >>backtransf
> >>    
> >>
> >>}
> >>
> >>I would like to apply this function to whole datasets, such as
> >>
> >>tapply(variable,list(A,B,C,D),backsin)
> >>
> >>Of course, this doesn?t work with the way in which the backsin()
> >>function is specified.
> >>
> >>Does anyone have suggestions on how I could improve my function?
> >>
> >>Regards,
> >>Christoph
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> >>http://www.R-project.org/posting-guide.html
> >>    
> >>
> >
> >Petr Pikal
> >petr.pikal at precheza.cz
> >
> >
> >
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From Ted.Harding at nessie.mcc.ac.uk  Fri Jan 28 13:13:03 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 28 Jan 2005 12:13:03 -0000 (GMT)
Subject: [R] R-Help : running MIX package
In-Reply-To: <XFMail.050128095202.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.050128121303.Ted.Harding@nessie.mcc.ac.uk>

On 28-Jan-05 Ted Harding wrote:
> On 28-Jan-05 Jeanhee Hong wrote:
>> Hello all. I am inexperienced with  R and am clumsily trying to work
>> through it for specific multiple imputations Id like to run for my
>> thesis.In running the MIX package, I keep getting an error message
>> regarding the use of the prelim.mix command. 
>>  
>> Error in as.integer.default(list(alcohol = c(1, 1, 1, 1, 1, 1, 1, 1,
>> 1,
>>: 
>>         (list) object cannot be coerced to integer
>> [...]
> [...]
> So make sure that x is indeed a *matrix* and not some other R
> structure (which may, e.g. a data-frame, look like a matrix but
> isn't one). 'matrix' is a very specific data structure in R,
> and is not to be confused with other structures which may look
> like matrices.
> [...]

By experiment I have confirmed my guess: with X a dataframe
(30 rows, 4 cols of which the first two categorical) I get
exactly the same error message as you with

  prelim.mix(X,2)

whereas, after

  x <- as.matrix(X)
  prelim.mix(x,2)

everything is fine . Note that both

  X

and

  x

give outputs which look exactly the same, though they are not
the same kind of object. Only 'x' will do for prelim.mix!

Cheers,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 28-Jan-05                                       Time: 12:13:03
------------------------------ XFMail ------------------------------



From nicolas.deig at epfl.ch  Fri Jan 28 13:55:00 2005
From: nicolas.deig at epfl.ch (NICOLAS DEIG)
Date: Fri, 28 Jan 2005 13:55:00 +0100
Subject: [R] xfig
Message-ID: <13dc71a1c2.1a1c213dc7@imap.epfl.ch>

hello all,

I know I can export a graphic from R to Xfig from the manuals, but I
just cannot figure out it works.
does anyone know have to do so?
thanks in advance.
Nicolas



From ligges at statistik.uni-dortmund.de  Fri Jan 28 13:58:52 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 28 Jan 2005 13:58:52 +0100
Subject: [R] Windows Batch File
In-Reply-To: <20742.1106912525@www6.gmx.net>
References: <20742.1106912525@www6.gmx.net>
Message-ID: <41FA370C.50904@statistik.uni-dortmund.de>

Elaine Han wrote:

> Hiya,
> 
> I have read the FAQ for windows and read point 2.10. I setup a dos batch
> file which contains the following line:
> C:\progra~1\R\rw2001\bin\Rterm.exe --vanilla

This line fires up R, but won't accept any file as input...

The FAQ tells you to use
"path_to_R\bin\Rterm.exe --no-restore --no-save < %1 > %1.out 2>&1"
in your batch file.


> I then tried:
> R CMD BATCH --argument myscript.R result.out

What is "--argument"? At least it is superflously here, the rest should 
work given you have set up your environment correctly (path to R)?

Uwe Ligges

> then it started R. I thought by running R CMD BATCH executes R
> non-interactively. I am working on a Windows XP. I tried it on a Mac OSX and
> that worked fine. So I'm not sure what I have done wrong here and have been
> searching the web for solution but to no avail. Please consult.
> 
> I thank you in advance.
> 
> Regards,
> Elaine
>



From jfox at mcmaster.ca  Fri Jan 28 14:09:12 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 28 Jan 2005 08:09:12 -0500
Subject: [R] Conflicts using Rcmdr, nlme and lme4
In-Reply-To: <200501281015.j0SAF8Id009565@mail1.slu.se>
Message-ID: <20050128130907.SUKZ1567.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear CG,

Doug Bates has already addressed the conflict between the nlme and lme4
packages. The Rcmdr package loads a number of other packages to use
functions in them. I frankly don't recall why I need nlme, but something in
it must be used somewhere in the Rcmdr. You could get rid of the dependency
of the Rcmdr on nlme by editing the package DESCRIPTION file and recompiling
the package. The fact that I can't remember why nlme is required suggests
that removing it won't break much, so you'll probably still be able to do
what you want.

As far as I'm aware, you cannot restart the R Commander from a saved R
workspace. You can, however, proceed as follows: Close the Commander window;
close R and save the workspace; (presumably in a later session) restart R;
load the Rcmdr package via library(Rcmdr).

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of CG Pettersson
> Sent: Friday, January 28, 2005 5:15 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Conflicts using Rcmdr, nlme and lme4
> 
> Hello all!
> 
> R2.0.1, W2k. All packages updated.
> 
> I?m heavily dependant on using mixed models. Up til?now I have used
> lme() from nlme as I have been told to. Together with 
> estimable() from gmodels it works smooth. I also often run 
> Rcmdr, mostly for quick graphics.
> 
> After using Rcmdr, on reopening the R workspace all help 
> libraries for Rcmdr (22 !) loads, among them nlme, but not 
> Rcmdr itself. Why? 
> 
> Now I saw on the list yesterday, that lmer() from lme4 offers 
> better coding possibilities for crossed random factors, it 
> feels natural to switch from lme() to lmer() if estimable() 
> still works on the objects.
> 
> So I installed lme4, but got a conflict with the (auto)loaded nlme:
> 
> > library(lme4)
> Loading required package: Matrix
> Loading required package: latticeExtra
> Error in fun(...) : Package lme4 conflicts with package nlme.
>  To attach lme4 you must restart R without package nlme.
> Error: .onLoad failed in loadNamespace for 'lme4'
> Error in library(lme4) : package/namespace load failed for 'lme4'
> 
> Several questions here:
> 
> Why do all these packages autoload, and could I avoid this in 
> some way?
> Why a conflict? Isn?t it possible to mask the conflicting 
> parts of nlme when loading lme4?
> Why are the mixed models tools from the same author(s?) 
> splitted between nlme and lme4? Are there any technical 
> reasons not to include
> lmer() in nlme?
> 
> Cheers
> /CG
> 
> CG Pettersson, MSci, PhD Stud.
> Swedish University of Agricultural Sciences Dep. of Ecology 
> and Crop Production. Box 7043 SE-750 07 Uppsala
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Fri Jan 28 14:29:43 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 28 Jan 2005 08:29:43 -0500
Subject: [R] Conflicts using Rcmdr, nlme and lme4
In-Reply-To: <200501281015.j0SAF8Id009565@mail1.slu.se>
Message-ID: <20050128132938.OHRO1919.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear CG,

An addendum to my previous response: If you do decide to recompile the Rcmdr
package to eliminate the dependency on nlme, as I suggested, you'll also
have to edit the .onLoad() function in the package (which is in the file
startup.R in the source package) to remove "nlme" from the vector of
required packages.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of CG Pettersson
> Sent: Friday, January 28, 2005 5:15 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Conflicts using Rcmdr, nlme and lme4
> 
> Hello all!
> 
> R2.0.1, W2k. All packages updated.
> 
> I?m heavily dependant on using mixed models. Up til?now I have used
> lme() from nlme as I have been told to. Together with 
> estimable() from gmodels it works smooth. I also often run 
> Rcmdr, mostly for quick graphics.
> 
> After using Rcmdr, on reopening the R workspace all help 
> libraries for Rcmdr (22 !) loads, among them nlme, but not 
> Rcmdr itself. Why? 
> 
> Now I saw on the list yesterday, that lmer() from lme4 offers 
> better coding possibilities for crossed random factors, it 
> feels natural to switch from lme() to lmer() if estimable() 
> still works on the objects.
> 
> So I installed lme4, but got a conflict with the (auto)loaded nlme:
> 
> > library(lme4)
> Loading required package: Matrix
> Loading required package: latticeExtra
> Error in fun(...) : Package lme4 conflicts with package nlme.
>  To attach lme4 you must restart R without package nlme.
> Error: .onLoad failed in loadNamespace for 'lme4'
> Error in library(lme4) : package/namespace load failed for 'lme4'
> 
> Several questions here:
> 
> Why do all these packages autoload, and could I avoid this in 
> some way?
> Why a conflict? Isn?t it possible to mask the conflicting 
> parts of nlme when loading lme4?
> Why are the mixed models tools from the same author(s?) 
> splitted between nlme and lme4? Are there any technical 
> reasons not to include
> lmer() in nlme?
> 
> Cheers
> /CG
> 
> CG Pettersson, MSci, PhD Stud.
> Swedish University of Agricultural Sciences Dep. of Ecology 
> and Crop Production. Box 7043 SE-750 07 Uppsala
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From karthikeyan_s at hotmail.com  Fri Jan 28 14:41:34 2005
From: karthikeyan_s at hotmail.com (KARTHIKEYAN SUBBIAH)
Date: Fri, 28 Jan 2005 13:41:34 +0000
Subject: [R] Double integration
Message-ID: <BAY101-F41DCFA8A5580D4308969CE96790@phx.gbl>



From ligges at statistik.uni-dortmund.de  Fri Jan 28 14:51:50 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 28 Jan 2005 14:51:50 +0100
Subject: [R] xfig
In-Reply-To: <13dc71a1c2.1a1c213dc7@imap.epfl.ch>
References: <13dc71a1c2.1a1c213dc7@imap.epfl.ch>
Message-ID: <41FA4376.4000509@statistik.uni-dortmund.de>

NICOLAS DEIG wrote:

> hello all,
> 
> I know I can export a graphic from R to Xfig from the manuals, but I
> just cannot figure out it works.
> does anyone know have to do so?
> thanks in advance.
> Nicolas

What about reading ?xfig

Use it like any other device:

xfig(filname)
plot(1:10)
dev.off()


Uwe Ligges



> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Fri Jan 28 14:44:46 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 28 Jan 2005 08:44:46 -0500
Subject: [R] Error: cannot allocate vector of size... but with a
 twist
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5DF@usrymx25.merck.com>

Just a couple of remarks below...

> From: James Muller
> 
> Hi,
> 
> I have a memory problem, one which I've seen pop up in the list a few 
> times, but which seems to be a little different. It is the 
> Error: cannot 
> allocate vector of size x problem. I'm running R2.0 on RH9.
> 
> My R program is joining big datasets together, so there are lots of 
> duplicate cases of data in memory. This (and other tasks) prompted me 
> to... expand... my swap partition to 16Gb. I have 0.5Gb of 
> regular, fast 
> DDR. The OS seems to be fine accepting the large amount of 
> memory, and 
> I'm not restricting memory use or vector size in any way.
> 
> R chews up memory up until the 3.5Gb area, then halts. Here's 
> the last 
> bit of output:
> 
>  > # join the data together
>  > cdata01.data <- 
> cbind(c.1,c.2,c.3,c.4,c.5,c.6,c.7,c.8,c.9,c.10,c.11,c.12,c.13,
> c.14,c.15,c.16,c.17,c.18,c.19,c.20,c.21,c.22,c.23,c.24,c.25,c.
> 26,c.27,c.28,c.29,c.30,c.31,c.32,c.33)
> Error: cannot allocate vector of size 145 Kb
> Execution halted
> 
> 145--Kb---?? This has me rather lost. Maybe on overflow of 
> some sort?? 
> Maybe on OS problem of some sort? I'm scratching here.
> 
> Before you question it, there is a legitimate reason for sticking all 
> these components in the one data.frame.

One possible way to get around this, if you really have no alternatives, is
to write the individual columns (I assume that's what those things you're
cbind()ing are) to files, and use `paste' to paste them into one file, and
read that into a fresh R session.
 
> One of the problems here is that tinkering is not really 
> feasible. This 
> cbind took 1.5 hrs to finally halt.

That's the price you pay for using your HDD as memory!

Andy
 
> Any help greatly appreciated,
> 
> James



From s938613 at mail.yzu.edu.tw  Fri Jan 28 15:32:01 2005
From: s938613 at mail.yzu.edu.tw (=?big5?B?qLlAeXp1?=)
Date: Fri, 28 Jan 2005 22:32:01 +0800
Subject: [R] SparseLogReg make r crash problem
Message-ID: <002101c50546$252b2000$a2df8c0a@NBDodolo>

We are currently using runSparseLogreg in Package SparseLogReg,

here is our code :

library(SparseLogReg)
randomdata <- matrix(runif(1500,min=0,max=30000),ncol=10)
class.zero.one<-c(rep(0,7),rep(1,3)) # give the ramdom gene expression 
samples with class label
significant.gene<-runSparseLogreg(numTrains=10, numGenes=150, 
numExperiments=3,intKfold=5, tol=1e-6, 
maxFeatures=20,inData=randomdata,inClass=class.zero.one)

#above code cause rugi.exe memory error , and we must to close the r 
program.

why does this happen ?

Any help is greatly appreciated.

Sincerely.

Cheng-Bang Chu
Email: s938613 at mail.yzu.edu.tw
Yuan Ze University
Address: 1609a Bioinformatics ,
135 Yuan-Tung Rd. Chung-Li,  Taiwan, 320 R.O.C.



From cervino at ifir.edu.ar  Fri Jan 28 16:07:04 2005
From: cervino at ifir.edu.ar (Ulises Cervino)
Date: Fri, 28 Jan 2005 12:07:04 -0300 (ART)
Subject: [R] Begginer with R
Message-ID: <60944.200.114.179.16.1106924824.squirrel@ifir.ifir.edu.ar>

Hello all,

Im just beggining using R. I have read a couple of introductory documents
but they are very general. Is there a document focused on classification?
(this is what Ill be using R for). Examples would be just fine.
Thanks in advance,

Ulises



From Charles.Annis at StatisticalEngineering.com  Fri Jan 28 15:55:53 2005
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Fri, 28 Jan 2005 09:55:53 -0500
Subject: [R] GLM fitting
In-Reply-To: <20050128101412.5842.qmail@web41214.mail.yahoo.com>
Message-ID: <200501281455.j0SEts4H026069@hypatia.math.ethz.ch>

Vito:

Please plot your data:

y <- c(1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0)
x <- c(37, 35, 33, 40, 45, 41, 42, 20, 21, 25, 27, 29, 18)
plot(x, y)

You will see that ANY step function between 29 < x < 33 will describe these
observations perfectly.


Charles Annis, P.E.
 
Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vito Ricci
Sent: Friday, January 28, 2005 5:14 AM
To: r-help at stat.math.ethz.ch
Subject: [R] GLM fitting

DeaR R-useRs,

I'm trying to fit a logist model with these data:

> dati
   y  x
1  1 37
2  1 35
3  1 33
4  1 40
5  1 45
6  1 41
7  1 42
8  0 20
9  0 21
10 0 25
11 0 27
12 0 29
13 0 18

I use glm(), having this output:

> g<-glm(y~x,family=binomial,data=dati)
Warning messages: 
1: Algorithm did not converge in: glm.fit(x = X, y =
Y, weights = weights, start = start, etastart =
etastart,  
2: fitted probabilities numerically 0 or 1 occurred
in: glm.fit(x = X, y = Y, weights = weights, start =
start, etastart = etastart,  
> g

Call:  glm(formula = y ~ x, family = binomial, data =
dati) 

Coefficients:
(Intercept)            x  
    -348.23        11.23  

Degrees of Freedom: 12 Total (i.e. Null);  11 Residual
Null Deviance:      17.94 
Residual Deviance: 7.011e-10    AIC: 4 

I don't understand the meaning of warning. Can anyone
help me? Many thanks.
Cheers,
Vito


=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese
http://www.modugno.it/archivio/palese/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jeszy at inf.unideb.hu  Fri Jan 28 16:22:46 2005
From: jeszy at inf.unideb.hu (Jeszenszky Peter)
Date: Fri, 28 Jan 2005 16:22:46 +0100 (CET)
Subject: [R] read.matrix.csr bug (e1071)?
Message-ID: <Pine.LNX.4.58.0501281616470.23492@morse.inf.unideb.hu>

Hello,

I would like to read and write sparse matrices using the
functions write.matrix.csr() and read.matrix.csr()
of the package e1071. Writing is OK but reading back the
matrix fails:

	x <- rnorm(100)
	m <- matrix(x, 10)
	m[m < 0.5] <- 0
	m.csr <- as.matrix.csr(m)
	write.matrix.csr(m, "sparse.dat")
	read.matrix("sparse.dat")

	Error in initialize(value, ...) : Can't use object of class "integer" in new():  Class "matrix.csr" does not extend that class

Is something wrong with the code above or it must be
considered as a bug?

Best regards,

Peter



From rkoenker at uiuc.edu  Fri Jan 28 16:35:59 2005
From: rkoenker at uiuc.edu (roger koenker)
Date: Fri, 28 Jan 2005 09:35:59 -0600
Subject: [R] read.matrix.csr bug (e1071)?
In-Reply-To: <Pine.LNX.4.58.0501281616470.23492@morse.inf.unideb.hu>
References: <Pine.LNX.4.58.0501281616470.23492@morse.inf.unideb.hu>
Message-ID: <4F8098CB-7142-11D9-9839-000A95A7E3AA@uiuc.edu>

Don't you want read.matrix.csr not read.matrix?

url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Jan 28, 2005, at 9:22 AM, Jeszenszky Peter wrote:

> Hello,
>
> I would like to read and write sparse matrices using the
> functions write.matrix.csr() and read.matrix.csr()
> of the package e1071. Writing is OK but reading back the
> matrix fails:
>
> 	x <- rnorm(100)
> 	m <- matrix(x, 10)
> 	m[m < 0.5] <- 0
> 	m.csr <- as.matrix.csr(m)
> 	write.matrix.csr(m, "sparse.dat")
> 	read.matrix("sparse.dat")
>
> 	Error in initialize(value, ...) : Can't use object of class "integer" 
> in new():  Class "matrix.csr" does not extend that class
>
> Is something wrong with the code above or it must be
> considered as a bug?
>
> Best regards,
>
> Peter
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From tfliao at uiuc.edu  Fri Jan 28 16:38:58 2005
From: tfliao at uiuc.edu (Tim F Liao)
Date: Fri, 28 Jan 2005 09:38:58 -0600
Subject: [R] GLM fitting
Message-ID: <746dd417.c5cc181c.820c000@expms6.cites.uiuc.edu>

To push the point a bit further, Vito, if you allow just a bit
variation in the data by changing one of the y=1 cases to 0 or
one of the y=0 cases to 1, then you'll be able to fit the glm
model.  If these are real-world data and if you still want to
describe them, then a deterministic statement

y=1 if x>a
y=0 if x<=a

where a can be any value between 29 and 33

would work because there is no uncertainty in the model (other
than the exact location of a).

Tim

---- Original message ----
>Date: Fri, 28 Jan 2005 09:55:53 -0500
>From: "Charles Annis, P.E."
<Charles.Annis at StatisticalEngineering.com>  
>Subject: RE: [R] GLM fitting  
>To: "'Vito Ricci'" <vito_ricci at yahoo.com>,
<r-help at stat.math.ethz.ch>
>
>Vito:
>
>Please plot your data:
>
>y <- c(1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0)
>x <- c(37, 35, 33, 40, 45, 41, 42, 20, 21, 25, 27, 29, 18)
>plot(x, y)
>
>You will see that ANY step function between 29 < x < 33 will
describe these
>observations perfectly.
>
>
>Charles Annis, P.E.
> 
>Charles.Annis at StatisticalEngineering.com
>phone: 561-352-9699
>eFax:  614-455-3265
>http://www.StatisticalEngineering.com
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vito Ricci
>Sent: Friday, January 28, 2005 5:14 AM
>To: r-help at stat.math.ethz.ch
>Subject: [R] GLM fitting
>
>DeaR R-useRs,
>
>I'm trying to fit a logist model with these data:
>
>> dati
>   y  x
>1  1 37
>2  1 35
>3  1 33
>4  1 40
>5  1 45
>6  1 41
>7  1 42
>8  0 20
>9  0 21
>10 0 25
>11 0 27
>12 0 29
>13 0 18
>
>I use glm(), having this output:
>
>> g<-glm(y~x,family=binomial,data=dati)
>Warning messages: 
>1: Algorithm did not converge in: glm.fit(x = X, y =
>Y, weights = weights, start = start, etastart =
>etastart,  
>2: fitted probabilities numerically 0 or 1 occurred
>in: glm.fit(x = X, y = Y, weights = weights, start =
>start, etastart = etastart,  
>> g
>
>Call:  glm(formula = y ~ x, family = binomial, data =
>dati) 
>
>Coefficients:
>(Intercept)            x  
>    -348.23        11.23  
>
>Degrees of Freedom: 12 Total (i.e. Null);  11 Residual
>Null Deviance:      17.94 
>Residual Deviance: 7.011e-10    AIC: 4 
>
>I don't understand the meaning of warning. Can anyone
>help me? Many thanks.
>Cheers,
>Vito
>
>
>=====
>Diventare costruttori di soluzioni
>Became solutions' constructors
>
>"The business of the statistician is to catalyze 
>the scientific learning process."  
>George E. P. Box
>
>Top 10 reasons to become a Statistician
>
>     1. Deviation is considered normal
>     2. We feel complete and sufficient
>     3. We are 'mean' lovers
>     4. Statisticians do it discretely and continuously
>     5. We are right 95% of the time
>     6. We can legally comment on someone's posterior
distribution
>     7. We may not be normal, but we are transformable
>     8. We never have to say we are certain
>     9. We are honestly significantly different
>    10. No one wants our jobs
>
>
>Visitate il portale http://www.modugno.it/
>e in particolare la sezione su Palese
>http://www.modugno.it/archivio/palese/
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jeszy at inf.unideb.hu  Fri Jan 28 16:39:11 2005
From: jeszy at inf.unideb.hu (Jeszenszky Peter)
Date: Fri, 28 Jan 2005 16:39:11 +0100 (CET)
Subject: [R] read.matrix.csr bug (e1071)?
In-Reply-To: <4F8098CB-7142-11D9-9839-000A95A7E3AA@uiuc.edu>
References: <Pine.LNX.4.58.0501281616470.23492@morse.inf.unideb.hu>
	<4F8098CB-7142-11D9-9839-000A95A7E3AA@uiuc.edu>
Message-ID: <Pine.LNX.4.58.0501281638100.23492@morse.inf.unideb.hu>

On Fri, 28 Jan 2005, roger koenker wrote:

> Don't you want read.matrix.csr not read.matrix?

Sorry for the mistake, the error is caused by the line

	read.matrix.csr("~/data/sparse.dat")



From kjetil at acelerate.com  Fri Jan 28 16:40:41 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 28 Jan 2005 11:40:41 -0400
Subject: [R] Conflicts using Rcmdr, nlme and lme4
In-Reply-To: <41FA1BC0.5040601@stat.wisc.edu>
References: <200501281015.j0SAF8Id009565@mail1.slu.se>
	<41FA1BC0.5040601@stat.wisc.edu>
Message-ID: <41FA5CF9.9080602@acelerate.com>

Douglas Bates wrote:

>
>
> I can guarantee that this is the last time that I will fundamentally 
> redesign the computational methods and data structures used in fitting 
> mixed-effects models.  This is the fifth time I have done it from 
> scratch and after this one I am quitting the business.
>

Hope that does'nt mean you are quitting completely from statistical 
computing and R?

Kjetil

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From ligges at statistik.uni-dortmund.de  Fri Jan 28 16:57:54 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 28 Jan 2005 16:57:54 +0100
Subject: [R] SparseLogReg make r crash problem
In-Reply-To: <002101c50546$252b2000$a2df8c0a@NBDodolo>
References: <002101c50546$252b2000$a2df8c0a@NBDodolo>
Message-ID: <41FA6102.6090204@statistik.uni-dortmund.de>

@yzu wrote:
> We are currently using runSparseLogreg in Package SparseLogReg,
> 
> here is our code :
> 
> library(SparseLogReg)
> randomdata <- matrix(runif(1500,min=0,max=30000),ncol=10)
> class.zero.one<-c(rep(0,7),rep(1,3)) # give the ramdom gene expression 
> samples with class label
> significant.gene<-runSparseLogreg(numTrains=10, numGenes=150, 
> numExperiments=3,intKfold=5, tol=1e-6, 
> maxFeatures=20,inData=randomdata,inClass=class.zero.one)
> 
> #above code cause rugi.exe memory error , and we must to close the r 
> program.
> 
> why does this happen ?
> 
> Any help is greatly appreciated.

Obviously a bug, please report bugs to the package maintainer (in CC).

Uwe Ligges


> Sincerely.
> 
> Cheng-Bang Chu
> Email: s938613 at mail.yzu.edu.tw
> Yuan Ze University
> Address: 1609a Bioinformatics ,
> 135 Yuan-Tung Rd. Chung-Li,  Taiwan, 320 R.O.C.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From helprhelp at gmail.com  Fri Jan 28 17:01:15 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Fri, 28 Jan 2005 11:01:15 -0500
Subject: [R] clustering
In-Reply-To: <Pine.GSO.3.95q.1050128114547.7798A-100000@sun11.math.uni-hamburg.de>
References: <cdf8178305012721196838aab5@mail.gmail.com>
	<Pine.GSO.3.95q.1050128114547.7798A-100000@sun11.math.uni-hamburg.de>
Message-ID: <cdf81783050128080117ae415d@mail.gmail.com>

Hi,
I think "truncated" normality is what I meant in the last email. The
experiments (, which might not be representative) show k-mean and EM
gave me comparable results, while EM is a little bit better. (I used
Weka for this purpose). I will try them on the real data and also try
pam() and if I have clear conclusion, I will post my result as a
future suggestion.

Thanks for all you guys' help!

Ed


On Fri, 28 Jan 2005 11:51:44 +0100 (MET), Christian Hennig
<fm3a004 at math.uni-hamburg.de> wrote:
> Hi,
> 
> EMclust in package mclust fits normal mixtures.
> Note that if you split your data values into intervals, the resulting
> distributions conditional on the intervals are not normals, but truncated
> normals!
> This is important if you try to check within group normality, unless you
> have strongly separated clusters (which does not seem to be the case).
> 
> Christian
> 
> 
> On Fri, 28 Jan 2005, WeiWei Shi wrote:
> 
> > Actually the problem I am trying to solve is to discretize a
> > continuous variable (which is my response variable (dependent
> > variable) in my project so that I can make a regression problem into a
> > classification one. (There are many reasons for doing this.)
> >
> > Since there is no class label for this variable (because this variable
> > is my class variable :), the unsupervised approach can be applied
> > here. However, checking the related papers shows there is little
> > research (in my knowledge, and I haven't checked the MCC yet) in this
> > field. Using qqnorm to check the normality and histogram indicates
> > there might be two normal distributions.
> >
> > My approach is splitting the values for this variable into 2 or 3
> > intervals and check each interval's normality again. If some approach
> > like clustering or the one Andy suggests works well, then I should get
> > much better normality. I will try that tomorrow.
> >
> > I am not sure if my idea works or not here, please be advised !
> >
> > Thanks,
> >
> > Ed
> >
> >
> > On Thu, 27 Jan 2005 18:58:28 -0500, Liaw, Andy <andy_liaw at merck.com> wrote:
> > > It depends a lot on what you know or don't know about the data, and what
> > > problem you're trying to solve.
> > >
> > > If you know for sure it's a mixture of gaussians, likelihood based
> > > approaches might be better.  MASS (the book) has an example of fitting
> > > univariate mixture of gaussians using various optimizers.  The code is even
> > > in $R_HOME/library/MASS/scripts/ch16.R.
> > >
> > > Andy
> > >
> > > > From: WeiWei Shi
> > > >
> > > > Hi,
> > > > thanks for reply. In fact, I tried both of them and I also tried the
> > > > other method and I found all of them gave me different boundaries (to
> > > > my real datasets). I am thinking about k-median but hoping to get more
> > > > suggestions from all of you in this forum.
> > > >
> > > > Cheers,
> > > >
> > > > Ed
> > > >
> > > >
> > > > On Thu, 27 Jan 2005 15:37:16 -0600, msck9 at mizzou.edu
> > > > <msck9 at mizzou.edu> wrote:
> > > > > The cluster analysis should be able to handle that. I think if you
> > > > > know how many clusters you have, "kmeans" is ok, or the EM algorithm
> > > > > can also do that.
> > > > > On Thu, Jan 27, 2005 at 03:44:42PM -0500, WeiWei Shi wrote:
> > > > > > Hi,
> > > > > > I just get a question (sorry if it is a dumb one) and I "phase" my
> > > > > > question in the following R codes:
> > > > > >
> > > > > > group1<-rnorm(n=50, mean=0, sd=1)
> > > > > > group2<-rnorm(n=20, mean=1, sd=1.5)
> > > > > > group3<-c(group1,group2)
> > > > > >
> > > > > >
> > > > > > Now, if I am given a dataset from group3, what method
> > > > (discriminant
> > > > > > analysis, clustering, maybe) is the best to cluster them
> > > > by using R.
> > > > > > The known info includes: 2 clusters, normal distribution (but the
> > > > > > parameters are unknown).
> > > > > >
> > > > > > Thanks,
> > > > > >
> > > > > > Ed
> > > > > >
> > > > > > ______________________________________________
> > > > > > R-help at stat.math.ethz.ch mailing list
> > > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > > PLEASE do read the posting guide!
> > > > http://www.R-project.org/posting-guide.html
> > > > >
> > > >
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > > > http://www.R-project.org/posting-guide.html
> > > >
> > > >
> > >
> > >
> > > ------------------------------------------------------------------------------
> > > Notice:  This e-mail message, together with any attachment...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ***********************************************************************
> Christian Hennig
> Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
> hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
> #######################################################################
> ich empfehle www.boag-online.de
> 
>



From dsmith at insightful.com  Fri Jan 28 17:15:09 2005
From: dsmith at insightful.com (David Smith)
Date: Fri, 28 Jan 2005 08:15:09 -0800
Subject: [R] JOB: Software Architect, Insightful Seattle
Message-ID: <EDAC416B87ECCA44BEAB4D0CF48034EF6B9380@se2kexch01.insightful.com>

Insightful is searching for a candidate to fill a technology leadership
position whose primary function is to drive the Insightful product and
solutions architectures.

Utilize your previous experience as you work closely with a seasoned
executive team and a world-class development team to help design and
implement a complete software architecture to deploy statistical analysis
into the enterprise. This position has the opportunity to have a tremendous
impact on the company, as you will drive projects at the leading edge of the
business. Insightful is an environment where you can make a difference
through innovation, drive and passion. 

Full details found at:
http://www.insightful.com/company/jobdescription.asp?JobID=53

-- 
David M Smith <dsmith at insightful.com>
Product Manager, Insightful Corp, Seattle WA
Tel: +1 (206) 802 2360
Fax: +1 (206) 283 6310

Insightful Corporation (www.insightful.com) provides analytical solutions for
text and data using S-PLUS, Insightful Miner, InFact and consulting services.



From bates at stat.wisc.edu  Fri Jan 28 18:05:38 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 28 Jan 2005 11:05:38 -0600
Subject: [R] Conflicts using Rcmdr, nlme and lme4
In-Reply-To: <41FA5CF9.9080602@acelerate.com>
References: <200501281015.j0SAF8Id009565@mail1.slu.se>	<41FA1BC0.5040601@stat.wisc.edu>
	<41FA5CF9.9080602@acelerate.com>
Message-ID: <41FA70E2.10400@stat.wisc.edu>

Kjetil Brinchmann Halvorsen wrote:
> Douglas Bates wrote:
> 
>>
>>
>> I can guarantee that this is the last time that I will fundamentally 
>> redesign the computational methods and data structures used in fitting 
>> mixed-effects models.  This is the fifth time I have done it from 
>> scratch and after this one I am quitting the business.
>>
> 
> Hope that does'nt mean you are quitting completely from statistical 
> computing and R?
> 

No, no.  Not at all.  It just means that I am tired of spending my time 
  thinking about computational methods for this particular model.  I 
think that doing five separate implementation of methods for the same 
problem, where each of the implementations is thousands of lines of 
code, is perhaps a sign of an obsessive compulsive disorder.  :-)

And I probably won't even keep that promise not to do it again.  As the 
title of the James Bond film cautioned, "Never say `never again'".

It's like the way that the omegahat project got named.  Several people 
including John Chambers, Duncan Temple Lang, Luke Tierney, Ross Ihaka 
and Robert Gentleman were discussing the idea of doing yet another 
implementation of an S-like language and Ross was feeling a bit drained 
from having worked on the last one, which is the core of R.  He insisted 
that he didn't want to mess around with alpha, beta versions, etc.  He 
wanted to go straight to omega, the final version.  We found that the 
domain omega.org was already taken so we settled on a statistical 
approximation to the last version, which was omegahat.



From paul.louisell at pw.utc.com  Fri Jan 28 18:05:36 2005
From: paul.louisell at pw.utc.com (Louisell, Paul T.)
Date: Fri, 28 Jan 2005 12:05:36 -0500
Subject: [R] Line types in the legend function
Message-ID: <B2F47383271169459B93D4654A522F1A03CF44DA@pusehe0r.eh.pweh.com>

Hi,

I'm using version 2.0.1 of R on a Windows 2000 platform. The legend function
has parameters _lty_ and _pch_ for drawing line types and point types in the
legend box, but I can't find any way of getting the patterns corresponding
to _type='b'_ in the _plot_ function. When you enter _type='b'_ as a
parameter in the plot function, you get an alternating series of dashes and
points. The closest I can come to duplicating this is to get the dash and
point lying on top of each other in the legend box. Any suggestions?

Thanks,

Paul Louisell
Pratt & Whitney
Statistician
TechNet: 435-5417
e-mail: paul.louisell at pw.utc.com



From samuel_mwalili at yahoo.com  Fri Jan 28 19:06:00 2005
From: samuel_mwalili at yahoo.com (Sameul M Mwalili)
Date: Fri, 28 Jan 2005 10:06:00 -0800 (PST)
Subject: [R] How to generate labels or names?
In-Reply-To: <47779aab0501270903ed12d56@mail.gmail.com>
Message-ID: <20050128180600.90942.qmail@web53404.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050128/34e670bb/attachment.pl

From Dax42 at web.de  Fri Jan 28 19:26:14 2005
From: Dax42 at web.de (dax42)
Date: Fri, 28 Jan 2005 19:26:14 +0100
Subject: [R] extracting from a data.frame
Message-ID: <17BCEBCA-715A-11D9-90CA-000393883D7E@web.de>

Hi,

I am sorry for this simple question, but... How do I extract something 
from a data.frame?
The following is my Problem:
I have got a dataframe "a" with various columns. One of those columns 
is called V3 and contains elements of the following levels:

 > levels(a$V3)
  [1] "C"   "CA"  "CB"  "CD"  "CD1" "CD2" "CE"  "CE1" "CE2" "CE3" "CG"
[12] "CG1" "CG2" "CH2" "CZ"  "CZ2" "CZ3" "N"   "ND1" "ND2" "NE"  "NE1"
[23] "NE2" "NH1" "NH2" "NZ"  "O"   "OD1" "OD2" "OE1" "OE2" "OG"  "OG1"
[34] "OH"  "OXT" "SD"  "SG"

Now I would like to get all rows of a, that have the value "O" in V3.
I tried a lot of different combinations of [... but I couldn't make it.

Any ideas?
Thanks a lot!
Dax.



From murdoch at stats.uwo.ca  Fri Jan 28 19:32:44 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 28 Jan 2005 18:32:44 +0000
Subject: [R] extracting from a data.frame
In-Reply-To: <17BCEBCA-715A-11D9-90CA-000393883D7E@web.de>
References: <17BCEBCA-715A-11D9-90CA-000393883D7E@web.de>
Message-ID: <k91lv0l8gima1pntn24rl6pmc1952c5c8a@4ax.com>

On Fri, 28 Jan 2005 19:26:14 +0100, dax42 <Dax42 at web.de> wrote :

>Hi,
>
>I am sorry for this simple question, but... How do I extract something 
>from a data.frame?
>The following is my Problem:
>I have got a dataframe "a" with various columns. One of those columns 
>is called V3 and contains elements of the following levels:
>
> > levels(a$V3)
>  [1] "C"   "CA"  "CB"  "CD"  "CD1" "CD2" "CE"  "CE1" "CE2" "CE3" "CG"
>[12] "CG1" "CG2" "CH2" "CZ"  "CZ2" "CZ3" "N"   "ND1" "ND2" "NE"  "NE1"
>[23] "NE2" "NH1" "NH2" "NZ"  "O"   "OD1" "OD2" "OE1" "OE2" "OG"  "OG1"
>[34] "OH"  "OXT" "SD"  "SG"
>
>Now I would like to get all rows of a, that have the value "O" in V3.
>I tried a lot of different combinations of [... but I couldn't make it.

a[a$V3 == "O", ]

or 

with(a, a[V3 == "O", ])

which makes more sense when the selection expression is more
complicated, because you don't need the a$ prefix on every column.

Duncan Murdoch

P.S. your return address fails.



From sdavis2 at mail.nih.gov  Fri Jan 28 19:32:43 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 28 Jan 2005 13:32:43 -0500
Subject: [R] extracting from a data.frame
In-Reply-To: <17BCEBCA-715A-11D9-90CA-000393883D7E@web.de>
References: <17BCEBCA-715A-11D9-90CA-000393883D7E@web.de>
Message-ID: <FF81CA4A-715A-11D9-B71B-000D933565E8@mail.nih.gov>

Look at ?subset.

Sean

On Jan 28, 2005, at 1:26 PM, dax42 wrote:

> Hi,
>
> I am sorry for this simple question, but... How do I extract something 
> from a data.frame?
> The following is my Problem:
> I have got a dataframe "a" with various columns. One of those columns 
> is called V3 and contains elements of the following levels:
>
> > levels(a$V3)
>  [1] "C"   "CA"  "CB"  "CD"  "CD1" "CD2" "CE"  "CE1" "CE2" "CE3" "CG"
> [12] "CG1" "CG2" "CH2" "CZ"  "CZ2" "CZ3" "N"   "ND1" "ND2" "NE"  "NE1"
> [23] "NE2" "NH1" "NH2" "NZ"  "O"   "OD1" "OD2" "OE1" "OE2" "OG"  "OG1"
> [34] "OH"  "OXT" "SD"  "SG"
>
> Now I would like to get all rows of a, that have the value "O" in V3.
> I tried a lot of different combinations of [... but I couldn't make it.
>
> Any ideas?
> Thanks a lot!
> Dax.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ccleland at optonline.net  Fri Jan 28 19:32:50 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 28 Jan 2005 13:32:50 -0500
Subject: [R] extracting from a data.frame
In-Reply-To: <17BCEBCA-715A-11D9-90CA-000393883D7E@web.de>
References: <17BCEBCA-715A-11D9-90CA-000393883D7E@web.de>
Message-ID: <41FA8552.5030506@optonline.net>

newdat <- subset(a, a$V3 == "O")

?subset

dax42 wrote:
> Hi,
> 
> I am sorry for this simple question, but... How do I extract something 
> from a data.frame?
> The following is my Problem:
> I have got a dataframe "a" with various columns. One of those columns is 
> called V3 and contains elements of the following levels:
> 
>  > levels(a$V3)
>  [1] "C"   "CA"  "CB"  "CD"  "CD1" "CD2" "CE"  "CE1" "CE2" "CE3" "CG"
> [12] "CG1" "CG2" "CH2" "CZ"  "CZ2" "CZ3" "N"   "ND1" "ND2" "NE"  "NE1"
> [23] "NE2" "NH1" "NH2" "NZ"  "O"   "OD1" "OD2" "OE1" "OE2" "OG"  "OG1"
> [34] "OH"  "OXT" "SD"  "SG"
> 
> Now I would like to get all rows of a, that have the value "O" in V3.
> I tried a lot of different combinations of [... but I couldn't make it.
> 
> Any ideas?
> Thanks a lot!
> Dax.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From helprhelp at gmail.com  Fri Jan 28 19:40:48 2005
From: helprhelp at gmail.com (WeiWei Shi)
Date: Fri, 28 Jan 2005 13:40:48 -0500
Subject: [R] Begginer with R
In-Reply-To: <60944.200.114.179.16.1106924824.squirrel@ifir.ifir.edu.ar>
References: <60944.200.114.179.16.1106924824.squirrel@ifir.ifir.edu.ar>
Message-ID: <cdf8178305012810407c81f272@mail.gmail.com>

http://www.liacc.up.pt/~ltorgo/DataMiningWithR/

might be a good start point.

also the book: MASS (4th Ed)

Check some previous lists and you will find some similar topics too. 

I used R just for like 2 weeks. Not sure of your level of knowledge in
data mining.
but hope this help.

Ed


On Fri, 28 Jan 2005 12:07:04 -0300 (ART), Ulises Cervino
<cervino at ifir.edu.ar> wrote:
> Hello all,
> 
> Im just beggining using R. I have read a couple of introductory documents
> but they are very general. Is there a document focused on classification?
> (this is what Ill be using R for). Examples would be just fine.
> Thanks in advance,
> 
> Ulises
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sfalcon at fhcrc.org  Fri Jan 28 19:47:00 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Fri, 28 Jan 2005 10:47:00 -0800
Subject: [R] Installing Problems
In-Reply-To: <ac0fd70d05012708486c7e52b8@mail.gmail.com>
References: <ac0fd70d05012708486c7e52b8@mail.gmail.com>
Message-ID: <20050128184700.GC11748@gerbil.fhcrc.org>

Hi Ashok,

On Thu, Jan 27, 2005 at 11:48:08AM -0500, Ashok Veeraraghavan wrote:
> I tried installing R on my MAC OS 10.3. After R installation I tried
> installing BioConductor which requires R. I ran into some problems
> with Bioconductor. Right now I want to remove (uninstall) all R and
> Bioconductor components from my machine and start afresh. Can somebody
> tell me how i can remove(uninstall) all R and Bioconductor components.

It is a little hard to answer that because you didn't tell us exactly
how you went about installing R and Bioconductor.

To remove the binary OS X installed R (and Bioconductor) I think it
suffices to delete the R application from /Applications and remove the R
framework from /Library/Frameworks.

For questions regarding Bioconductor, you may find the Bioconductor mail
list more receptive: https://stat.ethz.ch/mailman/listinfo/bioconductor

+ seth



From jlh599 at psu.edu  Fri Jan 28 20:24:05 2005
From: jlh599 at psu.edu (Jessica Higgs)
Date: Fri, 28 Jan 2005 14:24:05 -0500
Subject: [R] using RODBC
Message-ID: <5.2.0.9.2.20050128140619.02528a40@email.psu.edu>

I am trying to bring data into R from an excel spreadsheet in order to 
perform several statistical tests on it.  I was trying to use 
odbcConnectExcel in the RODBC package.  Once I am connected to the excel 
file, how do I select rows and columns from the file in order to analysis 
them in R.



From clint at ecy.wa.gov  Fri Jan 28 21:01:00 2005
From: clint at ecy.wa.gov (Clint Bowman)
Date: Fri, 28 Jan 2005 12:01:00 -0800 (PST)
Subject: [R] Exponential Fits to Distribution Tails
Message-ID: <Pine.LNX.4.44.0501281157070.7517-100000@aeolus.ecy.wa.gov>

EPA has suggested an exponential fit to the upper 10 percent of a PM10
distribution and using that fit to determine a once per year frequency of
occurrence.  My question is one of pedigree -- does such a technique have 
merit and status in the statistics community?  Or is there a better 
technique for determining the PM10 value that is expected to occur once 
per year?

TIA

Clint

-- 
Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Quality Modeler		INTERNET:	clint at math.utah.edu
Department of Ecology		VOICE:		(360) 407-6815
PO Box 47600			FAX:		(360) 407-7534
Olympia, WA 98504-7600



From krcabrer at unalmed.edu.co  Fri Jan 28 21:06:20 2005
From: krcabrer at unalmed.edu.co (Kenneth)
Date: Fri, 28 Jan 2005 15:06:20 -0500
Subject: [R] Direct sum of matrices
Message-ID: <opslbwsup3j2klo5@localhost.localdomain>

Hi R users:

How can I built a direct sum function of matrices in R?

I mean A(mxn), B(pxq), C(rxs),...

X<-ds(A,B,C,...)

X = [ A,  0,  0
            0,  B,  0
           0,   0,  C] ((m+p+r+...) x (n+q+s+...))

Thank you for your help.

Kenneth

-- 
Using Opera's revolutionary e-mail client: http://www.opera.com/m2/



From dr.mike at ntlworld.com  Fri Jan 28 21:12:53 2005
From: dr.mike at ntlworld.com (dr mike)
Date: Fri, 28 Jan 2005 20:12:53 -0000
Subject: [R] using RODBC
In-Reply-To: <5.2.0.9.2.20050128140619.02528a40@email.psu.edu>
Message-ID: <20050128201308.JFXP13480.aamta04-winn.mailhost.ntl.com@c400>

Have you consulted the R Data Import/Export Manual, available from the Rgui
via the Help tab?

Mike 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jessica Higgs
Sent: 28 January 2005 19:24
To: R-help at stat.math.ethz.ch
Subject: [R] using RODBC

I am trying to bring data into R from an excel spreadsheet in order to
perform several statistical tests on it.  I was trying to use
odbcConnectExcel in the RODBC package.  Once I am connected to the excel
file, how do I select rows and columns from the file in order to analysis
them in R.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From dalmiral at umich.edu  Fri Jan 28 21:22:21 2005
From: dalmiral at umich.edu (Daniel Almirall)
Date: Fri, 28 Jan 2005 15:22:21 -0500 (EST)
Subject: [R] Direct sum of matrices
In-Reply-To: <opslbwsup3j2klo5@localhost.localdomain>
References: <opslbwsup3j2klo5@localhost.localdomain>
Message-ID: <Pine.SOL.4.58.0501281520280.1403@rygar.gpcc.itd.umich.edu>


?bdiag

from package:assist may help you a bit.

HTH,
Danny



On Fri, 28 Jan 2005, Kenneth wrote:

> Hi R users:
>
> How can I built a direct sum function of matrices in R?
>
> I mean A(mxn), B(pxq), C(rxs),...
>
> X<-ds(A,B,C,...)
>
> X = [ A,  0,  0
>             0,  B,  0
>            0,   0,  C] ((m+p+r+...) x (n+q+s+...))
>
> Thank you for your help.
>
> Kenneth
>
> --
> Using Opera's revolutionary e-mail client: http://www.opera.com/m2/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>



From Avneet.Singh at graftech.com  Fri Jan 28 21:28:07 2005
From: Avneet.Singh at graftech.com (Singh, Avneet)
Date: Fri, 28 Jan 2005 15:28:07 -0500
Subject: [R] using RODBC
Message-ID: <A8722C0C0FB4D3118A13009027C3C82E042A811B@U742EXC1>

Would this function be useful to you. This imports the data from an excel
file and gives it to you as a data frame.

http://www.maths.lth.se/help/R/.R/library/gdata/html/read.xls.html

"I have no data yet. It is a capital mistake to theorize before one has
data. Insensibly one begins to twist facts to suit theories instead of
theories to suit facts."
~ Sir Arthur Conan Doyle (1859-1930), Sherlock Holmes 



-----Original Message-----
From: dr mike [mailto:dr.mike at ntlworld.com]
Sent: Friday, January 28, 2005 3:13 PM
To: 'Jessica Higgs'
Cc: R-help at stat.math.ethz.ch
Subject: RE: [R] using RODBC


Have you consulted the R Data Import/Export Manual, available from the Rgui
via the Help tab?

Mike 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jessica Higgs
Sent: 28 January 2005 19:24
To: R-help at stat.math.ethz.ch
Subject: [R] using RODBC

I am trying to bring data into R from an excel spreadsheet in order to
perform several statistical tests on it.  I was trying to use
odbcConnectExcel in the RODBC package.  Once I am connected to the excel
file, how do I select rows and columns from the file in order to analysis
them in R.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From arshia22 at yahoo.com  Fri Jan 28 21:30:53 2005
From: arshia22 at yahoo.com (ebashi)
Date: Fri, 28 Jan 2005 12:30:53 -0800 (PST)
Subject: [R] R for CGI
Message-ID: <20050128203053.93820.qmail@web81003.mail.yahoo.com>

Dear R Users;
Perl is the common language to write CGI scripts which
handle Forms. My question is that can R be as fast as
perl 
to do the same job(with using CGIwithR package). Is it
an optimal solution to connect R directly to a
commercial HTML webpages,
Sincerely,
Sean



From ggrothendieck at myway.com  Fri Jan 28 21:42:40 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 28 Jan 2005 20:42:40 +0000 (UTC)
Subject: [R] Direct sum of matrices
References: <opslbwsup3j2klo5@localhost.localdomain>
Message-ID: <loom.20050128T213801-953@post.gmane.org>

Kenneth <krcabrer <at> unalmed.edu.co> writes:

: 
: Hi R users:
: 
: How can I built a direct sum function of matrices in R?
: 
: I mean A(mxn), B(pxq), C(rxs),...
: 
: X<-ds(A,B,C,...)
: 
: X = [ A,  0,  0
:             0,  B,  0
:            0,   0,  C] ((m+p+r+...) x (n+q+s+...))
: 
: Thank you for your help.

Search r-help for blockdiag, adiag or bdiag.  

At that time I had posted a solution using zoo but the fill=
argument in merge.zoo has been added since then so here is an
update that's slightly shorter.  Suppose we have

m1 <- matrix(1:4, 2, 2)
m2 <- matrix(11:14, 2, 2)

# Then we create two time series with non-overlapping time
# intervals and merge them using a fill of zero.  You can
# use any number of zoo matrices in zoo, but here we just use two:

m <- merge( zoo(m1, 1:2), zoo(m2, 3:4), fill = 0)

# You may wish to clean the result up a bit with:

class(m) <- "matrix"; dimnames(m) <- NULL



From feldesmanm at pdx.edu  Fri Jan 28 21:36:43 2005
From: feldesmanm at pdx.edu (Marc R. Feldesman)
Date: Fri, 28 Jan 2005 12:36:43 -0800
Subject: [R] Why am I not getting "some" r-help posts?
Message-ID: <6.0.3.0.2.20050128122516.0331fb98@pop4.attglobal.net>

At the risk of being flamed for mortal stupidity, I'm trying to figure out 
what could have possibly changed (at my end almost certainly) that make 
Brian Ripley's posts (in particular) not show up on my mail server (at 
least not on the one where I receive my email).  All my email comes to a 
central university server from which it is forwarded to several different 
places where I can get my email more conveniently.  I've double, triple, 
and quadruple checked my .forward file and find nothing amiss [one line 
with the two servers I want the mail to go to.  This file hasn't been 
changed in more than a year]  (I don't use procmail or spamassasin on the 
central server, although they are setup and available for my use.  This is 
quite deliberate as I've got other effective spam filters on my "real" 
email systems).

So about a month ago, I noticed people responding to Prof Ripley's posts 
without me ever seeing the posts to which they were responding.  I looked 
on the main email server and found all of the originals there.  So clearly 
Brian's posts were making it out to my central mail server, from which they 
disappeared into the ether on their way to my other servers.  I started 
checking all the spam filters on the other two systems and couldn't see any 
obvious reason why this particular email address was causing trouble.  To 
be on the safe side, I deleted all the spam rules and deleted the entire 
file of rules (on both systems).  Despite this, those posts would not get 
forwarded to my preferred email systems.

I've gone back and checked with the sysadmins of our Unix system (Sequent) 
and they're clueless about what's going on.  It appears that the mail is 
being forwarded but is being swallowed somewhere downstream.

I realize this isn't an r-help problem, but with all the expertise on this 
board, I figured that someone here might have some suggestions of where to 
look.

FWIW, the email servers run either Linux or Windows XP Server 3.0 (or 
whatever it's called) and I use Eudora, Thunderbird, or webmail clients to 
read/respond to them.

Thanks for any help and/or suggestions for solving this mystery.




Dr. Marc R. Feldesman
Professor and Chairman Emeritus
Anthropology Department - Portland State University
email:  feldesmanm at pdx.edu
email:  feldesmanm at gmail.com
fax:    503-725-3905


"E-mail is not to be used to pass on factual information or important 
data.  It is for departmental use only"  Directive to Federal Employees



From Dax42 at web.de  Fri Jan 28 22:41:46 2005
From: Dax42 at web.de (dax42)
Date: Fri, 28 Jan 2005 22:41:46 +0100
Subject: [R] avoiding loops
Message-ID: <68827896-7175-11D9-90CA-000393883D7E@web.de>

Hi again,

thanks a lot for the quick answer. I just forgot the comma, always 
these stupid mistakes...

Anyways, as I said before, I have two data.frames containing about 1000 
rows and I would like to avoid looping through all of them...

In each data.frame are coordinates (x,y,z), so every row is giving the 
information on one single point.
I would like to calculate the distance from each point in the one frame 
to every point in the second...
But how to do this without loops, how to do it quickly? I would 
appreciate it very much, if someone would introduce me to the high art 
of R - avoiding loops :-).

Cheers, Dax.



From andy_liaw at merck.com  Fri Jan 28 22:54:53 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 28 Jan 2005 16:54:53 -0500
Subject: [R] avoiding loops
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5F0@usrymx25.merck.com>

Doesn't this work?

> d1 <- data.frame(x=rnorm(100), y=rnorm(100), z=rnorm(100))
> d2 <- data.frame(x=rnorm(100), y=rnorm(100), z=rnorm(100))
> dist.12 <- sqrt(rowSums((d1 - d2)^2))

Andy

> From: dax42
> 
> Hi again,
> 
> thanks a lot for the quick answer. I just forgot the comma, always 
> these stupid mistakes...
> 
> Anyways, as I said before, I have two data.frames containing 
> about 1000 
> rows and I would like to avoid looping through all of them...
> 
> In each data.frame are coordinates (x,y,z), so every row is 
> giving the 
> information on one single point.
> I would like to calculate the distance from each point in the 
> one frame 
> to every point in the second...
> But how to do this without loops, how to do it quickly? I would 
> appreciate it very much, if someone would introduce me to the 
> high art 
> of R - avoiding loops :-).
> 
> Cheers, Dax.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From p.dalgaard at biostat.ku.dk  Fri Jan 28 22:59:45 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Jan 2005 22:59:45 +0100
Subject: [R] Why am I not getting "some" r-help posts?
In-Reply-To: <6.0.3.0.2.20050128122516.0331fb98@pop4.attglobal.net>
References: <6.0.3.0.2.20050128122516.0331fb98@pop4.attglobal.net>
Message-ID: <x2vf9hqkoe.fsf@biostat.ku.dk>

"Marc R. Feldesman" <feldesmanm at pdx.edu> writes:

> At the risk of being flamed for mortal stupidity, I'm trying to figure
> out what could have possibly changed (at my end almost certainly) that
> make Brian Ripley's posts (in particular) not show up on my mail
> server (at least not on the one where I receive my email). 

Did you remember to turn off the Oxford-sarcasm filter? ;-)

        -p


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From sdutky at starpower.net  Sat Jan 29 00:16:08 2005
From: sdutky at starpower.net (Steve Dutky)
Date: Fri, 28 Jan 2005 18:16:08 -0500
Subject: [R] error in gmake CrossCompileBuild
Message-ID: <d7711fc4.c5f5f354.819d400@ms08.mrf.mail.rcn.net>

Dear all,

I expect that I am on thin ice trying to build this on 
freeBSD 4.10, however, after tweaking Makefile-rcb v11Oct04
/* MAKE=gmake; MINGW_CROSS = mingw-cross4 */, I encounter:

mingw32-gcc -isystem ~/RCrossBuild/cross-
tools/mingw32/include -O2 -Wall -pedantic -I../include -I. -
DHAVE_CONFIG_H -DR_DLL_BUILD  -c dynload.c -o dynload.o
dynload.c: In function `R_loadLibrary':
dynload.c:94: warning: implicit declaration of function 
`_controlfp'
dynload.c:94: error: `_MCW_IC' undeclared (first use in this 
function)
dynload.c:94: error: (Each undeclared identifier is reported 
only once
dynload.c:94: error: for each function it appears in.)
dynload.c:95: warning: implicit declaration of function 
`_clearfp'
dynload.c:99: error: `_MCW_EM' undeclared (first use in this 
function)
dynload.c:99: error: `_MCW_RC' undeclared (first use in this 
function)
dynload.c:99: error: `_MCW_PC' undeclared (first use in this 
function)
gmake[5]: *** [dynload.o] Error 1
gmake[4]: *** [../../bin/R.dll] Error 2
gmake[3]: *** [rbuild] Error 2
gmake[2]: *** [all] Error 2
gmake[2]: Leaving directory `~/RCrossBuild/WinR/R-
2.0.0/src/gnuwin32'
...<snip>...

I see _controlfp is declared in ~/RCrossBuild/cross-
tools/<snip>/include/float.h but does not appear to be 
defined elsewhere.

Any advice greatly appreciated.  

Thanks, Steve Dutky
sdutky at terpalum.umd.edu



From p.dalgaard at biostat.ku.dk  Sat Jan 29 00:36:53 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Jan 2005 00:36:53 +0100
Subject: [R] error in gmake CrossCompileBuild
In-Reply-To: <d7711fc4.c5f5f354.819d400@ms08.mrf.mail.rcn.net>
References: <d7711fc4.c5f5f354.819d400@ms08.mrf.mail.rcn.net>
Message-ID: <x2hdl1qg6i.fsf@biostat.ku.dk>

Steve Dutky <sdutky at starpower.net> writes:

> tools/<snip>/include/float.h but does not appear to be 
> defined elsewhere.
> 
> Any advice greatly appreciated.  

Not this bit I fear, but:

Please take it to the r-devel list; it is way beyond the level of
r-help. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From swanson at yellowstoneresearch.org  Sat Jan 29 01:21:23 2005
From: swanson at yellowstoneresearch.org (Alan Swanson)
Date: Fri, 28 Jan 2005 17:21:23 -0700
Subject: [R] test comparing spatial point patterns?
Message-ID: <20050129002114.C8D075FB6C@mail01.bridgeband.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050128/28f77849/attachment.pl

From andy_liaw at merck.com  Sat Jan 29 02:28:17 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 28 Jan 2005 20:28:17 -0500
Subject: [R] avoiding loops
Message-ID: <3A822319EB35174CA3714066D590DCD50994E5F2@usrymx25.merck.com>

I didn't read the question carefully.  You were asking all pairwise
distances between the two sets, right?

This isn't the most efficient way to do it, but it seems OK for problem of
the size you stated:

> pairdist <- function(m1, m2) {
+     idx1 <- rep(1:nrow(m1), each=nrow(m2))
+     idx2 <- rep(1:nrow(m2), nrow(m1))
+     d <- sqrt(rowSums((m1[idx1,] - m2[idx2,])^2))
+     dim(d) <- c(nrow(m1), nrow(m2))
+     d
+ }
> d1 <- matrix(rnorm(3e3), 1e3, 3)
> d2 <- matrix(rnorm(3e3), 1e3, 3)
> system.time(dm <- pairdist(d1, d2), gcFirst=TRUE)
[1] 0.73 0.13 0.92   NA   NA
> str(dm)
 num [1:1000, 1:1000] 2.44 2.21 2.45 1.58 3.58 ...

Andy


> From: Liaw, Andy
> 
> Doesn't this work?
> 
> > d1 <- data.frame(x=rnorm(100), y=rnorm(100), z=rnorm(100))
> > d2 <- data.frame(x=rnorm(100), y=rnorm(100), z=rnorm(100))
> > dist.12 <- sqrt(rowSums((d1 - d2)^2))
> 
> Andy
> 
> > From: dax42
> > 
> > Hi again,
> > 
> > thanks a lot for the quick answer. I just forgot the comma, always 
> > these stupid mistakes...
> > 
> > Anyways, as I said before, I have two data.frames containing 
> > about 1000 
> > rows and I would like to avoid looping through all of them...
> > 
> > In each data.frame are coordinates (x,y,z), so every row is 
> > giving the 
> > information on one single point.
> > I would like to calculate the distance from each point in the 
> > one frame 
> > to every point in the second...
> > But how to do this without loops, how to do it quickly? I would 
> > appreciate it very much, if someone would introduce me to the 
> > high art 
> > of R - avoiding loops :-).
> > 
> > Cheers, Dax.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any attachments, 
> contains information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New Jersey, USA 08889), and/or its 
> affiliates (which may be known outside the United States as 
> Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
> Banyu) that may be confidential, proprietary copyrighted 
> and/or legally privileged. It is intended solely for the use 
> of the individual or entity named on this message.  If you 
> are not the intended recipient, and have received this 
> message in error, please notify us immediately by reply 
> e-mail and then delete it from your system.
> --------------------------------------------------------------
> ----------------
>



From feldesmanm at pdx.edu  Sat Jan 29 03:31:52 2005
From: feldesmanm at pdx.edu (Marc R. Feldesman)
Date: Fri, 28 Jan 2005 18:31:52 -0800
Subject: [R] Why am I not getting "some" r-help posts?
In-Reply-To: <x2vf9hqkoe.fsf@biostat.ku.dk>
References: <6.0.3.0.2.20050128122516.0331fb98@pop4.attglobal.net>
	<x2vf9hqkoe.fsf@biostat.ku.dk>
Message-ID: <6.0.3.0.2.20050128182042.0291f1c8@psumail.pdx.edu>

At 01:59 PM 1/28/2005, Peter Dalgaard wrote:
 >"Marc R. Feldesman" <feldesmanm at pdx.edu> writes:
 >
 >> At the risk of being flamed for mortal stupidity, I'm trying to figure
 >> out what could have possibly changed (at my end almost certainly) that
 >> make Brian Ripley's posts (in particular) not show up on my mail
 >> server (at least not on the one where I receive my email).
 >
 >Did you remember to turn off the Oxford-sarcasm filter? ;-)
 >
 >        -p

that was the first one I turned off :-)

Really, I have NO filters whatsoever on the primary email server, which is 
why it lets through everything including Brian's emails to r-help.  What 
seems to be happening is that when emails get forwarded on to my "real" 
email systems (not the University's system but my private home network and 
my private office network), they simply vanish without a trace.  I don't 
use procmail, forward or spam assassin on any of these network systems and 
the email clients all have their own built-in filters.  If I manually 
forward Brian's messages from my University's main network they will now 
arrive at my office network, but not my home network.  But automatic 
forwarding doesn't send any of his messages anywhere.  On my home system, 
the one I'm writing from right now, I use Eudora Pro 6.0.3.  It has its own 
proprietary version of spam filtering (built on spam assassin).  Its rules 
and filters are easily accessible and I've renamed the whole filter file so 
that NOTHING gets filtered at all.  I've even turned off the built in spam 
filtering so that it will do nothing to process these messages.  Still no 
messages from Brian Ripley.

If I didn't have such high regard for Brian's comments, I wouldn't bother 
pursuing this as vigorously as I do.  After all, I still *can* read his 
comments if I go to the main mail server, but that really defeats the 
purpose of the forwarding system setup.

So I persist in trying to sort this all out.  I have talked in greater 
detail with our Unix team and they are investigating this on several 
parallel lines.  They are checking external filter configurations on their 
server to make certain that somewhere along the line Brian or Ripley (but 
not ox.uk) didn't get inadvertently killfiled as Peter (or someone else) 
suggested.

I'll post back when I get to the root of this.  Thanks for all the private 
and public suggestions.  I did really enjoy the "Brian Ripley sarcasm 
filter".  However, as a fan of sarcasm of the British variety, I rather 
revel in reading Brian's tart comments.  They sound an awful lot like some 
of things I write on student papers.

Cheers and thanks again,




Dr. Marc R. Feldesman
Professor and Chairman Emeritus
Anthropology Department - Portland State University
email:  feldesmanm at pdx.edu
email:  feldesmanm at gmail.com
fax:    503-725-3905


"E-mail is not to be used to pass on factual information or important 
data.  It is for departmental use only"  Directive to Federal Employees



From chenyong.cn at gmail.com  Sat Jan 29 07:53:55 2005
From: chenyong.cn at gmail.com (Willie Y. CHEN)
Date: Sat, 29 Jan 2005 14:53:55 +0800
Subject: [R] Database Connection Problem with RMySQL package
Message-ID: <879446cd050128225318452a96@mail.gmail.com>

Folks,

I failed to create a connection to the database under MySQL DBMS in
the R system via RMySQL's method dbConnection(...).

My setup is as follows:

Microsoft Windows XP 5.1.2600
MySQL 4.1.9
R 2.0.1
DBI 0.1-8
RMySQL 0.5-5 
Both of DBI and RMySQL packages were downloaded from bell lab: 

http://stat.bell-labs.com/RS-DBI/download/index.html

My case in R is as follows:

> library(DBI)
> library(RMySQL)
> mgr <- dbDriver("MySQL")
> con <- dbConnect(mgr, user="root", pass="*******", host="localhost", dbname="campus")
Error in mysqlNewConnection(drv, ...) : RS-DBI driver: (could not
connect root at localhost on dbname "campus")

What am I doing wrong with?... It is really an annoying question.
Thank you very much for your help and patience.

Yours truly,

Willie Y CHEN
-- 
************************************
Decisioneering Pty Ltd

PO Box 23
Doncaster, VIC 3108
Australia

O: +61-3-98558661
H: +86-21-64205688



From ripley at stats.ox.ac.uk  Sat Jan 29 09:37:55 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 29 Jan 2005 08:37:55 +0000 (GMT)
Subject: [R] error in gmake CrossCompileBuild
In-Reply-To: <x2hdl1qg6i.fsf@biostat.ku.dk>
References: <d7711fc4.c5f5f354.819d400@ms08.mrf.mail.rcn.net>
	<x2hdl1qg6i.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.61.0501290835180.1541@gannet.stats>

On Fri, 29 Jan 2005, Peter Dalgaard wrote:

> Steve Dutky <sdutky at starpower.net> writes:
>
>> tools/<snip>/include/float.h but does not appear to be
>> defined elsewhere.
>>
>> Any advice greatly appreciated.
>
> Not this bit I fear, but:
>
> Please take it to the r-devel list; it is way beyond the level of
> r-help.

And when you do, please tell us what the makefile is, what you are trying 
to cross-compile etc.  R does come with cross-compilation support for 
Windows, and those look rather like references to my cross-compiler, but 
I don't have any other idea of what this is about.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From statistician at jindan.homedns.org  Sat Jan 29 09:42:33 2005
From: statistician at jindan.homedns.org (Jindan Zhou)
Date: Sat, 29 Jan 2005 02:42:33 -0600
Subject: [R] lagged scatterplot within rows 
Message-ID: <Pine.CYG.4.58.0501290227220.3296@shenzhen>

Suppose the data is organized in a 10 by 10 matrix, each row is named R1
through R10, and each column is named C1 through C10, I need to do a
lagged scatterplot, for lag h = 1, 2, ..., the pairs will be chosen only
within all rows, i.e. the lags do not span across rows. What is the nicest
solution for the problem? Some tricks from lag.plot()?

Thanks for helping!



Jindan
Industrial Engineering
The University of Oklahoma


p.s.Just for your information, exactly I am trying to reproduce plots in
Isaaks E. H. and Srivastava R. M. (1989). An introduction to applied geostatistics.



From bates at stat.wisc.edu  Sat Jan 29 10:00:54 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 29 Jan 2005 03:00:54 -0600
Subject: [R] extracting from a data.frame
In-Reply-To: <41FA8552.5030506@optonline.net>
References: <17BCEBCA-715A-11D9-90CA-000393883D7E@web.de>
	<41FA8552.5030506@optonline.net>
Message-ID: <41FB50C6.7090703@stat.wisc.edu>

Chuck Cleland wrote:
> newdat <- subset(a, a$V3 == "O")

You can even use

newdat <- subset(a, V3 == "O")

because of the way that subset evaluates the "subset" expression.



From michael.beer at unifr.ch  Sat Jan 29 12:07:48 2005
From: michael.beer at unifr.ch (BEER Michael)
Date: Sat, 29 Jan 2005 12:07:48 +0100
Subject: [R] Database Connection Problem with RMySQL package
Message-ID: <24D0F1947691984E89F3151A7DC314DDAC2199@EXCHANGE2.unifr.ch>

> My setup is as follows:
> 
> Microsoft Windows XP 5.1.2600
> MySQL 4.1.9
> R 2.0.1
> DBI 0.1-8
> RMySQL 0.5-5
> 
> [...] 
>
> > dbname="campus")
> Error in mysqlNewConnection(drv, ...) : RS-DBI driver: (could 
> not connect root at localhost on dbname "campus")
> 
> What am I doing wrong with?... It is really an annoying question.
> Thank you very much for your help and patience.


Make sure your MySQL server accepts pre-4.1-style passwords for the user
you log on with (cf. http://dev.mysql.com/doc/mysql/en/old-client.html).
The MySQL library included in RMySQL 0.5-5 comes from the 4.0.18
distribution and uses an old authentication protocol that isn't
compatible with the one implemented in MySQL 4.1.x.

Hope this helps.

Michael



From christian.kamenik at ips.unibe.ch  Sat Jan 29 13:22:16 2005
From: christian.kamenik at ips.unibe.ch (Christian Kamenik)
Date: Sat, 29 Jan 2005 13:22:16 +0100
Subject: [R] glm and percentage data with many zero values
Message-ID: <41FB7FF8.8090608@ips.unibe.ch>

Dear R users,

I would like to summarize the answers I got to the following question:

I am interested in correctly testing effects of continuous  
environmental variables and ordered factors on bacterial abundance.  
Bacterial abundance is derived from counts and expressed as  percentage. 
My problem is that the abundance data contain many zero  values:
Bacteria <-  
c(2.23,0,0.03,0.71,2.34,0,0.2,0.2,0.02,2.07,0.85,0.12,0,0.59,0.02,2.3,0 
.29,0.39,1.32,0.07,0.52,1.2,0,0.85,1.09,0,0.5,1.4,0.08,0.11,0.05,0.17,0 
.31,0,0.12,0,0.99,1.11,1.78,0,0,0,2.33,0.07,0.66,1.03,0.15,0.15,0.59,0, 
0.03,0.16,2.86,0.2,1.66,0.12,0.09,0.01,0,0.82,0.31,0.2,0.48,0.15)

First I tried transforming the data (e.g., logit) but because of the  
zeros I was not satisfied. Next I converted the percentages into  
integer values by round(Bacteria*10) or ceiling(Bacteria*10) and  
calculated a glm with a Poisson error structure; however, I am not  very 
happy with this approach because it changes the original  percentage 
data substantially (e.g., 0.03 becomes either 0 or 1). The  same is true 
for converting the percentages into factors and  calculating a 
multinomial or proportional-odds model (anyway, I do not  know if this 
would be a meaningful approach).
I was searching the web and the best answer I could get was  
http://www.biostat.wustl.edu/archives/html/s-news/1998-12/ msg00010.html 
in which several persons suggested quasi-likelihood.  Would it be 
reasonable to use a glm with quasipoisson? If yes, how I  can I find the 
appropriate variance function? Any other suggestions?

> If you know the totals from which these "percentages" were derived,  
> then transform your Bacteria back to original observations and fit a  
> quasi-Poisson model with log(total) as an offset. That is:
>
> BCount <- round(tot * Bacteria)
> glm(Bcount  ~ x1+ x2 + offset(log(tot)), family=quasipoisson)
>
> cheers, jari oksanen 


> I have developed an R library for specificially dealing with positive
> continuous data with exact zeros.  For example, rainfall:  No rain
> means exactly zero is recorded, but when rain falls, a continuous
> amount is recorded (after suitable rounding).
>
> This library--available on CRAN--is called  tweedie.  The distributions
> used are Tweedie models, which belong to the EDM family and so
> can be used in generalized linear models.  The Tweedie models have
> a variance function  V(mu) = mu^p, for p not in the range (0, 1).
> For various values of p, we have:
>
>  Value of p          Distribution
> p <=0     Defined over whole real line
> p=0     Normal distribution
> 0 < p < 1     No distributions exist
> p=1     Poisson distribution (with phi=1)
> 1 < p < 2     Continuous over positive Y, with positive mass at Y=0
> p=2     Gamma distribution
> p >= 2     Continuous for positive Y
> p=3     Inverse Gaussian distribution
>
> Of particular interest are the distributions such that 1 < p < 2, 
> which can be seen as a Poisson sum of gamma random variables. They are 
> continuous for Y>0 with a positive probability that Y=0 exactly. For 
> this reason, the Tweedie densities with 1 < p < 2 have been called the 
> compound Poisson, compound gamma and the Poisson-gamma distribution.
>
> In your case, percentages with exact zeros may not exactly fall into
> this category because of the upper limit of 100%.  But provided there's
> very few values near 100%, the Tweedie models might be worth a try.
> (The data above seem to indicate few values near 100%.)
>
> Get the  tweedie  package from CRAN, or from
> http://www.sci.usq.edu.au/staff/dunn/twhtml/home.html
>
> You will also need the  statmod  package, also available on CRAN.
>
> All the best.
>
> P.
>
> -- 
> Dr Peter Dunn          (USQ CRICOS No. 00244B)
>   Web:    http://www.sci.usq.edu.au/staff/dunn
>   Email:  dunn @ usq.edu.au
> Opinions expressed are mine, not those of USQ.  Obviously...


> You might try with ZIP i.e. zero inflated poisson model. I did not 
> used it, but I have such data to work on. So if there is anyone hwo 
> can point how to do this in R - please. There is also a classs of ZINB 
> or something like that for zero inflated negative binomial models.
>
> Actually I just went on web and found a book from Simonoff "Analyzing 
> Categorical Data" and there are some examples in it for ZIP et al. 
> Look examples for sections 4.5 and 5.4
>
> http://www.stern.nyu.edu/~jsimonof/AnalCatData/Splus/analcatdata.s
> http://www.stern.nyu.edu/~jsimonof/AnalCatData/Splus/functions.s
>
> -- 
> Lep pozdrav / With regards,
>     Gregor GORJANC 


>The ZIP model can be fitted with Jim Lindsey's function fmr 
>from his gnlm library, see:
>
>http://popgen0146uns50.unimaas.nl/~jlindsey/rcode.html
>
>Bendix Carstensen
>
It turned out that the percentage data were calculated from 
concentrations resulting in positive continuous data with exact zeros. 
The Tweedie models did a fine job.

Many thanks, Christian Kamenik



From Dax42 at web.de  Sat Jan 29 13:53:52 2005
From: Dax42 at web.de (dax42)
Date: Sat, 29 Jan 2005 13:53:52 +0100
Subject: [R] unique rows
Message-ID: <D41130CD-71F4-11D9-90CA-000393883D7E@web.de>

Dear list,

I would like to extract from a matrix all those rows, that are unique.
By unique, I don't mean the unique that is accomplished by the function 
unique(), though...

Consider the following example:
 > h
      [,1] [,2]
[1,]    4    4
[2,]    1    4
[3,]    4    1

Now unique(h) returns exactly the same - because 1 4 and 4 1 is not the 
same for that function.
What I would like to see, though, are only the first two rows (or the 
first and the third, it does not matter).

Does anybody know how to do that?
Cheers, Dax.



From jfox at mcmaster.ca  Sat Jan 29 14:29:33 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 29 Jan 2005 08:29:33 -0500
Subject: [R] unique rows
In-Reply-To: <D41130CD-71F4-11D9-90CA-000393883D7E@web.de>
Message-ID: <20050129132927.OBHY2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Dax,

I'll bet that someone comes up with a better approach, but the following
does appear to work:

u <- unique(t(sapply(as.data.frame(t(h)), sort)))

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of dax42
> Sent: Saturday, January 29, 2005 7:54 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] unique rows
> 
> Dear list,
> 
> I would like to extract from a matrix all those rows, that are unique.
> By unique, I don't mean the unique that is accomplished by 
> the function unique(), though...
> 
> Consider the following example:
>  > h
>       [,1] [,2]
> [1,]    4    4
> [2,]    1    4
> [3,]    4    1
> 
> Now unique(h) returns exactly the same - because 1 4 and 4 1 
> is not the same for that function.
> What I would like to see, though, are only the first two rows 
> (or the first and the third, it does not matter).
> 
> Does anybody know how to do that?
> Cheers, Dax.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From pburns at pburns.seanet.com  Sat Jan 29 14:37:35 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Sat, 29 Jan 2005 13:37:35 +0000
Subject: [R] unique rows
In-Reply-To: <D41130CD-71F4-11D9-90CA-000393883D7E@web.de>
References: <D41130CD-71F4-11D9-90CA-000393883D7E@web.de>
Message-ID: <41FB919F.4080801@pburns.seanet.com>

There may be more efficient ways, but

unique(t(apply(h, 1, sort)))

does what I think you want.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

dax42 wrote:

> Dear list,
>
> I would like to extract from a matrix all those rows, that are unique.
> By unique, I don't mean the unique that is accomplished by the 
> function unique(), though...
>
> Consider the following example:
> > h
>      [,1] [,2]
> [1,]    4    4
> [2,]    1    4
> [3,]    4    1
>
> Now unique(h) returns exactly the same - because 1 4 and 4 1 is not 
> the same for that function.
> What I would like to see, though, are only the first two rows (or the 
> first and the third, it does not matter).
>
> Does anybody know how to do that?
> Cheers, Dax.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
>



From p.dalgaard at biostat.ku.dk  Sat Jan 29 15:21:59 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Jan 2005 15:21:59 +0100
Subject: [R] unique rows
In-Reply-To: <20050129132927.OBHY2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>
References: <20050129132927.OBHY2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <x2ekg4nwmw.fsf@biostat.ku.dk>

"John Fox" <jfox at mcmaster.ca> writes:

> Dear Dax,
> 
> I'll bet that someone comes up with a better approach, but the following
> does appear to work:
> 
> u <- unique(t(sapply(as.data.frame(t(h)), sort)))

Or maybe just

unique(t(apply(h,1,sort)))


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Ted.Harding at nessie.mcc.ac.uk  Sat Jan 29 15:26:31 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 29 Jan 2005 14:26:31 -0000 (GMT)
Subject: [R] unique rows
In-Reply-To: <D41130CD-71F4-11D9-90CA-000393883D7E@web.de>
Message-ID: <XFMail.050129142631.Ted.Harding@nessie.mcc.ac.uk>

On 29-Jan-05 dax42 wrote:
> Dear list,
> 
> I would like to extract from a matrix all those rows, that are unique.
> By unique, I don't mean the unique that is accomplished by the function
> unique(), though...
> 
> Consider the following example:
>  > h
>       [,1] [,2]
> [1,]    4    4
> [2,]    1    4
> [3,]    4    1
> 
> Now unique(h) returns exactly the same - because 1 4 and 4 1 is not the
> same for that function.
> What I would like to see, though, are only the first two rows (or the 
> first and the third, it does not matter).
> 
> Does anybody know how to do that?
> Cheers, Dax.

How about:

  h[!duplicated(t(apply(h,1,sort))),]
       [,1] [,2]
  [1,]    4    4
  [2,]    1    4

Better than

  unique(t(apply(h,1,sort)))
       [,1] [,2]
  [1,]    4    4
  [2,]    1    4

in general (though it comes to the same for your example)
since it preserves the order of elements in each row.

Cheers,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 29-Jan-05                                       Time: 14:26:31
------------------------------ XFMail ------------------------------



From bbands at gmail.com  Sat Jan 29 15:53:22 2005
From: bbands at gmail.com (John Bollinger)
Date: Sat, 29 Jan 2005 06:53:22 -0800
Subject: [R] Database Connection Problem with RMySQL package
In-Reply-To: <879446cd050128225318452a96@mail.gmail.com>
References: <879446cd050128225318452a96@mail.gmail.com>
Message-ID: <6e8360ad050129065320dc779c@mail.gmail.com>

On Sat, 29 Jan 2005 14:53:55 +0800, Willie Y. CHEN
<chenyong.cn at gmail.com> wrote:
> Folks,
> 
> I failed to create a connection to the database under MySQL DBMS in
> the R system via RMySQL's method dbConnection(...).
> 
> My setup is as follows:
> 
> Microsoft Windows XP 5.1.2600
> MySQL 4.1.9
> R 2.0.1
> DBI 0.1-8
> RMySQL 0.5-5
> Both of DBI and RMySQL packages were downloaded from bell lab:
> 
> http://stat.bell-labs.com/RS-DBI/download/index.html
> 
> My case in R is as follows:
> 
> > library(DBI)
> > library(RMySQL)
> > mgr <- dbDriver("MySQL")
> > con <- dbConnect(mgr, user="root", pass="*******", host="localhost", dbname="campus")
> Error in mysqlNewConnection(drv, ...) : RS-DBI driver: (could not
> connect root at localhost on dbname "campus")
> 
> What am I doing wrong with?... It is really an annoying question.
> Thank you very much for your help and patience.
> 
> Yours truly,
> 
> Willie Y CHEN
> --

Check RMySQL docs for how to use the .my.cnf file to make the
connection. We could not do it inline either, but when we put the info
into the file RMySQL worked fine.

See page three here:

http://cran.r-project.org/doc/packages/RMySQL.pdf

    jab

-- 
John Bollinger, CFA, CMT
www.BollingerBands.com

If you advance far enough, you arrive at the beginning.



From owzar001 at mc.duke.edu  Sat Jan 29 16:00:22 2005
From: owzar001 at mc.duke.edu (Kouros Owzar)
Date: Sat, 29 Jan 2005 10:00:22 -0500
Subject: [R] Kouros Owzar is ooo.
Message-ID: <OF7F1D309F.2A03743B-ON85256F98.00526E70-85256F98.00526E71@notes.duke.edu>

I will be out of the office starting  01/28/2005 and will not return until
02/02/2005.



From atp at piskorski.com  Sat Jan 29 16:35:54 2005
From: atp at piskorski.com (Andrew Piskorski)
Date: Sat, 29 Jan 2005 10:35:54 -0500
Subject: [R] Re: [R-sig-finance] R for CGI
In-Reply-To: <20050128203053.93820.qmail@web81003.mail.yahoo.com>
References: <20050128203053.93820.qmail@web81003.mail.yahoo.com>
Message-ID: <20050129153554.GA21405@piskorski.com>

On Fri, Jan 28, 2005 at 12:30:53PM -0800, ebashi wrote:

> Perl is the common language to write CGI scripts which handle
> Forms. My question is that can R be as fast as perl to do the same
> job(with using CGIwithR package). Is it an optimal solution to
> connect R directly to a commercial HTML webpages,

First of all, why are you asking this on the r-sig-finance list?  The
question does not belong there.

Secondly, if you care about speed and "optimal solutions", CGI is
absolutely the last thing you want to use, regardless of whether you
write your scripts in Perl, R, or any other language.

For high-performance dnamic web pages, the most typical approach is to
embed a scripting language interpretor directly into the web server -
Tcl for AOLserver, mod_perl for Apache, etc.  Alternative approaches
include designs like FastCGI.  This is basic web stuff that was all
figured out long ago, perhaps c. 1997.  (Go google and read up on it.)

I don't have any links handy, but there are definitely some existing R
projects which let a web server efficiently evaluate R code by passing
it to an already running R process/server.  That's what most people
want, not to build a sophisticated dynamic website entirely in R.

On the general web front, here are some ancient (but good)
introductions to some of the basic concepts:

  http://philip.greenspun.com/panda/
  http://philip.greenspun.com/wtr/aolserver/introduction-1.html

And the docs for one good current toolkit which uses all those ideas:

  http://openacs.org/doc/

-- 
Andrew Piskorski <atp at piskorski.com>
http://www.piskorski.com/



From edd at debian.org  Sat Jan 29 17:53:49 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 29 Jan 2005 10:53:49 -0600
Subject: [R] Re: [R-sig-finance] R for CGI
In-Reply-To: <20050129153554.GA21405@piskorski.com>
References: <20050128203053.93820.qmail@web81003.mail.yahoo.com>
	<20050129153554.GA21405@piskorski.com>
Message-ID: <16891.49053.327557.289290@basebud.nulle.part>


On 29 January 2005 at 10:35, Andrew Piskorski wrote:
| On Fri, Jan 28, 2005 at 12:30:53PM -0800, ebashi wrote:
| 
| > Perl is the common language to write CGI scripts which handle
| > Forms. My question is that can R be as fast as perl to do the same
| > job(with using CGIwithR package). Is it an optimal solution to
| > connect R directly to a commercial HTML webpages,
| 
| First of all, why are you asking this on the r-sig-finance list?  The
| question does not belong there.

Yup, and your's truly, with his r-sig-finance listmaster hat on, has
subsequently unsubscribed Mr "ebashi" from r-sig-finance as he has 

 * repeatedly crossposted (despite strong hints that this is not looked upon
   too kindly),
 * repeatedly asked essentially the same question, and nevertheless 
 * continues to persistently ignore to good advice given to him.

Going e.g. to this listarchive which can sort by author, you see for January
(URL is http://tolstoy.newcastle.edu.au/R/help/05/01/author.html)

  # ebashi

      * [R] R for CGI (29 Jan 2005)
      * [R] How to make R faster? (26 Jan 2005)
      * [R] CGIwithR (17 Jan 2005)

along with another R/PHP post in December (URL
http://tolstoy.newcastle.edu.au/R/help/04/12/author.html) 

  # ebashi

      * [R] R&PHP (29 Dec 2004)
      * [R] xy_plot (01 Dec 2004)

If you read those threads -- which the original poster clearly must have
avoided at almost all cost -- you'll find many good points, answers and tips
for further references.

But no, "ebashi" rather asks the same question over and over and over. For
good measure, I also got once personally to my inbox.

| Secondly, if you care about speed and "optimal solutions", CGI is
| absolutely the last thing you want to use, regardless of whether you
| write your scripts in Perl, R, or any other language.
| 
| For high-performance dnamic web pages, the most typical approach is to
| embed a scripting language interpretor directly into the web server -
| Tcl for AOLserver, mod_perl for Apache, etc.  Alternative approaches
| include designs like FastCGI.  This is basic web stuff that was all
| figured out long ago, perhaps c. 1997.  (Go google and read up on it.)
| 
| I don't have any links handy, but there are definitely some existing R
| projects which let a web server efficiently evaluate R code by passing
| it to an already running R process/server.  That's what most people
| want, not to build a sophisticated dynamic website entirely in R.
| 
| On the general web front, here are some ancient (but good)
| introductions to some of the basic concepts:
| 
|   http://philip.greenspun.com/panda/
|   http://philip.greenspun.com/wtr/aolserver/introduction-1.html
| 
| And the docs for one good current toolkit which uses all those ideas:
| 
|   http://openacs.org/doc/

All very good points.

Dirk

-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From msck9 at mizzou.edu  Sat Jan 29 18:03:08 2005
From: msck9 at mizzou.edu (msck9@mizzou.edu)
Date: Sat, 29 Jan 2005 11:03:08 -0600
Subject: [R] A "rude" question
In-Reply-To: <173924d3.c546a2d8.8210600@expms6.cites.uiuc.edu>
References: <173924d3.c546a2d8.8210600@expms6.cites.uiuc.edu>
Message-ID: <20050129170308.GA13013@localhost>

Thanks for all the good points. I raised the question not because I doubt the R
system, but I think it is a common point for all the open code software.
Personally I really appreciate the open source software, and nowadays,
there are more and more open source, especially in the academic area. 

Ming
On Thu, Jan 27, 2005 at 09:21:12AM -0600, Tim F Liao wrote:
> I'm in agreement with Tom with respect to all the points he
> made but two in particular:
> 
> Open code: very useful and much easier (than other software)
> to make sure the trustworthiness of the function/library.  I
> often do go into the code and make sure this is what I want
> and it is a good way to find out the "meaning" of certain
> parts of the output and to learn others' programming tricks. 
> And that's the power of R.
> 
> Pedigree of the contributors: top-notch.  I remember finding a
> "bug" (having to do with detecting heteroscedasticity) in SAS
> back in the early 90s and communicated to a SAS tech.  SAS was
> considered the industry's standard back then, but contributed
> mostly by professonal programmers.  In comparison, R's
> libraries are contributed by statisticians who are at the
> forefront of statistical methods research.
> 
> Tim
> 
> ---- Original message ----
> >Date: Thu, 27 Jan 2005 14:15:31 +0800
> >From: "Mulholland, Tom" <Tom.Mulholland at dpi.wa.gov.au>  
> >Subject: RE: [R] A "rude" question  
> >To: <msck9 at mizzou.edu>, <r-help at stat.math.ethz.ch>
> >
> >What makes you trust any software? 
> >
> >There are some obvious points. First of all the code is open
> so if you know enough you can actually read the code and make
> sure it does what you want. Secondly you can replicate a
> process using two pieces of software and compare the results.
> You can check the archives and you will find a number of posts
> that talk about the results produced by R and how they compare
> with other software. Typically R versus Excel or R versus SPSS
> / SAS. Just be careful as different answers does not
> automatically mean one is wrong, and it certainly doesn't mean
> R is wrong.
> >
> >Excel computes =ROUND(2.5,0) to be 3
> >R computes round(2.5) to be 2
> >
> >As I understand it both are right, they are just using
> different standards. I however have always used the latter and
> rounded to the even number where the figure to be rounded lies
> exactly at the halfway mark.
> >
> >Hang around this list for a short time and it will become
> evident that if this software didn't work; the people using it
> would have stopped using it long ago.
> >
> >Forget the commercial versus open software arguments that
> raise their head from time to time. The question is how well a
> piece of software is written / maintained & supported and not
> issues of payment or the greater good. There is some woeful 
> freeware, just as there is some woeful commercial products.
> >
> >The pedigree of the contributors to the base package is hard
> to beat. I wouldn't know the pedigree of those who write the
> other stats programmes, but I assume that R contributors are
> right in there, with the best.
> >
> >As to packages. They must vary with quality, and people do
> make mistakes. If you have something that in modern parlance
> is "mission critical" it wouldn't matter which product you
> had, you would test it to see that it fitted your requirements.
> >
> >You have raised a question that is often ignored or assumed.
> But to really know the answer for yourself you need to test it
> yourself or rely upon others that you trust. Whenever I start
> using a package I make sure it does not just what it states it
> can do, but also that it does what I want it to do.
> >
> >Tom
> >
> >
> >> -----Original Message-----
> >> From: msck9 at mizzou.edu [mailto:msck9 at mizzou.edu]
> >> Sent: Thursday, 27 January 2005 1:10 PM
> >> To: r-help at stat.math.ethz.ch
> >> Subject: [R] A "rude" question
> >> 
> >> 
> >> Dear all, 
> >>  I am beginner using R. I have a question about it. When
> you use it,
> >>  since it is written by so many authors, how do you know
> that the
> >>  results are trustable?(I don't want to affend anyone, also
> I trust
> >>  people). But I think this should be a question.
> >> 
> >>  Thanks,
> >>  Ming
> >> 
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! 
> >> http://www.R-project.org/posting-guide.html
> >>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From fws4 at cdrh.fda.gov  Sat Jan 29 18:13:51 2005
From: fws4 at cdrh.fda.gov (Frank Samuelson)
Date: Sat, 29 Jan 2005 12:13:51 -0500
Subject: [R] avoiding loops
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E5F0@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E5F0@usrymx25.merck.com>
Message-ID: <ctggce$im6$1@sea.gmane.org>

Along the lines of this thread,
is there a general apply type function that
allows me to take one vector at a time from a matrix (or
row from a data frame) and another vector from another matrix
and apply them on a general function?  Sort of a multidimensional
mapply, or an 'inner' routine where you can define the
function (like you can for  'outer'.)

I've found that I do this regularly in my coding and I usually
end up using indices (which are pretty fast in R):
inner<-function(a,b,FUN=function(x,y) sum(x*y))
   sapply(1:nrow(a),function(i) FUN(a[i,],b[,i]))

Is there existing routine that does this?

Thanks.

-Frank


Liaw, Andy wrote:
> Doesn't this work?
> 
> 
>>d1 <- data.frame(x=rnorm(100), y=rnorm(100), z=rnorm(100))
>>d2 <- data.frame(x=rnorm(100), y=rnorm(100), z=rnorm(100))
>>dist.12 <- sqrt(rowSums((d1 - d2)^2))
> 
> 
> Andy
> 
> 
>>From: dax42
>>
>>Hi again,
>>
>>thanks a lot for the quick answer. I just forgot the comma, always 
>>these stupid mistakes...
>>
>>Anyways, as I said before, I have two data.frames containing 
>>about 1000 
>>rows and I would like to avoid looping through all of them...
>>
>>In each data.frame are coordinates (x,y,z), so every row is 
>>giving the 
>>information on one single point.
>>I would like to calculate the distance from each point in the 
>>one frame 
>>to every point in the second...
>>But how to do this without loops, how to do it quickly? I would 
>>appreciate it very much, if someone would introduce me to the 
>>high art 
>>of R - avoiding loops :-).
>>
>>Cheers, Dax.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From maillists at visiv.co.uk  Sat Jan 29 14:15:54 2005
From: maillists at visiv.co.uk (Graham Jones)
Date: Sat, 29 Jan 2005 13:15:54 +0000
Subject: [R] Name conflicts when passing arguments for one function to
	another
Message-ID: <s7KqDWAKy4+BFwI6@visiv.co.uk>

I am fairly new to R. I find it surprising that

f <- function(x,a) {x-a}
uniroot(f, c(0,1), a=.5)

works, but

integrate(f, 0, 1, a=.5)

gives an error: Error in integrate(f, 0, 1, a = 0.5) : argument 4
matches multiple formal arguments

What is the best way of avoiding such surprises? Is there a way of
telling integrate() that the 'a' argument is for f()?

If I wrote my own function along the lines of uniroot() or integrate()
is there a better way of passing on arguments?

-- 
Graham Jones, author of SharpEye Music Reader
http://www.visiv.co.uk
21e Balnakeil, Durness, Lairg, Sutherland, IV27 4PT, Scotland, UK



From p.dalgaard at biostat.ku.dk  Sat Jan 29 19:20:32 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Jan 2005 19:20:32 +0100
Subject: [R] Name conflicts when passing arguments for one function to
	another
In-Reply-To: <s7KqDWAKy4+BFwI6@visiv.co.uk>
References: <s7KqDWAKy4+BFwI6@visiv.co.uk>
Message-ID: <x2acqsnllb.fsf@biostat.ku.dk>

Graham Jones <maillists at visiv.co.uk> writes:

> I am fairly new to R. I find it surprising that
> 
> f <- function(x,a) {x-a}
> uniroot(f, c(0,1), a=.5)
> 
> works, but
> 
> integrate(f, 0, 1, a=.5)
> 
> gives an error: Error in integrate(f, 0, 1, a = 0.5) : argument 4
> matches multiple formal arguments
> 
> What is the best way of avoiding such surprises? Is there a way of
> telling integrate() that the 'a' argument is for f()?
> 
> If I wrote my own function along the lines of uniroot() or integrate()
> is there a better way of passing on arguments?

It's mainly a design issue. Had the "..." in the definition of
integrate been further to the left, then your problem wouldn't exist,
but you wouldn't be able to use abbreviations for abs.tol and friends. 

A simple (if tedious) workaround is

integrate(f, 0, 1, a=.5, abs.tol=.Machine$double.eps^0.25, aux=NULL)

a more thoroughly effective one could be

myintegrate <- function (f, lower, upper, ..., subdivisions = 100,
    rel.tol = .Machine$double.eps^0.25, abs.tol = rel.tol,
    stop.on.error = TRUE, keep.xy = FALSE, aux = NULL) 
 integrate(f, lower, upper, subdivisions, rel.tol, abs.tol, 
             stop.on.error, keep.xy, aux, ...)


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Sat Jan 29 19:34:16 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 29 Jan 2005 18:34:16 +0000 (GMT)
Subject: [R] Name conflicts when passing arguments for one function to
	another
In-Reply-To: <s7KqDWAKy4+BFwI6@visiv.co.uk>
References: <s7KqDWAKy4+BFwI6@visiv.co.uk>
Message-ID: <Pine.LNX.4.61.0501291825420.21065@gannet.stats>

On Sat, 29 Jan 2005, Graham Jones wrote:

> I am fairly new to R. I find it surprising that
>
> f <- function(x,a) {x-a}
> uniroot(f, c(0,1), a=.5)
>
> works, but
>
> integrate(f, 0, 1, a=.5)
>
> gives an error: Error in integrate(f, 0, 1, a = 0.5) : argument 4
> matches multiple formal arguments
>
> What is the best way of avoiding such surprises? Is there a way of
> telling integrate() that the 'a' argument is for f()?
>
> If I wrote my own function along the lines of uniroot() or integrate()
> is there a better way of passing on arguments?

You are being caught by partial matching. Ideally integrate would have arg 
list

function (f, lower, upper, ..., subdivisions = 100,
     rel.tol = .Machine$double.eps^0.25,
     abs.tol = rel.tol, stop.on.error = TRUE, keep.xy = FALSE,
     aux = NULL)

(cf args(integrate)).  But the original author did not do it that way.
[The point is that args after ... have to be given in full.]

You can resolve this by explicitly giving abs.tol and aux:

>  integrate(f, 0, 1, abs.tol =1e-4, aux=NULL, a=.5)
1.758178e-18 with absolute error < 2.8e-15

or not using an argument name that has no meaning and is so subject to 
mis-matching.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Sat Jan 29 19:51:01 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 29 Jan 2005 13:51:01 -0500 (EST)
Subject: [R] avoiding loops
Message-ID: <20050129185101.80F343A5D@mprdmxin.myway.com>



From:   Frank Samuelson <fws4 at cdrh.fda.gov>
> Along the lines of this thread,
> is there a general apply type function that
> allows me to take one vector at a time from a matrix (or
> row from a data frame) and another vector from another matrix
> and apply them on a general function? Sort of a multidimensional
> mapply, or an 'inner' routine where you can define the
> function (like you can for 'outer'.)
> 
> I've found that I do this regularly in my coding and I usually
> end up using indices (which are pretty fast in R):
> inner<-function(a,b,FUN=function(x,y) sum(x*y))
> sapply(1:nrow(a),function(i) FUN(a[i,],b[,i]))
> 
> Is there existing routine that does this?
> 

inner <- function(a,b,f) {
	f <- match.fun(f)
	apply(b,2,function(x)apply(a,1,function(y)f(x,y)))
}

# e.g. 
inner(matrix(1:4,2), matrix(4:1,2), crossprod)

matrix(1:4,2) %*% matrix(4:1,2) # same



From je_lemaitre at hotmail.com  Sat Jan 29 20:14:29 2005
From: je_lemaitre at hotmail.com (=?iso-8859-1?B?Suly9G1lIExlbWHudHJl?=)
Date: Sat, 29 Jan 2005 14:14:29 -0500
Subject: [R] Bootstrapped eigenvector
Message-ID: <BAY103-DAV32A89611A04BDDED4102E907A0@phx.gbl>

Hello alls,

I found in the literature a technique that has been evaluated as one of the
more robust to assess statistically the significance of the loadings in a
PCA: bootstrapping the eigenvector (Jackson, Ecology 1993, 74: 2204-2214;
Peres-Neto and al. 2003. Ecology 84:2347-2363). However, I'm not able to
transform by myself the following steps into a R program, yet?

Can someone could help me with this?

I thank you very much by advance.

Here are the steps that I need to perform:
1) Resample 1000 times with replacement entire raws from the original data
sets (7 variables, 126 raws)
2) Conduct a PCA on each bootstrapped sample
3) To prevent axis reflexion and/or axis reordering in the bootstrap, here
are two more steps for each bootstrapped sample
3a) calculate correlation matrix between the PCA scores of the original and
those of the bootstrapped sample
3b) Examine whether the highest absolute correlation is between the
corresponding axis for the original and bootstrapped samples. When it is not
the case, reorder the eigenvectors. This means that if the highest
correlation is between the first original axis and the second bootstrapped
axis, the loadings for the second bootstrapped axis and use to estimate the
confidence interval for the original first PC axis.
4) Determine the p value for each loading. Obtained as follow: number of
loadings >=0 for loadings that were positive in the original matrix divided
by the number of boostrap samples (1000) and/or number of loadings =<0 for
loadings that were negative in the original matrix divided by the number of
boostrap samples (1000).



Thanks again

J?r?me Lema?tre

?tudiant au doctorat
Chaire Industrielle Sylvicuture-Faune, For?t Bor?ale, C?te-Nord
& D?partement de biologie,
Facult? des sciences et de g?nie
Pavillon Alexandre-Vachon
Universit? Laval
Qu?bec, QC  G1K 7P4
t?l : (418) 656-2131 poste 2917
Local : VCH-2044
Courriel: jerome.lemaitre.1 at ulaval.ca



From david.meyer at wu-wien.ac.at  Sun Jan 30 00:08:09 2005
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Sun, 30 Jan 2005 00:08:09 +0100
Subject: [R] read.matrix.csr bug (e1071)?
Message-ID: <20050130000809.267b032e.david.meyer@wu-wien.ac.at>

This is a bug, thanks for pointing this out.

Fixed for the next release of e1071.

David

-----------------


Hello,

I would like to read and write sparse matrices using the
functions write.matrix.csr() and read.matrix.csr()
of the package e1071. Writing is OK but reading back the
matrix fails:

	x <- rnorm(100)
	m <- matrix(x, 10)
	m[m < 0.5] <- 0
	m.csr <- as.matrix.csr(m)
	write.matrix.csr(m, "sparse.dat")
	read.matrix("sparse.dat")

	Error in initialize(value, ...) : Can't use object of class "integer"
in new():  Class "matrix.csr" does not extend that class

Is something wrong with the code above or it must be
considered as a bug?

Best regards,

Peter



From mgleahy at hotmail.com  Sun Jan 30 03:31:20 2005
From: mgleahy at hotmail.com (Mike Leahy)
Date: Sat, 29 Jan 2005 21:31:20 -0500
Subject: [R] New user...tips for spdep?
Message-ID: <BAY102-DAV15D03ACA7FE8E704C263C5BC7B0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050129/398d2093/attachment.pl

From stevec at berl.ab.ca  Sun Jan 30 03:55:22 2005
From: stevec at berl.ab.ca (Steve Cumming)
Date: Sat, 29 Jan 2005 19:55:22 -0700
Subject: [R] Testing Poisson GLMs with independent data: what's the Right
	Thing To Do?
Message-ID: <EBEGIDABEKPPANCGPHDMCEILDMAA.stevec@berl.ab.ca>

Folks, my question is not R-specific, but I've struck out twice on
sci.stat.consult, so I'm turning to the R community. Even if it's a silly
question, I expect that someone present will probably tell me so...

I have been using multiple Poisson GLMs and similar count-re?gression models
to analyse forest songbird abundance data. Many of the spe?cies-level models
seem to fit the data pretty well.

My next task is to validate/verify/test these models using an independen?t
dataset collected for this purpose (no, really!) It seems obvious that I
should apply predict.glm() to the new covariates and then somehow compare
the observed values to the predicted expectations, but I don't know how
exactly. Some specific questions:

	-what comparisons or performance measures are appropriate?
	-how should the results be interpreted?
	-is there some other (better) way to use the new data?
	-am I overlooking something big?

Also, the covariates in the t?raining and validation datasets are not even
approximately identically d?istributed (this was on purpose, for reasons I
will gladly explain to anyone interested). I expect this must matter, but
how?

My bibles (e.g. ?Cameron and Trivedi, McCullagh and Nelder) are silent on
these points, and? I can find nothing on the Web or the obvious list
archives (nothing I recognise, anyway). If any read?er of this group can
offer advice, suggestions, or references, I'd s?ure appreciate it.


Best regards

Steve Cumming
Boreal Ecosystems Research Ltd.
http://www.berl.ab.ca



From james.muller at internode.on.net  Sun Jan 30 07:01:01 2005
From: james.muller at internode.on.net (James Muller)
Date: Sun, 30 Jan 2005 17:01:01 +1100
Subject: [R] Error: cannot allocate vector of size... but with a twist
In-Reply-To: <Pine.LNX.4.61.0501280653190.18916@gannet.stats>
References: <41F9D7C8.2010606@internode.on.net>
	<Pine.LNX.4.61.0501280653190.18916@gannet.stats>
Message-ID: <41FC781D.6040605@internode.on.net>

Apparently not. I had to re-run things (after some modifications) to 
double-check the 32-bit theory (had to check whether I'd used 3.5GB or 
3GB of swap at crash time).

R crashed with the same error (Error: cannot allocate vector of size 145 
Kb), and here is the memory usage (of my whole system, not just R) (note 
I have >4GB swap partition and 0.5GBRAM):
  RAM: 459MB
  Swap: 2.8GB
  Total: 3.2GB
So there should be 0.8GB left to eat before any failure...??

I had garbage collection giving me output as we go; here is the output 
(truncated in the middle).

[R is initialized with nothing in memory here]
 > load("/mnt/projects/cdata/data/tmpb0.RData") # ~200MB uncompressed data
 > load("/mnt/projects/cdata/data/tmpb1.RData") # ~400MB uncompressed data
 > cdata01.data <- cbind(c.b.0,c.b.1) # bind all the loaded data together
Garbage collection 1531 = 40+13+1478 (level 2) ...
361503 cons cells free (44%)
2.3 Mbytes of heap free (0%)
Garbage collection 1532 = 40+13+1479 (level 2) ...
361488 cons cells free (44%)
2.3 Mbytes of heap free (0%)
[...]
Garbage collection 2281 = 41+13+2227 (level 2) ...
350235 cons cells free (42%)
2.3 Mbytes of heap free (0%)
Garbage collection 2282 = 41+13+2228 (level 2) ...
350220 cons cells free (42%)
2.3 Mbytes of heap free (0%)
Error: cannot allocate vector of size 145 Kb
 >

This is really frustrating. This little bit took a good 90 mins to crash.

Once again, 32bit Linux OS (RH9), 4GB swap, 0.5GB RAM, R2.0

Any theories?

James


Prof Brian Ripley wrote:

> On Fri, 28 Jan 2005, James Muller wrote:
>
>> Hi,
>>
>> I have a memory problem, one which I've seen pop up in the list a few 
>> times, but which seems to be a little different. It is the Error: 
>> cannot allocate vector of size x problem. I'm running R2.0 on RH9.
>>
>> My R program is joining big datasets together, so there are lots of 
>> duplicate cases of data in memory. This (and other tasks) prompted me 
>> to... expand... my swap partition to 16Gb. I have 0.5Gb of regular, 
>> fast DDR. The OS seems to be fine accepting the large amount of 
>> memory, and I'm not restricting memory use or vector size in any way.
>>
>> R chews up memory up until the 3.5Gb area, then halts. Here's the 
>> last bit of output:
>
>
> You have, presumably, a 32-bit computer with a 4GB-per-process memory 
> limit.  You have hit it (you get less than 4Gb as the OS services need 
> some and there is some fragmentation).  The last failed allocation may 
> be small, as you see, if you are allocating lots of smallish pieces.
>
> The only way to overcome that is to use a 64-bit OS and version of R.
>
> What was the `twist' mentioned in the title?  You will find a similar 
> overall limit mentioned about weekly on this list if you look in the 
> archives.
>
>>
>>> # join the data together
>>> cdata01.data <- 
>>
>> cbind(c.1,c.2,c.3,c.4,c.5,c.6,c.7,c.8,c.9,c.10,c.11,c.12,c.13,c.14,c.15,c.16,c.17,c.18,c.19,c.20,c.21,c.22,c.23,c.24,c.25,c.26,c.27,c.28,c.29,c.30,c.31,c.32,c.33) 
>>
>> Error: cannot allocate vector of size 145 Kb
>> Execution halted
>>
>> 145--Kb---?? This has me rather lost. Maybe on overflow of some 
>> sort?? Maybe on OS problem of some sort? I'm scratching here.
>>
>> Before you question it, there is a legitimate reason for sticking all 
>> these components in the one data.frame.
>>
>> One of the problems here is that tinkering is not really feasible. 
>> This cbind took 1.5 hrs to finally halt.
>>
>> Any help greatly appreciated,
>>
>> James
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>



From james.muller at internode.on.net  Sun Jan 30 08:04:08 2005
From: james.muller at internode.on.net (James Muller)
Date: Sun, 30 Jan 2005 18:04:08 +1100
Subject: [R] Error: cannot allocate vector of size... but with a twist
Message-ID: <41FC86E8.8070601@internode.on.net>

correction: I actually did run things before (as the gc output 
indicates), but deleted them from memory before. Everything else applied 
anyway.

James



From patrick.giraudoux at univ-fcomte.fr  Sun Jan 30 08:52:43 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux H)
Date: Sun, 30 Jan 2005 08:52:43 +0100
Subject: [R] error preparing a package for lazy loading with R CMD
In-Reply-To: <002f01c502b4$93854240$e502eb82@hblaptop>
References: <5.0.2.1.2.20050125062113.02c11ec0@utinam.univ-fcomte.fr>
Message-ID: <5.0.2.1.2.20050130084923.02b97af8@utinam.univ-fcomte.fr>

I have just tried adding a newline at the end of each file. Unsuccessfully. 
Lazy loading is still not accepted and the library can be compiled only 
with the "LazyLoad: no" option in the description file.

Thanks anyway,

Patrick

A 09:05 25/01/2005 +0100, Henrik Bengtsson a ?crit :
>A wild guess: Do you have one file one function? Could it be that the last
>line in one of the files does not end with a newline and this is not taken
>care of by the build with lazy loading? Try to add a newline at the end of
>each of your files.
>
>Henrik Bengtsson
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> > Patrick Giraudoux H
> > Sent: Tuesday, January 25, 2005 6:25 AM
> > To: r-help
> > Cc: Liaw, Andy
> > Subject: RE: [R] error preparing a package for lazy loading with R CMD
> >
> >
> > That's the way John Fox advised to turn the problem. Indeed
> > it works but
> > doe snot explain this bug in lazy loading. I don't think that
> > it may come
> > for a syntax error somewhere. All the functions have been
> > checked and the
> > problem does not occur if any of the 35 functions is removed.
> > Something
> > happens when I add another function to the 34 already
> > present. Strange.
> >
> >
> > A 08:45 24/01/2005 -0500, vous avez ?crit :
> > >Not sure if this will help, but have you tried loading the
> > package after
> > >install with no lazyload?   I've found that if there are
> > syntax errors in
> > >the R source, that can give the problem you described.  Just a guess.
> > >
> > >Andy
> > >
> > > > From: Patrick Giraudoux H
> > > >
> > > > Dear Lister,
> > > >
> > > > I  work with R 2.0.1 and Windows XP, and meet a strange trouble
> > > > trying to make a R package with a make-package.bat file
> > John Fox has
> > > > kindly provided
> > > > (see detailed script below). I am working with it since some
> > > > months with
> > > > excellent results (I do'nt use compiled C code so far). Just
> > > > adding a new
> > > > function in the R directory today, when running
> > make-package and thus
> > > > excuting the command ..\..\bin\R CMD build --force --binary
> > > > --auto-zip
> > > > %1,  I got the following message after the "compile" stage,
> > > > preparing the
> > > > package for lazy loading :
> > > >
> > > > preparing package pgirmess for lazy loading
> > > > Error in "names<-.default"(`*tmp*`, value =c("R", "Platform",
> > > > "Date", :
> > > >          names attribute[4] must be the same length as the vector
> > > > [1] Execution halted
> > > > make: ***[lazyload] Error 1
> > > > *** Installation of pgirmess failed ***
> > > >
> > > > (pgirmess is the package name)
> > > >
> > > > ... and the zip file is not generated.
> > > >
> > > > I have checked and rechecked everything during this (long)
> > > > afternoon... and get nothing except that if I drag out any of the
> > > > *.r file of the R folder,
> > > > everything comes to be OK (except the function that has been
> > > > dragged out is
> > > > missing...). It looks like if having added one extra function
> > > > in the R
> > > > folder on the top of the earlier 32 (+ 2 data frames)
> > makes problem.
> > > >
> > > > I have then consulted John Fox offlist and he seems quite
> > perplexed
> > > > "I'm not sure why you're experiencing this problem". On
> > his advise I
> > > > have included "LazyLoad: no" in the package description file. In
> > > > this case everything goes smoothly then (except LazyLoad
> > will not be
> > > > activated), the
> > > > zip file is generated and the package can be installed from R.
> > > >
> > > > Has anybody an idea about why a problem occurs when preparing the
> > > > package for lazy loading? Any remedy?
> > > >
> > > > Kind regards,
> > > >
> > > > Patrick
> > > >
> > > > Make-Package script:
> > > >
> > > > cd c:\R\rw2001\src\library
> > > > del %1\INDEX
> > > > del %1\data\00Index
> > > > del %1\chm\*.* /Q
> > > > ..\..\bin\R CMD build --force --binary --auto-zip %1
> > ..\..\bin\R CMD
> > > > build --force %1 ..\..\bin\R CMD check %1
> > > > cd %1.Rcheck
> > > > dvipdfm %1-manual
> > > > notepad 00check.log
> > > > cd ..
> > > > cd ..
> > > >
> > > >
> > > > >From: "John Fox" <jfox at mcmaster.ca>
> > > > >To: "'Patrick Giraudoux H'" <patrick.giraudoux at univ-fcomte.fr>
> > > > >Subject: RE: [R] writing a simple package in R 2.0 under
> > Windows XP
> > > > >Date: Sun, 23 Jan 2005 11:41:25 -0500
> > > > >X-Mailer: Microsoft Office Outlook, Build 11.0.6353
> > > > >X-MIME-Autoconverted: from quoted-printable to 8bit by
> > > > >utinam.univ-fcomte.fr id j0NGfJoD011345
> > > > >
> > > > >Dear Patrick,
> > > > >
> > > > >I'm not sure why you're experiencing this problem.
> > > > >
> > > > >Two suggestions: (1) Since the problem appears to be with
> > > > preparing the
> > > > >package for lazy loading, try adding the directive
> > > > "LazyLoad: no" (without
> > > > >the quotes) to the package's DESCRIPTION file. (2) Rather
> > > > than using my
> > > > >batch file, run the commands one at a time to see exactly
> > > > where the problem
> > > > >is produced; then you could send a message to r-help.
> > > > >
> > > > >I hope this helps,
> > > > >  John
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > > > http://www.R-project.org/posting-guide.html
> > > >
> > > >
> > >
> > >
> > >-------------------------------------------------------------
> > ----------
> > >-------
> > >Notice:  This e-mail message, together with any attachments,
> > contains
> > >information of Merck & Co., Inc. (One Merck Drive,
> > Whitehouse Station, New
> > >Jersey, USA 08889), and/or its affiliates (which may be
> > known outside the
> > >United States as Merck Frosst, Merck Sharp & Dohme or MSD
> > and in Japan, as
> > >Banyu) that may be confidential, proprietary copyrighted
> > and/or legally
> > >privileged. It is intended solely for the use of the
> > individual or entity
> > >named on this message.  If you are not the intended
> > recipient, and have
> > >received this message in error, please notify us immediately
> > by reply
> > >e-mail and then delete it from your system.
> > >-------------------------------------------------------------
> > -----------------
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >



From patrick.giraudoux at univ-fcomte.fr  Sun Jan 30 09:07:53 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux H)
Date: Sun, 30 Jan 2005 09:07:53 +0100
Subject: [R] MacOS X and vectorized files (emf, wmf,...)
Message-ID: <5.0.2.1.2.20050130085558.02bab458@utinam.univ-fcomte.fr>



From cg.pettersson at evp.slu.se  Sun Jan 30 09:11:38 2005
From: cg.pettersson at evp.slu.se (CG Pettersson)
Date: Sun, 30 Jan 2005 09:11:38 +0100
Subject: [R] Conflicts using Rcmdr, nlme and lme4
In-Reply-To: <20050128132938.OHRO1919.tomts22-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <200501300811.j0U8BcId009700@mail1.slu.se>

Thanks a lot, both to Prof Bates and Prof Fox.
I see at least some bits of the light now.

One original question remaines though:
Why do all these (22 of them) packages autoload in the first place?
I don?t want them to.

It must have something to do with the original loading from Rcmdr, as
(for example) nlme does *not* autoload by itself no matter how many
lme objects there are in a reopened workspace, this is good as far as
I am concerned. It is especially strange as Rcmdr itself does not 
outoload.

How do I supress this behavior?

Thanks
/CG


28 Jan,  John Fox wrote:
-------------------
> Dear CG,
> 
> An addendum to my previous response: If you do decide to recompile
the Rcmdr
> package to eliminate the dependency on nlme, as I suggested, you'll
also
> have to edit the .onLoad() function in the package (which is in the
file
> startup.R in the source package) to remove "nlme" from the vector of
> required packages.
> 
> Regards,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of CG
Pettersson
> > Sent: Friday, January 28, 2005 5:15 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Conflicts using Rcmdr, nlme and lme4
> > 
> > Hello all!
> > 
> > R2.0.1, W2k. All packages updated.
> > 
> > I?m heavily dependant on using mixed models. Up til?now I have
used
> > lme() from nlme as I have been told to. Together with 
> > estimable() from gmodels it works smooth. I also often run 
> > Rcmdr, mostly for quick graphics.

...snip...

> 
CG Pettersson, MSci, PhD Stud.
Swedish University of Agricultural Sciences
Dep. of Ecology and Crop Production. Box 7043
SE-750 07 Uppsala



From tomhopper at comcast.net  Sun Jan 30 14:06:07 2005
From: tomhopper at comcast.net (Thomas Hopper)
Date: Sun, 30 Jan 2005 08:06:07 -0500
Subject: [R] t-test or conf interval with known variance?
Message-ID: <ae9e871cde87b73458cdf4e1e28c891f@comcast.net>

Hello,

Is there a built-in test in R for hypothesis testing with samples of 
known variance?

For example, I've got a set of data, a mean to compare against, and a 
known variance, and I want to determine the p-value for which I can 
reject the null hypothesis (mu_1 = mu_0) and accept the alternative 
(mu_1 > mu_0). I've found that JMP and Minitab can both do this (in 
JMP, it's a one-sided confidence interval in the Distribution 
platform), but I haven't figured out how to do such a test in R.

Thanks,

Tom



From jfox at mcmaster.ca  Sun Jan 30 14:25:57 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 30 Jan 2005 08:25:57 -0500
Subject: [R] Conflicts using Rcmdr, nlme and lme4
In-Reply-To: <200501300811.j0U8BcId009700@mail1.slu.se>
Message-ID: <20050130132552.HVBC1694.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear CG,

> -----Original Message-----
> From: CG Pettersson [mailto:cg.pettersson at evp.slu.se] 
> Sent: Sunday, January 30, 2005 3:12 AM
> To: John Fox
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] Conflicts using Rcmdr, nlme and lme4
> 
> Thanks a lot, both to Prof Bates and Prof Fox.
> I see at least some bits of the light now.
> 
> One original question remaines though:
> Why do all these (22 of them) packages autoload in the first place?
> I don?t want them to.
> 
> It must have something to do with the original loading from 
> Rcmdr, as (for example) nlme does *not* autoload by itself no 
> matter how many lme objects there are in a reopened 
> workspace, this is good as far as I am concerned. It is 
> especially strange as Rcmdr itself does not outoload.
> 
> How do I supress this behavior?
> 

This shouldn't happen if you close the Commander window *before* closing R
and saving the workspace. The behaviour likely persists since the workspace
was previously saved with the Commander Window open. That's not really an
explanation, but it does describe what happens, I believe.

Regards,
 John

> Thanks
> /CG
> 
> 
> 28 Jan,  John Fox wrote:
> -------------------
> > Dear CG,
> > 
> > An addendum to my previous response: If you do decide to recompile
> the Rcmdr
> > package to eliminate the dependency on nlme, as I suggested, you'll
> also
> > have to edit the .onLoad() function in the package (which is in the
> file
> > startup.R in the source package) to remove "nlme" from the 
> vector of 
> > required packages.
> > 
> > Regards,
> >  John
> > 
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> > 
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch 
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of CG
> Pettersson
> > > Sent: Friday, January 28, 2005 5:15 AM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] Conflicts using Rcmdr, nlme and lme4
> > > 
> > > Hello all!
> > > 
> > > R2.0.1, W2k. All packages updated.
> > > 
> > > I?m heavily dependant on using mixed models. Up til?now I have
> used
> > > lme() from nlme as I have been told to. Together with
> > > estimable() from gmodels it works smooth. I also often run Rcmdr, 
> > > mostly for quick graphics.
> 
> ...snip...
> 
> > 
> CG Pettersson, MSci, PhD Stud.
> Swedish University of Agricultural Sciences Dep. of Ecology 
> and Crop Production. Box 7043 SE-750 07 Uppsala



From Roger.Bivand at nhh.no  Sun Jan 30 14:34:16 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 30 Jan 2005 14:34:16 +0100 (CET)
Subject: [R] New user...tips for spdep?
In-Reply-To: <BAY102-DAV15D03ACA7FE8E704C263C5BC7B0@phx.gbl>
Message-ID: <Pine.LNX.4.44.0501301415360.1715-100000@reclus.nhh.no>

On Sat, 29 Jan 2005, Mike Leahy wrote:

> Hello List,
> 
> I'm a very new user to the R system.  I'm only beginning to learn the
> basics, but so far I've been able to do little more than try a few examples,
> and of course begin reading the documentation.
> 
> My primary motivation for exploring R is the availability of tools like the
> 'spdep' package for calculating spatial statistics such as Geary's C and
> Moran's I, which I would like to use in an analysis for my thesis.  However,
> I am not really sure how to get started.  
> 

One place to start looking could be Luc Anselin's useful tutorial:

http://sal.agecon.uiuc.edu/csiss/pdf/rex1.pdf

>  
> 
> As a simple example, if I have a table with columns containing an ID field,
> X & Y coordinates, and an observation value, what steps should I follow to
> pre-process this data in order to use the moran() function?  I've been able
> to duplicate the example in the help documentation (e.g.,
> http://finzi.psych.upenn.edu/R/library/spdep/html/moran.html), but without
> understanding what the commands really do, I'm unable to proceed much
> further.  How might I figure out the data that gets loaded when I run
> 'data(oldcol)' for example?  
> 

In your case, you will need to make a neighbour list object to define what
you understand as the neighbours of your spatial units, for example by
triangulation (tri2nb()), distance bands (dnearneigh()), k-nearest
neighbours (knn2nb()), etc. These are decisions you have to take based on
your knowledge of the phenomena you are analysing. In any case, you would 
use the moran.test() or moran.mc() functions, rather than moran(), which 
just computes the statistic.

In Luc Anselin's example, the neighbour definitions are created in GeoDa, 
which reads shapefiles. In http://spatial.nhh.no/geo304/areal1-04s.pdf, 
they are created for grids and irregular polygons, not for simple points 
like yours. Have a look at the help pages for the point-based functions, 
and see whether any of them suit your purpose. You may have a distance 
range that you know suits your phenomena that you can use, or perhaps the 
triangulation and/or graph-based methods will be more appropriate.

Please feel free to contact me off-list. or to subscribe to the R-sig-geo 
list to follow this up.

(https://www.stat.math.ethz.ch/mailman/listinfo/r-sig-geo)

Roger Bivand

>  
> 
> Is there anyone that might be able to give me some tips, or point me in the
> right direction?
> 
>  
> 
> Thanks in advance for any suggestions.
> 
>  
> 
> Mike
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From edgar at math.uprm.edu  Sun Jan 30 15:06:51 2005
From: edgar at math.uprm.edu (Edgar Acuna)
Date: Sun, 30 Jan 2005 10:06:51 -0400
Subject: [R] Assistant Professor position
Message-ID: <20050130140344.M15321@math.uprm.edu>


The Department of Mathematics of the University of Puerto Rico at Mayaguez 
invites qualified individuals to apply for a tenure track position in 
statistics at the assistant professor level. Preference will be given to 
candidates whose interests are in applied statistics, computational 
statistics and bioinformatics. 
Applicants should hold a doctorate degree in Statistics,  Biostatistics, or 
a Mathematical Science with emphasis in statistics. Duties include  research 
and  teaching at both undergraduate and graduate levels. Fluency in Spanish 
would be an advantage, but it is not essential.  Review of  applications 
will begin February 25, 2005. 
Send application, official transcripts, and three references to:

Personnel Committee
Department of Mathematics
University of Puerto Rico at Mayaguez
PO. Box 9018
Mayaguez, PR 00681-9018 


Professor Edgar Acuna
Math Department
University of Puerto Rico at Mayaguez
--
Open WebMail Project (http://openwebmail.org)



From patrick.giraudoux at univ-fcomte.fr  Sun Jan 30 15:19:11 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux H)
Date: Sun, 30 Jan 2005 15:19:11 +0100
Subject: [R] MacOS X and vectorized files (emf, wmf,...)
Message-ID: <5.0.2.1.2.20050130151605.02b94c70@utinam.univ-fcomte.fr>



From patrick.giraudoux at univ-fcomte.fr  Sun Jan 30 15:45:18 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux H)
Date: Sun, 30 Jan 2005 15:45:18 +0100
Subject: [R] New user...tips for spdep
Message-ID: <5.0.2.1.2.20050130153703.02b99cf0@utinam.univ-fcomte.fr>



From Ted.Harding at nessie.mcc.ac.uk  Sun Jan 30 15:41:27 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 30 Jan 2005 14:41:27 -0000 (GMT)
Subject: [R] t-test or conf interval with known variance?
In-Reply-To: <ae9e871cde87b73458cdf4e1e28c891f@comcast.net>
Message-ID: <XFMail.050130144127.Ted.Harding@nessie.mcc.ac.uk>

On 30-Jan-05 Thomas Hopper wrote:
> Hello,
> 
> Is there a built-in test in R for hypothesis testing with samples of 
> known variance?
> 
> For example, I've got a set of data, a mean to compare against, and a 
> known variance, and I want to determine the p-value for which I can 
> reject the null hypothesis (mu_1 = mu_0) and accept the alternative 
> (mu_1 > mu_0). I've found that JMP and Minitab can both do this (in 
> JMP, it's a one-sided confidence interval in the Distribution 
> platform), but I haven't figured out how to do such a test in R.

Since you know the variance (and I think you may be implicitly
assuming that the data are Normally distributed though you don't
say so), it should be very straightforward. It's all so close
to "raw R" that there doesn;t seem to be a user-friendly function
already written!

Let V be the known variance.

The mean M of the data will have a normal distribution with mean
m1 and variance V/n where n is the sample size. Since you are
looking at alternatives m1 > m0, you need large values of M to
"reject" the H0, that m1 = m0.

On H0, (M - m0)/S has a standard normal distribution with mean 0
and variance 1, where

  S = sqrt(V/n)

Hence your P-value is

  1 - pnorm((mean(x) - m0)/sqrt(V/n)

and your (say 95%) lower 1-sided confidence limit for m1 is

  M + sqrt(V/n)*qnorm(p)

where p = (1 - 0.95) = 0.05. For any other confidence level,
use the corresponding value of p.

You can wrap this in a function my.test:

  my.test <- function(x,V,m0,P) {
    M<-mean(x); n<-length(x); S<-sqrt(V/n)
    p<-(1 - pnorm((M-m0)/S))
    LCL<-(M + S*qnorm(1-P/100))
    value<-list(Pvalue=p,LCL=LCL)
    print(sprintf("P-value = %g",p))
    print(sprintf("Lower %.2f%% Confidence Limit = %g",
                   P, LCL))
  }

where x is the sample, V is the known variance,
m0 is the mean to test against, and P is the *percentage*
confidence level (e.g. 95 for 95%) that you want for the
confidence limit.

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 30-Jan-05                                       Time: 14:41:27
------------------------------ XFMail ------------------------------



From patrick.giraudoux at univ-fcomte.fr  Sun Jan 30 16:23:28 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux H)
Date: Sun, 30 Jan 2005 16:23:28 +0100
Subject: [R] error preparing a package for lazy loading with R CMD
In-Reply-To: <41FCF462.4000203@statistik.uni-dortmund.de>
References: <5.0.2.1.2.20050130152313.02b6c5f8@utinam.univ-fcomte.fr>
	<5.0.2.1.2.20050130084923.02b97af8@utinam.univ-fcomte.fr>
	<5.0.2.1.2.20050125062113.02c11ec0@utinam.univ-fcomte.fr>
	<5.0.2.1.2.20050130084923.02b97af8@utinam.univ-fcomte.fr>
	<5.0.2.1.2.20050130152313.02b6c5f8@utinam.univ-fcomte.fr>
Message-ID: <5.0.2.1.2.20050130162037.02b936e0@utinam.univ-fcomte.fr>

Trouble just solved by Uwe Ligge! See below:

>You have a wrong "Built" field in your DESCRIPTION file!!!
>"Built: R 2.0.1;windows"
>Please don't specify such a line yourself, "R CMD build" does it for you.

Ashes on my head and all these sort of things...

Patrick



From jeaneid at chass.utoronto.ca  Sun Jan 30 16:52:00 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Sun, 30 Jan 2005 10:52:00 -0500
Subject: [R] trellis graphics in loops
Message-ID: <Pine.SGI.4.40.0501301029490.8734111-100000@origin.chass.utoronto.ca>


I have this awkward problem with trellis (lattice). I am trying to
generate some plots through loops but the .eps file is empty. When I
generate them in a list and print them outside the loop all is fine. this
is an example below:( nothing shows up in foo.eps, but all show up in
foo1.eps)
R vesion 2.0.1, lattice version  0.10-16, on a debian 2.6.8-1 kernel.


X <- data.frame(x=rnorm(10000), y=rnorm(10000), z=sample(c("foo1","foo2"),
10000,replace=T), year=sample(c(89:94), 10000,replace=T))
trellis.device("postscript", file="foo.eps")
for(i in unique(X$year)){
  temp <- X[X$year%in%i, ]
  xyplot(temp$x~temp$y|temp$z)
}
dev.off()

mylist <- list()
for(i in unique(X$year)){
  temp <- X[X$year%in%i, ]
  mylist[[i]] <- xyplot(temp$x~temp$y|temp$z)
}

trellis.device("postscript", file="foo1.eps")
mylist[[89]]
mylist[[90]]
mylist[[91]]
mylist[[92]]
mylist[[93]]
mylist[[94]]
dev.off()


Thank you,

Jean



From ccleland at optonline.net  Sun Jan 30 17:21:04 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Sun, 30 Jan 2005 11:21:04 -0500
Subject: [R] trellis graphics in loops
In-Reply-To: <Pine.SGI.4.40.0501301029490.8734111-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0501301029490.8734111-100000@origin.chass.utoronto.ca>
Message-ID: <41FD0970.9070406@optonline.net>

Wrap the xyplot in print:

X <- data.frame(x=rnorm(10000), y=rnorm(10000), z=sample(c("foo1","foo2"),
10000,replace=T), year=sample(c(89:94), 10000,replace=T))

trellis.device("postscript", file="foo.eps")

for(i in unique(X$year)){
   temp <- X[X$year%in%i, ]
   print(xyplot(temp$x~temp$y|temp$z))
}

dev.off()

hope this helps,

Chuck Cleland

Jean Eid wrote:
> I have this awkward problem with trellis (lattice). I am trying to
> generate some plots through loops but the .eps file is empty. When I
> generate them in a list and print them outside the loop all is fine. this
> is an example below:( nothing shows up in foo.eps, but all show up in
> foo1.eps)
> R vesion 2.0.1, lattice version  0.10-16, on a debian 2.6.8-1 kernel.
> 
> 
> X <- data.frame(x=rnorm(10000), y=rnorm(10000), z=sample(c("foo1","foo2"),
> 10000,replace=T), year=sample(c(89:94), 10000,replace=T))
> trellis.device("postscript", file="foo.eps")
> for(i in unique(X$year)){
>   temp <- X[X$year%in%i, ]
>   xyplot(temp$x~temp$y|temp$z)
> }
> dev.off()
> 
> mylist <- list()
> for(i in unique(X$year)){
>   temp <- X[X$year%in%i, ]
>   mylist[[i]] <- xyplot(temp$x~temp$y|temp$z)
> }
> 
> trellis.device("postscript", file="foo1.eps")
> mylist[[89]]
> mylist[[90]]
> mylist[[91]]
> mylist[[92]]
> mylist[[93]]
> mylist[[94]]
> dev.off()
> 
> 
> Thank you,
> 
> Jean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From edd at debian.org  Sun Jan 30 17:36:58 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 30 Jan 2005 10:36:58 -0600
Subject: [R] trellis graphics in loops
In-Reply-To: <Pine.SGI.4.40.0501301029490.8734111-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0501301029490.8734111-100000@origin.chass.utoronto.ca>
Message-ID: <16893.3370.832363.624334@basebud.nulle.part>


Jean,

On 30 January 2005 at 10:52, Jean Eid wrote:
| 
| I have this awkward problem with trellis (lattice). I am trying to
| generate some plots through loops but the .eps file is empty. When I
| generate them in a list and print them outside the loop all is fine. this

That is a FAQ -- wrap a 'print()' around the calls to xyplot() and friends: 

  > X <- data.frame(x=rnorm(10000), y=rnorm(10000),
                    z=sample(c("foo1","foo2"),
                             10000,replace=T), 
                    year=sample(c(89:94), 10000,replace=T))
  > pdf("/tmp/jean.pdf")
  > for(i in unique(X$year)) { temp <- X[X$year%in%i, ]; print(xyplot(temp$x~temp$y|temp$z)) }
  > dev.off()
    null device 
              1 
  >

creates fine pdf with six pages:

edd at basebud:~> ls -l /tmp/jean.pdf
-rw-r--r--  1 edd edd 592537 Jan 30 10:34 /tmp/jean.pdf


Hth, Dirk 

| is an example below:( nothing shows up in foo.eps, but all show up in
| foo1.eps)
| R vesion 2.0.1, lattice version  0.10-16, on a debian 2.6.8-1 kernel.
| 
| 
| X <- data.frame(x=rnorm(10000), y=rnorm(10000), z=sample(c("foo1","foo2"),
| 10000,replace=T), year=sample(c(89:94), 10000,replace=T))
| trellis.device("postscript", file="foo.eps")
| for(i in unique(X$year)){
|   temp <- X[X$year%in%i, ]
|   xyplot(temp$x~temp$y|temp$z)
| }
| dev.off()
| 
| mylist <- list()
| for(i in unique(X$year)){
|   temp <- X[X$year%in%i, ]
|   mylist[[i]] <- xyplot(temp$x~temp$y|temp$z)
| }
| 
| trellis.device("postscript", file="foo1.eps")
| mylist[[89]]
| mylist[[90]]
| mylist[[91]]
| mylist[[92]]
| mylist[[93]]
| mylist[[94]]
| dev.off()
| 
| 
| Thank you,
| 
| Jean
| 
| ______________________________________________
| R-help at stat.math.ethz.ch mailing list
| https://stat.ethz.ch/mailman/listinfo/r-help
| PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From p.dalgaard at biostat.ku.dk  Sun Jan 30 17:38:41 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Jan 2005 17:38:41 +0100
Subject: [R] trellis graphics in loops
In-Reply-To: <Pine.SGI.4.40.0501301029490.8734111-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0501301029490.8734111-100000@origin.chass.utoronto.ca>
Message-ID: <x23bwix46m.fsf@biostat.ku.dk>

Jean Eid <jeaneid at chass.utoronto.ca> writes:

> I have this awkward problem with trellis (lattice). I am trying to
> generate some plots through loops but the .eps file is empty. When I
> generate them in a list and print them outside the loop all is fine. this
> is an example below:( nothing shows up in foo.eps, but all show up in
> foo1.eps)
> R vesion 2.0.1, lattice version  0.10-16, on a debian 2.6.8-1 kernel.
> 
> 
> X <- data.frame(x=rnorm(10000), y=rnorm(10000), z=sample(c("foo1","foo2"),
> 10000,replace=T), year=sample(c(89:94), 10000,replace=T))
> trellis.device("postscript", file="foo.eps")
> for(i in unique(X$year)){
>   temp <- X[X$year%in%i, ]
>   xyplot(temp$x~temp$y|temp$z)
> }
> dev.off()

You need to print(xyplot(....)).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From patrick.giraudoux at univ-fcomte.fr  Sun Jan 30 18:03:26 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux H)
Date: Sun, 30 Jan 2005 18:03:26 +0100
Subject: [R] error preparing a package for lazy loading with R CMD
Message-ID: <5.0.2.1.2.20050130175527.02b80258@utinam.univ-fcomte.fr>


Trouble just solved by Uwe Ligge! See below:


>>You have a wrong "Built" field in your DESCRIPTION file!!!
>>"Built: R 2.0.1;windows". Please don't specify such a line yourself, "R 
>>CMD build" does it for you.


Ashes on my head and all these sort of things...

Patrick



From numero.primo at tele2.it  Sun Jan 30 18:23:09 2005
From: numero.primo at tele2.it (Landini Massimiliano)
Date: Sun, 30 Jan 2005 18:23:09 +0100
Subject: [R] Box-Cox / data transformation question
In-Reply-To: <41F65AE5.4020601@uni-jena.de>
References: <41F65AE5.4020601@uni-jena.de>
Message-ID: <n75qv0l7rgshubin4k53c4v1d0fbco7gkt@4ax.com>

On Tue, 25 Jan 2005 15:42:45 +0100, you wrote:

|=[:o)  Dear R users,
|=[:o)  
|=[:o)  Is it reasonable to transform data (measurements of plant height) to the 
|=[:o)  power of 1/4? I?ve used boxcox(response~A*B) and lambda was close to 0.25.
|=[:o)  

IMHO (I'm far to be a statistician) no. I think that Box Cox procedure must be a
help to people that had none experience in data transforming. In fact data
transforming include other methods that Box Cox procedure can't perform as rank
transformation, arcsine square root percent transformation, hyperbolic inverse
sine, log-log, probit, normit  and logit.
Transformation is not simply an application of a formula to massive data. Is
preferable decide appropriate transformation knowing deepening how and from
where data were collected.


|=[:o)  Regards,
|=[:o)  Christoph
|=[:o)  
|=[:o)  ______________________________________________
|=[:o)  R-help at stat.math.ethz.ch mailing list
|=[:o)  https://stat.ethz.ch/mailman/listinfo/r-help
|=[:o)  PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



-------------------------------------------------------------------------------------------------------------------------
Landini dr. Massimiliano
Tel. mob. (+39) 347 140 11 94
Tel./Fax. (+39) 051 762 196
e-mail: numero (dot) primo (at) tele2 (dot) it
-------------------------------------------------------------------------------------------------------------------------
Legge di Hanggi: Pi? stupida ? la tua ricerca, pi? verr? letta e approvata.
Corollario alla Legge di Hanggi: Pi? importante ? la tua ricerca, meno verr?
capita.



From jtk at cmp.uea.ac.uk  Sun Jan 30 21:01:37 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Sun, 30 Jan 2005 20:01:37 +0000
Subject: [R] Function to modify existing data.frame--Improving R Language
In-Reply-To: <Pine.A41.4.61b.0501191019110.332684@homer11.u.washington.edu>
References: <BE1402A7.D0AC%peterm@andrew.cmu.edu>
	<Pine.A41.4.61b.0501191019110.332684@homer11.u.washington.edu>
Message-ID: <20050130200137.GC8419@jtkpc.cmp.uea.ac.uk>

On Wed, Jan 19, 2005 at 10:31:13AM -0800, Thomas Lumley wrote:
> On Wed, 19 Jan 2005, Peter Muhlberger wrote:
> 
> >						 By not allowing any
> >straightforward passing by reference, R strikes me as a lot less 
> >flexible &
> >useful than it might be.  A basic operation in other stats languages is 
> >to
> >update a dataset using a program.  This proves very helpful for managing
> >data and setting up analyses.  But, this seems to be quite inelegant to 
> >do
> >in R.
> 
> I don't see why
>     mydata <- some.program(mydata)
> is much less elegant than
>     mydata.someProgram()
> as a way of updating a data set. It may use more memory, but that wasn't 
> the point at issue.

Somehow, this remark has stuck around in my head, and after a week
or so of pondering, I'd like to share two thoughts on this. These are
on a rather conceptual level, please skip this message if that's not
interesting to you.

(1) Regarding the net effect, these two are equivalent, but the notion
of "elegance" often differentiates between things which, with regard
to some given purpose, are equivalent. My interpretation of elegance in
this case is that it is "elegant" to have a bijection between instances
of objects in a program and entities in reality. This is realized by
"mydata <- some.program(mydata)" less than by "mydata.someProgram()",
assuming that the former operates on a copy of mydata and the latter on
a reference (i.e. "this" in Java).

The problem with "elegance" is, of course that it is a rather subjective
criterion which may well turn out to be irrelevant as one tries to
explain it objectively and formalize it. But sometimes, a discontent
that initially manifests itself as a perception of "inelegance"
may point towards deeper reaching issues, and this may be the case
here. The principle of a one-to-one relation between entities and their
representations in a representation scheme is quite fundamental. The first
normal form in relational database design reflects this, for example.

(2) The information provided by the syntax
"mydata <- some.program(mydata)" is sufficient to allow an R interpreter
to recognize that a call by reference could be used here, and
interestingly, it seems to me that the lazy evaluation mechanism already
provides most of what is needed to pull this off.

As far as I understand, the promise generated from a variable being used
as a parameter resolves to a sort of reference to the variable upon
being forced. Differently from a normal variable, a promise cannot be
modified in place e.g. by assignment to one of its elements. Therefore,
doing so triggers construction of a local variable, e.g.  in

    some.program <- function(x)
    {
      x[7] <- 4711;
      x;
    }

the line "x[7] <- 4711" results in copying the value of the promise
x, replacing the 7th element of the copy with 4711, and assigning
the resulting vector to x, which thereby becomes a normal local
variable.

However, if the variable's value is going to be replaced by the
value returned by the function, this copying is not necessary. As
the old value is going to be scrapped anyway, the function can as
well use that as a local variable.

The R interpreter could actually exploit this "recycling potential":
Before actually executing a function call, such as some.program(mydata),
the interpreter would have to determine what the result is used for. If
the result is assigned to a variable, then the interpreter should check
whether the variable is also used as a parameter in the function call,
and if that is the case, the promise generated for the corresponding
parameter should be flagged "no copying necessary".

On a technical level, I have no idea how easy or difficult realizing
this idea would be, as I haven't delved the R source and the docs
don't provide too much details about the internal workings of promises.
Furthermore, I would expect that the interpreter currently executes
function calls first and deals with using the results, e.g. by assigning
them to something, only after they are returned.

But perhaps, the more interesting perspective of this is on the
conceptual level, namely that, in fact, the assignment variant
is not per se less "elegant" than the method call variant -- it all
depends on how "elegantly" the interpreter does it.

> Of course there are advantages to the ability to pass by reference, and 
> disadvantages -- the most obvious disadvantage is that it is not easy to 
> tell which variables are modified by a given piece of code.

Yes -- this is something I always disliked about C++, whereas in C,
the fact that a pointer is passed in is reasonably obvious from the call
itself, i.e. without consulting the function declaration or the docs. As
an idea, making possible modification of parameter variables obvious in
R could perhaps look something like

    mydata <- some.program(make.promise(mydata, allow.modification = TRUE))

> It probably wouldn't be that hard to produce something that looked like 
> a data frame but was passed by reference, by wrapping it in a 
> environment.

This reminds me of Python, where everything is a dictionary...

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From tilo.blenk at charite.de  Sun Jan 30 20:40:59 2005
From: tilo.blenk at charite.de (Tilo Blenk)
Date: Sun, 30 Jan 2005 20:40:59 +0100
Subject: [R] Postgraduate distance learning courses for MSc in Statistics?
Message-ID: <DDD3D2B3-72F6-11D9-8F90-000D93AD656A@charite.de>

Does anybody has experience with postgraduate distance learning courses 
resulting in a MSc in Statistics? I am thinking about such a course to 
learn more about statistics and get a certification to be able to work 
as statistician. I would prefer a course in England or in the USA. A 
coworker told me about a course at the Sheffield Hallam University. Are 
there well known courses? Any idea how to find a good one? Are there 
any courses with some emphasis on computational statistics?

I am currently working in a medical research group in Germany managing 
studies and being responsible for data management and basic, 
explorative data analysis. I studied medicine and I am not that bad in 
programming.

Thanks in advance for any comments and suggestions.

Sorry for posting something only distantly related to R but I hope 
there some people reading this mailing list who could help me. 
Moreover, R is one of the reasons why I got interested in statistics.

Tilo



From ripley at stats.ox.ac.uk  Sun Jan 30 20:47:53 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 30 Jan 2005 19:47:53 +0000 (GMT)
Subject: [R] error preparing a package for lazy loading with R CMD
In-Reply-To: <5.0.2.1.2.20050130175527.02b80258@utinam.univ-fcomte.fr>
References: <5.0.2.1.2.20050130175527.02b80258@utinam.univ-fcomte.fr>
Message-ID: <Pine.LNX.4.61.0501301938590.29742@gannet.stats>

On Sun, 30 Jan 2005, Patrick Giraudoux H wrote:

> Trouble just solved by Uwe Ligge! See below:
>
>>> You have a wrong "Built" field in your DESCRIPTION file!!!
>>> "Built: R 2.0.1;windows". Please don't specify such a line yourself, "R 
>>> CMD build" does it for you.

Actually, installation does it: build adds Packaged: (which I have alwasys 
thought potentially confusing).

> Ashes on my head and all these sort of things...

One of the changes that has been in 2.0.1 patched for some time is (from 
its NEWS)

PACKAGE INSTALLATION CHANGES

     o	A package DESCRIPTION file which contains a Built field (it
 	should not!) is now worked around, with loud warnings.


BTW, you would have benefited from having read (in ONEWS now)

 	Packages are by default installed using lazy loading if they
 	have more than 25Kb of R code and did not use a saved image.
 	This can be overridden by INSTALL --[no-]lazy or via a field
 	in the DESCRIPTION file.  Note that as with --save, any other
 	packages which are required must be already installed.

and then you would have not found well-documented things `strange'.

The NEWS file is the `release notes' of R and does need to be read at each 
new release.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rab at nauticom.net  Sun Jan 30 23:47:31 2005
From: rab at nauticom.net (Rick Bilonick)
Date: Sun, 30 Jan 2005 17:47:31 -0500
Subject: [R] Box-Cox / data transformation question
In-Reply-To: <n75qv0l7rgshubin4k53c4v1d0fbco7gkt@4ax.com>
References: <41F65AE5.4020601@uni-jena.de>
	<n75qv0l7rgshubin4k53c4v1d0fbco7gkt@4ax.com>
Message-ID: <41FD6403.8080505@nauticom.net>

Landini Massimiliano wrote:

>On Tue, 25 Jan 2005 15:42:45 +0100, you wrote:
>
>|=[:o)  Dear R users,
>|=[:o)  
>|=[:o)  Is it reasonable to transform data (measurements of plant height) to the 
>|=[:o)  power of 1/4? I?ve used boxcox(response~A*B) and lambda was close to 0.25.
>|=[:o)  
>
>IMHO (I'm far to be a statistician) no. I think that Box Cox procedure must be a
>help to people that had none experience in data transforming. In fact data
>transforming include other methods that Box Cox procedure can't perform as rank
>transformation, arcsine square root percent transformation, hyperbolic inverse
>sine, log-log, probit, normit  and logit.
>Transformation is not simply an application of a formula to massive data. Is
>preferable decide appropriate transformation knowing deepening how and from
>where data were collected.
>
>
>|=[:o)  Regards,
>|=[:o)  Christoph
>|=[:o)  
>|=[:o)  ______________________________________________
>|=[:o)  R-help at stat.math.ethz.ch mailing list
>|=[:o)  https://stat.ethz.ch/mailman/listinfo/r-help
>|=[:o)  PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>-------------------------------------------------------------------------------------------------------------------------
>Landini dr. Massimiliano
>Tel. mob. (+39) 347 140 11 94
>Tel./Fax. (+39) 051 762 196
>e-mail: numero (dot) primo (at) tele2 (dot) it
>-------------------------------------------------------------------------------------------------------------------------
>Legge di Hanggi: Pi? stupida ? la tua ricerca, pi? verr? letta e approvata.
>Corollario alla Legge di Hanggi: Pi? importante ? la tua ricerca, meno verr?
>capita.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>
Why are you using a double square root transformation? Is the 
transformation for the response variable? Transfromation is one way to 
help insure that the error distribution is at least approximately 
normal. So if this is the reason, it certainly could make sense. There 
is no unique scale for making measurements. We choose a scale that helps 
us analyze the data appropriately.

Rick B.



From faheem at email.unc.edu  Mon Jan 31 00:03:29 2005
From: faheem at email.unc.edu (Faheem Mitha)
Date: Sun, 30 Jan 2005 18:03:29 -0500 (EST)
Subject: [R] Rinternals.h and iostream don't play nice together'
Message-ID: <Pine.LNX.4.61.0501301753460.10677@Chrestomanci>


Hi,

Consider the following file.

*******************************
foo.cc
*******************************
#include <R.h>
#include <Rinternals.h>
#include <iostream>
*******************************

R CMD SHLIB foo.cc

gives scads of errors. I've use C++ extensively with R before (using C 
linkage) but not with Rinternals.h. I'm puzzled. The errors make no sense 
to me. Am I doing something wrong? Is it impossible to use iostream here 
for some reason?

I'm using Debian Sarge with R 2.0.1 and gcc 3.3.5. Please cc me; I'm not 
subscribed. Thanks in advance.

                                                               Faheem.

R CMD SHLIB foo.cc
g++ -I/usr/lib/R/include     -fPIC  -g -O2 -c foo.cc -o foo.o
************************************************************************
In file included from /usr/include/c++/3.3/bits/locale_facets.h:528,
                  from /usr/include/c++/3.3/bits/basic_ios.h:44,
                  from /usr/include/c++/3.3/ios:51,
                  from /usr/include/c++/3.3/ostream:45,
                  from /usr/include/c++/3.3/iostream:45,
                  from foo.cc:3:
/usr/include/c++/3.3/bits/codecvt.h:110:52: macro "length" passed 4 arguments, but takes just 1
In file included from /usr/include/c++/3.3/bits/locale_facets.h:528,
                  from /usr/include/c++/3.3/bits/basic_ios.h:44,
                  from /usr/include/c++/3.3/ios:51,
                  from /usr/include/c++/3.3/ostream:45,
                  from /usr/include/c++/3.3/iostream:45,
                  from foo.cc:3:
/usr/include/c++/3.3/bits/codecvt.h:109: error: parse error before `const'
/usr/include/c++/3.3/bits/codecvt.h:113: error: semicolon missing after
    declaration of `std::__codecvt_abstract_base<_InternT, _ExternT, _StateT>'
/usr/include/c++/3.3/bits/codecvt.h:115: error: ISO C++ forbids defining types
    within return type
/usr/include/c++/3.3/bits/codecvt.h:115: error: extraneous `int' ignored
/usr/include/c++/3.3/bits/codecvt.h:115: error: non-member function `
    std::__codecvt_abstract_base<_InternT, _ExternT, _StateT> std::max_length()'
    cannot have `const' method qualifier
/usr/include/c++/3.3/bits/codecvt.h:115: error: semicolon missing after
    declaration of `class std::__codecvt_abstract_base<_InternT, _ExternT,
    _StateT>'
/usr/include/c++/3.3/bits/codecvt.h: In function `int std::max_length()':
/usr/include/c++/3.3/bits/codecvt.h:115: error: invalid use of `this' in
    non-member function
/usr/include/c++/3.3/bits/codecvt.h: At global scope:
/usr/include/c++/3.3/bits/codecvt.h:117: error: parse error before `protected'
/usr/include/c++/3.3/bits/codecvt.h:122: error: destructors must be member
    functions
/usr/include/c++/3.3/bits/codecvt.h:122: error: virtual outside class
    declaration
/usr/include/c++/3.3/bits/codecvt.h: In function `void
    std::__codecvt_abstract_base()':
/usr/include/c++/3.3/bits/codecvt.h:122: error: `void
    std::__codecvt_abstract_base()' redeclared as different kind of symbol
/usr/include/c++/3.3/bits/codecvt.h:65: error: previous declaration of `
    template<class _InternT, class _ExternT, class _StateT> class
    std::__codecvt_abstract_base<_InternT,_ExternT,_StateT>'
/usr/include/c++/3.3/bits/codecvt.h:65: error: previous non-function
    declaration `template<class _InternT, class _ExternT, class _StateT> class
    std::__codecvt_abstract_base<_InternT,_ExternT,_StateT>'
/usr/include/c++/3.3/bits/codecvt.h:122: error: conflicts with function
    declaration `void std::__codecvt_abstract_base()'
/usr/include/c++/3.3/bits/codecvt.h: At global scope:
/usr/include/c++/3.3/bits/codecvt.h:125: error: syntax error before `(' token
/usr/include/c++/3.3/bits/codecvt.h:131: error: syntax error before `(' token
/usr/include/c++/3.3/bits/codecvt.h:135: error: syntax error before `(' token
/usr/include/c++/3.3/bits/codecvt.h:141: error: virtual outside class
    declaration
/usr/include/c++/3.3/bits/codecvt.h:141: error: non-member function `int
    std::do_encoding()' cannot have `const' method qualifier
/usr/include/c++/3.3/bits/codecvt.h:141: error: function `int
    std::do_encoding()' is initialized like a variable
/usr/include/c++/3.3/bits/codecvt.h:144: error: virtual outside class
    declaration
/usr/include/c++/3.3/bits/codecvt.h:144: error: non-member function `bool
    std::do_always_noconv()' cannot have `const' method qualifier
/usr/include/c++/3.3/bits/codecvt.h:144: error: function `bool
    std::do_always_noconv()' is initialized like a variable
/usr/include/c++/3.3/bits/codecvt.h:147: error: parse error before `&' token
/usr/include/c++/3.3/bits/codecvt.h:148: error: virtual outside class
    declaration
/usr/include/c++/3.3/bits/codecvt.h:148: error: non-member function `int
    std::do_length(...)' cannot have `const' method qualifier
/usr/include/c++/3.3/bits/codecvt.h:148: error: function `int
    std::do_length(...)' is initialized like a variable
/usr/include/c++/3.3/bits/codecvt.h:151: error: virtual outside class
    declaration
/usr/include/c++/3.3/bits/codecvt.h:151: error: non-member function `int
    std::do_max_length()' cannot have `const' method qualifier
/usr/include/c++/3.3/bits/codecvt.h:151: error: function `int
    std::do_max_length()' is initialized like a variable
/usr/include/c++/3.3/bits/codecvt.h:158: error: parse error before `<' token
/usr/include/c++/3.3/bits/codecvt.h:163: error: syntax error before `;' token
/usr/include/c++/3.3/bits/codecvt.h:164: error: syntax error before `;' token
/usr/include/c++/3.3/bits/codecvt.h:165: error: syntax error before `;' token
/usr/include/c++/3.3/bits/codecvt.h:172: error: only declarations of
    constructors can be `explicit'
/usr/include/c++/3.3/bits/codecvt.h: In function `int codecvt(unsigned int)':
/usr/include/c++/3.3/bits/codecvt.h:172: error: `int codecvt(unsigned int)'
    redeclared as different kind of symbol
/usr/include/c++/3.3/bits/codecvt.h:158: error: previous declaration of `
    template<class _InternT, class _ExternT, class _StateT> class codecvt'
/usr/include/c++/3.3/bits/codecvt.h:158: error: previous non-function
    declaration `template<class _InternT, class _ExternT, class _StateT> class
    codecvt'
/usr/include/c++/3.3/bits/codecvt.h:172: error: conflicts with function
    declaration `int codecvt(unsigned int)'
/usr/include/c++/3.3/bits/codecvt.h:172: error: only constructors take base
    initializers
/usr/include/c++/3.3/bits/codecvt.h:172: error: parse error before `<' token
/usr/include/c++/3.3/bits/codecvt.h:172: confused by earlier errors, bailing out
make: *** [foo.o] Error 1



From edd at debian.org  Mon Jan 31 00:20:40 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 30 Jan 2005 17:20:40 -0600
Subject: [R] Rinternals.h and iostream don't play nice together'
In-Reply-To: <Pine.LNX.4.61.0501301753460.10677@Chrestomanci>
References: <Pine.LNX.4.61.0501301753460.10677@Chrestomanci>
Message-ID: <16893.27592.473887.542842@basebud.nulle.part>


On 30 January 2005 at 18:03, Faheem Mitha wrote:
| Consider the following file.
| 
| *******************************
| foo.cc
| *******************************
| #include <R.h>
| #include <Rinternals.h>
| #include <iostream>
| *******************************
| 
| R CMD SHLIB foo.cc

Two changes are required: You absolutely do need extern "C" ... to get C
headers in with C++, and the other is a simple reordering -- not sure where I
learned that can help at times ...

edd at basebud:/tmp> cat foo.cc
#include <iostream>
extern "C" {
  #include <R.h>
  #include <Rinternals.h>
}
edd at basebud:/tmp> R CMD SHLIB foo.cc
g++ -shared  -o foo.so foo.o   -L/usr/lib/R/lib -lR
edd at basebud:/tmp> ls -l foo.so
-rwxr-xr-x  1 edd edd 117872 Jan 30 17:16 foo.so
edd at basebud:/tmp>

Hth, Dirk

-- 
Better to have an approximate answer to the right question than a precise 
answer to the wrong question.  --  John Tukey as quoted by John Chambers



From roebuck at odin.mdacc.tmc.edu  Mon Jan 31 00:24:32 2005
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Sun, 30 Jan 2005 17:24:32 -0600 (CST)
Subject: [R] Rinternals.h and iostream don't play nice together'
In-Reply-To: <Pine.LNX.4.61.0501301753460.10677@Chrestomanci>
References: <Pine.LNX.4.61.0501301753460.10677@Chrestomanci>
Message-ID: <Pine.OSF.4.58.0501301708370.374256@odin.mdacc.tmc.edu>

On Sun, 30 Jan 2005, Faheem Mitha wrote:

> Consider the following file.
>
> *******************************
> foo.cc
> *******************************
> #include <R.h>

and if you use the following instead of "Rinternals.h"?
extern "C" {
#include <Rdefines.h>
}

> #include <Rinternals.h>
> #include <iostream>
> *******************************
>
> R CMD SHLIB foo.cc
>
> gives scads of errors. I've use C++ extensively with R before (using C
> linkage) but not with Rinternals.h. I'm puzzled. The errors make no sense
> to me. Am I doing something wrong? Is it impossible to use iostream here
> for some reason?
> [SNIP lots of error messages]

'Course I'm assuming you read that part in the 'R-exts.pdf'
about using C++ iostreams with R being best avoided? If you
really need to do so anyway, I'll give you a link to my iostream
manipulator source that interfaces with the R console. It handles
the scenario described in section 4.6 of above document but
requires the GNU C++ compiler (g++) in order to work though.

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From faheem at email.unc.edu  Mon Jan 31 00:28:18 2005
From: faheem at email.unc.edu (Faheem Mitha)
Date: Sun, 30 Jan 2005 18:28:18 -0500 (EST)
Subject: [R] Rinternals.h and iostream don't play nice together'
In-Reply-To: <16893.27592.473887.542842@basebud.nulle.part>
References: <Pine.LNX.4.61.0501301753460.10677@Chrestomanci>
	<16893.27592.473887.542842@basebud.nulle.part>
Message-ID: <Pine.LNX.4.61.0501301824160.10677@Chrestomanci>



On Sun, 30 Jan 2005, Dirk Eddelbuettel wrote:

>
> On 30 January 2005 at 18:03, Faheem Mitha wrote:
> | Consider the following file.
> |
> | *******************************
> | foo.cc
> | *******************************
> | #include <R.h>
> | #include <Rinternals.h>
> | #include <iostream>
> | *******************************
> |
> | R CMD SHLIB foo.cc
>
> Two changes are required: You absolutely do need extern "C" ... to get C
> headers in with C++, and the other is a simple reordering -- not sure where I
> learned that can help at times ...
>
> edd at basebud:/tmp> cat foo.cc
> #include <iostream>
> extern "C" {
>  #include <R.h>
>  #include <Rinternals.h>
> }

Ok, but R.h and Rinternals.h are already wrapped in extern "C", as you can 
check, so this is superfluous.

I've verified that putting iostream first fixes this, but this is really a 
workaround, though still useful for me.

Thanks.                                                          Faheem.



From faheem at email.unc.edu  Mon Jan 31 00:38:11 2005
From: faheem at email.unc.edu (Faheem Mitha)
Date: Sun, 30 Jan 2005 18:38:11 -0500 (EST)
Subject: [R] Rinternals.h and iostream don't play nice together'
In-Reply-To: <Pine.OSF.4.58.0501301708370.374256@odin.mdacc.tmc.edu>
References: <Pine.LNX.4.61.0501301753460.10677@Chrestomanci>
	<Pine.OSF.4.58.0501301708370.374256@odin.mdacc.tmc.edu>
Message-ID: <Pine.LNX.4.61.0501301828460.10677@Chrestomanci>



On Sun, 30 Jan 2005, Paul Roebuck wrote:

> and if you use the following instead of "Rinternals.h"?
> extern "C" {
> #include <Rdefines.h>
> }

Still the same errors. As Dirk pointed out, putting iostream first makes 
the errors go away in either case.

> 'Course I'm assuming you read that part in the 'R-exts.pdf' about using 
> C++ iostreams with R being best avoided? If you really need to do so 
> anyway, I'll give you a link to my iostream manipulator source that 
> interfaces with the R console. It handles the scenario described in 
> section 4.6 of above document but requires the GNU C++ compiler (g++) in 
> order to work though.

I'm just using iostreams for debugging purposes, not part of the code per 
se. Since I am trying to write an R wrapper for a prexisting C++ library, 
I already have print functions using iostreams, which I would prefer to 
reuse if possible. For my limited needs, iostreams works fine.

I also use g++, and would be interested in seeing your code in any case.

Thanks.                                                          Faheem.



From tomhopper at comcast.net  Mon Jan 31 01:45:19 2005
From: tomhopper at comcast.net (Thomas Hopper)
Date: Sun, 30 Jan 2005 19:45:19 -0500
Subject: [R] Startup Files (RProfile) and R-Aqua
Message-ID: <19e940812011f65c61b9492c8ef7ce2d@comcast.net>

Hello,

I'm having some difficulty understanding the documentation relative to 
the startup files with R-Aqua 2.0.1 for Mac OS X.

Specifically, I'm wondering: where does R search for the startup files 
(my home directory at Users:<me>:?); how should they be named 
(.RProfile will be treated by Mac OS X as a system file and be hidden, 
so I'm wondering if it should just be RProfile)? I can't find this 
platform-specific information in any of the documentation.

As a related issue, where would I store my home-grown functions? Would 
RProfile be a good place to keep them?

Thank you,

Tom



From lederer at trium.de  Mon Jan 31 02:10:10 2005
From: lederer at trium.de (lederer@trium.de)
Date: Mon, 31 Jan 2005 02:10:10 +0100 (CET)
Subject: [R] Evaluating code in a sandbox
Message-ID: <33560.217.229.8.208.1107133810.squirrel@mail.mytrium.com>


Dear R-Gurus,

is it possible to evaluate foreign R code in such a safe way, that it has
no chance to confuse the global environment?

The follwing does not suffice, because the untrusted code might e.g.
contain a superassignment ("<<-"):

connection <- textConnection(some.untrusted.code)
try(eval(parse(connection), envir=new.env()))
close(connection)


Christian



From paulangonzalez at hotmail.com  Mon Jan 31 02:44:21 2005
From: paulangonzalez at hotmail.com (pau gonzalez)
Date: Mon, 31 Jan 2005 01:44:21 +0000
Subject: [R] resampling two samples from a matrix
In-Reply-To: <Pine.LNX.4.61.0501301938590.29742@gannet.stats>
Message-ID: <BAY10-F7A7330DB2307FBC6EF3B8C67C0@phx.gbl>



From tlumley at u.washington.edu  Mon Jan 31 03:56:46 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 30 Jan 2005 18:56:46 -0800 (PST)
Subject: [R] Startup Files (RProfile) and R-Aqua
In-Reply-To: <19e940812011f65c61b9492c8ef7ce2d@comcast.net>
References: <19e940812011f65c61b9492c8ef7ce2d@comcast.net>
Message-ID: <Pine.A41.4.61b.0501301847300.168150@homer05.u.washington.edu>

On Sun, 30 Jan 2005, Thomas Hopper wrote:

> Hello,
>
> I'm having some difficulty understanding the documentation relative to the 
> startup files with R-Aqua 2.0.1 for Mac OS X.
>
> Specifically, I'm wondering: where does R search for the startup files (my 
> home directory at Users:<me>:?);

Yes.

> how should they be named (.RProfile will be 
> treated by Mac OS X as a system file and be hidden, so I'm wondering if it 
> should just be RProfile)?

No, it should be .Rprofile

>		 I can't find this platform-specific information in 
> any of the documentation.

That's because it isn't actually platform-specific.  OS X is like other 
Unix flavours in this respect.

> As a related issue, where would I store my home-grown functions? Would 
> RProfile be a good place to keep them?

Not especially, though there isn't anything really wrong with it.  You 
could also save() them in a binary file and load that file in your 
.Rprofile, or put them in a package.

Putting them in a package allows them to have documentation accessed with 
help().

Putting them in .Rprofile probably makes it harder to edit your functions, 
and it certainly makes it harder to separate functions for different 
purposes.


 	-thomas



From faheem at email.unc.edu  Mon Jan 31 06:00:22 2005
From: faheem at email.unc.edu (Faheem Mitha)
Date: Mon, 31 Jan 2005 00:00:22 -0500 (EST)
Subject: [R] type of list elements in .Call
Message-ID: <Pine.LNX.4.61.0501302353450.10677@Chrestomanci>


Dear People,

Here is something I do not understand. Consider

*************************************************
foo.cc
*************************************************
#include <iostream>
#include <R.h>
#include <Rinternals.h>

using std::cout;
using std::endl;

extern "C"
{
   SEXP printlst(SEXP lst);
}

SEXP printlst(SEXP lst)
{
   for(int i=0; i<length(lst); i++)
     for(int j=0; j<length(VECTOR_ELT(lst, i)); j++)
       cout << INTEGER(VECTOR_ELT(lst, i))[j] << endl;
   return lst;
}
*************************************************

> dyn.load("foo.so")
> .Call("printlst", list(c(1,2),c(3,4)))
0
1072693248
0
1074266112
[[1]]
[1] 1 2

[[2]]
[1] 3 4

If I replace INTEGER by REAL I get

> dyn.load("foo.so")
> .Call("printlst", list(c(1,2),c(3,4)))
1
2
3
4
[[1]]
[1] 1 2

[[2]]
[1] 3 4

as I would expect. I thought that if the vectors in the list could be 
regarded as integer vectors, they would be, but apparently not. Is there 
any way I can tell R to regard them as integer vectors?

Thanks.                                                    Faheem.



From Paul.Sorenson at vision-bio.com  Mon Jan 31 06:16:35 2005
From: Paul.Sorenson at vision-bio.com (Paul Sorenson)
Date: Mon, 31 Jan 2005 16:16:35 +1100
Subject: [R] aggregating dates
Message-ID: <5E06BFED29594F4C9C5EBE230DE320C6068027CD@ewok.vsl.com.au>

I have a frame which contains 3 columns:

"date" "defectnum" "state"

And I want to get the most recent state change for a given defect number.  date is POSIXct.

I have tried:
	aggregate(ev$date, by=list(ev$defectnum), max)

Which appears to be working except that the dates seem to come back as integers (presumably the internal representation of POSIXct).

When I execute max(ev$date) the result remains POSIXct.

I have been dredging through the help among DateTimeClasses and haven't found a function that converts these integers to some kind of date class.  Or a method for using aggregate which doesn't perform the conversion in the first place.

Any clues?



From Tom.Mulholland at dpi.wa.gov.au  Mon Jan 31 07:04:09 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Mon, 31 Jan 2005 14:04:09 +0800
Subject: [R] aggregating dates
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3C97F@afhex01.dpi.wa.gov.au>

This seems to work

toPOSIX <- function(x){
  y <- x -  as.numeric(ISOdate(2005,1,1))
  z <- ISOdate(2005,1,1) + y
  return(z)
  }

test <- as.numeric(ISOdate(2005,3,1) )
toPOSIX(test)

But whether one should be doing this I don't know. There are certainly functions that play aorund with the POSIX format, I have always assumed that I'm missing something fundamental. But I have always found it easier to write a function like this than to find out what I should have done to keep the format intact in the first place.

Tom

The caveat is the critical or otherwise nature of the endeavour. If it matters, find the right answer or convince yourself that this code works.

> -----Original Message-----
> From: Paul Sorenson [mailto:Paul.Sorenson at vision-bio.com]
> Sent: Monday, 31 January 2005 1:17 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] aggregating dates
> 
> 
> I have a frame which contains 3 columns:
> 
> "date" "defectnum" "state"
> 
> And I want to get the most recent state change for a given 
> defect number.  date is POSIXct.
> 
> I have tried:
> 	aggregate(ev$date, by=list(ev$defectnum), max)
> 
> Which appears to be working except that the dates seem to 
> come back as integers (presumably the internal representation 
> of POSIXct).
> 
> When I execute max(ev$date) the result remains POSIXct.
> 
> I have been dredging through the help among DateTimeClasses 
> and haven't found a function that converts these integers to 
> some kind of date class.  Or a method for using aggregate 
> which doesn't perform the conversion in the first place.
> 
> Any clues?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From patrick.giraudoux at univ-fcomte.fr  Mon Jan 31 08:06:14 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux H)
Date: Mon, 31 Jan 2005 08:06:14 +0100
Subject: [R] MacOS X and vectorized files (emf, wmf,...)
Message-ID: <5.0.2.1.2.20050131080559.02ba0d90@utinam.univ-fcomte.fr>

Dear Listers,

We are organising practical trainings for students with R 2.0.1 under MacOS 
X. I am used with R 2.0.1 under Windows XP and thus has been surprised not 
to find functions in the MacOS X version of R providing vectorized chart 
outputs to a file. For instance the equivalent of:

win.metafile()

or

savePlot()

... including a wmf or emf option.

Can one obtain only jpeg or bitmap or eps files with R under MacOS X or did 
I miss something?

Patrick Giraudoux



From patrick.giraudoux at univ-fcomte.fr  Mon Jan 31 08:06:47 2005
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux H)
Date: Mon, 31 Jan 2005 08:06:47 +0100
Subject: [R] New user...tips for spdep
Message-ID: <5.0.2.1.2.20050131080641.02baf5e8@utinam.univ-fcomte.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050131/ef9de226/attachment.pl

From roebuck at odin.mdacc.tmc.edu  Mon Jan 31 08:31:26 2005
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Mon, 31 Jan 2005 01:31:26 -0600 (CST)
Subject: [R] Startup Files (RProfile) and R-Aqua
In-Reply-To: <19e940812011f65c61b9492c8ef7ce2d@comcast.net>
References: <19e940812011f65c61b9492c8ef7ce2d@comcast.net>
Message-ID: <Pine.OSF.4.58.0501310053360.397214@odin.mdacc.tmc.edu>

On Sun, 30 Jan 2005, Thomas Hopper wrote:

> I'm having some difficulty understanding the documentation relative to
> the startup files with R-Aqua 2.0.1 for Mac OS X.
>
> Specifically, I'm wondering: where does R search for the startup files
> (my home directory at Users:<me>:?); how should they be named
> (.RProfile will be treated by Mac OS X as a system file and be hidden,
> so I'm wondering if it should just be RProfile)? I can't find this
> platform-specific information in any of the documentation.

Should be "$HOME/.Rprofile" for user-level default setting and
will be hidden by Finder. It's not something you're going to be
changing daily anyway and can be modified using TextWrangler's
'Open Hidden..." menu option by non-Terminal users.

This is not platform-specific and is described in Appendix B of
'Introduction to R' (pg 85 of R-intro.pdf).

> As a related issue, where would I store my home-grown functions?
> Would .RProfile be a good place to keep them?

I keep .First and .Last and a couple functions that pose as
typing shortcuts in ".Rprofile". Anything else I source an
external file..

Create a subdirectory under "~/Library/R" and store functions
there if nowhere else makes sense.

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From roebuck at odin.mdacc.tmc.edu  Mon Jan 31 08:37:55 2005
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Mon, 31 Jan 2005 01:37:55 -0600 (CST)
Subject: [R] type of list elements in .Call
In-Reply-To: <Pine.LNX.4.61.0501302353450.10677@Chrestomanci>
References: <Pine.LNX.4.61.0501302353450.10677@Chrestomanci>
Message-ID: <Pine.OSF.4.58.0501310134460.397214@odin.mdacc.tmc.edu>

On Mon, 31 Jan 2005, Faheem Mitha wrote:

> [SNIP]
> as I would expect. I thought that if the vectors in the list could be
> regarded as integer vectors, they would be, but apparently not. Is there
> any way I can tell R to regard them as integer vectors?

.Call("printlst", list(as.integer(c(1,2)),
                       as.integer(c(3,4)))

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From jarioksa at sun3.oulu.fi  Mon Jan 31 09:04:33 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Mon, 31 Jan 2005 10:04:33 +0200
Subject: [R] MacOS X and vectorized files (emf, wmf,...)
In-Reply-To: <5.0.2.1.2.20050131080559.02ba0d90@utinam.univ-fcomte.fr>
References: <5.0.2.1.2.20050131080559.02ba0d90@utinam.univ-fcomte.fr>
Message-ID: <1107158673.5422.2.camel@biol102145.oulu.fi>

On Mon, 2005-01-31 at 08:06 +0100, Patrick Giraudoux H wrote:
> Dear Listers,
> 
> We are organising practical trainings for students with R 2.0.1 under MacOS 
> X. I am used with R 2.0.1 under Windows XP and thus has been surprised not 
> to find functions in the MacOS X version of R providing vectorized chart 
> outputs to a file. For instance the equivalent of:
> 
> win.metafile()
> 
> or
> 
> savePlot()
> 
> ... including a wmf or emf option.
> 
> Can one obtain only jpeg or bitmap or eps files with R under MacOS X or did 
> I miss something?
> 
Saving a plot from a menu bar (click "save") saves a plot in pdf which
is vectorized graphic format native to MacOS X. Further, dev.copy2eps()
work normally, as do postcript() and pdf() devices. See appropriate help
pages.

Native MS Windows formats (such as wmf) may not work, but who needs
them?

cheers, jari oksanen 
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From ripley at stats.ox.ac.uk  Mon Jan 31 09:05:10 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 31 Jan 2005 08:05:10 +0000 (GMT)
Subject: [R] type of list elements in .Call
In-Reply-To: <Pine.LNX.4.61.0501302353450.10677@Chrestomanci>
References: <Pine.LNX.4.61.0501302353450.10677@Chrestomanci>
Message-ID: <Pine.LNX.4.61.0501310804200.12049@gannet.stats>

Please ask programming-related questions on the R-devel list -- see the 
advice in the posting guide.

On Mon, 31 Jan 2005, Faheem Mitha wrote:

>
> Dear People,
>
> Here is something I do not understand. Consider
>
> *************************************************
> foo.cc
> *************************************************
> #include <iostream>
> #include <R.h>
> #include <Rinternals.h>
>
> using std::cout;
> using std::endl;
>
> extern "C"
> {
>  SEXP printlst(SEXP lst);
> }
>
> SEXP printlst(SEXP lst)
> {
>  for(int i=0; i<length(lst); i++)
>    for(int j=0; j<length(VECTOR_ELT(lst, i)); j++)
>      cout << INTEGER(VECTOR_ELT(lst, i))[j] << endl;
>  return lst;
> }
> *************************************************
>
>> dyn.load("foo.so")
>> .Call("printlst", list(c(1,2),c(3,4)))
> 0
> 1072693248
> 0
> 1074266112
> [[1]]
> [1] 1 2
>
> [[2]]
> [1] 3 4
>
> If I replace INTEGER by REAL I get
>
>> dyn.load("foo.so")
>> .Call("printlst", list(c(1,2),c(3,4)))
> 1
> 2
> 3
> 4
> [[1]]
> [1] 1 2
>
> [[2]]
> [1] 3 4
>
> as I would expect. I thought that if the vectors in the list could be 
> regarded as integer vectors, they would be, but apparently not. Is there any 
> way I can tell R to regard them as integer vectors?
>
> Thanks.                                                    Faheem.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jan 31 09:14:42 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 31 Jan 2005 08:14:42 +0000 (GMT)
Subject: [R] MacOS X and vectorized files (emf, wmf,...)
In-Reply-To: <5.0.2.1.2.20050131080559.02ba0d90@utinam.univ-fcomte.fr>
References: <5.0.2.1.2.20050131080559.02ba0d90@utinam.univ-fcomte.fr>
Message-ID: <Pine.LNX.4.61.0501310806250.12049@gannet.stats>

On Mon, 31 Jan 2005, Patrick Giraudoux H wrote:

> Dear Listers,
>
> We are organising practical trainings for students with R 2.0.1 under MacOS 
> X. I am used with R 2.0.1 under Windows XP and thus has been surprised not to 
> find functions in the MacOS X version of R providing vectorized chart outputs 
> to a file. For instance the equivalent of:
>
> win.metafile()
>
> or
>
> savePlot()
>
> ... including a wmf or emf option.
>
> Can one obtain only jpeg or bitmap or eps files with R under MacOS X or did I 
> miss something?

Why did you expect to find Windows-specific formats under MacOS X?  (BTW, 
these are often bitmap formats.)

You missed pdf (the most obvious one on MacOS X whose native graphic is 
based on PDF) and svgDevice on CRAN.

Both postscript() and pdf() provide vector graphics (which may be what you 
mean by `vectorized chart') plots on all R platforms, as does xfig() 
although it is less useful unless you have xfig/fig2dev.

You will find lots of mentions over the years about making wmf/emf on 
Unix, but that is for use on Windows.  The format is poorly documented, 
but it would be possible to write a wmf() graphics device -- no one has 
contributed one as yet.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mikewhite.diu at tiscali.co.uk  Mon Jan 31 09:51:59 2005
From: mikewhite.diu at tiscali.co.uk (Mike White)
Date: Mon, 31 Jan 2005 08:51:59 -0000
Subject: [R] Extracting a numeric prefix from a string
Message-ID: <003e01c50772$2091a810$29122850@FSSFQCV7BGDVED>

Hi
Does anyone know if there is a function similar to as.numeric that will
extract a numeric prefix from a string as in the following examples?

x<-c(3, "abc", 5.67, "2.4a", "6a", "6b", "2.4.a", 3, "4.2a")
df.x<-data.frame(Code=x)
x.str<-levels(df.x[,1])
# required function  result
2.40 3.00 4.20 5.67 6.00 NA

Thanks
Mike White



From R-user at zutt.org  Mon Jan 31 10:33:37 2005
From: R-user at zutt.org (R user)
Date: Mon, 31 Jan 2005 10:33:37 +0100
Subject: [R] Extracting a numeric prefix from a string
In-Reply-To: <003e01c50772$2091a810$29122850@FSSFQCV7BGDVED>
References: <003e01c50772$2091a810$29122850@FSSFQCV7BGDVED>
Message-ID: <1107164017.21805.13.camel@dutiih.st.ewi.tudelft.nl>

You could use something like

y <- gsub('([0-9]+(.[0-9]+)?)?.*','\\1',x)
as.numeric(y)

But maybe there's a much nicer way.

Jonne.

On Mon, 2005-01-31 at 08:51 +0000, Mike White wrote:
> Hi
> Does anyone know if there is a function similar to as.numeric that will
> extract a numeric prefix from a string as in the following examples?
> 
> x<-c(3, "abc", 5.67, "2.4a", "6a", "6b", "2.4.a", 3, "4.2a")
> df.x<-data.frame(Code=x)
> x.str<-levels(df.x[,1])
> # required function  result
> 2.40 3.00 4.20 5.67 6.00 NA
> 
> Thanks
> Mike White
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
R user <R-user at zutt.org>



From thewavyx at gmail.com  Mon Jan 31 11:38:28 2005
From: thewavyx at gmail.com (Eric Rodriguez)
Date: Mon, 31 Jan 2005 11:38:28 +0100
Subject: [R] Propositions to fit a 2 pieces function
Message-ID: <47779aab050131023855d9d0f2@mail.gmail.com>

Hi,

I would like to have some advices about how to fit such function:

f(x) =  a.x^b                      if x <= t
          c.x - c.t + a.t^b       otherwise

I have to minimize the least-squares error  knowing x and f(x).

Do you have any propositions of what should I use to achieve it ?

Thanks a lot !

Eric



From ligges at statistik.uni-dortmund.de  Mon Jan 31 12:58:30 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 31 Jan 2005 12:58:30 +0100
Subject: [R] Propositions to fit a 2 pieces function
In-Reply-To: <47779aab050131023855d9d0f2@mail.gmail.com>
References: <47779aab050131023855d9d0f2@mail.gmail.com>
Message-ID: <41FE1D66.2020700@statistik.uni-dortmund.de>

Eric Rodriguez wrote:

> Hi,
> 
> I would like to have some advices about how to fit such function:
> 
> f(x) =  a.x^b                      if x <= t
>           c.x - c.t + a.t^b       otherwise
> 
> I have to minimize the least-squares error  knowing x and f(x).
> 
> Do you have any propositions of what should I use to achieve it ?

That's what ifelse() has been invented for.
Are you going to apply Nelder_Mead, BFGS or what? BFGS might be very 
problematic in this case (untested) ...

Uwe Ligges




> Thanks a lot !
> 
> Eric
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From vito_ricci at yahoo.com  Mon Jan 31 14:12:39 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Mon, 31 Jan 2005 14:12:39 +0100 (CET)
Subject: [R] Re: Begginer with R
Message-ID: <20050131131239.61768.qmail@web41205.mail.yahoo.com>

Hi,

see this web page:

http://www.stats.ox.ac.uk/~vos/DataMining.html

you'll find something about classification with
examples:
4. Classification
Decision theory, Linear discriminant analysis,
Quadratic discriminant analysis, Fisher's LDA,
Logistic discrimination, Non-parametric classification
tools, Classifier assessment, Tree-based  classifiers,
Bagging  and Boosting 

Best regards,
Vito


you wrote:

Hello all,

Im just beggining using R. I have read a couple of
introductory documents
but they are very general. Is there a document focused
on classification?
(this is what Ill be using R for). Examples would be
just fine.
Thanks in advance,

Ulises

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From cg.pettersson at evp.slu.se  Mon Jan 31 14:13:38 2005
From: cg.pettersson at evp.slu.se (CG Pettersson)
Date: Mon, 31 Jan 2005 14:13:38 +0100
Subject: [R] Postgraduate distance learning courses for MSc in Statistics
Message-ID: <200501311313.j0VDDcId028761@mail1.slu.se>

30 Januari, Tilo Blenk  wrote:

<Does anybody has experience with postgraduate distance learning
<courses 
<resulting in a MSc in Statistics? I am thinking about such a course
to 
 ... snip ...
<Sorry for posting something only distantly related to R but I hope 
<there some people reading this mailing list who could help me. 
<Moreover, R is one of the reasons why I got interested in statistics.

<Tilo

Dear Tilo
As it was surprisingly silent on this question, I can offer a tip for
you, 
nearer than both England or USA:

At KVL in Copenhagen, they give several courses with well developed
distance methodology. They use both SAS and R, not totally parallell 
though..

Check out the following link:

http://statmaster.sdu.dk/

Cheers
/CG


CG Pettersson, MSci, PhD Stud.
Swedish University of Agricultural Sciences
Dep. of Ecology and Crop Production. Box 7043
SE-750 07 Uppsala
Sweden



From roebuck at odin.mdacc.tmc.edu  Mon Jan 31 14:29:46 2005
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Mon, 31 Jan 2005 07:29:46 -0600 (CST)
Subject: [R] Re: [Rd] Typo in 'R Language Definition'
In-Reply-To: <Pine.LNX.4.61.0501311100100.20280@gannet.stats>
References: <Pine.OSF.4.58.0501301627200.374256@odin.mdacc.tmc.edu>
	<Pine.LNX.4.61.0501311100100.20280@gannet.stats>
Message-ID: <Pine.OSF.4.58.0501310721250.414441@odin.mdacc.tmc.edu>

On Mon, 31 Jan 2005, Prof Brian Ripley wrote:

> On Sun, 30 Jan 2005, Paul Roebuck wrote:
>
> > Section 3.5.3 The call stack (pg 23 of R-lang.pdf)
> > ...the computation the the currently active environment...
> >                   ^^^
>
> Which version of R is this?  I think it has been corrected a
> while ago in R-patched and R-devel.  Do see the posting guide...

version 2.0.1 (2004-11-15) DRAFT
If it has been corrected, the changes never made it to the
web site.

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From jarioksa at sun3.oulu.fi  Mon Jan 31 14:42:56 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Mon, 31 Jan 2005 15:42:56 +0200
Subject: [R] Bootstrapped eigenvector
In-Reply-To: <001401c50636$c14eba50$5006cb84@yourd93e6doqk7>
References: <001401c50636$c14eba50$5006cb84@yourd93e6doqk7>
Message-ID: <1107178976.5422.20.camel@biol102145.oulu.fi>

J?r?me,


On Sat, 2005-01-29 at 14:14 -0500, J?r?me Lema?tre wrote:
> Hello alls,
> 
> I found in the literature a technique that has been evaluated as one of the
> more robust to assess statistically the significance of the loadings in a
> PCA: bootstrapping the eigenvector (Jackson, Ecology 1993, 74: 2204-2214;
> Peres-Neto and al. 2003. Ecology 84:2347-2363). However, I'm not able to
> transform by myself the following steps into a R program, yet?
> 
> Can someone could help me with this?
> 
> I thank you very much by advance.
> 
> Here are the steps that I need to perform:
> 1) Resample 1000 times with replacement entire raws from the original data
> sets (7 variables, 126 raws)
> 2) Conduct a PCA on each bootstrapped sample
> 3) To prevent axis reflexion and/or axis reordering in the bootstrap, here
> are two more steps for each bootstrapped sample
> 3a) calculate correlation matrix between the PCA scores of the original and
> those of the bootstrapped sample
> 3b) Examine whether the highest absolute correlation is between the
> corresponding axis for the original and bootstrapped samples. When it is not
> the case, reorder the eigenvectors. This means that if the highest
> correlation is between the first original axis and the second bootstrapped
> axis, the loadings for the second bootstrapped axis and use to estimate the
> confidence interval for the original first PC axis.
> 4) Determine the p value for each loading. Obtained as follow: number of
> loadings >=0 for loadings that were positive in the original matrix divided
> by the number of boostrap samples (1000) and/or number of loadings =<0 for
> loadings that were negative in the original matrix divided by the number of
> boostrap samples (1000).
> 
The following function seems to run the analysis like Peres-Neto and
others defined:

> netoboot
function (x, permutations=1000, ...)
{
   pcnull <- princomp(x, ...)
   res <- pcnull$loadings
   out <- matrix(0, nrow=nrow(res), ncol=ncol(res))
   N <- nrow(x)
   for (i in 1:permutations) {
       pc <- princomp(x[sample(N, replace=TRUE), ], ...)
       pred <- predict(pc, newdata = x)
       r <-  cor(pcnull$scores, pred)
       k <- apply(abs(r), 2, which.max)
       reve <- sign(diag(r[k,]))
       sol <- pc$loadings[ ,k]
       sol <- sweep(sol, 2, reve, "*")
       out <- out + ifelse(res > 0, sol <=  0, sol >= 0)
   }
   out/permutations
}

With typical chemical data, you should pass option cor = TRUE to
princomp. Another issue is whether you should use this method. Opinions
may be divided here, but I'll let that to the proper Statistician to
comment on.

Best wishes, Jari Oksanen

-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From rainer.grohmann at gmx.net  Mon Jan 31 15:14:45 2005
From: rainer.grohmann at gmx.net (rainer grohmann)
Date: Mon, 31 Jan 2005 15:14:45 +0100 (MET)
Subject: [R] Results of MCD estimators in MASS and rrcov
References: <OF28E995B8.06ADFB96-ONC1256F9A.004DE2AC-C1256F9A.004DE8DC@EU.novartis.net>
Message-ID: <21454.1107180885@www4.gmx.net>

Thanks a lot!

Indeed, both implementations agree on the 'best' points. Your answer helped
me a great deal.

Rainer

> The two implementations use different consistency factors as well as
> different small sample correction factors.
> 
> 1. The search parts of both implementations produce the same result -
> compare rrcov.mcd$best and mass.mcd$best.
> 
> 2. The raw MCD covariance matrix is corrected as follows:
> 
> MASS:
>  - Rousseeuw and Leroy (1987), p.259 (eq. 1.26)
>  - Marazzi (1993) (or may be Rousseeuw and van Zomeren (1900) p.638 (eq 
> A.9)
> 
> rrcov:
>  - Croux and Haesbroeck (1999), Pison et.al. p. 337
>  - Pison et.al. (2002), p.338
> 
> 3. The reweighted (final) covariance matrix is corrected as follows:
> 
> MASS: no correction
> rrcov: Pison et.al. (2002) p. 339
> 
> This explains the different covariance matrices.
> As far as the location is concerned, in this particular case the raw MCD
> estimates in MASS identify one additional outlier - observation 53, which 
> is
> discarded from the computation of the reweighted estimates.
> Look at the following plots and judge yourself if this is an outlier or 
> not:
> 
>   covPlot(hbk, mcd=rrcov.mcd, which="distance", id.n=15)
>   covPlot(hbk, mcd=mass.mcd, which="distance", id.n=15)
> 
> valentin
> 

-- 
GMX im TV ... Die Gedanken sind frei ... Schon gesehen?
Jetzt Spot online ansehen: http://www.gmx.net/de/go/tv-spot



From faceasec at uapar.edu  Mon Jan 31 15:36:25 2005
From: faceasec at uapar.edu (=?ISO-8859-1?Q?=22Lic=2E_Adri=E1n_Cecotto_-_FACEA=22?=)
Date: Mon, 31 Jan 2005 11:36:25 -0300
Subject: [R] consultation
Message-ID: <41FE4269.6080901@uapar.edu>

R people,
I need to know if is possible to make data mining with R. If so, is 
there any manual or somewhere/one to consult about that.
Thank you very much
Adri?n

From ligges at statistik.uni-dortmund.de  Mon Jan 31 15:44:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 31 Jan 2005 15:44:44 +0100
Subject: [R] consultation
In-Reply-To: <41FE4269.6080901@uapar.edu>
References: <41FE4269.6080901@uapar.edu>
Message-ID: <41FE445C.7090908@statistik.uni-dortmund.de>

Lic. Adri?n Cecotto - FACEA wrote:

> R people,
> I need to know if is possible to make data mining with R. If so, is 

Almost sure, depends on your definition of data mining.
There are several manuals and books, see CRAN.

Uwe Ligges


> there any manual or somewhere/one to consult about that.
> Thank you very much
> Adri?n
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Mon Jan 31 15:56:41 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 31 Jan 2005 14:56:41 +0000 (UTC)
Subject: [R] aggregating dates
References: <5E06BFED29594F4C9C5EBE230DE320C6068027CD@ewok.vsl.com.au>
Message-ID: <loom.20050131T153150-992@post.gmane.org>

Paul Sorenson <Paul.Sorenson <at> vision-bio.com> writes:

: 
: I have a frame which contains 3 columns:
: 
: "date" "defectnum" "state"
: 
: And I want to get the most recent state change for a given defect number.  
date is POSIXct.
: 
: I have tried:
: 	aggregate(ev$date, by=list(ev$defectnum), max)
: 
: Which appears to be working except that the dates seem to come back as 
integers (presumably the internal
: representation of POSIXct).
: 
: When I execute max(ev$date) the result remains POSIXct.
: 
: I have been dredging through the help among DateTimeClasses and haven't 
found a function that converts
: these integers to some kind of date class.  Or a method for using aggregate 

RNews 4/1 has an article with this and other date conversions.  

: which doesn't perform the
: conversion in the first place.

You could sort ev by date and use aggregate on an index vector (untested):

ev <- ev[order(ev$date),]
ev[aggregate(1:nrow(ev), list(ev$defectnum), max),]



From vito_ricci at yahoo.com  Mon Jan 31 16:03:34 2005
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Mon, 31 Jan 2005 16:03:34 +0100 (CET)
Subject: [R] Re: consultation
Message-ID: <20050131150335.87648.qmail@web41201.mail.yahoo.com>

Hi, 

see these papers or books (some are available on the
web):

Diego Kuonen, Introduction au data mining avec R :
vers la reconqu?te du `knowledge discovery in
databases' par les statisticiens. Bulletin of the
Swiss Statistical Society, 40:3-7, 2001.
http://www.statoo.com/en/publications/2001.R.SSS.40/

Diego Kuonen and Reinhard Furrer, Data mining avec R
dans un monde libre. Flash Informatique Special ?t?,
pages 45-50, sep 2001.
http://sawww.epfl.ch/SIC/SA/publications/FI01/fi-sp-1/sp-1-page45.html

Brian D. Ripley, Datamining: Large Databases and
Methods, in Proceedings of ?useR! 2004 - The R User
Conference
http://www.ci.tuwien.ac.at/Conferences/useR-2004/Keynotes/Ripley.pdf

Brian D. Ripley, Using Databases with R, R News,
http://cran.r-project.org/doc/Rnews/Rnews_2001-1.pdf

B. D. Ripley, R. M. Ripley, Applications of R Clients
and Servers in Proceedings of the Distributed
Statistical Computing 2001 Workshop, 2001, Vienna
University of Technology.
http://www.ci.tuwien.ac.at/Conferences/DSC-2001/Proceedings/Ripley.pdf

Torsten Hothorn, David A. James, Brian D. Ripley, R/S
Interfaces to Databases in Proceedings of the
Distributed Statistical Computing 2001 Workshop,
2001,Vienna University of Technology.
http://www.ci.tuwien.ac.at/Conferences/DSC-2001/Proceedings/HothornJamesRipley.pdf

Luis Torgo, Data Mining with R. Learning by case
studies, Maggio 2003
http://www.liacc.up.pt/~ltorgo/DataMiningWithR/
        
Trevor Hastie , Robert Tibshirani, Jerome Friedman,
The Elements of Statistical Learning: Data Mining,
Inference, and Prediction, 2001, Springer-Verlag.
http://www-stat.stanford.edu/~tibs/ElemStatLearn/ 

Data Mining for Bioinformatics 
http://www.stats.ox.ac.uk/~vos/DataMining.html

Hoping I helped you.
Cordially,
Vito

you wrote:
R people,I need to know if is possible to make data
mining with R. If so, is there any manual or
somewhere/one to consult about that.
Thank you very much
Adri?n


=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box

Top 10 reasons to become a Statistician

     1. Deviation is considered normal
     2. We feel complete and sufficient
     3. We are 'mean' lovers
     4. Statisticians do it discretely and continuously
     5. We are right 95% of the time
     6. We can legally comment on someone's posterior distribution
     7. We may not be normal, but we are transformable
     8. We never have to say we are certain
     9. We are honestly significantly different
    10. No one wants our jobs


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From minhan.science at gmail.com  Mon Jan 31 17:11:08 2005
From: minhan.science at gmail.com (Min-Han Tan)
Date: Mon, 31 Jan 2005 11:11:08 -0500
Subject: [R] Postgraduate distance learning courses for MSc in Statistics
In-Reply-To: <200501311313.j0VDDcId028761@mail1.slu.se>
References: <200501311313.j0VDDcId028761@mail1.slu.se>
Message-ID: <7902152a0501310811529c3a2c@mail.gmail.com>

There are several institutions here which offer distance learning.

http://programs.gradschools.com/distance/statistics.html

Min-Han Tan

On Mon, 31 Jan 2005 14:13:38 +0100, CG Pettersson
<cg.pettersson at evp.slu.se> wrote:
> 30 Januari, Tilo Blenk  wrote:
> 
> <Does anybody has experience with postgraduate distance learning
> <courses
> <resulting in a MSc in Statistics? I am thinking about such a course
> to
>  ... snip ...
> <Sorry for posting something only distantly related to R but I hope
> <there some people reading this mailing list who could help me.
> <Moreover, R is one of the reasons why I got interested in statistics.
> 
> <Tilo
> 
> Dear Tilo
> As it was surprisingly silent on this question, I can offer a tip for
> you,
> nearer than both England or USA:
> 
> At KVL in Copenhagen, they give several courses with well developed
> distance methodology. They use both SAS and R, not totally parallell
> though..
> 
> Check out the following link:
> 
> http://statmaster.sdu.dk/
> 
> Cheers
> /CG
> 
> CG Pettersson, MSci, PhD Stud.
> Swedish University of Agricultural Sciences
> Dep. of Ecology and Crop Production. Box 7043
> SE-750 07 Uppsala
> Sweden
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Ted.Harding at nessie.mcc.ac.uk  Mon Jan 31 17:09:49 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 31 Jan 2005 16:09:49 -0000 (GMT)
Subject: [R] Extracting a numeric prefix from a string
In-Reply-To: <1107164017.21805.13.camel@dutiih.st.ewi.tudelft.nl>
Message-ID: <XFMail.050131160949.Ted.Harding@nessie.mcc.ac.uk>

On 31-Jan-05 R user wrote:
> You could use something like
> 
> y <- gsub('([0-9]+(.[0-9]+)?)?.*','\\1',x)
> as.numeric(y)
> 
> But maybe there's a much nicer way.
> 
> Jonne.

I doubt it -- full marks for neat regexp footwork!

To comply fully with Mike's request, I'd wrap it in sort():

  sort(as.numeric(y),na.last=TRUE)
  [1] 2.40 2.40 3.00 3.00 4.20 5.67 6.00 6.00   NA

or even

  unique(sort(as.numeric(y),na.last=TRUE))
  [1] 2.40 3.00 4.20 5.67 6.00   NA

Cheers,
Ted.

> On Mon, 2005-01-31 at 08:51 +0000, Mike White wrote:
>> Hi
>> Does anyone know if there is a function similar to as.numeric that
>> will
>> extract a numeric prefix from a string as in the following examples?
>> 
>> x<-c(3, "abc", 5.67, "2.4a", "6a", "6b", "2.4.a", 3, "4.2a")
>> df.x<-data.frame(Code=x)
>> x.str<-levels(df.x[,1])
>> # required function  result
>> 2.40 3.00 4.20 5.67 6.00 NA
>> 
>> Thanks
>> Mike White
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
> -- 
> R user <R-user at zutt.org>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 31-Jan-05                                       Time: 16:09:49
------------------------------ XFMail ------------------------------



From drcarbon at gmail.com  Mon Jan 31 18:04:12 2005
From: drcarbon at gmail.com (Dr Carbon)
Date: Mon, 31 Jan 2005 12:04:12 -0500
Subject: [R] changing the time base in a ts
Message-ID: <e89bb7ac0501310904786aebc5@mail.gmail.com>

I'm probably apporaching this all wrong to start but....

Suppose I have a monthly time series and I want to compute the mean of
months 6,7, and 8. I want to plot the original time series and the
seasonal time series, one above the other. When I do that as below the
time series don't line up for reasons that are obvious. How can I
change the base of the seasonal time series so I can make my plots
line up? That is, I want the points for the seasonal plot to line up
with cycle 6 of the first plot.

Thanks. 


dat <- ts(rnorm(12*20), start = c(1980,1), frequency = 12)
plot(dat)
dat.sub <- dat
dat.sub[cycle(dat.sub) < 6] <- NA
dat.sub[cycle(dat.sub) > 8] <- NA
dat.sub <- aggregate(dat.sub, nfrequency = 1, FUN = mean, na.rm = T)
tsp(dat)
tsp(dat.sub)
par(mfrow = c(2, 1))
plot(dat)
plot(dat.sub, type = "b")



From eesteves at ualg.pt  Mon Jan 31 18:29:42 2005
From: eesteves at ualg.pt (eesteves@ualg.pt)
Date: Mon, 31 Jan 2005 17:29:42 +0000
Subject: [R] help with vector operation
Message-ID: <20050131172942.gsxdhmhmtc4gk8ks@wmail.ualg.pt>

Dear All,

I'm trying to calculate the following:

g.rate<-(SL-int)/NO

where SL and NO are individual length and counts for the same subject, and the
int value is a day-specific value i.e. for each DAY=88, 101..., 172 I'd like to
use a different values of int=9.32, 8.43, ..., 9.81. All variables are compiled
in a larger data.frame of the form:

CODE  SL    NO  DAY (...)
123   12.5  14  88
124   11.4  13  88
125   10.8  11  101
(...)
234   16.9  19  172 (...)

I've been trying apply() but with little (NONE) success!
Thanks, Eduardo Esteves



From numero.primo at tele2.it  Mon Jan 31 18:30:24 2005
From: numero.primo at tele2.it (Landini Massimiliano)
Date: Mon, 31 Jan 2005 18:30:24 +0100
Subject: [R] Box-Cox / data transformation question
In-Reply-To: <41FD6403.8080505@nauticom.net>
References: <41F65AE5.4020601@uni-jena.de>
	<n75qv0l7rgshubin4k53c4v1d0fbco7gkt@4ax.com>
	<41FD6403.8080505@nauticom.net>
Message-ID: <t6osv09hq7mqoqhbo5f3k6ac8vhqgum542@4ax.com>

On Sun, 30 Jan 2005 17:47:31 -0500, you wrote:

<<<<<-----------------SNIP
|=[:o)  >
|=[:o)  >  
|=[:o)  >
|=[:o)  Why are you using a double square root transformation? Is the 
|=[:o)  transformation for the response variable? Transfromation is one way to 
|=[:o)  help insure that the error distribution is at least approximately 
|=[:o)  normal. So if this is the reason, it certainly could make sense. 

Are you sure that (data^0.25) had sense??? Coud you explain me which is the
sense??
I know sense of boxcox exponents near zero when data are positively skewed and
log(data) make it  normally distributed, or all those case where variances grow
proportionally to means or when i know that there are interaction effects that
not follow additive model (AnOVa assumption);

I know 0.5 exponent (square root) [ as sqr(data) if all data differ from zero
else sqr(data+.5) else Asconbe propose sqr(data +3/8) else Tukey & Freeman
propose sqr(data)+sqr(data+1) particularly suitable when data domain is  (0,2) ]
for right skewed data, frequently  applied to count-data or
count-of-something-over -a -surface (bacteria, virus, nematode, lions) due to
n*p*q (variance)  is almost proportional to its mean (n*p)  so AnOVa fundamental
assumption is basically violated....

I know 1/3 exponent  applied to count-of-something-in-a -volume...and so on...

What is worth is that i'm trying to ask to Christoph to sit down and think: what
kind of number are these??
E.coli/mL?? ...so...i try cuberoot transformation and/or log transformation
Timing of a slug vs snail speed race?? ...so..i think that inverse
transformation it best.

BoxCox procedure have produced a fantastic implement that can help many people
but (IMHO) none procedure can be superior than Ripley + Bates + other gurus
experience. If you ask to those great statisticians how do you manage
electrophoresis velocity they could respond with "data^-1 why......blah blah
blah"
If you push data in BoxCox algorithm it will respond with "-0.97847164..."
Which answer had more sense???
I prefer -1

 
|=[:o) There  is no unique scale for making measurements. We choose a scale that helps 
|=[:o)  us analyze the data appropriately.
|=[:o)  
|=[:o)  Rick B.
|=[:o)  
|=[:o)  ______________________________________________
|=[:o)  R-help at stat.math.ethz.ch mailing list
|=[:o)  https://stat.ethz.ch/mailman/listinfo/r-help
|=[:o)  PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
|=[:o)  
|=[:o)  
|=[:o)  ---
|=[:o)  avast! Antivirus: In arrivo messaggio pulito.
|=[:o)  Virus Database (VPS): 0504-4, 28/01/2005
|=[:o)  Controllato il: 31/01/2005 16.23.36
|=[:o)  avast! - copyright (c) 1988-2004 ALWIL Software.
|=[:o)  http://www.avast.com
|=[:o)  
|=[:o)  



-------------------------------------------------------------------------------------------------------------------------
Landini dr. Massimiliano
Tel. mob. (+39) 347 140 11 94
Tel./Fax. (+39) 051 762 196
e-mail: numero (dot) primo (at) tele2 (dot) it
-------------------------------------------------------------------------------------------------------------------------
Legge di Hanggi: Pi? stupida ? la tua ricerca, pi? verr? letta e approvata.
Corollario alla Legge di Hanggi: Pi? importante ? la tua ricerca, meno verr?
capita.



From ligges at statistik.uni-dortmund.de  Mon Jan 31 18:34:55 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 31 Jan 2005 18:34:55 +0100
Subject: [R] changing the time base in a ts
In-Reply-To: <e89bb7ac0501310904786aebc5@mail.gmail.com>
References: <e89bb7ac0501310904786aebc5@mail.gmail.com>
Message-ID: <41FE6C3F.5050709@statistik.uni-dortmund.de>

Dr Carbon wrote:
> I'm probably apporaching this all wrong to start but....
> 
> Suppose I have a monthly time series and I want to compute the mean of
> months 6,7, and 8. I want to plot the original time series and the
> seasonal time series, one above the other. When I do that as below the
> time series don't line up for reasons that are obvious. How can I
> change the base of the seasonal time series so I can make my plots
> line up? That is, I want the points for the seasonal plot to line up
> with cycle 6 of the first plot.
> 
> Thanks. 
> 
> 
> dat <- ts(rnorm(12*20), start = c(1980,1), frequency = 12)
> plot(dat)
> dat.sub <- dat
> dat.sub[cycle(dat.sub) < 6] <- NA
> dat.sub[cycle(dat.sub) > 8] <- NA
> dat.sub <- aggregate(dat.sub, nfrequency = 1, FUN = mean, na.rm = T)
> tsp(dat)
> tsp(dat.sub)
> par(mfrow = c(2, 1))
> plot(dat)
> plot(dat.sub, type = "b")


  plot(dat)
  plot(dat.sub, type = "b", xlim=c(1980, 2000))

Uwe Ligges


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From drcarbon at gmail.com  Mon Jan 31 18:41:49 2005
From: drcarbon at gmail.com (Dr Carbon)
Date: Mon, 31 Jan 2005 12:41:49 -0500
Subject: [R] changing the time base in a ts
In-Reply-To: <41FE6C3F.5050709@statistik.uni-dortmund.de>
References: <e89bb7ac0501310904786aebc5@mail.gmail.com>
	<41FE6C3F.5050709@statistik.uni-dortmund.de>
Message-ID: <e89bb7ac05013109415dfaf737@mail.gmail.com>

Sorry, what I meant was that I want dat.sub[1] to line up with dat[7],
dat.sub[2] to line up with dat[19], dat.sub[3] with dat[12+12+7],
etc...


On Mon, 31 Jan 2005 18:34:55 +0100, Uwe Ligges
<ligges at statistik.uni-dortmund.de> wrote:
> Dr Carbon wrote:
> > I'm probably apporaching this all wrong to start but....
> >
> > Suppose I have a monthly time series and I want to compute the mean of
> > months 6,7, and 8. I want to plot the original time series and the
> > seasonal time series, one above the other. When I do that as below the
> > time series don't line up for reasons that are obvious. How can I
> > change the base of the seasonal time series so I can make my plots
> > line up? That is, I want the points for the seasonal plot to line up
> > with cycle 6 of the first plot.
> >
> > Thanks.
> >
> >
> > dat <- ts(rnorm(12*20), start = c(1980,1), frequency = 12)
> > plot(dat)
> > dat.sub <- dat
> > dat.sub[cycle(dat.sub) < 6] <- NA
> > dat.sub[cycle(dat.sub) > 8] <- NA
> > dat.sub <- aggregate(dat.sub, nfrequency = 1, FUN = mean, na.rm = T)
> > tsp(dat)
> > tsp(dat.sub)
> > par(mfrow = c(2, 1))
> > plot(dat)
> > plot(dat.sub, type = "b")
> 
>   plot(dat)
>   plot(dat.sub, type = "b", xlim=c(1980, 2000))
> 
> Uwe Ligges
> 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From ligges at statistik.uni-dortmund.de  Mon Jan 31 18:46:11 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 31 Jan 2005 18:46:11 +0100
Subject: [R] help with vector operation
In-Reply-To: <20050131172942.gsxdhmhmtc4gk8ks@wmail.ualg.pt>
References: <20050131172942.gsxdhmhmtc4gk8ks@wmail.ualg.pt>
Message-ID: <41FE6EE3.3040603@statistik.uni-dortmund.de>

eesteves at ualg.pt wrote:

> Dear All,
> 
> I'm trying to calculate the following:

Select the rows of the data.frame and just calculate what you already 
have written:


> g.rate<-(SL-int)/NO

For the lines you cited this means, e.g.:

   X <- data.frame(CODE = c(123:125, 234),
                   SL = c(12.5, 11.4, 10.8, 16.9),
                   NO = c(14, 13, 11, 19),
                   DAY = c(88, 88, 101, 172))
   int <- c(9.32, 8.43, NA, 9.81)
   g.rate <- with(X, (SL - int) / NO)


That's it!

Uwe Ligges



> where SL and NO are individual length and counts for the same subject, and the
> int value is a day-specific value i.e. for each DAY=88, 101..., 172 I'd like to
> use a different values of int=9.32, 8.43, ..., 9.81. All variables are compiled
> in a larger data.frame of the form:
> 
> CODE  SL    NO  DAY (...)
> 123   12.5  14  88
> 124   11.4  13  88
> 125   10.8  11  101
> (...)
> 234   16.9  19  172 (...)
> 
> I've been trying apply() but with little (NONE) success!
> Thanks, Eduardo Esteves
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Mon Jan 31 18:51:31 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 31 Jan 2005 17:51:31 +0000 (UTC)
Subject: [R] changing the time base in a ts
References: <e89bb7ac0501310904786aebc5@mail.gmail.com>
Message-ID: <loom.20050131T185019-627@post.gmane.org>

Dr Carbon <drcarbon <at> gmail.com> writes:

: 
: I'm probably apporaching this all wrong to start but....
: 
: Suppose I have a monthly time series and I want to compute the mean of
: months 6,7, and 8. I want to plot the original time series and the
: seasonal time series, one above the other. When I do that as below the
: time series don't line up for reasons that are obvious. How can I
: change the base of the seasonal time series so I can make my plots
: line up? That is, I want the points for the seasonal plot to line up
: with cycle 6 of the first plot.
: 
: Thanks. 
: 
: dat <- ts(rnorm(12*20), start = c(1980,1), frequency = 12)
: plot(dat)
: dat.sub <- dat
: dat.sub[cycle(dat.sub) < 6] <- NA
: dat.sub[cycle(dat.sub) > 8] <- NA
: dat.sub <- aggregate(dat.sub, nfrequency = 1, FUN = mean, na.rm = T)
: tsp(dat)
: tsp(dat.sub)
: par(mfrow = c(2, 1))
: plot(dat)
: plot(dat.sub, type = "b")

Replace the four lines that begin with dat.sub with this single line:

dat.sub <- ts(filter(dat, c(1,1,1)/3)[cycle(dat)==7], start = 1980.5)



From ligges at statistik.uni-dortmund.de  Mon Jan 31 18:58:46 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 31 Jan 2005 18:58:46 +0100
Subject: [R] changing the time base in a ts
In-Reply-To: <e89bb7ac05013109415dfaf737@mail.gmail.com>
References: <e89bb7ac0501310904786aebc5@mail.gmail.com>	
	<41FE6C3F.5050709@statistik.uni-dortmund.de>
	<e89bb7ac05013109415dfaf737@mail.gmail.com>
Message-ID: <41FE71D6.1090103@statistik.uni-dortmund.de>

Dr Carbon wrote:

> Sorry, what I meant was that I want dat.sub[1] to line up with dat[7],
> dat.sub[2] to line up with dat[19], dat.sub[3] with dat[12+12+7],
> etc...


Sorry, I haven't read your mail carefully enough:

   plot(dat)
   plot(x = time(dat.sub, offset = 0.5), y = dat.sub,
     type = "b", xy.labels = FALSE, xlim = c(1980, 2000))

Uwe Ligges


> 
> On Mon, 31 Jan 2005 18:34:55 +0100, Uwe Ligges
> <ligges at statistik.uni-dortmund.de> wrote:
> 
>>Dr Carbon wrote:
>>
>>>I'm probably apporaching this all wrong to start but....
>>>
>>>Suppose I have a monthly time series and I want to compute the mean of
>>>months 6,7, and 8. I want to plot the original time series and the
>>>seasonal time series, one above the other. When I do that as below the
>>>time series don't line up for reasons that are obvious. How can I
>>>change the base of the seasonal time series so I can make my plots
>>>line up? That is, I want the points for the seasonal plot to line up
>>>with cycle 6 of the first plot.
>>>
>>>Thanks.
>>>
>>>
>>>dat <- ts(rnorm(12*20), start = c(1980,1), frequency = 12)
>>>plot(dat)
>>>dat.sub <- dat
>>>dat.sub[cycle(dat.sub) < 6] <- NA
>>>dat.sub[cycle(dat.sub) > 8] <- NA
>>>dat.sub <- aggregate(dat.sub, nfrequency = 1, FUN = mean, na.rm = T)
>>>tsp(dat)
>>>tsp(dat.sub)
>>>par(mfrow = c(2, 1))
>>>plot(dat)
>>>plot(dat.sub, type = "b")
>>
>>  plot(dat)
>>  plot(dat.sub, type = "b", xlim=c(1980, 2000))
>>
>>Uwe Ligges
>>
>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>



From silvia.simoni at ing.unitn.it  Mon Jan 31 19:52:26 2005
From: silvia.simoni at ing.unitn.it (silvia simoni)
Date: Mon, 31 Jan 2005 19:52:26 +0100
Subject: [R] installing R on Mac OS X
Message-ID: <b932ca9de8e5c5e328f62ae218df9854@ing.unitn.it>

I have a problem in installing R-2.0.1, downloaded from the R web site, 
on mac o sx version 10.3.7.
when i launch the command ./configure i get the following error message:

  checking for C compiler default output file name... configure: error: 
C compiler cannot create executables

i've aleady insalled Xcode. the version of the gcc in 3.3.
what can i do?
thanks

silvia



From sdavis2 at mail.nih.gov  Mon Jan 31 20:11:27 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Mon, 31 Jan 2005 14:11:27 -0500
Subject: [R] installing R on Mac OS X
In-Reply-To: <b932ca9de8e5c5e328f62ae218df9854@ing.unitn.it>
References: <b932ca9de8e5c5e328f62ae218df9854@ing.unitn.it>
Message-ID: <E7DB1579-73BB-11D9-9CDE-000D933565E8@mail.nih.gov>

Silvia

You may want to use the disk image instead.  It is here (for italy....):

http://microarrays.unife.it/CRAN/bin/macosx/R-2.0.1.dmg

Sean

On Jan 31, 2005, at 1:52 PM, silvia simoni wrote:

> I have a problem in installing R-2.0.1, downloaded from the R web 
> site, on mac o sx version 10.3.7.
> when i launch the command ./configure i get the following error 
> message:
>
>  checking for C compiler default output file name... configure: error: 
> C compiler cannot create executables
>
> i've aleady insalled Xcode. the version of the gcc in 3.3.
> what can i do?
> thanks
>
> silvia
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From nicolas.ferrari at insee.fr  Mon Jan 31 20:25:37 2005
From: nicolas.ferrari at insee.fr (Ferrari Nicolas)
Date: Mon, 31 Jan 2005 20:25:37 +0100
Subject: [R] Help about time series
Message-ID: <4BFA0FD18E9ED311AC040000F6AF048905EC6B58@S54X01>

Hello,

When I create a ts object, I would like to get a particular value of this
time serie according to the date and not the rank. 
However, it seems necessary to use the rank as if it were a simple vector.
I would be very grateful if you could help out of this.
I thank you in advance,

Nicolas Ferrari



From abunn at whrc.org  Mon Jan 31 20:36:45 2005
From: abunn at whrc.org (abunn)
Date: Mon, 31 Jan 2005 14:36:45 -0500
Subject: [R] Help about time series
In-Reply-To: <4BFA0FD18E9ED311AC040000F6AF048905EC6B58@S54X01>
Message-ID: <NEBBIPHDAMMOKDKPOFFIIEBMDAAA.abunn@whrc.org>

?time

dat <- ts(rnorm(10), start = c(1900), frequency = 1)
dat[time(dat) == 1905]

HTH, Andy

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Ferrari Nicolas
> Sent: Monday, January 31, 2005 2:26 PM
> To: 'R-help at lists.R-project.org'
> Subject: [R] Help about time series
>
>
> Hello,
>
> When I create a ts object, I would like to get a particular value of this
> time serie according to the date and not the rank.
> However, it seems necessary to use the rank as if it were a simple vector.
> I would be very grateful if you could help out of this.
> I thank you in advance,
>
> Nicolas Ferrari
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From thchung at tgen.org  Mon Jan 31 20:42:37 2005
From: thchung at tgen.org (Tae-Hoon Chung)
Date: Mon, 31 Jan 2005 12:42:37 -0700
Subject: [R] Special paper for postscript
Message-ID: <BE23D83D.2B9C%thchung@tgen.org>

Hi, All;

When I generate a "special" paper postscript image larger than "a4" or
"letter" using R, I can only see one-page portion of all image, of course.
What will be the simple solution for this? Is there any way I can set the
bounding box information on the image? Or any other suggestions?

Thanks in advance;
Tae-Hoon Chung
--------------------------------------------------
Tae-Hoon Chung
Post-Doctoral Researcher
Translational Genomics Research Institute (TGen)
445 N. 5th Street (Suite 530)
Phoenix, AZ 85004
1-602-343-8724 (Direct)
1-480-323-9820 (Mobile)
1-602-343-8840 (Fax)



From Benjamin.Osborne at uvm.edu  Mon Jan 31 20:46:01 2005
From: Benjamin.Osborne at uvm.edu (Benjamin M. Osborne)
Date: Mon, 31 Jan 2005 14:46:01 -0500
Subject: [R] coercing a list to a data frame, lists in foreloops
Message-ID: <1107200761.41fe8af9d1859@webmail.uvm.edu>

I have a set of time-series climate data with missing entries.  I need to add
rows for these missing entries to this data set.  The only way I know to do
this is unsing a foreloop, but this won't work on a list.  I've tried to
convert the list to a data frame, but that won't happen, either.

I want to fill rows in this table:

> newtest[10:15,]
    yrmos yearmo snow.sum snow.mean snow.dep.mean prcp.sum prcp.mean tmin.min
10 195410     NA       NA        NA            NA       NA        NA       NA
11 195411     NA       NA        NA            NA       NA        NA       NA
12 195412     NA       NA        NA            NA       NA        NA       NA
13 195501     NA       NA        NA            NA       NA        NA       NA
14 195502     NA       NA        NA            NA       NA        NA       NA
15 195503     NA       NA        NA            NA       NA        NA       NA
   tmin.mean tmax.max tmax.mean tmean.mean
10        NA       NA        NA         NA
11        NA       NA        NA         NA
12        NA       NA        NA         NA
13        NA       NA        NA         NA
14        NA       NA        NA         NA
15        NA       NA        NA         NA
>

from this one:

> mansNew[10:15,]
   yearmo snow.sum snow.mean snow.dep.mean prcp.sum prcp.mean  tmin.min
10 195508    0.000 0.0000000       0.00000  29.5910 0.9545484        NA
11 195509    0.000 0.0000000       0.00000   9.1948 0.3064933        NA
12 195510   20.320 0.6554839            NA  13.8684 0.4473677        NA
13 195511       NA        NA            NA       NA        NA -18.88889
14 195512   52.324 1.6878710      53.01226   6.4770 0.2089355        NA
15 195601   46.736 1.5076129            NA   8.0264 0.2589161        NA
   tmin.mean   tmax.max  tmax.mean tmean.mean
10        NA         NA         NA         NA
11        NA         NA         NA         NA
12        NA         NA         NA         NA
13  -8.62963 12.2222222 -0.6481481  -4.638889
14        NA -0.5555556 -9.3906810         NA
15        NA         NA         NA         NA
>
This may be a problem:
> newtest<-as.data.frame(newtest)
> mode(newtest)  ## returns "list"
[1] "list"
>
> mansNew<-as.data.frame(mansNew)
> mode(mansNew)  ## returns "list"
[1] "list"
>
I've checked to make sure each column is a vector, but the coercion still is not
allowed.

This is the code with which I'm attempting to perform this manipulation, as well
as the result:

> for (i in 1:100){
+ newtest[i,2:12]<-ifelse(is.element(newtest$yrmos[i],mansNew$yearmo),
subset(mansNew, yearmo == newtest$yrmos[i])[,1:11], c(rep(NA,11)))
+ }
> newtest[10:15,]
    yrmos yearmo snow.sum snow.mean snow.dep.mean prcp.sum prcp.mean tmin.min
10 195410     NA       NA        NA            NA       NA        NA       NA
11 195411 195411   195411    195411        195411   195411    195411   195411
12 195412 195412   195412    195412        195412   195412    195412   195412
13 195501 195501   195501    195501        195501   195501    195501   195501
14 195502 195502   195502    195502        195502   195502    195502   195502
15 195503 195503   195503    195503        195503   195503    195503   195503
   tmin.mean tmax.max tmax.mean tmean.mean
10        NA       NA        NA         NA
11    195411   195411    195411     195411
12    195412   195412    195412     195412
13    195501   195501    195501     195501
14    195502   195502    195502     195502
15    195503   195503    195503     195503
>

subset...  should return only one row.  This may be a simple comma problem, but
I think it has something to do with the lists.  Also, if there is a way to do
this without the foreloop, I'd be happy to hear about it.
Any suggestions will be appreciated.

Thanks,
Ben Osborne

-- 
Botany Department
University of Vermont
109 Carrigan Drive
Burlington, VT 05405

benjamin.osborne at uvm.edu
phone: 802-656-0297
fax: 802-656-0440



From michael.wolosin at duke.edu  Mon Jan 31 20:58:25 2005
From: michael.wolosin at duke.edu (Michael Wolosin)
Date: Mon, 31 Jan 2005 14:58:25 -0500
Subject: [R] naming list elements
Message-ID: <5.2.1.1.1.20050131143125.0220e2d0@mail-ms.acpub.duke.edu>

All -

Each time through a loop I create a new dataset, which I would like to 
append to a list object.  Each item of the list should be the data matrix 
created in that step of the loop; I would like the NAME (or tag) of that 
list item to be assigned the value of a character string:

I've tried something like this:

running.list <- numeric(0)
for(i in 1:num.files){
.....
running.list <- append(running.list, list(this.item.name=newdatamatrix))
}

but the name of the item is always "this.item.name" instead of the CONTENTS 
of the character string called "this.item.name".

Obviously, this is correct from R's standpoint, but it's not what I want.

I can force it to do what I want by doing this:

running.list <- numeric(0)
for(i in 1:num.files){
.....
junk <- list(newdatamatrix)
names(junk) <- list(this.item.name)
running.list <- append(running.list, junk)
}

but this requires an extra assigment step, and so has two whole copies of 
the data matrix laying around.  (Some might point out that I'm being 
ridiculous worrying about memory if I create a list in this way, but that's 
another question...)

In some macro languages I've used, there are ways to force the evaluation 
of a section of code before it is passed on to the function call (in this 
case, I'd like to evaluate this.item.name and then effectively call the 
string "list(blah=newdatamatrix)")
I've tried doing exactly that, i.e.,
eval(paste("list(",this.item.name,"=newdatamatrix)"),sep="")

but that doesn't seem to work either.  It seems like I should be able to do 
this with eval, call, or something...

Thanks in advance for any help the list has to offer,

Mike



From peterwyang at gmail.com  Mon Jan 31 20:55:42 2005
From: peterwyang at gmail.com (Peter Yang)
Date: Mon, 31 Jan 2005 14:55:42 -0500
Subject: [R] How to simulate quadratic phase couplilng
Message-ID: <a922041d0501311155216cfbde@mail.gmail.com>

Hi, I need to simulate the following two time series,

1. X=cos(omega1*t+phi1)+cos(omega2*t+phi2)+cos(omega3+phi3)
2. Y=cos(omega1*t+phi1)+cos(omega2*t+phi2)+cos(omega3+phi1+phi2)

where omega3=omega1+omega2 and phi1, phi2, phi3 are independent
uniform random variables from 0 to 2pi.

Question 1: when I simulate the time series, how should I make sure
that phi1, phi2, phi3 are independent?
Question 2: Since both sereis has non-zere values on FFT(omega1),
FFT(omega2), FFT(omega3), according to the estimation of 3rd-order  
periodgram=FFT(omega1)*FFT(omega2)*Conj(FFT(omega3)), then why the
third-order periodgram for X is 0 while not zero for Y.

Thanks a lot for your reply.



From carsten.steinhoff at stud.uni-goettingen.de  Mon Jan 31 21:02:33 2005
From: carsten.steinhoff at stud.uni-goettingen.de (Carsten Steinhoff)
Date: Mon, 31 Jan 2005 21:02:33 +0100
Subject: [R] ML-Fit for truncated distributions
Message-ID: <001f01c507cf$cd6218b0$8cac4c86@okuell1>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050131/a5642296/attachment.pl

From pkleiber at honlab.nmfs.hawaii.edu  Mon Jan 31 21:20:46 2005
From: pkleiber at honlab.nmfs.hawaii.edu (Pierre Kleiber)
Date: Mon, 31 Jan 2005 10:20:46 -1000
Subject: [R] naming list elements
In-Reply-To: <5.2.1.1.1.20050131143125.0220e2d0@mail-ms.acpub.duke.edu>
References: <5.2.1.1.1.20050131143125.0220e2d0@mail-ms.acpub.duke.edu>
Message-ID: <41FE931E.3040800@honlab.nmfs.hawaii.edu>

Will the following work for you?
    Cheers, Pierre

 > junk <- list()
 > for(i in 1:5) junk[[paste("Item",i,sep="")]] <- runif(4)
 > str(junk)
List of 5
  $ Item1: num [1:4] 0.687 0.508 0.734 0.704
  $ Item2: num [1:4] 0.5497 0.1777 0.4081 0.0129
  $ Item3: num [1:4] 0.768 0.808 0.592 0.697
  $ Item4: num [1:4] 0.650 0.966 0.265 0.461
  $ Item5: num [1:4] 0.9067 0.3396 0.0412 0.3956


Michael Wolosin wrote:
> All -
> 
> Each time through a loop I create a new dataset, which I would like to 
> append to a list object.  Each item of the list should be the data 
> matrix created in that step of the loop; I would like the NAME (or tag) 
> of that list item to be assigned the value of a character string:
> 
> I've tried something like this:
> 
> running.list <- numeric(0)
> for(i in 1:num.files){
> .....
> running.list <- append(running.list, list(this.item.name=newdatamatrix))
> }
> 
> but the name of the item is always "this.item.name" instead of the 
> CONTENTS of the character string called "this.item.name".
> 
> Obviously, this is correct from R's standpoint, but it's not what I want.
> 
> I can force it to do what I want by doing this:
> 
> running.list <- numeric(0)
> for(i in 1:num.files){
> .....
> junk <- list(newdatamatrix)
> names(junk) <- list(this.item.name)
> running.list <- append(running.list, junk)
> }
> 
> but this requires an extra assigment step, and so has two whole copies 
> of the data matrix laying around.  (Some might point out that I'm being 
> ridiculous worrying about memory if I create a list in this way, but 
> that's another question...)
> 
> In some macro languages I've used, there are ways to force the 
> evaluation of a section of code before it is passed on to the function 
> call (in this case, I'd like to evaluate this.item.name and then 
> effectively call the string "list(blah=newdatamatrix)")
> I've tried doing exactly that, i.e.,
> eval(paste("list(",this.item.name,"=newdatamatrix)"),sep="")
> 
> but that doesn't seem to work either.  It seems like I should be able to 
> do this with eval, call, or something...
> 
> Thanks in advance for any help the list has to offer,
> 
> Mike
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
-----------------------------------------------------------------
Pierre Kleiber, Ph.D       Email: pkleiber at honlab.nmfs.hawaii.edu
Fishery Biologist           Tel: 808 983-5399 / 808 737-7544 (hm)
NOAA FISHERIES - Honolulu Laboratory         Fax: 808 983-2902
2570 Dole St., Honolulu, HI 96822-2396
-----------------------------------------------------------------
  "God could have told Moses about galaxies and mitochondria and
   all.  But behold... It was good enough for government work."



From zvalim at gmail.com  Mon Jan 31 22:06:42 2005
From: zvalim at gmail.com (Uri)
Date: Mon, 31 Jan 2005 16:06:42 -0500
Subject: [R] Automatically Extracting F- and P- vals from ANOVA
Message-ID: <656a77fc0501311306735a87dd@mail.gmail.com>

Dear R community,

I'm currently using R to analyze functional Magnetic Resonance Imaging
data.  Each analysis involves running ~120,000 repeated-measures
ANOVAs.

I would like to know if there is any automatic way to access the F-
and P-value data that are associated with each of these 120,000
ANOVAs.

For example, if the summary output (for the 1st ANOVA of 120,000)
shows the following value for factor "C", then I would like to
automatically extract the F value (4.1) and P value (0.13) from the
summary() output:

> summary(anova.1)

Error: S
          Df Sum Sq Mean Sq F value Pr(>F)
Residuals  1   12.5    12.5

Error: S:C
          Df  Sum Sq Mean Sq F value Pr(>F)
C          3 128.500  42.833  4.2131 0.1341
Residuals  3  30.500  10.167

Of course, I would be interested only in factor "C".

I would then push these values to a hash of hashes (i.e, multi-dim
array) that summarizes the results of all the ANOVAs.  {Anova1,
(F=4.21), (P=0.13)} etc'.

I "dumped" a sample aov object, and my impression was that the F and P
values for each effect are not hard coded, but constructed on the fly
by the summary() function.

Is there a script or function that already does what I am looking for?
 If not, my second option would be to sink the summary of each anova
to a text file, and then post-process it, but this latter option is
sub-optimal.

Best,
Uri Hasson



From Wittner.Ben at mgh.harvard.edu  Mon Jan 31 22:32:46 2005
From: Wittner.Ben at mgh.harvard.edu (Wittner, Ben)
Date: Mon, 31 Jan 2005 16:32:46 -0500
Subject: [R] how to move x-axis labels down
Message-ID: <C6B4E960236FD311B4E00008C7F406E31404B781@phsexch11.mgh.harvard.edu>

Hi,

In the code below, the labels I put on the x-axis are too high (they cross the
axis). Can anyone tell me how to move them down? I've tried adj=, padj=, mar=,
and various other things, but cannot move them down.

Thanks.

-Ben

labs <- paste('sample', 1:10)
plot(1:10, xaxt='n', xlab='')
axis(1, at=1:10, labels=labs, padj=1, las=2)    # las is a par() parameter



From andy_liaw at merck.com  Mon Jan 31 22:43:39 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 31 Jan 2005 16:43:39 -0500
Subject: [R] how to move x-axis labels down
Message-ID: <3A822319EB35174CA3714066D590DCD50994E605@usrymx25.merck.com>

I guess setting par(mgp) would help.

Andy

> From: Wittner, Ben
> 
> Hi,
> 
> In the code below, the labels I put on the x-axis are too 
> high (they cross the
> axis). Can anyone tell me how to move them down? I've tried 
> adj=, padj=, mar=,
> and various other things, but cannot move them down.
> 
> Thanks.
> 
> -Ben
> 
> labs <- paste('sample', 1:10)
> plot(1:10, xaxt='n', xlab='')
> axis(1, at=1:10, labels=labs, padj=1, las=2)    # las is a 
> par() parameter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Robert.McGehee at geodecapital.com  Mon Jan 31 22:53:40 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Mon, 31 Jan 2005 16:53:40 -0500
Subject: [R] Evaluating code in a sandbox
Message-ID: <67DCA285A2D7754280D3B8E88EB5480206741E48@MSGBOSCLB2WIN.DMN1.FMR.COM>

Why not just save your current global environment and reload it after
running the offending code? See ?save.image to save, then rm(list =
ls()) to clear the global environment.

If a program tries to get or assign something to a global environment,
then either you rewrite the program to do otherwise, or you rewrite the
assignment functions, as one cannot silo the global environment like
such without writing it to disk (or another environment).

Robert

-----Original Message-----
From: lederer at trium.de [mailto:lederer at trium.de] 
Sent: Sunday, January 30, 2005 8:10 PM
To: R-help at stat.math.ethz.ch
Subject: [R] Evaluating code in a sandbox



Dear R-Gurus,

is it possible to evaluate foreign R code in such a safe way, that it
has
no chance to confuse the global environment?

The follwing does not suffice, because the untrusted code might e.g.
contain a superassignment ("<<-"):

connection <- textConnection(some.untrusted.code)
try(eval(parse(connection), envir=new.env()))
close(connection)


Christian

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Jan 31 22:55:23 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 31 Jan 2005 21:55:23 +0000 (GMT)
Subject: [R] ML-Fit for truncated distributions
In-Reply-To: <001f01c507cf$cd6218b0$8cac4c86@okuell1>
References: <001f01c507cf$cd6218b0$8cac4c86@okuell1>
Message-ID: <Pine.LNX.4.61.0501312149540.17357@gannet.stats>

On Mon, 31 Jan 2005, Carsten Steinhoff wrote:

> maybe that my Question is a "beginner"-Question, but up to now, my research
> didn't bring any useful result.
>
> I'm trying to fit a distribution (e.g. lognormal) to a given set of data
> (ML-Estimation). I KNOW about my data that there is a truncation for all
> data below a well known threshold. Is there an R-solution for an
> ML-estimation for this kind of data-problem? As far as I've seen the
> "fitdistr" in package "MASS" doesn't solve this problem.

I can intrepret that two ways.  If you have a truncated lognormal 
distribution on say (-a, Inf), fitdistr will do this: you just need to 
give it the density of the truncated distribution.

If the observations are rather censored (they are smaller than something, 
but the value is not otherwise known), for a lognormal you can use survreg 
with reversed time.

Function mle() in package stats4 is useful for general MLE problems.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Wittner.Ben at mgh.harvard.edu  Mon Jan 31 23:00:57 2005
From: Wittner.Ben at mgh.harvard.edu (Wittner, Ben)
Date: Mon, 31 Jan 2005 17:00:57 -0500
Subject: [R] how to move x-axis labels down
Message-ID: <C6B4E960236FD311B4E00008C7F406E31404B783@phsexch11.mgh.harvard.edu>

Bert,

Your question below prompted me to run the same code to the postscript device
and sure enough, the file created displays perfectly. So I guess the problem is
specific to my system and I should not try to fix it by changing my code.

My system is as follows:

platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    0.1
year     2004
month    11
day      15
language R

Thanks.

-Ben

> -----Original Message-----
> From: Berton Gunter [mailto:gunter.berton at gene.com]
> Sent: Monday, January 31, 2005 4:49 PM
> To: Wittner, Ben
> Subject: RE: [R] how to move x-axis labels down
> 
> I have no such problem on Windows, R2.01. What system and version are you
> on?
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
> 
> "The business of the statistician is to catalyze the scientific learning
> process."  - George E. P. Box
> 
> 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Wittner, Ben
> > Sent: Monday, January 31, 2005 1:33 PM
> > To: 'r-help at lists.R-project.org'
> > Subject: [R] how to move x-axis labels down
> >
> > Hi,
> >
> > In the code below, the labels I put on the x-axis are too
> > high (they cross the
> > axis). Can anyone tell me how to move them down? I've tried
> > adj=, padj=, mar=,
> > and various other things, but cannot move them down.
> >
> > Thanks.
> >
> > -Ben
> >
> > labs <- paste('sample', 1:10)
> > plot(1:10, xaxt='n', xlab='')
> > axis(1, at=1:10, labels=labs, padj=1, las=2)    # las is a
> > par() parameter
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >



From jfox at mcmaster.ca  Mon Jan 31 23:26:52 2005
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 31 Jan 2005 17:26:52 -0500
Subject: [R] Automatically Extracting F- and P- vals from ANOVA
In-Reply-To: <656a77fc0501311306735a87dd@mail.gmail.com>
Message-ID: <20050131222648.MCBP2026.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Uri,

You can save the object produced by summary() (i.e., assign it to a
variable, not print it and dump it to a file) and extract what you want from
that. You can use str() to see the structure of the object.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uri
> Sent: Monday, January 31, 2005 4:07 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Automatically Extracting F- and P- vals from ANOVA
> 
> Dear R community,
> 
> I'm currently using R to analyze functional Magnetic 
> Resonance Imaging data.  Each analysis involves running 
> ~120,000 repeated-measures ANOVAs.
> 
> I would like to know if there is any automatic way to access 
> the F- and P-value data that are associated with each of 
> these 120,000 ANOVAs.
> 
> For example, if the summary output (for the 1st ANOVA of 
> 120,000) shows the following value for factor "C", then I 
> would like to automatically extract the F value (4.1) and P 
> value (0.13) from the
> summary() output:
> 
> > summary(anova.1)
> 
> Error: S
>           Df Sum Sq Mean Sq F value Pr(>F)
> Residuals  1   12.5    12.5
> 
> Error: S:C
>           Df  Sum Sq Mean Sq F value Pr(>F)
> C          3 128.500  42.833  4.2131 0.1341
> Residuals  3  30.500  10.167
> 
> Of course, I would be interested only in factor "C".
> 
> I would then push these values to a hash of hashes (i.e, multi-dim
> array) that summarizes the results of all the ANOVAs.  
> {Anova1, (F=4.21), (P=0.13)} etc'.
> 
> I "dumped" a sample aov object, and my impression was that 
> the F and P values for each effect are not hard coded, but 
> constructed on the fly by the summary() function.
> 
> Is there a script or function that already does what I am looking for?
>  If not, my second option would be to sink the summary of 
> each anova to a text file, and then post-process it, but this 
> latter option is sub-optimal.
> 
> Best,
> Uri Hasson
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From MSchwartz at MedAnalytics.com  Mon Jan 31 23:32:29 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 31 Jan 2005 16:32:29 -0600
Subject: [R] Special paper for postscript
In-Reply-To: <BE23D83D.2B9C%thchung@tgen.org>
References: <BE23D83D.2B9C%thchung@tgen.org>
Message-ID: <1107210749.16083.122.camel@horizons.localdomain>

On Mon, 2005-01-31 at 12:42 -0700, Tae-Hoon Chung wrote:
> Hi, All;
> 
> When I generate a "special" paper postscript image larger than "a4" or
> "letter" using R, I can only see one-page portion of all image, of course.
> What will be the simple solution for this? Is there any way I can set the
> bounding box information on the image? Or any other suggestions?

Are you trying to generate an EPS file or just purely PS?

If the former, you need to specify:

 horizontal = FALSE, onefile = FALSE, paper = "special"

as it indicates in the "Details" section of ?postscript

In both cases, you also need to specify the 'height' and 'width'
arguments, when using the "special" paper type.

For example, this works for me:

postscript(file = "MyFile.ps", height = 10, width = 15, 
           paper = "special", onefile = FALSE, horizontal = FALSE)

barplot(1:10)

dev.off()

I may be missing what you are trying to do, so if this is not helpful,
please post back a reproducible example of what you are trying to do.

HTH,

Marc Schwartz



