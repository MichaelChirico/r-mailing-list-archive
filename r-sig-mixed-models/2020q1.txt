From j||bo97 @end|ng |rom gm@||@com  Thu Jan  2 06:46:33 2020
From: j||bo97 @end|ng |rom gm@||@com (Jill Brouwer)
Date: Thu, 2 Jan 2020 13:46:33 +0800
Subject: [R-sig-ME] singular fit
Message-ID: <CAB9nLEWTcmWFpV-BiFq-gGBcmvrZZESHo4weV1J9Q9UtByZyOw@mail.gmail.com>

Hi all,

I have fitted a GLMM using glmer in lme4, and when I run the model it comes
out with a singular fit warning.

However when I ran the isSingular command on it and changed the tolerance
to 1e-05 instead of the default 1e-04 that caused the original warning, it
comes out as false - no singular fit warning!

Does this mean that the first warning is a false positive?
I can't find anything that suggests what the tolerance ratio should be but
in the GLMM FAQ on github, the troubleshooting example uses 1e-05.

Is it fine to stay with this model - I would prefer it to include all the
random effects as they are all of interest to me, and the model itself is
structured based on how I ran my experiment.

Sorry if this is a basic question, I am still learning!

Kind regards,
Jill

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Thu Jan  2 09:46:50 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Thu, 2 Jan 2020 09:46:50 +0100
Subject: [R-sig-ME] singular fit
In-Reply-To: <CAB9nLEWTcmWFpV-BiFq-gGBcmvrZZESHo4weV1J9Q9UtByZyOw@mail.gmail.com>
References: <CAB9nLEWTcmWFpV-BiFq-gGBcmvrZZESHo4weV1J9Q9UtByZyOw@mail.gmail.com>
Message-ID: <CAJuCY5xO7NhK46tRCKu7UeQU+mFv3PkDUy_Mtdu7yfsfLct0mg@mail.gmail.com>

Dear Jill,

Can you share the model formula and the design of your experiment? It's
hard to answer your question without such basic information.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op do 2 jan. 2020 om 06:47 schreef Jill Brouwer <jilbo97 at gmail.com>:

> Hi all,
>
> I have fitted a GLMM using glmer in lme4, and when I run the model it comes
> out with a singular fit warning.
>
> However when I ran the isSingular command on it and changed the tolerance
> to 1e-05 instead of the default 1e-04 that caused the original warning, it
> comes out as false - no singular fit warning!
>
> Does this mean that the first warning is a false positive?
> I can't find anything that suggests what the tolerance ratio should be but
> in the GLMM FAQ on github, the troubleshooting example uses 1e-05.
>
> Is it fine to stay with this model - I would prefer it to include all the
> random effects as they are all of interest to me, and the model itself is
> structured based on how I ran my experiment.
>
> Sorry if this is a basic question, I am still learning!
>
> Kind regards,
> Jill
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From j||bo97 @end|ng |rom gm@||@com  Thu Jan  2 10:09:10 2020
From: j||bo97 @end|ng |rom gm@||@com (Jill Brouwer)
Date: Thu, 2 Jan 2020 17:09:10 +0800
Subject: [R-sig-ME] singular fit
In-Reply-To: <CAJuCY5xO7NhK46tRCKu7UeQU+mFv3PkDUy_Mtdu7yfsfLct0mg@mail.gmail.com>
References: <CAB9nLEWTcmWFpV-BiFq-gGBcmvrZZESHo4weV1J9Q9UtByZyOw@mail.gmail.com>
 <CAJuCY5xO7NhK46tRCKu7UeQU+mFv3PkDUy_Mtdu7yfsfLct0mg@mail.gmail.com>
Message-ID: <CAB9nLEXz91EfLyrZvy3HvX66nbAsGOwetcy7cCxWTD7z4ZaY5w@mail.gmail.com>

Sorry here is some more information:

My research is looking at whether ocean acidification affects patterns of
gamete compatibility between individual male/female mussels.
Here I am looking at whether the ph of the fertilisation assays also
influences male by female interactions.

The design consists of two males and two females, crossed in every
combination (so a total of four combinations) per block, with two
replicates in each.
There is a fixed effect of Fertilisation pH (just called Fertilisation
below)
Random effects are individual males and females (each assigned a unique
number, but specified as a factor for the model), and block.

the full model formula is this (which doesn't give the singular fit error):
fertphmodel <- glmer(cbind(Success,Failure) ~ Fertilisation + (1|Block) +
(1|Male) + (1|Female) + (1|Male:Female) +
                      (1|Male:Fertilisation) + (1|Female:Fertilisation) +
(1|Male:Female:Fertilisation),
                    family = "binomial", data = fertph)

I am using likelihood ratio testing to determine significance of the random
effects, however when I create the reduced model with (1|Male) removed, and
also the one for (1|Male:Female) removed, it spits out the singular fit
error. (Formulas below). I was also reading about boundary effect problems
with likelihood ratio testing, and am unsure how to account for this?

fertphmodel1 <- glmer(cbind(Success,Failure) ~ Fertilisation + (1|Block) +
(1|Female) + (1|Male:Female) +
                        (1|Male:Fertilisation) + (1|Female:Fertilisation) +
(1|Male:Female:Fertilisation),
                      family = "binomial", data = fertph)

fertphmodel3 <- glmer(cbind(Success,Failure) ~ Fertilisation + (1|Block) +
(1|Male) + (1|Female) +
                       (1|Male:Fertilisation) + (1|Female:Fertilisation) +
(1|Male:Female:Fertilisation),
                     family = "binomial", data = fertph)

For fertphmodel1, the summary output says that the female random effect has
an extremely low variance  (possibly a reason for singular fit?)
var: 7.070e-10 sd: 2.659e-05

And for fertphmodel3, the summary output says the Female:Fertilisation has
a very low variance
var 3.325e-10 sd 1.823e-05

However, in the full model the all of the variances of the random effects
are between 0.03 and 0.6.

Hopefully this helps a bit !

Thankyou,
Jill

On Thu, Jan 2, 2020 at 4:47 PM Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> Dear Jill,
>
> Can you share the model formula and the design of your experiment? It's
> hard to answer your question without such basic information.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op do 2 jan. 2020 om 06:47 schreef Jill Brouwer <jilbo97 at gmail.com>:
>
>> Hi all,
>>
>> I have fitted a GLMM using glmer in lme4, and when I run the model it
>> comes
>> out with a singular fit warning.
>>
>> However when I ran the isSingular command on it and changed the tolerance
>> to 1e-05 instead of the default 1e-04 that caused the original warning, it
>> comes out as false - no singular fit warning!
>>
>> Does this mean that the first warning is a false positive?
>> I can't find anything that suggests what the tolerance ratio should be but
>> in the GLMM FAQ on github, the troubleshooting example uses 1e-05.
>>
>> Is it fine to stay with this model - I would prefer it to include all the
>> random effects as they are all of interest to me, and the model itself is
>> structured based on how I ran my experiment.
>>
>> Sorry if this is a basic question, I am still learning!
>>
>> Kind regards,
>> Jill
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Thu Jan  2 11:12:53 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Thu, 2 Jan 2020 11:12:53 +0100
Subject: [R-sig-ME] singular fit
In-Reply-To: <CAB9nLEXz91EfLyrZvy3HvX66nbAsGOwetcy7cCxWTD7z4ZaY5w@mail.gmail.com>
References: <CAB9nLEWTcmWFpV-BiFq-gGBcmvrZZESHo4weV1J9Q9UtByZyOw@mail.gmail.com>
 <CAJuCY5xO7NhK46tRCKu7UeQU+mFv3PkDUy_Mtdu7yfsfLct0mg@mail.gmail.com>
 <CAB9nLEXz91EfLyrZvy3HvX66nbAsGOwetcy7cCxWTD7z4ZaY5w@mail.gmail.com>
Message-ID: <CAJuCY5zWg7dpJtstZqdfz_LdU7YsXWx2rb25AXuPBu+=3OC-kA@mail.gmail.com>

Dear Jill,

I presume you use different males and females for each block? How many
blocks? How many trials per block (success + failure)? Is fertilisation a
discrete variable?

Removing a main (random) effect like (1|Male) while keeping interactions
(1|Male:Female) doesn't make sense. You'll get exactly the same model fit
with a different parametrisation as the interaction will model the main
effect.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op do 2 jan. 2020 om 10:09 schreef Jill Brouwer <jilbo97 at gmail.com>:

> Sorry here is some more information:
>
> My research is looking at whether ocean acidification affects patterns of
> gamete compatibility between individual male/female mussels.
> Here I am looking at whether the ph of the fertilisation assays also
> influences male by female interactions.
>
> The design consists of two males and two females, crossed in every
> combination (so a total of four combinations) per block, with two
> replicates in each.
> There is a fixed effect of Fertilisation pH (just called Fertilisation
> below)
> Random effects are individual males and females (each assigned a unique
> number, but specified as a factor for the model), and block.
>
> the full model formula is this (which doesn't give the singular fit error):
> fertphmodel <- glmer(cbind(Success,Failure) ~ Fertilisation + (1|Block) +
> (1|Male) + (1|Female) + (1|Male:Female) +
>                       (1|Male:Fertilisation) + (1|Female:Fertilisation) +
> (1|Male:Female:Fertilisation),
>                     family = "binomial", data = fertph)
>
> I am using likelihood ratio testing to determine significance of the
> random effects, however when I create the reduced model with (1|Male)
> removed, and also the one for (1|Male:Female) removed, it spits out the
> singular fit error. (Formulas below). I was also reading about boundary
> effect problems with likelihood ratio testing, and am unsure how to account
> for this?
>
> fertphmodel1 <- glmer(cbind(Success,Failure) ~ Fertilisation + (1|Block) +
> (1|Female) + (1|Male:Female) +
>                         (1|Male:Fertilisation) + (1|Female:Fertilisation)
> + (1|Male:Female:Fertilisation),
>                       family = "binomial", data = fertph)
>
> fertphmodel3 <- glmer(cbind(Success,Failure) ~ Fertilisation + (1|Block) +
> (1|Male) + (1|Female) +
>                        (1|Male:Fertilisation) + (1|Female:Fertilisation) +
> (1|Male:Female:Fertilisation),
>                      family = "binomial", data = fertph)
>
> For fertphmodel1, the summary output says that the female random effect
> has an extremely low variance  (possibly a reason for singular fit?)
> var: 7.070e-10 sd: 2.659e-05
>
> And for fertphmodel3, the summary output says the Female:Fertilisation has
> a very low variance
> var 3.325e-10 sd 1.823e-05
>
> However, in the full model the all of the variances of the random effects
> are between 0.03 and 0.6.
>
> Hopefully this helps a bit !
>
> Thankyou,
> Jill
>
> On Thu, Jan 2, 2020 at 4:47 PM Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
>> Dear Jill,
>>
>> Can you share the model formula and the design of your experiment? It's
>> hard to answer your question without such basic information.
>>
>> Best regards,
>>
>> ir. Thierry Onkelinx
>> Statisticus / Statistician
>>
>> Vlaamse Overheid / Government of Flanders
>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>> AND FOREST
>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>> thierry.onkelinx at inbo.be
>> Havenlaan 88 bus 73, 1000 Brussel
>> www.inbo.be
>>
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>>
>> <https://www.inbo.be>
>>
>>
>> Op do 2 jan. 2020 om 06:47 schreef Jill Brouwer <jilbo97 at gmail.com>:
>>
>>> Hi all,
>>>
>>> I have fitted a GLMM using glmer in lme4, and when I run the model it
>>> comes
>>> out with a singular fit warning.
>>>
>>> However when I ran the isSingular command on it and changed the tolerance
>>> to 1e-05 instead of the default 1e-04 that caused the original warning,
>>> it
>>> comes out as false - no singular fit warning!
>>>
>>> Does this mean that the first warning is a false positive?
>>> I can't find anything that suggests what the tolerance ratio should be
>>> but
>>> in the GLMM FAQ on github, the troubleshooting example uses 1e-05.
>>>
>>> Is it fine to stay with this model - I would prefer it to include all the
>>> random effects as they are all of interest to me, and the model itself is
>>> structured based on how I ran my experiment.
>>>
>>> Sorry if this is a basic question, I am still learning!
>>>
>>> Kind regards,
>>> Jill
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>

	[[alternative HTML version deleted]]


From j||bo97 @end|ng |rom gm@||@com  Thu Jan  2 15:20:26 2020
From: j||bo97 @end|ng |rom gm@||@com (Jill Brouwer)
Date: Thu, 2 Jan 2020 22:20:26 +0800
Subject: [R-sig-ME] singular fit
In-Reply-To: <CAJuCY5zWg7dpJtstZqdfz_LdU7YsXWx2rb25AXuPBu+=3OC-kA@mail.gmail.com>
References: <CAB9nLEWTcmWFpV-BiFq-gGBcmvrZZESHo4weV1J9Q9UtByZyOw@mail.gmail.com>
 <CAJuCY5xO7NhK46tRCKu7UeQU+mFv3PkDUy_Mtdu7yfsfLct0mg@mail.gmail.com>
 <CAB9nLEXz91EfLyrZvy3HvX66nbAsGOwetcy7cCxWTD7z4ZaY5w@mail.gmail.com>
 <CAJuCY5zWg7dpJtstZqdfz_LdU7YsXWx2rb25AXuPBu+=3OC-kA@mail.gmail.com>
Message-ID: <CAB9nLEUCrxkUm4xYzdfLpcDDciS3ERh-mP0G9wUsjP2nsr4X5w@mail.gmail.com>

Dear Thierry,

There are 2 males and 2 females per block and 2 trials for each cross (so 8
trials in all) per treatment (pH). I have 16 blocks in total.

Fertilisation is binomial (success ie number of fertilised eggs out of 100)

I am trying to determine whether the random effects and their interactions
are significant in the model. I don?t understand what you mean by that last
point? Is there possibly a reference you could direct me to? Other
likelihood ratio tests I have done when removing male but keeping the
interaction have come up as significantly different to the full model ?
Sorry I am still quite new to all of this.

Kind regards,
Jill

On Thursday, January 2, 2020, Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> Dear Jill,
>
> I presume you use different males and females for each block? How many
> blocks? How many trials per block (success + failure)? Is fertilisation a
> discrete variable?
>
> Removing a main (random) effect like (1|Male) while keeping interactions
> (1|Male:Female) doesn't make sense. You'll get exactly the same model fit
> with a different parametrisation as the interaction will model the main
> effect.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88
> <https://www.google.com/maps/search/Havenlaan+88?entry=gmail&source=g>
> bus 73, 1000 Brussel
> www.inbo.be
>
> ////////////////////////////////////////////////////////////
> ///////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
> ////////////////////////////////////////////////////////////
> ///////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op do 2 jan. 2020 om 10:09 schreef Jill Brouwer <jilbo97 at gmail.com>:
>
>> Sorry here is some more information:
>>
>> My research is looking at whether ocean acidification affects patterns of
>> gamete compatibility between individual male/female mussels.
>> Here I am looking at whether the ph of the fertilisation assays also
>> influences male by female interactions.
>>
>> The design consists of two males and two females, crossed in every
>> combination (so a total of four combinations) per block, with two
>> replicates in each.
>> There is a fixed effect of Fertilisation pH (just called Fertilisation
>> below)
>> Random effects are individual males and females (each assigned a unique
>> number, but specified as a factor for the model), and block.
>>
>> the full model formula is this (which doesn't give the singular fit
>> error):
>> fertphmodel <- glmer(cbind(Success,Failure) ~ Fertilisation + (1|Block) +
>> (1|Male) + (1|Female) + (1|Male:Female) +
>>                       (1|Male:Fertilisation) + (1|Female:Fertilisation) +
>> (1|Male:Female:Fertilisation),
>>                     family = "binomial", data = fertph)
>>
>> I am using likelihood ratio testing to determine significance of the
>> random effects, however when I create the reduced model with (1|Male)
>> removed, and also the one for (1|Male:Female) removed, it spits out the
>> singular fit error. (Formulas below). I was also reading about boundary
>> effect problems with likelihood ratio testing, and am unsure how to account
>> for this?
>>
>> fertphmodel1 <- glmer(cbind(Success,Failure) ~ Fertilisation + (1|Block)
>> + (1|Female) + (1|Male:Female) +
>>                         (1|Male:Fertilisation) + (1|Female:Fertilisation)
>> + (1|Male:Female:Fertilisation),
>>                       family = "binomial", data = fertph)
>>
>> fertphmodel3 <- glmer(cbind(Success,Failure) ~ Fertilisation + (1|Block)
>> + (1|Male) + (1|Female) +
>>                        (1|Male:Fertilisation) + (1|Female:Fertilisation)
>> + (1|Male:Female:Fertilisation),
>>                      family = "binomial", data = fertph)
>>
>> For fertphmodel1, the summary output says that the female random effect
>> has an extremely low variance  (possibly a reason for singular fit?)
>> var: 7.070e-10 sd: 2.659e-05
>>
>> And for fertphmodel3, the summary output says the Female:Fertilisation
>> has a very low variance
>> var 3.325e-10 sd 1.823e-05
>>
>> However, in the full model the all of the variances of the random effects
>> are between 0.03 and 0.6.
>>
>> Hopefully this helps a bit !
>>
>> Thankyou,
>> Jill
>>
>> On Thu, Jan 2, 2020 at 4:47 PM Thierry Onkelinx <thierry.onkelinx at inbo.be>
>> wrote:
>>
>>> Dear Jill,
>>>
>>> Can you share the model formula and the design of your experiment? It's
>>> hard to answer your question without such basic information.
>>>
>>> Best regards,
>>>
>>> ir. Thierry Onkelinx
>>> Statisticus / Statistician
>>>
>>> Vlaamse Overheid / Government of Flanders
>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>> AND FOREST
>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>> thierry.onkelinx at inbo.be
>>> Havenlaan 88
>>> <https://www.google.com/maps/search/Havenlaan+88?entry=gmail&source=g>
>>> bus 73, 1000 Brussel
>>> www.inbo.be
>>>
>>> ////////////////////////////////////////////////////////////
>>> ///////////////////////////////
>>> To call in the statistician after the experiment is done may be no more
>>> than asking him to perform a post-mortem examination: he may be able to say
>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> The plural of anecdote is not data. ~ Roger Brinner
>>> The combination of some data and an aching desire for an answer does not
>>> ensure that a reasonable answer can be extracted from a given body of data.
>>> ~ John Tukey
>>> ////////////////////////////////////////////////////////////
>>> ///////////////////////////////
>>>
>>> <https://www.inbo.be>
>>>
>>>
>>> Op do 2 jan. 2020 om 06:47 schreef Jill Brouwer <jilbo97 at gmail.com>:
>>>
>>>> Hi all,
>>>>
>>>> I have fitted a GLMM using glmer in lme4, and when I run the model it
>>>> comes
>>>> out with a singular fit warning.
>>>>
>>>> However when I ran the isSingular command on it and changed the
>>>> tolerance
>>>> to 1e-05 instead of the default 1e-04 that caused the original warning,
>>>> it
>>>> comes out as false - no singular fit warning!
>>>>
>>>> Does this mean that the first warning is a false positive?
>>>> I can't find anything that suggests what the tolerance ratio should be
>>>> but
>>>> in the GLMM FAQ on github, the troubleshooting example uses 1e-05.
>>>>
>>>> Is it fine to stay with this model - I would prefer it to include all
>>>> the
>>>> random effects as they are all of interest to me, and the model itself
>>>> is
>>>> structured based on how I ran my experiment.
>>>>
>>>> Sorry if this is a basic question, I am still learning!
>>>>
>>>> Kind regards,
>>>> Jill
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Thu Jan  2 17:02:16 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Thu, 2 Jan 2020 17:02:16 +0100
Subject: [R-sig-ME] singular fit
In-Reply-To: <CAB9nLEUCrxkUm4xYzdfLpcDDciS3ERh-mP0G9wUsjP2nsr4X5w@mail.gmail.com>
References: <CAB9nLEWTcmWFpV-BiFq-gGBcmvrZZESHo4weV1J9Q9UtByZyOw@mail.gmail.com>
 <CAJuCY5xO7NhK46tRCKu7UeQU+mFv3PkDUy_Mtdu7yfsfLct0mg@mail.gmail.com>
 <CAB9nLEXz91EfLyrZvy3HvX66nbAsGOwetcy7cCxWTD7z4ZaY5w@mail.gmail.com>
 <CAJuCY5zWg7dpJtstZqdfz_LdU7YsXWx2rb25AXuPBu+=3OC-kA@mail.gmail.com>
 <CAB9nLEUCrxkUm4xYzdfLpcDDciS3ERh-mP0G9wUsjP2nsr4X5w@mail.gmail.com>
Message-ID: <CAJuCY5z4haBT=h17YaKb2XgC9Ftu5M5oKzudUNdxbSOMCnaQjg@mail.gmail.com>

Dear Jill,

See e.g. the help file of drop1(). The dummy example below demonstrates
that drop1() only removes main effects that are not contained in an
interaction. See also Venables (200) Exegeses on Linear Models.

m0 <- lm(Fertility ~ Agriculture * Education + Examination, data = swiss)
drop1(m0)

The LRT you ran, just compares two numbers based on both models. It does
not check whether the comparison makes sense.

I would strongly recommend that you look for help with a local statistician.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op do 2 jan. 2020 om 15:20 schreef Jill Brouwer <jilbo97 at gmail.com>:

> Dear Thierry,
>
> There are 2 males and 2 females per block and 2 trials for each cross (so
> 8 trials in all) per treatment (pH). I have 16 blocks in total.
>
> Fertilisation is binomial (success ie number of fertilised eggs out of 100)
>
> I am trying to determine whether the random effects and their interactions
> are significant in the model. I don?t understand what you mean by that last
> point? Is there possibly a reference you could direct me to? Other
> likelihood ratio tests I have done when removing male but keeping the
> interaction have come up as significantly different to the full model ?
> Sorry I am still quite new to all of this.
>
> Kind regards,
> Jill
>
> On Thursday, January 2, 2020, Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
>> Dear Jill,
>>
>> I presume you use different males and females for each block? How many
>> blocks? How many trials per block (success + failure)? Is fertilisation a
>> discrete variable?
>>
>> Removing a main (random) effect like (1|Male) while keeping interactions
>> (1|Male:Female) doesn't make sense. You'll get exactly the same model fit
>> with a different parametrisation as the interaction will model the main
>> effect.
>>
>> Best regards,
>>
>> ir. Thierry Onkelinx
>> Statisticus / Statistician
>>
>> Vlaamse Overheid / Government of Flanders
>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>> AND FOREST
>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>> thierry.onkelinx at inbo.be
>> Havenlaan 88
>> <https://www.google.com/maps/search/Havenlaan+88?entry=gmail&source=g>
>> bus 73, 1000 Brussel
>> www.inbo.be
>>
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>>
>> <https://www.inbo.be>
>>
>>
>> Op do 2 jan. 2020 om 10:09 schreef Jill Brouwer <jilbo97 at gmail.com>:
>>
>>> Sorry here is some more information:
>>>
>>> My research is looking at whether ocean acidification affects patterns
>>> of gamete compatibility between individual male/female mussels.
>>> Here I am looking at whether the ph of the fertilisation assays also
>>> influences male by female interactions.
>>>
>>> The design consists of two males and two females, crossed in every
>>> combination (so a total of four combinations) per block, with two
>>> replicates in each.
>>> There is a fixed effect of Fertilisation pH (just called Fertilisation
>>> below)
>>> Random effects are individual males and females (each assigned a unique
>>> number, but specified as a factor for the model), and block.
>>>
>>> the full model formula is this (which doesn't give the singular fit
>>> error):
>>> fertphmodel <- glmer(cbind(Success,Failure) ~ Fertilisation + (1|Block)
>>> + (1|Male) + (1|Female) + (1|Male:Female) +
>>>                       (1|Male:Fertilisation) + (1|Female:Fertilisation)
>>> + (1|Male:Female:Fertilisation),
>>>                     family = "binomial", data = fertph)
>>>
>>> I am using likelihood ratio testing to determine significance of the
>>> random effects, however when I create the reduced model with (1|Male)
>>> removed, and also the one for (1|Male:Female) removed, it spits out the
>>> singular fit error. (Formulas below). I was also reading about boundary
>>> effect problems with likelihood ratio testing, and am unsure how to account
>>> for this?
>>>
>>> fertphmodel1 <- glmer(cbind(Success,Failure) ~ Fertilisation + (1|Block)
>>> + (1|Female) + (1|Male:Female) +
>>>                         (1|Male:Fertilisation) +
>>> (1|Female:Fertilisation) + (1|Male:Female:Fertilisation),
>>>                       family = "binomial", data = fertph)
>>>
>>> fertphmodel3 <- glmer(cbind(Success,Failure) ~ Fertilisation + (1|Block)
>>> + (1|Male) + (1|Female) +
>>>                        (1|Male:Fertilisation) + (1|Female:Fertilisation)
>>> + (1|Male:Female:Fertilisation),
>>>                      family = "binomial", data = fertph)
>>>
>>> For fertphmodel1, the summary output says that the female random effect
>>> has an extremely low variance  (possibly a reason for singular fit?)
>>> var: 7.070e-10 sd: 2.659e-05
>>>
>>> And for fertphmodel3, the summary output says the Female:Fertilisation
>>> has a very low variance
>>> var 3.325e-10 sd 1.823e-05
>>>
>>> However, in the full model the all of the variances of the random
>>> effects are between 0.03 and 0.6.
>>>
>>> Hopefully this helps a bit !
>>>
>>> Thankyou,
>>> Jill
>>>
>>> On Thu, Jan 2, 2020 at 4:47 PM Thierry Onkelinx <
>>> thierry.onkelinx at inbo.be> wrote:
>>>
>>>> Dear Jill,
>>>>
>>>> Can you share the model formula and the design of your experiment? It's
>>>> hard to answer your question without such basic information.
>>>>
>>>> Best regards,
>>>>
>>>> ir. Thierry Onkelinx
>>>> Statisticus / Statistician
>>>>
>>>> Vlaamse Overheid / Government of Flanders
>>>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>>>> AND FOREST
>>>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>>>> thierry.onkelinx at inbo.be
>>>> Havenlaan 88
>>>> <https://www.google.com/maps/search/Havenlaan+88?entry=gmail&source=g>
>>>> bus 73, 1000 Brussel
>>>> www.inbo.be
>>>>
>>>>
>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>> To call in the statistician after the experiment is done may be no more
>>>> than asking him to perform a post-mortem examination: he may be able to say
>>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>>> The plural of anecdote is not data. ~ Roger Brinner
>>>> The combination of some data and an aching desire for an answer does
>>>> not ensure that a reasonable answer can be extracted from a given body of
>>>> data. ~ John Tukey
>>>>
>>>> ///////////////////////////////////////////////////////////////////////////////////////////
>>>>
>>>> <https://www.inbo.be>
>>>>
>>>>
>>>> Op do 2 jan. 2020 om 06:47 schreef Jill Brouwer <jilbo97 at gmail.com>:
>>>>
>>>>> Hi all,
>>>>>
>>>>> I have fitted a GLMM using glmer in lme4, and when I run the model it
>>>>> comes
>>>>> out with a singular fit warning.
>>>>>
>>>>> However when I ran the isSingular command on it and changed the
>>>>> tolerance
>>>>> to 1e-05 instead of the default 1e-04 that caused the original
>>>>> warning, it
>>>>> comes out as false - no singular fit warning!
>>>>>
>>>>> Does this mean that the first warning is a false positive?
>>>>> I can't find anything that suggests what the tolerance ratio should be
>>>>> but
>>>>> in the GLMM FAQ on github, the troubleshooting example uses 1e-05.
>>>>>
>>>>> Is it fine to stay with this model - I would prefer it to include all
>>>>> the
>>>>> random effects as they are all of interest to me, and the model itself
>>>>> is
>>>>> structured based on how I ran my experiment.
>>>>>
>>>>> Sorry if this is a basic question, I am still learning!
>>>>>
>>>>> Kind regards,
>>>>> Jill
>>>>>
>>>>>         [[alternative HTML version deleted]]
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>
>>>>

	[[alternative HTML version deleted]]


From @ror@j|gy@@@1992 @end|ng |rom gm@||@com  Sat Jan  4 10:58:48 2020
From: @ror@j|gy@@@1992 @end|ng |rom gm@||@com (jigyasa arora)
Date: Sat, 4 Jan 2020 18:58:48 +0900
Subject: [R-sig-ME] Hadfield et al 2014 co-evolution analysis
Message-ID: <CAO0HkAGZS8rBbdhEOt_3PrM1K22CC2QkJSRCXNGot2DP3mgrRg@mail.gmail.com>

Hey!

I want to reproduce Hadfield *et al* (2014) analysis in-
https://datadryad.org/stash/dataset/doi:10.5061/dryad.jf3tj. The
data-analysis from the paper is-
https://datadryad.org/stash/dataset/doi:10.5061/dryad.jf3tj

In the Analysis.R file in the data-analysis link, Hadfield *et al* created
an "*ndat"* dataframe which contains host and parasite counts. When I run
the original codes for MCMC run as it is (as shown below), I get the
following error-

##########################
##### MCMC Count Data ####
##########################
#distribution of counts is assumed to be overdispersed zero-truncated
Poisson

>priorC=list(R=list(V=1, nu=0))
>priorC$G<-lapply(1:9, function(x){list(V=1, nu=1, alpha.mu=0,
alpha.V=1000)})
>names(priorC$G)<-paste("G", 1:9, sep="")

>mC.MCMCa<-MCMCglmm(counts~log(no.hosts.sampled)+log(no.parasites.sampled),
random=~Region+Parasite.species+Host.species+Parasite.species.ide+Host.species.ide+Host.Parasite+Host.Parasite.ide+Host.Parasite.ide2+Host.Parasite.ide3,
family="ztpoisson", data=ndat, ginverse=list(Host.species=hostA,
Host.Parasite=host.parasiteA, Host.Parasite.ide2=host.parasiteAS,
Host.Parasite.ide3=host.parasiteSA), prior=priorC, nitt=1000000, thin=400,
burnin=200000)

Error-
Error in MCMCglmm(counts ~ log(no.hosts.sampled) +
log(no.parasites.sampled),  :
  Zero-truncated Poisson data must be positive integers

If I remove 0 values in "counts" columns from the "ndat" dataframe, then I
will have to change the prior itself according to the new error-

>library(tidyr)
>ndat_cleaned<-ndat %>% filter(counts>0)
>mC.MCMCai<-MCMCglmm(counts~1,
random=~Parasite.species+Host.species+Parasite.species.ide+Host.species.ide+Host.Parasite+Host.Parasite.ide+Host.Parasite.ide2+Host.Parasite.ide3,
family="ztpoisson", data=ndat_cleaned, ginverse=list(Host.species=hostA,
Host.Parasite=host.parasiteA, Host.Parasite.ide2=host.parasiteAS,
Host.Parasite.ide3=host.parasiteSA), prior=priorC, nitt=1000000, thin=400,
burnin=200000)

Error-
Error in MCMCglmm(counts ~ 1, random = ~Parasite.species + Host.species +
 :
  prior$G has the wrong number of structures

How do I make it work?
I want to ultimately check the effect of each of the random effects
separately on the "counts" variable to compare the effect of host
phylogeny, parasite phylogeny, and host-parasite coevolution. Would I need
to change the priors each time?

Thanks!
Jigyasa Arora

	[[alternative HTML version deleted]]


From mec@@te||@no@8 @end|ng |rom gm@||@com  Tue Jan  7 18:23:12 2020
From: mec@@te||@no@8 @end|ng |rom gm@||@com (Maria Eugenia Castellanos)
Date: Tue, 7 Jan 2020 11:23:12 -0600
Subject: [R-sig-ME] Fwd: Request for help using a generalized linear mixed
 model in correlated data
In-Reply-To: <CAKAqquXT3ce1tuBY24EBZAxy+SMuCzjVOqv0ehvSG7pJo8k5Ow@mail.gmail.com>
References: <SN6PR02MB5613378E7877BFEBA33E5B66A83F0@SN6PR02MB5613.namprd02.prod.outlook.com>
 <SN6PR02MB56134EDFD6D268C76B6239CBA83F0@SN6PR02MB5613.namprd02.prod.outlook.com>
 <SN6PR02MB56131FAD3FEE4415D694E992A83F0@SN6PR02MB5613.namprd02.prod.outlook.com>
 <CAKAqquXT3ce1tuBY24EBZAxy+SMuCzjVOqv0ehvSG7pJo8k5Ow@mail.gmail.com>
Message-ID: <CAKAqquVLQxAy4j7KhLdVzwf5==ZstTGree3-8JQzy3KZacE1-Q@mail.gmail.com>

Dear R-sig-mixed-models group,

Happy new year and hope you are fine. I will appreciate a lot your help in
this analysis in which I have spent several weeks.

I am an epidemiologist, trying to estimate risk differences between the
risk of tuberculosis (TB) in  2 groups, contacts of TB cases and contacts
of community controls.

My sample is 1043 contacts of controls and 1002 contacts of cases.

The variable index_contact1  (exposure) indicates if an observation comes
from a contact of a TB case or it is a contact of a community control
(coded 1 and 0 respectively). I want to adjust by age  (enr_age, continuous
variable), sex (enr_sex coded 2=women, 1=men) and HIV status (hiv,
1=infected, 0=non infected).

My binary outcome is called ?tst? and it is coded as 1 (infected) or 0 (not
infected).

One of the reviewers asked me to account for the clustering of these
contacts. So I have a variable called enr_lnkid that indicates the ID of a
TB case (n=122) or a community control (n=124).

All my variables are coded as factors, except for tst and for enr_age,
which they are coded as numeric. I had to write tst as a numeric variable
in order to work with the regression models.

First, I used a GEE model and it worked:

allnoc <- gee(tst ~ index_contact1 + enr_age + enr_sex + hiv, id=netid,
family=binomial ('identity'), corstr = "exchangeable", data = c3)

summary(allnoc)

GEE:  GENERALIZED LINEAR MODELS FOR DEPENDENT DATA

gee S-function, version 4.13 modified 98/01/27 (1998)



Model:

Link:                      Identity

 Variance to Mean Relation: Binomial

 Correlation Structure:     Exchangeable



Call:

gee(formula = tst ~ index_contact1 + enr_age + enr_sex + hiv,

    id = netid, data = c3, family = binomial("identity"), corstr =
"exchangeable")



Summary of Residuals:

       Min         1Q     Median         3Q        Max

-0.9229960 -0.4073128 -0.2338467  0.5127307  0.8636212





Coefficients:

                  Estimate   Naive S.E.    Naive z  Robust S.E.   Robust z

(Intercept)      0.1991535 0.0254609144  7.8219323 0.0254041176  7.8394200

index_contact11  0.1600779 0.0208047552  7.6942957 0.0210721716  7.5966513

enr_age          0.0086733 0.0007726673 11.2251418 0.0009238111  9.3886077

enr_sex2        -0.1061413 0.0208703279 -5.0857503 0.0206749937 -5.1337997

hiv1            -0.0393979 0.0414949448 -0.9494627 0.0418152072 -0.9421908



Estimated Scale Parameter:  1.002437

Number of Iterations:  3



Working Correlation

     [,1]

[1,]    1

>



The result was 0.16 (16%), which is very close to the crude risk difference
(approx.. 14%).

I am trying to replicate my findings, using a generalized linear mixed
model, so I use this code:

  Model1 <- glmer(tst ~  index_contact1 + enr_age + enr_sex + hiv +  (1 |
enr_lnkid), family=binomial, data = c3, control = glmerControl(optimizer =
"bobyqa"), nAGQ=0)



And it works



Generalized linear mixed model fit by maximum likelihood (Adaptive
Gauss-Hermite Quadrature, nAGQ = 0) ['glmerMod']

 Family: binomial  ( logit )

Formula: tst ~ index_contact1 + enr_age + enr_sex + hiv + (1 | enr_lnkid)

   Data: c3

      AIC       BIC    logLik  deviance  df.resid

 2584.565  2618.304 -1286.283  2572.565      2039

Random effects:

 Groups    Name        Std.Dev.

 enr_lnkid (Intercept) 0.7106

Number of obs: 2045, groups:  enr_lnkid, 246

Fixed Effects:

    (Intercept)  index_contact11          enr_age         enr_sex2
        hiv1

       -1.39891          0.75106          0.04386         -0.48146
    -0.35127





But my results come as link=logit whereas I need to have link=identity to
get risk differences.





I tried then this, using Poisson as the family binomial did not accept
link=identity:



#identity

  Model2 <- glmer(tst ~  index_contact1 + enr_age + enr_sex + hiv + (1 |
enr_lnkid), family=poisson(link=identity), data = c3, control =
glmerControl(optimizer = "bobyqa"), nAGQ=0)





But I get this error:



Error in (function (fr, X, reTrms, family, nAGQ = 1L, verbose = 0L,
maxit = 100L,  :

  (maxstephalfit) PIRLS step-halvings failed to reduce deviance in pwrssUpdate





I want to ask two things:



   1. Is there a way that I can convert the results for model 1 from
   link=logit to link=identity?
   2. Or how I can solve these error with the Poisson family?



Thanks for the help you can provide me, thank you!

Maria Eugenia Castellanos





Global Health Institute

College of Public Health

University of Georgia

Athens, GA, USA

	[[alternative HTML version deleted]]


From D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u  Wed Jan  8 03:12:26 2020
From: D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u (David Duffy)
Date: Wed, 8 Jan 2020 02:12:26 +0000
Subject: [R-sig-ME] 
 Fwd: Request for help using a generalized linear mixed
 model in correlated data
In-Reply-To: <CAKAqquVLQxAy4j7KhLdVzwf5==ZstTGree3-8JQzy3KZacE1-Q@mail.gmail.com>
References: <SN6PR02MB5613378E7877BFEBA33E5B66A83F0@SN6PR02MB5613.namprd02.prod.outlook.com>
 <SN6PR02MB56134EDFD6D268C76B6239CBA83F0@SN6PR02MB5613.namprd02.prod.outlook.com>
 <SN6PR02MB56131FAD3FEE4415D694E992A83F0@SN6PR02MB5613.namprd02.prod.outlook.com>
 <CAKAqquXT3ce1tuBY24EBZAxy+SMuCzjVOqv0ehvSG7pJo8k5Ow@mail.gmail.com>,
 <CAKAqquVLQxAy4j7KhLdVzwf5==ZstTGree3-8JQzy3KZacE1-Q@mail.gmail.com>
Message-ID: <773d904e9555489d93432b2cd97e95b7@qimrberghofer.edu.au>

Hi Maria.

The nature of the GEE (ie marginal) model means that it should agree with the "naive" model ignoring clustering.

One way you can use your logistic-normal GLMM is to predict risk for individuals with comparable covariate values using 
predict(mod, type="response"), and calculate the resulting risk difference. Or take your log odds ratio and apply it to a given base rate - 
the hypothesis testing done using the logistic link is correctly allowing for the clustering.  

Recall that the different links will entail different distributions for the cluster means and correlation - ie a logistic link might be more appropriate for the biology generating your data.See also the zoo of alternatives for glmmTMB (beta, beta-binomial, negative binomial etc) that you could send another few weeks on. 

Cheers, David Duffy.

From |uc@@@@nt|n|@eco @end|ng |rom gm@||@com  Thu Jan  9 11:28:32 2020
From: |uc@@@@nt|n|@eco @end|ng |rom gm@||@com (Luca Santini)
Date: Thu, 9 Jan 2020 11:28:32 +0100
Subject: [R-sig-ME] zero-inflation beta or gamma model?
Message-ID: <ECE6E5AF-C162-4402-9AA3-780C3E3BC3D1@gmail.com>

Hi,

I have a response variable expressed as proportion that varies from 0 to 1. The variable is highly left skewed, and about half of the values are zeros. I found a previous question related to a possible use of beta family together with a zero-inflation, to which B. Bolker replied it was possible using a hurdle approach in the glmmTMB package. I rescaled the response to 0 - 0.999 (only 1 value is = 1) and tried this approach.

However, when I run 

mod<-glmmTMB(TerrBeta2 ~ 1 + (1|Family/Genus/Species) + (1|Site/Year/Month), family=beta_family(), ziformula=~., data=data2)

I get the following error message (looks like the ziformula argument is ignored)

Error in eval(family$initialize) : y values must be 0 < y < 1

I also tried to specify the family differently

mod<-glmmTMB(TerrBeta2 ~ 1 + (1|Family/Genus/Species) + (1|Site/Year/Month), family=list(family="beta",link="logit"), ziformula=~., data=data2)

But I get this

Error in nlminb(start = par, objective = fn, gradient = gr, control = control$optCtrl) : 
  gradient function must return a numeric vector of length 15
In addition: Warning messages:
1: In glmmTMB(TerrBeta2 ~ 1 + (1 | Family/Genus/Species) + (1 | Site/Year/Month),  :
  some components missing from ?family?: downstream methods may fail
2: In mkTMBStruc(formula, ziformula, dispformula, combForm, mf, fr,  :
  specifying ?family? as a plain list is deprecated
3: In nlminb(start = par, objective = fn, gradient = gr, control = control$optCtrl) :
  NA/NaN function evaluation
Timing stopped at: 0.626 0.065 0.689

Any suggestion? 

Another option to avoid rescaling the variable may be using the gamma family, but I get into the same error messages. 
Any suggestion would be greatly appreciated!

Best




	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Thu Jan  9 15:26:36 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Thu, 9 Jan 2020 09:26:36 -0500
Subject: [R-sig-ME] zero-inflation beta or gamma model?
In-Reply-To: <ECE6E5AF-C162-4402-9AA3-780C3E3BC3D1@gmail.com>
References: <ECE6E5AF-C162-4402-9AA3-780C3E3BC3D1@gmail.com>
Message-ID: <CABghstTyk-Na3J=gapaixMC2URfQLwHTtpVOn4ACjbXo7Ubd8g@mail.gmail.com>

  The zero-inflation stuff is only available in the development
version of glmmTMB at the moment ... although we hope to submit a new
version to CRAN in the very near future.  (See
https://github.com/glmmTMB/glmmTMB/blob/master/README.md for
instructions on installing the development version.)

On Thu, Jan 9, 2020 at 5:28 AM Luca Santini <luca.santini.eco at gmail.com> wrote:
>
> Hi,
>
> I have a response variable expressed as proportion that varies from 0 to 1. The variable is highly left skewed, and about half of the values are zeros. I found a previous question related to a possible use of beta family together with a zero-inflation, to which B. Bolker replied it was possible using a hurdle approach in the glmmTMB package. I rescaled the response to 0 - 0.999 (only 1 value is = 1) and tried this approach.
>
> However, when I run
>
> mod<-glmmTMB(TerrBeta2 ~ 1 + (1|Family/Genus/Species) + (1|Site/Year/Month), family=beta_family(), ziformula=~., data=data2)
>
> I get the following error message (looks like the ziformula argument is ignored)
>
> Error in eval(family$initialize) : y values must be 0 < y < 1
>
> I also tried to specify the family differently
>
> mod<-glmmTMB(TerrBeta2 ~ 1 + (1|Family/Genus/Species) + (1|Site/Year/Month), family=list(family="beta",link="logit"), ziformula=~., data=data2)
>
> But I get this
>
> Error in nlminb(start = par, objective = fn, gradient = gr, control = control$optCtrl) :
>   gradient function must return a numeric vector of length 15
> In addition: Warning messages:
> 1: In glmmTMB(TerrBeta2 ~ 1 + (1 | Family/Genus/Species) + (1 | Site/Year/Month),  :
>   some components missing from ?family?: downstream methods may fail
> 2: In mkTMBStruc(formula, ziformula, dispformula, combForm, mf, fr,  :
>   specifying ?family? as a plain list is deprecated
> 3: In nlminb(start = par, objective = fn, gradient = gr, control = control$optCtrl) :
>   NA/NaN function evaluation
> Timing stopped at: 0.626 0.065 0.689
>
> Any suggestion?
>
> Another option to avoid rescaling the variable may be using the gamma family, but I get into the same error messages.
> Any suggestion would be greatly appreciated!
>
> Best
>
>
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From |uc@@@@nt|n|@eco @end|ng |rom gm@||@com  Fri Jan 10 13:36:00 2020
From: |uc@@@@nt|n|@eco @end|ng |rom gm@||@com (Luca Santini)
Date: Fri, 10 Jan 2020 13:36:00 +0100
Subject: [R-sig-ME] zero-inflation beta or gamma model?
In-Reply-To: <CABghstTyk-Na3J=gapaixMC2URfQLwHTtpVOn4ACjbXo7Ubd8g@mail.gmail.com>
References: <ECE6E5AF-C162-4402-9AA3-780C3E3BC3D1@gmail.com>
 <CABghstTyk-Na3J=gapaixMC2URfQLwHTtpVOn4ACjbXo7Ubd8g@mail.gmail.com>
Message-ID: <314533C3-E6A6-4077-987E-4158BBB5F475@gmail.com>

It works, thanks!

Luca


> On 9 Jan 2020, at 15:26, Ben Bolker <bbolker at gmail.com> wrote:
> 
>  The zero-inflation stuff is only available in the development
> version of glmmTMB at the moment ... although we hope to submit a new
> version to CRAN in the very near future.  (See
> https://github.com/glmmTMB/glmmTMB/blob/master/README.md for
> instructions on installing the development version.)
> 
> On Thu, Jan 9, 2020 at 5:28 AM Luca Santini <luca.santini.eco at gmail.com> wrote:
>> 
>> Hi,
>> 
>> I have a response variable expressed as proportion that varies from 0 to 1. The variable is highly left skewed, and about half of the values are zeros. I found a previous question related to a possible use of beta family together with a zero-inflation, to which B. Bolker replied it was possible using a hurdle approach in the glmmTMB package. I rescaled the response to 0 - 0.999 (only 1 value is = 1) and tried this approach.
>> 
>> However, when I run
>> 
>> mod<-glmmTMB(TerrBeta2 ~ 1 + (1|Family/Genus/Species) + (1|Site/Year/Month), family=beta_family(), ziformula=~., data=data2)
>> 
>> I get the following error message (looks like the ziformula argument is ignored)
>> 
>> Error in eval(family$initialize) : y values must be 0 < y < 1
>> 
>> I also tried to specify the family differently
>> 
>> mod<-glmmTMB(TerrBeta2 ~ 1 + (1|Family/Genus/Species) + (1|Site/Year/Month), family=list(family="beta",link="logit"), ziformula=~., data=data2)
>> 
>> But I get this
>> 
>> Error in nlminb(start = par, objective = fn, gradient = gr, control = control$optCtrl) :
>>  gradient function must return a numeric vector of length 15
>> In addition: Warning messages:
>> 1: In glmmTMB(TerrBeta2 ~ 1 + (1 | Family/Genus/Species) + (1 | Site/Year/Month),  :
>>  some components missing from ?family?: downstream methods may fail
>> 2: In mkTMBStruc(formula, ziformula, dispformula, combForm, mf, fr,  :
>>  specifying ?family? as a plain list is deprecated
>> 3: In nlminb(start = par, objective = fn, gradient = gr, control = control$optCtrl) :
>>  NA/NaN function evaluation
>> Timing stopped at: 0.626 0.065 0.689
>> 
>> Any suggestion?
>> 
>> Another option to avoid rescaling the variable may be using the gamma family, but I get into the same error messages.
>> Any suggestion would be greatly appreciated!
>> 
>> Best
>> 
>> 
>> 
>> 
>>        [[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From h|gh@t@t @end|ng |rom h|gh@t@t@com  Fri Jan 10 16:35:09 2020
From: h|gh@t@t @end|ng |rom h|gh@t@t@com (Highland Statistics Ltd)
Date: Fri, 10 Jan 2020 15:35:09 +0000
Subject: [R-sig-ME] zero-inflation beta or gamma model?
In-Reply-To: <mailman.18003.11.1578654002.44192.r-sig-mixed-models@r-project.org>
References: <mailman.18003.11.1578654002.44192.r-sig-mixed-models@r-project.org>
Message-ID: <eaf18097-3046-97c5-7aef-0ce7916fc8e3@highstat.com>




> Hi,
>
> I have a response variable expressed as proportion that varies from 0 to 1. The variable is highly left skewed, and about half of the values are zeros. I found a previous question related to a possible use of beta family together with a zero-inflation, to which B. Bolker replied it was possible using a hurdle approach in the glmmTMB package. I rescaled the response to 0 - 0.999 (only 1 value is = 1) and tried this approach.
>
> However, when I run
>
> mod<-glmmTMB(TerrBeta2 ~ 1 + (1|Family/Genus/Species) + (1|Site/Year/Month), family=beta_family(), ziformula=~., data=data2)
>
> I get the following error message (looks like the ziformula argument is ignored)
>
> Error in eval(family$initialize) : y values must be 0 < y < 1
>
> I also tried to specify the family differently
>
> mod<-glmmTMB(TerrBeta2 ~ 1 + (1|Family/Genus/Species) + (1|Site/Year/Month), family=list(family="beta",link="logit"), ziformula=~., data=data2)
>
> But I get this

Your model is very complication in terms of the random effects. 
Besides...I am not a fan of using year and month as random effects.

As to the zero inflation aspect of the beta GLMM....apologies for 
self-citing here, but Chapter 16 in this book:

http://highstat.com/index.php/beginner-s-guide-to-zero-inflated-models

has a fully worked out example using the zero-inflated beta GLMM with 
2-way nested random effects.

And Chapter 23 in Volume II of this one

http://highstat.com/index.php/beginner-s-guide-to-regression-models-with-spatial-and-temporal-correlation

touches on zero-inflated beta GAMs with spatial correlation and barrier 
models.

Kind regards,
Alain




> Error in nlminb(start = par, objective = fn, gradient = gr, control = control$optCtrl) :
>    gradient function must return a numeric vector of length 15
> In addition: Warning messages:
> 1: In glmmTMB(TerrBeta2 ~ 1 + (1 | Family/Genus/Species) + (1 | Site/Year/Month),  :
>    some components missing from ?family?: downstream methods may fail
> 2: In mkTMBStruc(formula, ziformula, dispformula, combForm, mf, fr,  :
>    specifying ?family? as a plain list is deprecated
> 3: In nlminb(start = par, objective = fn, gradient = gr, control = control$optCtrl) :
>    NA/NaN function evaluation
> Timing stopped at: 0.626 0.065 0.689
>
> Any suggestion?
>
> Another option to avoid rescaling the variable may be using the gamma family, but I get into the same error messages.
> Any suggestion would be greatly appreciated!
>
> Best
>
>
>
>
>          [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models




------------------------------

Subject: Digest Footer

_______________________________________________
R-sig-mixed-models mailing list
R-sig-mixed-models at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


------------------------------

End of R-sig-mixed-models Digest, Vol 157, Issue 6
**************************************************

-- 

Dr. Alain F. Zuur
Highland Statistics Ltd.
9 St Clair Wynd
AB41 6DZ Newburgh, UK
Email: highstat at highstat.com
URL:   www.highstat.com


From Ju||@n@G@v|r|@Lopez @end|ng |rom un|ge@ch  Sat Jan 11 12:59:32 2020
From: Ju||@n@G@v|r|@Lopez @end|ng |rom un|ge@ch (Julian Gaviria Lopez)
Date: Sat, 11 Jan 2020 11:59:32 +0000
Subject: [R-sig-ME] Modelling zero inflation with the poisson family. in
 glmmTMB
Message-ID: <eea25e7366164679b2b04c53614743ed@unige.ch>


Dear list members,


I modeled a count data set as follows:


zipoisson1 <- glmmTMB(Observations ~ A* B  + (1|ID), data=mDATA, family=poisson)

zipoisson2 <- glmmTMB(Observations ~ A* B + (1|ID), data=mDATA, ziformula=~ 1, family=poisson)

zipoisson3 <- glmmTMB(Observations ~ A* B + (1|ID), data=mDATA, ziformula=~ Observations, family=poisson)

The results of cross-validated zero inflation and over dispersion tests...

Test1: Easy stats functions:
https://easystats.github.io/performance/index.html

Test2: "DHARMa" package:
https://rdrr.io/cran/DHARMa/man/testDispersion.html
https://rdrr.io/cran/DHARMa/man/testZeroInflation.html


reported over-fitting due to zero-inflation. Strikingly, the results of the zeroinflation are the same, regardless of the model.



> check_zeroinflation(zipoisson1)
# Check for zero-inflation

   Observed zeros: 189
  Predicted zeros: 209
            Ratio: 1.11
Model is overfitting zeros

> check_zeroinflation(zipoisson2)
# Check for zero-inflation

   Observed zeros: 189
  Predicted zeros: 209
            Ratio: 1.11
Model is overfitting zeros.

I wonder whether I am properly modelling the "zi" part. I also downloaded the latest version (0.2.4) of the  "glmmTMB" package from github.

You may find the data set below:

ID      Observations    CAP     Time
6       0       Visual  m1
7       0       Visual  m1
8       0       Visual  m1
10      1       Visual  m1
11      1       Visual  m1
13      1       Visual  m1
15      0       Visual  m1
16      0       Visual  m1
20      1       Visual  m1
22      0       Visual  m1
23      0       Visual  m1
27      0       Visual  m1
30      0       Visual  m1
31      0       Visual  m1
32      0       Visual  m1
33      0       Visual  m1
34      0       Visual  m1
35      1       Visual  m1
37      0       Visual  m1
6       0       DMN     m1
7       0       DMN     m1
8       0       DMN     m1
10      1       DMN     m1
11      0       DMN     m1
13      0       DMN     m1
15      0       DMN     m1
16      1       DMN     m1
20      0       DMN     m1
22      0       DMN     m1
23      1       DMN     m1
27      0       DMN     m1
30      0       DMN     m1
31      0       DMN     m1
32      0       DMN     m1
33      0       DMN     m1
34      0       DMN     m1
35      1       DMN     m1
37      0       DMN     m1
6       1       DAN     m1
7       2       DAN     m1
8       1       DAN     m1
10      0       DAN     m1
11      1       DAN     m1
13      0       DAN     m1
15      1       DAN     m1
16      0       DAN     m1
20      1       DAN     m1
22      0       DAN     m1
23      0       DAN     m1
27      1       DAN     m1
30      0       DAN     m1
31      1       DAN     m1
32      1       DAN     m1
33      0       DAN     m1
34      1       DAN     m1
35      0       DAN     m1
37      0       DAN     m1
6       0       FPCN    m1
7       2       FPCN    m1
8       0       FPCN    m1
10      0       FPCN    m1
11      0       FPCN    m1
13      0       FPCN    m1
15      1       FPCN    m1
16      0       FPCN    m1
20      1       FPCN    m1
22      0       FPCN    m1
23      0       FPCN    m1
27      1       FPCN    m1
30      0       FPCN    m1
31      0       FPCN    m1
32      1       FPCN    m1
33      0       FPCN    m1
34      0       FPCN    m1
35      0       FPCN    m1
37      1       FPCN    m1
6       1       Visual  m2
7       0       Visual  m2
8       0       Visual  m2
10      1       Visual  m2
11      1       Visual  m2
13      2       Visual  m2
15      0       Visual  m2
16      1       Visual  m2
20      1       Visual  m2
22      1       Visual  m2
23      2       Visual  m2
27      1       Visual  m2
30      0       Visual  m2
31      1       Visual  m2
32      3       Visual  m2
33      0       Visual  m2
34      2       Visual  m2
35      1       Visual  m2
37      0       Visual  m2
6       0       DMN     m2
7       0       DMN     m2
8       2       DMN     m2
10      0       DMN     m2
11      2       DMN     m2
13      0       DMN     m2
15      2       DMN     m2
16      0       DMN     m2
20      1       DMN     m2
22      0       DMN     m2
23      0       DMN     m2
27      1       DMN     m2
30      2       DMN     m2
31      2       DMN     m2
32      0       DMN     m2
33      0       DMN     m2
34      1       DMN     m2
35      1       DMN     m2
37      0       DMN     m2
6       2       DAN     m2
7       1       DAN     m2
8       1       DAN     m2
10      1       DAN     m2
11      1       DAN     m2
13      3       DAN     m2
15      1       DAN     m2
16      1       DAN     m2
20      1       DAN     m2
22      1       DAN     m2
23      1       DAN     m2
27      1       DAN     m2
30      0       DAN     m2
31      0       DAN     m2
32      1       DAN     m2
33      0       DAN     m2
34      0       DAN     m2
35      1       DAN     m2
37      1       DAN     m2
6       2       FPCN    m2
7       0       FPCN    m2
8       0       FPCN    m2
10      0       FPCN    m2
11      1       FPCN    m2
13      1       FPCN    m2
15      1       FPCN    m2
16      1       FPCN    m2
20      2       FPCN    m2
22      0       FPCN    m2
23      0       FPCN    m2
27      0       FPCN    m2
30      1       FPCN    m2
31      0       FPCN    m2
32      1       FPCN    m2
33      0       FPCN    m2
34      0       FPCN    m2
35      0       FPCN    m2
37      1       FPCN    m2
6       0       Visual  m3
7       1       Visual  m3
8       0       Visual  m3
10      1       Visual  m3
11      2       Visual  m3
13      0       Visual  m3
15      0       Visual  m3
16      1       Visual  m3
20      1       Visual  m3
22      0       Visual  m3
23      0       Visual  m3
27      2       Visual  m3
30      1       Visual  m3
31      1       Visual  m3
32      1       Visual  m3
33      1       Visual  m3
34      1       Visual  m3
35      0       Visual  m3
37      0       Visual  m3
6       0       DMN     m3
7       0       DMN     m3
8       0       DMN     m3
10      0       DMN     m3
11      1       DMN     m3
13      0       DMN     m3
15      0       DMN     m3
16      3       DMN     m3
20      0       DMN     m3
22      0       DMN     m3
23      1       DMN     m3
27      2       DMN     m3
30      1       DMN     m3
31      1       DMN     m3
32      0       DMN     m3
33      0       DMN     m3
34      0       DMN     m3
35      0       DMN     m3
37      0       DMN     m3
6       2       DAN     m3
7       0       DAN     m3
8       1       DAN     m3
10      0       DAN     m3
11      1       DAN     m3
13      0       DAN     m3
15      1       DAN     m3
16      1       DAN     m3
20      1       DAN     m3
22      3       DAN     m3
23      1       DAN     m3
27      0       DAN     m3
30      0       DAN     m3
31      0       DAN     m3
32      3       DAN     m3
33      3       DAN     m3
34      1       DAN     m3
35      1       DAN     m3
37      0       DAN     m3
6       0       FPCN    m3
7       0       FPCN    m3
8       0       FPCN    m3
10      0       FPCN    m3
11      1       FPCN    m3
13      0       FPCN    m3
15      1       FPCN    m3
16      1       FPCN    m3
20      1       FPCN    m3
22      1       FPCN    m3
23      0       FPCN    m3
27      0       FPCN    m3
30      1       FPCN    m3
31      0       FPCN    m3
32      0       FPCN    m3
33      2       FPCN    m3
34      0       FPCN    m3
35      1       FPCN    m3
37      0       FPCN    m3
6       1       Visual  m4
7       1       Visual  m4
8       2       Visual  m4
10      1       Visual  m4
11      1       Visual  m4
13      1       Visual  m4
15      1       Visual  m4
16      1       Visual  m4
20      0       Visual  m4
22      1       Visual  m4
23      0       Visual  m4
27      0       Visual  m4
30      0       Visual  m4
31      0       Visual  m4
32      1       Visual  m4
33      0       Visual  m4
34      1       Visual  m4
35      2       Visual  m4
37      2       Visual  m4
6       0       DMN     m4
7       0       DMN     m4
8       0       DMN     m4
10      0       DMN     m4
11      1       DMN     m4
13      0       DMN     m4
15      0       DMN     m4
16      1       DMN     m4
20      0       DMN     m4
22      1       DMN     m4
23      0       DMN     m4
27      0       DMN     m4
30      1       DMN     m4
31      0       DMN     m4
32      0       DMN     m4
33      1       DMN     m4
34      2       DMN     m4
35      1       DMN     m4
37      0       DMN     m4
6       2       DAN     m4
7       1       DAN     m4
8       0       DAN     m4
10      0       DAN     m4
11      0       DAN     m4
13      1       DAN     m4
15      1       DAN     m4
16      1       DAN     m4
20      1       DAN     m4
22      1       DAN     m4
23      1       DAN     m4
27      1       DAN     m4
30      0       DAN     m4
31      1       DAN     m4
32      1       DAN     m4
33      1       DAN     m4
34      0       DAN     m4
35      1       DAN     m4
37      1       DAN     m4
6       1       FPCN    m4
7       1       FPCN    m4
8       0       FPCN    m4
10      0       FPCN    m4
11      0       FPCN    m4
13      1       FPCN    m4
15      1       FPCN    m4
16      1       FPCN    m4
20      1       FPCN    m4
22      1       FPCN    m4
23      0       FPCN    m4
27      1       FPCN    m4
30      0       FPCN    m4
31      1       FPCN    m4
32      0       FPCN    m4
33      0       FPCN    m4
34      0       FPCN    m4
35      1       FPCN    m4
37      0       FPCN    m4
6       0       Visual  m5
7       0       Visual  m5
8       0       Visual  m5
10      0       Visual  m5
11      0       Visual  m5
13      1       Visual  m5
15      0       Visual  m5
16      2       Visual  m5
20      2       Visual  m5
22      0       Visual  m5
23      1       Visual  m5
27      0       Visual  m5
30      0       Visual  m5
31      0       Visual  m5
32      0       Visual  m5
33      1       Visual  m5
34      0       Visual  m5
35      0       Visual  m5
37      2       Visual  m5
6       1       DMN     m5
7       1       DMN     m5
8       1       DMN     m5
10      1       DMN     m5
11      1       DMN     m5
13      0       DMN     m5
15      0       DMN     m5
16      0       DMN     m5
20      0       DMN     m5
22      1       DMN     m5
23      1       DMN     m5
27      1       DMN     m5
30      0       DMN     m5
31      1       DMN     m5
32      1       DMN     m5
33      2       DMN     m5
34      0       DMN     m5
35      1       DMN     m5
37      0       DMN     m5
6       1       DAN     m5
7       1       DAN     m5
8       1       DAN     m5
10      0       DAN     m5
11      1       DAN     m5
13      0       DAN     m5
15      1       DAN     m5
16      0       DAN     m5
20      1       DAN     m5
22      2       DAN     m5
23      0       DAN     m5
27      0       DAN     m5
30      2       DAN     m5
31      0       DAN     m5
32      1       DAN     m5
33      2       DAN     m5
34      0       DAN     m5
35      2       DAN     m5
37      1       DAN     m5
6       1       FPCN    m5
7       1       FPCN    m5
8       0       FPCN    m5
10      1       FPCN    m5
11      0       FPCN    m5
13      1       FPCN    m5
15      0       FPCN    m5
16      1       FPCN    m5
20      2       FPCN    m5
22      0       FPCN    m5
23      0       FPCN    m5
27      0       FPCN    m5
30      0       FPCN    m5
31      0       FPCN    m5
32      2       FPCN    m5
33      0       FPCN    m5
34      1       FPCN    m5
35      1       FPCN    m5
37      0       FPCN    m5


Thanks in advance for any comment on this regard,


Julian Gaviria
Neurology and Imaging of cognition lab (Labnic)
University of Geneva. Campus Biotech.
9 Chemin des Mines, 1202 Geneva, CH
Tel: +41 22 379 0380
Email: Julian.GaviriaLopez at unige.ch

	[[alternative HTML version deleted]]


From |uc@@@@nt|n|@eco @end|ng |rom gm@||@com  Sun Jan 12 14:29:45 2020
From: |uc@@@@nt|n|@eco @end|ng |rom gm@||@com (Luca Santini)
Date: Sun, 12 Jan 2020 14:29:45 +0100
Subject: [R-sig-ME] zero-inflation beta or gamma model?
In-Reply-To: <mailman.18005.7.1578740402.29977.r-sig-mixed-models@r-project.org>
References: <mailman.18005.7.1578740402.29977.r-sig-mixed-models@r-project.org>
Message-ID: <2DCF5435-2047-4734-B414-CD9F71386FBF@gmail.com>

Hi Alain,

Thank you for your suggestion, I have a couple of your books but not these ones, so I cannot really access the worked out examples..
I?m curious however to know more about your opinion on the ?too many? random effects. They are a lot indeed.
The thing is that have have data from multiple populations of different species in different sites, years and months.
My response variable may change throughout the months (different seasons may show different patterns), but I?m not really interested in this variation. The example I shared was an only-intercept model but I?m actually interested in including a number of fixed effects for which I do have good hypotheses. 
Is there a specific reason why you are against using months as random effects? What else would you suggest? 
Thanks

Luca


> On 11 Jan 2020, at 12:00, r-sig-mixed-models-request at r-project.org wrote:
> 
> Send R-sig-mixed-models mailing list submissions to
> 	r-sig-mixed-models at r-project.org
> 
> To subscribe or unsubscribe via the World Wide Web, visit
> 	https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> or, via email, send a message with subject or body 'help' to
> 	r-sig-mixed-models-request at r-project.org
> 
> You can reach the person managing the list at
> 	r-sig-mixed-models-owner at r-project.org
> 
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of R-sig-mixed-models digest..."
> 
> 
> Today's Topics:
> 
>   1. Re: zero-inflation beta or gamma model? (Luca Santini)
>   2. Re: zero-inflation beta or gamma model? (Highland Statistics Ltd)
> 
> ----------------------------------------------------------------------
> 
> Message: 1
> Date: Fri, 10 Jan 2020 13:36:00 +0100
> From: Luca Santini <luca.santini.eco at gmail.com>
> To: Ben Bolker <bbolker at gmail.com>
> Cc: R SIG Mixed Models <r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] zero-inflation beta or gamma model?
> Message-ID: <314533C3-E6A6-4077-987E-4158BBB5F475 at gmail.com>
> Content-Type: text/plain; charset="utf-8"
> 
> It works, thanks!
> 
> Luca
> 
> 
>> On 9 Jan 2020, at 15:26, Ben Bolker <bbolker at gmail.com> wrote:
>> 
>> The zero-inflation stuff is only available in the development
>> version of glmmTMB at the moment ... although we hope to submit a new
>> version to CRAN in the very near future.  (See
>> https://github.com/glmmTMB/glmmTMB/blob/master/README.md for
>> instructions on installing the development version.)
>> 
>> On Thu, Jan 9, 2020 at 5:28 AM Luca Santini <luca.santini.eco at gmail.com> wrote:
>>> 
>>> Hi,
>>> 
>>> I have a response variable expressed as proportion that varies from 0 to 1. The variable is highly left skewed, and about half of the values are zeros. I found a previous question related to a possible use of beta family together with a zero-inflation, to which B. Bolker replied it was possible using a hurdle approach in the glmmTMB package. I rescaled the response to 0 - 0.999 (only 1 value is = 1) and tried this approach.
>>> 
>>> However, when I run
>>> 
>>> mod<-glmmTMB(TerrBeta2 ~ 1 + (1|Family/Genus/Species) + (1|Site/Year/Month), family=beta_family(), ziformula=~., data=data2)
>>> 
>>> I get the following error message (looks like the ziformula argument is ignored)
>>> 
>>> Error in eval(family$initialize) : y values must be 0 < y < 1
>>> 
>>> I also tried to specify the family differently
>>> 
>>> mod<-glmmTMB(TerrBeta2 ~ 1 + (1|Family/Genus/Species) + (1|Site/Year/Month), family=list(family="beta",link="logit"), ziformula=~., data=data2)
>>> 
>>> But I get this
>>> 
>>> Error in nlminb(start = par, objective = fn, gradient = gr, control = control$optCtrl) :
>>> gradient function must return a numeric vector of length 15
>>> In addition: Warning messages:
>>> 1: In glmmTMB(TerrBeta2 ~ 1 + (1 | Family/Genus/Species) + (1 | Site/Year/Month),  :
>>> some components missing from ?family?: downstream methods may fail
>>> 2: In mkTMBStruc(formula, ziformula, dispformula, combForm, mf, fr,  :
>>> specifying ?family? as a plain list is deprecated
>>> 3: In nlminb(start = par, objective = fn, gradient = gr, control = control$optCtrl) :
>>> NA/NaN function evaluation
>>> Timing stopped at: 0.626 0.065 0.689
>>> 
>>> Any suggestion?
>>> 
>>> Another option to avoid rescaling the variable may be using the gamma family, but I get into the same error messages.
>>> Any suggestion would be greatly appreciated!
>>> 
>>> Best
>>> 
>>> 
>>> 
>>> 
>>>       [[alternative HTML version deleted]]
>>> 
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> 
> 
> ------------------------------
> 
> Message: 2
> Date: Fri, 10 Jan 2020 15:35:09 +0000
> From: Highland Statistics Ltd <highstat at highstat.com>
> To: "r-sig-mixed-models at r-project.org"
> 	<r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] zero-inflation beta or gamma model?
> Message-ID: <eaf18097-3046-97c5-7aef-0ce7916fc8e3 at highstat.com>
> Content-Type: text/plain; charset="utf-8"; Format="flowed"
> 
> 
> 
> 
>> Hi,
>> 
>> I have a response variable expressed as proportion that varies from 0 to 1. The variable is highly left skewed, and about half of the values are zeros. I found a previous question related to a possible use of beta family together with a zero-inflation, to which B. Bolker replied it was possible using a hurdle approach in the glmmTMB package. I rescaled the response to 0 - 0.999 (only 1 value is = 1) and tried this approach.
>> 
>> However, when I run
>> 
>> mod<-glmmTMB(TerrBeta2 ~ 1 + (1|Family/Genus/Species) + (1|Site/Year/Month), family=beta_family(), ziformula=~., data=data2)
>> 
>> I get the following error message (looks like the ziformula argument is ignored)
>> 
>> Error in eval(family$initialize) : y values must be 0 < y < 1
>> 
>> I also tried to specify the family differently
>> 
>> mod<-glmmTMB(TerrBeta2 ~ 1 + (1|Family/Genus/Species) + (1|Site/Year/Month), family=list(family="beta",link="logit"), ziformula=~., data=data2)
>> 
>> But I get this
> 
> Your model is very complication in terms of the random effects. 
> Besides...I am not a fan of using year and month as random effects.
> 
> As to the zero inflation aspect of the beta GLMM....apologies for 
> self-citing here, but Chapter 16 in this book:
> 
> http://highstat.com/index.php/beginner-s-guide-to-zero-inflated-models
> 
> has a fully worked out example using the zero-inflated beta GLMM with 
> 2-way nested random effects.
> 
> And Chapter 23 in Volume II of this one
> 
> http://highstat.com/index.php/beginner-s-guide-to-regression-models-with-spatial-and-temporal-correlation
> 
> touches on zero-inflated beta GAMs with spatial correlation and barrier 
> models.
> 
> Kind regards,
> Alain
> 
> 
> 
> 
>> Error in nlminb(start = par, objective = fn, gradient = gr, control = control$optCtrl) :
>>   gradient function must return a numeric vector of length 15
>> In addition: Warning messages:
>> 1: In glmmTMB(TerrBeta2 ~ 1 + (1 | Family/Genus/Species) + (1 | Site/Year/Month),  :
>>   some components missing from ?family?: downstream methods may fail
>> 2: In mkTMBStruc(formula, ziformula, dispformula, combForm, mf, fr,  :
>>   specifying ?family? as a plain list is deprecated
>> 3: In nlminb(start = par, objective = fn, gradient = gr, control = control$optCtrl) :
>>   NA/NaN function evaluation
>> Timing stopped at: 0.626 0.065 0.689
>> 
>> Any suggestion?
>> 
>> Another option to avoid rescaling the variable may be using the gamma family, but I get into the same error messages.
>> Any suggestion would be greatly appreciated!
>> 
>> Best
>> 
>> 
>> 
>> 
>>         [[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> 
> 
> ------------------------------
> 
> Subject: Digest Footer
> 
> _______________________________________________
> R-sig-mixed-models mailing list
> R-sig-mixed-models at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> ------------------------------
> 
> End of R-sig-mixed-models Digest, Vol 157, Issue 6
> **************************************************
> 
> -- 
> 
> Dr. Alain F. Zuur
> Highland Statistics Ltd.
> 9 St Clair Wynd
> AB41 6DZ Newburgh, UK
> Email: highstat at highstat.com
> URL:   www.highstat.com
> 
> 
> 
> 
> ------------------------------
> 
> Subject: Digest Footer
> 
> _______________________________________________
> R-sig-mixed-models mailing list
> R-sig-mixed-models at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> ------------------------------
> 
> End of R-sig-mixed-models Digest, Vol 157, Issue 7
> **************************************************


From h|gh@t@t @end|ng |rom h|gh@t@t@com  Mon Jan 13 16:12:02 2020
From: h|gh@t@t @end|ng |rom h|gh@t@t@com (Highland Statistics Ltd)
Date: Mon, 13 Jan 2020 15:12:02 +0000
Subject: [R-sig-ME] Fwd: Re: zero-inflation beta or gamma model
In-Reply-To: <mailman.18007.7.1578913202.45901.r-sig-mixed-models@r-project.org>
References: <mailman.18007.7.1578913202.45901.r-sig-mixed-models@r-project.org>
Message-ID: <1d5c8e3b-33e3-676a-9abb-4cb2251e5e87@highstat.com>



Message: 1
Date: Sun, 12 Jan 2020 14:29:45 +0100
From: Luca Santini <luca.santini.eco at gmail.com>
To: R SIG Mixed Models <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] zero-inflation beta or gamma model?
Message-ID: <2DCF5435-2047-4734-B414-CD9F71386FBF at gmail.com>
Content-Type: text/plain; charset="utf-8"

Hi Alain,

Thank you for your suggestion, I have a couple of your books but not 
these ones, so I cannot really access the worked out examples..
I?m curious however to know more about your opinion on the ?too many? 
random effects. They are a lot indeed.
The thing is that have have data from multiple populations of different 
species in different sites, years and months.
My response variable may change throughout the months (different seasons 
may show different patterns), but I?m not really interested in this 
variation. The example I shared was an only-intercept model but I?m 
actually interested in including a number of fixed effects for which I 
do have good hypotheses. Is there a specific reason why you are against 
using months as random effects?





Dear Luca,

Random effects are supposed to be independent and identical distributed 
(iid). I suggest that you extract your random effects year, and plot 
them in sequential order. Most likely you will see a temporal trend. Do 
the same for your random effect month. Most likely, you will see a 
seasonal pattern. That is not iid!

What I am suggesting is that such random effects are likely to be 
correlated over time.

Besides...you are going to gave 6 sigmas (for the random effects), a 
beta distribution, and you want to double them (?) with a zero-inflated 
beta model. It is a miracle that glmmTMB (or any other package) runs.

And if you have >50 sites, maybe check the random effects 'Site' for 
spatial correlation?

As to your question....'what would you suggest?'.? The answer is perhaps 
to start simple and build up the model. And that depends very much on 
how many sites, how many years, how many species, etc etc.

Kind regards,

Alain





What else would you suggest? Thanks

Luca


> On 11 Jan 2020, at 12:00, r-sig-mixed-models-request at r-project.org wrote:
>
> Send R-sig-mixed-models mailing list submissions to
> r-sig-mixed-models at r-project.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> or, via email, send a message with subject or body 'help' to
> r-sig-mixed-models-request at r-project.org
>
> You can reach the person managing the list at
> r-sig-mixed-models-owner at r-project.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of R-sig-mixed-models digest..."
>
>
> Today's Topics:
>
> 1. Re: zero-inflation beta or gamma model? (Luca Santini)
> 2. Re: zero-inflation beta or gamma model? (Highland Statistics Ltd)
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Fri, 10 Jan 2020 13:36:00 +0100
> From: Luca Santini <luca.santini.eco at gmail.com>
> To: Ben Bolker <bbolker at gmail.com>
> Cc: R SIG Mixed Models <r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] zero-inflation beta or gamma model?
> Message-ID: <314533C3-E6A6-4077-987E-4158BBB5F475 at gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> It works, thanks!
>
> Luca
>
>
>> On 9 Jan 2020, at 15:26, Ben Bolker <bbolker at gmail.com> wrote:
>>
>> The zero-inflation stuff is only available in the development
>> version of glmmTMB at the moment ... although we hope to submit a new
>> version to CRAN in the very near future. (See
>> https://github.com/glmmTMB/glmmTMB/blob/master/README.md for
>> instructions on installing the development version.)
>>
>> On Thu, Jan 9, 2020 at 5:28 AM Luca Santini 
>> <luca.santini.eco at gmail.com> wrote:
>>>
>>> Hi,
>>>
>>> I have a response variable expressed as proportion that varies from 
>>> 0 to 1. The variable is highly left skewed, and about half of the 
>>> values are zeros. I found a previous question related to a possible 
>>> use of beta family together with a zero-inflation, to which B. 
>>> Bolker replied it was possible using a hurdle approach in the 
>>> glmmTMB package. I rescaled the response to 0 - 0.999 (only 1 value 
>>> is = 1) and tried this approach.
>>>
>>> However, when I run
>>>
>>> mod<-glmmTMB(TerrBeta2 ~ 1 + (1|Family/Genus/Species) + 
>>> (1|Site/Year/Month), family=beta_family(), ziformula=~., data=data2)
>>>
>>> I get the following error message (looks like the ziformula argument 
>>> is ignored)
>>>
>>> Error in eval(family$initialize) : y values must be 0 < y < 1
>>>
>>> I also tried to specify the family differently
>>>
>>> mod<-glmmTMB(TerrBeta2 ~ 1 + (1|Family/Genus/Species) + 
>>> (1|Site/Year/Month), family=list(family="beta",link="logit"), 
>>> ziformula=~., data=data2)
>>>
>>> But I get this
>>>
>>> Error in nlminb(start = par, objective = fn, gradient = gr, control 
>>> = control$optCtrl) :
>>> gradient function must return a numeric vector of length 15
>>> In addition: Warning messages:
>>> 1: In glmmTMB(TerrBeta2 ~ 1 + (1 | Family/Genus/Species) + (1 | 
>>> Site/Year/Month), :
>>> some components missing from ?family?: downstream methods may fail
>>> 2: In mkTMBStruc(formula, ziformula, dispformula, combForm, mf, fr, :
>>> specifying ?family? as a plain list is deprecated
>>> 3: In nlminb(start = par, objective = fn, gradient = gr, control = 
>>> control$optCtrl) :
>>> NA/NaN function evaluation
>>> Timing stopped at: 0.626 0.065 0.689
>>>
>>> Any suggestion?
>>>
>>> Another option to avoid rescaling the variable may be using the 
>>> gamma family, but I get into the same error messages.
>>> Any suggestion would be greatly appreciated!
>>>
>>> Best
>>>
>>>
>>>
>>>
>>> [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>
>
> ------------------------------
>
> Message: 2
> Date: Fri, 10 Jan 2020 15:35:09 +0000
> From: Highland Statistics Ltd <highstat at highstat.com>
> To: "r-sig-mixed-models at r-project.org"
> <r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] zero-inflation beta or gamma model?
> Message-ID: <eaf18097-3046-97c5-7aef-0ce7916fc8e3 at highstat.com>
> Content-Type: text/plain; charset="utf-8"; Format="flowed"
>
>
>
>
>> Hi,
>>
>> I have a response variable expressed as proportion that varies from 0 
>> to 1. The variable is highly left skewed, and about half of the 
>> values are zeros. I found a previous question related to a possible 
>> use of beta family together with a zero-inflation, to which B. Bolker 
>> replied it was possible using a hurdle approach in the glmmTMB 
>> package. I rescaled the response to 0 - 0.999 (only 1 value is = 1) 
>> and tried this approach.
>>
>> However, when I run
>>
>> mod<-glmmTMB(TerrBeta2 ~ 1 + (1|Family/Genus/Species) + 
>> (1|Site/Year/Month), family=beta_family(), ziformula=~., data=data2)
>>
>> I get the following error message (looks like the ziformula argument 
>> is ignored)
>>
>> Error in eval(family$initialize) : y values must be 0 < y < 1
>>
>> I also tried to specify the family differently
>>
>> mod<-glmmTMB(TerrBeta2 ~ 1 + (1|Family/Genus/Species) + 
>> (1|Site/Year/Month), family=list(family="beta",link="logit"), 
>> ziformula=~., data=data2)
>>
>> But I get this
>
> Your model is very complication in terms of the random effects. 
> Besides...I am not a fan of using year and month as random effects.
>
> As to the zero inflation aspect of the beta GLMM....apologies for 
> self-citing here, but Chapter 16 in this book:
>
> http://highstat.com/index.php/beginner-s-guide-to-zero-inflated-models
>
> has a fully worked out example using the zero-inflated beta GLMM with 
> 2-way nested random effects.
>
> And Chapter 23 in Volume II of this one
>
> http://highstat.com/index.php/beginner-s-guide-to-regression-models-with-spatial-and-temporal-correlation
>
> touches on zero-inflated beta GAMs with spatial correlation and 
> barrier models.
>
> Kind regards,
> Alain
>
>
>
>
>> Error in nlminb(start = par, objective = fn, gradient = gr, control = 
>> control$optCtrl) :
>> gradient function must return a numeric vector of length 15
>> In addition: Warning messages:
>> 1: In glmmTMB(TerrBeta2 ~ 1 + (1 | Family/Genus/Species) + (1 | 
>> Site/Year/Month), :
>> some components missing from ?family?: downstream methods may fail
>> 2: In mkTMBStruc(formula, ziformula, dispformula, combForm, mf, fr, :
>> specifying ?family? as a plain list is deprecated
>> 3: In nlminb(start = par, objective = fn, gradient = gr, control = 
>> control$optCtrl) :
>> NA/NaN function evaluation
>> Timing stopped at: 0.626 0.065 0.689
>>
>> Any suggestion?
>>
>> Another option to avoid rescaling the variable may be using the gamma 
>> family, but I get into the same error messages.
>> Any suggestion would be greatly appreciated!
>>
>> Best
>>
>>
>>
>>
>> [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> R-sig-mixed-models mailing list
> R-sig-mixed-models at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
> ------------------------------
>
> End of R-sig-mixed-models Digest, Vol 157, Issue 6
> **************************************************
>
> -- 
> Dr. Alain F. Zuur
> Highland Statistics Ltd.
> 9 St Clair Wynd
> AB41 6DZ Newburgh, UK
> Email: highstat at highstat.com
> URL: www.highstat.com
>
>
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> R-sig-mixed-models mailing list
> R-sig-mixed-models at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
> ------------------------------
>
> End of R-sig-mixed-models Digest, Vol 157, Issue 7
> **************************************************




------------------------------

Subject: Digest Footer

_______________________________________________
R-sig-mixed-models mailing list
R-sig-mixed-models at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


------------------------------

End of R-sig-mixed-models Digest, Vol 157, Issue 9
**************************************************

-- 

Dr. Alain F. Zuur
Highland Statistics Ltd.
9 St Clair Wynd
AB41 6DZ Newburgh, UK
Email: highstat at highstat.com
URL:   www.highstat.com


From bbo|ker @end|ng |rom gm@||@com  Mon Jan 13 23:04:34 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 13 Jan 2020 17:04:34 -0500
Subject: [R-sig-ME] testing OpenMP-enabled glmmTMB on various configurations
Message-ID: <cb06e9f9-892b-c78b-1b78-4dd4e92fe58d@math.mcmaster.ca>

  Hi folks,

  We're trying to troubleshoot a version of glmmTMB that enables OpenMP
support, but we have a fairly limited set of test platforms and it would
be nice to know if many users are going to have installation difficulties.

  If you have development tools (and the 'remotes' package) installed
and a few free minutes, it would be great if you could try

remotes::install_github("glmmTMB/glmmTMB/glmmTMB at conditional_openmp")

and let us know if you have any trouble installing (it should install
whether or not you have OpenMP enabled ...)

If you're feeling energetic or distractible, you could try the tests here:

https://github.com/glmmTMB/glmmTMB/issues/481#issuecomment-572767245

the expected result is you should see *either* warnings that OpenMP is
not enabled *or* at least some speed-up for the parallel version over
the serial version.

Results (especially problematic ones!) welcome at
https://github.com/glmmTMB/glmmTMB/issues/481


From |uc@@@@nt|n|@eco @end|ng |rom gm@||@com  Tue Jan 14 14:00:19 2020
From: |uc@@@@nt|n|@eco @end|ng |rom gm@||@com (Luca Santini)
Date: Tue, 14 Jan 2020 14:00:19 +0100
Subject: [R-sig-ME] zero-inflation beta or gamma model
In-Reply-To: <mailman.18009.7.1578999601.34925.r-sig-mixed-models@r-project.org>
References: <mailman.18009.7.1578999601.34925.r-sig-mixed-models@r-project.org>
Message-ID: <8016A6A3-ACC2-46E5-B83F-8863B6704287@gmail.com>

Hi Alain,

These are very good points, thank you, I will follow your suggestions.

Cheers

Luca


From edg@rjgonz@|ez @end|ng |rom c|enc|@@@un@m@mx  Tue Jan 14 02:33:13 2020
From: edg@rjgonz@|ez @end|ng |rom c|enc|@@@un@m@mx (Edgar J Gonzalez)
Date: Mon, 13 Jan 2020 19:33:13 -0600
Subject: [R-sig-ME] 
 testing OpenMP-enabled glmmTMB on various configurations
In-Reply-To: <cb06e9f9-892b-c78b-1b78-4dd4e92fe58d@math.mcmaster.ca>
References: <cb06e9f9-892b-c78b-1b78-4dd4e92fe58d@math.mcmaster.ca>
Message-ID: <B5C064FD-4958-40A5-ACBB-3852DA257BB5@ymail.com>

Hi. This is what I got:

> library(remotes)
> remotes::install_github("glmmTMB/glmmTMB/glmmTMB at conditional_openmp")
Downloading GitHub repo glmmTMB/glmmTMB at conditional_openmp
These packages have more recent versions available.
Which would you like to update?

1: All                                      
2: CRAN packages only                       
3: None                                     
4: RcppEigen (0.3.3.5.0 -> 0.3.3.7.0) [CRAN]
5: Rcpp      (1.0.1     -> 1.0.3    ) [CRAN]

Enter one or more numbers, or an empty line to skip updates:
1
RcppEigen (0.3.3.5.0 -> 0.3.3.7.0) [CRAN]
Rcpp      (1.0.1     -> 1.0.3    ) [CRAN]
Installing 2 packages: RcppEigen, Rcpp
probando la URL 'https://cran.itam.mx/bin/macosx/el-capitan/contrib/3.6/RcppEigen_0.3.3.7.0.tgz <https://cran.itam.mx/bin/macosx/el-capitan/contrib/3.6/RcppEigen_0.3.3.7.0.tgz>'
Content type 'application/x-gzip' length 4842361 bytes (4.6 MB)
==================================================
downloaded 4.6 MB

probando la URL 'https://cran.itam.mx/bin/macosx/el-capitan/contrib/3.6/Rcpp_1.0.3.tgz <https://cran.itam.mx/bin/macosx/el-capitan/contrib/3.6/Rcpp_1.0.3.tgz>'
Content type 'application/x-gzip' length 3099696 bytes (3.0 MB)
==================================================
downloaded 3.0 MB


The downloaded binary packages are in
	/var/folders/bt/kwm2wvnn0m37kj9j7b4dxdch0000gn/T//RtmpYU7u2V/downloaded_packages
   checking for file ?/private/var/folders/bt/kwm2wvnn0m37kj9j7b4dxdch0000gn/T/R?  checking for file ?/private/var/folders/bt/kwm2wvnn0m37kj9j7b4dxdch0000gn/T/RtmpYU7u2V/remotes486647c4e2db/glmmTMB-glmmTMB-42b0d9c/glmmTMB/DESCRIPTION? (404ms)
?  preparing ?glmmTMB?:
?  checking DESCRIPTION meta-information ...
?  cleaning src
?  checking for LF line-endings in source and make files and shell scripts
?  checking for empty or unneeded directories
   Removed empty directory ?glmmTMB/inst/misc?
?  looking to see if a ?data/datalist? file should be added
?  building ?glmmTMB_1.0.0.tar.gz?
   
* installing *source* package ?glmmTMB? ...
** using staged installation
** libs
clang++ -arch x86_64 -ftemplate-depth-256 -I"/Library/Frameworks/R.framework/Resources/include" -DNDEBUG  -I"/Library/Frameworks/R.framework/Versions/3.6/Resources/library/TMB/include" -I"/Library/Frameworks/R.framework/Versions/3.6/Resources/library/RcppEigen/include" -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk -I/usr/local/include -fopenmp -fPIC  -O3 -mtune=native -march=native -Wno-unused-variable -Wno-unused-function  -Wno-macro-redefined -Wno-unknown-pragmas -c glmmTMB.cpp -o glmmTMB.o
clang: error: unsupported option '-fopenmp'
make: *** [glmmTMB.o] Error 1
ERROR: compilation failed for package ?glmmTMB?
* removing ?/Library/Frameworks/R.framework/Versions/3.6/Resources/library/glmmTMB?
* restoring previous ?/Library/Frameworks/R.framework/Versions/3.6/Resources/library/glmmTMB?
Error: Failed to install 'glmmTMB' from GitHub:
  (convertido del aviso) installation of package ?/var/folders/bt/kwm2wvnn0m37kj9j7b4dxdch0000gn/T//RtmpYU7u2V/file48666cf3703/glmmTMB_1.0.0.tar.gz? had non-zero exit status

Hope it helps...

> On Jan 13, 2020, at 4:04 PM, Ben Bolker <bbolker at gmail.com <mailto:bbolker at gmail.com>> wrote:
> 
>  Hi folks,
> 
>  We're trying to troubleshoot a version of glmmTMB that enables OpenMP
> support, but we have a fairly limited set of test platforms and it would
> be nice to know if many users are going to have installation difficulties.
> 
>  If you have development tools (and the 'remotes' package) installed
> and a few free minutes, it would be great if you could try
> 
> remotes::install_github("glmmTMB/glmmTMB/glmmTMB at conditional_openmp")
> 
> and let us know if you have any trouble installing (it should install
> whether or not you have OpenMP enabled ...)
> 
> If you're feeling energetic or distractible, you could try the tests here:
> 
> https://github.com/glmmTMB/glmmTMB/issues/481#issuecomment-572767245 <https://github.com/glmmTMB/glmmTMB/issues/481#issuecomment-572767245>
> 
> the expected result is you should see *either* warnings that OpenMP is
> not enabled *or* at least some speed-up for the parallel version over
> the serial version.
> 
> Results (especially problematic ones!) welcome at
> https://github.com/glmmTMB/glmmTMB/issues/481
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From gu|||@ume@|mon@@2 @end|ng |rom gm@||@com  Tue Jan 14 16:44:40 2020
From: gu|||@ume@|mon@@2 @end|ng |rom gm@||@com (Guillaume Adeux)
Date: Tue, 14 Jan 2020 16:44:40 +0100
Subject: [R-sig-ME] 
 testing OpenMP-enabled glmmTMB on various configurations
In-Reply-To: <cb06e9f9-892b-c78b-1b78-4dd4e92fe58d@math.mcmaster.ca>
References: <cb06e9f9-892b-c78b-1b78-4dd4e92fe58d@math.mcmaster.ca>
Message-ID: <CAENiVe9pTq+wkd7M7GEboiJ8BHbRYHadzO+PEPUdDsC6em8X8g@mail.gmail.com>

If, for once, I can be of any help to you... this is what I got:

> remotes::install_github("glmmTMB/glmmTMB/glmmTMB at conditional_openmp")Downloading GitHub repo glmmTMB/glmmTMB at conditional_openmpThese packages have more recent versions available.
Which would you like to update?

1:   All
2:   CRAN packages only
3:   None
4:   Rcpp      (1.0.1     -> 1.0.3    ) [CRAN]
5:   RcppEigen (0.3.3.5.0 -> 0.3.3.7.0) [CRAN]

Enter one or more numbers separated by spaces, or an empty line to
cancel1: 1Rcpp      (1.0.1     -> 1.0.3    ) [CRAN]
RcppEigen (0.3.3.5.0 -> 0.3.3.7.0) [CRAN]Installing 2 packages: Rcpp,
RcppEigentrying URL
'https://cran.rstudio.com/bin/windows/contrib/3.5/Rcpp_1.0.3.zip'Content
type 'application/zip' length 3011719 bytes (2.9 MB)downloaded 2.9 MB
trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.5/RcppEigen_0.3.3.7.0.zip'Content
type 'application/zip' length 2676104 bytes (2.6 MB)downloaded 2.6 MB
package ?Rcpp? successfully unpacked and MD5 sums checkedError:
(converted from warning) cannot remove prior installation of package
?Rcpp?

Cheers,

GA2


Le lun. 13 janv. 2020 ? 23:04, Ben Bolker <bbolker at gmail.com> a ?crit :

>   Hi folks,
>
>   We're trying to troubleshoot a version of glmmTMB that enables OpenMP
> support, but we have a fairly limited set of test platforms and it would
> be nice to know if many users are going to have installation difficulties.
>
>   If you have development tools (and the 'remotes' package) installed
> and a few free minutes, it would be great if you could try
>
> remotes::install_github("glmmTMB/glmmTMB/glmmTMB at conditional_openmp")
>
> and let us know if you have any trouble installing (it should install
> whether or not you have OpenMP enabled ...)
>
> If you're feeling energetic or distractible, you could try the tests here:
>
> https://github.com/glmmTMB/glmmTMB/issues/481#issuecomment-572767245
>
> the expected result is you should see *either* warnings that OpenMP is
> not enabled *or* at least some speed-up for the parallel version over
> the serial version.
>
> Results (especially problematic ones!) welcome at
> https://github.com/glmmTMB/glmmTMB/issues/481
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From h|gh@t@t @end|ng |rom h|gh@t@t@com  Thu Jan 16 10:46:35 2020
From: h|gh@t@t @end|ng |rom h|gh@t@t@com (Highland Statistics Ltd)
Date: Thu, 16 Jan 2020 09:46:35 +0000
Subject: [R-sig-ME] Course: Zero-inflated GAMs and GAMMs for the analysis of
 spatial and spatial-temporal correlated data using R-INLA
Message-ID: <678004ed-28c1-7fe3-878b-6693378e0443@highstat.com>


We would like to announce the following statistics course.

Course: Zero-inflated GAMs and GAMMs for the analysis of spatial and 
spatial-temporal correlated data using R-INLA

Where and when: NIOZ, Texel, The Netherlands. 20 - 24 April 2020

Course website: http://highstat.com/index.php/courses-upcoming
Course flyer: 
http://highstat.com/Courses/Flyers/2020/Flyer2020_05NIOZ_GAMZISPatTemp.pdf

Kind regards,


Alain Zuur

-- 

Dr. Alain F. Zuur
Highland Statistics Ltd.
9 St Clair Wynd
AB41 6DZ Newburgh, UK
Email: highstat at highstat.com
URL:   www.highstat.com


From bbo|ker @end|ng |rom gm@||@com  Fri Jan 17 22:00:25 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 17 Jan 2020 16:00:25 -0500
Subject: [R-sig-ME] incorporating effort as an effect in binomial GLMM
Message-ID: <773643a4-28b7-5f81-4e94-8762c44b8646@gmail.com>


  [this is not my question; it's posted on behalf of someone who wants
to remain anonymous ...]

I am testing the effect of a treatment to reduce bycatch in fishing
nets. Note the the design uses paired nets (control vs experiment)
soaked simultaneously but of different length (limited budget did no
allow to have an experimental net as long as control net).

The dependent variable are counts (no. individuals entangled), and I
have fishing effort and treatment (control vs experiment) as independent
variables. Since bycatch events were rare , the dataset is zero inflated
and positive catches are usually of 1 individual, therefore we switched
to a binomial model to test the probability of catching an individual
where if the catch is zero then probability =0, but if the catch is >0
then probability is a 1.
We used this model to predict bycatch probability in control and
experimental nets by setting fishing effort = 1.

There is an issue being raised, that Fishing effort being significantly
higher for control than experimental nets, the binomial model can yield
biased estimates of treatment and overestimate treatment efficiency.

I thought that including Effort as a fixed effect in the model would
mean that the model takes into account the difference in effort when
predicting the bycatch probability. Is that true?
However, I am not entirely sure HOW the glmer function does it and I
would like to know your opinion about the issue being raised."


From th|erry@onke||nx @end|ng |rom |nbo@be  Fri Jan 17 22:41:33 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Fri, 17 Jan 2020 22:41:33 +0100
Subject: [R-sig-ME] incorporating effort as an effect in binomial GLMM
In-Reply-To: <773643a4-28b7-5f81-4e94-8762c44b8646@gmail.com>
References: <773643a4-28b7-5f81-4e94-8762c44b8646@gmail.com>
Message-ID: <CAJuCY5zHjh4JDGDbFTZ8QNnhQ05BN=ooJkNDQKGZ7=Cof4DcCg@mail.gmail.com>

Dear Anonymous,

Here a few ideas

How did you check for zero-inflation? A lot of zero's does not imply
zero-inflation. E.g. table(rpois(1e6, lambda = 0.01)) has lots of zero's
but no zero-inflation. I'd recommend using a Poisson distribution. Then
check for zero-inflation by comparing the distribution of the number of
zero's from several datasets simulated based on the model with the observed
number of zero's.

The logit-link complicates the interpretation of the fishing effort in the
binomial model. I suggest using a Poisson model with log(length) of the
nets as a fixed effect to the model to correct from fishing effort. Then
you can get predictions in terms of number per unit length the net.

Best regards,


ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op vr 17 jan. 2020 om 22:01 schreef Ben Bolker <bbolker at gmail.com>:

>
>   [this is not my question; it's posted on behalf of someone who wants
> to remain anonymous ...]
>
> I am testing the effect of a treatment to reduce bycatch in fishing
> nets. Note the the design uses paired nets (control vs experiment)
> soaked simultaneously but of different length (limited budget did no
> allow to have an experimental net as long as control net).
>
> The dependent variable are counts (no. individuals entangled), and I
> have fishing effort and treatment (control vs experiment) as independent
> variables. Since bycatch events were rare , the dataset is zero inflated
> and positive catches are usually of 1 individual, therefore we switched
> to a binomial model to test the probability of catching an individual
> where if the catch is zero then probability =0, but if the catch is >0
> then probability is a 1.
> We used this model to predict bycatch probability in control and
> experimental nets by setting fishing effort = 1.
>
> There is an issue being raised, that Fishing effort being significantly
> higher for control than experimental nets, the binomial model can yield
> biased estimates of treatment and overestimate treatment efficiency.
>
> I thought that including Effort as a fixed effect in the model would
> mean that the model takes into account the difference in effort when
> predicting the bycatch probability. Is that true?
> However, I am not entirely sure HOW the glmer function does it and I
> would like to know your opinion about the issue being raised."
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From @|te@@ed@c2 @end|ng |rom gm@||@com  Sun Jan 19 14:29:54 2020
From: @|te@@ed@c2 @end|ng |rom gm@||@com (C. AMAL D. GLELE)
Date: Sun, 19 Jan 2020 13:29:54 +0000
Subject: [R-sig-ME] Problem_Downdated VtV is not positive definite
Message-ID: <CANrzCv0JTGCDDrJ0eRuVQGOKpdtMLvAy3XPBWNotJfkD4nECOw@mail.gmail.com>

Hello, everyone.
In the models below, mydata has 32 rows
village and Year are grouping factor variables with respectively 8 and 4
levels.
1)
model1.1 fits well
model1.1<-glmer(cbind(F,L)~village+Year+(1|Year)+(1|village),
family="binomial",data=mydata)
But, when I replace village+Year by village*Year, I get the warning and
error message below
model1.2<-glmer(cbind(F,L)~village*Year+(1|Year)+(1|village),
family="binomial",data=mydata)
Warning:
fixed-effect model matrix is rank deficient so dropping 2 columns /
coefficients
Error message:
Error in (function (fr, X, reTrms, family, nAGQ = 1L, verbose = 0L, maxit =
100L,  :
  Downdated VtV is not positive definite
I noticed a similar behavior when trying with glmmadmb:
model2.1 fits well
model2.1<-glmmadmb(cbind(F,L)~village+Year+(1|Year)+(1|village),family="binomial",data=na.omit

(mydata))
But, with village*Year, Iget the error message below:
model2.2<-glmmadmb(cbind(F,L)~village*Year+(1|Year)+(1|village),family="binomial",data=na.omit

(mydata))
Error:
Error in glmmadmb(cbind(F, L) ~ village * Year + (1 | Year) + (1 |
village),  :
  rank of X = 30 < ncol(X) = 32
##
in advance, thanks for helping solve these issues(I'm very interested in
interaction terms).
Kinds regards,

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Mon Jan 20 01:46:02 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sun, 19 Jan 2020 19:46:02 -0500
Subject: [R-sig-ME] Problem_Downdated VtV is not positive definite
In-Reply-To: <CANrzCv0JTGCDDrJ0eRuVQGOKpdtMLvAy3XPBWNotJfkD4nECOw@mail.gmail.com>
References: <CANrzCv0JTGCDDrJ0eRuVQGOKpdtMLvAy3XPBWNotJfkD4nECOw@mail.gmail.com>
Message-ID: <b102d218-f1b4-1628-7e73-a34664dc55b4@gmail.com>



On 2020-01-19 8:29 a.m., C. AMAL D. GLELE wrote:
> Hello, everyone.
> In the models below, mydata has 32 rows
> village and Year are grouping factor variables with respectively 8 and 4
> levels.
> 1)
> model1.1 fits well
> model1.1<-glmer(cbind(F,L)~village+Year+(1|Year)+(1|village),
> family="binomial",data=mydata)
> But, when I replace village+Year by village*Year, I get the warning and
> error message below
> model1.2<-glmer(cbind(F,L)~village*Year+(1|Year)+(1|village),
> family="binomial",data=mydata)
> Warning:
> fixed-effect model matrix is rank deficient so dropping 2 columns /
> coefficients
> Error message:
> Error in (function (fr, X, reTrms, family, nAGQ = 1L, verbose = 0L, maxit =
> 100L,  :
>   Downdated VtV is not positive definite

   When you include a fixed effect of village*Year, it expands to (1 +
village + Year + village:Year).

  Assuming Year is numeric, the combination of a fixed effect of Year
(treated as the overall slope of a linear relationship) and a random
intercept of Year (Year treated as a categorical grouping variable) is OK.

  But ... you also have village as categorical fixed effect *and* a
random intercept, which is redundant.

   You could use  ~ Year + village:Year  + (1|Year) + (1|village) *or*
                  ~ Year + (1+Year|village)



> I noticed a similar behavior when trying with glmmadmb:
> model2.1 fits well
> model2.1<-glmmadmb(cbind(F,L)~village+Year+(1|Year)+(1|village),family="binomial",data=na.omit
> 
> (mydata))
> But, with village*Year, Iget the error message below:
> model2.2<-glmmadmb(cbind(F,L)~village*Year+(1|Year)+(1|village),family="binomial",data=na.omit
> 
> (mydata))
> Error:
> Error in glmmadmb(cbind(F, L) ~ village * Year + (1 | Year) + (1 |
> village),  :
>   rank of X = 30 < ncol(X) = 32
> ##
> in advance, thanks for helping solve these issues(I'm very interested in
> interaction terms).
> Kinds regards,
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From @|te@@ed@c2 @end|ng |rom gm@||@com  Tue Jan 21 10:05:31 2020
From: @|te@@ed@c2 @end|ng |rom gm@||@com (C. AMAL D. GLELE)
Date: Tue, 21 Jan 2020 09:05:31 +0000
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 157, Issue 14
In-Reply-To: <mailman.18017.7.1579519054.43057.r-sig-mixed-models@r-project.org>
References: <mailman.18017.7.1579519054.43057.r-sig-mixed-models@r-project.org>
Message-ID: <CANrzCv2+9eSQVZ1rSTiNXy+rnuSy2Ebm=WF4j27mg+6YwMqoRg@mail.gmail.com>

Dear Ben,
thank you so much.
Best,

Le lun. 20 janv. 2020 ? 11:19, <r-sig-mixed-models-request at r-project.org> a
?crit :

> Send R-sig-mixed-models mailing list submissions to
>         r-sig-mixed-models at r-project.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> or, via email, send a message with subject or body 'help' to
>         r-sig-mixed-models-request at r-project.org
>
> You can reach the person managing the list at
>         r-sig-mixed-models-owner at r-project.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of R-sig-mixed-models digest..."
>
>
> Today's Topics:
>
>    1. Problem_Downdated VtV is not positive definite (C. AMAL D. GLELE)
>    2. Re: Problem_Downdated VtV is not positive definite (Ben Bolker)
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Sun, 19 Jan 2020 13:29:54 +0000
> From: "C. AMAL D. GLELE" <altessedac2 at gmail.com>
> To: R SIG Mixed Models <r-sig-mixed-models at r-project.org>
> Subject: [R-sig-ME] Problem_Downdated VtV is not positive definite
> Message-ID:
>         <
> CANrzCv0JTGCDDrJ0eRuVQGOKpdtMLvAy3XPBWNotJfkD4nECOw at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> Hello, everyone.
> In the models below, mydata has 32 rows
> village and Year are grouping factor variables with respectively 8 and 4
> levels.
> 1)
> model1.1 fits well
> model1.1<-glmer(cbind(F,L)~village+Year+(1|Year)+(1|village),
> family="binomial",data=mydata)
> But, when I replace village+Year by village*Year, I get the warning and
> error message below
> model1.2<-glmer(cbind(F,L)~village*Year+(1|Year)+(1|village),
> family="binomial",data=mydata)
> Warning:
> fixed-effect model matrix is rank deficient so dropping 2 columns /
> coefficients
> Error message:
> Error in (function (fr, X, reTrms, family, nAGQ = 1L, verbose = 0L, maxit =
> 100L,  :
>   Downdated VtV is not positive definite
> I noticed a similar behavior when trying with glmmadmb:
> model2.1 fits well
>
> model2.1<-glmmadmb(cbind(F,L)~village+Year+(1|Year)+(1|village),family="binomial",data=na.omit
>
> (mydata))
> But, with village*Year, Iget the error message below:
>
> model2.2<-glmmadmb(cbind(F,L)~village*Year+(1|Year)+(1|village),family="binomial",data=na.omit
>
> (mydata))
> Error:
> Error in glmmadmb(cbind(F, L) ~ village * Year + (1 | Year) + (1 |
> village),  :
>   rank of X = 30 < ncol(X) = 32
> ##
> in advance, thanks for helping solve these issues(I'm very interested in
> interaction terms).
> Kinds regards,
>
>         [[alternative HTML version deleted]]
>
>
>
>
> ------------------------------
>
> Message: 2
> Date: Sun, 19 Jan 2020 19:46:02 -0500
> From: Ben Bolker <bbolker at gmail.com>
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Problem_Downdated VtV is not positive definite
> Message-ID: <b102d218-f1b4-1628-7e73-a34664dc55b4 at gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
>
>
> On 2020-01-19 8:29 a.m., C. AMAL D. GLELE wrote:
> > Hello, everyone.
> > In the models below, mydata has 32 rows
> > village and Year are grouping factor variables with respectively 8 and 4
> > levels.
> > 1)
> > model1.1 fits well
> > model1.1<-glmer(cbind(F,L)~village+Year+(1|Year)+(1|village),
> > family="binomial",data=mydata)
> > But, when I replace village+Year by village*Year, I get the warning and
> > error message below
> > model1.2<-glmer(cbind(F,L)~village*Year+(1|Year)+(1|village),
> > family="binomial",data=mydata)
> > Warning:
> > fixed-effect model matrix is rank deficient so dropping 2 columns /
> > coefficients
> > Error message:
> > Error in (function (fr, X, reTrms, family, nAGQ = 1L, verbose = 0L,
> maxit =
> > 100L,  :
> >   Downdated VtV is not positive definite
>
>    When you include a fixed effect of village*Year, it expands to (1 +
> village + Year + village:Year).
>
>   Assuming Year is numeric, the combination of a fixed effect of Year
> (treated as the overall slope of a linear relationship) and a random
> intercept of Year (Year treated as a categorical grouping variable) is OK.
>
>   But ... you also have village as categorical fixed effect *and* a
> random intercept, which is redundant.
>
>    You could use  ~ Year + village:Year  + (1|Year) + (1|village) *or*
>                   ~ Year + (1+Year|village)
>
>
>
> > I noticed a similar behavior when trying with glmmadmb:
> > model2.1 fits well
> >
> model2.1<-glmmadmb(cbind(F,L)~village+Year+(1|Year)+(1|village),family="binomial",data=na.omit
> >
> > (mydata))
> > But, with village*Year, Iget the error message below:
> >
> model2.2<-glmmadmb(cbind(F,L)~village*Year+(1|Year)+(1|village),family="binomial",data=na.omit
> >
> > (mydata))
> > Error:
> > Error in glmmadmb(cbind(F, L) ~ village * Year + (1 | Year) + (1 |
> > village),  :
> >   rank of X = 30 < ncol(X) = 32
> > ##
> > in advance, thanks for helping solve these issues(I'm very interested in
> > interaction terms).
> > Kinds regards,
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
>
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> R-sig-mixed-models mailing list
> R-sig-mixed-models at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
> ------------------------------
>
> End of R-sig-mixed-models Digest, Vol 157, Issue 14
> ***************************************************
>

	[[alternative HTML version deleted]]


From @ouhey|@@ghebghoub @end|ng |rom gm@||@com  Wed Jan 22 14:54:36 2020
From: @ouhey|@@ghebghoub @end|ng |rom gm@||@com (Souheyla GHEBGHOUB)
Date: Wed, 22 Jan 2020 13:54:36 +0000
Subject: [R-sig-ME] sample size in glmer model
Message-ID: <CAEA998jmgNrazCsOm18B-VWz0b6p1k70gKf7f8JZ=qCzXYo6mQ@mail.gmail.com>

Hi,
In Field (2012) we need 10-15 *participants* per variable for regression,
in Levshina (2015) we need 10-15 *observations* per variable.

Is it participant or observation? I am so confused as I have 53
participants and 1484 observations.

Thank you,
Sou
PhD in Education
University of York

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Wed Jan 22 15:44:56 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Wed, 22 Jan 2020 15:44:56 +0100
Subject: [R-sig-ME] sample size in glmer model
In-Reply-To: <CAEA998jmgNrazCsOm18B-VWz0b6p1k70gKf7f8JZ=qCzXYo6mQ@mail.gmail.com>
References: <CAEA998jmgNrazCsOm18B-VWz0b6p1k70gKf7f8JZ=qCzXYo6mQ@mail.gmail.com>
Message-ID: <CAJuCY5zZARGyQDZtU_YmufyV+jW4jvYx8y2UCNFxHyb7OVdjVw@mail.gmail.com>

Dear Sou,

I'd suggest at least 10-15 observation per **parameter**. Categorical
variables with more than two levels require more than one parameter. An
interaction between two continuous variable requires 3 parameters (2 main
effect + 1 interaction). Don't forget to count the hyperparameters of the
random effects.

Best regards,

Thierry

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op wo 22 jan. 2020 om 14:55 schreef Souheyla GHEBGHOUB <
souheyla.ghebghoub at gmail.com>:

> Hi,
> In Field (2012) we need 10-15 *participants* per variable for regression,
> in Levshina (2015) we need 10-15 *observations* per variable.
>
> Is it participant or observation? I am so confused as I have 53
> participants and 1484 observations.
>
> Thank you,
> Sou
> PhD in Education
> University of York
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Wed Jan 22 17:21:02 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 22 Jan 2020 11:21:02 -0500
Subject: [R-sig-ME] sample size in glmer model
In-Reply-To: <CAJuCY5zZARGyQDZtU_YmufyV+jW4jvYx8y2UCNFxHyb7OVdjVw@mail.gmail.com>
References: <CAEA998jmgNrazCsOm18B-VWz0b6p1k70gKf7f8JZ=qCzXYo6mQ@mail.gmail.com>
 <CAJuCY5zZARGyQDZtU_YmufyV+jW4jvYx8y2UCNFxHyb7OVdjVw@mail.gmail.com>
Message-ID: <CABghstQH8KXEUnPvs19OuL6YZi=ZhPncU81YJ1p9TPtyPVxC_Q@mail.gmail.com>

  It's going to depend on whether the parameters you're trying to
estimate are capturing processes that vary at the level of
participants or at the level of observations.  For example, if you
wanted to compare trends over time among groups of participants, each
measured multiple times (i.e. a random-slopes model with a
fixed-effect interaction between treatment and slope) you'd need to
consider the number of participants. (In a sense the answer to this
question goes back to the classical experimental design bestiaries of
nested vs randomized block vs split-plot vs ... and deciding what the
"denominator degrees of freedom" are supposed to be in each case/for
each test of interest).

On Wed, Jan 22, 2020 at 9:45 AM Thierry Onkelinx via
R-sig-mixed-models <r-sig-mixed-models at r-project.org> wrote:
>
> Dear Sou,
>
> I'd suggest at least 10-15 observation per **parameter**. Categorical
> variables with more than two levels require more than one parameter. An
> interaction between two continuous variable requires 3 parameters (2 main
> effect + 1 interaction). Don't forget to count the hyperparameters of the
> random effects.
>
> Best regards,
>
> Thierry
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op wo 22 jan. 2020 om 14:55 schreef Souheyla GHEBGHOUB <
> souheyla.ghebghoub at gmail.com>:
>
> > Hi,
> > In Field (2012) we need 10-15 *participants* per variable for regression,
> > in Levshina (2015) we need 10-15 *observations* per variable.
> >
> > Is it participant or observation? I am so confused as I have 53
> > participants and 1484 observations.
> >
> > Thank you,
> > Sou
> > PhD in Education
> > University of York
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From odd|ty|ee @end|ng |rom gm@||@com  Wed Jan 22 20:01:26 2020
From: odd|ty|ee @end|ng |rom gm@||@com (Xia Li)
Date: Wed, 22 Jan 2020 11:01:26 -0800
Subject: [R-sig-ME] Need help on convergence issue when fitting
 zero-inflated Poisson with random coefficients using gamlss
Message-ID: <CANo7qN2p0AH3d4XKHLb1CP+w65AfgQDPROvqJ-CN27W5UVL2kw@mail.gmail.com>

Hello,

I have trouble getting convergence when fitting ZIP with (individual level)
random coefficients, the function that I used:

m1 <- gamlss(y ~ re(fixed = ~ treatment, random = ~ treatment|unit_id),
             family = ZIP, data = dat)

Specifically I wanted to include global or population level treatment
effect, and individual random effect nested under treatment.

I always got the following error messages:

Error in lme.formula(fixed = fix.formula, data = Data, random = random, :
   nlminb problem, convergence error code = 1
   message = iteration limit reached without convergence (10)

Is there anything that I can debug from? Changing the control parameters? I
tried different algorithms like method = CG() but seems it did not help
much.

Looking for help. Thanks!

-- 
Best,
Xia

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Wed Jan 22 22:13:25 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 22 Jan 2020 16:13:25 -0500
Subject: [R-sig-ME] sample size in glmer model
In-Reply-To: <CAEA998js=mW4gqf_6zEg=OjVh=6asKPzn-LPdp2yz8XQuouC1g@mail.gmail.com>
References: <CAEA998jmgNrazCsOm18B-VWz0b6p1k70gKf7f8JZ=qCzXYo6mQ@mail.gmail.com>
 <CAJuCY5zZARGyQDZtU_YmufyV+jW4jvYx8y2UCNFxHyb7OVdjVw@mail.gmail.com>
 <CABghstQH8KXEUnPvs19OuL6YZi=ZhPncU81YJ1p9TPtyPVxC_Q@mail.gmail.com>
 <CAEA998js=mW4gqf_6zEg=OjVh=6asKPzn-LPdp2yz8XQuouC1g@mail.gmail.com>
Message-ID: <CABghstTi-+fAGpez9cdqB4Ef2wHsaZM1of58SQhqqVZWBtriEQ@mail.gmail.com>

[please keep r-sig-mixed-models in the Cc: list when replying]

   What is V1, what level does it vary at (among vs within
participants), and what are you interested in comparing/testing with
this model?

   Note that Barr et al 2013's "keep it maximal" protocol would
suggest that you fit (item|Participant) (but ... without
factor-analytic models or some form of regularization this will never
actually be practical, as you'd be trying to estimate a 28x28
covariance matrix (= 378 parameters) from 53 participants ...  you
usually have to choose from some more restricted set of choices
[independence, compound symmetry, etc.]

On Wed, Jan 22, 2020 at 12:36 PM Souheyla GHEBGHOUB
<souheyla.ghebghoub at gmail.com> wrote:
>
> Hi Ben,
>
> I am not comparing between participants, it is a within group design, each participant answered 28 items
> My model is glmer ( score~ V1 + (1|Participant) + (1|item)  ??
>
> Thank you,
> Sou
>
> On Wed, 22 Jan 2020 at 16:21, Ben Bolker <bbolker at gmail.com> wrote:
>>
>>   It's going to depend on whether the parameters you're trying to
>> estimate are capturing processes that vary at the level of
>> participants or at the level of observations.  For example, if you
>> wanted to compare trends over time among groups of participants, each
>> measured multiple times (i.e. a random-slopes model with a
>> fixed-effect interaction between treatment and slope) you'd need to
>> consider the number of participants. (In a sense the answer to this
>> question goes back to the classical experimental design bestiaries of
>> nested vs randomized block vs split-plot vs ... and deciding what the
>> "denominator degrees of freedom" are supposed to be in each case/for
>> each test of interest).
>>
>> On Wed, Jan 22, 2020 at 9:45 AM Thierry Onkelinx via
>> R-sig-mixed-models <r-sig-mixed-models at r-project.org> wrote:
>> >
>> > Dear Sou,
>> >
>> > I'd suggest at least 10-15 observation per **parameter**. Categorical
>> > variables with more than two levels require more than one parameter. An
>> > interaction between two continuous variable requires 3 parameters (2 main
>> > effect + 1 interaction). Don't forget to count the hyperparameters of the
>> > random effects.
>> >
>> > Best regards,
>> >
>> > Thierry
>> >
>> > ir. Thierry Onkelinx
>> > Statisticus / Statistician
>> >
>> > Vlaamse Overheid / Government of Flanders
>> > INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
>> > FOREST
>> > Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>> > thierry.onkelinx at inbo.be
>> > Havenlaan 88 bus 73, 1000 Brussel
>> > www.inbo.be
>> >
>> > ///////////////////////////////////////////////////////////////////////////////////////////
>> > To call in the statistician after the experiment is done may be no more
>> > than asking him to perform a post-mortem examination: he may be able to say
>> > what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> > The plural of anecdote is not data. ~ Roger Brinner
>> > The combination of some data and an aching desire for an answer does not
>> > ensure that a reasonable answer can be extracted from a given body of data.
>> > ~ John Tukey
>> > ///////////////////////////////////////////////////////////////////////////////////////////
>> >
>> > <https://www.inbo.be>
>> >
>> >
>> > Op wo 22 jan. 2020 om 14:55 schreef Souheyla GHEBGHOUB <
>> > souheyla.ghebghoub at gmail.com>:
>> >
>> > > Hi,
>> > > In Field (2012) we need 10-15 *participants* per variable for regression,
>> > > in Levshina (2015) we need 10-15 *observations* per variable.
>> > >
>> > > Is it participant or observation? I am so confused as I have 53
>> > > participants and 1484 observations.
>> > >
>> > > Thank you,
>> > > Sou
>> > > PhD in Education
>> > > University of York
>> > >
>> > >         [[alternative HTML version deleted]]
>> > >
>> > > _______________________________________________
>> > > R-sig-mixed-models at r-project.org mailing list
>> > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> > >
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From @ouhey|@@ghebghoub @end|ng |rom gm@||@com  Thu Jan 23 11:15:02 2020
From: @ouhey|@@ghebghoub @end|ng |rom gm@||@com (Souheyla GHEBGHOUB)
Date: Thu, 23 Jan 2020 10:15:02 +0000
Subject: [R-sig-ME] sample size in glmer model
In-Reply-To: <CABghstTi-+fAGpez9cdqB4Ef2wHsaZM1of58SQhqqVZWBtriEQ@mail.gmail.com>
References: <CAEA998jmgNrazCsOm18B-VWz0b6p1k70gKf7f8JZ=qCzXYo6mQ@mail.gmail.com>
 <CAJuCY5zZARGyQDZtU_YmufyV+jW4jvYx8y2UCNFxHyb7OVdjVw@mail.gmail.com>
 <CABghstQH8KXEUnPvs19OuL6YZi=ZhPncU81YJ1p9TPtyPVxC_Q@mail.gmail.com>
 <CAEA998js=mW4gqf_6zEg=OjVh=6asKPzn-LPdp2yz8XQuouC1g@mail.gmail.com>
 <CABghstTi-+fAGpez9cdqB4Ef2wHsaZM1of58SQhqqVZWBtriEQ@mail.gmail.com>
Message-ID: <CAEA998gvTpZbywbv-iZni5s8o7+UVJwHvz1nOEV0yc_kMqNBdQ@mail.gmail.com>

Morning Ben,

I am not doing (item|Participant).
V1 is a duration (continuous) , I should have an interaction of Time of two
levels  :  score ~ V1*Time + (Time|Participant) + (Time|item) ,
This means I have 7 parameters including intercept (4 random effects +
intercept + V1 + TimeLevel1).
I have 1484 observations and 53 participants.  The rule is 10 per parameter.

My question: Is it 10 participants per parameter (i.e. I am allowed 5
parameters) or 10 observation per parameter (I am allowed 148 parameters!!!)
I guess it is likely per participant, and if its the case, then should I
still report results even the power is less (53 divided by 7 = I have 7.5
participants per parameter, not 10 as recommended)

Thats all
Thank you,
Souheyla

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Thu Jan 23 11:39:05 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Thu, 23 Jan 2020 11:39:05 +0100
Subject: [R-sig-ME] sample size in glmer model
In-Reply-To: <CAEA998gvTpZbywbv-iZni5s8o7+UVJwHvz1nOEV0yc_kMqNBdQ@mail.gmail.com>
References: <CAEA998jmgNrazCsOm18B-VWz0b6p1k70gKf7f8JZ=qCzXYo6mQ@mail.gmail.com>
 <CAJuCY5zZARGyQDZtU_YmufyV+jW4jvYx8y2UCNFxHyb7OVdjVw@mail.gmail.com>
 <CABghstQH8KXEUnPvs19OuL6YZi=ZhPncU81YJ1p9TPtyPVxC_Q@mail.gmail.com>
 <CAEA998js=mW4gqf_6zEg=OjVh=6asKPzn-LPdp2yz8XQuouC1g@mail.gmail.com>
 <CABghstTi-+fAGpez9cdqB4Ef2wHsaZM1of58SQhqqVZWBtriEQ@mail.gmail.com>
 <CAEA998gvTpZbywbv-iZni5s8o7+UVJwHvz1nOEV0yc_kMqNBdQ@mail.gmail.com>
Message-ID: <CAJuCY5wjFFVi1_MwZ=e9eZZ0MFnYrnTg906d6tkrSEUsjpvkng@mail.gmail.com>

Dear Souheyla,

Your fixed effects need 4 parameters: intercept, V1, TimeLevel2 and
V1:TimeLevel2
(Time|Participant) needs 3 parameters: variance of TimeLevel1, variance of
TimeLevel2 and their covariance.
The same goes for (Time|Item).
So you're using 10 parameters.

(Time|Participant) tries to estimate the difference between two times for
each participant. Such random effect requires much more information than
(1|Participant). In this case I'd recommended that you have 10 or more
observations for the majority of the Time/Participant combinations.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op do 23 jan. 2020 om 11:15 schreef Souheyla GHEBGHOUB <
souheyla.ghebghoub at gmail.com>:

> Morning Ben,
>
> I am not doing (item|Participant).
> V1 is a duration (continuous) , I should have an interaction of Time of two
> levels  :  score ~ V1*Time + (Time|Participant) + (Time|item) ,
> This means I have 7 parameters including intercept (4 random effects +
> intercept + V1 + TimeLevel1).
> I have 1484 observations and 53 participants.  The rule is 10 per
> parameter.
>
> My question: Is it 10 participants per parameter (i.e. I am allowed 5
> parameters) or 10 observation per parameter (I am allowed 148
> parameters!!!)
> I guess it is likely per participant, and if its the case, then should I
> still report results even the power is less (53 divided by 7 = I have 7.5
> participants per parameter, not 10 as recommended)
>
> Thats all
> Thank you,
> Souheyla
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From @ouhey|@@ghebghoub @end|ng |rom gm@||@com  Thu Jan 23 12:03:22 2020
From: @ouhey|@@ghebghoub @end|ng |rom gm@||@com (Souheyla GHEBGHOUB)
Date: Thu, 23 Jan 2020 11:03:22 +0000
Subject: [R-sig-ME] sample size in glmer model
In-Reply-To: <CAJuCY5wjFFVi1_MwZ=e9eZZ0MFnYrnTg906d6tkrSEUsjpvkng@mail.gmail.com>
References: <CAEA998jmgNrazCsOm18B-VWz0b6p1k70gKf7f8JZ=qCzXYo6mQ@mail.gmail.com>
 <CAJuCY5zZARGyQDZtU_YmufyV+jW4jvYx8y2UCNFxHyb7OVdjVw@mail.gmail.com>
 <CABghstQH8KXEUnPvs19OuL6YZi=ZhPncU81YJ1p9TPtyPVxC_Q@mail.gmail.com>
 <CAEA998js=mW4gqf_6zEg=OjVh=6asKPzn-LPdp2yz8XQuouC1g@mail.gmail.com>
 <CABghstTi-+fAGpez9cdqB4Ef2wHsaZM1of58SQhqqVZWBtriEQ@mail.gmail.com>
 <CAEA998gvTpZbywbv-iZni5s8o7+UVJwHvz1nOEV0yc_kMqNBdQ@mail.gmail.com>
 <CAJuCY5wjFFVi1_MwZ=e9eZZ0MFnYrnTg906d6tkrSEUsjpvkng@mail.gmail.com>
Message-ID: <CAEA998g8CpV2RXW1D5pTGWk1DvWQFYWus8M=uN5r6z0w0OCdFg@mail.gmail.com>

Thank you Thierry,

I was mistaken I have 2968 observations (2 Time ? 53 participants ? 28
items).

Is that enough for 7 parameters?
Do I have room for more interactions ( i would later do a backward
step-wise regression).

Thank you,
Souheyla

On Thu, 23 Jan 2020 at 10:39, Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> Dear Souheyla,
>
> Your fixed effects need 4 parameters: intercept, V1, TimeLevel2 and
> V1:TimeLevel2
> (Time|Participant) needs 3 parameters: variance of TimeLevel1, variance of
> TimeLevel2 and their covariance.
> The same goes for (Time|Item).
> So you're using 10 parameters.
>
> (Time|Participant) tries to estimate the difference between two times for
> each participant. Such random effect requires much more information than
> (1|Participant). In this case I'd recommended that you have 10 or more
> observations for the majority of the Time/Participant combinations.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op do 23 jan. 2020 om 11:15 schreef Souheyla GHEBGHOUB <
> souheyla.ghebghoub at gmail.com>:
>
>> Morning Ben,
>>
>> I am not doing (item|Participant).
>> V1 is a duration (continuous) , I should have an interaction of Time of
>> two
>> levels  :  score ~ V1*Time + (Time|Participant) + (Time|item) ,
>> This means I have 7 parameters including intercept (4 random effects +
>> intercept + V1 + TimeLevel1).
>> I have 1484 observations and 53 participants.  The rule is 10 per
>> parameter.
>>
>> My question: Is it 10 participants per parameter (i.e. I am allowed 5
>> parameters) or 10 observation per parameter (I am allowed 148
>> parameters!!!)
>> I guess it is likely per participant, and if its the case, then should I
>> still report results even the power is less (53 divided by 7 = I have 7.5
>> participants per parameter, not 10 as recommended)
>>
>> Thats all
>> Thank you,
>> Souheyla
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From ke@tre|1978 @end|ng |rom gm@||@com  Thu Jan 23 18:24:28 2020
From: ke@tre|1978 @end|ng |rom gm@||@com (Ronny Steen)
Date: Thu, 23 Jan 2020 18:24:28 +0100
Subject: [R-sig-ME] Ordered logistic regression with random effects (clmm2)
Message-ID: <CABVupNYNCdO0ZfUgutTeR9HC1g=epaXhf525Cj4ArU5jCi+qmA@mail.gmail.com>

Dear all,


I try to do a ordered logistic regression with random effects. I have tried
clmm2 from package 'ordinal'  and manage to run the test, but can not
figure out how to get det predictions and confidence intervals.


We have a small "cafeteria experiment" to check if a bird has dietary
preferences, indicated by free-choice feeding from boxes with food of
different qualities.


More details about the setup:

We have offered Jays three choices of acorns (ordered quality) to see if
they have preferences. Unfortunately we do not have control of their
identity, but we have monitored 5 different places (which is included as
random effect). In the experiment we have video recorded behaviour and
we offered the Jays with 3 boxes with acorns (one box with 5 *intact *acorns,
one with 5 acorns with *holes* and one with 5 *damaged* acorns).  Hence
intact acorns is of highest quality, below is acorns with holes ("infected"
by insect larvae) and lowest is damaged acorns (eaten at by a rodent). Our
hypothesis is that the Jay should prefer the intact acorns since they will
last longer for storage.


The data:


> head(Jay)
  State  Choice Place Intact_count Hole_count Damaged_count
1     0 Damaged    B1            5          5             2
2     0 Damaged    B1            5          5             1
3     2  Intact    B1            5          5             0
4     2  Intact    B1            4          5             0
5     2  Intact    B1            3          5             0
6     2  Intact    B1            2          5             0

> str(Jay)
'data.frame': 318 obs. of  6 variables:
 $ State        : Ord.factor w/ 3 levels "0"<"1"<"2": 1 1 3 3 3 3 3 2 2 2
...
 $ Choice       : Factor w/ 3 levels "Damaged","Holes",..: 1 1 3 3 3 3 3 2
2 2 ...
 $ Place        : Factor w/ 5 levels "B1","E1","E2",..: 1 1 1 1 1 1 1 1 1 1
...
 $ Intact_count : int  5 5 5 4 3 2 1 0 0 0 ...
 $ Hole_count   : int  5 5 5 5 5 5 5 5 4 3 ...
 $ Damaged_count: int  2 1 0 0 0 0 0 0 0 0 ...

State is the ordered Choice (Damaged = 0, Hole = 1 and Intact = 2), Place
is the random effect.  Intact_count, Hole_count and Damaged_count is
numeric and gives the information about how many acorns that are in each
box. At the beginning of the day it was 5 acorns in each box, and less as
the Jay collected acorns.


The best model seems to be:
fm3 <- clmm2(State~ Intact_count*Hole_count, random=Place, data=Jay,
             Hess=TRUE, nAGQ=10)


> (fm3)
Cumulative Link Mixed Model fitted with the adaptive Gauss-Hermite
quadrature approximation with 10 quadrature points

Call:
clmm2(location = State ~ Intact_count * Hole_count, random = Place,
    data = Jay, Hess = TRUE, nAGQ = 10)

Random effects:
               Var      Std.Dev
Place 9.486096e-09 9.739659e-05

Location coefficients:
           Intact_count              Hole_count Intact_count:Hole_count
              1.6015787               0.8237153              -0.3237785

No Scale coefficients

Threshold coefficients:
     0|1      1|2
1.458473 3.826715

log-likelihood: -258.4659
AIC: 528.9318


I would like to get predicted probabilities with confidence intervals. What
is the probability of choosing a intact vs. holes vs. damaged. How does
this vary with Intact_count, Hole_count (e.g. Intact_count = 1, 3 or 5,
Hole_count = 1, 3, 5)?


Best regards,

Ron

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Thu Jan 23 20:28:55 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Thu, 23 Jan 2020 14:28:55 -0500
Subject: [R-sig-ME] Need help on convergence issue when fitting
 zero-inflated Poisson with random coefficients using gamlss
In-Reply-To: <CANo7qN2p0AH3d4XKHLb1CP+w65AfgQDPROvqJ-CN27W5UVL2kw@mail.gmail.com>
References: <CANo7qN2p0AH3d4XKHLb1CP+w65AfgQDPROvqJ-CN27W5UVL2kw@mail.gmail.com>
Message-ID: <CABghstThAh5assG2OZQwdW+xZJ=HbV437WAwFrkGaSUm8zbE_g@mail.gmail.com>

    Since the proximal problem looks like (guessing from the error
messages) a call to the nlminb optimizer within a call to lme is
running out of iterations before converging, I'd try to see if there
is a way to set the number of parameters higher.  Unfortunately, I
don't know how to do this off the top of my head (a brief review of
the gamlss documentation didn't get me anywhere, and I haven't had
time to dig through the code to see if/how this is possible) ...

   There are other packages that are capable of fitting mixed ZIP
models (e.g. https://journal.r-project.org/archive/2017/RJ-2017-066/index.html
 shows examples using glmmTMB, inla, MCMCglmm, gamlss, ...) in case
you can't solve the problem with gamlss (and you aren't stuck using
gamlss for other reasons)

  cheers
   Ben Bolker


On Wed, Jan 22, 2020 at 2:01 PM Xia Li <odditylee at gmail.com> wrote:
>
> Hello,
>
> I have trouble getting convergence when fitting ZIP with (individual level)
> random coefficients, the function that I used:
>
> m1 <- gamlss(y ~ re(fixed = ~ treatment, random = ~ treatment|unit_id),
>              family = ZIP, data = dat)
>
> Specifically I wanted to include global or population level treatment
> effect, and individual random effect nested under treatment.
>
> I always got the following error messages:
>
> Error in lme.formula(fixed = fix.formula, data = Data, random = random, :
>    nlminb problem, convergence error code = 1
>    message = iteration limit reached without convergence (10)
>
> Is there anything that I can debug from? Changing the control parameters? I
> tried different algorithms like method = CG() but seems it did not help
> much.
>
> Looking for help. Thanks!
>
> --
> Best,
> Xia
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From d@|uedecke @end|ng |rom uke@de  Fri Jan 24 19:46:48 2020
From: d@|uedecke @end|ng |rom uke@de (=?iso-8859-1?Q?Daniel_L=FCdecke?=)
Date: Fri, 24 Jan 2020 19:46:48 +0100
Subject: [R-sig-ME] 
 Ordered logistic regression with random effects (clmm2)
In-Reply-To: <CABVupNYNCdO0ZfUgutTeR9HC1g=epaXhf525Cj4ArU5jCi+qmA@mail.gmail.com>
References: <CABVupNYNCdO0ZfUgutTeR9HC1g=epaXhf525Cj4ArU5jCi+qmA@mail.gmail.com>
Message-ID: <000001d5d2e6$a30d5a50$e9280ef0$@uke.de>

Hi Ron,

If you could use "clmm()" instead of "clmm2()", then you could use the
ggeffects-package to calculate predicted probabilities, and either print or
plot the results. Here's an example:

library(ggeffects)
library(ordinal)
model <- clmm(SURENESS ~ PROD * COLD + (1|RESP), data = soup)
pred <- ggpredict(model, c("PROD", "COLD"))

pred
plot(pred)

You can find a comprehensive documentation for the package here:
https://strengejacke.github.io/ggeffects/

Best
Daniel


-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im
Auftrag von Ronny Steen
Gesendet: Donnerstag, 23. Januar 2020 18:24
An: r-sig-mixed-models at r-project.org
Betreff: [R-sig-ME] Ordered logistic regression with random effects (clmm2)

Dear all,


I try to do a ordered logistic regression with random effects. I have tried
clmm2 from package 'ordinal'  and manage to run the test, but can not
figure out how to get det predictions and confidence intervals.


We have a small "cafeteria experiment" to check if a bird has dietary
preferences, indicated by free-choice feeding from boxes with food of
different qualities.


More details about the setup:

We have offered Jays three choices of acorns (ordered quality) to see if
they have preferences. Unfortunately we do not have control of their
identity, but we have monitored 5 different places (which is included as
random effect). In the experiment we have video recorded behaviour and
we offered the Jays with 3 boxes with acorns (one box with 5 *intact
*acorns,
one with 5 acorns with *holes* and one with 5 *damaged* acorns).  Hence
intact acorns is of highest quality, below is acorns with holes ("infected"
by insect larvae) and lowest is damaged acorns (eaten at by a rodent). Our
hypothesis is that the Jay should prefer the intact acorns since they will
last longer for storage.


The data:


> head(Jay)
  State  Choice Place Intact_count Hole_count Damaged_count
1     0 Damaged    B1            5          5             2
2     0 Damaged    B1            5          5             1
3     2  Intact    B1            5          5             0
4     2  Intact    B1            4          5             0
5     2  Intact    B1            3          5             0
6     2  Intact    B1            2          5             0

> str(Jay)
'data.frame': 318 obs. of  6 variables:
 $ State        : Ord.factor w/ 3 levels "0"<"1"<"2": 1 1 3 3 3 3 3 2 2 2
...
 $ Choice       : Factor w/ 3 levels "Damaged","Holes",..: 1 1 3 3 3 3 3 2
2 2 ...
 $ Place        : Factor w/ 5 levels "B1","E1","E2",..: 1 1 1 1 1 1 1 1 1 1
...
 $ Intact_count : int  5 5 5 4 3 2 1 0 0 0 ...
 $ Hole_count   : int  5 5 5 5 5 5 5 5 4 3 ...
 $ Damaged_count: int  2 1 0 0 0 0 0 0 0 0 ...

State is the ordered Choice (Damaged = 0, Hole = 1 and Intact = 2), Place
is the random effect.  Intact_count, Hole_count and Damaged_count is
numeric and gives the information about how many acorns that are in each
box. At the beginning of the day it was 5 acorns in each box, and less as
the Jay collected acorns.


The best model seems to be:
fm3 <- clmm2(State~ Intact_count*Hole_count, random=Place, data=Jay,
             Hess=TRUE, nAGQ=10)


> (fm3)
Cumulative Link Mixed Model fitted with the adaptive Gauss-Hermite
quadrature approximation with 10 quadrature points

Call:
clmm2(location = State ~ Intact_count * Hole_count, random = Place,
    data = Jay, Hess = TRUE, nAGQ = 10)

Random effects:
               Var      Std.Dev
Place 9.486096e-09 9.739659e-05

Location coefficients:
           Intact_count              Hole_count Intact_count:Hole_count
              1.6015787               0.8237153              -0.3237785

No Scale coefficients

Threshold coefficients:
     0|1      1|2
1.458473 3.826715

log-likelihood: -258.4659
AIC: 528.9318


I would like to get predicted probabilities with confidence intervals. What
is the probability of choosing a intact vs. holes vs. damaged. How does
this vary with Intact_count, Hole_count (e.g. Intact_count = 1, 3 or 5,
Hole_count = 1, 3, 5)?


Best regards,

Ron

	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING


From j@de@ @end|ng |rom uc@d@edu  Tue Jan 28 01:58:04 2020
From: j@de@ @end|ng |rom uc@d@edu (Ades, James)
Date: Tue, 28 Jan 2020 00:58:04 +0000
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 155, Issue 3
In-Reply-To: <mailman.17935.7.1572778801.27763.r-sig-mixed-models@r-project.org>
References: <mailman.17935.7.1572778801.27763.r-sig-mixed-models@r-project.org>
Message-ID: <8A07D164-DBA6-482E-B5A5-37947F466322@UCSD.edu>

Chia-yu,

I think you need to read through some of the information for lmer and mixed-modeling.

In your second model you have sex as a random slope, varying across patient (which isn?t right, maybe it would be if every single patient were transgender). Sex should be a fixed factor, with age, and drug doses. It looks like ?case? is merely a place holder, and since you have patient as a random factor, you?ve taken care of the repeated measures. That said, case could also be used as a timepoint indicator, in which case it?s fine as a fixed factor (or you could make that a random slope? (Case | Patient), but I don?t know the importance of time in your study.  You could do stepwise, where you sequentially add the fixed factors, though in doing so, you should also probably look into possible interactions of sex on drug and age on drug. I?m not sure the specifics of Drug A and B; it seems in one case, they?re administered at the same time. So maybe it?s best to pick out the most important interactions, since interactions can become unwieldy pretty quickly.

value ~ Age * Sex * DrugA * DrugB + (Case | Patient) would be an interaction on all terms?which might make sense if all IVs are significant. That said, good look reading that.

Test with the likelihood ratio test using ANOVA(m1, m2), though if you have absences for patients then your LRT will have different observations, and it won?t work. 

Best, 

James


> On Nov 3, 2019, at 3:00 AM, r-sig-mixed-models-request at r-project.org wrote:
> 
> Send R-sig-mixed-models mailing list submissions to
> 	r-sig-mixed-models at r-project.org
> 
> To subscribe or unsubscribe via the World Wide Web, visit
> 	https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> or, via email, send a message with subject or body 'help' to
> 	r-sig-mixed-models-request at r-project.org
> 
> You can reach the person managing the list at
> 	r-sig-mixed-models-owner at r-project.org
> 
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of R-sig-mixed-models digest..."
> 
> 
> Today's Topics:
> 
>   1. Doubtful significance in mixed effect model (Chia-Yu Chen)
>   2. Re: Your response to my R-sig-ME question (Lenth, Russell V)
> 
> ----------------------------------------------------------------------
> 
> Message: 1
> Date: Sat, 2 Nov 2019 19:40:15 +0100
> From: Chia-Yu Chen <jessica821112 at gmail.com>
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Doubtful significance in mixed effect model
> Message-ID: <78B7183F-D5C1-4622-BA8A-B1852C5AD8AB at gmail.com>
> Content-Type: text/plain; charset="utf-8"
> 
> Hi,
> 
> I have a problem on the significance of age and sex when running glmer on my longitudinal data. 
> 
> My data
> A longitudinal data where each patient is tested at 3 timepoints (here, define as ?case?). There are different treatments between cases. Along with ?case?, other factors include age, sex and drug dosages. So it looked something like this (there are 23 patients, each has 3 cases)
> 
> Patient   Case   Age   Sex   DrugA   DrugB   Value
>    1            1        10      0        5           10         20
>    1            2        10      0       10           0          30
>    1            3        10      0       15           0          55
> 
> What I want to do
> The goal of this study is to show that ?value? is significantly different across ?cases?. Age, sex, drugA, drugB are all potential confounders. Here I want to see if either of these factors has confounding effects, that is, whether adding these factors to the model will be better or not.
> 
> How I did it
> First, I constructed 2 nested models, and then I compared the 2 models with likelihood test. If m2 is better than m1, then I assume this factor has significance for value. Since it?s a longitudinal data, the ?patient? is treated as random factor. I ran through the factors one by one, here take ?sex? for example:
> 
> m1 <- lme4::glmer(data = subdata, formula =  value ~  Case + (1 | Patient))
> m2 <- lme4::glmer(data = subdata, formula =  value ~  Case + (1 | Patient) + ( Sex | Patient))
> p_value  <- lmtest::lrtest (m1, m2)$"Pr(>Chisq)"[2]
> 
> My Question
> I expected that m2 shouldn?t be better than m1 for sex and age, because for each patient they didn?t change over 3 cases. I thought by specifying "( Sex | Patient)? in the model would tell R that sex doesn?t change for each patient, and thus it doesn?t have any predictive ability for the value. However, lrtest showed that for some patients, m2 is better than m1, meaning that age or sex is significant. I?m wondering is there anything wrong in my codes? Doesn?t ( Sex | Patient) tell R that sex doesn?t change for each patient? How should I code so that m2 won?t be better than m1 for sex and age? Or is there any better way doing this?
> I?ve tried many combinations of the code, but I still can?t solve this problem. Could anyone give me some advices? Any suggestion is appreciated! Thank you in advance.
> 
> Best,
> Chia-Yu
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> 
> ------------------------------
> 
> Message: 2
> Date: Sat, 2 Nov 2019 22:25:37 +0000
> From: "Lenth, Russell V" <russell-lenth at uiowa.edu>
> To: Francesco Romano <fbromano77 at gmail.com>
> Cc: "r-sig-mixed-models at r-project.org"
> 	<r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] Your response to my R-sig-ME question
> Message-ID:
> 	<DM6PR04MB438084D57E44BFBA85B23DD7F17D0 at DM6PR04MB4380.namprd04.prod.outlook.com>
> 	
> Content-Type: text/plain; charset="iso-8859-1"
> 
> If the question is how to do multiplicity adjustments for multiple sets of comparisons in 'emmeans', that is pretty simple to do. For example, starting with
> 
>    library("emmeans")
>    emm <- emmeans(model, ~ factor1 | factor2)
> 
> By default,
> 
>    pairs(emm, by = "factor1")
> 
> will apply the Tukey adjustment to the pairwise comparisons of factor2 for each level of factor1, SEPARATELY. If instead, you want to multiplicity-adjust all of those simple comparisons as one family, summarize those results after removing the 'by' variable:
> 
>    summary(pairs(emm, by = "factor1"), by = NULL, adjust = "mvt")
> 
> (Note that the Tukey adjustment is not appropriate for that family because it is not ONE set of pairwise comparisons. The mvt adjustment is the same adjustment that the multcomp package appluies by default.)
> 
> You of course may also want simple comparisons of factor1 for each level of factor2; just reverse the roles of the two factors in the above.
> 
> If you want to combine both of those families into a single family consisting of all simple comparisons of both factor2|factor1 and factor1|factor2, that can be done as well via 'rbind':
> 
>    allcmps <- pairs(emm, simple = "each")   # creates a list of two emmGrid objects
>    summary(do.call(rbind, allcmps), adjust = "mvt")
> 
> I hope that helps
> 
> Russ
> 
> Russell V. Lenth  -  Professor Emeritus
> Department of Statistics and Actuarial Science   
> The University of Iowa  -  Iowa City, IA 52242  USA   
> Voice (319)335-0712 (Dept. office)  -  FAX (319)335-3017
> 
> 
> 
> -----Original Message-----
> 
> Date: Fri, 1 Nov 2019 21:00:03 -0400
> From: Ben Bolker <bbolker at gmail.com>
> To: Francesco Romano <fbromano77 at gmail.com>
> Cc: "r-sig-mixed-models at r-project.org"
> 	<r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] Your response to my R-sig-ME question
> Message-ID: <c5952d23-0ab9-e201-f775-5c90d89f5dff at gmail.com>
> Content-Type: text/plain; charset="utf-8"
> 
> 
>  [cc'ing r-sig-mixed-models]
> 
>  Honestly, it looks to me like you *do* need multiple-comparisons corrections here. I can't give you detailed advice about how to do it; emmeans does the pairwise comparisons, but it's not immediately obvious how to do correction for *multiple* sets of pairwise comparisons.
> (Perhaps you could get away with only doing the corrections at the level of sets of pairwise comparisons.)  As I mentioned before, this is not a particularly mixed-model-related question.  You could try CrossValidated (https://stats.stackexchange.com).  The emmeans and multcomp packages will probably be what you need in terms of machinery.
> 
>  sincerely
>   Ben Bolker
> 
> 
> 
> 
> ------------------------------
> 
> Subject: Digest Footer
> 
> _______________________________________________
> R-sig-mixed-models mailing list
> R-sig-mixed-models at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> ------------------------------
> 
> End of R-sig-mixed-models Digest, Vol 155, Issue 3
> **************************************************


From biii m@iii@g oii de@@ey@ws  Wed Jan 29 21:20:40 2020
From: biii m@iii@g oii de@@ey@ws (biii m@iii@g oii de@@ey@ws)
Date: Wed, 29 Jan 2020 15:20:40 -0500
Subject: [R-sig-ME] Error with nlme and varFixed
Message-ID: <053001d5d6e1$93deb750$bb9c25f0$@denney.ws>

Hello,

 

When trying to fit an NLME model using a vector of varFixed, I get an error
that appears to be related to a multiplication issue at this line of code:

https://github.com/cran/nlme/blob/c006dfa23ad390948a74e67978a9828a6d60d89b/R
/varFunc.R#L169

 

Is the below a bug or am I inaccurately applying varFixed (or something
else)?

 

Here is a reproducible example:

 

library(nlme)

 

d <-

  data.frame(

    obs=rnorm(n=97), # 97 chosen because it's prime and therefore can't be
the size of a rectangular matrix

    groups=rep(c("A", "B"), each=50)[1:97],

    wt=abs(rnorm(n=97))

  )

 

nlme(

  obs~b,

  fixed=b~1,

  random=b~1|groups,

  weights=varFixed(~wt),

 start=c(b=0),

  data=d

)

 

Which gets the error:

 

Error in recalc.varFunc(object[[i]], conLin) : 

  dims [product 12] do not match the length of object [97]

In addition: Warning message:

In conLin$Xy * varWeights(object) :

  longer object length is not a multiple of shorter object length

 

Thanks,

 

Bill


	[[alternative HTML version deleted]]


From wdenney @end|ng |rom hum@npred|ct|on@@com  Wed Jan 29 21:17:21 2020
From: wdenney @end|ng |rom hum@npred|ct|on@@com (Bill Denney)
Date: Wed, 29 Jan 2020 15:17:21 -0500
Subject: [R-sig-ME] Error with nlme and varFixed
Message-ID: <e8dcd82ce7d84dad7405a1f4fc6627be@mail.gmail.com>

Hello,



When trying to fit an NLME model using a vector of varFixed, I get an error
that appears to be related to a multiplication issue at this line of code:

https://github.com/cran/nlme/blob/c006dfa23ad390948a74e67978a9828a6d60d89b/R/varFunc.R#L169



Is the below a bug or am I inaccurately applying varFixed (or something
else)?



Here is a reproducible example:



library(nlme)



d <-

  data.frame(

    obs=rnorm(n=97), # 97 chosen because it's prime and therefore can't be
the size of a rectangular matrix

    groups=rep(c("A", "B"), each=50)[1:97],

    wt=abs(rnorm(n=97))

  )



nlme(

  obs~b,

  fixed=b~1,

  random=b~1|groups,

  weights=varFixed(~wt),

 start=c(b=0),

  data=d

)



Which gets the error:



Error in recalc.varFunc(object[[i]], conLin) :

  dims [product 12] do not match the length of object [97]

In addition: Warning message:

In conLin$Xy * varWeights(object) :

  longer object length is not a multiple of shorter object length



Thanks,



Bill

	[[alternative HTML version deleted]]


From ch|r|eu @end|ng |rom gm@||@com  Sat Feb  1 19:57:29 2020
From: ch|r|eu @end|ng |rom gm@||@com (=?UTF-8?Q?David_Villegas_R=C3=ADos?=)
Date: Sat, 1 Feb 2020 19:57:29 +0100
Subject: [R-sig-ME] bam model selection with 3 million data
Message-ID: <CALC46t_EROEaaJbTXuO1WGNbvr3H2h+pgRZWwNDWVe-zL5QrSw@mail.gmail.com>

Dear list,

I?m investigating the effect of three variables (X, Y, Z) on the
probability that an animal uses a particular habitat A. I have a time
series of relocations for each animal (>300 individuals), with one
relocation every 30 minutes. There are only two options for the response
variable: 1=present in habitat A, 0=not present in habitat A. The effects
of the three variables are expected to be non-linear so I?m using gam
models. My dataset is very large, with >3 million data points so I?m using
the bam function from the mgcv library in R. In my models I include a
random effect ?individual ID?, and a temporal autocorrelation term that
corrects much but not all of the autocorrelation in the models.

*Question 1.*

When I run a model with the three main effects (X, Y, Z) and the three
double interactions (X:Y, X:Z, Y:Z), I get that all terms are highly
significant, except for one interaction. If I remove it, then everything is
highly significant. However, I also wanted to run simpler models with only
one interaction, no interactions, only two main effects and only one main
effect. Then, if I compare all these models with AIC or BIC, I get that the
best model (by far) is the one with only main effects.

>
    AIC(codcoaAR2,codcoaAR2.1,codcoaAR2.2,codcoaAR2.3,codcoaAR2.4,codcoaAR2.5,codcoaAR2.6,codcoaAR2.7,codcoaAR2.8,codcoaAR2.9,codcoaAR)

                  df      AIC

codcoaAR2   306.1310 -1442543

codcoaAR2.1 293.1608 -1440642

codcoaAR2.2 292.9615 -1438219

codcoaAR2.3 294.3657 -1435346

codcoaAR2.4 284.0026 -1434286

codcoaAR2.5 280.3472 -1396765

codcoaAR2.6 279.6380 -1435862

codcoaAR2.7 269.4968 -1377806

codcoaAR2.8 269.0480 -1393897

codcoaAR2.9 281.8584 -1214270

codcoaAR    271.7066 -2353481  # model with only main effects



I wonder how this is possible if two of the interactions are highly
significant.

So my underlying question is: *for a model like this in which sample size
is huge, should I make model selection looking at the significance of the
different terms in the model, or should I rather look at AIC/BIC?*

*Question 2.*

Let?s assume the model with only main effects is indeed the optimal one.
Then I?d like to get the effect size of each explanatory variable. It?s not
clear to me how to do it even after reading some post on this and other
forums, but I tried to figure it out by sequentially running the model
without one explanatory variable at a time, and then comparing the deviance
explained in the optimal model with X, Y, Z with the deviance explained
with the reduced model with only Y and Z, for instance. Assuming that the
difference would the variance explained by X. *Is this correct? *Looking at
the results, the deviance explained by each variable X, Y, Z is quite low,
but if the three main effects explain so little variance, who is explaining
the rest?

Model

Deviance explained

X, Z, Y

69.3%

Y, Z

68.5%

X, Z

69.3%

X, Y

60.5%



*Question 3.*

In my models I usually get this error message:

Warning message:

In bgam.fitd(G, mf, gp, scale, nobs.extra = 0, rho = rho, coef = coef,  :

  fitted probabilities numerically 0 or 1 occurred



which seems to indicate that there is perfect separation in my logistic
regression. I?m not sure this is the case in my data, how could I check it
and correct for it if needed? Should it be always corrected?



Thanks for your help,

David

	[[alternative HTML version deleted]]


From c@c@voeten @end|ng |rom hum@|e|denun|v@n|  Sat Feb  1 20:39:08 2020
From: c@c@voeten @end|ng |rom hum@|e|denun|v@n| (Voeten, C.C.)
Date: Sat, 1 Feb 2020 19:39:08 +0000
Subject: [R-sig-ME] bam model selection with 3 million data
In-Reply-To: <CALC46t_EROEaaJbTXuO1WGNbvr3H2h+pgRZWwNDWVe-zL5QrSw@mail.gmail.com>
References: <CALC46t_EROEaaJbTXuO1WGNbvr3H2h+pgRZWwNDWVe-zL5QrSw@mail.gmail.com>
Message-ID: <24acbe45c2624637b108748d39f9c291@hum.leidenuniv.nl>

Hi David,

1) You cannot perform likelihood-based model comparisons with bam models, or -- for completeness' sake -- with gam models that were fitted using performance iteration or the EFS optimizer. All of these are based on PQL (penalized quasi-likelihood), which makes the log-likelihood (and hence LRT, AIC, BIC, etc) invalid for comparison purposes. See Wood (2017:149-151). gam() with the default outer iteration should be fine, though. Have you tried fitting your full model using bam with the select=TRUE argument to turn on mgcv's automatic smooth-term selection?

2) I am unsure if the deviance explained is or is not suitable for indicating effect size, so I can't comment on this question. I might, however, have an alternative suggestion: have you considered partial eta squared or partial omega squared? You should be able to calculate those based on the ANOVA table.

3) I agree with you that the warning suggests complete separation, but in my experience this doesn't automatically have to be a problem. Have you checked the summary for extremely large beta values, and also have you run gam.check() to see if your fit looks reasonable? If neither indicates a problem I wouldn't be too concerned about it.

Hope this helps,

Cesko

P.S.: please send messages in plain text only, as you can see the formatting of your message was slightly screwed up because the mailing list automatically strips HTML markup

-----Oorspronkelijk bericht-----
Van: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Namens David Villegas R?os
Verzonden: zaterdag 1 februari 2020 19:57
Aan: r-sig-mixed-models <r-sig-mixed-models at r-project.org>
Onderwerp: [R-sig-ME] bam model selection with 3 million data

Dear list,

I?m investigating the effect of three variables (X, Y, Z) on the probability that an animal uses a particular habitat A. I have a time series of relocations for each animal (>300 individuals), with one relocation every 30 minutes. There are only two options for the response
variable: 1=present in habitat A, 0=not present in habitat A. The effects of the three variables are expected to be non-linear so I?m using gam models. My dataset is very large, with >3 million data points so I?m using the bam function from the mgcv library in R. In my models I include a random effect ?individual ID?, and a temporal autocorrelation term that corrects much but not all of the autocorrelation in the models.

*Question 1.*

When I run a model with the three main effects (X, Y, Z) and the three double interactions (X:Y, X:Z, Y:Z), I get that all terms are highly significant, except for one interaction. If I remove it, then everything is highly significant. However, I also wanted to run simpler models with only one interaction, no interactions, only two main effects and only one main effect. Then, if I compare all these models with AIC or BIC, I get that the best model (by far) is the one with only main effects.

>
    AIC(codcoaAR2,codcoaAR2.1,codcoaAR2.2,codcoaAR2.3,codcoaAR2.4,codcoaAR2.5,codcoaAR2.6,codcoaAR2.7,codcoaAR2.8,codcoaAR2.9,codcoaAR)

                  df      AIC

codcoaAR2   306.1310 -1442543

codcoaAR2.1 293.1608 -1440642

codcoaAR2.2 292.9615 -1438219

codcoaAR2.3 294.3657 -1435346

codcoaAR2.4 284.0026 -1434286

codcoaAR2.5 280.3472 -1396765

codcoaAR2.6 279.6380 -1435862

codcoaAR2.7 269.4968 -1377806

codcoaAR2.8 269.0480 -1393897

codcoaAR2.9 281.8584 -1214270

codcoaAR    271.7066 -2353481  # model with only main effects



I wonder how this is possible if two of the interactions are highly significant.

So my underlying question is: *for a model like this in which sample size is huge, should I make model selection looking at the significance of the different terms in the model, or should I rather look at AIC/BIC?*

*Question 2.*

Let?s assume the model with only main effects is indeed the optimal one.
Then I?d like to get the effect size of each explanatory variable. It?s not clear to me how to do it even after reading some post on this and other forums, but I tried to figure it out by sequentially running the model without one explanatory variable at a time, and then comparing the deviance explained in the optimal model with X, Y, Z with the deviance explained with the reduced model with only Y and Z, for instance. Assuming that the difference would the variance explained by X. *Is this correct? *Looking at the results, the deviance explained by each variable X, Y, Z is quite low, but if the three main effects explain so little variance, who is explaining the rest?

Model

Deviance explained

X, Z, Y

69.3%

Y, Z

68.5%

X, Z

69.3%

X, Y

60.5%



*Question 3.*

In my models I usually get this error message:

Warning message:

In bgam.fitd(G, mf, gp, scale, nobs.extra = 0, rho = rho, coef = coef,  :

  fitted probabilities numerically 0 or 1 occurred



which seems to indicate that there is perfect separation in my logistic regression. I?m not sure this is the case in my data, how could I check it and correct for it if needed? Should it be always corrected?



Thanks for your help,

David

	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From yc2975 @end|ng |rom co|umb|@@edu  Sun Feb  2 22:08:36 2020
From: yc2975 @end|ng |rom co|umb|@@edu (Yi-Ru Cheng)
Date: Sun, 2 Feb 2020 16:08:36 -0500
Subject: [R-sig-ME] Need help on zero-inflated beta model and post-hoc test
Message-ID: <CAOmDo73Fv5Ew0ZtqXUQhTjv5N==Jty=aEYfdM5n2ULQ_OVj6og@mail.gmail.com>

Hi, everyone


I would like to compare the relatedness of individuals among plots. Since
relatedness is bound between 0 and 1 and most of them are zeros, I am
trying to fit with the zero-inflated beta model in the gamlss and glmmTMB
packages. After fitting the model, I want to perform the post-hoc test
between different plots. However, I have some conundrums here.


1.  gamlss

The beta model in the package gamlss gives a pretty good residual plot, but
the post-hoc tests in emmeans and multcomp don't work with the model...Here
is my model.



m1 <- gamlss(r ~ plot + random(year), family = BEINF, data = df)

emmeans(m1, pairwise ~ plot, type="response")

hist(resid(m1))


 "Error in emm_basis.gamlss(object, trms, xlev, grid, misc = attr(data,  :

  gamlss models with smoothing are not yet supported in 'emmeans'"



2. glmmTMB

I then tried the beta family in glmmTMB. For some reason, the simple
residual plot looks pretty skewed, but it looks alright with the scaled
residual plot in DHARMa. Otherwise, the post-hoc test in emmeas works fine.



m2 <- glmmTMB(r ~ plot + (1|year), ziformula =~ 1,  data=df,
family=beta_family(link="logit"))

emmeans(m2, pairwise ~ plot, type="response")

hist(resid(m2))  #skewed

DHARMa::simulateResiduals(m2)  ? no significant deviation



I?m not quite sure why the residual plots give different patterns in two
models.  Is it not correct to look at the residual distribution for
diagnosis? Could I see it?s a green light to use the model in glmmTMB based
on the simulated residuals in the DHAMRMa package even though the simple
residual plot looks skewed? Or is there any other post-hoc test designed
for gamlss with the smoothing process? Any suggestions would be
appreciated. Thanks in advance!


Best,

Yiru

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Mon Feb  3 01:57:06 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sun, 2 Feb 2020 19:57:06 -0500
Subject: [R-sig-ME] 
 Need help on zero-inflated beta model and post-hoc test
In-Reply-To: <CAOmDo73Fv5Ew0ZtqXUQhTjv5N==Jty=aEYfdM5n2ULQ_OVj6og@mail.gmail.com>
References: <CAOmDo73Fv5Ew0ZtqXUQhTjv5N==Jty=aEYfdM5n2ULQ_OVj6og@mail.gmail.com>
Message-ID: <e7a36b0d-339b-5564-065c-c529d7084c0d@gmail.com>



On 2020-02-02 4:08 p.m., Yi-Ru Cheng wrote:
> Hi, everyone
> 
> 
> I would like to compare the relatedness of individuals among plots. Since
> relatedness is bound between 0 and 1 and most of them are zeros, I am
> trying to fit with the zero-inflated beta model in the gamlss and glmmTMB
> packages. After fitting the model, I want to perform the post-hoc test
> between different plots. However, I have some conundrums here.
> 
> 
> 1.  gamlss
> 
> The beta model in the package gamlss gives a pretty good residual plot, but
> the post-hoc tests in emmeans and multcomp don't work with the model...Here
> is my model.
> 
> 
> 
> m1 <- gamlss(r ~ plot + random(year), family = BEINF, data = df)
> 
> emmeans(m1, pairwise ~ plot, type="response")
> 
> hist(resid(m1))
> 
> 
>  "Error in emm_basis.gamlss(object, trms, xlev, grid, misc = attr(data,  :
> 
>   gamlss models with smoothing are not yet supported in 'emmeans'"
> 
> 
> 
> 2. glmmTMB
> 
> I then tried the beta family in glmmTMB. For some reason, the simple
> residual plot looks pretty skewed, but it looks alright with the scaled
> residual plot in DHARMa. Otherwise, the post-hoc test in emmeas works fine.
> 
> m2 <- glmmTMB(r ~ plot + (1|year), ziformula =~ 1,  data=df,
> family=beta_family(link="logit"))
> 
> emmeans(m2, pairwise ~ plot, type="response")
> 
> hist(resid(m2))  #skewed
> 
> DHARMa::simulateResiduals(m2)  ? no significant deviation

   Quick answer: I don't think there's any reason to expect the
residuals of a Beta GLMM *not* to be skewed.  Try simulating a simple
example (you may even be able to use simulate() on your fitted model) to
see for yourself.  DHARMa does a plot of expected vs actual
distributions of residuals, not a raw plot of residuals.


> 
> 
> 
> I?m not quite sure why the residual plots give different patterns in two
> models.  Is it not correct to look at the residual distribution for
> diagnosis? Could I see it?s a green light to use the model in glmmTMB based
> on the simulated residuals in the DHAMRMa package even though the simple
> residual plot looks skewed? Or is there any other post-hoc test designed
> for gamlss with the smoothing process? Any suggestions would be
> appreciated. Thanks in advance!
> 
> 
> Best,
> 
> Yiru
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From c@c@voeten @end|ng |rom hum@|e|denun|v@n|  Mon Feb  3 13:09:07 2020
From: c@c@voeten @end|ng |rom hum@|e|denun|v@n| (Voeten, C.C.)
Date: Mon, 3 Feb 2020 12:09:07 +0000
Subject: [R-sig-ME] bam model selection with 3 million data
In-Reply-To: <CALC46t_VkdG8hGBLjZxZPsWVr6f+W_1ta3usUCUVevUXgit07w@mail.gmail.com>
References: <CALC46t_EROEaaJbTXuO1WGNbvr3H2h+pgRZWwNDWVe-zL5QrSw@mail.gmail.com>
 <24acbe45c2624637b108748d39f9c291@hum.leidenuniv.nl>
 <CALC46t_VkdG8hGBLjZxZPsWVr6f+W_1ta3usUCUVevUXgit07w@mail.gmail.com>
Message-ID: <b58aab128ee04900a040b9b0bad01410@hum.leidenuniv.nl>

Hi David,

Please keep the mailing list in cc.

Yes, gam() can indeed be prohibitively slow, hence why I suggested the select=TRUE approach. I hope the below explanation helps.

Smooth terms are composed of a null space and a range space, which respectively represent the completely smooth part of the effect and the wiggly part of the effect. In the mathematical representation, these are equivalent to fixed effects and random effects, respectively. As such, the range space is subject to penalization and can shrink to zero. In GAM terms, this would correspond to a smoothing parameter tending to infinity, resulting in a completely smooth effect: you would be left only with the null space. What select=TRUE does is add an additional penalty to all null spaces, so that these too can shrink towards zero. Effects which are shrunk to (near) zero in this way can then be removed from the model by the user.

The degree of shrinkage can be seen by looking at the edf. Smooths that are (near) zero will also use (near) zero edf. In practice, when I do model selection using select=TRUE, I always remove terms whose edf < 1. Then I fit the model again with select=TRUE to ensure that no further reduction is necessary, and then I fit a final model with select=FALSE.

Best,
Cesko

-----Oorspronkelijk bericht-----
Van: David Villegas R?os <chirleu at gmail.com> 
Verzonden: maandag 3 februari 2020 11:28
Aan: Voeten, C.C. <c.c.voeten at hum.leidenuniv.nl>
Onderwerp: Re: [R-sig-ME] bam model selection with 3 million data

Thanks Cesko, really appreciate your answer.
In my particular case however, I cannot run gam models (they take forever to run) so I?m still struggling on how to perform model selection in bam. I tried select=TRUE and the summary changed a bit, but I don?t really know what this argument is doing behind?the scenes, or whether I should trust the summary from the model using select=TRUE rather than the default select=FALSE.
Best wishes,?
David

El s?b., 1 feb. 2020 a las 20:39, Voeten, C.C. (<mailto:c.c.voeten at hum.leidenuniv.nl>) escribi?:
Hi David,

1) You cannot perform likelihood-based model comparisons with bam models, or -- for completeness' sake -- with gam models that were fitted using performance iteration or the EFS optimizer. All of these are based on PQL (penalized quasi-likelihood), which makes the log-likelihood (and hence LRT, AIC, BIC, etc) invalid for comparison purposes. See Wood (2017:149-151). gam() with the default outer iteration should be fine, though. Have you tried fitting your full model using bam with the select=TRUE argument to turn on mgcv's automatic smooth-term selection?

2) I am unsure if the deviance explained is or is not suitable for indicating effect size, so I can't comment on this question. I might, however, have an alternative suggestion: have you considered partial eta squared or partial omega squared? You should be able to calculate those based on the ANOVA table.

3) I agree with you that the warning suggests complete separation, but in my experience this doesn't automatically have to be a problem. Have you checked the summary for extremely large beta values, and also have you run gam.check() to see if your fit looks reasonable? If neither indicates a problem I wouldn't be too concerned about it.

Hope this helps,

Cesko

P.S.: please send messages in plain text only, as you can see the formatting of your message was slightly screwed up because the mailing list automatically strips HTML markup

-----Oorspronkelijk bericht-----
Van: R-sig-mixed-models <mailto:r-sig-mixed-models-bounces at r-project.org> Namens David Villegas R?os
Verzonden: zaterdag 1 februari 2020 19:57
Aan: r-sig-mixed-models <mailto:r-sig-mixed-models at r-project.org>
Onderwerp: [R-sig-ME] bam model selection with 3 million data

Dear list,

I?m investigating the effect of three variables (X, Y, Z) on the probability that an animal uses a particular habitat A. I have a time series of relocations for each animal (>300 individuals), with one relocation every 30 minutes. There are only two options for the response
variable: 1=present in habitat A, 0=not present in habitat A. The effects of the three variables are expected to be non-linear so I?m using gam models. My dataset is very large, with >3 million data points so I?m using the bam function from the mgcv library in R. In my models I include a random effect ?individual ID?, and a temporal autocorrelation term that corrects much but not all of the autocorrelation in the models.

*Question 1.*

When I run a model with the three main effects (X, Y, Z) and the three double interactions (X:Y, X:Z, Y:Z), I get that all terms are highly significant, except for one interaction. If I remove it, then everything is highly significant. However, I also wanted to run simpler models with only one interaction, no interactions, only two main effects and only one main effect. Then, if I compare all these models with AIC or BIC, I get that the best model (by far) is the one with only main effects.

>
? ? AIC(codcoaAR2,codcoaAR2.1,codcoaAR2.2,codcoaAR2.3,codcoaAR2.4,codcoaAR2.5,codcoaAR2.6,codcoaAR2.7,codcoaAR2.8,codcoaAR2.9,codcoaAR)

? ? ? ? ? ? ? ? ? df? ? ? AIC

codcoaAR2? ?306.1310 -1442543

codcoaAR2.1 293.1608 -1440642

codcoaAR2.2 292.9615 -1438219

codcoaAR2.3 294.3657 -1435346

codcoaAR2.4 284.0026 -1434286

codcoaAR2.5 280.3472 -1396765

codcoaAR2.6 279.6380 -1435862

codcoaAR2.7 269.4968 -1377806

codcoaAR2.8 269.0480 -1393897

codcoaAR2.9 281.8584 -1214270

codcoaAR? ? 271.7066 -2353481? # model with only main effects



I wonder how this is possible if two of the interactions are highly significant.

So my underlying question is: *for a model like this in which sample size is huge, should I make model selection looking at the significance of the different terms in the model, or should I rather look at AIC/BIC?*

*Question 2.*

Let?s assume the model with only main effects is indeed the optimal one.
Then I?d like to get the effect size of each explanatory variable. It?s not clear to me how to do it even after reading some post on this and other forums, but I tried to figure it out by sequentially running the model without one explanatory variable at a time, and then comparing the deviance explained in the optimal model with X, Y, Z with the deviance explained with the reduced model with only Y and Z, for instance. Assuming that the difference would the variance explained by X. *Is this correct? *Looking at the results, the deviance explained by each variable X, Y, Z is quite low, but if the three main effects explain so little variance, who is explaining the rest?

Model

Deviance explained

X, Z, Y

69.3%

Y, Z

68.5%

X, Z

69.3%

X, Y

60.5%



*Question 3.*

In my models I usually get this error message:

Warning message:

In bgam.fitd(G, mf, gp, scale, nobs.extra = 0, rho = rho, coef = coef,? :

? fitted probabilities numerically 0 or 1 occurred



which seems to indicate that there is perfect separation in my logistic regression. I?m not sure this is the case in my data, how could I check it and correct for it if needed? Should it be always corrected?



Thanks for your help,

David

? ? ? ? [[alternative HTML version deleted]]

_______________________________________________
mailto:R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From yc2975 @end|ng |rom co|umb|@@edu  Mon Feb  3 22:04:50 2020
From: yc2975 @end|ng |rom co|umb|@@edu (Yiru Cheng)
Date: Mon, 3 Feb 2020 16:04:50 -0500
Subject: [R-sig-ME] 
 Need help on zero-inflated beta model and post-hoc test
In-Reply-To: <mailman.18036.7.1580727602.35357.r-sig-mixed-models@r-project.org>
References: <mailman.18036.7.1580727602.35357.r-sig-mixed-models@r-project.org>
Message-ID: <CANX20fTg8izNQb+8iXuZY5=G58NmZhQCF-yqbW2y6HQH_GtLZg@mail.gmail.com>

Hi, Ben

Thanks for your clarification! The raw residuals for the simulated data
don't look normal as you said. Since the expected and observed residuals
have no sig. deviation, I guess I can go ahead and do the post-hoc test
using this model.

Yiru


>
>
> Message: 1
> Date: Sun, 2 Feb 2020 16:08:36 -0500
> From: Yi-Ru Cheng <yc2975 at columbia.edu>
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Need help on zero-inflated beta model and post-hoc
>         test
> Message-ID:
>         <CAOmDo73Fv5Ew0ZtqXUQhTjv5N==Jty=
> aEYfdM5n2ULQ_OVj6og at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> Hi, everyone
>
>
> I would like to compare the relatedness of individuals among plots. Since
> relatedness is bound between 0 and 1 and most of them are zeros, I am
> trying to fit with the zero-inflated beta model in the gamlss and glmmTMB
> packages. After fitting the model, I want to perform the post-hoc test
> between different plots. However, I have some conundrums here.
>
>
> 1.  gamlss
>
> The beta model in the package gamlss gives a pretty good residual plot, but
> the post-hoc tests in emmeans and multcomp don't work with the model...Here
> is my model.
>
>
>
> m1 <- gamlss(r ~ plot + random(year), family = BEINF, data = df)
>
> emmeans(m1, pairwise ~ plot, type="response")
>
> hist(resid(m1))
>
>
>  "Error in emm_basis.gamlss(object, trms, xlev, grid, misc = attr(data,  :
>
>   gamlss models with smoothing are not yet supported in 'emmeans'"
>
>
>
> 2. glmmTMB
>
> I then tried the beta family in glmmTMB. For some reason, the simple
> residual plot looks pretty skewed, but it looks alright with the scaled
> residual plot in DHARMa. Otherwise, the post-hoc test in emmeas works fine.
>
>
>
> m2 <- glmmTMB(r ~ plot + (1|year), ziformula =~ 1,  data=df,
> family=beta_family(link="logit"))
>
> emmeans(m2, pairwise ~ plot, type="response")
>
> hist(resid(m2))  #skewed
>
> DHARMa::simulateResiduals(m2)  ? no significant deviation
>
>
>
> I?m not quite sure why the residual plots give different patterns in two
> models.  Is it not correct to look at the residual distribution for
> diagnosis? Could I see it?s a green light to use the model in glmmTMB based
> on the simulated residuals in the DHAMRMa package even though the simple
> residual plot looks skewed? Or is there any other post-hoc test designed
> for gamlss with the smoothing process? Any suggestions would be
> appreciated. Thanks in advance!
>
>
> Best,
>
> Yiru
>
>         [[alternative HTML version deleted]]
>
>
>
>
> ------------------------------
>
> Message: 2
> Date: Sun, 2 Feb 2020 19:57:06 -0500
> From: Ben Bolker <bbolker at gmail.com>
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME]  Need help on zero-inflated beta model and
>         post-hoc test
> Message-ID: <e7a36b0d-339b-5564-065c-c529d7084c0d at gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
>
>
> On 2020-02-02 4:08 p.m., Yi-Ru Cheng wrote:
> > Hi, everyone
> >
> >
> > I would like to compare the relatedness of individuals among plots. Since
> > relatedness is bound between 0 and 1 and most of them are zeros, I am
> > trying to fit with the zero-inflated beta model in the gamlss and glmmTMB
> > packages. After fitting the model, I want to perform the post-hoc test
> > between different plots. However, I have some conundrums here.
> >
> >
> > 1.  gamlss
> >
> > The beta model in the package gamlss gives a pretty good residual plot,
> but
> > the post-hoc tests in emmeans and multcomp don't work with the
> model...Here
> > is my model.
> >
> >
> >
> > m1 <- gamlss(r ~ plot + random(year), family = BEINF, data = df)
> >
> > emmeans(m1, pairwise ~ plot, type="response")
> >
> > hist(resid(m1))
> >
> >
> >  "Error in emm_basis.gamlss(object, trms, xlev, grid, misc = attr(data,
> :
> >
> >   gamlss models with smoothing are not yet supported in 'emmeans'"
> >
> >
> >
> > 2. glmmTMB
> >
> > I then tried the beta family in glmmTMB. For some reason, the simple
> > residual plot looks pretty skewed, but it looks alright with the scaled
> > residual plot in DHARMa. Otherwise, the post-hoc test in emmeas works
> fine.
> >
> > m2 <- glmmTMB(r ~ plot + (1|year), ziformula =~ 1,  data=df,
> > family=beta_family(link="logit"))
> >
> > emmeans(m2, pairwise ~ plot, type="response")
> >
> > hist(resid(m2))  #skewed
> >
> > DHARMa::simulateResiduals(m2)  ? no significant deviation
>
>    Quick answer: I don't think there's any reason to expect the
> residuals of a Beta GLMM *not* to be skewed.  Try simulating a simple
> example (you may even be able to use simulate() on your fitted model) to
> see for yourself.  DHARMa does a plot of expected vs actual
> distributions of residuals, not a raw plot of residuals.
>
>
> >
> >
> >
> > I?m not quite sure why the residual plots give different patterns in two
> > models.  Is it not correct to look at the residual distribution for
> > diagnosis? Could I see it?s a green light to use the model in glmmTMB
> based
> > on the simulated residuals in the DHAMRMa package even though the simple
> > residual plot looks skewed? Or is there any other post-hoc test designed
> > for gamlss with the smoothing process? Any suggestions would be
> > appreciated. Thanks in advance!
> >
> >
> > Best,
> >
> > Yiru
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
>
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> R-sig-mixed-models mailing list
> R-sig-mixed-models at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
> ------------------------------
>
> End of R-sig-mixed-models Digest, Vol 158, Issue 3
> **************************************************

	[[alternative HTML version deleted]]


From ch|r|eu @end|ng |rom gm@||@com  Tue Feb  4 10:51:57 2020
From: ch|r|eu @end|ng |rom gm@||@com (=?UTF-8?Q?David_Villegas_R=C3=ADos?=)
Date: Tue, 4 Feb 2020 10:51:57 +0100
Subject: [R-sig-ME] bam model selection with 3 million data
In-Reply-To: <b58aab128ee04900a040b9b0bad01410@hum.leidenuniv.nl>
References: <CALC46t_EROEaaJbTXuO1WGNbvr3H2h+pgRZWwNDWVe-zL5QrSw@mail.gmail.com>
 <24acbe45c2624637b108748d39f9c291@hum.leidenuniv.nl>
 <CALC46t_VkdG8hGBLjZxZPsWVr6f+W_1ta3usUCUVevUXgit07w@mail.gmail.com>
 <b58aab128ee04900a040b9b0bad01410@hum.leidenuniv.nl>
Message-ID: <CALC46t8z61QnK=dQw_KDHqfjWwOX+q2A7KjPLwfOqQfBDupqpA@mail.gmail.com>

Thanks Cesko.
I tried your suggestion (using the select argument) and got to a reasonable
optimal model in which I keep all the main effects and interactions except
for one main effect.
I tried, however, to do model selection using the buildbam function
(buildmer library) and I get a different optimal model, where all effects
are highly significant. I?m not sure why. According to the buildmer package
documentation:

 "As bam uses PQL, only crit='deviance' is supported."
where
crit=Character string or vector determining the criterion used to test
terms for elimination. Possible options are 'LRT' (likelihood-ratio test;
this is the default), 'LL' (use the raw -2 log likelihood), 'AIC' (Akaike
Information Criterion), 'BIC' (Bayesian Information Criterion), and
'deviance' (explained deviance ? note that this is not a formal test)

I?m not sure if you are familiar with buildmer package though.

Thanks,

David

El lun., 3 feb. 2020 a las 13:09, Voeten, C.C. (<
c.c.voeten at hum.leidenuniv.nl>) escribi?:

> Hi David,
>
> Please keep the mailing list in cc.
>
> Yes, gam() can indeed be prohibitively slow, hence why I suggested the
> select=TRUE approach. I hope the below explanation helps.
>
> Smooth terms are composed of a null space and a range space, which
> respectively represent the completely smooth part of the effect and the
> wiggly part of the effect. In the mathematical representation, these are
> equivalent to fixed effects and random effects, respectively. As such, the
> range space is subject to penalization and can shrink to zero. In GAM
> terms, this would correspond to a smoothing parameter tending to infinity,
> resulting in a completely smooth effect: you would be left only with the
> null space. What select=TRUE does is add an additional penalty to all null
> spaces, so that these too can shrink towards zero. Effects which are shrunk
> to (near) zero in this way can then be removed from the model by the user.
>
> The degree of shrinkage can be seen by looking at the edf. Smooths that
> are (near) zero will also use (near) zero edf. In practice, when I do model
> selection using select=TRUE, I always remove terms whose edf < 1. Then I
> fit the model again with select=TRUE to ensure that no further reduction is
> necessary, and then I fit a final model with select=FALSE.
>
> Best,
> Cesko
>
> -----Oorspronkelijk bericht-----
> Van: David Villegas R?os <chirleu at gmail.com>
> Verzonden: maandag 3 februari 2020 11:28
> Aan: Voeten, C.C. <c.c.voeten at hum.leidenuniv.nl>
> Onderwerp: Re: [R-sig-ME] bam model selection with 3 million data
>
> Thanks Cesko, really appreciate your answer.
> In my particular case however, I cannot run gam models (they take forever
> to run) so I?m still struggling on how to perform model selection in bam. I
> tried select=TRUE and the summary changed a bit, but I don?t really know
> what this argument is doing behind the scenes, or whether I should trust
> the summary from the model using select=TRUE rather than the default
> select=FALSE.
> Best wishes,
> David
>
> El s?b., 1 feb. 2020 a las 20:39, Voeten, C.C. (<mailto:
> c.c.voeten at hum.leidenuniv.nl>) escribi?:
> Hi David,
>
> 1) You cannot perform likelihood-based model comparisons with bam models,
> or -- for completeness' sake -- with gam models that were fitted using
> performance iteration or the EFS optimizer. All of these are based on PQL
> (penalized quasi-likelihood), which makes the log-likelihood (and hence
> LRT, AIC, BIC, etc) invalid for comparison purposes. See Wood
> (2017:149-151). gam() with the default outer iteration should be fine,
> though. Have you tried fitting your full model using bam with the
> select=TRUE argument to turn on mgcv's automatic smooth-term selection?
>
> 2) I am unsure if the deviance explained is or is not suitable for
> indicating effect size, so I can't comment on this question. I might,
> however, have an alternative suggestion: have you considered partial eta
> squared or partial omega squared? You should be able to calculate those
> based on the ANOVA table.
>
> 3) I agree with you that the warning suggests complete separation, but in
> my experience this doesn't automatically have to be a problem. Have you
> checked the summary for extremely large beta values, and also have you run
> gam.check() to see if your fit looks reasonable? If neither indicates a
> problem I wouldn't be too concerned about it.
>
> Hope this helps,
>
> Cesko
>
> P.S.: please send messages in plain text only, as you can see the
> formatting of your message was slightly screwed up because the mailing list
> automatically strips HTML markup
>
> -----Oorspronkelijk bericht-----
> Van: R-sig-mixed-models <mailto:r-sig-mixed-models-bounces at r-project.org>
> Namens David Villegas R?os
> Verzonden: zaterdag 1 februari 2020 19:57
> Aan: r-sig-mixed-models <mailto:r-sig-mixed-models at r-project.org>
> Onderwerp: [R-sig-ME] bam model selection with 3 million data
>
> Dear list,
>
> I?m investigating the effect of three variables (X, Y, Z) on the
> probability that an animal uses a particular habitat A. I have a time
> series of relocations for each animal (>300 individuals), with one
> relocation every 30 minutes. There are only two options for the response
> variable: 1=present in habitat A, 0=not present in habitat A. The effects
> of the three variables are expected to be non-linear so I?m using gam
> models. My dataset is very large, with >3 million data points so I?m using
> the bam function from the mgcv library in R. In my models I include a
> random effect ?individual ID?, and a temporal autocorrelation term that
> corrects much but not all of the autocorrelation in the models.
>
> *Question 1.*
>
> When I run a model with the three main effects (X, Y, Z) and the three
> double interactions (X:Y, X:Z, Y:Z), I get that all terms are highly
> significant, except for one interaction. If I remove it, then everything is
> highly significant. However, I also wanted to run simpler models with only
> one interaction, no interactions, only two main effects and only one main
> effect. Then, if I compare all these models with AIC or BIC, I get that the
> best model (by far) is the one with only main effects.
>
> >
>
> AIC(codcoaAR2,codcoaAR2.1,codcoaAR2.2,codcoaAR2.3,codcoaAR2.4,codcoaAR2.5,codcoaAR2.6,codcoaAR2.7,codcoaAR2.8,codcoaAR2.9,codcoaAR)
>
>                   df      AIC
>
> codcoaAR2   306.1310 -1442543
>
> codcoaAR2.1 293.1608 -1440642
>
> codcoaAR2.2 292.9615 -1438219
>
> codcoaAR2.3 294.3657 -1435346
>
> codcoaAR2.4 284.0026 -1434286
>
> codcoaAR2.5 280.3472 -1396765
>
> codcoaAR2.6 279.6380 -1435862
>
> codcoaAR2.7 269.4968 -1377806
>
> codcoaAR2.8 269.0480 -1393897
>
> codcoaAR2.9 281.8584 -1214270
>
> codcoaAR    271.7066 -2353481  # model with only main effects
>
>
>
> I wonder how this is possible if two of the interactions are highly
> significant.
>
> So my underlying question is: *for a model like this in which sample size
> is huge, should I make model selection looking at the significance of the
> different terms in the model, or should I rather look at AIC/BIC?*
>
> *Question 2.*
>
> Let?s assume the model with only main effects is indeed the optimal one.
> Then I?d like to get the effect size of each explanatory variable. It?s
> not clear to me how to do it even after reading some post on this and other
> forums, but I tried to figure it out by sequentially running the model
> without one explanatory variable at a time, and then comparing the deviance
> explained in the optimal model with X, Y, Z with the deviance explained
> with the reduced model with only Y and Z, for instance. Assuming that the
> difference would the variance explained by X. *Is this correct? *Looking at
> the results, the deviance explained by each variable X, Y, Z is quite low,
> but if the three main effects explain so little variance, who is explaining
> the rest?
>
> Model
>
> Deviance explained
>
> X, Z, Y
>
> 69.3%
>
> Y, Z
>
> 68.5%
>
> X, Z
>
> 69.3%
>
> X, Y
>
> 60.5%
>
>
>
> *Question 3.*
>
> In my models I usually get this error message:
>
> Warning message:
>
> In bgam.fitd(G, mf, gp, scale, nobs.extra = 0, rho = rho, coef = coef,  :
>
>   fitted probabilities numerically 0 or 1 occurred
>
>
>
> which seems to indicate that there is perfect separation in my logistic
> regression. I?m not sure this is the case in my data, how could I check it
> and correct for it if needed? Should it be always corrected?
>
>
>
> Thanks for your help,
>
> David
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> mailto:R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From c@c@voeten @end|ng |rom hum@|e|denun|v@n|  Tue Feb  4 11:27:37 2020
From: c@c@voeten @end|ng |rom hum@|e|denun|v@n| (Cesko Voeten)
Date: Tue, 4 Feb 2020 11:27:37 +0100
Subject: [R-sig-ME] bam model selection with 3 million data
In-Reply-To: <CALC46t8z61QnK=dQw_KDHqfjWwOX+q2A7KjPLwfOqQfBDupqpA@mail.gmail.com>
References: <CALC46t_EROEaaJbTXuO1WGNbvr3H2h+pgRZWwNDWVe-zL5QrSw@mail.gmail.com>
 <24acbe45c2624637b108748d39f9c291@hum.leidenuniv.nl>
 <CALC46t_VkdG8hGBLjZxZPsWVr6f+W_1ta3usUCUVevUXgit07w@mail.gmail.com>
 <b58aab128ee04900a040b9b0bad01410@hum.leidenuniv.nl>
 <CALC46t8z61QnK=dQw_KDHqfjWwOX+q2A7KjPLwfOqQfBDupqpA@mail.gmail.com>
Message-ID: <e12a07fd-247f-2a07-df52-96482fd2ec0a@hum.leidenuniv.nl>

Hi David,

I wrote that package, so yes, I am very familiar with it :) :)

buildbam is based on the explained deviance. A term is included if the explained deviance with the term is higher than the explained deviance without it. This means that buildbam will favor models that contain (too?) many effects, as effects that model only noise but still explain perhaps 0.0001% more deviance due to chance will still be included. To be honest with you, I have come to believe that the buildbam function was a mistake in the first place. I had coded buildgam already and buildbam was an easy extension, and it was only later that I realized that bam using PQL meant that only the explained deviance would be a valid model-comparison criterion. But note that the explained deviance is not actually a _formal_ criterion, like the likelihood-ratio test is (which uses a well-known result that differences in log-likelihood are asymptotically chi-square-distributed) or like AIC or BIC (which are respectively based on information theory and on Bayesian inference). I had wanted to remove buildbam for a long time, but this would be wrong of me to do because now that it's out there people may be using it, and given the provided caveat that it is can only use the explained deviance, they may actually have a use case for it. Your message makes me think very seriously about officially deprecating this function in buildmer's next release, or at least issue a warning that explained deviance is not a formal criterion and will favor overfitted models. In fact, I should deprecate that as well -- I had only put it in for buildbam's use.

If buildbam and bam(select=TRUE) conflict, I would definitely trust bam(select=TRUE) over buildbam, as mgcv's automatic smoothness selection is a method derived from first principles by extending the approach used to fit smooth terms anyway in a very straightforward way. On the other hand, the explained deviance is completely ad hoc and has no formal justification. I'll just bite the bullet and deprecate buildbam, advising people to use buildgam or select=TRUE instead.

Thanks for helping me bite that bullet,

Cesko

Op 4-2-2020 om 10:51 schreef David Villegas R?os:
> Thanks Cesko.
> I tried your suggestion (using the select argument) and got to a reasonable optimal model in which I keep all the main effects and interactions except for one main effect.
> I tried, however, to do model selection using the buildbam function (buildmer library) and I get a different optimal model, where all effects are highly significant. I?m not sure why. According to the buildmer package documentation:
> 
>  ?"As bam uses PQL, only crit='deviance' is supported."
> where
> crit=Character string or vector determining the criterion used to test terms for elimination. Possible options are 'LRT' (likelihood-ratio test; this is the default), 'LL' (use the raw -2 log likelihood), 'AIC' (Akaike Information Criterion), 'BIC' (Bayesian Information Criterion), and 'deviance' (explained deviance ? note that this is not a formal test)
> 
> I?m not sure if you are familiar with buildmer package though.
> 
> Thanks,
> 
> David
> 
> El lun., 3 feb. 2020 a las 13:09, Voeten, C.C. (<c.c.voeten at hum.leidenuniv.nl <mailto:c.c.voeten at hum.leidenuniv.nl>>) escribi?:
> 
>     Hi David,
> 
>     Please keep the mailing list in cc.
> 
>     Yes, gam() can indeed be prohibitively slow, hence why I suggested the select=TRUE approach. I hope the below explanation helps.
> 
>     Smooth terms are composed of a null space and a range space, which respectively represent the completely smooth part of the effect and the wiggly part of the effect. In the mathematical representation, these are equivalent to fixed effects and random effects, respectively. As such, the range space is subject to penalization and can shrink to zero. In GAM terms, this would correspond to a smoothing parameter tending to infinity, resulting in a completely smooth effect: you would be left only with the null space. What select=TRUE does is add an additional penalty to all null spaces, so that these too can shrink towards zero. Effects which are shrunk to (near) zero in this way can then be removed from the model by the user.
> 
>     The degree of shrinkage can be seen by looking at the edf. Smooths that are (near) zero will also use (near) zero edf. In practice, when I do model selection using select=TRUE, I always remove terms whose edf < 1. Then I fit the model again with select=TRUE to ensure that no further reduction is necessary, and then I fit a final model with select=FALSE.
> 
>     Best,
>     Cesko
> 
>     -----Oorspronkelijk bericht-----
>     Van: David Villegas R?os <chirleu at gmail.com <mailto:chirleu at gmail.com>>
>     Verzonden: maandag 3 februari 2020 11:28
>     Aan: Voeten, C.C. <c.c.voeten at hum.leidenuniv.nl <mailto:c.c.voeten at hum.leidenuniv.nl>>
>     Onderwerp: Re: [R-sig-ME] bam model selection with 3 million data
> 
>     Thanks Cesko, really appreciate your answer.
>     In my particular case however, I cannot run gam models (they take forever to run) so I?m still struggling on how to perform model selection in bam. I tried select=TRUE and the summary changed a bit, but I don?t really know what this argument is doing behind?the scenes, or whether I should trust the summary from the model using select=TRUE rather than the default select=FALSE.
>     Best wishes,
>     David
> 
>     El s?b., 1 feb. 2020 a las 20:39, Voeten, C.C. (<mailto:c.c.voeten at hum.leidenuniv.nl <mailto:c.c.voeten at hum.leidenuniv.nl>>) escribi?:
>     Hi David,
> 
>     1) You cannot perform likelihood-based model comparisons with bam models, or -- for completeness' sake -- with gam models that were fitted using performance iteration or the EFS optimizer. All of these are based on PQL (penalized quasi-likelihood), which makes the log-likelihood (and hence LRT, AIC, BIC, etc) invalid for comparison purposes. See Wood (2017:149-151). gam() with the default outer iteration should be fine, though. Have you tried fitting your full model using bam with the select=TRUE argument to turn on mgcv's automatic smooth-term selection?
> 
>     2) I am unsure if the deviance explained is or is not suitable for indicating effect size, so I can't comment on this question. I might, however, have an alternative suggestion: have you considered partial eta squared or partial omega squared? You should be able to calculate those based on the ANOVA table.
> 
>     3) I agree with you that the warning suggests complete separation, but in my experience this doesn't automatically have to be a problem. Have you checked the summary for extremely large beta values, and also have you run gam.check() to see if your fit looks reasonable? If neither indicates a problem I wouldn't be too concerned about it.
> 
>     Hope this helps,
> 
>     Cesko
> 
>     P.S.: please send messages in plain text only, as you can see the formatting of your message was slightly screwed up because the mailing list automatically strips HTML markup
> 
>     -----Oorspronkelijk bericht-----
>     Van: R-sig-mixed-models <mailto:r-sig-mixed-models-bounces at r-project.org <mailto:r-sig-mixed-models-bounces at r-project.org>> Namens David Villegas R?os
>     Verzonden: zaterdag 1 februari 2020 19:57
>     Aan: r-sig-mixed-models <mailto:r-sig-mixed-models at r-project.org <mailto:r-sig-mixed-models at r-project.org>>
>     Onderwerp: [R-sig-ME] bam model selection with 3 million data
> 
>     Dear list,
> 
>     I?m investigating the effect of three variables (X, Y, Z) on the probability that an animal uses a particular habitat A. I have a time series of relocations for each animal (>300 individuals), with one relocation every 30 minutes. There are only two options for the response
>     variable: 1=present in habitat A, 0=not present in habitat A. The effects of the three variables are expected to be non-linear so I?m using gam models. My dataset is very large, with >3 million data points so I?m using the bam function from the mgcv library in R. In my models I include a random effect ?individual ID?, and a temporal autocorrelation term that corrects much but not all of the autocorrelation in the models.
> 
>     *Question 1.*
> 
>     When I run a model with the three main effects (X, Y, Z) and the three double interactions (X:Y, X:Z, Y:Z), I get that all terms are highly significant, except for one interaction. If I remove it, then everything is highly significant. However, I also wanted to run simpler models with only one interaction, no interactions, only two main effects and only one main effect. Then, if I compare all these models with AIC or BIC, I get that the best model (by far) is the one with only main effects.
> 
>      >
>      ? ? AIC(codcoaAR2,codcoaAR2.1,codcoaAR2.2,codcoaAR2.3,codcoaAR2.4,codcoaAR2.5,codcoaAR2.6,codcoaAR2.7,codcoaAR2.8,codcoaAR2.9,codcoaAR)
> 
>      ? ? ? ? ? ? ? ? ? df? ? ? AIC
> 
>     codcoaAR2? ?306.1310 -1442543
> 
>     codcoaAR2.1 293.1608 -1440642
> 
>     codcoaAR2.2 292.9615 -1438219
> 
>     codcoaAR2.3 294.3657 -1435346
> 
>     codcoaAR2.4 284.0026 -1434286
> 
>     codcoaAR2.5 280.3472 -1396765
> 
>     codcoaAR2.6 279.6380 -1435862
> 
>     codcoaAR2.7 269.4968 -1377806
> 
>     codcoaAR2.8 269.0480 -1393897
> 
>     codcoaAR2.9 281.8584 -1214270
> 
>     codcoaAR? ? 271.7066 -2353481? # model with only main effects
> 
> 
> 
>     I wonder how this is possible if two of the interactions are highly significant.
> 
>     So my underlying question is: *for a model like this in which sample size is huge, should I make model selection looking at the significance of the different terms in the model, or should I rather look at AIC/BIC?*
> 
>     *Question 2.*
> 
>     Let?s assume the model with only main effects is indeed the optimal one.
>     Then I?d like to get the effect size of each explanatory variable. It?s not clear to me how to do it even after reading some post on this and other forums, but I tried to figure it out by sequentially running the model without one explanatory variable at a time, and then comparing the deviance explained in the optimal model with X, Y, Z with the deviance explained with the reduced model with only Y and Z, for instance. Assuming that the difference would the variance explained by X. *Is this correct? *Looking at the results, the deviance explained by each variable X, Y, Z is quite low, but if the three main effects explain so little variance, who is explaining the rest?
> 
>     Model
> 
>     Deviance explained
> 
>     X, Z, Y
> 
>     69.3%
> 
>     Y, Z
> 
>     68.5%
> 
>     X, Z
> 
>     69.3%
> 
>     X, Y
> 
>     60.5%
> 
> 
> 
>     *Question 3.*
> 
>     In my models I usually get this error message:
> 
>     Warning message:
> 
>     In bgam.fitd(G, mf, gp, scale, nobs.extra = 0, rho = rho, coef = coef,? :
> 
>      ? fitted probabilities numerically 0 or 1 occurred
> 
> 
> 
>     which seems to indicate that there is perfect separation in my logistic regression. I?m not sure this is the case in my data, how could I check it and correct for it if needed? Should it be always corrected?
> 
> 
> 
>     Thanks for your help,
> 
>     David
> 
>      ? ? ? ? [[alternative HTML version deleted]]
> 
>     _______________________________________________
>     mailto:R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org> mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 

From ch|r|eu @end|ng |rom gm@||@com  Tue Feb  4 12:30:07 2020
From: ch|r|eu @end|ng |rom gm@||@com (=?UTF-8?Q?David_Villegas_R=C3=ADos?=)
Date: Tue, 4 Feb 2020 12:30:07 +0100
Subject: [R-sig-ME] bam model selection with 3 million data
In-Reply-To: <e12a07fd-247f-2a07-df52-96482fd2ec0a@hum.leidenuniv.nl>
References: <CALC46t_EROEaaJbTXuO1WGNbvr3H2h+pgRZWwNDWVe-zL5QrSw@mail.gmail.com>
 <24acbe45c2624637b108748d39f9c291@hum.leidenuniv.nl>
 <CALC46t_VkdG8hGBLjZxZPsWVr6f+W_1ta3usUCUVevUXgit07w@mail.gmail.com>
 <b58aab128ee04900a040b9b0bad01410@hum.leidenuniv.nl>
 <CALC46t8z61QnK=dQw_KDHqfjWwOX+q2A7KjPLwfOqQfBDupqpA@mail.gmail.com>
 <e12a07fd-247f-2a07-df52-96482fd2ec0a@hum.leidenuniv.nl>
Message-ID: <CALC46t9i=3C=P39WjHHZNotQpjKdELXOFh5ABFrNM0y999WgUw@mail.gmail.com>

Thanks Cesko,
I also tried the *compareML* function (itsadug library) to compare 1) the
model with all terms included (the best model according to "buildbam") and
2) the model with one main effect out (the best according to select=TRUE
procedure) and *compareML* supports the full model (same as buildbam) when
both models 1) and 2) are fit using select=FALSE.
Just in case you have experience with *compareML *too:)
David



El mar., 4 feb. 2020 a las 11:27, Cesko Voeten (<
c.c.voeten at hum.leidenuniv.nl>) escribi?:

> Hi David,
>
> I wrote that package, so yes, I am very familiar with it :) :)
>
> buildbam is based on the explained deviance. A term is included if the
> explained deviance with the term is higher than the explained deviance
> without it. This means that buildbam will favor models that contain (too?)
> many effects, as effects that model only noise but still explain perhaps
> 0.0001% more deviance due to chance will still be included. To be honest
> with you, I have come to believe that the buildbam function was a mistake
> in the first place. I had coded buildgam already and buildbam was an easy
> extension, and it was only later that I realized that bam using PQL meant
> that only the explained deviance would be a valid model-comparison
> criterion. But note that the explained deviance is not actually a _formal_
> criterion, like the likelihood-ratio test is (which uses a well-known
> result that differences in log-likelihood are asymptotically
> chi-square-distributed) or like AIC or BIC (which are respectively based on
> information theory and on Bayesian inference). I had wanted to remove
> buildbam for a long time, but this would be wrong of me to do because now
> that it's out there people may be using it, and given the provided caveat
> that it is can only use the explained deviance, they may actually have a
> use case for it. Your message makes me think very seriously about
> officially deprecating this function in buildmer's next release, or at
> least issue a warning that explained deviance is not a formal criterion and
> will favor overfitted models. In fact, I should deprecate that as well -- I
> had only put it in for buildbam's use.
>
> If buildbam and bam(select=TRUE) conflict, I would definitely trust
> bam(select=TRUE) over buildbam, as mgcv's automatic smoothness selection is
> a method derived from first principles by extending the approach used to
> fit smooth terms anyway in a very straightforward way. On the other hand,
> the explained deviance is completely ad hoc and has no formal
> justification. I'll just bite the bullet and deprecate buildbam, advising
> people to use buildgam or select=TRUE instead.
>
> Thanks for helping me bite that bullet,
>
> Cesko
>
> Op 4-2-2020 om 10:51 schreef David Villegas R?os:
> > Thanks Cesko.
> > I tried your suggestion (using the select argument) and got to a
> reasonable optimal model in which I keep all the main effects and
> interactions except for one main effect.
> > I tried, however, to do model selection using the buildbam function
> (buildmer library) and I get a different optimal model, where all effects
> are highly significant. I?m not sure why. According to the buildmer package
> documentation:
> >
> >   "As bam uses PQL, only crit='deviance' is supported."
> > where
> > crit=Character string or vector determining the criterion used to test
> terms for elimination. Possible options are 'LRT' (likelihood-ratio test;
> this is the default), 'LL' (use the raw -2 log likelihood), 'AIC' (Akaike
> Information Criterion), 'BIC' (Bayesian Information Criterion), and
> 'deviance' (explained deviance ? note that this is not a formal test)
> >
> > I?m not sure if you are familiar with buildmer package though.
> >
> > Thanks,
> >
> > David
> >
> > El lun., 3 feb. 2020 a las 13:09, Voeten, C.C. (<
> c.c.voeten at hum.leidenuniv.nl <mailto:c.c.voeten at hum.leidenuniv.nl>>)
> escribi?:
> >
> >     Hi David,
> >
> >     Please keep the mailing list in cc.
> >
> >     Yes, gam() can indeed be prohibitively slow, hence why I suggested
> the select=TRUE approach. I hope the below explanation helps.
> >
> >     Smooth terms are composed of a null space and a range space, which
> respectively represent the completely smooth part of the effect and the
> wiggly part of the effect. In the mathematical representation, these are
> equivalent to fixed effects and random effects, respectively. As such, the
> range space is subject to penalization and can shrink to zero. In GAM
> terms, this would correspond to a smoothing parameter tending to infinity,
> resulting in a completely smooth effect: you would be left only with the
> null space. What select=TRUE does is add an additional penalty to all null
> spaces, so that these too can shrink towards zero. Effects which are shrunk
> to (near) zero in this way can then be removed from the model by the user.
> >
> >     The degree of shrinkage can be seen by looking at the edf. Smooths
> that are (near) zero will also use (near) zero edf. In practice, when I do
> model selection using select=TRUE, I always remove terms whose edf < 1.
> Then I fit the model again with select=TRUE to ensure that no further
> reduction is necessary, and then I fit a final model with select=FALSE.
> >
> >     Best,
> >     Cesko
> >
> >     -----Oorspronkelijk bericht-----
> >     Van: David Villegas R?os <chirleu at gmail.com <mailto:
> chirleu at gmail.com>>
> >     Verzonden: maandag 3 februari 2020 11:28
> >     Aan: Voeten, C.C. <c.c.voeten at hum.leidenuniv.nl <mailto:
> c.c.voeten at hum.leidenuniv.nl>>
> >     Onderwerp: Re: [R-sig-ME] bam model selection with 3 million data
> >
> >     Thanks Cesko, really appreciate your answer.
> >     In my particular case however, I cannot run gam models (they take
> forever to run) so I?m still struggling on how to perform model selection
> in bam. I tried select=TRUE and the summary changed a bit, but I don?t
> really know what this argument is doing behind the scenes, or whether I
> should trust the summary from the model using select=TRUE rather than the
> default select=FALSE.
> >     Best wishes,
> >     David
> >
> >     El s?b., 1 feb. 2020 a las 20:39, Voeten, C.C. (<mailto:
> c.c.voeten at hum.leidenuniv.nl <mailto:c.c.voeten at hum.leidenuniv.nl>>)
> escribi?:
> >     Hi David,
> >
> >     1) You cannot perform likelihood-based model comparisons with bam
> models, or -- for completeness' sake -- with gam models that were fitted
> using performance iteration or the EFS optimizer. All of these are based on
> PQL (penalized quasi-likelihood), which makes the log-likelihood (and hence
> LRT, AIC, BIC, etc) invalid for comparison purposes. See Wood
> (2017:149-151). gam() with the default outer iteration should be fine,
> though. Have you tried fitting your full model using bam with the
> select=TRUE argument to turn on mgcv's automatic smooth-term selection?
> >
> >     2) I am unsure if the deviance explained is or is not suitable for
> indicating effect size, so I can't comment on this question. I might,
> however, have an alternative suggestion: have you considered partial eta
> squared or partial omega squared? You should be able to calculate those
> based on the ANOVA table.
> >
> >     3) I agree with you that the warning suggests complete separation,
> but in my experience this doesn't automatically have to be a problem. Have
> you checked the summary for extremely large beta values, and also have you
> run gam.check() to see if your fit looks reasonable? If neither indicates a
> problem I wouldn't be too concerned about it.
> >
> >     Hope this helps,
> >
> >     Cesko
> >
> >     P.S.: please send messages in plain text only, as you can see the
> formatting of your message was slightly screwed up because the mailing list
> automatically strips HTML markup
> >
> >     -----Oorspronkelijk bericht-----
> >     Van: R-sig-mixed-models <mailto:
> r-sig-mixed-models-bounces at r-project.org <mailto:
> r-sig-mixed-models-bounces at r-project.org>> Namens David Villegas R?os
> >     Verzonden: zaterdag 1 februari 2020 19:57
> >     Aan: r-sig-mixed-models <mailto:r-sig-mixed-models at r-project.org
> <mailto:r-sig-mixed-models at r-project.org>>
> >     Onderwerp: [R-sig-ME] bam model selection with 3 million data
> >
> >     Dear list,
> >
> >     I?m investigating the effect of three variables (X, Y, Z) on the
> probability that an animal uses a particular habitat A. I have a time
> series of relocations for each animal (>300 individuals), with one
> relocation every 30 minutes. There are only two options for the response
> >     variable: 1=present in habitat A, 0=not present in habitat A. The
> effects of the three variables are expected to be non-linear so I?m using
> gam models. My dataset is very large, with >3 million data points so I?m
> using the bam function from the mgcv library in R. In my models I include a
> random effect ?individual ID?, and a temporal autocorrelation term that
> corrects much but not all of the autocorrelation in the models.
> >
> >     *Question 1.*
> >
> >     When I run a model with the three main effects (X, Y, Z) and the
> three double interactions (X:Y, X:Z, Y:Z), I get that all terms are highly
> significant, except for one interaction. If I remove it, then everything is
> highly significant. However, I also wanted to run simpler models with only
> one interaction, no interactions, only two main effects and only one main
> effect. Then, if I compare all these models with AIC or BIC, I get that the
> best model (by far) is the one with only main effects.
> >
> >      >
> >
> AIC(codcoaAR2,codcoaAR2.1,codcoaAR2.2,codcoaAR2.3,codcoaAR2.4,codcoaAR2.5,codcoaAR2.6,codcoaAR2.7,codcoaAR2.8,codcoaAR2.9,codcoaAR)
> >
> >                        df      AIC
> >
> >     codcoaAR2   306.1310 -1442543
> >
> >     codcoaAR2.1 293.1608 -1440642
> >
> >     codcoaAR2.2 292.9615 -1438219
> >
> >     codcoaAR2.3 294.3657 -1435346
> >
> >     codcoaAR2.4 284.0026 -1434286
> >
> >     codcoaAR2.5 280.3472 -1396765
> >
> >     codcoaAR2.6 279.6380 -1435862
> >
> >     codcoaAR2.7 269.4968 -1377806
> >
> >     codcoaAR2.8 269.0480 -1393897
> >
> >     codcoaAR2.9 281.8584 -1214270
> >
> >     codcoaAR    271.7066 -2353481  # model with only main effects
> >
> >
> >
> >     I wonder how this is possible if two of the interactions are highly
> significant.
> >
> >     So my underlying question is: *for a model like this in which sample
> size is huge, should I make model selection looking at the significance of
> the different terms in the model, or should I rather look at AIC/BIC?*
> >
> >     *Question 2.*
> >
> >     Let?s assume the model with only main effects is indeed the optimal
> one.
> >     Then I?d like to get the effect size of each explanatory variable.
> It?s not clear to me how to do it even after reading some post on this and
> other forums, but I tried to figure it out by sequentially running the
> model without one explanatory variable at a time, and then comparing the
> deviance explained in the optimal model with X, Y, Z with the deviance
> explained with the reduced model with only Y and Z, for instance. Assuming
> that the difference would the variance explained by X. *Is this correct?
> *Looking at the results, the deviance explained by each variable X, Y, Z is
> quite low, but if the three main effects explain so little variance, who is
> explaining the rest?
> >
> >     Model
> >
> >     Deviance explained
> >
> >     X, Z, Y
> >
> >     69.3%
> >
> >     Y, Z
> >
> >     68.5%
> >
> >     X, Z
> >
> >     69.3%
> >
> >     X, Y
> >
> >     60.5%
> >
> >
> >
> >     *Question 3.*
> >
> >     In my models I usually get this error message:
> >
> >     Warning message:
> >
> >     In bgam.fitd(G, mf, gp, scale, nobs.extra = 0, rho = rho, coef =
> coef,  :
> >
> >        fitted probabilities numerically 0 or 1 occurred
> >
> >
> >
> >     which seems to indicate that there is perfect separation in my
> logistic regression. I?m not sure this is the case in my data, how could I
> check it and correct for it if needed? Should it be always corrected?
> >
> >
> >
> >     Thanks for your help,
> >
> >     David
> >
> >              [[alternative HTML version deleted]]
> >
> >     _______________________________________________
> >     mailto:R-sig-mixed-models at r-project.org <mailto:
> R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>

	[[alternative HTML version deleted]]


From c@c@voeten @end|ng |rom hum@|e|denun|v@n|  Tue Feb  4 13:30:09 2020
From: c@c@voeten @end|ng |rom hum@|e|denun|v@n| (Cesko Voeten)
Date: Tue, 4 Feb 2020 13:30:09 +0100
Subject: [R-sig-ME] bam model selection with 3 million data
In-Reply-To: <CALC46t9i=3C=P39WjHHZNotQpjKdELXOFh5ABFrNM0y999WgUw@mail.gmail.com>
References: <CALC46t_EROEaaJbTXuO1WGNbvr3H2h+pgRZWwNDWVe-zL5QrSw@mail.gmail.com>
 <24acbe45c2624637b108748d39f9c291@hum.leidenuniv.nl>
 <CALC46t_VkdG8hGBLjZxZPsWVr6f+W_1ta3usUCUVevUXgit07w@mail.gmail.com>
 <b58aab128ee04900a040b9b0bad01410@hum.leidenuniv.nl>
 <CALC46t8z61QnK=dQw_KDHqfjWwOX+q2A7KjPLwfOqQfBDupqpA@mail.gmail.com>
 <e12a07fd-247f-2a07-df52-96482fd2ec0a@hum.leidenuniv.nl>
 <CALC46t9i=3C=P39WjHHZNotQpjKdELXOFh5ABFrNM0y999WgUw@mail.gmail.com>
Message-ID: <0b97edda-add1-99e2-d7f6-98cd6e1d1a41@hum.leidenuniv.nl>

Hi David,

I've never used compareML myself, but after taking a quick look at the code it seems that it's just doing a likelihood-ratio test, which means that the same caveats apply. So if my understanding of this function is correct, it can be used with gam models fitted using outer iteration with method="ML" or method="REML" (which are based on penalized (restricted) maximum likelihood), but not with gam models fitted using GCV/UBRE/Cp (which are not likelihood-based), gam models fitted using non-default optimizers (these use PQL), or bam models (PQL). What I mean by this is that, while the numerical comparison of the optimized log-likelihood values may very well work for selecting the best model, the penalized quasilikelihood is not a true likelihood and hence cannot fall back on Wilks's theorem that the difference in log-likelihoods is chi-square-distributed. The analogous reasoning goes for AIC and BIC. But that doesn't mean that such comparisons are useless -- remember that all models are wrong, but some are useful.

Note that the only thing select=TRUE does is enable additional penalization of the smooths; it is up to you to determine how much shrinkage you deem enough to warrant removing a term from the model. So if you feel that your one main effect is important to you, you can always choose to leave it in. However, if you go this route, might I suggest taking a look at the parsimonious mixed models paper (https://arxiv.org/abs/1506.04967), where the bottom line is: if you have reasons to expect a term to be important or unimportant, why even bother with selection procedures and why not just fit the model that you believe represents your data the best? (In fact, I personally would only use stepwise-selection methods if I wasn't sure whether a term is or is not important, particularly with respect to achieving convergence...)

Best,
Cesko

Op 4-2-2020 om 12:30 schreef David Villegas R?os:
> Thanks Cesko,
> I also tried the /compareML/ function (itsadug library) to compare 1) the model with all terms included (the best model according to "buildbam") and 2) the model with one main effect out (the best according to select=TRUE procedure) and /compareML/ supports the full model (same as buildbam) when both models 1) and 2) are fit using select=FALSE.
> Just in case you have experience with /compareML /too:)
> David
> 
> 
> 
> El mar., 4 feb. 2020 a las 11:27, Cesko Voeten (<c.c.voeten at hum.leidenuniv.nl <mailto:c.c.voeten at hum.leidenuniv.nl>>) escribi?:
> 
>     Hi David,
> 
>     I wrote that package, so yes, I am very familiar with it :) :)
> 
>     buildbam is based on the explained deviance. A term is included if the explained deviance with the term is higher than the explained deviance without it. This means that buildbam will favor models that contain (too?) many effects, as effects that model only noise but still explain perhaps 0.0001% more deviance due to chance will still be included. To be honest with you, I have come to believe that the buildbam function was a mistake in the first place. I had coded buildgam already and buildbam was an easy extension, and it was only later that I realized that bam using PQL meant that only the explained deviance would be a valid model-comparison criterion. But note that the explained deviance is not actually a _formal_ criterion, like the likelihood-ratio test is (which uses a well-known result that differences in log-likelihood are asymptotically chi-square-distributed) or like AIC or BIC (which are respectively based on information theory and on Bayesian inference). I had
>     wanted to remove buildbam for a long time, but this would be wrong of me to do because now that it's out there people may be using it, and given the provided caveat that it is can only use the explained deviance, they may actually have a use case for it. Your message makes me think very seriously about officially deprecating this function in buildmer's next release, or at least issue a warning that explained deviance is not a formal criterion and will favor overfitted models. In fact, I should deprecate that as well -- I had only put it in for buildbam's use.
> 
>     If buildbam and bam(select=TRUE) conflict, I would definitely trust bam(select=TRUE) over buildbam, as mgcv's automatic smoothness selection is a method derived from first principles by extending the approach used to fit smooth terms anyway in a very straightforward way. On the other hand, the explained deviance is completely ad hoc and has no formal justification. I'll just bite the bullet and deprecate buildbam, advising people to use buildgam or select=TRUE instead.
> 
>     Thanks for helping me bite that bullet,
> 
>     Cesko
> 
>     Op 4-2-2020 om 10:51 schreef David Villegas R?os:
>      > Thanks Cesko.
>      > I tried your suggestion (using the select argument) and got to a reasonable optimal model in which I keep all the main effects and interactions except for one main effect.
>      > I tried, however, to do model selection using the buildbam function (buildmer library) and I get a different optimal model, where all effects are highly significant. I?m not sure why. According to the buildmer package documentation:
>      >
>      >? ?"As bam uses PQL, only crit='deviance' is supported."
>      > where
>      > crit=Character string or vector determining the criterion used to test terms for elimination. Possible options are 'LRT' (likelihood-ratio test; this is the default), 'LL' (use the raw -2 log likelihood), 'AIC' (Akaike Information Criterion), 'BIC' (Bayesian Information Criterion), and 'deviance' (explained deviance ? note that this is not a formal test)
>      >
>      > I?m not sure if you are familiar with buildmer package though.
>      >
>      > Thanks,
>      >
>      > David
>      >
>      > El lun., 3 feb. 2020 a las 13:09, Voeten, C.C. (<c.c.voeten at hum.leidenuniv.nl <mailto:c.c.voeten at hum.leidenuniv.nl> <mailto:c.c.voeten at hum.leidenuniv.nl <mailto:c.c.voeten at hum.leidenuniv.nl>>>) escribi?:
>      >
>      >? ? ?Hi David,
>      >
>      >? ? ?Please keep the mailing list in cc.
>      >
>      >? ? ?Yes, gam() can indeed be prohibitively slow, hence why I suggested the select=TRUE approach. I hope the below explanation helps.
>      >
>      >? ? ?Smooth terms are composed of a null space and a range space, which respectively represent the completely smooth part of the effect and the wiggly part of the effect. In the mathematical representation, these are equivalent to fixed effects and random effects, respectively. As such, the range space is subject to penalization and can shrink to zero. In GAM terms, this would correspond to a smoothing parameter tending to infinity, resulting in a completely smooth effect: you would be left only with the null space. What select=TRUE does is add an additional penalty to all null spaces, so that these too can shrink towards zero. Effects which are shrunk to (near) zero in this way can then be removed from the model by the user.
>      >
>      >? ? ?The degree of shrinkage can be seen by looking at the edf. Smooths that are (near) zero will also use (near) zero edf. In practice, when I do model selection using select=TRUE, I always remove terms whose edf < 1. Then I fit the model again with select=TRUE to ensure that no further reduction is necessary, and then I fit a final model with select=FALSE.
>      >
>      >? ? ?Best,
>      >? ? ?Cesko
>      >
>      >? ? ?-----Oorspronkelijk bericht-----
>      >? ? ?Van: David Villegas R?os <chirleu at gmail.com <mailto:chirleu at gmail.com> <mailto:chirleu at gmail.com <mailto:chirleu at gmail.com>>>
>      >? ? ?Verzonden: maandag 3 februari 2020 11:28
>      >? ? ?Aan: Voeten, C.C. <c.c.voeten at hum.leidenuniv.nl <mailto:c.c.voeten at hum.leidenuniv.nl> <mailto:c.c.voeten at hum.leidenuniv.nl <mailto:c.c.voeten at hum.leidenuniv.nl>>>
>      >? ? ?Onderwerp: Re: [R-sig-ME] bam model selection with 3 million data
>      >
>      >? ? ?Thanks Cesko, really appreciate your answer.
>      >? ? ?In my particular case however, I cannot run gam models (they take forever to run) so I?m still struggling on how to perform model selection in bam. I tried select=TRUE and the summary changed a bit, but I don?t really know what this argument is doing behind?the scenes, or whether I should trust the summary from the model using select=TRUE rather than the default select=FALSE.
>      >? ? ?Best wishes,
>      >? ? ?David
>      >
>      >? ? ?El s?b., 1 feb. 2020 a las 20:39, Voeten, C.C. (<mailto:c.c.voeten at hum.leidenuniv.nl <mailto:c.c.voeten at hum.leidenuniv.nl> <mailto:c.c.voeten at hum.leidenuniv.nl <mailto:c.c.voeten at hum.leidenuniv.nl>>>) escribi?:
>      >? ? ?Hi David,
>      >
>      >? ? ?1) You cannot perform likelihood-based model comparisons with bam models, or -- for completeness' sake -- with gam models that were fitted using performance iteration or the EFS optimizer. All of these are based on PQL (penalized quasi-likelihood), which makes the log-likelihood (and hence LRT, AIC, BIC, etc) invalid for comparison purposes. See Wood (2017:149-151). gam() with the default outer iteration should be fine, though. Have you tried fitting your full model using bam with the select=TRUE argument to turn on mgcv's automatic smooth-term selection?
>      >
>      >? ? ?2) I am unsure if the deviance explained is or is not suitable for indicating effect size, so I can't comment on this question. I might, however, have an alternative suggestion: have you considered partial eta squared or partial omega squared? You should be able to calculate those based on the ANOVA table.
>      >
>      >? ? ?3) I agree with you that the warning suggests complete separation, but in my experience this doesn't automatically have to be a problem. Have you checked the summary for extremely large beta values, and also have you run gam.check() to see if your fit looks reasonable? If neither indicates a problem I wouldn't be too concerned about it.
>      >
>      >? ? ?Hope this helps,
>      >
>      >? ? ?Cesko
>      >
>      >? ? ?P.S.: please send messages in plain text only, as you can see the formatting of your message was slightly screwed up because the mailing list automatically strips HTML markup
>      >
>      >? ? ?-----Oorspronkelijk bericht-----
>      >? ? ?Van: R-sig-mixed-models <mailto:r-sig-mixed-models-bounces at r-project.org <mailto:r-sig-mixed-models-bounces at r-project.org> <mailto:r-sig-mixed-models-bounces at r-project.org <mailto:r-sig-mixed-models-bounces at r-project.org>>> Namens David Villegas R?os
>      >? ? ?Verzonden: zaterdag 1 februari 2020 19:57
>      >? ? ?Aan: r-sig-mixed-models <mailto:r-sig-mixed-models at r-project.org <mailto:r-sig-mixed-models at r-project.org> <mailto:r-sig-mixed-models at r-project.org <mailto:r-sig-mixed-models at r-project.org>>>
>      >? ? ?Onderwerp: [R-sig-ME] bam model selection with 3 million data
>      >
>      >? ? ?Dear list,
>      >
>      >? ? ?I?m investigating the effect of three variables (X, Y, Z) on the probability that an animal uses a particular habitat A. I have a time series of relocations for each animal (>300 individuals), with one relocation every 30 minutes. There are only two options for the response
>      >? ? ?variable: 1=present in habitat A, 0=not present in habitat A. The effects of the three variables are expected to be non-linear so I?m using gam models. My dataset is very large, with >3 million data points so I?m using the bam function from the mgcv library in R. In my models I include a random effect ?individual ID?, and a temporal autocorrelation term that corrects much but not all of the autocorrelation in the models.
>      >
>      >? ? ?*Question 1.*
>      >
>      >? ? ?When I run a model with the three main effects (X, Y, Z) and the three double interactions (X:Y, X:Z, Y:Z), I get that all terms are highly significant, except for one interaction. If I remove it, then everything is highly significant. However, I also wanted to run simpler models with only one interaction, no interactions, only two main effects and only one main effect. Then, if I compare all these models with AIC or BIC, I get that the best model (by far) is the one with only main effects.
>      >
>      >? ? ? >
>      >? ? ? ? ? AIC(codcoaAR2,codcoaAR2.1,codcoaAR2.2,codcoaAR2.3,codcoaAR2.4,codcoaAR2.5,codcoaAR2.6,codcoaAR2.7,codcoaAR2.8,codcoaAR2.9,codcoaAR)
>      >
>      >? ? ? ? ? ? ? ? ? ? ? ? df? ? ? AIC
>      >
>      >? ? ?codcoaAR2? ?306.1310 -1442543
>      >
>      >? ? ?codcoaAR2.1 293.1608 -1440642
>      >
>      >? ? ?codcoaAR2.2 292.9615 -1438219
>      >
>      >? ? ?codcoaAR2.3 294.3657 -1435346
>      >
>      >? ? ?codcoaAR2.4 284.0026 -1434286
>      >
>      >? ? ?codcoaAR2.5 280.3472 -1396765
>      >
>      >? ? ?codcoaAR2.6 279.6380 -1435862
>      >
>      >? ? ?codcoaAR2.7 269.4968 -1377806
>      >
>      >? ? ?codcoaAR2.8 269.0480 -1393897
>      >
>      >? ? ?codcoaAR2.9 281.8584 -1214270
>      >
>      >? ? ?codcoaAR? ? 271.7066 -2353481? # model with only main effects
>      >
>      >
>      >
>      >? ? ?I wonder how this is possible if two of the interactions are highly significant.
>      >
>      >? ? ?So my underlying question is: *for a model like this in which sample size is huge, should I make model selection looking at the significance of the different terms in the model, or should I rather look at AIC/BIC?*
>      >
>      >? ? ?*Question 2.*
>      >
>      >? ? ?Let?s assume the model with only main effects is indeed the optimal one.
>      >? ? ?Then I?d like to get the effect size of each explanatory variable. It?s not clear to me how to do it even after reading some post on this and other forums, but I tried to figure it out by sequentially running the model without one explanatory variable at a time, and then comparing the deviance explained in the optimal model with X, Y, Z with the deviance explained with the reduced model with only Y and Z, for instance. Assuming that the difference would the variance explained by X. *Is this correct? *Looking at the results, the deviance explained by each variable X, Y, Z is quite low, but if the three main effects explain so little variance, who is explaining the rest?
>      >
>      >? ? ?Model
>      >
>      >? ? ?Deviance explained
>      >
>      >? ? ?X, Z, Y
>      >
>      >? ? ?69.3%
>      >
>      >? ? ?Y, Z
>      >
>      >? ? ?68.5%
>      >
>      >? ? ?X, Z
>      >
>      >? ? ?69.3%
>      >
>      >? ? ?X, Y
>      >
>      >? ? ?60.5%
>      >
>      >
>      >
>      >? ? ?*Question 3.*
>      >
>      >? ? ?In my models I usually get this error message:
>      >
>      >? ? ?Warning message:
>      >
>      >? ? ?In bgam.fitd(G, mf, gp, scale, nobs.extra = 0, rho = rho, coef = coef,? :
>      >
>      >? ? ? ? fitted probabilities numerically 0 or 1 occurred
>      >
>      >
>      >
>      >? ? ?which seems to indicate that there is perfect separation in my logistic regression. I?m not sure this is the case in my data, how could I check it and correct for it if needed? Should it be always corrected?
>      >
>      >
>      >
>      >? ? ?Thanks for your help,
>      >
>      >? ? ?David
>      >
>      >? ? ? ? ? ? ? [[alternative HTML version deleted]]
>      >
>      >? ? ?_______________________________________________
>      >? ? ?mailto:R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org> <mailto:R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org>> mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>      >
> 

From j|@ver|@@|mo @end|ng |rom gm@||@com  Tue Feb  4 13:46:38 2020
From: j|@ver|@@|mo @end|ng |rom gm@||@com (=?ISO-8859-1?Q?Jo=E3o_Ver=EDssimo?=)
Date: Tue, 04 Feb 2020 13:46:38 +0100
Subject: [R-sig-ME] bam model selection with 3 million data
In-Reply-To: <0b97edda-add1-99e2-d7f6-98cd6e1d1a41@hum.leidenuniv.nl>
References: <CALC46t_EROEaaJbTXuO1WGNbvr3H2h+pgRZWwNDWVe-zL5QrSw@mail.gmail.com>
 <24acbe45c2624637b108748d39f9c291@hum.leidenuniv.nl>
 <CALC46t_VkdG8hGBLjZxZPsWVr6f+W_1ta3usUCUVevUXgit07w@mail.gmail.com>
 <b58aab128ee04900a040b9b0bad01410@hum.leidenuniv.nl>
 <CALC46t8z61QnK=dQw_KDHqfjWwOX+q2A7KjPLwfOqQfBDupqpA@mail.gmail.com>
 <e12a07fd-247f-2a07-df52-96482fd2ec0a@hum.leidenuniv.nl>
 <CALC46t9i=3C=P39WjHHZNotQpjKdELXOFh5ABFrNM0y999WgUw@mail.gmail.com>
 <0b97edda-add1-99e2-d7f6-98cd6e1d1a41@hum.leidenuniv.nl>
Message-ID: <ad5b1aa0795cb6bb8d59a2d3a0ff9177ff4e2b54.camel@gmail.com>

If it helps: the compareML() help says that "model comparison is only
implemented for the methods GCV, fREML, REML, and ML."
(so GCV seems possible.)

In the following vignette, it is additionally stated that:
"For testing the difference in fixed effects predictors the method
fREML does not provide the most reliable test. Rather use ML. However,
ML takes longer to run and penalizes wigglyness more."

https://cran.r-project.org/web/packages/itsadug/vignettes/test.html

Jo?o

On Tue, 2020-02-04 at 13:30 +0100, Cesko Voeten wrote:
> Hi David,
> 
> I've never used compareML myself, but after taking a quick look at
> the code it seems that it's just doing a likelihood-ratio test, which
> means that the same caveats apply. So if my understanding of this
> function is correct, it can be used with gam models fitted using
> outer iteration with method="ML" or method="REML" (which are based on
> penalized (restricted) maximum likelihood), but not with gam models
> fitted using GCV/UBRE/Cp (which are not likelihood-based), gam models
> fitted using non-default optimizers (these use PQL), or bam models
> (PQL). What I mean by this is that, while the numerical comparison of
> the optimized log-likelihood values may very well work for selecting
> the best model, the penalized quasilikelihood is not a true
> likelihood and hence cannot fall back on Wilks's theorem that the
> difference in log-likelihoods is chi-square-distributed. The
> analogous reasoning goes for AIC and BIC. But that doesn't mean that
> such comparisons are useless -- remember that all models are wrong,
> but some are useful.
> 
> Note that the only thing select=TRUE does is enable additional
> penalization of the smooths; it is up to you to determine how much
> shrinkage you deem enough to warrant removing a term from the model.
> So if you feel that your one main effect is important to you, you can
> always choose to leave it in. However, if you go this route, might I
> suggest taking a look at the parsimonious mixed models paper (
> https://arxiv.org/abs/1506.04967), where the bottom line is: if you
> have reasons to expect a term to be important or unimportant, why
> even bother with selection procedures and why not just fit the model
> that you believe represents your data the best? (In fact, I
> personally would only use stepwise-selection methods if I wasn't sure
> whether a term is or is not important, particularly with respect to
> achieving convergence...)
> 
> Best,
> Cesko
> 
> Op 4-2-2020 om 12:30 schreef David Villegas R?os:
> > Thanks Cesko,
> > I also tried the /compareML/ function (itsadug library) to compare
> > 1) the model with all terms included (the best model according to
> > "buildbam") and 2) the model with one main effect out (the best
> > according to select=TRUE procedure) and /compareML/ supports the
> > full model (same as buildbam) when both models 1) and 2) are fit
> > using select=FALSE.
> > Just in case you have experience with /compareML /too:)
> > David
> > 
> > 
> > 
> > El mar., 4 feb. 2020 a las 11:27, Cesko Voeten (<
> > c.c.voeten at hum.leidenuniv.nl <mailto:c.c.voeten at hum.leidenuniv.nl>>
> > ) escribi?:
> > 
> >     Hi David,
> > 
> >     I wrote that package, so yes, I am very familiar with it :) :)
> > 
> >     buildbam is based on the explained deviance. A term is included
> > if the explained deviance with the term is higher than the
> > explained deviance without it. This means that buildbam will favor
> > models that contain (too?) many effects, as effects that model only
> > noise but still explain perhaps 0.0001% more deviance due to chance
> > will still be included. To be honest with you, I have come to
> > believe that the buildbam function was a mistake in the first
> > place. I had coded buildgam already and buildbam was an easy
> > extension, and it was only later that I realized that bam using PQL
> > meant that only the explained deviance would be a valid model-
> > comparison criterion. But note that the explained deviance is not
> > actually a _formal_ criterion, like the likelihood-ratio test is
> > (which uses a well-known result that differences in log-likelihood
> > are asymptotically chi-square-distributed) or like AIC or BIC
> > (which are respectively based on information theory and on Bayesian
> > inference). I had
> >     wanted to remove buildbam for a long time, but this would be
> > wrong of me to do because now that it's out there people may be
> > using it, and given the provided caveat that it is can only use the
> > explained deviance, they may actually have a use case for it. Your
> > message makes me think very seriously about officially deprecating
> > this function in buildmer's next release, or at least issue a
> > warning that explained deviance is not a formal criterion and will
> > favor overfitted models. In fact, I should deprecate that as well
> > -- I had only put it in for buildbam's use.
> > 
> >     If buildbam and bam(select=TRUE) conflict, I would definitely
> > trust bam(select=TRUE) over buildbam, as mgcv's automatic
> > smoothness selection is a method derived from first principles by
> > extending the approach used to fit smooth terms anyway in a very
> > straightforward way. On the other hand, the explained deviance is
> > completely ad hoc and has no formal justification. I'll just bite
> > the bullet and deprecate buildbam, advising people to use buildgam
> > or select=TRUE instead.
> > 
> >     Thanks for helping me bite that bullet,
> > 
> >     Cesko
> > 
> >     Op 4-2-2020 om 10:51 schreef David Villegas R?os:
> >      > Thanks Cesko.
> >      > I tried your suggestion (using the select argument) and got
> > to a reasonable optimal model in which I keep all the main effects
> > and interactions except for one main effect.
> >      > I tried, however, to do model selection using the buildbam
> > function (buildmer library) and I get a different optimal model,
> > where all effects are highly significant. I?m not sure why.
> > According to the buildmer package documentation:
> >      >
> >      >   "As bam uses PQL, only crit='deviance' is supported."
> >      > where
> >      > crit=Character string or vector determining the criterion
> > used to test terms for elimination. Possible options are 'LRT'
> > (likelihood-ratio test; this is the default), 'LL' (use the raw -2
> > log likelihood), 'AIC' (Akaike Information Criterion), 'BIC'
> > (Bayesian Information Criterion), and 'deviance' (explained
> > deviance ? note that this is not a formal test)
> >      >
> >      > I?m not sure if you are familiar with buildmer package
> > though.
> >      >
> >      > Thanks,
> >      >
> >      > David
> >      >
> >      > El lun., 3 feb. 2020 a las 13:09, Voeten, C.C. (<
> > c.c.voeten at hum.leidenuniv.nl <mailto:c.c.voeten at hum.leidenuniv.nl>
> > <mailto:c.c.voeten at hum.leidenuniv.nl <mailto:
> > c.c.voeten at hum.leidenuniv.nl>>>) escribi?:
> >      >
> >      >     Hi David,
> >      >
> >      >     Please keep the mailing list in cc.
> >      >
> >      >     Yes, gam() can indeed be prohibitively slow, hence why I
> > suggested the select=TRUE approach. I hope the below explanation
> > helps.
> >      >
> >      >     Smooth terms are composed of a null space and a range
> > space, which respectively represent the completely smooth part of
> > the effect and the wiggly part of the effect. In the mathematical
> > representation, these are equivalent to fixed effects and random
> > effects, respectively. As such, the range space is subject to
> > penalization and can shrink to zero. In GAM terms, this would
> > correspond to a smoothing parameter tending to infinity, resulting
> > in a completely smooth effect: you would be left only with the null
> > space. What select=TRUE does is add an additional penalty to all
> > null spaces, so that these too can shrink towards zero. Effects
> > which are shrunk to (near) zero in this way can then be removed
> > from the model by the user.
> >      >
> >      >     The degree of shrinkage can be seen by looking at the
> > edf. Smooths that are (near) zero will also use (near) zero edf. In
> > practice, when I do model selection using select=TRUE, I always
> > remove terms whose edf < 1. Then I fit the model again with
> > select=TRUE to ensure that no further reduction is necessary, and
> > then I fit a final model with select=FALSE.
> >      >
> >      >     Best,
> >      >     Cesko
> >      >
> >      >     -----Oorspronkelijk bericht-----
> >      >     Van: David Villegas R?os <chirleu at gmail.com <mailto:
> > chirleu at gmail.com> <mailto:chirleu at gmail.com <mailto:
> > chirleu at gmail.com>>>
> >      >     Verzonden: maandag 3 februari 2020 11:28
> >      >     Aan: Voeten, C.C. <c.c.voeten at hum.leidenuniv.nl <mailto:
> > c.c.voeten at hum.leidenuniv.nl> <mailto:c.c.voeten at hum.leidenuniv.nl
> > <mailto:c.c.voeten at hum.leidenuniv.nl>>>
> >      >     Onderwerp: Re: [R-sig-ME] bam model selection with 3
> > million data
> >      >
> >      >     Thanks Cesko, really appreciate your answer.
> >      >     In my particular case however, I cannot run gam models
> > (they take forever to run) so I?m still struggling on how to
> > perform model selection in bam. I tried select=TRUE and the summary
> > changed a bit, but I don?t really know what this argument is doing
> > behind the scenes, or whether I should trust the summary from the
> > model using select=TRUE rather than the default select=FALSE.
> >      >     Best wishes,
> >      >     David
> >      >
> >      >     El s?b., 1 feb. 2020 a las 20:39, Voeten, C.C. (<mailto:
> > c.c.voeten at hum.leidenuniv.nl <mailto:c.c.voeten at hum.leidenuniv.nl>
> > <mailto:c.c.voeten at hum.leidenuniv.nl <mailto:
> > c.c.voeten at hum.leidenuniv.nl>>>) escribi?:
> >      >     Hi David,
> >      >
> >      >     1) You cannot perform likelihood-based model comparisons
> > with bam models, or -- for completeness' sake -- with gam models
> > that were fitted using performance iteration or the EFS optimizer.
> > All of these are based on PQL (penalized quasi-likelihood), which
> > makes the log-likelihood (and hence LRT, AIC, BIC, etc) invalid for
> > comparison purposes. See Wood (2017:149-151). gam() with the
> > default outer iteration should be fine, though. Have you tried
> > fitting your full model using bam with the select=TRUE argument to
> > turn on mgcv's automatic smooth-term selection?
> >      >
> >      >     2) I am unsure if the deviance explained is or is not
> > suitable for indicating effect size, so I can't comment on this
> > question. I might, however, have an alternative suggestion: have
> > you considered partial eta squared or partial omega squared? You
> > should be able to calculate those based on the ANOVA table.
> >      >
> >      >     3) I agree with you that the warning suggests complete
> > separation, but in my experience this doesn't automatically have to
> > be a problem. Have you checked the summary for extremely large beta
> > values, and also have you run gam.check() to see if your fit looks
> > reasonable? If neither indicates a problem I wouldn't be too
> > concerned about it.
> >      >
> >      >     Hope this helps,
> >      >
> >      >     Cesko
> >      >
> >      >     P.S.: please send messages in plain text only, as you
> > can see the formatting of your message was slightly screwed up
> > because the mailing list automatically strips HTML markup
> >      >
> >      >     -----Oorspronkelijk bericht-----
> >      >     Van: R-sig-mixed-models <mailto:
> > r-sig-mixed-models-bounces at r-project.org <mailto:
> > r-sig-mixed-models-bounces at r-project.org> <mailto:
> > r-sig-mixed-models-bounces at r-project.org <mailto:
> > r-sig-mixed-models-bounces at r-project.org>>> Namens David Villegas
> > R?os
> >      >     Verzonden: zaterdag 1 februari 2020 19:57
> >      >     Aan: r-sig-mixed-models <mailto:
> > r-sig-mixed-models at r-project.org <mailto:
> > r-sig-mixed-models at r-project.org> <mailto:
> > r-sig-mixed-models at r-project.org <mailto:
> > r-sig-mixed-models at r-project.org>>>
> >      >     Onderwerp: [R-sig-ME] bam model selection with 3 million
> > data
> >      >
> >      >     Dear list,
> >      >
> >      >     I?m investigating the effect of three variables (X, Y,
> > Z) on the probability that an animal uses a particular habitat A. I
> > have a time series of relocations for each animal (>300
> > individuals), with one relocation every 30 minutes. There are only
> > two options for the response
> >      >     variable: 1=present in habitat A, 0=not present in
> > habitat A. The effects of the three variables are expected to be
> > non-linear so I?m using gam models. My dataset is very large, with
> > >3 million data points so I?m using the bam function from the mgcv
> > library in R. In my models I include a random effect ?individual
> > ID?, and a temporal autocorrelation term that corrects much but not
> > all of the autocorrelation in the models.
> >      >
> >      >     *Question 1.*
> >      >
> >      >     When I run a model with the three main effects (X, Y, Z)
> > and the three double interactions (X:Y, X:Z, Y:Z), I get that all
> > terms are highly significant, except for one interaction. If I
> > remove it, then everything is highly significant. However, I also
> > wanted to run simpler models with only one interaction, no
> > interactions, only two main effects and only one main effect. Then,
> > if I compare all these models with AIC or BIC, I get that the best
> > model (by far) is the one with only main effects.
> >      >
> >      >      >
> >      >         
> > AIC(codcoaAR2,codcoaAR2.1,codcoaAR2.2,codcoaAR2.3,codcoaAR2.4,codco
> > aAR2.5,codcoaAR2.6,codcoaAR2.7,codcoaAR2.8,codcoaAR2.9,codcoaAR)
> >      >
> >      >                        df      AIC
> >      >
> >      >     codcoaAR2   306.1310 -1442543
> >      >
> >      >     codcoaAR2.1 293.1608 -1440642
> >      >
> >      >     codcoaAR2.2 292.9615 -1438219
> >      >
> >      >     codcoaAR2.3 294.3657 -1435346
> >      >
> >      >     codcoaAR2.4 284.0026 -1434286
> >      >
> >      >     codcoaAR2.5 280.3472 -1396765
> >      >
> >      >     codcoaAR2.6 279.6380 -1435862
> >      >
> >      >     codcoaAR2.7 269.4968 -1377806
> >      >
> >      >     codcoaAR2.8 269.0480 -1393897
> >      >
> >      >     codcoaAR2.9 281.8584 -1214270
> >      >
> >      >     codcoaAR    271.7066 -2353481  # model with only main
> > effects
> >      >
> >      >
> >      >
> >      >     I wonder how this is possible if two of the interactions
> > are highly significant.
> >      >
> >      >     So my underlying question is: *for a model like this in
> > which sample size is huge, should I make model selection looking at
> > the significance of the different terms in the model, or should I
> > rather look at AIC/BIC?*
> >      >
> >      >     *Question 2.*
> >      >
> >      >     Let?s assume the model with only main effects is indeed
> > the optimal one.
> >      >     Then I?d like to get the effect size of each explanatory
> > variable. It?s not clear to me how to do it even after reading some
> > post on this and other forums, but I tried to figure it out by
> > sequentially running the model without one explanatory variable at
> > a time, and then comparing the deviance explained in the optimal
> > model with X, Y, Z with the deviance explained with the reduced
> > model with only Y and Z, for instance. Assuming that the difference
> > would the variance explained by X. *Is this correct? *Looking at
> > the results, the deviance explained by each variable X, Y, Z is
> > quite low, but if the three main effects explain so little
> > variance, who is explaining the rest?
> >      >
> >      >     Model
> >      >
> >      >     Deviance explained
> >      >
> >      >     X, Z, Y
> >      >
> >      >     69.3%
> >      >
> >      >     Y, Z
> >      >
> >      >     68.5%
> >      >
> >      >     X, Z
> >      >
> >      >     69.3%
> >      >
> >      >     X, Y
> >      >
> >      >     60.5%
> >      >
> >      >
> >      >
> >      >     *Question 3.*
> >      >
> >      >     In my models I usually get this error message:
> >      >
> >      >     Warning message:
> >      >
> >      >     In bgam.fitd(G, mf, gp, scale, nobs.extra = 0, rho =
> > rho, coef = coef,  :
> >      >
> >      >        fitted probabilities numerically 0 or 1 occurred
> >      >
> >      >
> >      >
> >      >     which seems to indicate that there is perfect separation
> > in my logistic regression. I?m not sure this is the case in my
> > data, how could I check it and correct for it if needed? Should it
> > be always corrected?
> >      >
> >      >
> >      >
> >      >     Thanks for your help,
> >      >
> >      >     David
> >      >
> >      >              [[alternative HTML version deleted]]
> >      >
> >      >     _______________________________________________
> >      >     mailto:R-sig-mixed-models at r-project.org <mailto:R-sig-
> > mixed-models at r-project.org> <mailto:R-sig-mixed-models at r-
> > project.org <mailto:R-sig-mixed-models at r-project.org>> mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >      >
> > 
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From c@c@voeten @end|ng |rom hum@|e|denun|v@n|  Tue Feb  4 14:23:37 2020
From: c@c@voeten @end|ng |rom hum@|e|denun|v@n| (Cesko Voeten)
Date: Tue, 4 Feb 2020 14:23:37 +0100
Subject: [R-sig-ME] bam model selection with 3 million data
In-Reply-To: <ad5b1aa0795cb6bb8d59a2d3a0ff9177ff4e2b54.camel@gmail.com>
References: <CALC46t_EROEaaJbTXuO1WGNbvr3H2h+pgRZWwNDWVe-zL5QrSw@mail.gmail.com>
 <24acbe45c2624637b108748d39f9c291@hum.leidenuniv.nl>
 <CALC46t_VkdG8hGBLjZxZPsWVr6f+W_1ta3usUCUVevUXgit07w@mail.gmail.com>
 <b58aab128ee04900a040b9b0bad01410@hum.leidenuniv.nl>
 <CALC46t8z61QnK=dQw_KDHqfjWwOX+q2A7KjPLwfOqQfBDupqpA@mail.gmail.com>
 <e12a07fd-247f-2a07-df52-96482fd2ec0a@hum.leidenuniv.nl>
 <CALC46t9i=3C=P39WjHHZNotQpjKdELXOFh5ABFrNM0y999WgUw@mail.gmail.com>
 <0b97edda-add1-99e2-d7f6-98cd6e1d1a41@hum.leidenuniv.nl>
 <ad5b1aa0795cb6bb8d59a2d3a0ff9177ff4e2b54.camel@gmail.com>
Message-ID: <1b1c8640-f9ed-fea2-bf75-f7c1f5791fb0@hum.leidenuniv.nl>

I've also taken a close look at Breslow & Clayton (1993), the paper cited in Wood (2017:149) and _for Normal errors_, PQL is equivalent to REML (or, _I assume_, ML if method="ML" is given). So it turns out that in the linear case only, model comparisons should be fine even if models were fitted using PQL. I can't comment on GCV as I'm not familiar with the way it is computed (I should take another look at Wood 2017...), but if the authors of itsadug (who are very well-respected in my field) claim it can be used, I will take their word for it.

Yes, REML indeed should not be used to compare models with different fixed-effect structures, and using ML is the answer even though it underestimates random effects (or things that look like them, such as smooth terms) and hence oversmooths.

Thanks for this discussion.

Cesko

Op 4-2-2020 om 13:46 schreef Jo?o Ver?ssimo:
> If it helps: the compareML() help says that "model comparison is only
> implemented for the methods GCV, fREML, REML, and ML."
> (so GCV seems possible.)
> 
> In the following vignette, it is additionally stated that:
> "For testing the difference in fixed effects predictors the method
> fREML does not provide the most reliable test. Rather use ML. However,
> ML takes longer to run and penalizes wigglyness more."
> 
> https://cran.r-project.org/web/packages/itsadug/vignettes/test.html
> 
> Jo?o
> 
> On Tue, 2020-02-04 at 13:30 +0100, Cesko Voeten wrote:
>> Hi David,
>>
>> I've never used compareML myself, but after taking a quick look at
>> the code it seems that it's just doing a likelihood-ratio test, which
>> means that the same caveats apply. So if my understanding of this
>> function is correct, it can be used with gam models fitted using
>> outer iteration with method="ML" or method="REML" (which are based on
>> penalized (restricted) maximum likelihood), but not with gam models
>> fitted using GCV/UBRE/Cp (which are not likelihood-based), gam models
>> fitted using non-default optimizers (these use PQL), or bam models
>> (PQL). What I mean by this is that, while the numerical comparison of
>> the optimized log-likelihood values may very well work for selecting
>> the best model, the penalized quasilikelihood is not a true
>> likelihood and hence cannot fall back on Wilks's theorem that the
>> difference in log-likelihoods is chi-square-distributed. The
>> analogous reasoning goes for AIC and BIC. But that doesn't mean that
>> such comparisons are useless -- remember that all models are wrong,
>> but some are useful.
>>
>> Note that the only thing select=TRUE does is enable additional
>> penalization of the smooths; it is up to you to determine how much
>> shrinkage you deem enough to warrant removing a term from the model.
>> So if you feel that your one main effect is important to you, you can
>> always choose to leave it in. However, if you go this route, might I
>> suggest taking a look at the parsimonious mixed models paper (
>> https://arxiv.org/abs/1506.04967), where the bottom line is: if you
>> have reasons to expect a term to be important or unimportant, why
>> even bother with selection procedures and why not just fit the model
>> that you believe represents your data the best? (In fact, I
>> personally would only use stepwise-selection methods if I wasn't sure
>> whether a term is or is not important, particularly with respect to
>> achieving convergence...)
>>
>> Best,
>> Cesko
>>
>> Op 4-2-2020 om 12:30 schreef David Villegas R?os:
>>> Thanks Cesko,
>>> I also tried the /compareML/ function (itsadug library) to compare
>>> 1) the model with all terms included (the best model according to
>>> "buildbam") and 2) the model with one main effect out (the best
>>> according to select=TRUE procedure) and /compareML/ supports the
>>> full model (same as buildbam) when both models 1) and 2) are fit
>>> using select=FALSE.
>>> Just in case you have experience with /compareML /too:)
>>> David
>>>
>>>
>>>
>>> El mar., 4 feb. 2020 a las 11:27, Cesko Voeten (<
>>> c.c.voeten at hum.leidenuniv.nl <mailto:c.c.voeten at hum.leidenuniv.nl>>
>>> ) escribi?:
>>>
>>>      Hi David,
>>>
>>>      I wrote that package, so yes, I am very familiar with it :) :)
>>>
>>>      buildbam is based on the explained deviance. A term is included
>>> if the explained deviance with the term is higher than the
>>> explained deviance without it. This means that buildbam will favor
>>> models that contain (too?) many effects, as effects that model only
>>> noise but still explain perhaps 0.0001% more deviance due to chance
>>> will still be included. To be honest with you, I have come to
>>> believe that the buildbam function was a mistake in the first
>>> place. I had coded buildgam already and buildbam was an easy
>>> extension, and it was only later that I realized that bam using PQL
>>> meant that only the explained deviance would be a valid model-
>>> comparison criterion. But note that the explained deviance is not
>>> actually a _formal_ criterion, like the likelihood-ratio test is
>>> (which uses a well-known result that differences in log-likelihood
>>> are asymptotically chi-square-distributed) or like AIC or BIC
>>> (which are respectively based on information theory and on Bayesian
>>> inference). I had
>>>      wanted to remove buildbam for a long time, but this would be
>>> wrong of me to do because now that it's out there people may be
>>> using it, and given the provided caveat that it is can only use the
>>> explained deviance, they may actually have a use case for it. Your
>>> message makes me think very seriously about officially deprecating
>>> this function in buildmer's next release, or at least issue a
>>> warning that explained deviance is not a formal criterion and will
>>> favor overfitted models. In fact, I should deprecate that as well
>>> -- I had only put it in for buildbam's use.
>>>
>>>      If buildbam and bam(select=TRUE) conflict, I would definitely
>>> trust bam(select=TRUE) over buildbam, as mgcv's automatic
>>> smoothness selection is a method derived from first principles by
>>> extending the approach used to fit smooth terms anyway in a very
>>> straightforward way. On the other hand, the explained deviance is
>>> completely ad hoc and has no formal justification. I'll just bite
>>> the bullet and deprecate buildbam, advising people to use buildgam
>>> or select=TRUE instead.
>>>
>>>      Thanks for helping me bite that bullet,
>>>
>>>      Cesko
>>>
>>>      Op 4-2-2020 om 10:51 schreef David Villegas R?os:
>>>       > Thanks Cesko.
>>>       > I tried your suggestion (using the select argument) and got
>>> to a reasonable optimal model in which I keep all the main effects
>>> and interactions except for one main effect.
>>>       > I tried, however, to do model selection using the buildbam
>>> function (buildmer library) and I get a different optimal model,
>>> where all effects are highly significant. I?m not sure why.
>>> According to the buildmer package documentation:
>>>       >
>>>       >   "As bam uses PQL, only crit='deviance' is supported."
>>>       > where
>>>       > crit=Character string or vector determining the criterion
>>> used to test terms for elimination. Possible options are 'LRT'
>>> (likelihood-ratio test; this is the default), 'LL' (use the raw -2
>>> log likelihood), 'AIC' (Akaike Information Criterion), 'BIC'
>>> (Bayesian Information Criterion), and 'deviance' (explained
>>> deviance ? note that this is not a formal test)
>>>       >
>>>       > I?m not sure if you are familiar with buildmer package
>>> though.
>>>       >
>>>       > Thanks,
>>>       >
>>>       > David
>>>       >
>>>       > El lun., 3 feb. 2020 a las 13:09, Voeten, C.C. (<
>>> c.c.voeten at hum.leidenuniv.nl <mailto:c.c.voeten at hum.leidenuniv.nl>
>>> <mailto:c.c.voeten at hum.leidenuniv.nl <mailto:
>>> c.c.voeten at hum.leidenuniv.nl>>>) escribi?:
>>>       >
>>>       >     Hi David,
>>>       >
>>>       >     Please keep the mailing list in cc.
>>>       >
>>>       >     Yes, gam() can indeed be prohibitively slow, hence why I
>>> suggested the select=TRUE approach. I hope the below explanation
>>> helps.
>>>       >
>>>       >     Smooth terms are composed of a null space and a range
>>> space, which respectively represent the completely smooth part of
>>> the effect and the wiggly part of the effect. In the mathematical
>>> representation, these are equivalent to fixed effects and random
>>> effects, respectively. As such, the range space is subject to
>>> penalization and can shrink to zero. In GAM terms, this would
>>> correspond to a smoothing parameter tending to infinity, resulting
>>> in a completely smooth effect: you would be left only with the null
>>> space. What select=TRUE does is add an additional penalty to all
>>> null spaces, so that these too can shrink towards zero. Effects
>>> which are shrunk to (near) zero in this way can then be removed
>>> from the model by the user.
>>>       >
>>>       >     The degree of shrinkage can be seen by looking at the
>>> edf. Smooths that are (near) zero will also use (near) zero edf. In
>>> practice, when I do model selection using select=TRUE, I always
>>> remove terms whose edf < 1. Then I fit the model again with
>>> select=TRUE to ensure that no further reduction is necessary, and
>>> then I fit a final model with select=FALSE.
>>>       >
>>>       >     Best,
>>>       >     Cesko
>>>       >
>>>       >     -----Oorspronkelijk bericht-----
>>>       >     Van: David Villegas R?os <chirleu at gmail.com <mailto:
>>> chirleu at gmail.com> <mailto:chirleu at gmail.com <mailto:
>>> chirleu at gmail.com>>>
>>>       >     Verzonden: maandag 3 februari 2020 11:28
>>>       >     Aan: Voeten, C.C. <c.c.voeten at hum.leidenuniv.nl <mailto:
>>> c.c.voeten at hum.leidenuniv.nl> <mailto:c.c.voeten at hum.leidenuniv.nl
>>> <mailto:c.c.voeten at hum.leidenuniv.nl>>>
>>>       >     Onderwerp: Re: [R-sig-ME] bam model selection with 3
>>> million data
>>>       >
>>>       >     Thanks Cesko, really appreciate your answer.
>>>       >     In my particular case however, I cannot run gam models
>>> (they take forever to run) so I?m still struggling on how to
>>> perform model selection in bam. I tried select=TRUE and the summary
>>> changed a bit, but I don?t really know what this argument is doing
>>> behind the scenes, or whether I should trust the summary from the
>>> model using select=TRUE rather than the default select=FALSE.
>>>       >     Best wishes,
>>>       >     David
>>>       >
>>>       >     El s?b., 1 feb. 2020 a las 20:39, Voeten, C.C. (<mailto:
>>> c.c.voeten at hum.leidenuniv.nl <mailto:c.c.voeten at hum.leidenuniv.nl>
>>> <mailto:c.c.voeten at hum.leidenuniv.nl <mailto:
>>> c.c.voeten at hum.leidenuniv.nl>>>) escribi?:
>>>       >     Hi David,
>>>       >
>>>       >     1) You cannot perform likelihood-based model comparisons
>>> with bam models, or -- for completeness' sake -- with gam models
>>> that were fitted using performance iteration or the EFS optimizer.
>>> All of these are based on PQL (penalized quasi-likelihood), which
>>> makes the log-likelihood (and hence LRT, AIC, BIC, etc) invalid for
>>> comparison purposes. See Wood (2017:149-151). gam() with the
>>> default outer iteration should be fine, though. Have you tried
>>> fitting your full model using bam with the select=TRUE argument to
>>> turn on mgcv's automatic smooth-term selection?
>>>       >
>>>       >     2) I am unsure if the deviance explained is or is not
>>> suitable for indicating effect size, so I can't comment on this
>>> question. I might, however, have an alternative suggestion: have
>>> you considered partial eta squared or partial omega squared? You
>>> should be able to calculate those based on the ANOVA table.
>>>       >
>>>       >     3) I agree with you that the warning suggests complete
>>> separation, but in my experience this doesn't automatically have to
>>> be a problem. Have you checked the summary for extremely large beta
>>> values, and also have you run gam.check() to see if your fit looks
>>> reasonable? If neither indicates a problem I wouldn't be too
>>> concerned about it.
>>>       >
>>>       >     Hope this helps,
>>>       >
>>>       >     Cesko
>>>       >
>>>       >     P.S.: please send messages in plain text only, as you
>>> can see the formatting of your message was slightly screwed up
>>> because the mailing list automatically strips HTML markup
>>>       >
>>>       >     -----Oorspronkelijk bericht-----
>>>       >     Van: R-sig-mixed-models <mailto:
>>> r-sig-mixed-models-bounces at r-project.org <mailto:
>>> r-sig-mixed-models-bounces at r-project.org> <mailto:
>>> r-sig-mixed-models-bounces at r-project.org <mailto:
>>> r-sig-mixed-models-bounces at r-project.org>>> Namens David Villegas
>>> R?os
>>>       >     Verzonden: zaterdag 1 februari 2020 19:57
>>>       >     Aan: r-sig-mixed-models <mailto:
>>> r-sig-mixed-models at r-project.org <mailto:
>>> r-sig-mixed-models at r-project.org> <mailto:
>>> r-sig-mixed-models at r-project.org <mailto:
>>> r-sig-mixed-models at r-project.org>>>
>>>       >     Onderwerp: [R-sig-ME] bam model selection with 3 million
>>> data
>>>       >
>>>       >     Dear list,
>>>       >
>>>       >     I?m investigating the effect of three variables (X, Y,
>>> Z) on the probability that an animal uses a particular habitat A. I
>>> have a time series of relocations for each animal (>300
>>> individuals), with one relocation every 30 minutes. There are only
>>> two options for the response
>>>       >     variable: 1=present in habitat A, 0=not present in
>>> habitat A. The effects of the three variables are expected to be
>>> non-linear so I?m using gam models. My dataset is very large, with
>>>> 3 million data points so I?m using the bam function from the mgcv
>>> library in R. In my models I include a random effect ?individual
>>> ID?, and a temporal autocorrelation term that corrects much but not
>>> all of the autocorrelation in the models.
>>>       >
>>>       >     *Question 1.*
>>>       >
>>>       >     When I run a model with the three main effects (X, Y, Z)
>>> and the three double interactions (X:Y, X:Z, Y:Z), I get that all
>>> terms are highly significant, except for one interaction. If I
>>> remove it, then everything is highly significant. However, I also
>>> wanted to run simpler models with only one interaction, no
>>> interactions, only two main effects and only one main effect. Then,
>>> if I compare all these models with AIC or BIC, I get that the best
>>> model (by far) is the one with only main effects.
>>>       >
>>>       >      >
>>>       >
>>> AIC(codcoaAR2,codcoaAR2.1,codcoaAR2.2,codcoaAR2.3,codcoaAR2.4,codco
>>> aAR2.5,codcoaAR2.6,codcoaAR2.7,codcoaAR2.8,codcoaAR2.9,codcoaAR)
>>>       >
>>>       >                        df      AIC
>>>       >
>>>       >     codcoaAR2   306.1310 -1442543
>>>       >
>>>       >     codcoaAR2.1 293.1608 -1440642
>>>       >
>>>       >     codcoaAR2.2 292.9615 -1438219
>>>       >
>>>       >     codcoaAR2.3 294.3657 -1435346
>>>       >
>>>       >     codcoaAR2.4 284.0026 -1434286
>>>       >
>>>       >     codcoaAR2.5 280.3472 -1396765
>>>       >
>>>       >     codcoaAR2.6 279.6380 -1435862
>>>       >
>>>       >     codcoaAR2.7 269.4968 -1377806
>>>       >
>>>       >     codcoaAR2.8 269.0480 -1393897
>>>       >
>>>       >     codcoaAR2.9 281.8584 -1214270
>>>       >
>>>       >     codcoaAR    271.7066 -2353481  # model with only main
>>> effects
>>>       >
>>>       >
>>>       >
>>>       >     I wonder how this is possible if two of the interactions
>>> are highly significant.
>>>       >
>>>       >     So my underlying question is: *for a model like this in
>>> which sample size is huge, should I make model selection looking at
>>> the significance of the different terms in the model, or should I
>>> rather look at AIC/BIC?*
>>>       >
>>>       >     *Question 2.*
>>>       >
>>>       >     Let?s assume the model with only main effects is indeed
>>> the optimal one.
>>>       >     Then I?d like to get the effect size of each explanatory
>>> variable. It?s not clear to me how to do it even after reading some
>>> post on this and other forums, but I tried to figure it out by
>>> sequentially running the model without one explanatory variable at
>>> a time, and then comparing the deviance explained in the optimal
>>> model with X, Y, Z with the deviance explained with the reduced
>>> model with only Y and Z, for instance. Assuming that the difference
>>> would the variance explained by X. *Is this correct? *Looking at
>>> the results, the deviance explained by each variable X, Y, Z is
>>> quite low, but if the three main effects explain so little
>>> variance, who is explaining the rest?
>>>       >
>>>       >     Model
>>>       >
>>>       >     Deviance explained
>>>       >
>>>       >     X, Z, Y
>>>       >
>>>       >     69.3%
>>>       >
>>>       >     Y, Z
>>>       >
>>>       >     68.5%
>>>       >
>>>       >     X, Z
>>>       >
>>>       >     69.3%
>>>       >
>>>       >     X, Y
>>>       >
>>>       >     60.5%
>>>       >
>>>       >
>>>       >
>>>       >     *Question 3.*
>>>       >
>>>       >     In my models I usually get this error message:
>>>       >
>>>       >     Warning message:
>>>       >
>>>       >     In bgam.fitd(G, mf, gp, scale, nobs.extra = 0, rho =
>>> rho, coef = coef,  :
>>>       >
>>>       >        fitted probabilities numerically 0 or 1 occurred
>>>       >
>>>       >
>>>       >
>>>       >     which seems to indicate that there is perfect separation
>>> in my logistic regression. I?m not sure this is the case in my
>>> data, how could I check it and correct for it if needed? Should it
>>> be always corrected?
>>>       >
>>>       >
>>>       >
>>>       >     Thanks for your help,
>>>       >
>>>       >     David
>>>       >
>>>       >              [[alternative HTML version deleted]]
>>>       >
>>>       >     _______________________________________________
>>>       >     mailto:R-sig-mixed-models at r-project.org <mailto:R-sig-
>>> mixed-models at r-project.org> <mailto:R-sig-mixed-models at r-
>>> project.org <mailto:R-sig-mixed-models at r-project.org>> mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>       >
>>>
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From j||bo97 @end|ng |rom gm@||@com  Tue Feb  4 17:44:34 2020
From: j||bo97 @end|ng |rom gm@||@com (Jill Brouwer)
Date: Wed, 5 Feb 2020 00:44:34 +0800
Subject: [R-sig-ME] threshold for singular fit
Message-ID: <CAB9nLEUEhOKR=U6uwYYdiXqA=aocRsf1LZ8cXn-UFEKHuAnm-g@mail.gmail.com>

Hi all,

Is there an acceptable threshold for singular fits?

I am trying to fit a GLMM with fixed effect of pH treatment (Chamber), and
random effects of male, female, male:female, and male:female:treatment
(interested in assessing differences in compatibility caused by pH). My
response variable is poisson sperm count data. There are 18 blocks with 2
replicates per 2x male and female cross in each. Observation level random
effect added to account for overdispersion.

When I try and fit this full model:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
countsmodel <- glmer(Count_total ~ Chamber + (1|Block) + (1|Male) +
(1|Female) + (1|Male:Female) +
                     (1|Male:Female:Chamber) + (1|Sample), family =
"poisson", data = counts)

it gives a singular fit error, however when I run the isSingular function
on the model with default threshold (1x10^-5), it comes out false. Below is
the model summary output. The singular fit warning is probably coming from
the low variance of male:female random effect.

Generalized linear mixed model fit by maximum likelihood (Laplace
Approximation) ['glmerMod']
 Family: poisson  ( log )
Formula: Count_total ~ Chamber + (1 | Block) + (1 | Male) + (1 | Female) +
    (1 | Male:Female) + (1 | Male:Female:Chamber) + (1 | Sample)
   Data: counts

     AIC      BIC   logLik deviance df.resid
  2941.8   2971.1  -1462.9   2925.8      280

Scaled residuals:
       Min         1Q     Median         3Q        Max
-2.0097349 -0.1649262  0.0246676  0.1418016  0.6601243

Random effects:
 Groups              Name        Variance    Std.Dev.
 Sample              (Intercept) 9.98380e-02 3.15971e-01
 Male:Female:Chamber (Intercept) 1.36128e-02 1.16674e-01
 Male:Female         (Intercept) 5.92690e-09 7.69864e-05
 Female              (Intercept) 3.84069e-03 6.19733e-02
 Male                (Intercept) 9.82313e-02 3.13419e-01
 Block               (Intercept) 2.87611e-02 1.69591e-01
Number of obs: 288, groups:
Sample, 288; Male:Female:Chamber, 144; Male:Female, 72; Female, 36; Male,
36; Block, 18

Fixed effects:
              Estimate Std. Error  z value Pr(>|z|)
(Intercept) 4.54183984 0.07348953 61.80254  < 2e-16 ***
Chamberlow  0.00964211 0.04389841  0.21965  0.82615
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
           (Intr)
Chamberlow -0.299
convergence code: 0
boundary (singular) fit: see ?isSingular
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
My question is: can I interpret any findings from this model given the
singular fit warning? If not, what is a suitable approach?

Also, I asked this question before and tried to get help from local
statisticians, but they didn't know. Is it appropriate to assess the
significance of the random effects using likelihood ratio testing comparing
full model to reduced model one random effect at a time? For example, can I
test for significance of M:F two-way interaction, whilst leaving in the
male:female:treatment higher order three way interaction? Does the Drop1
approach apply to random effects in GLMMs?

Kind regards,
Jill

	[[alternative HTML version deleted]]


From w|||em@k@|j@er @end|ng |rom un|-due@de  Mon Feb  3 10:03:40 2020
From: w|||em@k@|j@er @end|ng |rom un|-due@de (Willem (Wim) Kaijser)
Date: Mon, 03 Feb 2020 10:03:40 +0100
Subject: [R-sig-ME] glmmTMB warnings and no output Gamma distribution
Message-ID: <baf694e4416c6ac00fd62158bcd6984c@webmailer.uni-duisburg-essen.de>

Hello, It was suggested to me to place my question in this mailing-list. 
I have some problems and lack of knowledge constructing a GLMM with the 
glmmTMB package (version 0.2.03). Therefore, I am sorry If the 
explanation is not as sufficient as it should be, and want to thank you 
in advance for your time.

I try to include both fixed plus random effects and correction for 
serial correlation in a GLMM with a Gamma distribution (link = 
"inverse"). However, I get several warnings and the processing time is 
long, so I prematurely stopped the permutations (see the code below). If 
the Gamma distribution, with ?log? link, is used in the model containing 
only the fixed and random effects; these warnings do not occur. 
Including this with the model correcting for serial correlation it does 
return a warning, but I can extract the residuals (AIC and BIC are 
returned as NA though). This seems like the following issue: 
https://github.com/glmmTMB/glmmTMB/issues/329. However, this seems to be 
resolved. What might be the issue here (is it my coding)?

############################################################
#Code with only the fixed and random effects with warnings:#
############################################################

mod1 <- glmTMB(Chla ~ pCO2 + (1|River/ID), family = ?Gamma?, data = df)

There were 19 warnings (use warnings() to see them)
Timing stopped at: 33.7 0.08 34.19

Warning messages:
1: In nlminb(start = par, objective = fn, gradient = gr,  ... :
   NA/NaN function evaluation

#######################################################################################
#Code with both the fixed, random and correction for serial correlation 
with warnings:#
#######################################################################################

mod2 <- glmTMB(Chla ~ pCO2 + (1|River/ID) + ar1(1|Month), family = 
?Gamma?, data = df)

There were 20 warnings (use warnings() to see them)
Timing stopped at: 24.71 0.08 25.49

Warning messages:
1: In FUN(X[[i]], ...) : AR1 not meaningful with intercept
2: In nlminb(start = par, objective = fn, gradient = gr,  ... :
   NA/NaN function evaluation

Best regards,
-- 
Willem (Wim) Kaijser

Fakult?t f?r Biologie
Aquatische ?kologie
Universit?tsstr. 5
D-45141 Essen

Room: S05T03B02
Tel: +49.201.183.3113


From mo|||eebrook@ @end|ng |rom gm@||@com  Wed Feb  5 17:00:38 2020
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Wed, 5 Feb 2020 17:00:38 +0100
Subject: [R-sig-ME] glmmTMB warnings and no output Gamma distribution
In-Reply-To: <baf694e4416c6ac00fd62158bcd6984c@webmailer.uni-duisburg-essen.de>
References: <baf694e4416c6ac00fd62158bcd6984c@webmailer.uni-duisburg-essen.de>
Message-ID: <52B1504D-A0AB-4E7C-8A16-61041FC4E7DB@gmail.com>

Hi Willem,


> On 3Feb 2020, at 10:03, Willem (Wim) Kaijser <willem.kaijser at uni-due.de> wrote:
> 
> Hello, It was suggested to me to place my question in this mailing-list. I have some problems and lack of knowledge constructing a GLMM with the glmmTMB package (version 0.2.03). Therefore, I am sorry If the explanation is not as sufficient as it should be, and want to thank you in advance for your time.
> 
> I try to include both fixed plus random effects and correction for serial correlation in a GLMM with a Gamma distribution (link = "inverse"). However, I get several warnings and the processing time is long, so I prematurely stopped the permutations (see the code below). If the Gamma distribution, with ?log? link, is used in the model containing only the fixed and random effects; these warnings do not occur. Including this with the model correcting for serial correlation it does return a warning, but I can extract the residuals (AIC and BIC are returned as NA though). This seems like the following issue: https://github.com/glmmTMB/glmmTMB/issues/329. However, this seems to be resolved. What might be the issue here (is it my coding)?
> 
> ############################################################
> #Code with only the fixed and random effects with warnings:#
> ############################################################
> 
> mod1 <- glmTMB(Chla ~ pCO2 + (1|River/ID), family = ?Gamma?, data = df)
> 
> There were 19 warnings (use warnings() to see them)
> Timing stopped at: 33.7 0.08 34.19
> 
> Warning messages:
> 1: In nlminb(start = par, objective = fn, gradient = gr,  ... :
>  NA/NaN function evaluation
> 
This coding looks correct, but can you say something about River and ID in the design. Are the labels for the levels of one reused in the levels of the other?

The default in glmmTMB is to use an inverse link with the Gamma family because that?s what glm() does in base R, but I don?t actually know why. Since the mean has to be positive, I would guess that a log link is a good thing to try, but maybe someone else can explain why inverse is the default. 

> #######################################################################################
> #Code with both the fixed, random and correction for serial correlation with warnings:#
> #######################################################################################
> 
> mod2 <- glmTMB(Chla ~ pCO2 + (1|River/ID) + ar1(1|Month), family = ?Gamma?, data = df)

This coding is not correct. Check out the issue you linked to. https://github.com/glmmTMB/glmmTMB/issues/329 <https://github.com/glmmTMB/glmmTMB/issues/329>

If Month is an integer, then you could use ar1(Month+0 | X) where X is probably ID, or possibly River depending on the design.  This also depends on if the experiment ran for more than 12 months, in which case, you do not want to repeat 1:12 in the second year if it is based on calendar months. In ar1(Time +0 | X), you need Time to continue counting even if Month repeats according to the calendar in the data. 

Cheers,
Mollie

> 
> There were 20 warnings (use warnings() to see them)
> Timing stopped at: 24.71 0.08 25.49
> 
> Warning messages:
> 1: In FUN(X[[i]], ...) : AR1 not meaningful with intercept
> 2: In nlminb(start = par, objective = fn, gradient = gr,  ... :
>  NA/NaN function evaluation
> 
> Best regards,
> -- 
> Willem (Wim) Kaijser
> 
> Fakult?t f?r Biologie
> Aquatische ?kologie
> Universit?tsstr. 5
> D-45141 Essen
> 
> Room: S05T03B02
> Tel: +49.201.183.3113
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Wed Feb  5 19:33:29 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 5 Feb 2020 13:33:29 -0500
Subject: [R-sig-ME] glmmTMB warnings and no output Gamma distribution
In-Reply-To: <52B1504D-A0AB-4E7C-8A16-61041FC4E7DB@gmail.com>
References: <baf694e4416c6ac00fd62158bcd6984c@webmailer.uni-duisburg-essen.de>
 <52B1504D-A0AB-4E7C-8A16-61041FC4E7DB@gmail.com>
Message-ID: <CABghstSzccLHbMB5y+FBDLDrP8n7tqJy214tMxZSY5qR0pufiw@mail.gmail.com>

   For what it's worth, the use of the inverse link as the default for
the Gamma is because it's the *canonical* link for that distribution -
this can be derived from the form of the distribution function, and
has some nice mathematical properties (see Wikipedia
https://en.wikipedia.org/wiki/Generalized_linear_model or any GLM
book).  It is indeed often less numerically stable than the log link,
which restricts the domain appropriately.

On Wed, Feb 5, 2020 at 11:01 AM Mollie Brooks <mollieebrooks at gmail.com> wrote:
>
> Hi Willem,
>
>
> > On 3Feb 2020, at 10:03, Willem (Wim) Kaijser <willem.kaijser at uni-due.de> wrote:
> >
> > Hello, It was suggested to me to place my question in this mailing-list. I have some problems and lack of knowledge constructing a GLMM with the glmmTMB package (version 0.2.03). Therefore, I am sorry If the explanation is not as sufficient as it should be, and want to thank you in advance for your time.
> >
> > I try to include both fixed plus random effects and correction for serial correlation in a GLMM with a Gamma distribution (link = "inverse"). However, I get several warnings and the processing time is long, so I prematurely stopped the permutations (see the code below). If the Gamma distribution, with ?log? link, is used in the model containing only the fixed and random effects; these warnings do not occur. Including this with the model correcting for serial correlation it does return a warning, but I can extract the residuals (AIC and BIC are returned as NA though). This seems like the following issue: https://github.com/glmmTMB/glmmTMB/issues/329. However, this seems to be resolved. What might be the issue here (is it my coding)?
> >
> > ############################################################
> > #Code with only the fixed and random effects with warnings:#
> > ############################################################
> >
> > mod1 <- glmTMB(Chla ~ pCO2 + (1|River/ID), family = ?Gamma?, data = df)
> >
> > There were 19 warnings (use warnings() to see them)
> > Timing stopped at: 33.7 0.08 34.19
> >
> > Warning messages:
> > 1: In nlminb(start = par, objective = fn, gradient = gr,  ... :
> >  NA/NaN function evaluation
> >
> This coding looks correct, but can you say something about River and ID in the design. Are the labels for the levels of one reused in the levels of the other?
>
> The default in glmmTMB is to use an inverse link with the Gamma family because that?s what glm() does in base R, but I don?t actually know why. Since the mean has to be positive, I would guess that a log link is a good thing to try, but maybe someone else can explain why inverse is the default.
>
> > #######################################################################################
> > #Code with both the fixed, random and correction for serial correlation with warnings:#
> > #######################################################################################
> >
> > mod2 <- glmTMB(Chla ~ pCO2 + (1|River/ID) + ar1(1|Month), family = ?Gamma?, data = df)
>
> This coding is not correct. Check out the issue you linked to. https://github.com/glmmTMB/glmmTMB/issues/329 <https://github.com/glmmTMB/glmmTMB/issues/329>
>
> If Month is an integer, then you could use ar1(Month+0 | X) where X is probably ID, or possibly River depending on the design.  This also depends on if the experiment ran for more than 12 months, in which case, you do not want to repeat 1:12 in the second year if it is based on calendar months. In ar1(Time +0 | X), you need Time to continue counting even if Month repeats according to the calendar in the data.
>
> Cheers,
> Mollie
>
> >
> > There were 20 warnings (use warnings() to see them)
> > Timing stopped at: 24.71 0.08 25.49
> >
> > Warning messages:
> > 1: In FUN(X[[i]], ...) : AR1 not meaningful with intercept
> > 2: In nlminb(start = par, objective = fn, gradient = gr,  ... :
> >  NA/NaN function evaluation
> >
> > Best regards,
> > --
> > Willem (Wim) Kaijser
> >
> > Fakult?t f?r Biologie
> > Aquatische ?kologie
> > Universit?tsstr. 5
> > D-45141 Essen
> >
> > Room: S05T03B02
> > Tel: +49.201.183.3113
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u  Thu Feb  6 00:37:09 2020
From: D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u (David Duffy)
Date: Wed, 5 Feb 2020 23:37:09 +0000
Subject: [R-sig-ME] threshold for singular fit
In-Reply-To: <CAB9nLEUEhOKR=U6uwYYdiXqA=aocRsf1LZ8cXn-UFEKHuAnm-g@mail.gmail.com>
References: <CAB9nLEUEhOKR=U6uwYYdiXqA=aocRsf1LZ8cXn-UFEKHuAnm-g@mail.gmail.com>
Message-ID: <c8f7f7af510c40c89be67af18d53c10d@qimrberghofer.edu.au>

Jill Brouwer asks:

> I am trying to fit a GLMM with fixed effect of pH treatment (Chamber), and
> random effects of male, female, male:female, and male:female:treatment
> (interested in assessing differences in compatibility caused by pH). My
> response variable is poisson sperm count data. There are 18 blocks with 2
> replicates per 2x male and female cross in each. Observation level random
> effect added to account for overdispersion.


> countsmodel <- glmer(Count_total ~ Chamber + (1|Block) + (1|Male) +
> (1|Female) + (1|Male:Female) +
>                    (1|Male:Female:Chamber) + (1|Sample), family =
> "poisson", data = counts)

> it gives a singular fit error, however when I run the isSingular function
> Generalized linear mixed model fit by maximum likelihood (Laplace

> My question is: can I interpret any findings from this model given the
> singular fit warning? If not, what is a suitable approach?

You _might_ obtain different results using other than the Laplace approximation. I have been trying the 
the glmmsr package that allows 4 alternatives (importance sampler, sequential reduction, AGQ, Laplace).

> Also, I asked this question before and tried to get help from local
> statisticians, but they didn't know. Is it appropriate to assess the
> significance of the random effects using likelihood ratio testing comparing
> full model to reduced model one random effect at a time? 

Yes, but the distribution of the test statistic is usually some complex mixture of chi-squares. There are packages that can simulate these - or you can do it yourself. You can also just use a conservative ad hoc cutoff. The other way is to go to MCMC packages, one obvious choice being MCMCglmm.

2c, David Duffy.


From bbo|ker @end|ng |rom gm@||@com  Thu Feb  6 02:40:54 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 5 Feb 2020 20:40:54 -0500
Subject: [R-sig-ME] threshold for singular fit
In-Reply-To: <c8f7f7af510c40c89be67af18d53c10d@qimrberghofer.edu.au>
References: <CAB9nLEUEhOKR=U6uwYYdiXqA=aocRsf1LZ8cXn-UFEKHuAnm-g@mail.gmail.com>
 <c8f7f7af510c40c89be67af18d53c10d@qimrberghofer.edu.au>
Message-ID: <b0d161f9-2a9c-c9ad-f877-ab614f39de56@gmail.com>


  Agree with David's points below.  A couple of points:

  * the GLMMadaptive package provides an alternative way to fit
non-Laplace models (AGQ)
  * that said, I'd be surprised if different approximations did away
with the singularity problem.  You've got a medium-sized data set (I'm
not sure whether you have 8 or 4 observations per block, but at most 144
observations), and you're trying to partition the variability into 6
components (plus the intrinsic Poisson variability).
  * it's true that the null distribution of the differences in
log-likelihoods is complicated, but in those cases where one of the
variance terms is estimated as zero it's unlikely (impossible?) that it
will be significant no matter what you do ...

  Parametric bootstrapping via PBmodcomp() in the pbkrtest package will
be more or less the gold standard for frequentist testing of
random-effect variances (it'll be slow, since it requires refitting
models hundreds of times per model comparison).
   You might try a few PBmodcomp() tests and compare them against the
likelihood ratio test results to see how much difference it makes/how
much it's worth the hassle of the extra computation.

  Bayesian methods will allow you to add regularizing terms to avoid
singular fits, but will also complicate your task of computing p-values ...

On 2020-02-05 6:37 p.m., David Duffy wrote:
> Jill Brouwer asks:
> 
>> I am trying to fit a GLMM with fixed effect of pH treatment (Chamber), and
>> random effects of male, female, male:female, and male:female:treatment
>> (interested in assessing differences in compatibility caused by pH). My
>> response variable is poisson sperm count data. There are 18 blocks with 2
>> replicates per 2x male and female cross in each. Observation level random
>> effect added to account for overdispersion.
> 
>> countsmodel <- glmer(Count_total ~ Chamber + (1|Block) + (1|Male) +
>> (1|Female) + (1|Male:Female) +
>>                    (1|Male:Female:Chamber) + (1|Sample), family =
>> "poisson", data = counts)
> 
>> it gives a singular fit error, however when I run the isSingular function
>> Generalized linear mixed model fit by maximum likelihood (Laplace
> 
>> My question is: can I interpret any findings from this model given the
>> singular fit warning? If not, what is a suitable approach?
> 
> You _might_ obtain different results using other than the Laplace approximation. I have been trying the 
> the glmmsr package that allows 4 alternatives (importance sampler, sequential reduction, AGQ, Laplace).
> 
>> Also, I asked this question before and tried to get help from local
>> statisticians, but they didn't know. Is it appropriate to assess the
>> significance of the random effects using likelihood ratio testing comparing
>> full model to reduced model one random effect at a time? 
> 
> Yes, but the distribution of the test statistic is usually some complex mixture of chi-squares. There are packages that can simulate these - or you can do it yourself. You can also just use a conservative ad hoc cutoff. The other way is to go to MCMC packages, one obvious choice being MCMCglmm.
> 
> 2c, David Duffy.
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From @ouhey|@@ghebghoub @end|ng |rom gm@||@com  Fri Feb  7 11:25:47 2020
From: @ouhey|@@ghebghoub @end|ng |rom gm@||@com (Souheyla GHEBGHOUB)
Date: Fri, 7 Feb 2020 10:25:47 +0000
Subject: [R-sig-ME] glmer not supported class for vuong()closeness test
 (non-nested models)
Message-ID: <CAEA998guMb0e4tW0qdcwtE9poMXqTxkpzs+iPkq_VeJAWh5cMg@mail.gmail.com>

Hi,

I need to compare non-nested models fitted to the same DV using vuong()
closeness test in pscl package. It appears that the test only supports glm
class but not mixed models glmer.  I would like to retain glmer because my
random effects explain much of the variation but vuong does not support it!

Should I simply ignore the random effects at this point since they will be
equally removed from both compared models in this sig test, is there a way
to force the test to support this glmer class?

Thanks a lot for your time reading/answering this,

Souheyla GHEBGHOUB
4th year PhD student
Education department - Research Centre for Social Science
University of York

	[[alternative HTML version deleted]]


From mo|||eebrook@ @end|ng |rom gm@||@com  Fri Feb  7 11:57:46 2020
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Fri, 7 Feb 2020 11:57:46 +0100
Subject: [R-sig-ME] glmer not supported class for vuong()closeness test
 (non-nested models)
In-Reply-To: <CAEA998guMb0e4tW0qdcwtE9poMXqTxkpzs+iPkq_VeJAWh5cMg@mail.gmail.com>
References: <CAEA998guMb0e4tW0qdcwtE9poMXqTxkpzs+iPkq_VeJAWh5cMg@mail.gmail.com>
Message-ID: <93ED9E80-9BDF-4C24-B57F-189D6226FCF1@gmail.com>

If you want to use vuong() to test for zero inflation, then an alternative would be to use testZeroinflation() from the DHARMa package which supports glmer models. It performs a different test using simulated residuals.

See documentation here
https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html <https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html>

Cheers,
Mollie


> On 7Feb 2020, at 11:25, Souheyla GHEBGHOUB <souheyla.ghebghoub at gmail.com> wrote:
> 
> Hi,
> 
> I need to compare non-nested models fitted to the same DV using vuong()
> closeness test in pscl package. It appears that the test only supports glm
> class but not mixed models glmer.  I would like to retain glmer because my
> random effects explain much of the variation but vuong does not support it!
> 
> Should I simply ignore the random effects at this point since they will be
> equally removed from both compared models in this sig test, is there a way
> to force the test to support this glmer class?
> 
> Thanks a lot for your time reading/answering this,
> 
> Souheyla GHEBGHOUB
> 4th year PhD student
> Education department - Research Centre for Social Science
> University of York
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From @ouhey|@@ghebghoub @end|ng |rom gm@||@com  Fri Feb  7 12:17:18 2020
From: @ouhey|@@ghebghoub @end|ng |rom gm@||@com (Souheyla GHEBGHOUB)
Date: Fri, 7 Feb 2020 11:17:18 +0000
Subject: [R-sig-ME] glmer not supported class for vuong()closeness test
 (non-nested models)
In-Reply-To: <93ED9E80-9BDF-4C24-B57F-189D6226FCF1@gmail.com>
References: <CAEA998guMb0e4tW0qdcwtE9poMXqTxkpzs+iPkq_VeJAWh5cMg@mail.gmail.com>
 <93ED9E80-9BDF-4C24-B57F-189D6226FCF1@gmail.com>
Message-ID: <CAEA998jzHjdBgks4CtY3o5bXvpujzHNAbx+dO3qQjrDLw8=uOw@mail.gmail.com>

Hi Mollie,

Thank you for your reply.
I want to test whether 3 predictors are significantly different from each
other when fitted to the same data but in different models by basically
comparing the models, like when we do the likelihood ratio test but that
one fits for nested models only.

Best,
Souheyla

On Fri, 7 Feb 2020 at 10:57, Mollie Brooks <mollieebrooks at gmail.com> wrote:

> If you want to use vuong() to test for zero inflation, then an alternative
> would be to use testZeroinflation() from the DHARMa package which supports
> glmer models. It performs a different test using simulated residuals.
>
> See documentation here
> https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html
>
> Cheers,
> Mollie
>
>
> On 7Feb 2020, at 11:25, Souheyla GHEBGHOUB <souheyla.ghebghoub at gmail.com>
> wrote:
>
> Hi,
>
> I need to compare non-nested models fitted to the same DV using vuong()
> closeness test in pscl package. It appears that the test only supports glm
> class but not mixed models glmer.  I would like to retain glmer because my
> random effects explain much of the variation but vuong does not support it!
>
> Should I simply ignore the random effects at this point since they will be
> equally removed from both compared models in this sig test, is there a way
> to force the test to support this glmer class?
>
> Thanks a lot for your time reading/answering this,
>
> Souheyla GHEBGHOUB
> 4th year PhD student
> Education department - Research Centre for Social Science
> University of York
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>

	[[alternative HTML version deleted]]


From mo|||eebrook@ @end|ng |rom gm@||@com  Fri Feb  7 12:33:22 2020
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Fri, 7 Feb 2020 12:33:22 +0100
Subject: [R-sig-ME] glmer not supported class for vuong()closeness test
 (non-nested models)
In-Reply-To: <CAEA998jzHjdBgks4CtY3o5bXvpujzHNAbx+dO3qQjrDLw8=uOw@mail.gmail.com>
References: <CAEA998guMb0e4tW0qdcwtE9poMXqTxkpzs+iPkq_VeJAWh5cMg@mail.gmail.com>
 <93ED9E80-9BDF-4C24-B57F-189D6226FCF1@gmail.com>
 <CAEA998jzHjdBgks4CtY3o5bXvpujzHNAbx+dO3qQjrDLw8=uOw@mail.gmail.com>
Message-ID: <0628CF7C-7F43-448A-9D1A-747B7317D10E@gmail.com>

Sorry about that. My mistake. I wasn?t familiar with the general use.

Mollie

> On 7Feb 2020, at 12:17, Souheyla GHEBGHOUB <souheyla.ghebghoub at gmail.com> wrote:
> 
> Hi Mollie,
> 
> Thank you for your reply. 
> I want to test whether 3 predictors are significantly different from each other when fitted to the same data but in different models by basically comparing the models, like when we do the likelihood ratio test but that one fits for nested models only.
> 
> Best,
> Souheyla
> 
> On Fri, 7 Feb 2020 at 10:57, Mollie Brooks <mollieebrooks at gmail.com <mailto:mollieebrooks at gmail.com>> wrote:
> If you want to use vuong() to test for zero inflation, then an alternative would be to use testZeroinflation() from the DHARMa package which supports glmer models. It performs a different test using simulated residuals.
> 
> See documentation here
> https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html <https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html>
> 
> Cheers,
> Mollie
> 
> 
>> On 7Feb 2020, at 11:25, Souheyla GHEBGHOUB <souheyla.ghebghoub at gmail.com <mailto:souheyla.ghebghoub at gmail.com>> wrote:
>> 
>> Hi,
>> 
>> I need to compare non-nested models fitted to the same DV using vuong()
>> closeness test in pscl package. It appears that the test only supports glm
>> class but not mixed models glmer.  I would like to retain glmer because my
>> random effects explain much of the variation but vuong does not support it!
>> 
>> Should I simply ignore the random effects at this point since they will be
>> equally removed from both compared models in this sig test, is there a way
>> to force the test to support this glmer class?
>> 
>> Thanks a lot for your time reading/answering this,
>> 
>> Souheyla GHEBGHOUB
>> 4th year PhD student
>> Education department - Research Centre for Social Science
>> University of York
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org> mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
> 


	[[alternative HTML version deleted]]


From @ouhey|@@ghebghoub @end|ng |rom gm@||@com  Fri Feb  7 12:43:27 2020
From: @ouhey|@@ghebghoub @end|ng |rom gm@||@com (Souheyla GHEBGHOUB)
Date: Fri, 7 Feb 2020 11:43:27 +0000
Subject: [R-sig-ME] glmer not supported class for vuong()closeness test
 (non-nested models)
In-Reply-To: <0628CF7C-7F43-448A-9D1A-747B7317D10E@gmail.com>
References: <CAEA998guMb0e4tW0qdcwtE9poMXqTxkpzs+iPkq_VeJAWh5cMg@mail.gmail.com>
 <93ED9E80-9BDF-4C24-B57F-189D6226FCF1@gmail.com>
 <CAEA998jzHjdBgks4CtY3o5bXvpujzHNAbx+dO3qQjrDLw8=uOw@mail.gmail.com>
 <0628CF7C-7F43-448A-9D1A-747B7317D10E@gmail.com>
Message-ID: <CAEA998hOtkq_tRiDhOOa4h49Z+ptDao6UO2yi0GpxBCHWP_v=w@mail.gmail.com>

No problem, thanks! I hope someone has an answer to this.

Best,
Souheyla

On Fri, 7 Feb 2020 at 11:33, Mollie Brooks <mollieebrooks at gmail.com> wrote:

> Sorry about that. My mistake. I wasn?t familiar with the general use.
>
> Mollie
>
> On 7Feb 2020, at 12:17, Souheyla GHEBGHOUB <souheyla.ghebghoub at gmail.com>
> wrote:
>
> Hi Mollie,
>
> Thank you for your reply.
> I want to test whether 3 predictors are significantly different from each
> other when fitted to the same data but in different models by basically
> comparing the models, like when we do the likelihood ratio test but that
> one fits for nested models only.
>
> Best,
> Souheyla
>
> On Fri, 7 Feb 2020 at 10:57, Mollie Brooks <mollieebrooks at gmail.com>
> wrote:
>
>> If you want to use vuong() to test for zero inflation, then an
>> alternative would be to use testZeroinflation() from the DHARMa package
>> which supports glmer models. It performs a different test using simulated
>> residuals.
>>
>> See documentation here
>> https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html
>>
>> Cheers,
>> Mollie
>>
>>
>> On 7Feb 2020, at 11:25, Souheyla GHEBGHOUB <souheyla.ghebghoub at gmail.com>
>> wrote:
>>
>> Hi,
>>
>> I need to compare non-nested models fitted to the same DV using vuong()
>> closeness test in pscl package. It appears that the test only supports glm
>> class but not mixed models glmer.  I would like to retain glmer because my
>> random effects explain much of the variation but vuong does not support
>> it!
>>
>> Should I simply ignore the random effects at this point since they will be
>> equally removed from both compared models in this sig test, is there a way
>> to force the test to support this glmer class?
>>
>> Thanks a lot for your time reading/answering this,
>>
>> Souheyla GHEBGHOUB
>> 4th year PhD student
>> Education department - Research Centre for Social Science
>> University of York
>>
>> [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>>
>

	[[alternative HTML version deleted]]


From D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u  Sat Feb  8 11:50:49 2020
From: D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u (David Duffy)
Date: Sat, 8 Feb 2020 10:50:49 +0000
Subject: [R-sig-ME] glmer not supported class for vuong()closeness test
 (non-nested models)
In-Reply-To: <CAEA998guMb0e4tW0qdcwtE9poMXqTxkpzs+iPkq_VeJAWh5cMg@mail.gmail.com>
References: <CAEA998guMb0e4tW0qdcwtE9poMXqTxkpzs+iPkq_VeJAWh5cMg@mail.gmail.com>
Message-ID: <ac48d2e6f3314eb3a27361c873ec24e6@qimrberghofer.edu.au>

> I need to compare non-nested models fitted to the same DV using vuong()
> closeness test in pscl package. It appears that the test only supports glm
> class but not mixed models glmer.  I would like to retain glmer because my
> random effects explain much of the variation but vuong does not support it!

The examples for vuongtest() in the nonnest2 package include one comparing lmer models - I haven't used it.

From w|||em@k@|j@er @end|ng |rom un|-due@de  Wed Feb  5 17:58:53 2020
From: w|||em@k@|j@er @end|ng |rom un|-due@de (Willem (Wim) Kaijser)
Date: Wed, 05 Feb 2020 17:58:53 +0100
Subject: [R-sig-ME] glmmTMB warnings and no output Gamma distribution
In-Reply-To: <52B1504D-A0AB-4E7C-8A16-61041FC4E7DB@gmail.com>
References: <baf694e4416c6ac00fd62158bcd6984c@webmailer.uni-duisburg-essen.de>
 <52B1504D-A0AB-4E7C-8A16-61041FC4E7DB@gmail.com>
Message-ID: <21dabe4d747664345b052b9d637d2213@webmailer.uni-duisburg-essen.de>

Can you say something about River and ID in the design. Are the labels 
for the levels of one reused in the levels of the other?

I have 60 rivers and in each river an unique ID (sample location) 
returns. Thus, ID does not re-occur in other rivers and is unique to a 
river. However, it can occur that one river only has one ID and others 
10.
---
Willem (Wim) Kaijser

Fakult?t f?r Biologie
Aquatische ?kologie
Universit?tsstr. 5
D-45141 Essen

Room: S05T03B02
Tel: +49.201.183.3113

Mollie Brooks schreef op 05.02.2020 17:00:
> Hi Willem,
> 
>> On 3Feb 2020, at 10:03, Willem (Wim) Kaijser
>> <willem.kaijser at uni-due.de> wrote:
>> 
>> Hello, It was suggested to me to place my question in this
>> mailing-list. I have some problems and lack of knowledge
>> constructing a GLMM with the glmmTMB package (version 0.2.03).
>> Therefore, I am sorry If the explanation is not as sufficient as it
>> should be, and want to thank you in advance for your time.
>> 
>> I try to include both fixed plus random effects and correction for
>> serial correlation in a GLMM with a Gamma distribution (link =
>> "inverse"). However, I get several warnings and the processing time
>> is long, so I prematurely stopped the permutations (see the code
>> below). If the Gamma distribution, with ?log? link, is used in
>> the model containing only the fixed and random effects; these
>> warnings do not occur. Including this with the model correcting for
>> serial correlation it does return a warning, but I can extract the
>> residuals (AIC and BIC are returned as NA though). This seems like
>> the following issue: https://github.com/glmmTMB/glmmTMB/issues/329
>> [1]. However, this seems to be resolved. What might be the issue
>> here (is it my coding)?
> 
>> ############################################################
>> #Code with only the fixed and random effects with warnings:#
>> ############################################################
>> 
>> mod1 <- glmTMB(Chla ~ pCO2 + (1|River/ID), family = ?Gamma?,
>> data = df)
>> 
>> There were 19 warnings (use warnings() to see them)
>> Timing stopped at: 33.7 0.08 34.19
>> 
>> Warning messages:
>> 1: In nlminb(start = par, objective = fn, gradient = gr, ... :
>> NA/NaN function evaluation
> 
> This coding looks correct, but can you say something about River and
> ID in the design. Are the labels for the levels of one reused in the
> levels of the other?
> The default in glmmTMB is to use an inverse link with the Gamma family
> because that?s what glm() does in base R, but I don?t actually
> know why. Since the mean has to be positive, I would guess that a log
> link is a good thing to try, but maybe someone else can explain why
> inverse is the default.
> 
>> 
> #######################################################################################
>> #Code with both the fixed, random and correction for serial
>> correlation with warnings:#
>> 
> #######################################################################################
>> 
>> mod2 <- glmTMB(Chla ~ pCO2 + (1|River/ID) + ar1(1|Month), family =
>> ?Gamma?, data = df)
> 
> This coding is not correct. Check out the issue you linked to.
> https://github.com/glmmTMB/glmmTMB/issues/329 [1]
> 
> If Month is an integer, then you could use ar1(Month+0 | X) where X is
> probably ID, or possibly River depending on the design. This also
> depends on if the experiment ran for more than 12 months, in which
> case, you do not want to repeat 1:12 in the second year if it is based
> on calendar months. In ar1(Time +0 | X), you need Time to continue
> counting even if Month repeats according to the calendar in the data.
> 
> Cheers,
> Mollie
> 
>> There were 20 warnings (use warnings() to see them)
>> Timing stopped at: 24.71 0.08 25.49
>> 
>> Warning messages:
>> 1: In FUN(X[[i]], ...) : AR1 not meaningful with intercept
>> 2: In nlminb(start = par, objective = fn, gradient = gr, ... :
>> NA/NaN function evaluation
>> 
>> Best regards,
>> --
>> Willem (Wim) Kaijser
>> 
>> Fakult?t f?r Biologie
>> Aquatische ?kologie
>> Universit?tsstr. 5
>> D-45141 Essen
>> 
>> Room: S05T03B02
>> Tel: +49.201.183.3113
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models [2]
> 
> 
> 
> Links:
> ------
> [1] https://github.com/glmmTMB/glmmTMB/issues/329
> [2] https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From mtonc|c @end|ng |rom ||r|@hr  Sat Feb  8 20:49:58 2020
From: mtonc|c @end|ng |rom ||r|@hr (marKo)
Date: Sat, 8 Feb 2020 20:49:58 +0100
Subject: [R-sig-ME] family and link function literature (glmer,
 and other usage)
Message-ID: <9ab99a80-b1b3-1f13-afdd-a6e16dbe7be5@ffri.hr>

Hi,

Does someone have to suggest a good source/literature regarding 
distribution families and link functions?

Thanks,


Marko


From bbo|ker @end|ng |rom gm@||@com  Sun Feb  9 15:02:09 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sun, 9 Feb 2020 09:02:09 -0500
Subject: [R-sig-ME] family and link function literature (glmer,
 and other usage)
In-Reply-To: <9ab99a80-b1b3-1f13-afdd-a6e16dbe7be5@ffri.hr>
References: <9ab99a80-b1b3-1f13-afdd-a6e16dbe7be5@ffri.hr>
Message-ID: <df8a7f83-baab-54ee-ee0c-8cd7c98286b8@gmail.com>


 Not really a mixed-models question, but ...  Any decent generalized
linear models book should say something about this (McCullagh and Nelder
is the classic, also Dobson and Barnett, Faraway ...)  Were you looking
for something deeper/more specific?

On 2020-02-08 2:49 p.m., marKo via R-sig-mixed-models wrote:
> Hi,
> 
> Does someone have to suggest a good source/literature regarding
> distribution families and link functions?
> 
> Thanks,
> 
> 
> Marko
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From ch|r|eu @end|ng |rom gm@||@com  Mon Feb 10 09:26:35 2020
From: ch|r|eu @end|ng |rom gm@||@com (=?UTF-8?Q?David_Villegas_R=C3=ADos?=)
Date: Mon, 10 Feb 2020 09:26:35 +0100
Subject: [R-sig-ME] unable to predict from GAM model after warning on
 complete separation
Message-ID: <CALC46t9zswT7q=KPdyNnXc+eZfDHqb+_q0OPuy6Y5cGf6+dONQ@mail.gmail.com>

Dear list.
I?m fitting some logistic bam models, and for some of them I get the
following warning:

Warning message:
In bgam.fitd(G, mf, gp, scale, nobs.extra = 0, rho = rho, coef = coef,  :
  fitted probabilities numerically 0 or 1 occurred

However the model seems to converge and the summary(), plot() and gam.check
() functions produce apparently the right results.

My objective is to make some predictions using those models, but when I get
the warning above, the predict.bam is not working and yields the following
error:

Error in Predict.matrix.tprs.smooth(object, dk$data) :
  no data to predict at

Is there any workaround to solve this issue and get the predictions, or
predictions are simply not possible from models where probabilities
numerically 0 or 1 occur? I have no problem predicting from models that do
not produce the warning.

Thanks,

David

	[[alternative HTML version deleted]]


From ph||||p@@|d@y @end|ng |rom mp|@n|  Tue Feb 11 20:43:09 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Tue, 11 Feb 2020 20:43:09 +0100
Subject: [R-sig-ME] Error with nlme and varFixed
In-Reply-To: <e8dcd82ce7d84dad7405a1f4fc6627be@mail.gmail.com>
References: <e8dcd82ce7d84dad7405a1f4fc6627be@mail.gmail.com>
Message-ID: <a8d5323c-d8e2-d68a-510c-3be60f189da8@mpi.nl>

This may be a bug -- report it at https://bugs.r-project.org/bugzilla/
(but check to make sure a related bug hasn't already been filed).

Phillip

On 29/01/2020 21:17, Bill Denney wrote:
> Hello,
>
>
>
> When trying to fit an NLME model using a vector of varFixed, I get an error
> that appears to be related to a multiplication issue at this line of code:
>
> https://github.com/cran/nlme/blob/c006dfa23ad390948a74e67978a9828a6d60d89b/R/varFunc.R#L169
>
>
>
> Is the below a bug or am I inaccurately applying varFixed (or something
> else)?
>
>
>
> Here is a reproducible example:
>
>
>
> library(nlme)
>
>
>
> d <-
>
>   data.frame(
>
>     obs=rnorm(n=97), # 97 chosen because it's prime and therefore can't be
> the size of a rectangular matrix
>
>     groups=rep(c("A", "B"), each=50)[1:97],
>
>     wt=abs(rnorm(n=97))
>
>   )
>
>
>
> nlme(
>
>   obs~b,
>
>   fixed=b~1,
>
>   random=b~1|groups,
>
>   weights=varFixed(~wt),
>
>  start=c(b=0),
>
>   data=d
>
> )
>
>
>
> Which gets the error:
>
>
>
> Error in recalc.varFunc(object[[i]], conLin) :
>
>   dims [product 12] do not match the length of object [97]
>
> In addition: Warning message:
>
> In conLin$Xy * varWeights(object) :
>
>   longer object length is not a multiple of shorter object length
>
>
>
> Thanks,
>
>
>
> Bill
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From biii m@iii@g oii de@@ey@ws  Tue Feb 11 20:53:45 2020
From: biii m@iii@g oii de@@ey@ws (biii m@iii@g oii de@@ey@ws)
Date: Tue, 11 Feb 2020 14:53:45 -0500
Subject: [R-sig-ME] Error with nlme and varFixed
In-Reply-To: <a8d5323c-d8e2-d68a-510c-3be60f189da8@mpi.nl>
References: <e8dcd82ce7d84dad7405a1f4fc6627be@mail.gmail.com>
 <a8d5323c-d8e2-d68a-510c-3be60f189da8@mpi.nl>
Message-ID: <05fd01d5e114$f89a6080$e9cf2180$@denney.ws>

Hi Phillip,

Thanks for the confirmation.  I searched and didn't see one line it.  I've submitted it here: https://bugs.r-project.org/bugzilla/show_bug.cgi?id=17712

Thanks,

Bill

-----Original Message-----
From: Phillip Alday <phillip.alday at mpi.nl> 
Sent: Tuesday, February 11, 2020 2:43 PM
To: Bill Denney <wdenney at humanpredictions.com>; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Error with nlme and varFixed

This may be a bug -- report it at https://bugs.r-project.org/bugzilla/
(but check to make sure a related bug hasn't already been filed).

Phillip

On 29/01/2020 21:17, Bill Denney wrote:
> Hello,
>
> When trying to fit an NLME model using a vector of varFixed, I get an 
> error that appears to be related to a multiplication issue at this line of code:
>
> https://github.com/cran/nlme/blob/c006dfa23ad390948a74e67978a9828a6d60d89b/R/varFunc.R#L169
>
> Is the below a bug or am I inaccurately applying varFixed (or 
> something else)?
>
> Here is a reproducible example:
>
> library(nlme)
>
> d <-
>   data.frame(
>     obs=rnorm(n=97), # 97 chosen because it's prime and therefore  can't be the size of a rectangular matrix
>     groups=rep(c("A", "B"), each=50)[1:97],
>     wt=abs(rnorm(n=97))
>   )
>
> nlme(
>   obs~b,
>   fixed=b~1,
>   random=b~1|groups,
>   weights=varFixed(~wt),
>   start=c(b=0),
>   data=d
> )
>
> Which gets the error:
>
> Error in recalc.varFunc(object[[i]], conLin) :
>   dims [product 12] do not match the length of object [97]
> In addition: Warning message:
> In conLin$Xy * varWeights(object) :
>   longer object length is not a multiple of shorter object length
>
> Thanks,
>
> Bill


From b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com  Wed Feb 12 02:09:28 2020
From: b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com (Alessandra Bielli)
Date: Tue, 11 Feb 2020 19:09:28 -0600
Subject: [R-sig-ME] Fwd: model check for negative binomial model
In-Reply-To: <mailman.18062.3142.1581469252.1419.r-sig-mixed-models@r-project.org>
References: <mailman.18062.3142.1581469252.1419.r-sig-mixed-models@r-project.org>
Message-ID: <CA+6N3yV8ytczPND5x1VS5Vfe+H748vqjmc4=79wTnbJb1JmSbA@mail.gmail.com>

Dear list

I am fitting a poisson model to estimate the effect of a treatment on
emergence success of hatchlings. To estimate emergence success, I use
number of emerged and an offset(log(total clutch).

However, overdispersion was detected:

> overdisp_fun(m.emerged) #overdispersion detected

      chisq       ratio         rdf           p
3490.300836    5.684529  614.000000    0.000000

Therefore, I switched to a negative binomial. I know overdispersion is not
relevant for nb models, but the model plots don't look too good. I also
tried to fit a poisson model with OLRE, but still the  plots don't look
good.
How do I know if my model is good enough, and what can I do to improve it?

> summary(m.emerged)
Generalized linear mixed model fit by maximum likelihood (Laplace
Approximation) ['glmerMod']
 Family: Negative Binomial(7.604)  ( log )
Formula: Hatched ~ Relocation..Y.N. + SP + offset(log(Total_Clutch)) + (1
|Beach_ID) + (1 | Year)
   Data: main

     AIC      BIC   logLik deviance df.resid
  6015.6   6042.2  -3001.8   6003.6      614

Scaled residuals:
    Min      1Q  Median      3Q     Max
-2.6427 -0.3790  0.1790  0.5242  1.6583

Random effects:
 Groups   Name        Variance Std.Dev.
 Beach_ID (Intercept) 0.004438 0.06662
 Year     (Intercept) 0.001640 0.04050
Number of obs: 620, groups:  Beach_ID, 8; Year, 5

Fixed effects:
                  Estimate Std. Error z value Pr(>|z|)
(Intercept)       -0.29915    0.04055  -7.377 1.62e-13 ***
Relocation..Y.N.Y -0.16402    0.05052  -3.247  0.00117 **
SPL               -0.08311    0.04365  -1.904  0.05689 .
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
            (Intr) R..Y.N
Rlct..Y.N.Y -0.114
SPL         -0.497 -0.054


Thanks for your help,

Alessandra

-------------- next part --------------
A non-text attachment was scrubbed...
Name: image.png
Type: image/png
Size: 65277 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20200211/4946bfa0/attachment-0001.png>

From bbo|ker @end|ng |rom gm@||@com  Wed Feb 12 02:29:11 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 11 Feb 2020 20:29:11 -0500
Subject: [R-sig-ME] Fwd: model check for negative binomial model
In-Reply-To: <CA+6N3yV8ytczPND5x1VS5Vfe+H748vqjmc4=79wTnbJb1JmSbA@mail.gmail.com>
References: <mailman.18062.3142.1581469252.1419.r-sig-mixed-models@r-project.org>
 <CA+6N3yV8ytczPND5x1VS5Vfe+H748vqjmc4=79wTnbJb1JmSbA@mail.gmail.com>
Message-ID: <3cd5cea8-28c9-0799-9f17-d80a4aaa1ccc@gmail.com>


  Short answer: if emergence success gets much above 50%, then the
approximation you're making (Poisson + offset for binomial, or NB +
offset for negative binomial) doesn't work well.  You might try a
beta-binomial (with glmmTMB) or a binomial + an observation-level random
effect.

  (On the other hand, your intercept is -0.3, which corresponds to a
baseline emergence of 0.42 - not *very* high (but some beaches and years
will be well above that ...)

  Beyond that, are there any obvious patterns of mis-fit in the
predicted values ... ?

On 2020-02-11 8:09 p.m., Alessandra Bielli wrote:
> Dear list
> 
> I am fitting a poisson model to estimate the effect of a treatment on
> emergence success of hatchlings. To estimate emergence success, I use
> number of emerged and an offset(log(total clutch).
> 
> However, overdispersion was detected:
> 
>> overdisp_fun(m.emerged) #overdispersion detected
> 
>       chisq       ratio         rdf           p
> 3490.300836    5.684529  614.000000    0.000000
> 
> Therefore, I switched to a negative binomial. I know overdispersion is not
> relevant for nb models, but the model plots don't look too good. I also
> tried to fit a poisson model with OLRE, but still the  plots don't look
> good.
> How do I know if my model is good enough, and what can I do to improve it?
> 
>> summary(m.emerged)
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: Negative Binomial(7.604)  ( log )
> Formula: Hatched ~ Relocation..Y.N. + SP + offset(log(Total_Clutch)) + (1
> |Beach_ID) + (1 | Year)
>    Data: main
> 
>      AIC      BIC   logLik deviance df.resid
>   6015.6   6042.2  -3001.8   6003.6      614
> 
> Scaled residuals:
>     Min      1Q  Median      3Q     Max
> -2.6427 -0.3790  0.1790  0.5242  1.6583
> 
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  Beach_ID (Intercept) 0.004438 0.06662
>  Year     (Intercept) 0.001640 0.04050
> Number of obs: 620, groups:  Beach_ID, 8; Year, 5
> 
> Fixed effects:
>                   Estimate Std. Error z value Pr(>|z|)
> (Intercept)       -0.29915    0.04055  -7.377 1.62e-13 ***
> Relocation..Y.N.Y -0.16402    0.05052  -3.247  0.00117 **
> SPL               -0.08311    0.04365  -1.904  0.05689 .
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Correlation of Fixed Effects:
>             (Intr) R..Y.N
> Rlct..Y.N.Y -0.114
> SPL         -0.497 -0.054
> 
> 
> Thanks for your help,
> 
> Alessandra
> 
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From h|gh@t@t @end|ng |rom h|gh@t@t@com  Wed Feb 12 11:30:26 2020
From: h|gh@t@t @end|ng |rom h|gh@t@t@com (Highland Statistics Ltd)
Date: Wed, 12 Feb 2020 11:30:26 +0100
Subject: [R-sig-ME] Course: Zero-inflated models using R-INLA
Message-ID: <d317df87-66f6-2fed-02f9-3f9ea2d9c746@highstat.com>


We would like to announce the following statistics course.

Course: Introduction to zero-inflated models using R-INLA

Where and when: NAIT, Edmonton, Canada. 23 - 26 March 2020

Course website: http://highstat.com/index.php/courses-upcoming
Course flyer: 
http://highstat.com/Courses/Flyers/2020/Flyer2020_03NAIT_ZI.pdf

Kind regards,


Alain Zuur

-- 

Dr. Alain F. Zuur
Highland Statistics Ltd.
9 St Clair Wynd
AB41 6DZ Newburgh, UK
Email: highstat at highstat.com
URL: www.highstat.com


	[[alternative HTML version deleted]]


From morg@ne@br@chet @end|ng |rom hotm@||@com  Wed Feb 12 10:32:04 2020
From: morg@ne@br@chet @end|ng |rom hotm@||@com (Morgane Brachet)
Date: Wed, 12 Feb 2020 09:32:04 +0000
Subject: [R-sig-ME] zero one inflated beta mixed model
Message-ID: <PR3P194MB0521F92A8B361269C9E18FB0F71B0@PR3P194MB0521.EURP194.PROD.OUTLOOK.COM>

Hello,

I am writing to you following one of the posts on GitHub (https://github.com/glmmTMB/glmmTMB/issues/355). I am trying to fit proportion data with lots of 0s and a few 1s into a hurdle model using glmmTMB. Is this possible? Would you have any example code please?

Thank you!

Morgane
[https://avatars1.githubusercontent.com/u/13640228?s=400&v=4]<https://github.com/glmmTMB/glmmTMB/issues/355>
zero inflation for beta distribution model ? Issue #355 ? glmmTMB/glmmTMB ? GitHub<https://github.com/glmmTMB/glmmTMB/issues/355>
Hi Ben, Thanks for your reply. To fit hurdle model using glmmTMB, do you have any example code? will it still have the same issue? Actually i thought using the ziformula option is the way to fit hurdle model, but since it shows such errors of inappropriate values, i guess i may misse some options for hurdle model.
github.com


	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Wed Feb 12 15:56:44 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 12 Feb 2020 09:56:44 -0500
Subject: [R-sig-ME] zero one inflated beta mixed model
In-Reply-To: <PR3P194MB0521F92A8B361269C9E18FB0F71B0@PR3P194MB0521.EURP194.PROD.OUTLOOK.COM>
References: <PR3P194MB0521F92A8B361269C9E18FB0F71B0@PR3P194MB0521.EURP194.PROD.OUTLOOK.COM>
Message-ID: <CABghstQfprvqDm+=oVOjuiF2UKP_KaC+EKq2YRBAYnup+K_bGw@mail.gmail.com>

    At present glmmTMB doesn't do zero-one-inflated betas, only
zero-inflated betas. As far as I know your options are (1) use brms,
(2) squish your 1 values to something slightly less than 1, or (3) do
the hurdle model manually (i.e. fit two separate models, one for the
probability that the response== 1, and another (conditional) model for
the zero-inflated beta distribution applied only to the responses <1).

  Others on the list may have other suggestions ... (e.g. does INLA
does zero-one-inflated betas?)

On Wed, Feb 12, 2020 at 8:42 AM Morgane Brachet
<morgane.brachet at hotmail.com> wrote:
>
> Hello,
>
> I am writing to you following one of the posts on GitHub (https://github.com/glmmTMB/glmmTMB/issues/355). I am trying to fit proportion data with lots of 0s and a few 1s into a hurdle model using glmmTMB. Is this possible? Would you have any example code please?
>
> Thank you!
>
> Morgane
> [https://avatars1.githubusercontent.com/u/13640228?s=400&v=4]<https://github.com/glmmTMB/glmmTMB/issues/355>
> zero inflation for beta distribution model ? Issue #355 ? glmmTMB/glmmTMB ? GitHub<https://github.com/glmmTMB/glmmTMB/issues/355>
> Hi Ben, Thanks for your reply. To fit hurdle model using glmmTMB, do you have any example code? will it still have the same issue? Actually i thought using the ziformula option is the way to fit hurdle model, but since it shows such errors of inappropriate values, i guess i may misse some options for hurdle model.
> github.com
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com  Wed Feb 12 18:42:32 2020
From: b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com (Alessandra Bielli)
Date: Wed, 12 Feb 2020 11:42:32 -0600
Subject: [R-sig-ME] Fwd: model check for negative binomial model
In-Reply-To: <3cd5cea8-28c9-0799-9f17-d80a4aaa1ccc@gmail.com>
References: <mailman.18062.3142.1581469252.1419.r-sig-mixed-models@r-project.org>
 <CA+6N3yV8ytczPND5x1VS5Vfe+H748vqjmc4=79wTnbJb1JmSbA@mail.gmail.com>
 <3cd5cea8-28c9-0799-9f17-d80a4aaa1ccc@gmail.com>
Message-ID: <CA+6N3yXx23=4tajGD8DrZj=SOE+HbSX6K9sOQojt7YVVfgCJsw@mail.gmail.com>

Dear Ben

Thanks for your quick response.

Yes, emergence success is usually between 60 and 80% or higher.
I am not sure how to use a binomial, if my data are counts?

Can you explain why the approximation doesn't work well if success gets
much above 50%? Does it make sense, then, to have "unhatched" as dependent
variable, so that I predict mortality (usually below 50%) using a nb with
offset(log(total clutch)) ?

> summary(m.emerged)
Generalized linear mixed model fit by maximum likelihood (Laplace
Approximation) ['glmerMod']
 Family: Negative Binomial(2.2104)  ( log )
Formula: Unhatched ~ Relocation..Y.N. + SP + offset(log(Total_Clutch)) +
   (1 | Beach_ID) + (1 | Week)
   Data: main

     AIC      BIC   logLik deviance df.resid
  5439.4   5466.0  -2713.7   5427.4      614

Scaled residuals:
    Min      1Q  Median      3Q     Max
-1.4383 -0.7242 -0.2287  0.4866  4.0531

Random effects:
 Groups   Name        Variance Std.Dev.
 Week     (Intercept) 0.003092 0.0556
 Beach_ID (Intercept) 0.025894 0.1609
Number of obs: 620, groups:  Week, 31; Beach_ID, 8

Fixed effects:
                  Estimate Std. Error z value Pr(>|z|)
(Intercept)       -1.38864    0.08227 -16.879  < 2e-16 ***
Relocation..Y.N.Y  0.32105    0.09152   3.508 0.000452 ***
SPL                0.22218    0.08793   2.527 0.011508 *
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Correlation of Fixed Effects:
            (Intr) R..Y.N
Rlct..Y.N.Y -0.143
SPL         -0.540 -0.038

Thanks,

Alessandra

On Tue, Feb 11, 2020 at 7:29 PM Ben Bolker <bbolker at gmail.com> wrote:

>
>   Short answer: if emergence success gets much above 50%, then the
> approximation you're making (Poisson + offset for binomial, or NB +
> offset for negative binomial) doesn't work well.  You might try a
> beta-binomial (with glmmTMB) or a binomial + an observation-level random
> effect.
>
>   (On the other hand, your intercept is -0.3, which corresponds to a
> baseline emergence of 0.42 - not *very* high (but some beaches and years
> will be well above that ...)
>
>   Beyond that, are there any obvious patterns of mis-fit in the
> predicted values ... ?
>
> On 2020-02-11 8:09 p.m., Alessandra Bielli wrote:
> > Dear list
> >
> > I am fitting a poisson model to estimate the effect of a treatment on
> > emergence success of hatchlings. To estimate emergence success, I use
> > number of emerged and an offset(log(total clutch).
> >
> > However, overdispersion was detected:
> >
> >> overdisp_fun(m.emerged) #overdispersion detected
> >
> >       chisq       ratio         rdf           p
> > 3490.300836    5.684529  614.000000    0.000000
> >
> > Therefore, I switched to a negative binomial. I know overdispersion is
> not
> > relevant for nb models, but the model plots don't look too good. I also
> > tried to fit a poisson model with OLRE, but still the  plots don't look
> > good.
> > How do I know if my model is good enough, and what can I do to improve
> it?
> >
> >> summary(m.emerged)
> > Generalized linear mixed model fit by maximum likelihood (Laplace
> > Approximation) ['glmerMod']
> >  Family: Negative Binomial(7.604)  ( log )
> > Formula: Hatched ~ Relocation..Y.N. + SP + offset(log(Total_Clutch)) + (1
> > |Beach_ID) + (1 | Year)
> >    Data: main
> >
> >      AIC      BIC   logLik deviance df.resid
> >   6015.6   6042.2  -3001.8   6003.6      614
> >
> > Scaled residuals:
> >     Min      1Q  Median      3Q     Max
> > -2.6427 -0.3790  0.1790  0.5242  1.6583
> >
> > Random effects:
> >  Groups   Name        Variance Std.Dev.
> >  Beach_ID (Intercept) 0.004438 0.06662
> >  Year     (Intercept) 0.001640 0.04050
> > Number of obs: 620, groups:  Beach_ID, 8; Year, 5
> >
> > Fixed effects:
> >                   Estimate Std. Error z value Pr(>|z|)
> > (Intercept)       -0.29915    0.04055  -7.377 1.62e-13 ***
> > Relocation..Y.N.Y -0.16402    0.05052  -3.247  0.00117 **
> > SPL               -0.08311    0.04365  -1.904  0.05689 .
> > ---
> > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >
> > Correlation of Fixed Effects:
> >             (Intr) R..Y.N
> > Rlct..Y.N.Y -0.114
> > SPL         -0.497 -0.054
> >
> >
> > Thanks for your help,
> >
> > Alessandra
> >
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Rplot.pdf
Type: application/pdf
Size: 58626 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20200212/ccb30a6c/attachment-0001.pdf>

From h|gh@t@t @end|ng |rom h|gh@t@t@com  Wed Feb 12 19:08:53 2020
From: h|gh@t@t @end|ng |rom h|gh@t@t@com (Highland Statistics Ltd)
Date: Wed, 12 Feb 2020 19:08:53 +0100
Subject: [R-sig-ME] zero one inflated beta mixed model (Ben Bolker)
In-Reply-To: <mailman.18067.3193.1581529370.1419.r-sig-mixed-models@r-project.org>
References: <mailman.18067.3193.1581529370.1419.r-sig-mixed-models@r-project.org>
Message-ID: <0eea6b51-ebfd-9d68-dfec-8feab4658e87@highstat.com>


At present glmmTMB doesn't do zero-one-inflated betas, only
zero-inflated betas. As far as I know your options are (1) use brms,
(2) squish your 1 values to something slightly less than 1, or (3) do
the hurdle model manually (i.e. fit two separate models, one for the
probability that the response== 1, and another (conditional) model for
the zero-inflated beta distribution applied only to the responses <1).

Others on the list may have other suggestions ... (e.g. does INLA
does zero-one-inflated betas?)
----------



In INLA, you would have to do that manually. First, a 0-1 model, and 
then a beta model (without the zeros..and 'converted' ones). 
Actually....you can also fit a model in which the first column of the 
response variable contains the 0-1 data and the second column the 
remaining values of the response variable. This is nice for spatial 
data; you can test whether the binary part of the model and the 
non-binary part of the model have the same spatial correlation, or 
whether different spatial correlation terms are needed. Or whether they 
share spatial correlation. And you can even deal with barriers (e.g. an 
island for coral reef coverage data).

So..to summarize....if it is a 'simple' (zero-inflated) beta GLM or 
GLMM, then use glmmTMB (in two steps). If there is spatial correlation, 
then use INLA. Once you have fitted both parts, then you can use the 
expression in the following snapshot to re-assemble the model:


Kind regards,

Alain









**********************************

-- 

Dr. Alain F. Zuur
Highland Statistics Ltd.
9 St Clair Wynd
AB41 6DZ Newburgh, UK
Email: highstat at highstat.com
URL:   www.highstat.com


From h|gh@t@t @end|ng |rom h|gh@t@t@com  Wed Feb 12 19:20:43 2020
From: h|gh@t@t @end|ng |rom h|gh@t@t@com (Highland Statistics Ltd)
Date: Wed, 12 Feb 2020 19:20:43 +0100
Subject: [R-sig-ME] Fwd: Re: zero one inflated beta mixed model (Ben Bolker)
In-Reply-To: <0eea6b51-ebfd-9d68-dfec-8feab4658e87@highstat.com>
References: <0eea6b51-ebfd-9d68-dfec-8feab4658e87@highstat.com>
Message-ID: <f2dbf880-957b-05ff-d240-9e9c8668c7ce@highstat.com>




-------- Forwarded Message --------
Subject: 	Re: zero one inflated beta mixed model (Ben Bolker)
Date: 	Wed, 12 Feb 2020 19:08:53 +0100
From: 	Highland Statistics Ltd <highstat at highstat.com>
To: 	r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>




At present glmmTMB doesn't do zero-one-inflated betas, only
zero-inflated betas. As far as I know your options are (1) use brms,
(2) squish your 1 values to something slightly less than 1, or (3) do
the hurdle model manually (i.e. fit two separate models, one for the
probability that the response== 1, and another (conditional) model for
the zero-inflated beta distribution applied only to the responses <1).

Others on the list may have other suggestions ... (e.g. does INLA
does zero-one-inflated betas?)
----------



In INLA, you would have to do that manually. First, a 0-1 model, and 
then a beta model (without the zeros..and 'converted' ones). 
Actually....you can also fit a model in which the first column of the 
response variable contains the 0-1 data and the second column the 
remaining values of the response variable. This is nice for spatial 
data; you can test whether the binary part of the model and the 
non-binary part of the model have the same spatial correlation, or 
whether different spatial correlation terms are needed. Or whether they 
share spatial correlation. And you can even deal with barriers (e.g. an 
island for coral reef coverage data).

So..to summarize....if it is a 'simple' (zero-inflated) beta GLM or 
GLMM, then use glmmTMB (in two steps). If there is spatial correlation, 
then use INLA. Once you have fitted both parts, then you can? 
re-assemble the model (I tried to snapshot a picture of the equations, 
but it seems that this mailing lists blocks emails with pictures inside)


Kind regards,

Alain









**********************************

-- 

Dr. Alain F. Zuur
Highland Statistics Ltd.
9 St Clair Wynd
AB41 6DZ Newburgh, UK
Email:highstat at highstat.com
URL:www.highstat.com

-- 


	[[alternative HTML version deleted]]


From jonn@t|on@ @end|ng |rom gm@||@com  Wed Feb 12 21:37:30 2020
From: jonn@t|on@ @end|ng |rom gm@||@com (jonnations)
Date: Wed, 12 Feb 2020 14:37:30 -0600
Subject: [R-sig-ME] zero one inflated beta mixed model
Message-ID: <CAHta4sN8mPLuzpUyAv9kNO=73Y-T2KDvRJZeNXfokZcSL_YqXg@mail.gmail.com>

Hi Morgane,

Like Ben said, brms is a good option for zero one inflated beta models.
I have used proportional data in the past and, when learning about mixed
models, was surprised to find out that there is no obvious distribution
family for these data. In my own experience zero one inflated models
required more data than I had to make good parameter estimates. Depending
on the distribution of your data, an ordinal model could be a good choice!
You could set lots of thresholds if you want. There is a great
manuscript&tutorial on ordinal models in brms available here
https://psyarxiv.com/x8swp/  I think there is a formatted published version
out there for free as well.


Message: 2
Date: Wed, 12 Feb 2020 09:56:44 -0500
From: Ben Bolker <bbolker at gmail.com>
To: Morgane Brachet <morgane.brachet at hotmail.com>
Cc: "r-sig-mixed-models at r-project.org"
        <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] zero one inflated beta mixed model
Message-ID:
        <CABghstQfprvqDm+=oVOjuiF2UKP_KaC+EKq2YRBAYnup+K_bGw at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

    At present glmmTMB doesn't do zero-one-inflated betas, only
zero-inflated betas. As far as I know your options are (1) use brms,
(2) squish your 1 values to something slightly less than 1, or (3) do
the hurdle model manually (i.e. fit two separate models, one for the
probability that the response== 1, and another (conditional) model for
the zero-inflated beta distribution applied only to the responses <1).

  Others on the list may have other suggestions ... (e.g. does INLA
does zero-one-inflated betas?)

On Wed, Feb 12, 2020 at 8:42 AM Morgane Brachet
<morgane.brachet at hotmail.com> wrote:
>
> Hello,
>
> I am writing to you following one of the posts on GitHub (
https://github.com/glmmTMB/glmmTMB/issues/355). I am trying to fit
proportion data with lots of 0s and a few 1s into a hurdle model using
glmmTMB. Is this possible? Would you have any example code please?
>
> Thank you!
>
> Morgane
> [https://avatars1.githubusercontent.com/u/13640228?s=400&v=4]<
https://github.com/glmmTMB/glmmTMB/issues/355>
> zero inflation for beta distribution model ? Issue #355 ? glmmTMB/glmmTMB
? GitHub<https://github.com/glmmTMB/glmmTMB/issues/355>
> Hi Ben, Thanks for your reply. To fit hurdle model using glmmTMB, do you
have any example code? will it still have the same issue? Actually i
thought using the ziformula option is the way to fit hurdle model, but
since it shows such errors of inappropriate values, i guess i may misse
some options for hurdle model.
> github.com
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models




-- 
Jonathan A. Nations
PhD Candidate
Esselstyn Lab <https://esselstyn.github.io/>
Museum of Natural Sciences <https://www.lsu.edu/mns/>
Louisiana State University
jonnynations.com

	[[alternative HTML version deleted]]


From jepu@to @end|ng |rom gm@||@com  Wed Feb 12 23:48:30 2020
From: jepu@to @end|ng |rom gm@||@com (James Pustejovsky)
Date: Wed, 12 Feb 2020 16:48:30 -0600
Subject: [R-sig-ME] corAR1() structure in lme()
Message-ID: <CAFUVuJz-9_H02uKLFoHnM_y6FHr_AuSM-Gue97XUT_M4HdAzgQ@mail.gmail.com>

Hello,

I am writing some additional functionality for lme objects, and I have a
question about how lme() handles corAR1() structures. I noticed that
sometimes, if I specify a model with corAR1(), the fitted model has a
corStruct of class corARMA instead of (as you would expect) corAR1(). Here
are a few examples where this does and doesn't occur:

library(nlme)

data(Laski, package = "scdhlm")

Laski_AR1 <- lme(fixed = outcome ~ treatment,
                 random = ~ treatment | case,
                 correlation = corAR1(0.2, ~ time | case),
                 data = Laski)

inherits(Laski_AR1$modelStruct$corStruct, "corAR1") # TRUE

Laski_hetAR1 <- lme(fixed = outcome ~ treatment,
                    random = ~ treatment | case,
                    correlation = corAR1(0, ~ time | case),
                    weights = varIdent(form = ~ 1 | treatment),
                    data = Laski)

inherits(Laski_hetAR1$modelStruct$corStruct, "corAR1") # TRUE

data(egsingle, package = "mlmRev")

lme_2level <-
  lme(fixed = math ~ year + female + black + hispanic,
      random = ~ 1 | childid,
      correlation = corAR1(0, ~ year | childid),
      data = egsingle)

inherits(lme_2level$modelStruct$corStruct, "corAR1") # FALSE


lme_3level <-
  lme(fixed = math ~ year + female + black + hispanic,
      random = ~ 1 | schoolid/childid,
      correlation = corAR1(0.1, ~ year | schoolid/childid),
      data = egsingle)

inherits(lme_3level$modelStruct$corStruct, "corAR1") # FALSE

Does anyone have a sense of why the class of the fitted model's corStruct
is not consistent with the input class?

Thanks for any pointers!

Kind Regards,
James

	[[alternative HTML version deleted]]


From morg@ne@br@chet @end|ng |rom hotm@||@com  Thu Feb 13 09:35:09 2020
From: morg@ne@br@chet @end|ng |rom hotm@||@com (Morgane Brachet)
Date: Thu, 13 Feb 2020 08:35:09 +0000
Subject: [R-sig-ME] zero one inflated beta mixed model
In-Reply-To: <CAHta4sN8mPLuzpUyAv9kNO=73Y-T2KDvRJZeNXfokZcSL_YqXg@mail.gmail.com>
References: <CAHta4sN8mPLuzpUyAv9kNO=73Y-T2KDvRJZeNXfokZcSL_YqXg@mail.gmail.com>
Message-ID: <PR3P194MB05216E840C321DF0FBF0E800F71A0@PR3P194MB0521.EURP194.PROD.OUTLOOK.COM>

Thank you very much for your help, I will look into it!

Morgane
________________________________
De : jonnations <jonnations at gmail.com>
Envoy? : mercredi 12 f?vrier 2020 21:37
? : morgane.brachet at hotmail.com <morgane.brachet at hotmail.com>
Cc : r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Objet : Re: [R-sig-ME] zero one inflated beta mixed model

Hi Morgane,

Like Ben said, brms is a good option for zero one inflated beta models.
I have used proportional data in the past and, when learning about mixed models, was surprised to find out that there is no obvious distribution family for these data. In my own experience zero one inflated models required more data than I had to make good parameter estimates. Depending on the distribution of your data, an ordinal model could be a good choice! You could set lots of thresholds if you want. There is a great manuscript&tutorial on ordinal models in brms available here https://psyarxiv.com/x8swp/  I think there is a formatted published version out there for free as well.


Message: 2
Date: Wed, 12 Feb 2020 09:56:44 -0500
From: Ben Bolker <bbolker at gmail.com<mailto:bbolker at gmail.com>>
To: Morgane Brachet <morgane.brachet at hotmail.com<mailto:morgane.brachet at hotmail.com>>
Cc: "r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>"
        <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: Re: [R-sig-ME] zero one inflated beta mixed model
Message-ID:
        <CABghstQfprvqDm+=oVOjuiF2UKP_KaC+EKq2YRBAYnup+K_bGw at mail.gmail.com<mailto:oVOjuiF2UKP_KaC%2BEKq2YRBAYnup%2BK_bGw at mail.gmail.com>>
Content-Type: text/plain; charset="utf-8"

    At present glmmTMB doesn't do zero-one-inflated betas, only
zero-inflated betas. As far as I know your options are (1) use brms,
(2) squish your 1 values to something slightly less than 1, or (3) do
the hurdle model manually (i.e. fit two separate models, one for the
probability that the response== 1, and another (conditional) model for
the zero-inflated beta distribution applied only to the responses <1).

  Others on the list may have other suggestions ... (e.g. does INLA
does zero-one-inflated betas?)

On Wed, Feb 12, 2020 at 8:42 AM Morgane Brachet
<morgane.brachet at hotmail.com<mailto:morgane.brachet at hotmail.com>> wrote:
>
> Hello,
>
> I am writing to you following one of the posts on GitHub (https://github.com/glmmTMB/glmmTMB/issues/355). I am trying to fit proportion data with lots of 0s and a few 1s into a hurdle model using glmmTMB. Is this possible? Would you have any example code please?
>
> Thank you!
>
> Morgane
> [https://avatars1.githubusercontent.com/u/13640228?s=400&v=4]<https://github.com/glmmTMB/glmmTMB/issues/355>
> zero inflation for beta distribution model ? Issue #355 ? glmmTMB/glmmTMB ? GitHub<https://github.com/glmmTMB/glmmTMB/issues/355>
> Hi Ben, Thanks for your reply. To fit hurdle model using glmmTMB, do you have any example code? will it still have the same issue? Actually i thought using the ziformula option is the way to fit hurdle model, but since it shows such errors of inappropriate values, i guess i may misse some options for hurdle model.
> github.com<http://github.com>
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models




--
Jonathan A. Nations
PhD Candidate
Esselstyn Lab<https://esselstyn.github.io/>
Museum of Natural Sciences<https://www.lsu.edu/mns/>
Louisiana State University
jonnynations.com<https://jonnynations.com/>

	[[alternative HTML version deleted]]


From j@de@ @end|ng |rom he@|th@uc@d@edu  Fri Feb 14 08:59:34 2020
From: j@de@ @end|ng |rom he@|th@uc@d@edu (Ades, James)
Date: Fri, 14 Feb 2020 07:59:34 +0000
Subject: [R-sig-ME] Most principled reporting of mixed-effect model
 regression coefficients
Message-ID: <DM5PR1901MB20072EB8F9BE9DBE5E82002BEA150@DM5PR1901MB2007.namprd19.prod.outlook.com>

Hi all,



It?s been surprisingly difficult to find the most principled reporting of mixed-effect model regression coefficients (for individual fixed-effects). One stack overflow article lead me to this paper?a systematic review of the incorporating and reporting of GLMMs ( https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0112653#pone.0112653.s001)  which references a paper by Ben Bolker (https://www.sciencedirect.com/science/article/pii/S0169534709000196). Oddly, I don?t really find an answer to this in either of those. I?ve heard mixed things regarding fixed effect coefficients in LMM (that LMM/and GLMMs are more about the predictive power of an entire model than the individual predictors themselves), but overall, my understanding is that it?s kosher (and informative) to look at effect sizes of regression (fixed effect) coefficients?only that lme4 doesn?t currently provide p values (though Lmertest does).



It seems like reporting effect size of regression coefficients and their SEs should suffice; though sometimes people report CI with those as well (but isn?t that a little redundant). My PI is telling me to include p-values. So many different things, so little agreement.



I figured I?d turn here for something of a ?definitive? answer.



Ben, I definitely need to go back and read through your paper more thoroughly for a deeper understanding of the nuances of GLMMs. Currently watching?and reading?McElreath?s Statistical Rethinking, but I?m not quite at the level of implementing MCMCs.


Much thanks,


James


	[[alternative HTML version deleted]]


From emm@nue|@cur|@ @end|ng |rom p@r|@de@c@rte@@|r  Fri Feb 14 09:43:36 2020
From: emm@nue|@cur|@ @end|ng |rom p@r|@de@c@rte@@|r (Emmanuel Curis)
Date: Fri, 14 Feb 2020 09:43:36 +0100
Subject: [R-sig-ME] Most principled reporting of mixed-effect model
 regression coefficients
In-Reply-To: <DM5PR1901MB20072EB8F9BE9DBE5E82002BEA150@DM5PR1901MB2007.namprd19.prod.outlook.com>
References: <DM5PR1901MB20072EB8F9BE9DBE5E82002BEA150@DM5PR1901MB2007.namprd19.prod.outlook.com>
Message-ID: <20200214084336.GA2346@info124.pharmacie.univ-paris5.fr>

Hello,

For what it's worth, here a few ides about the different reporting.
The main one being that it is, first of all, a matter of conventions
that may vary with the community and time, so you should first see
what are the usages in your field, then decide if you find them suited
or not for your own purpose and follow them or not.

For instance, physicists and some chemists do like standard errors:
quite often, it is because error come from small measurement
uncertainties and the Gaussian approximation fairly holds, so
interpreting standard errors is straightforward - confidence intervals
are roughly ?2 standard error in this case, for the usual 95 % level.

Other chemists and most population-pharmacokineticians do not use
standard errors, but instead coefficient of variation, expressed as
percent (that is, 100?standard error/coefficient estimation). Beside
usage, this is also because quite often, the underlying assumption is
that variance change with the value or even more specifically that a
log-normal distribution holds, and interpretation as relative error is
then more straightforward.

Both methods can be quite misleading if the underlying distribution
assumption is not met, and especially in case of strongly skewed
distributions. In that case, confidence intervals are not anymore
given by ?2 standard error, and can be fairly different.  In such
cases, giving confidence intervals seems better, because
interpretation is straightforward.

P-values answer a quite different question: they say, more or less, if
the coefficient is different from a reference value, typically 0, but
they do not really convey any information about the coefficient
precision.

Hope this helps a little bit to make a thinked-about selection...  Of
course, this reflects only my own perception of these notions.

Best regards,

On Fri, Feb 14, 2020 at 07:59:34AM +0000, Ades, James wrote:
? Hi all,
? 
? 
? 
? It?s been surprisingly difficult to find the most principled reporting of mixed-effect model regression coefficients (for individual fixed-effects). One stack overflow article lead me to this paper?a systematic review of the incorporating and reporting of GLMMs ( https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0112653#pone.0112653.s001)  which references a paper by Ben Bolker (https://www.sciencedirect.com/science/article/pii/S0169534709000196). Oddly, I don?t really find an answer to this in either of those. I?ve heard mixed things regarding fixed effect coefficients in LMM (that LMM/and GLMMs are more about the predictive power of an entire model than the individual predictors themselves), but overall, my understanding is that it?s kosher (and informative) to look at effect sizes of regression (fixed effect) coefficients?only that lme4 doesn?t currently provide p values (though Lmertest does).
? 
? 
? 
? It seems like reporting effect size of regression coefficients and their SEs should suffice; though sometimes people report CI with those as well (but isn?t that a little redundant). My PI is telling me to include p-values. So many different things, so little agreement.
? 
? 
? 
? I figured I?d turn here for something of a ?definitive? answer.
? 
? 
? 
? Ben, I definitely need to go back and read through your paper more thoroughly for a deeper understanding of the nuances of GLMMs. Currently watching?and reading?McElreath?s Statistical Rethinking, but I?m not quite at the level of implementing MCMCs.
? 
? 
? Much thanks,
? 
? 
? James
? 
? 
? 	[[alternative HTML version deleted]]
? 

? _______________________________________________
? R-sig-mixed-models at r-project.org mailing list
? https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
                                Emmanuel CURIS
                                emmanuel.curis at parisdescartes.fr

Page WWW: http://emmanuel.curis.online.fr/index.html


From j|@ver|@@|mo @end|ng |rom gm@||@com  Fri Feb 14 09:44:47 2020
From: j|@ver|@@|mo @end|ng |rom gm@||@com (=?ISO-8859-1?Q?Jo=E3o_Ver=EDssimo?=)
Date: Fri, 14 Feb 2020 09:44:47 +0100
Subject: [R-sig-ME] Most principled reporting of mixed-effect model
 regression coefficients
In-Reply-To: <DM5PR1901MB20072EB8F9BE9DBE5E82002BEA150@DM5PR1901MB2007.namprd19.prod.outlook.com>
References: <DM5PR1901MB20072EB8F9BE9DBE5E82002BEA150@DM5PR1901MB2007.namprd19.prod.outlook.com>
Message-ID: <402e967e9857f0ca022f3690b879ad7b97ccea6d.camel@gmail.com>

This one came out recently, may help (particularly Table 7):
https://doi.org/10.1016/j.jml.2020.104092

Jo?o

On Fri, 2020-02-14 at 07:59 +0000, Ades, James wrote:
> Hi all,
> 
> 
> 
> Its been surprisingly difficult to find the most principled reporting
> of mixed-effect model regression coefficients (for individual fixed-
> effects). One stack overflow article lead me to this papera
> systematic review of the incorporating and reporting of GLMMs ( 
> https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0112653#pone.0112653.s001)
>   which references a paper by Ben Bolker (
> https://www.sciencedirect.com/science/article/pii/S0169534709000196).
> Oddly, I dont really find an answer to this in either of those. Ive
> heard mixed things regarding fixed effect coefficients in LMM (that
> LMM/and GLMMs are more about the predictive power of an entire model
> than the individual predictors themselves), but overall, my
> understanding is that its kosher (and informative) to look at effect
> sizes of regression (fixed effect) coefficientsonly that lme4 doesnt
> currently provide p values (though Lmertest does).
> 
> 
> 
> It seems like reporting effect size of regression coefficients and
> their SEs should suffice; though sometimes people report CI with
> those as well (but isnt that a little redundant). My PI is telling me
> to include p-values. So many different things, so little agreement.
> 
> 
> 
> I figured Id turn here for something of a definitive answer.
> 
> 
> 
> Ben, I definitely need to go back and read through your paper more
> thoroughly for a deeper understanding of the nuances of GLMMs.
> Currently watchingand readingMcElreaths Statistical Rethinking, but
> Im not quite at the level of implementing MCMCs.
> 
> 
> Much thanks,
> 
> 
> James
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From th|erry@onke||nx @end|ng |rom |nbo@be  Fri Feb 14 10:47:30 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Fri, 14 Feb 2020 10:47:30 +0100
Subject: [R-sig-ME] Most principled reporting of mixed-effect model
 regression coefficients
In-Reply-To: <DM5PR1901MB20072EB8F9BE9DBE5E82002BEA150@DM5PR1901MB2007.namprd19.prod.outlook.com>
References: <DM5PR1901MB20072EB8F9BE9DBE5E82002BEA150@DM5PR1901MB2007.namprd19.prod.outlook.com>
Message-ID: <CAJuCY5xhVYBgZwfwmb-gO9=YYhkg7RA7B4dDjo1_xp_xX6di5g@mail.gmail.com>

Dear James,

IMHO the estimate and its CI works best. They instantly provide the range
of uncertainty around the estimate without the reader having to do the
math. CI also work with skewed distributions. p-values don't offer much
added value over a CI.
Below are a few examples of four estimates and their uncertainties. The
first line displays the estimate and its SE. The second line displays the
estimate, SE and p-values. The third displays the estimate and a relative
error. While the last one displays the estimate and 95% CI.

Keep in mind that readers are more likely to understand CI rather than SE.

"1.2 ? 0.3"  "10.5 ? 4.5" "0.0 ? 0.3"  "0.0 ? 5.0"
"1.2 ? 0.3 (p = 0.0001)"  "10.5 ? 4.5 (p = 0.0196)" "0.0 ? 0.3 (p =
1.0000)"  "0.0 ? 5.0 (p = 1.0000)"
 "1.2 ? 25.0%"  "10.5 ? 42.9%" "0.0 ? Inf%"   "0.0 ? Inf%"
"1.2 (0.6; 1.8)"   "10.5 (1.7; 19.3)" "0.0 (-0.6; 0.6)"  "0.0 (-9.8; 9.8)"

Best regards,

Thierry

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op vr 14 feb. 2020 om 09:31 schreef Ades, James <jades at health.ucsd.edu>:

> Hi all,
>
>
>
> It?s been surprisingly difficult to find the most principled reporting of
> mixed-effect model regression coefficients (for individual fixed-effects).
> One stack overflow article lead me to this paper?a systematic review of the
> incorporating and reporting of GLMMs (
> https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0112653#pone.0112653.s001)
> which references a paper by Ben Bolker (
> https://www.sciencedirect.com/science/article/pii/S0169534709000196).
> Oddly, I don?t really find an answer to this in either of those. I?ve heard
> mixed things regarding fixed effect coefficients in LMM (that LMM/and GLMMs
> are more about the predictive power of an entire model than the individual
> predictors themselves), but overall, my understanding is that it?s kosher
> (and informative) to look at effect sizes of regression (fixed effect)
> coefficients?only that lme4 doesn?t currently provide p values (though
> Lmertest does).
>
>
>
> It seems like reporting effect size of regression coefficients and their
> SEs should suffice; though sometimes people report CI with those as well
> (but isn?t that a little redundant). My PI is telling me to include
> p-values. So many different things, so little agreement.
>
>
>
> I figured I?d turn here for something of a ?definitive? answer.
>
>
>
> Ben, I definitely need to go back and read through your paper more
> thoroughly for a deeper understanding of the nuances of GLMMs. Currently
> watching?and reading?McElreath?s Statistical Rethinking, but I?m not quite
> at the level of implementing MCMCs.
>
>
> Much thanks,
>
>
> James
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Sat Feb 15 10:37:56 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Sat, 15 Feb 2020 10:37:56 +0100
Subject: [R-sig-ME] Most principled reporting of mixed-effect model
 regression coefficients
In-Reply-To: <DM5PR1901MB2007B1C5BF040057769C97D7EA140@DM5PR1901MB2007.namprd19.prod.outlook.com>
References: <DM5PR1901MB20072EB8F9BE9DBE5E82002BEA150@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <CAJuCY5xhVYBgZwfwmb-gO9=YYhkg7RA7B4dDjo1_xp_xX6di5g@mail.gmail.com>
 <DM5PR1901MB2007B1C5BF040057769C97D7EA140@DM5PR1901MB2007.namprd19.prod.outlook.com>
Message-ID: <CAJuCY5zWjg0gsNiF+y+QmDXvz_K1tKVioYysVMgjhofovfK2aA@mail.gmail.com>

Dear James,

I tend to use qnorm(c(0.025, 0.975), mean = estimate, sd = SE)

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op za 15 feb. 2020 om 01:29 schreef Ades, James <jades at health.ucsd.edu>:

> Thanks, Thierry. This is what I was looking for!
>
> When I try confint(lme4_model) I get the following warning:
>
> ```{r}
>
> Computing profile confidence intervals ...Error in zeta(shiftpar, start = opt[seqpar1][-w]) :
>   profiling detected new, lower deviance
>
> ```
> Is there an easier way of extracting confidence intervals for fixed
> effects in lme4 than calculating them using the point estimate +/- Z * SE ?
>
> Best,
> James
> ------------------------------
> *From:* Thierry Onkelinx <thierry.onkelinx at inbo.be>
> *Sent:* Friday, February 14, 2020 1:47 AM
> *To:* Ades, James <jades at health.ucsd.edu>
> *Cc:* r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> *Subject:* Re: [R-sig-ME] Most principled reporting of mixed-effect model
> regression coefficients
>
> Dear James,
>
> IMHO the estimate and its CI works best. They instantly provide the range
> of uncertainty around the estimate without the reader having to do the
> math. CI also work with skewed distributions. p-values don't offer much
> added value over a CI.
> Below are a few examples of four estimates and their uncertainties. The
> first line displays the estimate and its SE. The second line displays the
> estimate, SE and p-values. The third displays the estimate and a relative
> error. While the last one displays the estimate and 95% CI.
>
> Keep in mind that readers are more likely to understand CI rather than SE.
>
> "1.2 ? 0.3"  "10.5 ? 4.5" "0.0 ? 0.3"  "0.0 ? 5.0"
> "1.2 ? 0.3 (p = 0.0001)"  "10.5 ? 4.5 (p = 0.0196)" "0.0 ? 0.3 (p =
> 1.0000)"  "0.0 ? 5.0 (p = 1.0000)"
>  "1.2 ? 25.0%"  "10.5 ? 42.9%" "0.0 ? Inf%"   "0.0 ? Inf%"
> "1.2 (0.6; 1.8)"   "10.5 (1.7; 19.3)" "0.0 (-0.6; 0.6)"  "0.0 (-9.8; 9.8)"
>
> Best regards,
>
> Thierry
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op vr 14 feb. 2020 om 09:31 schreef Ades, James <jades at health.ucsd.edu>:
>
> Hi all,
>
>
>
> It?s been surprisingly difficult to find the most principled reporting of
> mixed-effect model regression coefficients (for individual fixed-effects).
> One stack overflow article lead me to this paper?a systematic review of the
> incorporating and reporting of GLMMs (
> https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0112653#pone.0112653.s001)
> which references a paper by Ben Bolker (
> https://www.sciencedirect.com/science/article/pii/S0169534709000196).
> Oddly, I don?t really find an answer to this in either of those. I?ve heard
> mixed things regarding fixed effect coefficients in LMM (that LMM/and GLMMs
> are more about the predictive power of an entire model than the individual
> predictors themselves), but overall, my understanding is that it?s kosher
> (and informative) to look at effect sizes of regression (fixed effect)
> coefficients?only that lme4 doesn?t currently provide p values (though
> Lmertest does).
>
>
>
> It seems like reporting effect size of regression coefficients and their
> SEs should suffice; though sometimes people report CI with those as well
> (but isn?t that a little redundant). My PI is telling me to include
> p-values. So many different things, so little agreement.
>
>
>
> I figured I?d turn here for something of a ?definitive? answer.
>
>
>
> Ben, I definitely need to go back and read through your paper more
> thoroughly for a deeper understanding of the nuances of GLMMs. Currently
> watching?and reading?McElreath?s Statistical Rethinking, but I?m not quite
> at the level of implementing MCMCs.
>
>
> Much thanks,
>
>
> James
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>

	[[alternative HTML version deleted]]


From j@de@ @end|ng |rom he@|th@uc@d@edu  Sat Feb 15 01:29:06 2020
From: j@de@ @end|ng |rom he@|th@uc@d@edu (Ades, James)
Date: Sat, 15 Feb 2020 00:29:06 +0000
Subject: [R-sig-ME] Most principled reporting of mixed-effect model
 regression coefficients
In-Reply-To: <CAJuCY5xhVYBgZwfwmb-gO9=YYhkg7RA7B4dDjo1_xp_xX6di5g@mail.gmail.com>
References: <DM5PR1901MB20072EB8F9BE9DBE5E82002BEA150@DM5PR1901MB2007.namprd19.prod.outlook.com>,
 <CAJuCY5xhVYBgZwfwmb-gO9=YYhkg7RA7B4dDjo1_xp_xX6di5g@mail.gmail.com>
Message-ID: <DM5PR1901MB2007B1C5BF040057769C97D7EA140@DM5PR1901MB2007.namprd19.prod.outlook.com>

Thanks, Thierry. This is what I was looking for!

When I try confint(lme4_model) I get the following warning:

```{r}

Computing profile confidence intervals ...
Error in zeta(shiftpar, start = opt[seqpar1][-w]) :
  profiling detected new, lower deviance

```
Is there an easier way of extracting confidence intervals for fixed effects in lme4 than calculating them using the point estimate +/- Z * SE ?

Best,
James
________________________________
From: Thierry Onkelinx <thierry.onkelinx at inbo.be>
Sent: Friday, February 14, 2020 1:47 AM
To: Ades, James <jades at health.ucsd.edu>
Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients

Dear James,

IMHO the estimate and its CI works best. They instantly provide the range of uncertainty around the estimate without the reader having to do the math. CI also work with skewed distributions. p-values don't offer much added value over a CI.
Below are a few examples of four estimates and their uncertainties. The first line displays the estimate and its SE. The second line displays the estimate, SE and p-values. The third displays the estimate and a relative error. While the last one displays the estimate and 95% CI.

Keep in mind that readers are more likely to understand CI rather than SE.

"1.2 ? 0.3"  "10.5 ? 4.5" "0.0 ? 0.3"  "0.0 ? 5.0"
"1.2 ? 0.3 (p = 0.0001)"  "10.5 ? 4.5 (p = 0.0196)" "0.0 ? 0.3 (p = 1.0000)"  "0.0 ? 5.0 (p = 1.0000)"
 "1.2 ? 25.0%"  "10.5 ? 42.9%" "0.0 ? Inf%"   "0.0 ? Inf%"
"1.2 (0.6; 1.8)"   "10.5 (1.7; 19.3)" "0.0 (-0.6; 0.6)"  "0.0 (-9.8; 9.8)"

Best regards,

Thierry

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be<http://www.inbo.be>

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

[https://inbo-website-prd-532750756126.s3-eu-west-1.amazonaws.com/inbologoleeuw_nl.png]<https://www.inbo.be>


Op vr 14 feb. 2020 om 09:31 schreef Ades, James <jades at health.ucsd.edu<mailto:jades at health.ucsd.edu>>:
Hi all,



It?s been surprisingly difficult to find the most principled reporting of mixed-effect model regression coefficients (for individual fixed-effects). One stack overflow article lead me to this paper?a systematic review of the incorporating and reporting of GLMMs ( https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0112653#pone.0112653.s001)  which references a paper by Ben Bolker (https://www.sciencedirect.com/science/article/pii/S0169534709000196). Oddly, I don?t really find an answer to this in either of those. I?ve heard mixed things regarding fixed effect coefficients in LMM (that LMM/and GLMMs are more about the predictive power of an entire model than the individual predictors themselves), but overall, my understanding is that it?s kosher (and informative) to look at effect sizes of regression (fixed effect) coefficients?only that lme4 doesn?t currently provide p values (though Lmertest does).



It seems like reporting effect size of regression coefficients and their SEs should suffice; though sometimes people report CI with those as well (but isn?t that a little redundant). My PI is telling me to include p-values. So many different things, so little agreement.



I figured I?d turn here for something of a ?definitive? answer.



Ben, I definitely need to go back and read through your paper more thoroughly for a deeper understanding of the nuances of GLMMs. Currently watching?and reading?McElreath?s Statistical Rethinking, but I?m not quite at the level of implementing MCMCs.


Much thanks,


James


        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Sat Feb 15 15:38:59 2020
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Sat, 15 Feb 2020 14:38:59 +0000
Subject: [R-sig-ME] Most principled reporting of mixed-effect model
 regression coefficients
In-Reply-To: <26083_1581760306_01F9piGP004064_DM5PR1901MB2007B1C5BF040057769C97D7EA140@DM5PR1901MB2007.namprd19.prod.outlook.com>
References: <DM5PR1901MB20072EB8F9BE9DBE5E82002BEA150@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <CAJuCY5xhVYBgZwfwmb-gO9=YYhkg7RA7B4dDjo1_xp_xX6di5g@mail.gmail.com>
 <26083_1581760306_01F9piGP004064_DM5PR1901MB2007B1C5BF040057769C97D7EA140@DM5PR1901MB2007.namprd19.prod.outlook.com>
Message-ID: <66F70B69-D837-4C2F-BFF8-78278818B8F8@mcmaster.ca>

Dear James,

See the method argument in ?confint.merMod: method = c("profile", "Wald", "boot"). The default is "profile", which apparently has run into a problem. What Thierry suggests corresponds to "Wald".

I hope this helps,
 John

  -----------------------------
  John Fox, Professor Emeritus
  McMaster University
  Hamilton, Ontario, Canada
  Web: http::/socserv.mcmaster.ca/jfox

> On Feb 14, 2020, at 7:29 PM, Ades, James <jades at health.ucsd.edu> wrote:
> 
> Thanks, Thierry. This is what I was looking for!
> 
> When I try confint(lme4_model) I get the following warning:
> 
> ```{r}
> 
> Computing profile confidence intervals ...
> Error in zeta(shiftpar, start = opt[seqpar1][-w]) :
>  profiling detected new, lower deviance
> 
> ```
> Is there an easier way of extracting confidence intervals for fixed effects in lme4 than calculating them using the point estimate +/- Z * SE ?
> 
> Best,
> James
> ________________________________
> From: Thierry Onkelinx <thierry.onkelinx at inbo.be>
> Sent: Friday, February 14, 2020 1:47 AM
> To: Ades, James <jades at health.ucsd.edu>
> Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients
> 
> Dear James,
> 
> IMHO the estimate and its CI works best. They instantly provide the range of uncertainty around the estimate without the reader having to do the math. CI also work with skewed distributions. p-values don't offer much added value over a CI.
> Below are a few examples of four estimates and their uncertainties. The first line displays the estimate and its SE. The second line displays the estimate, SE and p-values. The third displays the estimate and a relative error. While the last one displays the estimate and 95% CI.
> 
> Keep in mind that readers are more likely to understand CI rather than SE.
> 
> "1.2 ? 0.3"  "10.5 ? 4.5" "0.0 ? 0.3"  "0.0 ? 5.0"
> "1.2 ? 0.3 (p = 0.0001)"  "10.5 ? 4.5 (p = 0.0196)" "0.0 ? 0.3 (p = 1.0000)"  "0.0 ? 5.0 (p = 1.0000)"
> "1.2 ? 25.0%"  "10.5 ? 42.9%" "0.0 ? Inf%"   "0.0 ? Inf%"
> "1.2 (0.6; 1.8)"   "10.5 (1.7; 19.3)" "0.0 (-0.6; 0.6)"  "0.0 (-9.8; 9.8)"
> 
> Best regards,
> 
> Thierry
> 
> ir. Thierry Onkelinx
> Statisticus / Statistician
> 
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be<http://www.inbo.be>
> 
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
> ///////////////////////////////////////////////////////////////////////////////////////////
> 
> [https://inbo-website-prd-532750756126.s3-eu-west-1.amazonaws.com/inbologoleeuw_nl.png]<https://www.inbo.be>
> 
> 
> Op vr 14 feb. 2020 om 09:31 schreef Ades, James <jades at health.ucsd.edu<mailto:jades at health.ucsd.edu>>:
> Hi all,
> 
> 
> 
> It?s been surprisingly difficult to find the most principled reporting of mixed-effect model regression coefficients (for individual fixed-effects). One stack overflow article lead me to this paper?a systematic review of the incorporating and reporting of GLMMs ( https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0112653#pone.0112653.s001)  which references a paper by Ben Bolker (https://www.sciencedirect.com/science/article/pii/S0169534709000196). Oddly, I don?t really find an answer to this in either of those. I?ve heard mixed things regarding fixed effect coefficients in LMM (that LMM/and GLMMs are more about the predictive power of an entire model than the individual predictors themselves), but overall, my understanding is that it?s kosher (and informative) to look at effect sizes of regression (fixed effect) coefficients?only that lme4 doesn?t currently provide p values (though Lmertest does).
> 
> 
> 
> It seems like reporting effect size of regression coefficients and their SEs should suffice; though sometimes people report CI with those as well (but isn?t that a little redundant). My PI is telling me to include p-values. So many different things, so little agreement.
> 
> 
> 
> I figured I?d turn here for something of a ?definitive? answer.
> 
> 
> 
> Ben, I definitely need to go back and read through your paper more thoroughly for a deeper understanding of the nuances of GLMMs. Currently watching?and reading?McElreath?s Statistical Rethinking, but I?m not quite at the level of implementing MCMCs.
> 
> 
> Much thanks,
> 
> 
> James
> 
> 
>        [[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From d@|uedecke @end|ng |rom uke@de  Sat Feb 15 16:00:21 2020
From: d@|uedecke @end|ng |rom uke@de (=?utf-8?Q?Daniel_L=C3=BCdecke?=)
Date: Sat, 15 Feb 2020 16:00:21 +0100
Subject: [R-sig-ME] Most principled reporting of mixed-effect model
 regression coefficients
In-Reply-To: <DM5PR1901MB2007B1C5BF040057769C97D7EA140@DM5PR1901MB2007.namprd19.prod.outlook.com>
References: <DM5PR1901MB20072EB8F9BE9DBE5E82002BEA150@DM5PR1901MB2007.namprd19.prod.outlook.com>,
 <CAJuCY5xhVYBgZwfwmb-gO9=YYhkg7RA7B4dDjo1_xp_xX6di5g@mail.gmail.com>
 <DM5PR1901MB2007B1C5BF040057769C97D7EA140@DM5PR1901MB2007.namprd19.prod.outlook.com>
Message-ID: <000001d5e410$a5919990$f0b4ccb0$@uke.de>

The "parameters" package (https://easystats.github.io/parameters/) offers some convenient functions to extract standard errors, p-values or confidence intervals for a vast range of models. Just use "model_parameters()" or "ci()" if you are only interested in CIs. Note that there is a small issue with p-values/CIs based on Kenward-Roger or Satterthwaite approximated degrees of freedom with the current CRAN version, however, these issues are fixed in the latest GitHub version.

According to your original question: it really depends on the field, or even on the journal what information is required. I would say estimate, CI and p-value are often the "standard", and some information on the random effect variances (which you can also get with the parameters-package, using "random_parameters()") and/or R2/ICC are also useful measures to have information about the proportion of explained variance that can be accounted to the random effect parameters (R2 and ICC, in turn, are available in the "performance" package - https://easystats.github.io/performance/).

I personally usually report estimates, CIs, p-values, within- and between-group-variances and ICC (and here again: "group" is sometimes called "subjects", sometimes "clusters", depending on the discipline).

Best
Daniel


-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im Auftrag von Ades, James
Gesendet: Samstag, 15. Februar 2020 01:29
An: Thierry Onkelinx <thierry.onkelinx at inbo.be>
Cc: r-sig-mixed-models at r-project.org
Betreff: Re: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients

Thanks, Thierry. This is what I was looking for!

When I try confint(lme4_model) I get the following warning:

```{r}

Computing profile confidence intervals ...
Error in zeta(shiftpar, start = opt[seqpar1][-w]) :
  profiling detected new, lower deviance

```
Is there an easier way of extracting confidence intervals for fixed effects in lme4 than calculating them using the point estimate +/- Z * SE ?

Best,
James
________________________________
From: Thierry Onkelinx <thierry.onkelinx at inbo.be>
Sent: Friday, February 14, 2020 1:47 AM
To: Ades, James <jades at health.ucsd.edu>
Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients

Dear James,

IMHO the estimate and its CI works best. They instantly provide the range of uncertainty around the estimate without the reader having to do the math. CI also work with skewed distributions. p-values don't offer much added value over a CI.
Below are a few examples of four estimates and their uncertainties. The first line displays the estimate and its SE. The second line displays the estimate, SE and p-values. The third displays the estimate and a relative error. While the last one displays the estimate and 95% CI.

Keep in mind that readers are more likely to understand CI rather than SE.

"1.2 ? 0.3"  "10.5 ? 4.5" "0.0 ? 0.3"  "0.0 ? 5.0"
"1.2 ? 0.3 (p = 0.0001)"  "10.5 ? 4.5 (p = 0.0196)" "0.0 ? 0.3 (p = 1.0000)"  "0.0 ? 5.0 (p = 1.0000)"
 "1.2 ? 25.0%"  "10.5 ? 42.9%" "0.0 ? Inf%"   "0.0 ? Inf%"
"1.2 (0.6; 1.8)"   "10.5 (1.7; 19.3)" "0.0 (-0.6; 0.6)"  "0.0 (-9.8; 9.8)"

Best regards,

Thierry

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be<http://www.inbo.be>

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

[https://inbo-website-prd-532750756126.s3-eu-west-1.amazonaws.com/inbologoleeuw_nl.png]<https://www.inbo.be>


Op vr 14 feb. 2020 om 09:31 schreef Ades, James <jades at health.ucsd.edu<mailto:jades at health.ucsd.edu>>:
Hi all,



It?s been surprisingly difficult to find the most principled reporting of mixed-effect model regression coefficients (for individual fixed-effects). One stack overflow article lead me to this paper?a systematic review of the incorporating and reporting of GLMMs ( https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0112653#pone.0112653.s001)  which references a paper by Ben Bolker (https://www.sciencedirect.com/science/article/pii/S0169534709000196). Oddly, I don?t really find an answer to this in either of those. I?ve heard mixed things regarding fixed effect coefficients in LMM (that LMM/and GLMMs are more about the predictive power of an entire model than the individual predictors themselves), but overall, my understanding is that it?s kosher (and informative) to look at effect sizes of regression (fixed effect) coefficients?only that lme4 doesn?t currently provide p values (though Lmertest does).



It seems like reporting effect size of regression coefficients and their SEs should suffice; though sometimes people report CI with those as well (but isn?t that a little redundant). My PI is telling me to include p-values. So many different things, so little agreement.



I figured I?d turn here for something of a ?definitive? answer.



Ben, I definitely need to go back and read through your paper more thoroughly for a deeper understanding of the nuances of GLMMs. Currently watching?and reading?McElreath?s Statistical Rethinking, but I?m not quite at the level of implementing MCMCs.


Much thanks,


James


        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING

From k@m@|@@tmeh @end|ng |rom hotm@||@com  Sat Feb 15 23:57:08 2020
From: k@m@|@@tmeh @end|ng |rom hotm@||@com (Kamal Atmeh)
Date: Sat, 15 Feb 2020 23:57:08 +0100
Subject: [R-sig-ME] Predicting values from MCMCglmm model with statistical
 weight in mev argument
Message-ID: <DB7PR07MB5077581CF394C1E788696B5BFE140@DB7PR07MB5077.eurprd07.prod.outlook.com>

Dear list,

I'm quite new to MCMCglmm and after reading much of Jarrod Hadfield's 
documentation I was able to understand the basic construction and 
interpretation of models. However, I am having a bit of trouble when 
predicting values from my gaussian MCMC model. I am running bayesian 
phylogenetic mixed models to determine factors responsible for the 
variation of my movement parameter. My response variable is continuous 
and each point has a standard error associated to it and which I added 
as a statistical weight in the model using the mev argument in the 
MCMCglmm function. I have 5 random effects: phylogeny (to which I 
attributed the species covariance matrix using the ginverse argument), 
species, population, year and individual. My model is thus as follows:

 >>> model <- MCMCglmm(lD ~ 
tactic*period*seasonality+complique_KF+lbody+lintdur+lnb.loc+lduration
 ???????????????????????????????????? , random = 
~sp_phylo+species2+phylo_pop+phylo_popY+phylo_pop_id
 ???????????????????????????????????? , family = "gaussian"
 ???????????????????????????????????? , mev = SE^2 # error variance 
associated to each data point
 ???????????????????????????????????? , ginverse = list(sp_phylo = 
inv.phylo$Ainv) # include a custom matrix for argument phylo
 ???????????????????????????????????? , prior = prior1
 ???????????????????????????????????? , data = Data
 ???????????????????????????????????? , nitt = 22e+04
 ???????????????????????????????????? , burnin = 20000
 ???????????????????????????????????? , thin = 100
 ???????????????????????????????????? , pr=TRUE)

When I try to predict values from the model above, I obtain the 
following error:

 >>> pred_DGLOB <- predict.MCMCglmm(model_D_GLOB_ch1
 ?????????????????????????????????? , type = "response"
 ?????????????????????????????????? , interval = "confidence"
 ?????????????????????????????????? , newdata = newdt)

 >>> Error in rep(as.numeric(rcomponents %in% mcomponents), 
object$Random$nrt) :
 ????? invalid 'times' argument

It appears that this error is originating from the following specific 
code in the predict.MCMCglmm function:

 >>> marginalise<-rep(as.numeric(rcomponents%in%mcomponents), 
object$Random$nrt)

It appears that object$Random$nrt has one additional element compared to 
as.numeric(rcomponents%in%mcomponents).

I already ran predict.MCMCglmm with models without any statistical 
weight and the predict.MCMCglmm function worked fine. Is this a problem 
caused by the fact that I have the standard error in the mev argument? 
If yes, is there a way to go around it? I may be skipping some steps 
before running the predict function but I did not find any documentation 
that addresses this issue.

I would greatly appreciate your help and happy to provide further 
information if needed!

Thank you in advance!

Kamal


From j@de@ @end|ng |rom he@|th@uc@d@edu  Sat Feb 15 22:37:37 2020
From: j@de@ @end|ng |rom he@|th@uc@d@edu (Ades, James)
Date: Sat, 15 Feb 2020 21:37:37 +0000
Subject: [R-sig-ME] Most principled reporting of mixed-effect model
 regression coefficients
In-Reply-To: <000001d5e410$a5919990$f0b4ccb0$@uke.de>
References: <DM5PR1901MB20072EB8F9BE9DBE5E82002BEA150@DM5PR1901MB2007.namprd19.prod.outlook.com>,
 <CAJuCY5xhVYBgZwfwmb-gO9=YYhkg7RA7B4dDjo1_xp_xX6di5g@mail.gmail.com>
 <DM5PR1901MB2007B1C5BF040057769C97D7EA140@DM5PR1901MB2007.namprd19.prod.outlook.com>,
 <000001d5e410$a5919990$f0b4ccb0$@uke.de>
Message-ID: <DM5PR1901MB20070529F6C725043646694DEA140@DM5PR1901MB2007.namprd19.prod.outlook.com>

Thanks a lot for the responses! These are great.

Sorry--I should've clarified field...it's for Neuroscience/Psych. Still, speaking for that specific field, I can't find any unanimous agreement.

Daniel, you're sjPlot package is amazing! I'm using it for our current paper and incorporated the patchwork package to create something I never could've otherwise. Easy to learn, simple and intuitive to execute, plentiful with options.

I just found this paper on reporting R^2 for mixed models: https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210x.2012.00261.x I think I'll also include that, as R^2 (adjusted), at least to me, provides a more intuitive interpretation to people both in and out of the scientific community. Thanks for referencing the "performance" package. I'll look into that.

Awesome! Thanks much, John, Thierry, and Daniel
[https://besjournals.onlinelibrary.wiley.com/cms/asset/d67e1788-346b-4b61-8614-30bb2b237646/mee3.2013.4.issue-2.cover.gif]<https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210x.2012.00261.x>
A general and simple method for obtaining R2 from generalized linear mixed?effects models<https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210x.2012.00261.x>
The use of both linear and generalized linear mixed?effects models (LMMs and GLMMs) has become popular not only in social and medical sciences, but also in biological sciences, especially in the ...
besjournals.onlinelibrary.wiley.com

________________________________
From: Daniel L?decke <d.luedecke at uke.de>
Sent: Saturday, February 15, 2020 7:00 AM
To: Ades, James <jades at health.ucsd.edu>
Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: AW: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients

The "parameters" package (https://easystats.github.io/parameters/) offers some convenient functions to extract standard errors, p-values or confidence intervals for a vast range of models. Just use "model_parameters()" or "ci()" if you are only interested in CIs. Note that there is a small issue with p-values/CIs based on Kenward-Roger or Satterthwaite approximated degrees of freedom with the current CRAN version, however, these issues are fixed in the latest GitHub version.

According to your original question: it really depends on the field, or even on the journal what information is required. I would say estimate, CI and p-value are often the "standard", and some information on the random effect variances (which you can also get with the parameters-package, using "random_parameters()") and/or R2/ICC are also useful measures to have information about the proportion of explained variance that can be accounted to the random effect parameters (R2 and ICC, in turn, are available in the "performance" package - https://easystats.github.io/performance/).

I personally usually report estimates, CIs, p-values, within- and between-group-variances and ICC (and here again: "group" is sometimes called "subjects", sometimes "clusters", depending on the discipline).

Best
Daniel


-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im Auftrag von Ades, James
Gesendet: Samstag, 15. Februar 2020 01:29
An: Thierry Onkelinx <thierry.onkelinx at inbo.be>
Cc: r-sig-mixed-models at r-project.org
Betreff: Re: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients

Thanks, Thierry. This is what I was looking for!

When I try confint(lme4_model) I get the following warning:

```{r}

Computing profile confidence intervals ...
Error in zeta(shiftpar, start = opt[seqpar1][-w]) :
  profiling detected new, lower deviance

```
Is there an easier way of extracting confidence intervals for fixed effects in lme4 than calculating them using the point estimate +/- Z * SE ?

Best,
James
________________________________
From: Thierry Onkelinx <thierry.onkelinx at inbo.be>
Sent: Friday, February 14, 2020 1:47 AM
To: Ades, James <jades at health.ucsd.edu>
Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients

Dear James,

IMHO the estimate and its CI works best. They instantly provide the range of uncertainty around the estimate without the reader having to do the math. CI also work with skewed distributions. p-values don't offer much added value over a CI.
Below are a few examples of four estimates and their uncertainties. The first line displays the estimate and its SE. The second line displays the estimate, SE and p-values. The third displays the estimate and a relative error. While the last one displays the estimate and 95% CI.

Keep in mind that readers are more likely to understand CI rather than SE.

"1.2 ? 0.3"  "10.5 ? 4.5" "0.0 ? 0.3"  "0.0 ? 5.0"
"1.2 ? 0.3 (p = 0.0001)"  "10.5 ? 4.5 (p = 0.0196)" "0.0 ? 0.3 (p = 1.0000)"  "0.0 ? 5.0 (p = 1.0000)"
 "1.2 ? 25.0%"  "10.5 ? 42.9%" "0.0 ? Inf%"   "0.0 ? Inf%"
"1.2 (0.6; 1.8)"   "10.5 (1.7; 19.3)" "0.0 (-0.6; 0.6)"  "0.0 (-9.8; 9.8)"

Best regards,

Thierry

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be<http://www.inbo.be>

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

[https://inbo-website-prd-532750756126.s3-eu-west-1.amazonaws.com/inbologoleeuw_nl.png]<https://www.inbo.be>


Op vr 14 feb. 2020 om 09:31 schreef Ades, James <jades at health.ucsd.edu<mailto:jades at health.ucsd.edu>>:
Hi all,



It?s been surprisingly difficult to find the most principled reporting of mixed-effect model regression coefficients (for individual fixed-effects). One stack overflow article lead me to this paper?a systematic review of the incorporating and reporting of GLMMs ( https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0112653#pone.0112653.s001)  which references a paper by Ben Bolker (https://www.sciencedirect.com/science/article/pii/S0169534709000196). Oddly, I don?t really find an answer to this in either of those. I?ve heard mixed things regarding fixed effect coefficients in LMM (that LMM/and GLMMs are more about the predictive power of an entire model than the individual predictors themselves), but overall, my understanding is that it?s kosher (and informative) to look at effect sizes of regression (fixed effect) coefficients?only that lme4 doesn?t currently provide p values (though Lmertest does).



It seems like reporting effect size of regression coefficients and their SEs should suffice; though sometimes people report CI with those as well (but isn?t that a little redundant). My PI is telling me to include p-values. So many different things, so little agreement.



I figured I?d turn here for something of a ?definitive? answer.



Ben, I definitely need to go back and read through your paper more thoroughly for a deeper understanding of the nuances of GLMMs. Currently watching?and reading?McElreath?s Statistical Rethinking, but I?m not quite at the level of implementing MCMCs.


Much thanks,


James


        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

        [[alternative HTML version deleted]]


--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING

	[[alternative HTML version deleted]]


From M@@rten@Jung @end|ng |rom m@||box@tu-dre@den@de  Sun Feb 16 09:36:17 2020
From: M@@rten@Jung @end|ng |rom m@||box@tu-dre@den@de (Maarten Jung)
Date: Sun, 16 Feb 2020 09:36:17 +0100
Subject: [R-sig-ME] Most principled reporting of mixed-effect model
 regression coefficients
In-Reply-To: <27250084e9e148ba860e21e55ca4f53e@MSX-L104.msx.ad.zih.tu-dresden.de>
References: <DM5PR1901MB20072EB8F9BE9DBE5E82002BEA150@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <CAJuCY5xhVYBgZwfwmb-gO9=YYhkg7RA7B4dDjo1_xp_xX6di5g@mail.gmail.com>
 <DM5PR1901MB2007B1C5BF040057769C97D7EA140@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <000001d5e410$a5919990$f0b4ccb0$@uke.de>
 <27250084e9e148ba860e21e55ca4f53e@MSX-L104.msx.ad.zih.tu-dresden.de>
Message-ID: <CAHr4Dyc7PXYWPax8Zzgvi+-FDmCUoWG=8-RyXDvjNb+Lau303w@mail.gmail.com>

Dear James,

I think most people from Psychology (and many from Neuroscience, but
probably more dependent on the subfield) are used to see the value of
a test statistic (often F-tests), the number(s) of degrees of freedom,
the corresponding p-value, and some sort of effect-size measure.
To calculate the semi-partial-R-squared values for each categorical
predictor (if you have a factorial design) as described in Jaeger,
Edwards, Das, and Sen (2017) [1], you can use the r2glmm::r2beta()
function which you should get from GitHub.
Note that there is (to my knowledge) no agreed-upon way to calculate
effect sizes for (linear) mixed models. My answer here [2] might be
helpful for a quick overview and my personal take on this topic.

[1] https://doi.org/10.1080/02664763.2016.1193725
[2] https://stats.stackexchange.com/a/439842/136579

Best,
Maarten

On Sun, Feb 16, 2020 at 8:30 AM Ades, James <jades at health.ucsd.edu> wrote:
>
> Thanks a lot for the responses! These are great.
>
> Sorry--I should've clarified field...it's for Neuroscience/Psych. Still, speaking for that specific field, I can't find any unanimous agreement.
>
> Daniel, you're sjPlot package is amazing! I'm using it for our current paper and incorporated the patchwork package to create something I never could've otherwise. Easy to learn, simple and intuitive to execute, plentiful with options.
>
> I just found this paper on reporting R^2 for mixed models: https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210x.2012.00261.x I think I'll also include that, as R^2 (adjusted), at least to me, provides a more intuitive interpretation to people both in and out of the scientific community. Thanks for referencing the "performance" package. I'll look into that.
>
> Awesome! Thanks much, John, Thierry, and Daniel
> [https://besjournals.onlinelibrary.wiley.com/cms/asset/d67e1788-346b-4b61-8614-30bb2b237646/mee3.2013.4.issue-2.cover.gif]<https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210x.2012.00261.x>
> A general and simple method for obtaining R2 from generalized linear mixed?effects models<https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210x.2012.00261.x>
> The use of both linear and generalized linear mixed?effects models (LMMs and GLMMs) has become popular not only in social and medical sciences, but also in biological sciences, especially in the ...
> besjournals.onlinelibrary.wiley.com
>
> ________________________________
> From: Daniel L?decke <d.luedecke at uke.de>
> Sent: Saturday, February 15, 2020 7:00 AM
> To: Ades, James <jades at health.ucsd.edu>
> Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> Subject: AW: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients
>
> The "parameters" package (https://easystats.github.io/parameters/) offers some convenient functions to extract standard errors, p-values or confidence intervals for a vast range of models. Just use "model_parameters()" or "ci()" if you are only interested in CIs. Note that there is a small issue with p-values/CIs based on Kenward-Roger or Satterthwaite approximated degrees of freedom with the current CRAN version, however, these issues are fixed in the latest GitHub version.
>
> According to your original question: it really depends on the field, or even on the journal what information is required. I would say estimate, CI and p-value are often the "standard", and some information on the random effect variances (which you can also get with the parameters-package, using "random_parameters()") and/or R2/ICC are also useful measures to have information about the proportion of explained variance that can be accounted to the random effect parameters (R2 and ICC, in turn, are available in the "performance" package - https://easystats.github.io/performance/).
>
> I personally usually report estimates, CIs, p-values, within- and between-group-variances and ICC (and here again: "group" is sometimes called "subjects", sometimes "clusters", depending on the discipline).
>
> Best
> Daniel
>
>
> -----Urspr?ngliche Nachricht-----
> Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im Auftrag von Ades, James
> Gesendet: Samstag, 15. Februar 2020 01:29
> An: Thierry Onkelinx <thierry.onkelinx at inbo.be>
> Cc: r-sig-mixed-models at r-project.org
> Betreff: Re: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients
>
> Thanks, Thierry. This is what I was looking for!
>
> When I try confint(lme4_model) I get the following warning:
>
> ```{r}
>
> Computing profile confidence intervals ...
> Error in zeta(shiftpar, start = opt[seqpar1][-w]) :
>   profiling detected new, lower deviance
>
> ```
> Is there an easier way of extracting confidence intervals for fixed effects in lme4 than calculating them using the point estimate +/- Z * SE ?
>
> Best,
> James
> ________________________________
> From: Thierry Onkelinx <thierry.onkelinx at inbo.be>
> Sent: Friday, February 14, 2020 1:47 AM
> To: Ades, James <jades at health.ucsd.edu>
> Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients
>
> Dear James,
>
> IMHO the estimate and its CI works best. They instantly provide the range of uncertainty around the estimate without the reader having to do the math. CI also work with skewed distributions. p-values don't offer much added value over a CI.
> Below are a few examples of four estimates and their uncertainties. The first line displays the estimate and its SE. The second line displays the estimate, SE and p-values. The third displays the estimate and a relative error. While the last one displays the estimate and 95% CI.
>
> Keep in mind that readers are more likely to understand CI rather than SE.
>
> "1.2 ? 0.3"  "10.5 ? 4.5" "0.0 ? 0.3"  "0.0 ? 5.0"
> "1.2 ? 0.3 (p = 0.0001)"  "10.5 ? 4.5 (p = 0.0196)" "0.0 ? 0.3 (p = 1.0000)"  "0.0 ? 5.0 (p = 1.0000)"
>  "1.2 ? 25.0%"  "10.5 ? 42.9%" "0.0 ? Inf%"   "0.0 ? Inf%"
> "1.2 (0.6; 1.8)"   "10.5 (1.7; 19.3)" "0.0 (-0.6; 0.6)"  "0.0 (-9.8; 9.8)"
>
> Best regards,
>
> Thierry
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be<http://www.inbo.be>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> [https://inbo-website-prd-532750756126.s3-eu-west-1.amazonaws.com/inbologoleeuw_nl.png]<https://www.inbo.be>
>
>
> Op vr 14 feb. 2020 om 09:31 schreef Ades, James <jades at health.ucsd.edu<mailto:jades at health.ucsd.edu>>:
> Hi all,
>
>
>
> It?s been surprisingly difficult to find the most principled reporting of mixed-effect model regression coefficients (for individual fixed-effects). One stack overflow article lead me to this paper?a systematic review of the incorporating and reporting of GLMMs ( https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0112653#pone.0112653.s001)  which references a paper by Ben Bolker (https://www.sciencedirect.com/science/article/pii/S0169534709000196). Oddly, I don?t really find an answer to this in either of those. I?ve heard mixed things regarding fixed effect coefficients in LMM (that LMM/and GLMMs are more about the predictive power of an entire model than the individual predictors themselves), but overall, my understanding is that it?s kosher (and informative) to look at effect sizes of regression (fixed effect) coefficients?only that lme4 doesn?t currently provide p values (though Lmertest does).
>
>
>
> It seems like reporting effect size of regression coefficients and their SEs should suffice; though sometimes people report CI with those as well (but isn?t that a little redundant). My PI is telling me to include p-values. So many different things, so little agreement.
>
>
>
> I figured I?d turn here for something of a ?definitive? answer.
>
>
>
> Ben, I definitely need to go back and read through your paper more thoroughly for a deeper understanding of the nuances of GLMMs. Currently watching?and reading?McElreath?s Statistical Rethinking, but I?m not quite at the level of implementing MCMCs.
>
>
> Much thanks,
>
>
> James
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>         [[alternative HTML version deleted]]
>
>
> --
>
> _____________________________________________________________________
>
> Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
> Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Marya Verdel
> _____________________________________________________________________
>
> SAVE PAPER - THINK BEFORE PRINTING
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From juh@@|@pp|@@jk @end|ng |rom gm@||@com  Sun Feb 16 15:48:05 2020
From: juh@@|@pp|@@jk @end|ng |rom gm@||@com (Juha Lappi)
Date: Sun, 16 Feb 2020 16:48:05 +0200
Subject: [R-sig-ME] glmmTMB: biased marginal predictions in zero-inflated
 negative binomial mixed models
Message-ID: <CALscAnbBxRZnzoeuBWSjN0v=SHou9KDUYaZRqNC7huht_93LqQ@mail.gmail.com>

I have used zero-inflated negative binomial models for modeling tree
ingrowth in forests. The data set contains 1255 observations from 850
sample plots, each plot having 1 or 2 measurements. The purpose is to use
the models outside the estimation data, so the marginal prediction is of
the main interest. I?m using glmmTMB with nbinom2 family and maximum
likelihood estimation.  When I estimate models without random effects, I
get unbiased marginal predictions and the estimated right-censored
distribution agrees exactly with the empirical distribution. There is very
high overdispersion, the overdispersion parameter is around 0.4.

When I add random plot effects to the conditional model, there are two
problems. If I just add random plot effect to the optimal fixed-effects
model, the estimation does not converge. Using the parameter values
obtained from the fixed-effect estimation as starting values, the
estimation converges. A reasonable looking model with higher likelihood is
obtained and all predictors in the conditional model remain significant and
all predictors except one remain significant also in the zero-inflation
model. The overdispersion parameter increases to around 4.  When I then
drop all predictors from the zero-inflation part except the intercept, the
likelihood increases even if it should decrease.  Starting from the
intercept-only model one can obtain quite simple model which has higher
likelihood than more complicated models where all predictors are anyhow
very significant. The simpler models have larger variance for the plot
effects and larger over dispersion parameter (i.e. smaller overdispersion),
as seems logical. Anyhow, it seems that better fitting models with more
parameters do not have higher likelihoods as they should.

Second problem is related to the marginal predictor. I have considered two
predictors:

(1-1/(1+exp(-linear zi-predictor))*exp(linear predictor for the conditional
model)

In the second predictor 0.5*var(random plot effect) is added to the linear
predictor of the conditional model. The first predictor is biased
downwards, as it should. But the second predictor is biased upwards. The
bias is worse for the simpler models with higher likelihoods. For instance,
the average pine ingrowth count is 0.9, the average prediction with the
optimal simple model is 4.2 and with a more complicated model with lower
likelihood the average is 1.7. The standard deviations of residuals are
also better in fixed-effect models than in the mixed effects models with
higher likelihoods.

I regard that the strange behavior in mixed models is caused by the fact
that the random effects have very non-normal distribution. Do you have
better explanations?



Juha

	[[alternative HTML version deleted]]


From j@de@ @end|ng |rom he@|th@uc@d@edu  Mon Feb 17 05:31:30 2020
From: j@de@ @end|ng |rom he@|th@uc@d@edu (Ades, James)
Date: Mon, 17 Feb 2020 04:31:30 +0000
Subject: [R-sig-ME] Most principled reporting of mixed-effect model
 regression coefficients
In-Reply-To: <CAHr4Dyc7PXYWPax8Zzgvi+-FDmCUoWG=8-RyXDvjNb+Lau303w@mail.gmail.com>
References: <DM5PR1901MB20072EB8F9BE9DBE5E82002BEA150@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <CAJuCY5xhVYBgZwfwmb-gO9=YYhkg7RA7B4dDjo1_xp_xX6di5g@mail.gmail.com>
 <DM5PR1901MB2007B1C5BF040057769C97D7EA140@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <000001d5e410$a5919990$f0b4ccb0$@uke.de>
 <27250084e9e148ba860e21e55ca4f53e@MSX-L104.msx.ad.zih.tu-dresden.de>,
 <CAHr4Dyc7PXYWPax8Zzgvi+-FDmCUoWG=8-RyXDvjNb+Lau303w@mail.gmail.com>
Message-ID: <DM5PR1901MB2007B8D4B7A1A5BF7B10317AEA160@DM5PR1901MB2007.namprd19.prod.outlook.com>

Thanks, Maarten. So I was planning on reporting R^2 (along with AIC) for the overall model fit, not for each predictor, since the regression coefficients themselves give a good indication of relationship (though I wasn't aware that R^2 is "riddled with complications") Is Henrik only saying this only with regard to LMMs and GLMMs?

When you say "there is no agreed upon way to calculate effect sizes" I'm a little confused. I read through your stack exchange posting, but Henrik's answer refers to standardized effect size. You write, later down, "Whenever possible, we report unstandardized effect sizes which is in line with general recommendation of how to report effect sizes"

I'm also working on a systematic review where there's disagreement over whether effect sizes should be standardized, but it does seem that yield any kind of meaningful comparison, effect sizes would have to be standardized. I don't usually report standardized effect sizes...however, there are times when I z-score IVs to put them on the same scale, and I guess the output of that would be a standardized effect size. I wasn't aware of push back on that practice. What issues would arise from this?

I learned that mixed models are used predominantly for overall predictions vs individual coefficients, but I still was under the impression that one could derive effect sizes from predictor variables, and that this was largely sound. Am I incorrect?

In this particular study, there are four timepoints with 1286 students, though at each timepoint, there are roughly 1000 students. All students complete the same executive function tasks, so in that regard, there isn't really a formal factorial design at play, though there are multiple independent variables.

Best,

James
________________________________
From: Maarten Jung <Maarten.Jung at mailbox.tu-dresden.de>
Sent: Sunday, February 16, 2020 12:36 AM
To: Ades, James <jades at health.ucsd.edu>
Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients

Dear James,

I think most people from Psychology (and many from Neuroscience, but
probably more dependent on the subfield) are used to see the value of
a test statistic (often F-tests), the number(s) of degrees of freedom,
the corresponding p-value, and some sort of effect-size measure.
To calculate the semi-partial-R-squared values for each categorical
predictor (if you have a factorial design) as described in Jaeger,
Edwards, Das, and Sen (2017) [1], you can use the r2glmm::r2beta()
function which you should get from GitHub.
Note that there is (to my knowledge) no agreed-upon way to calculate
effect sizes for (linear) mixed models. My answer here [2] might be
helpful for a quick overview and my personal take on this topic.

[1] https://doi.org/10.1080/02664763.2016.1193725
[2] https://stats.stackexchange.com/a/439842/136579

Best,
Maarten

On Sun, Feb 16, 2020 at 8:30 AM Ades, James <jades at health.ucsd.edu> wrote:
>
> Thanks a lot for the responses! These are great.
>
> Sorry--I should've clarified field...it's for Neuroscience/Psych. Still, speaking for that specific field, I can't find any unanimous agreement.
>
> Daniel, you're sjPlot package is amazing! I'm using it for our current paper and incorporated the patchwork package to create something I never could've otherwise. Easy to learn, simple and intuitive to execute, plentiful with options.
>
> I just found this paper on reporting R^2 for mixed models: https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210x.2012.00261.x I think I'll also include that, as R^2 (adjusted), at least to me, provides a more intuitive interpretation to people both in and out of the scientific community. Thanks for referencing the "performance" package. I'll look into that.
>
> Awesome! Thanks much, John, Thierry, and Daniel
> [https://besjournals.onlinelibrary.wiley.com/cms/asset/d67e1788-346b-4b61-8614-30bb2b237646/mee3.2013.4.issue-2.cover.gif]<https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210x.2012.00261.x>
> A general and simple method for obtaining R2 from generalized linear mixed?effects models<https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210x.2012.00261.x>
> The use of both linear and generalized linear mixed?effects models (LMMs and GLMMs) has become popular not only in social and medical sciences, but also in biological sciences, especially in the ...
> besjournals.onlinelibrary.wiley.com
>
> ________________________________
> From: Daniel L?decke <d.luedecke at uke.de>
> Sent: Saturday, February 15, 2020 7:00 AM
> To: Ades, James <jades at health.ucsd.edu>
> Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> Subject: AW: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients
>
> The "parameters" package (https://easystats.github.io/parameters/) offers some convenient functions to extract standard errors, p-values or confidence intervals for a vast range of models. Just use "model_parameters()" or "ci()" if you are only interested in CIs. Note that there is a small issue with p-values/CIs based on Kenward-Roger or Satterthwaite approximated degrees of freedom with the current CRAN version, however, these issues are fixed in the latest GitHub version.
>
> According to your original question: it really depends on the field, or even on the journal what information is required. I would say estimate, CI and p-value are often the "standard", and some information on the random effect variances (which you can also get with the parameters-package, using "random_parameters()") and/or R2/ICC are also useful measures to have information about the proportion of explained variance that can be accounted to the random effect parameters (R2 and ICC, in turn, are available in the "performance" package - https://easystats.github.io/performance/).
>
> I personally usually report estimates, CIs, p-values, within- and between-group-variances and ICC (and here again: "group" is sometimes called "subjects", sometimes "clusters", depending on the discipline).
>
> Best
> Daniel
>
>
> -----Urspr?ngliche Nachricht-----
> Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im Auftrag von Ades, James
> Gesendet: Samstag, 15. Februar 2020 01:29
> An: Thierry Onkelinx <thierry.onkelinx at inbo.be>
> Cc: r-sig-mixed-models at r-project.org
> Betreff: Re: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients
>
> Thanks, Thierry. This is what I was looking for!
>
> When I try confint(lme4_model) I get the following warning:
>
> ```{r}
>
> Computing profile confidence intervals ...
> Error in zeta(shiftpar, start = opt[seqpar1][-w]) :
>   profiling detected new, lower deviance
>
> ```
> Is there an easier way of extracting confidence intervals for fixed effects in lme4 than calculating them using the point estimate +/- Z * SE ?
>
> Best,
> James
> ________________________________
> From: Thierry Onkelinx <thierry.onkelinx at inbo.be>
> Sent: Friday, February 14, 2020 1:47 AM
> To: Ades, James <jades at health.ucsd.edu>
> Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients
>
> Dear James,
>
> IMHO the estimate and its CI works best. They instantly provide the range of uncertainty around the estimate without the reader having to do the math. CI also work with skewed distributions. p-values don't offer much added value over a CI.
> Below are a few examples of four estimates and their uncertainties. The first line displays the estimate and its SE. The second line displays the estimate, SE and p-values. The third displays the estimate and a relative error. While the last one displays the estimate and 95% CI.
>
> Keep in mind that readers are more likely to understand CI rather than SE.
>
> "1.2 ? 0.3"  "10.5 ? 4.5" "0.0 ? 0.3"  "0.0 ? 5.0"
> "1.2 ? 0.3 (p = 0.0001)"  "10.5 ? 4.5 (p = 0.0196)" "0.0 ? 0.3 (p = 1.0000)"  "0.0 ? 5.0 (p = 1.0000)"
>  "1.2 ? 25.0%"  "10.5 ? 42.9%" "0.0 ? Inf%"   "0.0 ? Inf%"
> "1.2 (0.6; 1.8)"   "10.5 (1.7; 19.3)" "0.0 (-0.6; 0.6)"  "0.0 (-9.8; 9.8)"
>
> Best regards,
>
> Thierry
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be<http://www.inbo.be>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> [https://inbo-website-prd-532750756126.s3-eu-west-1.amazonaws.com/inbologoleeuw_nl.png]<https://www.inbo.be>
>
>
> Op vr 14 feb. 2020 om 09:31 schreef Ades, James <jades at health.ucsd.edu<mailto:jades at health.ucsd.edu>>:
> Hi all,
>
>
>
> It?s been surprisingly difficult to find the most principled reporting of mixed-effect model regression coefficients (for individual fixed-effects). One stack overflow article lead me to this paper?a systematic review of the incorporating and reporting of GLMMs ( https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0112653#pone.0112653.s001)  which references a paper by Ben Bolker (https://www.sciencedirect.com/science/article/pii/S0169534709000196). Oddly, I don?t really find an answer to this in either of those. I?ve heard mixed things regarding fixed effect coefficients in LMM (that LMM/and GLMMs are more about the predictive power of an entire model than the individual predictors themselves), but overall, my understanding is that it?s kosher (and informative) to look at effect sizes of regression (fixed effect) coefficients?only that lme4 doesn?t currently provide p values (though Lmertest does).
>
>
>
> It seems like reporting effect size of regression coefficients and their SEs should suffice; though sometimes people report CI with those as well (but isn?t that a little redundant). My PI is telling me to include p-values. So many different things, so little agreement.
>
>
>
> I figured I?d turn here for something of a ?definitive? answer.
>
>
>
> Ben, I definitely need to go back and read through your paper more thoroughly for a deeper understanding of the nuances of GLMMs. Currently watching?and reading?McElreath?s Statistical Rethinking, but I?m not quite at the level of implementing MCMCs.
>
>
> Much thanks,
>
>
> James
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>         [[alternative HTML version deleted]]
>
>
> --
>
> _____________________________________________________________________
>
> Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
> Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Marya Verdel
> _____________________________________________________________________
>
> SAVE PAPER - THINK BEFORE PRINTING
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From d@|uedecke @end|ng |rom uke@de  Mon Feb 17 09:38:11 2020
From: d@|uedecke @end|ng |rom uke@de (=?utf-8?Q?Daniel_L=C3=BCdecke?=)
Date: Mon, 17 Feb 2020 09:38:11 +0100
Subject: [R-sig-ME] Most principled reporting of mixed-effect model
 regression coefficients
In-Reply-To: <DM5PR1901MB2007B8D4B7A1A5BF7B10317AEA160@DM5PR1901MB2007.namprd19.prod.outlook.com>
References: <DM5PR1901MB20072EB8F9BE9DBE5E82002BEA150@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <CAJuCY5xhVYBgZwfwmb-gO9=YYhkg7RA7B4dDjo1_xp_xX6di5g@mail.gmail.com>
 <DM5PR1901MB2007B1C5BF040057769C97D7EA140@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <000001d5e410$a5919990$f0b4ccb0$@uke.de>
 <27250084e9e148ba860e21e55ca4f53e@MSX-L104.msx.ad.zih.tu-dresden.de>,
 <CAHr4Dyc7PXYWPax8Zzgvi+-FDmCUoWG=8-RyXDvjNb+Lau303w@mail.gmail.com>
 <DM5PR1901MB2007B8D4B7A1A5BF7B10317AEA160@DM5PR1901MB2007.namprd19.prod.outlook.com>
Message-ID: <004001d5e56d$96452ed0$c2cf8c70$@uke.de>

Dear James,

> however, there are times when I z-score IVs to put them on the same scale, and I guess the output of that would be a standardized effect size. I wasn't aware of push back on that practice. What issues would arise from this

No, I don't think that standardizing your data before model fitting is a problem, and this indeed gives you comparable estimates (in the sense of "being on the same scale"). I think lme4 sometimes even gives a message that recommends rescaling predictors that are on very different scales.

The above mentioned comparison only refers to the summary output. You could also compute "classical" Anova tables with effects sizes, but this is what is problematic in mixed models, afaik. In the below example, "summary()" gives you "effect sizes" (estimates) for the interaction at each level of Species, while "anova()" does not. For the Anova-table it is difficult to calculate reliable effect sizes like eta-squared etc. in mixed models, but if you standardize your data before model fitting, the summary gives you comparable estimates. I think this is also a matter of terminology what people call "effect size", this also differs between fields/disciplines. 

> though I wasn't aware that R^2 is "riddled with complications"

I think the problem is the "variance decomposition" for more complex models like mixed models, in particular for other link-functions/families. That is why parts of the variance components in mixed models are not "exact", but rather approximated (see Nakagawa, S., Johnson, P. C. D., & Schielzeth, H. (2017)  doi: 10.1098/rsif.2017.0213), which is implemented in the "r2()" function from the performance-package) 

> reporting R^2 (along with AIC)

Note that when reporting AIC, in particular for model comparison, models need to be refitted with ML, not REML. "aic()" in lme4, however, does this automatically for you.

Best
Daniel

data(iris)
m <- lm(Sepal.Length ~ Species * Sepal.Width, data = iris)
summary(m)
#> 
#> Call:
#> lm(formula = Sepal.Length ~ Species * Sepal.Width, data = iris)
#> 
#> Residuals:
#>      Min       1Q   Median       3Q      Max 
#> -1.26067 -0.25861 -0.03305  0.18929  1.44917 
#> 
#> Coefficients:
#>                               Estimate Std. Error t value Pr(>|t|)    
#> (Intercept)                     2.6390     0.5715   4.618 8.53e-06 ***
#> Speciesversicolor               0.9007     0.7988   1.128    0.261    
#> Speciesvirginica                1.2678     0.8162   1.553    0.123    
#> Sepal.Width                     0.6905     0.1657   4.166 5.31e-05 ***
#> Speciesversicolor:Sepal.Width   0.1746     0.2599   0.672    0.503    
#> Speciesvirginica:Sepal.Width    0.2110     0.2558   0.825    0.411    
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#> 
#> Residual standard error: 0.4397 on 144 degrees of freedom
#> Multiple R-squared:  0.7274, Adjusted R-squared:  0.718 
#> F-statistic: 76.87 on 5 and 144 DF,  p-value: < 2.2e-16
anova(m)
#> Analysis of Variance Table
#> 
#> Response: Sepal.Length
#>                      Df Sum Sq Mean Sq  F value    Pr(>F)    
#> Species               2 63.212 31.6061 163.4417 < 2.2e-16 ***
#> Sepal.Width           1 10.953 10.9525  56.6379 5.221e-12 ***
#> Species:Sepal.Width   2  0.157  0.0786   0.4064    0.6668    
#> Residuals           144 27.846  0.1934                       
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1



-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org] Im Auftrag von Ades, James
Gesendet: Montag, 17. Februar 2020 05:32
An: Maarten Jung <Maarten.Jung at mailbox.tu-dresden.de>
Cc: r-sig-mixed-models at r-project.org
Betreff: Re: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients

Thanks, Maarten. So I was planning on reporting R^2 (along with AIC) for the overall model fit, not for each predictor, since the regression coefficients themselves give a good indication of relationship (though I wasn't aware that R^2 is "riddled with complications") Is Henrik only saying this only with regard to LMMs and GLMMs?

When you say "there is no agreed upon way to calculate effect sizes" I'm a little confused. I read through your stack exchange posting, but Henrik's answer refers to standardized effect size. You write, later down, "Whenever possible, we report unstandardized effect sizes which is in line with general recommendation of how to report effect sizes"

I'm also working on a systematic review where there's disagreement over whether effect sizes should be standardized, but it does seem that yield any kind of meaningful comparison, effect sizes would have to be standardized. I don't usually report standardized effect sizes...however, there are times when I z-score IVs to put them on the same scale, and I guess the output of that would be a standardized effect size. I wasn't aware of push back on that practice. What issues would arise from this?

I learned that mixed models are used predominantly for overall predictions vs individual coefficients, but I still was under the impression that one could derive effect sizes from predictor variables, and that this was largely sound. Am I incorrect?

In this particular study, there are four timepoints with 1286 students, though at each timepoint, there are roughly 1000 students. All students complete the same executive function tasks, so in that regard, there isn't really a formal factorial design at play, though there are multiple independent variables.

Best,

James
________________________________
From: Maarten Jung <Maarten.Jung at mailbox.tu-dresden.de>
Sent: Sunday, February 16, 2020 12:36 AM
To: Ades, James <jades at health.ucsd.edu>
Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients

Dear James,

I think most people from Psychology (and many from Neuroscience, but probably more dependent on the subfield) are used to see the value of a test statistic (often F-tests), the number(s) of degrees of freedom, the corresponding p-value, and some sort of effect-size measure.
To calculate the semi-partial-R-squared values for each categorical predictor (if you have a factorial design) as described in Jaeger, Edwards, Das, and Sen (2017) [1], you can use the r2glmm::r2beta() function which you should get from GitHub.
Note that there is (to my knowledge) no agreed-upon way to calculate effect sizes for (linear) mixed models. My answer here [2] might be helpful for a quick overview and my personal take on this topic.

[1] https://doi.org/10.1080/02664763.2016.1193725
[2] https://stats.stackexchange.com/a/439842/136579

Best,
Maarten

On Sun, Feb 16, 2020 at 8:30 AM Ades, James <jades at health.ucsd.edu> wrote:
>
> Thanks a lot for the responses! These are great.
>
> Sorry--I should've clarified field...it's for Neuroscience/Psych. Still, speaking for that specific field, I can't find any unanimous agreement.
>
> Daniel, you're sjPlot package is amazing! I'm using it for our current paper and incorporated the patchwork package to create something I never could've otherwise. Easy to learn, simple and intuitive to execute, plentiful with options.
>
> I just found this paper on reporting R^2 for mixed models: https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210x.2012.00261.x I think I'll also include that, as R^2 (adjusted), at least to me, provides a more intuitive interpretation to people both in and out of the scientific community. Thanks for referencing the "performance" package. I'll look into that.
>
> Awesome! Thanks much, John, Thierry, and Daniel 
> [https://besjournals.onlinelibrary.wiley.com/cms/asset/d67e1788-346b-4
> b61-8614-30bb2b237646/mee3.2013.4.issue-2.cover.gif]<https://besjourna
> ls.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210x.2012.00261.x>
> A general and simple method for obtaining R2 from generalized linear 
> mixed?effects 
> models<https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.
> 2041-210x.2012.00261.x> The use of both linear and generalized linear 
> mixed?effects models (LMMs and GLMMs) has become popular not only in social and medical sciences, but also in biological sciences, especially in the ...
> besjournals.onlinelibrary.wiley.com
>
> ________________________________
> From: Daniel L?decke <d.luedecke at uke.de>
> Sent: Saturday, February 15, 2020 7:00 AM
> To: Ades, James <jades at health.ucsd.edu>
> Cc: r-sig-mixed-models at r-project.org 
> <r-sig-mixed-models at r-project.org>
> Subject: AW: [R-sig-ME] Most principled reporting of mixed-effect 
> model regression coefficients
>
> The "parameters" package (https://easystats.github.io/parameters/) offers some convenient functions to extract standard errors, p-values or confidence intervals for a vast range of models. Just use "model_parameters()" or "ci()" if you are only interested in CIs. Note that there is a small issue with p-values/CIs based on Kenward-Roger or Satterthwaite approximated degrees of freedom with the current CRAN version, however, these issues are fixed in the latest GitHub version.
>
> According to your original question: it really depends on the field, or even on the journal what information is required. I would say estimate, CI and p-value are often the "standard", and some information on the random effect variances (which you can also get with the parameters-package, using "random_parameters()") and/or R2/ICC are also useful measures to have information about the proportion of explained variance that can be accounted to the random effect parameters (R2 and ICC, in turn, are available in the "performance" package - https://easystats.github.io/performance/).
>
> I personally usually report estimates, CIs, p-values, within- and between-group-variances and ICC (and here again: "group" is sometimes called "subjects", sometimes "clusters", depending on the discipline).
>
> Best
> Daniel
>
>
> -----Urspr?ngliche Nachricht-----
> Von: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> Im 
> Auftrag von Ades, James
> Gesendet: Samstag, 15. Februar 2020 01:29
> An: Thierry Onkelinx <thierry.onkelinx at inbo.be>
> Cc: r-sig-mixed-models at r-project.org
> Betreff: Re: [R-sig-ME] Most principled reporting of mixed-effect 
> model regression coefficients
>
> Thanks, Thierry. This is what I was looking for!
>
> When I try confint(lme4_model) I get the following warning:
>
> ```{r}
>
> Computing profile confidence intervals ...
> Error in zeta(shiftpar, start = opt[seqpar1][-w]) :
>   profiling detected new, lower deviance
>
> ```
> Is there an easier way of extracting confidence intervals for fixed effects in lme4 than calculating them using the point estimate +/- Z * SE ?
>
> Best,
> James
> ________________________________
> From: Thierry Onkelinx <thierry.onkelinx at inbo.be>
> Sent: Friday, February 14, 2020 1:47 AM
> To: Ades, James <jades at health.ucsd.edu>
> Cc: r-sig-mixed-models at r-project.org 
> <r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] Most principled reporting of mixed-effect 
> model regression coefficients
>
> Dear James,
>
> IMHO the estimate and its CI works best. They instantly provide the range of uncertainty around the estimate without the reader having to do the math. CI also work with skewed distributions. p-values don't offer much added value over a CI.
> Below are a few examples of four estimates and their uncertainties. The first line displays the estimate and its SE. The second line displays the estimate, SE and p-values. The third displays the estimate and a relative error. While the last one displays the estimate and 95% CI.
>
> Keep in mind that readers are more likely to understand CI rather than SE.
>
> "1.2   0.3"  "10.5   4.5" "0.0   0.3"  "0.0   5.0"
> "1.2   0.3 (p = 0.0001)"  "10.5   4.5 (p = 0.0196)" "0.0   0.3 (p = 1.0000)"  "0.0   5.0 (p = 1.0000)"
>  "1.2   25.0%"  "10.5   42.9%" "0.0   Inf%"   "0.0   Inf%"
> "1.2 (0.6; 1.8)"   "10.5 (1.7; 19.3)" "0.0 (-0.6; 0.6)"  "0.0 (-9.8; 9.8)"
>
> Best regards,
>
> Thierry
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders INSTITUUT VOOR NATUUR- EN 
> BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST Team Biometrie 
> & Kwaliteitszorg / Team Biometrics & Quality Assurance 
> thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be<http://www.inbo.be>
>
> //////////////////////////////////////////////////////////////////////
> ///////////////////// To call in the statistician after the experiment 
> is done may be no more than asking him to perform a post-mortem 
> examination: he may be able to say what the experiment died of. ~ Sir 
> Ronald Aylmer Fisher The plural of anecdote is not data. ~ Roger 
> Brinner The combination of some data and an aching desire for an 
> answer does not ensure that a reasonable answer can be extracted from 
> a given body of data. ~ John Tukey 
> //////////////////////////////////////////////////////////////////////
> /////////////////////
>
> [https://inbo-website-prd-532750756126.s3-eu-west-1.amazonaws.com/inbo
> logoleeuw_nl.png]<https://www.inbo.be>
>
>
> Op vr 14 feb. 2020 om 09:31 schreef Ades, James <jades at health.ucsd.edu<mailto:jades at health.ucsd.edu>>:
> Hi all,
>
>
>
> It s been surprisingly difficult to find the most principled reporting of mixed-effect model regression coefficients (for individual fixed-effects). One stack overflow article lead me to this paper a systematic review of the incorporating and reporting of GLMMs ( https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0112653#pone.0112653.s001)  which references a paper by Ben Bolker (https://www.sciencedirect.com/science/article/pii/S0169534709000196). Oddly, I don t really find an answer to this in either of those. I ve heard mixed things regarding fixed effect coefficients in LMM (that LMM/and GLMMs are more about the predictive power of an entire model than the individual predictors themselves), but overall, my understanding is that it s kosher (and informative) to look at effect sizes of regression (fixed effect) coefficients only that lme4 doesn t currently provide p values (though Lmertest does).
>
>
>
> It seems like reporting effect size of regression coefficients and their SEs should suffice; though sometimes people report CI with those as well (but isn t that a little redundant). My PI is telling me to include p-values. So many different things, so little agreement.
>
>
>
> I figured I d turn here for something of a  definitive  answer.
>
>
>
> Ben, I definitely need to go back and read through your paper more thoroughly for a deeper understanding of the nuances of GLMMs. Currently watching and reading McElreath s Statistical Rethinking, but I m not quite at the level of implementing MCMCs.
>
>
> Much thanks,
>
>
> James
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.o
> rg> mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>         [[alternative HTML version deleted]]
>
>
> --
>
> _____________________________________________________________________
>
> Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen 
> Rechts; Gerichtsstand: Hamburg | www.uke.de
> Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. 
> Dr. Uwe Koch-Gromus, Joachim Pr?l?, Marya Verdel 
> _____________________________________________________________________
>
> SAVE PAPER - THINK BEFORE PRINTING
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING

From M@@rten@Jung @end|ng |rom m@||box@tu-dre@den@de  Mon Feb 17 10:35:53 2020
From: M@@rten@Jung @end|ng |rom m@||box@tu-dre@den@de (Maarten Jung)
Date: Mon, 17 Feb 2020 10:35:53 +0100
Subject: [R-sig-ME] Most principled reporting of mixed-effect model
 regression coefficients
In-Reply-To: <e758416e0c78493eb4ed4b98b8080a02@MSX-L104.msx.ad.zih.tu-dresden.de>
References: <DM5PR1901MB20072EB8F9BE9DBE5E82002BEA150@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <CAJuCY5xhVYBgZwfwmb-gO9=YYhkg7RA7B4dDjo1_xp_xX6di5g@mail.gmail.com>
 <DM5PR1901MB2007B1C5BF040057769C97D7EA140@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <000001d5e410$a5919990$f0b4ccb0$@uke.de>
 <27250084e9e148ba860e21e55ca4f53e@MSX-L104.msx.ad.zih.tu-dresden.de>
 <CAHr4Dyc7PXYWPax8Zzgvi+-FDmCUoWG=8-RyXDvjNb+Lau303w@mail.gmail.com>
 <e758416e0c78493eb4ed4b98b8080a02@MSX-L104.msx.ad.zih.tu-dresden.de>
Message-ID: <CAHr4DycFj9omcOSFYvBZ3jSfdzTeAH2HV_Bwr4g5d=Es2wYY3w@mail.gmail.com>

> Thanks, Maarten. So I was planning on reporting R^2 (along with AIC) for the overall model fit, not for each predictor, since the regression coefficients themselves give a good indication of relationship (though I wasn't aware that R^2 is "riddled with complications") Is Henrik only saying this only with regard to LMMs and GLMMs?

That makes sense to me. For the overall model fit I would probably
still go with Johnson's version [1] which I describe in my
StackExchange post (and I think you mentioned it, or the Nakagawa and
Schielzeth version it is based on, earlier) and report both the
marginal and conditional R^2 values. The regression coefficients
provide unstandardized effect sizes on the response scale which I
think are a valid way to report effect sizes (see below).
I think Henrik refers to (G)LMMs and gives Rights & Sterba (2019) [2]
as reference. Also, the GLMM FAQ website provides a good overview [3].

> When you say "there is no agreed upon way to calculate effect sizes" I'm a little confused. I read through your stack exchange posting, but Henrik's answer refers to standardized effect size. You write, later down, "Whenever possible, we report unstandardized effect sizes which is in line with general recommendation of how to report effect sizes"

What you cite is still Henrik's opinion (and I hoped that I could make
this clear by writing "This is what he suggests [...]" and by using
the <blockquote> on StackExchange). And your citation still refers to
LMMs as he says "Unfortunately, due to the way that variance is
partitioned in linear mixed models (e.g., Rights & Sterba, 2019),
there does not exist an agreed upon way to calculate standard effect
sizes for individual model terms such as main effects or
interactions."
In general, I agree with him and with his recommendation to report
unstandardized effect sizes (e.g. regression coefficients) if they
have a "meaningful" interpretation.
The semi-partial R^2 I mentioned in my last e-mail is an
additional/alternative indicator of effect sizes that is probably more
in line with what psychologists are used to see reported in papers
(especially when results of factorial designs are reported) - and
that's the reason I mentioned it.

> I'm also working on a systematic review where there's disagreement over whether effect sizes should be standardized, but it does seem that yield any kind of meaningful comparison, effect sizes would have to be standardized. I don't usually report standardized effect sizes...however, there are times when I z-score IVs to put them on the same scale, and I guess the output of that would be a standardized effect size. I wasn't aware of push back on that practice. What issues would arise from this?

There is nothing wrong with standardizing (e.g. by diving by 1 or 2
standard deviations) predictor variables to get measures of variable
importance (within the same model).
Issues arise when standardized effect sizes such as R^2, partial
eta^2, etc. between different models are compared without thinking
about what differences in these measures can be attributed to (see
e.g. this question [4] or the Pek & Flora (2018) paper [5] that Henrik
cites). Note that these are general issues that apply to all
regression models, not only mixed models.

[1] https://doi.org/10.1111/2041-210X.12225
[2] https://doi.org/10.1037/met0000184
[3] https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#how-do-i-compute-a-coefficient-of-determination-r2-or-an-analogue-for-glmms
[4] https://stats.stackexchange.com/questions/13314/is-r2-useful-or-dangerous/13317
[5] https://doi.org/10.1037/met0000126

Best,
Maarten


From ph||||p@@|d@y @end|ng |rom mp|@n|  Mon Feb 17 11:48:06 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Mon, 17 Feb 2020 11:48:06 +0100
Subject: [R-sig-ME] corAR1() structure in lme()
In-Reply-To: <CAFUVuJz-9_H02uKLFoHnM_y6FHr_AuSM-Gue97XUT_M4HdAzgQ@mail.gmail.com>
References: <CAFUVuJz-9_H02uKLFoHnM_y6FHr_AuSM-Gue97XUT_M4HdAzgQ@mail.gmail.com>
Message-ID: <3185928c-4369-86df-a17e-55c995439f1d@mpi.nl>

I have no idea -- you may have to dig through the nlme source code to
find out (https://github.com/cran/nlme/tree/master/R). nlme is largely
in maintenance? mode at this point -- it's been 6 years since the last
commit, so even if this is undesired behavior (=bug), there may not be
much to be done about it unless you contribute the fix yourself. :(

From a higher-level perspective, Pinheiro and Bates (2000) may have some
insights into why this conversion is allowed (which admittedly is
different than why it is done). A quick search gives me this result on
page 236:

> As described in ?5.3.1, the AR(1) model is equivalent to an ARMA(1, 0)
> model, so that the corARMA class can also be used to represent an corAR1
> object. However, the corAR1 methods are designed to take advantage of
> the particular structure of the AR(1) model, and are substantially more
> efficient than the corresponding corARMA methods.
Hope that helps,

Phillip


On 12/02/2020 23:48, James Pustejovsky wrote:
> Hello,
>
> I am writing some additional functionality for lme objects, and I have a
> question about how lme() handles corAR1() structures. I noticed that
> sometimes, if I specify a model with corAR1(), the fitted model has a
> corStruct of class corARMA instead of (as you would expect) corAR1(). Here
> are a few examples where this does and doesn't occur:
>
> library(nlme)
>
> data(Laski, package = "scdhlm")
>
> Laski_AR1 <- lme(fixed = outcome ~ treatment,
>                  random = ~ treatment | case,
>                  correlation = corAR1(0.2, ~ time | case),
>                  data = Laski)
>
> inherits(Laski_AR1$modelStruct$corStruct, "corAR1") # TRUE
>
> Laski_hetAR1 <- lme(fixed = outcome ~ treatment,
>                     random = ~ treatment | case,
>                     correlation = corAR1(0, ~ time | case),
>                     weights = varIdent(form = ~ 1 | treatment),
>                     data = Laski)
>
> inherits(Laski_hetAR1$modelStruct$corStruct, "corAR1") # TRUE
>
> data(egsingle, package = "mlmRev")
>
> lme_2level <-
>   lme(fixed = math ~ year + female + black + hispanic,
>       random = ~ 1 | childid,
>       correlation = corAR1(0, ~ year | childid),
>       data = egsingle)
>
> inherits(lme_2level$modelStruct$corStruct, "corAR1") # FALSE
>
>
> lme_3level <-
>   lme(fixed = math ~ year + female + black + hispanic,
>       random = ~ 1 | schoolid/childid,
>       correlation = corAR1(0.1, ~ year | schoolid/childid),
>       data = egsingle)
>
> inherits(lme_3level$modelStruct$corStruct, "corAR1") # FALSE
>
> Does anyone have a sense of why the class of the fitted model's corStruct
> is not consistent with the input class?
>
> Thanks for any pointers!
>
> Kind Regards,
> James
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From th|erry@onke||nx @end|ng |rom |nbo@be  Mon Feb 17 14:18:23 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Mon, 17 Feb 2020 14:18:23 +0100
Subject: [R-sig-ME] Fwd: model check for negative binomial model
In-Reply-To: <CA+6N3yXx23=4tajGD8DrZj=SOE+HbSX6K9sOQojt7YVVfgCJsw@mail.gmail.com>
References: <mailman.18062.3142.1581469252.1419.r-sig-mixed-models@r-project.org>
 <CA+6N3yV8ytczPND5x1VS5Vfe+H748vqjmc4=79wTnbJb1JmSbA@mail.gmail.com>
 <3cd5cea8-28c9-0799-9f17-d80a4aaa1ccc@gmail.com>
 <CA+6N3yXx23=4tajGD8DrZj=SOE+HbSX6K9sOQojt7YVVfgCJsw@mail.gmail.com>
Message-ID: <CAJuCY5w-aYigW9iX+0ZTruzqmgNAQtr7+A2j5fN-x5j38iSSLg@mail.gmail.com>

Dear Alessandra,

Since you have both the number hatched and the total clutch size you can
calculate the number of successes and failures. That is sufficient for a
binomial distribution.

glmer(cbind(Hatched, Unhatched) ~ Relocation..Y.N. + SP + (1 | Beach_ID) +
(1 | Week), family = binmial)

A negative binomial or Poisson allow predictions larger than the offset.
Which is nonsense given that the number hatched cannot surpass the total
clutch size.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op wo 12 feb. 2020 om 18:42 schreef Alessandra Bielli <
bielli.alessandra at gmail.com>:

> Dear Ben
>
> Thanks for your quick response.
>
> Yes, emergence success is usually between 60 and 80% or higher.
> I am not sure how to use a binomial, if my data are counts?
>
> Can you explain why the approximation doesn't work well if success gets
> much above 50%? Does it make sense, then, to have "unhatched" as dependent
> variable, so that I predict mortality (usually below 50%) using a nb with
> offset(log(total clutch)) ?
>
> > summary(m.emerged)
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: Negative Binomial(2.2104)  ( log )
> Formula: Unhatched ~ Relocation..Y.N. + SP + offset(log(Total_Clutch)) +
>    (1 | Beach_ID) + (1 | Week)
>    Data: main
>
>      AIC      BIC   logLik deviance df.resid
>   5439.4   5466.0  -2713.7   5427.4      614
>
> Scaled residuals:
>     Min      1Q  Median      3Q     Max
> -1.4383 -0.7242 -0.2287  0.4866  4.0531
>
> Random effects:
>  Groups   Name        Variance Std.Dev.
>  Week     (Intercept) 0.003092 0.0556
>  Beach_ID (Intercept) 0.025894 0.1609
> Number of obs: 620, groups:  Week, 31; Beach_ID, 8
>
> Fixed effects:
>                   Estimate Std. Error z value Pr(>|z|)
> (Intercept)       -1.38864    0.08227 -16.879  < 2e-16 ***
> Relocation..Y.N.Y  0.32105    0.09152   3.508 0.000452 ***
> SPL                0.22218    0.08793   2.527 0.011508 *
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Correlation of Fixed Effects:
>             (Intr) R..Y.N
> Rlct..Y.N.Y -0.143
> SPL         -0.540 -0.038
>
> Thanks,
>
> Alessandra
>
> On Tue, Feb 11, 2020 at 7:29 PM Ben Bolker <bbolker at gmail.com> wrote:
>
> >
> >   Short answer: if emergence success gets much above 50%, then the
> > approximation you're making (Poisson + offset for binomial, or NB +
> > offset for negative binomial) doesn't work well.  You might try a
> > beta-binomial (with glmmTMB) or a binomial + an observation-level random
> > effect.
> >
> >   (On the other hand, your intercept is -0.3, which corresponds to a
> > baseline emergence of 0.42 - not *very* high (but some beaches and years
> > will be well above that ...)
> >
> >   Beyond that, are there any obvious patterns of mis-fit in the
> > predicted values ... ?
> >
> > On 2020-02-11 8:09 p.m., Alessandra Bielli wrote:
> > > Dear list
> > >
> > > I am fitting a poisson model to estimate the effect of a treatment on
> > > emergence success of hatchlings. To estimate emergence success, I use
> > > number of emerged and an offset(log(total clutch).
> > >
> > > However, overdispersion was detected:
> > >
> > >> overdisp_fun(m.emerged) #overdispersion detected
> > >
> > >       chisq       ratio         rdf           p
> > > 3490.300836    5.684529  614.000000    0.000000
> > >
> > > Therefore, I switched to a negative binomial. I know overdispersion is
> > not
> > > relevant for nb models, but the model plots don't look too good. I also
> > > tried to fit a poisson model with OLRE, but still the  plots don't look
> > > good.
> > > How do I know if my model is good enough, and what can I do to improve
> > it?
> > >
> > >> summary(m.emerged)
> > > Generalized linear mixed model fit by maximum likelihood (Laplace
> > > Approximation) ['glmerMod']
> > >  Family: Negative Binomial(7.604)  ( log )
> > > Formula: Hatched ~ Relocation..Y.N. + SP + offset(log(Total_Clutch)) +
> (1
> > > |Beach_ID) + (1 | Year)
> > >    Data: main
> > >
> > >      AIC      BIC   logLik deviance df.resid
> > >   6015.6   6042.2  -3001.8   6003.6      614
> > >
> > > Scaled residuals:
> > >     Min      1Q  Median      3Q     Max
> > > -2.6427 -0.3790  0.1790  0.5242  1.6583
> > >
> > > Random effects:
> > >  Groups   Name        Variance Std.Dev.
> > >  Beach_ID (Intercept) 0.004438 0.06662
> > >  Year     (Intercept) 0.001640 0.04050
> > > Number of obs: 620, groups:  Beach_ID, 8; Year, 5
> > >
> > > Fixed effects:
> > >                   Estimate Std. Error z value Pr(>|z|)
> > > (Intercept)       -0.29915    0.04055  -7.377 1.62e-13 ***
> > > Relocation..Y.N.Y -0.16402    0.05052  -3.247  0.00117 **
> > > SPL               -0.08311    0.04365  -1.904  0.05689 .
> > > ---
> > > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >
> > > Correlation of Fixed Effects:
> > >             (Intr) R..Y.N
> > > Rlct..Y.N.Y -0.114
> > > SPL         -0.497 -0.054
> > >
> > >
> > > Thanks for your help,
> > >
> > > Alessandra
> > >
> > >
> > > _______________________________________________
> > > R-sig-mixed-models at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > >
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com  Mon Feb 17 19:48:26 2020
From: b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com (Alessandra Bielli)
Date: Mon, 17 Feb 2020 12:48:26 -0600
Subject: [R-sig-ME] Fwd: model check for negative binomial model
In-Reply-To: <CAJuCY5w-aYigW9iX+0ZTruzqmgNAQtr7+A2j5fN-x5j38iSSLg@mail.gmail.com>
References: <mailman.18062.3142.1581469252.1419.r-sig-mixed-models@r-project.org>
 <CA+6N3yV8ytczPND5x1VS5Vfe+H748vqjmc4=79wTnbJb1JmSbA@mail.gmail.com>
 <3cd5cea8-28c9-0799-9f17-d80a4aaa1ccc@gmail.com>
 <CA+6N3yXx23=4tajGD8DrZj=SOE+HbSX6K9sOQojt7YVVfgCJsw@mail.gmail.com>
 <CAJuCY5w-aYigW9iX+0ZTruzqmgNAQtr7+A2j5fN-x5j38iSSLg@mail.gmail.com>
Message-ID: <CA+6N3yW0MHg4800dZiPyUW+YHAxYc5Kse9eru=QRJKkHKav4SQ@mail.gmail.com>

Dear Thierry

Thanks for your reply.

I read a bit about the prediction for a binomial model with
success/failures and I have a couple of questions.

If I use the predict function with the model you recommended, I obtain
log.odds or probabilities if I use "type=response":

tapply(predict(m.unhatched,type="response"),list(main$SP,main$Relocation..Y.N.),mean)
          N         Y
G 0.7314196 0.6414554
L 0.6983576 0.6003087

Are these probabilities of success (i.e. hatched) in one nest?

Thanks,

Alessandra

On Mon, Feb 17, 2020 at 7:18 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> Dear Alessandra,
>
> Since you have both the number hatched and the total clutch size you can
> calculate the number of successes and failures. That is sufficient for a
> binomial distribution.
>
> glmer(cbind(Hatched, Unhatched) ~ Relocation..Y.N. + SP + (1 | Beach_ID)
> + (1 | Week), family = binmial)
>
> A negative binomial or Poisson allow predictions larger than the offset.
> Which is nonsense given that the number hatched cannot surpass the total
> clutch size.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op wo 12 feb. 2020 om 18:42 schreef Alessandra Bielli <
> bielli.alessandra at gmail.com>:
>
>> Dear Ben
>>
>> Thanks for your quick response.
>>
>> Yes, emergence success is usually between 60 and 80% or higher.
>> I am not sure how to use a binomial, if my data are counts?
>>
>> Can you explain why the approximation doesn't work well if success gets
>> much above 50%? Does it make sense, then, to have "unhatched" as dependent
>> variable, so that I predict mortality (usually below 50%) using a nb with
>> offset(log(total clutch)) ?
>>
>> > summary(m.emerged)
>> Generalized linear mixed model fit by maximum likelihood (Laplace
>> Approximation) ['glmerMod']
>>  Family: Negative Binomial(2.2104)  ( log )
>> Formula: Unhatched ~ Relocation..Y.N. + SP + offset(log(Total_Clutch)) +
>>    (1 | Beach_ID) + (1 | Week)
>>    Data: main
>>
>>      AIC      BIC   logLik deviance df.resid
>>   5439.4   5466.0  -2713.7   5427.4      614
>>
>> Scaled residuals:
>>     Min      1Q  Median      3Q     Max
>> -1.4383 -0.7242 -0.2287  0.4866  4.0531
>>
>> Random effects:
>>  Groups   Name        Variance Std.Dev.
>>  Week     (Intercept) 0.003092 0.0556
>>  Beach_ID (Intercept) 0.025894 0.1609
>> Number of obs: 620, groups:  Week, 31; Beach_ID, 8
>>
>> Fixed effects:
>>                   Estimate Std. Error z value Pr(>|z|)
>> (Intercept)       -1.38864    0.08227 -16.879  < 2e-16 ***
>> Relocation..Y.N.Y  0.32105    0.09152   3.508 0.000452 ***
>> SPL                0.22218    0.08793   2.527 0.011508 *
>> ---
>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>
>> Correlation of Fixed Effects:
>>             (Intr) R..Y.N
>> Rlct..Y.N.Y -0.143
>> SPL         -0.540 -0.038
>>
>> Thanks,
>>
>> Alessandra
>>
>> On Tue, Feb 11, 2020 at 7:29 PM Ben Bolker <bbolker at gmail.com> wrote:
>>
>> >
>> >   Short answer: if emergence success gets much above 50%, then the
>> > approximation you're making (Poisson + offset for binomial, or NB +
>> > offset for negative binomial) doesn't work well.  You might try a
>> > beta-binomial (with glmmTMB) or a binomial + an observation-level random
>> > effect.
>> >
>> >   (On the other hand, your intercept is -0.3, which corresponds to a
>> > baseline emergence of 0.42 - not *very* high (but some beaches and years
>> > will be well above that ...)
>> >
>> >   Beyond that, are there any obvious patterns of mis-fit in the
>> > predicted values ... ?
>> >
>> > On 2020-02-11 8:09 p.m., Alessandra Bielli wrote:
>> > > Dear list
>> > >
>> > > I am fitting a poisson model to estimate the effect of a treatment on
>> > > emergence success of hatchlings. To estimate emergence success, I use
>> > > number of emerged and an offset(log(total clutch).
>> > >
>> > > However, overdispersion was detected:
>> > >
>> > >> overdisp_fun(m.emerged) #overdispersion detected
>> > >
>> > >       chisq       ratio         rdf           p
>> > > 3490.300836    5.684529  614.000000    0.000000
>> > >
>> > > Therefore, I switched to a negative binomial. I know overdispersion is
>> > not
>> > > relevant for nb models, but the model plots don't look too good. I
>> also
>> > > tried to fit a poisson model with OLRE, but still the  plots don't
>> look
>> > > good.
>> > > How do I know if my model is good enough, and what can I do to improve
>> > it?
>> > >
>> > >> summary(m.emerged)
>> > > Generalized linear mixed model fit by maximum likelihood (Laplace
>> > > Approximation) ['glmerMod']
>> > >  Family: Negative Binomial(7.604)  ( log )
>> > > Formula: Hatched ~ Relocation..Y.N. + SP + offset(log(Total_Clutch))
>> + (1
>> > > |Beach_ID) + (1 | Year)
>> > >    Data: main
>> > >
>> > >      AIC      BIC   logLik deviance df.resid
>> > >   6015.6   6042.2  -3001.8   6003.6      614
>> > >
>> > > Scaled residuals:
>> > >     Min      1Q  Median      3Q     Max
>> > > -2.6427 -0.3790  0.1790  0.5242  1.6583
>> > >
>> > > Random effects:
>> > >  Groups   Name        Variance Std.Dev.
>> > >  Beach_ID (Intercept) 0.004438 0.06662
>> > >  Year     (Intercept) 0.001640 0.04050
>> > > Number of obs: 620, groups:  Beach_ID, 8; Year, 5
>> > >
>> > > Fixed effects:
>> > >                   Estimate Std. Error z value Pr(>|z|)
>> > > (Intercept)       -0.29915    0.04055  -7.377 1.62e-13 ***
>> > > Relocation..Y.N.Y -0.16402    0.05052  -3.247  0.00117 **
>> > > SPL               -0.08311    0.04365  -1.904  0.05689 .
>> > > ---
>> > > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>> > >
>> > > Correlation of Fixed Effects:
>> > >             (Intr) R..Y.N
>> > > Rlct..Y.N.Y -0.114
>> > > SPL         -0.497 -0.054
>> > >
>> > >
>> > > Thanks for your help,
>> > >
>> > > Alessandra
>> > >
>> > >
>> > > _______________________________________________
>> > > R-sig-mixed-models at r-project.org mailing list
>> > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> > >
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Mon Feb 17 21:15:52 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 17 Feb 2020 15:15:52 -0500
Subject: [R-sig-ME] Fwd: model check for negative binomial model
In-Reply-To: <CA+6N3yW0MHg4800dZiPyUW+YHAxYc5Kse9eru=QRJKkHKav4SQ@mail.gmail.com>
References: <mailman.18062.3142.1581469252.1419.r-sig-mixed-models@r-project.org>
 <CA+6N3yV8ytczPND5x1VS5Vfe+H748vqjmc4=79wTnbJb1JmSbA@mail.gmail.com>
 <3cd5cea8-28c9-0799-9f17-d80a4aaa1ccc@gmail.com>
 <CA+6N3yXx23=4tajGD8DrZj=SOE+HbSX6K9sOQojt7YVVfgCJsw@mail.gmail.com>
 <CAJuCY5w-aYigW9iX+0ZTruzqmgNAQtr7+A2j5fN-x5j38iSSLg@mail.gmail.com>
 <CA+6N3yW0MHg4800dZiPyUW+YHAxYc5Kse9eru=QRJKkHKav4SQ@mail.gmail.com>
Message-ID: <6683a330-5d20-af99-a73d-813ae1ca15a0@gmail.com>


    That's correct.  There are some delicate issues about prediction:

* do you want to use the original (potentially unbalanced) data for
prediction? (That's what you're doing here).
* or, do you want to make predictions for a "typical" nest and week
combination, in which case you would use


  pframe <- with(your_data,
        expand.grid(Relocation..Y.N.=unique(Relocation..Y.N.),
            SP=unique(SP))
  predict(m.unhatched,type="response",re.form=NA,newdata=pframe))

  You could also use expand.grid() to generate a balanced design (i.e.
all combinations of weeks and nests), which would give yet another answer.

  There are a lot of packages designed for doing these kinds of
post-fitting manipulations (e.g. 'margins', 'emmeans'), you might find
them useful ...



On 2020-02-17 1:48 p.m., Alessandra Bielli wrote:
> Dear Thierry
> 
> Thanks for your reply.
> 
> I read a bit about the prediction for a binomial model with
> success/failures and I have a couple of questions.?
> 
> If I use the predict function with the model you recommended, I obtain
> log.odds or probabilities if I use "type=response":
> 
> tapply(predict(m.unhatched,type="response"),list(main$SP,main$Relocation..Y.N.),mean)
> ? ? ? ? ? N ? ? ? ? Y
> G 0.7314196 0.6414554
> L 0.6983576 0.6003087
> 
> Are these probabilities of success (i.e. hatched) in one nest??
> 
> Thanks,
> 
> Alessandra
> 
> On Mon, Feb 17, 2020 at 7:18 AM Thierry Onkelinx
> <thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>> wrote:
> 
>     Dear Alessandra,
> 
>     Since you have both the number hatched and the total clutch size you
>     can calculate the number of successes and failures. That is
>     sufficient for a binomial distribution.
> 
>     glmer(cbind(Hatched, Unhatched) ~ Relocation..Y.N. + SP + (1 |
>     Beach_ID) + (1 | Week), family = binmial)
> 
>     A negative binomial or Poisson allow predictions larger than the
>     offset. Which is nonsense given that the number hatched cannot
>     surpass the total clutch size.
> 
>     Best regards,
> 
>     ir. Thierry Onkelinx
>     Statisticus / Statistician
> 
>     Vlaamse Overheid / Government of Flanders
>     INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR
>     NATURE AND FOREST
>     Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>     thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>
>     Havenlaan 88 bus 73, 1000 Brussel
>     www.inbo.be <http://www.inbo.be>
> 
>     ///////////////////////////////////////////////////////////////////////////////////////////
>     To call in the statistician after the experiment is done may be no
>     more than asking him to perform a post-mortem examination: he may be
>     able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
>     The plural of anecdote is not data. ~ Roger Brinner
>     The combination of some data and an aching desire for an answer does
>     not ensure that a reasonable answer can be extracted from a given
>     body of data. ~ John Tukey
>     ///////////////////////////////////////////////////////////////////////////////////////////
> 
>     <https://www.inbo.be>
> 
> 
>     Op wo 12 feb. 2020 om 18:42 schreef Alessandra Bielli
>     <bielli.alessandra at gmail.com <mailto:bielli.alessandra at gmail.com>>:
> 
>         Dear Ben
> 
>         Thanks for your quick response.
> 
>         Yes, emergence success is usually between 60 and 80% or higher.
>         I am not sure how to use a binomial, if my data are counts?
> 
>         Can you explain why the approximation doesn't work well if
>         success gets
>         much above 50%? Does it make sense, then, to have "unhatched" as
>         dependent
>         variable, so that I predict mortality (usually below 50%) using
>         a nb with
>         offset(log(total clutch)) ?
> 
>         > summary(m.emerged)
>         Generalized linear mixed model fit by maximum likelihood (Laplace
>         Approximation) ['glmerMod']
>         ?Family: Negative Binomial(2.2104)? ( log )
>         Formula: Unhatched ~ Relocation..Y.N. + SP +
>         offset(log(Total_Clutch)) +
>         ? ?(1 | Beach_ID) + (1 | Week)
>         ? ?Data: main
> 
>         ? ? ?AIC? ? ? BIC? ?logLik deviance df.resid
>         ? 5439.4? ?5466.0? -2713.7? ?5427.4? ? ? 614
> 
>         Scaled residuals:
>         ? ? Min? ? ? 1Q? Median? ? ? 3Q? ? ?Max
>         -1.4383 -0.7242 -0.2287? 0.4866? 4.0531
> 
>         Random effects:
>         ?Groups? ?Name? ? ? ? Variance Std.Dev.
>         ?Week? ? ?(Intercept) 0.003092 0.0556
>         ?Beach_ID (Intercept) 0.025894 0.1609
>         Number of obs: 620, groups:? Week, 31; Beach_ID, 8
> 
>         Fixed effects:
>         ? ? ? ? ? ? ? ? ? Estimate Std. Error z value Pr(>|z|)
>         (Intercept)? ? ? ?-1.38864? ? 0.08227 -16.879? < 2e-16 ***
>         Relocation..Y.N.Y? 0.32105? ? 0.09152? ?3.508 0.000452 ***
>         SPL? ? ? ? ? ? ? ? 0.22218? ? 0.08793? ?2.527 0.011508 *
>         ---
>         Signif. codes:? 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
>         Correlation of Fixed Effects:
>         ? ? ? ? ? ? (Intr) R..Y.N
>         Rlct..Y.N.Y -0.143
>         SPL? ? ? ? ?-0.540 -0.038
> 
>         Thanks,
> 
>         Alessandra
> 
>         On Tue, Feb 11, 2020 at 7:29 PM Ben Bolker <bbolker at gmail.com
>         <mailto:bbolker at gmail.com>> wrote:
> 
>         >
>         >? ?Short answer: if emergence success gets much above 50%, then the
>         > approximation you're making (Poisson + offset for binomial, or
>         NB +
>         > offset for negative binomial) doesn't work well.? You might try a
>         > beta-binomial (with glmmTMB) or a binomial + an
>         observation-level random
>         > effect.
>         >
>         >? ?(On the other hand, your intercept is -0.3, which
>         corresponds to a
>         > baseline emergence of 0.42 - not *very* high (but some beaches
>         and years
>         > will be well above that ...)
>         >
>         >? ?Beyond that, are there any obvious patterns of mis-fit in the
>         > predicted values ... ?
>         >
>         > On 2020-02-11 8:09 p.m., Alessandra Bielli wrote:
>         > > Dear list
>         > >
>         > > I am fitting a poisson model to estimate the effect of a
>         treatment on
>         > > emergence success of hatchlings. To estimate emergence
>         success, I use
>         > > number of emerged and an offset(log(total clutch).
>         > >
>         > > However, overdispersion was detected:
>         > >
>         > >> overdisp_fun(m.emerged) #overdispersion detected
>         > >
>         > >? ? ? ?chisq? ? ? ?ratio? ? ? ? ?rdf? ? ? ? ? ?p
>         > > 3490.300836? ? 5.684529? 614.000000? ? 0.000000
>         > >
>         > > Therefore, I switched to a negative binomial. I know
>         overdispersion is
>         > not
>         > > relevant for nb models, but the model plots don't look too
>         good. I also
>         > > tried to fit a poisson model with OLRE, but still the? plots
>         don't look
>         > > good.
>         > > How do I know if my model is good enough, and what can I do
>         to improve
>         > it?
>         > >
>         > >> summary(m.emerged)
>         > > Generalized linear mixed model fit by maximum likelihood
>         (Laplace
>         > > Approximation) ['glmerMod']
>         > >? Family: Negative Binomial(7.604)? ( log )
>         > > Formula: Hatched ~ Relocation..Y.N. + SP +
>         offset(log(Total_Clutch)) + (1
>         > > |Beach_ID) + (1 | Year)
>         > >? ? Data: main
>         > >
>         > >? ? ? AIC? ? ? BIC? ?logLik deviance df.resid
>         > >? ?6015.6? ?6042.2? -3001.8? ?6003.6? ? ? 614
>         > >
>         > > Scaled residuals:
>         > >? ? ?Min? ? ? 1Q? Median? ? ? 3Q? ? ?Max
>         > > -2.6427 -0.3790? 0.1790? 0.5242? 1.6583
>         > >
>         > > Random effects:
>         > >? Groups? ?Name? ? ? ? Variance Std.Dev.
>         > >? Beach_ID (Intercept) 0.004438 0.06662
>         > >? Year? ? ?(Intercept) 0.001640 0.04050
>         > > Number of obs: 620, groups:? Beach_ID, 8; Year, 5
>         > >
>         > > Fixed effects:
>         > >? ? ? ? ? ? ? ? ? ?Estimate Std. Error z value Pr(>|z|)
>         > > (Intercept)? ? ? ?-0.29915? ? 0.04055? -7.377 1.62e-13 ***
>         > > Relocation..Y.N.Y -0.16402? ? 0.05052? -3.247? 0.00117 **
>         > > SPL? ? ? ? ? ? ? ?-0.08311? ? 0.04365? -1.904? 0.05689 .
>         > > ---
>         > > Signif. codes:? 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>         > >
>         > > Correlation of Fixed Effects:
>         > >? ? ? ? ? ? ?(Intr) R..Y.N
>         > > Rlct..Y.N.Y -0.114
>         > > SPL? ? ? ? ?-0.497 -0.054
>         > >
>         > >
>         > > Thanks for your help,
>         > >
>         > > Alessandra
>         > >
>         > >
>         > > _______________________________________________
>         > > R-sig-mixed-models at r-project.org
>         <mailto:R-sig-mixed-models at r-project.org> mailing list
>         > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>         > >
>         >
>         > _______________________________________________
>         > R-sig-mixed-models at r-project.org
>         <mailto:R-sig-mixed-models at r-project.org> mailing list
>         > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>         >
>         _______________________________________________
>         R-sig-mixed-models at r-project.org
>         <mailto:R-sig-mixed-models at r-project.org> mailing list
>         https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com  Tue Feb 18 01:01:48 2020
From: b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com (Alessandra Bielli)
Date: Mon, 17 Feb 2020 18:01:48 -0600
Subject: [R-sig-ME] Fwd: model check for negative binomial model
In-Reply-To: <6683a330-5d20-af99-a73d-813ae1ca15a0@gmail.com>
References: <mailman.18062.3142.1581469252.1419.r-sig-mixed-models@r-project.org>
 <CA+6N3yV8ytczPND5x1VS5Vfe+H748vqjmc4=79wTnbJb1JmSbA@mail.gmail.com>
 <3cd5cea8-28c9-0799-9f17-d80a4aaa1ccc@gmail.com>
 <CA+6N3yXx23=4tajGD8DrZj=SOE+HbSX6K9sOQojt7YVVfgCJsw@mail.gmail.com>
 <CAJuCY5w-aYigW9iX+0ZTruzqmgNAQtr7+A2j5fN-x5j38iSSLg@mail.gmail.com>
 <CA+6N3yW0MHg4800dZiPyUW+YHAxYc5Kse9eru=QRJKkHKav4SQ@mail.gmail.com>
 <6683a330-5d20-af99-a73d-813ae1ca15a0@gmail.com>
Message-ID: <CA+6N3yWDKD4NavO16QEiVannM1uMwVbm4e_5-ii-UxBB=KYqGQ@mail.gmail.com>

Dear Ben

I am trying to make a prediction for the combination of species (L or G)
and treatment (control/experiment).

I am still confused about the prediction values. I would like to present
results as a success rate for a nest, to say that treatment
increases/decreases success by ...%. But the value I have is the
probability that 1 egg in the nest succeeds, correct? I am not sure how to
use these predictions.

Thanks for your help!

Alessandra

On Mon, Feb 17, 2020 at 2:15 PM Ben Bolker <bbolker at gmail.com> wrote:

>
>     That's correct.  There are some delicate issues about prediction:
>
> * do you want to use the original (potentially unbalanced) data for
> prediction? (That's what you're doing here).
> * or, do you want to make predictions for a "typical" nest and week
> combination, in which case you would use
>
>
>   pframe <- with(your_data,
>         expand.grid(Relocation..Y.N.=unique(Relocation..Y.N.),
>             SP=unique(SP))
>   predict(m.unhatched,type="response",re.form=NA,newdata=pframe))
>
>   You could also use expand.grid() to generate a balanced design (i.e.
> all combinations of weeks and nests), which would give yet another answer.
>
>   There are a lot of packages designed for doing these kinds of
> post-fitting manipulations (e.g. 'margins', 'emmeans'), you might find
> them useful ...
>
>
>
> On 2020-02-17 1:48 p.m., Alessandra Bielli wrote:
> > Dear Thierry
> >
> > Thanks for your reply.
> >
> > I read a bit about the prediction for a binomial model with
> > success/failures and I have a couple of questions.
> >
> > If I use the predict function with the model you recommended, I obtain
> > log.odds or probabilities if I use "type=response":
> >
> >
> tapply(predict(m.unhatched,type="response"),list(main$SP,main$Relocation..Y.N.),mean)
> >           N         Y
> > G 0.7314196 0.6414554
> > L 0.6983576 0.6003087
> >
> > Are these probabilities of success (i.e. hatched) in one nest?
> >
> > Thanks,
> >
> > Alessandra
> >
> > On Mon, Feb 17, 2020 at 7:18 AM Thierry Onkelinx
> > <thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>> wrote:
> >
> >     Dear Alessandra,
> >
> >     Since you have both the number hatched and the total clutch size you
> >     can calculate the number of successes and failures. That is
> >     sufficient for a binomial distribution.
> >
> >     glmer(cbind(Hatched, Unhatched) ~ Relocation..Y.N. + SP + (1 |
> >     Beach_ID) + (1 | Week), family = binmial)
> >
> >     A negative binomial or Poisson allow predictions larger than the
> >     offset. Which is nonsense given that the number hatched cannot
> >     surpass the total clutch size.
> >
> >     Best regards,
> >
> >     ir. Thierry Onkelinx
> >     Statisticus / Statistician
> >
> >     Vlaamse Overheid / Government of Flanders
> >     INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR
> >     NATURE AND FOREST
> >     Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> >     thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>
> >     Havenlaan 88 bus 73, 1000 Brussel
> >     www.inbo.be <http://www.inbo.be>
> >
> >
>  ///////////////////////////////////////////////////////////////////////////////////////////
> >     To call in the statistician after the experiment is done may be no
> >     more than asking him to perform a post-mortem examination: he may be
> >     able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> >     The plural of anecdote is not data. ~ Roger Brinner
> >     The combination of some data and an aching desire for an answer does
> >     not ensure that a reasonable answer can be extracted from a given
> >     body of data. ~ John Tukey
> >
>  ///////////////////////////////////////////////////////////////////////////////////////////
> >
> >     <https://www.inbo.be>
> >
> >
> >     Op wo 12 feb. 2020 om 18:42 schreef Alessandra Bielli
> >     <bielli.alessandra at gmail.com <mailto:bielli.alessandra at gmail.com>>:
> >
> >         Dear Ben
> >
> >         Thanks for your quick response.
> >
> >         Yes, emergence success is usually between 60 and 80% or higher.
> >         I am not sure how to use a binomial, if my data are counts?
> >
> >         Can you explain why the approximation doesn't work well if
> >         success gets
> >         much above 50%? Does it make sense, then, to have "unhatched" as
> >         dependent
> >         variable, so that I predict mortality (usually below 50%) using
> >         a nb with
> >         offset(log(total clutch)) ?
> >
> >         > summary(m.emerged)
> >         Generalized linear mixed model fit by maximum likelihood (Laplace
> >         Approximation) ['glmerMod']
> >          Family: Negative Binomial(2.2104)  ( log )
> >         Formula: Unhatched ~ Relocation..Y.N. + SP +
> >         offset(log(Total_Clutch)) +
> >            (1 | Beach_ID) + (1 | Week)
> >            Data: main
> >
> >              AIC      BIC   logLik deviance df.resid
> >           5439.4   5466.0  -2713.7   5427.4      614
> >
> >         Scaled residuals:
> >             Min      1Q  Median      3Q     Max
> >         -1.4383 -0.7242 -0.2287  0.4866  4.0531
> >
> >         Random effects:
> >          Groups   Name        Variance Std.Dev.
> >          Week     (Intercept) 0.003092 0.0556
> >          Beach_ID (Intercept) 0.025894 0.1609
> >         Number of obs: 620, groups:  Week, 31; Beach_ID, 8
> >
> >         Fixed effects:
> >                           Estimate Std. Error z value Pr(>|z|)
> >         (Intercept)       -1.38864    0.08227 -16.879  < 2e-16 ***
> >         Relocation..Y.N.Y  0.32105    0.09152   3.508 0.000452 ***
> >         SPL                0.22218    0.08793   2.527 0.011508 *
> >         ---
> >         Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >
> >         Correlation of Fixed Effects:
> >                     (Intr) R..Y.N
> >         Rlct..Y.N.Y -0.143
> >         SPL         -0.540 -0.038
> >
> >         Thanks,
> >
> >         Alessandra
> >
> >         On Tue, Feb 11, 2020 at 7:29 PM Ben Bolker <bbolker at gmail.com
> >         <mailto:bbolker at gmail.com>> wrote:
> >
> >         >
> >         >   Short answer: if emergence success gets much above 50%, then
> the
> >         > approximation you're making (Poisson + offset for binomial, or
> >         NB +
> >         > offset for negative binomial) doesn't work well.  You might
> try a
> >         > beta-binomial (with glmmTMB) or a binomial + an
> >         observation-level random
> >         > effect.
> >         >
> >         >   (On the other hand, your intercept is -0.3, which
> >         corresponds to a
> >         > baseline emergence of 0.42 - not *very* high (but some beaches
> >         and years
> >         > will be well above that ...)
> >         >
> >         >   Beyond that, are there any obvious patterns of mis-fit in the
> >         > predicted values ... ?
> >         >
> >         > On 2020-02-11 8:09 p.m., Alessandra Bielli wrote:
> >         > > Dear list
> >         > >
> >         > > I am fitting a poisson model to estimate the effect of a
> >         treatment on
> >         > > emergence success of hatchlings. To estimate emergence
> >         success, I use
> >         > > number of emerged and an offset(log(total clutch).
> >         > >
> >         > > However, overdispersion was detected:
> >         > >
> >         > >> overdisp_fun(m.emerged) #overdispersion detected
> >         > >
> >         > >       chisq       ratio         rdf           p
> >         > > 3490.300836    5.684529  614.000000    0.000000
> >         > >
> >         > > Therefore, I switched to a negative binomial. I know
> >         overdispersion is
> >         > not
> >         > > relevant for nb models, but the model plots don't look too
> >         good. I also
> >         > > tried to fit a poisson model with OLRE, but still the  plots
> >         don't look
> >         > > good.
> >         > > How do I know if my model is good enough, and what can I do
> >         to improve
> >         > it?
> >         > >
> >         > >> summary(m.emerged)
> >         > > Generalized linear mixed model fit by maximum likelihood
> >         (Laplace
> >         > > Approximation) ['glmerMod']
> >         > >  Family: Negative Binomial(7.604)  ( log )
> >         > > Formula: Hatched ~ Relocation..Y.N. + SP +
> >         offset(log(Total_Clutch)) + (1
> >         > > |Beach_ID) + (1 | Year)
> >         > >    Data: main
> >         > >
> >         > >      AIC      BIC   logLik deviance df.resid
> >         > >   6015.6   6042.2  -3001.8   6003.6      614
> >         > >
> >         > > Scaled residuals:
> >         > >     Min      1Q  Median      3Q     Max
> >         > > -2.6427 -0.3790  0.1790  0.5242  1.6583
> >         > >
> >         > > Random effects:
> >         > >  Groups   Name        Variance Std.Dev.
> >         > >  Beach_ID (Intercept) 0.004438 0.06662
> >         > >  Year     (Intercept) 0.001640 0.04050
> >         > > Number of obs: 620, groups:  Beach_ID, 8; Year, 5
> >         > >
> >         > > Fixed effects:
> >         > >                   Estimate Std. Error z value Pr(>|z|)
> >         > > (Intercept)       -0.29915    0.04055  -7.377 1.62e-13 ***
> >         > > Relocation..Y.N.Y -0.16402    0.05052  -3.247  0.00117 **
> >         > > SPL               -0.08311    0.04365  -1.904  0.05689 .
> >         > > ---
> >         > > Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ?
> 1
> >         > >
> >         > > Correlation of Fixed Effects:
> >         > >             (Intr) R..Y.N
> >         > > Rlct..Y.N.Y -0.114
> >         > > SPL         -0.497 -0.054
> >         > >
> >         > >
> >         > > Thanks for your help,
> >         > >
> >         > > Alessandra
> >         > >
> >         > >
> >         > > _______________________________________________
> >         > > R-sig-mixed-models at r-project.org
> >         <mailto:R-sig-mixed-models at r-project.org> mailing list
> >         > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >         > >
> >         >
> >         > _______________________________________________
> >         > R-sig-mixed-models at r-project.org
> >         <mailto:R-sig-mixed-models at r-project.org> mailing list
> >         > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >         >
> >         _______________________________________________
> >         R-sig-mixed-models at r-project.org
> >         <mailto:R-sig-mixed-models at r-project.org> mailing list
> >         https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>

	[[alternative HTML version deleted]]


From @@|v@dor@@nchezco|on @end|ng |rom prod|gy@net@mx  Tue Feb 18 03:46:17 2020
From: @@|v@dor@@nchezco|on @end|ng |rom prod|gy@net@mx (Salvador SANCHEZ COLON)
Date: Mon, 17 Feb 2020 18:46:17 -0800
Subject: [R-sig-ME] Fwd: model check for negative binomial model
Message-ID: <J7XD8RMWV9U4.F8E4GUOTL1ZF1@mwweb14oc>

Cara Alessandra,


If I may interject into your conversation, the key to your question lies in the parameter estimates:for the fixed effects:


? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?Estimate? ? ? ? ? ?Std. Error? ? ? ? ? ? ?z value? ? ? ? Pr(>|z|)
(Intercept)? ? ? ? ? ? ? ?-1.38864? ? ? ? ? ? ?0.08227? ? ? ? ? ? ? -16.879? ? ? < 2e-16 ***
Relocation..Y.N.Y? ? ? ?0.32105? ? ? ? ? ? ? 0.09152? ? ? ? ? ? ? ? 3.508? ? ? ?0.000452 ***
SPL? ? ? ? ? ? ? ? ? ? ? ? ? 0.22218? ? ? ? ? ? ? ?0.08793? ? ? ? ? ? ? ? 2.527? ? ? ?0.011508 *


As I understand your design, you have two independent variables (factors): Species and treatment, each with two levels: L vs. G and Y vs N.?


Thus, the logit model that you obtain (omitting the random effects) is:


logit(p) = log(p/(1-p) = -1.38864 + 0.32105*Treatment + 0.22218*Species


Treatment and species are two-level factors which are coded as 0 or 1 and the model has to have parameter estimates for each of those levels.?By design, in generalized linear models parameters are estimated by taking one of the levels of each factor as the reference level and assigned a value of 0, and the parameters for the other levels are expressed in relation to the reference level. Thus, in your case, treatment N and species G are designated as the reference levels for their corresponding factors and, hence, their parameter values are both 0 (and not listed in the output). Then, treatment Y has a positive effect (0.32) on the odds ratio of hatching/not-hatching compared to treatment N; and species L also has a positive effect (0.22 times) on the odds ratio of hatching vs not-hatching.?


The intercept then (-1.38864) denotes the odds ratio for the combination of treatment N and species G; that is, when both factors are at their reference level of 0. This translates into a probability of hatching (p):


p =? exp(-1.38864)/(1+exp(-1.38864) = 0.1996


For treatment Y and species G, the model becomes:


p =? exp(-1.38864 +?0.32105)/(1+exp(-1.38864 +?0.32105) = 0.25569


as treatment Y has a positive effect on the odds ratio of hatching.?


For treatment N and species L, the model becomes:


p =? exp(-1.38864 +?0.22218)/(1+exp(-1.38864 +?0.22218) = 0.2375


as species L has a positive effect on the odds ratio of hatching.


Finally, for the combination of treament Y and species L, the model becomes:


p =? exp(-1.38864 + 0.32105 +?0.22218)/(1+exp(-1.38864?+ 0.32105 +?0.22218) = 0.3002


as both levels Y and L have positive effects on the odds ratio of hatching.


I hope this helps.


Best regards,


Salvador








En Lun, 17 Febrero, 2020 en 18:2, Alessandra Bielli <bielli.alessandra at gmail.com> escribi?:
?

Para: Ben Bolker
Cc: r-sig-mixed-models at r-project.orgDear Ben

I am trying to make a prediction for the combination of species (L or G)
and treatment (control/experiment).

I am still confused about the prediction values. I would like to present
results as a success rate for a nest, to say that treatment
increases/decreases success by ...%. But the value I have is the
probability that 1 egg in the nest succeeds, correct? I am not sure how to
use these predictions.

Thanks for your help!

Alessandra

On Mon, Feb 17, 2020 at 2:15 PM Ben Bolker <bbolker at gmail.com> wrote:

>
> That's correct. There are some delicate issues about prediction:
>
> * do you want to use the original (potentially unbalanced) data for
> prediction? (That's what you're doing here).
> * or, do you want to make predictions for a "typical" nest and week
> combination, in which case you would use
>
>
> pframe <- with(your_data,
> expand.grid(Relocation..Y.N.=unique(Relocation..Y.N.),
> SP=unique(SP))
> predict(m.unhatched,type="response",re.form=NA,newdata=pframe))
>
> You could also use expand.grid() to generate a balanced design (i.e.
> all combinations of weeks and nests), which would give yet another answer.
>
> There are a lot of packages designed for doing these kinds of
> post-fitting manipulations (e.g. 'margins', 'emmeans'), you might find
> them useful ...
>
>
>
> On 2020-02-17 1:48 p.m., Alessandra Bielli wrote:
> > Dear Thierry
> >
> > Thanks for your reply.
> >
> > I read a bit about the prediction for a binomial model with
> > success/failures and I have a couple of questions.
> >
> > If I use the predict function with the model you recommended, I obtain
> > log.odds or probabilities if I use "type=response":
> >
> >
> tapply(predict(m.unhatched,type="response"),list(main$SP,main$Relocation..Y.N.),mean)
> > N Y
> > G 0.7314196 0.6414554
> > L 0.6983576 0.6003087
> >
> > Are these probabilities of success (i.e. hatched) in one nest?
> >
> > Thanks,
> >
> > Alessandra
> >
> > On Mon, Feb 17, 2020 at 7:18 AM Thierry Onkelinx
> > <thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>> wrote:
> >
> > Dear Alessandra,
> >
> > Since you have both the number hatched and the total clutch size you
> > can calculate the number of successes and failures. That is
> > sufficient for a binomial distribution.
> >
> > glmer(cbind(Hatched, Unhatched) ~ Relocation..Y.N. + SP + (1 |
> > Beach_ID) + (1 | Week), family = binmial)
> >
> > A negative binomial or Poisson allow predictions larger than the
> > offset. Which is nonsense given that the number hatched cannot
> > surpass the total clutch size.
> >
> > Best regards,
> >
> > ir. Thierry Onkelinx
> > Statisticus / Statistician
> >
> > Vlaamse Overheid / Government of Flanders
> > INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR
> > NATURE AND FOREST
> > Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> > thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>
> > Havenlaan 88 bus 73, 1000 Brussel
> > www.inbo.be <http://www.inbo.be>
> >
> >
> ///////////////////////////////////////////////////////////////////////////////////////////
> > To call in the statistician after the experiment is done may be no
> > more than asking him to perform a post-mortem examination: he may be
> > able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> > The plural of anecdote is not data. ~ Roger Brinner
> > The combination of some data and an aching desire for an answer does
> > not ensure that a reasonable answer can be extracted from a given
> > body of data. ~ John Tukey
> >
> ///////////////////////////////////////////////////////////////////////////////////////////
> >
> > <https://www.inbo.be>
> >
> >
> > Op wo 12 feb. 2020 om 18:42 schreef Alessandra Bielli
> > <bielli.alessandra at gmail.com <mailto:bielli.alessandra at gmail.com>>:
> >
> > Dear Ben
> >
> > Thanks for your quick response.
> >
> > Yes, emergence success is usually between 60 and 80% or higher.
> > I am not sure how to use a binomial, if my data are counts?
> >
> > Can you explain why the approximation doesn't work well if
> > success gets
> > much above 50%? Does it make sense, then, to have "unhatched" as
> > dependent
> > variable, so that I predict mortality (usually below 50%) using
> > a nb with
> > offset(log(total clutch)) ?
> >
> > > summary(m.emerged)
> > Generalized linear mixed model fit by maximum likelihood (Laplace
> > Approximation) ['glmerMod']
> > Family: Negative Binomial(2.2104) ( log )
> > Formula: Unhatched ~ Relocation..Y.N. + SP +
> > offset(log(Total_Clutch)) +
> > (1 | Beach_ID) + (1 | Week)
> > Data: main
> >
> > AIC BIC logLik deviance df.resid
> > 5439.4 5466.0 -2713.7 5427.4 614
> >
> > Scaled residuals:
> > Min 1Q Median 3Q Max
> > -1.4383 -0.7242 -0.2287 0.4866 4.0531
> >
> > Random effects:
> > Groups Name Variance Std.Dev.
> > Week (Intercept) 0.003092 0.0556
> > Beach_ID (Intercept) 0.025894 0.1609
> > Number of obs: 620, groups: Week, 31; Beach_ID, 8
> >
> > Fixed effects:
> > Estimate Std. Error z value Pr(>|z|)
> > (Intercept) -1.38864 0.08227 -16.879 < 2e-16 ***
> > Relocation..Y.N.Y 0.32105 0.09152 3.508 0.000452 ***
> > SPL 0.22218 0.08793 2.527 0.011508 *
> > ---
> > Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >
> > Correlation of Fixed Effects:
> > (Intr) R..Y.N
> > Rlct..Y.N.Y -0.143
> > SPL -0.540 -0.038
> >
> > Thanks,
> >
> > Alessandra
> >
> > On Tue, Feb 11, 2020 at 7:29 PM Ben Bolker <bbolker at gmail.com
> > <mailto:bbolker at gmail.com>> wrote:
> >
> > >
> > > Short answer: if emergence success gets much above 50%, then
> the
> > > approximation you're making (Poisson + offset for binomial, or
> > NB +
> > > offset for negative binomial) doesn't work well. You might
> try a
> > > beta-binomial (with glmmTMB) or a binomial + an
> > observation-level random
> > > effect.
> > >
> > > (On the other hand, your intercept is -0.3, which
> > corresponds to a
> > > baseline emergence of 0.42 - not *very* high (but some beaches
> > and years
> > > will be well above that ...)
> > >
> > > Beyond that, are there any obvious patterns of mis-fit in the
> > > predicted values ... ?
> > >
> > > On 2020-02-11 8:09 p.m., Alessandra Bielli wrote:
> > > > Dear list
> > > >
> > > > I am fitting a poisson model to estimate the effect of a
> > treatment on
> > > > emergence success of hatchlings. To estimate emergence
> > success, I use
> > > > number of emerged and an offset(log(total clutch).
> > > >
> > > > However, overdispersion was detected:
> > > >
> > > >> overdisp_fun(m.emerged) #overdispersion detected
> > > >
> > > > chisq ratio rdf p
> > > > 3490.300836 5.684529 614.000000 0.000000
> > > >
> > > > Therefore, I switched to a negative binomial. I know
> > overdispersion is
> > > not
> > > > relevant for nb models, but the model plots don't look too
> > good. I also
> > > > tried to fit a poisson model with OLRE, but still the plots
> > don't look
> > > > good.
> > > > How do I know if my model is good enough, and what can I do
> > to improve
> > > it?
> > > >
> > > >> summary(m.emerged)
> > > > Generalized linear mixed model fit by maximum likelihood
> > (Laplace
> > > > Approximation) ['glmerMod']
> > > > Family: Negative Binomial(7.604) ( log )
> > > > Formula: Hatched ~ Relocation..Y.N. + SP +
> > offset(log(Total_Clutch)) + (1
> > > > |Beach_ID) + (1 | Year)
> > > > Data: main
> > > >
> > > > AIC BIC logLik deviance df.resid
> > > > 6015.6 6042.2 -3001.8 6003.6 614
> > > >
> > > > Scaled residuals:
> > > > Min 1Q Median 3Q Max
> > > > -2.6427 -0.3790 0.1790 0.5242 1.6583
> > > >
> > > > Random effects:
> > > > Groups Name Variance Std.Dev.
> > > > Beach_ID (Intercept) 0.004438 0.06662
> > > > Year (Intercept) 0.001640 0.04050
> > > > Number of obs: 620, groups: Beach_ID, 8; Year, 5
> > > >
> > > > Fixed effects:
> > > > Estimate Std. Error z value Pr(>|z|)
> > > > (Intercept) -0.29915 0.04055 -7.377 1.62e-13 ***
> > > > Relocation..Y.N.Y -0.16402 0.05052 -3.247 0.00117 **
> > > > SPL -0.08311 0.04365 -1.904 0.05689 .
> > > > ---
> > > > Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ?
> 1
> > > >
> > > > Correlation of Fixed Effects:
> > > > (Intr) R..Y.N
> > > > Rlct..Y.N.Y -0.114
> > > > SPL -0.497 -0.054
> > > >
> > > >
> > > > Thanks for your help,
> > > >
> > > > Alessandra
> > > >
> > > >
> > > > _______________________________________________
> > > > R-sig-mixed-models at r-project.org
> > <mailto:R-sig-mixed-models at r-project.org> mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > > >
> > >
> > > _______________________________________________
> > > R-sig-mixed-models at r-project.org
> > <mailto:R-sig-mixed-models at r-project.org> mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org
> > <mailto:R-sig-mixed-models at r-project.org> mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>

[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
.
 
	[[alternative HTML version deleted]]


From @@|v@dor@@nchezco|on @end|ng |rom prod|gy@net@mx  Tue Feb 18 04:27:38 2020
From: @@|v@dor@@nchezco|on @end|ng |rom prod|gy@net@mx (Salvador SANCHEZ COLON)
Date: Mon, 17 Feb 2020 19:27:38 -0800
Subject: [R-sig-ME] Fwd: model check for negative binomial model
Message-ID: <UNXRI5YWV9U4.7QC41ED29PMN3@mwweb11oc>

CORRECTION


Dear Alessandra,


My apologies, I msread the output you pasted in your previous messages. I did not realize that the parameter estimates that you were providing correspond to a negative binomial model, I assumed that they came from a binomial (logit) model. As Profr. Bolker mentioned, you cannot fit a negative binomial model to your data, it has to be a binomial (logit model) in which you model the number of eggs that hatch out of the total number of eggs in the nest. What the model predicts (after back transforming the odds ratio) is the probability, p, of an egg hatching, which can then be translated into the expected number of eggs that hatch simply by multiplying it by the number of eggs in the nest, np.


I hope this helps and, again, my apologies for the confusion and for messing things yp.


Salvador?


En Lun, 17 Febrero, 2020 en 20:46, yo <salvadorsanchezcolon at prodigy.net.mx> escribi?:
?

Para: bielli.alessandra at gmail.com
Cc: bbolker at gmail.com; r-sig-mixed-models at r-project.org

Cara Alessandra,


If I may interject into your conversation, the key to your question lies in the parameter estimates:for the fixed effects:


? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?Estimate? ? ? ? ? ?Std. Error? ? ? ? ? ? ?z value? ? ? ? Pr(>|z|)
(Intercept)? ? ? ? ? ? ? ?-1.38864? ? ? ? ? ? ?0.08227? ? ? ? ? ? ? -16.879? ? ? < 2e-16 ***
Relocation..Y.N.Y? ? ? ?0.32105? ? ? ? ? ? ? 0.09152? ? ? ? ? ? ? ? 3.508? ? ? ?0.000452 ***
SPL? ? ? ? ? ? ? ? ? ? ? ? ? 0.22218? ? ? ? ? ? ? ?0.08793? ? ? ? ? ? ? ? 2.527? ? ? ?0.011508 *


As I understand your design, you have two independent variables (factors): Species and treatment, each with two levels: L vs. G and Y vs N.?


Thus, the logit model that you obtain (omitting the random effects) is:


logit(p) = log(p/(1-p) = -1.38864 + 0.32105*Treatment + 0.22218*Species


Treatment and species are two-level factors which are coded as 0 or 1 and the model has to have parameter estimates for each of those levels.?By design, in generalized linear models parameters are estimated by taking one of the levels of each factor as the reference level and assigned a value of 0, and the parameters for the other levels are expressed in relation to the reference level. Thus, in your case, treatment N and species G are designated as the reference levels for their corresponding factors and, hence, their parameter values are both 0 (and not listed in the output). Then, treatment Y has a positive effect (0.32) on the odds ratio of hatching/not-hatching compared to treatment N; and species L also has a positive effect (0.22 times) on the odds ratio of hatching vs not-hatching.?


The intercept then (-1.38864) denotes the odds ratio for the combination of treatment N and species G; that is, when both factors are at their reference level of 0. This translates into a probability of hatching (p):


p =? exp(-1.38864)/(1+exp(-1.38864) = 0.1996


For treatment Y and species G, the model becomes:


p =? exp(-1.38864 +?0.32105)/(1+exp(-1.38864 +?0.32105) = 0.25569


as treatment Y has a positive effect on the odds ratio of hatching.?


For treatment N and species L, the model becomes:


p =? exp(-1.38864 +?0.22218)/(1+exp(-1.38864 +?0.22218) = 0.2375


as species L has a positive effect on the odds ratio of hatching.


Finally, for the combination of treament Y and species L, the model becomes:


p =? exp(-1.38864 + 0.32105 +?0.22218)/(1+exp(-1.38864?+ 0.32105 +?0.22218) = 0.3002


as both levels Y and L have positive effects on the odds ratio of hatching.


I hope this helps.


Best regards,


Salvador








En Lun, 17 Febrero, 2020 en 18:2, Alessandra Bielli <bielli.alessandra at gmail.com> escribi?:
?

Para: Ben Bolker
Cc: r-sig-mixed-models at r-project.orgDear Ben

I am trying to make a prediction for the combination of species (L or G)
and treatment (control/experiment).

I am still confused about the prediction values. I would like to present
results as a success rate for a nest, to say that treatment
increases/decreases success by ...%. But the value I have is the
probability that 1 egg in the nest succeeds, correct? I am not sure how to
use these predictions.

Thanks for your help!

Alessandra

On Mon, Feb 17, 2020 at 2:15 PM Ben Bolker <bbolker at gmail.com> wrote:

>
> That's correct. There are some delicate issues about prediction:
>
> * do you want to use the original (potentially unbalanced) data for
> prediction? (That's what you're doing here).
> * or, do you want to make predictions for a "typical" nest and week
> combination, in which case you would use
>
>
> pframe <- with(your_data,
> expand.grid(Relocation..Y.N.=unique(Relocation..Y.N.),
> SP=unique(SP))
> predict(m.unhatched,type="response",re.form=NA,newdata=pframe))
>
> You could also use expand.grid() to generate a balanced design (i.e.
> all combinations of weeks and nests), which would give yet another answer.
>
> There are a lot of packages designed for doing these kinds of
> post-fitting manipulations (e.g. 'margins', 'emmeans'), you might find
> them useful ...
>
>
>
> On 2020-02-17 1:48 p.m., Alessandra Bielli wrote:
> > Dear Thierry
> >
> > Thanks for your reply.
> >
> > I read a bit about the prediction for a binomial model with
> > success/failures and I have a couple of questions.
> >
> > If I use the predict function with the model you recommended, I obtain
> > log.odds or probabilities if I use "type=response":
> >
> >
> tapply(predict(m.unhatched,type="response"),list(main$SP,main$Relocation..Y.N.),mean)
> > N Y
> > G 0.7314196 0.6414554
> > L 0.6983576 0.6003087
> >
> > Are these probabilities of success (i.e. hatched) in one nest?
> >
> > Thanks,
> >
> > Alessandra
> >
> > On Mon, Feb 17, 2020 at 7:18 AM Thierry Onkelinx
> > <thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>> wrote:
> >
> > Dear Alessandra,
> >
> > Since you have both the number hatched and the total clutch size you
> > can calculate the number of successes and failures. That is
> > sufficient for a binomial distribution.
> >
> > glmer(cbind(Hatched, Unhatched) ~ Relocation..Y.N. + SP + (1 |
> > Beach_ID) + (1 | Week), family = binmial)
> >
> > A negative binomial or Poisson allow predictions larger than the
> > offset. Which is nonsense given that the number hatched cannot
> > surpass the total clutch size.
> >
> > Best regards,
> >
> > ir. Thierry Onkelinx
> > Statisticus / Statistician
> >
> > Vlaamse Overheid / Government of Flanders
> > INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR
> > NATURE AND FOREST
> > Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> > thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>
> > Havenlaan 88 bus 73, 1000 Brussel
> > www.inbo.be <http://www.inbo.be>
> >
> >
> ///////////////////////////////////////////////////////////////////////////////////////////
> > To call in the statistician after the experiment is done may be no
> > more than asking him to perform a post-mortem examination: he may be
> > able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> > The plural of anecdote is not data. ~ Roger Brinner
> > The combination of some data and an aching desire for an answer does
> > not ensure that a reasonable answer can be extracted from a given
> > body of data. ~ John Tukey
> >
> ///////////////////////////////////////////////////////////////////////////////////////////
> >
> > <https://www.inbo.be>
> >
> >
> > Op wo 12 feb. 2020 om 18:42 schreef Alessandra Bielli
> > <bielli.alessandra at gmail.com <mailto:bielli.alessandra at gmail.com>>:
> >
> > Dear Ben
> >
> > Thanks for your quick response.
> >
> > Yes, emergence success is usually between 60 and 80% or higher.
> > I am not sure how to use a binomial, if my data are counts?
> >
> > Can you explain why the approximation doesn't work well if
> > success gets
> > much above 50%? Does it make sense, then, to have "unhatched" as
> > dependent
> > variable, so that I predict mortality (usually below 50%) using
> > a nb with
> > offset(log(total clutch)) ?
> >
> > > summary(m.emerged)
> > Generalized linear mixed model fit by maximum likelihood (Laplace
> > Approximation) ['glmerMod']
> > Family: Negative Binomial(2.2104) ( log )
> > Formula: Unhatched ~ Relocation..Y.N. + SP +
> > offset(log(Total_Clutch)) +
> > (1 | Beach_ID) + (1 | Week)
> > Data: main
> >
> > AIC BIC logLik deviance df.resid
> > 5439.4 5466.0 -2713.7 5427.4 614
> >
> > Scaled residuals:
> > Min 1Q Median 3Q Max
> > -1.4383 -0.7242 -0.2287 0.4866 4.0531
> >
> > Random effects:
> > Groups Name Variance Std.Dev.
> > Week (Intercept) 0.003092 0.0556
> > Beach_ID (Intercept) 0.025894 0.1609
> > Number of obs: 620, groups: Week, 31; Beach_ID, 8
> >
> > Fixed effects:
> > Estimate Std. Error z value Pr(>|z|)
> > (Intercept) -1.38864 0.08227 -16.879 < 2e-16 ***
> > Relocation..Y.N.Y 0.32105 0.09152 3.508 0.000452 ***
> > SPL 0.22218 0.08793 2.527 0.011508 *
> > ---
> > Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >
> > Correlation of Fixed Effects:
> > (Intr) R..Y.N
> > Rlct..Y.N.Y -0.143
> > SPL -0.540 -0.038
> >
> > Thanks,
> >
> > Alessandra
> >
> > On Tue, Feb 11, 2020 at 7:29 PM Ben Bolker <bbolker at gmail.com
> > <mailto:bbolker at gmail.com>> wrote:
> >
> > >
> > > Short answer: if emergence success gets much above 50%, then
> the
> > > approximation you're making (Poisson + offset for binomial, or
> > NB +
> > > offset for negative binomial) doesn't work well. You might
> try a
> > > beta-binomial (with glmmTMB) or a binomial + an
> > observation-level random
> > > effect.
> > >
> > > (On the other hand, your intercept is -0.3, which
> > corresponds to a
> > > baseline emergence of 0.42 - not *very* high (but some beaches
> > and years
> > > will be well above that ...)
> > >
> > > Beyond that, are there any obvious patterns of mis-fit in the
> > > predicted values ... ?
> > >
> > > On 2020-02-11 8:09 p.m., Alessandra Bielli wrote:
> > > > Dear list
> > > >
> > > > I am fitting a poisson model to estimate the effect of a
> > treatment on
> > > > emergence success of hatchlings. To estimate emergence
> > success, I use
> > > > number of emerged and an offset(log(total clutch).
> > > >
> > > > However, overdispersion was detected:
> > > >
> > > >> overdisp_fun(m.emerged) #overdispersion detected
> > > >
> > > > chisq ratio rdf p
> > > > 3490.300836 5.684529 614.000000 0.000000
> > > >
> > > > Therefore, I switched to a negative binomial. I know
> > overdispersion is
> > > not
> > > > relevant for nb models, but the model plots don't look too
> > good. I also
> > > > tried to fit a poisson model with OLRE, but still the plots
> > don't look
> > > > good.
> > > > How do I know if my model is good enough, and what can I do
> > to improve
> > > it?
> > > >
> > > >> summary(m.emerged)
> > > > Generalized linear mixed model fit by maximum likelihood
> > (Laplace
> > > > Approximation) ['glmerMod']
> > > > Family: Negative Binomial(7.604) ( log )
> > > > Formula: Hatched ~ Relocation..Y.N. + SP +
> > offset(log(Total_Clutch)) + (1
> > > > |Beach_ID) + (1 | Year)
> > > > Data: main
> > > >
> > > > AIC BIC logLik deviance df.resid
> > > > 6015.6 6042.2 -3001.8 6003.6 614
> > > >
> > > > Scaled residuals:
> > > > Min 1Q Median 3Q Max
> > > > -2.6427 -0.3790 0.1790 0.5242 1.6583
> > > >
> > > > Random effects:
> > > > Groups Name Variance Std.Dev.
> > > > Beach_ID (Intercept) 0.004438 0.06662
> > > > Year (Intercept) 0.001640 0.04050
> > > > Number of obs: 620, groups: Beach_ID, 8; Year, 5
> > > >
> > > > Fixed effects:
> > > > Estimate Std. Error z value Pr(>|z|)
> > > > (Intercept) -0.29915 0.04055 -7.377 1.62e-13 ***
> > > > Relocation..Y.N.Y -0.16402 0.05052 -3.247 0.00117 **
> > > > SPL -0.08311 0.04365 -1.904 0.05689 .
> > > > ---
> > > > Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ?
> 1
> > > >
> > > > Correlation of Fixed Effects:
> > > > (Intr) R..Y.N
> > > > Rlct..Y.N.Y -0.114
> > > > SPL -0.497 -0.054
> > > >
> > > >
> > > > Thanks for your help,
> > > >
> > > > Alessandra
> > > >
> > > >
> > > > _______________________________________________
> > > > R-sig-mixed-models at r-project.org
> > <mailto:R-sig-mixed-models at r-project.org> mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > > >
> > >
> > > _______________________________________________
> > > R-sig-mixed-models at r-project.org
> > <mailto:R-sig-mixed-models at r-project.org> mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org
> > <mailto:R-sig-mixed-models at r-project.org> mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>

[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
.
  
	[[alternative HTML version deleted]]


From j@h@d||e|d @end|ng |rom ed@@c@uk  Tue Feb 18 12:05:17 2020
From: j@h@d||e|d @end|ng |rom ed@@c@uk (Jarrod Hadfield)
Date: Tue, 18 Feb 2020 11:05:17 +0000
Subject: [R-sig-ME] 
 Predicting values from MCMCglmm model with statistical
 weight in mev argument
In-Reply-To: <DB7PR07MB5077581CF394C1E788696B5BFE140@DB7PR07MB5077.eurprd07.prod.outlook.com>
References: <DB7PR07MB5077581CF394C1E788696B5BFE140@DB7PR07MB5077.eurprd07.prod.outlook.com>
Message-ID: <54f40b10-e31b-6310-e93e-f6ea1c4646da@ed.ac.uk>

Hi Kamal,

Can you post your sessionInfo()?

As a work around, use this model

model <- MCMCglmm(lD ~ tactic*period*seasonality+complique_KF+lbody+lintdur+lnb.loc+lduration
                                     , random = ~sp_phylo+species2+phylo_pop+phylo_popY+phylo_pop_id + idh(SE):units,
                                     , family = "gaussian"
                                     , ginverse = list(sp_phylo = inv.phylo$Ainv) # include a custom matrix for argument phylo
                                     , prior = prior1
                                     , data = Data
                                     , nitt = 22e+04
                                     , burnin = 20000
                                     , thin = 100
                                     , pr=TRUE)

BUT make sure to fix the prior variance associated with the final random effect term (idh(SE):units) to one. Its identical to the model you've fitted, but the predict function should work.

Cheers,

Jarrod

On 15/02/2020 22:57, Kamal Atmeh wrote:
model <- MCMCglmm(lD ~ tactic*period*seasonality+complique_KF+lbody+lintdur+lnb.loc+lduration
                                     , random = ~sp_phylo+species2+phylo_pop+phylo_popY+phylo_pop_id
                                     , family = "gaussian"
                                     , mev = SE^2 # error variance associated to each data point
                                     , ginverse = list(sp_phylo = inv.phylo$Ainv) # include a custom matrix for argument phylo
                                     , prior = prior1
                                     , data = Data
                                     , nitt = 22e+04
                                     , burnin = 20000
                                     , thin = 100
                                     , pr=TRUE)
The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336.

	[[alternative HTML version deleted]]


From k@m@|@@tmeh @end|ng |rom hotm@||@com  Tue Feb 18 13:37:29 2020
From: k@m@|@@tmeh @end|ng |rom hotm@||@com (Kamal Atmeh)
Date: Tue, 18 Feb 2020 13:37:29 +0100
Subject: [R-sig-ME] 
 Predicting values from MCMCglmm model with statistical
 weight in mev argument
In-Reply-To: <54f40b10-e31b-6310-e93e-f6ea1c4646da@ed.ac.uk>
References: <DB7PR07MB5077581CF394C1E788696B5BFE140@DB7PR07MB5077.eurprd07.prod.outlook.com>
 <54f40b10-e31b-6310-e93e-f6ea1c4646da@ed.ac.uk>
Message-ID: <DB7PR07MB507721EFE3925FF445CD0FBDFE110@DB7PR07MB5077.eurprd07.prod.outlook.com>

Hi Jarrod,

Thank you for your answer, the predict function worked! I used the 
following non-informative prior with a fixed variance for the final 
random effect as you suggested.

 >>> prior1<-list(G=list(G1=list(V=1,nu=0.02)
 ??????????????????????? ,G2=list(V=1,nu=0.02)
 ??????????????????????? ,G3=list(V=1,nu=0.02)
 ??????????????????????? ,G4=list(V=1,nu=0.02)
 ??????????????????????? ,G5=list(V=1,nu=0.02)
 ??????????????????????? ,G6=list(V=1,fix=1)),
 ???????????????? R=list(V=1,nu=0.02))

model <- MCMCglmm(lD ~ 
tactic*period*seasonality+complique_KF+lbody+lintdur+lnb.loc+lduration
 ???????????????????????????????????? , random = 
~sp_phylo+species2+phylo_pop+phylo_popY+phylo_pop_id + idh(SE):units,
 ???????????????????????????????????? , family = "gaussian"
 ???????????????????????????????????? , ginverse = list(sp_phylo = 
inv.phylo$Ainv) # include a custom matrix for argument phylo
 ???????????????????????????????????? , prior = prior1
 ???????????????????????????????????? , data = Data
 ???????????????????????????????????? , nitt = 22e+04
 ???????????????????????????????????? , burnin = 20000
 ???????????????????????????????????? , thin = 100
 ???????????????????????????????????? , pr=TRUE)

When doing expand.grid() to add in the predict function, I fixed the SE 
parameter to the mean of all standard errors of my original data. Is 
this a correct way to define the standard error column in my expand.grid 
or should I choose one value as I did for the other random effects?

 >>>>> newdt=expand.grid(tactic=c("F","H")
 ????????????????????? , period=c("PB","B")
 ????????????????????? , lbody=c(mean(Data$lbody),mean(Data$lbody) + 
sd(Data$lbody))
 ????????????????????? , complique_KF=c("OU/OUF", "BM")
 ????????????????????? , mean.dhi_ndviqa_f.3=seq(min(Data 
$mean.dhi_ndviqa_f.3), max(Data$mean.dhi_ndviqa_f.3),length.out = 500) 
## When only hider, use length.out=500
 ????????????????????? , lintdur=c(mean(Data$lintdur),mean(Data$lintdur) 
+ sd(Data$lintdur))
 ????????????????????? , 
lduration=c(mean(Data$lduration),mean(Data$lduration) + sd(Data$lduration))
 ????????????????????? , lnb.loc=c(mean(Data$lnb.loc),mean(Data 
$lnb.loc) + sd(Data $lnb.loc))
 ????????????????????? , sp_phylo_glenn="Odo_hem"
 ????????????????????? , species2="Odo_hem"
 ????????????????????? , phylo_pop="Odo_hem-wyoming"
 ????????????????????? , phylo_popY="Ant_ame-red_desert-2015"
 ????????????????????? , phylo_pop_id="Bis_bis-PANP-1001"
 ? ? ?? >>>> ???? , SE=mean(Data$SE))? ## MEAN OF ALL STANDARD ERRORS IN 
ORIGINAL DATA

I am posting below my sessionInfo() as you requested. Thanks again for 
the help.

Cheers,

Kamal

R version 3.6.2 (2019-12-12)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 18362)

Matrix products: default

locale:
[1] LC_COLLATE=French_France.1252? LC_CTYPE=French_France.1252 
LC_MONETARY=French_France.1252
[4] LC_NUMERIC=C?????????????????? LC_TIME=French_France.1252

attached base packages:
[1] grid????? parallel? stats???? graphics? grDevices utils datasets? 
methods?? base

other attached packages:
 ?[1] PerformanceAnalytics_1.5.3 xts_0.11-2 zoo_1.8-6
 ?[4] plyr_1.8.4???????????????? SDMTools_1.1-221.1 ggthemes_4.2.0
 ?[7] SyncMove_0.1-0???????????? timeline_0.9 gtable_0.3.0
[10] plot3D_1.1.1?????????????? beepr_1.3 gatepoints_0.1.3
[13] RPostgreSQL_0.6-2????????? DBI_1.0.0 trajr_1.3.0
[16] scales_1.0.0?????????????? FactoMineR_1.42 factoextra_1.0.5
[19] dismo_1.1-4??????????????? raster_2.9-5 rgdal_1.4-4
[22] plotrix_3.7-6????????????? corrplot_0.84 adehabitatHR_0.4.16
[25] adehabitatLT_0.3.24??????? CircStats_0.2-6 boot_1.3-23
[28] adehabitatMA_0.3.13??????? deldir_0.1-22 maptools_0.9-5
[31] ks_1.11.5????????????????? influence.ME_0.9-9 visreg_2.5-1
[34] rgeos_0.4-3??????????????? sp_1.3-1 cowplot_0.9.4
[37] RColorBrewer_1.1-2???????? rgl_0.100.30 misc3d_0.8-4
[40] MCMCglmm_2.29????????????? coda_0.19-3 MASS_7.3-51.4
[43] adephylo_1.1-11??????????? egg_0.4.5 gridExtra_2.3
[46] plotly_4.9.0?????????????? ggplot2_3.2.0 phytools_0.6-99
[49] maps_3.3.0???????????????? ape_5.3 rptR_0.9.22
[52] sjPlot_2.8.2?????????????? nlme_3.1-142 ade4_1.7-13
[55] MuMIn_1.43.6?????????????? glmm_1.3.0 doParallel_1.0.14
[58] iterators_1.0.10?????????? foreach_1.4.4 mvtnorm_1.0-11
[61] trust_0.1-7??????????????? phylobase_0.8.6 lmerTest_3.1-0
[64] lme4_1.1-21??????????????? Matrix_1.2-18

loaded via a namespace (and not attached):
 ? [1] R.utils_2.9.0?????????? tidyselect_0.2.5 htmlwidgets_1.3
 ? [4] combinat_0.0-8????????? RNeXML_2.3.0 munsell_0.5.0
 ? [7] animation_2.6?????????? codetools_0.2-16 effectsize_0.1.1
 ?[10] units_0.6-3???????????? miniUI_0.1.1.1 withr_2.1.2
 ?[13] audio_0.1-6???????????? colorspace_1.4-1 knitr_1.23
 ?[16] uuid_0.1-2????????????? rstudioapi_0.10 leaps_3.0
 ?[19] stats4_3.6.2??????????? emmeans_1.4.4 mnormt_1.5-5
 ?[22] LearnBayes_2.15.1?????? vctrs_0.2.0 generics_0.0.2
 ?[25] clusterGeneration_1.3.4 xfun_0.8 itertools_0.1-3
 ?[28] adegenet_2.1.1????????? R6_2.4.0 manipulateWidget_0.10.0
 ?[31] assertthat_0.2.1??????? promises_1.0.1 phangorn_2.5.5
 ?[34] rlang_0.4.0???????????? zeallot_0.1.0 scatterplot3d_0.3-41
 ?[37] splines_3.6.2?????????? lazyeval_0.2.2 broom_0.5.2
 ?[40] reshape2_1.4.3????????? modelr_0.1.5 crosstalk_1.0.0
 ?[43] backports_1.1.4???????? httpuv_1.5.1 tensorA_0.36.1
 ?[46] tools_3.6.2???????????? spData_0.3.2 cubature_2.0.3
 ?[49] Rcpp_1.0.1????????????? progress_1.2.2 classInt_0.3-3
 ?[52] purrr_0.3.2???????????? prettyunits_1.0.2 haven_2.1.1
 ?[55] ggrepel_0.8.1?????????? cluster_2.1.0 magrittr_1.5
 ?[58] data.table_1.12.2?????? gmodels_2.18.1 sjmisc_2.8.3
 ?[61] hms_0.5.0?????????????? mime_0.7 xtable_1.8-4
 ?[64] XML_3.98-1.20?????????? sjstats_0.17.9 mclust_5.4.4
 ?[67] ggeffects_0.14.1??????? compiler_3.6.2 tibble_2.1.3
 ?[70] KernSmooth_2.23-16????? crayon_1.3.4 R.oo_1.22.0
 ?[73] minqa_1.2.4???????????? htmltools_0.3.6 mgcv_1.8-31
 ?[76] corpcor_1.6.9?????????? later_0.8.0 spdep_1.1-3
 ?[79] tidyr_1.0.2???????????? expm_0.999-4 sjlabelled_1.1.3
 ?[82] sf_0.7-6??????????????? permute_0.9-5 R.methodsS3_1.7.1
 ?[85] quadprog_1.5-7????????? gdata_2.18.0 insight_0.8.1
 ?[88] igraph_1.2.4.1????????? forcats_0.4.0 pkgconfig_2.0.2
 ?[91] flashClust_1.01-2?????? rncl_0.8.3 numDeriv_2016.8-1.1
 ?[94] foreign_0.8-72????????? xml2_1.2.1 webshot_0.5.1
 ?[97] estimability_1.3??????? stringr_1.4.0 digest_0.6.20
[100] parameters_0.5.0??????? vegan_2.5-6 fastmatch_1.1-0
[103] shiny_1.3.2???????????? gtools_3.8.1 nloptr_1.2.1
[106] lifecycle_0.1.0???????? jsonlite_1.6 seqinr_3.6-1
[109] viridisLite_0.3.0?????? pillar_1.4.2 lattice_0.20-38
[112] httr_1.4.0????????????? glue_1.3.1 bayestestR_0.5.2
[115] class_7.3-15??????????? stringi_1.4.3 performance_0.4.4
[118] dplyr_0.8.3???????????? e1071_1.7-2

Le 18/02/2020 ? 12:05, Jarrod Hadfield a ?crit?:
>
> Hi Kamal,
>
> Can you post your sessionInfo()?
>
> As a work around, use this model
>
> model <- MCMCglmm(lD ~ 
> tactic*period*seasonality+complique_KF+lbody+lintdur+lnb.loc+lduration
> ???????????????????????????????????? , random = 
> ~sp_phylo+species2+phylo_pop+phylo_popY+phylo_pop_id + idh(SE):units,
> ???????????????????????????????????? , family = "gaussian"
> ???????????????????????????????????? , ginverse = list(sp_phylo = 
> inv.phylo$Ainv) # include a custom matrix for argument phylo
> ???????????????????????????????????? , prior = prior1
> ???????????????????????????????????? , data = Data
> ???????????????????????????????????? , nitt = 22e+04
> ???????????????????????????????????? , burnin = 20000
> ???????????????????????????????????? , thin = 100
> ???????????????????????????????????? , pr=TRUE)
>
> BUT make sure to fix the prior variance associated with the final 
> random effect term (idh(SE):units) to one. Its identical to the model 
> you've fitted, but the predict function should work.
>
> Cheers,
>
> Jarrod
>
> On 15/02/2020 22:57, Kamal Atmeh wrote:
>> model <- MCMCglmm(lD ~ 
>> tactic*period*seasonality+complique_KF+lbody+lintdur+lnb.loc+lduration
>> ???????????????????????????????????? , random = 
>> ~sp_phylo+species2+phylo_pop+phylo_popY+phylo_pop_id
>> ???????????????????????????????????? , family = "gaussian"
>> ???????????????????????????????????? , mev = SE^2 # error variance 
>> associated to each data point
>> ???????????????????????????????????? , ginverse = list(sp_phylo = 
>> inv.phylo$Ainv) # include a custom matrix for argument phylo
>> ???????????????????????????????????? , prior = prior1
>> ???????????????????????????????????? , data = Data
>> ???????????????????????????????????? , nitt = 22e+04
>> ???????????????????????????????????? , burnin = 20000
>> ???????????????????????????????????? , thin = 100
>> ???????????????????????????????????? , pr=TRUE) 
> The University of Edinburgh is a charitable body, registered in 
> Scotland, with registration number SC005336. 

	[[alternative HTML version deleted]]


From j@h@d||e|d @end|ng |rom ed@@c@uk  Tue Feb 18 19:25:58 2020
From: j@h@d||e|d @end|ng |rom ed@@c@uk (HADFIELD Jarrod)
Date: Tue, 18 Feb 2020 18:25:58 +0000
Subject: [R-sig-ME] 
 Predicting values from MCMCglmm model with statistical
 weight in mev argument
In-Reply-To: <DB7PR07MB507721EFE3925FF445CD0FBDFE110@DB7PR07MB5077.eurprd07.prod.outlook.com>
References: <DB7PR07MB5077581CF394C1E788696B5BFE140@DB7PR07MB5077.eurprd07.prod.outlook.com>
 <54f40b10-e31b-6310-e93e-f6ea1c4646da@ed.ac.uk>
 <DB7PR07MB507721EFE3925FF445CD0FBDFE110@DB7PR07MB5077.eurprd07.prod.outlook.com>
Message-ID: <CDE4F488-0625-4995-84A7-590AAAD2C707@ed.ac.uk>

Hi Kamal,

It doesn?t matter what you set the SE to because you are a) marginalising the random effects b) the data are Gaussian and c) you are getting confidence intervals rather than prediction intervals.

Cheers,

Jarrod



On 18 Feb 2020, at 12:37, Kamal Atmeh <kamal.atmeh at hotmail.com<mailto:kamal.atmeh at hotmail.com>> wrote:


Hi Jarrod,

Thank you for your answer, the predict function worked! I used the following non-informative prior with a fixed variance for the final random effect as you suggested.

>>> prior1<-list(G=list(G1=list(V=1,nu=0.02)
                        ,G2=list(V=1,nu=0.02)
                        ,G3=list(V=1,nu=0.02)
                        ,G4=list(V=1,nu=0.02)
                        ,G5=list(V=1,nu=0.02)
                        ,G6=list(V=1,fix=1)),
                 R=list(V=1,nu=0.02))

model <- MCMCglmm(lD ~ tactic*period*seasonality+complique_KF+lbody+lintdur+lnb.loc+lduration
                                     , random = ~sp_phylo+species2+phylo_pop+phylo_popY+phylo_pop_id + idh(SE):units,
                                     , family = "gaussian"
                                     , ginverse = list(sp_phylo = inv.phylo$Ainv) # include a custom matrix for argument phylo
                                     , prior = prior1
                                     , data = Data
                                     , nitt = 22e+04
                                     , burnin = 20000
                                     , thin = 100
                                     , pr=TRUE)

When doing expand.grid() to add in the predict function, I fixed the SE parameter to the mean of all standard errors of my original data. Is this a correct way to define the standard error column in my expand.grid or should I choose one value as I did for the other random effects?

>>>>> newdt=expand.grid(tactic=c("F","H")
                      , period=c("PB","B")
                      , lbody=c(mean(Data$lbody),mean(Data$lbody) + sd(Data$lbody))
                      , complique_KF=c("OU/OUF", "BM")
                      , mean.dhi_ndviqa_f.3=seq(min(Data $mean.dhi_ndviqa_f.3), max(Data$mean.dhi_ndviqa_f.3),length.out = 500) ## When only hider, use length.out=500
                      , lintdur=c(mean(Data$lintdur),mean(Data$lintdur) + sd(Data$lintdur))
                      , lduration=c(mean(Data$lduration),mean(Data$lduration) + sd(Data$lduration))
                      , lnb.loc=c(mean(Data$lnb.loc),mean(Data $lnb.loc) + sd(Data $lnb.loc))
                      , sp_phylo_glenn="Odo_hem"
                      , species2="Odo_hem"
                      , phylo_pop="Odo_hem-wyoming"
                      , phylo_popY="Ant_ame-red_desert-2015"
                      , phylo_pop_id="Bis_bis-PANP-1001"
       >>>>      , SE=mean(Data$SE))  ## MEAN OF ALL STANDARD ERRORS IN ORIGINAL DATA

I am posting below my sessionInfo() as you requested. Thanks again for the help.

Cheers,

Kamal

R version 3.6.2 (2019-12-12)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 18362)

Matrix products: default

locale:
[1] LC_COLLATE=French_France.1252  LC_CTYPE=French_France.1252    LC_MONETARY=French_France.1252
[4] LC_NUMERIC=C                   LC_TIME=French_France.1252

attached base packages:
[1] grid      parallel  stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
 [1] PerformanceAnalytics_1.5.3 xts_0.11-2                 zoo_1.8-6
 [4] plyr_1.8.4                 SDMTools_1.1-221.1         ggthemes_4.2.0
 [7] SyncMove_0.1-0             timeline_0.9               gtable_0.3.0
[10] plot3D_1.1.1               beepr_1.3                  gatepoints_0.1.3
[13] RPostgreSQL_0.6-2          DBI_1.0.0                  trajr_1.3.0
[16] scales_1.0.0               FactoMineR_1.42            factoextra_1.0.5
[19] dismo_1.1-4                raster_2.9-5               rgdal_1.4-4
[22] plotrix_3.7-6              corrplot_0.84              adehabitatHR_0.4.16
[25] adehabitatLT_0.3.24        CircStats_0.2-6            boot_1.3-23
[28] adehabitatMA_0.3.13        deldir_0.1-22              maptools_0.9-5
[31] ks_1.11.5                  influence.ME_0.9-9         visreg_2.5-1
[34] rgeos_0.4-3                sp_1.3-1                   cowplot_0.9.4
[37] RColorBrewer_1.1-2         rgl_0.100.30               misc3d_0.8-4
[40] MCMCglmm_2.29              coda_0.19-3                MASS_7.3-51.4
[43] adephylo_1.1-11            egg_0.4.5                  gridExtra_2.3
[46] plotly_4.9.0               ggplot2_3.2.0              phytools_0.6-99
[49] maps_3.3.0                 ape_5.3                    rptR_0.9.22
[52] sjPlot_2.8.2               nlme_3.1-142               ade4_1.7-13
[55] MuMIn_1.43.6               glmm_1.3.0                 doParallel_1.0.14
[58] iterators_1.0.10           foreach_1.4.4              mvtnorm_1.0-11
[61] trust_0.1-7                phylobase_0.8.6            lmerTest_3.1-0
[64] lme4_1.1-21                Matrix_1.2-18

loaded via a namespace (and not attached):
  [1] R.utils_2.9.0           tidyselect_0.2.5        htmlwidgets_1.3
  [4] combinat_0.0-8          RNeXML_2.3.0            munsell_0.5.0
  [7] animation_2.6           codetools_0.2-16        effectsize_0.1.1
 [10] units_0.6-3             miniUI_0.1.1.1          withr_2.1.2
 [13] audio_0.1-6             colorspace_1.4-1        knitr_1.23
 [16] uuid_0.1-2              rstudioapi_0.10         leaps_3.0
 [19] stats4_3.6.2            emmeans_1.4.4           mnormt_1.5-5
 [22] LearnBayes_2.15.1       vctrs_0.2.0             generics_0.0.2
 [25] clusterGeneration_1.3.4 xfun_0.8                itertools_0.1-3
 [28] adegenet_2.1.1          R6_2.4.0                manipulateWidget_0.10.0
 [31] assertthat_0.2.1        promises_1.0.1          phangorn_2.5.5
 [34] rlang_0.4.0             zeallot_0.1.0           scatterplot3d_0.3-41
 [37] splines_3.6.2           lazyeval_0.2.2          broom_0.5.2
 [40] reshape2_1.4.3          modelr_0.1.5            crosstalk_1.0.0
 [43] backports_1.1.4         httpuv_1.5.1            tensorA_0.36.1
 [46] tools_3.6.2             spData_0.3.2            cubature_2.0.3
 [49] Rcpp_1.0.1              progress_1.2.2          classInt_0.3-3
 [52] purrr_0.3.2             prettyunits_1.0.2       haven_2.1.1
 [55] ggrepel_0.8.1           cluster_2.1.0           magrittr_1.5
 [58] data.table_1.12.2       gmodels_2.18.1          sjmisc_2.8.3
 [61] hms_0.5.0               mime_0.7                xtable_1.8-4
 [64] XML_3.98-1.20           sjstats_0.17.9          mclust_5.4.4
 [67] ggeffects_0.14.1        compiler_3.6.2          tibble_2.1.3
 [70] KernSmooth_2.23-16      crayon_1.3.4            R.oo_1.22.0
 [73] minqa_1.2.4             htmltools_0.3.6         mgcv_1.8-31
 [76] corpcor_1.6.9           later_0.8.0             spdep_1.1-3
 [79] tidyr_1.0.2             expm_0.999-4            sjlabelled_1.1.3
 [82] sf_0.7-6                permute_0.9-5           R.methodsS3_1.7.1
 [85] quadprog_1.5-7          gdata_2.18.0            insight_0.8.1
 [88] igraph_1.2.4.1          forcats_0.4.0           pkgconfig_2.0.2
 [91] flashClust_1.01-2       rncl_0.8.3              numDeriv_2016.8-1.1
 [94] foreign_0.8-72          xml2_1.2.1              webshot_0.5.1
 [97] estimability_1.3        stringr_1.4.0           digest_0.6.20
[100] parameters_0.5.0        vegan_2.5-6             fastmatch_1.1-0
[103] shiny_1.3.2             gtools_3.8.1            nloptr_1.2.1
[106] lifecycle_0.1.0         jsonlite_1.6            seqinr_3.6-1
[109] viridisLite_0.3.0       pillar_1.4.2            lattice_0.20-38
[112] httr_1.4.0              glue_1.3.1              bayestestR_0.5.2
[115] class_7.3-15            stringi_1.4.3           performance_0.4.4
[118] dplyr_0.8.3             e1071_1.7-2

Le 18/02/2020 ? 12:05, Jarrod Hadfield a ?crit :

Hi Kamal,

Can you post your sessionInfo()?

As a work around, use this model

model <- MCMCglmm(lD ~ tactic*period*seasonality+complique_KF+lbody+lintdur+lnb.loc+lduration
                                     , random = ~sp_phylo+species2+phylo_pop+phylo_popY+phylo_pop_id + idh(SE):units,
                                     , family = "gaussian"
                                     , ginverse = list(sp_phylo = inv.phylo$Ainv) # include a custom matrix for argument phylo
                                     , prior = prior1
                                     , data = Data
                                     , nitt = 22e+04
                                     , burnin = 20000
                                     , thin = 100
                                     , pr=TRUE)

BUT make sure to fix the prior variance associated with the final random effect term (idh(SE):units) to one. Its identical to the model you've fitted, but the predict function should work.

Cheers,

Jarrod

On 15/02/2020 22:57, Kamal Atmeh wrote:
model <- MCMCglmm(lD ~ tactic*period*seasonality+complique_KF+lbody+lintdur+lnb.loc+lduration
                                     , random = ~sp_phylo+species2+phylo_pop+phylo_popY+phylo_pop_id
                                     , family = "gaussian"
                                     , mev = SE^2 # error variance associated to each data point
                                     , ginverse = list(sp_phylo = inv.phylo$Ainv) # include a custom matrix for argument phylo
                                     , prior = prior1
                                     , data = Data
                                     , nitt = 22e+04
                                     , burnin = 20000
                                     , thin = 100
                                     , pr=TRUE)
The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336.


	[[alternative HTML version deleted]]


From b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com  Tue Feb 18 22:25:47 2020
From: b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com (Alessandra Bielli)
Date: Tue, 18 Feb 2020 15:25:47 -0600
Subject: [R-sig-ME] Fwd: model check for negative binomial model
In-Reply-To: <UNXRI5YWV9U4.7QC41ED29PMN3@mwweb11oc>
References: <UNXRI5YWV9U4.7QC41ED29PMN3@mwweb11oc>
Message-ID: <CA+6N3yXjAUkuk3tDBQpWJnA+o_8T63m7jccuuP65SG=y-Y0b9w@mail.gmail.com>

Dear Salvador,

Thank you for the clarification, that was very helpful.

Best wishes

Alessandra

On Mon, Feb 17, 2020 at 9:27 PM Salvador SANCHEZ COLON <
salvadorsanchezcolon at prodigy.net.mx> wrote:

> CORRECTION
>
>
> Dear Alessandra,
>
>
> My apologies, I msread the output you pasted in your previous messages. I
> did not realize that the parameter estimates that you were providing
> correspond to a negative binomial model, I assumed that they came from a
> binomial (logit) model. As Profr. Bolker mentioned, you cannot fit a
> negative binomial model to your data, it has to be a binomial (logit model)
> in which you model the number of eggs that hatch out of the total number of
> eggs in the nest. What the model predicts (after back transforming the odds
> ratio) is the probability, p, of an egg hatching, which can then be
> translated into the expected number of eggs that hatch simply by
> multiplying it by the number of eggs in the nest, np.
>
>
> I hope this helps and, again, my apologies for the confusion and for
> messing things yp.
>
>
> Salvador
>
>
> En Lun, 17 Febrero, 2020 en 20:46, yo <salvadorsanchezcolon at prodigy.net.mx>
> escribi?:
>
> Para: bielli.alessandra at gmail.com
> Cc: bbolker at gmail.com; r-sig-mixed-models at r-project.org
>
> Cara Alessandra,
>
>
> If I may interject into your conversation, the key to your question lies
> in the parameter estimates:for the fixed effects:
>
>
>                                Estimate           Std. Error             z
> value        Pr(>|z|)
> (Intercept)               -1.38864             0.08227
> -16.879      < 2e-16 ***
> Relocation..Y.N.Y       0.32105              0.09152                3.508
>      0.000452 ***
> SPL                          0.22218               0.08793
> 2.527       0.011508 *
>
>
> As I understand your design, you have two independent variables (factors):
> Species and treatment, each with two levels: L vs. G and Y vs N.
>
>
> Thus, the logit model that you obtain (omitting the random effects) is:
>
>
> logit(p) = log(p/(1-p) = -1.38864 + 0.32105*Treatment + 0.22218*Species
>
> Treatment and species are two-level factors which are coded as 0 or 1 and
> the model has to have parameter estimates for each of those levels. By
> design, in generalized linear models parameters are estimated by taking one
> of the levels of each factor as the reference level and assigned a value of
> 0, and the parameters for the other levels are expressed in relation to the
> reference level. Thus, in your case, treatment N and species G are
> designated as the reference levels for their corresponding factors and,
> hence, their parameter values are both 0 (and not listed in the output).
> Then, treatment Y has a positive effect (0.32) on the odds ratio of
> hatching/not-hatching compared to treatment N; and species L also has a
> positive effect (0.22 times) on the odds ratio of hatching vs not-hatching.
>
>
> The intercept then (-1.38864) denotes the odds ratio for the combination
> of treatment N and species G; that is, when both factors are at their
> reference level of 0. This translates into a probability of hatching (p):
>
>
> p =  exp(-1.38864)/(1+exp(-1.38864) = 0.1996
>
>
> For treatment Y and species G, the model becomes:
>
>
> p =  exp(-1.38864 + 0.32105)/(1+exp(-1.38864 + 0.32105) = 0.25569
>
>
> as treatment Y has a positive effect on the odds ratio of hatching.
>
>
> For treatment N and species L, the model becomes:
>
>
> p =  exp(-1.38864 + 0.22218)/(1+exp(-1.38864 + 0.22218) = 0.2375
>
>
> as species L has a positive effect on the odds ratio of hatching.
>
>
> Finally, for the combination of treament Y and species L, the model
> becomes:
>
>
> p =  exp(-1.38864 + 0.32105 + 0.22218)/(1+exp(-1.38864 + 0.32105 + 0.22218)
> = 0.3002
>
>
> as both levels Y and L have positive effects on the odds ratio of hatching.
>
>
> I hope this helps.
>
>
> Best regards,
>
>
> Salvador
>
>
>
>
>
>
> En Lun, 17 Febrero, 2020 en 18:2, Alessandra Bielli <
> bielli.alessandra at gmail.com> escribi?:
>
> Para: Ben Bolker
> Cc: r-sig-mixed-models at r-project.org
> Dear Ben
>
> I am trying to make a prediction for the combination of species (L or G)
> and treatment (control/experiment).
>
> I am still confused about the prediction values. I would like to present
> results as a success rate for a nest, to say that treatment
> increases/decreases success by ...%. But the value I have is the
> probability that 1 egg in the nest succeeds, correct? I am not sure how to
> use these predictions.
>
> Thanks for your help!
>
> Alessandra
>
> On Mon, Feb 17, 2020 at 2:15 PM Ben Bolker <bbolker at gmail.com> wrote:
>
> >
> > That's correct. There are some delicate issues about prediction:
> >
> > * do you want to use the original (potentially unbalanced) data for
> > prediction? (That's what you're doing here).
> > * or, do you want to make predictions for a "typical" nest and week
> > combination, in which case you would use
> >
> >
> > pframe <- with(your_data,
> > expand.grid(Relocation..Y.N.=unique(Relocation..Y.N.),
> > SP=unique(SP))
> > predict(m.unhatched,type="response",re.form=NA,newdata=pframe))
> >
> > You could also use expand.grid() to generate a balanced design (i.e.
> > all combinations of weeks and nests), which would give yet another
> answer.
> >
> > There are a lot of packages designed for doing these kinds of
> > post-fitting manipulations (e.g. 'margins', 'emmeans'), you might find
> > them useful ...
> >
> >
> >
> > On 2020-02-17 1:48 p.m., Alessandra Bielli wrote:
> > > Dear Thierry
> > >
> > > Thanks for your reply.
> > >
> > > I read a bit about the prediction for a binomial model with
> > > success/failures and I have a couple of questions.
> > >
> > > If I use the predict function with the model you recommended, I obtain
> > > log.odds or probabilities if I use "type=response":
> > >
> > >
> >
> tapply(predict(m.unhatched,type="response"),list(main$SP,main$Relocation..Y.N.),mean)
> > > N Y
> > > G 0.7314196 0.6414554
> > > L 0.6983576 0.6003087
> > >
> > > Are these probabilities of success (i.e. hatched) in one nest?
> > >
> > > Thanks,
> > >
> > > Alessandra
> > >
> > > On Mon, Feb 17, 2020 at 7:18 AM Thierry Onkelinx
> > > <thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>> wrote:
> > >
> > > Dear Alessandra,
> > >
> > > Since you have both the number hatched and the total clutch size you
> > > can calculate the number of successes and failures. That is
> > > sufficient for a binomial distribution.
> > >
> > > glmer(cbind(Hatched, Unhatched) ~ Relocation..Y.N. + SP + (1 |
> > > Beach_ID) + (1 | Week), family = binmial)
> > >
> > > A negative binomial or Poisson allow predictions larger than the
> > > offset. Which is nonsense given that the number hatched cannot
> > > surpass the total clutch size.
> > >
> > > Best regards,
> > >
> > > ir. Thierry Onkelinx
> > > Statisticus / Statistician
> > >
> > > Vlaamse Overheid / Government of Flanders
> > > INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR
> > > NATURE AND FOREST
> > > Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> > > thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>
> > > Havenlaan 88 bus 73, 1000 Brussel
> > > www.inbo.be <http://www.inbo.be>
> > >
> > >
> >
> ///////////////////////////////////////////////////////////////////////////////////////////
> > > To call in the statistician after the experiment is done may be no
> > > more than asking him to perform a post-mortem examination: he may be
> > > able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> > > The plural of anecdote is not data. ~ Roger Brinner
> > > The combination of some data and an aching desire for an answer does
> > > not ensure that a reasonable answer can be extracted from a given
> > > body of data. ~ John Tukey
> > >
> >
> ///////////////////////////////////////////////////////////////////////////////////////////
> > >
> > > <https://www.inbo.be>
> > >
> > >
> > > Op wo 12 feb. 2020 om 18:42 schreef Alessandra Bielli
> > > <bielli.alessandra at gmail.com <mailto:bielli.alessandra at gmail.com>>:
> > >
> > > Dear Ben
> > >
> > > Thanks for your quick response.
> > >
> > > Yes, emergence success is usually between 60 and 80% or higher.
> > > I am not sure how to use a binomial, if my data are counts?
> > >
> > > Can you explain why the approximation doesn't work well if
> > > success gets
> > > much above 50%? Does it make sense, then, to have "unhatched" as
> > > dependent
> > > variable, so that I predict mortality (usually below 50%) using
> > > a nb with
> > > offset(log(total clutch)) ?
> > >
> > > > summary(m.emerged)
> > > Generalized linear mixed model fit by maximum likelihood (Laplace
> > > Approximation) ['glmerMod']
> > > Family: Negative Binomial(2.2104) ( log )
> > > Formula: Unhatched ~ Relocation..Y.N. + SP +
> > > offset(log(Total_Clutch)) +
> > > (1 | Beach_ID) + (1 | Week)
> > > Data: main
> > >
> > > AIC BIC logLik deviance df.resid
> > > 5439.4 5466.0 -2713.7 5427.4 614
> > >
> > > Scaled residuals:
> > > Min 1Q Median 3Q Max
> > > -1.4383 -0.7242 -0.2287 0.4866 4.0531
> > >
> > > Random effects:
> > > Groups Name Variance Std.Dev.
> > > Week (Intercept) 0.003092 0.0556
> > > Beach_ID (Intercept) 0.025894 0.1609
> > > Number of obs: 620, groups: Week, 31; Beach_ID, 8
> > >
> > > Fixed effects:
> > > Estimate Std. Error z value Pr(>|z|)
> > > (Intercept) -1.38864 0.08227 -16.879 < 2e-16 ***
> > > Relocation..Y.N.Y 0.32105 0.09152 3.508 0.000452 ***
> > > SPL 0.22218 0.08793 2.527 0.011508 *
> > > ---
> > > Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> > >
> > > Correlation of Fixed Effects:
> > > (Intr) R..Y.N
> > > Rlct..Y.N.Y -0.143
> > > SPL -0.540 -0.038
> > >
> > > Thanks,
> > >
> > > Alessandra
> > >
> > > On Tue, Feb 11, 2020 at 7:29 PM Ben Bolker <bbolker at gmail.com
> > > <mailto:bbolker at gmail.com>> wrote:
> > >
> > > >
> > > > Short answer: if emergence success gets much above 50%, then
> > the
> > > > approximation you're making (Poisson + offset for binomial, or
> > > NB +
> > > > offset for negative binomial) doesn't work well. You might
> > try a
> > > > beta-binomial (with glmmTMB) or a binomial + an
> > > observation-level random
> > > > effect.
> > > >
> > > > (On the other hand, your intercept is -0.3, which
> > > corresponds to a
> > > > baseline emergence of 0.42 - not *very* high (but some beaches
> > > and years
> > > > will be well above that ...)
> > > >
> > > > Beyond that, are there any obvious patterns of mis-fit in the
> > > > predicted values ... ?
> > > >
> > > > On 2020-02-11 8:09 p.m., Alessandra Bielli wrote:
> > > > > Dear list
> > > > >
> > > > > I am fitting a poisson model to estimate the effect of a
> > > treatment on
> > > > > emergence success of hatchlings. To estimate emergence
> > > success, I use
> > > > > number of emerged and an offset(log(total clutch).
> > > > >
> > > > > However, overdispersion was detected:
> > > > >
> > > > >> overdisp_fun(m.emerged) #overdispersion detected
> > > > >
> > > > > chisq ratio rdf p
> > > > > 3490.300836 5.684529 614.000000 0.000000
> > > > >
> > > > > Therefore, I switched to a negative binomial. I know
> > > overdispersion is
> > > > not
> > > > > relevant for nb models, but the model plots don't look too
> > > good. I also
> > > > > tried to fit a poisson model with OLRE, but still the plots
> > > don't look
> > > > > good.
> > > > > How do I know if my model is good enough, and what can I do
> > > to improve
> > > > it?
> > > > >
> > > > >> summary(m.emerged)
> > > > > Generalized linear mixed model fit by maximum likelihood
> > > (Laplace
> > > > > Approximation) ['glmerMod']
> > > > > Family: Negative Binomial(7.604) ( log )
> > > > > Formula: Hatched ~ Relocation..Y.N. + SP +
> > > offset(log(Total_Clutch)) + (1
> > > > > |Beach_ID) + (1 | Year)
> > > > > Data: main
> > > > >
> > > > > AIC BIC logLik deviance df.resid
> > > > > 6015.6 6042.2 -3001.8 6003.6 614
> > > > >
> > > > > Scaled residuals:
> > > > > Min 1Q Median 3Q Max
> > > > > -2.6427 -0.3790 0.1790 0.5242 1.6583
> > > > >
> > > > > Random effects:
> > > > > Groups Name Variance Std.Dev.
> > > > > Beach_ID (Intercept) 0.004438 0.06662
> > > > > Year (Intercept) 0.001640 0.04050
> > > > > Number of obs: 620, groups: Beach_ID, 8; Year, 5
> > > > >
> > > > > Fixed effects:
> > > > > Estimate Std. Error z value Pr(>|z|)
> > > > > (Intercept) -0.29915 0.04055 -7.377 1.62e-13 ***
> > > > > Relocation..Y.N.Y -0.16402 0.05052 -3.247 0.00117 **
> > > > > SPL -0.08311 0.04365 -1.904 0.05689 .
> > > > > ---
> > > > > Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ?
> > 1
> > > > >
> > > > > Correlation of Fixed Effects:
> > > > > (Intr) R..Y.N
> > > > > Rlct..Y.N.Y -0.114
> > > > > SPL -0.497 -0.054
> > > > >
> > > > >
> > > > > Thanks for your help,
> > > > >
> > > > > Alessandra
> > > > >
> > > > >
> > > > > _______________________________________________
> > > > > R-sig-mixed-models at r-project.org
> > > <mailto:R-sig-mixed-models at r-project.org> mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > > > >
> > > >
> > > > _______________________________________________
> > > > R-sig-mixed-models at r-project.org
> > > <mailto:R-sig-mixed-models at r-project.org> mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > > >
> > > _______________________________________________
> > > R-sig-mixed-models at r-project.org
> > > <mailto:R-sig-mixed-models at r-project.org> mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > >
> >
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> .
>

	[[alternative HTML version deleted]]


From c@meron@@o @end|ng |rom m@||@utoronto@c@  Tue Feb 18 23:31:26 2020
From: c@meron@@o @end|ng |rom m@||@utoronto@c@ (Cameron So)
Date: Tue, 18 Feb 2020 22:31:26 +0000
Subject: [R-sig-ME] MCMCglmm Different Outputs for ginverse Inputs
Message-ID: <YTOPR0101MB1867B141AD57BF39012C3799BC1C0@YTOPR0101MB1867.CANPRD01.PROD.OUTLOOK.COM>

Hi everyone,

I am using MCMCglmm to estimate additive genetic variance, dominance, and maternal effects in a plant population when exposed to a new environment and normal conditions (different individuals but replicates of genotype). I have two questions, the first being long and the latter being short.

Q1:
Since I am trying to incorporate a dominance random effect in my model, I initially followed the tutorial in Wolak's (2012) paper [ see: https://doi.org/10.1111/j.2041-210X.2012.00213.x ], in which the inverse additive matrix is indicated (see below). However, after checking model diagnostics and ensuring convergence and no autocorrelation, I was observing very low variance for additive, dominance, and subsequently heritability estimates (<0.1 for h2) for numerous univariate trait models. Obviously, this is not what I expected as heritability for various fitness-related traits is usually between 0.1 - 0.7 according to the literature.

Recently, I ran a new model without indicating the inverse additive matrix and let MCMCglmm automatically create it. The output for this model (although there is some high correlation issues that I am currently fixing) had an heritability estimate ~0.7, greater than the previous model.

Would anyone know the reasoning behind these model output differences when the inverse additive matrix is indicated or not? It appears that excluding the Ainv in the model yields a more accurate estimation.

Below is the an example of the R code for the trait plant height with parameter expanded priors.

Shared Priors:

prior9.2 <- list(R = list(V = 1, nu = 0.002),
                 G = list(G1 = list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),
                          G2 = list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),
                          G3 = list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000)))

Model 1 Example: Including Ainv

Ainv <- inverseA(ped[, 1:3])$Ainv
Dinv <- makeD(ped, 1:3])$Dinv

HW_model9.2 <- MCMCglmm(height ~ plot, random = ~animal + animalDom + matID,
ginverse = list(animal = Ainv, animalDom = Dinv),
family = "gaussian", data = heated.height, prior = prior9.2,
nitt = 2100000, thin = 1000, burnin = 100000, verbose = T, pr = TRUE)

Model 2 Example: Excluding Ainv

listD <- makeD(ped)
Dinv <- listD$Dinv

HW_model9.2b <- MCMCglmm(height ~ plot, random = ~animal + animalDom + matID,

ginverse = list(animalDom = Dinv),

family = "gaussian", data = heated.height, prior = prior9.2,

nitt = 2100000, thin = 1000, burnin = 100000, verbose = T, pr = TRUE)



..

Q2: Regarding priors, I have a shorter question on the parameterization of V and nu for the Inverse Wishart prior. My understanding is that the Inverse Gamma (0.001, 0.001) prior follows (V=1, nu = 0.002). However, for the Inverse Wishart prior, different sources allude to different parameterizations of V and nu. For example, for a multivariate model, once source says it is V=diag(n), nu = n - 0.998 where n is the number of traits, while another source states that it is V=diag(n), nu = n. Would someone be able to clarify my confusion? My apologies as I am quite new to using MCMCglmm and setting appropriate priors.




______

Cameron So

Master's Student | Plant Evolutionary Responses to Climate Change | Weis Lab
Department of Ecology & Evolutionary Biology
University of Toronto

	[[alternative HTML version deleted]]


From biii m@iii@g oii de@@ey@ws  Wed Feb 19 12:16:21 2020
From: biii m@iii@g oii de@@ey@ws (biii m@iii@g oii de@@ey@ws)
Date: Wed, 19 Feb 2020 06:16:21 -0500
Subject: [R-sig-ME] Why does nlme::getVarCov not Work for nlme Models?
Message-ID: <05cf01d5e716$0466cb40$0d3461c0$@denney.ws>

Hi,

 

I was just trying to summarize an nlme model using `broom.mixed::tidy()`,
and it doesn't give random effects or residual variability for nlme models.
As I looked deeper, this appears to be related to the fact that
`nlme::getVarCorr()` doesn't support nlme models for no reason that I can
discern [1].  I do see that it simply has a `stop()` call if the model is
nlme, but I don't know or see a commented reason why this would be.

 

I can imagine that in a nonlinear model the interpretation of variance is
different than in a linear model (namely that it is more likely to be
asymmetric and this result is asymptotic), but that doesn't seem like a
reason to prevent the use of the tool.  When I pretended that my model was
an lme model (set the class temporarily to "lme"), it gave the expected
result.

 

My questions are:

 

1.	Why does nlme::getVarCorr() not support nlme models?
2.	If there is not a known reason, does someone in R-core think that a
patch removing the stop below [1] would be accepted?

 

[1] https://svn.r-project.org/R-packages/trunk/nlme/R/VarCov.R the lines
are:

    if(any("nlme" == class(obj)))

        stop("not implemented for \"nlme\" objects")

 

Thanks,

 

Bill

 

(Based on a suggestion from the mailing list, this is reposted from r-help:
https://stat.ethz.ch/pipermail/r-help/2019-November/464826.html)


	[[alternative HTML version deleted]]


From k@m@|@@tmeh @end|ng |rom hotm@||@com  Wed Feb 19 15:00:34 2020
From: k@m@|@@tmeh @end|ng |rom hotm@||@com (Kamal Atmeh)
Date: Wed, 19 Feb 2020 15:00:34 +0100
Subject: [R-sig-ME] 
 Predicting values from MCMCglmm model with statistical
 weight in mev argument
In-Reply-To: <CDE4F488-0625-4995-84A7-590AAAD2C707@ed.ac.uk>
References: <DB7PR07MB5077581CF394C1E788696B5BFE140@DB7PR07MB5077.eurprd07.prod.outlook.com>
 <54f40b10-e31b-6310-e93e-f6ea1c4646da@ed.ac.uk>
 <DB7PR07MB507721EFE3925FF445CD0FBDFE110@DB7PR07MB5077.eurprd07.prod.outlook.com>
 <CDE4F488-0625-4995-84A7-590AAAD2C707@ed.ac.uk>
Message-ID: <DB7PR07MB50776F4BEFCDA08B5FD7C1ADFE100@DB7PR07MB5077.eurprd07.prod.outlook.com>

Hi Jarrod,

Perfect. Thank you very much for your answer and for your help.

Cheers,

Kamal


Le 18/02/2020 ? 19:25, HADFIELD Jarrod a ?crit?:
> Hi Kamal,
>
> It doesn?t matter what you set the SE to because you are a) 
> marginalising the random effects b) the data are Gaussian and c) you 
> are getting confidence intervals rather than prediction intervals.
>
> Cheers,
>
> Jarrod
>
>
>
>> On 18 Feb 2020, at 12:37, Kamal Atmeh <kamal.atmeh at hotmail.com 
>> <mailto:kamal.atmeh at hotmail.com>> wrote:
>>
>> Hi Jarrod,
>>
>> Thank you for your answer, the predict function worked! I used the 
>> following non-informative prior with a fixed variance for the final 
>> random effect as you suggested.
>>
>> >>> prior1<-list(G=list(G1=list(V=1,nu=0.02)
>> ??????????????????????? ,G2=list(V=1,nu=0.02)
>> ??????????????????????? ,G3=list(V=1,nu=0.02)
>> ??????????????????????? ,G4=list(V=1,nu=0.02)
>> ??????????????????????? ,G5=list(V=1,nu=0.02)
>> ??????????????????????? ,G6=list(V=1,fix=1)),
>> ???????????????? R=list(V=1,nu=0.02))
>>
>> model <- MCMCglmm(lD ~ 
>> tactic*period*seasonality+complique_KF+lbody+lintdur+lnb.loc+lduration
>> ???????????????????????????????????? , random = 
>> ~sp_phylo+species2+phylo_pop+phylo_popY+phylo_pop_id + idh(SE):units,
>> ???????????????????????????????????? , family = "gaussian"
>> ???????????????????????????????????? , ginverse = list(sp_phylo = 
>> inv.phylo$Ainv) # include a custom matrix for argument phylo
>> ???????????????????????????????????? , prior = prior1
>> ???????????????????????????????????? , data = Data
>> ???????????????????????????????????? , nitt = 22e+04
>> ???????????????????????????????????? , burnin = 20000
>> ???????????????????????????????????? , thin = 100
>> ???????????????????????????????????? , pr=TRUE)
>>
>> When doing expand.grid() to add in the predict function, I fixed the 
>> SE parameter to the mean of all standard errors of my original data. 
>> Is this a correct way to define the standard error column in my 
>> expand.grid or should I choose one value as I did for the other 
>> random effects?
>>
>> >>>>> newdt=expand.grid(tactic=c("F","H")
>> ????????????????????? , period=c("PB","B")
>> ????????????????????? , lbody=c(mean(Data$lbody),mean(Data$lbody) + 
>> sd(Data$lbody))
>> ????????????????????? , complique_KF=c("OU/OUF", "BM")
>> ????????????????????? , mean.dhi_ndviqa_f.3=seq(min(Data 
>> $mean.dhi_ndviqa_f.3), max(Data$mean.dhi_ndviqa_f.3),length.out = 
>> 500) ## When only hider, use length.out=500
>> ????????????????????? , 
>> lintdur=c(mean(Data$lintdur),mean(Data$lintdur) + sd(Data$lintdur))
>> ????????????????????? , 
>> lduration=c(mean(Data$lduration),mean(Data$lduration) + 
>> sd(Data$lduration))
>> ????????????????????? , lnb.loc=c(mean(Data$lnb.loc),mean(Data 
>> $lnb.loc) + sd(Data $lnb.loc))
>> ????????????????????? , sp_phylo_glenn="Odo_hem"
>> ????????????????????? , species2="Odo_hem"
>> ????????????????????? , phylo_pop="Odo_hem-wyoming"
>> ????????????????????? , phylo_popY="Ant_ame-red_desert-2015"
>> ????????????????????? , phylo_pop_id="Bis_bis-PANP-1001"
>> ? ? ?? >>>> ???? , SE=mean(Data$SE))? ## MEAN OF ALL STANDARD ERRORS 
>> IN ORIGINAL DATA
>>
>> I am posting below my sessionInfo() as you requested. Thanks again 
>> for the help.
>>
>> Cheers,
>>
>> Kamal
>>
>> R version 3.6.2 (2019-12-12)
>> Platform: x86_64-w64-mingw32/x64 (64-bit)
>> Running under: Windows 10 x64 (build 18362)
>>
>> Matrix products: default
>>
>> locale:
>> [1] LC_COLLATE=French_France.1252 LC_CTYPE=French_France.1252 
>> LC_MONETARY=French_France.1252
>> [4] LC_NUMERIC=C LC_TIME=French_France.1252
>>
>> attached base packages:
>> [1] grid????? parallel? stats???? graphics? grDevices utils???? 
>> datasets? methods?? base
>>
>> other attached packages:
>> ?[1] PerformanceAnalytics_1.5.3 xts_0.11-2???????????????? zoo_1.8-6
>> ?[4] plyr_1.8.4 SDMTools_1.1-221.1???????? ggthemes_4.2.0
>> ?[7] SyncMove_0.1-0 timeline_0.9?????????????? gtable_0.3.0
>> [10] plot3D_1.1.1 beepr_1.3????????????????? gatepoints_0.1.3
>> [13] RPostgreSQL_0.6-2 DBI_1.0.0????????????????? trajr_1.3.0
>> [16] scales_1.0.0 FactoMineR_1.42??????????? factoextra_1.0.5
>> [19] dismo_1.1-4 raster_2.9-5?????????????? rgdal_1.4-4
>> [22] plotrix_3.7-6 corrplot_0.84????????????? adehabitatHR_0.4.16
>> [25] adehabitatLT_0.3.24 CircStats_0.2-6??????????? boot_1.3-23
>> [28] adehabitatMA_0.3.13 deldir_0.1-22????????????? maptools_0.9-5
>> [31] ks_1.11.5 influence.ME_0.9-9???????? visreg_2.5-1
>> [34] rgeos_0.4-3 sp_1.3-1?????????????????? cowplot_0.9.4
>> [37] RColorBrewer_1.1-2 rgl_0.100.30?????????????? misc3d_0.8-4
>> [40] MCMCglmm_2.29 coda_0.19-3??????????????? MASS_7.3-51.4
>> [43] adephylo_1.1-11 egg_0.4.5????????????????? gridExtra_2.3
>> [46] plotly_4.9.0 ggplot2_3.2.0????????????? phytools_0.6-99
>> [49] maps_3.3.0 ape_5.3??????????????????? rptR_0.9.22
>> [52] sjPlot_2.8.2 nlme_3.1-142?????????????? ade4_1.7-13
>> [55] MuMIn_1.43.6 glmm_1.3.0???????????????? doParallel_1.0.14
>> [58] iterators_1.0.10 foreach_1.4.4????????????? mvtnorm_1.0-11
>> [61] trust_0.1-7 phylobase_0.8.6??????????? lmerTest_3.1-0
>> [64] lme4_1.1-21 Matrix_1.2-18
>>
>> loaded via a namespace (and not attached):
>> ? [1] R.utils_2.9.0?????????? tidyselect_0.2.5 htmlwidgets_1.3
>> ? [4] combinat_0.0-8????????? RNeXML_2.3.0 munsell_0.5.0
>> ? [7] animation_2.6?????????? codetools_0.2-16 effectsize_0.1.1
>> ?[10] units_0.6-3???????????? miniUI_0.1.1.1 withr_2.1.2
>> ?[13] audio_0.1-6???????????? colorspace_1.4-1 knitr_1.23
>> ?[16] uuid_0.1-2????????????? rstudioapi_0.10 leaps_3.0
>> ?[19] stats4_3.6.2??????????? emmeans_1.4.4 mnormt_1.5-5
>> ?[22] LearnBayes_2.15.1?????? vctrs_0.2.0 generics_0.0.2
>> ?[25] clusterGeneration_1.3.4 xfun_0.8 itertools_0.1-3
>> ?[28] adegenet_2.1.1????????? R6_2.4.0 manipulateWidget_0.10.0
>> ?[31] assertthat_0.2.1??????? promises_1.0.1 phangorn_2.5.5
>> ?[34] rlang_0.4.0???????????? zeallot_0.1.0 scatterplot3d_0.3-41
>> ?[37] splines_3.6.2?????????? lazyeval_0.2.2 broom_0.5.2
>> ?[40] reshape2_1.4.3????????? modelr_0.1.5 crosstalk_1.0.0
>> ?[43] backports_1.1.4???????? httpuv_1.5.1 tensorA_0.36.1
>> ?[46] tools_3.6.2???????????? spData_0.3.2 cubature_2.0.3
>> ?[49] Rcpp_1.0.1????????????? progress_1.2.2 classInt_0.3-3
>> ?[52] purrr_0.3.2???????????? prettyunits_1.0.2 haven_2.1.1
>> ?[55] ggrepel_0.8.1?????????? cluster_2.1.0 magrittr_1.5
>> ?[58] data.table_1.12.2?????? gmodels_2.18.1 sjmisc_2.8.3
>> ?[61] hms_0.5.0?????????????? mime_0.7 xtable_1.8-4
>> ?[64] XML_3.98-1.20?????????? sjstats_0.17.9 mclust_5.4.4
>> ?[67] ggeffects_0.14.1??????? compiler_3.6.2 tibble_2.1.3
>> ?[70] KernSmooth_2.23-16????? crayon_1.3.4 R.oo_1.22.0
>> ?[73] minqa_1.2.4???????????? htmltools_0.3.6 mgcv_1.8-31
>> ?[76] corpcor_1.6.9?????????? later_0.8.0 spdep_1.1-3
>> ?[79] tidyr_1.0.2???????????? expm_0.999-4 sjlabelled_1.1.3
>> ?[82] sf_0.7-6??????????????? permute_0.9-5 R.methodsS3_1.7.1
>> ?[85] quadprog_1.5-7????????? gdata_2.18.0 insight_0.8.1
>> ?[88] igraph_1.2.4.1????????? forcats_0.4.0 pkgconfig_2.0.2
>> ?[91] flashClust_1.01-2?????? rncl_0.8.3 numDeriv_2016.8-1.1
>> ?[94] foreign_0.8-72????????? xml2_1.2.1 webshot_0.5.1
>> ?[97] estimability_1.3??????? stringr_1.4.0 digest_0.6.20
>> [100] parameters_0.5.0??????? vegan_2.5-6 fastmatch_1.1-0
>> [103] shiny_1.3.2???????????? gtools_3.8.1 nloptr_1.2.1
>> [106] lifecycle_0.1.0???????? jsonlite_1.6 seqinr_3.6-1
>> [109] viridisLite_0.3.0?????? pillar_1.4.2 lattice_0.20-38
>> [112] httr_1.4.0????????????? glue_1.3.1 bayestestR_0.5.2
>> [115] class_7.3-15??????????? stringi_1.4.3 performance_0.4.4
>> [118] dplyr_0.8.3???????????? e1071_1.7-2
>>
>> Le 18/02/2020 ? 12:05, Jarrod Hadfield a ?crit?:
>>>
>>> Hi Kamal,
>>>
>>> Can you post your sessionInfo()?
>>>
>>> As a work around, use this model
>>>
>>> model <- MCMCglmm(lD ~ 
>>> tactic*period*seasonality+complique_KF+lbody+lintdur+lnb.loc+lduration
>>> ???????????????????????????????????? , random = 
>>> ~sp_phylo+species2+phylo_pop+phylo_popY+phylo_pop_id + idh(SE):units,
>>> ???????????????????????????????????? , family = "gaussian"
>>> ???????????????????????????????????? , ginverse = list(sp_phylo = 
>>> inv.phylo$Ainv) # include a custom matrix for argument phylo
>>> ???????????????????????????????????? , prior = prior1
>>> ???????????????????????????????????? , data = Data
>>> ???????????????????????????????????? , nitt = 22e+04
>>> ???????????????????????????????????? , burnin = 20000
>>> ???????????????????????????????????? , thin = 100
>>> ???????????????????????????????????? , pr=TRUE)
>>>
>>> BUT make sure to fix the prior variance associated with the final 
>>> random effect term (idh(SE):units) to one. Its identical to the 
>>> model you've fitted, but the predict function should work.
>>>
>>> Cheers,
>>>
>>> Jarrod
>>>
>>> On 15/02/2020 22:57, Kamal Atmeh wrote:
>>>> model <- MCMCglmm(lD ~ 
>>>> tactic*period*seasonality+complique_KF+lbody+lintdur+lnb.loc+lduration
>>>> ???????????????????????????????????? , random = 
>>>> ~sp_phylo+species2+phylo_pop+phylo_popY+phylo_pop_id
>>>> ???????????????????????????????????? , family = "gaussian"
>>>> ???????????????????????????????????? , mev = SE^2 # error variance 
>>>> associated to each data point
>>>> ???????????????????????????????????? , ginverse = list(sp_phylo = 
>>>> inv.phylo$Ainv) # include a custom matrix for argument phylo
>>>> ???????????????????????????????????? , prior = prior1
>>>> ???????????????????????????????????? , data = Data
>>>> ???????????????????????????????????? , nitt = 22e+04
>>>> ???????????????????????????????????? , burnin = 20000
>>>> ???????????????????????????????????? , thin = 100
>>>> ???????????????????????????????????? , pr=TRUE) 
>>> The University of Edinburgh is a charitable body, registered in 
>>> Scotland, with registration number SC005336. 
>

	[[alternative HTML version deleted]]


From cj|r2020 @end|ng |rom gm@||@com  Wed Feb 19 15:16:44 2020
From: cj|r2020 @end|ng |rom gm@||@com (Marcos Paulo)
Date: Wed, 19 Feb 2020 11:16:44 -0300
Subject: [R-sig-ME] Inclusion of correlation structure glmer function of
 package lme4
Message-ID: <CADkd1W=szL6T8HkTbzeeHvjRK5AzSiMaoVQvGuYMCTia3+AF=g@mail.gmail.com>

Dear,

Good night.

I am using the glmer function of the lme4 package of R. My answer is
Bernoulli and my data are longitudinal.

In the bild Package, I have already found article using correlation
structures.

Is there any way I can use the glmer function and be able to include a
correlation structure? If it is not possible, could you tell me which
default is?

The code in R is only to facilitate the help.

## generalized linear mixed model
library(lme4)
(gm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),
              data = cbpp, family = binomial))

Thanks

Marcos Paulo

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Wed Feb 19 16:57:50 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Wed, 19 Feb 2020 16:57:50 +0100
Subject: [R-sig-ME] Inclusion of correlation structure glmer function of
 package lme4
In-Reply-To: <CADkd1W=szL6T8HkTbzeeHvjRK5AzSiMaoVQvGuYMCTia3+AF=g@mail.gmail.com>
References: <CADkd1W=szL6T8HkTbzeeHvjRK5AzSiMaoVQvGuYMCTia3+AF=g@mail.gmail.com>
Message-ID: <CAJuCY5yzz31FK+0wmT5djFNAAFEjiR8TGQLmTOi9nqA4z=girQ@mail.gmail.com>

Dear Marcos,

You cannot add correlation structures to an lme4 model like you can with
nlme models.

Have a look at the INLA or glmmTMB packages instead.

Best regards,

Thierry

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op wo 19 feb. 2020 om 15:15 schreef Marcos Paulo <cjfr2020 at gmail.com>:

> Dear,
>
> Good night.
>
> I am using the glmer function of the lme4 package of R. My answer is
> Bernoulli and my data are longitudinal.
>
> In the bild Package, I have already found article using correlation
> structures.
>
> Is there any way I can use the glmer function and be able to include a
> correlation structure? If it is not possible, could you tell me which
> default is?
>
> The code in R is only to facilitate the help.
>
> ## generalized linear mixed model
> library(lme4)
> (gm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd),
>               data = cbpp, family = binomial))
>
> Thanks
>
> Marcos Paulo
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From mew0099 @end|ng |rom @uburn@edu  Wed Feb 19 03:46:50 2020
From: mew0099 @end|ng |rom @uburn@edu (Matthew Wolak)
Date: Wed, 19 Feb 2020 02:46:50 +0000
Subject: [R-sig-ME] MCMCglmm Different Outputs for ginverse Inputs
In-Reply-To: <YTOPR0101MB1867B141AD57BF39012C3799BC1C0@YTOPR0101MB1867.CANPRD01.PROD.OUTLOOK.COM>
References: <YTOPR0101MB1867B141AD57BF39012C3799BC1C0@YTOPR0101MB1867.CANPRD01.PROD.OUTLOOK.COM>
Message-ID: <BYAPR19MB2808FB22631EDE5693C51597C0100@BYAPR19MB2808.namprd19.prod.outlook.com>

Hi Cameron,

Q1:
In your second model (`HW_model9.2b`) `animal` does not represent a term for the additive genetic effects for each level of "animal" (well plant, I take it), correlated with one another as described by the relatedness matrix (when you supply `ginverse = list(animal = Ainv...`). Because you dropped the ginverse list entry for animal, the model assumes the effects associated with each "animal" are independent (i.e., a diagonal matrix instead of the relatedness matrix).

Depending on whether you have repeated measures or not, this might be modelling the exact same effects as the residuals.

Perhaps with this second model you were trying to utilize the old (and I think less preferred) method of indicating which term represented the additive genetic variance by using the special `animal` label, but you *also need to give an object to the `pedigree` argument.


Q2:
MCMCglmm is constructing inverse Wishart priors for the variance components. The inverse Gamma is a special case of the inverse Wishart (when `V = 1`) with the shape and scale parameters equal to `nu / 2` (so in your example, `...nu=0.002` yields inverse Gamma shape and scale parameters of 0.001. I think you can think of the inverse Wishart as the multivariate generalization of the inverse Gamma. So different arguments for `nu` are just different inverse Wishart distributions.

However, what you have specified (e.g., because `alpha.V = 1000` along with `V=1, nu=1`) are priors for a parameter expanded model that essentially causes the model to use priors from a different distribution. What you have actually given to your model are marginal priors for the variances that are F-distributions (see Gelman 2006 <-- full reference in either a Hadfield paper or the course notes).

Best of luck, I would be excited to see whether you see dominance-by-environment interactions!

Sincerely,
Matthew

****************************************************
Matthew E. Wolak, Ph.D.
Assistant Professor
Department of Biological Sciences
Auburn University
306 Funchess Hall
Auburn, AL 36849, USA

http://qgevoeco.com
Email: matthew.wolak at auburn.edu
Tel: 334-844-9242


________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of Cameron So <cameron.so at mail.utoronto.ca>
Sent: Tuesday, February 18, 2020 4:31 PM
To: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: [R-sig-ME] MCMCglmm Different Outputs for ginverse Inputs

Hi everyone,

I am using MCMCglmm to estimate additive genetic variance, dominance, and maternal effects in a plant population when exposed to a new environment and normal conditions (different individuals but replicates of genotype). I have two questions, the first being long and the latter being short.

Q1:
Since I am trying to incorporate a dominance random effect in my model, I initially followed the tutorial in Wolak's (2012) paper [ see: https://doi.org/10.1111/j.2041-210X.2012.00213.x ], in which the inverse additive matrix is indicated (see below). However, after checking model diagnostics and ensuring convergence and no autocorrelation, I was observing very low variance for additive, dominance, and subsequently heritability estimates (<0.1 for h2) for numerous univariate trait models. Obviously, this is not what I expected as heritability for various fitness-related traits is usually between 0.1 - 0.7 according to the literature.

Recently, I ran a new model without indicating the inverse additive matrix and let MCMCglmm automatically create it. The output for this model (although there is some high correlation issues that I am currently fixing) had an heritability estimate ~0.7, greater than the previous model.

Would anyone know the reasoning behind these model output differences when the inverse additive matrix is indicated or not? It appears that excluding the Ainv in the model yields a more accurate estimation.

Below is the an example of the R code for the trait plant height with parameter expanded priors.

Shared Priors:

prior9.2 <- list(R = list(V = 1, nu = 0.002),
                 G = list(G1 = list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),
                          G2 = list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000),
                          G3 = list(V = 1, nu = 1, alpha.mu = 0, alpha.V = 1000)))

Model 1 Example: Including Ainv

Ainv <- inverseA(ped[, 1:3])$Ainv
Dinv <- makeD(ped, 1:3])$Dinv

HW_model9.2 <- MCMCglmm(height ~ plot, random = ~animal + animalDom + matID,
ginverse = list(animal = Ainv, animalDom = Dinv),
family = "gaussian", data = heated.height, prior = prior9.2,
nitt = 2100000, thin = 1000, burnin = 100000, verbose = T, pr = TRUE)

Model 2 Example: Excluding Ainv

listD <- makeD(ped)
Dinv <- listD$Dinv

HW_model9.2b <- MCMCglmm(height ~ plot, random = ~animal + animalDom + matID,

ginverse = list(animalDom = Dinv),

family = "gaussian", data = heated.height, prior = prior9.2,

nitt = 2100000, thin = 1000, burnin = 100000, verbose = T, pr = TRUE)



..

Q2: Regarding priors, I have a shorter question on the parameterization of V and nu for the Inverse Wishart prior. My understanding is that the Inverse Gamma (0.001, 0.001) prior follows (V=1, nu = 0.002). However, for the Inverse Wishart prior, different sources allude to different parameterizations of V and nu. For example, for a multivariate model, once source says it is V=diag(n), nu = n - 0.998 where n is the number of traits, while another source states that it is V=diag(n), nu = n. Would someone be able to clarify my confusion? My apologies as I am quite new to using MCMCglmm and setting appropriate priors.




______

Cameron So

Master's Student | Plant Evolutionary Responses to Climate Change | Weis Lab
Department of Ecology & Evolutionary Biology
University of Toronto

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From mdevoto @end|ng |rom @gro@ub@@@r  Thu Feb 20 20:38:51 2020
From: mdevoto @end|ng |rom @gro@ub@@@r (Mariano Devoto)
Date: Thu, 20 Feb 2020 16:38:51 -0300
Subject: [R-sig-ME] query on binomial glm of pollination experiment
Message-ID: <CAJRWjBYqp2GXeWxFVrB6ZBkWXS-DeH6n5yxj1OvE_aeL2deZhA@mail.gmail.com>

Dear all,

I am trying to assess the effect of five pollination treatments on a
plant's fruit set. The data set is not too good (few samples and unbalanced
-sadly we lost many field samples to vandalism-, quite a few zeros). I
counted the n of flowers that received each treatment and the number of
fruits at the end of the experiment. I am using a binomial model.
Furthermore, given that several (though not all) plants received all the
treatments I'd like to include a random factor to account for between-plant
variation. I tried models with and without a random factor but results
either look awkward (particularly post hoc comparisons in model M1) or the
model does not converge (M2).

I'll be grateful for any advice on how to reliably test the significance of
the treatments and do multiple pairwise comparisons.

Thanks in advance for your help!

Here is the code to load the data and perform the analysis. The data set is
read from a Google sheet.

#loading data and curating data
require(RCurl); require(lme4); require(multcomp)
my.file <- getURL("
https://docs.google.com/spreadsheets/d/e/2PACX-1vSu7V5v00cpQ_SFZq5ajS-RUPCgRYf0q249SW3rh9g2q87h7awYOsgBX4xIRDGWQff4Jy6IRnSLuaxH/pub?output=csv
")
sistrep <- read.csv(textConnection(my.file), head=T)
str(sistrep)
nofruits <- sistrep$flowers-sistrep$fruits #calculates n of failed
pollinations

#plot the data
prop <- sistrep$fruits/sistrep$flowers
boxplot(prop ~ treatment, data=sistrep, las=1, ylab="Fruit set (n fruits/n
flowers)", boxwex=0.6)

#and here are the models & post hoc comparisons
M0 <- glm(cbind(fruits, nofruits) ~ 1, data = sistrep, family = "binomial")
M1 <- glm(cbind(fruits, nofruits) ~ treatment, data = sistrep, family =
"binomial")
M2 <- glmer(cbind(fruits, nofruits) ~ treatment + (1|plant), data =
sistrep, family = "binomial")
summary(M0)
summary(M1)
anova(M0,M1)
summary(M2)

summary(glht(M1, mcp(treatment = "Tukey", interaction_average=F)))
---------------------------------------------
Dr. Mariano Devoto

Profesor Adjunto - C?tedra de Bot?nica General, Facultad de Agronom?a de la
UBA
Investigador Adjunto del CONICET

Av. San Mart?n 4453 - C1417DSE - C. A. de Buenos Aires - Argentina
+5411 5287-0069
*https://www.researchgate.net/profile/Mariano_Devoto
<https://www.researchgate.net/profile/Mariano_Devoto>*

	[[alternative HTML version deleted]]


From mo|||eebrook@ @end|ng |rom gm@||@com  Thu Feb 20 21:12:34 2020
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Thu, 20 Feb 2020 21:12:34 +0100
Subject: [R-sig-ME] query on binomial glm of pollination experiment
In-Reply-To: <CAJRWjBYqp2GXeWxFVrB6ZBkWXS-DeH6n5yxj1OvE_aeL2deZhA@mail.gmail.com>
References: <CAJRWjBYqp2GXeWxFVrB6ZBkWXS-DeH6n5yxj1OvE_aeL2deZhA@mail.gmail.com>
Message-ID: <6B72DBCC-253A-4156-8C53-81EACC2F7F7B@gmail.com>

Hi Mariano,

I don?t have much time right now, but I was able to improve the model (including convergence) with a few small changes. I think that having nofruits as a separate object may have been causing a problem in the inner-workings of the software. So I moved it to a column of sistrep. I also reordered the treatment levels so that control is the baseline. Then the model appears to run in lme4 with a warning or in glmmTMB without a warning. It looks like some of the coefficients differ between the two models.

sistrep$nofruits <- sistrep$flowers-sistrep$fruits 
sistrep$treatment=factor(sistrep$treatment, levels=c("control", "automan", "cruzman", "tul", "voile"))

library(glmmTMB)
M3 <- glmmTMB(cbind(fruits, nofruits) ~ treatment + (1|plant), data =
sistrep, family = "binomial")
summary(M3)

It could be possible that having better gradient information allows glmmTMB to converge in this case.

BTW, I couldn?t get the getURL command to work, but I downloaded the csv from the web address and read it in without any problems. This may help in case anyone else tries to help.

cheers,
Mollie

> On 20Feb 2020, at 20:38, Mariano Devoto <mdevoto at agro.uba.ar> wrote:
> 
> Dear all,
> 
> I am trying to assess the effect of five pollination treatments on a
> plant's fruit set. The data set is not too good (few samples and unbalanced
> -sadly we lost many field samples to vandalism-, quite a few zeros). I
> counted the n of flowers that received each treatment and the number of
> fruits at the end of the experiment. I am using a binomial model.
> Furthermore, given that several (though not all) plants received all the
> treatments I'd like to include a random factor to account for between-plant
> variation. I tried models with and without a random factor but results
> either look awkward (particularly post hoc comparisons in model M1) or the
> model does not converge (M2).
> 
> I'll be grateful for any advice on how to reliably test the significance of
> the treatments and do multiple pairwise comparisons.
> 
> Thanks in advance for your help!
> 
> Here is the code to load the data and perform the analysis. The data set is
> read from a Google sheet.
> 
> #loading data and curating data
> require(RCurl); require(lme4); require(multcomp)
> my.file <- getURL("
> https://docs.google.com/spreadsheets/d/e/2PACX-1vSu7V5v00cpQ_SFZq5ajS-RUPCgRYf0q249SW3rh9g2q87h7awYOsgBX4xIRDGWQff4Jy6IRnSLuaxH/pub?output=csv
> ")
> sistrep <- read.csv(textConnection(my.file), head=T)
> str(sistrep)
> nofruits <- sistrep$flowers-sistrep$fruits #calculates n of failed
> pollinations
> 
> #plot the data
> prop <- sistrep$fruits/sistrep$flowers
> boxplot(prop ~ treatment, data=sistrep, las=1, ylab="Fruit set (n fruits/n
> flowers)", boxwex=0.6)
> 
> #and here are the models & post hoc comparisons
> M0 <- glm(cbind(fruits, nofruits) ~ 1, data = sistrep, family = "binomial")
> M1 <- glm(cbind(fruits, nofruits) ~ treatment, data = sistrep, family =
> "binomial")
> M2 <- glmer(cbind(fruits, nofruits) ~ treatment + (1|plant), data =
> sistrep, family = "binomial")
> summary(M0)
> summary(M1)
> anova(M0,M1)
> summary(M2)
> 
> summary(glht(M1, mcp(treatment = "Tukey", interaction_average=F)))
> ---------------------------------------------
> Dr. Mariano Devoto
> 
> Profesor Adjunto - C?tedra de Bot?nica General, Facultad de Agronom?a de la
> UBA
> Investigador Adjunto del CONICET
> 
> Av. San Mart?n 4453 - C1417DSE - C. A. de Buenos Aires - Argentina
> +5411 5287-0069
> *https://www.researchgate.net/profile/Mariano_Devoto
> <https://www.researchgate.net/profile/Mariano_Devoto>*
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u  Fri Feb 21 01:56:11 2020
From: D@v|d@Du||y @end|ng |rom q|mrbergho|er@edu@@u (David Duffy)
Date: Fri, 21 Feb 2020 00:56:11 +0000
Subject: [R-sig-ME] query on binomial glm of pollination experiment
In-Reply-To: <6B72DBCC-253A-4156-8C53-81EACC2F7F7B@gmail.com>
References: <CAJRWjBYqp2GXeWxFVrB6ZBkWXS-DeH6n5yxj1OvE_aeL2deZhA@mail.gmail.com>,
 <6B72DBCC-253A-4156-8C53-81EACC2F7F7B@gmail.com>
Message-ID: <a7a2186132584c198a0bea9c382b7f24@qimrberghofer.edu.au>

> ________________________________________
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of Mollie Brooks 

> I don?t have much time right now, but I was able to improve the model (including convergence) with a few small changes. I 
> think that having nofruits as a separate object may have been causing a problem in the inner-workings of the software. 
> So I moved it to a column of sistrep. I also reordered the treatment levels so that control is the baseline. Then the model 
> appears to run in lme4 with a warning or in glmmTMB without a warning. It looks like some of the coefficients differ 
> between the two models.

> sistrep$nofruits <- sistrep$flowers-sistrep$fruits
> sistrep$treatment=factor(sistrep$treatment, levels=c("control", "automan", "cruzman", "tul", "voile"))

> library(glmmTMB)
> M3 <- glmmTMB(cbind(fruits, nofruits) ~ treatment + (1|plant), data =
> sistrep, family = "binomial")
> summary(M3)

>> M2 <- glmer(cbind(fruits, nofruits) ~ treatment + (1|plant), data =
> > sistrep, family = "binomial")

I just ran stan_glmer(), on the off-chance it would be better behaved. 
library(rstanarm)
M4 <- stan_glmer(cbind(fruits, flowers-fruits) ~ treatment + (1|plant), data=x, family=binomial())

The treatment estimates (and SEs) were very similar to those from glmer(), except for "automan", which I presume is what makes glmer unhappy. 


From ph||||p@@|d@y @end|ng |rom mp|@n|  Fri Feb 21 13:55:54 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Fri, 21 Feb 2020 13:55:54 +0100
Subject: [R-sig-ME] query on binomial glm of pollination experiment
In-Reply-To: <49739d06-4e1e-cce0-050a-6415991a1bbd@mpi.nl>
References: <CAJRWjBYqp2GXeWxFVrB6ZBkWXS-DeH6n5yxj1OvE_aeL2deZhA@mail.gmail.com>
 <6B72DBCC-253A-4156-8C53-81EACC2F7F7B@gmail.com>
 <a7a2186132584c198a0bea9c382b7f24@qimrberghofer.edu.au>
 <49739d06-4e1e-cce0-050a-6415991a1bbd@mpi.nl>
Message-ID: <b88c615f-e4ce-5d0f-fee3-be54c126c999@mpi.nl>

And now again without PGP signature ....

Phillip



Hi all,

As the OP noted, part of the problem here is that it's not much data for
the model you want to estimate: 38 observations over 10 groups for one
variance component and five coefficients. For automan, you only have 4
observations and from the boxplot that category seems to behave quite
differently both in terms of mean and internal variability than the
other ones, so the huge standard errors are to be expected.

lme4 and glmmTMB actually deliver essentially the same model:

R> logLik(M3) #glmmTMB
'log Lik.' -117.8211 (df=6)

R> logLik(M2) # lm4 with the nlopt optimizer
'log Lik.' -117.8212 (df=6)

The gradient is part of the problem, but not in the sense of
convergence, but rather convergence tests and, to a lesser extent,
computation of standard errors. lme4 depends on finite-differences
approximations for the gradient and Hessian checks, which are computed
*after* gradient-free optimization, but glmmTMB uses the magic of
autodiff to have a much approximation to the gradient for both
optimization and tests.

(Note the lme4 warning about the finite-differences approximation
failing and falling back to a different estimate.)

If I recall correctly, rstanarm uses lightly regularizing priors (normal
with large sd), so you have something closer to L2-penalized (ridge)
regression. This bit of regularization then gets you better stability
and hence smaller standard errors for coefficients corresponding to few
observations. And of course, the convergence checks for MCMC don't
depend on gradients, even if Stan's HMC uses gradients to compute the
actual chains.

Fitting the same model in MixedModels.jl, we get essentially the same fit:

# sistrep$fruits_prop = with(sistrep, fruits / flowers)

julia> M4 = fit(MixedModel, @formula(fruits_prop ~ 1 + treatment +
(1|plant)), sistrep, Binomial(), LogitLink(),
wts=float.(sistrep.flowers), fast=false);

julia> loglikelihood(M4)

-117.82110003668843

Just for fun, we can crank up the quadrature points in Julia and see if
that changes anything

julia> @time M5 = fit(MixedModel, @formula(fruits_prop ~ 1 + treatment +
(1|plant)), sistrep,
Binomial(), LogitLink(), wts=float.(sistrep.flowers), fast=false,
nAGQ=9);
  0.899799 seconds (1.59 M allocations: 81.112 MiB, 5.96% gc
time)



julia>
loglikelihood(M5)


-117.82121127148437







julia> coef(M4) -
coef(M5)
5-element
Array{Float64,1}:


 6.14676620522836e-5


 0.4511892163672222


 2.2938186144294548e-5


 2.1950652206226273e-5


 9.892031197700213e-5

Only automan really changes and that change is minuscule compared to the
standard error.

My final comment would be: there is nothing wrong with this model per
se, but

1. You're always limited by your data, and with limited data, there's
not much to be done, unfortunately.

2. Regularization is the classic answer to helping out with such
problems. One way to achieve this is of course with priors in the
Bayesian framework. Note however that in your case that the estimates
essentially stay the same and only the error changes.

Best,

Phillip


On 21/02/2020 01:56, David Duffy wrote:
> I don?t have much time right now, but I was able to improve the model
> (including convergence) with a few small changes. I
>> think that having nofruits as a separate object may have been causing
a problem in the inner-workings of the software.
>> So I moved it to a column of sistrep. I also reordered the treatment
levels so that control is the baseline. Then the model
>> appears to run in lme4 with a warning or in glmmTMB without a
warning. It looks like some of the coefficients differ
>> between the two models.
>> sistrep$nofruits <- sistrep$flowers-sistrep$fruits
>> sistrep$treatment=factor(sistrep$treatment, levels=c("control",
"automan", "cruzman", "tul", "voile"))
>> library(glmmTMB)
>> M3 <- glmmTMB(cbind(fruits, nofruits) ~ treatment + (1|plant), data =
>> sistrep, family = "binomial")
>> summary(M3)
>>> M2 <- glmer(cbind(fruits, nofruits) ~ treatment + (1|plant), data =
>>> sistrep, family = "binomial")
> I just ran stan_glmer(), on the off-chance it would be better behaved.
> library(rstanarm)
> M4 <- stan_glmer(cbind(fruits, flowers-fruits) ~ treatment +
(1|plant), data=x, family=binomial())
>
> The treatment estimates (and SEs) were very similar to those from
glmer(), except for "automan", which I presume is what makes glmer unhappy.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From j@de@ @end|ng |rom he@|th@uc@d@edu  Wed Feb 26 06:19:09 2020
From: j@de@ @end|ng |rom he@|th@uc@d@edu (Ades, James)
Date: Wed, 26 Feb 2020 05:19:09 +0000
Subject: [R-sig-ME] Most principled reporting of mixed-effect model
 regression coefficients
In-Reply-To: <CAHr4DycFj9omcOSFYvBZ3jSfdzTeAH2HV_Bwr4g5d=Es2wYY3w@mail.gmail.com>
References: <DM5PR1901MB20072EB8F9BE9DBE5E82002BEA150@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <CAJuCY5xhVYBgZwfwmb-gO9=YYhkg7RA7B4dDjo1_xp_xX6di5g@mail.gmail.com>
 <DM5PR1901MB2007B1C5BF040057769C97D7EA140@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <000001d5e410$a5919990$f0b4ccb0$@uke.de>
 <27250084e9e148ba860e21e55ca4f53e@MSX-L104.msx.ad.zih.tu-dresden.de>
 <CAHr4Dyc7PXYWPax8Zzgvi+-FDmCUoWG=8-RyXDvjNb+Lau303w@mail.gmail.com>
 <e758416e0c78493eb4ed4b98b8080a02@MSX-L104.msx.ad.zih.tu-dresden.de>,
 <CAHr4DycFj9omcOSFYvBZ3jSfdzTeAH2HV_Bwr4g5d=Es2wYY3w@mail.gmail.com>
Message-ID: <DM5PR1901MB20077BF7D6CF8046F21D557DEAEA0@DM5PR1901MB2007.namprd19.prod.outlook.com>

Thanks, Daniel and Maarten!

I looked at both Nakagawa and Schielzeth and the Johnson paper; I also looked through your other references...thanks for those. I really liked the linked Stack Exchange post of WHuber's lucid response to R^2.

Johnson references the MuMIn package, which I wasn't familiar with, though he writes that the function "r.squaredGLMM" takes into account the random slope (something that N & S mention as tedious and then wave aside). Using the N&S equation, for one of my models, I get an R^2 of .35, while using r.squaredGLMM, I get an R^2 of .43. I can't imagine that the random slope of time would make that big of a difference. (The conditional R^2 is .95, and I have no idea how it's that high). Does anyone have any experience with the package?

While some models (not for model selection but looking at PCA, individual variables, or some kind of aggregate measure for executive function)  have comparatively large differences in AIC; using R^2 via MuMIn, they might have differences of .01. In other words, what seemed to be decent (and significant with LRT) differences, with r.squaredGLMM they became inconsequential.

AIC seems to do a commendable job of yielding parsimony, but it's utter lack of comparability (with same ?# of observations) is frustrating. While an AIC of 28,620 is better than one with 28,645, there is, to my knowledge, no real way of quantifying that difference. Alas, while WHuber writes, "Most of the time you can find a better statistic than R^2. For model selection you  can look to AIC and BIC," I think the
issue is not only in selecting models (which AIC seems to do quite well), but again, in
summarizing those models in intuitively quantitative ways.

I've also looked into doing some kind of multiple time series cross validation
though from what I've read (see below), this is similarly fraught. Maybe leave one out is
the best way to go. The structure of the data has four timepoints with executive function
data. The first two timepoints ('17 school year) and the final two timepoints ('18 school year)
correspond to each year's standardized test.

Thanks much!



http://www.stat.columbia.edu/~gelman/research/published/final_sub.pdf
Di culty of selecting among multilevel models using predictive accuracy<http://www.stat.columbia.edu/~gelman/research/published/final_sub.pdf>
Statistics and Its Interface Volume 7 (2014) 1 Di culty of selecting among multilevel models using predictive accuracy Wei Wang and Andrew Gelman
www.stat.columbia.edu

https://dl.acm.org/doi/10.1016/j.ins.2011.12.028
On the use of cross-validation for time series predictor evaluation | Information Sciences: an International Journal<https://dl.acm.org/doi/10.1016/j.ins.2011.12.028>
In time series predictor evaluation, we observe that with respect to the model selection procedure there is a gap between evaluation of traditional forecasting procedures, on the one hand, and evaluation of machine learning techniques on the other hand.
dl.acm.org


https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.02881
[https://onlinelibrary.wiley.com/cms/asset/e2a4b565-bd18-485c-8dff-a03abb9a0c13/ecog.2017.v40.i8.cover.gif]<https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.02881>
Cross?validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure - Roberts - 2017 - Ecography - Wiley Online Library<https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.02881>
Ideally, model validation, selection, and predictive errors should be calculated using independent data (Ara?jo et al. 2005).For example, validation may be undertaken with data from different geographic regions or spatially distinct subsets of the region, different time periods, such as historic species records from the recent past or from fossil records.
onlinelibrary.wiley.com



________________________________
From: Maarten Jung <Maarten.Jung at mailbox.tu-dresden.de>
Sent: Monday, February 17, 2020 1:35 AM
To: Ades, James <jades at health.ucsd.edu>
Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients

> Thanks, Maarten. So I was planning on reporting R^2 (along with AIC) for the overall model fit, not for each predictor, since the regression coefficients themselves give a good indication of relationship (though I wasn't aware that R^2 is "riddled with complications") Is Henrik only saying this only with regard to LMMs and GLMMs?

That makes sense to me. For the overall model fit I would probably
still go with Johnson's version [1] which I describe in my
StackExchange post (and I think you mentioned it, or the Nakagawa and
Schielzeth version it is based on, earlier) and report both the
marginal and conditional R^2 values. The regression coefficients
provide unstandardized effect sizes on the response scale which I
think are a valid way to report effect sizes (see below).
I think Henrik refers to (G)LMMs and gives Rights & Sterba (2019) [2]
as reference. Also, the GLMM FAQ website provides a good overview [3].

> When you say "there is no agreed upon way to calculate effect sizes" I'm a little confused. I read through your stack exchange posting, but Henrik's answer refers to standardized effect size. You write, later down, "Whenever possible, we report unstandardized effect sizes which is in line with general recommendation of how to report effect sizes"

What you cite is still Henrik's opinion (and I hoped that I could make
this clear by writing "This is what he suggests [...]" and by using
the <blockquote> on StackExchange). And your citation still refers to
LMMs as he says "Unfortunately, due to the way that variance is
partitioned in linear mixed models (e.g., Rights & Sterba, 2019),
there does not exist an agreed upon way to calculate standard effect
sizes for individual model terms such as main effects or
interactions."
In general, I agree with him and with his recommendation to report
unstandardized effect sizes (e.g. regression coefficients) if they
have a "meaningful" interpretation.
The semi-partial R^2 I mentioned in my last e-mail is an
additional/alternative indicator of effect sizes that is probably more
in line with what psychologists are used to see reported in papers
(especially when results of factorial designs are reported) - and
that's the reason I mentioned it.

> I'm also working on a systematic review where there's disagreement over whether effect sizes should be standardized, but it does seem that yield any kind of meaningful comparison, effect sizes would have to be standardized. I don't usually report standardized effect sizes...however, there are times when I z-score IVs to put them on the same scale, and I guess the output of that would be a standardized effect size. I wasn't aware of push back on that practice. What issues would arise from this?

There is nothing wrong with standardizing (e.g. by diving by 1 or 2
standard deviations) predictor variables to get measures of variable
importance (within the same model).
Issues arise when standardized effect sizes such as R^2, partial
eta^2, etc. between different models are compared without thinking
about what differences in these measures can be attributed to (see
e.g. this question [4] or the Pek & Flora (2018) paper [5] that Henrik
cites). Note that these are general issues that apply to all
regression models, not only mixed models.

[1] https://doi.org/10.1111/2041-210X.12225
[2] https://doi.org/10.1037/met0000184
[3] https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#how-do-i-compute-a-coefficient-of-determination-r2-or-an-analogue-for-glmms
[4] https://stats.stackexchange.com/questions/13314/is-r2-useful-or-dangerous/13317
[5] https://doi.org/10.1037/met0000126

Best,
Maarten

	[[alternative HTML version deleted]]


From p@u|@john@on @end|ng |rom g|@@gow@@c@uk  Wed Feb 26 10:28:30 2020
From: p@u|@john@on @end|ng |rom g|@@gow@@c@uk (Paul Johnson)
Date: Wed, 26 Feb 2020 09:28:30 +0000
Subject: [R-sig-ME] Most principled reporting of mixed-effect model
 regression coefficients
In-Reply-To: <DM5PR1901MB20077BF7D6CF8046F21D557DEAEA0@DM5PR1901MB2007.namprd19.prod.outlook.com>
References: <DM5PR1901MB20072EB8F9BE9DBE5E82002BEA150@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <CAJuCY5xhVYBgZwfwmb-gO9=YYhkg7RA7B4dDjo1_xp_xX6di5g@mail.gmail.com>
 <DM5PR1901MB2007B1C5BF040057769C97D7EA140@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <000001d5e410$a5919990$f0b4ccb0$@uke.de>
 <27250084e9e148ba860e21e55ca4f53e@MSX-L104.msx.ad.zih.tu-dresden.de>
 <CAHr4Dyc7PXYWPax8Zzgvi+-FDmCUoWG=8-RyXDvjNb+Lau303w@mail.gmail.com>
 <e758416e0c78493eb4ed4b98b8080a02@MSX-L104.msx.ad.zih.tu-dresden.de>
 <CAHr4DycFj9omcOSFYvBZ3jSfdzTeAH2HV_Bwr4g5d=Es2wYY3w@mail.gmail.com>
 <DM5PR1901MB20077BF7D6CF8046F21D557DEAEA0@DM5PR1901MB2007.namprd19.prod.outlook.com>
Message-ID: <D73963FD-4B0A-4A7C-B8FF-3B2325958797@glasgow.ac.uk>

Hi James, 

Jon Lefcheck's piecewiseSEM package has a good R-squared function (piecewiseSEM::rsquared) that implements the N&S marginal and conditional R-squared for GLMMs, and includes the extension to random slopes. This is the best supported and most up-to-date implementation that I?m aware of. There are other packages I?m less familiar with (e.g. r2glmm).

All the best,
Paul


> On 26 Feb 2020, at 05:19, Ades, James <jades at health.ucsd.edu> wrote:
> 
> Thanks, Daniel and Maarten!
> 
> I looked at both Nakagawa and Schielzeth and the Johnson paper; I also looked through your other references...thanks for those. I really liked the linked Stack Exchange post of WHuber's lucid response to R^2.
> 
> Johnson references the MuMIn package, which I wasn't familiar with, though he writes that the function "r.squaredGLMM" takes into account the random slope (something that N & S mention as tedious and then wave aside). Using the N&S equation, for one of my models, I get an R^2 of .35, while using r.squaredGLMM, I get an R^2 of .43. I can't imagine that the random slope of time would make that big of a difference. (The conditional R^2 is .95, and I have no idea how it's that high). Does anyone have any experience with the package?
> 
> While some models (not for model selection but looking at PCA, individual variables, or some kind of aggregate measure for executive function)  have comparatively large differences in AIC; using R^2 via MuMIn, they might have differences of .01. In other words, what seemed to be decent (and significant with LRT) differences, with r.squaredGLMM they became inconsequential.
> 
> AIC seems to do a commendable job of yielding parsimony, but it's utter lack of comparability (with same ?# of observations) is frustrating. While an AIC of 28,620 is better than one with 28,645, there is, to my knowledge, no real way of quantifying that difference. Alas, while WHuber writes, "Most of the time you can find a better statistic than R^2. For model selection you  can look to AIC and BIC," I think the
> issue is not only in selecting models (which AIC seems to do quite well), but again, in
> summarizing those models in intuitively quantitative ways.
> 
> I've also looked into doing some kind of multiple time series cross validation
> though from what I've read (see below), this is similarly fraught. Maybe leave one out is
> the best way to go. The structure of the data has four timepoints with executive function
> data. The first two timepoints ('17 school year) and the final two timepoints ('18 school year)
> correspond to each year's standardized test.
> 
> Thanks much!
> 
> 
> 
> http://www.stat.columbia.edu/~gelman/research/published/final_sub.pdf
> Di culty of selecting among multilevel models using predictive accuracy<http://www.stat.columbia.edu/~gelman/research/published/final_sub.pdf>
> Statistics and Its Interface Volume 7 (2014) 1 Di culty of selecting among multilevel models using predictive accuracy Wei Wang and Andrew Gelman
> www.stat.columbia.edu
> 
> https://dl.acm.org/doi/10.1016/j.ins.2011.12.028
> On the use of cross-validation for time series predictor evaluation | Information Sciences: an International Journal<https://dl.acm.org/doi/10.1016/j.ins.2011.12.028>
> In time series predictor evaluation, we observe that with respect to the model selection procedure there is a gap between evaluation of traditional forecasting procedures, on the one hand, and evaluation of machine learning techniques on the other hand.
> dl.acm.org
> 
> 
> https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.02881
> [https://onlinelibrary.wiley.com/cms/asset/e2a4b565-bd18-485c-8dff-a03abb9a0c13/ecog.2017.v40.i8.cover.gif]<https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.02881>
> Cross?validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure - Roberts - 2017 - Ecography - Wiley Online Library<https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.02881>
> Ideally, model validation, selection, and predictive errors should be calculated using independent data (Ara?jo et al. 2005).For example, validation may be undertaken with data from different geographic regions or spatially distinct subsets of the region, different time periods, such as historic species records from the recent past or from fossil records.
> onlinelibrary.wiley.com
> 
> 
> 
> ________________________________
> From: Maarten Jung <Maarten.Jung at mailbox.tu-dresden.de>
> Sent: Monday, February 17, 2020 1:35 AM
> To: Ades, James <jades at health.ucsd.edu>
> Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients
> 
>> Thanks, Maarten. So I was planning on reporting R^2 (along with AIC) for the overall model fit, not for each predictor, since the regression coefficients themselves give a good indication of relationship (though I wasn't aware that R^2 is "riddled with complications") Is Henrik only saying this only with regard to LMMs and GLMMs?
> 
> That makes sense to me. For the overall model fit I would probably
> still go with Johnson's version [1] which I describe in my
> StackExchange post (and I think you mentioned it, or the Nakagawa and
> Schielzeth version it is based on, earlier) and report both the
> marginal and conditional R^2 values. The regression coefficients
> provide unstandardized effect sizes on the response scale which I
> think are a valid way to report effect sizes (see below).
> I think Henrik refers to (G)LMMs and gives Rights & Sterba (2019) [2]
> as reference. Also, the GLMM FAQ website provides a good overview [3].
> 
>> When you say "there is no agreed upon way to calculate effect sizes" I'm a little confused. I read through your stack exchange posting, but Henrik's answer refers to standardized effect size. You write, later down, "Whenever possible, we report unstandardized effect sizes which is in line with general recommendation of how to report effect sizes"
> 
> What you cite is still Henrik's opinion (and I hoped that I could make
> this clear by writing "This is what he suggests [...]" and by using
> the <blockquote> on StackExchange). And your citation still refers to
> LMMs as he says "Unfortunately, due to the way that variance is
> partitioned in linear mixed models (e.g., Rights & Sterba, 2019),
> there does not exist an agreed upon way to calculate standard effect
> sizes for individual model terms such as main effects or
> interactions."
> In general, I agree with him and with his recommendation to report
> unstandardized effect sizes (e.g. regression coefficients) if they
> have a "meaningful" interpretation.
> The semi-partial R^2 I mentioned in my last e-mail is an
> additional/alternative indicator of effect sizes that is probably more
> in line with what psychologists are used to see reported in papers
> (especially when results of factorial designs are reported) - and
> that's the reason I mentioned it.
> 
>> I'm also working on a systematic review where there's disagreement over whether effect sizes should be standardized, but it does seem that yield any kind of meaningful comparison, effect sizes would have to be standardized. I don't usually report standardized effect sizes...however, there are times when I z-score IVs to put them on the same scale, and I guess the output of that would be a standardized effect size. I wasn't aware of push back on that practice. What issues would arise from this?
> 
> There is nothing wrong with standardizing (e.g. by diving by 1 or 2
> standard deviations) predictor variables to get measures of variable
> importance (within the same model).
> Issues arise when standardized effect sizes such as R^2, partial
> eta^2, etc. between different models are compared without thinking
> about what differences in these measures can be attributed to (see
> e.g. this question [4] or the Pek & Flora (2018) paper [5] that Henrik
> cites). Note that these are general issues that apply to all
> regression models, not only mixed models.
> 
> [1] https://doi.org/10.1111/2041-210X.12225
> [2] https://doi.org/10.1037/met0000184
> [3] https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#how-do-i-compute-a-coefficient-of-determination-r2-or-an-analogue-for-glmms
> [4] https://stats.stackexchange.com/questions/13314/is-r2-useful-or-dangerous/13317
> [5] https://doi.org/10.1037/met0000126
> 
> Best,
> Maarten
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From d@|uedecke @end|ng |rom uke@de  Wed Feb 26 16:49:54 2020
From: d@|uedecke @end|ng |rom uke@de (=?utf-8?Q?Daniel_L=C3=BCdecke?=)
Date: Wed, 26 Feb 2020 16:49:54 +0100
Subject: [R-sig-ME] Most principled reporting of mixed-effect model
 regression coefficients
In-Reply-To: <DM5PR1901MB20077BF7D6CF8046F21D557DEAEA0@DM5PR1901MB2007.namprd19.prod.outlook.com>
References: <DM5PR1901MB20072EB8F9BE9DBE5E82002BEA150@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <CAJuCY5xhVYBgZwfwmb-gO9=YYhkg7RA7B4dDjo1_xp_xX6di5g@mail.gmail.com>
 <DM5PR1901MB2007B1C5BF040057769C97D7EA140@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <000001d5e410$a5919990$f0b4ccb0$@uke.de>
 <27250084e9e148ba860e21e55ca4f53e@MSX-L104.msx.ad.zih.tu-dresden.de>
 <CAHr4Dyc7PXYWPax8Zzgvi+-FDmCUoWG=8-RyXDvjNb+Lau303w@mail.gmail.com>
 <e758416e0c78493eb4ed4b98b8080a02@MSX-L104.msx.ad.zih.tu-dresden.de>,
 <CAHr4DycFj9omcOSFYvBZ3jSfdzTeAH2HV_Bwr4g5d=Es2wYY3w@mail.gmail.com>
 <DM5PR1901MB20077BF7D6CF8046F21D557DEAEA0@DM5PR1901MB2007.namprd19.prod.outlook.com>
Message-ID: <001a01d5ecbc$63c884b0$2b598e10$@uke.de>

Hi James,

> Using the N&S equation, for one of my models, I get an R^2 of .35, while using r.squaredGLMM, I get an R^2 of .43. I can't imagine that the random slope of time would make that big of a difference. (The conditional R^2 is .95, and I have no idea how it's that high). 

The conditional R2 is usually higher, in case of random slopes probably much higher. You can try to compare your results with performance::r2_nakagawa(), piecewiseSEM::rsquared() and MuMIn::r.squaredGLMM() (see also next answer below).

> Does anyone have any experience with the package?

In most cases, these packages yield similar to identical results. There are, however, certain situations where the one or other package might fit better to your need. In my experience, these functions differ in following points

- r2_nakagawa() only uses log-approximation, while rsquared() and r.squaredGLMM() also yield the results from the delta-approximation.
- all three functions yield consistent results for linear mixed models, including random slopes
- rsquared() fails for binomial models where the response is cbind(trials, success) or similar
- r2_nakagawa() and r.squaredGLMM() can handle models from package glmmTMB
- all three functions yield different results for "glmer(count ~ spp + mined + (1 | site), family = poisson, data = Salamanders)"
- currently, only r2_nakagawa() supports glmmTMB with zero-inflation
- r2_nakagawa() probably supports more packages (like GLMMadaptive or afex)


> Most of the time you can find a better statistic than R^2

I personally would recommend to look at different aspects, not only "fit measures", but also which model makes theoretically more sense. Furthermore, it depends on what you're focussing. If you're interested in the variance of your random effects, i.e. how much variation do you have in your outcome depending on different groups/clusters/subjects, comparing the conditional and marginal R2 makes more sense than looking at AIC (you could also directly look at the ICC in such a situation, which goes into a similar direction as marginal & conditional R2). Also, AIC is most useful when comparing models, but it gives you less information about a certain model itself, while R2 is indeed a useful measure for a single model.

Best
Daniel

-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org] Im Auftrag von Ades, James
Gesendet: Mittwoch, 26. Februar 2020 06:19
An: Maarten Jung <Maarten.Jung at mailbox.tu-dresden.de>
Cc: r-sig-mixed-models at r-project.org
Betreff: Re: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients

Thanks, Daniel and Maarten!

I looked at both Nakagawa and Schielzeth and the Johnson paper; I also looked through your other references...thanks for those. I really liked the linked Stack Exchange post of WHuber's lucid response to R^2.

Johnson references the MuMIn package, which I wasn't familiar with, though he writes that the function "r.squaredGLMM" takes into account the random slope (something that N & S mention as tedious and then wave aside). Using the N&S equation, for one of my models, I get an R^2 of .35, while using r.squaredGLMM, I get an R^2 of .43. I can't imagine that the random slope of time would make that big of a difference. (The conditional R^2 is .95, and I have no idea how it's that high). Does anyone have any experience with the package?

While some models (not for model selection but looking at PCA, individual variables, or some kind of aggregate measure for executive function)  have comparatively large differences in AIC; using R^2 via MuMIn, they might have differences of .01. In other words, what seemed to be decent (and significant with LRT) differences, with r.squaredGLMM they became inconsequential.

AIC seems to do a commendable job of yielding parsimony, but it's utter lack of comparability (with same ?# of observations) is frustrating. While an AIC of 28,620 is better than one with 28,645, there is, to my knowledge, no real way of quantifying that difference. Alas, while WHuber writes, "Most of the time you can find a better statistic than R^2. For model selection you  can look to AIC and BIC," I think the issue is not only in selecting models (which AIC seems to do quite well), but again, in summarizing those models in intuitively quantitative ways.

I've also looked into doing some kind of multiple time series cross validation though from what I've read (see below), this is similarly fraught. Maybe leave one out is the best way to go. The structure of the data has four timepoints with executive function data. The first two timepoints ('17 school year) and the final two timepoints ('18 school year) correspond to each year's standardized test.

Thanks much!



http://www.stat.columbia.edu/~gelman/research/published/final_sub.pdf
Di culty of selecting among multilevel models using predictive accuracy<http://www.stat.columbia.edu/~gelman/research/published/final_sub.pdf>
Statistics and Its Interface Volume 7 (2014) 1 Di culty of selecting among multilevel models using predictive accuracy Wei Wang and Andrew Gelman www.stat.columbia.edu

https://dl.acm.org/doi/10.1016/j.ins.2011.12.028
On the use of cross-validation for time series predictor evaluation | Information Sciences: an International Journal<https://dl.acm.org/doi/10.1016/j.ins.2011.12.028>
In time series predictor evaluation, we observe that with respect to the model selection procedure there is a gap between evaluation of traditional forecasting procedures, on the one hand, and evaluation of machine learning techniques on the other hand.
dl.acm.org


https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.02881
[https://onlinelibrary.wiley.com/cms/asset/e2a4b565-bd18-485c-8dff-a03abb9a0c13/ecog.2017.v40.i8.cover.gif]<https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.02881>
Cross?validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure - Roberts - 2017 - Ecography - Wiley Online Library<https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.02881>
Ideally, model validation, selection, and predictive errors should be calculated using independent data (Ara?jo et al. 2005).For example, validation may be undertaken with data from different geographic regions or spatially distinct subsets of the region, different time periods, such as historic species records from the recent past or from fossil records.
onlinelibrary.wiley.com



________________________________
From: Maarten Jung <Maarten.Jung at mailbox.tu-dresden.de>
Sent: Monday, February 17, 2020 1:35 AM
To: Ades, James <jades at health.ucsd.edu>
Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients

> Thanks, Maarten. So I was planning on reporting R^2 (along with AIC) for the overall model fit, not for each predictor, since the regression coefficients themselves give a good indication of relationship (though I wasn't aware that R^2 is "riddled with complications") Is Henrik only saying this only with regard to LMMs and GLMMs?

That makes sense to me. For the overall model fit I would probably still go with Johnson's version [1] which I describe in my StackExchange post (and I think you mentioned it, or the Nakagawa and Schielzeth version it is based on, earlier) and report both the marginal and conditional R^2 values. The regression coefficients provide unstandardized effect sizes on the response scale which I think are a valid way to report effect sizes (see below).
I think Henrik refers to (G)LMMs and gives Rights & Sterba (2019) [2] as reference. Also, the GLMM FAQ website provides a good overview [3].

> When you say "there is no agreed upon way to calculate effect sizes" I'm a little confused. I read through your stack exchange posting, but Henrik's answer refers to standardized effect size. You write, later down, "Whenever possible, we report unstandardized effect sizes which is in line with general recommendation of how to report effect sizes"

What you cite is still Henrik's opinion (and I hoped that I could make this clear by writing "This is what he suggests [...]" and by using the <blockquote> on StackExchange). And your citation still refers to LMMs as he says "Unfortunately, due to the way that variance is partitioned in linear mixed models (e.g., Rights & Sterba, 2019), there does not exist an agreed upon way to calculate standard effect sizes for individual model terms such as main effects or interactions."
In general, I agree with him and with his recommendation to report unstandardized effect sizes (e.g. regression coefficients) if they have a "meaningful" interpretation.
The semi-partial R^2 I mentioned in my last e-mail is an additional/alternative indicator of effect sizes that is probably more in line with what psychologists are used to see reported in papers (especially when results of factorial designs are reported) - and that's the reason I mentioned it.

> I'm also working on a systematic review where there's disagreement over whether effect sizes should be standardized, but it does seem that yield any kind of meaningful comparison, effect sizes would have to be standardized. I don't usually report standardized effect sizes...however, there are times when I z-score IVs to put them on the same scale, and I guess the output of that would be a standardized effect size. I wasn't aware of push back on that practice. What issues would arise from this?

There is nothing wrong with standardizing (e.g. by diving by 1 or 2 standard deviations) predictor variables to get measures of variable importance (within the same model).
Issues arise when standardized effect sizes such as R^2, partial eta^2, etc. between different models are compared without thinking about what differences in these measures can be attributed to (see e.g. this question [4] or the Pek & Flora (2018) paper [5] that Henrik cites). Note that these are general issues that apply to all regression models, not only mixed models.

[1] https://doi.org/10.1111/2041-210X.12225
[2] https://doi.org/10.1037/met0000184
[3] https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#how-do-i-compute-a-coefficient-of-determination-r2-or-an-analogue-for-glmms
[4] https://stats.stackexchange.com/questions/13314/is-r2-useful-or-dangerous/13317
[5] https://doi.org/10.1037/met0000126

Best,
Maarten

	[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING

From c@meron@@o @end|ng |rom m@||@utoronto@c@  Thu Feb 27 23:04:35 2020
From: c@meron@@o @end|ng |rom m@||@utoronto@c@ (Cameron So)
Date: Thu, 27 Feb 2020 22:04:35 +0000
Subject: [R-sig-ME] MCMCglmm Prior for a Binary Trait with a Random
 Interaction w/ "animal"
Message-ID: <YTOPR0101MB18679399DC307C4DCE450E91BCEB0@YTOPR0101MB1867.CANPRD01.PROD.OUTLOOK.COM>

Hi all,

I am trying to measure the genetic variance in plasticity when different plant genotypes are planted into two different environments. The trait of interest is binary (germination).

Without fitting an interaction term, the prior for a binary trait follows a Chi square distribution of df = 1, based on suggestions in de Villemereuil's (2012) paper. However, I wish to add an interaction term with my "animal" term (which is, actually in my case, the ID of a plant individual). If I wish to keep this prior for an interaction between the environment (a.k.a treatment), should I specify the covariance matrix in the alpha.V or V section of the prior?

Essentially, I want to incorporate suggestions made in Arnold et al. (2019)'s paper.

Below is my code:

prior_germ <- list(R = list(V = 1, fix = 1),
                 G = list(G1 = list(V = 1, nu = 1000, alpha.mu = 0, alpha.V = diag(2)),
                              G2 = list(V = 1, nu = 1000, alpha.mu = 0, alpha.V = diag(2))))

MCMCglmm(germ ~ treatment + plot, random = ~idh(treatment):animal + ~idh(treatment):animal,
                        ginverse = list(animal = Ainv),
                        family = "threshold", data = plastic.Germination, prior = prior_germ, #Bernoulli distribution
                        nitt = 1100000, thin = 500, burnin = 100000, verbose = T, pr = TRUE, trunc = TRUE)


Thanks for any advice in advance!


Cameron

______

Cameron So

Master's Student | Plant Evolutionary Responses to Climate Change | Weis Lab
Department of Ecology & Evolutionary Biology
University of Toronto
ESC2083, St. George Campus

	[[alternative HTML version deleted]]


From p|erre@de@v|||emereu|| @end|ng |rom m@||oo@org  Fri Feb 28 09:44:39 2020
From: p|erre@de@v|||emereu|| @end|ng |rom m@||oo@org (Pierre de Villemereuil)
Date: Fri, 28 Feb 2020 09:44:39 +0100
Subject: [R-sig-ME] MCMCglmm Prior for a Binary Trait with a Random
 Interaction w/ "animal"
In-Reply-To: <YTOPR0101MB18679399DC307C4DCE450E91BCEB0@YTOPR0101MB1867.CANPRD01.PROD.OUTLOOK.COM>
References: <YTOPR0101MB18679399DC307C4DCE450E91BCEB0@YTOPR0101MB1867.CANPRD01.PROD.OUTLOOK.COM>
Message-ID: <1872760.sRKANWeQqj@flyosfixe>

Hi Cameron,

Both V and alpha.V are matrices and should be of same dimensions (which seems to be 2 in your case?).

A long time ago, I wrote a script to be able to visualise the prior distributions in the mono- and multivariate case of the extended parameters. I'm afraid it's not in the best shape as I wrote it quickly, but maybe it can help:
https://github.com/devillemereuil/prior-MCMCglmm/blob/master/priors.R

(Note that this script assumes only the second trait is binomial for the multivariate case, but you can change this easily by setting fix = 1 instead of fix = 2).

I don't guarantee that keeping nu = 1000 for the multivariate case is the best solution (or even a sane one, as it could be extremely informative on the covariances), so using the script to visualise the priors, especially for the correlations might be a good idea.

Hope this helps,
Pierre.

Le jeudi 27 f?vrier 2020, 23:04:35 CET Cameron So a ?crit :
> Hi all,
> 
> I am trying to measure the genetic variance in plasticity when different plant genotypes are planted into two different environments. The trait of interest is binary (germination).
> 
> Without fitting an interaction term, the prior for a binary trait follows a Chi square distribution of df = 1, based on suggestions in de Villemereuil's (2012) paper. However, I wish to add an interaction term with my "animal" term (which is, actually in my case, the ID of a plant individual). If I wish to keep this prior for an interaction between the environment (a.k.a treatment), should I specify the covariance matrix in the alpha.V or V section of the prior?
> 
> Essentially, I want to incorporate suggestions made in Arnold et al. (2019)'s paper.
> 
> Below is my code:
> 
> prior_germ <- list(R = list(V = 1, fix = 1),
>                  G = list(G1 = list(V = 1, nu = 1000, alpha.mu = 0, alpha.V = diag(2)),
>                               G2 = list(V = 1, nu = 1000, alpha.mu = 0, alpha.V = diag(2))))
> 
> MCMCglmm(germ ~ treatment + plot, random = ~idh(treatment):animal + ~idh(treatment):animal,
>                         ginverse = list(animal = Ainv),
>                         family = "threshold", data = plastic.Germination, prior = prior_germ, #Bernoulli distribution
>                         nitt = 1100000, thin = 500, burnin = 100000, verbose = T, pr = TRUE, trunc = TRUE)
> 
> 
> Thanks for any advice in advance!
> 
> 
> Cameron
> 
> ______
> 
> Cameron So
> 
> Master's Student | Plant Evolutionary Responses to Climate Change | Weis Lab
> Department of Ecology & Evolutionary Biology
> University of Toronto
> ESC2083, St. George Campus
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


From j@de@ @end|ng |rom he@|th@uc@d@edu  Fri Feb 28 08:18:51 2020
From: j@de@ @end|ng |rom he@|th@uc@d@edu (Ades, James)
Date: Fri, 28 Feb 2020 07:18:51 +0000
Subject: [R-sig-ME] Most principled reporting of mixed-effect model
 regression coefficients
In-Reply-To: <001a01d5ecbc$63c884b0$2b598e10$@uke.de>
References: <DM5PR1901MB20072EB8F9BE9DBE5E82002BEA150@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <CAJuCY5xhVYBgZwfwmb-gO9=YYhkg7RA7B4dDjo1_xp_xX6di5g@mail.gmail.com>
 <DM5PR1901MB2007B1C5BF040057769C97D7EA140@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <000001d5e410$a5919990$f0b4ccb0$@uke.de>
 <27250084e9e148ba860e21e55ca4f53e@MSX-L104.msx.ad.zih.tu-dresden.de>
 <CAHr4Dyc7PXYWPax8Zzgvi+-FDmCUoWG=8-RyXDvjNb+Lau303w@mail.gmail.com>
 <e758416e0c78493eb4ed4b98b8080a02@MSX-L104.msx.ad.zih.tu-dresden.de>,
 <CAHr4DycFj9omcOSFYvBZ3jSfdzTeAH2HV_Bwr4g5d=Es2wYY3w@mail.gmail.com>
 <DM5PR1901MB20077BF7D6CF8046F21D557DEAEA0@DM5PR1901MB2007.namprd19.prod.outlook.com>,
 <001a01d5ecbc$63c884b0$2b598e10$@uke.de>
Message-ID: <DM5PR1901MB20077F8E7E040F1E594CF722EAEA0@DM5PR1901MB2007.namprd19.prod.outlook.com>

Thanks, Daniel and Paul.

Yes, I did read that conditional R^2 is higher. From the N&S article, it seems that it represents variance explained by fixed and random factors. Still, depending on the outcome measure, it seems that there would exist a good deal of variance still unaccounted for even considering random factors.

Thanks for the synopses of different packages. I'll try them out and see whether they yield similar answers. It's also helpful to know the ways in which they differ for current and future use.

Regarding your last comment, I think the AIC does a good job of selection and parameter penalization (which is important to my focus), however, when it comes to comparing and communicating the differences between models using different performance measures, AIC, it seems, can really only say whether one model is better than another, but can't really say how much better. This is important, especially when you have to weigh pros and cons of different performance measures and metrics. For instance, with executive function tasks--some recent research demonstrates lack of test-retest reliability. If an aggregate is more reliable, but doesn't explain as much variance, then it's tough to answer which metric is better, which is where it would be nice to quantify the predictive capabilities differences between the models.

From what I've read, different methods of CV introduce different biases. Do you think just splitting the data 80/20 (with random sampling by participants and their corresponding timepoints) would be a sufficient/principled way forward? Then use RMSE or another summary measure to compare models?  If I'm using the predict function, it will output a prediction measure for every timepoint. Would I only take the 4th timepoint prediction to compare with the observed to then determine the RMSE? I.e.

```
predict.fun <- function(train.model) {
  predict(train.model, test.dataset, re.form = NA)
}
test.dataset$outcome <- predict.fun(train.model)
```
Just to be clear about R^2 in multilevel models--am I understanding correctly that it shouldn't be used to look at individual predictors and the variance explained; rather, it should only be used as a measure to quantify the variance explained by an entire model?

This is all super helpful. Thanks!

James


________________________________
From: Daniel L?decke <d.luedecke at uke.de>
Sent: Wednesday, February 26, 2020 7:49 AM
To: Ades, James <jades at health.ucsd.edu>
Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: AW: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients

Hi James,

> Using the N&S equation, for one of my models, I get an R^2 of .35, while using r.squaredGLMM, I get an R^2 of .43. I can't imagine that the random slope of time would make that big of a difference. (The conditional R^2 is .95, and I have no idea how it's that high).

The conditional R2 is usually higher, in case of random slopes probably much higher. You can try to compare your results with performance::r2_nakagawa(), piecewiseSEM::rsquared() and MuMIn::r.squaredGLMM() (see also next answer below).

> Does anyone have any experience with the package?

In most cases, these packages yield similar to identical results. There are, however, certain situations where the one or other package might fit better to your need. In my experience, these functions differ in following points

- r2_nakagawa() only uses log-approximation, while rsquared() and r.squaredGLMM() also yield the results from the delta-approximation.
- all three functions yield consistent results for linear mixed models, including random slopes
- rsquared() fails for binomial models where the response is cbind(trials, success) or similar
- r2_nakagawa() and r.squaredGLMM() can handle models from package glmmTMB
- all three functions yield different results for "glmer(count ~ spp + mined + (1 | site), family = poisson, data = Salamanders)"
- currently, only r2_nakagawa() supports glmmTMB with zero-inflation
- r2_nakagawa() probably supports more packages (like GLMMadaptive or afex)


> Most of the time you can find a better statistic than R^2

I personally would recommend to look at different aspects, not only "fit measures", but also which model makes theoretically more sense. Furthermore, it depends on what you're focussing. If you're interested in the variance of your random effects, i.e. how much variation do you have in your outcome depending on different groups/clusters/subjects, comparing the conditional and marginal R2 makes more sense than looking at AIC (you could also directly look at the ICC in such a situation, which goes into a similar direction as marginal & conditional R2). Also, AIC is most useful when comparing models, but it gives you less information about a certain model itself, while R2 is indeed a useful measure for a single model.

Best
Daniel

-----Urspr?ngliche Nachricht-----
Von: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org] Im Auftrag von Ades, James
Gesendet: Mittwoch, 26. Februar 2020 06:19
An: Maarten Jung <Maarten.Jung at mailbox.tu-dresden.de>
Cc: r-sig-mixed-models at r-project.org
Betreff: Re: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients

Thanks, Daniel and Maarten!

I looked at both Nakagawa and Schielzeth and the Johnson paper; I also looked through your other references...thanks for those. I really liked the linked Stack Exchange post of WHuber's lucid response to R^2.

Johnson references the MuMIn package, which I wasn't familiar with, though he writes that the function "r.squaredGLMM" takes into account the random slope (something that N & S mention as tedious and then wave aside). Using the N&S equation, for one of my models, I get an R^2 of .35, while using r.squaredGLMM, I get an R^2 of .43. I can't imagine that the random slope of time would make that big of a difference. (The conditional R^2 is .95, and I have no idea how it's that high). Does anyone have any experience with the package?

While some models (not for model selection but looking at PCA, individual variables, or some kind of aggregate measure for executive function)  have comparatively large differences in AIC; using R^2 via MuMIn, they might have differences of .01. In other words, what seemed to be decent (and significant with LRT) differences, with r.squaredGLMM they became inconsequential.

AIC seems to do a commendable job of yielding parsimony, but it's utter lack of comparability (with same ?# of observations) is frustrating. While an AIC of 28,620 is better than one with 28,645, there is, to my knowledge, no real way of quantifying that difference. Alas, while WHuber writes, "Most of the time you can find a better statistic than R^2. For model selection you  can look to AIC and BIC," I think the issue is not only in selecting models (which AIC seems to do quite well), but again, in summarizing those models in intuitively quantitative ways.

I've also looked into doing some kind of multiple time series cross validation though from what I've read (see below), this is similarly fraught. Maybe leave one out is the best way to go. The structure of the data has four timepoints with executive function data. The first two timepoints ('17 school year) and the final two timepoints ('18 school year) correspond to each year's standardized test.

Thanks much!



http://www.stat.columbia.edu/~gelman/research/published/final_sub.pdf
Di culty of selecting among multilevel models using predictive accuracy<http://www.stat.columbia.edu/~gelman/research/published/final_sub.pdf>
Statistics and Its Interface Volume 7 (2014) 1 Di culty of selecting among multilevel models using predictive accuracy Wei Wang and Andrew Gelman www.stat.columbia.edu<http://www.stat.columbia.edu>

https://dl.acm.org/doi/10.1016/j.ins.2011.12.028
On the use of cross-validation for time series predictor evaluation | Information Sciences: an International Journal<https://dl.acm.org/doi/10.1016/j.ins.2011.12.028>
In time series predictor evaluation, we observe that with respect to the model selection procedure there is a gap between evaluation of traditional forecasting procedures, on the one hand, and evaluation of machine learning techniques on the other hand.
dl.acm.org


https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.02881
[https://onlinelibrary.wiley.com/cms/asset/e2a4b565-bd18-485c-8dff-a03abb9a0c13/ecog.2017.v40.i8.cover.gif]<https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.02881>
Cross?validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure - Roberts - 2017 - Ecography - Wiley Online Library<https://onlinelibrary.wiley.com/doi/full/10.1111/ecog.02881>
Ideally, model validation, selection, and predictive errors should be calculated using independent data (Ara?jo et al. 2005).For example, validation may be undertaken with data from different geographic regions or spatially distinct subsets of the region, different time periods, such as historic species records from the recent past or from fossil records.
onlinelibrary.wiley.com



________________________________
From: Maarten Jung <Maarten.Jung at mailbox.tu-dresden.de>
Sent: Monday, February 17, 2020 1:35 AM
To: Ades, James <jades at health.ucsd.edu>
Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Most principled reporting of mixed-effect model regression coefficients

> Thanks, Maarten. So I was planning on reporting R^2 (along with AIC) for the overall model fit, not for each predictor, since the regression coefficients themselves give a good indication of relationship (though I wasn't aware that R^2 is "riddled with complications") Is Henrik only saying this only with regard to LMMs and GLMMs?

That makes sense to me. For the overall model fit I would probably still go with Johnson's version [1] which I describe in my StackExchange post (and I think you mentioned it, or the Nakagawa and Schielzeth version it is based on, earlier) and report both the marginal and conditional R^2 values. The regression coefficients provide unstandardized effect sizes on the response scale which I think are a valid way to report effect sizes (see below).
I think Henrik refers to (G)LMMs and gives Rights & Sterba (2019) [2] as reference. Also, the GLMM FAQ website provides a good overview [3].

> When you say "there is no agreed upon way to calculate effect sizes" I'm a little confused. I read through your stack exchange posting, but Henrik's answer refers to standardized effect size. You write, later down, "Whenever possible, we report unstandardized effect sizes which is in line with general recommendation of how to report effect sizes"

What you cite is still Henrik's opinion (and I hoped that I could make this clear by writing "This is what he suggests [...]" and by using the <blockquote> on StackExchange). And your citation still refers to LMMs as he says "Unfortunately, due to the way that variance is partitioned in linear mixed models (e.g., Rights & Sterba, 2019), there does not exist an agreed upon way to calculate standard effect sizes for individual model terms such as main effects or interactions."
In general, I agree with him and with his recommendation to report unstandardized effect sizes (e.g. regression coefficients) if they have a "meaningful" interpretation.
The semi-partial R^2 I mentioned in my last e-mail is an additional/alternative indicator of effect sizes that is probably more in line with what psychologists are used to see reported in papers (especially when results of factorial designs are reported) - and that's the reason I mentioned it.

> I'm also working on a systematic review where there's disagreement over whether effect sizes should be standardized, but it does seem that yield any kind of meaningful comparison, effect sizes would have to be standardized. I don't usually report standardized effect sizes...however, there are times when I z-score IVs to put them on the same scale, and I guess the output of that would be a standardized effect size. I wasn't aware of push back on that practice. What issues would arise from this?

There is nothing wrong with standardizing (e.g. by diving by 1 or 2 standard deviations) predictor variables to get measures of variable importance (within the same model).
Issues arise when standardized effect sizes such as R^2, partial eta^2, etc. between different models are compared without thinking about what differences in these measures can be attributed to (see e.g. this question [4] or the Pek & Flora (2018) paper [5] that Henrik cites). Note that these are general issues that apply to all regression models, not only mixed models.

[1] https://doi.org/10.1111/2041-210X.12225
[2] https://doi.org/10.1037/met0000184
[3] https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#how-do-i-compute-a-coefficient-of-determination-r2-or-an-analogue-for-glmms
[4] https://stats.stackexchange.com/questions/13314/is-r2-useful-or-dangerous/13317
[5] https://doi.org/10.1037/met0000126

Best,
Maarten

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

--

_____________________________________________________________________

Universit?tsklinikum Hamburg-Eppendorf; K?rperschaft des ?ffentlichen Rechts; Gerichtsstand: Hamburg | www.uke.de
Vorstandsmitglieder: Prof. Dr. Burkhard G?ke (Vorsitzender), Prof. Dr. Dr. Uwe Koch-Gromus, Joachim Pr?l?, Marya Verdel
_____________________________________________________________________

SAVE PAPER - THINK BEFORE PRINTING

	[[alternative HTML version deleted]]


From o||verhooker @end|ng |rom pr@t@t|@t|c@@com  Fri Feb 28 14:41:06 2020
From: o||verhooker @end|ng |rom pr@t@t|@t|c@@com (Oliver Hooker)
Date: Fri, 28 Feb 2020 13:41:06 +0000
Subject: [R-sig-ME] Introduction to Bayesian hierarchical modelling using R
 (IBHM04)
Message-ID: <CAEsSYzzTOBfOZvhV+Z_7C-GJ8TJpywYWZq6pwTTu-APDrYPwgw@mail.gmail.com>

Introduction to Bayesian hierarchical modelling using R (IBHM04)

https://www.psstatistics.com/course/introduction-to-bayesian-hierarchical-modelling-using-r-ibhm04/

Please feel free to share!

This course will be delivered by Dr Andrew Parnell form the 23rd -
27th March in Glasgow City Centre.

Course Overview:
This course will cover introductory hierarchical modelling for
real-world data sets from a Bayesian perspective. These methods lie at
the forefront of statistics research and are a vital tool in the
scientist?s toolbox. The course focuses on introducing concepts and
demonstrating good practice in hierarchical models. All methods are
demonstrated with data sets which participants can run themselves.
Participants will be taught how to fit hierarchical models using the
Bayesian modelling software Jags and Stan through the R software
interface. The course covers the full gamut from simple regression
models through to full generalised multivariate hierarchical
structures. A Bayesian approach is taken throughout, meaning that
participants can include all available information in their models and
estimates all unknown quantities with uncertainty. Participants are
encouraged to bring their own data sets for discussion with the course
tutors.

Monday 8th ? Classes from 09:00 to 17:00

Module 1: Introduction to Bayesian Statistics
Module 2: Linear and generalised linear models (GLMs)
Practical: Using R, Jags and Stan for fitting GLMs
Round table discussion: Understanding Bayesian models

Tuesday 9th ? Classes from 09:00 to 17:00

Module 3: Simple hierarchical regression models
Module 4: Hierarchical models for non-Gaussian data
Practical: Fitting hierarchical models
Round table discussion: Interpreting hierarchical model output

Wednesday 10th ? Classes from 09:00 to 17:00

Module 5: Hierarchical models vs mixed effects models
Module 6:  Multivariate and multi-layer hierarchical models
Practical: Advanced examples of hierarchical models
Round table discussion: Issues of continuous vs discrete time

Thursday 11th ? Classes from 09:00 to 16:00

Module 7: Shrinkage and variable selection
Module 8: Hierarchical models and partial pooling
Practical: Shrinkage modelling
Round table discussion Bring your own data set

Friday 12th ? Classes from 09:00 to 16:00

Final day for recap session, catch up time and bring your own data set


Email oliverhooker at PSstatistics.com

Check out our sister sites,
www.PRstatistics.com (Ecology and Life Sciences)
www.PRstatistics.com/consultancy (Statistical and bioinformatics
consultancy in all fields)
www.PRinformatics.com (Bioinformatics and data science)
www.PSstatistics.com (Behaviour and cognition)


1. March 23rd ? 27th
INTRODUCTION TO BAYESIAN HIERARCHICAL MODELLING USING R (IBHM04)
Glasgow, Scotland, Dr Andrew Parnell
https://www.psstatistics.com/course/introduction-to-bayesian-hierarchical-modelling-using-r-ibhm04/

2. March 30th ? April 3rd
INTRODUCTION TO STATISTICAL MODELLING FOR PSYCHOLOGISTS IN R (IPSY03)
Glasgow, Scotland, Dr Dale Barr
https://www.psstatistics.com/course/introduction-to-statistics-using-r-for-psychologists-ipsy03/

3. May 4th ? May 8th
PYTHON FOR DATA SCIENCE, MACHINE LEARNING, AND SCIENTIFIC COMPUTING (PDMS02)
Glasgow, Scotland, Dr. Mark Andrews
https://www.prinformatics.com/course/python-for-data-science-machine-learning-and-scientific-computing-pdms02/

4. May 11th ? 15th
BIOINFORMATICS WITH LINUX WITH PYTHON (BILP01)
Glasgow, Scotland, Dr. Martin Jones
https://www.prinformatics.com/course/introduction-to-linux-workflows-for-biologists-ibul04/

5. May 11th ? 15th
FORMALIZING UNCERTAINTY: FUZZY LOGIC IN SPECIES DISTRIBUTION AND
DIVERSITY PATTERNS (FLDM01)
Glasgow, Scotland, Dr. Marcia Barbosa
https://www.prstatistics.com/course/formalizing-uncertainty-fuzzy-logic-in-species-distribution-and-diversity-patterns-fldm01/

6. May 18th ? 22nd
STRUCTUAL EQUATION MODELLING FOR ECOLOGISTS AND EVOLUTIONARY BIOLOGISTS (SEMR02)
Glasgow, Scotland, Dr. Jonathan Lefcheck, Dr. Jim (ames) Grace
https://www.prstatistics.com/course/structural-equation-modelling-for-ecologists-and-evolutionary-biologists-semr02/

7. May 25th ? 29th
GENERALISED LINEAR (MIXED) (GLMM), NONLINEAR (NLGLM) AND GENERAL
ADDITIVE MODELS (MIXED) (GAMM) (GNAM01)
Glasgow, Scotland, Dr Mark Andrews
https://www.psstatistics.com/course/generalised-linear-glm-nonlinear-nlglm-and-general-additive-models-gam-gnam02/

8. June 1st ? 5th
LANDSCAPE GENETIC DATA ANALYSIS USING R (LNDG03)
Glasgow, Scotland, Prof. Rodney Dyer
http://www.prstatistics.com/course/landscape-genetic-data-analysis-using-r-lndg04/

9. June 8th ? 12th
STABLE ISOTOPE MIXING MODELS USING SIAR, SIBER AND MIXSIAR (SIMM06)
Glasgow, Scotland, Dr. Andrew Parnell, Dr. Andrew Jackson
www.prstatistics.com/course/stable-isotope-mixing-models-using-r-simm06/

10. June 17th ? 21st
ADVANCED PYTHON FOR BIOLOGISTS (APYB04)
Glasgow, Scotland, Dr. Martin Jones
www.prinformatics.com/course/advanced-python-biologists-apyb04/

11. July 20th ? 24th
APPLIED BAYESIAN MODELLING FOR ECOLOGISTS AND EPIDEMIOLOGISTS (ABME06)
Glasgow, Scotland, Dr Matt Denwood
https://www.prstatistics.com/course/applied-bayesian-modelling-for-ecologists-and-epidemiologists-abme06/

12. July 27th ? 31st
SPECIES DISTRIBUTION MODELLING (DBMR01)
Glasgow, Scotland, Dr Matt Fitzpatrick
www.prstatistics.com/course/species-distribution-models-using-r-sdmr01/

13. October 5th ? 9th
ECOLOGICAL NICHE MODELLING USING R (ENMR04)
Glasgow, Scotland, Dr. Neftali Sillero
http://www.prstatistics.com/course/ecological-niche-modelling-using-r-enmr04/

14. October 11th ? 16th
ADVANCED ECOLOGICAL NICHE MODELLING USING R (ABNMR01)
Glasgow, Scotland, Dr. Neftali Sillero
http://www.prstatistics.com/course/advanced-ecological-niche-modelling-using-r-anmr01/

-- 
Oliver Hooker PhD.
PR statistics

2019 publications;

A way forward with eco evo devo: an extended theory of resource
polymorphism with postglacial fishes as model systems. Biological
Reviews (2019).

prstatistics.com
facebook.com/prstatistics/
twitter.com/PRstatistics
groups.google.com/d/forum/pr-statistics-post-course-forum
prstatistics.com/organiser/oliver-hooker/

53 Morrison Street
Glasgow
G5 8LB
+44 (0) 7966500340


From ph||||p@@|d@y @end|ng |rom mp|@n|  Mon Mar  2 18:11:43 2020
From: ph||||p@@|d@y @end|ng |rom mp|@n| (Phillip Alday)
Date: Mon, 2 Mar 2020 18:11:43 +0100
Subject: [R-sig-ME] summer school course "Practical Mixed Effect Regression
 Modelling"
Message-ID: <260c0819-e5c7-db11-e64c-37f5eba077e2@mpi.nl>

Dear all,

We are happy to announce that registration is now open for the 2nd ed of
Practical Mixed Effect Regression Modelling for Psychology and Language
Science at the Radboud University summer school, taught by me and Laurel
Brehm. The course will take place from 3-7 August 2020 in Nijmegen,
the Netherlands. The intended audience is beginning to advanced
psychology and linguistics students who are just getting started
analyzing data. It?s hands-on and we will have you modeling your own
data by the end of the week!

More info and registration here: https://tinyurl.com/pmer2020

Best,
Phillip


From no@mt@|perry @end|ng |rom gm@||@com  Tue Mar  3 12:23:03 2020
From: no@mt@|perry @end|ng |rom gm@||@com (Noam Tal-Perry)
Date: Tue, 3 Mar 2020 13:23:03 +0200
Subject: [R-sig-ME] Dropping fixed factor from a converged glmer model leads
 to singular fit
Message-ID: <CA+bKFQR7Kiz7xpapeJRHB6Hk4i8diTJZ2yk52HoYHGQ8HH4kzw@mail.gmail.com>

Hey all,

I was fitting the following model to test whether condition (categorical, 4
levels) and reaction time (continuous, scaled) predict trial's result
(binomial, correct/incorrect).

model.acc.main_re = glmer(Result ~ 1 + Condition*scale(RT) + (1 +
Condition+scale(RT) | Subject_ID), data=behavioral_df.response_only, family
= binomial, control = glmerControl(optimizer ="bobyqa", optCtrl =
list(maxfun = 100000)))

This model successfully converged.
I was interested in doing an NHST for the interaction term, by comparing
this model with the same model sans the interaction (i.e. compute type-II
SS). So I dropped the interaction and fitted the following model:

model.acc.main_re.no_int = glmer(Result ~ 1 + Condition+scale(RT) + (1 +
Condition+scale(RT) | Subject_ID), data=behavioral_df.response_only, family
= binomial, control = glmerControl(optimizer ="bobyqa", optCtrl =
list(maxfun = 100000)))

Surprisingly, I got boundary (singular) fit error. I tried playing around
with the maxfun parameter but it didn't lead anywhere.

So I was wondering,
(1) How could this be? I was under the impression that with fewer factors,
it should be easier to fit the model, such that a sub-model of a converged
model should ought to converge as well.
(2) What is the recommended approach in such case? Should I drop re from
the main model (with the interaction) even though it converges, to reach a
point where the model without the interaction converges too, before
comparing with anova? Would it make more senes to drop re just from the
main effects model and preserve the re for the fuller model?

Would gladly share code and output if that may help.

Cheers,
NTP

Noam Tal-Perry
PhD student
Shlomit Yuval-Greenberg's Eye-Movement Research Lab
School of Psychological Sciences, Tel-Aviv University

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Tue Mar  3 14:36:10 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 3 Mar 2020 08:36:10 -0500
Subject: [R-sig-ME] 
 Dropping fixed factor from a converged glmer model leads
 to singular fit
In-Reply-To: <CA+bKFQR7Kiz7xpapeJRHB6Hk4i8diTJZ2yk52HoYHGQ8HH4kzw@mail.gmail.com>
References: <CA+bKFQR7Kiz7xpapeJRHB6Hk4i8diTJZ2yk52HoYHGQ8HH4kzw@mail.gmail.com>
Message-ID: <ee2c4466-86d0-e698-9758-1bba2ea5e612@gmail.com>



  I couldn't say exactly what happened, but I don't find this shocking.

  I might find time to a look at code & data if you send it.

  If you wanted to look into this in detail you could profile over the
interaction term, i.e. fit the model with the interaction parameter
fixed to values ranging from zero to the estimated value in the full
model, in order to understand how the random effects parameters change
as you go from the full to the nested model.

  I would go ahead and compare the full and nested model even though the
nested model is singular.

  cheers
   Ben Bolker



On 2020-03-03 6:23 a.m., Noam Tal-Perry wrote:
> Hey all,
> 
> I was fitting the following model to test whether condition (categorical, 4
> levels) and reaction time (continuous, scaled) predict trial's result
> (binomial, correct/incorrect).
> 
> model.acc.main_re = glmer(Result ~ 1 + Condition*scale(RT) + (1 +
> Condition+scale(RT) | Subject_ID), data=behavioral_df.response_only, family
> = binomial, control = glmerControl(optimizer ="bobyqa", optCtrl =
> list(maxfun = 100000)))
> 
> This model successfully converged.
> I was interested in doing an NHST for the interaction term, by comparing
> this model with the same model sans the interaction (i.e. compute type-II
> SS). So I dropped the interaction and fitted the following model:
> 
> model.acc.main_re.no_int = glmer(Result ~ 1 + Condition+scale(RT) + (1 +
> Condition+scale(RT) | Subject_ID), data=behavioral_df.response_only, family
> = binomial, control = glmerControl(optimizer ="bobyqa", optCtrl =
> list(maxfun = 100000)))
> 
> Surprisingly, I got boundary (singular) fit error. I tried playing around
> with the maxfun parameter but it didn't lead anywhere.
> 
> So I was wondering,
> (1) How could this be? I was under the impression that with fewer factors,
> it should be easier to fit the model, such that a sub-model of a converged
> model should ought to converge as well.
> (2) What is the recommended approach in such case? Should I drop re from
> the main model (with the interaction) even though it converges, to reach a
> point where the model without the interaction converges too, before
> comparing with anova? Would it make more senes to drop re just from the
> main effects model and preserve the re for the fuller model?
> 
> Would gladly share code and output if that may help.
> 
> Cheers,
> NTP
> 
> Noam Tal-Perry
> PhD student
> Shlomit Yuval-Greenberg's Eye-Movement Research Lab
> School of Psychological Sciences, Tel-Aviv University
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From kr@g|tcode @end|ng |rom gm@||@com  Tue Mar  3 23:29:50 2020
From: kr@g|tcode @end|ng |rom gm@||@com (Kate R)
Date: Tue, 3 Mar 2020 14:29:50 -0800
Subject: [R-sig-ME] AIC Comparison for MLM with Different Distributions
Message-ID: <CAGXFLecXrQqSL_FTeNYY4Th=BWQ166UaQanX0TYnfjhs9oKRAw@mail.gmail.com>

Hi all,

Thank you in advance for your time and consideration! I am a
non-mathematically-inclined graduate student in communication just learning
multilevel modeling.

I am trying to compare the AIC for 5 different models:


   1. model.mn5 <- lmer(anxious ~ num.cm + num.pmc + (1|userid), data = df,
   REML = F)
   2. model.mn5.log <- lmer(log(anxious) ~ num.cm + num.pmc + (1|userid),
   data = df, REML = F)
   3. model.mn5.gamma.log <- glmer(anxious ~ num.cm + num.pmc + (1|userid),
   data = df, family = Gamma(link="log"))
   4. model.mn5.gamma.id <- glmer(anxious ~ num.cm + num.pmc + (1|userid),
   data = df, family = Gamma(link="identity"))
   5. model.ord5 <- clmm(anxious ~ num.cm + num.pmc + (1|userid), data =
   df, na.action = na.omit)

(num.cm is the group mean and num.pmc is the group-mean-centered score of
the predictor)

Despite many posts on various help forums, I understand that it's possible
to compare non-nested models with different distributions as long as all
terms, including constants, are retained (i.e. see Burnham & Anderson, Ch
6.7 <https://www.springer.com/gp/book/9780387953649>), but that different R
packages or model classes might handle constants differently or use
different algorithms (see point 7 <https://robjhyndman.com/hyndsight/aic/>),
thus making it difficult to directly compare AIC values. To avoid
this non-comparability pitfall, it was suggested in one post to calculate
your own log-likelihood (though I'm having trouble finding this post again).

Please could you help with the following:

   - What is the best practice for comparing the AICs for these 5 models?
   - What is the R-code for manually calculating the log-likelihood and/or
   the AIC to retain all terms, including constants?
   - Can you compare ordinal models (clmm) with the continuous models?
   - Do you recommend any other methods and/or packages for comparing
   models with different distributions and/or links?

Many thanks in advance for your time and consideration! I greatly
appreciate any suggestions.

Kind regards,
K

	[[alternative HTML version deleted]]


From d@r|zopou|o@ @end|ng |rom er@@mu@mc@n|  Wed Mar  4 09:51:16 2020
From: d@r|zopou|o@ @end|ng |rom er@@mu@mc@n| (D. Rizopoulos)
Date: Wed, 4 Mar 2020 08:51:16 +0000
Subject: [R-sig-ME] Online Course Repeated Measurements
Message-ID: <75dcdd25-d62e-afbf-44c9-d6d59a94235f@erasmusmc.nl>

Dear All,

I would like to announce an online course on Analysis of Repeated 
Measurements Data I will be teaching from April 6 to 23, 2020: 
https://www.nihes.com/course/ce08_repeated_measurements/

The course covers marginal and mixed models for continuous data, and 
Generalized Estimating Equations and GLMMs for categorical data; slides 
available here: http://www.drizopoulos.com/courses/EMC/CE08.pdf

The course also features a dedicated shiny app that replicates all 
analyses shown in the slides: 
https://github.com/drizopoulos/Repeated_Measurements

It will be given in a blended-learning style integrating electronic and 
online media (i.e., pre-recorded videos with the lectures, online 
practicals) as well as traditional face-to-face teaching. The 
face-to-face teaching sessions will be online to follow via Zoom and are 
planned for April 6, 9, 14, 16, 20, 23 from 10:00 - 12:00 CEST.

Online participants receive a 50% discount 
(https://www.nihes.com/news/repeated-measurement-ce08-is-fully-online/).

Best,
Dimitris

-- 
Dimitris Rizopoulos
Professor of Biostatistics
Department of Biostatistics
Erasmus University Medical Center

Address: PO Box 2040, 3000 CA Rotterdam, the Netherlands
Tel: +31/(0)10/7043478
Fax: +31/(0)10/7043014
Web (personal): http://www.drizopoulos.com/
Web (work): http://www.erasmusmc.nl/biostatistiek/
Blog: http://iprogn.blogspot.nl/

From th|erry@onke||nx @end|ng |rom |nbo@be  Wed Mar  4 15:24:56 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Wed, 4 Mar 2020 15:24:56 +0100
Subject: [R-sig-ME] AIC Comparison for MLM with Different Distributions
In-Reply-To: <CAGXFLecXrQqSL_FTeNYY4Th=BWQ166UaQanX0TYnfjhs9oKRAw@mail.gmail.com>
References: <CAGXFLecXrQqSL_FTeNYY4Th=BWQ166UaQanX0TYnfjhs9oKRAw@mail.gmail.com>
Message-ID: <CAJuCY5yqYBTbqFoTdVPTv8u6f5zd+tUaajngGvCb_ZJR0N3MyA@mail.gmail.com>

Dear Kate,

If your response variable is an ordered factor, then use the clmm model as
that is one with the most appropriate distribution. All other models are
workarounds. Hence the AIC comparison is not relevant.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op di 3 mrt. 2020 om 23:30 schreef Kate R <kr.gitcode at gmail.com>:

> Hi all,
>
> Thank you in advance for your time and consideration! I am a
> non-mathematically-inclined graduate student in communication just learning
> multilevel modeling.
>
> I am trying to compare the AIC for 5 different models:
>
>
>    1. model.mn5 <- lmer(anxious ~ num.cm + num.pmc + (1|userid), data =
> df,
>    REML = F)
>    2. model.mn5.log <- lmer(log(anxious) ~ num.cm + num.pmc + (1|userid),
>    data = df, REML = F)
>    3. model.mn5.gamma.log <- glmer(anxious ~ num.cm + num.pmc +
> (1|userid),
>    data = df, family = Gamma(link="log"))
>    4. model.mn5.gamma.id <- glmer(anxious ~ num.cm + num.pmc + (1|userid),
>    data = df, family = Gamma(link="identity"))
>    5. model.ord5 <- clmm(anxious ~ num.cm + num.pmc + (1|userid), data =
>    df, na.action = na.omit)
>
> (num.cm is the group mean and num.pmc is the group-mean-centered score of
> the predictor)
>
> Despite many posts on various help forums, I understand that it's possible
> to compare non-nested models with different distributions as long as all
> terms, including constants, are retained (i.e. see Burnham & Anderson, Ch
> 6.7 <https://www.springer.com/gp/book/9780387953649>), but that different
> R
> packages or model classes might handle constants differently or use
> different algorithms (see point 7 <https://robjhyndman.com/hyndsight/aic/
> >),
> thus making it difficult to directly compare AIC values. To avoid
> this non-comparability pitfall, it was suggested in one post to calculate
> your own log-likelihood (though I'm having trouble finding this post
> again).
>
> Please could you help with the following:
>
>    - What is the best practice for comparing the AICs for these 5 models?
>    - What is the R-code for manually calculating the log-likelihood and/or
>    the AIC to retain all terms, including constants?
>    - Can you compare ordinal models (clmm) with the continuous models?
>    - Do you recommend any other methods and/or packages for comparing
>    models with different distributions and/or links?
>
> Many thanks in advance for your time and consideration! I greatly
> appreciate any suggestions.
>
> Kind regards,
> K
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From kr@g|tcode @end|ng |rom gm@||@com  Wed Mar  4 17:38:59 2020
From: kr@g|tcode @end|ng |rom gm@||@com (Kate R)
Date: Wed, 4 Mar 2020 08:38:59 -0800
Subject: [R-sig-ME] AIC Comparison for MLM with Different Distributions
In-Reply-To: <CAJuCY5yqYBTbqFoTdVPTv8u6f5zd+tUaajngGvCb_ZJR0N3MyA@mail.gmail.com>
References: <CAGXFLecXrQqSL_FTeNYY4Th=BWQ166UaQanX0TYnfjhs9oKRAw@mail.gmail.com>
 <CAJuCY5yqYBTbqFoTdVPTv8u6f5zd+tUaajngGvCb_ZJR0N3MyA@mail.gmail.com>
Message-ID: <CAGXFLefGF8zm9x8sM9L-KW=P3PSsMyHnV9m00WMRAegST8i9xw@mail.gmail.com>

Hi Thierry,

Thank you for your response!

We are running different models - some have ordered factors as the response
variable and others have continuous or count data as the response variable, and
so I would still be curious to learn how to compare the AIC for models 1-4.

One post suggested that in order to compare normal with log-normal, you
would transform the AIC for the log-normal model with the following code: AIC
+ 2*sum(log(anxious)). I am still unsure how to compare the lmer/normal
models with the glmer/gamma models, as well as between glmer/gamma models
with different link functions.

For the ordered factor, I'd prefer to use the clmm for this, but it's
unfortunately common practice in the journals we publish in to use
continuous models (for ease of interpretation and convention), and so I'd
like to be able to show that the model fit is best with the clmm. In Burnham
& Anderson's book, they compare continuous models with count models, so I
hope it's possible to compare continuous with ordinal?

For the models with count data (frequency of use) as the response variable,
I suppose that we might also want to be able to compare poisson and
negative binomial distributions...

Overall, I'd like to learn how to compare models with different
distributions and/or links for my general knowledge and future use with
different research questions.

Many thanks again for your help!
Katie

On Wed, Mar 4, 2020 at 6:25 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> Dear Kate,
>
> If your response variable is an ordered factor, then use the clmm model as
> that is one with the most appropriate distribution. All other models are
> workarounds. Hence the AIC comparison is not relevant.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op di 3 mrt. 2020 om 23:30 schreef Kate R <kr.gitcode at gmail.com>:
>
>> Hi all,
>>
>> Thank you in advance for your time and consideration! I am a
>> non-mathematically-inclined graduate student in communication just
>> learning
>> multilevel modeling.
>>
>> I am trying to compare the AIC for 5 different models:
>>
>>
>>    1. model.mn5 <- lmer(anxious ~ num.cm + num.pmc + (1|userid), data =
>> df,
>>    REML = F)
>>    2. model.mn5.log <- lmer(log(anxious) ~ num.cm + num.pmc + (1|userid),
>>    data = df, REML = F)
>>    3. model.mn5.gamma.log <- glmer(anxious ~ num.cm + num.pmc +
>> (1|userid),
>>    data = df, family = Gamma(link="log"))
>>    4. model.mn5.gamma.id <- glmer(anxious ~ num.cm + num.pmc +
>> (1|userid),
>>    data = df, family = Gamma(link="identity"))
>>    5. model.ord5 <- clmm(anxious ~ num.cm + num.pmc + (1|userid), data =
>>    df, na.action = na.omit)
>>
>> (num.cm is the group mean and num.pmc is the group-mean-centered score of
>> the predictor)
>>
>> Despite many posts on various help forums, I understand that it's possible
>> to compare non-nested models with different distributions as long as all
>> terms, including constants, are retained (i.e. see Burnham & Anderson, Ch
>> 6.7 <https://www.springer.com/gp/book/9780387953649>), but that
>> different R
>> packages or model classes might handle constants differently or use
>> different algorithms (see point 7 <https://robjhyndman.com/hyndsight/aic/
>> >),
>> thus making it difficult to directly compare AIC values. To avoid
>> this non-comparability pitfall, it was suggested in one post to calculate
>> your own log-likelihood (though I'm having trouble finding this post
>> again).
>>
>> Please could you help with the following:
>>
>>    - What is the best practice for comparing the AICs for these 5 models?
>>    - What is the R-code for manually calculating the log-likelihood and/or
>>    the AIC to retain all terms, including constants?
>>    - Can you compare ordinal models (clmm) with the continuous models?
>>    - Do you recommend any other methods and/or packages for comparing
>>    models with different distributions and/or links?
>>
>> Many thanks in advance for your time and consideration! I greatly
>> appreciate any suggestions.
>>
>> Kind regards,
>> K
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From @ndreo|@ouz@ @end|ng |rom y@hoo@com@br  Wed Mar  4 19:41:13 2020
From: @ndreo|@ouz@ @end|ng |rom y@hoo@com@br (Andre Oliveira)
Date: Wed, 4 Mar 2020 18:41:13 +0000 (UTC)
Subject: [R-sig-ME] =?utf-8?q?Extension__of_parameters_implemented_by_=28?=
 =?utf-8?b?SGFkIO+sgSBlbGQsIDIwMTAp?=
References: <1695805529.3044690.1583347273780.ref@mail.yahoo.com>
Message-ID: <1695805529.3044690.1583347273780@mail.yahoo.com>

Good afternoon,does anyone recommend literature describing the extension? of parameters implemented by (Had?eld, 2010) in the MCMglmm library? He mentions in class notes alpha.mu = 0, alpha.V = 25 ^ 2
Thanks



	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Thu Mar  5 10:01:18 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Thu, 5 Mar 2020 10:01:18 +0100
Subject: [R-sig-ME] AIC Comparison for MLM with Different Distributions
In-Reply-To: <CAGXFLefGF8zm9x8sM9L-KW=P3PSsMyHnV9m00WMRAegST8i9xw@mail.gmail.com>
References: <CAGXFLecXrQqSL_FTeNYY4Th=BWQ166UaQanX0TYnfjhs9oKRAw@mail.gmail.com>
 <CAJuCY5yqYBTbqFoTdVPTv8u6f5zd+tUaajngGvCb_ZJR0N3MyA@mail.gmail.com>
 <CAGXFLefGF8zm9x8sM9L-KW=P3PSsMyHnV9m00WMRAegST8i9xw@mail.gmail.com>
Message-ID: <CAJuCY5zStbQwZXT6FxYQ5WWKa_nCCE1-RyZQM=geABZZTqKzSQ@mail.gmail.com>

Dear Kate,

The difference between models 1 & 2 and 3 & 4 is the log transformation.
That is IMHO a design issue. You get additive effects with an identity link
(Y = \beta_0  + \beta_1 X) and multiplicative effects with the log link (Y
= e ^ \beta_0 e ^ (\beta_1 X)). Use domain knowledge to make and motivate
that choice.

Gaussian or gamma? You could start with Gaussian and check the assumptions.
If they all hold you can stick with Gaussian. If they don't hold and you
get indications that gamma might be better, then try gamma and check its
assumptions.

The same holds for Poisson or negative binomial. Start with Poisson, check
assumptions. If they hold, you are good to go. If they don't, think about
what would improve the model (negative binomial, zero-inflation, missing
variables, missing correlation structure, ...)

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op wo 4 mrt. 2020 om 17:39 schreef Kate R <kr.gitcode at gmail.com>:

> Hi Thierry,
>
> Thank you for your response!
>
> We are running different models - some have ordered factors as the
> response variable and others have continuous or count data as the
> response variable, and so I would still be curious to learn how to
> compare the AIC for models 1-4.
>
> One post suggested that in order to compare normal with log-normal, you
> would transform the AIC for the log-normal model with the following code: AIC
> + 2*sum(log(anxious)). I am still unsure how to compare the lmer/normal
> models with the glmer/gamma models, as well as between glmer/gamma models
> with different link functions.
>
> For the ordered factor, I'd prefer to use the clmm for this, but it's
> unfortunately common practice in the journals we publish in to use
> continuous models (for ease of interpretation and convention), and so I'd
> like to be able to show that the model fit is best with the clmm. In Burnham
> & Anderson's book, they compare continuous models with count models, so I
> hope it's possible to compare continuous with ordinal?
>
> For the models with count data (frequency of use) as the response
> variable, I suppose that we might also want to be able to compare poisson
> and negative binomial distributions...
>
> Overall, I'd like to learn how to compare models with different
> distributions and/or links for my general knowledge and future use with
> different research questions.
>
> Many thanks again for your help!
> Katie
>
> On Wed, Mar 4, 2020 at 6:25 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
>> Dear Kate,
>>
>> If your response variable is an ordered factor, then use the clmm model
>> as that is one with the most appropriate distribution. All other models are
>> workarounds. Hence the AIC comparison is not relevant.
>>
>> Best regards,
>>
>> ir. Thierry Onkelinx
>> Statisticus / Statistician
>>
>> Vlaamse Overheid / Government of Flanders
>> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE
>> AND FOREST
>> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
>> thierry.onkelinx at inbo.be
>> Havenlaan 88 bus 73, 1000 Brussel
>> www.inbo.be
>>
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> ///////////////////////////////////////////////////////////////////////////////////////////
>>
>> <https://www.inbo.be>
>>
>>
>> Op di 3 mrt. 2020 om 23:30 schreef Kate R <kr.gitcode at gmail.com>:
>>
>>> Hi all,
>>>
>>> Thank you in advance for your time and consideration! I am a
>>> non-mathematically-inclined graduate student in communication just
>>> learning
>>> multilevel modeling.
>>>
>>> I am trying to compare the AIC for 5 different models:
>>>
>>>
>>>    1. model.mn5 <- lmer(anxious ~ num.cm + num.pmc + (1|userid), data =
>>> df,
>>>    REML = F)
>>>    2. model.mn5.log <- lmer(log(anxious) ~ num.cm + num.pmc +
>>> (1|userid),
>>>    data = df, REML = F)
>>>    3. model.mn5.gamma.log <- glmer(anxious ~ num.cm + num.pmc +
>>> (1|userid),
>>>    data = df, family = Gamma(link="log"))
>>>    4. model.mn5.gamma.id <- glmer(anxious ~ num.cm + num.pmc +
>>> (1|userid),
>>>    data = df, family = Gamma(link="identity"))
>>>    5. model.ord5 <- clmm(anxious ~ num.cm + num.pmc + (1|userid), data =
>>>    df, na.action = na.omit)
>>>
>>> (num.cm is the group mean and num.pmc is the group-mean-centered score
>>> of
>>> the predictor)
>>>
>>> Despite many posts on various help forums, I understand that it's
>>> possible
>>> to compare non-nested models with different distributions as long as all
>>> terms, including constants, are retained (i.e. see Burnham & Anderson, Ch
>>> 6.7 <https://www.springer.com/gp/book/9780387953649>), but that
>>> different R
>>> packages or model classes might handle constants differently or use
>>> different algorithms (see point 7 <
>>> https://robjhyndman.com/hyndsight/aic/>),
>>> thus making it difficult to directly compare AIC values. To avoid
>>> this non-comparability pitfall, it was suggested in one post to calculate
>>> your own log-likelihood (though I'm having trouble finding this post
>>> again).
>>>
>>> Please could you help with the following:
>>>
>>>    - What is the best practice for comparing the AICs for these 5 models?
>>>    - What is the R-code for manually calculating the log-likelihood
>>> and/or
>>>    the AIC to retain all terms, including constants?
>>>    - Can you compare ordinal models (clmm) with the continuous models?
>>>    - Do you recommend any other methods and/or packages for comparing
>>>    models with different distributions and/or links?
>>>
>>> Many thanks in advance for your time and consideration! I greatly
>>> appreciate any suggestions.
>>>
>>> Kind regards,
>>> K
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>

	[[alternative HTML version deleted]]


From ebhod@ghe|@|th @end|ng |rom gm@||@com  Fri Mar  6 13:09:23 2020
From: ebhod@ghe|@|th @end|ng |rom gm@||@com (Ebhodaghe Faith)
Date: Fri, 6 Mar 2020 15:09:23 +0300
Subject: [R-sig-ME] Request: Making plots for glmm
Message-ID: <CAEatWUpgwLepUyoKJSVY6xkrp9F4o9N+vx0Q9OEeaTs6B6PChQ@mail.gmail.com>

Dear All,

I have a dataset with hierarchical structure and wish to model binomial
response (present or absent) using some categorical and continuous
predictor variables using the glmmTMB package. Could you help describe how
I could make a plot to graphically illustrate relationships between the
response and predictor variables?

Thanks in advance.

Faith

	[[alternative HTML version deleted]]


From gu|||@ume@|mon@@2 @end|ng |rom gm@||@com  Fri Mar  6 13:15:47 2020
From: gu|||@ume@|mon@@2 @end|ng |rom gm@||@com (Guillaume Adeux)
Date: Fri, 6 Mar 2020 13:15:47 +0100
Subject: [R-sig-ME] Request: Making plots for glmm
In-Reply-To: <CAEatWUpgwLepUyoKJSVY6xkrp9F4o9N+vx0Q9OEeaTs6B6PChQ@mail.gmail.com>
References: <CAEatWUpgwLepUyoKJSVY6xkrp9F4o9N+vx0Q9OEeaTs6B6PChQ@mail.gmail.com>
Message-ID: <CAENiVe8pem+rkCQuAa32dUzV0s-GFuCuAeAfDU3kc_cpe3MMtw@mail.gmail.com>

Hi Ebhodaghe,
I invite you to look at the effects package :
https://cran.r-project.org/web/packages/effects/index.html
and the hook-up ggplot package:
https://cran.r-project.org/web/packages/ggeffects/index.html
Cheers,
GA2

Le ven. 6 mars 2020 ? 13:09, Ebhodaghe Faith <ebhodaghefaith at gmail.com> a
?crit :

> Dear All,
>
> I have a dataset with hierarchical structure and wish to model binomial
> response (present or absent) using some categorical and continuous
> predictor variables using the glmmTMB package. Could you help describe how
> I could make a plot to graphically illustrate relationships between the
> response and predictor variables?
>
> Thanks in advance.
>
> Faith
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Fri Mar  6 15:00:25 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 6 Mar 2020 09:00:25 -0500
Subject: [R-sig-ME] Request: Making plots for glmm
In-Reply-To: <CAENiVe8pem+rkCQuAa32dUzV0s-GFuCuAeAfDU3kc_cpe3MMtw@mail.gmail.com>
References: <CAEatWUpgwLepUyoKJSVY6xkrp9F4o9N+vx0Q9OEeaTs6B6PChQ@mail.gmail.com>
 <CAENiVe8pem+rkCQuAa32dUzV0s-GFuCuAeAfDU3kc_cpe3MMtw@mail.gmail.com>
Message-ID: <CABghstSJR3ibnkY06_6axfthPkRs4X-DBys+PDyE6Pj48ABs1A@mail.gmail.com>

  Also emmeans and sjPlot packages ...

On Fri, Mar 6, 2020 at 7:19 AM Guillaume Adeux
<guillaumesimon.a2 at gmail.com> wrote:
>
> Hi Ebhodaghe,
> I invite you to look at the effects package :
> https://cran.r-project.org/web/packages/effects/index.html
> and the hook-up ggplot package:
> https://cran.r-project.org/web/packages/ggeffects/index.html
> Cheers,
> GA2
>
> Le ven. 6 mars 2020 ? 13:09, Ebhodaghe Faith <ebhodaghefaith at gmail.com> a
> ?crit :
>
> > Dear All,
> >
> > I have a dataset with hierarchical structure and wish to model binomial
> > response (present or absent) using some categorical and continuous
> > predictor variables using the glmmTMB package. Could you help describe how
> > I could make a plot to graphically illustrate relationships between the
> > response and predictor variables?
> >
> > Thanks in advance.
> >
> > Faith
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From ebhod@ghe|@|th @end|ng |rom gm@||@com  Fri Mar  6 17:27:40 2020
From: ebhod@ghe|@|th @end|ng |rom gm@||@com (Ebhodaghe Faith)
Date: Fri, 6 Mar 2020 08:27:40 -0800
Subject: [R-sig-ME] Request: Making plots for glmm
In-Reply-To: <CABghstSJR3ibnkY06_6axfthPkRs4X-DBys+PDyE6Pj48ABs1A@mail.gmail.com>
References: <CAEatWUpgwLepUyoKJSVY6xkrp9F4o9N+vx0Q9OEeaTs6B6PChQ@mail.gmail.com>
 <CAENiVe8pem+rkCQuAa32dUzV0s-GFuCuAeAfDU3kc_cpe3MMtw@mail.gmail.com>
 <CABghstSJR3ibnkY06_6axfthPkRs4X-DBys+PDyE6Pj48ABs1A@mail.gmail.com>
Message-ID: <CAEatWUrX4xUsMn8BtbcG9Kee_w6svkkGrQt_jpVpvrM0L2w=MA@mail.gmail.com>

Many thanks, Adeux and Ben.
I successfully installed the *effects* package but I'm getting an error
message trying to load the library.
Here it is:
> library("effects")
Error: package or namespace load failed for ?effects? in loadNamespace(i,
c(lib.loc, .libPaths()), versionCheck = vI[[i]]): there is no package
called ?Rcpp?

Could you help out?

Faith




On Fri, Mar 6, 2020 at 6:01 AM Ben Bolker <bbolker at gmail.com> wrote:

>   Also emmeans and sjPlot packages ...
>
> On Fri, Mar 6, 2020 at 7:19 AM Guillaume Adeux
> <guillaumesimon.a2 at gmail.com> wrote:
> >
> > Hi Ebhodaghe,
> > I invite you to look at the effects package :
> > https://cran.r-project.org/web/packages/effects/index.html
> > and the hook-up ggplot package:
> > https://cran.r-project.org/web/packages/ggeffects/index.html
> > Cheers,
> > GA2
> >
> > Le ven. 6 mars 2020 ? 13:09, Ebhodaghe Faith <ebhodaghefaith at gmail.com>
> a
> > ?crit :
> >
> > > Dear All,
> > >
> > > I have a dataset with hierarchical structure and wish to model binomial
> > > response (present or absent) using some categorical and continuous
> > > predictor variables using the glmmTMB package. Could you help describe
> how
> > > I could make a plot to graphically illustrate relationships between the
> > > response and predictor variables?
> > >
> > > Thanks in advance.
> > >
> > > Faith
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > _______________________________________________
> > > R-sig-mixed-models at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From gu|||@ume@|mon@@2 @end|ng |rom gm@||@com  Fri Mar  6 17:30:48 2020
From: gu|||@ume@|mon@@2 @end|ng |rom gm@||@com (Guillaume Adeux)
Date: Fri, 6 Mar 2020 17:30:48 +0100
Subject: [R-sig-ME] Request: Making plots for glmm
In-Reply-To: <CAEatWUrX4xUsMn8BtbcG9Kee_w6svkkGrQt_jpVpvrM0L2w=MA@mail.gmail.com>
References: <CAEatWUpgwLepUyoKJSVY6xkrp9F4o9N+vx0Q9OEeaTs6B6PChQ@mail.gmail.com>
 <CAENiVe8pem+rkCQuAa32dUzV0s-GFuCuAeAfDU3kc_cpe3MMtw@mail.gmail.com>
 <CABghstSJR3ibnkY06_6axfthPkRs4X-DBys+PDyE6Pj48ABs1A@mail.gmail.com>
 <CAEatWUrX4xUsMn8BtbcG9Kee_w6svkkGrQt_jpVpvrM0L2w=MA@mail.gmail.com>
Message-ID: <CAENiVe-a08aMKDPwrfnc0uwp+TXE_chcvUFWaCE7fc2jCxZonQ@mail.gmail.com>

Have you tried downloading the package "Rcpp"?
and trying again?
GA2

Le ven. 6 mars 2020 ? 17:27, Ebhodaghe Faith <ebhodaghefaith at gmail.com> a
?crit :

> Many thanks, Adeux and Ben.
> I successfully installed the *effects* package but I'm getting an error
> message trying to load the library.
> Here it is:
> > library("effects")
> Error: package or namespace load failed for ?effects? in loadNamespace(i,
> c(lib.loc, .libPaths()), versionCheck = vI[[i]]): there is no package
> called ?Rcpp?
>
> Could you help out?
>
> Faith
>
>
>
>
> On Fri, Mar 6, 2020 at 6:01 AM Ben Bolker <bbolker at gmail.com> wrote:
>
>>   Also emmeans and sjPlot packages ...
>>
>> On Fri, Mar 6, 2020 at 7:19 AM Guillaume Adeux
>> <guillaumesimon.a2 at gmail.com> wrote:
>> >
>> > Hi Ebhodaghe,
>> > I invite you to look at the effects package :
>> > https://cran.r-project.org/web/packages/effects/index.html
>> > and the hook-up ggplot package:
>> > https://cran.r-project.org/web/packages/ggeffects/index.html
>> > Cheers,
>> > GA2
>> >
>> > Le ven. 6 mars 2020 ? 13:09, Ebhodaghe Faith <ebhodaghefaith at gmail.com>
>> a
>> > ?crit :
>> >
>> > > Dear All,
>> > >
>> > > I have a dataset with hierarchical structure and wish to model
>> binomial
>> > > response (present or absent) using some categorical and continuous
>> > > predictor variables using the glmmTMB package. Could you help
>> describe how
>> > > I could make a plot to graphically illustrate relationships between
>> the
>> > > response and predictor variables?
>> > >
>> > > Thanks in advance.
>> > >
>> > > Faith
>> > >
>> > >         [[alternative HTML version deleted]]
>> > >
>> > > _______________________________________________
>> > > R-sig-mixed-models at r-project.org mailing list
>> > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> > >
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>

	[[alternative HTML version deleted]]


From @|mont@pper @end|ng |rom trentu@c@  Fri Mar  6 21:06:34 2020
From: @|mont@pper @end|ng |rom trentu@c@ (Simon Tapper)
Date: Fri, 6 Mar 2020 15:06:34 -0500
Subject: [R-sig-ME] Correct specification of ar1 structure in glmmTMB
Message-ID: <CALuZoL452Q5cbu+hb0-W4xFzTdtDG-qkmOOyEBahB9KEsjZ+hw@mail.gmail.com>

Hi All,

I'm wondering if someone would be able to clarify how to correctly specify
the ar1 structure in glmmTMB. I have read the help page and the vignette (
https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html),
and understand that the general formula is ar1(time + 0|grouping variable),
where time is a factor, but how does this work when time consists of two
seperate variables (i.e., date and hour)?

For instance, the dataset I'm working with consists of several ids, and
each id contains multiple days of data, and multiple measurements per day
(i.e., a measurement each hour). The structure in its basic form (no
additional variables) would be something like:

df <- data.frame(id = rep(seq(1,5,1), 3),
                 date = rep(seq(lubridate::ymd("2020-03-01"),
                                lubridate::ymd("2020-03-05"), 1),3),
                 hour = rep(seq(5, 9, 1), 3),
                 value = rnorm(15, 5, 2))

Now change 'hour' and 'date' to class factor and create a unique grouping
variable called 'id_date', consisting of each id associated with each date.

df <- df %>%
mutate(hour_factor = as.factor(hour), date_factor = as.factor(date)) %>%
unite(id_date, id, date_factor, remove=FALSE)

And now the model:

glmmTMB(value ~ ar1(hour_factor | id_date) + (1|id), data=df)

Is this the correct specification for the ar1 structure, when 'hour' is
nested within 'date', and when 'id' is a random effect? Or should 'date'
and 'hour' be combined into a single variable (e.g. 2020-03-01 05:00:00)
and then converted to a factor, with the grouping variable being 'id'?

glmmTMB(value ~ ar1(date_hour | id) + (1|id), data=df)

In my own dataset, both models work, but produce different estimates,
p-values and residuals (note that the example here won't work because of
too few observations).

Thanks,
Simon

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Wed Mar  4 15:55:55 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 4 Mar 2020 09:55:55 -0500
Subject: [R-sig-ME] AIC Comparison for MLM with Different Distributions
In-Reply-To: <CAGXFLecXrQqSL_FTeNYY4Th=BWQ166UaQanX0TYnfjhs9oKRAw@mail.gmail.com>
References: <CAGXFLecXrQqSL_FTeNYY4Th=BWQ166UaQanX0TYnfjhs9oKRAw@mail.gmail.com>
Message-ID: <eae1791a-e56a-7c32-b872-f6fa93157857@gmail.com>


  I agree with Thierry's big-picture comment that you should generally
use broader/qualitative criteria to decide on a model rather than
testing all possibilities.  The only exception I can think of is if you
are *only* interested in predictive accuracy (not in inference
[confidence intervals/p-values etc.]), and you make sure to use
cross-validation or a testing set to evaluate out-of-sample predictive
error (although AIC *should* generally give a reasonable approximation
to relative out-of-sample error).

  Beyond that, if you still want to compute AIC (e.g. your supervisor or
a reviewer is forcing to do it, and you don't think you're in a position
to push back effectively):

  * as long as you include the Jacobian correction when you transform
the predictor variable (i.e. #2), these log-likelihoods (and AICs)
should in principle be comparable (FWIW the robustness of the derivation
of AIC is much weaker for non-nested models; Brian Ripley [of MASS fame]
holds a minority opinion that one should *not* use AICs to compare
non-nested models)

  * computing log-likelihoods/AICs by hand is in principle a good idea,
but is often difficulty for multi-level models, as various integrals or
approximations of integrals are involved.  The lmer and glmer
likelihoods (1-4) are definitely comparable. To compare across platforms
I often try to think of a simplified model that *can* be fitted in both
platforms (e.g. in this case I think a proportional-odds ordinal
regression where the response has only two levels should be equivalent
to a binomial model with cloglog link ...)

  cheers
  Ben Bolker

On 2020-03-03 5:29 p.m., Kate R wrote:
> Hi all,
> 
> Thank you in advance for your time and consideration! I am a
> non-mathematically-inclined graduate student in communication just learning
> multilevel modeling.
> 
> I am trying to compare the AIC for 5 different models:
> 
> 
>    1. model.mn5 <- lmer(anxious ~ num.cm + num.pmc + (1|userid), data = df,
>    REML = F)
>    2. model.mn5.log <- lmer(log(anxious) ~ num.cm + num.pmc + (1|userid),
>    data = df, REML = F)
>    3. model.mn5.gamma.log <- glmer(anxious ~ num.cm + num.pmc + (1|userid),
>    data = df, family = Gamma(link="log"))
>    4. model.mn5.gamma.id <- glmer(anxious ~ num.cm + num.pmc + (1|userid),
>    data = df, family = Gamma(link="identity"))
>    5. model.ord5 <- clmm(anxious ~ num.cm + num.pmc + (1|userid), data =
>    df, na.action = na.omit)
> 
> (num.cm is the group mean and num.pmc is the group-mean-centered score of
> the predictor)
> 
> Despite many posts on various help forums, I understand that it's possible
> to compare non-nested models with different distributions as long as all
> terms, including constants, are retained (i.e. see Burnham & Anderson, Ch
> 6.7 <https://www.springer.com/gp/book/9780387953649>), but that different R
> packages or model classes might handle constants differently or use
> different algorithms (see point 7 <https://robjhyndman.com/hyndsight/aic/>),
> thus making it difficult to directly compare AIC values. To avoid
> this non-comparability pitfall, it was suggested in one post to calculate
> your own log-likelihood (though I'm having trouble finding this post again).
> 




> Please could you help with the following:
> 
>    - What is the best practice for comparing the AICs for these 5 models?
>    - What is the R-code for manually calculating the log-likelihood and/or
>    the AIC to retain all terms, including constants?
>    - Can you compare ordinal models (clmm) with the continuous models?
>    - Do you recommend any other methods and/or packages for comparing
>    models with different distributions and/or links?
> 
> Many thanks in advance for your time and consideration! I greatly
> appreciate any suggestions.
> 
> Kind regards,
> K
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From gu|d@nton|o@mt @end|ng |rom gm@||@com  Fri Mar  6 21:30:34 2020
From: gu|d@nton|o@mt @end|ng |rom gm@||@com (Guidantonio Malagoli Tagliazucchi)
Date: Fri, 6 Mar 2020 20:30:34 +0000
Subject: [R-sig-ME] info model glmer
Message-ID: <CAKcjJjb04tK_2AB0ACtQYhPp9C4gf5zZOoxao=5ha_ys5iHWMg@mail.gmail.com>

Hi,

I have a dataset with two main groups (e.g. healthy and disease, column
"status") and inside each group other two groups (status_genomic).  I have
another column with the tissues (tissue). Columns are genomic features.

input_df<-data.frame(status=as.factor(sample(0:1,2000,replace=T)),
status_genomic=as.factor(sample(0:1,2000,replace=T)),
tissue=paste('tissue',sample(c('a','b','c','d'),2000,replace=T),sep='_'),
replicate(40, rnorm(2000)))

colnames(input_df)[-c(1:3)]<-paste(rep(paste('genomic_feature'),40),1:40,sep='_')

Rows are subjects, columns genomic features.

My aim is identifying of the genomic features (genomic_feature) that are
predictive of the status (column status) and status_genomic. For example I
would like to identify the genomic_feature X with  a relation with the
status but also with the status_genomic. From my model I would like to
exclude the effect of the tissue that is a confounder.

I thought to the following model:

my_formula<-as.formula(status ~ genomic_feature1 + genomic_feature2 +
genomic_feature3 [...] + status_genomic + (1 | tissue))


glmer(my_formula, input_df, family= binomial("logit")


I am not totally sure that this is correct (especially for status_genomic).
Do you have any suggestion who you can provide to me to develop a better
model? Sorry for this question, but i am not an expert.

Thanks in advance for the support,

Gui

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Sat Mar  7 18:17:48 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sat, 7 Mar 2020 12:17:48 -0500
Subject: [R-sig-ME] AIC Comparison for MLM with Different Distributions
In-Reply-To: <CAGXFLecNWPVM7yqzvDMHu6wnPGn9dcDdWRRtrL4TWeXprKYS8w@mail.gmail.com>
References: <mailman.18133.5.1583578801.26076.r-sig-mixed-models@r-project.org>
 <CAGXFLecNWPVM7yqzvDMHu6wnPGn9dcDdWRRtrL4TWeXprKYS8w@mail.gmail.com>
Message-ID: <ec5b4a11-5d67-aea2-131c-19181814c1c4@gmail.com>


  Only when the response variable is transformed.
  [please keep r-sig-mixed-models in the cc: list if possible when
following up on questions ... ]

  cheers
    Ben Bolker

On 2020-03-07 12:16 p.m., Kate R wrote:
> 
> Hi Ben,
> 
> Thank you for your reply! Would I also apply the Jacobian correction to
> the Gamma with log-link, or is it only used when the response variable
> is transformed?
> 
> Many thanks again!
> Katie
> 
> 
>     ------------------------------
> 
>     Message: 2
>     Date: Wed, 4 Mar 2020 09:55:55 -0500
>     From: Ben Bolker <bbolker at gmail.com <mailto:bbolker at gmail.com>>
>     To: r-sig-mixed-models at r-project.org
>     <mailto:r-sig-mixed-models at r-project.org>
>     Subject: Re: [R-sig-ME] AIC Comparison for MLM with Different
>     ? ? ? ? Distributions
>     Message-ID: <eae1791a-e56a-7c32-b872-f6fa93157857 at gmail.com
>     <mailto:eae1791a-e56a-7c32-b872-f6fa93157857 at gmail.com>>
>     Content-Type: text/plain; charset="utf-8"
> 
> 
>     ? I agree with Thierry's big-picture comment that you should generally
>     use broader/qualitative criteria to decide on a model rather than
>     testing all possibilities.? The only exception I can think of is if you
>     are *only* interested in predictive accuracy (not in inference
>     [confidence intervals/p-values etc.]), and you make sure to use
>     cross-validation or a testing set to evaluate out-of-sample predictive
>     error (although AIC *should* generally give a reasonable approximation
>     to relative out-of-sample error).
> 
>     ? Beyond that, if you still want to compute AIC (e.g. your supervisor or
>     a reviewer is forcing to do it, and you don't think you're in a position
>     to push back effectively):
> 
>     ? * as long as you include the Jacobian correction when you transform
>     the predictor variable (i.e. #2), these log-likelihoods (and AICs)
>     should in principle be comparable (FWIW the robustness of the derivation
>     of AIC is much weaker for non-nested models; Brian Ripley [of MASS fame]
>     holds a minority opinion that one should *not* use AICs to compare
>     non-nested models)
> 
>     ? * computing log-likelihoods/AICs by hand is in principle a good idea,
>     but is often difficulty for multi-level models, as various integrals or
>     approximations of integrals are involved.? The lmer and glmer
>     likelihoods (1-4) are definitely comparable. To compare across platforms
>     I often try to think of a simplified model that *can* be fitted in both
>     platforms (e.g. in this case I think a proportional-odds ordinal
>     regression where the response has only two levels should be equivalent
>     to a binomial model with cloglog link ...)
> 
>     ? cheers
>     ? Ben Bolker
> 
>     On 2020-03-03 5:29 p.m., Kate R wrote:
>     > Hi all,
>     >
>     > Thank you in advance for your time and consideration! I am a
>     > non-mathematically-inclined graduate student in communication just
>     learning
>     > multilevel modeling.
>     >
>     > I am trying to compare the AIC for 5 different models:
>     >
>     >
>     >? ? 1. model.mn5 <- lmer(anxious ~ num.cm <http://num.cm> + num.pmc
>     + (1|userid), data = df,
>     >? ? REML = F)
>     >? ? 2. model.mn5.log <- lmer(log(anxious) ~ num.cm <http://num.cm>
>     + num.pmc + (1|userid),
>     >? ? data = df, REML = F)
>     >? ? 3. model.mn5.gamma.log <- glmer(anxious ~ num.cm
>     <http://num.cm> + num.pmc + (1|userid),
>     >? ? data = df, family = Gamma(link="log"))
>     >? ? 4. model.mn5.gamma.id <http://model.mn5.gamma.id> <-
>     glmer(anxious ~ num.cm <http://num.cm> + num.pmc + (1|userid),
>     >? ? data = df, family = Gamma(link="identity"))
>     >? ? 5. model.ord5 <- clmm(anxious ~ num.cm <http://num.cm> +
>     num.pmc + (1|userid), data =
>     >? ? df, na.action = na.omit)
>     >
>     > (num.cm <http://num.cm> is the group mean and num.pmc is the
>     group-mean-centered score of
>     > the predictor)
>     >
>     > Despite many posts on various help forums, I understand that it's
>     possible
>     > to compare non-nested models with different distributions as long
>     as all
>     > terms, including constants, are retained (i.e. see Burnham &
>     Anderson, Ch
>     > 6.7 <https://www.springer.com/gp/book/9780387953649>), but that
>     different R
>     > packages or model classes might handle constants differently or use
>     > different algorithms (see point 7
>     <https://robjhyndman.com/hyndsight/aic/>),
>     > thus making it difficult to directly compare AIC values. To avoid
>     > this non-comparability pitfall, it was suggested in one post to
>     calculate
>     > your own log-likelihood (though I'm having trouble finding this
>     post again).
>     >
> 
> 
> 
> 
>     > Please could you help with the following:
>     >
>     >? ? - What is the best practice for comparing the AICs for these 5
>     models?
>     >? ? - What is the R-code for manually calculating the
>     log-likelihood and/or
>     >? ? the AIC to retain all terms, including constants?
>     >? ? - Can you compare ordinal models (clmm) with the continuous models?
>     >? ? - Do you recommend any other methods and/or packages for comparing
>     >? ? models with different distributions and/or links?
>     >
>     > Many thanks in advance for your time and consideration! I greatly
>     > appreciate any suggestions.
>     >
>     > Kind regards,
>     > K
>     >
>     >? ? ? ?[[alternative HTML version deleted]]
>     >
>     > _______________________________________________
>     > R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>     >
> 
> 
> 
>


From c@meron@@o @end|ng |rom m@||@utoronto@c@  Sat Mar  7 23:39:18 2020
From: c@meron@@o @end|ng |rom m@||@utoronto@c@ (Cameron So)
Date: Sat, 7 Mar 2020 22:39:18 +0000
Subject: [R-sig-ME] MCMCglmm Prior for a Binary Trait with a Random
 Interaction w/ "animal"
In-Reply-To: <1872760.sRKANWeQqj@flyosfixe>
References: <YTOPR0101MB18679399DC307C4DCE450E91BCEB0@YTOPR0101MB1867.CANPRD01.PROD.OUTLOOK.COM>,
 <1872760.sRKANWeQqj@flyosfixe>
Message-ID: <YTOPR0101MB1867B6D465442AA993A1C732BCE00@YTOPR0101MB1867.CANPRD01.PROD.OUTLOOK.COM>

Hi,

Just to follow up on my original question. I've constructed (hopefully appropriate) priors for my multivariate models (binary-binary, binary-gaussian, etc).. However I am receiving error messages after my models have run.

Could anyone suggest on any possible solutions?

I am receiving these errors to my code below:

  1.  In MCMCglmm ... all observations are missing for error term 1 gaussian: liabilities sampled from Norm(0,1)
  2.  In MCMCglmm ... all observations are missing for error term 2 gaussian: liabilities sampled from Norm(0,1)
  3.  Some fixed effects not estimable and have been removed. Use singular.ok=TRUE to sample these effects.

plastic.survival <- total.2019
prior1.1 <- list(R = list(V = diag(2), nu = 2, fix = 1),
                 G = list(G1 = list(V = diag(2), nu = 15, alpha.mu = c(0,0), alpha.V = diag(c(1.25, 1.25))),
                          G2 = list(V = diag(2), nu = 15, alpha.mu = c(0,0), alpha.V = diag(c(1.25, 1.25)))))

PL_model1.1 <- MCMCglmm(flower ~ treatment + plot - 1, random = ~us(treatment):animal + us(treatment):matID,
                        ginverse = list(animal = Ainv), rcov = ~us(treatment):units,
                        family = "threshold", data = plastic.survival, prior = prior1.1, #Bernoulli distribution
                        nitt = 2100000, thin = 1000, burnin = 100000, verbose = T, pr = TRUE, trunc = TRUE)
..

On the other note.. I am also considering on moving to the package 'brms' in the near future since the syntax follows the commonly used lme4, and it seems more flexible to various distributions and multi-level experimental designs. Just wondering also if anyone has attempted constructing priors in this package for quantitative genetic models.



______

Cameron So

Masters Student | Plant Evolutionary Responses to Climate Change | Weis Lab
Department of Ecology & Evolutionary Biology
University of Toronto
ESC2083, St. George Campus

________________________________
From: Pierre de Villemereuil <pierre.de.villemereuil at mailoo.org>
Sent: February 28, 2020 3:44 AM
To: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Cc: Cameron So <cameron.so at mail.utoronto.ca>
Subject: Re: [R-sig-ME] MCMCglmm Prior for a Binary Trait with a Random Interaction w/ "animal"

Hi Cameron,

Both V and alpha.V are matrices and should be of same dimensions (which seems to be 2 in your case?).

A long time ago, I wrote a script to be able to visualise the prior distributions in the mono- and multivariate case of the extended parameters. I'm afraid it's not in the best shape as I wrote it quickly, but maybe it can help:
https://github.com/devillemereuil/prior-MCMCglmm/blob/master/priors.R

(Note that this script assumes only the second trait is binomial for the multivariate case, but you can change this easily by setting fix = 1 instead of fix = 2).

I don't guarantee that keeping nu = 1000 for the multivariate case is the best solution (or even a sane one, as it could be extremely informative on the covariances), so using the script to visualise the priors, especially for the correlations might be a good idea.

Hope this helps,
Pierre.

Le jeudi 27 f?vrier 2020, 23:04:35 CET Cameron So a ?crit :
> Hi all,
>
> I am trying to measure the genetic variance in plasticity when different plant genotypes are planted into two different environments. The trait of interest is binary (germination).
>
> Without fitting an interaction term, the prior for a binary trait follows a Chi square distribution of df = 1, based on suggestions in de Villemereuil's (2012) paper. However, I wish to add an interaction term with my "animal" term (which is, actually in my case, the ID of a plant individual). If I wish to keep this prior for an interaction between the environment (a.k.a treatment), should I specify the covariance matrix in the alpha.V or V section of the prior?
>
> Essentially, I want to incorporate suggestions made in Arnold et al. (2019)'s paper.
>
> Below is my code:
>
> prior_germ <- list(R = list(V = 1, fix = 1),
>                  G = list(G1 = list(V = 1, nu = 1000, alpha.mu = 0, alpha.V = diag(2)),
>                               G2 = list(V = 1, nu = 1000, alpha.mu = 0, alpha.V = diag(2))))
>
> MCMCglmm(germ ~ treatment + plot, random = ~idh(treatment):animal + ~idh(treatment):animal,
>                         ginverse = list(animal = Ainv),
>                         family = "threshold", data = plastic.Germination, prior = prior_germ, #Bernoulli distribution
>                         nitt = 1100000, thin = 500, burnin = 100000, verbose = T, pr = TRUE, trunc = TRUE)
>
>
> Thanks for any advice in advance!
>
>
> Cameron
>
> ______
>
> Cameron So
>
> Master's Student | Plant Evolutionary Responses to Climate Change | Weis Lab
> Department of Ecology & Evolutionary Biology
> University of Toronto
> ESC2083, St. George Campus
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>





	[[alternative HTML version deleted]]


From j@h@d||e|d @end|ng |rom ed@@c@uk  Sun Mar  8 05:08:24 2020
From: j@h@d||e|d @end|ng |rom ed@@c@uk (Jarrod)
Date: Sun, 8 Mar 2020 04:08:24 +0000
Subject: [R-sig-ME] MCMCglmm Prior for a Binary Trait with a Random
 Interaction w/ "animal"
In-Reply-To: <YTOPR0101MB1867B6D465442AA993A1C732BCE00@YTOPR0101MB1867.CANPRD01.PROD.OUTLOOK.COM>
References: <YTOPR0101MB18679399DC307C4DCE450E91BCEB0@YTOPR0101MB1867.CANPRD01.PROD.OUTLOOK.COM>
 <1872760.sRKANWeQqj@flyosfixe>
 <YTOPR0101MB1867B6D465442AA993A1C732BCE00@YTOPR0101MB1867.CANPRD01.PROD.OUTLOOK.COM>
Message-ID: <178E22CC-C528-4580-93DA-EC0F9598C431@ed.ac.uk>

Hi,

You have specified rcov=~us(treatment):units but any one observation can only be associated with one treatment, so the covariance can?t be estimated. Since the residual variance is also non-identifiable in binary models, you should use the default ~units.

Jarrod

On 7 Mar 2020, at 22:39, Cameron So <cameron.so at mail.utoronto.ca<mailto:cameron.so at mail.utoronto.ca>> wrote:

Hi,

Just to follow up on my original question. I've constructed (hopefully appropriate) priors for my multivariate models (binary-binary, binary-gaussian, etc).. However I am receiving error messages after my models have run.

Could anyone suggest on any possible solutions?

I am receiving these errors to my code below:

 1.  In MCMCglmm ... all observations are missing for error term 1 gaussian: liabilities sampled from Norm(0,1)
 2.  In MCMCglmm ... all observations are missing for error term 2 gaussian: liabilities sampled from Norm(0,1)
 3.  Some fixed effects not estimable and have been removed. Use singular.ok=TRUE to sample these effects.

plastic.survival <- total.2019
prior1.1 <- list(R = list(V = diag(2), nu = 2, fix = 1),
                G = list(G1 = list(V = diag(2), nu = 15, alpha.mu = c(0,0), alpha.V = diag(c(1.25, 1.25))),
                         G2 = list(V = diag(2), nu = 15, alpha.mu = c(0,0), alpha.V = diag(c(1.25, 1.25)))))

PL_model1.1 <- MCMCglmm(flower ~ treatment + plot - 1, random = ~us(treatment):animal + us(treatment):matID,
                       ginverse = list(animal = Ainv), rcov = ~us(treatment):units,
                       family = "threshold", data = plastic.survival, prior = prior1.1, #Bernoulli distribution
                       nitt = 2100000, thin = 1000, burnin = 100000, verbose = T, pr = TRUE, trunc = TRUE)
..

On the other note.. I am also considering on moving to the package 'brms' in the near future since the syntax follows the commonly used lme4, and it seems more flexible to various distributions and multi-level experimental designs. Just wondering also if anyone has attempted constructing priors in this package for quantitative genetic models.



______

Cameron So

Masters Student | Plant Evolutionary Responses to Climate Change | Weis Lab
Department of Ecology & Evolutionary Biology
University of Toronto
ESC2083, St. George Campus

________________________________
From: Pierre de Villemereuil <pierre.de.villemereuil at mailoo.org<mailto:pierre.de.villemereuil at mailoo.org>>
Sent: February 28, 2020 3:44 AM
To: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org> <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Cc: Cameron So <cameron.so at mail.utoronto.ca<mailto:cameron.so at mail.utoronto.ca>>
Subject: Re: [R-sig-ME] MCMCglmm Prior for a Binary Trait with a Random Interaction w/ "animal"

Hi Cameron,

Both V and alpha.V are matrices and should be of same dimensions (which seems to be 2 in your case?).

A long time ago, I wrote a script to be able to visualise the prior distributions in the mono- and multivariate case of the extended parameters. I'm afraid it's not in the best shape as I wrote it quickly, but maybe it can help:
https://github.com/devillemereuil/prior-MCMCglmm/blob/master/priors.R

(Note that this script assumes only the second trait is binomial for the multivariate case, but you can change this easily by setting fix = 1 instead of fix = 2).

I don't guarantee that keeping nu = 1000 for the multivariate case is the best solution (or even a sane one, as it could be extremely informative on the covariances), so using the script to visualise the priors, especially for the correlations might be a good idea.

Hope this helps,
Pierre.

Le jeudi 27 f?vrier 2020, 23:04:35 CET Cameron So a ?crit :
Hi all,

I am trying to measure the genetic variance in plasticity when different plant genotypes are planted into two different environments. The trait of interest is binary (germination).

Without fitting an interaction term, the prior for a binary trait follows a Chi square distribution of df = 1, based on suggestions in de Villemereuil's (2012) paper. However, I wish to add an interaction term with my "animal" term (which is, actually in my case, the ID of a plant individual). If I wish to keep this prior for an interaction between the environment (a.k.a treatment), should I specify the covariance matrix in the alpha.V or V section of the prior?

Essentially, I want to incorporate suggestions made in Arnold et al. (2019)'s paper.

Below is my code:

prior_germ <- list(R = list(V = 1, fix = 1),
                G = list(G1 = list(V = 1, nu = 1000, alpha.mu = 0, alpha.V = diag(2)),
                             G2 = list(V = 1, nu = 1000, alpha.mu = 0, alpha.V = diag(2))))

MCMCglmm(germ ~ treatment + plot, random = ~idh(treatment):animal + ~idh(treatment):animal,
                       ginverse = list(animal = Ainv),
                       family = "threshold", data = plastic.Germination, prior = prior_germ, #Bernoulli distribution
                       nitt = 1100000, thin = 500, burnin = 100000, verbose = T, pr = TRUE, trunc = TRUE)


Thanks for any advice in advance!


Cameron

______

Cameron So

Master's Student | Plant Evolutionary Responses to Climate Change | Weis Lab
Department of Ecology & Evolutionary Biology
University of Toronto
ESC2083, St. George Campus

      [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models






[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336.

	[[alternative HTML version deleted]]


From y@@hree19 @end|ng |rom gm@||@com  Wed Mar 11 21:00:51 2020
From: y@@hree19 @end|ng |rom gm@||@com (Yashree Mehta)
Date: Wed, 11 Mar 2020 21:00:51 +0100
Subject: [R-sig-ME] Interaction terms with random slopes
Message-ID: <CAOE=hq+MhwXLQW5Y84+0ir3b4_5Q0Y+XSiPyuW9nQSQ8L-0-3Q@mail.gmail.com>

Hi,

I have the following question:

I estimate a random intercept-random slope model. For my research question,
I want to interact the random slope variable with another level-1
variable(which is not necessary as a main effect in the model). I am
interesting in observing whether this level-1 variable moderates the random
slope on the dependent variable.

Do I have to include the level-1 variable as a main effect in the model (as
is required by some linear modelling literature)? I would prefer not to
include this level-1 variable as a main effect due to problems of
multicollinearity.

Thank you very much!

	[[alternative HTML version deleted]]


From n|co|@ycunh@ @end|ng |rom gm@||@com  Thu Mar 12 20:48:44 2020
From: n|co|@ycunh@ @end|ng |rom gm@||@com (Nicolay Cunha)
Date: Thu, 12 Mar 2020 16:48:44 -0300
Subject: [R-sig-ME] Doubt about model structure and extraction of slopes and
 intercepts
Message-ID: <CAAPK02Aa1vKQTw6xUbgm6heCKi_JJP9PihhmDF81H78K5V2fQg@mail.gmail.com>

Hi everyone,

I am trying to fit a mixed model with a data set that was extracted from a
grid. We grid a certain area and counted the frequency of occurrence of three
levels of a factor variable in each grid (fac = fac1, fac2 and fac3), I am
basically interested in testing the relationship of the frequency of
occurrence of each level of ?fac? with external variables (X1 and X2, for
instance). The issue is, I am not interested in knowing if the frequency of
occurrence of each level of ?fac? differs between each other (fac1 ? fac2 ?
fac3) because it may be influenced by sampling bias, but I am interested if
X1 and X2 explain the frequency of occurrence of fac1, fac2 and fac3. In
this case, my sample unit is each level of fac per cell of the grid, and
grid is my random factor (in my original data, there is one additional
complication. In my case, ?fac?, is a trait and each sample is a species
individual occurring in the grid, so I will also have to correct for this,
but I think that it can be put aside at the moment).


Below is a reproducible example:

############################################################

set.seed(457)
# generating the data
dat <- data.frame(X1 = runif(150,-2,2), X2 = runif(150,-2,2), fac = gl(n =
3,k = 50))
modmat <- model.matrix(~X1*X2*fac,dat)
betas <- runif(12,-2,2)
dat$y <- rnorm(150, modmat%*%betas, 1 ) # frequency of occurrence
dat$rand <- rep(c(1:5), each = 30) # random effect

library(lme4)
m1 <- lmer(y ~ fac/X1 + fac/X2 + (1|rand), data = dat, REML = FALSE)
library(car)
Anova(m1)
summary(m1)

#-----------First question--------------#
# Does it make sense to write the model the way it is ? by nesting fac with
the predictor variables, to assess the relationship I am interested in?
# Is there a better(or correct) way for doing this?

#############################################################################################
# plotting the results

with(dat, plot(X1, predict(m1), pch=21, col = "black", bg = c("green",
"purple", "salmon")[dat$fac]))
X1_seq = seq(min(dat$X1), max(dat$X1), by=0.001)
ypred_X1_fac1 = (fixef(m1)[1])  + (fixef(phylo_lmm_LF4)[4])*(X1_seq)
lines(X1_seq, ypred_X1_fac1, col = "green", lty = "solid", lwd = 1.5)
ypred_X1_fac2 = (fixef(m1)[2])  + (fixef(phylo_lmm_LF4)[5])*(X1_seq)
lines(X1_seq, ypred_X1_fac2, col = "purple", lty = "solid", lwd = 1.5)
ypred_X1_fac3 = (fixef(m1)[3])  + (fixef(phylo_lmm_LF4)[6])*(X1_seq)
lines(X1_seq, ypred_X1_fac3, col = "salmon", lty = "solid", lwd = 1.5)

#-----------Second question--------------#
# How to extract the slopes and intercepts for all combinations between X1,
X2 and fac in model this way?
# I tried plotting this but failed miserably and I am sure that I am
committing some terrible mistake.
#############################################################################################


I do thank any help, advise and orientation with the model building and
plotting the predictions.

Cheers,
Nicolay


------------------------------------------------------------------------
Grupo de Ecolog?a de la Polinizaci?n
(https://sites.google.com/view/ecopol/home)
INIBIOMA, CONICET-Universidad Nacional del Comahue
Quintral 1250
8400 San Carlos de Bariloche
Rio Negro, Argentina
------------------------------------------------------------------------

	[[alternative HTML version deleted]]


From mtonc|c @end|ng |rom ||r|@hr  Sat Mar 14 17:10:34 2020
From: mtonc|c @end|ng |rom ||r|@hr (marKo)
Date: Sat, 14 Mar 2020 17:10:34 +0100
Subject: [R-sig-ME] strange model fit- help
Message-ID: <93095de2-cc5a-e294-b378-6978e6c23e51@ffri.hr>

Hi.

I have fitted a relatively complicated model to electrodermal data (a 
simple resting and stimulus situation). The data summary follows.

 > summary(data)
        id             sc              t              stim
  g1_1   :  49   Min.   :26798   Min.   :  1.0   before  :3201
  g1_12  :  49   1st Qu.:32299   1st Qu.:123.0   after   :1543
  g1_13  :  49   Median :32486   Median :245.0
  g1_14  :  49   Mean   :32253   Mean   :244.9
  g1_15  :  49   3rd Qu.:32587   3rd Qu.:367.0
  g1_2   :  49   Max.   :32761   Max.   :489.0
  (Other):4450

id (person), and stim are factors, t is time (in s) and sc is skin 
conductance level. Sc distribution is quite negatively asymmetrical at 
the dataset level, although not that bad at the id level. As the 
stimulus occur at a specified time, those two variables are correlated 
(0.81).

The model follows.

m1<-lmer(sc~1+t+I(t^2)+stim+stim:t+stim:I(t^2)+(1+t+I(t^2)+stim+stim:t+stim:I(t^2)|id), 
data=data)

Here goes the summary.

 > summary(m1)
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: sc ~ 1 + t + I(t^2) + stim + stim:t + stim:I(t^2) + (1 + t + 
I(t^2) + stim + stim:t + stim:I(t^2) | id)
    Data: data

      AIC      BIC   logLik deviance df.resid
  62325.9  62506.9 -31134.9  62269.9     4716

Scaled residuals:
      Min       1Q   Median       3Q      Max
-24.3783  -0.1551  -0.0074   0.1392  12.5288

Random effects:
  Groups   Name          Variance  Std.Dev.  Corr
  id       (Intercept)   4.681e+04 2.164e+02
           t             7.925e+00 2.815e+00  1.00
           I(t^2)        2.559e-05 5.059e-03 -0.87 -0.87
           stim.L        1.591e+05 3.989e+02 -0.12 -0.12  0.17
           t:stim.L      1.105e+00 1.051e+00 -0.58 -0.58  0.78  0.33
           I(t^2):stim.L 2.367e-05 4.865e-03  0.06  0.06 -0.21 -0.76 -0.45
  Residual               2.049e+04 1.432e+02
Number of obs: 4744, groups:  id, 97

Fixed effects:
                 Estimate Std. Error t value
(Intercept)    2.960e+04  1.637e+02  180.85
t              1.291e+01  8.493e-01   15.20
I(t^2)        -1.579e-02  1.110e-03  -14.21
stim.L        -3.956e+03  2.329e+02  -16.98
t:stim.L       2.047e+01  1.136e+00   18.01
I(t^2):stim.L -2.477e-02  1.478e-03  -16.76

Correlation of Fixed Effects:
             (Intr) t      I(t^2) stim.L t:st.L
t           -0.886
I(t^2)       0.811 -0.966
stim.L       0.972 -0.930  0.868
t:stim.L    -0.989  0.911 -0.826 -0.973
I(t^2):st.L  0.917 -0.858  0.761  0.870 -0.947

The fit of the model is quite good (pseudo r2 is 0.96), but have some 
problems:
1: quite ?extreme? residuals (-24.3783,  12.5288)
2: quite high correlations among random effects
3: lousy qqplot (apart from the perfect fit on the  from -2 to +2 std 
normal quantiles)

Help please? What is wrong with the model (something is, I?m sure).	




-- 
Marko Ton?i?, PhD
Postdoctoral research assistant
University of Rijeka
Faculty of Humanities and Social Sciences
Department of Psychology
Sveucilisna avenija 4, 51000 Rijeka, CROATIA
e-mail: mtoncic at ffri.hr


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sun Mar 15 04:32:15 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 15 Mar 2020 16:32:15 +1300
Subject: [R-sig-ME] Error from glmmTMB().
Message-ID: <2ef51b3e-694a-1c9e-7e9f-c30fb946de3f@auckland.ac.nz>


I am getting an error, that I have no idea what to do about, from glmmTMB():

library(glmmTMB)
fmla <- cbind(Dead, Alive) ~ (Trt + 0)/Dose + (Dose | Rep)
X    <- dget("X.txt")
fit  <- glmmTMB(fmla, data = X, family = betabinomial(link = "cloglog"),
                dispformula = ~1)

> Error in optimHess(par.fixed, obj$fn, obj$gr) : 
>   gradient in optim evaluated to length 1 not 16
> In addition: There were 16 warnings (use warnings() to see them)

The warnings are all repetitions of

> 1: In nlminb(start = par, objective = fn, gradient = gr,  ... :
>   NA/NaN function evaluation

The error sounds to me like something is amiss in the code.

Can anyone confirm/deny/suggest what I might do to get this call to 
glmmTMB() to run?

Thanks.

cheers,

Rolf Turner

P.S.  The data set is attached.

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: X.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20200315/ce4880f3/attachment.txt>

From th|erry@onke||nx @end|ng |rom |nbo@be  Sun Mar 15 13:14:02 2020
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Sun, 15 Mar 2020 13:14:02 +0100
Subject: [R-sig-ME] strange model fit- help
In-Reply-To: <93095de2-cc5a-e294-b378-6978e6c23e51@ffri.hr>
References: <93095de2-cc5a-e294-b378-6978e6c23e51@ffri.hr>
Message-ID: <CAJuCY5wjt0ZtrR0+by3mMPf4GaDzuWSSp_fO4Gs5=h0peRrUgg@mail.gmail.com>

Dear Marko,

Keep in mind that squating time in seconds lead to large numbers (489 ^ 2 =
239121). This forces the parameters estimates to be small. You can solve
this either by using orthogonal polynomial (poly(t, 2)) or by rescaling t
(e.g. in minutes rather than seconds: 489 s = 8.15 min, 8.15 ^ 2 = 66.4225)
If you go for rescaling, then create 2 variables: t_min and t_min2 (= t_min
^ 2). That will make your formula more reable.

It looks like you coded stim as an ordered factor. That not required since
you have only two levels. Use a default factor with before as reference.

The problem with the strong correlations between t and t^2 random effects
is that they are highly correlated themselves. cor(0:489, (0:489) ^ 2) =
0.986 Note that is isn't solved by rescaling. Only centering works .eg
centering at 4 minutes yields cor(0:489 - 4 * 60 / 60, (0:489 - 4 * 60) ^
2) = 0.071 Orthogonal polynomials have the benefit that they are
uncorrelated by definition. cor(poly(0:489, 2))

Bottomline: always scale and center polynomials. I prefer to scale and
center to revelant values, e.g. scale to a different unit and center to an
important point near the middle of the domain. That keeps your parameters
interpretable without the need to recalculate them (as you would when
scaling by the standard deviation and center to the mean).

Best regards,

Thierry


ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op za 14 mrt. 2020 om 17:10 schreef marKo via R-sig-mixed-models <
r-sig-mixed-models at r-project.org>:

> Hi.
>
> I have fitted a relatively complicated model to electrodermal data (a
> simple resting and stimulus situation). The data summary follows.
>
>  > summary(data)
>         id             sc              t              stim
>   g1_1   :  49   Min.   :26798   Min.   :  1.0   before  :3201
>   g1_12  :  49   1st Qu.:32299   1st Qu.:123.0   after   :1543
>   g1_13  :  49   Median :32486   Median :245.0
>   g1_14  :  49   Mean   :32253   Mean   :244.9
>   g1_15  :  49   3rd Qu.:32587   3rd Qu.:367.0
>   g1_2   :  49   Max.   :32761   Max.   :489.0
>   (Other):4450
>
> id (person), and stim are factors, t is time (in s) and sc is skin
> conductance level. Sc distribution is quite negatively asymmetrical at
> the dataset level, although not that bad at the id level. As the
> stimulus occur at a specified time, those two variables are correlated
> (0.81).
>
> The model follows.
>
> m1<-lmer(sc~1+t+I(t^2)+stim+stim:t+stim:I(t^2)+(1+t+I(t^2)+stim+stim:t+stim:I(t^2)|id),
>
> data=data)
>
> Here goes the summary.
>
>  > summary(m1)
> Linear mixed model fit by maximum likelihood  ['lmerMod']
> Formula: sc ~ 1 + t + I(t^2) + stim + stim:t + stim:I(t^2) + (1 + t +
> I(t^2) + stim + stim:t + stim:I(t^2) | id)
>     Data: data
>
>       AIC      BIC   logLik deviance df.resid
>   62325.9  62506.9 -31134.9  62269.9     4716
>
> Scaled residuals:
>       Min       1Q   Median       3Q      Max
> -24.3783  -0.1551  -0.0074   0.1392  12.5288
>
> Random effects:
>   Groups   Name          Variance  Std.Dev.  Corr
>   id       (Intercept)   4.681e+04 2.164e+02
>            t             7.925e+00 2.815e+00  1.00
>            I(t^2)        2.559e-05 5.059e-03 -0.87 -0.87
>            stim.L        1.591e+05 3.989e+02 -0.12 -0.12  0.17
>            t:stim.L      1.105e+00 1.051e+00 -0.58 -0.58  0.78  0.33
>            I(t^2):stim.L 2.367e-05 4.865e-03  0.06  0.06 -0.21 -0.76 -0.45
>   Residual               2.049e+04 1.432e+02
> Number of obs: 4744, groups:  id, 97
>
> Fixed effects:
>                  Estimate Std. Error t value
> (Intercept)    2.960e+04  1.637e+02  180.85
> t              1.291e+01  8.493e-01   15.20
> I(t^2)        -1.579e-02  1.110e-03  -14.21
> stim.L        -3.956e+03  2.329e+02  -16.98
> t:stim.L       2.047e+01  1.136e+00   18.01
> I(t^2):stim.L -2.477e-02  1.478e-03  -16.76
>
> Correlation of Fixed Effects:
>              (Intr) t      I(t^2) stim.L t:st.L
> t           -0.886
> I(t^2)       0.811 -0.966
> stim.L       0.972 -0.930  0.868
> t:stim.L    -0.989  0.911 -0.826 -0.973
> I(t^2):st.L  0.917 -0.858  0.761  0.870 -0.947
>
> The fit of the model is quite good (pseudo r2 is 0.96), but have some
> problems:
> 1: quite ?extreme? residuals (-24.3783,  12.5288)
> 2: quite high correlations among random effects
> 3: lousy qqplot (apart from the perfect fit on the  from -2 to +2 std
> normal quantiles)
>
> Help please? What is wrong with the model (something is, I?m sure).
>
>
>
>
> --
> Marko Ton?i?, PhD
> Postdoctoral research assistant
> University of Rijeka
> Faculty of Humanities and Social Sciences
> Department of Psychology
> Sveucilisna avenija 4, 51000 Rijeka, CROATIA
> e-mail: mtoncic at ffri.hr
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From j@de@ @end|ng |rom he@|th@uc@d@edu  Mon Mar 16 04:27:34 2020
From: j@de@ @end|ng |rom he@|th@uc@d@edu (Ades, James)
Date: Mon, 16 Mar 2020 03:27:34 +0000
Subject: [R-sig-ME] Hierarchical Psychometric Function in BRMS
Message-ID: <DM5PR1901MB2007F953970156594F62EBFAEAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>

Hi all,

Given that this is a mixed-model listserv, I'm hoping that a BRMS question might fit within that purview.

A quick synopsis of the dataset: there are 14 different conditions of executive function tasks ( ~1000 3rd, 5th, 7th graders). Given that these tasks use an adaptive paradigm (tasks might have anywhere from 25 to 40 trials), I'm trying to disentangle the response window at which participants reach a 70% performance threshold. There are four separate timepoints. (I'm not sure whether the four timepoints can be fit at once because probability distributions for random factor of participant are already used to account for repeated measures of participant completing 14 conditions, but that question is secondary to ensuring that I'm fitting one time point correctly and adequately extracting those the intercept/slope parameters).

If I were to only input this into glmer without the priors, I'd write the model as:
```
glmer(response ~  condition * norm + (norm | pid/condition)
```
(In a glmer model, I can extract intercept/slope parameters fine).

My current model is below. My question isn't so much with the psychometric function or the priors, which, besides the threshold, I've borrowed from Treutwein and Strasburger: https://link.springer.com/article/10.3758/BF03211951--though if there are contentions with any of the those, feel free to raise them--as it is whether I've correctly structured the non-linear parameters. The reason for modeling all four parameters is to minimize bias, but threshold is the only estimate that I'm concerned with. So regarding the multi-level structure, I've created parameters for lapse, guess, spread, and threshold. It seems reasonable to expect that threshold and spread will vary for every participant for every condition, while lapse and guessing (forced yes/no) will likely not differ much from condition to condition within participant (though if there are arguments that it would make for an improved model, I'm fine including lapse and guess parameters for every condition as well).

The other problem I'm having is using coef() or fixef()/ranef() to withdraw (or locate) the overall intercept and slope such that I can use the qlogis() function to determine the psychometric threshold at 70% (since I don't think it would be accurate to directly pull the 70% threshold estimate from the parameter itself?).

Does all of that make sense? This is all a little bit over my head and though I've culled Buerkner's item-response vignettes (Here: https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html and here: https://arxiv.org/pdf/1905.09501.pdf, they're similar but fundamentally different, so they only get me so far).

I've included a small sample of ~five participants here: https://drive.google.com/file/d/1YFnQRSjnp5hVziQx5wQzaIhn75KigaGx/view?usp=sharing

Thanks in advance for any and all help! Hope everyone is staying healthy!

James


```
thresholds <- bf(
  response ~ (gamma + (1 - lambda - gamma) * Phi((norm - threshold)/spread)),
  threshold ~ 1 + (1|p|pid) + (1|c|condition),
  logitgamma  ~ 1 + (1|p|pid),
  nlf(gamma ~ inv_logit(logitgamma)),
  logitlambda ~ 1 + (1|p|pid),
  nlf(lambda ~ inv_logit(logitlambda)),
  spread ~ 1 + (1|p|pid) + (1|c|condition),
nl = TRUE)

prior <-
  prior(beta(9, 3), class = "b", nlpar = "threshold", lb = 0, ub = 1) +
  prior(beta(1.4, 1.4), class = "b", nlpar = "spread", lb = .005, ub = .5) +
  prior(beta(.5, 8), nlpar = "logitlambda", lb = 0, ub = .1)+
  prior(beta(1, 5), nlpar = "logitgamma", lb = 0, ub = .1)

fit_thresholds <- brm(
  formula = thresholds,
  data = ace.threshold.t1.samp,
  family = bernoulli(link = "identity"),
  prior = prior,
  control = list(adapt_delta = .85, max_treedepth = 15),
  inits = 0,
  chains = 1,
  cores = 16
)
```



[https://media.springernature.com/w110/springer-static/cover/journal/13414.jpg]<https://link.springer.com/article/10.3758/BF03211951>
Fitting the psychometric function | SpringerLink<https://link.springer.com/article/10.3758/BF03211951>
A constrained generalized maximum likelihood routine for fitting psychometric functions is proposed, which determines optimum values for the complete parameter set?that is, threshold and slopeas well as for guessing and lapsing probability. The constraints are realized by Bayesian prior distributions for each of these parameters. The fit itself results from maximizing the posterior ...
link.springer.com

Abstract R arXiv:1905.09501v2 [stat.CO] 20 Jul 2019<https://arxiv.org/pdf/1905.09501.pdf>
Paul-Christian B urkner 3 dictions via a nested non-linear formula syntax, the implementation of several distributions designed for response times data, and extentions of distributions for ordinal data, for example
arxiv.org

Estimating Non-Linear Models with brms<https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html>
Introduction. This vignette provides an introduction on how to fit non-linear multilevel models with brms.Non-linear models are incredibly flexible and powerful, but require much more care with respect to model specification and priors than typical generalized linear models.
cran.r-project.org



	[[alternative HTML version deleted]]


From b|mono@om @end|ng |rom gm@||@com  Mon Mar 16 10:10:44 2020
From: b|mono@om @end|ng |rom gm@||@com (=?UTF-8?B?UmVuw6k=?=)
Date: Mon, 16 Mar 2020 10:10:44 +0100
Subject: [R-sig-ME] Hierarchical Psychometric Function in BRMS
In-Reply-To: <DM5PR1901MB2007F953970156594F62EBFAEAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>
References: <DM5PR1901MB2007F953970156594F62EBFAEAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>
Message-ID: <CADcpBHPFYfZcE+13BseKceTWZf0vU8tGF6pvjkMtYyh6_Zy=9A@mail.gmail.com>

Hi James,

since I am working with brms and glmer, I feel I should be able to give a
response (although addressing Paul in the Stan-Forum might be
a better option), there seem to be two questions, and some missing details,
that might lead to even more questions.... let's begin....

My questions:
1. "14 executive functions". Does this mean every participant completed
each of 14 tasks supposed to measure different facets of the general
construct "executive functions in working memory"? (If not, please
clarify). What term is this in the model "condition" or "norm"? (Given that
you have random slopes for "norm" it seems to be "norm" ?) Then what is
condition?

2. "adaptive tasks with 25 to 40 trials" Does this mean "tailored testing"?
(I.e., the trial that comes next within the task depends on the decisions
(their error) from all previous trials?)

3. "Goal: disentangle the response window at which participants reach a
70%", - if you have tailored testing (I am not sure), which already is
designed to sort trials to meander around 75% accuracy for maximum
information/variance , this threshold seems a bit unmotivated, can you give
more background?

4. "four different time points" , I suppose these are four sessions, in
each the participants have completed subsets of the 14 tasks

Your (secondary) questions (I ignore points 1 to 3 now, but they need
clarification):
"I'm not sure whether the four timepoints can be fit at once because
probability distributions for random factor of participant are already used
to account for repeated measures of participant completing 14 conditions)."
My answer:
- Regardless of the technical details:  First, "time points"  has only four
levels, thus, it would not make sense to separate their "random" intercepts
from other variance sources in the design, no matter which. Computing
standard deviations of a distribution for which you only have 4
observations/levels is problematic. Second, nonetheless assuming that "time
points" (e.g., increasing ability over time) has an effect, then
controlling for it is pretty legit, so, it makes sense to include "time
points" into the fixed effects. Also legit.

5. "The other problem I'm having is using coef() or fixef()/ranef() to
withdraw (or locate) the overall intercept and slope such that I can use
the qlogis() function to determine the psychometric threshold at 70% (since
I don't think it would be accurate to directly pull the 70% threshold
estimate from the parameter itself?)."
My answer:
- Do you mean, by 70% threshold, the "location" on the predictor(s) (the
logit) at which the predicted probably of the response is 70%? (Please keep
in mind, that you have two interacting predictors in your model, which
means getting these estimates for one predictor requires to either ignore
variance of the other predictor, which needs theoretical clarification if
you want to interpret this; or taking it into account - see below.) Anyway,
the "manual" way to do this, is to make predictions, based on the
coefficients, and then search the point of crossing 70%. For this you want
to use the "emmeans" package which works for both glmer and brms (but I am
not sure whether it works also for the non-linear models; if not, you need
to ask Paul Buerkner in the Stan forum how to do it ;)); it sure works with
standard hierarchical regression output from brms.) . In the emmeans
package you find the function "emmip", which is what you desire.

#assuming this is your model with a continuous predictor ("continuous") and
a factorial predictor ("factor"):
model<-glmer(response ~  continuous * factor + (continuous | pid))
emmip(model,~continuous,at = list(continuous = c(1,2,3,4,5,6),
type="response",CIs=TRUE, engine="ggplot" )
# this gives you the probability predictions for "continuous" from 1 to 6
(you can make these as "fine" as you want), while ignoring "factor"
# if you want it "by factor" (taking the interaction into account) you can
write:
emmip(model,~continuous|factor ,at = list(continuous = c(1,2,3,4,5,6),
type="response",CIs=TRUE, engine="ggplot" )
#All you have to do is search for the point crossing 70% then :) .

However, as noted, non-linear brms models might not directly translate to
the emmeans architecture (I don't know), and there is a more elegant
solution anyway:

1. A standard logistic function predicts 50% when the logit becomes 0
(before applying the exponential ratio rule; I ignore the fact that your
gamma and lambda model terms absolutely destroy this property... :))
2. The "intercept" shifts the whole logit statically (or by factorial
conditions), such that it indicates "where" 50% is predicted (in a given
condition). For example, in standard models
1/(1+exp(intercept+varyingeffects)) the intercept says for which value of
varyingeffects  the term becomes 0).
3. You can "make the intercept" to indicate a 70% prediction instead of a
50% prediction, if you add a constant on the logit level; that is:
1/(1+exp(-.8477)) = (about) 70%; and
 1/(1+exp(-.8477+intercept+varyingeffects)) shifts the intercept by this
constant, such that it now indicates the value of varyingeffects which
predicts 70%. I guess. .. :)) There could be more detail to that (which I
don't see right now), but it sure is a starting point.

Hope this helps, with your actual questions.
The rest seems to be a different matter.... (e.g., taking dependencies of
tailored testing into account etc).

But one final note: I have once tried to fit simpler models with
constructing the logit myself, like you do, and then setting,  family =
bernoulli(link = "identity"), which never worked (it never converged). ...
Just saying: I think Paul makes some points about the identifiability of
those models in his vignettes, which you should check, if your model fails
converging.
(But dropping lambda and gamma, might be worth considering in any case. If
you simulate logistic functions hierarchically, then they do not
approximate 100% on average (which would be the reason you use gamma and
lambda), but the limited growth approximates e.g., 80 % depending on the
individual variations in the slope parameters of the logistic function.
This means, you don't need "maximum performance" parameters, but can
approximate this behavior by the assumption of hierarchically clustered
variance. Which also makes the model simpler... , and identifiable, and you
could use the "elegant" way of determining 70%).


Best, Ree



Am Mo., 16. M?rz 2020 um 04:28 Uhr schrieb Ades, James <
jades at health.ucsd.edu>:

> Hi all,
>
> Given that this is a mixed-model listserv, I'm hoping that a BRMS question
> might fit within that purview.
>
> A quick synopsis of the dataset: there are 14 different conditions of
> executive function tasks ( ~1000 3rd, 5th, 7th graders). Given that these
> tasks use an adaptive paradigm (tasks might have anywhere from 25 to 40
> trials), I'm trying to disentangle the response window at which
> participants reach a 70% performance threshold. There are four separate
> timepoints. (I'm not sure whether the four timepoints can be fit at once
> because probability distributions for random factor of participant are
> already used to account for repeated measures of participant completing 14
> conditions, but that question is secondary to ensuring that I'm fitting one
> time point correctly and adequately extracting those the intercept/slope
> parameters).
>
> If I were to only input this into glmer without the priors, I'd write the
> model as:
> ```
> glmer(response ~  condition * norm + (norm | pid/condition)
> ```
> (In a glmer model, I can extract intercept/slope parameters fine).
>
> My current model is below. My question isn't so much with the psychometric
> function or the priors, which, besides the threshold, I've borrowed from
> Treutwein and Strasburger:
> https://link.springer.com/article/10.3758/BF03211951--though if there are
> contentions with any of the those, feel free to raise them--as it is
> whether I've correctly structured the non-linear parameters. The reason for
> modeling all four parameters is to minimize bias, but threshold is the only
> estimate that I'm concerned with. So regarding the multi-level structure,
> I've created parameters for lapse, guess, spread, and threshold. It seems
> reasonable to expect that threshold and spread will vary for every
> participant for every condition, while lapse and guessing (forced yes/no)
> will likely not differ much from condition to condition within participant
> (though if there are arguments that it would make for an improved model,
> I'm fine including lapse and guess parameters for every condition as well).
>
> The other problem I'm having is using coef() or fixef()/ranef() to
> withdraw (or locate) the overall intercept and slope such that I can use
> the qlogis() function to determine the psychometric threshold at 70% (since
> I don't think it would be accurate to directly pull the 70% threshold
> estimate from the parameter itself?).
>
> Does all of that make sense? This is all a little bit over my head and
> though I've culled Buerkner's item-response vignettes (Here:
> https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html
> and here: https://arxiv.org/pdf/1905.09501.pdf, they're similar but
> fundamentally different, so they only get me so far).
>
> I've included a small sample of ~five participants here:
> https://drive.google.com/file/d/1YFnQRSjnp5hVziQx5wQzaIhn75KigaGx/view?usp=sharing
>
> Thanks in advance for any and all help! Hope everyone is staying healthy!
>
> James
>
>
> ```
> thresholds <- bf(
>   response ~ (gamma + (1 - lambda - gamma) * Phi((norm -
> threshold)/spread)),
>   threshold ~ 1 + (1|p|pid) + (1|c|condition),
>   logitgamma  ~ 1 + (1|p|pid),
>   nlf(gamma ~ inv_logit(logitgamma)),
>   logitlambda ~ 1 + (1|p|pid),
>   nlf(lambda ~ inv_logit(logitlambda)),
>   spread ~ 1 + (1|p|pid) + (1|c|condition),
> nl = TRUE)
>
> prior <-
>   prior(beta(9, 3), class = "b", nlpar = "threshold", lb = 0, ub = 1) +
>   prior(beta(1.4, 1.4), class = "b", nlpar = "spread", lb = .005, ub = .5)
> +
>   prior(beta(.5, 8), nlpar = "logitlambda", lb = 0, ub = .1)+
>   prior(beta(1, 5), nlpar = "logitgamma", lb = 0, ub = .1)
>
> fit_thresholds <- brm(
>   formula = thresholds,
>   data = ace.threshold.t1.samp,
>   family = bernoulli(link = "identity"),
>   prior = prior,
>   control = list(adapt_delta = .85, max_treedepth = 15),
>   inits = 0,
>   chains = 1,
>   cores = 16
> )
> ```
>
>
>
> [
> https://media.springernature.com/w110/springer-static/cover/journal/13414.jpg
> ]<https://link.springer.com/article/10.3758/BF03211951>
> Fitting the psychometric function | SpringerLink<
> https://link.springer.com/article/10.3758/BF03211951>
> A constrained generalized maximum likelihood routine for fitting
> psychometric functions is proposed, which determines optimum values for the
> complete parameter set?that is, threshold and slopeas well as for guessing
> and lapsing probability. The constraints are realized by Bayesian prior
> distributions for each of these parameters. The fit itself results from
> maximizing the posterior ...
> link.springer.com
>
> Abstract R arXiv:1905.09501v2 [stat.CO] 20 Jul 2019<
> https://arxiv.org/pdf/1905.09501.pdf>
> Paul-Christian B urkner 3 dictions via a nested non-linear formula syntax,
> the implementation of several distributions designed for response times
> data, and extentions of distributions for ordinal data, for example
> arxiv.org
>
> Estimating Non-Linear Models with brms<
> https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html
> >
> Introduction. This vignette provides an introduction on how to fit
> non-linear multilevel models with brms.Non-linear models are incredibly
> flexible and powerful, but require much more care with respect to model
> specification and priors than typical generalized linear models.
> cran.r-project.org
>
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From m@@nder@on@berd@| @end|ng |rom gm@||@com  Mon Mar 16 11:24:03 2020
From: m@@nder@on@berd@| @end|ng |rom gm@||@com (Monica Anderson Berdal)
Date: Mon, 16 Mar 2020 11:24:03 +0100
Subject: [R-sig-ME] MCMCglmm with missing values
Message-ID: <CAEOuUYGKLVM0ZNnomsD9Nk4hTz2G6VaGJYp9Xvm_ZGAB9KvyeQ@mail.gmail.com>

I?m trying to run a MCMCglmm where two of the response variables and three
of the fixed effects are sex specific. Each line represents one individual,
and the data looks like this:

Sex

Fitness_M

Fitness_F

Age_M

Age_F

Mate_mass

Behaviour

Mass

Temperature

Line

M

743

NA

140

NA

NA

69

229

21

X

M

515

NA

188

NA

NA

50

220

19

X

M

390

NA

231

NA

NA

50

109

17

Y

M

266

NA

960

NA

NA

39

113

19

Y

M

323

NA

105

NA

NA

55

171

25

Y

F

NA

112

NA

74

205

67

247

20

Y

F

NA

107

NA

74

163

60

139

26

Z

F

NA

8

NA

53

193

71

118

24

Z

F

NA

207

NA

55

74

37

219

21

Z

F

NA

300

NA

56

160

68

261

18

Z



All individuals have a behaviour and mass recorded, and only the sex
specific variables have NAs. I?ve been using the at.level function to
control for these missing values, but still get the error: Missing values
in the fixed predictors

cricketMCMC <- MCMCglmm(cbind(Fitness_F, Fitness_M, Behaviour, Mass)~

                          at.level(trait, 1):(Age_F + Mate_mass) +

                          at.level(trait, 2):(Age_M) +

                          at.level(trait, 3):(Sex + Temperature) +

                          at.level(trait, 4):(Sex + Temperature),

                        random=~us(trait):Line,rcov=~us(trait):units,

                        family=rep("gaussian",4), nitt=260000, thin=200,
burnin=60000,

                        verbose=FALSE, prior=prior.4t, pr=TRUE, data=Data)



To avoid the issues of missing values I added mean values for Age_F and
Mate_mass for the males and Age_M for the females, while still using the
at.level function. The idea was to overcome the problem with the missing
values while also ignoring the mean values for the sex specific traits.
This model ran without problems, and to see if the model ran properly, I
compared it to a second model where I added a random value instead of mean
values. I used set.seed() before running both models, but the outputs are
not the same, which means that the added values are still affecting the
results.

How can I avoid the problems of NAs when running an MCMCglmm on a data set
with this structure?

	[[alternative HTML version deleted]]


From m@@nder@on@berd@| @end|ng |rom gm@||@com  Mon Mar 16 14:24:35 2020
From: m@@nder@on@berd@| @end|ng |rom gm@||@com (Monica Anderson Berdal)
Date: Mon, 16 Mar 2020 14:24:35 +0100
Subject: [R-sig-ME] MCMCglmm with missing values
Message-ID: <CAEOuUYHQhkUJ+d==fgfhhUrYA=N1a=vo_=31JiRt6uPQNoxmgQ@mail.gmail.com>

Sorry, here is a plain text version with CSV data.

I?m trying to run a MCMCglmm where two of the response variables and
three of the fixed effects are sex specific. Each line represents one
individual, and the data looks like this:

Sex,Fitness_M,Fitness_F,Age_M,Age_F,Mate_mass,Behaviour,Mass,Temperature,Line

M,743,NA,140,NA,NA,69,229,21,X

M,515,NA,188,NA,NA,50,220,19,X

M,390,NA,231,NA,NA,50,109,17,Y

M,266,NA,96,NA,NA,39,113,19,Y

M,323,NA,105,NA,NA,55,171,25,Y

F,NA,112,NA,74,205,67,247,20,Y

F,NA,107,NA,74,163,60,139,26,Z

F,NA,8,NA,53,193,71,118,24,Z

F,NA,207,NA,55,74,37,219,21,Z

F,NA,300,NA,56,160,68,261,18,Z

All individuals have a behaviour mass recorded, and only the sex
specific variables have NAs. I?ve been using the at.level function to
control for these missing values, but still get the error: Missing
values in the fixed predictors

cricketMCMC <- MCMCglmm(cbind(Fitness_F, Fitness_M, Behaviour, Mass)~

                          at.level(trait, 1):(Age_F + Mate_mass) +

                          at.level(trait, 2):(Age_M) +

                          at.level(trait, 3):(Sex + Temperature) +

                          at.level(trait, 4):(Sex + Temperature),

                        random=~us(trait):Line,rcov=~us(trait):units,

                        family=rep("gaussian",4), nitt=260000,
thin=200, burnin=60000,

                        verbose=FALSE, prior=prior.4t, pr=TRUE, data=Data)



To avoid the issues of missing values I added mean values for Age_F
and Mate_mass for the males and Age_M for the females, while still
using the at.level function. The idea was to overcome the problem with
the missing values while also ignoring the mean values for the sex
specific traits. This model ran without problems, and to see if the
model ran properly, I compared it to a second model where I added a
random value instead of mean values. I used set.seed() before running
both models, but the outputs are not the same, which means that the
added values are still affecting the results.

How can I avoid the problems of NAs when running an MCMCglmm on a data
set with this structure?


From j@de@ @end|ng |rom he@|th@uc@d@edu  Mon Mar 16 22:44:33 2020
From: j@de@ @end|ng |rom he@|th@uc@d@edu (Ades, James)
Date: Mon, 16 Mar 2020 21:44:33 +0000
Subject: [R-sig-ME] Hierarchical Psychometric Function in BRMS
In-Reply-To: <CADcpBHPFYfZcE+13BseKceTWZf0vU8tGF6pvjkMtYyh6_Zy=9A@mail.gmail.com>
References: <DM5PR1901MB2007F953970156594F62EBFAEAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>,
 <CADcpBHPFYfZcE+13BseKceTWZf0vU8tGF6pvjkMtYyh6_Zy=9A@mail.gmail.com>
Message-ID: <DM5PR1901MB2007ED6A3460670F84E40537EAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>

Hi Ree,

Thanks for the response.

Responding to your questions:
1) Yes, essentially. So there are 7 tasks, some have two conditions. One has four conditions. This is the "condition" in the model. "Norm" is the normalized response window.

2) Yes, the response window for the following trials depends on whether the previous response is correct and was answered within the response window.

3) I'm not sure what you mean by "unmotivated," but hopefully I can provide some background that will give you a better idea. I'm hesitant about giving too much information for the sake of avoiding confusion, but the threshold was created to be 80%, but when I looked at proportion correct for participants many did not achieve this, so it seemed principled to extract thresholds at 70%. Ideally, the this performance threshold motivates performance (not too easy, but also not too hard). From there, we ask the question, what is the necessary RW for the participant to achieve 70% accuracy. This question is answered through the psychometric function. (In the Treutwein and Strasburger cited paper, they make the point that the psychometric function is best approximated using all four priors for threshold, spread, lapse, and guessing.

4) Yes, four sessions, completed over two years, equally spaced, more or less. I control for this in the model looking at executive function performance on standardized assessment outcome. I wasn't sure whether including timepoints within the psychometric function model would lead to more accurate estimation of participant psychometric functions.

Hopefully, that information helps.

Regarding your final point on convergence: as I'm sure you know, fitting this model with this data is no small feat. Using UCSD's super computer, it takes a little over a day. It did seem to converge though. You then write "(But dropping lambda and gamma, might be worth considering in any case. If you simulate logistic functions hierarchically, then they do not approximate 100% on average (which would be the reason you use gamma and lambda), but the limited growth approximates e.g., 80 % depending on the individual variations in the slope parameters of the logistic function. This means, you don't need "maximum performance" parameters, but can approximate this behavior by the assumption of hierarchically clustered variance. Which also makes the model simpler... , and identifiable, and you could use the "elegant" way of determining 70%)." So this is where I am mathematically over my head. Re Treut and Straus--they're claim is that the most principled approach to approximating the psychometric function of an adaptive paradigm is using prior on all four parameters. Is your argument that if you're using a hierarchical approach, you wouldn't need the gamma/lambda parameters? Can you say more about this or point me to an article that discusses the assumption of hierarchically clustered variance?

Thank you for the parameter extraction methods. I guess we'll figure out which one when we come to that road. Elegant is always nice. But I think the first think is making sure that I have the most principled and correct model. Is the one I currently have in BRMS correct given the clarifications above?

Much thanks!

James

________________________________
From: Ren? <bimonosom at gmail.com>
Sent: Monday, March 16, 2020 2:10 AM
To: Ades, James <jades at health.ucsd.edu>
Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Hierarchical Psychometric Function in BRMS

Hi James,

since I am working with brms and glmer, I feel I should be able to give a response (although addressing Paul in the Stan-Forum might be a better option), there seem to be two questions, and some missing details, that might lead to even more questions.... let's begin....

My questions:
1. "14 executive functions". Does this mean every participant completed each of 14 tasks supposed to measure different facets of the general construct "executive functions in working memory"? (If not, please clarify). What term is this in the model "condition" or "norm"? (Given that you have random slopes for "norm" it seems to be "norm" ?) Then what is condition?

2. "adaptive tasks with 25 to 40 trials" Does this mean "tailored testing"? (I.e., the trial that comes next within the task depends on the decisions (their error) from all previous trials?)

3. "Goal: disentangle the response window at which participants reach a 70%", - if you have tailored testing (I am not sure), which already is designed to sort trials to meander around 75% accuracy for maximum information/variance , this threshold seems a bit unmotivated, can you give more background?

4. "four different time points" , I suppose these are four sessions, in each the participants have completed subsets of the 14 tasks

Your (secondary) questions (I ignore points 1 to 3 now, but they need clarification):
"I'm not sure whether the four timepoints can be fit at once because probability distributions for random factor of participant are already used to account for repeated measures of participant completing 14 conditions)."
My answer:
- Regardless of the technical details:  First, "time points"  has only four levels, thus, it would not make sense to separate their "random" intercepts from other variance sources in the design, no matter which. Computing standard deviations of a distribution for which you only have 4 observations/levels is problematic. Second, nonetheless assuming that "time points" (e.g., increasing ability over time) has an effect, then controlling for it is pretty legit, so, it makes sense to include "time points" into the fixed effects. Also legit.

5. "The other problem I'm having is using coef() or fixef()/ranef() to withdraw (or locate) the overall intercept and slope such that I can use the qlogis() function to determine the psychometric threshold at 70% (since I don't think it would be accurate to directly pull the 70% threshold estimate from the parameter itself?)."
My answer:
- Do you mean, by 70% threshold, the "location" on the predictor(s) (the logit) at which the predicted probably of the response is 70%? (Please keep in mind, that you have two interacting predictors in your model, which means getting these estimates for one predictor requires to either ignore variance of the other predictor, which needs theoretical clarification if you want to interpret this; or taking it into account - see below.) Anyway, the "manual" way to do this, is to make predictions, based on the coefficients, and then search the point of crossing 70%. For this you want to use the "emmeans" package which works for both glmer and brms (but I am not sure whether it works also for the non-linear models; if not, you need to ask Paul Buerkner in the Stan forum how to do it ;)); it sure works with standard hierarchical regression output from brms.) . In the emmeans package you find the function "emmip", which is what you desire.

#assuming this is your model with a continuous predictor ("continuous") and a factorial predictor ("factor"):
model<-glmer(response ~  continuous * factor + (continuous | pid))
emmip(model,~continuous,at = list(continuous = c(1,2,3,4,5,6), type="response",CIs=TRUE, engine="ggplot" )
# this gives you the probability predictions for "continuous" from 1 to 6 (you can make these as "fine" as you want), while ignoring "factor"
# if you want it "by factor" (taking the interaction into account) you can write:
emmip(model,~continuous|factor ,at = list(continuous = c(1,2,3,4,5,6), type="response",CIs=TRUE, engine="ggplot" )
#All you have to do is search for the point crossing 70% then :) .

However, as noted, non-linear brms models might not directly translate to the emmeans architecture (I don't know), and there is a more elegant solution anyway:

1. A standard logistic function predicts 50% when the logit becomes 0 (before applying the exponential ratio rule; I ignore the fact that your gamma and lambda model terms absolutely destroy this property... :))
2. The "intercept" shifts the whole logit statically (or by factorial conditions), such that it indicates "where" 50% is predicted (in a given condition). For example, in standard models 1/(1+exp(intercept+varyingeffects)) the intercept says for which value of varyingeffects  the term becomes 0).
3. You can "make the intercept" to indicate a 70% prediction instead of a 50% prediction, if you add a constant on the logit level; that is: 1/(1+exp(-.8477)) = (about) 70%; and  1/(1+exp(-.8477+intercept+varyingeffects)) shifts the intercept by this constant, such that it now indicates the value of varyingeffects which predicts 70%. I guess. .. :)) There could be more detail to that (which I don't see right now), but it sure is a starting point.

Hope this helps, with your actual questions.
The rest seems to be a different matter.... (e.g., taking dependencies of tailored testing into account etc).

But one final note: I have once tried to fit simpler models with constructing the logit myself, like you do, and then setting,  family = bernoulli(link = "identity"), which never worked (it never converged). ... Just saying: I think Paul makes some points about the identifiability of those models in his vignettes, which you should check, if your model fails converging.
(But dropping lambda and gamma, might be worth considering in any case. If you simulate logistic functions hierarchically, then they do not approximate 100% on average (which would be the reason you use gamma and lambda), but the limited growth approximates e.g., 80 % depending on the individual variations in the slope parameters of the logistic function. This means, you don't need "maximum performance" parameters, but can approximate this behavior by the assumption of hierarchically clustered variance. Which also makes the model simpler... , and identifiable, and you could use the "elegant" way of determining 70%).


Best, Ree



Am Mo., 16. M?rz 2020 um 04:28 Uhr schrieb Ades, James <jades at health.ucsd.edu<mailto:jades at health.ucsd.edu>>:
Hi all,

Given that this is a mixed-model listserv, I'm hoping that a BRMS question might fit within that purview.

A quick synopsis of the dataset: there are 14 different conditions of executive function tasks ( ~1000 3rd, 5th, 7th graders). Given that these tasks use an adaptive paradigm (tasks might have anywhere from 25 to 40 trials), I'm trying to disentangle the response window at which participants reach a 70% performance threshold. There are four separate timepoints. (I'm not sure whether the four timepoints can be fit at once because probability distributions for random factor of participant are already used to account for repeated measures of participant completing 14 conditions, but that question is secondary to ensuring that I'm fitting one time point correctly and adequately extracting those the intercept/slope parameters).

If I were to only input this into glmer without the priors, I'd write the model as:
```
glmer(response ~  condition * norm + (norm | pid/condition)
```
(In a glmer model, I can extract intercept/slope parameters fine).

My current model is below. My question isn't so much with the psychometric function or the priors, which, besides the threshold, I've borrowed from Treutwein and Strasburger: https://link.springer.com/article/10.3758/BF03211951--though if there are contentions with any of the those, feel free to raise them--as it is whether I've correctly structured the non-linear parameters. The reason for modeling all four parameters is to minimize bias, but threshold is the only estimate that I'm concerned with. So regarding the multi-level structure, I've created parameters for lapse, guess, spread, and threshold. It seems reasonable to expect that threshold and spread will vary for every participant for every condition, while lapse and guessing (forced yes/no) will likely not differ much from condition to condition within participant (though if there are arguments that it would make for an improved model, I'm fine including lapse and guess parameters for every condition as well).

The other problem I'm having is using coef() or fixef()/ranef() to withdraw (or locate) the overall intercept and slope such that I can use the qlogis() function to determine the psychometric threshold at 70% (since I don't think it would be accurate to directly pull the 70% threshold estimate from the parameter itself?).

Does all of that make sense? This is all a little bit over my head and though I've culled Buerkner's item-response vignettes (Here: https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html and here: https://arxiv.org/pdf/1905.09501.pdf, they're similar but fundamentally different, so they only get me so far).

I've included a small sample of ~five participants here: https://drive.google.com/file/d/1YFnQRSjnp5hVziQx5wQzaIhn75KigaGx/view?usp=sharing

Thanks in advance for any and all help! Hope everyone is staying healthy!

James


```
thresholds <- bf(
  response ~ (gamma + (1 - lambda - gamma) * Phi((norm - threshold)/spread)),
  threshold ~ 1 + (1|p|pid) + (1|c|condition),
  logitgamma  ~ 1 + (1|p|pid),
  nlf(gamma ~ inv_logit(logitgamma)),
  logitlambda ~ 1 + (1|p|pid),
  nlf(lambda ~ inv_logit(logitlambda)),
  spread ~ 1 + (1|p|pid) + (1|c|condition),
nl = TRUE)

prior <-
  prior(beta(9, 3), class = "b", nlpar = "threshold", lb = 0, ub = 1) +
  prior(beta(1.4, 1.4), class = "b", nlpar = "spread", lb = .005, ub = .5) +
  prior(beta(.5, 8), nlpar = "logitlambda", lb = 0, ub = .1)+
  prior(beta(1, 5), nlpar = "logitgamma", lb = 0, ub = .1)

fit_thresholds <- brm(
  formula = thresholds,
  data = ace.threshold.t1.samp,
  family = bernoulli(link = "identity"),
  prior = prior,
  control = list(adapt_delta = .85, max_treedepth = 15),
  inits = 0,
  chains = 1,
  cores = 16
)
```



[https://media.springernature.com/w110/springer-static/cover/journal/13414.jpg]<https://link.springer.com/article/10.3758/BF03211951>
Fitting the psychometric function | SpringerLink<https://link.springer.com/article/10.3758/BF03211951>
A constrained generalized maximum likelihood routine for fitting psychometric functions is proposed, which determines optimum values for the complete parameter set?that is, threshold and slopeas well as for guessing and lapsing probability. The constraints are realized by Bayesian prior distributions for each of these parameters. The fit itself results from maximizing the posterior ...
link.springer.com<http://link.springer.com>

Abstract R arXiv:1905.09501v2 [stat.CO] 20 Jul 2019<https://arxiv.org/pdf/1905.09501.pdf>
Paul-Christian B urkner 3 dictions via a nested non-linear formula syntax, the implementation of several distributions designed for response times data, and extentions of distributions for ordinal data, for example
arxiv.org<http://arxiv.org>

Estimating Non-Linear Models with brms<https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html>
Introduction. This vignette provides an introduction on how to fit non-linear multilevel models with brms.Non-linear models are incredibly flexible and powerful, but require much more care with respect to model specification and priors than typical generalized linear models.
cran.r-project.org<http://cran.r-project.org>



        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From j@de@ @end|ng |rom he@|th@uc@d@edu  Mon Mar 16 22:47:38 2020
From: j@de@ @end|ng |rom he@|th@uc@d@edu (Ades, James)
Date: Mon, 16 Mar 2020 21:47:38 +0000
Subject: [R-sig-ME] Hierarchical Psychometric Function in BRMS
In-Reply-To: <DM5PR1901MB2007ED6A3460670F84E40537EAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>
References: <DM5PR1901MB2007F953970156594F62EBFAEAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>,
 <CADcpBHPFYfZcE+13BseKceTWZf0vU8tGF6pvjkMtYyh6_Zy=9A@mail.gmail.com>,
 <DM5PR1901MB2007ED6A3460670F84E40537EAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>
Message-ID: <DM5PR1901MB200773E896948650DCB58EB1EAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>

Just a quick follow-up; there are actually three other tasks but their adaptivity component isn't response window. One of them uses angle rotation of the target as the measure of difficulty (a precision WM task). The other two tasks are straight forward spatial span and backward span tasks, which are just object counts.
________________________________
From: Ades, James <jades at health.ucsd.edu>
Sent: Monday, March 16, 2020 2:44 PM
To: Ren? <bimonosom at gmail.com>
Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Hierarchical Psychometric Function in BRMS

Hi Ree,

Thanks for the response.

Responding to your questions:
1) Yes, essentially. So there are 7 tasks, some have two conditions. One has four conditions. This is the "condition" in the model. "Norm" is the normalized response window.

2) Yes, the response window for the following trials depends on whether the previous response is correct and was answered within the response window.

3) I'm not sure what you mean by "unmotivated," but hopefully I can provide some background that will give you a better idea. I'm hesitant about giving too much information for the sake of avoiding confusion, but the threshold was created to be 80%, but when I looked at proportion correct for participants many did not achieve this, so it seemed principled to extract thresholds at 70%. Ideally, the this performance threshold motivates performance (not too easy, but also not too hard). From there, we ask the question, what is the necessary RW for the participant to achieve 70% accuracy. This question is answered through the psychometric function. (In the Treutwein and Strasburger cited paper, they make the point that the psychometric function is best approximated using all four priors for threshold, spread, lapse, and guessing.

4) Yes, four sessions, completed over two years, equally spaced, more or less. I control for this in the model looking at executive function performance on standardized assessment outcome. I wasn't sure whether including timepoints within the psychometric function model would lead to more accurate estimation of participant psychometric functions.

Hopefully, that information helps.

Regarding your final point on convergence: as I'm sure you know, fitting this model with this data is no small feat. Using UCSD's super computer, it takes a little over a day. It did seem to converge though. You then write "(But dropping lambda and gamma, might be worth considering in any case. If you simulate logistic functions hierarchically, then they do not approximate 100% on average (which would be the reason you use gamma and lambda), but the limited growth approximates e.g., 80 % depending on the individual variations in the slope parameters of the logistic function. This means, you don't need "maximum performance" parameters, but can approximate this behavior by the assumption of hierarchically clustered variance. Which also makes the model simpler... , and identifiable, and you could use the "elegant" way of determining 70%)." So this is where I am mathematically over my head. Re Treut and Straus--they're claim is that the most principled approach to approximating the psychometric function of an adaptive paradigm is using prior on all four parameters. Is your argument that if you're using a hierarchical approach, you wouldn't need the gamma/lambda parameters? Can you say more about this or point me to an article that discusses the assumption of hierarchically clustered variance?

Thank you for the parameter extraction methods. I guess we'll figure out which one when we come to that road. Elegant is always nice. But I think the first think is making sure that I have the most principled and correct model. Is the one I currently have in BRMS correct given the clarifications above?

Much thanks!

James

________________________________
From: Ren? <bimonosom at gmail.com>
Sent: Monday, March 16, 2020 2:10 AM
To: Ades, James <jades at health.ucsd.edu>
Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Hierarchical Psychometric Function in BRMS

Hi James,

since I am working with brms and glmer, I feel I should be able to give a response (although addressing Paul in the Stan-Forum might be a better option), there seem to be two questions, and some missing details, that might lead to even more questions.... let's begin....

My questions:
1. "14 executive functions". Does this mean every participant completed each of 14 tasks supposed to measure different facets of the general construct "executive functions in working memory"? (If not, please clarify). What term is this in the model "condition" or "norm"? (Given that you have random slopes for "norm" it seems to be "norm" ?) Then what is condition?

2. "adaptive tasks with 25 to 40 trials" Does this mean "tailored testing"? (I.e., the trial that comes next within the task depends on the decisions (their error) from all previous trials?)

3. "Goal: disentangle the response window at which participants reach a 70%", - if you have tailored testing (I am not sure), which already is designed to sort trials to meander around 75% accuracy for maximum information/variance , this threshold seems a bit unmotivated, can you give more background?

4. "four different time points" , I suppose these are four sessions, in each the participants have completed subsets of the 14 tasks

Your (secondary) questions (I ignore points 1 to 3 now, but they need clarification):
"I'm not sure whether the four timepoints can be fit at once because probability distributions for random factor of participant are already used to account for repeated measures of participant completing 14 conditions)."
My answer:
- Regardless of the technical details:  First, "time points"  has only four levels, thus, it would not make sense to separate their "random" intercepts from other variance sources in the design, no matter which. Computing standard deviations of a distribution for which you only have 4 observations/levels is problematic. Second, nonetheless assuming that "time points" (e.g., increasing ability over time) has an effect, then controlling for it is pretty legit, so, it makes sense to include "time points" into the fixed effects. Also legit.

5. "The other problem I'm having is using coef() or fixef()/ranef() to withdraw (or locate) the overall intercept and slope such that I can use the qlogis() function to determine the psychometric threshold at 70% (since I don't think it would be accurate to directly pull the 70% threshold estimate from the parameter itself?)."
My answer:
- Do you mean, by 70% threshold, the "location" on the predictor(s) (the logit) at which the predicted probably of the response is 70%? (Please keep in mind, that you have two interacting predictors in your model, which means getting these estimates for one predictor requires to either ignore variance of the other predictor, which needs theoretical clarification if you want to interpret this; or taking it into account - see below.) Anyway, the "manual" way to do this, is to make predictions, based on the coefficients, and then search the point of crossing 70%. For this you want to use the "emmeans" package which works for both glmer and brms (but I am not sure whether it works also for the non-linear models; if not, you need to ask Paul Buerkner in the Stan forum how to do it ;)); it sure works with standard hierarchical regression output from brms.) . In the emmeans package you find the function "emmip", which is what you desire.

#assuming this is your model with a continuous predictor ("continuous") and a factorial predictor ("factor"):
model<-glmer(response ~  continuous * factor + (continuous | pid))
emmip(model,~continuous,at = list(continuous = c(1,2,3,4,5,6), type="response",CIs=TRUE, engine="ggplot" )
# this gives you the probability predictions for "continuous" from 1 to 6 (you can make these as "fine" as you want), while ignoring "factor"
# if you want it "by factor" (taking the interaction into account) you can write:
emmip(model,~continuous|factor ,at = list(continuous = c(1,2,3,4,5,6), type="response",CIs=TRUE, engine="ggplot" )
#All you have to do is search for the point crossing 70% then :) .

However, as noted, non-linear brms models might not directly translate to the emmeans architecture (I don't know), and there is a more elegant solution anyway:

1. A standard logistic function predicts 50% when the logit becomes 0 (before applying the exponential ratio rule; I ignore the fact that your gamma and lambda model terms absolutely destroy this property... :))
2. The "intercept" shifts the whole logit statically (or by factorial conditions), such that it indicates "where" 50% is predicted (in a given condition). For example, in standard models 1/(1+exp(intercept+varyingeffects)) the intercept says for which value of varyingeffects  the term becomes 0).
3. You can "make the intercept" to indicate a 70% prediction instead of a 50% prediction, if you add a constant on the logit level; that is: 1/(1+exp(-.8477)) = (about) 70%; and  1/(1+exp(-.8477+intercept+varyingeffects)) shifts the intercept by this constant, such that it now indicates the value of varyingeffects which predicts 70%. I guess. .. :)) There could be more detail to that (which I don't see right now), but it sure is a starting point.

Hope this helps, with your actual questions.
The rest seems to be a different matter.... (e.g., taking dependencies of tailored testing into account etc).

But one final note: I have once tried to fit simpler models with constructing the logit myself, like you do, and then setting,  family = bernoulli(link = "identity"), which never worked (it never converged). ... Just saying: I think Paul makes some points about the identifiability of those models in his vignettes, which you should check, if your model fails converging.
(But dropping lambda and gamma, might be worth considering in any case. If you simulate logistic functions hierarchically, then they do not approximate 100% on average (which would be the reason you use gamma and lambda), but the limited growth approximates e.g., 80 % depending on the individual variations in the slope parameters of the logistic function. This means, you don't need "maximum performance" parameters, but can approximate this behavior by the assumption of hierarchically clustered variance. Which also makes the model simpler... , and identifiable, and you could use the "elegant" way of determining 70%).


Best, Ree



Am Mo., 16. M?rz 2020 um 04:28 Uhr schrieb Ades, James <jades at health.ucsd.edu<mailto:jades at health.ucsd.edu>>:
Hi all,

Given that this is a mixed-model listserv, I'm hoping that a BRMS question might fit within that purview.

A quick synopsis of the dataset: there are 14 different conditions of executive function tasks ( ~1000 3rd, 5th, 7th graders). Given that these tasks use an adaptive paradigm (tasks might have anywhere from 25 to 40 trials), I'm trying to disentangle the response window at which participants reach a 70% performance threshold. There are four separate timepoints. (I'm not sure whether the four timepoints can be fit at once because probability distributions for random factor of participant are already used to account for repeated measures of participant completing 14 conditions, but that question is secondary to ensuring that I'm fitting one time point correctly and adequately extracting those the intercept/slope parameters).

If I were to only input this into glmer without the priors, I'd write the model as:
```
glmer(response ~  condition * norm + (norm | pid/condition)
```
(In a glmer model, I can extract intercept/slope parameters fine).

My current model is below. My question isn't so much with the psychometric function or the priors, which, besides the threshold, I've borrowed from Treutwein and Strasburger: https://link.springer.com/article/10.3758/BF03211951--though if there are contentions with any of the those, feel free to raise them--as it is whether I've correctly structured the non-linear parameters. The reason for modeling all four parameters is to minimize bias, but threshold is the only estimate that I'm concerned with. So regarding the multi-level structure, I've created parameters for lapse, guess, spread, and threshold. It seems reasonable to expect that threshold and spread will vary for every participant for every condition, while lapse and guessing (forced yes/no) will likely not differ much from condition to condition within participant (though if there are arguments that it would make for an improved model, I'm fine including lapse and guess parameters for every condition as well).

The other problem I'm having is using coef() or fixef()/ranef() to withdraw (or locate) the overall intercept and slope such that I can use the qlogis() function to determine the psychometric threshold at 70% (since I don't think it would be accurate to directly pull the 70% threshold estimate from the parameter itself?).

Does all of that make sense? This is all a little bit over my head and though I've culled Buerkner's item-response vignettes (Here: https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html and here: https://arxiv.org/pdf/1905.09501.pdf, they're similar but fundamentally different, so they only get me so far).

I've included a small sample of ~five participants here: https://drive.google.com/file/d/1YFnQRSjnp5hVziQx5wQzaIhn75KigaGx/view?usp=sharing

Thanks in advance for any and all help! Hope everyone is staying healthy!

James


```
thresholds <- bf(
  response ~ (gamma + (1 - lambda - gamma) * Phi((norm - threshold)/spread)),
  threshold ~ 1 + (1|p|pid) + (1|c|condition),
  logitgamma  ~ 1 + (1|p|pid),
  nlf(gamma ~ inv_logit(logitgamma)),
  logitlambda ~ 1 + (1|p|pid),
  nlf(lambda ~ inv_logit(logitlambda)),
  spread ~ 1 + (1|p|pid) + (1|c|condition),
nl = TRUE)

prior <-
  prior(beta(9, 3), class = "b", nlpar = "threshold", lb = 0, ub = 1) +
  prior(beta(1.4, 1.4), class = "b", nlpar = "spread", lb = .005, ub = .5) +
  prior(beta(.5, 8), nlpar = "logitlambda", lb = 0, ub = .1)+
  prior(beta(1, 5), nlpar = "logitgamma", lb = 0, ub = .1)

fit_thresholds <- brm(
  formula = thresholds,
  data = ace.threshold.t1.samp,
  family = bernoulli(link = "identity"),
  prior = prior,
  control = list(adapt_delta = .85, max_treedepth = 15),
  inits = 0,
  chains = 1,
  cores = 16
)
```



[https://media.springernature.com/w110/springer-static/cover/journal/13414.jpg]<https://link.springer.com/article/10.3758/BF03211951>
Fitting the psychometric function | SpringerLink<https://link.springer.com/article/10.3758/BF03211951>
A constrained generalized maximum likelihood routine for fitting psychometric functions is proposed, which determines optimum values for the complete parameter set?that is, threshold and slopeas well as for guessing and lapsing probability. The constraints are realized by Bayesian prior distributions for each of these parameters. The fit itself results from maximizing the posterior ...
link.springer.com<http://link.springer.com>

Abstract R arXiv:1905.09501v2 [stat.CO] 20 Jul 2019<https://arxiv.org/pdf/1905.09501.pdf>
Paul-Christian B urkner 3 dictions via a nested non-linear formula syntax, the implementation of several distributions designed for response times data, and extentions of distributions for ordinal data, for example
arxiv.org<http://arxiv.org>

Estimating Non-Linear Models with brms<https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html>
Introduction. This vignette provides an introduction on how to fit non-linear multilevel models with brms.Non-linear models are incredibly flexible and powerful, but require much more care with respect to model specification and priors than typical generalized linear models.
cran.r-project.org<http://cran.r-project.org>



        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From b|mono@om @end|ng |rom gm@||@com  Tue Mar 17 09:48:20 2020
From: b|mono@om @end|ng |rom gm@||@com (=?UTF-8?B?UmVuw6k=?=)
Date: Tue, 17 Mar 2020 09:48:20 +0100
Subject: [R-sig-ME] Hierarchical Psychometric Function in BRMS
In-Reply-To: <DM5PR1901MB200773E896948650DCB58EB1EAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>
References: <DM5PR1901MB2007F953970156594F62EBFAEAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <CADcpBHPFYfZcE+13BseKceTWZf0vU8tGF6pvjkMtYyh6_Zy=9A@mail.gmail.com>
 <DM5PR1901MB2007ED6A3460670F84E40537EAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <DM5PR1901MB200773E896948650DCB58EB1EAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>
Message-ID: <CADcpBHPvR6izCGq5heOk6bf0SNtNq5v697kk0R1ON2fYcJgDpg@mail.gmail.com>

Hey James,

thank you for these details. Step by step:

"1) Yes, essentially. So there are 7 tasks, some have two conditions. One
has four conditions. This is the "condition" in the model. "Norm" is the
normalized response window."
R1) I am sorry, I do not understand this. Does "condition" indicate the 14
tasks (i.e., with 14 factor levels) or the "some have two, some have four
conditions part?" If it is the latter, then why did you not include 7
"tasks" alternatively ? - Anyway - I actually would suggest using the 14
tasks as "condition", because the design matrix is not fully crossed.
(i.e., without any design, just all tasks; you still can perform post-hoc
comparisons).

2) The term response window is not self-explaining..., but I assume you
mean "time pressure" by this (how long do I have to give a response). And I
will go on to refer to this as such.
2b) Given "norm" is "time" then I can finally see where you want to go.
(Please correct me if I am wrong:

3. No offense, my choice of words was a bit clumsy. I mean a
clarification about the research question or psychological hypothesis about
which measure should predict another measure is always helpful to make
judgments about a models appropriateness. As noted: I get a grip now, and
it seems, you want to predict decision accuracy ("response") based on the
task ("condition") and the time provided to solve the task ("norm"). While
"norm" is a time window to complete the task, dynamically changing
depending on the accuracy (tailored testing). Now having spelled this out
reveals a circular causation in it: accuracy -> time window -> accuracy? It
would be good to search for a reference paper which used an equivalent
design (not just psychometric function). But to put it this way: Accuracy
(response) is not really informative, because the tasks (if they are
tailored) are -specifically designed- to that each participant has about
75% accuracy. That is, everybody will either pass a threshold (e.g., 70%)
or not (e.g., 80%), because everybody will be at 75%.  What IS informative
is how much time they need for achieving this. The underlying assumption is
that there is a level of "processing speed" which is just before I become
perfectly accurate, and the goal is to find this moment, because if I WOULD
(otherwise) be perfectly accurate in every task my ability is
unidentifiable (because the tasks were not difficult enough, or
statistically speaking: no variance), - but if I was only guessing then any
model about me is uninformative (guessing model).

3b. In other words, if you are searching for a latent ability that you want
to continuously describe in your sample, "response window" (time needed) is
the indicator. slow participants = low ability ; quick participants = high
ability.
In Item-Response-Theory you usually estimate the ability, while presenting
the same tasks to all participants (fully crossed) which allows to estimate
task difficulty (instead of manipulating it), and I would suggest searching
for related model solutions in this area. (I am not experienced in tailored
testing).

4. If you standardize the measurements within each of the four sessions,
then I would say there is no reason to further include the term in the
model. This, however, is a matter of theoretical rather than statistical
debate. One theoretical counter-argument could be: If you do not
standardize the measures, but simply include time-points as fixed effects
in the model, then you gain information (i.e., about the time effect),
without altering the content of your model (although you change a fixed
assumption - to a freely estimable one). You then could also take into
account, that some participants improve more quickly then others, which
would be a reasonable thing to do, if you think, that this is a thing.

5. What Treutwein and Strasburger write is, first, mainly about logistic
functions which have the most basic form of a one - parameter Rasch model.
Make a two-parameter Rasch model out of it, then you have the functional
form of standard logistic regression, as also performed in "lmer" and
"brms" if you write something like:
DV~Interceptvariable*Continuousvariable+(1|subjectID) + (1|trialID),
family=binomial(link=logit). with two differences 1) the R packages use a
different parameterization (e.g. dummy coding) 2) in Rasch models (or Item
Response Theory) you estimate the model terms based on items and
individuals, rather than predicting the DV based on conditions and
measurements (here is a paper that investigates the relation between
logistic models to predict accuracy and item response theory: Dixon, 2008,
Models of accuracy in repeated-measures designs). This should help getting
a "feeling" for the logistic function.

Then what Treutwein and Strasburger introduce can also be found in every
text-book namely gamma, which is a guessing parameter (gamma +
1/(1+exp(...))) which says the model can not predict 0 accuracy unless
gamma = 0, because something will always be`correct' by chance. Secondly,
however, adding gamma would lead the model to predictions larger than 1,
for why there is (1-gamma) involved. Third, the model assumes that 100%
accuracy might not be reached (for whatever reason), and lambda is
introduced to scale the model down again, giving,
gamma+(1-gamma-lambda)/...) which means the output of the logistic function
(1/(1+exp(beta(theta+x)))) is squashed between gamma and lambda.
Unfortunately, if you would try to estimate one value for each gamma,
lambda, and beta (or 1/sigma) for a single participant then the model is
simply unidentifiable because predicting a participants average behavior
(or deviation from something else) of - say 70% - can be achieved by
gamma=.3 (and lambda=0), or lambda=.3 (and gamma=0) while the logistic
function is 0 for theta... ; OR theta = -.847 (and gamma =0; lambda0) --
you see where this is going, right? I agree that it might be reasonable to
assume that participants "guess" sometimes, but this is not a matter of
estimation but a matter of your task. In a binary task gamma= .5 (lowest
probability of being correct); in a task with three responses gamma=1/3.
Measurement not required, just statistics. And the lambda parameter,
finally, is not necessary, because on the individual level it is (almost)
redundant with beta (or 1/sigma) - coming back to my initial argument. On
the average it might sometimes "look like" you can draw a horizontal line
at p=.8 to which the logistic function (on average) approaches. And one
could argue this justifies assuming a maximum of lambda=.8. However, simply
assuming hierarchical variation  in beta (or 1/sigma) either within a
participants across trials and/or tasks (or variation of beta (or
1/sigma) within a task across participants), on average, will never predict
p=1 without lambda being required, and thus provides a "natural"
performance cap, measured in terms of variation, not in terms of lambda.
Having both, again is not identifiable (in addition to the issues above).
Also, -if- "guessing" would vary between participants, then, I would argue,
one should think about the amount of trials (or which trials) in which they
guess, not about the percent being correct while guessing (which is defined
by the task at hand).

6. Finally, that all being said, I would suggest you use this model:

thresholds <- bf(
  norms ~ 0 +ability + task,
  ability ~ 0+(1|subjectID),
nl = TRUE)

## time taken to reach 75% accuracy is predicted (i.e. "norms") by the
participants 'constant' ability, while including variations over tasks
(depending on the task).
 # task estimates task difficulty - should be a factor coding all 14 tasks
(you still can compare them directly afterwards)
 # ability is a "linear" predictor, freely estimated, one for each
participant
# without intercepts (i.e., 0 in front of the formulas), the task will be
interpretable as task-specific intercepts (like grand thetas) and the
abilities centered around 0. If you "scale" norms beforehand (i.e., across
tasks, not within) to SD=1, then the prior for "ability" should be
Gaussian(0,1) as well. Voila, very simply measurement model :). You could
include more terms like time-point to control/test for training effects.

afterwards you can get the task and participant posterior estimates for
ability (I think) like this:
posterior_samples(modeloutput)
with different indices for the participants in the matrix. You then also
can directly compare single task-estimates with each other (and get Bayes
factors to check whether their difficulties differ, using a "slab-only"
approach, instead of "spike-and-slab", check the recent work of Rouder),

I can not see right now, why this should be any more complicated :) , as it
provides you with the information you want: "How much ability the
participant has" based on reaching the tailored testing performance of 75%
accuracy with a specific amount of time pressure, while controlling for
task difficulty. This also should lower the computational requirements :)


Otherwise, if you can provide a paper which estimated:
item difficulty (i.e., trial-wise), based on time pressure...
task difficulty (the 14 ones)
participant ability (unknown)
based on binary responses
in a tailored testing design

then please let me know. Sounds interesting in any case.

At least this is what I would say 'spontaneously' :))

Hope this helps,
Best Ren?


Am Mo., 16. M?rz 2020 um 22:47 Uhr schrieb Ades, James <
jades at health.ucsd.edu>:

> Just a quick follow-up; there are actually three other tasks but their
> adaptivity component isn't response window. One of them uses angle rotation
> of the target as the measure of difficulty (a precision WM task). The other
> two tasks are straight forward spatial span and backward span tasks, which
> are just object counts.
> ------------------------------
> *From:* Ades, James <jades at health.ucsd.edu>
> *Sent:* Monday, March 16, 2020 2:44 PM
> *To:* Ren? <bimonosom at gmail.com>
> *Cc:* r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> *Subject:* Re: [R-sig-ME] Hierarchical Psychometric Function in BRMS
>
> Hi Ree,
>
> Thanks for the response.
>
> Responding to your questions:
> 1) Yes, essentially. So there are 7 tasks, some have two conditions. One
> has four conditions. This is the "condition" in the model. "Norm" is the
> normalized response window.
>
> 2) Yes, the response window for the following trials depends on whether
> the previous response is correct and was answered within the response
> window.
>
> 3) I'm not sure what you mean by "unmotivated," but hopefully I can
> provide some background that will give you a better idea. I'm hesitant
> about giving too much information for the sake of avoiding confusion, but
> the threshold was created to be 80%, but when I looked at proportion
> correct for participants many did not achieve this, so it seemed principled
> to extract thresholds at 70%. Ideally, the this performance threshold
> motivates performance (not too easy, but also not too hard). From there, we
> ask the question, what is the necessary RW for the participant to achieve
> 70% accuracy. This question is answered through the psychometric function.
> (In the Treutwein and Strasburger cited paper, they make the point that the
> psychometric function is best approximated using all four priors for
> threshold, spread, lapse, and guessing.
>
> 4) Yes, four sessions, completed over two years, equally spaced, more or
> less. I control for this in the model looking at executive function
> performance on standardized assessment outcome. I wasn't sure whether
> including timepoints within the psychometric function model would lead to
> more accurate estimation of participant psychometric functions.
>
> Hopefully, that information helps.
>
> Regarding your final point on convergence: as I'm sure you know, fitting
> this model with this data is no small feat. Using UCSD's super computer, it
> takes a little over a day. It did seem to converge though. You then write "(But
> dropping lambda and gamma, might be worth considering in any case. If you
> simulate logistic functions hierarchically, then they do not approximate
> 100% on average (which would be the reason you use gamma and lambda), but
> the limited growth approximates e.g., 80 % depending on the individual
> variations in the slope parameters of the logistic function. This means,
> you don't need "maximum performance" parameters, but can approximate this
> behavior by the assumption of hierarchically clustered variance. Which also
> makes the model simpler... , and identifiable, and you could use the
> "elegant" way of determining 70%)." So this is where I am mathematically
> over my head. Re Treut and Straus--they're claim is that the most
> principled approach to approximating the psychometric function of an
> adaptive paradigm is using prior on all four parameters. Is your argument
> that if you're using a hierarchical approach, you wouldn't need the
> gamma/lambda parameters? Can you say more about this or point me to an
> article that discusses the assumption of hierarchically clustered variance?
>
> Thank you for the parameter extraction methods. I guess we'll figure out
> which one when we come to that road. Elegant is always nice. But I think
> the first think is making sure that I have the most principled and correct
> model. Is the one I currently have in BRMS correct given the clarifications
> above?
>
> Much thanks!
>
> James
>
> ------------------------------
> *From:* Ren? <bimonosom at gmail.com>
> *Sent:* Monday, March 16, 2020 2:10 AM
> *To:* Ades, James <jades at health.ucsd.edu>
> *Cc:* r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> *Subject:* Re: [R-sig-ME] Hierarchical Psychometric Function in BRMS
>
> Hi James,
>
> since I am working with brms and glmer, I feel I should be able to give a
> response (although addressing Paul in the Stan-Forum might be
> a better option), there seem to be two questions, and some missing details,
> that might lead to even more questions.... let's begin....
>
> My questions:
> 1. "14 executive functions". Does this mean every participant completed
> each of 14 tasks supposed to measure different facets of the general
> construct "executive functions in working memory"? (If not, please
> clarify). What term is this in the model "condition" or "norm"? (Given that
> you have random slopes for "norm" it seems to be "norm" ?) Then what is
> condition?
>
> 2. "adaptive tasks with 25 to 40 trials" Does this mean "tailored
> testing"? (I.e., the trial that comes next within the task depends on the
> decisions (their error) from all previous trials?)
>
> 3. "Goal: disentangle the response window at which participants reach a
> 70%", - if you have tailored testing (I am not sure), which already is
> designed to sort trials to meander around 75% accuracy for maximum
> information/variance , this threshold seems a bit unmotivated, can you give
> more background?
>
> 4. "four different time points" , I suppose these are four sessions, in
> each the participants have completed subsets of the 14 tasks
>
> Your (secondary) questions (I ignore points 1 to 3 now, but they need
> clarification):
> "I'm not sure whether the four timepoints can be fit at once because
> probability distributions for random factor of participant are already used
> to account for repeated measures of participant completing 14 conditions)."
> My answer:
> - Regardless of the technical details:  First, "time points"  has only
> four levels, thus, it would not make sense to separate their "random"
> intercepts from other variance sources in the design, no matter which.
> Computing standard deviations of a distribution for which you only have 4
> observations/levels is problematic. Second, nonetheless assuming that "time
> points" (e.g., increasing ability over time) has an effect, then
> controlling for it is pretty legit, so, it makes sense to include "time
> points" into the fixed effects. Also legit.
>
> 5. "The other problem I'm having is using coef() or fixef()/ranef() to
> withdraw (or locate) the overall intercept and slope such that I can use
> the qlogis() function to determine the psychometric threshold at 70% (since
> I don't think it would be accurate to directly pull the 70% threshold
> estimate from the parameter itself?)."
> My answer:
> - Do you mean, by 70% threshold, the "location" on the predictor(s) (the
> logit) at which the predicted probably of the response is 70%? (Please keep
> in mind, that you have two interacting predictors in your model, which
> means getting these estimates for one predictor requires to either ignore
> variance of the other predictor, which needs theoretical clarification if
> you want to interpret this; or taking it into account - see below.) Anyway,
> the "manual" way to do this, is to make predictions, based on the
> coefficients, and then search the point of crossing 70%. For this you want
> to use the "emmeans" package which works for both glmer and brms (but I am
> not sure whether it works also for the non-linear models; if not, you need
> to ask Paul Buerkner in the Stan forum how to do it ;)); it sure works with
> standard hierarchical regression output from brms.) . In the emmeans
> package you find the function "emmip", which is what you desire.
>
> #assuming this is your model with a continuous predictor ("continuous")
> and a factorial predictor ("factor"):
> model<-glmer(response ~  continuous * factor + (continuous | pid))
> emmip(model,~continuous,at = list(continuous = c(1,2,3,4,5,6),
> type="response",CIs=TRUE, engine="ggplot" )
> # this gives you the probability predictions for "continuous" from 1 to 6
> (you can make these as "fine" as you want), while ignoring "factor"
> # if you want it "by factor" (taking the interaction into account) you can
> write:
> emmip(model,~continuous|factor ,at = list(continuous = c(1,2,3,4,5,6),
> type="response",CIs=TRUE, engine="ggplot" )
> #All you have to do is search for the point crossing 70% then :) .
>
> However, as noted, non-linear brms models might not directly translate to
> the emmeans architecture (I don't know), and there is a more elegant
> solution anyway:
>
> 1. A standard logistic function predicts 50% when the logit becomes 0
> (before applying the exponential ratio rule; I ignore the fact that your
> gamma and lambda model terms absolutely destroy this property... :))
> 2. The "intercept" shifts the whole logit statically (or by factorial
> conditions), such that it indicates "where" 50% is predicted (in a given
> condition). For example, in standard models
> 1/(1+exp(intercept+varyingeffects)) the intercept says for which value of
> varyingeffects  the term becomes 0).
> 3. You can "make the intercept" to indicate a 70% prediction instead of a
> 50% prediction, if you add a constant on the logit level; that is:
> 1/(1+exp(-.8477)) = (about) 70%; and
>  1/(1+exp(-.8477+intercept+varyingeffects)) shifts the intercept by this
> constant, such that it now indicates the value of varyingeffects which
> predicts 70%. I guess. .. :)) There could be more detail to that (which I
> don't see right now), but it sure is a starting point.
>
> Hope this helps, with your actual questions.
> The rest seems to be a different matter.... (e.g., taking dependencies of
> tailored testing into account etc).
>
> But one final note: I have once tried to fit simpler models with
> constructing the logit myself, like you do, and then setting,  family =
> bernoulli(link = "identity"), which never worked (it never converged). ...
> Just saying: I think Paul makes some points about the identifiability of
> those models in his vignettes, which you should check, if your model fails
> converging.
> (But dropping lambda and gamma, might be worth considering in any case. If
> you simulate logistic functions hierarchically, then they do not
> approximate 100% on average (which would be the reason you use gamma and
> lambda), but the limited growth approximates e.g., 80 % depending on the
> individual variations in the slope parameters of the logistic function.
> This means, you don't need "maximum performance" parameters, but can
> approximate this behavior by the assumption of hierarchically clustered
> variance. Which also makes the model simpler... , and identifiable, and you
> could use the "elegant" way of determining 70%).
>
>
> Best, Ree
>
>
>
> Am Mo., 16. M?rz 2020 um 04:28 Uhr schrieb Ades, James <
> jades at health.ucsd.edu>:
>
> Hi all,
>
> Given that this is a mixed-model listserv, I'm hoping that a BRMS question
> might fit within that purview.
>
> A quick synopsis of the dataset: there are 14 different conditions of
> executive function tasks ( ~1000 3rd, 5th, 7th graders). Given that these
> tasks use an adaptive paradigm (tasks might have anywhere from 25 to 40
> trials), I'm trying to disentangle the response window at which
> participants reach a 70% performance threshold. There are four separate
> timepoints. (I'm not sure whether the four timepoints can be fit at once
> because probability distributions for random factor of participant are
> already used to account for repeated measures of participant completing 14
> conditions, but that question is secondary to ensuring that I'm fitting one
> time point correctly and adequately extracting those the intercept/slope
> parameters).
>
> If I were to only input this into glmer without the priors, I'd write the
> model as:
> ```
> glmer(response ~  condition * norm + (norm | pid/condition)
> ```
> (In a glmer model, I can extract intercept/slope parameters fine).
>
> My current model is below. My question isn't so much with the psychometric
> function or the priors, which, besides the threshold, I've borrowed from
> Treutwein and Strasburger:
> https://link.springer.com/article/10.3758/BF03211951--though if there are
> contentions with any of the those, feel free to raise them--as it is
> whether I've correctly structured the non-linear parameters. The reason for
> modeling all four parameters is to minimize bias, but threshold is the only
> estimate that I'm concerned with. So regarding the multi-level structure,
> I've created parameters for lapse, guess, spread, and threshold. It seems
> reasonable to expect that threshold and spread will vary for every
> participant for every condition, while lapse and guessing (forced yes/no)
> will likely not differ much from condition to condition within participant
> (though if there are arguments that it would make for an improved model,
> I'm fine including lapse and guess parameters for every condition as well).
>
> The other problem I'm having is using coef() or fixef()/ranef() to
> withdraw (or locate) the overall intercept and slope such that I can use
> the qlogis() function to determine the psychometric threshold at 70% (since
> I don't think it would be accurate to directly pull the 70% threshold
> estimate from the parameter itself?).
>
> Does all of that make sense? This is all a little bit over my head and
> though I've culled Buerkner's item-response vignettes (Here:
> https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html
> and here: https://arxiv.org/pdf/1905.09501.pdf, they're similar but
> fundamentally different, so they only get me so far).
>
> I've included a small sample of ~five participants here:
> https://drive.google.com/file/d/1YFnQRSjnp5hVziQx5wQzaIhn75KigaGx/view?usp=sharing
>
> Thanks in advance for any and all help! Hope everyone is staying healthy!
>
> James
>
>
> ```
> thresholds <- bf(
>   response ~ (gamma + (1 - lambda - gamma) * Phi((norm -
> threshold)/spread)),
>   threshold ~ 1 + (1|p|pid) + (1|c|condition),
>   logitgamma  ~ 1 + (1|p|pid),
>   nlf(gamma ~ inv_logit(logitgamma)),
>   logitlambda ~ 1 + (1|p|pid),
>   nlf(lambda ~ inv_logit(logitlambda)),
>   spread ~ 1 + (1|p|pid) + (1|c|condition),
> nl = TRUE)
>
> prior <-
>   prior(beta(9, 3), class = "b", nlpar = "threshold", lb = 0, ub = 1) +
>   prior(beta(1.4, 1.4), class = "b", nlpar = "spread", lb = .005, ub = .5)
> +
>   prior(beta(.5, 8), nlpar = "logitlambda", lb = 0, ub = .1)+
>   prior(beta(1, 5), nlpar = "logitgamma", lb = 0, ub = .1)
>
> fit_thresholds <- brm(
>   formula = thresholds,
>   data = ace.threshold.t1.samp,
>   family = bernoulli(link = "identity"),
>   prior = prior,
>   control = list(adapt_delta = .85, max_treedepth = 15),
>   inits = 0,
>   chains = 1,
>   cores = 16
> )
> ```
>
>
>
> [
> https://media.springernature.com/w110/springer-static/cover/journal/13414.jpg
> ]<https://link.springer.com/article/10.3758/BF03211951>
> Fitting the psychometric function | SpringerLink<
> https://link.springer.com/article/10.3758/BF03211951>
> A constrained generalized maximum likelihood routine for fitting
> psychometric functions is proposed, which determines optimum values for the
> complete parameter set?that is, threshold and slopeas well as for guessing
> and lapsing probability. The constraints are realized by Bayesian prior
> distributions for each of these parameters. The fit itself results from
> maximizing the posterior ...
> link.springer.com
>
> Abstract R arXiv:1905.09501v2 [stat.CO] 20 Jul 2019<
> https://arxiv.org/pdf/1905.09501.pdf>
> Paul-Christian B urkner 3 dictions via a nested non-linear formula syntax,
> the implementation of several distributions designed for response times
> data, and extentions of distributions for ordinal data, for example
> arxiv.org
>
> Estimating Non-Linear Models with brms<
> https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html
> >
> Introduction. This vignette provides an introduction on how to fit
> non-linear multilevel models with brms.Non-linear models are incredibly
> flexible and powerful, but require much more care with respect to model
> specification and priors than typical generalized linear models.
> cran.r-project.org
>
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>

	[[alternative HTML version deleted]]


From M@@rten@Jung @end|ng |rom m@||box@tu-dre@den@de  Tue Mar 17 10:47:56 2020
From: M@@rten@Jung @end|ng |rom m@||box@tu-dre@den@de (Maarten Jung)
Date: Tue, 17 Mar 2020 10:47:56 +0100
Subject: [R-sig-ME] Hierarchical Psychometric Function in BRMS
In-Reply-To: <ee34e71feae145e8a7692811aece362e@MSX-L104.msx.ad.zih.tu-dresden.de>
References: <DM5PR1901MB2007F953970156594F62EBFAEAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <CADcpBHPFYfZcE+13BseKceTWZf0vU8tGF6pvjkMtYyh6_Zy=9A@mail.gmail.com>
 <DM5PR1901MB2007ED6A3460670F84E40537EAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <DM5PR1901MB200773E896948650DCB58EB1EAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <ee34e71feae145e8a7692811aece362e@MSX-L104.msx.ad.zih.tu-dresden.de>
Message-ID: <CAHr4DyfuuJXVZqadUiES40XtAU7ZvbQRdQvZ-MavRB8ejfs_Ug@mail.gmail.com>

I don't know if this adds anything new at this point of the
discussion, but fwiw the paper "The Estimation of Item Response Models
with the lmer Function from the lme4 Package in R" by De Boeck et al.
(2011, [1]) might provide additional insights into how different
(types of) item response models translate into lme4/brms models.

[1] https://www.jstatsoft.org/article/view/v039i12

Best,
Maarten


On Tue, Mar 17, 2020 at 9:49 AM Ren? <bimonosom at gmail.com> wrote:
>
> Hey James,
>
> thank you for these details. Step by step:
>
> "1) Yes, essentially. So there are 7 tasks, some have two conditions. One
> has four conditions. This is the "condition" in the model. "Norm" is the
> normalized response window."
> R1) I am sorry, I do not understand this. Does "condition" indicate the 14
> tasks (i.e., with 14 factor levels) or the "some have two, some have four
> conditions part?" If it is the latter, then why did you not include 7
> "tasks" alternatively ? - Anyway - I actually would suggest using the 14
> tasks as "condition", because the design matrix is not fully crossed.
> (i.e., without any design, just all tasks; you still can perform post-hoc
> comparisons).
>
> 2) The term response window is not self-explaining..., but I assume you
> mean "time pressure" by this (how long do I have to give a response). And I
> will go on to refer to this as such.
> 2b) Given "norm" is "time" then I can finally see where you want to go.
> (Please correct me if I am wrong:
>
> 3. No offense, my choice of words was a bit clumsy. I mean a
> clarification about the research question or psychological hypothesis about
> which measure should predict another measure is always helpful to make
> judgments about a models appropriateness. As noted: I get a grip now, and
> it seems, you want to predict decision accuracy ("response") based on the
> task ("condition") and the time provided to solve the task ("norm"). While
> "norm" is a time window to complete the task, dynamically changing
> depending on the accuracy (tailored testing). Now having spelled this out
> reveals a circular causation in it: accuracy -> time window -> accuracy? It
> would be good to search for a reference paper which used an equivalent
> design (not just psychometric function). But to put it this way: Accuracy
> (response) is not really informative, because the tasks (if they are
> tailored) are -specifically designed- to that each participant has about
> 75% accuracy. That is, everybody will either pass a threshold (e.g., 70%)
> or not (e.g., 80%), because everybody will be at 75%.  What IS informative
> is how much time they need for achieving this. The underlying assumption is
> that there is a level of "processing speed" which is just before I become
> perfectly accurate, and the goal is to find this moment, because if I WOULD
> (otherwise) be perfectly accurate in every task my ability is
> unidentifiable (because the tasks were not difficult enough, or
> statistically speaking: no variance), - but if I was only guessing then any
> model about me is uninformative (guessing model).
>
> 3b. In other words, if you are searching for a latent ability that you want
> to continuously describe in your sample, "response window" (time needed) is
> the indicator. slow participants = low ability ; quick participants = high
> ability.
> In Item-Response-Theory you usually estimate the ability, while presenting
> the same tasks to all participants (fully crossed) which allows to estimate
> task difficulty (instead of manipulating it), and I would suggest searching
> for related model solutions in this area. (I am not experienced in tailored
> testing).
>
> 4. If you standardize the measurements within each of the four sessions,
> then I would say there is no reason to further include the term in the
> model. This, however, is a matter of theoretical rather than statistical
> debate. One theoretical counter-argument could be: If you do not
> standardize the measures, but simply include time-points as fixed effects
> in the model, then you gain information (i.e., about the time effect),
> without altering the content of your model (although you change a fixed
> assumption - to a freely estimable one). You then could also take into
> account, that some participants improve more quickly then others, which
> would be a reasonable thing to do, if you think, that this is a thing.
>
> 5. What Treutwein and Strasburger write is, first, mainly about logistic
> functions which have the most basic form of a one - parameter Rasch model.
> Make a two-parameter Rasch model out of it, then you have the functional
> form of standard logistic regression, as also performed in "lmer" and
> "brms" if you write something like:
> DV~Interceptvariable*Continuousvariable+(1|subjectID) + (1|trialID),
> family=binomial(link=logit). with two differences 1) the R packages use a
> different parameterization (e.g. dummy coding) 2) in Rasch models (or Item
> Response Theory) you estimate the model terms based on items and
> individuals, rather than predicting the DV based on conditions and
> measurements (here is a paper that investigates the relation between
> logistic models to predict accuracy and item response theory: Dixon, 2008,
> Models of accuracy in repeated-measures designs). This should help getting
> a "feeling" for the logistic function.
>
> Then what Treutwein and Strasburger introduce can also be found in every
> text-book namely gamma, which is a guessing parameter (gamma +
> 1/(1+exp(...))) which says the model can not predict 0 accuracy unless
> gamma = 0, because something will always be`correct' by chance. Secondly,
> however, adding gamma would lead the model to predictions larger than 1,
> for why there is (1-gamma) involved. Third, the model assumes that 100%
> accuracy might not be reached (for whatever reason), and lambda is
> introduced to scale the model down again, giving,
> gamma+(1-gamma-lambda)/...) which means the output of the logistic function
> (1/(1+exp(beta(theta+x)))) is squashed between gamma and lambda.
> Unfortunately, if you would try to estimate one value for each gamma,
> lambda, and beta (or 1/sigma) for a single participant then the model is
> simply unidentifiable because predicting a participants average behavior
> (or deviation from something else) of - say 70% - can be achieved by
> gamma=.3 (and lambda=0), or lambda=.3 (and gamma=0) while the logistic
> function is 0 for theta... ; OR theta = -.847 (and gamma =0; lambda0) --
> you see where this is going, right? I agree that it might be reasonable to
> assume that participants "guess" sometimes, but this is not a matter of
> estimation but a matter of your task. In a binary task gamma= .5 (lowest
> probability of being correct); in a task with three responses gamma=1/3.
> Measurement not required, just statistics. And the lambda parameter,
> finally, is not necessary, because on the individual level it is (almost)
> redundant with beta (or 1/sigma) - coming back to my initial argument. On
> the average it might sometimes "look like" you can draw a horizontal line
> at p=.8 to which the logistic function (on average) approaches. And one
> could argue this justifies assuming a maximum of lambda=.8. However, simply
> assuming hierarchical variation  in beta (or 1/sigma) either within a
> participants across trials and/or tasks (or variation of beta (or
> 1/sigma) within a task across participants), on average, will never predict
> p=1 without lambda being required, and thus provides a "natural"
> performance cap, measured in terms of variation, not in terms of lambda.
> Having both, again is not identifiable (in addition to the issues above).
> Also, -if- "guessing" would vary between participants, then, I would argue,
> one should think about the amount of trials (or which trials) in which they
> guess, not about the percent being correct while guessing (which is defined
> by the task at hand).
>
> 6. Finally, that all being said, I would suggest you use this model:
>
> thresholds <- bf(
>   norms ~ 0 +ability + task,
>   ability ~ 0+(1|subjectID),
> nl = TRUE)
>
> ## time taken to reach 75% accuracy is predicted (i.e. "norms") by the
> participants 'constant' ability, while including variations over tasks
> (depending on the task).
>  # task estimates task difficulty - should be a factor coding all 14 tasks
> (you still can compare them directly afterwards)
>  # ability is a "linear" predictor, freely estimated, one for each
> participant
> # without intercepts (i.e., 0 in front of the formulas), the task will be
> interpretable as task-specific intercepts (like grand thetas) and the
> abilities centered around 0. If you "scale" norms beforehand (i.e., across
> tasks, not within) to SD=1, then the prior for "ability" should be
> Gaussian(0,1) as well. Voila, very simply measurement model :). You could
> include more terms like time-point to control/test for training effects.
>
> afterwards you can get the task and participant posterior estimates for
> ability (I think) like this:
> posterior_samples(modeloutput)
> with different indices for the participants in the matrix. You then also
> can directly compare single task-estimates with each other (and get Bayes
> factors to check whether their difficulties differ, using a "slab-only"
> approach, instead of "spike-and-slab", check the recent work of Rouder),
>
> I can not see right now, why this should be any more complicated :) , as it
> provides you with the information you want: "How much ability the
> participant has" based on reaching the tailored testing performance of 75%
> accuracy with a specific amount of time pressure, while controlling for
> task difficulty. This also should lower the computational requirements :)
>
>
> Otherwise, if you can provide a paper which estimated:
> item difficulty (i.e., trial-wise), based on time pressure...
> task difficulty (the 14 ones)
> participant ability (unknown)
> based on binary responses
> in a tailored testing design
>
> then please let me know. Sounds interesting in any case.
>
> At least this is what I would say 'spontaneously' :))
>
> Hope this helps,
> Best Ren?
>
>
> Am Mo., 16. M?rz 2020 um 22:47 Uhr schrieb Ades, James <
> jades at health.ucsd.edu>:
>
> > Just a quick follow-up; there are actually three other tasks but their
> > adaptivity component isn't response window. One of them uses angle rotation
> > of the target as the measure of difficulty (a precision WM task). The other
> > two tasks are straight forward spatial span and backward span tasks, which
> > are just object counts.
> > ------------------------------
> > *From:* Ades, James <jades at health.ucsd.edu>
> > *Sent:* Monday, March 16, 2020 2:44 PM
> > *To:* Ren? <bimonosom at gmail.com>
> > *Cc:* r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> > *Subject:* Re: [R-sig-ME] Hierarchical Psychometric Function in BRMS
> >
> > Hi Ree,
> >
> > Thanks for the response.
> >
> > Responding to your questions:
> > 1) Yes, essentially. So there are 7 tasks, some have two conditions. One
> > has four conditions. This is the "condition" in the model. "Norm" is the
> > normalized response window.
> >
> > 2) Yes, the response window for the following trials depends on whether
> > the previous response is correct and was answered within the response
> > window.
> >
> > 3) I'm not sure what you mean by "unmotivated," but hopefully I can
> > provide some background that will give you a better idea. I'm hesitant
> > about giving too much information for the sake of avoiding confusion, but
> > the threshold was created to be 80%, but when I looked at proportion
> > correct for participants many did not achieve this, so it seemed principled
> > to extract thresholds at 70%. Ideally, the this performance threshold
> > motivates performance (not too easy, but also not too hard). From there, we
> > ask the question, what is the necessary RW for the participant to achieve
> > 70% accuracy. This question is answered through the psychometric function.
> > (In the Treutwein and Strasburger cited paper, they make the point that the
> > psychometric function is best approximated using all four priors for
> > threshold, spread, lapse, and guessing.
> >
> > 4) Yes, four sessions, completed over two years, equally spaced, more or
> > less. I control for this in the model looking at executive function
> > performance on standardized assessment outcome. I wasn't sure whether
> > including timepoints within the psychometric function model would lead to
> > more accurate estimation of participant psychometric functions.
> >
> > Hopefully, that information helps.
> >
> > Regarding your final point on convergence: as I'm sure you know, fitting
> > this model with this data is no small feat. Using UCSD's super computer, it
> > takes a little over a day. It did seem to converge though. You then write "(But
> > dropping lambda and gamma, might be worth considering in any case. If you
> > simulate logistic functions hierarchically, then they do not approximate
> > 100% on average (which would be the reason you use gamma and lambda), but
> > the limited growth approximates e.g., 80 % depending on the individual
> > variations in the slope parameters of the logistic function. This means,
> > you don't need "maximum performance" parameters, but can approximate this
> > behavior by the assumption of hierarchically clustered variance. Which also
> > makes the model simpler... , and identifiable, and you could use the
> > "elegant" way of determining 70%)." So this is where I am mathematically
> > over my head. Re Treut and Straus--they're claim is that the most
> > principled approach to approximating the psychometric function of an
> > adaptive paradigm is using prior on all four parameters. Is your argument
> > that if you're using a hierarchical approach, you wouldn't need the
> > gamma/lambda parameters? Can you say more about this or point me to an
> > article that discusses the assumption of hierarchically clustered variance?
> >
> > Thank you for the parameter extraction methods. I guess we'll figure out
> > which one when we come to that road. Elegant is always nice. But I think
> > the first think is making sure that I have the most principled and correct
> > model. Is the one I currently have in BRMS correct given the clarifications
> > above?
> >
> > Much thanks!
> >
> > James
> >
> > ------------------------------
> > *From:* Ren? <bimonosom at gmail.com>
> > *Sent:* Monday, March 16, 2020 2:10 AM
> > *To:* Ades, James <jades at health.ucsd.edu>
> > *Cc:* r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> > *Subject:* Re: [R-sig-ME] Hierarchical Psychometric Function in BRMS
> >
> > Hi James,
> >
> > since I am working with brms and glmer, I feel I should be able to give a
> > response (although addressing Paul in the Stan-Forum might be
> > a better option), there seem to be two questions, and some missing details,
> > that might lead to even more questions.... let's begin....
> >
> > My questions:
> > 1. "14 executive functions". Does this mean every participant completed
> > each of 14 tasks supposed to measure different facets of the general
> > construct "executive functions in working memory"? (If not, please
> > clarify). What term is this in the model "condition" or "norm"? (Given that
> > you have random slopes for "norm" it seems to be "norm" ?) Then what is
> > condition?
> >
> > 2. "adaptive tasks with 25 to 40 trials" Does this mean "tailored
> > testing"? (I.e., the trial that comes next within the task depends on the
> > decisions (their error) from all previous trials?)
> >
> > 3. "Goal: disentangle the response window at which participants reach a
> > 70%", - if you have tailored testing (I am not sure), which already is
> > designed to sort trials to meander around 75% accuracy for maximum
> > information/variance , this threshold seems a bit unmotivated, can you give
> > more background?
> >
> > 4. "four different time points" , I suppose these are four sessions, in
> > each the participants have completed subsets of the 14 tasks
> >
> > Your (secondary) questions (I ignore points 1 to 3 now, but they need
> > clarification):
> > "I'm not sure whether the four timepoints can be fit at once because
> > probability distributions for random factor of participant are already used
> > to account for repeated measures of participant completing 14 conditions)."
> > My answer:
> > - Regardless of the technical details:  First, "time points"  has only
> > four levels, thus, it would not make sense to separate their "random"
> > intercepts from other variance sources in the design, no matter which.
> > Computing standard deviations of a distribution for which you only have 4
> > observations/levels is problematic. Second, nonetheless assuming that "time
> > points" (e.g., increasing ability over time) has an effect, then
> > controlling for it is pretty legit, so, it makes sense to include "time
> > points" into the fixed effects. Also legit.
> >
> > 5. "The other problem I'm having is using coef() or fixef()/ranef() to
> > withdraw (or locate) the overall intercept and slope such that I can use
> > the qlogis() function to determine the psychometric threshold at 70% (since
> > I don't think it would be accurate to directly pull the 70% threshold
> > estimate from the parameter itself?)."
> > My answer:
> > - Do you mean, by 70% threshold, the "location" on the predictor(s) (the
> > logit) at which the predicted probably of the response is 70%? (Please keep
> > in mind, that you have two interacting predictors in your model, which
> > means getting these estimates for one predictor requires to either ignore
> > variance of the other predictor, which needs theoretical clarification if
> > you want to interpret this; or taking it into account - see below.) Anyway,
> > the "manual" way to do this, is to make predictions, based on the
> > coefficients, and then search the point of crossing 70%. For this you want
> > to use the "emmeans" package which works for both glmer and brms (but I am
> > not sure whether it works also for the non-linear models; if not, you need
> > to ask Paul Buerkner in the Stan forum how to do it ;)); it sure works with
> > standard hierarchical regression output from brms.) . In the emmeans
> > package you find the function "emmip", which is what you desire.
> >
> > #assuming this is your model with a continuous predictor ("continuous")
> > and a factorial predictor ("factor"):
> > model<-glmer(response ~  continuous * factor + (continuous | pid))
> > emmip(model,~continuous,at = list(continuous = c(1,2,3,4,5,6),
> > type="response",CIs=TRUE, engine="ggplot" )
> > # this gives you the probability predictions for "continuous" from 1 to 6
> > (you can make these as "fine" as you want), while ignoring "factor"
> > # if you want it "by factor" (taking the interaction into account) you can
> > write:
> > emmip(model,~continuous|factor ,at = list(continuous = c(1,2,3,4,5,6),
> > type="response",CIs=TRUE, engine="ggplot" )
> > #All you have to do is search for the point crossing 70% then :) .
> >
> > However, as noted, non-linear brms models might not directly translate to
> > the emmeans architecture (I don't know), and there is a more elegant
> > solution anyway:
> >
> > 1. A standard logistic function predicts 50% when the logit becomes 0
> > (before applying the exponential ratio rule; I ignore the fact that your
> > gamma and lambda model terms absolutely destroy this property... :))
> > 2. The "intercept" shifts the whole logit statically (or by factorial
> > conditions), such that it indicates "where" 50% is predicted (in a given
> > condition). For example, in standard models
> > 1/(1+exp(intercept+varyingeffects)) the intercept says for which value of
> > varyingeffects  the term becomes 0).
> > 3. You can "make the intercept" to indicate a 70% prediction instead of a
> > 50% prediction, if you add a constant on the logit level; that is:
> > 1/(1+exp(-.8477)) = (about) 70%; and
> >  1/(1+exp(-.8477+intercept+varyingeffects)) shifts the intercept by this
> > constant, such that it now indicates the value of varyingeffects which
> > predicts 70%. I guess. .. :)) There could be more detail to that (which I
> > don't see right now), but it sure is a starting point.
> >
> > Hope this helps, with your actual questions.
> > The rest seems to be a different matter.... (e.g., taking dependencies of
> > tailored testing into account etc).
> >
> > But one final note: I have once tried to fit simpler models with
> > constructing the logit myself, like you do, and then setting,  family =
> > bernoulli(link = "identity"), which never worked (it never converged). ...
> > Just saying: I think Paul makes some points about the identifiability of
> > those models in his vignettes, which you should check, if your model fails
> > converging.
> > (But dropping lambda and gamma, might be worth considering in any case. If
> > you simulate logistic functions hierarchically, then they do not
> > approximate 100% on average (which would be the reason you use gamma and
> > lambda), but the limited growth approximates e.g., 80 % depending on the
> > individual variations in the slope parameters of the logistic function.
> > This means, you don't need "maximum performance" parameters, but can
> > approximate this behavior by the assumption of hierarchically clustered
> > variance. Which also makes the model simpler... , and identifiable, and you
> > could use the "elegant" way of determining 70%).
> >
> >
> > Best, Ree
> >
> >
> >
> > Am Mo., 16. M?rz 2020 um 04:28 Uhr schrieb Ades, James <
> > jades at health.ucsd.edu>:
> >
> > Hi all,
> >
> > Given that this is a mixed-model listserv, I'm hoping that a BRMS question
> > might fit within that purview.
> >
> > A quick synopsis of the dataset: there are 14 different conditions of
> > executive function tasks ( ~1000 3rd, 5th, 7th graders). Given that these
> > tasks use an adaptive paradigm (tasks might have anywhere from 25 to 40
> > trials), I'm trying to disentangle the response window at which
> > participants reach a 70% performance threshold. There are four separate
> > timepoints. (I'm not sure whether the four timepoints can be fit at once
> > because probability distributions for random factor of participant are
> > already used to account for repeated measures of participant completing 14
> > conditions, but that question is secondary to ensuring that I'm fitting one
> > time point correctly and adequately extracting those the intercept/slope
> > parameters).
> >
> > If I were to only input this into glmer without the priors, I'd write the
> > model as:
> > ```
> > glmer(response ~  condition * norm + (norm | pid/condition)
> > ```
> > (In a glmer model, I can extract intercept/slope parameters fine).
> >
> > My current model is below. My question isn't so much with the psychometric
> > function or the priors, which, besides the threshold, I've borrowed from
> > Treutwein and Strasburger:
> > https://link.springer.com/article/10.3758/BF03211951--though if there are
> > contentions with any of the those, feel free to raise them--as it is
> > whether I've correctly structured the non-linear parameters. The reason for
> > modeling all four parameters is to minimize bias, but threshold is the only
> > estimate that I'm concerned with. So regarding the multi-level structure,
> > I've created parameters for lapse, guess, spread, and threshold. It seems
> > reasonable to expect that threshold and spread will vary for every
> > participant for every condition, while lapse and guessing (forced yes/no)
> > will likely not differ much from condition to condition within participant
> > (though if there are arguments that it would make for an improved model,
> > I'm fine including lapse and guess parameters for every condition as well).
> >
> > The other problem I'm having is using coef() or fixef()/ranef() to
> > withdraw (or locate) the overall intercept and slope such that I can use
> > the qlogis() function to determine the psychometric threshold at 70% (since
> > I don't think it would be accurate to directly pull the 70% threshold
> > estimate from the parameter itself?).
> >
> > Does all of that make sense? This is all a little bit over my head and
> > though I've culled Buerkner's item-response vignettes (Here:
> > https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html
> > and here: https://arxiv.org/pdf/1905.09501.pdf, they're similar but
> > fundamentally different, so they only get me so far).
> >
> > I've included a small sample of ~five participants here:
> > https://drive.google.com/file/d/1YFnQRSjnp5hVziQx5wQzaIhn75KigaGx/view?usp=sharing
> >
> > Thanks in advance for any and all help! Hope everyone is staying healthy!
> >
> > James
> >
> >
> > ```
> > thresholds <- bf(
> >   response ~ (gamma + (1 - lambda - gamma) * Phi((norm -
> > threshold)/spread)),
> >   threshold ~ 1 + (1|p|pid) + (1|c|condition),
> >   logitgamma  ~ 1 + (1|p|pid),
> >   nlf(gamma ~ inv_logit(logitgamma)),
> >   logitlambda ~ 1 + (1|p|pid),
> >   nlf(lambda ~ inv_logit(logitlambda)),
> >   spread ~ 1 + (1|p|pid) + (1|c|condition),
> > nl = TRUE)
> >
> > prior <-
> >   prior(beta(9, 3), class = "b", nlpar = "threshold", lb = 0, ub = 1) +
> >   prior(beta(1.4, 1.4), class = "b", nlpar = "spread", lb = .005, ub = .5)
> > +
> >   prior(beta(.5, 8), nlpar = "logitlambda", lb = 0, ub = .1)+
> >   prior(beta(1, 5), nlpar = "logitgamma", lb = 0, ub = .1)
> >
> > fit_thresholds <- brm(
> >   formula = thresholds,
> >   data = ace.threshold.t1.samp,
> >   family = bernoulli(link = "identity"),
> >   prior = prior,
> >   control = list(adapt_delta = .85, max_treedepth = 15),
> >   inits = 0,
> >   chains = 1,
> >   cores = 16
> > )
> > ```
> >
> >
> >
> > [
> > https://media.springernature.com/w110/springer-static/cover/journal/13414.jpg
> > ]<https://link.springer.com/article/10.3758/BF03211951>
> > Fitting the psychometric function | SpringerLink<
> > https://link.springer.com/article/10.3758/BF03211951>
> > A constrained generalized maximum likelihood routine for fitting
> > psychometric functions is proposed, which determines optimum values for the
> > complete parameter set?that is, threshold and slopeas well as for guessing
> > and lapsing probability. The constraints are realized by Bayesian prior
> > distributions for each of these parameters. The fit itself results from
> > maximizing the posterior ...
> > link.springer.com
> >
> > Abstract R arXiv:1905.09501v2 [stat.CO] 20 Jul 2019<
> > https://arxiv.org/pdf/1905.09501.pdf>
> > Paul-Christian B urkner 3 dictions via a nested non-linear formula syntax,
> > the implementation of several distributions designed for response times
> > data, and extentions of distributions for ordinal data, for example
> > arxiv.org
> >
> > Estimating Non-Linear Models with brms<
> > https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html
> > >
> > Introduction. This vignette provides an introduction on how to fit
> > non-linear multilevel models with brms.Non-linear models are incredibly
> > flexible and powerful, but require much more care with respect to model
> > specification and priors than typical generalized linear models.
> > cran.r-project.org
> >
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From mo|||eebrook@ @end|ng |rom gm@||@com  Tue Mar 17 14:50:46 2020
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Tue, 17 Mar 2020 14:50:46 +0100
Subject: [R-sig-ME] Error from glmmTMB().
In-Reply-To: <2ef51b3e-694a-1c9e-7e9f-c30fb946de3f@auckland.ac.nz>
References: <2ef51b3e-694a-1c9e-7e9f-c30fb946de3f@auckland.ac.nz>
Message-ID: <FA458F7A-4288-485C-BB47-570145F713EF@gmail.com>

It may be an overparameterization issue because I was able to get simpler models to work. We could make the error message clearer, but tried to document it at the bottom of the troubleshooting vignette. The general advice in that case is to (1) try rescaling predictor variables, (2) try a simpler model and build up, and (3) try different starting values, possibly based on a simpler model. 

I tried scaling Dose that that didn?t help. 

To get the right shape for starting values, you could check the structures in the unfitted model

mod  <- glmmTMB(fmla, data = X, family = betabinomial(link = "cloglog"),
              dispformula = ~1, doFit=FALSE)

dimnames(mod$data.tmb$X)[[2]]

dimnames(mod$data.tmb$Z)[[2]]

str(mod$parameters)

start <- list(beta=rep(0.1, 12), b=rep(.01, 12)) #fill in better values based on your best guess

fit3  <- glmmTMB(cbind(Dead, Alive) ~ Trt+0 +Dose + (1 | Rep), data = X, family = betabinomial(link = "cloglog"))


start <- list(beta = c(fixef(fit3)[[1]][1:6], rep(.1, 6)),
	b=c(unlist(ranef(fit3)[[1]]$Rep), rep(.1, 6)))
	
fit  <- glmmTMB(fmla, data = X, family = betabinomial(link = "cloglog"), start=start)

That was my best attempt at getting it to run, but it doesn?t work. 

If you think this is a bug, you could add it to the issue tracker on GitHub.

cheers,
Mollie

> On 15Mar 2020, at 4:32, Rolf Turner <r.turner at auckland.ac.nz> wrote:
> 
> 
> I am getting an error, that I have no idea what to do about, from glmmTMB():
> 
> library(glmmTMB)
> fmla <- cbind(Dead, Alive) ~ (Trt + 0)/Dose + (Dose | Rep)
> X    <- dget("X.txt")
> fit  <- glmmTMB(fmla, data = X, family = betabinomial(link = "cloglog"),
>               dispformula = ~1)
> 
>> Error in optimHess(par.fixed, obj$fn, obj$gr) :   gradient in optim evaluated to length 1 not 16
>> In addition: There were 16 warnings (use warnings() to see them)
> 
> The warnings are all repetitions of
> 
>> 1: In nlminb(start = par, objective = fn, gradient = gr,  ... :
>>  NA/NaN function evaluation
> 
> The error sounds to me like something is amiss in the code.
> 
> Can anyone confirm/deny/suggest what I might do to get this call to glmmTMB() to run?
> 
> Thanks.
> 
> cheers,
> 
> Rolf Turner
> 
> P.S.  The data set is attached.
> 
> -- 
> Honorary Research Fellow
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
> <X.txt>_______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From b|mono@om @end|ng |rom gm@||@com  Wed Mar 18 16:53:28 2020
From: b|mono@om @end|ng |rom gm@||@com (=?UTF-8?B?UmVuw6k=?=)
Date: Wed, 18 Mar 2020 16:53:28 +0100
Subject: [R-sig-ME] Hierarchical Psychometric Function in BRMS
In-Reply-To: <DM5PR1901MB20075C5C3C055F353A4FE9F6EAF70@DM5PR1901MB2007.namprd19.prod.outlook.com>
References: <DM5PR1901MB2007F953970156594F62EBFAEAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <CADcpBHPFYfZcE+13BseKceTWZf0vU8tGF6pvjkMtYyh6_Zy=9A@mail.gmail.com>
 <DM5PR1901MB2007ED6A3460670F84E40537EAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <DM5PR1901MB200773E896948650DCB58EB1EAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <CADcpBHPvR6izCGq5heOk6bf0SNtNq5v697kk0R1ON2fYcJgDpg@mail.gmail.com>
 <DM5PR1901MB20075C5C3C055F353A4FE9F6EAF70@DM5PR1901MB2007.namprd19.prod.outlook.com>
Message-ID: <CADcpBHMrYnvBCo=M=uJH7w7gmaZGR3u98YSezX7eFG+TqJE+Fg@mail.gmail.com>

Hey James,

I think the remaining questions are:
1) (Why) should one use RW (continuous) instead of RESPONSE (binary).
2) Is gamma (a guessing parameter) necessary for this.
(and a clarification below)

1)
if twenty kids run a mile, they will all have different times. Students
should be able to get 70% correct (the tasks are not inherently difficult),
it?s a question of what amount of time (how slow) is necessary in order for
them to achieve that 70% correct.
The case you have is:
Your kids run a mile and somebody says "pass" or "fail", because they
either did it in time or not, respectively. If they did it ("pass") you say
"Well that was obviously too easy for you, I want to find out if you can
also do it, if I raise the criterion by 10ms". And if the kids "fail" you
say "Well this was obviously too difficult for you, here is a little bit
more time (40ms), let's see whether you pass now."  Now : if fail lowers RW
by 40ms, and pass raises it by 10ms, then one has 4 remaining steps to
reach the level again at which one fails again. Meaning 4 right 1 wrong, 4
right 1 wrong, 4 right 1 wrong.... Just to be most clear: If my "ability"
let's me pass at criterion of 500ms, but not further, then when I reach
490ms, I will start failing. Let's just play it through: 500->pass;
490->fail; 530->pass; 520->pass; 510->pass; 500->pass; 490->fail; 530->pass
(the circle continues)... and so on- in the long run this means you
approach 4 passes, 1 fail, 4 passes, 1 fail ... which means the ratio of
accuracy will be -- for every participant --  4/5 = 80%. Now, the  average
time window for these 5-trial-circles will be (490+500+510+520+530) / 5 =
510; this means 510ms corresponds to 80% accuracy for this participant, and
thus indicates his/her ability to reach 80% accuracy. Of course, the
participants will differ in their abilities, - another participant has the
maximum ability to pass the criterion of only 600ms; thus 600->pass;
590->fail; 630->pass; 620.... thus,  610ms (on average) corresponds to 80%
accuracy. You see where this is going? Due to the test-procedure, every
participant ,meanders around 80% probably hardly reaching it (due to
behavioral noise). But the idea is - everybody is at 80%. And RW -
directly- tells you the ability of each participants, and this is extremely
nice (!) because you do not need to infer the participants ability
statistically anymore - you precisely measured it. I think, there is no
point in plugging a psychometric function in now. It adds no information.

So if you would simply change your question from "What is the RW threshold
to reach 70%" to "What is the RW threshold to reach 80% accuracy" then you
already have your answer: It is the final average response window of each
participant (due to the staircase procedure). (But one can still see
whether this RW varies between conditions) -- So I would suggest to change
the question, unless there is something very specific about 70%. But as you
noted for yourself, that you initially started of with 80% ... well you
might just rely on your test-procedure (staircase), which I think nobody
will argue about is valid.

2) Since the procedural design basically forbids guessing, there is no way
of "identifying" guessing parameters in further analyses.

Remaining note on my previous point 4) - I was referring to the four
-time-points-  (sessions) not RW, which might resolve the question.

Best
Ren?





Am Mi., 18. M?rz 2020 um 04:59 Uhr schrieb Ades, James <
jades at health.ucsd.edu>:

> Hi Rene,
>
> See comments in-line below, but I think the largest issue looking at your
> model is that you remove "response" as a DV, which means that we no longer
> have a psychometric function, despite the fact that we are dealing with
> binomial data.
>
> ?Hey James,
>
> thank you for these details. Step by step:
>
> "1) Yes, essentially. So there are 7 tasks, some have two conditions. One
> has four conditions. This is the "condition" in the model. "Norm" is the
> normalized response window."
> R1) I am sorry, I do not understand this. Does "condition" indicate the 14
> tasks (i.e., with 14 factor levels) or the "some have two, some have four
> conditions part?" If it is the latter, then why did you not include 7
> "tasks" alternatively ? - Anyway - I actually would suggest using the 14
> tasks as "condition", because the design matrix is not fully crossed.
> (i.e., without any design, just all tasks; you still can perform post-hoc
> comparisons).
> Condition = 14 factor levels which is every condition of every task.
>
> 2) The term response window is not self-explaining..., but I assume you
> mean "time pressure" by this (how long do I have to give a response). And I
> will go on to refer to this as such.
> 2b) Given "norm" is "time" then I can finally see where you want to go.
> (Please correct me if I am wrong:
> ?Overall, I think the jargon of the paradigms/fields is confusing
> communication. Just think of "Norm" as the normalized response window. If
> we're doing hierarchical, it's possible also that RW no longer needs to be
> standardized.
>
>
> 3. No offense, my choice of words was a bit clumsy. I mean a
> clarification about the research question or psychological hypothesis about
> which measure should predict another measure is always helpful to make
> judgments about a models appropriateness. As noted: I get a grip now, and
> it seems, you want to predict decision accuracy ("response") based on the
> task ("condition") and the time provided to solve the task ("norm"). While
> "norm" is a time window to complete the task, dynamically changing
> depending on the accuracy (tailored testing). Now having spelled this out
> reveals a circular causation in it: accuracy -> time window -> accuracy? It
> would be good to search for a reference paper which used an equivalent
> design (not just psychometric function). But to put it this way: Accuracy
> (response) is not really informative, because the tasks (if they are
> tailored) are -specifically designed- to that each participant has about
> 75% accuracy. That is, everybody will either pass a threshold (e.g., 70%)
> or not (e.g., 80%), because everybody will be at 75%.  What IS informative
> is how much time they need for achieving this. The underlying assumption is
> that there is a level of "processing speed" which is just before I become
> perfectly accurate, and the goal is to find this moment, because if I WOULD
> (otherwise) be perfectly accurate in every task my ability is
> unidentifiable (because the tasks were not difficult enough, or
> statistically speaking: no variance), - but if I was only guessing then any
> model about me is uninformative (guessing model).
>
> I see what you?re saying, but I don?t think the conclusion is accurate: if
> twenty kids run a mile, they will all have different times. Students should
> be able to get 70% correct (the tasks are not inherently difficult), it?s a
> question of what amount of time (how slow) is necessary in order for them
> to achieve that 70% correct. Norm (we might as well refer to it as response
> window (RW)) is a function of both time and response (accuracy), since
> students not responding within the allotted amount of time, will get that
> trial wrong, and the response window will slow (by 40ms); if they get it
> correct, response window increases by 10ms (the technical term is a
> ?staircase procedure?). You write: ?What IS informative is how much time
> they need for achieving this.? Yes, this is absolutely correct. At 70%
> probability, what is the response window for each participant for each
> condition (this would be the 70% threshold, a latent variable).
>
> 3b. In other words, if you are searching for a latent ability that you
> want to continuously describe in your sample, "response window" (time
> needed) is the indicator. slow participants = low ability ; quick
> participants = high ability.
> In Item-Response-Theory you usually estimate the ability, while presenting
> the same tasks to all participants (fully crossed) which allows to estimate
> task difficulty (instead of manipulating it), and I would suggest searching
> for related model solutions in this area. (I am not experienced in tailored
> testing).
>
> Yes, absolutely. Again, this is where I think paradigms are confusing us.
>
>
> 4. If you standardize the measurements within each of the four sessions,
> ?What measurements are you referring to here? RW?
> then I would say there is no reason to further include the term in the
> model.
> Wouldn't you have to include RW in the model?
> This, however, is a matter of theoretical rather than statistical debate.
> One theoretical counter-argument could be: If you do not standardize the
> measures, but simply include time-points as fixed effects in the model,
> then you gain information (i.e., about the time effect), without altering
> the content of your model (although you change a fixed assumption - to a
> freely estimable one). You then could also take into account, that some
> participants improve more quickly then others, which would be a reasonable
> thing to do, if you think, that this is a thing.
> ?The essence of what you're writing here seems appetizing, but I'm not
> following. How could you get around not including response window in the
> model?
>
> 5. What Treutwein and Strasburger write is, first, mainly about logistic
> functions which have the most basic form of a one - parameter Rasch model.
> Make a two-parameter Rasch model out of it, then you have the functional
> form of standard logistic regression, as also performed in "lmer" and
> "brms" if you write something like:
> DV~Interceptvariable*Continuousvariable+(1|subjectID) + (1|trialID),
> family=binomial(link=logit). with two differences 1) the R packages use a
> different parameterization (e.g. dummy coding) 2) in Rasch models (or Item
> Response Theory) you estimate the model terms based on items and
> individuals, rather than predicting the DV based on conditions and
> measurements (here is a paper that investigates the relation between
> logistic models to predict accuracy and item response theory: Dixon, 2008,
> Models of accuracy in repeated-measures designs). This should help getting
> a "feeling" for the logistic function.
>
> Then what Treutwein and Strasburger introduce can also be found in every
> text-book namely gamma, which is a guessing parameter (gamma +
> 1/(1+exp(...))) which says the model can not predict 0 accuracy unless
> gamma = 0, because something will always be`correct' by chance. Secondly,
> however, adding gamma would lead the model to predictions larger than 1,
> for why there is (1-gamma) involved.
> ?Makes sense.
> Third, the model assumes that 100% accuracy might not be reached (for
> whatever reason) (the assumption is that there are inevitable lapses in
> attention), and lambda is introduced to scale the model down again,
> giving, gamma+(1-gamma-lambda)/...) which means the output of the logistic
> function (1/(1+exp(beta(theta+x)))) is squashed between gamma and lambda.
> Unfortunately, if you would try to estimate one value for each gamma,
> lambda, and beta (or 1/sigma) for a single participant then the model is
> simply unidentifiable because predicting a participants average behavior
> (or deviation from something else) of - say 70% - can be achieved by
> gamma=.3 (and lambda=0), or lambda=.3 (and gamma=0) while the logistic
> function is 0 for theta... ; OR theta = -.847 (and gamma =0; lambda0) --
> you see where this is going, right? I agree that it might be reasonable to
> assume that participants "guess" sometimes, but this is not a matter of
> estimation but a matter of your task. In a binary task gamma= .5 (lowest
> probability of being correct); in a task with three responses gamma=1/3. Measurement
> not required, just statistics.
> ?Yes, this makes sense. But isn't this for one trial, not for the entire
> condition. Isn't that why Treutwein and Strasburger use priors to
> approximate this vs just .5 for instance?
> And the lambda parameter, finally, is not necessary, because on the
> individual level it is (almost) redundant with beta (or 1/sigma) - coming
> back to my initial argument. On the average it might sometimes "look like"
> you can draw a horizontal line at p=.8 to which the logistic function (on
> average) approaches. And one could argue this justifies assuming a maximum
> of lambda=.8. However, simply assuming hierarchical variation  in beta (or
> 1/sigma) either within a participants across trials and/or tasks (or
> variation of beta (or 1/sigma) within a task across participants), on
> average, will never predict p=1 without lambda being required, and thus
> provides a "natural" performance cap, measured in terms of variation, not
> in terms of lambda.
> Okay, I'll take your word for it. But could you point me somewhere where I
> could read more about this?
> Having both, again is not identifiable (in addition to the issues above).
> Also, -if- "guessing" would vary between participants, then, I would argue,
> one should think about the amount of trials (or which trials) in which they
> guess, not about the percent being correct while guessing (which is defined
> by the task at hand).
> Well...again, there is nothing inherently difficult with regard to the
> tasks. Given a large enough response window, one should be able to achieve
> 100% accuracy.
>
> 6. Finally, that all being said, I would suggest you use this model:
>
> thresholds <- bf(
>   norms ~ 0 +ability + task,
>   ability ~ 0+(1|subjectID),
> nl = TRUE)
> If you take out the "response" as the DV, you no longer have a binomial
> model or a psychometric function. Again, you're trying to figure out the RW
> at which participants achieve p=70% accuracy.
>
> ## time taken to reach 75% accuracy is predicted (i.e. "norms") by the
> participants 'constant' ability, while including variations over tasks
> (depending on the task).
>  # task estimates task difficulty - should be a factor coding all 14 tasks
> (you still can compare them directly afterwards)
>  # ability is a "linear" predictor, freely estimated, one for each
> participant
> # without intercepts (i.e., 0 in front of the formulas), the task will be
> interpretable as task-specific intercepts (like grand thetas) and the
> abilities centered around 0. If you "scale" norms beforehand (i.e., across
> tasks, not within) to SD=1, then the prior for "ability" should be
> Gaussian(0,1) as well. Voila, very simply measurement model :). You could
> include more terms like time-point to control/test for training effects.
>
> afterwards you can get the task and participant posterior estimates for
> ability (I think) like this:
> posterior_samples(modeloutput)
> with different indices for the participants in the matrix. You then also
> can directly compare single task-estimates with each other (and get Bayes
> factors to check whether their difficulties differ, using a "slab-only"
> approach, instead of "spike-and-slab", check the recent work of Rouder),
>
> I can not see right now, why this should be any more complicated :) , as
> it provides you with the information you want: "How much ability the
> participant has" based on reaching the tailored testing performance of 75%
> accuracy with a specific amount of time pressure, while controlling for
> task difficulty. This also should lower the computational requirements :)
>
>
> Otherwise, if you can provide a paper which estimated:
> item difficulty (i.e., trial-wise), based on time pressure...
> task difficulty (the 14 ones)
> participant ability (unknown)
> based on binary responses
> in a tailored testing design
>
> then please let me know. Sounds interesting in any case.
>
> At least this is what I would say 'spontaneously' :))
>
> Hope this helps,
> Best Ren?
>
>
> ------------------------------
> *From:* Ren? <bimonosom at gmail.com>
> *Sent:* Tuesday, March 17, 2020 1:48 AM
> *To:* Ades, James <jades at health.ucsd.edu>
> *Cc:* r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> *Subject:* Re: [R-sig-ME] Hierarchical Psychometric Function in BRMS
>
> Hey James,
>
> thank you for these details. Step by step:
>
> "1) Yes, essentially. So there are 7 tasks, some have two conditions. One
> has four conditions. This is the "condition" in the model. "Norm" is the
> normalized response window."
> R1) I am sorry, I do not understand this. Does "condition" indicate the 14
> tasks (i.e., with 14 factor levels) or the "some have two, some have four
> conditions part?" If it is the latter, then why did you not include 7
> "tasks" alternatively ? - Anyway - I actually would suggest using the 14
> tasks as "condition", because the design matrix is not fully crossed.
> (i.e., without any design, just all tasks; you still can perform post-hoc
> comparisons).
>
> 2) The term response window is not self-explaining..., but I assume you
> mean "time pressure" by this (how long do I have to give a response). And I
> will go on to refer to this as such.
> 2b) Given "norm" is "time" then I can finally see where you want to go.
> (Please correct me if I am wrong:
>
> 3. No offense, my choice of words was a bit clumsy. I mean a
> clarification about the research question or psychological hypothesis about
> which measure should predict another measure is always helpful to make
> judgments about a models appropriateness. As noted: I get a grip now, and
> it seems, you want to predict decision accuracy ("response") based on the
> task ("condition") and the time provided to solve the task ("norm"). While
> "norm" is a time window to complete the task, dynamically changing
> depending on the accuracy (tailored testing). Now having spelled this out
> reveals a circular causation in it: accuracy -> time window -> accuracy? It
> would be good to search for a reference paper which used an equivalent
> design (not just psychometric function). But to put it this way: Accuracy
> (response) is not really informative, because the tasks (if they are
> tailored) are -specifically designed- to that each participant has about
> 75% accuracy. That is, everybody will either pass a threshold (e.g., 70%)
> or not (e.g., 80%), because everybody will be at 75%.  What IS informative
> is how much time they need for achieving this. The underlying assumption is
> that there is a level of "processing speed" which is just before I become
> perfectly accurate, and the goal is to find this moment, because if I WOULD
> (otherwise) be perfectly accurate in every task my ability is
> unidentifiable (because the tasks were not difficult enough, or
> statistically speaking: no variance), - but if I was only guessing then any
> model about me is uninformative (guessing model).
>
> 3b. In other words, if you are searching for a latent ability that you
> want to continuously describe in your sample, "response window" (time
> needed) is the indicator. slow participants = low ability ; quick
> participants = high ability.
> In Item-Response-Theory you usually estimate the ability, while presenting
> the same tasks to all participants (fully crossed) which allows to estimate
> task difficulty (instead of manipulating it), and I would suggest searching
> for related model solutions in this area. (I am not experienced in tailored
> testing).
>
> 4. If you standardize the measurements within each of the four sessions,
> then I would say there is no reason to further include the term in the
> model. This, however, is a matter of theoretical rather than statistical
> debate. One theoretical counter-argument could be: If you do not
> standardize the measures, but simply include time-points as fixed effects
> in the model, then you gain information (i.e., about the time effect),
> without altering the content of your model (although you change a fixed
> assumption - to a freely estimable one). You then could also take into
> account, that some participants improve more quickly then others, which
> would be a reasonable thing to do, if you think, that this is a thing.
>
> 5. What Treutwein and Strasburger write is, first, mainly about logistic
> functions which have the most basic form of a one - parameter Rasch model.
> Make a two-parameter Rasch model out of it, then you have the functional
> form of standard logistic regression, as also performed in "lmer" and
> "brms" if you write something like:
> DV~Interceptvariable*Continuousvariable+(1|subjectID) + (1|trialID),
> family=binomial(link=logit). with two differences 1) the R packages use a
> different parameterization (e.g. dummy coding) 2) in Rasch models (or Item
> Response Theory) you estimate the model terms based on items and
> individuals, rather than predicting the DV based on conditions and
> measurements (here is a paper that investigates the relation between
> logistic models to predict accuracy and item response theory: Dixon, 2008,
> Models of accuracy in repeated-measures designs). This should help getting
> a "feeling" for the logistic function.
>
> Then what Treutwein and Strasburger introduce can also be found in every
> text-book namely gamma, which is a guessing parameter (gamma +
> 1/(1+exp(...))) which says the model can not predict 0 accuracy unless
> gamma = 0, because something will always be`correct' by chance. Secondly,
> however, adding gamma would lead the model to predictions larger than 1,
> for why there is (1-gamma) involved. Third, the model assumes that 100%
> accuracy might not be reached (for whatever reason), and lambda is
> introduced to scale the model down again, giving,
> gamma+(1-gamma-lambda)/...) which means the output of the logistic function
> (1/(1+exp(beta(theta+x)))) is squashed between gamma and lambda.
> Unfortunately, if you would try to estimate one value for each gamma,
> lambda, and beta (or 1/sigma) for a single participant then the model is
> simply unidentifiable because predicting a participants average behavior
> (or deviation from something else) of - say 70% - can be achieved by
> gamma=.3 (and lambda=0), or lambda=.3 (and gamma=0) while the logistic
> function is 0 for theta... ; OR theta = -.847 (and gamma =0; lambda0) --
> you see where this is going, right? I agree that it might be reasonable to
> assume that participants "guess" sometimes, but this is not a matter of
> estimation but a matter of your task. In a binary task gamma= .5 (lowest
> probability of being correct); in a task with three responses gamma=1/3.
> Measurement not required, just statistics. And the lambda parameter,
> finally, is not necessary, because on the individual level it is (almost)
> redundant with beta (or 1/sigma) - coming back to my initial argument. On
> the average it might sometimes "look like" you can draw a horizontal line
> at p=.8 to which the logistic function (on average) approaches. And one
> could argue this justifies assuming a maximum of lambda=.8. However, simply
> assuming hierarchical variation  in beta (or 1/sigma) either within a
> participants across trials and/or tasks (or variation of beta (or
> 1/sigma) within a task across participants), on average, will never predict
> p=1 without lambda being required, and thus provides a "natural"
> performance cap, measured in terms of variation, not in terms of lambda.
> Having both, again is not identifiable (in addition to the issues above).
> Also, -if- "guessing" would vary between participants, then, I would argue,
> one should think about the amount of trials (or which trials) in which they
> guess, not about the percent being correct while guessing (which is defined
> by the task at hand).
>
> 6. Finally, that all being said, I would suggest you use this model:
>
> thresholds <- bf(
>   norms ~ 0 +ability + task,
>   ability ~ 0+(1|subjectID),
> nl = TRUE)
>
> ## time taken to reach 75% accuracy is predicted (i.e. "norms") by the
> participants 'constant' ability, while including variations over tasks
> (depending on the task).
>  # task estimates task difficulty - should be a factor coding all 14 tasks
> (you still can compare them directly afterwards)
>  # ability is a "linear" predictor, freely estimated, one for each
> participant
> # without intercepts (i.e., 0 in front of the formulas), the task will be
> interpretable as task-specific intercepts (like grand thetas) and the
> abilities centered around 0. If you "scale" norms beforehand (i.e., across
> tasks, not within) to SD=1, then the prior for "ability" should be
> Gaussian(0,1) as well. Voila, very simply measurement model :). You could
> include more terms like time-point to control/test for training effects.
>
> afterwards you can get the task and participant posterior estimates for
> ability (I think) like this:
> posterior_samples(modeloutput)
> with different indices for the participants in the matrix. You then also
> can directly compare single task-estimates with each other (and get Bayes
> factors to check whether their difficulties differ, using a "slab-only"
> approach, instead of "spike-and-slab", check the recent work of Rouder),
>
> I can not see right now, why this should be any more complicated :) , as
> it provides you with the information you want: "How much ability the
> participant has" based on reaching the tailored testing performance of 75%
> accuracy with a specific amount of time pressure, while controlling for
> task difficulty. This also should lower the computational requirements :)
>
>
> Otherwise, if you can provide a paper which estimated:
> item difficulty (i.e., trial-wise), based on time pressure...
> task difficulty (the 14 ones)
> participant ability (unknown)
> based on binary responses
> in a tailored testing design
>
> then please let me know. Sounds interesting in any case.
>
> At least this is what I would say 'spontaneously' :))
>
> Hope this helps,
> Best Ren?
>
>
> Am Mo., 16. M?rz 2020 um 22:47 Uhr schrieb Ades, James <
> jades at health.ucsd.edu>:
>
> Just a quick follow-up; there are actually three other tasks but their
> adaptivity component isn't response window. One of them uses angle rotation
> of the target as the measure of difficulty (a precision WM task). The other
> two tasks are straight forward spatial span and backward span tasks, which
> are just object counts.
> ------------------------------
> *From:* Ades, James <jades at health.ucsd.edu>
> *Sent:* Monday, March 16, 2020 2:44 PM
> *To:* Ren? <bimonosom at gmail.com>
> *Cc:* r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> *Subject:* Re: [R-sig-ME] Hierarchical Psychometric Function in BRMS
>
> Hi Ree,
>
> Thanks for the response.
>
> Responding to your questions:
> 1) Yes, essentially. So there are 7 tasks, some have two conditions. One
> has four conditions. This is the "condition" in the model. "Norm" is the
> normalized response window.
>
> 2) Yes, the response window for the following trials depends on whether
> the previous response is correct and was answered within the response
> window.
>
> 3) I'm not sure what you mean by "unmotivated," but hopefully I can
> provide some background that will give you a better idea. I'm hesitant
> about giving too much information for the sake of avoiding confusion, but
> the threshold was created to be 80%, but when I looked at proportion
> correct for participants many did not achieve this, so it seemed principled
> to extract thresholds at 70%. Ideally, the this performance threshold
> motivates performance (not too easy, but also not too hard). From there, we
> ask the question, what is the necessary RW for the participant to achieve
> 70% accuracy. This question is answered through the psychometric function.
> (In the Treutwein and Strasburger cited paper, they make the point that the
> psychometric function is best approximated using all four priors for
> threshold, spread, lapse, and guessing.
>
> 4) Yes, four sessions, completed over two years, equally spaced, more or
> less. I control for this in the model looking at executive function
> performance on standardized assessment outcome. I wasn't sure whether
> including timepoints within the psychometric function model would lead to
> more accurate estimation of participant psychometric functions.
>
> Hopefully, that information helps.
>
> Regarding your final point on convergence: as I'm sure you know, fitting
> this model with this data is no small feat. Using UCSD's super computer, it
> takes a little over a day. It did seem to converge though. You then write "(But
> dropping lambda and gamma, might be worth considering in any case. If you
> simulate logistic functions hierarchically, then they do not approximate
> 100% on average (which would be the reason you use gamma and lambda), but
> the limited growth approximates e.g., 80 % depending on the individual
> variations in the slope parameters of the logistic function. This means,
> you don't need "maximum performance" parameters, but can approximate this
> behavior by the assumption of hierarchically clustered variance. Which also
> makes the model simpler... , and identifiable, and you could use the
> "elegant" way of determining 70%)." So this is where I am mathematically
> over my head. Re Treut and Straus--they're claim is that the most
> principled approach to approximating the psychometric function of an
> adaptive paradigm is using prior on all four parameters. Is your argument
> that if you're using a hierarchical approach, you wouldn't need the
> gamma/lambda parameters? Can you say more about this or point me to an
> article that discusses the assumption of hierarchically clustered variance?
>
> Thank you for the parameter extraction methods. I guess we'll figure out
> which one when we come to that road. Elegant is always nice. But I think
> the first think is making sure that I have the most principled and correct
> model. Is the one I currently have in BRMS correct given the clarifications
> above?
>
> Much thanks!
>
> James
>
> ------------------------------
> *From:* Ren? <bimonosom at gmail.com>
> *Sent:* Monday, March 16, 2020 2:10 AM
> *To:* Ades, James <jades at health.ucsd.edu>
> *Cc:* r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> *Subject:* Re: [R-sig-ME] Hierarchical Psychometric Function in BRMS
>
> Hi James,
>
> since I am working with brms and glmer, I feel I should be able to give a
> response (although addressing Paul in the Stan-Forum might be
> a better option), there seem to be two questions, and some missing details,
> that might lead to even more questions.... let's begin....
>
> My questions:
> 1. "14 executive functions". Does this mean every participant completed
> each of 14 tasks supposed to measure different facets of the general
> construct "executive functions in working memory"? (If not, please
> clarify). What term is this in the model "condition" or "norm"? (Given that
> you have random slopes for "norm" it seems to be "norm" ?) Then what is
> condition?
>
> 2. "adaptive tasks with 25 to 40 trials" Does this mean "tailored
> testing"? (I.e., the trial that comes next within the task depends on the
> decisions (their error) from all previous trials?)
>
> 3. "Goal: disentangle the response window at which participants reach a
> 70%", - if you have tailored testing (I am not sure), which already is
> designed to sort trials to meander around 75% accuracy for maximum
> information/variance , this threshold seems a bit unmotivated, can you give
> more background?
>
> 4. "four different time points" , I suppose these are four sessions, in
> each the participants have completed subsets of the 14 tasks
>
> Your (secondary) questions (I ignore points 1 to 3 now, but they need
> clarification):
> "I'm not sure whether the four timepoints can be fit at once because
> probability distributions for random factor of participant are already used
> to account for repeated measures of participant completing 14 conditions)."
> My answer:
> - Regardless of the technical details:  First, "time points"  has only
> four levels, thus, it would not make sense to separate their "random"
> intercepts from other variance sources in the design, no matter which.
> Computing standard deviations of a distribution for which you only have 4
> observations/levels is problematic. Second, nonetheless assuming that "time
> points" (e.g., increasing ability over time) has an effect, then
> controlling for it is pretty legit, so, it makes sense to include "time
> points" into the fixed effects. Also legit.
>
> 5. "The other problem I'm having is using coef() or fixef()/ranef() to
> withdraw (or locate) the overall intercept and slope such that I can use
> the qlogis() function to determine the psychometric threshold at 70% (since
> I don't think it would be accurate to directly pull the 70% threshold
> estimate from the parameter itself?)."
> My answer:
> - Do you mean, by 70% threshold, the "location" on the predictor(s) (the
> logit) at which the predicted probably of the response is 70%? (Please keep
> in mind, that you have two interacting predictors in your model, which
> means getting these estimates for one predictor requires to either ignore
> variance of the other predictor, which needs theoretical clarification if
> you want to interpret this; or taking it into account - see below.) Anyway,
> the "manual" way to do this, is to make predictions, based on the
> coefficients, and then search the point of crossing 70%. For this you want
> to use the "emmeans" package which works for both glmer and brms (but I am
> not sure whether it works also for the non-linear models; if not, you need
> to ask Paul Buerkner in the Stan forum how to do it ;)); it sure works with
> standard hierarchical regression output from brms.) . In the emmeans
> package you find the function "emmip", which is what you desire.
>
> #assuming this is your model with a continuous predictor ("continuous")
> and a factorial predictor ("factor"):
> model<-glmer(response ~  continuous * factor + (continuous | pid))
> emmip(model,~continuous,at = list(continuous = c(1,2,3,4,5,6),
> type="response",CIs=TRUE, engine="ggplot" )
> # this gives you the probability predictions for "continuous" from 1 to 6
> (you can make these as "fine" as you want), while ignoring "factor"
> # if you want it "by factor" (taking the interaction into account) you can
> write:
> emmip(model,~continuous|factor ,at = list(continuous = c(1,2,3,4,5,6),
> type="response",CIs=TRUE, engine="ggplot" )
> #All you have to do is search for the point crossing 70% then :) .
>
> However, as noted, non-linear brms models might not directly translate to
> the emmeans architecture (I don't know), and there is a more elegant
> solution anyway:
>
> 1. A standard logistic function predicts 50% when the logit becomes 0
> (before applying the exponential ratio rule; I ignore the fact that your
> gamma and lambda model terms absolutely destroy this property... :))
> 2. The "intercept" shifts the whole logit statically (or by factorial
> conditions), such that it indicates "where" 50% is predicted (in a given
> condition). For example, in standard models
> 1/(1+exp(intercept+varyingeffects)) the intercept says for which value of
> varyingeffects  the term becomes 0).
> 3. You can "make the intercept" to indicate a 70% prediction instead of a
> 50% prediction, if you add a constant on the logit level; that is:
> 1/(1+exp(-.8477)) = (about) 70%; and
>  1/(1+exp(-.8477+intercept+varyingeffects)) shifts the intercept by this
> constant, such that it now indicates the value of varyingeffects which
> predicts 70%. I guess. .. :)) There could be more detail to that (which I
> don't see right now), but it sure is a starting point.
>
> Hope this helps, with your actual questions.
> The rest seems to be a different matter.... (e.g., taking dependencies of
> tailored testing into account etc).
>
> But one final note: I have once tried to fit simpler models with
> constructing the logit myself, like you do, and then setting,  family =
> bernoulli(link = "identity"), which never worked (it never converged). ...
> Just saying: I think Paul makes some points about the identifiability of
> those models in his vignettes, which you should check, if your model fails
> converging.
> (But dropping lambda and gamma, might be worth considering in any case. If
> you simulate logistic functions hierarchically, then they do not
> approximate 100% on average (which would be the reason you use gamma and
> lambda), but the limited growth approximates e.g., 80 % depending on the
> individual variations in the slope parameters of the logistic function.
> This means, you don't need "maximum performance" parameters, but can
> approximate this behavior by the assumption of hierarchically clustered
> variance. Which also makes the model simpler... , and identifiable, and you
> could use the "elegant" way of determining 70%).
>
>
> Best, Ree
>
>
>
> Am Mo., 16. M?rz 2020 um 04:28 Uhr schrieb Ades, James <
> jades at health.ucsd.edu>:
>
> Hi all,
>
> Given that this is a mixed-model listserv, I'm hoping that a BRMS question
> might fit within that purview.
>
> A quick synopsis of the dataset: there are 14 different conditions of
> executive function tasks ( ~1000 3rd, 5th, 7th graders). Given that these
> tasks use an adaptive paradigm (tasks might have anywhere from 25 to 40
> trials), I'm trying to disentangle the response window at which
> participants reach a 70% performance threshold. There are four separate
> timepoints. (I'm not sure whether the four timepoints can be fit at once
> because probability distributions for random factor of participant are
> already used to account for repeated measures of participant completing 14
> conditions, but that question is secondary to ensuring that I'm fitting one
> time point correctly and adequately extracting those the intercept/slope
> parameters).
>
> If I were to only input this into glmer without the priors, I'd write the
> model as:
> ```
> glmer(response ~  condition * norm + (norm | pid/condition)
> ```
> (In a glmer model, I can extract intercept/slope parameters fine).
>
> My current model is below. My question isn't so much with the psychometric
> function or the priors, which, besides the threshold, I've borrowed from
> Treutwein and Strasburger:
> https://link.springer.com/article/10.3758/BF03211951--though if there are
> contentions with any of the those, feel free to raise them--as it is
> whether I've correctly structured the non-linear parameters. The reason for
> modeling all four parameters is to minimize bias, but threshold is the only
> estimate that I'm concerned with. So regarding the multi-level structure,
> I've created parameters for lapse, guess, spread, and threshold. It seems
> reasonable to expect that threshold and spread will vary for every
> participant for every condition, while lapse and guessing (forced yes/no)
> will likely not differ much from condition to condition within participant
> (though if there are arguments that it would make for an improved model,
> I'm fine including lapse and guess parameters for every condition as well).
>
> The other problem I'm having is using coef() or fixef()/ranef() to
> withdraw (or locate) the overall intercept and slope such that I can use
> the qlogis() function to determine the psychometric threshold at 70% (since
> I don't think it would be accurate to directly pull the 70% threshold
> estimate from the parameter itself?).
>
> Does all of that make sense? This is all a little bit over my head and
> though I've culled Buerkner's item-response vignettes (Here:
> https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html
> and here: https://arxiv.org/pdf/1905.09501.pdf, they're similar but
> fundamentally different, so they only get me so far).
>
> I've included a small sample of ~five participants here:
> https://drive.google.com/file/d/1YFnQRSjnp5hVziQx5wQzaIhn75KigaGx/view?usp=sharing
>
> Thanks in advance for any and all help! Hope everyone is staying healthy!
>
> James
>
>
> ```
> thresholds <- bf(
>   response ~ (gamma + (1 - lambda - gamma) * Phi((norm -
> threshold)/spread)),
>   threshold ~ 1 + (1|p|pid) + (1|c|condition),
>   logitgamma  ~ 1 + (1|p|pid),
>   nlf(gamma ~ inv_logit(logitgamma)),
>   logitlambda ~ 1 + (1|p|pid),
>   nlf(lambda ~ inv_logit(logitlambda)),
>   spread ~ 1 + (1|p|pid) + (1|c|condition),
> nl = TRUE)
>
> prior <-
>   prior(beta(9, 3), class = "b", nlpar = "threshold", lb = 0, ub = 1) +
>   prior(beta(1.4, 1.4), class = "b", nlpar = "spread", lb = .005, ub = .5)
> +
>   prior(beta(.5, 8), nlpar = "logitlambda", lb = 0, ub = .1)+
>   prior(beta(1, 5), nlpar = "logitgamma", lb = 0, ub = .1)
>
> fit_thresholds <- brm(
>   formula = thresholds,
>   data = ace.threshold.t1.samp,
>   family = bernoulli(link = "identity"),
>   prior = prior,
>   control = list(adapt_delta = .85, max_treedepth = 15),
>   inits = 0,
>   chains = 1,
>   cores = 16
> )
> ```
>
>
>
> [
> https://media.springernature.com/w110/springer-static/cover/journal/13414.jpg
> ]<https://link.springer.com/article/10.3758/BF03211951>
> Fitting the psychometric function | SpringerLink<
> https://link.springer.com/article/10.3758/BF03211951>
> A constrained generalized maximum likelihood routine for fitting
> psychometric functions is proposed, which determines optimum values for the
> complete parameter set?that is, threshold and slopeas well as for guessing
> and lapsing probability. The constraints are realized by Bayesian prior
> distributions for each of these parameters. The fit itself results from
> maximizing the posterior ...
> link.springer.com
>
> Abstract R arXiv:1905.09501v2 [stat.CO] 20 Jul 2019<
> https://arxiv.org/pdf/1905.09501.pdf>
> Paul-Christian B urkner 3 dictions via a nested non-linear formula syntax,
> the implementation of several distributions designed for response times
> data, and extentions of distributions for ordinal data, for example
> arxiv.org
>
> Estimating Non-Linear Models with brms<
> https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html
> >
> Introduction. This vignette provides an introduction on how to fit
> non-linear multilevel models with brms.Non-linear models are incredibly
> flexible and powerful, but require much more care with respect to model
> specification and priors than typical generalized linear models.
> cran.r-project.org
>
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>

	[[alternative HTML version deleted]]


From j@de@ @end|ng |rom he@|th@uc@d@edu  Wed Mar 18 19:33:53 2020
From: j@de@ @end|ng |rom he@|th@uc@d@edu (Ades, James)
Date: Wed, 18 Mar 2020 18:33:53 +0000
Subject: [R-sig-ME] Hierarchical Psychometric Function in BRMS
In-Reply-To: <CADcpBHMrYnvBCo=M=uJH7w7gmaZGR3u98YSezX7eFG+TqJE+Fg@mail.gmail.com>
References: <DM5PR1901MB2007F953970156594F62EBFAEAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <CADcpBHPFYfZcE+13BseKceTWZf0vU8tGF6pvjkMtYyh6_Zy=9A@mail.gmail.com>
 <DM5PR1901MB2007ED6A3460670F84E40537EAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <DM5PR1901MB200773E896948650DCB58EB1EAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <CADcpBHPvR6izCGq5heOk6bf0SNtNq5v697kk0R1ON2fYcJgDpg@mail.gmail.com>
 <DM5PR1901MB20075C5C3C055F353A4FE9F6EAF70@DM5PR1901MB2007.namprd19.prod.outlook.com>,
 <CADcpBHMrYnvBCo=M=uJH7w7gmaZGR3u98YSezX7eFG+TqJE+Fg@mail.gmail.com>
Message-ID: <DM5PR1901MB200778B339DD94928E54A7E9EAF70@DM5PR1901MB2007.namprd19.prod.outlook.com>

Hi Rene,

Yes, in an ideal world each participant would end up at 80% threshold. The reason I lowered it to 70% was because it was clear that many participants did not achieve that threshold. Why a good deal of students didn't achieve that is something for the methods section. Taking the final rw would be one way of doing it (as would average RW, which we also look at), but I think since a psychometric function takes into account the entire sampled RW distribution for each participant, it provides a more principled way of looking at a participant response.

I don't necessarily think gamma is essential (a participant would have 50% of getting a trial correct), but from everything I've read, people generally include it as a parameter. How a hierarchical model might change that, I'm not sure.

I do look at other performance methods in the paper, but one of them is psychometric function, so I'm really just trying to figure out how to change my psychometric model to be accurate within a hierarchical, Bayesian framework.

Thanks,

James
________________________________
From: Ren? <bimonosom at gmail.com>
Sent: Wednesday, March 18, 2020 8:53 AM
To: Ades, James <jades at health.ucsd.edu>
Cc: r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Hierarchical Psychometric Function in BRMS

Hey James,

I think the remaining questions are:
1) (Why) should one use RW (continuous) instead of RESPONSE (binary).
2) Is gamma (a guessing parameter) necessary for this.
(and a clarification below)

1)
if twenty kids run a mile, they will all have different times. Students should be able to get 70% correct (the tasks are not inherently difficult), it?s a question of what amount of time (how slow) is necessary in order for them to achieve that 70% correct.
The case you have is:
Your kids run a mile and somebody says "pass" or "fail", because they either did it in time or not, respectively. If they did it ("pass") you say "Well that was obviously too easy for you, I want to find out if you can also do it, if I raise the criterion by 10ms". And if the kids "fail" you say "Well this was obviously too difficult for you, here is a little bit more time (40ms), let's see whether you pass now."  Now : if fail lowers RW by 40ms, and pass raises it by 10ms, then one has 4 remaining steps to reach the level again at which one fails again. Meaning 4 right 1 wrong, 4 right 1 wrong, 4 right 1 wrong.... Just to be most clear: If my "ability" let's me pass at criterion of 500ms, but not further, then when I reach 490ms, I will start failing. Let's just play it through: 500->pass; 490->fail; 530->pass; 520->pass; 510->pass; 500->pass; 490->fail; 530->pass (the circle continues)... and so on- in the long run this means you approach 4 passes, 1 fail, 4 passes, 1 fail ... which means the ratio of accuracy will be -- for every participant --  4/5 = 80%. Now, the  average time window for these 5-trial-circles will be (490+500+510+520+530) / 5 = 510; this means 510ms corresponds to 80% accuracy for this participant, and thus indicates his/her ability to reach 80% accuracy. Of course, the participants will differ in their abilities, - another participant has the maximum ability to pass the criterion of only 600ms; thus 600->pass; 590->fail; 630->pass; 620.... thus,  610ms (on average) corresponds to 80% accuracy. You see where this is going? Due to the test-procedure, every participant ,meanders around 80% probably hardly reaching it (due to behavioral noise). But the idea is - everybody is at 80%. And RW - directly- tells you the ability of each participants, and this is extremely nice (!) because you do not need to infer the participants ability statistically anymore - you precisely measured it. I think, there is no point in plugging a psychometric function in now. It adds no information.

So if you would simply change your question from "What is the RW threshold to reach 70%" to "What is the RW threshold to reach 80% accuracy" then you already have your answer: It is the final average response window of each participant (due to the staircase procedure). (But one can still see whether this RW varies between conditions) -- So I would suggest to change the question, unless there is something very specific about 70%. But as you noted for yourself, that you initially started of with 80% ... well you might just rely on your test-procedure (staircase), which I think nobody will argue about is valid.

2) Since the procedural design basically forbids guessing, there is no way of "identifying" guessing parameters in further analyses.

Remaining note on my previous point 4) - I was referring to the four -time-points-  (sessions) not RW, which might resolve the question.

Best
Ren?





Am Mi., 18. M?rz 2020 um 04:59 Uhr schrieb Ades, James <jades at health.ucsd.edu<mailto:jades at health.ucsd.edu>>:
Hi Rene,

See comments in-line below, but I think the largest issue looking at your model is that you remove "response" as a DV, which means that we no longer have a psychometric function, despite the fact that we are dealing with binomial data.

?Hey James,

thank you for these details. Step by step:

"1) Yes, essentially. So there are 7 tasks, some have two conditions. One has four conditions. This is the "condition" in the model. "Norm" is the normalized response window."
R1) I am sorry, I do not understand this. Does "condition" indicate the 14 tasks (i.e., with 14 factor levels) or the "some have two, some have four conditions part?" If it is the latter, then why did you not include 7 "tasks" alternatively ? - Anyway - I actually would suggest using the 14 tasks as "condition", because the design matrix is not fully crossed. (i.e., without any design, just all tasks; you still can perform post-hoc comparisons).
Condition = 14 factor levels which is every condition of every task.

2) The term response window is not self-explaining..., but I assume you mean "time pressure" by this (how long do I have to give a response). And I will go on to refer to this as such.
2b) Given "norm" is "time" then I can finally see where you want to go. (Please correct me if I am wrong:
?Overall, I think the jargon of the paradigms/fields is confusing communication. Just think of "Norm" as the normalized response window. If we're doing hierarchical, it's possible also that RW no longer needs to be standardized.


3. No offense, my choice of words was a bit clumsy. I mean a clarification about the research question or psychological hypothesis about which measure should predict another measure is always helpful to make judgments about a models appropriateness. As noted: I get a grip now, and it seems, you want to predict decision accuracy ("response") based on the task ("condition") and the time provided to solve the task ("norm"). While "norm" is a time window to complete the task, dynamically changing depending on the accuracy (tailored testing). Now having spelled this out reveals a circular causation in it: accuracy -> time window -> accuracy? It would be good to search for a reference paper which used an equivalent design (not just psychometric function). But to put it this way: Accuracy (response) is not really informative, because the tasks (if they are tailored) are -specifically designed- to that each participant has about 75% accuracy. That is, everybody will either pass a threshold (e.g., 70%) or not (e.g., 80%), because everybody will be at 75%.  What IS informative is how much time they need for achieving this. The underlying assumption is that there is a level of "processing speed" which is just before I become perfectly accurate, and the goal is to find this moment, because if I WOULD (otherwise) be perfectly accurate in every task my ability is unidentifiable (because the tasks were not difficult enough, or statistically speaking: no variance), - but if I was only guessing then any model about me is uninformative (guessing model).

I see what you?re saying, but I don?t think the conclusion is accurate: if twenty kids run a mile, they will all have different times. Students should be able to get 70% correct (the tasks are not inherently difficult), it?s a question of what amount of time (how slow) is necessary in order for them to achieve that 70% correct. Norm (we might as well refer to it as response window (RW)) is a function of both time and response (accuracy), since students not responding within the allotted amount of time, will get that trial wrong, and the response window will slow (by 40ms); if they get it correct, response window increases by 10ms (the technical term is a ?staircase procedure?). You write: ?What IS informative is how much time they need for achieving this.? Yes, this is absolutely correct. At 70% probability, what is the response window for each participant for each condition (this would be the 70% threshold, a latent variable).

3b. In other words, if you are searching for a latent ability that you want to continuously describe in your sample, "response window" (time needed) is the indicator. slow participants = low ability ; quick participants = high ability.
In Item-Response-Theory you usually estimate the ability, while presenting the same tasks to all participants (fully crossed) which allows to estimate task difficulty (instead of manipulating it), and I would suggest searching for related model solutions in this area. (I am not experienced in tailored testing).

Yes, absolutely. Again, this is where I think paradigms are confusing us.


4. If you standardize the measurements within each of the four sessions,
?What measurements are you referring to here? RW?
then I would say there is no reason to further include the term in the model.
Wouldn't you have to include RW in the model?
This, however, is a matter of theoretical rather than statistical debate. One theoretical counter-argument could be: If you do not standardize the measures, but simply include time-points as fixed effects in the model, then you gain information (i.e., about the time effect), without altering the content of your model (although you change a fixed assumption - to a freely estimable one). You then could also take into account, that some participants improve more quickly then others, which would be a reasonable thing to do, if you think, that this is a thing.
?The essence of what you're writing here seems appetizing, but I'm not following. How could you get around not including response window in the model?

5. What Treutwein and Strasburger write is, first, mainly about logistic functions which have the most basic form of a one - parameter Rasch model. Make a two-parameter Rasch model out of it, then you have the functional form of standard logistic regression, as also performed in "lmer" and "brms" if you write something like: DV~Interceptvariable*Continuousvariable+(1|subjectID) + (1|trialID), family=binomial(link=logit). with two differences 1) the R packages use a different parameterization (e.g. dummy coding) 2) in Rasch models (or Item Response Theory) you estimate the model terms based on items and individuals, rather than predicting the DV based on conditions and measurements (here is a paper that investigates the relation between logistic models to predict accuracy and item response theory: Dixon, 2008, Models of accuracy in repeated-measures designs). This should help getting a "feeling" for the logistic function.

Then what Treutwein and Strasburger introduce can also be found in every text-book namely gamma, which is a guessing parameter (gamma + 1/(1+exp(...))) which says the model can not predict 0 accuracy unless gamma = 0, because something will always be`correct' by chance. Secondly, however, adding gamma would lead the model to predictions larger than 1, for why there is (1-gamma) involved.
?Makes sense.
Third, the model assumes that 100% accuracy might not be reached (for whatever reason) (the assumption is that there are inevitable lapses in attention), and lambda is introduced to scale the model down again, giving, gamma+(1-gamma-lambda)/...) which means the output of the logistic function (1/(1+exp(beta(theta+x)))) is squashed between gamma and lambda. Unfortunately, if you would try to estimate one value for each gamma, lambda, and beta (or 1/sigma) for a single participant then the model is simply unidentifiable because predicting a participants average behavior (or deviation from something else) of - say 70% - can be achieved by gamma=.3 (and lambda=0), or lambda=.3 (and gamma=0) while the logistic function is 0 for theta... ; OR theta = -.847 (and gamma =0; lambda0) -- you see where this is going, right? I agree that it might be reasonable to assume that participants "guess" sometimes, but this is not a matter of estimation but a matter of your task. In a binary task gamma= .5 (lowest probability of being correct); in a task with three responses gamma=1/3. Measurement not required, just statistics.
?Yes, this makes sense. But isn't this for one trial, not for the entire condition. Isn't that why Treutwein and Strasburger use priors to approximate this vs just .5 for instance?
And the lambda parameter, finally, is not necessary, because on the individual level it is (almost) redundant with beta (or 1/sigma) - coming back to my initial argument. On the average it might sometimes "look like" you can draw a horizontal line at p=.8 to which the logistic function (on average) approaches. And one could argue this justifies assuming a maximum of lambda=.8. However, simply assuming hierarchical variation  in beta (or 1/sigma) either within a participants across trials and/or tasks (or variation of beta (or 1/sigma) within a task across participants), on average, will never predict p=1 without lambda being required, and thus provides a "natural" performance cap, measured in terms of variation, not in terms of lambda.
Okay, I'll take your word for it. But could you point me somewhere where I could read more about this?
Having both, again is not identifiable (in addition to the issues above). Also, -if- "guessing" would vary between participants, then, I would argue, one should think about the amount of trials (or which trials) in which they guess, not about the percent being correct while guessing (which is defined by the task at hand).
Well...again, there is nothing inherently difficult with regard to the tasks. Given a large enough response window, one should be able to achieve 100% accuracy.

6. Finally, that all being said, I would suggest you use this model:

thresholds <- bf(
  norms ~ 0 +ability + task,
  ability ~ 0+(1|subjectID),
nl = TRUE)
If you take out the "response" as the DV, you no longer have a binomial model or a psychometric function. Again, you're trying to figure out the RW at which participants achieve p=70% accuracy.

## time taken to reach 75% accuracy is predicted (i.e. "norms") by the participants 'constant' ability, while including variations over tasks (depending on the task).
 # task estimates task difficulty - should be a factor coding all 14 tasks (you still can compare them directly afterwards)
 # ability is a "linear" predictor, freely estimated, one for each participant
# without intercepts (i.e., 0 in front of the formulas), the task will be interpretable as task-specific intercepts (like grand thetas) and the abilities centered around 0. If you "scale" norms beforehand (i.e., across tasks, not within) to SD=1, then the prior for "ability" should be Gaussian(0,1) as well. Voila, very simply measurement model :). You could include more terms like time-point to control/test for training effects.

afterwards you can get the task and participant posterior estimates for ability (I think) like this:
posterior_samples(modeloutput)
with different indices for the participants in the matrix. You then also can directly compare single task-estimates with each other (and get Bayes factors to check whether their difficulties differ, using a "slab-only" approach, instead of "spike-and-slab", check the recent work of Rouder),

I can not see right now, why this should be any more complicated :) , as it provides you with the information you want: "How much ability the participant has" based on reaching the tailored testing performance of 75% accuracy with a specific amount of time pressure, while controlling for task difficulty. This also should lower the computational requirements :)


Otherwise, if you can provide a paper which estimated:
item difficulty (i.e., trial-wise), based on time pressure...
task difficulty (the 14 ones)
participant ability (unknown)
based on binary responses
in a tailored testing design

then please let me know. Sounds interesting in any case.

At least this is what I would say 'spontaneously' :))

Hope this helps,
Best Ren?


________________________________
From: Ren? <bimonosom at gmail.com<mailto:bimonosom at gmail.com>>
Sent: Tuesday, March 17, 2020 1:48 AM
To: Ades, James <jades at health.ucsd.edu<mailto:jades at health.ucsd.edu>>
Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org> <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: Re: [R-sig-ME] Hierarchical Psychometric Function in BRMS

Hey James,

thank you for these details. Step by step:

"1) Yes, essentially. So there are 7 tasks, some have two conditions. One has four conditions. This is the "condition" in the model. "Norm" is the normalized response window."
R1) I am sorry, I do not understand this. Does "condition" indicate the 14 tasks (i.e., with 14 factor levels) or the "some have two, some have four conditions part?" If it is the latter, then why did you not include 7 "tasks" alternatively ? - Anyway - I actually would suggest using the 14 tasks as "condition", because the design matrix is not fully crossed. (i.e., without any design, just all tasks; you still can perform post-hoc comparisons).

2) The term response window is not self-explaining..., but I assume you mean "time pressure" by this (how long do I have to give a response). And I will go on to refer to this as such.
2b) Given "norm" is "time" then I can finally see where you want to go. (Please correct me if I am wrong:

3. No offense, my choice of words was a bit clumsy. I mean a clarification about the research question or psychological hypothesis about which measure should predict another measure is always helpful to make judgments about a models appropriateness. As noted: I get a grip now, and it seems, you want to predict decision accuracy ("response") based on the task ("condition") and the time provided to solve the task ("norm"). While "norm" is a time window to complete the task, dynamically changing depending on the accuracy (tailored testing). Now having spelled this out reveals a circular causation in it: accuracy -> time window -> accuracy? It would be good to search for a reference paper which used an equivalent design (not just psychometric function). But to put it this way: Accuracy (response) is not really informative, because the tasks (if they are tailored) are -specifically designed- to that each participant has about 75% accuracy. That is, everybody will either pass a threshold (e.g., 70%) or not (e.g., 80%), because everybody will be at 75%.  What IS informative is how much time they need for achieving this. The underlying assumption is that there is a level of "processing speed" which is just before I become perfectly accurate, and the goal is to find this moment, because if I WOULD (otherwise) be perfectly accurate in every task my ability is unidentifiable (because the tasks were not difficult enough, or statistically speaking: no variance), - but if I was only guessing then any model about me is uninformative (guessing model).

3b. In other words, if you are searching for a latent ability that you want to continuously describe in your sample, "response window" (time needed) is the indicator. slow participants = low ability ; quick participants = high ability.
In Item-Response-Theory you usually estimate the ability, while presenting the same tasks to all participants (fully crossed) which allows to estimate task difficulty (instead of manipulating it), and I would suggest searching for related model solutions in this area. (I am not experienced in tailored testing).

4. If you standardize the measurements within each of the four sessions, then I would say there is no reason to further include the term in the model. This, however, is a matter of theoretical rather than statistical debate. One theoretical counter-argument could be: If you do not standardize the measures, but simply include time-points as fixed effects in the model, then you gain information (i.e., about the time effect), without altering the content of your model (although you change a fixed assumption - to a freely estimable one). You then could also take into account, that some participants improve more quickly then others, which would be a reasonable thing to do, if you think, that this is a thing.

5. What Treutwein and Strasburger write is, first, mainly about logistic functions which have the most basic form of a one - parameter Rasch model. Make a two-parameter Rasch model out of it, then you have the functional form of standard logistic regression, as also performed in "lmer" and "brms" if you write something like: DV~Interceptvariable*Continuousvariable+(1|subjectID) + (1|trialID), family=binomial(link=logit). with two differences 1) the R packages use a different parameterization (e.g. dummy coding) 2) in Rasch models (or Item Response Theory) you estimate the model terms based on items and individuals, rather than predicting the DV based on conditions and measurements (here is a paper that investigates the relation between logistic models to predict accuracy and item response theory: Dixon, 2008, Models of accuracy in repeated-measures designs). This should help getting a "feeling" for the logistic function.

Then what Treutwein and Strasburger introduce can also be found in every text-book namely gamma, which is a guessing parameter (gamma + 1/(1+exp(...))) which says the model can not predict 0 accuracy unless gamma = 0, because something will always be`correct' by chance. Secondly, however, adding gamma would lead the model to predictions larger than 1, for why there is (1-gamma) involved. Third, the model assumes that 100% accuracy might not be reached (for whatever reason), and lambda is introduced to scale the model down again, giving, gamma+(1-gamma-lambda)/...) which means the output of the logistic function (1/(1+exp(beta(theta+x)))) is squashed between gamma and lambda. Unfortunately, if you would try to estimate one value for each gamma, lambda, and beta (or 1/sigma) for a single participant then the model is simply unidentifiable because predicting a participants average behavior (or deviation from something else) of - say 70% - can be achieved by gamma=.3 (and lambda=0), or lambda=.3 (and gamma=0) while the logistic function is 0 for theta... ; OR theta = -.847 (and gamma =0; lambda0) -- you see where this is going, right? I agree that it might be reasonable to assume that participants "guess" sometimes, but this is not a matter of estimation but a matter of your task. In a binary task gamma= .5 (lowest probability of being correct); in a task with three responses gamma=1/3. Measurement not required, just statistics. And the lambda parameter, finally, is not necessary, because on the individual level it is (almost) redundant with beta (or 1/sigma) - coming back to my initial argument. On the average it might sometimes "look like" you can draw a horizontal line at p=.8 to which the logistic function (on average) approaches. And one could argue this justifies assuming a maximum of lambda=.8. However, simply assuming hierarchical variation  in beta (or 1/sigma) either within a participants across trials and/or tasks (or variation of beta (or 1/sigma) within a task across participants), on average, will never predict p=1 without lambda being required, and thus provides a "natural" performance cap, measured in terms of variation, not in terms of lambda. Having both, again is not identifiable (in addition to the issues above). Also, -if- "guessing" would vary between participants, then, I would argue, one should think about the amount of trials (or which trials) in which they guess, not about the percent being correct while guessing (which is defined by the task at hand).

6. Finally, that all being said, I would suggest you use this model:

thresholds <- bf(
  norms ~ 0 +ability + task,
  ability ~ 0+(1|subjectID),
nl = TRUE)

## time taken to reach 75% accuracy is predicted (i.e. "norms") by the participants 'constant' ability, while including variations over tasks (depending on the task).
 # task estimates task difficulty - should be a factor coding all 14 tasks (you still can compare them directly afterwards)
 # ability is a "linear" predictor, freely estimated, one for each participant
# without intercepts (i.e., 0 in front of the formulas), the task will be interpretable as task-specific intercepts (like grand thetas) and the abilities centered around 0. If you "scale" norms beforehand (i.e., across tasks, not within) to SD=1, then the prior for "ability" should be Gaussian(0,1) as well. Voila, very simply measurement model :). You could include more terms like time-point to control/test for training effects.

afterwards you can get the task and participant posterior estimates for ability (I think) like this:
posterior_samples(modeloutput)
with different indices for the participants in the matrix. You then also can directly compare single task-estimates with each other (and get Bayes factors to check whether their difficulties differ, using a "slab-only" approach, instead of "spike-and-slab", check the recent work of Rouder),

I can not see right now, why this should be any more complicated :) , as it provides you with the information you want: "How much ability the participant has" based on reaching the tailored testing performance of 75% accuracy with a specific amount of time pressure, while controlling for task difficulty. This also should lower the computational requirements :)


Otherwise, if you can provide a paper which estimated:
item difficulty (i.e., trial-wise), based on time pressure...
task difficulty (the 14 ones)
participant ability (unknown)
based on binary responses
in a tailored testing design

then please let me know. Sounds interesting in any case.

At least this is what I would say 'spontaneously' :))

Hope this helps,
Best Ren?


Am Mo., 16. M?rz 2020 um 22:47 Uhr schrieb Ades, James <jades at health.ucsd.edu<mailto:jades at health.ucsd.edu>>:
Just a quick follow-up; there are actually three other tasks but their adaptivity component isn't response window. One of them uses angle rotation of the target as the measure of difficulty (a precision WM task). The other two tasks are straight forward spatial span and backward span tasks, which are just object counts.
________________________________
From: Ades, James <jades at health.ucsd.edu<mailto:jades at health.ucsd.edu>>
Sent: Monday, March 16, 2020 2:44 PM
To: Ren? <bimonosom at gmail.com<mailto:bimonosom at gmail.com>>
Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org> <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: Re: [R-sig-ME] Hierarchical Psychometric Function in BRMS

Hi Ree,

Thanks for the response.

Responding to your questions:
1) Yes, essentially. So there are 7 tasks, some have two conditions. One has four conditions. This is the "condition" in the model. "Norm" is the normalized response window.

2) Yes, the response window for the following trials depends on whether the previous response is correct and was answered within the response window.

3) I'm not sure what you mean by "unmotivated," but hopefully I can provide some background that will give you a better idea. I'm hesitant about giving too much information for the sake of avoiding confusion, but the threshold was created to be 80%, but when I looked at proportion correct for participants many did not achieve this, so it seemed principled to extract thresholds at 70%. Ideally, the this performance threshold motivates performance (not too easy, but also not too hard). From there, we ask the question, what is the necessary RW for the participant to achieve 70% accuracy. This question is answered through the psychometric function. (In the Treutwein and Strasburger cited paper, they make the point that the psychometric function is best approximated using all four priors for threshold, spread, lapse, and guessing.

4) Yes, four sessions, completed over two years, equally spaced, more or less. I control for this in the model looking at executive function performance on standardized assessment outcome. I wasn't sure whether including timepoints within the psychometric function model would lead to more accurate estimation of participant psychometric functions.

Hopefully, that information helps.

Regarding your final point on convergence: as I'm sure you know, fitting this model with this data is no small feat. Using UCSD's super computer, it takes a little over a day. It did seem to converge though. You then write "(But dropping lambda and gamma, might be worth considering in any case. If you simulate logistic functions hierarchically, then they do not approximate 100% on average (which would be the reason you use gamma and lambda), but the limited growth approximates e.g., 80 % depending on the individual variations in the slope parameters of the logistic function. This means, you don't need "maximum performance" parameters, but can approximate this behavior by the assumption of hierarchically clustered variance. Which also makes the model simpler... , and identifiable, and you could use the "elegant" way of determining 70%)." So this is where I am mathematically over my head. Re Treut and Straus--they're claim is that the most principled approach to approximating the psychometric function of an adaptive paradigm is using prior on all four parameters. Is your argument that if you're using a hierarchical approach, you wouldn't need the gamma/lambda parameters? Can you say more about this or point me to an article that discusses the assumption of hierarchically clustered variance?

Thank you for the parameter extraction methods. I guess we'll figure out which one when we come to that road. Elegant is always nice. But I think the first think is making sure that I have the most principled and correct model. Is the one I currently have in BRMS correct given the clarifications above?

Much thanks!

James

________________________________
From: Ren? <bimonosom at gmail.com<mailto:bimonosom at gmail.com>>
Sent: Monday, March 16, 2020 2:10 AM
To: Ades, James <jades at health.ucsd.edu<mailto:jades at health.ucsd.edu>>
Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org> <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: Re: [R-sig-ME] Hierarchical Psychometric Function in BRMS

Hi James,

since I am working with brms and glmer, I feel I should be able to give a response (although addressing Paul in the Stan-Forum might be a better option), there seem to be two questions, and some missing details, that might lead to even more questions.... let's begin....

My questions:
1. "14 executive functions". Does this mean every participant completed each of 14 tasks supposed to measure different facets of the general construct "executive functions in working memory"? (If not, please clarify). What term is this in the model "condition" or "norm"? (Given that you have random slopes for "norm" it seems to be "norm" ?) Then what is condition?

2. "adaptive tasks with 25 to 40 trials" Does this mean "tailored testing"? (I.e., the trial that comes next within the task depends on the decisions (their error) from all previous trials?)

3. "Goal: disentangle the response window at which participants reach a 70%", - if you have tailored testing (I am not sure), which already is designed to sort trials to meander around 75% accuracy for maximum information/variance , this threshold seems a bit unmotivated, can you give more background?

4. "four different time points" , I suppose these are four sessions, in each the participants have completed subsets of the 14 tasks

Your (secondary) questions (I ignore points 1 to 3 now, but they need clarification):
"I'm not sure whether the four timepoints can be fit at once because probability distributions for random factor of participant are already used to account for repeated measures of participant completing 14 conditions)."
My answer:
- Regardless of the technical details:  First, "time points"  has only four levels, thus, it would not make sense to separate their "random" intercepts from other variance sources in the design, no matter which. Computing standard deviations of a distribution for which you only have 4 observations/levels is problematic. Second, nonetheless assuming that "time points" (e.g., increasing ability over time) has an effect, then controlling for it is pretty legit, so, it makes sense to include "time points" into the fixed effects. Also legit.

5. "The other problem I'm having is using coef() or fixef()/ranef() to withdraw (or locate) the overall intercept and slope such that I can use the qlogis() function to determine the psychometric threshold at 70% (since I don't think it would be accurate to directly pull the 70% threshold estimate from the parameter itself?)."
My answer:
- Do you mean, by 70% threshold, the "location" on the predictor(s) (the logit) at which the predicted probably of the response is 70%? (Please keep in mind, that you have two interacting predictors in your model, which means getting these estimates for one predictor requires to either ignore variance of the other predictor, which needs theoretical clarification if you want to interpret this; or taking it into account - see below.) Anyway, the "manual" way to do this, is to make predictions, based on the coefficients, and then search the point of crossing 70%. For this you want to use the "emmeans" package which works for both glmer and brms (but I am not sure whether it works also for the non-linear models; if not, you need to ask Paul Buerkner in the Stan forum how to do it ;)); it sure works with standard hierarchical regression output from brms.) . In the emmeans package you find the function "emmip", which is what you desire.

#assuming this is your model with a continuous predictor ("continuous") and a factorial predictor ("factor"):
model<-glmer(response ~  continuous * factor + (continuous | pid))
emmip(model,~continuous,at = list(continuous = c(1,2,3,4,5,6), type="response",CIs=TRUE, engine="ggplot" )
# this gives you the probability predictions for "continuous" from 1 to 6 (you can make these as "fine" as you want), while ignoring "factor"
# if you want it "by factor" (taking the interaction into account) you can write:
emmip(model,~continuous|factor ,at = list(continuous = c(1,2,3,4,5,6), type="response",CIs=TRUE, engine="ggplot" )
#All you have to do is search for the point crossing 70% then :) .

However, as noted, non-linear brms models might not directly translate to the emmeans architecture (I don't know), and there is a more elegant solution anyway:

1. A standard logistic function predicts 50% when the logit becomes 0 (before applying the exponential ratio rule; I ignore the fact that your gamma and lambda model terms absolutely destroy this property... :))
2. The "intercept" shifts the whole logit statically (or by factorial conditions), such that it indicates "where" 50% is predicted (in a given condition). For example, in standard models 1/(1+exp(intercept+varyingeffects)) the intercept says for which value of varyingeffects  the term becomes 0).
3. You can "make the intercept" to indicate a 70% prediction instead of a 50% prediction, if you add a constant on the logit level; that is: 1/(1+exp(-.8477)) = (about) 70%; and  1/(1+exp(-.8477+intercept+varyingeffects)) shifts the intercept by this constant, such that it now indicates the value of varyingeffects which predicts 70%. I guess. .. :)) There could be more detail to that (which I don't see right now), but it sure is a starting point.

Hope this helps, with your actual questions.
The rest seems to be a different matter.... (e.g., taking dependencies of tailored testing into account etc).

But one final note: I have once tried to fit simpler models with constructing the logit myself, like you do, and then setting,  family = bernoulli(link = "identity"), which never worked (it never converged). ... Just saying: I think Paul makes some points about the identifiability of those models in his vignettes, which you should check, if your model fails converging.
(But dropping lambda and gamma, might be worth considering in any case. If you simulate logistic functions hierarchically, then they do not approximate 100% on average (which would be the reason you use gamma and lambda), but the limited growth approximates e.g., 80 % depending on the individual variations in the slope parameters of the logistic function. This means, you don't need "maximum performance" parameters, but can approximate this behavior by the assumption of hierarchically clustered variance. Which also makes the model simpler... , and identifiable, and you could use the "elegant" way of determining 70%).


Best, Ree



Am Mo., 16. M?rz 2020 um 04:28 Uhr schrieb Ades, James <jades at health.ucsd.edu<mailto:jades at health.ucsd.edu>>:
Hi all,

Given that this is a mixed-model listserv, I'm hoping that a BRMS question might fit within that purview.

A quick synopsis of the dataset: there are 14 different conditions of executive function tasks ( ~1000 3rd, 5th, 7th graders). Given that these tasks use an adaptive paradigm (tasks might have anywhere from 25 to 40 trials), I'm trying to disentangle the response window at which participants reach a 70% performance threshold. There are four separate timepoints. (I'm not sure whether the four timepoints can be fit at once because probability distributions for random factor of participant are already used to account for repeated measures of participant completing 14 conditions, but that question is secondary to ensuring that I'm fitting one time point correctly and adequately extracting those the intercept/slope parameters).

If I were to only input this into glmer without the priors, I'd write the model as:
```
glmer(response ~  condition * norm + (norm | pid/condition)
```
(In a glmer model, I can extract intercept/slope parameters fine).

My current model is below. My question isn't so much with the psychometric function or the priors, which, besides the threshold, I've borrowed from Treutwein and Strasburger: https://link.springer.com/article/10.3758/BF03211951--though if there are contentions with any of the those, feel free to raise them--as it is whether I've correctly structured the non-linear parameters. The reason for modeling all four parameters is to minimize bias, but threshold is the only estimate that I'm concerned with. So regarding the multi-level structure, I've created parameters for lapse, guess, spread, and threshold. It seems reasonable to expect that threshold and spread will vary for every participant for every condition, while lapse and guessing (forced yes/no) will likely not differ much from condition to condition within participant (though if there are arguments that it would make for an improved model, I'm fine including lapse and guess parameters for every condition as well).

The other problem I'm having is using coef() or fixef()/ranef() to withdraw (or locate) the overall intercept and slope such that I can use the qlogis() function to determine the psychometric threshold at 70% (since I don't think it would be accurate to directly pull the 70% threshold estimate from the parameter itself?).

Does all of that make sense? This is all a little bit over my head and though I've culled Buerkner's item-response vignettes (Here: https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html and here: https://arxiv.org/pdf/1905.09501.pdf, they're similar but fundamentally different, so they only get me so far).

I've included a small sample of ~five participants here: https://drive.google.com/file/d/1YFnQRSjnp5hVziQx5wQzaIhn75KigaGx/view?usp=sharing

Thanks in advance for any and all help! Hope everyone is staying healthy!

James


```
thresholds <- bf(
  response ~ (gamma + (1 - lambda - gamma) * Phi((norm - threshold)/spread)),
  threshold ~ 1 + (1|p|pid) + (1|c|condition),
  logitgamma  ~ 1 + (1|p|pid),
  nlf(gamma ~ inv_logit(logitgamma)),
  logitlambda ~ 1 + (1|p|pid),
  nlf(lambda ~ inv_logit(logitlambda)),
  spread ~ 1 + (1|p|pid) + (1|c|condition),
nl = TRUE)

prior <-
  prior(beta(9, 3), class = "b", nlpar = "threshold", lb = 0, ub = 1) +
  prior(beta(1.4, 1.4), class = "b", nlpar = "spread", lb = .005, ub = .5) +
  prior(beta(.5, 8), nlpar = "logitlambda", lb = 0, ub = .1)+
  prior(beta(1, 5), nlpar = "logitgamma", lb = 0, ub = .1)

fit_thresholds <- brm(
  formula = thresholds,
  data = ace.threshold.t1.samp,
  family = bernoulli(link = "identity"),
  prior = prior,
  control = list(adapt_delta = .85, max_treedepth = 15),
  inits = 0,
  chains = 1,
  cores = 16
)
```



[https://media.springernature.com/w110/springer-static/cover/journal/13414.jpg]<https://link.springer.com/article/10.3758/BF03211951>
Fitting the psychometric function | SpringerLink<https://link.springer.com/article/10.3758/BF03211951>
A constrained generalized maximum likelihood routine for fitting psychometric functions is proposed, which determines optimum values for the complete parameter set?that is, threshold and slopeas well as for guessing and lapsing probability. The constraints are realized by Bayesian prior distributions for each of these parameters. The fit itself results from maximizing the posterior ...
link.springer.com<http://link.springer.com>

Abstract R arXiv:1905.09501v2 [stat.CO] 20 Jul 2019<https://arxiv.org/pdf/1905.09501.pdf>
Paul-Christian B urkner 3 dictions via a nested non-linear formula syntax, the implementation of several distributions designed for response times data, and extentions of distributions for ordinal data, for example
arxiv.org<http://arxiv.org>

Estimating Non-Linear Models with brms<https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html>
Introduction. This vignette provides an introduction on how to fit non-linear multilevel models with brms.Non-linear models are incredibly flexible and powerful, but require much more care with respect to model specification and priors than typical generalized linear models.
cran.r-project.org<http://cran.r-project.org>



        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From d@r|zopou|o@ @end|ng |rom er@@mu@mc@n|  Wed Mar 18 22:03:40 2020
From: d@r|zopou|o@ @end|ng |rom er@@mu@mc@n| (D. Rizopoulos)
Date: Wed, 18 Mar 2020 21:03:40 +0000
Subject: [R-sig-ME] Error from glmmTMB().
In-Reply-To: <2ef51b3e-694a-1c9e-7e9f-c30fb946de3f@auckland.ac.nz>
References: <2ef51b3e-694a-1c9e-7e9f-c30fb946de3f@auckland.ac.nz>
Message-ID: <3f34c5f4894d4d2a937691006feafb7d@erasmusmc.nl>

The model seems to fit with the GLMMadaptive package, i.e.,

library("GLMMadaptive")

fm <- mixed_model(cbind(Dead, Alive) ~ (Trt + 0) / Dose, data = X,
                  random = ~ Dose | Rep, 
                  family = beta.binomial(link = "cloglog"), n_phis = 1)

summary(fm)

where

beta.binomial <- function (link = "logit") {
    .link <- link
    env <- new.env(parent = .GlobalEnv)
    assign(".link", link, envir = env)
    stats <- make.link(link)
    dbbinom <- function (x, size, prob, phi, log = FALSE) {
        A <- phi * prob
        B <- phi * (1 - prob)
        log_numerator <- lbeta(x + A, size - x + B)
        log_denominator <- lbeta(A, B)
        fact <- lchoose(size, x)
        if (log) {
            fact + log_numerator - log_denominator
        } else {
            exp(fact + log_numerator - log_denominator)
        }
    }
    log_dens <- function (y, eta, mu_fun, phis, eta_zi) {
        phi <- exp(phis)
        eta <- as.matrix(eta)
        mu_y <- mu_fun(eta)
        out <- if (NCOL(y) == 2L) {
            dbbinom(y[, 1L], y[, 1L] + y[, 2L], mu_y, phi, TRUE)
        } else {
            dbbinom(y, rep(1L, length(y)), mu_y, phi, TRUE)
        }
        attr(out, "mu_y") <- mu_y
        out
    }
    score_eta_fun <- function (y, mu, phis, eta_zi) {
        phi <- exp(phis)
        mu <- as.matrix(mu)
        if (NCOL(y) == 2L) {
            size <- y[, 1L] + y[, 2L]
            y <- y[, 1L]
        } else {
            size <- rep(1L, length(y))
        }
        phi_mu <- phi * mu
        phi_1mu <- phi * (1 - mu)
        comp1 <- (digamma(y + phi_mu) - digamma(size - y + phi_1mu)) * phi
        comp2 <- (digamma(phi_mu) - digamma(phi_1mu)) * phi
        mu.eta <- switch(.link,
                         "logit" = mu - mu * mu,
                         "cloglog" = - (1 - mu) * log(1 - mu))
        out <- (comp1 - comp2) * mu.eta
        out
    }
    score_phis_fun <- function (y, mu, phis, eta_zi) {
        phi <- exp(phis)
        mu <- as.matrix(mu)
        if (NCOL(y) == 2L) {
            size <- y[, 1L] + y[, 2L]
            y <- y[, 1L]
        } else {
            size <- rep(1L, length(y))
        }
        mu1 <- 1 - mu
        phi_mu <- phi * mu
        phi_1mu <- phi * mu1
        comp1 <- digamma(y + phi_mu) * mu + digamma(size - y + phi_1mu) * mu1 - 
            digamma(size + phi)
        comp2 <- digamma(phi_mu) * mu + digamma(phi_1mu) * mu1 - digamma(phi)
        out <- (comp1 - comp2) * phi
        out
    }
    structure(list(family = "beta binomial", link = stats$name, 
                   linkfun = stats$linkfun, linkinv = stats$linkinv, log_dens = log_dens,
                   score_eta_fun = score_eta_fun,
                   score_phis_fun = score_phis_fun),
              class = "family")
}

Best,
Dimitris


-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Rolf Turner
Sent: Sunday, March 15, 2020 4:32 AM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Error from glmmTMB().


I am getting an error, that I have no idea what to do about, from glmmTMB():

library(glmmTMB)
fmla <- cbind(Dead, Alive) ~ (Trt + 0)/Dose + (Dose | Rep)
X    <- dget("X.txt")
fit  <- glmmTMB(fmla, data = X, family = betabinomial(link = "cloglog"),
                dispformula = ~1)

> Error in optimHess(par.fixed, obj$fn, obj$gr) : 
>   gradient in optim evaluated to length 1 not 16 In addition: There 
> were 16 warnings (use warnings() to see them)

The warnings are all repetitions of

> 1: In nlminb(start = par, objective = fn, gradient = gr,  ... :
>   NA/NaN function evaluation

The error sounds to me like something is amiss in the code.

Can anyone confirm/deny/suggest what I might do to get this call to
glmmTMB() to run?

Thanks.

cheers,

Rolf Turner

P.S.  The data set is attached.

--
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

From r@turner @end|ng |rom @uck|@nd@@c@nz  Thu Mar 19 03:15:41 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Thu, 19 Mar 2020 15:15:41 +1300
Subject: [R-sig-ME] Error from glmmTMB().
In-Reply-To: <3f34c5f4894d4d2a937691006feafb7d@erasmusmc.nl>
References: <2ef51b3e-694a-1c9e-7e9f-c30fb946de3f@auckland.ac.nz>
 <3f34c5f4894d4d2a937691006feafb7d@erasmusmc.nl>
Message-ID: <cbd751e1-64d8-07b2-6372-b1bd9237534a@auckland.ac.nz>


Wow.  This is impressive, and fascinating, albeit somewhat mystifying.

I tried your code and yes indeed it runs without complaint; no errors, 
no warnings.

I conclude from this that mixed_model() is capable of fitting models 
using the beta binomial distribution, a fact of which I was previously 
unaware.  (I previously thought that only the glmmTMB package provided 
this capability.)

I gather, after reading TFM a bit more, that mixed_model() can 
accommodate custom-built family functions, such as the beta.binomial()
function that you used to produce a fit to my example.

However writing such custom-built family functions is a bit beyond my
very limited capabilities.

I am currently struggling to find suitable models to fit to a number of
rather recalcitrant data sets.  The model presented in my question to
r-sig-mixed-models was just one of a fairly large number with which I 
would like to experiment.  The data in question appear to exhibit 
over-dispersion w.r.t. the binomial distribution.  The random effects 
models that I am fitting seem to produce problematic results; I want to 
try using the beta binomial distribution in addition to (or perhaps 
instead of) random effects modelling.

With glmmTMB() I can (with *some* data sets) fit models like

     glmmTMB(cbind(Dead, Alive) ~ (Trt + 0)/Dose + (Dose | Rep),
             data = X, family = betabinomial(link = "cloglog"),
                dispformula = ~Dose)

I can also set dispformula equal to ~poly(Dose,2) or ~splines::ns(Dose,2).

Can custom-built family functions be created to do this sort of thing 
using mixed_model()?

If so, what would the appropriate value of n_phis be?  I am *guessing* 
that for dispformula = ~Dose, n_phis would be set equal to 2, and for
dispformula = ~poly(Dose,2) it would be 3.  What about for
dispformula = ~splines::ns(Dose,2) ?

Also (sorry for going on!) with glmmTMB() there is the possibility of
omitting random effects entirely and just trying to accommodate 
over-dispersion by means of the beta binomial distribution:

glmmTMB(cbind(Dead, Alive) ~ (Trt + 0)/Dose,data = X,
        family = betabinomial(link = "cloglog"),
                dispformula = ~Dose)

Omitting random effects does not seem to be possible with mixed_model().
Is that correct?  Or have I simply not been using the right syntax?

Thanks for any words of wisdom that you can spare me.

cheers,

Rolf

On 19/03/20 10:03 am, D. Rizopoulos wrote:

> The model seems to fit with the GLMMadaptive package, i.e.,
> 
> library("GLMMadaptive")
> 
> fm <- mixed_model(cbind(Dead, Alive) ~ (Trt + 0) / Dose, data = X,
>                    random = ~ Dose | Rep,
>                    family = beta.binomial(link = "cloglog"), n_phis = 1)
> 
> summary(fm)
> 
> where
> 
> beta.binomial <- function (link = "logit") {
>      .link <- link
>      env <- new.env(parent = .GlobalEnv)
>      assign(".link", link, envir = env)
>      stats <- make.link(link)
>      dbbinom <- function (x, size, prob, phi, log = FALSE) {
>          A <- phi * prob
>          B <- phi * (1 - prob)
>          log_numerator <- lbeta(x + A, size - x + B)
>          log_denominator <- lbeta(A, B)
>          fact <- lchoose(size, x)
>          if (log) {
>              fact + log_numerator - log_denominator
>          } else {
>              exp(fact + log_numerator - log_denominator)
>          }
>      }
>      log_dens <- function (y, eta, mu_fun, phis, eta_zi) {
>          phi <- exp(phis)
>          eta <- as.matrix(eta)
>          mu_y <- mu_fun(eta)
>          out <- if (NCOL(y) == 2L) {
>              dbbinom(y[, 1L], y[, 1L] + y[, 2L], mu_y, phi, TRUE)
>          } else {
>              dbbinom(y, rep(1L, length(y)), mu_y, phi, TRUE)
>          }
>          attr(out, "mu_y") <- mu_y
>          out
>      }
>      score_eta_fun <- function (y, mu, phis, eta_zi) {
>          phi <- exp(phis)
>          mu <- as.matrix(mu)
>          if (NCOL(y) == 2L) {
>              size <- y[, 1L] + y[, 2L]
>              y <- y[, 1L]
>          } else {
>              size <- rep(1L, length(y))
>          }
>          phi_mu <- phi * mu
>          phi_1mu <- phi * (1 - mu)
>          comp1 <- (digamma(y + phi_mu) - digamma(size - y + phi_1mu)) * phi
>          comp2 <- (digamma(phi_mu) - digamma(phi_1mu)) * phi
>          mu.eta <- switch(.link,
>                           "logit" = mu - mu * mu,
>                           "cloglog" = - (1 - mu) * log(1 - mu))
>          out <- (comp1 - comp2) * mu.eta
>          out
>      }
>      score_phis_fun <- function (y, mu, phis, eta_zi) {
>          phi <- exp(phis)
>          mu <- as.matrix(mu)
>          if (NCOL(y) == 2L) {
>              size <- y[, 1L] + y[, 2L]
>              y <- y[, 1L]
>          } else {
>              size <- rep(1L, length(y))
>          }
>          mu1 <- 1 - mu
>          phi_mu <- phi * mu
>          phi_1mu <- phi * mu1
>          comp1 <- digamma(y + phi_mu) * mu + digamma(size - y + phi_1mu) * mu1 -
>              digamma(size + phi)
>          comp2 <- digamma(phi_mu) * mu + digamma(phi_1mu) * mu1 - digamma(phi)
>          out <- (comp1 - comp2) * phi
>          out
>      }
>      structure(list(family = "beta binomial", link = stats$name,
>                     linkfun = stats$linkfun, linkinv = stats$linkinv, log_dens = log_dens,
>                     score_eta_fun = score_eta_fun,
>                     score_phis_fun = score_phis_fun),
>                class = "family")
> }
> 
> Best,
> Dimitris
> 
> 
> -----Original Message-----
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Rolf Turner
> Sent: Sunday, March 15, 2020 4:32 AM
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Error from glmmTMB().
> 
> 
> I am getting an error, that I have no idea what to do about, from glmmTMB():
> 
> library(glmmTMB)
> fmla <- cbind(Dead, Alive) ~ (Trt + 0)/Dose + (Dose | Rep)
> X    <- dget("X.txt")
> fit  <- glmmTMB(fmla, data = X, family = betabinomial(link = "cloglog"),
>                  dispformula = ~1)
> 
>> Error in optimHess(par.fixed, obj$fn, obj$gr) :
>>    gradient in optim evaluated to length 1 not 16 In addition: There
>> were 16 warnings (use warnings() to see them)
> 
> The warnings are all repetitions of
> 
>> 1: In nlminb(start = par, objective = fn, gradient = gr,  ... :
>>    NA/NaN function evaluation
> 
> The error sounds to me like something is amiss in the code.
> 
> Can anyone confirm/deny/suggest what I might do to get this call to
> glmmTMB() to run?


From bbo|ker @end|ng |rom gm@||@com  Thu Mar 19 03:24:15 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 18 Mar 2020 22:24:15 -0400
Subject: [R-sig-ME] Error from glmmTMB().
In-Reply-To: <FA458F7A-4288-485C-BB47-570145F713EF@gmail.com>
References: <2ef51b3e-694a-1c9e-7e9f-c30fb946de3f@auckland.ac.nz>
 <FA458F7A-4288-485C-BB47-570145F713EF@gmail.com>
Message-ID: <485c163b-028a-0b18-c79a-f00a2f7bc4f8@gmail.com>


  I did a little more poking around with this.

  It works (i.e. fits with no warnings with binomial() instead of
betabinomial().

  It works, sort of (i.e. gives 1 warning about NaN evaluation and a
non-pos-def Hessian warning, probably from an effectively singular
model, but gives sensible fixed-effect estimates and std devs) with
link=logit instead of cloglog.

   Debugging my way through glmmTMB:::fitTMB(), even the first
evaluation of the objective function gives NaN, and the gradient is a
vector of NaN values.  I suspect we have a failure to clamp the cloglog
function somewhere, and may know how to fix it.

  This is a very nice, simple example that may help us improve a bunch
of stuff (it will also help us put some tests in useful places to get
*slightly* less opaque error messages).

  I know this doesn't solve your problem immediately - but if I'm lucky
and have a bit of time I may be able to implement something soon.

  thanks Rolf!

  Ben

On 2020-03-17 9:50 a.m., Mollie Brooks wrote:
> It may be an overparameterization issue because I was able to get simpler models to work. We could make the error message clearer, but tried to document it at the bottom of the troubleshooting vignette. The general advice in that case is to (1) try rescaling predictor variables, (2) try a simpler model and build up, and (3) try different starting values, possibly based on a simpler model. 
> 
> I tried scaling Dose that that didn?t help. 
> 
> To get the right shape for starting values, you could check the structures in the unfitted model
> 
> mod  <- glmmTMB(fmla, data = X, family = betabinomial(link = "cloglog"),
>               dispformula = ~1, doFit=FALSE)
> 
> dimnames(mod$data.tmb$X)[[2]]
> 
> dimnames(mod$data.tmb$Z)[[2]]
> 
> str(mod$parameters)
> 
> start <- list(beta=rep(0.1, 12), b=rep(.01, 12)) #fill in better values based on your best guess
> 
> fit3  <- glmmTMB(cbind(Dead, Alive) ~ Trt+0 +Dose + (1 | Rep), data = X, family = betabinomial(link = "cloglog"))
> 
> 
> start <- list(beta = c(fixef(fit3)[[1]][1:6], rep(.1, 6)),
> 	b=c(unlist(ranef(fit3)[[1]]$Rep), rep(.1, 6)))
> 	
> fit  <- glmmTMB(fmla, data = X, family = betabinomial(link = "cloglog"), start=start)
> 
> That was my best attempt at getting it to run, but it doesn?t work. 
> 
> If you think this is a bug, you could add it to the issue tracker on GitHub.
> 
> cheers,
> Mollie
> 
>> On 15Mar 2020, at 4:32, Rolf Turner <r.turner at auckland.ac.nz> wrote:
>>
>>
>> I am getting an error, that I have no idea what to do about, from glmmTMB():
>>
>> library(glmmTMB)
>> fmla <- cbind(Dead, Alive) ~ (Trt + 0)/Dose + (Dose | Rep)
>> X    <- dget("X.txt")
>> fit  <- glmmTMB(fmla, data = X, family = betabinomial(link = "cloglog"),
>>               dispformula = ~1)
>>
>>> Error in optimHess(par.fixed, obj$fn, obj$gr) :   gradient in optim evaluated to length 1 not 16
>>> In addition: There were 16 warnings (use warnings() to see them)
>>
>> The warnings are all repetitions of
>>
>>> 1: In nlminb(start = par, objective = fn, gradient = gr,  ... :
>>>  NA/NaN function evaluation
>>
>> The error sounds to me like something is amiss in the code.
>>
>> Can anyone confirm/deny/suggest what I might do to get this call to glmmTMB() to run?
>>
>> Thanks.
>>
>> cheers,
>>
>> Rolf Turner
>>
>> P.S.  The data set is attached.
>>
>> -- 
>> Honorary Research Fellow
>> Department of Statistics
>> University of Auckland
>> Phone: +64-9-373-7599 ext. 88276
>> <X.txt>_______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From bbo|ker @end|ng |rom gm@||@com  Thu Mar 19 03:52:57 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 18 Mar 2020 22:52:57 -0400
Subject: [R-sig-ME] Error from glmmTMB().
In-Reply-To: <cbd751e1-64d8-07b2-6372-b1bd9237534a@auckland.ac.nz>
References: <2ef51b3e-694a-1c9e-7e9f-c30fb946de3f@auckland.ac.nz>
 <3f34c5f4894d4d2a937691006feafb7d@erasmusmc.nl>
 <cbd751e1-64d8-07b2-6372-b1bd9237534a@auckland.ac.nz>
Message-ID: <c93574ab-c0e1-87b9-8211-8f2607765802@gmail.com>


  Hijacking the stream back slightly in the direction of glmmTMB (not
intentional, just lazy): I made a small change that appears to allow the
original model to run correctly.  If you want to try it out, and have
development tools (compilers etc.) and the remotes package installed,

remotes::install_github("glmmTMB/glmmTMB/glmmTMB at cloglog_fix")

  should install the patched version (I think).

This will presumably be merged with the master branch sometime soon but
I want to add some tests etc.

On 2020-03-18 10:15 p.m., Rolf Turner wrote:
> 
> Wow.? This is impressive, and fascinating, albeit somewhat mystifying.
> 
> I tried your code and yes indeed it runs without complaint; no errors,
> no warnings.
> 
> I conclude from this that mixed_model() is capable of fitting models
> using the beta binomial distribution, a fact of which I was previously
> unaware.? (I previously thought that only the glmmTMB package provided
> this capability.)
> 
> I gather, after reading TFM a bit more, that mixed_model() can
> accommodate custom-built family functions, such as the beta.binomial()
> function that you used to produce a fit to my example.
> 
> However writing such custom-built family functions is a bit beyond my
> very limited capabilities.
> 
> I am currently struggling to find suitable models to fit to a number of
> rather recalcitrant data sets.? The model presented in my question to
> r-sig-mixed-models was just one of a fairly large number with which I
> would like to experiment.? The data in question appear to exhibit
> over-dispersion w.r.t. the binomial distribution.? The random effects
> models that I am fitting seem to produce problematic results; I want to
> try using the beta binomial distribution in addition to (or perhaps
> instead of) random effects modelling.
> 
> With glmmTMB() I can (with *some* data sets) fit models like
> 
> ??? glmmTMB(cbind(Dead, Alive) ~ (Trt + 0)/Dose + (Dose | Rep),
> ??????????? data = X, family = betabinomial(link = "cloglog"),
> ?????????????? dispformula = ~Dose)
> 
> I can also set dispformula equal to ~poly(Dose,2) or ~splines::ns(Dose,2).
> 
> Can custom-built family functions be created to do this sort of thing
> using mixed_model()?
> 
> If so, what would the appropriate value of n_phis be?? I am *guessing*
> that for dispformula = ~Dose, n_phis would be set equal to 2, and for
> dispformula = ~poly(Dose,2) it would be 3.? What about for
> dispformula = ~splines::ns(Dose,2) ?
> 
> Also (sorry for going on!) with glmmTMB() there is the possibility of
> omitting random effects entirely and just trying to accommodate
> over-dispersion by means of the beta binomial distribution:
> 
> glmmTMB(cbind(Dead, Alive) ~ (Trt + 0)/Dose,data = X,
> ?????? family = betabinomial(link = "cloglog"),
> ?????????????? dispformula = ~Dose)
> 
> Omitting random effects does not seem to be possible with mixed_model().
> Is that correct?? Or have I simply not been using the right syntax?
> 
> Thanks for any words of wisdom that you can spare me.
> 
> cheers,
> 
> Rolf
> 
> On 19/03/20 10:03 am, D. Rizopoulos wrote:
> 
>> The model seems to fit with the GLMMadaptive package, i.e.,
>>
>> library("GLMMadaptive")
>>
>> fm <- mixed_model(cbind(Dead, Alive) ~ (Trt + 0) / Dose, data = X,
>> ?????????????????? random = ~ Dose | Rep,
>> ?????????????????? family = beta.binomial(link = "cloglog"), n_phis = 1)
>>
>> summary(fm)
>>
>> where
>>
>> beta.binomial <- function (link = "logit") {
>> ???? .link <- link
>> ???? env <- new.env(parent = .GlobalEnv)
>> ???? assign(".link", link, envir = env)
>> ???? stats <- make.link(link)
>> ???? dbbinom <- function (x, size, prob, phi, log = FALSE) {
>> ???????? A <- phi * prob
>> ???????? B <- phi * (1 - prob)
>> ???????? log_numerator <- lbeta(x + A, size - x + B)
>> ???????? log_denominator <- lbeta(A, B)
>> ???????? fact <- lchoose(size, x)
>> ???????? if (log) {
>> ???????????? fact + log_numerator - log_denominator
>> ???????? } else {
>> ???????????? exp(fact + log_numerator - log_denominator)
>> ???????? }
>> ???? }
>> ???? log_dens <- function (y, eta, mu_fun, phis, eta_zi) {
>> ???????? phi <- exp(phis)
>> ???????? eta <- as.matrix(eta)
>> ???????? mu_y <- mu_fun(eta)
>> ???????? out <- if (NCOL(y) == 2L) {
>> ???????????? dbbinom(y[, 1L], y[, 1L] + y[, 2L], mu_y, phi, TRUE)
>> ???????? } else {
>> ???????????? dbbinom(y, rep(1L, length(y)), mu_y, phi, TRUE)
>> ???????? }
>> ???????? attr(out, "mu_y") <- mu_y
>> ???????? out
>> ???? }
>> ???? score_eta_fun <- function (y, mu, phis, eta_zi) {
>> ???????? phi <- exp(phis)
>> ???????? mu <- as.matrix(mu)
>> ???????? if (NCOL(y) == 2L) {
>> ???????????? size <- y[, 1L] + y[, 2L]
>> ???????????? y <- y[, 1L]
>> ???????? } else {
>> ???????????? size <- rep(1L, length(y))
>> ???????? }
>> ???????? phi_mu <- phi * mu
>> ???????? phi_1mu <- phi * (1 - mu)
>> ???????? comp1 <- (digamma(y + phi_mu) - digamma(size - y + phi_1mu))
>> * phi
>> ???????? comp2 <- (digamma(phi_mu) - digamma(phi_1mu)) * phi
>> ???????? mu.eta <- switch(.link,
>> ????????????????????????? "logit" = mu - mu * mu,
>> ????????????????????????? "cloglog" = - (1 - mu) * log(1 - mu))
>> ???????? out <- (comp1 - comp2) * mu.eta
>> ???????? out
>> ???? }
>> ???? score_phis_fun <- function (y, mu, phis, eta_zi) {
>> ???????? phi <- exp(phis)
>> ???????? mu <- as.matrix(mu)
>> ???????? if (NCOL(y) == 2L) {
>> ???????????? size <- y[, 1L] + y[, 2L]
>> ???????????? y <- y[, 1L]
>> ???????? } else {
>> ???????????? size <- rep(1L, length(y))
>> ???????? }
>> ???????? mu1 <- 1 - mu
>> ???????? phi_mu <- phi * mu
>> ???????? phi_1mu <- phi * mu1
>> ???????? comp1 <- digamma(y + phi_mu) * mu + digamma(size - y +
>> phi_1mu) * mu1 -
>> ???????????? digamma(size + phi)
>> ???????? comp2 <- digamma(phi_mu) * mu + digamma(phi_1mu) * mu1 -
>> digamma(phi)
>> ???????? out <- (comp1 - comp2) * phi
>> ???????? out
>> ???? }
>> ???? structure(list(family = "beta binomial", link = stats$name,
>> ??????????????????? linkfun = stats$linkfun, linkinv = stats$linkinv,
>> log_dens = log_dens,
>> ??????????????????? score_eta_fun = score_eta_fun,
>> ??????????????????? score_phis_fun = score_phis_fun),
>> ?????????????? class = "family")
>> }
>>
>> Best,
>> Dimitris
>>
>>
>> -----Original Message-----
>> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On
>> Behalf Of Rolf Turner
>> Sent: Sunday, March 15, 2020 4:32 AM
>> To: r-sig-mixed-models at r-project.org
>> Subject: [R-sig-ME] Error from glmmTMB().
>>
>>
>> I am getting an error, that I have no idea what to do about, from
>> glmmTMB():
>>
>> library(glmmTMB)
>> fmla <- cbind(Dead, Alive) ~ (Trt + 0)/Dose + (Dose | Rep)
>> X??? <- dget("X.txt")
>> fit? <- glmmTMB(fmla, data = X, family = betabinomial(link = "cloglog"),
>> ???????????????? dispformula = ~1)
>>
>>> Error in optimHess(par.fixed, obj$fn, obj$gr) :
>>> ?? gradient in optim evaluated to length 1 not 16 In addition: There
>>> were 16 warnings (use warnings() to see them)
>>
>> The warnings are all repetitions of
>>
>>> 1: In nlminb(start = par, objective = fn, gradient = gr,? ... :
>>> ?? NA/NaN function evaluation
>>
>> The error sounds to me like something is amiss in the code.
>>
>> Can anyone confirm/deny/suggest what I might do to get this call to
>> glmmTMB() to run?
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From r@turner @end|ng |rom @uck|@nd@@c@nz  Thu Mar 19 09:02:37 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Thu, 19 Mar 2020 21:02:37 +1300
Subject: [R-sig-ME] [FORGED] Re:  Error from glmmTMB().
In-Reply-To: <485c163b-028a-0b18-c79a-f00a2f7bc4f8@gmail.com>
References: <2ef51b3e-694a-1c9e-7e9f-c30fb946de3f@auckland.ac.nz>
 <FA458F7A-4288-485C-BB47-570145F713EF@gmail.com>
 <485c163b-028a-0b18-c79a-f00a2f7bc4f8@gmail.com>
Message-ID: <34d3ce25-7c5f-9cca-3557-1de933ab30b8@auckland.ac.nz>


On 19/03/20 3:24 pm, Ben Bolker wrote:

> 
>    I did a little more poking around with this.
> 
>    It works (i.e. fits with no warnings with binomial() instead of
> betabinomial().

Yep.

>    It works, sort of (i.e. gives 1 warning about NaN evaluation and a
> non-pos-def Hessian warning, probably from an effectively singular
> model, but gives sensible fixed-effect estimates and std devs) with
> link=logit instead of cloglog.

Yep.

>     Debugging my way through glmmTMB:::fitTMB(), even the first
> evaluation of the objective function gives NaN, and the gradient is a
> vector of NaN values.  I suspect we have a failure to clamp the cloglog
> function somewhere, and may know how to fix it.

I'm impressed.  Wish I had your brains and insight.  Said he, wistfully.

>    This is a very nice, simple example that may help us improve a bunch
> of stuff (it will also help us put some tests in useful places to get
> *slightly* less opaque error messages).
> 
>    I know this doesn't solve your problem immediately - but if I'm lucky
> and have a bit of time I may be able to implement something soon.
> 
>    thanks Rolf!

You are indeed welcome.  I'm glad that you found this example useful. 
Seems to be my main way of contributing to the advancement of 
statistical computing.  I've made similar contributions to Adrian 
Baddeley's spatstat package.  I have come to refer to my propensity for 
stumbling on examples that make software turn to custard as my "Sadim 
Touch".

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r@turner @end|ng |rom @uck|@nd@@c@nz  Thu Mar 19 09:09:05 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Thu, 19 Mar 2020 21:09:05 +1300
Subject: [R-sig-ME] [FORGED] Re:  Error from glmmTMB().
In-Reply-To: <c93574ab-c0e1-87b9-8211-8f2607765802@gmail.com>
References: <2ef51b3e-694a-1c9e-7e9f-c30fb946de3f@auckland.ac.nz>
 <3f34c5f4894d4d2a937691006feafb7d@erasmusmc.nl>
 <cbd751e1-64d8-07b2-6372-b1bd9237534a@auckland.ac.nz>
 <c93574ab-c0e1-87b9-8211-8f2607765802@gmail.com>
Message-ID: <ef6a627d-522e-c997-14fb-e5b24ba9c8f0@auckland.ac.nz>


On 19/03/20 3:52 pm, Ben Bolker wrote:

>    Hijacking the stream back slightly in the direction of glmmTMB (not
> intentional, just lazy): I made a small change that appears to allow the
> original model to run correctly.  If you want to try it out, and have
> development tools (compilers etc.) and the remotes package installed,
> 
> remotes::install_github("glmmTMB/glmmTMB/glmmTMB at cloglog_fix")
> 
>    should install the patched version (I think).
> 
> This will presumably be merged with the master branch sometime soon but
> I want to add some tests etc.

Jeez!  That was quick.  I should be able to do the 
remotes::install_github() --- but it's getting on for my bedtime!  We 
old men need our beauty sleep.  (And then some!).  I'll give it a burl
tomorrow morning, Orcland time, and let you know how I get on.

Thanks very much.

Stay safe, avoid crowds, and use lots of hand-sanitizer! :-)

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From b|mono@om @end|ng |rom gm@||@com  Thu Mar 19 09:21:32 2020
From: b|mono@om @end|ng |rom gm@||@com (=?UTF-8?B?UmVuw6k=?=)
Date: Thu, 19 Mar 2020 09:21:32 +0100
Subject: [R-sig-ME] Hierarchical Psychometric Function in BRMS
In-Reply-To: <DM5PR1901MB200778B339DD94928E54A7E9EAF70@DM5PR1901MB2007.namprd19.prod.outlook.com>
References: <DM5PR1901MB2007F953970156594F62EBFAEAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <CADcpBHPFYfZcE+13BseKceTWZf0vU8tGF6pvjkMtYyh6_Zy=9A@mail.gmail.com>
 <DM5PR1901MB2007ED6A3460670F84E40537EAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <DM5PR1901MB200773E896948650DCB58EB1EAF90@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <CADcpBHPvR6izCGq5heOk6bf0SNtNq5v697kk0R1ON2fYcJgDpg@mail.gmail.com>
 <DM5PR1901MB20075C5C3C055F353A4FE9F6EAF70@DM5PR1901MB2007.namprd19.prod.outlook.com>
 <CADcpBHMrYnvBCo=M=uJH7w7gmaZGR3u98YSezX7eFG+TqJE+Fg@mail.gmail.com>
 <DM5PR1901MB200778B339DD94928E54A7E9EAF70@DM5PR1901MB2007.namprd19.prod.outlook.com>
Message-ID: <CADcpBHO8ZH6Wf+UxSs1_1SqUcs_QYiGhwiwGbTNWX0uqOvH8Gw@mail.gmail.com>

Hey James,

please don't get me wrong. I am just saying: What you try to find out
psychologically does not require a psychometric function, and doing so adds
unnecessary complexity which is usually not preferred. But, of course, if
your general goal is to understand how to implement a logistic non-linear
model, you can just go ahead. But from everything you have said so far, I
would "predict" the model you have in mind will definitely not converge
(because it is not identifiable), and if so, it will be uninformative. (So,
for practicing modeling it would be good to have a nicer example. For
instance, every participant gets the same stimuli and the same RW's and
then they vary in there Accuracy and this variation in accuracy between
items and participants allows you to estimate item difficulty and
participant ability on a latent scale). -- In your paradigm both,
variations in item difficulty and participant accuracy are simply
eliminated by the procedure (staircase; i.e., no estimation of psychometric
functions based on those assumptions possible).

A simplified graphical illustration:

This is how the participants' behavior should be distributed over the
ongoing trials (simplified).

p(accurate)
100%
^
|----
|
|     --               ----------------------------------------   (about)
80%
|             -  ----
|  --  --   - --
|
| -       ..
0-------------------------------------------------------> ongoing trials
(starting at the beginning

In words: Due to the variations between the participants abilities, given
they all have the same -initial - RW, there is some variance in accuracy
-in the beginning of trials-, but this variance disappears over time due to
the staircase procedure;  eventually, all participants will reach the same
accuracy ceiling of 80% (4pass1fail4pass1fail...). From the moment that the
ceiling is reached, there is basically only noise in the data (random
errors), which means -- for a psychometric function -- you can throw away
these trials.  Not doing so would mean you try to fit noise based on
"norms", which is overfitting (by definition). The psychometric function,
as you want to implement it, would require a continuous relation between RW
and accuracy. This, however, is only true in the very first trials, due to
the staircase procedure. And without between participant variance in
accuracy, there is no way to estimate differences in ability based on
(constant) accuracy.
But as already outlined, the method (staircase) systematically gives you -
the ability equivalent - as  the variance in the accuracy is systematically
eliminated based on RW. This means, you "transferred" the variance you are
actually interested in from "response" to "RW" by methodological means.
Hence, your DV should be RW.


There is really nothing I can say more to this case :) except, : using
functions just because others do might not be the best way to justify
analyses. One very common example what -A LOT- of researchers are doing is
to calculate classic ANOVAs on dependent variables of "percent correct" (or
"percent Response X"). Thus, although this is a binomial, a lot of
researchers use parametric tests on averaged accuracy (or choices). Even
worse: What you can see in a lot of studies on the IOWA gambling task is,
that not simply p(response B) is taken to indicate the DV, but p(B) minus
p(A), with the (pseudo) argument that this reflects some
"action-direction-effect" or similar things (like: "we predicted that B
should be chosen more often, hence we expect the p(B) - p(A) to be
positive). Indeed, by looking at the literature  one could say this is the
canonical way of doing so... However, it is also "very problematic"
because, in such studies, p(A) and p(B) are mutually exclusive, such that
p(B) + p(A) = 1, ( but doing it correctly, i.e., testing p(a) against p=.5,
would unfortunately reduce the effect size from 14% difference to 7%
deviation from chance... and that is - an argument, I guess). So the
general message is :  Trust nobody but your own sanity. :)

(Unfortunately, I will not be able to continue this thread. )

Best
Ren?


Am Mi., 18. M?rz 2020 um 19:33 Uhr schrieb Ades, James <
jades at health.ucsd.edu>:

> Hi Rene,
>
> Yes, in an ideal world each participant would end up at 80% threshold. The
> reason I lowered it to 70% was because it was clear that many participants
> did not achieve that threshold. Why a good deal of students didn't achieve
> that is something for the methods section. Taking the final rw would be one
> way of doing it (as would average RW, which we also look at), but I think
> since a psychometric function takes into account the entire sampled RW
> distribution for each participant, it provides a more principled way of
> looking at a participant response.
>
> I don't necessarily think gamma is essential (a participant would have 50%
> of getting a trial correct), but from everything I've read, people
> generally include it as a parameter. How a hierarchical model might change
> that, I'm not sure.
>
> I do look at other performance methods in the paper, but one of them is
> psychometric function, so I'm really just trying to figure out how to
> change my psychometric model to be accurate within a hierarchical, Bayesian
> framework.
>
> Thanks,
>
> James
> ------------------------------
> *From:* Ren? <bimonosom at gmail.com>
> *Sent:* Wednesday, March 18, 2020 8:53 AM
> *To:* Ades, James <jades at health.ucsd.edu>
> *Cc:* r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> *Subject:* Re: [R-sig-ME] Hierarchical Psychometric Function in BRMS
>
> Hey James,
>
> I think the remaining questions are:
> 1) (Why) should one use RW (continuous) instead of RESPONSE (binary).
> 2) Is gamma (a guessing parameter) necessary for this.
> (and a clarification below)
>
> 1)
> if twenty kids run a mile, they will all have different times. Students
> should be able to get 70% correct (the tasks are not inherently difficult),
> it?s a question of what amount of time (how slow) is necessary in order for
> them to achieve that 70% correct.
> The case you have is:
> Your kids run a mile and somebody says "pass" or "fail", because they
> either did it in time or not, respectively. If they did it ("pass") you say
> "Well that was obviously too easy for you, I want to find out if you can
> also do it, if I raise the criterion by 10ms". And if the kids "fail" you
> say "Well this was obviously too difficult for you, here is a little bit
> more time (40ms), let's see whether you pass now."  Now : if fail lowers RW
> by 40ms, and pass raises it by 10ms, then one has 4 remaining steps to
> reach the level again at which one fails again. Meaning 4 right 1 wrong, 4
> right 1 wrong, 4 right 1 wrong.... Just to be most clear: If my "ability"
> let's me pass at criterion of 500ms, but not further, then when I reach
> 490ms, I will start failing. Let's just play it through: 500->pass;
> 490->fail; 530->pass; 520->pass; 510->pass; 500->pass; 490->fail; 530->pass
> (the circle continues)... and so on- in the long run this means you
> approach 4 passes, 1 fail, 4 passes, 1 fail ... which means the ratio of
> accuracy will be -- for every participant --  4/5 = 80%. Now, the  average
> time window for these 5-trial-circles will be (490+500+510+520+530) / 5 =
> 510; this means 510ms corresponds to 80% accuracy for this participant, and
> thus indicates his/her ability to reach 80% accuracy. Of course, the
> participants will differ in their abilities, - another participant has the
> maximum ability to pass the criterion of only 600ms; thus 600->pass;
> 590->fail; 630->pass; 620.... thus,  610ms (on average) corresponds to 80%
> accuracy. You see where this is going? Due to the test-procedure, every
> participant ,meanders around 80% probably hardly reaching it (due to
> behavioral noise). But the idea is - everybody is at 80%. And RW -
> directly- tells you the ability of each participants, and this is extremely
> nice (!) because you do not need to infer the participants ability
> statistically anymore - you precisely measured it. I think, there is no
> point in plugging a psychometric function in now. It adds no information.
>
> So if you would simply change your question from "What is the RW threshold
> to reach 70%" to "What is the RW threshold to reach 80% accuracy" then you
> already have your answer: It is the final average response window of each
> participant (due to the staircase procedure). (But one can still see
> whether this RW varies between conditions) -- So I would suggest to change
> the question, unless there is something very specific about 70%. But as you
> noted for yourself, that you initially started of with 80% ... well you
> might just rely on your test-procedure (staircase), which I think nobody
> will argue about is valid.
>
> 2) Since the procedural design basically forbids guessing, there is no way
> of "identifying" guessing parameters in further analyses.
>
> Remaining note on my previous point 4) - I was referring to the four
> -time-points-  (sessions) not RW, which might resolve the question.
>
> Best
> Ren?
>
>
>
>
>
> Am Mi., 18. M?rz 2020 um 04:59 Uhr schrieb Ades, James <
> jades at health.ucsd.edu>:
>
> Hi Rene,
>
> See comments in-line below, but I think the largest issue looking at your
> model is that you remove "response" as a DV, which means that we no longer
> have a psychometric function, despite the fact that we are dealing with
> binomial data.
>
> ?Hey James,
>
> thank you for these details. Step by step:
>
> "1) Yes, essentially. So there are 7 tasks, some have two conditions. One
> has four conditions. This is the "condition" in the model. "Norm" is the
> normalized response window."
> R1) I am sorry, I do not understand this. Does "condition" indicate the 14
> tasks (i.e., with 14 factor levels) or the "some have two, some have four
> conditions part?" If it is the latter, then why did you not include 7
> "tasks" alternatively ? - Anyway - I actually would suggest using the 14
> tasks as "condition", because the design matrix is not fully crossed.
> (i.e., without any design, just all tasks; you still can perform post-hoc
> comparisons).
> Condition = 14 factor levels which is every condition of every task.
>
> 2) The term response window is not self-explaining..., but I assume you
> mean "time pressure" by this (how long do I have to give a response). And I
> will go on to refer to this as such.
> 2b) Given "norm" is "time" then I can finally see where you want to go.
> (Please correct me if I am wrong:
> ?Overall, I think the jargon of the paradigms/fields is confusing
> communication. Just think of "Norm" as the normalized response window. If
> we're doing hierarchical, it's possible also that RW no longer needs to be
> standardized.
>
>
> 3. No offense, my choice of words was a bit clumsy. I mean a
> clarification about the research question or psychological hypothesis about
> which measure should predict another measure is always helpful to make
> judgments about a models appropriateness. As noted: I get a grip now, and
> it seems, you want to predict decision accuracy ("response") based on the
> task ("condition") and the time provided to solve the task ("norm"). While
> "norm" is a time window to complete the task, dynamically changing
> depending on the accuracy (tailored testing). Now having spelled this out
> reveals a circular causation in it: accuracy -> time window -> accuracy? It
> would be good to search for a reference paper which used an equivalent
> design (not just psychometric function). But to put it this way: Accuracy
> (response) is not really informative, because the tasks (if they are
> tailored) are -specifically designed- to that each participant has about
> 75% accuracy. That is, everybody will either pass a threshold (e.g., 70%)
> or not (e.g., 80%), because everybody will be at 75%.  What IS informative
> is how much time they need for achieving this. The underlying assumption is
> that there is a level of "processing speed" which is just before I become
> perfectly accurate, and the goal is to find this moment, because if I WOULD
> (otherwise) be perfectly accurate in every task my ability is
> unidentifiable (because the tasks were not difficult enough, or
> statistically speaking: no variance), - but if I was only guessing then any
> model about me is uninformative (guessing model).
>
> I see what you?re saying, but I don?t think the conclusion is accurate: if
> twenty kids run a mile, they will all have different times. Students should
> be able to get 70% correct (the tasks are not inherently difficult), it?s a
> question of what amount of time (how slow) is necessary in order for them
> to achieve that 70% correct. Norm (we might as well refer to it as response
> window (RW)) is a function of both time and response (accuracy), since
> students not responding within the allotted amount of time, will get that
> trial wrong, and the response window will slow (by 40ms); if they get it
> correct, response window increases by 10ms (the technical term is a
> ?staircase procedure?). You write: ?What IS informative is how much time
> they need for achieving this.? Yes, this is absolutely correct. At 70%
> probability, what is the response window for each participant for each
> condition (this would be the 70% threshold, a latent variable).
>
> 3b. In other words, if you are searching for a latent ability that you
> want to continuously describe in your sample, "response window" (time
> needed) is the indicator. slow participants = low ability ; quick
> participants = high ability.
> In Item-Response-Theory you usually estimate the ability, while presenting
> the same tasks to all participants (fully crossed) which allows to estimate
> task difficulty (instead of manipulating it), and I would suggest searching
> for related model solutions in this area. (I am not experienced in tailored
> testing).
>
> Yes, absolutely. Again, this is where I think paradigms are confusing us.
>
>
> 4. If you standardize the measurements within each of the four sessions,
> ?What measurements are you referring to here? RW?
> then I would say there is no reason to further include the term in the
> model.
> Wouldn't you have to include RW in the model?
> This, however, is a matter of theoretical rather than statistical debate.
> One theoretical counter-argument could be: If you do not standardize the
> measures, but simply include time-points as fixed effects in the model,
> then you gain information (i.e., about the time effect), without altering
> the content of your model (although you change a fixed assumption - to a
> freely estimable one). You then could also take into account, that some
> participants improve more quickly then others, which would be a reasonable
> thing to do, if you think, that this is a thing.
> ?The essence of what you're writing here seems appetizing, but I'm not
> following. How could you get around not including response window in the
> model?
>
> 5. What Treutwein and Strasburger write is, first, mainly about logistic
> functions which have the most basic form of a one - parameter Rasch model.
> Make a two-parameter Rasch model out of it, then you have the functional
> form of standard logistic regression, as also performed in "lmer" and
> "brms" if you write something like:
> DV~Interceptvariable*Continuousvariable+(1|subjectID) + (1|trialID),
> family=binomial(link=logit). with two differences 1) the R packages use a
> different parameterization (e.g. dummy coding) 2) in Rasch models (or Item
> Response Theory) you estimate the model terms based on items and
> individuals, rather than predicting the DV based on conditions and
> measurements (here is a paper that investigates the relation between
> logistic models to predict accuracy and item response theory: Dixon, 2008,
> Models of accuracy in repeated-measures designs). This should help getting
> a "feeling" for the logistic function.
>
> Then what Treutwein and Strasburger introduce can also be found in every
> text-book namely gamma, which is a guessing parameter (gamma +
> 1/(1+exp(...))) which says the model can not predict 0 accuracy unless
> gamma = 0, because something will always be`correct' by chance. Secondly,
> however, adding gamma would lead the model to predictions larger than 1,
> for why there is (1-gamma) involved.
> ?Makes sense.
> Third, the model assumes that 100% accuracy might not be reached (for
> whatever reason) (the assumption is that there are inevitable lapses in
> attention), and lambda is introduced to scale the model down again,
> giving, gamma+(1-gamma-lambda)/...) which means the output of the logistic
> function (1/(1+exp(beta(theta+x)))) is squashed between gamma and lambda.
> Unfortunately, if you would try to estimate one value for each gamma,
> lambda, and beta (or 1/sigma) for a single participant then the model is
> simply unidentifiable because predicting a participants average behavior
> (or deviation from something else) of - say 70% - can be achieved by
> gamma=.3 (and lambda=0), or lambda=.3 (and gamma=0) while the logistic
> function is 0 for theta... ; OR theta = -.847 (and gamma =0; lambda0) --
> you see where this is going, right? I agree that it might be reasonable to
> assume that participants "guess" sometimes, but this is not a matter of
> estimation but a matter of your task. In a binary task gamma= .5 (lowest
> probability of being correct); in a task with three responses gamma=1/3. Measurement
> not required, just statistics.
> ?Yes, this makes sense. But isn't this for one trial, not for the entire
> condition. Isn't that why Treutwein and Strasburger use priors to
> approximate this vs just .5 for instance?
> And the lambda parameter, finally, is not necessary, because on the
> individual level it is (almost) redundant with beta (or 1/sigma) - coming
> back to my initial argument. On the average it might sometimes "look like"
> you can draw a horizontal line at p=.8 to which the logistic function (on
> average) approaches. And one could argue this justifies assuming a maximum
> of lambda=.8. However, simply assuming hierarchical variation  in beta (or
> 1/sigma) either within a participants across trials and/or tasks (or
> variation of beta (or 1/sigma) within a task across participants), on
> average, will never predict p=1 without lambda being required, and thus
> provides a "natural" performance cap, measured in terms of variation, not
> in terms of lambda.
> Okay, I'll take your word for it. But could you point me somewhere where I
> could read more about this?
> Having both, again is not identifiable (in addition to the issues above).
> Also, -if- "guessing" would vary between participants, then, I would argue,
> one should think about the amount of trials (or which trials) in which they
> guess, not about the percent being correct while guessing (which is defined
> by the task at hand).
> Well...again, there is nothing inherently difficult with regard to the
> tasks. Given a large enough response window, one should be able to achieve
> 100% accuracy.
>
> 6. Finally, that all being said, I would suggest you use this model:
>
> thresholds <- bf(
>   norms ~ 0 +ability + task,
>   ability ~ 0+(1|subjectID),
> nl = TRUE)
> If you take out the "response" as the DV, you no longer have a binomial
> model or a psychometric function. Again, you're trying to figure out the RW
> at which participants achieve p=70% accuracy.
>
> ## time taken to reach 75% accuracy is predicted (i.e. "norms") by the
> participants 'constant' ability, while including variations over tasks
> (depending on the task).
>  # task estimates task difficulty - should be a factor coding all 14 tasks
> (you still can compare them directly afterwards)
>  # ability is a "linear" predictor, freely estimated, one for each
> participant
> # without intercepts (i.e., 0 in front of the formulas), the task will be
> interpretable as task-specific intercepts (like grand thetas) and the
> abilities centered around 0. If you "scale" norms beforehand (i.e., across
> tasks, not within) to SD=1, then the prior for "ability" should be
> Gaussian(0,1) as well. Voila, very simply measurement model :). You could
> include more terms like time-point to control/test for training effects.
>
> afterwards you can get the task and participant posterior estimates for
> ability (I think) like this:
> posterior_samples(modeloutput)
> with different indices for the participants in the matrix. You then also
> can directly compare single task-estimates with each other (and get Bayes
> factors to check whether their difficulties differ, using a "slab-only"
> approach, instead of "spike-and-slab", check the recent work of Rouder),
>
> I can not see right now, why this should be any more complicated :) , as
> it provides you with the information you want: "How much ability the
> participant has" based on reaching the tailored testing performance of 75%
> accuracy with a specific amount of time pressure, while controlling for
> task difficulty. This also should lower the computational requirements :)
>
>
> Otherwise, if you can provide a paper which estimated:
> item difficulty (i.e., trial-wise), based on time pressure...
> task difficulty (the 14 ones)
> participant ability (unknown)
> based on binary responses
> in a tailored testing design
>
> then please let me know. Sounds interesting in any case.
>
> At least this is what I would say 'spontaneously' :))
>
> Hope this helps,
> Best Ren?
>
>
> ------------------------------
> *From:* Ren? <bimonosom at gmail.com>
> *Sent:* Tuesday, March 17, 2020 1:48 AM
> *To:* Ades, James <jades at health.ucsd.edu>
> *Cc:* r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> *Subject:* Re: [R-sig-ME] Hierarchical Psychometric Function in BRMS
>
> Hey James,
>
> thank you for these details. Step by step:
>
> "1) Yes, essentially. So there are 7 tasks, some have two conditions. One
> has four conditions. This is the "condition" in the model. "Norm" is the
> normalized response window."
> R1) I am sorry, I do not understand this. Does "condition" indicate the 14
> tasks (i.e., with 14 factor levels) or the "some have two, some have four
> conditions part?" If it is the latter, then why did you not include 7
> "tasks" alternatively ? - Anyway - I actually would suggest using the 14
> tasks as "condition", because the design matrix is not fully crossed.
> (i.e., without any design, just all tasks; you still can perform post-hoc
> comparisons).
>
> 2) The term response window is not self-explaining..., but I assume you
> mean "time pressure" by this (how long do I have to give a response). And I
> will go on to refer to this as such.
> 2b) Given "norm" is "time" then I can finally see where you want to go.
> (Please correct me if I am wrong:
>
> 3. No offense, my choice of words was a bit clumsy. I mean a
> clarification about the research question or psychological hypothesis about
> which measure should predict another measure is always helpful to make
> judgments about a models appropriateness. As noted: I get a grip now, and
> it seems, you want to predict decision accuracy ("response") based on the
> task ("condition") and the time provided to solve the task ("norm"). While
> "norm" is a time window to complete the task, dynamically changing
> depending on the accuracy (tailored testing). Now having spelled this out
> reveals a circular causation in it: accuracy -> time window -> accuracy? It
> would be good to search for a reference paper which used an equivalent
> design (not just psychometric function). But to put it this way: Accuracy
> (response) is not really informative, because the tasks (if they are
> tailored) are -specifically designed- to that each participant has about
> 75% accuracy. That is, everybody will either pass a threshold (e.g., 70%)
> or not (e.g., 80%), because everybody will be at 75%.  What IS informative
> is how much time they need for achieving this. The underlying assumption is
> that there is a level of "processing speed" which is just before I become
> perfectly accurate, and the goal is to find this moment, because if I WOULD
> (otherwise) be perfectly accurate in every task my ability is
> unidentifiable (because the tasks were not difficult enough, or
> statistically speaking: no variance), - but if I was only guessing then any
> model about me is uninformative (guessing model).
>
> 3b. In other words, if you are searching for a latent ability that you
> want to continuously describe in your sample, "response window" (time
> needed) is the indicator. slow participants = low ability ; quick
> participants = high ability.
> In Item-Response-Theory you usually estimate the ability, while presenting
> the same tasks to all participants (fully crossed) which allows to estimate
> task difficulty (instead of manipulating it), and I would suggest searching
> for related model solutions in this area. (I am not experienced in tailored
> testing).
>
> 4. If you standardize the measurements within each of the four sessions,
> then I would say there is no reason to further include the term in the
> model. This, however, is a matter of theoretical rather than statistical
> debate. One theoretical counter-argument could be: If you do not
> standardize the measures, but simply include time-points as fixed effects
> in the model, then you gain information (i.e., about the time effect),
> without altering the content of your model (although you change a fixed
> assumption - to a freely estimable one). You then could also take into
> account, that some participants improve more quickly then others, which
> would be a reasonable thing to do, if you think, that this is a thing.
>
> 5. What Treutwein and Strasburger write is, first, mainly about logistic
> functions which have the most basic form of a one - parameter Rasch model.
> Make a two-parameter Rasch model out of it, then you have the functional
> form of standard logistic regression, as also performed in "lmer" and
> "brms" if you write something like:
> DV~Interceptvariable*Continuousvariable+(1|subjectID) + (1|trialID),
> family=binomial(link=logit). with two differences 1) the R packages use a
> different parameterization (e.g. dummy coding) 2) in Rasch models (or Item
> Response Theory) you estimate the model terms based on items and
> individuals, rather than predicting the DV based on conditions and
> measurements (here is a paper that investigates the relation between
> logistic models to predict accuracy and item response theory: Dixon, 2008,
> Models of accuracy in repeated-measures designs). This should help getting
> a "feeling" for the logistic function.
>
> Then what Treutwein and Strasburger introduce can also be found in every
> text-book namely gamma, which is a guessing parameter (gamma +
> 1/(1+exp(...))) which says the model can not predict 0 accuracy unless
> gamma = 0, because something will always be`correct' by chance. Secondly,
> however, adding gamma would lead the model to predictions larger than 1,
> for why there is (1-gamma) involved. Third, the model assumes that 100%
> accuracy might not be reached (for whatever reason), and lambda is
> introduced to scale the model down again, giving,
> gamma+(1-gamma-lambda)/...) which means the output of the logistic function
> (1/(1+exp(beta(theta+x)))) is squashed between gamma and lambda.
> Unfortunately, if you would try to estimate one value for each gamma,
> lambda, and beta (or 1/sigma) for a single participant then the model is
> simply unidentifiable because predicting a participants average behavior
> (or deviation from something else) of - say 70% - can be achieved by
> gamma=.3 (and lambda=0), or lambda=.3 (and gamma=0) while the logistic
> function is 0 for theta... ; OR theta = -.847 (and gamma =0; lambda0) --
> you see where this is going, right? I agree that it might be reasonable to
> assume that participants "guess" sometimes, but this is not a matter of
> estimation but a matter of your task. In a binary task gamma= .5 (lowest
> probability of being correct); in a task with three responses gamma=1/3.
> Measurement not required, just statistics. And the lambda parameter,
> finally, is not necessary, because on the individual level it is (almost)
> redundant with beta (or 1/sigma) - coming back to my initial argument. On
> the average it might sometimes "look like" you can draw a horizontal line
> at p=.8 to which the logistic function (on average) approaches. And one
> could argue this justifies assuming a maximum of lambda=.8. However, simply
> assuming hierarchical variation  in beta (or 1/sigma) either within a
> participants across trials and/or tasks (or variation of beta (or
> 1/sigma) within a task across participants), on average, will never predict
> p=1 without lambda being required, and thus provides a "natural"
> performance cap, measured in terms of variation, not in terms of lambda.
> Having both, again is not identifiable (in addition to the issues above).
> Also, -if- "guessing" would vary between participants, then, I would argue,
> one should think about the amount of trials (or which trials) in which they
> guess, not about the percent being correct while guessing (which is defined
> by the task at hand).
>
> 6. Finally, that all being said, I would suggest you use this model:
>
> thresholds <- bf(
>   norms ~ 0 +ability + task,
>   ability ~ 0+(1|subjectID),
> nl = TRUE)
>
> ## time taken to reach 75% accuracy is predicted (i.e. "norms") by the
> participants 'constant' ability, while including variations over tasks
> (depending on the task).
>  # task estimates task difficulty - should be a factor coding all 14 tasks
> (you still can compare them directly afterwards)
>  # ability is a "linear" predictor, freely estimated, one for each
> participant
> # without intercepts (i.e., 0 in front of the formulas), the task will be
> interpretable as task-specific intercepts (like grand thetas) and the
> abilities centered around 0. If you "scale" norms beforehand (i.e., across
> tasks, not within) to SD=1, then the prior for "ability" should be
> Gaussian(0,1) as well. Voila, very simply measurement model :). You could
> include more terms like time-point to control/test for training effects.
>
> afterwards you can get the task and participant posterior estimates for
> ability (I think) like this:
> posterior_samples(modeloutput)
> with different indices for the participants in the matrix. You then also
> can directly compare single task-estimates with each other (and get Bayes
> factors to check whether their difficulties differ, using a "slab-only"
> approach, instead of "spike-and-slab", check the recent work of Rouder),
>
> I can not see right now, why this should be any more complicated :) , as
> it provides you with the information you want: "How much ability the
> participant has" based on reaching the tailored testing performance of 75%
> accuracy with a specific amount of time pressure, while controlling for
> task difficulty. This also should lower the computational requirements :)
>
>
> Otherwise, if you can provide a paper which estimated:
> item difficulty (i.e., trial-wise), based on time pressure...
> task difficulty (the 14 ones)
> participant ability (unknown)
> based on binary responses
> in a tailored testing design
>
> then please let me know. Sounds interesting in any case.
>
> At least this is what I would say 'spontaneously' :))
>
> Hope this helps,
> Best Ren?
>
>
> Am Mo., 16. M?rz 2020 um 22:47 Uhr schrieb Ades, James <
> jades at health.ucsd.edu>:
>
> Just a quick follow-up; there are actually three other tasks but their
> adaptivity component isn't response window. One of them uses angle rotation
> of the target as the measure of difficulty (a precision WM task). The other
> two tasks are straight forward spatial span and backward span tasks, which
> are just object counts.
> ------------------------------
> *From:* Ades, James <jades at health.ucsd.edu>
> *Sent:* Monday, March 16, 2020 2:44 PM
> *To:* Ren? <bimonosom at gmail.com>
> *Cc:* r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> *Subject:* Re: [R-sig-ME] Hierarchical Psychometric Function in BRMS
>
> Hi Ree,
>
> Thanks for the response.
>
> Responding to your questions:
> 1) Yes, essentially. So there are 7 tasks, some have two conditions. One
> has four conditions. This is the "condition" in the model. "Norm" is the
> normalized response window.
>
> 2) Yes, the response window for the following trials depends on whether
> the previous response is correct and was answered within the response
> window.
>
> 3) I'm not sure what you mean by "unmotivated," but hopefully I can
> provide some background that will give you a better idea. I'm hesitant
> about giving too much information for the sake of avoiding confusion, but
> the threshold was created to be 80%, but when I looked at proportion
> correct for participants many did not achieve this, so it seemed principled
> to extract thresholds at 70%. Ideally, the this performance threshold
> motivates performance (not too easy, but also not too hard). From there, we
> ask the question, what is the necessary RW for the participant to achieve
> 70% accuracy. This question is answered through the psychometric function.
> (In the Treutwein and Strasburger cited paper, they make the point that the
> psychometric function is best approximated using all four priors for
> threshold, spread, lapse, and guessing.
>
> 4) Yes, four sessions, completed over two years, equally spaced, more or
> less. I control for this in the model looking at executive function
> performance on standardized assessment outcome. I wasn't sure whether
> including timepoints within the psychometric function model would lead to
> more accurate estimation of participant psychometric functions.
>
> Hopefully, that information helps.
>
> Regarding your final point on convergence: as I'm sure you know, fitting
> this model with this data is no small feat. Using UCSD's super computer, it
> takes a little over a day. It did seem to converge though. You then write "(But
> dropping lambda and gamma, might be worth considering in any case. If you
> simulate logistic functions hierarchically, then they do not approximate
> 100% on average (which would be the reason you use gamma and lambda), but
> the limited growth approximates e.g., 80 % depending on the individual
> variations in the slope parameters of the logistic function. This means,
> you don't need "maximum performance" parameters, but can approximate this
> behavior by the assumption of hierarchically clustered variance. Which also
> makes the model simpler... , and identifiable, and you could use the
> "elegant" way of determining 70%)." So this is where I am mathematically
> over my head. Re Treut and Straus--they're claim is that the most
> principled approach to approximating the psychometric function of an
> adaptive paradigm is using prior on all four parameters. Is your argument
> that if you're using a hierarchical approach, you wouldn't need the
> gamma/lambda parameters? Can you say more about this or point me to an
> article that discusses the assumption of hierarchically clustered variance?
>
> Thank you for the parameter extraction methods. I guess we'll figure out
> which one when we come to that road. Elegant is always nice. But I think
> the first think is making sure that I have the most principled and correct
> model. Is the one I currently have in BRMS correct given the clarifications
> above?
>
> Much thanks!
>
> James
>
> ------------------------------
> *From:* Ren? <bimonosom at gmail.com>
> *Sent:* Monday, March 16, 2020 2:10 AM
> *To:* Ades, James <jades at health.ucsd.edu>
> *Cc:* r-sig-mixed-models at r-project.org <r-sig-mixed-models at r-project.org>
> *Subject:* Re: [R-sig-ME] Hierarchical Psychometric Function in BRMS
>
> Hi James,
>
> since I am working with brms and glmer, I feel I should be able to give a
> response (although addressing Paul in the Stan-Forum might be
> a better option), there seem to be two questions, and some missing details,
> that might lead to even more questions.... let's begin....
>
> My questions:
> 1. "14 executive functions". Does this mean every participant completed
> each of 14 tasks supposed to measure different facets of the general
> construct "executive functions in working memory"? (If not, please
> clarify). What term is this in the model "condition" or "norm"? (Given that
> you have random slopes for "norm" it seems to be "norm" ?) Then what is
> condition?
>
> 2. "adaptive tasks with 25 to 40 trials" Does this mean "tailored
> testing"? (I.e., the trial that comes next within the task depends on the
> decisions (their error) from all previous trials?)
>
> 3. "Goal: disentangle the response window at which participants reach a
> 70%", - if you have tailored testing (I am not sure), which already is
> designed to sort trials to meander around 75% accuracy for maximum
> information/variance , this threshold seems a bit unmotivated, can you give
> more background?
>
> 4. "four different time points" , I suppose these are four sessions, in
> each the participants have completed subsets of the 14 tasks
>
> Your (secondary) questions (I ignore points 1 to 3 now, but they need
> clarification):
> "I'm not sure whether the four timepoints can be fit at once because
> probability distributions for random factor of participant are already used
> to account for repeated measures of participant completing 14 conditions)."
> My answer:
> - Regardless of the technical details:  First, "time points"  has only
> four levels, thus, it would not make sense to separate their "random"
> intercepts from other variance sources in the design, no matter which.
> Computing standard deviations of a distribution for which you only have 4
> observations/levels is problematic. Second, nonetheless assuming that "time
> points" (e.g., increasing ability over time) has an effect, then
> controlling for it is pretty legit, so, it makes sense to include "time
> points" into the fixed effects. Also legit.
>
> 5. "The other problem I'm having is using coef() or fixef()/ranef() to
> withdraw (or locate) the overall intercept and slope such that I can use
> the qlogis() function to determine the psychometric threshold at 70% (since
> I don't think it would be accurate to directly pull the 70% threshold
> estimate from the parameter itself?)."
> My answer:
> - Do you mean, by 70% threshold, the "location" on the predictor(s) (the
> logit) at which the predicted probably of the response is 70%? (Please keep
> in mind, that you have two interacting predictors in your model, which
> means getting these estimates for one predictor requires to either ignore
> variance of the other predictor, which needs theoretical clarification if
> you want to interpret this; or taking it into account - see below.) Anyway,
> the "manual" way to do this, is to make predictions, based on the
> coefficients, and then search the point of crossing 70%. For this you want
> to use the "emmeans" package which works for both glmer and brms (but I am
> not sure whether it works also for the non-linear models; if not, you need
> to ask Paul Buerkner in the Stan forum how to do it ;)); it sure works with
> standard hierarchical regression output from brms.) . In the emmeans
> package you find the function "emmip", which is what you desire.
>
> #assuming this is your model with a continuous predictor ("continuous")
> and a factorial predictor ("factor"):
> model<-glmer(response ~  continuous * factor + (continuous | pid))
> emmip(model,~continuous,at = list(continuous = c(1,2,3,4,5,6),
> type="response",CIs=TRUE, engine="ggplot" )
> # this gives you the probability predictions for "continuous" from 1 to 6
> (you can make these as "fine" as you want), while ignoring "factor"
> # if you want it "by factor" (taking the interaction into account) you can
> write:
> emmip(model,~continuous|factor ,at = list(continuous = c(1,2,3,4,5,6),
> type="response",CIs=TRUE, engine="ggplot" )
> #All you have to do is search for the point crossing 70% then :) .
>
> However, as noted, non-linear brms models might not directly translate to
> the emmeans architecture (I don't know), and there is a more elegant
> solution anyway:
>
> 1. A standard logistic function predicts 50% when the logit becomes 0
> (before applying the exponential ratio rule; I ignore the fact that your
> gamma and lambda model terms absolutely destroy this property... :))
> 2. The "intercept" shifts the whole logit statically (or by factorial
> conditions), such that it indicates "where" 50% is predicted (in a given
> condition). For example, in standard models
> 1/(1+exp(intercept+varyingeffects)) the intercept says for which value of
> varyingeffects  the term becomes 0).
> 3. You can "make the intercept" to indicate a 70% prediction instead of a
> 50% prediction, if you add a constant on the logit level; that is:
> 1/(1+exp(-.8477)) = (about) 70%; and
>  1/(1+exp(-.8477+intercept+varyingeffects)) shifts the intercept by this
> constant, such that it now indicates the value of varyingeffects which
> predicts 70%. I guess. .. :)) There could be more detail to that (which I
> don't see right now), but it sure is a starting point.
>
> Hope this helps, with your actual questions.
> The rest seems to be a different matter.... (e.g., taking dependencies of
> tailored testing into account etc).
>
> But one final note: I have once tried to fit simpler models with
> constructing the logit myself, like you do, and then setting,  family =
> bernoulli(link = "identity"), which never worked (it never converged). ...
> Just saying: I think Paul makes some points about the identifiability of
> those models in his vignettes, which you should check, if your model fails
> converging.
> (But dropping lambda and gamma, might be worth considering in any case. If
> you simulate logistic functions hierarchically, then they do not
> approximate 100% on average (which would be the reason you use gamma and
> lambda), but the limited growth approximates e.g., 80 % depending on the
> individual variations in the slope parameters of the logistic function.
> This means, you don't need "maximum performance" parameters, but can
> approximate this behavior by the assumption of hierarchically clustered
> variance. Which also makes the model simpler... , and identifiable, and you
> could use the "elegant" way of determining 70%).
>
>
> Best, Ree
>
>
>
> Am Mo., 16. M?rz 2020 um 04:28 Uhr schrieb Ades, James <
> jades at health.ucsd.edu>:
>
> Hi all,
>
> Given that this is a mixed-model listserv, I'm hoping that a BRMS question
> might fit within that purview.
>
> A quick synopsis of the dataset: there are 14 different conditions of
> executive function tasks ( ~1000 3rd, 5th, 7th graders). Given that these
> tasks use an adaptive paradigm (tasks might have anywhere from 25 to 40
> trials), I'm trying to disentangle the response window at which
> participants reach a 70% performance threshold. There are four separate
> timepoints. (I'm not sure whether the four timepoints can be fit at once
> because probability distributions for random factor of participant are
> already used to account for repeated measures of participant completing 14
> conditions, but that question is secondary to ensuring that I'm fitting one
> time point correctly and adequately extracting those the intercept/slope
> parameters).
>
> If I were to only input this into glmer without the priors, I'd write the
> model as:
> ```
> glmer(response ~  condition * norm + (norm | pid/condition)
> ```
> (In a glmer model, I can extract intercept/slope parameters fine).
>
> My current model is below. My question isn't so much with the psychometric
> function or the priors, which, besides the threshold, I've borrowed from
> Treutwein and Strasburger:
> https://link.springer.com/article/10.3758/BF03211951--though if there are
> contentions with any of the those, feel free to raise them--as it is
> whether I've correctly structured the non-linear parameters. The reason for
> modeling all four parameters is to minimize bias, but threshold is the only
> estimate that I'm concerned with. So regarding the multi-level structure,
> I've created parameters for lapse, guess, spread, and threshold. It seems
> reasonable to expect that threshold and spread will vary for every
> participant for every condition, while lapse and guessing (forced yes/no)
> will likely not differ much from condition to condition within participant
> (though if there are arguments that it would make for an improved model,
> I'm fine including lapse and guess parameters for every condition as well).
>
> The other problem I'm having is using coef() or fixef()/ranef() to
> withdraw (or locate) the overall intercept and slope such that I can use
> the qlogis() function to determine the psychometric threshold at 70% (since
> I don't think it would be accurate to directly pull the 70% threshold
> estimate from the parameter itself?).
>
> Does all of that make sense? This is all a little bit over my head and
> though I've culled Buerkner's item-response vignettes (Here:
> https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html
> and here: https://arxiv.org/pdf/1905.09501.pdf, they're similar but
> fundamentally different, so they only get me so far).
>
> I've included a small sample of ~five participants here:
> https://drive.google.com/file/d/1YFnQRSjnp5hVziQx5wQzaIhn75KigaGx/view?usp=sharing
>
> Thanks in advance for any and all help! Hope everyone is staying healthy!
>
> James
>
>
> ```
> thresholds <- bf(
>   response ~ (gamma + (1 - lambda - gamma) * Phi((norm -
> threshold)/spread)),
>   threshold ~ 1 + (1|p|pid) + (1|c|condition),
>   logitgamma  ~ 1 + (1|p|pid),
>   nlf(gamma ~ inv_logit(logitgamma)),
>   logitlambda ~ 1 + (1|p|pid),
>   nlf(lambda ~ inv_logit(logitlambda)),
>   spread ~ 1 + (1|p|pid) + (1|c|condition),
> nl = TRUE)
>
> prior <-
>   prior(beta(9, 3), class = "b", nlpar = "threshold", lb = 0, ub = 1) +
>   prior(beta(1.4, 1.4), class = "b", nlpar = "spread", lb = .005, ub = .5)
> +
>   prior(beta(.5, 8), nlpar = "logitlambda", lb = 0, ub = .1)+
>   prior(beta(1, 5), nlpar = "logitgamma", lb = 0, ub = .1)
>
> fit_thresholds <- brm(
>   formula = thresholds,
>   data = ace.threshold.t1.samp,
>   family = bernoulli(link = "identity"),
>   prior = prior,
>   control = list(adapt_delta = .85, max_treedepth = 15),
>   inits = 0,
>   chains = 1,
>   cores = 16
> )
> ```
>
>
>
> [
> https://media.springernature.com/w110/springer-static/cover/journal/13414.jpg
> ]<https://link.springer.com/article/10.3758/BF03211951>
> Fitting the psychometric function | SpringerLink<
> https://link.springer.com/article/10.3758/BF03211951>
> A constrained generalized maximum likelihood routine for fitting
> psychometric functions is proposed, which determines optimum values for the
> complete parameter set?that is, threshold and slopeas well as for guessing
> and lapsing probability. The constraints are realized by Bayesian prior
> distributions for each of these parameters. The fit itself results from
> maximizing the posterior ...
> link.springer.com
>
> Abstract R arXiv:1905.09501v2 [stat.CO] 20 Jul 2019<
> https://arxiv.org/pdf/1905.09501.pdf>
> Paul-Christian B urkner 3 dictions via a nested non-linear formula syntax,
> the implementation of several distributions designed for response times
> data, and extentions of distributions for ordinal data, for example
> arxiv.org
>
> Estimating Non-Linear Models with brms<
> https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html
> >
> Introduction. This vignette provides an introduction on how to fit
> non-linear multilevel models with brms.Non-linear models are incredibly
> flexible and powerful, but require much more care with respect to model
> specification and priors than typical generalized linear models.
> cran.r-project.org
>
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>

	[[alternative HTML version deleted]]


From m|ch@e|_rom@nov @end|ng |rom |nbox@ru  Thu Mar 19 14:14:35 2020
From: m|ch@e|_rom@nov @end|ng |rom |nbox@ru (=?UTF-8?B?TWljaGFlbCBSb21hbm92?=)
Date: Thu, 19 Mar 2020 16:14:35 +0300
Subject: [R-sig-ME] =?utf-8?q?Working_formula_for_glmm_model_in_R_=28bern?=
 =?utf-8?q?oulli_response=29?=
Message-ID: <1584623675.683551355@f417.i.mail.ru>


Dear all,
?
Sorry for the silly question, but I got stuck on it.
?
My dataset is occupancy of eagle territories in different years (see the example below). I?m trying to test my data for time trend, taking into account territory quality (as a random effect).
?
territory year occupied
CHV-1 2006 0
CHV-12 2006 0
CHV-120 2006 1
CHV-13 2009 0
CHV-14 2009 1
CHV-15 2010 1
CHV-16 2010 1
?
My thoughts are following. ?year? is a fixed effect variable and ?territory? is a random effect variable. For example, in lme4 package the formula could be following: occupied = year + (1|territory). However, lme4 doesn?t support Bernoulli family, so I chose glmm package. According to its specifications, the glmm function accepts two different formulae, separately for fixed and random effects:
?
glmm(fixed, random, varcomps.names, data, family.glmm, m, varcomps.equal, doPQL = TRUE,debug=FALSE, p1=1/3,p2=1/3, p3=1/3, rmax=1000,iterlim=1000, par.init, zeta=5, cluster=NULL)
?
So now I need two different formulae. I tried ?occupied ~ year?, ?occupied ~ territory?, but it doesn?t work. Namely, the RStudio gets into an infinite loop and doesn?t produce any output.
?
Can anyone help me to write a working formula (or formulae) to properly run the glmm model?
?
Thanks,
?
Michael
?
?
?
?
?
?
?
?
	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Fri Mar 20 00:33:44 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Fri, 20 Mar 2020 12:33:44 +1300
Subject: [R-sig-ME] 
 [FORGED] Working formula for glmm model in R (bernoulli response)
In-Reply-To: <1584623675.683551355@f417.i.mail.ru>
References: <1584623675.683551355@f417.i.mail.ru>
Message-ID: <90f1081e-8778-ce09-ddfc-62ac72aa6570@auckland.ac.nz>


On 20/03/20 2:14 am, Michael Romanov via R-sig-mixed-models wrote:

> Dear all,
>   
> Sorry for the silly question, but I got stuck on it.

The only silly question is the one that is not asked.
>   
> My dataset is occupancy of eagle territories in different years (see
> the example below). I?m trying to test my data for time trend, taking
> into account territory quality (as a random effect).
>   
> territory year occupied
> CHV-1 2006 0
> CHV-12 2006 0
> CHV-120 2006 1
> CHV-13 2009 0
> CHV-14 2009 1
> CHV-15 2010 1
> CHV-16 2010 1
>   
> My thoughts are following. ?year? is a fixed effect variable and
> ?territory? is a random effect variable. For example, in lme4 package
> the formula could be following: occupied = year + (1|territory).
> However, lme4 doesn?t support Bernoulli family,

This is simply NOT TRUE!!!  The Bernoulli family is just a special case 
of the binomial family. See below.

OTOH, the glmer() function from lme4 seems reluctant to handle the 
simple model that you appear to have in mind.  (Again, see below.)

This, in my experience, is a recurring problem with lme4 --- it cannot 
deal with the simple models that constitute edge cases.

(It is also quite possible that I've got the syntax wrong.  Perhaps a 
younger and wiser head will chime in.)

> so I chose glmm package. According to its specifications, the glmm
> function accepts two different formulae, separately for fixed and random
> effects: >
> glmm(fixed, random, varcomps.names, data, family.glmm, m,
> varcomps.equal, doPQL = TRUE,debug=FALSE, p1=1/3,p2=1/3, p3=1/3,
> rmax=1000,iterlim=1000, par.init, zeta=5, cluster=NULL)
> 
> So now I need two different formulae. I tried ?occupied ~ year?,
> ?occupied ~ territory?, but it doesn?t work. Namely, the RStudio gets
> into an infinite loop and doesn?t produce any output.

Please DO NOT confuse RStudio with R.  RStudio is simply an interface 
that allows those mentally handicapped people who can only use GUIs to 
access R.  :-)

>   
> Can anyone help me to write a working formula (or formulae) to properly
> run the glmm model?

I have no experience with glmm and don't really understand what you are 
trying to model.  Moreover you did not supply a reproducible example, as 
you are instructed to do by the Posting Guide.  This makes it very 
difficult to help you.  However I persisted, and constructed a simulated 
toy example data set, and fitted a model to it:

library(glmm)
set.seed(42)
X <- data.frame(territory=factor(paste0("CHV-",
                                  sample(6:10,200,TRUE))),
                 year=sample(2001:2020,200,TRUE),
                 occupied=sample(0:1,200,TRUE))
fit1 <- glmm(fixed=occupied~year,random=occupied~territory,
             family=bernoulli.glmm,varcomps.names="territory",
             data=X,m=1e3)

No infinite loops seem to have arisen.  Note that coef(fit1) gives:

> (Intercept)        year 
> 23.17057405 -0.01164075 

The "variance" component estimate is:
>            Estimate ... 
> territory 1.533e-05 ...

Now back to lme4:

X$unoccupied <- 1-X$occupied
library(lme4)
fit2 <- glmer(cbind(occupied,unoccupied) ~ year + (1|territory),
               family=binomial(),data=X)

This seems not to work properly; it says:

> boundary (singular) fit: see ?isSingular

However the fixed effects in fit2 are effectively the same as those in fit1:
> Fixed Effects:
> (Intercept)         year  
>    23.18216     -0.01164  

The variance component is said to be 0.

I then tried the glmmTMB() package:

library(glmmTMB)
fit3 <- glmmTMB(cbind(occupied,unoccupied) ~ year + (1|territory),
               family=binomial(),data=X)

This gave a mild squawk about a "Model convergence problem" but produced 
an answer with essentially the same fixed effect estimates as the other 
two methods:

>             Estimate ...
> (Intercept) 23.18216 ...
> year        -0.01164

The variance component estimate was vanishingly small: 1.062e-91.

Bottom line:  I think I'd go with the glmmTMB package for your problem,
but glmm seems to work, if you want to stick with that.

HTH

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r@turner @end|ng |rom @uck|@nd@@c@nz  Fri Mar 20 01:17:11 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Fri, 20 Mar 2020 13:17:11 +1300
Subject: [R-sig-ME] [FORGED] Re:  Error from glmmTMB() --- no joy.
In-Reply-To: <c93574ab-c0e1-87b9-8211-8f2607765802@gmail.com>
References: <2ef51b3e-694a-1c9e-7e9f-c30fb946de3f@auckland.ac.nz>
 <3f34c5f4894d4d2a937691006feafb7d@erasmusmc.nl>
 <cbd751e1-64d8-07b2-6372-b1bd9237534a@auckland.ac.nz>
 <c93574ab-c0e1-87b9-8211-8f2607765802@gmail.com>
Message-ID: <879573c2-c43a-e63a-2109-f101240489a3@auckland.ac.nz>


On 19/03/20 3:52 pm, Ben Bolker wrote:

<SNIP>

> I made a small change that appears to allow the
> original model to run correctly.  If you want to try it out, and have
> development tools (compilers etc.) and the remotes package installed,
> 
> remotes::install_github("glmmTMB/glmmTMB/glmmTMB at cloglog_fix")
> 
>    should install the patched version (I think).
> 
> This will presumably be merged with the master branch sometime soon but
> I want to add some tests etc.

I've finally got around to trying out your patch.  No problems with the
install_github() process, but after doing the install I executed:

> library(glmmTMB)
> fmla <- cbind(Dead, Alive) ~ (Trt + 0)/Dose + (Dose | Rep)
> X    <- dget("X.txt")
> fit  <- glmmTMB(fmla, data = X, family = betabinomial(link = "cloglog"),
>                dispformula = ~1
and got:

> Error in (function (start, objective, gradient = NULL, hessian = NULL,  : 
>   NA/NaN gradient evaluation
> In addition: Warning messages:
> 1: In (function (start, objective, gradient = NULL, hessian = NULL,  :
>   NA/NaN function evaluation
> 2: In (function (start, objective, gradient = NULL, hessian = NULL,  :
>   NA/NaN function evaluation
> 3: In (function (start, objective, gradient = NULL, hessian = NULL,  :
>   NA/NaN function evaluation
> 4: In (function (start, objective, gradient = NULL, hessian = NULL,  :
>   NA/NaN function evaluation
> 5: In (function (start, objective, gradient = NULL, hessian = NULL,  :
>   NA/NaN function evaluation
> Timing stopped at: 0.572 0 0.572

So my Sadim touch persists in its effect.

cheers,

Rolf

P. S. With the logit link I got warnings

> 1: In (function (start, objective, gradient = NULL, hessian = NULL,  :
>   NA/NaN function evaluation
> 2: In fitTMB(TMBStruc) :
>   Model convergence problem; non-positive-definite Hessian matrix. See vignette('troubleshooting')

but no errors as such.

R.

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From bbo|ker @end|ng |rom gm@||@com  Fri Mar 20 01:40:33 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Thu, 19 Mar 2020 20:40:33 -0400
Subject: [R-sig-ME] [FORGED] Re:  Error from glmmTMB() --- no joy.
In-Reply-To: <879573c2-c43a-e63a-2109-f101240489a3@auckland.ac.nz>
References: <2ef51b3e-694a-1c9e-7e9f-c30fb946de3f@auckland.ac.nz>
 <3f34c5f4894d4d2a937691006feafb7d@erasmusmc.nl>
 <cbd751e1-64d8-07b2-6372-b1bd9237534a@auckland.ac.nz>
 <c93574ab-c0e1-87b9-8211-8f2607765802@gmail.com>
 <879573c2-c43a-e63a-2109-f101240489a3@auckland.ac.nz>
Message-ID: <65c894ae-4895-977b-7a61-e541d11ce47b@gmail.com>


 Yeah, I realized today that I was wrong about this fixing the problem.
I'm not stuck yet/want to work on this, but I will pursue it in the
context of a glmmTMB issue.

  (I like everyone else am a little busier than usual these days ...)

 Ben

On 2020-03-19 8:17 p.m., Rolf Turner wrote:
> 
> On 19/03/20 3:52 pm, Ben Bolker wrote:
> 
> <SNIP>
> 
>> I made a small change that appears to allow the
>> original model to run correctly.? If you want to try it out, and have
>> development tools (compilers etc.) and the remotes package installed,
>>
>> remotes::install_github("glmmTMB/glmmTMB/glmmTMB at cloglog_fix")
>>
>> ?? should install the patched version (I think).
>>
>> This will presumably be merged with the master branch sometime soon but
>> I want to add some tests etc.
> 
> I've finally got around to trying out your patch.? No problems with the
> install_github() process, but after doing the install I executed:
> 
>> library(glmmTMB)
>> fmla <- cbind(Dead, Alive) ~ (Trt + 0)/Dose + (Dose | Rep)
>> X??? <- dget("X.txt")
>> fit? <- glmmTMB(fmla, data = X, family = betabinomial(link = "cloglog"),
>> ?????????????? dispformula = ~1
> and got:
> 
>> Error in (function (start, objective, gradient = NULL, hessian =
>> NULL,? : ? NA/NaN gradient evaluation
>> In addition: Warning messages:
>> 1: In (function (start, objective, gradient = NULL, hessian = NULL,? :
>> ? NA/NaN function evaluation
>> 2: In (function (start, objective, gradient = NULL, hessian = NULL,? :
>> ? NA/NaN function evaluation
>> 3: In (function (start, objective, gradient = NULL, hessian = NULL,? :
>> ? NA/NaN function evaluation
>> 4: In (function (start, objective, gradient = NULL, hessian = NULL,? :
>> ? NA/NaN function evaluation
>> 5: In (function (start, objective, gradient = NULL, hessian = NULL,? :
>> ? NA/NaN function evaluation
>> Timing stopped at: 0.572 0 0.572
> 
> So my Sadim touch persists in its effect.
> 
> cheers,
> 
> Rolf
> 
> P. S. With the logit link I got warnings
> 
>> 1: In (function (start, objective, gradient = NULL, hessian = NULL,? :
>> ? NA/NaN function evaluation
>> 2: In fitTMB(TMBStruc) :
>> ? Model convergence problem; non-positive-definite Hessian matrix. See
>> vignette('troubleshooting')
> 
> but no errors as such.
> 
> R.
>


From r@turner @end|ng |rom @uck|@nd@@c@nz  Fri Mar 20 03:42:02 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Fri, 20 Mar 2020 15:42:02 +1300
Subject: [R-sig-ME] [FORGED] Re:  Error from glmmTMB() --- no joy.
In-Reply-To: <65c894ae-4895-977b-7a61-e541d11ce47b@gmail.com>
References: <2ef51b3e-694a-1c9e-7e9f-c30fb946de3f@auckland.ac.nz>
 <3f34c5f4894d4d2a937691006feafb7d@erasmusmc.nl>
 <cbd751e1-64d8-07b2-6372-b1bd9237534a@auckland.ac.nz>
 <c93574ab-c0e1-87b9-8211-8f2607765802@gmail.com>
 <879573c2-c43a-e63a-2109-f101240489a3@auckland.ac.nz>
 <65c894ae-4895-977b-7a61-e541d11ce47b@gmail.com>
Message-ID: <e9fc5151-0a1e-f64c-ff02-3c727657f4aa@auckland.ac.nz>


On 20/03/20 1:40 pm, Ben Bolker wrote:

>   Yeah, I realized today that I was wrong about this fixing the problem.
> I'm not stuck yet/want to work on this, but I will pursue it in the
> context of a glmmTMB issue.

Sorry; don't understand what you mean by that last phrase.  Isn't this a 
glmmTMB issue?

>    (I like everyone else am a little busier than usual these days ...)

Indeed.  Completely understood.  Good luck.

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From bbo|ker @end|ng |rom gm@||@com  Fri Mar 20 03:55:40 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Thu, 19 Mar 2020 22:55:40 -0400
Subject: [R-sig-ME] [FORGED] Re:  Error from glmmTMB() --- no joy.
In-Reply-To: <e9fc5151-0a1e-f64c-ff02-3c727657f4aa@auckland.ac.nz>
References: <2ef51b3e-694a-1c9e-7e9f-c30fb946de3f@auckland.ac.nz>
 <3f34c5f4894d4d2a937691006feafb7d@erasmusmc.nl>
 <cbd751e1-64d8-07b2-6372-b1bd9237534a@auckland.ac.nz>
 <c93574ab-c0e1-87b9-8211-8f2607765802@gmail.com>
 <879573c2-c43a-e63a-2109-f101240489a3@auckland.ac.nz>
 <65c894ae-4895-977b-7a61-e541d11ce47b@gmail.com>
 <e9fc5151-0a1e-f64c-ff02-3c727657f4aa@auckland.ac.nz>
Message-ID: <CABghstQ=ZpZCLO_gioLCoEBMLRtiRQ98_=iTQjwmSfkd_PrFwQ@mail.gmail.com>

 An "issue" in the technical sense:
https://github.com/glmmTMB/glmmTMB/issues/564

  cheers
  Ben Bolker

On Thu, Mar 19, 2020 at 10:42 PM Rolf Turner <r.turner at auckland.ac.nz> wrote:
>
>
> On 20/03/20 1:40 pm, Ben Bolker wrote:
>
> >   Yeah, I realized today that I was wrong about this fixing the problem.
> > I'm not stuck yet/want to work on this, but I will pursue it in the
> > context of a glmmTMB issue.
>
> Sorry; don't understand what you mean by that last phrase.  Isn't this a
> glmmTMB issue?
>
> >    (I like everyone else am a little busier than usual these days ...)
>
> Indeed.  Completely understood.  Good luck.
>
> cheers,
>
> Rolf
>
> --
> Honorary Research Fellow
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276


From r@turner @end|ng |rom @uck|@nd@@c@nz  Fri Mar 20 22:39:29 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sat, 21 Mar 2020 10:39:29 +1300
Subject: [R-sig-ME] 
 [FORGED] Working formula for glmm model in R (bernoulli response)
In-Reply-To: <1584712200.691713641@f273.i.mail.ru>
References: <1584623675.683551355@f417.i.mail.ru>
 <90f1081e-8778-ce09-ddfc-62ac72aa6570@auckland.ac.nz>
 <1584712200.691713641@f273.i.mail.ru>
Message-ID: <091a627e-7a23-2469-63b6-373c8f1aacfb@auckland.ac.nz>


On 21/03/20 2:50 am, Michael Romanov wrote:

> Thank you very much, Rolf!
> Your advise helped much!
> lme4 works well, although is produces some warninngs:
> Warning messages:
> 1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,? :
>  ? Model failed to converge with max|grad| = 0.527726 (tol = 0.001, 
> component 1)
> 2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,? :
>  ? Model is nearly unidentifiable: very large eigenvalue
>  ?- Rescale variables?;Model is nearly unidentifiable: large eigenvalue 
> ratio
>  ?- Rescale variables?
> Nevertheless, it says that there?s a significant time trend in occupancy:
> Fixed effects:
>  ????????????? Estimate Std. Error z value Pr(>|z|)
> (Intercept) 94.7762510? 1.0176729?? 93.13?? <2e-16 ***
> year??????? -0.0466590? 0.0005135? -90.86?? <2e-16 ***
> ======================================
> glmm also works well, though quite slow. What I thought to be an 
> infinite loop, turned to be very long loop. It is running about an hour, 
> then produces some output. It also warns about lack of convergence.
> In glmmTMB package the model doesn?t converge at all, so it is useless 
> for me since I?m interested more in p-values of time trend than in 
> slope. I do not see how it is possible to rescale since they are binary.

You should be re-scaling your numeric predictor ("year"), *not* the 
response!!!

> I?m still working on it. Probably there will be more questions if you 
> don?t mind.

I don't really mind, but I probably won't be able to help you very much. 
  I'm not an expert in this area; I'm a user rather than a developer. 
You'd be better off seeking help on r-sig-mixed-models, to which I have 
taken the liberty of CC-ing this email.  You should keep posts on this 
topic on-list.  There are people on the list who have far more insight, 
knowledge and expertise.  They would be much more likely to be able to 
help you.

You will probably have to provide your data --- or a sample thereof; or 
perhaps a simulated version thereof (if the data are confidential; IMHO 
data should *never* be confidential, but that's another story) --- if 
you really want to get serious help.

cheers,

Rolf

> Best regards,
> Michael
> 
>     ???????, 20 ????? 2020, 10:33 +11:00 ?? Rolf Turner
>     <r.turner at auckland.ac.nz>:
> 
>     On 20/03/20 2:14 am, Michael Romanov via R-sig-mixed-models wrote:
> 
>      > Dear all,
>      >
>      > Sorry for the silly question, but I got stuck on it.
> 
>     The only silly question is the one that is not asked.
>      >
>      > My dataset is occupancy of eagle territories in different years (see
>      > the example below). I?m trying to test my data for time trend, taking
>      > into account territory quality (as a random effect).
>      >
>      > territory year occupied
>      > CHV-1 2006 0
>      > CHV-12 2006 0
>      > CHV-120 2006 1
>      > CHV-13 2009 0
>      > CHV-14 2009 1
>      > CHV-15 2010 1
>      > CHV-16 2010 1
>      >
>      > My thoughts are following. ?year? is a fixed effect variable and
>      > ?territory? is a random effect variable. For example, in lme4 package
>      > the formula could be following: occupied = year + (1|territory).
>      > However, lme4 doesn?t support Bernoulli family,
> 
>     This is simply NOT TRUE!!! The Bernoulli family is just a special case
>     of the binomial family. See below.
> 
>     OTOH, the glmer() function from lme4 seems reluctant to handle the
>     simple model that you appear to have in mind. (Again, see below.)
> 
>     This, in my experience, is a recurring problem with lme4 --- it cannot
>     deal with the simple models that constitute edge cases.
> 
>     (It is also quite possible that I've got the syntax wrong. Perhaps a
>     younger and wiser head will chime in.)
> 
>      > so I chose glmm package. According to its specifications, the glmm
>      > function accepts two different formulae, separately for fixed and
>     random
>      > effects: >
>      > glmm(fixed, random, varcomps.names, data, family.glmm, m,
>      > varcomps.equal, doPQL = TRUE,debug=FALSE, p1=1/3,p2=1/3, p3=1/3,
>      > rmax=1000,iterlim=1000, par.init, zeta=5, cluster=NULL)
>      >
>      > So now I need two different formulae. I tried ?occupied ~ year?,
>      > ?occupied ~ territory?, but it doesn?t work. Namely, the RStudio gets
>      > into an infinite loop and doesn?t produce any output.
> 
>     Please DO NOT confuse RStudio with R. RStudio is simply an interface
>     that allows those mentally handicapped people who can only use GUIs to
>     access R. :-)
> 
>      >
>      > Can anyone help me to write a working formula (or formulae) to
>     properly
>      > run the glmm model?
> 
>     I have no experience with glmm and don't really understand what you are
>     trying to model. Moreover you did not supply a reproducible example, as
>     you are instructed to do by the Posting Guide. This makes it very
>     difficult to help you. However I persisted, and constructed a simulated
>     toy example data set, and fitted a model to it:
> 
>     library(glmm)
>     set.seed(42)
>     X <- data.frame(territory=factor(paste0("CHV-",
>      ??????????????????????????????????sample(6:10,200,TRUE))),
>      ?????????????????year=sample(2001:2020,200,TRUE),
>      ?????????????????occupied=sample(0:1,200,TRUE))
>     fit1 <- glmm(fixed=occupied~year,random=occupied~territory,
>      ?????????????family=bernoulli.glmm,varcomps.names="territory",
>      ?????????????data=X,m=1e3)
> 
>     No infinite loops seem to have arisen. Note that coef(fit1) gives:
> 
>      > (Intercept) year
>      > 23.17057405 -0.01164075
> 
>     The "variance" component estimate is:
>      > Estimate ...
>      > territory 1.533e-05 ...
> 
>     Now back to lme4:
> 
>     X$unoccupied <- 1-X$occupied
>     library(lme4)
>     fit2 <- glmer(cbind(occupied,unoccupied) ~ year + (1|territory),
>      ???????????????family=binomial(),data=X)
> 
>     This seems not to work properly; it says:
> 
>      > boundary (singular) fit: see ?isSingular
> 
>     However the fixed effects in fit2 are effectively the same as those
>     in fit1:
>      > Fixed Effects:
>      > (Intercept) year
>      > 23.18216 -0.01164
> 
>     The variance component is said to be 0.
> 
>     I then tried the glmmTMB() package:
> 
>     library(glmmTMB)
>     fit3 <- glmmTMB(cbind(occupied,unoccupied) ~ year + (1|territory),
>      ???????????????family=binomial(),data=X)
> 
>     This gave a mild squawk about a "Model convergence problem" but produced
>     an answer with essentially the same fixed effect estimates as the other
>     two methods:
> 
>      > Estimate ...
>      > (Intercept) 23.18216 ...
>      > year -0.01164
> 
>     The variance component estimate was vanishingly small: 1.062e-91.
> 
>     Bottom line: I think I'd go with the glmmTMB package for your problem,
>     but glmm seems to work, if you want to stick with that.
> 
>     HTH


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sun Mar 22 09:01:34 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 22 Mar 2020 21:01:34 +1300
Subject: [R-sig-ME] predict.glmmTMB when "cloglog" link is used.
Message-ID: <5306b5eb-758c-d23e-9d26-742d275d3056@auckland.ac.nz>


Please consider the follow examples:

library(glmmTMB)
X <- dget("demoDat.txt")
fit1 <- glmmTMB(cbind(Dead,Alive) ~ (0+Trt)/Dose +
                 (Dose|Rep),family=binomial(link="logit"),data=X)
fit2 <- glmmTMB(cbind(Dead,Alive) ~ (0+Trt)/Dose +
                 (Dose|Rep),family=binomial(link="cloglog"),data=X)
p1 <- predict(fit1,type="response")
p2 <- predict(fit2,type="response")

The vector p1 appears to have "reasonable" entries i.e. they look (as 
they should) like probabilities:

>> p1 >   [1] 0.019609546 0.194977679 0.745729561 0.972612901 0.994688457 
0.998988370
>   [7] 0.999807998 0.999963583 0.022220571 0.120654723 0.833383226 0.967947980
>  [13] 0.994545468 0.999092456 0.999975082 0.106684473 0.261453062 0.512048780
> ....

The entries of p2 do not seem "reasonable".  In particular they all less 
than or equal to zero:

>> p2
>   [1]  -3.244681e+00  -1.597702e+00  -3.235952e-01  -6.683303e-04  -7.662901e-11
>   [6]  -5.930060e-33 -2.134861e-103   0.000000e+00  -3.360384e+00  -2.201879e+00
>  [11]  -3.222809e-01  -1.400287e-02  -6.947338e-07  -3.769347e-21 -2.254755e-225
> ....

Am I misunderstanding something, or is there a bug in predict.glmmTMB? 
The data set in question is attached as "demoDat.txt".

Thanks for any enlightenment.

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: demoDat.txt
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20200322/29abe03f/attachment.txt>

From r@turner @end|ng |rom @uck|@nd@@c@nz  Sun Mar 22 22:31:53 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Mon, 23 Mar 2020 10:31:53 +1300
Subject: [R-sig-ME] predict.glmmTMB when "cloglog" link is used.
In-Reply-To: <5306b5eb-758c-d23e-9d26-742d275d3056@auckland.ac.nz>
References: <5306b5eb-758c-d23e-9d26-742d275d3056@auckland.ac.nz>
Message-ID: <9fad3fca-bf37-ce1f-35f2-938b54c1234d@auckland.ac.nz>


Further remarks:

In an off-list message it was suggested to me that perhaps the value of 
p2 should be exponentiated.  Indeed p1 and exp(p2) are "reasonably 
similar".  I really don't understand how the necessity for 
exponentiation could arise, however.

When the "probit" link is used to fit the model, predict.glmmTMB() seems 
to work just fine.

I have stared a bit at the code for predict.glmmTMB() but the subtleties 
are too great for me to be able to figure out what is going on/wrong.

cheers,

Rolf Turner

On 22/03/20 9:01 pm, Rolf Turner wrote:
> 
> Please consider the follow examples:
> 
> library(glmmTMB)
> X <- dget("demoDat.txt")
> fit1 <- glmmTMB(cbind(Dead,Alive) ~ (0+Trt)/Dose +
>  ??????????????? (Dose|Rep),family=binomial(link="logit"),data=X)
> fit2 <- glmmTMB(cbind(Dead,Alive) ~ (0+Trt)/Dose +
>  ??????????????? (Dose|Rep),family=binomial(link="cloglog"),data=X)
> p1 <- predict(fit1,type="response")
> p2 <- predict(fit2,type="response")
> 
> The vector p1 appears to have "reasonable" entries i.e. they look (as 
> they should) like probabilities:
> 
>>> p1 >?? [1] 0.019609546 0.194977679 0.745729561 0.972612901 0.994688457 
> 0.998988370
>> ? [7] 0.999807998 0.999963583 0.022220571 0.120654723 0.833383226 
>> 0.967947980
>> ?[13] 0.994545468 0.999092456 0.999975082 0.106684473 0.261453062 
>> 0.512048780
>> ....
> 
> The entries of p2 do not seem "reasonable".? In particular they all less 
> than or equal to zero:
> 
>>> p2
>> ? [1]? -3.244681e+00? -1.597702e+00? -3.235952e-01? -6.683303e-04  
>> -7.662901e-11
>> ? [6]? -5.930060e-33 -2.134861e-103?? 0.000000e+00? -3.360384e+00  
>> -2.201879e+00
>> ?[11]? -3.222809e-01? -1.400287e-02? -6.947338e-07? -3.769347e-21 
>> -2.254755e-225
>> ....
> 
> Am I misunderstanding something, or is there a bug in predict.glmmTMB? 
> The data set in question is attached as "demoDat.txt".
> 
> Thanks for any enlightenment.


From mo|||eebrook@ @end|ng |rom gm@||@com  Mon Mar 23 11:51:15 2020
From: mo|||eebrook@ @end|ng |rom gm@||@com (Mollie Brooks)
Date: Mon, 23 Mar 2020 11:51:15 +0100
Subject: [R-sig-ME] predict.glmmTMB when "cloglog" link is used.
In-Reply-To: <9fad3fca-bf37-ce1f-35f2-938b54c1234d@auckland.ac.nz>
References: <5306b5eb-758c-d23e-9d26-742d275d3056@auckland.ac.nz>
 <9fad3fca-bf37-ce1f-35f2-938b54c1234d@auckland.ac.nz>
Message-ID: <6D894960-F1E3-498B-B47E-26A2CF9CAEB8@gmail.com>

This sounds like a bug, but I can?t repeat the behavior. I get reasonable values in p2 and they look highly correlated to p1. Are you using the latest version from CRAN, 1.0.1? It should be essentially the same as the master branch on GitHub (only one vignette is behind). I can?t remember any recent changes (to either the CRAN or GitHub versions) that would make a difference, but updating is worth a try.

cheers,
Mollie

> On 22Mar 2020, at 22:31, Rolf Turner <r.turner at auckland.ac.nz> wrote:
> 
> 
> Further remarks:
> 
> In an off-list message it was suggested to me that perhaps the value of p2 should be exponentiated.  Indeed p1 and exp(p2) are "reasonably similar".  I really don't understand how the necessity for exponentiation could arise, however.
> 
> When the "probit" link is used to fit the model, predict.glmmTMB() seems to work just fine.
> 
> I have stared a bit at the code for predict.glmmTMB() but the subtleties are too great for me to be able to figure out what is going on/wrong.
> 
> cheers,
> 
> Rolf Turner
> 
> On 22/03/20 9:01 pm, Rolf Turner wrote:
>> Please consider the follow examples:
>> library(glmmTMB)
>> X <- dget("demoDat.txt")
>> fit1 <- glmmTMB(cbind(Dead,Alive) ~ (0+Trt)/Dose +
>>                 (Dose|Rep),family=binomial(link="logit"),data=X)
>> fit2 <- glmmTMB(cbind(Dead,Alive) ~ (0+Trt)/Dose +
>>                 (Dose|Rep),family=binomial(link="cloglog"),data=X)
>> p1 <- predict(fit1,type="response")
>> p2 <- predict(fit2,type="response")
>> The vector p1 appears to have "reasonable" entries i.e. they look (as they should) like probabilities:
>>>> p1 >   [1] 0.019609546 0.194977679 0.745729561 0.972612901 0.994688457 
>> 0.998988370
>>>   [7] 0.999807998 0.999963583 0.022220571 0.120654723 0.833383226 0.967947980
>>>  [13] 0.994545468 0.999092456 0.999975082 0.106684473 0.261453062 0.512048780
>>> ....
>> The entries of p2 do not seem "reasonable".  In particular they all less than or equal to zero:
>>>> p2
>>>   [1]  -3.244681e+00  -1.597702e+00  -3.235952e-01  -6.683303e-04  -7.662901e-11
>>>   [6]  -5.930060e-33 -2.134861e-103   0.000000e+00  -3.360384e+00  -2.201879e+00
>>>  [11]  -3.222809e-01  -1.400287e-02  -6.947338e-07  -3.769347e-21 -2.254755e-225
>>> ....
>> Am I misunderstanding something, or is there a bug in predict.glmmTMB? The data set in question is attached as "demoDat.txt".
>> Thanks for any enlightenment.
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From @n@h|@r@|ern@ndez @end|ng |rom gm@||@com  Mon Mar 23 15:41:33 2020
From: @n@h|@r@|ern@ndez @end|ng |rom gm@||@com (=?UTF-8?B?QW5haMOtIEZlcm7DoW5kZXo=?=)
Date: Mon, 23 Mar 2020 11:41:33 -0300
Subject: [R-sig-ME] warning error question
Message-ID: <CAFr3rgov9vvzmVDNrLDM+_FYjb6SZKrkfPhskmLQk1P2PhgOzw@mail.gmail.com>

hi!! I run this model in lme4:
"M.4=glm(Cuenta~carga*categ.asoc+(1|campo/foto)
          +offset(log(area.foto)),family=poisson(link =
"log"),data=tipocat)"
And I have this warning message: "Warning message:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.00432818 (tol = 0.001,
component 1)"
I don?t know what is that means, could you help me, please!!
My datatable is attached...

Cheers, Anah?

-- 
Anah? R. Fernandez
Lab. Ecotono-INIBIOMA
CONICET- Univ. Nac. del Comahue
Quintral 1250 (8400), Bariloche, Argentina

From r@turner @end|ng |rom @uck|@nd@@c@nz  Mon Mar 23 22:58:57 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Tue, 24 Mar 2020 10:58:57 +1300
Subject: [R-sig-ME] predict.glmmTMB when "cloglog" link is used.
In-Reply-To: <6D894960-F1E3-498B-B47E-26A2CF9CAEB8@gmail.com>
References: <5306b5eb-758c-d23e-9d26-742d275d3056@auckland.ac.nz>
 <9fad3fca-bf37-ce1f-35f2-938b54c1234d@auckland.ac.nz>
 <6D894960-F1E3-498B-B47E-26A2CF9CAEB8@gmail.com>
Message-ID: <0711c6bb-5899-9aa9-81b5-5fd01e2dff07@auckland.ac.nz>


On 23/03/20 11:51 pm, Mollie Brooks wrote:

> This sounds like a bug, but I can?t repeat the behavior. I get
> reasonable values in p2 and they look highly correlated to p1. Are
> you using the latest version from CRAN, 1.0.1? It should be
> essentially the same as the master branch on GitHub (only one
> vignette is behind). I can?t remember any recent changes (to either
> the CRAN or GitHub versions) that would make a difference, but
> updating is worth a try.
> cheers,
> Mollie

Turned out I was using version 1.0.0; I upgraded to 1.0.1 and the 
problem went away.

The key to the mystery may be that I was actually using the *patched* 
version that Ben Bolker had put up on github, which I installed via

> remotes::install_github("glmmTMB/glmmTMB/glmmTMB at cloglog_fix") 

Ben was trying to alleviate a problem that I'd encountered in trying to 
use glmmTMB() to fit models that involved the betabinomial family and 
the "cloglog" link.  Perhaps he buggered something up in attempting the 
fix (which turned out not to work anyway!).

Anyhow, predict.glmmTMB() now seems to work, at least.

Thanks.

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r@turner @end|ng |rom @uck|@nd@@c@nz  Mon Mar 23 23:33:28 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Tue, 24 Mar 2020 11:33:28 +1300
Subject: [R-sig-ME] [FORGED]  warning error question
In-Reply-To: <CAFr3rgov9vvzmVDNrLDM+_FYjb6SZKrkfPhskmLQk1P2PhgOzw@mail.gmail.com>
References: <CAFr3rgov9vvzmVDNrLDM+_FYjb6SZKrkfPhskmLQk1P2PhgOzw@mail.gmail.com>
Message-ID: <0e69f527-be8f-e1a0-8e9f-6cd4eeef8b20@auckland.ac.nz>


On 24/03/20 3:41 am, Anah? Fern?ndez wrote:

> hi!! I run this model in lme4:
> "M.4=glm(Cuenta~carga*categ.asoc+(1|campo/foto)
>            +offset(log(area.foto)),family=poisson(link =
> "log"),data=tipocat)"
> And I have this warning message: "Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
>    Model failed to converge with max|grad| = 0.00432818 (tol = 0.001,
> component 1)"
> I don?t know what is that means, could you help me, please!!
> My datatable is attached...
> 
> Cheers, Anah?

(a) Since the function you invoke is glm() this would appear to be 
off-topic for r-sig-mixed-models.  OTOH your formula does indeed seem to 
involve random effects.  Did you *really* call glm()?  Or did you 
actually call glmer()?  If so you, you should be ashamed of yourself for 
such sloppiness in posing your question.  People are providing help out 
of the goodness of their hearts; don't impose on their good nature by 
expecting them to be telepathic.

(b) Assuming that you really did call glmer() --- my impression is that 
such warnings are usually false positives and may usually (???) be 
safely (???) ignored.  However I'm no expert; you should perhaps wait 
for confirmation of this from the more knowledgeable.

(c) Your "datatable" was *NOT* attached.  Most attachments get stripped 
by the system (for security reasons).  There are exceptions.  *READ* the 
posting guide, which you appear not to have done.

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From bbo|ker @end|ng |rom gm@||@com  Tue Mar 24 01:50:50 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 23 Mar 2020 20:50:50 -0400
Subject: [R-sig-ME] [FORGED] warning error question
In-Reply-To: <0e69f527-be8f-e1a0-8e9f-6cd4eeef8b20@auckland.ac.nz>
References: <CAFr3rgov9vvzmVDNrLDM+_FYjb6SZKrkfPhskmLQk1P2PhgOzw@mail.gmail.com>
 <0e69f527-be8f-e1a0-8e9f-6cd4eeef8b20@auckland.ac.nz>
Message-ID: <12e405f9-0223-267b-0e4c-7e8669465f58@gmail.com>



On 2020-03-23 6:33 p.m., Rolf Turner wrote:
> 
> On 24/03/20 3:41 am, Anah? Fern?ndez wrote:
> 
>> hi!! I run this model in lme4:
>> "M.4=glm(Cuenta~carga*categ.asoc+(1|campo/foto)
>> ?????????? +offset(log(area.foto)),family=poisson(link =
>> "log"),data=tipocat)"
>> And I have this warning message: "Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,? :
>> ?? Model failed to converge with max|grad| = 0.00432818 (tol = 0.001,
>> component 1)"
>> I don?t know what is that means, could you help me, please!!
>> My datatable is attached...
>>
>> Cheers, Anah?
> 
> (a) Since the function you invoke is glm() this would appear to be
> off-topic for r-sig-mixed-models.? OTOH your formula does indeed seem to
> involve random effects.? Did you *really* call glm()?? Or did you
> actually call glmer()?? If so you, you should be ashamed of yourself for
> such sloppiness in posing your question.? People are providing help out
> of the goodness of their hearts; don't impose on their good nature by
> expecting them to be telepathic.

  Rolf, can you tone it down slightly? I agree that the OP could be more
careful, but "you should be ashamed of yourself" seems way too strong.

> (b) Assuming that you really did call glmer() --- my impression is that
> such warnings are usually false positives and may usually (???) be
> safely (???) ignored.? However I'm no expert; you should perhaps wait
> for confirmation of this from the more knowledgeable.
> 
> (c) Your "datatable" was *NOT* attached.? Most attachments get stripped
> by the system (for security reasons).? There are exceptions.? *READ* the
> posting guide, which you appear not to have done.

  I did get the data from a previous interchange (Anah?, can you post
the data set somewhere publicly accessible?  CSV is strongly preferred
to XLSX ...).

  The bottom line here is that your baseline category has only a single
'Cuenta' value in it and only two unique 'carga' values, leading to
extreme estimates - this is essentially the analogue of 'complete
separation' in the logistic regression, and has the same solutions
(regularize somehow if you want sensible answers).

  cheers
   Ben Bolker


            Cuenta
categ.asoc     1   2   3   4   5   6   7   8   9
  highly      10   0   0   0   0   0   0   0   0
  isolated    78  20   8   4   1   0   1   0   0
  moderately  58   0   1   0   0   0   0   0   0
  poorly     120  47  24  16  12   9   0   1   1

round(coef(summary(M.4)),3)
                           Estimate Std. Error z value Pr(>|z|)
(Intercept)                  -2.852      1.264  -2.257    0.024
carga                         8.029     13.626   0.589    0.556
categ.asocisolated            1.776      1.255   1.415    0.157
categ.asocmoderately          1.299      1.256   1.034    0.301
categ.asocpoorly              1.929      1.251   1.542    0.123
carga:categ.asocisolated     -9.548     13.612  -0.701    0.483
carga:categ.asocmoderately   -9.418     13.614  -0.692    0.489
carga:categ.asocpoorly       -9.006     13.615  -0.661    0.508

> 
> cheers,
> 
> Rolf Turner
>


From john@m@|ndon@|d @end|ng |rom @nu@edu@@u  Tue Mar 24 02:18:17 2020
From: john@m@|ndon@|d @end|ng |rom @nu@edu@@u (John Maindonald)
Date: Tue, 24 Mar 2020 01:18:17 +0000
Subject: [R-sig-ME] [FORGED] warning error question
In-Reply-To: <12e405f9-0223-267b-0e4c-7e8669465f58@gmail.com>
References: <CAFr3rgov9vvzmVDNrLDM+_FYjb6SZKrkfPhskmLQk1P2PhgOzw@mail.gmail.com>
 <0e69f527-be8f-e1a0-8e9f-6cd4eeef8b20@auckland.ac.nz>
 <12e405f9-0223-267b-0e4c-7e8669465f58@gmail.com>
Message-ID: <C47B5105-08CD-4B2A-98A1-9B889D86D502@anu.edu.au>

What may be most useful is advice to, wherever possible
(i.e., unless one has an equipment setup that does not allow it),
copy the code as executed from the command line or script into
the email message.  Newies may take time to learn to make
such actions routine.

John Maindonald             email: john.maindonald at anu.edu.au<mailto:john.maindonald at anu.edu.au>


On 24/03/2020, at 13:50, Ben Bolker <bbolker at gmail.com<mailto:bbolker at gmail.com>> wrote:



On 2020-03-23 6:33 p.m., Rolf Turner wrote:

On 24/03/20 3:41 am, Anah? Fern?ndez wrote:

hi!! I run this model in lme4:
"M.4=glm(Cuenta~carga*categ.asoc+(1|campo/foto)
           +offset(log(area.foto)),family=poisson(link =
"log"),data=tipocat)"
And I have this warning message: "Warning message:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
   Model failed to converge with max|grad| = 0.00432818 (tol = 0.001,
component 1)"
I don?t know what is that means, could you help me, please!!
My datatable is attached...

Cheers, Anah?

(a) Since the function you invoke is glm() this would appear to be
off-topic for r-sig-mixed-models.  OTOH your formula does indeed seem to
involve random effects.  Did you *really* call glm()?  Or did you
actually call glmer()?  If so you, you should be ashamed of yourself for
such sloppiness in posing your question.  People are providing help out
of the goodness of their hearts; don't impose on their good nature by
expecting them to be telepathic.

 Rolf, can you tone it down slightly? I agree that the OP could be more
careful, but "you should be ashamed of yourself" seems way too strong.

(b) Assuming that you really did call glmer() --- my impression is that
such warnings are usually false positives and may usually (???) be
safely (???) ignored.  However I'm no expert; you should perhaps wait
for confirmation of this from the more knowledgeable.

(c) Your "datatable" was *NOT* attached.  Most attachments get stripped
by the system (for security reasons).  There are exceptions.  *READ* the
posting guide, which you appear not to have done.

 I did get the data from a previous interchange (Anah?, can you post
the data set somewhere publicly accessible?  CSV is strongly preferred
to XLSX ...).

 The bottom line here is that your baseline category has only a single
'Cuenta' value in it and only two unique 'carga' values, leading to
extreme estimates - this is essentially the analogue of 'complete
separation' in the logistic regression, and has the same solutions
(regularize somehow if you want sensible answers).

 cheers
  Ben Bolker


           Cuenta
categ.asoc     1   2   3   4   5   6   7   8   9
 highly      10   0   0   0   0   0   0   0   0
 isolated    78  20   8   4   1   0   1   0   0
 moderately  58   0   1   0   0   0   0   0   0
 poorly     120  47  24  16  12   9   0   1   1

round(coef(summary(M.4)),3)
                          Estimate Std. Error z value Pr(>|z|)
(Intercept)                  -2.852      1.264  -2.257    0.024
carga                         8.029     13.626   0.589    0.556
categ.asocisolated            1.776      1.255   1.415    0.157
categ.asocmoderately          1.299      1.256   1.034    0.301
categ.asocpoorly              1.929      1.251   1.542    0.123
carga:categ.asocisolated     -9.548     13.612  -0.701    0.483
carga:categ.asocmoderately   -9.418     13.614  -0.692    0.489
carga:categ.asocpoorly       -9.006     13.615  -0.661    0.508


cheers,

Rolf Turner


_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From @@|v@dor@@nchezco|on @end|ng |rom prod|gy@net@mx  Tue Mar 24 02:23:26 2020
From: @@|v@dor@@nchezco|on @end|ng |rom prod|gy@net@mx (Salvador SANCHEZ COLON)
Date: Mon, 23 Mar 2020 18:23:26 -0700
Subject: [R-sig-ME] [FORGED] warning error question
Message-ID: <6JEBTICM6AU4.22L2Y8RI92453@mwweb09oc>

Dear Profr. Bolker,


Thanks so much for your intervention. In my view, what is really shameful is this sort of bullying, unnecesarily rude, offensive responses. If I would feel that it would be a waste of my precious time to respond to a silly, trivial or ill posed question, nobody obliges me to respond...but there is no need either for me to send out a rude, patronizing raprimend. What if the problem stems from our having to cope not only with a complex statistical issue, but also the foreign language barrier?


With all due respect.


Salvador S?NCHEZ-COL?N






En Lun, 23 Marzo, 2020 en 18:51, Ben Bolker <bbolker at gmail.com> escribi?:
?

Para: r-sig-mixed-models at r-project.org


On 2020-03-23 6:33 p.m., Rolf Turner wrote:
> 
> On 24/03/20 3:41 am, Anah? Fern?ndez wrote:
> 
>> hi!! I run this model in lme4:
>> "M.4=glm(Cuenta~carga*categ.asoc+(1|campo/foto)
>> ?????????? +offset(log(area.foto)),family=poisson(link =
>> "log"),data=tipocat)"
>> And I have this warning message: "Warning message:
>> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,? :
>> ?? Model failed to converge with max|grad| = 0.00432818 (tol = 0.001,
>> component 1)"
>> I don?t know what is that means, could you help me, please!!
>> My datatable is attached...
>>
>> Cheers, Anah?
> 
> (a) Since the function you invoke is glm() this would appear to be
> off-topic for r-sig-mixed-models.? OTOH your formula does indeed seem to
> involve random effects.? Did you *really* call glm()?? Or did you
> actually call glmer()?? If so you, you should be ashamed of yourself for
> such sloppiness in posing your question.? People are providing help out
> of the goodness of their hearts; don't impose on their good nature by
> expecting them to be telepathic.

Rolf, can you tone it down slightly? I agree that the OP could be more
careful, but "you should be ashamed of yourself" seems way too strong.

> (b) Assuming that you really did call glmer() --- my impression is that
> such warnings are usually false positives and may usually (???) be
> safely (???) ignored.? However I'm no expert; you should perhaps wait
> for confirmation of this from the more knowledgeable.
> 
> (c) Your "datatable" was *NOT* attached.? Most attachments get stripped
> by the system (for security reasons).? There are exceptions.? *READ* the
> posting guide, which you appear not to have done.

I did get the data from a previous interchange (Anah?, can you post
the data set somewhere publicly accessible? CSV is strongly preferred
to XLSX ...).

The bottom line here is that your baseline category has only a single
'Cuenta' value in it and only two unique 'carga' values, leading to
extreme estimates - this is essentially the analogue of 'complete
separation' in the logistic regression, and has the same solutions
(regularize somehow if you want sensible answers).

cheers
Ben Bolker


Cuenta
categ.asoc 1 2 3 4 5 6 7 8 9
highly 10 0 0 0 0 0 0 0 0
isolated 78 20 8 4 1 0 1 0 0
moderately 58 0 1 0 0 0 0 0 0
poorly 120 47 24 16 12 9 0 1 1

round(coef(summary(M.4)),3)
Estimate Std. Error z value Pr(>|z|)
(Intercept) -2.852 1.264 -2.257 0.024
carga 8.029 13.626 0.589 0.556
categ.asocisolated 1.776 1.255 1.415 0.157
categ.asocmoderately 1.299 1.256 1.034 0.301
categ.asocpoorly 1.929 1.251 1.542 0.123
carga:categ.asocisolated -9.548 13.612 -0.701 0.483
carga:categ.asocmoderately -9.418 13.614 -0.692 0.489
carga:categ.asocpoorly -9.006 13.615 -0.661 0.508

> 
> cheers,
> 
> Rolf Turner
>

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
.
 
	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Tue Mar 24 05:27:03 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Tue, 24 Mar 2020 17:27:03 +1300
Subject: [R-sig-ME] [FORGED] warning error question
In-Reply-To: <6JEBTICM6AU4.22L2Y8RI92453@mwweb09oc>
References: <6JEBTICM6AU4.22L2Y8RI92453@mwweb09oc>
Message-ID: <35577937-9802-b423-b0d1-4b7e696fd588@auckland.ac.nz>


On 24/03/20 2:23 pm, Salvador SANCHEZ COLON wrote:

> Dear Profr. Bolker,
> 
> 
> Thanks so much for your intervention. In my view, what is really
> shameful is this sort of bullying, unnecesarily rude, offensive
> responses. If I would feel that it would be a waste of my precious
> time to respond to a silly, trivial or ill posed question, nobody
> obliges me to respond...but there is no need either for me to send
> out a rude, patronizing raprimend. What if the problem stems from our
> having to cope not only with a complex statistical issue, but also
> the foreign language barrier?
> 
> 
> With all due respect.

With all due respect, I think that any reasonable person would agree 
that *I* am the one getting bullied here.  At least I gave the OP some 
useful advice, which is more than I am getting from you.

I think it is shameful for you to bully me for insisting on a bit of 
clarity and a bit of consideration for those from whom help is requested.

What on earth does a language barrier have to do with writing glm() when 
glmer() is what is needed?  That's the *R* language, which we are all 
supposed to speak on this list!

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From @@|v@dor@@nchezco|on @end|ng |rom prod|gy@net@mx  Tue Mar 24 05:50:53 2020
From: @@|v@dor@@nchezco|on @end|ng |rom prod|gy@net@mx (Salvador SANCHEZ COLON)
Date: Mon, 23 Mar 2020 21:50:53 -0700
Subject: [R-sig-ME] [FORGED] warning error question
Message-ID: <W98PCPXN6AU4.JQ1VRJ54JRLB3@mwweb09oc>

Indeed, we have seen the sort of useful advice that you provide:


"...Please DO NOT confuse RStudio with R. RStudio is simply an interfacethat allows those mentally handicapped people who can only use GUIs to
access R. :-)..."


Thanks for that.


Salvador S?NCHEZ-COL?N


En Lun, 23 Marzo, 2020 en 22:27, Rolf Turner <r.turner at auckland.ac.nz> escribi?:
?

Para: Salvador SANCHEZ COLON
Cc: r-sig-mixed-models at r-project.org
On 24/03/20 2:23 pm, Salvador SANCHEZ COLON wrote:

> Dear Profr. Bolker,
> 
> 
> Thanks so much for your intervention. In my view, what is really
> shameful is this sort of bullying, unnecesarily rude, offensive
> responses. If I would feel that it would be a waste of my precious
> time to respond to a silly, trivial or ill posed question, nobody
> obliges me to respond...but there is no need either for me to send
> out a rude, patronizing raprimend. What if the problem stems from our
> having to cope not only with a complex statistical issue, but also
> the foreign language barrier?
> 
> 
> With all due respect.

With all due respect, I think that any reasonable person would agree 
that *I* am the one getting bullied here. At least I gave the OP some 
useful advice, which is more than I am getting from you.

I think it is shameful for you to bully me for insisting on a bit of 
clarity and a bit of consideration for those from whom help is requested.

What on earth does a language barrier have to do with writing glm() when 
glmer() is what is needed? That's the *R* language, which we are all 
supposed to speak on this list!

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276
.
 
	[[alternative HTML version deleted]]


From d@n @end|ng |rom mcc|oy@|n|o  Mon Mar 23 17:20:53 2020
From: d@n @end|ng |rom mcc|oy@|n|o (Dan McCloy)
Date: Mon, 23 Mar 2020 16:20:53 +0000
Subject: [R-sig-ME] warning error question
In-Reply-To: <CAFr3rgov9vvzmVDNrLDM+_FYjb6SZKrkfPhskmLQk1P2PhgOzw@mail.gmail.com>
References: <CAFr3rgov9vvzmVDNrLDM+_FYjb6SZKrkfPhskmLQk1P2PhgOzw@mail.gmail.com>
Message-ID: <YAU7bo2hwuFZmtKBPpq4FAQbGy1UC8KCFCADhArqQbcd1pDpbiLlJzubYKCAkMHve8A15T6xWGiRpZ-tWR0lj6dY802f4_FGaerlFhZfgtU=@mccloy.info>

A good place to start is the FAQ:
http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#convergence-warnings

Note also that email attachments are stripped by the mailing list server, so if you want to share your data you must either provide a download link, or include the results of `dput()` in your message.


-- dan
Daniel McCloy
https://dan.mccloy.info
Research Scientist
Institute for Learning and Brain Sciences
University of Washington

??????? Original Message ???????
On Monday, March 23, 2020 7:41 AM, Anah? Fern?ndez <anahi.r.fernandez at gmail.com> wrote:

> hi!! I run this model in lme4:
> "M.4=glm(Cuenta~carga*categ.asoc+(1|campo/foto)
> +offset(log(area.foto)),family=poisson(link =
> "log"),data=tipocat)"
> And I have this warning message: "Warning message:
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv, :
> Model failed to converge with max|grad| = 0.00432818 (tol = 0.001,
> component 1)"
> I don?t know what is that means, could you help me, please!!
> My datatable is attached...
>
> Cheers, Anah?
>
> --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
>
> Anah? R. Fernandez
> Lab. Ecotono-INIBIOMA
> CONICET- Univ. Nac. del Comahue
> Quintral 1250 (8400), Bariloche, Argentina
>
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From m@|r@|@toretto @end|ng |rom gm@||@com  Tue Mar 24 18:59:15 2020
From: m@|r@|@toretto @end|ng |rom gm@||@com (Maira Fatoretto)
Date: Tue, 24 Mar 2020 14:59:15 -0300
Subject: [R-sig-ME] REML=TRUE with non-Gaussian responses
Message-ID: <CAB4qdLN=fTDFtu=cfZ55P+wdr2sdoKtF4w7X5rASEwyCPhq4eA@mail.gmail.com>

Hello,

I have a question about glmmTMB, when I using REML=TRUE for binomial
family.

They suggest It may also be useful in some
cases with non-Gaussian responses (Millar 2011). However, I would like to
understand what this exactly does because in Millar (2011) there is not a
clear explication about this.

Thank you.
-- 
*Ma?ra Blumer Fatoretto*
Estat?stica - Universidade Estadual de Campinas- UNICAMP
Mestra em Ci?ncias(Estat?stica e Experimenta??o Agron?mica) - ESALQ/USP -
Piracicaba - SP
Doutoranda em Estat?stica e Experimenta??o Agron?mica
Telefone:(19) 988309481
E-mail: mairafatoretto at gmail.com

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Tue Mar 24 23:01:58 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Wed, 25 Mar 2020 11:01:58 +1300
Subject: [R-sig-ME] [FORGED] Re:  warning error question
In-Reply-To: <YAU7bo2hwuFZmtKBPpq4FAQbGy1UC8KCFCADhArqQbcd1pDpbiLlJzubYKCAkMHve8A15T6xWGiRpZ-tWR0lj6dY802f4_FGaerlFhZfgtU=@mccloy.info>
References: <CAFr3rgov9vvzmVDNrLDM+_FYjb6SZKrkfPhskmLQk1P2PhgOzw@mail.gmail.com>
 <YAU7bo2hwuFZmtKBPpq4FAQbGy1UC8KCFCADhArqQbcd1pDpbiLlJzubYKCAkMHve8A15T6xWGiRpZ-tWR0lj6dY802f4_FGaerlFhZfgtU=@mccloy.info>
Message-ID: <960dee8d-616b-b218-c970-e84709670e51@auckland.ac.nz>



On 24/03/20 5:20 am, Dan McCloy wrote:

> A good place to start is the FAQ:
> http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#convergence-warnings
>
>  Note also that email attachments are stripped by the mailing list
> server, so if you want to share your data you must either provide a
> download link, or include the results of `dput()` in your message.

Please correct me if I am wrong about this, but I believe that plain 
text files, that *have the extension* ".txt" are passed by the server.
As are (less relevantly) pdf files with the extension ".pdf".

But that's about it.

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From bbo|ker @end|ng |rom gm@||@com  Tue Mar 24 23:41:13 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 24 Mar 2020 18:41:13 -0400
Subject: [R-sig-ME] [FORGED] Re: warning error question
In-Reply-To: <960dee8d-616b-b218-c970-e84709670e51@auckland.ac.nz>
References: <CAFr3rgov9vvzmVDNrLDM+_FYjb6SZKrkfPhskmLQk1P2PhgOzw@mail.gmail.com>
 <YAU7bo2hwuFZmtKBPpq4FAQbGy1UC8KCFCADhArqQbcd1pDpbiLlJzubYKCAkMHve8A15T6xWGiRpZ-tWR0lj6dY802f4_FGaerlFhZfgtU=@mccloy.info>
 <960dee8d-616b-b218-c970-e84709670e51@auckland.ac.nz>
Message-ID: <CABghstQRHM+hXWWAq73cvbZne_9y1zzjEC_MLnWoY3Dn606skw@mail.gmail.com>

For the record, the current mailing list pass/filters are as follows.

Remove if MIME type is *not* in    this list:

multipart/mixed, multipart/alternative, text/plain, text/x-diff,
text/x-patch, application/pgp-signatu\
re, application/postscript, application/pdf, application/x-tar,
application/x-compressed-tar, applicat\
ion/x-gzip, image/png, message/rfc822

Then remove if file extension *is* in this list:

exe, bat, cmd, com, pif, scr, vbs, cpl

  cheers
   Ben

On Tue, Mar 24, 2020 at 6:02 PM Rolf Turner <r.turner at auckland.ac.nz> wrote:
>
>
>
> On 24/03/20 5:20 am, Dan McCloy wrote:
>
> > A good place to start is the FAQ:
> > http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#convergence-warnings
> >
> >  Note also that email attachments are stripped by the mailing list
> > server, so if you want to share your data you must either provide a
> > download link, or include the results of `dput()` in your message.
>
> Please correct me if I am wrong about this, but I believe that plain
> text files, that *have the extension* ".txt" are passed by the server.
> As are (less relevantly) pdf files with the extension ".pdf".
>
> But that's about it.
>
> cheers,
>
> Rolf
>
> --
> Honorary Research Fellow
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbo|ker @end|ng |rom gm@||@com  Wed Mar 25 02:02:44 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 24 Mar 2020 21:02:44 -0400
Subject: [R-sig-ME] REML=TRUE with non-Gaussian responses
In-Reply-To: <CAB4qdLN=fTDFtu=cfZ55P+wdr2sdoKtF4w7X5rASEwyCPhq4eA@mail.gmail.com>
References: <CAB4qdLN=fTDFtu=cfZ55P+wdr2sdoKtF4w7X5rASEwyCPhq4eA@mail.gmail.com>
Message-ID: <03d67177-bbf2-7d2e-97af-76beb03f825e@gmail.com>

This is a good question, but we (I) need to know a little bit more about
the level of explanation you're looking for.

In a book chapter of mine (Chapter 13 in Fox et al., Ecological
statistics: contemporary theory and application) I wrote

A broader way of thinking about REML is that it  describes any
statistical method where we integrate over the fixed effects when
estimating the variances.

  That's clear, but very short.

  Or you might prefer:

Bellio and Brazzale Stat Comput (2011) 21: 173?183DOI
10.1007/s11222-009-9157-4

The restricted likelihood function was originally defined as  the
marginal  likelihood  of  a  set  of  residual  contrasts (Patterson
and  Thompson1971).  Alternatively, it may  be computed as an integrated
likelihood following a Bayesian argument (Stiratelli et al.1984), or as
a modified profile likelihood function (Severini2000, Chap. 9).

Millar's book says (in section 9.3):

For normal linear models (including mixed-effects models), parameters ?
(the variance parameters) and ? ? ? (the regression coefficients) are
orthogonal, and the Laplace approximation in (9.18) is exact since the
log-likelihood is quadratic in ?. It follows that REML is equivalent to
integrated likelihood in these models. This equivalence was first
reported by Harville (1974) ...

Harville, D. A. (1974) Bayesian inference for variance components using
only error contrasts, Biometrika 61: 383?385.

  So; can you say a little more about what you mean by "I would like to
understand what this exactly does"?

On 2020-03-24 1:59 p.m., Maira Fatoretto wrote:
> Hello,
> 
> I have a question about glmmTMB, when I using REML=TRUE for binomial
> family.
> 
> They suggest It may also be useful in some
> cases with non-Gaussian responses (Millar 2011). However, I would like to
> understand what this exactly does because in Millar (2011) there is not a
> clear explication about this.
> 
> Thank you.
>


From john@m@|ndon@|d @end|ng |rom @nu@edu@@u  Wed Mar 25 07:29:05 2020
From: john@m@|ndon@|d @end|ng |rom @nu@edu@@u (John Maindonald)
Date: Wed, 25 Mar 2020 06:29:05 +0000
Subject: [R-sig-ME] REML=TRUE with non-Gaussian responses
In-Reply-To: <03d67177-bbf2-7d2e-97af-76beb03f825e@gmail.com>
References: <CAB4qdLN=fTDFtu=cfZ55P+wdr2sdoKtF4w7X5rASEwyCPhq4eA@mail.gmail.com>
 <03d67177-bbf2-7d2e-97af-76beb03f825e@gmail.com>
Message-ID: <5F32C940-8167-434E-AA1F-8DE2FEBD9B03@anu.edu.au>

There are comments that Maira might find helpful at
https://stats.stackexchange.com/questions/48671/what-is-restricted-maximum-likelihood-and-when-should-it-be-used

The usual (unbiased) variance estimate in an lm() type model
is effectively a REML estimate, not a maximum likelihood
estimate.   Maximum likelihood estimates of the residual
variance are highly biased in small samples.  The older (but
still, where it can be used, insightful) analysis of variance
table for suitably balanced designs extended this idea.
Expected values of stratum mean squares are, except for the
Residual stratum, linear combinations of variance components,
(well, I have simplified somewhat) and give equations that can
be successively solved to obtain what are basically the variance
estimates.

See Searle?s article at https://doi.org/10.3168/jds.s0022-0302(91)78599-8

?[This] ANOVA method of estimating variance components [is] ...
  based on equating ex- pected mean squares to their values
computed from data."

REML estimates extend these ideas.


John Maindonald             email: john.maindonald at anu.edu.au<mailto:john.maindonald at anu.edu.au>

On 25/03/2020, at 14:02, Ben Bolker <bbolker at gmail.com<mailto:bbolker at gmail.com>> wrote:

This is a good question, but we (I) need to know a little bit more about
the level of explanation you're looking for.

In a book chapter of mine (Chapter 13 in Fox et al., Ecological
statistics: contemporary theory and application) I wrote

A broader way of thinking about REML is that it  describes any
statistical method where we integrate over the fixed effects when
estimating the variances.

 That's clear, but very short.

 Or you might prefer:

Bellio and Brazzale Stat Comput (2011) 21: 173?183DOI
10.1007/s11222-009-9157-4

The restricted likelihood function was originally defined as  the
marginal  likelihood  of  a  set  of  residual  contrasts (Patterson
and  Thompson1971).  Alternatively, it may  be computed as an integrated
likelihood following a Bayesian argument (Stiratelli et al.1984), or as
a modified profile likelihood function (Severini2000, Chap. 9).

Millar's book says (in section 9.3):

For normal linear models (including mixed-effects models), parameters ?
(the variance parameters) and ? ? ? (the regression coefficients) are
orthogonal, and the Laplace approximation in (9.18) is exact since the
log-likelihood is quadratic in ?. It follows that REML is equivalent to
integrated likelihood in these models. This equivalence was first
reported by Harville (1974) ...

Harville, D. A. (1974) Bayesian inference for variance components using
only error contrasts, Biometrika 61: 383?385.

 So; can you say a little more about what you mean by "I would like to
understand what this exactly does"?

On 2020-03-24 1:59 p.m., Maira Fatoretto wrote:
Hello,

I have a question about glmmTMB, when I using REML=TRUE for binomial
family.

They suggest It may also be useful in some
cases with non-Gaussian responses (Millar 2011). However, I would like to
understand what this exactly does because in Millar (2011) there is not a
clear explication about this.

Thank you.


_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From @n@h|@r@|ern@ndez @end|ng |rom gm@||@com  Tue Mar 24 00:29:00 2020
From: @n@h|@r@|ern@ndez @end|ng |rom gm@||@com (=?UTF-8?B?QW5haMOtIEZlcm7DoW5kZXo=?=)
Date: Mon, 23 Mar 2020 20:29:00 -0300
Subject: [R-sig-ME] [FORGED]  warning error question
In-Reply-To: <0e69f527-be8f-e1a0-8e9f-6cd4eeef8b20@auckland.ac.nz>
References: <CAFr3rgov9vvzmVDNrLDM+_FYjb6SZKrkfPhskmLQk1P2PhgOzw@mail.gmail.com>
 <0e69f527-be8f-e1a0-8e9f-6cd4eeef8b20@auckland.ac.nz>
Message-ID: <CAFr3rgoTX3JBeyNcMBuM2MoyeiLHqvubUOxUoZcPD2o4JU1w0Q@mail.gmail.com>

Hi Rolf and everyone, It was a typing error. The correct model is:

M.4=glmer(Cuenta.de.categ.asoc~carga*categ.asoc+(1|campo/foto)
          +offset(log(area.foto)),family=poisson(link = "log"),data=tipocat)

And the warning message is: "Warning message:
In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.00167856 (tol = 0.001,
component 1)"

Sorry for the inconvenience, obviously it is not my intention to waste
anyone's time. Thanks Rolf for noticing the error even though I would have
expected another way to tell me.

Cheers, Anah?

El lun., 23 mar. 2020 a las 19:33, Rolf Turner (<r.turner at auckland.ac.nz>)
escribi?:

>
> On 24/03/20 3:41 am, Anah? Fern?ndez wrote:
>
> > hi!! I run this model in lme4:
> > "M.4=glm(Cuenta~carga*categ.asoc+(1|campo/foto)
> >            +offset(log(area.foto)),family=poisson(link =
> > "log"),data=tipocat)"
> > And I have this warning message: "Warning message:
> > In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
> >    Model failed to converge with max|grad| = 0.00432818 (tol = 0.001,
> > component 1)"
> > I don?t know what is that means, could you help me, please!!
> > My datatable is attached...
> >
> > Cheers, Anah?
>
> (a) Since the function you invoke is glm() this would appear to be
> off-topic for r-sig-mixed-models.  OTOH your formula does indeed seem to
> involve random effects.  Did you *really* call glm()?  Or did you
> actually call glmer()?  If so you, you should be ashamed of yourself for
> such sloppiness in posing your question.  People are providing help out
> of the goodness of their hearts; don't impose on their good nature by
> expecting them to be telepathic.
>
> (b) Assuming that you really did call glmer() --- my impression is that
> such warnings are usually false positives and may usually (???) be
> safely (???) ignored.  However I'm no expert; you should perhaps wait
> for confirmation of this from the more knowledgeable.
>
> (c) Your "datatable" was *NOT* attached.  Most attachments get stripped
> by the system (for security reasons).  There are exceptions.  *READ* the
> posting guide, which you appear not to have done.
>
> cheers,
>
> Rolf Turner
>
> --
> Honorary Research Fellow
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>


-- 
Anah? R. Fernandez
Lab. Ecotono-INIBIOMA
CONICET- Univ. Nac. del Comahue
Quintral 1250 (8400), Bariloche, Argentina

	[[alternative HTML version deleted]]


From @pro @end|ng |rom un|me|b@edu@@u  Tue Mar 24 05:54:43 2020
From: @pro @end|ng |rom un|me|b@edu@@u (Andrew Robinson)
Date: Tue, 24 Mar 2020 04:54:43 +0000
Subject: [R-sig-ME] [FORGED] warning error question
In-Reply-To: <35577937-9802-b423-b0d1-4b7e696fd588@auckland.ac.nz>
References: <6JEBTICM6AU4.22L2Y8RI92453@mwweb09oc>
 <35577937-9802-b423-b0d1-4b7e696fd588@auckland.ac.nz>
Message-ID: <c3a97eaa-d505-4848-8e92-34828b58698c@Spark>

I don't agree with you Rolf.  As I indicated in our offline conversation, I consider your response to the original question to be inappropriate.

Andrew

--
Andrew Robinson
Director, CEBRA and Professor of Biosecurity,
School/s of BioSciences and Mathematics & Statistics
University of Melbourne, VIC 3010 Australia
Tel: (+61) 0403 138 955
Email: apro at unimelb.edu.au
Website: https://researchers.ms.unimelb.edu.au/~apro at unimelb/

I acknowledge the Traditional Owners of the land I inhabit, and pay my respects to their Elders.
On Mar 24, 2020, 3:27 PM +1100, Rolf Turner <r.turner at auckland.ac.nz>, wrote:

On 24/03/20 2:23 pm, Salvador SANCHEZ COLON wrote:

Dear Profr. Bolker,


Thanks so much for your intervention. In my view, what is really
shameful is this sort of bullying, unnecesarily rude, offensive
responses. If I would feel that it would be a waste of my precious
time to respond to a silly, trivial or ill posed question, nobody
obliges me to respond...but there is no need either for me to send
out a rude, patronizing raprimend. What if the problem stems from our
having to cope not only with a complex statistical issue, but also
the foreign language barrier?


With all due respect.

With all due respect, I think that any reasonable person would agree
that *I* am the one getting bullied here. At least I gave the OP some
useful advice, which is more than I am getting from you.

I think it is shameful for you to bully me for insisting on a bit of
clarity and a bit of consideration for those from whom help is requested.

What on earth does a language barrier have to do with writing glm() when
glmer() is what is needed? That's the *R* language, which we are all
supposed to speak on this list!

cheers,

Rolf

--
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From j@h@d||e|d @end|ng |rom ed@@c@uk  Wed Mar 25 21:14:53 2020
From: j@h@d||e|d @end|ng |rom ed@@c@uk (Jarrod)
Date: Wed, 25 Mar 2020 20:14:53 +0000
Subject: [R-sig-ME] MCMCglmm with missing values
In-Reply-To: <CAEOuUYHQhkUJ+d==fgfhhUrYA=N1a=vo_=31JiRt6uPQNoxmgQ@mail.gmail.com>
References: <CAEOuUYHQhkUJ+d==fgfhhUrYA=N1a=vo_=31JiRt6uPQNoxmgQ@mail.gmail.com>
Message-ID: <46E26FB8-629A-4DD2-8F3A-3CDD71C2AD7E@ed.ac.uk>

Hi Monica,

You don?t need to split male and fitness up into separate columns. Just have a column for Fitness and use interactions between sex and trait to define the model you want. Your current specification is ill-defined because you?re trying to model a covariance between male and female fitness at the residual level.

Cheers,

Jarrod


> On 16 Mar 2020, at 13:24, Monica Anderson Berdal <m.anderson.berdal at gmail.com> wrote:
>
> Sorry, here is a plain text version with CSV data.
>
> I?m trying to run a MCMCglmm where two of the response variables and
> three of the fixed effects are sex specific. Each line represents one
> individual, and the data looks like this:
>
> Sex,Fitness_M,Fitness_F,Age_M,Age_F,Mate_mass,Behaviour,Mass,Temperature,Line
>
> M,743,NA,140,NA,NA,69,229,21,X
>
> M,515,NA,188,NA,NA,50,220,19,X
>
> M,390,NA,231,NA,NA,50,109,17,Y
>
> M,266,NA,96,NA,NA,39,113,19,Y
>
> M,323,NA,105,NA,NA,55,171,25,Y
>
> F,NA,112,NA,74,205,67,247,20,Y
>
> F,NA,107,NA,74,163,60,139,26,Z
>
> F,NA,8,NA,53,193,71,118,24,Z
>
> F,NA,207,NA,55,74,37,219,21,Z
>
> F,NA,300,NA,56,160,68,261,18,Z
>
> All individuals have a behaviour mass recorded, and only the sex
> specific variables have NAs. I?ve been using the at.level function to
> control for these missing values, but still get the error: Missing
> values in the fixed predictors
>
> cricketMCMC <- MCMCglmm(cbind(Fitness_F, Fitness_M, Behaviour, Mass)~
>
>                          at.level(trait, 1):(Age_F + Mate_mass) +
>
>                          at.level(trait, 2):(Age_M) +
>
>                          at.level(trait, 3):(Sex + Temperature) +
>
>                          at.level(trait, 4):(Sex + Temperature),
>
>                        random=~us(trait):Line,rcov=~us(trait):units,
>
>                        family=rep("gaussian",4), nitt=260000,
> thin=200, burnin=60000,
>
>                        verbose=FALSE, prior=prior.4t, pr=TRUE, data=Data)
>
>
>
> To avoid the issues of missing values I added mean values for Age_F
> and Mate_mass for the males and Age_M for the females, while still
> using the at.level function. The idea was to overcome the problem with
> the missing values while also ignoring the mean values for the sex
> specific traits. This model ran without problems, and to see if the
> model ran properly, I compared it to a second model where I added a
> random value instead of mean values. I used set.seed() before running
> both models, but the outputs are not the same, which means that the
> added values are still affecting the results.
>
> How can I avoid the problems of NAs when running an MCMCglmm on a data
> set with this structure?
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

The University of Edinburgh is a charitable body, registered in Scotland, with registration number SC005336.


From @@|v@dor@@nchezco|on @end|ng |rom prod|gy@net@mx  Wed Mar 25 23:35:42 2020
From: @@|v@dor@@nchezco|on @end|ng |rom prod|gy@net@mx (=?utf-8?Q?Salvador_S=C3=A1nchez-Col=C3=B3n?=)
Date: Wed, 25 Mar 2020 16:35:42 -0600
Subject: [R-sig-ME] MCMCglmm with missing values
In-Reply-To: <CAEOuUYHQhkUJ+d==fgfhhUrYA=N1a=vo_=31JiRt6uPQNoxmgQ@mail.gmail.com>
References: <CAEOuUYHQhkUJ+d==fgfhhUrYA=N1a=vo_=31JiRt6uPQNoxmgQ@mail.gmail.com>
Message-ID: <38CF4B69-5926-4AE0-B5F6-9C4F04EF474D@prodigy.net.mx>

My apologies for intruding but, would it not be easier and perfectly appropriate to construct separate models per sex for those response variables that are sex-specific? One cannot make between-sex comparisons for those variables anyway, as they are only defined for only one sex.

Salvador S?NCHEZ-COL?N 



> On 16 Mar 2020, at 7:25, Monica Anderson Berdal <m.anderson.berdal at gmail.com> wrote:
> 
> ?Sorry, here is a plain text version with CSV data.
> 
> I?m trying to run a MCMCglmm where two of the response variables and
> three of the fixed effects are sex specific. Each line represents one
> individual, and the data looks like this:
> 
> Sex,Fitness_M,Fitness_F,Age_M,Age_F,Mate_mass,Behaviour,Mass,Temperature,Line
> 
> M,743,NA,140,NA,NA,69,229,21,X
> 
> M,515,NA,188,NA,NA,50,220,19,X
> 
> M,390,NA,231,NA,NA,50,109,17,Y
> 
> M,266,NA,96,NA,NA,39,113,19,Y
> 
> M,323,NA,105,NA,NA,55,171,25,Y
> 
> F,NA,112,NA,74,205,67,247,20,Y
> 
> F,NA,107,NA,74,163,60,139,26,Z
> 
> F,NA,8,NA,53,193,71,118,24,Z
> 
> F,NA,207,NA,55,74,37,219,21,Z
> 
> F,NA,300,NA,56,160,68,261,18,Z
> 
> All individuals have a behaviour mass recorded, and only the sex
> specific variables have NAs. I?ve been using the at.level function to
> control for these missing values, but still get the error: Missing
> values in the fixed predictors
> 
> cricketMCMC <- MCMCglmm(cbind(Fitness_F, Fitness_M, Behaviour, Mass)~
> 
>                          at.level(trait, 1):(Age_F + Mate_mass) +
> 
>                          at.level(trait, 2):(Age_M) +
> 
>                          at.level(trait, 3):(Sex + Temperature) +
> 
>                          at.level(trait, 4):(Sex + Temperature),
> 
>                        random=~us(trait):Line,rcov=~us(trait):units,
> 
>                        family=rep("gaussian",4), nitt=260000,
> thin=200, burnin=60000,
> 
>                        verbose=FALSE, prior=prior.4t, pr=TRUE, data=Data)
> 
> 
> 
> To avoid the issues of missing values I added mean values for Age_F
> and Mate_mass for the males and Age_M for the females, while still
> using the at.level function. The idea was to overcome the problem with
> the missing values while also ignoring the mean values for the sex
> specific traits. This model ran without problems, and to see if the
> model ran properly, I compared it to a second model where I added a
> random value instead of mean values. I used set.seed() before running
> both models, but the outputs are not the same, which means that the
> added values are still affecting the results.
> 
> How can I avoid the problems of NAs when running an MCMCglmm on a data
> set with this structure?
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From mtonc|c @end|ng |rom ||r|@hr  Thu Mar 26 16:08:15 2020
From: mtonc|c @end|ng |rom ||r|@hr (marKo)
Date: Thu, 26 Mar 2020 16:08:15 +0100
Subject: [R-sig-ME] strange model fit- help
In-Reply-To: <CAJuCY5wjt0ZtrR0+by3mMPf4GaDzuWSSp_fO4Gs5=h0peRrUgg@mail.gmail.com>
References: <93095de2-cc5a-e294-b378-6978e6c23e51@ffri.hr>
 <CAJuCY5wjt0ZtrR0+by3mMPf4GaDzuWSSp_fO4Gs5=h0peRrUgg@mail.gmail.com>
Message-ID: <8faaf9ca-3926-758e-45ca-fb2acf2943f9@ffri.hr>

Thank a lot for the hints.
Rescaling into minutes helped a little. I would like to retain both the 
raw polynomial terms, both the non-centered time (for ease of 
readability). The model was written explicitly (not poly(time, 2, 
raw=TRUE) for the same reasons.

The random part is quite complex but i will retain it as it is, because 
of the almost perfect fit.

Thanks,

Marko

On 15. 03. 2020. 13:14, Thierry Onkelinx wrote:
> Dear Marko,
> 
> Keep in mind that squating time in seconds lead to large numbers (489 ^ 
> 2 = 239121). This forces the parameters estimates to be small. You can 
> solve this either by using orthogonal polynomial (poly(t, 2)) or by 
> rescaling t (e.g. in minutes rather than seconds: 489 s = 8.15 min, 8.15 
> ^ 2 = 66.4225) If you go for rescaling, then create 2 variables: t_min 
> and t_min2 (= t_min ^ 2). That will make your formula more reable.
> 
> It looks like you coded stim as an ordered factor. That not required 
> since you have only two levels. Use a default factor with before as 
> reference.
> 
> The problem with the strong correlations between t and t^2 random 
> effects is that they are highly correlated themselves.?cor(0:489, 
> (0:489) ^ 2) = 0.986 Note that is isn't solved by rescaling. Only 
> centering works .eg centering at 4 minutes yields cor(0:489 - 4 * 60 / 
> 60, (0:489 - 4 * 60) ^ 2) = 0.071 Orthogonal polynomials have the 
> benefit that they are uncorrelated by definition.?cor(poly(0:489, 2))
> 
> Bottomline: always scale and center polynomials. I prefer to scale and 
> center to revelant values, e.g. scale to a different unit and center to 
> an important point near the middle of the domain. That keeps your 
> parameters interpretable without the need to recalculate them (as you 
> would when scaling by the standard deviation and center to the mean).
> 
> Best regards,
> 
> Thierry
> 
> 
> ir. Thierry Onkelinx
> Statisticus / Statistician
> 
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE 
> AND FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be <http://www.inbo.be>
> 
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more 
> than asking him to perform a post-mortem examination: he may be able to 
> say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not 
> ensure that a reasonable answer can be extracted from a given body of 
> data. ~ John Tukey
> ///////////////////////////////////////////////////////////////////////////////////////////
> 
> <https://www.inbo.be>
> 
> 
> Op za 14 mrt. 2020 om 17:10 schreef marKo via R-sig-mixed-models 
> <r-sig-mixed-models at r-project.org 
> <mailto:r-sig-mixed-models at r-project.org>>:
> 
>     Hi.
> 
>     I have fitted a relatively complicated model to electrodermal data (a
>     simple resting and stimulus situation). The data summary follows.
> 
>      ?> summary(data)
>      ? ? ? ? id? ? ? ? ? ? ?sc? ? ? ? ? ? ? t? ? ? ? ? ? ? stim
>      ? g1_1? ?:? 49? ?Min.? ?:26798? ?Min.? ?:? 1.0? ?before? :3201
>      ? g1_12? :? 49? ?1st Qu.:32299? ?1st Qu.:123.0? ?after? ?:1543
>      ? g1_13? :? 49? ?Median :32486? ?Median :245.0
>      ? g1_14? :? 49? ?Mean? ?:32253? ?Mean? ?:244.9
>      ? g1_15? :? 49? ?3rd Qu.:32587? ?3rd Qu.:367.0
>      ? g1_2? ?:? 49? ?Max.? ?:32761? ?Max.? ?:489.0
>      ? (Other):4450
> 
>     id (person), and stim are factors, t is time (in s) and sc is skin
>     conductance level. Sc distribution is quite negatively asymmetrical at
>     the dataset level, although not that bad at the id level. As the
>     stimulus occur at a specified time, those two variables are correlated
>     (0.81).
> 
>     The model follows.
> 
>     m1<-lmer(sc~1+t+I(t^2)+stim+stim:t+stim:I(t^2)+(1+t+I(t^2)+stim+stim:t+stim:I(t^2)|id),
> 
>     data=data)
> 
>     Here goes the summary.
> 
>      ?> summary(m1)
>     Linear mixed model fit by maximum likelihood? ['lmerMod']
>     Formula: sc ~ 1 + t + I(t^2) + stim + stim:t + stim:I(t^2) + (1 + t +
>     I(t^2) + stim + stim:t + stim:I(t^2) | id)
>      ? ? Data: data
> 
>      ? ? ? AIC? ? ? BIC? ?logLik deviance df.resid
>      ? 62325.9? 62506.9 -31134.9? 62269.9? ? ?4716
> 
>     Scaled residuals:
>      ? ? ? Min? ? ? ?1Q? ?Median? ? ? ?3Q? ? ? Max
>     -24.3783? -0.1551? -0.0074? ?0.1392? 12.5288
> 
>     Random effects:
>      ? Groups? ?Name? ? ? ? ? Variance? Std.Dev.? Corr
>      ? id? ? ? ?(Intercept)? ?4.681e+04 2.164e+02
>      ? ? ? ? ? ?t? ? ? ? ? ? ?7.925e+00 2.815e+00? 1.00
>      ? ? ? ? ? ?I(t^2)? ? ? ? 2.559e-05 5.059e-03 -0.87 -0.87
>      ? ? ? ? ? ?stim.L? ? ? ? 1.591e+05 3.989e+02 -0.12 -0.12? 0.17
>      ? ? ? ? ? ?t:stim.L? ? ? 1.105e+00 1.051e+00 -0.58 -0.58? 0.78? 0.33
>      ? ? ? ? ? ?I(t^2):stim.L 2.367e-05 4.865e-03? 0.06? 0.06 -0.21
>     -0.76 -0.45
>      ? Residual? ? ? ? ? ? ? ?2.049e+04 1.432e+02
>     Number of obs: 4744, groups:? id, 97
> 
>     Fixed effects:
>      ? ? ? ? ? ? ? ? ?Estimate Std. Error t value
>     (Intercept)? ? 2.960e+04? 1.637e+02? 180.85
>     t? ? ? ? ? ? ? 1.291e+01? 8.493e-01? ?15.20
>     I(t^2)? ? ? ? -1.579e-02? 1.110e-03? -14.21
>     stim.L? ? ? ? -3.956e+03? 2.329e+02? -16.98
>     t:stim.L? ? ? ?2.047e+01? 1.136e+00? ?18.01
>     I(t^2):stim.L -2.477e-02? 1.478e-03? -16.76
> 
>     Correlation of Fixed Effects:
>      ? ? ? ? ? ? ?(Intr) t? ? ? I(t^2) stim.L t:st.L
>     t? ? ? ? ? ?-0.886
>     I(t^2)? ? ? ?0.811 -0.966
>     stim.L? ? ? ?0.972 -0.930? 0.868
>     t:stim.L? ? -0.989? 0.911 -0.826 -0.973
>     I(t^2):st.L? 0.917 -0.858? 0.761? 0.870 -0.947
> 
>     The fit of the model is quite good (pseudo r2 is 0.96), but have some
>     problems:
>     1: quite ?extreme? residuals (-24.3783,? 12.5288)
>     2: quite high correlations among random effects
>     3: lousy qqplot (apart from the perfect fit on the? from -2 to +2 std
>     normal quantiles)
> 
>     Help please? What is wrong with the model (something is, I?m sure).
> 
> 
> 
> 
>     -- 
>     Marko Ton?i?, PhD
>     Postdoctoral research assistant
>     University of Rijeka
>     Faculty of Humanities and Social Sciences
>     Department of Psychology
>     Sveucilisna avenija 4, 51000 Rijeka, CROATIA
>     e-mail: mtoncic at ffri.hr <mailto:mtoncic at ffri.hr>
> 
>     _______________________________________________
>     R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


-- 
Marko Ton?i?, PhD
Postdoctoral research assistant
University of Rijeka
Faculty of Humanities and Social Sciences
Department of Psychology
Sveucilisna avenija 4, 51000 Rijeka, CROATIA
e-mail: mtoncic at ffri.hr


From b@te@ @end|ng |rom @t@t@w|@c@edu  Thu Mar 26 16:51:02 2020
From: b@te@ @end|ng |rom @t@t@w|@c@edu (Douglas Bates)
Date: Thu, 26 Mar 2020 10:51:02 -0500
Subject: [R-sig-ME] Modifying the subject in replies
Message-ID: <CAO7JsnT3HYsdv0NR46Gqt4Qy8cmRzTw6xP4G1w0NGPwfd9KP_g@mail.gmail.com>

There are several messages on the list now that have a subject line
starting with

[R-sig-ME] [FORGED]

If your mail client is adding the `[FORGED]` to the subject line I would
appreciate it if you stopped its doing so. One of the first rules of
interacting on a mailing list is "Don't change the subject line".  Most
mail clients thread messages according to the subject line and by changing
it you ensure that the conversation is spread over multiple threads instead
of a single thread.  Changing the subject line to include alarming and
incorrect characterizations of the message is first class bush league
stuff.  (And, yes, "first class bush league" is a contradiction in terms.)

	[[alternative HTML version deleted]]


From d@n @end|ng |rom mcc|oy@|n|o  Thu Mar 26 18:44:03 2020
From: d@n @end|ng |rom mcc|oy@|n|o (Dan McCloy)
Date: Thu, 26 Mar 2020 17:44:03 +0000
Subject: [R-sig-ME] Modifying the subject in replies
In-Reply-To: <CAO7JsnT3HYsdv0NR46Gqt4Qy8cmRzTw6xP4G1w0NGPwfd9KP_g@mail.gmail.com>
References: <CAO7JsnT3HYsdv0NR46Gqt4Qy8cmRzTw6xP4G1w0NGPwfd9KP_g@mail.gmail.com>
Message-ID: <gkmp7GQDH5Bf84FM88CkAlHirMGzht4dLkjK841CBkPdcECJVLJm0dBv7KbLNV7f4Anf6_ZxIvvkP9i_nLKGZQcKuzAbmnoEoVUofLN7p8g=@mccloy.info>

While I agree that breaking message threading is annoying and users should not wilfully change subject lines, I would like to note that what you are asking is analogous to asking people to turn off their virus scanning software. My email client does not add [FORGED] to subject lines, but it does give me a big red warning at the top of your message, Doug, saying "This email has failed its domain's authentication requirements. It may be spoofed or improperly forwarded!" It is likely that the [FORGED] message is meant to convey the same warning to users of other mail systems / clients. This may or may not be under user control. In sum: this problem may ultimately be the fault of the list's mailserver doing something non-standard, and may not be within the power of the individual subscribers to alter.

-- dan
Daniel McCloy
https://dan.mccloy.info
Research Scientist
Institute for Learning and Brain Sciences
University of Washington

??????? Original Message ???????
On Thursday, March 26, 2020 8:51 AM, Douglas Bates <bates at stat.wisc.edu> wrote:

> There are several messages on the list now that have a subject line
> starting with
>
> [R-sig-ME] [FORGED]
>
> If your mail client is adding the `[FORGED]` to the subject line I would
> appreciate it if you stopped its doing so. One of the first rules of
> interacting on a mailing list is "Don't change the subject line". Most
> mail clients thread messages according to the subject line and by changing
> it you ensure that the conversation is spread over multiple threads instead
> of a single thread. Changing the subject line to include alarming and
> incorrect characterizations of the message is first class bush league
> stuff. (And, yes, "first class bush league" is a contradiction in terms.)
>
> [[alternative HTML version deleted]]
>
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From Tom_Ph|||pp| @end|ng |rom np@@gov  Thu Mar 26 18:57:11 2020
From: Tom_Ph|||pp| @end|ng |rom np@@gov (Philippi, Tom)
Date: Thu, 26 Mar 2020 17:57:11 +0000
Subject: [R-sig-ME] Modifying the subject in replies
Message-ID: <SA9PR09MB4640BF7AB1DB0DA2CFBA6BE0F3CF0@SA9PR09MB4640.namprd09.prod.outlook.com>

Not that Doug needs my defending him, but in a reply to the list it is simple enough to edit back out the [External] or [Forged] tag your email system inserted in the subject line.  That can be done in corporate/government gmail and corporate/government MS outlook, so I suspect in more user-centric systems, too.

I think the trick is to remove anything before either [R-sig-ME] or "Re: [R-sig-ME]" depending on whether you are replying to a reply.
Momentarily, we'll see if I got it right...  (if so, perhaps add this to the posting guidance?)

Tom

-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> On Behalf Of Dan McCloy
Sent: Thursday, March 26, 2020 10:44 AM
To: Douglas Bates <bates at stat.wisc.edu>
Cc: R-mixed models mailing list <r-sig-mixed-models at r-project.org>
Subject: [EXTERNAL] Re: [R-sig-ME] Modifying the subject in replies

While I agree that breaking message threading is annoying and users should not wilfully change subject lines, I would like to note that what you are asking is analogous to asking people to turn off their virus scanning software. My email client does not add [FORGED] to subject lines, but it does give me a big red warning at the top of your message, Doug, saying "This email has failed its domain's authentication requirements. It may be spoofed or improperly forwarded!" It is likely that the [FORGED] message is meant to convey the same warning to users of other mail systems / clients. This may or may not be under user control. In sum: this problem may ultimately be the fault of the list's mailserver doing something non-standard, and may not be within the power of the individual subscribers to alter.

-- dan
Daniel McCloy
https://dan.mccloy.info
Research Scientist
Institute for Learning and Brain Sciences University of Washington

??????? Original Message ???????
On Thursday, March 26, 2020 8:51 AM, Douglas Bates <bates at stat.wisc.edu> wrote:

> There are several messages on the list now that have a subject line 
> starting with
>
> [R-sig-ME] [FORGED]
>
> If your mail client is adding the `[FORGED]` to the subject line I 
> would appreciate it if you stopped its doing so. One of the first 
> rules of interacting on a mailing list is "Don't change the subject 
> line". Most mail clients thread messages according to the subject line 
> and by changing it you ensure that the conversation is spread over 
> multiple threads instead of a single thread. Changing the subject line 
> to include alarming and incorrect characterizations of the message is 
> first class bush league stuff. (And, yes, "first class bush league" is 
> a contradiction in terms.)
>
> [[alternative HTML version deleted]]
>
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Mar 26 19:19:27 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 26 Mar 2020 11:19:27 -0700
Subject: [R-sig-ME] Modifying the subject in replies
In-Reply-To: <CAO7JsnT3HYsdv0NR46Gqt4Qy8cmRzTw6xP4G1w0NGPwfd9KP_g@mail.gmail.com>
References: <CAO7JsnT3HYsdv0NR46Gqt4Qy8cmRzTw6xP4G1w0NGPwfd9KP_g@mail.gmail.com>
Message-ID: <FE71C89E-A127-43E7-ABED-A36703DFFFB2@dcn.davis.ca.us>

While I also don't appreciate the "[FORGED]" modification, email clients which depend on the subject line to maintain threading are broken. Direct complaints to the email client maintainer(s) to support RFC822-4.6.2. The real pain is when other people use broken clients that don't support that 38-year- old standard.

On March 26, 2020 8:51:02 AM PDT, Douglas Bates <bates at stat.wisc.edu> wrote:
>There are several messages on the list now that have a subject line
>starting with
>
>[R-sig-ME] [FORGED]
>
>If your mail client is adding the `[FORGED]` to the subject line I
>would
>appreciate it if you stopped its doing so. One of the first rules of
>interacting on a mailing list is "Don't change the subject line".  Most
>mail clients thread messages according to the subject line and by
>changing
>it you ensure that the conversation is spread over multiple threads
>instead
>of a single thread.  Changing the subject line to include alarming and
>incorrect characterizations of the message is first class bush league
>stuff.  (And, yes, "first class bush league" is a contradiction in
>terms.)
>
>	[[alternative HTML version deleted]]
>
>_______________________________________________
>R-sig-mixed-models at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

-- 
Sent from my phone. Please excuse my brevity.


From ||myo @end|ng |rom ucm@||@uc@edu  Sun Mar 29 05:17:46 2020
From: ||myo @end|ng |rom ucm@||@uc@edu (Lim, Youn Seon (limyo))
Date: Sun, 29 Mar 2020 03:17:46 +0000
Subject: [R-sig-ME] Error in ranef
Message-ID: <BE33E16A-60D1-423E-A446-061599A2BF71@contoso.com>

Dear Dr. Bolker,
Thanks for publishing such a wonderful R package.   I have a quick question -- In the example of ?ranef? (lme4 package pdf file, page 102), I was wondering how I could get the ?condsd?.  ranef doesn?t extract the info (although it is listed under the values), but you used it in the example.  I?d truly appreciate it if you?d help me figuring it out.
All the best,
Youn Seon

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Sun Mar 29 17:54:26 2020
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sun, 29 Mar 2020 11:54:26 -0400
Subject: [R-sig-ME] Error in ranef
In-Reply-To: <BE33E16A-60D1-423E-A446-061599A2BF71@contoso.com>
References: <BE33E16A-60D1-423E-A446-061599A2BF71@contoso.com>
Message-ID: <251b3bf8-ff6b-c377-f413-83f257ed9afa@gmail.com>


  Applying as.data.frame() to the results of ranef() (an object of class
"ranef.mer") returns a data frame with a condsd column: see
?as.data.frame.ranef.mer (the second half of the "Value:" section)

  cheers
   Ben Bolker

On 2020-03-28 11:17 p.m., Lim, Youn Seon (limyo) wrote:
> Dear Dr. Bolker,
> Thanks for publishing such a wonderful R package.   I have a quick question -- In the example of ?ranef? (lme4 package pdf file, page 102), I was wondering how I could get the ?condsd?.  ranef doesn?t extract the info (although it is listed under the values), but you used it in the example.  I?d truly appreciate it if you?d help me figuring it out.
> All the best,
> Youn Seon
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


