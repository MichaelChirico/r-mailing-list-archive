From daniel_rubi at ymail.com  Sat Oct  1 09:35:45 2016
From: daniel_rubi at ymail.com (Daniel Rubi)
Date: Sat, 1 Oct 2016 07:35:45 +0000 (UTC)
Subject: [R-sig-ME] GLM for comparing relative growth rates under different
	conditions
References: <496376740.8611.1475307345039.ref@mail.yahoo.com>
Message-ID: <496376740.8611.1475307345039@mail.yahoo.com>

Hi,I have this experimental design:?I have two sets of cell lines. One in which I mutated a gene and another in which I didn't and therefore serves as the control. I grow each of these cell lines under two conditions - one is selective and the other is normal and is therefore the control.What I'm measuring is a proxy for the number of cells after 100 hours - the end of the experiment, which is not an integer but rather a float. Since I start the experiment with the same number of cells for both cell lines and both conditions, this is assumed to be a proxy for growth rate.I'd like to estimate the effect of the genotype (gene mutated or gene normal) on growth under the selective condition relative to the normal condition. In simple words, is the ratio of the proxy of the number of cells from the mutated cell line under the selective condition to the proxy of the number of cells from the mutated line under the normal condition different from the ratio of the non-mutated cell line under selective and normal conditions?I have 6 replicates of each cell line in each of the conditions. I cannot assume that the ratios are in the [0,1] range and therefore a beta regression is not likely to fit here.Also, none of the four groups (2 cell lines in 2 conditions) are paired in any way.What would be the appropriate GLM for this problem?

	[[alternative HTML version deleted]]


From carlosfamilia at gmail.com  Sun Oct  2 17:57:14 2016
From: carlosfamilia at gmail.com (Carlos Familia)
Date: Sun, 2 Oct 2016 16:57:14 +0100
Subject: [R-sig-ME] Model diagnostics show slope in residuals plot and slope
 on the observed vs fitted plot is different than y = x
Message-ID: <9A728328-DD65-40D0-B4F5-E3F0FB0FEE76@gmail.com>

Hello,

I have in hands a quite large and unbalanced dataset, for which a Y continuous dependent variable was measured in 3 different conditions (C) for about 3000 subjects (ID) (although, not all subjects have Y values for the 3 conditions). Additionally, there is continuous measure W which was measured for all subjects.

I am interested in testing the following: 

- Is the effect of W significant overall
- Is the effect of W significant at each level of C
- Is the effect of C significant

In order to try to answer this, I have specified the following model with lmer:

lmer( Y ~ W * C + (1 | ID), data = df)

Which seems to proper reflect the structure of the data (I might be wrong here, any suggestions would be welcome).
However when running the diagnostic plots I noticed a slope in the residuals plot and a slope different than y = x for the observed vs fitted plot (as shown bellow). Which made me question the validity of the model for inference.

Could I still use this model for inference? Should I specify a different formula? Should I turn to lme and try to include different variances for each level of conditions (C)? Any ideas?

I would be really appreciated if anyone could help me with this.

Thanks in advance,
Carlos Fam?lia


From thierry.onkelinx at inbo.be  Mon Oct  3 09:50:25 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 3 Oct 2016 09:50:25 +0200
Subject: [R-sig-ME] Model diagnostics show slope in residuals plot and
 slope on the observed vs fitted plot is different than y = x
In-Reply-To: <9A728328-DD65-40D0-B4F5-E3F0FB0FEE76@gmail.com>
References: <9A728328-DD65-40D0-B4F5-E3F0FB0FEE76@gmail.com>
Message-ID: <CAJuCY5zGP9YT2StyofxD0ckkWLj3LfE7V6SF2t3mCGDQcD453w@mail.gmail.com>

Dear Carlos,

Your plot got stripped from your mail. Try sending it as pdf or put it
someone online and send us the URL.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-10-02 17:57 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com>:

> Hello,
>
> I have in hands a quite large and unbalanced dataset, for which a Y
> continuous dependent variable was measured in 3 different conditions (C)
> for about 3000 subjects (ID) (although, not all subjects have Y values for
> the 3 conditions). Additionally, there is continuous measure W which was
> measured for all subjects.
>
> I am interested in testing the following:
>
> - Is the effect of W significant overall
> - Is the effect of W significant at each level of C
> - Is the effect of C significant
>
> In order to try to answer this, I have specified the following model with
> lmer:
>
> lmer( Y ~ W * C + (1 | ID), data = df)
>
> Which seems to proper reflect the structure of the data (I might be wrong
> here, any suggestions would be welcome).
> However when running the diagnostic plots I noticed a slope in the
> residuals plot and a slope different than y = x for the observed vs fitted
> plot (as shown bellow). Which made me question the validity of the model
> for inference.
>
> Could I still use this model for inference? Should I specify a different
> formula? Should I turn to lme and try to include different variances for
> each level of conditions (C)? Any ideas?
>
> I would be really appreciated if anyone could help me with this.
>
> Thanks in advance,
> Carlos Fam?lia
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From carlosfamilia at gmail.com  Mon Oct  3 10:34:50 2016
From: carlosfamilia at gmail.com (Carlos Familia)
Date: Mon, 3 Oct 2016 09:34:50 +0100
Subject: [R-sig-ME] Model diagnostics show slope in residuals plot and
 slope on the observed vs fitted plot is different than y = x
In-Reply-To: <CAJuCY5zGP9YT2StyofxD0ckkWLj3LfE7V6SF2t3mCGDQcD453w@mail.gmail.com>
References: <9A728328-DD65-40D0-B4F5-E3F0FB0FEE76@gmail.com>
	<CAJuCY5zGP9YT2StyofxD0ckkWLj3LfE7V6SF2t3mCGDQcD453w@mail.gmail.com>
Message-ID: <A8D1BC76-E9A5-49B6-BD98-597E06CC3334@gmail.com>

Hello,

The image can be found here https://s18.postimg.org/rbx2vh2ex/Pasted_Graphic_4.png <https://s18.postimg.org/rbx2vh2ex/Pasted_Graphic_4.png>

Best regards,
Carlos Fam?lia

> On 3 Oct 2016, at 08:50, Thierry Onkelinx <thierry.onkelinx at inbo.be> wrote:
> 
> Dear Carlos,
> 
> Your plot got stripped from your mail. Try sending it as pdf or put it someone online and send us the URL.
> 
> Best regards,
> 
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest 
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance 
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
> 
> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner 
> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
> 
> 2016-10-02 17:57 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com <mailto:carlosfamilia at gmail.com>>:
> Hello,
> 
> I have in hands a quite large and unbalanced dataset, for which a Y continuous dependent variable was measured in 3 different conditions (C) for about 3000 subjects (ID) (although, not all subjects have Y values for the 3 conditions). Additionally, there is continuous measure W which was measured for all subjects.
> 
> I am interested in testing the following:
> 
> - Is the effect of W significant overall
> - Is the effect of W significant at each level of C
> - Is the effect of C significant
> 
> In order to try to answer this, I have specified the following model with lmer:
> 
> lmer( Y ~ W * C + (1 | ID), data = df)
> 
> Which seems to proper reflect the structure of the data (I might be wrong here, any suggestions would be welcome).
> However when running the diagnostic plots I noticed a slope in the residuals plot and a slope different than y = x for the observed vs fitted plot (as shown bellow). Which made me question the validity of the model for inference.
> 
> Could I still use this model for inference? Should I specify a different formula? Should I turn to lme and try to include different variances for each level of conditions (C)? Any ideas?
> 
> I would be really appreciated if anyone could help me with this.
> 
> Thanks in advance,
> Carlos Fam?lia
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Mon Oct  3 10:40:51 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 3 Oct 2016 10:40:51 +0200
Subject: [R-sig-ME] Model diagnostics show slope in residuals plot and
 slope on the observed vs fitted plot is different than y = x
In-Reply-To: <A8D1BC76-E9A5-49B6-BD98-597E06CC3334@gmail.com>
References: <9A728328-DD65-40D0-B4F5-E3F0FB0FEE76@gmail.com>
	<CAJuCY5zGP9YT2StyofxD0ckkWLj3LfE7V6SF2t3mCGDQcD453w@mail.gmail.com>
	<A8D1BC76-E9A5-49B6-BD98-597E06CC3334@gmail.com>
Message-ID: <CAJuCY5yw__tkj1xYxZf720Fvx8GnREqKuqUrK+ep2+_-R00yKg@mail.gmail.com>

Dear Carlos,

Can you show us a plot of the residuals versus W for each level of C? It
looks like either the relation of Y and W is not linear, or you are missing
an important covariate.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-10-03 10:34 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com>:

> Hello,
>
> The image can be found here https://s18.postimg.org/
> rbx2vh2ex/Pasted_Graphic_4.png
>
> Best regards,
> Carlos Fam?lia
>
> On 3 Oct 2016, at 08:50, Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
> Dear Carlos,
>
> Your plot got stripped from your mail. Try sending it as pdf or put it
> someone online and send us the URL.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-10-02 17:57 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com>:
>
>> Hello,
>>
>> I have in hands a quite large and unbalanced dataset, for which a Y
>> continuous dependent variable was measured in 3 different conditions (C)
>> for about 3000 subjects (ID) (although, not all subjects have Y values for
>> the 3 conditions). Additionally, there is continuous measure W which was
>> measured for all subjects.
>>
>> I am interested in testing the following:
>>
>> - Is the effect of W significant overall
>> - Is the effect of W significant at each level of C
>> - Is the effect of C significant
>>
>> In order to try to answer this, I have specified the following model with
>> lmer:
>>
>> lmer( Y ~ W * C + (1 | ID), data = df)
>>
>> Which seems to proper reflect the structure of the data (I might be wrong
>> here, any suggestions would be welcome).
>> However when running the diagnostic plots I noticed a slope in the
>> residuals plot and a slope different than y = x for the observed vs fitted
>> plot (as shown bellow). Which made me question the validity of the model
>> for inference.
>>
>> Could I still use this model for inference? Should I specify a different
>> formula? Should I turn to lme and try to include different variances for
>> each level of conditions (C)? Any ideas?
>>
>> I would be really appreciated if anyone could help me with this.
>>
>> Thanks in advance,
>> Carlos Fam?lia
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>

	[[alternative HTML version deleted]]


From carlosfamilia at gmail.com  Mon Oct  3 10:51:36 2016
From: carlosfamilia at gmail.com (Carlos Familia)
Date: Mon, 3 Oct 2016 09:51:36 +0100
Subject: [R-sig-ME] Model diagnostics show slope in residuals plot and
 slope on the observed vs fitted plot is different than y = x
In-Reply-To: <CAJuCY5yw__tkj1xYxZf720Fvx8GnREqKuqUrK+ep2+_-R00yKg@mail.gmail.com>
References: <9A728328-DD65-40D0-B4F5-E3F0FB0FEE76@gmail.com>
	<CAJuCY5zGP9YT2StyofxD0ckkWLj3LfE7V6SF2t3mCGDQcD453w@mail.gmail.com>
	<A8D1BC76-E9A5-49B6-BD98-597E06CC3334@gmail.com>
	<CAJuCY5yw__tkj1xYxZf720Fvx8GnREqKuqUrK+ep2+_-R00yKg@mail.gmail.com>
Message-ID: <83D5211F-B7FF-4202-A467-F8BBD0759A07@gmail.com>

Dear Thierry,

The image can be found here https://s4.postimg.org/lj5xf0rpp/Screen_Shot_2016_10_03_at_09_44_28.png <https://s4.postimg.org/lj5xf0rpp/Screen_Shot_2016_10_03_at_09_44_28.png>

Let me add another thing to the discussion, I was trying different models, and I tried the following

lmer( Y ~ X + (1 | C), data = df)

For which the residuals are distributed in a form I was expecting, however I am missing the part of the same individual being measured for different conditions, the plots can be found here, https://s25.postimg.org/oupckrapr/Screen_Shot_2016_10_03_at_09_49_20.png <https://s25.postimg.org/oupckrapr/Screen_Shot_2016_10_03_at_09_49_20.png> 

Thank you,
Carlos Fam?lia



> On 3 Oct 2016, at 09:40, Thierry Onkelinx <thierry.onkelinx at inbo.be> wrote:
> 
> Dear Carlos,
> 
> Can you show us a plot of the residuals versus W for each level of C? It looks like either the relation of Y and W is not linear, or you are missing an important covariate.
> 
> Best regards,
> 
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest 
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance 
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
> 
> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner 
> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
> 
> 2016-10-03 10:34 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com <mailto:carlosfamilia at gmail.com>>:
> Hello,
> 
> The image can be found here https://s18.postimg.org/rbx2vh2ex/Pasted_Graphic_4.png <https://s18.postimg.org/rbx2vh2ex/Pasted_Graphic_4.png>
> 
> Best regards,
> Carlos Fam?lia
> 
>> On 3 Oct 2016, at 08:50, Thierry Onkelinx <thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>> wrote:
>> 
>> Dear Carlos,
>> 
>> Your plot got stripped from your mail. Try sending it as pdf or put it someone online and send us the URL.
>> 
>> Best regards,
>> 
>> ir. Thierry Onkelinx
>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest 
>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance 
>> Kliniekstraat 25
>> 1070 Anderlecht
>> Belgium
>> 
>> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner 
>> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
>> 
>> 2016-10-02 17:57 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com <mailto:carlosfamilia at gmail.com>>:
>> Hello,
>> 
>> I have in hands a quite large and unbalanced dataset, for which a Y continuous dependent variable was measured in 3 different conditions (C) for about 3000 subjects (ID) (although, not all subjects have Y values for the 3 conditions). Additionally, there is continuous measure W which was measured for all subjects.
>> 
>> I am interested in testing the following:
>> 
>> - Is the effect of W significant overall
>> - Is the effect of W significant at each level of C
>> - Is the effect of C significant
>> 
>> In order to try to answer this, I have specified the following model with lmer:
>> 
>> lmer( Y ~ W * C + (1 | ID), data = df)
>> 
>> Which seems to proper reflect the structure of the data (I might be wrong here, any suggestions would be welcome).
>> However when running the diagnostic plots I noticed a slope in the residuals plot and a slope different than y = x for the observed vs fitted plot (as shown bellow). Which made me question the validity of the model for inference.
>> 
>> Could I still use this model for inference? Should I specify a different formula? Should I turn to lme and try to include different variances for each level of conditions (C)? Any ideas?
>> 
>> I would be really appreciated if anyone could help me with this.
>> 
>> Thanks in advance,
>> Carlos Fam?lia
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org> mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
> 


	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Mon Oct  3 10:57:53 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 3 Oct 2016 10:57:53 +0200
Subject: [R-sig-ME] Model diagnostics show slope in residuals plot and
 slope on the observed vs fitted plot is different than y = x
In-Reply-To: <83D5211F-B7FF-4202-A467-F8BBD0759A07@gmail.com>
References: <9A728328-DD65-40D0-B4F5-E3F0FB0FEE76@gmail.com>
	<CAJuCY5zGP9YT2StyofxD0ckkWLj3LfE7V6SF2t3mCGDQcD453w@mail.gmail.com>
	<A8D1BC76-E9A5-49B6-BD98-597E06CC3334@gmail.com>
	<CAJuCY5yw__tkj1xYxZf720Fvx8GnREqKuqUrK+ep2+_-R00yKg@mail.gmail.com>
	<83D5211F-B7FF-4202-A467-F8BBD0759A07@gmail.com>
Message-ID: <CAJuCY5wUFGU4t42+Kxb0he5izADnsVD+GwXURMs9Dw3_FWBoyQ@mail.gmail.com>

Dear Carlos,

Is X an other variable? Or did you ment W? The graphs give me a strong
indication for a missing covariate.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-10-03 10:51 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com>:

> Dear Thierry,
>
> The image can be found here https://s4.postimg.org/
> lj5xf0rpp/Screen_Shot_2016_10_03_at_09_44_28.png
>
> Let me add another thing to the discussion, I was trying different models,
> and I tried the following
>
> lmer( Y ~ X + (1 | C), data = df)
>
> For which the residuals are distributed in a form I was expecting, however
> I am missing the part of the same individual being measured for different
> conditions, the plots can be found here, https://s25.postimg.org/
> oupckrapr/Screen_Shot_2016_10_03_at_09_49_20.png
>
> Thank you,
> Carlos Fam?lia
>
>
>
> On 3 Oct 2016, at 09:40, Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
> Dear Carlos,
>
> Can you show us a plot of the residuals versus W for each level of C? It
> looks like either the relation of Y and W is not linear, or you are missing
> an important covariate.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-10-03 10:34 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com>:
>
>> Hello,
>>
>> The image can be found here https://s18.postimg.org/r
>> bx2vh2ex/Pasted_Graphic_4.png
>>
>> Best regards,
>> Carlos Fam?lia
>>
>> On 3 Oct 2016, at 08:50, Thierry Onkelinx <thierry.onkelinx at inbo.be>
>> wrote:
>>
>> Dear Carlos,
>>
>> Your plot got stripped from your mail. Try sending it as pdf or put it
>> someone online and send us the URL.
>>
>> Best regards,
>>
>> ir. Thierry Onkelinx
>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>> and Forest
>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>> Kliniekstraat 25
>> 1070 Anderlecht
>> Belgium
>>
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> 2016-10-02 17:57 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com>:
>>
>>> Hello,
>>>
>>> I have in hands a quite large and unbalanced dataset, for which a Y
>>> continuous dependent variable was measured in 3 different conditions (C)
>>> for about 3000 subjects (ID) (although, not all subjects have Y values for
>>> the 3 conditions). Additionally, there is continuous measure W which was
>>> measured for all subjects.
>>>
>>> I am interested in testing the following:
>>>
>>> - Is the effect of W significant overall
>>> - Is the effect of W significant at each level of C
>>> - Is the effect of C significant
>>>
>>> In order to try to answer this, I have specified the following model
>>> with lmer:
>>>
>>> lmer( Y ~ W * C + (1 | ID), data = df)
>>>
>>> Which seems to proper reflect the structure of the data (I might be
>>> wrong here, any suggestions would be welcome).
>>> However when running the diagnostic plots I noticed a slope in the
>>> residuals plot and a slope different than y = x for the observed vs fitted
>>> plot (as shown bellow). Which made me question the validity of the model
>>> for inference.
>>>
>>> Could I still use this model for inference? Should I specify a different
>>> formula? Should I turn to lme and try to include different variances for
>>> each level of conditions (C)? Any ideas?
>>>
>>> I would be really appreciated if anyone could help me with this.
>>>
>>> Thanks in advance,
>>> Carlos Fam?lia
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>>
>
>

	[[alternative HTML version deleted]]


From carlosfamilia at gmail.com  Mon Oct  3 11:00:19 2016
From: carlosfamilia at gmail.com (Carlos Familia)
Date: Mon, 3 Oct 2016 10:00:19 +0100
Subject: [R-sig-ME] Model diagnostics show slope in residuals plot and
 slope on the observed vs fitted plot is different than y = x
In-Reply-To: <CAJuCY5wUFGU4t42+Kxb0he5izADnsVD+GwXURMs9Dw3_FWBoyQ@mail.gmail.com>
References: <9A728328-DD65-40D0-B4F5-E3F0FB0FEE76@gmail.com>
	<CAJuCY5zGP9YT2StyofxD0ckkWLj3LfE7V6SF2t3mCGDQcD453w@mail.gmail.com>
	<A8D1BC76-E9A5-49B6-BD98-597E06CC3334@gmail.com>
	<CAJuCY5yw__tkj1xYxZf720Fvx8GnREqKuqUrK+ep2+_-R00yKg@mail.gmail.com>
	<83D5211F-B7FF-4202-A467-F8BBD0759A07@gmail.com>
	<CAJuCY5wUFGU4t42+Kxb0he5izADnsVD+GwXURMs9Dw3_FWBoyQ@mail.gmail.com>
Message-ID: <EE70C2B4-5F8B-4FDF-A737-2F0C167081D5@gmail.com>

Dear Thierry,

I am sorry I meant W:

lmer( Y ~ W + ( 1 | C ), data = df)

Thank you,
Carlos

> On 3 Oct 2016, at 09:57, Thierry Onkelinx <thierry.onkelinx at inbo.be> wrote:
> 
> Dear Carlos,
> 
> Is X an other variable? Or did you ment W? The graphs give me a strong indication for a missing covariate.
> 
> Best regards,
> 
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest 
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance 
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
> 
> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner 
> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
> 
> 2016-10-03 10:51 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com <mailto:carlosfamilia at gmail.com>>:
> Dear Thierry,
> 
> The image can be found here https://s4.postimg.org/lj5xf0rpp/Screen_Shot_2016_10_03_at_09_44_28.png <https://s4.postimg.org/lj5xf0rpp/Screen_Shot_2016_10_03_at_09_44_28.png>
> 
> Let me add another thing to the discussion, I was trying different models, and I tried the following
> 
> lmer( Y ~ X + (1 | C), data = df)
> 
> For which the residuals are distributed in a form I was expecting, however I am missing the part of the same individual being measured for different conditions, the plots can be found here, https://s25.postimg.org/oupckrapr/Screen_Shot_2016_10_03_at_09_49_20.png <https://s25.postimg.org/oupckrapr/Screen_Shot_2016_10_03_at_09_49_20.png> 
> 
> Thank you,
> Carlos Fam?lia
> 
> 
> 
>> On 3 Oct 2016, at 09:40, Thierry Onkelinx <thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>> wrote:
>> 
>> Dear Carlos,
>> 
>> Can you show us a plot of the residuals versus W for each level of C? It looks like either the relation of Y and W is not linear, or you are missing an important covariate.
>> 
>> Best regards,
>> 
>> ir. Thierry Onkelinx
>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest 
>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance 
>> Kliniekstraat 25
>> 1070 Anderlecht
>> Belgium
>> 
>> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner 
>> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
>> 
>> 2016-10-03 10:34 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com <mailto:carlosfamilia at gmail.com>>:
>> Hello,
>> 
>> The image can be found here https://s18.postimg.org/rbx2vh2ex/Pasted_Graphic_4.png <https://s18.postimg.org/rbx2vh2ex/Pasted_Graphic_4.png>
>> 
>> Best regards,
>> Carlos Fam?lia
>> 
>>> On 3 Oct 2016, at 08:50, Thierry Onkelinx <thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>> wrote:
>>> 
>>> Dear Carlos,
>>> 
>>> Your plot got stripped from your mail. Try sending it as pdf or put it someone online and send us the URL.
>>> 
>>> Best regards,
>>> 
>>> ir. Thierry Onkelinx
>>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest 
>>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance 
>>> Kliniekstraat 25
>>> 1070 Anderlecht
>>> Belgium
>>> 
>>> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> The plural of anecdote is not data. ~ Roger Brinner 
>>> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
>>> 
>>> 2016-10-02 17:57 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com <mailto:carlosfamilia at gmail.com>>:
>>> Hello,
>>> 
>>> I have in hands a quite large and unbalanced dataset, for which a Y continuous dependent variable was measured in 3 different conditions (C) for about 3000 subjects (ID) (although, not all subjects have Y values for the 3 conditions). Additionally, there is continuous measure W which was measured for all subjects.
>>> 
>>> I am interested in testing the following:
>>> 
>>> - Is the effect of W significant overall
>>> - Is the effect of W significant at each level of C
>>> - Is the effect of C significant
>>> 
>>> In order to try to answer this, I have specified the following model with lmer:
>>> 
>>> lmer( Y ~ W * C + (1 | ID), data = df)
>>> 
>>> Which seems to proper reflect the structure of the data (I might be wrong here, any suggestions would be welcome).
>>> However when running the diagnostic plots I noticed a slope in the residuals plot and a slope different than y = x for the observed vs fitted plot (as shown bellow). Which made me question the validity of the model for inference.
>>> 
>>> Could I still use this model for inference? Should I specify a different formula? Should I turn to lme and try to include different variances for each level of conditions (C)? Any ideas?
>>> 
>>> I would be really appreciated if anyone could help me with this.
>>> 
>>> Thanks in advance,
>>> Carlos Fam?lia
>>> 
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org> mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
>> 
> 
> 


	[[alternative HTML version deleted]]


From carlosfamilia at gmail.com  Mon Oct  3 11:08:08 2016
From: carlosfamilia at gmail.com (Carlos Familia)
Date: Mon, 3 Oct 2016 10:08:08 +0100
Subject: [R-sig-ME] Model diagnostics show slope in residuals plot and
 slope on the observed vs fitted plot is different than y = x
In-Reply-To: <CAJuCY5wUFGU4t42+Kxb0he5izADnsVD+GwXURMs9Dw3_FWBoyQ@mail.gmail.com>
References: <9A728328-DD65-40D0-B4F5-E3F0FB0FEE76@gmail.com>
	<CAJuCY5zGP9YT2StyofxD0ckkWLj3LfE7V6SF2t3mCGDQcD453w@mail.gmail.com>
	<A8D1BC76-E9A5-49B6-BD98-597E06CC3334@gmail.com>
	<CAJuCY5yw__tkj1xYxZf720Fvx8GnREqKuqUrK+ep2+_-R00yKg@mail.gmail.com>
	<83D5211F-B7FF-4202-A467-F8BBD0759A07@gmail.com>
	<CAJuCY5wUFGU4t42+Kxb0he5izADnsVD+GwXURMs9Dw3_FWBoyQ@mail.gmail.com>
Message-ID: <0F6078D2-EF30-4C23-B640-082F1EAA2D3F@gmail.com>

Dear Thierry,

If that is the case, would the initial model be of any use for inference given that I have no other data or covariate and most likely I won?t be able to get it?

Many thanks,
Carlos Fam?lia

> On 3 Oct 2016, at 09:57, Thierry Onkelinx <thierry.onkelinx at inbo.be> wrote:
> 
> Dear Carlos,
> 
> Is X an other variable? Or did you ment W? The graphs give me a strong indication for a missing covariate.
> 
> Best regards,
> 
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest 
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance 
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
> 
> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner 
> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
> 
> 2016-10-03 10:51 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com <mailto:carlosfamilia at gmail.com>>:
> Dear Thierry,
> 
> The image can be found here https://s4.postimg.org/lj5xf0rpp/Screen_Shot_2016_10_03_at_09_44_28.png <https://s4.postimg.org/lj5xf0rpp/Screen_Shot_2016_10_03_at_09_44_28.png>
> 
> Let me add another thing to the discussion, I was trying different models, and I tried the following
> 
> lmer( Y ~ X + (1 | C), data = df)
> 
> For which the residuals are distributed in a form I was expecting, however I am missing the part of the same individual being measured for different conditions, the plots can be found here, https://s25.postimg.org/oupckrapr/Screen_Shot_2016_10_03_at_09_49_20.png <https://s25.postimg.org/oupckrapr/Screen_Shot_2016_10_03_at_09_49_20.png> 
> 
> Thank you,
> Carlos Fam?lia
> 
> 
> 
>> On 3 Oct 2016, at 09:40, Thierry Onkelinx <thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>> wrote:
>> 
>> Dear Carlos,
>> 
>> Can you show us a plot of the residuals versus W for each level of C? It looks like either the relation of Y and W is not linear, or you are missing an important covariate.
>> 
>> Best regards,
>> 
>> ir. Thierry Onkelinx
>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest 
>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance 
>> Kliniekstraat 25
>> 1070 Anderlecht
>> Belgium
>> 
>> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner 
>> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
>> 
>> 2016-10-03 10:34 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com <mailto:carlosfamilia at gmail.com>>:
>> Hello,
>> 
>> The image can be found here https://s18.postimg.org/rbx2vh2ex/Pasted_Graphic_4.png <https://s18.postimg.org/rbx2vh2ex/Pasted_Graphic_4.png>
>> 
>> Best regards,
>> Carlos Fam?lia
>> 
>>> On 3 Oct 2016, at 08:50, Thierry Onkelinx <thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>> wrote:
>>> 
>>> Dear Carlos,
>>> 
>>> Your plot got stripped from your mail. Try sending it as pdf or put it someone online and send us the URL.
>>> 
>>> Best regards,
>>> 
>>> ir. Thierry Onkelinx
>>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest 
>>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance 
>>> Kliniekstraat 25
>>> 1070 Anderlecht
>>> Belgium
>>> 
>>> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> The plural of anecdote is not data. ~ Roger Brinner 
>>> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
>>> 
>>> 2016-10-02 17:57 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com <mailto:carlosfamilia at gmail.com>>:
>>> Hello,
>>> 
>>> I have in hands a quite large and unbalanced dataset, for which a Y continuous dependent variable was measured in 3 different conditions (C) for about 3000 subjects (ID) (although, not all subjects have Y values for the 3 conditions). Additionally, there is continuous measure W which was measured for all subjects.
>>> 
>>> I am interested in testing the following:
>>> 
>>> - Is the effect of W significant overall
>>> - Is the effect of W significant at each level of C
>>> - Is the effect of C significant
>>> 
>>> In order to try to answer this, I have specified the following model with lmer:
>>> 
>>> lmer( Y ~ W * C + (1 | ID), data = df)
>>> 
>>> Which seems to proper reflect the structure of the data (I might be wrong here, any suggestions would be welcome).
>>> However when running the diagnostic plots I noticed a slope in the residuals plot and a slope different than y = x for the observed vs fitted plot (as shown bellow). Which made me question the validity of the model for inference.
>>> 
>>> Could I still use this model for inference? Should I specify a different formula? Should I turn to lme and try to include different variances for each level of conditions (C)? Any ideas?
>>> 
>>> I would be really appreciated if anyone could help me with this.
>>> 
>>> Thanks in advance,
>>> Carlos Fam?lia
>>> 
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org> mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
>> 
> 
> 


	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Mon Oct  3 11:14:36 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 3 Oct 2016 11:14:36 +0200
Subject: [R-sig-ME] Model diagnostics show slope in residuals plot and
 slope on the observed vs fitted plot is different than y = x
In-Reply-To: <0F6078D2-EF30-4C23-B640-082F1EAA2D3F@gmail.com>
References: <9A728328-DD65-40D0-B4F5-E3F0FB0FEE76@gmail.com>
	<CAJuCY5zGP9YT2StyofxD0ckkWLj3LfE7V6SF2t3mCGDQcD453w@mail.gmail.com>
	<A8D1BC76-E9A5-49B6-BD98-597E06CC3334@gmail.com>
	<CAJuCY5yw__tkj1xYxZf720Fvx8GnREqKuqUrK+ep2+_-R00yKg@mail.gmail.com>
	<83D5211F-B7FF-4202-A467-F8BBD0759A07@gmail.com>
	<CAJuCY5wUFGU4t42+Kxb0he5izADnsVD+GwXURMs9Dw3_FWBoyQ@mail.gmail.com>
	<0F6078D2-EF30-4C23-B640-082F1EAA2D3F@gmail.com>
Message-ID: <CAJuCY5w3zh3eHDo7Zn41HbR=zsAO+BGD1NS+MysBCM22RMqy3g@mail.gmail.com>

Dear Carlos,

Can you send us the dataset? I have some more questions on the data and
have the data would be easier to look into this problem.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-10-03 11:08 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com>:

> Dear Thierry,
>
> If that is the case, would the initial model be of any use for inference
> given that I have no other data or covariate and most likely I won?t be
> able to get it?
>
> Many thanks,
> Carlos Fam?lia
>
> On 3 Oct 2016, at 09:57, Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
> Dear Carlos,
>
> Is X an other variable? Or did you ment W? The graphs give me a strong
> indication for a missing covariate.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-10-03 10:51 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com>:
>
>> Dear Thierry,
>>
>> The image can be found here https://s4.postimg.org/lj
>> 5xf0rpp/Screen_Shot_2016_10_03_at_09_44_28.png
>>
>> Let me add another thing to the discussion, I was trying different
>> models, and I tried the following
>>
>> lmer( Y ~ X + (1 | C), data = df)
>>
>> For which the residuals are distributed in a form I was expecting,
>> however I am missing the part of the same individual being measured for
>> different conditions, the plots can be found here,
>> https://s25.postimg.org/oupckrapr/Screen_Shot_2016_10_03_at_09_49_20.png
>>
>> Thank you,
>> Carlos Fam?lia
>>
>>
>>
>> On 3 Oct 2016, at 09:40, Thierry Onkelinx <thierry.onkelinx at inbo.be>
>> wrote:
>>
>> Dear Carlos,
>>
>> Can you show us a plot of the residuals versus W for each level of C? It
>> looks like either the relation of Y and W is not linear, or you are missing
>> an important covariate.
>>
>> Best regards,
>>
>> ir. Thierry Onkelinx
>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>> and Forest
>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>> Kliniekstraat 25
>> 1070 Anderlecht
>> Belgium
>>
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> 2016-10-03 10:34 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com>:
>>
>>> Hello,
>>>
>>> The image can be found here https://s18.postimg.org/r
>>> bx2vh2ex/Pasted_Graphic_4.png
>>>
>>> Best regards,
>>> Carlos Fam?lia
>>>
>>> On 3 Oct 2016, at 08:50, Thierry Onkelinx <thierry.onkelinx at inbo.be>
>>> wrote:
>>>
>>> Dear Carlos,
>>>
>>> Your plot got stripped from your mail. Try sending it as pdf or put it
>>> someone online and send us the URL.
>>>
>>> Best regards,
>>>
>>> ir. Thierry Onkelinx
>>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>>> and Forest
>>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>>> Kliniekstraat 25
>>> 1070 Anderlecht
>>> Belgium
>>>
>>> To call in the statistician after the experiment is done may be no more
>>> than asking him to perform a post-mortem examination: he may be able to say
>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> The plural of anecdote is not data. ~ Roger Brinner
>>> The combination of some data and an aching desire for an answer does not
>>> ensure that a reasonable answer can be extracted from a given body of data.
>>> ~ John Tukey
>>>
>>> 2016-10-02 17:57 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com>:
>>>
>>>> Hello,
>>>>
>>>> I have in hands a quite large and unbalanced dataset, for which a Y
>>>> continuous dependent variable was measured in 3 different conditions (C)
>>>> for about 3000 subjects (ID) (although, not all subjects have Y values for
>>>> the 3 conditions). Additionally, there is continuous measure W which was
>>>> measured for all subjects.
>>>>
>>>> I am interested in testing the following:
>>>>
>>>> - Is the effect of W significant overall
>>>> - Is the effect of W significant at each level of C
>>>> - Is the effect of C significant
>>>>
>>>> In order to try to answer this, I have specified the following model
>>>> with lmer:
>>>>
>>>> lmer( Y ~ W * C + (1 | ID), data = df)
>>>>
>>>> Which seems to proper reflect the structure of the data (I might be
>>>> wrong here, any suggestions would be welcome).
>>>> However when running the diagnostic plots I noticed a slope in the
>>>> residuals plot and a slope different than y = x for the observed vs fitted
>>>> plot (as shown bellow). Which made me question the validity of the model
>>>> for inference.
>>>>
>>>> Could I still use this model for inference? Should I specify a
>>>> different formula? Should I turn to lme and try to include different
>>>> variances for each level of conditions (C)? Any ideas?
>>>>
>>>> I would be really appreciated if anyone could help me with this.
>>>>
>>>> Thanks in advance,
>>>> Carlos Fam?lia
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>>
>>>
>>
>>
>
>

	[[alternative HTML version deleted]]


From paul.johnson at glasgow.ac.uk  Mon Oct  3 11:17:59 2016
From: paul.johnson at glasgow.ac.uk (Paul Johnson)
Date: Mon, 3 Oct 2016 09:17:59 +0000
Subject: [R-sig-ME] Model diagnostics show slope in residuals plot and
 slope on the observed vs fitted plot is different than y = x
In-Reply-To: <9A728328-DD65-40D0-B4F5-E3F0FB0FEE76@gmail.com>
References: <9A728328-DD65-40D0-B4F5-E3F0FB0FEE76@gmail.com>
Message-ID: <4FA6973B-B635-4EF1-B1AA-FE1138113166@glasgow.ac.uk>

Hi Carlos,

Your plot didn?t come through, as Thierry noted. However it?s expected that, unlike a standard linear regression model, an LMM with a nested structure such as yours will give a positive linear relationship between the residuals and the fitted values (might also be true for other structures?), provided the residual and random effect variances are > 0. Somebody will hopefully chip in with a formal explanation, but it?s basically a similar phenomenon to regression to the mean. 

Imagine a group of students taking each taking an aptitude test three times. There are two random factors: the difference in underlying aptitude between the students, modelled by the student ID random effect; and random variation between time points within each student (e.g. how good a particular student is feeling on a particular day). I?m ignoring variation between tests ? let?s unrealistically assume they?s all the same and students completely forget about them between tests. 

The students with the best mean scores will be a mixture of excellent students having three so-so (some good, some bad) days, and moderately good students having the good luck to have three good days, and the very best scores will come from students who were both excellent and lucky (although this category will be small). An important point is that there is no way of using the data to separate the moderate-student-lucky-days high scores from the excellent-student-average-days scores. If we simply took the mean of the scores, we would be overestimating the performance of the students on average (we?d have good estimates of the excellent students and overestimates of the moderate ones), so the best estimate is achieved by shrinking the scores towards the mean. 

This is what happens when the model is fitted. Each student is given a residual (random effect) at the student level (how good the student is relative to the value predicted by the fixed effects) and three residuals at the observation (between-test-within-student) level. For students with good scores, this will be a compromise between the inseparable excellent-student-average-days scenario and the moderate-student-lucky-days scenario. As a result, students with high student-level residuals (the student random effects) will also tend to have high inter-test residuals. The same is also true in negative for poor students and students having three bad days. So the student random effects (which are part of the fitted values) and the residuals will be positively correlated. 

You can check this using by simulating new data from the fitted model re-fitting the model, and comparing the residuals-x-fitted plot (which will be "perfect?) to the one from your data. Here?s a function that does this for lme4 fits:

devtools::install_github("pcdjohnson/GLMMmisc")
library(GLMMmisc)
sim.residplot(fit) # repeat this a few times to account for sampling error

If all is well, you should see a similar slope between the real and the simulated plots, in fact the general pattern of the residuals should be similar.

(The new package DHARMa ? https://theoreticalecology.wordpress.com/2016/08/28/dharma-an-r-package-for-residual-diagnostics-of-glmms/ ? takes a similar approach to assessing residuals, but in a less quick-and-dirty, more formally justified way.)

All the best,
Paul




> On 2 Oct 2016, at 16:57, Carlos Familia <carlosfamilia at gmail.com> wrote:
> 
> Hello,
> 
> I have in hands a quite large and unbalanced dataset, for which a Y continuous dependent variable was measured in 3 different conditions (C) for about 3000 subjects (ID) (although, not all subjects have Y values for the 3 conditions). Additionally, there is continuous measure W which was measured for all subjects.
> 
> I am interested in testing the following: 
> 
> - Is the effect of W significant overall
> - Is the effect of W significant at each level of C
> - Is the effect of C significant
> 
> In order to try to answer this, I have specified the following model with lmer:
> 
> lmer( Y ~ W * C + (1 | ID), data = df)
> 
> Which seems to proper reflect the structure of the data (I might be wrong here, any suggestions would be welcome).
> However when running the diagnostic plots I noticed a slope in the residuals plot and a slope different than y = x for the observed vs fitted plot (as shown bellow). Which made me question the validity of the model for inference.
> 
> Could I still use this model for inference? Should I specify a different formula? Should I turn to lme and try to include different variances for each level of conditions (C)? Any ideas?
> 
> I would be really appreciated if anyone could help me with this.
> 
> Thanks in advance,
> Carlos Fam?lia
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From carlosfamilia at gmail.com  Mon Oct  3 12:58:49 2016
From: carlosfamilia at gmail.com (Carlos Familia)
Date: Mon, 3 Oct 2016 11:58:49 +0100
Subject: [R-sig-ME] Model diagnostics show slope in residuals plot and
 slope on the observed vs fitted plot is different than y = x
In-Reply-To: <4FA6973B-B635-4EF1-B1AA-FE1138113166@glasgow.ac.uk>
References: <9A728328-DD65-40D0-B4F5-E3F0FB0FEE76@gmail.com>
	<4FA6973B-B635-4EF1-B1AA-FE1138113166@glasgow.ac.uk>
Message-ID: <5B3C62C8-C662-4008-B682-81ABF032E8C8@gmail.com>

Dear Paul,

Thank you for your reply.
The image can be found here  https://s18.postimg.org/rbx2vh2ex/Pasted_Graphic_4.png <https://s18.postimg.org/rbx2vh2ex/Pasted_Graphic_4.png>
Do you think it would be acceptable (for publication) if the analysis was performed separately for each group?

Thanks,
Carlos

> On 3 Oct 2016, at 10:17, Paul Johnson <paul.johnson at glasgow.ac.uk> wrote:
> 
> Hi Carlos,
> 
> Your plot didn?t come through, as Thierry noted. However it?s expected that, unlike a standard linear regression model, an LMM with a nested structure such as yours will give a positive linear relationship between the residuals and the fitted values (might also be true for other structures?), provided the residual and random effect variances are > 0. Somebody will hopefully chip in with a formal explanation, but it?s basically a similar phenomenon to regression to the mean. 
> 
> Imagine a group of students taking each taking an aptitude test three times. There are two random factors: the difference in underlying aptitude between the students, modelled by the student ID random effect; and random variation between time points within each student (e.g. how good a particular student is feeling on a particular day). I?m ignoring variation between tests ? let?s unrealistically assume they?s all the same and students completely forget about them between tests. 
> 
> The students with the best mean scores will be a mixture of excellent students having three so-so (some good, some bad) days, and moderately good students having the good luck to have three good days, and the very best scores will come from students who were both excellent and lucky (although this category will be small). An important point is that there is no way of using the data to separate the moderate-student-lucky-days high scores from the excellent-student-average-days scores. If we simply took the mean of the scores, we would be overestimating the performance of the students on average (we?d have good estimates of the excellent students and overestimates of the moderate ones), so the best estimate is achieved by shrinking the scores towards the mean. 
> 
> This is what happens when the model is fitted. Each student is given a residual (random effect) at the student level (how good the student is relative to the value predicted by the fixed effects) and three residuals at the observation (between-test-within-student) level. For students with good scores, this will be a compromise between the inseparable excellent-student-average-days scenario and the moderate-student-lucky-days scenario. As a result, students with high student-level residuals (the student random effects) will also tend to have high inter-test residuals. The same is also true in negative for poor students and students having three bad days. So the student random effects (which are part of the fitted values) and the residuals will be positively correlated. 
> 
> You can check this using by simulating new data from the fitted model re-fitting the model, and comparing the residuals-x-fitted plot (which will be "perfect?) to the one from your data. Here?s a function that does this for lme4 fits:
> 
> devtools::install_github("pcdjohnson/GLMMmisc")
> library(GLMMmisc)
> sim.residplot(fit) # repeat this a few times to account for sampling error
> 
> If all is well, you should see a similar slope between the real and the simulated plots, in fact the general pattern of the residuals should be similar.
> 
> (The new package DHARMa ? https://theoreticalecology.wordpress.com/2016/08/28/dharma-an-r-package-for-residual-diagnostics-of-glmms/ <https://theoreticalecology.wordpress.com/2016/08/28/dharma-an-r-package-for-residual-diagnostics-of-glmms/> ? takes a similar approach to assessing residuals, but in a less quick-and-dirty, more formally justified way.)
> 
> All the best,
> Paul
> 
> 
> 
> 
>> On 2 Oct 2016, at 16:57, Carlos Familia <carlosfamilia at gmail.com <mailto:carlosfamilia at gmail.com>> wrote:
>> 
>> Hello,
>> 
>> I have in hands a quite large and unbalanced dataset, for which a Y continuous dependent variable was measured in 3 different conditions (C) for about 3000 subjects (ID) (although, not all subjects have Y values for the 3 conditions). Additionally, there is continuous measure W which was measured for all subjects.
>> 
>> I am interested in testing the following: 
>> 
>> - Is the effect of W significant overall
>> - Is the effect of W significant at each level of C
>> - Is the effect of C significant
>> 
>> In order to try to answer this, I have specified the following model with lmer:
>> 
>> lmer( Y ~ W * C + (1 | ID), data = df)
>> 
>> Which seems to proper reflect the structure of the data (I might be wrong here, any suggestions would be welcome).
>> However when running the diagnostic plots I noticed a slope in the residuals plot and a slope different than y = x for the observed vs fitted plot (as shown bellow). Which made me question the validity of the model for inference.
>> 
>> Could I still use this model for inference? Should I specify a different formula? Should I turn to lme and try to include different variances for each level of conditions (C)? Any ideas?
>> 
>> I would be really appreciated if anyone could help me with this.
>> 
>> Thanks in advance,
>> Carlos Fam?lia
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org> mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Mon Oct  3 14:02:12 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 3 Oct 2016 14:02:12 +0200
Subject: [R-sig-ME] Model diagnostics show slope in residuals plot and
 slope on the observed vs fitted plot is different than y = x
In-Reply-To: <4FA6973B-B635-4EF1-B1AA-FE1138113166@glasgow.ac.uk>
References: <9A728328-DD65-40D0-B4F5-E3F0FB0FEE76@gmail.com>
	<4FA6973B-B635-4EF1-B1AA-FE1138113166@glasgow.ac.uk>
Message-ID: <CAJuCY5xR0jrDhPNFyXyE7R+7TxxFURTDK-Lv+ZfxemN5+LR-Mw@mail.gmail.com>

Dear Carlos,

I concur with Paul. After play a bit with the data you send me privately, I
see a few things which cause problems:
1) the number of measurements per ID is low. 1/3 has one measurement in
each level of C, 1/3 in two out of three levels of C and 1/3 in only one
level of C.
2) the variance of ID is larger than the residual variance
3) the effect of W is small compared to the variance of ID

If possible try to add extra covariates. If not I'd fall back on a simple
lm. Either with ignoring the repeated measurements or by sampling the data
so you have only one observation per ID.

Best regards,


ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-10-03 11:17 GMT+02:00 Paul Johnson <paul.johnson at glasgow.ac.uk>:

> Hi Carlos,
>
> Your plot didn?t come through, as Thierry noted. However it?s expected
> that, unlike a standard linear regression model, an LMM with a nested
> structure such as yours will give a positive linear relationship between
> the residuals and the fitted values (might also be true for other
> structures?), provided the residual and random effect variances are > 0.
> Somebody will hopefully chip in with a formal explanation, but it?s
> basically a similar phenomenon to regression to the mean.
>
> Imagine a group of students taking each taking an aptitude test three
> times. There are two random factors: the difference in underlying aptitude
> between the students, modelled by the student ID random effect; and random
> variation between time points within each student (e.g. how good a
> particular student is feeling on a particular day). I?m ignoring variation
> between tests ? let?s unrealistically assume they?s all the same and
> students completely forget about them between tests.
>
> The students with the best mean scores will be a mixture of excellent
> students having three so-so (some good, some bad) days, and moderately good
> students having the good luck to have three good days, and the very best
> scores will come from students who were both excellent and lucky (although
> this category will be small). An important point is that there is no way of
> using the data to separate the moderate-student-lucky-days high scores from
> the excellent-student-average-days scores. If we simply took the mean of
> the scores, we would be overestimating the performance of the students on
> average (we?d have good estimates of the excellent students and
> overestimates of the moderate ones), so the best estimate is achieved by
> shrinking the scores towards the mean.
>
> This is what happens when the model is fitted. Each student is given a
> residual (random effect) at the student level (how good the student is
> relative to the value predicted by the fixed effects) and three residuals
> at the observation (between-test-within-student) level. For students with
> good scores, this will be a compromise between the inseparable
> excellent-student-average-days scenario and the moderate-student-lucky-days
> scenario. As a result, students with high student-level residuals (the
> student random effects) will also tend to have high inter-test residuals.
> The same is also true in negative for poor students and students having
> three bad days. So the student random effects (which are part of the fitted
> values) and the residuals will be positively correlated.
>
> You can check this using by simulating new data from the fitted model
> re-fitting the model, and comparing the residuals-x-fitted plot (which will
> be "perfect?) to the one from your data. Here?s a function that does this
> for lme4 fits:
>
> devtools::install_github("pcdjohnson/GLMMmisc")
> library(GLMMmisc)
> sim.residplot(fit) # repeat this a few times to account for sampling error
>
> If all is well, you should see a similar slope between the real and the
> simulated plots, in fact the general pattern of the residuals should be
> similar.
>
> (The new package DHARMa ? https://theoreticalecology.
> wordpress.com/2016/08/28/dharma-an-r-package-for-
> residual-diagnostics-of-glmms/ ? takes a similar approach to assessing
> residuals, but in a less quick-and-dirty, more formally justified way.)
>
> All the best,
> Paul
>
>
>
>
> > On 2 Oct 2016, at 16:57, Carlos Familia <carlosfamilia at gmail.com> wrote:
> >
> > Hello,
> >
> > I have in hands a quite large and unbalanced dataset, for which a Y
> continuous dependent variable was measured in 3 different conditions (C)
> for about 3000 subjects (ID) (although, not all subjects have Y values for
> the 3 conditions). Additionally, there is continuous measure W which was
> measured for all subjects.
> >
> > I am interested in testing the following:
> >
> > - Is the effect of W significant overall
> > - Is the effect of W significant at each level of C
> > - Is the effect of C significant
> >
> > In order to try to answer this, I have specified the following model
> with lmer:
> >
> > lmer( Y ~ W * C + (1 | ID), data = df)
> >
> > Which seems to proper reflect the structure of the data (I might be
> wrong here, any suggestions would be welcome).
> > However when running the diagnostic plots I noticed a slope in the
> residuals plot and a slope different than y = x for the observed vs fitted
> plot (as shown bellow). Which made me question the validity of the model
> for inference.
> >
> > Could I still use this model for inference? Should I specify a different
> formula? Should I turn to lme and try to include different variances for
> each level of conditions (C)? Any ideas?
> >
> > I would be really appreciated if anyone could help me with this.
> >
> > Thanks in advance,
> > Carlos Fam?lia
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From carlosfamilia at gmail.com  Mon Oct  3 14:06:54 2016
From: carlosfamilia at gmail.com (Carlos Familia)
Date: Mon, 3 Oct 2016 13:06:54 +0100
Subject: [R-sig-ME] Model diagnostics show slope in residuals plot and
 slope on the observed vs fitted plot is different than y = x
In-Reply-To: <CAJuCY5xR0jrDhPNFyXyE7R+7TxxFURTDK-Lv+ZfxemN5+LR-Mw@mail.gmail.com>
References: <9A728328-DD65-40D0-B4F5-E3F0FB0FEE76@gmail.com>
	<4FA6973B-B635-4EF1-B1AA-FE1138113166@glasgow.ac.uk>
	<CAJuCY5xR0jrDhPNFyXyE7R+7TxxFURTDK-Lv+ZfxemN5+LR-Mw@mail.gmail.com>
Message-ID: <D16EB223-D5C3-43D9-B519-D566893845D2@gmail.com>

Dear Thierry,

When you say sampling the data to have only one observation per ID, you mean reducing the dataset to cases where the Y variable was only measured in 1 condition?
I have though about going lm  with this, but it just didn?t feel wright...

Many thanks,
Carlos

> On 3 Oct 2016, at 13:02, Thierry Onkelinx <thierry.onkelinx at inbo.be> wrote:
> 
> Dear Carlos,
> 
> I concur with Paul. After play a bit with the data you send me privately, I see a few things which cause problems: 
> 1) the number of measurements per ID is low. 1/3 has one measurement in each level of C, 1/3 in two out of three levels of C and 1/3 in only one level of C. 
> 2) the variance of ID is larger than the residual variance
> 3) the effect of W is small compared to the variance of ID
> 
> If possible try to add extra covariates. If not I'd fall back on a simple lm. Either with ignoring the repeated measurements or by sampling the data so you have only one observation per ID. 
> 
> Best regards,
> 
> 
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest 
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance 
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
> 
> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner 
> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
> 
> 2016-10-03 11:17 GMT+02:00 Paul Johnson <paul.johnson at glasgow.ac.uk <mailto:paul.johnson at glasgow.ac.uk>>:
> Hi Carlos,
> 
> Your plot didn?t come through, as Thierry noted. However it?s expected that, unlike a standard linear regression model, an LMM with a nested structure such as yours will give a positive linear relationship between the residuals and the fitted values (might also be true for other structures?), provided the residual and random effect variances are > 0. Somebody will hopefully chip in with a formal explanation, but it?s basically a similar phenomenon to regression to the mean.
> 
> Imagine a group of students taking each taking an aptitude test three times. There are two random factors: the difference in underlying aptitude between the students, modelled by the student ID random effect; and random variation between time points within each student (e.g. how good a particular student is feeling on a particular day). I?m ignoring variation between tests ? let?s unrealistically assume they?s all the same and students completely forget about them between tests.
> 
> The students with the best mean scores will be a mixture of excellent students having three so-so (some good, some bad) days, and moderately good students having the good luck to have three good days, and the very best scores will come from students who were both excellent and lucky (although this category will be small). An important point is that there is no way of using the data to separate the moderate-student-lucky-days high scores from the excellent-student-average-days scores. If we simply took the mean of the scores, we would be overestimating the performance of the students on average (we?d have good estimates of the excellent students and overestimates of the moderate ones), so the best estimate is achieved by shrinking the scores towards the mean.
> 
> This is what happens when the model is fitted. Each student is given a residual (random effect) at the student level (how good the student is relative to the value predicted by the fixed effects) and three residuals at the observation (between-test-within-student) level. For students with good scores, this will be a compromise between the inseparable excellent-student-average-days scenario and the moderate-student-lucky-days scenario. As a result, students with high student-level residuals (the student random effects) will also tend to have high inter-test residuals. The same is also true in negative for poor students and students having three bad days. So the student random effects (which are part of the fitted values) and the residuals will be positively correlated.
> 
> You can check this using by simulating new data from the fitted model re-fitting the model, and comparing the residuals-x-fitted plot (which will be "perfect?) to the one from your data. Here?s a function that does this for lme4 fits:
> 
> devtools::install_github("pcdjohnson/GLMMmisc")
> library(GLMMmisc)
> sim.residplot(fit) # repeat this a few times to account for sampling error
> 
> If all is well, you should see a similar slope between the real and the simulated plots, in fact the general pattern of the residuals should be similar.
> 
> (The new package DHARMa ? https://theoreticalecology.wordpress.com/2016/08/28/dharma-an-r-package-for-residual-diagnostics-of-glmms/ <https://theoreticalecology.wordpress.com/2016/08/28/dharma-an-r-package-for-residual-diagnostics-of-glmms/> ? takes a similar approach to assessing residuals, but in a less quick-and-dirty, more formally justified way.)
> 
> All the best,
> Paul
> 
> 
> 
> 
> > On 2 Oct 2016, at 16:57, Carlos Familia <carlosfamilia at gmail.com <mailto:carlosfamilia at gmail.com>> wrote:
> >
> > Hello,
> >
> > I have in hands a quite large and unbalanced dataset, for which a Y continuous dependent variable was measured in 3 different conditions (C) for about 3000 subjects (ID) (although, not all subjects have Y values for the 3 conditions). Additionally, there is continuous measure W which was measured for all subjects.
> >
> > I am interested in testing the following:
> >
> > - Is the effect of W significant overall
> > - Is the effect of W significant at each level of C
> > - Is the effect of C significant
> >
> > In order to try to answer this, I have specified the following model with lmer:
> >
> > lmer( Y ~ W * C + (1 | ID), data = df)
> >
> > Which seems to proper reflect the structure of the data (I might be wrong here, any suggestions would be welcome).
> > However when running the diagnostic plots I noticed a slope in the residuals plot and a slope different than y = x for the observed vs fitted plot (as shown bellow). Which made me question the validity of the model for inference.
> >
> > Could I still use this model for inference? Should I specify a different formula? Should I turn to lme and try to include different variances for each level of conditions (C)? Any ideas?
> >
> > I would be really appreciated if anyone could help me with this.
> >
> > Thanks in advance,
> > Carlos Fam?lia
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org> mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>


	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Mon Oct  3 14:10:05 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 3 Oct 2016 14:10:05 +0200
Subject: [R-sig-ME] Model diagnostics show slope in residuals plot and
 slope on the observed vs fitted plot is different than y = x
In-Reply-To: <D16EB223-D5C3-43D9-B519-D566893845D2@gmail.com>
References: <9A728328-DD65-40D0-B4F5-E3F0FB0FEE76@gmail.com>
	<4FA6973B-B635-4EF1-B1AA-FE1138113166@glasgow.ac.uk>
	<CAJuCY5xR0jrDhPNFyXyE7R+7TxxFURTDK-Lv+ZfxemN5+LR-Mw@mail.gmail.com>
	<D16EB223-D5C3-43D9-B519-D566893845D2@gmail.com>
Message-ID: <CAJuCY5wsaozm44TurnHBU6v6OMN-HzK_R4EB-eRzQGdkHh2dLw@mail.gmail.com>

In case you have more than one measurement per ID, select one of them at
random. Something like

library(dplyr)
df %>%
group_by(id) %>%
sample_n(1)



ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-10-03 14:06 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com>:

> Dear Thierry,
>
> When you say sampling the data to have only one observation per ID, you
> mean reducing the dataset to cases where the Y variable was only measured
> in 1 condition?
> I have though about going lm  with this, but it just didn?t feel wright...
>
> Many thanks,
> Carlos
>
> On 3 Oct 2016, at 13:02, Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
> Dear Carlos,
>
> I concur with Paul. After play a bit with the data you send me privately,
> I see a few things which cause problems:
> 1) the number of measurements per ID is low. 1/3 has one measurement in
> each level of C, 1/3 in two out of three levels of C and 1/3 in only one
> level of C.
> 2) the variance of ID is larger than the residual variance
> 3) the effect of W is small compared to the variance of ID
>
> If possible try to add extra covariates. If not I'd fall back on a simple
> lm. Either with ignoring the repeated measurements or by sampling the data
> so you have only one observation per ID.
>
> Best regards,
>
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-10-03 11:17 GMT+02:00 Paul Johnson <paul.johnson at glasgow.ac.uk>:
>
>> Hi Carlos,
>>
>> Your plot didn?t come through, as Thierry noted. However it?s expected
>> that, unlike a standard linear regression model, an LMM with a nested
>> structure such as yours will give a positive linear relationship between
>> the residuals and the fitted values (might also be true for other
>> structures?), provided the residual and random effect variances are > 0.
>> Somebody will hopefully chip in with a formal explanation, but it?s
>> basically a similar phenomenon to regression to the mean.
>>
>> Imagine a group of students taking each taking an aptitude test three
>> times. There are two random factors: the difference in underlying aptitude
>> between the students, modelled by the student ID random effect; and random
>> variation between time points within each student (e.g. how good a
>> particular student is feeling on a particular day). I?m ignoring variation
>> between tests ? let?s unrealistically assume they?s all the same and
>> students completely forget about them between tests.
>>
>> The students with the best mean scores will be a mixture of excellent
>> students having three so-so (some good, some bad) days, and moderately good
>> students having the good luck to have three good days, and the very best
>> scores will come from students who were both excellent and lucky (although
>> this category will be small). An important point is that there is no way of
>> using the data to separate the moderate-student-lucky-days high scores from
>> the excellent-student-average-days scores. If we simply took the mean of
>> the scores, we would be overestimating the performance of the students on
>> average (we?d have good estimates of the excellent students and
>> overestimates of the moderate ones), so the best estimate is achieved by
>> shrinking the scores towards the mean.
>>
>> This is what happens when the model is fitted. Each student is given a
>> residual (random effect) at the student level (how good the student is
>> relative to the value predicted by the fixed effects) and three residuals
>> at the observation (between-test-within-student) level. For students with
>> good scores, this will be a compromise between the inseparable
>> excellent-student-average-days scenario and the moderate-student-lucky-days
>> scenario. As a result, students with high student-level residuals (the
>> student random effects) will also tend to have high inter-test residuals.
>> The same is also true in negative for poor students and students having
>> three bad days. So the student random effects (which are part of the fitted
>> values) and the residuals will be positively correlated.
>>
>> You can check this using by simulating new data from the fitted model
>> re-fitting the model, and comparing the residuals-x-fitted plot (which will
>> be "perfect?) to the one from your data. Here?s a function that does this
>> for lme4 fits:
>>
>> devtools::install_github("pcdjohnson/GLMMmisc")
>> library(GLMMmisc)
>> sim.residplot(fit) # repeat this a few times to account for sampling error
>>
>> If all is well, you should see a similar slope between the real and the
>> simulated plots, in fact the general pattern of the residuals should be
>> similar.
>>
>> (The new package DHARMa ? https://theoreticalecology.wor
>> dpress.com/2016/08/28/dharma-an-r-package-for-residual-
>> diagnostics-of-glmms/ ? takes a similar approach to assessing residuals,
>> but in a less quick-and-dirty, more formally justified way.)
>>
>> All the best,
>> Paul
>>
>>
>>
>>
>> > On 2 Oct 2016, at 16:57, Carlos Familia <carlosfamilia at gmail.com>
>> wrote:
>> >
>> > Hello,
>> >
>> > I have in hands a quite large and unbalanced dataset, for which a Y
>> continuous dependent variable was measured in 3 different conditions (C)
>> for about 3000 subjects (ID) (although, not all subjects have Y values for
>> the 3 conditions). Additionally, there is continuous measure W which was
>> measured for all subjects.
>> >
>> > I am interested in testing the following:
>> >
>> > - Is the effect of W significant overall
>> > - Is the effect of W significant at each level of C
>> > - Is the effect of C significant
>> >
>> > In order to try to answer this, I have specified the following model
>> with lmer:
>> >
>> > lmer( Y ~ W * C + (1 | ID), data = df)
>> >
>> > Which seems to proper reflect the structure of the data (I might be
>> wrong here, any suggestions would be welcome).
>> > However when running the diagnostic plots I noticed a slope in the
>> residuals plot and a slope different than y = x for the observed vs fitted
>> plot (as shown bellow). Which made me question the validity of the model
>> for inference.
>> >
>> > Could I still use this model for inference? Should I specify a
>> different formula? Should I turn to lme and try to include different
>> variances for each level of conditions (C)? Any ideas?
>> >
>> > I would be really appreciated if anyone could help me with this.
>> >
>> > Thanks in advance,
>> > Carlos Fam?lia
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>
>

	[[alternative HTML version deleted]]


From carlosfamilia at gmail.com  Mon Oct  3 14:20:17 2016
From: carlosfamilia at gmail.com (Carlos Familia)
Date: Mon, 3 Oct 2016 13:20:17 +0100
Subject: [R-sig-ME] Model diagnostics show slope in residuals plot and
 slope on the observed vs fitted plot is different than y = x
In-Reply-To: <CAJuCY5wsaozm44TurnHBU6v6OMN-HzK_R4EB-eRzQGdkHh2dLw@mail.gmail.com>
References: <9A728328-DD65-40D0-B4F5-E3F0FB0FEE76@gmail.com>
	<4FA6973B-B635-4EF1-B1AA-FE1138113166@glasgow.ac.uk>
	<CAJuCY5xR0jrDhPNFyXyE7R+7TxxFURTDK-Lv+ZfxemN5+LR-Mw@mail.gmail.com>
	<D16EB223-D5C3-43D9-B519-D566893845D2@gmail.com>
	<CAJuCY5wsaozm44TurnHBU6v6OMN-HzK_R4EB-eRzQGdkHh2dLw@mail.gmail.com>
Message-ID: <C7A4790F-059C-48FD-AE45-DDB8187EDC38@gmail.com>

Do you think this approach is sound and easily justifiable in a paper?

Many thanks,
Carlos
> On 3 Oct 2016, at 13:10, Thierry Onkelinx <thierry.onkelinx at inbo.be> wrote:
> 
> In case you have more than one measurement per ID, select one of them at random. Something like
> 
> library(dplyr)
> df %>%
> group_by(id) %>%
> sample_n(1)
> 
> 
> 
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest 
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance 
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
> 
> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner 
> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
> 
> 2016-10-03 14:06 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com <mailto:carlosfamilia at gmail.com>>:
> Dear Thierry,
> 
> When you say sampling the data to have only one observation per ID, you mean reducing the dataset to cases where the Y variable was only measured in 1 condition?
> I have though about going lm  with this, but it just didn?t feel wright...
> 
> Many thanks,
> Carlos
> 
>> On 3 Oct 2016, at 13:02, Thierry Onkelinx <thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>> wrote:
>> 
>> Dear Carlos,
>> 
>> I concur with Paul. After play a bit with the data you send me privately, I see a few things which cause problems: 
>> 1) the number of measurements per ID is low. 1/3 has one measurement in each level of C, 1/3 in two out of three levels of C and 1/3 in only one level of C. 
>> 2) the variance of ID is larger than the residual variance
>> 3) the effect of W is small compared to the variance of ID
>> 
>> If possible try to add extra covariates. If not I'd fall back on a simple lm. Either with ignoring the repeated measurements or by sampling the data so you have only one observation per ID. 
>> 
>> Best regards,
>> 
>> 
>> ir. Thierry Onkelinx
>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest 
>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance 
>> Kliniekstraat 25
>> 1070 Anderlecht
>> Belgium
>> 
>> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner 
>> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
>> 
>> 2016-10-03 11:17 GMT+02:00 Paul Johnson <paul.johnson at glasgow.ac.uk <mailto:paul.johnson at glasgow.ac.uk>>:
>> Hi Carlos,
>> 
>> Your plot didn?t come through, as Thierry noted. However it?s expected that, unlike a standard linear regression model, an LMM with a nested structure such as yours will give a positive linear relationship between the residuals and the fitted values (might also be true for other structures?), provided the residual and random effect variances are > 0. Somebody will hopefully chip in with a formal explanation, but it?s basically a similar phenomenon to regression to the mean.
>> 
>> Imagine a group of students taking each taking an aptitude test three times. There are two random factors: the difference in underlying aptitude between the students, modelled by the student ID random effect; and random variation between time points within each student (e.g. how good a particular student is feeling on a particular day). I?m ignoring variation between tests ? let?s unrealistically assume they?s all the same and students completely forget about them between tests.
>> 
>> The students with the best mean scores will be a mixture of excellent students having three so-so (some good, some bad) days, and moderately good students having the good luck to have three good days, and the very best scores will come from students who were both excellent and lucky (although this category will be small). An important point is that there is no way of using the data to separate the moderate-student-lucky-days high scores from the excellent-student-average-days scores. If we simply took the mean of the scores, we would be overestimating the performance of the students on average (we?d have good estimates of the excellent students and overestimates of the moderate ones), so the best estimate is achieved by shrinking the scores towards the mean.
>> 
>> This is what happens when the model is fitted. Each student is given a residual (random effect) at the student level (how good the student is relative to the value predicted by the fixed effects) and three residuals at the observation (between-test-within-student) level. For students with good scores, this will be a compromise between the inseparable excellent-student-average-days scenario and the moderate-student-lucky-days scenario. As a result, students with high student-level residuals (the student random effects) will also tend to have high inter-test residuals. The same is also true in negative for poor students and students having three bad days. So the student random effects (which are part of the fitted values) and the residuals will be positively correlated.
>> 
>> You can check this using by simulating new data from the fitted model re-fitting the model, and comparing the residuals-x-fitted plot (which will be "perfect?) to the one from your data. Here?s a function that does this for lme4 fits:
>> 
>> devtools::install_github("pcdjohnson/GLMMmisc")
>> library(GLMMmisc)
>> sim.residplot(fit) # repeat this a few times to account for sampling error
>> 
>> If all is well, you should see a similar slope between the real and the simulated plots, in fact the general pattern of the residuals should be similar.
>> 
>> (The new package DHARMa ? https://theoreticalecology.wordpress.com/2016/08/28/dharma-an-r-package-for-residual-diagnostics-of-glmms/ <https://theoreticalecology.wordpress.com/2016/08/28/dharma-an-r-package-for-residual-diagnostics-of-glmms/> ? takes a similar approach to assessing residuals, but in a less quick-and-dirty, more formally justified way.)
>> 
>> All the best,
>> Paul
>> 
>> 
>> 
>> 
>> > On 2 Oct 2016, at 16:57, Carlos Familia <carlosfamilia at gmail.com <mailto:carlosfamilia at gmail.com>> wrote:
>> >
>> > Hello,
>> >
>> > I have in hands a quite large and unbalanced dataset, for which a Y continuous dependent variable was measured in 3 different conditions (C) for about 3000 subjects (ID) (although, not all subjects have Y values for the 3 conditions). Additionally, there is continuous measure W which was measured for all subjects.
>> >
>> > I am interested in testing the following:
>> >
>> > - Is the effect of W significant overall
>> > - Is the effect of W significant at each level of C
>> > - Is the effect of C significant
>> >
>> > In order to try to answer this, I have specified the following model with lmer:
>> >
>> > lmer( Y ~ W * C + (1 | ID), data = df)
>> >
>> > Which seems to proper reflect the structure of the data (I might be wrong here, any suggestions would be welcome).
>> > However when running the diagnostic plots I noticed a slope in the residuals plot and a slope different than y = x for the observed vs fitted plot (as shown bellow). Which made me question the validity of the model for inference.
>> >
>> > Could I still use this model for inference? Should I specify a different formula? Should I turn to lme and try to include different variances for each level of conditions (C)? Any ideas?
>> >
>> > I would be really appreciated if anyone could help me with this.
>> >
>> > Thanks in advance,
>> > Carlos Fam?lia
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org> mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org> mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
> 
> 


	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Mon Oct  3 15:19:26 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Mon, 3 Oct 2016 13:19:26 +0000
Subject: [R-sig-ME] Model diagnostics show slope in residuals plot and
 slope on the observed vs fitted plot is different than y = x
In-Reply-To: <C7A4790F-059C-48FD-AE45-DDB8187EDC38@gmail.com>
References: <9A728328-DD65-40D0-B4F5-E3F0FB0FEE76@gmail.com>
	<4FA6973B-B635-4EF1-B1AA-FE1138113166@glasgow.ac.uk>
	<CAJuCY5xR0jrDhPNFyXyE7R+7TxxFURTDK-Lv+ZfxemN5+LR-Mw@mail.gmail.com>
	<D16EB223-D5C3-43D9-B519-D566893845D2@gmail.com>
	<CAJuCY5wsaozm44TurnHBU6v6OMN-HzK_R4EB-eRzQGdkHh2dLw@mail.gmail.com>
	<C7A4790F-059C-48FD-AE45-DDB8187EDC38@gmail.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC836581B89@FHSDB2D11-2.csu.mcmaster.ca>

Dear Carlos,

If I understand properly what's troubling you and the nature of the suggestion, I wouldn't do it -- that is, randomly discard data to get one observation per case. 

Paul Johnson explained clearly why the pattern you noticed in the residuals vs. fitted values plot occurred, as a consequence of shrinkage. One way of thinking about this is that using a mixed model is *more* important when you have few observations per case, where shrinkage will be greater, than when you have many observations per case, where the "estimates" of the random effects will nearly coincide with the case means (or in a more complex model, within-case regressions).

Best,
 John

-----------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario
Canada L8S 4M4
Web: socserv.mcmaster.ca/jfox



> -----Original Message-----
> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org]
> On Behalf Of Carlos Familia
> Sent: October 3, 2016 8:20 AM
> To: Thierry Onkelinx <thierry.onkelinx at inbo.be>
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Model diagnostics show slope in residuals plot and
> slope on the observed vs fitted plot is different than y = x
> 
> Do you think this approach is sound and easily justifiable in a paper?
> 
> Many thanks,
> Carlos
> > On 3 Oct 2016, at 13:10, Thierry Onkelinx <thierry.onkelinx at inbo.be> wrote:
> >
> > In case you have more than one measurement per ID, select one of them
> > at random. Something like
> >
> > library(dplyr)
> > df %>%
> > group_by(id) %>%
> > sample_n(1)
> >
> >
> >
> > ir. Thierry Onkelinx
> > Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> > and Forest team Biometrie & Kwaliteitszorg / team Biometrics & Quality
> > Assurance Kliniekstraat 25
> > 1070 Anderlecht
> > Belgium
> >
> > To call in the statistician after the experiment is done may be no
> > more than asking him to perform a post-mortem examination: he may be
> > able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> > The plural of anecdote is not data. ~ Roger Brinner The combination of
> > some data and an aching desire for an answer does not ensure that a
> > reasonable answer can be extracted from a given body of data. ~ John
> > Tukey
> >
> > 2016-10-03 14:06 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com
> <mailto:carlosfamilia at gmail.com>>:
> > Dear Thierry,
> >
> > When you say sampling the data to have only one observation per ID, you
> mean reducing the dataset to cases where the Y variable was only measured in
> 1 condition?
> > I have though about going lm  with this, but it just didn?t feel wright...
> >
> > Many thanks,
> > Carlos
> >
> >> On 3 Oct 2016, at 13:02, Thierry Onkelinx <thierry.onkelinx at inbo.be
> <mailto:thierry.onkelinx at inbo.be>> wrote:
> >>
> >> Dear Carlos,
> >>
> >> I concur with Paul. After play a bit with the data you send me privately, I see
> a few things which cause problems:
> >> 1) the number of measurements per ID is low. 1/3 has one measurement in
> each level of C, 1/3 in two out of three levels of C and 1/3 in only one level of C.
> >> 2) the variance of ID is larger than the residual variance
> >> 3) the effect of W is small compared to the variance of ID
> >>
> >> If possible try to add extra covariates. If not I'd fall back on a simple lm.
> Either with ignoring the repeated measurements or by sampling the data so
> you have only one observation per ID.
> >>
> >> Best regards,
> >>
> >>
> >> ir. Thierry Onkelinx
> >> Instituut voor natuur- en bosonderzoek / Research Institute for
> >> Nature and Forest team Biometrie & Kwaliteitszorg / team Biometrics &
> >> Quality Assurance Kliniekstraat 25
> >> 1070 Anderlecht
> >> Belgium
> >>
> >> To call in the statistician after the experiment is done may be no
> >> more than asking him to perform a post-mortem examination: he may be
> >> able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> >> The plural of anecdote is not data. ~ Roger Brinner The combination
> >> of some data and an aching desire for an answer does not ensure that
> >> a reasonable answer can be extracted from a given body of data. ~
> >> John Tukey
> >>
> >> 2016-10-03 11:17 GMT+02:00 Paul Johnson <paul.johnson at glasgow.ac.uk
> <mailto:paul.johnson at glasgow.ac.uk>>:
> >> Hi Carlos,
> >>
> >> Your plot didn?t come through, as Thierry noted. However it?s expected that,
> unlike a standard linear regression model, an LMM with a nested structure
> such as yours will give a positive linear relationship between the residuals and
> the fitted values (might also be true for other structures?), provided the
> residual and random effect variances are > 0. Somebody will hopefully chip in
> with a formal explanation, but it?s basically a similar phenomenon to
> regression to the mean.
> >>
> >> Imagine a group of students taking each taking an aptitude test three times.
> There are two random factors: the difference in underlying aptitude between
> the students, modelled by the student ID random effect; and random variation
> between time points within each student (e.g. how good a particular student is
> feeling on a particular day). I?m ignoring variation between tests ? let?s
> unrealistically assume they?s all the same and students completely forget about
> them between tests.
> >>
> >> The students with the best mean scores will be a mixture of excellent
> students having three so-so (some good, some bad) days, and moderately good
> students having the good luck to have three good days, and the very best
> scores will come from students who were both excellent and lucky (although
> this category will be small). An important point is that there is no way of using
> the data to separate the moderate-student-lucky-days high scores from the
> excellent-student-average-days scores. If we simply took the mean of the
> scores, we would be overestimating the performance of the students on
> average (we?d have good estimates of the excellent students and
> overestimates of the moderate ones), so the best estimate is achieved by
> shrinking the scores towards the mean.
> >>
> >> This is what happens when the model is fitted. Each student is given a
> residual (random effect) at the student level (how good the student is relative
> to the value predicted by the fixed effects) and three residuals at the
> observation (between-test-within-student) level. For students with good
> scores, this will be a compromise between the inseparable excellent-student-
> average-days scenario and the moderate-student-lucky-days scenario. As a
> result, students with high student-level residuals (the student random effects)
> will also tend to have high inter-test residuals. The same is also true in negative
> for poor students and students having three bad days. So the student random
> effects (which are part of the fitted values) and the residuals will be positively
> correlated.
> >>
> >> You can check this using by simulating new data from the fitted model re-
> fitting the model, and comparing the residuals-x-fitted plot (which will be
> "perfect?) to the one from your data. Here?s a function that does this for lme4
> fits:
> >>
> >> devtools::install_github("pcdjohnson/GLMMmisc")
> >> library(GLMMmisc)
> >> sim.residplot(fit) # repeat this a few times to account for sampling
> >> error
> >>
> >> If all is well, you should see a similar slope between the real and the
> simulated plots, in fact the general pattern of the residuals should be similar.
> >>
> >> (The new package DHARMa ?
> >> https://theoreticalecology.wordpress.com/2016/08/28/dharma-an-r-packa
> >> ge-for-residual-diagnostics-of-glmms/
> >> <https://theoreticalecology.wordpress.com/2016/08/28/dharma-an-r-pack
> >> age-for-residual-diagnostics-of-glmms/> ? takes a similar approach to
> >> assessing residuals, but in a less quick-and-dirty, more formally
> >> justified way.)
> >>
> >> All the best,
> >> Paul
> >>
> >>
> >>
> >>
> >> > On 2 Oct 2016, at 16:57, Carlos Familia <carlosfamilia at gmail.com
> <mailto:carlosfamilia at gmail.com>> wrote:
> >> >
> >> > Hello,
> >> >
> >> > I have in hands a quite large and unbalanced dataset, for which a Y
> continuous dependent variable was measured in 3 different conditions (C) for
> about 3000 subjects (ID) (although, not all subjects have Y values for the 3
> conditions). Additionally, there is continuous measure W which was measured
> for all subjects.
> >> >
> >> > I am interested in testing the following:
> >> >
> >> > - Is the effect of W significant overall
> >> > - Is the effect of W significant at each level of C
> >> > - Is the effect of C significant
> >> >
> >> > In order to try to answer this, I have specified the following model with
> lmer:
> >> >
> >> > lmer( Y ~ W * C + (1 | ID), data = df)
> >> >
> >> > Which seems to proper reflect the structure of the data (I might be wrong
> here, any suggestions would be welcome).
> >> > However when running the diagnostic plots I noticed a slope in the
> residuals plot and a slope different than y = x for the observed vs fitted plot (as
> shown bellow). Which made me question the validity of the model for
> inference.
> >> >
> >> > Could I still use this model for inference? Should I specify a different
> formula? Should I turn to lme and try to include different variances for each
> level of conditions (C)? Any ideas?
> >> >
> >> > I would be really appreciated if anyone could help me with this.
> >> >
> >> > Thanks in advance,
> >> > Carlos Fam?lia
> >> >
> >> > _______________________________________________
> >> > R-sig-mixed-models at r-project.org
> >> > <mailto:R-sig-mixed-models at r-project.org> mailing list
> >> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >> > <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
> >>
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org
> >> <mailto:R-sig-mixed-models at r-project.org> mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >> <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
> >
> >
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From carlosfamilia at gmail.com  Mon Oct  3 20:03:07 2016
From: carlosfamilia at gmail.com (Carlos Familia)
Date: Mon, 3 Oct 2016 19:03:07 +0100
Subject: [R-sig-ME] Model diagnostics show slope in residuals plot and
 slope on the observed vs fitted plot is different than y = x
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC836581B89@FHSDB2D11-2.csu.mcmaster.ca>
References: <9A728328-DD65-40D0-B4F5-E3F0FB0FEE76@gmail.com>
	<4FA6973B-B635-4EF1-B1AA-FE1138113166@glasgow.ac.uk>
	<CAJuCY5xR0jrDhPNFyXyE7R+7TxxFURTDK-Lv+ZfxemN5+LR-Mw@mail.gmail.com>
	<D16EB223-D5C3-43D9-B519-D566893845D2@gmail.com>
	<CAJuCY5wsaozm44TurnHBU6v6OMN-HzK_R4EB-eRzQGdkHh2dLw@mail.gmail.com>
	<C7A4790F-059C-48FD-AE45-DDB8187EDC38@gmail.com>
	<ACD1644AA6C67E4FBD0C350625508EC836581B89@FHSDB2D11-2.csu.mcmaster.ca>
Message-ID: <9DF92740-074D-4C75-A679-2855668201DF@gmail.com>

Dear John,
 
Do you suggest then that I should get a different linear regression model per condition, and instead of analysing the overall effect of W across conditions, analyse it for each condition separately? 
Do you think this would be fine for a referee? 
Please note that I have no problem with this.


Many thanks,
Carlos Fam?lia


> On 3 Oct 2016, at 14:19, Fox, John <jfox at mcmaster.ca> wrote:
> 
> Dear Carlos,
> 
> If I understand properly what's troubling you and the nature of the suggestion, I wouldn't do it -- that is, randomly discard data to get one observation per case. 
> 
> Paul Johnson explained clearly why the pattern you noticed in the residuals vs. fitted values plot occurred, as a consequence of shrinkage. One way of thinking about this is that using a mixed model is *more* important when you have few observations per case, where shrinkage will be greater, than when you have many observations per case, where the "estimates" of the random effects will nearly coincide with the case means (or in a more complex model, within-case regressions).
> 
> Best,
> John
> 
> -----------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> Web: socserv.mcmaster.ca/jfox <http://socserv.mcmaster.ca/jfox>
> 
> 
> 
>> -----Original Message-----
>> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org <mailto:r-sig-mixed-models-bounces at r-project.org>]
>> On Behalf Of Carlos Familia
>> Sent: October 3, 2016 8:20 AM
>> To: Thierry Onkelinx <thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>>
>> Cc: r-sig-mixed-models at r-project.org <mailto:r-sig-mixed-models at r-project.org>
>> Subject: Re: [R-sig-ME] Model diagnostics show slope in residuals plot and
>> slope on the observed vs fitted plot is different than y = x
>> 
>> Do you think this approach is sound and easily justifiable in a paper?
>> 
>> Many thanks,
>> Carlos
>>> On 3 Oct 2016, at 13:10, Thierry Onkelinx <thierry.onkelinx at inbo.be> wrote:
>>> 
>>> In case you have more than one measurement per ID, select one of them
>>> at random. Something like
>>> 
>>> library(dplyr)
>>> df %>%
>>> group_by(id) %>%
>>> sample_n(1)
>>> 
>>> 
>>> 
>>> ir. Thierry Onkelinx
>>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>>> and Forest team Biometrie & Kwaliteitszorg / team Biometrics & Quality
>>> Assurance Kliniekstraat 25
>>> 1070 Anderlecht
>>> Belgium
>>> 
>>> To call in the statistician after the experiment is done may be no
>>> more than asking him to perform a post-mortem examination: he may be
>>> able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> The plural of anecdote is not data. ~ Roger Brinner The combination of
>>> some data and an aching desire for an answer does not ensure that a
>>> reasonable answer can be extracted from a given body of data. ~ John
>>> Tukey
>>> 
>>> 2016-10-03 14:06 GMT+02:00 Carlos Familia <carlosfamilia at gmail.com
>> <mailto:carlosfamilia at gmail.com <mailto:carlosfamilia at gmail.com>>>:
>>> Dear Thierry,
>>> 
>>> When you say sampling the data to have only one observation per ID, you
>> mean reducing the dataset to cases where the Y variable was only measured in
>> 1 condition?
>>> I have though about going lm  with this, but it just didn?t feel wright...
>>> 
>>> Many thanks,
>>> Carlos
>>> 
>>>> On 3 Oct 2016, at 13:02, Thierry Onkelinx <thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>
>> <mailto:thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>>> wrote:
>>>> 
>>>> Dear Carlos,
>>>> 
>>>> I concur with Paul. After play a bit with the data you send me privately, I see
>> a few things which cause problems:
>>>> 1) the number of measurements per ID is low. 1/3 has one measurement in
>> each level of C, 1/3 in two out of three levels of C and 1/3 in only one level of C.
>>>> 2) the variance of ID is larger than the residual variance
>>>> 3) the effect of W is small compared to the variance of ID
>>>> 
>>>> If possible try to add extra covariates. If not I'd fall back on a simple lm.
>> Either with ignoring the repeated measurements or by sampling the data so
>> you have only one observation per ID.
>>>> 
>>>> Best regards,
>>>> 
>>>> 
>>>> ir. Thierry Onkelinx
>>>> Instituut voor natuur- en bosonderzoek / Research Institute for
>>>> Nature and Forest team Biometrie & Kwaliteitszorg / team Biometrics &
>>>> Quality Assurance Kliniekstraat 25
>>>> 1070 Anderlecht
>>>> Belgium
>>>> 
>>>> To call in the statistician after the experiment is done may be no
>>>> more than asking him to perform a post-mortem examination: he may be
>>>> able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>>> The plural of anecdote is not data. ~ Roger Brinner The combination
>>>> of some data and an aching desire for an answer does not ensure that
>>>> a reasonable answer can be extracted from a given body of data. ~
>>>> John Tukey
>>>> 
>>>> 2016-10-03 11:17 GMT+02:00 Paul Johnson <paul.johnson at glasgow.ac.uk <mailto:paul.johnson at glasgow.ac.uk>
>> <mailto:paul.johnson at glasgow.ac.uk <mailto:paul.johnson at glasgow.ac.uk>>>:
>>>> Hi Carlos,
>>>> 
>>>> Your plot didn?t come through, as Thierry noted. However it?s expected that,
>> unlike a standard linear regression model, an LMM with a nested structure
>> such as yours will give a positive linear relationship between the residuals and
>> the fitted values (might also be true for other structures?), provided the
>> residual and random effect variances are > 0. Somebody will hopefully chip in
>> with a formal explanation, but it?s basically a similar phenomenon to
>> regression to the mean.
>>>> 
>>>> Imagine a group of students taking each taking an aptitude test three times.
>> There are two random factors: the difference in underlying aptitude between
>> the students, modelled by the student ID random effect; and random variation
>> between time points within each student (e.g. how good a particular student is
>> feeling on a particular day). I?m ignoring variation between tests ? let?s
>> unrealistically assume they?s all the same and students completely forget about
>> them between tests.
>>>> 
>>>> The students with the best mean scores will be a mixture of excellent
>> students having three so-so (some good, some bad) days, and moderately good
>> students having the good luck to have three good days, and the very best
>> scores will come from students who were both excellent and lucky (although
>> this category will be small). An important point is that there is no way of using
>> the data to separate the moderate-student-lucky-days high scores from the
>> excellent-student-average-days scores. If we simply took the mean of the
>> scores, we would be overestimating the performance of the students on
>> average (we?d have good estimates of the excellent students and
>> overestimates of the moderate ones), so the best estimate is achieved by
>> shrinking the scores towards the mean.
>>>> 
>>>> This is what happens when the model is fitted. Each student is given a
>> residual (random effect) at the student level (how good the student is relative
>> to the value predicted by the fixed effects) and three residuals at the
>> observation (between-test-within-student) level. For students with good
>> scores, this will be a compromise between the inseparable excellent-student-
>> average-days scenario and the moderate-student-lucky-days scenario. As a
>> result, students with high student-level residuals (the student random effects)
>> will also tend to have high inter-test residuals. The same is also true in negative
>> for poor students and students having three bad days. So the student random
>> effects (which are part of the fitted values) and the residuals will be positively
>> correlated.
>>>> 
>>>> You can check this using by simulating new data from the fitted model re-
>> fitting the model, and comparing the residuals-x-fitted plot (which will be
>> "perfect?) to the one from your data. Here?s a function that does this for lme4
>> fits:
>>>> 
>>>> devtools::install_github("pcdjohnson/GLMMmisc")
>>>> library(GLMMmisc)
>>>> sim.residplot(fit) # repeat this a few times to account for sampling
>>>> error
>>>> 
>>>> If all is well, you should see a similar slope between the real and the
>> simulated plots, in fact the general pattern of the residuals should be similar.
>>>> 
>>>> (The new package DHARMa ?
>>>> https://theoreticalecology.wordpress.com/2016/08/28/dharma-an-r-packa <https://theoreticalecology.wordpress.com/2016/08/28/dharma-an-r-packa>
>>>> ge-for-residual-diagnostics-of-glmms/
>>>> <https://theoreticalecology.wordpress.com/2016/08/28/dharma-an-r-pack <https://theoreticalecology.wordpress.com/2016/08/28/dharma-an-r-pack>
>>>> age-for-residual-diagnostics-of-glmms/> ? takes a similar approach to
>>>> assessing residuals, but in a less quick-and-dirty, more formally
>>>> justified way.)
>>>> 
>>>> All the best,
>>>> Paul
>>>> 
>>>> 
>>>> 
>>>> 
>>>>> On 2 Oct 2016, at 16:57, Carlos Familia <carlosfamilia at gmail.com <mailto:carlosfamilia at gmail.com>
>> <mailto:carlosfamilia at gmail.com <mailto:carlosfamilia at gmail.com>>> wrote:
>>>>> 
>>>>> Hello,
>>>>> 
>>>>> I have in hands a quite large and unbalanced dataset, for which a Y
>> continuous dependent variable was measured in 3 different conditions (C) for
>> about 3000 subjects (ID) (although, not all subjects have Y values for the 3
>> conditions). Additionally, there is continuous measure W which was measured
>> for all subjects.
>>>>> 
>>>>> I am interested in testing the following:
>>>>> 
>>>>> - Is the effect of W significant overall
>>>>> - Is the effect of W significant at each level of C
>>>>> - Is the effect of C significant
>>>>> 
>>>>> In order to try to answer this, I have specified the following model with
>> lmer:
>>>>> 
>>>>> lmer( Y ~ W * C + (1 | ID), data = df)
>>>>> 
>>>>> Which seems to proper reflect the structure of the data (I might be wrong
>> here, any suggestions would be welcome).
>>>>> However when running the diagnostic plots I noticed a slope in the
>> residuals plot and a slope different than y = x for the observed vs fitted plot (as
>> shown bellow). Which made me question the validity of the model for
>> inference.
>>>>> 
>>>>> Could I still use this model for inference? Should I specify a different
>> formula? Should I turn to lme and try to include different variances for each
>> level of conditions (C)? Any ideas?
>>>>> 
>>>>> I would be really appreciated if anyone could help me with this.
>>>>> 
>>>>> Thanks in advance,
>>>>> Carlos Fam?lia
>>>>> 
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org>
>>>>> <mailto:R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org>> mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
>>>>> <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
>>>> 
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org>
>>>> <mailto:R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org>> mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
>>>> <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>>
>>> 
>>> 
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org> mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>

	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Mon Oct  3 20:21:47 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Mon, 3 Oct 2016 18:21:47 +0000
Subject: [R-sig-ME] Model diagnostics show slope in residuals plot and
 slope on the observed vs fitted plot is different than y = x
In-Reply-To: <9DF92740-074D-4C75-A679-2855668201DF@gmail.com>
References: <9A728328-DD65-40D0-B4F5-E3F0FB0FEE76@gmail.com>
	<4FA6973B-B635-4EF1-B1AA-FE1138113166@glasgow.ac.uk>
	<CAJuCY5xR0jrDhPNFyXyE7R+7TxxFURTDK-Lv+ZfxemN5+LR-Mw@mail.gmail.com>
	<D16EB223-D5C3-43D9-B519-D566893845D2@gmail.com>
	<CAJuCY5wsaozm44TurnHBU6v6OMN-HzK_R4EB-eRzQGdkHh2dLw@mail.gmail.com>
	<C7A4790F-059C-48FD-AE45-DDB8187EDC38@gmail.com>
	<ACD1644AA6C67E4FBD0C350625508EC836581B89@FHSDB2D11-2.csu.mcmaster.ca>
	<9DF92740-074D-4C75-A679-2855668201DF@gmail.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC836581CAB@FHSDB2D11-2.csu.mcmaster.ca>

Dear Carlos,

> -----Original Message-----
> From: Carlos Familia [mailto:carlosfamilia at gmail.com]
> Sent: Monday, October 3, 2016 2:03 PM
> To: Fox, John <jfox at mcmaster.ca>
> Cc: r-sig-mixed-models at r-project.org; Thierry Onkelinx
> <thierry.onkelinx at inbo.be>
> Subject: Re: [R-sig-ME] Model diagnostics show slope in residuals plot
> and slope on the observed vs fitted plot is different than y = x
> 
> Dear John,
> 
> Do you suggest then that I should get a different linear regression
> model per condition, and instead of analysing the overall effect of W
> across conditions, analyse it for each condition separately?

No, I'm not recommending that, or anything in particular beyond the observation that randomly discarding data in order to fit a fixed effect model to the remaining independent observations is probably not a good idea. I also don't see what's to be gained by fitting separate models to the conditions; if you expect interactions with condition, why not model them? I don't think that it would be responsible for me to give you statistical advice by email knowing next to nothing about your research.

> Do you think this would be fine for a referee?

I have no idea. I probably wouldn't feel that I could predict an unknown referee's response to your work even if I were in a position to recommend what you should do.

I'm sorry that I can't be more helpful.

Best,
 John

> Please note that I have no problem with this.
> 
> 
> Many thanks,
> Carlos Fam?lia
> 
> 
> 
> 	On 3 Oct 2016, at 14:19, Fox, John <jfox at mcmaster.ca
> <mailto:jfox at mcmaster.ca> > wrote:
> 
> 	Dear Carlos,
> 
> 	If I understand properly what's troubling you and the nature of the
> suggestion, I wouldn't do it -- that is, randomly discard data to get
> one observation per case.
> 
> 	Paul Johnson explained clearly why the pattern you noticed in the
> residuals vs. fitted values plot occurred, as a consequence of
> shrinkage. One way of thinking about this is that using a mixed model is
> *more* important when you have few observations per case, where
> shrinkage will be greater, than when you have many observations per
> case, where the "estimates" of the random effects will nearly coincide
> with the case means (or in a more complex model, within-case
> regressions).
> 
> 	Best,
> 	John
> 
> 	-----------------------------
> 	John Fox, Professor
> 	McMaster University
> 	Hamilton, Ontario
> 	Canada L8S 4M4
> 	Web: socserv.mcmaster.ca/jfox <http://socserv.mcmaster.ca/jfox>
> 
> 
> 
> 
> 
> 		-----Original Message-----
> 		From: R-sig-mixed-models [mailto:r-sig-mixed-models-
> bounces at r-project.org]
> 		On Behalf Of Carlos Familia
> 		Sent: October 3, 2016 8:20 AM
> 		To: Thierry Onkelinx <thierry.onkelinx at inbo.be
> <mailto:thierry.onkelinx at inbo.be> >
> 		Cc: r-sig-mixed-models at r-project.org <mailto:r-sig-mixed-
> models at r-project.org>
> 		Subject: Re: [R-sig-ME] Model diagnostics show slope in
> residuals plot and
> 		slope on the observed vs fitted plot is different than y = x
> 
> 		Do you think this approach is sound and easily justifiable in
> a paper?
> 
> 		Many thanks,
> 		Carlos
> 
> 
> 			On 3 Oct 2016, at 13:10, Thierry Onkelinx
> <thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be> > wrote:
> 
> 			In case you have more than one measurement per ID,
> select one of them
> 			at random. Something like
> 
> 			library(dplyr)
> 			df %>%
> 			group_by(id) %>%
> 			sample_n(1)
> 
> 
> 
> 			ir. Thierry Onkelinx
> 			Instituut voor natuur- en bosonderzoek / Research
> Institute for Nature
> 			and Forest team Biometrie & Kwaliteitszorg / team
> Biometrics & Quality
> 			Assurance Kliniekstraat 25
> 			1070 Anderlecht
> 			Belgium
> 
> 			To call in the statistician after the experiment is done
> may be no
> 			more than asking him to perform a post-mortem
> examination: he may be
> 			able to say what the experiment died of. ~ Sir Ronald
> Aylmer Fisher
> 			The plural of anecdote is not data. ~ Roger Brinner The
> combination of
> 			some data and an aching desire for an answer does not
> ensure that a
> 			reasonable answer can be extracted from a given body of
> data. ~ John
> 			Tukey
> 
> 			2016-10-03 14:06 GMT+02:00 Carlos Familia
> <carlosfamilia at gmail.com <mailto:carlosfamilia at gmail.com>
> 
> 
> 		<mailto:carlosfamilia at gmail.com>>:
> 
> 
> 			Dear Thierry,
> 
> 			When you say sampling the data to have only one
> observation per ID, you
> 
> 
> 		mean reducing the dataset to cases where the Y variable was
> only measured in
> 		1 condition?
> 
> 
> 			I have though about going lm  with this, but it just
> didn?t feel wright...
> 
> 			Many thanks,
> 			Carlos
> 
> 
> 
> 				On 3 Oct 2016, at 13:02, Thierry Onkelinx
> <thierry.onkelinx at inbo.be <mailto:thierry.onkelinx at inbo.be>
> 
> 
> 		<mailto:thierry.onkelinx at inbo.be>> wrote:
> 
> 
> 
> 				Dear Carlos,
> 
> 				I concur with Paul. After play a bit with the data
> you send me privately, I see
> 
> 
> 		a few things which cause problems:
> 
> 
> 				1) the number of measurements per ID is low. 1/3
> has one measurement in
> 
> 
> 		each level of C, 1/3 in two out of three levels of C and 1/3
> in only one level of C.
> 
> 
> 				2) the variance of ID is larger than the residual
> variance
> 				3) the effect of W is small compared to the
> variance of ID
> 
> 				If possible try to add extra covariates. If not I'd
> fall back on a simple lm.
> 
> 
> 		Either with ignoring the repeated measurements or by sampling
> the data so
> 		you have only one observation per ID.
> 
> 
> 
> 				Best regards,
> 
> 
> 				ir. Thierry Onkelinx
> 				Instituut voor natuur- en bosonderzoek / Research
> Institute for
> 				Nature and Forest team Biometrie & Kwaliteitszorg /
> team Biometrics &
> 				Quality Assurance Kliniekstraat 25
> 				1070 Anderlecht
> 				Belgium
> 
> 				To call in the statistician after the experiment is
> done may be no
> 				more than asking him to perform a post-mortem
> examination: he may be
> 				able to say what the experiment died of. ~ Sir
> Ronald Aylmer Fisher
> 				The plural of anecdote is not data. ~ Roger Brinner
> The combination
> 				of some data and an aching desire for an answer
> does not ensure that
> 				a reasonable answer can be extracted from a given
> body of data. ~
> 				John Tukey
> 
> 				2016-10-03 11:17 GMT+02:00 Paul Johnson
> <paul.johnson at glasgow.ac.uk <mailto:paul.johnson at glasgow.ac.uk>
> 
> 
> 		<mailto:paul.johnson at glasgow.ac.uk>>:
> 
> 
> 				Hi Carlos,
> 
> 				Your plot didn?t come through, as Thierry noted.
> However it?s expected that,
> 
> 
> 		unlike a standard linear regression model, an LMM with a
> nested structure
> 		such as yours will give a positive linear relationship
> between the residuals and
> 		the fitted values (might also be true for other structures?),
> provided the
> 		residual and random effect variances are > 0. Somebody will
> hopefully chip in
> 		with a formal explanation, but it?s basically a similar
> phenomenon to
> 		regression to the mean.
> 
> 
> 
> 				Imagine a group of students taking each taking an
> aptitude test three times.
> 
> 
> 		There are two random factors: the difference in underlying
> aptitude between
> 		the students, modelled by the student ID random effect; and
> random variation
> 		between time points within each student (e.g. how good a
> particular student is
> 		feeling on a particular day). I?m ignoring variation between
> tests ? let?s
> 		unrealistically assume they?s all the same and students
> completely forget about
> 		them between tests.
> 
> 
> 
> 				The students with the best mean scores will be a
> mixture of excellent
> 
> 
> 		students having three so-so (some good, some bad) days, and
> moderately good
> 		students having the good luck to have three good days, and
> the very best
> 		scores will come from students who were both excellent and
> lucky (although
> 		this category will be small). An important point is that
> there is no way of using
> 		the data to separate the moderate-student-lucky-days high
> scores from the
> 		excellent-student-average-days scores. If we simply took the
> mean of the
> 		scores, we would be overestimating the performance of the
> students on
> 		average (we?d have good estimates of the excellent students
> and
> 		overestimates of the moderate ones), so the best estimate is
> achieved by
> 		shrinking the scores towards the mean.
> 
> 
> 
> 				This is what happens when the model is fitted. Each
> student is given a
> 
> 
> 		residual (random effect) at the student level (how good the
> student is relative
> 		to the value predicted by the fixed effects) and three
> residuals at the
> 		observation (between-test-within-student) level. For students
> with good
> 		scores, this will be a compromise between the inseparable
> excellent-student-
> 		average-days scenario and the moderate-student-lucky-days
> scenario. As a
> 		result, students with high student-level residuals (the
> student random effects)
> 		will also tend to have high inter-test residuals. The same is
> also true in negative
> 		for poor students and students having three bad days. So the
> student random
> 		effects (which are part of the fitted values) and the
> residuals will be positively
> 		correlated.
> 
> 
> 
> 				You can check this using by simulating new data
> from the fitted model re-
> 
> 
> 		fitting the model, and comparing the residuals-x-fitted plot
> (which will be
> 		"perfect?) to the one from your data. Here?s a function that
> does this for lme4
> 		fits:
> 
> 
> 
> 				devtools::install_github("pcdjohnson/GLMMmisc")
> 				library(GLMMmisc)
> 				sim.residplot(fit) # repeat this a few times to
> account for sampling
> 				error
> 
> 				If all is well, you should see a similar slope
> between the real and the
> 
> 
> 		simulated plots, in fact the general pattern of the residuals
> should be similar.
> 
> 
> 
> 				(The new package DHARMa ?
> 
> 	https://theoreticalecology.wordpress.com/2016/08/28/dharma-an-r-
> packa
> 				ge-for-residual-diagnostics-of-glmms/
> 
> 	<https://theoreticalecology.wordpress.com/2016/08/28/dharma-an-r-
> pack
> 				age-for-residual-diagnostics-of-glmms/> ? takes a
> similar approach to
> 				assessing residuals, but in a less quick-and-dirty,
> more formally
> 				justified way.)
> 
> 				All the best,
> 				Paul
> 
> 
> 
> 
> 
> 
> 					On 2 Oct 2016, at 16:57, Carlos Familia
> <carlosfamilia at gmail.com <mailto:carlosfamilia at gmail.com>
> 
> 
> 		<mailto:carlosfamilia at gmail.com>> wrote:
> 
> 
> 
> 					Hello,
> 
> 					I have in hands a quite large and unbalanced
> dataset, for which a Y
> 
> 
> 		continuous dependent variable was measured in 3 different
> conditions (C) for
> 		about 3000 subjects (ID) (although, not all subjects have Y
> values for the 3
> 		conditions). Additionally, there is continuous measure W
> which was measured
> 		for all subjects.
> 
> 
> 
> 					I am interested in testing the following:
> 
> 					- Is the effect of W significant overall
> 					- Is the effect of W significant at each
> level of C
> 					- Is the effect of C significant
> 
> 					In order to try to answer this, I have
> specified the following model with
> 
> 
> 		lmer:
> 
> 
> 
> 					lmer( Y ~ W * C + (1 | ID), data = df)
> 
> 					Which seems to proper reflect the structure
> of the data (I might be wrong
> 
> 
> 		here, any suggestions would be welcome).
> 
> 
> 					However when running the diagnostic plots I
> noticed a slope in the
> 
> 
> 		residuals plot and a slope different than y = x for the
> observed vs fitted plot (as
> 		shown bellow). Which made me question the validity of the
> model for
> 		inference.
> 
> 
> 
> 					Could I still use this model for inference?
> Should I specify a different
> 
> 
> 		formula? Should I turn to lme and try to include different
> variances for each
> 		level of conditions (C)? Any ideas?
> 
> 
> 
> 					I would be really appreciated if anyone could
> help me with this.
> 
> 					Thanks in advance,
> 					Carlos Fam?lia
> 
> 
> 	_______________________________________________
> 					R-sig-mixed-models at r-project.org <mailto:R-
> sig-mixed-models at r-project.org>
> 					<mailto:R-sig-mixed-models at r-project.org>
> mailing list
> 					https://stat.ethz.ch/mailman/listinfo/r-sig-
> mixed-models
> 					<https://stat.ethz.ch/mailman/listinfo/r-sig-
> mixed-models>
> 
> 
> 
> 				_______________________________________________
> 				R-sig-mixed-models at r-project.org <mailto:R-sig-
> mixed-models at r-project.org>
> 				<mailto:R-sig-mixed-models at r-project.org> mailing
> list
> 				https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-
> models
> 				<https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-
> models>
> 
> 
> 
> 
> 
> 
> 		[[alternative HTML version deleted]]
> 
> 		_______________________________________________
> 		R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-
> models at r-project.org>  mailing list
> 		https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


From nicolas.o.rode at gmail.com  Tue Oct  4 10:15:57 2016
From: nicolas.o.rode at gmail.com (Nicolas Rode)
Date: Tue, 4 Oct 2016 10:15:57 +0200
Subject: [R-sig-ME] fit correlation between two random effects
Message-ID: <CAAnfhNh8NashHDyWAy8RvZSHVjfK8wUkJpwEnyYUZzCRzJkOsg@mail.gmail.com>

Dear all,

I am trying to fit the correlation between two random effects (intercepts
sampled from a bivariate normal distribution).
I was wondering if there would be a way to do this in lmer?

Thanks,

Nicolas

Here is a small example with the correlation I am tryign to estimate:

library(lme4)
library(MASS)


numStrain <- 10
data <- expand.grid(Strain=1:numStrain,NeighStrain=1:numStrain,rep=1:3)
data$Strain <- as.factor(data$Strain)
data$NeighStrain <- as.factor(data$NeighStrain)

sig1 <- 5
sig2 <- 2
rho <- 0.7
Sigma <- matrix(c(sig1^2, rho*sig1*sig2, rho*sig1* sig2, sig2^2),2,2)

## Group level random intercept for the focal strain and its effect as a
neighbour
BV <- mvrnorm(n=numStrain,mu=rep(0,2),Sigma)

Fi <- BV[,1]
names(Fi) <- levels(data$Strain)

Nj <- BV[,2]
names(Nj) <- levels(data$Strain)

## Creating a matrix with observations as rows and strain identity as
columns (300 observations x 41 strains)
L1 <- model.matrix(~-1+Strain,data)
dim(L1)
data$U_j1 <- L1 %*% Fi # strain genotypic value


## Creating a matrix with observations as rows and neighbour strain
identity as columns (240 observations x 41 strains)
L2 <- model.matrix(~-1+NeighStrain,data)
dim(L2)
data$U_j2 <- L2 %*% Nj # neighbour strain genotypic value

## Response variable with the two random effects and a random residual error
data$y <-  data$U_j1+data$U_j2+rnorm(nrow(data))

m1 <- lmer(y ~ 1 + (1 | Strain)+ (1 | NeighStrain), data)
summary(m1)

## Correlation I would like to fit directly in lmer
cor.test(unlist(ranef(m1)$Strain),unlist(ranef(m1)$NeighStrain))

	[[alternative HTML version deleted]]


From msheehan at cornell.edu  Tue Oct  4 18:00:44 2016
From: msheehan at cornell.edu (Michael Sheehan)
Date: Tue, 4 Oct 2016 16:00:44 +0000
Subject: [R-sig-ME] Priors for MCMCglmm with haplodiploid relatedness matrix
In-Reply-To: <CY1PR04MB2187960A0720219040819B92C4C50@CY1PR04MB2187.namprd04.prod.outlook.com>
References: <CY1PR04MB2187960A0720219040819B92C4C50@CY1PR04MB2187.namprd04.prod.outlook.com>
Message-ID: <CY1PR04MB21870F25433BF01FD115597BC4C50@CY1PR04MB2187.namprd04.prod.outlook.com>


Dear all,


I am attempting to examine the heritability of a number of traits in a haplodiplod species using MCMCglmm. This has presented a number of challenges since the standard relatedness matrix calculated from a pedigree as part of the program assumes diploid relatedness, which underestimates relatedness among sisters. I have been able to make a haplodiploid appropriate relatedness matrix but when I run the following:


model1=MCMCglmm(trait~1, random=~animal, ginverse=list(animal=haplo.mat.inverse), data=data, family="ordinal", prior=prior1, nitt=105000, thin=100, burnin=5000, verbose=T)


I immediately get the following error:


"G-structure 1 is ill-conditioned (possibly because of ginverse): use proper priors if you haven't, or rescale data if you have"


If I run the same code with a diploid relatedness matrix (calling it through pedigree or ginverse) the program runs without error. I have tried a wide range of priors and even run without priors and get the same error for the haplodiploid matrix while the diploid matrix runs.


I am curious if others have received a similar message and how it was dealt with.


Thanks in advance for any help!

Mike Sheehan



	[[alternative HTML version deleted]]


From j.hadfield at ed.ac.uk  Tue Oct  4 22:07:38 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Tue, 4 Oct 2016 21:07:38 +0100
Subject: [R-sig-ME] Priors for MCMCglmm with haplodiploid relatedness
 matrix
In-Reply-To: <CY1PR04MB21870F25433BF01FD115597BC4C50@CY1PR04MB2187.namprd04.prod.outlook.com>
References: <CY1PR04MB2187960A0720219040819B92C4C50@CY1PR04MB2187.namprd04.prod.outlook.com>
	<CY1PR04MB21870F25433BF01FD115597BC4C50@CY1PR04MB2187.namprd04.prod.outlook.com>
Message-ID: <e4fe499f-5f49-6dee-a6ed-f0d0dba2f5d6@ed.ac.uk>

Hi Mike,

Can you tell us how haplo.mat.inverse is formed or provide the code. The 
error suggests that the matrix is (near) singular.

Cheers,

Jarrod



On 04/10/2016 17:00, Michael Sheehan wrote:
> Dear all,
>
>
> I am attempting to examine the heritability of a number of traits in a haplodiplod species using MCMCglmm. This has presented a number of challenges since the standard relatedness matrix calculated from a pedigree as part of the program assumes diploid relatedness, which underestimates relatedness among sisters. I have been able to make a haplodiploid appropriate relatedness matrix but when I run the following:
>
>
> model1=MCMCglmm(trait~1, random=~animal, ginverse=list(animal=haplo.mat.inverse), data=data, family="ordinal", prior=prior1, nitt=105000, thin=100, burnin=5000, verbose=T)
>
>
> I immediately get the following error:
>
>
> "G-structure 1 is ill-conditioned (possibly because of ginverse): use proper priors if you haven't, or rescale data if you have"
>
>
> If I run the same code with a diploid relatedness matrix (calling it through pedigree or ginverse) the program runs without error. I have tried a wide range of priors and even run without priors and get the same error for the haplodiploid matrix while the diploid matrix runs.
>
>
> I am curious if others have received a similar message and how it was dealt with.
>
>
> Thanks in advance for any help!
>
> Mike Sheehan
>
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From s06mw3 at abdn.ac.uk  Wed Oct  5 12:56:26 2016
From: s06mw3 at abdn.ac.uk (Matthew Wolak)
Date: Wed, 5 Oct 2016 11:56:26 +0100
Subject: [R-sig-ME] Priors for MCMCglmm with haplodiploid relatedness
 matrix
In-Reply-To: <CY1PR04MB21870F25433BF01FD115597BC4C50@CY1PR04MB2187.namprd04.prod.outlook.com>
References: <CY1PR04MB2187960A0720219040819B92C4C50@CY1PR04MB2187.namprd04.prod.outlook.com>
	<CY1PR04MB21870F25433BF01FD115597BC4C50@CY1PR04MB2187.namprd04.prod.outlook.com>
Message-ID: <174bc215-35f2-c7ac-4aa1-a4264bd1972b@abdn.ac.uk>

Hi Mike,

As Jarrod suggests, the singularity could be due to a numerical
dependence among entries in the matrix that could arise either because
of the structure of the pedigree or because of (more or less) rounding
errors. Have you/can you compare your haplo.mat.inverse to what you get
if you run the pedigree through the `makeS()` function in the nadiv
package? Does supplying the makeS inverse matrix allow the model to run?

For example, using the `FG90` example pedigree in nadiv:

nadiv.haplo.mat.inverse <- makeS(FG90, heterogametic = "0", DosageComp =
"ngdc")$Sinv

# Where the `heterogametic` argument gives the code in a 4th column of
the pedigree denoting the haploid sex


Matthew

....................................................
Dr. Matthew E. Wolak
School of Biological Sciences
Zoology Building
University of Aberdeen
Tillydrone Avenue
Aberdeen AB24 2TZ

Zoology room 202
office phone: +44 (0)1224 273255

On 04/10/16 17:00, Michael Sheehan wrote:
> Dear all,
>
>
> I am attempting to examine the heritability of a number of traits in a haplodiplod species using MCMCglmm. This has presented a number of challenges since the standard relatedness matrix calculated from a pedigree as part of the program assumes diploid relatedness, which underestimates relatedness among sisters. I have been able to make a haplodiploid appropriate relatedness matrix but when I run the following:
>
>
> model1=MCglmm(trait~1, random=~animal, ginverse=list(animal=haplo.mat.inverse), data?ta, family="ordinal", prior=prior1, nitt5000, thin0, burninP00, verbose=T)
>
>
> I immediately get the following error:
>
>
> "G-structure 1 is ill-conditioned (possibly because of ginverse): use proper priors if you haven't, or rescale data if you have"
>
>
> If I run the same code with a diploid relatedness matrix (calling it through pedigree or ginverse) the program runs without error. I have tried a wide range of priors and even run without priors and get the same error for the haplodiploid matrix while the diploid matrix runs.
>
>
> I am curious if others have received a similar message and how it was dealt with.
>
>
> Thanks in advance for any help!
>
> Mike Sheehan
>
>
>
>       [[alternative HTML version deleted]]
>
>



The University of Aberdeen is a charity registered in Scotland, No SC013683.
Tha Oilthigh Obar Dheathain na charthannas cl?raichte ann an Alba, ?ir. SC013683.


From msheehan at cornell.edu  Wed Oct  5 16:54:20 2016
From: msheehan at cornell.edu (Michael Sheehan)
Date: Wed, 5 Oct 2016 14:54:20 +0000
Subject: [R-sig-ME] Priors for MCMCglmm with haplodiploid relatedness
 matrix
In-Reply-To: <174bc215-35f2-c7ac-4aa1-a4264bd1972b@abdn.ac.uk>
References: <CY1PR04MB2187960A0720219040819B92C4C50@CY1PR04MB2187.namprd04.prod.outlook.com>
	<CY1PR04MB21870F25433BF01FD115597BC4C50@CY1PR04MB2187.namprd04.prod.outlook.com>,
	<174bc215-35f2-c7ac-4aa1-a4264bd1972b@abdn.ac.uk>
Message-ID: <CY1PR04MB2187DD8886091DF80750820BC4C40@CY1PR04MB2187.namprd04.prod.outlook.com>

Matthew and Jared,


Many thanks for the replies. Using nadiv's makeS function I was able to run the model, so it seems that it was a problem with the code that I had been using to generate the pedigree.


At the very least let this thread stand as an advertisement for the makeS function in nadiv for aiding haplodiploid animal models!


Best

Mike

________________________________
From: Matthew Wolak <s06mw3 at abdn.ac.uk>
Sent: Wednesday, October 5, 2016 6:56:26 AM
To: Michael Sheehan; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Priors for MCMCglmm with haplodiploid relatedness matrix

Hi Mike,

As Jarrod suggests, the singularity could be due to a numerical
dependence among entries in the matrix that could arise either because
of the structure of the pedigree or because of (more or less) rounding
errors. Have you/can you compare your haplo.mat.inverse to what you get
if you run the pedigree through the `makeS()` function in the nadiv
package? Does supplying the makeS inverse matrix allow the model to run?

For example, using the `FG90` example pedigree in nadiv:

nadiv.haplo.mat.inverse <- makeS(FG90, heterogametic = "0", DosageComp =
"ngdc")$Sinv

# Where the `heterogametic` argument gives the code in a 4th column of
the pedigree denoting the haploid sex


Matthew

....................................................
Dr. Matthew E. Wolak
School of Biological Sciences
Zoology Building
University of Aberdeen
Tillydrone Avenue
Aberdeen AB24 2TZ

Zoology room 202
office phone: +44 (0)1224 273255

On 04/10/16 17:00, Michael Sheehan wrote:
> Dear all,
>
>
> I am attempting to examine the heritability of a number of traits in a haplodiplod species using MCMCglmm. This has presented a number of challenges since the standard relatedness matrix calculated from a pedigree as part of the program assumes diploid relatedness, which underestimates relatedness among sisters. I have been able to make a haplodiploid appropriate relatedness matrix but when I run the following:
>
>
> model1=MCglmm(trait~1, random=~animal, ginverse=list(animal=haplo.mat.inverse), data?ta, family="ordinal", prior=prior1, nitt5000, thin0, burninP00, verbose=T)
>
>
> I immediately get the following error:
>
>
> "G-structure 1 is ill-conditioned (possibly because of ginverse): use proper priors if you haven't, or rescale data if you have"
>
>
> If I run the same code with a diploid relatedness matrix (calling it through pedigree or ginverse) the program runs without error. I have tried a wide range of priors and even run without priors and get the same error for the haplodiploid matrix while the diploid matrix runs.
>
>
> I am curious if others have received a similar message and how it was dealt with.
>
>
> Thanks in advance for any help!
>
> Mike Sheehan
>
>
>
>       [[alternative HTML version deleted]]
>
>



The University of Aberdeen is a charity registered in Scotland, No SC013683.
Tha Oilthigh Obar Dheathain na charthannas cl?raichte ann an Alba, ?ir. SC013683.

	[[alternative HTML version deleted]]


From Margaret.MacDougall at ed.ac.uk  Wed Oct  5 20:59:30 2016
From: Margaret.MacDougall at ed.ac.uk (MACDOUGALL Margaret)
Date: Wed, 5 Oct 2016 18:59:30 +0000
Subject: [R-sig-ME] Managing person identifier variable
Message-ID: <VI1PR0502MB3021908EF72C4FFC35FBA698C5C40@VI1PR0502MB3021.eurprd05.prod.outlook.com>

Hello



I would be most grateful for some advice in relation to the interpretation of a person identifier variable (persID, say),  in R. I would like to represent persons, as an independent variable, by a random effect. However, there are over 200 such persons. Each person is allocated a random numerical code as a unique identifier.  Currently, R is reading the identifier variable as a numeric variable. Is there a quick way of addressing this problem by recoding the variable?  (I do not wish to bin the values into category ranges; rather, I wish to avoid the numerical codes being interpreted literally.)



Many thanks



Margaret





-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20161005/3aacfa32/attachment.pl>

From marc_schwartz at me.com  Wed Oct  5 21:15:34 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Wed, 05 Oct 2016 14:15:34 -0500
Subject: [R-sig-ME] Managing person identifier variable
In-Reply-To: <VI1PR0502MB3021908EF72C4FFC35FBA698C5C40@VI1PR0502MB3021.eurprd05.prod.outlook.com>
References: <VI1PR0502MB3021908EF72C4FFC35FBA698C5C40@VI1PR0502MB3021.eurprd05.prod.outlook.com>
Message-ID: <222C5EBC-A58F-4348-B057-9FC54D11A103@me.com>


> On Oct 5, 2016, at 1:59 PM, MACDOUGALL Margaret <Margaret.MacDougall at ed.ac.uk> wrote:
> 
> Hello
> 
> I would be most grateful for some advice in relation to the interpretation of a person identifier variable (persID, say),  in R. I would like to represent persons, as an independent variable, by a random effect. However, there are over 200 such persons. Each person is allocated a random numerical code as a unique identifier.  Currently, R is reading the identifier variable as a numeric variable. Is there a quick way of addressing this problem by recoding the variable?  (I do not wish to bin the values into category ranges; rather, I wish to avoid the numerical codes being interpreted literally.)
> 
> Many thanks
> 
> Margaret



Margaret,

How are you reading the data into R? Are you using read.table() or read.csv()?

If so, see ?read.table and note the 'colClasses' argument. 

If that argument is the default NA, ?type.convert will be used to convert the columns in the incoming data from the character based source to numeric, etc. as apropos.

If you explicitly define colClasses for each column, you can set the persID column to "character" and it will not be coerced to numeric by default.

Some of these issues are covered in the R Data Import/Export manual in the section on Spreadsheet-like data:

  https://cran.r-project.org/doc/manuals/r-release/R-data.html#Variations-on-read_002etable <https://cran.r-project.org/doc/manuals/r-release/R-data.html#Variations-on-read_002etable>


Regards,

Marc Schwartz


	[[alternative HTML version deleted]]


From thlytras at gmail.com  Wed Oct  5 21:21:38 2016
From: thlytras at gmail.com (Theodore Lytras)
Date: Wed, 05 Oct 2016 22:21:38 +0300
Subject: [R-sig-ME] Managing person identifier variable
In-Reply-To: <VI1PR0502MB3021908EF72C4FFC35FBA698C5C40@VI1PR0502MB3021.eurprd05.prod.outlook.com>
References: <VI1PR0502MB3021908EF72C4FFC35FBA698C5C40@VI1PR0502MB3021.eurprd05.prod.outlook.com>
Message-ID: <1682825.eChPh1C0BU@equinox2>

???? ???????, 5 ????????? 2016 6:59:30 ?.?. EEST MACDOUGALL Margaret ??????:
> I would be most grateful for some advice in relation to the interpretation
> of a person identifier variable (persID, say),  in R. I would like to
> represent persons, as an independent variable, by a random effect. However,
> there are over 200 such persons. Each person is allocated a random
> numerical code as a unique identifier.  Currently, R is reading the
> identifier variable as a numeric variable. Is there a quick way of
> addressing this problem by recoding the variable?  (I do not wish to bin
> the values into category ranges; rather, I wish to avoid the numerical
> codes being interpreted literally.)

Just recode it as a factor, i.e. factor(persID).

By the way, lme4 does that implicitly if you specify a numeric variable as a 
random effect in a model formula, i.e. you can just say: y ~ x + (1|persID) 
instead of: y ~ x + (1|factor(persID))


From marc_schwartz at me.com  Wed Oct  5 21:28:31 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Wed, 05 Oct 2016 14:28:31 -0500
Subject: [R-sig-ME] Managing person identifier variable
In-Reply-To: <1682825.eChPh1C0BU@equinox2>
References: <VI1PR0502MB3021908EF72C4FFC35FBA698C5C40@VI1PR0502MB3021.eurprd05.prod.outlook.com>
	<1682825.eChPh1C0BU@equinox2>
Message-ID: <85F897BE-9AE2-483B-BFB0-3AAF030A69CA@me.com>


> On Oct 5, 2016, at 2:21 PM, Theodore Lytras <thlytras at gmail.com> wrote:
> 
> ???? ???????, 5 ????????? 2016 6:59:30 ?.?. EEST MACDOUGALL Margaret ??????:
>> I would be most grateful for some advice in relation to the interpretation
>> of a person identifier variable (persID, say),  in R. I would like to
>> represent persons, as an independent variable, by a random effect. However,
>> there are over 200 such persons. Each person is allocated a random
>> numerical code as a unique identifier.  Currently, R is reading the
>> identifier variable as a numeric variable. Is there a quick way of
>> addressing this problem by recoding the variable?  (I do not wish to bin
>> the values into category ranges; rather, I wish to avoid the numerical
>> codes being interpreted literally.)
> 
> Just recode it as a factor, i.e. factor(persID).
> 
> By the way, lme4 does that implicitly if you specify a numeric variable as a 
> random effect in a model formula, i.e. you can just say: y ~ x + (1|persID) 
> instead of: y ~ x + (1|factor(persID))


Just a quick pointer here which is that if the persID values contained leading zeros that are a material part of the unique IDs, such as:

  01234
  001234

then coercing to factors, after having been coerced to numeric values, will result in both of the above being 1234:

> factor(as.numeric("01234"))
[1] 1234
Levels: 1234

> factor(as.numeric("001234"))
[1] 1234
Levels: 1234


Food for thought...

Regards,

Marc Schwartz


	[[alternative HTML version deleted]]


From Margaret.MacDougall at ed.ac.uk  Wed Oct  5 22:33:08 2016
From: Margaret.MacDougall at ed.ac.uk (MACDOUGALL Margaret)
Date: Wed, 5 Oct 2016 20:33:08 +0000
Subject: [R-sig-ME] Managing person identifier variable
In-Reply-To: <d8c6db7e7d6843f1bc35cc4e793ee755@CSTATWINEX1.cstatwin.msu.edu>
References: <VI1PR0502MB3021908EF72C4FFC35FBA698C5C40@VI1PR0502MB3021.eurprd05.prod.outlook.com>
	<d8c6db7e7d6843f1bc35cc4e793ee755@CSTATWINEX1.cstatwin.msu.edu>
Message-ID: <VI1PR0502MB30213BDD22424483837572F1C5C40@VI1PR0502MB3021.eurprd05.prod.outlook.com>

Thanks to all those who kindly provided a swift reply to my query at the foot of this email. I am sharing Steve's reply below, as I found it particularly helpful and trust that others may also benefit. My query has now been addressed. 
Best wishes
Margaret


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


-----Original Message-----
From: Steve Pierce [mailto:Steve.Pierce at cstat.msu.edu] 
Sent: 05 October 2016 20:28
To: MACDOUGALL Margaret <Margaret.MacDougall at ed.ac.uk>
Subject: RE: [R-sig-ME] Managing person identifier variable

Margaret,

Convert that variable to a factor. Suppose your data frame is (cleverly) called mydata, and the variable is called PID. The following code will coerce your numerical PID variable into a categorical factor variable called PIDf. You can also just overwrite the original PID variable. 

mydata$PIDf <- factor(mydata$PID)    # Creates a new variable 
mydata$PID  <- factor(mydata$PID)    # Overwrites the original variable


Steven J. Pierce, Ph.D.
Associate Director
Center for Statistical Training & Consulting (CSTAT) Michigan State University Giltner Hall
293 Farm Lane, Room 178
East Lansing, MI 48824

Office Phone: (517) 353-9288
Office Fax: (517) 353-9307
E-mail: Steve.Pierce at cstat.msu.edu
Web: http://www.cstat.msu.edu 


-----Original Message-----
From: MACDOUGALL Margaret [mailto:Margaret.MacDougall at ed.ac.uk]
Sent: Wednesday, October 05, 2016 3:00 PM
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Managing person identifier variable

Hello



I would be most grateful for some advice in relation to the interpretation of a person identifier variable (persID, say),  in R. I would like to represent persons, as an independent variable, by a random effect. However, there are over 200 such persons. Each person is allocated a random numerical code as a unique identifier.  Currently, R is reading the identifier variable as a numeric variable. Is there a quick way of addressing this problem by recoding the variable?  (I do not wish to bin the values into category ranges; rather, I wish to avoid the numerical codes being interpreted literally.)



Many thanks



Margaret


From ahmadr215 at tpg.com.au  Sun Oct  9 15:56:40 2016
From: ahmadr215 at tpg.com.au (Ahmad)
Date: Mon, 10 Oct 2016 00:56:40 +1100
Subject: [R-sig-ME] equation for linear mixed model for repeated measures
	data
Message-ID: <000001d22234$f87a63e0$e96f2ba0$@tpg.com.au>

Hi

 

I have used "nlme" package in R for linear mixed model with repeated
measures using lme function (below):

 

model.1 <- lme(hta ~ group + days, data = dat, random = ~ 1 | id)

where is "hta" is the dependent variable, with 3 "groups" measured for 7
"days", and "id" is the induvial observations (subject) that were repeated
on for 7 days.

 

I am looking for the scientific question of this model for publication
(e.g.; yij = ?0 + ?1X1ij ,..) in "nlme" package, but couldn't find it. 

 

Any help comments would be greatly appreciated!

 

Ahmad

 


	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Mon Oct 10 09:12:08 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 10 Oct 2016 09:12:08 +0200
Subject: [R-sig-ME] equation for linear mixed model for repeated
	measures data
In-Reply-To: <000001d22234$f87a63e0$e96f2ba0$@tpg.com.au>
References: <000001d22234$f87a63e0$e96f2ba0$@tpg.com.au>
Message-ID: <CAJuCY5zuahKT8KJ2D1=k4oLudR7PxtUCU6Ji8_Dt7Zs+E1Shqg@mail.gmail.com>

Dear Ahmad,

You can find that in books like Pinheiro & Bates (2000) or Zuur et al
(2009).

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-10-09 15:56 GMT+02:00 Ahmad <ahmadr215 at tpg.com.au>:

> Hi
>
>
>
> I have used "nlme" package in R for linear mixed model with repeated
> measures using lme function (below):
>
>
>
> model.1 <- lme(hta ~ group + days, data = dat, random = ~ 1 | id)
>
> where is "hta" is the dependent variable, with 3 "groups" measured for 7
> "days", and "id" is the induvial observations (subject) that were repeated
> on for 7 days.
>
>
>
> I am looking for the scientific question of this model for publication
> (e.g.; yij = ?0 + ?1X1ij ,..) in "nlme" package, but couldn't find it.
>
>
>
> Any help comments would be greatly appreciated!
>
>
>
> Ahmad
>
>
>
>
>         [[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From luob041 at nenu.edu.cn  Mon Oct 10 01:37:09 2016
From: luob041 at nenu.edu.cn (=?UTF-8?B?572X5rOi?=)
Date: Mon, 10 Oct 2016 07:37:09 +0800 (GMT+08:00)
Subject: [R-sig-ME] =?utf-8?q?call_for_help?=
Message-ID: <AHEA0QCuAFFFO851yOyhV4o0.1.1476056229592.Hmail.luob041@nenu.edu.cn>

Dear Experts,


Nice to meet you! I am a Ph.D. student from Northeast Normal University in China. My research field is behavioral ecology. Thanks for your contribution to develop the package 'lme4'.
That's really nice and valuable!


Recently, I would like to fit the linear mixed model using the experimental data. Following the examples listed  in the  guide, I know how to achieve it using your package. Yet, I wonder if my data
satisfy the model assumption. I have two major confusions (see followings). Would you please give me some help?


Does it require normality of model residuals, homoscedasticity, absenceof collinearity between predictors or perhaps others?


If so, how can I check whether  my data satisfy the assumption? Would you please provide some advice or R code?


Really need your help! Thanks very much!


Best,
Bo





	[[alternative HTML version deleted]]


From carlosfamilia at gmail.com  Mon Oct 10 16:54:18 2016
From: carlosfamilia at gmail.com (Carlos Familia)
Date: Mon, 10 Oct 2016 15:54:18 +0100
Subject: [R-sig-ME] call for help
In-Reply-To: <AHEA0QCuAFFFO851yOyhV4o0.1.1476056229592.Hmail.luob041@nenu.edu.cn>
References: <AHEA0QCuAFFFO851yOyhV4o0.1.1476056229592.Hmail.luob041@nenu.edu.cn>
Message-ID: <9413A18C-323D-430B-BF9D-1C04625DC5E0@gmail.com>

Hi,

I normally use the following code to check for assumptions:

par(mfrow=c(2,2))

plot(fitted(mdl),residuals(mdl), pch = '.')
abline(lm(y ~ x, data = data.frame(y = residuals(mdl), x = fitted(mdl))), col = 'red')
abline(a=0, b=0, lty=3)

plot(fitted(mdl), amps.long$mic, pch = '.', ylab = 'observed')
abline(lm(y ~ x, data = data.frame(y = amps.long$mic, x = fitted(mdl))), col = 'red')
abline(a = 0, b = 1, lty=3)

qqnorm(residuals(mdl))
qqline(residuals(mdl))

hist(residuals(mdl),breaks = 50, probability = T)
lines(density(residuals(mdl)), col = "red")
par(mfrow=c(1,1))

There are other ways to do it, and you should know what are the assumptions, for that I suggest for you to take a loot at Douglas Bates book.

Best regards,
Carlos Fam?lia

Carlos Familia, MPharm, PharmD
ISCSEM Assistant Lecturer
ISCSEM Laboratory of Molecular Pathology Researcher
ISCSEM Erasmus Mundus Master in Forensic Sciences Vice-coordinator
Tel. +351 212946700 (Main) / +351 212946769 (Lab)
e-mail: carlosfamilia at gmail.com / carlosfamilia at egasmoniz.edu.pt

The information in this e-mail and any attachments may be confidential. If you have received this email in error please notify the sender immediately and remove it from your system. Do not disclose the contents to another person or take copies.

> On 10 Oct 2016, at 00:37, ?? <luob041 at nenu.edu.cn> wrote:
> 
> Dear Experts,
> 
> 
> Nice to meet you! I am a Ph.D. student from Northeast Normal University in China. My research field is behavioral ecology. Thanks for your contribution to develop the package 'lme4'.
> That's really nice and valuable!
> 
> 
> Recently, I would like to fit the linear mixed model using the experimental data. Following the examples listed  in the  guide, I know how to achieve it using your package. Yet, I wonder if my data
> satisfy the model assumption. I have two major confusions (see followings). Would you please give me some help?
> 
> 
> Does it require normality of model residuals, homoscedasticity, absenceof collinearity between predictors or perhaps others?
> 
> 
> If so, how can I check whether  my data satisfy the assumption? Would you please provide some advice or R code?
> 
> 
> Really need your help! Thanks very much!
> 
> 
> Best,
> Bo
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From highstat at highstat.com  Mon Oct 10 17:57:49 2016
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Mon, 10 Oct 2016 16:57:49 +0100
Subject: [R-sig-ME] Stats course in Montreal
Message-ID: <3186ece5-f202-6e9c-8bff-32448a894a34@highstat.com>


We would like to announce the following statistics course:

Course: Data exploration, regression, GLM & GAM with R

Where:  Montreal, Canada

When:   9-13 January 2017

Course website: http://www.highstat.com/statscourse.htm

Course flyer: 
http://highstat.com/Courses/Flyers/Flyer2017_01Montreal_RGG.pdf


Kind regards,

Alain Zuur


Other open courses in 2017:

Data exploration, regression, GLM & GAM with introduction to R. 13-17 
February 2017. Lisbon.
Introduction to Regression Models with Spatial and Temporal Correlation. 
20-24 February 2017. Lisbon.
Introduction to Regression Models with Spatial and Temporal Correlation. 
8-12 May 2017. Genoa.
Linear Mixed Effects Models and GLMM with R. Frequentist and Bayesian 
approaches. 9-13 October 2017. Trondheim.
Introduction to Regression Models with Spatial and Temporal Correlation. 
23-27 October 2017. Southampton


-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Beginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email: highstat at highstat.com
URL: www.highstat.com


	[[alternative HTML version deleted]]


From ahmadr215 at tpg.com.au  Mon Oct 10 20:17:58 2016
From: ahmadr215 at tpg.com.au (Ahmad)
Date: Tue, 11 Oct 2016 05:17:58 +1100
Subject: [R-sig-ME] equation for linear mixed model for repeated
	measures data
In-Reply-To: <CAJuCY5zuahKT8KJ2D1=k4oLudR7PxtUCU6Ji8_Dt7Zs+E1Shqg@mail.gmail.com>
References: <000001d22234$f87a63e0$e96f2ba0$@tpg.com.au>
	<CAJuCY5zuahKT8KJ2D1=k4oLudR7PxtUCU6Ji8_Dt7Zs+E1Shqg@mail.gmail.com>
Message-ID: <000801d22322$a3961d30$eac25790$@tpg.com.au>

Dear Thierry

 

Thanks for your help on this,

I will try to find the book. Can I have the name of journal that Zuur?s paaper is published.

 

Thanks again!

 

Ahmad

 

 

From: Thierry Onkelinx [mailto:thierry.onkelinx at inbo.be] 
Sent: Monday, 10 October 2016 6:12 PM
To: Ahmad <ahmadr215 at tpg.com.au>
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] equation for linear mixed model for repeated measures data

 

Dear Ahmad,

 

You can find that in books like Pinheiro & Bates (2000) or Zuur et al (2009).

 

Best regards,




ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest 
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance 
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner 
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey

 

2016-10-09 15:56 GMT+02:00 Ahmad <ahmadr215 at tpg.com.au <mailto:ahmadr215 at tpg.com.au> >:

Hi



I have used "nlme" package in R for linear mixed model with repeated
measures using lme function (below):



model.1 <- lme(hta ~ group + days, data = dat, random = ~ 1 | id)

where is "hta" is the dependent variable, with 3 "groups" measured for 7
"days", and "id" is the induvial observations (subject) that were repeated
on for 7 days.



I am looking for the scientific question of this model for publication
(e.g.; yij = ?0 + ?1X1ij ,..) in "nlme" package, but couldn't find it.



Any help comments would be greatly appreciated!



Ahmad




        [[alternative HTML version deleted]]


_______________________________________________
R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org>  mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models




	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Tue Oct 11 00:17:49 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 10 Oct 2016 15:17:49 -0700
Subject: [R-sig-ME] equation for linear mixed model for repeated
	measures data
In-Reply-To: <000801d22322$a3961d30$eac25790$@tpg.com.au>
References: <000001d22234$f87a63e0$e96f2ba0$@tpg.com.au>
	<CAJuCY5zuahKT8KJ2D1=k4oLudR7PxtUCU6Ji8_Dt7Zs+E1Shqg@mail.gmail.com>
	<000801d22322$a3961d30$eac25790$@tpg.com.au>
Message-ID: <8B2D874A-77E3-425D-A16A-A6468106F0FE@comcast.net>


> On Oct 10, 2016, at 11:17 AM, Ahmad <ahmadr215 at tpg.com.au> wrote:
> 
> Dear Thierry
> 
> 
> 
> Thanks for your help on this,
> 
> I will try to find the book. Can I have the name of journal that Zuur?s paaper is published.
> 

https://www.google.com/search?q=zuur+2009+lme&oq=zuur+2009+lme
> 
> 
> Thanks again!
> 
> 
> 
> Ahmad
> 
> 
> 
> 
> 
> From: Thierry Onkelinx [mailto:thierry.onkelinx at inbo.be] 
> Sent: Monday, 10 October 2016 6:12 PM
> To: Ahmad <ahmadr215 at tpg.com.au>
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] equation for linear mixed model for repeated measures data
> 
> 
> 
> Dear Ahmad,
> 
> 
> 
> You can find that in books like Pinheiro & Bates (2000) or Zuur et al (2009).
> 
> 
> 
> Best regards,
> 
> 
> 
> 
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest 
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance 
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
> 
> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner 
> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey
> 
> 
> 
> 2016-10-09 15:56 GMT+02:00 Ahmad <ahmadr215 at tpg.com.au <mailto:ahmadr215 at tpg.com.au> >:
> 
> Hi
> 
> 
> 
> I have used "nlme" package in R for linear mixed model with repeated
> measures using lme function (below):
> 
> 
> 
> model.1 <- lme(hta ~ group + days, data = dat, random = ~ 1 | id)
> 
> where is "hta" is the dependent variable, with 3 "groups" measured for 7
> "days", and "id" is the induvial observations (subject) that were repeated
> on for 7 days.
> 
> 
> 
> I am looking for the scientific question of this model for publication
> (e.g.; yij = ?0 + ?1X1ij ,..) in "nlme" package, but couldn't find it.
> 
> 
> 
> Any help comments would be greatly appreciated!
> 
> 
> 
> Ahmad
> 
> 
> 
> 
>        [[alternative HTML version deleted]]
> 
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org>  mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

David Winsemius
Alameda, CA, USA


From tim.cole at ucl.ac.uk  Tue Oct 11 11:29:13 2016
From: tim.cole at ucl.ac.uk (Cole, Tim)
Date: Tue, 11 Oct 2016 09:29:13 +0000
Subject: [R-sig-ME] Exponent random effect in nlmer
Message-ID: <D42269FD.4ECAF%tim.cole@ucl.ac.uk>

I have a model of the form
  m1 <- lmer(y ~ f + (1 | id) )
where y is a continuous variable centred on zero, f is a unordered factor with coefficients b such 0 < b < 1, and there is a signficant random subject intercept.

The random intercept can lead to predicted values outside the valid range (0, 1). For this reason I'd like to reformulate the model as
m2 <- nlmer(y ~ (f - 1) ^ exp(1 | id) )   (using a invalid but I hope obvious notation), where the random effect is now a power centred on 1. This would constrain the fitted values to be within c(0, 1).

My question is: can this be done in nlmer, and if so how? Please can someone point me in the right direction?

Thanks,
Tim Cole
---
Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk> Phone +44(0)20 7905 2666 Fax +44(0)20 7905 2381
Population, Policy and Practice Programme
UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK


	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Tue Oct 11 12:06:07 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Tue, 11 Oct 2016 12:06:07 +0200
Subject: [R-sig-ME] Exponent random effect in nlmer
In-Reply-To: <D42269FD.4ECAF%tim.cole@ucl.ac.uk>
References: <D42269FD.4ECAF%tim.cole@ucl.ac.uk>
Message-ID: <CAJuCY5wmX6jYMvrtjf_M2LYWOt8gYXq0vSCqJzOTb7sCnHbUnw@mail.gmail.com>

Dear Tim,

y centred on 0 and a valid range (0, 1) seems to be conflicting statements.

Here a some solutions depending on y

- y stems from a binomial process
     - use a binomial glmm.
- y is continuous and you are willing to transform y
    - 0 < y <  1
        - apply a logit transformation on y. lmer(plogis(y) ~ f + (1 | id) )
    - 0 <= y < 1
        - apply a log transformation on y. lmer(log(y) ~ f + (1 | id) )
    - 0 < y <= 1
        - apply a log transformation on 1 - y. lmer(log(1 - y) ~ f + (1 |
id) )
- y is continuous are not willing to transform y
   - use a beta regression with 0 and/or 1 inflation in case you have 0 or
1 in the data. Have a look at the gamlss package to fit this model.

Best regards,


ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-10-11 11:29 GMT+02:00 Cole, Tim <tim.cole at ucl.ac.uk>:

> I have a model of the form
>   m1 <- lmer(y ~ f + (1 | id) )
> where y is a continuous variable centred on zero, f is a unordered factor
> with coefficients b such 0 < b < 1, and there is a signficant random
> subject intercept.
>
> The random intercept can lead to predicted values outside the valid range
> (0, 1). For this reason I'd like to reformulate the model as
> m2 <- nlmer(y ~ (f - 1) ^ exp(1 | id) )   (using a invalid but I hope
> obvious notation), where the random effect is now a power centred on 1.
> This would constrain the fitted values to be within c(0, 1).
>
> My question is: can this be done in nlmer, and if so how? Please can
> someone point me in the right direction?
>
> Thanks,
> Tim Cole
> ---
> Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk> Phone +44(0)20 7905 2666
> Fax +44(0)20 7905 2381
> Population, Policy and Practice Programme
> UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From tim.cole at ucl.ac.uk  Tue Oct 11 12:30:47 2016
From: tim.cole at ucl.ac.uk (Cole, Tim)
Date: Tue, 11 Oct 2016 10:30:47 +0000
Subject: [R-sig-ME] Exponent random effect in nlmer
In-Reply-To: <CAJuCY5wmX6jYMvrtjf_M2LYWOt8gYXq0vSCqJzOTb7sCnHbUnw@mail.gmail.com>
References: <D42269FD.4ECAF%tim.cole@ucl.ac.uk>
	<CAJuCY5wmX6jYMvrtjf_M2LYWOt8gYXq0vSCqJzOTb7sCnHbUnw@mail.gmail.com>
Message-ID: <D4227B1D.4ED19%tim.cole@ucl.ac.uk>

Dear Thierry,

Thanks very much for your speedy response.

I agree my model looks odd, but it has a theoretical basis which I'd prefer not to spell out at this stage. Suffice to say that
* -Inf < y < Inf
* 0 < E(y) < 1
* there is a subject random effect.

For these reasons the usual models and/or transformations won't work, whereas my proposed exponent random effect ought to. I just need to fit it, to see if I'm right!

Best wishes,
Tim
---
Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk> Phone +44(0)20 7905 2666 Fax +44(0)20 7905 2381
Population, Policy and Practice Programme
UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK


From: Thierry Onkelinx <thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>>
Date: Tuesday, 11 October 2016 11:06
To: Tim Cole <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk>>
Cc: "r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>" <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: Re: [R-sig-ME] Exponent random effect in nlmer

Dear Tim,

y centred on 0 and a valid range (0, 1) seems to be conflicting statements.

Here a some solutions depending on y

- y stems from a binomial process
     - use a binomial glmm.
- y is continuous and you are willing to transform y
    - 0 < y <  1
        - apply a logit transformation on y. lmer(plogis(y) ~ f + (1 | id) )
    - 0 <= y < 1
        - apply a log transformation on y. lmer(log(y) ~ f + (1 | id) )
    - 0 < y <= 1
        - apply a log transformation on 1 - y. lmer(log(1 - y) ~ f + (1 | id) )
- y is continuous are not willing to transform y
   - use a beta regression with 0 and/or 1 inflation in case you have 0 or 1 in the data. Have a look at the gamlss package to fit this model.

Best regards,


ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey

2016-10-11 11:29 GMT+02:00 Cole, Tim <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk>>:
I have a model of the form
  m1 <- lmer(y ~ f + (1 | id) )
where y is a continuous variable centred on zero, f is a unordered factor with coefficients b such 0 < b < 1, and there is a signficant random subject intercept.

The random intercept can lead to predicted values outside the valid range (0, 1). For this reason I'd like to reformulate the model as
m2 <- nlmer(y ~ (f - 1) ^ exp(1 | id) )   (using a invalid but I hope obvious notation), where the random effect is now a power centred on 1. This would constrain the fitted values to be within c(0, 1).

My question is: can this be done in nlmer, and if so how? Please can someone point me in the right direction?

Thanks,
Tim Cole
---
Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk><mailto:Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk>> Phone +44(0)20 7905 2666<tel:%2B44%280%2920%207905%202666> Fax +44(0)20 7905 2381<tel:%2B44%280%2920%207905%202381>
Population, Policy and Practice Programme
UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK


        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


	[[alternative HTML version deleted]]


From boccigionata at gmail.com  Tue Oct 11 13:59:13 2016
From: boccigionata at gmail.com (Gionata Bocci)
Date: Tue, 11 Oct 2016 13:59:13 +0200
Subject: [R-sig-ME] Erratic cross-platform behavior of glmmADMB
Message-ID: <CADGFu4-X+uiM_iW=-2xirT2XD=ZApG299hSmGdjHSGfEHPR+OA@mail.gmail.com>

Dear list members,

I am writing because I've come across a puzzling situation when working
with glmmADMB::glmmmadmb.
I have a glmmmadmb(..., family = "nbinom") model which runs smoothly on a
Windows machine, while, on my Ubuntu 16.04 returns the following messages
(sorry for the rather long output):

  ---------------------------------

  "matrix not pos definite in sparse choleski
   !!!! trace_log: zero diagonal element
  [...many other trace_log warnings...]

   matrix not pos definite in sparse choleski
   Error: Invalid index 157882075 used for array range [0, 107] in "double&
dvector::operator[] (int i)".
    array bound exceeded -- index too high

   Parameters were estimated, but standard errors were not: the most likely
problem is that the curvature at MLE was zero or negative
   Error in glmmadmb(perc_fill ~ treatment * variety + year + (1 |
ls.def/d_new),  :
   The function maximizer failed (couldn't find parameter file)
Troubleshooting steps include (1) run with 'save.dir' set and inspect
output files; (2) change run parameters: see '?admbControl';(3) re-run with
debug=TRUE for more information on failure mode
   In addition: Warning message:  running command './glmmadmb -maxfn 500
-maxph 5 -noinit -shess' had status 42 "

  ---------------------------------


  I've tried the suggested troubleshootings but I got the same results.
  On both machines I am using the github version of the package and the
latest R release (3.3.1).
  I've also tried to use the package version found at "
http://glmmadmb.r-forge.r-project.org/repos" (as suggested at the glmmADMB
website) but with no luck (I also wanted to try the "Buildbot" version, but
it looks like the link is broken).

  Looking at the source code, I've noticed the following points:

   * the buildbot page is down, thus the glmmADMB:::get_bbot_versions()
function does not work (but this shouldn't be an issue in my case)
   * it looks like that the only platform-specific commands are those
related to "founding the binary of glmmadmb", "create a tmp
directory"...nothing which should affect the output of the function.


  Has anybody faced a similar situation? Do you have any tentative
explanation (and possible fixes) for this "erratic" behavior?

  Best,


Gionata.

	[[alternative HTML version deleted]]


From jdpo223 at g.uky.edu  Tue Oct 11 16:17:02 2016
From: jdpo223 at g.uky.edu (Poe, John)
Date: Tue, 11 Oct 2016 10:17:02 -0400
Subject: [R-sig-ME] Exponent random effect in nlmer
In-Reply-To: <D4227B1D.4ED19%tim.cole@ucl.ac.uk>
References: <D42269FD.4ECAF%tim.cole@ucl.ac.uk>
	<CAJuCY5wmX6jYMvrtjf_M2LYWOt8gYXq0vSCqJzOTb7sCnHbUnw@mail.gmail.com>
	<D4227B1D.4ED19%tim.cole@ucl.ac.uk>
Message-ID: <CAFW8BypL9U8ve4fsi4tYAi8CGHuF1fpdHi3_m2VK=Rn4b2kKHA@mail.gmail.com>

If you can't use a link function to constrain the parameters then you might
have to use a two stage model where you estimate the random effect in a
null model, generate the random effect directly as a variable, transform
it, and incorporate it in the full model as a covariate.

On Oct 11, 2016 6:31 AM, "Cole, Tim" <tim.cole at ucl.ac.uk> wrote:

> Dear Thierry,
>
> Thanks very much for your speedy response.
>
> I agree my model looks odd, but it has a theoretical basis which I'd
> prefer not to spell out at this stage. Suffice to say that
> * -Inf < y < Inf
> * 0 < E(y) < 1
> * there is a subject random effect.
>
> For these reasons the usual models and/or transformations won't work,
> whereas my proposed exponent random effect ought to. I just need to fit it,
> to see if I'm right!
>
> Best wishes,
> Tim
> ---
> Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk> Phone +44(0)20 7905 2666
> Fax +44(0)20 7905 2381
> Population, Policy and Practice Programme
> UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK
>
>
> From: Thierry Onkelinx <thierry.onkelinx at inbo.be<mailto:
> thierry.onkelinx at inbo.be>>
> Date: Tuesday, 11 October 2016 11:06
> To: Tim Cole <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk>>
> Cc: "r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models@
> r-project.org>" <r-sig-mixed-models at r-project.org<mailto:
> r-sig-mixed-models at r-project.org>>
> Subject: Re: [R-sig-ME] Exponent random effect in nlmer
>
> Dear Tim,
>
> y centred on 0 and a valid range (0, 1) seems to be conflicting statements.
>
> Here a some solutions depending on y
>
> - y stems from a binomial process
>      - use a binomial glmm.
> - y is continuous and you are willing to transform y
>     - 0 < y <  1
>         - apply a logit transformation on y. lmer(plogis(y) ~ f + (1 | id)
> )
>     - 0 <= y < 1
>         - apply a log transformation on y. lmer(log(y) ~ f + (1 | id) )
>     - 0 < y <= 1
>         - apply a log transformation on 1 - y. lmer(log(1 - y) ~ f + (1 |
> id) )
> - y is continuous are not willing to transform y
>    - use a beta regression with 0 and/or 1 inflation in case you have 0 or
> 1 in the data. Have a look at the gamlss package to fit this model.
>
> Best regards,
>
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-10-11 11:29 GMT+02:00 Cole, Tim <tim.cole at ucl.ac.uk<mailto:tim
> .cole at ucl.ac.uk>>:
> I have a model of the form
>   m1 <- lmer(y ~ f + (1 | id) )
> where y is a continuous variable centred on zero, f is a unordered factor
> with coefficients b such 0 < b < 1, and there is a signficant random
> subject intercept.
>
> The random intercept can lead to predicted values outside the valid range
> (0, 1). For this reason I'd like to reformulate the model as
> m2 <- nlmer(y ~ (f - 1) ^ exp(1 | id) )   (using a invalid but I hope
> obvious notation), where the random effect is now a power centred on 1.
> This would constrain the fitted values to be within c(0, 1).
>
> My question is: can this be done in nlmer, and if so how? Please can
> someone point me in the right direction?
>
> Thanks,
> Tim Cole
> ---
> Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk><mailto:Tim.cole at ucl.ac.uk
> <mailto:Tim.cole at ucl.ac.uk>> Phone +44(0)20 7905 2666<tel:%2B44%280%2920%207905%202666>
> Fax +44(0)20 7905 2381<tel:%2B44%280%2920%207905%202381>
> Population, Policy and Practice Programme
> UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>
> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From davef at otter-rsch.com  Tue Oct 11 16:02:56 2016
From: davef at otter-rsch.com (dave fournier)
Date: Tue, 11 Oct 2016 07:02:56 -0700
Subject: [R-sig-ME] Erratic cross-platform behavior of glmmADMB
In-Reply-To: <CADGFu4-X+uiM_iW=-2xirT2XD=ZApG299hSmGdjHSGfEHPR+OA@mail.gmail.com>
References: <CADGFu4-X+uiM_iW=-2xirT2XD=ZApG299hSmGdjHSGfEHPR+OA@mail.gmail.com>
Message-ID: <a292b141-7b71-19f0-3850-e98f01e61d03@otter-rsch.com>


Most likely cause it slightly different versions of the code.
If you send me the save.dir output i.e. the glmmadmb.dat and 
glmmadmb.pin files
I can run it on linux for you can see what happens.  The usual cause of 
this kind of failure
is having some really large outliers.

     Dave


From ledee at umn.edu  Tue Oct 11 18:02:37 2016
From: ledee at umn.edu (Laura Dee)
Date: Tue, 11 Oct 2016 11:02:37 -0500
Subject: [R-sig-ME] Assumptions of random effects for unbiased estimates
Message-ID: <CABEErbZX5_+i9OQO6rSz2Xskz_05WBEB_uTeHK+2mF1XG6t00Q@mail.gmail.com>

Dear all,
Random effects are more efficient estimators ? however they come at the
cost of the assumption that the random effect is not correlated with the
included explanatory variables. Otherwise, using random effects leads to
biased estimates (e.g., as laid out in Woolridge
<https://faculty.fuqua.duke.edu/~moorman/Wooldridge,%20FE%20and%20RE.pdf>'s
Econometrics text). This assumption is a strong one for many observational
datasets, and most analyses in economics do not use random effects for this
reason. *Is there a reason why observational ecological datasets would be
fundamentally different that I am missing? Why is this important assumption
(to have unbiased estimates from random effects) not emphasized in ecology?
*

Thanks!

Laura

-- 
Laura Dee
Post-doctoral Associate
University of Minnesota
ledee at umn.edu
lauraedee.com

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Tue Oct 11 20:50:36 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 11 Oct 2016 14:50:36 -0400
Subject: [R-sig-ME] Assumptions of random effects for unbiased estimates
In-Reply-To: <CABEErbZX5_+i9OQO6rSz2Xskz_05WBEB_uTeHK+2mF1XG6t00Q@mail.gmail.com>
References: <CABEErbZX5_+i9OQO6rSz2Xskz_05WBEB_uTeHK+2mF1XG6t00Q@mail.gmail.com>
Message-ID: <c1e7e88e-b182-1783-f280-784969db1d13@gmail.com>


  I didn't respond to this offline, as it took me a while even to start
to come up to speed on the question.  Random effects are indeed defined
from *very* different points of view in the two communities
([bio]statistical vs. econometric); I'm sure there are points of
contact, but I've been having a hard time getting my head around it all.

Econometric definition:

The wikipedia page <https://en.wikipedia.org/wiki/Random_effects_model>
and CrossValidated question
<http://stats.stackexchange.com/questions/66161/why-do-random-effect-models-require-the-effects-to-be-uncorrelated-with-the-inpu>
were both helpful for me.

 In the (bio)statistical world fixed and random effects are usually
justified practically in terms of shrinkage estimators, or
philosophically in terms of random draws from an exchangeable set of
levels: e.g. see
<http://stats.stackexchange.com/questions/4700/what-is-the-difference-between-fixed-effect-random-effect-and-mixed-effect-mode/>
for links.

  I don't think I can really write an answer yet.  I'm still trying to
understand at an intuitive or heuristic level what it means for
Cov(x_it,c_i)=0, where x_it is a set of explanatory variables over time
for an individual subject and c_i is the conditional mode (=BLUP in
linear mixed-model-land) for the deviation of the individual i from the
population mean ... or more particularly what it means for that
condition to be violated, which is the point at which fixed effects
would become preferred.

  As a side note, some statisticians (Andrew Gelman is the one who
springs to mind) have commented on the possible overemphasis on bias.
(All else being equal unbiased estimators are preferred to biased
estimators but all else is not always equal). Two examples: (1)
penalized estimators such as lasso/ridge regression (closely related to
mixed models) give biased parameter estimates with lower mean squared
error. (2) When estimating variability, one has to choose a particular
scale (variance, standard error, log(standard error), etc.) on which one
would prefer to get an unbiased answer.

On 16-10-11 12:02 PM, Laura Dee wrote:
> Dear all,
> Random effects are more efficient estimators ? however they come at the
> cost of the assumption that the random effect is not correlated with the
> included explanatory variables. Otherwise, using random effects leads to
> biased estimates (e.g., as laid out in Woolridge
> <https://faculty.fuqua.duke.edu/~moorman/Wooldridge,%20FE%20and%20RE.pdf>'s
> Econometrics text). This assumption is a strong one for many
> observational datasets, and most analyses in economics do not use random
> effects for this reason. *Is there a reason why observational ecological
> datasets would be fundamentally different that I am missing? Why is this
> important assumption (to have unbiased estimates from random effects)
> not emphasized in ecology? *
> 
> Thanks!
> 
> Laura
> 
> -- 
> Laura Dee
> Post-doctoral Associate
> University of Minnesota
> ledee at umn.edu <mailto:ledee at umn.edu> 
> lauraedee.com <http://lauraedee.com>


From jake.a.westfall at gmail.com  Tue Oct 11 21:32:27 2016
From: jake.a.westfall at gmail.com (Jake Westfall)
Date: Tue, 11 Oct 2016 14:32:27 -0500
Subject: [R-sig-ME] Assumptions of random effects for unbiased estimates
In-Reply-To: <c1e7e88e-b182-1783-f280-784969db1d13@gmail.com>
References: <CABEErbZX5_+i9OQO6rSz2Xskz_05WBEB_uTeHK+2mF1XG6t00Q@mail.gmail.com>
	<c1e7e88e-b182-1783-f280-784969db1d13@gmail.com>
Message-ID: <CAE9_Wg5vDaFBEVrDbKTQHO+nvVRd=Kyn=DLiaAMPtStFSfXWbQ@mail.gmail.com>

Hi Laura and Ben,

I like this paper on this topic:
http://psych.colorado.edu/~westfaja/FixedvsRandom.pdf

What it comes down to essentially is that if the cluster effects are
correlated with the "time-varying" (i.e., within-cluster varying) X
predictor -- so that, for example, some clusters have high means on X and
others have low means on X -- then there is the possibility that the
average within-cluster effect (which is what the fixed effect model
estimates) differs from the overall effect of X, not conditional on the
clusters. An extreme example of this is Simpson's paradox. Now since the
estimate from the random-effects model can be seen as a weighted average of
these two effects, it will generally be pulled to some extent away from the
fixed-effect estimate toward the unconditional estimate, which is the bias
that econometricians fret about. However, if the cluster effects are not
correlated with X, so that each cluster has the same mean on X, then this
situation is not possible, so the random-effect model will give the same
unbiased estimate as the fixed-effect model.

A simple solution to this problem is to retain the random-effect model, but
to split the predictor X into two components, one representing the
within-cluster variation of X and the other representing the
between-cluster variation of X, and estimate separate slopes for these two
effects. One can even test whether these two slopes differ from each other,
which is conceptually similar to what the Hausman test does. As described
in the paper linked above, the estimate of the within-cluster component of
the X effect equals the estimate one would obtain from a fixed-effect model.

As for the original question, I can't speak for common practice in ecology,
but I suspect it may be like it is in my home field of psychology, where we
do worry about this issue (to some extent), but we discuss it using
completely different language. That is, we discuss it in terms of whether
there are different effects of the predictor at the within-cluster and
between-cluster levels, and how our model might account for that.

Jake

On Tue, Oct 11, 2016 at 1:50 PM, Ben Bolker <bbolker at gmail.com> wrote:

>
>   I didn't respond to this offline, as it took me a while even to start
> to come up to speed on the question.  Random effects are indeed defined
> from *very* different points of view in the two communities
> ([bio]statistical vs. econometric); I'm sure there are points of
> contact, but I've been having a hard time getting my head around it all.
>
> Econometric definition:
>
> The wikipedia page <https://en.wikipedia.org/wiki/Random_effects_model>
> and CrossValidated question
> <http://stats.stackexchange.com/questions/66161/why-do-
> random-effect-models-require-the-effects-to-be-uncorrelated-with-the-inpu>
> were both helpful for me.
>
>  In the (bio)statistical world fixed and random effects are usually
> justified practically in terms of shrinkage estimators, or
> philosophically in terms of random draws from an exchangeable set of
> levels: e.g. see
> <http://stats.stackexchange.com/questions/4700/what-is-
> the-difference-between-fixed-effect-random-effect-and-mixed-effect-mode/>
> for links.
>
>   I don't think I can really write an answer yet.  I'm still trying to
> understand at an intuitive or heuristic level what it means for
> Cov(x_it,c_i)=0, where x_it is a set of explanatory variables over time
> for an individual subject and c_i is the conditional mode (=BLUP in
> linear mixed-model-land) for the deviation of the individual i from the
> population mean ... or more particularly what it means for that
> condition to be violated, which is the point at which fixed effects
> would become preferred.
>
>   As a side note, some statisticians (Andrew Gelman is the one who
> springs to mind) have commented on the possible overemphasis on bias.
> (All else being equal unbiased estimators are preferred to biased
> estimators but all else is not always equal). Two examples: (1)
> penalized estimators such as lasso/ridge regression (closely related to
> mixed models) give biased parameter estimates with lower mean squared
> error. (2) When estimating variability, one has to choose a particular
> scale (variance, standard error, log(standard error), etc.) on which one
> would prefer to get an unbiased answer.
>
> On 16-10-11 12:02 PM, Laura Dee wrote:
> > Dear all,
> > Random effects are more efficient estimators ? however they come at the
> > cost of the assumption that the random effect is not correlated with the
> > included explanatory variables. Otherwise, using random effects leads to
> > biased estimates (e.g., as laid out in Woolridge
> > <https://faculty.fuqua.duke.edu/~moorman/Wooldridge,%20FE%20and%20RE.pdf
> >'s
> > Econometrics text). This assumption is a strong one for many
> > observational datasets, and most analyses in economics do not use random
> > effects for this reason. *Is there a reason why observational ecological
> > datasets would be fundamentally different that I am missing? Why is this
> > important assumption (to have unbiased estimates from random effects)
> > not emphasized in ecology? *
> >
> > Thanks!
> >
> > Laura
> >
> > --
> > Laura Dee
> > Post-doctoral Associate
> > University of Minnesota
> > ledee at umn.edu <mailto:ledee at umn.edu>
> > lauraedee.com <http://lauraedee.com>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From jdpo223 at g.uky.edu  Wed Oct 12 03:11:19 2016
From: jdpo223 at g.uky.edu (Poe, John)
Date: Tue, 11 Oct 2016 21:11:19 -0400
Subject: [R-sig-ME] Assumptions of random effects for unbiased estimates
In-Reply-To: <CAE9_Wg5vDaFBEVrDbKTQHO+nvVRd=Kyn=DLiaAMPtStFSfXWbQ@mail.gmail.com>
References: <CABEErbZX5_+i9OQO6rSz2Xskz_05WBEB_uTeHK+2mF1XG6t00Q@mail.gmail.com>
	<c1e7e88e-b182-1783-f280-784969db1d13@gmail.com>
	<CAE9_Wg5vDaFBEVrDbKTQHO+nvVRd=Kyn=DLiaAMPtStFSfXWbQ@mail.gmail.com>
Message-ID: <CAFW8Byq7ZQrj40sM=gdf36WUTwjwh8Y_fjiZGW=Gge7-GP5qDQ@mail.gmail.com>

My reading of modern work by panel data econometricians is that they seem
very fine with the use of mixed effects models that properly differentiate
effects at different levels of analysis and the tools to do so have existed
in that literature since the early 1980s. They have been borrowing heavily
from the mixed effects literature in designing econometric models and talk
about them in panel data textbooks. This hasn't typically filtered down to
applied economists who tend to misunderstand what other fields do because
other fields just tend to talk about them differently.

The short version:
Everyone in the mixed effects literature just uses group/grand mean
centering and random coefficients to deal with endogeneity bias. If you are
an economist and someone outside of econ says mixed effects models you
should think *correlated random effects models* and not *random effects
models*.

The long version:
Economists are pretty afraid error structures that are correlated with
independent variables in general and have built up pretty elaborate
statistical models to deal with the problem. In panel data, this manifests
itself as wanting to avoid confounding effects at different levels of
analysis so that within group varying effects are segregated from between
group varying effects. It can also happen when you are omitting higher
level random effects
<http://methods.johndavidpoe.com/2016/09/09/independence-across-levels-in-mixed-effects-models/>
and they are distorting the structure of the random effects that you are
including. This is generally a good thing as you want to be able to test
hypotheses at specific levels of analysis without confounding.

It's a big enough theoretical concern in the discipline that they usually
just want to remove all between group effects from the data as a *default* to
get level one effects because it is simpler and more fool proof than
dealing with the problem in a mixed effects setting. It's so pervasive that
they are often socialized into not designing hypotheses for any between
group or cross-level variation and just focus on within group (time
varying) variability when at all possible (what economists call *within
effects*).

What economists refer to as fixed effects models just difference out all
between group variation so that it cannot contaminate within group effects
(bias level one coefficients). It's the equivalent to including group
indicator variables in the model instead of a random effect and just
accepting that you can't make substantive inferences about anything at the
group level (what economists call *between effects*).

The typical conventional wisdom in applied econometrics is to use a Hausman
test which is a generic test comparing coefficients between a random
effects model (with no level 2 covariates) and a model with all between
group variability removed from the data. If there are differences between
the two, then they prefer to go with the latter. This is bad practice
according to econometrics textbooks but applied people don't seem to care
(Baltagi 2013 ch 4.3). This only makes sense if you don't care about group
invariant variables that only differ crosssectionally and/or you think of
their effects as contamination. Panel data econometrics textbooks tend to
argue for a wider range of options here but in practice not that many
economists seem to use them.

There's an alternative framework in econ for dealing with this problem that
they call a Mundlak device (Mundlak 1978) or correlated random effects
models (Baltagi Handbook of Panel Data 2014 ch 6.3.3 or really any panel
data textbook) which is equivalent to a hierarchical linear model with
group mean centering for level-one variables. This approach is used in
econometrics by some pretty standard advanced panel data models (e.g.
Hausman-Taylor and Arellano Bond). The other alternative that is advocated
by panel data econometricians but doesn't seem to have filtered down to
rank and file economists is to use random coefficients models and just
allow the random effects to be correlated with level one variables (Hsiao
2014 chapter 6 and most of his other written work).

It is important to understand that efficiency isn't the primary reason for
use of a mixed effects model over a fixed effects model for most research.
A common reason to use a mixed effects model is that you have hypotheses
about variables operating at higher levels of analysis or cross-level
interactions and those questions cannot be answered by fixed effects panel
models that have removed all between group variability from the analysis.
You are sacrificing the ability to test group variant hypotheses by using a
basic fixed effects model over a mixed effects model. For nonlinear models
like a logistic regression it can also be very difficult to use an unbiased
fixed effects model (though there are ways in a panel setting e.g. Hahn and
Newy 2004) and trivial to use a mixed effects model.

Panel data econometricians almost always talk about typical practice among
applied economists using fixed effects as flawed (see Baltagi 2013 ch.
4.3). Mark Nerlov's 2000 History of Panel Data Econometrics is my favorite
example:

The absurdity of the contention that possible correlation between some of
> the observed explanatory variables and the individual-specific component of
> the disturbance is a ground for using fixed effects should be clear from
> the following example: Consider a panel of households with data on
> consumption and income. We are trying to estimate a consumption function.
> Income varies across households and over time. The variation across
> households is related to ability of the main earner and other household
> specific factors which vary little over time, that is to say, reflect
> mainly differences in permanent income. Such permanent differences in
> income are widely believed to be the source of most differences in
> consumption both crosssectionally and over time, whereas, variations of
> income over time are likely to be mostly transitory and unrelated to
> consumption in most categories. Yet, fixed-effects regressions are
> equivalent to using only this variation and discarding the information on
> the consumption-income relationship contained the cross-section variation
> among the household means.


See the last couple of pages of this lecture
<http://www.johndavidpoe.com/wp-content/uploads/2012/09/Blalock-Lecture.pdf>
for
the citations in the econometrics and multilevel literature that I
referenced.



On Tue, Oct 11, 2016 at 3:32 PM, Jake Westfall <jake.a.westfall at gmail.com>
wrote:

> Hi Laura and Ben,
>
> I like this paper on this topic:
> http://psych.colorado.edu/~westfaja/FixedvsRandom.pdf
>
> What it comes down to essentially is that if the cluster effects are
> correlated with the "time-varying" (i.e., within-cluster varying) X
> predictor -- so that, for example, some clusters have high means on X and
> others have low means on X -- then there is the possibility that the
> average within-cluster effect (which is what the fixed effect model
> estimates) differs from the overall effect of X, not conditional on the
> clusters. An extreme example of this is Simpson's paradox. Now since the
> estimate from the random-effects model can be seen as a weighted average of
> these two effects, it will generally be pulled to some extent away from the
> fixed-effect estimate toward the unconditional estimate, which is the bias
> that econometricians fret about. However, if the cluster effects are not
> correlated with X, so that each cluster has the same mean on X, then this
> situation is not possible, so the random-effect model will give the same
> unbiased estimate as the fixed-effect model.
>
> A simple solution to this problem is to retain the random-effect model, but
> to split the predictor X into two components, one representing the
> within-cluster variation of X and the other representing the
> between-cluster variation of X, and estimate separate slopes for these two
> effects. One can even test whether these two slopes differ from each other,
> which is conceptually similar to what the Hausman test does. As described
> in the paper linked above, the estimate of the within-cluster component of
> the X effect equals the estimate one would obtain from a fixed-effect
> model.
>
> As for the original question, I can't speak for common practice in ecology,
> but I suspect it may be like it is in my home field of psychology, where we
> do worry about this issue (to some extent), but we discuss it using
> completely different language. That is, we discuss it in terms of whether
> there are different effects of the predictor at the within-cluster and
> between-cluster levels, and how our model might account for that.
>
> Jake
>
> On Tue, Oct 11, 2016 at 1:50 PM, Ben Bolker <bbolker at gmail.com> wrote:
>
> >
> >   I didn't respond to this offline, as it took me a while even to start
> > to come up to speed on the question.  Random effects are indeed defined
> > from *very* different points of view in the two communities
> > ([bio]statistical vs. econometric); I'm sure there are points of
> > contact, but I've been having a hard time getting my head around it all.
> >
> > Econometric definition:
> >
> > The wikipedia page <https://en.wikipedia.org/wiki/Random_effects_model>
> > and CrossValidated question
> > <http://stats.stackexchange.com/questions/66161/why-do-
> > random-effect-models-require-the-effects-to-be-
> uncorrelated-with-the-inpu>
> > were both helpful for me.
> >
> >  In the (bio)statistical world fixed and random effects are usually
> > justified practically in terms of shrinkage estimators, or
> > philosophically in terms of random draws from an exchangeable set of
> > levels: e.g. see
> > <http://stats.stackexchange.com/questions/4700/what-is-
> > the-difference-between-fixed-effect-random-effect-and-
> mixed-effect-mode/>
> > for links.
> >
> >   I don't think I can really write an answer yet.  I'm still trying to
> > understand at an intuitive or heuristic level what it means for
> > Cov(x_it,c_i)=0, where x_it is a set of explanatory variables over time
> > for an individual subject and c_i is the conditional mode (=BLUP in
> > linear mixed-model-land) for the deviation of the individual i from the
> > population mean ... or more particularly what it means for that
> > condition to be violated, which is the point at which fixed effects
> > would become preferred.
> >
> >   As a side note, some statisticians (Andrew Gelman is the one who
> > springs to mind) have commented on the possible overemphasis on bias.
> > (All else being equal unbiased estimators are preferred to biased
> > estimators but all else is not always equal). Two examples: (1)
> > penalized estimators such as lasso/ridge regression (closely related to
> > mixed models) give biased parameter estimates with lower mean squared
> > error. (2) When estimating variability, one has to choose a particular
> > scale (variance, standard error, log(standard error), etc.) on which one
> > would prefer to get an unbiased answer.
> >
> > On 16-10-11 12:02 PM, Laura Dee wrote:
> > > Dear all,
> > > Random effects are more efficient estimators ? however they come at the
> > > cost of the assumption that the random effect is not correlated with
> the
> > > included explanatory variables. Otherwise, using random effects leads
> to
> > > biased estimates (e.g., as laid out in Woolridge
> > > <https://faculty.fuqua.duke.edu/~moorman/Wooldridge,%20FE%
> 20and%20RE.pdf
> > >'s
> > > Econometrics text). This assumption is a strong one for many
> > > observational datasets, and most analyses in economics do not use
> random
> > > effects for this reason. *Is there a reason why observational
> ecological
> > > datasets would be fundamentally different that I am missing? Why is
> this
> > > important assumption (to have unbiased estimates from random effects)
> > > not emphasized in ecology? *
> > >
> > > Thanks!
> > >
> > > Laura
> > >
> > > --
> > > Laura Dee
> > > Post-doctoral Associate
> > > University of Minnesota
> > > ledee at umn.edu <mailto:ledee at umn.edu>
> > > lauraedee.com <http://lauraedee.com>
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



-- 




Thanks,
John


John Poe
Doctoral Candidate
Department of Political Science
Research Methodologist
UK Center for Public Health Services & Systems Research
University of Kentucky
111 Washington Avenue, Room 203a
Lexington, KY 40536
www.johndavidpoe.com

	[[alternative HTML version deleted]]


From jake.a.westfall at gmail.com  Wed Oct 12 03:49:45 2016
From: jake.a.westfall at gmail.com (Jake Westfall)
Date: Tue, 11 Oct 2016 20:49:45 -0500
Subject: [R-sig-ME] Assumptions of random effects for unbiased estimates
In-Reply-To: <CAFW8Byq7ZQrj40sM=gdf36WUTwjwh8Y_fjiZGW=Gge7-GP5qDQ@mail.gmail.com>
References: <CABEErbZX5_+i9OQO6rSz2Xskz_05WBEB_uTeHK+2mF1XG6t00Q@mail.gmail.com>
	<c1e7e88e-b182-1783-f280-784969db1d13@gmail.com>
	<CAE9_Wg5vDaFBEVrDbKTQHO+nvVRd=Kyn=DLiaAMPtStFSfXWbQ@mail.gmail.com>
	<CAFW8Byq7ZQrj40sM=gdf36WUTwjwh8Y_fjiZGW=Gge7-GP5qDQ@mail.gmail.com>
Message-ID: <CAE9_Wg6+ZFXh-9on=nmuUwLKO6ScXjMRfbgf4y+XpGNhVAwJqA@mail.gmail.com>

What a nice contribution from John!

Jake

On Tue, Oct 11, 2016 at 8:11 PM, Poe, John <jdpo223 at g.uky.edu> wrote:

> My reading of modern work by panel data econometricians is that they seem
> very fine with the use of mixed effects models that properly differentiate
> effects at different levels of analysis and the tools to do so have existed
> in that literature since the early 1980s. They have been borrowing heavily
> from the mixed effects literature in designing econometric models and talk
> about them in panel data textbooks. This hasn't typically filtered down to
> applied economists who tend to misunderstand what other fields do because
> other fields just tend to talk about them differently.
>
> The short version:
> Everyone in the mixed effects literature just uses group/grand mean
> centering and random coefficients to deal with endogeneity bias. If you are
> an economist and someone outside of econ says mixed effects models you
> should think *correlated random effects models* and not *random effects
> models*.
>
> The long version:
> Economists are pretty afraid error structures that are correlated with
> independent variables in general and have built up pretty elaborate
> statistical models to deal with the problem. In panel data, this manifests
> itself as wanting to avoid confounding effects at different levels of
> analysis so that within group varying effects are segregated from between
> group varying effects. It can also happen when you are omitting higher
> level random effects
> <http://methods.johndavidpoe.com/2016/09/09/independence-across-levels-in-mixed-effects-models/>
> and they are distorting the structure of the random effects that you are
> including. This is generally a good thing as you want to be able to test
> hypotheses at specific levels of analysis without confounding.
>
> It's a big enough theoretical concern in the discipline that they usually
> just want to remove all between group effects from the data as a *default* to
> get level one effects because it is simpler and more fool proof than
> dealing with the problem in a mixed effects setting. It's so pervasive that
> they are often socialized into not designing hypotheses for any between
> group or cross-level variation and just focus on within group (time
> varying) variability when at all possible (what economists call *within
> effects*).
>
> What economists refer to as fixed effects models just difference out all
> between group variation so that it cannot contaminate within group effects
> (bias level one coefficients). It's the equivalent to including group
> indicator variables in the model instead of a random effect and just
> accepting that you can't make substantive inferences about anything at the
> group level (what economists call *between effects*).
>
> The typical conventional wisdom in applied econometrics is to use a
> Hausman test which is a generic test comparing coefficients between a
> random effects model (with no level 2 covariates) and a model with all
> between group variability removed from the data. If there are differences
> between the two, then they prefer to go with the latter. This is bad
> practice according to econometrics textbooks but applied people don't seem
> to care (Baltagi 2013 ch 4.3). This only makes sense if you don't care
> about group invariant variables that only differ crosssectionally and/or
> you think of their effects as contamination. Panel data econometrics
> textbooks tend to argue for a wider range of options here but in practice
> not that many economists seem to use them.
>
> There's an alternative framework in econ for dealing with this problem
> that they call a Mundlak device (Mundlak 1978) or correlated random effects
> models (Baltagi Handbook of Panel Data 2014 ch 6.3.3 or really any panel
> data textbook) which is equivalent to a hierarchical linear model with
> group mean centering for level-one variables. This approach is used in
> econometrics by some pretty standard advanced panel data models (e.g.
> Hausman-Taylor and Arellano Bond). The other alternative that is advocated
> by panel data econometricians but doesn't seem to have filtered down to
> rank and file economists is to use random coefficients models and just
> allow the random effects to be correlated with level one variables (Hsiao
> 2014 chapter 6 and most of his other written work).
>
> It is important to understand that efficiency isn't the primary reason for
> use of a mixed effects model over a fixed effects model for most research.
> A common reason to use a mixed effects model is that you have hypotheses
> about variables operating at higher levels of analysis or cross-level
> interactions and those questions cannot be answered by fixed effects panel
> models that have removed all between group variability from the analysis.
> You are sacrificing the ability to test group variant hypotheses by using a
> basic fixed effects model over a mixed effects model. For nonlinear models
> like a logistic regression it can also be very difficult to use an unbiased
> fixed effects model (though there are ways in a panel setting e.g. Hahn and
> Newy 2004) and trivial to use a mixed effects model.
>
> Panel data econometricians almost always talk about typical practice among
> applied economists using fixed effects as flawed (see Baltagi 2013 ch.
> 4.3). Mark Nerlov's 2000 History of Panel Data Econometrics is my favorite
> example:
>
> The absurdity of the contention that possible correlation between some of
>> the observed explanatory variables and the individual-specific component of
>> the disturbance is a ground for using fixed effects should be clear from
>> the following example: Consider a panel of households with data on
>> consumption and income. We are trying to estimate a consumption function.
>> Income varies across households and over time. The variation across
>> households is related to ability of the main earner and other household
>> specific factors which vary little over time, that is to say, reflect
>> mainly differences in permanent income. Such permanent differences in
>> income are widely believed to be the source of most differences in
>> consumption both crosssectionally and over time, whereas, variations of
>> income over time are likely to be mostly transitory and unrelated to
>> consumption in most categories. Yet, fixed-effects regressions are
>> equivalent to using only this variation and discarding the information on
>> the consumption-income relationship contained the cross-section variation
>> among the household means.
>
>
> See the last couple of pages of this lecture
> <http://www.johndavidpoe.com/wp-content/uploads/2012/09/Blalock-Lecture.pdf> for
> the citations in the econometrics and multilevel literature that I
> referenced.
>
>
>
> On Tue, Oct 11, 2016 at 3:32 PM, Jake Westfall <jake.a.westfall at gmail.com>
> wrote:
>
>> Hi Laura and Ben,
>>
>> I like this paper on this topic:
>> http://psych.colorado.edu/~westfaja/FixedvsRandom.pdf
>>
>> What it comes down to essentially is that if the cluster effects are
>> correlated with the "time-varying" (i.e., within-cluster varying) X
>> predictor -- so that, for example, some clusters have high means on X and
>> others have low means on X -- then there is the possibility that the
>> average within-cluster effect (which is what the fixed effect model
>> estimates) differs from the overall effect of X, not conditional on the
>> clusters. An extreme example of this is Simpson's paradox. Now since the
>> estimate from the random-effects model can be seen as a weighted average
>> of
>> these two effects, it will generally be pulled to some extent away from
>> the
>> fixed-effect estimate toward the unconditional estimate, which is the bias
>> that econometricians fret about. However, if the cluster effects are not
>> correlated with X, so that each cluster has the same mean on X, then this
>> situation is not possible, so the random-effect model will give the same
>> unbiased estimate as the fixed-effect model.
>>
>> A simple solution to this problem is to retain the random-effect model,
>> but
>> to split the predictor X into two components, one representing the
>> within-cluster variation of X and the other representing the
>> between-cluster variation of X, and estimate separate slopes for these two
>> effects. One can even test whether these two slopes differ from each
>> other,
>> which is conceptually similar to what the Hausman test does. As described
>> in the paper linked above, the estimate of the within-cluster component of
>> the X effect equals the estimate one would obtain from a fixed-effect
>> model.
>>
>> As for the original question, I can't speak for common practice in
>> ecology,
>> but I suspect it may be like it is in my home field of psychology, where
>> we
>> do worry about this issue (to some extent), but we discuss it using
>> completely different language. That is, we discuss it in terms of whether
>> there are different effects of the predictor at the within-cluster and
>> between-cluster levels, and how our model might account for that.
>>
>> Jake
>>
>> On Tue, Oct 11, 2016 at 1:50 PM, Ben Bolker <bbolker at gmail.com> wrote:
>>
>> >
>> >   I didn't respond to this offline, as it took me a while even to start
>> > to come up to speed on the question.  Random effects are indeed defined
>> > from *very* different points of view in the two communities
>> > ([bio]statistical vs. econometric); I'm sure there are points of
>> > contact, but I've been having a hard time getting my head around it all.
>> >
>> > Econometric definition:
>> >
>> > The wikipedia page <https://en.wikipedia.org/wiki/Random_effects_model>
>> > and CrossValidated question
>> > <http://stats.stackexchange.com/questions/66161/why-do-
>> > random-effect-models-require-the-effects-to-be-uncorrelated-
>> with-the-inpu>
>> > were both helpful for me.
>> >
>> >  In the (bio)statistical world fixed and random effects are usually
>> > justified practically in terms of shrinkage estimators, or
>> > philosophically in terms of random draws from an exchangeable set of
>> > levels: e.g. see
>> > <http://stats.stackexchange.com/questions/4700/what-is-
>> > the-difference-between-fixed-effect-random-effect-and-mixed-
>> effect-mode/>
>> > for links.
>> >
>> >   I don't think I can really write an answer yet.  I'm still trying to
>> > understand at an intuitive or heuristic level what it means for
>> > Cov(x_it,c_i)=0, where x_it is a set of explanatory variables over time
>> > for an individual subject and c_i is the conditional mode (=BLUP in
>> > linear mixed-model-land) for the deviation of the individual i from the
>> > population mean ... or more particularly what it means for that
>> > condition to be violated, which is the point at which fixed effects
>> > would become preferred.
>> >
>> >   As a side note, some statisticians (Andrew Gelman is the one who
>> > springs to mind) have commented on the possible overemphasis on bias.
>> > (All else being equal unbiased estimators are preferred to biased
>> > estimators but all else is not always equal). Two examples: (1)
>> > penalized estimators such as lasso/ridge regression (closely related to
>> > mixed models) give biased parameter estimates with lower mean squared
>> > error. (2) When estimating variability, one has to choose a particular
>> > scale (variance, standard error, log(standard error), etc.) on which one
>> > would prefer to get an unbiased answer.
>> >
>> > On 16-10-11 12:02 PM, Laura Dee wrote:
>> > > Dear all,
>> > > Random effects are more efficient estimators ? however they come at
>> the
>> > > cost of the assumption that the random effect is not correlated with
>> the
>> > > included explanatory variables. Otherwise, using random effects leads
>> to
>> > > biased estimates (e.g., as laid out in Woolridge
>> > > <https://faculty.fuqua.duke.edu/~moorman/Wooldridge,%20FE%20
>> and%20RE.pdf
>> > >'s
>> > > Econometrics text). This assumption is a strong one for many
>> > > observational datasets, and most analyses in economics do not use
>> random
>> > > effects for this reason. *Is there a reason why observational
>> ecological
>> > > datasets would be fundamentally different that I am missing? Why is
>> this
>> > > important assumption (to have unbiased estimates from random effects)
>> > > not emphasized in ecology? *
>> > >
>> > > Thanks!
>> > >
>> > > Laura
>> > >
>> > > --
>> > > Laura Dee
>> > > Post-doctoral Associate
>> > > University of Minnesota
>> > > ledee at umn.edu <mailto:ledee at umn.edu>
>> > > lauraedee.com <http://lauraedee.com>
>> >
>> > _______________________________________________
>> > R-sig-mixed-models at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>
>
> --
>
>
>
>
> Thanks,
> John
>
>
> John Poe
> Doctoral Candidate
> Department of Political Science
> Research Methodologist
> UK Center for Public Health Services & Systems Research
> University of Kentucky
> 111 Washington Avenue, Room 203a
> Lexington, KY 40536
> www.johndavidpoe.com
>

	[[alternative HTML version deleted]]


From jdpo223 at g.uky.edu  Wed Oct 12 04:47:41 2016
From: jdpo223 at g.uky.edu (Poe, John)
Date: Tue, 11 Oct 2016 22:47:41 -0400
Subject: [R-sig-ME] Assumptions of random effects for unbiased estimates
In-Reply-To: <CAE9_Wg6+ZFXh-9on=nmuUwLKO6ScXjMRfbgf4y+XpGNhVAwJqA@mail.gmail.com>
References: <CABEErbZX5_+i9OQO6rSz2Xskz_05WBEB_uTeHK+2mF1XG6t00Q@mail.gmail.com>
	<c1e7e88e-b182-1783-f280-784969db1d13@gmail.com>
	<CAE9_Wg5vDaFBEVrDbKTQHO+nvVRd=Kyn=DLiaAMPtStFSfXWbQ@mail.gmail.com>
	<CAFW8Byq7ZQrj40sM=gdf36WUTwjwh8Y_fjiZGW=Gge7-GP5qDQ@mail.gmail.com>
	<CAE9_Wg6+ZFXh-9on=nmuUwLKO6ScXjMRfbgf4y+XpGNhVAwJqA@mail.gmail.com>
Message-ID: <CAFW8ByrhGyML6DE=dMnmNm7xSeWB6zBDgvR_HaDy2Vnn53hnPQ@mail.gmail.com>

Thanks Jake!

On Oct 11, 2016 9:50 PM, "Jake Westfall" <jake.a.westfall at gmail.com> wrote:

> What a nice contribution from John!
>
> Jake
>
> On Tue, Oct 11, 2016 at 8:11 PM, Poe, John <jdpo223 at g.uky.edu> wrote:
>
> > My reading of modern work by panel data econometricians is that they seem
> > very fine with the use of mixed effects models that properly
> differentiate
> > effects at different levels of analysis and the tools to do so have
> existed
> > in that literature since the early 1980s. They have been borrowing
> heavily
> > from the mixed effects literature in designing econometric models and
> talk
> > about them in panel data textbooks. This hasn't typically filtered down
> to
> > applied economists who tend to misunderstand what other fields do because
> > other fields just tend to talk about them differently.
> >
> > The short version:
> > Everyone in the mixed effects literature just uses group/grand mean
> > centering and random coefficients to deal with endogeneity bias. If you
> are
> > an economist and someone outside of econ says mixed effects models you
> > should think *correlated random effects models* and not *random effects
> > models*.
> >
> > The long version:
> > Economists are pretty afraid error structures that are correlated with
> > independent variables in general and have built up pretty elaborate
> > statistical models to deal with the problem. In panel data, this
> manifests
> > itself as wanting to avoid confounding effects at different levels of
> > analysis so that within group varying effects are segregated from between
> > group varying effects. It can also happen when you are omitting higher
> > level random effects
> > <http://methods.johndavidpoe.com/2016/09/09/independence-
> across-levels-in-mixed-effects-models/>
> > and they are distorting the structure of the random effects that you are
> > including. This is generally a good thing as you want to be able to test
> > hypotheses at specific levels of analysis without confounding.
> >
> > It's a big enough theoretical concern in the discipline that they usually
> > just want to remove all between group effects from the data as a
> *default* to
> > get level one effects because it is simpler and more fool proof than
> > dealing with the problem in a mixed effects setting. It's so pervasive
> that
> > they are often socialized into not designing hypotheses for any between
> > group or cross-level variation and just focus on within group (time
> > varying) variability when at all possible (what economists call *within
> > effects*).
> >
> > What economists refer to as fixed effects models just difference out all
> > between group variation so that it cannot contaminate within group
> effects
> > (bias level one coefficients). It's the equivalent to including group
> > indicator variables in the model instead of a random effect and just
> > accepting that you can't make substantive inferences about anything at
> the
> > group level (what economists call *between effects*).
> >
> > The typical conventional wisdom in applied econometrics is to use a
> > Hausman test which is a generic test comparing coefficients between a
> > random effects model (with no level 2 covariates) and a model with all
> > between group variability removed from the data. If there are differences
> > between the two, then they prefer to go with the latter. This is bad
> > practice according to econometrics textbooks but applied people don't
> seem
> > to care (Baltagi 2013 ch 4.3). This only makes sense if you don't care
> > about group invariant variables that only differ crosssectionally and/or
> > you think of their effects as contamination. Panel data econometrics
> > textbooks tend to argue for a wider range of options here but in practice
> > not that many economists seem to use them.
> >
> > There's an alternative framework in econ for dealing with this problem
> > that they call a Mundlak device (Mundlak 1978) or correlated random
> effects
> > models (Baltagi Handbook of Panel Data 2014 ch 6.3.3 or really any panel
> > data textbook) which is equivalent to a hierarchical linear model with
> > group mean centering for level-one variables. This approach is used in
> > econometrics by some pretty standard advanced panel data models (e.g.
> > Hausman-Taylor and Arellano Bond). The other alternative that is
> advocated
> > by panel data econometricians but doesn't seem to have filtered down to
> > rank and file economists is to use random coefficients models and just
> > allow the random effects to be correlated with level one variables (Hsiao
> > 2014 chapter 6 and most of his other written work).
> >
> > It is important to understand that efficiency isn't the primary reason
> for
> > use of a mixed effects model over a fixed effects model for most
> research.
> > A common reason to use a mixed effects model is that you have hypotheses
> > about variables operating at higher levels of analysis or cross-level
> > interactions and those questions cannot be answered by fixed effects
> panel
> > models that have removed all between group variability from the analysis.
> > You are sacrificing the ability to test group variant hypotheses by
> using a
> > basic fixed effects model over a mixed effects model. For nonlinear
> models
> > like a logistic regression it can also be very difficult to use an
> unbiased
> > fixed effects model (though there are ways in a panel setting e.g. Hahn
> and
> > Newy 2004) and trivial to use a mixed effects model.
> >
> > Panel data econometricians almost always talk about typical practice
> among
> > applied economists using fixed effects as flawed (see Baltagi 2013 ch.
> > 4.3). Mark Nerlov's 2000 History of Panel Data Econometrics is my
> favorite
> > example:
> >
> > The absurdity of the contention that possible correlation between some of
> >> the observed explanatory variables and the individual-specific
> component of
> >> the disturbance is a ground for using fixed effects should be clear from
> >> the following example: Consider a panel of households with data on
> >> consumption and income. We are trying to estimate a consumption
> function.
> >> Income varies across households and over time. The variation across
> >> households is related to ability of the main earner and other household
> >> specific factors which vary little over time, that is to say, reflect
> >> mainly differences in permanent income. Such permanent differences in
> >> income are widely believed to be the source of most differences in
> >> consumption both crosssectionally and over time, whereas, variations of
> >> income over time are likely to be mostly transitory and unrelated to
> >> consumption in most categories. Yet, fixed-effects regressions are
> >> equivalent to using only this variation and discarding the information
> on
> >> the consumption-income relationship contained the cross-section
> variation
> >> among the household means.
> >
> >
> > See the last couple of pages of this lecture
> > <http://www.johndavidpoe.com/wp-content/uploads/2012/09/
> Blalock-Lecture.pdf> for
> > the citations in the econometrics and multilevel literature that I
> > referenced.
> >
> >
> >
> > On Tue, Oct 11, 2016 at 3:32 PM, Jake Westfall <
> jake.a.westfall at gmail.com>
> > wrote:
> >
> >> Hi Laura and Ben,
> >>
> >> I like this paper on this topic:
> >> http://psych.colorado.edu/~westfaja/FixedvsRandom.pdf
> >>
> >> What it comes down to essentially is that if the cluster effects are
> >> correlated with the "time-varying" (i.e., within-cluster varying) X
> >> predictor -- so that, for example, some clusters have high means on X
> and
> >> others have low means on X -- then there is the possibility that the
> >> average within-cluster effect (which is what the fixed effect model
> >> estimates) differs from the overall effect of X, not conditional on the
> >> clusters. An extreme example of this is Simpson's paradox. Now since the
> >> estimate from the random-effects model can be seen as a weighted average
> >> of
> >> these two effects, it will generally be pulled to some extent away from
> >> the
> >> fixed-effect estimate toward the unconditional estimate, which is the
> bias
> >> that econometricians fret about. However, if the cluster effects are not
> >> correlated with X, so that each cluster has the same mean on X, then
> this
> >> situation is not possible, so the random-effect model will give the same
> >> unbiased estimate as the fixed-effect model.
> >>
> >> A simple solution to this problem is to retain the random-effect model,
> >> but
> >> to split the predictor X into two components, one representing the
> >> within-cluster variation of X and the other representing the
> >> between-cluster variation of X, and estimate separate slopes for these
> two
> >> effects. One can even test whether these two slopes differ from each
> >> other,
> >> which is conceptually similar to what the Hausman test does. As
> described
> >> in the paper linked above, the estimate of the within-cluster component
> of
> >> the X effect equals the estimate one would obtain from a fixed-effect
> >> model.
> >>
> >> As for the original question, I can't speak for common practice in
> >> ecology,
> >> but I suspect it may be like it is in my home field of psychology, where
> >> we
> >> do worry about this issue (to some extent), but we discuss it using
> >> completely different language. That is, we discuss it in terms of
> whether
> >> there are different effects of the predictor at the within-cluster and
> >> between-cluster levels, and how our model might account for that.
> >>
> >> Jake
> >>
> >> On Tue, Oct 11, 2016 at 1:50 PM, Ben Bolker <bbolker at gmail.com> wrote:
> >>
> >> >
> >> >   I didn't respond to this offline, as it took me a while even to
> start
> >> > to come up to speed on the question.  Random effects are indeed
> defined
> >> > from *very* different points of view in the two communities
> >> > ([bio]statistical vs. econometric); I'm sure there are points of
> >> > contact, but I've been having a hard time getting my head around it
> all.
> >> >
> >> > Econometric definition:
> >> >
> >> > The wikipedia page <https://en.wikipedia.org/
> wiki/Random_effects_model>
> >> > and CrossValidated question
> >> > <http://stats.stackexchange.com/questions/66161/why-do-
> >> > random-effect-models-require-the-effects-to-be-uncorrelated-
> >> with-the-inpu>
> >> > were both helpful for me.
> >> >
> >> >  In the (bio)statistical world fixed and random effects are usually
> >> > justified practically in terms of shrinkage estimators, or
> >> > philosophically in terms of random draws from an exchangeable set of
> >> > levels: e.g. see
> >> > <http://stats.stackexchange.com/questions/4700/what-is-
> >> > the-difference-between-fixed-effect-random-effect-and-mixed-
> >> effect-mode/>
> >> > for links.
> >> >
> >> >   I don't think I can really write an answer yet.  I'm still trying to
> >> > understand at an intuitive or heuristic level what it means for
> >> > Cov(x_it,c_i)=0, where x_it is a set of explanatory variables over
> time
> >> > for an individual subject and c_i is the conditional mode (=BLUP in
> >> > linear mixed-model-land) for the deviation of the individual i from
> the
> >> > population mean ... or more particularly what it means for that
> >> > condition to be violated, which is the point at which fixed effects
> >> > would become preferred.
> >> >
> >> >   As a side note, some statisticians (Andrew Gelman is the one who
> >> > springs to mind) have commented on the possible overemphasis on bias.
> >> > (All else being equal unbiased estimators are preferred to biased
> >> > estimators but all else is not always equal). Two examples: (1)
> >> > penalized estimators such as lasso/ridge regression (closely related
> to
> >> > mixed models) give biased parameter estimates with lower mean squared
> >> > error. (2) When estimating variability, one has to choose a particular
> >> > scale (variance, standard error, log(standard error), etc.) on which
> one
> >> > would prefer to get an unbiased answer.
> >> >
> >> > On 16-10-11 12:02 PM, Laura Dee wrote:
> >> > > Dear all,
> >> > > Random effects are more efficient estimators ? however they come at
> >> the
> >> > > cost of the assumption that the random effect is not correlated with
> >> the
> >> > > included explanatory variables. Otherwise, using random effects
> leads
> >> to
> >> > > biased estimates (e.g., as laid out in Woolridge
> >> > > <https://faculty.fuqua.duke.edu/~moorman/Wooldridge,%20FE%20
> >> and%20RE.pdf
> >> > >'s
> >> > > Econometrics text). This assumption is a strong one for many
> >> > > observational datasets, and most analyses in economics do not use
> >> random
> >> > > effects for this reason. *Is there a reason why observational
> >> ecological
> >> > > datasets would be fundamentally different that I am missing? Why is
> >> this
> >> > > important assumption (to have unbiased estimates from random
> effects)
> >> > > not emphasized in ecology? *
> >> > >
> >> > > Thanks!
> >> > >
> >> > > Laura
> >> > >
> >> > > --
> >> > > Laura Dee
> >> > > Post-doctoral Associate
> >> > > University of Minnesota
> >> > > ledee at umn.edu <mailto:ledee at umn.edu>
> >> > > lauraedee.com <http://lauraedee.com>
> >> >
> >> > _______________________________________________
> >> > R-sig-mixed-models at r-project.org mailing list
> >> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >> >
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
> >
> >
> >
> > --
> >
> >
> >
> >
> > Thanks,
> > John
> >
> >
> > John Poe
> > Doctoral Candidate
> > Department of Political Science
> > Research Methodologist
> > UK Center for Public Health Services & Systems Research
> > University of Kentucky
> > 111 Washington Avenue, Room 203a
> > Lexington, KY 40536
> > www.johndavidpoe.com
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From M.Fairbrother at bristol.ac.uk  Wed Oct 12 10:12:53 2016
From: M.Fairbrother at bristol.ac.uk (Malcolm Fairbrother)
Date: Wed, 12 Oct 2016 09:12:53 +0100
Subject: [R-sig-ME] Assumptions of random effects for unbiased estimates
Message-ID: <CAAH-yP9GFu47dZwueSVbODnibzSwf9fjnTHvVCiESEPoot16jg@mail.gmail.com>

As others have said, there are rather peculiar inconsistencies between what
the methodological literature knows and what empirical economists actually
do.

I think the paper Jake cited (by my colleagues at Bristol/Sheffield) is
indeed one of the most useful on all this. The published version of the
paper is at: http://dx.doi.org/10.1017/psrm.2014.7

The following working paper (by them and me) takes up similar themes:
https://www.researchgate.net/publication/299604336_Fixed_and_Random_effects_making_an_informed_choice

One of the additional limitations we note here with fixed effects models
(using a simulation study) is that they can be anti-conservative, in the
sense that the SEs they return are too small if the data are generated from
a random slopes model.

In brief, Laura, fixed effects models only estimate within-group
relationships, whereas random effects (AKA multilevel, mixed) models can
estimate within- and between-group relationships. The estimation of fixed
effects models implicitly entails group mean centering (though the models
are typically written out as though unit dummies are estimated). But
group-mean centering can also be done with random effects models, with the
same benefit you get with fixed effects models (isolation of the within
effects), while still allowing for estimation of the between relationships.
You might have less confidence that the between component of some x_ij is
uncorrelated with the unit error term, but it is still possible for the
within (group-mean-centered) component to be correlated with the
observation-level error term. So I would agree that bias is worth thinking
about, but using fixed effects is no more helpful than random effects as a
solution to the problem.

Hope that's useful,
Malcolm


Dr Malcolm Fairbrother
Reader in Global Policy and Politics
School of Geographical Sciences  ?  Cabot Institute  ?  Centre for
Multilevel Modelling
University of Bristol




Date: Tue, 11 Oct 2016 20:49:45 -0500
> From: Jake Westfall <jake.a.westfall at gmail.com>
> To: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Assumptions of random effects for unbiased
>         estimates
> Message-ID:
>         <CAE9_Wg6+ZFXh-9on=nmuUwLKO6ScXjMRfbgf4y+XpGNhVAwJqA at mail.
> gmail.com>
> Content-Type: text/plain; charset="UTF-8"
>
> What a nice contribution from John!
>
> Jake
>
> On Tue, Oct 11, 2016 at 8:11 PM, Poe, John <jdpo223 at g.uky.edu> wrote:
>
> > My reading of modern work by panel data econometricians is that they seem
> > very fine with the use of mixed effects models that properly
> differentiate
> > effects at different levels of analysis and the tools to do so have
> existed
> > in that literature since the early 1980s. They have been borrowing
> heavily
> > from the mixed effects literature in designing econometric models and
> talk
> > about them in panel data textbooks. This hasn't typically filtered down
> to
> > applied economists who tend to misunderstand what other fields do because
> > other fields just tend to talk about them differently.
> >
> > The short version:
> > Everyone in the mixed effects literature just uses group/grand mean
> > centering and random coefficients to deal with endogeneity bias. If you
> are
> > an economist and someone outside of econ says mixed effects models you
> > should think *correlated random effects models* and not *random effects
> > models*.
> >
> > The long version:
> > Economists are pretty afraid error structures that are correlated with
> > independent variables in general and have built up pretty elaborate
> > statistical models to deal with the problem. In panel data, this
> manifests
> > itself as wanting to avoid confounding effects at different levels of
> > analysis so that within group varying effects are segregated from between
> > group varying effects. It can also happen when you are omitting higher
> > level random effects
> > <http://methods.johndavidpoe.com/2016/09/09/independence-
> across-levels-in-mixed-effects-models/>
> > and they are distorting the structure of the random effects that you are
> > including. This is generally a good thing as you want to be able to test
> > hypotheses at specific levels of analysis without confounding.
> >
> > It's a big enough theoretical concern in the discipline that they usually
> > just want to remove all between group effects from the data as a
> *default* to
> > get level one effects because it is simpler and more fool proof than
> > dealing with the problem in a mixed effects setting. It's so pervasive
> that
> > they are often socialized into not designing hypotheses for any between
> > group or cross-level variation and just focus on within group (time
> > varying) variability when at all possible (what economists call *within
> > effects*).
> >
> > What economists refer to as fixed effects models just difference out all
> > between group variation so that it cannot contaminate within group
> effects
> > (bias level one coefficients). It's the equivalent to including group
> > indicator variables in the model instead of a random effect and just
> > accepting that you can't make substantive inferences about anything at
> the
> > group level (what economists call *between effects*).
> >
> > The typical conventional wisdom in applied econometrics is to use a
> > Hausman test which is a generic test comparing coefficients between a
> > random effects model (with no level 2 covariates) and a model with all
> > between group variability removed from the data. If there are differences
> > between the two, then they prefer to go with the latter. This is bad
> > practice according to econometrics textbooks but applied people don't
> seem
> > to care (Baltagi 2013 ch 4.3). This only makes sense if you don't care
> > about group invariant variables that only differ crosssectionally and/or
> > you think of their effects as contamination. Panel data econometrics
> > textbooks tend to argue for a wider range of options here but in practice
> > not that many economists seem to use them.
> >
> > There's an alternative framework in econ for dealing with this problem
> > that they call a Mundlak device (Mundlak 1978) or correlated random
> effects
> > models (Baltagi Handbook of Panel Data 2014 ch 6.3.3 or really any panel
> > data textbook) which is equivalent to a hierarchical linear model with
> > group mean centering for level-one variables. This approach is used in
> > econometrics by some pretty standard advanced panel data models (e.g.
> > Hausman-Taylor and Arellano Bond). The other alternative that is
> advocated
> > by panel data econometricians but doesn't seem to have filtered down to
> > rank and file economists is to use random coefficients models and just
> > allow the random effects to be correlated with level one variables (Hsiao
> > 2014 chapter 6 and most of his other written work).
> >
> > It is important to understand that efficiency isn't the primary reason
> for
> > use of a mixed effects model over a fixed effects model for most
> research.
> > A common reason to use a mixed effects model is that you have hypotheses
> > about variables operating at higher levels of analysis or cross-level
> > interactions and those questions cannot be answered by fixed effects
> panel
> > models that have removed all between group variability from the analysis.
> > You are sacrificing the ability to test group variant hypotheses by
> using a
> > basic fixed effects model over a mixed effects model. For nonlinear
> models
> > like a logistic regression it can also be very difficult to use an
> unbiased
> > fixed effects model (though there are ways in a panel setting e.g. Hahn
> and
> > Newy 2004) and trivial to use a mixed effects model.
> >
> > Panel data econometricians almost always talk about typical practice
> among
> > applied economists using fixed effects as flawed (see Baltagi 2013 ch.
> > 4.3). Mark Nerlov's 2000 History of Panel Data Econometrics is my
> favorite
> > example:
> >
> > The absurdity of the contention that possible correlation between some of
> >> the observed explanatory variables and the individual-specific
> component of
> >> the disturbance is a ground for using fixed effects should be clear from
> >> the following example: Consider a panel of households with data on
> >> consumption and income. We are trying to estimate a consumption
> function.
> >> Income varies across households and over time. The variation across
> >> households is related to ability of the main earner and other household
> >> specific factors which vary little over time, that is to say, reflect
> >> mainly differences in permanent income. Such permanent differences in
> >> income are widely believed to be the source of most differences in
> >> consumption both crosssectionally and over time, whereas, variations of
> >> income over time are likely to be mostly transitory and unrelated to
> >> consumption in most categories. Yet, fixed-effects regressions are
> >> equivalent to using only this variation and discarding the information
> on
> >> the consumption-income relationship contained the cross-section
> variation
> >> among the household means.
> >
> >
> > See the last couple of pages of this lecture
> > <http://www.johndavidpoe.com/wp-content/uploads/2012/09/
> Blalock-Lecture.pdf> for
> > the citations in the econometrics and multilevel literature that I
> > referenced.
> >
> >
> >
> > On Tue, Oct 11, 2016 at 3:32 PM, Jake Westfall <
> jake.a.westfall at gmail.com>
> > wrote:
> >
> >> Hi Laura and Ben,
> >>
> >> I like this paper on this topic:
> >> http://psych.colorado.edu/~westfaja/FixedvsRandom.pdf
> >>
> >> What it comes down to essentially is that if the cluster effects are
> >> correlated with the "time-varying" (i.e., within-cluster varying) X
> >> predictor -- so that, for example, some clusters have high means on X
> and
> >> others have low means on X -- then there is the possibility that the
> >> average within-cluster effect (which is what the fixed effect model
> >> estimates) differs from the overall effect of X, not conditional on the
> >> clusters. An extreme example of this is Simpson's paradox. Now since the
> >> estimate from the random-effects model can be seen as a weighted average
> >> of
> >> these two effects, it will generally be pulled to some extent away from
> >> the
> >> fixed-effect estimate toward the unconditional estimate, which is the
> bias
> >> that econometricians fret about. However, if the cluster effects are not
> >> correlated with X, so that each cluster has the same mean on X, then
> this
> >> situation is not possible, so the random-effect model will give the same
> >> unbiased estimate as the fixed-effect model.
> >>
> >> A simple solution to this problem is to retain the random-effect model,
> >> but
> >> to split the predictor X into two components, one representing the
> >> within-cluster variation of X and the other representing the
> >> between-cluster variation of X, and estimate separate slopes for these
> two
> >> effects. One can even test whether these two slopes differ from each
> >> other,
> >> which is conceptually similar to what the Hausman test does. As
> described
> >> in the paper linked above, the estimate of the within-cluster component
> of
> >> the X effect equals the estimate one would obtain from a fixed-effect
> >> model.
> >>
> >> As for the original question, I can't speak for common practice in
> >> ecology,
> >> but I suspect it may be like it is in my home field of psychology, where
> >> we
> >> do worry about this issue (to some extent), but we discuss it using
> >> completely different language. That is, we discuss it in terms of
> whether
> >> there are different effects of the predictor at the within-cluster and
> >> between-cluster levels, and how our model might account for that.
> >>
> >> Jake
> >>
> >> On Tue, Oct 11, 2016 at 1:50 PM, Ben Bolker <bbolker at gmail.com> wrote:
> >>
> >> >
> >> >   I didn't respond to this offline, as it took me a while even to
> start
> >> > to come up to speed on the question.  Random effects are indeed
> defined
> >> > from *very* different points of view in the two communities
> >> > ([bio]statistical vs. econometric); I'm sure there are points of
> >> > contact, but I've been having a hard time getting my head around it
> all.
> >> >
> >> > Econometric definition:
> >> >
> >> > The wikipedia page <https://en.wikipedia.org/
> wiki/Random_effects_model>
> >> > and CrossValidated question
> >> > <http://stats.stackexchange.com/questions/66161/why-do-
> >> > random-effect-models-require-the-effects-to-be-uncorrelated-
> >> with-the-inpu>
> >> > were both helpful for me.
> >> >
> >> >  In the (bio)statistical world fixed and random effects are usually
> >> > justified practically in terms of shrinkage estimators, or
> >> > philosophically in terms of random draws from an exchangeable set of
> >> > levels: e.g. see
> >> > <http://stats.stackexchange.com/questions/4700/what-is-
> >> > the-difference-between-fixed-effect-random-effect-and-mixed-
> >> effect-mode/>
> >> > for links.
> >> >
> >> >   I don't think I can really write an answer yet.  I'm still trying to
> >> > understand at an intuitive or heuristic level what it means for
> >> > Cov(x_it,c_i)=0, where x_it is a set of explanatory variables over
> time
> >> > for an individual subject and c_i is the conditional mode (=BLUP in
> >> > linear mixed-model-land) for the deviation of the individual i from
> the
> >> > population mean ... or more particularly what it means for that
> >> > condition to be violated, which is the point at which fixed effects
> >> > would become preferred.
> >> >
> >> >   As a side note, some statisticians (Andrew Gelman is the one who
> >> > springs to mind) have commented on the possible overemphasis on bias.
> >> > (All else being equal unbiased estimators are preferred to biased
> >> > estimators but all else is not always equal). Two examples: (1)
> >> > penalized estimators such as lasso/ridge regression (closely related
> to
> >> > mixed models) give biased parameter estimates with lower mean squared
> >> > error. (2) When estimating variability, one has to choose a particular
> >> > scale (variance, standard error, log(standard error), etc.) on which
> one
> >> > would prefer to get an unbiased answer.
> >> >
> >> > On 16-10-11 12:02 PM, Laura Dee wrote:
> >> > > Dear all,
> >> > > Random effects are more efficient estimators ? however they come at
> >> the
> >> > > cost of the assumption that the random effect is not correlated with
> >> the
> >> > > included explanatory variables. Otherwise, using random effects
> leads
> >> to
> >> > > biased estimates (e.g., as laid out in Woolridge
> >> > > <https://faculty.fuqua.duke.edu/~moorman/Wooldridge,%20FE%20
> >> and%20RE.pdf
> >> > >'s
> >> > > Econometrics text). This assumption is a strong one for many
> >> > > observational datasets, and most analyses in economics do not use
> >> random
> >> > > effects for this reason. *Is there a reason why observational
> >> ecological
> >> > > datasets would be fundamentally different that I am missing? Why is
> >> this
> >> > > important assumption (to have unbiased estimates from random
> effects)
> >> > > not emphasized in ecology? *
> >> > >
> >> > > Thanks!
> >> > >
> >> > > Laura
> >> > >
> >> > > --
> >> > > Laura Dee
> >> > > Post-doctoral Associate
> >> > > University of Minnesota
> >> > > ledee at umn.edu <mailto:ledee at umn.edu>
> >> > > lauraedee.com <http://lauraedee.com>
> >> >
> >> > _______________________________________________
> >> > R-sig-mixed-models at r-project.org mailing list
> >> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >> >
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
> >
> >
> >
> > --
> >
> >
> >
> >
> > Thanks,
> > John
> >
> >
> > John Poe
> > Doctoral Candidate
> > Department of Political Science
> > Research Methodologist
> > UK Center for Public Health Services & Systems Research
> > University of Kentucky
> > 111 Washington Avenue, Room 203a
> > Lexington, KY 40536
> > www.johndavidpoe.com
> >
>
>         [[alternative HTML version deleted]]
>
>
>
> ------------------------------
>
> Message: 2
> Date: Tue, 11 Oct 2016 22:47:41 -0400
> From: "Poe, John" <jdpo223 at g.uky.edu>
> To: Jake Westfall <jake.a.westfall at gmail.com>
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] Assumptions of random effects for unbiased
>         estimates
> Message-ID:
>         <CAFW8ByrhGyML6DE=dMnmNm7xSeWB6zBDgvR_HaDy2Vnn53hnPQ at mail.
> gmail.com>
> Content-Type: text/plain; charset="UTF-8"
>
> Thanks Jake!
>
> On Oct 11, 2016 9:50 PM, "Jake Westfall" <jake.a.westfall at gmail.com>
> wrote:
>
> > What a nice contribution from John!
> >
> > Jake
> >
> > On Tue, Oct 11, 2016 at 8:11 PM, Poe, John <jdpo223 at g.uky.edu> wrote:
> >
> > > My reading of modern work by panel data econometricians is that they
> seem
> > > very fine with the use of mixed effects models that properly
> > differentiate
> > > effects at different levels of analysis and the tools to do so have
> > existed
> > > in that literature since the early 1980s. They have been borrowing
> > heavily
> > > from the mixed effects literature in designing econometric models and
> > talk
> > > about them in panel data textbooks. This hasn't typically filtered down
> > to
> > > applied economists who tend to misunderstand what other fields do
> because
> > > other fields just tend to talk about them differently.
> > >
> > > The short version:
> > > Everyone in the mixed effects literature just uses group/grand mean
> > > centering and random coefficients to deal with endogeneity bias. If you
> > are
> > > an economist and someone outside of econ says mixed effects models you
> > > should think *correlated random effects models* and not *random effects
> > > models*.
> > >
> > > The long version:
> > > Economists are pretty afraid error structures that are correlated with
> > > independent variables in general and have built up pretty elaborate
> > > statistical models to deal with the problem. In panel data, this
> > manifests
> > > itself as wanting to avoid confounding effects at different levels of
> > > analysis so that within group varying effects are segregated from
> between
> > > group varying effects. It can also happen when you are omitting higher
> > > level random effects
> > > <http://methods.johndavidpoe.com/2016/09/09/independence-
> > across-levels-in-mixed-effects-models/>
> > > and they are distorting the structure of the random effects that you
> are
> > > including. This is generally a good thing as you want to be able to
> test
> > > hypotheses at specific levels of analysis without confounding.
> > >
> > > It's a big enough theoretical concern in the discipline that they
> usually
> > > just want to remove all between group effects from the data as a
> > *default* to
> > > get level one effects because it is simpler and more fool proof than
> > > dealing with the problem in a mixed effects setting. It's so pervasive
> > that
> > > they are often socialized into not designing hypotheses for any between
> > > group or cross-level variation and just focus on within group (time
> > > varying) variability when at all possible (what economists call *within
> > > effects*).
> > >
> > > What economists refer to as fixed effects models just difference out
> all
> > > between group variation so that it cannot contaminate within group
> > effects
> > > (bias level one coefficients). It's the equivalent to including group
> > > indicator variables in the model instead of a random effect and just
> > > accepting that you can't make substantive inferences about anything at
> > the
> > > group level (what economists call *between effects*).
> > >
> > > The typical conventional wisdom in applied econometrics is to use a
> > > Hausman test which is a generic test comparing coefficients between a
> > > random effects model (with no level 2 covariates) and a model with all
> > > between group variability removed from the data. If there are
> differences
> > > between the two, then they prefer to go with the latter. This is bad
> > > practice according to econometrics textbooks but applied people don't
> > seem
> > > to care (Baltagi 2013 ch 4.3). This only makes sense if you don't care
> > > about group invariant variables that only differ crosssectionally
> and/or
> > > you think of their effects as contamination. Panel data econometrics
> > > textbooks tend to argue for a wider range of options here but in
> practice
> > > not that many economists seem to use them.
> > >
> > > There's an alternative framework in econ for dealing with this problem
> > > that they call a Mundlak device (Mundlak 1978) or correlated random
> > effects
> > > models (Baltagi Handbook of Panel Data 2014 ch 6.3.3 or really any
> panel
> > > data textbook) which is equivalent to a hierarchical linear model with
> > > group mean centering for level-one variables. This approach is used in
> > > econometrics by some pretty standard advanced panel data models (e.g.
> > > Hausman-Taylor and Arellano Bond). The other alternative that is
> > advocated
> > > by panel data econometricians but doesn't seem to have filtered down to
> > > rank and file economists is to use random coefficients models and just
> > > allow the random effects to be correlated with level one variables
> (Hsiao
> > > 2014 chapter 6 and most of his other written work).
> > >
> > > It is important to understand that efficiency isn't the primary reason
> > for
> > > use of a mixed effects model over a fixed effects model for most
> > research.
> > > A common reason to use a mixed effects model is that you have
> hypotheses
> > > about variables operating at higher levels of analysis or cross-level
> > > interactions and those questions cannot be answered by fixed effects
> > panel
> > > models that have removed all between group variability from the
> analysis.
> > > You are sacrificing the ability to test group variant hypotheses by
> > using a
> > > basic fixed effects model over a mixed effects model. For nonlinear
> > models
> > > like a logistic regression it can also be very difficult to use an
> > unbiased
> > > fixed effects model (though there are ways in a panel setting e.g. Hahn
> > and
> > > Newy 2004) and trivial to use a mixed effects model.
> > >
> > > Panel data econometricians almost always talk about typical practice
> > among
> > > applied economists using fixed effects as flawed (see Baltagi 2013 ch.
> > > 4.3). Mark Nerlov's 2000 History of Panel Data Econometrics is my
> > favorite
> > > example:
> > >
> > > The absurdity of the contention that possible correlation between some
> of
> > >> the observed explanatory variables and the individual-specific
> > component of
> > >> the disturbance is a ground for using fixed effects should be clear
> from
> > >> the following example: Consider a panel of households with data on
> > >> consumption and income. We are trying to estimate a consumption
> > function.
> > >> Income varies across households and over time. The variation across
> > >> households is related to ability of the main earner and other
> household
> > >> specific factors which vary little over time, that is to say, reflect
> > >> mainly differences in permanent income. Such permanent differences in
> > >> income are widely believed to be the source of most differences in
> > >> consumption both crosssectionally and over time, whereas, variations
> of
> > >> income over time are likely to be mostly transitory and unrelated to
> > >> consumption in most categories. Yet, fixed-effects regressions are
> > >> equivalent to using only this variation and discarding the information
> > on
> > >> the consumption-income relationship contained the cross-section
> > variation
> > >> among the household means.
> > >
> > >
> > > See the last couple of pages of this lecture
> > > <http://www.johndavidpoe.com/wp-content/uploads/2012/09/
> > Blalock-Lecture.pdf> for
> > > the citations in the econometrics and multilevel literature that I
> > > referenced.
> > >
> > >
> > >
> > > On Tue, Oct 11, 2016 at 3:32 PM, Jake Westfall <
> > jake.a.westfall at gmail.com>
> > > wrote:
> > >
> > >> Hi Laura and Ben,
> > >>
> > >> I like this paper on this topic:
> > >> http://psych.colorado.edu/~westfaja/FixedvsRandom.pdf
> > >>
> > >> What it comes down to essentially is that if the cluster effects are
> > >> correlated with the "time-varying" (i.e., within-cluster varying) X
> > >> predictor -- so that, for example, some clusters have high means on X
> > and
> > >> others have low means on X -- then there is the possibility that the
> > >> average within-cluster effect (which is what the fixed effect model
> > >> estimates) differs from the overall effect of X, not conditional on
> the
> > >> clusters. An extreme example of this is Simpson's paradox. Now since
> the
> > >> estimate from the random-effects model can be seen as a weighted
> average
> > >> of
> > >> these two effects, it will generally be pulled to some extent away
> from
> > >> the
> > >> fixed-effect estimate toward the unconditional estimate, which is the
> > bias
> > >> that econometricians fret about. However, if the cluster effects are
> not
> > >> correlated with X, so that each cluster has the same mean on X, then
> > this
> > >> situation is not possible, so the random-effect model will give the
> same
> > >> unbiased estimate as the fixed-effect model.
> > >>
> > >> A simple solution to this problem is to retain the random-effect
> model,
> > >> but
> > >> to split the predictor X into two components, one representing the
> > >> within-cluster variation of X and the other representing the
> > >> between-cluster variation of X, and estimate separate slopes for these
> > two
> > >> effects. One can even test whether these two slopes differ from each
> > >> other,
> > >> which is conceptually similar to what the Hausman test does. As
> > described
> > >> in the paper linked above, the estimate of the within-cluster
> component
> > of
> > >> the X effect equals the estimate one would obtain from a fixed-effect
> > >> model.
> > >>
> > >> As for the original question, I can't speak for common practice in
> > >> ecology,
> > >> but I suspect it may be like it is in my home field of psychology,
> where
> > >> we
> > >> do worry about this issue (to some extent), but we discuss it using
> > >> completely different language. That is, we discuss it in terms of
> > whether
> > >> there are different effects of the predictor at the within-cluster and
> > >> between-cluster levels, and how our model might account for that.
> > >>
> > >> Jake
> > >>
> > >> On Tue, Oct 11, 2016 at 1:50 PM, Ben Bolker <bbolker at gmail.com>
> wrote:
> > >>
> > >> >
> > >> >   I didn't respond to this offline, as it took me a while even to
> > start
> > >> > to come up to speed on the question.  Random effects are indeed
> > defined
> > >> > from *very* different points of view in the two communities
> > >> > ([bio]statistical vs. econometric); I'm sure there are points of
> > >> > contact, but I've been having a hard time getting my head around it
> > all.
> > >> >
> > >> > Econometric definition:
> > >> >
> > >> > The wikipedia page <https://en.wikipedia.org/
> > wiki/Random_effects_model>
> > >> > and CrossValidated question
> > >> > <http://stats.stackexchange.com/questions/66161/why-do-
> > >> > random-effect-models-require-the-effects-to-be-uncorrelated-
> > >> with-the-inpu>
> > >> > were both helpful for me.
> > >> >
> > >> >  In the (bio)statistical world fixed and random effects are usually
> > >> > justified practically in terms of shrinkage estimators, or
> > >> > philosophically in terms of random draws from an exchangeable set of
> > >> > levels: e.g. see
> > >> > <http://stats.stackexchange.com/questions/4700/what-is-
> > >> > the-difference-between-fixed-effect-random-effect-and-mixed-
> > >> effect-mode/>
> > >> > for links.
> > >> >
> > >> >   I don't think I can really write an answer yet.  I'm still trying
> to
> > >> > understand at an intuitive or heuristic level what it means for
> > >> > Cov(x_it,c_i)=0, where x_it is a set of explanatory variables over
> > time
> > >> > for an individual subject and c_i is the conditional mode (=BLUP in
> > >> > linear mixed-model-land) for the deviation of the individual i from
> > the
> > >> > population mean ... or more particularly what it means for that
> > >> > condition to be violated, which is the point at which fixed effects
> > >> > would become preferred.
> > >> >
> > >> >   As a side note, some statisticians (Andrew Gelman is the one who
> > >> > springs to mind) have commented on the possible overemphasis on
> bias.
> > >> > (All else being equal unbiased estimators are preferred to biased
> > >> > estimators but all else is not always equal). Two examples: (1)
> > >> > penalized estimators such as lasso/ridge regression (closely related
> > to
> > >> > mixed models) give biased parameter estimates with lower mean
> squared
> > >> > error. (2) When estimating variability, one has to choose a
> particular
> > >> > scale (variance, standard error, log(standard error), etc.) on which
> > one
> > >> > would prefer to get an unbiased answer.
> > >> >
> > >> > On 16-10-11 12:02 PM, Laura Dee wrote:
> > >> > > Dear all,
> > >> > > Random effects are more efficient estimators ? however they come
> at
> > >> the
> > >> > > cost of the assumption that the random effect is not correlated
> with
> > >> the
> > >> > > included explanatory variables. Otherwise, using random effects
> > leads
> > >> to
> > >> > > biased estimates (e.g., as laid out in Woolridge
> > >> > > <https://faculty.fuqua.duke.edu/~moorman/Wooldridge,%20FE%20
> > >> and%20RE.pdf
> > >> > >'s
> > >> > > Econometrics text). This assumption is a strong one for many
> > >> > > observational datasets, and most analyses in economics do not use
> > >> random
> > >> > > effects for this reason. *Is there a reason why observational
> > >> ecological
> > >> > > datasets would be fundamentally different that I am missing? Why
> is
> > >> this
> > >> > > important assumption (to have unbiased estimates from random
> > effects)
> > >> > > not emphasized in ecology? *
> > >> > >
> > >> > > Thanks!
> > >> > >
> > >> > > Laura
> > >> > >
> > >> > > --
> > >> > > Laura Dee
> > >> > > Post-doctoral Associate
> > >> > > University of Minnesota
> > >> > > ledee at umn.edu <mailto:ledee at umn.edu>
> > >> > > lauraedee.com <http://lauraedee.com>
> > >> >
> > >> > _______________________________________________
> > >> > R-sig-mixed-models at r-project.org mailing list
> > >> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > >> >
> > >>
> > >>         [[alternative HTML version deleted]]
> > >>
> > >> _______________________________________________
> > >> R-sig-mixed-models at r-project.org mailing list
> > >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > >>
> > >
> > >
> > >
> > > --
> > >
> > >
> > >
> > >
> > > Thanks,
> > > John
> > >
> > >
> > > John Poe
> > > Doctoral Candidate
> > > Department of Political Science
> > > Research Methodologist
> > > UK Center for Public Health Services & Systems Research
> > > University of Kentucky
> > > 111 Washington Avenue, Room 203a
> > > Lexington, KY 40536
> > > www.johndavidpoe.com
>

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Wed Oct 12 10:26:10 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Wed, 12 Oct 2016 10:26:10 +0200
Subject: [R-sig-ME] Exponent random effect in nlmer
In-Reply-To: <D4227B1D.4ED19%tim.cole@ucl.ac.uk>
References: <D42269FD.4ECAF%tim.cole@ucl.ac.uk>
	<CAJuCY5wmX6jYMvrtjf_M2LYWOt8gYXq0vSCqJzOTb7sCnHbUnw@mail.gmail.com>
	<D4227B1D.4ED19%tim.cole@ucl.ac.uk>
Message-ID: <CAJuCY5z0m9JrEEjHeLtuW7Ta7nDtgFk0c+ZNmiXk_fxh5STeFg@mail.gmail.com>

Hi Tim,

AFAIK nlmer requires the fixed and random effects to be additive. The model
to be used _after_ this this summation can be non linear.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-10-11 12:30 GMT+02:00 Cole, Tim <tim.cole at ucl.ac.uk>:

> Dear Thierry,
>
> Thanks very much for your speedy response.
>
> I agree my model looks odd, but it has a theoretical basis which I'd
> prefer not to spell out at this stage. Suffice to say that
> ? -Inf < y < Inf
> ? 0 < E(y) < 1
> ? there is a subject random effect.
>
> For these reasons the usual models and/or transformations won't work,
> whereas my proposed exponent random effect ought to. I just need to fit it,
> to see if I'm right!
>
> Best wishes,
> Tim
> ---
> Tim.cole at ucl.ac.uk Phone +44(0)20 7905 2666 Fax +44(0)20 7905 2381
> Population, Policy and Practice Programme
> UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK
>
>
> From: Thierry Onkelinx <thierry.onkelinx at inbo.be>
> Date: Tuesday, 11 October 2016 11:06
> To: Tim Cole <tim.cole at ucl.ac.uk>
> Cc: "r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] Exponent random effect in nlmer
>
> Dear Tim,
>
> y centred on 0 and a valid range (0, 1) seems to be conflicting
> statements.
>
> Here a some solutions depending on y
>
> - y stems from a binomial process
>      - use a binomial glmm.
> - y is continuous and you are willing to transform y
>     - 0 < y <  1
>         - apply a logit transformation on y. lmer(plogis(y) ~ f + (1 |
> id) )
>     - 0 <= y < 1
>         - apply a log transformation on y. lmer(log(y) ~ f + (1 | id) )
>     - 0 < y <= 1
>         - apply a log transformation on 1 - y. lmer(log(1 - y) ~ f + (1 |
> id) )
> - y is continuous are not willing to transform y
>    - use a beta regression with 0 and/or 1 inflation in case you have 0 or
> 1 in the data. Have a look at the gamlss package to fit this model.
>
> Best regards,
>
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-10-11 11:29 GMT+02:00 Cole, Tim <tim.cole at ucl.ac.uk>:
>
>> I have a model of the form
>>   m1 <- lmer(y ~ f + (1 | id) )
>> where y is a continuous variable centred on zero, f is a unordered factor
>> with coefficients b such 0 < b < 1, and there is a signficant random
>> subject intercept.
>>
>> The random intercept can lead to predicted values outside the valid range
>> (0, 1). For this reason I'd like to reformulate the model as
>> m2 <- nlmer(y ~ (f - 1) ^ exp(1 | id) )   (using a invalid but I hope
>> obvious notation), where the random effect is now a power centred on 1.
>> This would constrain the fitted values to be within c(0, 1).
>>
>> My question is: can this be done in nlmer, and if so how? Please can
>> someone point me in the right direction?
>>
>> Thanks,
>> Tim Cole
>> ---
>> Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk> Phone +44(0)20 7905 2666
>> Fax +44(0)20 7905 2381
>> Population, Policy and Practice Programme
>> UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK
>>
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>

	[[alternative HTML version deleted]]


From tim.cole at ucl.ac.uk  Wed Oct 12 11:02:17 2016
From: tim.cole at ucl.ac.uk (Cole, Tim)
Date: Wed, 12 Oct 2016 09:02:17 +0000
Subject: [R-sig-ME] Exponent random effect in nlmer
In-Reply-To: <CAFW8BypL9U8ve4fsi4tYAi8CGHuF1fpdHi3_m2VK=Rn4b2kKHA@mail.gmail.com>
References: <D42269FD.4ECAF%tim.cole@ucl.ac.uk>
	<CAJuCY5wmX6jYMvrtjf_M2LYWOt8gYXq0vSCqJzOTb7sCnHbUnw@mail.gmail.com>
	<D4227B1D.4ED19%tim.cole@ucl.ac.uk>
	<CAFW8BypL9U8ve4fsi4tYAi8CGHuF1fpdHi3_m2VK=Rn4b2kKHA@mail.gmail.com>
Message-ID: <D423B8FF.14B6E%tim.cole@ucl.ac.uk>

That?s a useful thought, though I?m not sure how I would estimate the random effect in a null model.

Also, incorporating it in the full model as a covariate would need it to be additive, which would violate the constraint on E(y).
---
tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk> Phone +44(0)20 7905 2666 Fax +44(0)20 7905 2381
Population Policy and Practice Programme
UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK

From: "Poe, John" <jdpo223 at g.uky.edu<mailto:jdpo223 at g.uky.edu>>
Date: Tuesday, 11 October 2016 15:17
To: Tim Cole <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk>>
Cc: Thierry Onkelinx <thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>>, "r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>" <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: Re: [R-sig-ME] Exponent random effect in nlmer


If you can't use a link function to constrain the parameters then you might have to use a two stage model where you estimate the random effect in a null model, generate the random effect directly as a variable, transform it, and incorporate it in the full model as a covariate.

On Oct 11, 2016 6:31 AM, "Cole, Tim" <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk>> wrote:
Dear Thierry,

Thanks very much for your speedy response.

I agree my model looks odd, but it has a theoretical basis which I'd prefer not to spell out at this stage. Suffice to say that
* -Inf < y < Inf
* 0 < E(y) < 1
* there is a subject random effect.

For these reasons the usual models and/or transformations won't work, whereas my proposed exponent random effect ought to. I just need to fit it, to see if I'm right!

Best wishes,
Tim
---
Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk><mailto:Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk>> Phone +44(0)20 7905 2666 Fax +44(0)20 7905 2381
Population, Policy and Practice Programme
UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK


From: Thierry Onkelinx <thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be><mailto:thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>>>
Date: Tuesday, 11 October 2016 11:06
To: Tim Cole <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk><mailto:tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk>>>
Cc: "r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org><mailto:r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>" <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org><mailto:r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>>
Subject: Re: [R-sig-ME] Exponent random effect in nlmer

Dear Tim,

y centred on 0 and a valid range (0, 1) seems to be conflicting statements.

Here a some solutions depending on y

- y stems from a binomial process
     - use a binomial glmm.
- y is continuous and you are willing to transform y
    - 0 < y <  1
        - apply a logit transformation on y. lmer(plogis(y) ~ f + (1 | id) )
    - 0 <= y < 1
        - apply a log transformation on y. lmer(log(y) ~ f + (1 | id) )
    - 0 < y <= 1
        - apply a log transformation on 1 - y. lmer(log(1 - y) ~ f + (1 | id) )
- y is continuous are not willing to transform y
   - use a beta regression with 0 and/or 1 inflation in case you have 0 or 1 in the data. Have a look at the gamlss package to fit this model.

Best regards,


ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey

2016-10-11 11:29 GMT+02:00 Cole, Tim <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk><mailto:tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk>>>:
I have a model of the form
  m1 <- lmer(y ~ f + (1 | id) )
where y is a continuous variable centred on zero, f is a unordered factor with coefficients b such 0 < b < 1, and there is a signficant random subject intercept.

The random intercept can lead to predicted values outside the valid range (0, 1). For this reason I'd like to reformulate the model as
m2 <- nlmer(y ~ (f - 1) ^ exp(1 | id) )   (using a invalid but I hope obvious notation), where the random effect is now a power centred on 1. This would constrain the fitted values to be within c(0, 1).

My question is: can this be done in nlmer, and if so how? Please can someone point me in the right direction?

Thanks,
Tim Cole
---
Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk><mailto:Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk>><mailto:Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk><mailto:Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk>>> Phone +44(0)20 7905 2666<tel:%2B44%280%2920%207905%202666> Fax +44(0)20 7905 2381<tel:%2B44%280%2920%207905%202381>
Population, Policy and Practice Programme
UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK


        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org><mailto:R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From tim.cole at ucl.ac.uk  Wed Oct 12 15:28:40 2016
From: tim.cole at ucl.ac.uk (Cole, Tim)
Date: Wed, 12 Oct 2016 13:28:40 +0000
Subject: [R-sig-ME] Exponent random effect in nlmer
In-Reply-To: <CAJuCY5z0m9JrEEjHeLtuW7Ta7nDtgFk0c+ZNmiXk_fxh5STeFg@mail.gmail.com>
References: <D42269FD.4ECAF%tim.cole@ucl.ac.uk>
	<CAJuCY5wmX6jYMvrtjf_M2LYWOt8gYXq0vSCqJzOTb7sCnHbUnw@mail.gmail.com>
	<D4227B1D.4ED19%tim.cole@ucl.ac.uk>
	<CAJuCY5z0m9JrEEjHeLtuW7Ta7nDtgFk0c+ZNmiXk_fxh5STeFg@mail.gmail.com>
Message-ID: <D423F846.14BA5%tim.cole@ucl.ac.uk>

Thierry,

Yes, that makes sense. In fact my sitar growth curve model is of the form y = a + h( (t-b)/exp(c) ) where h(.) is  a B-spline curve and a, b and c are fixed and random effects, and the exp(c) term fits fine.

Best wishes,
Tim
---
tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk> Phone +44(0)20 7905 2666 Fax +44(0)20 7905 2381
Population Policy and Practice Programme
UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK

From: Thierry Onkelinx <thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>>
Date: Wednesday, 12 October 2016 09:26
To: Tim Cole <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk>>
Cc: "r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>" <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: Re: [R-sig-ME] Exponent random effect in nlmer

Hi Tim,

AFAIK nlmer requires the fixed and random effects to be additive. The model to be used _after_ this this summation can be non linear.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey

2016-10-11 12:30 GMT+02:00 Cole, Tim <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk>>:
Dear Thierry,

Thanks very much for your speedy response.

I agree my model looks odd, but it has a theoretical basis which I'd prefer not to spell out at this stage. Suffice to say that
* -Inf < y < Inf
* 0 < E(y) < 1
* there is a subject random effect.

For these reasons the usual models and/or transformations won't work, whereas my proposed exponent random effect ought to. I just need to fit it, to see if I'm right!

Best wishes,
Tim
---
Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk> Phone +44(0)20 7905 2666<tel:%2B44%280%2920%207905%202666> Fax +44(0)20 7905 2381<tel:%2B44%280%2920%207905%202381>
Population, Policy and Practice Programme
UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK


From: Thierry Onkelinx <thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>>
Date: Tuesday, 11 October 2016 11:06
To: Tim Cole <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk>>
Cc: "r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>" <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: Re: [R-sig-ME] Exponent random effect in nlmer

Dear Tim,

y centred on 0 and a valid range (0, 1) seems to be conflicting statements.

Here a some solutions depending on y

- y stems from a binomial process
     - use a binomial glmm.
- y is continuous and you are willing to transform y
    - 0 < y <  1
        - apply a logit transformation on y. lmer(plogis(y) ~ f + (1 | id) )
    - 0 <= y < 1
        - apply a log transformation on y. lmer(log(y) ~ f + (1 | id) )
    - 0 < y <= 1
        - apply a log transformation on 1 - y. lmer(log(1 - y) ~ f + (1 | id) )
- y is continuous are not willing to transform y
   - use a beta regression with 0 and/or 1 inflation in case you have 0 or 1 in the data. Have a look at the gamlss package to fit this model.

Best regards,


ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey

2016-10-11 11:29 GMT+02:00 Cole, Tim <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk>>:
I have a model of the form
  m1 <- lmer(y ~ f + (1 | id) )
where y is a continuous variable centred on zero, f is a unordered factor with coefficients b such 0 < b < 1, and there is a signficant random subject intercept.

The random intercept can lead to predicted values outside the valid range (0, 1). For this reason I'd like to reformulate the model as
m2 <- nlmer(y ~ (f - 1) ^ exp(1 | id) )   (using a invalid but I hope obvious notation), where the random effect is now a power centred on 1. This would constrain the fitted values to be within c(0, 1).

My question is: can this be done in nlmer, and if so how? Please can someone point me in the right direction?

Thanks,
Tim Cole
---
Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk><mailto:Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk>> Phone +44(0)20 7905 2666<tel:%2B44%280%2920%207905%202666> Fax +44(0)20 7905 2381<tel:%2B44%280%2920%207905%202381>
Population, Policy and Practice Programme
UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK


        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



	[[alternative HTML version deleted]]


From ledee at umn.edu  Wed Oct 12 15:37:26 2016
From: ledee at umn.edu (Laura Dee)
Date: Wed, 12 Oct 2016 08:37:26 -0500
Subject: [R-sig-ME] Assumptions of random effects for unbiased estimates
In-Reply-To: <CAAH-yP9GFu47dZwueSVbODnibzSwf9fjnTHvVCiESEPoot16jg@mail.gmail.com>
References: <CAAH-yP9GFu47dZwueSVbODnibzSwf9fjnTHvVCiESEPoot16jg@mail.gmail.com>
Message-ID: <CABEErbb3ng7Xa3cqFj=gEuxSezdt4ogcYa0u6pJNooYOZ4YaXA@mail.gmail.com>

Dear all,
Thanks all - very interesting and helpful responses. I think I should have
been clearer with my question: in my case, the unobserved heterogeneity
between groups as not being of interest to study (but something to be
controlled for to isolate the effects of other x_ij's). Also I'm using a
linear model setting. The paper Jake and Malcolm sent was very help to lay
out these issues and suggest the within and between RE model when you want
to be studying the between group variation. And, I'll go through John Poe's
slides in detail.

There are four points that have emerged from this discussion that I think
are worth teasing apart:

*1)*  Interest in studying the mean effects and how they differ between
groups, which FE do not allow because they remove the mean effect. However,
even with RE and the ability to study those between differences, you still
have the challenge of credibly identifying the mean effects of income on
y_ij -- and whether you have ruled out/controlled for other factors that
vary cross-sectionally. RE do not solve this issue but are preferred
because the mean effect between groups is what is of interest. Therfore,
one is willing to accept some bias in the estimates if there are other
unobserved variables that vary cross-sectionally and influence the outcome.

Further, Malcolm, I agree that both FE and RE both try to account for a
group mean but not this statement because of the assumption of RE: "But
group-mean centering can also be done with random effects models, with the
same benefit you get with fixed effects models (isolation of the within
effects), while still allowing for estimation of the between relationships"
However, estimates are unbiased if FE are correlated with the error term,
which is not the case for RE. Though, agreed, if it is the between group
variation that is of interest, then it does not make sense to use a FE
model and there is too much focus on bias over other issues (i.e.,
estimating the effect of interest).

*Question: *in the Bell & Jones paper that Jake sent, they present
the Pl?mper and Troeger?s (2007) fixed effects vector decomposition. Is
that used often? I don't think it has made it's way to ecology.

*2)* Based on John Poe's response and example with the income, I think that
is an argument that the model identification is wrong if you don't allow
mean income versus deviations from mean income to have different effects on
consumption, rather than an argument that RE solve the problem of
unobserved heterogeneity more credibly than FE. This is a point about model
specification rather than dealing with unobservable heterogeneity.

*3) *Agreed that FE are biased with some forms of non-linear models. Could
anyone send me some more recent papers on this topic?

*4) *Ben raised the issue of a bias-variance trade-off, which is a good
point and economists seem to focus more (and maybe too much) on bias.
However, with enough observations, it's less of a trade-off.

Many thanks to everyone,
Laura



On Wed, Oct 12, 2016 at 3:12 AM, Malcolm Fairbrother <
M.Fairbrother at bristol.ac.uk> wrote:

> As others have said, there are rather peculiar inconsistencies between
> what the methodological literature knows and what empirical economists
> actually do.
>
> I think the paper Jake cited (by my colleagues at Bristol/Sheffield) is
> indeed one of the most useful on all this. The published version of the
> paper is at: http://dx.doi.org/10.1017/psrm.2014.7
>
> The following working paper (by them and me) takes up similar themes:
> https://www.researchgate.net/publication/299604336_Fixed_
> and_Random_effects_making_an_informed_choice
>
> One of the additional limitations we note here with fixed effects models
> (using a simulation study) is that they can be anti-conservative, in the
> sense that the SEs they return are too small if the data are generated from
> a random slopes model.
>
> In brief, Laura, fixed effects models only estimate within-group
> relationships, whereas random effects (AKA multilevel, mixed) models can
> estimate within- and between-group relationships. The estimation of fixed
> effects models implicitly entails group mean centering (though the models
> are typically written out as though unit dummies are estimated). But
> group-mean centering can also be done with random effects models, with the
> same benefit you get with fixed effects models (isolation of the within
> effects), while still allowing for estimation of the between relationships.
> You might have less confidence that the between component of some x_ij is
> uncorrelated with the unit error term, but it is still possible for the
> within (group-mean-centered) component to be correlated with the
> observation-level error term. So I would agree that bias is worth thinking
> about, but using fixed effects is no more helpful than random effects as a
> solution to the problem.
>
> Hope that's useful,
> Malcolm
>
>
> Dr Malcolm Fairbrother
> Reader in Global Policy and Politics
> School of Geographical Sciences  ?  Cabot Institute  ?  Centre for
> Multilevel Modelling
> University of Bristol
>
>
>
>
> Date: Tue, 11 Oct 2016 20:49:45 -0500
>> From: Jake Westfall <jake.a.westfall at gmail.com>
>> To: r-sig-mixed-models at r-project.org
>> Subject: Re: [R-sig-ME] Assumptions of random effects for unbiased
>>         estimates
>> Message-ID:
>>         <CAE9_Wg6+ZFXh-9on=nmuUwLKO6ScXjMRfbgf4y+XpGNhVAwJqA at mail.gm
>> ail.com>
>> Content-Type: text/plain; charset="UTF-8"
>>
>>
>> What a nice contribution from John!
>>
>> Jake
>>
>> On Tue, Oct 11, 2016 at 8:11 PM, Poe, John <jdpo223 at g.uky.edu> wrote:
>>
>> > My reading of modern work by panel data econometricians is that they
>> seem
>> > very fine with the use of mixed effects models that properly
>> differentiate
>> > effects at different levels of analysis and the tools to do so have
>> existed
>> > in that literature since the early 1980s. They have been borrowing
>> heavily
>> > from the mixed effects literature in designing econometric models and
>> talk
>> > about them in panel data textbooks. This hasn't typically filtered down
>> to
>> > applied economists who tend to misunderstand what other fields do
>> because
>> > other fields just tend to talk about them differently.
>> >
>> > The short version:
>> > Everyone in the mixed effects literature just uses group/grand mean
>> > centering and random coefficients to deal with endogeneity bias. If you
>> are
>> > an economist and someone outside of econ says mixed effects models you
>> > should think *correlated random effects models* and not *random effects
>> > models*.
>> >
>> > The long version:
>> > Economists are pretty afraid error structures that are correlated with
>> > independent variables in general and have built up pretty elaborate
>> > statistical models to deal with the problem. In panel data, this
>> manifests
>> > itself as wanting to avoid confounding effects at different levels of
>> > analysis so that within group varying effects are segregated from
>> between
>> > group varying effects. It can also happen when you are omitting higher
>> > level random effects
>> > <http://methods.johndavidpoe.com/2016/09/09/independence-acr
>> oss-levels-in-mixed-effects-models/>
>> > and they are distorting the structure of the random effects that you are
>> > including. This is generally a good thing as you want to be able to test
>> > hypotheses at specific levels of analysis without confounding.
>> >
>> > It's a big enough theoretical concern in the discipline that they
>> usually
>> > just want to remove all between group effects from the data as a
>> *default* to
>> > get level one effects because it is simpler and more fool proof than
>> > dealing with the problem in a mixed effects setting. It's so pervasive
>> that
>> > they are often socialized into not designing hypotheses for any between
>> > group or cross-level variation and just focus on within group (time
>> > varying) variability when at all possible (what economists call *within
>> > effects*).
>> >
>> > What economists refer to as fixed effects models just difference out all
>> > between group variation so that it cannot contaminate within group
>> effects
>> > (bias level one coefficients). It's the equivalent to including group
>> > indicator variables in the model instead of a random effect and just
>> > accepting that you can't make substantive inferences about anything at
>> the
>> > group level (what economists call *between effects*).
>> >
>> > The typical conventional wisdom in applied econometrics is to use a
>> > Hausman test which is a generic test comparing coefficients between a
>> > random effects model (with no level 2 covariates) and a model with all
>> > between group variability removed from the data. If there are
>> differences
>> > between the two, then they prefer to go with the latter. This is bad
>> > practice according to econometrics textbooks but applied people don't
>> seem
>> > to care (Baltagi 2013 ch 4.3). This only makes sense if you don't care
>> > about group invariant variables that only differ crosssectionally and/or
>> > you think of their effects as contamination. Panel data econometrics
>> > textbooks tend to argue for a wider range of options here but in
>> practice
>> > not that many economists seem to use them.
>> >
>> > There's an alternative framework in econ for dealing with this problem
>> > that they call a Mundlak device (Mundlak 1978) or correlated random
>> effects
>> > models (Baltagi Handbook of Panel Data 2014 ch 6.3.3 or really any panel
>> > data textbook) which is equivalent to a hierarchical linear model with
>> > group mean centering for level-one variables. This approach is used in
>> > econometrics by some pretty standard advanced panel data models (e.g.
>> > Hausman-Taylor and Arellano Bond). The other alternative that is
>> advocated
>> > by panel data econometricians but doesn't seem to have filtered down to
>> > rank and file economists is to use random coefficients models and just
>> > allow the random effects to be correlated with level one variables
>> (Hsiao
>> > 2014 chapter 6 and most of his other written work).
>> >
>> > It is important to understand that efficiency isn't the primary reason
>> for
>> > use of a mixed effects model over a fixed effects model for most
>> research.
>> > A common reason to use a mixed effects model is that you have hypotheses
>> > about variables operating at higher levels of analysis or cross-level
>> > interactions and those questions cannot be answered by fixed effects
>> panel
>> > models that have removed all between group variability from the
>> analysis.
>> > You are sacrificing the ability to test group variant hypotheses by
>> using a
>> > basic fixed effects model over a mixed effects model. For nonlinear
>> models
>> > like a logistic regression it can also be very difficult to use an
>> unbiased
>> > fixed effects model (though there are ways in a panel setting e.g. Hahn
>> and
>> > Newy 2004) and trivial to use a mixed effects model.
>> >
>> > Panel data econometricians almost always talk about typical practice
>> among
>> > applied economists using fixed effects as flawed (see Baltagi 2013 ch.
>> > 4.3). Mark Nerlov's 2000 History of Panel Data Econometrics is my
>> favorite
>> > example:
>> >
>> > The absurdity of the contention that possible correlation between some
>> of
>> >> the observed explanatory variables and the individual-specific
>> component of
>> >> the disturbance is a ground for using fixed effects should be clear
>> from
>> >> the following example: Consider a panel of households with data on
>> >> consumption and income. We are trying to estimate a consumption
>> function.
>> >> Income varies across households and over time. The variation across
>> >> households is related to ability of the main earner and other household
>> >> specific factors which vary little over time, that is to say, reflect
>> >> mainly differences in permanent income. Such permanent differences in
>> >> income are widely believed to be the source of most differences in
>> >> consumption both crosssectionally and over time, whereas, variations of
>> >> income over time are likely to be mostly transitory and unrelated to
>> >> consumption in most categories. Yet, fixed-effects regressions are
>> >> equivalent to using only this variation and discarding the information
>> on
>> >> the consumption-income relationship contained the cross-section
>> variation
>> >> among the household means.
>> >
>> >
>> > See the last couple of pages of this lecture
>> > <http://www.johndavidpoe.com/wp-content/uploads/2012/09/Blal
>> ock-Lecture.pdf> for
>>
>> > the citations in the econometrics and multilevel literature that I
>> > referenced.
>> >
>> >
>> >
>> > On Tue, Oct 11, 2016 at 3:32 PM, Jake Westfall <
>> jake.a.westfall at gmail.com>
>> > wrote:
>> >
>> >> Hi Laura and Ben,
>> >>
>> >> I like this paper on this topic:
>> >> http://psych.colorado.edu/~westfaja/FixedvsRandom.pdf
>> >>
>> >> What it comes down to essentially is that if the cluster effects are
>> >> correlated with the "time-varying" (i.e., within-cluster varying) X
>> >> predictor -- so that, for example, some clusters have high means on X
>> and
>> >> others have low means on X -- then there is the possibility that the
>> >> average within-cluster effect (which is what the fixed effect model
>> >> estimates) differs from the overall effect of X, not conditional on the
>> >> clusters. An extreme example of this is Simpson's paradox. Now since
>> the
>> >> estimate from the random-effects model can be seen as a weighted
>> average
>> >> of
>> >> these two effects, it will generally be pulled to some extent away from
>> >> the
>> >> fixed-effect estimate toward the unconditional estimate, which is the
>> bias
>> >> that econometricians fret about. However, if the cluster effects are
>> not
>> >> correlated with X, so that each cluster has the same mean on X, then
>> this
>> >> situation is not possible, so the random-effect model will give the
>> same
>> >> unbiased estimate as the fixed-effect model.
>> >>
>> >> A simple solution to this problem is to retain the random-effect model,
>> >> but
>> >> to split the predictor X into two components, one representing the
>> >> within-cluster variation of X and the other representing the
>> >> between-cluster variation of X, and estimate separate slopes for these
>> two
>> >> effects. One can even test whether these two slopes differ from each
>> >> other,
>> >> which is conceptually similar to what the Hausman test does. As
>> described
>> >> in the paper linked above, the estimate of the within-cluster
>> component of
>> >> the X effect equals the estimate one would obtain from a fixed-effect
>> >> model.
>> >>
>> >> As for the original question, I can't speak for common practice in
>> >> ecology,
>> >> but I suspect it may be like it is in my home field of psychology,
>> where
>> >> we
>> >> do worry about this issue (to some extent), but we discuss it using
>> >> completely different language. That is, we discuss it in terms of
>> whether
>> >> there are different effects of the predictor at the within-cluster and
>> >> between-cluster levels, and how our model might account for that.
>> >>
>> >> Jake
>> >>
>> >> On Tue, Oct 11, 2016 at 1:50 PM, Ben Bolker <bbolker at gmail.com> wrote:
>> >>
>> >> >
>> >> >   I didn't respond to this offline, as it took me a while even to
>> start
>> >> > to come up to speed on the question.  Random effects are indeed
>> defined
>> >> > from *very* different points of view in the two communities
>> >> > ([bio]statistical vs. econometric); I'm sure there are points of
>> >> > contact, but I've been having a hard time getting my head around it
>> all.
>> >> >
>> >> > Econometric definition:
>> >> >
>> >> > The wikipedia page <https://en.wikipedia.org/wiki
>> /Random_effects_model>
>> >> > and CrossValidated question
>> >> > <http://stats.stackexchange.com/questions/66161/why-do-
>> >> > random-effect-models-require-the-effects-to-be-uncorrelated-
>> >> with-the-inpu>
>> >> > were both helpful for me.
>> >> >
>> >> >  In the (bio)statistical world fixed and random effects are usually
>> >> > justified practically in terms of shrinkage estimators, or
>> >> > philosophically in terms of random draws from an exchangeable set of
>> >> > levels: e.g. see
>> >> > <http://stats.stackexchange.com/questions/4700/what-is-
>> >> > the-difference-between-fixed-effect-random-effect-and-mixed-
>> >> effect-mode/>
>> >> > for links.
>> >> >
>> >> >   I don't think I can really write an answer yet.  I'm still trying
>> to
>> >> > understand at an intuitive or heuristic level what it means for
>> >> > Cov(x_it,c_i)=0, where x_it is a set of explanatory variables over
>> time
>> >> > for an individual subject and c_i is the conditional mode (=BLUP in
>> >> > linear mixed-model-land) for the deviation of the individual i from
>> the
>> >> > population mean ... or more particularly what it means for that
>> >> > condition to be violated, which is the point at which fixed effects
>> >> > would become preferred.
>> >> >
>> >> >   As a side note, some statisticians (Andrew Gelman is the one who
>> >> > springs to mind) have commented on the possible overemphasis on bias.
>> >> > (All else being equal unbiased estimators are preferred to biased
>> >> > estimators but all else is not always equal). Two examples: (1)
>> >> > penalized estimators such as lasso/ridge regression (closely related
>> to
>> >> > mixed models) give biased parameter estimates with lower mean squared
>> >> > error. (2) When estimating variability, one has to choose a
>> particular
>> >> > scale (variance, standard error, log(standard error), etc.) on which
>> one
>> >> > would prefer to get an unbiased answer.
>> >> >
>> >> > On 16-10-11 12:02 PM, Laura Dee wrote:
>> >> > > Dear all,
>> >> > > Random effects are more efficient estimators ? however they come at
>>
>> >> the
>> >> > > cost of the assumption that the random effect is not correlated
>> with
>> >> the
>> >> > > included explanatory variables. Otherwise, using random effects
>> leads
>> >> to
>> >> > > biased estimates (e.g., as laid out in Woolridge
>> >> > > <https://faculty.fuqua.duke.edu/~moorman/Wooldridge,%20FE%20
>> >> and%20RE.pdf
>> >> > >'s
>> >> > > Econometrics text). This assumption is a strong one for many
>> >> > > observational datasets, and most analyses in economics do not use
>> >> random
>> >> > > effects for this reason. *Is there a reason why observational
>> >> ecological
>> >> > > datasets would be fundamentally different that I am missing? Why is
>> >> this
>> >> > > important assumption (to have unbiased estimates from random
>> effects)
>> >> > > not emphasized in ecology? *
>> >> > >
>> >> > > Thanks!
>> >> > >
>> >> > > Laura
>> >> > >
>> >> > > --
>> >> > > Laura Dee
>> >> > > Post-doctoral Associate
>> >> > > University of Minnesota
>> >> > > ledee at umn.edu <mailto:ledee at umn.edu>
>> >> > > lauraedee.com <http://lauraedee.com>
>> >> >
>> >> > _______________________________________________
>> >> > R-sig-mixed-models at r-project.org mailing list
>> >> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >> >
>> >>
>> >>         [[alternative HTML version deleted]]
>> >>
>> >> _______________________________________________
>> >> R-sig-mixed-models at r-project.org mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >>
>> >
>> >
>> >
>> > --
>> >
>> >
>> >
>> >
>> > Thanks,
>> > John
>> >
>> >
>> > John Poe
>> > Doctoral Candidate
>> > Department of Political Science
>> > Research Methodologist
>> > UK Center for Public Health Services & Systems Research
>> > University of Kentucky
>> > 111 Washington Avenue, Room 203a
>> > Lexington, KY 40536
>> > www.johndavidpoe.com
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>>
>>
>> ------------------------------
>>
>> Message: 2
>> Date: Tue, 11 Oct 2016 22:47:41 -0400
>> From: "Poe, John" <jdpo223 at g.uky.edu>
>> To: Jake Westfall <jake.a.westfall at gmail.com>
>> Cc: r-sig-mixed-models at r-project.org
>> Subject: Re: [R-sig-ME] Assumptions of random effects for unbiased
>>         estimates
>> Message-ID:
>>         <CAFW8ByrhGyML6DE=dMnmNm7xSeWB6zBDgvR_HaDy2Vnn53hnPQ at mail.gm
>> ail.com>
>> Content-Type: text/plain; charset="UTF-8"
>>
>>
>> Thanks Jake!
>>
>> On Oct 11, 2016 9:50 PM, "Jake Westfall" <jake.a.westfall at gmail.com>
>> wrote:
>>
>> > What a nice contribution from John!
>> >
>> > Jake
>> >
>> > On Tue, Oct 11, 2016 at 8:11 PM, Poe, John <jdpo223 at g.uky.edu> wrote:
>> >
>> > > My reading of modern work by panel data econometricians is that they
>> seem
>> > > very fine with the use of mixed effects models that properly
>> > differentiate
>> > > effects at different levels of analysis and the tools to do so have
>> > existed
>> > > in that literature since the early 1980s. They have been borrowing
>> > heavily
>> > > from the mixed effects literature in designing econometric models and
>> > talk
>> > > about them in panel data textbooks. This hasn't typically filtered
>> down
>> > to
>> > > applied economists who tend to misunderstand what other fields do
>> because
>> > > other fields just tend to talk about them differently.
>> > >
>> > > The short version:
>> > > Everyone in the mixed effects literature just uses group/grand mean
>> > > centering and random coefficients to deal with endogeneity bias. If
>> you
>> > are
>> > > an economist and someone outside of econ says mixed effects models you
>> > > should think *correlated random effects models* and not *random
>> effects
>> > > models*.
>> > >
>> > > The long version:
>> > > Economists are pretty afraid error structures that are correlated with
>> > > independent variables in general and have built up pretty elaborate
>> > > statistical models to deal with the problem. In panel data, this
>> > manifests
>> > > itself as wanting to avoid confounding effects at different levels of
>> > > analysis so that within group varying effects are segregated from
>> between
>> > > group varying effects. It can also happen when you are omitting higher
>> > > level random effects
>> > > <http://methods.johndavidpoe.com/2016/09/09/independence-
>> > across-levels-in-mixed-effects-models/>
>> > > and they are distorting the structure of the random effects that you
>> are
>> > > including. This is generally a good thing as you want to be able to
>> test
>> > > hypotheses at specific levels of analysis without confounding.
>> > >
>> > > It's a big enough theoretical concern in the discipline that they
>> usually
>> > > just want to remove all between group effects from the data as a
>> > *default* to
>> > > get level one effects because it is simpler and more fool proof than
>> > > dealing with the problem in a mixed effects setting. It's so pervasive
>> > that
>> > > they are often socialized into not designing hypotheses for any
>> between
>> > > group or cross-level variation and just focus on within group (time
>> > > varying) variability when at all possible (what economists call
>> *within
>> > > effects*).
>> > >
>> > > What economists refer to as fixed effects models just difference out
>> all
>> > > between group variation so that it cannot contaminate within group
>> > effects
>> > > (bias level one coefficients). It's the equivalent to including group
>> > > indicator variables in the model instead of a random effect and just
>> > > accepting that you can't make substantive inferences about anything at
>> > the
>> > > group level (what economists call *between effects*).
>> > >
>> > > The typical conventional wisdom in applied econometrics is to use a
>> > > Hausman test which is a generic test comparing coefficients between a
>> > > random effects model (with no level 2 covariates) and a model with all
>> > > between group variability removed from the data. If there are
>> differences
>> > > between the two, then they prefer to go with the latter. This is bad
>> > > practice according to econometrics textbooks but applied people don't
>> > seem
>> > > to care (Baltagi 2013 ch 4.3). This only makes sense if you don't care
>> > > about group invariant variables that only differ crosssectionally
>> and/or
>> > > you think of their effects as contamination. Panel data econometrics
>> > > textbooks tend to argue for a wider range of options here but in
>> practice
>> > > not that many economists seem to use them.
>> > >
>> > > There's an alternative framework in econ for dealing with this problem
>> > > that they call a Mundlak device (Mundlak 1978) or correlated random
>> > effects
>> > > models (Baltagi Handbook of Panel Data 2014 ch 6.3.3 or really any
>> panel
>> > > data textbook) which is equivalent to a hierarchical linear model with
>> > > group mean centering for level-one variables. This approach is used in
>> > > econometrics by some pretty standard advanced panel data models (e.g.
>> > > Hausman-Taylor and Arellano Bond). The other alternative that is
>> > advocated
>> > > by panel data econometricians but doesn't seem to have filtered down
>> to
>> > > rank and file economists is to use random coefficients models and just
>> > > allow the random effects to be correlated with level one variables
>> (Hsiao
>> > > 2014 chapter 6 and most of his other written work).
>> > >
>> > > It is important to understand that efficiency isn't the primary reason
>> > for
>> > > use of a mixed effects model over a fixed effects model for most
>> > research.
>> > > A common reason to use a mixed effects model is that you have
>> hypotheses
>> > > about variables operating at higher levels of analysis or cross-level
>> > > interactions and those questions cannot be answered by fixed effects
>> > panel
>> > > models that have removed all between group variability from the
>> analysis.
>> > > You are sacrificing the ability to test group variant hypotheses by
>> > using a
>> > > basic fixed effects model over a mixed effects model. For nonlinear
>> > models
>> > > like a logistic regression it can also be very difficult to use an
>> > unbiased
>> > > fixed effects model (though there are ways in a panel setting e.g.
>> Hahn
>> > and
>> > > Newy 2004) and trivial to use a mixed effects model.
>> > >
>> > > Panel data econometricians almost always talk about typical practice
>> > among
>> > > applied economists using fixed effects as flawed (see Baltagi 2013 ch.
>> > > 4.3). Mark Nerlov's 2000 History of Panel Data Econometrics is my
>> > favorite
>> > > example:
>> > >
>> > > The absurdity of the contention that possible correlation between
>> some of
>> > >> the observed explanatory variables and the individual-specific
>> > component of
>> > >> the disturbance is a ground for using fixed effects should be clear
>> from
>> > >> the following example: Consider a panel of households with data on
>> > >> consumption and income. We are trying to estimate a consumption
>> > function.
>> > >> Income varies across households and over time. The variation across
>> > >> households is related to ability of the main earner and other
>> household
>> > >> specific factors which vary little over time, that is to say, reflect
>> > >> mainly differences in permanent income. Such permanent differences in
>> > >> income are widely believed to be the source of most differences in
>> > >> consumption both crosssectionally and over time, whereas, variations
>> of
>> > >> income over time are likely to be mostly transitory and unrelated to
>> > >> consumption in most categories. Yet, fixed-effects regressions are
>> > >> equivalent to using only this variation and discarding the
>> information
>> > on
>> > >> the consumption-income relationship contained the cross-section
>> > variation
>> > >> among the household means.
>> > >
>> > >
>> > > See the last couple of pages of this lecture
>> > > <http://www.johndavidpoe.com/wp-content/uploads/2012/09/
>> > Blalock-Lecture.pdf> for
>> > > the citations in the econometrics and multilevel literature that I
>> > > referenced.
>> > >
>> > >
>> > >
>> > > On Tue, Oct 11, 2016 at 3:32 PM, Jake Westfall <
>> > jake.a.westfall at gmail.com>
>> > > wrote:
>> > >
>> > >> Hi Laura and Ben,
>> > >>
>> > >> I like this paper on this topic:
>> > >> http://psych.colorado.edu/~westfaja/FixedvsRandom.pdf
>> > >>
>> > >> What it comes down to essentially is that if the cluster effects are
>> > >> correlated with the "time-varying" (i.e., within-cluster varying) X
>> > >> predictor -- so that, for example, some clusters have high means on X
>> > and
>> > >> others have low means on X -- then there is the possibility that the
>> > >> average within-cluster effect (which is what the fixed effect model
>> > >> estimates) differs from the overall effect of X, not conditional on
>> the
>> > >> clusters. An extreme example of this is Simpson's paradox. Now since
>> the
>> > >> estimate from the random-effects model can be seen as a weighted
>> average
>> > >> of
>> > >> these two effects, it will generally be pulled to some extent away
>> from
>> > >> the
>> > >> fixed-effect estimate toward the unconditional estimate, which is the
>> > bias
>> > >> that econometricians fret about. However, if the cluster effects are
>> not
>> > >> correlated with X, so that each cluster has the same mean on X, then
>> > this
>> > >> situation is not possible, so the random-effect model will give the
>> same
>> > >> unbiased estimate as the fixed-effect model.
>> > >>
>> > >> A simple solution to this problem is to retain the random-effect
>> model,
>> > >> but
>> > >> to split the predictor X into two components, one representing the
>> > >> within-cluster variation of X and the other representing the
>> > >> between-cluster variation of X, and estimate separate slopes for
>> these
>> > two
>> > >> effects. One can even test whether these two slopes differ from each
>> > >> other,
>> > >> which is conceptually similar to what the Hausman test does. As
>> > described
>> > >> in the paper linked above, the estimate of the within-cluster
>> component
>> > of
>> > >> the X effect equals the estimate one would obtain from a fixed-effect
>> > >> model.
>> > >>
>> > >> As for the original question, I can't speak for common practice in
>> > >> ecology,
>> > >> but I suspect it may be like it is in my home field of psychology,
>> where
>> > >> we
>> > >> do worry about this issue (to some extent), but we discuss it using
>> > >> completely different language. That is, we discuss it in terms of
>> > whether
>> > >> there are different effects of the predictor at the within-cluster
>> and
>> > >> between-cluster levels, and how our model might account for that.
>> > >>
>> > >> Jake
>> > >>
>> > >> On Tue, Oct 11, 2016 at 1:50 PM, Ben Bolker <bbolker at gmail.com>
>> wrote:
>> > >>
>> > >> >
>> > >> >   I didn't respond to this offline, as it took me a while even to
>> > start
>> > >> > to come up to speed on the question.  Random effects are indeed
>> > defined
>> > >> > from *very* different points of view in the two communities
>> > >> > ([bio]statistical vs. econometric); I'm sure there are points of
>> > >> > contact, but I've been having a hard time getting my head around it
>> > all.
>> > >> >
>> > >> > Econometric definition:
>> > >> >
>> > >> > The wikipedia page <https://en.wikipedia.org/
>> > wiki/Random_effects_model>
>> > >> > and CrossValidated question
>> > >> > <http://stats.stackexchange.com/questions/66161/why-do-
>> > >> > random-effect-models-require-the-effects-to-be-uncorrelated-
>> > >> with-the-inpu>
>> > >> > were both helpful for me.
>> > >> >
>> > >> >  In the (bio)statistical world fixed and random effects are usually
>> > >> > justified practically in terms of shrinkage estimators, or
>> > >> > philosophically in terms of random draws from an exchangeable set
>> of
>> > >> > levels: e.g. see
>> > >> > <http://stats.stackexchange.com/questions/4700/what-is-
>> > >> > the-difference-between-fixed-effect-random-effect-and-mixed-
>> > >> effect-mode/>
>> > >> > for links.
>> > >> >
>> > >> >   I don't think I can really write an answer yet.  I'm still
>> trying to
>> > >> > understand at an intuitive or heuristic level what it means for
>> > >> > Cov(x_it,c_i)=0, where x_it is a set of explanatory variables over
>> > time
>> > >> > for an individual subject and c_i is the conditional mode (=BLUP in
>> > >> > linear mixed-model-land) for the deviation of the individual i from
>> > the
>> > >> > population mean ... or more particularly what it means for that
>> > >> > condition to be violated, which is the point at which fixed effects
>> > >> > would become preferred.
>> > >> >
>> > >> >   As a side note, some statisticians (Andrew Gelman is the one who
>> > >> > springs to mind) have commented on the possible overemphasis on
>> bias.
>> > >> > (All else being equal unbiased estimators are preferred to biased
>> > >> > estimators but all else is not always equal). Two examples: (1)
>> > >> > penalized estimators such as lasso/ridge regression (closely
>> related
>> > to
>> > >> > mixed models) give biased parameter estimates with lower mean
>> squared
>> > >> > error. (2) When estimating variability, one has to choose a
>> particular
>> > >> > scale (variance, standard error, log(standard error), etc.) on
>> which
>> > one
>> > >> > would prefer to get an unbiased answer.
>> > >> >
>> > >> > On 16-10-11 12:02 PM, Laura Dee wrote:
>> > >> > > Dear all,
>> > >> > > Random effects are more efficient estimators ? however they come
>> at
>>
>> > >> the
>> > >> > > cost of the assumption that the random effect is not correlated
>> with
>> > >> the
>> > >> > > included explanatory variables. Otherwise, using random effects
>> > leads
>> > >> to
>> > >> > > biased estimates (e.g., as laid out in Woolridge
>> > >> > > <https://faculty.fuqua.duke.edu/~moorman/Wooldridge,%20FE%20
>> > >> and%20RE.pdf
>> > >> > >'s
>> > >> > > Econometrics text). This assumption is a strong one for many
>> > >> > > observational datasets, and most analyses in economics do not use
>> > >> random
>> > >> > > effects for this reason. *Is there a reason why observational
>> > >> ecological
>> > >> > > datasets would be fundamentally different that I am missing? Why
>> is
>> > >> this
>> > >> > > important assumption (to have unbiased estimates from random
>> > effects)
>> > >> > > not emphasized in ecology? *
>> > >> > >
>> > >> > > Thanks!
>> > >> > >
>> > >> > > Laura
>> > >> > >
>> > >> > > --
>> > >> > > Laura Dee
>> > >> > > Post-doctoral Associate
>> > >> > > University of Minnesota
>> > >> > > ledee at umn.edu <mailto:ledee at umn.edu>
>> > >> > > lauraedee.com <http://lauraedee.com>
>> > >> >
>> > >> > _______________________________________________
>> > >> > R-sig-mixed-models at r-project.org mailing list
>> > >> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> > >> >
>> > >>
>> > >>         [[alternative HTML version deleted]]
>> > >>
>> > >> _______________________________________________
>> > >> R-sig-mixed-models at r-project.org mailing list
>> > >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> > >>
>> > >
>> > >
>> > >
>> > > --
>> > >
>> > >
>> > >
>> > >
>> > > Thanks,
>> > > John
>> > >
>> > >
>> > > John Poe
>> > > Doctoral Candidate
>> > > Department of Political Science
>> > > Research Methodologist
>> > > UK Center for Public Health Services & Systems Research
>> > > University of Kentucky
>> > > 111 Washington Avenue, Room 203a
>> > > Lexington, KY 40536
>> > > www.johndavidpoe.com
>>
>


-- 
Laura Dee
Post-doctoral Associate
University of Minnesota
ledee at umn.edu
lauraedee.com

	[[alternative HTML version deleted]]


From talischen at hotmail.com  Wed Oct 12 16:30:36 2016
From: talischen at hotmail.com (Chen Chun)
Date: Wed, 12 Oct 2016 14:30:36 +0000
Subject: [R-sig-ME] =?gb2312?b?tPC4tDogIE5vbi1ub3JtYWwgcmFuZG9tIGVmZmVj?=
 =?gb2312?b?dCBpbiBnbG1t?=
In-Reply-To: <DB6PR0101MB24230DB767F3AC06A2080433ACDD0@DB6PR0101MB2423.eurprd01.prod.exchangelabs.com>
References: <DB6PR0101MB24230DB767F3AC06A2080433ACDD0@DB6PR0101MB2423.eurprd01.prod.exchangelabs.com>
Message-ID: <DB6PR0101MB24237EB6677EA403AEC5C6DFACDD0@DB6PR0101MB2423.eurprd01.prod.exchangelabs.com>

Dear all,


I am applying a mixed model with binomial distribution on a very large data set (around 400000 samples) with binary outcome (very few event, around 4%).  Some respondents but not all are repeated measured over the years, that's why a mixed model is applied. The model can be written as :

mod <- glmer(response ~ AGE + SEX...+ YEAR + (1 | respondentID), family=binomial, data=dat)

The distribution of the random effect (ID) from the model output shows an obvious non-normal distribution: a large proportion of close to zero values and very few large values around 10. I am wondering if in this case the glmm model is still valid?  if not valid, what kind of alternative model can I try? Can someone give some suggestion?

A consequent problem is when I calculate the explained variance from the model:
VarF <- var(as.vector(fixef(mod ) %*% t(mod @pp$X)))
VarF/(VarF + VarCorr(mod )$respondentID[1] + (pi^2)/3)

the variance of the fixed effect (VarF) from the model is only 1.6, while the variance of the random effect (VarCorr(mod )$respondentID[1]) is 149. Due to the non-normal distribution, the variance of the random effect is very large as compared to the fixed effect. Does this imply that the model performs bad? Or I should compute conditional R square?

To summarize, my questions are:

1) What's the influence in estimation of the fixed effect and its explained variance (R squared) when the random effect does not follow a normal distribution? If the influence is large, any suggestions to solve it?

2) In a more general sense, how to comment a model where a large amount of variation comes from the random effects?

Thanks

Regards,
Chun



	[[alternative HTML version deleted]]


From jdpo223 at g.uky.edu  Wed Oct 12 17:55:19 2016
From: jdpo223 at g.uky.edu (Poe, John)
Date: Wed, 12 Oct 2016 11:55:19 -0400
Subject: [R-sig-ME] Assumptions of random effects for unbiased estimates
In-Reply-To: <CABEErbb3ng7Xa3cqFj=gEuxSezdt4ogcYa0u6pJNooYOZ4YaXA@mail.gmail.com>
References: <CAAH-yP9GFu47dZwueSVbODnibzSwf9fjnTHvVCiESEPoot16jg@mail.gmail.com>
	<CABEErbb3ng7Xa3cqFj=gEuxSezdt4ogcYa0u6pJNooYOZ4YaXA@mail.gmail.com>
Message-ID: <CAFW8ByqQHCB9pKbkCenK+C_dsJesOQrcfJ7XJdnUuF=EGHdnnQ@mail.gmail.com>

Laura,

I think we might be talking past each other somewhat on the FE vs RE
discussion. An RE model that has only level one (time or group varying)
covariates and a latent variable for the expected value of group membership
on Y is problematic for the reasons that you are talking about. But mixed
effects models aren't typically the same specification as the FE model or
the RE model. They either use group/grand mean centering of variables to
instrument the problem or random coefficients to stop making the assumption
of no correlation altogether.

The primary benefit of a mixed effects approach is that you have a better
handle on all of the factors that are influencing Y and you can make decent
predictions about cases given a particular context. By using the fixed
effects model you are literally making the choice not to care about things
that you know influence Y.

My usual advice if you don't care about between group effects or
cross-level effects AND you don't care about making predictions about
individuals or groups then fixed effects models that difference out between
group variability are fine for linear models. Given Malcolm's working paper
I might reevaluate that recommendation going forward.

On point 1 that you raise about credibility of the argument that group mean
centered models actually do fix the problems associated with random effects
correlation:

   - There's not actually a credibility problem here. There is a
   specification test in econometrics that is analogous to a Hausman test but
   compares fixed effects estimates to a hierarchical linear model. It's
   called a Mundlak specification test
   <http://blog.stata.com/2015/10/29/fixed-effects-or-random-effects-the-mundlak-approach/>.
   So you can demonstrate that it's not a problem.
   - The typical response when this test shows that there is still a
   violation of the no correlation between a random effect and a level 1
   variable assumption is to stop making that assumption and use a random
   coefficients model. The Bell and Jones paper does a good job of working
   through the logic here.

On your point about the comment :"But group-mean centering can also be done
with random effects models, with the same benefit you get with fixed
effects models (isolation of the within effects), while still allowing for
estimation of the between relationships"

   - Group mean centering of variables tends to instrument the endogeneity
   between the RE and the level one variables so that the correlation isn't an
   issue anymore. In the basic RE model without level 2 covariates and without
   centering you have a problem. But that model doesn't tend to get used much
   without criticism in political science, psychology, or economics (I'm not
   an ecologist so I can't say for practice in your field). It seems to be
   something of a straw-man in my experience.

On point 2

Based on John Poe's response and example with the income, I think that is
> an argument that the model identification is wrong if you don't allow mean
> income versus deviations from mean income to have different effects on consumption,
> rather than an argument that RE solve the problem of unobserved
> heterogeneity more credibly than FE. This is a point about model specification
> rather than dealing with unobservable heterogeneity.


   - All unobserved heterogeneity problems are about model specification.
   It's an omitted variable bias problem. Random effects models are literally
   just a version of the model with an additional latent variable for the
   expected value of group membership. That latent variable can be generated
   directly from coefficients off of dummy variables so they are esentially
   reparamaterizations of one another.
   - In a random effects setting you are including a new latent variable
   and in a random coefficients model you are adding an interaction between
   the random effect and the variable
   - In most cases you can deal with the endogeneity issue by including
   group means and then taking their deviations. When you can't do that, you
   can use dummy variables or the group-level expectations for a random effect
   and then interact that with the endogenous covariate.

 On point 3 for FE in nonlinear models

   - The basic issue is that in high dimensional problems the root finding
   algorithms for nonlinear MLE tend to start to give biased answers as the
   mode diverges away from the mean. This divergence between the mode and the
   mean works as a function of the number of groups and the size of the groups
   so that if T grows to infinity then there's no bias problem. I haven't seen
   solid research on how unbalanced group size influences this but my
   intuition says that it's likely to increase the bias problem. You get bias
   in as few as ten dimensions and there's really no way to fix it with convex
   optimization. So if you want to include dummy variables in something like a
   logit you either need to use some version of expectation maximization or
   integrate over the data then take the jacobian and hessian directly. At
   that point you know where the mean is and you don't have to rely on an
   optimizer so you will get unbiased results. That's why it's not an issue in
   Poisson or a linear model. You can just calculate the derivatives directly.

As far as citations on incidental parameters bias:

   - Lancaster, Tony. 2000. "The incidental parameter problem since 1948."
   Journal of Econometrics 95 (2):391-413.
   - Katz, Ethan. 2001. "Bias in conditional and unconditional fixed
   effects logit estimation." Political Analysis 9 (4):379-84.
   - Hahn, J., & Newey, W. (2004). Jackknife and analytical bias reduction
   for nonlinear panel models. Econometrica, 72(4), 1295-1319.
   - Greene, William. 2004. "The behaviour of the maximum likelihood
   estimator of limited dependent variable models in the presence of fixed
   effects." The Econometrics Journal 7 (1):98-119.
   - Bill Greene's chapter in Baltagi, B. H. (2014). *The Oxford Handbook
   of Panel Data*. Oxford University Press, USA.
   - Note that one of his recommendations is just to use group mean
      centering and the mixed effects framework
   - Beck, Nathaniel. 2015. Estimating grouped data models with a binary
   dependent variable and fixed effects: What are the issues? Paper read at
   annual meeting of the Society for Political Methodology, July

On the bias variance trade-off question I think it's mostly just that
economists aren't interested in prediction as much as estimating average
causal effects. They don't seem to care if their results are applicable to
any particular case so long as they describe the average effect of X on Y
well.


On Wed, Oct 12, 2016 at 9:37 AM, Laura Dee <ledee at umn.edu> wrote:

> Dear all,
> Thanks all - very interesting and helpful responses. I think I should have
> been clearer with my question: in my case, the unobserved heterogeneity
> between groups as not being of interest to study (but something to be
> controlled for to isolate the effects of other x_ij's). Also I'm using a
> linear model setting. The paper Jake and Malcolm sent was very help to lay
> out these issues and suggest the within and between RE model when you want
> to be studying the between group variation. And, I'll go through John Poe's
> slides in detail.
>
> There are four points that have emerged from this discussion that I think
> are worth teasing apart:
>
> *1)*  Interest in studying the mean effects and how they differ between
> groups, which FE do not allow because they remove the mean effect. However,
> even with RE and the ability to study those between differences, you still
> have the challenge of credibly identifying the mean effects of income on
> y_ij -- and whether you have ruled out/controlled for other factors that
> vary cross-sectionally. RE do not solve this issue but are preferred
> because the mean effect between groups is what is of interest. Therfore,
> one is willing to accept some bias in the estimates if there are other
> unobserved variables that vary cross-sectionally and influence the outcome.
>
> Further, Malcolm, I agree that both FE and RE both try to account for a
> group mean but not this statement because of the assumption of RE: "But
> group-mean centering can also be done with random effects models, with the
> same benefit you get with fixed effects models (isolation of the within
> effects), while still allowing for estimation of the between relationships"
> However, estimates are unbiased if FE are correlated with the error term,
> which is not the case for RE. Though, agreed, if it is the between group
> variation that is of interest, then it does not make sense to use a FE
> model and there is too much focus on bias over other issues (i.e.,
> estimating the effect of interest).
>
> *Question: *in the Bell & Jones paper that Jake sent, they present
> the Pl?mper and Troeger?s (2007) fixed effects vector decomposition. Is
> that used often? I don't think it has made it's way to ecology.
>
> *2)* Based on John Poe's response and example with the income, I think that
> is an argument that the model identification is wrong if you don't allow
> mean income versus deviations from mean income to have different effects on
> consumption, rather than an argument that RE solve the problem of
> unobserved heterogeneity more credibly than FE. This is a point about model
> specification rather than dealing with unobservable heterogeneity.
>
> *3) *Agreed that FE are biased with some forms of non-linear models. Could
> anyone send me some more recent papers on this topic?
>
> *4) *Ben raised the issue of a bias-variance trade-off, which is a good
> point and economists seem to focus more (and maybe too much) on bias.
> However, with enough observations, it's less of a trade-off.
>
> Many thanks to everyone,
> Laura
>
>
>
> On Wed, Oct 12, 2016 at 3:12 AM, Malcolm Fairbrother <
> M.Fairbrother at bristol.ac.uk> wrote:
>
> > As others have said, there are rather peculiar inconsistencies between
> > what the methodological literature knows and what empirical economists
> > actually do.
> >
> > I think the paper Jake cited (by my colleagues at Bristol/Sheffield) is
> > indeed one of the most useful on all this. The published version of the
> > paper is at: http://dx.doi.org/10.1017/psrm.2014.7
> >
> > The following working paper (by them and me) takes up similar themes:
> > https://www.researchgate.net/publication/299604336_Fixed_
> > and_Random_effects_making_an_informed_choice
> >
> > One of the additional limitations we note here with fixed effects models
> > (using a simulation study) is that they can be anti-conservative, in the
> > sense that the SEs they return are too small if the data are generated
> from
> > a random slopes model.
> >
> > In brief, Laura, fixed effects models only estimate within-group
> > relationships, whereas random effects (AKA multilevel, mixed) models can
> > estimate within- and between-group relationships. The estimation of fixed
> > effects models implicitly entails group mean centering (though the models
> > are typically written out as though unit dummies are estimated). But
> > group-mean centering can also be done with random effects models, with
> the
> > same benefit you get with fixed effects models (isolation of the within
> > effects), while still allowing for estimation of the between
> relationships.
> > You might have less confidence that the between component of some x_ij is
> > uncorrelated with the unit error term, but it is still possible for the
> > within (group-mean-centered) component to be correlated with the
> > observation-level error term. So I would agree that bias is worth
> thinking
> > about, but using fixed effects is no more helpful than random effects as
> a
> > solution to the problem.
> >
> > Hope that's useful,
> > Malcolm
> >
> >
> > Dr Malcolm Fairbrother
> > Reader in Global Policy and Politics
> > School of Geographical Sciences  ?  Cabot Institute  ?  Centre for
> > Multilevel Modelling
> > University of Bristol
> >
> >
> >
> >
> > Date: Tue, 11 Oct 2016 20:49:45 -0500
> >> From: Jake Westfall <jake.a.westfall at gmail.com>
> >> To: r-sig-mixed-models at r-project.org
> >> Subject: Re: [R-sig-ME] Assumptions of random effects for unbiased
> >>         estimates
> >> Message-ID:
> >>         <CAE9_Wg6+ZFXh-9on=nmuUwLKO6ScXjMRfbgf4y+XpGNhVAwJqA at mail.gm
> >> ail.com>
> >> Content-Type: text/plain; charset="UTF-8"
> >>
> >>
> >> What a nice contribution from John!
> >>
> >> Jake
> >>
> >> On Tue, Oct 11, 2016 at 8:11 PM, Poe, John <jdpo223 at g.uky.edu> wrote:
> >>
> >> > My reading of modern work by panel data econometricians is that they
> >> seem
> >> > very fine with the use of mixed effects models that properly
> >> differentiate
> >> > effects at different levels of analysis and the tools to do so have
> >> existed
> >> > in that literature since the early 1980s. They have been borrowing
> >> heavily
> >> > from the mixed effects literature in designing econometric models and
> >> talk
> >> > about them in panel data textbooks. This hasn't typically filtered
> down
> >> to
> >> > applied economists who tend to misunderstand what other fields do
> >> because
> >> > other fields just tend to talk about them differently.
> >> >
> >> > The short version:
> >> > Everyone in the mixed effects literature just uses group/grand mean
> >> > centering and random coefficients to deal with endogeneity bias. If
> you
> >> are
> >> > an economist and someone outside of econ says mixed effects models you
> >> > should think *correlated random effects models* and not *random
> effects
> >> > models*.
> >> >
> >> > The long version:
> >> > Economists are pretty afraid error structures that are correlated with
> >> > independent variables in general and have built up pretty elaborate
> >> > statistical models to deal with the problem. In panel data, this
> >> manifests
> >> > itself as wanting to avoid confounding effects at different levels of
> >> > analysis so that within group varying effects are segregated from
> >> between
> >> > group varying effects. It can also happen when you are omitting higher
> >> > level random effects
> >> > <http://methods.johndavidpoe.com/2016/09/09/independence-acr
> >> oss-levels-in-mixed-effects-models/>
> >> > and they are distorting the structure of the random effects that you
> are
> >> > including. This is generally a good thing as you want to be able to
> test
> >> > hypotheses at specific levels of analysis without confounding.
> >> >
> >> > It's a big enough theoretical concern in the discipline that they
> >> usually
> >> > just want to remove all between group effects from the data as a
> >> *default* to
> >> > get level one effects because it is simpler and more fool proof than
> >> > dealing with the problem in a mixed effects setting. It's so pervasive
> >> that
> >> > they are often socialized into not designing hypotheses for any
> between
> >> > group or cross-level variation and just focus on within group (time
> >> > varying) variability when at all possible (what economists call
> *within
> >> > effects*).
> >> >
> >> > What economists refer to as fixed effects models just difference out
> all
> >> > between group variation so that it cannot contaminate within group
> >> effects
> >> > (bias level one coefficients). It's the equivalent to including group
> >> > indicator variables in the model instead of a random effect and just
> >> > accepting that you can't make substantive inferences about anything at
> >> the
> >> > group level (what economists call *between effects*).
> >> >
> >> > The typical conventional wisdom in applied econometrics is to use a
> >> > Hausman test which is a generic test comparing coefficients between a
> >> > random effects model (with no level 2 covariates) and a model with all
> >> > between group variability removed from the data. If there are
> >> differences
> >> > between the two, then they prefer to go with the latter. This is bad
> >> > practice according to econometrics textbooks but applied people don't
> >> seem
> >> > to care (Baltagi 2013 ch 4.3). This only makes sense if you don't care
> >> > about group invariant variables that only differ crosssectionally
> and/or
> >> > you think of their effects as contamination. Panel data econometrics
> >> > textbooks tend to argue for a wider range of options here but in
> >> practice
> >> > not that many economists seem to use them.
> >> >
> >> > There's an alternative framework in econ for dealing with this problem
> >> > that they call a Mundlak device (Mundlak 1978) or correlated random
> >> effects
> >> > models (Baltagi Handbook of Panel Data 2014 ch 6.3.3 or really any
> panel
> >> > data textbook) which is equivalent to a hierarchical linear model with
> >> > group mean centering for level-one variables. This approach is used in
> >> > econometrics by some pretty standard advanced panel data models (e.g.
> >> > Hausman-Taylor and Arellano Bond). The other alternative that is
> >> advocated
> >> > by panel data econometricians but doesn't seem to have filtered down
> to
> >> > rank and file economists is to use random coefficients models and just
> >> > allow the random effects to be correlated with level one variables
> >> (Hsiao
> >> > 2014 chapter 6 and most of his other written work).
> >> >
> >> > It is important to understand that efficiency isn't the primary reason
> >> for
> >> > use of a mixed effects model over a fixed effects model for most
> >> research.
> >> > A common reason to use a mixed effects model is that you have
> hypotheses
> >> > about variables operating at higher levels of analysis or cross-level
> >> > interactions and those questions cannot be answered by fixed effects
> >> panel
> >> > models that have removed all between group variability from the
> >> analysis.
> >> > You are sacrificing the ability to test group variant hypotheses by
> >> using a
> >> > basic fixed effects model over a mixed effects model. For nonlinear
> >> models
> >> > like a logistic regression it can also be very difficult to use an
> >> unbiased
> >> > fixed effects model (though there are ways in a panel setting e.g.
> Hahn
> >> and
> >> > Newy 2004) and trivial to use a mixed effects model.
> >> >
> >> > Panel data econometricians almost always talk about typical practice
> >> among
> >> > applied economists using fixed effects as flawed (see Baltagi 2013 ch.
> >> > 4.3). Mark Nerlov's 2000 History of Panel Data Econometrics is my
> >> favorite
> >> > example:
> >> >
> >> > The absurdity of the contention that possible correlation between some
> >> of
> >> >> the observed explanatory variables and the individual-specific
> >> component of
> >> >> the disturbance is a ground for using fixed effects should be clear
> >> from
> >> >> the following example: Consider a panel of households with data on
> >> >> consumption and income. We are trying to estimate a consumption
> >> function.
> >> >> Income varies across households and over time. The variation across
> >> >> households is related to ability of the main earner and other
> household
> >> >> specific factors which vary little over time, that is to say, reflect
> >> >> mainly differences in permanent income. Such permanent differences in
> >> >> income are widely believed to be the source of most differences in
> >> >> consumption both crosssectionally and over time, whereas, variations
> of
> >> >> income over time are likely to be mostly transitory and unrelated to
> >> >> consumption in most categories. Yet, fixed-effects regressions are
> >> >> equivalent to using only this variation and discarding the
> information
> >> on
> >> >> the consumption-income relationship contained the cross-section
> >> variation
> >> >> among the household means.
> >> >
> >> >
> >> > See the last couple of pages of this lecture
> >> > <http://www.johndavidpoe.com/wp-content/uploads/2012/09/Blal
> >> ock-Lecture.pdf> for
> >>
> >> > the citations in the econometrics and multilevel literature that I
> >> > referenced.
> >> >
> >> >
> >> >
> >> > On Tue, Oct 11, 2016 at 3:32 PM, Jake Westfall <
> >> jake.a.westfall at gmail.com>
> >> > wrote:
> >> >
> >> >> Hi Laura and Ben,
> >> >>
> >> >> I like this paper on this topic:
> >> >> http://psych.colorado.edu/~westfaja/FixedvsRandom.pdf
> >> >>
> >> >> What it comes down to essentially is that if the cluster effects are
> >> >> correlated with the "time-varying" (i.e., within-cluster varying) X
> >> >> predictor -- so that, for example, some clusters have high means on X
> >> and
> >> >> others have low means on X -- then there is the possibility that the
> >> >> average within-cluster effect (which is what the fixed effect model
> >> >> estimates) differs from the overall effect of X, not conditional on
> the
> >> >> clusters. An extreme example of this is Simpson's paradox. Now since
> >> the
> >> >> estimate from the random-effects model can be seen as a weighted
> >> average
> >> >> of
> >> >> these two effects, it will generally be pulled to some extent away
> from
> >> >> the
> >> >> fixed-effect estimate toward the unconditional estimate, which is the
> >> bias
> >> >> that econometricians fret about. However, if the cluster effects are
> >> not
> >> >> correlated with X, so that each cluster has the same mean on X, then
> >> this
> >> >> situation is not possible, so the random-effect model will give the
> >> same
> >> >> unbiased estimate as the fixed-effect model.
> >> >>
> >> >> A simple solution to this problem is to retain the random-effect
> model,
> >> >> but
> >> >> to split the predictor X into two components, one representing the
> >> >> within-cluster variation of X and the other representing the
> >> >> between-cluster variation of X, and estimate separate slopes for
> these
> >> two
> >> >> effects. One can even test whether these two slopes differ from each
> >> >> other,
> >> >> which is conceptually similar to what the Hausman test does. As
> >> described
> >> >> in the paper linked above, the estimate of the within-cluster
> >> component of
> >> >> the X effect equals the estimate one would obtain from a fixed-effect
> >> >> model.
> >> >>
> >> >> As for the original question, I can't speak for common practice in
> >> >> ecology,
> >> >> but I suspect it may be like it is in my home field of psychology,
> >> where
> >> >> we
> >> >> do worry about this issue (to some extent), but we discuss it using
> >> >> completely different language. That is, we discuss it in terms of
> >> whether
> >> >> there are different effects of the predictor at the within-cluster
> and
> >> >> between-cluster levels, and how our model might account for that.
> >> >>
> >> >> Jake
> >> >>
> >> >> On Tue, Oct 11, 2016 at 1:50 PM, Ben Bolker <bbolker at gmail.com>
> wrote:
> >> >>
> >> >> >
> >> >> >   I didn't respond to this offline, as it took me a while even to
> >> start
> >> >> > to come up to speed on the question.  Random effects are indeed
> >> defined
> >> >> > from *very* different points of view in the two communities
> >> >> > ([bio]statistical vs. econometric); I'm sure there are points of
> >> >> > contact, but I've been having a hard time getting my head around it
> >> all.
> >> >> >
> >> >> > Econometric definition:
> >> >> >
> >> >> > The wikipedia page <https://en.wikipedia.org/wiki
> >> /Random_effects_model>
> >> >> > and CrossValidated question
> >> >> > <http://stats.stackexchange.com/questions/66161/why-do-
> >> >> > random-effect-models-require-the-effects-to-be-uncorrelated-
> >> >> with-the-inpu>
> >> >> > were both helpful for me.
> >> >> >
> >> >> >  In the (bio)statistical world fixed and random effects are usually
> >> >> > justified practically in terms of shrinkage estimators, or
> >> >> > philosophically in terms of random draws from an exchangeable set
> of
> >> >> > levels: e.g. see
> >> >> > <http://stats.stackexchange.com/questions/4700/what-is-
> >> >> > the-difference-between-fixed-effect-random-effect-and-mixed-
> >> >> effect-mode/>
> >> >> > for links.
> >> >> >
> >> >> >   I don't think I can really write an answer yet.  I'm still trying
> >> to
> >> >> > understand at an intuitive or heuristic level what it means for
> >> >> > Cov(x_it,c_i)=0, where x_it is a set of explanatory variables over
> >> time
> >> >> > for an individual subject and c_i is the conditional mode (=BLUP in
> >> >> > linear mixed-model-land) for the deviation of the individual i from
> >> the
> >> >> > population mean ... or more particularly what it means for that
> >> >> > condition to be violated, which is the point at which fixed effects
> >> >> > would become preferred.
> >> >> >
> >> >> >   As a side note, some statisticians (Andrew Gelman is the one who
> >> >> > springs to mind) have commented on the possible overemphasis on
> bias.
> >> >> > (All else being equal unbiased estimators are preferred to biased
> >> >> > estimators but all else is not always equal). Two examples: (1)
> >> >> > penalized estimators such as lasso/ridge regression (closely
> related
> >> to
> >> >> > mixed models) give biased parameter estimates with lower mean
> squared
> >> >> > error. (2) When estimating variability, one has to choose a
> >> particular
> >> >> > scale (variance, standard error, log(standard error), etc.) on
> which
> >> one
> >> >> > would prefer to get an unbiased answer.
> >> >> >
> >> >> > On 16-10-11 12:02 PM, Laura Dee wrote:
> >> >> > > Dear all,
> >> >> > > Random effects are more efficient estimators ? however they come
> at
> >>
> >> >> the
> >> >> > > cost of the assumption that the random effect is not correlated
> >> with
> >> >> the
> >> >> > > included explanatory variables. Otherwise, using random effects
> >> leads
> >> >> to
> >> >> > > biased estimates (e.g., as laid out in Woolridge
> >> >> > > <https://faculty.fuqua.duke.edu/~moorman/Wooldridge,%20FE%20
> >> >> and%20RE.pdf
> >> >> > >'s
> >> >> > > Econometrics text). This assumption is a strong one for many
> >> >> > > observational datasets, and most analyses in economics do not use
> >> >> random
> >> >> > > effects for this reason. *Is there a reason why observational
> >> >> ecological
> >> >> > > datasets would be fundamentally different that I am missing? Why
> is
> >> >> this
> >> >> > > important assumption (to have unbiased estimates from random
> >> effects)
> >> >> > > not emphasized in ecology? *
> >> >> > >
> >> >> > > Thanks!
> >> >> > >
> >> >> > > Laura
> >> >> > >
> >> >> > > --
> >> >> > > Laura Dee
> >> >> > > Post-doctoral Associate
> >> >> > > University of Minnesota
> >> >> > > ledee at umn.edu <mailto:ledee at umn.edu>
> >> >> > > lauraedee.com <http://lauraedee.com>
> >> >> >
> >> >> > _______________________________________________
> >> >> > R-sig-mixed-models at r-project.org mailing list
> >> >> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >> >> >
> >> >>
> >> >>         [[alternative HTML version deleted]]
> >> >>
> >> >> _______________________________________________
> >> >> R-sig-mixed-models at r-project.org mailing list
> >> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >> >>
> >> >
> >> >
> >> >
> >> > --
> >> >
> >> >
> >> >
> >> >
> >> > Thanks,
> >> > John
> >> >
> >> >
> >> > John Poe
> >> > Doctoral Candidate
> >> > Department of Political Science
> >> > Research Methodologist
> >> > UK Center for Public Health Services & Systems Research
> >> > University of Kentucky
> >> > 111 Washington Avenue, Room 203a
> >> > Lexington, KY 40536
> >> > www.johndavidpoe.com
> >> >
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >>
> >>
> >> ------------------------------
> >>
> >> Message: 2
> >> Date: Tue, 11 Oct 2016 22:47:41 -0400
> >> From: "Poe, John" <jdpo223 at g.uky.edu>
> >> To: Jake Westfall <jake.a.westfall at gmail.com>
> >> Cc: r-sig-mixed-models at r-project.org
> >> Subject: Re: [R-sig-ME] Assumptions of random effects for unbiased
> >>         estimates
> >> Message-ID:
> >>         <CAFW8ByrhGyML6DE=dMnmNm7xSeWB6zBDgvR_HaDy2Vnn53hnPQ at mail.gm
> >> ail.com>
> >> Content-Type: text/plain; charset="UTF-8"
> >>
> >>
> >> Thanks Jake!
> >>
> >> On Oct 11, 2016 9:50 PM, "Jake Westfall" <jake.a.westfall at gmail.com>
> >> wrote:
> >>
> >> > What a nice contribution from John!
> >> >
> >> > Jake
> >> >
> >> > On Tue, Oct 11, 2016 at 8:11 PM, Poe, John <jdpo223 at g.uky.edu> wrote:
> >> >
> >> > > My reading of modern work by panel data econometricians is that they
> >> seem
> >> > > very fine with the use of mixed effects models that properly
> >> > differentiate
> >> > > effects at different levels of analysis and the tools to do so have
> >> > existed
> >> > > in that literature since the early 1980s. They have been borrowing
> >> > heavily
> >> > > from the mixed effects literature in designing econometric models
> and
> >> > talk
> >> > > about them in panel data textbooks. This hasn't typically filtered
> >> down
> >> > to
> >> > > applied economists who tend to misunderstand what other fields do
> >> because
> >> > > other fields just tend to talk about them differently.
> >> > >
> >> > > The short version:
> >> > > Everyone in the mixed effects literature just uses group/grand mean
> >> > > centering and random coefficients to deal with endogeneity bias. If
> >> you
> >> > are
> >> > > an economist and someone outside of econ says mixed effects models
> you
> >> > > should think *correlated random effects models* and not *random
> >> effects
> >> > > models*.
> >> > >
> >> > > The long version:
> >> > > Economists are pretty afraid error structures that are correlated
> with
> >> > > independent variables in general and have built up pretty elaborate
> >> > > statistical models to deal with the problem. In panel data, this
> >> > manifests
> >> > > itself as wanting to avoid confounding effects at different levels
> of
> >> > > analysis so that within group varying effects are segregated from
> >> between
> >> > > group varying effects. It can also happen when you are omitting
> higher
> >> > > level random effects
> >> > > <http://methods.johndavidpoe.com/2016/09/09/independence-
> >> > across-levels-in-mixed-effects-models/>
> >> > > and they are distorting the structure of the random effects that you
> >> are
> >> > > including. This is generally a good thing as you want to be able to
> >> test
> >> > > hypotheses at specific levels of analysis without confounding.
> >> > >
> >> > > It's a big enough theoretical concern in the discipline that they
> >> usually
> >> > > just want to remove all between group effects from the data as a
> >> > *default* to
> >> > > get level one effects because it is simpler and more fool proof than
> >> > > dealing with the problem in a mixed effects setting. It's so
> pervasive
> >> > that
> >> > > they are often socialized into not designing hypotheses for any
> >> between
> >> > > group or cross-level variation and just focus on within group (time
> >> > > varying) variability when at all possible (what economists call
> >> *within
> >> > > effects*).
> >> > >
> >> > > What economists refer to as fixed effects models just difference out
> >> all
> >> > > between group variation so that it cannot contaminate within group
> >> > effects
> >> > > (bias level one coefficients). It's the equivalent to including
> group
> >> > > indicator variables in the model instead of a random effect and just
> >> > > accepting that you can't make substantive inferences about anything
> at
> >> > the
> >> > > group level (what economists call *between effects*).
> >> > >
> >> > > The typical conventional wisdom in applied econometrics is to use a
> >> > > Hausman test which is a generic test comparing coefficients between
> a
> >> > > random effects model (with no level 2 covariates) and a model with
> all
> >> > > between group variability removed from the data. If there are
> >> differences
> >> > > between the two, then they prefer to go with the latter. This is bad
> >> > > practice according to econometrics textbooks but applied people
> don't
> >> > seem
> >> > > to care (Baltagi 2013 ch 4.3). This only makes sense if you don't
> care
> >> > > about group invariant variables that only differ crosssectionally
> >> and/or
> >> > > you think of their effects as contamination. Panel data econometrics
> >> > > textbooks tend to argue for a wider range of options here but in
> >> practice
> >> > > not that many economists seem to use them.
> >> > >
> >> > > There's an alternative framework in econ for dealing with this
> problem
> >> > > that they call a Mundlak device (Mundlak 1978) or correlated random
> >> > effects
> >> > > models (Baltagi Handbook of Panel Data 2014 ch 6.3.3 or really any
> >> panel
> >> > > data textbook) which is equivalent to a hierarchical linear model
> with
> >> > > group mean centering for level-one variables. This approach is used
> in
> >> > > econometrics by some pretty standard advanced panel data models
> (e.g.
> >> > > Hausman-Taylor and Arellano Bond). The other alternative that is
> >> > advocated
> >> > > by panel data econometricians but doesn't seem to have filtered down
> >> to
> >> > > rank and file economists is to use random coefficients models and
> just
> >> > > allow the random effects to be correlated with level one variables
> >> (Hsiao
> >> > > 2014 chapter 6 and most of his other written work).
> >> > >
> >> > > It is important to understand that efficiency isn't the primary
> reason
> >> > for
> >> > > use of a mixed effects model over a fixed effects model for most
> >> > research.
> >> > > A common reason to use a mixed effects model is that you have
> >> hypotheses
> >> > > about variables operating at higher levels of analysis or
> cross-level
> >> > > interactions and those questions cannot be answered by fixed effects
> >> > panel
> >> > > models that have removed all between group variability from the
> >> analysis.
> >> > > You are sacrificing the ability to test group variant hypotheses by
> >> > using a
> >> > > basic fixed effects model over a mixed effects model. For nonlinear
> >> > models
> >> > > like a logistic regression it can also be very difficult to use an
> >> > unbiased
> >> > > fixed effects model (though there are ways in a panel setting e.g.
> >> Hahn
> >> > and
> >> > > Newy 2004) and trivial to use a mixed effects model.
> >> > >
> >> > > Panel data econometricians almost always talk about typical practice
> >> > among
> >> > > applied economists using fixed effects as flawed (see Baltagi 2013
> ch.
> >> > > 4.3). Mark Nerlov's 2000 History of Panel Data Econometrics is my
> >> > favorite
> >> > > example:
> >> > >
> >> > > The absurdity of the contention that possible correlation between
> >> some of
> >> > >> the observed explanatory variables and the individual-specific
> >> > component of
> >> > >> the disturbance is a ground for using fixed effects should be clear
> >> from
> >> > >> the following example: Consider a panel of households with data on
> >> > >> consumption and income. We are trying to estimate a consumption
> >> > function.
> >> > >> Income varies across households and over time. The variation across
> >> > >> households is related to ability of the main earner and other
> >> household
> >> > >> specific factors which vary little over time, that is to say,
> reflect
> >> > >> mainly differences in permanent income. Such permanent differences
> in
> >> > >> income are widely believed to be the source of most differences in
> >> > >> consumption both crosssectionally and over time, whereas,
> variations
> >> of
> >> > >> income over time are likely to be mostly transitory and unrelated
> to
> >> > >> consumption in most categories. Yet, fixed-effects regressions are
> >> > >> equivalent to using only this variation and discarding the
> >> information
> >> > on
> >> > >> the consumption-income relationship contained the cross-section
> >> > variation
> >> > >> among the household means.
> >> > >
> >> > >
> >> > > See the last couple of pages of this lecture
> >> > > <http://www.johndavidpoe.com/wp-content/uploads/2012/09/
> >> > Blalock-Lecture.pdf> for
> >> > > the citations in the econometrics and multilevel literature that I
> >> > > referenced.
> >> > >
> >> > >
> >> > >
> >> > > On Tue, Oct 11, 2016 at 3:32 PM, Jake Westfall <
> >> > jake.a.westfall at gmail.com>
> >> > > wrote:
> >> > >
> >> > >> Hi Laura and Ben,
> >> > >>
> >> > >> I like this paper on this topic:
> >> > >> http://psych.colorado.edu/~westfaja/FixedvsRandom.pdf
> >> > >>
> >> > >> What it comes down to essentially is that if the cluster effects
> are
> >> > >> correlated with the "time-varying" (i.e., within-cluster varying) X
> >> > >> predictor -- so that, for example, some clusters have high means
> on X
> >> > and
> >> > >> others have low means on X -- then there is the possibility that
> the
> >> > >> average within-cluster effect (which is what the fixed effect model
> >> > >> estimates) differs from the overall effect of X, not conditional on
> >> the
> >> > >> clusters. An extreme example of this is Simpson's paradox. Now
> since
> >> the
> >> > >> estimate from the random-effects model can be seen as a weighted
> >> average
> >> > >> of
> >> > >> these two effects, it will generally be pulled to some extent away
> >> from
> >> > >> the
> >> > >> fixed-effect estimate toward the unconditional estimate, which is
> the
> >> > bias
> >> > >> that econometricians fret about. However, if the cluster effects
> are
> >> not
> >> > >> correlated with X, so that each cluster has the same mean on X,
> then
> >> > this
> >> > >> situation is not possible, so the random-effect model will give the
> >> same
> >> > >> unbiased estimate as the fixed-effect model.
> >> > >>
> >> > >> A simple solution to this problem is to retain the random-effect
> >> model,
> >> > >> but
> >> > >> to split the predictor X into two components, one representing the
> >> > >> within-cluster variation of X and the other representing the
> >> > >> between-cluster variation of X, and estimate separate slopes for
> >> these
> >> > two
> >> > >> effects. One can even test whether these two slopes differ from
> each
> >> > >> other,
> >> > >> which is conceptually similar to what the Hausman test does. As
> >> > described
> >> > >> in the paper linked above, the estimate of the within-cluster
> >> component
> >> > of
> >> > >> the X effect equals the estimate one would obtain from a
> fixed-effect
> >> > >> model.
> >> > >>
> >> > >> As for the original question, I can't speak for common practice in
> >> > >> ecology,
> >> > >> but I suspect it may be like it is in my home field of psychology,
> >> where
> >> > >> we
> >> > >> do worry about this issue (to some extent), but we discuss it using
> >> > >> completely different language. That is, we discuss it in terms of
> >> > whether
> >> > >> there are different effects of the predictor at the within-cluster
> >> and
> >> > >> between-cluster levels, and how our model might account for that.
> >> > >>
> >> > >> Jake
> >> > >>
> >> > >> On Tue, Oct 11, 2016 at 1:50 PM, Ben Bolker <bbolker at gmail.com>
> >> wrote:
> >> > >>
> >> > >> >
> >> > >> >   I didn't respond to this offline, as it took me a while even to
> >> > start
> >> > >> > to come up to speed on the question.  Random effects are indeed
> >> > defined
> >> > >> > from *very* different points of view in the two communities
> >> > >> > ([bio]statistical vs. econometric); I'm sure there are points of
> >> > >> > contact, but I've been having a hard time getting my head around
> it
> >> > all.
> >> > >> >
> >> > >> > Econometric definition:
> >> > >> >
> >> > >> > The wikipedia page <https://en.wikipedia.org/
> >> > wiki/Random_effects_model>
> >> > >> > and CrossValidated question
> >> > >> > <http://stats.stackexchange.com/questions/66161/why-do-
> >> > >> > random-effect-models-require-the-effects-to-be-uncorrelated-
> >> > >> with-the-inpu>
> >> > >> > were both helpful for me.
> >> > >> >
> >> > >> >  In the (bio)statistical world fixed and random effects are
> usually
> >> > >> > justified practically in terms of shrinkage estimators, or
> >> > >> > philosophically in terms of random draws from an exchangeable set
> >> of
> >> > >> > levels: e.g. see
> >> > >> > <http://stats.stackexchange.com/questions/4700/what-is-
> >> > >> > the-difference-between-fixed-effect-random-effect-and-mixed-
> >> > >> effect-mode/>
> >> > >> > for links.
> >> > >> >
> >> > >> >   I don't think I can really write an answer yet.  I'm still
> >> trying to
> >> > >> > understand at an intuitive or heuristic level what it means for
> >> > >> > Cov(x_it,c_i)=0, where x_it is a set of explanatory variables
> over
> >> > time
> >> > >> > for an individual subject and c_i is the conditional mode (=BLUP
> in
> >> > >> > linear mixed-model-land) for the deviation of the individual i
> from
> >> > the
> >> > >> > population mean ... or more particularly what it means for that
> >> > >> > condition to be violated, which is the point at which fixed
> effects
> >> > >> > would become preferred.
> >> > >> >
> >> > >> >   As a side note, some statisticians (Andrew Gelman is the one
> who
> >> > >> > springs to mind) have commented on the possible overemphasis on
> >> bias.
> >> > >> > (All else being equal unbiased estimators are preferred to biased
> >> > >> > estimators but all else is not always equal). Two examples: (1)
> >> > >> > penalized estimators such as lasso/ridge regression (closely
> >> related
> >> > to
> >> > >> > mixed models) give biased parameter estimates with lower mean
> >> squared
> >> > >> > error. (2) When estimating variability, one has to choose a
> >> particular
> >> > >> > scale (variance, standard error, log(standard error), etc.) on
> >> which
> >> > one
> >> > >> > would prefer to get an unbiased answer.
> >> > >> >
> >> > >> > On 16-10-11 12:02 PM, Laura Dee wrote:
> >> > >> > > Dear all,
> >> > >> > > Random effects are more efficient estimators ? however they
> come
> >> at
> >>
> >> > >> the
> >> > >> > > cost of the assumption that the random effect is not correlated
> >> with
> >> > >> the
> >> > >> > > included explanatory variables. Otherwise, using random effects
> >> > leads
> >> > >> to
> >> > >> > > biased estimates (e.g., as laid out in Woolridge
> >> > >> > > <https://faculty.fuqua.duke.edu/~moorman/Wooldridge,%20FE%20
> >> > >> and%20RE.pdf
> >> > >> > >'s
> >> > >> > > Econometrics text). This assumption is a strong one for many
> >> > >> > > observational datasets, and most analyses in economics do not
> use
> >> > >> random
> >> > >> > > effects for this reason. *Is there a reason why observational
> >> > >> ecological
> >> > >> > > datasets would be fundamentally different that I am missing?
> Why
> >> is
> >> > >> this
> >> > >> > > important assumption (to have unbiased estimates from random
> >> > effects)
> >> > >> > > not emphasized in ecology? *
> >> > >> > >
> >> > >> > > Thanks!
> >> > >> > >
> >> > >> > > Laura
> >> > >> > >
> >> > >> > > --
> >> > >> > > Laura Dee
> >> > >> > > Post-doctoral Associate
> >> > >> > > University of Minnesota
> >> > >> > > ledee at umn.edu <mailto:ledee at umn.edu>
> >> > >> > > lauraedee.com <http://lauraedee.com>
> >> > >> >
> >> > >> > _______________________________________________
> >> > >> > R-sig-mixed-models at r-project.org mailing list
> >> > >> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >> > >> >
> >> > >>
> >> > >>         [[alternative HTML version deleted]]
> >> > >>
> >> > >> _______________________________________________
> >> > >> R-sig-mixed-models at r-project.org mailing list
> >> > >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >> > >>
> >> > >
> >> > >
> >> > >
> >> > > --
> >> > >
> >> > >
> >> > >
> >> > >
> >> > > Thanks,
> >> > > John
> >> > >
> >> > >
> >> > > John Poe
> >> > > Doctoral Candidate
> >> > > Department of Political Science
> >> > > Research Methodologist
> >> > > UK Center for Public Health Services & Systems Research
> >> > > University of Kentucky
> >> > > 111 Washington Avenue, Room 203a
> >> > > Lexington, KY 40536
> >> > > www.johndavidpoe.com
> >>
> >
>
>
> --
> Laura Dee
> Post-doctoral Associate
> University of Minnesota
> ledee at umn.edu
> lauraedee.com
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



-- 




Thanks,
John


John Poe
Doctoral Candidate
Department of Political Science
Research Methodologist
UK Center for Public Health Services & Systems Research
University of Kentucky
111 Washington Avenue, Room 203a
Lexington, KY 40536
www.johndavidpoe.com

	[[alternative HTML version deleted]]


From ledee at umn.edu  Wed Oct 12 18:03:14 2016
From: ledee at umn.edu (Laura Dee)
Date: Wed, 12 Oct 2016 11:03:14 -0500
Subject: [R-sig-ME] Assumptions of random effects for unbiased estimates
In-Reply-To: <CAFW8ByqQHCB9pKbkCenK+C_dsJesOQrcfJ7XJdnUuF=EGHdnnQ@mail.gmail.com>
References: <CAAH-yP9GFu47dZwueSVbODnibzSwf9fjnTHvVCiESEPoot16jg@mail.gmail.com>
	<CABEErbb3ng7Xa3cqFj=gEuxSezdt4ogcYa0u6pJNooYOZ4YaXA@mail.gmail.com>
	<CAFW8ByqQHCB9pKbkCenK+C_dsJesOQrcfJ7XJdnUuF=EGHdnnQ@mail.gmail.com>
Message-ID: <CABEErbZb01RX5B0umeP6HehkX3HZtg_sXP9xyozhOtd_1CRq3Q@mail.gmail.com>

Thank you, John, for taking the time to write such a detailed response.
Much appreciated.

Laura

On Wednesday, October 12, 2016, Poe, John <jdpo223 at g.uky.edu> wrote:

> Laura,
>
> I think we might be talking past each other somewhat on the FE vs RE
> discussion. An RE model that has only level one (time or group varying)
> covariates and a latent variable for the expected value of group membership
> on Y is problematic for the reasons that you are talking about. But mixed
> effects models aren't typically the same specification as the FE model or
> the RE model. They either use group/grand mean centering of variables to
> instrument the problem or random coefficients to stop making the assumption
> of no correlation altogether.
>
> The primary benefit of a mixed effects approach is that you have a better
> handle on all of the factors that are influencing Y and you can make decent
> predictions about cases given a particular context. By using the fixed
> effects model you are literally making the choice not to care about things
> that you know influence Y.
>
> My usual advice if you don't care about between group effects or
> cross-level effects AND you don't care about making predictions about
> individuals or groups then fixed effects models that difference out between
> group variability are fine for linear models. Given Malcolm's working paper
> I might reevaluate that recommendation going forward.
>
> On point 1 that you raise about credibility of the argument that group
> mean centered models actually do fix the problems associated with random
> effects correlation:
>
>    - There's not actually a credibility problem here. There is a
>    specification test in econometrics that is analogous to a Hausman test but
>    compares fixed effects estimates to a hierarchical linear model. It's
>    called a Mundlak specification test
>    <http://blog.stata.com/2015/10/29/fixed-effects-or-random-effects-the-mundlak-approach/>.
>    So you can demonstrate that it's not a problem.
>    - The typical response when this test shows that there is still a
>    violation of the no correlation between a random effect and a level 1
>    variable assumption is to stop making that assumption and use a random
>    coefficients model. The Bell and Jones paper does a good job of working
>    through the logic here.
>
> On your point about the comment :"But group-mean centering can also be
> done with random effects models, with the same benefit you get with fixed
> effects models (isolation of the within effects), while still allowing for
> estimation of the between relationships"
>
>    - Group mean centering of variables tends to instrument the
>    endogeneity between the RE and the level one variables so that the
>    correlation isn't an issue anymore. In the basic RE model without level 2
>    covariates and without centering you have a problem. But that model doesn't
>    tend to get used much without criticism in political science, psychology,
>    or economics (I'm not an ecologist so I can't say for practice in your
>    field). It seems to be something of a straw-man in my experience.
>
> On point 2
>
> Based on John Poe's response and example with the income, I think that is
>> an argument that the model identification is wrong if you don't allow mean
>> income versus deviations from mean income to have different effects on consumption,
>> rather than an argument that RE solve the problem of unobserved
>> heterogeneity more credibly than FE. This is a point about model specification
>> rather than dealing with unobservable heterogeneity.
>
>
>    - All unobserved heterogeneity problems are about model specification.
>    It's an omitted variable bias problem. Random effects models are literally
>    just a version of the model with an additional latent variable for the
>    expected value of group membership. That latent variable can be generated
>    directly from coefficients off of dummy variables so they are esentially
>    reparamaterizations of one another.
>    - In a random effects setting you are including a new latent variable
>    and in a random coefficients model you are adding an interaction between
>    the random effect and the variable
>    - In most cases you can deal with the endogeneity issue by including
>    group means and then taking their deviations. When you can't do that, you
>    can use dummy variables or the group-level expectations for a random effect
>    and then interact that with the endogenous covariate.
>
>  On point 3 for FE in nonlinear models
>
>    - The basic issue is that in high dimensional problems the root
>    finding algorithms for nonlinear MLE tend to start to give biased answers
>    as the mode diverges away from the mean. This divergence between the mode
>    and the mean works as a function of the number of groups and the size of
>    the groups so that if T grows to infinity then there's no bias problem. I
>    haven't seen solid research on how unbalanced group size influences this
>    but my intuition says that it's likely to increase the bias problem. You
>    get bias in as few as ten dimensions and there's really no way to fix it
>    with convex optimization. So if you want to include dummy variables in
>    something like a logit you either need to use some version of expectation
>    maximization or integrate over the data then take the jacobian and hessian
>    directly. At that point you know where the mean is and you don't have to
>    rely on an optimizer so you will get unbiased results. That's why it's not
>    an issue in Poisson or a linear model. You can just calculate the
>    derivatives directly.
>
> As far as citations on incidental parameters bias:
>
>    - Lancaster, Tony. 2000. "The incidental parameter problem since
>    1948." Journal of Econometrics 95 (2):391-413.
>    - Katz, Ethan. 2001. "Bias in conditional and unconditional fixed
>    effects logit estimation." Political Analysis 9 (4):379-84.
>    - Hahn, J., & Newey, W. (2004). Jackknife and analytical bias
>    reduction for nonlinear panel models. Econometrica, 72(4), 1295-1319.
>    - Greene, William. 2004. "The behaviour of the maximum likelihood
>    estimator of limited dependent variable models in the presence of fixed
>    effects." The Econometrics Journal 7 (1):98-119.
>    - Bill Greene's chapter in Baltagi, B. H. (2014). *The Oxford Handbook
>    of Panel Data*. Oxford University Press, USA.
>    - Note that one of his recommendations is just to use group mean
>       centering and the mixed effects framework
>    - Beck, Nathaniel. 2015. Estimating grouped data models with a binary
>    dependent variable and fixed effects: What are the issues? Paper read at
>    annual meeting of the Society for Political Methodology, July
>
> On the bias variance trade-off question I think it's mostly just that
> economists aren't interested in prediction as much as estimating average
> causal effects. They don't seem to care if their results are applicable to
> any particular case so long as they describe the average effect of X on Y
> well.
>
>
> On Wed, Oct 12, 2016 at 9:37 AM, Laura Dee <ledee at umn.edu
> <javascript:_e(%7B%7D,'cvml','ledee at umn.edu');>> wrote:
>
>> Dear all,
>> Thanks all - very interesting and helpful responses. I think I should have
>> been clearer with my question: in my case, the unobserved heterogeneity
>> between groups as not being of interest to study (but something to be
>> controlled for to isolate the effects of other x_ij's). Also I'm using a
>> linear model setting. The paper Jake and Malcolm sent was very help to lay
>> out these issues and suggest the within and between RE model when you want
>> to be studying the between group variation. And, I'll go through John
>> Poe's
>> slides in detail.
>>
>> There are four points that have emerged from this discussion that I think
>> are worth teasing apart:
>>
>> *1)*  Interest in studying the mean effects and how they differ between
>> groups, which FE do not allow because they remove the mean effect.
>> However,
>> even with RE and the ability to study those between differences, you still
>> have the challenge of credibly identifying the mean effects of income on
>> y_ij -- and whether you have ruled out/controlled for other factors that
>> vary cross-sectionally. RE do not solve this issue but are preferred
>> because the mean effect between groups is what is of interest. Therfore,
>> one is willing to accept some bias in the estimates if there are other
>> unobserved variables that vary cross-sectionally and influence the
>> outcome.
>>
>> Further, Malcolm, I agree that both FE and RE both try to account for a
>> group mean but not this statement because of the assumption of RE: "But
>> group-mean centering can also be done with random effects models, with the
>> same benefit you get with fixed effects models (isolation of the within
>> effects), while still allowing for estimation of the between
>> relationships"
>> However, estimates are unbiased if FE are correlated with the error term,
>> which is not the case for RE. Though, agreed, if it is the between group
>> variation that is of interest, then it does not make sense to use a FE
>> model and there is too much focus on bias over other issues (i.e.,
>> estimating the effect of interest).
>>
>> *Question: *in the Bell & Jones paper that Jake sent, they present
>> the Pl?mper and Troeger?s (2007) fixed effects vector decomposition. Is
>> that used often? I don't think it has made it's way to ecology.
>>
>> *2)* Based on John Poe's response and example with the income, I think
>> that
>> is an argument that the model identification is wrong if you don't allow
>> mean income versus deviations from mean income to have different effects
>> on
>> consumption, rather than an argument that RE solve the problem of
>> unobserved heterogeneity more credibly than FE. This is a point about
>> model
>> specification rather than dealing with unobservable heterogeneity.
>>
>> *3) *Agreed that FE are biased with some forms of non-linear models. Could
>> anyone send me some more recent papers on this topic?
>>
>> *4) *Ben raised the issue of a bias-variance trade-off, which is a good
>> point and economists seem to focus more (and maybe too much) on bias.
>> However, with enough observations, it's less of a trade-off.
>>
>> Many thanks to everyone,
>> Laura
>>
>>
>>
>> On Wed, Oct 12, 2016 at 3:12 AM, Malcolm Fairbrother <
>> M.Fairbrother at bristol.ac.uk
>> <javascript:_e(%7B%7D,'cvml','M.Fairbrother at bristol.ac.uk');>> wrote:
>>
>> > As others have said, there are rather peculiar inconsistencies between
>> > what the methodological literature knows and what empirical economists
>> > actually do.
>> >
>> > I think the paper Jake cited (by my colleagues at Bristol/Sheffield) is
>> > indeed one of the most useful on all this. The published version of the
>> > paper is at: http://dx.doi.org/10.1017/psrm.2014.7
>> >
>> > The following working paper (by them and me) takes up similar themes:
>> > https://www.researchgate.net/publication/299604336_Fixed_
>> > and_Random_effects_making_an_informed_choice
>> >
>> > One of the additional limitations we note here with fixed effects models
>> > (using a simulation study) is that they can be anti-conservative, in the
>> > sense that the SEs they return are too small if the data are generated
>> from
>> > a random slopes model.
>> >
>> > In brief, Laura, fixed effects models only estimate within-group
>> > relationships, whereas random effects (AKA multilevel, mixed) models can
>> > estimate within- and between-group relationships. The estimation of
>> fixed
>> > effects models implicitly entails group mean centering (though the
>> models
>> > are typically written out as though unit dummies are estimated). But
>> > group-mean centering can also be done with random effects models, with
>> the
>> > same benefit you get with fixed effects models (isolation of the within
>> > effects), while still allowing for estimation of the between
>> relationships.
>> > You might have less confidence that the between component of some x_ij
>> is
>> > uncorrelated with the unit error term, but it is still possible for the
>> > within (group-mean-centered) component to be correlated with the
>> > observation-level error term. So I would agree that bias is worth
>> thinking
>> > about, but using fixed effects is no more helpful than random effects
>> as a
>> > solution to the problem.
>> >
>> > Hope that's useful,
>> > Malcolm
>> >
>> >
>> > Dr Malcolm Fairbrother
>> > Reader in Global Policy and Politics
>> > School of Geographical Sciences  ?  Cabot Institute  ?  Centre for
>> > Multilevel Modelling
>> > University of Bristol
>> >
>> >
>> >
>> >
>> > Date: Tue, 11 Oct 2016 20:49:45 -0500
>> >> From: Jake Westfall <jake.a.westfall at gmail.com
>> <javascript:_e(%7B%7D,'cvml','jake.a.westfall at gmail.com');>>
>> >> To: r-sig-mixed-models at r-project.org
>> <javascript:_e(%7B%7D,'cvml','r-sig-mixed-models at r-project.org');>
>> >> Subject: Re: [R-sig-ME] Assumptions of random effects for unbiased
>> >>         estimates
>> >> Message-ID:
>> >>         <CAE9_Wg6+ZFXh-9on=nmuUwLKO6ScXjMRfbgf4y+XpGNhVAwJqA at mail.gm
>> <javascript:_e(%7B%7D,'cvml','nmuUwLKO6ScXjMRfbgf4y%2BXpGNhVAwJqA at mail.gm');>
>> >> ail.com>
>> >> Content-Type: text/plain; charset="UTF-8"
>> >>
>> >>
>> >> What a nice contribution from John!
>> >>
>> >> Jake
>> >>
>> >> On Tue, Oct 11, 2016 at 8:11 PM, Poe, John <jdpo223 at g.uky.edu
>> <javascript:_e(%7B%7D,'cvml','jdpo223 at g.uky.edu');>> wrote:
>> >>
>> >> > My reading of modern work by panel data econometricians is that they
>> >> seem
>> >> > very fine with the use of mixed effects models that properly
>> >> differentiate
>> >> > effects at different levels of analysis and the tools to do so have
>> >> existed
>> >> > in that literature since the early 1980s. They have been borrowing
>> >> heavily
>> >> > from the mixed effects literature in designing econometric models and
>> >> talk
>> >> > about them in panel data textbooks. This hasn't typically filtered
>> down
>> >> to
>> >> > applied economists who tend to misunderstand what other fields do
>> >> because
>> >> > other fields just tend to talk about them differently.
>> >> >
>> >> > The short version:
>> >> > Everyone in the mixed effects literature just uses group/grand mean
>> >> > centering and random coefficients to deal with endogeneity bias. If
>> you
>> >> are
>> >> > an economist and someone outside of econ says mixed effects models
>> you
>> >> > should think *correlated random effects models* and not *random
>> effects
>> >> > models*.
>> >> >
>> >> > The long version:
>> >> > Economists are pretty afraid error structures that are correlated
>> with
>> >> > independent variables in general and have built up pretty elaborate
>> >> > statistical models to deal with the problem. In panel data, this
>> >> manifests
>> >> > itself as wanting to avoid confounding effects at different levels of
>> >> > analysis so that within group varying effects are segregated from
>> >> between
>> >> > group varying effects. It can also happen when you are omitting
>> higher
>> >> > level random effects
>> >> > <http://methods.johndavidpoe.com/2016/09/09/independence-acr
>> >> oss-levels-in-mixed-effects-models/>
>> >> > and they are distorting the structure of the random effects that you
>> are
>> >> > including. This is generally a good thing as you want to be able to
>> test
>> >> > hypotheses at specific levels of analysis without confounding.
>> >> >
>> >> > It's a big enough theoretical concern in the discipline that they
>> >> usually
>> >> > just want to remove all between group effects from the data as a
>> >> *default* to
>> >> > get level one effects because it is simpler and more fool proof than
>> >> > dealing with the problem in a mixed effects setting. It's so
>> pervasive
>> >> that
>> >> > they are often socialized into not designing hypotheses for any
>> between
>> >> > group or cross-level variation and just focus on within group (time
>> >> > varying) variability when at all possible (what economists call
>> *within
>> >> > effects*).
>> >> >
>> >> > What economists refer to as fixed effects models just difference out
>> all
>> >> > between group variation so that it cannot contaminate within group
>> >> effects
>> >> > (bias level one coefficients). It's the equivalent to including group
>> >> > indicator variables in the model instead of a random effect and just
>> >> > accepting that you can't make substantive inferences about anything
>> at
>> >> the
>> >> > group level (what economists call *between effects*).
>> >> >
>> >> > The typical conventional wisdom in applied econometrics is to use a
>> >> > Hausman test which is a generic test comparing coefficients between a
>> >> > random effects model (with no level 2 covariates) and a model with
>> all
>> >> > between group variability removed from the data. If there are
>> >> differences
>> >> > between the two, then they prefer to go with the latter. This is bad
>> >> > practice according to econometrics textbooks but applied people don't
>> >> seem
>> >> > to care (Baltagi 2013 ch 4.3). This only makes sense if you don't
>> care
>> >> > about group invariant variables that only differ crosssectionally
>> and/or
>> >> > you think of their effects as contamination. Panel data econometrics
>> >> > textbooks tend to argue for a wider range of options here but in
>> >> practice
>> >> > not that many economists seem to use them.
>> >> >
>> >> > There's an alternative framework in econ for dealing with this
>> problem
>> >> > that they call a Mundlak device (Mundlak 1978) or correlated random
>> >> effects
>> >> > models (Baltagi Handbook of Panel Data 2014 ch 6.3.3 or really any
>> panel
>> >> > data textbook) which is equivalent to a hierarchical linear model
>> with
>> >> > group mean centering for level-one variables. This approach is used
>> in
>> >> > econometrics by some pretty standard advanced panel data models (e.g.
>> >> > Hausman-Taylor and Arellano Bond). The other alternative that is
>> >> advocated
>> >> > by panel data econometricians but doesn't seem to have filtered down
>> to
>> >> > rank and file economists is to use random coefficients models and
>> just
>> >> > allow the random effects to be correlated with level one variables
>> >> (Hsiao
>> >> > 2014 chapter 6 and most of his other written work).
>> >> >
>> >> > It is important to understand that efficiency isn't the primary
>> reason
>> >> for
>> >> > use of a mixed effects model over a fixed effects model for most
>> >> research.
>> >> > A common reason to use a mixed effects model is that you have
>> hypotheses
>> >> > about variables operating at higher levels of analysis or cross-level
>> >> > interactions and those questions cannot be answered by fixed effects
>> >> panel
>> >> > models that have removed all between group variability from the
>> >> analysis.
>> >> > You are sacrificing the ability to test group variant hypotheses by
>> >> using a
>> >> > basic fixed effects model over a mixed effects model. For nonlinear
>> >> models
>> >> > like a logistic regression it can also be very difficult to use an
>> >> unbiased
>> >> > fixed effects model (though there are ways in a panel setting e.g.
>> Hahn
>> >> and
>> >> > Newy 2004) and trivial to use a mixed effects model.
>> >> >
>> >> > Panel data econometricians almost always talk about typical practice
>> >> among
>> >> > applied economists using fixed effects as flawed (see Baltagi 2013
>> ch.
>> >> > 4.3). Mark Nerlov's 2000 History of Panel Data Econometrics is my
>> >> favorite
>> >> > example:
>> >> >
>> >> > The absurdity of the contention that possible correlation between
>> some
>> >> of
>> >> >> the observed explanatory variables and the individual-specific
>> >> component of
>> >> >> the disturbance is a ground for using fixed effects should be clear
>> >> from
>> >> >> the following example: Consider a panel of households with data on
>> >> >> consumption and income. We are trying to estimate a consumption
>> >> function.
>> >> >> Income varies across households and over time. The variation across
>> >> >> households is related to ability of the main earner and other
>> household
>> >> >> specific factors which vary little over time, that is to say,
>> reflect
>> >> >> mainly differences in permanent income. Such permanent differences
>> in
>> >> >> income are widely believed to be the source of most differences in
>> >> >> consumption both crosssectionally and over time, whereas,
>> variations of
>> >> >> income over time are likely to be mostly transitory and unrelated to
>> >> >> consumption in most categories. Yet, fixed-effects regressions are
>> >> >> equivalent to using only this variation and discarding the
>> information
>> >> on
>> >> >> the consumption-income relationship contained the cross-section
>> >> variation
>> >> >> among the household means.
>> >> >
>> >> >
>> >> > See the last couple of pages of this lecture
>> >> > <http://www.johndavidpoe.com/wp-content/uploads/2012/09/Blal
>> >> ock-Lecture.pdf> for
>> >>
>> >> > the citations in the econometrics and multilevel literature that I
>> >> > referenced.
>> >> >
>> >> >
>> >> >
>> >> > On Tue, Oct 11, 2016 at 3:32 PM, Jake Westfall <
>> >> jake.a.westfall at gmail.com
>> <javascript:_e(%7B%7D,'cvml','jake.a.westfall at gmail.com');>>
>> >> > wrote:
>> >> >
>> >> >> Hi Laura and Ben,
>> >> >>
>> >> >> I like this paper on this topic:
>> >> >> http://psych.colorado.edu/~westfaja/FixedvsRandom.pdf
>> >> >>
>> >> >> What it comes down to essentially is that if the cluster effects are
>> >> >> correlated with the "time-varying" (i.e., within-cluster varying) X
>> >> >> predictor -- so that, for example, some clusters have high means on
>> X
>> >> and
>> >> >> others have low means on X -- then there is the possibility that the
>> >> >> average within-cluster effect (which is what the fixed effect model
>> >> >> estimates) differs from the overall effect of X, not conditional on
>> the
>> >> >> clusters. An extreme example of this is Simpson's paradox. Now since
>> >> the
>> >> >> estimate from the random-effects model can be seen as a weighted
>> >> average
>> >> >> of
>> >> >> these two effects, it will generally be pulled to some extent away
>> from
>> >> >> the
>> >> >> fixed-effect estimate toward the unconditional estimate, which is
>> the
>> >> bias
>> >> >> that econometricians fret about. However, if the cluster effects are
>> >> not
>> >> >> correlated with X, so that each cluster has the same mean on X, then
>> >> this
>> >> >> situation is not possible, so the random-effect model will give the
>> >> same
>> >> >> unbiased estimate as the fixed-effect model.
>> >> >>
>> >> >> A simple solution to this problem is to retain the random-effect
>> model,
>> >> >> but
>> >> >> to split the predictor X into two components, one representing the
>> >> >> within-cluster variation of X and the other representing the
>> >> >> between-cluster variation of X, and estimate separate slopes for
>> these
>> >> two
>> >> >> effects. One can even test whether these two slopes differ from each
>> >> >> other,
>> >> >> which is conceptually similar to what the Hausman test does. As
>> >> described
>> >> >> in the paper linked above, the estimate of the within-cluster
>> >> component of
>> >> >> the X effect equals the estimate one would obtain from a
>> fixed-effect
>> >> >> model.
>> >> >>
>> >> >> As for the original question, I can't speak for common practice in
>> >> >> ecology,
>> >> >> but I suspect it may be like it is in my home field of psychology,
>> >> where
>> >> >> we
>> >> >> do worry about this issue (to some extent), but we discuss it using
>> >> >> completely different language. That is, we discuss it in terms of
>> >> whether
>> >> >> there are different effects of the predictor at the within-cluster
>> and
>> >> >> between-cluster levels, and how our model might account for that.
>> >> >>
>> >> >> Jake
>> >> >>
>> >> >> On Tue, Oct 11, 2016 at 1:50 PM, Ben Bolker <bbolker at gmail.com
>> <javascript:_e(%7B%7D,'cvml','bbolker at gmail.com');>> wrote:
>> >> >>
>> >> >> >
>> >> >> >   I didn't respond to this offline, as it took me a while even to
>> >> start
>> >> >> > to come up to speed on the question.  Random effects are indeed
>> >> defined
>> >> >> > from *very* different points of view in the two communities
>> >> >> > ([bio]statistical vs. econometric); I'm sure there are points of
>> >> >> > contact, but I've been having a hard time getting my head around
>> it
>> >> all.
>> >> >> >
>> >> >> > Econometric definition:
>> >> >> >
>> >> >> > The wikipedia page <https://en.wikipedia.org/wiki
>> >> /Random_effects_model>
>> >> >> > and CrossValidated question
>> >> >> > <http://stats.stackexchange.com/questions/66161/why-do-
>> >> >> > random-effect-models-require-the-effects-to-be-uncorrelated-
>> >> >> with-the-inpu>
>> >> >> > were both helpful for me.
>> >> >> >
>> >> >> >  In the (bio)statistical world fixed and random effects are
>> usually
>> >> >> > justified practically in terms of shrinkage estimators, or
>> >> >> > philosophically in terms of random draws from an exchangeable set
>> of
>> >> >> > levels: e.g. see
>> >> >> > <http://stats.stackexchange.com/questions/4700/what-is-
>> >> >> > the-difference-between-fixed-effect-random-effect-and-mixed-
>> >> >> effect-mode/>
>> >> >> > for links.
>> >> >> >
>> >> >> >   I don't think I can really write an answer yet.  I'm still
>> trying
>> >> to
>> >> >> > understand at an intuitive or heuristic level what it means for
>> >> >> > Cov(x_it,c_i)=0, where x_it is a set of explanatory variables over
>> >> time
>> >> >> > for an individual subject and c_i is the conditional mode (=BLUP
>> in
>> >> >> > linear mixed-model-land) for the deviation of the individual i
>> from
>> >> the
>> >> >> > population mean ... or more particularly what it means for that
>> >> >> > condition to be violated, which is the point at which fixed
>> effects
>> >> >> > would become preferred.
>> >> >> >
>> >> >> >   As a side note, some statisticians (Andrew Gelman is the one who
>> >> >> > springs to mind) have commented on the possible overemphasis on
>> bias.
>> >> >> > (All else being equal unbiased estimators are preferred to biased
>> >> >> > estimators but all else is not always equal). Two examples: (1)
>> >> >> > penalized estimators such as lasso/ridge regression (closely
>> related
>> >> to
>> >> >> > mixed models) give biased parameter estimates with lower mean
>> squared
>> >> >> > error. (2) When estimating variability, one has to choose a
>> >> particular
>> >> >> > scale (variance, standard error, log(standard error), etc.) on
>> which
>> >> one
>> >> >> > would prefer to get an unbiased answer.
>> >> >> >
>> >> >> > On 16-10-11 12:02 PM, Laura Dee wrote:
>> >> >> > > Dear all,
>> >> >> > > Random effects are more efficient estimators ? however they
>> come at
>> >>
>> >> >> the
>> >> >> > > cost of the assumption that the random effect is not correlated
>> >> with
>> >> >> the
>> >> >> > > included explanatory variables. Otherwise, using random effects
>> >> leads
>> >> >> to
>> >> >> > > biased estimates (e.g., as laid out in Woolridge
>> >> >> > > <https://faculty.fuqua.duke.edu/~moorman/Wooldridge,%20FE%20
>> >> >> and%20RE.pdf
>> >> >> > >'s
>> >> >> > > Econometrics text). This assumption is a strong one for many
>> >> >> > > observational datasets, and most analyses in economics do not
>> use
>> >> >> random
>> >> >> > > effects for this reason. *Is there a reason why observational
>> >> >> ecological
>> >> >> > > datasets would be fundamentally different that I am missing?
>> Why is
>> >> >> this
>> >> >> > > important assumption (to have unbiased estimates from random
>> >> effects)
>> >> >> > > not emphasized in ecology? *
>> >> >> > >
>> >> >> > > Thanks!
>> >> >> > >
>> >> >> > > Laura
>> >> >> > >
>> >> >> > > --
>> >> >> > > Laura Dee
>> >> >> > > Post-doctoral Associate
>> >> >> > > University of Minnesota
>> >> >> > > ledee at umn.edu <javascript:_e(%7B%7D,'cvml','ledee at umn.edu');>
>> <mailto:ledee at umn.edu <javascript:_e(%7B%7D,'cvml','ledee at umn.edu');>>
>> >> >> > > lauraedee.com <http://lauraedee.com>
>> >> >> >
>> >> >> > _______________________________________________
>> >> >> > R-sig-mixed-models at r-project.org
>> <javascript:_e(%7B%7D,'cvml','R-sig-mixed-models at r-project.org');>
>> mailing list
>> >> >> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >> >> >
>> >> >>
>> >> >>         [[alternative HTML version deleted]]
>> >> >>
>> >> >> _______________________________________________
>> >> >> R-sig-mixed-models at r-project.org
>> <javascript:_e(%7B%7D,'cvml','R-sig-mixed-models at r-project.org');>
>> mailing list
>> >> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >> >>
>> >> >
>> >> >
>> >> >
>> >> > --
>> >> >
>> >> >
>> >> >
>> >> >
>> >> > Thanks,
>> >> > John
>> >> >
>> >> >
>> >> > John Poe
>> >> > Doctoral Candidate
>> >> > Department of Political Science
>> >> > Research Methodologist
>> >> > UK Center for Public Health Services & Systems Research
>> >> > University of Kentucky
>> >> > 111 Washington Avenue, Room 203a
>> >> > Lexington, KY 40536
>> >> > www.johndavidpoe.com
>> >> >
>> >>
>> >>         [[alternative HTML version deleted]]
>> >>
>> >>
>> >>
>> >> ------------------------------
>> >>
>> >> Message: 2
>> >> Date: Tue, 11 Oct 2016 22:47:41 -0400
>> >> From: "Poe, John" <jdpo223 at g.uky.edu
>> <javascript:_e(%7B%7D,'cvml','jdpo223 at g.uky.edu');>>
>> >> To: Jake Westfall <jake.a.westfall at gmail.com
>> <javascript:_e(%7B%7D,'cvml','jake.a.westfall at gmail.com');>>
>> >> Cc: r-sig-mixed-models at r-project.org
>> <javascript:_e(%7B%7D,'cvml','r-sig-mixed-models at r-project.org');>
>> >> Subject: Re: [R-sig-ME] Assumptions of random effects for unbiased
>> >>         estimates
>> >> Message-ID:
>> >>         <CAFW8ByrhGyML6DE=dMnmNm7xSeWB6zBDgvR_HaDy2Vnn53hnPQ at mail.gm
>> <javascript:_e(%7B%7D,'cvml','dMnmNm7xSeWB6zBDgvR_HaDy2Vnn53hnPQ at mail.gm');>
>> >> ail.com>
>> >> Content-Type: text/plain; charset="UTF-8"
>> >>
>> >>
>> >> Thanks Jake!
>> >>
>> >> On Oct 11, 2016 9:50 PM, "Jake Westfall" <jake.a.westfall at gmail.com
>> <javascript:_e(%7B%7D,'cvml','jake.a.westfall at gmail.com');>>
>> >> wrote:
>> >>
>> >> > What a nice contribution from John!
>> >> >
>> >> > Jake
>> >> >
>> >> > On Tue, Oct 11, 2016 at 8:11 PM, Poe, John <jdpo223 at g.uky.edu
>> <javascript:_e(%7B%7D,'cvml','jdpo223 at g.uky.edu');>> wrote:
>> >> >
>> >> > > My reading of modern work by panel data econometricians is that
>> they
>> >> seem
>> >> > > very fine with the use of mixed effects models that properly
>> >> > differentiate
>> >> > > effects at different levels of analysis and the tools to do so have
>> >> > existed
>> >> > > in that literature since the early 1980s. They have been borrowing
>> >> > heavily
>> >> > > from the mixed effects literature in designing econometric models
>> and
>> >> > talk
>> >> > > about them in panel data textbooks. This hasn't typically filtered
>> >> down
>> >> > to
>> >> > > applied economists who tend to misunderstand what other fields do
>> >> because
>> >> > > other fields just tend to talk about them differently.
>> >> > >
>> >> > > The short version:
>> >> > > Everyone in the mixed effects literature just uses group/grand mean
>> >> > > centering and random coefficients to deal with endogeneity bias. If
>> >> you
>> >> > are
>> >> > > an economist and someone outside of econ says mixed effects models
>> you
>> >> > > should think *correlated random effects models* and not *random
>> >> effects
>> >> > > models*.
>> >> > >
>> >> > > The long version:
>> >> > > Economists are pretty afraid error structures that are correlated
>> with
>> >> > > independent variables in general and have built up pretty elaborate
>> >> > > statistical models to deal with the problem. In panel data, this
>> >> > manifests
>> >> > > itself as wanting to avoid confounding effects at different levels
>> of
>> >> > > analysis so that within group varying effects are segregated from
>> >> between
>> >> > > group varying effects. It can also happen when you are omitting
>> higher
>> >> > > level random effects
>> >> > > <http://methods.johndavidpoe.com/2016/09/09/independence-
>> >> > across-levels-in-mixed-effects-models/>
>> >> > > and they are distorting the structure of the random effects that
>> you
>> >> are
>> >> > > including. This is generally a good thing as you want to be able to
>> >> test
>> >> > > hypotheses at specific levels of analysis without confounding.
>> >> > >
>> >> > > It's a big enough theoretical concern in the discipline that they
>> >> usually
>> >> > > just want to remove all between group effects from the data as a
>> >> > *default* to
>> >> > > get level one effects because it is simpler and more fool proof
>> than
>> >> > > dealing with the problem in a mixed effects setting. It's so
>> pervasive
>> >> > that
>> >> > > they are often socialized into not designing hypotheses for any
>> >> between
>> >> > > group or cross-level variation and just focus on within group (time
>> >> > > varying) variability when at all possible (what economists call
>> >> *within
>> >> > > effects*).
>> >> > >
>> >> > > What economists refer to as fixed effects models just difference
>> out
>> >> all
>> >> > > between group variation so that it cannot contaminate within group
>> >> > effects
>> >> > > (bias level one coefficients). It's the equivalent to including
>> group
>> >> > > indicator variables in the model instead of a random effect and
>> just
>> >> > > accepting that you can't make substantive inferences about
>> anything at
>> >> > the
>> >> > > group level (what economists call *between effects*).
>> >> > >
>> >> > > The typical conventional wisdom in applied econometrics is to use a
>> >> > > Hausman test which is a generic test comparing coefficients
>> between a
>> >> > > random effects model (with no level 2 covariates) and a model with
>> all
>> >> > > between group variability removed from the data. If there are
>> >> differences
>> >> > > between the two, then they prefer to go with the latter. This is
>> bad
>> >> > > practice according to econometrics textbooks but applied people
>> don't
>> >> > seem
>> >> > > to care (Baltagi 2013 ch 4.3). This only makes sense if you don't
>> care
>> >> > > about group invariant variables that only differ crosssectionally
>> >> and/or
>> >> > > you think of their effects as contamination. Panel data
>> econometrics
>> >> > > textbooks tend to argue for a wider range of options here but in
>> >> practice
>> >> > > not that many economists seem to use them.
>> >> > >
>> >> > > There's an alternative framework in econ for dealing with this
>> problem
>> >> > > that they call a Mundlak device (Mundlak 1978) or correlated random
>> >> > effects
>> >> > > models (Baltagi Handbook of Panel Data 2014 ch 6.3.3 or really any
>> >> panel
>> >> > > data textbook) which is equivalent to a hierarchical linear model
>> with
>> >> > > group mean centering for level-one variables. This approach is
>> used in
>> >> > > econometrics by some pretty standard advanced panel data models
>> (e.g.
>> >> > > Hausman-Taylor and Arellano Bond). The other alternative that is
>> >> > advocated
>> >> > > by panel data econometricians but doesn't seem to have filtered
>> down
>> >> to
>> >> > > rank and file economists is to use random coefficients models and
>> just
>> >> > > allow the random effects to be correlated with level one variables
>> >> (Hsiao
>> >> > > 2014 chapter 6 and most of his other written work).
>> >> > >
>> >> > > It is important to understand that efficiency isn't the primary
>> reason
>> >> > for
>> >> > > use of a mixed effects model over a fixed effects model for most
>> >> > research.
>> >> > > A common reason to use a mixed effects model is that you have
>> >> hypotheses
>> >> > > about variables operating at higher levels of analysis or
>> cross-level
>> >> > > interactions and those questions cannot be answered by fixed
>> effects
>> >> > panel
>> >> > > models that have removed all between group variability from the
>> >> analysis.
>> >> > > You are sacrificing the ability to test group variant hypotheses by
>> >> > using a
>> >> > > basic fixed effects model over a mixed effects model. For nonlinear
>> >> > models
>> >> > > like a logistic regression it can also be very difficult to use an
>> >> > unbiased
>> >> > > fixed effects model (though there are ways in a panel setting e.g.
>> >> Hahn
>> >> > and
>> >> > > Newy 2004) and trivial to use a mixed effects model.
>> >> > >
>> >> > > Panel data econometricians almost always talk about typical
>> practice
>> >> > among
>> >> > > applied economists using fixed effects as flawed (see Baltagi 2013
>> ch.
>> >> > > 4.3). Mark Nerlov's 2000 History of Panel Data Econometrics is my
>> >> > favorite
>> >> > > example:
>> >> > >
>> >> > > The absurdity of the contention that possible correlation between
>> >> some of
>> >> > >> the observed explanatory variables and the individual-specific
>> >> > component of
>> >> > >> the disturbance is a ground for using fixed effects should be
>> clear
>> >> from
>> >> > >> the following example: Consider a panel of households with data on
>> >> > >> consumption and income. We are trying to estimate a consumption
>> >> > function.
>> >> > >> Income varies across households and over time. The variation
>> across
>> >> > >> households is related to ability of the main earner and other
>> >> household
>> >> > >> specific factors which vary little over time, that is to say,
>> reflect
>> >> > >> mainly differences in permanent income. Such permanent
>> differences in
>> >> > >> income are widely believed to be the source of most differences in
>> >> > >> consumption both crosssectionally and over time, whereas,
>> variations
>> >> of
>> >> > >> income over time are likely to be mostly transitory and unrelated
>> to
>> >> > >> consumption in most categories. Yet, fixed-effects regressions are
>> >> > >> equivalent to using only this variation and discarding the
>> >> information
>> >> > on
>> >> > >> the consumption-income relationship contained the cross-section
>> >> > variation
>> >> > >> among the household means.
>> >> > >
>> >> > >
>> >> > > See the last couple of pages of this lecture
>> >> > > <http://www.johndavidpoe.com/wp-content/uploads/2012/09/
>> >> > Blalock-Lecture.pdf> for
>> >> > > the citations in the econometrics and multilevel literature that I
>> >> > > referenced.
>> >> > >
>> >> > >
>> >> > >
>> >> > > On Tue, Oct 11, 2016 at 3:32 PM, Jake Westfall <
>> >> > jake.a.westfall at gmail.com
>> <javascript:_e(%7B%7D,'cvml','jake.a.westfall at gmail.com');>>
>> >> > > wrote:
>> >> > >
>> >> > >> Hi Laura and Ben,
>> >> > >>
>> >> > >> I like this paper on this topic:
>> >> > >> http://psych.colorado.edu/~westfaja/FixedvsRandom.pdf
>> >> > >>
>> >> > >> What it comes down to essentially is that if the cluster effects
>> are
>> >> > >> correlated with the "time-varying" (i.e., within-cluster varying)
>> X
>> >> > >> predictor -- so that, for example, some clusters have high means
>> on X
>> >> > and
>> >> > >> others have low means on X -- then there is the possibility that
>> the
>> >> > >> average within-cluster effect (which is what the fixed effect
>> model
>> >> > >> estimates) differs from the overall effect of X, not conditional
>> on
>> >> the
>> >> > >> clusters. An extreme example of this is Simpson's paradox. Now
>> since
>> >> the
>> >> > >> estimate from the random-effects model can be seen as a weighted
>> >> average
>> >> > >> of
>> >> > >> these two effects, it will generally be pulled to some extent away
>> >> from
>> >> > >> the
>> >> > >> fixed-effect estimate toward the unconditional estimate, which is
>> the
>> >> > bias
>> >> > >> that econometricians fret about. However, if the cluster effects
>> are
>> >> not
>> >> > >> correlated with X, so that each cluster has the same mean on X,
>> then
>> >> > this
>> >> > >> situation is not possible, so the random-effect model will give
>> the
>> >> same
>> >> > >> unbiased estimate as the fixed-effect model.
>> >> > >>
>> >> > >> A simple solution to this problem is to retain the random-effect
>> >> model,
>> >> > >> but
>> >> > >> to split the predictor X into two components, one representing the
>> >> > >> within-cluster variation of X and the other representing the
>> >> > >> between-cluster variation of X, and estimate separate slopes for
>> >> these
>> >> > two
>> >> > >> effects. One can even test whether these two slopes differ from
>> each
>> >> > >> other,
>> >> > >> which is conceptually similar to what the Hausman test does. As
>> >> > described
>> >> > >> in the paper linked above, the estimate of the within-cluster
>> >> component
>> >> > of
>> >> > >> the X effect equals the estimate one would obtain from a
>> fixed-effect
>> >> > >> model.
>> >> > >>
>> >> > >> As for the original question, I can't speak for common practice in
>> >> > >> ecology,
>> >> > >> but I suspect it may be like it is in my home field of psychology,
>> >> where
>> >> > >> we
>> >> > >> do worry about this issue (to some extent), but we discuss it
>> using
>> >> > >> completely different language. That is, we discuss it in terms of
>> >> > whether
>> >> > >> there are different effects of the predictor at the within-cluster
>> >> and
>> >> > >> between-cluster levels, and how our model might account for that.
>> >> > >>
>> >> > >> Jake
>> >> > >>
>> >> > >> On Tue, Oct 11, 2016 at 1:50 PM, Ben Bolker <bbolker at gmail.com
>> <javascript:_e(%7B%7D,'cvml','bbolker at gmail.com');>>
>> >> wrote:
>> >> > >>
>> >> > >> >
>> >> > >> >   I didn't respond to this offline, as it took me a while even
>> to
>> >> > start
>> >> > >> > to come up to speed on the question.  Random effects are indeed
>> >> > defined
>> >> > >> > from *very* different points of view in the two communities
>> >> > >> > ([bio]statistical vs. econometric); I'm sure there are points of
>> >> > >> > contact, but I've been having a hard time getting my head
>> around it
>> >> > all.
>> >> > >> >
>> >> > >> > Econometric definition:
>> >> > >> >
>> >> > >> > The wikipedia page <https://en.wikipedia.org/
>> >> > wiki/Random_effects_model>
>> >> > >> > and CrossValidated question
>> >> > >> > <http://stats.stackexchange.com/questions/66161/why-do-
>> >> > >> > random-effect-models-require-the-effects-to-be-uncorrelated-
>> >> > >> with-the-inpu>
>> >> > >> > were both helpful for me.
>> >> > >> >
>> >> > >> >  In the (bio)statistical world fixed and random effects are
>> usually
>> >> > >> > justified practically in terms of shrinkage estimators, or
>> >> > >> > philosophically in terms of random draws from an exchangeable
>> set
>> >> of
>> >> > >> > levels: e.g. see
>> >> > >> > <http://stats.stackexchange.com/questions/4700/what-is-
>> >> > >> > the-difference-between-fixed-effect-random-effect-and-mixed-
>> >> > >> effect-mode/>
>> >> > >> > for links.
>> >> > >> >
>> >> > >> >   I don't think I can really write an answer yet.  I'm still
>> >> trying to
>> >> > >> > understand at an intuitive or heuristic level what it means for
>> >> > >> > Cov(x_it,c_i)=0, where x_it is a set of explanatory variables
>> over
>> >> > time
>> >> > >> > for an individual subject and c_i is the conditional mode
>> (=BLUP in
>> >> > >> > linear mixed-model-land) for the deviation of the individual i
>> from
>> >> > the
>> >> > >> > population mean ... or more particularly what it means for that
>> >> > >> > condition to be violated, which is the point at which fixed
>> effects
>> >> > >> > would become preferred.
>> >> > >> >
>> >> > >> >   As a side note, some statisticians (Andrew Gelman is the one
>> who
>> >> > >> > springs to mind) have commented on the possible overemphasis on
>> >> bias.
>> >> > >> > (All else being equal unbiased estimators are preferred to
>> biased
>> >> > >> > estimators but all else is not always equal). Two examples: (1)
>> >> > >> > penalized estimators such as lasso/ridge regression (closely
>> >> related
>> >> > to
>> >> > >> > mixed models) give biased parameter estimates with lower mean
>> >> squared
>> >> > >> > error. (2) When estimating variability, one has to choose a
>> >> particular
>> >> > >> > scale (variance, standard error, log(standard error), etc.) on
>> >> which
>> >> > one
>> >> > >> > would prefer to get an unbiased answer.
>> >> > >> >
>> >> > >> > On 16-10-11 12:02 PM, Laura Dee wrote:
>> >> > >> > > Dear all,
>> >> > >> > > Random effects are more efficient estimators ? however they
>> come
>> >> at
>> >>
>> >> > >> the
>> >> > >> > > cost of the assumption that the random effect is not
>> correlated
>> >> with
>> >> > >> the
>> >> > >> > > included explanatory variables. Otherwise, using random
>> effects
>> >> > leads
>> >> > >> to
>> >> > >> > > biased estimates (e.g., as laid out in Woolridge
>> >> > >> > > <https://faculty.fuqua.duke.edu/~moorman/Wooldridge,%20FE%20
>> >> > >> and%20RE.pdf
>> >> > >> > >'s
>> >> > >> > > Econometrics text). This assumption is a strong one for many
>> >> > >> > > observational datasets, and most analyses in economics do not
>> use
>> >> > >> random
>> >> > >> > > effects for this reason. *Is there a reason why observational
>> >> > >> ecological
>> >> > >> > > datasets would be fundamentally different that I am missing?
>> Why
>> >> is
>> >> > >> this
>> >> > >> > > important assumption (to have unbiased estimates from random
>> >> > effects)
>> >> > >> > > not emphasized in ecology? *
>> >> > >> > >
>> >> > >> > > Thanks!
>> >> > >> > >
>> >> > >> > > Laura
>> >> > >> > >
>> >> > >> > > --
>> >> > >> > > Laura Dee
>> >> > >> > > Post-doctoral Associate
>> >> > >> > > University of Minnesota
>> >> > >> > > ledee at umn.edu <javascript:_e(%7B%7D,'cvml','ledee at umn.edu');>
>> <mailto:ledee at umn.edu <javascript:_e(%7B%7D,'cvml','ledee at umn.edu');>>
>> >> > >> > > lauraedee.com <http://lauraedee.com>
>> >> > >> >
>> >> > >> > _______________________________________________
>> >> > >> > R-sig-mixed-models at r-project.org
>> <javascript:_e(%7B%7D,'cvml','R-sig-mixed-models at r-project.org');>
>> mailing list
>> >> > >> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >> > >> >
>> >> > >>
>> >> > >>         [[alternative HTML version deleted]]
>> >> > >>
>> >> > >> _______________________________________________
>> >> > >> R-sig-mixed-models at r-project.org
>> <javascript:_e(%7B%7D,'cvml','R-sig-mixed-models at r-project.org');>
>> mailing list
>> >> > >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> >> > >>
>> >> > >
>> >> > >
>> >> > >
>> >> > > --
>> >> > >
>> >> > >
>> >> > >
>> >> > >
>> >> > > Thanks,
>> >> > > John
>> >> > >
>> >> > >
>> >> > > John Poe
>> >> > > Doctoral Candidate
>> >> > > Department of Political Science
>> >> > > Research Methodologist
>> >> > > UK Center for Public Health Services & Systems Research
>> >> > > University of Kentucky
>> >> > > 111 Washington Avenue, Room 203a
>> >> > > Lexington, KY 40536
>> >> > > www.johndavidpoe.com
>> >>
>> >
>>
>>
>> --
>> Laura Dee
>> Post-doctoral Associate
>> University of Minnesota
>> ledee at umn.edu <javascript:_e(%7B%7D,'cvml','ledee at umn.edu');>
>> lauraedee.com
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org
>> <javascript:_e(%7B%7D,'cvml','R-sig-mixed-models at r-project.org');>
>> mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>
>
> --
>
>
>
>
> Thanks,
> John
>
>
> John Poe
> Doctoral Candidate
> Department of Political Science
> Research Methodologist
> UK Center for Public Health Services & Systems Research
> University of Kentucky
> 111 Washington Avenue, Room 203a
> Lexington, KY 40536
> www.johndavidpoe.com
>


-- 
Laura Dee
Post-doctoral Associate
University of Minnesota
ledee at umn.edu
lauraedee.com

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Thu Oct 13 10:36:14 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Thu, 13 Oct 2016 10:36:14 +0200
Subject: [R-sig-ME]
	=?utf-8?b?562U5aSNOiBOb24tbm9ybWFsIHJhbmRvbSBlZmZlY3Qg?=
	=?utf-8?q?in_glmm?=
In-Reply-To: <DB6PR0101MB24237EB6677EA403AEC5C6DFACDD0@DB6PR0101MB2423.eurprd01.prod.exchangelabs.com>
References: <DB6PR0101MB24230DB767F3AC06A2080433ACDD0@DB6PR0101MB2423.eurprd01.prod.exchangelabs.com>
	<DB6PR0101MB24237EB6677EA403AEC5C6DFACDD0@DB6PR0101MB2423.eurprd01.prod.exchangelabs.com>
Message-ID: <CAJuCY5zu-1ern9A3Mu1aVB-trEWReF8L5ExyUQ7P33Bm7t0TQg@mail.gmail.com>

Dear Chun,

Have a look at the subjects with high random intercepts. They are likely
subjects with all positive outcomes. The high random intercepts are the
result of complete separation.

I don't bother with calculating the proportion of variance explained in
case of generalised linear models. This is something like R?: a nice and
simple property of _linear_ models due to the Gaussian distribution where
mean and variance are independent. But very hard with distribution where
mean and variance are linked.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-10-12 16:30 GMT+02:00 Chen Chun <talischen at hotmail.com>:

> Dear all,
>
>
> I am applying a mixed model with binomial distribution on a very large
> data set (around 400000 samples) with binary outcome (very few event,
> around 4%).  Some respondents but not all are repeated measured over the
> years, that's why a mixed model is applied. The model can be written as :
>
> mod <- glmer(response ~ AGE + SEX...+ YEAR + (1 | respondentID),
> family=binomial, data=dat)
>
> The distribution of the random effect (ID) from the model output shows an
> obvious non-normal distribution: a large proportion of close to zero values
> and very few large values around 10. I am wondering if in this case the
> glmm model is still valid?  if not valid, what kind of alternative model
> can I try? Can someone give some suggestion?
>
> A consequent problem is when I calculate the explained variance from the
> model:
> VarF <- var(as.vector(fixef(mod ) %*% t(mod @pp$X)))
> VarF/(VarF + VarCorr(mod )$respondentID[1] + (pi^2)/3)
>
> the variance of the fixed effect (VarF) from the model is only 1.6, while
> the variance of the random effect (VarCorr(mod )$respondentID[1]) is 149.
> Due to the non-normal distribution, the variance of the random effect is
> very large as compared to the fixed effect. Does this imply that the model
> performs bad? Or I should compute conditional R square?
>
> To summarize, my questions are:
>
> 1) What's the influence in estimation of the fixed effect and its
> explained variance (R squared) when the random effect does not follow a
> normal distribution? If the influence is large, any suggestions to solve it?
>
> 2) In a more general sense, how to comment a model where a large amount of
> variation comes from the random effects?
>
> Thanks
>
> Regards,
> Chun
>
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From shebrahimi_3622 at yahoo.com  Fri Oct 14 15:07:46 2016
From: shebrahimi_3622 at yahoo.com (shahla ebrahimi)
Date: Fri, 14 Oct 2016 13:07:46 +0000 (UTC)
Subject: [R-sig-ME] assumptions of discrete time survival analysis
References: <1341930886.208917.1476450466347.ref@mail.yahoo.com>
Message-ID: <1341930886.208917.1476450466347@mail.yahoo.com>

Dear?Professors

 I would greatly appreciate if you could let me know how to test the following assumptions when performing a discrete time survival analysis (using lme4).

 1- linear additivity assumption

 2- proportionality assumption

 3- no observed heterogeneity assumption (LR test)

Thanks in advance.
Best regards,
	[[alternative HTML version deleted]]


From tim.cole at ucl.ac.uk  Fri Oct 14 15:41:31 2016
From: tim.cole at ucl.ac.uk (Cole, Tim)
Date: Fri, 14 Oct 2016 13:41:31 +0000
Subject: [R-sig-ME] Exponent random effect in nlmer
In-Reply-To: <CAJuCY5z0m9JrEEjHeLtuW7Ta7nDtgFk0c+ZNmiXk_fxh5STeFg@mail.gmail.com>
References: <D42269FD.4ECAF%tim.cole@ucl.ac.uk>
	<CAJuCY5wmX6jYMvrtjf_M2LYWOt8gYXq0vSCqJzOTb7sCnHbUnw@mail.gmail.com>
	<D4227B1D.4ED19%tim.cole@ucl.ac.uk>
	<CAJuCY5z0m9JrEEjHeLtuW7Ta7nDtgFk0c+ZNmiXk_fxh5STeFg@mail.gmail.com>
Message-ID: <D4265F96.4EE74%tim.cole@ucl.ac.uk>

Dear Ben,

I have a model that can be written as:

nlmer(y ~ (g - 1) ^ exp(k | id) , data=data, weights=w)   (using a invalid but I hope obvious notation)

where
? -Inf < y < Inf
? 0 < E(y) < 1
? g is a grouping factor
? k is a subject random effect

k raises the group means to a subject-specific power while respecting the constraint on E(y).

I've coded it in nlmer, but it gives Error in initializePtr() : Downdated VtV is not positive definite

You have previously indicated that this can arise from badly rescaled parameters, so I'm wondering where I've gone wrong.

Here is a simple example with 3 groups.

> dim(data)
[1] 759   4
>
> head(data)
    id      y     g      w
1 1021 2.9968 01.02 0.2650
2 1022 0.8592 01.02 0.3931
3 1023 3.4657 01.02 0.2650
4 1024 0.3989 01.02 2.3648
5 1025 1.7230 01.02 0.2219
6 1026 0.7451 01.02 0.7046
>
> summary(moma <- with(data, model.matrix(~g - 1)))
     g01.02          g01.03          g02.03
 Min.   :0.000   Min.   :0.000   Min.   :0.000
 1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.000
 Median :0.000   Median :0.000   Median :0.000
 Mean   :0.333   Mean   :0.335   Mean   :0.332
 3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:1.000
 Max.   :1.000   Max.   :1.000   Max.   :1.000
>
> (start <- c(coef(lm(y ~g - 1, data=data, weights=w)), k=0))
g01.02 g01.03 g02.03      k
0.7979 0.6355 0.9056 0.0000
>
> data <- cbind(data, moma)
> nlmerfn <- function(g01.02,g01.03,g02.03, k) {
+     gc <- cbind(g01.02,g01.03,g02.03)
+     gc <- as.vector(as.matrix(gc * moma) %*% rep(1, ncol(moma)))
+     gc[gc <= 0] <- 1e-6
+     gk <- gc ^ exp(k)
+     grad <- moma * gk / gc * exp(k)
+     grad <- cbind(grad, k=gk * log(gk))
+     attr(gk, 'gradient') <- grad
+     gk
+ }
> nlmer1 <- nlmer(y ~ nlmerfn(g01.02,g01.03,g02.03, k) ~
+                     g01.02+g01.03+g02.03 + k + (k | id), data=data, weights=w, start=start)
Error in initializePtr() : Downdated VtV is not positive definite

Perhaps I need to include an intercept of some sort, but I'd very much value your thoughts.

Best wishes,
Tim
---
Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk> Phone +44(0)20 7905 2666 Fax +44(0)20 7905 2381
Population, Policy and Practice Programme
UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK


From: Thierry Onkelinx <thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>>
Date: Wednesday, 12 October 2016 09:26
To: Tim Cole <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk>>
Cc: "r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>" <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: Re: [R-sig-ME] Exponent random effect in nlmer

Hi Tim,

AFAIK nlmer requires the fixed and random effects to be additive. The model to be used _after_ this this summation can be non linear.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey

2016-10-11 12:30 GMT+02:00 Cole, Tim <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk>>:
Dear Thierry,

Thanks very much for your speedy response.

I agree my model looks odd, but it has a theoretical basis which I'd prefer not to spell out at this stage. Suffice to say that
? -Inf < y < Inf
? 0 < E(y) < 1
? there is a subject random effect.

For these reasons the usual models and/or transformations won't work, whereas my proposed exponent random effect ought to. I just need to fit it, to see if I'm right!

Best wishes,
Tim
---
Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk> Phone +44(0)20 7905 2666<tel:%2B44%280%2920%207905%202666> Fax +44(0)20 7905 2381<tel:%2B44%280%2920%207905%202381>
Population, Policy and Practice Programme
UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK


From: Thierry Onkelinx <thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>>
Date: Tuesday, 11 October 2016 11:06
To: Tim Cole <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk>>
Cc: "r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>" <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: Re: [R-sig-ME] Exponent random effect in nlmer

Dear Tim,

y centred on 0 and a valid range (0, 1) seems to be conflicting statements.

Here a some solutions depending on y

- y stems from a binomial process
     - use a binomial glmm.
- y is continuous and you are willing to transform y
    - 0 < y <  1
        - apply a logit transformation on y. lmer(plogis(y) ~ f + (1 | id) )
    - 0 <= y < 1
        - apply a log transformation on y. lmer(log(y) ~ f + (1 | id) )
    - 0 < y <= 1
        - apply a log transformation on 1 - y. lmer(log(1 - y) ~ f + (1 | id) )
- y is continuous are not willing to transform y
   - use a beta regression with 0 and/or 1 inflation in case you have 0 or 1 in the data. Have a look at the gamlss package to fit this model.

Best regards,


ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John Tukey

2016-10-11 11:29 GMT+02:00 Cole, Tim <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk>>:
I have a model of the form
  m1 <- lmer(y ~ f + (1 | id) )
where y is a continuous variable centred on zero, f is a unordered factor with coefficients b such 0 < b < 1, and there is a signficant random subject intercept.

The random intercept can lead to predicted values outside the valid range (0, 1). For this reason I'd like to reformulate the model as
m2 <- nlmer(y ~ (f - 1) ^ exp(1 | id) )   (using a invalid but I hope obvious notation), where the random effect is now a power centred on 1. This would constrain the fitted values to be within c(0, 1).

My question is: can this be done in nlmer, and if so how? Please can someone point me in the right direction?

Thanks,
Tim Cole
---
Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk><mailto:Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk>> Phone +44(0)20 7905 2666<tel:%2B44%280%2920%207905%202666> Fax +44(0)20 7905 2381<tel:%2B44%280%2920%207905%202381>
Population, Policy and Practice Programme
UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK


        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models



	[[alternative HTML version deleted]]


From liaoxiyue2011 at gmail.com  Wed Oct 12 19:48:46 2016
From: liaoxiyue2011 at gmail.com (Xiyue Liao)
Date: Wed, 12 Oct 2016 13:48:46 -0400
Subject: [R-sig-ME] question about an unbalanced design using lmer
Message-ID: <CAC7dBHSrMwC=W3w_dtpWU2Vivqn-8yQTDwpFEy4dZuLce081fw@mail.gmail.com>

Hi,

I'm using lmer in the R package lme4 to do a one-way anova analysis with a
fixed effect term and a random effect term. So the fixed effect is about
four medical conditions and the random effect is about randomly sampled
donors. Now for some combinations of donors and medical conditions, there
are more than one measurement, which makes the whole design unbalanced. I
think that lmer can handle such a case, and I have run the code without any
error message. However, I don't understand how this routine put weight on
the cells with more measurements than other cells. Could you give me some
hint?

Thanks in advance for your help.

Sincerely,

Xiyue

	[[alternative HTML version deleted]]


From jwmille7 at ncsu.edu  Fri Oct 14 20:08:37 2016
From: jwmille7 at ncsu.edu (Jonathan Miller)
Date: Fri, 14 Oct 2016 14:08:37 -0400
Subject: [R-sig-ME] bootstrapping random effects using confint() in lme4
Message-ID: <CAGomFPW7G+znG+gXAmGQ=GrZ3cVxO5Q75xdGE6FecjP+xMUZ4A@mail.gmail.com>

Dr. Bolker,

I am sorry for what I imagine is a pretty straightforward question.  My
name is Jonathan Miller and I am a Phd student at NCSU in Civil
Engineering.  I have tried to look through online resources for a post on
this, but have not been successful in determining what I need to do.

I have been using lme4 to run a logisitic mixed model and it has worked
very well.   Recently, though, I was interested in determining the
uncertainty in the random effects in my model.  It contains three levels of
random effects, estuaries(32), states(5) and programs(7).  I think the
function confint() is what I need to do, but I am having trouble getting
the outputs for the individual random effects(i.e. estuaries, states,
programs).

My code is below:

m3=glmer(Pres~ Temp_mean + Sal_mean + salsq + NEAR_DIST
         + (1|STATE) + (1 | UNIQUEID) + (1|program),
         data=bighead, na.action = na.exclude, nAGQ=0,
         family=binomial(link="logit"))

confint.merMod(m3,method="boot",nsim=100)

My output just gives me a summary of each fixed effect and each random
effect group.  I think there must be a relatively simple way to get the
output of the bootstrapping for each individual random effect.  It seems
the answer lies in FUN, but I haven't gotten it to work for me yet.


                                            2.5 %      97.5 %
sd_(Intercept)|UNIQUEID  4.720458e-01  0.98029041
sd_(Intercept)|program       3.168555e-01  1.26836628
sd_(Intercept)|STATE        3.019507e-07  1.30279935
(Intercept)                       -6.088106e+00 -3.81230673
Temp_mean                    -6.882682e-02 -0.05817473
Sal_mean                        1.696714e-01  0.20005360
salsq                               -2.253061e+00 -1.89913153
NEAR_DIST                    3.895820e-01  0.50612062


Any help would be greatly appreciated.

Thank you,

Jonathan Miller

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Fri Oct 14 21:13:13 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 14 Oct 2016 15:13:13 -0400
Subject: [R-sig-ME] bootstrapping random effects using confint() in lme4
In-Reply-To: <CAGomFPW7G+znG+gXAmGQ=GrZ3cVxO5Q75xdGE6FecjP+xMUZ4A@mail.gmail.com>
References: <CAGomFPW7G+znG+gXAmGQ=GrZ3cVxO5Q75xdGE6FecjP+xMUZ4A@mail.gmail.com>
Message-ID: <afb37dfb-3dec-d52d-8a1b-8b60d250c9e4@gmail.com>


  I answered this offline but wanted to encourage everyone, when in
doubt, to post to r-sig-mixed-models at r-project.org rather than e-mailing
me directly ...

  confint.merMod has a FUN argument that allows bootstrap CIs on an
arbitrary function (as long as it returns a numeric vector) to be
computed, e.g.

   confint(fitted_model,method="boot",FUN=function(m) unlist(ranef(m)))

(We should probably add this as an example to the documentation ...)

  One can also get an estimate of the uncertainty in the conditional
modes by extracting the "postVar" attributes from the elements of
ranef(fitted_model,condVar=TRUE) ... if fm1 is the fitted model,

> cvar <- lapply(ranef(fm1,condVar=TRUE),function(x) attr(x,"postVar"))
> apply(cvar[[1]],3,diag)



On 16-10-14 02:08 PM, Jonathan Miller wrote:
> Dr. Bolker,
> 
> I am sorry for what I imagine is a pretty straightforward question.  My
> name is Jonathan Miller and I am a Phd student at NCSU in Civil
> Engineering.  I have tried to look through online resources for a post on
> this, but have not been successful in determining what I need to do.
> 
> I have been using lme4 to run a logisitic mixed model and it has worked
> very well.   Recently, though, I was interested in determining the
> uncertainty in the random effects in my model.  It contains three levels of
> random effects, estuaries(32), states(5) and programs(7).  I think the
> function confint() is what I need to do, but I am having trouble getting
> the outputs for the individual random effects(i.e. estuaries, states,
> programs).
> 
> My code is below:
> 
> m3=glmer(Pres~ Temp_mean + Sal_mean + salsq + NEAR_DIST
>          + (1|STATE) + (1 | UNIQUEID) + (1|program),
>          data=bighead, na.action = na.exclude, nAGQ=0,
>          family=binomial(link="logit"))
> 
> confint.merMod(m3,method="boot",nsim=100)
> 
> My output just gives me a summary of each fixed effect and each random
> effect group.  I think there must be a relatively simple way to get the
> output of the bootstrapping for each individual random effect.  It seems
> the answer lies in FUN, but I haven't gotten it to work for me yet.
> 
> 
>                                             2.5 %      97.5 %
> sd_(Intercept)|UNIQUEID  4.720458e-01  0.98029041
> sd_(Intercept)|program       3.168555e-01  1.26836628
> sd_(Intercept)|STATE        3.019507e-07  1.30279935
> (Intercept)                       -6.088106e+00 -3.81230673
> Temp_mean                    -6.882682e-02 -0.05817473
> Sal_mean                        1.696714e-01  0.20005360
> salsq                               -2.253061e+00 -1.89913153
> NEAR_DIST                    3.895820e-01  0.50612062
> 
> 
> Any help would be greatly appreciated.
> 
> Thank you,
> 
> Jonathan Miller
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From S.Ellison at LGCGroup.com  Mon Oct 17 01:39:21 2016
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Mon, 17 Oct 2016 00:39:21 +0100
Subject: [R-sig-ME] lmertest - pairwise comparison p-value correction
In-Reply-To: <afb37dfb-3dec-d52d-8a1b-8b60d250c9e4@gmail.com>
References: <CAGomFPW7G+znG+gXAmGQ=GrZ3cVxO5Q75xdGE6FecjP+xMUZ4A@mail.gmail.com>,
	<afb37dfb-3dec-d52d-8a1b-8b60d250c9e4@gmail.com>
Message-ID: <1A8C1289955EF649A09086A153E2672403FE7A2EE3@GBTEDVPEXCMB04.corp.lgc-group.com>

A hopefully quick question: Can anyone point me to the p-value correction method used for multiple pairwise testing - if any - by difflsmeans() in the lmerTest package?

It's not obvious from the package documentation and the code I've scanned doesn't seem to include a correction.


S Ellison



*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From Phillip.Alday at unisa.edu.au  Mon Oct 17 04:42:11 2016
From: Phillip.Alday at unisa.edu.au (Phillip Alday)
Date: Mon, 17 Oct 2016 02:42:11 +0000
Subject: [R-sig-ME] lmertest - pairwise comparison p-value correction
In-Reply-To: <1A8C1289955EF649A09086A153E2672403FE7A2EE3@GBTEDVPEXCMB04.corp.lgc-group.com>
References: <CAGomFPW7G+znG+gXAmGQ=GrZ3cVxO5Q75xdGE6FecjP+xMUZ4A@mail.gmail.com>
	<afb37dfb-3dec-d52d-8a1b-8b60d250c9e4@gmail.com>
	<1A8C1289955EF649A09086A153E2672403FE7A2EE3@GBTEDVPEXCMB04.corp.lgc-group.com>
Message-ID: <41BB7744-4957-4D5B-BB82-522F3377134F@unisa.edu.au>

Without taking the time to look that up, you could also just use the lsmeans package, which has several different options for multiple-comparisons corrections and also uses the least-square means approach.

Best,
Phillip 


> On 17 Oct 2016, at 10:09, S Ellison <S.Ellison at lgcgroup.com> wrote:
> 
> A hopefully quick question: Can anyone point me to the p-value correction method used for multiple pairwise testing - if any - by difflsmeans() in the lmerTest package?
> 
> It's not obvious from the package documentation and the code I've scanned doesn't seem to include a correction.
> 
> 
> S Ellison
> 
> 
> 
> *******************************************************************
> This email and any attachments are confidential. Any u...{{dropped:6}}


From Phillip.Alday at unisa.edu.au  Mon Oct 17 06:58:12 2016
From: Phillip.Alday at unisa.edu.au (Phillip Alday)
Date: Mon, 17 Oct 2016 04:58:12 +0000
Subject: [R-sig-ME] lmertest - pairwise comparison p-value correction
In-Reply-To: <1A8C1289955EF649A09086A153E2672403FE7A2EE6@GBTEDVPEXCMB04.corp.lgc-group.com>
References: <CAGomFPW7G+znG+gXAmGQ=GrZ3cVxO5Q75xdGE6FecjP+xMUZ4A@mail.gmail.com>
	<afb37dfb-3dec-d52d-8a1b-8b60d250c9e4@gmail.com>
	<1A8C1289955EF649A09086A153E2672403FE7A2EE3@GBTEDVPEXCMB04.corp.lgc-group.com>
	<41BB7744-4957-4D5B-BB82-522F3377134F@unisa.edu.au>
	<1A8C1289955EF649A09086A153E2672403FE7A2EE6@GBTEDVPEXCMB04.corp.lgc-group.com>
Message-ID: <5C142478-A640-4415-B133-DE9CEF76942F@unisa.edu.au>

Ever lmerTest model is also a valid lme4 model (class merModLmerTest extends class lmerMod), so that shouldn't be a problem. You can also force the issue with something like:

lsmeans.command( as("lmerMod",lmertest.model))

Or going the other way:

summary(as("merModLmerTest",lme4.model),ddf="Satterthwaite")

Both examples assume that you have the necessary packages loaded.

Best,
Phillip 

PS: Keep the list in CC, please. I often forget to do this myself, so feel free to remind me when I forget ...


> On 17 Oct 2016, at 15:18, S Ellison <S.Ellison at lgcgroup.com> wrote:
> 
> Thanks for the suggestion, but I was also looking for the dreaded p-values for lmer models.  Hence lmerTest. That does not play nicely with lsmeans, so I was hoping to stick to one package.
> 
> But maybe not ...
> 
> Steve E
> 
> 
> ________________________________________
> From: Phillip Alday [Phillip.Alday at unisa.edu.au]
> Sent: 17 October 2016 03:42
> To: S Ellison
> Cc: r-sig-mixed-models at r-project.org
> Subject: Re: [R-sig-ME] lmertest - pairwise comparison p-value correction
> 
> Without taking the time to look that up, you could also just use the lsmeans package, which has several different options for multiple-comparisons corrections and also uses the least-square means approach.
> 
> Best,
> Phillip
> 
> 
>> On 17 Oct 2016, at 10:09, S Ellison <S.Ellison at lgcgroup.com> wrote:
>> 
>> A hopefully quick question: Can anyone point me to the p-value correction method used for multiple pairwise testing - if any - by difflsmeans() in the lmerTest package?
>> 
>> It's not obvious from the package documentation and the code I've scanned doesn't seem to include a correction.
>> 
>> 
>> S Ellison
>> 
>> 
>> 
>> *******************************************************************
>> This email and any attachments are confidential. Any use...{{dropped:8}}
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> 
> *******************************************************************
> This email and any attachments are confidential. Any u...{{dropped:8}}


From thierry.onkelinx at inbo.be  Mon Oct 17 11:09:45 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 17 Oct 2016 11:09:45 +0200
Subject: [R-sig-ME] question about an unbalanced design using lmer
In-Reply-To: <CAC7dBHSrMwC=W3w_dtpWU2Vivqn-8yQTDwpFEy4dZuLce081fw@mail.gmail.com>
References: <CAC7dBHSrMwC=W3w_dtpWU2Vivqn-8yQTDwpFEy4dZuLce081fw@mail.gmail.com>
Message-ID: <CAJuCY5wuJX=0if_eX5NRVriv0oTSBb=kDxzyE08Ub7pDnEEb-A@mail.gmail.com>

Dear Xiyue,

Don't think in terms of cells but in terms of observations. The model tries
to minimise the residuals. So combinations with more observations have more
residuals and thus a stronger impact on the MSE.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-10-12 19:48 GMT+02:00 Xiyue Liao <liaoxiyue2011 at gmail.com>:

> Hi,
>
> I'm using lmer in the R package lme4 to do a one-way anova analysis with a
> fixed effect term and a random effect term. So the fixed effect is about
> four medical conditions and the random effect is about randomly sampled
> donors. Now for some combinations of donors and medical conditions, there
> are more than one measurement, which makes the whole design unbalanced. I
> think that lmer can handle such a case, and I have run the code without any
> error message. However, I don't understand how this routine put weight on
> the cells with more measurements than other cells. Could you give me some
> hint?
>
> Thanks in advance for your help.
>
> Sincerely,
>
> Xiyue
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From singmann at psychologie.uzh.ch  Mon Oct 17 12:17:31 2016
From: singmann at psychologie.uzh.ch (Henrik Singmann)
Date: Mon, 17 Oct 2016 12:17:31 +0200
Subject: [R-sig-ME] question about an unbalanced design using lmer
In-Reply-To: <CAJuCY5wuJX=0if_eX5NRVriv0oTSBb=kDxzyE08Ub7pDnEEb-A@mail.gmail.com>
References: <CAC7dBHSrMwC=W3w_dtpWU2Vivqn-8yQTDwpFEy4dZuLce081fw@mail.gmail.com>
	<CAJuCY5wuJX=0if_eX5NRVriv0oTSBb=kDxzyE08Ub7pDnEEb-A@mail.gmail.com>
Message-ID: <947e3012-00fb-e08e-c7fb-4be5faba7a29@psychologie.uzh.ch>

Dear Xiyue and Thiery,

While more data points may affect the estimation process in that way 
they do not seem to affect the fixed-effects estimates in that way. To 
be more precise, the fixed effect estimate seems to correspond to the 
unweighted mean (i.e., the mean in which each level of the random effect 
is weighted equally) and not to the weighted mean (in which each data 
point is weighted equally).

I had a similar problem some time ago:
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q3/022478.html

Thanks to the help of Jake Westfall I was able to get the desired result 
(i.e., a fixed-effect estimate corresponding to the weighted mean), by 
adding group size as fixed effect to my model, see:
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q3/022481.html

There might be other approaches to achieve this as well (i.e., some 
post-fit weighting), but I am not sure how to implement this (perhaps 
using lsmeans somehow).

I hope this helps,
Henrik


Am 17.10.2016 um 11:09 schrieb Thierry Onkelinx:
> Dear Xiyue,
>
> Don't think in terms of cells but in terms of observations. The model tries
> to minimise the residuals. So combinations with more observations have more
> residuals and thus a stronger impact on the MSE.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-10-12 19:48 GMT+02:00 Xiyue Liao <liaoxiyue2011 at gmail.com>:
>
>> Hi,
>>
>> I'm using lmer in the R package lme4 to do a one-way anova analysis with a
>> fixed effect term and a random effect term. So the fixed effect is about
>> four medical conditions and the random effect is about randomly sampled
>> donors. Now for some combinations of donors and medical conditions, there
>> are more than one measurement, which makes the whole design unbalanced. I
>> think that lmer can handle such a case, and I have run the code without any
>> error message. However, I don't understand how this routine put weight on
>> the cells with more measurements than other cells. Could you give me some
>> hint?
>>
>> Thanks in advance for your help.
>>
>> Sincerely,
>>
>> Xiyue
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
> 	[[alternative HTML version deleted]]
>


From singmann at psychologie.uzh.ch  Mon Oct 17 12:18:08 2016
From: singmann at psychologie.uzh.ch (Henrik Singmann)
Date: Mon, 17 Oct 2016 12:18:08 +0200
Subject: [R-sig-ME] question about an unbalanced design using lmer
In-Reply-To: <CAJuCY5wuJX=0if_eX5NRVriv0oTSBb=kDxzyE08Ub7pDnEEb-A@mail.gmail.com>
References: <CAC7dBHSrMwC=W3w_dtpWU2Vivqn-8yQTDwpFEy4dZuLce081fw@mail.gmail.com>
	<CAJuCY5wuJX=0if_eX5NRVriv0oTSBb=kDxzyE08Ub7pDnEEb-A@mail.gmail.com>
Message-ID: <21bb32e3-fde6-97dc-a106-641524d19218@psychologie.uzh.ch>

Dear Xiyue and Thiery,

While more data points may affect the estimation process in that way 
they do not seem to affect the fixed-effects estimates in that way. To 
be more precise, the fixed effect estimate seems to correspond to the 
unweighted mean (i.e., the mean in which each level of the random effect 
is weighted equally) and not to the weighted mean (in which each data 
point is weighted equally).

I had a similar problem some time ago:
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q3/022478.html

Thanks to the help of Jake Westfall I was able to get the desired result 
(i.e., a fixed-effect estimate corresponding to the weighted mean), by 
adding group size as fixed effect to my model, see:
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q3/022481.html

There might be other approaches to achieve this as well (i.e., some 
post-fit weighting), but I am not sure how to implement this (perhaps 
using lsmeans somehow).

I hope this helps,
Henrik


Am 17.10.2016 um 11:09 schrieb Thierry Onkelinx:
> Dear Xiyue,
>
> Don't think in terms of cells but in terms of observations. The model tries
> to minimise the residuals. So combinations with more observations have more
> residuals and thus a stronger impact on the MSE.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-10-12 19:48 GMT+02:00 Xiyue Liao <liaoxiyue2011 at gmail.com>:
>
>> Hi,
>>
>> I'm using lmer in the R package lme4 to do a one-way anova analysis with a
>> fixed effect term and a random effect term. So the fixed effect is about
>> four medical conditions and the random effect is about randomly sampled
>> donors. Now for some combinations of donors and medical conditions, there
>> are more than one measurement, which makes the whole design unbalanced. I
>> think that lmer can handle such a case, and I have run the code without any
>> error message. However, I don't understand how this routine put weight on
>> the cells with more measurements than other cells. Could you give me some
>> hint?
>>
>> Thanks in advance for your help.
>>
>> Sincerely,
>>
>> Xiyue
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
> 	[[alternative HTML version deleted]]
>


From singmann at psychologie.uzh.ch  Mon Oct 17 12:18:18 2016
From: singmann at psychologie.uzh.ch (Henrik Singmann)
Date: Mon, 17 Oct 2016 12:18:18 +0200
Subject: [R-sig-ME] question about an unbalanced design using lmer
In-Reply-To: <CAJuCY5wuJX=0if_eX5NRVriv0oTSBb=kDxzyE08Ub7pDnEEb-A@mail.gmail.com>
References: <CAC7dBHSrMwC=W3w_dtpWU2Vivqn-8yQTDwpFEy4dZuLce081fw@mail.gmail.com>
	<CAJuCY5wuJX=0if_eX5NRVriv0oTSBb=kDxzyE08Ub7pDnEEb-A@mail.gmail.com>
Message-ID: <29927682-8976-babf-f86b-a39c59377080@psychologie.uzh.ch>

Dear Xiyue and Thiery,

While more data points may affect the estimation process in that way 
they do not seem to affect the fixed-effects estimates in that way. To 
be more precise, the fixed effect estimate seems to correspond to the 
unweighted mean (i.e., the mean in which each level of the random effect 
is weighted equally) and not to the weighted mean (in which each data 
point is weighted equally).

I had a similar problem some time ago:
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q3/022478.html

Thanks to the help of Jake Westfall I was able to get the desired result 
(i.e., a fixed-effect estimate corresponding to the weighted mean), by 
adding group size as fixed effect to my model, see:
https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q3/022481.html

There might be other approaches to achieve this as well (i.e., some 
post-fit weighting), but I am not sure how to implement this (perhaps 
using lsmeans somehow).

I hope this helps,
Henrik


Am 17.10.2016 um 11:09 schrieb Thierry Onkelinx:
> Dear Xiyue,
>
> Don't think in terms of cells but in terms of observations. The model tries
> to minimise the residuals. So combinations with more observations have more
> residuals and thus a stronger impact on the MSE.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-10-12 19:48 GMT+02:00 Xiyue Liao <liaoxiyue2011 at gmail.com>:
>
>> Hi,
>>
>> I'm using lmer in the R package lme4 to do a one-way anova analysis with a
>> fixed effect term and a random effect term. So the fixed effect is about
>> four medical conditions and the random effect is about randomly sampled
>> donors. Now for some combinations of donors and medical conditions, there
>> are more than one measurement, which makes the whole design unbalanced. I
>> think that lmer can handle such a case, and I have run the code without any
>> error message. However, I don't understand how this routine put weight on
>> the cells with more measurements than other cells. Could you give me some
>> hint?
>>
>> Thanks in advance for your help.
>>
>> Sincerely,
>>
>> Xiyue
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
> 	[[alternative HTML version deleted]]
>


From singmann at psychologie.uzh.ch  Mon Oct 17 14:28:32 2016
From: singmann at psychologie.uzh.ch (Henrik Singmann)
Date: Mon, 17 Oct 2016 14:28:32 +0200
Subject: [R-sig-ME] question about an unbalanced design using lmer
In-Reply-To: <947e3012-00fb-e08e-c7fb-4be5faba7a29@psychologie.uzh.ch>
References: <CAC7dBHSrMwC=W3w_dtpWU2Vivqn-8yQTDwpFEy4dZuLce081fw@mail.gmail.com>
	<CAJuCY5wuJX=0if_eX5NRVriv0oTSBb=kDxzyE08Ub7pDnEEb-A@mail.gmail.com>
	<947e3012-00fb-e08e-c7fb-4be5faba7a29@psychologie.uzh.ch>
Message-ID: <nu2g5e$7s0$1@blaine.gmane.org>

PS: Sorry for sending this out three times. My mail client has some 
problems with gmane lately...


Am 17.10.2016 um 12:17 schrieb Henrik Singmann:
> Dear Xiyue and Thiery,
>
> While more data points may affect the estimation process in that way
> they do not seem to affect the fixed-effects estimates in that way. To
> be more precise, the fixed effect estimate seems to correspond to the
> unweighted mean (i.e., the mean in which each level of the random effect
> is weighted equally) and not to the weighted mean (in which each data
> point is weighted equally).
>
> I had a similar problem some time ago:
> https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q3/022478.html
>
> Thanks to the help of Jake Westfall I was able to get the desired result
> (i.e., a fixed-effect estimate corresponding to the weighted mean), by
> adding group size as fixed effect to my model, see:
> https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q3/022481.html
>
> There might be other approaches to achieve this as well (i.e., some
> post-fit weighting), but I am not sure how to implement this (perhaps
> using lsmeans somehow).
>
> I hope this helps,
> Henrik
>
>
> Am 17.10.2016 um 11:09 schrieb Thierry Onkelinx:
>> Dear Xiyue,
>>
>> Don't think in terms of cells but in terms of observations. The model
>> tries
>> to minimise the residuals. So combinations with more observations have
>> more
>> residuals and thus a stronger impact on the MSE.
>>
>> Best regards,
>>
>> ir. Thierry Onkelinx
>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>> and
>> Forest
>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>> Kliniekstraat 25
>> 1070 Anderlecht
>> Belgium
>>
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able
>> to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of
>> data.
>> ~ John Tukey
>>
>> 2016-10-12 19:48 GMT+02:00 Xiyue Liao
>> <liaoxiyue2011 at gmail.com>:
>>
>>> Hi,
>>>
>>> I'm using lmer in the R package lme4 to do a one-way anova analysis
>>> with a
>>> fixed effect term and a random effect term. So the fixed effect is about
>>> four medical conditions and the random effect is about randomly sampled
>>> donors. Now for some combinations of donors and medical conditions,
>>> there
>>> are more than one measurement, which makes the whole design
>>> unbalanced. I
>>> think that lmer can handle such a case, and I have run the code
>>> without any
>>> error message. However, I don't understand how this routine put
>>> weight on
>>> the cells with more measurements than other cells. Could you give me
>>> some
>>> hint?
>>>
>>> Thanks in advance for your help.
>>>
>>> Sincerely,
>>>
>>> Xiyue
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>>     [[alternative HTML version deleted]]
>>
>


From jbischof1 at gmail.com  Wed Oct 19 03:24:35 2016
From: jbischof1 at gmail.com (Jon Bischof)
Date: Wed, 19 Oct 2016 01:24:35 +0000
Subject: [R-sig-ME] MCMCglmm multivariate meta-analysis with covariance
Message-ID: <CAPB59fcoHeL2OpKs1PKocDMukoQukwzGo+pYryRMDbRJNHpzCg@mail.gmail.com>

I'm interested in fitting a multivariate meta-analysis model with
correlated measurement error. This means fixing the error to a covariance
matrix per row.

I saw this post
<https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q2/020180.html> on
the mailing list about non-correlated outcomes, but the noise correlation
is too large to ignore in my use case. Professor Hadfield implies in the
post that it is possible but "complicated". Does anyone know how to do it?

Thanks!
Jon Bischof

	[[alternative HTML version deleted]]


From jbischof.stat at gmail.com  Wed Oct 19 06:41:13 2016
From: jbischof.stat at gmail.com (Jon Bischof)
Date: Tue, 18 Oct 2016 21:41:13 -0700
Subject: [R-sig-ME] MCMCglmm multivariate meta-analysis with covariance
Message-ID: <CAL+h=zYwZAcv5kPbixqpf7FXmz6tAcs3TM57kZOD3D-Qh4A1Jw@mail.gmail.com>

I'm interested in fitting a multivariate meta-analysis model with
correlated measurement error. This means fixing the error to a covariance
matrix per row.

I saw this post
<https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q2/020180.html> on
the mailing list about non-correlated outcomes, but the noise correlation
is too large to ignore in my use case. Professor Hadfield implies in the
post that it is possible but "complicated". Does anyone know how to do it?

Thanks!
Jon Bischof

	[[alternative HTML version deleted]]


From j.hadfield at ed.ac.uk  Wed Oct 19 08:45:40 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Wed, 19 Oct 2016 07:45:40 +0100
Subject: [R-sig-ME] MCMCglmm multivariate meta-analysis with covariance
In-Reply-To: <CAL+h=zYwZAcv5kPbixqpf7FXmz6tAcs3TM57kZOD3D-Qh4A1Jw@mail.gmail.com>
References: <CAL+h=zYwZAcv5kPbixqpf7FXmz6tAcs3TM57kZOD3D-Qh4A1Jw@mail.gmail.com>
Message-ID: <a9adf50e-73af-8d1d-26ac-f2e5ba4a8a50@ed.ac.uk>

Hi Jon,

If you have the covariance matrix for your observations, then take its 
inverse and store it in sparse format:

Cinv_sparse<-as(Cinv, "dgCMatrix")

where Cinv is the inverse in dense format. When you say multivariate do 
you mean something like an explicit bivariate response such that the 
fixed formula is of the form cbind(y_1, y_2)~...?  If so you need to 
organise your data in long format and pass a single response vector. You 
can include a variable that denotes whether the observation is y_1 or 
y_2 and use it like "trait", and include a variable that denotes the 
original row for the observation and use it like "units". If we call 
this second variable "row" then having fit "row" as a random effect, and 
pass the argument ginverse=list(row=Cinv_sparse) to MCMCglmm. You will 
also need to fix the "row" variance to one in the prior:

G1=list(V=1, fix=1)

Presumably covariances are only non-zero between observations from the 
same original row? If so make sure the sparse Matrix also represents 
this: numerical issues during inversion may cause zero entries to differ 
slightly from zero and hence not be represented as zero.

Cheers,

Jarrod




Then you can fit the term ~trait:units





On 19/10/2016 05:41, Jon Bischof wrote:
> I'm interested in fitting a multivariate meta-analysis model with
> correlated measurement error. This means fixing the error to a covariance
> matrix per row.
>
> I saw this post
> <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q2/020180.html> on
> the mailing list about non-correlated outcomes, but the noise correlation
> is too large to ignore in my use case. Professor Hadfield implies in the
> post that it is possible but "complicated". Does anyone know how to do it?
>
> Thanks!
> Jon Bischof
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From wolfgang.viechtbauer at maastrichtuniversity.nl  Wed Oct 19 09:50:43 2016
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (SP))
Date: Wed, 19 Oct 2016 07:50:43 +0000
Subject: [R-sig-ME] MCMCglmm multivariate meta-analysis with covariance
In-Reply-To: <CAPB59fcoHeL2OpKs1PKocDMukoQukwzGo+pYryRMDbRJNHpzCg@mail.gmail.com>
References: <CAPB59fcoHeL2OpKs1PKocDMukoQukwzGo+pYryRMDbRJNHpzCg@mail.gmail.com>
Message-ID: <c31c6d3235f6483da8c9189fbfcf5fe2@UM-MAIL3216.unimaas.nl>

For ML/REML estimation, you can also use metafor, mvmeta, and metaSEM.

An illustration of a multivariate meta-analysis with the metafor package can be found here:

http://www.metafor-project.org/doku.php/analyses:berkey1998

Best,
Wolfgang

-- 
Wolfgang Viechtbauer, Ph.D., Statistician | Department of Psychiatry and    
Neuropsychology | Maastricht University | P.O. Box 616 (VIJV1) | 6200 MD    
Maastricht, The Netherlands | +31 (43) 388-4170 | http://www.wvbauer.com    

> -----Original Message-----
> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-
> project.org] On Behalf Of Jon Bischof
> Sent: Wednesday, October 19, 2016 03:25
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] MCMCglmm multivariate meta-analysis with covariance
> 
> I'm interested in fitting a multivariate meta-analysis model with
> correlated measurement error. This means fixing the error to a covariance
> matrix per row.
> 
> I saw this post
> <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q2/020180.html> on
> the mailing list about non-correlated outcomes, but the noise correlation
> is too large to ignore in my use case. Professor Hadfield implies in the
> post that it is possible but "complicated". Does anyone know how to do
> it?
> 
> Thanks!
> Jon Bischof


From kamokoi55 at gmail.com  Wed Oct 19 21:46:13 2016
From: kamokoi55 at gmail.com (Mark Albins)
Date: Wed, 19 Oct 2016 14:46:13 -0500
Subject: [R-sig-ME] Error in glmmADMB: no PSV file found -- link to
 known fix is broken
Message-ID: <CA+DkWKJ7ELTY9Wc336B-U+V5hRnOaENGFPsVYk_+KSdw1m4MWQ@mail.gmail.com>

Thanks for providing the new URL for the buildbot page!

It looks like all of the Windows binaries are for Windows 10.

Is there anywhere where I can find a binary for my ancient Windows 7 OS?

Thanks for the help!

Best,

Mark Albins

---------- Forwarded message ----------
From: Ben Bolker <bbolker at gmail.com>
To: "r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org>
Cc:
Date: Thu, 29 Sep 2016 15:59:27 -0400
Subject: Re: [R-sig-ME] Error in glmmADMB: no PSV file found -- link to
known fix is broken


The maintainer has let me know the new URL is
http://buildbot.admb-project.org/glmmadmb/ . I'll update the
documentation/web page

    -------- Forwarded Message --------
    Subject: [R-sig-ME] Error in glmmADMB: no PSV file found -- link to
    known fix is broken
    Date: Thu, 29 Sep 2016 12:36:42 -0500
    From: Mark Albins <kamokoi55 at gmail.com <mailto:kamokoi55 at gmail.com>>
    To: r-sig-mixed-models at r-project.org
    <mailto:r-sig-mixed-models at r-project.org>

    Hi r-sig-ME-ers,

    I'm running a glmmADMB model with a single random effect, zero
    inflation,
    and a negative binomial distribution (session info below).  The
    model seems
    to run fine on its own (i.e. no error message, sensible outcome,
    etc.), but
    throws a "no PSV file found" error when I try to run it with mcmc =
    TRUE.

    I found the following solution to a similar problem posted here on the
    r-sig-ME list:

    https://stat.ethz.ch/pipermail/r-sig-mixed-models/2016q1/024490.html
    <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2016q1/024490.html>

    Unfortunately, when I tried to follow the instructions at:

    http://glmmadmb.r-forge.r-project.org/
    <http://glmmadmb.r-forge.r-project.org/>

    ..I found the link to the "Buildbot page" was broken...

    http://www.admb-project.org/buildbot/glmmadmb/
    <http://www.admb-project.org/buildbot/glmmadmb/>

    A google search for "glmmADMB buildbot page" did not uncover any obvious
    alternate locations for the various OS compatible binaries.

    I don't know how to build the binary from its TPL on my system.

    Any suggestions?

    Thanks!

    Mark Albins

    R version 3.3.1 (2016-06-21)
    Platform: x86_64-w64-mingw32/x64 (64-bit)
    Running under: Windows 7 x64 (build 7601) Service Pack 1

    locale:
    [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United
    States.1252
    [3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C
    [5] LC_TIME=English_United States.1252

    attached base packages:
    [1] stats     graphics  grDevices utils     datasets  methods   base

    other attached packages:
    [1] glmmADMB_0.8.3.3 MASS_7.3-45      ggplot2_2.1.0    lattice_0.20-33
    [5] nlme_3.1-128

    loaded via a namespace (and not attached):
     [1] Rcpp_0.12.6      digest_0.6.10    grid_3.3.1       plyr_1.8.4
     [5] gtable_0.2.0     magrittr_1.5     coda_0.18-1      scales_0.4.0
     [9] stringi_1.1.1    Matrix_1.2-6     labeling_0.3     tools_3.3.1
    [13] stringr_1.1.0    munsell_0.4.3    colorspace_1.2-6 R2admb_0.7.13

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Thu Oct 20 00:32:32 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 19 Oct 2016 18:32:32 -0400
Subject: [R-sig-ME] Error in glmmADMB: no PSV file found -- link to
 known fix is broken
In-Reply-To: <CA+DkWKJ7ELTY9Wc336B-U+V5hRnOaENGFPsVYk_+KSdw1m4MWQ@mail.gmail.com>
References: <CA+DkWKJ7ELTY9Wc336B-U+V5hRnOaENGFPsVYk_+KSdw1m4MWQ@mail.gmail.com>
Message-ID: <f7c5b14f-3875-97f2-5ae6-df2b1eeed08a@gmail.com>


   It's a bit more work, but your best bet is probably to try to set up
ADMB on your own machine.  Hopefully you can install the ADMB IDE from
here http://www.admb-project.org/downloads/#binaries ; if you have
trouble, we might refer you the ADMB mailing list.

   I would suggest trying glmmTMB (which is more self-contained)
instead, but at present it doesn't have the MCMC option ...  I might try
to hack up a demo though ...  (I just did, see

https://rawgit.com/glmmTMB/glmmTMB/master/glmmTMB/vignettes/mcmc.html

and the corresponding rmd file in the vignettes directory of the
repository ...)

  cheers
    Ben Bolker

On 16-10-19 03:46 PM, Mark Albins wrote:
> Thanks for providing the new URL for the buildbot page!
> 
> It looks like all of the Windows binaries are for Windows 10.
> 
> Is there anywhere where I can find a binary for my ancient Windows 7 OS?
> 
> Thanks for the help!
> 
> Best,
> 
> Mark Albins
> 
> ---------- Forwarded message ----------
> From: Ben Bolker <bbolker at gmail.com>
> To: "r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org>
> Cc:
> Date: Thu, 29 Sep 2016 15:59:27 -0400
> Subject: Re: [R-sig-ME] Error in glmmADMB: no PSV file found -- link to
> known fix is broken
> 
> 
> The maintainer has let me know the new URL is
> http://buildbot.admb-project.org/glmmadmb/ . I'll update the
> documentation/web page
> 
>     -------- Forwarded Message --------
>     Subject: [R-sig-ME] Error in glmmADMB: no PSV file found -- link to
>     known fix is broken
>     Date: Thu, 29 Sep 2016 12:36:42 -0500
>     From: Mark Albins <kamokoi55 at gmail.com <mailto:kamokoi55 at gmail.com>>
>     To: r-sig-mixed-models at r-project.org
>     <mailto:r-sig-mixed-models at r-project.org>
> 
>     Hi r-sig-ME-ers,
> 
>     I'm running a glmmADMB model with a single random effect, zero
>     inflation,
>     and a negative binomial distribution (session info below).  The
>     model seems
>     to run fine on its own (i.e. no error message, sensible outcome,
>     etc.), but
>     throws a "no PSV file found" error when I try to run it with mcmc =
>     TRUE.
> 
>     I found the following solution to a similar problem posted here on the
>     r-sig-ME list:
> 
>     https://stat.ethz.ch/pipermail/r-sig-mixed-models/2016q1/024490.html
>     <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2016q1/024490.html>
> 
>     Unfortunately, when I tried to follow the instructions at:
> 
>     http://glmmadmb.r-forge.r-project.org/
>     <http://glmmadmb.r-forge.r-project.org/>
> 
>     ..I found the link to the "Buildbot page" was broken...
> 
>     http://www.admb-project.org/buildbot/glmmadmb/
>     <http://www.admb-project.org/buildbot/glmmadmb/>
> 
>     A google search for "glmmADMB buildbot page" did not uncover any obvious
>     alternate locations for the various OS compatible binaries.
> 
>     I don't know how to build the binary from its TPL on my system.
> 
>     Any suggestions?
> 
>     Thanks!
> 
>     Mark Albins
> 
>     R version 3.3.1 (2016-06-21)
>     Platform: x86_64-w64-mingw32/x64 (64-bit)
>     Running under: Windows 7 x64 (build 7601) Service Pack 1
> 
>     locale:
>     [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United
>     States.1252
>     [3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C
>     [5] LC_TIME=English_United States.1252
> 
>     attached base packages:
>     [1] stats     graphics  grDevices utils     datasets  methods   base
> 
>     other attached packages:
>     [1] glmmADMB_0.8.3.3 MASS_7.3-45      ggplot2_2.1.0    lattice_0.20-33
>     [5] nlme_3.1-128
> 
>     loaded via a namespace (and not attached):
>      [1] Rcpp_0.12.6      digest_0.6.10    grid_3.3.1       plyr_1.8.4
>      [5] gtable_0.2.0     magrittr_1.5     coda_0.18-1      scales_0.4.0
>      [9] stringi_1.1.1    Matrix_1.2-6     labeling_0.3     tools_3.3.1
>     [13] stringr_1.1.0    munsell_0.4.3    colorspace_1.2-6 R2admb_0.7.13
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From NDeCesare at mt.gov  Thu Oct 20 13:25:18 2016
From: NDeCesare at mt.gov (DeCesare, Nicholas)
Date: Thu, 20 Oct 2016 11:25:18 +0000
Subject: [R-sig-ME] FW: [Lme4-authors] sandwich estimators in lme4?
Message-ID: <22fb37b507534cbf8f5b84a8a9b05deb@doaisd5273.state.mt.ads>

Hi All,
I?m wondering if you would ever consider incorporating Huber-White sandwhich estimators as an option for glmer in lme4.  Some google searching has revealed various posts about this, as well as the recent ?robustlmm? package (Gaussian data only).

My own interest stems from this being a recommended approach in the ecology literature.  A common application of glmms in ecology is the use of logistic regression to model animal selection of resources.  A random intercept per animal is often used to account for correlation of data within each animal, but in the response data the 1?s (autocorrelated animal locations) and 0?s (random locations in a polygon) can have different correlation structures.  Koper and Manseau (2009 - attached) recommended using a glmm with a random intercept per animal in combination with a Huber White sandwhich estimator to get proper variance estimates.    I can?t speak to the statistical validity of the approach, but it has some appeal for avoiding underestimates of variance of coefficients.

Thanks for all of your time and efforts ?Nick

------------------------------------
Nick DeCesare
Research Wildlife Biologist | Montana Fish, Wildlife & Parks
406-542-5558 (office) | 406-370-3403 (cell)
ndecesare at mt.gov

From jbischof.stat at gmail.com  Thu Oct 20 20:09:23 2016
From: jbischof.stat at gmail.com (Jon Bischof)
Date: Thu, 20 Oct 2016 11:09:23 -0700
Subject: [R-sig-ME] MCMCglmm multivariate meta-analysis with covariance
In-Reply-To: <a9adf50e-73af-8d1d-26ac-f2e5ba4a8a50@ed.ac.uk>
References: <CAL+h=zYwZAcv5kPbixqpf7FXmz6tAcs3TM57kZOD3D-Qh4A1Jw@mail.gmail.com>
	<a9adf50e-73af-8d1d-26ac-f2e5ba4a8a50@ed.ac.uk>
Message-ID: <CAL+h=zbBCW33S4UbSE=QC=B0OfhHEiodNmsZZyp9nhudgrXS2Q@mail.gmail.com>

Jarrod,

Thanks for your detailed response! Your understanding of my model is
correct: it's just a single grouping metaanalysis in two dimensions:

(y_i,1, y_i,2) ~ Normal ( (theta_i,1, theta_i,2), V_i )
(theta_i,1, theta_i,2) ~ Normal ( (mu_1, mu_2), S )

where V_i is a known covariance matrix of measurement error.

As a new user of mcmcglmm I will need some time to experiment with your
idea to confirm that it works. My understanding, however, was that residual
error was specified in the R matrix and not the G matrix. Do we need to fix
R as well? Will we still be able to estimate S?

Thanks!
Jon

On Tue, Oct 18, 2016 at 11:45 PM, Jarrod Hadfield <j.hadfield at ed.ac.uk>
wrote:

> Hi Jon,
>
> If you have the covariance matrix for your observations, then take its
> inverse and store it in sparse format:
>
> Cinv_sparse<-as(Cinv, "dgCMatrix")
>
> where Cinv is the inverse in dense format. When you say multivariate do
> you mean something like an explicit bivariate response such that the fixed
> formula is of the form cbind(y_1, y_2)~...?  If so you need to organise
> your data in long format and pass a single response vector. You can include
> a variable that denotes whether the observation is y_1 or y_2 and use it
> like "trait", and include a variable that denotes the original row for the
> observation and use it like "units". If we call this second variable "row"
> then having fit "row" as a random effect, and pass the argument
> ginverse=list(row=Cinv_sparse) to MCMCglmm. You will also need to fix the
> "row" variance to one in the prior:
>
> G1=list(V=1, fix=1)
>
> Presumably covariances are only non-zero between observations from the
> same original row? If so make sure the sparse Matrix also represents this:
> numerical issues during inversion may cause zero entries to differ slightly
> from zero and hence not be represented as zero.
>
> Cheers,
>
> Jarrod
>
>
>
>
> Then you can fit the term ~trait:units
>
>
>
>
>
> On 19/10/2016 05:41, Jon Bischof wrote:
>
>> I'm interested in fitting a multivariate meta-analysis model with
>> correlated measurement error. This means fixing the error to a covariance
>> matrix per row.
>>
>> I saw this post
>> <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q2/020180.html> on
>> the mailing list about non-correlated outcomes, but the noise correlation
>> is too large to ignore in my use case. Professor Hadfield implies in the
>> post that it is possible but "complicated". Does anyone know how to do it?
>>
>> Thanks!
>> Jon Bischof
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>
> --
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
>

	[[alternative HTML version deleted]]


From j.hadfield at ed.ac.uk  Thu Oct 20 20:29:15 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 20 Oct 2016 19:29:15 +0100
Subject: [R-sig-ME] MCMCglmm multivariate meta-analysis with covariance
In-Reply-To: <CAL+h=zbBCW33S4UbSE=QC=B0OfhHEiodNmsZZyp9nhudgrXS2Q@mail.gmail.com>
References: <CAL+h=zYwZAcv5kPbixqpf7FXmz6tAcs3TM57kZOD3D-Qh4A1Jw@mail.gmail.com>
	<a9adf50e-73af-8d1d-26ac-f2e5ba4a8a50@ed.ac.uk>
	<CAL+h=zbBCW33S4UbSE=QC=B0OfhHEiodNmsZZyp9nhudgrXS2Q@mail.gmail.com>
Message-ID: <783e2a23-1e30-51a8-bb87-2140b4328d7b@ed.ac.uk>

Hi Jon,

MCMCglmm fits random-effect meta-analysis (I think this is what it is 
caused) which assumes that even after correcting for sampling errors 
there will be some 'real' between-observation variance (and in your case 
covariance). I'm not sure what you are modelling, but at least in the 
types of data I work with I can't really believe there isn't some  
'real' between-observation variance.

Cheers,

Jarrod




On 20/10/2016 19:09, Jon Bischof wrote:
> Jarrod,
>
> Thanks for your detailed response! Your understanding of my model is 
> correct: it's just a single grouping metaanalysis in two dimensions:
>
> (y_i,1, y_i,2) ~ Normal ( (theta_i,1, theta_i,2), V_i )
> (theta_i,1, theta_i,2) ~ Normal ( (mu_1, mu_2), S )
>
> where V_i is a known covariance matrix of measurement error.
>
> As a new user of mcmcglmm I will need some time to experiment with 
> your idea to confirm that it works. My understanding, however, was 
> that residual error was specified in the R matrix and not the G 
> matrix. Do we need to fix R as well? Will we still be able to estimate S?
>
> Thanks!
> Jon
>
> On Tue, Oct 18, 2016 at 11:45 PM, Jarrod Hadfield <j.hadfield at ed.ac.uk 
> <mailto:j.hadfield at ed.ac.uk>> wrote:
>
>     Hi Jon,
>
>     If you have the covariance matrix for your observations, then take
>     its inverse and store it in sparse format:
>
>     Cinv_sparse<-as(Cinv, "dgCMatrix")
>
>     where Cinv is the inverse in dense format. When you say
>     multivariate do you mean something like an explicit bivariate
>     response such that the fixed formula is of the form cbind(y_1,
>     y_2)~...?  If so you need to organise your data in long format and
>     pass a single response vector. You can include a variable that
>     denotes whether the observation is y_1 or y_2 and use it like
>     "trait", and include a variable that denotes the original row for
>     the observation and use it like "units". If we call this second
>     variable "row" then having fit "row" as a random effect, and pass
>     the argument ginverse=list(row=Cinv_sparse) to MCMCglmm. You will
>     also need to fix the "row" variance to one in the prior:
>
>     G1=list(V=1, fix=1)
>
>     Presumably covariances are only non-zero between observations from
>     the same original row? If so make sure the sparse Matrix also
>     represents this: numerical issues during inversion may cause zero
>     entries to differ slightly from zero and hence not be represented
>     as zero.
>
>     Cheers,
>
>     Jarrod
>
>
>
>
>     Then you can fit the term ~trait:units
>
>
>
>
>
>     On 19/10/2016 05:41, Jon Bischof wrote:
>
>         I'm interested in fitting a multivariate meta-analysis model with
>         correlated measurement error. This means fixing the error to a
>         covariance
>         matrix per row.
>
>         I saw this post
>         <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q2/020180.html
>         <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q2/020180.html>>
>         on
>         the mailing list about non-correlated outcomes, but the noise
>         correlation
>         is too large to ignore in my use case. Professor Hadfield
>         implies in the
>         post that it is possible but "complicated". Does anyone know
>         how to do it?
>
>         Thanks!
>         Jon Bischof
>
>                 [[alternative HTML version deleted]]
>
>         _______________________________________________
>         R-sig-mixed-models at r-project.org
>         <mailto:R-sig-mixed-models at r-project.org> mailing list
>         https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>         <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
>
>
>
>     -- 
>     The University of Edinburgh is a charitable body, registered in
>     Scotland, with registration number SC005336.
>
>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20161020/c4707692/attachment-0001.pl>

From jbischof.stat at gmail.com  Thu Oct 20 21:31:04 2016
From: jbischof.stat at gmail.com (Jon Bischof)
Date: Thu, 20 Oct 2016 12:31:04 -0700
Subject: [R-sig-ME] MCMCglmm multivariate meta-analysis with covariance
In-Reply-To: <783e2a23-1e30-51a8-bb87-2140b4328d7b@ed.ac.uk>
References: <CAL+h=zYwZAcv5kPbixqpf7FXmz6tAcs3TM57kZOD3D-Qh4A1Jw@mail.gmail.com>
	<a9adf50e-73af-8d1d-26ac-f2e5ba4a8a50@ed.ac.uk>
	<CAL+h=zbBCW33S4UbSE=QC=B0OfhHEiodNmsZZyp9nhudgrXS2Q@mail.gmail.com>
	<783e2a23-1e30-51a8-bb87-2140b4328d7b@ed.ac.uk>
Message-ID: <CAL+h=zb5JRzVD3d9O4mqjsH6mSA_mjWBe_zWcDjcnwn6=9WgMQ@mail.gmail.com>

Hi Jarrod,

You are making an excellent point, but I am working in a big data setting
where only group aggregates, not individual outcomes, will fit in memory.

In brief: this model estimates the log ratio of advertiser spend and
performance between a control and experiment condition. These two outcomes
are measured at a click level, but we have far too many clicks to store
individually in memory. Instead we want to fit a model to advertiser
sufficient statistics, which are the log ratio mean and within variance.

For either of these outcomes individually, I was able to fit the following
model and recover the true hyperparameters from fake data:

# Prior distribution for model
> kMCMCglmmPrior <- list(R=list(fix=1, V=1e-6), G=list(G1=list(V=1e-1,
> nu=-2)))
> # dat: A data.frame with columns 'group', 'outcome', and 'obsVar'
> m <- MCMCglmm::MCMCglmm(outcome ~ 1, random=~group, mev=dat$obsVar,
>                           data=dat, prior=kMCMCglmmPrior)


However, for many GLMM implementations (including LMER), the within and
between variance are not separately identifiable when aggregate data are
provided. This is not true statistically, but the aggregate likelihood
requires special handling and cannot be fit from a program that expects one
row for each y_ij.

Can MCMCglmm fit my one-dimensional model when R is not fixed and there is
only one row per group?

Thanks!
Jon



On Thu, Oct 20, 2016 at 11:29 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk>
wrote:

> Hi Jon,
>
> MCMCglmm fits random-effect meta-analysis (I think this is what it is
> caused) which assumes that even after correcting for sampling errors there
> will be some 'real' between-observation variance (and in your case
> covariance). I'm not sure what you are modelling, but at least in the types
> of data I work with I can't really believe there isn't some  'real'
> between-observation variance.
>
> Cheers,
>
> Jarrod
>
>
>
>
> On 20/10/2016 19:09, Jon Bischof wrote:
>
> Jarrod,
>
> Thanks for your detailed response! Your understanding of my model is
> correct: it's just a single grouping metaanalysis in two dimensions:
>
> (y_i,1, y_i,2) ~ Normal ( (theta_i,1, theta_i,2), V_i )
> (theta_i,1, theta_i,2) ~ Normal ( (mu_1, mu_2), S )
>
> where V_i is a known covariance matrix of measurement error.
>
> As a new user of mcmcglmm I will need some time to experiment with your
> idea to confirm that it works. My understanding, however, was that residual
> error was specified in the R matrix and not the G matrix. Do we need to fix
> R as well? Will we still be able to estimate S?
>
> Thanks!
> Jon
>
> On Tue, Oct 18, 2016 at 11:45 PM, Jarrod Hadfield <j.hadfield at ed.ac.uk>
> wrote:
>
>> Hi Jon,
>>
>> If you have the covariance matrix for your observations, then take its
>> inverse and store it in sparse format:
>>
>> Cinv_sparse<-as(Cinv, "dgCMatrix")
>>
>> where Cinv is the inverse in dense format. When you say multivariate do
>> you mean something like an explicit bivariate response such that the fixed
>> formula is of the form cbind(y_1, y_2)~...?  If so you need to organise
>> your data in long format and pass a single response vector. You can include
>> a variable that denotes whether the observation is y_1 or y_2 and use it
>> like "trait", and include a variable that denotes the original row for the
>> observation and use it like "units". If we call this second variable "row"
>> then having fit "row" as a random effect, and pass the argument
>> ginverse=list(row=Cinv_sparse) to MCMCglmm. You will also need to fix the
>> "row" variance to one in the prior:
>>
>> G1=list(V=1, fix=1)
>>
>> Presumably covariances are only non-zero between observations from the
>> same original row? If so make sure the sparse Matrix also represents this:
>> numerical issues during inversion may cause zero entries to differ slightly
>> from zero and hence not be represented as zero.
>>
>> Cheers,
>>
>> Jarrod
>>
>>
>>
>>
>> Then you can fit the term ~trait:units
>>
>>
>>
>>
>>
>> On 19/10/2016 05:41, Jon Bischof wrote:
>>
>>> I'm interested in fitting a multivariate meta-analysis model with
>>> correlated measurement error. This means fixing the error to a covariance
>>> matrix per row.
>>>
>>> I saw this post
>>> <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q2/020180.html>
>>> on
>>> the mailing list about non-correlated outcomes, but the noise correlation
>>> is too large to ignore in my use case. Professor Hadfield implies in the
>>> post that it is possible but "complicated". Does anyone know how to do
>>> it?
>>>
>>> Thanks!
>>> Jon Bischof
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>>
>> --
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
>>
>
>
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
>

	[[alternative HTML version deleted]]


From j.hadfield at ed.ac.uk  Thu Oct 20 22:04:24 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 20 Oct 2016 21:04:24 +0100
Subject: [R-sig-ME] MCMCglmm multivariate meta-analysis with covariance
In-Reply-To: <CAL+h=zb5JRzVD3d9O4mqjsH6mSA_mjWBe_zWcDjcnwn6=9WgMQ@mail.gmail.com>
References: <CAL+h=zYwZAcv5kPbixqpf7FXmz6tAcs3TM57kZOD3D-Qh4A1Jw@mail.gmail.com>
	<a9adf50e-73af-8d1d-26ac-f2e5ba4a8a50@ed.ac.uk>
	<CAL+h=zbBCW33S4UbSE=QC=B0OfhHEiodNmsZZyp9nhudgrXS2Q@mail.gmail.com>
	<783e2a23-1e30-51a8-bb87-2140b4328d7b@ed.ac.uk>
	<CAL+h=zb5JRzVD3d9O4mqjsH6mSA_mjWBe_zWcDjcnwn6=9WgMQ@mail.gmail.com>
Message-ID: <da19eb6e-d39d-500b-dfac-f4828888ce3a@ed.ac.uk>

Hi Jon,

In this instance you are actually fitting 3 group-level random effects; 
1 in the random part of the model, one in residual part of the model and 
one in the argument to mev.

The random effects and the residual effects are confounded and are not 
identifiable in the likelihood. You would be better just fitting the model:

m <- MCMCglmm::MCMCglmm(outcome ~ 1, rcov=~group, 
mev=dat$obsVar,data=dat, prior=list(R=list(V=1, nu=0)))

it is virtually the same as your original model (since the residual 
variance was set close to zero)  but it will iterate faster and mix 
better. In this model you estimate the between-observation (in your case 
between-group) variance after accounting for the sampling variance. 
There are two additional, equivalent, ways of writing this model:

m.a <- MCMCglmm::MCMCglmm(outcome ~ 1, random=~us(sqrt(obsVar)):group, 
rcov=~group,data=dat, prior=list(R=list(V=1, nu=0), G=list(G1=list(V=1, 
fix=1))))

and

Cinv_sparse<-as(diag(1/dat$obsVar), "dgCMatrix")

rownames(Cinv_sparse)<-dat$group

m.b <- MCMCglmm::MCMCglmm(outcome ~ 1, random=~group, 
rcov=~group,data=dat, ginverse=list(group=Cinv_sparse), 
prior=list(R=list(V=1, nu=0), G=list(G1=list(V=1, fix=1))))

It is this latter equivalence that I suggested you exploit in the 
bivariate case. Lets say there are now two rows per group (one for spend 
and one for performance) and you have a column in the data-frame 
indicating response type (lets call it sORp)

I don't know how your 2x2 sampling covariance matrices for each group 
are stored, but lets say they are in list form. As an example, with two 
groups:

C<-list(S1=matrix(c(1,0.5, 0.5,1),2,2), S2=matrix(c(2,0, 0,1),2,2))

Cinv_sparse<-as(list2bdiag(lapply(C,solve)), "dgCMatrix")

Assuming the data are sorted response type with group (i.e. in the same 
order as C) then you can fit

dat$observation<-1:nrow(dat)

rownames(Cinv_sparse)<-1:nrow(dat)

m.biv <- MCMCglmm::MCMCglmm(outcome ~ sORp-1, random=~observation, 
rcov=~us(sORp):group,data=dat, ginverse=list(observation=Cinv_sparse), 
prior=list(R=list(V=diag(2), nu=0), G=list(G1=list(V=1, fix=1))))

The rcov term models between-observation (ie. between group) 
(co)variation not due to sampling error. These variances could be zero, 
but you would have to have a pretty effective way of randomising the 
original (unsummarised) observations between groups.

Cheers,

Jarrod




On 20/10/2016 20:31, Jon Bischof wrote:
> Hi Jarrod,
>
> You are making an excellent point, but I am working in a big data 
> setting where only group aggregates, not individual outcomes, will fit 
> in memory.
>
> In brief: this model estimates the log ratio of advertiser spend and 
> performance between a control and experiment condition. These two 
> outcomes are measured at a click level, but we have far too many 
> clicks to store individually in memory. Instead we want to fit a model 
> to advertiser sufficient statistics, which are the log ratio mean and 
> within variance.
>
> For either of these outcomes individually, I was able to fit the 
> following model and recover the true hyperparameters from fake data:
>
>     # Prior distribution for model
>     kMCMCglmmPrior <- list(R=list(fix=1, V=1e-6),
>     G=list(G1=list(V=1e-1, nu=-2)))
>     # dat: A data.frame with columns 'group', 'outcome', and 'obsVar'
>     m <- MCMCglmm::MCMCglmm(outcome ~ 1, random=~group, mev=dat$obsVar,
>         data=dat, prior=kMCMCglmmPrior)
>
>
> However, for many GLMM implementations (including LMER), the within 
> and between variance are not separately identifiable when aggregate 
> data are provided. This is not true statistically, but the aggregate 
> likelihood requires special handling and cannot be fit from a program 
> that expects one row for each y_ij.
>
> Can MCMCglmm fit my one-dimensional model when R is not fixed and 
> there is only one row per group?
>
> Thanks!
> Jon
>
>
>
> On Thu, Oct 20, 2016 at 11:29 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk 
> <mailto:j.hadfield at ed.ac.uk>> wrote:
>
>     Hi Jon,
>
>     MCMCglmm fits random-effect meta-analysis (I think this is what it
>     is caused) which assumes that even after correcting for sampling
>     errors there will be some 'real' between-observation variance (and
>     in your case covariance). I'm not sure what you are modelling, but
>     at least in the types of data I work with I can't really believe
>     there isn't some  'real' between-observation variance.
>
>     Cheers,
>
>     Jarrod
>
>
>
>
>     On 20/10/2016 19:09, Jon Bischof wrote:
>>     Jarrod,
>>
>>     Thanks for your detailed response! Your understanding of my model
>>     is correct: it's just a single grouping metaanalysis in two
>>     dimensions:
>>
>>     (y_i,1, y_i,2) ~ Normal ( (theta_i,1, theta_i,2), V_i )
>>     (theta_i,1, theta_i,2) ~ Normal ( (mu_1, mu_2), S )
>>
>>     where V_i is a known covariance matrix of measurement error.
>>
>>     As a new user of mcmcglmm I will need some time to experiment
>>     with your idea to confirm that it works. My understanding,
>>     however, was that residual error was specified in the R matrix
>>     and not the G matrix. Do we need to fix R as well? Will we still
>>     be able to estimate S?
>>
>>     Thanks!
>>     Jon
>>
>>     On Tue, Oct 18, 2016 at 11:45 PM, Jarrod Hadfield
>>     <j.hadfield at ed.ac.uk <mailto:j.hadfield at ed.ac.uk>> wrote:
>>
>>         Hi Jon,
>>
>>         If you have the covariance matrix for your observations, then
>>         take its inverse and store it in sparse format:
>>
>>         Cinv_sparse<-as(Cinv, "dgCMatrix")
>>
>>         where Cinv is the inverse in dense format. When you say
>>         multivariate do you mean something like an explicit bivariate
>>         response such that the fixed formula is of the form
>>         cbind(y_1, y_2)~...?  If so you need to organise your data in
>>         long format and pass a single response vector. You can
>>         include a variable that denotes whether the observation is
>>         y_1 or y_2 and use it like "trait", and include a variable
>>         that denotes the original row for the observation and use it
>>         like "units". If we call this second variable "row" then
>>         having fit "row" as a random effect, and pass the argument
>>         ginverse=list(row=Cinv_sparse) to MCMCglmm. You will also
>>         need to fix the "row" variance to one in the prior:
>>
>>         G1=list(V=1, fix=1)
>>
>>         Presumably covariances are only non-zero between observations
>>         from the same original row? If so make sure the sparse Matrix
>>         also represents this: numerical issues during inversion may
>>         cause zero entries to differ slightly from zero and hence not
>>         be represented as zero.
>>
>>         Cheers,
>>
>>         Jarrod
>>
>>
>>
>>
>>         Then you can fit the term ~trait:units
>>
>>
>>
>>
>>
>>         On 19/10/2016 05:41, Jon Bischof wrote:
>>
>>             I'm interested in fitting a multivariate meta-analysis
>>             model with
>>             correlated measurement error. This means fixing the error
>>             to a covariance
>>             matrix per row.
>>
>>             I saw this post
>>             <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q2/020180.html
>>             <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q2/020180.html>>
>>             on
>>             the mailing list about non-correlated outcomes, but the
>>             noise correlation
>>             is too large to ignore in my use case. Professor Hadfield
>>             implies in the
>>             post that it is possible but "complicated". Does anyone
>>             know how to do it?
>>
>>             Thanks!
>>             Jon Bischof
>>
>>                     [[alternative HTML version deleted]]
>>
>>             _______________________________________________
>>             R-sig-mixed-models at r-project.org
>>             <mailto:R-sig-mixed-models at r-project.org> mailing list
>>             https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>             <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
>>
>>
>>
>>         -- 
>>         The University of Edinburgh is a charitable body, registered in
>>         Scotland, with registration number SC005336.
>>
>>
>
>
>     The University of Edinburgh is a charitable body, registered in
>     Scotland, with registration number SC005336.
>
>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20161020/1671baca/attachment.pl>

From jbischof.stat at gmail.com  Thu Oct 20 23:51:29 2016
From: jbischof.stat at gmail.com (Jon Bischof)
Date: Thu, 20 Oct 2016 14:51:29 -0700
Subject: [R-sig-ME] MCMCglmm multivariate meta-analysis with covariance
In-Reply-To: <da19eb6e-d39d-500b-dfac-f4828888ce3a@ed.ac.uk>
References: <CAL+h=zYwZAcv5kPbixqpf7FXmz6tAcs3TM57kZOD3D-Qh4A1Jw@mail.gmail.com>
	<a9adf50e-73af-8d1d-26ac-f2e5ba4a8a50@ed.ac.uk>
	<CAL+h=zbBCW33S4UbSE=QC=B0OfhHEiodNmsZZyp9nhudgrXS2Q@mail.gmail.com>
	<783e2a23-1e30-51a8-bb87-2140b4328d7b@ed.ac.uk>
	<CAL+h=zb5JRzVD3d9O4mqjsH6mSA_mjWBe_zWcDjcnwn6=9WgMQ@mail.gmail.com>
	<da19eb6e-d39d-500b-dfac-f4828888ce3a@ed.ac.uk>
Message-ID: <CAL+h=zYj_Vf6LOMj3FY8XxtkWKUVLUSqqbDTfP--UJzbYfe-sQ@mail.gmail.com>

Jarrod,

First, I want to thank you for your detailed responses---this is extremely
generous and helpful!

I'll get back to you after trying these new model specifications out. The
covariances not due to sampling error (which I call S in my earlier email)
are exactly what I'm trying to estimate! If they are non-zero it means we
have a heterogenous treatment effect across advertisers. The overall mean
mu is important as well, but the posterior of S is the quantity most
difficult to estimate with simple models.

Jon

On Thu, Oct 20, 2016 at 1:04 PM, Jarrod Hadfield <j.hadfield at ed.ac.uk>
wrote:

> Hi Jon,
>
> In this instance you are actually fitting 3 group-level random effects; 1
> in the random part of the model, one in residual part of the model and one
> in the argument to mev.
>
> The random effects and the residual effects are confounded and are not
> identifiable in the likelihood. You would be better just fitting the model:
>
> m <- MCMCglmm::MCMCglmm(outcome ~ 1, rcov=~group, mev=dat$obsVar,data=dat,
> prior=list(R=list(V=1, nu=0)))
>
> it is virtually the same as your original model (since the residual
> variance was set close to zero)  but it will iterate faster and mix better.
> In this model you estimate the between-observation (in your case
> between-group) variance after accounting for the sampling variance. There
> are two additional, equivalent, ways of writing this model:
>
> m.a <- MCMCglmm::MCMCglmm(outcome ~ 1, random=~us(sqrt(obsVar)):group,
> rcov=~group,data=dat, prior=list(R=list(V=1, nu=0), G=list(G1=list(V=1,
> fix=1))))
> and
>
> Cinv_sparse<-as(diag(1/dat$obsVar), "dgCMatrix")
>
> rownames(Cinv_sparse)<-dat$group
>
> m.b <- MCMCglmm::MCMCglmm(outcome ~ 1, random=~group, rcov=~group,data=dat,
> ginverse=list(group=Cinv_sparse), prior=list(R=list(V=1, nu=0), G=list(G1=list(V=1,
> fix=1))))
>
> It is this latter equivalence that I suggested you exploit in the
> bivariate case. Lets say there are now two rows per group (one for spend
> and one for performance) and you have a column in the data-frame indicating
> response type (lets call it sORp)
>
> I don't know how your 2x2 sampling covariance matrices for each group are
> stored, but lets say they are in list form. As an example, with two groups:
>
> C<-list(S1=matrix(c(1,0.5, 0.5,1),2,2), S2=matrix(c(2,0, 0,1),2,2))
>
> Cinv_sparse<-as(list2bdiag(lapply(C,solve)), "dgCMatrix")
>
> Assuming the data are sorted response type with group (i.e. in the same
> order as C) then you can fit
>
> dat$observation<-1:nrow(dat)
>
> rownames(Cinv_sparse)<-1:nrow(dat)
>
> m.biv <- MCMCglmm::MCMCglmm(outcome ~ sORp-1, random=~observation,
> rcov=~us(sORp):group,data=dat, ginverse=list(observation=Cinv_sparse),
> prior=list(R=list(V=diag(2), nu=0), G=list(G1=list(V=1, fix=1))))
>
> The rcov term models between-observation (ie. between group) (co)variation
> not due to sampling error. These variances could be zero, but you would
> have to have a pretty effective way of randomising the original
> (unsummarised) observations between groups.
>
> Cheers,
>
> Jarrod
>
>
>
>
> On 20/10/2016 20:31, Jon Bischof wrote:
>
> Hi Jarrod,
>
> You are making an excellent point, but I am working in a big data setting
> where only group aggregates, not individual outcomes, will fit in memory.
>
> In brief: this model estimates the log ratio of advertiser spend and
> performance between a control and experiment condition. These two outcomes
> are measured at a click level, but we have far too many clicks to store
> individually in memory. Instead we want to fit a model to advertiser
> sufficient statistics, which are the log ratio mean and within variance.
>
> For either of these outcomes individually, I was able to fit the following
> model and recover the true hyperparameters from fake data:
>
> # Prior distribution for model
>> kMCMCglmmPrior <- list(R=list(fix=1, V=1e-6), G=list(G1=list(V=1e-1,
>> nu=-2)))
>> # dat: A data.frame with columns 'group', 'outcome', and 'obsVar'
>> m <- MCMCglmm::MCMCglmm(outcome ~ 1, random=~group, mev=dat$obsVar,
>>                           data=dat, prior=kMCMCglmmPrior)
>
>
> However, for many GLMM implementations (including LMER), the within and
> between variance are not separately identifiable when aggregate data are
> provided. This is not true statistically, but the aggregate likelihood
> requires special handling and cannot be fit from a program that expects one
> row for each y_ij.
>
> Can MCMCglmm fit my one-dimensional model when R is not fixed and there is
> only one row per group?
>
> Thanks!
> Jon
>
>
>
> On Thu, Oct 20, 2016 at 11:29 AM, Jarrod Hadfield <j.hadfield at ed.ac.uk>
> wrote:
>
>> Hi Jon,
>>
>> MCMCglmm fits random-effect meta-analysis (I think this is what it is
>> caused) which assumes that even after correcting for sampling errors there
>> will be some 'real' between-observation variance (and in your case
>> covariance). I'm not sure what you are modelling, but at least in the types
>> of data I work with I can't really believe there isn't some  'real'
>> between-observation variance.
>>
>> Cheers,
>>
>> Jarrod
>>
>>
>>
>>
>> On 20/10/2016 19:09, Jon Bischof wrote:
>>
>> Jarrod,
>>
>> Thanks for your detailed response! Your understanding of my model is
>> correct: it's just a single grouping metaanalysis in two dimensions:
>>
>> (y_i,1, y_i,2) ~ Normal ( (theta_i,1, theta_i,2), V_i )
>> (theta_i,1, theta_i,2) ~ Normal ( (mu_1, mu_2), S )
>>
>> where V_i is a known covariance matrix of measurement error.
>>
>> As a new user of mcmcglmm I will need some time to experiment with your
>> idea to confirm that it works. My understanding, however, was that residual
>> error was specified in the R matrix and not the G matrix. Do we need to fix
>> R as well? Will we still be able to estimate S?
>>
>> Thanks!
>> Jon
>>
>> On Tue, Oct 18, 2016 at 11:45 PM, Jarrod Hadfield <j.hadfield at ed.ac.uk>
>> wrote:
>>
>>> Hi Jon,
>>>
>>> If you have the covariance matrix for your observations, then take its
>>> inverse and store it in sparse format:
>>>
>>> Cinv_sparse<-as(Cinv, "dgCMatrix")
>>>
>>> where Cinv is the inverse in dense format. When you say multivariate do
>>> you mean something like an explicit bivariate response such that the fixed
>>> formula is of the form cbind(y_1, y_2)~...?  If so you need to organise
>>> your data in long format and pass a single response vector. You can include
>>> a variable that denotes whether the observation is y_1 or y_2 and use it
>>> like "trait", and include a variable that denotes the original row for the
>>> observation and use it like "units". If we call this second variable "row"
>>> then having fit "row" as a random effect, and pass the argument
>>> ginverse=list(row=Cinv_sparse) to MCMCglmm. You will also need to fix the
>>> "row" variance to one in the prior:
>>>
>>> G1=list(V=1, fix=1)
>>>
>>> Presumably covariances are only non-zero between observations from the
>>> same original row? If so make sure the sparse Matrix also represents this:
>>> numerical issues during inversion may cause zero entries to differ slightly
>>> from zero and hence not be represented as zero.
>>>
>>> Cheers,
>>>
>>> Jarrod
>>>
>>>
>>>
>>>
>>> Then you can fit the term ~trait:units
>>>
>>>
>>>
>>>
>>>
>>> On 19/10/2016 05:41, Jon Bischof wrote:
>>>
>>>> I'm interested in fitting a multivariate meta-analysis model with
>>>> correlated measurement error. This means fixing the error to a
>>>> covariance
>>>> matrix per row.
>>>>
>>>> I saw this post
>>>> <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2013q2/020180.html>
>>>> on
>>>> the mailing list about non-correlated outcomes, but the noise
>>>> correlation
>>>> is too large to ignore in my use case. Professor Hadfield implies in the
>>>> post that it is possible but "complicated". Does anyone know how to do
>>>> it?
>>>>
>>>> Thanks!
>>>> Jon Bischof
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>
>>>
>>> --
>>> The University of Edinburgh is a charitable body, registered in
>>> Scotland, with registration number SC005336.
>>>
>>>
>>
>>
>> The University of Edinburgh is a charitable body, registered in
>> Scotland, with registration number SC005336.
>>
>>
>
>
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
>

	[[alternative HTML version deleted]]


From i.mandl at gmx.at  Fri Oct 21 16:52:32 2016
From: i.mandl at gmx.at (Isabella Mandl)
Date: Fri, 21 Oct 2016 15:52:32 +0100
Subject: [R-sig-ME] Beginner help for mixed effects model
Message-ID: <cd9f3d36-9d9e-dd39-b8ac-c22f8cea58ea@gmx.at>

Dear all,
I apologise for the rather simplistic questions I am about to ask but I 
am very much at the beginning of my analysis and have only just worked 
my way through to mixed effect models in R. I collected some ecological 
data over the past years and have been advised by my PhD supervisors to 
use a GLMM. It should have been rather straightforward:
I have measured "looking time" in response to different playback stimuli 
(0,1,2,3) in a group of 13 individuals - measures were repeated four 
times over the year. I now wanted to look at the effect of Stimulus 
Type, Season and Sex on "looking time" as well as look at whether 
different sexes have different looking times in different seasons 
(sex-season interaction). Random effects are ID of the animal and Trial.
This is the model I built (using lme4):

glmer(LookingTime~Stimulus+Season*Sex+(1|ID)+(1|Trial), 
family=Gamma(link="log"), data=playbacks)
which gives me the following output:

Generalized linear mixed model fit by maximum likelihood (Laplace 
Approximation) ['glmerMod']
  Family: Gamma  ( log )
Formula: VigilanceTowardsAdjus ~ Stimulus + Season * Sex + (1 | ID) 
+      (1 | Trial)
    Data: playbacks

      AIC      BIC   logLik deviance df.resid
   2731.9   2775.1  -1354.0   2707.9      258

Scaled residuals:
     Min      1Q  Median      3Q     Max
-1.4529 -0.5723 -0.0321  0.6412  2.1003

Random effects:
  Groups   Name        Variance  Std.Dev.
  Trial    (Intercept) 3.985e-01 0.6313040
  ID       (Intercept) 6.185e-08 0.0002487
  Residual             4.464e-01 0.6681655
Number of obs: 270, groups:  Trial, 135; ID, 13

Fixed effects:
                                        Estimate      Std. Error   t 
value     Pr(>|z|)
(Intercept)                       3.425875   0.184784  18.540 < 2e-16 ***
Stimulus                           0.181798   0.036725   4.950 7.41e-07 ***
Season[T.ED]                   0.382235   0.259361   1.474 0.1405
Season[T.EW]                  0.338762   0.259361   1.306 0.1915
Season[T.W]                    0.138404   0.259609   0.533 0.5939
Sex[T.M]                           0.207001   0.272354   0.760 0.4472
Season[T.ED]:Sex[T.M] -0.649771   0.387518  -1.677   0.0936 .
Season[T.EW]:Sex[T.M] -0.428328   0.378500  -1.132   0.2578
Season[T.W]:Sex[T.M]  -0.008209   0.385262  -0.021   0.9830
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
             (Intr) Stimls Ss[T.ED] Ss[T.EW] Ss[T.W] S[T.M] S[T.ED]: 
S[T.EW]:
Stimulus -0.204
Seasn[T.ED] -0.684 0.004
Seasn[T.EW] -0.684  0.004 0.486
Season[T.W] -0.681 -0.003  0.486 0.486
Sex[T.M]    -0.651  0.004  0.463    0.463 0.463
S[T.ED]:S[T  0.458 -0.002 -0.669   -0.326   -0.325 -0.703
S[T.EW]:S[T  0.468 -0.003 -0.333   -0.685   -0.333  -0.720 0.506
S[T.W]:S[T.  0.457  0.011 -0.327   -0.327   -0.664  -0.707 0.497    0.509
convergence code: 0
unable to evaluate scaled gradient
Model failed to converge: degenerate  Hessian with 1 negative eigenvalues

The model fails to converge and gives me the following residual plot 
(which to me looks bad):

I don't really know what to look for to make it fit better. There is an 
inbalance in trial numbers (double the amount of 0-Stimulus trials than 
any of the others) and a slight inbalance in subjects (not all tested 
all four times) - could that have something to do with it? Should I be 
looking at PQL instead of ML? Or is it the error distribution that I'm 
getting wrong?
I am grateful for any help that points me into the right direction 
because I feel like I'm missing obvious things here.
Kindest regards,
Isabella


From bbolker at gmail.com  Sat Oct 22 23:16:56 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 22 Oct 2016 17:16:56 -0400
Subject: [R-sig-ME] Beginner help for mixed effects model
In-Reply-To: <cd9f3d36-9d9e-dd39-b8ac-c22f8cea58ea@gmx.at>
References: <cd9f3d36-9d9e-dd39-b8ac-c22f8cea58ea@gmx.at>
Message-ID: <CABghstR-V=KjjPT6dUJgdUX_ApeAiBHX3wNHQ_+T2MtuiOyfuQ@mail.gmail.com>

  Comments inline marked with BMB>

On Fri, Oct 21, 2016 at 10:52 AM, Isabella Mandl <i.mandl at gmx.at> wrote:
> Dear all,
> I apologise for the rather simplistic questions I am about to ask but I am
> very much at the beginning of my analysis and have only just worked my way
> through to mixed effect models in R. I collected some ecological data over
> the past years and have been advised by my PhD supervisors to use a GLMM. It
> should have been rather straightforward:
> I have measured "looking time" in response to different playback stimuli
> (0,1,2,3) in a group of 13 individuals - measures were repeated four times
> over the year. I now wanted to look at the effect of Stimulus Type, Season
> and Sex on "looking time" as well as look at whether different sexes have
> different looking times in different seasons (sex-season interaction).
> Random effects are ID of the animal and Trial.
> This is the model I built (using lme4):
>
> glmer(LookingTime~Stimulus+Season*Sex+(1|ID)+(1|Trial),
> family=Gamma(link="log"), data=playbacks)
> which gives me the following output:
>
> Generalized linear mixed model fit by maximum likelihood (Laplace
> Approximation) ['glmerMod']
>  Family: Gamma  ( log )
> Formula: VigilanceTowardsAdjus ~ Stimulus + Season * Sex + (1 | ID) +
> (1 | Trial)
>    Data: playbacks
>
>      AIC      BIC   logLik deviance df.resid
>   2731.9   2775.1  -1354.0   2707.9      258
>
> Scaled residuals:
>     Min      1Q  Median      3Q     Max
> -1.4529 -0.5723 -0.0321  0.6412  2.1003
>
> Random effects:
>  Groups   Name        Variance  Std.Dev.
>  Trial    (Intercept) 3.985e-01 0.6313040
>  ID       (Intercept) 6.185e-08 0.0002487
>  Residual             4.464e-01 0.6681655
> Number of obs: 270, groups:  Trial, 135; ID, 13

BMB>  this reflects essentially no among-ID variation once other variation is
taken into account.  (Not a big problem/surprising but worth noting)

>
> Fixed effects:
>                                        Estimate      Std. Error   t value
> Pr(>|z|)
> (Intercept)                       3.425875   0.184784  18.540 < 2e-16 ***
> Stimulus                           0.181798   0.036725   4.950 7.41e-07 ***
> Season[T.ED]                   0.382235   0.259361   1.474 0.1405
> Season[T.EW]                  0.338762   0.259361   1.306 0.1915
> Season[T.W]                    0.138404   0.259609   0.533 0.5939
> Sex[T.M]                           0.207001   0.272354   0.760 0.4472
> Season[T.ED]:Sex[T.M] -0.649771   0.387518  -1.677   0.0936 .
> Season[T.EW]:Sex[T.M] -0.428328   0.378500  -1.132   0.2578
> Season[T.W]:Sex[T.M]  -0.008209   0.385262  -0.021   0.9830
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> convergence code: 0
> unable to evaluate scaled gradient
> Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
>
> The model fails to converge and gives me the following residual plot (which
> to me looks bad):

BMB>  The mailing list strips out graphical attachments.  Any chance you
can post the graph somewhere, e.g. imgur.com ?

  For failure to converge, see ?convergence

  I would strongly recommend that you try a log-Normal model as a
complement/backup for the Gamma model (i.e.
lmer(log(VigilanceTowardsAdjus) ~ ....) ) - in general log-LMMs
and Gamma-GLMMs give fairly similar results, and the former are a
little more stable.

>
> I don't really know what to look for to make it fit better. There is an
> inbalance in trial numbers (double the amount of 0-Stimulus trials than any
> of the others) and a slight inbalance in subjects (not all tested all four
> times) - could that have something to do with it?

  That shouldn't be a big deal.

> Should I be looking at PQL
> instead of ML?

Probably not ...

> Or is it the error distribution that I'm getting wrong?
> I am grateful for any help that points me into the right direction because I
> feel like I'm missing obvious things here.
> Kindest regards,
> Isabella

   Just guessing without the diagnostic plots, but I often look at the raw data
for things that might be violations of the model  (nonlinearities on
the log scale of the response to Stimulus - at the moment you're
assuming log-linear response), outliers ...)

  It's conceivable that the convergence warning is just wrong in the
singular fit case (which yours is very close to) - if you like, you
can send me your data (in our never-ending struggle to get the
convergence warnings for lme4 to be just right)


From dmichl at uni-potsdam.de  Sun Oct 23 21:01:49 2016
From: dmichl at uni-potsdam.de (Diana Michl)
Date: Sun, 23 Oct 2016 21:01:49 +0200
Subject: [R-sig-ME] package "ordinal" - failure to converge and slow
	calculations
Message-ID: <fe6f0815-5ae5-f63b-a126-881373f139d1@uni-potsdam.de>

Dear Rune Haubo,

I'm using 'ordinal' to model my data - thank you for making it possible! 
But I'm running into a problem and would be very grateful for any 
response on this.

I have an ordered rating response ranging from 1-5 (Wert). The 
predictors are sex (M_W), origin (Richtung), education (Bildung2), and 
age (Alter) of the participants who rated 122-244 items (ItemId). Age is 
numeric, the rest is helmert or sum-contrast coded, Wert is an ordered 
factor. Please see the table below.

Now, I have 3 more data tables that look practically the same and the 
models run okay with those. But with the one below and this code:

ord.allnwo <- clmm(Wert ~ M_W + Alter + Richtung + Bildung2 + (1|ItemId) 
+ (1|Id), data=spnwounsc,                   model=T, Hess=T, 
link="logit", na.action=na.omit, threshold="flexible",         
               control=clmm.control(grtol=1e-6))

I always get the message "Error: optimizer nlminb failed to converge". I 
tried changing the command "control=clmm.control(grtol=5e-4))" to 
numbers between 5e-4 and 1e-9, as you suggested to someone else with the 
same problem. Changing the optimizer to'ucminf' is impossible, according 
to the R error message.

head(spnwounsc)    X  Id M_W Alter Bundesl Richtung            Bildung Liste beide.L ItemId Wert Bildung2
1 1 265   2    55  Ba-Wue        3 Hoch/Fachhochschul     2      ja      1    3        c
2 1 265   2    55  Ba-Wue        3 Hoch/Fachhochschul     2      ja      2    5        c
3 1 265   2    55  Ba-Wue        3 Hoch/Fachhochschul     2      ja      3    5        c
4 1 265   2    55  Ba-Wue        3 Hoch/Fachhochschul     2      ja      4    4        c
5 1 265   2    55  Ba-Wue        3 Hoch/Fachhochschul     2      ja      5    4        c
6 1 265   2    55  Ba-Wue        3 Hoch/Fachhochschul     2      ja      6    5        c
.
.
.
770 4 273   1    46       Hamburg        1 Hoch/Fachhochschul     1    nein     38    2
771 4 273   1    46       Hamburg        1 Hoch/Fachhochschul     1    nein     39    3
772 4 273   1    46       Hamburg        1 Hoch/Fachhochschul     1    nein     40    1
773 4 273   1    46       Hamburg        1 Hoch/Fachhochschul     1    nein     41    5
774 4 273   1    46       Hamburg        1 Hoch/Fachhochschul     1    nein     42    5
775 4 273   1    46       Hamburg        1 Hoch/Fachhochschul     1    nein     43    1
776 4 273   1    46       Hamburg        1 Hoch/Fachhochschul     1    nein     44    3
777 4 273   1    46       Hamburg        1 Hoch/Fachhochschul     1    nein     45    4
778 4 273   1    46       Hamburg        1 Hoch/Fachhochschul     1    nein     46 <NA>
779 4 273   1    46       Hamburg        1 Hoch/Fachhochschul     1    nein     47 <NA>
780 4 273   1    46       Hamburg        1 Hoch/Fachhochschul     1    nein     48 <NA>
781 4 273   1    46       Hamburg        1 Hoch/Fachhochschul     1    nein     49 <NA>
782 4 273   1    46       Hamburg        1 Hoch/Fachhochschul     1    nein     50 <NA>
783 4 273   1    46       Hamburg        1 Hoch/Fachhochschul     1    nein     51 <NA>
784 4 273   1    46       Hamburg        1 Hoch/Fachhochschul     1    nein     52 <NA>
785 4 273   1    46       Hamburg        1 Hoch/Fachhochschul     1    nein     53 <NA>
.
.
.


Do you know what's going on and how I can remedy this?

Also, R takes several minutes until it finally spits out either model or 
error message. Is this a reason to worry, maybe indicating false 
results? Or is it only because my data frames contains 26000-31000 rows?

Many thanks in advance and kindregards

Diana Michl

-- 
Diana Michl, M.A.
PhD candidate
International Experimental
and Clinical Linguistics
Universit?t Potsdam
www.ling.uni-potsdam.de/staff/dmichl


	[[alternative HTML version deleted]]


From talischen at hotmail.com  Mon Oct 24 12:08:47 2016
From: talischen at hotmail.com (Chen Chun)
Date: Mon, 24 Oct 2016 10:08:47 +0000
Subject: [R-sig-ME] How to estimate the standard error of every single
 random intercept in a mixed linear model?
In-Reply-To: <DB6PR0101MB24230DB767F3AC06A2080433ACDD0@DB6PR0101MB2423.eurprd01.prod.exchangelabs.com>
References: <DB6PR0101MB24230DB767F3AC06A2080433ACDD0@DB6PR0101MB2423.eurprd01.prod.exchangelabs.com>
Message-ID: <VI1PR0101MB2430182E2678121E2C58D6C8ACA90@VI1PR0101MB2430.eurprd01.prod.exchangelabs.com>

Dear all,


I am running a mixed linear model with group (a_i) as random intercept:


y_ij=mu + a_i + e_ij


By using lmer() function, the model outputs an estimated variance of a_i (i.e. var_hat(a)), and it is the sum of (1) the variance of the estimated group mean (i.e. between group variance) and (2) the sum of variance for each estimated group mean a_i_hat,   (i.e. sum of within group variance).


for (1) I can compute it as var(ranef(model)$group). However, I dont know how to compute (2), which is the SE of the estimated random intercept for each group. I know that using se.ranef() function in arm package can help me to extract such variance. But I would like to know how these variance are computed? it's relations to residuals and number of observations per group?


Thanks


 Chen


	[[alternative HTML version deleted]]


From xav.harrison at gmail.com  Wed Oct 26 09:46:45 2016
From: xav.harrison at gmail.com (Xav Harrison)
Date: Wed, 26 Oct 2016 08:46:45 +0100
Subject: [R-sig-ME] G vs R posterior correlation in bivariate MCMCglmm
Message-ID: <CAL8VemeFW2h4w4WnkwfE9FmZgUi-rFpcg9qrcnxS1LgaV=21PQ@mail.gmail.com>

Hi Folks

A potentially rookie question here, but here goes.

I'm using MCMCglmm to fit a bivariate response model where one response is
a Poisson count of pathogen load (Disease) and one is a Gaussian
phentotypic measure of the host (Phenotype).

I've fit a model with fixed effects that one would expect to influence each
of these predictors, and including a random effect of site, of the form:

prior1<-list(R=list(V=diag(2), nu=3),G=list(G1=list(V=diag(2),nu=3, alpha.mu
=c(0,0),alpha.V=diag(2)*1000)))

MCMCglmm(cbind(Disease,Phenotype) ~ trait-1 +
trait:predictors,rcov=~us(trait):units,random=~us(trait):site,family=c("poisson","gaussian"),prior=prior1,verbose=F,nitt=42000,burnin=2000,thin=20)

The golden egg here, and my hypothesis, is that after accounting for some
predictors there will be a negative posterior correlation between the
response traits, where the posterior correlation is calculated as:

Cov(Disease,Phenotype) / sqrt(Var(Disease)*Var(Phenotype))

My question is whether this correlation is relevant at the G structure
level (Site random effect) or at the R structure level, which I take to be
the residual variance at the observation (individual) level?

I suspect the answer is the latter, but I'm struggling to interpret what it
means if there is a negative correlation at the Site level? Does it mean
that the variances of the two traits at the site level are not independent,
in that higher values in one trait for a site tend to produce lower values
for the other?

Any help greatly appreciated.

Cheers

Xav



-- 
-------------------------------------------------------
Dr Xavier Harrison
Research Fellow
Institute of Zoology
Regent's Park
London NW1 4RY
www.zsl.org/xavierharrison
+44 (0) 207 449 6621

	[[alternative HTML version deleted]]


From j.hadfield at ed.ac.uk  Wed Oct 26 11:34:51 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Wed, 26 Oct 2016 10:34:51 +0100
Subject: [R-sig-ME] G vs R posterior correlation in bivariate MCMCglmm
In-Reply-To: <CAL8VemeFW2h4w4WnkwfE9FmZgUi-rFpcg9qrcnxS1LgaV=21PQ@mail.gmail.com>
References: <CAL8VemeFW2h4w4WnkwfE9FmZgUi-rFpcg9qrcnxS1LgaV=21PQ@mail.gmail.com>
Message-ID: <849263db-22f6-72b8-3f81-f5b2399ff53c@ed.ac.uk>

Hi,

If the relationship between the two is causal  - lets say phenotype 
affects disease - then the regression at the two levels will be 
identical: COV(Disease, phenotype)/VAR(phenotype) is the same at the 
site level and the units level. However, they can easily differ. Imagine 
that the amount of resource varies between sites, but within sites all 
individuals have access to the same amount of resource. If there is a 
trade-off then, for a given amount of resource, there  will be a 
positive relationship (if high values of the phenotype are 'good') 
between the two variables observed at the units level. However, imagine 
that as the amount of resource increases individuals can increase their 
phenotype but also reduce the amount of disease. As a consequence the 
between site correlation may well be negative. So, correlations at both 
levels tell you something interesting. However, it should be noted that 
if you can assume causality you are better just fitting a univariate 
model with phenotype in as a predictor: you get an increase in precision 
for your assumption.

Cheers,

Jarrod





On 26/10/16 08:46, Xav Harrison wrote:
> Hi Folks
>
> A potentially rookie question here, but here goes.
>
> I'm using MCMCglmm to fit a bivariate response model where one response is
> a Poisson count of pathogen load (Disease) and one is a Gaussian
> phentotypic measure of the host (Phenotype).
>
> I've fit a model with fixed effects that one would expect to influence each
> of these predictors, and including a random effect of site, of the form:
>
> prior1<-list(R=list(V=diag(2), nu=3),G=list(G1=list(V=diag(2),nu=3, alpha.mu
> =c(0,0),alpha.V=diag(2)*1000)))
>
> MCMCglmm(cbind(Disease,Phenotype) ~ trait-1 +
> trait:predictors,rcov=~us(trait):units,random=~us(trait):site,family=c("poisson","gaussian"),prior=prior1,verbose=F,nitt=42000,burnin=2000,thin=20)
>
> The golden egg here, and my hypothesis, is that after accounting for some
> predictors there will be a negative posterior correlation between the
> response traits, where the posterior correlation is calculated as:
>
> Cov(Disease,Phenotype) / sqrt(Var(Disease)*Var(Phenotype))
>
> My question is whether this correlation is relevant at the G structure
> level (Site random effect) or at the R structure level, which I take to be
> the residual variance at the observation (individual) level?
>
> I suspect the answer is the latter, but I'm struggling to interpret what it
> means if there is a negative correlation at the Site level? Does it mean
> that the variances of the two traits at the site level are not independent,
> in that higher values in one trait for a site tend to produce lower values
> for the other?
>
> Any help greatly appreciated.
>
> Cheers
>
> Xav
>
>
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From xav.harrison at gmail.com  Wed Oct 26 14:16:04 2016
From: xav.harrison at gmail.com (Xav Harrison)
Date: Wed, 26 Oct 2016 13:16:04 +0100
Subject: [R-sig-ME] G vs R posterior correlation in bivariate MCMCglmm
In-Reply-To: <849263db-22f6-72b8-3f81-f5b2399ff53c@ed.ac.uk>
References: <CAL8VemeFW2h4w4WnkwfE9FmZgUi-rFpcg9qrcnxS1LgaV=21PQ@mail.gmail.com>
	<849263db-22f6-72b8-3f81-f5b2399ff53c@ed.ac.uk>
Message-ID: <CAL8Vemc6aJxV+-5qinfBA9S4GAP-5VoTGXXCH22SvsuZJtPppA@mail.gmail.com>

Hi Jarrod

Thanks for the swift and helpful reply. That makes a lot of sense. I should
have said in the original email that I have taken this approach rather than
fitting a Disease ~ Phenotype model because we have predictors that
influence both simultaneously. For example we know both mean disease
intensity and mean phenotype were lower in 2014. I was hoping to use the
bivariate model approach to estimate the posterior correlation between the
two traits once 'controlling' for the predictors and seeing what's left.
Does this sound sensible?

If you wouldn't mind elaborating further, I'm trying to work out what it
means when there is a non-zero correlation at the site level but not at the
units level, which some models have recovered. Is this a problem suggestive
of insufficient data to estimate both matrices accurately, or something
that could be biologically plausible? I'm afraid of thinking I've found the
golden egg but in fact have built myself a nice random number generator by
asking too much of the models.

Thanks again

Xav


On 26 October 2016 at 10:34, Jarrod Hadfield <j.hadfield at ed.ac.uk> wrote:

> Hi,
>
> If the relationship between the two is causal  - lets say phenotype
> affects disease - then the regression at the two levels will be identical:
> COV(Disease, phenotype)/VAR(phenotype) is the same at the site level and
> the units level. However, they can easily differ. Imagine that the amount
> of resource varies between sites, but within sites all individuals have
> access to the same amount of resource. If there is a trade-off then, for a
> given amount of resource, there  will be a positive relationship (if high
> values of the phenotype are 'good') between the two variables observed at
> the units level. However, imagine that as the amount of resource increases
> individuals can increase their phenotype but also reduce the amount of
> disease. As a consequence the between site correlation may well be
> negative. So, correlations at both levels tell you something interesting.
> However, it should be noted that if you can assume causality you are better
> just fitting a univariate model with phenotype in as a predictor: you get
> an increase in precision for your assumption.
>
> Cheers,
>
> Jarrod
>
>
>
>
>
>
> On 26/10/16 08:46, Xav Harrison wrote:
>
>> Hi Folks
>>
>> A potentially rookie question here, but here goes.
>>
>> I'm using MCMCglmm to fit a bivariate response model where one response is
>> a Poisson count of pathogen load (Disease) and one is a Gaussian
>> phentotypic measure of the host (Phenotype).
>>
>> I've fit a model with fixed effects that one would expect to influence
>> each
>> of these predictors, and including a random effect of site, of the form:
>>
>> prior1<-list(R=list(V=diag(2), nu=3),G=list(G1=list(V=diag(2),nu=3,
>> alpha.mu
>> =c(0,0),alpha.V=diag(2)*1000)))
>>
>> MCMCglmm(cbind(Disease,Phenotype) ~ trait-1 +
>> trait:predictors,rcov=~us(trait):units,random=~us(trait):
>> site,family=c("poisson","gaussian"),prior=prior1,verbose=F,
>> nitt=42000,burnin=2000,thin=20)
>>
>> The golden egg here, and my hypothesis, is that after accounting for some
>> predictors there will be a negative posterior correlation between the
>> response traits, where the posterior correlation is calculated as:
>>
>> Cov(Disease,Phenotype) / sqrt(Var(Disease)*Var(Phenotype))
>>
>> My question is whether this correlation is relevant at the G structure
>> level (Site random effect) or at the R structure level, which I take to be
>> the residual variance at the observation (individual) level?
>>
>> I suspect the answer is the latter, but I'm struggling to interpret what
>> it
>> means if there is a negative correlation at the Site level? Does it mean
>> that the variances of the two traits at the site level are not
>> independent,
>> in that higher values in one trait for a site tend to produce lower values
>> for the other?
>>
>> Any help greatly appreciated.
>>
>> Cheers
>>
>> Xav
>>
>>
>>
>>
>
> --
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>
>


-- 
-------------------------------------------------------
Dr Xavier Harrison
Research Fellow
Institute of Zoology
Regent's Park
London NW1 4RY
www.zsl.org/xavierharrison
+44 (0) 207 449 6621

	[[alternative HTML version deleted]]


From wingyeechow.zoey at gmail.com  Wed Oct 26 16:13:44 2016
From: wingyeechow.zoey at gmail.com (Wing Yee Chow)
Date: Wed, 26 Oct 2016 15:13:44 +0100
Subject: [R-sig-ME] questions about parsimonious mixed models
Message-ID: <003301d22f93$29757ff0$7c607fd0$@gmail.com>

Hi there,

 

I tried to follow the suggestions in Bates et al.'s parsimonious mixed model
paper (link <https://arxiv.org/abs/1506.04967> ) and I'm having some
problems. Basically, I started with a maximal model (m0) and then a
zero-correlation-parameter model (m1), then I reduced the model by taking
out near-zero variance components (m2). Once I extended the reduced model
with correlation parameters (m3), the model becomes degenerate (some
correlation parameters are 1 or -1). However, the likelihood ratio test
(anova()) suggests that m3 provides a significantly better fit than m2. How
should I proceed? What would be the optimal model in cases like this? 

 

I'm not sure if I did something wrong in the process, so I'll detail what I
did below.

 

Thanks!

Wing-Yee Chow

 

==================================

 

My experiment has a 2x2 within-participant design with 24 participants and
48 items. Both factors (Congruity and SentenceType) are categorical and I
specified the contrasts this way:

> contrasts(tempdata$congruity) <- contr.sum(2)/2

> contrasts(tempdata$sentencetype) <- contr.sum(2)/2

 

I started with a maximal model, and rePCA() suggests that it's
over-parameterised:

> summary(m0 <- lmer(value ~ sentencetype * congruity + (sentencetype *
congruity|subj) + (sentencetype * congruity|item), REML=FALSE,

           data=tempdata))


Linear mixed model fit by maximum likelihood  ['lmerMod']

Formula: value ~ sentencetype * congruity + (sentencetype * congruity |
subj) + (sentencetype * congruity | item)

   Data: tempdata

 

     AIC      BIC   logLik deviance df.resid 

 14415.9  14540.2  -7182.9  14365.9     1045 

 

Scaled residuals: 

    Min      1Q  Median      3Q     Max 

-2.6228 -0.6275 -0.1785  0.4124  6.1246 

 

Random effects:

Groups   Name                     Variance Std.Dev. Corr             

 item     (Intercept)               5412.2   73.568                   

          sentencetype1             3316.3   57.588   0.92            

          congruity1                 866.0   29.429   0.58  0.85      

          sentencetype1:congruity1   138.1   11.752   0.51  0.15 -0.40

subj     (Intercept)              10789.8  103.874                   

          sentencetype1               56.2    7.497   1.00            

          congruity1                 835.1   28.898  -1.00 -1.00      

          sentencetype1:congruity1  2279.8   47.747   1.00  1.00 -1.00

Residual                          34511.5  185.773                   

Number of obs: 1070, groups:  item, 48; subj, 24

 

Fixed effects:

                         Estimate Std. Error t value

(Intercept)                388.84      24.39  15.944

sentencetype1               56.27      14.18   3.968

congruity1                 -18.19      13.51  -1.346

sentencetype1:congruity1   -20.90      24.82  -0.842

 

Correlation of Fixed Effects:

            (Intr) sntnc1 cngrt1

sentenctyp1  0.328              

congruity1  -0.300  0.109       

sntnctyp1:1  0.357  0.047 -0.186

 

 

>  summary(rePCA(m0))


$item

Importance of components:

                         [,1]    [,2]      [,3] [,4]

Standard deviation     0.5070 0.15787 1.228e-06    0

Proportion of Variance 0.9116 0.08838 0.000e+00    0

Cumulative Proportion  0.9116 1.00000 1.000e+00    1

 

$subj

Importance of components:

                        [,1]      [,2]      [,3] [,4]

Standard deviation     0.636 1.045e-06 1.895e-07    0

Proportion of Variance 1.000 0.000e+00 0.000e+00    0

Cumulative Proportion  1.000 1.000e+00 1.000e+00    1

 

 

Then, I converted the factors to numeric covariates using model.matrix() and
did the zero-correlation-parameter model:

 

> summary(m1 <- lmer(value ~ sentencetype * congruity + (cSenttype *
cCongruity||subj) + (cSenttype * cCongruity||item), REML=FALSE,

           data=tempdata))


Linear mixed model fit by maximum likelihood  ['lmerMod']

Formula: value ~ sentencetype * congruity + ((1 | subj) + (0 + cSenttype |  

    subj) + (0 + cCongruity | subj) + (0 + cSenttype:cCongruity |
subj)) + ((1 | item) + (0 + cSenttype | item) + (0 + cCongruity |  

    item) + (0 + cSenttype:cCongruity | item))

   Data: tempdata

 

     AIC      BIC   logLik deviance df.resid 

 14419.9  14484.5  -7196.9  14393.9     1057 

 

Scaled residuals: 

    Min      1Q  Median      3Q     Max 

-2.4709 -0.6110 -0.1823  0.4011  6.1156 

 

Random effects:

Groups   Name                 Variance  Std.Dev. 

 item     cSenttype:cCongruity 0.000e+00 0.000e+00

item.1   cCongruity           0.000e+00 0.000e+00

item.2   cSenttype            2.478e+03 4.978e+01

item.3   (Intercept)          5.268e+03 7.258e+01

subj     cSenttype:cCongruity 0.000e+00 0.000e+00

subj.1   cCongruity           3.473e+01 5.893e+00

subj.2   cSenttype            3.819e-11 6.180e-06

subj.3   (Intercept)          1.079e+04 1.039e+02

Residual                      3.543e+04 1.882e+02

Number of obs: 1070, groups:  item, 48; subj, 24

 

Fixed effects:

                         Estimate Std. Error t value

(Intercept)                388.41      24.34  15.957

sentencetype1               57.23      13.59   4.210

congruity1                 -17.72      11.60  -1.527

sentencetype1:congruity1   -20.81      23.07  -0.902

 

Correlation of Fixed Effects:

            (Intr) sntnc1 cngrt1

sentenctyp1 -0.002              

congruity1   0.000 -0.001       

sntnctyp1:1  0.000 -0.002 -0.009

 

 

Once again rePCA() suggests that the model is over-parameterised:

> summary(rePCA(m1))


$item

Importance of components:

                         [,1]   [,2] [,3] [,4]

Standard deviation     0.3856 0.2645    0    0

Proportion of Variance 0.6801 0.3200    0    0

Cumulative Proportion  0.6801 1.0000    1    1

 

$subj

Importance of components:

                         [,1]    [,2]      [,3] [,4]

Standard deviation     0.5518 0.03131 3.283e-08    0

Proportion of Variance 0.9968 0.00321 0.000e+00    0

Cumulative Proportion  0.9968 1.00000 1.000e+00    1

 

So then I reduced the model by taking out 4 variance components that are
zero/near-zero, and rePCA suggests that this reduced model is no longer
over-parameterised:

 

> summary(m2<-lmer(value ~ sentencetype * congruity + (cCongruity||subj) +
(cSenttype||item),REML=FALSE, data=tempdata))


Linear mixed model fit by maximum likelihood  ['lmerMod']

Formula: value ~ sentencetype * congruity + ((1 | subj) + (0 + cCongruity |
subj)) + ((1 | item) + (0 + cSenttype | item))

   Data: tempdata

 

     AIC      BIC   logLik deviance df.resid 

 14411.9  14456.6  -7196.9  14393.9     1061 

 

Scaled residuals: 

    Min      1Q  Median      3Q     Max 

-2.4709 -0.6110 -0.1823  0.4011  6.1156 

 

Random effects:

Groups   Name        Variance Std.Dev.

item     cSenttype    2478.42  49.784 

 item.1   (Intercept)  5267.94  72.581 

 subj     cCongruity     34.73   5.894 

 subj.1   (Intercept) 10785.48 103.853 

 Residual             35428.11 188.224 

Number of obs: 1070, groups:  item, 48; subj, 24

 

Fixed effects:

                         Estimate Std. Error t value

(Intercept)                388.41      24.34  15.957

sentencetype1               57.23      13.59   4.210

congruity1                 -17.72      11.60  -1.527

sentencetype1:congruity1   -20.81      23.07  -0.902

 

Correlation of Fixed Effects:

            (Intr) sntnc1 cngrt1

sentenctyp1 -0.002              

congruity1   0.000 -0.001       

sntnctyp1:1  0.000 -0.002 -0.009

 

 

> summary(rePCA(m2))


$item

Importance of components:

                         [,1]   [,2]

Standard deviation     0.3856 0.2645

Proportion of Variance 0.6801 0.3200

Cumulative Proportion  0.6801 1.0000

 

$subj

Importance of components:

                         [,1]    [,2]

Standard deviation     0.5518 0.03131

Proportion of Variance 0.9968 0.00321

Cumulative Proportion  0.9968 1.00000

 

Then I followed Bates et al's guidelines in extending this reduced model
with correlation parameters:

> summary(m3<-lmer(value ~ sentencetype * congruity + (cCongruity|subj) +
(cSenttype|item),REML=FALSE, data=tempdata))


Linear mixed model fit by maximum likelihood  ['lmerMod']

Formula: value ~ sentencetype * congruity + (cCongruity | subj) + (cSenttype
|      item)

   Data: tempdata

 

     AIC      BIC   logLik deviance df.resid 

 14395.2  14450.0  -7186.6  14373.2     1059 

 

Scaled residuals: 

    Min      1Q  Median      3Q     Max 

-2.6509 -0.6383 -0.1679  0.4151  5.9618 

 

Random effects:

Groups   Name        Variance Std.Dev. Corr 

 item     (Intercept)  5391.4   73.43        

          cSenttype    3085.8   55.55   1.00 

 subj     (Intercept) 10803.3  103.94        

          cCongruity    826.1   28.74   -1.00

Residual             35022.7  187.14        

Number of obs: 1070, groups:  item, 48; subj, 24

 

Fixed effects:

                         Estimate Std. Error t value

(Intercept)                388.76      24.40  15.933

sentencetype1               56.88      13.99   4.066

congruity1                 -18.10      12.88  -1.405

sentencetype1:congruity1   -20.94      22.93  -0.913

 

Correlation of Fixed Effects:

            (Intr) sntnc1 cngrt1

sentenctyp1  0.247              

congruity1  -0.396 -0.001       

sntnctyp1:1  0.000 -0.002 -0.007

 

The correlation parameters of 1 and -1 suggest that this model is
degenerate. This is consistent with the results of rePCA():

 

> summary(rePCA(m3))


$item

Importance of components:

                        [,1] [,2]

Standard deviation     0.492    0

Proportion of Variance 1.000    0

Cumulative Proportion  1.000    1

 

$subj

Importance of components:

                         [,1]      [,2]

Standard deviation     0.5762 1.678e-07

Proportion of Variance 1.0000 0.000e+00

Cumulative Proportion  1.0000 1.000e+00

 

However, the likelihood ration test suggests that m3 provides a
significantly better fit than m2:

> anova(m2, m3)


Data: tempdata

Models:

m2: value ~ sentencetype * congruity + ((1 | subj) + (0 + cCongruity | 

m2:     subj)) + ((1 | item) + (0 + cSenttype | item))

m3: value ~ sentencetype * congruity + (cCongruity | subj) + (cSenttype | 

m3:     item)

   Df   AIC   BIC  logLik deviance  Chisq Chi Df Pr(>Chisq)    

m2  9 14412 14457 -7196.9    14394                             

m3 11 14395 14450 -7186.6    14373 20.639      2  3.298e-05 ***

---

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

 

Questions: Should I have reduced the model further before extending it with
correlation parameters? How should I proceed (or what should I have done
differently)? 


	[[alternative HTML version deleted]]


From n.pubblic at gmail.com  Wed Oct 26 18:16:48 2016
From: n.pubblic at gmail.com (mio nome)
Date: Wed, 26 Oct 2016 12:16:48 -0400
Subject: [R-sig-ME] Differences in standard and mixed effects logistic
 regression - rounding error or conceptual mistake?
Message-ID: <CAE1jcjDzcAymsCiuPmkAxF9V2rVZJ88kqirnDnAXrkoA+zXSJA@mail.gmail.com>

I'm a bit confused. To my understanding, the standard logistic regression
should be equivalent to a mixed effect logistic regression where the
statistical unit is defined as random effect - but I found the results of
the two analysis to be different.

As example, let's say that I have an experiment with 20 participants. To
each participant I ask x questions, and their answer is scored either as
either "right" or "wrong".

I simulated the dataset with:


*data0 = cbind(data.frame(matrix(sample(1:10, 40, replace=TRUE), nrow = 20,
byrow = TRUE)), rep(c('a', 'b'), each=10))names(data0) = c('s','f','c')*

Let's say that the column *s* has the number of right answers, the column
*f* has the number of wrong answer, and the column *c* represents a
between-subject experimental condition. The subjects are organized per rows.

To test the effect of *c* I can write:

*anova(glm ( cbind( s, f) ~ 1, data = data0, family = binomial), glm (
cbind( s, f) ~ 1 + c, data = data0, family = binomial), test="Chisq")*

which gives me:





*Model 1: cbind(s, f) ~ 1Model 2: cbind(s, f) ~ 1 + c  Resid. Df Resid. Dev
Df Deviance Pr(>Chi)1        19     49.1442        18     42.978  1
6.1659  0.01302 **

So I tried to model the same data with a logistic mixed effect model.

First I converted it to a long format with:











*data1=NULLfor (i in 1 : nrow( data0) ) {tt = c(rep(1, data0$s[i]), rep(0,
data0$f[i]))t = cbind(tt, rep(i, length(tt)), rep(data0$c[i],
length(tt)))data1 = rbind( data1, t)}data1 = data.frame(data1)names(data1)
= c("r","s", "c")data1$s = factor(data1$s)data1$c = factor(data1$c)*
Where *s* is the subject ID. Then I tested the effect of *c* with:


*library(lme4)anova(glmer( r ~ 1 + ( 1| s), data = data1, family =
binomial), glmer( r ~ 1 + c + ( 1| s), data = data1, family = binomial),
test="Chisq")*

Now the same analysis gives me a different result!














*glmer(r ~ 1 + (1 | s), data = data1, family = binomial): r ~ 1 + (1 |
s)glmer(r ~ 1 + c + (1 | s), data = data1, family = binomial): r ~ 1 + c +
(1 | s)                                                            Df
 AIC    BICglmer(r ~ 1 + (1 | s), data = data1, family = binomial)      2
290.72 297.45glmer(r ~ 1 + c + (1 | s), data = data1, family = binomial)  3
289.97 300.07
 logLik devianceglmer(r ~ 1 + (1 | s), data = data1, family = binomial)
-143.36   286.72glmer(r ~ 1 + c + (1 | s), data = data1, family = binomial)
-141.98   283.97
 Chisq Chi Dfglmer(r ~ 1 + (1 | s), data = data1, family = binomial)glmer(r
~ 1 + c + (1 | s), data = data1, family = binomial) 2.7548      1
                                                  Pr(>Chisq)glmer(r ~ 1 +
(1 | s), data = data1, family = binomial)glmer(r ~ 1 + c + (1 | s), data =
data1, family = binomial)    0.09696 .*

Is it because of rounding errors in the implementation of the mathematical
functions, or there is some fundamental concept I'm missing here?

PS: the results of the two analysis can be closer that in the case I
reported - I cherry picked an extreme example for the sake of a clear
explanation.

	[[alternative HTML version deleted]]


From reinhold.kliegl at gmail.com  Wed Oct 26 19:03:45 2016
From: reinhold.kliegl at gmail.com (Reinhold Kliegl)
Date: Wed, 26 Oct 2016 19:03:45 +0200
Subject: [R-sig-ME] questions about parsimonious mixed models
In-Reply-To: <003301d22f93$29757ff0$7c607fd0$@gmail.com>
References: <003301d22f93$29757ff0$7c607fd0$@gmail.com>
Message-ID: <CAG+WrEyjBgw1AoCMwfvU8tWA-ZoCFG8T-Zus3_2wxLt4u-QiUQ@mail.gmail.com>

In my opinion, LRTs are only meaningful for the comparison of
non-degenerate models. Thus, if extending the reduced model with
correlation parameters leads to overparameterization, I would stay with the
reduced model.

This is a quick response. I did not look at your example in detail.

On 26 Oct 2016, at 16:13, Wing Yee Chow <wingyeechow.zoey at gmail.com> wrote:

Hi there,



I tried to follow the suggestions in Bates et al.'s parsimonious mixed model
paper (link <https://arxiv.org/abs/1506.04967> ) and I'm having some
problems. Basically, I started with a maximal model (m0) and then a
zero-correlation-parameter model (m1), then I reduced the model by taking
out near-zero variance components (m2). Once I extended the reduced model
with correlation parameters (m3), the model becomes degenerate (some
correlation parameters are 1 or -1). However, the likelihood ratio test
(anova()) suggests that m3 provides a significantly better fit than m2. How
should I proceed? What would be the optimal model in cases like this?



I'm not sure if I did something wrong in the process, so I'll detail what I
did below.



Thanks!

Wing-Yee Chow



==================================



My experiment has a 2x2 within-participant design with 24 participants and
48 items. Both factors (Congruity and SentenceType) are categorical and I
specified the contrasts this way:

contrasts(tempdata$congruity) <- contr.sum(2)/2


contrasts(tempdata$sentencetype) <- contr.sum(2)/2




I started with a maximal model, and rePCA() suggests that it's
over-parameterised:

summary(m0 <- lmer(value ~ sentencetype * congruity + (sentencetype *

congruity|subj) + (sentencetype * congruity|item), REML=FALSE,

          data=tempdata))


Linear mixed model fit by maximum likelihood  ['lmerMod']

Formula: value ~ sentencetype * congruity + (sentencetype * congruity |
subj) + (sentencetype * congruity | item)

  Data: tempdata



    AIC      BIC   logLik deviance df.resid

14415.9  14540.2  -7182.9  14365.9     1045



Scaled residuals:

   Min      1Q  Median      3Q     Max

-2.6228 -0.6275 -0.1785  0.4124  6.1246



Random effects:

Groups   Name                     Variance Std.Dev. Corr

item     (Intercept)               5412.2   73.568

         sentencetype1             3316.3   57.588   0.92

         congruity1                 866.0   29.429   0.58  0.85

         sentencetype1:congruity1   138.1   11.752   0.51  0.15 -0.40

subj     (Intercept)              10789.8  103.874

         sentencetype1               56.2    7.497   1.00

         congruity1                 835.1   28.898  -1.00 -1.00

         sentencetype1:congruity1  2279.8   47.747   1.00  1.00 -1.00

Residual                          34511.5  185.773

Number of obs: 1070, groups:  item, 48; subj, 24



Fixed effects:

                        Estimate Std. Error t value

(Intercept)                388.84      24.39  15.944

sentencetype1               56.27      14.18   3.968

congruity1                 -18.19      13.51  -1.346

sentencetype1:congruity1   -20.90      24.82  -0.842



Correlation of Fixed Effects:

           (Intr) sntnc1 cngrt1

sentenctyp1  0.328

congruity1  -0.300  0.109

sntnctyp1:1  0.357  0.047 -0.186





summary(rePCA(m0))



$item

Importance of components:

                        [,1]    [,2]      [,3] [,4]

Standard deviation     0.5070 0.15787 1.228e-06    0

Proportion of Variance 0.9116 0.08838 0.000e+00    0

Cumulative Proportion  0.9116 1.00000 1.000e+00    1



$subj

Importance of components:

                       [,1]      [,2]      [,3] [,4]

Standard deviation     0.636 1.045e-06 1.895e-07    0

Proportion of Variance 1.000 0.000e+00 0.000e+00    0

Cumulative Proportion  1.000 1.000e+00 1.000e+00    1





Then, I converted the factors to numeric covariates using model.matrix() and
did the zero-correlation-parameter model:



summary(m1 <- lmer(value ~ sentencetype * congruity + (cSenttype *

cCongruity||subj) + (cSenttype * cCongruity||item), REML=FALSE,

          data=tempdata))


Linear mixed model fit by maximum likelihood  ['lmerMod']

Formula: value ~ sentencetype * congruity + ((1 | subj) + (0 + cSenttype |

   subj) + (0 + cCongruity | subj) + (0 + cSenttype:cCongruity |
subj)) + ((1 | item) + (0 + cSenttype | item) + (0 + cCongruity |

   item) + (0 + cSenttype:cCongruity | item))

  Data: tempdata



    AIC      BIC   logLik deviance df.resid

14419.9  14484.5  -7196.9  14393.9     1057



Scaled residuals:

   Min      1Q  Median      3Q     Max

-2.4709 -0.6110 -0.1823  0.4011  6.1156



Random effects:

Groups   Name                 Variance  Std.Dev.

item     cSenttype:cCongruity 0.000e+00 0.000e+00

item.1   cCongruity           0.000e+00 0.000e+00

item.2   cSenttype            2.478e+03 4.978e+01

item.3   (Intercept)          5.268e+03 7.258e+01

subj     cSenttype:cCongruity 0.000e+00 0.000e+00

subj.1   cCongruity           3.473e+01 5.893e+00

subj.2   cSenttype            3.819e-11 6.180e-06

subj.3   (Intercept)          1.079e+04 1.039e+02

Residual                      3.543e+04 1.882e+02

Number of obs: 1070, groups:  item, 48; subj, 24



Fixed effects:

                        Estimate Std. Error t value

(Intercept)                388.41      24.34  15.957

sentencetype1               57.23      13.59   4.210

congruity1                 -17.72      11.60  -1.527

sentencetype1:congruity1   -20.81      23.07  -0.902



Correlation of Fixed Effects:

           (Intr) sntnc1 cngrt1

sentenctyp1 -0.002

congruity1   0.000 -0.001

sntnctyp1:1  0.000 -0.002 -0.009





Once again rePCA() suggests that the model is over-parameterised:

summary(rePCA(m1))



$item

Importance of components:

                        [,1]   [,2] [,3] [,4]

Standard deviation     0.3856 0.2645    0    0

Proportion of Variance 0.6801 0.3200    0    0

Cumulative Proportion  0.6801 1.0000    1    1



$subj

Importance of components:

                        [,1]    [,2]      [,3] [,4]

Standard deviation     0.5518 0.03131 3.283e-08    0

Proportion of Variance 0.9968 0.00321 0.000e+00    0

Cumulative Proportion  0.9968 1.00000 1.000e+00    1



So then I reduced the model by taking out 4 variance components that are
zero/near-zero, and rePCA suggests that this reduced model is no longer
over-parameterised:



summary(m2<-lmer(value ~ sentencetype * congruity + (cCongruity||subj) +

(cSenttype||item),REML=FALSE, data=tempdata))


Linear mixed model fit by maximum likelihood  ['lmerMod']

Formula: value ~ sentencetype * congruity + ((1 | subj) + (0 + cCongruity |
subj)) + ((1 | item) + (0 + cSenttype | item))

  Data: tempdata



    AIC      BIC   logLik deviance df.resid

14411.9  14456.6  -7196.9  14393.9     1061



Scaled residuals:

   Min      1Q  Median      3Q     Max

-2.4709 -0.6110 -0.1823  0.4011  6.1156



Random effects:

Groups   Name        Variance Std.Dev.

item     cSenttype    2478.42  49.784

item.1   (Intercept)  5267.94  72.581

subj     cCongruity     34.73   5.894

subj.1   (Intercept) 10785.48 103.853

Residual             35428.11 188.224

Number of obs: 1070, groups:  item, 48; subj, 24



Fixed effects:

                        Estimate Std. Error t value

(Intercept)                388.41      24.34  15.957

sentencetype1               57.23      13.59   4.210

congruity1                 -17.72      11.60  -1.527

sentencetype1:congruity1   -20.81      23.07  -0.902



Correlation of Fixed Effects:

           (Intr) sntnc1 cngrt1

sentenctyp1 -0.002

congruity1   0.000 -0.001

sntnctyp1:1  0.000 -0.002 -0.009





summary(rePCA(m2))



$item

Importance of components:

                        [,1]   [,2]

Standard deviation     0.3856 0.2645

Proportion of Variance 0.6801 0.3200

Cumulative Proportion  0.6801 1.0000



$subj

Importance of components:

                        [,1]    [,2]

Standard deviation     0.5518 0.03131

Proportion of Variance 0.9968 0.00321

Cumulative Proportion  0.9968 1.00000



Then I followed Bates et al's guidelines in extending this reduced model
with correlation parameters:

summary(m3<-lmer(value ~ sentencetype * congruity + (cCongruity|subj) +

(cSenttype|item),REML=FALSE, data=tempdata))


Linear mixed model fit by maximum likelihood  ['lmerMod']

Formula: value ~ sentencetype * congruity + (cCongruity | subj) + (cSenttype
|      item)

  Data: tempdata



    AIC      BIC   logLik deviance df.resid

14395.2  14450.0  -7186.6  14373.2     1059



Scaled residuals:

   Min      1Q  Median      3Q     Max

-2.6509 -0.6383 -0.1679  0.4151  5.9618



Random effects:

Groups   Name        Variance Std.Dev. Corr

item     (Intercept)  5391.4   73.43

         cSenttype    3085.8   55.55   1.00

subj     (Intercept) 10803.3  103.94

         cCongruity    826.1   28.74   -1.00

Residual             35022.7  187.14

Number of obs: 1070, groups:  item, 48; subj, 24



Fixed effects:

                        Estimate Std. Error t value

(Intercept)                388.76      24.40  15.933

sentencetype1               56.88      13.99   4.066

congruity1                 -18.10      12.88  -1.405

sentencetype1:congruity1   -20.94      22.93  -0.913



Correlation of Fixed Effects:

           (Intr) sntnc1 cngrt1

sentenctyp1  0.247

congruity1  -0.396 -0.001

sntnctyp1:1  0.000 -0.002 -0.007



The correlation parameters of 1 and -1 suggest that this model is
degenerate. This is consistent with the results of rePCA():



summary(rePCA(m3))



$item

Importance of components:

                       [,1] [,2]

Standard deviation     0.492    0

Proportion of Variance 1.000    0

Cumulative Proportion  1.000    1



$subj

Importance of components:

                        [,1]      [,2]

Standard deviation     0.5762 1.678e-07

Proportion of Variance 1.0000 0.000e+00

Cumulative Proportion  1.0000 1.000e+00



However, the likelihood ration test suggests that m3 provides a
significantly better fit than m2:

anova(m2, m3)



Data: tempdata

Models:

m2: value ~ sentencetype * congruity + ((1 | subj) + (0 + cCongruity |

m2:     subj)) + ((1 | item) + (0 + cSenttype | item))

m3: value ~ sentencetype * congruity + (cCongruity | subj) + (cSenttype |

m3:     item)

  Df   AIC   BIC  logLik deviance  Chisq Chi Df Pr(>Chisq)

m2  9 14412 14457 -7196.9    14394

m3 11 14395 14450 -7186.6    14373 20.639      2  3.298e-05 ***

---

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1



Questions: Should I have reduced the model further before extending it with
correlation parameters? How should I proceed (or what should I have done
differently)?


[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From alison.fairbrass.10 at ucl.ac.uk  Tue Nov  1 10:14:58 2016
From: alison.fairbrass.10 at ucl.ac.uk (Alison Fairbrass)
Date: Tue, 1 Nov 2016 09:14:58 +0000
Subject: [R-sig-ME] Using as.formula with glmmADMB
Message-ID: <CADSCVYefK9GBgy=qOj=EDf0Jwd3ZUGF0O5XABgncUY2knUxteQ@mail.gmail.com>

Dear all,

I am using as.formula to create a number of models with the same main
effects and different interaction terms using the following code:

mainEffects <- "poly(LC1,2) + poly(LC2,2) + poly(LC3,2) + poly(LC4,2) +
poly(LC5,2) +"

interactionTerms <- c("poly(LC1,2):poly(LC2,2)", "poly(LC1,2):poly(LC3,2)",
"poly(LC1,2):poly(LC4,2)", "poly(LC1,2):poly(LC5,2)")

loop<-0

for (interactionTerm in interactionTerms){

  newformula <- as.formula(paste('SPR ~', mainEffects, interactionTerm,'+
(1|Site)'))

  loop<-loop+1

  assign(paste("M", loop, sep=""), glmer(newformula, family=poisson, data =
dat))

}

This works fine when fitting models using glmer. However, when assigning
the model using glmmADMB (e.g. assign(paste("M", loop, sep=""),
glmmadmb(newformula, zeroInflation=TRUE, family="nbinom", data=dat)) I
receive the following error message:

Error in f[2] : object of type 'symbol' is not subsettable

Does glmmADMB not work with as.formula?

I am using glmmADMB v.0.7 with R v.3.1.2.

Many thanks in advance for your assistance,

-- 

Alison

------
Alison Fairbrass
Engineering Doctorate
University College London
Civil, Environmental and Geomatic Engineering
+447912616367

Twitter: @AlisonFairbrass

Webpage: https://www.ucl.ac.uk/cber/people/alison-fairbrass

alison.fairbrass.10 at ucl.ac.uk
alison.fairbrass at gmail.com

	[[alternative HTML version deleted]]


From jmbatchou at uchicago.edu  Tue Nov  1 19:27:03 2016
From: jmbatchou at uchicago.edu (Joelle Mbatchou)
Date: Tue, 1 Nov 2016 18:27:03 +0000
Subject: [R-sig-ME] Question about the predict function in MCMCglmm
Message-ID: <24D8F4EBBCB6E34AB77AEDAC4DBA25E0418AF307@xm-mbx-05-prod>

Hi all,

I am a confused about what the predict function from MCMCglmm actually does to get the "predicted probabilities" in the context of a logistic mixed model.

More precisely, I have a binary response Y and logit(E(Y|u)) = L =  Xb + u
where X is a matrix of covariates and u~N(0, sigma_u^2 * V) where V is known.

Since the response is binary, the residual term is fixed at 1. I'd fit this model in R as:
idU <- 1:length(Y)
prior1 <- list(R = list(V = 1, fix=1), G = list(G1 = list(V = 1, nu = 1, alpha.mu=0, alpha.V=5^2)))
mMCMC <- MCMCglmm(Y ~ 1 + Z, random =~idU, family = "categorical", ginverse = list(idU = inverse_V), prior=prior1, pr = TRUE, verbose = F)

Also in mMCMC$Sol and mMCMC$VCV, the samples from the joint posterior distribution of p(b,u,sigma_u^2| y) are stored? And so to approximate the marginal posterior distributions (i.e. p(b|y), p(u|y) and p(sigma_u^2|y)) we could just plot histograms of each quantity separately (for example plot(mMCMC$VCV[,1]) for sigma_u^2)?

So in the context of getting posterior predictions for the probability that Y=1 with the predict function, I am confused about what quantities are given by:

pred1 <- predict(mMCMC, marginal = ~idU, type = "response")

vs.

pred2 <- predict(mMCMC, marginal = NULL, posterior = "mean", type = "response")

More specifically, I don't understand what they represent (and how they are obtained). I thought for pred1 'Xb/sqrt(1+k^2*sigma^2)' was computed for each MCMC iteration (eqn 2.14 in CourseNotes) to approximate taking the expectation of E(Y|b, u, sigma^2) over the random effects u and then the average was taken over the MCMC iterations for b and sigma^2; however, when I do this 'by hand', I get different results. Also for pred2, I thought it was giving the predicted probabilities P(Y=1|b,u,sigma^2) so

> beta_hat <- as.matrix(mMCMC$Sol[,1:2])
> X <- as.matrix(mMCMC$X)
> k <- ((16*sqrt(3))/(15*pi))^2
> pred1_hand <- colMeans(plogis(tcrossprod(beta_hat, X) / sqrt(1 + k^2 * rowSums(mMCMC$VCV))))
> U <- as.matrix(mMCMC$Sol[,-(1:2)])
> pred2_hand <- colMeans(plogis(tcrossprod(beta_hat, X) + U))

> head(cbind(pred1, pred2, pred1_hand, pred2_hand))                                                         pred1_hand pred2_hand
1 0.08467802 0.21552374 0.03564130  0.1448464
2 0.23766175 0.17228113 0.15399697  0.2798503
3 0.19338132 0.07845114 0.11215077  0.4200217
4 0.15685811 0.10811526 0.08232281  0.1037176
5 0.14048409 0.35230597 0.07032301  0.2009921
6 0.17927898 0.09011039 0.10013013  0.1568070

Any clarification on this would be greatly appreciated. Thank you!

Cheers,
Joelle

	[[alternative HTML version deleted]]


From dmccabe at broadinstitute.org  Wed Nov  2 16:21:05 2016
From: dmccabe at broadinstitute.org (Devin McCabe)
Date: Wed, 2 Nov 2016 11:21:05 -0400
Subject: [R-sig-ME] Modeling crossed/non-nested effects for a binomial
	probability
Message-ID: <CANtcQZZ-bNnqR_OXmRg3sJRg4LJnXmsQ3nAto5WZU9kF-Exnqg@mail.gmail.com>

I have a data set consisting of success/failure counts for samples
belonging to two independent grouping variables x1 and x2, i.e.

    x1   x2    n   success   failure
1   A1   B1   10         4         6
2   A1   B2   20        10        10
3   A1   B3   15         6         9
4   A2   B1   12         6         6
5   A2   B2   20        12         8
   ...  ...  ...       ...       ...

I'd like to model the fact that the probability of success, p, for each
observation is 0.5, but that this probability is independently skewed on
the x1-level by s1 and on the x2-level by s2.

This could be a multiplicative effect (p_ij = 0.5 * s1_i * s2_j) or an
additive one (p_ij = 0.5 + s1_i + s2_j). The objects of my interest are the
independent skew vectors s1 and s2.

Is it possible to do this in with glmer or some other package? I'm
currently doing the following and getting accurate estimates with simulated
data, but I'm unsure of its statistical or computational correctness:

library(lme4)
library(boot)

mod <- glmer(cbind(success, failure) ~ 0 + (1 | x1) + (1 | x2),
             data = d, family = "binomial")

coefs <- coef(mod)
s1 <- inv.logit(coefs$x1$`(Intercept)`) - 0.5
s2 <- inv.logit(coefs$x2$`(Intercept)`) - 0.5

Thanks for your help.

	[[alternative HTML version deleted]]


From j.hadfield at ed.ac.uk  Wed Nov  2 17:07:56 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Wed, 2 Nov 2016 16:07:56 +0000
Subject: [R-sig-ME] Question about the predict function in MCMCglmm
In-Reply-To: <24D8F4EBBCB6E34AB77AEDAC4DBA25E0418AF307@xm-mbx-05-prod>
References: <24D8F4EBBCB6E34AB77AEDAC4DBA25E0418AF307@xm-mbx-05-prod>
Message-ID: <631c2019-75fa-269c-8165-9f740f93244c@ed.ac.uk>

Dear Joelle,

Your intuition about the different predict functions is correct. Perhaps 
the discrepancy is that the diagonal elements of your user inverse V 
does not have one along the diagonals? If not you should use:

mMCMC$VCV[,"units"]+mMCMC$VCV[,"idU"]*diag(V)

rather than:

rwoSums(mMCMC$VCV)

If this does or does not fix the problem can you let me know.

Cheers,

Jarrod


On 01/11/2016 18:27, Joelle Mbatchou wrote:
> Hi all,
>
> I am a confused about what the predict function from MCMCglmm actually does to get the "predicted probabilities" in the context of a logistic mixed model.
>
> More precisely, I have a binary response Y and logit(E(Y|u)) = L =  Xb + u
> where X is a matrix of covariates and u~N(0, sigma_u^2 * V) where V is known.
>
> Since the response is binary, the residual term is fixed at 1. I'd fit this model in R as:
> idU <- 1:length(Y)
> prior1 <- list(R = list(V = 1, fix=1), G = list(G1 = list(V = 1, nu = 1, alpha.mu=0, alpha.V=5^2)))
> mMCMC <- MCMCglmm(Y ~ 1 + Z, random =~idU, family = "categorical", ginverse = list(idU = inverse_V), prior=prior1, pr = TRUE, verbose = F)
>
> Also in mMCMC$Sol and mMCMC$VCV, the samples from the joint posterior distribution of p(b,u,sigma_u^2| y) are stored? And so to approximate the marginal posterior distributions (i.e. p(b|y), p(u|y) and p(sigma_u^2|y)) we could just plot histograms of each quantity separately (for example plot(mMCMC$VCV[,1]) for sigma_u^2)?
>
> So in the context of getting posterior predictions for the probability that Y=1 with the predict function, I am confused about what quantities are given by:
>
> pred1 <- predict(mMCMC, marginal = ~idU, type = "response")
>
> vs.
>
> pred2 <- predict(mMCMC, marginal = NULL, posterior = "mean", type = "response")
>
> More specifically, I don't understand what they represent (and how they are obtained). I thought for pred1 'Xb/sqrt(1+k^2*sigma^2)' was computed for each MCMC iteration (eqn 2.14 in CourseNotes) to approximate taking the expectation of E(Y|b, u, sigma^2) over the random effects u and then the average was taken over the MCMC iterations for b and sigma^2; however, when I do this 'by hand', I get different results. Also for pred2, I thought it was giving the predicted probabilities P(Y=1|b,u,sigma^2) so
>
>> beta_hat <- as.matrix(mMCMC$Sol[,1:2])
>> X <- as.matrix(mMCMC$X)
>> k <- ((16*sqrt(3))/(15*pi))^2
>> pred1_hand <- colMeans(plogis(tcrossprod(beta_hat, X) / sqrt(1 + k^2 * rowSums(mMCMC$VCV))))
>> U <- as.matrix(mMCMC$Sol[,-(1:2)])
>> pred2_hand <- colMeans(plogis(tcrossprod(beta_hat, X) + U))
>> head(cbind(pred1, pred2, pred1_hand, pred2_hand))                                                         pred1_hand pred2_hand
> 1 0.08467802 0.21552374 0.03564130  0.1448464
> 2 0.23766175 0.17228113 0.15399697  0.2798503
> 3 0.19338132 0.07845114 0.11215077  0.4200217
> 4 0.15685811 0.10811526 0.08232281  0.1037176
> 5 0.14048409 0.35230597 0.07032301  0.2009921
> 6 0.17927898 0.09011039 0.10013013  0.1568070
>
> Any clarification on this would be greatly appreciated. Thank you!
>
> Cheers,
> Joelle
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From j.hadfield at ed.ac.uk  Wed Nov  2 17:27:00 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Wed, 2 Nov 2016 16:27:00 +0000
Subject: [R-sig-ME] Question about the predict function in MCMCglmm
In-Reply-To: <631c2019-75fa-269c-8165-9f740f93244c@ed.ac.uk>
References: <24D8F4EBBCB6E34AB77AEDAC4DBA25E0418AF307@xm-mbx-05-prod>
	<631c2019-75fa-269c-8165-9f740f93244c@ed.ac.uk>
Message-ID: <76869547-e81a-d42d-050c-7b7f4feb632c@ed.ac.uk>

Hi,

Also

tcrossprod(beta_hat, X) + U

should be

(tcrossprod(beta_hat, X) + U)/sqrt(1 + k^2 * rowSums(mMCMC$VCV[,"units"]))

for pred2_hand

Cheers,

Jarrod



On 02/11/2016 16:07, Jarrod Hadfield wrote:
> tcrossprod(beta_hat, X) + U

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20161102/05af1b92/attachment-0001.pl>

From jaidoo at ualberta.ca  Wed Nov  2 19:37:35 2016
From: jaidoo at ualberta.ca (Joseph Aidoo)
Date: Wed, 2 Nov 2016 12:37:35 -0600
Subject: [R-sig-ME] Help with Linear Model
Message-ID: <CAH8i2pORE-n8MkqaT1Mn_L1e5BKAAsxXTkMFbgjWQo1sumpg1g@mail.gmail.com>

My experiment was set up as a RCBD with
5 oat varieties
4 nitrogen rates
at 2 sites
over 3 years .

I am investigating the effect of Variety and Nitrogen on Yield.

In the second year of the experiment we had a drought so the data was
awful. So i expect differences in Years

I am using Variety, Nitrogen and Year as my fixed effects.
and Sites as my random effect.

I expect and interaction between Variety X Nitrogen.
This is the model i came up with

Yield ~ nitrogen*variety*Year + (Year|Site/block)

My data doesnt seem normal because of the odd year with the drought.
I have tried to normally transform the data but nothing seems to work.

I need advice on what to do next? Is it ideal to use a glmm? What could the
best model be?

Thanks

	[[alternative HTML version deleted]]


From jmbatchou at uchicago.edu  Thu Nov  3 00:19:48 2016
From: jmbatchou at uchicago.edu (Joelle Mbatchou)
Date: Wed, 2 Nov 2016 23:19:48 +0000
Subject: [R-sig-ME] Question about the predict function in MCMCglmm
In-Reply-To: <76869547-e81a-d42d-050c-7b7f4feb632c@ed.ac.uk>
References: <24D8F4EBBCB6E34AB77AEDAC4DBA25E0418AF307@xm-mbx-05-prod>
	<631c2019-75fa-269c-8165-9f740f93244c@ed.ac.uk>,
	<76869547-e81a-d42d-050c-7b7f4feb632c@ed.ac.uk>
Message-ID: <24D8F4EBBCB6E34AB77AEDAC4DBA25E0418AF4C9@xm-mbx-05-prod>

Hi Jarrod,

Thanks for the quick reply! The V matrix I provide has 1 on the diagonal; I implemented what you suggested:

> pred1 <- predict(mMCMC, marginal = ~idU, type = "response")
> pred2 <- predict(mMCMC, marginal = NULL, posterior = "mean", type = "response")
> pred3 <- predict(mMCMC, marginal = NULL, posterior = "all", type = "response")

> beta_hat <- as.matrix(mMCMC$Sol[,1:2])
> beta_hat_mean <- colMeans(beta_hat)  # Posterior mean for beta
> U <- as.matrix(mMCMC$Sol[,-(1:2)])
> U_mean <- colMeans(U)  # Posterior mean for random effects u
> X <- as.matrix(mMCMC$X)
> k <- ((16*sqrt(3))/(15*pi))^2
> sig_sq <- mMCMC$VCV[,"units"] + tcrossprod(mMCMC$VCV[,"idU"], diag(V))  #Variance of latent variables for each MCMC iteration

> pred1_hand <- colMeans(plogis(tcrossprod(beta_hat, X) / sqrt(1 + k^2 * sig_sq)))
> pred2_hand <- plogis( (tcrossprod(beta_hat_mean, X) + U_mean)/sqrt(1 + k^2 * mean(mMCMC$VCV[,"units"])) )
> pred3_hand <- colMeans(plogis( (tcrossprod(beta_hat, X) + U)/sqrt(1 + k^2 * mMCMC$VCV[,"units"]) ))

> head(cbind(pred1, pred1_hand, pred2, pred2_hand, pred3, pred3_hand))
       pred1 pred1_hand      pred2 pred2_hand     pred3 pred3_hand
1 0.08467802  0.1772529 0.21552374 0.11490073 0.1622893  0.1869202
2 0.23766175  0.3115590 0.17228113 0.24496249 0.2938466  0.3114496
3 0.19338132  0.2764658 0.07845114 0.43098835 0.4278642  0.4364018
4 0.15685811  0.2458417 0.10811526 0.06985046 0.1201754  0.1443062
5 0.14048409  0.2314295 0.35230597 0.17401086 0.2186460  0.2418515
6 0.17927898  0.2648570 0.09011039 0.11425514 0.1730352  0.1961313

I do get that pred3 and pred3_hand are similar though not equal. I understand it's because in the pred3 the expectation of logit^-1(Xb+u+e) is taken over the distribution of the residual e whereas in pred3_hand, I am getting an approximation of that.

I have looked at the code for predict to get a better understanding and I saw was that when 'posterior=mean' is specified in pred2, the posterior mean for sigma_u^2 is computed (code line 93-94):
object$VCV <- matrix(colMeans(object$VCV), 1, ncol(object$VCV))
it <- 1

but only the first MCMC iteration is kept for b and u (code line 109-110):
object$Sol <- object$Sol[it, , drop = FALSE]

Is there a reason why the posterior mean is not obtained for b and u as well before computing the latent variable for each sample (like I used for pred2_hand)? I understand that the predictions obtained won't depend on sigma_u^2 since we fix u and only consider residual e (that has variance of 1) as random when taking expectation of logit^-1(Xb+u+e).

Cheers,
Joelle
________________________________
From: Jarrod Hadfield [j.hadfield at ed.ac.uk]
Sent: Wednesday, November 02, 2016 11:27 AM
To: Joelle Mbatchou; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Question about the predict function in MCMCglmm


Hi,

Also

tcrossprod(beta_hat, X) + U

should be

(tcrossprod(beta_hat, X) + U)/sqrt(1 + k^2 * rowSums(mMCMC$VCV[,"units"]))

for pred2_hand

Cheers,

Jarrod


On 02/11/2016 16:07, Jarrod Hadfield wrote:
tcrossprod(beta_hat, X) + U


	[[alternative HTML version deleted]]


From j.hadfield at ed.ac.uk  Thu Nov  3 07:15:40 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 3 Nov 2016 06:15:40 +0000
Subject: [R-sig-ME] Question about the predict function in MCMCglmm
In-Reply-To: <24D8F4EBBCB6E34AB77AEDAC4DBA25E0418AF4C9@xm-mbx-05-prod>
References: <24D8F4EBBCB6E34AB77AEDAC4DBA25E0418AF307@xm-mbx-05-prod>
	<631c2019-75fa-269c-8165-9f740f93244c@ed.ac.uk>
	<76869547-e81a-d42d-050c-7b7f4feb632c@ed.ac.uk>
	<24D8F4EBBCB6E34AB77AEDAC4DBA25E0418AF4C9@xm-mbx-05-prod>
Message-ID: <88baa145-041c-0cde-02ba-127df3dc52f1@ed.ac.uk>

Oh dear - it is a bug with the predict function. The default 
posterior="all" which gets the posterior predicted mean is fine, but as 
you say making the prediction on the posterior mean/mode of the 
parameters is not. Having

       object$Sol<-matrix(colMeans(object$Sol), 1, ncol(object$Sol))

below (L66)

       object$VCV<-matrix(colMeans(object$VCV), 1, ncol(object$VCV))

and

       object$Sol<-matrix(posterior.mode(object$Sol, ...), 1, 
ncol(object$Sol))

below (L70)

       object$VCV<-matrix(posterior.mode(object$VCV, ...), 1, 
ncol(object$VCV))

fixes it. I will update MCMCglmm today.

Cheers,

Jarrod



On 02/11/2016 23:19, Joelle Mbatchou wrote:
> Hi Jarrod,
>
> Thanks for the quick reply! The V matrix I provide has 1 on the 
> diagonal; I implemented what you suggested:
>
> > pred1 <- predict(mMCMC, marginal = ~idU, type = "response")
> > pred2 <- predict(mMCMC, marginal = NULL, posterior = "mean", type = 
> "response")
> > pred3 <- predict(mMCMC, marginal = NULL, posterior = "all", type = 
> "response")
>
> > beta_hat <- as.matrix(mMCMC$Sol[,1:2])
> > beta_hat_mean <- colMeans(beta_hat)  # Posterior mean for beta
> > U <- as.matrix(mMCMC$Sol[,-(1:2)])
> > U_mean <- colMeans(U)  # Posterior mean for random effects u
> > X <- as.matrix(mMCMC$X)
> > k <- ((16*sqrt(3))/(15*pi))^2
> > sig_sq <- mMCMC$VCV[,"units"] + tcrossprod(mMCMC$VCV[,"idU"], diag(V)) 
>  #Variance of latent variables for each MCMC iteration
>
> > pred1_hand <- colMeans(plogis(tcrossprod(beta_hat, X) / sqrt(1 + k^2 
> * sig_sq)))
> > pred2_hand <- plogis( (tcrossprod(beta_hat_mean, X) + U_mean)/sqrt(1 
> + k^2 * mean(mMCMC$VCV[,"units"])) )
> > pred3_hand <- colMeans(plogis( (tcrossprod(beta_hat, X) + U)/sqrt(1 + 
> k^2 * mMCMC$VCV[,"units"]) ))
>
> > head(cbind(pred1, pred1_hand, pred2,pred2_hand,pred3, pred3_hand))
>        pred1 pred1_hand      pred2 pred2_hand     pred3 pred3_hand
> 1 0.08467802  0.1772529 0.21552374 0.11490073 0.1622893  0.1869202
> 2 0.23766175  0.3115590 0.17228113 0.24496249 0.2938466  0.3114496
> 3 0.19338132  0.2764658 0.07845114 0.43098835 0.4278642  0.4364018
> 4 0.15685811  0.2458417 0.10811526 0.06985046 0.1201754  0.1443062
> 5 0.14048409  0.2314295 0.35230597 0.17401086 0.2186460  0.2418515
> 6 0.17927898  0.2648570 0.09011039 0.11425514 0.1730352  0.1961313
>
> I do get that pred3 and pred3_hand are similar though not equal. I 
> understand it's because in the pred3 the expectation of 
> logit^-1(Xb+u+e) is taken over the distribution of the residual e 
> whereas in pred3_hand, I am getting an approximation of that.
>
> I have looked at the code for predict to get a better understanding 
> and I saw was that when 'posterior=mean' is specified in pred2, the 
> posterior mean for sigma_u^2 is computed (code line 93-94):
> object$VCV <- matrix(colMeans(object$VCV), 1, ncol(object$VCV))
> it <- 1
>
> but only the first MCMC iteration is kept for b and u (code line 109-110):
> object$Sol <- object$Sol[it, , drop = FALSE]
>
> Is there a reason why the posterior mean is not obtained for b and u 
> as well before computing the latent variable for each sample (like I 
> used for pred2_hand)? I understand that the predictions obtained won't 
> depend on sigma_u^2 since we fix u and only consider residual e (that 
> has variance of 1) as random when taking expectation of logit^-1(Xb+u+e).
>
> Cheers,
> Joelle
> ------------------------------------------------------------------------
> *From:* Jarrod Hadfield [j.hadfield at ed.ac.uk]
> *Sent:* Wednesday, November 02, 2016 11:27 AM
> *To:* Joelle Mbatchou; r-sig-mixed-models at r-project.org
> *Subject:* Re: [R-sig-ME] Question about the predict function in MCMCglmm
>
> Hi,
>
> Also
>
> tcrossprod(beta_hat, X) + U
>
> should be
>
> (tcrossprod(beta_hat, X) + U)/sqrt(1 + k^2 * rowSums(mMCMC$VCV[,"units"]))
>
> for pred2_hand
>
> Cheers,
>
> Jarrod
>
>
>
> On 02/11/2016 16:07, Jarrod Hadfield wrote:
>> tcrossprod(beta_hat, X) + U
>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-mixed-models/attachments/20161103/037234d2/attachment.pl>

From Phillip.Alday at unisa.edu.au  Fri Nov  4 06:06:18 2016
From: Phillip.Alday at unisa.edu.au (Phillip Alday)
Date: Fri, 4 Nov 2016 05:06:18 +0000
Subject: [R-sig-ME] Help with Linear Model
In-Reply-To: <CAH8i2pORE-n8MkqaT1Mn_L1e5BKAAsxXTkMFbgjWQo1sumpg1g@mail.gmail.com>
References: <CAH8i2pORE-n8MkqaT1Mn_L1e5BKAAsxXTkMFbgjWQo1sumpg1g@mail.gmail.com>
Message-ID: <CE8A3688-4EC3-493C-934E-9693FA642EA9@unisa.edu.au>

Hi Joseph,

Are you treating year as a continuous variable or a categorical? If you only have three years and one is outlier-isn, it may not make sense to treat them as a continuous variable (whether linear, quadratic or some other form).

Also, 2 sites is not enough levels for a random effect -- remember that RE are variance estimates and it doesn't really make sense to make a variance estimate of two levels. How many blocks do you have per site? Maybe Site:block is sufficient and if need be you can include Site as a nuisance parameter in the fixed effects?

Best,
Phillip

> On 3 Nov 2016, at 05:07, Joseph Aidoo <jaidoo at ualberta.ca> wrote:
> 
> My experiment was set up as a RCBD with
> 5 oat varieties
> 4 nitrogen rates
> at 2 sites
> over 3 years .
> 
> I am investigating the effect of Variety and Nitrogen on Yield.
> 
> In the second year of the experiment we had a drought so the data was
> awful. So i expect differences in Years
> 
> I am using Variety, Nitrogen and Year as my fixed effects.
> and Sites as my random effect.
> 
> I expect and interaction between Variety X Nitrogen.
> This is the model i came up with
> 
> Yield ~ nitrogen*variety*Year + (Year|Site/block)
> 
> My data doesnt seem normal because of the odd year with the drought.
> I have tried to normally transform the data but nothing seems to work.
> 
> I need advice on what to do next? Is it ideal to use a glmm? What could the
> best model be?
> 
> Thanks
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From Phillip.Alday at unisa.edu.au  Fri Nov  4 06:29:36 2016
From: Phillip.Alday at unisa.edu.au (Phillip Alday)
Date: Fri, 4 Nov 2016 05:29:36 +0000
Subject: [R-sig-ME] How to estimate the standard error of every single
 random intercept in a mixed linear model?
In-Reply-To: <VI1PR0101MB2430182E2678121E2C58D6C8ACA90@VI1PR0101MB2430.eurprd01.prod.exchangelabs.com>
References: <DB6PR0101MB24230DB767F3AC06A2080433ACDD0@DB6PR0101MB2423.eurprd01.prod.exchangelabs.com>
	<VI1PR0101MB2430182E2678121E2C58D6C8ACA90@VI1PR0101MB2430.eurprd01.prod.exchangelabs.com>
Message-ID: <75C2C1CF-E567-47F8-AA06-CB22255445C0@unisa.edu.au>

Dear Chen,

have you tried entering the following in the R command line?

> library(arm)
> se.ranef

That will output the source code for arm::se.ranef. It's surprisingly simple and is basically the square root of the diagonal of variance-covariance RE matrix.

Best,
Phillip 


> On 24 Oct 2016, at 20:38, Chen Chun <talischen at hotmail.com> wrote:
> 
> Dear all,
> 
> 
> I am running a mixed linear model with group (a_i) as random intercept:
> 
> 
> y_ij=mu + a_i + e_ij
> 
> 
> By using lmer() function, the model outputs an estimated variance of a_i (i.e. var_hat(a)), and it is the sum of (1) the variance of the estimated group mean (i.e. between group variance) and (2) the sum of variance for each estimated group mean a_i_hat,   (i.e. sum of within group variance).
> 
> 
> for (1) I can compute it as var(ranef(model)$group). However, I dont know how to compute (2), which is the SE of the estimated random intercept for each group. I know that using se.ranef() function in arm package can help me to extract such variance. But I would like to know how these variance are computed? it's relations to residuals and number of observations per group?
> 
> 
> Thanks
> 
> 
> Chen
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From Phillip.Alday at unisa.edu.au  Fri Nov  4 06:38:51 2016
From: Phillip.Alday at unisa.edu.au (Phillip Alday)
Date: Fri, 4 Nov 2016 05:38:51 +0000
Subject: [R-sig-ME] Help with Linear Model
In-Reply-To: <CAH8i2pOmrAS7s0VS7jmPP5JFSxRmiFnsKzQbcb6o2g0v1zWTvA@mail.gmail.com>
References: <CAH8i2pORE-n8MkqaT1Mn_L1e5BKAAsxXTkMFbgjWQo1sumpg1g@mail.gmail.com>
	<CE8A3688-4EC3-493C-934E-9693FA642EA9@unisa.edu.au>
	<CAH8i2pOmrAS7s0VS7jmPP5JFSxRmiFnsKzQbcb6o2g0v1zWTvA@mail.gmail.com>
Message-ID: <522D77FC-DBD1-48CB-B7D0-BFFA61590146@unisa.edu.au>

One possible model would be:

 Yield ~ nitrogen*variety*Year + (Year|Site:block)

You won't explicitly model the variation between sites, but you should still capture at least some of it via the Site:block interaction. (If your blocks are labelled uniquely across sites, then you can just use block instead of Site:block). You have a total of 8 levels for blocks, which still isn't a lot but should work.

You could also include a fixed effect for Site:  

 Yield ~ nitrogen*variety*Year*Site + (Year|Site:block)

but this will add a fair amount of complexity to the model and I'm not sure you have enough data for that.

I'm not familiar with oat yields, so I don't know if you should transform Yield, the example in the oats dataset (?oats in R) don't transform Yield.

Best,
Phillip

> On 4 Nov 2016, at 15:58, Joseph Aidoo <jaidoo at ualberta.ca> wrote:
> 
> Thank you Philip  for your quick response. I am treating Year as a factor.
> 
>  I am not interested in the year effect. However I wish to account for it. 
> At each site we had four blocks for the experiment so I assumed blocks will be in sites.  
> Given this clarification how would have out my model?
> 
> I have read some papers which combines site*years = environment.  I don't know if its suitable in this experiment.
> 
> Thank you
> 
> 
> On Thu, Nov 3, 2016 at 11:06 PM, Phillip Alday <Phillip.Alday at unisa.edu.au> wrote:
> Hi Joseph,
> 
> Are you treating year as a continuous variable or a categorical? If you only have three years and one is outlier-isn, it may not make sense to treat them as a continuous variable (whether linear, quadratic or some other form).
> 
> Also, 2 sites is not enough levels for a random effect -- remember that RE are variance estimates and it doesn't really make sense to make a variance estimate of two levels. How many blocks do you have per site? Maybe Site:block is sufficient and if need be you can include Site as a nuisance parameter in the fixed effects?
> 
> Best,
> Phillip
> 
> > On 3 Nov 2016, at 05:07, Joseph Aidoo <jaidoo at ualberta.ca> wrote:
> >
> > My experiment was set up as a RCBD with
> > 5 oat varieties
> > 4 nitrogen rates
> > at 2 sites
> > over 3 years .
> >
> > I am investigating the effect of Variety and Nitrogen on Yield.
> >
> > In the second year of the experiment we had a drought so the data was
> > awful. So i expect differences in Years
> >
> > I am using Variety, Nitrogen and Year as my fixed effects.
> > and Sites as my random effect.
> >
> > I expect and interaction between Variety X Nitrogen.
> > This is the model i came up with
> >
> > Yield ~ nitrogen*variety*Year + (Year|Site/block)
> >
> > My data doesnt seem normal because of the odd year with the drought.
> > I have tried to normally transform the data but nothing seems to work.
> >
> > I need advice on what to do next? Is it ideal to use a glmm? What could the
> > best model be?
> >
> > Thanks
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 


From Phillip.Alday at unisa.edu.au  Fri Nov  4 14:17:31 2016
From: Phillip.Alday at unisa.edu.au (Phillip Alday)
Date: Fri, 4 Nov 2016 13:17:31 +0000
Subject: [R-sig-ME] Help with Linear Model
In-Reply-To: <CAH8i2pPriVBFq8ygTWTkFpcUhEe5-g-jdu6e7RaxDKWuOx4nmQ@mail.gmail.com>
References: <CAH8i2pORE-n8MkqaT1Mn_L1e5BKAAsxXTkMFbgjWQo1sumpg1g@mail.gmail.com>
	<CE8A3688-4EC3-493C-934E-9693FA642EA9@unisa.edu.au>
	<CAH8i2pOmrAS7s0VS7jmPP5JFSxRmiFnsKzQbcb6o2g0v1zWTvA@mail.gmail.com>
	<522D77FC-DBD1-48CB-B7D0-BFFA61590146@unisa.edu.au>
	<CAH8i2pPriVBFq8ygTWTkFpcUhEe5-g-jdu6e7RaxDKWuOx4nmQ@mail.gmail.com>
Message-ID: <1478265450.2008.34.camel@unisa.edu.au>

(Try to remember to keep the list in CC -- I'm really bad at it, too!)

Looking at your QQ plot (consider posting it somewhere online for
posteriority as the list strips attachments), it seems that the
deviation from normality occurs in the form of heavy tails of the sort
you would find with a t-distribution. If you were doing Bayesian
modelling, I would suggest that you just use a t-likelihood instead of
a normal one .... but that may not even be necessary here.

(For one thing, it's not the data that have to be normally distributed,
but rather the residuals or equivalently, the reponse *conditioned* on
the predictors. The linear model is after all given by Y = B*X + B0 +
e, with e ~ N(0,sigma). And in the mixed-model case, you have
additional normal distributions mixing in there via the random
effects.)

I would check your model fit by checking e.g.?

?- fitted vs. actual (you can fake this by using faceting in ggplot or
the | operator in lattice, or you can do it for real using the
predict() function)?
?- fitted vs. residuals ( plot(lme.model) does this for you )

The former will tell you whether your model accurately represent your
data (and places where it fails to do so), while the latter will give
you a visual impression about how bad the violation of normality of the
residuals is.

More important than fulfilling distributional assumptions for me is
seeing how well the model actually fits the data and how good it is at
predicting new data. (Techniques like cross-validation or posterior
predictive check in the Bayesian framework are based on this idea as
well.) Violating the testing assumptions does mean that not all the
?frequentist guarantees hold, but if your model does a good job at
describing old and predicitng new data in practice, then that may be
enough for many applications.?

Now, if you're focussed on significance tests or traditional confidence
intervals, violations of model assumptions can ruin your day, as you
can no longer trust that the null distribution is correct. However, you
can still use bootstrapped confidence intervals, altough those take
much longer to compute.

One final thing: I see you're using Type-III tests. Even though it's
the default in certain popular commercial statistical packages, I would
encourage you to think about twice about doing so. Read Venables'
Exegeses on Linear Models and see some of the stuff John Fox (author of
the car packages) has written on this topic (e.g.?https://stat.ethz.ch/
pipermail/r-help/2006-August/111927.html).?A lot of ink has been
spilled on this topic and it's not always black and white, but I
generally find that Type-II tests are actually the thing that I want.

Best,
Phillip


On Fri, 2016-11-04 at 06:16 -0600, Joseph Aidoo wrote:
> Yield data needs to be transformed... Boxcox doesnt seem to be
> working for me. Here is the output for
> FAT.lme<-
> lmer(Yield.kg_ha~N.rate*Variety*Year+(Year|Site:Block),data=Jo_data14
> 15.dat)
> Anova(FAT.lme,type=3,test="F")
> 
> 
> 
> Response: Yield.kg_ha
> 
> 
> 
> ? ? ? ? ? ? ? ? ? ? ? ? ? ?F Df Df.res ? ?Pr(>F) ? ?
> (Intercept) ? ? ? ? 526.2004 ?1 ?49.26 < 2.2e-16 ***
> N.rate ? ? ? ? ? ? ? 11.5523 ?3 393.00 2.845e-07 ***
> Variety ? ? ? ? ? ? ? 7.7304 ?4 393.70 5.270e-06 ***
> Year ? ? ? ? ? ? ? ? 34.7598 ?2 ?22.02 1.537e-07 ***
> N.rate:Variety ? ? ? ?1.9513 12 393.54 ? 0.02746 * ?
> N.rate:Year ? ? ? ? ? 2.4641 ?6 393.30 ? 0.02373 * ?
> Variety:Year ? ? ? ? ?1.8262 ?8 393.25 ? 0.07068 . ?
> N.rate:Variety:Year ? 0.8795 24 393.25 ? 0.63104 ? ?
> ---
> Signif. codes: ?0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >?
> 
> 
> Shapiro-Wilk normality test
> 
> data: ?resslme
> W = 0.98449, p-value = 5.986e-05
> 
> 
> 
> 
> 
> On Thu, Nov 3, 2016 at 11:38 PM, Phillip Alday <Phillip.Alday at unisa.e
> du.au> wrote:
> > One possible model would be:
> > 
> > ?Yield ~ nitrogen*variety*Year + (Year|Site:block)
> > 
> > You won't explicitly model the variation between sites, but you
> > should still capture at least some of it via the Site:block
> > interaction. (If your blocks are labelled uniquely across sites,
> > then you can just use block instead of Site:block). You have a
> > total of 8 levels for blocks, which still isn't a lot but should
> > work.
> > 
> > You could also include a fixed effect for Site:
> > 
> > ?Yield ~ nitrogen*variety*Year*Site + (Year|Site:block)
> > 
> > but this will add a fair amount of complexity to the model and I'm
> > not sure you have enough data for that.
> > 
> > I'm not familiar with oat yields, so I don't know if you should
> > transform Yield, the example in the oats dataset (?oats in R) don't
> > transform Yield.
> > 
> > Best,
> > Phillip
> > 
> > > On 4 Nov 2016, at 15:58, Joseph Aidoo <jaidoo at ualberta.ca> wrote:
> > >
> > > Thank you Philip? for your quick response. I am treating Year as
> > a factor.
> > >
> > >? I am not interested in the year effect. However I wish to
> > account for it.
> > > At each site we had four blocks for the experiment so I assumed
> > blocks will be in sites.
> > > Given this clarification how would have out my model?
> > >
> > > I have read some papers which combines site*years = environment.?
> > I don't know if its suitable in this experiment.
> > >
> > > Thank you
> > >
> > >
> > > On Thu, Nov 3, 2016 at 11:06 PM, Phillip Alday <Phillip.Alday at uni
> > sa.edu.au> wrote:
> > > Hi Joseph,
> > >
> > > Are you treating year as a continuous variable or a categorical?
> > If you only have three years and one is outlier-isn, it may not
> > make sense to treat them as a continuous variable (whether linear,
> > quadratic or some other form).
> > >
> > > Also, 2 sites is not enough levels for a random effect --
> > remember that RE are variance estimates and it doesn't really make
> > sense to make a variance estimate of two levels. How many blocks do
> > you have per site? Maybe Site:block is sufficient and if need be
> > you can include Site as a nuisance parameter in the fixed effects?
> > >
> > > Best,
> > > Phillip
> > >
> > > > On 3 Nov 2016, at 05:07, Joseph Aidoo <jaidoo at ualberta.ca>
> > wrote:
> > > >
> > > > My experiment was set up as a RCBD with
> > > > 5 oat varieties
> > > > 4 nitrogen rates
> > > > at 2 sites
> > > > over 3 years .
> > > >
> > > > I am investigating the effect of Variety and Nitrogen on Yield.
> > > >
> > > > In the second year of the experiment we had a drought so the
> > data was
> > > > awful. So i expect differences in Years
> > > >
> > > > I am using Variety, Nitrogen and Year as my fixed effects.
> > > > and Sites as my random effect.
> > > >
> > > > I expect and interaction between Variety X Nitrogen.
> > > > This is the model i came up with
> > > >
> > > > Yield ~ nitrogen*variety*Year + (Year|Site/block)
> > > >
> > > > My data doesnt seem normal because of the odd year with the
> > drought.
> > > > I have tried to normally transform the data but nothing seems
> > to work.
> > > >
> > > > I need advice on what to do next? Is it ideal to use a glmm?
> > What could the
> > > > best model be?
> > > >
> > > > Thanks
> > > >
> > > >? ? ? ?[[alternative HTML version deleted]]
> > > >
> > > > _______________________________________________
> > > > R-sig-mixed-models at r-project.org mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> > >
> > >
> > 
> > 

From jmbatchou at uchicago.edu  Thu Nov  3 22:09:26 2016
From: jmbatchou at uchicago.edu (Joelle Mbatchou)
Date: Thu, 3 Nov 2016 21:09:26 +0000
Subject: [R-sig-ME] Question about the predict function in MCMCglmm
In-Reply-To: <88baa145-041c-0cde-02ba-127df3dc52f1@ed.ac.uk>
References: <24D8F4EBBCB6E34AB77AEDAC4DBA25E0418AF307@xm-mbx-05-prod>
	<631c2019-75fa-269c-8165-9f740f93244c@ed.ac.uk>
	<76869547-e81a-d42d-050c-7b7f4feb632c@ed.ac.uk>
	<24D8F4EBBCB6E34AB77AEDAC4DBA25E0418AF4C9@xm-mbx-05-prod>,
	<88baa145-041c-0cde-02ba-127df3dc52f1@ed.ac.uk>
Message-ID: <24D8F4EBBCB6E34AB77AEDAC4DBA25E0418AF6FA@xm-mbx-05-prod>

Hi Jarrod,

Thank you for the clarification! I tried both of the approximations specified in the course notes and I do get closer predictions using the approximation from McCulloch and Searle (2001) but only when marginalization is done with respect to the residuals.

When marginalizing over the random effects, I get that the approximation gives quite different results from those of the predict function:

######## Using predict.MCMCglmm() -- I modified function for pred2
pred1 <- as.vector(predict(mMCMC, marginal = ~idU, type = "response"))
pred2 <- as.vector(predict_MCMC(mMCMC, marginal = NULL, posterior = "mean", type = "response"))
pred3 <- as.vector(predict(mMCMC, marginal = NULL, posterior = "all", type = "response"))

sig_sq <- mMCMC$VCV[,"units"] + mMCMC$VCV[,"idU"]

######## Using approximation from Diggle et al. (2004)
pred1_hand_Da <- colMeans(plogis(Xb / sqrt(1 + ck^2 * sig_sq)))
pred2_hand_Da <- as.vector(plogis( (Xb_mean + U_mean)/ sqrt(1 + k^2 * 1) ))
pred3_hand_Da <- colMeans(plogis( (Xb + U)/sqrt(1 + k^2 * 1) ))

######## Using approximation from McCulloch and Searle (2001)
pred1_hand_MS <- colMeans(plogis( Xb - 0.5 * sig_sq * tanh(Xb * (1 + 2 * exp(-0.5 * sig_sq))/6) ))
pred2_hand_MS <- as.vector(plogis( (Xb_mean + U_mean - 0.5 * 1 * tanh( (Xb_mean + U_mean) * (1 + 2 * exp(-0.5 * 1))/6)) ))
pred3_hand_MS <- colMeans(plogis( (Xb + U) - 0.5 * 1 * tanh((Xb + U) * (1 + 2 * exp(-0.5 * 1))/6) ))

#################
###### Comparison
## Marginalizing over u and e
       pred1 pred1_hand_Da   pred1_hand_MS
1 0.08467802    0.03564130       0.3434171
2 0.23766175    0.15399697       0.4817053
3 0.19338132    0.11215077       0.4522571
4 0.15685811    0.08232281       0.4229498
5 0.14048409    0.07032301       0.4079111
6 0.17927898    0.10013013       0.4415578
## Taking posterior mean of b and u then marginalizing over e
          pred2 pred2_hand_Da   pred2_hand_MS
[1,] 0.07628308    0.11490073      0.07631337
[2,] 0.20802371    0.24496249      0.20949077
[3,] 0.41934658    0.43098835      0.42044645
[4,] 0.03839855    0.06985046      0.03826145
[5,] 0.13323146    0.17401086      0.13387440
[6,] 0.07569975    0.11425514      0.07572540
## Marginalizing over e then taking the posterior mean
      pred3 pred3_hand_Da   pred3_hand_MS
1 0.1622893     0.1869202       0.1626918
2 0.2938466     0.3114496       0.2942767
3 0.4278642     0.4364018       0.4282031
4 0.1201754     0.1443062       0.1205276
5 0.2186460     0.2418515       0.2191097
6 0.1730352     0.1961313       0.1734039


As you can see, I can approximate pred2 and pred3 pretty well in 3rd column. I am trying to reason why the pred1 approximation is so off... It seems to me that having a variance different from 1 in the normal pdf used in the integration of logit^-1(xb+u+e) should not make the approximation that much worse. Or is that not the case? I compared the results of 'normal.multilogistic(x,v)' and 'plogis( x - 0.5 * v * tanh(x * (1 + 2 * exp(-0.5 * v))/6) )' for a few (x,v) values from data:

> x <- Xb[,1]; v <- sig_sq[,1] # Looking at first individual across all MCMC iterations
> difference <- sapply(1:length(x), function(i) abs(normal.multilogistic(x[i],v[i])-plogis( x[i] - 0.5 * v[i] * tanh(x[i] * (1 + 2 * exp(-0.5 * v[i]))/6) )))
> summary(difference)
     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
0.0000004 0.0065390 0.0614500 0.2588000 0.4812000 0.9545000
> summary(x)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
-11.760  -6.042  -4.865  -5.346  -4.084  -2.678
> summary(x[difference>.1])  ## Not much difference
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
-11.760  -8.156  -6.268  -6.755  -5.422  -3.826
> summary(v)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  1.146   5.414   9.154  13.670  16.490  70.350
> summary(v[difference>.1])
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  9.699  13.030  17.630  23.380  33.950  70.350

So the difference observed seems due to very high values of Var(u+e). I guess I answered my own question (still including code above if others have similar Q)... So in that case the other approximation from Diggle et al. (2004) would produce overall more accurate results.

One last Q: Is there an intuitive explanation as to why one might prefer using pred2 vs pred3, that is getting posterior means of b and u then marginalizing logit^-1(Xb+u+e) (i.e. posterior = "mean") versus first marginalizing logit^-1(Xb+u+e) and then taking the mean over all iterations (i.e. posterior = "all")?

Thank you for the very quick replies and going through all my messages!

Cheers,
Joelle


________________________________
From: Jarrod Hadfield [j.hadfield at ed.ac.uk]
Sent: Thursday, November 03, 2016 1:15 AM
To: Joelle Mbatchou; r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] Question about the predict function in MCMCglmm


Oh dear - it is a bug with the predict function. The default posterior="all" which gets the posterior predicted mean is fine, but as you say making the prediction on the posterior mean/mode of the parameters is not. Having

      object$Sol<-matrix(colMeans(object$Sol), 1, ncol(object$Sol))

below (L66)

      object$VCV<-matrix(colMeans(object$VCV), 1, ncol(object$VCV))

and

      object$Sol<-matrix(posterior.mode(object$Sol, ...), 1, ncol(object$Sol))

below (L70)

      object$VCV<-matrix(posterior.mode(object$VCV, ...), 1, ncol(object$VCV))

fixes it. I will update MCMCglmm today.

Cheers,

Jarrod


On 02/11/2016 23:19, Joelle Mbatchou wrote:
Hi Jarrod,

Thanks for the quick reply! The V matrix I provide has 1 on the diagonal; I implemented what you suggested:

> pred1 <- predict(mMCMC, marginal = ~idU, type = "response")
> pred2 <- predict(mMCMC, marginal = NULL, posterior = "mean", type = "response")
> pred3 <- predict(mMCMC, marginal = NULL, posterior = "all", type = "response")

> beta_hat <- as.matrix(mMCMC$Sol[,1:2])
> beta_hat_mean <- colMeans(beta_hat)  # Posterior mean for beta
> U <- as.matrix(mMCMC$Sol[,-(1:2)])
> U_mean <- colMeans(U)  # Posterior mean for random effects u
> X <- as.matrix(mMCMC$X)
> k <- ((16*sqrt(3))/(15*pi))^2
> sig_sq <- mMCMC$VCV[,"units"] + tcrossprod(mMCMC$VCV[,"idU"], diag(V))  #Variance of latent variables for each MCMC iteration

> pred1_hand <- colMeans(plogis(tcrossprod(beta_hat, X) / sqrt(1 + k^2 * sig_sq)))
> pred2_hand <- plogis( (tcrossprod(beta_hat_mean, X) + U_mean)/sqrt(1 + k^2 * mean(mMCMC$VCV[,"units"])) )
> pred3_hand <- colMeans(plogis( (tcrossprod(beta_hat, X) + U)/sqrt(1 + k^2 * mMCMC$VCV[,"units"]) ))

> head(cbind(pred1, pred1_hand, pred2, pred2_hand, pred3, pred3_hand))
       pred1 pred1_hand      pred2 pred2_hand     pred3 pred3_hand
1 0.08467802  0.1772529 0.21552374 0.11490073 0.1622893  0.1869202
2 0.23766175  0.3115590 0.17228113 0.24496249 0.2938466  0.3114496
3 0.19338132  0.2764658 0.07845114 0.43098835 0.4278642  0.4364018
4 0.15685811  0.2458417 0.10811526 0.06985046 0.1201754  0.1443062
5 0.14048409  0.2314295 0.35230597 0.17401086 0.2186460  0.2418515
6 0.17927898  0.2648570 0.09011039 0.11425514 0.1730352  0.1961313

I do get that pred3 and pred3_hand are similar though not equal. I understand it's because in the pred3 the expectation of logit^-1(Xb+u+e) is taken over the distribution of the residual e whereas in pred3_hand, I am getting an approximation of that.

I have looked at the code for predict to get a better understanding and I saw was that when 'posterior=mean' is specified in pred2, the posterior mean for sigma_u^2 is computed (code line 93-94):
object$VCV <- matrix(colMeans(object$VCV), 1, ncol(object$VCV))
it <- 1

but only the first MCMC iteration is kept for b and u (code line 109-110):
object$Sol <- object$Sol[it, , drop = FALSE]

Is there a reason why the posterior mean is not obtained for b and u as well before computing the latent variable for each sample (like I used for pred2_hand)? I understand that the predictions obtained won't depend on sigma_u^2 since we fix u and only consider residual e (that has variance of 1) as random when taking expectation of logit^-1(Xb+u+e).

Cheers,
Joelle
________________________________
From: Jarrod Hadfield [j.hadfield at ed.ac.uk<mailto:j.hadfield at ed.ac.uk>]
Sent: Wednesday, November 02, 2016 11:27 AM
To: Joelle Mbatchou; r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Question about the predict function in MCMCglmm


Hi,

Also

tcrossprod(beta_hat, X) + U

should be

(tcrossprod(beta_hat, X) + U)/sqrt(1 + k^2 * rowSums(mMCMC$VCV[,"units"]))

for pred2_hand

Cheers,

Jarrod


On 02/11/2016 16:07, Jarrod Hadfield wrote:
tcrossprod(beta_hat, X) + U



	[[alternative HTML version deleted]]


From Valerie.Bares at sdstate.edu  Fri Nov  4 18:11:37 2016
From: Valerie.Bares at sdstate.edu (Bares, Valerie)
Date: Fri, 4 Nov 2016 17:11:37 +0000
Subject: [R-sig-ME] glmer function in lme4
Message-ID: <SN1PR0601MB1901C2CE9B6885503706F43CE9A20@SN1PR0601MB1901.namprd06.prod.outlook.com>

I am utilizing the glmer function in the lme4 package.  My data is of longitudinal nature where subjects have one row per month of collected data.  My ultimate goal is to create a predictive model off of this data but also want to make inferences off of the generated model.  I currently am using glmer to generate a model while using time (months) as a random effect.  For example,

glmer(target ~  var1 + (var1|months), family=binomial(link='logit'), data=train)

I initially used glm to create a model for each month separately but wanted to create one model in order to make inferences on variables in the model as related over time.  The output from coef() from the above example will give almost identical coefficients to glm(target ~ var1, family=binomial(link='logit'), data=train[which(train$months==i),]), where i represents each separate monthly models.

My questions are:

*        Is there a direct relationship between glmer and glm even when using months as a random effect (as shown above).

*        It appears that the coefficients from printing the glmer(...) model is just the average of the results from coef(glmer(...)), is this correct?

*        How are the coefficients that are conditional on the month variable interpreted? (output from coef(glmer(...)))

*        When adding in an additional variable into the model and attempting to generate similar results with glm, the second variable needs to take out the random intercept term to match results, can you explain how and if the following two models are related?

1.      glm(target ~ var1 + var2, family=binomial(link='logit'), data=train[which(train$months==i),]), where i represents each separate monthly models

2.      glmer(target ~  var1 + var2 + (var1|months) + (0 + var2|months), family=binomial(link='logit'), data=train)

Any insight on this function and how to interpret the output would be greatly appreciated.  I can also send the generated output from the above examples if this will help in any explanations.

Thank you,
Valerie Bares
Computation Science and Statistics PhD Candidate
South Dakota State University


	[[alternative HTML version deleted]]


From bbolker at gmail.com  Fri Nov  4 19:04:21 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 4 Nov 2016 14:04:21 -0400
Subject: [R-sig-ME] FW:  Exponent random effect in nlmer
In-Reply-To: <D4425043.4F9C2%tim.cole@ucl.ac.uk>
References: <D42269FD.4ECAF%tim.cole@ucl.ac.uk>
	<CAJuCY5wmX6jYMvrtjf_M2LYWOt8gYXq0vSCqJzOTb7sCnHbUnw@mail.gmail.com>
	<D4227B1D.4ED19%tim.cole@ucl.ac.uk>
	<CAJuCY5z0m9JrEEjHeLtuW7Ta7nDtgFk0c+ZNmiXk_fxh5STeFg@mail.gmail.com>
	<D4265F96.4EE74%tim.cole@ucl.ac.uk> <D4425043.4F9C2%tim.cole@ucl.ac.uk>
Message-ID: <030aa702-2482-2b63-d4ad-3ebbd50a1335@gmail.com>

  [cc'ing to r-sig-mixed-models]

  OK, I'll put this back on the queue ... is there any chance that you
can send me/us a reproducible example?

  The error message is driven from merPredD::updateDecomp in
src/predModule.cpp (line 291), in the package C++ code.  This makes it
considerably harder to debug.  There is no easily passable debug flag,
but if you download the source code and set debug=1 in line 258, you
will get at least a little bit more information about progress.

  As for what the code is actually doing in the updateDecomp step
("update L, RZX, and RX"), your best bet is probably to look at
vignette("lmer",package="lme4"), eqs 48-52 and the code after it ... not
that it's easy ...

On 16-11-04 11:06 AM, Cole, Tim wrote:
> Dear Ben,
> 
> I emailed you a while ago about this error message I was getting in
> nlmer: *Error in initializePtr() : Downdated VtV is not positive
> definite.* I'd be really grateful if you could respond - it's holding me
> up and I'd like to understand what is happening. 
> 
> Do you have any thoughts on what might be causing it? I can't find the
> code for initializePtr ? is it accessible? Which matrix is VtV? Is it
> possible to set a breakpoint to track it?
> 
> Any pointers would be really helpful?
> 
> Best wishes,
> Tim
> --- 
> Tim.cole at ucl.ac.uk <mailto:Tim.cole at ucl.ac.uk> Phone +44(0)20 7905 2666
> Fax +44(0)20 7905 2381 
> Population, Policy and Practice Programme
> UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK 
> 
> From: Tim Cole <tim.cole at ucl.ac.uk <mailto:tim.cole at ucl.ac.uk>>
> Date: Friday, 14 October 2016 14:41
> To: Ben Bolker <bbolker at gmail.com <mailto:bbolker at gmail.com>>
> Cc: "r-sig-mixed-models at r-project.org
> <mailto:r-sig-mixed-models at r-project.org>"
> <r-sig-mixed-models at r-project.org <mailto:r-sig-mixed-models at r-project.org>>
> Subject: Re: [R-sig-ME] Exponent random effect in nlmer
> 
>     Dear Ben,
> 
>     I have a model that can be written as:
> 
>     nlmer(y ~ (g - 1) ^ exp(k | id) , data=data, weights=w)   (using a
>     invalid but I hope obvious notation)
> 
>     where 
>     ? -Inf < y < Inf
>     ? 0 < E(y) < 1
>     ? g is a grouping factor
>     ? k is a subject random effect 
> 
>     k raises the group means to a subject-specific power while
>     respecting the constraint on E(y).
> 
>     I've coded it in nlmer, but it gives *Error in initializePtr() :
>     Downdated VtV is not positive definite*
> 
>     You have previously indicated that this can arise from badly
>     rescaled parameters, so I'm wondering where I've gone wrong. 
> 
>     Here is a simple example with 3 groups. 
> 
>     > dim(data)
>     [1] 759   4
>     > 
>     > head(data)
>         id      y     g      w
>     1 1021 2.9968 01.02 0.2650
>     2 1022 0.8592 01.02 0.3931
>     3 1023 3.4657 01.02 0.2650
>     4 1024 0.3989 01.02 2.3648
>     5 1025 1.7230 01.02 0.2219
>     6 1026 0.7451 01.02 0.7046
>     > 
>     > summary(moma <- with(data, model.matrix(~g - 1)))
>          g01.02          g01.03          g02.03     
>      Min.   :0.000   Min.   :0.000   Min.   :0.000  
>      1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.000  
>      Median :0.000   Median :0.000   Median :0.000  
>      Mean   :0.333   Mean   :0.335   Mean   :0.332  
>      3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:1.000  
>      Max.   :1.000   Max.   :1.000   Max.   :1.000  
>     > 
>     > (start <- c(coef(lm(y ~g - 1, data=data, weights=w)), k=0))
>     g01.02 g01.03 g02.03      k 
>     0.7979 0.6355 0.9056 0.0000 
>     > 
>     > data <- cbind(data, moma)
>     > nlmerfn <- function(g01.02,g01.03,g02.03, k) {
>     +     gc <- cbind(g01.02,g01.03,g02.03)
>     +     gc <- as.vector(as.matrix(gc * moma) %*% rep(1, ncol(moma)))
>     +     gc[gc <= 0] <- 1e-6
>     +     gk <- gc ^ exp(k)
>     +     grad <- moma * gk / gc * exp(k)
>     +     grad <- cbind(grad, k=gk * log(gk))
>     +     attr(gk, 'gradient') <- grad
>     +     gk
>     + }
>     > nlmer1 <- nlmer(y ~ nlmerfn(g01.02,g01.03,g02.03, k) ~
>     +                     g01.02+g01.03+g02.03 + k + (k | id),
>     data=data, weights=w, start=start)
>     Error in initializePtr() : Downdated VtV is not positive definite
> 
>     Perhaps I need to include an intercept of some sort, but I'd very
>     much value your thoughts.
> 
>     Best wishes,
>     Tim
>     --- 
>     Tim.cole at ucl.ac.uk <mailto:Tim.cole at ucl.ac.uk> Phone +44(0)20 7905
>     2666 Fax +44(0)20 7905 2381 
>     Population, Policy and Practice Programme
>     UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK 
> 
> 
>     From: Thierry Onkelinx <thierry.onkelinx at inbo.be
>     <mailto:thierry.onkelinx at inbo.be>>
>     Date: Wednesday, 12 October 2016 09:26
>     To: Tim Cole <tim.cole at ucl.ac.uk <mailto:tim.cole at ucl.ac.uk>>
>     Cc: "r-sig-mixed-models at r-project.org
>     <mailto:r-sig-mixed-models at r-project.org>"
>     <r-sig-mixed-models at r-project.org
>     <mailto:r-sig-mixed-models at r-project.org>>
>     Subject: Re: [R-sig-ME] Exponent random effect in nlmer
> 
>         Hi Tim,
> 
>         AFAIK nlmer requires the fixed and random effects to be
>         additive. The model to be used _after_ this this summation can
>         be non linear.
> 
>         Best regards,
> 
>         ir. Thierry Onkelinx
>         Instituut voor natuur- en bosonderzoek / Research Institute for
>         Nature and Forest
>         team Biometrie & Kwaliteitszorg / team Biometrics & Quality
>         Assurance
>         Kliniekstraat 25
>         1070 Anderlecht
>         Belgium
> 
>         To call in the statistician after the experiment is done may be
>         no more than asking him to perform a post-mortem examination: he
>         may be able to say what the experiment died of. ~ Sir Ronald
>         Aylmer Fisher
>         The plural of anecdote is not data. ~ Roger Brinner
>         The combination of some data and an aching desire for an answer
>         does not ensure that a reasonable answer can be extracted from a
>         given body of data. ~ John Tukey
> 
>         2016-10-11 12:30 GMT+02:00 Cole, Tim <tim.cole at ucl.ac.uk
>         <mailto:tim.cole at ucl.ac.uk>>:
> 
>             Dear Thierry,
> 
>             Thanks very much for your speedy response.
> 
>             I agree my model looks odd, but it has a theoretical basis
>             which I'd prefer not to spell out at this stage. Suffice to
>             say that 
>             ? -Inf < y < Inf
>             ? 0 < E(y) < 1
>             ? there is a subject random effect. 
> 
>             For these reasons the usual models and/or transformations
>             won't work, whereas my proposed exponent random effect ought
>             to. I just need to fit it, to see if I'm right!
> 
>             Best wishes,
>             Tim
>             --- 
>             Tim.cole at ucl.ac.uk <mailto:Tim.cole at ucl.ac.uk> Phone
>             +44(0)20 7905 2666 <tel:%2B44%280%2920%207905%202666> Fax
>             +44(0)20 7905 2381 <tel:%2B44%280%2920%207905%202381> 
>             Population, Policy and Practice Programme
>             UCL Great Ormond Street Institute of Child Health, London
>             WC1N 1EH, UK 
> 
> 
>             From: Thierry Onkelinx <thierry.onkelinx at inbo.be
>             <mailto:thierry.onkelinx at inbo.be>>
>             Date: Tuesday, 11 October 2016 11:06
>             To: Tim Cole <tim.cole at ucl.ac.uk <mailto:tim.cole at ucl.ac.uk>>
>             Cc: "r-sig-mixed-models at r-project.org
>             <mailto:r-sig-mixed-models at r-project.org>"
>             <r-sig-mixed-models at r-project.org
>             <mailto:r-sig-mixed-models at r-project.org>>
>             Subject: Re: [R-sig-ME] Exponent random effect in nlmer
> 
>                 Dear Tim,
> 
>                 y centred on 0 and a valid range (0, 1) seems to be
>                 conflicting statements. 
> 
>                 Here a some solutions depending on y
> 
>                 - y stems from a binomial process
>                      - use a binomial glmm. 
>                 - y is continuous and you are willing to transform y
>                     - 0 < y <  1
>                         - apply a logit transformation on
>                 y. lmer(plogis(y) ~ f + (1 | id) )
>                     - 0 <= y < 1
>                         - apply a log transformation on y. lmer(log(y) ~
>                 f + (1 | id) )
>                     - 0 < y <= 1
>                         - apply a log transformation on 1 -
>                 y. lmer(log(1 - y) ~ f + (1 | id) )
>                 - y is continuous are not willing to transform y
>                    - use a beta regression with 0 and/or 1 inflation in
>                 case you have 0 or 1 in the data. Have a look at the
>                 gamlss package to fit this model.
> 
>                 Best regards,
> 
> 
>                 ir. Thierry Onkelinx
>                 Instituut voor natuur- en bosonderzoek / Research
>                 Institute for Nature and Forest
>                 team Biometrie & Kwaliteitszorg / team Biometrics &
>                 Quality Assurance
>                 Kliniekstraat 25
>                 1070 Anderlecht
>                 Belgium
> 
>                 To call in the statistician after the experiment is done
>                 may be no more than asking him to perform a post-mortem
>                 examination: he may be able to say what the experiment
>                 died of. ~ Sir Ronald Aylmer Fisher
>                 The plural of anecdote is not data. ~ Roger Brinner
>                 The combination of some data and an aching desire for an
>                 answer does not ensure that a reasonable answer can be
>                 extracted from a given body of data. ~ John Tukey
> 
>                 2016-10-11 11:29 GMT+02:00 Cole, Tim <tim.cole at ucl.ac.uk
>                 <mailto:tim.cole at ucl.ac.uk>>:
> 
>                     I have a model of the form
>                       m1 <- lmer(y ~ f + (1 | id) )
>                     where y is a continuous variable centred on zero, f
>                     is a unordered factor with coefficients b such 0 < b
>                     < 1, and there is a signficant random subject intercept.
> 
>                     The random intercept can lead to predicted values
>                     outside the valid range (0, 1). For this reason I'd
>                     like to reformulate the model as
>                     m2 <- nlmer(y ~ (f - 1) ^ exp(1 | id) )   (using a
>                     invalid but I hope obvious notation), where the
>                     random effect is now a power centred on 1. This
>                     would constrain the fitted values to be within c(0, 1).
> 
>                     My question is: can this be done in nlmer, and if so
>                     how? Please can someone point me in the right direction?
> 
>                     Thanks,
>                     Tim Cole
>                     ---
>                     Tim.cole at ucl.ac.uk
>                     <mailto:Tim.cole at ucl.ac.uk><mailto:Tim.cole at ucl.ac.uk <mailto:Tim.cole at ucl.ac.uk>>
>                     Phone +44(0)20 7905 2666
>                     <tel:%2B44%280%2920%207905%202666> Fax +44(0)20 7905
>                     2381 <tel:%2B44%280%2920%207905%202381>
>                     Population, Policy and Practice Programme
>                     UCL Great Ormond Street Institute of Child Health,
>                     London WC1N 1EH, UK
> 
> 
>                             [[alternative HTML version deleted]]
> 
>                     _______________________________________________
>                     R-sig-mixed-models at r-project.org
>                     <mailto:R-sig-mixed-models at r-project.org> mailing list
>                     https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>                     <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
> 
> 
>


From Phillip.Alday at unisa.edu.au  Sun Nov  6 05:10:22 2016
From: Phillip.Alday at unisa.edu.au (Phillip Alday)
Date: Sun, 6 Nov 2016 04:10:22 +0000
Subject: [R-sig-ME] Help with Linear Model
In-Reply-To: <BB222728-C106-4816-94EF-D46F2651EE58@ualberta.ca>
References: <CAH8i2pORE-n8MkqaT1Mn_L1e5BKAAsxXTkMFbgjWQo1sumpg1g@mail.gmail.com>
	<CE8A3688-4EC3-493C-934E-9693FA642EA9@unisa.edu.au>
	<CAH8i2pOmrAS7s0VS7jmPP5JFSxRmiFnsKzQbcb6o2g0v1zWTvA@mail.gmail.com>
	<522D77FC-DBD1-48CB-B7D0-BFFA61590146@unisa.edu.au>
	<CAH8i2pPriVBFq8ygTWTkFpcUhEe5-g-jdu6e7RaxDKWuOx4nmQ@mail.gmail.com>
	<1478265450.2008.34.camel@unisa.edu.au>
	<BB222728-C106-4816-94EF-D46F2651EE58@ualberta.ca>
Message-ID: <1478405420.2110.15.camel@unisa.edu.au>

Whether or not you're interested in a parameter has little to no
influence on whether or not it should be in your model nor whether or
not it is a fixed or random effects. Instead, that is determined by the
structure of your data. In your case, it would in many ways be nicer to
have years as a random effect (as you only care about the *variance*
due to year and not any particular fixed year), but you simply don't
have the data structure to do so with so few levels.

In your case, the year parameter is simply a nuisance parameter -- it
captures important variation, even if it's variation you don't care
about. Now you can marginalize over that effect either before modelling
(e.g. by aggregating / averaging over the time dimension) or after
modelling (your choice of test type (II, III) or using predicted
marginal / least-square means (e.g. via package lsmeans) or implicitly
(just completely omitting it from the model specification). I would
tend to leave the year effect in the model if possible -- I'm a big fan
of modelling as much as you can so that your model is as good as
possible, even if there are only a few predictors you actually care
about.?

That said, if you don't have enough data for such a complex model, then
you should leave out less-interesting dimensions in your model
specification (and potentially aggregate across them beforehand). The
Bates et al preprint on parsimonious mixed models really emphasizes
this point -- it doesn't do much good to model everything and have a
degenerate and/or overfitted model. You know your data best and so you
know what tradeoffs are best for your case.

Best,
Phillip

On Fri, 2016-11-04 at 09:51 -0600, Joseph Aidoo wrote:
> Hi Phillip, I am going to bug you one last time.?
> I'm considering focusing on the effect of Variety and Nitrogen only
> on yield.?
> I am not interested in the effect of the ?years. How do I account for
> variation in years in the model now. Perhaps using at a random
> factor??
> How would you lay it out in a model.?
> 
> Regards?
> 
> Sent from my iPhone
> 
> On Nov 4, 2016, at 7:17 AM, Phillip Alday <Phillip.Alday at unisa.edu.au
> > wrote:
> 
> > (Try to remember to keep the list in CC -- I'm really bad at it,
> > too!)
> > 
> > Looking at your QQ plot (consider posting it somewhere online for
> > posteriority as the list strips attachments), it seems that the
> > deviation from normality occurs in the form of heavy tails of the
> > sort
> > you would find with a t-distribution. If you were doing Bayesian
> > modelling, I would suggest that you just use a t-likelihood instead
> > of
> > a normal one .... but that may not even be necessary here.
> > 
> > (For one thing, it's not the data that have to be normally
> > distributed,
> > but rather the residuals or equivalently, the reponse *conditioned*
> > on
> > the predictors. The linear model is after all given by Y = B*X + B0
> > +
> > e, with e ~ N(0,sigma). And in the mixed-model case, you have
> > additional normal distributions mixing in there via the random
> > effects.)
> > 
> > I would check your model fit by checking e.g.?
> > 
> > ?- fitted vs. actual (you can fake this by using faceting in ggplot
> > or
> > the | operator in lattice, or you can do it for real using the
> > predict() function)?
> > ?- fitted vs. residuals ( plot(lme.model) does this for you )
> > 
> > The former will tell you whether your model accurately represent
> > your
> > data (and places where it fails to do so), while the latter will
> > give
> > you a visual impression about how bad the violation of normality of
> > the
> > residuals is.
> > 
> > More important than fulfilling distributional assumptions for me is
> > seeing how well the model actually fits the data and how good it is
> > at
> > predicting new data. (Techniques like cross-validation or posterior
> > predictive check in the Bayesian framework are based on this idea
> > as
> > well.) Violating the testing assumptions does mean that not all the
> > ?frequentist guarantees hold, but if your model does a good job at
> > describing old and predicitng new data in practice, then that may
> > be
> > enough for many applications.?
> > 
> > Now, if you're focussed on significance tests or traditional
> > confidence
> > intervals, violations of model assumptions can ruin your day, as
> > you
> > can no longer trust that the null distribution is correct. However,
> > you
> > can still use bootstrapped confidence intervals, altough those take
> > much longer to compute.
> > 
> > One final thing: I see you're using Type-III tests. Even though
> > it's
> > the default in certain popular commercial statistical packages, I
> > would
> > encourage you to think about twice about doing so. Read Venables'
> > Exegeses on Linear Models and see some of the stuff John Fox
> > (author of
> > the car packages) has written on this topic (e.g.?https://stat.ethz
> > .ch/
> > pipermail/r-help/2006-August/111927.html).?A lot of ink has been
> > spilled on this topic and it's not always black and white, but I
> > generally find that Type-II tests are actually the thing that I
> > want.
> > 
> > Best,
> > Phillip
> > 
> > 
> > On Fri, 2016-11-04 at 06:16 -0600, Joseph Aidoo wrote:
> > > Yield data needs to be transformed... Boxcox doesnt seem to be
> > 
> > working for me. Here is the output for
> 
> FAT.lme<-
> 
> lmer(Yield.kg_ha~N.rate*Variety*Year+(Year|Site:Block),data=Jo_data14
> 
> 15.dat)
> 
> Anova(FAT.lme,type=3,test="F")
> 
> 
> 
> 
> Response: Yield.kg_ha
> 
> 
> 
> 
> ? ? ? ? ? ? ? ? ? ? ? ? ? ?F Df Df.res ? ?Pr(>F) ? ?
> 
> (Intercept) ? ? ? ? 526.2004 ?1 ?49.26 < 2.2e-16 ***
> 
> N.rate ? ? ? ? ? ? ? 11.5523 ?3 393.00 2.845e-07 ***
> 
> Variety ? ? ? ? ? ? ? 7.7304 ?4 393.70 5.270e-06 ***
> 
> Year ? ? ? ? ? ? ? ? 34.7598 ?2 ?22.02 1.537e-07 ***
> 
> N.rate:Variety ? ? ? ?1.9513 12 393.54 ? 0.02746 * ?
> 
> N.rate:Year ? ? ? ? ? 2.4641 ?6 393.30 ? 0.02373 * ?
> 
> Variety:Year ? ? ? ? ?1.8262 ?8 393.25 ? 0.07068 . ?
> 
> N.rate:Variety:Year ? 0.8795 24 393.25 ? 0.63104 ? ?
> 
> ---
> 
> Signif. codes: ?0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> > ?
> 
> 
> 
> Shapiro-Wilk normality test
> 
> 
> data: ?resslme
> 
> W = 0.98449, p-value = 5.986e-05
> 
> 
> 
> 
> 
> 
> On Thu, Nov 3, 2016 at 11:38 PM, Phillip Alday <Phillip.Alday at unisa.e
> 
> du.au> wrote:
> 
> > One possible model would be:
> 
> > 
> 
> > ?Yield ~ nitrogen*variety*Year + (Year|Site:block)
> 
> > 
> 
> > You won't explicitly model the variation between sites, but you
> 
> > should still capture at least some of it via the Site:block
> 
> > interaction. (If your blocks are labelled uniquely across sites,
> 
> > then you can just use block instead of Site:block). You have a
> 
> > total of 8 levels for blocks, which still isn't a lot but should
> 
> > work.
> 
> > 
> 
> > You could also include a fixed effect for Site:
> 
> > 
> 
> > ?Yield ~ nitrogen*variety*Year*Site + (Year|Site:block)
> 
> > 
> 
> > but this will add a fair amount of complexity to the model and I'm
> 
> > not sure you have enough data for that.
> 
> > 
> 
> > I'm not familiar with oat yields, so I don't know if you should
> 
> > transform Yield, the example in the oats dataset (?oats in R) don't
> 
> > transform Yield.
> 
> > 
> 
> > Best,
> 
> > Phillip
> 
> > 
> 
> > > On 4 Nov 2016, at 15:58, Joseph Aidoo <jaidoo at ualberta.ca> wrote:
> 
> > > 
> 
> > > Thank you Philip? for your quick response. I am treating Year as
> 
> > a factor.
> 
> > > 
> 
> > > ? I am not interested in the year effect. However I wish to
> 
> > account for it.
> 
> > > At each site we had four blocks for the experiment so I assumed
> 
> > blocks will be in sites.
> 
> > > Given this clarification how would have out my model?
> 
> > > 
> 
> > > I have read some papers which combines site*years = environment.?
> 
> > I don't know if its suitable in this experiment.
> 
> > > 
> 
> > > Thank you
> 
> > > 
> 
> > > 
> 
> > > On Thu, Nov 3, 2016 at 11:06 PM, Phillip Alday <Phillip.Alday at uni
> 
> > sa.edu.au> wrote:
> 
> > > Hi Joseph,
> 
> > > 
> 
> > > Are you treating year as a continuous variable or a categorical?
> 
> > If you only have three years and one is outlier-isn, it may not
> 
> > make sense to treat them as a continuous variable (whether linear,
> 
> > quadratic or some other form).
> 
> > > 
> 
> > > Also, 2 sites is not enough levels for a random effect --
> 
> > remember that RE are variance estimates and it doesn't really make
> 
> > sense to make a variance estimate of two levels. How many blocks do
> 
> > you have per site? Maybe Site:block is sufficient and if need be
> 
> > you can include Site as a nuisance parameter in the fixed effects?
> 
> > > 
> 
> > > Best,
> 
> > > Phillip
> 
> > > 
> 
> > > > On 3 Nov 2016, at 05:07, Joseph Aidoo <jaidoo at ualberta.ca>
> 
> > wrote:
> 
> > > > 
> 
> > > > My experiment was set up as a RCBD with
> 
> > > > 5 oat varieties
> 
> > > > 4 nitrogen rates
> 
> > > > at 2 sites
> 
> > > > over 3 years .
> 
> > > > 
> 
> > > > I am investigating the effect of Variety and Nitrogen on Yield.
> 
> > > > 
> 
> > > > In the second year of the experiment we had a drought so the
> 
> > data was
> 
> > > > awful. So i expect differences in Years
> 
> > > > 
> 
> > > > I am using Variety, Nitrogen and Year as my fixed effects.
> 
> > > > and Sites as my random effect.
> 
> > > > 
> 
> > > > I expect and interaction between Variety X Nitrogen.
> 
> > > > This is the model i came up with
> 
> > > > 
> 
> > > > Yield ~ nitrogen*variety*Year + (Year|Site/block)
> 
> > > > 
> 
> > > > My data doesnt seem normal because of the odd year with the
> 
> > drought.
> 
> > > > I have tried to normally transform the data but nothing seems
> 
> > to work.
> 
> > > > 
> 
> > > > I need advice on what to do next? Is it ideal to use a glmm?
> 
> > What could the
> 
> > > > best model be?
> 
> > > > 
> 
> > > > Thanks
> 
> > > > 
> 
> > > > ? ? ? ?[[alternative HTML version deleted]]
> 
> > > > 
> 
> > > > _______________________________________________
> 
> > > > R-sig-mixed-models at r-project.org mailing list
> 
> > > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> > > 
> 
> > > 
> 
> > 
> 
> > 

From tim.cole at ucl.ac.uk  Mon Nov  7 12:55:48 2016
From: tim.cole at ucl.ac.uk (Cole, Tim)
Date: Mon, 7 Nov 2016 11:55:48 +0000
Subject: [R-sig-ME] Exponent random effect in nlmer
Message-ID: <D446070F.4FA7D%tim.cole@ucl.ac.uk>

Thanks very much Ben. You've rather confirmed what I feared, that it's hairy to investigate.

I've gone back to my model and simplified it to be a weighted mean of 3 groups, with 10 subjects and either an additive or a multiplicative subject random effect. The additive model works fine while the multiplicative model fails with:
Error in initializePtr() : Downdated VtV is not positive definite

Have I got the multiplicative model right?

Best wishes,
Tim

>   data
   id        y g        w
1   1  1.71121 1 0.451575
2   2  1.02902 1 4.163295
3   3  0.80209 1 2.577572
4   4  0.62700 1 6.181288
5   5  0.50321 1 0.794809
6   6  1.08428 1 0.925991
7   7  0.86268 1 0.109342
8   8  0.59772 1 1.116947
9   9  0.24041 1 0.030605
10 10  2.73586 1 0.023148
11  1  2.38925 2 0.451575
12  2  0.89707 2 4.163295
13  3  0.56480 2 2.577572
14  4  0.82516 2 6.181288
15  5 -0.07298 2 0.794809
16  6  0.92487 2 0.925991
17  7  0.05092 2 0.109342
18  8  0.58145 2 1.116947
19  9 -1.50610 2 0.030605
20 10  5.32357 2 0.023148
21  1  1.39623 3 1.322324
22  2  0.87177 3 4.408423
23  3  0.70416 3 1.658296
24  4  1.31604 3 2.430068
25  5 -0.14503 3 0.201259
26  6  0.85298 3 1.088659
27  7  0.05903 3 0.081374
28  8  0.97278 3 0.399049
29  9 -6.26479 3 0.001769
30 10  1.94585 3 0.173258
>
> # additive subject random effect works
>   lmer1 <- lmer(y ~g - 1 + (1|id), data=data, weights=w)
>
> # multiplicative subject random effect fails
> # model is effectively: y ~(g - 1) * (k|id) [where * indicates times not factor crossing]
>
>   (start <- c(coef(lm1 <- lm(y ~g - 1, data=data, weights=w)), k=1))
    g1     g2     g3      k
0.8084 0.7878 0.9923 1.0000
>   moma <- with(data, model.matrix(~g - 1))
>   data <- cbind(data, moma)
>
>   nlmerfn <- function(g1,g2,g3,k) {
+     gk <- apply(cbind(g1,g2,g3) * moma, 1, sum) * k
+     attr(gk, 'gradient') <- cbind(moma * k, k=gk)
+     print(gk) # for monitoring
+     gk
+   }
>
>   nlmer1 <- nlmer(y ~ nlmerfn(g1,g2,g3,k) ~ g1+g2+g3+k + (k|id), data=data, weights=w, start=start)

 [1] 0.8084 0.8084 0.8084 0.8084 0.8084 0.8084 0.8084 0.8084 0.8084 0.8084 0.7878 0.7878 0.7878 0.7878 0.7878 0.7878 0.7878 0.7878 0.7878 0.7878 0.9923 0.9923 0.9923
[24] 0.9923 0.9923 0.9923 0.9923 0.9923 0.9923 0.9923
attr(,"gradient")
   g1 g2 g3      k
1   1  0  0 0.8084
2   1  0  0 0.8084
3   1  0  0 0.8084
4   1  0  0 0.8084
5   1  0  0 0.8084
6   1  0  0 0.8084
7   1  0  0 0.8084
8   1  0  0 0.8084
9   1  0  0 0.8084
10  1  0  0 0.8084
11  0  1  0 0.7878
12  0  1  0 0.7878
13  0  1  0 0.7878
14  0  1  0 0.7878
15  0  1  0 0.7878
16  0  1  0 0.7878
17  0  1  0 0.7878
18  0  1  0 0.7878
19  0  1  0 0.7878
20  0  1  0 0.7878
21  0  0  1 0.9923
22  0  0  1 0.9923
23  0  0  1 0.9923
24  0  0  1 0.9923
25  0  0  1 0.9923
26  0  0  1 0.9923
27  0  0  1 0.9923
28  0  0  1 0.9923
29  0  0  1 0.9923
30  0  0  1 0.9923
Error in initializePtr() : Downdated VtV is not positive definite
>
> # multiplicative subject fixed effects for info
>   data$ey <- fitted(lm1)
>   (fe <- c(by(data, data$id, function(z) weighted.mean(z$y/z$ey, z$w))))
      1       2       3       4       5       6       7       8       9      10
 1.8810  1.0925  0.8193  0.9796  0.2187  1.1103  0.4286  0.7753 -0.9618  2.6167

---
Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk> Phone +44(0)20 7905 2666 Fax +44(0)20 7905 2381
Population, Policy and Practice Programme
UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK


From: Ben Bolker <bbolker at gmail.com<mailto:bbolker at gmail.com>>
Date: Friday, 4 November 2016 19:04
To: Tim Cole <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk>>, "r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>" <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>>
Subject: Re: FW: [R-sig-ME] Exponent random effect in nlmer

  [cc'ing to r-sig-mixed-models]

  OK, I'll put this back on the queue ... is there any chance that you
can send me/us a reproducible example?

  The error message is driven from merPredD::updateDecomp in
src/predModule.cpp (line 291), in the package C++ code.  This makes it
considerably harder to debug.  There is no easily passable debug flag,
but if you download the source code and set debug=1 in line 258, you
will get at least a little bit more information about progress.

  As for what the code is actually doing in the updateDecomp step
("update L, RZX, and RX"), your best bet is probably to look at
vignette("lmer",package="lme4"), eqs 48-52 and the code after it ... not
that it's easy ...

On 16-11-04 11:06 AM, Cole, Tim wrote:
Dear Ben,
I emailed you a while ago about this error message I was getting in
nlmer: *Error in initializePtr() : Downdated VtV is not positive
definite.* I'd be really grateful if you could respond - it's holding me
up and I'd like to understand what is happening.
Do you have any thoughts on what might be causing it? I can't find the
code for initializePtr ? is it accessible? Which matrix is VtV? Is it
possible to set a breakpoint to track it?
Any pointers would be really helpful?
Best wishes,
Tim
---
Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk> <mailto:Tim.cole at ucl.ac.uk> Phone +44(0)20 7905 2666
Fax +44(0)20 7905 2381
Population, Policy and Practice Programme
UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK
From: Tim Cole <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk> <mailto:tim.cole at ucl.ac.uk>>
Date: Friday, 14 October 2016 14:41
To: Ben Bolker <bbolker at gmail.com<mailto:bbolker at gmail.com> <mailto:bbolker at gmail.com>>
Cc: "r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
<mailto:r-sig-mixed-models at r-project.org>"
<r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org> <mailto:r-sig-mixed-models at r-project.org>>
Subject: Re: [R-sig-ME] Exponent random effect in nlmer
     Dear Ben,
     I have a model that can be written as:
     nlmer(y ~ (g - 1) ^ exp(k | id) , data=data, weights=w)   (using a
     invalid but I hope obvious notation)
     where
     ? -Inf < y < Inf
     ? 0 < E(y) < 1
     ? g is a grouping factor
     ? k is a subject random effect
     k raises the group means to a subject-specific power while
     respecting the constraint on E(y).
     I've coded it in nlmer, but it gives *Error in initializePtr() :
     Downdated VtV is not positive definite*
     You have previously indicated that this can arise from badly
     rescaled parameters, so I'm wondering where I've gone wrong.
     Here is a simple example with 3 groups.
     > dim(data)
     [1] 759   4
     >
     > head(data)
         id      y     g      w
     1 1021 2.9968 01.02 0.2650
     2 1022 0.8592 01.02 0.3931
     3 1023 3.4657 01.02 0.2650
     4 1024 0.3989 01.02 2.3648
     5 1025 1.7230 01.02 0.2219
     6 1026 0.7451 01.02 0.7046
     >
     > summary(moma <- with(data, model.matrix(~g - 1)))
          g01.02          g01.03          g02.03
      Min.   :0.000   Min.   :0.000   Min.   :0.000
      1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.000
      Median :0.000   Median :0.000   Median :0.000
      Mean   :0.333   Mean   :0.335   Mean   :0.332
      3rd Qu.:1.000   3rd Qu.:1.000   3rd Qu.:1.000
      Max.   :1.000   Max.   :1.000   Max.   :1.000
     >
     > (start <- c(coef(lm(y ~g - 1, data=data, weights=w)), k=0))
     g01.02 g01.03 g02.03      k
     0.7979 0.6355 0.9056 0.0000
     >
     > data <- cbind(data, moma)
     > nlmerfn <- function(g01.02,g01.03,g02.03, k) {
     +     gc <- cbind(g01.02,g01.03,g02.03)
     +     gc <- as.vector(as.matrix(gc * moma) %*% rep(1, ncol(moma)))
     +     gc[gc <= 0] <- 1e-6
     +     gk <- gc ^ exp(k)
     +     grad <- moma * gk / gc * exp(k)
     +     grad <- cbind(grad, k=gk * log(gk))
     +     attr(gk, 'gradient') <- grad
     +     gk
     + }
     > nlmer1 <- nlmer(y ~ nlmerfn(g01.02,g01.03,g02.03, k) ~
     +                     g01.02+g01.03+g02.03 + k + (k | id),
     data=data, weights=w, start=start)
     Error in initializePtr() : Downdated VtV is not positive definite
     Perhaps I need to include an intercept of some sort, but I'd very
     much value your thoughts.
     Best wishes,
     Tim
     ---
     Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk> <mailto:Tim.cole at ucl.ac.uk> Phone +44(0)20 7905
     2666 Fax +44(0)20 7905 2381
     Population, Policy and Practice Programme
     UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK
     From: Thierry Onkelinx <thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>
     <mailto:thierry.onkelinx at inbo.be>>
     Date: Wednesday, 12 October 2016 09:26
     To: Tim Cole <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk> <mailto:tim.cole at ucl.ac.uk>>
     Cc: "r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
     <mailto:r-sig-mixed-models at r-project.org>"
     <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
     <mailto:r-sig-mixed-models at r-project.org>>
     Subject: Re: [R-sig-ME] Exponent random effect in nlmer
         Hi Tim,
         AFAIK nlmer requires the fixed and random effects to be
         additive. The model to be used _after_ this this summation can
         be non linear.
         Best regards,
         ir. Thierry Onkelinx
         Instituut voor natuur- en bosonderzoek / Research Institute for
         Nature and Forest
         team Biometrie & Kwaliteitszorg / team Biometrics & Quality
         Assurance
         Kliniekstraat 25
         1070 Anderlecht
         Belgium
         To call in the statistician after the experiment is done may be
         no more than asking him to perform a post-mortem examination: he
         may be able to say what the experiment died of. ~ Sir Ronald
         Aylmer Fisher
         The plural of anecdote is not data. ~ Roger Brinner
         The combination of some data and an aching desire for an answer
         does not ensure that a reasonable answer can be extracted from a
         given body of data. ~ John Tukey
         2016-10-11 12:30 GMT+02:00 Cole, Tim <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk>
         <mailto:tim.cole at ucl.ac.uk>>:
             Dear Thierry,
             Thanks very much for your speedy response.
             I agree my model looks odd, but it has a theoretical basis
             which I'd prefer not to spell out at this stage. Suffice to
             say that
             ? -Inf < y < Inf
             ? 0 < E(y) < 1
             ? there is a subject random effect.
             For these reasons the usual models and/or transformations
             won't work, whereas my proposed exponent random effect ought
             to. I just need to fit it, to see if I'm right!
             Best wishes,
             Tim
             ---
             Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk> <mailto:Tim.cole at ucl.ac.uk> Phone
             +44(0)20 7905 2666 <tel:%2B44%280%2920%207905%202666> Fax
             +44(0)20 7905 2381 <tel:%2B44%280%2920%207905%202381>
             Population, Policy and Practice Programme
             UCL Great Ormond Street Institute of Child Health, London
             WC1N 1EH, UK
             From: Thierry Onkelinx <thierry.onkelinx at inbo.be<mailto:thierry.onkelinx at inbo.be>
             <mailto:thierry.onkelinx at inbo.be>>
             Date: Tuesday, 11 October 2016 11:06
             To: Tim Cole <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk> <mailto:tim.cole at ucl.ac.uk>>
             Cc: "r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
             <mailto:r-sig-mixed-models at r-project.org>"
             <r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
             <mailto:r-sig-mixed-models at r-project.org>>
             Subject: Re: [R-sig-ME] Exponent random effect in nlmer
                 Dear Tim,
                 y centred on 0 and a valid range (0, 1) seems to be
                 conflicting statements.
                 Here a some solutions depending on y
                 - y stems from a binomial process
                      - use a binomial glmm.
                 - y is continuous and you are willing to transform y
                     - 0 < y <  1
                         - apply a logit transformation on
                 y. lmer(plogis(y) ~ f + (1 | id) )
                     - 0 <= y < 1
                         - apply a log transformation on y. lmer(log(y) ~
                 f + (1 | id) )
                     - 0 < y <= 1
                         - apply a log transformation on 1 -
                 y. lmer(log(1 - y) ~ f + (1 | id) )
                 - y is continuous are not willing to transform y
                    - use a beta regression with 0 and/or 1 inflation in
                 case you have 0 or 1 in the data. Have a look at the
                 gamlss package to fit this model.
                 Best regards,
                 ir. Thierry Onkelinx
                 Instituut voor natuur- en bosonderzoek / Research
                 Institute for Nature and Forest
                 team Biometrie & Kwaliteitszorg / team Biometrics &
                 Quality Assurance
                 Kliniekstraat 25
                 1070 Anderlecht
                 Belgium
                 To call in the statistician after the experiment is done
                 may be no more than asking him to perform a post-mortem
                 examination: he may be able to say what the experiment
                 died of. ~ Sir Ronald Aylmer Fisher
                 The plural of anecdote is not data. ~ Roger Brinner
                 The combination of some data and an aching desire for an
                 answer does not ensure that a reasonable answer can be
                 extracted from a given body of data. ~ John Tukey
                 2016-10-11 11:29 GMT+02:00 Cole, Tim <tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk>
                 <mailto:tim.cole at ucl.ac.uk>>:
                     I have a model of the form
                       m1 <- lmer(y ~ f + (1 | id) )
                     where y is a continuous variable centred on zero, f
                     is a unordered factor with coefficients b such 0 < b
                     < 1, and there is a signficant random subject intercept.
                     The random intercept can lead to predicted values
                     outside the valid range (0, 1). For this reason I'd
                     like to reformulate the model as
                     m2 <- nlmer(y ~ (f - 1) ^ exp(1 | id) )   (using a
                     invalid but I hope obvious notation), where the
                     random effect is now a power centred on 1. This
                     would constrain the fitted values to be within c(0, 1).
                     My question is: can this be done in nlmer, and if so
                     how? Please can someone point me in the right direction?
                     Thanks,
                     Tim Cole
                     ---
                     Tim.cole at ucl.ac.uk<mailto:Tim.cole at ucl.ac.uk>
                     <mailto:Tim.cole at ucl.ac.uk><mailto:Tim.cole at ucl.ac.uk <mailto:Tim.cole at ucl.ac.uk>>
                     Phone +44(0)20 7905 2666
                     <tel:%2B44%280%2920%207905%202666> Fax +44(0)20 7905
                     2381 <tel:%2B44%280%2920%207905%202381>
                     Population, Policy and Practice Programme
                     UCL Great Ormond Street Institute of Child Health,
                     London WC1N 1EH, UK
                             [[alternative HTML version deleted]]
                     _______________________________________________
                     R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org>
                     <mailto:R-sig-mixed-models at r-project.org> mailing list
                     https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
                     <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>


	[[alternative HTML version deleted]]


From tasmacetus at gmail.com  Mon Nov  7 15:17:13 2016
From: tasmacetus at gmail.com (Luciana Motta)
Date: Mon, 7 Nov 2016 15:17:13 +0100
Subject: [R-sig-ME] Poisson or Gaussian when modelling count data +
 heteroscedasticity in predictor variables
Message-ID: <CAPKq9_vPebKw8upM--eo4Higyn--YDJ4K5Fbycjj4K2SbJkvSA@mail.gmail.com>

Hello,

my name is Lucy, and I'm studying richness of aquatic insect in lakes. I
took samples from different habitats in each lake, for which I though of a
mixed model with my predictors as fixed effects, and lake/habitat as random
effects. I did a model using "glmer", to be able to use Poisson
distribution for residuals, due to my type of response variable (count data
-richness).

But studying the data graphically, I suspected variance heterogeneity in 2
predictors.

I continued doing model selection with glmer with Poisson distribution, but
also made a model using "lmer" (therefore, Gaussian distribution of
residuals), to be able to model variance heterogeneity of those predictors
and see if models fit better with it.

Finally, yes..."lmer" model, with Gaussian distribution and varExp
modelling for the variance of those predictors seem much more adequate than
the "glmer" with Poisson (conclusion I arrived to by studying residuals,
fitted values, qqplot and normality tests).

Can heteroscedasticity be a larger problem to be accounted for, than the
distribution of the errors for count data? I read that sometimes
heteroscedasticity
can be masking what we think is a normality problem. Also that Poisson
distribution accounts for heteroscedasticity....but in my case, model seems
much worse. Is just that since Poisson, Neg.Binom. etc., is so recommended
for count data, that I don't really know if I'm plain wrong in even
considering staying with Gaussian. Any suggestions/further readings about
this?

Thank you very much,

-- 
Luciana M. Motta
Licenciada en Cs. Biol?gicas FCEyN, U.B.A.

	[[alternative HTML version deleted]]


From Tom.Wilding at sams.ac.uk  Mon Nov  7 16:56:43 2016
From: Tom.Wilding at sams.ac.uk (Tom Wilding)
Date: Mon, 7 Nov 2016 15:56:43 +0000
Subject: [R-sig-ME] Poisson or Gaussian when modelling count data +
 heteroscedasticity in predictor variables
In-Reply-To: <CAPKq9_vPebKw8upM--eo4Higyn--YDJ4K5Fbycjj4K2SbJkvSA@mail.gmail.com>
References: <CAPKq9_vPebKw8upM--eo4Higyn--YDJ4K5Fbycjj4K2SbJkvSA@mail.gmail.com>
Message-ID: <AM5PR0202MB2499B90D13F0E1A939EEA6D6C4A70@AM5PR0202MB2499.eurprd02.prod.outlook.com>

Hi Luciana - if your count data is large (not 'near' zero) then the normal model might be fine and you could then account for heteroscedasticity using GLS (as you seem to have done) - if the model diagnostics check-out then it should be OK.  You could log or log+1 transform your response and see how that looks too, just for interest (if you have zeros then this approach is not likely to be successful).  Also, you could stick with the Poisson GLMM and include an individual observation level term (Elston, D. A., et al. (2001). "Analysis of aggregation, a worked example: numbers of ticks on red grouse chicks." Parasitology 122(05): 563-569), - this should also be reasonable (and is very easy to implement) and might be of interest (though may have its detractors).  Your negative binomial solution should also address the over-dispersion issue though I'm less sure what the residual patterns should look like (you could fake-up some data to check these).

Best

Tom.



-----Original Message-----
From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org] On Behalf Of Luciana Motta
Sent: 07 November 2016 14:17
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] Poisson or Gaussian when modelling count data + heteroscedasticity in predictor variables

Hello,

my name is Lucy, and I'm studying richness of aquatic insect in lakes. I took samples from different habitats in each lake, for which I though of a mixed model with my predictors as fixed effects, and lake/habitat as random effects. I did a model using "glmer", to be able to use Poisson distribution for residuals, due to my type of response variable (count data -richness).

But studying the data graphically, I suspected variance heterogeneity in 2 predictors.

I continued doing model selection with glmer with Poisson distribution, but also made a model using "lmer" (therefore, Gaussian distribution of residuals), to be able to model variance heterogeneity of those predictors and see if models fit better with it.

Finally, yes..."lmer" model, with Gaussian distribution and varExp modelling for the variance of those predictors seem much more adequate than the "glmer" with Poisson (conclusion I arrived to by studying residuals, fitted values, qqplot and normality tests).

Can heteroscedasticity be a larger problem to be accounted for, than the distribution of the errors for count data? I read that sometimes heteroscedasticity can be masking what we think is a normality problem. Also that Poisson distribution accounts for heteroscedasticity....but in my case, model seems much worse. Is just that since Poisson, Neg.Binom. etc., is so recommended for count data, that I don't really know if I'm plain wrong in even considering staying with Gaussian. Any suggestions/further readings about this?

Thank you very much,

--
Luciana M. Motta
Licenciada en Cs. Biol?gicas FCEyN, U.B.A.

[[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
The Scottish Association for Marine Science (SAMS) is registered in Scotland as a Company Limited by Guarantee (SC009292) and is a registered charity (9206). SAMS has two actively trading wholly owned subsidiary companies: SAMS Research Services Ltd (SC224404) and SAMS Ltd (SC306912). All Companies in the group are registered in Scotland and share a registered office at Scottish Marine Institute, Oban Argyll PA37 1QA. The content of this message may contain personal views which are not the views of SAMS unless specifically stated. Please note that all email traffic is monitored for purposes of security and spam filtering. As such individual emails may be examined in more detail.

From tasmacetus at gmail.com  Mon Nov  7 17:28:01 2016
From: tasmacetus at gmail.com (Luciana Motta)
Date: Mon, 7 Nov 2016 17:28:01 +0100
Subject: [R-sig-ME] Poisson or Gaussian when modelling count data +
 heteroscedasticity in predictor variables
In-Reply-To: <AM5PR0202MB2499B90D13F0E1A939EEA6D6C4A70@AM5PR0202MB2499.eurprd02.prod.outlook.com>
References: <CAPKq9_vPebKw8upM--eo4Higyn--YDJ4K5Fbycjj4K2SbJkvSA@mail.gmail.com>
	<AM5PR0202MB2499B90D13F0E1A939EEA6D6C4A70@AM5PR0202MB2499.eurprd02.prod.outlook.com>
Message-ID: <CAPKq9_uaQDBd32CE_fpNmoJTb2aTZp20K3GXASM5Mvx1FsWBHw@mail.gmail.com>

Thank you Tom.

My richness data go from 5 to 20, don't think that applies to "large". But
the model diagnostics check do look good.

Model does not check good with neg binom, just like with Poisson (there is
no overdisperson anyways)

Will check about the individual observation level term.

Any other reading suggestions about heteroscedasticity vs normality?

Thank you again




On Mon, Nov 7, 2016 at 4:56 PM, Tom Wilding <Tom.Wilding at sams.ac.uk> wrote:

> Hi Luciana - if your count data is large (not 'near' zero) then the normal
> model might be fine and you could then account for heteroscedasticity using
> GLS (as you seem to have done) - if the model diagnostics check-out then it
> should be OK.  You could log or log+1 transform your response and see how
> that looks too, just for interest (if you have zeros then this approach is
> not likely to be successful).  Also, you could stick with the Poisson GLMM
> and include an individual observation level term (Elston, D. A., et al.
> (2001). "Analysis of aggregation, a worked example: numbers of ticks on red
> grouse chicks." Parasitology 122(05): 563-569), - this should also be
> reasonable (and is very easy to implement) and might be of interest (though
> may have its detractors).  Your negative binomial solution should also
> address the over-dispersion issue though I'm less sure what the residual
> patterns should look like (you could fake-up some data to check these).
>
> Best
>
> Tom.
>
>
>
> -----Original Message-----
> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org]
> On Behalf Of Luciana Motta
> Sent: 07 November 2016 14:17
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] Poisson or Gaussian when modelling count data +
> heteroscedasticity in predictor variables
>
> Hello,
>
> my name is Lucy, and I'm studying richness of aquatic insect in lakes. I
> took samples from different habitats in each lake, for which I though of a
> mixed model with my predictors as fixed effects, and lake/habitat as random
> effects. I did a model using "glmer", to be able to use Poisson
> distribution for residuals, due to my type of response variable (count data
> -richness).
>
> But studying the data graphically, I suspected variance heterogeneity in 2
> predictors.
>
> I continued doing model selection with glmer with Poisson distribution,
> but also made a model using "lmer" (therefore, Gaussian distribution of
> residuals), to be able to model variance heterogeneity of those predictors
> and see if models fit better with it.
>
> Finally, yes..."lmer" model, with Gaussian distribution and varExp
> modelling for the variance of those predictors seem much more adequate than
> the "glmer" with Poisson (conclusion I arrived to by studying residuals,
> fitted values, qqplot and normality tests).
>
> Can heteroscedasticity be a larger problem to be accounted for, than the
> distribution of the errors for count data? I read that sometimes
> heteroscedasticity can be masking what we think is a normality problem.
> Also that Poisson distribution accounts for heteroscedasticity....but in my
> case, model seems much worse. Is just that since Poisson, Neg.Binom. etc.,
> is so recommended for count data, that I don't really know if I'm plain
> wrong in even considering staying with Gaussian. Any suggestions/further
> readings about this?
>
> Thank you very much,
>
> --
> Luciana M. Motta
> Licenciada en Cs. Biol?gicas FCEyN, U.B.A.
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> The Scottish Association for Marine Science (SAMS) is registered in
> Scotland as a Company Limited by Guarantee (SC009292) and is a registered
> charity (9206). SAMS has two actively trading wholly owned subsidiary
> companies: SAMS Research Services Ltd (SC224404) and SAMS Ltd (SC306912).
> All Companies in the group are registered in Scotland and share a
> registered office at Scottish Marine Institute, Oban Argyll PA37 1QA. The
> content of this message may contain personal views which are not the views
> of SAMS unless specifically stated. Please note that all email traffic is
> monitored for purposes of security and spam filtering. As such individual
> emails may be examined in more detail.
>



-- 
Luciana M. Motta
Licenciada en Cs. Biol?gicas FCEyN, U.B.A.
CENAC (Parque Nacional Nahuel Huapi) - CONICET
Argentina
www.cenacbariloche.com.ar

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Mon Nov  7 17:36:00 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 7 Nov 2016 11:36:00 -0500
Subject: [R-sig-ME] Poisson or Gaussian when modelling count data +
 heteroscedasticity in predictor variables
In-Reply-To: <CAPKq9_uaQDBd32CE_fpNmoJTb2aTZp20K3GXASM5Mvx1FsWBHw@mail.gmail.com>
References: <CAPKq9_vPebKw8upM--eo4Higyn--YDJ4K5Fbycjj4K2SbJkvSA@mail.gmail.com>
	<AM5PR0202MB2499B90D13F0E1A939EEA6D6C4A70@AM5PR0202MB2499.eurprd02.prod.outlook.com>
	<CAPKq9_uaQDBd32CE_fpNmoJTb2aTZp20K3GXASM5Mvx1FsWBHw@mail.gmail.com>
Message-ID: <47d7bd4c-8d10-cb15-56b1-2328fe35441d@gmail.com>


  Do you know if you have over- or underdispersion?  For species
richness, if there is a relatively restricted regional species pool,
species richness can actually be *underdispersed*, which would also make
Poisson and negative binomial be poor fits.

  Generally, heteroscedasticity is a bigger problem than Normality
(sorry, don't have a reference handy), so if you can deal with the
heteroscedasticity effectively with a linear model, I would say go for it.


On 16-11-07 11:28 AM, Luciana Motta wrote:
> Thank you Tom.
> 
> My richness data go from 5 to 20, don't think that applies to "large". But
> the model diagnostics check do look good.
> 
> Model does not check good with neg binom, just like with Poisson (there is
> no overdisperson anyways)
> 
> Will check about the individual observation level term.
> 
> Any other reading suggestions about heteroscedasticity vs normality?
> 
> Thank you again
> 
> On Mon, Nov 7, 2016 at 4:56 PM, Tom Wilding <Tom.Wilding at sams.ac.uk> wrote:
> 
>> Hi Luciana - if your count data is large (not 'near' zero) then the normal
>> model might be fine and you could then account for heteroscedasticity using
>> GLS (as you seem to have done) - if the model diagnostics check-out then it
>> should be OK.  You could log or log+1 transform your response and see how
>> that looks too, just for interest (if you have zeros then this approach is
>> not likely to be successful).  Also, you could stick with the Poisson GLMM
>> and include an individual observation level term (Elston, D. A., et al.
>> (2001). "Analysis of aggregation, a worked example: numbers of ticks on red
>> grouse chicks." Parasitology 122(05): 563-569), - this should also be
>> reasonable (and is very easy to implement) and might be of interest (though
>> may have its detractors).  Your negative binomial solution should also
>> address the over-dispersion issue though I'm less sure what the residual
>> patterns should look like (you could fake-up some data to check these).
>>
>> Best
>>
>> Tom.
>>
>>
>>
>> -----Original Message-----
>> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org]
>> On Behalf Of Luciana Motta
>> Sent: 07 November 2016 14:17
>> To: r-sig-mixed-models at r-project.org
>> Subject: [R-sig-ME] Poisson or Gaussian when modelling count data +
>> heteroscedasticity in predictor variables
>>
>> Hello,
>>
>> my name is Lucy, and I'm studying richness of aquatic insect in lakes. I
>> took samples from different habitats in each lake, for which I though of a
>> mixed model with my predictors as fixed effects, and lake/habitat as random
>> effects. I did a model using "glmer", to be able to use Poisson
>> distribution for residuals, due to my type of response variable (count data
>> -richness).
>>
>> But studying the data graphically, I suspected variance heterogeneity in 2
>> predictors.
>>
>> I continued doing model selection with glmer with Poisson distribution,
>> but also made a model using "lmer" (therefore, Gaussian distribution of
>> residuals), to be able to model variance heterogeneity of those predictors
>> and see if models fit better with it.
>>
>> Finally, yes..."lmer" model, with Gaussian distribution and varExp
>> modelling for the variance of those predictors seem much more adequate than
>> the "glmer" with Poisson (conclusion I arrived to by studying residuals,
>> fitted values, qqplot and normality tests).
>>
>> Can heteroscedasticity be a larger problem to be accounted for, than the
>> distribution of the errors for count data? I read that sometimes
>> heteroscedasticity can be masking what we think is a normality problem.
>> Also that Poisson distribution accounts for heteroscedasticity....but in my
>> case, model seems much worse. Is just that since Poisson, Neg.Binom. etc.,
>> is so recommended for count data, that I don't really know if I'm plain
>> wrong in even considering staying with Gaussian. Any suggestions/further
>> readings about this?
>>
>> Thank you very much,
>>
>> --
>> Luciana M. Motta
>> Licenciada en Cs. Biol?gicas FCEyN, U.B.A.
>>
>> [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> The Scottish Association for Marine Science (SAMS) is registered in
>> Scotland as a Company Limited by Guarantee (SC009292) and is a registered
>> charity (9206). SAMS has two actively trading wholly owned subsidiary
>> companies: SAMS Research Services Ltd (SC224404) and SAMS Ltd (SC306912).
>> All Companies in the group are registered in Scotland and share a
>> registered office at Scottish Marine Institute, Oban Argyll PA37 1QA. The
>> content of this message may contain personal views which are not the views
>> of SAMS unless specifically stated. Please note that all email traffic is
>> monitored for purposes of security and spam filtering. As such individual
>> emails may be examined in more detail.
>>
> 
> 
>


From arives at wisc.edu  Mon Nov  7 17:49:42 2016
From: arives at wisc.edu (Anthony R. Ives)
Date: Mon, 07 Nov 2016 16:49:42 +0000
Subject: [R-sig-ME] Poisson or Gaussian when modelling count data +
 heteroscedasticity in predictor variables
In-Reply-To: <CAPKq9_uaQDBd32CE_fpNmoJTb2aTZp20K3GXASM5Mvx1FsWBHw@mail.gmail.com>
References: <CAPKq9_vPebKw8upM--eo4Higyn--YDJ4K5Fbycjj4K2SbJkvSA@mail.gmail.com>
	<AM5PR0202MB2499B90D13F0E1A939EEA6D6C4A70@AM5PR0202MB2499.eurprd02.prod.outlook.com>
	<CAPKq9_uaQDBd32CE_fpNmoJTb2aTZp20K3GXASM5Mvx1FsWBHw@mail.gmail.com>
Message-ID: <CD8F0E68-4877-4E65-AB95-4F77BFE86D08@wisc.edu>

Luciana,

Although it is always hard to say, 5 to 20 is not necessarily "small". Really, it is all about the diagnostics, which you say point to using a linear model. In your original email you made a distinction heteroscedasticity and "count data". This really isn't a distinction, because the main statistical thing GLMs do is to account for heteroscedasticity being driven by different sampling distributions (i.e., the variance-mean relationship). If you have enough data points to identify heteroscedasticity, then I think the heteroscedasticity should be your focus. Note, however, that estimating heteroscedasticity and incorporating this into your analysis can be problematic for small counts. You are very right to worry about heteroscedasticity in general, which can play havoc with type I error.

Linear models (with GLS to account for heteroscedasticity) can have fine performance in terms of type I errors. There can be a loss of power, but not always. You might find a recent paper useful: Warton et al. 2016  http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12552/abstract

Cheers, Tony


-----Original Message-----
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of Luciana Motta <tasmacetus at gmail.com>
Date: Monday, November 7, 2016 at 10:28 AM
To: Tom Wilding <Tom.Wilding at sams.ac.uk>
Cc: "r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] Poisson or Gaussian when modelling count data + heteroscedasticity in predictor variables

    Thank you Tom.
    
    My richness data go from 5 to 20, don't think that applies to "large". But
    the model diagnostics check do look good.
    
    Model does not check good with neg binom, just like with Poisson (there is
    no overdisperson anyways)
    
    Will check about the individual observation level term.
    
    Any other reading suggestions about heteroscedasticity vs normality?
    
    Thank you again
    
    
    
    
    On Mon, Nov 7, 2016 at 4:56 PM, Tom Wilding <Tom.Wilding at sams.ac.uk> wrote:
    
    > Hi Luciana - if your count data is large (not 'near' zero) then the normal
    > model might be fine and you could then account for heteroscedasticity using
    > GLS (as you seem to have done) - if the model diagnostics check-out then it
    > should be OK.  You could log or log+1 transform your response and see how
    > that looks too, just for interest (if you have zeros then this approach is
    > not likely to be successful).  Also, you could stick with the Poisson GLMM
    > and include an individual observation level term (Elston, D. A., et al.
    > (2001). "Analysis of aggregation, a worked example: numbers of ticks on red
    > grouse chicks." Parasitology 122(05): 563-569), - this should also be
    > reasonable (and is very easy to implement) and might be of interest (though
    > may have its detractors).  Your negative binomial solution should also
    > address the over-dispersion issue though I'm less sure what the residual
    > patterns should look like (you could fake-up some data to check these).
    >
    > Best
    >
    > Tom.
    >
    >
    >
    > -----Original Message-----
    > From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-project.org]
    > On Behalf Of Luciana Motta
    > Sent: 07 November 2016 14:17
    > To: r-sig-mixed-models at r-project.org
    > Subject: [R-sig-ME] Poisson or Gaussian when modelling count data +
    > heteroscedasticity in predictor variables
    >
    > Hello,
    >
    > my name is Lucy, and I'm studying richness of aquatic insect in lakes. I
    > took samples from different habitats in each lake, for which I though of a
    > mixed model with my predictors as fixed effects, and lake/habitat as random
    > effects. I did a model using "glmer", to be able to use Poisson
    > distribution for residuals, due to my type of response variable (count data
    > -richness).
    >
    > But studying the data graphically, I suspected variance heterogeneity in 2
    > predictors.
    >
    > I continued doing model selection with glmer with Poisson distribution,
    > but also made a model using "lmer" (therefore, Gaussian distribution of
    > residuals), to be able to model variance heterogeneity of those predictors
    > and see if models fit better with it.
    >
    > Finally, yes..."lmer" model, with Gaussian distribution and varExp
    > modelling for the variance of those predictors seem much more adequate than
    > the "glmer" with Poisson (conclusion I arrived to by studying residuals,
    > fitted values, qqplot and normality tests).
    >
    > Can heteroscedasticity be a larger problem to be accounted for, than the
    > distribution of the errors for count data? I read that sometimes
    > heteroscedasticity can be masking what we think is a normality problem.
    > Also that Poisson distribution accounts for heteroscedasticity....but in my
    > case, model seems much worse. Is just that since Poisson, Neg.Binom. etc.,
    > is so recommended for count data, that I don't really know if I'm plain
    > wrong in even considering staying with Gaussian. Any suggestions/further
    > readings about this?
    >
    > Thank you very much,
    >
    > --
    > Luciana M. Motta
    > Licenciada en Cs. Biol?gicas FCEyN, U.B.A.
    >
    > [[alternative HTML version deleted]]
    >
    > _______________________________________________
    > R-sig-mixed-models at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
    > The Scottish Association for Marine Science (SAMS) is registered in
    > Scotland as a Company Limited by Guarantee (SC009292) and is a registered
    > charity (9206). SAMS has two actively trading wholly owned subsidiary
    > companies: SAMS Research Services Ltd (SC224404) and SAMS Ltd (SC306912).
    > All Companies in the group are registered in Scotland and share a
    > registered office at Scottish Marine Institute, Oban Argyll PA37 1QA. The
    > content of this message may contain personal views which are not the views
    > of SAMS unless specifically stated. Please note that all email traffic is
    > monitored for purposes of security and spam filtering. As such individual
    > emails may be examined in more detail.
    >
    
    
    
    -- 
    Luciana M. Motta
    Licenciada en Cs. Biol?gicas FCEyN, U.B.A.
    CENAC (Parque Nacional Nahuel Huapi) - CONICET
    Argentina
    www.cenacbariloche.com.ar
    
    	[[alternative HTML version deleted]]
    
    _______________________________________________
    R-sig-mixed-models at r-project.org mailing list
    https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From tasmacetus at gmail.com  Tue Nov  8 00:14:02 2016
From: tasmacetus at gmail.com (Luciana Motta)
Date: Tue, 8 Nov 2016 00:14:02 +0100
Subject: [R-sig-ME] underdispersion, (Poisson vs Gaussian)
Message-ID: <CAPKq9_tDi8vgUn8cyEF+strhLi7gm0LuZW3A8H7MTvWzBfjw+A@mail.gmail.com>

Dear Ben Bolker

as you correctly aim, I checked dispersIon parameter, and I actually have
underdispersion:

verdisp_fun(Model.Poiss)

chisq      ratio          p
16.3184448  0.4594328  0.9991835

In this regard, I checked collineality of variable or outliers, something
Zuur (2009) points that could be a reason for this, and that is not serious
in my data either.

As regards Mr Ives comments: I have 6 richness values in each lake, thru 8
lakes: 48 observations. Though I understand what you mention about main
statistical use of GLMs to account for heteroscedasticity, the use of
gaussian instead of Poisson would mean it will treat my response variable
as "continuous" when it is not...and that is something I don't complete to
understand. Sorry, I probably have more studying to do in that regard. Will
check Warton article.

Many thanks!
Luciana

On Mon, Nov 7, 2016 at 5:49 PM, Anthony R. Ives <arives at wisc.edu> wrote:

> Luciana,
>
> Although it is always hard to say, 5 to 20 is not necessarily "small".
> Really, it is all about the diagnostics, which you say point to using a
> linear model. In your original email you made a distinction
> heteroscedasticity and "count data". This really isn't a distinction,
> because the main statistical thing GLMs do is to account for
> heteroscedasticity being driven by different sampling distributions (i.e.,
> the variance-mean relationship. If you have enough data points to identify
> heteroscedasticity, then I think the heteroscedasticity should be your
> focus. Note, however, that estimating heteroscedasticity and incorporating
> this into your analysis can be problematic for small counts. You are very
> right to worry about heteroscedasticity in general, which can play havoc
> with type I error.
>
> Linear models (with GLS to account for heteroscedasticity) can have fine
> performance in terms of type I errors. There can be a loss of power, but
> not always. You might find a recent paper useful: Warton et al. 2016
> http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12552/abstract
>
> Cheers, Tony
>
>
> -----Original Message-----
> From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on
> behalf of Luciana Motta <tasmacetus at gmail.com>
> Date: Monday, November 7, 2016 at 10:28 AM
> To: Tom Wilding <Tom.Wilding at sams.ac.uk>
> Cc: "r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org>
> Subject: Re: [R-sig-ME] Poisson or Gaussian when modelling count data +
> heteroscedasticity in predictor variables
>
>     Thank you Tom.
>
>     My richness data go from 5 to 20, don't think that applies to "large".
> But
>     the model diagnostics check do look good.
>
>     Model does not check good with neg binom, just like with Poisson
> (there is
>     no overdisperson anyways)
>
>     Will check about the individual observation level term.
>
>     Any other reading suggestions about heteroscedasticity vs normality?
>
>     Thank you again
>
>
>
>
>     On Mon, Nov 7, 2016 at 4:56 PM, Tom Wilding <Tom.Wilding at sams.ac.uk>
> wrote:
>
>     > Hi Luciana - if your count data is large (not 'near' zero) then the
> normal
>     > model might be fine and you could then account for
> heteroscedasticity using
>     > GLS (as you seem to have done) - if the model diagnostics check-out
> then it
>     > should be OK.  You could log or log+1 transform your response and
> see how
>     > that looks too, just for interest (if you have zeros then this
> approach is
>     > not likely to be successful).  Also, you could stick with the
> Poisson GLMM
>     > and include an individual observation level term (Elston, D. A., et
> al.
>     > (2001). "Analysis of aggregation, a worked example: numbers of ticks
> on red
>     > grouse chicks." Parasitology 122(05): 563-569), - this should also be
>     > reasonable (and is very easy to implement) and might be of interest
> (though
>     > may have its detractors).  Your negative binomial solution should
> also
>     > address the over-dispersion issue though I'm less sure what the
> residual
>     > patterns should look like (you could fake-up some data to check
> these).
>     >
>     > Best
>     >
>     > Tom.
>     >
>     >
>     >
>     > -----Original Message-----
>     > From: R-sig-mixed-models [mailto:r-sig-mixed-models-
> bounces at r-project.org]
>     > On Behalf Of Luciana Motta
>     > Sent: 07 November 2016 14:17
>     > To: r-sig-mixed-models at r-project.org
>     > Subject: [R-sig-ME] Poisson or Gaussian when modelling count data +
>     > heteroscedasticity in predictor variables
>     >
>     > Hello,
>     >
>     > my name is Lucy, and I'm studying richness of aquatic insect in
> lakes. I
>     > took samples from different habitats in each lake, for which I
> though of a
>     > mixed model with my predictors as fixed effects, and lake/habitat as
> random
>     > effects. I did a model using "glmer", to be able to use Poisson
>     > distribution for residuals, due to my type of response variable
> (count data
>     > -richness).
>     >
>     > But studying the data graphically, I suspected variance
> heterogeneity in 2
>     > predictors.
>     >
>     > I continued doing model selection with glmer with Poisson
> distribution,
>     > but also made a model using "lmer" (therefore, Gaussian distribution
> of
>     > residuals), to be able to model variance heterogeneity of those
> predictors
>     > and see if models fit better with it.
>     >
>     > Finally, yes..."lmer" model, with Gaussian distribution and varExp
>     > modelling for the variance of those predictors seem much more
> adequate than
>     > the "glmer" with Poisson (conclusion I arrived to by studying
> residuals,
>     > fitted values, qqplot and normality tests).
>     >
>     > Can heteroscedasticity be a larger problem to be accounted for, than
> the
>     > distribution of the errors for count data? I read that sometimes
>     > heteroscedasticity can be masking what we think is a normality
> problem.
>     > Also that Poisson distribution accounts for
> heteroscedasticity....but in my
>     > case, model seems much worse. Is just that since Poisson, Neg.Binom.
> etc.,
>     > is so recommended for count data, that I don't really know if I'm
> plain
>     > wrong in even considering staying with Gaussian. Any
> suggestions/further
>     > readings about this?
>     >
>     > Thank you very much,
>     >
>     > --
>     > Luciana M. Motta
>     > Licenciada en Cs. Biol?gicas FCEyN, U.B.A.
>     >
>     > [[alternative HTML version deleted]]
>     >
>     > _______________________________________________
>     > R-sig-mixed-models at r-project.org mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>     > The Scottish Association for Marine Science (SAMS) is registered in
>     > Scotland as a Company Limited by Guarantee (SC009292) and is a
> registered
>     > charity (9206). SAMS has two actively trading wholly owned subsidiary
>     > companies: SAMS Research Services Ltd (SC224404) and SAMS Ltd
> (SC306912).
>     > All Companies in the group are registered in Scotland and share a
>     > registered office at Scottish Marine Institute, Oban Argyll PA37
> 1QA. The
>     > content of this message may contain personal views which are not the
> views
>     > of SAMS unless specifically stated. Please note that all email
> traffic is
>     > monitored for purposes of security and spam filtering. As such
> individual
>     > emails may be examined in more detail.
>     >
>
>
>
>     --
>     Luciana M. Motta
>     Licenciada en Cs. Biol?gicas FCEyN, U.B.A.
>     CENAC (Parque Nacional Nahuel Huapi) - CONICET
>     Argentina
>     www.cenacbariloche.com.ar
>
>         [[alternative HTML version deleted]]
>
>     _______________________________________________
>     R-sig-mixed-models at r-project.org mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>


-- 
Luciana M. Motta
Licenciada en Cs. Biol?gicas FCEyN, U.B.A.
CENAC (Parque Nacional Nahuel Huapi) - CONICET
Argentina
www.cenacbariloche.com.ar

	[[alternative HTML version deleted]]


From stephanie.periquet at gmail.com  Tue Nov  8 08:40:59 2016
From: stephanie.periquet at gmail.com (=?UTF-8?Q?St=c3=a9phanie_P=c3=a9riquet?=)
Date: Tue, 8 Nov 2016 09:40:59 +0200
Subject: [R-sig-ME] Mixed model for compositional data
Message-ID: <699cf5a6-8d40-835b-1d31-8c6148b7d09d@gmail.com>

Hi all,

I have repeated measures of individual diet composition (I actually have 
the number of each item eaten and the duration of each observation) and 
I would like to model the effect of variables such as season and 
moonlight on diet composition (not the actual number of item eaten).
I gathered that I should use compositional but the packed 'compostions' 
doesn't allow the inclusion of random factors.

Does anyone has experience whit such data and analyses and point me in 
the right direction?

Thanks a lot in advance for your replies!

Regards,
St?phanie
-- 
*St?phanie PERIQUET (PhD) * - Bat-Eared Fox Research Project
/Dept of Zoology & Entomology/
/University of the Free State, Qwaqwa Campus/
/Private Bag X13/
/9866 Phuthaditjhaba, Free State/
/South Africa/
*Cell: +27 79 570 2683*
ResearchGate <https://www.researchgate.net/profile/Stephanie_Periquet> & 
Twitter <https://twitter.com/ShumbAfrica>

Kalahari bat-eared foxes on Twitter <https://twitter.com/kal_batearedfox>

//////

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Tue Nov  8 09:31:18 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Tue, 8 Nov 2016 09:31:18 +0100
Subject: [R-sig-ME] Mixed model for compositional data
In-Reply-To: <699cf5a6-8d40-835b-1d31-8c6148b7d09d@gmail.com>
References: <699cf5a6-8d40-835b-1d31-8c6148b7d09d@gmail.com>
Message-ID: <CAJuCY5yEY5JY2T5FyrJyjp6wgKmfhbxh0746SqTMO8ZgD27Lnw@mail.gmail.com>

Dear St?phanie,

A workaround would be to model the numbers per item and use the (log of)
the total number per night as an offset. Then the model parameters are
essentially ratio's.

no effects of season and moonlite: number ~ offset(log(total)) + item +
(1|animal)
only season: number ~ offset(log(total)) + item*season + (1|animal)
season:moonlight: number ~ offset(log(total)) + item*season*moonlight +
(1|animal)

The model fit will not be guaranteed to sum to 1 per night and animal.

Best regards,

Thierry

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-11-08 8:40 GMT+01:00 St?phanie P?riquet <stephanie.periquet at gmail.com>:

> Hi all,
>
> I have repeated measures of individual diet composition (I actually have
> the number of each item eaten and the duration of each observation) and
> I would like to model the effect of variables such as season and
> moonlight on diet composition (not the actual number of item eaten).
> I gathered that I should use compositional but the packed 'compostions'
> doesn't allow the inclusion of random factors.
>
> Does anyone has experience whit such data and analyses and point me in
> the right direction?
>
> Thanks a lot in advance for your replies!
>
> Regards,
> St?phanie
> --
> *St?phanie PERIQUET (PhD) * - Bat-Eared Fox Research Project
> /Dept of Zoology & Entomology/
> /University of the Free State, Qwaqwa Campus/
> /Private Bag X13/
> /9866 Phuthaditjhaba, Free State/
> /South Africa/
> *Cell: +27 79 570 2683*
> ResearchGate <https://www.researchgate.net/profile/Stephanie_Periquet> &
> Twitter <https://twitter.com/ShumbAfrica>
>
> Kalahari bat-eared foxes on Twitter <https://twitter.com/kal_batearedfox>
>
> //////
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From paolo.canal at iusspavia.it  Tue Nov  8 12:18:05 2016
From: paolo.canal at iusspavia.it (Paolo Canal)
Date: Tue, 8 Nov 2016 12:18:05 +0100
Subject: [R-sig-ME] Specify the appropriate model for an Event Related
 Potentials (ERPs) study: what should I do with trial order (and other
 terms)
Message-ID: <4d8c6c91-4c6d-a712-9bdd-e008fe0d860d@iusspavia.it>

Dear Mixed-Group,
I have acquired my data from one Experiment using a rather common 
paradigm in psycholinguistics. The experiment aimed at investigating the 
electro-physiological correlates of reading Typical (e.g., /chair/) vs 
Atypical (e.g., /foot rest/) members of a number (N=85) of semantic 
categories (e.g., /a kind of //Furniture/). In particular, we were 
interested in looking at differences associated with Education level 
(University N=24 vs non-University students N=23), and a three 
individual predictors. My issue is how to deal with some factors that 
are absolutely important in allowing for a better fit of the model, but 
make interpretations too "complicated".

The two main factors of interest thus Typicality (categorical, Typical 
vs Atypical) and Education (categorical, Hi vs Low Education). I already 
know that the choice of taking these factors as dichotomic is 
questionable, but I believe, defensible: in fact, although the measure 
of Typicality is actually continuous (a proportion varying from 0 to 1) 
it is paired within each semantic category, because when we selected the 
materials we took the pair of exemplars that showed the largest 
difference in Typicality, so within each category is the difference in 
typicality that actually matters. Treating Education as categorical is 
less defensible, but in some way we wanted to compare the predictive 
power of this variable with more continuous variables representing a set 
of abilities (3 cognitive measure, one of which moderated by years of 
education and age), in some way to possibly show that some brain 
mechanisms are better described when accounting for individual variation 
rather than group differences.

I used lmer in lme4 to analyze the effect of my independent variables on 
the average EEG voltage (continuous) from a set of EEG channels in two 
different time-windows of interest (I know GAMM would be even more 
appropriate than LMM, as what I am dealing with here are time-series, 
but I am not yet ready to try).

I first determined the random effect structure, selecting three grouping 
factors (subject, semantic category and channel) which are clusters of 
repeated measures: for each item I have several subjects, for each 
subject I have several items and for each channel I have several items 
and subjects (perhaps channel might be nested in subject and item rather 
than stand alone, any hints?). For each grouping factor, I allowed 
intercepts to vary (e.g., 1|subject). Moreover, because I wanted to be 
conservative and data are rather malleable (no convergence failure, no 
variance = 0 or 1, not too high correlations between terms) I included a 
set of terms to adjust by-subject and by-item slopes. I allowed 
by-subject and by-item slope adjustments for Typicality (as it varies 
within subjects and within semantic category) and by-item slope 
adjustments for Education level.

Things get more complicated when thinking of the influence of two 
variables that actually account for a lot of variation in the data: 
frequency of use of words and trial order. The first variable is also 
theoretically important and I want to include it as fixed effect; the 
second variable increases models' fit but because it makes the results 
less straightforward to interpret, I would not like to include in the 
fixed part of the model.

This brings me to the fixed effect structure and the actual questions to 
the list:

The initial design was very simple (2X2 plus covariates). The strategy 
was to fit the simple model Typicality + Frequency and evaluate if 
adding the interaction between Education (or the three covariates) and 
Typicality leads to relevant increase in likelihood, using always with 
the same random structure (the complex one).

Now I am not so sure this is appropriate and I have a list of doubts:
- Am I allowed to use the same complex random structure to compare the 
likelihood of models that have "simpler" fixed effects? In principle I 
guess it is correct to have the same random structure across comparisons.
- I am not interested in the effect of serial presentation (trial 
order), as it increases the order of the highest interaction. Is it 
appropriate to use it in the random structure only, or should I always 
discuss it in interaction with my factors of interest?

Thanks for any help
Paolo

	[[alternative HTML version deleted]]


From anvarsour at ut.ac.ir  Tue Nov  8 19:24:00 2016
From: anvarsour at ut.ac.ir (anvarsour)
Date: Tue, 08 Nov 2016 21:54:00 +0330
Subject: [R-sig-ME] request
Message-ID: <06b3b2fc10516835de1597a614bace09@ut.ac.ir>

I'm sorry, I would be grateful to help me, if is possible for you for
solving this problem. 

I used mixed effect model (with fixed and random factor) for analysis
relationships between species richness and biomass. as following: 

mod.s05x05 <- fm1_logB2 <- glmer (S ~ disturbance*log(sumBiomass) +
disturbance*I(log(sumBiomass)^2) + (1|site), data= abbb, family=
poisson, nAGQ=0) 

in above model disturbance* log(sumBiomass and
disturbance*I(log(sumBiomass)^2) are fixed factors and (1|site) is
random factor 

disturbance level has five levels. 

 but I had problem when I want to run plotting prediction for that. 

as you see our model is log-quadratic. 

coefs05 <- fixef(mod.s05x05) # Extract fixed-effects estimates 

coefs05
(Intercept) disturbance 
-3.83519985 -0.26622660 
log(sumBiomass) I(log(sumBiomass)^2) 
3.04840462 -0.37551457 
disturbance:log(sumBiomass) disturbance:I(log(sumBiomass)^2) 
-0.01181700 0.01711205 

as you see in "coefs05" this model has many estimated coefficients, such
as: disturbance, log(sumBiomass), I(log(sumBiomass)^2),
disturbance:log(sumBiomass)  and disturbance:I(log(sumBiomass)^2)  

how can I write above estimated coefficients for newdat and mm and
newdat$y as you see in the following? 

I faced with error when I want to run following functions. because
following functions not conformable arguments. 

newdat<-data.frame(x=log(5:300), x2=log(5:300)^2) 

mm <- model.matrix(~x+x2) 

newdat$y<-mm%*%fixef(mod.s05x05) 

Best Regards, 

Anvar,

-- -- --
Anvar Sanaei

Ph.D. Student in Range Management, Department of Reclamation of Arid and
Mountainous Regions, Natural Resources Faculty, The University of
Tehran, Karaj, Iran.
	[[alternative HTML version deleted]]


From wingyeechow.zoey at gmail.com  Wed Nov  9 11:44:55 2016
From: wingyeechow.zoey at gmail.com (Wing Yee Chow)
Date: Wed, 9 Nov 2016 10:44:55 -0000
Subject: [R-sig-ME] parsimonious mixed models
Message-ID: <012801d23a76$4f5c80e0$ee1582a0$@gmail.com>

Hi there, 

I'm trying to follow Bates et al.'s guidelines for parsimonious mixed models
and have two questions:

1.       In reducing (near-zero) variance components from the random effects
structure, is the criterion simply the size of the variance? Or do I need to
keep a simple effect as long as I'm keeping a higher order effect that
involves that factor? That is, with the following random effects structure,
do I take out (i) just cSenttype:cCongruity for item, or (ii) both
cSenttype:cCongruity for item AND cCongruity for subj?

 

Random effects:

Groups   Name                 Variance Std.Dev.

item     cSenttype:cCongruity       0     0.0  

 item.1   cCongruity            205437   453.3  

 item.2   cSenttype             100765   317.4  

 item.3   (Intercept)           124422   352.7  

 subj     cSenttype:cCongruity  134752   367.1  

 subj.1   cCongruity                 0     0.0  

 subj.2   cSenttype              87161   295.2  

 subj.3   (Intercept)          2434746  1560.4  

 Residual                      4891977  2211.8  

Number of obs: 1082, groups:  item, 48; subj, 24

 

2.       In another dataset, I have a 3 x 2 within-participant and
within-item design. I specified the contrasts for the two factors in this
way:

contrasts(tempdata$congruity) <- contr.sum(2)/2

contrasts(tempdata$sentencetype)=cbind("AvsBC" = c(-2/3, 1/3, 1/3), "BvsC" =
c(0, -1/2, 1/2))

 

I got the model matrix (mmatrix) from the maximal model (m0) to construct
the zero-correlation parameter model (m1). I think I know how to do this for
a factor with two levels (i.e., only one contrast), but I'm not sure about
the current case since I have both cSenttypeAvsBC and cSenttypeBvsC for the
3-level factor sentencetype. Is this the right syntax?

 

cSenttypeAvsBC <- mmatrix [,2] # columns 2 and 3 encode different contrasts
of sentencetype

cSenttypeBvsC <- mmatrix[,3]

cCongruity <- mmatrix[,4]

m1<- lmer(value ~ sentencetype * congruity + ((cSenttypeAvsBC +
cSenttypeBvsC) * cCongruity||subj) + ((cSenttypeAvsBC + cSenttypeBvsC) *
cCongruity||item), REML=FALSE,

                          data=tempdata)

 

Many thanks!!

Wing-Yee Chow


	[[alternative HTML version deleted]]


From stephanie.periquet at gmail.com  Thu Nov 10 07:24:37 2016
From: stephanie.periquet at gmail.com (=?UTF-8?Q?St=c3=a9phanie_P=c3=a9riquet?=)
Date: Thu, 10 Nov 2016 08:24:37 +0200
Subject: [R-sig-ME] Mixed model for compositional data
In-Reply-To: <0195b64f-777b-02b6-a3c4-22b044445258@gmail.com>
References: <699cf5a6-8d40-835b-1d31-8c6148b7d09d@gmail.com>
	<CAJuCY5yEY5JY2T5FyrJyjp6wgKmfhbxh0746SqTMO8ZgD27Lnw@mail.gmail.com>
	<0195b64f-777b-02b6-a3c4-22b044445258@gmail.com>
Message-ID: <520c5865-40d0-4c67-0b57-707a2666962c@gmail.com>

Thanks for your reply Thierry.

We did try this as a first approach. The model was over dispersed and 
zero-inflated so is hard to fit but we are getting there using R2OpenBUGS.

Best,
St?phanie

On 10/11/16 08:18, St?phanie P?riquet wrote:
> Thanks for your reply Thierry.
>
> We did try this as a first approach. The model was over dispersed and 
> zero-inflated so is hard to fit but we are getting there using R2OpenBUGS.
>
> Best,
> St?phanie


-- 
*St?phanie PERIQUET (PhD) * - Bat-Eared Fox Research Project
/Dept of Zoology & Entomology/
/University of the Free State, Qwaqwa Campus/
/Private Bag X13/
/9866 Phuthaditjhaba, Free State/
/South Africa/
*Cell: +27 79 570 2683*
ResearchGate <https://www.researchgate.net/profile/Stephanie_Periquet> & 
Twitter <https://twitter.com/ShumbAfrica>

Kalahari bat-eared foxes on Twitter <https://twitter.com/kal_batearedfox>

//////

	[[alternative HTML version deleted]]


From Phillip.Alday at unisa.edu.au  Thu Nov 10 13:17:20 2016
From: Phillip.Alday at unisa.edu.au (Phillip Alday)
Date: Thu, 10 Nov 2016 12:17:20 +0000
Subject: [R-sig-ME] Specify the appropriate model for an Event Related
 Potentials (ERPs) study: what should I do with trial order (and other
 terms)
In-Reply-To: <4d8c6c91-4c6d-a712-9bdd-e008fe0d860d@iusspavia.it>
References: <4d8c6c91-4c6d-a712-9bdd-e008fe0d860d@iusspavia.it>
Message-ID: <8549F1AD-B501-4503-9A79-117458B730C7@unisa.edu.au>

Dear Paolo,

Using Subject (or Participant, if you want to avoid some ambiguity in a linguistic context) and Item as grouping factors is fairly standard in the ERP literature as is use LMM with the mean amplitude in a given time window as the dependent variable (at least when mixed models are used, many colleagues seem reticent to abandon ANOVA despite Clark 1973 and Judd, Westfall and Kenny 2012 and many other papers emphasising the advantages of explicit regression over ANOVA). GAMMs over the whole time course of the ERP are still relatively new and not widely used, although people like Harald Baayen and Stefanie Nickels are working on this. Beyond the enhanced computational complexity, GAMMs also suffer from the whole *additive* bit, which can be addressed, but is difficult for cognitive neuroscience, where the interactions are often the most interesting bits.

I hesitate to use Channel as a grouping variable, although this is the approach taken by e.g. Payne et al (2015), because the the distribution of effects for channels is not multivariate normal (the assumed distribution in lme4) for most references. Indeed, we know that Channel effects vary systematically (this the whole notion of "topography" in EEG), and I personally feel that we should actually model channel effects parametrically using a suitable coordinate system, such as the one that the 10-20 system is actually based on (angular deviations from the apical electrode). However, this is again much more complex. Including Channel as a categorical fixed effect is also not particularly satisfying as this will add n_chans-1 coefficients for the main effect of channel as well as many interaction terms. You could potentially have regions of interest (ROIs) / topographical factors (left-right, anterior-posterior) in your model fixed effects and then either ignore channel (as is actually done for the traditional rmANOVA analysis of ERP data) or include an intercept-only random-effect term for channel under the assumption that there is a multivariate normal distribution of effects within a given ROI. However, this assumption will generally only hold for high-density setups with topographically small ROIs. Larger ROIs will of course show systematic variation as you move from one edge to another. And you will also run into problems if the number of channels within each ROI are small as this will bias your random-effect estimates: remember that random effects are *variance* components and like all variance estimates, they require several observed levels for accurate estimation. (One rule of thumb I've heard is 10ish.) And as Payne et al saw in their own data, the channel factor typically doesn't help with model fit anyway and can hurt convergence, so I would just leave it out completely if you don't want to model it parametrically.

I'm not sure why you mentioned "semantic category" in your random-effect structure. In my experience, semantic category is typically something for which we care about the individual levels (of which there are not that many in any one experiment) and so are better modelled by fixed effects. (In other words, we care about the differences in processing between Furniture and People, not just that different categories show differences.) Items are good random effects, semantic categories are not. And it's not a problem if each item only belong to certain semantic categories. lme4 can handle such nesting structures. If you only have a few semantic categories, then you'll also run into computational / statistical trouble with treating them as random effects (see the last paragraph).

In short, I would propose the following model structure:

mean_voltage ~ 1 + typicality * education * frequency * semantic_category + (1+... | subject) + (1+ ... | item)

Your particular choice of which slopes to include for each random-effect grouping term is a difficult one, as has been highlighted by the Baayen et al (2008), Barr et al (2013), Barr (2013) and the recent set of Bates preprints on parsimonious mixed models as well as a number of threads on this mailing list. Generally, I start off with main effects and if that model converges, great, if not, then I reduce more. In my experience with EEG studies on language, interactions in the random-effects structure just lead to overly complex models that take a long time to compute, fail to converge or show others signs of being degenerate. In other words, I would consider the following RE structure for your data:


(1 +  typicality + frequency + semantic_category | subject) + (1 +  education  | item)

I left a lot out of the RE structure for Item because, assuming that each Item represents a single lemma / word, then it doesn't have different frequencies / categories / typicalities and so it doesn't make sense to consider a variable effect for something that is constant within the grouping unit. Similarly for education and subject.

If you don't model semantic category explicitly, then your item random effect should absorb the variance due to it.  You just won't have an explicit term in the model to point to that only describes the effect of semantic category (as item-level variance will cover a whole host of other effects related to the differences between words).

(For posterity -- I think we discussed some of these issues previously on r-help: https://stat.ethz.ch/pipermail/r-help/2015-September/432561.html )

To address some of your explicit questions more directly:

> - Am I allowed to use the same complex random structure to compare the 
> likelihood of models that have "simpler" fixed effects? In principle I 
> guess it is correct to have the same random structure across comparisons.

Not quite. You should not have random slopes for effects not in the fixed-effect model structure because the mixed-model formulation used by lme4 assumes zero-mean for the random effects. In other words, lme4 random effects are estimates of how much the different grouping factors lead to variance around the population-level estimates delivered by the fixed effects. 

> - I am not interested in the effect of serial presentation (trial 
> order), as it increases the order of the highest interaction. Is it 
> appropriate to use it in the random structure only, or should I always 
> discuss it in interaction with my factors of interest?


No, for the reason above. But you could have the order of serial presentation a non-interacting / main-effect only fixed effect. Also, if you did the usual thing and you counterbalanced presentation order (e.g. via several different pseudo-random presentation orders/lists) across participants, then the usual assumption is that any effects of presentation order cancel out across participants. The item grouping factor will also absorb some of this variance. 

Best,
Phillip

> On 8 Nov 2016, at 21:48, Paolo Canal <paolo.canal at iusspavia.it> wrote:
> 
> Dear Mixed-Group,
> I have acquired my data from one Experiment using a rather common 
> paradigm in psycholinguistics. The experiment aimed at investigating the 
> electro-physiological correlates of reading Typical (e.g., /chair/) vs 
> Atypical (e.g., /foot rest/) members of a number (N=85) of semantic 
> categories (e.g., /a kind of //Furniture/). In particular, we were 
> interested in looking at differences associated with Education level 
> (University N=24 vs non-University students N=23), and a three 
> individual predictors. My issue is how to deal with some factors that 
> are absolutely important in allowing for a better fit of the model, but 
> make interpretations too "complicated".
> 
> The two main factors of interest thus Typicality (categorical, Typical 
> vs Atypical) and Education (categorical, Hi vs Low Education). I already 
> know that the choice of taking these factors as dichotomic is 
> questionable, but I believe, defensible: in fact, although the measure 
> of Typicality is actually continuous (a proportion varying from 0 to 1) 
> it is paired within each semantic category, because when we selected the 
> materials we took the pair of exemplars that showed the largest 
> difference in Typicality, so within each category is the difference in 
> typicality that actually matters. Treating Education as categorical is 
> less defensible, but in some way we wanted to compare the predictive 
> power of this variable with more continuous variables representing a set 
> of abilities (3 cognitive measure, one of which moderated by years of 
> education and age), in some way to possibly show that some brain 
> mechanisms are better described when accounting for individual variation 
> rather than group differences.
> 
> I used lmer in lme4 to analyze the effect of my independent variables on 
> the average EEG voltage (continuous) from a set of EEG channels in two 
> different time-windows of interest (I know GAMM would be even more 
> appropriate than LMM, as what I am dealing with here are time-series, 
> but I am not yet ready to try).
> 
> I first determined the random effect structure, selecting three grouping 
> factors (subject, semantic category and channel) which are clusters of 
> repeated measures: for each item I have several subjects, for each 
> subject I have several items and for each channel I have several items 
> and subjects (perhaps channel might be nested in subject and item rather 
> than stand alone, any hints?). For each grouping factor, I allowed 
> intercepts to vary (e.g., 1|subject). Moreover, because I wanted to be 
> conservative and data are rather malleable (no convergence failure, no 
> variance = 0 or 1, not too high correlations between terms) I included a 
> set of terms to adjust by-subject and by-item slopes. I allowed 
> by-subject and by-item slope adjustments for Typicality (as it varies 
> within subjects and within semantic category) and by-item slope 
> adjustments for Education level.
> 
> Things get more complicated when thinking of the influence of two 
> variables that actually account for a lot of variation in the data: 
> frequency of use of words and trial order. The first variable is also 
> theoretically important and I want to include it as fixed effect; the 
> second variable increases models' fit but because it makes the results 
> less straightforward to interpret, I would not like to include in the 
> fixed part of the model.
> 
> This brings me to the fixed effect structure and the actual questions to 
> the list:
> 
> The initial design was very simple (2X2 plus covariates). The strategy 
> was to fit the simple model Typicality + Frequency and evaluate if 
> adding the interaction between Education (or the three covariates) and 
> Typicality leads to relevant increase in likelihood, using always with 
> the same random structure (the complex one).
> 
> Now I am not so sure this is appropriate and I have a list of doubts:
> - Am I allowed to use the same complex random structure to compare the 
> likelihood of models that have "simpler" fixed effects? In principle I 
> guess it is correct to have the same random structure across comparisons.
> - I am not interested in the effect of serial presentation (trial 
> order), as it increases the order of the highest interaction. Is it 
> appropriate to use it in the random structure only, or should I always 
> discuss it in interaction with my factors of interest?
> 
> Thanks for any help
> Paolo
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From Phillip.Alday at unisa.edu.au  Thu Nov 10 13:23:52 2016
From: Phillip.Alday at unisa.edu.au (Phillip Alday)
Date: Thu, 10 Nov 2016 12:23:52 +0000
Subject: [R-sig-ME] parsimonious mixed models
In-Reply-To: <012801d23a76$4f5c80e0$ee1582a0$@gmail.com>
References: <012801d23a76$4f5c80e0$ee1582a0$@gmail.com>
Message-ID: <8ACA0328-33D2-442A-8533-CB12D2CFE5CA@unisa.edu.au>

Having an interaction without the corresponding main effect is a bit unusual, but not unheard of ... in your case though, I would remove the interactions and take look at the new random-effects estimates. I suspect you'll see that more variance is now explained by the main effects. 

I also suspect that your variables aren't quite coded right -- cSenttype is probably categorical, is it not? Yet it's estimate in the random-effects structure is that of a continuous variable (single estimate) and not a factor (set of estimates for different levels / contrasts).

Best,
Phillip

> On 9 Nov 2016, at 21:14, Wing Yee Chow <wingyeechow.zoey at gmail.com> wrote:
> 
> Hi there, 
> 
> I'm trying to follow Bates et al.'s guidelines for parsimonious mixed models
> and have two questions:
> 
> 1.       In reducing (near-zero) variance components from the random effects
> structure, is the criterion simply the size of the variance? Or do I need to
> keep a simple effect as long as I'm keeping a higher order effect that
> involves that factor? That is, with the following random effects structure,
> do I take out (i) just cSenttype:cCongruity for item, or (ii) both
> cSenttype:cCongruity for item AND cCongruity for subj?
> 
> 
> 
> Random effects:
> 
> Groups   Name                 Variance Std.Dev.
> 
> item     cSenttype:cCongruity       0     0.0  
> 
> item.1   cCongruity            205437   453.3  
> 
> item.2   cSenttype             100765   317.4  
> 
> item.3   (Intercept)           124422   352.7  
> 
> subj     cSenttype:cCongruity  134752   367.1  
> 
> subj.1   cCongruity                 0     0.0  
> 
> subj.2   cSenttype              87161   295.2  
> 
> subj.3   (Intercept)          2434746  1560.4  
> 
> Residual                      4891977  2211.8  
> 
> Number of obs: 1082, groups:  item, 48; subj, 24
> 
> 
> 
> 2.       In another dataset, I have a 3 x 2 within-participant and
> within-item design. I specified the contrasts for the two factors in this
> way:
> 
> contrasts(tempdata$congruity) <- contr.sum(2)/2
> 
> contrasts(tempdata$sentencetype)=cbind("AvsBC" = c(-2/3, 1/3, 1/3), "BvsC" =
> c(0, -1/2, 1/2))
> 
> 
> 
> I got the model matrix (mmatrix) from the maximal model (m0) to construct
> the zero-correlation parameter model (m1). I think I know how to do this for
> a factor with two levels (i.e., only one contrast), but I'm not sure about
> the current case since I have both cSenttypeAvsBC and cSenttypeBvsC for the
> 3-level factor sentencetype. Is this the right syntax?
> 
> 
> 
> cSenttypeAvsBC <- mmatrix [,2] # columns 2 and 3 encode different contrasts
> of sentencetype
> 
> cSenttypeBvsC <- mmatrix[,3]
> 
> cCongruity <- mmatrix[,4]
> 
> m1<- lmer(value ~ sentencetype * congruity + ((cSenttypeAvsBC +
> cSenttypeBvsC) * cCongruity||subj) + ((cSenttypeAvsBC + cSenttypeBvsC) *
> cCongruity||item), REML=FALSE,
> 
>                          data=tempdata)
> 
> 
> 
> Many thanks!!
> 
> Wing-Yee Chow
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From Phillip.Alday at unisa.edu.au  Thu Nov 10 13:26:59 2016
From: Phillip.Alday at unisa.edu.au (Phillip Alday)
Date: Thu, 10 Nov 2016 12:26:59 +0000
Subject: [R-sig-ME] Specify the appropriate model for an Event Related
 Potentials (ERPs) study: what should I do with trial order (and other
 terms)
In-Reply-To: <8549F1AD-B501-4503-9A79-117458B730C7@unisa.edu.au>
References: <4d8c6c91-4c6d-a712-9bdd-e008fe0d860d@iusspavia.it>
	<8549F1AD-B501-4503-9A79-117458B730C7@unisa.edu.au>
Message-ID: <E147DFEF-E4DB-440A-B385-4EAE2A7F3066@unisa.edu.au>

Dear Paolo,

I forgot to state this, but make sure you're using sum coding for your variables! Dale Barr and I have both written things on this:

http://talklab.psy.gla.ac.uk/tvw/catpred/

http://palday.bitbucket.org/stats/coding.html

In R, you can do this by placing the following at the top of your script (this variant will give you better named contrasts than the usual way of doing it):

> library(car)
> options(contrasts = c("contr.Sum","contr.poly"))

Best,
Phillip
 


> On 10 Nov 2016, at 22:47, Phillip Alday <phillip.alday at unisa.edu.au> wrote:
> 
> Dear Paolo,
> 
> Using Subject (or Participant, if you want to avoid some ambiguity in a linguistic context) and Item as grouping factors is fairly standard in the ERP literature as is use LMM with the mean amplitude in a given time window as the dependent variable (at least when mixed models are used, many colleagues seem reticent to abandon ANOVA despite Clark 1973 and Judd, Westfall and Kenny 2012 and many other papers emphasising the advantages of explicit regression over ANOVA). GAMMs over the whole time course of the ERP are still relatively new and not widely used, although people like Harald Baayen and Stefanie Nickels are working on this. Beyond the enhanced computational complexity, GAMMs also suffer from the whole *additive* bit, which can be addressed, but is difficult for cognitive neuroscience, where the interactions are often the most interesting bits.
> 
> I hesitate to use Channel as a grouping variable, although this is the approach taken by e.g. Payne et al (2015), because the the distribution of effects for channels is not multivariate normal (the assumed distribution in lme4) for most references. Indeed, we know that Channel effects vary systematically (this the whole notion of "topography" in EEG), and I personally feel that we should actually model channel effects parametrically using a suitable coordinate system, such as the one that the 10-20 system is actually based on (angular deviations from the apical electrode). However, this is again much more complex. Including Channel as a categorical fixed effect is also not particularly satisfying as this will add n_chans-1 coefficients for the main effect of channel as well as many interaction terms. You could potentially have regions of interest (ROIs) / topographical factors (left-right, anterior-posterior) in your model fixed effects and then either ignore channel (as is!
>  actually done for the traditional rmANOVA analysis of ERP data) or include an intercept-only random-effect term for channel under the assumption that there is a multivariate normal distribution of effects within a given ROI. However, this assumption will generally only hold for high-density setups with topographically small ROIs. Larger ROIs will of course show systematic variation as you move from one edge to another. And you will also run into problems if the number of channels within each ROI are small as this will bias your random-effect estimates: remember that random effects are *variance* components and like all variance estimates, they require several observed levels for accurate estimation. (One rule of thumb I've heard is 10ish.) And as Payne et al saw in their own data, the channel factor typically doesn't help with model fit anyway and can hurt convergence, so I would just leave it out completely if you don't want to model it parametrically.
> 
> I'm not sure why you mentioned "semantic category" in your random-effect structure. In my experience, semantic category is typically something for which we care about the individual levels (of which there are not that many in any one experiment) and so are better modelled by fixed effects. (In other words, we care about the differences in processing between Furniture and People, not just that different categories show differences.) Items are good random effects, semantic categories are not. And it's not a problem if each item only belong to certain semantic categories. lme4 can handle such nesting structures. If you only have a few semantic categories, then you'll also run into computational / statistical trouble with treating them as random effects (see the last paragraph).
> 
> In short, I would propose the following model structure:
> 
> mean_voltage ~ 1 + typicality * education * frequency * semantic_category + (1+... | subject) + (1+ ... | item)
> 
> Your particular choice of which slopes to include for each random-effect grouping term is a difficult one, as has been highlighted by the Baayen et al (2008), Barr et al (2013), Barr (2013) and the recent set of Bates preprints on parsimonious mixed models as well as a number of threads on this mailing list. Generally, I start off with main effects and if that model converges, great, if not, then I reduce more. In my experience with EEG studies on language, interactions in the random-effects structure just lead to overly complex models that take a long time to compute, fail to converge or show others signs of being degenerate. In other words, I would consider the following RE structure for your data:
> 
> 
> (1 +  typicality + frequency + semantic_category | subject) + (1 +  education  | item)
> 
> I left a lot out of the RE structure for Item because, assuming that each Item represents a single lemma / word, then it doesn't have different frequencies / categories / typicalities and so it doesn't make sense to consider a variable effect for something that is constant within the grouping unit. Similarly for education and subject.
> 
> If you don't model semantic category explicitly, then your item random effect should absorb the variance due to it.  You just won't have an explicit term in the model to point to that only describes the effect of semantic category (as item-level variance will cover a whole host of other effects related to the differences between words).
> 
> (For posterity -- I think we discussed some of these issues previously on r-help: https://stat.ethz.ch/pipermail/r-help/2015-September/432561.html )
> 
> To address some of your explicit questions more directly:
> 
>> - Am I allowed to use the same complex random structure to compare the 
>> likelihood of models that have "simpler" fixed effects? In principle I 
>> guess it is correct to have the same random structure across comparisons.
> 
> Not quite. You should not have random slopes for effects not in the fixed-effect model structure because the mixed-model formulation used by lme4 assumes zero-mean for the random effects. In other words, lme4 random effects are estimates of how much the different grouping factors lead to variance around the population-level estimates delivered by the fixed effects. 
> 
>> - I am not interested in the effect of serial presentation (trial 
>> order), as it increases the order of the highest interaction. Is it 
>> appropriate to use it in the random structure only, or should I always 
>> discuss it in interaction with my factors of interest?
> 
> 
> No, for the reason above. But you could have the order of serial presentation a non-interacting / main-effect only fixed effect. Also, if you did the usual thing and you counterbalanced presentation order (e.g. via several different pseudo-random presentation orders/lists) across participants, then the usual assumption is that any effects of presentation order cancel out across participants. The item grouping factor will also absorb some of this variance. 
> 
> Best,
> Phillip
> 
>> On 8 Nov 2016, at 21:48, Paolo Canal <paolo.canal at iusspavia.it> wrote:
>> 
>> Dear Mixed-Group,
>> I have acquired my data from one Experiment using a rather common 
>> paradigm in psycholinguistics. The experiment aimed at investigating the 
>> electro-physiological correlates of reading Typical (e.g., /chair/) vs 
>> Atypical (e.g., /foot rest/) members of a number (N=85) of semantic 
>> categories (e.g., /a kind of //Furniture/). In particular, we were 
>> interested in looking at differences associated with Education level 
>> (University N=24 vs non-University students N=23), and a three 
>> individual predictors. My issue is how to deal with some factors that 
>> are absolutely important in allowing for a better fit of the model, but 
>> make interpretations too "complicated".
>> 
>> The two main factors of interest thus Typicality (categorical, Typical 
>> vs Atypical) and Education (categorical, Hi vs Low Education). I already 
>> know that the choice of taking these factors as dichotomic is 
>> questionable, but I believe, defensible: in fact, although the measure 
>> of Typicality is actually continuous (a proportion varying from 0 to 1) 
>> it is paired within each semantic category, because when we selected the 
>> materials we took the pair of exemplars that showed the largest 
>> difference in Typicality, so within each category is the difference in 
>> typicality that actually matters. Treating Education as categorical is 
>> less defensible, but in some way we wanted to compare the predictive 
>> power of this variable with more continuous variables representing a set 
>> of abilities (3 cognitive measure, one of which moderated by years of 
>> education and age), in some way to possibly show that some brain 
>> mechanisms are better described when accounting for individual variation 
>> rather than group differences.
>> 
>> I used lmer in lme4 to analyze the effect of my independent variables on 
>> the average EEG voltage (continuous) from a set of EEG channels in two 
>> different time-windows of interest (I know GAMM would be even more 
>> appropriate than LMM, as what I am dealing with here are time-series, 
>> but I am not yet ready to try).
>> 
>> I first determined the random effect structure, selecting three grouping 
>> factors (subject, semantic category and channel) which are clusters of 
>> repeated measures: for each item I have several subjects, for each 
>> subject I have several items and for each channel I have several items 
>> and subjects (perhaps channel might be nested in subject and item rather 
>> than stand alone, any hints?). For each grouping factor, I allowed 
>> intercepts to vary (e.g., 1|subject). Moreover, because I wanted to be 
>> conservative and data are rather malleable (no convergence failure, no 
>> variance = 0 or 1, not too high correlations between terms) I included a 
>> set of terms to adjust by-subject and by-item slopes. I allowed 
>> by-subject and by-item slope adjustments for Typicality (as it varies 
>> within subjects and within semantic category) and by-item slope 
>> adjustments for Education level.
>> 
>> Things get more complicated when thinking of the influence of two 
>> variables that actually account for a lot of variation in the data: 
>> frequency of use of words and trial order. The first variable is also 
>> theoretically important and I want to include it as fixed effect; the 
>> second variable increases models' fit but because it makes the results 
>> less straightforward to interpret, I would not like to include in the 
>> fixed part of the model.
>> 
>> This brings me to the fixed effect structure and the actual questions to 
>> the list:
>> 
>> The initial design was very simple (2X2 plus covariates). The strategy 
>> was to fit the simple model Typicality + Frequency and evaluate if 
>> adding the interaction between Education (or the three covariates) and 
>> Typicality leads to relevant increase in likelihood, using always with 
>> the same random structure (the complex one).
>> 
>> Now I am not so sure this is appropriate and I have a list of doubts:
>> - Am I allowed to use the same complex random structure to compare the 
>> likelihood of models that have "simpler" fixed effects? In principle I 
>> guess it is correct to have the same random structure across comparisons.
>> - I am not interested in the effect of serial presentation (trial 
>> order), as it increases the order of the highest interaction. Is it 
>> appropriate to use it in the random structure only, or should I always 
>> discuss it in interaction with my factors of interest?
>> 
>> Thanks for any help
>> Paolo
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From stephanie.periquet at gmail.com  Thu Nov 10 07:18:46 2016
From: stephanie.periquet at gmail.com (=?UTF-8?Q?St=c3=a9phanie_P=c3=a9riquet?=)
Date: Thu, 10 Nov 2016 08:18:46 +0200
Subject: [R-sig-ME] Mixed model for compositional data
In-Reply-To: <CAJuCY5yEY5JY2T5FyrJyjp6wgKmfhbxh0746SqTMO8ZgD27Lnw@mail.gmail.com>
References: <699cf5a6-8d40-835b-1d31-8c6148b7d09d@gmail.com>
	<CAJuCY5yEY5JY2T5FyrJyjp6wgKmfhbxh0746SqTMO8ZgD27Lnw@mail.gmail.com>
Message-ID: <0195b64f-777b-02b6-a3c4-22b044445258@gmail.com>

Thanks for your reply Thierry.

We did try this as a first approach. The model was over dispersed and 
zero-inflated so is hard to fit but we are getting there using R2OpenBUGS.

Best,
St?phanie

On 08/11/16 10:31, Thierry Onkelinx wrote:
> Dear St?phanie,
>
> A workaround would be to model the numbers per item and use the (log 
> of) the total number per night as an offset. Then the model parameters 
> are essentially ratio's.
>
> no effects of season and moonlite: number ~ offset(log(total)) + item 
> + (1|animal)
> only season: number ~ offset(log(total)) + item*season + (1|animal)
> season:moonlight: number ~ offset(log(total)) + item*season*moonlight 
> + (1|animal)
>
> The model fit will not be guaranteed to sum to 1 per night and animal.
>
> Best regards,
>
> Thierry
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature 
> and Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no 
> more than asking him to perform a post-mortem examination: he may be 
> able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does 
> not ensure that a reasonable answer can be extracted from a given body 
> of data. ~ John Tukey
>
> 2016-11-08 8:40 GMT+01:00 St?phanie P?riquet 
> <stephanie.periquet at gmail.com <mailto:stephanie.periquet at gmail.com>>:
>
>     Hi all,
>
>     I have repeated measures of individual diet composition (I
>     actually have
>     the number of each item eaten and the duration of each
>     observation) and
>     I would like to model the effect of variables such as season and
>     moonlight on diet composition (not the actual number of item eaten).
>     I gathered that I should use compositional but the packed
>     'compostions'
>     doesn't allow the inclusion of random factors.
>
>     Does anyone has experience whit such data and analyses and point me in
>     the right direction?
>
>     Thanks a lot in advance for your replies!
>
>     Regards,
>     St?phanie
>     --
>     *St?phanie PERIQUET (PhD) * - Bat-Eared Fox Research Project
>     /Dept of Zoology & Entomology/
>     /University of the Free State, Qwaqwa Campus/
>     /Private Bag X13/
>     /9866 Phuthaditjhaba, Free State/
>     /South Africa/
>     *Cell: +27 79 570 2683 <tel:%2B27%2079%20570%202683>*
>     ResearchGate
>     <https://www.researchgate.net/profile/Stephanie_Periquet
>     <https://www.researchgate.net/profile/Stephanie_Periquet>> &
>     Twitter <https://twitter.com/ShumbAfrica
>     <https://twitter.com/ShumbAfrica>>
>
>     Kalahari bat-eared foxes on Twitter
>     <https://twitter.com/kal_batearedfox
>     <https://twitter.com/kal_batearedfox>>
>
>     //////
>
>             [[alternative HTML version deleted]]
>
>     _______________________________________________
>     R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>     <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
>
>


-- 
*St?phanie PERIQUET (PhD) * - Bat-Eared Fox Research Project
/Dept of Zoology & Entomology/
/University of the Free State, Qwaqwa Campus/
/Private Bag X13/
/9866 Phuthaditjhaba, Free State/
/South Africa/
*Cell: +27 79 570 2683*
ResearchGate <https://www.researchgate.net/profile/Stephanie_Periquet> & 
Twitter <https://twitter.com/ShumbAfrica>








Kalahari bat-eared foxes on Twitter <https://twitter.com/kal_batearedfox>

//////

From Phillip.Alday at unisa.edu.au  Fri Nov 11 00:48:49 2016
From: Phillip.Alday at unisa.edu.au (Phillip Alday)
Date: Thu, 10 Nov 2016 23:48:49 +0000
Subject: [R-sig-ME] How to estimate the standard error of every single
 random intercept in a mixed linear model?
In-Reply-To: <VI1PR0101MB2430BD82209009E9742530ABACA70@VI1PR0101MB2430.eurprd01.prod.exchangelabs.com>
References: <DB6PR0101MB24230DB767F3AC06A2080433ACDD0@DB6PR0101MB2423.eurprd01.prod.exchangelabs.com>
	<VI1PR0101MB2430182E2678121E2C58D6C8ACA90@VI1PR0101MB2430.eurprd01.prod.exchangelabs.com>
	,<75C2C1CF-E567-47F8-AA06-CB22255445C0@unisa.edu.au>
	<VI1PR0101MB2430BD82209009E9742530ABACA70@VI1PR0101MB2430.eurprd01.prod.exchangelabs.com>
Message-ID: <1478821729.2251.7.camel@unisa.edu.au>

(Try to keep the list in CC.)

See ?lme4::ranef

And take a look at the lme4 source code for ranef() and condVar(), as
well as the papers / manuscripts included with the lme4 package:

vignette("lmer",package="lme4")?
vignette("Theory",package="lme4")

Maybe one of the lme4 developers could contribute an answer faster than
I can go through the math / source again??

Best,
Phillip


On Mon, 2016-11-07 at 20:58 +0000, Chen Chun wrote:
> Dear Phillip,
> 
> Thanks for the reply. yes,?se,ranef directly calculates the square
> root of the posterior variance.?
> 
> 
> attr(ranef(fit, condVar = TRUE)[[1]], "postVar")
> 
> Do you know how the posterior variance for each level of the random
> factor is computed?
> 
> Thanks
> 
> Regards,
> Chen
> 
> ???: Phillip Alday <Phillip.Alday at unisa.edu.au>
> ????: 2016?11?4? 5:29
> ???: Chen Chun
> ??: r-sig-mixed-models at r-project.org
> ??: Re: [R-sig-ME] How to estimate the standard error of every single
> random intercept in a mixed linear model?
> ?
> Dear Chen,
> 
> have you tried entering the following in the R command line?
> 
> > library(arm)
> > se.ranef
> 
> That will output the source code for arm::se.ranef. It's surprisingly
> simple and is basically the square root of the diagonal of variance-
> covariance RE matrix.
> 
> Best,
> Phillip?
> 
> 
> > On 24 Oct 2016, at 20:38, Chen Chun <talischen at hotmail.com> wrote:
> >?
> > Dear all,
> >?
> >?
> > I am running a mixed linear model with group (a_i) as random
> intercept:
> >?
> >?
> > y_ij=mu + a_i + e_ij
> >?
> >?
> > By using lmer() function, the model outputs an estimated variance
> of a_i (i.e. var_hat(a)), and it is the sum of (1) the variance of
> the estimated group mean (i.e. between group variance) and (2) the
> sum of variance for each estimated group mean a_i_hat,?? (i.e. sum of
> within group variance).
> >?
> >?
> > for (1) I can compute it as var(ranef(model)$group). However, I
> dont know how to compute (2), which is the SE of the estimated random
> intercept for each group. I know that using se.ranef() function in
> arm package can help me to extract such variance. But I would like to
> know how these variance are computed? it's relations to residuals and
> number of observations per group?
> >?
> >?
> > Thanks
> >?
> >?
> > Chen
> >?
> >?
> >??????? [[alternative HTML version deleted]]
> >?
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> ? R-sig-mixed-models Info Page - ETH Zurich stat.ethz.ch
> R-sig-mixed-models -- R special interest group on Mixed Effect
> Models, notably lmer() related About R-sig-mixed-models
> 
> 
> 

From paolo.canal at iusspavia.it  Fri Nov 11 11:43:28 2016
From: paolo.canal at iusspavia.it (Paolo Canal)
Date: Fri, 11 Nov 2016 11:43:28 +0100
Subject: [R-sig-ME] Specify the appropriate model for an Event Related
 Potentials (ERPs) study: what should I do with trial order (and other
 terms)
In-Reply-To: <E147DFEF-E4DB-440A-B385-4EAE2A7F3066@unisa.edu.au>
References: <4d8c6c91-4c6d-a712-9bdd-e008fe0d860d@iusspavia.it>
	<8549F1AD-B501-4503-9A79-117458B730C7@unisa.edu.au>
	<E147DFEF-E4DB-440A-B385-4EAE2A7F3066@unisa.edu.au>
Message-ID: <7e86024a-f710-4a00-1666-d87ed66ccd70@iusspavia.it>

Dear Phillip,

Thanks for your reply, I will try to better motivate my choices and add 
further observations.

On 10/11/2016 13:26, Phillip Alday wrote:
> Dear Paolo,
>
> I forgot to state this, but make sure you're using sum coding for your variables! Dale Barr and I have both written things on this:
>
> http://talklab.psy.gla.ac.uk/tvw/catpred/
>
> http://palday.bitbucket.org/stats/coding.html

Thanks for the links. I have already coded my categorical variable with 
sum, or actually with deviation coding. Continuous predictors were 
centered around 0 instead. This should also help in reducing correlation 
in the variance-covariance matrices, and indeed the correlations are 
less than 0.2.

>
> In R, you can do this by placing the following at the top of your script (this variant will give you better named contrasts than the usual way of doing it):
>
>> library(car)
>> options(contrasts = c("contr.Sum","contr.poly"))
> Best,
> Phillip
>   
>
>
>> On 10 Nov 2016, at 22:47, Phillip Alday <phillip.alday at unisa.edu.au> wrote:
>>
>> Dear Paolo,
>>
>> Using Subject (or Participant, if you want to avoid some ambiguity in a linguistic context) and Item as grouping factors is fairly standard in the ERP literature as is use LMM with the mean amplitude in a given time window as the dependent variable (at least when mixed models are used, many colleagues seem reticent to abandon ANOVA despite Clark 1973 and Judd, Westfall and Kenny 2012 and many other papers emphasising the advantages of explicit regression over ANOVA). GAMMs over the whole time course of the ERP are still relatively new and not widely used, although people like Harald Baayen and Stefanie Nickels are working on this. Beyond the enhanced computational complexity, GAMMs also suffer from the whole *additive* bit, which can be addressed, but is difficult for cognitive neuroscience, where the interactions are often the most interesting bits.

I would add that also the work of van Rij and Wieling is inspiring.

>> I hesitate to use Channel as a grouping variable, although this is the approach taken by e.g. Payne et al (2015), because the the distribution of effects for channels is not multivariate normal (the assumed distribution in lme4) for most references. Indeed, we know that Channel effects vary systematically (this the whole notion of "topography" in EEG), and I personally feel that we should actually model channel effects parametrically using a suitable coordinate system, such as the one that the 10-20 system is actually based on (angular deviations from the apical electrode). However, this is again much more complex. Including Channel as a categorical fixed effect is also not particularly satisfying as this will add n_chans-1 coefficients for the main effect of channel as well as many interaction terms. You could potentially have regions of interest (ROIs) / topographical factors (left-right, anterior-posterior) in your model fixed effects and then either ignore channel (as is!
>>   actually done for the traditional rmANOVA analysis of ERP data) or include an intercept-only random-effect term for channel under the assumption that there is a multivariate normal distribution of effects within a given ROI. However, this assumption will generally only hold for high-density setups with topographically small ROIs. Larger ROIs will of course show systematic variation as you move from one edge to another. And you will also run into problems if the number of channels within each ROI are small as this will bias your random-effect estimates: remember that random effects are *variance* components and like all variance estimates, they require several observed levels for accurate estimation. (One rule of thumb I've heard is 10ish.) And as Payne et al saw in their own data, the channel factor typically doesn't help with model fit anyway and can hurt convergence, so I would just leave it out completely if you don't want to model it parametrically.

I think we already had an exchange on this point, a few months ago. I 
may have not specified that the analysis are carried out on two (small) 
subsets of contiguous electrodes (11 and 13 electrodes out of 60), where 
I would assume that all electrodes are representative of the effect, 
with some variation. For this reason I included channel as an 
intercept-only random-effect term. I keep on doing this because model's 
fit increases and I had no problems in convergence. I do not use this 
factor in the fixed effect structure as I am assuming that differences 
between selected electrodes are not relevant.

>> I'm not sure why you mentioned "semantic category" in your random-effect structure. In my experience, semantic category is typically something for which we care about the individual levels (of which there are not that many in any one experiment) and so are better modelled by fixed effects. (In other words, we care about the differences in processing between Furniture and People, not just that different categories show differences.) Items are good random effects, semantic categories are not. And it's not a problem if each item only belong to certain semantic categories. lme4 can handle such nesting structures. If you only have a few semantic categories, then you'll also run into computational / statistical trouble with treating them as random effects (see the last paragraph).

I do not think this is the way to go for our experimental design. In 
fact we focused on a large number (85) of semantic categories (maybe you 
are thinking of a handful of categories such as living/non-living) and 
we were interested in Typicality across all these categories. And I also 
think that it is correct to model random slopes for Typicality for each 
different semantic category because for each semantic category we have 2 
different words (one Typical and one Atypical member) so the 
manipulation is within item and the single item is the semantic 
category. I believe that the inclusion of Typicality as random slope for 
item (semantic categories) serves the idea of making the analysis more 
conservative, and the model "more" maximal: in fact each semantic 
category is associated with larger or smaller differences in typicality 
between the pair of words, and adding the slopes should relax the 
assumption that this difference is the same across categories.
>>
>> In short, I would propose the following model structure:
>>
>> mean_voltage ~ 1 + typicality * education * frequency * semantic_category + (1+... | subject) + (1+ ... | item)

I would agree with you if we were looking at differences between few 
categories. But because we are not, I guess the following would be ok:

mean_voltage ~ 1 + typicality * education * frequency + (1+... | 
subject) + (1+ ... | item) [not necessarily + (1|ch)]



>> Your particular choice of which slopes to include for each random-effect grouping term is a difficult one, as has been highlighted by the Baayen et al (2008), Barr et al (2013), Barr (2013) and the recent set of Bates preprints on parsimonious mixed models as well as a number of threads on this mailing list. Generally, I start off with main effects and if that model converges, great, if not, then I reduce more. In my experience with EEG studies on language, interactions in the random-effects structure just lead to overly complex models that take a long time to compute, fail to converge or show others signs of being degenerate. In other words, I would consider the following RE structure for your data:
>>
>>
>> (1 +  typicality + frequency + semantic_category | subject) + (1 +  education  | item)
>>
>> I left a lot out of the RE structure for Item because, assuming that each Item represents a single lemma / word, then it doesn't have different frequencies / categories / typicalities and so it doesn't make sense to consider a variable effect for something that is constant within the grouping unit. Similarly for education and subject.

As explained above I would keep slopes for item because item is not a 
single lemma but a pair of lemmas.

(1 +  typicality + frequency  | subject) + (1 +  education + typicality  
| item)

Now, this is a random structure that I like because it is simpler than 
the one I had.

>> If you don't model semantic category explicitly, then your item random effect should absorb the variance due to it.  You just won't have an explicit term in the model to point to that only describes the effect of semantic category (as item-level variance will cover a whole host of other effects related to the differences between words).
>>
>> (For posterity -- I think we discussed some of these issues previously on r-help: https://stat.ethz.ch/pipermail/r-help/2015-September/432561.html )
>>
>> To address some of your explicit questions more directly:
>>
>>> - Am I allowed to use the same complex random structure to compare the
>>> likelihood of models that have "simpler" fixed effects? In principle I
>>> guess it is correct to have the same random structure across comparisons.
>> Not quite. You should not have random slopes for effects not in the fixed-effect model structure because the mixed-model formulation used by lme4 assumes zero-mean for the random effects. In other words, lme4 random effects are estimates of how much the different grouping factors lead to variance around the population-level estimates delivered by the fixed effects.

OK so the random adjustments for trial order would only mitigate the 
population effect of trial order but does not help in better estimate 
the the other terms ("absorbing" some variance) if I do not include the 
term in the fixed effect structure. If I include it only in the fixed 
part, as a main effect, the model can explain the variance associated 
with fatigue or adaptation to the experimental setting, but it should 
not affect (interact with) the manipulation of Typicality, if lists were 
properly "randomized" (so I can motivate the choice of not looking at 
the interaction between trial and experimental factors)... it makes sense.

>>> - I am not interested in the effect of serial presentation (trial
>>> order), as it increases the order of the highest interaction. Is it
>>> appropriate to use it in the random structure only, or should I always
>>> discuss it in interaction with my factors of interest?
>>
>> No, for the reason above. But you could have the order of serial presentation a non-interacting / main-effect only fixed effect. Also, if you did the usual thing and you counterbalanced presentation order (e.g. via several different pseudo-random presentation orders/lists) across participants, then the usual assumption is that any effects of presentation order cancel out across participants. The item grouping factor will also absorb some of this variance.
>>
>> Best,
>> Phillip

To sum up. I will start with the following model, driven by the 
experimental design:

mod_between=lmer(eeg~trial + typicality * frequency * edu ...)

the random structure will be conservative but not over-specified:

(1 +  typicality + frequency  | subject) + (1 +  education + typicality  
| item)

to test the influence of cognitive variables on the ERP pattern 
associated with typicality I will keep the very same random structure 
and perform likelihood ratio tests between nested models such as

model_PredX=lmer(eeg~trial + typicality * frequency * edu + PredictorX ...)
model_PredXinteraction=lmer(eeg~trial + typicality * frequency * edu + 
PredictorX + typicality:PredictorX ...)
anova(model_PredX,model_PredXinteraction)

Keeping a less complex random structure has also the benefit of saving 
some degrees of freedom thus allowing for more easily detect higher 
order interactions.

My mind seems now a little bit clearer.
Thank you very much!
Paolo

>>> On 8 Nov 2016, at 21:48, Paolo Canal <paolo.canal at iusspavia.it> wrote:
>>>
>>> Dear Mixed-Group,
>>> I have acquired my data from one Experiment using a rather common
>>> paradigm in psycholinguistics. The experiment aimed at investigating the
>>> electro-physiological correlates of reading Typical (e.g., /chair/) vs
>>> Atypical (e.g., /foot rest/) members of a number (N=85) of semantic
>>> categories (e.g., /a kind of //Furniture/). In particular, we were
>>> interested in looking at differences associated with Education level
>>> (University N=24 vs non-University students N=23), and a three
>>> individual predictors. My issue is how to deal with some factors that
>>> are absolutely important in allowing for a better fit of the model, but
>>> make interpretations too "complicated".
>>>
>>> The two main factors of interest thus Typicality (categorical, Typical
>>> vs Atypical) and Education (categorical, Hi vs Low Education). I already
>>> know that the choice of taking these factors as dichotomic is
>>> questionable, but I believe, defensible: in fact, although the measure
>>> of Typicality is actually continuous (a proportion varying from 0 to 1)
>>> it is paired within each semantic category, because when we selected the
>>> materials we took the pair of exemplars that showed the largest
>>> difference in Typicality, so within each category is the difference in
>>> typicality that actually matters. Treating Education as categorical is
>>> less defensible, but in some way we wanted to compare the predictive
>>> power of this variable with more continuous variables representing a set
>>> of abilities (3 cognitive measure, one of which moderated by years of
>>> education and age), in some way to possibly show that some brain
>>> mechanisms are better described when accounting for individual variation
>>> rather than group differences.
>>>
>>> I used lmer in lme4 to analyze the effect of my independent variables on
>>> the average EEG voltage (continuous) from a set of EEG channels in two
>>> different time-windows of interest (I know GAMM would be even more
>>> appropriate than LMM, as what I am dealing with here are time-series,
>>> but I am not yet ready to try).
>>>
>>> I first determined the random effect structure, selecting three grouping
>>> factors (subject, semantic category and channel) which are clusters of
>>> repeated measures: for each item I have several subjects, for each
>>> subject I have several items and for each channel I have several items
>>> and subjects (perhaps channel might be nested in subject and item rather
>>> than stand alone, any hints?). For each grouping factor, I allowed
>>> intercepts to vary (e.g., 1|subject). Moreover, because I wanted to be
>>> conservative and data are rather malleable (no convergence failure, no
>>> variance = 0 or 1, not too high correlations between terms) I included a
>>> set of terms to adjust by-subject and by-item slopes. I allowed
>>> by-subject and by-item slope adjustments for Typicality (as it varies
>>> within subjects and within semantic category) and by-item slope
>>> adjustments for Education level.
>>>
>>> Things get more complicated when thinking of the influence of two
>>> variables that actually account for a lot of variation in the data:
>>> frequency of use of words and trial order. The first variable is also
>>> theoretically important and I want to include it as fixed effect; the
>>> second variable increases models' fit but because it makes the results
>>> less straightforward to interpret, I would not like to include in the
>>> fixed part of the model.
>>>
>>> This brings me to the fixed effect structure and the actual questions to
>>> the list:
>>>
>>> The initial design was very simple (2X2 plus covariates). The strategy
>>> was to fit the simple model Typicality + Frequency and evaluate if
>>> adding the interaction between Education (or the three covariates) and
>>> Typicality leads to relevant increase in likelihood, using always with
>>> the same random structure (the complex one).
>>>
>>> Now I am not so sure this is appropriate and I have a list of doubts:
>>> - Am I allowed to use the same complex random structure to compare the
>>> likelihood of models that have "simpler" fixed effects? In principle I
>>> guess it is correct to have the same random structure across comparisons.
>>> - I am not interested in the effect of serial presentation (trial
>>> order), as it increases the order of the highest interaction. Is it
>>> appropriate to use it in the random structure only, or should I always
>>> discuss it in interaction with my factors of interest?
>>>
>>> Thanks for any help
>>> Paolo
>>>
>>> 	[[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From Paul.Louisell at pw.utc.com  Sat Nov 12 00:52:44 2016
From: Paul.Louisell at pw.utc.com (Louisell, Paul T           PW)
Date: Fri, 11 Nov 2016 23:52:44 +0000
Subject: [R-sig-ME] nlme & varIdent
Message-ID: <96E6B0084A068C43B02C637889A5E0B73F7F37@UUSNWE1N.na.utcmail.com>

Hello,

All the help I've read (including Pinheiro and Bates book, 'Mixed Effects Models in S and S-PLUS') regarding how to fit a linear mixed-effects model where variances change with a factor's levels indicates this is done through the 'weights' argument to 'lme', using something like 'weights=varIdent(form=~v|g)' where 'v' is a variance covariate and 'g' is the grouping factor whose strata have different random effect variances.

My question: Suppose I have more than 1 variance covariate, say v1, ..., vk, and I want _each_ of these to have variances that change with the levels of g giving a total of k*nlevels(g) parameters (k*nlevels(g) - k allowing for identifiability). How is this handled in the nlme package? A simple example would be random slope and intercepts, _both_ of which have variances changing with the levels of g. I haven't found any examples of this online or in Pinheiro & Bates, and I haven't been able to figure this out using the various varFunc/pdMat classes. I'd use the 'lme4' package (instead of nlme), but I need the correlated residuals structure (e.g., 'corAR1', 'corARMA') provided in nlme.

Help/advice would be greatly appreciated.

Thanks,

Paul Louisell
Statistical Specialist
Paul.Louisell at pw.utc.com
860-565-8104

Still, tomorrow's going to be another working day, and I'm trying to get some rest.
That's all, I'm trying to get some rest. 
Paul Simon, "American Tune"


From thierry.onkelinx at inbo.be  Mon Nov 14 10:04:14 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 14 Nov 2016 10:04:14 +0100
Subject: [R-sig-ME] nlme & varIdent
In-Reply-To: <96E6B0084A068C43B02C637889A5E0B73F7F37@UUSNWE1N.na.utcmail.com>
References: <96E6B0084A068C43B02C637889A5E0B73F7F37@UUSNWE1N.na.utcmail.com>
Message-ID: <CAJuCY5yiEjcrDnTuo-6as6jM72LCS4eKw4wtxR7SYFDtYXobJQ@mail.gmail.com>

Dear Paul,

Note that variance functions work on the residuals, not the random effect
variances. I can't comment further on this as your question is not very
clear to me. Can you provide a more detailed example. E.g. the formula and
who you want to variance or correlation functions to work.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-11-12 0:52 GMT+01:00 Louisell, Paul T PW <Paul.Louisell at pw.utc.com>:

> Hello,
>
> All the help I've read (including Pinheiro and Bates book, 'Mixed Effects
> Models in S and S-PLUS') regarding how to fit a linear mixed-effects model
> where variances change with a factor's levels indicates this is done
> through the 'weights' argument to 'lme', using something like
> 'weights=varIdent(form=~v|g)' where 'v' is a variance covariate and 'g' is
> the grouping factor whose strata have different random effect variances.
>
> My question: Suppose I have more than 1 variance covariate, say v1, ...,
> vk, and I want _each_ of these to have variances that change with the
> levels of g giving a total of k*nlevels(g) parameters (k*nlevels(g) - k
> allowing for identifiability). How is this handled in the nlme package? A
> simple example would be random slope and intercepts, _both_ of which have
> variances changing with the levels of g. I haven't found any examples of
> this online or in Pinheiro & Bates, and I haven't been able to figure this
> out using the various varFunc/pdMat classes. I'd use the 'lme4' package
> (instead of nlme), but I need the correlated residuals structure (e.g.,
> 'corAR1', 'corARMA') provided in nlme.
>
> Help/advice would be greatly appreciated.
>
> Thanks,
>
> Paul Louisell
> Statistical Specialist
> Paul.Louisell at pw.utc.com
> 860-565-8104
>
> Still, tomorrow's going to be another working day, and I'm trying to get
> some rest.
> That's all, I'm trying to get some rest.
> Paul Simon, "American Tune"
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From kestrel1978 at gmail.com  Mon Nov 14 10:14:46 2016
From: kestrel1978 at gmail.com (Ronny Steen)
Date: Mon, 14 Nov 2016 10:14:46 +0100
Subject: [R-sig-ME] How to do exponential regression mixed effect model with
	function 'negexp.SSival'?
Message-ID: <CABVupNZTYsNmeJLKE7KW3LWJEn7kvu6huBihYP2Jwc61Ks1HvA@mail.gmail.com>

Hi,

I need help with fitting a non-linear mixed effects model (nested random
effect). I look at the relationship between 'wingbeat frequency' (beats per
sec) and 'wing length' in hummingbirds, and I will find a model with the
best fit.

I manage to fit Negative Natural Exponential regression and Assymptotic
(SSasymp-function) nonlinear regression, but I don't figure out how to fit
Negative Exponential regression (negexp.SSival) with nested random effect.

*My question:* Could I get some help adapting the Negative Exponential
regression model (*'nls'*, without random effect) to model with nested
random effect (*'nlme' or ''lme4*)?

Script and access to data:

library(nlme)

library(MASS)


WBF <-read.csv(url("
https://www.dropbox.com/s/hin8o27i1kmdloe/Species2016.csv.csv?raw=1"))

plot(WBF$WL,WBF$Beat_freq,type="p",ylab="WBF")# Make a plot

#Negative Natural Exponential regression with nested random effect

fm1<-lme(Beat_freq ~ exp(-WL),

data = WBF,

random = ~ 1|Species/ID, method = "ML")

#Assymptotic (SSasymp-function) nonlinear regression with nested random
effect

fm2 <- nlme(Beat_freq ~ SSasymp(WL, Asym, R0, lrc),

        data = WBF,

        fixed = Asym + R0 + lrc ~ 1,

        random = Asym ~ 1|Species/ID,

        start = c(Asym = 27, R0 = 6000, lrc = 0.19))

#Negative Exponential regression - HELP NEEDED TO INCLUDE NESTED RANDOM
EFFECT ('nlme')

negexp<-selfStart(model = ~b0 + b1*exp(-x/th), initial =

    negexp.SSival,parameters=c("b0","b1","th"),

    template=function(x,b0,b1,th){})

fm3<-nls(Beat_freq~negexp(WL,B0,B1,theta),

     data=WBF,control=list(maxiter =5000),trace=T)

Regards,

Ron

	[[alternative HTML version deleted]]


From benhog at hotmail.com  Tue Nov 15 00:48:38 2016
From: benhog at hotmail.com (ben hogan)
Date: Mon, 14 Nov 2016 23:48:38 +0000
Subject: [R-sig-ME] MCMCglmm phylogenetically controlled categorical R
 structure and priors help
Message-ID: <HE1PR0901MB10192C8EC800C27B9C0415A9C6BC0@HE1PR0901MB1019.eurprd09.prod.outlook.com>

Hello,


I have collected categorical data about the colouration of a number of bird species (4 levels), and am attempting to see if colouration is correlated with the proportions (as percentage) of different prey types, and the birds' length. There is only one observation of each factor for each bird species, i.e. one row in the data frame.


I am having difficulty in understanding the outcome of the R-structure, and how to properly define the priors. I don't have any specific predictions about priors, and I believe the code I have used is supposed to generate flat/uninformed ones, but am not sure if that it in fact the case (very new to the subject). I am not sure if both pedigree and ginverse are necessary, I get the similar output without pedigree.


As it stands my model and priors for a model on F_Wing2 are;


k <- length(levels(Data$F_Wing2))
IJ <- (1/k) * (diag(k-1) + matrix(1, k-1, k-1))

prior.phyl = list(R = list(V = IJ, nu = 0),G = list( G1 = list(V = IJ, n = k-1 , nu = 0) ) )


Ainv<-inverseA(Tree, scale=FALSE, nodes="TIPS")$Ainv

myMCMC.phyl<- MCMCglmm(F_Wing2 ~ Birds+Reptiles+Amphibians+Fish+Mammals+as.numeric(Length),
random=~us(trait):species,
rcov = ~us(trait):units,
pedigree=Tree,
scale=FALSE,
ginverse = list(species=Ainv),
data = Data, family="categorical",
prior=prior.phyl,
nitt=10000,
thin=25,
burnin=2000)

This runs, and nitt etc are artificially low for testing purposes. The outcome is something like this;

DIC: 160.7702

 G-structure:  ~us(trait):species

                                                    post.mean l-95% CI u-95% CI eff.samp
traitF_Wing2.Bicolour:traitF_Wing2.Bicolour.species    112.05    2.267    237.3    2.048
traitF_Wing2.Mottled:traitF_Wing2.Bicolour.species      75.29    1.450    160.9    4.397
traitF_Wing2.Plain:traitF_Wing2.Bicolour.species       125.34    3.015    268.0    2.707
traitF_Wing2.Bicolour:traitF_Wing2.Mottled.species      75.29    1.450    160.9    4.397
traitF_Wing2.Mottled:traitF_Wing2.Mottled.species       52.12    1.345    123.3    7.140
traitF_Wing2.Plain:traitF_Wing2.Mottled.species         85.01    2.861    188.4    4.175
traitF_Wing2.Bicolour:traitF_Wing2.Plain.species       125.34    3.015    268.0    2.707
traitF_Wing2.Mottled:traitF_Wing2.Plain.species         85.01    2.861    188.4    4.175
traitF_Wing2.Plain:traitF_Wing2.Plain.species          143.20    5.880    310.2    3.252

 R-structure:  ~us(trait):units

                                                  post.mean l-95% CI u-95% CI eff.samp
traitF_Wing2.Bicolour:traitF_Wing2.Bicolour.units      0.50     0.50     0.50        0
traitF_Wing2.Mottled:traitF_Wing2.Bicolour.units       0.25     0.25     0.25        0
traitF_Wing2.Plain:traitF_Wing2.Bicolour.units         0.25     0.25     0.25        0
traitF_Wing2.Bicolour:traitF_Wing2.Mottled.units       0.25     0.25     0.25        0
traitF_Wing2.Mottled:traitF_Wing2.Mottled.units        0.50     0.50     0.50        0
traitF_Wing2.Plain:traitF_Wing2.Mottled.units          0.25     0.25     0.25        0
traitF_Wing2.Bicolour:traitF_Wing2.Plain.units         0.25     0.25     0.25        0
traitF_Wing2.Mottled:traitF_Wing2.Plain.units          0.25     0.25     0.25        0
traitF_Wing2.Plain:traitF_Wing2.Plain.units            0.50     0.50     0.50        0

 Location effects: F_Wing2 ~ Birds + Reptiles + Amphibians + Fish + Mammals + as.numeric(Length)

                    post.mean   l-95% CI   u-95% CI eff.samp  pMCMC
(Intercept)        -10.128704 -19.972170  -0.276258   14.864 0.0187 *
Birds               -0.058857  -0.116441  -0.005308   24.074 0.0312 *
Reptiles            -0.059507  -0.148332   0.011748    6.368 0.1313
Amphibians          -0.098699  -0.336341   0.078327   12.190 0.3438
Fish                -0.104092  -0.226801  -0.010597   11.712 0.0563 .
Mammals             -0.006519  -0.062639   0.044000   17.301 0.7250
as.numeric(Length)   0.097417  -0.013097   0.232505    9.862 0.1187
---

As you can see, the R-Structure seems not to have been affected by the running of the model, and effective sample sizes are 0. 1) I am not sure why this is, and 2) the model doesn't run unless rcov = ~us(trait):units, but I do not understand what "units" refers to.

Any help greatly appreciated!

Best,
Ben Hogan





	[[alternative HTML version deleted]]


From jordirosich16 at gmail.com  Mon Nov 14 22:33:51 2016
From: jordirosich16 at gmail.com (Jordi Rosich)
Date: Mon, 14 Nov 2016 22:33:51 +0100
Subject: [R-sig-ME] Mixed Models convergence problems, Jordi Rosich
Message-ID: <CAJ6PopzBUtMaO5pLh+h0rPryk63fwFdfsHufCObKZ+neOXiYpw@mail.gmail.com>

Hello,



I'm Jordi Rosich, a student currently collaborating with the Biology
Conservation Group of the University of Barcelona. I'm writing you because
I'm having model convergence troubles with some GLMMs using the function
glmer of the package lme4 of R.

Our research addresses nest-site selection of the Goshawk, a territorial
bird of prey, and specifically we aim to understand which environmental
variables are relevant for nest site-selection in this species.




*Our approach**:*

1)     We sampled several environmental variables in sites used by this
bird species in nest (1) and unoccupied sites (0), and therefore occupation
status (0/1) is our dependent variable and the environmental variables our
independent variables.

2)     We performed  three analysis at 3 different spatial-scales: nest
tree, nest-site forest (being the 18 meters radius circular area around the
nest-tree), and landscape around nest-site (500 meters radius circular area
around the nest-tree).

3)     We sampled 29 Goshawk nests comprised in 13 breeding pairs
territories (each territory may hold several nests) and 30 control
non-occupied random trees comprised in 25 "pseudo-territories" (the near
trees being included in this "territories"). To avoid pseudoreplication of
nests of the same breeding pair (or territory) we have considered the
factor "Territory" as our random factor in the mixed models. The model
definition is approximately Y = explanatory variables + (1|id  Territory),
where Y is the occupation status (0/1) (See an example below).

4)     Our independent variables are both categorical (e.g. tree species;
aspect: an angle recoded into 4 categories) and continuous (components
resulting of a PCA on several original environmental variables, performed
to reduce the number of original variables). For more details for each
analysis:


-Nest-tree scale: 2 continuous variables (FAC1TreeHeight, FAC2TreeWidth)
and 1 four level categorical variable (TreeSpecies).



-Nest-site forest scale: 4 continuous variables (FAC1BroadLeavedTrees,
FAC2YoungPines, FAC3MaturePines, FAC4SlopeAndShrubs) and 1 four level
categorical variable (ForestAspect).



-Landscape scale: 3 continuous variables (FAC1PinusVSQuercus,
FAC2HumanizedLand, FAC3DistanceToRoads).







One example of the script used to model would be:



mod1 <- glmer(Occupation~FAC1Treeheight+FAC2TreeWidth+Sp+(1|Territory),
family=binomial, data=trees)





*The problem: *



While creating the candidate models with glmer function to later select the
best ones by their AICcs, we've faced some warnings telling that some of
the models are failing to converge:



In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :

  Model failed to converge with max|grad| = 0.0483015 (tol = 0.001,
component 1)



It happens specially, although not always, with the models with more
parameters and also with those containing categorical variables.




*My question: *



Given our variables, random factor and data is there any particular reason
why our models could fail to converge? If so, is there a possible solution
to the convergence problem?





Thank you very much in advance. Waiting for your answer,


Jordi Rosich

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Tue Nov 15 05:56:19 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 14 Nov 2016 23:56:19 -0500
Subject: [R-sig-ME] Fwd: Re: unplausible zero intercept variance in lmer
In-Reply-To: <ab5bdd41-70a1-453b-a0f5-3b5d2f7acb1d@mcmaster.ca>
References: <ab5bdd41-70a1-453b-a0f5-3b5d2f7acb1d@mcmaster.ca>
Message-ID: <f96bbae7-98eb-7e3d-0f90-68648536c01c@gmail.com>


(forwarded from off-line query)

... I have consulted the help pages and noticed that others have
> posted similar problems, but I could not find a satisfactory solution.
> The problem is that lmer evaluates the intercept variance at 0, although
> it is clear that there is variance when you look at the data.
> 
> The problem is in the model fit.disteffect (see below for the output).
> This includes only one factor from a two-factorial design. In the full
> model, the intercept variance is non-zero. A reviewer asked for effect
> size information, so I calculated partial models, and then I noticed
> this odd result.
> 
> I?d be very glad if you could have a quick look and let me know how to
> circumvent the problem. Code is below, data attached. I am hoping that
> this allows you to reproduce the error.
> 

The problem here is really a fundamental one; it's not a mistake,
although you could argue (see below) that it is a limitation of the
class of models that lme4 implements. The basic problem is that realized
(observed) among-group variation (which includes a term due to
among-group variance and a term due to residual variance) is actually
_smaller_ than the among-group variation that would be expected from
independent observations.  This is discussed some at
http://tinyurl.com/glmmFAQ#zero-variance (to the general readership of
r-sig-mixed-models : clarifications, feedback, additional references are
always welcome!).  Another way of putting it is that the "group effects"
model implemented by lme4  (i.e. y_{ij} = (fixed covariates) + eps_{1,i}
+ eps_{2,ij}) only allows for positive correlations among the
observations within a group.


Data not included in this post, but here is the data manipulation:

library(lme4)
library(reshape2)
dat <- read.csv2("kuc_vod_study3.csv",header=TRUE)
d <- melt(dat,
          measure.vars=c("dist_1_20","dist_100_20",
          "dist_1_80","dist_100_80"))
d <- transform(d,
       variable=factor(variable,levels(variable)[c(1,3,2,4)]),
       dist=ifelse(grepl("^dist_1_",variable),-0.5,0.5),
       exp=ifelse(grepl("_80$",variable),0.5,-0.5))
## define observations within subjects
library(plyr)
d <- ddply(d,"subj_no",mutate,obs=factor(1:4))

 Here's the basic model:

fit.disteffect <- lmer(value ~ dist + (1  | subj_no), data = d)

... and as promised the estimated among-subject variance is zero.

VarCorr(fit.disteffect)
##  Groups   Name        Std.Dev.
##  subj_no  (Intercept)  0.000
##  Residual             37.849

... although the *upper* confidence interval of the among-group std dev
is perfectly sensible.

confint(fit.disteffect,parm="theta_")
Computing profile confidence intervals ...
          2.5 %    97.5 %
.sig01  0.00000  7.534858
.sigma 36.27066 39.393044

One solution to this is to use nlme::lme, which allows for a
compound-symmetric model:

library(nlme)
fit.disteffect2 <- lme(value ~ dist,
                       random=list(subj_no=pdCompSymm(~obs-1)),
                       data = d)
VarCorr(fit.disteffect2)
## subj_no = pdCompSymm(obs - 1)
##          Variance  StdDev   Corr
## obs1     1255.9355 35.43918
## obs2     1255.9355 35.43918 -0.011
## obs3     1255.9355 35.43918 -0.011 -0.011
## obs4     1255.9355 35.43918 -0.011 -0.011 -0.011
## Residual  176.6014 13.28915

  ... this shows that the estimated within-group variances are
(slightly) negative

intervals(fit.disteffect2)
## Approximate 95% confidence intervals
...

## Random Effects:
##  Level: subj_no
##               lower       est.        upper
## std. dev  0.01073749 35.4391804 1.169673e+05
##corr.    -0.16154262 -0.0112256 2.091866e-01

  The confidence interval on the correlations (-0.16 to 0.2) seems
reasonable, but the CIs on the standard deviation are ridiculous (i.e.,
the Wald approximation has failed)

The glmmTMB package also has some (experimental!) capacity for fitting
compound-symmetric models, and lme4 _could_ be hacked to do
compound-symmetric models (without touching the underlying code) if one
wanted to badly enough ...


From Phillip.Alday at unisa.edu.au  Tue Nov 15 06:19:08 2016
From: Phillip.Alday at unisa.edu.au (Phillip Alday)
Date: Tue, 15 Nov 2016 05:19:08 +0000
Subject: [R-sig-ME] Mixed Models convergence problems, Jordi Rosich
In-Reply-To: <CAJ6PopzBUtMaO5pLh+h0rPryk63fwFdfsHufCObKZ+neOXiYpw@mail.gmail.com>
References: <CAJ6PopzBUtMaO5pLh+h0rPryk63fwFdfsHufCObKZ+neOXiYpw@mail.gmail.com>
Message-ID: <1479187148.2575.10.camel@unisa.edu.au>

Hi Jordi,?

Without really knowing anything about your data (or more generally
types of data common to your field) ....?

- Your model doesn't seem exceptionally complex -- just main effects
and a single scalar (intercept-only) random effect. Of course, a simple
model can still be "too" complex if you don't have much data.

- However, it sometimes makes sense to use a more complicated model
when you have convergence issues on a simple model -- sometimes, you
really do need covariates to get any type of decent fit. (Based on your
email, you may have already ?experienced this.)

- Categorical variables are particularly 'nasty' when it comes to the
number of model parameters as a categorical variable with n levels
requires n-1 parameters in the model! Continuous variables only require
1 parameter apiece (correlation parameters in the random effects
excepted).

- Watch out for multicollinearity -- how strongly do tree height and
tree width correlate with each other??

- Your particular convergence warning often means that the optimiser
was still moving along towards convergence / the solution when
optimisation was stoppe. Sometimes this can be helped by just
increasing the number of iterations that the optimiser is allowed to
take, although this will increase computer time.

- Make sure to check out the help page: ?convergence (after loading
lme4) has many tips and tricks.

Best,
Phillip

On Mon, 2016-11-14 at 22:33 +0100, Jordi Rosich wrote:
> Hello,
> 
> 
> 
> I'm Jordi Rosich, a student currently collaborating with the Biology
> Conservation Group of the University of Barcelona. I'm writing you
> because
> I'm having model convergence troubles with some GLMMs using the
> function
> glmer of the package lme4 of R.
> 
> Our research addresses nest-site selection of the Goshawk, a
> territorial
> bird of prey, and specifically we aim to understand which
> environmental
> variables are relevant for nest site-selection in this species.
> 
> 
> 
> 
> *Our approach**:*
> 
> 1)?????We sampled several environmental variables in sites used by
> this
> bird species in nest (1) and unoccupied sites (0), and therefore
> occupation
> status (0/1) is our dependent variable and the environmental
> variables our
> independent variables.
> 
> 2)?????We performed??three analysis at 3 different spatial-scales:
> nest
> tree, nest-site forest (being the 18 meters radius circular area
> around the
> nest-tree), and landscape around nest-site (500 meters radius
> circular area
> around the nest-tree).
> 
> 3)?????We sampled 29 Goshawk nests comprised in 13 breeding pairs
> territories (each territory may hold several nests) and 30 control
> non-occupied random trees comprised in 25 "pseudo-territories" (the
> near
> trees being included in this "territories"). To avoid
> pseudoreplication of
> nests of the same breeding pair (or territory) we have considered the
> factor "Territory" as our random factor in the mixed models. The
> model
> definition is approximately Y = explanatory variables +
> (1|id??Territory),
> where Y is the occupation status (0/1) (See an example below).
> 
> 4)?????Our independent variables are both categorical (e.g. tree
> species;
> aspect: an angle recoded into 4 categories) and continuous
> (components
> resulting of a PCA on several original environmental variables,
> performed
> to reduce the number of original variables). For more details for
> each
> analysis:
> 
> 
> -Nest-tree scale: 2 continuous variables (FAC1TreeHeight,
> FAC2TreeWidth)
> and 1 four level categorical variable (TreeSpecies).
> 
> 
> 
> -Nest-site forest scale: 4 continuous variables
> (FAC1BroadLeavedTrees,
> FAC2YoungPines, FAC3MaturePines, FAC4SlopeAndShrubs) and 1 four level
> categorical variable (ForestAspect).
> 
> 
> 
> -Landscape scale: 3 continuous variables (FAC1PinusVSQuercus,
> FAC2HumanizedLand, FAC3DistanceToRoads).
> 
> 
> 
> 
> 
> 
> 
> One example of the script used to model would be:
> 
> 
> 
> mod1 <-
> glmer(Occupation~FAC1Treeheight+FAC2TreeWidth+Sp+(1|Territory),
> family=binomial, data=trees)
> 
> 
> 
> 
> 
> *The problem: *
> 
> 
> 
> While creating the candidate models with glmer function to later
> select the
> best ones by their AICcs, we've faced some warnings telling that some
> of
> the models are failing to converge:
> 
> 
> 
> In checkConv(attr(opt, "derivs"), opt$par, ctrl =
> control$checkConv,??:
> 
> ? Model failed to converge with max|grad| = 0.0483015 (tol = 0.001,
> component 1)
> 
> 
> 
> It happens specially, although not always, with the models with more
> parameters and also with those containing categorical variables.
> 
> 
> 
> 
> *My question: *
> 
> 
> 
> Given our variables, random factor and data is there any particular
> reason
> why our models could fail to converge? If so, is there a possible
> solution
> to the convergence problem?
> 
> 
> 
> 
> 
> Thank you very much in advance. Waiting for your answer,
> 
> 
> Jordi Rosich
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

From j.hadfield at ed.ac.uk  Tue Nov 15 15:34:06 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Tue, 15 Nov 2016 14:34:06 +0000
Subject: [R-sig-ME] MCMCglmm phylogenetically controlled categorical R
 structure and priors help
In-Reply-To: <HE1PR0901MB10192C8EC800C27B9C0415A9C6BC0@HE1PR0901MB1019.eurprd09.prod.outlook.com>
References: <HE1PR0901MB10192C8EC800C27B9C0415A9C6BC0@HE1PR0901MB1019.eurprd09.prod.outlook.com>
Message-ID: <3d713c1f-6bf2-249d-4524-b51b31301af7@ed.ac.uk>

Hi Ben,

If each species can only belong to one category (?) the choice of 
R-structure is arbitrary since observation-level variation is not 
identifiable in the likelihood.

You probably want to add trait as a main effect, and interact it with 
your other predictors too. Currently the model assumes that the 
probability of  being Bicoloured/Mottled/Plain compared to the base-line 
category (not sure what that is) is the same and that they are all 
affected by diet in the same way. This may make sense depending on what 
the base-line category is, but I doubt it.

The phylogenetic variance looks large, but it is hard to say because you 
have specified scale=FALSE. If it really is large it is possible that 
you will run into numerical errors. If you have pl=TRUE in your call to 
MCMCglmm, and then do a histogram of my_model$Liab, make sure the 
distribution is contained within +/-25. If this is not so then you will 
have to use software that assumes the phylogenetic heritability is 1. I 
have developed algorithms under this scenario but only for ordered 
categorical traits.

I would just use the ginverse argument (the pedigree argument is 
redundant) and use the default scale=TRUE and  nodes="ALL" not 
nodes="TIPS". The model is the same, but the former allows sparse linear 
solvers to exploit the special sparse structure of phylogenies. It will 
probably be orders of magnitude faster for large phylogenies.

Note that this type of analysis is VERY data demanding and you will need 
many hundreds of species to get precise estimates, particularly if some 
wing pattern types are rare. If the data set is modest in size expect 
the results to be sensitive to your choice of prior for the phylogenetic 
species effects.

Cheers,

Jarrod



On 14/11/2016 23:48, ben hogan wrote:
> Hello,
>
>
> I have collected categorical data about the colouration of a number of bird species (4 levels), and am attempting to see if colouration is correlated with the proportions (as percentage) of different prey types, and the birds' length. There is only one observation of each factor for each bird species, i.e. one row in the data frame.
>
>
> I am having difficulty in understanding the outcome of the R-structure, and how to properly define the priors. I don't have any specific predictions about priors, and I believe the code I have used is supposed to generate flat/uninformed ones, but am not sure if that it in fact the case (very new to the subject). I am not sure if both pedigree and ginverse are necessary, I get the similar output without pedigree.
>
>
> As it stands my model and priors for a model on F_Wing2 are;
>
>
> k <- length(levels(Data$F_Wing2))
> IJ <- (1/k) * (diag(k-1) + matrix(1, k-1, k-1))
>
> prior.phyl = list(R = list(V = IJ, nu = 0),G = list( G1 = list(V = IJ, n = k-1 , nu = 0) ) )
>
>
> Ainv<-inverseA(Tree, scale=FALSE, nodes="TIPS")$Ainv
>
> myMCMC.phyl<- MCMCglmm(F_Wing2 ~ Birds+Reptiles+Amphibians+Fish+Mammals+as.numeric(Length),
> random=~us(trait):species,
> rcov = ~us(trait):units,
> pedigree=Tree,
> scale=FALSE,
> ginverse = list(species=Ainv),
> data = Data, family="categorical",
> prior=prior.phyl,
> nitt=10000,
> thin=25,
> burnin=2000)
>
> This runs, and nitt etc are artificially low for testing purposes. The outcome is something like this;
>
> DIC: 160.7702
>
>   G-structure:  ~us(trait):species
>
>                                                      post.mean l-95% CI u-95% CI eff.samp
> traitF_Wing2.Bicolour:traitF_Wing2.Bicolour.species    112.05    2.267    237.3    2.048
> traitF_Wing2.Mottled:traitF_Wing2.Bicolour.species      75.29    1.450    160.9    4.397
> traitF_Wing2.Plain:traitF_Wing2.Bicolour.species       125.34    3.015    268.0    2.707
> traitF_Wing2.Bicolour:traitF_Wing2.Mottled.species      75.29    1.450    160.9    4.397
> traitF_Wing2.Mottled:traitF_Wing2.Mottled.species       52.12    1.345    123.3    7.140
> traitF_Wing2.Plain:traitF_Wing2.Mottled.species         85.01    2.861    188.4    4.175
> traitF_Wing2.Bicolour:traitF_Wing2.Plain.species       125.34    3.015    268.0    2.707
> traitF_Wing2.Mottled:traitF_Wing2.Plain.species         85.01    2.861    188.4    4.175
> traitF_Wing2.Plain:traitF_Wing2.Plain.species          143.20    5.880    310.2    3.252
>
>   R-structure:  ~us(trait):units
>
>                                                    post.mean l-95% CI u-95% CI eff.samp
> traitF_Wing2.Bicolour:traitF_Wing2.Bicolour.units      0.50     0.50     0.50        0
> traitF_Wing2.Mottled:traitF_Wing2.Bicolour.units       0.25     0.25     0.25        0
> traitF_Wing2.Plain:traitF_Wing2.Bicolour.units         0.25     0.25     0.25        0
> traitF_Wing2.Bicolour:traitF_Wing2.Mottled.units       0.25     0.25     0.25        0
> traitF_Wing2.Mottled:traitF_Wing2.Mottled.units        0.50     0.50     0.50        0
> traitF_Wing2.Plain:traitF_Wing2.Mottled.units          0.25     0.25     0.25        0
> traitF_Wing2.Bicolour:traitF_Wing2.Plain.units         0.25     0.25     0.25        0
> traitF_Wing2.Mottled:traitF_Wing2.Plain.units          0.25     0.25     0.25        0
> traitF_Wing2.Plain:traitF_Wing2.Plain.units            0.50     0.50     0.50        0
>
>   Location effects: F_Wing2 ~ Birds + Reptiles + Amphibians + Fish + Mammals + as.numeric(Length)
>
>                      post.mean   l-95% CI   u-95% CI eff.samp  pMCMC
> (Intercept)        -10.128704 -19.972170  -0.276258   14.864 0.0187 *
> Birds               -0.058857  -0.116441  -0.005308   24.074 0.0312 *
> Reptiles            -0.059507  -0.148332   0.011748    6.368 0.1313
> Amphibians          -0.098699  -0.336341   0.078327   12.190 0.3438
> Fish                -0.104092  -0.226801  -0.010597   11.712 0.0563 .
> Mammals             -0.006519  -0.062639   0.044000   17.301 0.7250
> as.numeric(Length)   0.097417  -0.013097   0.232505    9.862 0.1187
> ---
>
> As you can see, the R-Structure seems not to have been affected by the running of the model, and effective sample sizes are 0. 1) I am not sure why this is, and 2) the model doesn't run unless rcov = ~us(trait):units, but I do not understand what "units" refers to.
>
> Any help greatly appreciated!
>
> Best,
> Ben Hogan
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From chanratana.pin at gmail.com  Fri Nov 18 05:29:10 2016
From: chanratana.pin at gmail.com (Pin chanratana)
Date: Fri, 18 Nov 2016 11:29:10 +0700
Subject: [R-sig-ME] Mixed effect model, Hurdle function
Message-ID: <CANW7wbdigTVrs+Y211KHx=vLcrR=Gz6knYs0zbi4B2YO9th_6Q@mail.gmail.com>

Hi everyone here,

My name's Ratana, and I'm a Msc. student studying conservation ecology.

I'm new to the using of mixed effects model. I try to fit the mixed effect
models include an offset term (which is trapnight)  of my camera-trap data
by using hurdle function. The following are the model that I fit.

m1 <- hurdle(GI ~
depth+Ele+dRoad+offset(log(TN))|depth+Ele+offset(log(TN)), data=ndata2,
dist="poisson", zero.dist="poisson")

m2 <- hurdle(GI ~ depth+Ele+dRoad+offset(TN2)|depth+Ele+offset(TN2),
data=ndata2, dist="poisson", zero.dist="poisson")


GI: Giant ibis
depth: depth of waterholes
Ele: Elevation
dRoad: distance to Road
TN: Trap-night
TN2: standardize or scale of Trap-night

But, I could not fit model m1 and there are error message: Error in
glm.fit(X, Y, family = poisson(), weights = weights, offset = offsetx) :
NA/NaN/Inf in 'y'

Model m2 is work fine and the result look reasonable.

I would like to ask if anyone know, is the way I do with model 2 is correct
by standardize the Trap-night that's use for offset in the fitting model?
If it's ok to do so, is there any reference (that I can cite) regarding to
this kind of process ?

Regards,

Ratana

	[[alternative HTML version deleted]]


From tim.cole at ucl.ac.uk  Fri Nov 18 12:09:47 2016
From: tim.cole at ucl.ac.uk (Cole, Tim)
Date: Fri, 18 Nov 2016 11:09:47 +0000
Subject: [R-sig-ME] nlme, nlmer and gradient
Message-ID: <D45490E6.4FF71%tim.cole@ucl.ac.uk>

Resending with correct r-sig-mixed-models address

Hi Ben,

My CRAN sitar library implements Mary Lindstrom's 1995 growth curve model using nlme, a natural B-spline mean curve and 3 subject random effects. I'm now trying to port it to lme4 by adding the gradient attribute to the function. I'm fairly sure the derivatives are correct, but it fails with error: step factor reduced below 0.001 without reducing pwrss.

I've seen your Algal (non)-linear mixed model example<http://www.rpubs.com/bbolker/3423> where nlmer fails with the same error, which you attribute to the fragility of nlmer. Is that likely to be the explanation here too?

The curious thing is that the same model fits fine in nlme, unless I include the gradient attribute in which case it fails with the equivalent error: step halving factor reduced below minimum in PNLS step. I'm wondering why the "fragility" of nlmer should also show itself in nlme when the gradient is provided, but not otherwise.

Some example code follows, using data from the heights dataframe in sitar and a spline curve with 6 df.

  library(sitar)
  library(lme4)

# nlme function without gradient
  fitnlme <- function(x,s1,s2,s3,s4,s5,s6,a,b,c) {
    c(
      (cbind(s1,s2,s3,s4,s5,s6) * ns((x - b) * exp(c), knots=knots, B=bounds))
      %*% matrix(rep(1,df), ncol=1)
    ) + a
  }

# nlmer function with gradient
  fitnlmer <- function(x,s1,s2,s3,s4,s5,s6,a,b,c) {
    nsx <- function(xx, bb, cc) ns((xx - bb) * exp(cc), knots=knots, B=bounds)
    ey <- function(mat1, mat2) c((mat1 * mat2) %*% matrix(rep(1, df), ncol=1))
    css <- cbind(s1,s2,s3,s4,s5,s6)
    cns <- nsx(x, b, c)
    dimnames(cns)[[2]] <- dimnames(css)[[2]]
    y <- ey(css, cns)
    delta <- 0.1
    dydb <- (ey(css, nsx(x, b + delta, c)) - y) / delta
    dydc <- (ey(css, nsx(x, b, c + delta)) - y) / delta
    attr(y, 'gradient') <- cbind(cns, a=1, b=dydb, c=dydc)
    y + a
  }

# set up model
  x <- with(heights, age - mean(age))
  bounds <- range(x) + 0.04 * c(-1,1) * diff(range(x))
  df <- 6
  knots <- quantile(x, 1:(df-1)/df)

# starting values
  start <- setNames(coef(lm(height ~ ns(x, knots=knots, B=bounds), data=heights)), c('a', paste0('s', 1:df)))
  start <- c(start[c(1:df+1, 1)], b=0, c=0)

# nlme fits using fitnlme without gradient
  nlme1 <- nlme(height ~ fitnlme(x,s1,s2,s3,s4,s5,s6,a,b,c),
       fixed = s1+s2+s3+s4+s5+s6+a+b+c ~ 1, random = a+b+c ~ 1 | id, data = heights, start = start)

# the equivalent nlme model in the sitar library also fits
  sitar1 <- sitar(age, height, id, heights, 6)

# nlme fails using fitnlmer with gradient
  nlme2 <- nlme(height ~ fitnlmer(x,s1,s2,s3,s4,s5,s6,a,b,c),
       fixed = s1+s2+s3+s4+s5+s6+a+b+c ~ 1, random = a+b+c ~ 1 | id, data = heights, start = start)

# nlmer fails
  nlmer1 <- nlmer(height ~ fitnlmer(x,s1,s2,s3,s4,s5,s6,a,b,c) ~ s1+s2+s3+s4+s5+s6+a+b+c + (a+b+c)|id, data=heights, start=start)

I'd appreciate your thoughts. And would adding starting values for theta likely make any difference?

Best wishes,
Tim
---
tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk> Phone +44(0)20 7905 2666 Fax +44(0)20 7905 2381
Population Policy and Practice Programme
UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Fri Nov 18 15:38:20 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 18 Nov 2016 09:38:20 -0500
Subject: [R-sig-ME] Mixed effect model, Hurdle function
In-Reply-To: <CANW7wbdigTVrs+Y211KHx=vLcrR=Gz6knYs0zbi4B2YO9th_6Q@mail.gmail.com>
References: <CANW7wbdigTVrs+Y211KHx=vLcrR=Gz6knYs0zbi4B2YO9th_6Q@mail.gmail.com>
Message-ID: <d6e0c641-d654-0605-89c5-fd6d78338861@gmail.com>


  This isn't really a "mixed effect model" in the standard terminology
(there's no random effect). Nevertheless ...

On 16-11-17 11:29 PM, Pin chanratana wrote:
> Hi everyone here,
> 
> My name's Ratana, and I'm a Msc. student studying conservation ecology.
> 
> I'm new to the using of mixed effects model. I try to fit the mixed effect
> models include an offset term (which is trapnight)  of my camera-trap data
> by using hurdle function. The following are the model that I fit.
> 
> m1 <- hurdle(GI ~
> depth+Ele+dRoad+offset(log(TN))|depth+Ele+offset(log(TN)), data=ndata2,
> dist="poisson", zero.dist="poisson")
> 
> m2 <- hurdle(GI ~ depth+Ele+dRoad+offset(TN2)|depth+Ele+offset(TN2),
> data=ndata2, dist="poisson", zero.dist="poisson")

  I'm unfamiliar with models that use the censored Poisson for their
hurdle model (binomial is more standard in my experience), but whatever.


> 
> 
> GI: Giant ibis
> depth: depth of waterholes
> Ele: Elevation
> dRoad: distance to Road
> TN: Trap-night
> TN2: standardize or scale of Trap-night
> 
> But, I could not fit model m1 and there are error message: Error in
> glm.fit(X, Y, family = poisson(), weights = weights, offset = offsetx) :
> NA/NaN/Inf in 'y'
> 
> Model m2 is work fine and the result look reasonable.

  Your first model looks more correct/standard; log(exposure) is the
standard offset in a Poisson count model.  (Not as clear what to use as
an offset for the hurdle; I would actually say that a log-exposure
offset with a complementary log-log link would actually make the most
sense for a binomial, but I haven't thought about how that would go
together with a censored Poisson ...)

  Is it possible that you have some observations in your data with TN=0?
That would cause the first model to fail. (It wouldn't really make
sense, but I've seen observational data like this where the person
taking the data rounded down to zero trap-nights.)

  Could you please send follow-up questions, if any, to
r-sig-ecology at r-project.org ?

> 
> I would like to ask if anyone know, is the way I do with model 2 is correct
> by standardize the Trap-night that's use for offset in the fitting model?
> If it's ok to do so, is there any reference (that I can cite) regarding to
> this kind of process ?
> 
> Regards,
> 
> Ratana
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From chanratana.pin at gmail.com  Fri Nov 18 16:05:06 2016
From: chanratana.pin at gmail.com (Pin chanratana)
Date: Fri, 18 Nov 2016 22:05:06 +0700
Subject: [R-sig-ME] Mixed effect model, Hurdle function
In-Reply-To: <d6e0c641-d654-0605-89c5-fd6d78338861@gmail.com>
References: <CANW7wbdigTVrs+Y211KHx=vLcrR=Gz6knYs0zbi4B2YO9th_6Q@mail.gmail.com>
	<d6e0c641-d654-0605-89c5-fd6d78338861@gmail.com>
Message-ID: <CANW7wbeaJ+vRLcb7kCFKPu7ig1Z-aQyCALY9j21AsCYNW7cpzg@mail.gmail.com>

Hi Ben,

Thanks for your response, there is an zero in the TN that I accidentally
made it. Now I fixed it, and now it work fine.

Best,

Ratana

On Fri, Nov 18, 2016 at 9:38 PM, Ben Bolker <bbolker at gmail.com> wrote:

>
>   This isn't really a "mixed effect model" in the standard terminology
> (there's no random effect). Nevertheless ...
>
> On 16-11-17 11:29 PM, Pin chanratana wrote:
> > Hi everyone here,
> >
> > My name's Ratana, and I'm a Msc. student studying conservation ecology.
> >
> > I'm new to the using of mixed effects model. I try to fit the mixed
> effect
> > models include an offset term (which is trapnight)  of my camera-trap
> data
> > by using hurdle function. The following are the model that I fit.
> >
> > m1 <- hurdle(GI ~
> > depth+Ele+dRoad+offset(log(TN))|depth+Ele+offset(log(TN)), data=ndata2,
> > dist="poisson", zero.dist="poisson")
> >
> > m2 <- hurdle(GI ~ depth+Ele+dRoad+offset(TN2)|depth+Ele+offset(TN2),
> > data=ndata2, dist="poisson", zero.dist="poisson")
>
>   I'm unfamiliar with models that use the censored Poisson for their
> hurdle model (binomial is more standard in my experience), but whatever.
>
>
> >
> >
> > GI: Giant ibis
> > depth: depth of waterholes
> > Ele: Elevation
> > dRoad: distance to Road
> > TN: Trap-night
> > TN2: standardize or scale of Trap-night
> >
> > But, I could not fit model m1 and there are error message: Error in
> > glm.fit(X, Y, family = poisson(), weights = weights, offset = offsetx) :
> > NA/NaN/Inf in 'y'
> >
> > Model m2 is work fine and the result look reasonable.
>
>   Your first model looks more correct/standard; log(exposure) is the
> standard offset in a Poisson count model.  (Not as clear what to use as
> an offset for the hurdle; I would actually say that a log-exposure
> offset with a complementary log-log link would actually make the most
> sense for a binomial, but I haven't thought about how that would go
> together with a censored Poisson ...)
>
>   Is it possible that you have some observations in your data with TN=0?
> That would cause the first model to fail. (It wouldn't really make
> sense, but I've seen observational data like this where the person
> taking the data rounded down to zero trap-nights.)
>
>   Could you please send follow-up questions, if any, to
> r-sig-ecology at r-project.org ?
>
> >
> > I would like to ask if anyone know, is the way I do with model 2 is
> correct
> > by standardize the Trap-night that's use for offset in the fitting model?
> > If it's ok to do so, is there any reference (that I can cite) regarding
> to
> > this kind of process ?
> >
> > Regards,
> >
> > Ratana
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From mlevins at purdue.edu  Mon Nov 21 16:43:06 2016
From: mlevins at purdue.edu (Levine, Michael)
Date: Mon, 21 Nov 2016 15:43:06 +0000
Subject: [R-sig-ME] About glmer.nb
Message-ID: <d79f710cd02f41afb351c7547bdcfbf3@wppexc04.purdue.lcl>

Dear Prof. B. Bolker,

While working on one of my projects, I encountered the function glmer.nb that is a part of the package lme4 that you maintain.  As far as I can see, one can only obtain a variance-covariance matrix of estimated fixed effects using this function...Is there any way for me to obtain a full variance-covariance matrix that includes covariances between fixed effects and estimated variance of the Gaussian random effect?

 Thank you very much in advance for your help!

Yours,

Michael Levine
Associate Professor of Statistics

Department of Statistics
Purdue University
250 North University Street
West Lafayette, IN 47907 USA

email: mlevins at purdue.edu
Phone: +1-765-496-7571
Fax:?? +1-765-494-0558
URL:?? www.stat.purdue.edu/~mlevins


From bbolker at gmail.com  Tue Nov 22 04:48:33 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 21 Nov 2016 22:48:33 -0500
Subject: [R-sig-ME] About glmer.nb
In-Reply-To: <d79f710cd02f41afb351c7547bdcfbf3@wppexc04.purdue.lcl>
References: <d79f710cd02f41afb351c7547bdcfbf3@wppexc04.purdue.lcl>
Message-ID: <287176c8-38d5-7b89-7d43-c6eb6db87d21@gmail.com>


For GLMMs, R computes the full Hessian of all of the top-level
(theta=var-cov parameters + beta=fixed-effect parameters); it is
available, with a little digging, in the fitted object.

Set up example:

library(lme4)
## from ?glmer.nb
set.seed(101)
dd <- expand.grid(f1 = factor(1:3),
                  f2 = LETTERS[1:2], g=1:9, rep=1:15,
                  KEEP.OUT.ATTRS=FALSE)
mu <- 5*(-4 + with(dd, as.integer(f1) + 4*as.numeric(f2)))
dd$y <- rnbinom(nrow(dd), mu = mu, size = 0.5)
m.nb <- glmer.nb(y ~ f1*f2 + (1|g), data=dd)


   Now pull out the full variance-covariance matrix:

## extract Hessian
hh <- m.nb at optinfo$derivs$Hessian
## invert
vv <- solve(hh)
## double/symmetrize (move from deviance to log-likelihood scale)
v2 <- unname(as.matrix(forceSymmetric(vv + t(vv))))
## compare ...
v3 <- unname(as.matrix(vcov(m.nb)))
all.equal(v2[-1,-1],v3)

Note that this is the covariance between the fixed-effect parameters
and the "theta" parameters, i.e. the lower triangle of the Cholesky
factor ... in the simple case of a scalar random effect, this reduces
to the standard deviation of the random effect. (In the LMM case you
also have to account for the fact that the thetas give the _scaled_
random effects var-cov, i.e. relative to the residual variance.)

  If you're doing lots of NB fitting you might also be interested in the
glmmTMB package (on github ...) -- there, vcov(.,full=TRUE) gives the
desired full variance-covariance matrix.

On 16-11-21 10:43 AM, Levine, Michael wrote:
> 
> While working on one of my projects, I encountered the function
> glmer.nb that is a part of the package lme4 that you maintain.  As
> far as I can see, one can only obtain a variance-covariance matrix of
> estimated fixed effects using this function...Is there any way for me
> to obtain a full variance-covariance matrix that includes covariances
> between fixed effects and estimated variance of the Gaussian random
> effect?
> 
> Thank you very much in advance for your help!
>


From vickmoe7 at gmail.com  Tue Nov 22 17:26:04 2016
From: vickmoe7 at gmail.com (Vickly Mobilim)
Date: Wed, 23 Nov 2016 00:26:04 +0800
Subject: [R-sig-ME] GLMM - Firefly Flash
Message-ID: <CAPnu2Jr6bnPa+P4q=sDD3hUfYwHXKKgm078MHwumzvJZhuKf-Q@mail.gmail.com>

Greetings,

I've read several writing of yours about GLMM and I thought it would be the
best tool to answer my research questions. However, I wasn't sure if I
really need it and my data permit me to use it. That said, I have 78
individuals of firefly divided into four groups (A= 20 indv., B = 20 indv.,
C = 20 indv. and D = 18 indv.). This is due to several limitations that I
can't take more samples of firefly. I will explain the details of the
experiment below.

I'm hoping that you can advise me on this issue, whether you have seen such
cases of low sample size using GLMM or whether GLMM is not suitable for my
study.



I expose the fireflies with several intensity of white light according to
their group (Group A = 0.05lux, B = 0.1lux, C = 0.3lux and D = 0.5lux) then
measure their flash rates and duration before, during and after exposure to
light (repeated measure design). Temperature, humidity and individual
eye-to-body size ratio were also measured. My main aim was to measure the
impact of several light pollution intensity to their flash rates and
duration and taking temperature, humidity and eye-to-body size into account.

I realized that calculating changes in their flash rates and duration are
achievable by subtracting post-experiment result with pre-experiment result
then use unpaired t-test to compare the results. However, my data was not
normal and I used Mann-Whitney U test instead. But this does not take
temperature, humidity and eye-to-body size into account. As I was looking
into the possibility of taking them into account, I found several modelling
technique that is suitable including GLMM but I am not sure if I can employ
them because according to a statistician I am in consult with, the sample
size is too small to be developed into a model that it would invite more
problem in analysis.

-- 
Regards,
Vickly Mobilim

	[[alternative HTML version deleted]]


From jordirosich16 at gmail.com  Tue Nov 22 15:20:19 2016
From: jordirosich16 at gmail.com (Jordi Rosich)
Date: Tue, 22 Nov 2016 15:20:19 +0100
Subject: [R-sig-ME] Mixed Models convergence problems, Jordi Rosich
In-Reply-To: <1479187148.2575.10.camel@unisa.edu.au>
References: <CAJ6PopzBUtMaO5pLh+h0rPryk63fwFdfsHufCObKZ+neOXiYpw@mail.gmail.com>
	<1479187148.2575.10.camel@unisa.edu.au>
Message-ID: <CAJ6PopwVkj5KzX379Gu0mTJtZSDp6yv9NT30GVdj7ZKJsK5MiQ@mail.gmail.com>

Thank you very much for your answer,


We have checked some of the possible solutions you proposed as well as the
?convergence help page:




-Center and scale predictor variables didn?t solve the convergence problems
because our variables are components obtained in a PCA analysis and are
?already centered?.


-Increasing the number of iterations of the optimizer didn?t solve the
convergence problem in all the different models.

-We didn?t check singularity or recompute gradient and Hessian with
Richardson extrapolation as we got unexpected values in some parts of
the process or didn?t fully understand how the process works.

-We tried to run the models with different optimizers as explained in the
?convergence page and got the attached output. It seems that changing the
optimizers didn't





One of the reasons we think our models could continue to fail is because in
our data (see previous mail for details) the levels of the random factor
Territory totally explain if a tree/nest-site/landscape is occupied or
unoccupied. For exemple, in Territory 1 all four trees/nest-site
forests/landscapes are occupied; in Territory 15 all three trees/nest-site
forests/landscapes are unoccupied. This happens with all our territories,
all nests/"no-nests" of a territory are either occupied or unoccupied.
Could this situation be problematic when estimating the random factor
variance and thus, is there a possibility that the convergence problem has
a relation with this fact?


Thank you in advance. Waiting for your answer,



Jordi Rosich

2016-11-15 6:19 GMT+01:00 Phillip Alday <Phillip.Alday at unisa.edu.au>:

> Hi Jordi,
>
> Without really knowing anything about your data (or more generally
> types of data common to your field) ....
>
> - Your model doesn't seem exceptionally complex -- just main effects
> and a single scalar (intercept-only) random effect. Of course, a simple
> model can still be "too" complex if you don't have much data.
>
> - However, it sometimes makes sense to use a more complicated model
> when you have convergence issues on a simple model -- sometimes, you
> really do need covariates to get any type of decent fit. (Based on your
> email, you may have already  experienced this.)
>
> - Categorical variables are particularly 'nasty' when it comes to the
> number of model parameters as a categorical variable with n levels
> requires n-1 parameters in the model! Continuous variables only require
> 1 parameter apiece (correlation parameters in the random effects
> excepted).
>
> - Watch out for multicollinearity -- how strongly do tree height and
> tree width correlate with each other?
>
> - Your particular convergence warning often means that the optimiser
> was still moving along towards convergence / the solution when
> optimisation was stoppe. Sometimes this can be helped by just
> increasing the number of iterations that the optimiser is allowed to
> take, although this will increase computer time.
>
> - Make sure to check out the help page: ?convergence (after loading
> lme4) has many tips and tricks.
>
> Best,
> Phillip
>
> On Mon, 2016-11-14 at 22:33 +0100, Jordi Rosich wrote:
> > Hello,
> >
> >
> >
> > I'm Jordi Rosich, a student currently collaborating with the Biology
> > Conservation Group of the University of Barcelona. I'm writing you
> > because
> > I'm having model convergence troubles with some GLMMs using the
> > function
> > glmer of the package lme4 of R.
> >
> > Our research addresses nest-site selection of the Goshawk, a
> > territorial
> > bird of prey, and specifically we aim to understand which
> > environmental
> > variables are relevant for nest site-selection in this species.
> >
> >
> >
> >
> > *Our approach**:*
> >
> > 1)     We sampled several environmental variables in sites used by
> > this
> > bird species in nest (1) and unoccupied sites (0), and therefore
> > occupation
> > status (0/1) is our dependent variable and the environmental
> > variables our
> > independent variables.
> >
> > 2)     We performed  three analysis at 3 different spatial-scales:
> > nest
> > tree, nest-site forest (being the 18 meters radius circular area
> > around the
> > nest-tree), and landscape around nest-site (500 meters radius
> > circular area
> > around the nest-tree).
> >
> > 3)     We sampled 29 Goshawk nests comprised in 13 breeding pairs
> > territories (each territory may hold several nests) and 30 control
> > non-occupied random trees comprised in 25 "pseudo-territories" (the
> > near
> > trees being included in this "territories"). To avoid
> > pseudoreplication of
> > nests of the same breeding pair (or territory) we have considered the
> > factor "Territory" as our random factor in the mixed models. The
> > model
> > definition is approximately Y = explanatory variables +
> > (1|id  Territory),
> > where Y is the occupation status (0/1) (See an example below).
> >
> > 4)     Our independent variables are both categorical (e.g. tree
> > species;
> > aspect: an angle recoded into 4 categories) and continuous
> > (components
> > resulting of a PCA on several original environmental variables,
> > performed
> > to reduce the number of original variables). For more details for
> > each
> > analysis:
> >
> >
> > -Nest-tree scale: 2 continuous variables (FAC1TreeHeight,
> > FAC2TreeWidth)
> > and 1 four level categorical variable (TreeSpecies).
> >
> >
> >
> > -Nest-site forest scale: 4 continuous variables
> > (FAC1BroadLeavedTrees,
> > FAC2YoungPines, FAC3MaturePines, FAC4SlopeAndShrubs) and 1 four level
> > categorical variable (ForestAspect).
> >
> >
> >
> > -Landscape scale: 3 continuous variables (FAC1PinusVSQuercus,
> > FAC2HumanizedLand, FAC3DistanceToRoads).
> >
> >
> >
> >
> >
> >
> >
> > One example of the script used to model would be:
> >
> >
> >
> > mod1 <-
> > glmer(Occupation~FAC1Treeheight+FAC2TreeWidth+Sp+(1|Territory),
> > family=binomial, data=trees)
> >
> >
> >
> >
> >
> > *The problem: *
> >
> >
> >
> > While creating the candidate models with glmer function to later
> > select the
> > best ones by their AICcs, we've faced some warnings telling that some
> > of
> > the models are failing to converge:
> >
> >
> >
> > In checkConv(attr(opt, "derivs"), opt$par, ctrl =
> > control$checkConv,  :
> >
> >   Model failed to converge with max|grad| = 0.0483015 (tol = 0.001,
> > component 1)
> >
> >
> >
> > It happens specially, although not always, with the models with more
> > parameters and also with those containing categorical variables.
> >
> >
> >
> >
> > *My question: *
> >
> >
> >
> > Given our variables, random factor and data is there any particular
> > reason
> > why our models could fail to converge? If so, is there a possible
> > solution
> > to the convergence problem?
> >
> >
> >
> >
> >
> > Thank you very much in advance. Waiting for your answer,
> >
> >
> > Jordi Rosich
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
-------------- next part --------------
source(system.file("utils", "allFit.R", package="lme4"))
mod1.all <- allFit(mod7)
ss <- summary(mod7.all)

> source(system.file("utils", "allFit.R", package="lme4"))
Loading required package: optimx
Loading required package: dfoptim
Warning messages:
1: package ?optimx? was built under R version 3.1.3 
2: package ?dfoptim? was built under R version 3.1.3 
> 
> mod7.all <- allFit(mod7)
bobyqa : [OK]
Nelder_Mead : [OK]
nlminbw : [OK]
nmkbw : [OK]
optimx.L-BFGS-B : Warning messages:
1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  unable to evaluate scaled gradient
2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 2 negative eigenvalues
3: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.0348916 (tol = 0.001, component 1)
4: In optwrap(optimizer, devfun, start, rho$lower, control = control,  :
  convergence code 52 from optimx
[OK]
nloptwrap.NLOPT_LN_NELDERMEAD : Warning messages:
1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  unable to evaluate scaled gradient
2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 2 negative eigenvalues
[OK]
nloptwrap.NLOPT_LN_BOBYQA : Warning messages:
1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  unable to evaluate scaled gradient
2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
[OK]
Warning messages:
1: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  unable to evaluate scaled gradient
2: In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
> ss <- summary(mod7.all)
> 
> ss
$which.OK
                       bobyqa                   Nelder_Mead 
                         TRUE                          TRUE 
                      nlminbw                         nmkbw 
                         TRUE                          TRUE 
              optimx.L-BFGS-B nloptwrap.NLOPT_LN_NELDERMEAD 
                         TRUE                          TRUE 
    nloptwrap.NLOPT_LN_BOBYQA 
                         TRUE 

$msgs
$msgs$bobyqa
[1] "unable to evaluate scaled gradient"                                       
[2] "Model failed to converge: degenerate  Hessian with 2 negative eigenvalues"

$msgs$Nelder_Mead
[1] "Model failed to converge with max|grad| = 0.0348916 (tol = 0.001, component 1)"

$msgs$nlminbw
NULL

$msgs$nmkbw
NULL

$msgs$`optimx.L-BFGS-B`
[1] "unable to evaluate scaled gradient"                                       
[2] "Model failed to converge: degenerate  Hessian with 2 negative eigenvalues"

$msgs$nloptwrap.NLOPT_LN_NELDERMEAD
[1] "unable to evaluate scaled gradient"                                       
[2] "Model failed to converge: degenerate  Hessian with 1 negative eigenvalues"

$msgs$nloptwrap.NLOPT_LN_BOBYQA
[1] "unable to evaluate scaled gradient"                                       
[2] "Model failed to converge: degenerate  Hessian with 1 negative eigenvalues"


$fixef
                              (Intercept)      FAC1         Sp      FAC2
bobyqa                           16.50173 100.19109 -10.120367 15.785298
Nelder_Mead                      14.29626  92.75704  -9.035921 14.328520
nlminbw                          16.84478 104.24393 -10.379773 16.282572
nmkbw                            16.84713 104.25701 -10.380784 16.283287
optimx.L-BFGS-B                  16.78863 104.46214 -10.365431 16.283422
nloptwrap.NLOPT_LN_NELDERMEAD     6.80864  29.37206  -5.122633  7.822627
nloptwrap.NLOPT_LN_BOBYQA         6.80864  29.37206  -5.122633  7.822627
                                 FAC1:Sp
bobyqa                        -28.000155
Nelder_Mead                   -25.885299
nlminbw                       -29.141723
nmkbw                         -29.145838
optimx.L-BFGS-B               -29.207095
nloptwrap.NLOPT_LN_NELDERMEAD  -8.478911
nloptwrap.NLOPT_LN_BOBYQA      -8.478911

$llik
                       bobyqa                   Nelder_Mead 
                    -14.93282                     -15.25304 
                      nlminbw                         nmkbw 
                    -14.92859                     -14.92859 
              optimx.L-BFGS-B nloptwrap.NLOPT_LN_NELDERMEAD 
                    -14.92882                     -15.49699 
    nloptwrap.NLOPT_LN_BOBYQA 
                    -15.49699 

$sdcor
                              Territori.(Intercept)
bobyqa                                     78.94909
Nelder_Mead                                46.62540
nlminbw                                    84.60254
nmkbw                                      84.60060
optimx.L-BFGS-B                            83.86274
nloptwrap.NLOPT_LN_NELDERMEAD              23.59960
nloptwrap.NLOPT_LN_BOBYQA                  23.59960

$theta
                              Territori.(Intercept)
bobyqa                                     78.94909
Nelder_Mead                                46.62540
nlminbw                                    84.60254
nmkbw                                      84.60060
optimx.L-BFGS-B                            83.86274
nloptwrap.NLOPT_LN_NELDERMEAD              23.59960
nloptwrap.NLOPT_LN_BOBYQA                  23.59960

$times
                              user.self sys.self elapsed user.child sys.child
bobyqa                            41.67     0.00   41.73         NA        NA
Nelder_Mead                        2.86     0.00    2.85         NA        NA
nlminbw                            0.78     0.00    0.78         NA        NA
nmkbw                              1.14     0.00    1.15         NA        NA
optimx.L-BFGS-B                    3.18     0.02    3.30         NA        NA
nloptwrap.NLOPT_LN_NELDERMEAD      0.39     0.00    0.41         NA        NA
nloptwrap.NLOPT_LN_BOBYQA          0.36     0.01    0.41         NA        NA

$feval
                       bobyqa                   Nelder_Mead 
                        30031                          1792 
                      nlminbw                         nmkbw 
                           NA                           696 
              optimx.L-BFGS-B nloptwrap.NLOPT_LN_NELDERMEAD 
                          246                           289 
    nloptwrap.NLOPT_LN_BOBYQA 
                          289 

From vickmoe7 at gmail.com  Tue Nov 22 18:01:11 2016
From: vickmoe7 at gmail.com (Vickly Mobilim)
Date: Wed, 23 Nov 2016 01:01:11 +0800
Subject: [R-sig-ME] GLMM -Firefly Flash
Message-ID: <CAPnu2JrvDF-ZZ4TkQt_nOEjwoe07sfhxf4JgybbP1WrwFy8yBg@mail.gmail.com>

Greetings,

I've read several writing of yours about GLMM and I thought it would be the
best tool to answer my research questions. However, I wasn't sure if I
really need it and my data permit me to use it. That said, I have 78
individuals of firefly divided into four groups (A= 20 indv., B = 20 indv.,
C = 20 indv. and D = 18 indv.). This is due to several limitations that I
can't take more samples of firefly. I will explain the details of the
experiment below.

I'm hoping that you can advise me on this issue, whether you have seen such
cases of low sample size using GLMM or whether GLMM is not suitable for my
study.



I expose the fireflies with several intensity of white light according to
their group (Group A = 0.05lux, B = 0.1lux, C = 0.3lux and D = 0.5lux) then
measure their flash rates and duration before, during and after exposure to
light (repeated measure design). Temperature, humidity and individual
eye-to-body size ratio were also measured. My main aim was to measure the
impact of several light pollution intensity to their flash rates and
duration and taking temperature, humidity and eye-to-body size into account.

I realized that calculating changes in their flash rates and duration are
achievable by subtracting post-experiment result with pre-experiment result
then use unpaired t-test to compare the results. However, my data was not
normal and I used Mann-Whitney U test instead. But this does not take
temperature, humidity and eye-to-body size into account. As I was looking
into the possibility of taking them into account, I found several modelling
technique that is suitable including GLMM but I am not sure if I can employ
them because according to a statistician I am in consult with, the sample
size is too small to be developed into a model that it would invite more
problem in analysis.

-- 
Regards,
Vickly Mobilim

	[[alternative HTML version deleted]]


From profiack at gmail.com  Tue Nov 22 18:44:50 2016
From: profiack at gmail.com (Cleber Iack)
Date: Tue, 22 Nov 2016 17:44:50 +0000
Subject: [R-sig-ME] Generalized Linerar Model vs Logistic regression
Message-ID: <CAM-sWA165EBFSv6k8nSHiu8VnL8NmHihskjLrSwycYPoG_et_g@mail.gmail.com>

Dear,

Good night.

I am Phd student, but I have a model with reply Binaria, and I own 23
predictive variables, among them school (4) and Posto (5)

a) My ICC on the school is showing 0.23, can I use this information to
corroborate the use of Generalized Linerar Model instead of a Logistic
regression?

b) If the letter a) is not true, I can verify through the AIC and BIC?


I appreciate any feedback.

Thank you

Cleber

	[[alternative HTML version deleted]]


From profiack at gmail.com  Tue Nov 22 19:38:06 2016
From: profiack at gmail.com (Cleber Iack)
Date: Tue, 22 Nov 2016 18:38:06 +0000
Subject: [R-sig-ME] Questions on Mixed Model analysis
Message-ID: <CAM-sWA36CUaf9u5T3zs_-cAYu5bxBYfobOmya_uAxyww9wgnDQ@mail.gmail.com>

Dear,

I would like if it was possible that the Lord would help me in a review,
lest it incur in error.

If it were a Simple Logistic regression, I  in school "A" could calculate
the Odds Ratio for example in relation to the "P1", I would exp
(-0.44448621), but since I'm using a generalized linar mixed model with
Reply Binaria, I can analyze the same way this exit?

If I'm wrong, could you give me an example of these data down the analysis
of any school cited in relation to any predictor variable

Thank you

Cleber

> ranef(mt7)
$escola
      (Intercept)          P1                P2              P3
 P4
A     1.6902746  -0.4448621  -0.6658758 -3.4604301  -3.3696828
B   -1.2843136   1.0089079   0.3088816  0.1211393   1.6589110
C  -0.5780668  -0.9977792   0.5655049  2.1674668   0.1728413
D   0.1886338   0.4412192  -0.2184985  1.5010123   1.7992507"

Number of obs: 79811, groups:  escola, 4
Fixed Effects:
    (Intercept)              cem       anoscMedio       anoscMuito
 anoscFinalizado
       -2.44242         -0.22111         -0.53089         -1.80689
-2.45414
           cugm              cri              cam              hpm
    caim
        0.06979         -0.55041          0.20136          0.17523
-0.12234
            cpm             qutm              ism                     P1
               P2
        0.21953         -0.06551          0.07528         -0.64913
-1.50175
     P3                        P4
       -2.19105         -1.88287

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Tue Nov 22 21:24:37 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 22 Nov 2016 15:24:37 -0500
Subject: [R-sig-ME] Generalized Linerar Model vs Logistic regression
In-Reply-To: <CAM-sWA165EBFSv6k8nSHiu8VnL8NmHihskjLrSwycYPoG_et_g@mail.gmail.com>
References: <CAM-sWA165EBFSv6k8nSHiu8VnL8NmHihskjLrSwycYPoG_et_g@mail.gmail.com>
Message-ID: <a7fe583e-a774-b146-d3f7-0f578331774b@gmail.com>



On 16-11-22 12:44 PM, Cleber Iack wrote:
> Dear,
> 
> Good night.
> 
> I am Phd student, but I have a model with reply Binaria, and I own 23
> predictive variables, among them school (4) and Posto (5)


  I'm guessing that what you mean by this is that school is a
categorical predictor with 4 levels and Posto is categorical with 5 levels?

  According to Frank Harrell's (_Regression Modeling Strategies_) rules
of thumb, you need about 20 times as many effective observations as the
number of parameters in the model you're trying to fit; in the case of
binary data, 'effective' observations means the minimum of (number of
zeros, number of ones) in your response, i.e. the number of the
less-common response. So I hope you have a large data set (at least
hundreds and preferably thousands of observations ...)

> 
> a) My ICC on the school is showing 0.23, can I use this information to
> corroborate the use of Generalized Linerar Model instead of a Logistic
> regression?
> 
> b) If the letter a) is not true, I can verify through the AIC and BIC?

  I don't know about ICC. You can in principle use AIC or BIC, or a
likelihood ratio test (LRT), to justify the use of a mixed model. My
personal preference would be to use the mixed model if it makes sense in
the context of your observational/experimental design (e.g. you have a
number of discrete groups in your population that can be thought of as
exchangeable, i.e. changing the identity of the groups wouldn't change
your conclusions), and not to try to use quantitative tests for this
purpose.  You can read more about the use and caveats of AIC, BIC, LRT,
etc. at http://tinyurl.com/glmmFAQ

> 
> 
> I appreciate any feedback.
> 
> Thank you
> 
> Cleber
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From bbolker at gmail.com  Wed Nov 23 01:12:01 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 22 Nov 2016 19:12:01 -0500
Subject: [R-sig-ME] Generalized Linerar Model vs Logistic regression
In-Reply-To: <CAM-sWA0f7hfpCSm5DouHMWXMxzo_Vyfbcyx6dAhTLMNz23WUyw@mail.gmail.com>
References: <CAM-sWA165EBFSv6k8nSHiu8VnL8NmHihskjLrSwycYPoG_et_g@mail.gmail.com>
	<a7fe583e-a774-b146-d3f7-0f578331774b@gmail.com>
	<CAM-sWA0f7hfpCSm5DouHMWXMxzo_Vyfbcyx6dAhTLMNz23WUyw@mail.gmail.com>
Message-ID: <ae561e3e-45c3-8379-e39c-0683e1cf6e91@gmail.com>


  (Please keep r-sig-mixed-models at r-project.org in the Cc: list ...)

On 16-11-22 06:57 PM, Cleber Iack wrote:
> Thank you very much for the return.
> 
> Yes, school is a categorical predictor with 4 levels and Posto is
> categorical with 5 levels
> 
> I have 16000 individuals, being that predictive variables are in media
> with 5 longitudinal observations for individual. I have 76000 lines.
> This base would be enough?  I apologize for not having understood very well.

  That sounds like enough provided that the response is not extremely
rare or extremely common.  With this kind of design you do have to be a
little bit careful with another aspect, which is that the Laplace
approximation may be a bit questionable in this case (low effective
sample size *per cluster*) - it would be a good idea, if you can, to
bump up the value of the nAGQ argument to, say, nAGQ=10 (the fit will be
slower, and this won't work if you have complex random effect structures).
> 
> The intraclass correlation coefficient (ICC) what I mean would be to
> assess how much of the total variance of the population is explained by
> the difference between schools.
> 
> Each one of my schools have a very different identity, but I thought of
> a test in order to be able to demonstrate to an audience in a
> presentation for example.

  I personally wouldn't need any convincing that you should be using a
mixed model in this case.  Quoting the ICC along with its confidence
interval would be useful if the among-individual variability is actually
of practical interest.  Other significance tests are available, as I
suggested in my previous message.

> 
> Once again thank you very much and I thank others comments as you can do.
> 
> 2016-11-22 20:24 GMT+00:00 Ben Bolker <bbolker at gmail.com
> <mailto:bbolker at gmail.com>>:
> 
> 
> 
>     On 16-11-22 12:44 PM, Cleber Iack wrote:
>     > Dear,
>     >
>     > Good night.
>     >
>     > I am Phd student, but I have a model with reply Binaria, and I own 23
>     > predictive variables, among them school (4) and Posto (5)
> 
> 
>       I'm guessing that what you mean by this is that school is a
>     categorical predictor with 4 levels and Posto is categorical with 5
>     levels?
> 
>       According to Frank Harrell's (_Regression Modeling Strategies_) rules
>     of thumb, you need about 20 times as many effective observations as the
>     number of parameters in the model you're trying to fit; in the case of
>     binary data, 'effective' observations means the minimum of (number of
>     zeros, number of ones) in your response, i.e. the number of the
>     less-common response. So I hope you have a large data set (at least
>     hundreds and preferably thousands of observations ...)
> 
>     >
>     > a) My ICC on the school is showing 0.23, can I use this information to
>     > corroborate the use of Generalized Linerar Model instead of a Logistic
>     > regression?
>     >
>     > b) If the letter a) is not true, I can verify through the AIC and BIC?
> 
>       I don't know about ICC. You can in principle use AIC or BIC, or a
>     likelihood ratio test (LRT), to justify the use of a mixed model. My
>     personal preference would be to use the mixed model if it makes sense in
>     the context of your observational/experimental design (e.g. you have a
>     number of discrete groups in your population that can be thought of as
>     exchangeable, i.e. changing the identity of the groups wouldn't change
>     your conclusions), and not to try to use quantitative tests for this
>     purpose.  You can read more about the use and caveats of AIC, BIC, LRT,
>     etc. at http://tinyurl.com/glmmFAQ
> 
>     >
>     >
>     > I appreciate any feedback.
>     >
>     > Thank you
>     >
>     > Cleber
>     >
>     >       [[alternative HTML version deleted]]
>     >
>     > _______________________________________________
>     > R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>     <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
>     >
> 
>     _______________________________________________
>     R-sig-mixed-models at r-project.org
>     <mailto:R-sig-mixed-models at r-project.org> mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>     <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
> 
>


From thierry.onkelinx at inbo.be  Wed Nov 23 10:27:00 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Wed, 23 Nov 2016 10:27:00 +0100
Subject: [R-sig-ME] GLMM -Firefly Flash
In-Reply-To: <CAPnu2JrvDF-ZZ4TkQt_nOEjwoe07sfhxf4JgybbP1WrwFy8yBg@mail.gmail.com>
References: <CAPnu2JrvDF-ZZ4TkQt_nOEjwoe07sfhxf4JgybbP1WrwFy8yBg@mail.gmail.com>
Message-ID: <CAJuCY5xc-UfR4netWX-NiQiMnWC-Qz9NKTY=N-epHfGmf=XKuw@mail.gmail.com>

Dear Vickly,

I assume you have measurements on the individual animals and you can
identify the animal during the different exposures. I think you want a
model like this: flash_rate ~ treatment * exposure + temperature + humidity
+ size_ratio + (1|animal_id) This requires -1 + 4 * 3 + 1 + 1 + 1 + 1 = 15
parameters. You have 78 * 3 = 234 observations. That is 234 / 15 = 15.6
observations per parameter, which reasonable to fit the model.

Best regards,


ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-11-22 18:01 GMT+01:00 Vickly Mobilim <vickmoe7 at gmail.com>:

> Greetings,
>
> I've read several writing of yours about GLMM and I thought it would be the
> best tool to answer my research questions. However, I wasn't sure if I
> really need it and my data permit me to use it. That said, I have 78
> individuals of firefly divided into four groups (A= 20 indv., B = 20 indv.,
> C = 20 indv. and D = 18 indv.). This is due to several limitations that I
> can't take more samples of firefly. I will explain the details of the
> experiment below.
>
> I'm hoping that you can advise me on this issue, whether you have seen such
> cases of low sample size using GLMM or whether GLMM is not suitable for my
> study.
>
>
>
> I expose the fireflies with several intensity of white light according to
> their group (Group A = 0.05lux, B = 0.1lux, C = 0.3lux and D = 0.5lux) then
> measure their flash rates and duration before, during and after exposure to
> light (repeated measure design). Temperature, humidity and individual
> eye-to-body size ratio were also measured. My main aim was to measure the
> impact of several light pollution intensity to their flash rates and
> duration and taking temperature, humidity and eye-to-body size into
> account.
>
> I realized that calculating changes in their flash rates and duration are
> achievable by subtracting post-experiment result with pre-experiment result
> then use unpaired t-test to compare the results. However, my data was not
> normal and I used Mann-Whitney U test instead. But this does not take
> temperature, humidity and eye-to-body size into account. As I was looking
> into the possibility of taking them into account, I found several modelling
> technique that is suitable including GLMM but I am not sure if I can employ
> them because according to a statistician I am in consult with, the sample
> size is too small to be developed into a model that it would invite more
> problem in analysis.
>
> --
> Regards,
> Vickly Mobilim
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Wed Nov 23 11:05:02 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Wed, 23 Nov 2016 11:05:02 +0100
Subject: [R-sig-ME] GLMM -Firefly Flash
In-Reply-To: <CAPnu2JrB-_D-YdO+HbCOQOV43HBiWgbdZNDyAWWhN+rJrMdOuA@mail.gmail.com>
References: <CAPnu2JrvDF-ZZ4TkQt_nOEjwoe07sfhxf4JgybbP1WrwFy8yBg@mail.gmail.com>
	<CAJuCY5xc-UfR4netWX-NiQiMnWC-Qz9NKTY=N-epHfGmf=XKuw@mail.gmail.com>
	<CAPnu2Jrxs9g-Z5B1wEWk2xHuqvTFBDHze=Me2mfJqObw1a1eQA@mail.gmail.com>
	<CAPnu2Jrzs-pV8YiDXYjQaWOGNbeYw65oPHOxdw5cDosQYbOFoA@mail.gmail.com>
	<CAPnu2JrB-_D-YdO+HbCOQOV43HBiWgbdZNDyAWWhN+rJrMdOuA@mail.gmail.com>
Message-ID: <CAJuCY5zKxP8duKeWqoSn299rxdnQa-wDvZeuHa4eE7o-YS_J5w@mail.gmail.com>

Dear Vickly,

Please keep the mailing list in cc.

The idea is that you need a sufficient number of observations per
parameter. 10 to 20 is often used as a rule of thumb. If you have a lower
number, the model is too complex given the data will probably overfit.
Think about a simple linear model (intercept + 1 parameter for slope).
Although you can technically fit this model when you have 2 or 3
observations, the resulting model is not very useful.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-11-23 10:37 GMT+01:00 Vickly Mobilim <vickmoe7 at gmail.com>:

> Hi Thierry,
>
> Thank you for the kind reply! That is very helpful.
>
> May I know more about the calculation? I have never seen it. How do you
> use it to know if it is sufficient to build a model?
>
> On Nov 23, 2016 5:27 PM, "Thierry Onkelinx" <thierry.onkelinx at inbo.be>
> wrote:
>
> Dear Vickly,
>
> I assume you have measurements on the individual animals and you can
> identify the animal during the different exposures. I think you want a
> model like this: flash_rate ~ treatment * exposure + temperature + humidity
> + size_ratio + (1|animal_id) This requires -1 + 4 * 3 + 1 + 1 + 1 + 1 = 15
> parameters. You have 78 * 3 = 234 observations. That is 234 / 15 = 15.6
> observations per parameter, which reasonable to fit the model.
>
> Best regards,
>
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-11-22 18:01 GMT+01:00 Vickly Mobilim <vickmoe7 at gmail.com>:
>
>> Greetings,
>>
>> I've read several writing of yours about GLMM and I thought it would be
>> the
>> best tool to answer my research questions. However, I wasn't sure if I
>> really need it and my data permit me to use it. That said, I have 78
>> individuals of firefly divided into four groups (A= 20 indv., B = 20
>> indv.,
>> C = 20 indv. and D = 18 indv.). This is due to several limitations that I
>> can't take more samples of firefly. I will explain the details of the
>> experiment below.
>>
>> I'm hoping that you can advise me on this issue, whether you have seen
>> such
>> cases of low sample size using GLMM or whether GLMM is not suitable for my
>> study.
>>
>>
>>
>> I expose the fireflies with several intensity of white light according to
>> their group (Group A = 0.05lux, B = 0.1lux, C = 0.3lux and D = 0.5lux)
>> then
>> measure their flash rates and duration before, during and after exposure
>> to
>> light (repeated measure design). Temperature, humidity and individual
>> eye-to-body size ratio were also measured. My main aim was to measure the
>> impact of several light pollution intensity to their flash rates and
>> duration and taking temperature, humidity and eye-to-body size into
>> account.
>>
>> I realized that calculating changes in their flash rates and duration are
>> achievable by subtracting post-experiment result with pre-experiment
>> result
>> then use unpaired t-test to compare the results. However, my data was not
>> normal and I used Mann-Whitney U test instead. But this does not take
>> temperature, humidity and eye-to-body size into account. As I was looking
>> into the possibility of taking them into account, I found several
>> modelling
>> technique that is suitable including GLMM but I am not sure if I can
>> employ
>> them because according to a statistician I am in consult with, the sample
>> size is too small to be developed into a model that it would invite more
>> problem in analysis.
>>
>> --
>> Regards,
>> Vickly Mobilim
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>
>

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Wed Nov 23 15:06:43 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 23 Nov 2016 09:06:43 -0500
Subject: [R-sig-ME] Questions on Mixed Model analysis
In-Reply-To: <CAM-sWA36CUaf9u5T3zs_-cAYu5bxBYfobOmya_uAxyww9wgnDQ@mail.gmail.com>
References: <CAM-sWA36CUaf9u5T3zs_-cAYu5bxBYfobOmya_uAxyww9wgnDQ@mail.gmail.com>
Message-ID: <CABghstTex8WN7FKKafrc+hpFEqHHCX4zEuf532yA3hidJ0bSSw@mail.gmail.com>

On Tue, Nov 22, 2016 at 1:38 PM, Cleber Iack <profiack at gmail.com> wrote:
> Dear,
>
> I would like if it was possible that the Lord would help me in a review,
> lest it incur in error.
>
> If it were a Simple Logistic regression, I  in school "A" could calculate
> the Odds Ratio for example in relation to the "P1", I would exp
> (-0.44448621), but since I'm using a generalized linar mixed model with
> Reply Binaria, I can analyze the same way this exit?
>
> If I'm wrong, could you give me an example of these data down the analysis
> of any school cited in relation to any predictor variable
>
> Thank you
>
> Cleber
>
>> ranef(mt7)
> $escola
>       (Intercept)          P1                P2              P3
>  P4
> A     1.6902746  -0.4448621  -0.6658758 -3.4604301  -3.3696828
> B   -1.2843136   1.0089079   0.3088816  0.1211393   1.6589110
> C  -0.5780668  -0.9977792   0.5655049  2.1674668   0.1728413
> D   0.1886338   0.4412192  -0.2184985  1.5010123   1.7992507"

   The random effects represent deviations from the population-level
value of the parameter.  So the odds ratio represents the deviation
from the population average.  If you want the overall odds ratio
(deviation of school A from even odds in the P1 parameter), try coef()


>
> Number of obs: 79811, groups:  escola, 4
> Fixed Effects:
>     (Intercept)              cem       anoscMedio       anoscMuito
>  anoscFinalizado
>        -2.44242         -0.22111         -0.53089         -1.80689
> -2.45414
>            cugm              cri              cam              hpm
>     caim
>         0.06979         -0.55041          0.20136          0.17523
> -0.12234
>             cpm             qutm              ism                     P1
>                P2
>         0.21953         -0.06551          0.07528         -0.64913
> -1.50175
>      P3                        P4
>        -2.19105         -1.88287
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From marc.jacobs012 at gmail.com  Wed Nov 23 15:58:18 2016
From: marc.jacobs012 at gmail.com (Marc Jacobs)
Date: Wed, 23 Nov 2016 15:58:18 +0100
Subject: [R-sig-ME] Time-varying random effects
Message-ID: <CAKqapw9SPTDxiyXAPrnR--m2Vi+C-L_MOf+-qcm_YQomHJOg2w@mail.gmail.com>

Hi,



By request of Prof. Bolker, i am posting my question here.


I am currently in the process of analyzing a growth model in pigs. Due to
the confidentiality of the data, I cannot add any data which is of course
the preferred course, but I hope to gain some insight here. I apologize in
advance if the description is unclear.



The data shows growth in 300+ pigs over 168 days, measured on 11
time-points. These 168 days can be divided in three separate phases:
farrowing/mom (2 timepoints), nursery (4 timepoints), and growth-finish (5
timepoints).



During each of these phases, the animals are placed in different rooms and
pens (nested in the rooms), which by definition are random factors. Also,
there is a genetic dependency of pigs (litter) nested in moms, which would
be a crossed effect, since the effect takes place across the entire
dataset, separate from the room/pen (pigs are separated from the litter
after the farrowing/mom phase).



As such, from my point of view, the room/pen are now time-varying random
effects. Since I wish to model the entire growth curve, I was wondering if
anybody knows how to incorporate time-varying random effects?



My gut feeling tells me this is quite easy, but my models do not converge.



If you need more information, please let me know.



Marc

	[[alternative HTML version deleted]]


From vickmoe7 at gmail.com  Thu Nov 24 12:42:23 2016
From: vickmoe7 at gmail.com (Vickly Mobilim)
Date: Thu, 24 Nov 2016 19:42:23 +0800
Subject: [R-sig-ME] GLMM -Firefly Flash
In-Reply-To: <CAJuCY5zKxP8duKeWqoSn299rxdnQa-wDvZeuHa4eE7o-YS_J5w@mail.gmail.com>
References: <CAPnu2JrvDF-ZZ4TkQt_nOEjwoe07sfhxf4JgybbP1WrwFy8yBg@mail.gmail.com>
	<CAJuCY5xc-UfR4netWX-NiQiMnWC-Qz9NKTY=N-epHfGmf=XKuw@mail.gmail.com>
	<CAPnu2Jrxs9g-Z5B1wEWk2xHuqvTFBDHze=Me2mfJqObw1a1eQA@mail.gmail.com>
	<CAPnu2Jrzs-pV8YiDXYjQaWOGNbeYw65oPHOxdw5cDosQYbOFoA@mail.gmail.com>
	<CAPnu2JrB-_D-YdO+HbCOQOV43HBiWgbdZNDyAWWhN+rJrMdOuA@mail.gmail.com>
	<CAJuCY5zKxP8duKeWqoSn299rxdnQa-wDvZeuHa4eE7o-YS_J5w@mail.gmail.com>
Message-ID: <CAPnu2JoTFYe+BBOs_Qp=htgkWc03ivjS=PcEW+Eu8sVG2wShgg@mail.gmail.com>

Hello Thierry,

Thank you very much. I will try doing it first. You have been vary helpful.

On Wed, Nov 23, 2016 at 6:05 PM, Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> Dear Vickly,
>
> Please keep the mailing list in cc.
>
> The idea is that you need a sufficient number of observations per
> parameter. 10 to 20 is often used as a rule of thumb. If you have a lower
> number, the model is too complex given the data will probably overfit.
> Think about a simple linear model (intercept + 1 parameter for slope).
> Although you can technically fit this model when you have 2 or 3
> observations, the resulting model is not very useful.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-11-23 10:37 GMT+01:00 Vickly Mobilim <vickmoe7 at gmail.com>:
>
>> Hi Thierry,
>>
>> Thank you for the kind reply! That is very helpful.
>>
>> May I know more about the calculation? I have never seen it. How do you
>> use it to know if it is sufficient to build a model?
>>
>> On Nov 23, 2016 5:27 PM, "Thierry Onkelinx" <thierry.onkelinx at inbo.be>
>> wrote:
>>
>> Dear Vickly,
>>
>> I assume you have measurements on the individual animals and you can
>> identify the animal during the different exposures. I think you want a
>> model like this: flash_rate ~ treatment * exposure + temperature + humidity
>> + size_ratio + (1|animal_id) This requires -1 + 4 * 3 + 1 + 1 + 1 + 1 = 15
>> parameters. You have 78 * 3 = 234 observations. That is 234 / 15 = 15.6
>> observations per parameter, which reasonable to fit the model.
>>
>> Best regards,
>>
>>
>> ir. Thierry Onkelinx
>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>> and Forest
>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>> Kliniekstraat 25
>> 1070 Anderlecht
>> Belgium
>>
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> 2016-11-22 18:01 GMT+01:00 Vickly Mobilim <vickmoe7 at gmail.com>:
>>
>>> Greetings,
>>>
>>> I've read several writing of yours about GLMM and I thought it would be
>>> the
>>> best tool to answer my research questions. However, I wasn't sure if I
>>> really need it and my data permit me to use it. That said, I have 78
>>> individuals of firefly divided into four groups (A= 20 indv., B = 20
>>> indv.,
>>> C = 20 indv. and D = 18 indv.). This is due to several limitations that I
>>> can't take more samples of firefly. I will explain the details of the
>>> experiment below.
>>>
>>> I'm hoping that you can advise me on this issue, whether you have seen
>>> such
>>> cases of low sample size using GLMM or whether GLMM is not suitable for
>>> my
>>> study.
>>>
>>>
>>>
>>> I expose the fireflies with several intensity of white light according to
>>> their group (Group A = 0.05lux, B = 0.1lux, C = 0.3lux and D = 0.5lux)
>>> then
>>> measure their flash rates and duration before, during and after exposure
>>> to
>>> light (repeated measure design). Temperature, humidity and individual
>>> eye-to-body size ratio were also measured. My main aim was to measure the
>>> impact of several light pollution intensity to their flash rates and
>>> duration and taking temperature, humidity and eye-to-body size into
>>> account.
>>>
>>> I realized that calculating changes in their flash rates and duration are
>>> achievable by subtracting post-experiment result with pre-experiment
>>> result
>>> then use unpaired t-test to compare the results. However, my data was not
>>> normal and I used Mann-Whitney U test instead. But this does not take
>>> temperature, humidity and eye-to-body size into account. As I was looking
>>> into the possibility of taking them into account, I found several
>>> modelling
>>> technique that is suitable including GLMM but I am not sure if I can
>>> employ
>>> them because according to a statistician I am in consult with, the sample
>>> size is too small to be developed into a model that it would invite more
>>> problem in analysis.
>>>
>>> --
>>> Regards,
>>> Vickly Mobilim
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>>
>>
>


-- 
Regards,
Vickly Mobilim

	[[alternative HTML version deleted]]


From tim.cole at ucl.ac.uk  Thu Nov 24 14:08:57 2016
From: tim.cole at ucl.ac.uk (Cole, Tim)
Date: Thu, 24 Nov 2016 13:08:57 +0000
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 119, Issue 26
In-Reply-To: <mailman.1.1479985201.29649.r-sig-mixed-models@r-project.org>
References: <mailman.1.1479985201.29649.r-sig-mixed-models@r-project.org>
Message-ID: <D45C955F.15AB9%tim.cole@ucl.ac.uk>

Hi Marc,

Ken Beath described a growth curve model with time-varying covariates in Statist Med 2007; 26:2547-2564, where his example was the effect of breastfeeding on infant weight. His appendix includes the relevant R code.

My CRAN sitar library implements a simplified version of his model without the time-varying element.

Best wishes,
Tim
---
tim.cole at ucl.ac.uk<mailto:tim.cole at ucl.ac.uk> Phone +44(0)20 7905 2666 Fax +44(0)20 7905 2381
Population Policy and Practice Programme
UCL Great Ormond Street Institute of Child Health, London WC1N 1EH, UK

Date: Wed, 23 Nov 2016 15:58:18 +0100
From: Marc Jacobs <marc.jacobs012 at gmail.com<mailto:marc.jacobs012 at gmail.com>>
To: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
Subject: [R-sig-ME] Time-varying random effects

Hi,

By request of Prof. Bolker, i am posting my question here.

I am currently in the process of analyzing a growth model in pigs. Due to
the confidentiality of the data, I cannot add any data which is of course
the preferred course, but I hope to gain some insight here. I apologize in
advance if the description is unclear.

The data shows growth in 300+ pigs over 168 days, measured on 11
time-points. These 168 days can be divided in three separate phases:
farrowing/mom (2 timepoints), nursery (4 timepoints), and growth-finish (5
timepoints).

During each of these phases, the animals are placed in different rooms and
pens (nested in the rooms), which by definition are random factors. Also,
there is a genetic dependency of pigs (litter) nested in moms, which would
be a crossed effect, since the effect takes place across the entire
dataset, separate from the room/pen (pigs are separated from the litter
after the farrowing/mom phase).

As such, from my point of view, the room/pen are now time-varying random
effects. Since I wish to model the entire growth curve, I was wondering if
anybody knows how to incorporate time-varying random effects?

My gut feeling tells me this is quite easy, but my models do not converge.

If you need more information, please let me know.

Marc


	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Thu Nov 24 15:43:13 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Thu, 24 Nov 2016 15:43:13 +0100
Subject: [R-sig-ME] Time-varying random effects
In-Reply-To: <CAKqapw9SPTDxiyXAPrnR--m2Vi+C-L_MOf+-qcm_YQomHJOg2w@mail.gmail.com>
References: <CAKqapw9SPTDxiyXAPrnR--m2Vi+C-L_MOf+-qcm_YQomHJOg2w@mail.gmail.com>
Message-ID: <CAJuCY5yx-Z-s=dQi8mZktVR0tnsXaBJq33hn03vTuKxtZ_xxGg@mail.gmail.com>

Hi Mark,

I have some questions on the design.
- Can you identify the individual pigs in the data?
- How is the grouping of the pigs? Is it constant (e.g. all pigs from the
same litter stay together)? Or does the grouping changes over time?
- Do expect any effect of the pens itself? Or are the pens rather a just
group of pigs.

Best regards,

Thierry

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-11-23 15:58 GMT+01:00 Marc Jacobs <marc.jacobs012 at gmail.com>:

> Hi,
>
>
>
> By request of Prof. Bolker, i am posting my question here.
>
>
> I am currently in the process of analyzing a growth model in pigs. Due to
> the confidentiality of the data, I cannot add any data which is of course
> the preferred course, but I hope to gain some insight here. I apologize in
> advance if the description is unclear.
>
>
>
> The data shows growth in 300+ pigs over 168 days, measured on 11
> time-points. These 168 days can be divided in three separate phases:
> farrowing/mom (2 timepoints), nursery (4 timepoints), and growth-finish (5
> timepoints).
>
>
>
> During each of these phases, the animals are placed in different rooms and
> pens (nested in the rooms), which by definition are random factors. Also,
> there is a genetic dependency of pigs (litter) nested in moms, which would
> be a crossed effect, since the effect takes place across the entire
> dataset, separate from the room/pen (pigs are separated from the litter
> after the farrowing/mom phase).
>
>
>
> As such, from my point of view, the room/pen are now time-varying random
> effects. Since I wish to model the entire growth curve, I was wondering if
> anybody knows how to incorporate time-varying random effects?
>
>
>
> My gut feeling tells me this is quite easy, but my models do not converge.
>
>
>
> If you need more information, please let me know.
>
>
>
> Marc
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Thu Nov 24 16:55:58 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 24 Nov 2016 10:55:58 -0500
Subject: [R-sig-ME] Time-varying random effects
In-Reply-To: <CAJuCY5yx-Z-s=dQi8mZktVR0tnsXaBJq33hn03vTuKxtZ_xxGg@mail.gmail.com>
References: <CAKqapw9SPTDxiyXAPrnR--m2Vi+C-L_MOf+-qcm_YQomHJOg2w@mail.gmail.com>
	<CAJuCY5yx-Z-s=dQi8mZktVR0tnsXaBJq33hn03vTuKxtZ_xxGg@mail.gmail.com>
Message-ID: <b9a78ae5-4cd0-8283-3a35-6091effdf6bf@gmail.com>


  And I'll chime in: how bad is your failure to converge?
  As regular readers of the list know, there are a lot of false
positives. What kind of convergence failures?  Have you checked the
?convergence page ?

On 16-11-24 09:43 AM, Thierry Onkelinx wrote:
> Hi Mark,
> 
> I have some questions on the design.
> - Can you identify the individual pigs in the data?
> - How is the grouping of the pigs? Is it constant (e.g. all pigs from the
> same litter stay together)? Or does the grouping changes over time?
> - Do expect any effect of the pens itself? Or are the pens rather a just
> group of pigs.
> 
> Best regards,
> 
> Thierry
> 
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
> 
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
> 
> 2016-11-23 15:58 GMT+01:00 Marc Jacobs <marc.jacobs012 at gmail.com>:
> 
>> Hi,
>>
>>
>>
>> By request of Prof. Bolker, i am posting my question here.
>>
>>
>> I am currently in the process of analyzing a growth model in pigs. Due to
>> the confidentiality of the data, I cannot add any data which is of course
>> the preferred course, but I hope to gain some insight here. I apologize in
>> advance if the description is unclear.
>>
>>
>>
>> The data shows growth in 300+ pigs over 168 days, measured on 11
>> time-points. These 168 days can be divided in three separate phases:
>> farrowing/mom (2 timepoints), nursery (4 timepoints), and growth-finish (5
>> timepoints).
>>
>>
>>
>> During each of these phases, the animals are placed in different rooms and
>> pens (nested in the rooms), which by definition are random factors. Also,
>> there is a genetic dependency of pigs (litter) nested in moms, which would
>> be a crossed effect, since the effect takes place across the entire
>> dataset, separate from the room/pen (pigs are separated from the litter
>> after the farrowing/mom phase).
>>
>>
>>
>> As such, from my point of view, the room/pen are now time-varying random
>> effects. Since I wish to model the entire growth curve, I was wondering if
>> anybody knows how to incorporate time-varying random effects?
>>
>>
>>
>> My gut feeling tells me this is quite easy, but my models do not converge.
>>
>>
>>
>> If you need more information, please let me know.
>>
>>
>>
>> Marc
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From rebecca.hooper at evobio.eu  Thu Nov 24 15:35:20 2016
From: rebecca.hooper at evobio.eu (Rebecca Hooper)
Date: Thu, 24 Nov 2016 14:35:20 +0000
Subject: [R-sig-ME] MCMCglmm zero inflated poisson model issue
Message-ID: <CABN6br280CQmsdq+nV3Y9rjb0epcUSwu2rEegqci2wGVwuWKvg@mail.gmail.com>

Dear List,

I am building a zero inflated poisson model in MCMCglmm, and am
experiencing some issues with my model, specifically with prior
specification.

My response variable is number of offspring per year, and is 85% zeros. My
predictor variables are categorical: one has four levels (social style) and
one has three levels (time period). The random effect is individual
identity. My code is as follows:

#### main effects model ####

m1 <- MCMCglmm(N.OFFSPR.YR ~ trait-1 +
at.level(trait,1):tp +
at.level(trait,1):social.style,
random=~us(trait):ANIMAL_ID,
rcov=~us(trait):units,
prior=prior_overdisp,
data=rm,
family="zipoisson",
verbose=FALSE,
burnin = 15000,
pl = TRUE,
singular.ok=TRUE,
nitt=40000,
thin = 20)

#### interaction model ####

m2 <- MCMCglmm(N.OFFSPR.YR ~ trait-1 +
at.level(trait,1):tp +
at.level(trait,1):social.style*tp +
at.level(trait,1):social.style,
random=~us(trait):ANIMAL_ID,
rcov=~us(trait):units,
prior=prior_overdisp,
data=rm,
family="zipoisson",
verbose=FALSE,
burnin = 15000,
pl = TRUE,
singular.ok=TRUE,
nitt=40000,
thin = 20)

with prior as follows:

prior_overdisp <- list(R=list(V=diag(c(1,1)),nu=0.002,
fix=2),G=list(list(V=diag(c(1,1e-6)),nu=0.002, fix=2)))
as described by Bolker et al (2012) in their Owl example paper.

Originally, another variable was included in both these models, which was
continuous (year of life). Both models ran with no issues when this
variable was included - the model converged and chains mixed well. I no
longer wish to include the continuous variable though, and without it
neither model runs. I get the error message:

Mixed model equations singular: use a (stronger) prior

I don't know what to make of this, and am loathe to mess around with my
prior specification without fully understanding what it is I am doing. I
would be very grateful for any help and direction in the matter!

Many thanks,
Beki (master's student extremely new to bayesian stats)

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Fri Nov 25 10:23:01 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Fri, 25 Nov 2016 10:23:01 +0100
Subject: [R-sig-ME] Time-varying random effects
In-Reply-To: <CAKqapw9mBZ3v0VGikhAqjDo6_R_b_gefz8wZnYiiTgPdn1u4eg@mail.gmail.com>
References: <CAKqapw9SPTDxiyXAPrnR--m2Vi+C-L_MOf+-qcm_YQomHJOg2w@mail.gmail.com>
	<CAJuCY5yx-Z-s=dQi8mZktVR0tnsXaBJq33hn03vTuKxtZ_xxGg@mail.gmail.com>
	<CAKqapw9mBZ3v0VGikhAqjDo6_R_b_gefz8wZnYiiTgPdn1u4eg@mail.gmail.com>
Message-ID: <CAJuCY5z9DeZTk7smqrmUp4a3=hzmuAyRN9kNt5qnX=rj5eHPHw@mail.gmail.com>

Dear Marc,

I would use al least (1|Sow/Pig) + (1|Blocking). Each pig will have 3
different values for Blocking, one for each stage. At the litter stage this
will coincide with Sow. However Sow rather indicates the overall genetic
effect, where Blocking at the farrowing stage indicates the additional
effect of the circumstances at that point in time.

A (1|Pen) effect is only relevant in case the same pens are used to house
multiple Blocking during the study.

Some example data describing the design would be useful.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-11-25 8:05 GMT+01:00 Marc Jacobs <marc.jacobs012 at gmail.com>:

> Hi all,
>
> thnx tor the replies but in the answers I have not found what I was
> looking for.
>
> The pigs are separated from their litter in the nursery phase being placed
> in pens based on a blocking factor (Bodyweight). This happens again in the
> growth-finish fase. Thus yes, they are moved around at least two times, all
> of them.
>
> Hence, although the genetic similarity remains across the entire study
> (pigs nested in sows), there are crossed effects with blocks, rooms, and
> pen, because it changes. Since pigs are social animals, the pen effect
> should matter and hence should be taken into account. The Blocking effect
> speaks for itself I think.
>
> Normally, this data set would be analyzed three times - once for the
> farrowing phase, once for the nursery phase, and once for the growth finish
> fase. This way, you have no time-varying RANDOM effects, but I want to
> model the entire growth curve, whilst taking into account random factors
> that change over time.
>
> Thank you,
>
> Marc
>
> 2016-11-24 15:43 GMT+01:00 Thierry Onkelinx <thierry.onkelinx at inbo.be>:
>
>> Hi Mark,
>>
>> I have some questions on the design.
>> - Can you identify the individual pigs in the data?
>> - How is the grouping of the pigs? Is it constant (e.g. all pigs from the
>> same litter stay together)? Or does the grouping changes over time?
>> - Do expect any effect of the pens itself? Or are the pens rather a just
>> group of pigs.
>>
>> Best regards,
>>
>> Thierry
>>
>> ir. Thierry Onkelinx
>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>> and Forest
>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>> Kliniekstraat 25
>> 1070 Anderlecht
>> Belgium
>>
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> 2016-11-23 15:58 GMT+01:00 Marc Jacobs <marc.jacobs012 at gmail.com>:
>>
>>> Hi,
>>>
>>>
>>>
>>> By request of Prof. Bolker, i am posting my question here.
>>>
>>>
>>> I am currently in the process of analyzing a growth model in pigs. Due to
>>> the confidentiality of the data, I cannot add any data which is of course
>>> the preferred course, but I hope to gain some insight here. I apologize
>>> in
>>> advance if the description is unclear.
>>>
>>>
>>>
>>> The data shows growth in 300+ pigs over 168 days, measured on 11
>>> time-points. These 168 days can be divided in three separate phases:
>>> farrowing/mom (2 timepoints), nursery (4 timepoints), and growth-finish
>>> (5
>>> timepoints).
>>>
>>>
>>>
>>> During each of these phases, the animals are placed in different rooms
>>> and
>>> pens (nested in the rooms), which by definition are random factors. Also,
>>> there is a genetic dependency of pigs (litter) nested in moms, which
>>> would
>>> be a crossed effect, since the effect takes place across the entire
>>> dataset, separate from the room/pen (pigs are separated from the litter
>>> after the farrowing/mom phase).
>>>
>>>
>>>
>>> As such, from my point of view, the room/pen are now time-varying random
>>> effects. Since I wish to model the entire growth curve, I was wondering
>>> if
>>> anybody knows how to incorporate time-varying random effects?
>>>
>>>
>>>
>>> My gut feeling tells me this is quite easy, but my models do not
>>> converge.
>>>
>>>
>>>
>>> If you need more information, please let me know.
>>>
>>>
>>>
>>> Marc
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From gustaf.granath at gmail.com  Fri Nov 25 14:16:30 2016
From: gustaf.granath at gmail.com (Gustaf Granath)
Date: Fri, 25 Nov 2016 14:16:30 +0100
Subject: [R-sig-ME] zero-inflated models in MCMCglmm
Message-ID: <d4a6fd3a-151a-32e0-1fea-12ff7fe48466@gmail.com>

Hi,
A few questions regarding zero-inflated models using MCMCglmm.

1. Odd contrasts are created when using, e.g. :
y ~ trait-1 + at.level(trait, 1):(X1+X2), family = "zipoisson" #X1 and 
X2 are factors with 2 levels
COEF TABLE
traity
traitzi_y
at.level(trait, 1):X1lev1
at.level(trait, 1):X1lev2
at.level(trait, 1):X2lev2

It doesnt look like this in the course notes (what I can see). Is 
at.level(trait, 1):X1lev1 the reference level for everything below? I 
also get very high (>1000) estimates for traity, at.level(trait, 
1):X1lev1 and at.level(trait, 1):X1lev2.

2. I made four models
a) y ~ X1*X2, family = "poisson"
Large overdispersion (units ~ 5) but everything looks fine (traceplots, 
random effects (including units) ). Predictive checks show that the 
models predict too few zeros though. And alarming is that predictions 
are really bad, e.g. a treatment mean of 40 is predicted to have 300 counts.

b) y ~ trait-1 + at.level(trait, 1):(X1+X2), family = "zipoisson"
Model looks OK, but again, predictions are way too high for the higher 
tretament means.

c:d) y ~ trait-1 + at.level(trait, 1):(X1+X2), family = "zapoisson" or 
"hupoisson"
Predicted values are much better. But is it the same way get predictions 
here? Possible to get predictions from the separate models 
(poisson/binomial) in the hurdle case? (code available?)
#get e.g. treatment predictions?
predict.MCMCglmm(model, marginal = ~ random_factor, type="response", 
posterior = "mean")

The "zapoisson" model performs best but I dont understand why the 
"zipoisson" is so bad. Any typical things to look for when it looks like 
this?
Interpreting "zapoisson" isnt easy, any good literature/tutorials on 
this model?


Cheers

Gustaf


-- 

Gustaf Granath
Post doc
Swedish University of Agricultural Sciences


From marc.jacobs012 at gmail.com  Fri Nov 25 08:05:12 2016
From: marc.jacobs012 at gmail.com (Marc Jacobs)
Date: Fri, 25 Nov 2016 08:05:12 +0100
Subject: [R-sig-ME] Time-varying random effects
In-Reply-To: <CAJuCY5yx-Z-s=dQi8mZktVR0tnsXaBJq33hn03vTuKxtZ_xxGg@mail.gmail.com>
References: <CAKqapw9SPTDxiyXAPrnR--m2Vi+C-L_MOf+-qcm_YQomHJOg2w@mail.gmail.com>
	<CAJuCY5yx-Z-s=dQi8mZktVR0tnsXaBJq33hn03vTuKxtZ_xxGg@mail.gmail.com>
Message-ID: <CAKqapw9mBZ3v0VGikhAqjDo6_R_b_gefz8wZnYiiTgPdn1u4eg@mail.gmail.com>

Hi all,

thnx tor the replies but in the answers I have not found what I was looking
for.

The pigs are separated from their litter in the nursery phase being placed
in pens based on a blocking factor (Bodyweight). This happens again in the
growth-finish fase. Thus yes, they are moved around at least two times, all
of them.

Hence, although the genetic similarity remains across the entire study
(pigs nested in sows), there are crossed effects with blocks, rooms, and
pen, because it changes. Since pigs are social animals, the pen effect
should matter and hence should be taken into account. The Blocking effect
speaks for itself I think.

Normally, this data set would be analyzed three times - once for the
farrowing phase, once for the nursery phase, and once for the growth finish
fase. This way, you have no time-varying RANDOM effects, but I want to
model the entire growth curve, whilst taking into account random factors
that change over time.

Thank you,

Marc

2016-11-24 15:43 GMT+01:00 Thierry Onkelinx <thierry.onkelinx at inbo.be>:

> Hi Mark,
>
> I have some questions on the design.
> - Can you identify the individual pigs in the data?
> - How is the grouping of the pigs? Is it constant (e.g. all pigs from the
> same litter stay together)? Or does the grouping changes over time?
> - Do expect any effect of the pens itself? Or are the pens rather a just
> group of pigs.
>
> Best regards,
>
> Thierry
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-11-23 15:58 GMT+01:00 Marc Jacobs <marc.jacobs012 at gmail.com>:
>
>> Hi,
>>
>>
>>
>> By request of Prof. Bolker, i am posting my question here.
>>
>>
>> I am currently in the process of analyzing a growth model in pigs. Due to
>> the confidentiality of the data, I cannot add any data which is of course
>> the preferred course, but I hope to gain some insight here. I apologize in
>> advance if the description is unclear.
>>
>>
>>
>> The data shows growth in 300+ pigs over 168 days, measured on 11
>> time-points. These 168 days can be divided in three separate phases:
>> farrowing/mom (2 timepoints), nursery (4 timepoints), and growth-finish (5
>> timepoints).
>>
>>
>>
>> During each of these phases, the animals are placed in different rooms and
>> pens (nested in the rooms), which by definition are random factors. Also,
>> there is a genetic dependency of pigs (litter) nested in moms, which would
>> be a crossed effect, since the effect takes place across the entire
>> dataset, separate from the room/pen (pigs are separated from the litter
>> after the farrowing/mom phase).
>>
>>
>>
>> As such, from my point of view, the room/pen are now time-varying random
>> effects. Since I wish to model the entire growth curve, I was wondering if
>> anybody knows how to incorporate time-varying random effects?
>>
>>
>>
>> My gut feeling tells me this is quite easy, but my models do not converge.
>>
>>
>>
>> If you need more information, please let me know.
>>
>>
>>
>> Marc
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>

	[[alternative HTML version deleted]]


From j.hadfield at ed.ac.uk  Sat Nov 26 08:28:13 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Sat, 26 Nov 2016 07:28:13 +0000
Subject: [R-sig-ME] MCMCglmm zero inflated poisson model issue
In-Reply-To: <CABN6br280CQmsdq+nV3Y9rjb0epcUSwu2rEegqci2wGVwuWKvg@mail.gmail.com>
References: <CABN6br280CQmsdq+nV3Y9rjb0epcUSwu2rEegqci2wGVwuWKvg@mail.gmail.com>
Message-ID: <a4041124-fcb0-bc85-6f58-b77845e78460@ed.ac.uk>

Hi,

The two reasons are most likely:

a) you are trying to estimate the residual covariance, for which there 
is no information in the data, you are trying to estimate the 
between-individual covariance for which there is information but given 
you have set the between-individual variance in zero-inflation to zero 
you don't want to estimate.  Replacing us() with idh() resolves these 
issues.

b) you have the argument singular.ok=TRUE. This will retain 
non-identifiable contrasts, but you certainly want to drop them so just 
use the default.

Cheers,

Jarrod



On 24/11/2016 14:35, Rebecca Hooper wrote:
> Dear List,
>
> I am building a zero inflated poisson model in MCMCglmm, and am
> experiencing some issues with my model, specifically with prior
> specification.
>
> My response variable is number of offspring per year, and is 85% zeros. My
> predictor variables are categorical: one has four levels (social style) and
> one has three levels (time period). The random effect is individual
> identity. My code is as follows:
>
> #### main effects model ####
>
> m1 <- MCMCglmm(N.OFFSPR.YR ~ trait-1 +
> at.level(trait,1):tp +
> at.level(trait,1):social.style,
> random=~us(trait):ANIMAL_ID,
> rcov=~us(trait):units,
> prior=prior_overdisp,
> data=rm,
> family="zipoisson",
> verbose=FALSE,
> burnin = 15000,
> pl = TRUE,
> singular.ok=TRUE,
> nitt=40000,
> thin = 20)
>
> #### interaction model ####
>
> m2 <- MCMCglmm(N.OFFSPR.YR ~ trait-1 +
> at.level(trait,1):tp +
> at.level(trait,1):social.style*tp +
> at.level(trait,1):social.style,
> random=~us(trait):ANIMAL_ID,
> rcov=~us(trait):units,
> prior=prior_overdisp,
> data=rm,
> family="zipoisson",
> verbose=FALSE,
> burnin = 15000,
> pl = TRUE,
> singular.ok=TRUE,
> nitt=40000,
> thin = 20)
>
> with prior as follows:
>
> prior_overdisp <- list(R=list(V=diag(c(1,1)),nu=0.002,
> fix=2),G=list(list(V=diag(c(1,1e-6)),nu=0.002, fix=2)))
> as described by Bolker et al (2012) in their Owl example paper.
>
> Originally, another variable was included in both these models, which was
> continuous (year of life). Both models ran with no issues when this
> variable was included - the model converged and chains mixed well. I no
> longer wish to include the continuous variable though, and without it
> neither model runs. I get the error message:
>
> Mixed model equations singular: use a (stronger) prior
>
> I don't know what to make of this, and am loathe to mess around with my
> prior specification without fully understanding what it is I am doing. I
> would be very grateful for any help and direction in the matter!
>
> Many thanks,
> Beki (master's student extremely new to bayesian stats)
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From j.hadfield at ed.ac.uk  Sat Nov 26 08:59:39 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Sat, 26 Nov 2016 07:59:39 +0000
Subject: [R-sig-ME] zero-inflated models in MCMCglmm
In-Reply-To: <d4a6fd3a-151a-32e0-1fea-12ff7fe48466@gmail.com>
References: <d4a6fd3a-151a-32e0-1fea-12ff7fe48466@gmail.com>
Message-ID: <a5c89be4-f711-7b25-90de-b38e15002f4d@ed.ac.uk>

Hi,

If X1 was has two levels then it is odd that you have an intercept AND 2 
X1 contrasts. Have you used singular.ok=TRUE? If not then are you sure 
there is not a third (rare) level to X1? Try unique(your_data$X1): 
trailing whitespace from excel spreadsheets is a common cause of such a 
problem.

Regarding the predictions, the issue may be with the above. However, 
note that there was a bug with posterior="mean" in the call to 
predict.MCMCglmm (the first posterior iteration of the fixed effects was 
used rather than there mean). This bug is fixed in the current version, 
and there was no bug in the default posterior="all".

Cheers,

Jarrod



On 25/11/2016 13:16, Gustaf Granath wrote:
> Hi,
> A few questions regarding zero-inflated models using MCMCglmm.
>
> 1. Odd contrasts are created when using, e.g. :
> y ~ trait-1 + at.level(trait, 1):(X1+X2), family = "zipoisson" #X1 and 
> X2 are factors with 2 levels
> COEF TABLE
> traity
> traitzi_y
> at.level(trait, 1):X1lev1
> at.level(trait, 1):X1lev2
> at.level(trait, 1):X2lev2
>
> It doesnt look like this in the course notes (what I can see). Is 
> at.level(trait, 1):X1lev1 the reference level for everything below? I 
> also get very high (>1000) estimates for traity, at.level(trait, 
> 1):X1lev1 and at.level(trait, 1):X1lev2.
>
> 2. I made four models
> a) y ~ X1*X2, family = "poisson"
> Large overdispersion (units ~ 5) but everything looks fine 
> (traceplots, random effects (including units) ). Predictive checks 
> show that the models predict too few zeros though. And alarming is 
> that predictions are really bad, e.g. a treatment mean of 40 is 
> predicted to have 300 counts.
>
> b) y ~ trait-1 + at.level(trait, 1):(X1+X2), family = "zipoisson"
> Model looks OK, but again, predictions are way too high for the higher 
> tretament means.
>
> c:d) y ~ trait-1 + at.level(trait, 1):(X1+X2), family = "zapoisson" or 
> "hupoisson"
> Predicted values are much better. But is it the same way get 
> predictions here? Possible to get predictions from the separate models 
> (poisson/binomial) in the hurdle case? (code available?)
> #get e.g. treatment predictions?
> predict.MCMCglmm(model, marginal = ~ random_factor, type="response", 
> posterior = "mean")
>
> The "zapoisson" model performs best but I dont understand why the 
> "zipoisson" is so bad. Any typical things to look for when it looks 
> like this?
> Interpreting "zapoisson" isnt easy, any good literature/tutorials on 
> this model?
>
>
> Cheers
>
> Gustaf
>
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From T.Houslay at exeter.ac.uk  Sat Nov 26 11:02:53 2016
From: T.Houslay at exeter.ac.uk (Houslay, Tom)
Date: Sat, 26 Nov 2016 10:02:53 +0000
Subject: [R-sig-ME] zero-inflated models in MCMCglmm
In-Reply-To: <mailman.1261.1480147196.3878.r-sig-mixed-models@r-project.org>
References: <mailman.1261.1480147196.3878.r-sig-mixed-models@r-project.org>
Message-ID: <DB6PR0301MB2311BAD1A7B1AFA36677681CD2880@DB6PR0301MB2311.eurprd03.prod.outlook.com>

Hi Gustaf, just on your question of interpreting zapoisson in MCMCglmm - my recent paper uses zero-altered poisson models (for calling effort in male crickets), which might (hopefully?) help with interpretation:

http://onlinelibrary.wiley.com/doi/10.1111/1365-2435.12766/full


I also found David Atkins' paper (and associated tutorial) on zero-altered count models helpful - paper and code at his website here:

https://depts.washington.edu/cshrb/statistics-resources-tutorials-tutorial-on-count-regression/


Cheers, and good luck!


Tom



________________________________


Message: 1
Date: Fri, 25 Nov 2016 14:16:30 +0100
From: Gustaf Granath <gustaf.granath at gmail.com>
To: r-sig-mixed-models at r-project.org
Subject: [R-sig-ME] zero-inflated models in MCMCglmm
Message-ID: <d4a6fd3a-151a-32e0-1fea-12ff7fe48466 at gmail.com>
Content-Type: text/plain; charset=utf-8; format=flowed

Hi,
A few questions regarding zero-inflated models using MCMCglmm.

1. Odd contrasts are created when using, e.g. :
y ~ trait-1 + at.level(trait, 1):(X1+X2), family = "zipoisson" #X1 and
X2 are factors with 2 levels
COEF TABLE
traity
traitzi_y
at.level(trait, 1):X1lev1
at.level(trait, 1):X1lev2
at.level(trait, 1):X2lev2

It doesnt look like this in the course notes (what I can see). Is
at.level(trait, 1):X1lev1 the reference level for everything below? I
also get very high (>1000) estimates for traity, at.level(trait,
1):X1lev1 and at.level(trait, 1):X1lev2.

2. I made four models
a) y ~ X1*X2, family = "poisson"
Large overdispersion (units ~ 5) but everything looks fine (traceplots,
random effects (including units) ). Predictive checks show that the
models predict too few zeros though. And alarming is that predictions
are really bad, e.g. a treatment mean of 40 is predicted to have 300 counts.

b) y ~ trait-1 + at.level(trait, 1):(X1+X2), family = "zipoisson"
Model looks OK, but again, predictions are way too high for the higher
tretament means.

c:d) y ~ trait-1 + at.level(trait, 1):(X1+X2), family = "zapoisson" or
"hupoisson"
Predicted values are much better. But is it the same way get predictions
here? Possible to get predictions from the separate models
(poisson/binomial) in the hurdle case? (code available?)
#get e.g. treatment predictions?
predict.MCMCglmm(model, marginal = ~ random_factor, type="response",
posterior = "mean")

The "zapoisson" model performs best but I dont understand why the
"zipoisson" is so bad. Any typical things to look for when it looks like
this?
Interpreting "zapoisson" isnt easy, any good literature/tutorials on
this model?


Cheers

Gustaf


--

Gustaf Granath
Post doc
Swedish University of Agricultural Sciences




	[[alternative HTML version deleted]]


From gustaf.granath at gmail.com  Mon Nov 28 12:11:40 2016
From: gustaf.granath at gmail.com (Gustaf Granath)
Date: Mon, 28 Nov 2016 12:11:40 +0100
Subject: [R-sig-ME] zero-inflated models in MCMCglmm
In-Reply-To: <a5c89be4-f711-7b25-90de-b38e15002f4d@ed.ac.uk>
References: <d4a6fd3a-151a-32e0-1fea-12ff7fe48466@gmail.com>
	<a5c89be4-f711-7b25-90de-b38e15002f4d@ed.ac.uk>
Message-ID: <f8ca8f5f-a6c1-7e35-d94e-25b611fa56d2@gmail.com>

Jarrod,

singular.ok=TRUE was the problem. I did some copy-paste and didnt clean 
up the code. However, Im still confused about the coefs. Say that X1 and 
X2 each have two levels: no/yes and empty/full respectively. For a 
"zipoisson" I get the coef table:

traity
traitzi_y
at.level(trait, 1):X1no
at.level(trait, 1):X2full
at.level(trait, 1):X1yes:X2full

But when running "zapiosson" I get:

traity
traitza_y
at.level(trait, 1):X1yes
at.level(trait, 1):X2full
at.level(trait, 1):X1yes:X2full
+ traitza_...

The "zapiosson" output makes sense to me but the "zipoisson" looks odd 
to me.

Predictions: ok, estimates actually changed after I updated the package. 
I didnt make a huge difference though and it doesnt clear things up 
completely. The strange thing is that conditional predictions captures 
the data really well - both for zi, za and hu. However, the estimated 
marginal predictions are sensible for hu, and za (ie conditional and 
marginal predictions are quite similar), while for zi they are way too 
large (getting values much higher than you find in the observed data). 
Yes, marginal predictions should be larger by definition, and may be 
quite much higher if you have some sites with very high means that pulls 
the expected "random site" prediction above the "average site" estimate. 
However, this crazy large "blow up" effect that I see (predictions 
outside data range), I guess can happen when random effects are large. 
But what are actually "good" predictions here? If all classic model 
diagnostics looks good and conditional predictions are close to raw 
(pooled) treatments means but marginal predictions are way too high, 
what is then "wrong" with the model? Where do you start looking to 
identify whats going on? Rarely are predicted estimates reported in 
papers (cond nor marginal) and Im trying to get my head around how they 
can be better used in model checking.

Tom> Thanks for your input. I will look closer at your paper.

Gustaf


On 2016-11-26 08:59, Jarrod Hadfield wrote:
> Hi,
>
> If X1 was has two levels then it is odd that you have an intercept AND 
> 2 X1 contrasts. Have you used singular.ok=TRUE? If not then are you 
> sure there is not a third (rare) level to X1? Try 
> unique(your_data$X1): trailing whitespace from excel spreadsheets is a 
> common cause of such a problem.
>
> Regarding the predictions, the issue may be with the above. However, 
> note that there was a bug with posterior="mean" in the call to 
> predict.MCMCglmm (the first posterior iteration of the fixed effects 
> was used rather than there mean). This bug is fixed in the current 
> version, and there was no bug in the default posterior="all".
>
> Cheers,
>
> Jarrod
>
>
>
> On 25/11/2016 13:16, Gustaf Granath wrote:
>> Hi,
>> A few questions regarding zero-inflated models using MCMCglmm.
>>
>> 1. Odd contrasts are created when using, e.g. :
>> y ~ trait-1 + at.level(trait, 1):(X1+X2), family = "zipoisson" #X1 
>> and X2 are factors with 2 levels
>> COEF TABLE
>> traity
>> traitzi_y
>> at.level(trait, 1):X1lev1
>> at.level(trait, 1):X1lev2
>> at.level(trait, 1):X2lev2
>>
>> It doesnt look like this in the course notes (what I can see). Is 
>> at.level(trait, 1):X1lev1 the reference level for everything below? I 
>> also get very high (>1000) estimates for traity, at.level(trait, 
>> 1):X1lev1 and at.level(trait, 1):X1lev2.
>>
>> 2. I made four models
>> a) y ~ X1*X2, family = "poisson"
>> Large overdispersion (units ~ 5) but everything looks fine 
>> (traceplots, random effects (including units) ). Predictive checks 
>> show that the models predict too few zeros though. And alarming is 
>> that predictions are really bad, e.g. a treatment mean of 40 is 
>> predicted to have 300 counts.
>>
>> b) y ~ trait-1 + at.level(trait, 1):(X1+X2), family = "zipoisson"
>> Model looks OK, but again, predictions are way too high for the 
>> higher tretament means.
>>
>> c:d) y ~ trait-1 + at.level(trait, 1):(X1+X2), family = "zapoisson" 
>> or "hupoisson"
>> Predicted values are much better. But is it the same way get 
>> predictions here? Possible to get predictions from the separate 
>> models (poisson/binomial) in the hurdle case? (code available?)
>> #get e.g. treatment predictions?
>> predict.MCMCglmm(model, marginal = ~ random_factor, type="response", 
>> posterior = "mean")
>>
>> The "zapoisson" model performs best but I dont understand why the 
>> "zipoisson" is so bad. Any typical things to look for when it looks 
>> like this?
>> Interpreting "zapoisson" isnt easy, any good literature/tutorials on 
>> this model?
>>
>>
>> Cheers
>>
>> Gustaf
>>
>>
>
>

-- 
Gustaf Granath
Post doc
Swedish University of Agricultural Sciences
Department of Ecology


From jari.miina at luke.fi  Wed Nov 30 15:25:54 2016
From: jari.miina at luke.fi (Miina Jari (Luke))
Date: Wed, 30 Nov 2016 14:25:54 +0000
Subject: [R-sig-ME] predict() with cumulative link mixed models fitted with
	clmm2
Message-ID: <61ad3eabf727446bac3277b8a675660a@C119S212VM042.msvyvi.vaha.local>

Dear list,

I would like to predict() in the ordinal package using a model fitted with clmm2. The aim is to calculate the classification table of the ordered logistic regression model.

Using the wine data and clm the classes can be predicted and the classification efficiency can be calculated.
> m1 <- clm(rating ~ temp + contact, data = wine)
> m1
formula: rating ~ temp + contact
data:    wine

link  threshold nobs logLik AIC    niter max.grad cond.H
 logit flexible  72   -86.49 184.98 6(0)  4.01e-12 2.7e+01

Coefficients:
  tempwarm contactyes
     2.503      1.528

Threshold coefficients:
   1|2    2|3    3|4    4|5
-1.344  1.251  3.467  5.006
> predict(m1, newdata=wine, type="class")
$fit
[1] 2 2 3 3 3 3 4 4 2 2 3 3 3 3 4 4 2 2 3 3 3 3 4 4 2 2 3 3 3 3 4 4 2 2 3
[36] 3 3 3 4 4 2 2 3 3 3 3 4 4 2 2 3 3 3 3 4 4 2 2 3 3 3 3 4 4 2 2 3 3 3 3
[71] 4 4
Levels: 1 2 3 4 5

But when a model is fitted using clmm2 so that there are a nominal predictor (temp) and random effect (judge), predict() doesn't give classes but probabilities. The probabilities are predicted for the classes of rating indicated in the data, aren't they?
> m4 <- clmm2(rating ~ contact, random = judge, nominal = ~ temp, data = wine, Hess = TRUE)
> m4
Cumulative Link Mixed Model fitted with the Laplace approximation

Call:
clmm2(location = rating ~ contact, nominal = ~temp, random = judge,
    data = wine, Hess = TRUE)

Random effects:
           Var Std.Dev
judge 1.197886 1.09448

Location coefficients:
contactyes
  1.784829

No Scale coefficients

Threshold coefficients:
                   1|2       2|3       3|4       4|5
(Intercept)  -1.530285  1.355902  4.450757  21.96606
tempwarm    -26.859452 -2.683005 -3.352416 -19.09974

log-likelihood: -80.39439
AIC: 180.7888
> head(predict(m4, newdata=wine, type="class"))
[1] 0.61714084 0.19337206 0.54060344 0.06501408 0.19620705 0.19620705

Note that probabilities are predicted for an average judge by including the data used to fit the model in the newdata argument of predict.

My questions:
(1) How do I predict() the probabilities for all the classes (1-5) of rating to determine the classification efficiency?
(2) It seems that the sign of the predictor will change when it appears as a nominal predictor (see coefficients for tempwarm in m1 and m4). However, according to the equations (1) and (3) in the tutorial on fitting CLMs with ordinal (https://cran.r-project.org/web/packages/ordinal/vignettes/clm_tutorial.pdf), the sign should not change. To me, the sign of the coefficients of nominal predictors and threshold parameters should be the same, or have I missed something?

Regards,
Jari Miina



	[[alternative HTML version deleted]]


From sorek.hamer at gmail.com  Wed Nov 30 07:51:17 2016
From: sorek.hamer at gmail.com (Meytar Sorek-Hamer)
Date: Tue, 29 Nov 2016 22:51:17 -0800
Subject: [R-sig-ME] How does lmer treat a date.format (as.Date) variable?
Message-ID: <CAB+XKfm7jPWcn90Qpzy7z+Tjw4UdWMpcvr-XtNfmwGtcTiQipQ@mail.gmail.com>

Dear R project team,
I have been working for a while with lmer (lme4) and I have a question
please. I don't fully understand yet how does lmer treat a random variable
in a date format (as.Date).
This is the model syntax:

Y~X+(1+X|date)

I compared model results based on the same dataset just each time (model) I
changed the date variable format (i.e. as.factor, as.numeric, as.Date) and
received different results, while the best results were obtained when the
date was in a date.format.

I would highly appreciate your clarification for the differences between
the three formats in lmer.

Thank you very much in advance


*Meytar Sorek-Hamer*, NPP Research Fellow (USRA)
Earth System Science, Atmospherics
NASA Ames Research Center
Building 245, Room: 280L
Moffett Field, CA 94035  USA
ph: 650-604-0153  cell: 669-264-8000

	[[alternative HTML version deleted]]


From j.hadfield at ed.ac.uk  Wed Nov 30 21:21:29 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Wed, 30 Nov 2016 20:21:29 +0000
Subject: [R-sig-ME] zero-inflated models in MCMCglmm
In-Reply-To: <f8ca8f5f-a6c1-7e35-d94e-25b611fa56d2@gmail.com>
References: <d4a6fd3a-151a-32e0-1fea-12ff7fe48466@gmail.com>
	<a5c89be4-f711-7b25-90de-b38e15002f4d@ed.ac.uk>
	<f8ca8f5f-a6c1-7e35-d94e-25b611fa56d2@gmail.com>
Message-ID: <49bbc5f5-4576-079c-2108-1f2ce2ba31c2@ed.ac.uk>

Hi Gustaf,

Is it possible to post your data/code so I can have a look? I can't 
think of any reason why the base-line level of X1 would change for the 
main effect but be the same for the interaction.

Cheers,

Jarrod


On 28/11/2016 11:11, Gustaf Granath wrote:
> Jarrod,
>
> singular.ok=TRUE was the problem. I did some copy-paste and didnt 
> clean up the code. However, Im still confused about the coefs. Say 
> that X1 and X2 each have two levels: no/yes and empty/full 
> respectively. For a "zipoisson" I get the coef table:
>
> traity
> traitzi_y
> at.level(trait, 1):X1no
> at.level(trait, 1):X2full
> at.level(trait, 1):X1yes:X2full
>
> But when running "zapiosson" I get:
>
> traity
> traitza_y
> at.level(trait, 1):X1yes
> at.level(trait, 1):X2full
> at.level(trait, 1):X1yes:X2full
> + traitza_...
>
> The "zapiosson" output makes sense to me but the "zipoisson" looks odd 
> to me.
>
> Predictions: ok, estimates actually changed after I updated the 
> package. I didnt make a huge difference though and it doesnt clear 
> things up completely. The strange thing is that conditional 
> predictions captures the data really well - both for zi, za and hu. 
> However, the estimated marginal predictions are sensible for hu, and 
> za (ie conditional and marginal predictions are quite similar), while 
> for zi they are way too large (getting values much higher than you 
> find in the observed data). Yes, marginal predictions should be larger 
> by definition, and may be quite much higher if you have some sites 
> with very high means that pulls the expected "random site" prediction 
> above the "average site" estimate. However, this crazy large "blow up" 
> effect that I see (predictions outside data range), I guess can happen 
> when random effects are large. But what are actually "good" 
> predictions here? If all classic model diagnostics looks good and 
> conditional predictions are close to raw (pooled) treatments means but 
> marginal predictions are way too high, what is then "wrong" with the 
> model? Where do you start looking to identify whats going on? Rarely 
> are predicted estimates reported in papers (cond nor marginal) and Im 
> trying to get my head around how they can be better used in model 
> checking.
>
> Tom> Thanks for your input. I will look closer at your paper.
>
> Gustaf
>
>
> On 2016-11-26 08:59, Jarrod Hadfield wrote:
>> Hi,
>>
>> If X1 was has two levels then it is odd that you have an intercept 
>> AND 2 X1 contrasts. Have you used singular.ok=TRUE? If not then are 
>> you sure there is not a third (rare) level to X1? Try 
>> unique(your_data$X1): trailing whitespace from excel spreadsheets is 
>> a common cause of such a problem.
>>
>> Regarding the predictions, the issue may be with the above. However, 
>> note that there was a bug with posterior="mean" in the call to 
>> predict.MCMCglmm (the first posterior iteration of the fixed effects 
>> was used rather than there mean). This bug is fixed in the current 
>> version, and there was no bug in the default posterior="all".
>>
>> Cheers,
>>
>> Jarrod
>>
>>
>>
>> On 25/11/2016 13:16, Gustaf Granath wrote:
>>> Hi,
>>> A few questions regarding zero-inflated models using MCMCglmm.
>>>
>>> 1. Odd contrasts are created when using, e.g. :
>>> y ~ trait-1 + at.level(trait, 1):(X1+X2), family = "zipoisson" #X1 
>>> and X2 are factors with 2 levels
>>> COEF TABLE
>>> traity
>>> traitzi_y
>>> at.level(trait, 1):X1lev1
>>> at.level(trait, 1):X1lev2
>>> at.level(trait, 1):X2lev2
>>>
>>> It doesnt look like this in the course notes (what I can see). Is 
>>> at.level(trait, 1):X1lev1 the reference level for everything below? 
>>> I also get very high (>1000) estimates for traity, at.level(trait, 
>>> 1):X1lev1 and at.level(trait, 1):X1lev2.
>>>
>>> 2. I made four models
>>> a) y ~ X1*X2, family = "poisson"
>>> Large overdispersion (units ~ 5) but everything looks fine 
>>> (traceplots, random effects (including units) ). Predictive checks 
>>> show that the models predict too few zeros though. And alarming is 
>>> that predictions are really bad, e.g. a treatment mean of 40 is 
>>> predicted to have 300 counts.
>>>
>>> b) y ~ trait-1 + at.level(trait, 1):(X1+X2), family = "zipoisson"
>>> Model looks OK, but again, predictions are way too high for the 
>>> higher tretament means.
>>>
>>> c:d) y ~ trait-1 + at.level(trait, 1):(X1+X2), family = "zapoisson" 
>>> or "hupoisson"
>>> Predicted values are much better. But is it the same way get 
>>> predictions here? Possible to get predictions from the separate 
>>> models (poisson/binomial) in the hurdle case? (code available?)
>>> #get e.g. treatment predictions?
>>> predict.MCMCglmm(model, marginal = ~ random_factor, type="response", 
>>> posterior = "mean")
>>>
>>> The "zapoisson" model performs best but I dont understand why the 
>>> "zipoisson" is so bad. Any typical things to look for when it looks 
>>> like this?
>>> Interpreting "zapoisson" isnt easy, any good literature/tutorials on 
>>> this model?
>>>
>>>
>>> Cheers
>>>
>>> Gustaf
>>>
>>>
>>
>>
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From profiack at gmail.com  Wed Nov 30 21:41:32 2016
From: profiack at gmail.com (Cleber Iack)
Date: Wed, 30 Nov 2016 20:41:32 +0000
Subject: [R-sig-ME] ROC Curve - Mixed Model
Message-ID: <CAM-sWA37BZLYHTmm=JLnrhk8uvRT2XdBq8HLxRj9PZo5kfnWaQ@mail.gmail.com>

Dear,

I would like if it was possible that the Lord would help me in a review,
lest it incur in error.

If it were a simple logistic regression, I could use The Area Under an ROC
Curve, but since I'm using a generalized linear mixed model with reply
Binaria, can analyze the same way this output?

If not, what would you suggest?

Thank you

Cleber

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Wed Nov 30 21:56:24 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 30 Nov 2016 15:56:24 -0500
Subject: [R-sig-ME] How does lmer treat a date.format (as.Date) variable?
In-Reply-To: <CAB+XKfm7jPWcn90Qpzy7z+Tjw4UdWMpcvr-XtNfmwGtcTiQipQ@mail.gmail.com>
References: <CAB+XKfm7jPWcn90Qpzy7z+Tjw4UdWMpcvr-XtNfmwGtcTiQipQ@mail.gmail.com>
Message-ID: <c5ed7ff7-ccee-3502-4f9b-aa6325ad099d@gmail.com>



On 16-11-30 01:51 AM, Meytar Sorek-Hamer wrote:
> Dear R project team,
> I have been working for a while with lmer (lme4) and I have a question
> please. I don't fully understand yet how does lmer treat a random variable
> in a date format (as.Date).
> This is the model syntax:
> 
> Y~X+(1+X|date)
> 
> I compared model results based on the same dataset just each time (model) I
> changed the date variable format (i.e. as.factor, as.numeric, as.Date) and
> received different results, while the best results were obtained when the
> date was in a date.format.
> 
> I would highly appreciate your clarification for the differences between
> the three formats in lmer.
> 
> Thank you very much in advance
> 

  This seems weird. lme4 will always attempt to convert the grouping
variable (date in your case) to a factor.  Can we have a (small!)
reproducible example please? (What do you mean by "best results" in this
case?)

  Ben Bolker


From gustaf.granath at gmail.com  Thu Dec  1 00:30:17 2016
From: gustaf.granath at gmail.com (Gustaf Granath)
Date: Thu, 1 Dec 2016 00:30:17 +0100
Subject: [R-sig-ME] zero-inflated models in MCMCglmm
In-Reply-To: <49bbc5f5-4576-079c-2108-1f2ce2ba31c2@ed.ac.uk>
References: <d4a6fd3a-151a-32e0-1fea-12ff7fe48466@gmail.com>
	<a5c89be4-f711-7b25-90de-b38e15002f4d@ed.ac.uk>
	<f8ca8f5f-a6c1-7e35-d94e-25b611fa56d2@gmail.com>
	<49bbc5f5-4576-079c-2108-1f2ce2ba31c2@ed.ac.uk>
Message-ID: <3c4f809e-bb38-faa4-5417-3fa7fe6b059d@gmail.com>

Hi,

Thanks for looking at this Jarrod. I have included R code below and data 
is loaded from github.

I realized that I had missed one thing though. I must have suppressed 
warnings, because I now see that my zip-model gives a warning about not 
being able to estimate all fixed effects (thus removed). I get no 
warnings for poisson and zap models. The warning is telling my that 
there is a reason why the contrasts are messed up, but I havent been 
able to pinpoint whats going on though (I have checked that the 
experimental design is correct etc). This is likely very case specific, 
but I'd appreciate any qualified guess that can help me.

Cheers

Gustaf

#################
# test data
require(RCurl)
zero.dat 
<-read.csv(text=getURL("https://raw.githubusercontent.com/ggranath/zero_inf_models/master/test_data.csv"),header=T)
#zero.dat <- read.csv("test_data.csv")

#plots nested in site (X3 treatments at plot-level)
zero.dat$nested_plot <- factor(with(zero.dat, paste(site, plot, sep= 
"_")) )

#zip model
nitt = 2000 #low for testing
thin = 10 #low for testing
prior = list( R = list(V = diag(2), nu = 0.002, fix = 2),
               G=list(G1=list(V=1, nu=0.002,alpha.mu=0, alpha.V=625^2)))
zip <- MCMCglmm(y ~ trait -1 + at.level(trait, 1):(X1*X2*X3_nest),
                 random = ~ nested_plot, rcov = ~idh(trait):units,
                 data=zero.dat, family = "zipoisson",  nitt = nitt,
                 burnin = 100, thin=thin, prior=prior, pr = TRUE, pl = TRUE)
summary(zip)
# Gives a warning!! And coef table seems messed up.
# Something is going on here.....dont know what though.

#zap model
zap <- MCMCglmm(y ~ trait*(X1*X2*X3_nest),
                     random = ~ nested_plot, rcov = ~idh(trait):units,
                     data=zero.dat, family = "zapoisson",  nitt = nitt,
                     burnin = 100, thin=thin,  prior=prior, pr = TRUE, 
pl = TRUE)
summary(zap)
# No warning and everything looks good

# Poisson model
prior = list( R = list(V = diag(1), nu = 0.002),
               G=list(G1=list(V=1, nu=0.002,alpha.mu=0, alpha.V=625^2)))
pois <- MCMCglmm(y ~ X1*X2*X3_nest,
                 random = ~ nested_plot,
                 data=zero.dat, family = "poisson",  nitt = nitt,
                 burnin = 100, thin=thin,  prior=prior, pr = TRUE, pl = 
TRUE)
summary(pois)
# No warning

# Some marginal predictions
p.zip <- predict(zip, marginal = ~ nested_plot,  type="response", 
posterior = "mean")
p.zap <- predict(zap, marginal = ~ nested_plot,  type="response", 
posterior = "mean")
p.pois <- predict(pois, marginal = ~ nested_plot, type="response", 
posterior = "mean")
cbind(aggregate(y ~ X1*X2*X3_nest, zero.dat, mean),
       zip = aggregate(p.zip ~ fire*retention*micro_hab.two, dat, mean)$V1,
       zap = aggregate(p.zap ~ fire*retention*micro_hab.two, dat, mean)$V1,
       pois = aggregate(p.pois ~ fire*retention*micro_hab.two, dat, 
mean)$V1)
# poisson model gives extreme (unrealistic) values.


On 2016-11-30 21:21, Jarrod Hadfield wrote:
> Hi Gustaf,
>
> Is it possible to post your data/code so I can have a look? I can't 
> think of any reason why the base-line level of X1 would change for the 
> main effect but be the same for the interaction.
>
> Cheers,
>
> Jarrod
>
>
> On 28/11/2016 11:11, Gustaf Granath wrote:
>> Jarrod,
>>
>> singular.ok=TRUE was the problem. I did some copy-paste and didnt 
>> clean up the code. However, Im still confused about the coefs. Say 
>> that X1 and X2 each have two levels: no/yes and empty/full 
>> respectively. For a "zipoisson" I get the coef table:
>>
>> traity
>> traitzi_y
>> at.level(trait, 1):X1no
>> at.level(trait, 1):X2full
>> at.level(trait, 1):X1yes:X2full
>>
>> But when running "zapiosson" I get:
>>
>> traity
>> traitza_y
>> at.level(trait, 1):X1yes
>> at.level(trait, 1):X2full
>> at.level(trait, 1):X1yes:X2full
>> + traitza_...
>>
>> The "zapiosson" output makes sense to me but the "zipoisson" looks 
>> odd to me.
>>
>> Predictions: ok, estimates actually changed after I updated the 
>> package. I didnt make a huge difference though and it doesnt clear 
>> things up completely. The strange thing is that conditional 
>> predictions captures the data really well - both for zi, za and hu. 
>> However, the estimated marginal predictions are sensible for hu, and 
>> za (ie conditional and marginal predictions are quite similar), while 
>> for zi they are way too large (getting values much higher than you 
>> find in the observed data). Yes, marginal predictions should be 
>> larger by definition, and may be quite much higher if you have some 
>> sites with very high means that pulls the expected "random site" 
>> prediction above the "average site" estimate. However, this crazy 
>> large "blow up" effect that I see (predictions outside data range), I 
>> guess can happen when random effects are large. But what are actually 
>> "good" predictions here? If all classic model diagnostics looks good 
>> and conditional predictions are close to raw (pooled) treatments 
>> means but marginal predictions are way too high, what is then "wrong" 
>> with the model? Where do you start looking to identify whats going 
>> on? Rarely are predicted estimates reported in papers (cond nor 
>> marginal) and Im trying to get my head around how they can be better 
>> used in model checking.
>>
>> Tom> Thanks for your input. I will look closer at your paper.
>>
>> Gustaf
>>
>>
>> On 2016-11-26 08:59, Jarrod Hadfield wrote:
>>> Hi,
>>>
>>> If X1 was has two levels then it is odd that you have an intercept 
>>> AND 2 X1 contrasts. Have you used singular.ok=TRUE? If not then are 
>>> you sure there is not a third (rare) level to X1? Try 
>>> unique(your_data$X1): trailing whitespace from excel spreadsheets is 
>>> a common cause of such a problem.
>>>
>>> Regarding the predictions, the issue may be with the above. However, 
>>> note that there was a bug with posterior="mean" in the call to 
>>> predict.MCMCglmm (the first posterior iteration of the fixed effects 
>>> was used rather than there mean). This bug is fixed in the current 
>>> version, and there was no bug in the default posterior="all".
>>>
>>> Cheers,
>>>
>>> Jarrod
>>>
>>>
>>>
>>> On 25/11/2016 13:16, Gustaf Granath wrote:
>>>> Hi,
>>>> A few questions regarding zero-inflated models using MCMCglmm.
>>>>
>>>> 1. Odd contrasts are created when using, e.g. :
>>>> y ~ trait-1 + at.level(trait, 1):(X1+X2), family = "zipoisson" #X1 
>>>> and X2 are factors with 2 levels
>>>> COEF TABLE
>>>> traity
>>>> traitzi_y
>>>> at.level(trait, 1):X1lev1
>>>> at.level(trait, 1):X1lev2
>>>> at.level(trait, 1):X2lev2
>>>>
>>>> It doesnt look like this in the course notes (what I can see). Is 
>>>> at.level(trait, 1):X1lev1 the reference level for everything below? 
>>>> I also get very high (>1000) estimates for traity, at.level(trait, 
>>>> 1):X1lev1 and at.level(trait, 1):X1lev2.
>>>>
>>>> 2. I made four models
>>>> a) y ~ X1*X2, family = "poisson"
>>>> Large overdispersion (units ~ 5) but everything looks fine 
>>>> (traceplots, random effects (including units) ). Predictive checks 
>>>> show that the models predict too few zeros though. And alarming is 
>>>> that predictions are really bad, e.g. a treatment mean of 40 is 
>>>> predicted to have 300 counts.
>>>>
>>>> b) y ~ trait-1 + at.level(trait, 1):(X1+X2), family = "zipoisson"
>>>> Model looks OK, but again, predictions are way too high for the 
>>>> higher tretament means.
>>>>
>>>> c:d) y ~ trait-1 + at.level(trait, 1):(X1+X2), family = "zapoisson" 
>>>> or "hupoisson"
>>>> Predicted values are much better. But is it the same way get 
>>>> predictions here? Possible to get predictions from the separate 
>>>> models (poisson/binomial) in the hurdle case? (code available?)
>>>> #get e.g. treatment predictions?
>>>> predict.MCMCglmm(model, marginal = ~ random_factor, 
>>>> type="response", posterior = "mean")
>>>>
>>>> The "zapoisson" model performs best but I dont understand why the 
>>>> "zipoisson" is so bad. Any typical things to look for when it looks 
>>>> like this?
>>>> Interpreting "zapoisson" isnt easy, any good literature/tutorials 
>>>> on this model?
>>>>
>>>>
>>>> Cheers
>>>>
>>>> Gustaf
>>>>
>>>>
>>>
>>>
>>


From j.hadfield at ed.ac.uk  Thu Dec  1 07:48:56 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 1 Dec 2016 06:48:56 +0000
Subject: [R-sig-ME] zero-inflated models in MCMCglmm
In-Reply-To: <3c4f809e-bb38-faa4-5417-3fa7fe6b059d@gmail.com>
References: <d4a6fd3a-151a-32e0-1fea-12ff7fe48466@gmail.com>
	<a5c89be4-f711-7b25-90de-b38e15002f4d@ed.ac.uk>
	<f8ca8f5f-a6c1-7e35-d94e-25b611fa56d2@gmail.com>
	<49bbc5f5-4576-079c-2108-1f2ce2ba31c2@ed.ac.uk>
	<3c4f809e-bb38-faa4-5417-3fa7fe6b059d@gmail.com>
Message-ID: <448e2b5c-4826-aa2d-df26-3783cf314c3c@ed.ac.uk>

Hi Gustaf,

I don't have dat, so I can't run the final bit of code.

However, these are my thoughts.

1/ In the zap model you are allowing X1 to X3 to effect the level of 
zero alteration, whereas in the zip model you are just fitting a 
constant level  of zero inflation across X1 to X3. In this sense the zap 
model will provide a superior fit because it has more parameters. The 
warning message about dropping terms is fine, although the default 
contrast for the zip model is a bit annoying: I would have preferred the 
main effect for X1 to be yes rather than no, but I guess its no big deal.

2/ You have fitted a single nested_plot effect in the random effects. 
This is a little odd, except in the case where the data are not 
zero-inflated. In this case having a single nested_plot term in the zap 
model is equivalent to fitting a nested_plot term in the standard 
Poisson. If the data are zero-inflated, and/or the model is not a zap 
model, then the case for having a single term is not well justified. I 
would use us or idh and in the latter case consider fixing the second 
variance (associated with zero-inflation/alteration) close to zero.

3/ The marginal predictions do not take into account the covariances 
between traits. This is generally OK, but its not ideal when the traits 
refer to the multiple parameters of a single distribution as with 
za/zi/hu models. I should put a warning in. You can also use the 
simulate function on the model and then average over the draws to get 
the predictions, and this will take into account any covariances. Note 
that if you use idh(trait):nested_plot there are no covariances so 
predict should be fine.

Cheers,

Jarrod






On 30/11/2016 23:30, Gustaf Granath wrote:
> cbind(aggregate(y ~ X1*X2*X3_nest, zero.dat, mean),
>       zip = aggregate(p.zip ~ fire*retention*micro_hab.two, dat, 
> mean)$V1,
>       zap = aggregate(p.zap ~ fire*retention*micro_hab.two, dat, 
> mean)$V1,
>       pois = aggregate(p.pois ~ fire*retention*micro_hab.two, dat, 
> mean)$V1)


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From chanratana.pin at gmail.com  Thu Dec  1 08:36:00 2016
From: chanratana.pin at gmail.com (Pin chanratana)
Date: Thu, 1 Dec 2016 14:36:00 +0700
Subject: [R-sig-ME] Zero-inflation with glmmadmb
Message-ID: <CANW7wbfChL3QY_4bggXykDoEfxUGEj+N1Ozb8h-2atO0OWQH+w@mail.gmail.com>

Hi,

I have some question regarding to zero-inflation and hurdle model

- I fit a glmmadmb model of a species camera-trapped at waterholes during
the dry season. The data contain 51.9% of zeros and the
variance/mean=10.41. Following is a fit model and its result:

m1 <- glmmadmb(WS ~ depth + offset(log(TN)), ndata4, "nbinom1")

Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)   -3.677      0.214  -17.16   <2e-16 ***
depth          0.318      0.136    2.34     0.02 *

Negative binomial dispersion parameter: 6.6339 (std. err.: 1.7732)

- And then I check for overdispersion
E <- resid(m1, type= 'pearson')
N <- nrow(ndata4)
p <- length(coef(m1))
sum(E^2)/(N-p)  = 14.22

My questions are:
1. With the information above, should I go for more extend such as
zero-inflation and hurdle model?
2. To fit zero-inflation model, which approach is better way between
glmmadmb, zero-inflation and hurdle function?

Thanks,

	[[alternative HTML version deleted]]


From mmisalem at gmail.com  Thu Dec  1 12:00:12 2016
From: mmisalem at gmail.com (Mohamed Salem)
Date: Thu, 1 Dec 2016 13:00:12 +0200
Subject: [R-sig-ME] binary trait
Message-ID: <CADJLSs+QKRGD3qXE-Rv+uKpiAP9rdyrTCcKv6aSSCPP0pc27-w@mail.gmail.com>

Dears,
I am trying to use MCMCglmm to estimate heritability for binary trait.
I used this model
"

prior <- list(R = list(V = 1, fix = 1), G = list(G1 =

list(V = 1, nu = 1000, alpha.mu = 0, alpha.V = 1)))



model1 <- MCMCglmm(SB ~ 1 + Farm +year, random = ~animal, family =
"ordinal",

prior = prior, pedigree = Ped, data = Data, nitt = 1e+06,burnin = 10000,
thin = 100)
and when I diagnosed the MCMC work by autocorr.diag(model1$VCV)
I found this results
         animal units
Lag 0    1.0000000   NaN
Lag 100  0.9786790   NaN
Lag 500  0.9092071   NaN
Lag 1000 0.8360430   NaN
Lag 5000 0.4860764   NaN
 how can I avoid this problem?
-- 
*Mohamed M. I. Salem*

Lecturer
Department of Animal Production
Faculty of Agriculture, Alexandria University
Aflaton St., El-Shatby, P. O. Box 21545
Alexandria, Egypt
Tel: (+203) 5915427, (+203) 5924391
Fax (+203)5922780
e-mail: mmisalem at gmail.com
           mohamed.salim at alexu.edu.eg
skype: mmisalem

	[[alternative HTML version deleted]]


From j.hadfield at ed.ac.uk  Thu Dec  1 12:24:35 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 1 Dec 2016 11:24:35 +0000
Subject: [R-sig-ME] binary trait
In-Reply-To: <CADJLSs+QKRGD3qXE-Rv+uKpiAP9rdyrTCcKv6aSSCPP0pc27-w@mail.gmail.com>
References: <CADJLSs+QKRGD3qXE-Rv+uKpiAP9rdyrTCcKv6aSSCPP0pc27-w@mail.gmail.com>
Message-ID: <c7b6f45e-c95e-72b5-12fb-44ae2fa2665b@ed.ac.uk>

Hi Mohamed,

Binary animals models tend to mix poorly but this is quite extreme. From 
the output you have does it look like h2 is very small or very large? 
Also, could you give a quick summary of the data (size, number of farms, 
number of years, many relatives/few relatives ...)

Cheers,

Jarrod





On 01/12/2016 11:00, Mohamed Salem wrote:
> Dears,
> I am trying to use MCMCglmm to estimate heritability for binary trait.
> I used this model
> "
>
> prior <- list(R = list(V = 1, fix = 1), G = list(G1 =
>
> list(V = 1, nu = 1000, alpha.mu = 0, alpha.V = 1)))
>
>
>
> model1 <- MCMCglmm(SB ~ 1 + Farm +year, random = ~animal, family =
> "ordinal",
>
> prior = prior, pedigree = Ped, data = Data, nitt = 1e+06,burnin = 10000,
> thin = 100)
> and when I diagnosed the MCMC work by autocorr.diag(model1$VCV)
> I found this results
>           animal units
> Lag 0    1.0000000   NaN
> Lag 100  0.9786790   NaN
> Lag 500  0.9092071   NaN
> Lag 1000 0.8360430   NaN
> Lag 5000 0.4860764   NaN
>   how can I avoid this problem?


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From arianna.cecchetti at uac.pt  Thu Dec  1 13:42:01 2016
From: arianna.cecchetti at uac.pt (Arianna Cecchetti)
Date: Thu, 1 Dec 2016 12:42:01 +0000
Subject: [R-sig-ME] GEE and GLMM convergence issues
Message-ID: <E790ADDA-D1FE-4C8C-8072-52613C1D4C72@uac.pt>

Dear all,

I am investigating differences of activity budget of dolphins across different temporal scales (i.e. years, months and and day time) and in relation to group size. I attach a subset of the data as example. I tried different approaches, but got convergence issues that I am struggling to deal with:

1)       multinomial GEE (multgee package)
I first run an independence structure being this one faster. When I run the most complete models they fail the fitting and initial values are asked. So I provided some values with the bstart parameter including 3 and 4 initial values for the 3 and 4 predictors of the two models:

mod6 <- nomLORgee(ActivityState~GroupSize + GroupSize*as.factor(Year) + GroupSize*Month, data=test, id=NSequence, repeated=Rep, LORstr="independence", bstart=c(1,0,1))

mod7 <- nomLORgee(ActivityState~GroupSize + GroupSize*as.factor(Year) + GroupSize*Month + GroupSize*Day, data=test, id=NSequence, repeated=Rep, LORstr="independence", bstart=c(1,0,1,0))


but I get the following error message:

Error in nomLORgee(ActivityState ~ GroupSize + GroupSize * as.factor(Year) +  :
  'bstart' and 'beta' differ in length

I am wondering if I am providing the initial values in the correct way. When I try to run the same models with the time.exch structure even the most simple one including only one variable get stuck in a loop and won?t generate any output.

2)       Four separated binomial GEE models for each activity state.
In one case values for estimates and standard errors doesn?t seem reliable.

geeglm(formula = ActivityState ~ GroupSize + as.factor(Year) + Month + Day + GroupSize * as.factor(Year) + GroupSize * Month + GroupSize * Day, family = binomial(link = "logit"), data = testsoc, id = NSequence, waves = Rep, corstr = "exch", scale.fix = T)

Coefficients:
                                                                                                Estimate   Std.err   Wald Pr(>|W|)
(Intercept)                                                                            5.84e+15  1.09e+16   0.29   0.5933
GroupSize                                                                             -1.75e+15  5.55e+14   9.91   0.0016 **
as.factor(Year)2014                                                           9.45e+15  1.60e+16   0.35   0.5533
as.factor(Month)August                                                   -3.62e+16  6.52e+15  30.93  2.7e-08 ***
as.factor(Month)July                                                         -3.53e+16  7.47e+15  22.36  2.3e-06 ***
as.factor(Month)June                                                        1.78e+17  9.80e+16   3.31   0.0689 .
as.factor(Month)May                                                        -1.31e+15  1.32e+16   0.01   0.9207
as.factor(Month)September                                           -1.29e+15  2.36e+16   0.00   0.9566
?..

Estimated Correlation Parameters:
   Estimate             Std.err
alpha 1.44e+15     2.43e+31
Number of clusters:   183   Maximum cluster size: 50

3)       GLMM (package lme4, glmer function)
For the models which failed convergence I applied different optimizers, but some still report convergence failure although they provide an output (here is an example)
ActivityState ~ log(GroupSize) + (1 | NSequence) + as.factor(Year) + Month + Day + log(GroupSize) * as.factor(Year) + log(GroupSize) * Month + log(GroupSize) * Day
Data: testtrav
Control: glmerControl(optimizer = "bobyqa")
Scaled residuals:
   Min     1Q Median     3Q    Max
-2.565 -0.517 -0.150  0.552  5.757

Random effects:
Groups    Name        Variance Std.Dev.
NSequence (Intercept) 6.59     2.57
Number of obs: 2207, groups:  NSequence, 183

Fixed effects:
                                                                                                Estimate Std. Error z value Pr(>|z|)
(Intercept)                                                                            -5.0105     3.5342   -1.42  0.15628
log(GroupSize)                                                                     0.8871     1.2095    0.73  0.46331
as.factor(Year)2014                                                           1.4536     1.4187    1.02  0.30553
as.factor(Month)August                                                   2.4192     3.6134    0.67  0.50317
as.factor(Month)July                                                         1.5645     3.4415    0.45  0.64940
as.factor(Month)June                                                        9.4559     4.2818    2.21  0.02722 *

?..

Correlation matrix not shown by default, as p = 20 > 12.
Use print(x, correlation=TRUE)  or
                vcov(x)  if you need it

convergence code: 1
Model failed to converge with max|grad| = 0.00394719 (tol = 0.001, component 1)

Using max(abs(relgrad)) code all models failed to be reliable being the values well above 0.001.

So at this point I am a bit stuck in deciding which approach would be the best to deal with these convergence issues.
I greatly appreciate any suggestions at this regard.

Thank you!

Best,

Arianna







From mmisalem at gmail.com  Thu Dec  1 16:29:44 2016
From: mmisalem at gmail.com (Mohamed Salem)
Date: Thu, 1 Dec 2016 17:29:44 +0200
Subject: [R-sig-ME] binary trait
Message-ID: <CADJLSsKwK_CM9CKft9YZ+izthYZXCrordHREGicYwGi+p7ymBg@mail.gmail.com>

Dears,
I am trying to use MCMCglmm to estimate heritability for binary trait.
I used this model
"

prior <- list(R = list(V = 1, fix = 1), G = list(G1 =

list(V = 1, nu = 1000, alpha.mu = 0, alpha.V = 1)))



model1 <- MCMCglmm(SB ~ 1 + Farm +year, random = ~animal, family =
"ordinal",

prior = prior, pedigree = Ped, data = Data, nitt = 1e+06,burnin = 10000,
thin = 100)
and when I diagnosed the MCMC work by autocorr.diag(model1$VCV)
I found this results
         animal units
Lag 0    1.0000000   NaN
Lag 100  0.9786790   NaN
Lag 500  0.9092071   NaN
Lag 1000 0.8360430   NaN
Lag 5000 0.4860764   NaN

the total number of animals is 2800 animal

the summary of the analysis are
Iterations = 10001:999901
 Thinning interval  = 100
 Sample size  = 9900

 DIC: 1347.1

 G-structure:  ~animal

       post.mean  l-95% CI u-95% CI eff.samp
animal     1.433 8.014e-08    4.598    64.81

 R-structure:  ~units

      post.mean l-95% CI u-95% CI eff.samp
units         1        1        1        0

 Location effects: SB ~ 1 + Farm + year

             post.mean   l-95% CI   u-95% CI eff.samp    pMCMC
(Intercept) -5.836e+02 -1.003e+03 -4.714e+00    2.632 0.000404 ***
Farm2       -7.124e-02 -6.184e-01  4.535e-01 3785.279 0.803636
Farm3       -2.997e-01 -8.896e-01  2.803e-01  886.117 0.288889
Farm4       -1.194e-01 -1.213e+00  9.619e-01 7819.001 0.836162
Farm5        5.514e-01 -9.988e-02  1.279e+00  652.534 0.071919 .
Farm6        4.886e-01 -1.597e-01  1.149e+00  655.714 0.097172 .
year1978     3.215e+02 -4.543e+02  8.054e+02    3.190 0.353535
year1979     1.188e+03  1.480e+02  2.099e+03    1.665  < 1e-04 ***
year1980     1.952e+02 -1.496e+02  6.365e+02    5.395 0.394343
year1981     5.817e+02 -1.770e+00  9.969e+02    2.623 0.001818 **
year1982     3.986e+02 -3.734e+02  9.482e+02    2.515 0.356768
year1983     4.984e+02 -6.546e+01  1.014e+03    2.381 0.167071
year1984     5.007e+02 -8.501e+01  9.976e+02    2.620 0.193535
year1985     2.057e+02 -1.166e+02  6.239e+02    5.649 0.350505
year1986     5.812e+02  1.908e+00  1.000e+03    2.619 0.002424 **
year1987     5.809e+02  2.574e+00  1.001e+03    2.627 0.002222 **
year1988     5.816e+02  2.922e+00  1.001e+03    2.557 0.001616 **
year1989     5.800e+02  1.863e+00  1.000e+03    2.623 0.002828 **
year1990     5.807e+02  1.117e+00  9.997e+02    2.558 0.002020 **
year1991     5.811e+02  2.930e+00  1.002e+03    2.549 0.001818 **
year1992     5.800e+02  1.759e+00  1.000e+03    2.626 0.002222 **
year1993     5.812e+02  2.556e+00  1.002e+03    2.629 0.001818 **
year1994     5.035e+02 -3.250e+00  8.543e+02    3.221 0.013939 *
year1995     5.809e+02  2.055e+00  1.001e+03    2.626 0.002222 **
year1996     5.801e+02 -1.762e+00  9.969e+02    2.627 0.002424 **
year1997     5.808e+02  1.968e+00  1.001e+03    2.554 0.001818 **
year1998     5.805e+02  2.003e+00  1.000e+03    2.627 0.002020 **
year1999     5.803e+02 -1.759e+00  9.969e+02    2.622 0.002828 **
year2000     5.814e+02  2.903e+00  1.001e+03    2.627 0.001818 **
year2001     5.809e+02  2.474e+00  1.001e+03    2.628 0.002020 **
year2002     5.813e+02  2.494e+00  1.001e+03    2.557 0.001818 **
year2003     5.815e+02  3.182e+00  1.002e+03    2.627 0.001818 **
year2004     5.813e+02  2.462e+00  1.001e+03    2.628 0.001818 **
year2005     5.815e+02  2.719e+00  1.002e+03    2.629 0.001818 **
year2006     5.820e+02  3.711e+00  1.002e+03    2.628 0.001414 **
year2007     5.821e+02  3.275e+00  1.002e+03    2.631 0.001616 **
year2008     5.818e+02  3.401e+00  1.002e+03    2.556 0.001414 **
year2009     5.815e+02  2.774e+00  1.001e+03    2.626 0.001818 **
year2010     5.822e+02  3.230e+00  1.002e+03    2.559 0.001010 **
year2011     5.815e+02  2.777e+00  1.001e+03    2.625 0.002020 **
year2012     5.817e+02  2.849e+00  1.001e+03    2.628 0.001818 **
year2013     5.812e+02  2.790e+00  1.001e+03    2.550 0.002020 **
year2014     1.103e+02 -3.772e+02  4.805e+02    3.673 0.673737


-- 
*Mohamed M. I. Salem*

Lecturer
Department of Animal Production
Faculty of Agriculture, Alexandria University
Aflaton St., El-Shatby, P. O. Box 21545
Alexandria, Egypt
Tel: (+203) 5915427, (+203) 5924391
Fax (+203)5922780
e-mail: mmisalem at gmail.com
           mohamed.salim at alexu.edu.eg
skype: mmisalem

	[[alternative HTML version deleted]]


From j.hadfield at ed.ac.uk  Thu Dec  1 20:49:05 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Thu, 1 Dec 2016 19:49:05 +0000
Subject: [R-sig-ME] binary trait
In-Reply-To: <CADJLSsKwK_CM9CKft9YZ+izthYZXCrordHREGicYwGi+p7ymBg@mail.gmail.com>
References: <CADJLSsKwK_CM9CKft9YZ+izthYZXCrordHREGicYwGi+p7ymBg@mail.gmail.com>
Message-ID: <2daf4008-83b3-acf9-76e2-49ea0685107a@ed.ac.uk>

Hi,

It looks like you might have complete separation in the fixed effects, 
and therefore under/overflow. Are there some years or farms where all 
animals have a zero or one? Treating them as random effects might 
alleviate the problem, and be a sensible option anyway given the number 
of levels.

Cheers,

Jarrod


On 01/12/2016 15:29, Mohamed Salem wrote:
> Dears,
> I am trying to use MCMCglmm to estimate heritability for binary trait.
> I used this model
> "
>
> prior <- list(R = list(V = 1, fix = 1), G = list(G1 =
>
> list(V = 1, nu = 1000, alpha.mu = 0, alpha.V = 1)))
>
>
>
> model1 <- MCMCglmm(SB ~ 1 + Farm +year, random = ~animal, family =
> "ordinal",
>
> prior = prior, pedigree = Ped, data = Data, nitt = 1e+06,burnin = 10000,
> thin = 100)
> and when I diagnosed the MCMC work by autocorr.diag(model1$VCV)
> I found this results
>           animal units
> Lag 0    1.0000000   NaN
> Lag 100  0.9786790   NaN
> Lag 500  0.9092071   NaN
> Lag 1000 0.8360430   NaN
> Lag 5000 0.4860764   NaN
>
> the total number of animals is 2800 animal
>
> the summary of the analysis are
> Iterations = 10001:999901
>   Thinning interval  = 100
>   Sample size  = 9900
>
>   DIC: 1347.1
>
>   G-structure:  ~animal
>
>         post.mean  l-95% CI u-95% CI eff.samp
> animal     1.433 8.014e-08    4.598    64.81
>
>   R-structure:  ~units
>
>        post.mean l-95% CI u-95% CI eff.samp
> units         1        1        1        0
>
>   Location effects: SB ~ 1 + Farm + year
>
>               post.mean   l-95% CI   u-95% CI eff.samp    pMCMC
> (Intercept) -5.836e+02 -1.003e+03 -4.714e+00    2.632 0.000404 ***
> Farm2       -7.124e-02 -6.184e-01  4.535e-01 3785.279 0.803636
> Farm3       -2.997e-01 -8.896e-01  2.803e-01  886.117 0.288889
> Farm4       -1.194e-01 -1.213e+00  9.619e-01 7819.001 0.836162
> Farm5        5.514e-01 -9.988e-02  1.279e+00  652.534 0.071919 .
> Farm6        4.886e-01 -1.597e-01  1.149e+00  655.714 0.097172 .
> year1978     3.215e+02 -4.543e+02  8.054e+02    3.190 0.353535
> year1979     1.188e+03  1.480e+02  2.099e+03    1.665  < 1e-04 ***
> year1980     1.952e+02 -1.496e+02  6.365e+02    5.395 0.394343
> year1981     5.817e+02 -1.770e+00  9.969e+02    2.623 0.001818 **
> year1982     3.986e+02 -3.734e+02  9.482e+02    2.515 0.356768
> year1983     4.984e+02 -6.546e+01  1.014e+03    2.381 0.167071
> year1984     5.007e+02 -8.501e+01  9.976e+02    2.620 0.193535
> year1985     2.057e+02 -1.166e+02  6.239e+02    5.649 0.350505
> year1986     5.812e+02  1.908e+00  1.000e+03    2.619 0.002424 **
> year1987     5.809e+02  2.574e+00  1.001e+03    2.627 0.002222 **
> year1988     5.816e+02  2.922e+00  1.001e+03    2.557 0.001616 **
> year1989     5.800e+02  1.863e+00  1.000e+03    2.623 0.002828 **
> year1990     5.807e+02  1.117e+00  9.997e+02    2.558 0.002020 **
> year1991     5.811e+02  2.930e+00  1.002e+03    2.549 0.001818 **
> year1992     5.800e+02  1.759e+00  1.000e+03    2.626 0.002222 **
> year1993     5.812e+02  2.556e+00  1.002e+03    2.629 0.001818 **
> year1994     5.035e+02 -3.250e+00  8.543e+02    3.221 0.013939 *
> year1995     5.809e+02  2.055e+00  1.001e+03    2.626 0.002222 **
> year1996     5.801e+02 -1.762e+00  9.969e+02    2.627 0.002424 **
> year1997     5.808e+02  1.968e+00  1.001e+03    2.554 0.001818 **
> year1998     5.805e+02  2.003e+00  1.000e+03    2.627 0.002020 **
> year1999     5.803e+02 -1.759e+00  9.969e+02    2.622 0.002828 **
> year2000     5.814e+02  2.903e+00  1.001e+03    2.627 0.001818 **
> year2001     5.809e+02  2.474e+00  1.001e+03    2.628 0.002020 **
> year2002     5.813e+02  2.494e+00  1.001e+03    2.557 0.001818 **
> year2003     5.815e+02  3.182e+00  1.002e+03    2.627 0.001818 **
> year2004     5.813e+02  2.462e+00  1.001e+03    2.628 0.001818 **
> year2005     5.815e+02  2.719e+00  1.002e+03    2.629 0.001818 **
> year2006     5.820e+02  3.711e+00  1.002e+03    2.628 0.001414 **
> year2007     5.821e+02  3.275e+00  1.002e+03    2.631 0.001616 **
> year2008     5.818e+02  3.401e+00  1.002e+03    2.556 0.001414 **
> year2009     5.815e+02  2.774e+00  1.001e+03    2.626 0.001818 **
> year2010     5.822e+02  3.230e+00  1.002e+03    2.559 0.001010 **
> year2011     5.815e+02  2.777e+00  1.001e+03    2.625 0.002020 **
> year2012     5.817e+02  2.849e+00  1.001e+03    2.628 0.001818 **
> year2013     5.812e+02  2.790e+00  1.001e+03    2.550 0.002020 **
> year2014     1.103e+02 -3.772e+02  4.805e+02    3.673 0.673737
>
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From rune.haubo at gmail.com  Thu Dec  1 21:30:11 2016
From: rune.haubo at gmail.com (Rune Haubo)
Date: Thu, 1 Dec 2016 21:30:11 +0100
Subject: [R-sig-ME] predict() with cumulative link mixed models fitted
 with clmm2
In-Reply-To: <61ad3eabf727446bac3277b8a675660a@C119S212VM042.msvyvi.vaha.local>
References: <61ad3eabf727446bac3277b8a675660a@C119S212VM042.msvyvi.vaha.local>
Message-ID: <CAG_uk91++xLzSOc3D4OkV5tek1Pwr9jCw1LeGOEWaB38amQvzQ@mail.gmail.com>

On 30 November 2016 at 15:25, Miina Jari (Luke) <jari.miina at luke.fi> wrote:
> Dear list,
>
> I would like to predict() in the ordinal package using a model fitted with clmm2. The aim is to calculate the classification table of the ordered logistic regression model.
>
> Using the wine data and clm the classes can be predicted and the classification efficiency can be calculated.
>> m1 <- clm(rating ~ temp + contact, data = wine)
>> m1
> formula: rating ~ temp + contact
> data:    wine
>
> link  threshold nobs logLik AIC    niter max.grad cond.H
>  logit flexible  72   -86.49 184.98 6(0)  4.01e-12 2.7e+01
>
> Coefficients:
>   tempwarm contactyes
>      2.503      1.528
>
> Threshold coefficients:
>    1|2    2|3    3|4    4|5
> -1.344  1.251  3.467  5.006
>> predict(m1, newdata=wine, type="class")
> $fit
> [1] 2 2 3 3 3 3 4 4 2 2 3 3 3 3 4 4 2 2 3 3 3 3 4 4 2 2 3 3 3 3 4 4 2 2 3
> [36] 3 3 3 4 4 2 2 3 3 3 3 4 4 2 2 3 3 3 3 4 4 2 2 3 3 3 3 4 4 2 2 3 3 3 3
> [71] 4 4
> Levels: 1 2 3 4 5
>
> But when a model is fitted using clmm2 so that there are a nominal predictor (temp) and random effect (judge), predict() doesn't give classes but probabilities. The probabilities are predicted for the classes of rating indicated in the data, aren't they?

Yes, that is correct. The predict method for clmm2 dispatches to
predict.clm2, which does not have a type argument which explains why
it has no effect in the example below. However, if you look at
?predict.clm2 there is a section in the examples explaining how to
compute the class predictions (referencing predict.polr).  You should
be able to achieve the same with clmm2.

>> m4 <- clmm2(rating ~ contact, random = judge, nominal = ~ temp, data = wine, Hess = TRUE)
>> m4
> Cumulative Link Mixed Model fitted with the Laplace approximation
>
> Call:
> clmm2(location = rating ~ contact, nominal = ~temp, random = judge,
>     data = wine, Hess = TRUE)
>
> Random effects:
>            Var Std.Dev
> judge 1.197886 1.09448
>
> Location coefficients:
> contactyes
>   1.784829
>
> No Scale coefficients
>
> Threshold coefficients:
>                    1|2       2|3       3|4       4|5
> (Intercept)  -1.530285  1.355902  4.450757  21.96606
> tempwarm    -26.859452 -2.683005 -3.352416 -19.09974
>
> log-likelihood: -80.39439
> AIC: 180.7888
>> head(predict(m4, newdata=wine, type="class"))
> [1] 0.61714084 0.19337206 0.54060344 0.06501408 0.19620705 0.19620705
>
> Note that probabilities are predicted for an average judge by including the data used to fit the model in the newdata argument of predict.
>
> My questions:
> (1) How do I predict() the probabilities for all the classes (1-5) of rating to determine the classification efficiency?

Please see above.

> (2) It seems that the sign of the predictor will change when it appears as a nominal predictor (see coefficients for tempwarm in m1 and m4). However, according to the equations (1) and (3) in the tutorial on fitting CLMs with ordinal (https://cran.r-project.org/web/packages/ordinal/vignettes/clm_tutorial.pdf), the sign should not change. To me, the sign of the coefficients of nominal predictors and threshold parameters should be the same, or have I missed something?

That is my mistake - sorry, and thanks for pointing it out. The
nominal effects are implemented as effects on the thresholds (theta_j)
 - not the regression parameters (beta) and therefore the sign is
different from what is reflected in eq 3 for the nominal effects. The
right-hand side of eq 3 should have read theta_1j + theta_2j(contact)
- beta(temp) to be true to the implementation.

Hope this helps
Rune

>
> Regards,
> Jari Miina
>
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From gustaf.granath at gmail.com  Thu Dec  1 23:36:22 2016
From: gustaf.granath at gmail.com (Gustaf Granath)
Date: Thu, 1 Dec 2016 23:36:22 +0100
Subject: [R-sig-ME] zero-inflated models in MCMCglmm
In-Reply-To: <448e2b5c-4826-aa2d-df26-3783cf314c3c@ed.ac.uk>
References: <d4a6fd3a-151a-32e0-1fea-12ff7fe48466@gmail.com>
	<a5c89be4-f711-7b25-90de-b38e15002f4d@ed.ac.uk>
	<f8ca8f5f-a6c1-7e35-d94e-25b611fa56d2@gmail.com>
	<49bbc5f5-4576-079c-2108-1f2ce2ba31c2@ed.ac.uk>
	<3c4f809e-bb38-faa4-5417-3fa7fe6b059d@gmail.com>
	<448e2b5c-4826-aa2d-df26-3783cf314c3c@ed.ac.uk>
Message-ID: <dfb6b645-0e2e-7a3a-42f9-06db9c280c25@gmail.com>

Jarrod,

Thanks for explaining this. I now start to understand how these models 
are set up in MCMCglmm. The weird contrasts seem impossible to get rid 
of though, and Im still struggling a bit to work out how to combine the 
effects.

Great idea to use the simulation function. I used it for predictive 
model checking of zeros, and I do have one question. Does simulate() 
include (marginalise) the residual error (units)? I get quite different 
results if I use a "by hand" simulation function (from course notes) and 
simulate(), for a standard Poisson model.

Cheers

Gustaf

####### R code

# get test data
require(MCMCglmm)
require(RCurl)
zero.dat 
<-read.csv(text=getURL("https://raw.githubusercontent.com/ggranath/zero_inf_models/master/test_data.csv"),header=T)

#plots nested in site (X3 treatments at plot-level)
zero.dat$nested_plot <- factor(with(zero.dat, paste(site, plot, sep= 
"_")) )

#zip model
nitt = 10000 #low for testing
thin = 10 #low for testing
burnin = 1000 #low for testing
prior = list( R = list(V = diag(2), nu = 0.002, fix = 2),
               G=list(G1=list(V=diag(2)*c(1,0.001), nu=0.002, fix=2)))
zip <- MCMCglmm(y ~ trait -1 + at.level(trait, 1):(X1*X2*X3_nest),
                 random = ~ idh(trait):nested_plot, rcov = 
~idh(trait):units,
                 data=zero.dat, family = "zipoisson",  nitt = nitt,
                 burnin = burnin, thin=thin, prior=prior, pr = TRUE, pl 
= TRUE)
summary(zip)
# Gives a warning!! And contrasts are not straight forward to interpret.

#zap model
prior = list( R = list(V = diag(2), nu = 0.002, fix = 2),
               G=list(G1=list(V=1, nu=0.002,alpha.mu=0, alpha.V=625^2)))
zap <- MCMCglmm(y ~ trait*(X1*X2*X3_nest),
                     random = ~ nested_plot, rcov = ~idh(trait):units,
                     data=zero.dat, family = "zapoisson",  nitt = nitt,
                     burnin = burnin, thin=thin,  prior=prior, pr = 
TRUE, pl = TRUE)
summary(zap)
# No warning and everything looks good

# Poisson model
prior = list( R = list(V = diag(1), nu = 0.002),
               G=list(G1=list(V=1, nu=0.002,alpha.mu=0, alpha.V=625^2)))
pois <- MCMCglmm(y ~ X1*X2*X3_nest,
                 random = ~ nested_plot,
                 data=zero.dat, family = "poisson",  nitt = nitt,
                 burnin = burnin, thin=thin,  prior=prior, pr = TRUE, pl 
= TRUE)
summary(pois)
# No warning

# Some predictions
# by default, all random factors are marginalised
p.zip <- predict(zip,   type="response", posterior = "mean")
p.zap <- predict(zap,  type="response", posterior = "mean")
p.pois <- predict(pois,   type="response", posterior = "mean")
cbind(aggregate(y ~ X1*X2*X3_nest, zero.dat, mean),
       zip = aggregate(p.zip ~ X1*X2*X3_nest, zero.dat, mean)$V1,
       zap = aggregate(p.zap ~ X1*X2*X3_nest, zero.dat, mean)$V1,
       pois = aggregate(p.pois ~ X1*X2*X3_nest, zero.dat, mean)$V1)
# poisson model give extreme (unrealistic) values. zap best I think.


# Predictive testing: zeros

# zip model
oz <- sum(zero.dat == 0)
sim.zi <- simulate(zip, type="response", posterior = "all", nsim=1000)
dist.zeros.zi <- apply(sim.zi, 2, function (x) sum(x==0))
hist(dist.zeros.zi)
abline(v = oz, col = "red")

# zap model
oz <- sum(zero.dat == 0)
sim.za <- simulate(zap, type="response", posterior = "mean", nsim=1000)
dist.zeros.za <- apply(sim.za, 2, function (x) sum(x==0))
hist(dist.zeros.za)
abline(v = oz, col = "red")
# very good prediction of zeros!

# Poisson model
oz <- sum(zero.dat == 0)
sim.pois <- simulate(pois, type="response", posterior = "mean", nsim=1000)
dist.zeros.pois <- apply(sim.pois, 2, function (x) sum(x==0))
hist(dist.zeros.pois, xlim=c(900,1250))
abline(v = oz, col = "red")
# Zero-inflated!

# Simulation from pois model, by hand.
mc.samp <- nrow(pois$VCV)
nz <- 1:mc.samp
oz <- sum(zero.dat == 0)
for (i in 1:mc.samp) {
   pred.l <- rnorm(nrow(zero.dat), (cbind(pois$X,pois$Z) %*% 
pois$Sol[1,])@x, sqrt(pois$VCV[i,]))
   nz[i] <- sum(rpois(nrow(zero.dat), exp(pred.l)) == 0)
}
hist(nz, xlim=c(900,1250))
abline(v = oz, col = "red")


# plot zero distributions
zeroDens <- data.frame(y = c(dist.zeros.zi, dist.zeros.za, 
dist.zeros.pois, sample(nz,1000)),
                        model = rep(c("zi", "za", "pois", 
"pois.byHand"), each=1000))
library(ggplot2)
ggplot(zeroDens,aes(x=y, fill=model)) + geom_density(alpha=0.25) + 
geom_vline(xintercept=oz)
# only ZAP that captures all the zeros. Zip not that bad though.


On 2016-12-01 07:48, Jarrod Hadfield wrote:
> Hi Gustaf,
>
> I don't have dat, so I can't run the final bit of code.
>
> However, these are my thoughts.
>
> 1/ In the zap model you are allowing X1 to X3 to effect the level of 
> zero alteration, whereas in the zip model you are just fitting a 
> constant level  of zero inflation across X1 to X3. In this sense the 
> zap model will provide a superior fit because it has more parameters. 
> The warning message about dropping terms is fine, although the default 
> contrast for the zip model is a bit annoying: I would have preferred 
> the main effect for X1 to be yes rather than no, but I guess its no 
> big deal.
>
> 2/ You have fitted a single nested_plot effect in the random effects. 
> This is a little odd, except in the case where the data are not 
> zero-inflated. In this case having a single nested_plot term in the 
> zap model is equivalent to fitting a nested_plot term in the standard 
> Poisson. If the data are zero-inflated, and/or the model is not a zap 
> model, then the case for having a single term is not well justified. I 
> would use us or idh and in the latter case consider fixing the second 
> variance (associated with zero-inflation/alteration) close to zero.
>
> 3/ The marginal predictions do not take into account the covariances 
> between traits. This is generally OK, but its not ideal when the 
> traits refer to the multiple parameters of a single distribution as 
> with za/zi/hu models. I should put a warning in. You can also use the 
> simulate function on the model and then average over the draws to get 
> the predictions, and this will take into account any covariances. Note 
> that if you use idh(trait):nested_plot there are no covariances so 
> predict should be fine.
>
> Cheers,
>
> Jarrod
>
>
>
>
>
>
> On 30/11/2016 23:30, Gustaf Granath wrote:
>> cbind(aggregate(y ~ X1*X2*X3_nest, zero.dat, mean),
>>       zip = aggregate(p.zip ~ fire*retention*micro_hab.two, dat, 
>> mean)$V1,
>>       zap = aggregate(p.zap ~ fire*retention*micro_hab.two, dat, 
>> mean)$V1,
>>       pois = aggregate(p.pois ~ fire*retention*micro_hab.two, dat, 
>> mean)$V1)
>
>


From j.hadfield at ed.ac.uk  Fri Dec  2 07:52:25 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Fri, 2 Dec 2016 06:52:25 +0000
Subject: [R-sig-ME] zero-inflated models in MCMCglmm
In-Reply-To: <dfb6b645-0e2e-7a3a-42f9-06db9c280c25@gmail.com>
References: <d4a6fd3a-151a-32e0-1fea-12ff7fe48466@gmail.com>
	<a5c89be4-f711-7b25-90de-b38e15002f4d@ed.ac.uk>
	<f8ca8f5f-a6c1-7e35-d94e-25b611fa56d2@gmail.com>
	<49bbc5f5-4576-079c-2108-1f2ce2ba31c2@ed.ac.uk>
	<3c4f809e-bb38-faa4-5417-3fa7fe6b059d@gmail.com>
	<448e2b5c-4826-aa2d-df26-3783cf314c3c@ed.ac.uk>
	<dfb6b645-0e2e-7a3a-42f9-06db9c280c25@gmail.com>
Message-ID: <4fb2e6fd-a75a-863b-ec15-05dfc7bb8bee@ed.ac.uk>

Hi,

The automatic contrasts from model.matrix are sometimes a bit illogical: 
you can just multiply  at.level(trait, 1):X1no by -1 to get 
at.level(trait, 1):X1yes.

simulate is not simulating expectations but new observations. If ~ 
idh(trait):nested_plot is specified in the marginal argument new levels 
of nested_plot are simulated (a new one for each observation) if the 
marginal argument is NULL then the actual levels of nested_plot are used.

To get the expectations as in predict simulate 1000 (for example) draws 
and then take the rowMeans to get an average over the distribution of 
random and residual effects.

Cheers,

Jarrod



On 01/12/2016 22:36, Gustaf Granath wrote:
> Jarrod,
>
> Thanks for explaining this. I now start to understand how these models 
> are set up in MCMCglmm. The weird contrasts seem impossible to get rid 
> of though, and Im still struggling a bit to work out how to combine 
> the effects.
>
> Great idea to use the simulation function. I used it for predictive 
> model checking of zeros, and I do have one question. Does simulate() 
> include (marginalise) the residual error (units)? I get quite 
> different results if I use a "by hand" simulation function (from 
> course notes) and simulate(), for a standard Poisson model.
>
> Cheers
>
> Gustaf
>
> ####### R code
>
> # get test data
> require(MCMCglmm)
> require(RCurl)
> zero.dat 
> <-read.csv(text=getURL("https://raw.githubusercontent.com/ggranath/zero_inf_models/master/test_data.csv"),header=T)
>
> #plots nested in site (X3 treatments at plot-level)
> zero.dat$nested_plot <- factor(with(zero.dat, paste(site, plot, sep= 
> "_")) )
>
> #zip model
> nitt = 10000 #low for testing
> thin = 10 #low for testing
> burnin = 1000 #low for testing
> prior = list( R = list(V = diag(2), nu = 0.002, fix = 2),
>               G=list(G1=list(V=diag(2)*c(1,0.001), nu=0.002, fix=2)))
> zip <- MCMCglmm(y ~ trait -1 + at.level(trait, 1):(X1*X2*X3_nest),
>                 random = ~ idh(trait):nested_plot, rcov = 
> ~idh(trait):units,
>                 data=zero.dat, family = "zipoisson",  nitt = nitt,
>                 burnin = burnin, thin=thin, prior=prior, pr = TRUE, pl 
> = TRUE)
> summary(zip)
> # Gives a warning!! And contrasts are not straight forward to interpret.
>
> #zap model
> prior = list( R = list(V = diag(2), nu = 0.002, fix = 2),
>               G=list(G1=list(V=1, nu=0.002,alpha.mu=0, alpha.V=625^2)))
> zap <- MCMCglmm(y ~ trait*(X1*X2*X3_nest),
>                     random = ~ nested_plot, rcov = ~idh(trait):units,
>                     data=zero.dat, family = "zapoisson",  nitt = nitt,
>                     burnin = burnin, thin=thin,  prior=prior, pr = 
> TRUE, pl = TRUE)
> summary(zap)
> # No warning and everything looks good
>
> # Poisson model
> prior = list( R = list(V = diag(1), nu = 0.002),
>               G=list(G1=list(V=1, nu=0.002,alpha.mu=0, alpha.V=625^2)))
> pois <- MCMCglmm(y ~ X1*X2*X3_nest,
>                 random = ~ nested_plot,
>                 data=zero.dat, family = "poisson",  nitt = nitt,
>                 burnin = burnin, thin=thin,  prior=prior, pr = TRUE, 
> pl = TRUE)
> summary(pois)
> # No warning
>
> # Some predictions
> # by default, all random factors are marginalised
> p.zip <- predict(zip,   type="response", posterior = "mean")
> p.zap <- predict(zap,  type="response", posterior = "mean")
> p.pois <- predict(pois,   type="response", posterior = "mean")
> cbind(aggregate(y ~ X1*X2*X3_nest, zero.dat, mean),
>       zip = aggregate(p.zip ~ X1*X2*X3_nest, zero.dat, mean)$V1,
>       zap = aggregate(p.zap ~ X1*X2*X3_nest, zero.dat, mean)$V1,
>       pois = aggregate(p.pois ~ X1*X2*X3_nest, zero.dat, mean)$V1)
> # poisson model give extreme (unrealistic) values. zap best I think.
>
>
> # Predictive testing: zeros
>
> # zip model
> oz <- sum(zero.dat == 0)
> sim.zi <- simulate(zip, type="response", posterior = "all", nsim=1000)
> dist.zeros.zi <- apply(sim.zi, 2, function (x) sum(x==0))
> hist(dist.zeros.zi)
> abline(v = oz, col = "red")
>
> # zap model
> oz <- sum(zero.dat == 0)
> sim.za <- simulate(zap, type="response", posterior = "mean", nsim=1000)
> dist.zeros.za <- apply(sim.za, 2, function (x) sum(x==0))
> hist(dist.zeros.za)
> abline(v = oz, col = "red")
> # very good prediction of zeros!
>
> # Poisson model
> oz <- sum(zero.dat == 0)
> sim.pois <- simulate(pois, type="response", posterior = "mean", 
> nsim=1000)
> dist.zeros.pois <- apply(sim.pois, 2, function (x) sum(x==0))
> hist(dist.zeros.pois, xlim=c(900,1250))
> abline(v = oz, col = "red")
> # Zero-inflated!
>
> # Simulation from pois model, by hand.
> mc.samp <- nrow(pois$VCV)
> nz <- 1:mc.samp
> oz <- sum(zero.dat == 0)
> for (i in 1:mc.samp) {
>   pred.l <- rnorm(nrow(zero.dat), (cbind(pois$X,pois$Z) %*% 
> pois$Sol[1,])@x, sqrt(pois$VCV[i,]))
>   nz[i] <- sum(rpois(nrow(zero.dat), exp(pred.l)) == 0)
> }
> hist(nz, xlim=c(900,1250))
> abline(v = oz, col = "red")
>
>
> # plot zero distributions
> zeroDens <- data.frame(y = c(dist.zeros.zi, dist.zeros.za, 
> dist.zeros.pois, sample(nz,1000)),
>                        model = rep(c("zi", "za", "pois", 
> "pois.byHand"), each=1000))
> library(ggplot2)
> ggplot(zeroDens,aes(x=y, fill=model)) + geom_density(alpha=0.25) + 
> geom_vline(xintercept=oz)
> # only ZAP that captures all the zeros. Zip not that bad though.
>
>
> On 2016-12-01 07:48, Jarrod Hadfield wrote:
>> Hi Gustaf,
>>
>> I don't have dat, so I can't run the final bit of code.
>>
>> However, these are my thoughts.
>>
>> 1/ In the zap model you are allowing X1 to X3 to effect the level of 
>> zero alteration, whereas in the zip model you are just fitting a 
>> constant level  of zero inflation across X1 to X3. In this sense the 
>> zap model will provide a superior fit because it has more parameters. 
>> The warning message about dropping terms is fine, although the 
>> default contrast for the zip model is a bit annoying: I would have 
>> preferred the main effect for X1 to be yes rather than no, but I 
>> guess its no big deal.
>>
>> 2/ You have fitted a single nested_plot effect in the random effects. 
>> This is a little odd, except in the case where the data are not 
>> zero-inflated. In this case having a single nested_plot term in the 
>> zap model is equivalent to fitting a nested_plot term in the standard 
>> Poisson. If the data are zero-inflated, and/or the model is not a zap 
>> model, then the case for having a single term is not well justified. 
>> I would use us or idh and in the latter case consider fixing the 
>> second variance (associated with zero-inflation/alteration) close to 
>> zero.
>>
>> 3/ The marginal predictions do not take into account the covariances 
>> between traits. This is generally OK, but its not ideal when the 
>> traits refer to the multiple parameters of a single distribution as 
>> with za/zi/hu models. I should put a warning in. You can also use the 
>> simulate function on the model and then average over the draws to get 
>> the predictions, and this will take into account any covariances. 
>> Note that if you use idh(trait):nested_plot there are no covariances 
>> so predict should be fine.
>>
>> Cheers,
>>
>> Jarrod
>>
>>
>>
>>
>>
>>
>> On 30/11/2016 23:30, Gustaf Granath wrote:
>>> cbind(aggregate(y ~ X1*X2*X3_nest, zero.dat, mean),
>>>       zip = aggregate(p.zip ~ fire*retention*micro_hab.two, dat, 
>>> mean)$V1,
>>>       zap = aggregate(p.zap ~ fire*retention*micro_hab.two, dat, 
>>> mean)$V1,
>>>       pois = aggregate(p.pois ~ fire*retention*micro_hab.two, dat, 
>>> mean)$V1)
>>
>>
>


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From kn.journal.news at gmail.com  Fri Dec  2 12:03:13 2016
From: kn.journal.news at gmail.com (Koen Neijenhuijs)
Date: Fri, 2 Dec 2016 12:03:13 +0100
Subject: [R-sig-ME] Using individual differences from model A as predictor
	in model B
Message-ID: <CABL7txS=85Nprzur1KTeYrsVjU3VweNsqc2+W1q6R1nhh8p_Cg@mail.gmail.com>

Dear all,


we've run an experiment with two groups, which we followed for 3 weeks.
Each participant got three trials per week, and our dependent variable is
the adherence, defined as whether they replied to the trial or not. In the
third week, we introduced a manipulation, which was balanced across the two
groups. We want to test the effect of the manipulation, moderated for
intrinsic motivation to adhere to the trials. We are struggling with the
operationalization of intrinsic motivation.

We ran a binomial mixed-effect model on the data of the first two weeks, to
estimate intrinsic motivation. So far, we've come up with three methods to
do so, but each comes with their own concerns. I was hoping to hear your
thoughts on this.

1. The first method is simply to use the aggregated (sum) adherence of each
participant. This method would be seemingly valid, as the model on the
first two weeks shows no main effect of time, group, nor the interaction
time*group. However, I am reluctant to go this route as this method is less
detailed than the other options.

2. The second method is to extract the random-adjusted intercept and
random-adjusted slope of time (random effects + fixed effects), per
participant. The interaction of these two represent intrinsic motivation as
it inherits both the intercept of adherence as well as its' development
over time; this combination is capable of representing every possible
motivation timeline (start high and go lower over time; start high and stay
high over time; start low and go up over time; etc). However, using this
method, to test the effect we're interested in will result in a three-way
interaction (intercept*slope*manipulation), and a four-way interaction to
check moderation of prior group characteristics. It is unlikely we have
enough power to test this, as our sample size is limited.

3. The third method is to extract the prediction equation from the model of
the first two weeks and apply this to the data of the third week. This
method will give us one representation of motivation instead of two, which
does include both fixed and random effects. However, as the method is
applied to data of the third week, I am uncertain whether it is valid as a
representation of intrinsic motivation over the first two weeks.

Sorry for the long wall of text. What are your thoughts on this? Are there
other ways of operationalizing individual differences on adherence in the
first two weeks to use as an independent variable on adherence in the third
week?

Cheers,

Koen

	[[alternative HTML version deleted]]


From ahmatias at gmail.com  Fri Dec  2 12:20:54 2016
From: ahmatias at gmail.com (Toni Hernandez-Matias)
Date: Fri, 2 Dec 2016 12:20:54 +0100
Subject: [R-sig-ME] Mixed Models convergence problems, Jordi Rosich
In-Reply-To: <CAJ6PopwVkj5KzX379Gu0mTJtZSDp6yv9NT30GVdj7ZKJsK5MiQ@mail.gmail.com>
References: <CAJ6PopzBUtMaO5pLh+h0rPryk63fwFdfsHufCObKZ+neOXiYpw@mail.gmail.com>
	<1479187148.2575.10.camel@unisa.edu.au>
	<CAJ6PopwVkj5KzX379Gu0mTJtZSDp6yv9NT30GVdj7ZKJsK5MiQ@mail.gmail.com>
Message-ID: <CA+hwERka8B0g8E+xXeMY8TuPEhsMbaaBQH=NRZ_Nw5NT3PssWw@mail.gmail.com>

Dear all,

Could some one help us with this problem of convergence?

Our alternative is only using 1 observation per territory and then using
glm instead of glmm, but we will miss a lot of info.

Many thanks!

Toni

On Tue, Nov 22, 2016 at 3:20 PM, Jordi Rosich <jordirosich16 at gmail.com>
wrote:

> Thank you very much for your answer,
>
>
> We have checked some of the possible solutions you proposed as well as the
> ?convergence help page:
>
>
>
>
> -Center and scale predictor variables didn?t solve the convergence problems
> because our variables are components obtained in a PCA analysis and are
> ?already centered?.
>
>
> -Increasing the number of iterations of the optimizer didn?t solve the
> convergence problem in all the different models.
>
> -We didn?t check singularity or recompute gradient and Hessian with
> Richardson extrapolation as we got unexpected values in some parts of
> the process or didn?t fully understand how the process works.
>
> -We tried to run the models with different optimizers as explained in the
> ?convergence page and got the attached output. It seems that changing the
> optimizers didn't
>
>
>
>
>
> One of the reasons we think our models could continue to fail is because in
> our data (see previous mail for details) the levels of the random factor
> Territory totally explain if a tree/nest-site/landscape is occupied or
> unoccupied. For exemple, in Territory 1 all four trees/nest-site
> forests/landscapes are occupied; in Territory 15 all three trees/nest-site
> forests/landscapes are unoccupied. This happens with all our territories,
> all nests/"no-nests" of a territory are either occupied or unoccupied.
> Could this situation be problematic when estimating the random factor
> variance and thus, is there a possibility that the convergence problem has
> a relation with this fact?
>
>
> Thank you in advance. Waiting for your answer,
>
>
>
> Jordi Rosich
>
> 2016-11-15 6:19 GMT+01:00 Phillip Alday <Phillip.Alday at unisa.edu.au>:
>
> > Hi Jordi,
> >
> > Without really knowing anything about your data (or more generally
> > types of data common to your field) ....
> >
> > - Your model doesn't seem exceptionally complex -- just main effects
> > and a single scalar (intercept-only) random effect. Of course, a simple
> > model can still be "too" complex if you don't have much data.
> >
> > - However, it sometimes makes sense to use a more complicated model
> > when you have convergence issues on a simple model -- sometimes, you
> > really do need covariates to get any type of decent fit. (Based on your
> > email, you may have already  experienced this.)
> >
> > - Categorical variables are particularly 'nasty' when it comes to the
> > number of model parameters as a categorical variable with n levels
> > requires n-1 parameters in the model! Continuous variables only require
> > 1 parameter apiece (correlation parameters in the random effects
> > excepted).
> >
> > - Watch out for multicollinearity -- how strongly do tree height and
> > tree width correlate with each other?
> >
> > - Your particular convergence warning often means that the optimiser
> > was still moving along towards convergence / the solution when
> > optimisation was stoppe. Sometimes this can be helped by just
> > increasing the number of iterations that the optimiser is allowed to
> > take, although this will increase computer time.
> >
> > - Make sure to check out the help page: ?convergence (after loading
> > lme4) has many tips and tricks.
> >
> > Best,
> > Phillip
> >
> > On Mon, 2016-11-14 at 22:33 +0100, Jordi Rosich wrote:
> > > Hello,
> > >
> > >
> > >
> > > I'm Jordi Rosich, a student currently collaborating with the Biology
> > > Conservation Group of the University of Barcelona. I'm writing you
> > > because
> > > I'm having model convergence troubles with some GLMMs using the
> > > function
> > > glmer of the package lme4 of R.
> > >
> > > Our research addresses nest-site selection of the Goshawk, a
> > > territorial
> > > bird of prey, and specifically we aim to understand which
> > > environmental
> > > variables are relevant for nest site-selection in this species.
> > >
> > >
> > >
> > >
> > > *Our approach**:*
> > >
> > > 1)     We sampled several environmental variables in sites used by
> > > this
> > > bird species in nest (1) and unoccupied sites (0), and therefore
> > > occupation
> > > status (0/1) is our dependent variable and the environmental
> > > variables our
> > > independent variables.
> > >
> > > 2)     We performed  three analysis at 3 different spatial-scales:
> > > nest
> > > tree, nest-site forest (being the 18 meters radius circular area
> > > around the
> > > nest-tree), and landscape around nest-site (500 meters radius
> > > circular area
> > > around the nest-tree).
> > >
> > > 3)     We sampled 29 Goshawk nests comprised in 13 breeding pairs
> > > territories (each territory may hold several nests) and 30 control
> > > non-occupied random trees comprised in 25 "pseudo-territories" (the
> > > near
> > > trees being included in this "territories"). To avoid
> > > pseudoreplication of
> > > nests of the same breeding pair (or territory) we have considered the
> > > factor "Territory" as our random factor in the mixed models. The
> > > model
> > > definition is approximately Y = explanatory variables +
> > > (1|id  Territory),
> > > where Y is the occupation status (0/1) (See an example below).
> > >
> > > 4)     Our independent variables are both categorical (e.g. tree
> > > species;
> > > aspect: an angle recoded into 4 categories) and continuous
> > > (components
> > > resulting of a PCA on several original environmental variables,
> > > performed
> > > to reduce the number of original variables). For more details for
> > > each
> > > analysis:
> > >
> > >
> > > -Nest-tree scale: 2 continuous variables (FAC1TreeHeight,
> > > FAC2TreeWidth)
> > > and 1 four level categorical variable (TreeSpecies).
> > >
> > >
> > >
> > > -Nest-site forest scale: 4 continuous variables
> > > (FAC1BroadLeavedTrees,
> > > FAC2YoungPines, FAC3MaturePines, FAC4SlopeAndShrubs) and 1 four level
> > > categorical variable (ForestAspect).
> > >
> > >
> > >
> > > -Landscape scale: 3 continuous variables (FAC1PinusVSQuercus,
> > > FAC2HumanizedLand, FAC3DistanceToRoads).
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > > One example of the script used to model would be:
> > >
> > >
> > >
> > > mod1 <-
> > > glmer(Occupation~FAC1Treeheight+FAC2TreeWidth+Sp+(1|Territory),
> > > family=binomial, data=trees)
> > >
> > >
> > >
> > >
> > >
> > > *The problem: *
> > >
> > >
> > >
> > > While creating the candidate models with glmer function to later
> > > select the
> > > best ones by their AICcs, we've faced some warnings telling that some
> > > of
> > > the models are failing to converge:
> > >
> > >
> > >
> > > In checkConv(attr(opt, "derivs"), opt$par, ctrl =
> > > control$checkConv,  :
> > >
> > >   Model failed to converge with max|grad| = 0.0483015 (tol = 0.001,
> > > component 1)
> > >
> > >
> > >
> > > It happens specially, although not always, with the models with more
> > > parameters and also with those containing categorical variables.
> > >
> > >
> > >
> > >
> > > *My question: *
> > >
> > >
> > >
> > > Given our variables, random factor and data is there any particular
> > > reason
> > > why our models could fail to converge? If so, is there a possible
> > > solution
> > > to the convergence problem?
> > >
> > >
> > >
> > >
> > >
> > > Thank you very much in advance. Waiting for your answer,
> > >
> > >
> > > Jordi Rosich
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > _______________________________________________
> > > R-sig-mixed-models at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>



-- 
*********************************************************

Antonio Hernandez Matias

Equip de Biologia de la Conservaci?
Departament de Biologia Evolutiva, Ecolog?a i Ci?ncies Ambientals
Facultat de Biologia  i Institut de Recerca de la Biodiversitat (IRBio)
Universitat de Barcelona (UB)
Av. Diagonal, 643
Barcelona      08028
Spain
Telephone: +34-934035857
FAX: +34-934035740
e-mail: ahernandezmatias at ub.edu

***********************************************************

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Fri Dec  2 13:23:23 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Fri, 2 Dec 2016 13:23:23 +0100
Subject: [R-sig-ME] Using individual differences from model A as
 predictor in model B
In-Reply-To: <CABL7txS=85Nprzur1KTeYrsVjU3VweNsqc2+W1q6R1nhh8p_Cg@mail.gmail.com>
References: <CABL7txS=85Nprzur1KTeYrsVjU3VweNsqc2+W1q6R1nhh8p_Cg@mail.gmail.com>
Message-ID: <CAJuCY5zXAMSCh1VMduDDioCvqDPjMur-Ekh=xCo97rmL2iMzuw@mail.gmail.com>

Dear Koen,

I think you can fit this in a single model. Here a a few options:

with lme4:
Adherence ~ BeforeAfter * Treatment + (1 + BeforeAfter|Participant)
Adherence ~ BeforeAfter * Treatment + (1| Participant:BeforeAfter)

with INLA:
Adherence ~ BeforeAfter * Treatment + f(Time, model = "rw1", replicate =
Participant)

The BeforeAfter:Treatment interaction is the effect you are interested in.
The lme4 random effect allow for an additional treatment effect for
individual participants. The INLA random effect allows for correlated
random intercept along Time for the individual participant. rw1 stands for
random walk of order 1, which models the differences between consecutive
time points.

Best regards,


ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-12-02 12:03 GMT+01:00 Koen Neijenhuijs <kn.journal.news at gmail.com>:

> Dear all,
>
>
> we've run an experiment with two groups, which we followed for 3 weeks.
> Each participant got three trials per week, and our dependent variable is
> the adherence, defined as whether they replied to the trial or not. In the
> third week, we introduced a manipulation, which was balanced across the two
> groups. We want to test the effect of the manipulation, moderated for
> intrinsic motivation to adhere to the trials. We are struggling with the
> operationalization of intrinsic motivation.
>
> We ran a binomial mixed-effect model on the data of the first two weeks, to
> estimate intrinsic motivation. So far, we've come up with three methods to
> do so, but each comes with their own concerns. I was hoping to hear your
> thoughts on this.
>
> 1. The first method is simply to use the aggregated (sum) adherence of each
> participant. This method would be seemingly valid, as the model on the
> first two weeks shows no main effect of time, group, nor the interaction
> time*group. However, I am reluctant to go this route as this method is less
> detailed than the other options.
>
> 2. The second method is to extract the random-adjusted intercept and
> random-adjusted slope of time (random effects + fixed effects), per
> participant. The interaction of these two represent intrinsic motivation as
> it inherits both the intercept of adherence as well as its' development
> over time; this combination is capable of representing every possible
> motivation timeline (start high and go lower over time; start high and stay
> high over time; start low and go up over time; etc). However, using this
> method, to test the effect we're interested in will result in a three-way
> interaction (intercept*slope*manipulation), and a four-way interaction to
> check moderation of prior group characteristics. It is unlikely we have
> enough power to test this, as our sample size is limited.
>
> 3. The third method is to extract the prediction equation from the model of
> the first two weeks and apply this to the data of the third week. This
> method will give us one representation of motivation instead of two, which
> does include both fixed and random effects. However, as the method is
> applied to data of the third week, I am uncertain whether it is valid as a
> representation of intrinsic motivation over the first two weeks.
>
> Sorry for the long wall of text. What are your thoughts on this? Are there
> other ways of operationalizing individual differences on adherence in the
> first two weeks to use as an independent variable on adherence in the third
> week?
>
> Cheers,
>
> Koen
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From kn.journal.news at gmail.com  Fri Dec  2 13:31:32 2016
From: kn.journal.news at gmail.com (Koen Neijenhuijs)
Date: Fri, 2 Dec 2016 13:31:32 +0100
Subject: [R-sig-ME] Using individual differences from model A as
 predictor in model B
In-Reply-To: <CAJuCY5zXAMSCh1VMduDDioCvqDPjMur-Ekh=xCo97rmL2iMzuw@mail.gmail.com>
References: <CABL7txS=85Nprzur1KTeYrsVjU3VweNsqc2+W1q6R1nhh8p_Cg@mail.gmail.com>
	<CAJuCY5zXAMSCh1VMduDDioCvqDPjMur-Ekh=xCo97rmL2iMzuw@mail.gmail.com>
Message-ID: <CABL7txRspxdUWM_p6MjFdR0WZRg+48V8yZJOgQ4pBJiPw91s4w@mail.gmail.com>

Dear Thierry,

thanks for the quick response! That would be the easiest model, but I'm not
entirely sure whether this model represents motivation the way we envision
it. In essence what you propose is a model where we test whether adherence
has changed from week1&week2 to week3, and whether this change is different
for the two different groups. However, this model does not have a concrete
moderation of the individual differences inside of the fixed effect, which
is what we're after. In essence, the research question isn't so much about
the two different groups (the groups merely exist to balance the experiment
itself, and is thus a control variable), but what the effect of the
manipulation is for participants of different intrinsic motivations. Our
problem is that intrinsic motivation is inherently intertwined with the
dependent variable, and while putting time in the model as you propose is
one way to approach the question, the interaction of BeforeAfter*Treatment
doesn't allow us to disentagle the question regarding motivation.

Kind regards,

Koen

2016-12-02 13:23 GMT+01:00 Thierry Onkelinx <thierry.onkelinx at inbo.be>:

> Dear Koen,
>
> I think you can fit this in a single model. Here a a few options:
>
> with lme4:
> Adherence ~ BeforeAfter * Treatment + (1 + BeforeAfter|Participant)
> Adherence ~ BeforeAfter * Treatment + (1| Participant:BeforeAfter)
>
> with INLA:
> Adherence ~ BeforeAfter * Treatment + f(Time, model = "rw1", replicate =
> Participant)
>
> The BeforeAfter:Treatment interaction is the effect you are interested in.
> The lme4 random effect allow for an additional treatment effect for
> individual participants. The INLA random effect allows for correlated
> random intercept along Time for the individual participant. rw1 stands for
> random walk of order 1, which models the differences between consecutive
> time points.
>
> Best regards,
>
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-12-02 12:03 GMT+01:00 Koen Neijenhuijs <kn.journal.news at gmail.com>:
>
>> Dear all,
>>
>>
>> we've run an experiment with two groups, which we followed for 3 weeks.
>> Each participant got three trials per week, and our dependent variable is
>> the adherence, defined as whether they replied to the trial or not. In the
>> third week, we introduced a manipulation, which was balanced across the
>> two
>> groups. We want to test the effect of the manipulation, moderated for
>> intrinsic motivation to adhere to the trials. We are struggling with the
>> operationalization of intrinsic motivation.
>>
>> We ran a binomial mixed-effect model on the data of the first two weeks,
>> to
>> estimate intrinsic motivation. So far, we've come up with three methods to
>> do so, but each comes with their own concerns. I was hoping to hear your
>> thoughts on this.
>>
>> 1. The first method is simply to use the aggregated (sum) adherence of
>> each
>> participant. This method would be seemingly valid, as the model on the
>> first two weeks shows no main effect of time, group, nor the interaction
>> time*group. However, I am reluctant to go this route as this method is
>> less
>> detailed than the other options.
>>
>> 2. The second method is to extract the random-adjusted intercept and
>> random-adjusted slope of time (random effects + fixed effects), per
>> participant. The interaction of these two represent intrinsic motivation
>> as
>> it inherits both the intercept of adherence as well as its' development
>> over time; this combination is capable of representing every possible
>> motivation timeline (start high and go lower over time; start high and
>> stay
>> high over time; start low and go up over time; etc). However, using this
>> method, to test the effect we're interested in will result in a three-way
>> interaction (intercept*slope*manipulation), and a four-way interaction to
>> check moderation of prior group characteristics. It is unlikely we have
>> enough power to test this, as our sample size is limited.
>>
>> 3. The third method is to extract the prediction equation from the model
>> of
>> the first two weeks and apply this to the data of the third week. This
>> method will give us one representation of motivation instead of two, which
>> does include both fixed and random effects. However, as the method is
>> applied to data of the third week, I am uncertain whether it is valid as a
>> representation of intrinsic motivation over the first two weeks.
>>
>> Sorry for the long wall of text. What are your thoughts on this? Are there
>> other ways of operationalizing individual differences on adherence in the
>> first two weeks to use as an independent variable on adherence in the
>> third
>> week?
>>
>> Cheers,
>>
>> Koen
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>
>

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Fri Dec  2 14:20:03 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Fri, 2 Dec 2016 14:20:03 +0100
Subject: [R-sig-ME] Using individual differences from model A as
 predictor in model B
In-Reply-To: <CABL7txRspxdUWM_p6MjFdR0WZRg+48V8yZJOgQ4pBJiPw91s4w@mail.gmail.com>
References: <CABL7txS=85Nprzur1KTeYrsVjU3VweNsqc2+W1q6R1nhh8p_Cg@mail.gmail.com>
	<CAJuCY5zXAMSCh1VMduDDioCvqDPjMur-Ekh=xCo97rmL2iMzuw@mail.gmail.com>
	<CABL7txRspxdUWM_p6MjFdR0WZRg+48V8yZJOgQ4pBJiPw91s4w@mail.gmail.com>
Message-ID: <CAJuCY5zV55tqYPjTjfMpD50ZijbGD0L9Y2OKnT1S4UKVQ42tQA@mail.gmail.com>

Dear Koen,

Have you tried writing down the model you envision as a (set of)
mathematical expressions? This can help to better understand what you want
and how you can fit it.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-12-02 13:31 GMT+01:00 Koen Neijenhuijs <kn.journal.news at gmail.com>:

> Dear Thierry,
>
> thanks for the quick response! That would be the easiest model, but I'm
> not entirely sure whether this model represents motivation the way we
> envision it. In essence what you propose is a model where we test whether
> adherence has changed from week1&week2 to week3, and whether this change is
> different for the two different groups. However, this model does not have a
> concrete moderation of the individual differences inside of the fixed
> effect, which is what we're after. In essence, the research question isn't
> so much about the two different groups (the groups merely exist to balance
> the experiment itself, and is thus a control variable), but what the effect
> of the manipulation is for participants of different intrinsic motivations.
> Our problem is that intrinsic motivation is inherently intertwined with the
> dependent variable, and while putting time in the model as you propose is
> one way to approach the question, the interaction of BeforeAfter*Treatment
> doesn't allow us to disentagle the question regarding motivation.
>
> Kind regards,
>
> Koen
>
> 2016-12-02 13:23 GMT+01:00 Thierry Onkelinx <thierry.onkelinx at inbo.be>:
>
>> Dear Koen,
>>
>> I think you can fit this in a single model. Here a a few options:
>>
>> with lme4:
>> Adherence ~ BeforeAfter * Treatment + (1 + BeforeAfter|Participant)
>> Adherence ~ BeforeAfter * Treatment + (1| Participant:BeforeAfter)
>>
>> with INLA:
>> Adherence ~ BeforeAfter * Treatment + f(Time, model = "rw1", replicate =
>> Participant)
>>
>> The BeforeAfter:Treatment interaction is the effect you are interested
>> in. The lme4 random effect allow for an additional treatment effect for
>> individual participants. The INLA random effect allows for correlated
>> random intercept along Time for the individual participant. rw1 stands for
>> random walk of order 1, which models the differences between consecutive
>> time points.
>>
>> Best regards,
>>
>>
>> ir. Thierry Onkelinx
>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>> and Forest
>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>> Kliniekstraat 25
>> 1070 Anderlecht
>> Belgium
>>
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> 2016-12-02 12:03 GMT+01:00 Koen Neijenhuijs <kn.journal.news at gmail.com>:
>>
>>> Dear all,
>>>
>>>
>>> we've run an experiment with two groups, which we followed for 3 weeks.
>>> Each participant got three trials per week, and our dependent variable is
>>> the adherence, defined as whether they replied to the trial or not. In
>>> the
>>> third week, we introduced a manipulation, which was balanced across the
>>> two
>>> groups. We want to test the effect of the manipulation, moderated for
>>> intrinsic motivation to adhere to the trials. We are struggling with the
>>> operationalization of intrinsic motivation.
>>>
>>> We ran a binomial mixed-effect model on the data of the first two weeks,
>>> to
>>> estimate intrinsic motivation. So far, we've come up with three methods
>>> to
>>> do so, but each comes with their own concerns. I was hoping to hear your
>>> thoughts on this.
>>>
>>> 1. The first method is simply to use the aggregated (sum) adherence of
>>> each
>>> participant. This method would be seemingly valid, as the model on the
>>> first two weeks shows no main effect of time, group, nor the interaction
>>> time*group. However, I am reluctant to go this route as this method is
>>> less
>>> detailed than the other options.
>>>
>>> 2. The second method is to extract the random-adjusted intercept and
>>> random-adjusted slope of time (random effects + fixed effects), per
>>> participant. The interaction of these two represent intrinsic motivation
>>> as
>>> it inherits both the intercept of adherence as well as its' development
>>> over time; this combination is capable of representing every possible
>>> motivation timeline (start high and go lower over time; start high and
>>> stay
>>> high over time; start low and go up over time; etc). However, using this
>>> method, to test the effect we're interested in will result in a three-way
>>> interaction (intercept*slope*manipulation), and a four-way interaction
>>> to
>>> check moderation of prior group characteristics. It is unlikely we have
>>> enough power to test this, as our sample size is limited.
>>>
>>> 3. The third method is to extract the prediction equation from the model
>>> of
>>> the first two weeks and apply this to the data of the third week. This
>>> method will give us one representation of motivation instead of two,
>>> which
>>> does include both fixed and random effects. However, as the method is
>>> applied to data of the third week, I am uncertain whether it is valid as
>>> a
>>> representation of intrinsic motivation over the first two weeks.
>>>
>>> Sorry for the long wall of text. What are your thoughts on this? Are
>>> there
>>> other ways of operationalizing individual differences on adherence in the
>>> first two weeks to use as an independent variable on adherence in the
>>> third
>>> week?
>>>
>>> Cheers,
>>>
>>> Koen
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From David.Duffy at qimrberghofer.edu.au  Sun Dec  4 05:58:29 2016
From: David.Duffy at qimrberghofer.edu.au (David Duffy)
Date: Sun, 4 Dec 2016 04:58:29 +0000
Subject: [R-sig-ME] Mixed Models convergence problems, Jordi Rosich
In-Reply-To: <CA+hwERka8B0g8E+xXeMY8TuPEhsMbaaBQH=NRZ_Nw5NT3PssWw@mail.gmail.com>
References: <CAJ6PopzBUtMaO5pLh+h0rPryk63fwFdfsHufCObKZ+neOXiYpw@mail.gmail.com>
	<1479187148.2575.10.camel@unisa.edu.au>
	<CAJ6PopwVkj5KzX379Gu0mTJtZSDp6yv9NT30GVdj7ZKJsK5MiQ@mail.gmail.com>,
	<CA+hwERka8B0g8E+xXeMY8TuPEhsMbaaBQH=NRZ_Nw5NT3PssWw@mail.gmail.com>
Message-ID: <4737E17E7C8C3C4A8B5C1CE5346371D48A67B403@EXCH06S.adqimr.ad.lan>

> [In] all our territories, all nests/"no-nests" of a territory are either occupied or unoccupied.

Given your controls are unoccupied trees from within each territory, did a conditional logistic regression work (with territory as stratum indicator)? You need to either provide a data set (not too big!), or speak to a local statistician.

From dp at dparis.com  Mon Dec  5 05:29:12 2016
From: dp at dparis.com (Dena Paris)
Date: Mon, 5 Dec 2016 15:29:12 +1100
Subject: [R-sig-ME] parameter estimates for all factor levels MCMCglmm
Message-ID: <C0C14536-B315-4669-A816-D80D94E723B8@dparis.com>

Hi all,

I'm new to Bayesian stats and the MCMCglmm package. I'm trying to understand how to interpret the results from a fitted model. I've fitted a model with a binomial response (reject/select), two categorical fixed effects (taxon with 16 levels, and size class with 3 levels), and a single random effect (bird ID). My model is mixing well and the results fit the data. My problem is that I'm not getting estimates for all levels of the fixed effects (19). How do I get the post mean and CIs for all levels in order to correctly interpret/write up the results?

As I have (near) complete separation in the data, I've used the fixed effect prior structure suggested in the Course Notes, fixed the residuals, and removed the global intercept.

prior.1 = list(
  B = list(mu = rep(0, 18), V = (diag(18)) * (1 + pi^2/3)),
  R = list(fix=1, V=1, n = k - 1),
  G = list(G1 = list(V = 1, n = 1))
)?

m.1 <- MCMCglmm(Selected ~ -1 + Arthropod + Size, random = ~bID, family = "categorical", prior = prior.1, data = type.selected, verbose = FALSE, nitt = 5e+05, burnin = 5000, thin = 100)

Thank you for your guidance,
Dena


Dena Paris

-----
Dena Paris
Ph.D Candidate
School of Environmental Sciences
Institute for Land, Water and Society
Charles Sturt University
PO Box 789
Albury NSW 2640
M: +61 424 451 858?


	[[alternative HTML version deleted]]


From j.hadfield at ed.ac.uk  Mon Dec  5 07:49:55 2016
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Mon, 5 Dec 2016 06:49:55 +0000
Subject: [R-sig-ME] parameter estimates for all factor levels MCMCglmm
In-Reply-To: <C0C14536-B315-4669-A816-D80D94E723B8@dparis.com>
References: <C0C14536-B315-4669-A816-D80D94E723B8@dparis.com>
Message-ID: <5c661daf-cad1-e380-6700-4a9d1bf3c819@ed.ac.uk>

Hi,

You have an intercept which will be the estimate for the first level of 
Arthropod and Size, the remaining effects are deviations from these 
levels (15 and 2 contrasts respectively).

Cheers,

Jarrod



On 05/12/2016 04:29, Dena Paris wrote:
> Hi all,
>
> I'm new to Bayesian stats and the MCMCglmm package. I'm trying to understand how to interpret the results from a fitted model. I've fitted a model with a binomial response (reject/select), two categorical fixed effects (taxon with 16 levels, and size class with 3 levels), and a single random effect (bird ID). My model is mixing well and the results fit the data. My problem is that I'm not getting estimates for all levels of the fixed effects (19). How do I get the post mean and CIs for all levels in order to correctly interpret/write up the results?
>
> As I have (near) complete separation in the data, I've used the fixed effect prior structure suggested in the Course Notes, fixed the residuals, and removed the global intercept.
>
> prior.1 = list(
>    B = list(mu = rep(0, 18), V = (diag(18)) * (1 + pi^2/3)),
>    R = list(fix=1, V=1, n = k - 1),
>    G = list(G1 = list(V = 1, n = 1))
> )?
>
> m.1 <- MCMCglmm(Selected ~ -1 + Arthropod + Size, random = ~bID, family = "categorical", prior = prior.1, data = type.selected, verbose = FALSE, nitt = 5e+05, burnin = 5000, thin = 100)
>
> Thank you for your guidance,
> Dena
>
>
> Dena Paris
>
> -----
> Dena Paris
> Ph.D Candidate
> School of Environmental Sciences
> Institute for Land, Water and Society
> Charles Sturt University
> PO Box 789
> Albury NSW 2640
> M: +61 424 451 858?
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From elisabeth.schubach at uni-jena.de  Mon Dec  5 11:20:25 2016
From: elisabeth.schubach at uni-jena.de (Elisabeth Schubach)
Date: Mon, 5 Dec 2016 11:20:25 +0100
Subject: [R-sig-ME] variable-and person-centered approaches and multilevel
	models
Message-ID: <009801d24ee1$31d02b60$95708220$@uni-jena.de>

Dear all,
I am currently trying to figure out which parts of a multilevel model are
person-centered and which ones are variable-centered.
I have a three-level structure, with assessment points (Level 1) nested
in relationships (Level 2) nested in individuals (Level 3). And I model
closeness to relationship partner r of individual i at time t as a function
of PERSSONALITY-withinti 
(i.e., a time-varying covariate that was centered within individuals,
representing within-person deviations from an
individual's own mean level across time. To account for between-person
differences in the
respective PERSONALITY trait, I included personal mean (i.e., mean across
time points for
each specific individual, centered on the grand mean) as a time-invariant
covariate. I now wonder whether the within part is person-centered and
whether the between-part is variable-centered or whether 
the parameters ?0 and ?1 are variable-centered (because they represent
average associations) and the parameters b0i, b0ri, b1ri are person-centered
(because they are specific for each individual).
I would be very happy if you can tell me which specific parts in mlm are
person-centered and which ones variable-centered.
Best,
Elisabeth



CLOSENESS tri = (?0 + b0i + b0ri) 
	+ ?1 (TIMEtri)
	+ (?1 + b1ri)  (PERSONALITY-withinti) 
	+ ?2 (PERSONALITY-betweeni)
	+?tri

 with i = individual i
        r = relationship partner r 
        t = time t


From bates at stat.wisc.edu  Mon Dec  5 23:55:42 2016
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 05 Dec 2016 22:55:42 +0000
Subject: [R-sig-ME] Notebooks on some topics in mixed-effects modeling
Message-ID: <CAO7JsnQ=matXA-vuay5Vzq82yKrf8UkftPiLKb38rDfunqNxug@mail.gmail.com>

I have started writing notebooks on some aspects of mixed-effects modeling
that I find are sometimes misunderstood.  The notebooks are in

https://github.com/dmbates/MixedModelsinJulia

The first notebook is on singular estimates of the covariance of
vector-valued random effects.

It can be a bit hit-or-miss for some of the plots in the display directly
from github.com.  If you have jupyter installed (see https://jupyter.org)
you should be able to view the notebooks in a Jupyter session.

An alternative is to establish an account on https://juliabox.com and,
under the "Files" tab, link the github repository.  Then open the notebook
in the Jupyter tab.

	[[alternative HTML version deleted]]


From dp at dparis.com  Tue Dec  6 03:00:36 2016
From: dp at dparis.com (Dena Paris)
Date: Tue, 6 Dec 2016 13:00:36 +1100
Subject: [R-sig-ME] parameter estimates for all factor levels MCMCglmm
In-Reply-To: <5c661daf-cad1-e380-6700-4a9d1bf3c819@ed.ac.uk>
References: <C0C14536-B315-4669-A816-D80D94E723B8@dparis.com>
	<5c661daf-cad1-e380-6700-4a9d1bf3c819@ed.ac.uk>
Message-ID: <D1E36190-7A92-48A8-9C1D-478D6F7CB7C5@dparis.com>

Hi Jarrod,

Thanks for the very prompt reply! I?m still struggling with this a bit. My understanding was that by removing the global intercept (-1) all factor levels would be estimated, and if I have only one fixed effect, say Arthropod, this is exactly what happens. However, when I include the second fixed effect, Size, it returns estimates for all 16 levels of Arthropod, but estimates for only 2 of the 3 Size classes. So, in the case with two fixed effects and the global intercept removed, is there a way to calculate the estimate for the third Size class, or is it the level from which the other levels deviate? 

Thanks again for all your help.

Dena

> On 5 Dec. 2016, at 5:49 pm, Jarrod Hadfield <j.hadfield at ed.ac.uk> wrote:
> 
> Hi,
> 
> You have an intercept which will be the estimate for the first level of Arthropod and Size, the remaining effects are deviations from these levels (15 and 2 contrasts respectively).
> 
> Cheers,
> 
> Jarrod
> 
> 
> 
> On 05/12/2016 04:29, Dena Paris wrote:
>> Hi all,
>> 
>> I'm new to Bayesian stats and the MCMCglmm package. I'm trying to understand how to interpret the results from a fitted model. I've fitted a model with a binomial response (reject/select), two categorical fixed effects (taxon with 16 levels, and size class with 3 levels), and a single random effect (bird ID). My model is mixing well and the results fit the data. My problem is that I'm not getting estimates for all levels of the fixed effects (19). How do I get the post mean and CIs for all levels in order to correctly interpret/write up the results?
>> 
>> As I have (near) complete separation in the data, I've used the fixed effect prior structure suggested in the Course Notes, fixed the residuals, and removed the global intercept.
>> 
>> prior.1 = list(
>>   B = list(mu = rep(0, 18), V = (diag(18)) * (1 + pi^2/3)),
>>   R = list(fix=1, V=1, n = k - 1),
>>   G = list(G1 = list(V = 1, n = 1))
>> )?
>> 
>> m.1 <- MCMCglmm(Selected ~ -1 + Arthropod + Size, random = ~bID, family = "categorical", prior = prior.1, data = type.selected, verbose = FALSE, nitt = 5e+05, burnin = 5000, thin = 100)
>> 
>> Thank you for your guidance,
>> Dena
>> 
>> 
>> Dena Paris
>> 
>> -----
>> Dena Paris
>> Ph.D Candidate
>> School of Environmental Sciences
>> Institute for Land, Water and Society
>> Charles Sturt University
>> PO Box 789
>> Albury NSW 2640
>> M: +61 424 451 858?
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> 
> -- 
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
> 


From conor.goold at nmbu.no  Tue Dec  6 09:02:56 2016
From: conor.goold at nmbu.no (Conor Michael Goold)
Date: Tue, 6 Dec 2016 08:02:56 +0000
Subject: [R-sig-ME] parameter estimates for all factor levels MCMCglmm
In-Reply-To: <D1E36190-7A92-48A8-9C1D-478D6F7CB7C5@dparis.com>
References: <C0C14536-B315-4669-A816-D80D94E723B8@dparis.com>
	<5c661daf-cad1-e380-6700-4a9d1bf3c819@ed.ac.uk>,
	<D1E36190-7A92-48A8-9C1D-478D6F7CB7C5@dparis.com>
Message-ID: <1481011376552.14674@nmbu.no>

Hi Dena, 

I personally find removing the intercept without pre-defining how the categorical predictors are coded is confusing. 

It might be easiest if you set up the contrasts you want to look at before running the model, then you can keep track of exactly what the output is showing. A great webpage for different types of coding for categorical predictors is http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm. 

If you are running a large model (e.g. many predictors/levels of categorical variables), or a model that you may repeat with different data, I would set up the design matrix of predictors first using the model.matrix() function, within which you can specify a list of contrast arguments (see contrast.arg in the model.matrix() function). Then you can plug that into the regression formula. In the long run, I've found it saves a lot of time doing it this way. As a simple example:

#==========================================================================

require(nlme)
data("Machines")
d <- Machines

# set predictors as factor variables
d[,c("Machine","Worker")] <- apply(d[,c("Machine","Worker")], 2, as.factor)

# define model matrix, coding predictors as deflections from their reference categories
designMatrix <- model.matrix(~ Machine + Worker, data = d, 
                             contrasts.arg = list(Machine = "contr.treatment", Worker = "contr.treatment"))

# glm model, using Worker as a fixed effect rather than a random effect
# remove intercept because it is already included in designMatrix. Alternatively, remove the intercept column from the design matrix 
summary(fit <- glm(d$score ~ -1 + designMatrix))

#==========================================================================

You can then post-process the fitted model object as you like. 

Hope that helps a little.

Best regards
Conor Goold
PhD Student
Phone:        +47 67 23 27 24



Norwegian University of Life Sciences
Campus ?s. www.nmbu.no

________________________________________
From: R-sig-mixed-models <r-sig-mixed-models-bounces at r-project.org> on behalf of Dena Paris <dp at dparis.com>
Sent: Tuesday, December 6, 2016 3:00 AM
To: Jarrod Hadfield
Cc: r-sig-mixed-models at r-project.org
Subject: Re: [R-sig-ME] parameter estimates for all factor levels MCMCglmm

Hi Jarrod,

Thanks for the very prompt reply! I?m still struggling with this a bit. My understanding was that by removing the global intercept (-1) all factor levels would be estimated, and if I have only one fixed effect, say Arthropod, this is exactly what happens. However, when I include the second fixed effect, Size, it returns estimates for all 16 levels of Arthropod, but estimates for only 2 of the 3 Size classes. So, in the case with two fixed effects and the global intercept removed, is there a way to calculate the estimate for the third Size class, or is it the level from which the other levels deviate?

Thanks again for all your help.

Dena

> On 5 Dec. 2016, at 5:49 pm, Jarrod Hadfield <j.hadfield at ed.ac.uk> wrote:
>
> Hi,
>
> You have an intercept which will be the estimate for the first level of Arthropod and Size, the remaining effects are deviations from these levels (15 and 2 contrasts respectively).
>
> Cheers,
>
> Jarrod
>
>
>
> On 05/12/2016 04:29, Dena Paris wrote:
>> Hi all,
>>
>> I'm new to Bayesian stats and the MCMCglmm package. I'm trying to understand how to interpret the results from a fitted model. I've fitted a model with a binomial response (reject/select), two categorical fixed effects (taxon with 16 levels, and size class with 3 levels), and a single random effect (bird ID). My model is mixing well and the results fit the data. My problem is that I'm not getting estimates for all levels of the fixed effects (19). How do I get the post mean and CIs for all levels in order to correctly interpret/write up the results?
>>
>> As I have (near) complete separation in the data, I've used the fixed effect prior structure suggested in the Course Notes, fixed the residuals, and removed the global intercept.
>>
>> prior.1 = list(
>>   B = list(mu = rep(0, 18), V = (diag(18)) * (1 + pi^2/3)),
>>   R = list(fix=1, V=1, n = k - 1),
>>   G = list(G1 = list(V = 1, n = 1))
>> )?
>>
>> m.1 <- MCMCglmm(Selected ~ -1 + Arthropod + Size, random = ~bID, family = "categorical", prior = prior.1, data = type.selected, verbose = FALSE, nitt = 5e+05, burnin = 5000, thin = 100)
>>
>> Thank you for your guidance,
>> Dena
>>
>>
>> Dena Paris
>>
>> -----
>> Dena Paris
>> Ph.D Candidate
>> School of Environmental Sciences
>> Institute for Land, Water and Society
>> Charles Sturt University
>> PO Box 789
>> Albury NSW 2640
>> M: +61 424 451 858?
>>
>>
>>      [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
> --
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
>

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbolker at gmail.com  Sun Dec  4 19:23:00 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 4 Dec 2016 13:23:00 -0500
Subject: [R-sig-ME] Mixed Models convergence problems, Jordi Rosich
In-Reply-To: <4737E17E7C8C3C4A8B5C1CE5346371D48A67B403@EXCH06S.adqimr.ad.lan>
References: <CAJ6PopzBUtMaO5pLh+h0rPryk63fwFdfsHufCObKZ+neOXiYpw@mail.gmail.com>
	<1479187148.2575.10.camel@unisa.edu.au>
	<CAJ6PopwVkj5KzX379Gu0mTJtZSDp6yv9NT30GVdj7ZKJsK5MiQ@mail.gmail.com>
	<CA+hwERka8B0g8E+xXeMY8TuPEhsMbaaBQH=NRZ_Nw5NT3PssWw@mail.gmail.com>
	<4737E17E7C8C3C4A8B5C1CE5346371D48A67B403@EXCH06S.adqimr.ad.lan>
Message-ID: <2838064f-bcae-65d2-5679-2876a9958b9f@gmail.com>


My other thought about this was that *if* other covariates don't vary
within territory:treatment combinations, and the response is identical
within a territory:treatment combination, you wouldn't lose any actual
inferential power by collapsing to a single value per
territory:treatment combo ... ?
   Sorry if that isn't relevant, I haven't been following too closely ...

On 16-12-03 11:58 PM, David Duffy wrote:
>> [In] all our territories, all nests/"no-nests" of a territory are
>> either occupied or unoccupied.
> 
> Given your controls are unoccupied trees from within each territory,
> did a conditional logistic regression work (with territory as stratum
> indicator)? You need to either provide a data set (not too big!), or
> speak to a local statistician. 
> _______________________________________________ 
> R-sig-mixed-models at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From santoro at ebd.csic.es  Tue Dec  6 14:41:19 2016
From: santoro at ebd.csic.es (Simone Santoro)
Date: Tue, 6 Dec 2016 14:41:19 +0100
Subject: [R-sig-ME] GLMM for underdispersed count data:
 Conway-Maxwell-Poisson and Ordinal
In-Reply-To: <a354afb2-6835-8457-dcd6-d2ae89ded0b4@ebd.csic.es>
References: <a354afb2-6835-8457-dcd6-d2ae89ded0b4@ebd.csic.es>
Message-ID: <9fe6f0cc-eb40-d81e-2915-bdbda1957ba2@ebd.csic.es>

Dear all,

I am trying to find an appropriate GLMM (with temporal and individual 
crossed random effects) to model underdispersed count data (clutch 
size). I have found several possible ways of doing that. A good 
distribution for data like this would seem to be the 
Conway-Maxwell-Poisson but I have not found a way of using it within a 
GLMM in R (I have asked here 
<http://stats.stackexchange.com/questions/249738/how-to-define-the-nu-parameter-of-conway-maxwell-poisson-in-spamm-package> 
and here 
<http://stats.stackexchange.com/questions/249798/conway-maxwell-poisson-with-crossed-random-effects-in-r>).
I have seen that Ben Bolker suggested (here 
<https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q1/021945.html>and 
here 
<http://stats.stackexchange.com/questions/92156/how-to-handle-underdispersion-in-glmm-binomial-outcome-variable>) 
to use an ordinal model in cases like this(e.g. _ordinal:clmm_). I have 
tried this solution and the results I obtain makes (biological) sense to 
me. However, I wonder why but I cannot put all the three crossed random 
effects I have in the clmm model (_Error: no. random effects (=1254) >= 
no. observations (=854)_) whereas it is not a problem for the glmer 
model (the no. of levels of each single random effect does not exceed 854)*.
Beyond that, and that's what I would like to ask you, *I cannot find a 
reference to justify I used the ordinal model* to deal with 
underdispersed count data (referee will ask it for sure).
Best,

Simone

* FMglmer<- glmer(fledges ~ habitatF * (areaPatchFath + poligF01 + 
StdLayingDate + ageFath1 + ageMoth1) + (1|year) + (1|ringMoth) + 
(1|ringFath), data = datiDRS)
    FMclmm<- glmer(as.factor(fledges)~ habitatF * (areaPatchFath + 
poligF01 + StdLayingDate + ageFath1 + ageMoth1) + (1|year) + 
(1|ringMoth) + (1|ringFath), data = datiDRS)


	[[alternative HTML version deleted]]


From cvonende at niu.edu  Tue Dec  6 17:49:48 2016
From: cvonende at niu.edu (Carl Von Ende)
Date: Tue, 6 Dec 2016 16:49:48 +0000
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 120,
 Issue 10: Doug Bates Notebooks on Mixed Models in Julia
Message-ID: <F6FC71FF-49D5-4474-BA18-4AA5801A7B71@niu.edu>

Dear Dr. Bates,

As a novice user of Julia, I plead ignorance in following one aspect of your instructions for accessing your Mixed Model notebook in the gitbhub repository from JuliaBox, as you indicate below:  

?An alternative is to establish an account on https://juliabox.com and,
under the "Files" tab, link the github repository.  Then open the notebook
in the Jupyter tab.?

I have established an account for JuliaBox and see Julia tutorials under the Files tab, under the Jupyter Tab.  I also see how files can be uploaded, but I don?t see how to add the link to the github repository for your Notebook on Mixed Models, so I can access the notebook in Jupyter. 

Thanks for any help you can provide.

Carl von Ende

On 12/6/16, 5:00 AM, "R-sig-mixed-models on behalf of r-sig-mixed-models-request at r-project.org" <r-sig-mixed-models-bounces at r-project.org on behalf of r-sig-mixed-models-request at r-project.org> wrote:

    https://github.com/dmbates/MixedModelsinJulia


From bates at stat.wisc.edu  Tue Dec  6 18:46:57 2016
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 06 Dec 2016 17:46:57 +0000
Subject: [R-sig-ME] R-sig-mixed-models Digest, Vol 120,
 Issue 10: Doug Bates Notebooks on Mixed Models in Julia
In-Reply-To: <F6FC71FF-49D5-4474-BA18-4AA5801A7B71@niu.edu>
References: <F6FC71FF-49D5-4474-BA18-4AA5801A7B71@niu.edu>
Message-ID: <CAO7JsnSttndRYkNyosSFrxZbcNLMvTRgFf1ko3n81-sg3+7N=w@mail.gmail.com>

My apologies.  I was on my way to catch the bus and did not check that my
instructions were correct.  I should have said the "Sync" tab, not the
"Files" tab.

Let me know if that works.

By the way, if it is considered an abuse of the list to discuss the Julia
implementation I can create another list.  Of course, as I am the moderator
of this list, I do have a certain amount of latitude in what I will allow
myself to do.

On Tue, Dec 6, 2016 at 10:50 AM Carl Von Ende <cvonende at niu.edu> wrote:

> Dear Dr. Bates,
>
> As a novice user of Julia, I plead ignorance in following one aspect of
> your instructions for accessing your Mixed Model notebook in the gitbhub
> repository from JuliaBox, as you indicate below:
>
> ?An alternative is to establish an account on https://juliabox.com and,
> under the "Files" tab, link the github repository.  Then open the notebook
> in the Jupyter tab.?
>
> I have established an account for JuliaBox and see Julia tutorials under
> the Files tab, under the Jupyter Tab.  I also see how files can be
> uploaded, but I don?t see how to add the link to the github repository for
> your Notebook on Mixed Models, so I can access the notebook in Jupyter.
>
> Thanks for any help you can provide.
>
> Carl von Ende
>
> On 12/6/16, 5:00 AM, "R-sig-mixed-models on behalf of
> r-sig-mixed-models-request at r-project.org" <
> r-sig-mixed-models-bounces at r-project.org on behalf of
> r-sig-mixed-models-request at r-project.org> wrote:
>
>     https://github.com/dmbates/MixedModelsinJulia
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From kn.journal.news at gmail.com  Wed Dec  7 10:17:47 2016
From: kn.journal.news at gmail.com (Koen Neijenhuijs)
Date: Wed, 7 Dec 2016 10:17:47 +0100
Subject: [R-sig-ME] Using individual differences from model A as
 predictor in model B
In-Reply-To: <CAJuCY5zV55tqYPjTjfMpD50ZijbGD0L9Y2OKnT1S4UKVQ42tQA@mail.gmail.com>
References: <CABL7txS=85Nprzur1KTeYrsVjU3VweNsqc2+W1q6R1nhh8p_Cg@mail.gmail.com>
	<CAJuCY5zXAMSCh1VMduDDioCvqDPjMur-Ekh=xCo97rmL2iMzuw@mail.gmail.com>
	<CABL7txRspxdUWM_p6MjFdR0WZRg+48V8yZJOgQ4pBJiPw91s4w@mail.gmail.com>
	<CAJuCY5zV55tqYPjTjfMpD50ZijbGD0L9Y2OKnT1S4UKVQ42tQA@mail.gmail.com>
Message-ID: <CABL7txQWxMSrKXt7A2bWkoP5aG5jJu3Dh4xZqxeY_FvqefaTrw@mail.gmail.com>

Hi Thierry,

we've been doing this for the past couple of days, and realised that
certain lag-models might represent our construct of motivation best. Thanks
for the advice!

Best,

Koen

2016-12-02 14:20 GMT+01:00 Thierry Onkelinx <thierry.onkelinx at inbo.be>:

> Dear Koen,
>
> Have you tried writing down the model you envision as a (set of)
> mathematical expressions? This can help to better understand what you want
> and how you can fit it.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-12-02 13:31 GMT+01:00 Koen Neijenhuijs <kn.journal.news at gmail.com>:
>
>> Dear Thierry,
>>
>> thanks for the quick response! That would be the easiest model, but I'm
>> not entirely sure whether this model represents motivation the way we
>> envision it. In essence what you propose is a model where we test whether
>> adherence has changed from week1&week2 to week3, and whether this change is
>> different for the two different groups. However, this model does not have a
>> concrete moderation of the individual differences inside of the fixed
>> effect, which is what we're after. In essence, the research question isn't
>> so much about the two different groups (the groups merely exist to balance
>> the experiment itself, and is thus a control variable), but what the effect
>> of the manipulation is for participants of different intrinsic motivations.
>> Our problem is that intrinsic motivation is inherently intertwined with the
>> dependent variable, and while putting time in the model as you propose is
>> one way to approach the question, the interaction of BeforeAfter*Treatment
>> doesn't allow us to disentagle the question regarding motivation.
>>
>> Kind regards,
>>
>> Koen
>>
>> 2016-12-02 13:23 GMT+01:00 Thierry Onkelinx <thierry.onkelinx at inbo.be>:
>>
>>> Dear Koen,
>>>
>>> I think you can fit this in a single model. Here a a few options:
>>>
>>> with lme4:
>>> Adherence ~ BeforeAfter * Treatment + (1 + BeforeAfter|Participant)
>>> Adherence ~ BeforeAfter * Treatment + (1| Participant:BeforeAfter)
>>>
>>> with INLA:
>>> Adherence ~ BeforeAfter * Treatment + f(Time, model = "rw1", replicate =
>>> Participant)
>>>
>>> The BeforeAfter:Treatment interaction is the effect you are interested
>>> in. The lme4 random effect allow for an additional treatment effect for
>>> individual participants. The INLA random effect allows for correlated
>>> random intercept along Time for the individual participant. rw1 stands for
>>> random walk of order 1, which models the differences between consecutive
>>> time points.
>>>
>>> Best regards,
>>>
>>>
>>> ir. Thierry Onkelinx
>>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>>> and Forest
>>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>>> Kliniekstraat 25
>>> 1070 Anderlecht
>>> Belgium
>>>
>>> To call in the statistician after the experiment is done may be no more
>>> than asking him to perform a post-mortem examination: he may be able to say
>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> The plural of anecdote is not data. ~ Roger Brinner
>>> The combination of some data and an aching desire for an answer does not
>>> ensure that a reasonable answer can be extracted from a given body of data.
>>> ~ John Tukey
>>>
>>> 2016-12-02 12:03 GMT+01:00 Koen Neijenhuijs <kn.journal.news at gmail.com>:
>>>
>>>> Dear all,
>>>>
>>>>
>>>> we've run an experiment with two groups, which we followed for 3 weeks.
>>>> Each participant got three trials per week, and our dependent variable
>>>> is
>>>> the adherence, defined as whether they replied to the trial or not. In
>>>> the
>>>> third week, we introduced a manipulation, which was balanced across the
>>>> two
>>>> groups. We want to test the effect of the manipulation, moderated for
>>>> intrinsic motivation to adhere to the trials. We are struggling with the
>>>> operationalization of intrinsic motivation.
>>>>
>>>> We ran a binomial mixed-effect model on the data of the first two
>>>> weeks, to
>>>> estimate intrinsic motivation. So far, we've come up with three methods
>>>> to
>>>> do so, but each comes with their own concerns. I was hoping to hear your
>>>> thoughts on this.
>>>>
>>>> 1. The first method is simply to use the aggregated (sum) adherence of
>>>> each
>>>> participant. This method would be seemingly valid, as the model on the
>>>> first two weeks shows no main effect of time, group, nor the interaction
>>>> time*group. However, I am reluctant to go this route as this method is
>>>> less
>>>> detailed than the other options.
>>>>
>>>> 2. The second method is to extract the random-adjusted intercept and
>>>> random-adjusted slope of time (random effects + fixed effects), per
>>>> participant. The interaction of these two represent intrinsic
>>>> motivation as
>>>> it inherits both the intercept of adherence as well as its' development
>>>> over time; this combination is capable of representing every possible
>>>> motivation timeline (start high and go lower over time; start high and
>>>> stay
>>>> high over time; start low and go up over time; etc). However, using this
>>>> method, to test the effect we're interested in will result in a
>>>> three-way
>>>> interaction (intercept*slope*manipulation), and a four-way interaction
>>>> to
>>>> check moderation of prior group characteristics. It is unlikely we have
>>>> enough power to test this, as our sample size is limited.
>>>>
>>>> 3. The third method is to extract the prediction equation from the
>>>> model of
>>>> the first two weeks and apply this to the data of the third week. This
>>>> method will give us one representation of motivation instead of two,
>>>> which
>>>> does include both fixed and random effects. However, as the method is
>>>> applied to data of the third week, I am uncertain whether it is valid
>>>> as a
>>>> representation of intrinsic motivation over the first two weeks.
>>>>
>>>> Sorry for the long wall of text. What are your thoughts on this? Are
>>>> there
>>>> other ways of operationalizing individual differences on adherence in
>>>> the
>>>> first two weeks to use as an independent variable on adherence in the
>>>> third
>>>> week?
>>>>
>>>> Cheers,
>>>>
>>>> Koen
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>>
>>>
>>
>

	[[alternative HTML version deleted]]


From kristin.precoda at mq.edu.au  Wed Dec  7 04:08:55 2016
From: kristin.precoda at mq.edu.au (Kristin Precoda)
Date: Wed, 7 Dec 2016 03:08:55 +0000
Subject: [R-sig-ME] clmm2() thresholds - forcing symmetry around zero
Message-ID: <MEXPR01MB06641997EAA231B509D0BD75B6850@MEXPR01MB0664.ausprd01.prod.outlook.com>

Hi,

I'm using clmm2() to fit a model with three possible response categories (win, tie, lose).  For the particular problem, the thresholds need to be symmetric around zero, so win|tie = -1 * tie|lose.  Is there a way either to force the thresholds to be symmetric around zero, or to just set the thresholds (for example, to -1 and +1) and not optimize them?


Thanks very much,

Kristin

	[[alternative HTML version deleted]]


From cvonende at niu.edu  Wed Dec  7 19:39:13 2016
From: cvonende at niu.edu (Carl Von Ende)
Date: Wed, 7 Dec 2016 18:39:13 +0000
Subject: [R-sig-ME] FW:  R-sig-mixed-models Digest, Vol 120,
 Issue 10: Doug Bates Notebooks on Mixed Models in Julia
In-Reply-To: <5A0EB9AF-B98B-4FE1-BE14-69756066DFA9@niu.edu>
References: <F6FC71FF-49D5-4474-BA18-4AA5801A7B71@niu.edu>
	<CAO7JsnSttndRYkNyosSFrxZbcNLMvTRgFf1ko3n81-sg3+7N=w@mail.gmail.com>
	<5A0EB9AF-B98B-4FE1-BE14-69756066DFA9@niu.edu>
Message-ID: <A2C46B3C-34AD-4176-9D7D-CF1EA05CA6A7@niu.edu>



From: Carl Von Ende <cvonende at niu.edu>
Date: Tuesday, December 6, 2016 at 12:28 PM
To: Douglas Bates <bates at stat.wisc.edu>
Subject: Re: [R-sig-ME] R-sig-mixed-models Digest, Vol 120, Issue 10: Doug Bates Notebooks on Mixed Models in Julia

Yes, that works. Now I can read the notebook in github and the figures look fine. Thanks very much!

Personally, I?m fine with Julia notes with respect to mixed models on this list.

Carl

From: Douglas Bates <bates at stat.wisc.edu>
Date: Tuesday, December 6, 2016 at 11:46 AM
To: Carl Von Ende <cvonende at niu.edu>, "r-sig-mixed-models at r-project.org" <r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] R-sig-mixed-models Digest, Vol 120, Issue 10: Doug Bates Notebooks on Mixed Models in Julia

My apologies.  I was on my way to catch the bus and did not check that my instructions were correct.  I should have said the "Sync" tab, not the "Files" tab.

Let me know if that works.

By the way, if it is considered an abuse of the list to discuss the Julia implementation I can create another list.  Of course, as I am the moderator of this list, I do have a certain amount of latitude in what I will allow myself to do.

On Tue, Dec 6, 2016 at 10:50 AM Carl Von Ende <cvonende at niu.edu<mailto:cvonende at niu.edu>> wrote:
Dear Dr. Bates,

As a novice user of Julia, I plead ignorance in following one aspect of your instructions for accessing your Mixed Model notebook in the gitbhub repository from JuliaBox, as you indicate below:

?An alternative is to establish an account on https://juliabox.com and,
under the "Files" tab, link the github repository.  Then open the notebook
in the Jupyter tab.?

I have established an account for JuliaBox and see Julia tutorials under the Files tab, under the Jupyter Tab.  I also see how files can be uploaded, but I don?t see how to add the link to the github repository for your Notebook on Mixed Models, so I can access the notebook in Jupyter.

Thanks for any help you can provide.

Carl von Ende

On 12/6/16, 5:00 AM, "R-sig-mixed-models on behalf of r-sig-mixed-models-request at r-project.org<mailto:r-sig-mixed-models-request at r-project.org>" <r-sig-mixed-models-bounces at r-project.org<mailto:r-sig-mixed-models-bounces at r-project.org> on behalf of r-sig-mixed-models-request at r-project.org<mailto:r-sig-mixed-models-request at r-project.org>> wrote:

    https://github.com/dmbates/MixedModelsinJulia

_______________________________________________
R-sig-mixed-models at r-project.org<mailto:R-sig-mixed-models at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From mollieebrooks at gmail.com  Wed Dec  7 20:22:35 2016
From: mollieebrooks at gmail.com (Mollie Brooks)
Date: Wed, 7 Dec 2016 20:22:35 +0100
Subject: [R-sig-ME] GLMM for underdispersed count data:
	Conway-Maxwell-Poisson and Ordinal
In-Reply-To: <9fe6f0cc-eb40-d81e-2915-bdbda1957ba2@ebd.csic.es>
References: <a354afb2-6835-8457-dcd6-d2ae89ded0b4@ebd.csic.es>
	<9fe6f0cc-eb40-d81e-2915-bdbda1957ba2@ebd.csic.es>
Message-ID: <444E4ED7-DFBA-4EBA-964D-E27450D6619E@ufl.edu>

Dear Simone,

I?ve been working on adding the Conway-Maxwell-Poisson distribution to the glmmTMB package. It isn?t published yet, but I?ve tested it with simulated data and 2 real data sets. It seems to be working well, so I plan to introduce it in a manuscript on Biorxiv in the near future. You?re welcome to try it with your data and tell me how it goes. 

First, you?ll have to install the genpois branch of glmmTMB with the following

devtools::install_github("glmmTMB/glmmTMB/glmmTMB", ref="genpois")

Then, you could fit your model with this code

FMCMP <- glmmTMB(fledges ~ habitatF * (areaPatchFath + poligF01 + 
StdLayingDate + ageFath1 + ageMoth1) + (1|year) + (1|ringMoth) + 
(1|ringFath), data = datiDRS, family="compois")

For an explanation of the dispersion parameter, see ?sigma.glmmTMB

If you want to try it out on simulated data, there?s an rCMP function available here
https://github.com/James-Thorson/Conway-Maxwell-Poisson <https://github.com/James-Thorson/Conway-Maxwell-Poisson>

cheers,
Mollie

???????????
Mollie E. Brooks, Ph.D.
Postdoctoral Researcher
National Institute of Aquatic Resources
Technical University of Denmark

> On 6Dec 2016, at 14:41, Simone Santoro <santoro at ebd.csic.es> wrote:
> 
> Dear all,
> 
> I am trying to find an appropriate GLMM (with temporal and individual 
> crossed random effects) to model underdispersed count data (clutch 
> size). I have found several possible ways of doing that. A good 
> distribution for data like this would seem to be the 
> Conway-Maxwell-Poisson but I have not found a way of using it within a 
> GLMM in R (I have asked here 
> <http://stats.stackexchange.com/questions/249738/how-to-define-the-nu-parameter-of-conway-maxwell-poisson-in-spamm-package> 
> and here 
> <http://stats.stackexchange.com/questions/249798/conway-maxwell-poisson-with-crossed-random-effects-in-r>).
> I have seen that Ben Bolker suggested (here 
> <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q1/021945.html>and 
> here 
> <http://stats.stackexchange.com/questions/92156/how-to-handle-underdispersion-in-glmm-binomial-outcome-variable>) 
> to use an ordinal model in cases like this(e.g. _ordinal:clmm_). I have 
> tried this solution and the results I obtain makes (biological) sense to 
> me. However, I wonder why but I cannot put all the three crossed random 
> effects I have in the clmm model (_Error: no. random effects (=1254) >= 
> no. observations (=854)_) whereas it is not a problem for the glmer 
> model (the no. of levels of each single random effect does not exceed 854)*.
> Beyond that, and that's what I would like to ask you, *I cannot find a 
> reference to justify I used the ordinal model* to deal with 
> underdispersed count data (referee will ask it for sure).
> Best,
> 
> Simone
> 
> * FMglmer<- glmer(fledges ~ habitatF * (areaPatchFath + poligF01 + 
> StdLayingDate + ageFath1 + ageMoth1) + (1|year) + (1|ringMoth) + 
> (1|ringFath), data = datiDRS)
>    FMclmm<- glmer(as.factor(fledges)~ habitatF * (areaPatchFath + 
> poligF01 + StdLayingDate + ageFath1 + ageMoth1) + (1|year) + 
> (1|ringMoth) + (1|ringFath), data = datiDRS)
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 


	[[alternative HTML version deleted]]


From bbolker at gmail.com  Thu Dec  8 04:32:05 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 7 Dec 2016 22:32:05 -0500
Subject: [R-sig-ME] GLMM for underdispersed count data:
 Conway-Maxwell-Poisson and Ordinal
In-Reply-To: <9fe6f0cc-eb40-d81e-2915-bdbda1957ba2@ebd.csic.es>
References: <a354afb2-6835-8457-dcd6-d2ae89ded0b4@ebd.csic.es>
	<9fe6f0cc-eb40-d81e-2915-bdbda1957ba2@ebd.csic.es>
Message-ID: <cd745e2c-9b7f-a347-6568-ecd79e8ec2a3@gmail.com>


   One reference that uses ordinal regression in a similar situation
(litter size of Florida panthers) is
http://link.springer.com/article/10.1007/s00442-011-2083-0 ("Does
genetic introgression improve female reproductive performance? A test on
the endangered Florida panther")

  Not sure about the number-of-random-effects error: a reproducible
example would probably be needed (smaller is better!)

  Ben Bolker


On 16-12-06 08:41 AM, Simone Santoro wrote:
> Dear all,
> 
> I am trying to find an appropriate GLMM (with temporal and individual 
> crossed random effects) to model underdispersed count data (clutch 
> size). I have found several possible ways of doing that. A good 
> distribution for data like this would seem to be the 
> Conway-Maxwell-Poisson but I have not found a way of using it within a 
> GLMM in R (I have asked here 
> <http://stats.stackexchange.com/questions/249738/how-to-define-the-nu-parameter-of-conway-maxwell-poisson-in-spamm-package> 
> and here 
> <http://stats.stackexchange.com/questions/249798/conway-maxwell-poisson-with-crossed-random-effects-in-r>).
> I have seen that Ben Bolker suggested (here 
> <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q1/021945.html>and 
> here 
> <http://stats.stackexchange.com/questions/92156/how-to-handle-underdispersion-in-glmm-binomial-outcome-variable>) 
> to use an ordinal model in cases like this(e.g. _ordinal:clmm_). I have 
> tried this solution and the results I obtain makes (biological) sense to 
> me. However, I wonder why but I cannot put all the three crossed random 
> effects I have in the clmm model (_Error: no. random effects (=1254) >= 
> no. observations (=854)_) whereas it is not a problem for the glmer 
> model (the no. of levels of each single random effect does not exceed 854)*.
> Beyond that, and that's what I would like to ask you, *I cannot find a 
> reference to justify I used the ordinal model* to deal with 
> underdispersed count data (referee will ask it for sure).
> Best,
> 
> Simone
> 
> * FMglmer<- glmer(fledges ~ habitatF * (areaPatchFath + poligF01 + 
> StdLayingDate + ageFath1 + ageMoth1) + (1|year) + (1|ringMoth) + 
> (1|ringFath), data = datiDRS)
>     FMclmm<- glmer(as.factor(fledges)~ habitatF * (areaPatchFath + 
> poligF01 + StdLayingDate + ageFath1 + ageMoth1) + (1|year) + 
> (1|ringMoth) + (1|ringFath), data = datiDRS)
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From briauxj at gmail.com  Thu Dec  8 21:25:05 2016
From: briauxj at gmail.com (justine briaux)
Date: Thu, 8 Dec 2016 21:25:05 +0100
Subject: [R-sig-ME] Weighting analysis in Lme4
Message-ID: <CACv1VTmUT=otAP+XOV-_3_Ji0sZ4bAJkMwiiUq-hMJh+=dnnoA@mail.gmail.com>

 Dear Mr Bolker,

I am a PHD student in Public Health and I am currently working on data
collected with a  complex survey design. These data were collected in 5
different districts/ strata (codpref) and 162 villages/cluster (numvill).
In each village a representative sample of mother-infant pairs was
surveyed. Some mother-infant pairs come from the same household (household=
idmen).
In order to consider those three interlocked levels I've done mixed model
using the lme4 package in R

glmer(undernutrition~household food insecurity+(1|codpref)+(1|numv
ill)+(1|idmen),data=menme,family=binomial)

I am wondering if I should weight my analysis in order to take into account
the survey weights (pond) as the "survey package" would have done it.

I saw in the R documentation that the argument "weight" exist in the lme4
package, does it correspond to survey weights? For instance, would it be
correct to write:

 glmer(undernutrition~household food
insecurity+(1|codpref)+(1|numvill)+(1|idmen),data=menme,
weight= pond, family=binomial)

I wanted to use the survey package but it does not allow me to do a mixed
model and thus to take into accound the household level (idmen).

I am really confused. What should I do?


Thanks a lot for your help.
Looking forward to hearing from you.

Warm regards.

Justine Briaux
PHD student
IRD, France

<r-sig-mixed-models at r-project.org>

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Fri Dec  9 02:26:41 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 8 Dec 2016 20:26:41 -0500
Subject: [R-sig-ME] Weighting analysis in Lme4
In-Reply-To: <CACv1VTmUT=otAP+XOV-_3_Ji0sZ4bAJkMwiiUq-hMJh+=dnnoA@mail.gmail.com>
References: <CACv1VTmUT=otAP+XOV-_3_Ji0sZ4bAJkMwiiUq-hMJh+=dnnoA@mail.gmail.com>
Message-ID: <9633ebb6-a5ca-5206-d367-7e2ab3810206@gmail.com>


  Hopefully someone else will respond as well, but:

  As I understand it combining survey weighting with lme4's
regression-based approach is tricky.  lme4's weights argument does *not*
(again as I understand it, this isn't my area) correspond to sample weights.

If you google "site://stat.ethz.ch/pipermail/r-sig-mixed-models/ survey
weights" you'll find a lot of discussion on the list, e.g.

https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q4/022795.html

  Anyone have any new insights into this problem?

  Ben B.

On 16-12-08 03:25 PM, justine briaux wrote:
>  Dear Mr Bolker,
> 
> I am a PHD student in Public Health and I am currently working on data
> collected with a  complex survey design. These data were collected in 5
> different districts/ strata (codpref) and 162 villages/cluster (numvill).
> In each village a representative sample of mother-infant pairs was
> surveyed. Some mother-infant pairs come from the same household (household=
> idmen).
> In order to consider those three interlocked levels I've done mixed model
> using the lme4 package in R
> 
> glmer(undernutrition~household food insecurity+(1|codpref)+(1|numv
> ill)+(1|idmen),data=menme,family=binomial)
> 
> I am wondering if I should weight my analysis in order to take into account
> the survey weights (pond) as the "survey package" would have done it.
> 
> I saw in the R documentation that the argument "weight" exist in the lme4
> package, does it correspond to survey weights? For instance, would it be
> correct to write:
> 
>  glmer(undernutrition~household food
> insecurity+(1|codpref)+(1|numvill)+(1|idmen),data=menme,
> weight= pond, family=binomial)
> 
> I wanted to use the survey package but it does not allow me to do a mixed
> model and thus to take into accound the household level (idmen).
> 
> I am really confused. What should I do?
> 
> 
> Thanks a lot for your help.
> Looking forward to hearing from you.
> 
> Warm regards.
> 
> Justine Briaux
> PHD student
> IRD, France
> 
> <r-sig-mixed-models at r-project.org>
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From bachlaw01 at outlook.com  Fri Dec  9 05:03:02 2016
From: bachlaw01 at outlook.com (Jonathan Judge)
Date: Fri, 9 Dec 2016 04:03:02 +0000
Subject: [R-sig-ME] GLMM for underdispersed count
	data:	Conway-Maxwell-Poisson and Ordinal
In-Reply-To: <444E4ED7-DFBA-4EBA-964D-E27450D6619E@ufl.edu>
References: <a354afb2-6835-8457-dcd6-d2ae89ded0b4@ebd.csic.es>
	<9fe6f0cc-eb40-d81e-2915-bdbda1957ba2@ebd.csic.es>,
	<444E4ED7-DFBA-4EBA-964D-E27450D6619E@ufl.edu>
Message-ID: <BN3PR16MB0865C9ACF466C10EC70A5978AF870@BN3PR16MB0865.namprd16.prod.outlook.com>

The Conway-Maxwell-Poisson may well be the way to go here; I?ll note only that I have sometimes found underdispersed counting data being driven by excess zeroes. If zero-inflation of some kind is in fact the culprit, and you still wish to use multi-level modeling, the brms front-end for Stan offers a variety of easy-to-use fitting options for zero-inflation / hurdle / adjustment.



Best,

Jonathan



Sent from Mail<https://go.microsoft.com/fwlink/?LinkId=550986> for Windows 10



From: Mollie Brooks<mailto:mollieebrooks at gmail.com>
Sent: Wednesday, December 7, 2016 7:36 PM
To: Simone Santoro<mailto:santoro at ebd.csic.es>
Cc: r-sig-mixed-models at r-project.org<mailto:r-sig-mixed-models at r-project.org>
Subject: Re: [R-sig-ME] GLMM for underdispersed count data: Conway-Maxwell-Poisson and Ordinal



Dear Simone,

I?ve been working on adding the Conway-Maxwell-Poisson distribution to the glmmTMB package. It isn?t published yet, but I?ve tested it with simulated data and 2 real data sets. It seems to be working well, so I plan to introduce it in a manuscript on Biorxiv in the near future. You?re welcome to try it with your data and tell me how it goes.

First, you?ll have to install the genpois branch of glmmTMB with the following

devtools::install_github("glmmTMB/glmmTMB/glmmTMB", ref="genpois")

Then, you could fit your model with this code

FMCMP <- glmmTMB(fledges ~ habitatF * (areaPatchFath + poligF01 +
StdLayingDate + ageFath1 + ageMoth1) + (1|year) + (1|ringMoth) +
(1|ringFath), data = datiDRS, family="compois")

For an explanation of the dispersion parameter, see ?sigma.glmmTMB

If you want to try it out on simulated data, there?s an rCMP function available here
https://github.com/James-Thorson/Conway-Maxwell-Poisson <https://github.com/James-Thorson/Conway-Maxwell-Poisson>

cheers,
Mollie

???????????
Mollie E. Brooks, Ph.D.
Postdoctoral Researcher
National Institute of Aquatic Resources
Technical University of Denmark

> On 6Dec 2016, at 14:41, Simone Santoro <santoro at ebd.csic.es> wrote:
>
> Dear all,
>
> I am trying to find an appropriate GLMM (with temporal and individual
> crossed random effects) to model underdispersed count data (clutch
> size). I have found several possible ways of doing that. A good
> distribution for data like this would seem to be the
> Conway-Maxwell-Poisson but I have not found a way of using it within a
> GLMM in R (I have asked here
> <http://stats.stackexchange.com/questions/249738/how-to-define-the-nu-parameter-of-conway-maxwell-poisson-in-spamm-package>
> and here
> <http://stats.stackexchange.com/questions/249798/conway-maxwell-poisson-with-crossed-random-effects-in-r>).
> I have seen that Ben Bolker suggested (here
> <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q1/021945.html>and
> here
> <http://stats.stackexchange.com/questions/92156/how-to-handle-underdispersion-in-glmm-binomial-outcome-variable>)
> to use an ordinal model in cases like this(e.g. _ordinal:clmm_). I have
> tried this solution and the results I obtain makes (biological) sense to
> me. However, I wonder why but I cannot put all the three crossed random
> effects I have in the clmm model (_Error: no. random effects (=1254) >=
> no. observations (=854)_) whereas it is not a problem for the glmer
> model (the no. of levels of each single random effect does not exceed 854)*.
> Beyond that, and that's what I would like to ask you, *I cannot find a
> reference to justify I used the ordinal model* to deal with
> underdispersed count data (referee will ask it for sure).
> Best,
>
> Simone
>
> * FMglmer<- glmer(fledges ~ habitatF * (areaPatchFath + poligF01 +
> StdLayingDate + ageFath1 + ageMoth1) + (1|year) + (1|ringMoth) +
> (1|ringFath), data = datiDRS)
>    FMclmm<- glmer(as.factor(fledges)~ habitatF * (areaPatchFath +
> poligF01 + StdLayingDate + ageFath1 + ageMoth1) + (1|year) +
> (1|ringMoth) + (1|ringFath), data = datiDRS)
>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


        [[alternative HTML version deleted]]

_______________________________________________
R-sig-mixed-models at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From b.pelzer at maw.ru.nl  Fri Dec  9 13:03:55 2016
From: b.pelzer at maw.ru.nl (Ben Pelzer)
Date: Fri, 9 Dec 2016 13:03:55 +0100
Subject: [R-sig-ME] Weighting analysis in Lme4
In-Reply-To: <9633ebb6-a5ca-5206-d367-7e2ab3810206@gmail.com>
References: <CACv1VTmUT=otAP+XOV-_3_Ji0sZ4bAJkMwiiUq-hMJh+=dnnoA@mail.gmail.com>
	<9633ebb6-a5ca-5206-d367-7e2ab3810206@gmail.com>
Message-ID: <731363ec-f43c-ebd8-91fc-743d747da58b@maw.ru.nl>

Dear Justine and Ben,

I compared results from using option pweight in Stata, which is for 
sampling weights or inverse probability weights, with results from lmer 
and the "weights=" option.  I transformed the weights with method A 
described by Carle in:

http://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-9-49

The lmer and Stata results were exactly the same, for the fixed effects, 
their standard errors and the random effects.

However, method B described by Carle, produced different results in 
Stata and lmer.

So my conclusion is that with method A, lmer produces correct results. 
Hope this helps a bit... Kind regards,

Ben.


On 9-12-2016 2:26, Ben Bolker wrote:
>    Hopefully someone else will respond as well, but:
>
>    As I understand it combining survey weighting with lme4's
> regression-based approach is tricky.  lme4's weights argument does *not*
> (again as I understand it, this isn't my area) correspond to sample weights.
>
> If you google "site://stat.ethz.ch/pipermail/r-sig-mixed-models/ survey
> weights" you'll find a lot of discussion on the list, e.g.
>
> https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q4/022795.html
>
>    Anyone have any new insights into this problem?
>
>    Ben B.
>
> On 16-12-08 03:25 PM, justine briaux wrote:
>>   Dear Mr Bolker,
>>
>> I am a PHD student in Public Health and I am currently working on data
>> collected with a  complex survey design. These data were collected in 5
>> different districts/ strata (codpref) and 162 villages/cluster (numvill).
>> In each village a representative sample of mother-infant pairs was
>> surveyed. Some mother-infant pairs come from the same household (household=
>> idmen).
>> In order to consider those three interlocked levels I've done mixed model
>> using the lme4 package in R
>>
>> glmer(undernutrition~household food insecurity+(1|codpref)+(1|numv
>> ill)+(1|idmen),data=menme,family=binomial)
>>
>> I am wondering if I should weight my analysis in order to take into account
>> the survey weights (pond) as the "survey package" would have done it.
>>
>> I saw in the R documentation that the argument "weight" exist in the lme4
>> package, does it correspond to survey weights? For instance, would it be
>> correct to write:
>>
>>   glmer(undernutrition~household food
>> insecurity+(1|codpref)+(1|numvill)+(1|idmen),data=menme,
>> weight= pond, family=binomial)
>>
>> I wanted to use the survey package but it does not allow me to do a mixed
>> model and thus to take into accound the household level (idmen).
>>
>> I am really confused. What should I do?
>>
>>
>> Thanks a lot for your help.
>> Looking forward to hearing from you.
>>
>> Warm regards.
>>
>> Justine Briaux
>> PHD student
>> IRD, France
>>
>> <r-sig-mixed-models at r-project.org>
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From santoro at ebd.csic.es  Fri Dec  9 19:40:50 2016
From: santoro at ebd.csic.es (Simone Santoro)
Date: Fri, 9 Dec 2016 19:40:50 +0100
Subject: [R-sig-ME] GLMM for underdispersed count data:
 Conway-Maxwell-Poisson and Ordinal
In-Reply-To: <cd745e2c-9b7f-a347-6568-ecd79e8ec2a3@gmail.com>
References: <a354afb2-6835-8457-dcd6-d2ae89ded0b4@ebd.csic.es>
	<9fe6f0cc-eb40-d81e-2915-bdbda1957ba2@ebd.csic.es>
	<cd745e2c-9b7f-a347-6568-ecd79e8ec2a3@gmail.com>
Message-ID: <37ba41e9-d157-b1de-676e-c7b2030efd9e@ebd.csic.es>

Hi,

Thank you all very much your hints. They have been really really helpful 
for me. Below you may find a reproducible code to see how three 
approaches fit a simulated data set (clmm::ordinal, glmmTMB::glmmTMB, 
fitme:spaMM). Results seem to me qualitatively similar but with 
clmm:ordinal I cannot use the three crossed random effects because I get 
an error like this:

Error: no. random effects (=135) >= no. observations (=100)


set.seed(1234)
library(ordinal)
library(glmmTMB)
library(spaMM)
dati<- data.frame(fledges= rpois(100,10), habitatF= 
as.factor(rbinom(100,1,0.5)), areaPatchFath= rnorm(100), poligF01= 
as.factor(rbinom(100,1,0.5)),StdLayingDate= rnorm(100), ageFath1= 
rpois(100,3), ageMoth1= rpois(100,3), year= as.factor(rpois(100,200)), 
ringMoth= as.factor(rpois(100,200)), ringFath= as.factor(rpois(100,200)))
str(dati)

system.time(Fitclm<- clmm(as.factor(fledges) ~ 
habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(1|ringMoth)+(1|ringFath),data=dati,Hess=T))
# this way it works...
system.time(Fitclm1<- clmm(as.factor(fledges) ~ 
habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(1|ringFath),data=dati,Hess=T))
summary(Fitclm1)

system.time(FitglmmTMB<- glmmTMB(as.factor(fledges) ~ 
habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(1|ringMoth)+(1|ringFath),data=dati,family= 
"compois"))
summary(FitglmmTMB)

system.time(FitglmmTMB<- glmmTMB(as.factor(fledges) ~ 
habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(1|ringMoth)+(1|ringFath),data=dati,family= 
"compois"))
summary(FitglmmTMB)

# This lasts much more (3-4')
system.time(Fitfitme<- fitme(fledges ~ 
habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(1|ringFath)+(1|ringMoth),data=dati,COMPoisson(),method 
= "ML"))
summary(Fitfitme)











El 08/12/2016 a las 4:32, Ben Bolker escribi?:
>     One reference that uses ordinal regression in a similar situation
> (litter size of Florida panthers) is
> http://link.springer.com/article/10.1007/s00442-011-2083-0  ("Does
> genetic introgression improve female reproductive performance? A test on
> the endangered Florida panther")
>
>    Not sure about the number-of-random-effects error: a reproducible
> example would probably be needed (smaller is better!)
>
>    Ben Bolker
>
>
> On 16-12-06 08:41 AM, Simone Santoro wrote:
>> Dear all,
>>
>> I am trying to find an appropriate GLMM (with temporal and individual
>> crossed random effects) to model underdispersed count data (clutch
>> size). I have found several possible ways of doing that. A good
>> distribution for data like this would seem to be the
>> Conway-Maxwell-Poisson but I have not found a way of using it within a
>> GLMM in R (I have asked here
>> <http://stats.stackexchange.com/questions/249738/how-to-define-the-nu-parameter-of-conway-maxwell-poisson-in-spamm-package>  
>> and here
>> <http://stats.stackexchange.com/questions/249798/conway-maxwell-poisson-with-crossed-random-effects-in-r>).
>> I have seen that Ben Bolker suggested (here
>> <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q1/021945.html>and
>> here
>> <http://stats.stackexchange.com/questions/92156/how-to-handle-underdispersion-in-glmm-binomial-outcome-variable>)
>> to use an ordinal model in cases like this(e.g. _ordinal:clmm_). I have
>> tried this solution and the results I obtain makes (biological) sense to
>> me. However, I wonder why but I cannot put all the three crossed random
>> effects I have in the clmm model (_Error: no. random effects (=1254) >=
>> no. observations (=854)_) whereas it is not a problem for the glmer
>> model (the no. of levels of each single random effect does not exceed 854)*.
>> Beyond that, and that's what I would like to ask you, *I cannot find a
>> reference to justify I used the ordinal model* to deal with
>> underdispersed count data (referee will ask it for sure).
>> Best,
>>
>> Simone
>>
>> * FMglmer<- glmer(fledges ~ habitatF * (areaPatchFath + poligF01 +
>> StdLayingDate + ageFath1 + ageMoth1) + (1|year) + (1|ringMoth) +
>> (1|ringFath), data = datiDRS)
>>      FMclmm<- glmer(as.factor(fledges)~ habitatF * (areaPatchFath +
>> poligF01 + StdLayingDate + ageFath1 + ageMoth1) + (1|year) +
>> (1|ringMoth) + (1|ringFath), data = datiDRS)
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org  mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>

-- 
Simone Santoro
PhD
Department of Ethology and Biodiversity Conservation
Do?ana Biological Station
Calle Am?rico Vespucio s/n
41092 Seville - Spain
Phone no. +34 954 466 700 (ext. 1213)
http://www.researchgate.net/profile/Simone_Santoro
http://orcid.org/0000-0003-0986-3278


	[[alternative HTML version deleted]]


From martinab at ugr.es  Sat Dec 10 00:27:22 2016
From: martinab at ugr.es (martinab)
Date: Sat, 10 Dec 2016 00:27:22 +0100
Subject: [R-sig-ME] MCMCglmm Prior-set for zipoisson with continuous and
 categorical randoms
Message-ID: <8b70f25b4f3541e6b4a58d4d6d357763@ugr.es>

Hi everyone,


I'm trying to run a zero-inflated MCMCglmm model from a repeated 
measures data set of aphid abundance on plants (2680 observations) with 
four random variables: 2 categorical (Plant (n = 140) and Block (n = 6)) 
and 2 continuous (Date (n = 15) and Temperature (n = 13)).

Without specifying a prior, this is the further I can go modelling:

pent_Plant_Block_Date_Temp <- MCMCglmm(Aphids~Flowers + Flowers_block, 
random = ~ idh(trait):Plant + idh(trait):Block + Temperature + Date, 
family="zipoisson", rcov=~us(trait):units, burnin = 2000, nitt = 100000, 
thin = 100, pr= TRUE, verbose= FALSE, data=RF2013_st_pentatomidae)

Nevertheless, the model is terrible in terms of effective size and 
autocorrelation, I can?t even plot it as it states "margins are too 
large". I guess I should fit a proper prior and run the model with the 
following random structure:

pent_Plant_Block_Date_Temp <- MCMCglmm(Aphids~Flowers + Flowers_block, 
random = ~ us(trait):Plant + us(trait):Block + Temperature + Date, 
family="zipoisson", rcov=~us(trait):units, burnin = 2000, nitt = 100000, 
thin = 100, prior= ???, pr= TRUE, verbose= FALSE, 
data=RF2013_st_pentatomidae)

I have tried plenty of different priors and none seems to have worked, 
as I always get "V is the wrong dimension for some prior$G/prior$R 
elements". I would appreciate the following:

a) advice for a proper prior-set
b) In case I would run the model as family="categorical" by observing 
just aphid presence/absence instead of abundance with rcov=~trait:units, 
how would it affect the prior??


Thanks in advance. Best,


Martin Aguirrebengoa
PhD student
Zoology Department
University of Granada


From paul.buerkner at gmail.com  Sat Dec 10 02:10:17 2016
From: paul.buerkner at gmail.com (Paul Buerkner)
Date: Sat, 10 Dec 2016 02:10:17 +0100
Subject: [R-sig-ME] GLMM for underdispersed count data:
 Conway-Maxwell-Poisson and Ordinal
In-Reply-To: <37ba41e9-d157-b1de-676e-c7b2030efd9e@ebd.csic.es>
References: <a354afb2-6835-8457-dcd6-d2ae89ded0b4@ebd.csic.es>
	<9fe6f0cc-eb40-d81e-2915-bdbda1957ba2@ebd.csic.es>
	<cd745e2c-9b7f-a347-6568-ecd79e8ec2a3@gmail.com>
	<37ba41e9-d157-b1de-676e-c7b2030efd9e@ebd.csic.es>
Message-ID: <CAGoSky9sJnqfv0trU2Zj3dFRCStR=7nXq_b7F56+L7FD=pkcJg@mail.gmail.com>

You could try using brms::brm with family "cumulative" as an alternative to
ordinal::clmm as the former is able to handle models with more random
effects than observations. The call would look as follows:

brm(as.factor(fledges) ~
habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(
1|ringMoth)+(1|ringFath),
data=dati, family = cumulative())

Note that you need a C++ compiler to get this to work since brms is a
front-end of Stan.

Best,
Paul

2016-12-09 19:40 GMT+01:00 Simone Santoro <santoro at ebd.csic.es>:

> Hi,
>
> Thank you all very much your hints. They have been really really helpful
> for me. Below you may find a reproducible code to see how three
> approaches fit a simulated data set (clmm::ordinal, glmmTMB::glmmTMB,
> fitme:spaMM). Results seem to me qualitatively similar but with
> clmm:ordinal I cannot use the three crossed random effects because I get
> an error like this:
>
> Error: no. random effects (=135) >= no. observations (=100)
>
>
> set.seed(1234)
> library(ordinal)
> library(glmmTMB)
> library(spaMM)
> dati<- data.frame(fledges= rpois(100,10), habitatF=
> as.factor(rbinom(100,1,0.5)), areaPatchFath= rnorm(100), poligF01=
> as.factor(rbinom(100,1,0.5)),StdLayingDate= rnorm(100), ageFath1=
> rpois(100,3), ageMoth1= rpois(100,3), year= as.factor(rpois(100,200)),
> ringMoth= as.factor(rpois(100,200)), ringFath= as.factor(rpois(100,200)))
> str(dati)
>
> system.time(Fitclm<- clmm(as.factor(fledges) ~
> habitatF*(areaPatchFath+poligF01+StdLayingDate+
> ageFath1+ageMoth1)+(1|year)+(1|ringMoth)+(1|ringFath),data=dati,Hess=T))
> # this way it works...
> system.time(Fitclm1<- clmm(as.factor(fledges) ~
> habitatF*(areaPatchFath+poligF01+StdLayingDate+
> ageFath1+ageMoth1)+(1|year)+(1|ringFath),data=dati,Hess=T))
> summary(Fitclm1)
>
> system.time(FitglmmTMB<- glmmTMB(as.factor(fledges) ~
> habitatF*(areaPatchFath+poligF01+StdLayingDate+
> ageFath1+ageMoth1)+(1|year)+(1|ringMoth)+(1|ringFath),data=dati,family=
> "compois"))
> summary(FitglmmTMB)
>
> system.time(FitglmmTMB<- glmmTMB(as.factor(fledges) ~
> habitatF*(areaPatchFath+poligF01+StdLayingDate+
> ageFath1+ageMoth1)+(1|year)+(1|ringMoth)+(1|ringFath),data=dati,family=
> "compois"))
> summary(FitglmmTMB)
>
> # This lasts much more (3-4')
> system.time(Fitfitme<- fitme(fledges ~
> habitatF*(areaPatchFath+poligF01+StdLayingDate+
> ageFath1+ageMoth1)+(1|year)+(1|ringFath)+(1|ringMoth),data=
> dati,COMPoisson(),method
> = "ML"))
> summary(Fitfitme)
>
>
>
>
>
>
>
>
>
>
>
> El 08/12/2016 a las 4:32, Ben Bolker escribi?:
> >     One reference that uses ordinal regression in a similar situation
> > (litter size of Florida panthers) is
> > http://link.springer.com/article/10.1007/s00442-011-2083-0  ("Does
> > genetic introgression improve female reproductive performance? A test on
> > the endangered Florida panther")
> >
> >    Not sure about the number-of-random-effects error: a reproducible
> > example would probably be needed (smaller is better!)
> >
> >    Ben Bolker
> >
> >
> > On 16-12-06 08:41 AM, Simone Santoro wrote:
> >> Dear all,
> >>
> >> I am trying to find an appropriate GLMM (with temporal and individual
> >> crossed random effects) to model underdispersed count data (clutch
> >> size). I have found several possible ways of doing that. A good
> >> distribution for data like this would seem to be the
> >> Conway-Maxwell-Poisson but I have not found a way of using it within a
> >> GLMM in R (I have asked here
> >> <http://stats.stackexchange.com/questions/249738/how-to-
> define-the-nu-parameter-of-conway-maxwell-poisson-in-spamm-package>
> >> and here
> >> <http://stats.stackexchange.com/questions/249798/conway-
> maxwell-poisson-with-crossed-random-effects-in-r>).
> >> I have seen that Ben Bolker suggested (here
> >> <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q1/021945.html
> >and
> >> here
> >> <http://stats.stackexchange.com/questions/92156/how-to-
> handle-underdispersion-in-glmm-binomial-outcome-variable>)
> >> to use an ordinal model in cases like this(e.g. _ordinal:clmm_). I have
> >> tried this solution and the results I obtain makes (biological) sense to
> >> me. However, I wonder why but I cannot put all the three crossed random
> >> effects I have in the clmm model (_Error: no. random effects (=1254) >=
> >> no. observations (=854)_) whereas it is not a problem for the glmer
> >> model (the no. of levels of each single random effect does not exceed
> 854)*.
> >> Beyond that, and that's what I would like to ask you, *I cannot find a
> >> reference to justify I used the ordinal model* to deal with
> >> underdispersed count data (referee will ask it for sure).
> >> Best,
> >>
> >> Simone
> >>
> >> * FMglmer<- glmer(fledges ~ habitatF * (areaPatchFath + poligF01 +
> >> StdLayingDate + ageFath1 + ageMoth1) + (1|year) + (1|ringMoth) +
> >> (1|ringFath), data = datiDRS)
> >>      FMclmm<- glmer(as.factor(fledges)~ habitatF * (areaPatchFath +
> >> poligF01 + StdLayingDate + ageFath1 + ageMoth1) + (1|year) +
> >> (1|ringMoth) + (1|ringFath), data = datiDRS)
> >>
> >>
> >>      [[alternative HTML version deleted]]
> >>
> >> _______________________________________________
> >> R-sig-mixed-models at r-project.org  mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> >>
>
> --
> Simone Santoro
> PhD
> Department of Ethology and Biodiversity Conservation
> Do?ana Biological Station
> Calle Am?rico Vespucio s/n
> 41092 Seville - Spain
> Phone no. +34 954 466 700 (ext. 1213)
> http://www.researchgate.net/profile/Simone_Santoro
> http://orcid.org/0000-0003-0986-3278
>
>
>         [[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From drki.musa at gmail.com  Sat Dec 10 10:57:05 2016
From: drki.musa at gmail.com (K Imran M)
Date: Sat, 10 Dec 2016 17:57:05 +0800
Subject: [R-sig-ME] p-values from lme::anova VS fixed-effects of lme
Message-ID: <CADop7+gRigz_pM3OxUR2GVq2YfBocOkg3Ee1OyQOyO00P6dQzQ@mail.gmail.com>

Dear all,


Firstly, my apology if this question is trivial. This question is related
to this post at
http://stats.stackexchange.com/questions/7185/what-is-the-difference-between-using-aov-and-lme-in-analyzing-a-longitudinal
.


My I ask a question is: If I rerun these codes (lme)


#### LME ######

res <- lme(distance ~ age*Sex, random = ~ 1 | Subject, data = Orthodont)

summary(res)

################


I would obtain this

#####################################################

Fixed effects: distance ~ age * Sex

                  Value Std.Error DF   t-value p-value

(Intercept)   16.340625 0.9813122 79 16.651810  0.0000

age            0.784375 0.0775011 79 10.120823  0.0000

SexFemale      1.032102 1.5374208 25  0.671321  0.5082

age:SexFemale -0.304830 0.1214209 79 -2.510520  0.0141

########################################################


Next, if I run 'anova(res)', I would obtain this


###ANOVA####

           numDF denDF  F-value p-value

(Intercept)     1    79 4123.156  <.0001

age             1    79  122.450  <.0001

Sex             1    25    9.292  0.0054

age:Sex         1    79    6.303  0.0141

###############


I do not understand why the p-values for age and age:Sex (from anova) are
similar with (lme::anova) BUT for sex: it is different (anova, p-val =
0.0054 vs lme::anova, p-val = 0.508).


Your kind attention is very much appreciated.


Thanks and best wishes,


Kamarul
-- 
Dr. Kamarul Imran Musa (MD MCommunityMed)
Associate Professor (Epidemiology and Biostatistics) &
Public Health Physician,

Address:
Dept of Community Medicine,
School of Medical Sciences,
Universiti Sains Malaysia,
16150 Kbg Kerian Kelantan
MALAYSIA

ResearcherID: http://www.researcherid.com/rid/N-3198-2015
Google-scholar: 'Kamarul Imran Musa'  at https://goo.gl/D3o3y6
ORCID ID: orcid.org/0000-0002-3708-0628
ScopusID: 18634847200
Personal web: www.myanalytics.com.my
Alt emails : drkamarul at usm.my , k.musa at lancaster.ac.uk

	[[alternative HTML version deleted]]


From karl at huftis.org  Sun Dec 11 11:08:55 2016
From: karl at huftis.org (Karl Ove Hufthammer)
Date: Sun, 11 Dec 2016 11:08:55 +0100
Subject: [R-sig-ME] p-values from lme::anova VS fixed-effects of lme
In-Reply-To: <CADop7+gRigz_pM3OxUR2GVq2YfBocOkg3Ee1OyQOyO00P6dQzQ@mail.gmail.com>
References: <CADop7+gRigz_pM3OxUR2GVq2YfBocOkg3Ee1OyQOyO00P6dQzQ@mail.gmail.com>
Message-ID: <87db5df5-c9ce-8506-b482-9568b3486955@huftis.org>

K Imran M skreiv 10. des. 2016 10:57:
> #### LME ######
>
> res <- lme(distance ~ age*Sex, random = ~ 1 | Subject, data = Orthodont)
>
> summary(res)
> [?]
> Next, if I run 'anova(res)', I would obtain this
> [?]
> I do not understand why the p-values for age and age:Sex (from anova) are
> similar with (lme::anova) BUT for sex: it is different (anova, p-val =
> 0.0054 vs lme::anova, p-val = 0.508).

The P-values are only identical for the interaction, *not* for age and 
*not* for Sex. The P-values for age just happened to both be very small. 
It?s easier to see this for a smaller dataset, with larger P-values:

     set.seed(1)
     d = Orthodont[sample(nrow(Orthodont), 20),]
     res = lme(distance ~ age*Sex, random = ~ 1 | Subject, data = d)

Now, summary(res) gives us:

                   Value Std.Error DF   t-value p-value
(Intercept)   19.259688 1.7255889 13 11.161226  0.0000
age            0.581252 0.1428037  3  4.070286  0.0268
SexFemale     -1.859006 2.3131153 13 -0.803681  0.4360
age:SexFemale -0.224017 0.1865328  3 -1.200955  0.3159

while anova(res) gives us:

             numDF denDF   F-value p-value
(Intercept)     1    13 1306.1220  <.0001
age             1     3   24.4811  0.0158
Sex             1    13   10.3881  0.0067
age:Sex         1     3    1.4423  0.3159

So only the interaction age:Sex gives the same P-value. The reason is 
that anova(res) gives the *sequential* tests (first the effect of age, 
then the effect of Sex given age, and then the effect of age:Sex given 
age and sex). Only the last test will be identical.

For  a similar test using anova(), you have to request marginal tests:

 > anova(res, type="marginal")
             numDF denDF   F-value p-value
(Intercept)     1    13 124.57297  <.0001
age             1     3  16.56723  0.0268
Sex             1    13   0.64590  0.4360
age:Sex         1     3   1.44229  0.3159

Mind you, the marginal tests (usually) don?t make much sense when you 
have an interaction. In your original example, the marginal P-value of 
0.51 for ?Sex? does *not* mean that sex doesn?t have an effect on the 
outcome variable. Since the age:Sex interaction is statistically 
significant (P-value ~0.01), Sex *obviously* has en effect; it?s just 
that the effect of sex depends on the age of the individual. And the 
P-values for the ?marginal effects? depend very much on the 
parametrisation. For example, if you change ?age? by adding or 
subtracting a constant (e.g. instead of measuring age as ?years from 
birth?, you measure it as ?years since starting school? or ?years until 
the person turns eighteen?), the ?marginal? P-value for ?Sex? will 
change (perhaps to a very low or high P-value).

-- 
Karl Ove Hufthammer


From rune.haubo at gmail.com  Mon Dec 12 08:38:48 2016
From: rune.haubo at gmail.com (Rune Haubo)
Date: Mon, 12 Dec 2016 08:38:48 +0100
Subject: [R-sig-ME] clmm2() thresholds - forcing symmetry around zero
In-Reply-To: <MEXPR01MB06641997EAA231B509D0BD75B6850@MEXPR01MB0664.ausprd01.prod.outlook.com>
References: <MEXPR01MB06641997EAA231B509D0BD75B6850@MEXPR01MB0664.ausprd01.prod.outlook.com>
Message-ID: <CAG_uk91cHFrkv_g2UnVQ5RGHzU6W1xXVb6giQDsi3jASi5EO+A@mail.gmail.com>

It seems that you are looking for clmm(...., threshold="symmetric2").
This threshold argument is valid for clmm, but apparently I forgot to
document it for clmm, so take a look at help(clm) instead.

> library(ordinal)
> fmm1 <- clmm(rating ~ temp + contact + (1|judge), data=wine,
+ threshold="symmetric2")
> logLik(fmm1)
'log Lik.' -89.97856 (df=5)

If you want to fix one or more parameters while optimizing the rest,
you will have to use fmm2 <- clmm( ...., doFit=FALSE). fmm2 is a
'model environment' which you can optimize, e.g. using, say nlminb or
ucminf:

> fmm2 <- clmm(rating ~ temp + contact + (1|judge), data=wine,
+ threshold="symmetric2", doFit=FALSE)
> obj.fun <- ordinal:::getNLA.ssr # objective function: negative Laplace log-likelihood
> obj.fun(fmm2)
[1] 98.5057
> library(ucminf)
> fit <- ucminf(fmm2$par, function(par) obj.fun(fmm2, par))
> fit$value
[1] 89.97856

Fixing one or more parameters means modifying 'function(par)
obj.fun(fmm2, par)' to only optimize over select parameters.

Cheers
Rune

On 7 December 2016 at 04:08, Kristin Precoda <kristin.precoda at mq.edu.au> wrote:
> Hi,
>
> I'm using clmm2() to fit a model with three possible response categories (win, tie, lose).  For the particular problem, the thresholds need to be symmetric around zero, so win|tie = -1 * tie|lose.  Is there a way either to force the thresholds to be symmetric around zero, or to just set the thresholds (for example, to -1 and +1) and not optimize them?
>
>
> Thanks very much,
>
> Kristin
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From oyangxz0412 at 163.com  Mon Dec 12 03:14:04 2016
From: oyangxz0412 at 163.com (=?GBK?B?xbfR9M/m19M=?=)
Date: Mon, 12 Dec 2016 10:14:04 +0800 (CST)
Subject: [R-sig-ME] Questions of nlme package in R
Message-ID: <3d93895f.27e6.158f0cef28c.Coremail.oyangxz0412@163.com>

Dear professor??
hello??my name is Sharon. I have a question about the nlme package in R. When apply this package to do multilevel model??the function is ??lme??. Why the summary part didn't show the significance test of the random effect??
I will appreciate that you could answer my questions. Looking forward to your reply??thank you??


regards??
Sharon


	[[alternative HTML version deleted]]


From bbolker at gmail.com  Mon Dec 12 20:03:03 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 12 Dec 2016 14:03:03 -0500
Subject: [R-sig-ME] Questions of nlme package in R
In-Reply-To: <3d93895f.27e6.158f0cef28c.Coremail.oyangxz0412@163.com>
References: <3d93895f.27e6.158f0cef28c.Coremail.oyangxz0412@163.com>
Message-ID: <CABghstTy=Dkd_42feKA45T4NfWtH2y=d3jREPP=UuC2eMDdM0g@mail.gmail.com>

Could you please send this to r-sig-mixed-models at r-project.org, after
subscribing to the list (see
https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models ) ?

  thanks
   Ben Bolker


On Sun, Dec 11, 2016 at 9:14 PM, ???? <oyangxz0412 at 163.com> wrote:
> Dear professor?
> hello?my name is Sharon. I have a question about the nlme package in R. When apply this package to do multilevel model?the function is ?lme?. Why the summary part didn't show the significance test of the random effect?
> I will appreciate that you could answer my questions. Looking forward to your reply?thank you?
>
>
> regards?
> Sharon
>
>
>         [[alternative HTML version deleted]]
>
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbolker at gmail.com  Mon Dec 12 20:13:38 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 12 Dec 2016 14:13:38 -0500
Subject: [R-sig-ME] Questions of nlme package in R
In-Reply-To: <3d93895f.27e6.158f0cef28c.Coremail.oyangxz0412@163.com>
References: <3d93895f.27e6.158f0cef28c.Coremail.oyangxz0412@163.com>
Message-ID: <CABghstR-US3Cr1HFakptyGdeadbTzmmZdhCgW4jni4v7e7e1EQ@mail.gmail.com>

On Sun, Dec 11, 2016 at 9:14 PM, ???? <oyangxz0412 at 163.com> wrote:
> Dear professor?
> hello?my name is Sharon. I have a question about the nlme package in R. When apply this package to do multilevel model?the function is ?lme?. Why the summary part didn't show the significance test of the random effect?
> I will appreciate that you could answer my questions. Looking forward to your reply?thank you?
>
>
> regards?
> Sharon
>
>

  (Sorry; I replied privately that this should go to the list but
didn't see that you had *already* sent it to the list.)

   If you read Pinheiro and Bates 2000 (Mixed-effect models in S and
S-PLUS) you'll see that in the modern mixed model framework, testing
variance components is a little tricky; typically you can use
likelihood ratio tests, but they are known to be conservative.
http://tinyurl.com/glmmFAQ#testing-significance-of-random-effects also
has a bit of information, although it doesn't give the basic recipe:
to test the significance of a random effect, fit the model with (call
the fitted model m1) and without (m0) the random effect, and use

anova(m0,m1)

to perform a likelihood ratio test.

  sincerely
    Ben Bolker


From dan_476 at hotmail.co.uk  Tue Dec 13 09:48:44 2016
From: dan_476 at hotmail.co.uk (Dan Jackson)
Date: Tue, 13 Dec 2016 08:48:44 +0000
Subject: [R-sig-ME] "varFunc" classes
Message-ID: <DB5PR0201MB1574EC6E9783D33B81E7E8B4939B0@DB5PR0201MB1574.eurprd02.prod.outlook.com>

Dear lme4 authors,

I am sure you are very busy so I will just ask my question very quickly. I
was reading the book "Mixed-effects models in S and S-plus" by Pinheiro and
Bates. On the top of page 208 of this book, there is a Table 5.1 that
implements various "varFunc" classes. One of these classes would seem to be
what I need for my data: varIdent - different variances per stratum. I do
know that different subets in my data have very different variances you see,
so I would need to include this.

However this book relates to S-plus and I am not sure if this has been
implemented in R, in the glmer package? My data are continuous so I would
just need this for lmer (and not glmer). If it has not been implemented is
there any "workaround"?

Thanks in advance for any advice, Dan Jackson


	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Tue Dec 13 10:15:38 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Tue, 13 Dec 2016 10:15:38 +0100
Subject: [R-sig-ME] "varFunc" classes
In-Reply-To: <DB5PR0201MB1574EC6E9783D33B81E7E8B4939B0@DB5PR0201MB1574.eurprd02.prod.outlook.com>
References: <DB5PR0201MB1574EC6E9783D33B81E7E8B4939B0@DB5PR0201MB1574.eurprd02.prod.outlook.com>
Message-ID: <CAJuCY5yG+5TaAHE8edcLR+mwKEr=+m+AEwH4LtZRfh2hgKeedQ@mail.gmail.com>

Dear Dan,

You are looking for the nlme package.

Best regards,

Thierry

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-12-13 9:48 GMT+01:00 Dan Jackson <dan_476 at hotmail.co.uk>:

> Dear lme4 authors,
>
> I am sure you are very busy so I will just ask my question very quickly. I
> was reading the book "Mixed-effects models in S and S-plus" by Pinheiro and
> Bates. On the top of page 208 of this book, there is a Table 5.1 that
> implements various "varFunc" classes. One of these classes would seem to be
> what I need for my data: varIdent - different variances per stratum. I do
> know that different subets in my data have very different variances you
> see,
> so I would need to include this.
>
> However this book relates to S-plus and I am not sure if this has been
> implemented in R, in the glmer package? My data are continuous so I would
> just need this for lmer (and not glmer). If it has not been implemented is
> there any "workaround"?
>
> Thanks in advance for any advice, Dan Jackson
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From wolfgang.viechtbauer at maastrichtuniversity.nl  Tue Dec 13 10:17:40 2016
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (SP))
Date: Tue, 13 Dec 2016 09:17:40 +0000
Subject: [R-sig-ME] "varFunc" classes
In-Reply-To: <DB5PR0201MB1574EC6E9783D33B81E7E8B4939B0@DB5PR0201MB1574.eurprd02.prod.outlook.com>
References: <DB5PR0201MB1574EC6E9783D33B81E7E8B4939B0@DB5PR0201MB1574.eurprd02.prod.outlook.com>
Message-ID: <d8ab5061183e4c448f9624adf23c4580@UM-MAIL3216.unimaas.nl>

Hi Dan (nice to see you on this list),

Pinheiro and Bates (2000) covers the nlme package, which was ported to R. So, the book is just as much about S/S-Plus as it is about R.

If you are fitting linear mixed-effects models, then you can do that with the lme() function from nlme or the lmer() function from lme4 (and various other packages). Only lme() supports the various varFunc classes, so you should use that.

Best,
Wolfgang

-- 
Wolfgang Viechtbauer, Ph.D., Statistician | Department of Psychiatry and    
Neuropsychology | Maastricht University | P.O. Box 616 (VIJV1) | 6200 MD    
Maastricht, The Netherlands | +31 (43) 388-4170 | http://www.wvbauer.com    

> -----Original Message-----
> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-
> project.org] On Behalf Of Dan Jackson
> Sent: Tuesday, December 13, 2016 09:49
> To: r-sig-mixed-models at r-project.org; daniel.jackson at mrc-bsu.cam.ac.uk
> Subject: [R-sig-ME] "varFunc" classes
> 
> Dear lme4 authors,
> 
> I am sure you are very busy so I will just ask my question very quickly.
> I was reading the book "Mixed-effects models in S and S-plus" by Pinheiro
> and Bates. On the top of page 208 of this book, there is a Table 5.1 that
> implements various "varFunc" classes. One of these classes would seem to
> be what I need for my data: varIdent - different variances per stratum. I
> do know that different subets in my data have very different variances
> you see, so I would need to include this.
> 
> However this book relates to S-plus and I am not sure if this has been
> implemented in R, in the glmer package? My data are continuous so I would
> just need this for lmer (and not glmer). If it has not been implemented
> is there any "workaround"?
> 
> Thanks in advance for any advice, Dan Jackson
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From gustaf.granath at gmail.com  Tue Dec 13 11:42:23 2016
From: gustaf.granath at gmail.com (Gustaf Granath)
Date: Tue, 13 Dec 2016 11:42:23 +0100
Subject: [R-sig-ME] metafor: estimate correlation between response variables
 (meta-analysis)
Message-ID: <2c4b2c6d-d3c1-8c16-1ce4-a74ca4783cfc@gmail.com>

Hi,
Im trying the estimate the correlation between two response variable in 
a meta-analysis. Basically, are effects on y1 associated with effects on 
y2 across studies. Unfortunately, I dont have information on y1-y2 
correlations within each study. In addition, each study contain multiple 
treatments, adding within-study dependence for each response variable.

Because I dont have the y1-y2 covariance in each study, my idea is to 
run analyses with different covariance/correlation values to explore how 
this covariance affect the result. Reading the rma.mv() documentation, 
this seems possible through the R= argument, but I cant figure out how. 
Alternatively, I can add covariances in the matrix describing the known 
var-covariance matrix of the within-study dependence, but it seemed like 
the R argument is a easier solution (I may be wrong though). Below 
follow code illustrating my problem using the dat.berkey1998 example.

Cheers

Gustaf

# Estimate correlation between two response variables
# with within-study dependence
library(metafor)

# Make up an example based on the berkey1998 data.
# Multiple outcomes (multiple treatments)
# within each study, for each response variable, are created.
# Covariances between response variables (within studies) are unknown.
dat <- get(data(dat.berkey1998))
st.vcov <- dat[, c("v1i", "v2i")] # save vcov matrices to add later
dat <- rbind(dat, dat)
dat <- dat[order(dat$trial, dat$outcome),] # fix order
dat[,c("v1i", "v2i")] <- rbind(st.vcov,st.vcov) # put back matrices
dat$trial.treat <-paste(rep(1:2, nrow(dat)/2), dat$trial, sep="_") # id 
for treatments within trials

# Covariances within studies, for each response variable, is known. 
Hence, a
# varcov-matrix for each study and response, can be made and added as blocks
# in a large varcov-matrix for the data set.
# First a within-study dependence dummy must be added
dat$stud.unit <-  interaction(dat$trial, dat$outcome) # within-trial 
dependence dummy
# Put together known varcov matrix
V <- bldiag(lapply(split(dat[,c("v1i", "v2i")], dat$stud.unit), as.matrix))

# plot relationship between response variables
dat.wide <- reshape(dat, direction="wide",  v.names = "yi", timevar = 
"outcome", idvar = "trial.treat")
plot(yi.AL ~yi.PD, dat.wide)
cor(dat.wide$yi.AL, dat.wide$yi.PD)
# r = 0.39, if using the study outcomes ignoring all dependence/uncertainty.

# Run analysis
res <- rma.mv(yi, V, mods = ~ outcome - 1,
               random = ~ outcome | trial,
               struct="UN", data=dat, method="REML")
print(res, digits=3)
# correlation = 0.51, when accounting for dependence etc. But, y1 and y2 
are assumed to be independent
# within each study. How to perform sensitivity analyses by adding 
different values
# on this within-study correlation between the two response variables?

-- 
Gustaf Granath (PhD)
Post doc
Swedish University of Agricultural Sciences
Department of Ecology


From wolfgang.viechtbauer at maastrichtuniversity.nl  Tue Dec 13 13:41:00 2016
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (SP))
Date: Tue, 13 Dec 2016 12:41:00 +0000
Subject: [R-sig-ME] metafor: estimate correlation between response
 variables (meta-analysis)
In-Reply-To: <2c4b2c6d-d3c1-8c16-1ce4-a74ca4783cfc@gmail.com>
References: <2c4b2c6d-d3c1-8c16-1ce4-a74ca4783cfc@gmail.com>
Message-ID: <7baa7258b2604e1d858e951be1a08e8e@UM-MAIL3216.unimaas.nl>

I have a hard time making sense of that toy example.

But if I understand you correctly, you have data like this:

study trt respvar yi vi
-----------------------
1     1   1       .  .
1     1   2       .  .
1     2   1       .  .
1     2   2       .  .
2     1   1       .  .
2     1   2       .  .
2     2   1       .  .
2     2   2       .  .
...

where 'yi' and 'vi' are the observed outcomes and corresponding variances.

Within studies, can we assume that 'yi' is independent for different treatments? Then V (the var-cov matrix of the 'yi' vector) will be block-diagonal, each block being a 2x2 matrix (since you only need to consider the covariance between 'yi' for respvar 1 and 'yi' for respvar 2). And the problem is that the covariances are unknown. Correct so far?

If so, the 'R' argument has nothing to do with this. If you want to approach this by means of a sensitivity analysis, then just impute the unknown covariances directly into 'V' and analyze by means of an appropriate multilevel/multivariate model. Something like this:

dat$study.trt <- interaction(dat$study, dat$trt)
rma.mv(yi, V, mods = ~ factor(trt) - 1, random = list(~ 1 | study, ~ respvar | study.trt), struct="UN", data=dat)

To impute the covariances, you may be able to use cor*sqrt(v1*v2), where 'cor' is some kind of assumed correlation (maybe constant across studies, maybe not). However, whether this is appropriate depends on your outcome measure. For example, this would be fine for means or mean differences, but the covariance between standardized mean differences cannot be computed that way (see Gleser & Olkin, 2009, for the correct equation).

Gleser, L. J., & Olkin, I. (2009). Stochastically dependent effect sizes. In H. Cooper, L. V. Hedges, & J. C. Valentine (Eds.), The handbook of research synthesis and meta-analysis (2nd ed., pp. 357-376). New York: Russell Sage Foundation.

Best,
Wolfgang

> -----Original Message-----
> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-
> project.org] On Behalf Of Gustaf Granath
> Sent: Tuesday, December 13, 2016 11:42
> To: r-sig-mixed-models at r-project.org
> Subject: [R-sig-ME] metafor: estimate correlation between response
> variables (meta-analysis)
> 
> Hi,
> Im trying the estimate the correlation between two response variable in
> a meta-analysis. Basically, are effects on y1 associated with effects on
> y2 across studies. Unfortunately, I dont have information on y1-y2
> correlations within each study. In addition, each study contain multiple
> treatments, adding within-study dependence for each response variable.
> 
> Because I dont have the y1-y2 covariance in each study, my idea is to
> run analyses with different covariance/correlation values to explore how
> this covariance affect the result. Reading the rma.mv() documentation,
> this seems possible through the R= argument, but I cant figure out how.
> Alternatively, I can add covariances in the matrix describing the known
> var-covariance matrix of the within-study dependence, but it seemed like
> the R argument is a easier solution (I may be wrong though). Below
> follow code illustrating my problem using the dat.berkey1998 example.
> 
> Cheers
> 
> Gustaf
> 
> # Estimate correlation between two response variables
> # with within-study dependence
> library(metafor)
> 
> # Make up an example based on the berkey1998 data.
> # Multiple outcomes (multiple treatments)
> # within each study, for each response variable, are created.
> # Covariances between response variables (within studies) are unknown.
> dat <- get(data(dat.berkey1998))
> st.vcov <- dat[, c("v1i", "v2i")] # save vcov matrices to add later
> dat <- rbind(dat, dat)
> dat <- dat[order(dat$trial, dat$outcome),] # fix order
> dat[,c("v1i", "v2i")] <- rbind(st.vcov,st.vcov) # put back matrices
> dat$trial.treat <-paste(rep(1:2, nrow(dat)/2), dat$trial, sep="_") # id
> for treatments within trials
> 
> # Covariances within studies, for each response variable, is known.
> Hence, a
> # varcov-matrix for each study and response, can be made and added as
> blocks
> # in a large varcov-matrix for the data set.
> # First a within-study dependence dummy must be added
> dat$stud.unit <-  interaction(dat$trial, dat$outcome) # within-trial
> dependence dummy
> # Put together known varcov matrix
> V <- bldiag(lapply(split(dat[,c("v1i", "v2i")], dat$stud.unit),
> as.matrix))
> 
> # plot relationship between response variables
> dat.wide <- reshape(dat, direction="wide",  v.names = "yi", timevar =
> "outcome", idvar = "trial.treat")
> plot(yi.AL ~yi.PD, dat.wide)
> cor(dat.wide$yi.AL, dat.wide$yi.PD)
> # r = 0.39, if using the study outcomes ignoring all
> dependence/uncertainty.
> 
> # Run analysis
> res <- rma.mv(yi, V, mods = ~ outcome - 1,
>                random = ~ outcome | trial,
>                struct="UN", data=dat, method="REML")
> print(res, digits=3)
> # correlation = 0.51, when accounting for dependence etc. But, y1 and y2
> are assumed to be independent
> # within each study. How to perform sensitivity analyses by adding
> different values
> # on this within-study correlation between the two response variables?
> 
> --
> Gustaf Granath (PhD)
> Post doc
> Swedish University of Agricultural Sciences
> Department of Ecology
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From gustaf.granath at gmail.com  Tue Dec 13 15:55:02 2016
From: gustaf.granath at gmail.com (Gustaf Granath)
Date: Tue, 13 Dec 2016 15:55:02 +0100
Subject: [R-sig-ME] metafor: estimate correlation between response
 variables (meta-analysis)
In-Reply-To: <7baa7258b2604e1d858e951be1a08e8e@UM-MAIL3216.unimaas.nl>
References: <2c4b2c6d-d3c1-8c16-1ce4-a74ca4783cfc@gmail.com>
	<7baa7258b2604e1d858e951be1a08e8e@UM-MAIL3216.unimaas.nl>
Message-ID: <eadd2e69-36c1-8a9f-b2d8-b80836e5bfa6@gmail.com>

Hi,

Im sorry if my toy example wasnt clear. You almost got it right though. 
But yi is not independent for different treatments within a study. I 
have the estimated covariance though, so no problem to account for this. 
In my example I did include this covariance when I constructed V (I used 
the outcome covariance in the berkey1998 data to illustrate this). If I 
understand you correctly, the best way check the influence of 
within-study correlation between responses, is to manually add 
covariances (try a range of reasonable values) between responses in V. I 
will try that - although, Im not sure that it is easy to implement.

Thanks,

Gustaf


On 2016-12-13 13:41, Viechtbauer Wolfgang (SP) wrote:
> I have a hard time making sense of that toy example.
>
> But if I understand you correctly, you have data like this:
>
> study trt respvar yi vi
> -----------------------
> 1     1   1       .  .
> 1     1   2       .  .
> 1     2   1       .  .
> 1     2   2       .  .
> 2     1   1       .  .
> 2     1   2       .  .
> 2     2   1       .  .
> 2     2   2       .  .
> ...
>
> where 'yi' and 'vi' are the observed outcomes and corresponding variances.
>
> Within studies, can we assume that 'yi' is independent for different treatments? Then V (the var-cov matrix of the 'yi' vector) will be block-diagonal, each block being a 2x2 matrix (since you only need to consider the covariance between 'yi' for respvar 1 and 'yi' for respvar 2). And the problem is that the covariances are unknown. Correct so far?
>
> If so, the 'R' argument has nothing to do with this. If you want to approach this by means of a sensitivity analysis, then just impute the unknown covariances directly into 'V' and analyze by means of an appropriate multilevel/multivariate model. Something like this:
>
> dat$study.trt <- interaction(dat$study, dat$trt)
> rma.mv(yi, V, mods = ~ factor(trt) - 1, random = list(~ 1 | study, ~ respvar | study.trt), struct="UN", data=dat)
>
> To impute the covariances, you may be able to use cor*sqrt(v1*v2), where 'cor' is some kind of assumed correlation (maybe constant across studies, maybe not). However, whether this is appropriate depends on your outcome measure. For example, this would be fine for means or mean differences, but the covariance between standardized mean differences cannot be computed that way (see Gleser & Olkin, 2009, for the correct equation).
>
> Gleser, L. J., & Olkin, I. (2009). Stochastically dependent effect sizes. In H. Cooper, L. V. Hedges, & J. C. Valentine (Eds.), The handbook of research synthesis and meta-analysis (2nd ed., pp. 357-376). New York: Russell Sage Foundation.
>
> Best,
> Wolfgang
>
>> -----Original Message-----
>> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-
>> project.org] On Behalf Of Gustaf Granath
>> Sent: Tuesday, December 13, 2016 11:42
>> To: r-sig-mixed-models at r-project.org
>> Subject: [R-sig-ME] metafor: estimate correlation between response
>> variables (meta-analysis)
>>
>> Hi,
>> Im trying the estimate the correlation between two response variable in
>> a meta-analysis. Basically, are effects on y1 associated with effects on
>> y2 across studies. Unfortunately, I dont have information on y1-y2
>> correlations within each study. In addition, each study contain multiple
>> treatments, adding within-study dependence for each response variable.
>>
>> Because I dont have the y1-y2 covariance in each study, my idea is to
>> run analyses with different covariance/correlation values to explore how
>> this covariance affect the result. Reading the rma.mv() documentation,
>> this seems possible through the R= argument, but I cant figure out how.
>> Alternatively, I can add covariances in the matrix describing the known
>> var-covariance matrix of the within-study dependence, but it seemed like
>> the R argument is a easier solution (I may be wrong though). Below
>> follow code illustrating my problem using the dat.berkey1998 example.
>>
>> Cheers
>>
>> Gustaf
>>
>> # Estimate correlation between two response variables
>> # with within-study dependence
>> library(metafor)
>>
>> # Make up an example based on the berkey1998 data.
>> # Multiple outcomes (multiple treatments)
>> # within each study, for each response variable, are created.
>> # Covariances between response variables (within studies) are unknown.
>> dat <- get(data(dat.berkey1998))
>> st.vcov <- dat[, c("v1i", "v2i")] # save vcov matrices to add later
>> dat <- rbind(dat, dat)
>> dat <- dat[order(dat$trial, dat$outcome),] # fix order
>> dat[,c("v1i", "v2i")] <- rbind(st.vcov,st.vcov) # put back matrices
>> dat$trial.treat <-paste(rep(1:2, nrow(dat)/2), dat$trial, sep="_") # id
>> for treatments within trials
>>
>> # Covariances within studies, for each response variable, is known.
>> Hence, a
>> # varcov-matrix for each study and response, can be made and added as
>> blocks
>> # in a large varcov-matrix for the data set.
>> # First a within-study dependence dummy must be added
>> dat$stud.unit <-  interaction(dat$trial, dat$outcome) # within-trial
>> dependence dummy
>> # Put together known varcov matrix
>> V <- bldiag(lapply(split(dat[,c("v1i", "v2i")], dat$stud.unit),
>> as.matrix))
>>
>> # plot relationship between response variables
>> dat.wide <- reshape(dat, direction="wide",  v.names = "yi", timevar =
>> "outcome", idvar = "trial.treat")
>> plot(yi.AL ~yi.PD, dat.wide)
>> cor(dat.wide$yi.AL, dat.wide$yi.PD)
>> # r = 0.39, if using the study outcomes ignoring all
>> dependence/uncertainty.
>>
>> # Run analysis
>> res <- rma.mv(yi, V, mods = ~ outcome - 1,
>>                 random = ~ outcome | trial,
>>                 struct="UN", data=dat, method="REML")
>> print(res, digits=3)
>> # correlation = 0.51, when accounting for dependence etc. But, y1 and y2
>> are assumed to be independent
>> # within each study. How to perform sensitivity analyses by adding
>> different values
>> # on this within-study correlation between the two response variables?
>>
>> --
>> Gustaf Granath (PhD)
>> Post doc
>> Swedish University of Agricultural Sciences
>> Department of Ecology
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From john.morrongiello at unimelb.edu.au  Wed Dec 14 06:56:02 2016
From: john.morrongiello at unimelb.edu.au (John Morrongiello)
Date: Wed, 14 Dec 2016 05:56:02 +0000
Subject: [R-sig-ME] multivariate normality (MVN) assumption in MCMCglmm
Message-ID: <MEXPR01MB0565018C6EF158CD721360BCC49A0@MEXPR01MB0565.ausprd01.prod.outlook.com>

Dear list,
I've fit a multivariate mixed model using the MCMCglmm package with six response variables. I have log transformed some of these responses and assume they follow a multivariate normal distribution. Univariate plots of response variables indicate that they look pretty 'normal'. I have two questions:


1)      Is there a more formal way than univariate histograms/qqplots to test that the assumption of multivariate normality is met in a multivariate mixed model?

2)      How robust are multivariate mixed models to the assumption of MVN?

I've previously used tests like Mardia's MVN test, Rayston's MVN test (MVN package) and the Shapiro-Wilk MVN test(mvnnormtest package). However, I get the feeling they are not necessarily doing what I think they are as the response variables are strongly conditional on other fixed effects in the multivariate MCMCglmm, and the data are not independent (i.e. repeated measures of each trait nested within individuals).

Cheers
John

	[[alternative HTML version deleted]]


From wolfgang.viechtbauer at maastrichtuniversity.nl  Wed Dec 14 11:14:00 2016
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (SP))
Date: Wed, 14 Dec 2016 10:14:00 +0000
Subject: [R-sig-ME] metafor: estimate correlation between response
 variables (meta-analysis)
In-Reply-To: <eadd2e69-36c1-8a9f-b2d8-b80836e5bfa6@gmail.com>
References: <2c4b2c6d-d3c1-8c16-1ce4-a74ca4783cfc@gmail.com>
	<7baa7258b2604e1d858e951be1a08e8e@UM-MAIL3216.unimaas.nl>
	<eadd2e69-36c1-8a9f-b2d8-b80836e5bfa6@gmail.com>
Message-ID: <4f26495753164d03973a2da4de2b5478@UM-MAIL3216.unimaas.nl>

So there really is a 4x4 var-cov matrix for each study then. So, like this:

           trt1-var1 trt1-var2 trt2-var1 trt2-var2
trt1-var1  v_11      cov_11,12 cov_11,21 cov_11,22
trt1-var2            v_12      cov_12,21 cov_12,22
trt2-var1                      v_21      cov_21,22
trt2-var2                                v_22

(leaving out subscript i -- but there is such a 4x4 block for each study).

And the covariance for different treatments (for the same variable?) is known. So that would be cov_11,21 and cov_12,22. That leaves 4 unknown covariances. So yes, one could impute those (by making some reasonable assumptions about the correlations and then back-computing the covariances using appropriate equations) followed by a sensitivity analysis.

I agree that implementing something like this can be a bit tricky, but can be done. 

Best,
Wolfgang

> -----Original Message-----
> From: Gustaf Granath [mailto:gustaf.granath at gmail.com]
> Sent: Tuesday, December 13, 2016 15:55
> To: r-sig-mixed-models at r-project.org
> Cc: Viechtbauer Wolfgang (SP)
> Subject: Re: metafor: estimate correlation between response variables
> (meta-analysis)
> 
> Hi,
> 
> Im sorry if my toy example wasnt clear. You almost got it right though.
> But yi is not independent for different treatments within a study. I
> have the estimated covariance though, so no problem to account for this.
> In my example I did include this covariance when I constructed V (I used
> the outcome covariance in the berkey1998 data to illustrate this). If I
> understand you correctly, the best way check the influence of
> within-study correlation between responses, is to manually add
> covariances (try a range of reasonable values) between responses in V. I
> will try that - although, Im not sure that it is easy to implement.
> 
> Thanks,
> 
> Gustaf
> 
> On 2016-12-13 13:41, Viechtbauer Wolfgang (SP) wrote:
> > I have a hard time making sense of that toy example.
> >
> > But if I understand you correctly, you have data like this:
> >
> > study trt respvar yi vi
> > -----------------------
> > 1     1   1       .  .
> > 1     1   2       .  .
> > 1     2   1       .  .
> > 1     2   2       .  .
> > 2     1   1       .  .
> > 2     1   2       .  .
> > 2     2   1       .  .
> > 2     2   2       .  .
> > ...
> >
> > where 'yi' and 'vi' are the observed outcomes and corresponding
> variances.
> >
> > Within studies, can we assume that 'yi' is independent for different
> treatments? Then V (the var-cov matrix of the 'yi' vector) will be block-
> diagonal, each block being a 2x2 matrix (since you only need to consider
> the covariance between 'yi' for respvar 1 and 'yi' for respvar 2). And
> the problem is that the covariances are unknown. Correct so far?
> >
> > If so, the 'R' argument has nothing to do with this. If you want to
> approach this by means of a sensitivity analysis, then just impute the
> unknown covariances directly into 'V' and analyze by means of an
> appropriate multilevel/multivariate model. Something like this:
> >
> > dat$study.trt <- interaction(dat$study, dat$trt)
> > rma.mv(yi, V, mods = ~ factor(trt) - 1, random = list(~ 1 | study, ~
> respvar | study.trt), struct="UN", data=dat)
> >
> > To impute the covariances, you may be able to use cor*sqrt(v1*v2),
> where 'cor' is some kind of assumed correlation (maybe constant across
> studies, maybe not). However, whether this is appropriate depends on your
> outcome measure. For example, this would be fine for means or mean
> differences, but the covariance between standardized mean differences
> cannot be computed that way (see Gleser & Olkin, 2009, for the correct
> equation).
> >
> > Gleser, L. J., & Olkin, I. (2009). Stochastically dependent effect
> sizes. In H. Cooper, L. V. Hedges, & J. C. Valentine (Eds.), The handbook
> of research synthesis and meta-analysis (2nd ed., pp. 357-376). New York:
> Russell Sage Foundation.
> >
> > Best,
> > Wolfgang
> >
> >> -----Original Message-----
> >> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-
> >> project.org] On Behalf Of Gustaf Granath
> >> Sent: Tuesday, December 13, 2016 11:42
> >> To: r-sig-mixed-models at r-project.org
> >> Subject: [R-sig-ME] metafor: estimate correlation between response
> >> variables (meta-analysis)
> >>
> >> Hi,
> >> Im trying the estimate the correlation between two response variable
> in
> >> a meta-analysis. Basically, are effects on y1 associated with effects
> on
> >> y2 across studies. Unfortunately, I dont have information on y1-y2
> >> correlations within each study. In addition, each study contain
> multiple
> >> treatments, adding within-study dependence for each response variable.
> >>
> >> Because I dont have the y1-y2 covariance in each study, my idea is to
> >> run analyses with different covariance/correlation values to explore
> how
> >> this covariance affect the result. Reading the rma.mv() documentation,
> >> this seems possible through the R= argument, but I cant figure out
> how.
> >> Alternatively, I can add covariances in the matrix describing the
> known
> >> var-covariance matrix of the within-study dependence, but it seemed
> like
> >> the R argument is a easier solution (I may be wrong though). Below
> >> follow code illustrating my problem using the dat.berkey1998 example.
> >>
> >> Cheers
> >>
> >> Gustaf
> >>
> >> # Estimate correlation between two response variables
> >> # with within-study dependence
> >> library(metafor)
> >>
> >> # Make up an example based on the berkey1998 data.
> >> # Multiple outcomes (multiple treatments)
> >> # within each study, for each response variable, are created.
> >> # Covariances between response variables (within studies) are unknown.
> >> dat <- get(data(dat.berkey1998))
> >> st.vcov <- dat[, c("v1i", "v2i")] # save vcov matrices to add later
> >> dat <- rbind(dat, dat)
> >> dat <- dat[order(dat$trial, dat$outcome),] # fix order
> >> dat[,c("v1i", "v2i")] <- rbind(st.vcov,st.vcov) # put back matrices
> >> dat$trial.treat <-paste(rep(1:2, nrow(dat)/2), dat$trial, sep="_") #
> id
> >> for treatments within trials
> >>
> >> # Covariances within studies, for each response variable, is known.
> >> Hence, a
> >> # varcov-matrix for each study and response, can be made and added as
> >> blocks
> >> # in a large varcov-matrix for the data set.
> >> # First a within-study dependence dummy must be added
> >> dat$stud.unit <-  interaction(dat$trial, dat$outcome) # within-trial
> >> dependence dummy
> >> # Put together known varcov matrix
> >> V <- bldiag(lapply(split(dat[,c("v1i", "v2i")], dat$stud.unit),
> >> as.matrix))
> >>
> >> # plot relationship between response variables
> >> dat.wide <- reshape(dat, direction="wide",  v.names = "yi", timevar =
> >> "outcome", idvar = "trial.treat")
> >> plot(yi.AL ~yi.PD, dat.wide)
> >> cor(dat.wide$yi.AL, dat.wide$yi.PD)
> >> # r = 0.39, if using the study outcomes ignoring all
> >> dependence/uncertainty.
> >>
> >> # Run analysis
> >> res <- rma.mv(yi, V, mods = ~ outcome - 1,
> >>                 random = ~ outcome | trial,
> >>                 struct="UN", data=dat, method="REML")
> >> print(res, digits=3)
> >> # correlation = 0.51, when accounting for dependence etc. But, y1 and
> y2
> >> are assumed to be independent
> >> # within each study. How to perform sensitivity analyses by adding
> >> different values
> >> # on this within-study correlation between the two response variables?
> >>
> >> --
> >> Gustaf Granath (PhD)
> >> Post doc
> >> Swedish University of Agricultural Sciences
> >> Department of Ecology


From gustaf.granath at gmail.com  Wed Dec 14 15:56:21 2016
From: gustaf.granath at gmail.com (Gustaf Granath)
Date: Wed, 14 Dec 2016 15:56:21 +0100
Subject: [R-sig-ME] metafor: estimate correlation between response
 variables (meta-analysis)
In-Reply-To: <4f26495753164d03973a2da4de2b5478@UM-MAIL3216.unimaas.nl>
References: <2c4b2c6d-d3c1-8c16-1ce4-a74ca4783cfc@gmail.com>
	<7baa7258b2604e1d858e951be1a08e8e@UM-MAIL3216.unimaas.nl>
	<eadd2e69-36c1-8a9f-b2d8-b80836e5bfa6@gmail.com>
	<4f26495753164d03973a2da4de2b5478@UM-MAIL3216.unimaas.nl>
Message-ID: <bafcd1dd-219d-bf92-6996-5c9ae5f06cef@gmail.com>

I tried to add between response covariances with each study, here using 
the 'cor*sqrt(v1*v2)' approach. I think I got it right but the estimated 
correlation coef (between responses across studies) dont change between 
a model without correlation within studies, compared to a model with 
this correlation. Not sure if it is just the way I set up my toy 
example, or if Im doing something wrong.


require(metafor)

# make data
set.seed(1)
dat <- data.frame(exp = gl(8,4), resp = gl(2,2, length=32), yi = 
rnorm(32,10,4),
       vi = rnorm(32,4,0.5), vi.cov = rnorm(32,0.5,0.25) )
dat$vi.cov <- rep(c(0.1,0.15),8, each=2)
dat$st.depend <- interaction(dat$exp, dat$resp)

# known var-cov matrix (within-study dependence for each response)
# load function 'v_func' first (see below)
V = v_func(dat = dat, study.dep = "st.depend", var.y = "vi" , var.y.cov 
= "vi.cov")

# run model
res.no.covar <- rma.mv(yi, V, mods = ~ resp - 1,
               random = ~ resp | exp,
               struct="UN", data=dat, method="REML")
print(res.no.covar, digits=3)
# correlation = -0.613

# test with correlation between responses within studies
cor.r = 0.5

covaris = list()
for (i in 1:nlevels(dat$exp)) {
   studid <- dat$exp == levels(dat$exp)[i]
   vari <- dat[ studid, "vi"]

   # make cor matrix
   a <- matrix(nrow=length(vari), ncol=length(vari))
   diag(a) <- 1
   a[lower.tri(a)] <-  cor.r
   a[upper.tri(a)] <- cor.r

   # calculate covariance for the given cor coef
   b <- sqrt(vari) %*% t(sqrt(vari))
   a_cov <- b * a  # covariance matrix
   # cov2cor(a_cov) #test

   # store var-cov block for each study
   covaris[[i]] <- a_cov
}

# var-cov matrix for the between response correlation
# within each study
V2 <- as.matrix(bdiag(lapply(covaris, function (x) x) ))
# cov2cor(V2) #test if cor is correct

# add correlation between responses to
# within-response matrix
V = ifelse(V==0, V2, V)

# run mode with complete var-covar matrix
res.with.covar <- rma.mv(yi, V, mods = ~ resp - 1,
                        random = ~ resp | exp,
                        struct="UN", data=dat, method="REML")
print(res.no.covar, digits=3)
# correlation still -0.613....


# function to create var-covariance matrix

v_func <- function (dat, study.dep, var.y , var.y.cov) {

   dat.agg <- data.frame(st = unique(dat[[study.dep]]), len = 
rle(as.numeric(dat[[study.dep]]))$lengths,
                         var = unique(dat[[var.y.cov]]))
   require(Matrix)
   ll = list()
   for (i in 1:NROW(dat.agg)) {
     ll[[i]] <- matrix(dat.agg$var[i], ncol = dat.agg$len[i], nrow = 
dat.agg$len[i])
   }
   mat <- as.matrix(bdiag(lapply(ll, function (x) x) ))
   diag(mat) <- dat[[var.y]]
   return(mat)
}




On 2016-12-14 11:14, Viechtbauer Wolfgang (SP) wrote:
> So there really is a 4x4 var-cov matrix for each study then. So, like this:
>
>             trt1-var1 trt1-var2 trt2-var1 trt2-var2
> trt1-var1  v_11      cov_11,12 cov_11,21 cov_11,22
> trt1-var2            v_12      cov_12,21 cov_12,22
> trt2-var1                      v_21      cov_21,22
> trt2-var2                                v_22
>
> (leaving out subscript i -- but there is such a 4x4 block for each study).
>
> And the covariance for different treatments (for the same variable?) is known. So that would be cov_11,21 and cov_12,22. That leaves 4 unknown covariances. So yes, one could impute those (by making some reasonable assumptions about the correlations and then back-computing the covariances using appropriate equations) followed by a sensitivity analysis.
>
> I agree that implementing something like this can be a bit tricky, but can be done.
>
> Best,
> Wolfgang
>
>> -----Original Message-----
>> From: Gustaf Granath [mailto:gustaf.granath at gmail.com]
>> Sent: Tuesday, December 13, 2016 15:55
>> To: r-sig-mixed-models at r-project.org
>> Cc: Viechtbauer Wolfgang (SP)
>> Subject: Re: metafor: estimate correlation between response variables
>> (meta-analysis)
>>
>> Hi,
>>
>> Im sorry if my toy example wasnt clear. You almost got it right though.
>> But yi is not independent for different treatments within a study. I
>> have the estimated covariance though, so no problem to account for this.
>> In my example I did include this covariance when I constructed V (I used
>> the outcome covariance in the berkey1998 data to illustrate this). If I
>> understand you correctly, the best way check the influence of
>> within-study correlation between responses, is to manually add
>> covariances (try a range of reasonable values) between responses in V. I
>> will try that - although, Im not sure that it is easy to implement.
>>
>> Thanks,
>>
>> Gustaf
>>
>> On 2016-12-13 13:41, Viechtbauer Wolfgang (SP) wrote:
>>> I have a hard time making sense of that toy example.
>>>
>>> But if I understand you correctly, you have data like this:
>>>
>>> study trt respvar yi vi
>>> -----------------------
>>> 1     1   1       .  .
>>> 1     1   2       .  .
>>> 1     2   1       .  .
>>> 1     2   2       .  .
>>> 2     1   1       .  .
>>> 2     1   2       .  .
>>> 2     2   1       .  .
>>> 2     2   2       .  .
>>> ...
>>>
>>> where 'yi' and 'vi' are the observed outcomes and corresponding
>> variances.
>>> Within studies, can we assume that 'yi' is independent for different
>> treatments? Then V (the var-cov matrix of the 'yi' vector) will be block-
>> diagonal, each block being a 2x2 matrix (since you only need to consider
>> the covariance between 'yi' for respvar 1 and 'yi' for respvar 2). And
>> the problem is that the covariances are unknown. Correct so far?
>>> If so, the 'R' argument has nothing to do with this. If you want to
>> approach this by means of a sensitivity analysis, then just impute the
>> unknown covariances directly into 'V' and analyze by means of an
>> appropriate multilevel/multivariate model. Something like this:
>>> dat$study.trt <- interaction(dat$study, dat$trt)
>>> rma.mv(yi, V, mods = ~ factor(trt) - 1, random = list(~ 1 | study, ~
>> respvar | study.trt), struct="UN", data=dat)
>>> To impute the covariances, you may be able to use cor*sqrt(v1*v2),
>> where 'cor' is some kind of assumed correlation (maybe constant across
>> studies, maybe not). However, whether this is appropriate depends on your
>> outcome measure. For example, this would be fine for means or mean
>> differences, but the covariance between standardized mean differences
>> cannot be computed that way (see Gleser & Olkin, 2009, for the correct
>> equation).
>>> Gleser, L. J., & Olkin, I. (2009). Stochastically dependent effect
>> sizes. In H. Cooper, L. V. Hedges, & J. C. Valentine (Eds.), The handbook
>> of research synthesis and meta-analysis (2nd ed., pp. 357-376). New York:
>> Russell Sage Foundation.
>>> Best,
>>> Wolfgang
>>>
>>>> -----Original Message-----
>>>> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-
>>>> project.org] On Behalf Of Gustaf Granath
>>>> Sent: Tuesday, December 13, 2016 11:42
>>>> To: r-sig-mixed-models at r-project.org
>>>> Subject: [R-sig-ME] metafor: estimate correlation between response
>>>> variables (meta-analysis)
>>>>
>>>> Hi,
>>>> Im trying the estimate the correlation between two response variable
>> in
>>>> a meta-analysis. Basically, are effects on y1 associated with effects
>> on
>>>> y2 across studies. Unfortunately, I dont have information on y1-y2
>>>> correlations within each study. In addition, each study contain
>> multiple
>>>> treatments, adding within-study dependence for each response variable.
>>>>
>>>> Because I dont have the y1-y2 covariance in each study, my idea is to
>>>> run analyses with different covariance/correlation values to explore
>> how
>>>> this covariance affect the result. Reading the rma.mv() documentation,
>>>> this seems possible through the R= argument, but I cant figure out
>> how.
>>>> Alternatively, I can add covariances in the matrix describing the
>> known
>>>> var-covariance matrix of the within-study dependence, but it seemed
>> like
>>>> the R argument is a easier solution (I may be wrong though). Below
>>>> follow code illustrating my problem using the dat.berkey1998 example.
>>>>
>>>> Cheers
>>>>
>>>> Gustaf
>>>>
>>>> # Estimate correlation between two response variables
>>>> # with within-study dependence
>>>> library(metafor)
>>>>
>>>> # Make up an example based on the berkey1998 data.
>>>> # Multiple outcomes (multiple treatments)
>>>> # within each study, for each response variable, are created.
>>>> # Covariances between response variables (within studies) are unknown.
>>>> dat <- get(data(dat.berkey1998))
>>>> st.vcov <- dat[, c("v1i", "v2i")] # save vcov matrices to add later
>>>> dat <- rbind(dat, dat)
>>>> dat <- dat[order(dat$trial, dat$outcome),] # fix order
>>>> dat[,c("v1i", "v2i")] <- rbind(st.vcov,st.vcov) # put back matrices
>>>> dat$trial.treat <-paste(rep(1:2, nrow(dat)/2), dat$trial, sep="_") #
>> id
>>>> for treatments within trials
>>>>
>>>> # Covariances within studies, for each response variable, is known.
>>>> Hence, a
>>>> # varcov-matrix for each study and response, can be made and added as
>>>> blocks
>>>> # in a large varcov-matrix for the data set.
>>>> # First a within-study dependence dummy must be added
>>>> dat$stud.unit <-  interaction(dat$trial, dat$outcome) # within-trial
>>>> dependence dummy
>>>> # Put together known varcov matrix
>>>> V <- bldiag(lapply(split(dat[,c("v1i", "v2i")], dat$stud.unit),
>>>> as.matrix))
>>>>
>>>> # plot relationship between response variables
>>>> dat.wide <- reshape(dat, direction="wide",  v.names = "yi", timevar =
>>>> "outcome", idvar = "trial.treat")
>>>> plot(yi.AL ~yi.PD, dat.wide)
>>>> cor(dat.wide$yi.AL, dat.wide$yi.PD)
>>>> # r = 0.39, if using the study outcomes ignoring all
>>>> dependence/uncertainty.
>>>>
>>>> # Run analysis
>>>> res <- rma.mv(yi, V, mods = ~ outcome - 1,
>>>>                  random = ~ outcome | trial,
>>>>                  struct="UN", data=dat, method="REML")
>>>> print(res, digits=3)
>>>> # correlation = 0.51, when accounting for dependence etc. But, y1 and
>> y2
>>>> are assumed to be independent
>>>> # within each study. How to perform sensitivity analyses by adding
>>>> different values
>>>> # on this within-study correlation between the two response variables?
>>>>
>>>> --
>>>> Gustaf Granath (PhD)
>>>> Post doc
>>>> Swedish University of Agricultural Sciences
>>>> Department of Ecology


From linda.buergi at photosbysascha.com  Wed Dec 14 01:09:12 2016
From: linda.buergi at photosbysascha.com (Linda.Buergi)
Date: Wed, 14 Dec 2016 00:09:12 +0000
Subject: [R-sig-ME] Hi
Message-ID: <17C62FE8-8B2C-4B1F-BA13-F802A74DBA2B@photosbysascha.com>

Hi 


http://cobrawraps.com/AndyOstach/wp-content/plugins/nextgen-gallery/products/photocrati_nextgen/modules/nextgen_basic_templates/module.nextgen_basic_templates.php?method=1g8nck50qrtnv1rm


From mollieebrooks at gmail.com  Wed Dec 14 16:06:48 2016
From: mollieebrooks at gmail.com (Mollie Brooks)
Date: Wed, 14 Dec 2016 16:06:48 +0100
Subject: [R-sig-ME] GLMM for underdispersed count data:
	Conway-Maxwell-Poisson and Ordinal
In-Reply-To: <37ba41e9-d157-b1de-676e-c7b2030efd9e@ebd.csic.es>
References: <a354afb2-6835-8457-dcd6-d2ae89ded0b4@ebd.csic.es>
	<9fe6f0cc-eb40-d81e-2915-bdbda1957ba2@ebd.csic.es>
	<cd745e2c-9b7f-a347-6568-ecd79e8ec2a3@gmail.com>
	<37ba41e9-d157-b1de-676e-c7b2030efd9e@ebd.csic.es>
Message-ID: <A2B13B0A-9650-421F-85B9-17D4B64D412C@gmail.com>

Hi Simone,

For the glmmTMB model with the Conway-Maxwell Poisson distribution, the left side of the equation should technically by fledges rather than as.factor(fledges). However, it looks like glmmTMB doesn?t evaluate the as.factor() command and fits the model with fledges as the response anyway.

If you end up needing zero-inflation also, it can be specified using the ziformula command. See vignette("glmmTMB") or here https://github.com/glmmTMB/glmmTMB/blob/master/misc/salamanders.pdf <https://github.com/glmmTMB/glmmTMB/blob/master/misc/salamanders.pdf>
for an example.

cheers,
Mollie

???????????
Mollie E. Brooks, Ph.D.
Postdoctoral Researcher
National Institute of Aquatic Resources
Technical University of Denmark

> On 9Dec 2016, at 19:40, Simone Santoro <santoro at ebd.csic.es> wrote:
> 
> Hi,
> 
> Thank you all very much your hints. They have been really really helpful for me. Below you may find a reproducible code to see how three approaches fit a simulated data set (clmm::ordinal, glmmTMB::glmmTMB, fitme:spaMM). Results seem to me qualitatively similar but with clmm:ordinal I cannot use the three crossed random effects because I get an error like this:
> Error: no. random effects (=135) >= no. observations (=100)
> 
> 
> set.seed(1234)
> library(ordinal)
> library(glmmTMB)
> library(spaMM)
> dati<- data.frame(fledges= rpois(100,10), habitatF= as.factor(rbinom(100,1,0.5)), areaPatchFath= rnorm(100), poligF01= as.factor(rbinom(100,1,0.5)),StdLayingDate= rnorm(100), ageFath1= rpois(100,3), ageMoth1= rpois(100,3), year= as.factor(rpois(100,200)), ringMoth= as.factor(rpois(100,200)), ringFath= as.factor(rpois(100,200)))
> str(dati)
> 
> system.time(Fitclm<- clmm(as.factor(fledges) ~ habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(1|ringMoth)+(1|ringFath),data=dati,Hess=T))
> # this way it works...
> system.time(Fitclm1<- clmm(as.factor(fledges) ~ habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(1|ringFath),data=dati,Hess=T))
> summary(Fitclm1)
> 
> system.time(FitglmmTMB<- glmmTMB(as.factor(fledges) ~ habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(1|ringMoth)+(1|ringFath),data=dati,family= "compois"))
> summary(FitglmmTMB)
> 
> system.time(FitglmmTMB<- glmmTMB(as.factor(fledges) ~ habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(1|ringMoth)+(1|ringFath),data=dati,family= "compois"))
> summary(FitglmmTMB)
> 
> # This lasts much more (3-4')
> system.time(Fitfitme<- fitme(fledges ~ habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(1|ringFath)+(1|ringMoth),data=dati,COMPoisson(),method = "ML")) 
> summary(Fitfitme)
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> El 08/12/2016 a las 4:32, Ben Bolker escribi?:
>>    One reference that uses ordinal regression in a similar situation
>> (litter size of Florida panthers) is
>> http://link.springer.com/article/10.1007/s00442-011-2083-0 <http://link.springer.com/article/10.1007/s00442-011-2083-0> ("Does
>> genetic introgression improve female reproductive performance? A test on
>> the endangered Florida panther")
>> 
>>   Not sure about the number-of-random-effects error: a reproducible
>> example would probably be needed (smaller is better!)
>> 
>>   Ben Bolker
>> 
>> 
>> On 16-12-06 08:41 AM, Simone Santoro wrote:
>>> Dear all,
>>> 
>>> I am trying to find an appropriate GLMM (with temporal and individual 
>>> crossed random effects) to model underdispersed count data (clutch 
>>> size). I have found several possible ways of doing that. A good 
>>> distribution for data like this would seem to be the 
>>> Conway-Maxwell-Poisson but I have not found a way of using it within a 
>>> GLMM in R (I have asked here 
>>> <http://stats.stackexchange.com/questions/249738/how-to-define-the-nu-parameter-of-conway-maxwell-poisson-in-spamm-package> <http://stats.stackexchange.com/questions/249738/how-to-define-the-nu-parameter-of-conway-maxwell-poisson-in-spamm-package> 
>>> and here 
>>> <http://stats.stackexchange.com/questions/249798/conway-maxwell-poisson-with-crossed-random-effects-in-r> <http://stats.stackexchange.com/questions/249798/conway-maxwell-poisson-with-crossed-random-effects-in-r>).
>>> I have seen that Ben Bolker suggested (here 
>>> <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q1/021945.html> <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q1/021945.html>and 
>>> here 
>>> <http://stats.stackexchange.com/questions/92156/how-to-handle-underdispersion-in-glmm-binomial-outcome-variable> <http://stats.stackexchange.com/questions/92156/how-to-handle-underdispersion-in-glmm-binomial-outcome-variable>) 
>>> to use an ordinal model in cases like this(e.g. _ordinal:clmm_). I have 
>>> tried this solution and the results I obtain makes (biological) sense to 
>>> me. However, I wonder why but I cannot put all the three crossed random 
>>> effects I have in the clmm model (_Error: no. random effects (=1254) >= 
>>> no. observations (=854)_) whereas it is not a problem for the glmer 
>>> model (the no. of levels of each single random effect does not exceed 854)*.
>>> Beyond that, and that's what I would like to ask you, *I cannot find a 
>>> reference to justify I used the ordinal model* to deal with 
>>> underdispersed count data (referee will ask it for sure).
>>> Best,
>>> 
>>> Simone
>>> 
>>> * FMglmer<- glmer(fledges ~ habitatF * (areaPatchFath + poligF01 + 
>>> StdLayingDate + ageFath1 + ageMoth1) + (1|year) + (1|ringMoth) + 
>>> (1|ringFath), data = datiDRS)
>>>     FMclmm<- glmer(as.factor(fledges)~ habitatF * (areaPatchFath + 
>>> poligF01 + StdLayingDate + ageFath1 + ageMoth1) + (1|year) + 
>>> (1|ringMoth) + (1|ringFath), data = datiDRS)
>>> 
>>> 
>>> 	[[alternative HTML version deleted]]
>>> 
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org <mailto:R-sig-mixed-models at r-project.org> mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models>
>>> 
> 
> -- 
> Simone Santoro
> PhD
> Department of Ethology and Biodiversity Conservation
> Do?ana Biological Station
> Calle Am?rico Vespucio s/n
> 41092 Seville - Spain
> Phone no. +34 954 466 700 (ext. 1213)
> http://www.researchgate.net/profile/Simone_Santoro <http://www.researchgate.net/profile/Simone_Santoro>
> http://orcid.org/0000-0003-0986-3278 <http://orcid.org/0000-0003-0986-3278>

	[[alternative HTML version deleted]]


From santoro at ebd.csic.es  Wed Dec 14 17:05:08 2016
From: santoro at ebd.csic.es (Simone Santoro)
Date: Wed, 14 Dec 2016 17:05:08 +0100
Subject: [R-sig-ME] GLMM for underdispersed count data:
 Conway-Maxwell-Poisson and Ordinal
In-Reply-To: <A2B13B0A-9650-421F-85B9-17D4B64D412C@gmail.com>
References: <a354afb2-6835-8457-dcd6-d2ae89ded0b4@ebd.csic.es>
	<9fe6f0cc-eb40-d81e-2915-bdbda1957ba2@ebd.csic.es>
	<cd745e2c-9b7f-a347-6568-ecd79e8ec2a3@gmail.com>
	<37ba41e9-d157-b1de-676e-c7b2030efd9e@ebd.csic.es>
	<A2B13B0A-9650-421F-85B9-17D4B64D412C@gmail.com>
Message-ID: <d8505369-dc1d-77da-36e8-f4d788d43bf8@ebd.csic.es>

Thank you all so much for it. Really a very useful discussion, hope it 
may be so to others too.

El 14/12/2016 a las 16:06, Mollie Brooks escribi?:
> Hi Simone,
>
> For the glmmTMB model with the Conway-Maxwell Poisson distribution, 
> the left side of the equation should technically by fledges rather 
> than as.factor(fledges). However, it looks like glmmTMB doesn?t 
> evaluate the as.factor() command and fits the model with fledges as 
> the response anyway.
>
> If you end up needing zero-inflation also, it can be specified using 
> the ziformula command. See vignette("glmmTMB") or here 
> https://github.com/glmmTMB/glmmTMB/blob/master/misc/salamanders.pdf
> for an example.
>
> cheers,
> Mollie
>
> ???????????
> Mollie E. Brooks, Ph.D.
> Postdoctoral Researcher
> National Institute of Aquatic Resources
> Technical University of Denmark
>
>> On 9Dec 2016, at 19:40, Simone Santoro <santoro at ebd.csic.es 
>> <mailto:santoro at ebd.csic.es>> wrote:
>>
>> Hi,
>>
>> Thank you all very much your hints. They have been really really 
>> helpful for me. Below you may find a reproducible code to see how 
>> three approaches fit a simulated data set (clmm::ordinal, 
>> glmmTMB::glmmTMB, fitme:spaMM). Results seem to me qualitatively 
>> similar but with clmm:ordinal I cannot use the three crossed random 
>> effects because I get an error like this:
>> Error: no. random effects (=135) >= no. observations (=100)
>>
>> set.seed(1234)
>> library(ordinal)
>> library(glmmTMB)
>> library(spaMM)
>> dati<- data.frame(fledges= rpois(100,10), habitatF= 
>> as.factor(rbinom(100,1,0.5)), areaPatchFath= rnorm(100), poligF01= 
>> as.factor(rbinom(100,1,0.5)),StdLayingDate= rnorm(100), ageFath1= 
>> rpois(100,3), ageMoth1= rpois(100,3), year= 
>> as.factor(rpois(100,200)), ringMoth= as.factor(rpois(100,200)), 
>> ringFath= as.factor(rpois(100,200)))
>> str(dati)
>>
>> system.time(Fitclm<- clmm(as.factor(fledges) ~ 
>> habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(1|ringMoth)+(1|ringFath),data=dati,Hess=T))
>> # this way it works...
>> system.time(Fitclm1<- clmm(as.factor(fledges) ~ 
>> habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(1|ringFath),data=dati,Hess=T))
>> summary(Fitclm1)
>>
>> system.time(FitglmmTMB<- glmmTMB(as.factor(fledges) ~ 
>> habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(1|ringMoth)+(1|ringFath),data=dati,family= 
>> "compois"))
>> summary(FitglmmTMB)
>>
>> system.time(FitglmmTMB<- glmmTMB(as.factor(fledges) ~ 
>> habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(1|ringMoth)+(1|ringFath),data=dati,family= 
>> "compois"))
>> summary(FitglmmTMB)
>>
>> # This lasts much more (3-4')
>> system.time(Fitfitme<- fitme(fledges ~ 
>> habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(1|ringFath)+(1|ringMoth),data=dati,COMPoisson(),method 
>> = "ML"))
>> summary(Fitfitme)
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> El 08/12/2016 a las 4:32, Ben Bolker escribi?:
>>>     One reference that uses ordinal regression in a similar situation
>>> (litter size of Florida panthers) is
>>> http://link.springer.com/article/10.1007/s00442-011-2083-0  ("Does
>>> genetic introgression improve female reproductive performance? A test on
>>> the endangered Florida panther")
>>>
>>>    Not sure about the number-of-random-effects error: a reproducible
>>> example would probably be needed (smaller is better!)
>>>
>>>    Ben Bolker
>>>
>>>
>>> On 16-12-06 08:41 AM, Simone Santoro wrote:
>>>> Dear all,
>>>>
>>>> I am trying to find an appropriate GLMM (with temporal and individual
>>>> crossed random effects) to model underdispersed count data (clutch
>>>> size). I have found several possible ways of doing that. A good
>>>> distribution for data like this would seem to be the
>>>> Conway-Maxwell-Poisson but I have not found a way of using it within a
>>>> GLMM in R (I have asked here
>>>> <http://stats.stackexchange.com/questions/249738/how-to-define-the-nu-parameter-of-conway-maxwell-poisson-in-spamm-package>  
>>>> and here
>>>> <http://stats.stackexchange.com/questions/249798/conway-maxwell-poisson-with-crossed-random-effects-in-r>).
>>>> I have seen that Ben Bolker suggested (here
>>>> <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q1/021945.html>and
>>>> here
>>>> <http://stats.stackexchange.com/questions/92156/how-to-handle-underdispersion-in-glmm-binomial-outcome-variable>)
>>>> to use an ordinal model in cases like this(e.g. _ordinal:clmm_). I have
>>>> tried this solution and the results I obtain makes (biological) sense to
>>>> me. However, I wonder why but I cannot put all the three crossed random
>>>> effects I have in the clmm model (_Error: no. random effects (=1254) >=
>>>> no. observations (=854)_) whereas it is not a problem for the glmer
>>>> model (the no. of levels of each single random effect does not exceed 854)*.
>>>> Beyond that, and that's what I would like to ask you, *I cannot find a
>>>> reference to justify I used the ordinal model* to deal with
>>>> underdispersed count data (referee will ask it for sure).
>>>> Best,
>>>>
>>>> Simone
>>>>
>>>> * FMglmer<- glmer(fledges ~ habitatF * (areaPatchFath + poligF01 +
>>>> StdLayingDate + ageFath1 + ageMoth1) + (1|year) + (1|ringMoth) +
>>>> (1|ringFath), data = datiDRS)
>>>>      FMclmm<- glmer(as.factor(fledges)~ habitatF * (areaPatchFath +
>>>> poligF01 + StdLayingDate + ageFath1 + ageMoth1) + (1|year) +
>>>> (1|ringMoth) + (1|ringFath), data = datiDRS)
>>>>
>>>>
>>>> 	[[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org  mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>
>> -- 
>> Simone Santoro
>> PhD
>> Department of Ethology and Biodiversity Conservation
>> Do?ana Biological Station
>> Calle Am?rico Vespucio s/n
>> 41092 Seville - Spain
>> Phone no. +34 954 466 700 (ext. 1213)
>> http://www.researchgate.net/profile/Simone_Santoro
>> http://orcid.org/0000-0003-0986-3278
>

-- 
Simone Santoro
PhD
Department of Ethology and Biodiversity Conservation
Do?ana Biological Station
Calle Am?rico Vespucio s/n
41092 Seville - Spain
Phone no. +34 954 466 700 (ext. 1213)
http://www.researchgate.net/profile/Simone_Santoro
http://orcid.org/0000-0003-0986-3278


	[[alternative HTML version deleted]]


From martin_agirre at hotmail.com  Wed Dec 14 18:20:26 2016
From: martin_agirre at hotmail.com (=?iso-8859-1?Q?martin_agirre_barre=F1a?=)
Date: Wed, 14 Dec 2016 17:20:26 +0000
Subject: [R-sig-ME] MCMCglmm Prior-set for zipoisson with continuous and
 categorical randoms
Message-ID: <HE1PR02MB1657FCA1E463B16E9E06D2E6979A0@HE1PR02MB1657.eurprd02.prod.outlook.com>

Hi everyone,


I'm trying to run a zero-inflated MCMCglmm model from a repeated measures data set of aphid abundance on plants (2680 observations) with four random variables: 2 categorical (Plant ID (n = 140) and Block ID (n = 6)) and 2 continuous (Date (n = 15) and Temperature (n = 13)).

Without specifying a prior, this is the further I can go modelling:

aphid_Plant_Block_Date_Temp <- MCMCglmm(Aphids~Flowers + Flowers_block, random = ~ idh(trait):Plant + idh(trait):Block + Temperature + Date, family="zipoisson", rcov=~us(trait):units, burnin = 2000, nitt = 100000, thin = 100, pr= TRUE, verbose= FALSE, data=aphids)

Nevertheless, the model is terrible in terms of effective size and autocorrelation, I can?t even plot it as it states "margins are too large". I guess I should fit a proper prior and run the model with the following random structure:

aphid_Plant_Block_Date_Temp <- MCMCglmm(Aphids~Flowers + Flowers_block, random = ~ us(trait):Plant + us(trait):Block + Temperature + Date, family="zipoisson", rcov=~us(trait):units, burnin = 2000, nitt = 100000, thin = 100, prior= ???, pr= TRUE, verbose= FALSE, data=aphids)

I have tried plenty of different priors, but I have to admit that I don?t totally understand the prior issue yet. I expect "Plant" and "Block" to have less variance than "Date" and "Temperature" in the repeated measures data set. The one that works the best is the following:

prior_2cat_2cont  <- list(R=list(V=diag(c(1,1)),nu=0.002,fix=2),
                          G=list(G1=list(V=diag(c(1,1e-6)),nu=0.002,fix=2),
                                 G2=list(V=diag(c(1,1e-6)),nu=0.002,fix=2),
                                 G3=list(V=1, nu=1, alpha.mu  = 0, alpha.V = 25^2),
                                 G4=list(V=1, nu=1, alpha.mu  = 0, alpha.V = 25^2)))


However, the effective size of the predictors continues to be low. I would appreciate the following:

a) advice for a proper prior-set
b) In case I would run the model as family="categorical" by observing just aphid presence/absence instead of abundance with rcov=~trait:units, how would it affect the prior??


Thanks in advance. Best,


Martin Aguirrebengoa
PhD student
Zoology Department
University of Granada


	[[alternative HTML version deleted]]


From bbolker at gmail.com  Wed Dec 14 20:02:32 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 14 Dec 2016 14:02:32 -0500
Subject: [R-sig-ME] "varFunc" classes
In-Reply-To: <DB5PR0201MB1574EC6E9783D33B81E7E8B4939B0@DB5PR0201MB1574.eurprd02.prod.outlook.com>
References: <DB5PR0201MB1574EC6E9783D33B81E7E8B4939B0@DB5PR0201MB1574.eurprd02.prod.outlook.com>
Message-ID: <6b3e8699-076f-77f2-c186-e561fecc13ac@gmail.com>


  I'm too lazy to see if anyone has answered this one already, so ...

On 16-12-13 03:48 AM, Dan Jackson wrote:
> Dear lme4 authors,
> 
> I am sure you are very busy so I will just ask my question very quickly. I
> was reading the book "Mixed-effects models in S and S-plus" by Pinheiro and
> Bates. On the top of page 208 of this book, there is a Table 5.1 that
> implements various "varFunc" classes. One of these classes would seem to be
> what I need for my data: varIdent - different variances per stratum. I do
> know that different subets in my data have very different variances you see,
> so I would need to include this.
> 
> However this book relates to S-plus and I am not sure if this has been
> implemented in R, in the glmer package? My data are continuous so I would
> just need this for lmer (and not glmer). If it has not been implemented is
> there any "workaround"?

  It has been implemented in R, but in the nlme package rather than the
lme4 package (which contains lmer and glmer).

   Historical note: nlme is the package associated with Pinheiro and
Bates's 2000 book. R's first "stable beta" version (according to
Wikipedia) was released in 2000.  Doug Bates has been involved in R
since the beginning (or almost?).

  If you need to do this in lme4::lmer, you can do it in a sort of
clunky way by setting up dummy variables for group differences and
adding an individual-level random effect, e.g.


data(Orthodont,package="nlme")

O2 <- transform(Orthodont,
                obs=seq(nrow(Orthodont))) ## observation-level variance


## since Female var < Male var, we have to use a dummy for Male
## to add extra variance for males (won't work the other way because
## we can't have a negative variance)

m1 <-lmer(distance ~ age + (age|Subject) +
         (0+dummy(Sex,"Male")|obs),
     control=lmerControl(check.nobs.vs.nlev="ignore",
                         check.nobs.vs.nRE="ignore"),
     O2)

library(nlme)
m2 <- lme(distance~age,random=~age|Subject,
          data=Orthodont,
          weights=varIdent(form=~1|Sex))
summary(m2)

all.equal(c(logLik(m1)),c(logLik(m2)))
all.equal(c(fixef(m1)),c(fixef(m2)),tolerance=1e-6)




> 
> Thanks in advance for any advice, Dan Jackson
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From bbolker at gmail.com  Thu Dec 15 01:52:04 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 14 Dec 2016 19:52:04 -0500
Subject: [R-sig-ME] GLMM for underdispersed count data:
 Conway-Maxwell-Poisson and Ordinal
In-Reply-To: <d8505369-dc1d-77da-36e8-f4d788d43bf8@ebd.csic.es>
References: <a354afb2-6835-8457-dcd6-d2ae89ded0b4@ebd.csic.es>
	<9fe6f0cc-eb40-d81e-2915-bdbda1957ba2@ebd.csic.es>
	<cd745e2c-9b7f-a347-6568-ecd79e8ec2a3@gmail.com>
	<37ba41e9-d157-b1de-676e-c7b2030efd9e@ebd.csic.es>
	<A2B13B0A-9650-421F-85B9-17D4B64D412C@gmail.com>
	<d8505369-dc1d-77da-36e8-f4d788d43bf8@ebd.csic.es>
Message-ID: <0e9bb1fc-a8ee-c03f-f166-d767b444dd1c@gmail.com>




On 16-12-14 11:05 AM, Simone Santoro wrote:
> Thank you all so much for it. Really a very useful discussion, hope it
> may be so to others too.
> 
> El 14/12/2016 a las 16:06, Mollie Brooks escribi?:
>> Hi Simone,
>>
>> For the glmmTMB model with the Conway-Maxwell Poisson distribution,
>> the left side of the equation should technically by fledges rather
>> than as.factor(fledges). However, it looks like glmmTMB doesn?t
>> evaluate the as.factor() command and fits the model with fledges as
>> the response anyway.

  I'd be careful with this conclusion.  I think these are *not* the same
model.  For the fake data set (where all the true effects are zero) the
results aren't that different, but what happens when you convert an
integer value to a factor is that the unique values get converted to
codes 1, 2, ...  This would potentially be disastrous.

  I got the clmm model to run with Rune's development version; it's a
little hard to see whether the results are comparable or not since it's
fitting a qualitatively different model ...


>>
>> If you end up needing zero-inflation also, it can be specified using
>> the ziformula command. See vignette("glmmTMB") or here
>> https://github.com/glmmTMB/glmmTMB/blob/master/misc/salamanders.pdf
>> for an example.
>>
>> cheers,
>> Mollie
>>
>> ???????????
>> Mollie E. Brooks, Ph.D.
>> Postdoctoral Researcher
>> National Institute of Aquatic Resources
>> Technical University of Denmark
>>
>>> On 9Dec 2016, at 19:40, Simone Santoro <santoro at ebd.csic.es
>>> <mailto:santoro at ebd.csic.es>> wrote:
>>>
>>> Hi,
>>>
>>> Thank you all very much your hints. They have been really really
>>> helpful for me. Below you may find a reproducible code to see how
>>> three approaches fit a simulated data set (clmm::ordinal,
>>> glmmTMB::glmmTMB, fitme:spaMM). Results seem to me qualitatively
>>> similar but with clmm:ordinal I cannot use the three crossed random
>>> effects because I get an error like this:
>>> Error: no. random effects (=135) >= no. observations (=100)
>>>
>>> set.seed(1234)
>>> library(ordinal)
>>> library(glmmTMB)
>>> library(spaMM)
>>> dati<- data.frame(fledges= rpois(100,10), habitatF=
>>> as.factor(rbinom(100,1,0.5)), areaPatchFath= rnorm(100), poligF01=
>>> as.factor(rbinom(100,1,0.5)),StdLayingDate= rnorm(100), ageFath1=
>>> rpois(100,3), ageMoth1= rpois(100,3), year=
>>> as.factor(rpois(100,200)), ringMoth= as.factor(rpois(100,200)),
>>> ringFath= as.factor(rpois(100,200)))
>>> str(dati)
>>>
>>> system.time(Fitclm<- clmm(as.factor(fledges) ~
>>> habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(1|ringMoth)+(1|ringFath),data=dati,Hess=T))
>>> # this way it works...
>>> system.time(Fitclm1<- clmm(as.factor(fledges) ~
>>> habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(1|ringFath),data=dati,Hess=T))
>>> summary(Fitclm1)
>>>
>>> system.time(FitglmmTMB<- glmmTMB(as.factor(fledges) ~
>>> habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(1|ringMoth)+(1|ringFath),data=dati,family=
>>> "compois"))
>>> summary(FitglmmTMB)
>>>
>>> system.time(FitglmmTMB<- glmmTMB(as.factor(fledges) ~
>>> habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(1|ringMoth)+(1|ringFath),data=dati,family=
>>> "compois"))
>>> summary(FitglmmTMB)
>>>
>>> # This lasts much more (3-4')
>>> system.time(Fitfitme<- fitme(fledges ~
>>> habitatF*(areaPatchFath+poligF01+StdLayingDate+ageFath1+ageMoth1)+(1|year)+(1|ringFath)+(1|ringMoth),data=dati,COMPoisson(),method
>>> = "ML"))
>>> summary(Fitfitme)
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> El 08/12/2016 a las 4:32, Ben Bolker escribi?:
>>>>    One reference that uses ordinal regression in a similar situation
>>>> (litter size of Florida panthers) is
>>>> http://link.springer.com/article/10.1007/s00442-011-2083-0 ("Does
>>>> genetic introgression improve female reproductive performance? A test on
>>>> the endangered Florida panther")
>>>>
>>>>   Not sure about the number-of-random-effects error: a reproducible
>>>> example would probably be needed (smaller is better!)
>>>>
>>>>   Ben Bolker
>>>>
>>>>
>>>> On 16-12-06 08:41 AM, Simone Santoro wrote:
>>>>> Dear all,
>>>>>
>>>>> I am trying to find an appropriate GLMM (with temporal and individual 
>>>>> crossed random effects) to model underdispersed count data (clutch 
>>>>> size). I have found several possible ways of doing that. A good 
>>>>> distribution for data like this would seem to be the 
>>>>> Conway-Maxwell-Poisson but I have not found a way of using it within a 
>>>>> GLMM in R (I have asked here 
>>>>> <http://stats.stackexchange.com/questions/249738/how-to-define-the-nu-parameter-of-conway-maxwell-poisson-in-spamm-package> 
>>>>> and here 
>>>>> <http://stats.stackexchange.com/questions/249798/conway-maxwell-poisson-with-crossed-random-effects-in-r>).
>>>>> I have seen that Ben Bolker suggested (here 
>>>>> <https://stat.ethz.ch/pipermail/r-sig-mixed-models/2014q1/021945.html>and 
>>>>> here 
>>>>> <http://stats.stackexchange.com/questions/92156/how-to-handle-underdispersion-in-glmm-binomial-outcome-variable>) 
>>>>> to use an ordinal model in cases like this(e.g. _ordinal:clmm_). I have 
>>>>> tried this solution and the results I obtain makes (biological) sense to 
>>>>> me. However, I wonder why but I cannot put all the three crossed random 
>>>>> effects I have in the clmm model (_Error: no. random effects (=1254) >= 
>>>>> no. observations (=854)_) whereas it is not a problem for the glmer 
>>>>> model (the no. of levels of each single random effect does not exceed 854)*.
>>>>> Beyond that, and that's what I would like to ask you, *I cannot find a 
>>>>> reference to justify I used the ordinal model* to deal with 
>>>>> underdispersed count data (referee will ask it for sure).
>>>>> Best,
>>>>>
>>>>> Simone
>>>>>
>>>>> * FMglmer<- glmer(fledges ~ habitatF * (areaPatchFath + poligF01 + 
>>>>> StdLayingDate + ageFath1 + ageMoth1) + (1|year) + (1|ringMoth) + 
>>>>> (1|ringFath), data = datiDRS)
>>>>>     FMclmm<- glmer(as.factor(fledges)~ habitatF * (areaPatchFath + 
>>>>> poligF01 + StdLayingDate + ageFath1 + ageMoth1) + (1|year) + 
>>>>> (1|ringMoth) + (1|ringFath), data = datiDRS)
>>>>>
>>>>>
>>>>> 	[[alternative HTML version deleted]]
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-mixed-models at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>>
>>>
>>> -- 
>>> Simone Santoro
>>> PhD
>>> Department of Ethology and Biodiversity Conservation
>>> Do?ana Biological Station
>>> Calle Am?rico Vespucio s/n
>>> 41092 Seville - Spain
>>> Phone no. +34 954 466 700 (ext. 1213)
>>> http://www.researchgate.net/profile/Simone_Santoro
>>> http://orcid.org/0000-0003-0986-3278
>>
> 
> -- 
> Simone Santoro
> PhD
> Department of Ethology and Biodiversity Conservation
> Do?ana Biological Station
> Calle Am?rico Vespucio s/n
> 41092 Seville - Spain
> Phone no. +34 954 466 700 (ext. 1213)
> http://www.researchgate.net/profile/Simone_Santoro
> http://orcid.org/0000-0003-0986-3278
>


From wolfgang.viechtbauer at maastrichtuniversity.nl  Thu Dec 15 09:50:17 2016
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (SP))
Date: Thu, 15 Dec 2016 08:50:17 +0000
Subject: [R-sig-ME] metafor: estimate correlation between response
 variables (meta-analysis)
In-Reply-To: <bafcd1dd-219d-bf92-6996-5c9ae5f06cef@gmail.com>
References: <2c4b2c6d-d3c1-8c16-1ce4-a74ca4783cfc@gmail.com>
	<7baa7258b2604e1d858e951be1a08e8e@UM-MAIL3216.unimaas.nl>
	<eadd2e69-36c1-8a9f-b2d8-b80836e5bfa6@gmail.com>
	<4f26495753164d03973a2da4de2b5478@UM-MAIL3216.unimaas.nl>
	<bafcd1dd-219d-bf92-6996-5c9ae5f06cef@gmail.com>
Message-ID: <60541c030af2425ba64eaa9e3d2383f7@UM-MAIL3216.unimaas.nl>

After you fit the "res.with.covar" model, you have:

print(res.no.covar, digits=3)

which should be:

print(res.with.covar, digits=3)

which shows a correlation of -0.967.

I haven't really delved into the rest of the code, so I cannot say much about that.

Best,
Wolfgang

> -----Original Message-----
> From: Gustaf Granath [mailto:gustaf.granath at gmail.com]
> Sent: Wednesday, December 14, 2016 15:56
> To: r-sig-mixed-models at r-project.org
> Cc: Viechtbauer Wolfgang (SP)
> Subject: Re: metafor: estimate correlation between response variables
> (meta-analysis)
> 
> I tried to add between response covariances with each study, here using
> the 'cor*sqrt(v1*v2)' approach. I think I got it right but the estimated
> correlation coef (between responses across studies) dont change between
> a model without correlation within studies, compared to a model with
> this correlation. Not sure if it is just the way I set up my toy
> example, or if Im doing something wrong.
> 
> require(metafor)
> 
> # make data
> set.seed(1)
> dat <- data.frame(exp = gl(8,4), resp = gl(2,2, length=32), yi =
> rnorm(32,10,4),
>        vi = rnorm(32,4,0.5), vi.cov = rnorm(32,0.5,0.25) )
> dat$vi.cov <- rep(c(0.1,0.15),8, each=2)
> dat$st.depend <- interaction(dat$exp, dat$resp)
> 
> # known var-cov matrix (within-study dependence for each response)
> # load function 'v_func' first (see below)
> V = v_func(dat = dat, study.dep = "st.depend", var.y = "vi" , var.y.cov
> = "vi.cov")
> 
> # run model
> res.no.covar <- rma.mv(yi, V, mods = ~ resp - 1,
>                random = ~ resp | exp,
>                struct="UN", data=dat, method="REML")
> print(res.no.covar, digits=3)
> # correlation = -0.613
> 
> # test with correlation between responses within studies
> cor.r = 0.5
> 
> covaris = list()
> for (i in 1:nlevels(dat$exp)) {
>    studid <- dat$exp == levels(dat$exp)[i]
>    vari <- dat[ studid, "vi"]
> 
>    # make cor matrix
>    a <- matrix(nrow=length(vari), ncol=length(vari))
>    diag(a) <- 1
>    a[lower.tri(a)] <-  cor.r
>    a[upper.tri(a)] <- cor.r
> 
>    # calculate covariance for the given cor coef
>    b <- sqrt(vari) %*% t(sqrt(vari))
>    a_cov <- b * a  # covariance matrix
>    # cov2cor(a_cov) #test
> 
>    # store var-cov block for each study
>    covaris[[i]] <- a_cov
> }
> 
> # var-cov matrix for the between response correlation
> # within each study
> V2 <- as.matrix(bdiag(lapply(covaris, function (x) x) ))
> # cov2cor(V2) #test if cor is correct
> 
> # add correlation between responses to
> # within-response matrix
> V = ifelse(V==0, V2, V)
> 
> # run mode with complete var-covar matrix
> res.with.covar <- rma.mv(yi, V, mods = ~ resp - 1,
>                         random = ~ resp | exp,
>                         struct="UN", data=dat, method="REML")
> print(res.no.covar, digits=3)
> # correlation still -0.613....
> 
> # function to create var-covariance matrix
> 
> v_func <- function (dat, study.dep, var.y , var.y.cov) {
> 
>    dat.agg <- data.frame(st = unique(dat[[study.dep]]), len =
> rle(as.numeric(dat[[study.dep]]))$lengths,
>                          var = unique(dat[[var.y.cov]]))
>    require(Matrix)
>    ll = list()
>    for (i in 1:NROW(dat.agg)) {
>      ll[[i]] <- matrix(dat.agg$var[i], ncol = dat.agg$len[i], nrow =
> dat.agg$len[i])
>    }
>    mat <- as.matrix(bdiag(lapply(ll, function (x) x) ))
>    diag(mat) <- dat[[var.y]]
>    return(mat)
> }
> 
> On 2016-12-14 11:14, Viechtbauer Wolfgang (SP) wrote:
> > So there really is a 4x4 var-cov matrix for each study then. So, like
> this:
> >
> >             trt1-var1 trt1-var2 trt2-var1 trt2-var2
> > trt1-var1  v_11      cov_11,12 cov_11,21 cov_11,22
> > trt1-var2            v_12      cov_12,21 cov_12,22
> > trt2-var1                      v_21      cov_21,22
> > trt2-var2                                v_22
> >
> > (leaving out subscript i -- but there is such a 4x4 block for each
> study).
> >
> > And the covariance for different treatments (for the same variable?) is
> known. So that would be cov_11,21 and cov_12,22. That leaves 4 unknown
> covariances. So yes, one could impute those (by making some reasonable
> assumptions about the correlations and then back-computing the
> covariances using appropriate equations) followed by a sensitivity
> analysis.
> >
> > I agree that implementing something like this can be a bit tricky, but
> can be done.
> >
> > Best,
> > Wolfgang
> >
> >> -----Original Message-----
> >> From: Gustaf Granath [mailto:gustaf.granath at gmail.com]
> >> Sent: Tuesday, December 13, 2016 15:55
> >> To: r-sig-mixed-models at r-project.org
> >> Cc: Viechtbauer Wolfgang (SP)
> >> Subject: Re: metafor: estimate correlation between response variables
> >> (meta-analysis)
> >>
> >> Hi,
> >>
> >> Im sorry if my toy example wasnt clear. You almost got it right
> though.
> >> But yi is not independent for different treatments within a study. I
> >> have the estimated covariance though, so no problem to account for
> this.
> >> In my example I did include this covariance when I constructed V (I
> used
> >> the outcome covariance in the berkey1998 data to illustrate this). If
> I
> >> understand you correctly, the best way check the influence of
> >> within-study correlation between responses, is to manually add
> >> covariances (try a range of reasonable values) between responses in V.
> I
> >> will try that - although, Im not sure that it is easy to implement.
> >>
> >> Thanks,
> >>
> >> Gustaf
> >>
> >> On 2016-12-13 13:41, Viechtbauer Wolfgang (SP) wrote:
> >>> I have a hard time making sense of that toy example.
> >>>
> >>> But if I understand you correctly, you have data like this:
> >>>
> >>> study trt respvar yi vi
> >>> -----------------------
> >>> 1     1   1       .  .
> >>> 1     1   2       .  .
> >>> 1     2   1       .  .
> >>> 1     2   2       .  .
> >>> 2     1   1       .  .
> >>> 2     1   2       .  .
> >>> 2     2   1       .  .
> >>> 2     2   2       .  .
> >>> ...
> >>>
> >>> where 'yi' and 'vi' are the observed outcomes and corresponding
> >> variances.
> >>> Within studies, can we assume that 'yi' is independent for different
> >> treatments? Then V (the var-cov matrix of the 'yi' vector) will be
> block-
> >> diagonal, each block being a 2x2 matrix (since you only need to
> consider
> >> the covariance between 'yi' for respvar 1 and 'yi' for respvar 2). And
> >> the problem is that the covariances are unknown. Correct so far?
> >>> If so, the 'R' argument has nothing to do with this. If you want to
> >> approach this by means of a sensitivity analysis, then just impute the
> >> unknown covariances directly into 'V' and analyze by means of an
> >> appropriate multilevel/multivariate model. Something like this:
> >>> dat$study.trt <- interaction(dat$study, dat$trt)
> >>> rma.mv(yi, V, mods = ~ factor(trt) - 1, random = list(~ 1 | study, ~
> >> respvar | study.trt), struct="UN", data=dat)
> >>> To impute the covariances, you may be able to use cor*sqrt(v1*v2),
> >> where 'cor' is some kind of assumed correlation (maybe constant across
> >> studies, maybe not). However, whether this is appropriate depends on
> your
> >> outcome measure. For example, this would be fine for means or mean
> >> differences, but the covariance between standardized mean differences
> >> cannot be computed that way (see Gleser & Olkin, 2009, for the correct
> >> equation).
> >>> Gleser, L. J., & Olkin, I. (2009). Stochastically dependent effect
> >> sizes. In H. Cooper, L. V. Hedges, & J. C. Valentine (Eds.), The
> handbook
> >> of research synthesis and meta-analysis (2nd ed., pp. 357-376). New
> York:
> >> Russell Sage Foundation.
> >>> Best,
> >>> Wolfgang
> >>>
> >>>> -----Original Message-----
> >>>> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-
> >>>> project.org] On Behalf Of Gustaf Granath
> >>>> Sent: Tuesday, December 13, 2016 11:42
> >>>> To: r-sig-mixed-models at r-project.org
> >>>> Subject: [R-sig-ME] metafor: estimate correlation between response
> >>>> variables (meta-analysis)
> >>>>
> >>>> Hi,
> >>>> Im trying the estimate the correlation between two response variable
> >> in
> >>>> a meta-analysis. Basically, are effects on y1 associated with
> effects
> >> on
> >>>> y2 across studies. Unfortunately, I dont have information on y1-y2
> >>>> correlations within each study. In addition, each study contain
> >> multiple
> >>>> treatments, adding within-study dependence for each response
> variable.
> >>>>
> >>>> Because I dont have the y1-y2 covariance in each study, my idea is
> to
> >>>> run analyses with different covariance/correlation values to explore
> >> how
> >>>> this covariance affect the result. Reading the rma.mv()
> documentation,
> >>>> this seems possible through the R= argument, but I cant figure out
> >> how.
> >>>> Alternatively, I can add covariances in the matrix describing the
> >> known
> >>>> var-covariance matrix of the within-study dependence, but it seemed
> >> like
> >>>> the R argument is a easier solution (I may be wrong though). Below
> >>>> follow code illustrating my problem using the dat.berkey1998
> example.
> >>>>
> >>>> Cheers
> >>>>
> >>>> Gustaf
> >>>>
> >>>> # Estimate correlation between two response variables
> >>>> # with within-study dependence
> >>>> library(metafor)
> >>>>
> >>>> # Make up an example based on the berkey1998 data.
> >>>> # Multiple outcomes (multiple treatments)
> >>>> # within each study, for each response variable, are created.
> >>>> # Covariances between response variables (within studies) are
> unknown.
> >>>> dat <- get(data(dat.berkey1998))
> >>>> st.vcov <- dat[, c("v1i", "v2i")] # save vcov matrices to add later
> >>>> dat <- rbind(dat, dat)
> >>>> dat <- dat[order(dat$trial, dat$outcome),] # fix order
> >>>> dat[,c("v1i", "v2i")] <- rbind(st.vcov,st.vcov) # put back matrices
> >>>> dat$trial.treat <-paste(rep(1:2, nrow(dat)/2), dat$trial, sep="_") #
> >> id
> >>>> for treatments within trials
> >>>>
> >>>> # Covariances within studies, for each response variable, is known.
> >>>> Hence, a
> >>>> # varcov-matrix for each study and response, can be made and added
> as
> >>>> blocks
> >>>> # in a large varcov-matrix for the data set.
> >>>> # First a within-study dependence dummy must be added
> >>>> dat$stud.unit <-  interaction(dat$trial, dat$outcome) # within-trial
> >>>> dependence dummy
> >>>> # Put together known varcov matrix
> >>>> V <- bldiag(lapply(split(dat[,c("v1i", "v2i")], dat$stud.unit),
> >>>> as.matrix))
> >>>>
> >>>> # plot relationship between response variables
> >>>> dat.wide <- reshape(dat, direction="wide",  v.names = "yi", timevar
> =
> >>>> "outcome", idvar = "trial.treat")
> >>>> plot(yi.AL ~yi.PD, dat.wide)
> >>>> cor(dat.wide$yi.AL, dat.wide$yi.PD)
> >>>> # r = 0.39, if using the study outcomes ignoring all
> >>>> dependence/uncertainty.
> >>>>
> >>>> # Run analysis
> >>>> res <- rma.mv(yi, V, mods = ~ outcome - 1,
> >>>>                  random = ~ outcome | trial,
> >>>>                  struct="UN", data=dat, method="REML")
> >>>> print(res, digits=3)
> >>>> # correlation = 0.51, when accounting for dependence etc. But, y1
> and
> >> y2
> >>>> are assumed to be independent
> >>>> # within each study. How to perform sensitivity analyses by adding
> >>>> different values
> >>>> # on this within-study correlation between the two response
> variables?
> >>>>
> >>>> --
> >>>> Gustaf Granath (PhD)
> >>>> Post doc
> >>>> Swedish University of Agricultural Sciences
> >>>> Department of Ecology


From gustaf.granath at gmail.com  Thu Dec 15 10:03:01 2016
From: gustaf.granath at gmail.com (Gustaf Granath)
Date: Thu, 15 Dec 2016 10:03:01 +0100
Subject: [R-sig-ME] metafor: estimate correlation between response
 variables (meta-analysis)
In-Reply-To: <60541c030af2425ba64eaa9e3d2383f7@UM-MAIL3216.unimaas.nl>
References: <2c4b2c6d-d3c1-8c16-1ce4-a74ca4783cfc@gmail.com>
	<7baa7258b2604e1d858e951be1a08e8e@UM-MAIL3216.unimaas.nl>
	<eadd2e69-36c1-8a9f-b2d8-b80836e5bfa6@gmail.com>
	<4f26495753164d03973a2da4de2b5478@UM-MAIL3216.unimaas.nl>
	<bafcd1dd-219d-bf92-6996-5c9ae5f06cef@gmail.com>
	<60541c030af2425ba64eaa9e3d2383f7@UM-MAIL3216.unimaas.nl>
Message-ID: <95691254-1f94-f5e2-13bc-7da99ea85486@gmail.com>

Oh, that is embarrassing. Thanks for spotting my mistake. It also 
explains why I got more sensible results when I tried it on my real data.

Short additional question. Is the plan to implement BLUPs for rma.mv?

Cheers

Gustaf


On 2016-12-15 09:50, Viechtbauer Wolfgang (SP) wrote:
> After you fit the "res.with.covar" model, you have:
>
> print(res.no.covar, digits=3)
>
> which should be:
>
> print(res.with.covar, digits=3)
>
> which shows a correlation of -0.967.
>
> I haven't really delved into the rest of the code, so I cannot say much about that.
>
> Best,
> Wolfgang
>
>> -----Original Message-----
>> From: Gustaf Granath [mailto:gustaf.granath at gmail.com]
>> Sent: Wednesday, December 14, 2016 15:56
>> To: r-sig-mixed-models at r-project.org
>> Cc: Viechtbauer Wolfgang (SP)
>> Subject: Re: metafor: estimate correlation between response variables
>> (meta-analysis)
>>
>> I tried to add between response covariances with each study, here using
>> the 'cor*sqrt(v1*v2)' approach. I think I got it right but the estimated
>> correlation coef (between responses across studies) dont change between
>> a model without correlation within studies, compared to a model with
>> this correlation. Not sure if it is just the way I set up my toy
>> example, or if Im doing something wrong.
>>
>> require(metafor)
>>
>> # make data
>> set.seed(1)
>> dat <- data.frame(exp = gl(8,4), resp = gl(2,2, length=32), yi =
>> rnorm(32,10,4),
>>         vi = rnorm(32,4,0.5), vi.cov = rnorm(32,0.5,0.25) )
>> dat$vi.cov <- rep(c(0.1,0.15),8, each=2)
>> dat$st.depend <- interaction(dat$exp, dat$resp)
>>
>> # known var-cov matrix (within-study dependence for each response)
>> # load function 'v_func' first (see below)
>> V = v_func(dat = dat, study.dep = "st.depend", var.y = "vi" , var.y.cov
>> = "vi.cov")
>>
>> # run model
>> res.no.covar <- rma.mv(yi, V, mods = ~ resp - 1,
>>                 random = ~ resp | exp,
>>                 struct="UN", data=dat, method="REML")
>> print(res.no.covar, digits=3)
>> # correlation = -0.613
>>
>> # test with correlation between responses within studies
>> cor.r = 0.5
>>
>> covaris = list()
>> for (i in 1:nlevels(dat$exp)) {
>>     studid <- dat$exp == levels(dat$exp)[i]
>>     vari <- dat[ studid, "vi"]
>>
>>     # make cor matrix
>>     a <- matrix(nrow=length(vari), ncol=length(vari))
>>     diag(a) <- 1
>>     a[lower.tri(a)] <-  cor.r
>>     a[upper.tri(a)] <- cor.r
>>
>>     # calculate covariance for the given cor coef
>>     b <- sqrt(vari) %*% t(sqrt(vari))
>>     a_cov <- b * a  # covariance matrix
>>     # cov2cor(a_cov) #test
>>
>>     # store var-cov block for each study
>>     covaris[[i]] <- a_cov
>> }
>>
>> # var-cov matrix for the between response correlation
>> # within each study
>> V2 <- as.matrix(bdiag(lapply(covaris, function (x) x) ))
>> # cov2cor(V2) #test if cor is correct
>>
>> # add correlation between responses to
>> # within-response matrix
>> V = ifelse(V==0, V2, V)
>>
>> # run mode with complete var-covar matrix
>> res.with.covar <- rma.mv(yi, V, mods = ~ resp - 1,
>>                          random = ~ resp | exp,
>>                          struct="UN", data=dat, method="REML")
>> print(res.no.covar, digits=3)
>> # correlation still -0.613....
>>
>> # function to create var-covariance matrix
>>
>> v_func <- function (dat, study.dep, var.y , var.y.cov) {
>>
>>     dat.agg <- data.frame(st = unique(dat[[study.dep]]), len =
>> rle(as.numeric(dat[[study.dep]]))$lengths,
>>                           var = unique(dat[[var.y.cov]]))
>>     require(Matrix)
>>     ll = list()
>>     for (i in 1:NROW(dat.agg)) {
>>       ll[[i]] <- matrix(dat.agg$var[i], ncol = dat.agg$len[i], nrow =
>> dat.agg$len[i])
>>     }
>>     mat <- as.matrix(bdiag(lapply(ll, function (x) x) ))
>>     diag(mat) <- dat[[var.y]]
>>     return(mat)
>> }
>>
>> On 2016-12-14 11:14, Viechtbauer Wolfgang (SP) wrote:
>>> So there really is a 4x4 var-cov matrix for each study then. So, like
>> this:
>>>              trt1-var1 trt1-var2 trt2-var1 trt2-var2
>>> trt1-var1  v_11      cov_11,12 cov_11,21 cov_11,22
>>> trt1-var2            v_12      cov_12,21 cov_12,22
>>> trt2-var1                      v_21      cov_21,22
>>> trt2-var2                                v_22
>>>
>>> (leaving out subscript i -- but there is such a 4x4 block for each
>> study).
>>> And the covariance for different treatments (for the same variable?) is
>> known. So that would be cov_11,21 and cov_12,22. That leaves 4 unknown
>> covariances. So yes, one could impute those (by making some reasonable
>> assumptions about the correlations and then back-computing the
>> covariances using appropriate equations) followed by a sensitivity
>> analysis.
>>> I agree that implementing something like this can be a bit tricky, but
>> can be done.
>>> Best,
>>> Wolfgang
>>>
>>>> -----Original Message-----
>>>> From: Gustaf Granath [mailto:gustaf.granath at gmail.com]
>>>> Sent: Tuesday, December 13, 2016 15:55
>>>> To: r-sig-mixed-models at r-project.org
>>>> Cc: Viechtbauer Wolfgang (SP)
>>>> Subject: Re: metafor: estimate correlation between response variables
>>>> (meta-analysis)
>>>>
>>>> Hi,
>>>>
>>>> Im sorry if my toy example wasnt clear. You almost got it right
>> though.
>>>> But yi is not independent for different treatments within a study. I
>>>> have the estimated covariance though, so no problem to account for
>> this.
>>>> In my example I did include this covariance when I constructed V (I
>> used
>>>> the outcome covariance in the berkey1998 data to illustrate this). If
>> I
>>>> understand you correctly, the best way check the influence of
>>>> within-study correlation between responses, is to manually add
>>>> covariances (try a range of reasonable values) between responses in V.
>> I
>>>> will try that - although, Im not sure that it is easy to implement.
>>>>
>>>> Thanks,
>>>>
>>>> Gustaf
>>>>
>>>> On 2016-12-13 13:41, Viechtbauer Wolfgang (SP) wrote:
>>>>> I have a hard time making sense of that toy example.
>>>>>
>>>>> But if I understand you correctly, you have data like this:
>>>>>
>>>>> study trt respvar yi vi
>>>>> -----------------------
>>>>> 1     1   1       .  .
>>>>> 1     1   2       .  .
>>>>> 1     2   1       .  .
>>>>> 1     2   2       .  .
>>>>> 2     1   1       .  .
>>>>> 2     1   2       .  .
>>>>> 2     2   1       .  .
>>>>> 2     2   2       .  .
>>>>> ...
>>>>>
>>>>> where 'yi' and 'vi' are the observed outcomes and corresponding
>>>> variances.
>>>>> Within studies, can we assume that 'yi' is independent for different
>>>> treatments? Then V (the var-cov matrix of the 'yi' vector) will be
>> block-
>>>> diagonal, each block being a 2x2 matrix (since you only need to
>> consider
>>>> the covariance between 'yi' for respvar 1 and 'yi' for respvar 2). And
>>>> the problem is that the covariances are unknown. Correct so far?
>>>>> If so, the 'R' argument has nothing to do with this. If you want to
>>>> approach this by means of a sensitivity analysis, then just impute the
>>>> unknown covariances directly into 'V' and analyze by means of an
>>>> appropriate multilevel/multivariate model. Something like this:
>>>>> dat$study.trt <- interaction(dat$study, dat$trt)
>>>>> rma.mv(yi, V, mods = ~ factor(trt) - 1, random = list(~ 1 | study, ~
>>>> respvar | study.trt), struct="UN", data=dat)
>>>>> To impute the covariances, you may be able to use cor*sqrt(v1*v2),
>>>> where 'cor' is some kind of assumed correlation (maybe constant across
>>>> studies, maybe not). However, whether this is appropriate depends on
>> your
>>>> outcome measure. For example, this would be fine for means or mean
>>>> differences, but the covariance between standardized mean differences
>>>> cannot be computed that way (see Gleser & Olkin, 2009, for the correct
>>>> equation).
>>>>> Gleser, L. J., & Olkin, I. (2009). Stochastically dependent effect
>>>> sizes. In H. Cooper, L. V. Hedges, & J. C. Valentine (Eds.), The
>> handbook
>>>> of research synthesis and meta-analysis (2nd ed., pp. 357-376). New
>> York:
>>>> Russell Sage Foundation.
>>>>> Best,
>>>>> Wolfgang
>>>>>
>>>>>> -----Original Message-----
>>>>>> From: R-sig-mixed-models [mailto:r-sig-mixed-models-bounces at r-
>>>>>> project.org] On Behalf Of Gustaf Granath
>>>>>> Sent: Tuesday, December 13, 2016 11:42
>>>>>> To: r-sig-mixed-models at r-project.org
>>>>>> Subject: [R-sig-ME] metafor: estimate correlation between response
>>>>>> variables (meta-analysis)
>>>>>>
>>>>>> Hi,
>>>>>> Im trying the estimate the correlation between two response variable
>>>> in
>>>>>> a meta-analysis. Basically, are effects on y1 associated with
>> effects
>>>> on
>>>>>> y2 across studies. Unfortunately, I dont have information on y1-y2
>>>>>> correlations within each study. In addition, each study contain
>>>> multiple
>>>>>> treatments, adding within-study dependence for each response
>> variable.
>>>>>> Because I dont have the y1-y2 covariance in each study, my idea is
>> to
>>>>>> run analyses with different covariance/correlation values to explore
>>>> how
>>>>>> this covariance affect the result. Reading the rma.mv()
>> documentation,
>>>>>> this seems possible through the R= argument, but I cant figure out
>>>> how.
>>>>>> Alternatively, I can add covariances in the matrix describing the
>>>> known
>>>>>> var-covariance matrix of the within-study dependence, but it seemed
>>>> like
>>>>>> the R argument is a easier solution (I may be wrong though). Below
>>>>>> follow code illustrating my problem using the dat.berkey1998
>> example.
>>>>>> Cheers
>>>>>>
>>>>>> Gustaf
>>>>>>
>>>>>> # Estimate correlation between two response variables
>>>>>> # with within-study dependence
>>>>>> library(metafor)
>>>>>>
>>>>>> # Make up an example based on the berkey1998 data.
>>>>>> # Multiple outcomes (multiple treatments)
>>>>>> # within each study, for each response variable, are created.
>>>>>> # Covariances between response variables (within studies) are
>> unknown.
>>>>>> dat <- get(data(dat.berkey1998))
>>>>>> st.vcov <- dat[, c("v1i", "v2i")] # save vcov matrices to add later
>>>>>> dat <- rbind(dat, dat)
>>>>>> dat <- dat[order(dat$trial, dat$outcome),] # fix order
>>>>>> dat[,c("v1i", "v2i")] <- rbind(st.vcov,st.vcov) # put back matrices
>>>>>> dat$trial.treat <-paste(rep(1:2, nrow(dat)/2), dat$trial, sep="_") #
>>>> id
>>>>>> for treatments within trials
>>>>>>
>>>>>> # Covariances within studies, for each response variable, is known.
>>>>>> Hence, a
>>>>>> # varcov-matrix for each study and response, can be made and added
>> as
>>>>>> blocks
>>>>>> # in a large varcov-matrix for the data set.
>>>>>> # First a within-study dependence dummy must be added
>>>>>> dat$stud.unit <-  interaction(dat$trial, dat$outcome) # within-trial
>>>>>> dependence dummy
>>>>>> # Put together known varcov matrix
>>>>>> V <- bldiag(lapply(split(dat[,c("v1i", "v2i")], dat$stud.unit),
>>>>>> as.matrix))
>>>>>>
>>>>>> # plot relationship between response variables
>>>>>> dat.wide <- reshape(dat, direction="wide",  v.names = "yi", timevar
>> =
>>>>>> "outcome", idvar = "trial.treat")
>>>>>> plot(yi.AL ~yi.PD, dat.wide)
>>>>>> cor(dat.wide$yi.AL, dat.wide$yi.PD)
>>>>>> # r = 0.39, if using the study outcomes ignoring all
>>>>>> dependence/uncertainty.
>>>>>>
>>>>>> # Run analysis
>>>>>> res <- rma.mv(yi, V, mods = ~ outcome - 1,
>>>>>>                   random = ~ outcome | trial,
>>>>>>                   struct="UN", data=dat, method="REML")
>>>>>> print(res, digits=3)
>>>>>> # correlation = 0.51, when accounting for dependence etc. But, y1
>> and
>>>> y2
>>>>>> are assumed to be independent
>>>>>> # within each study. How to perform sensitivity analyses by adding
>>>>>> different values
>>>>>> # on this within-study correlation between the two response
>> variables?
>>>>>> --
>>>>>> Gustaf Granath (PhD)
>>>>>> Post doc
>>>>>> Swedish University of Agricultural Sciences
>>>>>> Department of Ecology


From wolfgang.viechtbauer at maastrichtuniversity.nl  Thu Dec 15 10:29:36 2016
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (SP))
Date: Thu, 15 Dec 2016 09:29:36 +0000
Subject: [R-sig-ME] metafor: estimate correlation between response
 variables (meta-analysis)
In-Reply-To: <95691254-1f94-f5e2-13bc-7da99ea85486@gmail.com>
References: <2c4b2c6d-d3c1-8c16-1ce4-a74ca4783cfc@gmail.com>
	<7baa7258b2604e1d858e951be1a08e8e@UM-MAIL3216.unimaas.nl>
	<eadd2e69-36c1-8a9f-b2d8-b80836e5bfa6@gmail.com>
	<4f26495753164d03973a2da4de2b5478@UM-MAIL3216.unimaas.nl>
	<bafcd1dd-219d-bf92-6996-5c9ae5f06cef@gmail.com>
	<60541c030af2425ba64eaa9e3d2383f7@UM-MAIL3216.unimaas.nl>
	<95691254-1f94-f5e2-13bc-7da99ea85486@gmail.com>
Message-ID: <2a4f775944d44e2495a2ae7a66487f44@UM-MAIL3216.unimaas.nl>

Yes, making blup() and ranef() work with 'rma.mv' objects is on my to-do list.

Best,
Wolfgang

> -----Original Message-----
> From: Gustaf Granath [mailto:gustaf.granath at gmail.com]
> Sent: Thursday, December 15, 2016 10:03
> To: r-sig-mixed-models at r-project.org
> Cc: Viechtbauer Wolfgang (SP)
> Subject: Re: metafor: estimate correlation between response variables
> (meta-analysis)
> 
> Oh, that is embarrassing. Thanks for spotting my mistake. It also
> explains why I got more sensible results when I tried it on my real data.
> 
> Short additional question. Is the plan to implement BLUPs for rma.mv?
> 
> Cheers
> 
> Gustaf
> 
> On 2016-12-15 09:50, Viechtbauer Wolfgang (SP) wrote:
> > After you fit the "res.with.covar" model, you have:
> >
> > print(res.no.covar, digits=3)
> >
> > which should be:
> >
> > print(res.with.covar, digits=3)
> >
> > which shows a correlation of -0.967.
> >
> > I haven't really delved into the rest of the code, so I cannot say much
> about that.
> >
> > Best,
> > Wolfgang


From dan_476 at hotmail.co.uk  Thu Dec 15 12:45:27 2016
From: dan_476 at hotmail.co.uk (Dan Jackson)
Date: Thu, 15 Dec 2016 11:45:27 +0000
Subject: [R-sig-ME] "varFunc" classes
In-Reply-To: <585281B4.80002@mrc-bsu.cam.ac.uk>
References: <DB5PR0201MB1574EC6E9783D33B81E7E8B4939B0@DB5PR0201MB1574.eurprd02.prod.outlook.com>
	<6b3e8699-076f-77f2-c186-e561fecc13ac@gmail.com>,
	<585281B4.80002@mrc-bsu.cam.ac.uk>
Message-ID: <VI1PR0201MB15826172467746074534B375939D0@VI1PR0201MB1582.eurprd02.prod.outlook.com>

Thanks to everyone for their help, Ben and Wolfgang in particular. I managed to get some results for a toy-warm up example using

fm10 <- lme(OUT ~ factor(study)+treat, data = DD, random = ~ treat-1|study,weights=varIdent(form = ~ 1 | study),control=list(maxIter = 500, msMaxIter = 500))

here DD is a dataframe containing simulated data: 10,000 rows of OUT ("outcome", just normally distributed data) treat (0/1 with probability 0.5 for "success") and study contains the study number from 1 to 10, 1 for the first 1000 observations, 2 for the second 1000 and so on. For fun I made the first and last groups have larger variances to see if varIndent would pick this up -- which it clearly did which is wonderful.

I would not have gotten this far without your help Ben and Wolfgang in particular. My only slight residual (no pun intended?!) concern is that I seem to be getting some O's for DOF for the which is a bit perplexing (for the the factor(study) s in the model). I have done some googling and this DOF issue seems to be a special issue in its own right. Since I am happy to use normal approximations I can get my inferences but this is something that might be worth thinking about, DOF=0 would appear to be just plain wrong to me!?!

I am confident I will be able to fit this type of model to my real data in the new year. Thanks all, Merry Christmas, Dan



________________________________
From: dan at mrc-bsu.cam.ac.uk <dan at mrc-bsu.cam.ac.uk> on behalf of Dan Jackson <dan.jackson at mrc-bsu.cam.ac.uk>
Sent: 15 December 2016 11:42
To: Ben Bolker
Cc: Dan Jackson; r-sig-mixed-models at r-project.org; daniel.jackson at mrc-bsu.cam.ac.uk
Subject: Re: [R-sig-ME] "varFunc" classes

Thanks to everyone for their help, Ben and Wolfgang in particular. I
managed to get some results for a toy-warm up example using

fm10 <- lme(OUT ~ factor(study)+treat, data = DD, random = ~
treat-1|study,weights=varIdent(form = ~ 1 | study),control=list(maxIter
= 500, msMaxIter = 500))

here DD is a dataframe containing simulated data: 10,000 rows of OUT
("outcome", just normally distributed data) treat (0/1 with probability
0.5 for "success") and study contains the study number from 1 to 10, 1
for the first 1000 observations, 2 for the second 1000 and so on. For
fun I made the first and last groups have larger variances to see if
varIndent would pick this up -- which it clearly did which is wonderful.

I would not have gotten this far without your help Ben and Wolfgang in
particular. My only slight residual (no pun intended?!) concern is that
I seem to be getting some O's for DOF for the which is a bit perplexing
(for the the factor(study) s in the model). I have done some googling
and this DOF issue seems to be a special issue in its own right. Since I
am happy to use normal approximations I can get my inferences but this
is something that might be worth thinking about, DOF=0 would appear to
[[elided Hotmail spam]]

I am confident I will be able to fit this type of model to my real data
in the new year. Thanks all, Merry Christmas, Dan

On 14/12/2016 19:02, Ben Bolker wrote:
>    I'm too lazy to see if anyone has answered this one already, so ...
>
> On 16-12-13 03:48 AM, Dan Jackson wrote:
>> Dear lme4 authors,
>>
>> I am sure you are very busy so I will just ask my question very quickly. I
>> was reading the book "Mixed-effects models in S and S-plus" by Pinheiro and
>> Bates. On the top of page 208 of this book, there is a Table 5.1 that
>> implements various "varFunc" classes. One of these classes would seem to be
>> what I need for my data: varIdent - different variances per stratum. I do
>> know that different subets in my data have very different variances you see,
>> so I would need to include this.
>>
>> However this book relates to S-plus and I am not sure if this has been
>> implemented in R, in the glmer package? My data are continuous so I would
>> just need this for lmer (and not glmer). If it has not been implemented is
>> there any "workaround"?
>    It has been implemented in R, but in the nlme package rather than the
> lme4 package (which contains lmer and glmer).
>
>     Historical note: nlme is the package associated with Pinheiro and
> Bates's 2000 book. R's first "stable beta" version (according to
> Wikipedia) was released in 2000.  Doug Bates has been involved in R
> since the beginning (or almost?).
>
>    If you need to do this in lme4::lmer, you can do it in a sort of
> clunky way by setting up dummy variables for group differences and
> adding an individual-level random effect, e.g.
>
>
> data(Orthodont,package="nlme")
>
> O2 <- transform(Orthodont,
>                  obs=seq(nrow(Orthodont))) ## observation-level variance
>
>
> ## since Female var < Male var, we have to use a dummy for Male
> ## to add extra variance for males (won't work the other way because
> ## we can't have a negative variance)
>
> m1 <-lmer(distance ~ age + (age|Subject) +
>           (0+dummy(Sex,"Male")|obs),
>       control=lmerControl(check.nobs.vs.nlev="ignore",
>                           check.nobs.vs.nRE="ignore"),
>       O2)
>
> library(nlme)
> m2 <- lme(distance~age,random=~age|Subject,
>            data=Orthodont,
>            weights=varIdent(form=~1|Sex))
> summary(m2)
>
> all.equal(c(logLik(m1)),c(logLik(m2)))
> all.equal(c(fixef(m1)),c(fixef(m2)),tolerance=1e-6)
>
>
>
>
>> Thanks in advance for any advice, Dan Jackson
>>
>>
>>       [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>


	[[alternative HTML version deleted]]


From bbolker at gmail.com  Thu Dec 15 17:58:32 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 15 Dec 2016 11:58:32 -0500
Subject: [R-sig-ME] "varFunc" classes
In-Reply-To: <585281B4.80002@mrc-bsu.cam.ac.uk>
References: <DB5PR0201MB1574EC6E9783D33B81E7E8B4939B0@DB5PR0201MB1574.eurprd02.prod.outlook.com>
	<6b3e8699-076f-77f2-c186-e561fecc13ac@gmail.com>
	<585281B4.80002@mrc-bsu.cam.ac.uk>
Message-ID: <CABghstTCH4E1YXJpk9qRgfxii1jb8c0qb6RVLpiv0Gopup=0oA@mail.gmail.com>

  It doesn't generally make sense to include a factor ("study" in your
case) as both a random and a fixed effect in the same model; I
appreciate that you're trying to include only the variability among
treatments within studies, but this is a little bit tricky from both a
practical and a conceptual point of view.  Is there a reason that
you're using

 OUT ~ factor(study)+treat, data = DD, random = ~ treat-1|study

and not

 OUT ~ treat, data=DD, random= ~treat|study

or

  OUT ~ treat, data=DD, random=~1|study/treat

?  The latter estimates the variation of treatments within studies as
compound symmetric, which is a little bit easier computationally.



On Thu, Dec 15, 2016 at 6:42 AM, Dan Jackson
<dan.jackson at mrc-bsu.cam.ac.uk> wrote:
> Thanks to everyone for their help, Ben and Wolfgang in particular. I managed
> to get some results for a toy-warm up example using
>
> fm10 <- lme(OUT ~ factor(study)+treat, data = DD, random = ~
> treat-1|study,weights=varIdent(form = ~ 1 | study),control=list(maxIter =
> 500, msMaxIter = 500))
>
> here DD is a dataframe containing simulated data: 10,000 rows of OUT
> ("outcome", just normally distributed data) treat (0/1 with probability 0.5
> for "success") and study contains the study number from 1 to 10, 1 for the
> first 1000 observations, 2 for the second 1000 and so on. For fun I made the
> first and last groups have larger variances to see if varIndent would pick
> this up -- which it clearly did which is wonderful.
>
> I would not have gotten this far without your help Ben and Wolfgang in
> particular. My only slight residual (no pun intended?!) concern is that I
> seem to be getting some O's for DOF for the which is a bit perplexing (for
> the the factor(study) s in the model). I have done some googling and this
> DOF issue seems to be a special issue in its own right. Since I am happy to
> use normal approximations I can get my inferences but this is something that
> might be worth thinking about, DOF=0 would appear to be just plain wrong to
> me!?!
>
> I am confident I will be able to fit this type of model to my real data in
> the new year. Thanks all, Merry Christmas, Dan
>
>
> On 14/12/2016 19:02, Ben Bolker wrote:
>>
>>    I'm too lazy to see if anyone has answered this one already, so ...
>>
>> On 16-12-13 03:48 AM, Dan Jackson wrote:
>>>
>>> Dear lme4 authors,
>>>
>>> I am sure you are very busy so I will just ask my question very quickly.
>>> I
>>> was reading the book "Mixed-effects models in S and S-plus" by Pinheiro
>>> and
>>> Bates. On the top of page 208 of this book, there is a Table 5.1 that
>>> implements various "varFunc" classes. One of these classes would seem to
>>> be
>>> what I need for my data: varIdent - different variances per stratum. I do
>>> know that different subets in my data have very different variances you
>>> see,
>>> so I would need to include this.
>>>
>>> However this book relates to S-plus and I am not sure if this has been
>>> implemented in R, in the glmer package? My data are continuous so I would
>>> just need this for lmer (and not glmer). If it has not been implemented
>>> is
>>> there any "workaround"?
>>
>>    It has been implemented in R, but in the nlme package rather than the
>> lme4 package (which contains lmer and glmer).
>>
>>     Historical note: nlme is the package associated with Pinheiro and
>> Bates's 2000 book. R's first "stable beta" version (according to
>> Wikipedia) was released in 2000.  Doug Bates has been involved in R
>> since the beginning (or almost?).
>>
>>    If you need to do this in lme4::lmer, you can do it in a sort of
>> clunky way by setting up dummy variables for group differences and
>> adding an individual-level random effect, e.g.
>>
>>
>> data(Orthodont,package="nlme")
>>
>> O2 <- transform(Orthodont,
>>                  obs=seq(nrow(Orthodont))) ## observation-level variance
>>
>>
>> ## since Female var < Male var, we have to use a dummy for Male
>> ## to add extra variance for males (won't work the other way because
>> ## we can't have a negative variance)
>>
>> m1 <-lmer(distance ~ age + (age|Subject) +
>>           (0+dummy(Sex,"Male")|obs),
>>       control=lmerControl(check.nobs.vs.nlev="ignore",
>>                           check.nobs.vs.nRE="ignore"),
>>       O2)
>>
>> library(nlme)
>> m2 <- lme(distance~age,random=~age|Subject,
>>            data=Orthodont,
>>            weights=varIdent(form=~1|Sex))
>> summary(m2)
>>
>> all.equal(c(logLik(m1)),c(logLik(m2)))
>> all.equal(c(fixef(m1)),c(fixef(m2)),tolerance=1e-6)
>>
>>
>>
>>
>>> Thanks in advance for any advice, Dan Jackson
>>>
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>


From dan.jackson at mrc-bsu.cam.ac.uk  Thu Dec 15 12:42:44 2016
From: dan.jackson at mrc-bsu.cam.ac.uk (Dan Jackson)
Date: Thu, 15 Dec 2016 11:42:44 +0000
Subject: [R-sig-ME] "varFunc" classes
In-Reply-To: <6b3e8699-076f-77f2-c186-e561fecc13ac@gmail.com>
References: <DB5PR0201MB1574EC6E9783D33B81E7E8B4939B0@DB5PR0201MB1574.eurprd02.prod.outlook.com>
	<6b3e8699-076f-77f2-c186-e561fecc13ac@gmail.com>
Message-ID: <585281B4.80002@mrc-bsu.cam.ac.uk>

Thanks to everyone for their help, Ben and Wolfgang in particular. I 
managed to get some results for a toy-warm up example using

fm10 <- lme(OUT ~ factor(study)+treat, data = DD, random = ~ 
treat-1|study,weights=varIdent(form = ~ 1 | study),control=list(maxIter 
= 500, msMaxIter = 500))

here DD is a dataframe containing simulated data: 10,000 rows of OUT 
("outcome", just normally distributed data) treat (0/1 with probability 
0.5 for "success") and study contains the study number from 1 to 10, 1 
for the first 1000 observations, 2 for the second 1000 and so on. For 
fun I made the first and last groups have larger variances to see if 
varIndent would pick this up -- which it clearly did which is wonderful.

I would not have gotten this far without your help Ben and Wolfgang in 
particular. My only slight residual (no pun intended?!) concern is that 
I seem to be getting some O's for DOF for the which is a bit perplexing 
(for the the factor(study) s in the model). I have done some googling 
and this DOF issue seems to be a special issue in its own right. Since I 
am happy to use normal approximations I can get my inferences but this 
is something that might be worth thinking about, DOF=0 would appear to 
be just plain wrong to me!?!

I am confident I will be able to fit this type of model to my real data 
in the new year. Thanks all, Merry Christmas, Dan

On 14/12/2016 19:02, Ben Bolker wrote:
>    I'm too lazy to see if anyone has answered this one already, so ...
>
> On 16-12-13 03:48 AM, Dan Jackson wrote:
>> Dear lme4 authors,
>>
>> I am sure you are very busy so I will just ask my question very quickly. I
>> was reading the book "Mixed-effects models in S and S-plus" by Pinheiro and
>> Bates. On the top of page 208 of this book, there is a Table 5.1 that
>> implements various "varFunc" classes. One of these classes would seem to be
>> what I need for my data: varIdent - different variances per stratum. I do
>> know that different subets in my data have very different variances you see,
>> so I would need to include this.
>>
>> However this book relates to S-plus and I am not sure if this has been
>> implemented in R, in the glmer package? My data are continuous so I would
>> just need this for lmer (and not glmer). If it has not been implemented is
>> there any "workaround"?
>    It has been implemented in R, but in the nlme package rather than the
> lme4 package (which contains lmer and glmer).
>
>     Historical note: nlme is the package associated with Pinheiro and
> Bates's 2000 book. R's first "stable beta" version (according to
> Wikipedia) was released in 2000.  Doug Bates has been involved in R
> since the beginning (or almost?).
>
>    If you need to do this in lme4::lmer, you can do it in a sort of
> clunky way by setting up dummy variables for group differences and
> adding an individual-level random effect, e.g.
>
>
> data(Orthodont,package="nlme")
>
> O2 <- transform(Orthodont,
>                  obs=seq(nrow(Orthodont))) ## observation-level variance
>
>
> ## since Female var < Male var, we have to use a dummy for Male
> ## to add extra variance for males (won't work the other way because
> ## we can't have a negative variance)
>
> m1 <-lmer(distance ~ age + (age|Subject) +
>           (0+dummy(Sex,"Male")|obs),
>       control=lmerControl(check.nobs.vs.nlev="ignore",
>                           check.nobs.vs.nRE="ignore"),
>       O2)
>
> library(nlme)
> m2 <- lme(distance~age,random=~age|Subject,
>            data=Orthodont,
>            weights=varIdent(form=~1|Sex))
> summary(m2)
>
> all.equal(c(logLik(m1)),c(logLik(m2)))
> all.equal(c(fixef(m1)),c(fixef(m2)),tolerance=1e-6)
>
>
>
>
>> Thanks in advance for any advice, Dan Jackson
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>


From dan.jackson at mrc-bsu.cam.ac.uk  Thu Dec 15 18:39:09 2016
From: dan.jackson at mrc-bsu.cam.ac.uk (dan jackson)
Date: Thu, 15 Dec 2016 17:39:09 -0000
Subject: [R-sig-ME] "varFunc" classes
In-Reply-To: <CABghstTCH4E1YXJpk9qRgfxii1jb8c0qb6RVLpiv0Gopup=0oA@mail.gmail.com>
References: <DB5PR0201MB1574EC6E9783D33B81E7E8B4939B0@DB5PR0201MB1574.eurprd02.prod.outlook.com>
	<6b3e8699-076f-77f2-c186-e561fecc13ac@gmail.com>
	<585281B4.80002@mrc-bsu.cam.ac.uk>
	<CABghstTCH4E1YXJpk9qRgfxii1jb8c0qb6RVLpiv0Gopup=0oA@mail.gmail.com>
Message-ID: <000001d256fa$24b5e980$6e21bc80$@mrc-bsu.cam.ac.uk>

Hi thanks for this Ben I am still at the playing stage (and I am new to all this and so am at the bewildered and learning stage) but I am trying to fit an individual patient data meta-analysis with continuous data for the first time.

In the paper "A general framework for the use of logistic regression models in meta-analysis", Simmonds and Higgins suggest fitting the model for binary outcome data 

glmer(cbind(event,n-event) ? factor(study) + factor(treat) + (treat-1|study), data=HRT2, family=binomial(link="logit?)) 

(see their appendix) so I thought 

fm10 <- lme(OUT ~ factor(study)+treat, data = DD, random = ~  treat-1|study,weights=varIdent(form = ~ 1 | study),control=list(maxIter
= 500, msMaxIter = 500))

Would just be a natural continuous version of this (allowing different residual variances in each study also, this is usually considered important, and also upping the maxits to get convergence). In Higgins and Simmonds treat is binary so +treat and +factor(treat) are the same thing in the fixed effect part of the model, the intuition is (as I understand it) you want a different baseline average in each study (so factor(study)) because this is bound to vary and also the treatment effect to differ randomly from one study to the next, to relax the strong assumption that the treatment effect is the same for each study.

I hope that helps explain my motivation, essentially I am "just" trying to "copy" what Simmonds and Higgins did for binary data in the continuous case.

Dan


-----Original Message-----
From: Ben Bolker [mailto:bbolker at gmail.com] 
Sent: 15 December 2016 16:59
To: Dan Jackson
Cc: Dan Jackson; r-sig-mixed-models at r-project.org; daniel.jackson at mrc-bsu.cam.ac.uk
Subject: Re: [R-sig-ME] "varFunc" classes

  It doesn't generally make sense to include a factor ("study" in your
case) as both a random and a fixed effect in the same model; I appreciate that you're trying to include only the variability among treatments within studies, but this is a little bit tricky from both a practical and a conceptual point of view.  Is there a reason that you're using

 OUT ~ factor(study)+treat, data = DD, random = ~ treat-1|study

and not

 OUT ~ treat, data=DD, random= ~treat|study

or

  OUT ~ treat, data=DD, random=~1|study/treat

?  The latter estimates the variation of treatments within studies as compound symmetric, which is a little bit easier computationally.



On Thu, Dec 15, 2016 at 6:42 AM, Dan Jackson <dan.jackson at mrc-bsu.cam.ac.uk> wrote:
> Thanks to everyone for their help, Ben and Wolfgang in particular. I 
> managed to get some results for a toy-warm up example using
>
> fm10 <- lme(OUT ~ factor(study)+treat, data = DD, random = ~
> treat-1|study,weights=varIdent(form = ~ 1 | 
> treat-1|study),control=list(maxIter =
> 500, msMaxIter = 500))
>
> here DD is a dataframe containing simulated data: 10,000 rows of OUT 
> ("outcome", just normally distributed data) treat (0/1 with 
> probability 0.5 for "success") and study contains the study number 
> from 1 to 10, 1 for the first 1000 observations, 2 for the second 1000 
> and so on. For fun I made the first and last groups have larger 
> variances to see if varIndent would pick this up -- which it clearly did which is wonderful.
>
> I would not have gotten this far without your help Ben and Wolfgang in 
> particular. My only slight residual (no pun intended?!) concern is 
> that I seem to be getting some O's for DOF for the which is a bit 
> perplexing (for the the factor(study) s in the model). I have done 
> some googling and this DOF issue seems to be a special issue in its 
> own right. Since I am happy to use normal approximations I can get my 
> inferences but this is something that might be worth thinking about, 
> DOF=0 would appear to be just plain wrong to me!?!
>
> I am confident I will be able to fit this type of model to my real 
> data in the new year. Thanks all, Merry Christmas, Dan
>
>
> On 14/12/2016 19:02, Ben Bolker wrote:
>>
>>    I'm too lazy to see if anyone has answered this one already, so ...
>>
>> On 16-12-13 03:48 AM, Dan Jackson wrote:
>>>
>>> Dear lme4 authors,
>>>
>>> I am sure you are very busy so I will just ask my question very quickly.
>>> I
>>> was reading the book "Mixed-effects models in S and S-plus" by 
>>> Pinheiro and Bates. On the top of page 208 of this book, there is a 
>>> Table 5.1 that implements various "varFunc" classes. One of these 
>>> classes would seem to be what I need for my data: varIdent - 
>>> different variances per stratum. I do know that different subets in 
>>> my data have very different variances you see, so I would need to 
>>> include this.
>>>
>>> However this book relates to S-plus and I am not sure if this has 
>>> been implemented in R, in the glmer package? My data are continuous 
>>> so I would just need this for lmer (and not glmer). If it has not 
>>> been implemented is there any "workaround"?
>>
>>    It has been implemented in R, but in the nlme package rather than 
>> the
>> lme4 package (which contains lmer and glmer).
>>
>>     Historical note: nlme is the package associated with Pinheiro and 
>> Bates's 2000 book. R's first "stable beta" version (according to
>> Wikipedia) was released in 2000.  Doug Bates has been involved in R 
>> since the beginning (or almost?).
>>
>>    If you need to do this in lme4::lmer, you can do it in a sort of 
>> clunky way by setting up dummy variables for group differences and 
>> adding an individual-level random effect, e.g.
>>
>>
>> data(Orthodont,package="nlme")
>>
>> O2 <- transform(Orthodont,
>>                  obs=seq(nrow(Orthodont))) ## observation-level 
>> variance
>>
>>
>> ## since Female var < Male var, we have to use a dummy for Male ## to 
>> add extra variance for males (won't work the other way because ## we 
>> can't have a negative variance)
>>
>> m1 <-lmer(distance ~ age + (age|Subject) +
>>           (0+dummy(Sex,"Male")|obs),
>>       control=lmerControl(check.nobs.vs.nlev="ignore",
>>                           check.nobs.vs.nRE="ignore"),
>>       O2)
>>
>> library(nlme)
>> m2 <- lme(distance~age,random=~age|Subject,
>>            data=Orthodont,
>>            weights=varIdent(form=~1|Sex))
>> summary(m2)
>>
>> all.equal(c(logLik(m1)),c(logLik(m2)))
>> all.equal(c(fixef(m1)),c(fixef(m2)),tolerance=1e-6)
>>
>>
>>
>>
>>> Thanks in advance for any advice, Dan Jackson
>>>
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list 
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>


From dan_476 at hotmail.co.uk  Thu Dec 15 18:57:09 2016
From: dan_476 at hotmail.co.uk (Dan Jackson)
Date: Thu, 15 Dec 2016 17:57:09 +0000
Subject: [R-sig-ME] "varFunc" classes
In-Reply-To: <CABghstTCH4E1YXJpk9qRgfxii1jb8c0qb6RVLpiv0Gopup=0oA@mail.gmail.com>
References: <DB5PR0201MB1574EC6E9783D33B81E7E8B4939B0@DB5PR0201MB1574.eurprd02.prod.outlook.com>
	<6b3e8699-076f-77f2-c186-e561fecc13ac@gmail.com>
	<585281B4.80002@mrc-bsu.cam.ac.uk>,
	<CABghstTCH4E1YXJpk9qRgfxii1jb8c0qb6RVLpiv0Gopup=0oA@mail.gmail.com>
Message-ID: <VI1PR0201MB1582E0EAFBF6BAA08870D8A1939D0@VI1PR0201MB1582.eurprd02.prod.outlook.com>

Hi thanks for this Ben I am still at the playing stage (and I am new to all this and so am at the bewildered and learning stage) but I am trying to fit an individual patient data meta-analysis with continuous data for the first time.



In the paper "A general framework for the use of logistic regression models in meta-analysis", Simmonds and Higgins suggest fitting the model for binary outcome data



glmer(cbind(event,n-event) ?? factor(study) + factor(treat) + (treat-1|study), data=HRT2, family=binomial(link="logit??))



(see their appendix) so I thought



fm10 <- lme(OUT ~ factor(study)+treat, data = DD, random = ~  treat-1|study,weights=varIdent(form = ~ 1 | study),control=list(maxIter = 500, msMaxIter = 500))



Would just be a natural continuous version of this (allowing different residual variances in each study also, this is usually considered important, and also upping the maxits to get convergence). In Higgins and Simmonds treat is binary so +treat and +factor(treat) are the same thing in the fixed effect part of the model, the intuition is (as I understand it) you want a different baseline average in each study (so factor(study)) because this is bound to vary and also the treatment effect to differ randomly from one study to the next, to relax the strong assumption that the treatment effect is the same for each study.



I hope that helps explain my motivation, essentially I am "just" trying to "copy" what Simmonds and Higgins did for binary data in the continuous case.



Dan



________________________________
From: Ben Bolker <bbolker at gmail.com>
Sent: 15 December 2016 16:58
To: Dan Jackson
Cc: Dan Jackson; r-sig-mixed-models at r-project.org; daniel.jackson at mrc-bsu.cam.ac.uk
Subject: Re: [R-sig-ME] "varFunc" classes


  It doesn't generally make sense to include a factor ("study" in your
case) as both a random and a fixed effect in the same model; I
appreciate that you're trying to include only the variability among
treatments within studies, but this is a little bit tricky from both a
practical and a conceptual point of view.  Is there a reason that
you're using

 OUT ~ factor(study)+treat, data = DD, random = ~ treat-1|study

and not

 OUT ~ treat, data=DD, random= ~treat|study

or

  OUT ~ treat, data=DD, random=~1|study/treat

?  The latter estimates the variation of treatments within studies as
compound symmetric, which is a little bit easier computationally.



On Thu, Dec 15, 2016 at 6:42 AM, Dan Jackson
<dan.jackson at mrc-bsu.cam.ac.uk> wrote:
> Thanks to everyone for their help, Ben and Wolfgang in particular. I managed
> to get some results for a toy-warm up example using
>
> fm10 <- lme(OUT ~ factor(study)+treat, data = DD, random = ~
> treat-1|study,weights=varIdent(form = ~ 1 | study),control=list(maxIter =
> 500, msMaxIter = 500))
>
> here DD is a dataframe containing simulated data: 10,000 rows of OUT
> ("outcome", just normally distributed data) treat (0/1 with probability 0.5
> for "success") and study contains the study number from 1 to 10, 1 for the
> first 1000 observations, 2 for the second 1000 and so on. For fun I made the
> first and last groups have larger variances to see if varIndent would pick
> this up -- which it clearly did which is wonderful.
>
> I would not have gotten this far without your help Ben and Wolfgang in
> particular. My only slight residual (no pun intended?!) concern is that I
> seem to be getting some O's for DOF for the which is a bit perplexing (for
> the the factor(study) s in the model). I have done some googling and this
> DOF issue seems to be a special issue in its own right. Since I am happy to
> use normal approximations I can get my inferences but this is something that
> might be worth thinking about, DOF=0 would appear to be just plain wrong to
> me!?!
>
> I am confident I will be able to fit this type of model to my real data in
> the new year. Thanks all, Merry Christmas, Dan
>
>
> On 14/12/2016 19:02, Ben Bolker wrote:
>>
>>    I'm too lazy to see if anyone has answered this one already, so ...
>>
>> On 16-12-13 03:48 AM, Dan Jackson wrote:
>>>
>>> Dear lme4 authors,
>>>
>>> I am sure you are very busy so I will just ask my question very quickly.
>>> I
>>> was reading the book "Mixed-effects models in S and S-plus" by Pinheiro
>>> and
>>> Bates. On the top of page 208 of this book, there is a Table 5.1 that
>>> implements various "varFunc" classes. One of these classes would seem to
>>> be
>>> what I need for my data: varIdent - different variances per stratum. I do
>>> know that different subets in my data have very different variances you
>>> see,
>>> so I would need to include this.
>>>
>>> However this book relates to S-plus and I am not sure if this has been
>>> implemented in R, in the glmer package? My data are continuous so I would
>>> just need this for lmer (and not glmer). If it has not been implemented
>>> is
>>> there any "workaround"?
>>
>>    It has been implemented in R, but in the nlme package rather than the
>> lme4 package (which contains lmer and glmer).
>>
>>     Historical note: nlme is the package associated with Pinheiro and
>> Bates's 2000 book. R's first "stable beta" version (according to
>> Wikipedia) was released in 2000.  Doug Bates has been involved in R
>> since the beginning (or almost?).
>>
>>    If you need to do this in lme4::lmer, you can do it in a sort of
>> clunky way by setting up dummy variables for group differences and
>> adding an individual-level random effect, e.g.
>>
>>
>> data(Orthodont,package="nlme")
>>
>> O2 <- transform(Orthodont,
>>                  obs=seq(nrow(Orthodont))) ## observation-level variance
>>
>>
>> ## since Female var < Male var, we have to use a dummy for Male
>> ## to add extra variance for males (won't work the other way because
>> ## we can't have a negative variance)
>>
>> m1 <-lmer(distance ~ age + (age|Subject) +
>>           (0+dummy(Sex,"Male")|obs),
>>       control=lmerControl(check.nobs.vs.nlev="ignore",
>>                           check.nobs.vs.nRE="ignore"),
>>       O2)
>>
>> library(nlme)
>> m2 <- lme(distance~age,random=~age|Subject,
>>            data=Orthodont,
>>            weights=varIdent(form=~1|Sex))
>> summary(m2)
>>
>> all.equal(c(logLik(m1)),c(logLik(m2)))
>> all.equal(c(fixef(m1)),c(fixef(m2)),tolerance=1e-6)
>>
>>
>>
>>
>>> Thanks in advance for any advice, Dan Jackson
>>>
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>
>

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Thu Dec 15 19:34:28 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 15 Dec 2016 13:34:28 -0500
Subject: [R-sig-ME] "varFunc" classes
In-Reply-To: <000001d256fa$24b5e980$6e21bc80$@mrc-bsu.cam.ac.uk>
References: <DB5PR0201MB1574EC6E9783D33B81E7E8B4939B0@DB5PR0201MB1574.eurprd02.prod.outlook.com>
	<6b3e8699-076f-77f2-c186-e561fecc13ac@gmail.com>
	<585281B4.80002@mrc-bsu.cam.ac.uk>
	<CABghstTCH4E1YXJpk9qRgfxii1jb8c0qb6RVLpiv0Gopup=0oA@mail.gmail.com>
	<000001d256fa$24b5e980$6e21bc80$@mrc-bsu.cam.ac.uk>
Message-ID: <adc53ff6-181e-6ac1-e172-7631157db856@gmail.com>


  That Simmonds and Higgins's example uses a numeric treatment (0/1)
variable is what makes this work.  If you have a categorical treatment
variable, if it's binary you should turn it into a numeric dummy
variable (you can use dummy() as shown in one of my earlier examples);
if it has >2 categories, this is going to be a little tricky. It makes
perfect sense that you want to allow for variation in the intercept
(baseline) among studies, but I don't quite know why S&H made the main
effect of study a fixed effect, which makes all of this harder ... ?

  Ben Bolker

On 16-12-15 12:39 PM, dan jackson wrote:
> Hi thanks for this Ben I am still at the playing stage (and I am new
> to all this and so am at the bewildered and learning stage) but I am
> trying to fit an individual patient data meta-analysis with
> continuous data for the first time.
> 
> In the paper "A general framework for the use of logistic regression
> models in meta-analysis", Simmonds and Higgins suggest fitting the
> model for binary outcome data
> 
> glmer(cbind(event,n-event) ? factor(study) + factor(treat) +
> (treat-1|study), data=HRT2, family=binomial(link="logit?))
> 
> (see their appendix) so I thought
> 
> fm10 <- lme(OUT ~ factor(study)+treat, data = DD, random = ~
> treat-1|study,weights=varIdent(form = ~ 1 |
> study),control=list(maxIter = 500, msMaxIter = 500))
> 
> Would just be a natural continuous version of this (allowing
> different residual variances in each study also, this is usually
> considered important, and also upping the maxits to get convergence).
> In Higgins and Simmonds treat is binary so +treat and +factor(treat)
> are the same thing in the fixed effect part of the model, the
> intuition is (as I understand it) you want a different baseline
> average in each study (so factor(study)) because this is bound to
> vary and also the treatment effect to differ randomly from one study
> to the next, to relax the strong assumption that the treatment effect
> is the same for each study.
> 
> I hope that helps explain my motivation, essentially I am "just"
> trying to "copy" what Simmonds and Higgins did for binary data in the
> continuous case.
> 
> Dan
> 
> 
> -----Original Message----- From: Ben Bolker
> [mailto:bbolker at gmail.com] Sent: 15 December 2016 16:59 To: Dan
> Jackson Cc: Dan Jackson; r-sig-mixed-models at r-project.org;
> daniel.jackson at mrc-bsu.cam.ac.uk Subject: Re: [R-sig-ME] "varFunc"
> classes
> 
> It doesn't generally make sense to include a factor ("study" in your 
> case) as both a random and a fixed effect in the same model; I
> appreciate that you're trying to include only the variability among
> treatments within studies, but this is a little bit tricky from both
> a practical and a conceptual point of view.  Is there a reason that
> you're using
> 
> OUT ~ factor(study)+treat, data = DD, random = ~ treat-1|study
> 
> and not
> 
> OUT ~ treat, data=DD, random= ~treat|study
> 
> or
> 
> OUT ~ treat, data=DD, random=~1|study/treat
> 
> ?  The latter estimates the variation of treatments within studies as
> compound symmetric, which is a little bit easier computationally.
> 
> 
> 
> On Thu, Dec 15, 2016 at 6:42 AM, Dan Jackson
> <dan.jackson at mrc-bsu.cam.ac.uk> wrote:
>> Thanks to everyone for their help, Ben and Wolfgang in particular.
>> I managed to get some results for a toy-warm up example using
>> 
>> fm10 <- lme(OUT ~ factor(study)+treat, data = DD, random = ~ 
>> treat-1|study,weights=varIdent(form = ~ 1 | 
>> treat-1|study),control=list(maxIter = 500, msMaxIter = 500))
>> 
>> here DD is a dataframe containing simulated data: 10,000 rows of
>> OUT ("outcome", just normally distributed data) treat (0/1 with 
>> probability 0.5 for "success") and study contains the study number
>>  from 1 to 10, 1 for the first 1000 observations, 2 for the second
>> 1000 and so on. For fun I made the first and last groups have
>> larger variances to see if varIndent would pick this up -- which it
>> clearly did which is wonderful.
>> 
>> I would not have gotten this far without your help Ben and Wolfgang
>> in particular. My only slight residual (no pun intended?!) concern
>> is that I seem to be getting some O's for DOF for the which is a
>> bit perplexing (for the the factor(study) s in the model). I have
>> done some googling and this DOF issue seems to be a special issue
>> in its own right. Since I am happy to use normal approximations I
>> can get my inferences but this is something that might be worth
>> thinking about, DOF=0 would appear to be just plain wrong to me!?!
>> 
>> I am confident I will be able to fit this type of model to my real
>>  data in the new year. Thanks all, Merry Christmas, Dan
>> 
>> 
>> On 14/12/2016 19:02, Ben Bolker wrote:
>>> 
>>> I'm too lazy to see if anyone has answered this one already, so
>>> ...
>>> 
>>> On 16-12-13 03:48 AM, Dan Jackson wrote:
>>>> 
>>>> Dear lme4 authors,
>>>> 
>>>> I am sure you are very busy so I will just ask my question very
>>>> quickly. I was reading the book "Mixed-effects models in S and
>>>> S-plus" by Pinheiro and Bates. On the top of page 208 of this
>>>> book, there is a Table 5.1 that implements various "varFunc"
>>>> classes. One of these classes would seem to be what I need for
>>>> my data: varIdent - different variances per stratum. I do know
>>>> that different subets in my data have very different variances
>>>> you see, so I would need to include this.
>>>> 
>>>> However this book relates to S-plus and I am not sure if this
>>>> has been implemented in R, in the glmer package? My data are
>>>> continuous so I would just need this for lmer (and not glmer).
>>>> If it has not been implemented is there any "workaround"?
>>> 
>>> It has been implemented in R, but in the nlme package rather than
>>>  the lme4 package (which contains lmer and glmer).
>>> 
>>> Historical note: nlme is the package associated with Pinheiro and
>>>  Bates's 2000 book. R's first "stable beta" version (according
>>> to Wikipedia) was released in 2000.  Doug Bates has been involved
>>> in R since the beginning (or almost?).
>>> 
>>> If you need to do this in lme4::lmer, you can do it in a sort of
>>>  clunky way by setting up dummy variables for group differences
>>> and adding an individual-level random effect, e.g.
>>> 
>>> 
>>> data(Orthodont,package="nlme")
>>> 
>>> O2 <- transform(Orthodont, obs=seq(nrow(Orthodont))) ##
>>> observation-level variance
>>> 
>>> 
>>> ## since Female var < Male var, we have to use a dummy for Male
>>> ## to add extra variance for males (won't work the other way
>>> because ## we can't have a negative variance)
>>> 
>>> m1 <-lmer(distance ~ age + (age|Subject) + 
>>> (0+dummy(Sex,"Male")|obs), 
>>> control=lmerControl(check.nobs.vs.nlev="ignore", 
>>> check.nobs.vs.nRE="ignore"), O2)
>>> 
>>> library(nlme) m2 <- lme(distance~age,random=~age|Subject, 
>>> data=Orthodont, weights=varIdent(form=~1|Sex)) summary(m2)
>>> 
>>> all.equal(c(logLik(m1)),c(logLik(m2))) 
>>> all.equal(c(fixef(m1)),c(fixef(m2)),tolerance=1e-6)
>>> 
>>> 
>>> 
>>> 
>>>> Thanks in advance for any advice, Dan Jackson
>>>> 
>>>> 
>>>> [[alternative HTML version deleted]]
>>>> 
>>>> _______________________________________________ 
>>>> R-sig-mixed-models at r-project.org mailing list 
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>> 
>> 
>


From dan_476 at hotmail.co.uk  Thu Dec 15 19:49:13 2016
From: dan_476 at hotmail.co.uk (Dan Jackson)
Date: Thu, 15 Dec 2016 18:49:13 +0000
Subject: [R-sig-ME] "varFunc" classes
In-Reply-To: <adc53ff6-181e-6ac1-e172-7631157db856@gmail.com>
References: <DB5PR0201MB1574EC6E9783D33B81E7E8B4939B0@DB5PR0201MB1574.eurprd02.prod.outlook.com>
	<6b3e8699-076f-77f2-c186-e561fecc13ac@gmail.com>
	<585281B4.80002@mrc-bsu.cam.ac.uk>
	<CABghstTCH4E1YXJpk9qRgfxii1jb8c0qb6RVLpiv0Gopup=0oA@mail.gmail.com>
	<000001d256fa$24b5e980$6e21bc80$@mrc-bsu.cam.ac.uk>,
	<adc53ff6-181e-6ac1-e172-7631157db856@gmail.com>
Message-ID: <DB5PR0201MB15740F36F63A547C0EBF23E9939D0@DB5PR0201MB1574.eurprd02.prod.outlook.com>

Higgins and Simmonds probably did not want to use random study effects because this results in the recovery of inter trial information - this may result in bias and to some people is just plain wrong wrong wrong. The best explanation of this is in the Stephen Senn paper "Hans Van Houwelingen and the art of summing up". However despite this I think the alternative of using a random-study effect is probably preferable on balance.


If I am doing something badly wrong by ignoring the DOF=0 and just taking all the estimates and standard errors as valid and at face value (with normal approximations for inferences, ie assuming that the fitted model is ok) then you should probably tell me but otherwise I think this is what I will do - not that I will be making inferences about the study effects in factor(study) anyway so its a rather mute point, tho its a bit disconcerting to be greeted with DOF=O and NaN's in the tables but these things are the realities of using statistical software!


The debate about the best way to use GLMMs for IPD meta-analysis are just getting started I think! Dan




________________________________
From: Ben Bolker <bbolker at gmail.com>
Sent: 15 December 2016 18:34
To: dan jackson
Cc: 'Dan Jackson'; r-sig-mixed-models at r-project.org; daniel.jackson at mrc-bsu.cam.ac.uk
Subject: Re: [R-sig-ME] "varFunc" classes


  That Simmonds and Higgins's example uses a numeric treatment (0/1)
variable is what makes this work.  If you have a categorical treatment
variable, if it's binary you should turn it into a numeric dummy
variable (you can use dummy() as shown in one of my earlier examples);
if it has >2 categories, this is going to be a little tricky. It makes
perfect sense that you want to allow for variation in the intercept
(baseline) among studies, but I don't quite know why S&H made the main
effect of study a fixed effect, which makes all of this harder ... ?

  Ben Bolker

On 16-12-15 12:39 PM, dan jackson wrote:
> Hi thanks for this Ben I am still at the playing stage (and I am new
> to all this and so am at the bewildered and learning stage) but I am
> trying to fit an individual patient data meta-analysis with
> continuous data for the first time.
>
> In the paper "A general framework for the use of logistic regression
> models in meta-analysis", Simmonds and Higgins suggest fitting the
> model for binary outcome data
>
> glmer(cbind(event,n-event) ?? factor(study) + factor(treat) +
> (treat-1|study), data=HRT2, family=binomial(link="logit??))
>
> (see their appendix) so I thought
>
> fm10 <- lme(OUT ~ factor(study)+treat, data = DD, random = ~
> treat-1|study,weights=varIdent(form = ~ 1 |
> study),control=list(maxIter = 500, msMaxIter = 500))
>
> Would just be a natural continuous version of this (allowing
> different residual variances in each study also, this is usually
> considered important, and also upping the maxits to get convergence).
> In Higgins and Simmonds treat is binary so +treat and +factor(treat)
> are the same thing in the fixed effect part of the model, the
> intuition is (as I understand it) you want a different baseline
> average in each study (so factor(study)) because this is bound to
> vary and also the treatment effect to differ randomly from one study
> to the next, to relax the strong assumption that the treatment effect
> is the same for each study.
>
> I hope that helps explain my motivation, essentially I am "just"
> trying to "copy" what Simmonds and Higgins did for binary data in the
> continuous case.
>
> Dan
>
>
> -----Original Message----- From: Ben Bolker
> [mailto:bbolker at gmail.com] Sent: 15 December 2016 16:59 To: Dan
> Jackson Cc: Dan Jackson; r-sig-mixed-models at r-project.org;
> daniel.jackson at mrc-bsu.cam.ac.uk Subject: Re: [R-sig-ME] "varFunc"
> classes
>
> It doesn't generally make sense to include a factor ("study" in your
> case) as both a random and a fixed effect in the same model; I
> appreciate that you're trying to include only the variability among
> treatments within studies, but this is a little bit tricky from both
> a practical and a conceptual point of view.  Is there a reason that
> you're using
>
> OUT ~ factor(study)+treat, data = DD, random = ~ treat-1|study
>
> and not
>
> OUT ~ treat, data=DD, random= ~treat|study
>
> or
>
> OUT ~ treat, data=DD, random=~1|study/treat
>
> ?  The latter estimates the variation of treatments within studies as
> compound symmetric, which is a little bit easier computationally.
>
>
>
> On Thu, Dec 15, 2016 at 6:42 AM, Dan Jackson
> <dan.jackson at mrc-bsu.cam.ac.uk> wrote:
>> Thanks to everyone for their help, Ben and Wolfgang in particular.
>> I managed to get some results for a toy-warm up example using
>>
>> fm10 <- lme(OUT ~ factor(study)+treat, data = DD, random = ~
>> treat-1|study,weights=varIdent(form = ~ 1 |
>> treat-1|study),control=list(maxIter = 500, msMaxIter = 500))
>>
>> here DD is a dataframe containing simulated data: 10,000 rows of
>> OUT ("outcome", just normally distributed data) treat (0/1 with
>> probability 0.5 for "success") and study contains the study number
>>  from 1 to 10, 1 for the first 1000 observations, 2 for the second
>> 1000 and so on. For fun I made the first and last groups have
>> larger variances to see if varIndent would pick this up -- which it
>> clearly did which is wonderful.
>>
>> I would not have gotten this far without your help Ben and Wolfgang
>> in particular. My only slight residual (no pun intended?!) concern
>> is that I seem to be getting some O's for DOF for the which is a
>> bit perplexing (for the the factor(study) s in the model). I have
>> done some googling and this DOF issue seems to be a special issue
>> in its own right. Since I am happy to use normal approximations I
>> can get my inferences but this is something that might be worth
>> thinking about, DOF=0 would appear to be just plain wrong to me!?!
>>
>> I am confident I will be able to fit this type of model to my real
>>  data in the new year. Thanks all, Merry Christmas, Dan
>>
>>
>> On 14/12/2016 19:02, Ben Bolker wrote:
>>>
>>> I'm too lazy to see if anyone has answered this one already, so
>>> ...
>>>
>>> On 16-12-13 03:48 AM, Dan Jackson wrote:
>>>>
>>>> Dear lme4 authors,
>>>>
>>>> I am sure you are very busy so I will just ask my question very
>>>> quickly. I was reading the book "Mixed-effects models in S and
>>>> S-plus" by Pinheiro and Bates. On the top of page 208 of this
>>>> book, there is a Table 5.1 that implements various "varFunc"
>>>> classes. One of these classes would seem to be what I need for
>>>> my data: varIdent - different variances per stratum. I do know
>>>> that different subets in my data have very different variances
>>>> you see, so I would need to include this.
>>>>
>>>> However this book relates to S-plus and I am not sure if this
>>>> has been implemented in R, in the glmer package? My data are
>>>> continuous so I would just need this for lmer (and not glmer).
>>>> If it has not been implemented is there any "workaround"?
>>>
>>> It has been implemented in R, but in the nlme package rather than
>>>  the lme4 package (which contains lmer and glmer).
>>>
>>> Historical note: nlme is the package associated with Pinheiro and
>>>  Bates's 2000 book. R's first "stable beta" version (according
>>> to Wikipedia) was released in 2000.  Doug Bates has been involved
>>> in R since the beginning (or almost?).
>>>
>>> If you need to do this in lme4::lmer, you can do it in a sort of
>>>  clunky way by setting up dummy variables for group differences
>>> and adding an individual-level random effect, e.g.
>>>
>>>
>>> data(Orthodont,package="nlme")
>>>
>>> O2 <- transform(Orthodont, obs=seq(nrow(Orthodont))) ##
>>> observation-level variance
>>>
>>>
>>> ## since Female var < Male var, we have to use a dummy for Male
>>> ## to add extra variance for males (won't work the other way
>>> because ## we can't have a negative variance)
>>>
>>> m1 <-lmer(distance ~ age + (age|Subject) +
>>> (0+dummy(Sex,"Male")|obs),
>>> control=lmerControl(check.nobs.vs.nlev="ignore",
>>> check.nobs.vs.nRE="ignore"), O2)
>>>
>>> library(nlme) m2 <- lme(distance~age,random=~age|Subject,
>>> data=Orthodont, weights=varIdent(form=~1|Sex)) summary(m2)
>>>
>>> all.equal(c(logLik(m1)),c(logLik(m2)))
>>> all.equal(c(fixef(m1)),c(fixef(m2)),tolerance=1e-6)
>>>
>>>
>>>
>>>
>>>> Thanks in advance for any advice, Dan Jackson
>>>>
>>>>
>>>> [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-mixed-models at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>>>
>>
>

	[[alternative HTML version deleted]]


From pierre.de.villemereuil at mailoo.org  Fri Dec 16 13:43:30 2016
From: pierre.de.villemereuil at mailoo.org (Pierre de Villemereuil)
Date: Fri, 16 Dec 2016 13:43:30 +0100
Subject: [R-sig-ME] GLMM with bounded parameters
Message-ID: <9950991.z2NfXqJvfC@flyosflip>

Dear all,

I'm trying to fit a predictive bell curve on count data with Poisson noise 
around the curve. The idea is to estimate the optimum of fitness according to 
some trait.

The good news is that I don't need to resort to non-linear modelling to do 
that, because the exponential link combined with a polynomial formula can do 
the job as shown in the dummy example below:
x <- runif(300,0,10)
fitness <- rpois(300,10*dnorm(x,mean=5,sd=2))

mod = glm(fitness ~ x + I(x^2),family="poisson")
plot(fitness ~ x)
points(predict(mod,type="response") ~ x,col="red")

My problem is that I'd like to impose some constraints on the parameters to 
ensure that a bell-shape is fitted, e.g. that the parameter for x is positive 
and the parameter for I(x^2) is negative.

Is there a way to enforce such constraints in mixed models packages available 
in R? I'm currently using lme4, but I'm happy to switch to any other package.

Cheers,
Pierre.


From pierre.de.villemereuil at mailoo.org  Fri Dec 16 13:32:10 2016
From: pierre.de.villemereuil at mailoo.org (Pierre de Villemereuil)
Date: Fri, 16 Dec 2016 13:32:10 +0100
Subject: [R-sig-ME] GLMM with bounded parameters
Message-ID: <1803818.bQCDlFdsum@flyosflip>

Dear all,

I'm trying to fit a predictive bell curve on count data with Poisson noise 
around the curve. The idea is to estimate the optimum of fitness according to 
some trait.

The good news is that I don't need to resort to non-linear modelling to do 
that, because the exponential link combined with a polynomial formula can do 
the job as shown in the dummy example below:
x <- runif(300,0,10)
fitness <- rpois(300,10*dnorm(x,mean=5,sd=2))

mod = glm(fitness ~ x + I(x^2),family="poisson")
plot(fitness ~ x)
points(predict(mod,type="response") ~ x,col="red")

My problem is that I'd like to impose some constraints on the parameters to 
ensure that a bell-shape is fitted, e.g. that the parameter for x is positive 
and the parameter for I(x^2) is negative.

Is there a way to enforce such constraints in mixed models packages available 
in R? I'm currently using lme4, but I'm happy to switch to any other package.

Cheers,
Pierre.


From thierry.onkelinx at inbo.be  Fri Dec 16 17:11:02 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Fri, 16 Dec 2016 17:11:02 +0100
Subject: [R-sig-ME] GLMM with bounded parameters
In-Reply-To: <9950991.z2NfXqJvfC@flyosflip>
References: <9950991.z2NfXqJvfC@flyosflip>
Message-ID: <CAJuCY5yXuAVZEWEm2LhYg3yJioK6aUTcjw-Ebp6Sd=hof9rbWQ@mail.gmail.com>

Dear Pierre,

You could fit the model with a Bayesian technique and use an informative
prior.

Here is an example using INLA (download it from r-inla.org)

Note that I've forced x to be negative and x2 to be positive to illustrate
their effect.

x <- runif(300,0,10)
fitness <- rpois(300,10*dnorm(x,mean=5,sd=2))
library(INLA)
library(ggplot2)
dataset <- data.frame(
  fitness,
  x,
  x2 = x ^ 2
)
model <- inla(
  fitness ~ x + x2,
  data = dataset,
  family = "poisson",
  control.fixed = list(
    mean = list(x = -10, x2 = .2),
    prec = list(x = 1, x2 = 1e4)
  ),
  control.predictor = list(compute = TRUE)
)
summary(model)
dataset$fit <- model$summary.fitted.values$mean
ggplot(dataset, aes(x = x, y = fitness)) +
  geom_point() +
  geom_line(aes(y = fit))

Best regards,


ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-12-16 13:43 GMT+01:00 Pierre de Villemereuil <
pierre.de.villemereuil at mailoo.org>:

> Dear all,
>
> I'm trying to fit a predictive bell curve on count data with Poisson noise
> around the curve. The idea is to estimate the optimum of fitness according
> to
> some trait.
>
> The good news is that I don't need to resort to non-linear modelling to do
> that, because the exponential link combined with a polynomial formula can
> do
> the job as shown in the dummy example below:
> x <- runif(300,0,10)
> fitness <- rpois(300,10*dnorm(x,mean=5,sd=2))
>
> mod = glm(fitness ~ x + I(x^2),family="poisson")
> plot(fitness ~ x)
> points(predict(mod,type="response") ~ x,col="red")
>
> My problem is that I'd like to impose some constraints on the parameters to
> ensure that a bell-shape is fitted, e.g. that the parameter for x is
> positive
> and the parameter for I(x^2) is negative.
>
> Is there a way to enforce such constraints in mixed models packages
> available
> in R? I'm currently using lme4, but I'm happy to switch to any other
> package.
>
> Cheers,
> Pierre.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From luca.corlatti at boku.ac.at  Mon Dec 19 09:50:32 2016
From: luca.corlatti at boku.ac.at (Luca Corlatti)
Date: Mon, 19 Dec 2016 09:50:32 +0100
Subject: [R-sig-ME] Random intercept for subgroup
Message-ID: <5857AD68020000680001D24D@gwia2.boku.ac.at>

Dear list members, 
suppose I have a case study with 2 different populations, A and B. For
both groups I have data on body mass, length and sex of a given species.
For group A data were collected over different years, and I may be
interested to include the cohort in the analysis as a random effect,
while for group B I have no such information. The dataframe would look
something like:

Pop		Coh		Weight	Length	Sex
A		2010	12.1		34		m
A		2011	13.4		42		m
A		2012	10.3		36		f
A		2010	10.5		32		m
A		2013	12.1		37		f
A		2014	12.4		35		f
A		2011	14.1		32		f
...
B		NA		13.1		36		m
B		NA		10.5		35		f
B		NA		12.4		32		fB		NA		12.6		35		m
?

Let's now suppose I want to investigate the relationship between Weight
and Length, for males and females of different populations, i.e. using a
3-way interaction.  Would it still be possible to run the analysis in
one single mixed model with a random intercept (cohort) only for group
A, or would I be forced to use 2 different models, one for population A
(random intercept model) and one for population B (fixed effect model)?

With my actual dataset, a lmer (Weight~Length*Sex*Pop + (1 + Pop | Coh)
) fitted changing NA to 1 seems to give the same estimate for Population
B as the lm (Weight~Length*Sex*Pop), but I am not quite sure this is the
correct approach.

Thanks,

Luca



From francois.rousset at umontpellier.fr  Mon Dec 19 10:05:55 2016
From: francois.rousset at umontpellier.fr (=?UTF-8?Q?Fran=c3=a7ois_Rousset?=)
Date: Mon, 19 Dec 2016 10:05:55 +0100
Subject: [R-sig-ME] GLMM with bounded parameters
In-Reply-To: <9950991.z2NfXqJvfC@flyosflip>
References: <9950991.z2NfXqJvfC@flyosflip>
Message-ID: <e34440b3-4322-18af-3a35-b8b5cfbf4578@umontpellier.fr>


Le 16/12/2016 ? 13:43, Pierre de Villemereuil a ?crit :
> Dear all,
>
> I'm trying to fit a predictive bell curve on count data with Poisson noise
> around the curve. The idea is to estimate the optimum of fitness according to
> some trait.
>
> The good news is that I don't need to resort to non-linear modelling to do
> that, because the exponential link combined with a polynomial formula can do
> the job as shown in the dummy example below:
> x <- runif(300,0,10)
> fitness <- rpois(300,10*dnorm(x,mean=5,sd=2))
>
> mod = glm(fitness ~ x + I(x^2),family="poisson")
> plot(fitness ~ x)
> points(predict(mod,type="response") ~ x,col="red")
>
> My problem is that I'd like to impose some constraints on the parameters to
> ensure that a bell-shape is fitted, e.g. that the parameter for x is positive
> and the parameter for I(x^2) is negative.
One can use offsets and a brute-force optim() over offsets that satisfy 
the constraints (and no mixed models in the present case)

x <- runif(300,0,10)
fitness <- rpois(300,10*dnorm(x,mean=5,sd=2))

objfn <- function(par) {
   off <- (par[1]+par[2]*x)*x
   fit <- glm(fitness ~ 1,family="poisson",offset=off)
   - logLik(fit)
}
opt <- optim(c(1,-1),objfn,lower=c(0,-Inf),upper=c(Inf,0),method="L-BFGS-B")

mod <- glm(fitness ~ 
1,family="poisson",offset=with(opt,(par[1]+par[2]*x)*x))

plot(fitness ~ x)
points(predict(mod,type="response") ~ x,col="red")

Best,

F.



>
> Is there a way to enforce such constraints in mixed models packages available
> in R? I'm currently using lme4, but I'm happy to switch to any other package.
>
> Cheers,
> Pierre.
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From pierre.de.villemereuil at mailoo.org  Mon Dec 19 16:44:49 2016
From: pierre.de.villemereuil at mailoo.org (Pierre de Villemereuil)
Date: Mon, 19 Dec 2016 16:44:49 +0100
Subject: [R-sig-ME] GLMM with bounded parameters
In-Reply-To: <e34440b3-4322-18af-3a35-b8b5cfbf4578@umontpellier.fr>
References: <9950991.z2NfXqJvfC@flyosflip>
	<e34440b3-4322-18af-3a35-b8b5cfbf4578@umontpellier.fr>
Message-ID: <2431754.8UasHn8p4S@flyosflip>

Dear all, 

Thank you for your interesting suggestions!

I finally resorted to JAGS to analyse the data, which gives me more control on 
the estimation process (e.g. distinguishing between location and shape 
parameters).

Here's the BUGS model for people interested:

data {
    N <- length(Y)
}

model{
    for (i in 1:N) {
        lat[i] <- scale*exp(-((X[i] - location)/sigma)^2)
        Y[i] ~ dpois(lat[i])
    }
    
    scale ~ dunif(0,20)
    sigma ~ dunif(0,2)
    location ~ dunif(-2,2)
}

Cheers,
Pierre

Le lundi 19 d?cembre 2016, 10:05:55 CET Fran?ois Rousset a ?crit :
> Le 16/12/2016 ? 13:43, Pierre de Villemereuil a ?crit :
> > Dear all,
> > 
> > I'm trying to fit a predictive bell curve on count data with Poisson noise
> > around the curve. The idea is to estimate the optimum of fitness according
> > to some trait.
> > 
> > The good news is that I don't need to resort to non-linear modelling to do
> > that, because the exponential link combined with a polynomial formula can
> > do the job as shown in the dummy example below:
> > x <- runif(300,0,10)
> > fitness <- rpois(300,10*dnorm(x,mean=5,sd=2))
> > 
> > mod = glm(fitness ~ x + I(x^2),family="poisson")
> > plot(fitness ~ x)
> > points(predict(mod,type="response") ~ x,col="red")
> > 
> > My problem is that I'd like to impose some constraints on the parameters
> > to
> > ensure that a bell-shape is fitted, e.g. that the parameter for x is
> > positive and the parameter for I(x^2) is negative.
> 
> One can use offsets and a brute-force optim() over offsets that satisfy
> the constraints (and no mixed models in the present case)
> 
> x <- runif(300,0,10)
> fitness <- rpois(300,10*dnorm(x,mean=5,sd=2))
> 
> objfn <- function(par) {
>    off <- (par[1]+par[2]*x)*x
>    fit <- glm(fitness ~ 1,family="poisson",offset=off)
>    - logLik(fit)
> }
> opt <- optim(c(1,-1),objfn,lower=c(0,-Inf),upper=c(Inf,0),method="L-BFGS-B")
> 
> mod <- glm(fitness ~
> 1,family="poisson",offset=with(opt,(par[1]+par[2]*x)*x))
> 
> plot(fitness ~ x)
> points(predict(mod,type="response") ~ x,col="red")
> 
> Best,
> 
> F.
> 
> > Is there a way to enforce such constraints in mixed models packages
> > available in R? I'm currently using lme4, but I'm happy to switch to any
> > other package.
> > 
> > Cheers,
> > Pierre.
> > 
> > _______________________________________________
> > R-sig-mixed-models at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models


From bbolker at gmail.com  Mon Dec 19 16:54:35 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 19 Dec 2016 10:54:35 -0500
Subject: [R-sig-ME] GLMM with bounded parameters
In-Reply-To: <2431754.8UasHn8p4S@flyosflip>
References: <9950991.z2NfXqJvfC@flyosflip>
	<e34440b3-4322-18af-3a35-b8b5cfbf4578@umontpellier.fr>
	<2431754.8UasHn8p4S@flyosflip>
Message-ID: <7044a90b-6518-9750-61f9-90bac4ac8dae@gmail.com>


  Late on this, but future reference: for glmer (not lmer), parameters
can be bounded by using the modular framework: following the example
from ?modular:

glmod <- glFormula(incidence/size ~ period + (1 | herd),
     data = cbpp, family = binomial, weight=size)
devfun <- do.call(mkGlmerDevfun, glmod)
opt <- optimizeGlmer(devfun)

At this point we can't quite use updateGlmerDevfun exactly as written

rho <- environment(devfun)
rho$nAGQ <- nAGQ  ## specify
## this is the default setting; you can substitute any other lower
##   bounds you like here ...
rho$lower <- c(rho$lower, rep.int(-Inf, length(rho$pp$beta0)))
rho$lp0 <- rho$pp$linPred(1)
rho$dpars <- seq_along(rho$pp$theta)
rho$baseOffset <- forceCopy(rho$resp$offset)
rho$GQmat <- GHrule(nAGQ)
rho$fac <- reTrms$flist[[1]]
devfun <- mkdevfun(rho, nAGQ)

  Then you're ready to finish:

opt <- optimizeGlmer(devfun, stage=2)
mkMerMod(environment(devfun), opt, glmod$reTrms, fr = glmod$fr)


On 16-12-19 10:44 AM, Pierre de Villemereuil wrote:
> Dear all, 
> 
> Thank you for your interesting suggestions!
> 
> I finally resorted to JAGS to analyse the data, which gives me more control on 
> the estimation process (e.g. distinguishing between location and shape 
> parameters).
> 
> Here's the BUGS model for people interested:
> 
> data {
>     N <- length(Y)
> }
> 
> model{
>     for (i in 1:N) {
>         lat[i] <- scale*exp(-((X[i] - location)/sigma)^2)
>         Y[i] ~ dpois(lat[i])
>     }
>     
>     scale ~ dunif(0,20)
>     sigma ~ dunif(0,2)
>     location ~ dunif(-2,2)
> }
> 
> Cheers,
> Pierre
> 
> Le lundi 19 d?cembre 2016, 10:05:55 CET Fran?ois Rousset a ?crit :
>> Le 16/12/2016 ? 13:43, Pierre de Villemereuil a ?crit :
>>> Dear all,
>>>
>>> I'm trying to fit a predictive bell curve on count data with Poisson noise
>>> around the curve. The idea is to estimate the optimum of fitness according
>>> to some trait.
>>>
>>> The good news is that I don't need to resort to non-linear modelling to do
>>> that, because the exponential link combined with a polynomial formula can
>>> do the job as shown in the dummy example below:
>>> x <- runif(300,0,10)
>>> fitness <- rpois(300,10*dnorm(x,mean=5,sd=2))
>>>
>>> mod = glm(fitness ~ x + I(x^2),family="poisson")
>>> plot(fitness ~ x)
>>> points(predict(mod,type="response") ~ x,col="red")
>>>
>>> My problem is that I'd like to impose some constraints on the parameters
>>> to
>>> ensure that a bell-shape is fitted, e.g. that the parameter for x is
>>> positive and the parameter for I(x^2) is negative.
>>
>> One can use offsets and a brute-force optim() over offsets that satisfy
>> the constraints (and no mixed models in the present case)
>>
>> x <- runif(300,0,10)
>> fitness <- rpois(300,10*dnorm(x,mean=5,sd=2))
>>
>> objfn <- function(par) {
>>    off <- (par[1]+par[2]*x)*x
>>    fit <- glm(fitness ~ 1,family="poisson",offset=off)
>>    - logLik(fit)
>> }
>> opt <- optim(c(1,-1),objfn,lower=c(0,-Inf),upper=c(Inf,0),method="L-BFGS-B")
>>
>> mod <- glm(fitness ~
>> 1,family="poisson",offset=with(opt,(par[1]+par[2]*x)*x))
>>
>> plot(fitness ~ x)
>> points(predict(mod,type="response") ~ x,col="red")
>>
>> Best,
>>
>> F.
>>
>>> Is there a way to enforce such constraints in mixed models packages
>>> available in R? I'm currently using lme4, but I'm happy to switch to any
>>> other package.
>>>
>>> Cheers,
>>> Pierre.
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From diego.pavonjordan at gmail.com  Tue Dec 20 17:49:46 2016
From: diego.pavonjordan at gmail.com (Diego Pavon)
Date: Tue, 20 Dec 2016 17:49:46 +0100
Subject: [R-sig-ME] Advice on GLMM with verIdent
Message-ID: <CAD93_FozHF30_X812pv9MjDWEUjfN6kk+6X-bEyF_TTUyO3x1g@mail.gmail.com>

Dear all,

I write you because I need some advice about the model I want to fit to my
data, which I suspect it is not too 'correct'...

My response variable is continuous (mean weighted latitude) of 24 species.
I have these mean latitudes calculated for 4 periods (95-99, 00-04, 05-09,
10-13), for each species. Therefore I have 96 observations. I want to see
if there is a trend of the mean latitude over time (indicating a shift in
the population... all the species together). In this case, I would use
Period as a continuous varaible and not a factor. I want to fit a GLMM with
random = SpeciesID. However, data exploration revealed a quadratic
relationship between my response (mean latitude) and Period (my continuous
covariate for time). But also, I am interested also to see whether there
are differences between functional groups (some species may move faster
than others). Thus I include the variable Phylogeny, which is a factor with
5 levels. Thus, the model in nlme notation is:

lme(NEnessKM ~ Period_std + I(Period_std^2) + factor(Phylogeny)
          + Period_std : factor(Phylogeny) + I(Period_std^2) :
factor(Phylogeny),
          random = ~1| factor(Species),
          weights = varIdent(form =~ 1 | factor(Species)),
          control = ctrl2,
          method = "REML",
          data = NEness5y)


My concern is that this model might be too complicated. I have only 96
observation. If I follow the rule of 15 observation per parameter
estimated, this model i way to complex. I understand that in order to
include quadratic terms, one must include also the linear effect... and
that would also apply for the interactions. If the relationship between my
response (NEness = Mean Latitude) and Period is a parabola, I guess I
should include also the Period^2 in the interaction (Period^2 * Phylogeny)?
But in that case, I have to include the interaction of Period * Phylogany?

Is there a way to reduce the complexity? Is it totally wrong if I do not
include the linear effects and keep only Period^2?

Thanks for sharing your knowledge and for the advice.

Best

Diego




-- 
*Diego Pav?n Jord?n*

*Finnish Museum of Natural History*
*PO BOX 17 *

*Helsinki. Finland*

*0445061210https://www.researchgate.net/profile/Diego_Pavon-jordan
<https://www.researchgate.net/profile/Diego_Pavon-jordan>*

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Tue Dec 20 22:03:43 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Tue, 20 Dec 2016 22:03:43 +0100
Subject: [R-sig-ME] Advice on GLMM with verIdent
In-Reply-To: <CAD93_FozHF30_X812pv9MjDWEUjfN6kk+6X-bEyF_TTUyO3x1g@mail.gmail.com>
References: <CAD93_FozHF30_X812pv9MjDWEUjfN6kk+6X-bEyF_TTUyO3x1g@mail.gmail.com>
Message-ID: <CAJuCY5ydBD1Q3-bvF-sW-tUdKGF=g7-dtps7jxPq98prSF4bfQ@mail.gmail.com>

Dear Diego,

The linear trend is required otherwise the optimum of the parabola is fixed
a x = 0.

You could try to fit the phylogeny effect as a random effect instead of a
fixed effect. poly(Period, 2) | Phyloge ny and add species as a nested
random intercept.

If you have the individual latitudes you can try to use those instead of
using only their average.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-12-20 17:49 GMT+01:00 Diego Pavon <diego.pavonjordan at gmail.com>:

> Dear all,
>
> I write you because I need some advice about the model I want to fit to my
> data, which I suspect it is not too 'correct'...
>
> My response variable is continuous (mean weighted latitude) of 24 species.
> I have these mean latitudes calculated for 4 periods (95-99, 00-04, 05-09,
> 10-13), for each species. Therefore I have 96 observations. I want to see
> if there is a trend of the mean latitude over time (indicating a shift in
> the population... all the species together). In this case, I would use
> Period as a continuous varaible and not a factor. I want to fit a GLMM with
> random = SpeciesID. However, data exploration revealed a quadratic
> relationship between my response (mean latitude) and Period (my continuous
> covariate for time). But also, I am interested also to see whether there
> are differences between functional groups (some species may move faster
> than others). Thus I include the variable Phylogeny, which is a factor with
> 5 levels. Thus, the model in nlme notation is:
>
> lme(NEnessKM ~ Period_std + I(Period_std^2) + factor(Phylogeny)
>           + Period_std : factor(Phylogeny) + I(Period_std^2) :
> factor(Phylogeny),
>           random = ~1| factor(Species),
>           weights = varIdent(form =~ 1 | factor(Species)),
>           control = ctrl2,
>           method = "REML",
>           data = NEness5y)
>
>
> My concern is that this model might be too complicated. I have only 96
> observation. If I follow the rule of 15 observation per parameter
> estimated, this model i way to complex. I understand that in order to
> include quadratic terms, one must include also the linear effect... and
> that would also apply for the interactions. If the relationship between my
> response (NEness = Mean Latitude) and Period is a parabola, I guess I
> should include also the Period^2 in the interaction (Period^2 * Phylogeny)?
> But in that case, I have to include the interaction of Period * Phylogany?
>
> Is there a way to reduce the complexity? Is it totally wrong if I do not
> include the linear effects and keep only Period^2?
>
> Thanks for sharing your knowledge and for the advice.
>
> Best
>
> Diego
>
>
>
>
> --
> *Diego Pav?n Jord?n*
>
> *Finnish Museum of Natural History*
> *PO BOX 17 *
>
> *Helsinki. Finland*
>
> *0445061210https://www.researchgate.net/profile/Diego_Pavon-jordan
> <https://www.researchgate.net/profile/Diego_Pavon-jordan>*
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models

	[[alternative HTML version deleted]]


From diego.pavonjordan at gmail.com  Tue Dec 20 22:34:47 2016
From: diego.pavonjordan at gmail.com (Diego Pavon)
Date: Tue, 20 Dec 2016 22:34:47 +0100
Subject: [R-sig-ME] Advice on GLMM with verIdent
In-Reply-To: <CAJuCY5ydBD1Q3-bvF-sW-tUdKGF=g7-dtps7jxPq98prSF4bfQ@mail.gmail.com>
References: <CAD93_FozHF30_X812pv9MjDWEUjfN6kk+6X-bEyF_TTUyO3x1g@mail.gmail.com>
	<CAJuCY5ydBD1Q3-bvF-sW-tUdKGF=g7-dtps7jxPq98prSF4bfQ@mail.gmail.com>
Message-ID: <CAD93_Fo0VOrqee+mykqzmd1eRD89n8F266-Mbw_1N=kzNULR3w@mail.gmail.com>

Hello Thierry

Thanks for your answer!

I am interested in the interaction between period and phylogeny, to see
whether the trends differ between groups. If I understood correctly, I
shouldn't put something that I am interested in in the random part, am I
right?

Best

Di


2016-12-20 22:03 GMT+01:00 Thierry Onkelinx <thierry.onkelinx at inbo.be>:

> Dear Diego,
>
> The linear trend is required otherwise the optimum of the parabola is
> fixed a x = 0.
>
> You could try to fit the phylogeny effect as a random effect instead of a
> fixed effect. poly(Period, 2) | Phyloge ny and add species as a nested
> random intercept.
>
> If you have the individual latitudes you can try to use those instead of
> using only their average.
>
> Best regards,
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-12-20 17:49 GMT+01:00 Diego Pavon <diego.pavonjordan at gmail.com>:
>
>> Dear all,
>>
>> I write you because I need some advice about the model I want to fit to my
>> data, which I suspect it is not too 'correct'...
>>
>> My response variable is continuous (mean weighted latitude) of 24 species.
>> I have these mean latitudes calculated for 4 periods (95-99, 00-04, 05-09,
>> 10-13), for each species. Therefore I have 96 observations. I want to see
>> if there is a trend of the mean latitude over time (indicating a shift in
>> the population... all the species together). In this case, I would use
>> Period as a continuous varaible and not a factor. I want to fit a GLMM
>> with
>> random = SpeciesID. However, data exploration revealed a quadratic
>> relationship between my response (mean latitude) and Period (my continuous
>> covariate for time). But also, I am interested also to see whether there
>> are differences between functional groups (some species may move faster
>> than others). Thus I include the variable Phylogeny, which is a factor
>> with
>> 5 levels. Thus, the model in nlme notation is:
>>
>> lme(NEnessKM ~ Period_std + I(Period_std^2) + factor(Phylogeny)
>>           + Period_std : factor(Phylogeny) + I(Period_std^2) :
>> factor(Phylogeny),
>>           random = ~1| factor(Species),
>>           weights = varIdent(form =~ 1 | factor(Species)),
>>           control = ctrl2,
>>           method = "REML",
>>           data = NEness5y)
>>
>>
>> My concern is that this model might be too complicated. I have only 96
>> observation. If I follow the rule of 15 observation per parameter
>> estimated, this model i way to complex. I understand that in order to
>> include quadratic terms, one must include also the linear effect... and
>> that would also apply for the interactions. If the relationship between my
>> response (NEness = Mean Latitude) and Period is a parabola, I guess I
>> should include also the Period^2 in the interaction (Period^2 *
>> Phylogeny)?
>> But in that case, I have to include the interaction of Period * Phylogany?
>>
>> Is there a way to reduce the complexity? Is it totally wrong if I do not
>> include the linear effects and keep only Period^2?
>>
>> Thanks for sharing your knowledge and for the advice.
>>
>> Best
>>
>> Diego
>>
>>
>>
>>
>> --
>> *Diego Pav?n Jord?n*
>>
>> *Finnish Museum of Natural History*
>> *PO BOX 17 *
>>
>> *Helsinki. Finland*
>>
>> *0445061210https://www.researchgate.net/profile/Diego_Pavon-jordan
>> <https://www.researchgate.net/profile/Diego_Pavon-jordan>*
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-mixed-models at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>
>
>


-- 
*Diego Pav?n Jord?n*

*Finnish Museum of Natural History*
*PO BOX 17 *

*Helsinki. Finland*

*0445061210https://www.researchgate.net/profile/Diego_Pavon-jordan
<https://www.researchgate.net/profile/Diego_Pavon-jordan>*

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Wed Dec 21 09:40:14 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Wed, 21 Dec 2016 09:40:14 +0100
Subject: [R-sig-ME] Advice on GLMM with verIdent
In-Reply-To: <CAD93_Fo0VOrqee+mykqzmd1eRD89n8F266-Mbw_1N=kzNULR3w@mail.gmail.com>
References: <CAD93_FozHF30_X812pv9MjDWEUjfN6kk+6X-bEyF_TTUyO3x1g@mail.gmail.com>
	<CAJuCY5ydBD1Q3-bvF-sW-tUdKGF=g7-dtps7jxPq98prSF4bfQ@mail.gmail.com>
	<CAD93_Fo0VOrqee+mykqzmd1eRD89n8F266-Mbw_1N=kzNULR3w@mail.gmail.com>
Message-ID: <CAJuCY5ymvHew6vXh=KqGugEFa4EZY0JwM-8Eh0=AV4hPs1vUyg@mail.gmail.com>

Hi Diego,

poly(Period, 2) | Phylogeny with fit a different parabola for each level of
Pylogeny. It allows the same fit as poly(Period, 2):Phylogeny The main
difference is that the parameters are smaller due to shrinkage.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-12-20 22:34 GMT+01:00 Diego Pavon <diego.pavonjordan at gmail.com>:

> Hello Thierry
>
> Thanks for your answer!
>
> I am interested in the interaction between period and phylogeny, to see
> whether the trends differ between groups. If I understood correctly, I
> shouldn't put something that I am interested in in the random part, am I
> right?
>
> Best
>
> Di
>
>
> 2016-12-20 22:03 GMT+01:00 Thierry Onkelinx <thierry.onkelinx at inbo.be>:
>
>> Dear Diego,
>>
>> The linear trend is required otherwise the optimum of the parabola is
>> fixed a x = 0.
>>
>> You could try to fit the phylogeny effect as a random effect instead of a
>> fixed effect. poly(Period, 2) | Phyloge ny and add species as a nested
>> random intercept.
>>
>> If you have the individual latitudes you can try to use those instead of
>> using only their average.
>>
>> Best regards,
>>
>> ir. Thierry Onkelinx
>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>> and Forest
>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>> Kliniekstraat 25
>> 1070 Anderlecht
>> Belgium
>>
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> 2016-12-20 17:49 GMT+01:00 Diego Pavon <diego.pavonjordan at gmail.com>:
>>
>>> Dear all,
>>>
>>> I write you because I need some advice about the model I want to fit to
>>> my
>>> data, which I suspect it is not too 'correct'...
>>>
>>> My response variable is continuous (mean weighted latitude) of 24
>>> species.
>>> I have these mean latitudes calculated for 4 periods (95-99, 00-04,
>>> 05-09,
>>> 10-13), for each species. Therefore I have 96 observations. I want to see
>>> if there is a trend of the mean latitude over time (indicating a shift in
>>> the population... all the species together). In this case, I would use
>>> Period as a continuous varaible and not a factor. I want to fit a GLMM
>>> with
>>> random = SpeciesID. However, data exploration revealed a quadratic
>>> relationship between my response (mean latitude) and Period (my
>>> continuous
>>> covariate for time). But also, I am interested also to see whether there
>>> are differences between functional groups (some species may move faster
>>> than others). Thus I include the variable Phylogeny, which is a factor
>>> with
>>> 5 levels. Thus, the model in nlme notation is:
>>>
>>> lme(NEnessKM ~ Period_std + I(Period_std^2) + factor(Phylogeny)
>>>           + Period_std : factor(Phylogeny) + I(Period_std^2) :
>>> factor(Phylogeny),
>>>           random = ~1| factor(Species),
>>>           weights = varIdent(form =~ 1 | factor(Species)),
>>>           control = ctrl2,
>>>           method = "REML",
>>>           data = NEness5y)
>>>
>>>
>>> My concern is that this model might be too complicated. I have only 96
>>> observation. If I follow the rule of 15 observation per parameter
>>> estimated, this model i way to complex. I understand that in order to
>>> include quadratic terms, one must include also the linear effect... and
>>> that would also apply for the interactions. If the relationship between
>>> my
>>> response (NEness = Mean Latitude) and Period is a parabola, I guess I
>>> should include also the Period^2 in the interaction (Period^2 *
>>> Phylogeny)?
>>> But in that case, I have to include the interaction of Period *
>>> Phylogany?
>>>
>>> Is there a way to reduce the complexity? Is it totally wrong if I do not
>>> include the linear effects and keep only Period^2?
>>>
>>> Thanks for sharing your knowledge and for the advice.
>>>
>>> Best
>>>
>>> Diego
>>>
>>>
>>>
>>>
>>> --
>>> *Diego Pav?n Jord?n*
>>>
>>> *Finnish Museum of Natural History*
>>> *PO BOX 17 *
>>>
>>> *Helsinki. Finland*
>>>
>>> *0445061210https://www.researchgate.net/profile/Diego_Pavon-jordan
>>> <https://www.researchgate.net/profile/Diego_Pavon-jordan>*
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-mixed-models at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>>
>>
>>
>
>
> --
> *Diego Pav?n Jord?n*
>
> *Finnish Museum of Natural History*
> *PO BOX 17 *
>
> *Helsinki. Finland*
>
> *0445061210https://www.researchgate.net/profile/Diego_Pavon-jordan
> <https://www.researchgate.net/profile/Diego_Pavon-jordan>*
>

	[[alternative HTML version deleted]]


From yokeyong.wong at gmail.com  Thu Dec 22 02:09:05 2016
From: yokeyong.wong at gmail.com (Wong Yoke Yong)
Date: Thu, 22 Dec 2016 09:09:05 +0800
Subject: [R-sig-ME] Fwd: Error using nlme; Malformed factors
In-Reply-To: <CACoL8j5yVNrRUbKbcG=cZbWm89Sxj+Oi-DYgFg6MNqxq4ucEKA@mail.gmail.com>
References: <CACoL8j5yVNrRUbKbcG=cZbWm89Sxj+Oi-DYgFg6MNqxq4ucEKA@mail.gmail.com>
Message-ID: <CACoL8j7Mb--JZ9wM7=woqsED=cg-hg+iY1ScBNnA6Kb8jec2hQ@mail.gmail.com>

Hi all,

Please refer to the attached message.


Regards,
Yoke Yong

---------- Forwarded message ----------
From: Wong Yoke Yong <yokeyong.wong at gmail.com>
Date: Wed, Dec 21, 2016 at 10:24 AM
Subject: Error using nlme; Malformed factors
To: r-help at r-project.org


HI all,

I am using the nlme package to learn multilevel models, and following
examples from the textbook "Discovering Statistics Using R" when it
happened.

[Mixed Models Code][1]

The data set is Honeymoon Period.dat, also downloadable under their
companion website.

[Data Set - Multilevel Models][2]

    require(nlme)
    require(reshape2)
    satisfactionData = read.delim("Honeymoon Period.dat",  header = TRUE)

    restructuredData<-melt(satisfactionData, id = c("Person", "Gender"),
measured = c("Satisfaction_Base", "Satisfaction_6_Months",
"Satisfaction_12_Months", "Satisfaction_18_Months"))
    names(restructuredData)<-c("Person", "Gender", "Time",
"Life_Satisfaction")


    #print(restructuredData)
    #restructuredData.sorted<-restructuredData[order(Person),]

    intercept <-gls(Life_Satisfaction~1, data = restructuredData, method =
"ML", na.action = na.exclude)
    randomIntercept <-lme(Life_Satisfaction ~1, data = restructuredData,
random = ~1|Person, method = "ML",  na.action = na.exclude, control =
list(opt="optim"))
    anova(intercept, randomIntercept)

    timeRI<-update(randomIntercept, .~. + Time)
    timeRS<-update(timeRI, random = ~Time|Person)
    ARModel<-update(timeRS, correlation = corAR1(0, form = ~Time|Person))

The error occured at this moment, when I am trying to update "timeRS"
model. The error is as follows:

    Error in as.character.factor(X[[i]], ...) : malformed factor

Help would be appreciated. Thanks!

  [1]: https://studysites.uk.sagepub.com/dsur/study/DSUR%20R%
20Script%20Files/Chapter%2019%20DSUR%20Mixed%20Models.R
  [2]: https://studysites.uk.sagepub.com/dsur/study/articles.htm


Regards,
Yoke Yong

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Thu Dec 22 10:39:45 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Thu, 22 Dec 2016 10:39:45 +0100
Subject: [R-sig-ME] Fwd: Error using nlme; Malformed factors
In-Reply-To: <CACoL8j7Mb--JZ9wM7=woqsED=cg-hg+iY1ScBNnA6Kb8jec2hQ@mail.gmail.com>
References: <CACoL8j5yVNrRUbKbcG=cZbWm89Sxj+Oi-DYgFg6MNqxq4ucEKA@mail.gmail.com>
	<CACoL8j7Mb--JZ9wM7=woqsED=cg-hg+iY1ScBNnA6Kb8jec2hQ@mail.gmail.com>
Message-ID: <CAJuCY5z4JQ+gm2hj9Qsequz_PAPqKHmzjgBRV5SMVrzwu3rp7g@mail.gmail.com>

Dear Yoke Yong,

You need to transform Time from a factor to an integer.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-12-22 2:09 GMT+01:00 Wong Yoke Yong <yokeyong.wong at gmail.com>:

> Hi all,
>
> Please refer to the attached message.
>
>
> Regards,
> Yoke Yong
>
> ---------- Forwarded message ----------
> From: Wong Yoke Yong <yokeyong.wong at gmail.com>
> Date: Wed, Dec 21, 2016 at 10:24 AM
> Subject: Error using nlme; Malformed factors
> To: r-help at r-project.org
>
>
> HI all,
>
> I am using the nlme package to learn multilevel models, and following
> examples from the textbook "Discovering Statistics Using R" when it
> happened.
>
> [Mixed Models Code][1]
>
> The data set is Honeymoon Period.dat, also downloadable under their
> companion website.
>
> [Data Set - Multilevel Models][2]
>
>     require(nlme)
>     require(reshape2)
>     satisfactionData = read.delim("Honeymoon Period.dat",  header = TRUE)
>
>     restructuredData<-melt(satisfactionData, id = c("Person", "Gender"),
> measured = c("Satisfaction_Base", "Satisfaction_6_Months",
> "Satisfaction_12_Months", "Satisfaction_18_Months"))
>     names(restructuredData)<-c("Person", "Gender", "Time",
> "Life_Satisfaction")
>
>
>     #print(restructuredData)
>     #restructuredData.sorted<-restructuredData[order(Person),]
>
>     intercept <-gls(Life_Satisfaction~1, data = restructuredData, method =
> "ML", na.action = na.exclude)
>     randomIntercept <-lme(Life_Satisfaction ~1, data = restructuredData,
> random = ~1|Person, method = "ML",  na.action = na.exclude, control =
> list(opt="optim"))
>     anova(intercept, randomIntercept)
>
>     timeRI<-update(randomIntercept, .~. + Time)
>     timeRS<-update(timeRI, random = ~Time|Person)
>     ARModel<-update(timeRS, correlation = corAR1(0, form = ~Time|Person))
>
> The error occured at this moment, when I am trying to update "timeRS"
> model. The error is as follows:
>
>     Error in as.character.factor(X[[i]], ...) : malformed factor
>
> Help would be appreciated. Thanks!
>
>   [1]: https://studysites.uk.sagepub.com/dsur/study/DSUR%20R%
> 20Script%20Files/Chapter%2019%20DSUR%20Mixed%20Models.R
>   [2]: https://studysites.uk.sagepub.com/dsur/study/articles.htm
>
>
> Regards,
> Yoke Yong
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>

	[[alternative HTML version deleted]]


From mrguilfoyle at gmail.com  Mon Dec 26 20:21:17 2016
From: mrguilfoyle at gmail.com (Mathew Guilfoyle)
Date: Mon, 26 Dec 2016 19:21:17 +0000
Subject: [R-sig-ME] gamm error/problem fitting a smooth by factor
Message-ID: <45C1844D-27A1-40DA-B101-D2711DB98413@gmail.com>

I have a (unbalanced) dataset of time series collected across several subjects (n~500, ~60000 observations total).  I would like to model the overall smooth time trend of a variable and how this trend differs by various categorical factors, with the subject as a random effect, using gamm (mgcv).

My baseline model 

m1 = gamm(v1 ~ s(time), random=list(id=~1+time), data=dat)

where v1 is a continuous numerical variable, time is measured at irregular subject-specific intervals, id is subject identifier.  Model m1 shows that time is a significant term.

I have then tried to run this model:

m2 = gamm(v1 ~ s(time)+s(time, by=fac1), random=list(id=~1+time), data=dat)

where fac1 is a binary factor.

My intuitive understanding of this is that the first smooth term will capture the overall major trend common to both groups of fac1 and the second smooth will model the deviation from this mean trend for the fac1==1 subgroup.  However, I get the error:

Error in MEestimate(lmeSt, grps) : 
 Singularity in backsolve at level 0, block 1 

I've tried other models to isolate the problem but get the exact same error

m3 = gamm(v1 ~ s(time)+s(time, by=fac1), random=list(id=~1), data=dat)   #remove the time random effect
m4 = gamm(v1 ~ fac1+s(time)+s(time, by=fac1), random=list(id=~1+time), data=dat)   #have the factor as a main effect also

I'm not sure if the whole notion of what I'm trying to do is wrong-headed or if I need to adjust some parameters to get the model (m2) to fit.

Thanks


From mrguilfoyle at gmail.com  Mon Dec 26 20:21:17 2016
From: mrguilfoyle at gmail.com (Mathew Guilfoyle)
Date: Mon, 26 Dec 2016 19:21:17 +0000
Subject: [R-sig-ME] gamm error/problem fitting a smooth by factor
Message-ID: <45C1844D-27A1-40DA-B101-D2711DB98413@gmail.com>

I have a (unbalanced) dataset of time series collected across several subjects (n~500, ~60000 observations total).  I would like to model the overall smooth time trend of a variable and how this trend differs by various categorical factors, with the subject as a random effect, using gamm (mgcv).

My baseline model 

m1 = gamm(v1 ~ s(time), random=list(id=~1+time), data=dat)

where v1 is a continuous numerical variable, time is measured at irregular subject-specific intervals, id is subject identifier.  Model m1 shows that time is a significant term.

I have then tried to run this model:

m2 = gamm(v1 ~ s(time)+s(time, by=fac1), random=list(id=~1+time), data=dat)

where fac1 is a binary factor.

My intuitive understanding of this is that the first smooth term will capture the overall major trend common to both groups of fac1 and the second smooth will model the deviation from this mean trend for the fac1==1 subgroup.  However, I get the error:

Error in MEestimate(lmeSt, grps) : 
 Singularity in backsolve at level 0, block 1 

I've tried other models to isolate the problem but get the exact same error

m3 = gamm(v1 ~ s(time)+s(time, by=fac1), random=list(id=~1), data=dat)   #remove the time random effect
m4 = gamm(v1 ~ fac1+s(time)+s(time, by=fac1), random=list(id=~1+time), data=dat)   #have the factor as a main effect also

I'm not sure if the whole notion of what I'm trying to do is wrong-headed or if I need to adjust some parameters to get the model (m2) to fit.

Thanks


From bbolker at gmail.com  Mon Dec 26 20:33:06 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 26 Dec 2016 14:33:06 -0500
Subject: [R-sig-ME] gamm error/problem fitting a smooth by factor
In-Reply-To: <45C1844D-27A1-40DA-B101-D2711DB98413@gmail.com>
References: <45C1844D-27A1-40DA-B101-D2711DB98413@gmail.com>
Message-ID: <1a5822ea-68c0-3564-17fc-99c116d12ec0@gmail.com>


 I think (others may correct me) you don't want both s(time) and
s(time,by=fac1) in the same model, these will be redundant.

>From ?s:

In the factor ?by? variable case a replicate of the smooth is produced
for each factor level (these smooths will be centered, so the factor
usually needs to be added as a main effect as well).

  This suggests to me that you want

> m2 = gamm(v1 ~ fac1 +s(time, by=fac1), random=list(id=~1+time), data=dat)

  I would try that and check that the results make sense (plot
predictions, etc etc)



On 16-12-26 02:21 PM, Mathew Guilfoyle wrote:
> I have a (unbalanced) dataset of time series collected across several subjects (n~500, ~60000 observations total).  I would like to model the overall smooth time trend of a variable and how this trend differs by various categorical factors, with the subject as a random effect, using gamm (mgcv).
> 
> My baseline model 
> 
> m1 = gamm(v1 ~ s(time), random=list(id=~1+time), data=dat)
> 
> where v1 is a continuous numerical variable, time is measured at irregular subject-specific intervals, id is subject identifier.  Model m1 shows that time is a significant term.
> 
> I have then tried to run this model:
> 
> m2 = gamm(v1 ~ s(time)+s(time, by=fac1), random=list(id=~1+time), data=dat)
> 
> where fac1 is a binary factor.
> 
> My intuitive understanding of this is that the first smooth term will capture the overall major trend common to both groups of fac1 and the second smooth will model the deviation from this mean trend for the fac1==1 subgroup.  However, I get the error:
> 
> Error in MEestimate(lmeSt, grps) : 
>  Singularity in backsolve at level 0, block 1 
> 
> I've tried other models to isolate the problem but get the exact same error
> 
> m3 = gamm(v1 ~ s(time)+s(time, by=fac1), random=list(id=~1), data=dat)   #remove the time random effect
> m4 = gamm(v1 ~ fac1+s(time)+s(time, by=fac1), random=list(id=~1+time), data=dat)   #have the factor as a main effect also
> 
> I'm not sure if the whole notion of what I'm trying to do is wrong-headed or if I need to adjust some parameters to get the model (m2) to fit.
> 
> Thanks
> 
> _______________________________________________
> R-sig-mixed-models at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models
>


From huangrenhuai at gmail.com  Fri Dec 30 23:24:52 2016
From: huangrenhuai at gmail.com (Ren-Huai Huang)
Date: Fri, 30 Dec 2016 16:24:52 -0600
Subject: [R-sig-ME] How to perform a latent variable model over a random
	effect, 
Message-ID: <CAAd=uj4HTPHEXGLvynGq8APq_vyU68gLgWh+=pfrJckCctdAdg@mail.gmail.com>

Re: Random Effect Latent Variable Model



Dear mixed model members,


I am trying to do a weighted latent variable modeling with random effects
in R, which is similar to sas nlmixed general log likelihood over a random
effect (sas code attached here
<https://huangrh.github.io/relvm/vignettes/sascode.html>). The purpose is
to calculate a latent variable from 7 observed variables x1, x2, ? , x7
with weightings w1, w2, ? , w7 to the corresponding likelihood of the
variables (the dataset is attached here
<https://github.com/huangrh/relvm/blob/master/inst/extdata/dat244.csv>).


Both equations of the weighted log likelihood and the random effect log
likelihood are linked here
<https://huangrh.github.io/relvm/vignettes/formula.html> (equations 2 and
3). I was wondering if the subjective function should be the joint
probability of both equations 2 and 3, as shown in equation 4. How to join
the equation 2 with the random effect?


I tried to optimize the function (link to the R code, lines 36-37
<https://github.com/huangrh/relvm/blob/master/R/nll.R>) using optim in R (link
to vignette
<https://huangrh.github.io/relvm/vignettes/Random_Effect_Latent_Variable_Model.html>).
But the result is very different from the sas nlmixed mentioned above
<https://huangrh.github.io/relvm/vignettes/sascode.html>.


Any suggestions are very welcome to help me to do this right in R. Thank
you very much in advance.



Sincerely,

Ren-Huai Huang

	[[alternative HTML version deleted]]


From jasnie111 at gmail.com  Sat Dec 31 12:07:14 2016
From: jasnie111 at gmail.com (jas ni)
Date: Sat, 31 Dec 2016 11:07:14 +0000
Subject: [R-sig-ME] Latent class in Gaussian Mixture model
Message-ID: <CACEyfM8vs3nbGAgeqKPACLpNKYow0i5xMAZ057LuyxzMm7U9NA@mail.gmail.com>

Hi Guys,

I would like to get any advice on the latent class in the mixture model.
But i wish to do latent code by hand without relying on the existing R
package. This is my snippet code to do the finite mixture:

no<-nrows(myData.obs)
prob1 = pi1*dmvnorm(myData.obs,m1,Sigma1);   # probabilities of sample
points under model 1
prob2 = pi2*dmvnorm(myData.obs,m2,Sigma2);   # same for model 2
Z<-rbinom(no,1,prob1/(prob1 + prob2 ))

pi1<-rbeta(1,sum(Z)+1/2,no-sum(Z)+1/2)if (pi1>1/2) {
  pi1<-1-pi1
  Z<-1-Z}

I want to generate the Z values which is able to classify the data points
into the particular Gaussian component such as 0 for the first Gaussian and
1 for the second Gaussian component with the given mean and sigma. However,
if i run the code with 30 number of iterations, only the first and second
iteration giving some changes on the data classification. The next
iteration is remain unchanged which is means that the data point are not
properly assigned to the particular Gaussian component.The results for
first until third iteration for Z are shown below.

The result for the first iteration:

[1] 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0
1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0[61] 0 1 0 0 1 0
1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0
1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0[121] 1 0 1 0 1 0 1 0 1 0 0 1 0 0
0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 1 0
0 1 0 0 0 1 0 0 1 0 1[181] 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0
1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1
0 1 0[241] 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0

The result for the second iteration:

[1] 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0
0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0
 [61] 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0
1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0[121] 1 0 0 1 0
0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0
1 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0[181] 1 0 0 0 1 0 0 1 0 1 0 1 0
0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0
0 0 0 1 0 1 0 1 1 0 0 1[241] 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0
0 1 0 1 1 0 0 1 0 1 0

The result for the third iteration:

[1] 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0
0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0
 [61] 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0
1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0[121] 1 0 0 1 0
0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0
1 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0[181] 1 0 0 0 1 0 0 1 0 1 0 1 0
0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0
0 0 0 1 0 1 0 1 1 0 0 1[241] 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0
0 1 0 1 1 0 0 1 0 1 0

What i want to do is, the Z values for latent class should be changed /
updated based on the current probability prob1 and prob2 in each iteration.

I would feel grateful if you could give me any advice or comment to improve
the solution.

Thank you in advance

- Jas

	[[alternative HTML version deleted]]


